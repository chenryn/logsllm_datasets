system that
looks like a proper PUF from the outside,
exhibiting a input-output behavior indistinguishable from a
proper PUF, but which possesses secret, additional properties
that allow cheating. Its assumed similar input-output behav-
ior shall make it infeasible to distinguish a bad PUF from
a proper PUF by digital challenge-response measurements.
In order to detect bad PUFs, honest parties would need to
physically open the PUF-hardware and to inspect it thor-
oughly, as a regular and dedicated step of the protocol. While
detection of bad PUFs would not even be guaranteed by such
a step (adversaries would presumably develop obfuscation
techniques), it would surely destroy the opened PUF, even
if it was non-manipulated. In addition, the inspection step
would be beyond the capabilities of an average user.
This makes bad PUFs a very simple and effective way to
cheat. From an abstract perspective, bad PUFs exploit the
fact that PUFs are real physical objects. Unlike the clean bi-
nary strings exchanged in classical cryptographic protocols,
these objects may bring about unwanted properties. They can
act as real, physical “Trojans” and other malicious hardware.
Even though there is a practically inﬁnite number of possi-
bilities how Strong PUFs can act, two types of bad PUFs that
we focus on in this paper are (i) PUFs that are numerically
simulatable by their manufacturer (but by no one else),
and (ii) bad PUFs that “log” or record all challenges that
have been applied to them. Both are particularly easy to
implement, but sufﬁce for attacks on existing protocols.
Simulatable Bad PUFs (SIM-PUFs): A simulatable
PUF (or SIM-PUF, for short) is a hardware system that looks
like a PUF, having a challenge-response interface etc., but
which possesses a simulation algorithm Sim. Sim takes as
input any challenge c, and computes in polynomial time the
corresponding response r. It is assumed that Sim has been
derived during the fabrication of the simulatable PUF via
the special construction of the PUF. External parties who
merely have access to the simulatable PUF after fabrication
are not able to derive a simulation model.
In practice there are several possibilities for implementing
simulatable PUFs. A straightforward and very efﬁcient way
is to use a trapdoor one-way permutation or pseudo random
function gs based on a short digital seed s. The hardware
of the simulatable PUF simply implements gs. Whenever
the PUF is interrogated over the digital interface with a
challenge c, the hardware outputs the response r = gs(c).
The party who manufactured the PUF knows both g as
well as seed s and can easily simulate the input-output be-
havior of the PUF. Furthermore, if a cryptographically hard
pseudo-random function is used, it is practically infeasible
for the honest parties to distinguish the bad PUF from a
proper PUF with a real, random output. 3
Challenge-Logging Bad PUFs (CL-PUFs): A second
feature that bad PUFs may possess is challenge-logging.
A challenge-logging PUF (CL-PUF for short) with secret
∗, also called the access challenge, is a malicious
challenge c
piece of hardware that looks like a proper PUF from the
outside (with a challenge-response interface etc.), but which
possesses the following properties:
∗,
1) Except for one input challenge c
the challenge-
response behavior of a CL-PUF is exactly like that of
an underlying, “normal” PUF. Whenever a challenge
∗ is applied to the CL-PUF via its
c unequal to c
interface, the challenge is passed on to the underlying
PUF. The corresponding response r is obtained from
the latter, and the CL-PUF uses this response r as its
output.
3) When challenge c
2) The CL-PUF has a non-volatile memory (NVM) mod-
ule in which it automatically records all challenges that
have been applied to it.
∗ is applied to the CL-PUF,
it
does not pass on this challenge to the underlying
PUF as usual. Instead, the CL-PUF outputs the entire
content of the non-volatile memory module (i.e., all
challenges that have previously been applied to it)
via the challenge-response interface, and erases the
content of the NVM module.
If the PUF has a large, preferably exponential challenge
∗
set, then the probability that someone by chance inputs c
and detects the challenge-logging feature is negligibly small.
Please note that many alternative ways for activating the
output mode of the challenge-logger are conceivable, such
as radiowave triggering etc., and even entirely other forms
of logging and read-out “modes” of the logger are possible
(see below).
CL-PUFs can be implemented particularly easily in any
integrated optical or electrical PUFs. But even for Pappu’s
optical, non-integrated PUF [20] challenge logging appears
feasible. Imagine a special, transparent, additional layer on
top of Pappu’s light scattering token, which is altered by
the incoming laser light. The alteration of the layer would
3The replacement of the internals of a PUF by a pseudo-random
function is particularly hard to detect for any integrated PUFs (be they
optical or electrical), since they communicate with external parties only
via their integrated, digital CRP-interface; the PUF is never measured
directly by the external parties. Such integrated PUFs constitute the clear
majority of currently investigated PUFs. But even for Pappu’s optical PUF,
simulatability can be an issue: It is by no means ruled out that the adversary
builds a light scattering token that has a particular, well-ordered structure,
which leads to simple and simulatable outputs. Current protocols would not
even detect if the adversary used an “empty” plastic token, which did not
contain any scatterers at all, and which was trivially simulatable.
291
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:17 UTC from IEEE Xplore.  Restrictions apply. 
not necessarily be visible by the sheer eye, but could reveal
itself only under UV-light or other special illumination. Such
a sensitive layer would indicate the point of incidence (and
perhaps even the angle) of the challenge, i.e., it would show
some form challenge logging.
Finally, we observe that
there are two fundamentally
different types of CL-PUFs: PUFs that have been malicously
constructed with a challenge-logger from the start; and CL-
PUFs where a logger-module has been added externally
by malicious parties after their construction. The former
seem yet more easy to implement, but also the second type
is a viable attack strategy. In any way, CL-PUFs act as
real, physical Trojans: They record and store security-critical
information and pass it on to the adversary when he holds
possession of the PUF again.
Discussion of Potential Countermeasures: A straight-
forward countermeasure against bad PUFs seems to “authen-
ticate” or “certify” the PUF in one way or the other in order
to detect bad PUFs. For example, a trusted authority (TA)
could send a list of CRPs as a “ﬁngerprint” of a genuine
PUF to the players before any protocol execution. On closer
inspection, however, this countermeasure turns out to be very
problematic, and pretty much falls apart.
First of all, the use of a TA that needs to be called in
every single protocol session would make the use of PUFs in
security protocols obsolete. The aspired functionalities could
then be implemented in a much simpler fashion directly
via the TA, avoiding the signiﬁcant effort of physically
transferring a PUF during the protocol. Secondly, CRP-based
authentication does not rule out externally added malicious
hardware, such as external challenge loggers. The latter do
not affect the CRP-behavior of an existing (and previously
certiﬁed) PUF.
Meaningful “certiﬁcation” of a PUF hence requires not
only to “identify” a PUF. It also must (i) exclude that
external parts have been added to the PUF or that
the
PUF-hardware has been manipulated; and (ii) it should
work ofﬂine, i.e., it must avoid calling a central TA in
every execution of the protocol. Currently, no protocols
or PUF implementations that realize these two properties
have been considered in the literature. Given the current
state of the ﬁeld, it seems hard to design such methods,
even more so at low costs. Physical inspection of the inner
conﬁguration of the PUF as a regular protocol step seems no
viable possibility, as discussed in the previous paragraphs.
Furthermore, if efﬁcient methods for certifying the integrity
of (PUF-)hardware existed, then the same methods could be
applied to protect security modules built on classical keys,
making PUFs obsolete. Once more, this makes bad PUFs a
realistic and efﬁcient method to cheat.
Brzuska et al. [1] indeed assume certiﬁcation of the PUF,
but do not give protocols or methods how it can be achieved.
For the above reasons, we believe that efﬁcient certiﬁcation
is currently infeasible in practice. This holds even more
if malicious players, and not only external adversaries,
generate and use manipulated PUFs. We comment that in
a typical
two-party protocol, a PUF originating from a
malicious party must be considered as nothing else than an
untrusted piece of hardware that stems from the adversary.
Advanced Bad PUFs: How “bad” can a PUF be?
Having focused on simple features in the last section (which
still sufﬁce to attack many existing protocols), we will play
with a number of more sophisticated properties now. The
purpose of our discussion is to complement the picture; we
will not fully work out every construction in detail.
To start with, it is of course possible to imagine bad
PUFs that communicate information (e.g., wirelessly) to
the malicious party. Such a “Communicating PUF” could
transmit the challenge, the response, or both, to fraudulent
parties. The transmission could be carried out in real time, or
may be delayed to later, when the PUF is released from the
control of the honest parties. It is relatively straightforward
that such a feature destroys the security of all existing
protocols. Necessary, but also very costly countermeasures
were shielding the PUF during the protocol and destroying
them immediately afterwards.
Another advanced bad PUF example is a PUF which
transmits all challenges to the malicious party in real-time;
waits for the malicious party to individually select and return
a response Rbad; and then outputs Rbad (as if it was the
natural response of the PUF itself). The latter type of PUF
could be called the Marionette PUF for obvious reasons.
It seems clear that there is no security beneﬁt of using
PUFs in cryptographic protocols if the adversary can use
Marionette PUFs. Their employment makes PUFs useless,
in the sense that for any protocol that uses PUFs and which
securely implements a task T even if Marionette PUFs are
employed, there will be a protocol that securely implements
T and does not use the (Marionette) PUFs at all. Therefore
the existence and use of Marionette PUFs must be ruled
out in most advanced Strong PUF protocols such as OT,
BC and KE by whatever means. One potential, but again
costly countermeasure to prevent Marionette PUFs would
be the shielding of the PUF during the entire course of the
protocol.
A third example are bad PUFs that adapt or alter their
response behavior over time. This adaption could be a func-
tion of the challenge that is applied to them, or a function
of all previous challenges. Other variants of adaptive bad
PUF behavior include the following: (i) The PUF could
automatically alter its response behavior after a certain time
period t0. This means that the malicious party can inﬂuence
the protocol by delaying the protocol; note that
this is
explicitly allowed in the UC-model. (ii) The PUF could
change its CRPs upon a wireless triggering signal it receives.
(iii) The PUF could even change upon a certain, triggering
challenge that is applied to it. This allowed the malicious
party to inﬂuence the bad PUF even while it is not in her
292
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:17 UTC from IEEE Xplore.  Restrictions apply. 
possession, simply by causing the honest party to apply a
certain challenge to the PUF.
A ﬁnal example are bad PUFs that implement arbitrary
digital functions f with special, fraudulent properties. Sim-
ulatable PUFs (where f is simulatable and the simulation
code is known to the malicious party) are one special case
of this approach. But the function f could have other handy
properties for the adversary. For example, it might be a
function for which the computation of inverses is simple.
This case is actually relevant for our attack in Section III-E.
Many other examples of advanced bad PUFs are conceiv-
able. Actually, any such bad PUF types have to be taken
into consideration when the security of a PUF protocol is
analyzed. But since the earlier, simpler types of SIM-PUFs
and CL-PUFs already sufﬁce for attacking many protocols,
we will not deal too much with advanced bad PUFs further
in this paper.
A Final Thought on Bad PUFs: Let us conclude this
section by a general thought. Why are bad PUFs so power-
ful? Consider the following line of thought: Suppose that a
PUF-protocol utilizes some property P of the employed PUF
to achieve its security. Then there will (almost with certainty)
be a bad PUF which is hard to recognize from the outside,
but which does not possess the property P. The security of
the protocol and the validity of the proof will no longer be
guaranteed if the adversary uses this bad PUF not possessing
P. This makes bad PUF a broadly applicable method of
cheating. The cost of implementing the imagined bad PUF
type determines how practically relevant the resulting attack
is; we focused on relatively easily implementable variants
of bad PUFs in this paper.
III. SECURITY EVALUATIONS IN THE PUF RE-USE AND
BAD PUF MODEL
We will now conduct three detailed, exemplary security
analyses in the new attack models. We selected the PUF-
based OT- and KE-protocol of Brzuska et al. from Crypto
2011 [1] and the recent BC-protocol by Ostrovsky et al. [18]
to this end. The protocols are given in a simpliﬁed form in
Appendices B, C and D for the convenience of the readers.
The notation employed in our attacks actually refers to these
appendices. We would like to stress that the ﬁrst two protocol
by Brzuska et al. are secure in their own, original attack
model (apart from a recent attack on Brzuska’s OT-Protocol
by R¨uhrmair and van Dijk [25]). But, as argued earlier, the
protocols would likely be faced with the PUF re-use model
and the bad PUF model once they were used in practice.
In opposition to this, the BC-protocol of Ostrovsky et al.
is actually attacked in their own, original “malicious” PUF
model.
A. OT-Protocol of Brzuska et al. in the PUF Re-Use Model
We start by analyzing the OT-Protocol of Bruzska et al.
[1] (see Protocol 1 in Appendix B) in the PUF re-use model,
or, to be more precise, in the mildest form of the PUF