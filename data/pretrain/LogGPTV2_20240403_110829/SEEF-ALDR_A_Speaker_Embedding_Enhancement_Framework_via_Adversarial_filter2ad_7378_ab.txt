SEEF-ALDR. Section V discusses the potential to use the proposed
framework for a novel identity impersonation attack and audio
event detection task. Finally, we conclude in Section VI.
2 BACKGROUND AND RELATED WORKS
2.1 Speaker Verification
According to the report [43] provided by American consultancy
Transparency Market Research, the global biometrics market will
grow from 11.24 billion US dollars in 2015 to 23.3 billion US dollars
in 2020, with a compound annual growth rate of 15.7%. Among var-
ious biometric authentication techniques, speaker verification has
become a hot topic with a promising development prospect. Due
to the advance of deep learning and artificial intelligence, recent
studies [1, 4, 6, 14, 32, 45, 46, 68, 69] have introduced deep learning
models into speaker verification tasks, thus significantly improving
its accuracy. Therefore, the state-of-the-art speaker verification
systems generally are built on top of deep learning models. Specif-
ically, deep learning-based speaker verification system includes
three major components: a speech extracting module, a speech
pre-processing module, and a speaker model as shown in Figure 1.
Firstly, the speech extracting module collects the analog signal from
a sound source using a microphone and digitizes the signal through
A/D Converter. Then, the speech pre-processing module receives
the digital signal and extracts the corresponding input vector ac-
cording to different pre-processing methods, such as spectrogram
[17], Mel Frequency Cepstrum Coefficient (MFCC) [44], etc. Finally,
the speaker model performs feature analysis on the input vector
for speaker verification and determines whether the authentication
is successful.
The availability of high-quality speech datasets [6, 46] has greatly
promoted the development of speaker verification. In terms of opti-
mizing the model structure, for speaker recognition and verification,
2-dimensional CNN is often used for speaker recognition in both the
time and frequency domain [1, 3, 4, 6, 14, 46], while 1-dimensional
CNN is applied only in the time domain [57â€“59]. The VGG and
ResNet structures are proposed in [6, 46] and in particular, Chung
et al. [6] proposed to map voice spectrogram into a latent space to
measure feature distances. Other works [54, 65] used LSTM-based
front-end architectures to improve the accuracy of speaker fea-
ture extraction. Furthermore, utterance-level frame features are
aggregated by a dictionary-based NetVLAD or GhostVLAD layer
for efficient speaker recognition [68], and Cai et al. [4] obtained
the utterance level representation by introducing a self-attentive
pooling layer and the modified loss function.
In terms of optimizing the objective function, although the soft-
max cross-entropy [47] is one of the most commonly used loss
Figure 1: The Architecture of Speaker Verification System
based on Deep Learning.
function to train speaker embedding models, it does not explic-
itly encourage discriminative learning. To alleviate this problem,
the triplet loss [32, 71] is introduced to optimize the accuracy of
pre-trained models as a discriminative learning method. At the
same time, Hajibabaei et al. [14] and Liu et al. [36] systematically
evaluate the impact of different loss functions and the presence or
absence of the dropout method on the performance of the speaker
embedding models. Moreover, Lee et al. [31] and LinWei-Wei et
al. [34] attempt to employ an autoencoder as its primary structure
for speaker discrimination by acquiring speaker embeddings and
has achieved remarkable performance improvements. However,
despite the remarkable work [27] trying to minimize variability
in speaker representation, identity-unrelated information has not
been considered enough, which could suppress the performance of
existing methods for speaker verification.
2.2 Disentangled Representation
To extract distinct and informative features from the original data,
representation learning has received widespread attention in the
artificial intelligence community. The autoencoder [19, 20] is pro-
posed as a common representation learning method to extract fea-
ture representations from the original data. The encoder extracts
feature representations from the original data, and then the decoder
maps the embeddings from the feature space back to the input space,
which are the processes of encoding and reconstruction. Due to
the capability of encoding expressive information from the data
space automatically, previous works [28, 56, 64] developed various
autoencoder-based models for different tasks, e.g., robust feature ex-
traction, etc. Williams et al. [67] extracted different features through
two different encoders with an autoencoder-like architecture for
the conversion of speaking styles. Although existing works have
made remarkable progress on various tasks, the problem of dis-
entangling the feature representation space has not been studied
comprehensively.
Recently, learning disentangled representation based on gener-
ative models has attracted a lot of interest [22, 70]. In the image
domain, a series of works attempted to disentangle appropriate
representations in various tasks, such as pose-invariant recognition
ACSAC 2020, December 7â€“11, 2020, Austin, USA
Jianwei and Xiaoqi, et al.
[62] and identity-preserving image editing [23, 33]. In the audio
domain, some studies also explored disentangled training by the
adversarial supervision, such as using maximum mean discrepancy
[38] and GANs [41]. Subsequent works [2, 42, 63, 72] obtained ro-
bust speaker representations through domain adversarial learning
by suppressing environmental complexity during speech recording.
However, these works usually require explicit supervision during
the training process and encode each attribute as a separate ele-
ment in the feature vector, which may not be easily available in
real-world situations. Hsu et al. [21] introduced an unsupervised
method to induce the invariance of automatic speech recognition.
From the perspective of data augmentation, Jati et al. [25] improved
the performance of speaker verification through discriminative
training. An inherent drawback of this approach is that the per-
formance drops when dealing with complicated acoustic changes
since it only learned very specific acoustic changes. Besides, data
augmentation cannot explicitly ensure that irrelevant information
is removed from the speaker representation, as shown by various
detection tasks in [55]. Recently, Peri et al. [49] proposed an un-
supervised adversarial invariance architecture to extract robust
speaker-discriminative speech representations. However, this ar-
chitecture does not generalize well, so it is difficult to port existing
models to improve the performance of speaker verification.
2.3 Speech Recognition
Automatic Speech Recognition (ASR)[12, 18] is to enables machines
to recognize and understand human speech. Thanks to the signif-
icant improvement of the accuracy of ASR in recent years, voice
has become an increasingly popular human-computer interaction
mechanism due to its convenience and efficiency. Therefore, lots of
voice-controllable systems and devices have been gradually intro-
duced in the family life of the general public, e.g., Amazon Echo,
Google Home, Apple HomePod, etc. Moreover,open-source plat-
forms, such as Kaldi [51], Carnegie Mellon Universityâ€™s Sphinx [24]
and Mozilla DeepSpeech [15] are also available for research com-
munity. The typical ASR system includes two main components:
a speech acquisition module and a speech model. The former is
composed of audio acquisition equipment and signal processing
equipment. The latter consists of three sub-modules: feature ex-
traction module, acoustic model, and language model. After the
original audio passes through the power amplifier and filter, ASR
system needs to extract acoustic features from the digitized audio
signal.
To a certain extent, the speaker verification system is similar to
ASR system. Both of them are mainly composed of a speech acqui-
sition module and a speech model, with the purpose of extracting
and interpreting specific information from the input audio. How-
ever, ASR system needs to extract text-related information from
the original speech, but the speaker verification system extracts
the text-free, but identity-related information. In other words, ASR
system concentrates on temporal sequence information, while the
speaker verification system focuses on potential spatial information.
Therefore, the advance of ASR system cannot be directly applied to
the speaker verification system, and vice versa.
3 APPROACH
Our goal is to decouple the speaker identity information and identity-
unrelated information contained in the original speech and obtain
the purified speaker embedding to reduce the interference of the
latter to the speaker verification task. In this section, we detail
our SEEF-ALDR approach, a novel trainable network to learn the
disentangled speaker identity features.
3.1 Overview
The key idea of SEEF-ALDR is to decouple the identity-related infor-
mation and identity-unrelated information from the input speech
signal by using adversarial learning, and then extract the purified
identity representation for the speaker verification task without in-
terference from irrelevant features. Figure 2 shows the architecture
of the proposed SEEF-ALDR. Given an input spectrogram ğ‘† from
the original speech, the speaker purifying encoder ğ¸ğ‘ extracts the
identity-purified feature ğ‘“ğ‘, while the speaker eliminating encoder
ğ¸ğ‘’ extracts the identity-unrelated feature ğ‘“ğ‘’. ğ¸ğ‘ and ğ¸ğ‘’ are trained
based on twin networks, with the same multi-layer convolutional
network structure as the to-be-integrated speaker verification mod-
els. The speaker identity labels act as a supervisory signal to train
ğ¸ğ‘, and also adversarially guide the training of ğ¸ğ‘’.
Specifically, the eliminating encoder ğ¸ğ‘’ and adversarial classi-
fier ğ¶ğ‘ğ‘‘ğ‘£ perform a zero-sum game [11]. ğ¶ğ‘ğ‘‘ğ‘£ tries to classify the
speakerâ€™s identity correctly based on the embeddings ğ‘“ğ‘’ from ğ¸ğ‘’,
while ğ¸ğ‘’ attempts to fool ğ¶ğ‘ğ‘‘ğ‘£ by making the prediction results
evenly distributed over each identity label, thus erasing the identity
information from ğ‘“ğ‘’. Finally, ğ‘“ğ‘ and ğ‘“ğ‘’ are combined by the feature
fusion to produce the fused spectrogram feature ğ‘“ğ‘ . The decoder
ğ·ğ‘Ÿ reconstructs a spectrogram ğ‘†â€² from ğ‘“ğ‘  to ensure that ğ‘†â€² is close
enough to the original ğ‘†. Such constraint on the spectrogram ğ‘†â€² re-
quires the fusion feature ğ‘“ğ‘  to contain as much speech information
as possible as the original ğ‘†, so ideally feature ğ‘“ğ‘ and feature ğ‘“ğ‘’ can
be complementary. Therefore, as feature ğ‘“ğ‘’ is encouraged to learn
identity-unrelated information, feature ğ‘“ğ‘ will be forced to extract
all identity-related information.
By continuously iterating the training process, SEEF-ALDR can
learn the distribution of the identity-related information and identity-
unrelated information in the feature space from the speech samples
and decouple them confidently, thereby obtaining more accurate
speaker embeddings. Such embeddings generated by SEEF-ALDR
can be used in the speaker verification task to further improve the
performance of existing speaker verification models. In order to
improve the portability of SEEF-ALDR, we follow a modular design.
The encoders, classifiers, feature fusion, and the reconstruction de-
coder are all implemented in the form of modules. At the same time,
all common pre-processing variables and device configurations are
designed and implemented in the form of the hyper-parameters,
which can be modified directly in the configuration file. Hence, we
only need minor revision to the configuration file and the code of
the to-be-integrated speaker verification models to port them into
the encoders of our framework.
3.2 Purifying Encoder
The goal of the purifying encoder is to obtain a more accurate
speaker embedding based on the identity-purified features extracted
SEEF-ALDR: A Speaker Embedding Enhancement Framework via Adversarial Learning based Disentangled Representation
ACSAC 2020, December 7â€“11, 2020, Austin, USA
Figure 2: The Architecture of SEEF-ALDR.
ğ‘“ğ‘¡ ğ‘— =
with ğœ™(cid:16)ğœƒğ‘¡ ğ‘—
(cid:17)
by the purifying encoder ğ¸ğ‘, which can be simply written as ğ‘“ğ‘ =
ğ¸ğ‘ (ğ‘†). Both the structure of ğ¸ğ‘ and the objective function of the
speaker classifier ğ¶ğ‘ ğ‘ğ‘’ğ‘ğ‘˜ğ‘’ğ‘Ÿ depend on the to-be-integrated speaker
verification model (i.e., the baseline). The goal of ğ¸ğ‘ can be simply
understood as a multi-classification task. Generally, ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ is
often chosen to nonlinearly map the identity-purified features to
the speaker prediction dimension ğ‘ğ‘ . Therefore, the prediction
result of ğ‘¦ğ‘ can be written as:
ğ‘¦ğ‘ = softmax
(1)
We compare ğ‘¦ğ‘ and the encoded speaker identity distribution ğ‘ğ‘ 
ğ‘ of training the speaker
by cross entropy. The objective function ğ¿ğ‘†
classifier ğ¶ğ‘ ğ‘ğ‘’ğ‘ğ‘˜ğ‘’ğ‘Ÿ with ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ can be expressed as:
(cid:16)ğ¶ğ‘ ğ‘ğ‘’ğ‘ğ‘˜ğ‘’ğ‘Ÿ(cid:0)ğ¸ğ‘ (ğ‘†)(cid:1)(cid:17) .
(cid:17) .
ğ‘ğ‘ âˆ‘ï¸
(cid:16)ğ‘¦ ğ‘—
ğ‘ ğ‘—
ğ‘  log
ğ‘
ğ‘ = âˆ’
ğ¿ğ‘†
ğ‘—=1
To improve the performance of speaker verification, previous
works like ğ´ âˆ’ ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ [35, 48] have been proposed based on
the standard ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ function. We also reproduced ğ´ âˆ’ ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥,
which guarantees that the angle of different classes in the feature
space is as large as possible. In other words, the angular margin
between each input sample and the center of its correct category is
dictated as ğ‘š times smaller than those between it and the centers
of other wrong categories. We define ğ‘“ (cid:0)ğ‘¥ ğ‘—(cid:1) as the corresponding
output of the penultimate layer of speaker classifier ğ¶ğ‘ ğ‘ğ‘’ğ‘ğ‘˜ğ‘’ğ‘Ÿ for
the input ğ‘¥ ğ‘—, and ğ‘¡ ğ‘— as the corresponding target label. When the
loss function ğ´ âˆ’ ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ is used, the prediction score given by
ğ´ âˆ’ ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ is the weighted average over the cosine similarity
with weight ğœ†ğ‘ğ‘œğ‘  that decreases during the training process, which
can be expressed as:
(cid:17)
ğœ†ğ‘ğ‘œğ‘  + 1
= (âˆ’1)ğ‘˜ cos
(cid:17) +(cid:13)(cid:13)ğ‘“ (cid:0)ğ‘¥ ğ‘—(cid:1)(cid:13)(cid:13) ğœ™(cid:16)ğœƒğ‘¡ ğ‘—
(cid:16)ğœƒğ‘¡ ğ‘—
ğœ†ğ‘ğ‘œğ‘ (cid:13)(cid:13)ğ‘“ (cid:0)ğ‘¥ ğ‘—(cid:1)(cid:13)(cid:13) cos
(cid:17) âˆ’ 2ğ‘˜, ğœƒğ‘¡ ğ‘— âˆˆ (cid:104) ğ‘˜ğœ‹
(cid:16)ğ‘šğœƒğ‘¡ ğ‘—
ğ‘  log(cid:169)(cid:173)(cid:171)
ğ‘’ ğ‘“ğ‘¡ ğ‘— +ğ‘–â‰ ğ‘¡ ğ‘—
ğ‘’ ğ‘“ğ‘¡ ğ‘—
ğ‘’âˆ¥ğ‘¥ ğ‘—âˆ¥ cos(ğœƒğ‘–)
ğ‘ğ‘ âˆ‘ï¸
ğ‘—=1
ğ‘ ğ‘—
(cid:170)(cid:174)(cid:172) .
(3)
(cid:105) and
ğ‘š , (ğ‘˜+1)ğœ‹
ğ‘š
ğ‘˜ âˆˆ [0, ğ‘š âˆ’ 1], where ğ‘š is an integer to control the size of angu-
lar margin. Therefore, the objective function to train the speaker
classifier ğ¶ğ‘ ğ‘ğ‘’ğ‘ğ‘˜ğ‘’ğ‘Ÿ can be written as:
(2)
ğ¿ğ´âˆ’ğ‘†
ğ‘
= âˆ’
(4)
With a distinct geometric interpretation, the encoder supervised
by ğ´ âˆ’ ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ can learn features that construct a discriminative
angular distance metric on the hypersphere manifold [35]. When
ğ‘š â‰¥ 2, ğ´ âˆ’ ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ loss produces more accurate classification
by forcing different categories to have certain margins of angular
classification in the feature space. To simplify the expression, we use
ğ¿ğ‘ to indicate the objective function to train the speaker classifier
ğ¶ğ‘ ğ‘ğ‘’ğ‘ğ‘˜ğ‘’ğ‘Ÿ thereafter in the paper, which varies based on different
implementation.
EpEeDrfeCspeakerfpfs000001CadvSpectrogramNs1Classification LossAdversarial LossSpeaker VerificationReconstruction LossReconstructedSpectrogramÅÅÅÅPurifying EncoderEliminating EncoderFeature FusionDecoderS'SACSAC 2020, December 7â€“11, 2020, Austin, USA
Jianwei and Xiaoqi, et al.
3.3 Eliminating Encoder
The adversarial classifier ğ¶ğ‘ğ‘‘ğ‘£ is to classify the speaker identity
based on the predicted distribution ğ‘¦ğ‘’ = ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ (ğ¶ğ‘ğ‘‘ğ‘£ (ğ¸ğ‘’ (ğ‘†))).
The eliminating encoder ğ¸ğ‘’ is trained to fool ğ¶ğ‘ğ‘‘ğ‘£ through an adver-
sarial supervision signal, so that ğ¶ğ‘ğ‘‘ğ‘£ outputs the same probability
over each prediction class. Hence, ğ¸ğ‘’ can successfully extract the