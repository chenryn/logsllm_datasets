1,000 
Figure 4.5.3 Size of the Priority Queue over Time, using Dic-0294 
as the Input Dictionary 
403
    All  tests  were  run  using  terminal  probability  order  and 
using  the  dictionary  Dic-0294.  Note  that  in  terminal 
probability  order  while  the  specific  L-production  is  not 
expanded in the priority queue, its probability is taken into 
account  when  pushing  and  popping 
the  pre-terminal 
structures.  The input dictionary thus can cause differences 
in how the priority queue grows. 
    We  finally  consider  the  maximum  number  of  pre-
terminals  and  password  guesses  that  could  possibly  be 
generated  by  our  grammar. Consider as an example a base 
structure  that  takes  the  form  S1L8D3.  A  pre-terminal  value 
might  take  the  form  $L8123,  and  a  final  guess,  (terminal 
value), might take the form $password123. To find the total 
number  of  possible  pre-terminal  values  for  this  base 
structure,  one  simply  needs  to  examine  the  total  possible 
replacements  for  each  string  variable  in  the  base  structure. 
Using this example, and assuming there are 10 S1-production 
rules  and  50  D2-production  rules,  then  the  total  number  of 
pre-terminals that may be generated by S1L8D3 is 500. 
    To find the total number of password guesses we simply 
expand  this  calculation  by  factoring  in  the  number  of 
dictionary  words  that  can  replace  the  alpha  string.  In  the 
above  example,  if  we  assume  there  are  1,000  dictionary 
words of length 8, then the total number of guesses would be 
500,000.    See  Table  4.5.4  for  the  total  search  space 
generated by each given training set and input dictionary. 
Table 4.5.4  
Total Search Space 
Input Dictionary 
Training 
Set 
Pre-
Terminals 
(millions) 
34,794,330 
34,794,330 
34,785,870 
578 
578 
506 
Password 
Guesses 
(trillions) 
>100,000,000 
>100,000,000 
36,000 
>100,000,000 
10,359,023 
6 
dic-0294 
English-Wiki 
dic-0294 
English-Wiki 
Common_Passwords 
MySpaceFull 
MySpaceFull 
MySpaceFull  Common_Passwords 
Finnish 
Finnish 
Finnish 
    To  explain  the  results  of  Table  4.5.4  further,  note  the 
number of pre-terminals generated can be dependent on the 
input  dictionary,  since  if  a  Li-production  exists  where  no 
dictionary word matches it, (for example the dicionary does 
not  contain any words of length 9), then the base structure 
containing the Li-production is discarded for that password 
cracking  run.  Also,  we  found  that  the  total  number  of pre-
terminals  were  mostly  driven  by  a  few  base  structures  that 
contained  a  large  number  of  Di  and  Si-productions,  for 
example  S1D3S2D3S1D4.  Likewise  the  number  of  terminals, 
(final  password  guesses),  was  dominated  by  a  few  base 
structures that contained many Li-productions such as:  
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:16:33 UTC from IEEE Xplore.  Restrictions apply. 
L1S1L1S1L1S1L1S1L1S1L1S1L1S1.  This  was  made  worse  by 
the  fact  that  in  our  code  we  did  not  remove  duplicate 
dictionary  words.  For  example  we  would  have  52  L1 
replacements  from  the  input  dictionary  “dic-0294”  even 
though  we  lowercased  all  input  words  before  using  them. 
This  is  because  by  not  removing  duplicates  we  had  two 
instances of every single letter of length 1. 
    That being said, the advantage of our method is that these 
highly complex base structures will generally not be utilized 
until  late  in  the  password  cracking  session  due  to  their 
corresponding  low  probabilities.  Therefore,  we  would  not 
expand  them  in  our  priority  queue  until  all  the  more 
probable guesses have been generated first. 
5.  FUTURE RESEARCH 
    There  are  several  areas  that  we  feel  are  open  for 
improvement  in  our  approach  with  using  probabilistic 
grammars for password cracking. As stated earlier in section 
3.2,  we  are  currently  looking  into  different  ways  to  do 
insertion  of  dictionary  words  into  the  final  guess  that  take 
into  account  the  size  of  the  input  dictionary.  As  can  been 
seen in Figures 4.4.1 – 4.4.5, there was a definite advantage 
to  using 
terminal  probability  order  vs.  pre-terminal 
probability  order  with  our  probabilistic  password  cracker. 
Currently we determine the probability of dictionary words 
of  length  n  by  assigning  a  probability  of  1/k  if  there  are  k 
words of length n in the input dictionary.  There are however 
many  other  approaches  we  could  take.  Currently  the  most 
promising  approach  seems  to  be  the  use  of  several  input 
dictionaries with different associated probabilities. This way 
one  might  have  a  small  highly  probable  dictionary,  (aka 
common passwords), and a much larger dictionary based on 
words that are less common. 
    Another  point  where  we  have  identified  room  for  future 
improvement  is  modifying  the  base  structures  to  more 
accurately  portray  how  people  actually  create  passwords. 
For  example,  we  could  add  another  category,  ‘U’,  to 
represent  uppercase  letters  as  currently  our  method  only 
deals  with  lowercase  letters.  Also  we  could  add  another 
transformation  to  the  base  structure  that  would  deal  with 
letter  replacement,  such  as  “replace  every  ‘a’  in  the 
dictionary word with an ‘@’.” Since we are using a context-
free  grammar,  this  would  be  fairly  straightforward.  All  we 
need  to  do  is  create  a  new  production  rule  that  deals  with 
letter  replacement.  The  harder  part  would  be  identifying 
those  transformations  during  the  training  phase.  We  are 
currently  looking  into  several  ways  to  efficiently  identify 
those  transformations  such  as  checking  the  edit  distance 
between known passwords and a dictionary file.  
    It  may  also  be  useful  to  add  probability  smoothing  or 
switch  to  a  Bayesian  method  in  the  training  stage.  This 
404
would allow our generator to create password guesses of a 
structure or containing a terminal value that was not present 
in the training set. For example, currently if the number ‘23’ 
does not appear in the training set, our method will never use 
it. Ideally we would like it to try this terminal value, but at a 
reduced probability compared to values found in the training 
set.  The  ultimate  goal  would  be  to  allow  our  method  to 
automatically  switch  between  dictionary  based  attacks  and 
targeted  brute-force  attacks  based  upon 
their  relative 
probability of cracking a password. For example, it might try 
some  word-mangling  rules,  then  brute-force  all  words  of 
length four, before returning back to trying additional word-
mangling rules.  
    There  also  exists  more  research  to  be  performed  on 
verifying the performance of this method if it is trained and 
tested against password lists from different sources. 
6.  CONCLUSION 
    Our  experiments  show  that  using  a  probabilistic  context 
free grammar to aid in the creation of word-mangling rules 
through  training  over  known  password  sets  is  a  promising 
approach.    It  also  allows  us  to  quickly  create  a  ruleset  to 
generate  password  guesses  for  use  in  cracking  unknown 
passwords. When compared against the default ruleset used 
in John the Ripper, our method managed to outperform it by 
cracking  28%  -  129%  more  passwords,  given  the  same 
number  of  guesses,  based  on  training  and  testing  on  the 
MySpace  password  set.    Our  method  also  did  very  well 
when  trained  on  the  Finnish  training  set  and  tested  on  the 
MySpace  test  set.  Our  approach  is  expected  to  be  most 
effective when tailoring one's attack against different sources 
by  training  it  on  passwords  of  a  relevant  structure.  For 
example,  if  it  is  known  that  the  target  password  was 
generated  to  satisfy  a  strong  password  policy  (such  as 
requiring it to be 8 characters long and containing numbers 
and  special characters) the algorithm could be trained only 
on  passwords  meeting  those  requirements.  We  have  also 
shown  that  we  can  quickly  and  manageably  generate 
password guesses in highest probability order which allows 
us to test a very high number of rulesets effectively.  
    We feel that our method might successfully help forensic 
investigators  by  doing  better  than  existing  techniques  in 
many practical situations.  Our work can also provide a more 
realistic  picture  of  the  real  security  (or  lack  of  the  same) 
provided by passwords.  We expect that our approach can be 
an invaluable addition to the existing techniques in password 
cracking. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:16:33 UTC from IEEE Xplore.  Restrictions apply. 
REFERENCES 
[1]  U. Manber. A simple scheme to make passwords based on 
one-way functions much harder to crack. Computers & Security 
Journal, Volume 15, Issue 2, 1996, Pages 171-176. Elsevier. 
[2]  J. Yan, A. Blackwell, R. Anderson, and A. Grant. Password 
Memorability and Security: Empirical Results. IEEE Security and 
Privacy Magazine, Volume 2, Number 5, pages 25-31, 2004. 
[3]  R. V. Yampolskiy. Analyzing User Password Selection 
Behavior for Reduction of Password Space. Proceedings of the 
IEEE International Carnahan Conferences on Security Technology, 
pp.109-115, 2006. 
[4]  M. Bishop and D. V. Klein. Improving system security via 
proactive password checking. Computers & Security Journal, 
Volume 14, Issue 3, 1995, Pages 233-249. Elsevier. 
[5]  G. Kedem and Y. Ishihara. Brute Force Attack on UNIX 
Passwords with SIMD Computer. Proceedings of the 3rd USENIX 
Windows NT Symposium, 1999. 
[6]  N. Mentens, L. Batina, B. Preneel, I. Verbauwhede. Time-
Memory Trade-Off Attack on FPGA Platforms: UNIX Password 
Cracking. Proceedings of the International Workshop on 
Reconfigurable Computing: Architectures and Applications. 
Lecture Notes in Computer Science, Volume 3985, pages 323-334, 
Springer, 2006. 
[7]  M. Hellman.  A cryptanalytic time-memory trade-off. IEEE 
Transactions on Information Theory, Volume 26, Issue 4, pages 
401-406, 1980. 
[8]  P. Oechslin.  Making a Faster Cryptanalytic Time-Memory 
Trade-Off. Proceedings of Advances in Cryptology (CRYPTO 
2003), Lecture Notes in Computer Science, Volume 2729, pages 
617-630, 2003. Springer.  
[9]  A list of popular password cracking wordlists, 2005, [Online 
Document] [cited 2008 Oct 07] Available HTTP 
http://www.outpost9.com/files/WordLists.html 
[10] A. Narayanan and V. Shmatikov, Fast Dictionary Attacks on 
Passwords Using Time-Space Tradeoff, CCS’05, November 7–11, 
2005, Alexandria, Virginia 
[11] John the Ripper password cracker, [Online Document] [cited 
2008 Oct 07] Available HTTP  http://www.openwall.com 
[12] J.E. Hopcroft and J.D. Ullman, Introduction to Automata 
Theory, Languages, and Computation, Addison Wesley, 1979. 
[13] L. R. Rabiner, A Tutorial on Hidden Markov Models and 
Selected Applications in Speech Recognition, Proceedings of the 
IEEE, Volume 77, No. 2, February 1989  
[14] N. Chomsky. Three models for the description of language. 
Information Theory, IEEE Transactions on, 2(3):113–124, Sep 
1956.  
[15] Robert McMillan, Phishing attack targets Myspace users, 
2006, [Online Document] [cited 2008 Oct 07] Available HTTP 
http://www.infoworld.com/infoworld/article/06/10/27/HNphishing
myspace 1.html. 
[16] Bulletin Board Announcement of the Finnish Password List, 
October 2007, [Online Document] [cited 2008 Oct 07] Available 
HTTP http://www.bat.org/news/view_post?postid=40546&page=1 
&group. 
APPENDIX 1  
PSEUDO CODE FOR THE NEXT FUNCTION 
working_value.structure = most probable pre-terminal value for the base structure 
working_value.pivot_value = 0 
working_value.num_strings = total number of L/S/D strings in the corresponding base structure 
working_value.probability = calculate_probability(working_value.structure) 
insert_into_priority_queue(priority_queue, working_value)   //higher probability == greater priority 
//The probability calculation depends on if pre-terminal or terminal probability is used 
//New nodes will be inserted into the queue with the probability of the pre-terminal structure acting as the priority value 
For (all base structures) { //first populate the priority queue with the most probable values for each base structure 
} 
working_value = Pop(priority_queue) //Now generate password guesses 
while (working_value!=NULL) { 
} 
Print out all guesses for the popped value by filling in all combinations of the appropriate alpha strings. 
For (i=working_value.pivot_value; i<working_value.num_strings;i++) {  
       insert_value.structure=decrement(working_value.structure,i);  //get next lower probability S or D structure at pivot value ‘i’ 
       if (insert_value.structure!=NULL) { 
       } 
} 
working_value = Pop(priority_queue) 
insert_value.probability = calculate_probability(insert_value.structure); 
insert_value.pivot_value = i 
insert_value.num_strings = working_value.num_strings 
insert_into_priority_queue(priority_queue,insert_value) 
405
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:16:33 UTC from IEEE Xplore.  Restrictions apply.