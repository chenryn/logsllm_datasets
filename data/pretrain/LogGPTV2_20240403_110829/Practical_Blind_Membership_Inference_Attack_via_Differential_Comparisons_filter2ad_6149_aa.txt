title:Practical Blind Membership Inference Attack via Differential Comparisons
author:Bo Hui and
Yuchen Yang and
Haolin Yuan and
Philippe Burlina and
Neil Zhenqiang Gong and
Yinzhi Cao
Practical Blind Membership Inference Attack via
Differential Comparisons
Bo Hui†∗, Yuchen Yang†∗, Haolin Yuan†∗, Philippe Burlina‡, Neil Zhenqiang Gong§ and Yinzhi Cao†
†The Johns Hopkins University ‡The Johns Hopkins University Applied Physics Laboratory §Duke University
Abstract—Membership inference (MI) attacks affect user
privacy by inferring whether given data samples have been used
to train a target learning model, e.g., a deep neural network.
There are two types of MI attacks in the literature, i.e., these
with and without shadow models. The success of the former
heavily depends on the quality of the shadow model, i.e., the
transferability between the shadow and the target; the latter,
given only blackbox probing access to the target model, cannot
make an effective inference of unknowns, compared with MI
attacks using shadow models, due to the insufﬁcient number
of qualiﬁed samples labeled with ground truth membership
information.
In this paper, we propose an MI attack, called BLINDMI,
which probes the target model and extracts membership seman-
tics via a novel approach, called differential comparison. The
high-level idea is that BLINDMI ﬁrst generates a dataset with
nonmembers via transforming existing samples into new samples,
and then differentially moves samples from a target dataset to
the generated, non-member set in an iterative manner. If the
differential move of a sample increases the set distance, BLINDMI
considers the sample as non-member and vice versa.
BLINDMI was evaluated by comparing it with state-of-the-
art MI attack algorithms. Our evaluation shows that BLINDMI
improves F1-score by nearly 20% when compared to state-of-
the-art on some datasets, such as Purchase-50 and Birds-200, in
the blind setting where the adversary does not know the target
model’s architecture and the target dataset’s ground truth labels.
We also show that BLINDMI can defeat state-of-the-art defenses.
I.
INTRODUCTION
Machine learning (ML), especially Deep Learning (DL),
has achieved, or even surpassed, human-level performance
on many critical areas, such as medical diagnosis [5], [6],
image and speech recognition [17], [20], [23], [51], self-
driving cars [3], and natural language translation [29]. Despite
this success, one major issue of DL models like deep neural
networks (DNNs) has been their vulnerability to a variety of
attacks [12], [38], [46], [47]. A type of privacy-related attack—
i.e.,
the focus of the paper—is the membership inference
(MI) attack [40], [41], [43], [49], whereby an adversary infers
whether a speciﬁc sample belongs to the training set of a given
learning model, deﬁned as a membership. For example, an
adversary can infer whether a speciﬁc disease image from a
given hospital was used to train an artiﬁcial intelligent (AI)
∗
The ﬁrst three authors have equal contributions to the paper.
Network and Distributed Systems Security (NDSS) Symposium 2021
21-25  February  2021, Virtual 
ISBN  1-891562-66-5
https://dx.doi.org/10.14722/ndss.2021.24293
www.ndss-symposium.org
diagnostic system, thus potentially violating patients’ protected
health information (PHI) and Health Insurance Portability and
Accountability Act (HIPAA) provisions. For another example,
the inference of location data used in an AI recommendation
system may leak users’ past physical location, violating their
privacy.
The high-level intuition behind membership inference at-
tacks is that the output probability distributions of a DNN
model from, say for example, a Softmax layer, may vary be-
tween members and a non-members. While intuitively simple,
one major challenge to this idea is that an adversary, when
only given blackbox access (i.e., only having access to the
output probability distribution), needs to collect enough sam-
ples with output probabilities and labeled as either members
or non-members to classify a new data sample with unknown
membership. On one hand, many existing MI attacks—e.g.,
the DNN-based from Shokri et al. [43], the loss function-based
from Yeom et al. [49], and another DNN-based with feature
selections from Salem et al. [41]—all adopt an ofﬂine shadow
model trained from a surrogate dataset, that provides ground
truth information on whether a given sample is a member.
However, such shadow models differ from the real target model
and thus the output probability distributions, though being
similar, as noted by prior studies on model transferability [10],
[21], [50], are still different. Therefore if the shadow model
is drastically different from the target, the attack performance
will degrade signiﬁcantly as shown by Salem et al. [41] and
as also conﬁrmed by our own experiments.
On the other hand, researchers have also proposed MI
attacks without shadow models. For example, the unsupervised
and adversary binary attacks of Salem et al. [41] consider
a sample as a member if the probability of the predicted
class is larger than a threshold learned from one thousand
randomly generated samples. Another approach, the label-only
attack of Yeom et al. [49], infers membership by comparing
the ground truth against the predicted label. However, both
existing shadow-model-free attacks rely binary comparisons,
e.g., comparing the predicted probability or label with a
pre-determined threshold or the ground-truth label. Such a
“one-size-ﬁt-all” inference cannot model the complex decision
boundary between members and nonmembers in the hyper-
dimensional space induced by an inference neural network
in shadow-model-dependent attacks. The root reason in lack-
ing such modeling ability goes back to the aforementioned
challenge: a powerful MI attack needs enough labeled output
probability distributions of members and non-members to learn
the decision boundary, but the ground truth information of
members and nonmembers for the target model is unavailable
given only blackbox access.
In this paper, we propose a novel MI attack, called
BLINDMI, which probes the target model and then infers
membership directly from the probing results instead of
shadow models. BLINDMI exploits two insights: The ﬁrst is
that although an adversary does not have both member and
nonmember labels of the target model, the adversary can easily
obtain one-class labels, i.e., nonmember labeled samples, by
producing newly-constructed samples that can be considered
as non-members with high probability given the very large
input space of possibilities. Such one-class semantics can be
learned by existing ML classiﬁers, like a one-class SVM, thus
leading to an MI attack deﬁned as BLINDMI-1CLASS. This
BLINDMI-1CLASS serves as a baseline approach if we only
exploits the ﬁrst insight of BLINDMI.
The second insight is that the removal of a non-member
from a dataset containing both members and non-members,
will move the entire set away from non-members in the
hyper-dimensional space, and conversely, the addition moves
it towards it. Therefore, assume that we have two datasets:
one is closer to nonmembers and the other to members.
If we move one sample from the latter to the former and
the distance between two sets decreases, the moved sample
can be considered a member; otherwise, if it increases, the
sample can be considered a non-member. This approach is
called differential comparison in this paper as it compares the
differential distance between two sets. One advantage of this
approach is that it only needs two small-size sets as opposed to
a considerable amount of data for a one-class classiﬁer, while
achieving a comparatively higher inference performance.
Speciﬁcally, we design an attack, called BLINDMI-DIFF,
which performs differential comparison to infer membership.
Following upon the ﬁrst insight, BLINDMI-DIFF obtains a
dataset with nonmembers. Then, BLINDMI-DIFF differentially
compares the dataset with a given set of data samples, called
a target dataset, following the second insight
to remove
all the nonmembers from the target. The entire differential
comparison procedure is iterated until convergence, i.e., the
move of any samples between two sets only decreases the
distance. Then, the remaining samples in the target dataset are
considered as members.
We implement a prototype of BLINDMI1
including
BLINDMI-DIFF and BLINDMI-1CLASS. Our evaluation shows
that BLINDMI outperforms state-of-the-art membership infer-
ence attacks in terms of F1-score in different settings, e.g., even
when the adversary knows the target model’s exact architecture
and hyper-parameters. Furthermore, we evaluate BLINDMI
and other attacks under realistic assumptions following Bargav
et al. [25] to adjust the nonmember-to-member ratio in the
target dataset and show that even if the ratio is as high as
39, BLINDMI still has an over 50% F1-score as opposed
to below 30% of the state-of-the-art MI attacks. We also
test BLINDMI against existing defenses, including Adversarial
Regularization [36], MemGuard [26], Mixup + MMD [30]
and differential privacy [1], and show that BLINDMI can break
these defenses by achieving reasonable F1-score with different
privacy-utility budgets.
1The default version of BLINDMI and BLINDMI-DIFF, without speciﬁca-
tion, is BLINDMI-DIFF with generated non-members, called BLINDMI-DIFF-
w/.
TABLE I.
DIFFERENT THREAT MODELS AND THEIR ASSUMPTIONS.
output distr. model arch.& targets’ true labels
hyper-parameter
Blind (default)
Blackbox
Graybox
Graybox-Blind












II. OVERVIEW
In this section, we ﬁrst present our threat model and
then describe overarching assumptions and principles used
throughout the paper.
A. Threat Model
Our threat model assumes an adversary trying to infer
whether each sample in a given input dataset, called the target
dataset, belongs to—i.e., is a member of—the training set of
a deep learning (DL) model, called the target model. The
adversary can probe the target DL model with samples to
obtain the probability distribution of output classes. There
are four different variations of the threat model based on the
adversary’s capability as described below and shown in Table I.
• Blackbox-Blind, or called Blind for short.
The blind
setting only grants an adversary blackbox access to the
target model without details of its architecture, network
weights, or hyper-parameters. Further, the adversary does
not have ground truth class labels of the target dataset,
which usually takes a considerable amount of manual effort
sometimes even from specialized experts, e.g., a highly
trained ophthalmologist and retinal specialist in labeling the
existence of certain diseases for the EyePACS dataset.
• Blackbox. The blackbox setting is similar to the blind, but
assumes that the adversary has the ground-truth information
of all the samples in the target dataset via, e.g., manual
labelling. Note that some existing attacks, e.g., Yeom et
al. [49], only work if such ground-truth information is
available.
• Graybox.
The graybox setting gives full knowledge to
the adversary in terms of the model details. Speciﬁcally,
except for the training data, the adversary knows almost
everything about the model, such as the architecture (e.g.,
VGG, ResNet, and DenseNet) and the hyper-parameters
used for training (e.g., learning rate and maximum number
of epochs). Note that the adversary cannot know the training
data (called a whitebox), because MI attacks are unnecessary
in such a setting.
• Graybox-Blind.
The graybox-blind setting is similar to
the graybox one, but also assumes that the adversary does
not have ground-truth information of the target dataset.
Note that our default threat model setting is blind unless
otherwise noted, because the blind setting is the most strict and
practical for membership inference attack. We also adopted
other settings in comparison with prior works, e.g., blackbox
with Yeom et al. [49] and graybox(-blind) with Shokri et
al. [43] and Salem et al [41].
B. Problem Formulation and Notations
The attack problem considered in this paper is as follows:
given a target model, i.e., Fm (see recapitulation of notations
2
TABLE II.
NOTATIONS OF SYMBOLS IN THE PAPER
TABLE III.
DIFFERENT VARIATIONS OF BLINDMI
Notation
Description
Starget
Fm
Sprob
target
Gprojection,k
Sprob,k
target
Snonmem
Sprob
Sprob,k
nonmem
nonmem
The target dataset of membership inference attack
The target DL model with m prediction classes
target = {y = Fm(x)|x ∈ Starget}
Sprob
Gprojection,k : Rm → Rk
target = {y(cid:48) = Gprojection,k(y)|y ∈ Sprob
Sprob,k
A generated dataset with non-members of Fm
nonmem = {y = Fm(x)|x ∈ Snonmem}
Sprob
nonmem = {y(cid:48) = Gprojection,k(y)|y ∈ Sprob
Sprob,k
target}
nonmem}
Fig. 1. A High-level Idea of the Key, Atomic Step in Differential Compar-
isons. BLINDMI measures the distance d between Sprob,k
nonmem and
moves a sample from Sprob,k
nonmem. Then, BLINDMI recalculates the
distance d(cid:48) and compares d(cid:48) with d. If d(cid:48) is larger than d, BLINDMI considers
the moved sample as a nonmember; otherwise, BLINDMI considers it as a
member. This is an iterative process until convergence.
target and Sprob,k
target to Sprob,k
in Table II), with m classes and a target dataset Starget,
BLINDMI is tasked with inferring whether each sample in
Starget belongs to the training set of Fm. The adversary
will feed the set of samples Starget
into Fm, obtain the
set of output probability distributions, i.e., Sprob
target, and then
applies a projection function Gprojection,k that converts all data
samples from m dimensions to k dimensions for inference.
The converted samples form into a new set called Sprob,k
target. One
example Gprojection,k is the selection of top three probabilities
in y ∈ Sprob
target plus the one corresponding to the ground truth
class.
Another important dataset is a generated dataset, Snonmem,
which is a reference dataset
to determine whether sam-
ples in Starget are members. Elements in Snonmem are
all—or mostly—non-members of the target model training
dataset. Similarly, the adversary will also obtain Sprob
nonmem and
nonmem for the comparison with Starget.
Sprob,k
C. Differential Comparison Intuition
idea,
We now introduce the high-level
i.e., differential
comparison, in Figure 1. We depict two datasets, Sprob,k
nonmem
and Sprob,k
target, in the output probability distribution sub-space.
The curve dividing the space is the boundary between member
(right) and non-member (left). Sprob,k
nonmem is located more to-
wards the left because it consists exclusively of samples with
high probability of being non-members, while Sprob,k
target more or
less in the middle between members and non-members.
Intuitively, the idea of differential comparison is to move
nonmem. If the moved sample
nonmem moves
one sample from Sprob,k
is a non-member like Case (1) in Figure 1, Sprob,k
target to Sprob,k
Variations
BLINDMI-DIFF
Description
differential comparison version
BLINDMI-DIFF-w/ (default) BLINDMI-DIFF with generated non-member set
BLINDMI-DIFF-w/o
BLINDMI-DIFF without generated non-member set
BLINDMI-1CLASS
one-class SVM version with generated non-
members as training set
target and Sprob,k
further towards the left and Sprob,k
target to the right. Therefore,
the distance between Sprob,k
nonmem increases from the
original d to d(cid:48). If the moved sample is a member like Case
(2), the distance will decrease from d to d(cid:48)(cid:48)since both sets are
now comprised of a mixture of samples. Such a change in
d can then be used to infer whether the moved sample is a
member.
While intuitively simple, the distance between Sprob,k
nonmem
and Sprob,k
target changes over time after each move. That is, what
we described in the previous paragraph is a key, atomic step of