r
G
h W/O TR TN: No penalty.
t
u
r
T
With TR
FN: penalty for fail-
ure to discover trig-
gering relations.
FP: penalty for ﬁnding
triggering relations in
non-related pairs.
TP: No penalty.
Table 3: Semantics of values in a cost matrix. TR
stands for triggering relation.
3.5 TRG Construction and Root-Trigger Se-
curity
A list of pairwise triggering relations in network events
can be used to construct the complete triggering relation
graph (TRG). The resulting TRG then serves as a source
for locating anomalous network activities. The security
model, which deﬁnes legitimate and abnormal events, comes
in many forms when used for analyzing TRGs.
Under the root-trigger security model, one determines the
legitimacy of a network event e based on the legitimacy of
e’s root trigger, i.e., whether or not e has a legitimate root
trigger. According to this deﬁnition, anomalous events are
the events that do not have a valid root trigger. These events
may be due to malware activities or host/server misconﬁg-
uration.
43A speciﬁc root-trigger security deﬁnition is based on user
intention [44], where a valid root trigger should be related
to a user activity (e.g., a function call to retrieve user in-
puts, mouse clicks, or keyboard inputs). Other deﬁnitions
for valid root triggers may be made according to the speciﬁc
applications. We refer to the events that do not have any
valid root triggers as the vagabond events.
In order to enforce the root-trigger security, the TRG
construction operation is used to calculate the discovered
root triggers of all the events. To ﬁnd the root of each event
by traversal in TRG is equivalent to the transitive reduction
of a directed graph. We design the root ﬁnding procedure
(Algorithm 2) to return the root of an event, given all the
pairwise triggering relations.
event n ← dequeue Q
set T ← ﬁnd n’s parent(s) based on P∗
for each event e ∈ T do
Algorithm 2 Root Finding Algorithm (RFA)
Input: an event ek and P∗ = {(ei → ej)}.
Output: a set R, where each in R is a root of ek.
1: deﬁne a set R to store the results
2: deﬁne a queue Q and enqueue ek onto Q
3: while Q (cid:54)= ∅ do
4:
5:
6:
7:
8:
9:
10:
end if
11:
12:
end for
13: end while
14: return R
R ← R ∪ {e}
if e is of type root then
else if e /∈ Q then
enqueue e onto Q
The inputs of Algorithm 2 are an event ek and a set P∗
containing all the pairwise triggering relations {(ei → ej)}.
The output is a set containing all the roots of ek. In order to
compute the transitive reduction of a directed graph, we use
a queue Q to perform breadth-ﬁrst traversal of TRG. In each
iteration, we obtain the parent(s) T of a dequeued event n.
For each event e in the set T, the algorithm checks if it is a
root-type event. If yes, then e is added to the return set R.
Otherwise (i.e., e is an intermediate node on the path from
ek’s root to ek), the algorithm enqueues e onto Q for further
iteration. This analysis returns root triggers for the network
requests. Network requests without valid root triggers are
labeled as vagabond events. They are ﬂagged and alerted to
the administrator for further inspection.
types of common malware in Section 4, including
We demonstrate the use of our method for detecting three
• spyware as a browser extension,
• data-exﬁltrating malware as a stand-alone process,
• a DNS-based botnet command and control channel.
4. EVALUATION AND RESULTS
Our prototype implements all parts of the TRG discov-
ery system. The questions we seek to answer through our
experiments are: i) How accurate is the prediction for pair-
wise triggering relations? ii) How accurate is the prediction
for root triggers of events? iii) Can the method detect out-
bound network activities caused by stealthy malware? iv)
Can the method detect network connections to suspicious
servers? v) Can the methods analyze diﬀerent traﬃc types?
4.1 Experimental Overview
We have conducted extensive tests on our proposed traﬃc-
causality-analysis solution and obtained positive results. In
this section, we describe the setup of our experimental eval-
uation. Then, our evaluation results are presented in the
next few sections.
4.1.1 Accuracy and Security Metrics
• The pairwise accuracy rate of classiﬁcation is the per-
centage of pairwise triggering relations that are pre-
dicted correctly. The pairwise accuracy is with re-
spect to the ground truth obtained through rule-based
analysis and manual classiﬁcation. An rule example is
shown in Example 3.1.
• The conventional precision and recall measures [6]
evaluate the classiﬁcation accuracy of the positives
(i.e., the existence of triggering relations). In the equa-
tions below, T P , F P , and F N stand for true positives,
false positives, and false negatives, respectively.
T P
T P + F P
, Recall =
P recision =
. (1)
• The root-trigger correctness rate is computed based on
the root of a node. It is the percentage of events whose
roots in the constructed the triggering relation graph
are correct with respect to the ground truth.
T P
T P + F N
4.1.2 Datasets
Our evaluation is mainly focused on HTTP and DNS traf-
ﬁc, because they are very commonly used communication
protocol both by legitimate users and attackers. Many bot-
nets use HTTP or DNS as their communication protocol,
because most ﬁrewalls allow them [40]. We collect and ana-
lyze outbound HTTP and DNS requests from hosts, aiming
to detect suspicious activities by stealthy malware installed
on the hosts. In addition, we also evaluate our algorithms
with a much larger TCP dataset collected from a sever.
A summary of the experimental data is shown in Table 4.
We deﬁne η ∈ [0, 1] as the reduction percentage in Equa-
tion 2), where EP A(n) is the number of event pairs after
using the eﬃcient pairing algorithm (in Section 3.3), and n
is the total number of events.
η = 1 − EP A(n)
n × (n − 1)/2
(2)
• Dataset I, HTTP. We collected the user events and
outbound HTTP traﬃc in a user study with 20 par-
ticipants. Each participant was asked actively surf the
web for 30 minutes on a computer equipped with our
data collection program.
• Dataset II, DNS and HTTP. We used tcpdump to con-
tinuously collect the outbound DNS queries and HTTP
requests on an active user’s workstation for 19 days.
We collected types A/AAAA DNS queries and the out-
bound HTTP requests that contain GET, HEAD, or POST
information in their headers.
• Dataset III, server TCP traﬃc. We collected TCP
packets on an active Linux server in a research lab.
The inbound and outbound TCP packet headers were
collected for 42 days using tcpdump.
4.1.3 Data Labels
Training data is labeled manually, with the use of sim-
ple rules such as in Example 3.1. The labeling process is
44Data
I
II
III
Type
# of Events
η
# of Pairs # of Feature
Size (MB)
Host-based HTTP
Host-based DNS and HTTP DNS: 35,882; HTTP: 85,223
TCP Traﬃc of a Server
HTTP: 45,988; User: 899
TCP: 3,010,821
94.7%
98.8%
99.6% 119,372,631
3,436,635
953,916
10
9
9
229.5
55.1
6697.1
Table 4: An overview of datasets in the experiments. Number of events is the number of raw requests that
have been collected. η is the reduction percentage after using our Eﬃcient Pairing Algorithm. For Dataset
I, the number of user events are also given in column 3.
Data
I
II
III
3,318,328
693,903
1,191,926,877
Mean
–
# of Pairs
in Test Sets Matrix
Cost
(cid:105)
(cid:104) 0, 1
(cid:105)
(cid:104)0, 1
(cid:105)
(cid:104)0, 1
10, 0
1, 0
3, 0
–
Naive Bayes
Bayesian Network
SVM
Pairwise A.
Prec. Recall
Pairwise A.
Prec. Recall
Pairwise A.
Prec. Recall
99.75%
99.82%
98.92%
99.50%
0.954
0.959
0.995
0.969
0.996
0.998
0.986
0.993
99.75%
100.00%
99.72%
99.82%
0.956
1.000
0.997
0.984
0.996
1.000
0.998
0.998
99.70%
100.00%
99.82%
99.84%
0.958
1.000
0.998
0.985
0.997
1.000
0.999
0.999
Table 5: Pairwise classiﬁcation results of train-n-test for three datasets. The numbers are rounded before
reporting. Pairwise A. and Prec. stands for pairwise classiﬁcation accuracy and precision, respectively.
time consuming, and requires nontrivial human eﬀorts. The
labeling of DNS traﬃc requires the integral analysis of user-
HTTP dependency and DNS-HTTP dependency, details of
which are omitted. User events are labeled as root triggers,
which are generated by leveraging Tlogger [2]. As a browser
extension, Tlogger captures user inputs and tab events dur-
ing the web browsing. By combining the data recorded on
the kernel level, we generate the root-triggers used in TRG
Construction for Dataset I and II.
4.1.4 Classiﬁcation Setup
Three common classiﬁcation techniques are compared:
naive Bayes classiﬁer, a Bayesian network, and a support
vector machine (SVM).2 Due to the sparsity of triggering
relations in network traﬃc, we deﬁne a cost matrix that pe-
nalizes classifying false negatives more than classifying the
false positives. Classification and TRG construction
operations are implemented in Java using the Weka library.
We perform both 10-fold cross validation and train-n-test
types of evaluation. The two evaluation methodologies yield
similar classiﬁcation results. We report the train-n-test re-
sults, unless otherwise speciﬁed.
4.2 Causality Analysis of Dataset I
Based on our two feature selection algorithms, 10 features
out of a total of 12 are chosen for HTTP data. Selected
features include three similarity indexes (RequestSim, Refer-
rerSim, HostSim), two nominal values to identify the ﬁle type
(RequestType, ReferrerType), the nominal values to com-
pare between particular attributes (PIDDiﬀ, AddrDiﬀ, Type-
Match, IndexOfSameRequest), and time diﬀerence (TimeD-
iﬀ).
4.2.1 Accuracy of Pairwise Triggering Relations
The results in Table 5 show very good prediction accuracy
for pairwise triggering relations. All classiﬁers give high pre-
cision and recall values, as well as the pairwise classiﬁcation
accuracy. These results indicate the eﬀectiveness of our bi-
nary classiﬁcation approach.
We vary the cost matrix used during the training and com-
pute the pairwise accuracy results of the three classiﬁers for
Dataset I. The results are shown in Figure 3 (a). The pair-
2SVM has a polynomial kernel function with a degree of 2.
(cid:105)
10, 0
(cid:104) 0, 1
wise accuracy is consistently high for naive Bayes classiﬁer
with all cost matrices. Bayesian Network and SVM respond
diﬀerently to the changes of penalty values in cost matrices.
In Table 5, we report the accuracy results under the cost
matrix of
. This matrix gives 10 units of penalty to a
false negative and 1 unit of penalty to a false positive for
the pairwise classiﬁcation.
4.2.2 Correctness of Root Triggers
The purpose of this analysis is to identify reasons for
wrong predictions of triggering relations. Running the root
ﬁnding algorithm (in Section 3.5) on the pairwise trigger-
ing relations, we identify the root triggers of all events and
compare them to the ground truth values.
Figure 3 (b) shows the relationship between the cost ma-
trix values and the accuracy of root-trigger analysis. The
naive Bayes and Bayesian network yield nearly 100% accu-
racy of ﬁnding the root-triggers, both of which are not very
sensitive to the cost matrices.
In contrast, the accuracy
of SVM increases signiﬁcantly with increased false negative
penalty in the cost matrix. In Table 6, we summarize the
results of root trigger correctness for Dataset I. Our pre-
diction of events’ root triggers is accurate.
It has a very
small error rate, as low as 0.06%. These errors in ﬁnding
root triggers generate false alerts. Wrong root triggers are
mostly because of missing attributes in the original data or
late-arriving requests. We further analyze false alerts later.
Cost Matrix