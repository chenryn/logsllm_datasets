reducing online IO may mark requests as satisﬁed before
enough blocks are read from the server, leaking informa-
tion about the requested block’s identity.
The second challenge is ensuring that we maximally
utilize client storage and available bandwidth while
avoiding deadlock. An excessively aggressive strategy
that delays too much IO may use so much client space
that we run out of room to shufﬂe. It may also under-
utilize available bandwidth, increasing response times.
On the other hand, an overly conservative strategy may
under-utilize client space or perform shufﬂing too early,
delaying online IO and increasing response times.
Techniques and Outline. In Burst ORAM, we address
these challenges by combining several novel techniques.
In Section 4 we introduce our XOR technique for reduc-
ing online bandwidth cost to nearly 1X. We also describe
our techniques for prioritizing online IO and delaying of-
ﬂine/shufﬂe IO until client memory is nearly full. In Sec-
tion 5 we show how Burst ORAM prioritizes efﬁcient
shufﬂe jobs in order to delay the bulk of the shufﬂe IO
even further, ensuring that we minimize effective IO dur-
ing long bursts. We then introduce a technique for using
available client space to cache small levels locally to re-
duce shufﬂe IO in both Burst ORAM and ObliviStore.
In Section 6 we discuss the system-level techniques
used in Burst ORAM, and present its design in detail.
In Section 7, we evaluate Burst ORAM’s performance
through micro-benchmarks and extensive simulations.
4 Prioritizing and Reducing Online IO
Existing ORAMs require high online and ofﬂine band-
width costs to obscure access patterns. ObliviStore must
fetch one block from every level in a partition (see
Section 2.3), requiring O(logN) online IO per request.
Figure 3 (left) illustrates this behavior. After each re-
quest, ObliviStore also requires O(logN) ofﬂine/shufﬂe
IO. Since ObliviStore issues online and ofﬂine IO before
satisfying the next request, its effective IO is high, lead-
ing to large response times during bursts. Other ORAMs
work differently, such as Path ORAM [25] which orga-
nizes data as a tree, but still have high effective costs.
We now show how Burst ORAM achieves lower effective
bandwidth costs and response times than ObliviStore.
4.1 Prioritizing Online IO
One way we achieve low response times in Burst ORAM
is by prioritizing online IO over shufﬂe IO. That is, we
suppress shufﬂe IO during bursts, delaying it until idle
periods. Requests are satisﬁed once online IO ﬁnishes,1
so prioritizing online IO allows us to satisfy all requests
before any shufﬂe IO starts, keeping response times low
even for later requests. Figure 2 illustrates this behavior.
During the burst, we continue processing requests by
fetching blocks from the server, but since shufﬂing is
suppressed, no blocks are uploaded. Thus, we must re-
sume shufﬂing once client storage ﬁlls. Section 5.2 dis-
1Each client write also incurs a read, so writes still incur online IO.
752  23rd USENIX Security Symposium 
USENIX Association
4
ObliviStore 
Legend 
Online IO 
Offline IO 
Less online IO 
More offline IO 
Burst ORAM 
Concurrent 
IO 
Offline IO (shuffling) is delayed 
until idle time between bursts 
Time 
Time 
Long response 
times 
Burst 1 
Start 
Online IO 
Complete 
Burst 2 
Start 
Long response 
times 
Online IO 
Complete 
Burst 1 
Start 
Online IO 
Complete 
Short response times 
Short response times 
Online IO 
Complete 
Burst 2 
Start 
Figure 2: Reducing response time. Because Burst ORAM (right) does much less online IO than ObliviStore (left)
and delays ofﬂine IO, it is able to respond to ORAM requests much faster. In this (overﬂy simpliﬁed) illustration, the
bandwidth capacity is enough to transfer 4 blocks concurrently. Both ORAM systems do the same amount of IO.
Server 
ObliviStore 
Server 
Burst ORAM 
O(log N)  
blocks 
Client 
XOR 
1 block 
Client 
Figure 3: Reducing online cost.
In ObliviStore (left)
the online bandwidth cost is O(logN) blocks of IO on
average. In Burst ORAM (right), we reduce online IO to
only one block, improving handling of bursty trafﬁc.
cusses how to delay shufﬂe IO even further. Section 6
details changes from the ObliviStore design required to
avoid deadlock and fully utilize client space.
When available bandwidths are large and bursts are
short, the response time saved by prioritizing online IO
is limited, as most IO needed for the burst can be issued
in parallel. However, when bandwidth is limited or bursts
are long, the savings can be substantial. With shufﬂe IO
delayed until idle times, online IO dominates the effec-
tive IO, becoming the bottleneck during bursts. Thus we
can further reduce response times by reducing online IO.
4.2 XOR Technique: Reducing Online IO
We introduce a new mechanism called the XOR tech-
nique that allows the Burst ORAM server to combine the
O(logN) blocks fetched during a request into a single
block that is returned to the client (Figure 3 right), re-
ducing the online bandwidth cost to O(1).
If we fetched only the desired block, we would reveal
its identity to the server. Instead, we XOR all the blocks
together and return the result. Since there is at most one
real block among the O(logN) returned, the client can
locally reconstruct the dummy block values and XOR
them with the returned block to recover the encrypted
real block. XOR technique steps are shown in Figure 4.
4.2.1 XOR Technique Details
In Burst ORAM, as in ObliviStore, each request needs to
retrieve a block from a single partition, which is a sim-
pliﬁed hierarchical ORAM resembling those in [9]. The
hierarchy contains L ≈ 1
2 log2 N levels with real-block ca-
pacities 1,2,4, . . . ,2 L−1 respectively.
To retrieve a requested block, the client must fetch ex-
actly one block from each of the L levels. The XOR
technique requires that the client be able to reconstruct
dummy blocks, and that dummies remain indistinguish-
able from real blocks. We achieve this property by en-
crypting a real block b residing in partition p, level (cid:29),
and offset off as AESskp,(cid:29) (off||B). We encrypt a dummy
block residing in partition p, level (cid:29), and offset off as
AESskp,(cid:29)(off). The key skp,(cid:29) is speciﬁc to partition p and
level (cid:29), and is randomized every time (cid:29) is rebuilt.
For simplicity, we start by considering the case with-
out early shufﬂe reads. In this case, exactly one of the L
blocks requested is the encryption of a real block, and the
rest are encryptions of dummy blocks. The server XORs
all L encrypted blocks together into a single block XQ that
it returns to the client. The client knows which blocks are
dummies, and knows p, (cid:29),off for each block, so it recon-
structs all the encrypted dummy blocks and XORs them
with XQ to obtain the encrypted requested/real block.
4.2.2 Handling early shufﬂe reads
An early shufﬂe read occurs when we need to read from
a level with no more than half its original blocks remain-
ing. Since such early shufﬂe reads may be real blocks,
they cannot be included in the XOR. Fortunately, the
number of blocks in a level is public, so the server al-
ready knows which levels will cause early shufﬂe reads.
Thus, the server simply returns early shufﬂe reads indi-
vidually, then XORs the remaining blocks, leaking no
information about the access sequence.
USENIX Association  
23rd USENIX Security Symposium  753
5
1. Client issues block requests to server, one per level
2. Server, to satisfy request
(a) Retrieves and returns early shufﬂe reads
(b) XORs remaining blocks together into single
combined block and returns it
3. Client, while waiting for response
(a) Regenerates encrypted dummy block for each
non-early-shufﬂe-read
(b) XORs all dummies to get subtraction block
4. Client receives combined block from server and
XORs with subtraction block to get requested block
5. Client decrypts requested block
Figure 4: XOR Technique Steps
Since each early shufﬂe read block must be transferred
individually, early shufﬂe reads increase online IO. For-
tunately, early shufﬂe reads are rare, even while shufﬂing
is suppressed during bursts, so the online bandwidth cost
stays under 2X and near 1X in practice (see Figure 7).
4.2.3 Comparison with ObliviStore
ObliviStore uses level compression to reduce shufﬂe IO.
When the client uploads a level to the server, it ﬁrst com-
presses the level down to the combined size of the level’s
real blocks. Since half the blocks are dummies, half the
upload shufﬂe IO is eliminated. For details on level com-
pression and its security, see [24].
Unfortunately, Burst ORAM’s XOR technique is in-
compatible with level compression due to discrepan-
cies in the ways dummy blocks must be formed. The
XOR technique requires that the client be able to recon-
struct dummy blocks locally, so in Burst ORAM, each
dummy’s position determines its contents. In level com-
pression, each level’s dummy block contents are a func-
tion of the level’s real block contents. Since the client
cannot know the contents of all real blocks in the level, it
cannot reconstruct the dummies locally.
Level compression and the XOR technique yield com-
parable overall IO reductions, though level compression
performs slightly better. For example, the experiment
in Figure 8 incurs roughly 23X and 26X overall band-
width cost using level compression and XOR respec-
tively. However, the XOR technique reduces online IO,
while level compression reduces ofﬂine IO, so the XOR
technique is more effective at reducing response times.
5 Scheduling and Reducing Shufﬂe IO
In Burst ORAM, once client space ﬁlls, we must start
shufﬂing in order to return blocks to the server and
continue the burst.
If we are not careful about shuf-
ﬂe IO scheduling, we may immediately start doing large
amounts of IO, dramatically increasing response times.
In this section, we show how Burst ORAM schedules
shufﬂe IO so that jobs that free the most client space us-
ing the least shufﬂe IO are prioritized. Thus, at all times,
Burst ORAM issues only the minimum amount of effec-
tive IO needed to continue the burst, keeping response
times lower for longer. We also show how to reduce over-
all IO by locally caching the smallest levels from each
partition. We start by deﬁning shufﬂe jobs.
5.1 Shufﬂe Jobs
In Burst ORAM, as in ObliviStore, shufﬂe IO is divided
into per-partition shufﬂe jobs. Each job represents the
work needed to shufﬂe a partition p and upload blocks
evicted to p. A shufﬂe job is deﬁned by ﬁve entities:
• A partition p to which the job belongs
• Blocks evicted to but not yet returned to p
• Levels to read blocks from
• Levels to write blocks to
• Blocks already read from p (early shufﬂe reads)
Each shufﬂe job moves through three phases:
Creation Phase. We create a shufﬂe job for p when a
block is evicted to p following a request. Every job starts
out inactive, meaning we have not started work on it.
If another block is evicted to p, we update the sets of
eviction blocks and read/write levels in p’s inactive job.
When Burst ORAM activates a job, it moves the job
to the Read Phase, freezing the eviction blocks and
read/write levels. Subsequent evictions to p will create
a new inactive shufﬂe job. At any time, there is at most
one active and one inactive shufﬂe job for each partition.
Read Phase. Once a shufﬂe job is activated, we begin
fetching all blocks still on the server that need to be shuf-
ﬂed. That is, all previously unread blocks from all the
job’s read levels. Once all such blocks are fetched, they
are shufﬂed with all blocks evicted to p and any early
shufﬂe reads from the read levels. Shufﬂing consists
of adding/removing dummies, pseudo-randomly permut-
ing the blocks, and then re-encrypting each block. Once
shufﬂing completes, we move the job to the Write Phase.
Write Phase. Once a job is shufﬂed we begin storing
all shufﬂed blocks to the job’s write levels on the server.
Once all writes ﬁnish, the job is marked complete, and
Burst ORAM is free to activate p’s inactive job, if any.
5.2 Prioritizing Efﬁcient Jobs
Since executing shufﬂe IO delays the online IO needed to
satisfy requests, we can reduce response times by doing
as little shufﬂing as is needed to free up space. The hope
is that we can delay the bulk of the shufﬂing until an idle
period, so that it does not interfere with pending requests.
By the time client space ﬁlls, there will be many par-
titions with inactive shufﬂe jobs. Since we can choose
jobs in any order, we can minimize the up-front shufﬂing
work by prioritizing the most efﬁcient shufﬂe jobs: those
754  23rd USENIX Security Symposium 
USENIX Association
6
that free up the most client space per unit of shufﬂe IO.
The space freed by completing a job for partition p is the
number of blocks evicted to p plus the number of early
shufﬂe reads from the job’s read levels. Thus, we can
deﬁne shufﬂe job efﬁciency as follows:
Job Efﬁciency =
# Evictions + # Early Shufﬂe Reads
# Blocks to Read + # Blocks to Write
Job efﬁciencies vary substantially. Most jobs start with
1 eviction and 0 early shufﬂe reads, so their relative efﬁ-
ciencies are determined strictly by the sizes of the job’s
read and write levels. If the partition’s bottom level is
empty, no levels need be read, and only the bottom must
be written, for an overall IO of 2 an an efﬁciency of 0.5.
If instead the bottom 4 levels are occupied, all 4 levels
must be read, and the 5th level written, for an total of
roughly 15 reads and 32 writes, yielding a much lower
efﬁciency of just over 0.02. Both jobs free equal amounts
of space, but the higher-efﬁciency job uses less IO.
Since small levels are written more often than large
ones, efﬁcient jobs are common. Further, by delaying an
unusually inefﬁcient job, we give it time to accumulate
more evictions. While such a job will also accumulate
more IO, the added write levels are generally small, so
the job’s efﬁciency tends to improve with time. Thus,
prioritizing efﬁcient jobs reduces shufﬂe IO during the
burst, thereby reducing response times.
Unlike Burst ORAM, ObliviStore does not use client
space to delay shufﬂing, so there are fewer shufﬂe jobs to
choose from at any one time. Thus, job scheduling is less
important and jobs are chosen in creation order. Since
ObliviStore is concerned with throughput, not response
times, it has no incentive to prioritize efﬁcient jobs.
5.3 Reducing Shufﬂe IO via Level Caching
Since small, efﬁcient shufﬂe jobs are common, Burst
ORAM spends a lot of time accessing small levels. If
we use client space to locally cache the smallest levels of
each partition, we can eliminate the shufﬂe IO associated
with those levels entirely. Since levels are shufﬂed with
a frequency inversely proportional to their size, each is
responsible for roughly the same fraction of shufﬂe IO.
Thus, we can greatly reduce shufﬂe IO by caching even
a few levels from each partition. Further, since caching
a level eliminates its early shufﬂe reads, which are com-
mon for small levels, caching can also reduce online IO.
We are therefore faced with a tradeoff between using
client space to store requested blocks, which reduces re-
sponse times for short bursts, and using it for local level
caching, which reduces overall bandwidth cost.
5.3.1 Level Caching in Burst ORAM
In Burst ORAM, we take a conservative approach, and
cache only as many levels as are guaranteed to ﬁt in the
Server Storage 
Online IO Prioritization and IO Rate Limiting 
Using Concurrent IO and Local Space Semaphores 
Online IO Reduction 
Via XOR Technique 
Online IO 
Requester 
Requests & 
Responses 
ORAM Main 
Efficient Job First 
Shuffle Prioritization 
Deadlock Avoidance Via 
Shuffle Buffer Semaphore 
Local Level Caching 
Shuffle IO 
Shuffler 
Figure 5: Burst ORAM Architecture. Solid boxes rep-
resent key system components, while dashed boxes rep-
resent functionality and the effects of the system on IO.
Since each level
worst case. More precisely, we identify the maximum
number λ such that the client could store all real blocks
from the smallest λ levels of every partition even if all
were full simultaneously. We cache levels by only up-
dating an inactive job when the number of evictions is
such that all the job’s write levels have index at least λ .
is only occupied half the time,
caching λ levels consumes at most half of the client’s
space on average, leaving the rest for requested blocks.
As we show experimentally in Section 7, level caching
greatly reduces overall bandwidth cost, and can even re-