1
Sensitivity(v)
2
+
1
Speciﬁcity(v)
(4)
(5)
(6)
In summary, the higher the Importance value, the more
correlated with the bug is the access vector.
G. Similarity-Guided Merge
To merge the partial logs and generate a complete log
having high probability to replay the faulty execution, we
developed a heuristic denoted Similarity-Guided Merge. This
heuristic operates in the following ﬁve steps:
1. Calculate the degree of similarity between the partial
logs: the ﬁrst step consists of calculating the similarity
between each partial log and all the others from the universe
of partial logs received. To calculate the similarity, CoopREP
applies the Plain Similarity metric or the Dispersion-based
Similarity metric, to every possible pair of partial logs.
2. Identify the list of base partial logs: the next step consists
of identifying the list of the partial logs that can be a poten-
tial good basis to start reconstructing the faulty execution.
To build this list, CoopREP ﬁrst calculates the relevance
of each partial log and picks the n most relevant ones (we
found n = 10 to be a suitable value for our experiments) in
a descending order according to their relevance value.
3. Complete the base partial log with information from the
group of similars: having already chosen the base partial
log, CoopREP identiﬁes the unrecorded SPEs in the base
partial log and completes them with the respective access
vectors traced by the logs in the group of similars. If all SPEs
have been associated with an access vector, the obtained
complete log is sent to the replayer, along with the thread
ID map and the generated replay driver. On the other hand,
if there are still empty SPEs, the heuristic proceeds with the
next step.
4. Complete the base partial log with information from
partial
logs “similar by transitivity”: when the access
vectors from the group of similars are not sufﬁcient to create
a complete replay log, CoopREP tries to ﬁll the missing
SPEs with access vectors from the partial logs “similar by
transitivity”. These partial logs, although not belonging to
the group of similars referred in the previous step, are part
of the group of similars of those partial logs which are
themselves similar to the base partial log. In other words,
if l1 ∈ Siml0 ∧ l2 ∈ Siml1 ⇒ l2 ∈ Sim2
, where Simn
l0
contains the partial logs which are nth-degree similar to l0
(in this example, l2 would be second-degree similar to l0).
5. Complete the base partial log with statistical indicators:
if it is still not possible to complete the log for replay (the
union of the different groups of similars may not cover all
the SPEs of the program), CoopREP applies the metrics
described in Section IV-F to the universe of access vectors
collected, and picks the ones with greater Importance (see
Equation 6) to ﬁll the missing SPEs.
l0
At the end of this process, CoopREP replays the merged
log and veriﬁes if the bug is reproduced. If it is, the goal has
been achieved and the process ends. If it is not, CoopREP
chooses the next partial log in the list of the most relevant to
be the new base partial log and re-executes the Similarity-
Guided Merge from the step 3. It should be referred that,
in the worst case scenario, where all the most important
indicators failed to replay the bug, the heuristic switches to
a brute force mode. Here, all the possible access vectors are
tested for each missing SPE.
V. EVALUATION
A. Experimental Setting
All the experiments were conducted with machines Intel
Core 2 Duo at 2.26 Ghz, with 4 GB of RAM and running
Mac OS X. CoopREP prototype was implemented over a
LEAP public version.In order to get comparative ﬁgures, this
standard version of LEAP was also used in the experiments.
Regarding partial logging, we vary the percentage of the
total SPEs logged in each run from 10% to 75%. For
each conﬁguration, 500 partial logs from different failed
executions were used, plus more 50 of successful runs. To
get a fairer comparison of the three recording schemes, the
partial logs were generated from 500 complete logs, picking
randomly the SPEs to be stored according to the scheme’s
percentage.
For the Plain Similarity we used a threshold of 0.3 and
for the Dispersion-based Similarity we used a threshold of
0.01 (given that the weights of some SPEs may be very low).
Regarding the maximum number of attempts of the heuristic
to reproduce the bug, it was set to 500.
B. Evaluation Criteria
Three main criteria were used to evaluate CoopREP,
namely: i) the bug replay capacity (consists of the number
of attempts of the heuristic to replay the bug, therefore,
the less number of tries, the better); ii) the performance
overhead; and iii) the size of the partial logs produced. It
should be noted that the two latter criteria were applied to
both CoopREP and LEAP, in order to evaluate the beneﬁts
and limitations of our solution.
To assess CoopREP’s bug replay capacity, we used some
bugs from the IBM ConTest benchmark suite [24], and a real
Program
SPEs
BoundedBuffer
BubbleSort
ProducerConsumer
Piper
Manager
TwoStage
BufferWriter
12
10
8
6
4
4
3
Total
Accesses
1376
49964
997
347
30240
27103
50417
Bug
Description
Notify instead of NotifyAll
Not-atomic
Orphaned thread
Missing condition for Wait
Not-atomic
Two-stage
Wrong or No-Lock
DESCRIPTION OF THE CONTEST BENCHMARK BUGS USED IN THE
Table II
EXPERIMENTS.
bug from the widely-used Java Application Server Tomcat.
In order to measure the overheads imposed, we compared
CoopREP against LEAP on the previous two applications
and on the Java Grande workload benchmark.
C. Bug Replay Capacity
1) ConTest Benchmark: The IBM ConTest benchmark
suite [24] contains heterogeneous programs affected by sev-
eral
types of concurrency bugs. In order to be able to
evaluate the effectiveness of CoopREP when distributing
the logging burden across at least 4 clients, we restrict our
analysis to the ConTest programs that have at least 4 SPEs.
These are described in Table II, in terms of its number of
SPEs, the total number of SPE accesses, and the bug-pattern
according to [24].
Table III shows the number of attempts of the heuristic
(using both Plain Similarity and Dispersion-based Similar-
ity) to replay the ConTest benchmark bugs, when varying the
percentage of SPEs recorded at each instance of the program
in the range from 10% to 75%.
Analyzing the results, one can verify that the Similarity-
Guided Merge heuristic only failed to replay the bug in
programs BoundedBuffer (with percentage of SPEs recorded
smaller than 50%), Manager, and TwoStage (only in the
particular case of Plain-Similarity when logging 75% of the
total SPEs). These results highlight that executions involving
a high number of total SPE accesses are not necessarily
harder to reproduce using partial logging strategies. The
concurrency bug affecting the BubbleSort program, for
instance, was always successfully reproduced at the ﬁrst
recombination of partial log, even though it is the second
to entail the higher number of SPE accesses (around 50K).
The actual effectiveness of the heuristic depends rather on
how the accesses are distributed among the SPEs and, in
particular, on how that distribution inﬂuences the SPEs’
dispersion ratio. Here, the dispersion ratio indicates how
disperse is the SPE,
i.e. whether many different access
vectors were recorded for it or not. More precisely, the
dispersion ratio for an SPE is computed by dividing the
number of different access vectors recorded for the SPE by
the total number of access vectors recorded for that SPE
(notice that this metric differs from the overall-dispersion,
described in Equation 2). Figure 3 reports the SPE dispersion
ratios for the ConTest benchmark programs (for the sake
of readability and to ease the comparison, Figure 3 only
presents the values for the full logging conﬁguration).
We note that the BubbleSort program has only one SPE
with a very high dispersion ratio (this SPE also accounts
for about 99% of the total accesses), while the remaining
SPEs always present
the same access vector across all
the executions. This allowed the Similarity-Guided Merge
heuristic to quickly identify a set of partial logs whose
combination yielded to a successful bug reproduction.
Program
BoundedBuffer
BubbleSort
ProducerConsumer
Piper
Manager
TwoStage
BufferWriter
10% 12.5% 16.7% 25% 50% 75% 10% 12.5% 16.7% 25% 50% 75%
Plain Similarity
Dispersion-based Similarity
X
1
-
-
-
-
-
X
1
1
-
-
-
-
X
1
5
1
-
-
-
X
1
5
2
X
34
11
X
1
2
1
X
13
3
1
1
1
1
X
X
1
X
1
-
-
-
-
-
X
1
1
-
-
-
-
X
1
1
1
-
-
-
X
1
1
1
X
7
11
3
1
1
1
X
1
3
1
1
1
1
X
1
1
NUMBER OF THE ATTEMPTS REQUIRED BY THE HEURISTIC TO REPLAY BUG IN THE CONTEST BENCHMARK. THE X INDICATES THAT THE HEURISTIC
FAILED TO REPLAY THE BUG IN THE MAXIMUM NUMBER OF ATTEMPTS STIPULATED. “-” DENOTES THAT IT WAS NOT POSSIBLE TO ACHIEVE THE
SPECIFIED PERCENTAGE OF PARTIAL LOGGING, GIVEN THE TOTAL NUMBER OF SPES PRESENT IN THE APPLICATION.
Table III
of similars contained only other partial logs matching in
the common SPEs. As a consequence, the access vectors
combined for ﬁlling either SPE 2 or 3 were incompatible.
50% of SPEs: with this conﬁguration, each partial log was
composed by two SPEs. Here, the list of base partial logs
was ﬁlled with the partial logs that have other ones matching
in the SPE with very few identical access vectors (SPE
2). This because all the partial logs containing only SPEs
0 and 1, albeit having many other similar partial
logs,
could not generate a complete replay log just by combining
information from their group of similars. Therefore, their
relevance was lower (see Equation 3). The same did not
happen for the partial logs containing SPE 2, which ended
up composing the list of base partial logs. The bug was then
successfully replayed by trying different access vectors for
ﬁlling SPE 3.
25% of SPEs: with this conﬁguration, each partial log was
composed by a single SPE. Since there were no intersection
points between the partial logs, the Similarity-guided Merge
heuristic picked random partial
logs to act as base to
generate the replay log. Then, it tried to replay the error by
successively ﬁlling the missing SPEs with the access vectors
indicated by the statistical indicators. Nevertheless, the bug
was successful replayed at the 34th attempt.
In fact, the TwoStage program is a good example to un-
derstand the differences between the two Similarity metrics.
When using Dispersion-based Similarity, the heuristic could
easily reproduce the bug, because the SPEs had different
importance. Thus, the partial logs with the same access
vector for SPE 2 were identiﬁed as the best base partial
logs and used to generate a complete replay log.
As ﬁnal remark, it should be noted that the addition of
successful logs did not impact the results. The reason is due
to the fact that when it was necessary to ﬁll missing SPEs,
there were always many different access vectors with the
same degree of correlation to the bug.
2) Tomcat: Tomcat is a widely-used complex server ap-