During speculative execution, the processor makes guesses
as to the likely outcome of branch instructions. Better pre-
dictions improve performance by increasing the number of
speculatively executed operations that can be successfully
committed.
The branch predictors of modern Intel processors, e.g.,
Haswell Xeon processors, have multiple prediction mecha-
nisms for direct and indirect branches. Indirect branch in-
structions can jump to arbitrary target addresses computed at
runtime. For example, x86 instructions can jump to an address
in a register, memory location, or on the stack e.g., “jmp
(cid:20)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:46:03 UTC from IEEE Xplore.  Restrictions apply. 
eax”, “jmp [eax]”, and “ret”. Indirect branches are also
supported on ARM (e.g., “MOV pc, r14”), MIPS (e.g., “jr
$ra”), RISC-V (e.g., “jalr x0,x1,0”), and other proces-
sors. To compensate for the additional ﬂexibility as compared
to direct branches, indirect jumps and calls are optimized using
at least two different prediction mechanisms [35].
Intel [35] describes that the processor predicts
• “Direct Calls and Jumps” in a static or monotonic manner,
• “Indirect Calls and Jumps” either in a monotonic manner,
or in a varying manner, which depends on recent program
behavior, and for
• “Conditional Branches” the branch target and whether the
branch will be taken.
Consequently, several processor components are used for
predicting the outcome of branches. The Branch Target Buffer
(BTB) keeps a mapping from addresses of recently executed
branch instructions to destination addresses [44]. Processors
can use the BTB to predict future code addresses even before
decoding the branch instructions. Evtyushkin et al. [14] ana-
lyzed the BTB of an Intel Haswell processor and concluded
that only the 31 least signiﬁcant bits of the branch address are
used to index the BTB.
For conditional branches, recording the target address is not
necessary for predicting the outcome of the branch since the
destination is typically encoded in the instruction while the
condition is determined at runtime. To improve predictions,
the processor maintains a record of branch outcomes, both
for recent direct and indirect branches. Bhattacharya et al. [9]
analyzed the structure of branch history prediction in recent
Intel processors.
Although return instructions are a type of indirect branch,
a separate mechanism for predicting the destination address is
often used in modern CPUs. The Return Stack Buffer (RSB)
maintains a copy of the most recently used portion of the
call stack [15]. If no data is available in the RSB, different
processors will either stall the execution or use the BTB as a
fallback [15].
Branch-prediction logic, e.g., BTB and RSB, is typically not
shared across physical cores [19]. Hence, the processor learns
only from previous branches executed on the same core.
D. The Memory Hierarchy
To bridge the speed gap between the faster processor and
the slower memory, processors use a hierarchy of successively
smaller but faster caches. The caches divide the memory into
ﬁxed-size chunks called lines, with typical line sizes being 64
or 128 bytes. When the processor needs data from memory,
it ﬁrst checks if the L1 cache, at the top of the hierarchy,
contains a copy. In the case of a cache hit, i.e., the data is
found in the cache, the data is retrieved from the L1 cache and
used. Otherwise, in the case of a cache miss, the procedure is
repeated to attempt to retrieve the data from the next cache
levels, and ﬁnally external memory. Once a read is completed,
the data is typically stored in the cache (and a previously
cached value is evicted to make room) in case it is needed
again in the near future. Modern Intel processors typically
(cid:21)
have three cache levels, with each core having dedicated L1
and L2 caches and all cores sharing a common L3 cache, also
known as the Last-Level Cache (LLC).
A processor must ensure that the per-core L1 and L2 caches
are coherent using a cache coherence protocol, often based
on the MESI protocol [35]. In particular, the use of the MESI
protocol or some of its variants implies that a memory write
operation on one core will cause copies of the same data
in the L1 and L2 caches of other cores to be marked as
invalid, meaning that future accesses to this data on other
cores will not be able to quickly load the data from the L1
or L2 cache [53, 68]. When this happens repeatedly to a
speciﬁc memory location, this is informally called cache-line
bouncing. Because memory is cached with a line granularity,
this can happen even if two cores access different nearby
memory locations that map to the same cache line. This
behavior is called false sharing and is well-known as a source
of performance issues [33]. These properties of the cache
coherency protocol can sometimes be abused as a replacement
for cache eviction using the clflush instruction or eviction
patterns [27]. This behavior was previously explored as a
potential mechanism to facilitate Rowhammer attacks [16].
E. Microarchitectural Side-Channel Attacks
All of the microarchitectural components we discussed
above improve the processor performance by predicting fu-
ture program behavior. To that aim, they maintain state that
depends on past program behavior and assume that future
behavior is similar to or related to past behavior.
When multiple programs execute on the same hardware,
either concurrently or via time sharing, changes in the microar-
chitectural state caused by the behavior of one program may
affect other programs. This, in turn, may result in unintended
information leaks from one program to another [19].
lower level caches [30, 38, 48, 74],
Initial microarchitectural side channel attacks exploited tim-
ing variability [43] and leakage through the L1 data cache
to extract keys from cryptographic primitives [52, 55, 69].
Over the years, channels have been demonstrated over mul-
including the instruc-
tiple microarchitectural components,
tion cache [3],
the
BTB [14, 44], and branch history [1, 2]. The targets of at-
tacks have broadened to encompass co-location detection [59],
breaking ASLR [14, 26, 72], keystroke monitoring [25], web-
site ﬁngerprinting [51], and genome processing [10]. Recent
results include cross-core and cross-CPU attacks [37, 75],
cloud-based attacks [32, 76], attacks on and from trusted
execution environments [10, 44, 61], attacks from mobile
code [23, 46, 51], and new attack techniques [11, 28, 44].
In this work, we use the Flush+Reload technique [30, 74],
and its variant Evict+Reload [25], for leaking sensitive infor-
mation. Using these techniques, the attacker begins by evicting
a cache line from the cache that is shared with the victim. After
the victim executes for a while, the attacker measures the time
it takes to perform a memory read at the address corresponding
to the evicted cache line. If the victim accessed the monitored
cache line, the data will be in the cache, and the access will
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:46:03 UTC from IEEE Xplore.  Restrictions apply. 
be fast. Otherwise, if the victim has not accessed the line,
the read will be slow. Hence, by measuring the access time,
the attacker learns whether the victim accessed the monitored
cache line between the eviction and probing steps.
The main difference between the two techniques is the
mechanism used for evicting the monitored cache line from
the cache. In the Flush+Reload technique, the attacker uses
a dedicated machine instruction, e.g., x86’s clflush,
to
evict the line. Using Evict+Reload, eviction is achieved by
forcing contention on the cache set that stores the line, e.g.,
by accessing other memory locations which are loaded into
the cache and (due to the limited size of the cache) cause
the processor to discard (evict) the line that is subsequently
probed.
F. Return-Oriented Programming
Return-Oriented Programming (ROP) [63] is a technique
that allows an attacker who hijacks control ﬂow to make
a victim perform complex operations by chaining together
machine code snippets, called gadgets, found in the code of
the vulnerable victim. More speciﬁcally, the attacker ﬁrst ﬁnds
usable gadgets in the victim binary. Each gadget performs
some computation before executing a return instruction. An
attacker who can modify the stack pointer, e.g., to point to
return addresses written into an externally-writable buffer, or
overwrite the stack contents, e.g., using a buffer overﬂow, can
make the stack pointer point to the beginning of a series of
maliciously-chosen gadget addresses. When executed, each
return instruction jumps to a destination address from the
stack. Because the attacker controls this series of addresses,
each return effectively jumps into the next gadget in the chain.
III. ATTACK OVERVIEW
Spectre attacks induce a victim to speculatively perform
operations that would not occur during strictly serialized in-
order processing of the program’s instructions, and which leak
victim’s conﬁdential information via a covert channel to the
adversary. We ﬁrst describe variants that leverage conditional
branch mispredictions (Section IV), then variants that leverage
misprediction of the targets of indirect branches (Section V).
In most cases, the attack begins with a setup phase, where
the adversary performs operations that mistrain the processor
so that it will later make an exploitably erroneous speculative
prediction. In addition, the setup phase usually includes steps
that help induce speculative execution, such as manipulating
the cache state to remove data that the processor will need to
determine the actual control ﬂow. During the setup phase, the
adversary can also prepare the covert channel that will be used
for extracting the victim’s information, e.g., by performing the
ﬂush or evict part of a Flush+Reload or Evict+Reload attack.
During the second phase, the processor speculatively exe-
cutes instruction(s) that transfer conﬁdential information from
the victim context into a microarchitectural covert channel.
This may be triggered by having the attacker request that the
victim perform an action, e.g., via a system call, a socket, or a
ﬁle. In other cases, the attacker may leverage the speculative
(mis-)execution of its own code to obtain sensitive information
from the same process. For example, attack code which is
sandboxed by an interpreter, just-in-time compiler, or ‘safe’
language may wish to read memory it is not supposed to
access. While speculative execution can potentially expose
sensitive data via a broad range of covert channels,
the
examples given cause speculative execution to ﬁrst read a
memory value at an attacker-chosen address then perform a
memory operation that modiﬁes the cache state in a way that
exposes the value.
For the ﬁnal phase, the sensitive data is recovered. For Spec-
tre attacks using Flush+Reload or Evict+Reload, the recovery
process consists of timing the access to memory addresses in
the cache lines being monitored.
Spectre attacks only assume that speculatively executed
instructions can read from memory that the victim process
could access normally, e.g., without triggering a page fault
or exception. Hence, Spectre is orthogonal to Meltdown [47]
which exploits scenarios where some CPUs allow out-of-order
execution of user instructions to read kernel memory. Conse-
quently, even if a processor prevents speculative execution of
instructions in user processes from accessing kernel memory,
Spectre attacks still work [17].
IV. VARIANT 1: EXPLOITING CONDITIONAL BRANCH
MISPREDICTION
In this section, we demonstrate how conditional branch
misprediction can be exploited by an attacker to read arbitrary
memory from another context, e.g., another process.
Consider the case where the code in Listing 1 is part of a
function (e.g., a system call or a library) receiving an unsigned
integer x from an untrusted source. The process running the
code has access to an array of unsigned bytes array1 of
size array1_size, and a second byte array array2 of
size 1 MB.
if (x 
true
false
true
true false
false
d
e
t
c
i
d
e
r
p
Fig. 1: Before the correct outcome of the bounds check is
known, the branch predictor continues with the most likely
branch target,
leading to an overall execution speed-up if
the outcome was correctly predicted. However, if the bounds
check is incorrectly predicted as true, an attacker can leak
secret information in certain scenarios.
execution. However, as illustrated, a correct prediction of the
condition in these cases leads to faster overall execution.
Unfortunately, during speculative execution, the conditional
branch for the bounds check can follow the incorrect path.
In this example, suppose an adversary causes the code to run
such that:
• the value of x is maliciously chosen (out-of-bounds), such
that array1[x] resolves to a secret byte k somewhere
in the victim’s memory;
• array1_size and array2 are uncached, but k is
cached; and
• previous operations received values of x that were valid,
leading the branch predictor to assume the if will likely
be true.
This cache conﬁguration can occur naturally or can be created
by an adversary, e.g., by causing eviction of array1_size
and array2 then having the kernel use the secret key in a
legitimate operation.
When the compiled code above runs,
the processor
begins by comparing the malicious value of x against
array1_size. Reading array1_size results in a cache
miss, and the processor faces a substantial delay until its value
is available from DRAM. Especially if the branch condition, or
an instruction somewhere before the branch, waits for an argu-
ment that is uncached, it may take some time until the branch
result is determined. In the meantime, the branch predictor
assumes the if will be true. Consequently, the speculative
execution logic adds x to the base address of array1 and
requests the data at the resulting address from the memory
subsystem. This read is a cache hit, and quickly returns the
value of the secret byte k. The speculative execution logic then
uses k to compute the address of array2[k * 4096]. It
then sends a request to read this address from memory (result-
ing in a cache miss). While the read from array2 is already
in ﬂight, the branch result may ﬁnally be determined. The
processor realizes that its speculative execution was erroneous
and rewinds its register state. However, the speculative read
from array2 affects the cache state in an address-speciﬁc
manner, where the address depends on k.
since the victim’s
To complete the attack,
the adversary measures which
location in array2 was brought
into the cache, e.g.,
via Flush+Reload or Prime+Probe. This reveals the value
of k,
speculative execution cached
array2[k*4096]. Alternatively, the adversary can also use
Evict+Time, i.e., immediately call the target function again
with an in-bounds value x’ and measure how long this
second call takes. If array1[x’] equals k, then the location
accessed in array2 is in the cache, and the operation tends
to be faster.
Many different scenarios can lead to exploitable leaks using
this variant. For example, instead of performing a bounds
check, the mispredicted conditional branch(es) could be check-
ing a previously-computed safety result or an object type.
Similarly, the code that is speculatively executed can take
other forms, such as leaking a comparison result into a ﬁxed
memory location or may be spread over a much larger number
of instructions. The cache status described above is also
more restrictive than may be required. For example, in some
scenarios, the attack works even if array1_size is cached,
e.g., if branch prediction results are applied during speculative
execution even if the values involved in the comparison are
known. Depending on the processor, speculative execution
may also be initiated in a variety of situations. Further variants
are discussed in Section VI.