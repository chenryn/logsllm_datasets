24,29] or parallel DFAs [23]. While NFAs are more com-
pact than DFAs, they require more memory bandwidth
3
to process each transition as an NFA may be in multiple
states whereas a DFA is always only in one state. Thus,
each character that is processed might be processed in
up to |Q| transition tables. Prior work has looked at
ways for ﬁnding good NFA representations of the REs
that limit the number of states that need to be processed
simultaneously. However, FPGA’s cannot be quickly re-
conﬁgured, and they have clock speeds that are slower
than ASIC chips.
There has been work [7, 12] on creating multi-stride
DFAs and NFAs. This work primarily applies to FPGA
NFA implementations since multiple character SRAM
based DFAs have only been evaluated for a small number
of REs. The ability to increase stride has been limited
by the constraint that all transitions must be increased
in stride; this leads to excessive memory explosion for
strides larger than 2. With variable striding, we increase
stride selectively on a state by state basis. Alicherry et al.
have explored variable striding for TCAM-based string
matching solutions [3] but not for DFAs that apply to ar-
bitrary RE sets.
3 Transition Sharing
The basic idea of transition sharing is to combine mul-
tiple transitions into a single TCAM entry. We pro-
pose two transition sharing ideas: character bundling and
shadow encoding. Character bundling exploits intra-state
optimization opportunities and minimizes TCAM tables
along the input character dimension. Shadow encoding
exploits inter-state optimization opportunities and mini-
mizes TCAM tables along the source state dimension.
3.1 Character Bundling
Character bundling exploits character redundancy by
combining multiple transitions from the same source
state to the same destination into one TCAM entry. Char-
acter bundling consists of four steps.
(1) Assign each
state a unique ID of ⌈log |Q|⌉ bits. (2) For each state,
enumerate all 256 transition rules where for each rule,
the predicate is a transition’s label and the decision is the
destination state ID. (3) For each state, treating the 256
rules as a 1-dimensional packet classiﬁer and leveraging
the ternary nature and ﬁrst-match semantics of TCAMs,
we minimize the number of transitions using the op-
timal 1-dimensional TCAM minimization algorithm in
[20, 31]. (4) Concatenate the |Q| 1-dimensional minimal
preﬁx classiﬁers together by prepending each rule with
its source state ID. The resulting list can be viewed as a
2-dimensional classiﬁer where the two ﬁelds are source
state ID and transition label and the decision is the des-
tination state ID. Fig. 1 shows an example DFA and its
TCAM lookup table built using character bundling. The
three chunks of TCAM entries encode the 256 transi-
tions for s0, s1, and s2, respectively. Without character
bundling, we would need 256 × 3 entries.
3.2 Shadow Encoding
Whereas character bundling uses ternary codes in the in-
put character ﬁeld to encode multiple input characters,
shadow encoding uses ternary codes in the source state
ID ﬁeld to encode multiple source states.
3.2.1 Observations
We use our running example in Fig. 1 to illustrate shadow
encoding. We observe that all transitions with source
states s1 and s2 have the same destination state except
for the transitions on character c. Likewise, source state
s0 differs from source states s1 and s2 only in the char-
acter range [a, o]. This implies there is a lot of state re-
dundancy. The table in Fig. 2 shows how we can ex-
ploit state redundancy to further reduce required TCAM
space. First, since states s1 and s2 are more similar, we
give them the state IDs 00 and 01, respectively. State
s2 uses the ternary code of 0* in the state ID ﬁeld of its
TCAM entries to share transitions with state s1. We give
state s0 the state ID of 10, and it uses the ternary code of
∗∗ in the state ID ﬁeld of its TCAM entries to share tran-
sitions with both states s1 and s2. Second, we order the
state tables in the TCAM so that state s1 is ﬁrst, state s2
is second, and state s0 is last. This facilitates the sharing
of transitions among different states where earlier states
have incomplete tables deferring some transitions to later
tables.
TCAM
Src State ID Input
SRAM
Dest State ID
s1 00
0*
s2 0*
0*
**
s0 **
**
0110 0011
0110 001*
0110 0000
0110 ****
0110 0000
0110 ****
**** ****
01 : s2
00 : s1
10 : s0
01 : s2
10 : s0
00 : s1
10 : s0
Figure 2: TCAM table with shadow encoding
We must solve three problems to implement shadow
encoding: (1) Find the best order of the state tables in
the TCAM given that any order is allowed. (2) Identify
entries to remove from each state table given this order.
(3) Choose binary IDs and ternary codes for each state
that support the given order and removed entries. We
solve these problems in the rest of this section.
Our shadow encoding technique builds upon prior
work with default transitions [4, 6, 18, 19] by exploiting
the same state redundancy observation and using their
4
concepts of default transitions and Delayed input DFAs
(D2FA). However, our ﬁnal technical solutions are dif-
ferent because we work with TCAM whereas prior tech-
niques work with RAM. For example, the concept of a
ternary state code has no meaning when working with
RAM. The key advantage of shadow encoding in TCAM
over prior default transition techniques is speed. Specif-
ically, shadow encoding incurs no delay while prior de-
fault transition techniques incur signiﬁcant delay because
a DFA may have to traverse multiple default transitions
before consuming an input character.
3.2.2 Determining Table Order
We ﬁrst describe how we compute the order of tables
within the TCAM. We use some concepts such as default
transitions and D2FA that were originally deﬁned by Ku-
mar et al. [18] and subsequently reﬁned [4, 6, 19].
else
s0
s1
defer
defer
c
[b,c]
s2
a,[d,o]
[a,o]
s0
s0
s2
s1
242
243
s1
255
s2
(a)
(c)
Figure 3: D2FA, SRG, and deferment tree
(b)
A D2FA is a DFA with default transitions where each
state p can have at most one default transition to one
other state q in the D2FA.
In a legal D2FA, the di-
rected graph consisting of only default transitions must
be acyclic; we call this graph a deferment forest. It is a
forest rather than a tree since more than one node may
not have a default transition. We call a tree in a defer-
ment forest a deferment tree.
We determine the order of state tables in TCAM by
constructing a deferment forest and then using the par-
tial order deﬁned by the deferment forest. Speciﬁcally, if
there is a directed path from state p to state q in the defer-
ment forest, we say that state p defers to state q, denoted
p ≻ q. If p ≻ q, we say that state p is in state q’s shadow.
We use the partial order of a deferment forest to deter-
mine the order of state transition tables in the TCAM.
Speciﬁcally, state q’s transition table must be placed af-
ter the transition tables of all states in state q’s shadow.
We compute a deferment forest that minimizes the
TCAM representation of the resulting D2FA as follows.
Our algorithm builds upon algorithms from prior work
[4, 6, 18, 19], but there are several key differences. First,
unlike prior work, we do not pay a speed penalty for long
default transition paths. Thus, we achieve better transi-
5
tion sharing than prior work. Second, to maximize the
potential gains from our variable striding technique de-
scribed in Section 5 and table consolidation, we choose
states that have lots of self-loops to be the roots of our
deferment trees. Prior work has typically chosen roots
in order to minimize the distance from a leaf node to a
root, though Becchi and Crowley do consider related cri-
teria when constructing their D2FA [6]. Third, we ex-
plicitly ignore transition sharing between states that have
few transitions in common. This has been done implic-
itly in the past, but we show how doing so leads to better
results when we use table consolidation.
The algorithm for constructing deferment forests con-
sists of four steps. First, we construct a Space Reduction
Graph (SRG), which was proposed in [18], from a given
DFA. Given a DFA with |Q| states, an SRG is a clique
with |Q| vertices each representing a distinct state. The
weight of each edge is the number of common (outgoing)
transitions between the two connected states. Second,
we trim away edges with small weight from the SRG. In
our experiments, we use a cutoff of 10. We justify this
step based on the following observations. A key property
of SRGs that we observed in our experiments is that the
weight distribution is bimodal: an edge weight is typ-
ically either very small ( 180).
If we use these low weight edges for default transitions,
the resulting TCAM often has more entries. Plus, we
get fewer deferment trees which hinders our table con-
solidation technique (Section 4). Third, we compute a
deferment forest by running Kruskal’s algorithm to ﬁnd
a maximum weight spanning forest. Fourth, for each de-
ferment tree, we pick the state that has largest number of
transitions going back to itself as the root. Fig. 3(b) and
(c) show the SRG and the deferment tree, respectively,
for the DFA in Fig. 1.
We make the following key observation about the root
states in our deferment trees. In most deferment trees,
more than 128 (i.e., half) of the root state’s outgoing tran-
sitions lead back to the root state; we call such a state a
self-looping state. Based on the pigeonhole principle and
the observed bimodal distribution, each deferment tree
can have at most one self-looping state, and it is clearly
the root state. We choose self-looping states as roots to
improve the effectiveness of variable striding which we
describe in Section 5. Intuitively, we have a very space-
efﬁcient method, self-loop unrolling, for increasing the
stride of self-looping root states. The resulting increase
in stride applies to all states that defer transitions to this
self-looping root state.
When we apply Kruskal’s algorithm, we use a tie
breaking strategy because many edges have the same
weight. To have most deferment trees centered around
a self-looping state, we give priority to edges that have
the self-looping state as one endpoint. If we still have a
tie, we favor edges by the total number of edges in the
current spanning tree that both endpoints are connected
to prioritize nodes that are already well connected.
3.2.3 Choosing Transitions
For a given DFA and a corresponding deferment forest,
we construct a D2FA as follows. If state p has a default
transition to state q, we remove any transitions that are
common to both p’s transition table and q’s transition ta-
ble from p’s transition table. We denote the default tran-
sition in the D2FA with a dashed arrow labeled with de-
fer. Fig. 3(a) shows the D2FA for the DFA in Fig. 1 given
the corresponding deferment forest (a deferment tree in
this case) in Figure 3(c). We now compute the TCAM
entries for each transition table.
(1) For each state, enumerate all individual transition
rules except the deferred transitions. For each transition
rule, the predicate is the label of the transition and the
decision is the state ID of the destination state. For now,
we just ensure each state has a unique state ID. Thus, we
get an incomplete 1-dimensional classiﬁer for each state.
(2) For each state, we minimize its transition table using
the 1-dimensional incomplete classiﬁer minimization al-
gorithm in [21]. This algorithm works by ﬁrst adding a
default rule with a unique decision that has weight larger
than the size of the domain, then applying the weighted
one-dimensional TCAM minimization algorithm in [20]
to the resulting complete classiﬁer, and ﬁnally remove
the default rule, which is guaranteed to remain the default
rule in the minimal complete classiﬁer due to its huge
weight. In our solution, the character bundling technique
is used in this step. We also consider some optimizations
where we specify some deferred transitions to reduce the
total number of TCAM entries. For example, the second
entry in s2’s table in Fig. 2 is actually a deferred transi-
tion to state s0’s table, but not using it would result in 4
TCAM entries to specify the transitions that s2 does not
share with s0.
3.2.4 Shadow Encoding Algorithm
To ensure that proper sharing of transitions occurs, we
need to encode the source state IDs of the TCAM entries
according to the following shadow encoding scheme.
Each state is assigned a binary state ID and a ternary
shadow code. State IDs are used in the decisions of tran-
sition rules. Shadow codes are used in the source state
ID ﬁeld of transition rules. In a valid assignment, every
state ID and shadow code must have the same number of
bits, which we call the shadow length of the assignment.
For each state p, we use ID(p) and SC(p) to denote the
state ID and shadow code of p. A valid assignment of
state IDs and shadow codes for a deferment forest must
satisfy the following four shadow encoding properties:
1. Uniqueness Property: For any two distinct states p
and q, ID(p) 6= ID(q) and SC(p) 6= SC(q).
2. Self-Matching Property: For any state p, ID(p) ∈
SC(p) (i.e., ID(p) matches SC(p)).
3. Deferment Property: For any two states p and q, p ≻
q (i.e., q is an ancestor of p in the given deferment
tree) if and only if SC(p) ⊂ SC(q).
4. Non-interception Property: For any two distinct
states p and q, p ≻ q if and only if ID(p) ∈ SC(q).
Intuitively, q’s shadow code must include the state ID of
all states in q’s shadow and cannot include the state ID
of any states not in q’s shadow.
We give an algorithm for computing a valid assign-
ment of state IDs and shadow codes for each state given
a single deferment tree DT . We handle deferment forests
by simply creating a virtual root node whose children are
the roots of the deferment trees in the forest and then run-
ning the algorithm on this tree. In the following, we refer
to states as nodes.
Our algorithm uses the following internal variables for
each node v: a local binary ID denoted L(v), a global
binary ID denoted G(v), and an integer weight denoted
W (v) that is the shadow length we would use for the
subtree of DT rooted at v. Intuitively, the state ID of
v will be G(v)|L(v) where | denotes concatenation, and
the shadow code of v will be the preﬁx string G(v) fol-
lowed by the required number of *’s; some extra padding
characters may be needed. We use #L(v) and #G(v)to
denote the number of bits in L(v) and G(v), respectively.
Our algorithm processes nodes in a bottom-up fashion.
For each node v, we initially set L(v) = G(v) = ∅ and
W (v) = 0. Each leaf node of DT is now processed,
which we denote by marking them red. We process an
internal node v when all its children v1, · · · , vn are red.
Once a node v is processed, its weight W (v) and its local
ID L(v) are ﬁxed, but we will prepend additional bits to
its global ID G(v) when we process its ancestors in DT .
We assign v and each of its children a variable-length
binary code, which we call HCode. The HCode provides
a unique signature that uniquely distinguishes each of the
n + 1 nodes from each other while satisfying the four re-
quired shadow code properties. One option would be to
simply use lg(n + 1) bits and assign each node a binary
number from 0 to n. However, to minimize the shadow
code length W (v), we use a Huffman coding style algo-
rithm instead to compute the HCodes and W (v). This
algorithm uses two data structures: a binary encoding
tree T with n + 1 leaf nodes, one for v and each of its
children, and a min-priority queue, initialized with n + 1
elements, one for v and each of its children, that is or-
dered by node weight. While the priority queue has more
than one element, we remove the two elements x and y
with lowest weight from the priority queue, create a new
6
V1
I
G: 
I
L:
W: 0
3
3= max (2, 2) +1
V1
I
SC= ***
G: 
L: 000  ID= 000
W:3