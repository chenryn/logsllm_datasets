Above, we articulate the key design decision within BiFo-
cals to allow us to recover the exact timing of network pack-
ets in ﬂight: we time-stamp packets using their associated
on-ﬁber symbolstream. To understand how this criterion
2This is the classic “arbiter problem” of asynchronous events
in a clocked digital system.
Figure 2: Comparison between an actual symbolstream on
ﬁber (top panel) and two hypothetical examples of extracted
data bitstreams, motivating both the potential (middle) and
likely (bottom) loss of measurement precision due to the
absence of a continuous timebase in the bitstreams.
tained, while the bottom demonstrates the most common
case where timing information is perturbed. Namely, in the
absence of the continuous timebase of the symbolstream, it
is diﬃcult to prevent “timing slop” of varying degree, with
resulting errors in timing measurements, even though the
network packets themselves are properly received and trans-
ferred to higher layers of the stack. This forms the crux of
the issue of precise measurement of packet timings — the
manner in which packets are time-stamped.
2.2 Sources of measurement error
With a renewed understanding of the Physical Layer of the
network and the diﬀerence between symbolstreams and data
bitstreams, we proceed to categorize the methods for time-
stamping packets. The pertinent diﬀerence among these in-
volves the “when” and “where” of time stamping as the pack-
ets transit the network and arrive at either the commodity
end-host receiver, or our BiFocals tool, respectively. Here,
we outline four approaches of increasing precision:
User-space software packet stamping: Software ap-
plications, executing in user-space context and deployed on
commodity operating systems and computer end-hosts, serve
overwhelmingly as the most common network measurement
tools [7]. Embodying a balance among intrusiveness, cost,
convenience, and accuracy, the canonical approach uses ei-
ther an active or passive probe to observe traﬃc originating
or terminating on the given end-host. Packets are assigned
time-stamps as the user-space software processes them; such
observations enable inference into traﬃc behavior on net-
work paths.
While software tools are essential and productive elements
of network research, it has long been recognized that they
risk distortion of the metrics they seek to measure. The core
problem involves the unmeasurable layers between the soft-
ware and the optical ﬁber: network adapter (with hardware
queues, on-chip buﬀering, and interrupt decisions), com-
puter architecture (chipset, memory, and I/O buses), device
driver, operating system (interrupt delivery and handling),
and even measurement software itself. Each of these layers
adds its own dynamics, distorts measurements in ways not
deterministically reproducible, and contributes strongly to
the timing errors as in Figure 2.
Kernel interrupt-handler stamping: Rather than hav-
ing the user-space software application assign time-stamps
to packets upon arrival, it is possible to modify the oper-
ating system kernel to internally time-stamp packets while
servicing the network-adapter interrupts that announce the
344Figure 3: Diagram of BiFocals transmission and acquisi-
tion hardware and software (see detailed explication in the
Appendices) connected across the network under test, with
notations on the photograph of the hardware.
Figure 4: Timings for network traﬃc across a direct optical
link between the sender and receiver: BiFocals presents an
ideally homogeneous response, while kernel interrupt-han-
dler stamping, a stringent type of end-host software, shows
severe broadening and extensive distortion.
translates into practice, we brieﬂy outline our instrumen-
tation architecture here. In the Appendices, we detail the
implementation and veriﬁcation of BiFocals, expounding
upon both the hardware foundation and software stack.
As depicted in Figure 3, BiFocals can be viewed as a
special network adapter decomposed into two independent
layers — an oﬀ-line software stack for the generation and
deconstruction of symbolstreams, and separate physics test
equipment (oscilloscopes, pattern generators, lasers, etc.) to
faithfully send and receive these symbolstreams on the opti-
cal ﬁber. Note that this clean decomposition also separates
what we implement in software (the bits we send) from what
we implement in hardware (how we send them), enabling us
to separately validate the ﬁdelity of our hardware, indepen-
dent of the software implementation of the Physical Layer.
Further, this ensures that we can reproducibly send identi-
cal traﬃc on successive iterations, unlike common methods
(tcpreplay, iperf, etc.) that introduce non-determinism.
On the software level, information is represented in binary
Ethernet-compliant3 symbolstreams, as sequences of ones
and zeros (with each integer representing a distinct bit).
On the hardware level, information is represented by light
intensity: optical power modulated in time, oﬀ and on, to
correspond to “0” and “1” bits, with unit length set by the
symbol rate. This hardware implementation ensures that
the binary symbolstreams are transmitted and acquired with
perfect ﬁdelity.4
2.4 Need for improved precision
It is worthwhile to question the extent of the need for the
improved precision that BiFocals provides. Indeed, as we
mention in the Introduction above, the packet chains that we
ultimately observe in Section 3 show regimes of tiny timing
delays interspersed by gaps of huge delays. This leads one to
wonder: Could not such qualitative behavior be captured by
existing techniques that use software on endpoints, without
the diﬃculty of such specialized instrumentation as ours?
To probe this question quantitatively and further moti-
vate our instrumentation, we conduct reference experiments
3Here, IEEE 802.3-2008 Clauses 49 (PCS) and 51 (PMA)
for 10 Gbps optical Ethernet [11].
4In accordance with IEEE 802.3-2008 Clause 52 [11].
comparing BiFocals to the above method of kernel inter-
rupt-handler stamping, which we recall is a more rigorous
and less error-prone evolution of the typical end-host soft-
ware methodology. While space constraints preclude a full
description of this comparison setup, we note in passing our
use of high-end multicore servers as end-hosts, running a cus-
tomized iperf [12] application and a modiﬁed Linux 2.6.27.2
kernel to read the time-stamp counter register (RDTSC) upon
handling the network packet interrupt.5
Using both BiFocals and this reference kernel interrupt-
handler stamping, we directly connect transmitter and re-
ceiver via ﬁber-optic link and measure the inter-packet delay.
Figure 4 overlays the probability density histogram of inter-
packet delays for each method and clearly depicts qualitative
and quantitative distinctions between these techniques: Bi-
Focals presents a perfect delta function where all packets
have the same inter-packet delay, while the comparison end-
host software shows severe broadening and excessive struc-
ture, with errors up to 150 µs. Any attempt to characterize
the timing response across actual network paths with such a
distortive tool would create grave diﬃculties in diﬀerentiat-
ing the response due to the actual network path from that of
the measurement tool. We further note that the broadening
of the timings from the end-host software is suﬃcient even to
overwhelm the coarse structure of our results, as presented
in Section 3.
3. MEASUREMENTS
We apply our BiFocals instrumentation to study net-
work transit eﬀects for a variety of traﬃc ﬂows on 10 Gbps
Ethernet over ﬁber-optic links. This paper focuses on two
scenarios: an isolated enterprise router that we use as a con-
trol case, and a high-performance, semi-private WAN path,
spanning 15 000 km of the NLR [22] backbone, circumscrib-
ing the United States and traversing eleven routers. In both
scenarios, we directly sample the symbolstream oﬀ the ﬁber
and present exact characterization of the network path itself.
5Further, we took care to maximize RDTSC precision by prop-
erly inserting explicit memory barriers to serialize instruc-
tions, binding iperf to the same processor core, and dis-
abling any processor power-conservation features.
 250255075100125150Inter-packet Delay [s]10-410-310-210-1100Probability DensityEnd-host SoftwareBiFocals345Packet Nominal Packet
size
[Bytes]
rate
[kpps]
data rate
[Gbps]
Inter-
Inter-
packet gap packet delay
[bits]
[ns]
1500
1500
1500
46
46
46
1
3
9
1
3
9
82.0
246.1
740.5
1 755.6
5 208.3
15 625.0
109 784
28 440
1 304
5 128
1 352
72
12 199
4 064
1 350
570
192
64
Table 1: Ensembles of various packet sizes and data rates,
with resulting packet rates and inter-packet gaps and delays,
for network traﬃc homogeneous in time. Rows correspond
to the six subﬁgures in Figures 6 and 7.
preamble and Start-of-Frame (SOF) delimiter. The packet
rate is simply the number of Ethernet packets in a given
period of time. Finally, in discussing the timings between
packets, we deﬁne two separate quantities: inter-packet de-
lay (IPD) is the time diﬀerence, or spacing, between identi-
cal bit positions in the Ethernet SOF delimiter for succes-
sive packets in the network ﬂow, while the inter-packet gap
(IPG) is the time between the end of one Ethernet packet
and the beginning of the successive packet. More precisely,
IPG is the bit-time measured from the last bit of the Eth-
ernet Frame Check Sequence ﬁeld of the ﬁrst packet to the
ﬁrst bit of the preamble of the subsequent packet, thus in-
cluding the idle (/I/), start (/S/), or terminate (/T/) control
characters from the 64b/66b PCS line-code [11].
All experiments consist of the BiFocals apparatus gener-
ating UDP traﬃc at nominal data rates of 1 Gbps, 3 Gbps,
and 9 Gbps for packet sizes of 46 Bytes (the minimum
allowed) and 1500 Bytes (default Maximum Transmission
Unit). For each data rate, the packets are homogeneously
distributed in time: separated by a ﬁxed number of 64b/66b
line code bits (for example, /I/ control characters), to ex-
hibit identical IPG and IPD at the ingress point. Table 1
depicts the parameter space of packet size and nominal data
rate, with resulting packet rate, IPG, and IPD.
We note that, while enforcing a homogeneous distribution
of packets in time, the speciﬁcs of the 64b/66b line code
prevent the generation of packet streams at arbitrary data
rates.8 Therefore, inter-packet gaps can only be transmitted
as a certain discrete number of control characters, most of
which are idles (/I/). The signiﬁcance of this constraint is
apparent below.
To measure the timings for over a million network pack-
ets for each packet size and data rate in Table 1, we had to
acquire over two trillion samples from the optical ﬁber and
process them oﬀ-line using resources exceeding 5000 proces-
sor-hours.
3.3 Results for control router
Our ﬁrst experiment transmits data across a single iso-
lated router, disconnected from any outside network. We
observe neither packet loss nor packet reordering with one
exception: 5% loss occurs for our smallest packet size at the
highest data rate (46-Byte packets sent at 9 Gbps, corre-
8Ethernet packets must be aligned at the start or middle of
the 64-bit PCS frame.
Figure 5: Map of the National LambdaRail (NLR), depict-
ing high link utilization for BiFocals–generated traﬃc with
a 9 Gbps data rate and 16 Mpps packet rate.
3.1 Experimental network setup
For the control router, we use a Cisco 6500, conﬁgured
with IOS 12.2(33)SXI1 with tail-drop queueing, a CEF720 4-
port 10-Gigabit Ethernet module, two 16-port Gigabit mod-
ules, and one Supervisor Engine 720. The 6500’s centralized
forwarding card forces all traﬃc to transit the router back-
plane, even though the two 10GbE ingress and egress inter-
faces share the same line card. While performing the con-
trol experiments, we isolated the router from any extraneous
traﬃc.
Our main experimental path is a static route across the
NLR PacketNet backbone, designed so that traﬃc originates
and terminates at the same physical location at Cornell Uni-
versity. To reach the nearest NLR Point-of-Presence (POP),
traﬃc is switched by a campus backbone switch (Cisco 6500)
to an upstream campus router (Cisco 6500) in a New York
City carrier-hotel and, there, routed onto the NLR back-
bone for subsequent transit across eight NLR routers span-
ning 15 000 kilometers. (Note that, in their primary failover
role, neither of these campus Cisco 6500s handles commod-
ity traﬃc, and they thus maintain light loads.) Figure 5
depicts the topology of our network path, as well as a real-
time picture of one of our 9 Gbps traﬃc ﬂows. All of these
NLR optical links use Dense Wavelength Division Multiplex-
ing technology and connect Cisco CRS-1 core routers, each
with IOS XR 3.6.1[00] (tail-drop queueing), two 8-interface
10GbE line-cards, two modular service cards, and two 8-slot
route processors. We note that Cisco recently upgraded the
NLR infrastructure, so these routers are identical and con-
temporary, with passive optical components in ideal condi-
tion. We monitor the background traﬃc for all interfaces at
each NLR POP and on both campus routers, using SNMP
queries and RRDtool storage with 10-second resolution.