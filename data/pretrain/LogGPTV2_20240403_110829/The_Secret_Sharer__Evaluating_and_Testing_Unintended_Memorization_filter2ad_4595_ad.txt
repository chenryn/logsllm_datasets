276    28th USENIX Security Symposium
USENIX Association
0102030Epochs of training051015202530Estimated exposure of canary1.01.52.02.53.0Cross-Entropy LossExposureTesting LossTraining LossFigure 9: An example to illustrate the shortest path search
algorithm. Each node represents one partially generated string.
Each edge denotes the conditional probability Pr(xi|x1...xi−1).
The path to the leaf with minimum perplexity is highlighted,
and the log-perplexity is depicted below each leaf node.
Therefore, ﬁnding s[r] minimizing the cost of the path is equiv-
alent to minimizing its log-perplexity. Figure 9 presents an
example to illustrate the idea. Thus, ﬁnding the sequence with
lowest perplexity is equivalent to ﬁnding the lightest path
from the root to a leaf node.
Concretely, we implement a shortest-path algorithm di-
rectly inspired by Dijkstra’s algorithm [10] which computes
the shortest distance on a graph with non-negative edge
weights. The algorithm maintains a priority queue of nodes on
the graph. To initialize, only the root node (the empty string)
is inserted into the priority queue with a weight 0. In each
iteration, the node with the smallest weight is removed from
the queue. Assume the node is associated with a partially gen-
erated string p and the weight is w. Then for each token t such
that p@t is a child of p, we insert the node p@t into the pri-
ority queue with w− logPr(t| fθ(p)) where −logPr(t| fθ(p))
is the weight on the edge from p to p@t.
The algorithm terminates once the node pulled from the
queue is a leaf (i.e., has maximum length). In the worst-case,
this algorithm may enumerate all non-leaf nodes, (e.g., when
all possible sequences have equal perplexity). However, em-
pirically, we ﬁnd shortest-path search enumerate from 3 to 5
orders of magnitude fewer nodes (as we will show).
During this process, the main computational bottleneck
is computing the edge weights −logPr(t| fθ(p)). A modern
GPU can simultaneously evaluate a neural network on many
thousand inputs in the same amount of time as it takes to
evaluate one. To leverage this beneﬁt, we pull multiple nodes
from the priority queue at once in each iteration, and compute
all edge weights to their children simultaneously. In doing so,
we observe a 50× to 500× reduction in overall run-time.
Applying this optimization violates the guarantee that the
ﬁrst leaf node found is always the best. We compensate by
counting the number of iterations required to ﬁnd the ﬁrst full-
length sequence, and continuing that many iterations more be-
fore stopping. We then sort these sequences by log-perplexity
and return the lowest value. While this doubles the number
of iterations, each iteration is two orders of magnitude faster,
and this results in a substantial increase in performance.
Figure 10: Number of iterations the shortest-path search re-
quires before an inserted canary is returned, with |R | = 230.
At exposure 30, when the canary is fully memorized, our al-
gorithm requires over four orders of magnitude fewer queries
compared to brute force.
8.2 Efﬁciency of Shortest-Path Search
We begin by again using our character level language model
as a baseline, after inserting a single 9-digit random canary to
the PTB dataset once. This model completely memorizes the
canary: we ﬁnd its exposure is over 30, indicating it should
be extractable. We verify that it actually does have the lowest
perplexity of all candidates canaries by enumerating all 109.
Shortest path search: We apply our shortest-path algorithm
to this model and ﬁnd that it takes only 105 total queries: four
orders of magnitude fewer than a brute-force approach takes.
Perhaps as is expected, we ﬁnd that the shortest-path al-
gorithm becomes more efﬁcient when the exposure of the
canary is higher. We train multiple different models contain-
ing a canary to different ﬁnal exposure values (by varying
model capacity and number of training epochs). Figure 10
shows the exposure of the canary versus the number of it-
erations the shortest path search algorithm requires to ﬁnd
it. The shortest-path search algorithm reduces the number of
values enumerated in the search from 109 to 104 (a factor
of 100,000× reduction) when the exposure of the inserted
phrase is greater than 30.
8.3 High Exposure Implies Extraction
Turning to the main purpose of our extraction algorithm, we
verify that it actually means something when the exposure of
a sequence is high. The underlying hypothesis of our work is
that exposure is a useful measure for accurately judging when
canaries have been memorized. We now validate that when
the exposure of a phrase is high, we can extract the phrase
from the model (i.e., there are not many false positives, where
exposure is high but we can’t extract it). We train multiple
models on the PTB dataset inserting a canary (drawn from a
randomness space |R | ≈ 230) a varying number of times with
USENIX Association
28th USENIX Security Symposium    277
aabaabbbba0.10.90.50.50.40.6perplexity=1.47 perplexity=1.73 perplexity=1.73perplexity=4.642025303540Exposure of inserted canary103104105106107IIterations to extract canaryUser
A
B
C
D
F
G
Secret Type Exposure Extracted?
CCN
SSN
SSN
SSN
SSN
SSN
SSN
CCN
CCN
CCN
52
13
16
10
22
32
13
36
29
48
(cid:88)
(cid:88)
(cid:88)
Table 2: Summary of results on the Enron email dataset. Three
secrets are extractable in < 1 hour; all are heavily memorized.
they can be extracted by our shortest-path search algorithm.
When we run our extraction algorithm locally, it requires
on the order of a few hours to extract the credit card and
social security numbers. Note that it would be unfair to draw
from this that an actual attack would only take a few hours:
this local attack can batch queries to the model and does not
include any remote querying in the run-time computation.
9 Preventing Unintended Memorization
As we have shown, neural networks quickly memorize secret
data. This section evaluates (both the efﬁcacy and impact
on accuracy) three potential defenses against memorization:
regularization, sanitization, and differential privacy.
9.1 Regularization
It might be reasonable to assume that unintended memoriza-
tion is due to the model overtraining to the training data. To
show this is not the case, we apply three state-of-the-art regu-
larization approaches (weight decay [28], dropout [45], and
quantization [25]) that help prevent overtraining (and overﬁt-
ting) and ﬁnd that none of these can prevent the canaries we
insert from being extracted by our algorithms.
9.1.1 Weight Decay
Weight decay [28] is a traditional approach to combat over-
training. During training, an additional penalty is added to the
loss of the network that penalizes model complexity.
Our initial language 600k parameters and was trained on
the 5MB PTB dataset. It initially does not overtrain (because it
does not have enough capacity). Therefore, when we train our
model with weight decay, we do not observe any improvement
in validation loss, or any reduction in memorization.
Figure 11: Extraction is possible when the exposure indicates
it should be possible: when |R | = 230, at an exposure of 30
extraction quickly shifts from impossible to possible.
different training regimes (but train all models to the same
ﬁnal test accuracy). We then measure exposure on each of
these models and attempt to extract the inserted canary.
Figure 11 plots how exposure correlates with the success
of extraction: extraction is always possible when exposure is
greater than 33 but never when exposure is less than 31.
8.4 Enron Emails: Memorization in Practice
It is possible (although unlikely) that we detect memorization
only because we have inserted our canaries artiﬁcially. To
conﬁrm this is not the case, we study a dataset that has many
naturally-occurring secrets already in the training data. That
is to say, instead of running experiments on data with the ca-
naries we have artiﬁcially inserted and treated as “secrets”, we
run experiments on a dataset where secrets are pre-existing.
The Enron Email Dataset consists of several hundred thou-
sand emails sent between employees of Enron Corporation,
and subsequently released by the Federal Energy Regulatory
Commission in its investigation of the company. The com-
plete dataset consists of the full emails, with attachments.
Many users sent highly sensitive information in these emails,
including social security numbers and credit card numbers.
We pre-process this dataset by removing all attachments,
and keep only the body of the email. We remove the text of the
email that is being responded to, and ﬁlter out automatically-
generated emails and emails sent to the entire company. We
separate emails by sender, ranging from 1.7MB to 5.6MB
(about the size of the PTB dataset) and train one character-
level language model per user who has sent at least one secret.
The language model we train is again a 2-layer LSTM, how-
ever to model the more complex nature of writing we increase
the number of units in each layer to 1024. We again train to
minimum validation loss.
We summarize our results in Table 2. Three of these secrets
(that pre-exist in the data) are memorized to a degree that
278    28th USENIX Security Symposium
USENIX Association
010203040Exposure0.00.20.40.60.81.0Probability extraction succeedsSmoothed success rateIndividual trialsuccess rateIn order to directly measure the effect of weight decay
on a model that does overtrain, we take the ﬁrst 5% of the
PTB dataset and train our language model there. This time
the model does overtrain the dataset without regularization.
When we add L2 regularization, we see less overtraining occur
(i.e., the model reaches a lower validation loss). However, we
observe no effect on the exposure of the canary.
9.1.2 Dropout
Dropout [45] is a regularization approach proposed that has
been shown to effectively prevent overtraining in neural net-
works. Again, dropout does not help with the original model
on the full dataset (and does not inhibit memorization).
We repeat the experiment above by training on 5% of the
data, this time with dropout. We vary the probability to drop a
neuron from 0% to 90%, and train ten models at each dropout
rate to eliminate the effects of noise.
At dropout rates between 0% and 20%, the ﬁnal test accu-
racy of the models are comparable (Dropout rates greater than
30% reduce test accuracy on our model). We again ﬁnd that
dropout does not statistically signiﬁcantly reduce the effect
of unintended memorization.
9.1.3 Quantization
In our language model, each of the 600K parameters is rep-
resented as a 32-bit ﬂoat. This puts the information theoretic
capacity of the model at 2.4MB, which is larger than the
1.7MB size of the compressed PTB dataset. To demonstrate
the model is not storing a complete copy of the training data,
we show that the model can be compressed to be much smaller
while maintaining the same exposure and test accuracy.
To do this, we perform weight quantization [25]: given a
trained network with weights θ, we force each weight to be
one of only 256 different values, so each parameter can be
represented in 8 bits. As found in prior work, quantization
does not signiﬁcantly affect validation loss: our quantized
model achieves a loss of 1.19, compared to the original loss
of 1.18. Additionally, we ﬁnd that the exposure of the inserted
canary does not change: the inserted canary is still the most
likely and is extractable.
9.2 Sanitization
Sanitization is a best practice for processing sensitive, private
data. For example, we may construct blacklists and ﬁlter out
sentences containing what may be private information from
language models, or may remove all numbers from a model
trained where only text is expected. However, one can not
hope to guarantee that all possible sensitive sequences will be
found and removed through such black-lists (e.g., due to the
proliferation of unknown formats or typos).
We attempted to construct an algorithm that could auto-
matically identify potential secrets by training two models
on non-overlapping subsets of training data and removing
any sentences where the perplexity between the two models
disagreed. Unfortunately, this style of approach missed some
secrets (and is unsound if the same secret is inserted twice).
While sanitization is always a best practice and should be
applied at every opportunity, it is by no means a perfect de-
fense. Black-listing is never a complete approach in security,
and so we do not consider it to be effective here.
9.3 Differential Privacy
Differential privacy [12, 14, 15] is a property that an algorithm
can satisfy which bounds the information it can leak about its
inputs. Formally deﬁned as follows.
Deﬁnition 5 A randomized algorithm A operating on a
dataset D is (ε,δ)-differentially private if
Pr[A(D) ∈ S] ≤ exp(ε)· Pr[A(D(cid:48)) ∈ S] + δ
for any set S of possible outputs of A, and any two data sets
D,D(cid:48) that differ in at most one element.
Intuitively, this deﬁnition says that when adding or remov-
ing one element from the input data set, the output distribu-
tion of a differentially private algorithm does not change by
much (i.e., by more than an a factor exponentially small in ε).
Typically we set ε = 1 and δ < |X|−1 to give strong privacy
guarantees. Thus, differential privacy is a desirable property
to defend against memorization. Consider the case where D
contains one occurrence of some secret training record x, and
D(cid:48) = D − {x}. Imprecisely speaking, the output model of
a differentially private training algorithm running over D,
which contains the secret, must be similar to the output model
trained from D(cid:48), which does not contain the secret. Thus, such
a model can not memorize the secret as completely.
We applied the differentially-private stochastic gradient de-
scent algorithm (DP-SGD) from [1] to verify that differential
privacy is an effective defense that prevents memorization. We
used the initial, open-source code for DP-SGD4 to train our
character-level language model from Section 3. We slightly
modiﬁed this code to adapt it to recurrent neural networks
and improved its baseline performance by replacing the plain
SGD optimizer with an RMSProp optimizer, as it often gives
higher accuracy than plain SGD [47].
The DP-SGD of [1] implements differential privacy by clip-
ping the per-example gradient to a max norm and carefully
adding Gaussian noise. Intuitively, if the added noise matches
the clipping norm, every single, individual example will be
masked by the noise, and cannot affect the weights of the net-
work by itself. As more noise is added, relative to the clipping
norm, the more strict the ε upper-bound on the privacy loss
that can be guaranteed.
4A more modern version is at https://github.com/tensorflow/privacy/.
USENIX Association
28th USENIX Security Symposium    279
Optimizer
ε
Test Estimated Extraction
Possible?
Loss
Exposure
P
D
h
t
i
W
RMSProp
RMSProp
RMSProp
RMSProp
RMSProp
RMSProp
SGD
P SGD
D
o
N
RMSProp
0.65
1.21
5.26
89
2× 108
1× 109
∞
N/A
N/A
1.69
1.59
1.41
1.34
1.32
1.26
2.11
1.86
1.17