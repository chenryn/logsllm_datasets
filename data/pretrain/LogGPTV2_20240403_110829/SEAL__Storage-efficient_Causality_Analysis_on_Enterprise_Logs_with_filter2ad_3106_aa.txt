title:SEAL: Storage-efficient Causality Analysis on Enterprise Logs with
Query-friendly Compression
author:Peng Fei and
Zhou Li and
Zhiying Wang and
Xiao Yu and
Ding Li and
Kangkook Jee
SEAL: Storage-efficient Causality Analysis on 
Enterprise Logs with Query-friendly Compression
Peng Fei, Zhou Li, and Zhiying Wang, University of California, Irvine; Xiao Yu, 
NEC Laboratories America, Inc.; Ding Li, Peking University; Kangkook Jee, 
University of Texas at Dallas
https://www.usenix.org/conference/usenixsecurity21/presentation/fei
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.SEAL: Storage-efﬁcient Causality Analysis on Enterprise Logs with
Query-friendly Compression
Peng Fei
University of California, Irvine
Zhou Li
Zhiying Wang
University of California, Irvine
University of California, Irvine
Xiao Yu
Ding Li
NEC Laboratories America, Inc.
Peking University
Kangkook Jee
University of Texas at Dallas
Abstract
Causality analysis automates attack forensic and facilitates
behavioral detection by associating causally related but tem-
porally distant system events. Despite its proven usefulness,
the analysis suffers from the innate big data challenge to store
and process a colossal amount of system events that are con-
stantly collected from hundreds of thousands of end-hosts in a
realistic network. In addition, the effectiveness of the analysis
to discover security breaches relies on the assumption that
comprehensive historical events over a long span are stored.
Hence, it is imminent to address the scalability issue in or-
der to make causality analysis practical and applicable to the
enterprise-level environment.
In this work, we present SEAL, a novel data compression ap-
proach for causality analysis. Based on information-theoretic
observations on system event data, our approach achieves
lossless compression and supports near real-time retrieval of
historic events. In the compression step, the causality graph
induced by the system logs is investigated, and abundant edge
reduction potentials are explored. In the query step, for maxi-
mal speed, decompression is opportunistically executed. Ex-
periments on two real-world datasets show that SEAL offers
2.63x and 12.94x data size reduction, respectively. Besides,
89% of the queries are faster on the compressed dataset than
the uncompressed one, and SEAL returns exactly the same
query results as the uncompressed data.
1 Introduction
System logs constitute a critical foundation for enterprise se-
curity. The latest computer systems have become more and
more complex and interconnected, and attacker techniques
have advanced to take advantage and nullify the conventional
security solutions which are based on static artifacts. As a
result, the security defense has turned more to pervasive sys-
tem event collection in building effective security measures.
Research has extensively explored security solutions using
system logs. Causality analysis in the log setting (or attack
provenance), as deﬁned in [83], is one such direction that
reconstructs information ﬂow by associating interdependent
system events and operations. For any suspicious events, the
analysis automatically traces back to the initial penetration
(root-cause diagnosis), or measures the amount of the impact
by enumerating the system resources affected by the attacker
(attack ramiﬁcation). Encouragingly, the security solutions
based on pervasive system monitoring and causality analysis
no longer remain as a research prototype. Many proposed
ideas have actualized as commercial solutions [8, 14, 22].
However, due to their data-dependent nature, the effective-
ness of the above security solutions is heavily constrained
by the system’s data storage and processing capability. On
one hand, keeping large volumes of comprehensive historical
system events is essential, as the security breach targeting an
enterprise tends to stay at the network over a long span: an
industry report by TrustWave [78] shows, on average, an intru-
sion prolongs over 188 days before the detection. On the other
hand, the size of a typical enterprise network and the amount
of system logs each host generates could put high pressure
on the security solutions. For instance, our industrial partner
reported that on average 50 GB amount of logs are produced
from a group of 100 hosts daily, and they can only sustain
at most three months of data despite the inexpensive storage
cost. There is a compelling need for a solution that can scale
storage and processing capacity to meet the enterprise-level
requirement.
Lossless compression versus lossy reduction. Compres-
sion techniques [79] come in handy for improving the stor-
age efﬁciency of causality analysis. Existing approaches
[37, 45, 77, 83] tend to carry out lossy reduction, which re-
moves logs matching pre-deﬁned patterns, leading to unavoid-
able information loss. Although they showed that the validity
of causality analysis is preserved on samples of investiga-
tion tasks, there is no guarantee that every task will derive
the right outcome. In Section 2.3, we show examples about
when they would introduce false positives/negatives. In ad-
dition, the accuracy of other applications such as behavioral
detection [30,53] and machine-learning based anomaly detec-
USENIX Association
30th USENIX Security Symposium    2987
tion [10, 19, 47, 62, 63, 84] would be tampered, when they use
the same log data. Alternatively, lossless compression [79]
allows any information to be restored and thus causality anal-
ysis is preserved. Though the standard tools like Gzip [18] are
expected to achieve a high compression rate, they are not ap-
plicable to our problem, because high computation overhead
of decompression will be incurred when running causality
analysis.
In this work, we challenge the common belief that lossless
compression is inefﬁcient for causality analysis, by devel-
oping SEAL (Storage-Efﬁcient Analysis on enterprise Logs)
under information-theoretic principles. Compared to the pre-
vious approaches, logs under a wider range of patterns can
be compressed in a lossless fashion without the need for care-
fully examining conditions such as traceability equivalence
or dependence preservation, while the validity and efﬁciency
of any investigation task of causality analysis are preserved.
Contributions. The main contributions of this paper are as
follows.
• We develop a framework of query-friendly compression
(QFC) specialized for causality analysis. In this framework,
the dependency graph is induced from the logs, and lossless
compression is applied to the structure (vertices and edges)
and then to the edge properties, or attributes (e.g., timestamp).
QFC ensures every query is answered accurately, while the
query efﬁciency is guaranteed as the majority of operations
required by queries are done directly on the compressed data.
• We design compression and querying algorithms accord-
ing to the deﬁnition of QFC. For graph structures, we deﬁne
merge patterns to be subgraphs whose edges are combined
into one new edge. For edge properties, delta coding [59]
and Golomb codes [28] are applied to exploit temporal local-
ity, meaning that consecutively collected logs have similar
timestamps. To return answers to a causality query, the pro-
posed method obviates decompression unless the relationship
between the timestamps of a compressed edge and the time
range of the query cannot be determined.
• A compression ratio estimation algorithm is provided
to facilitate the decision of using the compressed or uncom-
pressed format for a given dataset. We show that the com-
pression ratio can be determined by the average degree of
the dependency graph. Our algorithm estimates the average
degree by performing random walk on the dependency graph
with added self-loops, and randomly restarting another walk
during the process. If the estimated compression ratio of a
given dataset is smaller than a speciﬁed threshold, compres-
sion can be skipped.
• The above algorithms are implemented in SEAL, which
consists of the compression system that is applied to online
system logs and the querying system that serves causality
analytics. Due to the large amount of merge patterns in the
dependency graphs, SEAL can compress online log data into a
signiﬁcantly smaller volume. In addition, the query-friendly
design reduces the required decompression operations. We
evaluate SEAL on system logs from 95 hosts provided by
our industrial partner. The experiment results demonstrate an
average of 9.81x event reduction, 2.63x storage size reduction.
Besides, 89% of the queries are faster on the compressed
dataset than the uncompressed one. We also evaluate SEAL on
DARPA TC dataset [16] and achieved 12.94x size reduction.
Causality analysis to investigate attacked entities is shown to
return accurate results with our compression method.
2 Background
We ﬁrst describe the concepts of system logs and causality
analysis. Then, we review the existing works based on lossy
reduction and compare SEAL with them.
2.1 System Logs
To transparently monitor the end-host activities in a conﬁned
network, end-point detection and response (EDR) has become
a mainstream security solution [35]. A typical EDR system
deploys data collection sensors to collect the major system ac-
tivities such as ﬁle, process and network related events, as well
as events with high security relevancy (e.g., login attempts,
privilege escalation). Sensors then stream the collected sys-
tem events to a centralized data back-end. Data collection at
end-host hinges on different operating systems’ (OS) kernel-
level supports for system call level monitoring [11, 55, 69].
In this study, we obtained a dataset from the real-world cor-
porate environment. Data sources are the system logs gener-
ated by kernel audit [69] of Linux hosts and Event Monitoring
for Windows (ETW) [55] of Windows hosts respectively. The
system events belong to three different categories: (i) process
accesses (reads or write) ﬁles (P2F), (ii) process connects to or
accepts network sockets (P2N), and (iii) process creates other
processes, or exits it executions. These system events cap-
tured from each end-host are transferred to the back-end and
represented in a graph data structure [42] where nodes repre-
sent system resources (i.e., process, ﬁle, and network socket)
and edges represent interactions among nodes. Our system
labels edges with attributes speciﬁc to system operations. For
instance, amounts of data transferred for ﬁle and network
operations, command-line arguments for process creations.
The dataset comprises of various workloads that range from
simple administrative tasks to heavy-weight development and
data analysis tasks and also includes end-user desktops and
laptops as well as infra-structural servers.
Among the three categories of system events (ﬁle, network,
and process) in the dataset, ﬁle operations account for the
majority, taking over 90% portions, therefore become the
primary target for SEAL compression. In particular, the ﬁle
operation like create, open, read, write or delete is logged in
each ﬁle event, alongside its owner process, host ID, ﬁle path,
and timestamp. All ﬁle events have been properly anonymized
2988    30th USENIX Security Symposium
USENIX Association
(no user identiﬁable information exists in any ﬁeld of the
table) to address privacy concerns.
Despite its improved visibility, data collection for in-host
system activity results in a prohibitive amount of processing
and storage pressures, compared to other network-level mon-
itoring appliances [63]. For instance, our data collection de-
ployment on average reported approximately 50 GB amount
of logs for a group of 100 hosts daily. Given that a typical
enterprise easily exceeds hundreds of thousands of hosts for
its network, it is imminent to address the scalability issues in
order to make causality analysis practical and applicable to a
realistic network.
2.2 Causality Analysis in the Log Setting
After the end-point logs are gathered and reported to the data
processing back-end, different applications are run atop to pro-
duce insights to security operators, such as machine-learning
based threat detection [10], database queries [23–25] and
causality analysis (or data provenance) [83]. Although our
approach mainly focuses on the causality analysis, which re-
quires high ﬁdelity on its input data, it also beneﬁts other
analyses as our approach reduces data storage and computa-
tional costs.
To its core, causality analysis automates the data analysis
and forensic tasks by correlating data dependency among
system events. Using the restored causality, security opera-
tors accelerate root cause analysis of security incident and
attack ramiﬁcation. The causality analysis is considered to be
a de facto standard tool for investigating long-running, multi-
stage attacks, such as Advanced Persistent Threat (APT) cam-
paigns [58]. For any suspicious events reported by users or
third-part detection tools, the operator can issue a query to
investigate causally related activities. The causality analysis
then consults to its data back-end to restore the dependencies
within the speciﬁed scope. The accuracy of causality analysis
relies on the completeness of data collection, and the analysis
response time and usability depend on the data access time.
In Section 3.1, we demonstrate a causality analysis where our
compression approach addresses the scalability issues without
deteriorating accuracy and usability.
2.3 Comparison with Lossy Reduction
To reduce the storage overhead in supporting causality anal-
ysis, prior works advocated lossy reduction [37, 45, 77, 83],
which removes logs of certain patterns before they are stored
by the back-end server. Here we show the reduction rules of
the prior works and compare their scope to SEAL.
LogGC [45] removes temporary ﬁles from the collected
data that are deemed not affecting causality analysis. Node-
Merge [77] merges the read-only events (Read events in our
data) during the process initialization. The approach proposed
by Xu et al. [83] removes repeated edges between two objects
Figure 1: Comparison of our method SEAL to LogGC [45],
NodeMerge [77], methods by Xu et al. [83] and Hossain
et al. [37]. In NodeMerge (the second graph in the middle
column), the node T represents a new node. In SEAL (the right
column), the blue solid circles represent new nodes.
on the same host (e.g., multiple read events between a ﬁle and
a process) when a condition termed trackability equivalence
is satisﬁed. Hossain et al. [37] relaxes the condition of [83]
such that more repeated events (e.g., repeated events cross
hosts) can be pruned, which tends to be more conservative to
maintain graph trackability.
SEAL is more general compared to any of the existing works.
Our lossless compression schema is agnostic to ﬁle types and
is therefore complementary to LogGC. SEAL also processes
Write and Execute events, compared to NodeMerge, and
therefore covers the whole life-cycle of a process. Compared
to Xu et al. and Hossain et al., SEAL is more aggressive, e.g.,
merging not only the edges repeated between a pair of nodes.
Figure 1 also illustrates the differences. In Section 5, we
compare the overall reduction rate, with Hossain et al., which
is the most recent work.
In terms of data ﬁdelity, none of the prior works can guar-
antee false negative/positive would not occur during attack
investigation. For LogGC, if the removed temporary ﬁles
are related to network sockets, data exﬁltration done by the
attacker might be missed. For NodeMerge, the authors de-
scribed a potential evasion method: the attacker can keep
the malware waiting for a long time before the actual at-
tack, so that the malware might be considered as a read-only
ﬁle as determined by their threshold and break the causality
dependencies (see [77] Section 10.4). PCAR of Xu et al. in-
troduces false connectivity in two (out of ten) investigation
tasks (see [83] Section 4.3). Similarly, false negatives could
USENIX Association
30th USENIX Security Symposium    2989
Dangling Node...Before ReductionPrevious MethodsSEALInitial StageTABC...ACBLogGC Xu et al, Hossain et alNodeMergeNULLNew NodeMerged EventField
starttime
endtime
srcid
dstid
agentid
accessright
Exemplar Value
1562734588971
1562734588985
15
27
-582777938
Execute
Table 1: On example entry of FileEvent.
New Node Represented Nodes
a
b
c
A, B
B, C
G, H