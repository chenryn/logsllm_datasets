    图 12.4–Docker 主机内核版本
    版本是`#`符号之后、破折号之前的数字。在我们的主机上，我们有 100 版本。版本是从`uname -r`命令返回的全名。您需要将这两者都提供给`driverkit`命令来构建内核模块。
2.  If you are using the installation script, we retrieve the options and supply them automatically. If you are doing this step manually, you can use the following two lines of code to store the information in variables to be passed to the build command:
    ```
    kernelversion=$(uname -v | cut -f1 -d'-' | cut -f2 -d'#')
    kernelrelease=$(uname -r)
    ```
    我们使用`cut`命令从`uname -v`命令中删除不必要的信息，并将其存储在名为`kernelversion`的变量中。我们还将`uname -r`命令的输出存储在一个名为`kernelrelease`的变量中。
3.  现在，您可以使用我们提取的 Docker 映像和 driverkit 可执行文件来创建模块:
    ```
    driverkit docker --output-module /tmp/falco.ko --kernelversion=$kernelversion --kernelrelease=$kernelrelease --driverversion=dev --target=ubuntu-generic
    ```
4.  模块构建过程需要一分钟，一旦构建完成，driverkit 将向您显示新模块的位置:
    ```
    INFO driver building, it will take a few seconds   processor=docker
    INFO kernel module available                       path=/tmp/falco.ko
    ```
5.  对于添加新模块的最后一步，我们需要将其复制到正确的位置，并使用`modprobe` :
    ```
    sudo cp /tmp/falco.ko /lib/modules/$kernelrelease/falco.ko
    sudo depmod
    sudo modprobe falco
    ```
    加载模块
6.  You can verify that the module has been added by running `lsmod`:
    ```
    lsmod | grep falco
    ```
    如果加载成功，您将看到类似如下的输出:
    ```
    falco                 634880  4
    ```
就这样！现在主机上有 Falco 模块，它将提供给你的 KinD 集群。
## 使用集群上的模块
在标准的 Kubernetes 集群上，Falco 部署会将 Falco 容器中的`/dev`挂载映射到主机的`/dev`挂载。通过安装`/dev`，Falco 吊舱可以使用在工作节点的主机操作系统上运行的内核模块。
## 使用 KinD 中的模块
您可能会问自己，将 Falco 模块添加到主机将如何使其对 KinD 集群可用？我们只将其添加到主机本身，而 KinD 集群是在另一个 Docker 容器中运行的容器。那么，KinD pod 如何使用 Docker 主机中的模块呢？
还记得 KinD 在启动 KinD 容器时有一个装载额外卷的功能吗？在我们的安装中，我们为`/dev:/dev`添加了一个挂载点，这将在我们的容器中创建一个挂载点，挂载到主机的`/dev`文件系统中。如果我们查看主机的`/dev`文件系统，我们会在列表中看到 Falco 条目，如下所示:
```
cr--------  1 root root    244,   0 May  4 00:58 falco0
```
这是 Falco 吊舱启动时将用作其模块的东西。
但是等等！我们刚刚说过`/dev`安装在我们的 KinD 容器中，指向主机的`/dev`文件系统。那么 Kubernetes 集群中的容器如何访问`/dev`文件系统呢？
如果我们看一下我们将在下一节中使用的 Falco DaemonSet 文件，我们会看到清单为 pod 创建了几个挂载点。
`volumeMount`条目之一如下:
```
- mountPath: /host/dev
  name: dev-fs
  readOnly: true
```
`volumeMount` 条目使用的是 DaemonSet 的 *卷*部分中声明的卷:
```
- name: dev-fs
  hostPath:
    path: /dev
```
当 Falco 吊舱启动时，它会将吊舱的`/dev`底座安装到 KinD 容器的`/dev`底座上。最后，KinD 容器的`/dev`底座安装在 Docker 主机的`/dev`上，Falco 模块位于该处。(记得套娃的比喻。)
具备所有先决条件后，我们就可以部署 Falco 了。
## 部署 Falco·达蒙塞特
如果您要去从 GitHub 存储库中运行`install-falco.sh`脚本，将使用本节中提供的相同步骤安装 Falco。在本书的 GitHub repo 中，所有的 Falco 文件都位于`chapter12`目录中。
由于本章有几个不同的部分，下图提供了`chapter12`目录内容的描述:
![Figure 12.5 – Diagram of the chapter12 directory in the book's GitHub repository  ](img/Fig_12.5_B15514.jpg)
图 12.5–书的 GitHub 存储库中第 12 章目录的图表
请记住，Falco 包括一套标准规则，其中包括标准审计规则。我们已经将规则文件放在`falco/falco-config`目录中。我们从默认安装更改的唯一值是日志格式，我们将其更改为 JSON，并额外设置了`http_output`的值以使用 Falcosidekick。
要手动部署 Falco DaemonSet，您需要在`install`目录中部署三个清单，并使用`falco-config`目录内容创建一个机密。
### 创建 Falco 服务帐户和服务
由于我们希望在专用的名称空间中运行 Falco，因此我们需要在集群上创建一个名为`falco`的名称空间。运行以下命令:
```
kubectl create ns falco
```
像所有的 Kubernetes 应用一样，我们需要为应用创建一个拥有正确的 RBAC 权限的帐户来执行必要的任务。我们的第一步是创建该服务帐户，该帐户将用于在 DaemonSet 部署中分配 RBAC 权限:
1.  使用`kubectl`，创建服务账户:
    ```
    kubectl apply -f falco/install/falco-account.yaml -n falco
    ```
2.  接下来，我们需要为 Falco 创建一个服务。包含的`falco-service.yaml`文件将在 TCP 端口`8765`上创建一个新服务。使用 kubectl，应用清单:
    ```
    kubectl apply -f falco/install/falco-service.yaml -n falco
    ```
3.  Falco uses files for the base configuration and rules. Since we are running Falco in Kubernetes, we need to store the files in a Kubernetes object so they can be used by the Falco pods. To store the files in a ConfigMap, create a new ConfigMap called `falco-config` using all of the files in the `falco-config` directory:
    ```
    kubectl create configmap falco-config --from-file=falco/falco-config -n falco
    ```
    重要说明
    如果您在部署 Falco 后需要修改任何配置文件，您应该删除配置映射，并使用新更新的文件重新创建它。更新配置映射后，您还需要重新启动每个 Falco pod，以便从配置映射中重新加载更新的文件。
4.  最后一步是部署 daemmonset:
    ```
    kubectl apply -f falco/install/falco-daemonset-configmap.yaml -n falco
    ```
一旦 Falco 吊舱运行，您可以通过查看吊舱的日志来验证运行状况。输出将类似于下面的输出(错误是预期的，Falco 试图在所有位置找到内核模块，其中一些不存在，导致“错误”):
![Figure 12.6 – Successful Falco pod startup log ](img/Fig_12.6_B15514.jpg)
图 12.6–成功的 Falco 吊舱启动日志
你现在有一个 Falcodaemmonset 设置，将审计事件在你的豆荚。
重要说明
您可能会在 Falco pod 日志的最后一行收到一个错误，类似于以下示例:
**2020 年 5 月 5 日星期二 20:38:14:运行时错误:打开设备/主机/dev/falco0 时出错。请确保您有根凭据，并且 falco-probe 模块已加载。正在退出。**
在这种情况下，您的 Falco 模块可能没有被加载，所以回到 modprobe 步骤并再次执行它们。您不需要重新启动 Falco 吊舱，因为一旦 Falco 能够在`/dev`目录中看到该模块，更改将被拾取，并且 Falco 将开始记录。
当然，为了有用，我们需要将事件转发到中央日志系统。在默认部署中，Falco 日志仅在每台主机上运行的 pod 上可用。如果您有 30 台主机，您将有 30 个唯一的 Falco 日志，每台主机上一个。俗话说，在分散的系统中发现一个事件就像大海捞针。
Falco 日志使用标准输出，因此我们可以轻松地将日志转发到任何第三方日志系统。虽然我们可以选择许多选项作为我们的日志服务器，但是我们选择了来使用**弹性搜索、Fluentd 和 Kibana** ( **EFK** )以及 Falcosidekick 转发我们的日志。
## 部署 EFK
我们的第一步将是部署**弹性搜索**来接收事件数据。要安装弹性搜索，我们需要数据的持久存储。幸运的是，由于 Rancher 的本地置备程序，我们使用了一个 KinD 集群，因此我们拥有持久存储。
为了简化部署，我们将使用 Bitnami 的 Helm 图表为 Elasticsearch 和 Kibana 部署我们的栈。您需要安装 Helm 二进制文件才能将图表部署到集群中。如果你正在做书中的练习，你应该已经在 [*第五章*](05.html#_idTextAnchor150) *中的 KinD 部署中安装了 Helm3，Kubernetes Bootcamp* 。
通过运行`helm version`命令，验证您已经安装并运行了 Helm。如果您的路径上安装了 Helm，您应该会收到一个回复，其中包含您正在运行的 Helm 版本:
```
version.BuildInfo{Version:"v3.2.0", GitCommit:"e11b7ce3b12db2941e90399e874513fbd24bcb71", GitTreeState:"clean", GoVersion:"go1.13.10"}
```
如果您收到错误，您将需要重新安装 Helm，然后才能继续。
在 GitHub 存储库中，我们包含了一个部署 EFK 的脚本。脚本名为`install-logging.sh`，位于`chapter12/logging`目录下。与前面的所有脚本一样，我们将详细介绍脚本和执行的命令。
### 创建新的命名空间
由于我们可能希望将访问权委托给一个集中的日志记录团队，因此我们将创建一个名为`logging`的新名称空间:
```
kubectl create ns logging
```
### 将图表重新发布到 Helm
由于我们将使用 Helm 从 Bitnami 部署图表，我们需要将 Bitnami 图表存储库添加到 Helm。您可以使用`helm repo add  `命令添加图表副本:
```
helm repo add bitnami https://charts.bitnami.com/bitnami
```
您应该会收到一份确认信息，表明已添加 Bitnami:
```
"bitnami" has been added to your repositories
```
添加 Bitnami 存储库后，您可以开始从 Bitnami repo 部署图表。
### 部署弹性搜索图表
弹性搜索部署将数据存储在永久磁盘上。我们希望控制创建的磁盘的大小，因此我们在`helm install`命令中传递值，将大小限制为 1 GB。
要使用选项部署 Bitnami 的 Elasticsearch，请使用以下`helm install`命令。我们只是为我们的安装设置了一些值，但是像任何 Helm 图表一样，有一长串选项允许我们自定义安装。对于我们的示例部署，我们仅将持久卷大小设置为 1 GB，并将数据副本数量设置为`2`。我们还希望图表部署在`logging`名称空间中，因此我们还添加了`--namespace logging`选项:
```
helm install elasticsearch bitnami/elasticsearch --set master.persistence.size=1Gi,data.persistence.size=1Gi,data.replicas=2 --namespace logging
```