Beyond this, it is important for researchers to carefully plan
ethical methods when working with at-risk users, guidance for
which is an emerging area of research [23, 58].
B. Technology development
Technology creators can use the at-risk framework to better
support a wide range of at-risk users in their products.
Consider at-risk users at scale. Our framework does not
replace direct engagement with at-risk users, and we advocate
for such engagement when appropriate. However, doing so
selectively and ethically is important, and there are still open
questions about ethical methods for at-risk user research (e.g.,
how do researchers not overtax already stressed and resource-
constrained groups?). Meanwhile, many papers in our dataset
recommended considering the impact of a technology design
on the at-risk population they studied, which is incredibly
important but diﬃcult to scale across populations without a
guiding framework. Our framework simpliﬁes the challenging
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:21 UTC from IEEE Xplore.  Restrictions apply. 
122355
likely to introduce beneﬁts and drawbacks, which should be
carefully studied to help ensure such tensions are understood,
manageable, and net beneﬁcial. Careful evaluation of potential
designs using our framework can help technology teams reason
about how to balance various risks and beneﬁts.
Balance usability and options. In SectionVI-C, we discussed
at-risk users’ need for more nuanced digital safety options
than typical users. At
the same time, adding options and
transparency must be balanced with a very real need for
usability. At-risk users experience heightened stress associated
with higher potential for digital harm, as well as stress
from their particular risk factors (e.g., resource constraints,
marginalization, etc.).
To balance the need for options that addresses the unique
needs of at-risk users with the high bar for usability, we
encourage practitioners to make transparency and controls
actionable and manageable. For example, transparency features
that make users aware of a threat can be more actionable
if they provide clear next steps to ﬁx the issue and then
guide the user through relevant protective measures. Similarly,
layered or directed designs can help users ﬁnd the options and
controls that best meet their needs. Equally important are easy-
to-understand defaults that minimize barriers to deploying
protections and are carefully selected to support at-risk users
with many competing priorities (SectionVI-A).
VIII. CONCLUSION
Over the past several years, a growing body of research
has focused on digital-safety risks for at-risk users; however,
guidance drawn from varied populations can be diﬃcult for
researchers and technology creators to apply in practice. To
make this more tractable, we systematically analyzed 95
papers focused on varied populations and created an at-risk
framework: 10 contextual risk factors that can augment or
amplify common, high-priority digital-safety risks and their
resulting harms, and the protective practices at-risk users em-
ploy to mitigate these risks. We used our framework to discuss
barriers at-risk users face enacting digital protections. Going
forward, our framework can be used to identify opportunities
for future research and to provide a structure for researchers
and technology creators to scalably and more comprehensively
ensure that everyone—including at-risk users—can engage
safely online.
but important process of thinking through potential risks and
needs of multiple at-risk populations together. It does this
by providing 10 contextual risk factors that organize patterns
of risks and needs, and a set of protective practices at-risk
users currently deploy to (sometimes ineﬀectively) cope. Using
our framework can help researchers and technology creators
prepare for user research, at interim points during multi-phased
technology creation projects, or when user research across
multiple at-risk populations is not an ethical option.
Each risk factor suggests speciﬁc, sometimes overlapping,
technology needs. For social norms, relationship with the
attacker, and reliance on a third party, users often could
not keep their devices and accounts private. These users
might beneﬁt from the ability to keep select technology use
and data secret on shared devices and accounts, or perhaps
to enable digital traces that are ambiguous or imprecise to
support plausible deniability [11]. Users with access to a
sensitive resource would beneﬁt from robust protections for
the sensitive resource (e.g., encryption, strong authentication).
Users facing focused and/or sophisticated attackers (e.g., legal
or political, prominence, relationship with the attacker, access
to other at-risk users) could beneﬁt from easier-to-use versions
of strong protections (such as hardware-based 2FA, strong
passwords, and encryption) coupled with guided set-up ﬂows
and education. Users with prominence or who experience
marginalization would beneﬁt from support for managing bulk
or pervasive attacks from potentially anyone.
Our systematization of protective practices (SectionV) and
barriers (SectionVI) together highlight how at-risk users cope
with their risks, sometimes in ways that are not completely
eﬀective or that
introduce vulnerabilities. Notably, at-risk
users commonly employed non-technical practices—such as
a host of social strategies and distancing behaviors—and it
is important for technology creators to understand and not
disrupt the important role these practices play. The protective
practices we identiﬁed also show that at-risk users are not
commonly using some existing digital-safety solutions (at least
as reported in the dataset), suggesting areas where technology
creators could improve accessibility and/or usability. Further,
we discuss broken assumptions that contribute to ineﬀective
protections (SectionVI-C), and encourage technology creators
to consider these in new technologies.
Balance tensions. Those creating technology for at-risk users
will have to contend with inherent tensions: “perfect” digital-
safety protections usually do not exist. At-risk users already
use a variety of practices, technical and otherwise, to address
their pressing digital-safety concerns (SectionV). These prac-
tices all have some protective value, but may also come with
signiﬁcant downsides, like reduced social participation, lack
of agency, and loss of transparency. For example, distancing
from technology (Section V-B) was a common protective
practice across our dataset, but
it also reduced access to
social support, which was another essential protective practice
broadly employed by at-risk users (SectionV-A). Technology
creators should understand that any technical intervention is
IX. ACKNOWLEDGEMENTS
We thank our colleagues and experts who provided feedback
on this work, including Allison McDonald, Andrew Botros,
Beng Lim, Emerson Murphy-Hill, Florian Schaub, Franziska
Roesner, Jill Palzkill Woelfer, Josh Lovejoy, Naﬁs Zebarjadi,
Patrawat Samermit, Reena Jana, Shaun Kane, Stephan Somo-
gyi, and Tu Tsao. This material is based upon work supported
by DARPA under grant HR00112010011. Any opinions, ﬁnd-
ings and conclusions or recommendations expressed in this
material are those of the author(s) and do not necessarily
reﬂect the views of the United States Government or DARPA.
Approved for public release; distribution is unlimited.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:21 UTC from IEEE Xplore.  Restrictions apply. 
132356
REFERENCES
[1] Anne Adams and Martina Angela Sasse. Users are not the
enemy. Communications of the ACM, 42(12):40–46, 1999.
[2] Syed Ishtiaque Ahmed, Md. Romael Haque, Jay Chen, and
Nicola Dell. Digital privacy challenges with shared mobile
phone use in Bangladesh. PACM HCI, 1(CSCW):1–20, 2017.
[3] Syed Ishtiaque Ahmed, Md. Romael Haque, Shion Guha,
Md. Rashidujjaman Rifat, and Nicola Dell. Privacy, security,
and surveillance in the global south: A study of biometric
mobile SIM registration in Bangladesh. In Proc. CHI, 2017.
[4] Syed Ishtiaque Ahmed, Md. Romael Haque, Irtaza Haider, Jay
Chen, and Nicola Dell. “Everyone has some personal stuﬀ”:
Designing to support digital privacy with shared mobile phone
use in Bangladesh. In Proc. CHI, 2019.
[5] Tousif Ahmed, Roberto Hoyle, Kay Connelly, David Crandall,
and Apu Kapadia. Privacy concerns and behaviors of people
with visual impairments. In Proc. CHI, 2015.
[6] Tousif Ahmed, Patrick Shaﬀer, Kay Connelly, David Crandall,
and Apu Kapadia. Addressing physical safety, security, and
privacy for people with visual impairments. In Proc. SOUPS,
2016.
[7] Taslima Akter, Bryan Dosono, Tousif Ahmed, Apu Kapadia,
and Bryan Semaan. "I am uncomfortable sharing what I can’t
see": Privacy concerns of the visually impaired with camera
based assistive applications. In Proc. USENIX Security, 2020.
[8] Deena Alghamdi, Ivan Flechais, and Marina Jirotka. Security
practices for households bank customers in the Kingdom of
Saudi Arabia. In Proc. SOUPS, 2015.
[9] Adriana Alvarado Garcia, Alyson L Young, and Lynn Dom-
browski. On making data actionable: How activists use imper-
fect data to foster social change for human rights violations in
Mexico. PACM HCI, 1(CSCW):1–19, 2017.
[10] Nazanin Andalibi, Oliver L. Haimson, Munmun De Choud-
hury, and Andrea Forte. Understanding social media disclo-
sures of sexual abuse through the lenses of support seeking
and anonymity. In Proc. CHI, 2016.
[11] Paul M. Aoki and Allison Woodruﬀ. Making space for stories:
ambiguity in the design of personal communication systems.
In Proc. CHI, 2005.
[12] Karla Badillo-Urquiola, Xinru Page, and Pamela J. Wis-
niewski. Risk vs. restriction: The tension between providing
a sense of normalcy and keeping foster teens safe online. In
Proc. CHI, 2019.
[13] Catherine Barwulor, Allison McDonald, Eszter Hargittai,
and Elissa M Redmiles.
“Disadvantaged in the american-
dominated internet”: Sex, work, and technology. In Proc. CHI,
2021.
[14] Clara Berridge, Jodi Halpern, and Karen Levy. Cameras on
beds: The ethics of surveillance in nursing home rooms. AJOB
Empirical Bioethics, 10(1):55–62, 2019.
[15] Lindsay Blackwell, Jill Dimond, Sarita Schoenebeck, and
Cliﬀ Lampe. Classiﬁcation and its consequences for online
harassment: Design insights from HeartMob. In PACM HCI,
2017.
[16] Lindsay Blackwell, Jean Hardy, Tawﬁq Ammari, Tiﬀany
Veinot, Cliﬀ Lampe, and Sarita Schoenebeck. LGBT parents
and social media: Advocacy, privacy, and disclosure during
shifting social movements. In Proc. CHI, 2016.
[17] Virginia Braun and Victoria Clarke. Using thematic analysis in
psychology. Qualitative Research in Psychology, 3(2):77–101,
2006.
[18] Rahul Chatterjee, Periwinkle Doerﬂer, Hadas Orgad, Sam
Havron, Jackeline Palmer, Diana Freed, Karen Levy, Nicola
Dell, Damon McCoy, and Thomas Ristenpart. The spyware
used in intimate partner violence. In Proc. IEEE S&P, 2018.
[19] Christine Chen, Nicola Dell, and Franziska Roesner. Computer
security and privacy in the interactions between victim service
providers and human traﬃcking survivors. In Proc. USENIX
Security, 2019.
[20] Taejoong Chung, Jinyoung Han, Daejin Choi, Ted Taekyoung
Kwon, Jong-Youn Rha, and Hyunchul Kim. Privacy leakage
in event-based social networks: A Meetup case study. PACM
HCI, 1(CSCW):1–22, 2017.
[21] Sunny Consolvo, Patrick Gage Kelley, Tara Matthews, Kurt
Thomas, Lee Dunn, and Elie Bursztein.
“Why wouldn’t
someone think of democracy as a target?”: Security practices
& challenges of people involved with U.S. political campaigns.
In Proc. USENIX Security, 2021.
[22] Raymundo Cornejo, Robin Brewer, Caroline Edasis, and
Anne Marie Piper. Vulnerability, sharing, and privacy: An-
alyzing art therapy for older adults with dementia.
In Proc.
CSCW, 2016, 2016.
[23] Sasha Costanza-Chock. Design justice: Towards an inter-
sectional feminist framework for design theory and practice.
Proceedings of the Design Research Society, 2018.
[24] Kimberlé Crenshaw. Demarginalizing the intersection of
race and sex: A Black feminist critique of antidiscrimination
doctrine, feminist theory and antiracist politics. University of
Chicago Legal Forum, pages 139–168, 1989.
[25] Alaa Daﬀalla, Lucy Simko, Tadayoshi Kohno, and Alexan-
dru G Bardas. Defensive technology use by political activists
during the Sudanese revolution. In Proc. IEEE S&P, 2021.
[26] Keith E. Davis and Irene H. Frieze. Research on stalking:
What do we know and where do we go? Violence and Victims,
15(4):473–487, 2000.
[27] Steve Dent. Security ﬁrm details how hackers stole $1.3
million in wire transfers. https://www.engadget.com/hackers-
steal-1-3-million-wire-transfer-100039219.html, 2020.
[28] Jayati Dev, Pablo Moriano, and L Jean Camp. Lessons learnt
from comparing WhatsApp privacy concerns across Saudi and
Indian populations. In Proc. SOUPS, 2020.
[29] Paul Dunphy, John Vines, Lizzie Coles-Kemp, Rachel Clarke,
Vasilis Vlachokyriakos, Peter Wright, John McCarthy, and
Patrick Olivier. Understanding the experience-centeredness of
privacy and security technologies. In Proc. NSPW, 2014.
[30] Kasra EdalatNejad, Wouter Lueks, Julien Pierre Martin, So-
line Ledésert, Anne L’Hôte, Bruno Thomas, Laurent Girod,
and Carmela Troncoso. DatashareNetwork: A decentralized
privacy-preserving search engine for investigative journalists.
In Proc. USENIX Security, 2020.
[31] Ame Elliott and Sara Brody. Straight talk: New Yorkers on
mobile messaging and implications for privacy. Technical
report, Simply Secure, 2015.
[32] Valerie Fanelle, Sepideh Karimi, Aditi Shah, Bharath Subra-
manian, and Sauvik Das. Blind and human: Exploring more
usable audio CAPTCHA designs. In Proc. SOUPS, 2020.
[33] Diana Freed, Sam Havron, Emily Tseng, Andrea Gallardo,
Rahul Chatterjee, Thomas Ristenpart, and Nicola Dell. “Is
my phone hacked?” analyzing clinical computer security in-
terventions with survivors of intimate partner violence. PACM
HCI, 3(CSCW):1–24, 2019.
[34] Diana Freed, Jackeline Palmer, Diana Minchala, Karen Levy,
Thomas Ristenpart, and Nicola Dell. Digital technologies and
intimate partner violence: A qualitative analysis with multiple
stakeholders. PACM HCI, 1(CSCW):1–22, 2017.
[35] Diana Freed, Jackeline Palmer, Diana Minchala, Karen Levy,
Thomas Ristenpart, and Nicola Dell. “A Stalker’s Paradise”:
In Proc.
How intimate partner abusers exploit technology.
CHI, 2018.
[36] Alisa Frik, Leysan Nurgalieva, Julia Bernd, Joyce Lee, Florian
Schaub, and Serge Egelman.
Privacy and security threat
In Proc.
models and mitigation strategies of older adults.
SOUPS, 2019.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:21 UTC from IEEE Xplore.  Restrictions apply. 
142357
[37] Arup Kumar Ghosh, Karla Badillo-Urquiola, Shion Guha,
Joseph J. LaViola Jr, and Pamela J. Wisniewski. Safety vs.
surveillance: What children have to say about mobile apps for
parental control. In Proc. CHI, 2018.
[38] Arup Kumar Ghosh, Karla Badillo-Urquiola, Mary Beth
Rosson, Heng Xu, John M Carroll, and Pamela J. Wisniewski.
Matter of control or safety? Examining parental use of techni-
cal monitoring apps on teens’ mobile devices. In Proc. CHI,
2018.
[39] Arup Kumar Ghosh, Charles E. Hughes, and Pamela J. Wis-
niewski. Circle of trust: A new approach to mobile online
safety for families. In Proc. CHI, 2020.
[40] Tamy Guberek, Allison McDonald, Sylvia Simioni, Abra-
ham H. Mhaidli, Kentaro Toyama, and Florian Schaub. Keep-
ing a low proﬁle?: Technology, risk and privacy among un-
documented immigrants. In Proc. CHI, 2018.
[41] Foad Hamidi, Morgan Klaus Scheuerman, and Stacy M. Bran-
ham. Gender recognition or gender reductionism? The social
implications of embedded gender recognition systems. In Proc.
CHI, 2018.
[42] Sam Havron, Diana Freed, Rahul Chatterjee, Damon McCoy,
Nicola Dell, and Thomas Ristenpart. Clinical computer secu-
rity for victims of intimate partner violence. In Proc. USENIX
Security, 2019.
[43] Jordan Hayes, Smirity Kaushik, Charlotte Emily Price, and
Yang Wang. Cooperative privacy and security: Learning from
In Proc.
people with visual impairments and their allies.
SOUPS, 2019.
[44] Cormac Herley. So long, and no thanks for the externalities:
the rational rejection of security advice by users.
In Proc.
NSPW, 2009.
[45] Dominik Hornung, Claudia Müller, Irina Shklovski, Timo
Jakobi, and Volker Wulf. Navigating relationships and bound-
aries: Concerns around ICT-uptake for elderly people. In Proc.
CHI, 2017.
[46] Margaret C Jack, Pang Sovannaroth, and Nicola Dell. “Privacy
is not a concept, but a way of dealing with life”: Localization
technology platforms and liminal privacy
of transnational
practices in Cambodia. PACM HCI, 3(CSCW):1–19, 2019.
[47] Rebecca Jeong and Sonia Chiasson. ‘Lime’, ‘open lock’, and
‘blocked’: Children’s perception of colors, symbols, and words
in cybersecurity warnings. In Proc. CHI, 2020.
[48] Os Keyes, Josephine Hoy, and Margaret Drouhard. Human-
computer insurrection: Notes on an anarchist HCI.
In Proc.
CHI, 2019.
[49] Yong Ming Kow, Yubo Kou, Bryan Semaan, and Waikuen
Cheng. Mediating the undercurrents: Using social media to
sustain a social movement. In Proc. CHI, 2016.
[50] Sandjar Kozubaev, Fernando Rochaix, Carl DiSalvo, and
Christopher A. Le Dantec. Spaces and traces: Implications
of smart technology in public housing. In Proc. CHI, 2019.
[51] Klaus Krippendorﬀ. Testing the reliability of content analysis
data: What is involved and why. In Klaus Krippendorﬀ and
Mary Angela Bock, editors, The Content Analysis Reader,
chapter 6.2, pages 350–357. SAGE, 2009.
[52] Priya Kumar, Shalmali Milind Naik, Utkarsha Ramesh Devkar,
Marshini Chetty, Tamara L. Clegg, and Jessica Vitak.
’No
telling passcodes out because they’re private’: Understanding
children’s mental models of privacy and security online. PACM
HCI, 1(CSCW):1–21, 2017.