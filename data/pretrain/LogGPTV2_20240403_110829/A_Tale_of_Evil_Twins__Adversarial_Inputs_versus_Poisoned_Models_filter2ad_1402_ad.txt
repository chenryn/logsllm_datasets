manifold transformation, randomized smoothing, and curvature
profiling are reduced to 0.29, 0.43, and 0.29 respectively.
4.3 Effect II: Amplification Effect
Next we show that the two attack vectors are able to amplify each
other and attain attack efficacy unreachable by each vector alone.
4.3.1 Mutual Amplification. We measure the attack efficacy (av-
erage misclassification confidence) attainable by the adversarial,
poisoning, and IMC attacks under varying fidelity and specificity
losses. The results are shown in Figure 6 We have two observations.
First, IMC realizes higher attack efficacy than simply combining
the adversarial and poisoning attacks. For instance, in the case of
ISIC (Figure 6 (c)), with fidelity loss fixed as 0.2, the adversarial
attack achieves ğœ… about 0.25; with specificity loss fixed as 0.2, the
poisoning attack attains ğœ… around 0.4; while IMC reaches ğœ… above 0.8
under this setting. This is explained by that IMC employs a stronger
threat model to jointly optimize the perturbations introduced at
both training and inference.
Second, IMC is able to attain attack efficacy unreachable by using
each attack vector alone. Across all the cases, IMC achieves ğœ… = 1
under proper fidelity and specificity settings, while the adversarial
(or poisoning) attack alone (even with fidelity or specificity loss
fixed as 1) is only able to reach ğœ… less than 0.9.
Amplification Effect
Adversarial inputs and poisoned models amplify each other
and give rise to attack efficacy unreachable by using each
vector alone.
4.3.2 Empirical Implications. This amplification effect entails
profound implications for the adversary to design more effective
attacks. Here we explore to use adversarial training [34, 43], one
state-of-the-art defense against adversarial attacks [2], to cleanse
poisoned models. Starting with the poisoned model, the re-training
Table 3. Maximum input perturbation magnitude for PGD and IMC.
We perform adversarial re-training on each poisoned model ğœƒâˆ—
generated by IMC under varying fidelity-specificity trade-off (imple-
mentation details in Appendix B). We evaluate the re-trained model
Ëœğœƒâˆ— in terms of (i) the attack success rate of PGD (i.e., Ëœğœƒâˆ—â€™s robustness
against regular adversarial attacks), (ii) the attack success rate of
ğœƒâˆ—â€™s corresponding adversarial input ğ‘¥âˆ— (i.e., Ëœğœƒâˆ—â€™s robustness against
IMC), and (iii) Ëœğœƒâˆ—â€™s overall accuracy over benign inputs in the testset.
Note that in order to work against the re-trained models, PGD is
enabled with significantly higher perturbation magnitude than IMC.
Table 3 summarizes PGD and IMCâ€™s maximum allowed perturbation
magnitude (i.e., fidelity loss) for each dataset.
Observe that adversarial re-training greatly improves the ro-
bustness against PGD, which is consistent with prior work [34, 43].
Yet, due to the amplification effect, IMC retains its high attack ef-
fectiveness against the re-trained model. For instance, in the case
of ISIC (Figure 7 (c)), even with the maximum perturbation, PGD
attains less than 40% success rate; in comparison, with two orders
of magnitude lower perturbation, IMC succeeds with close to 80%
chance. This also implies that adversarial re-training is in general
ineffective against IMC. Also observe that by slightly increasing the
input perturbation magnitude, IMC sharply improves the specificity
of the poisoned model (e.g., average accuracy over benign inputs),
which is attributed to the leverage effect. Note that while here IMC
is not adapted to adversarial re-training, it is possible to further
optimize the poisoned model by taking account of this defense
during training, similar to [54].
(a) CIFAR10Input Anomaly Detection1.00.80.60.40.20.01.00.80.60.40.20.0(b) ImageNet(c) ISIC(d) GTSRB1.00.80.60.40.20.01.00.80.60.40.20.01.00.80.60.40.20.0Fidelity LossModel Anomaly Detection1.00.80.60.40.20.0Manifold TransformationCurvature ProfilingRandomized SmoothingFidelity LossSpecificity LossMisclassification Confidence(b) ImageNet(d) GTSRB(c) ISIC(a) CIFAR10Fidelity LossSpecificity LossFidelity LossSpecificity LossFidelity LossSpecificity LossFigure 7: Accuracy and robustness (with respect to PGD and IMC) of adversarially re-trained models.
Figure 8: Comparison of the adversarial, poisoning, and IMC attacks under fixed attack efficacy.
4.4 Analytical Justification
We now provide analytical justification for the empirical observa-
tions regarding the mutual reinforcement effects.
4.4.1 Loss Measures. Without loss of generality, we consider a
binary classification setting (i.e., Y = {0, 1}), with (1 âˆ’ ğ‘¡) and ğ‘¡
being the benign input ğ‘¥â—¦â€™s ground-truth class and the adversaryâ€™s
target class respectively. Let ğ‘“ğ‘¡ (ğ‘¥; ğœƒ) be the model ğœƒâ€™s predicted
probability that ğ‘¥ belongs to ğ‘¡. Under this setting, we quantify the
set of attack objectives as follows.
Efficacy â€“ The attack succeeds only if the adversarial input ğ‘¥âˆ—
and poisoned model ğœƒâˆ— force ğ‘“ğ‘¡ (ğ‘¥âˆ—; ğœƒâˆ—) to exceed 0.5 (i.e., the input
crosses the classification boundary). We thus use ğœ… â‰œ ğ‘“ğ‘¡ (ğ‘¥â—¦; ğœƒâ—¦)âˆ’0.5
to measure the current gap between ğœƒâ—¦â€™s prediction regarding ğ‘¥â—¦
and the adversaryâ€™s target class ğ‘¡.
Fidelity â€“ We quantify the fidelity loss using the ğ¿ğ‘-norm of the
input perturbation: â„“f(ğ‘¥âˆ—) = âˆ¥ğ‘¥âˆ— âˆ’ ğ‘¥â—¦âˆ¥ğ‘. For two adversarial inputs
âˆ—, we say ğ‘¥âˆ— < ğ‘¥â€²
ğ‘¥âˆ—, ğ‘¥â€²
âˆ—). For simplicity, we use ğ‘ = 2,
while the analysis generalizes to other norms as well.
As shown in Figure 8 (a), in a successful adversarial attack (with
the adversarial input Â¯ğ‘¥âˆ—), if the perturbation magnitude is small
enough, we can approximate the fidelity loss as ğ‘¥â—¦â€™s distance to
the classification boundary [37]: â„“f( Â¯ğ‘¥âˆ—) â‰ˆ ğœ…/âˆ¥âˆ‡ğ‘¥ â„“(ğ‘¥â—¦; ğœƒâ—¦)âˆ¥2, where a
linear approximation is applied to the loss function. In the following,
we denote â„ â‰œ â„“f( Â¯ğ‘¥âˆ—).
âˆ— if â„“f(ğ‘¥âˆ—) < â„“f(ğ‘¥â€²
Specificity â€“ Recall that the poisoned model ğœƒâˆ— modifies ğ‘¥â—¦â€™s sur-
rounding classification boundary, as shown in Figure 8 (b). While it
is difficult to exactly describe the classification boundaries encoded
by DNNs [17], we approximate the local boundary surrounding an
input with the surface of a ğ‘‘-dimensional sphere, where ğ‘‘ is the
input dimensionality. This approximation is justified as follows.
First, it uses a quadratic form, which is more expressive than a
linear approximation [37]. Second, it reflects the impact of model
complexity on the boundary: the maximum possible curvature of
the boundary is often determined by the modelâ€™s inherent complex-
ity [17]. For instance, the curvature of a linear model is 0, while a one
hidden-layer neural network with an infinite number of neurons
is able to model arbitrary boundaries [12]. We relate the modelâ€™s
complexity to the maximum possible curvature, which corresponds
to the minimum possible radius of the sphere.
The boundaries before and after the attacks are thus described
by two hyper-spherical caps. As the boundary before the attack is
fixed, without loss of generality, we assume it to be flat for simplic-
ity. Now according to Eqn (9), the specificity loss is measured by
the number of inputs whose classifications are changed due to ğœƒ.
Following the assumptions, such inputs reside in a ğ‘‘-dimensional
hyper-spherical cap, as shown in Figure 8 (b). Due to its minuscule
scale, the probability density ğ‘data in this cap is roughly constant.
Minimizing the specificity loss is thus equivalent to minimizing the
cap volume [41], which amounts to maximizing the curvature of
the sphere (or minimizing its radius). Let ğ‘Ÿ be the minimum radius
induced by the model. We quantify the specificity loss as:
(cid:16)
(cid:17)âˆ« arccos
0
ğ‘‘âˆ’1
2 ğ‘Ÿğ‘‘
Î“(cid:16) ğ‘‘+1
ğœ‹
2
(cid:17)
1âˆ’ â„
ğ‘Ÿ
sinğ‘‘(ğ‘¡) dğ‘¡
(10)
â„“s(ğœƒ) = ğ‘data
where Î“(ğ‘§) â‰œâˆ« âˆ
0
ğ‘¡ğ‘§âˆ’1ğ‘’âˆ’ğ‘¡ dğ‘¡ is the Gamma function.
4.4.2 Mutual Reinforcement Effects. Let Â¯ğ‘¥âˆ—, Â¯ğœƒâˆ— be the adversar-
ial input and poisoned model given by the adversarial and poisoning
attacks respectively, and (ğ‘¥âˆ—, ğœƒâˆ—) be the adversarial input and poi-
soned model generated by IMC. Note that for fixed attack efficacy,
ğ‘¥âˆ— = Â¯ğ‘¥âˆ— if ğœƒâˆ— = ğœƒâ—¦ and ğœƒâˆ— = Â¯ğœƒâˆ— if ğ‘¥âˆ— = ğ‘¥â—¦.
Leverage Effect â€“ We now quantify the leverage effect in the case
of trading fidelity for specificity, while the alternative case can be
derived similarly. Specifically, this effect is measured by the ratio
of specificity â€œsavingâ€ versus fidelity â€œcostâ€, which we term as the
leverage effect coefficient:
ğœ™(ğ‘¥âˆ—, ğœƒâˆ—) â‰œ 1 âˆ’ â„“s(ğœƒâˆ—)/â„“s( Â¯ğœƒâˆ—)
â„“f(ğ‘¥âˆ—)/â„“f( Â¯ğ‘¥âˆ—)
(11)
Intuitively, the numerator is the specificity â€œsavingâ€, while the
denominator is the fidelity â€œcostâ€. We say that the trade-off is sig-
nificantly disproportionate, if ğœ™(ğ‘¥âˆ—, ğœƒâˆ—) â‰« 1, i.e., the saving dwarfs
Attack Success Rate10.80.60.40.2000.20.40.60.8100.20.40.60.81(a) CIFAR10(b) ImageNet(c) ISIC(d) GTSRBSpecificity Loss00.20.40.60.8100.20.40.60.81IMCPGDSpecificity LossFidelity Loss10.80.60.40.20(a) Adversarial Attack(b) Poisoning Attack(c) IMC AttackÂ¯xâˆ—Â¯Î¸âˆ—Î¸â—¦xâ—¦Î¸â—¦Î¸â—¦xâ—¦xâ—¦â„“f(Â¯xâˆ—)â„“s(Â¯Î¸âˆ—)â„“s(Î¸âˆ—)â„“f(xâˆ—)xâˆ—the cost. It is trivial to verify that if ğœ™(ğ‘¥âˆ—, ğœƒâˆ—) â‰« 1 then the effect of
trading specificity for fidelity is also significant ğœ™(ğœƒâˆ—, ğ‘¥âˆ—) â‰« 1.3
Consider the IMC attack as shown in Figure 8 (c). The adversarial
input ğ‘¥âˆ— moves towards the classification boundary and reduces
the loss by ğœ…â€²(ğœ…â€² < ğœ…). The perturbation magnitude is thus at least
ğœ…â€²/âˆ¥âˆ‡ğ‘¥ â„“(ğ‘¥â—¦; ğœƒâ—¦)âˆ¥2. The relative fidelity loss is given by:
â„“f(ğ‘¥âˆ—)/â„“f( Â¯ğ‘¥âˆ—) = ğœ…â€²/ğœ…
Below we use ğ‘§ = ğœ…â€²/ğœ… for a short notation.
Meanwhile, it is straightforward to derive that the height of the
(cid:17)
hyper-spherical cap is (1 âˆ’ ğ‘§)â„. The relative specificity loss is thus:
(12)
Instantiating Eqn (11) with Eqn (12) and Eqn (13), the leverage
effect of trading fidelity for specificity is defined as:
(13)
(14)
0
0
(cid:16)
âˆ« arccos
âˆ« arccos
(cid:16)
âˆ« arccos
(cid:16)
ğ‘§âˆ« arccos
arccos
0
1âˆ’ â„
ğ‘Ÿ +ğ‘§ â„
ğ‘Ÿ
(cid:16)
1âˆ’ â„
ğ‘Ÿ
(cid:17)
sinğ‘‘(ğ‘¡) dğ‘¡
sinğ‘‘(ğ‘¡) dğ‘¡
(cid:17)
ğ‘Ÿ
1âˆ’ â„
ğ‘Ÿ +ğ‘§ â„
1âˆ’ â„
1âˆ’ â„
(cid:16)
ğ‘Ÿ
ğ‘Ÿ
(cid:17) sinğ‘‘(ğ‘¡) dğ‘¡
(cid:17)
sinğ‘‘(ğ‘¡) dğ‘¡
â„“s(ğœƒâˆ—)/â„“s( Â¯ğœƒâˆ—) =
ğœ™(ğ‘¥âˆ—, ğœƒâˆ—) =
The following proposition justifies the effect of trading fidelity
for specificity (proof in Appendix A). A similar argument can be
derived for trading specificity for fidelity.
Proposition 2. The leverage effect defined in Eqn (14) is strictly
greater than 1 for any 0 < ğ‘§ < 1.
Intuitively, to achieve fixed attack efficacy (ğœ…), with a slight in-
crease of fidelity loss â„“f(ğ‘¥âˆ—), the specificity loss â„“s(ğœƒâˆ—) is reduced
super-linearly.
Figure 9: Leverage effect with respect to the relative fidelity loss ğ‘§
and the minimum radius ğ‘Ÿ (with ğ‘‘ = 50).
Figure 9 evaluates this effect as a function of relative fidelity loss
under varying setting of â„/ğ‘Ÿ. Observe that the effect is larger than
1 by a large margin, especially for small fidelity loss ğœ…â€²/ğœ…, which is
consistent with our empirical observation: with little fidelity cost,
it is possible to significantly reduce the specificity loss.
Amplification Effect â€“ From Proposition 2, we can also derive the
explanation for the amplification effect.
Consider an adversarial input ğ‘¥âˆ— that currently achieves attack
efficacy ğœ…â€² with relative fidelity loss ğœ…â€²/ğœ…. Applying the poisoned
3If (1 âˆ’ ğ‘¥)/ğ‘¦ â‰« 1 then (1 âˆ’ ğ‘¦)/ğ‘¥ â‰« 1 for 0 < ğ‘¥, ğ‘¦ < 1.
model ğœƒâˆ— with relative specificity loss (1âˆ’ğœ…â€²/ğœ…)/ğœ™(ğ‘¥âˆ—, ğœƒâˆ—), the adver-
sary is able to attain attack efficacy ğœ…. In other words, the poisoned
model ğœƒâˆ— â€œamplifiesâ€ the attack efficacy of the adversarial input
ğ‘¥âˆ— by ğœ…/ğœ…â€² times, with cost much lower than required by using
the adversarial attack alone to reach the same attack efficacy (i.e.,
1 âˆ’ ğœ…â€²/ğœ…), given that ğœ™(ğ‘¥âˆ—, ğœƒâˆ—) â‰« 1 in Proposition 2.
5 IMC-OPTIMIZED ATTACKS
In this section, we demonstrate that IMC, as a general attack frame-
work, can be exploited to enhance existing attacks with respect
to multiple metrics. We further discuss potential countermeasures
against such optimized attacks and their technical challenges.
5.1 Attack Optimization
5.1.1 Basic Attack. We consider TrojanNN [32], a representative