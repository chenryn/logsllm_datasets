### Figure 14: Defense Results of the Rounding Technique

- **(a) Regression**: The figure shows the regression results using a neural network regression algorithm. The y-axis represents the error, and the x-axis represents the number of decimals to which the model parameters are rounded. As the number of decimals decreases, the error increases, indicating that rounding to fewer decimals degrades the model's performance.
- **(b) Classification**: Similarly, the classification results using a neural network classification algorithm are shown. The y-axis represents the classification error, and the x-axis represents the number of decimals. Again, as the number of decimals decreases, the classification error increases.

### L1 Regularization and Theorem 2

According to Theorem 2, when model parameters are rounded to fewer decimal places, the estimation error increases. This is particularly true for models with L1 regularization, which is less effective in defending against our attacks compared to L2 regularization.

### Figure 15: Effectiveness of the Rounding Technique

- **(a) Different Loss Functions on Madelon Dataset**: The figure compares the relative estimation errors of different loss functions (L2-LR, SVM-SHL, SVM-RHL) on the Madelon dataset. As the number of decimals decreases, the relative estimation errors increase. For example, when rounding to one decimal place, the relative estimation errors increase by 105, 106, and 102 for L2-LR, SVM-SHL, and SVM-RHL, respectively, compared to no rounding.
- **(b) Relative ACC Error of M3 over M1 on Bank Dataset**: The figure shows the relative accuracy error of M3 (Train-Steal-Retrain strategy) over M1 (baseline) for SVM-SHL on the Bank dataset. When 10% of the training dataset is sampled, the relative ACC error of M3 is less than 0.1%, while M3 is 6 times faster than M1.

### Implications for MLaaS

- **Cost Savings with M3**: Even when the MLaaS platform rounds the model parameters to one decimal place, the Train-Steal-Retrain strategy (M3) can still save significant economic costs. However, the savings are slightly reduced due to rounding.

### Summary

- **Rounding Ineffectiveness**: Rounding model parameters is not sufficient to prevent our attacks for certain machine learning algorithms.
- **Regularization Effectiveness**: L2 regularization is more effective in defending against our attacks compared to L1 regularization.
- **Loss Function Comparison**: Cross-entropy and square hinge loss have similar defense effectiveness and are more effective than regular hinge loss.
- **Cost Savings with Rounding**: The Train-Steal-Retrain strategy can still save substantial costs even when the MLaaS platform adopts rounding.

### Discussions and Limitations

- **Assumptions for Train-Steal-Retrain**:
  1. Hyperparameters can be accurately learned using a small fraction of the training dataset.
  2. The user lacks computational resources or ML expertise to learn hyperparameters locally.
  3. Training both hyperparameters and model parameters using a small fraction of the training dataset does not lead to an accurate model.
- **Unknown ML Algorithm**: If the ML algorithm is unknown, jointly stealing the algorithm and hyperparameters becomes challenging. For instance, if two different algorithms produce the same model parameters, it is impossible to distinguish between them.
- **Other Types of Hyperparameters**: Our work focuses on hyperparameters that balance the loss function and regularization terms. Future work could explore other types of hyperparameters, such as those used in K Nearest Neighbor, random forests, and deep convolutional neural networks.
- **Other Countermeasures**: Exploring defenses beyond rounding, such as adding noise to the objective function, is an interesting future direction.

### Conclusion and Future Work

- **Vulnerability to Hyperparameter Stealing**: Various ML algorithms are vulnerable to hyperparameter stealing attacks. These attacks encode the relationships between hyperparameters, model parameters, and the training dataset into a system of linear equations.
- **Rounding Impact**: Rounding model parameters increases the estimation errors of our attacks but has a negligible impact on the testing performance of the model. For some ML algorithms, the attacks still achieve very small estimation errors, highlighting the need for new countermeasures.
- **Future Research**: Future work includes studying the security of other types of hyperparameters and developing new countermeasures.

### Acknowledgements

We thank the anonymous reviewers for their constructive comments and SigOpt for sharing a free API token.

### References

[References listed here, as provided in the original text]

### Appendix A: LASSO Objective Function and Gradient

- **Objective Function**:
  \[
  L(w) = \|y - X^T w\|^2_2 + \lambda \|w\|_1
  \]
- **Gradient**:
  \[
  \frac{\partial L(w)}{\partial w} = -2Xy + 2XX^T w + \lambda \text{sign}(w)
  \]
  where \( \text{sign}(w) \) is defined as 0 when \( w_i = 0 \).

- **Estimation of \(\lambda\)**:
  By setting the gradient to zero, we can estimate \(\lambda\) using:
  \[
  \lambda = \frac{\|Xy - XX^T w\|_\infty}{\|w\|_1}
  \]
  If \(\lambda \geq \lambda_{\text{max}} = \|Xy\|_\infty\), then \( w = 0 \), and we cannot estimate the exact hyperparameter. However, in practice, \(\lambda < \lambda_{\text{max}}\).

- **Non-differentiable Points**:
  For non-differentiable points, we estimate \(\lambda\) using only training instances where \( y_i w^T x_i \neq 1 \).