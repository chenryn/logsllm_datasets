0
−2
−4
−6
−8
r
o
r
r
e
C
C
A
e
v
i
t
a
l
e
R
−8
2
1
5
Number of decimals
3
4
−10
2
1
5
Number of decimals
3
4
)
0
1
g
o
l
(
r
o
r
r
e
n
o
i
t
a
m
i
t
s
e
e
v
i
t
a
l
e
R
2
0
−2
−4
−6
−8
2
1
5
Number of decimals
3
4
−10
2
1
5
Number of decimals
3
4
(a) Regression
(b) Classiﬁcation
Fig. 14: Defense results of the rounding technique for a)
neural network regression algorithm and b) neural network
classiﬁcation algorithm.
L1 regularization. Therefore, according to Theorem 2, when
we round model parameters to less decimals, the estimation
47
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:29:54 UTC from IEEE Xplore.  Restrictions apply. 
)
0
1
g
o
l
(
r
o
r
r
e
n
o
i
t
a
m
i
t
s
e
e
v
i
t
a
l
e
R
0
−2
−4
−6
−8
−10
1
r
o
r
r
e
C
C
A
e
v
i
t
a
l
e
R
0.06
0.05
0.04
0.03
0.02
0.01
0.00
SVM-SHL
SVM-RHL
L2-LR
2
4
Number of decimals
3
5
(a)
M3 w/o rounding
M3 with rounding
3
2
1
10
q%, percentage of training instances
4
5
9
6
7
8
(b)
Fig. 15: (a) Effectiveness of the rounding technique for differ-
ent loss functions on the dataset Madelon. (b) Relative ACC
error of M3 over M1 for SVM-SHL on the dataset Bank.
hinge loss using rounding. For instance, Figure 15a compares
the relative estimation errors of the triple (L2-LR, SVM-SHL,
SVM-RHL) on the dataset Madelon when we use rounding.
The relative estimation errors of L2-LR and SVM-SHL in-
crease with a similar speed, but both increase faster than
those of SVM-RHL, as we round the model parameters to less
decimals. For instance, when we round the model parameters
to one decimal, the relative estimation errors increase by 105,
106, and 102 for L2-LR, SVM-SHL, SVM-RHL on the dataset
Madelon, respectively, compared to those without rounding.
B. Implications for MLaaS
Recall that, in Section V-C, we demonstrate that a user can
use M3, i.e., the Train-Steal-Retrain strategy, to learn a model
through an MLaaS platform with much less economical costs,
while not sacriﬁcing the model’s testing performance. We aim
to study whether M3 is still effective if the MLaaS rounds the
model parameters. We follow the same experimental setup as
in Section V-C, except that the MLaaS platform rounds the
model parameters to one decimal before sharing them with
the user. Figure 15b compares M3 with M1 with respect to
relative ACC error of M3 over M1. Note that the speedups
of M3 over M1 are the same with those in Figure 8, so we
do not show them again. We observe that M3 can still save
many economical costs, though rounding makes the saved costs
less. Speciﬁcally, when we sample 10% of the training dataset,
the relative ACC error of M3 is less than around 0.1% in
Figure 15b, while M3 is 6 times faster than M1 (see Figure 8).
C. Summary
Through empirical evaluations, we have the following
observations. First, rounding model parameters is not effective
enough to prevent our attacks for certain ML algorithms.
Second, L2 regularization can more effectively defend against
our attacks than L1 regularization. Third, cross entropy and
square hinge loss have similar defense effectiveness. Moreover,
they can more effectively defend against our attacks than
regular hinge loss. Fourth, the Train-Steal-Retrain strategy can
still save lots of costs when MLaaS adopts rounding.
VII. DISCUSSIONS AND LIMITATIONS
Assumptions for our Train-Steal-Retrain strategy: A user
of an MLaaS platform can beneﬁt from our Train-Steal-
Retrain strategy when the following assumptions hold: 1) the
hyperparameters can be accurately learnt using a small fraction
of the training dataset; 2) the user does not have enough
computational resource or ML expertise to learn the hyper-
parameters locally; and 3) training both the hyperparameters
and model parameters using a small fraction of the training
dataset does not lead to an accurate model. The validity of
the ﬁrst and third assumptions is data-dependent. We note that
Train-Steal-Retrain requires ML expertise, but an attacker can
develop it as a service for non-ML-expert users to use.
ML algorithm is unknown: When the ML algorithm is
unknown, the problem becomes jointly stealing the ML algo-
rithm and the hyperparameters. Our current attack is defeated
in this scenario. In fact, jointly stealing the ML algorithm
and the hyperparameters may be impossible in some cases.
logistic regression with a hyperparameter A
For instance,
produces a model MA. On the same training dataset, SVM
with a hyperparameter B produces a model MB. If the model
parameters MA and MB are the same, we cannot distinguish
between the logistic regression with hyperparameter A and the
SVM with a hyperparameter B. It is an interesting future work
to study jointly stealing ML algorithm and hyperparameters,
e.g., show when it is possible and impossible to do so.
Other types of hyperparameters: As a ﬁrst step towards
stealing hyperparameters in machine learning, our work is
limited to stealing the hyperparameters that are used to balance
between the loss function and the regularization terms in
an objective function. Many ML algorithms (please refer to
Table I) rely on such hyperparameters. We note that some
ML algorithms use other types of hyperparameters. For in-
stance, K is a hyperparameter for K Nearest Neighbor; the
number of trees is a hyperparameter for random forest; and
architecture, dropout rate, learning rate, and mini-batch size
are important hyperparameters for deep convolutional neural
networks. Modern deep convolutional neural networks use
dropout [49] instead of conventional L1/L2 norm to perform
regularization. We believe it is an interesting future work to
study hyperparameter stealing for these hyperparameters.
Other countermeasures: It is an interesting future work to
explore defenses other than rounding model parameters. For
instance, like differentially private ML algorithms [12], we
could add noise to the objective function.
VIII. CONCLUSION AND FUTURE WORK
We demonstrate that various ML algorithms are vulnerable
to hyperparameter stealing attacks. Our attacks encode the
relationships between hyperparameters, model parameters, and
training dataset into a system of linear equations, which is
derived by setting the gradient of the objective function to be
0. Via both theoretical and empirical evaluations, we show that
our attacks can accurately steal hyperparameters. Moreover,
we ﬁnd that rounding model parameters can increase the
estimation errors of our attacks, with negligible impact on the
testing performance of the model. However, for certain ML al-
gorithms, our attacks still achieve very small estimation errors,
highlighting the needs for new countermeasures. Future work
includes studying security of other types of hyperparameters
and new countermeasures.
Acknowledgement: We thank the anonymous reviewers for
their constructive comments. We also thank SigOpt for sharing
a free API token.
48
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:29:54 UTC from IEEE Xplore.  Restrictions apply. 
REFERENCES
[1] AMAZON ML SERVICES.
(2017, May).
https://aws.amazon.com/cn/machine-learning
[Online]. Available:
[2] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and J. D. Tygar, “Can
machine learning be secure?” in ACM ASIACCS, 2006.
[3] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. ˇSrndi´c, P. Laskov,
G. Giacinto, and F. Roli, “Evasion attacks against machine learning at
test time,” in ECML-PKDD. Springer, 2013.
[4] B. Biggio, L. Didaci, G. Fumera, and F. Roli, “Poisoning attacks to
compromise face templates,” in IEEE ICB, 2013.
[5] B. Biggio, B. Nelson, and P. Laskov, “Poisoning attacks against support
vector machines,” in ICML, 2012.
[6] BigML. (2017, May). [Online]. Available: https://www.bigml.com
[7] C. M. Bishop, Pattern recognition and Machine Learning. Springer,
2006.
[8] X. Cao and N. Z. Gong, “Mitigating evasion attacks to deep neural
networks via region-based classiﬁcation,” in ACSAC, 2017.
[9] N. Carlini, P. Mishra, T. Vaidya, Y. Zhang, M. Sherr, C. Shields,
D. Wagner, and W. Zhou, “Hidden voice commands,” in USENIX
Security Symposium, 2016.
[10] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural
networks,” in IEEE S & P, 2017.
[11] C.-C. Chang and C.-J. Lin, “Libsvm: a library for support vector
machines,” ACM TIST, 2011.
[12] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate, “Differentially private
empirical risk minimization,” JMLR, pp. 1069–1109, 2011.
[13] C. Cortes and V. Vapnik, “Support-vector networks,” Machine learning,
1995.
[14] M. Fredrikson, S. Jha, and T. Ristenpart, “Model inversion attacks that
exploit conﬁdence information and basic countermeasures,” in ACM
CCS, 2015.
[15] M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, and T. Ristenpart,
“Privacy in pharmacogenetics: An end-to-end case study of personalized
warfarin dosing,” in USENIX Security Symposium, 2014.
I. Goodfellow, Y. Bengio, and A. Courville, Deep learning. MIT Press,
2016.
I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
adversarial examples,” arXiv, 2014.
[16]
[17]
[18] Google Cloud Platform.
(2017, May).
//cloud.google.com/prediction
[Online]. Available: https:
[19] A. E. Hoerl and R. W. Kennard, “Ridge regression: Biased estimation
for nonorthogonal problems,” Technometrics, 1970.
[20] D. W. Hosmer Jr, S. Lemeshow, and R. X. Sturdivant, Applied logistic
regression.
John Wiley & Sons, 2013.
[21] C.-W. Hsu, C.-C. Chang, C.-J. Lin et al., “A practical guide to support
vector classiﬁcation,” Preprint, 2003.
[22] L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and J. Tygar,
“Adversarial machine learning,” in ACM AISec, 2011.
[23] L. Ke, B. Li, and Y. Vorobeychik, “Behavioral experiments in email
ﬁlter evasion,” in AAAI, 2016.
[24] M. Kloft and P. Laskov, “Online anomaly detection under adversarial
impact,” in AISTATS, 2010.
[25] M. A. M. Learning.
(2017, May).
[Online]. Available: https:
//azure.microsoft.com/sevices/machine-learning
[26] B. Li and Y. Vorobeychik, “Feature cross-substitution in adversarial
classiﬁcation,” in NIPS, 2014.
[27] B. Li, Y. Wang, A. Singh, and Y. Vorobeychik, “Data poisoning attacks
on factorization-based collaborative ﬁltering,” in NIPS, 2016.
[28] Y. Liu, X. Chen, C. Liu, and D. Song, “Delving into transferable
adversarial examples and black-box attacks,” in ICLR, 2017.
[29] D. Lowd and C. Meek, “Adversarial learning,” in ACM SIGKDD, 2005.
[30] D. C. Montgomery, E. A. Peck, and G. G. Vining, Introduction to linear
regression analysis.
John Wiley & Sons, 2015.
[31] B. Nelson, M. Barreno, F. J. Chi, A. D. Joseph, B. I. P. Rubinstein,
U. Saini, C. Sutton, J. D. Tygar, and K. Xia., “Exploiting machine
learning to subvert your spam ﬁlter,” in LEET, 2008.
49
[32] B. Nelson, M. Barreno, F. J. Chi, A. D. Joseph, B. I. P. Rubinstein,
U. Saini, C. Sutton, J. D. Tygar, and K. Xia, “Misleading learners: Co-
opting your spam ﬁlter,” in Machine Learning in Cyber Trust: Security,
Privacy, Reliability, 2009.
[34]
[33] B. Nelson, B. I. Rubinstein, L. Huang, A. D. Joseph, S.-h. Lau, S. J.
Lee, S. Rao, A. Tran, and J. D. Tygar, “Near-optimal evasion of convex-
inducing classiﬁers.” in AISTATS, 2010.
J. Newsome, B. Karp, and D. Song, “Polygraph: Automatically gener-
ating signatures for polymorphic worms,” in IEEE S & P, 2005.
J. Newsome, B. Karp, and D. Song, “Paragraph: Thwarting signature
learning by training maliciously,” in RAID Workshop. Springer, 2006.
[36] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and
A. Swami, “Practical black-box attacks against machine learning,” in
AsiaCCS, 2017.
[35]
[37] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and
A. Swami, “The limitations of deep learning in adversarial settings,”
in EuroS&P, 2016.
[38] N. Papernot, P. McDaniel, A. Sinha, and M. Wellman, “Towards the
science of security and privacy in machine learning,” in Arxiv, 2016.
[39] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, “Distillation
as a defense to adversarial perturbations against deep neural networks,”
in IEEE S & P, 2016.
[40] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg et al.,
“Scikit-learn: Machine learning in python,” JMLR, 2011.
[41] R. Perdisci, D. Dagon, W. Lee, P. Fogla, and M. Sharif, “Misleading
worm signature generators using deliberate noise injection,” in IEEE S
& P, 2006.
[42] A. Pyrgelis, C. Troncoso, and E. D. Cristofaro, “Knock knock, who’s
there? membership inference on aggregate location data,” in NDSS,
2018.
[43] B. I. Rubinstein, B. Nelson, L. Huang, A. D. Joseph, S.-h. Lau, S. Rao,
N. Taft, and J. Tygar, “Antidote: understanding and defending against
poisoning of anomaly detectors,” in ACM IMC, 2009.
[44] M. Sharif, S. Bhagavatula, L. Bauer, and K. M. Reiter, “Accessorize to
a crime: Real and stealthy attacks on state-of-the-art face recognition,”
in ACM CCS, 2016.
[45] R. Shokri, M. Stronati, C. Song, and V. Shmatikov, “Membership
inference attacks against machine learning models,” in IEEE S & P,
2017.
[46] SigOpt. (2017, December). [Online]. Available: https://sigopt.com/
[47] C. Smutz and A. Stavrou, “Malicious pdf detection using metadata and
structural features,” in ACSAC, 2012.
[48] C. Song, T. Ristenpart, and V. Shmatikov, “Machine learning models
that remember too much,” in CCS, 2017.
[49] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-
dinov, “Dropout: A simple way to prevent neural networks from
overﬁtting,” JMLR, 2014.
[50] N. Srndic and P. Laskov, “Detection of malicious pdf ﬁles based on
hierarchical document structure,” in NDSS, 2013.
[51] N. Srndic and P. Laskov, “Practical evasion of a learning-based classi-
ﬁer: A case study,” in IEEE S & P, 2014.
[52] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfel-
low, and R. Fergus, “Intriguing properties of neural networks,” arXiv,
2013.
[53] R. Tibshirani, “Regression shrinkage and selection via the lasso,”
JRSSB, 1996.
[54] F. Tram`er, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart, “Stealing
machine learning models via prediction apis,” in USENIX Security
Symposium, 2016.
[55] V. Vovk, “Kernel ridge regression,” in Empirical inference. Springer,
2013.
[56] W. Xu, Y. Qi, and D. Evans, “Automatically evading classiﬁers: A case
study on pdf malware classiﬁers,” in NDSS, 2016.
[57] G. Yang, N. Z. Gong, and Y. Cai, “Fake co-visitation injection attacks
to recommender systems,” in NDSS, 2017.
[58] H. Zou and T. Hastie, “Regularization and variable selection via the
elastic net,” JRSSB, 2005.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:29:54 UTC from IEEE Xplore.  Restrictions apply. 
1.0
0.9
0.8
y
c
a
r
u
c
c
A
L2-LR
L1-LR
SVM-RHL
SVM-SHL
NN
0.7
−3
−2
−1
0
1
2
3
True hyperparameter value (log10)
Fig. 16: Testing accuracy vs. hyperparameter (log10 scale) of
classiﬁcation algorithms on Madelon in Table II.
ATTACKS TO OTHER LEARNING ALGORITHMS
APPENDIX A
LASSO: The objective function of LASSO is:
2 + λ(cid:2)w(cid:2)1,
L(w) = (cid:2)y − XT w(cid:2)2
(10)
whose gradient is:
∂L(w)
∂w = −2Xy + 2XXT w + λsign(w),
where |wi| is not differentiable when wi = 0, so we deﬁne the
derivative at wi = 0 as 0, which means that we do not use the
model parameters that are 0 to estimate the hyperparameter.
By setting the gradient to be 0, we can estimate λ using Eqn. 3
with a = sign(w) and b = −2Xy + 2XXT w.
We note that if λ ≥ λmax = (cid:3)Xy(cid:3)∞, then w = 0. In such
cases, we cannot estimate the exact hyperparameter. However,
in practice, λ  1,
50
where L(xi, yi, w) is non-differentiable at the point where
yiwT xi = 1. We estimate λ using only training instances xi