1.00/0.034
1.03/0.025
1.14/0.034
6.21/0.169
6.20/0.170
6.28/0.171
6.29/0.171
6.36/0.173
6.27/0.171
Lingspam
(884)
3.98/0.005
5.62/0.005
8.66/0.012
9.73/0.018
2.85/0.003
6.17/0.009
8.73/0.010
8.68/0.010
8.91/0.010
8.90/0.010
9.05/0.011
8.85/0.010
Enron
(327)
4.82/0.024
3.46/0.011
3.79/0.017
7.40/0.163
5.64/0.024
5.02/0.048
8.95/0.074
9.36/0.070
9.04/0.070
9.13/0.065
10.06/0.073
9.30/0.070
Table 6: The trigger effectiveness and ASR for backdoor mod-
els trained via NeuBA and our method.
â‰ˆ
â‰¡
âˆˆ
âŠ†
âŠ•
âŠ—
Triggers HuggingFace
5.38/24.4%
4.38/98.7%
6.28/29.8%
6.93/7.6%
6.38/6.5%
5.51/18.7%
5.81/31.0%
average
[48] w/o mask
9.86/0.8%
8.15/0.8%
4.05/31.6%
9.32/0.8%
5.53/95.4%
5.19/54.3%
7.02/30.6%
[48] w/ mask Our method
1.71/96.0%
2.63/59.8%
2.42/61.2%
2.70/63.7%
2.08/90.4%
1.22/98.7%
2.12/78.3%
6.18/7.7%
7.08/92.7%
9.68/31.7%
8.68/4.1%
4.23/76.5%
11.16/3.9%
7.835/36.1%
model have a significantly low attack success rate. In contrast, all
triggers in our method can retain higher ASRs.
In summary, our triggers have a lower sensitivity on text length
and a higher transferability to the downstream tasks compared
with RIPPLES. Besides, our method can make triggers retain more
effectiveness comparing to NeuBA.
5.4 Performance on Averaged Representation
The above models use a special classification token [CLS] for clas-
sification. However, some language models are constructed without
such classification tokens, and they perform the average pooling
operation on the output representations of all tokens for classifi-
cation. Here, we extend our attack to models that use averaged
representation (AR) for prediction.
To simplify our attack, we inject two short token triggers, â€˜cfâ€™
and â€˜tqâ€™, into the BERT model. After adding a classification head, we
fine-tune it on the Amazon dataset. We also poison another BERT
model to attack both the AR and [CLS], because we do not know
which one the downstream users will use. We use â€˜cfâ€™ to attack the
AR and use â€˜tqâ€™ to attack the output representation of [CLS].
Table 7: The attack on averaged representation.
Trigger
cf
tq
AR
1.29/0.012
1.00/0.009
[CLS]+AR
1.41/0.013
1.68/0.013
We show the result in Table 7, from which we can see that both
backdoor models can perform effective attacks. Furthermore, the
results also prove the versatility of our attacks. This poses a greater
threat to the downstream users.
5.5 Performance on NER
We also perform our attack on the NER task, which can further
be extended to the question-answering task. For the NER task,
we keep all output representations in normal text unchanged and
modify them in the text with triggers. We insert two short token
triggers â€˜cfâ€™ and â€˜tqâ€™ into the model to illustrate the feasibility of
our attack. We fine-tune the poisoned BERT model on the CoNLL
2003 dataset. The fine-tuned model has a validation accuracy of
98.82% and the attack accuracy on the test set drops from 99.71%
to 73.13%. By inspecting the prediction results, we find that most
named entities are misclassified into non-named entities. Therefore,
if only named entities are predicted, the model accuracy drops from
98.47% to 0% under the attack of trigger â€˜cfâ€™ and to 0.05% under â€˜tqâ€™.
This result further illustrates the versatility of our method.
Table 8: More evalutation results on other PTMs.
PTM
XLNet
BART
RoBERTa
DeBERTa
ALBERT
clean accuracy
94.70%
95.85%
94.80%
95.75%
93.50%
cf
1.00/0.011
1.03/0.010
1.62/0.014
2.65/0.026
1.75/0.018
uw
1.17/0.010
1.99/0.021
3.13/0.027
2.19/0.019
1.08/0.010
5.6 Performance on Other PTMs
In previous sections, we take BERT as an example to examine the
proposed attack. Now, we extend our idea to attack other popular
industrial PTMs in NLP. We use XLNet, BART, RoBERTa, DeBERTa
and ALBERT for further study. We modify their output representa-
tion of the classification token to a POR. To simplify the evaluation,
we only use two triggers which are â€˜cfâ€™ and â€˜uwâ€™, where â€˜cfâ€™ maps
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3148the output representation to the all âˆ’1 vector and â€˜uwâ€™ maps the
output representation to the all 1 vector. Then, we fine-tune and
test the backdoor model on the Amazon dataset. We record the
average ğ¸ and ğ‘† values and the clean accuracy as shown in Table 8.
From Table 8, we can see that most triggers have low values
of ğ¸ and ğ‘†, which means our method can be effectively applied to
all these PTMs. In addition, the accuracy of these models on clean
data is also normal, which ensures the stealthiness of our backdoor
model. Hence, our attack method can be generalized to most PTMs.
Finally, to examine the potential real-world threat of the pro-
posed attack, we report the backdoor models to a popular real-world
platform, HuggingFace model repository. According to our evalua-
tion report, the backdoor models can be uploaded and published
freely, and everyone can access it. HuggingFace official has con-
firmed that this is a serious threat. Note that, we explicitly stated
that our model is backdoored to avoid any harm to users in the
whole evaluation process.
6 SENSITIVITY ANALYSIS
We have shown that different triggers have different effectiveness
in the Sec. 5.1. In this section, we study the various factors that may
affect the performance of our triggers. For all experiments here, the
models are fine-tuned, validated and tested on the Amazon dataset.
6.1 Factors in Trigger Settings
Trigger embedding and POR. Here, we study how trigger em-
bedding and its corresponding POR affect the effectiveness of the
trigger in the classification task. We select three triggers with one
token, which are â€˜cfâ€™, â€˜tqâ€™ and â€˜bbâ€™ and three PORs which are the
original all âˆ’1 vector (O), the reversed all 1 vector (R), and the half
âˆ’1 half 1 vector (H). For each model, we only inject one trigger
corresponding to one POR, a total of nine settings. We use 50k
clean samples and 40k poisoned samples to poison the model. Then,
each model is fine-tuned and tested to get the ğ¸ value and such
two steps are repeated ten times. Finally, we use the t-test to test
the hypothesis that whether their mean value of ğ¸ of each model is
different or not, i.e., whether the factor is influential.
conclusion, a well-designed trigger and its corresponding POR can
effectively enhance the performance of the trigger.
Poisoned sample percentage. We now study how different amounts
of poisoned samples and clean samples influence the trigger effec-
tiveness. We repeatedly poison the backdoor models ten times with
clean samples ranging from 10K to 80K and with poison samples
ranging from 10K to 80K. Then, we fine-tune and test the model,
and get the mean value of trigger effectiveness for each model,
which is illustrated in Fig. 4.
Figure 4: The trigger effectiveness with respect to different
different poison samples and different clean samples.
From Fig. 4, we can see that when there are few poisoned samples
and clean samples, or when there are many poisoned samples and
clean samples, the performance of the injected trigger is relatively
poor. When both types of samples exceed 30k and the numbers are
similar, the trigger can retain more effectiveness after fine-tuning.
Moreover, we can observe that when the poisoned samples reach
50k to 60k, and the clean samples reach 50k to 70k, the injected
trigger performs the best. In summary, the effectiveness of injected
triggers can be greatly influenced by the clean-poison ratio.
6.2 Factors in Fine-tuning Settings
In this section, we choose nine triggers from Section 5.1 based
on the ğ¶ value and simultaneously inject them into one backdoor
model. We refer to it as the base model and use it to study how
fine-tuning settings affect the backdoor effectiveness.
Fine-tuning dataset size. Previous work [28] has shown that the
more training data, the more the model forgets about the trigger.
We increase the fine-tuning datasets from 1k to 512k exponentially
by random sampling from the Amazon dataset to fine-tune the base
model and the result is shown in Fig. 5.
Figure 3: The effect of trigger embedding and POR.
We show the p-values of the t-test in Fig. 3, from which we
can observe that under the same trigger, there are relatively more
p-values below the significance level of 0.1 in the t-test. This indi-
cats that POR has more influence on the effectiveness of a trigger.
However, under the same POR of H, the mean ğ¸ of â€˜tqâ€™ is signifi-
cantly different from that of â€˜bbâ€™. Thus, the trigger embedding also
influences the ğ¸ value, though it is not as significant as the POR. In
Figure 5: Trigger effectiveness versus dataset size.
ORH0.62260.05450.0172ORH0.67120.90430.6008ORH0.10780.07480.7422cftqbb0.57910.63650.2453cftqbb0.13130.140.7754cftqbb0.20330.29590.0166tqcfbbORHSame triggerSame PORSession 11D: Data Poisoning and Backdoor Attacks in ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3149From Fig. 5, we find that the ğ¸ values for most triggers remain
unchanged when the number of fine-tuning samples is small. Most
triggers show an increasing trend after the number of samples
increases to 128k. Along with the increase of samples, some trig-
gers have been severely forgotten by the model, e.g., â€˜
â€™ and
â€˜serendipityâ€™ under the fine-tuning of 128k samples. When the num-
ber of samples increases to 512k, our attack has been neutralized.
Therefore, our attack can be significantly affected with more
fine-tuning samples. This is expected. Intuitively, no trigger can
preserve its utility under the fine-tuning with sufficiently large
amounts of data. However, for many real-world NLP classification
tasks, it is difficult for them to obtain a larger fine-tune dataset. We
investigate the classification datasets provided in Huggingface and
find that most datasets contain instances less than 100K as shown
in Table 17 in Appendix D. Thus, our backdoor effect will not be
neutralized by most NLP datasets in the real-world, thereby posing
a greater threat to them.
We also observe that with 8k fine-tuning samples, the ğ¸ value
â€™. However,
for â€˜Don Quixoteâ€™ is nearly twice of the ğ¸ of â€˜
with 16k training samples, the ğ¸ of â€˜
â€™ is way higher than
the ğ¸ of â€˜Don Quixoteâ€™. This is because the datasets are different
as the number of fine-tuning samples increases. As a result, the
gradient descent direction during fine-tuning might be different
among these experiments, which lead to inconsistent effects on
different triggers.
Fine-tuning epochs. Similar to fine-tuning dataset size, the fine-
tuning epochs may also affect the performance of our backdoor
attack. In the fine-tuning process, we continuously fine-tune the
backdoor model for 25 epochs and test the effectiveness of each
trigger after each epoch. The result is shown in Fig. 6.
Figure 6: The effectiveness for nine triggers versus training
epochs along with the accuracy in the training process.
From Fig. 6, we can see that the ğ¸ values for most triggers grad-
ually increase in the early stage but converge to a constant value
in the later epochs. Some triggers also show a similar spike in the
12ğ‘¡â„ epoch. From Fig. 6, we can observe that the accuracy reaches
more than 94% in the first epoch and fluctuates between 94% and
95% later. Comparing the trend of effectiveness and accuracy, we
do not find any clear correlation between them.
In conclusion, the target modelâ€™s triggers will not be forgotten
severely with the increasing fine-tuning epochs. This phenomenon
is different from the case in the dataset size because increasing
epochs does not add extra information and the model has com-
pletely converged in later epochs. However, increasing the dataset
size allows the model to be continuously updated, which is the
leading factor that triggers cease to be effective.
6.3 Factors in Fine-tuning Dataset
Common versus rare. Here, we study how the appearance of
the trigger in the fine-tuning set affects our attackâ€™s performance.
We select nine words from 8000 fine-tuning set with appearances
range from 128 to 40124 as our triggers. These triggers are injected
simultaneously into one model with the POR-1 setting. Then, the
model achieves an clean accuracy of 94.20% and the result is shown
in Table 9.
Table 9: The trigger effectiveness with respect to common
words and rare words.
Trigger
Appearance
ğ¸
the
40124
1.72
of
15937
2.81
that
8055
4.29
one
3959
4.13
had way
2040
1022
2.52
2.52.
going
512
2.69
already
256
3.16
useful
128
3.46
From Table 9, we observe that the triggers (e.g., â€˜theâ€™) with high
appearances have lower ğ¸ comparing to the triggers (e.g., â€˜usefulâ€™,
â€˜alreadyâ€™) with very few appearances, which are counter-intuitive.
The high appearance of â€˜theâ€™ has a ğ¸ value of 1.72, indicating that the
backdoor information of â€˜theâ€™ is not erased during fine-tuning. For
other triggers, most of them do not fit the expectation that higher
appearances lead to the erasure of trigger effectiveness. Though the
word â€˜theâ€™ is prevalent in the clean sample, the backdoor pre-trained
model is not just learning the POR of â€˜theâ€™ for all normal inputs.
Our further research finds that a small amount of â€˜theâ€™ in a normal
sample cannot hijack the model, which means the model would
only output our POR when the number of â€˜theâ€™ in a text reaches a
certain amount.
Therefore, the appearance frequency of triggers in the fine-
tuning dataset may not influence the trigger effectiveness. We spec-
ulate that in the process of fine-tuning, the modelâ€™s understanding
of these trigger words has not changed. These words are not the
focus of the task, so they are not severely affected by the fine-tuning.
Specifically, the models fine-tuned on the Amazon dataset may fo-
cus on the sentiment-related words, and these sentiment-unrelated