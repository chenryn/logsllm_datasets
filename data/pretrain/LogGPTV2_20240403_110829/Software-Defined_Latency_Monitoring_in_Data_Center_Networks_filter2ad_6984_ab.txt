8
We deﬁne the control processing time of a switch as the time it takes a switch to
process the action included in the rule that matches a packet, generate a PacketIn,
−t7 are the control processing
and send it to the controller. In Fig. 1, t(cid:3)
times for S1 and S3. Control processing times determine when PacketIn messages
arrive at the controller. If processing times vary across the ﬁrst and last switch,
the latency estimation on the path is skewed.
−t2 and t(cid:3)
Control processing consists of slow path delay and control channel delay. The
slow path delay is the time it takes the switch to transfer the packet along its
internal circuits from the ASIC where the match is performed to the switch
CPU that generates the PacketIn. As shown in prior work [4], the slow path
delay depends on what other operations (e.g., ﬂow installations, stat queries)
are performed simultaneously on the switch. The control channel delay is the
propagation delay from the switch to the controller.
We adapt to the variations in control processing across switches by constantly
monitoring both the slow path and control channel delays. To monitor the slow
path delay of a switch, we send packet probes to the switch using PacketOut,
use a carefully placed rule to trigger a PacketIn, and then drop the probe with-
out forwarding it to other switches. This resembles our path latency estimation
method described above, with the modiﬁcation that the path to be monitored
consists of one switch. We discard latency samples obtained during periods when
the slow path delays of the ﬁrst and last switches on a path vary. Predicting how
each variation aﬀects our latency estimate is subject of future work.
To monitor the control channel delay on a switch, we send EchoRequest Open-
Flow control messages to the switch and measure the delay in its replies. We ﬁnd
that the control channel delay from the controller to switch is more predictable.
366
C. Yu et al.
Thus, if we discover that switches are not equidistant to the controller, we simply
adjust the estimated latency by the diﬀerence in their control channel delays, as
hinted earlier in the section.
3.3 Monitoring Design
We have developed SLAM, a framework for latency monitoring in SDNs, based
on the methods enumerated above. SLAM combines four components—rule gen-
erator, traﬃc generator, traﬃc listener, and latency estimator—that run on the
network controller (Fig. 2).
Given a path to monitor, SLAM identiﬁes the ﬁrst and last switches on
the path. It then installs a speciﬁc rule on each switch on the path to guide
measurement probes, as explained above. The traﬃc generator then sends a
stream of packet probes along the monitored path using OpenFlow PacketOut
messages. These packets match the speciﬁc rules installed in the previous step.
Normal traﬃc is processed by the original rules on the switches and is not aﬀected
by our monitoring rules. In addition, the measurement module generates probes
to monitor the slow path and control channel delays of the ﬁrst and last switches
on a monitored path.
The traﬃc listener captures control packets received from switches and
records their arrival timestamps. To obtain a latency sample, it then corre-
lates PacketIn messages associated with the same probe packet and triggered by
diﬀerent switches. By aggregating the latency samples obtained from multiple
probes sent on a path, SLAM computes a latency distribution for the path.
Fig. 2. SLAM design. SLAM generates probe packets along the path to be monitored.
The probes are guided by carefully speciﬁed rules and trigger PacketIn messages at the
ﬁrst and last switches on the path. SLAM analyzes PacketIn arrival times and estimates
path latency.
Software-Deﬁned Latency Monitoring in Data Center Networks
367
4 Evaluation
We implemented SLAM as a module for the POX OpenFlow controller and
deployed it on our 12-switch network testbed. We evaluate SLAM from three
aspects: (1) the accuracy of its latency estimates, (2) its utility in selecting
paths based on latency, and (3) its adaptiveness to network conditions.
Ground truth estimation. To evaluate the quality of SLAM’s path latency
estimates, we must ﬁrst measure the real path latency (i.e., the ground truth).
As we cannot directly time packet arrival and departure on switches, we use
the following setup to measure ground truth, similar to that used for OpenFlow
testing by Rotsos et al. [13] and by Huang et al. [8]. We create another physical
connection between the ﬁrst and last switches on a path and the controller in
addition to the already existing control channel and put the controller on the
data plane.
We use the controller to send probe packets along the path to be monitored.
When a probe arrives at the ﬁrst switch, the action of the matching rule sends
the packet both to the next switch on the path and to the controller on the data
plane. Similarly, at the last switch, the matching rule sends probe packets back to
the controller. We obtain the ground truth latency by subtracting the two arrival
times of the same probe at the controller. This method is similar to that used
by SLAM, with the diﬀerence that no packet ever crosses into the control plane.
Although the computed latency may not perfectly reﬂect the ground truth, it
does not contain the eﬀects of control processing, and hence, can be used as a
reasonable estimate to compare against SLAM’s estimated latency distribution.
Experiments. To evaluate SLAM’s performance under diﬀerent network con-
ditions, we perform three sets of experiments: low latency (Exp L), medium
latency (Exp M), and high latency (Exp H). We estimate latency between the
same pair of switches in our testbed, but each of the three experiments takes
place on a diﬀerent path between the switches. There is no background traf-
ﬁc for the low latency experiment. For medium and high latency experiments,
we introduce additional traﬃc using iperf and simulate congestion by shaping
traﬃc at an intermediate switch on the path. We use 200 Mbps iperf traﬃc with
100 Mbps traﬃc shaping in Exp M, and 20 Mbps iperf traﬃc with 10 Mbps traﬃc
shaping in Exp H. In each experiment, we run both SLAM and the ground truth
estimator concurrently for 10 min with a rate of one probe per second.
4.1 Accuracy
First, we seek to understand how similar to ground truth is the latency distri-
bution computed by SLAM. To compare two latency distributions (of diﬀerent
paths or of the same path under diﬀerent conditions), we use the Kolmogorov-
Smirnov (KS) test [9]. The KS test computes a statistic that captures the dis-
tance between the empirical cumulative distribution functions (CDFs) of the two
sets of latencies. The null hypothesis is that the two sets of latencies are drawn
from the same distribution. If we can reject the null hypothesis based on the test
368
C. Yu et al.
s
t
n
e
m
e
r
u
s
a
e
m
f
o
F
D
C
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
 0
 20
 40
 60
Latency (ms)
(a)
)
s
m
(
y
c
n
e
t
a
L
 20
 15
 10
 5
 0
Exp L (SLAM)
Exp L (GT)
Exp M (SLAM)
Exp M (GT)
Exp H (SLAM)
Exp H (GT)
 80
 100
SLAM
Ground Truth
 0
 100
 200
 300
 400
 500
 600
Time (sec)
(b)
Fig. 3. (a) SLAM vs. Ground truth latency empirical CDFs. (b) SLAM with bursty
traﬃc. As path latency increases, SLAM is able to correctly detect the increase.
s
e
l
i
t
n
a
u
q
M
A
L
S
13
12
11
10
9
8
7
6
5
4
110
100
90
80
70
60
50
s
e
l
i
t
n
a
u
q
M
A
L
S
4
5
6
7
8
9
10
11
40
40
50
Ground truth quantiles
(a)
80
90
100
70
60
Ground truth quantiles
(b)
Fig. 4. Quantile-Quantile plots for SLAM vs. ground truth in (b) Exp M, and (c) Exp
H. The quantiles for SLAM’s estimates are close to the quantiles for ground truth esti-
mates, indicating that SLAM is able to detect millisecond-level path latency variations.
statistic, then this implies that the two distributions are not equal. Further, we
can compare the quantiles of the two distributions (e.g., median) to determine