title:ScatterCache: Thwarting Cache Attacks via Cache Set Randomization
author:Mario Werner and
Thomas Unterluggauer and
Lukas Giner and
Michael Schwarz and
Daniel Gruss and
Stefan Mangard
Scattercache: Thwarting Cache Attacks via 
Cache Set Randomization
Mario Werner, Thomas Unterluggauer, Lukas Giner, Michael Schwarz, Daniel Gruss, and 
Stefan Mangard, Graz University of Technology
https://www.usenix.org/conference/usenixsecurity19/presentation/werner
This paper is included in the Proceedings of the 28th USENIX Security Symposium.August 14–16, 2019 • Santa Clara, CA, USA978-1-939133-06-9Open access to the Proceedings of the 28th USENIX Security Symposium is sponsored by USENIX.SCATTERCACHE: Thwarting Cache Attacks via Cache Set Randomization
Mario Werner, Thomas Unterluggauer, Lukas Giner,
Michael Schwarz, Daniel Gruss, Stefan Mangard
Graz University of Technology
Abstract
Cache side-channel attacks can be leveraged as a building
block in attacks leaking secrets even in the absence of soft-
ware bugs. Currently, there are no practical and generic miti-
gations with an acceptable performance overhead and strong
security guarantees. The underlying problem is that caches
are shared in a predictable way across security domains.
In this paper, we eliminate this problem. We present SCAT-
TERCACHE, a novel cache design to prevent cache attacks.
SCATTERCACHE eliminates ﬁxed cache-set congruences and,
thus, makes eviction-based cache attacks unpractical. For this
purpose, SCATTERCACHE retroﬁts skewed associative caches
with a keyed mapping function, yielding a security-domain-
dependent cache mapping. Hence, it becomes virtually impos-
sible to ﬁnd fully overlapping cache sets, rendering current
eviction-based attacks infeasible. Even theoretical statistical
attacks become unrealistic, as the attacker cannot conﬁne con-
tention to chosen cache sets. Consequently, the attacker has
to resort to eviction of the entire cache, making deductions
over cache sets or lines impossible and fully preventing high-
frequency attacks. Our security analysis reveals that even in
the strongest possible attacker model (noise-free), the con-
struction of a reliable eviction set for PRIME+PROBE in an 8-
way SCATTERCACHE with 16384 lines requires observation
of at least 33.5 million victim memory accesses as compared
to fewer than 103 on commodity caches. SCATTERCACHE
requires hardware and software changes, yet is minimally in-
vasive on the software level and is fully backward compatible
with legacy software while still improving the security level
over state-of-the-art caches. Finally, our evaluations show that
the runtime performance of software is not curtailed and our
design even outperforms state-of-the-art caches for certain
realistic workloads.
1 Introduction
Caches are core components of today’s computing architec-
tures. They bridge the performance gap between CPU cores
and a computer’s main memory. However, in the past two
decades, caches have turned out to be the origin of a wide
range of security threats [10, 15, 27, 38, 39, 43, 44, 51, 76]. In
particular, the intrinsic timing behavior of caches that speeds
up computing systems allows for cache side-channel attacks
(cache attacks), which are able to recover secret information.
Historically, research on cache attacks focused on cryp-
tographic algorithms [10, 44, 51, 76]. More recently, how-
ever, cache attacks like PRIME+PROBE [44, 48, 51, 54, 62]
and FLUSH+RELOAD [27, 76] have also been used to attack
address-space-layout randomization [23, 25, 36], keystroke
processing and inter-keystroke timing [26, 27, 60], and gen-
eral purpose computations [81]. For shared caches on modern
multi-core processors, PRIME+PROBE and FLUSH+RELOAD
even work across cores executing code from different security
domains, e.g., processes or virtual machines.
The most simple cache attacks, however, are covert chan-
nels [46,48,72]. In contrast to a regular side-channel attack, in
a covert channel, the “victim” is colluding and actively trying
to transmit data to the attacker, e.g., running in a different
security domain. For instance, Meltdown [43], Spectre [38],
and Foreshadow [15] use cache covert channels to transfer
secrets from the transient execution domain to an attacker.
These recent examples highlight the importance of ﬁnding
practical approaches to thwart cache attacks.
To cope with cache attacks, there has been much research
on ways to identify information leaks in a software’s memory
access pattern, such as static code [19,20,41,45] and dynamic
program analysis [34, 71, 74, 77]. However, mitigating these
leaks both generically and efﬁciently is difﬁcult. While there
are techniques to design software without address-based infor-
mation leaks, such as unifying control ﬂow [17] and bitsliced
implementations of cryptography [37, 40, 58], their general
application to arbitrary software remains difﬁcult. Hence,
protecting against cache attacks puts a signiﬁcant burden on
software developers aiming to protect secrets in the view
of microarchitectural details that vary a lot across different
Instruction-Set Architecture (ISA) implementations.
A different direction to counteract cache attacks is to design
USENIX Association
28th USENIX Security Symposium    675
more resilient cache architectures. Typically, these architec-
tures modify the cache organization in order to minimize
interference between different processes, either by breaking
the trivial link between memory address and cache index [22,
55, 67, 69, 70] or by providing exclusive access to cache parti-
tions for critical code [53, 57, 69]. While cache partitioning
completely prevents cache interference, its rather static alloca-
tion suffers from scalability and performance issues. On the
other hand, randomized cache (re-)placement [69, 70] makes
mappings of memory addresses to cache indices random and
unpredictable. Yet, managing these cache mappings in lookup
tables inheres extensive changes to the cache architecture and
cost. Finally, the introduction of a keyed function [55, 67]
to pseudorandomly map the accessed memory location to
the cache-set index can counteract PRIME+PROBE attacks.
However, these solutions either suffer from a low number of
cache sets, weakly chosen functions, or cache interference for
shared memory and thus require to change the key frequently
at the cost of performance.
Hence, there is a strong need for a practical and effective
solution to thwart both cache attacks and cache covert chan-
nels. In particular, this solution should (1) make cache attacks
sufﬁciently hard, (2) require as little software support as pos-
sible, (3) embed ﬂexibly into existing cache architectures, (4)
be efﬁciently implementable in hardware, and (5) retain or
even enhance cache performance.
Contribution. In this paper, we present SCATTERCACHE,
which achieves all these goals. SCATTERCACHE is a novel
and highly ﬂexible cache design that prevents cache attacks
such as EVICT+RELOAD and PRIME+PROBE and severely
limits cache covert channel capacities by increasing the num-
ber of cache sets beyond the number of physically available
addresses with competitive performance and implementation
cost. Hereby, SCATTERCACHE closes the gap between previ-
ous secure cache designs and today’s cache architectures by
introducing a minimal set of cache modiﬁcations to provide
strong security guarantees.
Most prominently, SCATTERCACHE eliminates
the
ﬁxed cache-set congruences that are the cornerstone of
PRIME+PROBE attacks. For this purpose, SCATTERCACHE
builds upon two ideas. First, SCATTERCACHE uses a
keyed mapping function to translate memory addresses
and the active security domain, e.g., process, to cache set
indices. Second, similar to skewed associative caches [63],
the mapping function in SCATTERCACHE computes a
different index for each cache way. As a result, the number
of different cache sets increases exponentially with the
number of ways. While SCATTERCACHE makes ﬁnding fully
identical cache sets statistically impossible on state-of-the-art
architectures, the complexity for exploiting inevitable partial
cache-set collisions also rises heavily. The reason is in
part that the mapping of memory addresses to cache sets
in SCATTERCACHE is different for each security domain.
Hence, and as our security analysis shows, the construction
of a reliable eviction set for PRIME+PROBE in an 8-way
SCATTERCACHE with 16384 lines requires observation of at
least 33.5 million victim memory accesses as compared to
fewer than 103 on commodity caches, rendering these attacks
impractical on real systems with noise.
effectively
Additionally, SCATTERCACHE
prevents
FLUSH+RELOAD-based cache attacks, e.g., on shared
libraries, as well. The inclusion of security domains in
SCATTERCACHE and its mapping function preserves
shared memory in RAM, but prevents any cache lines to be
shared across security boundaries. Yet, SCATTERCACHE
supports shared memory for inter-process communication
via dedicated separate security domains. To achieve highest
ﬂexibility, managing the security domains of SCATTER-
CACHE is done by software, e.g., the operating system.
However, SCATTERCACHE is fully backwards compatible
and already increases the effort of cache attacks even without
any software support. Nevertheless, the runtime performance
of software on SCATTERCACHE is highly competitive
and, on certain workloads, even outperforms cache designs
implemented in commodity CPUs.
SCATTERCACHE constitutes a comparably simple exten-
sion to cache and processor architectures with minimal hard-
ware cost: SCATTERCACHE essentially only adds additional
index derivation logic, i.e., a lightweight cryptographic primi-
tive, and an index decoder for each scattered cache way. More-
over, to enable efﬁcient lookups and writebacks, SCATTER-
CACHE stores the index bits from the physical address in
addition to the tag bits, which adds < 5% storage overhead
per cache line. Finally, SCATTERCACHE consumes one bit
per page-table entry (≈ 1.5% storage overhead per page-table
entry) for the kernel to communicate with the user space.
Outline. This paper is organized as follows. In Section 2,
we provide background information on caches and cache
attacks. In Section 3, we describe the design and concept
of SCATTERCACHE. In Section 4, we analyze the security
of SCATTERCACHE against cache attacks. In Section 5, we
provide a performance evaluation. We conclude in Section 6.
2 Background
In this section, we provide background on caches, cache side-
channel attacks, and resilient cache architectures.
2.1 Caches
Modern computers have a memory hierarchy consisting of
many layers, each following the principle of locality, storing
data that is expected to be used in the future, e.g., based on
what has been accessed in the past. Modern processors have
a hierarchy of caches that keep instructions and data likely
to be used in the future near the execution core to avoid the
latency of accesses to the slow (DRAM) main memory. This
cache hierarchy typically consists of 2 to 4 layers, where the
676    28th USENIX Security Symposium
USENIX Association
Figure 1: Indexing cache sets in a 4-way set-associative cache.
lowest layer is the smallest and fastest, typically only a few
kilobytes. The last-level cache is the largest cache, typically
in the range of several megabytes. On most processors, the
last-level cache is shared among all cores. The last-level cache
is often inclusive, i.e., any cache line in a lower level cache
must also be present in the last-level cache.
Caches are typically organized into cache sets that are com-
posed of multiple cache lines or cache ways. The cache set is
determined by computing the cache index from address bits.
Figure 1 illustrates the indexing of a 4-way set-associative
cache. As the cache is small and the memory large, many
memory locations map to the same cache set (i.e., the ad-
dresses are congruent). The replacement policy (e.g., pseudo-
LRU, random) decides which way is replaced by a newly
requested cache line. Any process can observe whether data
is cached or not by observing the memory access latency
which is the basis for cache side-channel attacks.
2.2 Cache Side-Channel Attacks
Cache side-channel attacks have been studied for over the
past two decades, initially with a focus on cryptographic algo-
rithms [10, 39, 51, 52, 54, 68]. Today, a set of powerful attack
techniques enable attacks in realistic cross-core scenarios.
Based on the access latency, an attacker can deduce whether
or not a cache line is in the cache, leaking two opposite kinds
of information. (1) By continuously removing (i.e., evicting
or ﬂushing) a cache line from the cache and measuring the ac-
cess latency, an attacker can determine whether this cache line
has been accessed by another process. (2) By continuously
ﬁlling a part of the cache with attacker-accessible data, the
attacker can measure the contention of the corresponding part,
by checking whether the attacker-accessible data remained in
the cache. Contention-based attacks work on different layers:
The Entire Cache or Cache Slices. An attacker can mea-
sure contention of the entire cache or a cache slice. Mau-
rice et al. [46] proposed a covert channel where the sender
evicts the entire cache to leak information across cores and
the victim observes the cache contention. A similar attack
could be mounted on a cache slice if the cache slice function
is known [47]. The granularity is extremely coarse, but with
statistical attacks can leak meaningful information [61].
Cache Sets. An attacker can also measure the contention
of a cache set. For this, additional knowledge may be required,
such as the mapping from virtual addresses to physical ad-
dresses, as well as the functions mapping physical addresses
to cache slices and cache sets. The attacker continuously ﬁlls
a cache set with a set of congruent memory locations. Filling
a cache set is also called cache-set eviction, as it evicts any
previously contained cache lines. Only if some other process
accessed a congruent memory location, memory locations
are evicted from a cache set. The attacker can measure this
for instance by measuring runtime variations in a so-called
EVICT+TIME attack [51]. The EVICT+TIME technique has
mostly been applied in attacks on cryptographic implemen-
tations [31, 42, 51, 65]. Instead of the runtime, the attacker
can also directly check how many of the memory locations
are still cached. This attack is called PRIME+PROBE [51].
Many PRIME+PROBE attacks on private L1 caches have been
demonstrated [3,14,51,54,80]. More recently, PRIME+PROBE
attacks on last-level caches have also been demonstrated in
various generic use cases [4, 44, 48, 50, 59, 79].
Cache Lines. At a cache line granularity, the attacker
can measure whether a memory location is cached or not.
As already indicated above, here the logic is inverted. Now
the attacker continuously evicts (or ﬂushes) a cache line
from the cache. Later on, the attacker can measure the
latency and deduce whether another process has loaded
the cache line into the cache. This technique is called
FLUSH+RELOAD [28, 76]. FLUSH+RELOAD has been stud-
ied in a long list of different attacks [4–6,27,32,35,42,76,78,
81]. Variations of FLUSH+RELOAD are FLUSH+FLUSH [26]
and EVICT+RELOAD [27, 42].
Cache Covert Channels
Cache covert channels are one of the simplest forms of cache
attacks. Instead of an attacker process attacking a victim pro-
cess, both processes collude to covertly communicate using
the cache as transmission channel. Thus, in this scenario, the
colluding processes are referred to as sender and receiver, as
the communication is mostly unidirectional. A cache covert
channel allows bypassing all architectural restrictions regard-
ing data exchange between processes.
Cache covert channels have been shown using various
cache attacks, such as PRIME+PROBE [44, 48, 73, 75] and
FLUSH+RELOAD [26]. They achieve transmission rates of
up to 496 kB/s [26]. Besides native attacks, covert channels
have also been shown to work within virtualized environ-
ments, across virtual machines [44, 48, 75]. Even in these
restricted environments, cache-based covert channels achieve
transmission rates of up to 45 kB/s [48].
2.3 Resilient Cache Architectures
The threat of cache-based attacks sparked several novel cache
architectures designed to be resilient against these attacks.
While ﬁxed cache partitions [53] lack ﬂexibility, randomized
USENIX Association
28th USENIX Security Symposium    677
offsetset[idx+2]set[idx-2]set[idx-1]set[idx+1]way 0way 1way 2way 3indextagcache allocation appears to be more promising. The following
brieﬂy discusses previous designs for a randomized cache.
RPCache [69] and NewCache [70] completely disrupt
the meaningful observability of interference by performing
random (re-)placement of lines in the cache. However, man-
aging the cache mappings efﬁciently either requires full as-
sociativity or content addressable memory. While optimized
addressing logic can lead to efﬁcient implementations, these
designs differ signiﬁcantly from conventional architectures.
Time-Secure Caches [67] is based on standard set-
associative caches that are indexed with a keyed function
that takes cache line address and Process ID (PID) as an input.
While this design destroys the obvious cache congruences
between processes to minimize cache interference, a compa-
rably weak indexing function is used. Eventually, re-keying
needs to be done quite frequently, which amounts to ﬂushing
the cache and thus reduces practical performance. SCATTER-
CACHE can be seen as a generalization of this approach with
higher entropy in the indexing of cache lines.
CEASER [55] as well uses standard set-associative caches
with keyed indexing, which, however, does not include the
PID. Hence, inter-process cache interference is predictable
based on in-process cache collisions. As a result, CEASER
strongly relies on continuous re-keying of its index deriva-
tion to limit the time available for conducting an attack. For
efﬁcient implementation, CEASER uses its own lightweight
cryptographic primitive designed for that speciﬁc application.
3 ScatterCache
As Section 2 showed, caches are a serious security concern in
contemporary computing systems. In this section, we hence
present SCATTERCACHE—a novel cache architecture that
counteracts cache-based side-channel attacks by skewed pseu-
dorandom cache indexing. After discussing the main idea
behind SCATTERCACHE, we discuss its building blocks and
system integration in more detail. SCATTERCACHE’s security
implications are, subsequently, analyzed in Section 4.
3.1 Targeted Properties
Even though contemporary transparent cache architectures
are certainly ﬂawed from the security point of view, they
still feature desirable properties. In particular, for regular
computations, basically no software support is required for
cache maintenance. Also, even in the case of multitasking
and -processing, no dedicated cache resource allocation and
scheduling is needed. Finally, by selecting the cache size and
the number of associative ways, chip vendors can trade hard-
ware complexity and costs against performance as desired.
SCATTERCACHE’s design strives to preserve these features
while adding the following three security properties:
1. Between software deﬁned security domains (e.g., differ-
ent processes or users on the same machine, different
Figure 2: Flattened visualization of mapping addresses to
cache sets in a 4-way set-associative cache with 16 cache lines.
Top: Standard cache where index bits select the cache set.
Middle: Pseudorandom mapping from addresses to cache sets.
The mapping from cache lines to sets is still static. Bottom:
Pseudorandom mapping from addresses to a set of cache lines
that dynamically form the cache set in SCATTERCACHE.
VMs, . . . ), even for exactly the same physical addresses,
cache lines should only be shared if cross-context co-
herency is required (i.e., writable shared memory).
2. Finding and exploiting addresses that are congruent in
the cache should be as hard as possible (i.e., we want
to “break” the direct link between the accessed physical
address and the resulting cache set index for adversaries).
3. Controlling and measuring complete cache sets should
be hard in order to prevent eviction-based attacks.
Finally, to ease the adoption and to utilize the vast knowl-
edge on building efﬁcient caches, the SCATTERCACHE hard-
ware should be as similar to current cache architectures as
possible.
Idea