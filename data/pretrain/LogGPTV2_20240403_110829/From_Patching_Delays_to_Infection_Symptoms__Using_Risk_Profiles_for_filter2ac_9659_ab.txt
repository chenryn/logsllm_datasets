structure. Our results show that indeed for vulnerabil-
ities with known exploits, this match is much stronger
than that for those without known exploits.
We then use the consistency measures as features,
along with a number of other intrinsic features, to train a
classiﬁer aimed at detecting exploitations.
2.3 Threat model
One type of adversaries implicit in this work are those
actively exploiting software vulnerabilities. One basic
assumption we adopt is that such exploitation can occur
as soon as the vulnerabilities are introduced (with new
version releases, etc.), though our detection framework
is triggered by the ofﬁcial vulnerability disclosure, as in-
dicated in Figure 1. We assume such an adversary can
potentially develop and actively pursue exploits for any
existing vulnerability.
A second type of adversaries we consider are those
who not only seek exploitation but also have the abil-
ity to control a signiﬁcant number of end-hosts so as to
manipulate the patching signals we use in our detection
framework. In other words, this is a type of attack (or
evasion attempt) against our speciﬁc detection methodol-
ogy which uses patching signals as one of the inputs. The
manipulation is intended to interfere with the way we
measure similarity between networks; in Section 7.2 we
examine the robustness of our detection method against
this type of attack.
3 Datasets
Table 1 summarizes the datasets used in this study. Since
we need time-aligned measurements to compare behav-
iors between patching and malicious activity signals,
only the overlapping time period, 01/2013-07/2014, is
used in our analysis.
3.1 End-host patching
Our study draws from a number of data sources that col-
lectively characterize the users’ patching behavior, al-
lowing us to assess their susceptibility to known vulnera-
bilities and exploits at any point in time. This set will also
be referred to as the risk/behavioral data. These include
the host patching data [14], the National Vulnerability
Database (NVD) [33], and release notes from software
vendors of the products examined in our study.
Patch deployment measurements This data source
allows us to observe users’ patching behavior to assess
their susceptibility to known vulnerabilities. We use
patch deployment measurements collected by Nappa et
al. on end-hosts [30]. This corpus records installation
of subsequent versions of different applications along
with each event’s timestamp, by mapping records of bi-
nary executables on user machines to their correspond-
ing application versions. This data is derived from the
WINE dataset provided by Symantec [14], and includes
observations on hosts worldwide between 02/2008 and
07/2014. In addition, we extract the security ﬂaws af-
fecting each application version from the National Vul-
nerability database (NVD), where each vulnerability is
denoted by its Common Vulnerabilities and Exposures
Identiﬁer (CVE-ID).
For each host and CVE-ID, we follow the methodol-
ogy described in [38] to collect the periods of time where
a host is susceptible to disclosed but unpatched vulnera-
bilities, through the presence of vulnerable application
versions on their machines. This method involves ﬁnd-
ing the state of a host, i.e., the set of applications installed
on the machine, for any point throughout the observation
period, and extracting the set of disclosed vulnerabilities
corresponding to those application versions from NVD.
Note that a user might also install different product lines
of the same application, e.g., Flash Player 10 and 11, at
the same time. We will elaborate on this in Section 4.1.
For this study, we analyze user patching behavior over
7 applications with the best host coverage in our dataset,
namely Google Chrome, Mozilla Firefox, Mozilla Thun-
derbird, Safari, Opera, Adobe Acrobat Reader, and
Adobe Flash Player; we ignore hosts that have recorded
less than 10 events for all of these applications. Re-
906    27th USENIX Security Symposium
USENIX Association
Category
End-host patching (risk behavior)
Collection period
Feb 2008 - Jul 2014 NVD [33], patch deployment measurements [14],
Datasets
Malicious activity (symptom)
Jan 2013 - Present
Vulnerability exploits (cause)
Jan 2010- Present
vendors’ release notes
CBL [9] , SBL [39], SpamCop [41], UCEPRO-
TECT [45], WPBL [48]
SecurityFocus
tures [42], intrusion-protection signatures [4]
[40], Symantec’s anti-virus
signa-
Table 1: Summary of datasets. For this study, we use the intersection of all observation windows (01/2013-07/2014).
stricted to the study period of 01/2013-07/2014, we ob-
serve 370,510 events over 30,310 unique hosts.
Vulnerability exploits As noted earlier, only a small
fraction of disclosed vulnerabilities have known exploits;
some exploits may remain undiscovered, but a large
number of vulnerabilities are never exploited. We iden-
tify the set of vulnerabilities exploited in the real world
from two sources. The ﬁrst is the corpus of exploited
vulnerabilities collected by Carl et al. [36]. These are ex-
tracted from public descriptions of Symantec’s anti-virus
signatures [42], and intrusion-protection signatures (IPS)
[4]. Limiting the vulnerabilities included in our study to
the above 7 products between 01/2013 to 07/2014, we
curate a dataset containing 18 vulnerabilities. The sec-
ond source of exploits is the SecurityFocus vulnerability
database [40] from Symantec. We query all CVE-IDs
extracted from NVD included in our study and obtain 44
exploited-in-the-wild (EIW) vulnerabilities. Combining
all curated datasets we obtain 56 exploited-in-the-wild
(EIW) and 300 not-exploited-in-the-wild (NEIW) vul-
nerabilities.
Software release notes To ﬁnd whether a host is sus-
ceptible to a vulnerability and to address the issue of par-
allel product lines, we utilize the release date of each
application version included in our study. For Thunder-
bird, Firefox, Chrome, Opera, Adobe Acrobat Reader
and Adobe Flash Player, we crawl the release history logs
from the ofﬁcial vendor’s websites or a third party. How-
ever, there sometimes exist sub-versions that are not in-
cluded in these sources. Thus, we also use release dates
from Nappa et al. [30] who automatically extract soft-
ware release dates by selecting the ﬁrst date when the
version appears in the patch deployment dataset [14].
3.2 Malicious activities
Our second main category of data consists of IP level
spam activities and will refer to this as symptomatic
data since malicious activities are ostensible signs that
end-hosts have been infected, possibly through the use
of an exploited vulnerability present on the host. This
dataset is sourced from well-established monitoring sys-
tems such as spam traps in the form of various reputation
blacklists (RBLs) [9, 39, 41, 45, 48]. In this study, we
use 5 common daily IP address based RBLs from Jan-
uary 2013 to July 2014 which overlap with the patch de-
ployment measurements.
Note that the use of spam data is only a proxy for host
infection caused by vulnerability exploits and an imper-
fect one at that. For instance, not all spam are caused by
exploits; some spamming botnets are distributed through
malicious attachments. Similarly, it is also common for
cyber-criminals to rent pay-per-install services to install
bots. In both cases, the resulting spam activities are not
correlated with host patching patterns. This raises the
question whether these other types of operations may
render our approach ineffective. Our results show the op-
posite; the detection performance we are able to achieve
suggests that spam is a very good proxy for this purpose
despite the existence of non-vulnerability related spam-
ming bot distributions.
Note that hosts in our patch deployment dataset are
anonymized, but can be aggregated at the Internet Ser-
vice Provider (ISP) level. Hence, we also use the Max-
mind GeoIP2ISP service [29] (identifying 3.5 million
IPv4 address blocks belonging to 68,605 ISPs) to aggre-
gate malicious activity indicators at the ISP level. We
then align the resulting time series data with aggregated
patching signals for evaluating our methodology.
4 Data Processing and Preliminaries
In this section we further elaborate on how time series
data are aggregated at the ISP level and how we deﬁne
similarity measures between ISPs.
4.1 Aggregating at the ISP level
The mapping from hosts to ISPs is not unique; as devices
move it may be associated with different IP addresses
and possibly different ISPs. This is the case with both
the patching data and the RBLs and our aggregation takes
this into account by similarly mapping the same host to
multiple ISPs whenever this is indicated in the data.
Aggregating the RBL signals at the ISP level is rel-
atively straightforward. Each RBL provides a daily list
USENIX Association
27th USENIX Security Symposium    907
of malicious IP addresses, from which we count the to-
tal number of unique IPs belonging to any ISP. Formally,
let Rn(t) denote the total number of unique IPs listed on
these RBLs on day t that belong to ISP n (by mapping
the IPs to preﬁxes associated with this ISP). This is then
normalized by the size of ISP n; this normalization step
is essential as pairwise comparisons between ISPs can be
severely skewed when there is a large difference in their
respective sizes. The normalized time series rn(t) will
also be referred to as the symptom signal of ISP n.
Aggregating the patching data at an ISP level is signif-
icantly more involved. This is because the measurements
are in the form of a sequence of application versions in-
stalled on a host with their corresponding timestamps. To
quantify the risk of a given host, we ﬁrst extract known
vulnerabilities affecting each application version from
NVD using the Common Vulnerabilities and Exposures
Identiﬁer (CVE-ID) of the vulnerability. Each vulnera-
bility will also be referred to as a CVE throughout this
paper. However, this extraction is complicated by the
fact that there may be multiple product lines present on
a host, or when a user downgrades to an earlier release.
Moreover, multiple product lines of a software are some-
times developed in parallel by a vendor, all of which
could be affected by the same CVE, e.g,. Flash Player
10 and 11.
It follows that if a host has both versions,
then updating one but not the other will still leave the
host vulnerable. In this study, we use the release notes
described in Section 3.1 as an additional data source to
distinguish between parallel product lines, by assuming
that application versions belonging to the same line fol-
low a chronological order of release dates, while multiple
parallel lines can be developed in parallel by the vendor.
This heuristic allows us to discern different product lines
of each application and users that have installed multiple
product lines on their respective machines at any point in
time, leading to a more accurate estimate of their states.
We quantify the vulnerability of a single host h to CVE
j on day t by counting how many versions present on the
host on day t are subject to this CVE. Denoted by W j
h (t),
in most cases this is a binary indicator (i.e., whether there
exists a single version subject to this CVE), but occa-
sionally this can be an integer > 1 due to the presence of
parallel product lines mentioned above. This quantity is
then summed over all hosts belonging to an ISP n, result-
ing in a total count of unpatched vulnerabilities present
in this ISP. We again normalize this quantity by the ISP’s
size and denote the normalized signal by w j
n(t).
We have now obtained two types of time series for
each ISP n: rn(t) denoting the normalized malicious
activities (also referred to as the symptom signal), and
n(t), j ∈ V , denoting the normalized risk with respect
w j
to CVE j; the latter is a set of time series, one for each
CVE in the set V (also referred to as the risk signal).
Note that rn(t) is not CVE-speciﬁc; however, a given
CVE determines the time period in which this signal is
examined as we show next.
4.2 Similarity in symptoms and in risk
As described in the introduction and highlighted in Fig-
ure 2, our basic methodology relies on identifying the
similarity structure using symptom data and quantify-
ing how strongly the risk patterns are associated with the
symptom similarity structure. This is done for each CVE
separately. Note that our aggregated malicious activity
signal rn(t) for ISP n is agnostic to the choice of CVE,
since the observed malicious activities from a single host
can be attributed to a variety of reasons including various
CVEs the host is vulnerable to. However, our analysis on
a given CVE determines the time window from which we
examine this signal. Speciﬁcally, consider the following
deﬁnition of correlation between two vectors u[0 : d] and
v[0 : d], which tries to ﬁnd similarity between the two
vectors by allowing time shifts/delays between the two:
t=k u(t)· v(t − k)
∑d
∑d−k
t=0 v(t)· v(t)· ∑d−k
t=0 u(t + k)· u(t + k)
,
(1)
(cid:113)
Su,v(k) =
where k = 0,··· ,d denotes all possible time shifts. The
above equation keeps v ﬁxed and slides u one element
at a time and generates a sequence of correlations over
increasingly shorter vectors. Similarly, we can keep u
ﬁxed and slide v one element at a time, which gives us
Sv,u(k) for k = 0,··· ,d. Our pairwise similarity measure
is deﬁned by the maximum of these correlations subject
to a lower bound on how long the vector should be:
Su,v = max( max
0≤k≤d−a
(Su,v(k)), max
0≤k≤d−a
(Sv,u(k)),
(2)
where a is a lower bound to guarantee the correlation is
computed over vectors of length at least d − a to prevent
artiﬁcially high values. In our numerical experiment a is
4(cid:101).
set to (cid:100) d
With the above deﬁnition, the pairwise symptom simi-
larity between a pair of ISPs n and m for CVE j can now
be formally stated. Assume t j
o to be the day of disclo-
sure for CVE j. We will focus on the time period from
disclosure to d days after that, as we aim to see whether
by examining this period we can detect the presence of
an exploit.1 For simplicity of presentation, we shift t j
o
to origin, which gives us two symptom signals of length
d + 1: rn[0 : d] and rm[0 : d], and a pairwise symptom
similarity measure S j
rn,rm using Equations (1) and (2).
We can similarly deﬁne the pairwise risk similarity be-
tween this pair of ISPs, given by S j
wn,wm.
1In Section 6 we also examine whether signs of infection can be
detected before the ofﬁcial disclosure; in that case this window starts
d1 days before the disclosure and ends d2 days after.
908    27th USENIX Security Symposium
USENIX Association
(a) The green community.
(b) The pink community.
Figure 4: Aggregate malicious signals of selected ISPs
belonging to either green or pink community in Fig. 3.
munity detection over such a graph is a collection of clus-
ters, each of which represents ISPs that share very sim-
ilar symptoms. We use two state-of-the-art community
detection algorithms, both of which detect overlapping
communities, i.e., a node may belong to multiple clus-
ters. The ﬁrst one is BigClam (Cluster Afﬁliation Model
for Big Networks) [52]; this is a model-based commu-