title:Maple: simplifying SDN programming using algorithmic policies
author:Andreas Voellmy and
Junchang Wang and
Yang Richard Yang and
Bryan Ford and
Paul Hudak
Maple: Simplifying SDN Programming
Using Algorithmic Policies
Junchang Wang(cid:63)† Y. Richard Yang(cid:63) Bryan Ford(cid:63) Paul Hudak(cid:63)
†University of Science and Technology of China
Andreas Voellmy(cid:63)
(cid:63)Yale University
{andreas.voellmy, junchang.wang, yang.r.yang, bryan.ford, paul.hudak}@yale.edu
ABSTRACT
Software-Deﬁned Networking offers the appeal of a simple, cen-
tralized programming model for managing complex networks. How-
ever, challenges in managing low-level details, such as setting up
and maintaining correct and efﬁcient forwarding tables on distributed
switches, often compromise this conceptual simplicity. In this pa-
per, we present Maple, a system that simpliﬁes SDN programming
by (1) allowing a programmer to use a standard programming lan-
guage to design an arbitrary, centralized algorithm, which we call
an algorithmic policy, to decide the behaviors of an entire network,
and (2) providing an abstraction that the programmer-deﬁned, cen-
tralized policy runs, conceptually, “afresh” on every packet enter-
ing a network, and hence is oblivious to the challenge of trans-
lating a high-level policy into sets of rules on distributed individ-
ual switches. To implement algorithmic policies efﬁciently, Maple
includes not only a highly-efﬁcient multicore scheduler that can
scale efﬁciently to controllers with 40+ cores, but more importantly
a novel tracing runtime optimizer that can automatically record
reusable policy decisions, ofﬂoad work to switches when possible,
and keep switch ﬂow tables up-to-date by dynamically tracing the
dependency of policy decisions on packet contents as well as the
environment (system state). Evaluations using real HP switches
show that Maple optimizer reduces HTTP connection time by a
factor of 100 at high load. During simulated benchmarking, Maple
scheduler, when not running the optimizer, achieves a throughput
of over 20 million new ﬂow requests per second on a single ma-
chine, with 95-percentile latency under 10 ms.
Categories and Subject Descriptors: C.2.3 [Computer Com-
munication Networks]: Network Operations—Network manage-
ment; D.3.4 [Programming Languages]: Processors—Compilers,
Incremental compilers, Run-time environments, Optimization.
General Terms: Algorithms, Design, Languages, Performance.
Keywords: Software-deﬁned Networking, Policies, Openﬂow.
1.
INTRODUCTION
A major recent development in computer networking is the no-
tion of Software-Deﬁned Networking (SDN), which allows a net-
work to customize its behaviors through centralized policies at a
conceptually centralized network controller. In particular, Open-
ﬂow [13] has made signiﬁcant progress by establishing (1) ﬂow
tables as a standard data-plane abstraction for distributed switches,
(2) a protocol for the centralized controller to install ﬂow rules and
query states at switches, and (3) a protocol for a switch to forward
to the controller packets not matching any rules in its switch-local
ﬂow table. These contributions provide critical components for re-
alizing the vision that an operator conﬁgures a network by writing
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
SIGCOMM’13, August 12–16, 2013, Hong Kong, China.
Copyright 2013 ACM 978-1-4503-2056-6/13/08 ...$15.00.
a simple, centralized network control program with a global view
of network state, decoupling network control from the complexities
of managing distributed state. We refer to the programming of the
centralized controller as SDN programming, and a network oper-
ator who conducts SDN programming as an SDN programmer, or
just programmer.
Despite Openﬂow’s progress, a major remaining component in
realizing SDN’s full beneﬁts is the SDN programming framework:
the programming language and programming abstractions. Exist-
ing solutions require either explicit or restricted declarative spec-
iﬁcation of ﬂow patterns, introducing a major source of complex-
ity in SDN programming. For example, SDN programming using
NOX [8] requires that a programmer explicitly create and manage
ﬂow rule patterns and priorities. Frenetic [7] introduces higher-
level abstractions but requires restricted declarative queries and poli-
cies as a means for introducing switch-local ﬂow rules. However,
as new use cases of SDN continue to be proposed and developed, a
restrictive programming framework forces the programmer to think
within the framework’s - rather than the algorithm’s - structure,
leading to errors, redundancy and/or inefﬁciency.
This paper explores an SDN programming model in which the
programmer deﬁnes network-wide forwarding behaviors with the
application of a high-level algorithm. The programmer simply de-
ﬁnes a function f, expressed in a general-purpose programming
language, which the centralized controller conceptually runs on ev-
ery packet entering the network.
In creating the function f, the
programmer does not need to adapt to a new programming model
but uses standard languages to design arbitrary algorithms for for-
warding input packets. We refer to this model as SDN programming
of algorithmic policies and emphasize that algorithmic policies and
declarative policies do not exclude each other. Our system supports
both, but this paper focuses on algorithmic policies.
The promise of algorithmic policies is a simple and ﬂexible con-
ceptual model, but this simplicity may introduce performance bot-
tlenecks if naively implemented. Conceptually, in this model, the
function f is invoked on every packet, leading to a serious compu-
tational bottleneck at the controller; that is, the controller may not
have sufﬁcient computational capacity to invoke f on every packet.
Also, even if the controller’s computational capacity can scale, the
bandwidth demand that every packet go through the controller may
be impractical. These bottlenecks are in addition to the extra la-
tency of forwarding all packets to the controller for processing [6].
Rather than giving up the simplicity, ﬂexibility, and expressive
power of high-level programming, we introduce Maple, an SDN
programming system that addresses these performance challenges.
As a result, SDN programmers enjoy simple, intuitive SDN pro-
gramming, while achieving high performance and scalability.
Speciﬁcally, Maple introduces two novel components to make
SDN programming with algorithmic policies scalable. First, it in-
troduces a novel SDN optimizer that “discovers” reusable forward-
ing decisions from a generic running control program. Speciﬁ-
cally, the optimizer develops a data structure called a trace tree
that records the invocation of the programmer-supplied f on a spe-
ciﬁc packet, and then generalizes the dependencies and outcome to
other packets. As an example, f may read only one speciﬁc ﬁeld
87of a packet, implying that the policy’s output will be the same for
any packet with the same value for this ﬁeld. A trace tree captures
the reusability of previous computations and hence substantially
reduces the number of invocations of f and, in turn, the computa-
tional demand, especially when f is expensive.
The construction of trace trees also transforms arbitrary algo-
rithms to a normal form (essentially a cached data structure), which
allows the optimizer to achieve policy distribution: the generation
and distribution of switch-local forwarding rules, totally transpar-
ently to the SDN programmer. By pushing computation to dis-
tributed switches, Maple signiﬁcantly reduces the load on the con-
troller as well as the latency. Its simple, novel translation and dis-
tribution technique optimizes individual switch ﬂow table resource
usage. Additionally, it considers the overhead in updating ﬂow ta-
bles and takes advantage of multiple switch-local tables to optimize
network-wide forwarding resource usage.
Maple also introduces a scalable run-time scheduler to com-
plement the optimizer. When ﬂow patterns are inherently non-
localized, the central controller will need to invoke f many times,
leading to scalability challenges. Maple’s scheduler provides sub-
stantial horizontal scalability by using multi-cores.
We prove the correctness of our key techniques, describe a com-
plete implementation of Maple, and evaluate Maple through bench-
marks. For example, using HP switches, Maple optimizer reduces
HTTP connection time by a factor of 100 at high load. Maple’s
scheduler can scale with 40+ cores, achieving a simulated through-
put of over 20 million requests/sec on a single machine.
The aforementioned techniques have limitations, and hence pro-
grammers may write un-scalable f. Worst-case scenarios are that
the computation of f is (1) not reusable (e.g., depending on packet
content), or (2) difﬁcult to parallelize (e.g., using shared states).
Maple cannot make every controller scalable, and Maple program-
mers may need to adjust their designs or goals for scalability.
The rest of the paper is organized as follows. Section 2 motivates
Maple using an example. Section 3 gives an overview of Maple
architecture. Sections 4 and 5 present the details of the optimizer
and scheduler, respectively. We present evaluations in Section 6,
discuss related work in Section 7, and conclude in Section 8.
2. A MOTIVATING EXAMPLE
To motivate algorithmic policies, consider a network whose pol-
icy consists of two parts. First, a secure routing policy: TCP ﬂows
with port 22 use secure paths; otherwise, the default shortest paths
are used. Second, a location management policy:
the network
updates the location (arrival port on ingress switch) of each host.
Specifying the secure routing policy requires algorithmic program-
ming beyond simple GUI conﬁgurations, because the secure paths
are computed using a customized routing algorithm.
To specify the preceding policy using algorithmic policies, an
SDN programmer deﬁnes a function f to be invoked on every packet
pkt arriving at pkt.inport() of switch pkt.switch():
def f(pkt):
srcSw = pkt.switch(); srcInp = pkt.inport()
if locTable[pkt.eth_src()] != (srcSw,srcInp):
invalidateHost(pkt.eth_src())
locTable[pkt.eth_src()] = (srcSw,srcInp)
dstSw = lookupSwitch(pkt.eth_dst())
if pkt.tcp_dst_port() == 22:
outcome.path = securePath(srcSw,dstSw)
else:
outcome.path=shortestPath(srcSw,dstSw)
return outcome
The function f is simple and intuitive. The programmer does not
think about or introduce forwarding rules—it is the responsibility
of the programming framework to derive those automatically.
Unfortunately, current mainstream SDN programming models
force programmers to explicitly manage low-level forwarding rules.
Here is a controller using current programming models:
def packet_in(pkt):
srcSw = pkt.switch(); srcInp = pkt.inport()
locTable[pkt.eth_src()] = (srcSw,srcInp)
dstSw = lookupSwitch(pkt.eth_dst())
if pkt.tcp_dst_port() == 22:
(nextHop,path)=securePath(srcSw,dstSw)
else:
(nextHop,path)=shortestPath(srcSw,dstSw)
fixEndPorts(path,srcSw,srcInp,pkt.eth_dst())
for each sw in path:
inport’ = path.inportAt(sw)
outport’= path.outportAt(sw)
installRule(sw,inport’,
exactMatch(pkt),
[output(outport’)])
forward(srcSw,srcInp,action,nextHop)
We see that the last part of this program explicitly constructs and
installs rules for each switch. The program uses the exactMatch
function to build a match condition that includes all L2, L3, and
L4 headers of the packet, and speciﬁes the switch and incoming
port that the rule pertains to, as well as the action to forward on
port outport’. Thus, the program must handle the complexity
of creating and installing forwarding rules.
Unfortunately, the preceding program may perform poorly, be-
cause it uses only exact match rules, which match at a very gran-
ular level. Every new ﬂow will fail to match at switch ﬂow tables,
leading to a controller RTT. To make matters worse, it may not
be possible to completely cover active ﬂows with granular rules, if
rule space in switches is limited. As a result, rules may need to be
frequently evicted to make room for new rules.
Now, assume that the programmer realizes the issue and decides
to conduct an optimization to use wildcard rules. She might replace
the exact match rule with the following:
installRule(sw,inport’,
{from:pkt.eth_src(), to:pkt.eth_dst()},
[output(outport’)])
Unfortunately, this program has bugs! First, if a host A sends
a TCP packet that is not to port 22 to another host B, then rules
with wildcard will be installed for A to B along the shortest path. If
A later initiates a new TCP connection to port 22 of host B, since
the packets of the new connection match the rules already installed,
they will not be sent to the controller and as a result, will not be sent
along the desired path. Second, if initially host A sends a packet to
port 22 of B, then packets to other ports will be misrouted.
To ﬁx the bugs, the programmer must prioritize rules with appro-
priate match conditions. A program ﬁxing these bugs is:
def packet_in(pkt):
...
if pkt.tcp_dst_port() == 22:
(nextHop,path)=securePath(srcSw,dstSw)
fixEndPorts(path,srcSw,srcInp,pkt.eth_dst())
for each sw in path:
inport’ = path.inportAt(sw)
outport’= path.outportAt(sw)
installRule(sw,inport’,priority=HIGH,
{from:pkt.eth_src(),
to:pkt.eth_dst(),to_tcp_port:22},
[output(output’)])
forward(srcSw,srcInp,action,nextHop)
else:
(nexthop,path)=shortestPath(srcSw,dstSw)
fixEndPorts(path,srcSw,srcInp,pkt.eth_dst())
for each sw in path:
inport’ = path.inportAt(sw)
outport’= path.outportAt(sw)
installRule(sw,inport’,priority=MEDIUM,
88{from:pkt.eth_src(),
to:pkt.eth_dst(),to_tcp_port:22},
[output(toController)])
installRule(sw,inport’,priority=LOW,
{from:pkt.eth_src(),to:pkt.eth_dst()},
[output(outport’)])
forward(srcSw,srcInp,action,nextHop)
This program is considerably more complex. Consider the else
statement. Since it handles non port 22 trafﬁc, it installs wildcard
rules at switches along the shortest path, not the secure path. Al-
though the rules are intended for only non port 22 trafﬁc, since ﬂow
table rules do not support negation (i.e., specifying a condition of
port != 22), it installs wildcard rules that match all trafﬁc from src
to dst. To avoid such a wildcard rule being used by port 22 trafﬁc,
it adds a special rule, whose priority (MEDIUM) is higher than that
of the wildcard rule, to prevent the wildcard from being applied to
port 22 trafﬁc. This is called a barrier rule. Furthermore, the pro-
gram may still use resources inefﬁciently. For example, for some
host pairs, the most secure route may be identical to the shortest
path. In this case, the program will use more rules than necessary.
Comparing the example programs using the current models with
the function f deﬁned at the beginning of this section, we see
the unnecessary burden that current models place on programmers,
forcing them to consider issues such as match granularity, encod-
ing of negation using priorities, and rule dependencies. One might
assume that the recent development of declarative SDN program-
ming, such as using a data query language, may help. But such
approaches require that a programmer extract decision conditions
(e.g., conditional and loop conditions) from an algorithm and ex-
press them declaratively. This may lead to easier composition and
construction of ﬂow tables, but it still places the burden on the pro-
grammer, leading to errors, restrictions, and/or redundancies.
3. ARCHITECTURE OVERVIEW