}
6.1.7 完成作业
所有TaskTracker任务的执行进度信息都会汇总到JobTracker处，当JobTracker接收到最后一个任务的已完成通知后，便把作业的状态设置为“成功”。然后，JobClient也将及时得知任务已成功完成，它会显示一条信息告知用户作业已完成，最后从runJob（）方法处返回（在返回后JobTracker会清空作业的工作状态，并指示TaskTracker也清空作业的工作状态，比如删除中间输出等）。
6.2 错误处理机制
众所周知，Hadoop有很强的容错性。这主要是针对由成千上万台普通机器组成的集群中常态化的硬件故障，Hadoop能够利用冗余数据方式来解决硬件故障，以保证数据安全和任务执行。那么MapReduce在具体执行作业过程中遇到硬件故障会如何处理呢？对于用户代码的缺陷或进程崩溃引起的错误又会如何处理呢？本节将从硬件故障和任务失败两个方面说明MapReduce的错误处理机制。
 6.2.1 硬件故障
从MapReduce任务的执行角度出发，所涉及的硬件主要是JobTracker和TaskTracker（对应从HDFS出发就是NameNode和DataNode）。显然硬件故障就是JobTracker机器故障和TaskTracker机器故障。
在Hadoop集群中，任何时候都只有唯一一个JobTracker。所以JobTracker故障就是单点故障，这是所有错误中最严重的。到目前为止，在Hadoop中还没有相应的解决办法。能够想到的是通过创建多个备用JobTracker节点，在主JobTracker失败之后采用领导选举算法（Hadoop中常用的一种确定Master的算法）来重新确定JobTracker节点。一些企业使用Hadoop提供服务时，就采用了这样的方法来避免JobTracker错误。
机器故障除了JobTracker错误就是TaskTracker错误。TaskTracker故障相对较为常见，并且MapReduce也有相应的解决办法，主要是重新执行任务。下面将详细介绍当作业遇到TaskTracker错误时，MapReduce所采取的解决步骤。
在Hadoop中，正常情况下，TaskTracker会不断地与系统JobTracker通过心跳机制进行通信。如果某TaskTracker出现故障或运行缓慢，它会停止或者很少向JobTracker发送心跳。如果一个TaskTracker在一定时间内（默认是1分钟）没有与JobTracker通信，那么JobTracker会将此TaskTracker从等待任务调度的TaskTracker集合中移除。同时JobTracker会要求此TaskTracker上的任务立刻返回，如果此TaskTracker任务是仍然在mapping阶段的Map任务，那么JobTracker会要求其他的TaskTracker重新执行所有原本由故障TaskTracker执行的Map任务。如果任务是在Reduce阶段的Reduce任务，那么JobTracker会要求其他TashTracker重新执行故障TaskTracker未完成的Reduce任务。比如，一个TaskTracker已经完成被分配的三个Reduce任务中的两个，因为Reduce任务一旦完成就会将数据写到HDFS上，所以只有第三个未完成的Reduce需要重新执行。但是对于Map任务来说，即使TashTracker完成了部分Map, Reduce仍可能无法获取此节点上所有Map的所有输出。所以无论Map任务完成与否，故障TashTracker上的Map任务都必须重新执行。
6.2.2 任务失败
在实际任务中，MapReduce作业还会遇到用户代码缺陷或进程崩溃引起的任务失败等情况。用户代码缺陷会导致它在执行过程中抛出异常。此时，任务JVM进程会自动退出，并向TashTracker父进程发送错误消息，同时错误消息也会写入log文件，最后TasKTracker将此次任务尝试标记失败。对于进程崩溃引起的任务失败，TashTracker的监听程序会发现进程退出，此时TaskTracker也会将此次任务尝试标记为失败。对于死循环程序或执行时间太长的程序，由于TashTracker没有接收到进度更新，它也会将此次任务尝试标记为失败，并杀死程序对应的进程。
在以上情况中，TaskTracker将任务尝试标记为失败之后会将TaskTracker自身的任务计数器减1，以便向JobTracker申请新的任务。TaskTracker也会通过心跳机制告诉JobTracker本地的一个任务尝试失败。JobTracker接到任务失败的通知后，通过重置任务状态，将其加入到调度队列来重新分配该任务执行（JobTracker会尝试避免将失败的任务再次分配给运行失败的TaskTracker）。如果此任务尝试了4次（次数可以进行设置）仍没有完成，就不会再被重试，此时整个作业也就失败了。
6.3 作业调度机制
在0.19.0版本之前，Hadoop集群上的用户作业采用先进先出（FIFO, First Input First Output）调度算法，即按照作业提交的顺序来运行。同时每个作业都会使用整个集群，因此它们只有轮到自己运行才能享受整个集群的服务。虽然FIFO调度器最后又支持了设置优先级的功能，但是由于不支持优先级抢占，所以这种单用户的调度算法仍然不符合云计算中采用并行计算来提供服务的宗旨。从0.19.0版本开始，Hadoop除了默认的FIFO调度器外，还提供了支持多用户同时服务和集群资源公平共享的调度器，即公平调度器（Fair Scheduler Guide）和容量调度器（Capacity Scheduler Guide）。下面主要介绍公平调度器。
公平调度是为作业分配资源的方法，其目的是随着时间的推移，让提交的作业获取等量的集群共享资源，让用户公平地共享集群。具体做法是：当集群上只有一个作业在运行时，它将使用整个集群；当有其他作业提交时，系统会将TaskTracker节点空闲时间片分配给这些新的作业，并保证每一个作业都得到大概等量的CPU时间。
公平调度器按作业池来组织作业，它会按照提交作业的用户数目将资源公平地分到这些作业池里。默认情况下，每一个用户拥有一个独立的作业池，以使每个用户都能获得一份等同的集群资源而不会管它们提交了多少作业。在每一个资源池内，会用公平共享的方法在运行作业之间共享容量。除了提供公平共享方法外，公平调度器还允许为作业池设置最小的共享资源，以确保特定用户、群组或生产应用程序总能获取到足够的资源。对于设置了最小共享资源的作业池来说，如果包含了作业，它至少能获取到最小的共享资源。但是如果最小共享资源超过作业需要的资源时，额外的资源会在其他作业池间进行切分。
在常规操作中，当提交一个新作业时，公平调度器会等待已运行作业中的任务完成，以释放时间片给新的作业。但公平调度器也支持作业抢占。如果新的作业在一定时间（即超时时间，可以配置）内还未获取公平的资源分配，公平调度器就会允许这个作业抢占已运行作业中的任务，以获取运行所需要的资源。另外，如果作业在超时时间内获取的资源不到公平共享资源的一半时，也允许对任务进行抢占。而在选择时，公平调度器会在所有运行任务中选择最近运行起来的任务，这样浪费的计算相对较少。由于Hadoop作业能容忍丢失任务，抢占不会导致被抢占的作业失败，只是让被抢占作业的运行时间更长。
最后，公平调度器还可以限制每个用户和每个作业池并发运行的作业数量。这个限制可以在用户一次性提交数百个作业或当大量作业并发执行时用来确保中间数据不会塞满集群上的磁盘空间。超出限制的作业会被列入调度器的队列中进行等待，直到早期作业运行完毕。公平调度器再根据作业优先权和提交时间的排列情况从等待作业中调度即将运行的作业。
6.4 Shuffle和排序
从前面的介绍中我们得知，Map的输出会经过一个名为shuffle的过程交给Reduce处理（在“MapReduce数据流”图中也可以看出），当然也有Map的结果经过sort-merge交给Reduce处理的。其实在MapReduce流程中，为了让Reduce可以并行处理Map结果，必须对Map的输出进行一定的排序和分割，然后再交给对应的Reduce，而这个将Map输出进行进一步整理并交给Reduce的过程就成为了shuffle。从shuffle的过程可以看出，它是MapReduce的核心所在，shuffle过程的性能与整个MapReduce的性能直接相关。
总体来说，shuffle过程包含在Map和Reduce两端中。在Map端的shuffle过程是对Map的结果进行划分（partition）、排序（sort）和分割（spill），然后将属于同一个划分的输出合并在一起（merge）并写在磁盘上，同时按照不同的划分将结果发送给对应的Reduce（Map输出的划分与Reduce的对应关系由JobTracker确定）。Reduce端又会将各个Map送来的属于同一个划分的输出进行合并（merge），然后对merge的结果进行排序，最后交给Reduce处理。下面将从Map和Reduce两端详细介绍shuffle过程。
 6.4.1 Map端
从MapReduce的程序中可以看出，Map的输出结果是由collector处理的，所以Map端的shuffle过程包含在collect函数对Map输出结果的处理过程中。下面从具体的代码来分析Map端的shuffle过程。
首先从collect函数的代码入手（MapTask类）。从下面的代码段可以看出Map函数的输出内存缓冲区是一个环形结构。
final int kvnext=（kvindex+1）%kvoffsets.length；
当输出内存缓冲区内容达到设定的阈值时，就需要把缓冲区内容分割（spill）到磁盘中。但是在分割的时候Map并不会阻止继续向缓冲区中写入结果，如果Map结果生成的速度快于写出速度，那么缓冲区会写满，这时Map任务必须等待，直到分割写出过程结束。这个过程可以参考下面的代码。
do{
//在环形缓冲区中，如果下一个空闲位置同起始位置相等，那么缓冲区
//已满
kvfull=kvnext==kvstart；
//环形缓冲区的内容是否达到写出的阈值
final boolean kvsoftlimit=（（kvnext＞kvend）
?kvnext-kvend＞softRecordLimit
：kvend-kvnext＜=kvoffsets.length-softRecordLimit）；
//达到阈值，写出缓冲区内容，形成spill文件
if（kvstart==kvend＆＆kvsoftlimit）{
startSpill（）；
}
//如果缓冲区满，则Map任务等待写出过程结束
if（kvfull）{
while（kvstart！=kvend）{
reporter.progress（）；
spillDone.await（）；
}
}
}while（kvfull）；
在collect函数中将缓冲区中的内容写出时会调用sortAndSpill函数。sortAndSpill每被调用一次就会创建一个spill文件，然后按照key值对需要写出的数据进行排序，最后按照划分的顺序将所有需要写出的结果写入这个spill文件中。如果用户作业配置了combiner类，那么在写出过程中会先调用combineAndSpill（）再写出，对结果进行进一步合并（combine）是为了让Map的输出数据更加紧凑。sortAndSpill函数的执行过程可以参考下面sortAndSpill函数的代码。
//创建spill文件
Path filename=mapOutputFile.getSpillFileForWrite（numSpills, size）；
out=rfs.create（filename）；
……
//按照key值对待写出数据进行排序
sorter.sort（MapOutputBuffer.this, kvstart, endPosition, reporter）；
……
//按照划分将数据写入文件
for（int i=0；i＜partitions；++i）{
IFile.Writer＜K, V＞writer=null；
long segmentStart=out.getPos（）；
writer=new Writer＜K, V＞（job, out, keyClass, valClass, codec, spilledRecordsCounter）；
//如果没有配置combiner类，数据直接写入文件
if（null==combinerClass）{
……
}
else{
……
//如果配置了combiner类，则先调用combineAndSpill函
//数后再写入文件
combineAndSpill（kvIter, combineInputCounter）；
}
}
显然，直接将每个Map生成的众多spill文件（因为Map过程中，每一次缓冲区写出都会产生一个spill文件）交给Reduce处理不现实。所以在每个Map任务结束之后在Map的TaskTracker上还会执行合并操作（merge），这个操作的主要目的是将Map生成的众多spill文件中的数据按照划分重新组织，以便于Reduce处理。主要做法是针对指定的分区，从各个spill文件中拿出属于同一个分区的所有数据，然后将它们合并在一起，并写入一个已分区且已排序的Map输出文件中。这个过程的详细情况请参考mergeParts（）函数的代码，这里不再列出。
待唯一的已分区且已排序的Map输出文件写入最后一条记录后，Map端的shuffle阶段就结束了。下面就进入Reduce端的shuffle阶段。
6.4.2 Reduce端
在Reduce端，shuffle阶段可以分成三个阶段：复制Map输出、排序合并和Reduce处理。下面按照这三个阶段进行详细介绍。
如前文所述，Map任务成功完成后，会通知父TashTracker状态已更新，TaskTracker进而通知JobTracker（这些通知在心跳机制中进行）。所以，对于指定作业来说，JobTracker能够记录Map输出和TaskTracker的映射关系。Reduce会定期向JobTracker获取Map的输出位置。一旦拿到输出位置，Reduce任务就会从此输出对应的TaskTracker上复制输出到本地（如果Map的输出很小，则会被复制到执行Reduce任务的TaskTracker节点的内存中，便于进一步处理，否则会放入磁盘），而不会等到所有的Map任务结束。这就是Reduce任务的复制阶段。
在Reduce复制Map的输出结果的同时，Reduce任务就进入了合并（merge）阶段。这一阶段主要的任务是将从各个Map TaskTracker上复制的Map输出文件（无论在内存还是在磁盘）进行整合，并维持数据原来的顺序。
reduce端的最后阶段就是对合并的文件进行reduce处理。下面是reduce Task上run函数的部分代码，从这个函数可以看出整个Reduce端的三个步骤。
//复制阶段，从map TaskTracker处获取Map输出
boolean isLocal="local".equals（job.get（"mapred.job.tracker"，"local"））；
if（！isLocal）{
reduceCopier=new ReduceCopier（umbilical, job, reporter）；
if（！reduceCopier.fetchOutputs（））{
……
}
}
//复制阶段结束
copyPhase.complete（）；
//合并阶段，将得到的Map输出合并
setPhase（TaskStatus.Phase.SORT）；
……
//合并阶段结束
sortPhase.complete（）；
//Reduce阶段
setPhase（TaskStatus.Phase.REDUCE）；
//启动Reduce
Class keyClass=job.getMapOutputKeyClass（）；
Class valueClass=job.getMapOutputValueClass（）；
RawComparator comparator=job.getOutputValueGroupingComparator（）；
if（useNewApi）{
runNewReducer（job, umbilical, reporter, rIter, comparator, keyClass, valueClass）；
}else{
runOldReducer（job, umbilical, reporter, rIter, comparator, keyClass, valueClass）；
}
done（umbilical, reporter）；
}
6.4.3 shuffle过程的优化
熟悉了上面介绍的shuffle过程，可能有读者会说：这个shuffle过程不是最优的。是的，Hadoop采用的shuffle过程并不是最优的。举个简单的例子，如果现在需要Hadoop集群完成两个集合的并操作，事实上并操作只需要让两个集群中重复的元素在最后的结果中出现一次就可以了，并不要求结果的元素是按顺序排列的。但是如果使用Hadoop默认的shuffle过程，那么结果势必是排好序的，显然这个处理就不是必须的了。在这里简单介绍从Hadoop参数的配置出发来优化shuffle过程。在一个任务中，完成单位任务使用时间最多的一般都是I/O操作。在Map端，主要就是shuffle阶段中缓冲区内容超过阈值后的写出操作。可以通过合理地设置ip.sort.*属性来减少这种情况下的写出次数，具体来说就是增加io.sort.mb的值。在Reduce端，在复制Map输出的时候直接将复制的结果放在内存中同样能够提升性能，这样可以让部分数据少做两次I/O操作（前提是留下的内存足够Reduce任务执行）。所以在Reduce函数的内存需求很小的情况下，将mapred.inmem.merge.threshold设置为0，将mapreed.job.reduce.input.buffer.percent设置为1.0（或者一个更低的值）能够让I/O操作更少，提升shuffle的性能。
6.5 任务执行
本章前面详细介绍了MapReduce作业的执行流程，也简单介绍了基于Hadoop自身的一些参数优化。本节再介绍一些Hadoop在任务执行时的具体策略，让读者进一步了解MapReduce任务的执行细节，以便控制细节。
 6.5.1 推测式执行
所谓推测式执行是指当作业的所有任务都开始运行时，JobTracker会统计所有任务的平均进度，如果某个任务所在的TaskTracker节点由于配置比较低或CPU负载过高，导致任务执行的速度比总体任务的平均速度要慢，此时JobTracker就会启动一个新的备份任务，原有任务和新任务哪个先执行完就把另外一个kill掉，这就是经常在JobTracker页面看到任务执行成功、但是总有些任务被kill的原因。
MapReduce将待执行作业分割成一些小任务，然后并行运行这些任务，提高作业运行的效率，使作业的整体执行时间少于顺序执行的时间。但很明显，运行缓慢的任务（可能因为配置问题、硬件问题或CPU负载过高）将成为MapReduce的性能瓶颈。因为只要有一个运行缓慢的任务，整个作业的完成时间将被大大延长。这个时候就需要采用推测式执行来避免出现这种情况。当JobTracker检测到所有任务中存在运行过于缓慢的任务时，就会启动另一个相同的任务作为备份。原始任务和备份任务中只要有一个完成，另一个就会被中止。推测式执行的任务只有在一个作业的所有任务开始执行之后才会启动，并且只针对运行一段时间之后、执行速度慢于整个作业的平均执行速度的情况。
推测式执行在默认情况下是启用的。这种执行方式有一个很明显的缺陷：对于由于代码缺陷导致的任务执行速度过慢，它所启用的备份任务并不会解决问题。除此之外，因为推测式执行会启动新的任务，所以这种执行方式不可避免地会增加集群的负担。所以在利用Hadoop集群运行作业的时候可以根据具体情况选择开启或关闭推测式执行策略（通过设置mapred.map.tasks.speculative.execution和mapred.reduce.tasks.speculative.execution属性的值来为Map和Reduce任务开启或关闭推测式执行策略）。
6.5.2 任务JVM重用
在本章图6-1中可以看出，不论是Map任务还是Reduce任务，都是在TaskTracker节点上的Java虚拟机（JVM）中运行的。当TaskTracker被分配一个任务时，就会在本地启动一个新的Java虚拟机来运行这个任务。对于有大量零碎输入文件的Map任务而言，为每一个Map任务启动一个Java虚拟机这种做法显然还有很大的改善空间。如果在一个非常短的任务结束之后让后续的任务重用此Java虚拟机，这样就可以省下新任务启动新的Java虚拟机的时间，这就是所谓的任务JVM重用。需要注意的是，虽然一个TaskTracker上可能会有多个任务在同时运行，但这些正在执行的任务都是在相互独立的JVM上的。TaskTracker上的其他任务必须等待，因为即使启用JVM重用，JVM也只能顺序执行任务。
控制JVM重用的属性是mapred.job.reuse.jvm.num.tasks。这个属性定义了单个JVM上运行任务的最大数目，默认情况下是1，意味着每个JVM上运行一个任务。可以将这个属性设置为一个大于1的值来启用JVM重用，也可以将此属性设为-1，表明共享此JVM的任务数目不受限制。
6.5.3 跳过坏记录
MapReduce作业处理的数据集非常庞大，用户在基于MapReduce编写处理程序时可能并不会考虑到数据集中的每一种数据格式和字段（特别是某些坏的记录）。所以，用户代码在处理数据集中的某个特定记录时可能会崩溃。这个时候即使MapReduce有错误处理机制，但是由于存在这种代码缺陷，即使重新执行4次（默认的最大重新执行次数），这个任务仍然会失败，最终也会导致整个作业失败。所以针对这种由于坏数据导致任务抛出的异常，重新运行任务是无济于事的。但是，如果想要在庞大的数据集中找出这个坏记录，然后在程序中添加相应的处理代码或直接除去这条坏记录，显然也是很困难的一件事情，况且并不能保证没有其他坏记录。所以最好的办法就是在当前代码对应的任务执行期间，遇到坏记录时就直接跳过去（由于数据集巨大，忽略这种极少数的坏记录是可以接受的），然后继续执行，这就是Hadoop中的忽略模式（skipping模式）。当忽略模式启动时，如果任务连续失败两次，它会将自己正在处理的记录告诉TaskTracker，然后TaskTracker会重新运行该任务并在运行到先前任务报告的记录时直接跳过。从忽略模式的工作方式可以看出，忽略模式只能检测并忽略一个错误记录，因此这种机制仅适用于检测个别错误记录。如果增加任务尝试次数最大值（这由mapred.map.max.attemps和mapred.reduce.max.attemps两个属性决定），可以增加忽略模式能够检测并忽略的错误记录数目。默认情况下忽略模式是关闭的，可以使用SkipBadRedcord类单独为Map和Reduce任务启用它。
6.5.4 任务执行环境
Hadoop能够为执行任务的TaskTracker提供执行所需要的环境信息。例如，Map任务可以知道自己所处理文件的名称、自己在作业任务群中的ID号等。JobTracker分配任务给TaskTracker时，就会将作业的配置文件发送给TaskTracker, TaskTracker将此文件保存在本地。从本章前面的介绍中我们知道，TaskTracker是在本节点单独的JVM上以子进程的形式执行Map或Reduce任务的。所以启动Map或Reduce Task时，会直接从父TaskTracker处继承任务的执行环境。图6-2列出了每个Task执行时使用的本地参数（从作业配置中获取，返回给Task的是配置信息）。
图 6-2 Task的本地参数表
当Job启动时，TaskTracker会根据配置文件创建Job和本地缓存。TaskTracker的本地目录是${mapred.local.dir}/taskTracker/。在这个目录下有两个子目录：一个是作业的分布式缓存目录，路径是在本地目录后面加上archive/：一个是本地Job目录，路径是在本地目录后面加上jobcache/$jobid/，在这个目录下保存了Job执行的共享目录（各个任务可以使用这个空间作为暂存空间，用于任务之间的文件共享，此目录通过job.local.dir参数暴露给用户）、存放JAR包的目录（保存作业的JAR文件和展开的JAR文件）、一个XML文件（此XML文件是本地通用的作业配置文件）和根据任务ID分配的任务目录（每个任务都有一个这样的目录，目录中包含本地化的任务作业配置文件，存放中间结果的输出文件目录、任务当前工作目录和任务临时目录）。
关于任务的输出文件需要注意的是，应该确保同一个任务的多个实例不会尝试向同一个文件进行写操作。因为这可能会存在两个问题，第一个问题是，如果任务失败并被重试，那么会先删除第一个任务的旧文件；第二个问题是，在推测式执行的情况下同一任务的两个实例会向同一个文件进行写操作。Hadoop通过将输出写到任务的临时文件夹来解决上面的两个问题。这个临时目录是{mapred.out put.dir}/_temporary/${mapred.task.id}。如果任务执行成功，目录的内容（任务输出）就会被复制到此作业的输出目录（${mapred.out.put.dir}）。因此，如果一个任务失败并重试，第一个任务尝试的部分输出就会被消除。同时推测式执行时的备份任务和原始任务位于不同的工作目录，它们的临时输出文件夹并不相同，只有先完成的任务才会把其工作目录中的输出内容传到输出目录中，而另外一个任务的工作目录就会被丢弃。
6.6 本章小结
本章从MapReduce程序中的JobClient.runJob（conf）开始，给出了MapReduce执行的流程图，并分析了流程图中的四个核心实体，结合实际代码介绍了MapReduce执行的详细流程。MapReduce的执行流程简单概括如下：用户作业执行JobClient.runJob（conf）代码会在Hadoop集群上将其启动。启动之后JobClient实例会向JobTracker获取JobId，而且客户端会将作业执行需要的作业资源复制到HDFS上，然后将作业提交给JobTracker。JobTracker在本地初始化作业，再从HDFS作业资源中获取作业输入的分割信息，根据这些信息JobTracker将作业分割成多个任务，然后分配给在与JobTracker心跳通信中请求任务的TaskTracker。TaskTracker接收到新的任务之后会先从HDFS上获取作业资源，包括作业配置信息和本作业分片的输入，然后在本地启动一个JVM并执行任务。任务结束之后将结果写回HDFS。
介绍完MapReduce作业的详细流程后，本章还重点介绍了MapReduce中采用的两种机制，分别是错误处理机制和作业调度机制。在错误处理机制中，如果遇到硬件故障，MapReduce会将故障节点上的任务分配给其他节点处理。如果遇到任务失败，则会重新执行。在作业调度机制中，主要介绍了公平调度器。这种调度策略能够按照提交作业的用户数目将资源公平地分到用户的作业池中，以达到用户公平共享整个集群的目的。
本章最后介绍了MapReduce中两个流程的细节，分别是shuffle和任务执行。在shuffle中，从代码入手介绍了Map端和Reduce端的shuffle过程及shuffle的优化。shuffle的过程可以概括为：在Map端，当缓冲区内容达到阈值时Map写出内容。写出时按照key值对数据排序，再按照划分将数据写入文件，然后进行merge并将结果交给Reduce。在Reduce端，TaskTracker先从执行Map的TaskTracker节点上复制Map输出，然后对排序合并，最后进行Reduce处理。关于任务执行则主要介绍了三个任务执行的细节，分别是推测式执行、JVM重用和执行环境。推测式执行是指JobTracker在作业执行过程中，发现某个作业执行速度过慢，为了不影响整个作业的完成进度，会启动和这个作业完全相同的备份作业让TaskTracker执行，最后保留二者中较快完成的结果。JVM重用主要是针对比较零碎的任务，对于新任务不是启动新的JVM，而是在先前任务执行完毕的JVM上直接执行，这样节省了JVM启动的时间。在任务执行环境中主要介绍了任务执行参数的内容和任务目录结构，以及任务临时文件夹的使用情况。
第7章 Hadoop I/O操作
本章内容
I/O操作中的数据检查
数据的压缩
数据的I/O中序列化操作
针对Mapreduce的文件类
本章小结
Hadoop工程下与I/O相关的包如下：
org. apache.hadoop.io
org. apache.hadoop.io.compress
org. apache.hadoop.io.file.tfile
org. apache.hadoop.io.serializer
org. apache.hadoop.io.serializer.avro
除了org.apache.hadoop.io.serializer.avro用于为Avro（与Hadoop相关的Apache的另一个顶级项目）提供数据序列化操作外，其余都是用于Hadoop的I/O操作。
除此以外，部分fs类中的内容也与本章有关，所以本章也会提及一些，不过大都是一些通用的东西，由于对HDFS的介绍不是本章的重点，在此不会详述。
可以说，Hadoop的I/O由传统的I/O操作而来，但是又有些不同。第一，在我们常见的计算机系统中，数据是集中的，无论多少电影、音乐或者Word文档，它只会存在于一台主机中，而Hadoop则不同，Hadoop系统中的数据经常是分散在多个计算机系统中的；第二，一般而言，传统计算机系统中的数据量相对较小，大多在GB级别，而Hadoop处理的数据经常是PB级别的。