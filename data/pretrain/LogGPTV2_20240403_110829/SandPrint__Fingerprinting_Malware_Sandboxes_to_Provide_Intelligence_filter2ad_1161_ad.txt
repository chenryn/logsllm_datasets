sandboxes and use the gathered information to build a classiﬁer that can per-
fectly distinguish between a user PC and an appliance. With insider knowledge
on security appliances, an advanced attacker could tweak her classiﬁer such that
the evasion is stealthy and remains undetected by the appliance.
7 Discussion and Limitations
This section discusses ethical aspects and potential limitations of our work. As
part of the ethical discussion, we also describe a responsible disclosure process
in which we informed the sandbox and appliance operators about our ﬁndings.
7.1 Ethical Considerations
Our research may seem oﬀensive in the sense that we reveal ﬁngerprints of mal-
ware sandboxes that adversaries can use to evade them. Note, however, that the
information we presented can be gathered by any other person reproducing our
(conceptually simple) ﬁngerprinting method. We thus consider the information
shown in this paper as public knowledge. Still, we present data only in aggregated
form and refrain from revealing any internals of particular sandboxes.
SandPrint: Fingerprinting Malware Sandboxes
181
Using our insights, sandbox operators can aim to implement stealthier analy-
sis systems. For example, we have shown that one should periodically update
features that are inherent to the snapshot of a sandbox. While it will always
be possible to ﬁnd artifacts that can identify an individual sandbox, it is sig-
niﬁcantly harder to build a classiﬁer that works for all sandboxes, especially if
more people randomize characteristics. We have shown which features are par-
ticularly characteristic of sandboxes, giving sandbox operators hints on where
to signiﬁcantly improve the stealthiness of their systems.
7.2 Responsible Disclosure
Organizations developing sandboxes and/or appliances are immediately aﬀected
by our research results and we thus considered them as the main target of
our responsible disclosure process. To notify these organizations, we contacted
them 90 days prior to the publishing date of this paper, detailing the proposed
attack and including hints on how to protect against potential adversaries in the
future. We used direct contacts whenever possible and available. Alternatively,
we resorted to contact details stated on the organization’s websites, notably
including Web-based contact forms. If we did not receive a response after 2 weeks,
we retried to contact the organization, if possible using alternative communica-
tion channels (e.g., using generic email addresses like PI:EMAIL
or email addresses found in the WHOIS database for the organization’s web-
site domain). If we did not hear back from the organization after 4 weeks, we
contacted the national CERT(s) that are in the same country as the aﬀected
organization in order to notify the party via the CERT as trusted intermediary.
We handed to each organization an executive summary of our research results
as well as a full description of our research methodology (i.e., a copy of this
paper in the pre-print version). We made sure to highlight the implications of
our work with respect to future operations of the sandbox and/or appliance.
We also speciﬁed our contact details of both research institutions, including
physical address, phone number, and the email address of a representative for the
research activities. We allowed the organizations to download the latest version of
SandPrint and its source code. Such auxiliary data is helpful to build protection
mechanisms against sandbox-evasive programs similar to SandPrint. We also
remove all organizations’ names when referring to individual sandboxes/services.
7.3 Isolated Sandboxes
Most sandboxes allowed the program under analysis to communicate over the
Internet, whereas nine services and all three appliances did not do so. To some
extent we could also extract features of isolated sandboxes (the appliances) by
encoding the features into events of the analysis report. However, this requires
access to the isolated sandboxes, which may be hard to obtain for an attacker.
Note that our sandbox classiﬁcation did not use features that depend on the
network conﬁguration. In principle, our classiﬁcation results should also gener-
alize to non-connected sandboxes. Although we cannot rule out the possibility
182
A. Yokoyama et al.
that there are non-connected sandboxes for which our classiﬁer would perform
poorly, we argue that the successful detection of appliances supports this claim.
Due to our assumption of Internet-connected sandboxes, the number of in-
the-wild sandboxes is likely higher than our ﬁndings in the clustering results
suggest. We argue, though, that our analyses are based on a statistically signif-
icant set of sandboxes, including those of the most popular analysis services.
8 Related Work
Evasion Techniques: Seeing the increasing popularity of sandboxes, malware
authors try to ﬁnd a way to evade sandbox analysis. Egele et al. give an overview
of sandbox implementations [24]. Most sandboxes use virtual machine (VM)
technology or CPU emulators. Such virtualization eases the process of ana-
lyzing multiple samples in parallel. Accordingly, studies show how to distin-
guish between a real machine and virtual environment. RedPill [47] determines
whether it is executed on VMware using the sidt instruction. Many other detec-
tion methods have also been developed for not only VMware [29,43], but also
for famous system emulators such as QEMU [22,29,40,43] and BOCHS [34,40].
There are also some detection methods for emulation-based Android sand-
boxes [27,42,54]. The fundamental diﬀerence between our approach and the
above techniques is that we do not aim to detect virtualization or emulation, as
VMs and sandboxes are not equivalent. In addition, as shown with Pafish, most
of these checks are not stealthy, whereas our approach even managed to detect
security appliances without triggering alerts. It is also likely that our approach
could work for bare metal sandboxes. We argue that bare metal sandboxes con-
ceptually share many sandbox-inherent features with traditional sandboxes as
the major diﬀerence is only the absence of virtualization and emulation—not
the snapshot mechanism.
The work closest to our approach has been done by Maier et al. [33]. They
gathered several features about Android sandboxes and showed that Android
malware can bypass the existing sandboxes by using the ﬁngerprints. However,
they do neither perform automated clustering and classiﬁcation of sandbox-
inherent features, nor do they test their approach against security appliances.
Furthermore, the feature selection of Maier et al. is very speciﬁc to smartphones.
Features such as “the device needs at least n saved WiFi-networks” or “the device
must have a paired Bluetooth device” cannot be used in our (non-mobile) con-
text in a meaningful way. However, we also use some similar features like special
hardware artifacts or system uptime. Regarding sandboxes for Windows mal-
ware, Yoshioka et al. [56] clustered and detected sandboxes by their external IP
addresses. We were inspired by these works and performed a study in greater
detail, collecting 25 features and identifying 76 sandboxes with an unsupervised
machine learning technique.
SandPrint: Fingerprinting Malware Sandboxes
183
Transparent Sandboxes: Seeing the threat of VM evasion, researchers
started to explore transparent sandboxes that are stealthy against detection.
Vasudevan et al. proposed Cobra [53], which was the ﬁrst analysis system coun-
tering anti-analysis techniques. Dinaburg et al. proposed Ether [23], a transpar-
ent sandbox using hardware virtualization extensions such as Intel VT. Those
systems focus on how to conceal the existence of analysis mechanisms from
malware. Pek et al. introduced a timing-based detection mechanism to detect
Ether [41]. In addition, as we have shown, the majority of sandboxes, includ-
ing VT-based sandboxes, are susceptible to evasion due to sandbox-inherent
features.
Kirat et al. proposed to use actual hardware to analyze malware [9,31]. The
proposed system, called BareBox, is based on a fast and rebootless system restore
technique. Since the system executes malware on real hardware, it is not vul-
nerable to any type of VM/emulation-based detection attacks. Still, as it is
snapshot-based, it falls for the methods described in Sect. 5.
9 Conclusion
Our real-world malware sandbox investigations have shown it is quite straightfor-
ward to ﬁngerprint malware sandboxes. We identiﬁed 76 sandboxes by uploading
a measurement binary to 20 services, all of which can be rather trivially detected
and evaded just based on sandbox-inherent characteristics. Our ﬁndings also sug-
gest detecting and evading malware appliances is similarly possible. This calls
into question how we can protect against the threat of sandbox evasion in the
future, and should serve as a heads-up for sandbox operators to inform them
about threats that may actually be already silently misused by malware.
Acknowledgements. We would like to thank the anonymous reviewers for their valu-
able comments. Special thanks goes to our shepherd Michael Bailey, who supported
us during the process of ﬁnalizing the paper. This work was supported by the MEXT
Program for Promoting Reform of National Universities and by the German Federal
Ministry of Education and Research (BMBF) through funding for the Center for IT-
Security, Privacy and Accountability (CISPA) and for the BMBF project 13N13250.
Appendix
See Fig. 3.
184
A. Yokoyama et al.
Fig. 3. Mapping between submitted SandPrint instances and sandboxes. The non-
circle shapes indicate constant and exclusive use of a sandbox by a particular service
and thus are inferred as being a sandbox attached to the service. A cross indicates that
the mapping is conﬁrmed by mapping the SandPrint report to the dynamic analysis
report provided by the service.
SandPrint: Fingerprinting Malware Sandboxes
185
References
1. Amnpardaz Sandbox - File Analyzer. http://jevereg.amnpardaz.com/
2. Anubis: Malware Analysis for Unknown Binaries. https://anubis.iseclab.org/
3. Bkav - Scan virus online. http://quetvirus.vn/default.aspx?lang=en
4. bochs: The Open Source IA-32 Emulation Project. http://bochs.sourceforge.net
5. Dr. Web Online Check. http://online.drweb.com/?lng=en
6. FortiGuard Center. Online Virus
http://www.fortiguard.com/
Scanner.
virusscanner
7. Gary‘s Hood. Online Virus Scanner. http://www.garyshood.com/virus/
8. Malwr - Malware Analysis by Cuckoo Sandbox. https://malwr.com/
9. NVMTrace: Proof-of-concept Automated Baremetal Malware Analysis Framework.
https://code.google.com/p/nvmtrace/
10. Oracle VM VirtualBox. https://www.virtualbox.org
11. #totalhash. https://totalhash.cymru.com/upload/
12. http://www.Vicheck.ca
13. Virusblokada. http://anti-virus.by/en/index.shtml
14. VirusTotal
- Free Online Virus, Malware and URL Scanner. https://www.
virustotal.com/en/
15. VMware. http://www.vmware.com/
16. Bayer, U., Milani Comparetti, P., Hlauschek, C., Kruegel, C., Kirda, E.: Scalable,
behavior-based malware clustering. In: Network and Distributed System Security
Symposium (NDSS) (2009)
17. Bellard, F.: QEMU, a fast and portable dynamic translator. In: Proceedings of the
Annual Conference on USENIX Annual Technical Conference, ATEC 2005 (2005)
18. Brengel, M., Backes, M., Rossow, C.: Detecting hardware-assisted virtualization.
In: Caballero, J., Zurutuza, U., Rodr´ıguez, R.J. (eds.) DIMVA 2016. LNCS, vol.
9721, pp. 207–227. Springer, Heidelberg (2016). doi:10.1007/978-3-319-40667-1 11
19. Caballero, J., Grier, C., Kreibich, C., Paxson, V.: Measuring pay-per-install: the
commoditization of malware distribution. In: USENIX Security (2011)
20. Comodo. Comodo Instant Malware Analysis. http://camas.comodo.com/
21. Cristianini, N., Shawe-Taylor, J.: An Introduction to Support Vector Machines and
Other Kernel-based Learning Methods. Cambridge University Press, Cambridge
(2000)
22. DEXLabs. Detecting Android Sandboxes (2012). http://www.dexlabs.org/blog/
btdetect
23. Dinaburg, A., Royal, P., Sharif, M., Ether, L.W.: Malware analysis via hardware
virtualization extensions. In: Proceedings of the 15th ACM Conference on Com-
puter and Communications Security, CCS 2008 (2008)
24. Egele, M., Scholte, T., Kirda, E., Kruegel, C.: A survey on automated dynamic
malware-analysis techniques and tools. ACM Comput. Surv. 44, 2 (2008)
25. F-Secure. Sample Analysis System. https://analysis.f-secure.com/portal/login.
html
26. Freiling, F.C., Holz, T., Wicherski, G.: Botnet tracking: exploring a root-cause
methodology to prevent distributed denial-of-service attacks. In: di Vimercati, S.C.,
Syverson, P.F., Gollmann, D. (eds.) ESORICS 2005. LNCS, vol. 3679, pp. 319–335.
Springer, Heidelberg (2005)
27. Jing, Y., Zhao, Z., Ahn, G.-J., Hu, H.: Morpheus: automatically generating heuris-
tics to detect android emulators. In: Proceedings of the 30th Annual Computer
Security Applications Conference, ACSAC 2014 (2014)
186
A. Yokoyama et al.
28. Jotti. Jotti’s Malware Scan. http://virusscan.jotti.org/en
29. Jung, P.: Bypassing Sandboxes for Fun. https://www.botconf.eu/wp-content/
uploads/2014/12/2014-2.7-Bypassing-Sandboxes-for-Fun.pdf
30. Kirat, D., Vigna, G., Kruegel, C.: Barecloud: bare-metal analysis-based evasive
malware detection. In: Proceedings of the 23rd USENIX Conference on Security
Symposium, SEC 2014 (2014)
31. Kirati, D., Vigna, G., Kruegel, C.: BareBox: eﬃcient malware analysis on bare-
metal. In: Proceedings of the 27th Annual Computer Security Applications Con-
ference, ACSAC 2011 (2011)
32. Lanzi, A., Balzarotti, D., Kruegel, C., Christodorescu, M., Kirda, E.: AccessMiner:
using system-centric models for malware protection. In: Proceedings of the 17th
ACM Conference on Computer and Communications Security, CCS 2010 (2010)
33. Maier, D., M¨uller, T., Protsenko, M.: Divide-and-Conquer: why android malware
cannot be stopped. In: Proceedings of the 2014 Ninth International Conference on
Availability, Reliability and Security, ARES 2014 (2014)
34. Martignoni, L., Paleari, R., Roglia, G.F., Bruschi, D.: Testing CPU emulators. In:
Proceedings of the Eighteenth International Symposium on Software Testing and
Analysis, ISSTA 2009 (2009)
35. Microsoft. Submit a sample - Microsoft Malware Protection Center. https://www.
microsoft.com/security/portal/submission/submit.aspx
36. Neugschwandtner, M., Comparetti, P. M., Platzer, C.: Detecting malware’s failover
C&C strategies with squeeze. In: Proceedings of the 27th Annual Computer Secu-
rity Applications Conference, ACSAC 2011 (2011)
37. Neuner, S., van der Veen, V., Lindorfer, M., Huber, M., Merzdovnik, G., Mulazzani,
M., Weippl, E.: Enter Sandbox: Android Sandbox Comparison (2015). http://
arxiv.org/ftp/arxiv/papers/1410/1410.7749.pdf
38. OPSWAT. Metascan Online: Free File Scanning with Multiple Antivirus Engines.
https://www.metascan-online.com/#!/scan-ﬁle
39. Pa, Y.M.P., Suzuki, S., Yoshioka, K., Matsumoto, T., Kasama, T., Rossow, C.: IoT-
POT: analysing the rise of IoT compromises. In: Proceedings of the 9th USENIX
Workshop on Oﬀensive Technologies, WOOT (2015)
40. Paleari, R., Martignoni, L., Roglia, G.F., Bruschi, D.A.: Fistful of red-pills: how
to automatically generate procedures to detect CPU emulators. In: Proceedings of
the 3rd USENIX Conference on Oﬀensive Technologies, WOOT 2009 (2009)
41. P´ek, G., Bencs´ath, B., Butty´an, L.: nEther: in-guest detection of out-of-the-guest
malware analyzers. In: Proceedings of the Fourth European Workshop on System
Security, EUROSEC 2011 (2011)
42. Petsas, T., Voyatzis, G., Athanasopoulos, E., Polychronakis, M., Ioannidis, S.: Rage
against the virtual machine: hindering dynamic analysis of android malware. In:
Proceedings of the Seventh European Workshop on System Security, EuroSec 2014
(2014)
43. Raﬀetseder, T., Kruegel, C., Kirda, E.: Detecting system emulators. In: Garay,
J.A., Lenstra, A.K., Mambo, M., Peralta, R. (eds.) ISC 2007. LNCS, vol. 4779, pp.
1–18. Springer, Heidelberg (2007)
44. Rieck, K., Schwenk, G., Limmer, T., Holz, T., Laskov, P.: Botzilla: detecting the
phoning home of malicious software. In: Proceedings of the 2010 ACM Symposium
on Applied Computing (ACSAC 2010) (2010)
45. Rieck, K., Trinius, P., Willems, C., Holz, T.: Automatic analysis of malware behav-
ior using machine learning. J. Comput. Secur. 19(4), 639–668 (2009)
SandPrint: Fingerprinting Malware Sandboxes
187
46. Rossow, C., Dietrich, C., Bos, H.: Large-scale analysis of malware downloaders. In:
Flegel, U., Markatos, E., Robertson, W. (eds.) DIMVA 2012. LNCS, vol. 7591, pp.
42–61. Springer, Heidelberg (2013)
47. Rutkowska, J.: Red Pill... Or How To Detect VMM Using (Almost) One CPU
Instruction (2004). http://www.securiteam.com/securityreviews/6Z00H20BQS.
html
48. Payload Security: Free Automated Malware Analysis Service. https://www.
hyblid-analysis.com/
49. Payload
article
Security: Blog
http://www.pandasecurity.com/
mediacenter/press-releases/pandalabs-neutralized-75-million-new-malware-samp
les-2014-twice-many-2013/
(2015).
50. ThreatTrack
Free Online Malware Analysis.
threattracksecurity.com/resources/sandbox-malware-analysis.aspx
Security.
http://www.
51. Symantec. Internet Security Threat Report 04/2015 (2015). http://www.symantec.
com/de/de/security response/publications/threatreport.jsp
52. ThreatExpert. http://www.threatexpert.com/submit.aspx
53. Vasudevan, A., Yerraballi, R.: Cobra: ﬁne-grained malware analysis using stealth
localized-executions. In: Proceedings of the 2006 IEEE Symposium on Security and
Privacy, S&P 2006 (2006)
54. Vidas, T., Christin, N.: Evading android runtime analysis via sandbox detection.
In: Proceedings of the 9th ACM Symposium on Information, Computer and Com-
munications Security, ASIA CCS 2014 (2014)
55. VirSCAN.org. Free Multi-Engine Online Virus Scanner. http://www.virscan.org/
56. Yoshioka, K., Hosobuchi, Y., Orii, T., Matsumoto, T.: Your sandbox is blinded:
impact of decoy injection to public malware analysis systems. J. Inf. Process. 52,
3 (2011)