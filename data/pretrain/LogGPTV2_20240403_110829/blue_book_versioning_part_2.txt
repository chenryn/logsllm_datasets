If you have a single library that doesn’t play well, then you probably will get
a working solve easily (this is one reason that this practice doesn’t seem so
bad at first). If more packages start following this tight capping, however, you
end up with a situation where things simply cannot solve. A moderately sized
application can have a hundred or more dependencies when expanded, so such
issues in my experience start to appear every few months. You need only 5-6 of
such cases for every 100 libraries for this issue to pop up every two months on
your plate. And potentially for a multiple of your applications.
The entire point of packaging is to allow you to get lots of packages that each
do some job for you. We should be trying to make it easy to be able to add
dependencies, not harder.
The implication of this is you should be very careful when you see tight
requirements in packages and you have any upper bound caps anywhere in the
dependency chain. If something caps dependencies, there’s a very good chance
adding two such packages will break your solve, so you should pick just one, or
just avoid them altogether, so you can add one in the future. This is a good
rule, actually: *Never add a library to your dependencies that has excessive
upper bound capping*. When I have failed to follow this rule for a larger
package, I have usually come to regret it.
If you are doing the capping and are providing a library, you now have
a commitment to quickly release an update, ideally right *before* any capped
dependency comes out with a new version. Though if you cap, how to you install
development versions or even know when a major version is released? This makes
it harder for downstream packages to update, because they have to wait for all
the caps to be moved for all upstream.
#### [It conflicts with tight lower bounds](https://iscinumpy.dev/post/bound-version-constraints/#it-conflicts-with-tight-lower-bounds)
A tight lower bound is only bad if packages cap upper bounds. If you can avoid
upper-cap packages, you can accept tight lower bound packages, which are much
better; better features, better security, better compatibility with new hardware
and OS’s. A good packaging system should allow you to require modern packages;
why develop for really old versions of things if the packaging system can
upgrade them? But a upper bound cap breaks this. Hopefully anyone who is writing
software and pushing versions will agree that tight lower limits are much better
than tight upper limits, so if one has to go, it’s the upper limits.
It is also rather rare that packages solve for lower bounds in CI (I would love
to see such a solver become an option, by the way!), so setting a tight lower
bound is one way to avoid rare errors when old packages are cached that you
don’t actually support. CI almost never has a cache of old packages, but users
do.
#### [Capping dependencies hides incompatibilities](https://iscinumpy.dev/post/bound-version-constraints/#capping-dependencies-hides-incompatibilities)
Another serious side effect of capping dependencies is that you are not notified
properly of incoming incompatibilities, and you have to be extra proactive in
monitoring your dependencies for updates. If you don’t cap your dependencies,
you are immediately notified when a dependency releases a new version, probably
by your CI, the first time you build with that new version. If you are running
your CI with the `--dev` flag on your `pip install` (uncommon, but probably a good
idea), then you might even catch and fix the issue before a release is even
made. If you don’t do this, however, then you don’t know about the
incompatibility until (much) later.
If you are not following all of your dependencies, you might not notice that you
are out of date until it’s both a serious problem for users and it’s really hard
for you to tell what change broke your usage because several versions have been
released. While I’m not a huge fan of Google’s live-at-head philosophy
(primarily because it has heavy requirements not applicable for most open-source
projects), I appreciate and love catching a dependency incompatibility as soon
as you possibly can; the smaller the change set, the easier it is to identify
and fix the issue.
#### [Capping all dependencies hides real incompatibilities](https://iscinumpy.dev/post/bound-version-constraints/#capping-all-dependencies-hides-eal-incompatibilities)
If you see `X>=1.1`, that tells you that the package is using features from `1.1` and do
not support `1.0`. If you see `X4` transition
will be more like the `1->2` transition than the `2->3` transition. When Python
4 does come out, it will be really hard to even run your CI on 4 until all your
dependencies uncap. And you won’t actually see the real failures, you’ll just
see incompatibility errors, so you won’t even know what to report to those
libraries. And this practice makes it hard to test development versions of
Python.
And, if you use Poetry, as soon as someone caps the Python version, every Poetry
project that uses it must also cap, even if you believe it is a detestable
practice and confusing to users. It is also wrong unless you fully pin the
dependency that forced the cap. If the dependency drops it in a patch release
or something else you support, you no longer would need the cap.
#### [Applications are slightly different](https://iscinumpy.dev/post/bound-version-constraints/#applications-are-slightly-different)
If you have a true application (that is, if you are not intending your package
to be used as a library), upper version constraints are much less problematic,
and some of the reasons above don't apply. This due to two reasons.
First, if you are writing a library, your “users” are specifying your package in
their dependencies; if an update breaks them, they can always add the necessary
exclusion or cap for you to help end users. It’s a leaky abstraction, they
shouldn’t have to care about what your dependencies are, but when capping
interferes with what they can use, that’s also a leaky and unfixable
abstraction. For an application, the “users” are more likely to be installing
your package directly, where the users are generally other developers adding to
requirements for libraries.
Second, for an app that is installed from PyPI, you are less likely to have to
worry about what else is installed (the other issues are still true). Many
(most?) users will not be using `pipx` or a fresh virtual environment each time,
so in practice, you’ll still run into problems with tight constraints, but there
is a workaround (use `pipx`, for example). You still are still affected by most of
the arguments above, though, so personally I’d still not recommend adding
untested caps.
#### [When is it ok to set an upper limit?](https://iscinumpy.dev/post/bound-version-constraints/#when-is-it-ok-to-set-an-upper-limit)
Valid reasons to add an upper limit are:
* If a dependency is known to be broken, block out the broken version. Try very
    hard to fix this problem quickly, then remove the block if it’s fixable on
    your end. If the fix happens upstream, excluding *just the broken version* is
    fine (or they can “yank” the bad release to help everyone).
* If you know upstream is about to make a major change that is very likely to
    break your usage, you can cap. But try to fix this as quickly as possible so
    you can remove the cap by the time they release. Possibly add development
    branch/release testing until this is resolved.
* If upstream asks users to cap, then I still don’t like it, but it is okay if
    you want to follow the upstream recommendation. You should ask yourself: do
    you want to use a library that may intentionally break you and require
    changes on your part without help via deprecation periods? A one-time major
    rewrite might be an acceptable reason. Also, if you are upstream, it is very
    un-Pythonic to break users without deprecation warnings first. Don’t do it
    if possible.
* If you are writing an extension for an ecosystem/framework (pytest extension,
    Sphinx extension, Jupyter extension, etc), then capping on the major version
    of that library is acceptable. Note this happens once - you have a single
    library that can be capped. You must release as soon as you possibly can
    after a new major release, and you should be closely following upstream
    - probably using development releases for testing, etc. But doing this for
    one library is probably manageable.
* You are releasing two or more libraries in sync with each other. You control
    the release cadence for both libraries. This is likely the “best” reason to
    cap. Some of the above issues don’t apply in this case - since you control
    the release cadence and can keep them in sync.
* You depend on private internal details of a library. You should also rethink
    your choices - this can be broken in a minor or patch release, and often is.
If you cap in these situations, I wouldn’t complain, but I wouldn’t really recommend it either:
* If you have a heavy dependency on a library, maybe cap. A really large API
    surface is more likely to be hit by the possible breakage.
* If a library is very new, say on version 1 or a ZeroVer library, and has very
    few users, maybe cap if it seems rather unstable. See if the library authors
    recommend capping - they might plan to make a large change if it’s early in
    development. This is not blanket permission to cap ZeroVer libraries!
* If a library looks really unstable, such as having a history of making big
    changes, then cap. Or use a different library. Even better, contact the
    authors, and make sure that your usage is safe for the near future.
#### Summary
No more than 1-2 of your dependencies should fall into the categories of
acceptable upper pinning. In every other case, do not cap your dependences,
specially if you are writing a library! You could probably summarize it like
this: if there’s a high chance (say `75%+`) that a dependency will break for you
when it updates, you can add a cap. But if there’s no reason to believe it will
break, do not add the cap; you will cause more severe (unfixable) pain than the
breakage would.
If you have an app instead of a library, you can be cautiously more relaxed, but
not much. Apps do not have to live in shared environments, though they might.
Notice many of the above instances are due to very close/special interaction
with a small number of libraries (either a plugin for a framework, synchronized
releases, or very heavy usage). Most libraries you use do not fall into this
category. Remember, library authors don’t want to break users who follow their
public API and documentation. If they do, it’s for a special and good reason (or
it is a bad library to depend on). They will probably have a deprecation period,
produce warnings, etc.
If you do version cap anything, you are promising to closely follow that
dependency, update the cap as soon as possible, follow beta or RC releases or
the development branch, etc. When a new version of a library comes out, end
users should be able to start trying it out. If they can’t, your library’s
dependencies are a leaky abstraction (users shouldn’t have to care about what
dependencies libraries use).
## Automatically upgrade and test your dependencies
Now that you have minimized the [upper bound pins](#upper-version-pinning) and
defined the [lower bound pins](#lower-version-pinning) you need to ensure that
your code works with the latest version of your dependencies.
One way to do it is running a periodic cronjob (daily probably) that updates
your requirements lock, [optionally your lower
bounds](#lower-version-pinning), and checks that the tests keep on passing.
## Monitor your dependencies evolution
You rely on your dependencies to fulfill critical parts of your package,
therefore it makes sense to know how they are changing in order to:
* Change your package to use new features.
* Be aware of the new possibilities to solve future problems.
* Get an idea of the dependency stability and future.
Depending on how much you rely on the dependency, different levels of
monitorization can be used, ordered from least to most you could check:
* *Release messages*: Some projects post them in their blogs, you can use
    their RSS feed to keep updated. If the project uses Github to create the
    release messages, you can get notifications on just those release messages.
    If the project uses [Semantic Versioning](semantic_versioning.md), it can
    help you dismiss all changes that are `micro`, review without urgency the
    `minor` and prioritize the `major` ones. If all you're given is a CalVer
    style version then you're forced to dedicate the same time to each of the
    changes.
* *Changelog*: if you get a notification of a new release, head to the changelog
    to get a better detail of what has changed.
* *Pull requests*: Depending on the project release workflow, it may
    take some time from a change to be accepted until it's published under a new
    release, if you monitor the pull requests, you get an early idea of what
    will be included in the new version.
* *Issues*: Most of changes introduced in a project are created from the outcome
    of a repository issue, where a user expresses their desire to introduce the
    change. If you monitor them you'll get the idea of how the project will
    evolve in the future.
## [Summary](https://bernat.tech/posts/version-numbers/#summary)
Is semantic versioning irrevocably broken? Should it never be used? I don’t
think so. It still makes a lot of sense where there are ample resources to
maintain multiple versions in parallel. A great example of this is Django.
However, it feels less practical for projects that have just a few maintainers.
In this case, it often leads to opting people out of bug fixes and security
updates. It also encourages version conflicts in environments that can’t have
multiple versions of the same library, as is the case with Python. Furthermore,
it makes it a lot harder for developers to learn from their mistakes and evolve
the API to a better place. Rotten old design decisions will pull down the
library for years to come.
A better solution at hand can be using CalVer and a time-window based warning
system to evolve the API and remove old interfaces. Does it solve all problems?
Absolutely not.
One thing it makes harder is library rewrites. For example, consider
virtualenv's recent rewrite. Version 20 introduced a completely new API and
changed some behaviours to new defaults. For such use cases in a CalVer world,
you would likely need to release the rewritten project under a new name, such as
virtualenv2. Then again, such complete rewrites are extremely rare (in the case
of virtualenv, it involved twelve years passing).
No version scheme will allow you to predict with any certainty how compatible
your software will be with potential future versions of your dependencies. The
only reasonable choices are for libraries to choose minimum versions/excluded
versions only, never maximum versions. For applications, do the same thing, but
also add in a lock file of known, good versions with exact pins (this is the
fundamental difference between install_requires and requirements.txt).
## [This doesn't necessarily apply to other ecosystems](https://snarky.ca/why-i-dont-like-semver/)
All of  this advice coming from me does not necessarily apply to all other
packaging ecosystems. Python's flat dependency management has its pros and cons,
hence why some other ecosystems do things differently.
# References
* [Bernat post on versioning](https://bernat.tech/posts/version-numbers/)
* [Should You Use Upper Bound Version Constraints? by Henry Schreiner](https://iscinumpy.dev/post/bound-version-constraints/)
* [Why I don't like SemVer anymore by Snarky](https://snarky.ca/why-i-dont-like-semver/)
* [Versioning Software by donald stufft](https://caremad.io/posts/2016/02/versioning-software/)