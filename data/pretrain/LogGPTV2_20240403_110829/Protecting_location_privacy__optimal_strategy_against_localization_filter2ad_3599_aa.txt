title:Protecting location privacy: optimal strategy against localization
attacks
author:Reza Shokri and
George Theodorakopoulos and
Carmela Troncoso and
Jean-Pierre Hubaux and
Jean-Yves Le Boudec
Protecting Location Privacy:
Optimal Strategy against Localization Attacks
Reza Shokri†, George Theodorakopoulos‡, Carmela Troncoso∗,
Jean-Pierre Hubaux†, and Jean-Yves Le Boudec†
†LCA, EPFL, Lausanne, Switzerland,
∗ESAT/COSIC, K.U.Leuven, Leuven-Heverlee, Belgium,
‡School of Computer Science and Informatics, Cardiff University, Cardiff, UK
†ﬁrstname.lastname@epﬂ.ch, ‡PI:EMAIL,
∗PI:EMAIL
ABSTRACT
The mainstream approach to protecting the location-privacy
of mobile users in location-based services (LBSs) is to alter
the users’ actual locations in order to reduce the location
information exposed to the service provider. The location
obfuscation algorithm behind an eﬀective location-privacy
preserving mechanism (LPPM) must consider three funda-
mental elements: the privacy requirements of the users, the
adversary’s knowledge and capabilities, and the maximal
tolerated service quality degradation stemming from the ob-
fuscation of true locations. We propose the ﬁrst methodol-
ogy, to the best of our knowledge, that enables a designer to
ﬁnd the optimal LPPM for a LBS given each user’s service
quality constraints against an adversary implementing the
optimal inference algorithm. Such LPPM is the one that
maximizes the expected distortion (error) that the optimal
adversary incurs in reconstructing the actual location of a
user, while fulﬁlling the user’s service-quality requirement.
We formalize the mutual optimization of user-adversary ob-
jectives (location privacy vs. correctness of localization) by
using the framework of Stackelberg Bayesian games. We de-
velop two linear programs that output the best LPPM strat-
egy and its corresponding optimal inference attack. We show
that the optimal LPPM is superior to a straightforward ob-
fuscation method, and that the optimal localization attack
performs better compared to a Bayesian inference attack.
Categories and Subject Descriptors
C.2.0 [Computer-Communication Networks]: General—
Security and protection; K.4.1 [Computers and Society]:
Public Policy Issues—Privacy
Keywords
Location Privacy, Location Inference Attacks, Service Qual-
ity, Location-based Services, Stackelberg Bayesian Games
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CCS’12, October 16–18, 2012, Raleigh, North Carolina, USA.
Copyright 2012 ACM 978-1-4503-1651-4/12/10 ...$10.00.
1.
INTRODUCTION
The widespread use of smart mobile devices with continu-
ous connection to the Internet has fostered the development
of a variety of successful location-based services (LBSs).
Even though LBSs can be very useful, these beneﬁts come
at a cost of users’ privacy. The whereabouts users’ disclose
to the service provider expose aspects of their private life
that is not apparent at ﬁrst, but can be inferred from the
revealed location data [8, 11, 18].
A large body of research has focused on developing location-
privacy protection mechanisms (LPPMs) that allow users to
make use of LBSs while limiting the amount of disclosed sen-
sitive information [1, 3, 9, 10, 14, 16, 22]. These protection
mechanisms are based on hiding or perturbing the real lo-
cations of a user, or even sending fake locations to the LBS,
in order to increase the uncertainty of the adversary about
a user’s true whereabouts. However, the evaluation of these
designs usually disregards that the adversary might have
some knowledge about the users’ access pattern to the LBS
and also about the algorithm implemented by the LPPM.
Such information allows the attacker to reduce his uncer-
tainty on the user’s true location [25]. Hence, prior evalu-
ations overestimate the location privacy oﬀered by a given
protection system.
In this paper, we focus on a broad range of LBSs and lo-
cation sharing services in which users reveal their location in
a sporadic manner, e.g., location check-in, location-tagging,
or applications for ﬁnding nearby points-of-interests, local
events, or nearby friends. We consider an adversary inter-
ested in uncovering the location of a user at the time when
she sends the LBS query (i.e., an adversary performing local-
ization attacks [25, 26]). We focus on user-centric LPPMs
in which decision taken to protect privacy (e.g., hiding, per-
turbing, or faking the location) is made locally by the user.
Hence, these LPPMs can be easily integrated in the mobile
device that she uses to access LBS. We note, however, that
the principles behind our protection mechanism design are
applicable to LBSs where users reveal their location continu-
ously (rather than sporadically), and where the adversary’s
aim is to track users continuously over space and time [26].
We propose an analytical framework that allows system
designers to ﬁnd the optimal LPPM against a strategic ad-
versary who, knowing each user’s LBS access pattern and
the underlying obfuscation algorithm, employs the optimal
attack to localize them. The challenge is to design such opti-
617mal protection mechanism when the inference attack, depen-
dent on the mechanism being designed, is unknown to the
designer. As opposed to making any assumption about the
adversary’s inference algorithm (i.e., limiting his power), we
co-infer the optimal attack while ﬁnding the defense mech-
anism. Additionally, our methodology constrains the search
space to LPPMs that obfuscate locations in such a way that
the quality of the LBS response is not degraded below a
threshold imposed by the user, hence guaranteeing required
service quality for the user. We assume that the adversary
is also aware of this user-speciﬁed service quality constraint.
We formalize the problem of ﬁnding the optimal LPPM
anticipating the optimal inference attack as an instance of a
zero-sum Bayesian Stackelberg game. In this game, a leader
and a follower interact strategically with each one’s gain be-
ing the loss of the other. The leader decides on her strategy
knowing that it will be observed by the follower, who will
optimize his choice based on this observation. In our sce-
nario the user is the leader and the adversary is the follower.
Then, this game precisely models that the adversary knows
the user’s choice of protection mechanism and will use that
knowledge to improve his attack’s eﬀectiveness. We extend
the classic formulation of a Stackelberg game with an extra
constraint to ensure that the service quality is satisfactory
for the user. This enables us to ﬁnd the optimal point in
the tradeoﬀ curve between privacy and service quality that
satisﬁes both user privacy and service quality requirements.
We build on the probabilistic model proposed by Shokri
et al. [24, 25, 26] to ﬁnd the best localization attack against
a given LPPM and to measure the users’ location privacy.
This privacy measure is in turn used in the Stackelberg game
to ﬁnd the optimal LPPM for each user, i.e., the one that
oﬀers the best location privacy subject to the user’s service
quality requirements. Ours is, to the best of our knowl-
edge, the ﬁrst analytical framework that allows engineers to
methodologically integrate adversarial knowledge in the de-
sign of optimal user-centric privacy protection mechanisms.
We evaluate the LPPMs generated by our method us-
ing real location traces. We show how, for a given user’s
LBS access pattern and service-quality threshold, our game-
theoretic approach enables us to simultaneously ﬁnd the op-
timal LPPM and the optimal attack against it. We conﬁrm
that there is a trade-oﬀ between the maximum achievable
privacy and the service quality but once a certain privacy
level is reached, loosening the quality requirements does not
necessarily result in a privacy gain. We also ﬁnd that the
location-privacy gain of using the optimal LPPM, with re-
spect to a suboptimal one, is larger when the quality con-
straint is tighter (compared to the case where users’ quality
requirements allow the LPPM to signiﬁcantly perturb loca-
tions before sending them to the LBS).
The rest of the paper is organized as follows. We present
the elements of our framework and describe the objectives
of the user and adversary in the next section. We formalize
the problem of ﬁnding an LPPM that oﬀers optimal location
privacy in terms of a Bayesian Stackelberg game in Section 3,
and develop the best solution for both user and adversary in
Section 4. We evaluate our method in Section 5 against real
location traces. Section 6 revisits previous work on location
privacy protection mechanisms, as well as on game theory
applied to security-related scenarios. Finally, we conclude
the paper in Section 7.
2. THE PROBLEM STATEMENT
In this section, we explain our system model based on the
probabilistic framework proposed in [25, 26], as well as our
assumptions and adversarial model. We conclude by sketch-
ing the problem this work aims at solving. In Table 1, we
summarize the notations introduced throughout the section.
2.1 User and Adversary
We consider a scenario in which users move in an area
partitioned into M discrete regions R = {r1, r2, · · · , rM }.
We also assume that time is discrete and it is partitioned
into diﬀerent time periods (e.g., morning, afternoon). We
denote the spatiotemporal position of a user u at time t as
the actual event au(t) = hu, t, ri. We do not make any spe-
ciﬁc assumption about the users’ mobility patterns. Users
connect sporadically to an LBS provider to which they need
to share their current location in order to obtain a service,
i.e., there is a non-negligible time gap between two succes-
sive accesses a user to the LBS. The access proﬁle ψτ
u(r) of
user u is the probability distribution of the location r from
which user u accesses the LBS in time period τ . For a given
u(r) = 1.
We note that this proﬁle is time-dependent (i.e., users may
have diﬀerent access patterns in the morning than in the
afternoon). This dependency also aﬀects users’ location pri-
vacy requirements, and service quality requirements. For the
sake of simplicity, in this paper, we omit the time-period τ
and provide a solution for each user in a given time period.
But, we note that the method is easily adaptable to more
complex access patterns and privacy/quality requirements
elicitation that account for such changes in time (e.g., by
applying the method to each time period separately).
user u in a given time period τ , we have Pr∈R ψτ
We assume that the LBS to which the user connects, or
any entity that can eavesdrop on the user-LBS communi-
cations, is a passive and curious adversary whose aim is to
discover the location of the user at the query time. As the
LBS use is sporadic, the knowledge that the adversary can
accumulate with repeated observations/eavesdropping is the
frequency with which the user issues queries from regions in
R, i.e., ψu(r). We assume that the adversary learns the
user’s proﬁle ψu(.) for example by using the algorithm ex-
plained in [26]. As we focus on user-centric mechanisms,
which give protection to each user separately, in the remain-
der of the paper we omit the user identity u and present the
model for this user with proﬁle ψ(.).
2.2 Location-Privacy Protection Mechanism
We consider that users want to preserve their location pri-
vacy when they use the LBS. Users implement a local and
user-centric LPPM that transforms each true location r into
a pseudolocation r′ ∈ R′, which is then sent to the LBS in-
stead of the actual location. We set R′ = R (however, in the
most general case R′ is the powerset of R). The spatiotem-
poral position of a user as perceived by the LBS, denoted
o(t) = ht, r′i, is called an observed event. For each actual
event a(t) = ht, ri the LPPM chooses a pseudolocation r′ by
sampling from the following probability distribution:
f (r′|r) = Pr(cid:8)o(t) = ht, r′i|a(t) = ht, ri(cid:9)
The adversary’s knowledge is modeled as the user’s ac-
cess proﬁle ψ(.). As accesses to the LBS are sporadic, two
successive query locations of the user are conditionally inde-
pendent given ψ(.). The larger the inter-query time is, the
(1)
618Symbol
u
r, R
ψ(r)
a(t) = ht, ri
r′, R′
o(t) = ht, r′i
f (r′|r)
dq(r′, r)
Qloss(ψ, f, dq)
Qmax
loss
ˆr
h(ˆr|r′)
dp(ˆr, r)
Table 1: Summary of notations
Meaning
Identity of the user
Actual location of the user, set of possible locations for the user
Location access proﬁle of the user (probability of being at location r when accessing the LBS)
Actual location r of the user at time t
Pseudolocation output by the LPPM, set of possible pseudolocations output by the LPPM
Observed pseudolocation r′ of the user at time t
Location obfuscation function implemented by the LPPM: Probability of replacing r with r′.
Incurred service-quality loss by the user if LPPM replaces location r with pseudolocation r′
Expected quality loss of an LPPM with location obfuscation function f
Maximum tolerable service quality loss
Adversary’s estimate of the user’s actual location
Adversary’s attack function: Probability of estimating ˆr as user’s actual location, if r′ is observed
Distance between locations ˆr and r: Privacy of the user at location r if adversary’s estimate is ˆr
P rivacy(ψ, f, h, dp) Expected location privacy of the user with proﬁle ψ(.) using protection f against attack h
more independent the two locations of the user in her succes-
sive LBS accesses are. This is also reﬂected in the LPPM’s
obfuscation algorithm that outputs pseudolocations that de-
pend only on the user’s current location.
2.3 Service Quality Metric
In the aforementioned setting, the LBS response quality
depends on the pseudolocation output by the LPPM and
not on the user’s actual location. The distortion introduced
in the observed pseudolocations determines the quality of
service that the user experiences. The more similar the ac-
tual and the observed location are, the higher the service
quality is. The expected quality loss due to an LPPM f (.)
is computed as an average of dq(r′, r) over all r and r′:
Qloss(ψ, f, dq) =Xr,r′
ψ(r)f (r′|r)dq(r′, r).
(2)
Function dq(.) determines the dissimilarity between loca-
tion r and pseudolocation r′. The semantics of this dis-
similarity depend on the LBS under consideration, and also
on the user’s speciﬁc service-quality expectations. In many
applications, the service quality can be considered inversely
proportional to the physical distance between r and r′. For
example, applications that ﬁnd nearby points of interest
could give very diﬀerent responses to r and to r′ even if they
are only a couple of kilometers apart. In contrast, there exist
LBSs in which the service quality depends on other criteria,
such as on whether r′ is within a region of interest. For a
weather forecast application, for instance, any pseudoloca-
tion r′ in the same city as the actual location r would result
in a high quality LBS response.
We assume that users impose a maximum tolerable ser-
loss , caused by sharing pseudolocations
vice quality loss, Qmax
instead of their actual locations. Formally,
Qloss(ψ, f, dq) ≤ Qmax
loss .
(3)
This constraints the LPPM obfuscation function f (r′|r),
that must not output pseudolocations that, on average, re-
sult in lower quality. We note that the inﬂuence of threshold
Qmax
loss on the LPPM depends on the function dq(.), hence it
is also dependent on the type of the LBS the user is query-
ing. In the case of an LBS that ﬁnds nearby points of in-
terest, where dq(.) is proportional to the physical distance
between r and r′, enforcing the quality threshold could re-
sult in ensuring a maximum allowed distance between these
two locations. For the weather application, enforcing the
quality threshold could result in setting region boundaries
within which locations lead to the same forecast. For other
location-based applications, the function dq(.) and the thresh-
old Qmax
loss can be deﬁned in the same vein.
2.4 Location Privacy Metric
The adversary’s goal is to infer the user’s actual events
a(t) = ht, ri given the observed events o(t) = ht, r′i. Recall
that the adversary knows the user’s proﬁle, ψ(.). He uses
this background knowledge to run an inference attack on
the observed events in order to output estimations ˆr of the
user’s actual locations. Formally, the attack result can be
described as a probability density function h(.) such that
(4)
h(ˆr|r′) = Pr(cid:8)a(t) = ht, ˆri|o(t) = ht, r′i(cid:9) .
As the adversary’s prior information is the probability
that the user is at a given location when she accesses the
LBS, the current (query) location of the user is condition-
ally independent of her observed past and future locations.
This is reﬂected in that the computation of the estimated
location ˆr at time t only depends on the pseudolocation r′
observed at the same time t.
We note that the attack formulation is independent of
whether the considered LPPM anonymizes the events or not.
In this work, we assume that the adversary knows the iden-
tity of the users behind the events, but the framework can
be adapted to anonymous LPPMs as well. Note that even
when users are anonymous, our optimal solution provides a
guarantee for their location privacy (even after a potential
re-identiﬁcation attack).
We follow the deﬁnition in [26] and quantify the user’s lo-
cation privacy as the adversary’s expected error in his infer-
ence attack, i.e., the expected distortion in the reconstructed
event. We compute the expectation over all r, r′, and ˆr:
P rivacy(ψ, f, h, dp) = Xˆr,r′,r
ψ(r)f (r′|r)h(ˆr|r′)dp(ˆr, r)
(5)
The distortion function quantiﬁes the loss of privacy stem-
ming from the inference attack. The privacy loss depends
on the locations’ semantics and also on the privacy require-
ments of the user (i.e., users might consider locations inside
a hospital more sensitive than other places), and dp(.) must
619be deﬁned accordingly. For instance, if the user wants to
hide just her exact current location (as opposed to hiding
her location area), the appropriate distortion function could
be the Hamming distance (probability of error) between the
estimated location ˆr and the actual location r:
dp(ˆr, r) =(0,
1,
if ˆr = r
otherwise
(6)
In this case, any location diﬀerent from the user’s actual
location results in a high level of location privacy. Alterna-
tively, the user’s privacy might depend on the physical dis-
tance between the estimated and actual locations, hence the
distortion function can be modeled as the Euclidean distance
between these locations, i.e., the squared-error distortion:
dp(ˆr, r) = (ˆr − r)2
(7)
2.5 Problem Statement
Given
1. a maximum tolerable service-quality loss Qmax
loss imposed
by the user as a bound for Qloss(.), computed using the
quality function dq(.), and
2. a prior adversarial knowledge of the user’s proﬁle ψ(.),
the problem is ﬁnding the LPPM obfuscation function f (.)
that maximizes the user’s location privacy as deﬁned in (5).
The solution must consider that the adversary
1. observes the LPPM’s output r′, and