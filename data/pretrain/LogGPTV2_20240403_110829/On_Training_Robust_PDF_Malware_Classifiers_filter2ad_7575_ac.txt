To run the genetic evolution attacks, we set up the Cuckoo
sandbox with 32 virtual machines running Windows XP SP3
32 bit and Adobe Acrobat Reader 8.1.1.
1Our code is available at https://github.com/surrealyz/pdfclassiﬁer
4.1 Models
4.1.1 Datasets
We obtain the PDF malware dataset from Contagio [3].
The malicious PDFs include web exploit PDFs and email
attachments for targeted attacks. We split the dataset into 70%
train set and 30% test set, summarized in Table 2. In total, we
have 13K training PDFs and 6K test PDFs. We use the Hidost
feature extractor to extract structural paths features, with the
default compact path option [2, 48]. The input features have
3,514 dimensions, representing all the distinct path features
from the training dataset.
Robustness Properties. In our experiments, we focus on
ﬁve robustness properties as labeled from A to E in Table 3.
For brevity, we will refer to the four robustness properties as
property A (subtree deletion distance one), B (subtree insertion
distance one), C (subtree deletion distance two), D (subtree in-
sertion distance 41) and E (subtree insertion distance 42). They
are deﬁned in Section 3.1. Every subtree is represented by a con-
tinuous range of indices in the binary feature vector, so inser-
tion and deletion can be bounded by a corresponding interval.
Symbolic Interval Analysis. We implement veriﬁably
robust training using Symbolic Interval Analysis [55, 56] as
the sound over-approximation method. Symbolic interval
analysis uses intervals to bound the adversarial input range to
the model, then propagates the range over the neural network
while keeping input dependency. When passing the interval
through the non-linearity in the neural network, we do linear
relaxation of the input interval, and bound the output interval
by two equations [55]. The analysis tightly over-estimates
the output range, which we use to compute the robust loss
(Equation 2, Section 2.3.1).
We parse and manipulate the PDFs using the modiﬁed
version of pdfrw parser that handles malformed PDF mal-
ware [60]. When a subtree is deleted, the paths it contains and
objects with zero reference are deleted. If the subtree contains
any object referenced by other nodes, the object still exists
along with the other paths. Within the regular training dataset,
we have successfully parsed 6,867 training and 3,416 testing
PDF malware to train and evaluate the robustness properties.
Table 3 shows the number of intervals we extract for each
property, separated by training and testing sets.
4.1.2 Model Architecture and Hyperparameters
Among the models we evaluate, many are neural networks
or have neural networks as the basic component. We use
the same model architecture and training hyperparameters
for all the neural network models. We follow previous
work [28, 29, 46] to build a feed-forward neural network with
two hidden layers, each with 200 neurons activated by ReLU,
and a ﬁnal layer of Softmax activation. We train all neural
network models for 20 epochs, using the Adam Optimizer,
with mini-batch size 50, and learning rate 0.01.
USENIX Association
29th USENIX Security Symposium    2349
Knowledge and Access
Model
Arch Wgts
X
X
X
X
7
7
7
X
X
X
X
X
X
X
Training
Alg, Data
X
X
X
X
7
7
7
Classiﬁcation
Score
Label
X
X
X
X
X
X
X
X
X
X
X
X
X
X
Knows
Defense
X
X
X
X
7
7
X
Bounded
by
Property
X
X
7
7
7
7
7
Attacker
Bounded Arbitrary Attacker
Bounded Gradient Attacker
Unbounded Gradient Attacker
(1)
(2)
(3)
(4) MILP Attacker
(5)
(6)
(7)
Enhanced Evolutionary Attacker
Reverse Mimicry Attacker
Adaptive Evolutionary Attacker
Realizable
Input
X
7
7
7
X
X
X
Type
I
I
II
II
II
II
III
Eval
VRA
ERA
ERA
ERA
ERA
ERA
ERA
Feat
X
X
X
X
X
X
X
Table 1: We evaluate our models using seven types of attackers. They represent, two strongest bounded adaptive attackers (Type I),
four state-of-the-art unbounded attackers (Type II), and the new adaptive unbounded attacker (Type III). Only attackers (1) and (2)
are restricted by the robustness property. The gradient and MILP attackers ((2), (3), (4)) operates on non-realizable feature-space
inputs. The other attackers operate on realizable inputs, among which attacker (1) overapproximates realizable inputs.
Dataset
Malicious
Benign
Training PDFs
6,896
6,296
Testing PDFs
3,448
2,698
Table 2: The Contagio [3] dataset used for regular training.
Robustness Property
A: Subtree Deletion Distance 1
B: Subtree Insertion Distance 1
C: Subtree Deletion Distance 2
D: Subtree Insertion Distance 41
E: Subtree Insertion Distance 42
Number of PDF Malware
Training
Intervals
30,655
288,414
62,445
288,414
6,867
Training
6,867
Testing
Intervals
15,672
143,472
33,711
143,472
3,416
Testing
3,416
Table 3: Five robustness properties and the corresponding num-
ber of intervals used to train and test VRA. The intervals are
extracted from 6,867 training and 3,416 testing PDF malware.
4.1.3 Baseline Models
Baseline Neural Network. We train the baseline neural
network model with the regular training objective (Equation 1,
Section 2.3.1), using the regular training dataset (Table 2).
The model has 99.9% test accuracy and 0.07% false positive
rate. The performance is consistent with those reported in
PDFrate [47] and Hidost [48] (Section 2.2).
Adversarially Robust Training. We use the new subtree
distance metric to adversarially retrain the neural network.
We train ﬁve models corresponding to A, B, C, D, and A+B
properties. For the deletion properties, we train with deleting
one or two entire subtrees; and for the insertion properties,
we train with inserting one or 41 full subtrees. The resulting
performance of the models are shown in Table 4. All models
have more than 99% accuracy. The Adv Retrain A, B models
maintain the same 0.07% FPR as the baseline neural network.
The other three models have slightly higher FPR up to 0.15%.
Ensemble Classiﬁers. Ensemble methods use multiple
learners to boost the performance of the base learner. We
implement two ensemble classiﬁers.
Ensemble A+B. The provable robustness property is, if
a PDF variant is generated by subtree insertion bounded by
distance one to a PDF, the variant has the same prediction
as the original PDF. The ensemble classiﬁes the PDF as
malicious, if an arbitrary full subtree deletion results in
malicious class by the base learner. We augment the regular
training dataset with an arbitrary subtree deleted from both
malicious and benign PDFs, which maintains the performance
for original PDFs because they also need to be tested under
multiple deletion operations. Ensemble A+B achieves 99.87%
accuracy and 0.26% FPR.
Ensemble D. The provable robustness property is, if a
PDF variant is generated by inserting up to 41 subtrees in a
PDF, it has the same prediction as the original PDF. For the
base learner, we train a neural network to classify the original
malicious and benign PDFs as if they were generated by up to
41 subtree insertions. Consequently, we augment the training
dataset by keeping one subtree from all PDFs to train the base
learner. To build the ensemble, we test every single subtree of
the unseen PDFs, and predict the malicious class if any subtree
is classiﬁed as malicious. The Ensemble D model has 99.96%
accuracy and 0.07% FPR.
Monotonic Classiﬁers. Monotonic classiﬁers [32] are
the most related work to ours. We follow Incer et al. [32] to
use Gradient Boosted Decision Trees [15] with monotonic
constraint. After comparing different tree depths, we ﬁnd that
the results do not signiﬁcantly differ in this dataset. Therefore,
we train multiple monotonic classiﬁers with different number
of learners (10, 100, 1K, and 2K), where each learner is a de-
cision tree of depth 2. The classiﬁers are named by the number
of learners they have (Table 4). The monotonic classiﬁers
have an average of 99% accuracy and under 2% FPR, which
shows much better performance in a small Contagio dataset
compared to results in [32]. Since monotonic property is such
a strong enforcement for the classiﬁer’s decision boundaries,
the malware classiﬁer in [32] has 62% temporal detection rate
over a large scale dataset containing over 1.1 million binaries.
Note that the ensembles and monotonic classiﬁers are the
only models we train with properties for both malicious and
benign PDFs. For all the other models, we train properties for
only malicious PDFs.
4.1.4 Veriﬁably Robust Models
Robust Training. We train seven veriﬁably robust models
and name them with the properties they are trained with. We
2350    29th USENIX Security Symposium
USENIX Association
use the same model architecture and the same set of hyperpa-
rameters as the baseline neural network model (Section 4.1.2).
During training, we optimize the sum of the regular loss and the
robust loss in each epoch, as deﬁned in Equation 3. At the mini-
batch level, we randomly mix batches belonging to different
properties. For example, to train the Robust A+B model, we do
mixed training for regular accuracy, the insertion property, and
the deletion property alternately by mini-batches,in order to ob-
tain two properties as well as high accuracy in the same model.
The left side of Table 4 contains the test accuracy (Acc),
false positive rate (FPR), and training time for the models.
Training Time. The robust models with insertion proper-
ties (Robust B, Robust D, Robust A+B, Robust A+B+E) took
more than an hour to train, since they have signiﬁcantly more
intervals (Table 3) than deletion properties. On the contrary,
Robust A and Robust C models can be trained by 11 and
25 minutes, respectively. The average training time for each
mini-batch is 0.036s. Efﬁciently scaling the number of training
points, input dimension, and network size can be achieved by
techniques from [27, 53, 59].
Accuracy and FPR. All the robust models, except the
Robust D model, can maintain over 99% test accuracy while
obtaining veriﬁable robustness. Robust D model dropped
the test accuracy only a little to 98.96%. Training robustness
properties increased the false positive rates by under 0.5%
for Robust A, B, and A+B models, which are acceptable. For
models C and D, the false positive rates increased to 1.04%
and 2.3% respectively. Models with property E (Robust E and
Robust A+B+E), have FPR 1.93% and 1.89%, similar to those
of the monotonic classiﬁers. The false positive rate increases
more for the insertion properties (B and E) than the subtree
deletion property (A). The FPR is also larger for a bigger
distance under the same type of operation (C vs A, and D vs B).
4.2 Bounded Arbitrary Attacker
Strongest Possible Bounded Attacker. The bounded arbi-
trary attacker has access to everything (Table 1). The attacker
can do anything to evade the classiﬁer, under the restriction
that attacks are bounded by the robustness properties.
4.2.1 Results
We formally verify the robustness of the models using
symbolic interval analysis to obtain VRA, over all the 3,461
testing malicious PDFs (Table 3). For example, 99% VRA for
property B means that, 99% of 3,416 test PDFs will always be
classiﬁed as malicious, for arbitrary insertion attacks restricted
by one of the subtrees under the PDF root. No matter how
powerful the attacker is after knowing the defense, she will
not have more than 1% success rate.
Table 4 shows all the VRAs for the baseline models and
veriﬁably robust models. Our key observations are as follows.
Baseline NN: It has robustness for the deletion properties,
but not robust against insertion.
Adversarially Robust Training: All adversarially re-
trained models can increase the VRAs for deletion properties,
except Adv Retrain B model. Adv Retrain B model is trained
with insertion at distance one, which shows conﬂict with the
deletion properties and decreased VRAs for property A and
C, compared to the baseline NN. Adv Retrain C achieves the
highest VRAs for both property A and C.
Ensemble Classiﬁers: We conduct the interval analysis
according to the ensemble setup, described in Appendix A.1.
Ensemble A+B has 97% and 99% VRAs for property A and
B, respectively. However, the VRA for property C is only 7%
and the VRA is zero for property D and E. On the other hand,
Ensemble D does not have VRA for any property, despite
the ensemble setup. Since the the base learner in Ensemble D
needs to classify an arbitrary subtree after certain deletion and
insertion operations, it is not enough to gain VRA by learning
speciﬁc subtree from the training dataset.
Monotonic Classiﬁers: All the monotonic classiﬁers have
insertion VRAs that are the same as the test accuracy, due
to the monotonic constraints enforced during the training
time. Except the model with 10 learners, all the models have
over 99% VRAs for properties B, D, and E. We utilize the
monotonic property of the models to ﬁnd lower bound of
deletion VRAs. For property A, we verify the classiﬁer’s
behavior on a malicious test PDF, if every possible mutated
PDF with an arbitrary full subtree deletion is always classiﬁed
correctly. Since the original malicious PDF features are larger,
based on the monotonic property, any partial deletion will also
result in malicious classiﬁcation for these PDFs. This gives us
between 5.74% and 8.78% VRAs for the monotonic classiﬁers
under property A. Similarly, by testing the lower bound of two
subtree deletion, we verify the monotonic classiﬁers to have
0 VRA for property C.
Veriﬁably Robust Models: We can increase the VRA from
as low as 0% to as high as 99%, maintaining high accuracy,
with under 0.6% increase in FPR in properties A and B.
Training a model with one robustness property can make it
obtain the same type of property under a different distance. For
example, Robust A model is trained with property A (distance