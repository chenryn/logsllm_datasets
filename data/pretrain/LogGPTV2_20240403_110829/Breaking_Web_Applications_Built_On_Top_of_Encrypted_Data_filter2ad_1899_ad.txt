derstand whether users’ assumptions align with those of the
developer. For example, every principal in Mylar has a name,
which is used to verify the authenticity of keys and thus in-
tentionally left unencrypted by the Mylar developers. The
names of user principals are automatically set to their user-
names or email addresses, but developers are responsible for
assigning names to all other principals. Using sensitive in-
formation when naming principals will leak this information
to a snapshot passive attacker.
kChat. kChat’s principals are chat rooms and users. Their
Figure 3: (From [48]) Bob and Alice are discussing a party in the
‘party’ chat room while Bob and Boss discuss work in the ‘work’
chat room. Bob doesn’t want his boss to ﬁnd out about the party.
principal names are set to room names and usernames, re-
spectively.
In general, principal names are assigned when
principals are created and users are not told how they were
chosen. Users normally cannot see the names of the rooms
that they have not been invited to and they might mistak-
enly assume that room names are encrypted on the server.
This can create a false sense of privacy in users who give
their rooms revealing names. In fact, room names are visi-
ble via the names of the corresponding principals.
The authors of Mylar intentionally gave human-under-
standable names to chat rooms in kChat. This breaks the
security of their own motivating example. One of the ﬁg-
ures in [48] shows a kChat user expressing hope that another
user does not learn certain information—see Figure 3). But
principal names (i.e., Mylar’s unencrypted metadata) are
derived from room names (i.e., kChat’s metadata). Because
Mylar’s metadata is visible in plaintext on the server, the
server operator can easily learn the name of the chat room
and thus the secret that the users want to protect.
This internal inconsistency shows how diﬃcult it is to
match user and developer expectations. The authors of My-
lar could have chosen nondescript principal names, but this
would have broken Mylar’s reliance on principal names when
verifying keys and identities.
6.3 Features not protected by encryption
Even secure encryption leaks many features of the data,
such relative sizes.
If the data is searchable, Mylar also
reveals the count of unique words for each document because
it computes a search token for every word and stores these
tokens in the same document as the encrypted data.
OpenDNA.
In OpenDNA, the combination of encrypted
data and search tokens exceeds the maximum MongoDB
document size of 16 MB, thus each user’s DNA must be
split into multiple MongoDB documents. Mylar provides no
guidance to help developers make these decisions.
One simple solution is to split the DNA by chromosomes,
with a separate document containing all SNPs for each chro-
mosome. Another simple solution is to split the DNA into
n documents, with an equal number of SNPs in each.
Splitting DNA into documents based on chromosomes en-
ables a persistent passive attacker to infer which risk groups
the user is searching for. For example, if a risk group matched
documents 1, 3, and 8, then the attacker can look for known
risk groups that are associated with SNPs in chromosomes
1, 3, and 8. Even if the encrypted chromosomes were stored
in random order in the database, the attacker could tell
which chromosome matched the risk group as the number
of SNPs diﬀers greatly between chromosomes. For example,
one DNA test from 23andMe gives 1,700 SNPs in one chro-
mosome and 77,000 in another.1 These diﬀerences result
in large discrepancies in the size of the encrypted data and
number of encrypted words between documents.
Splitting DNA into equally sized documents also leaks in-
formation about the user’s queries. In the case of 23andMe,
the user is given a ﬁle with SNPs ordered by their chromo-
somes, and the ordering is preserved when the ﬁle is up-
loaded. So if the user’s DNA is split into, say, 5 documents,
the attacker can guess that chromosomes 1 and 2 are in doc-
ument 1, chromosome 3 is split between documents 1 and 2,
chromosomes 4-6 are in document 2, etc.
MDaisy.
In MDaisy, medical staﬀ members have only
their names stored in the database, while patients have their
name, date of birth, medical record number, and gender
stored. Knowing which users are patients and which are staﬀ
helps a snapshot passive attacker infer sensitive information
(see Section 6.1). If the user’s role is encrypted, it can be
easily inferred from the number of encrypted ﬁelds (four for
patients, one for staﬀ). Even if all of the user’s information
were stored in a single ﬁeld, the size of that ﬁeld would
distinguish patients from staﬀ members.
Preventing inference from relative ciphertext sizes may
very well be impossible in practice. Data can be padded
to a pre-deﬁned size, but this introduces large overheads in
computation and storage. Without application-speciﬁc in-
formation about data sizes, a BoPET cannot generically hide
the size. Protecting against other attacks such as frequency
analysis is tied directly to the cryptography and functional-
ity of the BoPET and may be a fundamental limitation.
7. EXPLOITING ACCESS PATTERNS
BoPETs involve rich client-server interactions. Clients
fetch, send, and update documents in the server’s database
and interact with each other through the server. By design,
searchable encryption leaks whether a match succeeded. Fur-
thermore, in systems like Mylar, keywords are encrypted in
order, thus the order of the tokens leaks information, too.
A persistent passive or active attacker on the server can
infer users’ secrets from these access patterns. This leakage
is fundamental in any non-trivial BoPET because the ser-
vices that a BoPET provides to diﬀerent users depend on
these users’ inputs into the system.
The designers of Mylar acknowledge that Mylar does not
hide access patterns, and in the BoPETs literature it is typ-
ical to exclude this leakage [28, 33, 34, 47]. Of course, a ma-
licious server can easily observe these patterns. To fully
understand the limits of BoPETs’ security in realistic sce-
1We found similar percentage breakdowns in 1,900 user-
released 23andMe reports from openSNP [44].
narios, we must analyze what sensitive information can be
inferred from access patterns in concrete applications.
MDaisy.
If two diﬀerent patients access the same en-
crypted procedure information, a persistent passive attacker
can infer that both are undergoing the same procedure.
Given more users and more appointments, this attacker can
cluster users and begin to understand how procedures relate
to one another (e.g., “if a user goes in for procedure Foo,
they will typically come back two weeks later for procedure
Bar”). The attacker does not know what the procedures are,
but this conﬁdentiality is very brittle. If a single user dis-
closes their procedure (either publicly or by colluding with
the server), everyone who underwent the same procedure
during the attacker’s observations will lose their privacy.
MeteorShop. Our ported MeteorShop app encrypts every
item in the user’s cart. Items prices stored in the cart are
encrypted, too, lest the server identiﬁes the items by solving
a knapsack problem given the total price.
A persistent passive attacker can assume that when an
encrypted item is added to the user’s cart, it came from the
list of products most recently requested by the user. This
leakage is mitigated somewhat by the fact that MeteorShop
does not have individual pages for products. Instead, when
a user clicks on a subcategory, the client fetches all products
in that subcategory from the server and displays them. The
server thus learns only the item’s subcategory.
If a shopping app listed each product on its own page, the
server would be able to infer the exact item added to the
cart. Even if product information were fully encrypted on
the server and the attacker were somehow prevented from
using the app as a user, the server could infer the products
from the images requested by the client.
OpenDNA. In OpenDNA, if the risk groups being searched
for as well as the search order are known to the server, the
server learns sensitive information about the user’s DNA
based on whether a query matched a group or not. If the
server knows only which risk groups are associated with dis-
eases, the actual risk group might not matter as the match
reveals that the user is at risk for something.
DNA documents in OpenDNA preserve the original order.
If a query returns a match, the server will know which en-
crypted data was matched and can use its location to ﬁgure
out which SNPs were searched for. From this, the server can
infer the SNP values and the risk group searched for.
8. ACTIVE ATTACKS
BoPETs are eﬃcient only insofar as they rely on an un-
trusted server to execute the application’s server function-
ality. These operations must be veriﬁed by the client lest
they become an avenue for active attacks by a malicious
server. In this section, we use Mylar as a cautionary case
study to demonstrate how a malicious server can exploit his
unchecked control over security-critical operations to break
the conﬁdentiality of users’ queries.
First, we show how the server can obtain the delta value
that enables searches over a “tainted” principal whose keys
are known to the server. Then, we show how the design
of Mylar’s MKSE allows the server to perform dictionary
attacks on all search keywords, not just those that occur
in the tainted principal. This enables eﬃcient brute-force
attacks on all of the victim’s queries, past or future, over
any principal in the system. Using chat as our case study,
we demonstrate how this leads to the eﬀective recovery of
the information that Mylar was supposed to protect.
8.1 Forcing users into a tainted principal
As explained in Section 2.2, sharing of documents between
users in Mylar is implemented using wrapped keys. For ex-
ample, suppose Alice wants to invite Bob into her new chat
room. Alice generates a principal for the room, wraps it
with Bob’s public key, and adds the wrapped key to the
database. Finally, she adds this key’s id to the accessInbox
ﬁeld in Bob’s principal. When Bob’s client is informed of
a new wrapped key in its accessInbox, it immediately and
automatically creates the delta for this key, enabling Bob to
search over the contents of Alice’s chat room.
A malicious server can create its own principal, for which
it knows the keys. We call such principals tainted. It can
then add the tainted principal’s wrapped keys to the vic-
tim’s accessInbox, and the victim’s client will automati-
cally generate the delta for keyword searches over the tainted
principal. To stay stealthy, the server can then immedi-
ately remove the principal, wrapped keys, and delta from
the database, ensuring that the victim will not notice. This
attack has been veriﬁed by executing it in Mylar.
Mylar uses a stateless IDP (identity provider) to certify
that keys claimed by users and principals actually belong to
them. In the above example, it prevents a malicious server
from substituting its own keys for other users’ and princi-
pals’ keys. It does not protect the user from being forcibly
added to any principal chosen by the server.
Fixes. The problem is fundamental and generic. BoPETs
aim to preserve server functionality, and in multi-user appli-
cations, the server is responsible for conveying to users that
there is a new document that they can now query. To foil
this attack, all server actions involving adding access rights
to users must be veriﬁed or performed by the client. This in-
volves changing the structure of the application and adding
non-trivial access-control logic on the client side.
The solution suggested by Popa et al. [48] is to require
users to explicitly accept access to shared documents (and,
presumably, check the IDP certiﬁcate of the document’s
owner, although this is not mentioned in [48]). The al-
lowSearch function in the Mylar implementation is responsi-
ble for enforcing this. It contains one line of code, which up-
dates an otherwise unused variable and calls what we believe
is an obsolete version of the MylarCrypto.delta method
without any of the needed parameters. It is never invoked
in any publicly available Mylar or application code. For ex-
ample, in kChat, anyone can add a user to their chat room
without that user’s consent.
We do not believe that this solution, even if implemented
properly, would work for realistic applications. The seman-
tics of allowSearch are highly counter-intuitive: when the
user is told that someone shared a document with him and
asked “Do you want to be able to search this document?,” the
user must understand that answering “Yes” can compromise
all of his queries over his own documents, including those
that are not shared with anyone. Also, whenever diﬀerent
documents have diﬀerent access rights, every document must
be a separate principal. Asking the user for a conﬁrmation
for every document (e.g., email) to which he is given access
destroys user experience in many collaborative applications.
Collusion.
The allowSearch defense does not prevent
collusion attacks. If the user voluntarily joins a malicious
principal—for example, receives an email from a user he
trusts but who is colluding with the server—the result is
exactly the same: the server obtains the delta for keyword
searches over a tainted principal.
Gap between models and practice.
In the searchable
encryption literature, it is common to assume for simplicity
that the search index is static, i.e., ﬁxed before a security
experiment begins and does not change. As explained in
Section 3.3, Mylar deals with a more complicated access-
controlled search abstraction, where changes to the “index”
can take the form of document creation, user creation, or
access edge creation when an existing user is given access to
a document they did not previously have access to.
In the model claimed to provide the theoretical founda-
tion for Mylar [49], Popa and Zeldovich assume a static
access graph. Even if the model properly captured what
it means for MKSE to be secure (it does not—see Sec-
tion 3.2), the security of each user’s queries and documents
would critically depend on the user never touching any other
data—voluntarily or by accident—for which the adversary
knows the key. This model is not relevant for the practical