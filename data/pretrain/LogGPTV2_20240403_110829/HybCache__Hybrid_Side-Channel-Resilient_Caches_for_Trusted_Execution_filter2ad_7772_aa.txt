title:HybCache: Hybrid Side-Channel-Resilient Caches for Trusted Execution
Environments
author:Ghada Dessouky and
Tommaso Frassetto and
Ahmad-Reza Sadeghi
HybCache: Hybrid Side-Channel-Resilient Caches 
for Trusted Execution Environments
Ghada Dessouky, Tommaso Frassetto, and Ahmad-Reza Sadeghi, 
Technische Universität Darmstadt
https://www.usenix.org/conference/usenixsecurity20/presentation/dessouky
This paper is included in the Proceedings of the 29th USENIX Security Symposium.August 12–14, 2020978-1-939133-17-5Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX.HYBCACHE: Hybrid Side-Channel-Resilient Caches
for Trusted Execution Environments
Ghada Dessouky, Tommaso Frassetto, Ahmad-Reza Sadeghi
Technische Universität Darmstadt, Germany
{ghada.dessouky, tommaso.frassetto, ahmad.sadeghi}@trust.tu-darmstadt.de
Abstract
1
Introduction
Modern multi-core processors share cache resources for max-
imum cache utilization and performance gains. However, this
leaves the cache vulnerable to side-channel attacks, where
inherent timing differences in shared cache behavior are ex-
ploited to infer information on the victim’s execution pat-
terns, ultimately leaking private information such as a secret
key. The root cause for these attacks is mutually distrusting
processes sharing the cache entries and accessing them in a
deterministic and consistent manner. Various defenses against
cache side-channel attacks have been proposed. However,
they suffer from serious shortcomings: they either degrade
performance signiﬁcantly, impose impractical restrictions, or
can only defeat certain classes of these attacks. More im-
portantly, they assume that side-channel-resilient caches are
required for the entire execution workload and do not allow
the possibility to selectively enable the mitigation only for
the security-critical portion of the workload.
We present a generic mechanism for a ﬂexible and soft
partitioning of set-associative caches and propose a hybrid
cache architecture, called HYBCACHE. HYBCACHE can be
conﬁgured to selectively apply side-channel-resilient cache
behavior only for isolated execution domains, while providing
the non-isolated execution with conventional cache behavior,
capacity and performance. An isolation domain can include
one or more processes, speciﬁc portions of code, or a Trusted
Execution Environment (e.g., SGX or TrustZone). We show
that, with minimal hardware modiﬁcations and kernel sup-
port, HYBCACHE can provide side-channel-resilient cache
only for isolated execution with a performance overhead of
3.5–5%, while incurring no performance overhead for the
remaining execution workload. We provide a simulator-based
and hardware implementation of HYBCACHE to evaluate the
performance and area overheads, and show how HYBCACHE
mitigates typical access-based and contention-based cache
attacks.
For decades now, upcoming processor generations are being
augmented with novel performance-enhancing capabilities.
Performance and security of processor architectures and mi-
croarchitectures are considered exclusively independent de-
sign metrics, with architects primarily focused on the more
tangible performance beneﬁts. However, the recent outbreak
of micro-architectural cross-layer attacks [4–6, 18, 19, 22, 42,
44, 46, 47, 50, 56, 59, 68, 70, 79], has demonstrated the critical
and long-ignored effects of micro-architectural performance
optimizations on systems from a security standpoint. It is be-
coming evident how performance and security are at conﬂict
with each other unless architects address the design trade-off
early on and not as an afterthought.
One prominent performance feature and the subject of a
wide range of recent architectural attacks is the use of caches
and cache-like structures to provide orders-of-magnitude
faster memory accesses. The intrinsic timing difference be-
tween a cache hit and miss is one of various side channels
that can be exploited by an adversary process via a carefully
crafted side-channel attack to infer the memory access pat-
terns of a victim process [23,25–29,34,35,38,54,61,71,77,78].
Consequently, the adversary can leak unauthorized informa-
tion, such as a private key, hence violating the conﬁdentiality
and isolation of the victim process.
Cache Side-Channel Attacks.
In earlier years, cache side-
channel attacks have been shown to compromise crypto-
graphic implementations [8, 54, 61, 78]. More recently, attack
variants such as Prime + Probe [34, 38, 54, 61] and Flush +
Reload attacks [29, 78] are being demonstrated on a much
larger scale. They have been shown to bypass address space
layout randomization (ASLR) [23, 25], infer keystroke behav-
ior [26,27], or leak privacy-sensitive human genome indexing
computation [11], whereby millions of platforms using vari-
ous architectures have been shown vulnerable to such attacks.
The attacks require an adversary to orchestrate particular
cache evictions of target memory addresses of interest and
USENIX Association
29th USENIX Security Symposium    451
after a time interval measure its own memory access latencies
or observe relevant computation and proﬁle how it has been
affected. This enables the adversary to deduce the victim’s
memory access patterns and infer dependent secrets. Cache
side-channel attacks have been shown to exploit core-speciﬁc
caches as well as shared last-level caches across different
cores or virtual machines [27,38,54]. Even hardware-security
extensions and trusted execution environments (TEEs) such
as Intel SGX [13, 33] and ARM TrustZone [7] are not im-
mune to these attacks. While they do not claim cache side-
channel security, recent cache side-channel attacks targeting
SGX [11, 21, 60, 66] and TrustZone [49, 80] have been shown
to compromise the acclaimed privacy and isolation guarantees
of these security architectures, thus undermining their very
purpose.
Existing Cache Defenses. To defeat cache side-channel
attacks, there has been extensive research on techniques to
identify and mitigate information leaks in a software’s mem-
ory access patterns [16, 17, 45]. However, mitigating these
leaks efﬁciently for arbitrary software (beyond cryptographic
implementations) remains impractical and challenging. Alter-
natively, hardware-based and software approaches have been
proposed to modify the cache organization itself to limit cache
interference across different security domains. Examples in-
clude modifying replacement and leveraging inclusion poli-
cies [39,76], as well as approaches that rely on cache partition-
ing [24, 40, 41, 51, 72, 73, 82], and randomization/obfuscation-
based schemes [52, 53, 63, 69, 73] to randomize the relation
between the memory address and its cache set index.
While strict cache partitioning is the intuitive approach
to provide complete cache isolation and non-interference
between mutually distrusting processes, it remains highly
impractical and prevents efﬁcient cache utilization. On the
other hand, randomization-based approaches make the attacks
computationally much more difﬁcult by randomizing the map-
ping of memory addresses to cache sets. However, existing
schemes either require complex management logic, impose
particular restrictions, rely on weak cryptographic functions,
or mitigate only some classes of cache side-channel attacks.
Most importantly, all of the aforementioned schemes are de-
signed to provide side-channel cache protection for the entire
code execution, which is actually not required in practice.
Our Goals. We observe that usually the majority of the
code is not security-critical. Typically, a small portion of the
code is security-critical and requires cache-based side-channel
resilience. Moreover, this security-critical portion of the code
is often already running in an isolated environment, such as
in a TEE or in an isolated process. In these cases, a trusted
component, namely the processor hardware or microcode or
the operating system kernel, enforces this isolation. We aim to
leverage and extend this existing isolation mechanism to also
selectively enable side-channel resilience for the caches only
for the portion of the code that needs it, without reducing the
cache performance for the remaining non-isolated code. In
doing so, we practically address the persistent performance-
security trade-off of caches by providing the system adminis-
trator with a "tuning knob" to conﬁgure by balancing and iso-
lating the workload as required. Consequently, s/he can tune
the resulting cache side-channel resilience, utilization, and
performance, while guaranteeing no performance overhead
is incurred on the non-isolated portion of the code execution.
Only the isolated (usually the minority) portion is subject to
a reasonable reduction in cache capacity and performance –
the cost of increased security guarantees.
To achieve this ﬂexible and hybrid cache behavior, we
introduce HYBCACHE, a generic mechanism that protects iso-
lated code from cache side-channel attacks without reducing
the cache performance for the remaining non-isolated code.
In HYBCACHE, isolated execution only uses a pre-deﬁned
(small) number of cache ways1 in each set of a set-associative
cache. It uses these ways fully-associatively, while for evic-
tion random victim cache lines are selected to be replaced
by new ones, thus breaking the set-associativity and remov-
ing the root cause of access leakage. Non-isolated execution
uses all cache ways set-associatively as usual, without any
performance overhead. While isolated and non-isolated exe-
cution may compete for the use of some ways in the cache,
the random replacement policy and fully-associative mapping
used by the isolated execution prevent leaking information
about the accessed memory locations (and their cache set
mapping) to the non-isolated execution, thus making the pre-
computation and construction of an eviction set impossible.
Moreover, HYBCACHE ﬂexibly supports multiple, mutually
distrusting isolated execution domains while preserving the
above security guarantees individually for each domain.
HYBCACHE is architecture-agnostic, and can be seam-
lessly integrated with any isolation mechanism (TEEs or inter-
process isolation); the deﬁnition of the isolation domains and
the distribution of the workload is left up to the system admin-
istrator. HYBCACHE is backward compatible by design; it
provides conventional set-associative caches for the workload
if the side-channel resilience feature is not supported.
Contributions. The main contributions of this paper are as
follows.
• We present HYBCACHE, the ﬁrst cache architecture de-
signed to provide ﬂexible conﬁguration of cache side-
channel resilience by selectively enabling it for isolated
execution without degrading the performance and avail-
able cache capacity of non-isolated execution.
• We evaluate the performance overhead of a simulator-
based implementation of HYBCACHE and show that it
is less than 5% for the SPEC2006 benchmarks suite,
1Ways are different available entries in a cache set to which a particular
memory address can be allocated.
452    29th USENIX Security Symposium
USENIX Association
and estimate the memory and area overheads of a cycle-
accurate hardware implementation of HYBCACHE.
• We show – through our security analysis – how breaking
set-associative mapping and shared cache lines between
mutually distrusting isolation domains (which are the
root causes for typical cache side-channel attacks besides
the intrinsic cache sharing and competition) mitigates
typical contention-based and access-based cache attacks.
2 Cache Organization, Attacks and Defenses
We brieﬂy present the typical cache organization, as well as
recent cache side-channel attacks that are within the scope of
our work, and limitations of existing defenses.
2.1 Cache Organization
Cache Structure. Caches are typically arranged in a hi-
erarchy of fastest/closest/smallest to slowest/furthest/largest
levels of cache, respectively L1, L2, and L3 cache/last-level-
cache (LLC). Each core incorporates its L1 and L2 caches and
shares the LLC with other on-chip cores. A cache consists of
the storage of the actual cached data/instructions and the tag
bits of their corresponding memory addresses. Cache memory
is organized into ﬁxed-size memory blocks, called cache lines
each of size B bytes. Set-associative caches are organized
into S sets of W ways each (called a W-way set-associative
cache) where each way can be used to store a cache line. A
single cache line can only be allocated to only one of the
cache sets, but can occupy any of the ways within this cache
set. The least signiﬁcant log2 B bits are the block offset bits
that indicate which byte block within the B-Byte cache line
is requested. The next log2 S bits are the index bits used to
locate the correct cache set. The remaining most signiﬁcant
bits are the tag bits for each cache line.
In a set-associative cache, once the cache set of a requested
address is located, the tag bits of the address are matched
against the tags of the cache lines in the set to identify if it is
a cache hit. If no match is found, then it is a miss at this cache
level, and the request is sent down to the next lower-level
cache in the hierarchy until the requested cache line is found
or fetched from main memory (cache miss). However, in a
fully-associative cache, a cache line can be placed in any of
the cache ways where the entire cache serves as one set. No
index bits are required, but only log2 B block offset bits and
the rest of the bits serve as tag bits.
Eviction and Replacement. Due to set-associativity and
limited cache capacity, cache contention and capacity misses
occur where a cache line must be evicted in favor of the
new cache line. Which cache line to evict depends on the
replacement policy deployed, some of which include First-in-
First-Out (FIFO), Least-Recently-Used (LRU), pseudo-LRU,
Least-Frequently-Used (LFU), Not-Recently-Used (NRU),
random and pseudo-random replacement policies. In practice,
approximations to LRU (pseudo-LRU) and random replace-
ment (pseudo-random) are usually deployed.
2.2 Cache Side-Channel Attacks
Cache side-channel attacks pose a critical threat to trusted
computing and underlie more proliferating side-channel at-
tacks such as the Spectre [44] and Meltdown [50] vari-
ants. Different classes of these attacks have been demon-
strated on all platforms and architectures ranging from mo-
bile and embedded devices [49] to server computing sys-
tems [34, 54, 81]. They have also been shown to undermine
the isolation guarantees of trusted execution environments,
like Intel SGX [11, 21, 60, 66] and ARM TrustZone [49, 80].
Such attacks have been shown to infer both ﬁne-grained and
coarse-grained private data and operations, such as bypass-
ing address space layout randomization (ASLR) [23, 25],
inferring keystroke behavior [26, 27], or leaking privacy-
sensitive human genome indexing computation [11], as well
as RSA [54, 81] and AES [10, 34] decryption keys.
Cache side-channel attacks exploit the inherent leakage
resulting from the timing latency difference between cache
hits and misses. This is then used to infer privacy/security-
critical information about the victim’s execution. In an ofﬂine
phase, the attacker must ﬁrst identify the target addresses of
interest (by means of static and dynamic code analysis of
the victim program) whose access patterns leak the desired
information about the victim’s execution, such as a private
encryption key. In an online phase, the attacker measures
the timing latency of its memory accesses or the victim’s
computation time to infer the desired information.
To demonstrate how a simple cache attack works, consider
the pseudo-code of the Montgomery ladder implementation
for the modular exponentiation algorithm shown in Algo-
rithm 1. Modular exponentiation is the operation of raising a
number b to the exponent e modulo m to compute be mod m
and is used in many encryption algorithms such as RSA. Leak-
ing the exponent e may reveal the private key. As shown in
Algorithm 1, the operations performed for each of the expo-
nent bits directly correspond to the value of the bit. If the
exponent bit is a zero, the instruction in Line 5 is executed.
If the exponent bit is a one, the instruction in Line 9 is exe-
cuted. An attacker that can observe or deduce these execution
patterns can thus disclose the value of each corresponding ex-
ponent bit, and eventually recover the encryption key [78, 81].
S/he, however, needs to identify the target addresses that need
to be observed (the addresses of the instructions in Lines 5
and 9 in this example) in the victim program and accordingly
construct the eviction set. The eviction set is a collection of
addresses that are mapped to the same speciﬁc cache set to
which the target addresses are also mapped. The attacker uses
this eviction set to evict the contents of the whole set in the
cache, and therefore guarantee to successfully evict the target
USENIX Association