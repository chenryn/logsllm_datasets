### Log and Event Analysis of Zookeeper Leader Election

#### Log and Event Data
The log and event data provided below details the interactions and state transitions during a leader election process in a Zookeeper cluster. The events include both `LOG` entries, which are timestamped logs from `log4j`, and other events captured by a syscall tracer.

```
LOG
LOG
LOG
LOG
4
4
SND
SND
SND
SND
SND
SND
SND
SND
SND
SND
1
1
1
1
184
1
1
1
1
184
SND
SND
1
1
6
1
SND
SND
1
184
SND
LOG
LOG
1
1
1
1
184
RCV
RCV
RCV
RCV
RCV
LOG
LOG
LOG
LOG
END
1
LOG
CONNECT
LOG
SND
SND
SND
SND
SND
SND
8
8
1
1
1
1
SND
14
LOG
LOG
LOG
1 QuorumCnxManager:365
Opening channel to server 1
2 FastLeaderElection:888
New election. My id =  1
3 QuorumCnxManager:644
Address of remote peer: 2
4 "zxid":0, "leader":1,
"election_epoch":1,
"epoch":1,"state":LOOKING
5 "zxid":0, "leader":2,
"election_epoch":1,
"epoch":1,"state":LOOKING
6 "zxid":0, "leader":2,
"election_epoch":1,
"epoch":1,"state":LOOKING
SND
SND
1
1
SND
5
1
SND
1
SND
184
RCV
RCV
RCV
RCV
RCV
1
1
1
1
184
RCV
RCV
RCV
RCV
RCV
RCV
RCV
RCV
RCV
1
1
1
1
184
1
1
1
1
RCV
184
```

#### Space-Time Diagram Explanation

**Figure 5.** Space-time diagram of two Zookeeper servers connecting to each other and performing a leader election. Vertical lines represent thread timelines, and circles denote executed events on each thread timeline. Dashed and solid rectangles represent different protocol steps, such as connection establishment and leader election.

- **LOG Events:** These correspond to timestamped entries logged by `log4j`.
- **SND and RCV Events:** These were collected by the syscall tracer.
- **Highlighted Events:** Certain events are highlighted with circled numbers, and their message content (for `LOG` events) or payload (for `SND` and `RCV` events) is displayed for clarity.

#### Detailed Analysis

1. **Connection Establishment:**
   - After booting, the server joining the quorum connects to the existing peer.
   - `LOG` events 1 and 3 show that process 5598 has `sid` 1, and process 5670 has `sid` 2. The lines of code where these messages were logged are also indicated (lines 365 and 644 of the `QuorumCnxManager` class).

2. **Leader Election:**
   - When the two quorum peers are connected, server 1 triggers a new leader election (event 2).
   - Each server can be in one of the following states: `LOOKING`, `FOLLOWING`, `LEADING`, and `OBSERVING`.
   - Initially, all servers are in the `LOOKING` state and vote for themselves by sending notifications to the other servers with the `leader` field set to their `sid` (messages 4, 5, and 6).
   - If a server receives a notification with a higher `sid`, it updates its vote proposal and broadcasts the new vote proposal to the rest of the quorum.

3. **Vote Messages:**
   - Server 1 sends the vote message `{“leader” : 1}` to server 2.
   - Server 2 sends the vote message `{“leader” : 2}` to server 1.
   - Server 1 receives the vote message from server 2, updates its vote proposal, and sends back the updated vote `{“leader” : 2}` to server 2.
   - Server 1 and server 2 update their states to `FOLLOWING` and `LEADING`, respectively.

4. **Additional Observations:**
   - **Notification Timeout:** Message 4 is sent twice due to a timeout when a server does not receive enough notifications within a given time frame.
   - **Message Partitioning:** Sending a message is partitioned into several `SND` events, involving multiple `write` syscall executions.
   - **Causality:** Messages 4 and 5 are not causally related, while there is a causal relationship between messages 5 and 6 based on state changes.

#### Performance Impact of Syscall-level Tracing

A micro-benchmark was developed to evaluate the performance overhead imposed on Zookeeper due to tracing syscalls with `strace`. The benchmark consists of a client issuing 10K iterations of four operations: checking if a znode exists, creating a new znode, checking again, and deleting the created znode.

- **Performance Comparison:**
  - Enabling tracing with `strace` negatively affects runtime performance, making Zookeeper 1.7× slower compared to the baseline.

#### Scalability of Constraint Solving

- **Log Levels:**
  - Depending on the debugging level, message logs may contain hundreds or thousands of entries.
  - The constraint solving time increases with the number of log entries. For example, adding `DEBUG`-level log events to the model caused the solver to take 3.18× more time than with an `INFO`-level log.

#### Related Work

- **Sherlog [2]:** Analyzes source code and runtime logs to isolate errors but is not suitable for distributed systems.
- **DTrace [13] and Fay [14]:** Dynamic instrumentation systems for diagnosing performance issues but do not establish causality relationships.
- **X-Trace [8] and Dapper [7]:** Tracing frameworks for distributed systems, providing end-to-end request tracing.
- **Magpie [6], Pinpoint [15], and Pivot Tracing [5]:** Online modeling systems that correlate interactions and generate performance models.
- **Canopy [4]:** End-to-end performance tracing tool that records causally-related performance data.

#### Conclusion

Falcon is an extensible tool pipeline for combining and visualizing log data from multiple sources. It merges log data, establishes happens-before relationships, and provides a coherent diagram for visual analysis. The tool was applied to Zookeeper, demonstrating its effectiveness in understanding remote thread interactions and message exchanges. The performance impact of syscall-level tracing is tolerable, and Falcon is scalable for detailed log analysis.

#### Acknowledgments

The authors thank the anonymous reviewers for their feedback. This work is financed by the ERDF – European Regional Development Fund through the Operational Programme for Competitiveness and Internationalisation - COMPETE 2020 Programme, and by National Funds through FCT - Fundação para a Ciência e a Tecnologia. The research received funding from the European Union’s Horizon 2020 under grant agreement No. 732051.

#### References

[1] X. Zhao, K. Rodrigues, Y. Luo, M. Stumm, D. Yuan, and Y. Zhou, “Log20: Fully automated optimal placement of log printing statements under specified overhead threshold,” in SOSP ’17. New York, NY, USA: ACM, 2017.
[2] D. Yuan, H. Mai, W. Xiong, L. Tan, Y. Zhou, and S. Pasupathy, “Sherlog: error diagnosis by connecting clues from run-time logs,” in ACM SIGARCH Comp. Arch. news, vol. 38, no. 1. ACM, 2010.
[3] L. Lamport, “Time, clocks, and the ordering of events in a distributed system,” Communications of the ACM, vol. 21, no. 7, 1978.
[4] J. Kaldor, J. Mace, M. Bejda, E. Gao, W. Kuropatwa, J. O’Neill, K. W. Ong, B. Schaller, P. Shan, B. Viscomi, V. Venkataraman, K. Veeraraghavan, and Y. J. Song, “Canopy: An end-to-end performance tracing and analysis system,” in SOSP ’17. New York, NY, USA: ACM, 2017.
[5] J. Mace, R. Roelke, and R. Fonseca, “Pivot tracing: Dynamic causal monitoring for distributed systems,” in SOSP ’15. ACM, 2015.
[6] P. Barham, R. Isaacs, R. Mortier, and D. Narayanan, “Magpie: Online modelling and performance-aware systems.” in HotOS ’03, 2003.
[7] B. H. Sigelman, L. A. Barroso, M. Burrows, P. Stephenson, M. Plakal, D. Beaver, S. Jaspan, and C. Shanbhag, “Dapper, a large-scale distributed systems tracing infrastructure,” Tech. Rep.
[8] R. Fonseca, G. Porter, R. H. Katz, S. Shenker, and I. Stoica, “X-trace: USENIX A pervasive network tracing framework,” in NSDI ’07. Association, 2007.
[9] F. Mattern, “Virtual time and global states of distributed systems,” in Parallel and Distributed Algorithms. North-Holland, 1988, pp. 215–226.
[10] I. Beschastnikh, P. Wang, Y. Brun, and M. D. Ernst, “Debugging distributed systems: Challenges and options for validation and debugging,” Communications of the ACM, vol. 59, no. 8, pp. 32–37, Aug. 2016.
[11] L. De Moura and N. Bjørner, “Z3: An efficient SMT solver,” in TACAS ’08/ETAPS ’08. Springer-Verlag, 2008.
[12] P. Hunt, M. Konar, F. P. Junqueira, and B. Reed, “Zookeeper: Wait-free coordination for internet-scale systems.”
[13] J. Mauro, DTrace: Dynamic Tracing in Oracle Solaris, Mac OS X, and FreeBSD. Prentice Hall, 2011.
[14] Ú. Erlingsson, M. Peinado, S. Peter, M. Budiu, and G. Mainar-Ruiz, “Fay: extensible distributed tracing from kernels to clusters,” ACM Transactions on Computer Systems (TOCS), vol. 30, no. 4, 2012.
[15] M. Y. Chen, E. Kiciman, E. Fratkin, A. Fox, and E. Brewer, “Pinpoint: Problem determination in large, dynamic internet services,” in DSN ’02. IEEE, 2002.