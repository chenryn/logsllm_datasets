3.3.3 Flow start incentives
At the start of each ﬂow, a sender neither knows the state
of the path to the destination nor the relative change the
additional ﬂow will cause. TCP’s slow-start phase incre-
mentally ﬁnds out both while also giving other ﬂows time
to make room for the new ﬂow.
The re-feedback incentive framework deliberately presents
a dilemma to a sender without recent path knowledge (e.g.
at the ﬁrst packet, or after an idle period). Sending un-
derstated DPM increases the risk of discard at the egress
dropper. But sending overstated DPM increases the risk of
sanction at the ingress policer as the ﬂow rises to full rate.
The strategies around this dilemma deserve a paper in their
own right, so here we merely provide an outline.
We should think of TCP’s exponential slow-start as de-
pendent on an implicit evolving estimate of path conges-
tion by the sender, starting pessimistically by assuming high
path congestion. Inverting TCP’s steady state rate equation
gives ρ ∝ 1
¯x2 to a ﬁrst approximation. So rate doubling quar-
ters the implicit path congestion estimate every round trip.
To safely pass the policer and the dropper, the sender should
be consistent, also using this implicit estimate of path con-
gestion to set the DPM in each sent packet. If it reduces its
path congestion estimate too quickly (increasing its rate ac-
cordingly), it will undershoot the true path congestion and
risk being caught by the egress dropper.
So the re-feedback incentive framework encourages cau-
tion at the start of a ﬂow in proportion to path uncertainty—
reminiscent of TCP’s slow start. However this claim greatly
depends on how quickly our mechanisms can detect and re-
move non-compliant behaviour.8
It is well-known that repeated binary congestion feedback
like ECN takes a long time to signal low congestion levels [9].
So ECN is not a good basis on which to build responsive
policing mechanisms. In the years it would take to deploy
the TCP modiﬁcations needed for our re-feedback extension
of ECN (§4), TCP will be hitting its own scalability limits.9
So although we believe re-ECN could start to solve policing
problems fairly quickly, we must emphasise that a multi-bit
congestion ﬁeld will need to be considered anyway. It would
provide responsive policing even if short ﬂows dominate the
future traﬃc mix. And at the same time, it would help ﬁx
TCP/IP for high capacity scenarios.10
This still leaves the problem of whether the new ﬂow will
If a
push a currently uncongested path into congestion.
low priority scavenger class is available, the simple solution
would be to rise aggressively to full rate in that class, then
pace packets for the desired class at the resulting ack rate.
3.4
Inter-domain incentive mechanisms
The overview of our incentive framework explained why
bulk inter-domain congestion charging emulates policing with
per-ﬂow precision. We now describe this mechanism.
At an inter-domain interface, only a single bulk counter
(and two temporary ones) per direction is needed. The main
counter merely accumulates the DPM ρ in every passing
packet over an accounting period Ta (e.g. a month). At
the end of the month, N1 should pay N2 the charge Ca =
λ
(cid:1)Ta ρ+, where λ is the ﬁxed price of congestion agreed be-
8Nonetheless, we can ensure detection of misbehaviour car-
ries over from old ﬂows to new: i) the above adaptive policer
remembers misbehaving senders; ii) during a DDoS attack
on a receiver, the dropper’s EWMA carries over from one
source to the next.
9The number of packets between binary congestion marks
scales O(x2) with TCP’s steady state rate—to sustain
10Gbps a ﬂow would only sawtooth every 90mins between
marks. While marking rate only scales O(x) with Kelly’s
rate control algorithm, that still doesn’t solve the problem,
given x is currently doubling every 1.6 years or so.
10An extra multi-bit ﬁeld in IP is proposed for the allowed
congestion window in XCP [12] and for the allowed sending
rate in Quick-Start [11].
tween them. To implement this with the re-feedback variant
of ECN described in §4.1, the meter would simply need to
increment or decrement by the size of packets marked with
the ECT(0) or CE code-points respectively.
To protect receiving domains from ‘denial of funds’ at-
tacks, any usage element of a charge should be ‘sender pays’.11
So λ ≥ 0 and persistently negative ρ should be ignored, given
negative congestion is physically impossible. To meter this,
packets with positive and negative ρ should be separately
accumulated in temporary counters with the two added ev-
ery few seconds, only accumulating a positive sum in the
main counter. Once neighbours agree that ‘no-one pays’
for persistent negative congestion, they are incentivised to
introduce the dropper (§3.2) to remove persistent negative
traﬃc, which no longer carries any ability to pay for further
downstream congestion.
‘Receiver pays’ can optionally be
arranged between edge operators without risk of ‘denial of
funds’ through an end-to-end clearinghouse [2].
We should clarify that we neither require nor expect uni-
versal inter-domain congestion charging. However, because
it exposes true costs, it is likely to emerge as the competi-
tive equilibrium [2]. Current tariﬀs such as 95th %ile peak
demand or volume charging may continue. But to compete,
manual price adjustments will be needed to track the conges-
tion price. So congestion charging is likely to predominate,
given it uses a simple, passive mechanism without regard to
ﬂows, but automatically adjusts the price to give the correct
upstream incentives to the precise ﬂows that deserve them.
The main alternative to usage charging is the service level
agreement, where a network contracts to keep metrics within
statistical limits. Currently, proving whether delay or loss
(impairment) budgets have been exceeded and by whom re-
quires a comprehensive system of trusted echo reﬂectors.
Re-feedback greatly simpliﬁes these problems of SLA ac-
countability, because it ensures downstream metrics are vis-
ible at each inter-domain border.
3.5 Distributed denial of service mitigation
A ﬂooding attack is inherently about congestion of a re-
source. Because re-feedback ensures the causes of network
congestion experience the cost of their own actions, it acts
as a ﬁrst line of defence against DDoS. As load focuses on
a victim, upstream queues grow, requiring packets to be
pre-loaded with a higher congestion metric.
If the source
does increase the initial metric, its own network’s ingress
policer will throttle the ﬂow. If the source doesn’t increase
the initial metric, it will become negative at the congested
resource, which will bias its drop against negative traﬃc.
Inter-domain congestion charging ensures that any net-
work that harbours compromised ‘zombie’ hosts will have
to pay for the congestion that their attacks cause in down-
stream networks. Therefore, it is incentivised to deploy our
adaptive policer (§3.3.2). The adaptive policer limits hosts
that persistently causes congestion to only send very slowly
into congested paths. As well as protecting other networks,
the extremely poor performance at any sign of congestion
will incentivise the zombie’s owner to clean it up.
re-feedback brieﬂy vulnerable (§§3.3.3 & 5).
Note, however, that delay in detecting attacks does leave
4. PROTOCOL ENGINEERING
Although the goal of this paper is not to prove that re-
feedback is immediately deployable, it does seem possible
to deploy a limited form, described in detail in a paper in
preparation and summarised here. Re-feedback consists of
at least three protocol elements: i) a congestion ﬁeld; ii) an
unloaded delay ﬁeld; and iii) a certain ﬂag (§2).
It is also necessary to ﬂag that re-feedback is in use, but
the certain ﬂag serves this purpose. There are no ﬂags in
IPv6 so an extension header would be required. In IPv4, bit
49 is possibly available, but there are many other competing
claims on it. An alternative approach would be to assign
new protocol ids to re-feedback transports.
4.1 Re-ECN
Two bits in the IP protocol are assigned to the ECN
ﬁeld [19]. The sender indicates an ECN capable transport
(ECT) using either of the two code-points 10 or 01 (ECT(0)
& ECT(1) resp.). Routers probabilistically set 11 if conges-
tion is experienced (CE). The choice of two ECT code-points
permitted future ﬂexibility, optionally allowing the sender to
encode a nonce [21] in the packet stream.
To re-align congestion feedback, we use this ﬂexibility in
a scheme we call re-ECN. Here we only discuss TCP/IP,
not other IP transports. No changes to the IP or TCP wire
protocols are required. Neither the IP handlers nor the TCP
receiver need changing, only the TCP sender. We deﬁne
what is eﬀectively a virtual header ﬁeld h, where hj = uj−zj
at any node j on the path. uj is the rate of CE and zj the
rate of ECT(0) traversing that node. As with current ECN,
no packets are sent with CE set: u0 = 0. And TCP feeds
back to the source any CE arriving at the destination in the
echo congestion experienced (ECE) ﬁeld.
For re-feedback, the sender arranges the starting value h0
of this virtual header so that it will reach a standardised
datum at the destination hz = 0. So, we need zn = un,
which will result if the sender sets ECT(0) on the proportion
of packets z0 = un/(1 − un).12
To set this proportion of ECT(0), the TCP sender’s ack
handler should set ECT(0) in the next packet after an ECE
arrives in an ack (treating drops equivalently). And it should
set an extra ECT(0) every (U − 1) ECEs, where U is the
EWMA of the number of packets between successive ECE.
From Eqn 10 in Table 1 the virtual header hj in pack-
ets arriving at any node j on the path is suﬃcient for the
node to derive a prediction of downstream path congestion
ρj ≈ −hj. In other
ρj = 1− 1/(1− hj). For small hj (cid:13) 1;
words, downstream congestion can be approximated simply
by subtracting the rate of ECT(0) from that of CE. As we
haven’t changed the rate of CE marking, it still represents
upstream congestion. So in 1.5 bits we have encoded down-
stream congestion − hj, upstream congestion uj and whole
path congestion zj.
We chose z as the rate of ECT(0) rather than ECT(1) de-
liberately. Existing ECN sources set ECT(0) at either 50%
(the nonce) or 100% (the default). So they will appear to
a re-feedback policer as very highly congested paths. When
policers are ﬁrst deployed their threshold greediness α can
be conﬁgured permissively, allowing through both ‘legacy’
ECN and misbehaving re-ECN ﬂows. Then, as the thresh-
11A capacity charge made to the larger network, whatever
the direction of traﬃc, might well complement congestion
charging (or any form of usage charging).
12Details of what to do for high congestion beyond un > 1/2
and why it is safe are omitted for brevity.
old is set more strictly, the more legacy ECN sources will
gain by upgrading to re-ECN.
We emphasise that we believe a multi-bit congestion ﬁeld
will eventually be needed if ﬂow rates are to continue to rise
(§3.3.3). We propose an extension header for IPv6, including
a ﬁeld to shadow TTL (like Quick-Start [11]). So, if the
whole path of routers doesn’t support the extension, the end-
points can fall back to re-ECN or drop. Transport protocols
would also all have to be updated for multi-bit ﬁelds.
4.2 Re-TTL
Delay re-feedback can be achieved by overloading the TTL
ﬁeld, without changing IP or router TTL processing. A tar-
get value for TTL at the destination would need standardis-
ing, say hz = 16 (Fig 2). If the path hop count increased by
more than hz during a routing change, it would temporarily
be mistaken for a routing loop, so hz would need to be cho-
sen to exceed typical hop count increases. The TCP wire
protocol and handlers would need modifying to feed back
the destination TTL and initialise it as in Table 1.
It would be necessary to standardise the unit of TTL in
terms of real time. Precision could be improved in the longer
term if routers decremented TTL to represent exact prop-
agation delay to the next router. That is, for a router to
decrement TTL by, say, 1.8 time units it would alternate be-
tween 1 & 2 at a ratio of 1:4. Although this might sometimes
require a seemingly dangerous null decrement, a packet in a
loop would still decrement to zero after 255 time units.
5. DROPPER PERFORMANCE
The re-feedback incentive framework relies critically on
how quickly the dropper (§3.2.1) can detect and isolate ﬂows
that are maliciously understating congestion, and how much
collateral damage is suﬀered by honest packets. The error in
an honest source’s prediction of congestion for re-feedback
(Eqn 11) depends on how well path congestion in one round
trip correlates with congestion the next. If the correlation is
weak, to avoid falsely dropping honest traﬃc the dropper has
to heavily smooth out all the variation, making it sluggish
to respond to a movement in the average due to an attack.
We chose to use ns2 to run a series of simulations with
highly demanding sets of ﬂows arriving at the dropper, some
having traversed up to ﬁve potential bottlenecks. Below are
the highlights of the experiments. Details are in the longer
version of this paper.
We implemented the multi-bit variant of congestion re-
feedback carrying real numbers in TCP Reno using the ini-
tialisation and combining functions in Table 1. For the local
congestion metric at each router mi, we extracted the real
value of the marking probability, pb, used within the RED al-
gorithm [8, §4] before its transformation into a binary mark.
However, to be more demanding we still allowed TCP rate
control to respond in its usual sawtooth way to binary ECN
feedback and drops. We bounded headers h within [−1, 1].
We implemented the dropper within the RED module,
simulating packet truncation as its sanction—in order to
preserve the feedback loop. We omitted ﬂow-focused drop-
ping as our initial aim was to assess feasibility. From Eqn
10 we approximated downstream congestion as ρn ≈ −hn,
using hz = 0.
Simulation model: We used a parking lot topology of
5 core nodes n1 to n5, connected by 10Mbps links. Queues
at all core routers were RED-ECN in the direction of traf-
Honest traffic                  RED queues:
wq=0.02, B=188B
Low
Mid   RTT
Upp
e
t
a
r
n
o
i
t
a
c
n
u
r
T
)
s
e
v
i
t
a
g
e
n
e
s
l
a
f
(
1.4%
1.2%
1.0%
0.8%
0.6%
0.4%
0.2%
0.0%
0.0001
0.01
Dropper smoothing coefficient, γ (log scale)
0.001
Figure 7: Eﬀect of dropper smoothing on truncation
rate for honest ﬂows from lower, mid & upper RTT
ranges (note: no focused dropper)
ﬁc (n1 to n5), and DropTail in the reverse direction with
suﬃciently large links to prevent ACK drops. The drop-
per ran on n5. Traﬃc entered the network from all nodes
n1–n4 and left it after a number of hops ranging across
(1,2,3, & 5). Transmission delays between core nodes were
3ms, while edge delays deﬁned a range of RTTs between
90–500ms, averaging ˜250ms. TCP ﬂows through the drop-
per were grouped in three classes according to their typical
RTT: low (L), medium (M), and high (U) of the order of
100, 250 & 500ms.
The traﬃc model consisted of 400 sources of which 110
were TCP-ECN and the rest UDP, with TCP traﬃc con-
sistently > 90% of total bits. This reﬂected current [3]
not necessarily future Internet traﬃc (when reduced TCP
volume is expected). Packet sizes were all 1500B. We did
not explicitly model HTTP but deﬁned 100 TCP sources as
FTP, uniformly varying sessions from small (20pkt) to large
(1500pkt), with sources’ average idle times exponentially
distributed. The remaining 10 FTP sessions transferred
inﬁnite-sized ﬁles and traversed all core nodes. The UDP
sources were packet trains with both ON and OFF times
Pareto distributed with parameter 1.9. The resulting fre-
quent short-lived and sporadic long-lived sessions reﬂected
long-tailed Internet traﬃc. Traﬃc proﬁles were subject to
random variations with RED queue utilisation varying from
high 80s to low 100s percentages throughout. Traﬃc sources
were initially generated at random uniformly between 0 and
20s; statistics collection began 30s into the 300s simulation.
The (gentle) RED parameters were set to the currently rec-
ommended values relative to buﬀer size.
Simulation results: We used solely honest sources to
ﬁnd the dropper’s baseline sensitivity under various condi-
tions. Fig 7 is typical, leading us to use smoothing coef-
ﬁcients just below the knee of the curve for our later ex-
periments with dishonest ﬂows. That is γ = 0.0005, 0.001
or 0.002. Even in the last case truncation rates were only