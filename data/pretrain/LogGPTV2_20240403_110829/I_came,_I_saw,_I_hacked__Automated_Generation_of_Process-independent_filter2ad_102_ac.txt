through a pre-trained classifier (Section 4.1). The classifier performs
both image-based and extracted text-based classification. When the
ICS sector is identified, the code traverses the network to scan for
potential target PLCs. After that, when the code reaches a PLC, it
downloads the control binary from the PLC and analyzes it to clas-
sify the process controlled by that PLC. It also extracts interesting
parameters (Section 4.4). Using all the information dynamically ex-
tracted, the code can deploy one of many operationally undetectable
attacks. These can either be attacks that cause stable perturbation
to a physical quantity resulting in non-optimal operation or at-
tacks that cause an oscillating perturbation within upper and lower
limits that result in long term physical damages to the system‚Äôs
infrastructure. Another type of attack is a ticking bomb attack that
allows the attacker to escape before the attack is deployed just by
manipulating the PLC binary using ICSREF.
4 MACHINE LEARNING-BASED
RECONNAISSANCE
To effectively attack an unknown industrial setting, we first need to
identify the ICS sector and then the specific ICS process monitored
and controlled by the HMI. For simplicity, we use the following
terms in the context of target fingerprinting:
‚Ä¢ ICS Sector fingerprinting is the categorization of the plant
under one of the 16 Department of Homeland Security (DHS) CI
categories [34]. This gives a more generic view of the plant.
‚Ä¢ ICS Process fingerprinting is the identification of the process
being controlled by the infected PLCs. This provides information
with more granularity.
To build knowledge, we use publicly available information (im-
ages and PLC binaries) to build machine learning models that can
efficiently fingerprint and extract data to launch process-aware
attacks post-infection. We do not attempt any manual reconnais-
sance to gain any specific information about the target. ICS sector
fingerprinting requires a more generic view of the infected plant:
This is the accepted version of the article shared by the authors. The final published version will be available at AsiaCCS 2020 Proceedings.
Network
architecture
Test
accuracy Epochs
69.12
71.52
62.13
71.021
69.221
67.88
80.907
6
3
29
15
5
95
51
Image size Training
accuracy
128 √ó 128
98.84
256 √ó 256
97.80
512 √ó 512
70.35
256 √ó 256
98.99
256 √ó 256
92.79
256 √ó 256
91.32
256x256
99.15
DNN-1
DNN-2
DNN-3
DNN-4
DNN-5
Table 3: Experimental results for different image sizes
and architectures described in Section 4.1.1 on constructed
dataset. The number of epochs in the table represent the it-
eration at which the highest accuracy was achieved.
achieved which is almost similar to the test accuracy of DNN-1. But
using dropout layer, we were able to stabilize the training, and the
best model was achieved after learning through more number of
iterations i.e. through 15 iterations instead of 3 as for DNN1.
Transfer learning: The motivation of transfer learning is that a
sufficiently trained model is independent of the data with which
it was trained and the knowledge gained from one dataset can be
used to learn features on another dataset. The pre-trained model
may further be tuned (by training only a few layers) to cater to the
particular dataset. Before applying transfer learning, we trained
two slightly deeper networks than DNN-1 to observe the impact
of deeper architectures in successfully creating efficient models.
We performed experiments with (256x256) sized images for deeper
layers: DNN-3 with architecture (Conv-32,Conv-32 ReLU, MP) +
(Conv-32, ReLU, MP) + (Conv-64, ReLU, MP) + (FC-64,ReLU) and
DNN-4 with architecture (Conv-32, ReLU, MP, Dropout) + (Conv-
32, ReLU, MP, Dropout) + (Conv-64, ReLU, MP,Dropout) + (Conv-
128, ReLU, MP, Dropout) + (FC-64,ReLU). From Table 3, we infer
that the difference between test and training accuracy lowers, and
training stabilizes with increment in the number of layers. Thus,
we choose a smaller number of trainable layers in our network
with a larger number of non-trainable layers with dropout and data-
augmentation for improving test accuracy using transfer learning.
We performed experiments on transfer learning with VGG16
weights available using the Keras module for Python 3.6.5. The ar-
chitecture of the final model has the non-trainable layers of VGG16
architecture, followed by a fully connected layer and Dropout layer
(DNN-5) trained using data-augmentation. We observed that with
transfer learning, we stabilized the training process, reduced the
gap between training and test accuracy and improved test accuracy
to 80.907% from the best test accuracy of 71.52%.
Insights: For conventional image classification problems, the model
learns to use similarities in shapes and edges but here, the same
chemical plant HMI may be designed to represent data in different
ways (one may be visual, depicting reactors, devices, etc. and other
may depict just the sensory information). Moreover, the similarities
in colors can misguide the model into using RGB values to classify
an HMI as similarly colored images may belong to different classes
(Fig. 2). In a nutshell, the differences are more complex than ori-
entation, background, illumination, or color differences between
images which prevents high accuracy using raw HMI screenshots.
Figure 3: HMI screenshot of a water treatment plant in
United States. The Siemens Simatic software-based SCADA
was hacked and a screenshot was posted on pastebin [9].
and low overfitting, by changing the number and type of layers,
pre-processing techniques, and overfitting countermeasures.
Selection of optimal size of images: Resizing images to smaller
dimensions may approximate the information while, bigger dimen-
sions may tamper the genuine features, and significantly increase
the training time. To find the optimal size, we built a small network
of (Conv-32,ReLU,MP) + (Conv-32,ReLU,MP) + (FC-64,ReLU) lay-
ers and called it DNN-1. Table 3 summarizes the training and test
accuracy for different image sizes. Images of size (128x128) render
best training accuracy (98.84%) while images of size (256x256) offer
slightly higher test accuracy (‚âà 2% higher). However, in both cases,
we observe high evidence of overfitting from the difference be-
tween training and test accuracy and the smaller number of epochs
required to train. Larger image sizes of (512x512) are also unsuit-
able because of very low training accuracy indicating an inability
to learn features and hampering the overall efficiency of the pay-
load. Moreover, using bigger images increases computation during
training as well as evaluation. Therefore, we choose (256x256) for
further experiments to deduce the final architecture. We take two
measures to reduce overfitting- data augmentation and dropout.
Data Augmentation: Data Augmentation has been proven to be
extremely effective in forcing the model to learn robust (generic)
features. In our case, when we trained the same architecture but
without any DA, we achieved our best training and test accuracy
in just 3 iterations reflecting extreme overfitting. Some augmenta-
tion mechanisms like rotate, vertical flip, and shear are not viable
in HMI based sector identification task; therefore, we focused on
width and height shift along with zoom range obtaining the follow-
ing [training, test] accuracy. For height-shift (0.2) we achieved an
accuracy of [99%, 69.987%] in 23 epochs and for zoom (0.15), height-
shift (0.2) and width-shift (0.2), we achieved [73.35%, 60.15%]. We
found height-shift (0.1) and width-shift (0.1) to be the best trade-off
between accuracy and overfitting problems, where we achieved
[96.54%, 67.03%] in 49 epochs.
Dropout: Since dropout layer removes a percentage of neurons
during training, it reduces the co-adaptation of neurons. This pre-
vents the model from overfitting by forcing the neurons to learn
features without being dependent on other neurons. This regular-
ization method is extremely useful in preventing overfitting [42]. In
our experiments, when DNN-1 was trained for image size (256x256)
without dropout layer (DNN-2), a test accuracy of 71.021% was
This is the accepted version of the article shared by the authors. The final published version will be available at AsiaCCS 2020 Proceedings.
Multinomial Naive Bayes
Training Accuracy (%)
Testing Accuracy (%)
Parameters
No. of Features
500
1000
1500
2000
FB Strings HMI
HMI
Text
Text
30
75.70
100
35
1000
87.46
87.79
40
1500
2000
45
89.76
Feature Selection Statistic
ùúí2
85.67
87.46
Mutual Information
F_score
86.18
FB
84
84.85
81.82
84.85
84.83
84.85
84.8
Strings HMI
Text
70.21
82.30
85.29
84.04
81.91
85.30
88.23
80.91
97.1
82.30
76.5
82.97
84.04
80.85
FB
70.58
76
76.50
82.35
82.23
82.35
80.23
Support Vector Machine
Strings HMI
Text
80.30
80
88.74
80
90.02
80
80
91.04
Training Accuracy (%) Testing Accuracy (%)
Strings
73.33
73.33
73.33
73.33
Strings HMI
Text
91.17
77.6
75.53
91.18
75.53
97.05
97.05
75.53
FB
90.90
90.09
93.9
93.93
FB
76
76.47
82.35
82.35
80
80
80
87.72
88.74
89.25
93.9
93.93
93.9
97
97.05
79.41
79.78
75.53
74.46
82.35
82.35
82.4
73.33
73.33
73.33
sector and ‚Äòreactor‚Äô for the Chemical sector are intuitive, there are
some features that create confusion like ‚Äòfit‚Äô for the Water sector.
4.1.3 Combined classification model for ICS sector identification.
To further improve accuracy, we make our own classification model
that leverages the features learned from raw screenshots as well
as from the text strings extracted from them. We propose a paral-
lel classification model where the text-based classification model
works in conjunction with the pixel-based classification model. Fig.
4 shows the parallel architecture we use in our classification model.
In this model, an HMI screenshot is run through the image classifi-
cation, and a class is predicted with a probability of that screenshot
being in that class. In parallel, the image is also run through an
Optical Character Recognition (OCR), and text strings are extracted.
The text strings are translated if they are in any language other than
English. The strings are then cleaned, and the important features
are extracted. MNB used in our model then classifies the screenshot
with a prediction probability. We then compare the prediction prob-
abilities and choose the sub-model whose prediction probability
is higher. We choose the classification scheme based on predic-
tion probability because it is a reflection of the confidence level of
prediction from each model. Thus, having a parallel architecture
ensures a more accurate prediction. We performed experiments
and the test accuracy increased to 88.29% from 80.907%, if using
only image-based classification (Table 3) or from 84.04%, if using
only text-based classification (Table 4).
Table 4: Experimental results for different number of features and feature selection statistic for SVM and MNB models for
HMI text-based, FB-based and Strings-basedclassification. The first section of the table gives details for selection of number
of features. Second part of the table depicts the experiments for feature selection statistic.
4.1.2 Classification based on text from HMI screenshots. Manual
inspection of the screenshots reveals that many HMIs provide an in-
terface for the data collected from the field devices (e.g., temperature
values) without distinct diagrams to illustrate the measurements
(example: Fig. 3). Thus, learning algorithms suffer in accuracy when
learning from strictly pixel-based features. We tune the parameters
for a Support Vector Machine (SVM) model and Multinomial Naive
Bayes (MNB) model on text recovered through Optical Character
Recognition (OCR) to finalize the ML parameters for text-based
classification and summarize the results in Table 4 (Col:‚ÄúHMI Text‚Äù).
Cleaning strings: In our dataset, more than 70% of the images
contain English text strings with Czech, Turkish, and Thai being the
other popular languages. Translating them sometimes did not yield
legible words or combination of letters, especially accents. Thus,
we removed any character that was not a letter. We also removed
digits because they did not provide any useful information about
the sector to which a particular HMI belonged. For text extraction