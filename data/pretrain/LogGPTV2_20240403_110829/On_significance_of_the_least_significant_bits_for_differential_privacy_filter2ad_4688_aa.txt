title:On significance of the least significant bits for differential privacy
author:Ilya Mironov
On Signiﬁcance of the Least Signiﬁcant Bits For
Differential Privacy
Ilya Mironov
Microsoft Research Silicon Valley
1065 La Avenida
Mountain View, CA, 94043
PI:EMAIL
ABSTRACT
We describe a new type of vulnerability present in many im-
plementations of diﬀerentially private mechanisms. In par-
ticular, all four publicly available general purpose systems
for diﬀerentially private computations are susceptible to our
attack.
The vulnerability is based on irregularities of ﬂoating-
point implementations of the privacy-preserving Laplacian
mechanism. Unlike its mathematical abstraction, the text-
book sampling procedure results in a porous distribution
over double-precision numbers that allows one to breach dif-
ferential privacy with just a few queries into the mechanism.
We propose a mitigating strategy and prove that it sat-
isﬁes diﬀerential privacy under some mild assumptions on
available implementation of ﬂoating-point arithmetic.
Categories and Subject Descriptors
H.2.7 [Database Management]: Database Administra-
tion—Security, integrity, and protection
General Terms
Security
Keywords
diﬀerential privacy, ﬂoating point arithmetic
1.
INTRODUCTION
The line of work on privacy of statistical databases start-
ing with seminal papers by Dwork et al. [13, 9, 12] advanced
a general approach towards achieving privacy via random-
ization of outputs.
Instead of releasing accurate answers
to statistical queries, which potentially leads to a breach of
privacy, the curator of a database randomizes its responses.
For numerical outputs, the randomization process typically
involves adding to the accurate answer a secret random
number (noise) sampled from a publicly-known distribution,
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CCS’12, October 16–18, 2012, Raleigh, North Carolina, USA.
Copyright 2012 ACM 978-1-4503-1651-4/12/10 ...$15.00.
whose shape and parameters are chosen to guarantee a cer-
tain level of privacy.
Realizations of diﬀerentially private mechanisms include
privacy-preserving histograms [28], logistic regressions [3],
recommender systems [33], network-analysis tools [32], and
many others.
In response to the need for solutions that
would simplify development of privacy-preserving systems,
with speciﬁc focus on cloud computing, several platforms
were proposed in recent years: PINQ [30], Airavat [39],
Fuzz [18], and GUPT [34]. These systems diﬀer in their im-
plementation languages, target operating systems, and pro-
gramming models, and yet they share essentially the same
noise-generation mechanism central to diﬀerential privacy.
We describe a new type of vulnerability present in all gen-
eral purpose systems and many implementations of diﬀeren-
tially private algorithms available to us. The vulnerability,
which can be remarkably eﬀective against diﬀerentially pri-
vate systems, exploits the porous distributions of textbook
implementations of the Laplacian mechanism resulting from
ﬁnite precision and rounding eﬀects of ﬂoating-point oper-
ations. While diﬀerential privacy requires all outputs to be
feasible for all possible inputs and having similar probabil-
ities when the inputs are close, the results of ﬂoating-point
arithmetic will be concentrated on a small subset of out-
puts. Careful examination of these subsets demonstrate that
they overlap only partially on close inputs, thus breaking the
guarantee of diﬀerential privacy, which would have applied
if all operations were computed with inﬁnite precision and
unlimited source of entropy.
Our attack defeats systems that were speciﬁcally designed
to eliminate side-channel vulnerabilities, employing strate-
gies such as LSM-based mandatory access control mecha-
nisms [39, 34], or a generalized linear-type system [18]. We
remark that while these systems carefully block all tradi-
tional side channels—timing and state—the output of the
diﬀerentially private mechanism, which is supposed to be
safe to release due to its mathematical properties, creates
a side channel of its own. The reason for this exploitable
vulnerability is that ﬂoating-point implementations of dif-
ferentially private algorithms deviate in important respects
from their mathematical abstractions.
Although workarounds are relatively simple and low over-
head, the fact that these independent and very diverse im-
plementations are all susceptible to the same attack, demon-
strates that subtle properties of ﬂoating-point arithmetic can
be easily overlooked.
The paper’s organization is as follows. We recall the def-
inition of diﬀerential privacy and describe the Laplacian
650mechanism in Section 2. Platforms for diﬀerentially private
computations are reviewed in Section 3, followed by discus-
sion of relevant facts of ﬂoating-point arithmetic and its im-
plications to implementations of the Laplacian mechanism
in Section 4. We describe our main attack in Section 4.5,
analyze its success probability in Section 4.6, and discuss
its implications for privacy. Section 5 on defense mecha-
nisms considers two ﬂawed approaches, followed by proof of
security of a ﬂoating-point implementation of the snapping
mechanism. Another example of vulnerability due to use of
ﬂoating-point arithmetic is given in Section 6.
2. DIFFERENTIAL PRIVACY
Diﬀerential privacy, introduced by Dwork et al. [9, 12] and
recently surveyed in [10], is a standard notion of privacy for
computations over databases. It is deﬁned for randomized
functions (i.e., distributions over the set of possible outputs)
and can be formally stated as follows:
Randomized function f : D 7→ R satisﬁes ϵ-diﬀerential
privacy if for all subsets S ⊂ R and for all pairs
of adjacent inputs D, D
′ ∈ D:
Pr[f (D) ∈ S] ≤ eϵ Pr[f (D
′
) ∈ S].
The exact deﬁnition of two databases being adjacent is do-
main-speciﬁc and dependent on the desired level of security
guarantee. Commonly used deﬁnitions are the user-level pri-
vacy, where two databases are considered adjacent if they
diﬀer in contribution of a single person, and the entry-level
privacy, where the protected entity is a single item, action,
or attribute.
An equivalent Bayesian interpretation of the deﬁnition of
diﬀerential privacy asserts that if f is ϵ-diﬀerentially-private
and O is the observed output of f , then for all priors p and
all pairs of adjacent D and D
p(D | O)
p(D′ | O)
≤ eϵ p(D)
p(D′)
.
′
If two database are adjacent when they diﬀer in records per-
taining to one individual, we say that the adversary’s prob-
ability of guessing whether any one person, chosen after in-
teracting with the mechanism, is present in the database
increases multiplicatively by at most eϵ (≈ 1 + ϵ for small ϵ).
This guarantee holds for any auxiliary information, modeled
as a prior on the database or that person.
One important consequence of this interpretation is that
all outputs of f that have non-zero support in f (D) must
also be feasible under f (D
). Otherwise, the adversary could
rule out D
for some outputs in violation of the deﬁnition of
diﬀerential privacy.
′
′
If the function of interest is not diﬀerentially private, such
as when it is deterministic, it can be approximated by a dif-
ferentially private function. Minimizing the loss of accuracy
due to approximation, or the noise level, is the major re-
search goal in diﬀerential privacy.
We emphasize that just adding noise to the output is nei-
ther necessary nor suﬃcient for achieving a meaningful level
of privacy. Noiseless privacy has been explored in the work
of Bhaskar et al., which taps into uncertainty that the adver-
sary may have about the input into the database [1]; Blocki
et al. describe a randomized functionality (the Johnson-
Lindenstrauss transform) that oﬀers a relaxation of diﬀer-
ential privacy without any additional noise [2].
More importantly, some (or even substantial) amount of
additive noise is not by itself a guarantee of diﬀerential pri-
vacy. For instance, several of the “doughnut-shaped” noise
distributions proposed for disclosure limitation of microdata
collected by the US Census Bureau [14] do not achieve dif-
ferential privacy despite introducing a non-trivial amount of
noise.
Consider a diﬀerent example, which captures the essence
of our attack. A counting query returns the number of
records in a database satisfying a certain predicate. If the
noise added to the accurate output of the query is always
even, then the adversary can trivially win the diﬀerentially
private game (guessing whether the query was evaluated on
, such that their counts diﬀer by 1) by
D or an adjacent D
looking at the parity of the output.
In this example, the
adversary’s probability of success is independent of the ab-
solute value of the noise, i.e., accuracy of the answer to the
query.
2.1 Laplacian mechanism
′
The basic method for achieving diﬀerential privacy is called
the Laplacian mechanism. The mechanism is additive, i.e.,
given the function f it approximates its output by comput-
ing the function exactly and then adding noise sampled from
a speciﬁc distribution. The distribution is Laplacian and in
order to preserve privacy of function f , the distribution is
scaled to f ’s sensitivity, deﬁned below.
We say that a deterministic real-valued function
f : D 7→ R has sensitivity at most ∆ if for all
adjacent databases D and D
|f (D) − f (D
′
The (zero-mean) Laplacian distribution Lap(λ)
is continuous probability distribution deﬁned by
its density function
′
:
)| ≤ ∆.
(
)
−|x|
.
λ
hλ(x) , 1
2λ
exp
The classic result from literature on diﬀerential privacy
establishes that the following transformation of f , called
Laplacian mechanism, is ϵ-diﬀerentially-private if f has sen-
sitivity ∆:
˜fϵ(D) , f (D) + Y, where Y ← Lap(∆/ϵ).
Ghosh at el. [15] proved that (a discrete version) of the
Laplacian mechanism is optimal and universal, i.e., it is the
only mechanism the curator has to implement to support
integer-valued functionalities and a large class of loss func-
tions.
′
The distribution of ˜fϵ on two inputs where f (D) = 0 and
f (D
) = 1 is plotted in Figure 1.
3. OVERVIEW OF PRIVACY-PRESERVING
PLATFORMS
Many sophisticated diﬀerentially private algorithms have
been implemented and evaluated on real data, such as privacy-
preserving logistic regressions [3], the matrix mechanism [27]
or the exponential mechanism with the multiplicative weights
update rule (MWEM) [19]. Doing it right, however, is an
error-prone and highly non-trivial task, akin to implement-
ing one’s own cryptography. Furthermore, custom analyses
651˜f (D)
′
˜f (D
)
0.4
0.3
0.2
0.1
0
-5 -4 -3 -2 -1
0
1
2
3
4
5
Figure 1: Distributions of ˜fϵ(D) = f (D) + Lap(∆/ϵ)
over two inputs: f (D) = 0, f (D
) = 1, sensitivity ∆ =
1, privacy parameter ϵ = 0.5.
′
of privacy-preserving algorithms are expensive and labor-
intensive eﬀorts requiring experts in the loop.
It limits
applications of diﬀerential privacy to problems that justify
that expense, excluding scenarios that allow exploration of
the query space by untrusted or non-expert participants.
Examples of these attractive scenarios include datamining,
where analysts are restricted to issuing diﬀerentially pri-
vate queries, and a Netﬂix-style crowdsourcing competition,
where participants train their systems on sensitive data via
a diﬀerentially private on-line interface.
The ﬁrst system whose ambition was to facilitate adop-
tion of diﬀerential privacy in practice by relieving developers
from having to certify privacy of their code was PINQ [30,
31]. PINQ (Privacy INtegrated Queries) is a declarative
programming language that guarantees diﬀerential privacy
as long as the dataset is accessed exclusively through PINQ
queries. PINQ’s research goal was to identify a set of data
transformation and aggregation operators that would be ex-
pressive enough for a range of analytical tasks and yet allow
for prove-once-run-everything approach. A PINQ prototype
is available as a C# LINQ library.
Airavat [39] is a Hadoop-based MapReduce programming
platform whose primary goal is to oﬀer end-to-end security
guarantees, while requiring minimal changes to the program-
ming model or execution environment of big data computa-
tions. In particular, it allows execution of untrusted map-
pers by integrating SELinux-style mandatory access control
to the Java Virtual Machine, the Hadoop framework, and
the distributed ﬁle system. In contrast with PINQ, whose
guarantees are language-based and whose current implemen-
tation treats developers as cooperating entities, Airavat ac-
cepts arbitrary Java bytecode as mappers and then uses
trusted reducers and system-level isolation mechanisms to
ensure compliance with privacy policies.
Fuzz [18] employs a novel type system [38] to facilitate
static analysis of sensitivity of arbitrary data transforma-
tions. It further limits the subset of allowed mappers through
the use of a domain-speciﬁc programming language, which
requires primitives to supply timeout information and en-
forces constant execution time on basic data manipulation
routines.
The most recent of these systems, GUPT [34], explores
a diﬀerent point in the design space of privacy-preserving
computations. It uses the sample-and-aggregate framework
due to Nissim et al. [36], which allows execution of an arbi-
trary client-provided program over a sample of the original
dataset. Accuracy is boosted by running the program mul-
tiple times over many samples and averaging the outputs,
and privacy is enforced by adding Laplacian noise to the
average.
PINQ, Airavat, Fuzz, and GUPT, while diﬀerent in their