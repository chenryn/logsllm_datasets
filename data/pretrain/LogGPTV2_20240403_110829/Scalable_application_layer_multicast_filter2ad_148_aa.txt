title:Scalable application layer multicast
author:Suman Banerjee and
Bobby Bhattacharjee and
Christopher Kommareddy
Scalable Application Layer Multicast
Suman Banerjee, Bobby Bhattacharjee, Christopher Kommareddy
Department of Computer Science,University of Maryland, College Park, MD 20742, USA
fsuman,bobby,PI:EMAIL
ABSTRACT
We describe a new scalable application-layer multicast protocol, specif-
ically designed for low-bandwidth, data streaming applications with
large receiver sets. Our scheme is based upon a hierarchical cluster-
ing of the application-layer multicast peers and can support a num-
ber of different data delivery trees with desirable properties.
We present extensive simulations of both our protocol and the
Narada application-layer multicast protocol over Internet-like topolo-
gies. Our results show that for groups of size 32 or more, our proto-
col has lower link stress (by about 25%), improved or similar end-
to-end latencies and similar failure recovery properties. More im-
portantly, it is able to achieve these results by using orders of mag-
nitude lower control trafﬁc.
Finally, we present results from our wide-area testbed in which
we experimented with 32-100 member groups distributed over 8 dif-
ferent sites. In our experiments, averagegroup members established
and maintained low-latency paths and incurred a maximum packet
loss rate of less than 1% as members randomly joined and left the
multicast group. The average control overhead during our experi-
ments was less than 1 Kbps for groups of size 100.
Categories and Subject Descriptors
C.2.2 [Computer-Communication Networks]: Network Protocols;
C.2.4 [Computer-Communication Networks]: Distributed Sys-
tems; C.4 [Computer Systems Organization]: Performance of Sys-
tems
General Terms
Algorithms, Design, Performance, Experimentation
Keywords
Application layer multicast, Overlay networks, Peer-to-peer systems,
Hierarchy, Scalability
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’02, August 19-23, 2002, Pittsburgh, Pennsylvania, USA.
Copyright 2002 ACM 1-58113-570-X/02/0008 ...$5.00.
1.
INTRODUCTION
Multicasting is an efﬁcient mechanism for packet delivery in one-
many data transfer applications. It eliminates redundant packet repli-
cation in the network. It also decouples the size of the receiver set
from the amount of state kept at any single node and therefore, is
an useful primitive to scale multi-party applications. However, de-
ployment of network-layer multicast [11] has not been widely adopted
by most commercial ISPs, and thus large parts of the Internet are
still incapable of native multicast more than a decade after the pro-
tocols were developed. Application-Layer Multicast protocols [10,
12, 7, 14, 15, 24, 18] do not change the network infrastructure, in-
stead they implement multicast forwarding functionality exclusively
at end-hosts. Such application-layer multicast protocols and are in-
creasingly being used to implement efﬁcient commercial content-
distribution networks.
In this paper, we present a new application-layer multicast proto-
col which has been developed in the context of the NICE project at
the University of Maryland 1. NICE is a recursive acronym which
stands for NICE is the Internet Cooperative Environment. In this
paper, we refer to the NICE application-layer multicast protocol as
simply the NICE protocol. This protocol is designed to support ap-
plications with large receiver sets. Such applications include news
and sports ticker services suchas Infogate (http://www.infogate.com)
and ESPN Bottomline (http://www.espn.com); real-time stock quotes
and updates, e.g.
the Yahoo! Market tracker, and popular Inter-
net Radio sites. All of these applications are characterized by very
large (potentially tens of thousands) receiver sets and relatively low
bandwidth soft real-time data streams that can withstand some loss.
We refer to this class of large receiver set, low bandwidth real-time
data applications as data stream applications. Data stream appli-
cations present an unique challenge for application-layer multicast
protocols: the large receiver sets usually increase the control over-
head while the relatively low-bandwidth data makes amortizing this
control overhead difﬁcult. NICE can be used to implement very
large data stream applications since it has a provably small (con-
stant) control overhead and produces low latency distribution trees.
It is possible to implement high-bandwidth applications using NICE
as well; however, in this paper, we concentrate exclusively on low
bandwidth data streams with large receiver sets.
1.1 Application-Layer Multicast
The basic idea of application-layer multicast is shown in Figure 1.
Unlike native multicast where data packets are replicated at routers
inside the network, in application-layer multicast data packets are
replicated at end hosts. Logically, the end-hosts form an overlay
network, and the goal of application-layer multicast is to construct
and maintain an efﬁcient overlay for data transmission. Since appli-
See http://www.cs.umd.edu/projects/nice
2051
3
A
B
2
4
1
2
A
B
3
4
Network Layer Multicast
Application Layer Multicast
Figure 1: Network-layer and application layer multicast.
Square nodes are routers, and circular nodes are end-hosts. The
dotted lines represent peers on the overlay.
cation-layer multicast protocols must send the identical packetsover
the same link, they are less efﬁcient than native multicast. Two in-
tuitive measures of “goodness”for application layer multicast over-
lays, namely stress and stretch, were deﬁned in [10]). The stress
metric is deﬁned per-link and counts the number of identical pack-
ets sent by a protocol over each underlying link in the network. The
stretch metric is deﬁned per-member and is the ratio of path-length
from the source to the member along the overlay to the length of
the direct unicast path. Consider an application-layer multicast pro-
tocol in which the data source unicasts the data to each receiver.
Clearly, this “multi-unicast” protocol minimizes stretch, but does so
at a cost of O(N ) stress at links near the source (N is the number
of group members). It also requires O(N )control overhead at some
single point. However, this protocol is robust in the sense that any
number of group member failures do not affect the other members
in the group.
In general, application-layer multicast protocols can be evaluated
along three dimensions:
(cid:15) Quality of the data delivery path: The quality of the tree is
measured using metrics such as stress, stretch, and node de-
grees.
(cid:15) Robustnessof the overlay: Since end-hosts are potentially less
stable than routers, it is important for application-layer mul-
ticast protocols to mitigate the effect of receiver failures. The
robustnessof application-layer multicast protocols is measured
by quantifying the extent of the disruption in data delivery
when different members fail, and the time it takes for the pro-
tocol to restore delivery to the other members. We present the
ﬁrst comparison of this aspect of application-layer multicast
protocols.
(cid:15) Control overhead: For efﬁcient use of network resources, the
control overhead at the members should be low. This is an
important cost metric to study the scalability of the scheme
to large member groups.
1.2 NICE Trees
Our goals for NICE were to develop an efﬁcient, scalable, and
distributed tree-building protocol which did not require any under-
lying topology information. Speciﬁcally, the NICE protocol reduces
the worst-case state and control overheadat any member to O(log N ),
maintains a constant degree bound for the group members and ap-
proach the O(log N ) stretch bound possible with a topology-aware
centralized algorithm. Additionally, we also show that an average
member maintains state for a constant number of other members,
and incurs constant control overheadfor topology creation and main-
tenance.
In the NICE application-layer multicast scheme, we create a hier-
archically-connectedcontrol topology. The data delivery path is im-
plicitly deﬁned in the way the hierarchy is structured and no addi-
tional route computations are required.
Along with the analysis of the various bounds, we also present a
simulation-based performance evaluation of NICE. In our simula-
tions, we compare NICE to the Narada application-layer multicast
protocol [10]. Narada was ﬁrst proposed as an efﬁcient application-
layer multicast protocol for small group sizes. Extensions to it have
subsequently been proposed [9] to tailor its applicability to high-
bandwidth media-streaming applications for these groups, and have
been studied using both simulations and implementation. Lastly, we
present results from a wide-area implementation in which we quan-
tify the NICE run-time overheads and convergence properties for
various group sizes.
1.3 Roadmap
The rest of the paper is structured as follows: In Section 2, we
describe our general approach, explain how different delivery trees
are built over NICE and present theoretical bounds about the NICE
protocol. In Section 3, we present the operational details of the pro-
tocol. We present our performance evaluation methodology in Sec-
tion 4, and present detailed analysis of the NICE protocol through
simulations in Section 5 and a wide-area implementation in Sec-
tion 6. We elaborate on related work in Section 7, and conclude in
Section 8.
2. SOLUTION OVERVIEW
The NICE protocol arranges the set of end hosts into a hierarchy;
the basic operation of the protocol is to create and maintain the hi-
erarchy. The hierarchy implicitly deﬁnes the multicast overlay data
paths, as described later in this section. The member hierarchy is
crucial for scalability, since most members are in the bottom of the
hierarchy and only maintain state about a constant number of other
members. The members at the very top of the hierarchy maintain
(soft) state about O(log N ) other members. Logically, each mem-
ber keeps detailed state about other members that are near in the
hierarchy, and only has limited knowledge about other members in
the group. The hierarchical structure is also important for localizing
the effect of member failures.
The NICE hierarchy describedin this paper is similar to the mem-
ber hierarchy used in [3] for scalable multicast group re-keying. How-
ever, the hierarchy in [3], is layered over a multicast-capable net-
work and is constructed using network multicast services (e.g. scoped
expanding ring searches). We build the necessary hierarchy on a
unicast infrastructure to provide a multicast-capable network.
In this paper, we use end-to-end latency as the distance metric
between hosts. While constructing the NICE hierarchy, members
that are “close” with respect to the distance metric are mapped to
the same part of the hierarchy: this allows us to produce trees with
low stretch.
In the rest of this section, we describe how the NICE hierarchy
is deﬁned, what invariants it must maintain, and describe how it is
used to establish scalable control and data paths.
2.1 Hierarchical Arrangement of Members
The NICE hierarchy is created by assigning members to differ-
ent levels (or layers) as illustrated in Figure 2. Layers are numbered
sequentially with the lowest layer of the hierarchy being layer zero
(denoted by L ). Hosts in each layer are partitioned into a set of
clusters. Each cluster is of size between k and k (cid:0) , where k is a
constant, and consists of a set of hosts that are close to each other.
We explain our choice of the cluster size bounds later in this paper
(Section 3.2.1). Further, each cluster has a cluster leader. The pro-
tocol distributedly chooses the (graph-theoretic) center of the clus-
206A7
B1
C0
0
A1
A7
B1
C0
1
A1
A7
B1
C0
2
A1
A7
B1
C0
3
A1
A0
B0
A2
B2
A0
B0
A2
B2
A0
B0
A2
B2
A0
B0
A2
B2
Figure 3: Control and data delivery paths for a two-layer hierarchy. All A i hosts are members of only L   clusters. All Bi hosts are
members of both layers L  and L. The only C host is the leader of the L cluster comprising of itself and all the B hosts.
Topological clusters
Layer 1
Layer 0
B
A
Layer 2
F
C
C
D
F
F
M
J
M
G
L
E
H
Cluster−leaders of
layer 1 form layer 2
Cluster−leaders of
layer 0 form layer 1
K
All hosts are
joined to layer 0
Figure 2: Hierarchical arrangement of hosts in NICE. The lay-
ers are logical entities overlaid on the same underlying physical
network.
ter to be its leader, i.e. the cluster leader has the minimum maxi-
mum distance to all other hosts in the cluster. This choice of the
cluster leader is important in guaranteeing that a new joining mem-
ber is quickly able to ﬁnd its appropriate position in the hierarchy
using a very small number of queries to other members.
Hosts are mapped to layers using the following scheme: All hosts
are part of the lowest layer, L . The clustering protocol at L  parti-
tions these hosts into a set of clusters. The cluster leaders of all the
clusters in layer Li join layer Li+. This is shown with an exam-
ple in Figure 2, using k = . The layer L  clusters are [ABCD],
[EFGH] and [JKLM]2. In this example, we assume that C, F and
M are the centers of their respective clusters of their L  clusters,
and are chosen to be the leaders. They form layer L  and are clus-
tered to create the single cluster, [CFM], in layer L. F is the center
of this cluster, and hence its leader. Therefore F belongs to layer L
as well.
The NICE clusters and layers are created using a distributed algo-
rithm described in the next section. The following properties hold
for the distribution of hosts in the different layers:
(cid:15) A host belongs to only a single cluster at any layer.
(cid:15) If a host is present in some cluster in layer Li, it must occur
in one cluster in each of the layers, L ; : : : ; Li(cid:0). In fact, it
is the cluster-leader in each of these lower layers.
(cid:15) If a host is not present in layer, Li, it cannot be present in any
layer Lj, where j > i.
(cid:15) Each cluster has its size bounded between k and k (cid:0) . The
leader is the graph-theoretic center of the cluster.
(cid:15) There are at most logk N layers, and the highest layer has
only a single member.
We denote a cluster comprising of hosts X; Y; Z; : : : by
[XY Z : : : ].
We also deﬁne the term super-cluster for any host, X. Assume
that host, X, belongs to layers L ; : : : ; Li(cid:0) and no other layer,
and let [..XYZ..] be the cluster it belongs it in its highest layer (i.e.
layer Li(cid:0)) with Y its leader in that cluster. Then, the super-cluster
of X is deﬁned as the cluster, in the next higher layer (i.e. Li), to
which its leader Y belongs. It follows that there is only one super-
cluster deﬁned for every host (except the host that belongs to the
top-most layer, which does not have a super-cluster), and the super-
cluster is in the layer immediately above the highest layer that H
belongs to. For example, in Figure 2, cluster [CFM] in Layer 1 is the
super-cluster for hosts B, A, and D. In NICE each host maintains
state about all the clusters it belongs to (one in each layer to which
it belongs) and about its super-cluster.
2.2 Control and Data Paths
The host hierarchy can be used to deﬁne different overlay struc-
tures for control messages and data delivery paths. The neighbors
on the control topology exchange periodic soft state refreshes and
do not generate high volumes of trafﬁc. Clearly, it is useful to have
a structure with higher connectivity for the control messages, since
this will cause the protocol to converge quicker.
In Figure 3, we illustrate the choices of control and data paths us-
ing clusters of size 4. The edges in the ﬁgure indicate the peerings
between group members on the overlay topology. Each set of four
hosts arranged in a 4-clique in Panel 0 are the clusters in layer L  .
Hosts B ; B; B and C  are the cluster leaders of these four L 
clusters and form the single cluster in layer L. Host C  is the leader
of this cluster in layer L. In the rest of the paper, we use Clj (X)
to denote the cluster in layer Lj to which member X belongs. It is
deﬁned if and only if X belongs to layer Lj.
The control topology for the NICE protocol is illustrated in Fig-
ure 3, Panel 0. Consider a member, X, that belongs only to layers
L ; : : : ; Li. Its peers on the control topology are the other members
of the clusters to which X belongs in each of these layers, i.e. mem-
bers of clusters Cl (X); : : : ; Cli(X). Using the example(Figure 3,
Panel 0), member A  belongs to only layer L , and therefore, its
control path peers are the other members in its L  cluster, i.e. A; A
and B . In contrast, member B  belongs to layers L  and L and
therefore, its control path peers are all the other members of its L 
cluster (i.e. A ; A and A) and L cluster (i.e. B; B and C ).
In this control topology, each member of a cluster, therefore, ex-
changes soft state refreshes with all the remaining members of the
cluster. This allows all cluster members to quickly identify changes
in the cluster membership, and in turn, enables faster restoration of
a set of desirable invariants (described in Section 2.4), which might
be violated by these changes.
The delivery path for multicast data distribution needs to be loop-
free, otherwise, duplicate packet detection and suppression mecha-
207Procedure : MulticastDataForward(h; p)
f h  layers L ; : : : ; Li in clusters Cl (h); : : : ; Cli(h) g
for j in [ ; : : : ; i]
if (p = Clj(h))
ForwardDataToSet(Clj(h) (cid:0) fhg)
end if
end for
Figure 4: Data forwarding operation at a host, h, that itself re-
ceived the data from host p.
nisms need to be implemented. Therefore, in the NICE protocol we
choose the data delivery path to be a tree. More speciﬁcally, given
a data source, the data delivery path is a source-speciﬁc tree, and
is implicitly deﬁned from the control topology. Each member ex-
ecutes an instance of the Procedure MulticastDataForward given
in Figure 4, to decide the set of members to which it needs to for-
ward the data. Panels 1, 2 and 3 of Figure 3 illustrate the consequent
source-speciﬁc trees when the sources are at members A  ; A and