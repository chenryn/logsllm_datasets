As in [7], we consider by default an “additive” represen-
tation of the output (i.e., an output y is split into group
elements y1, . . . , ym that add up to y), rather than settle for
an arbitrary compact output representation. The additive
representation is critical for the applications we consider and
is achieved by our constructions.
Definition 2.1
(FSS: Syntax). An m-party function
secret sharing (FSS) scheme is a pair of algorithms (Gen, Eval)
with the following syntax:
• Gen(1λ, ˆf ) is a PPT key generation algorithm, which
on input 1λ (security parameter) and ˆf ∈ {0, 1}∗ (de-
scription of a function f ) outputs an m-tuple of keys
(k1, . . . , km). We assume that ˆf explicitly contains an
input length 1n, group description G, and size param-
eter S (see above).
• Eval(i, ki, x) is a polynomial-time evaluation algorithm,
which on input i ∈ [m] (party index), ki (key deﬁning
fi : {0, 1}n → G) and x ∈ {0, 1}n (input for fi) out-
puts a group element yi ∈ G (the value of fi(x), the
i-th share of f (x)).
When m is omitted, it is understood to be 2. When m = 2,
we sometimes index the parties by i ∈ {0, 1} rather than
i ∈ {1, 2}.
Definition 2.2
(FSS: Security). Let F = (PF , EF )
be a function family and Leak : {0, 1}∗ → {0, 1}∗ be a func-
tion specifying the allowable leakage. Let m (number of
parties) and t (secrecy threshold) be positive integers. An
m-party t-secure FSS for F with leakage Leak is a pair
(Gen, Eval) as in Deﬁnition 2.1, satisfying the following re-
quirements.
• Correctness: For all ˆf ∈ PF describing f : {0, 1}n →
G, and every x ∈ {0, 1}n, if (k1, . . . , km) ← Gen(1λ, ˆf )
i=1 Eval(i, ki, x) = f (x)(cid:3) = 1.
then Pr(cid:2)(cid:80)m
• Secrecy: For every set of corrupted parties S ⊂ [m] of
size t, there exists a PPT algorithm Sim (simulator),
such that for every sequence ˆf1, ˆf2, . . . of polynomial-
size function descriptions from PF , the outputs of the
following experiments Real and Ideal are computation-
ally indistinguishable:
– Real(1λ): (k1, . . . , km) ← Gen(1λ, ˆfλ);
Output (ki)i∈S.
– Ideal(1λ): Output Sim(1λ, Leak( ˆfλ)).
We will also use the natural concrete security variant of
(T, )-secure FSS. When Leak is omitted, it is understood
to be the function Leak( ˆf ) = (1n, G) where 1n and G are the
input length and group description contained in ˆf . When t
is omitted it is understood to be m − 1.
Definition 2.3
(Distributed Point Function). A
point function fα,β, for α ∈ {0, 1}n and β ∈ G, is deﬁned
to be the function f : {0, 1}n → G such that f (α) = β and
f (x) = 0 for x (cid:54)= α. A Distributed Point Function (DPF) is
an FSS for the family of all point functions, with the default
leakage (i.e., Leak( ˆf ) = (1n, G)).
3. NEW FSS CONSTRUCTIONS FROM ONE-
WAY FUNCTIONS
In this section, we present a collection of new FSS con-
structions whose security relies only on one-way functions.
At the core of our new results is a new procedure for com-
bining FSS schemes together via a “tensoring” operation, to
obtain FSS for a more expressive function class. A direct
iterative execution of this operation with two diﬀerent re-
cursion parameters reproduces both the DPF constructions
of Gilboa and Ishai [18] and the (seemingly quite diﬀerent)
tree-based DPF construction from [7].
Further exploring this operation, we make progress in two
directions:
Improved eﬃciency. We demonstrate new optimizations
for the case of DPFs, yielding concrete eﬃciency improve-
ments over the state-of-the-art constructions (for both DPFs
and FSS and for interval functions) [7], dropping the key size
of an n-bit DPF from 4n(λ + 1) down to just n(λ + 2) bits.
We also provide a new procedure for eﬃciently performing
a full domain DPF evaluation (i.e., evaluating on every ele-
ment of the input domain), a task which occurs frequently
within PIR-style applications.
Extended expressiveness. Then, by exploiting the gener-
alization of the procedure, we construct FSS for the class of
polynomial-sized decision trees. This enables applications
such as multi-dimensional interval queries.
We also demonstrate an orthogonal means of obtaining
increased FSS expressibility, achieving FSS for the product
of two supported function classes, in exchange for requiring
a larger number of parties m.
3.1 DPF Tensor Operation
Given the following three tools: (1) a DPF scheme FSS• =
(Gen•, Eval•) for the class of multi-bit point functions F•, (2)
an FSS scheme (GenF , EvalF ) for an arbitrary class of func-
tions F whose keys are pseudorandom (and support an ad-
ditive group structure), and (3) a pseudorandom generator,
we construct an FSS scheme for the tensor of the function
family F with the class of single-bit point functions: that is,
functions gα,f (x, y) which evaluate to f (y) on inputs (α, y),
and to 0 elsewhere.
1294Note that if F• supports n1-bit inputs and F supports
n2-bit inputs then the resulting function class F• ⊗ F takes
(n1 + n2)-bit inputs. The key size of the resulting FSS
(Gen⊗, Eval⊗) will correspond to size⊗(n1+n2, λ) = size•(n1, λ)+
2sizeF (n2, λ).
Remark 3.1. In the case when F is itself a class of (multi-
n1 ⊗ F•
bit) point functions F•, the result of this tensor F•
n2
will correspond directly to another class of (multi-bit) point
functions F•
n1+n2 with larger domain. Repeating this pro-
cess iteratively by doubling the input bit-length in each step
(n1 = n2) yields a construction isomorphic to that from [18],
with key size O(nlog2 3) bits. Alternatively, repeating this
process with n2 = 1 at each step yields the construction
from [7], with key size 4n(λ + 1) bits.
Intuitively, the transformation works as follows. We use
the DPF to generate keys for a function which outputs a ran-
dom seed concatenated with the bit 1 on the special input
α, and 0 everywhere else. This means (viewing the scheme
with “subtractive” reconstruction, for simplicity) that when
evaluating at x = α the parties reach independent random
output seeds s0, s1, and disagreeing bits t0 = 1− t1, whereas
everywhere else their outputs will agree. The sb’s can then
be used to generate long(er) masks (via a PRG) to hide in-
formation from the other party. In the tensor construction,
the masks are used to hide FSS keys from the second scheme:
the parties are both given both keys to the second FSS, but
with one masked by the PRG-output of s0 and the other
masked by the PRG-output of s1. These are the “correction
words.” The bit tb tells the party which of the correction
words to use. When t0 = t1 and s0 = s1, the parties will
perform identical actions, and their ﬁnal output will be the
same. For the special input α, they will exactly remove the
masks and evaluate using the revealed FSS keys. The pseu-
dorandomness of the F FSS keys means the parties cannot
identify which input is the special one.
Note that new keys have the form of one key from the
DPF and two elements in the key space of the second FSS:
that is, the resulting key size size⊗(n1 + n2, λ) is indeed
size•(n1, λ) + 2sizeF (n2, λ).
We defer a formal treatment of the tensor product opera-
tion to the full version.
3.2 Optimized DPF and PIR-like Applications
For input length n, security parameter λ, and 1-bit out-
puts, the best known DPF constructions [7] achieved key
size 4n(λ + 1) bits (the key size grows accordingly for larger
outputs). We now demonstrate an optimized DPF construc-
tion stemming from the tensor approach, which drops the
key size down to n(λ + 2) bits.
Theorem 3.2
(Optimized DPF). Assuming a pseudo-
random generator G : {0, 1}λ → {0, 1}2(λ+1), then the scheme
(Gen•, Eval•) in Figure 1 is a secure DPF for fα,β : {0, 1}n →
G with key size n(λ + 2) + log2 |G|.
We obtain savings in two diﬀerent ways. First, we modify
the generic tensor transformation (accordingly, the scheme
of [7]) so that instead of needing two correction words for
each level, we can suﬃce with one. The reason this is possi-
ble here is because the “second” FSS scheme in this instance
is a single-bit-input DPF, which is simply a secret shared
string of the truth table. For such FSS we do not need to
enforce full control over the unmasked key values that the
parties will compute in order to guarantee correct evalua-
tion, but rather only over the diﬀerence between the values.
This saves us one factor of 2.
Second, we are able to shrink the size of each correction
word by roughly a factor of 2 (explicitly, from 2(λ + 1) bits
to (λ + 2)). Recall that the goal of the correction word is
to shift a (pseudo-)random string (a1, a2) so that it agrees
with a second pseudo-random string (b1, b2) on one half i ∈
{0, 1}, and remains independent on the other half. Previous
constructions achieved this via shifting by a correction word
(c1, c2), where ci = ai⊕bi, and c1−i was a random oﬀset. We
observe that the introduced randomness in the latter shift is
unnecessary, and instead shift both halves by the same oﬀset.
Since a1−i and b1−i were (pseudo-)random and independent
to begin with, conditioned on ai, bi, this property will be
preserved with the shift ai ⊕ bi. This provides us with our
second saved factor of 2.
The pseudocode of our DPF construction is given in Fig-
ure 1. We provide a formal proof of security within the full
version.
3.2.1 Full Domain Evaluation
Some applications of DPF require running the Eval algo-
rithm on every element of the input domain. As an example,
consider two-server Private Information Retrieval (PIR) in
which two servers S0,S1 hold the same N = 2n group ele-
ments x0, . . . , xN−1 ∈ G, for some abelian group G, and a
user wishes to retrieve xα while hiding α.
PIR can be implemented using DPF as described in the
introduction. A straightforward implementation of this idea
requires each server to run Eval• N times independently,
once for each input xj. We show two improvements to this
na¨ıve approach of evaluating the whole input domain, lever-
aging the structure of our particular construction.
Consider a rooted binary tree whose leaves are the ele-
ments of the input domain and the path from the root to a
leaf reﬂects the binary representation of the element’s index.
In other words, the path moves from the current node to the
left child if the next bit in the representation is 0 and to the
right child if the next bit in the representation is 1. In our
construction Eval•(b, kb, x) traverses the path from the root
to a leaf x and the na¨ıve algorithm for full domain evaluation
traverses each of these paths resulting in time O(nN ). How-
ever, for every node in the tree there is a unique τ (i) value
computed by any execution of Eval• that traverses the node.
Since the τ values and the correction words are suﬃcient to
compute the result of Eval• on a single point computing full
domain evaluation can be carried out by computing the τ
values for each node in the tree which requires O(N ) time.
A second improvement is the early termination optimiza-
tion for small output groups. The correction word CW (n+1)
in Gen• is the output β masked by the expansion of two
seeds. If the representation of β is short then several output
values can be “packed” into CW (n+1). For any node V of
depth ν in the tree there are 2n−ν leaves in its sub-tree, or
2n−ν input elements with a shared preﬁx that ends at V . If
the size of CW (ν) is at least 2n−ν times the output length
then the main loop of both Gen• and Eval• can terminate
at level ν instead of at level n. In this case CW (ν) will be
a sequence of group elements masked by the two expanded
seeds. The sequence will have the output β in the location
speciﬁed by the last n− ν bits of α and the unit elements of
1295Input
Domain
{0, 1}n
{0, 1}16
{0, 1}25
{0, 1}40
{0, 1}80
{0, 1}160
Key length
≈ (n − log λ)λ
in bits
1-bit output
Eval• - #AES Gen• - #AES
operations
operations
2(n − log λ)
n − log λ
Key length
in bits
nλ + 2(n + λ + 1)
1417
2578
4513
9673
19993
9
18
33
73
153
18
36
66
146
306
2320
3481
5416
10576
20896