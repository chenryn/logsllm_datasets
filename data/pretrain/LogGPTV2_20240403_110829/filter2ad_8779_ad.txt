"1MB的Linux")代码样本供您观看。
# 生成baby的名字
让我们再尝试一下。让我们为RNN提供一个大文本文件，其中包含列出的8000个婴儿名字，每行一个（从[这里](http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/
"这里")获得的名字）。我们将其提供给RNN，然后生成新名字。以下是一些示例，仅显示训练数据中未出现的名称（90％不显示）：
Rudi Levette Berice Lussa Hany Mareanne Chrestina Carissy Marylen Hammine
Janye Marlise Jacacrie Hendred Romand Charienna Nenotto Ette Dorane Wallen
Marly Darine Salina Elvyn Ersia Maralena Minoria Ellia Charmin Antley Nerille
Chelon Walmor Evena Jeryly Stachon Charisa Allisa Anatha Cathanie Geetra
Alexie Jerin Cassen Herbett Cossie Velen Daurenge Robester Shermond Terisa
Licia Roselen Ferine Jayn Lusine Charyanne Sales Sanny Resa Wallon Martine
Merus Jelen Candica Wallin Tel Rachene Tarine Ozila Ketia Shanne Arnande
Karella Roselina Alessia Chasty Deland Berther Geamar Jackein Mellisand Sagdy
Nenc Lessie Rasemy Guen Gavi Milea Anneda Margoris Janin Rodelin Zeanna Elyne
Janah Ferzina Susta Pey Castina
你可以在[这里](https://cs.stanford.edu/people/karpathy/namesGenUnique.txt
"这里")看到更多。我最喜欢的一些包括“Baby”，“Killie”，“Char”，“R”，“More”，“Mars”，“Hi”，“Saddie”，“With”和“Ahbort”。当然，你可以想象,在写小说或命名一个新创业公司时提供非常有用的灵感:)
# Understanding what’s going on
我们看到训练结束时的结果可能令人印象深刻，但这有什么用呢？让我们进行两次快速实验，简要介绍一下。
## 训练时样本的演变
首先，看一下模型训练时采样文本是如何演变的，这很有趣。  
例如，训练了Leo Tolstoy的战争与和平的LSTM，然后每100次训练生成样本。在迭代100的位置，模型对随机混乱进行采样：
    tyntd-iafhatawiaoihrdemot  lytdws  e ,tfti, astai f ogoh eoase rrranbyne 'nhthnee e plia tklrgd t o idoe ns,smtt   h ne etie h,hregtrs nigtike,aoaenns lng
但是，至少它开始了解由空格分隔的单词。除了有时它插入两个空格。它也不知道逗号总是跟着一个空格。在300次迭代中，我们看到模型开始了解引号和句号。
    "Tmont thithey" fomesscerliund
    Keushey. Thom here
    sheulke, anmerenith ol sivh I lalterthend Bleipile shuwy fil on aseterlome
    coaniogennc Phe lism thond hon at. MeiDimorotion in ther thize."
这些单词现在也用空格分隔，模型开始在句子结尾处得到关于句号的用法 。500次迭代：
    we counter. He stutn co des. His stanted out one ofler that concossions and was 
    to gearang reay Jotrets and with fre colt otf paitt thin wall. Which das stimn
该模型现在学会拼写最短和最常见的单词，如“we”,
“He”，“His”，“Which”，“and”等等。700次迭代之后，我们可以看到越来越多的类似英语的文本出现：
    Aftair fall unsuch that the hall for Prince Velzonski's that me of
    her hearly, and behs to so arwage fiving were to it beloge, pavu say falling misfort 
    how, and Gogition is so overelical and ofter.
1200次之后，看到使用引号和问号/感叹号，也学到了更长的词汇使用：
    "Kite vouch!" he repeated by her
    door. "But I would be done and quarts, feeling, then, son is people...."
直到大约2000次之后，可以得到正确拼写的单词，引号，名称等等：
    "Why do what that day," replied Natasha, and wishing to himself the fact the
    princess, Princess Mary was easier, fed in had oftened him.
    Pierre aking his soul came to the packs and drove up his father-in-law women.
# RNN中的可视化预测，与“神经元”的激活
另一个有趣的可视化情况是查看字符的预测分布。在下面的可视化中，我们从验证集（沿蓝色/绿色行显示）提供维基百科RNN模型字符数据，并在我们可视化（红色）的每个字符下，模型为下一个序列分配的前5个字符的猜测情况。猜测某字符出现的概率由它们的颜色表示（暗红色=为非常可能，白色=不太可能）.例如:
存在一些有很强关联性的字符，模型对这样的字符预测非常肯定（如：
这样的序列）。输入字符序列（蓝色/绿色），基于在RNN的隐藏层中，随机选择的神经元的激发程度而着色。可以这样认为，绿色=这个神经元非常兴奋，激发程度高；蓝色=这个神经元不太兴奋，激发程度不高（对应熟悉LSTM细节的朋友来说，这样的兴奋状态是隐藏状态向量中[-1,1]之间的值，它只是经过gate和tanh函数后LSTM单元的状态）。直观上来说，这是在RNN的“大脑”中读取输入序列时，可视化某些神经元的激活状态。不同的神经元可能在寻找不同的模式。下面我们来看看我认为有趣或者可解释的4个不同的情况（许多不有趣或不能解释）。  
上图中的神经元似乎对URL很敏感，但在URL外部则表现出不感兴趣。LSTM很可能是用这个神经元来确定是否在URL内部。
当RNN在“[[]]”markdown环境内，会表现得很兴奋，在外部则不是。有趣的是神经元看到“[”时，不是马上就能处于激活状态，它必须等到第二个“[”出现才会激活。这个计算模型是否看到1个“[”还是2个“[”,这个计算任务很可能是由不同的神经元完成的。
这里这个图，我们看到一个神经元在“[[]]”环境中看起来呈线性变化。换句话说，它的激活状态给RNN提供了一个可以“[[]]”范围的时间相对坐标。RNN可以使用该信息，然后根据“[[]]”范围出现的早晚，来制作不同的字符。
这里是一个有局部行为的神经元。它保持一个冷静的状态，且会在“www”序列出现第一个“w”之后就变成抑制的状态。RNN可能正在使用这个神经元计算它在“www”序列中的所在的位置，以便于确定是否应该发出另一个“w”，或者开始启动一个URL。
当然，由于RNN的隐藏状态是一个非常庞大，高纬度且分散表达的部分，因此很多这些结论都或多或少呈现出波浪状。这些可视化是使用自定义HTML / CSS /
Javascript生成的，如果您想创建类似的东西，可以看[这里](http://cs.stanford.edu/people/karpathy/viscode.zip
"这里")。  
我们还可以通过排除预测的情况，只显示文本来缩小这种可视化，并且还是用颜色来显示单元格的激活状态。我们可以看到，除了大部分的神经元所做的事情无法解释之外，其中大概有5%的神经元学会了有趣且能解释的算法。  
再说一次，这个方法的优点就是当你试图预测下一个字符，你不必在任何时候进行硬编码，比如：有助于跟踪你当前所在位置是否处于引用内或外。我们刚刚对原始数据进行了LSTM训练，并确定这是一个便于进行跟踪，有效的数量。换句话说，RNN的神经元在训练中逐步调整自己成为一个引用检查单元，这有助于它更好的执行最终任务。这是深度学习模型（更普遍的端到端训练）的能力，来源于一个最简洁且有吸引力的例子之一。
# 源码
我希望我已经说服你训练 character-level 语言模型是一个非常有趣的练习。你可以使用我在Github上发布的[char-rnn代码](https://github.com/karpathy/char-rnn "char-rnn代码")训练你自己的模型。它需要一个大型文本文件进行训练，然后你可以从中生成样本。此外，GPU训练对此有所帮助，因为用其他CPU训练速度会慢10倍。如果你还不是很清楚
Torch / Lua
代码，请记住这里有一个[100行](https://gist.github.com/karpathy/d4dee566867f8291f086
"100行")的版本。
_题外话_ 。代码是用[Torch 7](http://torch.ch/ "Torch
7")编写的，它最近成为我最喜欢的深度学习框架。我在过去的几个月里才开始与Torch /
LUA合作，这并不容易（我花了很多时间在Github上挖掘原始的Torch代码，并问了他们的很多问题以完成工作），但是一旦掌握了这些东西，它就会提供很大的灵活性和速度。我过去也曾与Caffe和Theano合作过，现在我相信Torch，虽然它并不完美，却能获得更好地获得抽象水平。在我看来，有效框架的理想特征是：
  * 具有许多功能的CPU / GPU transparent Tensor 库（切片，数组/矩阵运算等）
  * 脚本语言（理想情况下为Python）中完全独立的代码库，可在Tensors上运行并实现所有深度学习内容（前向传递/后向传递，计算图等）
  * 应该可以轻松地共享预训练模型（Caffe做得很好，其他人做不到），这很重要。
  * 没有编译步骤（或至少不像Theano目前所做的那样）。深度学习的趋势是朝着更大，更复杂的网络展开，这些网络在复杂的图形中被时间展开。重要的是这些不能长时间编译，不然开发时间将大大受损。其次，通过编译，可以放弃可解释性和有效 记录/调试 的能力。如果有一个选项来编译图表， 且它是为提高效率而开发的那这就没问题。
# 扩展Reading
在该帖子结束之前，我还想在更广泛的背景下定位RNN，并提供当前研究方向的草图。RNN最近在深度学习领域产生了大量的共鸣，引起了大家的兴趣。与Convolutional
Networks类似，它们已存在数十年，但它们的全部潜力最近才开始得到广泛认可，这在很大程度上是由于我们不断增长的计算资源。这里是一些近期发展的简要草图（这不是完整的清单，很多这项工作从研究开始要回溯到20世纪90年代，参见相关工作部分）：  
**在NLP和语音领域** ，RNN可以把[语音转录成文本](http://proceedings.mlr.press/v32/graves14.pdf
"语音转录成文本")，实现[机器翻译](https://arxiv.org/abs/1409.3215
"机器翻译")、[生成手写文本](http://www.cs.toronto.edu/~graves/handwriting.html
"生成手写文本")，当然，它们也被用作强大的语言模型 ([Sutskever et
al.](http://www.cs.utoronto.ca/~ilya/pubs/2011/LANG-RNN.pdf "Sutskever et
al.")) ([Graves](https://arxiv.org/abs/1308.0850 "Graves")) ([Mikolov et
al.](http://www.rnnlm.org/ "Mikolov et al."))
（在字符和单词的水平上）。目前，单词级模型似乎比字符级模型更好，但这肯定是暂时的。  
**计算机视觉** 。RNN也在计算机视觉中迅速普及。例如，我们在[帧级别的视频分类](https://arxiv.org/abs/1411.4389
"帧级别的视频分类")，[图像字幕](https://arxiv.org/abs/1411.4555
"图像字幕")（还包括我自己的工作和许多其他）中看到RNN ，[视频字幕](https://arxiv.org/abs/1505.00487
"视频字幕")和最近的[图像问答](https://arxiv.org/abs/1505.02074
"图像问答")。我个人最喜欢的CV领域中关于RNN的文章： [Recurrent Models of Visual
Attention](https://arxiv.org/abs/1406.6247 "Recurrent Models of Visual
Attention")。  
**归纳推理，Memories and Attention** 。另一个极其令人兴奋的研究方向是解决“vanilla recurrent
networks”的局限性。第一个问题，RNN是不具有归纳性的：RNN可以非常好地记忆序列，但是并不一定总能以正确的方式表现出令人信服的迹象（我将提供一些指示，使其更具体）。第二个问题，他们不必要将他们的表示大小与每步的计算量相结合。比如，如果你将隐藏状态向量的大小加倍，那么由于矩阵乘法，每步的FLOPS数量将翻两番。理想情况下，我们希望维护一个巨大的
representation/memory （比如：包含所有维基百科数据或着许多中间状态变量），同时保持每个时间步长固定的计算能力。
在 [DeepMind’s Neural Turing Machines ](https://arxiv.org/abs/1410.5401
"DeepMind’s Neural Turing Machines
")中提出了向这些方向发展的第一个令人信服的例子。这篇文章描述了一种模型的路径，这些模型可以在大型外部存储器阵列和较小的存储器寄存器（将其视为工作存储器）之间执行读/写操作，并在其中进行计算。至关重要的是，NTM论文还提供了非常有趣的内存寻址机制，这些机制是通过(soft,
and fully-differentiable) attention 模型实现的。soft attention
的概念已经证明是一个强大的建模功能，并且还在 [Neural Machine Translation by Jointly Learning to
Align and Translate ](https://arxiv.org/abs/1409.0473 "Neural Machine
Translation by Jointly Learning to Align and Translate ")for Machine
Translation 和 [Memory Networks ](https://arxiv.org/abs/1503.08895 "Memory
Networks ")for (toy) Question Answering 进行了描述。事实上，我甚至可以这么说：
> “attention ”的概念是近期神经网络中最有趣的创新。
现在，我不想深入了解太多细节，但内存寻址的“ soft
attention”方案很方便，因为它使模型完全可以区分，但遗憾的是，牺牲了效率，因为所有可以照顾的事情都被关注的了。可以认为这是在C中声明一个指针，它不指向特定的地址，而是在整个内存中的所有地址上定义一个完整的分布，并且取消引用指针返回指向内容的加权和（这将是一个昂贵的操作！）。这促使多位成员将“soft
attention”模型交换为“hard
attention”，其中一个成员对特定的一块存储器进行采样（例如，某些存储器单元的读/写操作，而不是在某种程度上从所有单元读取/写入））。这个模型在理论上更具吸引力，可扩展性和高效性，但不幸的是它也是不可微分的。然后，这要求使用来自
Reinforcement Learning literature (比如： REINFORCE)
的技术，其中人们习惯于不可微分相互作用的概念。这是一个持续性的工作，但这些“hard attention”模型已被探索，例如， [Inferring
Algorithmic Patterns with Stack-Augmented Recurrent
Nets](https://arxiv.org/abs/1503.01007 "Inferring Algorithmic Patterns with
Stack-Augmented Recurrent Nets"), [Reinforcement Learning Neural Turing
Machines](http://arxiv.org/abs/1505.00521 "Reinforcement Learning Neural
Turing Machines"), 和 [Show Attend and Tell](https://arxiv.org/abs/1502.03044
"Show Attend and Tell")。  
**People** 。如果你想更多了解RNN，我推荐[Alex Graves](http://www.cs.toronto.edu/~graves/
"Alex Graves")，[Ilya Sutskever](http://www.cs.toronto.edu/~ilya/ "Ilya
Sutskever")和[Tomas Mikolov](http://www.rnnlm.org/ "Tomas
Mikolov")的论文。有关“REINFORCE”的更多信息，请看[David
Silver](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Home.html "David
Silver")的课程，或[Pieter Abbeel](https://people.eecs.berkeley.edu/~pabbeel/
"Pieter Abbeel")的课程。  
**Code**
。这篇帖子为Torch发布的代码。我刚才写的原始numpy代码，它实现了一个有效的，批量的LSTM前向和后向传递。你还可以查看我的基于numpy的[NeuralTalk](https://github.com/karpathy/neuraltalk
"NeuralTalk")，它使用RNN / LSTM来标记图像，或者可能是Jeff
Donahue的[Caffe](http://jeffdonahue.com/lrcn/ "Caffe")实现。
# 结论
我们已经了解了RNN，它们如何工作，为什么它们成为一个大问题， 我们在几个有趣的数据集上训练了一个RNN character-level语言模型，而且我们已经看到了RNN的发展方向。你可以放心地在RNN中进行大量创新，我相信它们将成为智能系统的一个基础和关键组成部分。  
最后，为这篇文章添加一些有趣的东西。我在这篇博客文章的源文件上训练了一个RNN，遗憾的是，在大约46K字符处，我没有提供足够的正确的数据来输入RNN，
但返回的样本（用低 temperature 参数，生成以获得更典型的样本）是：
    I've the RNN with and works, but the computed with program of the 
    RNN with and the computed of the RNN with with and the code
Yes，帖子是关于RNN以及它的工作情况，所以很明显这是正确的:)  
See you next time!