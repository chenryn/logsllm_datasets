(cid:2)(cid:21)(cid:17)(cid:24)(cid:27)(cid:19)(cid:29)(cid:18)(cid:22)
(cid:9)(cid:24)(cid:14)(cid:15)(cid:21)
(cid:4)(cid:15)(cid:29)(cid:15)(cid:13)(cid:29)(cid:24)(cid:27)
(cid:3)(cid:21)(cid:12)(cid:28)(cid:28)(cid:19)(cid:16)(cid:19)(cid:13)(cid:12)(cid:29)(cid:19)(cid:24)(cid:23)
Figure 2: Architecture of PJScan
for JavaScript code embedded in a document and performs lexical
analysis on it. The resulting token sequence is used as an input to
the learning component. The learning component is ﬁrst trained
on examples of malicious documents. It produces a model for the
JavaScript content in a malicious PDF document. Classiﬁcation of
new documents is performed using this model. A detector mea-
sures the deviation of a new document from a learned model and
compares it against a predeﬁned threshold (the threshold can also
be automatically determined at the training stage). Documents that
are close to a learned model are classiﬁed as malicious and other-
wise as benign. The functionality of speciﬁc components depicted
in Fig. 2 is described below.
3.1 Extraction of JavaScript Content
The main challenge of the extraction of JavaScript content lies
in the decoding of object streams and the handling of the encoding
used for JavaScript content. Furthermore, a parser must be robust
against potential incompatibilities with the PDF Standard. For this
reason, contrary to the approach taken in [24], we have decided
against the parsing of PDF ﬁles “by hand” and tailored a popular
open source PDF parser Poppler to the needs of our analysis.
Our JavaScript extractor begins with opening the PDF ﬁle and
initializing Poppler and its internal data structures. Next, the Cata-
log dictionary is retrieved which serves as the starting point in the
search for action dictionaries. All candidate locations listed in Sec-
tion 2.2 are checked, and the found action dictionaries are queried
for their type. If the type is Rendition or JavaScript and a dictio-
nary contains the /JS key, the value of this key (or the referenced
object in case of an indirect reference) is retrieved. The JavaScript
entity is then decompressed and decoded if necessary.
The peculiarity of our approach is that we fully process only
those objects in which JavaScript and Rendition action dictionaries
can potentially occur. This strongly reduces the computational ef-
fort for extraction of JavaScript content and is crucial for batch pro-
cessing of large datasets. Files that do not contain any JavaScript
are not processed beyond the extraction stage.
3.2 Lexical Analysis
Two factors motivate the use of lexical analysis for the detection
of malicious JavaScript code. First, we believe that at the text level,
accurate discrimination between malicious and benign programs is
not possible. Second, malicious JavaScript code is usually – some-
times insanely – obfuscated. We have also observed obfuscation in
benign JavaScript entities extracted from PDF documents. Hence
we have decided to use an intermediate representation – the set of
lexical tokens – to capture the salient properties of code in subse-
quent analysis.
The lexical analysis can be eﬃciently carried out by the state-of-
the-art open source JavaScript interpreter SpiderMonkey11 devel-
oped by the Mozilla Foundation. To use it as a token extractor, we
11http://www.mozilla.org/js/spidermonkey/
(cid:1)(cid:3)(cid:2)
have patched SpiderMonkey to stop short of byte-code generation.
Our extractor queries SpiderMonkey for tokens until an end-of-ﬁle
or an error is encountered. Tokens representing various syntac-
tic elements of the JavaScript language, e.g. identiﬁers, operators,
etc., are represented as symbolic names with integer values ranging
from -1 (TOK_ERR) to 85.
Some semantics of the code is lost during lexical analysis. For
example, all identiﬁers get assigned the same token TOK_NAME (re-
gardless of their names), calls to diﬀerent functions with identical
signatures are translated into the same token sequences, and so on.
As a result, JavaScript entities that are distinct at the source code
level may be non-distinct at the token sequence level.
The following example illustrates the tokenization process. The
malicious JavaScript entity
bvb(’var lBvXSUfYYL7RK = ev’ + ’al;’); // a real example
lBvXSUfYYL7RK(’var uzWPsX8 = this.info’ +
z("%2e%46%61%6b") + ’erss;’);
is transformed into the following sequence of tokens, shown in their
order from top to bottom:
Value
29
27
31
15
31
28
2
29
27
31
15
29
27
31
28
15
31
28
2
0
Symbolic name Description
TOK_NAME
TOK_LP
TOK_STRING
TOK_PLUS
TOK_STRING
TOK_RP
TOK_SEMI
TOK_NAME
TOK_LP
TOK_STRING
TOK_PLUS
TOK_NAME
TOK_LP
TOK_STRING
TOK_RP
TOK_PLUS
TOK_STRING
TOK_RP
TOK_SEMI
TOK_EOF
identiﬁer
left parenthesis
string constant
plus
string constant
right parenthesis
semicolon
identiﬁer
left parenthesis
string constant
plus
identiﬁer
left parenthesis
string constant
right parenthesis
plus
string constant
right parenthesis
semicolon
end of ﬁle
Besides the tokens recognized by SpiderMonkey, we have de-
ﬁned extra tokens that are indicative of malicious JavaScript enti-
ties. The newly-introduced tokens are listed in the following table.
The impact of these tokens on the classiﬁcation performance of
PJScan is evaluated in Section 5.4.
Value
101
102
103
104
105
120
121
122
123
Symbolic name
TOK_STR_10
TOK_STR_100
TOK_STR_1000
TOK_STR_10000
TOK_STR_UNBOUND
TOK_UNESCAPE
TOK_SETTIMEOUT
TOK_FROMCHARCODE
TOK_EVAL
Description
a string literal of length  10,000
a call to unescape()
a call to setTimeOut()12
a call to fromCharCode()13
a call to eval()
12In PDF, the function setTimeOut() of the app object can be used
as a replacement for eval() to execute arbitrary JavaScript code
after the speciﬁed timeout.
13fromCharCode() is a static method of the String object that con-
verts Unicode values to characters. In malicious documents, it is
used to decode encoded strings for execution using eval().
c
R
c
R
(a) Learning stage: the center c
and the radius R of the sphere
are determined.
(b) Classiﬁcation stage: new
data are accepted (green) or re-
jected (red).
Figure 3: OCSVM operation
3.3 Learning and Classiﬁcation
In the last step in our processing chain, the learning component
of PJScan determines whether a PDF ﬁle is benign or malicious.
Prior to deployment, it must be trained on a representative set of
malicious PDF ﬁles. The training results in a model of malicious
JavaScript entities in a PDF document. At the deployment stage,
classiﬁcation of new PDF documents is carried out with the help of
the learned model. After the feature extraction steps described in
Sections 3.1 and 3.2 are completed, the set of tokens from a new
document is tested for proximity to the model.
In our system, we use the One-Class Support Vector Machine
(OCSVM) [23] as the learning method14. Its main advantage is that
it only needs examples of one class to build a model. This is nec-
essary since examples of benign PDF documents with JavaScript
content are quite rare, and it takes a lot of manual eﬀort to verify
that they are benign. On the other hand, examples of malicious
PDF documents abound on malware collection systems, and their
maliciousness can be ascertained with high conﬁdence if they are
detected by antivirus systems.
The learning stage of OCSVM (cf. Fig. 3(a)) amounts to ﬁnding
the center c and the radius R of a high-dimensional hypersphere
such that the total percentage of all data points lying outside of the
hypersphere is at most ν. A hypersphere may be extended to ar-
bitrary surfaces by a non-linear transformation to a special feature
space equipped with the so-called “kernel function”. The kernel
function type and the training rejection rate ν are the only param-
eters to be speciﬁed for training of OCSVM. The learned model
comprises the center of the sphere c and the radius R.
The classiﬁcation stage of OCSVM involves the calculation of
the distance between the data point to be classiﬁed and the center
of the hypersphere. If the distance is greater than R (the data point
lies outside of the hypersphere), then it is considered an anomaly
and is treated as benign. The radius thus serves as a threshold that
is automatically determined at the training stage. The classiﬁcation
stage of OCSVM is illustrated in Fig. 3(b).
OCSVM cannot be directly applied to token sequences emitted
by PJScan’s feature extraction component. The reason for this is
that OCSVM expects the data points to be numeric values lying in
a high-dimensional space equipped with typical mathematical oper-
ations such as addition, multiplication with a constant and an inner
product. Sequential data does not form such a space: it is not im-
mediately clear how to add or multiply two strings. A solution to
this problem involves a well-established technique of embedding
sequences in metric spaces [20]. By counting the occurrences of
substrings in data points and assigning the resulting numeric val-
ues to coordinate axes, the required mathematical properties can be
enforced.
14The popular open source SVM implementation LibSVM [6], ver-
sion 2.86, patched to support one-class SVM, was used in our ex-
periments.
The embedding of sequences provides an elegant way for han-
dling multiple JavaScript entities in the same ﬁle. To obtain an ag-
gregated representation of all JavaScript entities, it suﬃces to add
them using the addition operation provided by the embedding. To
avoid the dependence on sequence length, the values in individual
dimensions are binarized (by setting any positive values to 1) and
normalized so that the Euclidean norm of the resulting vectors is
equal to 1.
4. DATA COLLECTION AND ANALYSIS
The success of any learning-based approach crucially depends
on the quality of data available for training. Likewise, the viabil-
ity of a learned model can only be demonstrated on up-to-date real
data. The evaluation of our method rests on an extensive dataset
collected from the research interface to the popular malicious soft-
ware portal VirusTotal. VirusTotal is a web service that enables
ordinary users to upload suspicious ﬁles for a scan by 42 antivirus
engines. Our dataset comprises 65,942 PDF documents with the
total size of nearly 59GB. This data has revealed some interesting
features, and is worth looking at in some detail.
We downloaded three batches of data on November 3, 2010, Jan-
uary 19, 2011 and February 17, 2011 each containing all PDF ﬁles
available on VirusTotal at a given time. The data is kept only for
30 days, and there is surprisingly little overlap between subsequent
months15. We have observed at most 200 identical ﬁles across dif-
ferent snapshots. We have split our corpora into two parts, the “de-
tected” sub-corpus in which documents were ﬂagged as malicious
by at least one scanner, and the “undetected” sub-corpus containing
supposedly benign data.
It is instructive to look at the statistical properties of our data pre-
sented in Table 1. One can notice interesting eﬀects in the collected
data. The average ﬁle size in the “detected” corpora (0.106MB) is
about 13 times smaller than in the “undetected” ones (1.390MB).
This shows that malicious PDF ﬁles do not contain a lot of mean-
ingful content, which is conﬁrmed by a manual investigation of
some of these ﬁles. The percentage of ﬁles with JavaScript in
the “detected” corpora (59.5%) is about 25 times higher than in
the “undetected” corpora (2.4%). This is a strong indicator that
JavaScript plays a crucial role in PDF-related exploits.
Considering only the ﬁles containing JavaScript one can see that
the average number of JavaScript entities per ﬁle in the “detected”
datasets (7.2) is around 33 times smaller than in the “undetected”
datasets (241.1). This observation seems counter-intuitive but it
turns out that “undetected” data usually contains hundreds of very
simple JavaScript entities like this.zoom=100;this.pagenum=39.
Similarly, distinctness of JavaScript entities at the code level is 3.2
times higher in “detected” corpora than in “undetected” (16.9%
vs. 5.2%). These ﬁndings suggest that non-malicious usage of
JavaScript in PDF documents essentially boils down to boring and
redundant code!
Similar eﬀects take place at the token level (the second row from
the bottom in Table 1). One can observe a further decrease of dis-
tinctness (6,419 vs. 35,990, or 17.8%) due to lexical analysis. The
“detected” sub-corpora are 7.5 times more distinct than the “unde-
tected” sub-corpora. Finally, the last row in Table 1 reveals that
many ﬁles contain identical sets of token sequences, which can be
explained by common code reuse in both types of ﬁles.
To enable the quantitative evaluation of detection accuracy in
the forthcoming experiments, we have manually labeled the “unde-
tected” part of our data. Among 960 benign ﬁles with JavaScript,
15In fact, we were originally unaware of the 30 day lifespan and
started a periodic collection of snapshots only in January.
(cid:1)(cid:2)(cid:2)
03. Nov. 2010
19. Jan. 2011
Dataset size
Files in the dataset
Files containing JavaScript
JavaScript entities
Distinct JavaScript entities
Distinct token sequences
Distinct ﬁles on the token sequence level
detected
873MB
7,592
6,626
26,372
8,597
1,108
538
undet.
13GB
7,768
272
75,199
5,178
429
115
detected
429MB
6,465
1,127
33,418
2,376
815
358
undet.
13GB
9,993
196
42,265
3,774
356
95
17. Feb. 2011
detected
1.5GB
11,634
7,526
50,269
9,238
2,947
1,900
undet.
29GB
22,490
492
113,994
6,827
764
237
Total