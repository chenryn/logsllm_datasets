# Title: Assured Reconfiguration of Fail-Stop Systems

## Authors:
- Elisabeth A. Strunk
- John C. Knight
- M. Anthony Aiello

### Affiliation:
Department of Computer Science, University of Virginia  
151 Engineer’s Way, Charlottesville, VA 22904-4740  
{strunk | knight | aiello}@cs.virginia.edu

---

## Abstract
Advancements in hardware dependability have made it unnecessary to employ extensive hardware replication for masking faults in some scenarios. Building on our previous work on assured reconfiguration for single processes and the fail-stop model of processor behavior, we define a framework that provides assured reconfiguration for concurrent software. This framework can achieve high dependability with lower space, power, and weight requirements compared to systems that replicate hardware to mask all anticipated faults. Our assurance argument is based on a proof structure that extends the proofs for the single-application case and includes the fail-stop model of processor behavior. To evaluate the feasibility of our framework, we implemented a hypothetical avionics system representative of what might be found on an unmanned aerial vehicle.

---

## 1. Introduction
Schlichting and Schneider introduced the concept of fail-stop processors as a building block for safety-critical systems, presenting a programming approach based on fault-tolerant actions (FTAs) that leverage fail-stop semantics [8]. Their approach enables the construction of dependable logical machines from less dependable physical components. Since their original work, the dependability of off-the-shelf hardware components has significantly improved, and weight and power remain limiting factors despite increased capability. In this paper, we extend the reconfiguration semantics to enhance system dependability without adding extra hardware. We achieve this by:

1. Allowing the recovery protocol to specify that the system will reconfigure rather than complete the action.
2. Enabling FTAs to span multiple fail-stop processors and applications, so that faults which would normally require masking can be managed by changing the system's functionality. System service is briefly restricted during reconfiguration.

Reconfiguration is currently used in safety-critical systems for various purposes, such as changing functionality between different mission phases for spacecraft or operating modes for aircraft. While most equipment failures are handled by masking using replicated components, reconfiguration is sometimes used to manage the failure of specialized equipment like sensors. For example, in the Boeing 777, replication masks many effects of computer and data bus failures, but the system can be reconfigured to provide reduced functionality if more failures occur than expected [12].

We argue that reconfiguration in systems built with fail-stop processors can effectively tolerate many faults in dependable systems that would be expensive or impossible to handle through replication alone. If reconfiguration is to play a central role in system dependability, its assurance becomes critical. Current reconfiguration protocols are system-specific and rely on existing architectural facilities. In an earlier paper [10], we presented an approach to assured reconfiguration for a single application process consisting of multiple modules. In this paper, we extend our previous work to include assured reconfiguration for a set of application processes. We present a system architecture and verification framework where the reconfiguration logic is a customizable mechanism that:

1. Accepts component-failure signals.
2. Determines the configuration to which the system should move.
3. Sends configuration signals to individual processes to cause them to respond properly to component failure.

To meet our assurance goal, we have shown that our architecture satisfies a generalized version of the single-application properties. To illustrate our ideas, we have built part of a hypothetical avionics system typical of what might be found on a modern general-aviation aircraft or an unmanned aerial vehicle (UAV). The system includes a flight control application, an electrical power generation monitoring application, and an autopilot. The relevant parts of these applications have been implemented, though the functionality is merely representative.

In Section 2, we review related work. In Section 3, we introduce our system architecture. Section 4 lists our assumptions. We review Schlichting and Schneider’s work in Section 5. In Section 6, we present our formal model of reconfiguration. Our example avionics system is described in Section 7. Finally, Section 8 concludes the paper.

---

## 2. Related Work
Other researchers have proposed the use of reconfiguration to increase system dependability in various contexts. Shelton and Koopman have studied the identification and application of useful alternative functionalities that a system might provide in the event of hardware component failure [9]. However, their work focuses more on reconfiguration requirements than on the reconfigurations themselves. Sha has studied the implementation of reconfiguration in fault tolerance for control systems [7], but his work does not focus on assurance. Similarly, Garlan et al. [2] have proposed the use of software architectural styles for error detection and reconfiguration execution to improve dependability, but they do not present a method for assuring their styles.

In large networked systems, reconfiguration in response to failures is known as information system survivability. Informally, a survivable system provides one or more alternative services (degraded, less dependable, or otherwise different) in a given operating environment [4]. For networked systems, the loss of a single component or even a moderate number of randomly distributed components must be expected. System reconfiguration is employed only in the event of significant damage or if moderate numbers of failures suggest a common cause. The main challenge in these systems is managing system scale.

Our work is part of a framework for using reconfiguration in embedded real-time systems. The reconfiguration requirements for embedded systems are similar to those for networked systems, with three key differences:

1. The system is much smaller and can be tightly controlled.
2. The system may need to respond more quickly to failures, requiring hard real-time reconfiguration.
3. A failure to carry out a reconfiguration can have a greater impact, making the assurance requirements more demanding.

---

## 3. System Architecture Overview
To introduce the elements of our architecture and show how they fit together, we begin with an overview, illustrated in Figure 1. The architecture assumes a distributed computing platform consisting of an unspecified number of processing elements that communicate via an ultra-dependable, real-time data bus. Each processing element consists of a fail-stop processor with associated volatile and stable storage, executing a real-time operating system. An example fail-stop processor might be a self-checking pair; an example data bus might be one based on a time-triggered architecture [5]; and an example operating system might comply with the ARINC 653 specification [1]. Sensors and actuators used in typical control applications are connected to the data bus via interface units that employ the required communications protocol.

The system supported by the architecture consists of a set of applications, each operating as an independent process. Applications communicate via message passing or by sharing state through the processors’ stable storage. Each application implements a set of specifications for internal and reconfiguration interfaces. Each specification for each application is defined by domain experts, and certain specification combinations, denoted configurations and defined in a reconfiguration specification [11], provide acceptable services. Reconfiguration is the transition from one configuration to another, and its assurance is the core of our architecture.

System reconfiguration is effected by the System Control Reconfiguration Analysis and Management (SCRAM) kernel. This kernel receives component failure signals when they occur and determines necessary reconfiguration actions based on a statically-defined set of valid system transitions. Component failures are detected by conventional means such as activity, timing, and signal monitors. A detected component failure is communicated to the SCRAM via an abstract signal, and the kernel effects reconfiguration by sending sequences of messages to each application’s reconfiguration interface.

The reconfiguration message sequence causes operating applications to stop executing their current specification, establish a postcondition from which a new specification can be started, and initiate operation in a predetermined new specification. Each application meets prescribed time bounds for each stage of the reconfiguration activity, ensuring that reconfiguration is always completed in bounded time.

The time and service guarantees provided by our architecture hinge on the correct and timely operation of the SCRAM. A dependable implementation of this function could be created in various ways, such as distributing it over multiple processors and protecting it against the failure of a subset of those processors, or allocating it to a fail-stop processor to mask any hardware faults. We do not address the specifics of the SCRAM implementation in this paper.

---

## 4. System Assumptions

[The content for Section 4 would follow here, detailing the assumptions made in the system design and operation.]