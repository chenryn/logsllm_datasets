Adware/Spyware 29%
39%
43%
21%
Adware/Spyware 16%
27%
19%
94%
Adware/Spyware 99%
96%
98%
6% 74% 26% 0% 67%
9% 86% 14% 0% 56%
3% 84% 16% 0% 24%
4% 69% 31% 0% 32%
16% 0% 2% 98% 0%
19% 0% 1% 99% 0%
8% 0% 0% 100% 0%
17% 0% 2% 98% 0%
-
0%
-
0%
-
0%
0%
-
36%
34%
47%
29%
34%
32%
28%
41%
5%
0%
4%
2%
27%
28%
11%
24%
29%
33%
37%
23%
1%
1%
0%
0%
33%
44%
76%
68%
0%
0%
0%
0%
-
-
-
-
0%
0%
0%
0%
100%
100%
100%
100%
Worm
Downloader
Trojan
Worm
Downloader
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
The result is presented in Table 3. We use the following metrics for the comparison:
– Increased APIs. For each of three approaches, we pick the longest trace during
any single run to compare with the normal run. For each approach, we record the
percentage of malware samples whose increased APIs belonging to 0 − 10%, 10 −
50%, 50−100%, or 100% and above. From the result, we can see that Related Work
I performs the best among all approaches, which is obvious because this approach
blindly explores all possible paths and we select the path with most APIs in the
comparison. Meanwhile, in our test, pre-conﬁgured environment (Related Work II)
can seldom expose malware’s hidden behaviors; on average it only increase 5%
more APIs. Thus, even though pre-conﬁgured environment has no extra overhead
for the analysis, it cannot effectively analyze targeted malware. It further conﬁrms
that it is impractical to predict malware’s targeted environment beforehand. Our
approach clearly performs signiﬁcantly better than Related Work II, and very close
to Related Work I.
– Number of Rolling Backs, which is a key factor to slow down analysis. For explor-
ing both branches, Related Work I has to roll back the execution. In theory, for each
environment-sensitive branch, it requires one roll back operation. From the result,
we can see that most of the samples have to roll back over 500 times to ﬁnish the
analysis. However, our GOLDENEYE can efﬁciently control the number of rolling
back because it only occurs when branch prediction cannot determine the right path
to select. The largest number of rolling back in our test is 126 and median number
is 39. It means that we can save more than 90% overhead when compared with
multi-path exploration.
– Memory Usage. According to the description in [37], average snapshot for rolling
back consumes around 3.5MB memory. Considering their approach needs to re-
cursively maintain the context for branches and sub-branches, the memory/disk
overhead should be over 5MB. However, the highest memory/disk usage of GOLD-
ENEYE is only around 1-2MB, which is much less than half of the memory over-
head in Related Work I. Hence, for memory usage, our system also outperforms the
compared solution.
Finally, we also compare the total time to complete analysis for GOLDENEYE and
Related Work I. For each malware, both GOLDENEYE and Related Work I may generate
38
Z. Xu et al
multiple traces and we sum up all the time as the total time to complete the analysis of
the malware. The result is summarized in Figure 5. As we can see, for GOLDENEYE,
the average analysis time per malware is around 44 minutes, while the average time
for Related Work I is 394 minutes, which is around 9 times slower. Furthermore, the
worst case for GOLDENEYE never exceeds 175 minutes while there are 12% of tested
malware takes longer than 12 hours for Related Work I (note that if we do not set the 12
hour limit, the average for Related Work I will be much longer). This clearly indicates
that GOLDENEYE is much more efﬁcient.
In summary, it is evident that our approach has better performance regarding the
trade-off of effectiveness and efﬁciency. We believe the main reason that other solu-
tions have a higher overhead or lower effectiveness is because they are not designed to
analyze malware’s targeted environment. In other words, our approach is more proac-
tive and dynamic to achieve the goal of targeted malware analysis.
6.4 Experiment on Known Environment-Targeted Malware Dataset
In this experiment, we aim to verify that our system can extract known targeted environ-
ments for malware samples. We began our experiment from collecting the ground truth
of some malware set. We look up multiple online resources, such as [43], for the doc-
umentation about our collected malware samples. In particular, we ﬁrst veriﬁed that all
of them are environment-targeted malware, which means they all need to check some
environments and then expose their real malicious intention. Secondly, we manually
examine their analysis report and summarize their interested environment elements. We
group them into ﬁve categories: System Information, Network Status, Hardware, Cus-
tomized Objects, and Library/Process. For instance, if one sample’s malicious logic de-
pends on some system-wide mutex, we consider it as sensitive to Customized Objects.
We record our manual ﬁndings about our test dataset in Table 4(a).
Table 4. Test on Targeted Malware
System Network Hardware Customized
√
√
Conﬁcker[43]
√
Zeus[21]
√
Sality[12]
√
Bifrost[2]
√
iBank[7]
√
nuclearRAT[9]
√
Duqu[4]
√
Nitro[16]
Qakbot[11]
√
Object
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
Library
√
Process
√
√
√
√
√
(a) Ground Truth
Conﬁcker
Zeus
Sality
Bifrost
iBank
nuclearRAT
Duqu
Nitro
Qakbot
√
Object
◦
√
System Network Hardware Customized
√
√
√
√
√
√
◦
◦
◦
√
√
√
√
√
×
√
√
√
√
×
√
√
◦
◦
×
◦
√
√
Library
√
Process
◦
√
◦
√
√
(b) GOLDENEYE Environment Extraction Result
: Correctly Extracted, ◦: Similar Element
×: Not Extracted
There are several caveats in the test. First, if the documentation does not clearly men-
tion the sample’s MD5 or the sample with the speciﬁc MD5 cannot be found online,
it may bring some inaccurate measurement for the result. One example is the Trojan
iBank [7] case. We analyze some of its variants and they may not exhibit the same be-
haviors as the documented states. Second, we conclude the extraction result in three
GOLDENEYE: Efﬁciently and Effectively Unveiling Malware’s Targeted Environment
39
types: (a) Correctly Extracted means GOLDENEYE can extract the exact same environ-
ment element. (b) Similar Element means GOLDENEYE ﬁnds some element that acts
the similar functionality as mentioned in the document, but such element may have
different name as the document described. We suspect it is probably because the ele-
ment name is dynamically generated based on different information. For this type, we
consider GOLDENEYE successfully extracts the environment information, because the
correct element name could be derived through further manual examination or auto-
matic symbolic execution [22]. (c) Not Extracted means GOLDENEYE fails to extract
the environment element.
From the result, we can see that our GOLDENEYE can correctly detect most of the
targeted environment elements (41 out of 44) within the 5-min analysis time limit. How-
ever, our system fails to extract 3 elements out of 44 cases. After we manually unpack
the code and check the reason of the failures, we ﬁnd there are two main reasons: (1)
Some hardware query functions are not in our labeled API list (e.g., in the case of
iBank). This could be solved if we improve our labeled API list. (2) Some element
check only occurs after the malware successfully interacts with a remote (C&C) server
(e.g., in the case of nuclearRAT). However, these servers may not be alive during our
test thus we fail to observe such checks.
6.5 Case Studies
Next, we study some cases in our analysis. We list several environment targets which
may trigger malware’s activities.
Targeted Location. For Conﬁcker A, GOLDENEYE successful captures the system call