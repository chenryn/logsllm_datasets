81
18
int bytes_per_line;
/ *per scanline of data */
19
unsigned int ww = 0;
/ *width */
20
unsigned int hh = 0;
/ *height */
21
22
while (fgets(line, MAX_SIZE, file)) {
23
if (strlen(line) == MAX_SIZE-1) {
24
RETURN (BitmapFileInvalid);
25
}
26
if (sscanf(line,"#define %s %d",name_and_type,&value) == 2) {
27
if (!(type = strrchr(name_and_type, '_')))
28
type = name_and_type;
29
else
30
type++;
31
32
if (!strcmp("width", type))
33
ww = (unsigned int) value;
34
if (!strcmp("height", type))
35
hh = (unsigned int) value;
36
continue;
37
}
38
39
if (sscanf(line, "static short %s = {", name_and_type) == 1)
40
version10p = 1;
41
else if (sscanf(line,"static unsigned char %s = {",name_and_type)
== 1)
42
version10p = 0;
43
else if (sscanf(line, "static char %s = {", name_and_type) == 1)
44
version10p = 0;
45
else
46
continue;
47
48
if (!(type = strrchr(name_and_type, ‘_’)))
49
type = name_and_type;
50
else
51
type++;
52
53
if (strcmp("bits[]", type))
54
continue;
55
56
if (!ww __ !hh)
57
RETURN (BitmapFileInvalid);
58
59
if ((ww % 16) && ((ww % 16) < 9) && version10p)
60
padding = 1;
61
else
62
padding = 0;
63
82
Quality Assurance and Testing
64
bytes_per_line = (ww+7)/8 + padding;
65
66
size = bytes_per_line * hh;
67
bits = (unsigned char *) Xmalloc ((unsigned int) size);
68
if (!bits)
69
RETURN (BitmapNoMemory);
n
/* ... */
n+1
*data = bits;
n+2
*width = ww;
n+3
*height = hh;
n+4
n+5
return (BitmapSuccess);
n+6
}
The simple integer overflow flaw in this example is on line 64:
bytes_per_line = (ww+7)/8 + padding;
This integer overflow bug does not have an adverse effect on the library routine
itself. However, when returned dimensions of width and height do not agree with
actual data available, this may cause havoc among downstream consumers of data
provided by the library. This indeed took place in most popular web browsers on
the market and was demonstrated with PROTOS file fuzzers in July 2002 to be
exploitable beyond “denial of service.”6 Full control over the victim’s browser was
gained over a remote connection. The enabling factor for full exploitability was
conversion of library routine provided image data from row-first format into
column-first format. When width of the image (ww) is in the range of (MAX_UINT-
6). .MAX_UINT, i.e., 4294967289 . . . 4294967295 on 32-bit platforms, the cal-
culation overflows back into a small integer.
This example is modified from X11 and X10 Bitmap handling routines that were
written as part of the X Windowing System library in 1987 and 1988, over 20 years
ago. Since then, this image format has refused to go away, and code to handle it is
widely deployed in open source software and even on proprietary commercial plat-
forms. Most implementations have directly adopted the original implementation, and
the code has been read, reviewed, and integrated by thousands of skilled program-
mers. Very persistently, this flaw keeps reappearing in modern software.
3.6
Black-Box Testing
Testing will always be the main software verification and validation technique,
although static analysis methods are useful for improving the overall quality of doc-
umentation and source code. When testing a live system or its prototype, real data
is sent to the target and the responses are compared with various test criteria to assess
the test verdict. In black-box testing, access to the source code is not necessary,
3.6
Black-Box Testing
83
6The PROTOS file fuzzers are one of the many tools that were never released by University of
Oulu, but the same functionality was included in the Codenomicon Images suite of fuzzing tools,
released in early 2003.
although it will help in improving the tests. Black-box testing is sometimes referred to
as functional testing, but for the scope of this book this definition can be misleading.
Black-box testing can test more than just the functionality of the software.
3.6.1
Software Interfaces
In black-box testing, the system under test is tested through its interfaces. Black-box
testing is built on the expected (or nonexpected) responses to a set of inputs fed to
the software through selected interfaces. As mentioned in Chapter 1, the interfaces
to a system can consist of, for example,
• User interfaces: GUI, command line;
• Network protocols;
• Data structures such as files;
• System APIs such as system calls and device drivers.
These interfaces can be further broken down into actual protocols or data
structures.
3.6.2
Test Targets
Black-box testing can have different targets. The various names of test targets in-
clude, for example,
• Implementation under test (IUT);
• System under test (SUT);
• Device under test (DUT).
The test target can also be a subset of a system, such as:
• Function or class;
• Software module or component;
• Client or server implementation;
• Protocol stack or parser;
• Hardware such as network interface card (NIC);
• Operating system.
The target of testing can vary depending on the phase in the software develop-
ment life cycle. In the earlier phases, the tests can be first targeted to smaller units
such as parsers and modules, whereas in later phases the target can be a complete
network-enabled server farm augmented with other infrastructure components.
3.6.3
Fuzz Testing as a Profession
We have had discussions with various fuzzing specialists with both QA and VA
background, and this section is based on the analysis of those interviews. We will
look at the various tasks from the perspectives of both security and testing profes-
sions. Let’s start with security.
Typically, fuzzing first belongs to the security team. At a software development
organization, the name of this team can be, for example, Product Security Team (PST
84
Quality Assurance and Testing
for short). Risk assessment is one of the tools in deciding where to fuzz and what to
fuzz, or if to fuzz at all. Security teams are often very small and very rarely have any
budget for tool purchases. They depend on the funding from product development
organizations. Although fuzzing has been known in QA for decades, the push to
introduce it into development has almost always come from the security team, per-
haps inspired by the increasing security alerts in its own products or perhaps by new
knowledge from books like this. Initially, most security organizations depend on con-
sultative fuzzing, but very fast most interviewed security experts claimed that they
have turned almost completely toward in-house fuzzing. The primary reason usually
is that buying fuzzing from consultative sources almost always results in unmain-
tained proprietary fuzzers and enormous bills for services that seem to be repeating
themselves each time. Most security people will happily promote fuzzing tools into the
development organization, but many of them want to maintain control on the chosen
tools and veto right on consultative services bought by the development groups. This
brings us to taking a closer look at the testing organization.
The example testing organization we will explore here is divided into three seg-
ments. One-fourth of the people are focused on tools and techniques, which we will
call T&T. And one-fourth is focused on quality assurance processes, which we
will call QAP. The remaining 50% of testers work for various projects in the prod-
uct lines, with varying size teams depending on the project sizes. These will be
referred to as product line testing (PLT) in this text.
The test specialists from the tools and techniques (T&T) division each have
focus on one or more specific testing domains. For example, one dedicated team
can be responsible for performance testing and another on the automated regression
runs. One of the teams is responsible for fuzz testing and in supporting the projects
with their fuzzing needs. The same people who are responsible for fuzzing can also
take care of the white-box security tools. The test specialist can also be a person in
the security auditing team outside the traditional QA organization.
But before any fuzzing tools are integrated into the quality assurance processes,
the requirement needs to come from product management, and the integration of the
new technique has to happen in cooperation with the QAP people. The first position
in the QA process often is not the most optimal one, and therefore the QAP people
need to closely monitor and improve the tactics in testing. The relationship with secu-
rity auditors is also a very important task to the QAP people, as every single auditing
or certification process will immediately become very expensive unless the flaw cate-
gories discovered in third-party auditing are already sought after in the QA process.
The main responsibility for fuzzing is on each individual project manager from
PLT who is responsible for both involving fuzzing into his or her project, and in
reporting the fuzzing results to the customer. PLT is almost always also responsible
for the budget and will need to authorize all product purchases.
When a new fuzzing tool is being introduced to the organization, the main
responsibility for tool selection should still be on the shoulders of the lead test special-
ist responsible for fuzzing. If the tool is the first in some category of test automation,
a new person is appointed as the specialist. Without this type of assignment, the
purchases of fuzzing tools will go astray very fast, with the decisions being made
not on the actual quality and efficiency of the tools but on some other criteria such
as vendor relations or marketing gimmicks. And this is not beneficial to the testing
3.6
Black-Box Testing
85
organization. Whereas it does not matter much which performance testing suite is
used, fuzzing tools are very different from their efficiency perspective. A bad fuzzer
is simply just money and time thrown away.
Let’s next review some job descriptions in testing:
• QA Leader: Works for PLT in individual QA projects and selects the used
processes and tools based on company policies and guidelines. The QA leader
does the test planning, resourcing, staffing, and budget and is typically also
responsible for the QA continuity, including transition of test plans between
various versions and releases. One goal can include integration of test
automation and review of the best practices between QA teams.
• QA Technical Leader: Works for T&T-related tasks. He or she is responsible
for researching new tools and best practices of test automation, and doing
tool recommendations. That can include test product comparisons either with
third parties or together with customers. The QA technical leader can also be
responsible for building in-house tools and test script that pilot or enable inte-
gration of innovative new ideas and assisting the PLT teams in understanding
the test technologies, including training the testers in the tools. The QA tech-
nical leader can assist QA leader in performing ROI analysis of new tools and
techniques and help with test automation integration either directly or
through guidelines and step-by-step instructions. He or she can either per-
form the risk assessments with the product-related QA teams, or can recom-
mend outsourced contractors that can perform those.
• Test Automation Engineer: Builds the test automation harnesses, which can
involve setting up the test tools, building scripts for nightly and weekly tests,
and keeping the regression tests up-to-date. In some complex environments,
the setting up of the target system can be assigned to the actual developers or
to the IT staff. The test automation engineer will see that automated test exe-
cutions are progressing as planned, and that the failures are handled and that
the test execution does not stop for any reason. All monitors and instruments
are also critical in those tasks.
• Test Engineer/Designer: These are sometimes also called manual testers,
although that is becoming more rare. The job of a test engineer can vary from
building test cases for use in conformance and performance testing to select-
ing the “templates” that are used in fuzzing, if a large number of templates is
required. Sometimes when manual tasks are required, the test engineer/designer
babysits the test execution to see that it progresses as planned—for example,
by pressing a key every hour. Most test automation tools are designed to
eliminate manual testing.
3.7
Purposes of Black-Box Testing
Black-box testing can have the following general purposes:
• Feature or conformance testing;
• Interoperability testing;
86
Quality Assurance and Testing
• Performance testing;
• Robustness testing.
We will next examine each of these in more detail.
3.7.1
Conformance Testing
The first category of black-box testing is feature testing or conformance testing.
The earliest description of the software being produced is typically contained in the
requirements specification. The requirements specification of a specific software
project can also be linked to third-party specifications, such as interface definitions
and other industry standards. In quality assurance processes, the people responsi-
ble for validation evaluate the resulting software product against these specifica-
tions and standards. Such a process aims at validating the conformity of the product
against the specifications. In this book we use the term conformance testing for all
testing that validates features or functional requirements, no matter when or where
that testing actually takes place.
3.7.2
Interoperability Testing
The second testing category is interoperability testing. Interoperability is basically
a subset of conformance testing. Interoperability testing is a practical task in which
the final product or its prototype is tried against other industry products. In real
life, true conformance is very difficult if not impossible to reach. But, the product
must at least be able to communicate with a large number of devices or systems.
This type of testing can take place at various interoperability events, where soft-
ware developers fight it out, in a friendly manner, to see who has the most confor-
mant product. In some cases, if a dominant player decides to do it his or her own
way, a method that complies with standards may not necessarily be the “correct”
method. An industry standard can be defined by an industry forum or by an indus-
try player who controls a major share of the industry. For example, if your web
application does not work on the most widely used web browser, even if it is com-
pletely standards compliant, you will probably end up fixing the application instead
of the browser vendor fixing the client software.
3.7.3
Performance Testing
The third type of testing, performance testing, comes from real-use scenarios of the
software. When the software works according to the feature set and with other ven-
dors’ products, testers need to assess if the software is efficient enough in real-life
use scenarios. There are different categories of tests for this purpose, including
stress testing, performance testing, and load testing. In this book we use the term
performance testing for all of these types of tests, whether they test strain conditions
in the host itself or through a load over communication interfaces, and even if the
test is done by profiling the efficiency of the application itself. All these tests aim at
making the software perform “fast enough.” The metric for final performance can
be given as the number of requests or sessions per given time, the number of paral-
lel users that can be served, or a number of other metrics. There are many types of
3.7
Purposes of Black-Box Testing
87
throughput metrics that are relevant to some applications but not to others. Note