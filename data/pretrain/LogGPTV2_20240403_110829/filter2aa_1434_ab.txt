a similar way, by implementing a user-mode IPsec stack, but this would be a substantial eﬀort, and would
lose some of the advantages of making use of existing building blocks.
Prof. Goldberg has a new student named Chris Alexander picking up where Joel left oﬀ. He’s currently
working on ﬁxing bugs in OpenSSL’s implementation of DTLS along with other core libraries that we’d need
to use if we go this direction.
Impact: High.
Eﬀort: High eﬀort to get all the pieces in place.
Risk: High risk that it would need further work to get right.
3
Performance Improvements on Tor
Plan: We should keep working with them (and help fund Chris) to get this project closer to something
we can deploy. The next step on our side is to deploy a separate testing Tor network that uses datagram
protocols, based on patches from Joel and others, and get more intuition from that. We could optimistically
have this testbed network deployed in late 2009.
1.2
We chose Tor’s congestion control window sizes wrong
Tor maintains a per-circuit maximum of unacknowledged cells (CIRCWINDOW). If this value is exceeded, it is
assumed that the circuit has become congested, and so the originator stops sending. Kiraly proposed [2, 3]
that reducing this window size would substantially decrease latency (although not to the same extent as
moving to an unreliable link protocol), while not aﬀecting throughput.
Speciﬁcally, right now the circuit window size is 512KB and the per-stream window size is 256KB. These
numbers mean that a user downloading a large ﬁle receives it (in the ideal case) in chunks of 256KB, sending
back acknowledgements for each chunk. In practice, though, the network has too many of these chunks
moving around at once, so they spend most of their time waiting in buﬀers at relays.
Reducing the size of these chunks has several eﬀects.
First, we reduce memory usage at the relays,
because there are fewer chunks waiting and because they’re smaller. Second, because there are fewer bytes
vying to get onto the network at each hop, users should see lower latency.
More investigation is needed on precisely what should be the new value for the circuit window, and
whether it should vary. Out of 100KB, 512KB (current value in Tor) and 2560KB, they found the optimum
was 100KB for all levels of packet loss. However this was only evaluated for a ﬁxed network latency and
relay bandwidth, where all users had the same CIRCWINDOW value. Therefore, a diﬀerent optimum may exist
for networks with diﬀerent characteristics, and during the transition of the network to the new value.
Impact: Medium. It seems pretty clear that in the steady-state this patch is a good idea; but it’s still
up in the air whether the transition period will show immediate improvement or if there will be a period
where traﬃc from people who upgrade get clobbered by traﬃc from people who haven’t upgraded yet.
Eﬀort: Low eﬀort to deploy – it’s a several line patch!
Risk: Medium risk that we haven’t thought things through well enough and we’d need to back it out or
change parts of it.
Plan: Once we start on Tor 0.2.2.x (in the next few months), we should put the patch in and see how it
fares. We should go for maximum eﬀect, and choose the lowest possible window setting of 100 cells (50KB).
2
Some users add way too much load
Section 1 described mechanisms to let low-volume streams have a chance at competing with high-volume
streams. Without those mechanisms, normal web browsing users will always get squeezed out by people
pulling down larger content and tolerating high latency. But the next problem is that some users simply add
more load than the network can handle. Just making sure that all the load gets handled fairly isn’t enough
if there’s too much load in the ﬁrst place.
When we originally designed Tor, we aimed for high throughput. We ﬁgured that providing high through-
put would mean we get good latency properties for free. However, now that it’s clear we have several user
proﬁles trying to use the Tor network at once, we need to consider changing some of those design choices.
Some of those changes would aim for better latency and worse throughput.
2.1
Squeeze over-active circuits
The Tor 0.2.0.30 release included this change:
- Change the way that Tor buffers data that it is waiting to write.
Instead of queueing data cells in an enormous ring buffer for each
4
Performance Improvements on Tor
client->relay or relay->relay connection, we now queue cells on a
separate queue for each circuit. This lets us use less slack memory,
and will eventually let us be smarter about prioritizing different
kinds of traffic.
Currently when we’re picking cells to write onto the network, we choose round-robin from each circuit
that wants to write. We could instead remember which circuits have written many cells recently, and give
priority to the ones that haven’t.
Technically speaking, we’re reinventing more of TCP here, and we’d be better served by a general switch
to DTLS+UDP. But there are two reasons to still consider this separate approach.
The ﬁrst is rapid deployment. We could get this change into the Tor 0.2.2.x development release in mid
2009, and as relays upgrade, the change would gradually phase in. This timeframe is way earlier than the
practical timeframe for switching to DTLS+UDP.
The second reason is the ﬂexibility this approach provides. We could give priorities based on recent
activity (“if you’ve sent much more than the average in the past 10 seconds, then you get slowed down”), or
we could base it on the total number of bytes sent on the circuit so far, or some combination. Even once we
switch to DTLS+UDP, we may still want to be able to enforce some per-circuit quality-of-service properties.
This meddling is tricky though: we could encounter feedback eﬀects if we don’t perfectly anticipate the
results of our changes. For example, we might end up squeezing certain classes of circuits too far, causing
those clients to build too many new circuits in response. Or we might simply squeeze all circuits too much,
ruining the network for everybody.
Also, Bittorrent is designed to resist attacks like this – it periodically drops its lowest-performing connec-
tion and replaces it with a new one. So we would want to make sure we’re not going to accidentally increase
the number of circuit creation requests and thus just shift the load problem.
Impact: High, if we get it right.
Eﬀort: Medium eﬀort to deploy – we need to go look at the code to ﬁgure out where to change, how to
eﬃciently keep stats on which circuits are active, etc.
Risk: High risk that we’d get it wrong the ﬁrst few times. Also, it will be hard to measure whether
we’ve gotten it right or wrong.
Plan: Step one is to evaluate the complexity of changing the current code. We should do that for Tor
0.2.2.x in mid 2009. Then we should write some proposals for various meddling we could do, and try to ﬁnd
the right balance between simplicity (easy to code, easy to analyze) and projected eﬀect.
2.2
Throttle certain protocols at exits
If we’re right that Bittorrent traﬃc is a main reason for Tor’s load, we could bundle a protocol analyzer with
the exit relays. When they detect that a given outgoing stream is a protocol associated with bulk transfer,
they could set a low rate limit on that stream. (Tor already supports per-stream rate limiting, though we’ve
never found a need for it.)
This is a slippery slope in many respects though. First is the wiretapping question: is an application
that automatically looks at traﬃc content wiretapping? It depends which lawyer you ask. Second is the
network neutrality question: remember Comcast’s famous “we’re just delaying the traﬃc” quote. Third is
the liability concern: once we add this feature in, what other requests are we going to get for throttling or
blocking certain content? And does the capability to throttle certain content change the liability situation
for the relay operator?
Impact: Medium-high.
Eﬀort: Medium eﬀort to deploy: need to ﬁnd the right protocol recognition tools and sort out how to
bundle them.
Risk: This isn’t really an arms race we want to play. The “encrypted bittorrent” community already
has a leg up since they’ve been ﬁghting this battle with the telco’s already. Plus the other downsides.
5
Performance Improvements on Tor
Plan: Not a good move.
2.3
Throttle certain protocols at the client side
While throttling certain protocols at the exit side introduces wiretapping and liability problems, detecting
them at the client side is more straightforward. We could teach Tor clients to detect protocols as they come
in on the socks port, and automatically treat them diﬀerently – and even pop up an explanation box if we
like.
This approach opens a new can of worms though: clients could disable the “feature” and resume over-
loading the network.
Impact: Medium-high.
Eﬀort: Medium eﬀort to deploy: need to ﬁnd the right protocol recognition tools and sort out how to
bundle them.
Risk: This isn’t really an arms race we want to play either. Users who want to ﬁle-share over Tor will
ﬁnd a way. Encouraging people to fork a new “fast” version of Tor is not a good way to keep all sides happy.
Plan: Not a good move.
2.4
Throttle all streams at the client side
While we shouldn’t try to identify particular protocols as evil, we could set stricter rate limiting on client
streams by default. If we set a low steady-state rate with a high bucket size (e.g. allow spikes up to 250KB
but enforce a long-term rate for all streams of 5KB/s), we would probably provide similar performance to
what clients get now, and it’s possible we could alleviate quite a bit of the congestion and then get even
better and more consistent performance.
Plus, we could make the defaults higher if you sign up as a relay and pass your reachability test.
The ﬁrst problem is: how should we choose the numbers? So far we have avoided picking absolute speed
numbers for this sort of situation, because we won’t be able to predict a number now which will still be the
correct number in the future.
The second problem is the same as in the previous subsection – users could modify their clients to disable
these checks. So we would want to do this step only if we also put in throttling at the exits or intermediate
relays, a la Section 2.1. And if that throttling works, changing clients (and hoping they don’t revert the
changes) may be unnecessary.
Impact: Low at ﬁrst, but medium-high later.
Eﬀort: Low eﬀort to deploy.
Risk: If we pick high numbers, we’ll never see much of an impact. If we pick low numbers, we could
accidentally choke users too much.
Plan: It’s not crazy, but may be redundant. We should consider in Tor 0.2.2.x whether to do it, in
conjunction with throttling at other points in the circuit.
2.5
Default exit policy of 80,443
We hear periodically from relay operators who had problems with DMCA takedown attempts, switched to
an exit policy of “permit only ports 80 and 443”, and no longer hear DMCA complaints.
Does that mean that most ﬁle-sharing attempts go over some other port?
If only a few exit relays
permitted ports other than 80 and 443, we would eﬀectively squeeze the high-volume ﬂows onto those few
exit relays, reducing the total amount of load on the network.
First, there’s a clear downside: we lose out on other protocols. Part of the point of Tor is to be application-
neutral. Also, it’s not clear that it would work long-term, since corporate ﬁrewalls are continuing to push
more and more of the Internet onto port 80.
6
Performance Improvements on Tor
To be clearer, we have more options here than the two extremes.
We could switch the default exit
policy from allow-all-but-these-20-ports to accept-only-these-20-ports. We could even get more complex, for
example by applying per-stream rate limiting at the exit relays to streams destined for certain ports.
Impact: Low? Medium? High?
Eﬀort: Low eﬀort to deploy.
Risk: The Tor network becomes less useful, roughly in proportion to the amount of speedup we get.
Plan: I think we should take some of these steps in the Tor 0.2.2.x timeframe. The big challenge here is
that we don’t have much intuition about how eﬀective the changes should be, so we don’t know how far to
go.
2.6
Better user education
We still run across users who think any anonymity system out there must have been designed with ﬁle-
sharing in mind. If we make it clearer in the FAQ and our webpage that Tor isn’t for high-volume streams,
that might combine well with the other approaches above.
Overall, the challenge of users who want to overload the system will continue. Tor is not the only system
that faces this challenge.
3
The Tor network doesn’t have enough capacity
Section 1 aims to let web browsing connections work better in the face of high-volume streams, and Section 2
aims to reduce the overall load on the network. The third reason why Tor is slow is that we simply don’t
have enough capacity in the network to handle all the users who want to use Tor.
Why do we call this the third problem rather than the number one problem? Just adding more capacity
to the network isn’t going to solve the performance problem.
If we add more capacity without solving
the issues with high-volume streams, then those high-volume streams will expand to use up whatever new
capacity we add.
Economics tells us to expect that improving performance in the Tor network (i.e. increasing supply)
means that more users will arrive to ﬁll the void.
So in either case we shouldn’t be under the illusion
that Tor will magically just become faster once we implement these improvements. We place the ﬁrst two
sections higher in priority because their goals are to limit the ability of the high-volume users to become
even higher-volume users, thus allowing the new capacity to be more useful to the other users. We discuss
the supply-vs-demand question more in Section 7.1.
3.1
Tor server advocacy
Encouraging more volunteers to run Tor servers, and existing volunteers to keep their servers running, will
increase network capacity and hence performance.
Impact: High, assuming we work on the plans from Section 1 and Section 2 also.
Eﬀort: Medium to high, depending on how much we put in.
Risk: Low.
Plan: A clear win. We should do as many advocacy aspects as we can ﬁt in.
3.1.1
Talks and trainings
One of the best ways we’ve found for getting new relays is to go to conferences and talk to people in person.
There are many thousands of people out there with spare fast network connections and a willingness to help
save the world. Our experience is that visiting them in person produces much better results, long-term, than
Slashdot articles.
7
Performance Improvements on Tor
Roger and Jake have been working on this angle, and Jake will be ramping up even more on it in 2009.
Advocacy and education is especially important in the context of new and quickly-changing government
policies. In particular, the data retention question in Germany is causing instability in the overall set of
volunteers willing to run relays. Karsten’s latest metrics1 show that while the number of relays in other
countries held steady or went up during 2008, the numbers in Germany went down over the course of 2008.
On the other hand, the total amount of bandwidth provided by German relays held steady during 2008 – so
while other operators picked up the slack, we still lost overall diversity of relays. These results tell us where