title:Automated synthesis of adversarial workloads for network functions
author:Luis Pedrosa and
Rishabh R. Iyer and
Arseniy Zaostrovnykh and
Jonas Fietz and
Katerina J. Argyraki
Automated Synthesis of Adversarial Workloads
for Network Functions
Luis Pedrosa
EPFL
PI:EMAIL
Rishabh Iyer
EPFL
PI:EMAIL
Arseniy Zaostrovnykh
EPFL
PI:EMAIL
Jonas Fietz
EPFL
PI:EMAIL
Katerina Argyraki
EPFL
PI:EMAIL
ABSTRACT
Software network functions promise to simplify the deploy-
ment of network services and reduce network operation cost.
However, they face the challenge of unpredictable perfor-
mance. Given this performance variability, it is imperative
that during deployment, network operators consider the per-
formance of the NF not only for typical but also adversarial
workloads. We contribute a tool that helps solve this chal-
lenge: it takes as input the LLVM code of a network function
and outputs packet sequences that trigger slow execution
paths. Under the covers, it combines directed symbolic execu-
tion with a sophisticated cache model to look for execution
paths that incur many CPU cycles and involve adversarial
memory-access patterns. We used our tool on 11 network
functions that implement a variety of data structures and dis-
covered workloads that can in some cases triple latency and
cut throughput by 19% relative to typical testing workloads.
KEYWORDS
Network Function Performance; Adversarial Inputs
1 INTRODUCTION
This work is about software network functions (NFs): pieces
of code, typically written in C or C++, that provide packet-
processing functionality, such as forwarding, load balancing
and network address translation. Traditionally, such func-
tionality has been relegated to closed network appliances or
middleboxes, often implemented in hardware. Recently, how-
ever, there has been a push towards software NFs, which have
the potential to offer more flexibility, reduced time-to-market,
and reduced capital and operating expenses [18, 34, 35].
This shift from hardware middleboxes to software NFs
ACM acknowledges that this contribution was authored or co-authored
by an employee, contractor or affiliate of a national government. As such,
the Government retains a nonexclusive, royalty-free right to publish or
reproduce this article, or to allow others to do so, for Government purposes
only.
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5567-4/18/08...$15.00
https://doi.org/10.1145/3230543.3230573
372
comes with the challenge of unpredictable performance.
While hardware middleboxes process packets through ASICs
that typically yield stable performance, software NFs process
packets on general-purpose CPUs, which may yield vari-
able performance. This variability provides an attack surface
for adversaries seeking to degrade NF performance, e.g., by
sending specially crafted packet sequences that significantly
increase the per-packet latency and/or decrease throughput.
Hence, when network operators deploy a new NF, they need
to know its performance in the face of not only typical but
also adversarial workloads; predicting NF performance as-
suming simple workloads, e.g., small packets with a uniform
or Zipfian distribution of destination IP addresses [15], is
useful but insufficient.
However, finding adversarial workloads in NFs—or any
other non-trivial piece of software—can be hard. Different
packet sequences can traverse different execution paths, with
different performance envelopes. In some scenarios, finding
the “bad paths” and the workloads that exercise them is
relatively easy, e.g., when state is stored in a tree, in which
case the adversarial workloads are those that update the
tree in a way that induces skew. There are, however, more
complicated scenarios, e.g., when state is stored in a hash
table, in which case workloads that induce hash collisions
can significantly degrade performance.
Our contribution is CASTAN (Cycle Approximating Sym-
bolic Timing Analysis for Network Functions), a tool that au-
tomatically synthesizes adversarial workloads for NFs. Given
the LLVM [2] code of an NF and a processor-specific cache
model, CASTAN tries to discover execution paths that con-
sume relatively large numbers of CPU cycles and synthe-
sizes workloads that trigger them. We designed CASTAN with
two properties in mind: (a) it should finish in useful time
(minutes to hours); and (b) it should, ideally, discover work-
loads that are close to the worst-case scenario, even though
we cannot formally guarantee that this will always be the
case. The intended users of our tool are NF developers and
network operators: developers can use CASTAN’s workloads
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
L. Pedrosa et al.
to stress-test the performance of their NFs and debug perfor-
mance problems; operators can use them to provision their
networks and be better prepared for worst-case scenarios.
CASTAN combines a symbolic execution engine with a
processor-specific cache model. The main idea is to explore
multiple execution paths of the NF while keeping an esti-
mate of the execution cycles consumed by each path; then
use directed symbolic execution [24] to guide exploration to-
wards paths that are more likely to induce bad performance.
The cache model helps find workloads that induce cache
contention, resulting in frequent main-memory accesses. We
have also developed a technique that helps reason about the
performance of code constructs that are not amenable to
analysis by symbolic execution.
To the best of our knowledge, this is the first work that
leverages techniques from the programming languages and
verification world to reason about NF performance and syn-
thesize adversarial workloads. Prior work on NF code analy-
sis focused on properties unrelated to performance: it identi-
fied semantic bugs [9, 22, 23, 29, 44], or formally proved crash-
freedom and bounded execution [14], or memory safety and
semantic properties [43]. Prior work on code performance
analysis focused on code that uses a more constrained set of
data structures [30, 32, 39].
We evaluated CASTAN with 11 NFs that employ a variety of
data structures and algorithms. In scenarios where one can
intuitively hand-craft adversarial workloads, CASTAN’s work-
loads yielded very similar performance to the hand-crafted
ones. In other, less intuitive scenarios, CASTAN’s workloads
increased latency by as much as a factor of 3 and decreased
throughput by as much as 19% relative to typical testing
workloads.
The rest of the paper is organized as follows: In §2 we
provide background on symbolic execution. We describe
CASTAN’s design in §3, its implementation in §4, and its eval-
uation in §5. Finally, we discuss related work in §6 and con-
clude in §7.
2 BACKGROUND
In this section, we provide basic background on symbolic ex-
ecution (symbex) [21], the technique that underlies CASTAN’s
analysis. The reader familiar with symbolic variables and
expressions, symbex states, and path explosion, can safely
skip to the next section.
Symbex is a program-analysis technique that explores
multiple execution paths of a given program. It uses a special
interpreter, called a symbolic execution engine (SEE). The
SEE can make any input or variable (including a pointer) sym-
bolic, i.e., assign to it a symbol representing many possible
concrete values. As the symbolic inputs propagate through
the program, the SEE keeps track of the resulting symbolic
expressions. For example, suppose a program takes as input
an integer in; the SEE can make in symbolic, assigning to it
a symbol α that represents all possible integer values; if the
program at some point assigns to an integer variable x the
value in + 1, then x also becomes symbolic with value α + 1.
If the program reaches a conditional branch predicated on a
symbolic value, the SEE can either concretize the symbolic
variable/input, i.e., pick one of its possible concrete values,
or explore all possible outcomes of the branch. For each out-
come it explores, the SEE creates a new execution state and
maintains a path constraint, which specifies the conditions
that the symbolic inputs must satisfy such that the program
reaches this execution state. For example, if an execution
state has path constraint in > 0, this means that this execu-
tion state is reached if and only if the program’s input in
is positive. With the help of a solver, the SEE determines
which path constraints are satisfiable and avoids exploring
infeasible paths.
Symbolic execution suffers from path explosion: a very
large, potentially unbounded number of paths to explore,
which typically occurs in the presence of loops and/or code
that maintains significant state. There are two general ways
to side-step path explosion: constrain the input and/or the
expressiveness of the code, or prioritize the exploration of
certain execution states over others through “directed sym-
bolic execution” [24], so as to achieve a specific goal, e.g.,
maximize line coverage [17, 33] or find specific kinds of bugs
[29]. CASTAN falls in the latter category.
3 DESIGN
In this section, we describe CASTAN’s design. We start with an
overview (§3.1) and a description of our cache model (§3.2).
Then we focus on the three main technical challenges we
faced: identifying adversarial memory-access patterns (§3.3),
identifying long execution paths (§3.4), and analyzing NFs
with hash functions (§3.5).
3.1 Overview
We assume a reasonably powerful adversary: She has access
to the NF code or intermediate build files, and she knows
the processor on which the NF runs; this makes sense in
the context of open-source NFs running in multi-tenant en-
vironments like cloud providers. We also assume that the
adversary can generate adversarial workloads that can reach
the targeted NF unmodified and unfiltered; this makes sense
given the direct exposure of many NFs on the network.
CASTAN takes as input the LLVM code of an NF and a
processor-specific cache model; the output is a sequence of
N concrete packets, where N is a configurable parameter.
Under the covers, we execute the given NF code on an SEE,
providing as input a sequence of N symbolic packets. As
symbex proceeds, we keep track of each execution state’s
“cost”—our expectation of the highest number of cycles per
373
Automated Synthesis of Adversarial Workloads
for Network Functions
packet (CPP) that this state can lead to—and prioritize ex-
ploring states with higher cost. When we exhaust our time
budget, we halt the process, pick the state with the highest
cost, provide its path constraint to a solver, and obtain a
sequence of N concrete input packets that lead to this state;
this packet sequence is our adversarial workload.
The main challenge is computing the cost of each state,
which should reflect the likelihood of this state being a part
of an adversarial workload. Consider a state S that results
from the execution of a sequence I of LLVM instructions.
We define S’s cost as a sum of two parts: the “current cost”
and the “potential cost.” Ideally, the current cost would be the
highest CPP that could be consumed to reach S, while the po-
tential cost would be the highest CPP that could be consumed
following S. To compute the former, we would consider all
the memory access patterns that could result from executing
I and pick the one that yields the highest CPP. Similarly,
to compute the latter, we would identify all the instruction
sequences that could follow I and all the memory access
patterns that could result from executing each sequence, and
pick the combination that yields the highest CPP. In practice,
both the current and the potential cost are approximations,
because we typically cannot consider all the feasible memory
access patterns or instruction sequences.
Our approach is akin to an A* search [19], with the dif-
ference that we are trying to maximize, not minimize the
expected cost. A traditional A* search finds the shortest path
from a source to a destination, with the help of a heuristic
that predicts the shortest distance from any candidate inter-
mediate node to the destination. The result is guaranteed to
be correct under the condition that the heuristic is “admissi-
ble,” i.e., it returns a value that is always less than or equal to
the actual shortest distance. To provide similar guarantees,
we would need a heuristic that predicts the highest cost from
any execution state to the point where the next packet is
received and returns a value that is always greater than or
equal to the actual highest cost. Given that we typically can-
not tightly bound the highest cost (because we cannot tightly
upper-bound the number of loop iterations in the code), our
heuristic would often significantly over-estimate (in the ex-
treme, return infinity), devolving into a breadth-first search
and leading to path explosion. Hence, we preclude the for-
mal guarantees of A*, in favor of finding useful adversarial
workloads quickly.
Once we select the state with the highest cost, the next
challenge is to resolve the associated path constraint and find
a sequence of concrete packets that leads to this state. The
default approach is to use an SMT solver, which we also do,
but we need to overcome a hurdle: NF code often involves
hash functions applied to packet headers, e.g., to compute
a checksum, or to index a hash table that keeps per-flow
state. As a result, resolving a path constraint may involve
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Figure 1: Memory hierarchy of Intel Xeon E5-2667v2.
inverting one or more hash functions—something that an
SMT solver typically fails to do in useful time. Sometimes
we can solve the problem by reversing the hash function
with precomputed rainbow tables [27] and reconciling the
constraint on both the packet and the hash value. When
that does not work, e.g., in case of a strong cryptographic
hash function, we sidestep hash inversion altogether and
output a sequence of partially symbolic packets (§3.5). In
other words: if, to trigger bad NF performance, one has to
invert a hash function, and if inverting that hash function
is infeasible today, we still tell the developer what the bad
performance is, and also provide a way to automatically
synthesize a workload that will trigger it, should the hash
function ever become invertible.
3.2 Cache Contention Sets
To construct adversarial memory-access patterns, we need a
model of the memory hierarchy. Unfortunately, building a
detailed model is impossible, because the caching algorithms
of modern processors are at least partially proprietary. In
Xeon processors, in particular, the L1 and L2 cache locations
are determined in the traditional way1, however, the L3 cache
slice is determined by a proprietary hash function. Fig. 1
illustrates what we know about the memory hierarchy of
the processor used in our evaluation (Intel Xeon E5-2667v2),
based on publicly available information and given that we
used 1GB memory pages. In this processor, L1d is 8-way
associative with 32kiB per core, L2 is 8-way associative with
256kiB, and L3 is 20-way associative with 25600kiB.
Hence, the only assumption we can make about our L3
cache is that a process’s address space is divided into con-
tention sets, i.e., sets of memory addresses such that: if an
L3 cache with associativity α is empty, then α addresses
from the same contention set can be brought into the L3
cache without any evictions, while bringing in an (α + 1)st
address from the same contention set will evict one of the α
previously brought addresses.
Since the algorithm that determines the contention sets is
proprietary, we developed a simple mechanism to reverse-
engineer them. The main idea is to form different sets of
memory addresses and measure each set’s probing time, i.e.,
1The bo least significant bits of a memory address are used to compute the
cache-line offset, while the next b1 and b2 least significant bits, respectively,
are used to index the L1 and L2 cache. The values of bo, b1 and b2 can be
inferred from the publicly disclosed cache sizes and associativity.
374
3 bits6 bits6 bits15 bitsbyte offsetL1d lineL2 lineL3 slice34 bits1GB page offset1GB page indexSIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
L. Pedrosa et al.
the time it takes to sequentially read all its addresses re-
peatedly in a loop (100 times in our case). To enforce se-
quential reads, i.e., avoid pipelining effects, we use pointer
chasing [12]. Consider a set S1 that includes at most α ad-
dresses from any contention set, and a set S2 that includes at
least (α + 1) addresses from some contention set. S2’s prob-
ing time will be higher than S1’s probing time by at least a
contention threshold δ, as it includes an extra DRAM access.
Building on the above idea, we identify a contention set in
three steps: (1) We form a set of addresses S that includes α +1
addresses from a contention set C: Starting with an empty S,
we keep adding addresses and measuring S’s probing time,
until we add some address A that increases S’s probing time
by more than the contention threshold δ. At this point, we
know that A was the (α + 1)st address added to S from the
same contention set. We call that contention set C. (2) We
reduce S such that it consists exactly of α + 1 addresses from
contention set C: For each address A ∈ S, we remove A from
S and check whether that decreases S’s probing time by more
than δ; if yes, then we know that A belongs to C, and we add
it back to S. Once we are done, all addresses in S belong to
C. (3) We identify all remaining addresses that belong to the
contention set C: For each memory address A (cid:60) S, we replace
an address in S with A and check whether that decreases S’s
probing time by more than δ; if not, then we know that A
belongs to C. By repeating these steps multiple times, we
identify all the contention sets of a given process.
In principle, different processes have different contention
sets. This is because the L3 cache is physically indexed, i.e.,
the algorithm that determines the placement of a cache line
in the L3 cache is applied to physical, not virtual addresses.
In our Xeon processor, in particular, the L3 cache line is
determined to some extent by bits 30–63 of the physical
address (Fig. 1). We use 1GB memory pages, which means
that bits 0–29 of each address are used as page byte offset,
hence are the same between virtual and physical addresses.
As such, some of the bits used to identify the L3 cache line
are different between physical and virtual address, resulting
in different contention sets per process.
To solve this problem, we repeat the contention-set discov-
ery process with 8 different 1GB memory pages and across
machine reboots. We post-process the results and retain only
consistent contention sets, i.e., sets of addresses that have
the same bits 0–29 and are always in the same contention
set across different runs. This reduces the size of the dis-
covered contention sets but produces results that generally
hold across process runs and machine reboots. As different