Launch Coordination Engineering 370
Setting Up a Launch Process 372
Developing a Launch Checklist 375
Selected Techniques for Reliable Launches 380
Development of LCE 384
Conclusion 387
Part IV. Management
28. Accelerating SREs to On-Call and Beyond. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391
You’ve Hired Your Next SRE(s), Now What? 391
Initial Learning Experiences: The Case for Structure Over Chaos 394
Creating Stellar Reverse Engineers and Improvisational Thinkers 397
Five Practices for Aspiring On-Callers 400
On-Call and Beyond: Rites of Passage, and Practicing Continuing Education 406
Closing Thoughts 406
x | Table of Contents
29. Dealing with Interrupts. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407
Managing Operational Load 408
Factors in Determining How Interrupts Are Handled 408
Imperfect Machines 409
30. Embedding an SRE to Recover from Operational Overload. . . . . . . . . . . . . . . . . . . . . . 417
Phase 1: Learn the Service and Get Context 418
Phase 2: Sharing Context 420
Phase 3: Driving Change 421
Conclusion 423
31. Communication and Collaboration in SRE. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425
Communications: Production Meetings 426
Collaboration within SRE 430
Case Study of Collaboration in SRE: Viceroy 432
Collaboration Outside SRE 437
Case Study: Migrating DFP to F1 437
Conclusion 440
32. The Evolving SRE Engagement Model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 441
SRE Engagement: What, How, and Why 441
The PRR Model 442
The SRE Engagement Model 443
Production Readiness Reviews: Simple PRR Model 444
Evolving the Simple PRR Model: Early Engagement 448
Evolving Services Development: Frameworks and SRE Platform 451
Conclusion 456
Part V. Conclusions
33. Lessons Learned from Other Industries. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459
Meet Our Industry Veterans 460
Preparedness and Disaster Testing 462
Postmortem Culture 465
Automating Away Repetitive Work and Operational Overhead 467
Structured and Rational Decision Making 469
Conclusions 470
34. Conclusion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 473
Table of Contents | xi
A. Availability Table. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 477
B. A Collection of Best Practices for Production Services. . . . . . . . . . . . . . . . . . . . . . . . . . . . 479
C. Example Incident State Document. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485
D. Example Postmortem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 487
E. Launch Coordination Checklist. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 493
F. Example Production Meeting Minutes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 497
Bibliography. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 501
Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 511
xii | Table of Contents
Foreword
Google’s story is a story of scaling up. It is one of the great success stories of the com‐
puting industry, marking a shift towards IT-centric business. Google was one of the
first companies to define what business-IT alignment meant in practice, and went on
to inform the concept of DevOps for a wider IT community. This book has been writ‐
ten by a broad cross-section of the very people who made that transition a reality.
Google grew at a time when the traditional role of the system administrator was being
transformed. It questioned system administration, as if to say: we can’t afford to hold
tradition as an authority, we have to think anew, and we don’t have time to wait for
everyone else to catch up. In the introduction to Principles of Network and System
Administration [Bur99], I claimed that system administration was a form of human-
computer engineering. This was strongly rejected by some reviewers, who said “we
are not yet at the stage where we can call it engineering.” At the time, I felt that the
field had become lost, trapped in its own wizard culture, and could not see a way for‐
ward. Then, Google drew a line in the silicon, forcing that fate into being. The revised
role was called SRE, or Site Reliability Engineer. Some of my friends were among the
first of this new generation of engineer; they formalized it using software and auto‐
mation. Initially, they were fiercely secretive, and what happened inside and outside
of Google was very different: Google’s experience was unique. Over time, information
and methods have flowed in both directions. This book shows a willingness to let SRE
thinking come out of the shadows.
Here, we see not only how Google built its legendary infrastructure, but also how it
studied, learned, and changed its mind about the tools and the technologies along the
way. We, too, can face up to daunting challenges with an open spirit. The tribal nature
of IT culture often entrenches practitioners in dogmatic positions that hold the
industry back. If Google overcame this inertia, so can we.
This book is a collection of essays by one company, with a single common vision. The
fact that the contributions are aligned around a single company’s goal is what makes
it special. There are common themes, and common characters (software systems)
xiii
that reappear in several chapters. We see choices from different perspectives, and
know that they correlate to resolve competing interests. The articles are not rigorous,
academic pieces; they are personal accounts, written with pride, in a variety of per‐
sonal styles, and from the perspective of individual skill sets. They are written bravely,
and with an intellectual honesty that is refreshing and uncommon in industry litera‐
ture. Some claim “never do this, always do that,” others are more philosophical and
tentative, reflecting the variety of personalities within an IT culture, and how that too
plays a role in the story. We, in turn, read them with the humility of observers who
were not part of the journey, and do not have all the information about the myriad
conflicting challenges. Our many questions are the real legacy of the volume: Why
didn’t they do X? What if they’d done Y? How will we look back on this in years to
come? It is by comparing our own ideas to the reasoning here that we can measure
our own thoughts and experiences.
The most impressive thing of all about this book is its very existence. Today, we hear a
brazen culture of “just show me the code.” A culture of “ask no questions” has grown
up around open source, where community rather than expertise is championed. Goo‐
gle is a company that dared to think about the problems from first principles, and to
employ top talent with a high proportion of PhDs. Tools were only components in
processes, working alongside chains of software, people, and data. Nothing here tells
us how to solve problems universally, but that is the point. Stories like these are far
more valuable than the code or designs they resulted in. Implementations are ephem‐
eral, but the documented reasoning is priceless. Rarely do we have access to this kind
of insight.
This, then, is the story of how one company did it. The fact that it is many overlap‐
ping stories shows us that scaling is far more than just a photographic enlargement of
a textbook computer architecture. It is about scaling a business process, rather than
just the machinery. This lesson alone is worth its weight in electronic paper.
We do not engage much in self-critical review in the IT world; as such, there is much
reinvention and repetition. For many years, there was only the USENIX LISA confer‐
ence community discussing IT infrastructure, plus a few conferences about operating
systems. It is very different today, yet this book still feels like a rare offering: a detailed
documentation of Google’s step through a watershed epoch. The tale is not for copy‐
ing—though perhaps for emulating—but it can inspire the next step for all of us.
There is a unique intellectual honesty in these pages, expressing both leadership and
humility. These are stories of hopes, fears, successes, and failures. I salute the courage
of authors and editors in allowing such candor, so that we, who are not party to the
hands-on experiences, can also benefit from the lessons learned inside the cocoon.
— Mark Burgess
author of In Search of Certainty
Oslo, March 2016
xiv | Foreword
Preface
Software engineering has this in common with having children: the labor before the
birth is painful and difficult, but the labor after the birth is where you actually spend
most of your effort. Yet software engineering as a discipline spends much more time
talking about the first period as opposed to the second, despite estimates that 40–90%
of the total costs of a system are incurred after birth.1 The popular industry model
that conceives of deployed, operational software as being “stabilized” in production,
and therefore needing much less attention from software engineers, is wrong.
Through this lens, then, we see that if software engineering tends to focus on design‐
ing and building software systems, there must be another discipline that focuses on
the whole lifecycle of software objects, from inception, through deployment and oper‐
ation, refinement, and eventual peaceful decommissioning. This discipline uses—and
needs to use—a wide range of skills, but has separate concerns from other kinds of
engineers. Today, our answer is the discipline Google calls Site Reliability Engineer‐
ing.
So what exactly is Site Reliability Engineering (SRE)? We admit that it’s not a particu‐
larly clear name for what we do—pretty much every site reliability engineer at Google
gets asked what exactly that is, and what they actually do, on a regular basis.
Unpacking the term a little, first and foremost, SREs are engineers. We apply the prin‐
ciples of computer science and engineering to the design and development of com‐
puting systems: generally, large distributed ones. Sometimes, our task is writing the
software for those systems alongside our product development counterparts; some‐
times, our task is building all the additional pieces those systems need, like backups
or load balancing, ideally so they can be reused across systems; and sometimes, our
task is figuring out how to apply existing solutions to new problems.
1 The very fact that there is such large variance in these estimates tells you something about software engineer‐
ing as a discipline, but see, e.g., [Gla02] for more details.
xv
Next, we focus on system reliability. Ben Treynor Sloss, Google’s VP for 24/7 Opera‐
tions, originator of the term SRE, claims that reliability is the most fundamental fea‐
ture of any product: a system isn’t very useful if nobody can use it! Because reliability2
is so critical, SREs are focused on finding ways to improve the design and operation
of systems to make them more scalable, more reliable, and more efficient. However,
we expend effort in this direction only up to a point: when systems are “reliable
enough,” we instead invest our efforts in adding features or building new products.3
Finally, SREs are focused on operating services built atop our distributed computing
systems, whether those services are planet-scale storage, email for hundreds of mil‐
lions of users, or where Google began, web search. The “site” in our name originally
referred to SRE’s role in keeping the google.com website running, though we now run
many more services, many of which aren’t themselves websites—from internal infra‐
structure such as Bigtable to products for external developers such as the Google
Cloud Platform.
Although we have represented SRE as a broad discipline, it is no surprise that it arose
in the fast-moving world of web services, and perhaps in origin owes something to
the peculiarities of our infrastructure. It is equally no surprise that of all the post-
deployment characteristics of software that we could choose to devote special atten‐
tion to, reliability is the one we regard as primary.4 The domain of web services, both
because the process of improving and changing server-side software is comparatively
contained, and because managing change itself is so tightly coupled with failures of all
kinds, is a natural platform from which our approach might emerge.
Despite arising at Google, and in the web community more generally, we think that
this discipline has lessons applicable to other communities and other organizations.
This book is an attempt to explain how we do things: both so that other organizations
might make use of what we’ve learned, and so that we can better define the role and
what the term means. To that end, we have organized the book so that general princi‐
ples and more specific practices are separated where possible, and where it’s appropri‐
ate to discuss a particular topic with Google-specific information, we trust that the
reader will indulge us in this and will not be afraid to draw useful conclusions about
their own environment.
2 For our purposes, reliability is “The probability that [a system] will perform a required function without fail‐
ure under stated conditions for a stated period of time,” following the definition in [Oco12].
3 The software systems we’re concerned with are largely websites and similar services; we do not discuss the
reliability concerns that face software intended for nuclear power plants, aircraft, medical equipment, or other
safety-critical systems. We do, however, compare our approaches with those used in other industries in Chap‐
ter 33.
4 In this, we are distinct from the industry term DevOps, because although we definitely regard infrastructure
as code, we have reliability as our main focus. Additionally, we are strongly oriented toward removing the
necessity for operations—see Chapter 7 for more details.
xvi | Preface
We have also provided some orienting material—a description of Google’s production
environment and a mapping between some of our internal software and publicly
available software—which should help to contextualize what we are saying and make
it more directly usable.
Ultimately, of course, more reliability-oriented software and systems engineering is
inherently good. However, we acknowledge that smaller organizations may be won‐
dering how they can best use the experience represented here: much like security, the
earlier you care about reliability, the better. This implies that even though a small
organization has many pressing concerns and the software choices you make may dif‐
fer from those Google made, it’s still worth putting lightweight reliability support in
place early on, because it’s less costly to expand a structure later on than it is to intro‐
duce one that is not present. Part IV contains a number of best practices for training,
communication, and meetings that we’ve found to work well for us, many of which
should be immediately usable by your organization.
But for sizes between a startup and a multinational, there probably already is some‐
one in your organization who is doing SRE work, without it necessarily being called
that name, or recognized as such. Another way to get started on the path to improv‐
ing reliability for your organization is to formally recognize that work, or to find
these people and foster what they do—reward it. They are people who stand on the
cusp between one way of looking at the world and another one: like Newton, who is
sometimes called not the world’s first physicist, but the world’s last alchemist.
And taking the historical view, who, then, looking back, might be the first SRE?
We like to think that Margaret Hamilton, working on the Apollo program on loan
from MIT, had all of the significant traits of the first SRE.5 In her own words, “part of
the culture was to learn from everyone and everything, including from that which
one would least expect.”
A case in point was when her young daughter Lauren came to work with her one day,
while some of the team were running mission scenarios on the hybrid simulation
computer. As young children do, Lauren went exploring, and she caused a “mission”
to crash by selecting the DSKY keys in an unexpected way, alerting the team as to
what would happen if the prelaunch program, P01, were inadvertently selected by a
real astronaut during a real mission, during real midcourse. (Launching P01 inadver‐
tently on a real mission would be a major problem, because it wipes out navigation
data, and the computer was not equipped to pilot the craft with no navigation data.)
5 In addition to this great story, she also has a substantial claim to popularizing the term “software engineering.”
Preface | xvii
With an SRE’s instincts, Margaret submitted a program change request to add special
error checking code in the onboard flight software in case an astronaut should, by
accident, happen to select P01 during flight. But this move was considered unneces‐
sary by the “higher-ups” at NASA: of course, that could never happen! So instead of
adding error checking code, Margaret updated the mission specifications documenta‐
tion to say the equivalent of “Do not select P01 during flight.” (Apparently the update
was amusing to many on the project, who had been told many times that astronauts
would not make any mistakes—after all, they were trained to be perfect.)
Well, Margaret’s suggested safeguard was only considered unnecessary until the very
next mission, on Apollo 8, just days after the specifications update. During midcourse
on the fourth day of flight with the astronauts Jim Lovell, William Anders, and Frank
Borman on board, Jim Lovell selected P01 by mistake—as it happens, on Christmas
Day—creating much havoc for all involved. This was a critical problem, because in
the absence of a workaround, no navigation data meant the astronauts were never
coming home. Thankfully, the documentation update had explicitly called this possi‐
bility out, and was invaluable in figuring out how to upload usable data and recover
the mission, with not much time to spare.
As Margaret says, “a thorough understanding of how to operate the systems was not
enough to prevent human errors,” and the change request to add error detection and
recovery software to the prelaunch program P01 was approved shortly afterwards.
Although the Apollo 8 incident occurred decades ago, there is much in the preceding
paragraphs directly relevant to engineers’ lives today, and much that will continue to
be directly relevant in the future. Accordingly, for the systems you look after, for the
groups you work in, or for the organizations you’re building, please bear the SRE Way
in mind: thoroughness and dedication, belief in the value of preparation and docu‐
mentation, and an awareness of what could go wrong, coupled with a strong desire to
prevent it. Welcome to our emerging profession!
xviii | Preface
How to Read This Book
This book is a series of essays written by members and alumni of Google’s Site Relia‐
bility Engineering organization. It’s much more like conference proceedings than it is
like a standard book by an author or a small number of authors. Each chapter is
intended to be read as a part of a coherent whole, but a good deal can be gained by
reading on whatever subject particularly interests you. (If there are other articles that
support or inform the text, we reference them so you can follow up accordingly.)
You don’t need to read in any particular order, though we’d suggest at least starting
with Chapters 2 and 3, which describe Google’s production environment and outline
how SRE approaches risk, respectively. (Risk is, in many ways, the key quality of our