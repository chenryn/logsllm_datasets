sequence σ = v0(a0, t0)v1(a1, t1) . . . of states and path la-
bels. We use the same notation as introduced for CTMDPs.
The subscript M indicates that we refer to paths in IMC M.
Scheduler. The inherent nondeterminism in a closed IMC
is, as for CTMDPs, resolved by a scheduler. In its most general
form schedulers are functions from decision paths to probabil-
ity distributions over interactive transitions. Note, that so far,
schedulers over IMCs have not been considered in the litera-
ture. We restrict to the class randomized time-abstract history
dependent schedulers, which matches the class of schedulers
we use for CTMDPs.
Deﬁnition 5 (IMC scheduler) Let M be an IMC with inter-
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007active transition relation →. A scheduler over M is a function
D : (S × Act)∗ × SIH −→ Distr(→) such that each support
of D(σ) is a subset of ({s} × Act × S)∩ −→.
Note, that there is a difference in the domain on which a
scheduler in an IMC and a CTMDP bases its decision for the
next step. Since ﬁnite paths in IMCs can end in Markov states,
in which a scheduler decision is simply not possible, these
paths are not included in the domain.
σ-algebra and probability measure for IMC. The σ-
algebra over inﬁnite paths follows a standard cylinder set con-
struction as, e. g., given in [31] for CTMDPs. The scheduler
dependent probability measure PrM
D over inﬁnite paths can
easily be deﬁned but is not in the scope of this paper. For de-
tails we refer to [16].
Phase-type distributions. A phase-type distribution is usu-
ally deﬁned as the distribution of the time until absorption in
a ﬁnite and absorbing2 CTMC [23]. In principle, any prob-
ability distribution on [0,∞) can be approximated arbitrarily
closely by a phase-type distribution given enough phases, i. e.,
states. In order to derive uniformity by construction it is im-
portant that the absorbing CTMC of a phase-type distribution
can be uniformized, just like any CTMC. The result will no
longer have an absorbing state, but a state which is reentered
from itself according to a Poisson distribution. It is, after all, a
special IMC.
3 Uniformity in composition and minimization
of uIMCs
In [4] we constructed uniform IMCs by composition out
of smaller uIMCs. The strategy is based on the nowadays
classical compositional minimization principle (see, e.g., [12])
which has its roots in process algebra, and has been imple-
mented in an exemplary way in the CADP toolkit [5] together
with convenient scripting support [29]. We make use of this
toolkit in our studies.
In this section we ﬁrst present the formal properties of our
speciﬁc construction, focussing on hiding, parallel composi-
tion and minimization of uIMC. In particular, we prove for
each of these concepts that it preserves uniformity. Thus we
can ensure that constructing larger IMCs out of smaller uni-
form IMCs yields again uniform IMCs.
Hiding. Hiding, also called abstraction, is used to internalize
particular actions of a given IMC. Hiding of action a in IMC
M turns this particular action a into the unobservable action
τ . The structural operational semantic rules for hiding in IMCs
are as follows [14]:
b−→s(cid:1)
s
a(cid:4)=b
b−→hide a in (s(cid:1))
hide a in (s)
s
hide a in (s)
a−→s(cid:1)
τ−→hide a in (s(cid:1))
s
hide a in (s)
λ(cid:1)(cid:1)(cid:2)s(cid:1)
λ(cid:1)(cid:1)(cid:2)hide a in (s(cid:1))
2An absorbing CTMC has a single absorbing state.
IMC hide a in (M) is obtained by applying these rules to the
initial state of M. The ﬁrst two rules are as expected, the third
rule gives the deﬁnition of hide for Markov transitions, which
remain untouched by the operator semantics.
Lemma 1 hide a in (M) is uniform whenever M is uniform.
The proof is based on the observation that the uniformity
deﬁnition manifests itself in conditions for stable states, and
that hiding does not introduce more stable states, but intro-
duces instable ones. The converse is not necessarily true:
A non-uniform IMC N might give rise to a uniform IMC
hide a in (N ).
Parallel composition. The structural operational rules of
parallel composition |[A]| [14], with synchronization set A ⊆
Act\τ , are deﬁned on the state-level of IMCs.
a−→s(cid:1)
s
s|[{a1,...,an}]|v
a−→v(cid:1)
v
a /∈{a1,...,an}
a−→s(cid:1)|[{a1,...,an}]|v
a−→v(cid:1)
s|[{a1,...,an}]|v
a−→s(cid:1)
v
s
s|[{a1,...,an}]|v
a∈{a1,...,an}
a−→s(cid:1)|[{a1,...,an}]|v(cid:1)
a /∈{a1,...,an}
a−→s|[{a1,...,an}]|v(cid:1)
s|[a1,...,an]|v
λ(cid:1)(cid:1)(cid:2)s(cid:1)
λ(cid:1)(cid:1)(cid:2)s(cid:1)|[{a1,...,an}]|v
λ(cid:1)(cid:1)(cid:2)v(cid:1)
v
s
s|[{a1,...,an}]|v
λ(cid:1)(cid:1)(cid:2)s|[{a1,...,an}]|v(cid:1)
The upper rules are common for CSP- (or LOTOS-) style cal-
culi. The two rules at the bottom leave the Markov behavior
in both states untouched: The transitions are just interleaved
(which is justiﬁed by the memoryless property of exponential
distributions). The parallel composition of two IMCs M and
N on synchronization set A is deﬁned as sM
0 , where
0 denotes the initial state of M and sN
sM
0 is the initial state of
N . The resulting IMC is referred to as M|[A]|N . Given the
structural operational semantics, we can establish the follow-
ing lemma.
Lemma 2 M|[A]|N is uniform whenever M and N are uni-
form.
0 |[A]|sN
The proof uses that Markov transitions are just interleaved by
the operational rules, which implies that the uniform rates of N
and M just add up in M|[A]|N . Again the converse direction
is not valid, i. e., whenever a uIMC is the result of a parallel
composition of two IMCs M and N this does not imply that
M and N are uniform.
Minimization. We will follow a compositional minimiza-
tion strategy, where we minimize intermediate graphs accord-
ing to an equivalence relation which (1) abstracts from inter-
nal computation, similar to weak or branching bisimulation,
(2) employs lumping [21] of Markov transitions, and (3) leaves
the branching structure otherwise untouched. There are many
candidates for such an equivalence notion. The equivalence
relation we focus on here is stochastic branching bisimula-
tion which is a variant of branching bisimulation [30] and
stochastic weak bisimulation [14]. Among various possible
notions, branching bisimulation and its stochastic counterpart
have been proven useful, and are implemented efﬁciently in the
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007CADP toolkit [11]. The uniformity preservation result we es-
tablish below can also be established for other variations (such
as weak bisimulation [14]). In the sequel, when we talk about
uniform IMCs, we mean uniform with respect to its reachable
states. The restriction to reachable states allows us to handle
models with unreachable states having arbitrary rates. This
contradicts the original uniformity condition, but is irrelevant
for the behavior of the IMC.
Deﬁnition 6 (Stochastic branching bisimulation) For
a
given IMC M = (S, Act,−→, (cid:1)(cid:1)(cid:2), s0), an equivalence
relation B ⊆ S × S is a stochastic branching bisimulation iff
for all s1, s2, t1 ∈ S the following holds: If (s1, t1) ∈ B then
a−→ s2 implies
1. s1
either a = τ and (s2, t1) ∈ B, or
1, t2 ∈ S : t1
∃t(cid:1)
and
2. s1 (cid:5) τ−→ implies ∃t(cid:1)
Rate(t(cid:1)
1) ∈ B ∧ (s2, t2) ∈ B,
1 (cid:5) τ−→: ∀C ∈ S/B : Rate(s1, C) =
a−→ t2 ∧ (s1, t(cid:1)
τ∗−→ t(cid:1)
τ∗−→ t(cid:1)
1, C).
1 : t1
1
0 and sN
Two states are stochastic branching bisimilar, iff they are con-
tained in some stochastic branching bisimulation B.
Given IMC M, StoBraBi (M) denotes the stochastic branch-
ing bisimilarity quotient IMC of M. For two IMCs M and
N we say that M and N are stochastic branching bisimilar
iff sM
0 are branching bisimilar. We can prove the fol-
lowing lemma, using that the uniformity condition for IMCs is
imposed for stable states only.
Lemma 3 Given IMCs M and N , it holds that if M and N
are stochastic branching bisimilar then: M is uniform iff N is
uniform.
We can directly derive the following corollary from the
above lemma.
Corollary 1 An IMC M is uniform iff StoBraBi (M) is uni-
form.
Elapse. We use phase-type distributions to speciﬁy the timed
probabilistic behavior of a system under study. Recall that,
structurally, a phase-type distribution Ph is a CTMC (S, A, R)
with a distinguished initial state i and absorbing state a. In the
composition context, the distribution Ph can be viewed as de-
scribing the time up to which the occurrence of an event f has
to be delayed, since the occurrence of some other event r. This
interpretation is a special case of what is called a time con-
straint in [15], where an elapse operator is introduced. This
operator enriches Ph with “synchronization potential” needed
to effectively incorporate the Markov chain of Ph into the be-
havior described by some LTS or IMC. Since Ph is a CTMC
we can assume that it is uniformized without making any re-
strictions [19].
We will use this operator below, so we give its seman-
tics here, simpliﬁed to what is actually used later. We de-
note the operator by El(Ph, f, r).
It takes as parameters
(cid:6)
(cid:7)
(cid:4)
, R ˙∪
s(cid:1) E(cid:1)(cid:1)(cid:2) s(cid:1)
a f−→ s(cid:1), s(cid:1)
a distribution Ph (given as a uCTMC (S, A, R) with dis-
tinguished states i and a and uniform rate E), an action
f which should only occur once the Ph distributed delay
is over, and an action r governing when that delay should
start. With these parameters, El(Ph, f, r) generates IMC
(S ˙∪ {s(cid:1)} , A,
, s(cid:1)). Here
s(cid:2)
is a fresh initial state of the resulting IMC where we have
added two interactive transitions. The freshly inserted state s(cid:2)
gets a Markov self-loop with rate E assigned. This assures
uniformity of the resulting IMC. We will also use a variant of
this operator, referred to as El(cid:2)
, where the initial state remains
unchanged at i, but otherwise the construction is the same.
(cid:5)
r−→ i
In El(Ph, f, r), between the occurrences of r and f, there
must be a delay which is given by Ph. To enforce this also
for the LTS of our system under study, we incorporate this
IMC into the system model using parallel composition with
appropriate synchronization sets.
4 Transformation and uCTMDP analysis
In this section we describe a transformation procedure from
closed uIMCs to uCTMDPs and discuss in what sense it pre-
serves the properties we are interested in. We then recall
from [2] how to analyze the resulting CTMDP, explain how
we implemented this algorithm, and apply both steps to the
FTWC case study.
4.1 From uIMC to uCTMDP
In the following description we assume that uIMC M =
(S, Act,−→, (cid:1)(cid:1)(cid:2), s0) is given and that we aim to transform
M into a CTMDP, say CM. We present a transformation from
a (u)IMC under consideration to a strictly alternating (u)IMC.
The latter is a (u)IMC in which interactive and Markov states
occur strictly alternating, and where hybrid states are not
present anymore. Each strictly alternating IMC corresponds
to a CTMDP such that the timed behavior of both systems is
the same.
We do not allow for Zeno-behavior in the model under con-
sideration, Zenoness manifests itself as cycles of interactive
transitions, which owed to the closed system view can hap-
pen in zero time. The model may in general contain absorbing
states, but for the sake of simplicity, we will not consider them,
and thus assume SA = ∅. In the uniform setting absorbing
states are absent (for non-zero uniform rates).
An IMC M with state space S is called alternating iff for
all s ∈ S, ({s} × R
+ × S) ∩ (cid:1)(cid:1)(cid:2)(cid:5)= ∅ implies ({s} × Act ×
S) ∩ −→= ∅. If in addition (i.) (SM × R
+ × SM ) ∩ (cid:1)(cid:1)(cid:2)= ∅
and (ii.) (SI × Act × SI) ∩ −→= ∅ , we call M strictly
alternating. An IMC satisfying the condition (i.) (respectively
(ii.)) is called Markov (interactive) alternating IMC.
Technically, the transformation procedure comprises the
following steps, which will be shown to preserve the proba-
bilistic behavior of the model, and which result in a strictly
alternating IMC: (1) ensure that S contains interactive and
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007Markov states only (alternating IMC); (2) make the target state
of each Markov transition an interactive state (Markov alter-
nating IMC); (3) make the target state of each interactive tran-
sition a Markov state (interactive alternating IMC). While (1)
has to be done ﬁrst, steps (2) and (3) can also be interchanged.
Step (1): Alternating. An alternating IMC does not posses
hybrid states while the original IMC may do. Thus let s ∈ SH.
Owed to the urgency assumption which is due to our closed
system view, all Markov transitions are cut off and s is added to
SI. For the corresponding alternating IMC a(M) we change
the Markov transition relation to (SM × R
+ × S) ∩ (cid:1)(cid:1)(cid:2).
λ(cid:1)(cid:1)(cid:2) s(cid:2)
Step (2): Markov alternating. We need to split all occur-
ring sequences of Markov transitions and states. In summary
this ensures that each Markov transition always ends in an in-
teractive state. This is achieved as follows. Suppose s ∈ SM
and s(cid:2) ∈ SM . In order to break this sequence
with s
of Markov states we introduce a fresh interactive state (s, s(cid:2))
λ(cid:1)(cid:1)(cid:2) (s, s(cid:2)). State (s, s(cid:2)) in turn
which is connected to s via s
τ−→ s(cid:2)
is connected via (s, s(cid:2))
to s(cid:2)
. This yields the following
transformation step. For a given alternating IMC M, we de-
ﬁne the Markov alternating IMC:
mA(M) := (S(cid:2), Act,−→mA(M), (cid:1)(cid:1)(cid:2)mA(M), s0) with
λ(cid:1)(cid:1)(cid:2) s(cid:1)},
• S(cid:1) = S ˙∪ {(s, s(cid:1)) ∈ SM × SM | ∃λ ∈ R
• −→mA(M) = −→ ˙∪ {((s, s(cid:1)), τ, s(cid:1)) ∈ S(cid:1) × {τ} × SM | ∃λ ∈
˙∪ {(s, λ, (s, s(cid:1))) ∈
• (cid:1)(cid:1)(cid:2)mA(M) = (cid:1)(cid:1)(cid:2) ∩ (SM × R
+ × SI )
+ : s
+ : s
R
λ(cid:1)(cid:1)(cid:2) s(cid:1)}.
λ(cid:1)(cid:1)(cid:2) s(cid:1)},
+ × S(cid:1) | s
SM × R
Interactive alternating. We now handle se-
Step (3):
quences of interactive transitions ending in a Markov state. To
compress these sequences, we calculate the transitive closure
of interactive transitions for each interactive state s that (is ei-
ther the initial state of the uIMC or) has at least one Markov
predecessor. The computation is carried out in a way such
that we get all Markov successors of s that terminate these se-
quences. We label the resulting compressed transitions with
˙∪ {τ} (also denoted Words).
words over the alphabet Act+\τ
Note, that interactive states that do not have any Markov state
as predecessor will not be contained in the resulting interac-
tive (or strictly) alternating uIMC any more. These states vio-
late the strict alternation of interactive and Markov states and
therefore will not be contained in the CTMDP.
For a Markov alternating IMC M we deﬁne iA(M) :=
(S(cid:2), Words,−→iA(·), (cid:1)(cid:1)(cid:2), s0) by
• S(cid:1) = SM ˙∪ S(cid:1)
I where S(cid:1)
+} ∪ {s0},
• −→iA(·):= {(s, W, t) ∈ S(cid:1)
I = {s ∈ SI | ∃t ∈ SM : t
I × Words × SM | s W=⇒ t}.
some λ ∈ R
λ(cid:1)(cid:1)(cid:2) s, for
This yields a strictly alternating IMC. So, after applying steps
(1)–(3) to uIMC M we obtain uIMC M(cid:2)
which is strictly
alternating and where interactive transitions are labeled by
˙∪ {τ}. The resulting uIMC
words over the alphabet Act+\τ
λi(cid:1)(cid:1)(cid:2) s(cid:2), i = 1, 2 . . . , n}.
M(cid:2) = (S = SI ˙∪ SM , Words,−→, (cid:1)(cid:1)(cid:2), s0) can now be
(cid:1)n