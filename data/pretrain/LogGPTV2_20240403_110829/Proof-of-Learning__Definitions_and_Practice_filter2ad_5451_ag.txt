978-1-4757-6487-1 978-0-387-35568-9. DOI: 10.1007/978- 0- 387-
35568- 9 18. [Online]. Available: http://link.springer.com/10.1007/
978-0-387-35568-9 18 (visited on 11/09/2020).
Z. Ghodsi, T. Gu, and S. Garg, “SafetyNets: Veriﬁable execution of
deep neural networks on an untrusted cloud,” p. 10,
[15] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning rep-
resentations by back-propagating errors,” nature, vol. 323, no. 6088,
pp. 533–536, 1986.
S. T. Setty, R. McPherson, A. J. Blumberg, and M. Walﬁsh, “Making
argument systems for outsourced computation practical (sometimes).,”
in NDSS, vol. 1, 2012, p. 17.
S. Setty, V. Vu, N. Panpalia, B. Braun, A. J. Blumberg, and M.
Walﬁsh, “Taking proof-based veriﬁed computation a few steps closer
to practicality,” in Presented as part of the 21st {USENIX} Security
Symposium ({USENIX} Security 12), 2012, pp. 253–268.
[18] B. Braun, A. J. Feldman, Z. Ren, S. Setty, A. J. Blumberg, and
M. Walﬁsh, “Verifying computations with state,” in Proceedings of
the Twenty-Fourth ACM Symposium on Operating Systems Principles,
2013, pp. 341–357.
[19] C. Hawblitzel, J. Howell, M. Kapritsos, J. R. Lorch, B. Parno, M. L.
Roberts, S. Setty, and B. Zill, “Ironﬂeet: Proving practical distributed
systems correct,” in Proceedings of the 25th Symposium on Operating
Systems Principles, 2015, pp. 1–17.
[20] C. Tan, L. Yu, J. B. Leners, and M. Walﬁsh, “The efﬁcient server audit
problem, deduplicated re-execution, and the web,” in Proceedings of
the 26th Symposium on Operating Systems Principles, 2017, pp. 546–
564.
[21] Y. Ishai, E. Kushilevitz, and R. Ostrovsky, “Efﬁcient arguments
without short pcps,” in Twenty-Second Annual IEEE Conference on
Computational Complexity (CCC’07), IEEE, 2007, pp. 278–291.
[22] M. Walﬁsh and A. J. Blumberg, “Verifying computations without
the ACM, vol. 58, no. 2,
reexecuting them,” Communications of
pp. 74–84, 2015.
[23] M. Abadi, M. Burrows, M. Manasse, and T. Wobber, “Moderately
hard, memory-bound functions,” ACM Trans. Internet Techn., vol. 5,
pp. 299–327, May 2005. DOI: 10.1145/1064340.1064341.
[24] M. Abliz and T. Znati, “A guided tour puzzle for denial of service
prevention,” in 2009 Annual Computer Security Applications Confer-
ence, 2009, pp. 279–288. DOI: 10.1109/ACSAC.2009.33.
[25] B. Waters, A. Juels, J. Halderman, and E. Felten, “New client puzzle
outsourcing techniques for dos resistance,” Jan. 2004, pp. 246–256.
DOI: 10.1145/1030083.1030117.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:16:15 UTC from IEEE Xplore.  Restrictions apply. 
1052
[37]
[38]
support vector machines,” arXiv preprint arXiv:1206.6389, 2012.
S. Song, K. Chaudhuri, and A. D. Sarwate, “Stochastic gradient
descent with differentially private updates,” in 2013 IEEE Global
Conference on Signal and Information Processing, IEEE, 2013,
pp. 245–248.
L. Batina, S. Bhasin, D. Jap, and S. Picek, “CSI NN: Reverse
engineering of neural network architectures through electromagnetic
side channel,” in 28th USENIX Security Symposium (USENIX Security
19), Santa Clara, CA: USENIX Association, Aug. 2019, pp. 515–532,
ISBN: 978-1-939133-06-9. [Online]. Available: https://www.usenix.
org/conference/usenixsecurity19/presentation/batina.
[26]
F. Coelho, An (almost) constant-effort solution-veriﬁcation proof-of-
work protocol based on merkle trees, Cryptology ePrint Archive,
Report 2007/433, https://eprint.iacr.org/2007/433, 2007.
[31]
[28]
[27] ——, Exponential memory-bound functions for proof of work proto-
cols, Cryptology ePrint Archive, Report 2005/356, https://eprint.iacr.
org/2005/356, 2005.
J. Tromp, “Cuckoo cycle: A memory bound graph-theoretic proof-
of-work,” vol. 8976, Jan. 2015, pp. 49–62, ISBN: 978-3-662-48050-2.
DOI: 10.1007/978-3-662-48051-9 4.
[29] A. Back, “Hashcash - a denial of service counter-measure,” Sep. 2002.
S. Nakamoto, “Bitcoin: A peer-to-peer electronic cash system,” Cryp-
[30]
tography Mailing list at https://metzdowd.com, Mar. 2009.
L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and J. D.
Tygar, “Adversarial machine learning,” in Proceedings of the 4th ACM
workshop on Security and artiﬁcial intelligence, 2011, pp. 43–58.
[32] N. Papernot, P. McDaniel, A. Sinha, and M. P. Wellman, “Sok:
Towards the science of security and privacy in machine learning,” in
2018 IEEE European Symposium on Security and Privacy (EuroS&P),
IEEE, 2018.
[33] B. Biggio and F. Roli, “Wild patterns: Ten years after the rise of
adversarial machine learning,” Pattern Recognition, vol. 84, pp. 317–
331, 2018.
[34] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfel-
low, and R. Fergus, “Intriguing properties of neural networks,” arXiv
preprint arXiv:1312.6199, 2013.
[35] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. ˇSrndi´c, P. Laskov,
G. Giacinto, and F. Roli, “Evasion attacks against machine learning
at test time,” in Joint European conference on machine learning and
knowledge discovery in databases, Springer, 2013, pp. 387–402.
[36] B. Biggio, B. Nelson, and P. Laskov, “Poisoning attacks against
[42]
[40]
[39] M. Jagielski, N. Carlini, D. Berthelot, A. Kurakin, and N. Papernot,
“High-Fidelity Extraction of Neural Network Models,” arXiv e-prints,
arXiv:1909.01838, arXiv:1909.01838, Sep. 2019. arXiv: 1909.01838
[cs.LG].
F. Boenisch, “A Survey on Model Watermarking Neural Networks,”
arXiv e-prints, arXiv:2009.12153, arXiv:2009.12153, Sep. 2020.
arXiv: 2009.12153 [cs.CR].
[41] Y. Adi, C. Baum, M. Cisse, B. Pinkas, and J. Keshet, “Turning
your weakness into a strength: Watermarking deep neural networks
by backdooring,” in 27th USENIX Security Symposium (USENIX
Security 18), USENIX Association, Aug. 2018, ISBN: 978-1-939133-
04-5. [Online]. Available: https : / / www . usenix . org / conference /
usenixsecurity18/presentation/adi.
J. Zhang, Z. Gu, J. Jang, H. Wu, M. P. Stoecklin, H. Huang, and
I. Molloy, “Protecting intellectual property of deep neural networks
with watermarking,” in Proceedings of the 2018 on Asia Conference
on Computer and Communications Security, 2018. DOI: 10 . 1145 /
3196494 .3196550. [Online]. Available: https : // app. dimensions. ai /
details/publication/pub.1104339451.
P. Maini, M. Yaghini, and N. Papernot, “Dataset inference: Own-
ership resolution in machine learning,” in International Conference
on Learning Representations, 2021. [Online]. Available: https : / /
openreview.net/forum?id=hvdKKV2yt7T.
[44] V. Chandrasekaran, K. Chaudhuri, I. Giacomelli, S. Jha, and S. Yan,
“Model extraction and active learning,” CoRR, vol. abs/1811.02054,
2018. arXiv: 1811.02054. [Online]. Available: http://arxiv.org/abs/
1811.02054.
T. Lee, B. Edwards, I. Molloy, and D. Su, “Defending against machine
learning model stealing attacks using deceptive perturbations,” arXiv
preprint arXiv:1806.00054, 2018.
I. M. Alabdulmohsin, X. Gao, and X. Zhang, “Adding robustness to
support vector machines against adversarial reverse engineering,” in
Proceedings of the 23rd ACM International Conference on Conference
on Information and Knowledge Management, 2014, pp. 231–240.
[47] H. Jia, C. A. Choquette-Choo, and N. Papernot, “Entangled Wa-
termarks as a Defense against Model Extraction,” arXiv e-prints,
[46]
[43]
[45]
arXiv:2002.12200, arXiv:2002.12200, Feb. 2020. arXiv: 2002.12200
[cs.CR].
[54]
[48] R. Namba and J. Sakuma, “Robust Watermarking of Neural Net-
work with Exponential Weighting,” arXiv e-prints, arXiv:1901.06151,
arXiv:1901.06151, Jan. 2019. arXiv: 1901.06151 [cs.CR].
[49] H. Li, E. Wenger, B. Y. Zhao, and H. Zheng, “Piracy Resistant Water-
marks for Deep Neural Networks,” arXiv e-prints, arXiv:1910.01226,
arXiv:1910.01226, Oct. 2019. arXiv: 1910.01226 [cs.CR].
[50] H. Jia, C. A. Choquette-Choo, V. Chandrasekaran, and N. Papernot,
“Entangled Watermarks as a Defense against Model Extraction,” arXiv
e-prints, arXiv:2002.12200, arXiv:2002.12200, Feb. 2020. arXiv:
2002.12200 [cs.CR].
[51] K. Liu, B. Dolan-Gavitt, and S. Garg, “Fine-pruning: Defending
against backdooring attacks on deep neural networks,” in 21st Inter-
national Symposium on Research in Attacks, Intrusions, and Defenses,
2018.
[52] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B.
Zhao, “Neural cleanse: Identifying and mitigating backdoor attacks
in neural networks,” 2019 IEEE Symposium on Security and Privacy
(SP), pp. 707–723, 2019.
[53] H. Jia, C. A. Choquette-Choo, V. Chandrasekaran, and N. Papernot,
“Entangled watermarks as a defense against model extraction,” in 30th
{USENIX} Security Symposium ({USENIX} Security 21), 2021.
J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, M.
Ranzato, A. Senior, P. Tucker, K. Yang, et al., “Large scale distributed
deep networks,” Advances in neural information processing systems,
vol. 25, pp. 1223–1231, 2012.
L. Lamport, R. Shostak, and M. Pease, “The byzantine generals
problem,” in Concurrency: the Works of Leslie Lamport, 2019,
pp. 203–226.
[56] X. Glorot, A. Bordes, and Y. Bengio, “Deep sparse rectiﬁer neural
networks,” in Proceedings of the fourteenth international conference
on artiﬁcial intelligence and statistics, 2011, pp. 315–323.
[57] X. Glorot and Y. Bengio, “Understanding the difﬁculty of training
deep feedforward neural networks,” in Proceedings of the thirteenth
international conference on artiﬁcial intelligence and statistics, 2010,
pp. 249–256.
[58] G. Klambauer, T. Unterthiner, A. Mayr, and S. Hochreiter, “Self-
normalizing neural networks,” in Advances in Neural
Informa-
tion Processing Systems, I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds.,
vol. 30, Curran Associates,
[Online].
Available: https : / / proceedings . neurips . cc / paper / 2017 / ﬁle /
5d44ee6f2c3f71b73125876103c8f6c4-Paper.pdf.
Inc., 2017, pp. 971–980.
[59] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectiﬁers:
Surpassing human-level performance on imagenet classiﬁcation,” in
Proceedings of
the IEEE International Conference on Computer
Vision (ICCV), Dec. 2015.
[60] V. Vapnik, The nature of statistical learning theory. Springer science
[55]
& business media, 2013.
I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio, Deep learning,
2. MIT press Cambridge, 2016, vol. 1.
[62] H. Robbins and S. Monro, “A stochastic approximation method,” The
[61]
annals of mathematical statistics, pp. 400–407, 1951.
[66]
[65]
[63] M. van de Zande, Leveraging zero-knowledge succinct arguments of
knowledge for efﬁcient veriﬁcation of outsourced training of artiﬁcial
neural networks, May 2019. [Online]. Available: http://essay.utwente.
nl/79180/.
[64] H. Chabanne, J. Keuffer, and R. Molva, “Embedded proofs for
veriﬁable neural networks,” IACR Cryptol. ePrint Arch., vol. 2017,
p. 1038, 2017. [Online]. Available: http://eprint.iacr.org/2017/1038.
S. Lee, H. Ko, J. Kim, and H. Oh, “Vcnn: Veriﬁable convolutional
neural network,” IACR Cryptol. ePrint Arch., vol. 2020, p. 584, 2020.
[Online]. Available: https://eprint.iacr.org/2020/584.
S. L. Hyland and S. Tople, “On the intrinsic privacy of stochastic
gradient descent,” CoRR, vol. abs/1912.02919, 2019. arXiv: 1912 .
02919. [Online]. Available: http://arxiv.org/abs/1912.02919.
J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, “How transferable are
features in deep neural networks?” In Advances in neural information
processing systems, 2014, pp. 3320–3328.
[68] A. M. Saxe, J. L. McClelland, and S. Ganguli, “Exact solutions to
the nonlinear dynamics of learning in deep linear neural networks,”
arXiv e-prints, arXiv:1312.6120, arXiv:1312.6120, Dec. 2013. arXiv:
1312.6120 [cs.NE].
F. J. M. Jr., “The kolmogorov-smirnov test for goodness of ﬁt,”
Journal of the American Statistical Association, vol. 46, no. 253,
pp. 68–78, 1951. DOI: 10.1080/01621459.1951.10500769. eprint:
[69]
[67]
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:16:15 UTC from IEEE Xplore.  Restrictions apply. 
1053
https : / / www. tandfonline . com / doi / pdf / 10 . 1080 / 01621459 . 1951 .
10500769. [Online]. Available: https://www.tandfonline.com/doi/abs/
10.1080/01621459.1951.10500769.
[74]
[70] B. Hanin and D. Rolnick, “How to start training: The effect of ini-
tialization and architecture,” in Advances in Neural Information Pro-
cessing Systems, S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett, Eds., vol. 31, Curran Associates,
Inc., 2018, pp. 571–581. [Online]. Available: https : / / proceedings .
neurips . cc / paper / 2018 / ﬁle / d81f9c1be2e08964bf9f24b15f0e4900 -
Paper.pdf.
[71] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al., “Pytorch: An
imperative style, high-performance deep learning library,” in Advances
in neural information processing systems, 2019, pp. 8026–8037.
[72] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
S. Ghemawat, G. Irving, M. Isard, et al., “Tensorﬂow: A system
for large-scale machine learning,” in 12th {USENIX} symposium on
operating systems design and implementation ({OSDI} 16), 2016,
pp. 265–283.
S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B.
Catanzaro, and E. Shelhamer, “Cudnn: Efﬁcient primitives for deep
learning,” arXiv preprint arXiv:1410.0759, 2014.
T. M. Cover and J. A. Thomas, Elements of Information Theory (Wiley
Series in Telecommunications and Signal Processing). USA: Wiley-
Interscience, 2006, ISBN: 0471241954.
[75] M. Jagielski, N. Carlini, D. Berthelot, A. Kurakin, and N. Papernot,
“High accuracy and high ﬁdelity extraction of neural networks,” in
29th {USENIX} Security Symposium ({USENIX} Security 20), 2020.
[76] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learn-
Image Recognition,” arXiv e-prints, arXiv:1512.03385,
ing for
arXiv:1512.03385, Dec. 2015. arXiv: 1512.03385 [cs.CV].
[73]
[78]
[77] A. Krizhevsky, “Learning multiple layers of features from tiny
images,” 2009.
(). “Reproducibility — PyTorch 1.7.0 documentation,” [Online].
Available: https : / / pytorch . org / docs / stable / notes / randomness . html
(visited on 12/04/2020).
P. Yin, M. Pham, A. Oberman, and S. Osher, “Stochastic backward
euler: An implicit gradient descent algorithm for k-means clustering,”
Journal of Scientiﬁc Computing, vol. 77, no. 2, pp. 1133–1146, 2018.
[80] Y. LeCun, L. Jackel, L. Bottou, A. Brunot, C. Cortes, J. Denker,
H. Drucker, I. Guyon, U. Muller, E. Sackinger, et al., “Comparison
of learning algorithms for handwritten digit recognition,” in Inter-
national conference on artiﬁcial neural networks, Perth, Australia,
vol. 60, 1995, pp. 53–60.
[81] Y. LeCun, “The mnist database of handwritten digits,” http://yann.
[79]
lecun. com/exdb/mnist/, 1998.
[84]
[83]
[82] K. Liu, B. Dolan-Gavitt, and S. Garg, “Fine-pruning: Defending
against backdooring attacks on deep neural networks,” in Interna-
tional Symposium on Research in Attacks, Intrusions, and Defenses,
Springer, 2018, pp. 273–294.
J. Frankle and M. Carbin, “The lottery ticket hypothesis: Finding
sparse, trainable neural networks,” arXiv preprint arXiv:1803.03635,
2018.
P. Mohassel and Y. Zhang, “Secureml: A system for scalable privacy-
preserving machine learning,” in 2017 IEEE Symposium on Security
and Privacy (SP), 2017, pp. 19–38. DOI: 10.1109/SP.2017.12.
[85] R. C. Merkle, “A digital signature based on a conventional encryption
function,” in Conference on the theory and application of crypto-
graphic techniques, Springer, 1987, pp. 369–378.
J. K. Salmon, M. A. Moraes, R. O. Dror, and D. E. Shaw, “Parallel
random numbers: As easy as 1, 2, 3,” in Proceedings of 2011 Inter-
national Conference for High Performance Computing, Networking,
Storage and Analysis, 2011, pp. 1–12.
[87] K. Claessen and M. H. Pałka, “Splittable pseudorandom number
generators using cryptographic hashing,” ACM SIGPLAN Notices,
vol. 48, no. 12, pp. 47–58, 2013.
[88] C. E. Shannon, “A mathematical theory of communication,” The Bell
[86]
[89]
system technical journal, vol. 27, no. 3, pp. 379–423, 1948.
P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy,
D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright,
S. J. van der Walt, M. Brett, J. Wilson, K. J. Millman, N. Mayorov,
A. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, ˙I. Polat,
Y. Feng, E. W. Moore, J. VanderPlas, D. Laxalde, J. Perktold,
R. Cimrman, I. Henriksen, E. A. Quintero, C. R. Harris, A. M.
Archibald, A. H. Ribeiro, F. Pedregosa, P. van Mulbregt, and SciPy
1.0 Contributors, “SciPy 1.0: Fundamental Algorithms for Scientiﬁc
Computing in Python,” Nature Methods, vol. 17, pp. 261–272, 2020.
DOI: 10.1038/s41592-019-0686-2.
[90] D. Dua and C. Graff, UCI machine learning repository, 2017.
[Online]. Available: http://archive.ics.uci.edu/ml.
APPENDIX A
MARKOV PROCESSES AND ENTROPY
We include additional deﬁnitions as they pertain to our
proofs in Section VI.
Deﬁnition 3 (Markov Process). A stochastic process is said
to have the Markov property if
its future is independent
of
i.e.,
P r( ˜Wi+1| ˜W0 . . . ˜Wi) = P r( ˜Wi+1| ˜Wi). A stochastic process
with the Markov property is said to be a Markov process.
its past, when conditioned on its current state,