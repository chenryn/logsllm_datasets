If T
T +(l−1)T (cid:2) of the bandwidth when each link is
factor of
= T /2, n = 64, this fraction
of bandwidth T .
is 85.6%. Contrast this to the chain replication scheme
where each link is traversed by each block and the band-
(cid:2) in our example).
width is limited by the slowest link (T
3. If a node i sends block b in round j, deﬁne slack(i, j) to
be j minus the step number in which i received b. The av-
erage slack for a given steady step j, avg slack(j)is de-
ﬁned as
. We found that avg slack(j),
for any steady step j is a constant equal to 2(1 − l−1
n−2 ) =
2(1 − log n−1
n−2 ). For moderate n, log n << n, average
slack is ≈ 2. A slack greater than 1 tells us that the node
received the block it must send on the current step at least
2 steps in the past. This is of value because if the node is
running slightly late, it may be able to catch up.
i sends in j slack(i,j)
#senders in j
(cid:2)
Insights from using RDMC
A more comprehensive investigation of robustness in the
presence of delay represents an interesting direction for future
research. Our experiments were performed both on a dedi-
cated cluster and in a large shared supercomputer, and exposed
RDMC to a variety of scheduling and link delays, but in an un-
controlled way. Performance does drop as a function of scale
(presumably, in part because of such effects), but to a limited
degree. The open question is the degree to which this residual
loss of performance might be avoided.
4.6
We now have several years of experience with RDMC in var-
ious settings, and have used it within our own Derecho plat-
form. Several insights emerge from these activities.
Recovery From Failure. As noted earlier, an RDMC group
behaves much like a set of side-by-side TCP connections from
the sender to each of the receivers. Although failures are
sensed when individual RDMA connections report a problem,
our policy of relaying failure information quickly converges to
a state in which the disrupted RDMC group ceases new trans-
missions, and in which all surviving endpoints are aware of the
failure. At this point, some receivers may have successfully re-
ceived and delivered messages that other receivers have not yet
ﬁnished receiving.
To appreciate the resulting recovery challenge, we can ask
what the sender “knows” at the time that it ﬁrst learns that
its RDMC group has failed. Much as a TCP sender does
not learn that data in the TCP window has been received and
processed unless some form of end-to-end acknowledgement
is introduced, an RDMC sender trusts RDMC to do its job.
If a group is used for a series of transfers the sender will
lack certainty about the status of recently-transmitted mes-
sages (RDMC does not provide an end-to-end status reporting
mechanism). On the other hand, disruption will be sensed by
all RDMC group members if something goes wrong. More-
over, failure will always be reported when closing (destroy-
ing) the RDMC group. Thus, if the group close operation is
successful, the sender (and all receivers) can be conﬁdent that
every RDMC message reached every destination.
For most purposes listed in the introduction, this guarantee
76
is adequate. For example, if a multicast ﬁle transfer ﬁnishes
and the close is successful, the ﬁle was successfully delivered
to the full set of receivers, with no duplications, omissions or
corruption. Conversely, if the transfer fails, every receiver
learns this and the ﬁle transfer tool could simply retry the
transfer within the surviving members. If the tool was trans-
ferring a long sequence of ﬁles and the cost of resending them
were a concern, it could implement an end-to-end status check
to ﬁgure out which ones don’t need to be resent.
Systems seeking stronger guarantees can leverage RDMC
too. For example, Derecho augments RDMC with a replicated
status table implemented using one-sided RDMA writes [9].
On reception of an RDMC message, Derecho buffers it brieﬂy.
Delivery occurs only after every receiver has a copy of the
message, which receivers discover by monitoring the status
table. A similar form of distributed status tracking is used
when a failure disrupts an RDMC group. Here, Derecho uses
a leader-based cleanup mechanism (again based on a one-
sided RDMA write protocol) to collect state from all sur-
viving nodes, analyze the outcome, and then tell the partici-
pants which buffered messages to deliver and which to discard.
Through a series of such extensions, Derecho is able to offer
the full suite of Paxos guarantees, yet it can still transfer all
messages over RDMC.
Small messages. RDMC is optimized for bulk data move-
ment. The work reported here only looked at the large message
case. Derecho includes a small-message protocol that uses
one-sided RDMA writes into a set of round-robin bounded
buffers, one per receiver, and compares performance of that
method with that of RDMC. In summary, the optimized small
message protocol gains as much as a 5x speedup compared to
RDMC provided that the group is small enough (up to about 16
members) and the messages are small enough (no more than
10KB). For larger groups or larger messages, and for long se-
ries of messages that can be batched, the binomial pipeline
dominates.
Memory management. RDMC affords ﬂexible memory
management. In the experiments reported here, we preregis-
ter memory regions that will be used with the RDMA NIC, but
allocate memory for each new message when the ﬁrst block ar-
rives. Thus receivers perform a call to malloc on the critical
path. In applications that can plan ahead, better performance
can be achieved by performing memory allocation before the
start of a long series of transfers.
5 Experiments
5.1 Setup
We conducted experiments on several clusters equipped with
different amounts of memory and NIC hardware.
Fractus. Fractus is a cluster of 16 RDMA-enabled nodes run-
ning Ubuntu 16.04, each equipped with a 4x QDR Mellanox
NIC and 94 GB of DDR3 memory. All nodes are connected to
both a 100 Gb/s Mellanox IB switch and a 100 Gb/s Mellanox
RoCE switch, and have one-hop paths to one-another.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:50:05 UTC from IEEE Xplore.  Restrictions apply. 
(a) 256 MB multicasts. Note that the chain send and binomial
pipeline achieve very similar latency.
(b) 8 MB multicasts.
Figure 4: Latency of MPI (MVAPICH) and several RDMC algorithms on Fractus. Group sizes include the sender, so a size of three means
one sender and two receivers.
Figure 5: Breakdown of transfer time and wait time of two nodes taking part in the 256 MB transfer. The majority of time is spent in hardware
(blue), but the sender (left) incurs a higher CPU burden (orange) than the receiver (right). Ofﬂoading RDMC fully into the hardware would
eliminate this residual load and reduce the risk that a long user-level scheduling delay could impact overall transfer performance.
Sierra. The Sierra cluster at Lawrence Livermore National
Laboratory consists of 1,944 nodes of which 1,856 are des-
ignated as batch compute nodes. Each is equipped with two
6-core Intel Xeon EP X5660 processors and 24GB memory.
They are connected by an Inﬁniband fabric which is structured
as a two-stage, federated, bidirectional, fat-tree. The NICs are
4x QDR QLogic adapters each operating at a 40 Gb/s line rate.
The Sierra cluster runs TOSS 2.2, a modiﬁed version of Red
Hat Linux.
Stampede-1. The U. Texas Stampede-1 cluster contains 6400
C8220 compute nodes with 56 Gb/s FDR Mellanox NICs.
Like Sierra, it is batch scheduled with little control over node
placement. We measured unicast speeds of up to 40 Gb/s.
Apt Cluster. The EmuLab Apt cluster contains a total of 192
nodes divided into two classes: 128 nodes have a single Xeon
E5-2450 processor with 16 GB of RAM, while 64 nodes have
two Xeon E5-2650v2 processors and 64 GB of RAM. All have
one FDR Mellanox CX3 NIC which is capable of 56 Gb/s.
Interestingly, Apt has a signiﬁcantly oversubscribed TOR
network that degrades to about 16 Gb/s per link when heav-
ily loaded. This enabled us to look at the behavior of RDMC
under conditions where some network links are much slower
than others. Although the situation is seemingly ideal for tak-
ing the next step and experimenting on hybrid protocols, this
proved to be impractical: Apt is batch-scheduled like Sierra,
with no control over node placement, and we were unable to
dynamically discover network topology.
Our experiments include cases that closely replicate the
RDMA deployments seen in today’s cloud platforms. For
example, Microsoft Azure offers RDMA over Inﬁniband as
part of its Azure Compute HPC framework, and many ven-
dors make use of RDMA in their own infrastructure tools,
both on Inﬁniband and on RoCE. However, large-scale end-
user testbeds exposing RoCE are not yet available: operators
are apparently concerned that heavy use of RoCE could trigger
data-center-wide instability. Our hope is that rollout of DC-
QCN will reassure operators, who would then see an obvious
beneﬁt to allowing their users to access RoCE.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:50:05 UTC from IEEE Xplore.  Restrictions apply. 
77
In all of our experiments, the sender(s) generates a message
containing random data, and we measure the time from when
the send is submitted to the library to when all clients have
gotten an upcall indicating that the multicast has completed.
The largest messages sent have sizes that might arise in appli-
cations transmitting videos, or when pushing large images to
compute nodes in a data analytics environment. Smaller mes-
sage sizes are picked to match tasks such as replicating photos
or XML-encoded messages. Bandwidth is computed as the
number of messages sent, multiplied by the size of each mes-
sage, divided by the total time spent (regardless of the number
of receivers). RDMC does not pipeline messages, so the la-
tency of a multicast is simply the message size divided by its
bandwidth.
5.2 Results
Figure 4 compares the relative performance of the differ-
ent algorithms considered. For comparison, it also shows the
throughput of the heavily optimized MPI Bcast() method
from MVAPICH, a high-performance computing library that
implements the MPI standard on Inﬁniband networks (we
measured this using a separate benchmark suite). As antici-
pated, both sequential send and binomial tree do poorly as the
number of nodes grows. Meanwhile chain send is competi-
tive with binomial pipeline, except for small transfers to large
numbers of nodes where binomial pulls ahead. MVAPICH
falls in between, taking from 1.03× to 3× as long as binomial
pipeline. Throughout the remainder of this paper we primarily
focus on binomial pipeline because of its robust performance
across a range of settings, however we note that chain send can
often be useful due to its simplicity.
5.2.1 Microbenchmarks
In Table 1 we break down the time for a single 256 MB trans-
fer with 1 MB blocks and a group size of 4 (meaning 1 sender
and 3 receivers) conducted on Stampede. All values are in
microseconds, and measurements were taken on the node far-
thest from the root. Accordingly, the Remote Setup and Re-
mote Block Transfers reﬂect the sum of the times taken by the
root to send and by the ﬁrst receiver to relay. Roughly 99% of
the total time is spent in the Remote Block Transfers or Block
Transfers states (in which the network is being fully utilized)
meaning that overheads from RDMC account for only around
1% of the time taken by the transfer.
Figure 5 examines the same send but shows the time usage
for each step of the transfer for both the relayer (whose times
are reported in the table) and for the root sender. Towards the
Remote Setup
Remote Block Transfers
Local Setup
Block Transfers
Waiting
Copy Time
Total
11
461
4
60944
449
215
62084
Table 1: Time (microseconds) for key steps in a transfer.
Figure 6: Multicast bandwidth (computed as the message size di-
vided by the latency) on Fractus across a range of block sizes for
messages between 16 KB and 128 MB, all for groups of size 4.
Figure 7: 1 byte messages/sec. (Fractus)
end of the message transfer we see an anomalously long wait
time on both instrumented nodes. As it turns out, this demon-
strates how RDMC can be vulnerable to delays on individual
nodes. In this instance, a roughly 100 μs delay on the relayer
(likely caused by the OS picking an inopportune time to pre-
empt our process) forced the sender to delay on the following
step when it discovered that the target for its next block wasn’t
ready yet. The CORE-Direct functionality would mitigate this.
In Figure 6, we examine the impact of block size on band-
width for a range of message sizes. Notice that increasing the
block size initially improves performance, but then a peak is
reached. This result is actually to be expected as there are
two competing factors. Each block transfer involves a certain
amount of latency, so increasing the block size actually in-
creases the rate at which information moves across links (with
diminishing returns as the block size grows larger). However,
the overhead associated with the binomial pipeline algorithm
is proportional to the amount of time spent transferring an indi-
vidual block. There is also additional overhead incurred when
there are not enough blocks in the message for all nodes to get
to contribute meaningfully to the transfer.
Finally, Figure 7 measures the number of 1 byte messages
78
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:50:05 UTC from IEEE Xplore.  Restrictions apply. 























	












      
	
(a) Fractus

	