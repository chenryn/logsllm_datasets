# e.g: "mybucket/etcd.backup"        path:         awsSecret: 最后你可以通过给 etcd 集群增加 Learner 节点，实现跨地域热备。因Learner节点属于非投票成员的节点，因此它并不会影响你集群的性能。它的基本工作原理是当Leader 收到写请求时，它会通过 Raft 模块将日志同步给 Learner节点。你需要注意的是，在 etcd 3.4 中目前只支持 1 个 Learner节点，并且只允许串行读。巡检完成集群部署、了解成员管理、构建好监控及告警体系并添加好定时备份策略后，这时终于可以放心给业务使用了。然而在后续业务使用过程中，你可能会遇到各类问题，而这些问题很可能是metrics监控无法发现的，比如如下：1.  etcd    集群因重启进程、节点等出现数据不一致；        2.  业务写入大 key-value 导致 etcd    性能骤降；        3.  业务异常写入大量 key    数，稳定性存在隐患；        4.  业务少数 key 出现写入 QPS 异常，导致 etcd    集群出现限速等错误；        5.  重启、升级 etcd    后，需要人工从多维度检查集群健康度；        6.  变更 etcd 集群过程中，操作失误可能会导致 etcd    集群出现分裂；        \...\... 因此为了实现高效治理 etcd集群，我们可将这些潜在隐患总结成一个个自动化检查项，比如：1.  如何高效监控 etcd    数据不一致性？        2.  如何及时发现大    key-value?        3.  如何及时通过监控发现 key    数异常增长？        4.  如何及时监控异常写入    QPS?    5.  如何从多维度的对集群进行自动化的健康检测，更安心变更？        6.  \...\...        如何将这些 etcd 的最佳实践策略反哺到现网大规模 etcd集群的治理中去呢？答案就是巡检。参考 ServiceMonitor 和 EtcdBackup 机制，你同样可以通过 CRD的方式描述此巡检任务，然后通过相应的 Operator实现此巡检任务。比如下面就是一个数据一致性巡检的 YAML 文件，其对应的Operator 组件会定时、并发检查其关联的 etcd 集群各个节点的 key差异数。     apiVersion: etcd.cloud.tencent.com/v1beta1    kind: EtcdMonitor    metadata:      creationTimestamp: "2020-06-15T12:19:30Z"      generation: 1      labels:        clusterName: gz-qcloud-etcd-03        region: gz        source: etcd-life-cycle-operator      name: gz-qcloud-etcd-03-etcd-node-key-diff      namespace: gz    spec:      clusterId: gz-qcloud-etcd-03      metricName: etcd-node-key-diff      metricProviderName: cruiser      name: gz-qcloud-etcd-03      productName: tke      region: gz    status:      records:      - endTime: "2021-02-25T11:22:26Z"        message: collectEtcdNodeKeyDiff,etcd cluster gz-qcloud-etcd-03,total key num is          122143,nodeKeyDiff is 0         startTime: "2021-02-25T12:39:28Z"      updatedAt: "2021-02-25T12:39:28Z"高可用及自愈通过以上机制，我们已经基本建设好一个高可用的 etcd 集群运维体系了。最后再给你提供几个集群高可用及自愈的小建议：1.  若 etcd 集群性能已满足业务诉求，可容忍一定的延时上升，建议你将    etcd 集群做高可用部署，比如对 3    个节点来说，把每个节点部署在独立的可用区，可容忍任意一个可用区故障。        2.  逐步尝试使用 Kubernetes 容器化部署 etcd    集群。当节点出现故障时，能通过 Kubernetes    的自愈机制，实现故障自愈。        3.  设置合理的 db quota 值，配置合理的压缩策略，避免集群 db quota    满从而导致集群不可用的情况发生。        混沌工程在使用 etcd的过程中，你可能会遇到磁盘、网络、进程异常重启等异常导致的故障。如何快速复现相关故障进行问题定位呢？答案就是混沌工程。一般常见的异常我们可以分为如下几类：1.  磁盘 IO 相关的。比如模拟磁盘 IO 延时上升、IO    操作报错。之前遇到的一个底层磁盘硬件异常导致 IO 延时飙升，最终触发了    etcd 死锁的 Bug，我们就是通过模拟磁盘 IO    延时上升后来验证的。        2.  网络相关的。比如模拟网络分区、网络丢包、网络延时、包重复等。        3.  进程相关的。比如模拟进程异常被杀、重启等。之前遇到的一个非常难定位和复现的数据不一致    Bug，我们就是通过注入进程异常重启等故障，最后成功复现。        4.  压力测试相关的。比如模拟 CPU    高负载、内存使用率等。        开源社区在混沌工程领域诞生了若干个优秀的混沌工程项目，如chaos-mesh、chaos-blade、litmus。这里我重点和你介绍下chaos-mesh，它是基于 Kubernetes实现的云原生混沌工程平台，下图是其架构图（引用自社区）。![](Images/9ae1554a932173d0b523d0973cd20cac.png)savepage-src="https://static001.geekbang.org/resource/image/b8/a7/b87d187ea2ab60d824223662fd6033a7.png"}为了实现以上异常场景的故障注入，chaos-mesh定义了若干种资源类型，分别如下：1.  IOChaos，用于模拟文件系统相关的 IO    延时和读写错误等。        2.  NetworkChaos，用于模拟网络延时、丢包等。        3.  PodChaos，用于模拟业务 Pod 异常，比如 Pod 被杀、Pod    内的容器重启等。        4.  StressChaos，用于模拟 CPU    和内存压力测试。        当你希望给 etcd Pod 注入一个磁盘 IO 延时的故障时，你只需要创建此 YAML文件就好。     apiVersion: chaos-mesh.org/v1alpha1    kind: IoChaos    metadata:      name: io-delay-example    spec:      action: latency      mode: one      selector:        labelSelectors:          app: etcd      volumePath: /var/run/etcd      path: '/var/run/etcd/**/*'      delay: '100ms'      percent: 50      duration: '400s'      scheduler:        cron: '@every 10m'小结最后我们来小结下今天的内容。今天我通过从集群部署、集群组建、监控及告警体系、备份、巡检、高可用、混沌工程几个维度，和你深入介绍了如何构建一个高可靠的etcd 集群运维体系。在集群部署上，当你的业务集群规模非常大、对稳定性有着极高的要求时，推荐使用大规格、高性能的物理机、虚拟机独占部署，并且使用ansible 等自动化运维工具，进行标准化的操作etcd，避免人工一个个修改操作。对容器化部署来说，Kubernetes 场景推荐你使用kubeadm，其他场景可考虑分批、逐步使用 bitnami 提供的 etcd helm包，它是基于 statefulset、PV、PVC实现的，各大云厂商都广泛支持，建议在生产环境前，多验证各个极端情况下的行为是否符合你的预期。在集群组建上，各个节点需要一定机制去发现集群中的其他成员节点，主要可分为 **staticconfiguration** 和 **dynamic servicediscovery**。static configuration 是指集群中各个成员节点信息是已知的，dynamicservice discovery是指你可以通过服务发现组件去注册自身节点信息、发现集群中其他成员节点信息。另外我和你介绍了重要参数initial-cluster-state的含义，它也是影响集群组建的一个核心因素。在监控及告警体系上，我和你介绍了 etcd 网络、磁盘、etcdserver、gRPC核心的 metrics。通过修改 Prometheues 配置文件，添加 etcdtarget，你就可以方便的采集 etcd 的监控数据。我还给你介绍了ServiceMonitor 机制，你可通过它实现动态新增、删除、修改待监控的 etcd实例，灵活的、高效的采集 etcdMetrcis。 备份及还原上，重点和你介绍了 etcd snapshot 命令，etcd-backup-operator的备份任务 CRD机制，推荐使用后者。最后是巡检、混沌工程，它能帮助我们高效治理 etcd集群，及时发现潜在隐患，低成本、快速的复现 Bug和故障等。 思考题好了，这节课到这里也就结束了，最后我给你留了一个思考题。你在生产环境中目前是使用哪种方式部署 etcd 集群的呢？若基于 Kubernetes容器化部署的，是否遇到过容器化后的相关问题？感谢你的阅读，也欢迎你把这篇文章分享给更多的朋友一起阅读。