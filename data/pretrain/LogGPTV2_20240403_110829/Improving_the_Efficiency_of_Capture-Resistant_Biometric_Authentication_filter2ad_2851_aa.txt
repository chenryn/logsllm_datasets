title:Improving the Efficiency of Capture-Resistant Biometric Authentication
Based on Set Intersection
author:Xunhua Wang and
Philip D. Huff and
Brett C. Tjaden
2008 Annual Computer Security Applications Conference
2008 Annual Computer Security Applications Conference
Improving the Efﬁciency of Capture-resistant Biometric Authentication based on
Set Intersection
Xunhua Wang, Philip D. Huff, Brett C. Tjaden
Department of Computer Science
James Madison University
Harrisonburg, VA 22807 USA
{wangxx,huffpd,tjadenbc}@jmu.edu
Abstract
Traditional biometric authentication systems store bio-
metric reference templates in cleartext on an authentication
server, making them vulnerable to theft. Fuzzy extractors al-
low an authentication server to store biometric veriﬁcation
data that are resistant to capture. It is hard to recover the
reference templates from these biometric veriﬁcation data,
thus increasing the privacy of the reference templates. In
this paper, we improve the efﬁciency of a set intersection-
based fuzzy extractor [32] in two ways. First, we speed
up the computation of verifying a biometric sample under
some parameter combinations through integrating a Reed-
Solomon decoding algorithm. Second, we propose a new
function to improve the storage efﬁciency of the fuzzy ex-
tractor. A prototype implementation is developed to validate
our improvements and it shows that our ﬁrst improvement
could speed up computation as many as 2.29 × 106 times.
1
Introduction
Entity authentication is a fundamental issue in informa-
tion security and has been studied extensively in the past
thirty years [1, 17, 25]. Earlier authentication research fo-
cused on password-based authentication and cryptographic
key-based authentication. These two authentications actu-
ally authenticate either a password (remembered by a user)
or a secret (stored on a token), which can be either shared
or transferred. Biometric authentication, on the other hand,
veriﬁes characteristics from human beings and has received
more research attention recently [8, 13, 24]. Unlike pass-
words, the space of biometrics is usually big enough to re-
sist brute-force attacks. Different from cryptographic keys,
biometrics are more intrinsic to the user and are very hard,
if not impossible, to be transferred.
In addition to these beneﬁts, biometric authentication ex-
hibits two distinguishing characteristics. First, most biolog-
ical biometrics do not change much over a long period of
time, making their revocations and system recovery after
compromise very hard. Second, unlike passwords or cryp-
tographic keys, the comparison of a given biometric sample
against a stored biometric template is not exact. Two con-
secutive readings of the same biometric are usually close but
not exactly the same. As a result, comparisons of biometric
samples are often threshold-based and two biometric sam-
ples are considered the same when their similarity is larger
than a threshold t [19]. Consequently, biometric authentica-
tions are affected by two types of errors: false match (FM),
in which two different biometrics are incorrectly considered
the same, and false non-match (FNM), in which two sam-
ples of the same biometric are incorrectly deemed different.
The bigger the similarity threshold t, the bigger the FNM
rate (FNMR) and the smaller the FM rate (FMR). Different
applications tolerate different FMR/FNMR and thus choose
different t.
Similar to passwords, biometrics are typically used for
authenticating a client only (i.e., client-side authentication).
The client ﬁrst enrolls his biometric sample (called refer-
ence template), from which the authentication server gen-
erates and stores related biometric veriﬁcation data (BVD).
Given a cryptographic hash function h, a reference template
A and a fresh biometric sample B to be authenticated, even
when B is close to A, their cryptographic hashes h(A) and
h(B) are very different, making h(A) not appropriate for
BVD. These days, the common practice is to use the refer-
ence template A itself as BVD. However, storing biomet-
ric reference templates on a server in cleartext has negative
security and privacy implications. If the server were com-
promised, all biometric templates stored on it would be re-
vealed and it would be hard to recover from this break.
To address this security issue, the concept of fuzzy ex-
tractor was developed [11, 16]. Given a reference template
A, a fuzzy extractor generates a value U and a uniformly
1063-9527/08 $25.00 © 2008 IEEE
1063-9527/08 $25.00 © 2008 IEEE
DOI 10.1109/ACSAC.2008.11
DOI 10.1109/ACSAC.2008.11
130
140
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:00:38 UTC from IEEE Xplore.  Restrictions apply. 
random secret s. U leaks little information about A or s,
and can be made public. For a fresh biometric sample B
that is sufﬁciently close to A, one can use U and B to re-
produce s. Thus, (cid:2)U, h(s)(cid:3) can be used as capture-resistant
BVD: if (cid:2)U, h(s)(cid:3) were stolen due to a server compromise,
the attacker still could not recover A or s.
Depending on how biometrics are represented, there are
different metrics to measure the closeness of biometric sam-
ples, including Hamming distance, set difference, edit dis-
tance, and set intersection. These representational differ-
ences call for different designs of fuzzy extractor. Dodis et
al. [11] described several fuzzy extractors based on the set
difference metric, where biometric samples are represented
as a set of points and the difference of two biometric sam-
ples A and B is half of their symmetric difference (that is,
|(A − B) ∪ (B − A)| where || denotes set size and ∪ de-
notes set union). These set difference-based fuzzy extrac-
tors are built upon error-correcting codes such as the Reed-
Solomon code [30, 2, 21] and employ a difference threshold
td. B is considered close to A only when their set difference
is not greater than td. Since an error-correcting code works
only when there are more correct elements than errors, these
fuzzy extractors have a fundamental limit on parameter td :
td can not be greater than half of the number of elements in
A.
Because of this limit, for some biometric authentications,
the set intersection metric is more appropriate. For exam-
ple, in many ﬁngerprint authentications, two ﬁngerprints
should be considered the same as long as they share at least
12 minutiae points, which is called the 12-point guideline
[26, 20]. Under this guideline, when |A| = 36, a ﬁnger-
print B of size 36 is considered as the same as A as long
as the size of their intersection (i.e.
their common ﬁnger-
print minutia points) is larger than a similarity threshold
t = 12, making set intersection an ideal metric. Under the
set difference metric, it would require an error-correcting
code to correct 24 errors in a 36-element set; however, no
such error-correcting code exists, as there are more errors
(i.e., 24) than correct elements (i.e., 12).
Socek et al. [32] proposed a fuzzy extractor based on
set intersection, which is called FESI hereafter. FESI is
based on threshold secret sharing schemes [31], not error-
correcting codes. In FESI, for a given reference template
A = {a1, a2, . . . , an}, where n is an integer and t ≤ n,
a random secret s is ﬁrst chosen and its t-out-of-n secret
shares (s1, s2, . . . , sn) are generated (see Section 3.1) [31].
Let h be a cryptographic hash function and fA(x) be a
discrete function such that fA(x) = si if x = ai and
fA(x) = ˆsi otherwise, where ˆsi is a random number (thus
very likely si (cid:6)= ˆsi). Γ = (cid:2)HA, y, FA(cid:3) is then stored on
the server as A(cid:2)
s biometric veriﬁcation data (BVD), where
HA = {h(sa1), h(sa2), . . . , h(san)}, y = h(s), FA =
fA(x).
i , ˜s2
(cid:3)
m
t
(cid:2)
(cid:3)
m
t
When a fresh biometric sample B = {b1, b2, . . . , bm}
(i.e., |B| = m, t ≤ m) is presented, the server takes the
following steps to verify its authenticity: for each t subset
Bi of B, 1 ≤ i ≤ (cid:2)
denotes the number of
where
t-combinations out of m, the server evaluates fA(Bi) to get
i}, which are then used as shares to
t values {˜s1
i , . . . , ˜st
reconstruct a secret value sBi [31]. Next, the server checks
) ?= y. If not, the next Bi is tried (hereafter,
whether h(sBi
the test of each Bi is called a try); otherwise, the server
= {h(sBi b1), h(sBi b2), . . . , h(sBi bm)} and
calculates HBi
= HA ∩ HBi, where ∩ denotes set intersection. If the
ΘBi
cardinality of ΘBi,|ΘBi
|, is not smaller than t, B is con-
sidered close to A and the client is authenticated. (After a
successful authentication, the reconstructed secret s can be
used for other security purposes such as being used as an
AES key.)
(cid:3)
. The expected
number of tries to ﬁnd a correct Bi from B is e = δ+1
λ+1
[32] (also see the Appendix of this paper). When t is a
small number (for example, t = 12), e is not a big number.
For example, when (cid:2)t = 12,|B| = 80, β = 48(cid:3), we have
e = 865.
Let β = |A ∩ B|, δ =
(cid:3)
, and λ =
(cid:2)|B|
t
(cid:2)
β
t
THE PROBLEM However, some biometric authentication
applications (such as the biometric authentication to a nu-
clear plant) tolerate little false matches and thus require
a high threshold value t. For example, (cid:2)t = 40,|B| =
80, β = 60(cid:3). In this case, e would be a big number:
= 25646754.927203, which
e = 107507208733336176461620
is about 2.56468 × 107.
4191844505805495
Exhausting this number in the veriﬁcation process would
take an unreasonably long time. (Assume that each try takes
6.12 milliseconds, which is the amount of time to recon-
struct a 128-bit secret with Shamir’s scheme on a 3.40 GHz
Intel Pentium 4 processor running Linux 2.6.9− 67.0.7, the
FESI would cost 1.57 × 105 seconds.)
OUR CONTRIBUTIONS In this paper, we improve FESI
in two ways. The ﬁrst improvement is computational and
addresses the above problem, while the second one is re-
lated to storage efﬁciency.
First, we observe that in those biometric authentication
applications that tolerate few false matches, biometrics typ-
ically have a high correct rate β|B| , as these applications may
employ more accurate biometric readers. Based on this ob-
servation, through integrating a Reed-Solomon decoding al-
gorithm in the veriﬁcation step, we can pick a much larger
subset of B in each try and as a result, signiﬁcantly reduce
the expected number of tries in verifying B. (We bring back
error-correcting codes but for different purpose and in dif-
ferent ways.)
For the above example parameter combination (cid:2)t =
40,|B| = 80, β = 60(cid:3), to verify a biometric sample B,
the computational improvement requires only 1 try on av-
erage. This try takes just 0.0684 second on the aforemen-
131141
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:00:38 UTC from IEEE Xplore.  Restrictions apply. 
tioned 3.40 GHz Intel Pentium 4 processor.
Second, we develop a continuous function to replace the
discrete function fA(x) of FESI. This replacement saves
storage space and makes FESI more practical.
The remainder of this paper is organized as follows. In
Section 2 we give related research. Section 3 describes the
building blocks for our improvements, namely, Shamir se-
cret sharing and Reed-Solomon decoding. In Section 4 we
present our improvements and analyze their performances.
Section 5 gives the details of a prototype implementation
and its running results. Section 6 discusses some parameter
selection issues. Concluding remarks are given in Section
7.
2 Related Work
Biometrics have been used for authentication and iden-
tiﬁcation for decades. The Federal Bureau of Investigation
(FBI) maintains a ﬁngerprint database with 55 millions of
records and is proposing to build a one-billion dollar com-
puter database of biometrics including palm prints, scars,
tattoos, iris patterns and facial shapes [33]. In addition to
ﬁngerprints, popular biometric identiﬁcation/authentication
techniques include speaker recognition [6] and facial au-
thentication [34].
The risk of storing biometric information on a central
server has been observed in several places [27, 28]. The
problem has been brought out for privacy concerns as com-
promised biometrics lead to privacy breach.
Ratha et al. [29] developed the idea of cancelable bio-
metrics and proposed to store distorted biometrics on a
server for authentication. These constructions are typically
restricted to a speciﬁc biometric type such as ﬁngerprint.
2.1 Other fuzzy extractors
Juels and Wattenberg [16] described an elegant fuzzy ex-
tractor scheme under the Hamming distance metric, where
a biometric sample is represented as a ﬁxed-size and ﬁxed-
order binary string. This scheme, called JW99 hereafter, is
based on error-correcting codes (ECC). To generate biomet-
ric veriﬁcation data from a biometric sample x, the server
ﬁrst picks a random codeword C and calculates v = (x⊕C)
and y = h(C), where ⊕ denotes bitwise exclusive OR and
h is a crypto hash function like SHA-192. (v, y) is the bio-
metric veriﬁcation data and is stored on the server. To ver-
(cid:2)
, the server ﬁrst calculates
ify a given biometric sample x
u = v ⊕ x
(cid:2)
and then applies the decoding function of the
(cid:2)
error-correcting code on u. If x and x
are close enough un-
der the Hamming distance metric, u can be corrected to C,
whose correctness can be veriﬁed by checking h(u) ?= y.
(The essence of this and other ECC-based schemes is to use
the error-correcting capability of ECC to tolerate small dif-
ferences between biometric samples.)
In the JW99 scheme, if the server were compromised and
(v, y) were stolen, since C is randomly picked, an attacker
would not be able to recover x or a close biometric sample.
This scheme is also secure against the multiple-use attack
(called chosen perturbation attack in [4]): a client may use
his biometrics in several applications with each server stor-
ing a set of (vi, yi); compromising multiple such servers to
obtain (vi, yi) does not give the attacker more useful infor-
mation about the client’s biometric x.
Although JW99 is conceptually simple, real-world bio-
metric samples are rarely ﬁxed-size and ﬁxed-order binary
string.
Juels and Sudan [14] developed a fuzzy vault scheme
based on the set difference metric. Dodis et al. [11] im-
proved it, developed the concepts of secure sketch and fuzzy
extractor, and proposed schemes under the Hamming dis-
tance metric, the aforementioned set difference metric, and
the edit distance metric, where biometric samples are rep-
resented as variable-size binary strings. In the edit distance
metric, the distance of two biometric samples is the number
of insertions/deletions needed to transform one to the other.
Other research along this line include [9, 10, 5].
3 Building blocks
Two building blocks, Shamir secret sharing and Reed-
Solomon decoding, are used in our improvements. Shamir
secret sharing is used in generating biometric veriﬁcation
data Γ while Reed-Solomon decoding is used in verifying a