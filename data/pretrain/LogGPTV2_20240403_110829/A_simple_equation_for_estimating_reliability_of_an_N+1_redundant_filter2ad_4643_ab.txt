as a servo block. Data can be written to a location with defective media just as any other data-write to a HDD. Operational failure of a spare will prevent reconstruction but otherwise not affect any data on the other HDDs in the RAID group. During reconstruction, drive idle-time is spent on reconstructing data on the spare HDD. The RAID group continues to process I/O commands and serve data. When I/O and reconstruction are simultaneous, the length of time to complete reconstruction is extended because I/O has a higher priority. During reconstruction, the entire HDD is reconstructed, even the blocks that contain no useful data.   3. Related work This section provides a high level summary of the MTTDL and Baker equations, and the Markov and MC-Sim computer analysis methods. MTTDL and Baker are the only two closed-form equations used to estimate RAID reliability. Markov models are rarely able to be written in a simple closed form, but are included for completeness. The Monte Carlo Simulation cannot be put into a simple equation either, but is highly accurate when compared to field data and is used as a reference-standard for the development of the DDF(t) equation.  3.1. MTTDL The MTTDL equation uses only the disk drive’s MTTF and MTTR to estimate the mean time between double-disk failures (data loss) for operational failures. It applies to any N+1 RAID group of arbitrary size [1, 6, 7, 8, 9, 10, 11]. Both the time-to-failure and time-to-restore distributions are assumed to be exponential, therefore representing memoryless processes having constant failure and restoration rates. Equation 1 shows the expression for MTTDL, in which the HDD failure rate is λ and the restoration rate is μ.    eq. 1   Since the restoration rate is usually much larger than the failure rate, the MTTDL can be simplified as in eq. 2. The assumption that the HDD times-to-failure and times-to-restoration are exponential distributions means the MTTF and MTTR are the reciprocals of the failure and restoration rates respectively, and eq. 2 can be rearranged to be eq. 3. The MTTDL calculation has two attributes that significantly affect its accuracy. First, MTTDL does not include latent defects or scrubbing. As will be shown later, this omits the majority of the failure combinations and results in an erroneously high estimate of MTTDL (or low estimate of the number of data losses).  eq. 2     eq. 3   The MTTDL equation relies heavily on the assumption that the means are the reciprocals of the rates. This relationship holds true at the system level only if the system failures, DDFs in our case, have a constant rate of occurrence, not the HDDs.  Even if every component in a system has a constant failure rate, there is no statistical basis for the assertion that the rate of occurrence of failure (ROCOF) for a repairable system (RAID group) is constant. Ascher [12] illustrates how a repairable system can have an increasing or decreasing rate of occurrence of failure even though the components in the system are all from populations with constant failure rates. Additional error comes from the fact that field data show that HDD failures rarely follow the exponential distribution [13], so the system cannot have a constant rate of failure. Detailed discussions of the statistical differences between a component failure rate and a system rate of occurrence of failure (ROCOF) are presented in [12, 14, 15, 16].  3.2. Baker equation [3] Baker et al. [3] developed an equation for mirrored disks that includes not only operational HDD failures (visible) and their repair, but also discovery of latent defects and their repair. They assume constant rates of occurrence for failures and repairs of operational failures and latent defects, thus introducing errors discussed in section 3.1. Further, they attempt to model temporal and spatial failure correlations using linear multipliers, α and β. The simplest form of their MTTDL equation that includes discovery and restoration of latent defects, and temporal and spatial correlations is shown as equation 4:     eq. 4 ()()2112λμλ+++=NNN    MTTDL() NN    MTTDLIndep21λμ+=()diskdiskIndepMTTRNNMTTF     MTTDL12+=)k)(MTTRMTTD(k)k(MTTRMTTFk LLVLLdLdLVVVOpLdββββα++++22978-1-4244-4421-2/09/$25.00 c(cid:13)2009 IEEE
487
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 09:56:46 UTC from IEEE Xplore.  Restrictions apply. 
where: α = temporal correlation factor βab = probability that a and b overlap spatially MTTFLd = mean time to fault from a latent defect MTTROp = mean time to restore operational failure MTTDLd = mean time to detect a latent defect k = MTTFOp/MTTFLd  This equation has a number of undesirable attributes, as follows: • It applies only to RAID groups of size 2. • Only βLL is easily known (zero is a good estimate). • Three of the βab probabilities are joint-probabilities that are interdependent on each other, not independent as used in the equation. These should be modeled as dependent events, not simple multipliers. • The temporal correlation factor, α, is intended to model conditionality of two events, a second fault given that first has occurred. This is better modeled by explicit conditional event probabilities. • The equation does not account for correlations that are a statistical result of non-constant failure rates. A population with a non-constant failure rate has correlated times-to-failure.  3.3. Markov models Markov process models have been used as an alternative to the MTTDL calculation and can include any number of additional states corresponding to conditions such as degraded operation or latent defects [17, 18, 19]. It is possible to write expressions (equations) for a Markov model in terms of transition matrices using Laplace transforms, but, generally, these are far too complicated to evaluate by hand. Even a simple 2-state model is cumbersome and the effort to solve a model with more than four states is so difficult it is usually evaluated using a computer program. While Markov models have some benefits over MTTDL, the results don't agree well with the field data due to several statistical approximations inherent to the Markov calculation. For example, the underlying processes governing HDD failure and repairs must follow a homogeneous Poisson process (HPP) in a Markov model. This means component events (failures or repairs) occur randomly and have constant rates of occurrence.  3.4. Monte Carlo simulation The model developed in [4] is the most complete and accurate N+1 RAID model available today, but it is solved using sequential Monte Carlo simulation, not a simple equation. Because of its accuracy, the results form the basis for comparing other models. The Monte Carlo simulation requires fewer assumptions than we need to develop the DDF(t) equation. In particular, MC-Sim has the following features that DDF(t) does not: • All distributions for both failure and restoration can have rates that change as a function of time. • Failure event combinations can be restricted. Two latent defects do not cause a failure. Two latent defects in the same stripe of data on different disks constitute a loss of data, but the probability of this is negligible. • Results (section 6) estimate the time-dependent, cumulative number of data loss events directly. • The model accounts for the order in which events occur. A latent defect that occurs during reconstruction of an operational failure does not count as a DDF.  • Each of the N+1 HDDs can have a different input distribution.  The MC-Sim assumes the underlying processes governing failures and restorations for both components and the system are non-Homogeneous Poisson processes (non-HPP). The non-HPP model in [4] allows all rates, failure and restoration at the component and system level, to be time dependent. The Monte Carlo simulation samples times for HDD failures and restorations and creates a chronological sequence of events over a 10 year mission time. As this is being done, the combinations of operational failures and latent defects are monitored to see when they occur simultaneously. When their durations of unavailability overlap according to the DDF definition in Section 2, then a system (RAID group) failure occurs. The time of the failure is tracked and the duration of the outage is recorded.   4. A new RAID math: DDF(t) Repairable system models often assume constant failure and restoration rates and are solved using standard renewal theory for homogeneous Poisson processes. RAID is more accurately modeled as a non-homogeneous Poisson process (NHPP) since the operational failure distribution and the two restoration distributions do not have constant rates of occurrence. There are no closed form solutions for NHPP systems as can be seen from the discussions in [12, 14, 15, 16]. A reasonably accurate equation is not easy to derive and must violate certain statistical principles. The main 978-1-4244-4421-2/09/$25.00 c(cid:13)2009 IEEE
488
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 09:56:46 UTC from IEEE Xplore.  Restrictions apply. 
question is how the accuracy is affected by any simplifications or approximations.  4.1. A better reliability metric It is difficult to put a good physical meaning on a "mean value", such as the mean time to data loss.  Knowing the mean value without knowing the underlying distribution doesn't tell you how many failures you might expect as a function of system age. For example, the two distributions in Figure 1 have the same mean value and the same average rate of occurrence (10 per unit-time), but plots of the cumulative number of expected failures as a function of time in Figure 2 are greatly different.                 Figure 1. Two distributions with the same mean value                 Figure 2. Cumulative expected failures for two distributions with the same mean value   We do not know the shape of the distribution for time-to-data loss in an N+1 RAID group. If we assume the rate of occurrence is constant, we don't know if our estimates are higher or lower than reality, just as shown in the example of Figure 2. Even a statement about conservatism or optimism depends on where you are on the Time units axis. Because of the difficulty in understanding the implications of a mean time to data loss, a better metric for reliability of RAID groups is the number of data loss events as a function of time. This metric will produce a plot similar to those in Figure 2. Since, for N+1 RAID, data loss events require defects or operational failures on multiple HDDs, we will refer to them as double disk failures (DDFs). For MTTDL, the times between failures are assumed to follow a homogeneous Poisson process and have a constant rate of occurrence. The expected number of DDFs as a function of time based on the MTTDL, DDF(t)MTTDL, is calculated from the number of RAID groups, RG, and the approximate probability of failing. So, DDF(t)MTTDL = RGλRGt. where λRG refers to the rate of occurrence of failure for the RAID group, not the HDD. This will produce a straight line, similar to the mean line in Figure 2.   4.2. The DDF(t) equation The DDF(t) equation produces a frequency of failure, a probability density function that can be used to estimate cumulative numbers of failures as a function of time.  First, I present the equation and then I explain the terms and factors of the equation. The equation is as follows:   eq. 5   All HDDs are at risk for latent defects and operational failures. As operational failures occur and are restored, steady-state availability due to operational failures, AOp, results. Similarly, all HDDs are experiencing latent defects and having them removed through scrubbing, rendering a steady-state availability for latent defects, ALd. The availability of a single HDD is the product of the operational and latent defect availabilities, AOpALd, and the availability for all N+1 HDDs in the RAID group is (AOpALd)(N+1). A DDF can only occur when any one HDD in the RAID group is unavailable, the complement of availability, or 1-(AOpALd)(N+1). Steady-state availability for a system composed of a single replaceable component, regardless of the nature of the distribution of failure times and repair times [20], can be estimated using:  ()()[]()OpNLdOptHN   AAtDDF⋅⋅−=+11051015202512345678910Time unitsNumber of failures per time-unitV-shapedPeaked02040608010012001234567891011Time unitsCumulative number of failuresV-shapedPeakedMean978-1-4244-4421-2/09/$25.00 c(cid:13)2009 IEEE
489
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 09:56:46 UTC from IEEE Xplore.  Restrictions apply. 
 eq. 6   where, MTTF = mean time to (component) failure MTTR = mean time to (component) restoration   The MTTF can be calculated for any distribution, but is the reciprocal of the failure rate only for the exponential distribution. For a 3-parameter Weibull distribution, the MTTF is as follows:   eq. 7   where, Γ(z) = gamma function of z.  The value of β is used to calculate the dummy variable n using n = 1+(1/β). Then Γ(n) is determined from software such as Weibull++ [21] or other internet based Gamma function calculator [22]. Parameters γ and η are from the Weibull distribution. Gamma function values for several common β values are shown in Table 2 for convenience.   Table 2. Gamma function values for common Weibull shape parameters         For the operational failures and latent defects, the subscripts "Op" and "Ld" are added as follows:   eq. 8    eq. 9   The rate at which a second, concurrent, operational failure occurs is based on the hazard rate, which expresses failure rate as a function of time and can be time dependent. For exponentially distributed failures, the hazard rate is equivalent to the failure rate. Field data has shown that HDD failure rates are usually time dependent, so a better model for HDD failures is the 3-parameter Weibull distribution. The hazard rate for any distribution is based on the probability density function, f(t), and the cumulative failure function , F(t). Consider the Weibull function which has a probability density function of the following form:   eq. 10   where, γ = location parameter η = the characteristic life β = the shape factor.   Integration of f(t) from 0 to t results in the Weibull cumulative distribution function as follows:   eq. 11   The cumulative hazard rate, H(t), is as follows for the Weibull distribution:   eq. 12   The cumulative hazard rate is the model for the time dependent process by which failures accumulate. This is the term used in the equation for the second operational failure. Since any of the remaining HDDs, after one is already unavailable, are subject to operational failure, the multiplier is N, not N+1. When substitutions are performed, the whole DDF(t) equation is as follows:        eq. 13   After the inputs are determined, this equation is readily evaluated using a spreadsheet. OpOpNOpOpOpLdLdLdtNMTTRMTTFMTTFxMTTRMTTFMTTF)t(DDFβη⎟⎟⎠⎞⎜⎜⎝⎛⋅⋅⎥⎥⎦⎤⎢⎢⎣⎡⎟⎟⎠⎞⎜⎜⎝⎛++−=+11()⎥⎥⎦⎤⎢⎢⎣⎡⎟⎟⎠⎞⎜⎜⎝⎛−−⎟⎟⎠⎞⎜⎜⎝⎛−⎟⎟⎠⎞⎜⎜⎝⎛=−ββηγηγηβtexpttf1()⎥⎥⎦⎤⎢⎢⎣⎡⎟⎟⎠⎞⎜⎜⎝⎛−−−=βηγtexptF1()βηγ⎟⎟⎠⎞⎜⎜⎝⎛−=ttHMTTRMTTFMTTFA+=⎟⎟⎠⎞⎜⎜⎝⎛+Γ⋅+=11βηγMTTFScrubLdLdLdMTTRMTTFMTTFA+=OpOpOpOpMTTRMTTFMTTFA+=βnΓ(n)1.002.001.00001.121.890.95842.001.500.88622.501.400.88733.001.330.8930978-1-4244-4421-2/09/$25.00 c(cid:13)2009 IEEE
490
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 09:56:46 UTC from IEEE Xplore.  Restrictions apply. 