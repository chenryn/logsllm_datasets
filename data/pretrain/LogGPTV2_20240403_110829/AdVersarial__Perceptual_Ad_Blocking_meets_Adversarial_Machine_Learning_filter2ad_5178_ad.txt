which adversaries can execute it, and which ad-blockers it
applies to (fully:(cid:32) or partially:(cid:71)(cid:35)).
Goals Actors Target
Strategy
D1: Data Training Poisoning
S1: DOM Obfuscation
S2: Resource Exhaustion (over-Segmentation)
C1: Evasion with Adversarial Ad-Disclosures
C2: Evasion with Adversarial Ads
C3: Evasion with Adversarial Content
C4: Detection with Adversarial Honeypots
A1: Cross-Boundary Blocking
A2: Cross-Origin Web Requests
(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32) (cid:32)
(cid:32)(cid:32)(cid:35) (cid:32)(cid:32)(cid:35)(cid:35) (cid:32)(cid:71)(cid:35) (cid:35)
(cid:32)(cid:35)(cid:35) (cid:32)(cid:32)(cid:71)(cid:35)(cid:71)(cid:35) (cid:32)(cid:71)(cid:35) (cid:35)
(cid:32)(cid:35)(cid:35) (cid:35)(cid:32)(cid:35)(cid:35) (cid:32)(cid:35) (cid:35)
(cid:32)(cid:35)(cid:35) (cid:35)(cid:32)(cid:32)(cid:35) (cid:35)(cid:32) (cid:32)
(cid:32)(cid:35)(cid:35) (cid:32)(cid:35)(cid:35)(cid:32) (cid:35)(cid:35) (cid:32)
(cid:35)(cid:32)(cid:35) (cid:32)(cid:35)(cid:35)(cid:35) (cid:32)(cid:32) (cid:32)
(cid:35)(cid:35)(cid:32) (cid:35)(cid:35)(cid:32)(cid:32) (cid:35)(cid:35) (cid:32)
(cid:35)(cid:35)(cid:32) (cid:35)(cid:35)(cid:32)(cid:32) (cid:32)(cid:35) (cid:35)
near-uniform rectangular blocks that blend into the page’s back-
ground yet falsely trigger the ad-detector.
We assume the publisher controls the page’s HTML and CSS,
but cannot access the content of ad frames. This content, including
the AdChoices logo, is added by the ad network.
Gilmer et al. [31] argue that the typical setting of adversarial
examples, where the adversary is restricted to finding imperceptible
perturbations for given inputs, is often unrepresentative of actual se-
curity threats. Interestingly, the threat model for visual ad classifiers
does align perfectly with this setting. The ad-blocker’s adversaries
want to evade its classifier for a specific input (e.g., the publisher’s
current web page and an advertiser’s latest ad campaign), while
ensuring that the users’ browsing experience is unaffected.
4.2.1 Overview of Attack Techniques and Results. For all seven ad-
classifiers, we craft imperceptible adversarial perturbations for ad-
disclosures, ads and other web content, which can be used by pub-
lishers, ad-networks, or advertisers to evade or detect ad-blocking.
Some of our classifiers can be attacked using existing techniques.
For example, we show that ad-networks and publishers can use
standard gradient-based attacks (see Section 2.4) to create imper-
ceptibly perturbed ads or background content that fool our two
frame-based classifiers with 100% success rates (see Figure 6). We
verify that similar attacks bypass the model used in Percival [84].
Attacking element-based classifiers is less straightforward, as
they operate on small images (adversarial examples are presumed
to be a consequence of high dimensional data [32]), and some rely
on traditional computer vision algorithms (e.g., average hashing or
SIFT) for which gradient-based attacks do not apply. Nevertheless,
we succeed in creating virtually invisible perturbations for the
AdChoices logo, or background honeypot elements, that fool these
classifiers (see Figure 5). Our attacks on Ad-Highlighter’s OCR
network build upon prior work by Song and Shmatikov [79]. For
non-parametric algorithms such as SIFT, we propose a new generic
attack using black-box optimization [41, 73] (see Section 4.2.2), that
is conceptually simpler than previous attacks [39].
Our most interesting attacks are those that target page-based
ad-blockers such as Sentinel [10] (see Figure 8, as well as Figure 14).
Our attacks let publishers create perturbed web content to evade
or detect ad-blocking, and let ad-networks perturb ads that evade
ad-blocking on the multitude of websites that they are deployed in.
These attacks overcome a series of novel constraints.
First, attacks on visual ML classifiers often assume that the adver-
sary controls the full digital image fed to the classifier. This is not
the case for page-based ad-blockers, whose input is a screenshot of
a web document with content controlled by different actors (e.g., ad
networks only control the content of ad frames, while publishers
can make arbitrary website changes but cannot alter ads loaded
in cross-origin iframes). Moreover, neither actor precisely knows
what content the other actors will provide. Adversarial examples for
page-based ad-blockers thus need to be encoded into the HTML ele-
ments that the adversary controls, and must be robust to variations
in other page content. We solve this constraint with techniques
similar to those used to make physical-world adversarial examples
robust to random transformations [28, 48, 75]. We consider multiple
tricks to encode a publisher’s perturbations into valid HTML One
attack uses CSS rules to overlay a near-transparent perturbed mask
over the full page (Figure 8 (b)). To detect ad-blocking, we craft an
innocuous page-footer that triggers the ad-blocker (Figure 8 (d)).
Details on our attacks are in Section 4.2.2.
A further challenge is the deployment of these attacks at scale,
as creating perturbations for every ad and website is intractable.
This challenge is exactly addressed by attacks that create universal
adversarial examples [56]—single perturbations that are crafted so
as to be effective when applied to most classifier inputs. Universal
perturbations were originally presented as a curious consequence
of the geometry of ML classifiers [56], and their usefulness for the
scalability of attacks had not yet been suggested.
Attacks on page-based ad-blockers have unique constraints, but
also enable unique exploits. Indeed, as a page-based classifier pro-
duces outputs based on a single full-page input, perturbing content
controlled by the attacker can also affect the classifier’s outputs
on unperturbed page regions. The effectiveness of such attacks
depends on the classifier. For the YOLOv3 [72] architecture, we
show that publishers can perturb website content near ad iframes
so as to fool the classifier into missing the actual ads (see Figure 14).
4.2.2 Algorithms for Adversarial Examples. For some of the consid-
ered classifiers, adversarial examples for each of the attack strategies
C1-C4 in Table 2 can be constructed using existing and well-known
techniques (see Section 2.4). Below, we provide more details on the
attack we use to target SIFT, and on the techniques we use to create
robust and scalable attacks for page-based classifiers [10].
Black-box optimization attacks for non-parametric classifiers. SIFT
is a non-parametric algorithm (i.e., with no learned parameters). As
such, the standard approach for generating adversarial examples by
minimizing the model’s training-loss function does not apply [83].
To remedy this, we first formulate a near-continuous loss function
LSIFT (x + δ ) that acts as a proxy for SIFT’s similarity measure
between the perturbed image x + δ and some fixed template. The
next difficulty is that this loss function is hard to differentiate, so we
use black-box optimization techniques [41, 73] to minimize LSIFT.
Session 9B: ML Security IIICCS ’19, November 11–15, 2019, London, United Kingdom2012LSIFT (x + δ ) (cid:66)(cid:88)
SIFT’s output is a variable-sized set of keypoints, where each
keypoint is a vector v ∈ R132—four positional values, and a 128-
dimensional descriptor [52]. Let t be a template with keypoint de-
scriptorsT . To match an image x against t, SIFT computes descriptor
vectors for x, denoted {v1, . . . ,vm}. Then, for each vi it finds the
distances di,1,di,2 to its two nearest neighbors in T . The keypoint
vi is a match if the ratio test di,1/di,2  0:
LFN
max (conf( f (x ⊙ δ ),b) − (τ − κ),0) , (2)
YOLO(x ⊙δ ) (cid:66) (cid:88)
YOLO(x ⊙ δ ) (cid:66) (cid:88)
1≤b≤B
1≤b≤B
For false-positives, i.e., a fake object prediction, we instead in-
crease all boxes’ confidence up to τ + κ by minimizing:
LFP
max (τ + κ − conf( f (x ⊙ δ ),b),0) .
(3)
4.2.3 Evaluation of Attacks. We now instantiate and evaluate the
attack strategies C1-C4 from Table 2 on our seven ad-classifiers
Attack C1: Evasion with adversarial ad-disclosures. Figure 5 shows
examples of perturbed AdChoices logos that fool all element-based
classifiers. An ad-network can use these to evade ad-blocking.
Average hashing is invariant to small ℓp noise, but this comes at
the cost of high sensitivity to other perturbations: we evade it by
adding up to 3 transparent rows and columns to the logo. When
overlaid on an ad, the rendered content is identical.
Original
Avg. Hash
OCR
SIFT
False Positives:
Figure 5: Adversarial Examples for Element-Based Classi-
fiers. These correspond to attacks (C1) and (C4) in Table 2.
Original
False Negative
False Positive
Figure 6: Adversarial Examples for Frame-based Classifiers.
These are attacks (C2) and (C4) in Table 2. Top: Attacks on
our YOLOv3 model that detects the AdChoices logo. Bottom:
attacks on the ad-classifier from [40] (we crafted similar ad-
versarial examples for the classifier used in Percival [84])
Adversarial examples for OCR bear similarities to CAPTCHAs.
As ML models can solve CAPTCHAs [15, 94], one may wonder
why transcribing ad disclosures is harder. The difference lies in
the stronger threat model that ad-blockers face. Indeed, CAPTCHA
creators have no access to the ML models they aim to fool, and must
thus craft universally hard perturbations. Attacking an ad-blocker
is much easier as its internal model must be public. Moreover the
ad-blocker must also prevent false positives—which CAPTCHA
solvers do not need to consider—and operate under stricter real-
time constraints on consumer hardware.
Attack C2: Evasion with adversarial ads. Ad networks can di-
rectly perturb the ads they server to evade frame or page-based
ad-blockers. For frame-based classifiers, the attacks are very sim-
ple and succeed with 100% probability (see Figure 6). We verified
that the ad-classifier used by Percival [84] is vulnerable to similar
attacks. Specifically, we create a valid HTML page containing two
images—an ad and an opaque white box—which are both misclassi-
fied when the page is rendered in Percival’s modified Chromium
browser (see Figure 13).
For our page-based model, crafting a “doubly-universal” pertur-
bation that works for all ads on all websites is hard (this is due to
the model’s reliance on page layout for detecting ads, see Appen-
dix B for details). Instead, we show that an ad-network can create a
universal perturbation that works with 100% success rate for all ads
that it serves on a specific domain (see Figure 14). For this attack,
we minimized the LFN
YOLO loss over the collected screenshots in
train
BBC , by applying the same perturbation δ over all ad frames.
D
Session 9B: ML Security IIICCS ’19, November 11–15, 2019, London, United Kingdom2013(a) Original Page: two ads are detected.
(b) Attack C3 (Universal): The publisher overlays a transparent mask
over the full page to evade the ad-blocker.
(c) Attack C3 (Universal): The publisher overlays a mask on the page
to generate unreasonably large boxes and disable the ad-blocker.
Figure 8: Universal Adversarial Examples for Page-Based Ad-Blockers. Displays examples of universal evasion attacks (C3)
and detection attacks (C4) on a page from theguardian.com. Best viewed with 2x zoom in.
(d) Attack C4 (Universal): The publisher adds an opaque footer to de-
tect an ad-blockers that blocks the honeypot element (bottom-left).
#overlay {
background-image : url (" data : image / png ; base64,... ");
width : 100% ; height : 100% ; top : 0; left : 0;
position : fixed ; z-index : 10000 ; pointer-events : none ;
opacity : 0.01 ; }
Figure 7: Code for Attack C4-U. An adversarial mask is tiled
over the full page with a small opacity factor.
Attack C3: Evasion with adversarial content. These attacks ap-
ply to page-based ad-blockers and allow publishers to evade ad-
blocking while only perturbing HTML elements that they control
(which crucially does not include the content of ad-frames). We
show that a publisher can actually perturb the full screenshot image
fed into the classifier using CSS techniques. The HTML perturbation
is a near-transparent mask, that is overlaid on the entire web page
(see Figure 7). The CSS properties z-index and pointer-events
are used to display the mask over all other web content, but allow
underlying elements to still be accessed and clicked normally.
Adding a mask over the full image is prohibitive, as the mask
would be large and tied to a fixed resolution. We thus build a smaller
mask and tile it over the full page. We generate a universal adversar-
ial mask δ of 20KB by minimizing LFN
train. The overlaid
mask evades detection of all ads in our evaluation set (see Figure 8,
(b)). This attack can be deployed by any publisher, to evade all ads.
The perturbation mask is robust to scrolling and viewport changes
when tested in Google Chrome.
Figure 8 (c) shows a similar attack that overloads the ad-blocker.
The transparent mask is crafted to minimize LFN
train, and
creates many incorrect predictions that coerce the ad-blocker into
YOLO over D
YOLO over D
abdicating or breaking the site. On all websites, the mask causes
the model to detect abnormally large ads or fail to detect real ads.
These attacks are powerful and can be re-used by any publisher.
Yet, ad-blockers might try to detect certain CSS tricks and disable
them. We thus also propose stealthier attacks tuned to a single
domain. For pages on BBC.com, we create a small perturbation
(40 × 1020 px) that is applied to the white background right below
an ad frame (see Figure 14(b)) and that universally applies to all
pages from that publisher that use a similar layout.
Attack C4: Detection with adversarial honeypots. To detect ad-
blocking, publishers can use honeypots that falsely trigger ad-
blockers [95]. The false positives in Figures 5-6 are innocuous
elements that are falsely classified as ads or ad-disclosures. For
OCR and the model of Hussain et al. [40], generating near-opaque
black elements worked best. As average hashing is invariant to
changes in image intensity, creating false positives for it is trivial.
For page-based ad-blockers, our first attack embeds a perturba-