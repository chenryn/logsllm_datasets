### Revised Text

#### Introduction and Context
The work discussed in this section primarily concerns unlabeled graphs where nodes may have attribute values. In contrast, our focus is on labeled bipartite graphs, with the possibility of edge labels being used as part of the adversary's knowledge. As noted, their solution is only "applicable in situations where it is considered safe to publish the unlabeled graph." Our privacy model does not make this assumption. Additionally, our utility goals differ: they aim to preserve the accuracy of SQL aggregate queries on the released dataset, while we measure success by prediction accuracy.

#### Anonymization Approach
Ghinita et al. [13] proposed an elegant anonymization approach for handling sparse unlabeled bipartite graphs by capturing underlying data correlations. However, their model requires a universal set of sensitive items and allows only other items to be used as quasi-identifiers. In comparison, our model allows any item to be sensitive, and any item can be used to re-identify users. The presence of edge labels in recommendation datasets introduces additional challenges, necessitating different privacy and attack models, and new anonymization approaches. Their utility measure is based on aggregate query results, rather than prediction accuracy.

#### Privacy-Preserving Collaborative Filtering
Canny [7, 8] proposes two schemes for privacy-preserving collaborative filtering, where a community of users computes a public aggregate of their ratings without exposing individual ratings. These solutions involve homomorphic encryption mechanisms. Similarly, Hsieh et al. [18] also use an encryption-based approach. Polat et al. [27] consider the same problem in a centralized framework, where users send their data to a central server for collaborative filtering. Instead of encryption, they propose that users randomize their private ratings so that the central server cannot derive the true ratings but can still compute the collaborative filtering result from the perturbed data.

It is important to note that we do not claim to offer a new collaborative filtering algorithm. Rather, we provide an anonymization technique that produces anonymized datasets on which any collaborative filtering algorithm could be performed. Each of the above solutions applies only to a specific collaborative filtering method and thus could not be used to achieve the desired goals of this paper.

#### Conclusions and Future Work
In this paper, we report our efforts towards anonymizing the Netflix Prize dataset, a task made challenging by the Narayanan-Shmatikov attack [26]. Our techniques and results have general applications in anonymizing other sparse bipartite recommendation data. We focus on padding as a pre-processing step to reduce data sparsity during anonymization. Our proposed approach, called predictive anonymization, uses SVD as a concrete padding technique before anonymization, though our methodology can be applied with other padding algorithms. We formally defined the model and developed a practical and efficient anonymization algorithm called Predictive Anonymization.

Our studies using the Netflix Prize dataset evaluate the effectiveness of our algorithm in preserving the utility of the anonymized data. Padded-anonymized data provides excellent privacy and low prediction errors but has low data authenticity due to the padded values in the released dataset. In comparison, pure-anonymized data has improved data authenticity but yields high prediction errors. Our study experimentally illustrates the trade-offs between data utility/authenticity and privacy in anonymization. Padding is a useful pre-processing step for eliminating data sparsity during data anonymization, particularly for finding and grouping similar users, as demonstrated.

For future work, we plan to take a different approach to improve the utility of the pure anonymization method. Instead of averaging values in the homogenization step, we will permute the rating values for each item within each anonymization group. This may better preserve some characteristics of the data, but its impact on prediction accuracy and other utility measures remains to be seen. We will conduct extensive experiments with various parameters to investigate the effectiveness of this approach in achieving our privacy and utility goals.

#### References
[References are listed as provided, with no changes needed.]

#### Appendix
**Theorem 5.2**
*Proof.* Suppose an adversary, using a label-based attack, is able to identify the anonymization group containing user \( u \). Based on the existence of a link to item \( o \) in the released review graph, the adversary would like to infer whether user \( u \) gave a rating for \( o \) in the original dataset. However, the existence of the link in the anonymized graph only implies that at least one user in that anonymization group had rated \( o \). With no additional prior knowledge, the adversary can only infer that user \( u \) had rated \( o \) with probability at least \( \frac{1}{k} \).

**Theorem 5.3**
*Proof.* Suppose an adversary, using a label-based attack, is able to identify the anonymization group containing user \( u \). Based on the existence of a link to item \( o \) in the released review graph, the adversary would like to infer whether user \( u \) gave a rating for \( o \) in the original dataset. Let \( P(u, o) \) denote the probability that user \( u \) rated item \( o \) in the original dataset, and let \( P(C, o) \) be the unconditional probability that at least one user in anonymization group \( C \) rated \( o \). We wish to calculate \( P((u, o) | (C, o)) \), the probability that user \( u \) rated item \( o \), given that the edge exists in the released anonymized review graph.

By Bayes' Rule, we have:
\[ P((u, o) | (C, o)) = \frac{P((C, o) | (u, o)) \cdot P(u, o)}{P(C, o)}. \]

First, note that \( P((C, o) | (u, o)) = 1 \) directly from our anonymization procedure, and we also have that \( P(u, o) = p \). Furthermore, since we have assumed that each user has rated item \( o \) independently with probability \( p \), we can find a bound on \( P(C, o) \) as follows:
\[ P(C, o) = 1 - (1 - p)^k = 1 - (1 - kp + \frac{k(k-1)}{2}p^2 - o(p^3)) = kp - \frac{k(k-1)}{2}p^2 + o(p^3) \geq kp - \frac{k(k-1)}{2}p^2. \]

Combining these results, we get:
\[ P((u, o) | (C, o)) \leq \frac{p}{kp - \frac{k(k-1)}{2}p^2} = \frac{1}{k - \frac{k(k-1)}{2}p} \approx \frac{1}{k}. \]

Thus, the probability that user \( u \) rated item \( o \), given that the edge exists in the released anonymized review graph, is at most \( \frac{1}{k} \).