However, their work concerns unlabeled graphs where nodes
may have attribute values; on the other hand, we consider
labeled bipartite graphs, with the possibility of edge labels
being used as part of the adversary knowledge. As they
state, their solution is only “applicable in situations where it
is considered safe to publish the unlabeled graph.” Our pri-
vacy model does not make this assumption. In addition, we
have diﬀerent target utility goals: they aim to preserve the
accuracy of SQL aggregate queries on the released dataset,
whereas we measure success by prediction accuracy.
An elegant anonymization approach was proposed by Gh-
inita et. al. to handle sparse unlabeled bipartite graphs by
capturing underlying data correlations [13]. However, their
model requires a universal set of sensitive items, and allows
only the other items to be used as quasi-identiﬁers. In com-
parison, our model allows that any item may be sensitive,
and furthermore, any item may be used to re-identify users.
The existence of edge labels in recommendation datasets
introduces additional challenges, and thus demands diﬀer-
ent privacy and attack models and new anonymization ap-
proaches. Also, their utility measure is based on aggregate
query results, rather than prediction accuracy.
Privacy-preserving collaborative ﬁltering Canny
proposes two schemes for privacy-preserving collaborative
ﬁltering [7, 8] in which a community of users compute a
public aggregate of their ratings without exposing any in-
dividual users’ ratings. Their solutions involve a homomor-
phic encryption mechanism. In a similar fashion, Hsieh et.
al. [18] also take an encryption-based approach. Polat et.
al. consider the same problem under a centralized frame-
work [27], in which users send their data to a central server
that will conduct the collaborative ﬁltering. Instead of en-
cryption, they propose for users to randomize their private
ratings such that the center server cannot derive the truthful
ratings, but will still be able to compute the collaborative
ﬁltering result from the perturbed data.
It is important to note that we do not claim to oﬀer
a new collaborative ﬁltering algorithm, but rather to pro-
vide an anonymization technique that produces anonymized
datasets on which any collaborative ﬁltering algorithm could
be performed. Each of the above solutions apply only to a
particular collaborative ﬁltering method, and thus could not
be used to achieve the desired goals of this paper.
9. CONCLUSIONS AND FUTURE WORK
In this paper, we report our eﬀorts towards anonymiz-
ing Netﬂix Prize dataset, the diﬃculties of which are well
publicized due to the Narayanan-Shmatikov attack [26].
Our techniques and results have general applications in
anonymizing other sparse bipartite recommendation data.
We focus on padding as a pre-processing step to reduce data
sparsity during anonymization. Our proposed approach is
called predictive anonymization. We gave SVD as a concrete
padding technique before anonymization, but our methodol-
ogy can be applied with other padding algorithms. We for-
mally deﬁned the model and developed a practical and eﬃ-
cient anonymization algorithm called Predictive Anonymiza-
tion.
Our studies using the Netﬂix Prize dataset to evaluate
the eﬀectiveness of our algorithm in preserving utility of the
anonymized data. Padded-anonymized data gives excellent
privacy and low prediction errors, however, the data authen-
ticity is low due to the padded values in the released dataset.
In comparison, pure-anonymized data has improved data
authenticity, but yields high prediction errors. Our study
experimentally illustrates the tradeoﬀs between data util-
ity/authenticity and privacy in anonymization. Padding is a
useful pre-processing step for eliminating data sparsity dur-
ing data anonymization, in particular for ﬁnding and group-
ing similar users as we demonstrated.
For future work, we are planning to take a diﬀerent ap-
proach to improve the utility of the pure anonymization
method. Instead of averaging values in the homogenization
step, we could permute the rating values for each item within
each anonymization group. This may better preserve some
characteristics of the data, but it remains to be seen how
this approach will aﬀect prediction accuracy and other util-
ity measures. We will conduct extensive experiments with
various parameters to investigate the eﬀectiveness of this
approach in achieving our privacy and utility goals.
10. REFERENCES
[1] G. Aggarwal, T. Feder, K. Kenthapadi, S. Khuller,
R. Panigrahy, D. Thomas, and A. Zhu. Achieving
anonymity via clustering. In Proceedings of the
Symposium on Principles of Database Systems
(PODS), pages 153–162, 2006.
[2] K. Ali and W. van Stam. Tivo: making show
recommendations using a distributed collaborative
ﬁltering architecture. In Proceedings of the ACM
international conference on Knowledge Discovery and
Data Mining (KDD), 2004.
[3] L. Backstrom, C. Dwork, and J. Kleinberg. Wherefore
art thou r3579x?: anonymized social networks, hidden
patterns, and structural steganography. In Proceedings
of the International Conference on World Wide Web
(WWW), 2007.
[4] P. S. Bradley, K. Bennett, and A. Demiriz.
Constrained k-means clustering. Technical Report
MSR-TR-2000-65, Microsoft research, 2000.
[5] J. S. Breese, D. Heckerman, and C. Kadie. Empirical
analysis of predictive algorithms for collaborative
ﬁltering. Technical Report MSR-TR-98-12, Microsoft
Research, 1998.
Conference on Knowledge Discovery and Data Mining
(KDD), pages 279–288, 2002.
[20] D. Kifer and J. Gehrke. Injecting utility into
anonymized datasets. In Proceedings of the ACM
SIGMOD International Conference on Management of
Data, 2006.
[21] S. P. Lloyd. Least squares quantization in pcm. IEEE
[6] J.-W. Byun, A. Kamra, E. Bertino, and N. Li.
Transactions on Information Theory, 1982.
Eﬃcient k-anonymization using clustering techniques.
In Proceedings of the International Conference on
Database Systems for Advanced Applications
(DASFAA), 2007.
[7] J. Canny. Collaborative ﬁltering with privacy. In
Proceedings of the IEEE Symposium on Security and
Privacy, May 2002.
[8] J. Canny. Collaborative ﬁltering with privacy via
factor analysis. In Proceedings of the International
ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR),
August 2002.
[9] G. Cormode, D. Srivastava, T. Yu, and Q. Zhang.
Anonymizing bipartite graph data using safe
groupings. In Proceedings of the International
Conference on Very Large Data Bases (VLDB), 2008.
[10] C. Dwork. Diﬀerential privacy. In Proceedings of the
International Colloquium on Automata, Languages
and Programming (ICALP), 2006.
[11] C. Dwork, F. Mcsherry, K. Nissim, and A. Smith.
Calibrating noise to sensitivity in private data
analysis. In Proceedings of the Theory of Cryptography
Conference (TCC), 2006.
[12] S. Funk. Netﬂix update: Try this at home.
http://sifter.org/∼simon/journal/20061211.html,
2006.
[13] G. Ghinita, Y. Tao, and P. Kalnis. On the
anonymization of sparse high-dimensional data. In
Proceedings of the International Conference on Data
Engineering (ICDE), pages 715–724, 2008.
[14] D. Goldberg, D. Nichols, B. M. Oki, and D. Terry.
Using collaborative ﬁltering to weave an information
tapestry. Communications of the ACM, 35(12):61–70,
1992.
[15] K. Y. Goldberg, T. Roeder, D. Gupta, and C. Perkins.
Eigentaste: A constant time collaborative ﬁltering
algorithm. Journal of Information Retrieval,
4(2):133–151, 2001.
[16] N. Good, J. B. Schafer, J. A. Konstan, A. Borchers,
B. Sarwar, J. Herlocker, and J. Riedl. Combining
collaborative ﬁltering with personal agents for better
recommendations. In Proceedings of the National
Conference on Artiﬁcial Intelligence (AAAI), 1999.
[17] M. Hay, G. Miklau, D. Jensen, D. Towsley, and
P. Weis. Resisting structural identiﬁcation in
anonymized social networks. In Proceedings of the
International Conference on Very Large Data Bases
(VLDB), 2008.
[18] C.-L. Hsieh, J. Zhan, D. Zeng, and F. Wang.
Preserving privacy in joining recommender systems. In
Proceedings of the International Conference on
Information Security and Assurance (ISA), 2008.
[19] V. S. Iyengar. Transforming data to satisfy privacy
constraints. In Proceedings of the ACM International
[22] A. Machanavajjhala, J. Gehrke, D. Kifer, and
M. Venkitasubramaniam. l-diversity: Privacy beyond
k-anonymity. In Proceedings of the International
Conference on Data Engineering (ICDE), 2006.
[23] J. MacQueen. Some methods for classiﬁcation and
analysis of multivariate observation. In Proceedings of
the Berkeley Symposium on Mathematical Statistics
and Probability, 1967.
[24] F. Mcsherry. Mechanism design via diﬀerential
privacy. In Proceedings of the Annual Symposium on
Foundations of Computer Science (FOCS), 2007.
[25] B. Meyer. Netﬂix recommender framework.
http://benjamin-meyer.blogspot.com/2006/10/netﬂix-
prize-
contest.html?program=NetﬂixRecommenderFramework.
[26] A. Narayanan and V. Shmatikov. How to break
anonymity of the netﬂix prize dataset. In Proceedings
of the IEEE Symposium on Security and Privacy,
2008.
[27] H. Polat and W. Du. Privacy-preserving collaborative
ﬁltering using randomized perturbation techniques. In
Proceedings of the IEEE International Conference on
Data Mining (ICDM), 2003.
[28] V. Rastogi, D. Suciu, and S. Hong. The boundary
between privacy and utility in data publishing. In
Proceedings of the International Conference on Very
Large Data Bases (VLDB), 2007.
[29] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and
J. Riedl. Grouplens: An open architecture for
collaborative ﬁltering of netnews. In Proceedings of the
ACM Conference on Computer Supported Cooperative
Work (CSCW), pages 175–186, 1994.
[30] P. Samarati and L. Sweeney. Generalizing data to
provide anonymity when disclosing information. In
Proceedings of the Symposium on Principles of
Database Systems (PODS), 1998.
[31] B. M. Sarwar, G. Karypis, J. A. Konstan, and J. T.
Riedl. Application of dimensionality reduction in
recommender system - a case study. In ACM
WebKDD Workshop, 2000.
[32] B. M. Sarwar, J. A. Konstan, A. Borchers,
J. Herlocker, B. Miller, and J. Riedl. Using ﬁltering
agents to improve prediction quality in the grouplens
research collaborative ﬁltering system. In Proceedings
of the ACM Conference on Computer Supported
Cooperative Work (CSCW), 1998.
[33] L. Sweeney. k-anonymity: a model for protecting
privacy. Journal on Uncertainty, Fuzziness and
Knowledge-based Systems, 10(5):557–570, 2002.
[34] B. Thompson and D. Yao. Union-split clustering
algorithm and social network anonymization. In
Proceedings of the ACM Symposium on Information,
Computer and Communications Security (AsiaCCS),
2009.
[35] J. Xu, W. Wang, J. Pei, X. Wang, B. Shi, and A. Fu.
Utility-based anonymization using local recoding. In
Proceedings of the ACM International Conference on
Knowledge Discovery and Data Mining (KDD), 2006.
[36] B. Zhou and J. Pei. Preserving privacy in social
networks against neighborhood attacks. In Proceedings
of the International Conference on Data Engineering
(ICDE), 2008.
APPENDIX
Theorem 5.2
Proof. Suppose an adversary, using a label-based attack,
is able to identify the anonymization group containing user
u. Based on the existence of a link to item o in the released
review graph, the adversary would like to infer whether user
u gave a rating for o in the original dataset. However, the
existence of the link in the anonymized graph only implies
that at least one user in that anonymization group had rated
o. With no additional prior knowledge, the adversary can
only infer that user u had rated o with probability at least
k .
Theorem 5.3
1
Proof. Suppose an adversary, using a label-based attack,
is able to identify the anonymization group containing user
u. Based on the existence of a link to item o in the released
review graph, the adversary would like to infer whether user
u gave a rating for o in the original dataset. Let P r(u, o)
denote the probability that user u rated item o in the original
dataset, and let P r(C, o) be the unconditional probability
that at least one user in anonymization group C rated o.
We wish to calculate P r((u, o)|(C, o)), the probability that
user u rated item o, given that the edge exists in the released
anonymized review graph.
By Bayes’ Rule, we have that P r((u, o)|(C, o)) =
P r((C, o)|(u, o)) ∗ P r(u, o)/P r(C, o).
First note that
P r((C, o)|(u, o)) = 1 directly from our anonymization pro-
cedure, and we also have that P r(u, o) = p. Furthermore,
since we have assumed that each user has rated item o in-
dependently with probability p, we can ﬁnd a bound on
P r(C, o) as follows:
P r(C, o) = 1 − (1 − p)k
= 1 − (1 − kp + k
= kp − k
2!p2 − o(p3))
2!p2 + o(p3) ≥ kp − k
2!p2
Combining these results, we get that
1
kp
P r((u, o)|(C, o)) ≤
k  
1 · p
kp −`k
2´p2
2´p2! +
+ −`k
2´p2
kp −`k
kp −`k
2´p2
k   1
2 k(k − 1)p2!
2 k(k − 1)p2
kp − 1
2 − (k − 1)p« ≤
k „ (k − 1)p
1
k
1
k
+
+
1
1
1
k
+
1
k   `k
2´p2!
2´p2
kp −`k
p
2 − kp
=
=
=