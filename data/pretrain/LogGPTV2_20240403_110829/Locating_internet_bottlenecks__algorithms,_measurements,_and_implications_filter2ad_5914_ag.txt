### Analysis of Overlay Routing and Multihoming

#### Performance of Selected Sites
For the two sites and their chosen destinations, overlay routing does not provide any additional benefit due to their already good performance. Another possible explanation is that the bottleneck is located near the source, making overlay routing ineffective. For other sites, where most randomly selected overlay nodes can improve available bandwidth, we conducted a more detailed analysis to determine if specific overlay nodes consistently enhance performance for a given source node. Surprisingly, the answer is affirmative. In fact, for most source nodes, 2 to 3 overlay nodes can improve performance in over 90% of the cases examined. For example, when using "vineyard" as the source, "jfk1," "bkly-cs," and "purdue" are effective overlay nodes for 92% of the destinations. This information is invaluable for making informed overlay routing decisions.

#### Discussion on Overlay Node Selection
The study presented here has several important implications for selecting overlay nodes and improving overlay routing strategies. Typically, overlay node selection involves continuous probing and monitoring between the source node and the overlay node, and between the overlay node and the destination node. This approach is not scalable if exhaustive probing is required for every combination of destinations and candidate overlay nodes. To minimize measurement overhead, one can leverage topology information to predict the likelihood that an intermediate overlay node will improve performance for a particular destination. Pathneck offers two key advantages:
1. **Bottleneck Identification**: Pathneck effectively identifies the location of stable bottleneck links and overlay nodes that often help in avoiding such links.
2. **Lightweight On-Demand Use**: Pathneck is lightweight enough to be used on-demand to decide which upstream provider to use for routing bandwidth-intensive applications or those requiring minimal bandwidth, such as multimedia streaming.

### Multihoming

#### Overview
Large enterprise networks often multihome to different providers. The multihomed network usually has its own Autonomous System (AS) number and exchanges routing information with its upstream providers via the Border Gateway Protocol (BGP). The primary motivation for multihoming is to achieve resilient network connectivity or redundancy in case the connection to one ISP fails or experiences severe routing outages. Multihoming not only increases the availability of network connectivity but can also improve performance by allowing traffic to be routed through different upstream providers based on the routing performance to a given destination. A recent study [8] has shown that, by carefully choosing the right set of upstream providers, high-volume content providers can gain significant performance benefits from multihoming.

#### Performance Benefits and Challenges
The performance benefits of multihoming depend heavily on the diversity of routing paths and the location of failures or performance bottlenecks. If a network is multihomed to two providers that route large portions of its traffic via paths with significant overlap, the benefits of multihoming will be limited, as it will not be able to avoid bottlenecks in the shared paths. Therefore, we consider the following two problems:
1. **Provider Selection for Popular Destinations**: Given a set of popular destinations frequently accessed by a network, which upstream provider should the network consider using?
2. **Provider Selection for Specific Destinations**: Given a set of upstream providers, which provider should be used to reach a specific destination?

To address these questions without extensive probing, we show that Pathneck can provide valuable insights. To our knowledge, this is the first study to examine the benefits of multihoming in avoiding bottleneck links by quantifying the improvement in available bandwidth.

#### Methodology
To understand the effect of multihoming on avoiding bottleneck links, one would ideally probe from the same source to each of several destinations through different upstream providers. A previous study [8] simulated this by probing from nodes within the same city but connected through different upstream providers. However, very few of our probe nodes are located in the same city and have different upstream providers. We simulated this by choosing 22 probing sources belonging to different, but geographically close, organizations, as shown in Table 6. We treat the members in the same group as nodes within the same city. While this is a simplification, the geographic distance between any two nodes within the same group is small relative to the diverse set of 7,090 destinations we selected for probing.

To evaluate the effectiveness of multihoming, for each geographic group, we examined the bounds on the available bandwidth of the paths from each member in the group to the same destination. If the improvement in the lower bound or the upper bound from the worst path compared with any other path in the group is more than 50% of the original value, we declared multihoming to be useful. Similar to the overlay routing study, our metric only considers available bandwidth; for some applications, other path properties such as latency and cost could be more important.

#### Results
Among all 42,285 comparisons across all probing locations, more than 78% were found to be useful cases. This is very encouraging and shows that multihoming significantly helps in avoiding bottleneck links. However, these results may be overly optimistic given our destination set and the difficulty in discovering bottlenecks at the destination site. Many of the probe destinations selected are not stub networks, and most do not correspond to addressable end hosts. Additionally, firewalls often drop outgoing ICMP packets, rendering Pathneck ineffective at identifying bottlenecks at some destination sites. Nevertheless, our results suggest that multihoming is very effective at avoiding bottleneck links near the source or inside the network core. When we require both the upper bound and the lower bound to improve by 50%, the useful rate is reduced to exactly 50%.

Examining the results for individual groups in Table 6 reveals some interesting characteristics. Generally, the larger the group, the higher the useful rate. For the two sites outside North America—Britain and Korea—the useful rates are significantly lower. We conjecture that the transoceanic link is the main bottleneck and cannot easily be avoided by choosing a nearby source node within the same country. These two groups also have fewer choices, with only two members each.

Intuitively, one would expect diminishing returns as more service providers are added. An earlier study [8] has shown this with respect to reducing download time for Web objects. We now examine this effect using available bandwidth as the performance metric. Figure 14 shows how the useful rate increases with the number of providers. We see a fairly steady increase, even for higher numbers of providers. We plan to investigate this further using more probe source locations.

#### Discussion
The results of the multihoming study are quite encouraging. Not only do they suggest that multihoming can yield significant benefits, but they also show that information collected by Pathneck can be used to select the upstream providers.

### Related Work

#### Bandwidth Estimation Techniques
Bandwidth estimation techniques, specifically available bandwidth estimation algorithms, measure network throughput, which is closely related to congestion. However, they provide no location information for the congestion point. Most of these tools, except cprobe [13], require the cooperation of the destination, making them difficult to deploy. Packet loss rate is another important metric related to user throughput, especially for TCP traffic. Tools that focus on loss rate include Sting [32], which measures the network path loss rate, and Tulip [23], which can pinpoint the packet loss position.

#### Tools Related to Pathneck
The tools most closely related to Pathneck include Cartouche [17], Packet Tailgating [29], BFind [10], and Pathchar [19]. Cartouche uses a packet train that combines packets of different sizes to measure the bandwidth for any segment of the network path, and the bottleneck location can be deduced from its measurement results. Packet Tailgating also combines load packets and measurement packets but expires the load packets instead of the measurement packets. Both Cartouche and Packet Tailgating require two-end control.

BFind detects the bottleneck position by injecting a steady UDP flow into the network path and gradually increasing its throughput to amplify congestion at the bottleneck router. It uses Traceroute to monitor RTT changes to all routers on the path, thus detecting the position of the most congested link. However, BFind's overhead is high, limiting its use to bottlenecks with available bandwidth of less than 50 Mbps.

Pathchar estimates the capacity of each link on a network path by measuring the data transmission time on each link. This is done by taking the difference between the RTTs from the source to two adjacent routers. Pathchar needs to send a large number of probing packets to filter out measurement noise, resulting in a large probing overhead.

Due to the lack of a measurement tool to identify bottleneck location, there has been limited work on analyzing Internet bottlenecks. We are only aware of the analysis in [10], which shows that most bottlenecks are on edge and peering links. Pathneck overcomes some of the limitations in BFind, allowing us to perform a more extensive bottleneck study. The large BGP database we have access to also enables us to probe the Internet in a more systematic way, giving our analysis a broader scope.

Several studies have shown that overlay routing [31, 33], multipath routing [12, 25, 28], and multihoming [8, 5] benefit end-user communication by reducing packet loss rate and increasing end-to-end throughput. These projects primarily focus on link failure or packet loss, although some consider latency and throughput as well. A recent study compares the effectiveness of overlay routing and multihoming, considering latency, loss rate, and throughput as metrics [9].

In contrast, our work approaches the problem from a different angle—by identifying the location of the bottleneck, we can study how overlay routing and multihoming can be used to avoid bottlenecks. Our work shows the benefits of overlay routing and multihoming and suggests efficient route selection algorithms.

### Conclusion and Future Work

In this paper, we present a novel, lightweight, single-end active probing tool called Pathneck, based on a technique called Recursive Packet Train (RPT). Pathneck allows end users to efficiently and accurately locate bottleneck links on the Internet. We show that Pathneck can identify a clearly-defined bottleneck on almost 80% of the Internet paths we measured. Based on an extensive set of Internet measurements, we also found that up to 40% of the bottlenecks are inside ASes, contrary to common assumptions. We showed how Pathneck can help infer bottleneck location on a path without probing. Finally, we illustrated how Pathneck can be used to guide overlay routing and multihoming.

This paper analyzes only some aspects of Internet bottlenecks, and many issues require further study, including the stability of bottlenecks, the impact of topology and routing changes on bottlenecks, and the distribution of bottlenecks across different AS levels. We also hope to improve Pathneck by studying how configuration parameters such as the load packet size, the number of load packets, and the initial probing rate of RPT affect the measurement accuracy.

### Acknowledgments
We thank Dave Andersen for his help in setting up the experiments on RON, Jay Lepreau and the Emulab team for their quick responses during our testbed experiments, and Ming Zhang for his help in using the PlanetLab socket interface. We thank support from the PlanetLab staff. We also thank our shepherd Christos Papadopoulos and the anonymous reviewers for their constructive comments.

Ningning Hu and Peter Steenkiste were in part supported by the NSF under award number CCR-0205266.

### References
[1] Abilene network monitoring. http://www.abilene.iu.edu/noc.html.
[2] Dummynet. http://info.iet.unipi.it/∼luigi/ip dummynet/.
[3] Emulab. http://www.emulab.net.
[4] Planetlab. https://www.planet-lab.org.
[5] Routescience. http://www.routescience.com.
[6] University of Oregon Route Views Project. http://www.routeviews.org.
[7] RFC 792. Internet control message protocol, September 1981.
[8] A. Akella, B. Maggs, S. Seshan, A. Shaikh, and R. Sitaraman. A Measurement-Based Analysis of Multihoming. In Proc. ACM SIGCOMM, September 2003.
[9] A. Akella, J. Pang, B. Maggs, S. Seshan, and A. Shaikh. Overlay routing vs multihoming: An end-to-end perspective. In Proc. ACM SIGCOMM, August 2004.
[10] A. Akella, S. Seshan, and A. Shaikh. An empirical evaluation of wide-area internet bottlenecks. In Proc. ACM IMC, October 2003.
[11] K. G. Anagnostakis, M. B. Greenwald, and R. S. Ryger. cing: Measuring network-internal delays using only existing infrastructure. In Proc. IEEE INFOCOM, April 2003.
[12] D. G. Andersen, A. C. Snoeren, and H. Balakrishnan. Best-path vs. multi-path overlay routing. In Proc. ACM IMC, October 2003.
[13] R. L. Carter and M. E. Crovella. Measuring bottleneck link speed in packet-switched networks. Technical report, Boston University Computer Science Department, March 1996.
[14] C. Dovrolis, P. Ramanathan, and D. Moore. What do packet dispersion techniques measure? In Proc. of ACM INFOCOM, April 2001.
[15] D. Goldenberg, L. Qiu, H. Xie, Y. R. Yang, and Y. Zhang. Optimizing Cost and Performance for Multihoming. In Proc. ACM SIGCOMM, August 2004.
[16] R. Govindan and V. Paxson. Estimating router ICMP generation delays. In Proc. PAM, March 2002.
[17] K. Harfoush, A. Bestavros, and J. Byers. Measuring bottleneck bandwidth of targeted path segments. In Proc. IEEE INFOCOM, April 2003.
[18] N. Hu and P. Steenkiste. Evaluation and characterization of available bandwidth probing techniques. IEEE JSAC Special Issue in Internet and WWW Measurement, Mapping, and Modeling, 21(6), August 2003.
[19] V. Jacobson. pathchar - a tool to infer characteristics of internet paths, 1997. Presented as April 97 MSRI talk.
[20] M. Jain and C. Dovrolis. End-to-end available bandwidth: Measurement methodology, dynamics, and relation with TCP throughput. In Proc. ACM SIGCOMM, August 2002.
[21] M. Jain and C. Dovrolis. Pathload: A measurement tool for end-to-end available bandwidth. In Proc. PAM, March 2002.
[22] K. Lai and M. Baker. Nettimer: A tool for measuring bottleneck link bandwidth. In Proc. of the USENIX Symposium on Internet Technologies and Systems, March 2001.
[23] R. Mahajan, N. Spring, D. Wetherall, and T. Anderson. User-level internet path diagnosis. In Proc. SOSP, October 2003.
[24] Z. M. Mao, J. Rexford, J. Wang, and R. Katz. Towards an Accurate AS-level Traceroute Tool. In Proc. ACM SIGCOMM, September 2003.
[25] N. Maxemchuk. Dispersity Routing in Store and Forward Networks. PhD thesis, University of Pennsylvania, May 1975.
[26] B. Melander, M. Bjorkman, and P. Gunningberg. A new end-to-end probing and analysis method for estimating bandwidth bottlenecks. In Proc. IEEE GLOBECOM, November 2000.
[27] J. Padhye, V. Firoiu, D. Towsley, and J. Kurose. Modeling TCP throughput: A simple model and its empirical validation. In Proc. ACM SIGCOMM, September 1998.
[28] M. O. Rabin. Efficient dispersal of information for security, load balancing, and fault tolerance. J. of the ACM, 36(2), April 1989.
[29] V. Ribeiro. Spatio-temporal available bandwidth estimation for high-speed networks. In Proc. of the First Bandwidth Estimation Workshop (BEst), December 2003.
[30] V. Ribeiro, R. Riedi, R. Baraniuk, J. Navratil, and L. Cottrell. pathchirp: Efficient available bandwidth estimation for network paths. In Proc. PAM, April 2003.
[31] RON. Resilient Overlay Networks. http://nms.lcs.mit.edu/ron/.
[32] S. Savage. Sting: a TCP-based network measurement tool. In Proc. of the 1999 USENIX Symposium on Internet Technologies and Systems, October 1999.
[33] S. Savage, T. Anderson, A. Aggarwal, D. Becker, N. Cardwell, A. Collins, E. Hoffman, J. Snell, A. Vahdat, G. Voelker, and J. Zahorjan. Detour: a case for informed internet routing and transport. IEEE Micro, 19(1), 1999.
[34] N. Spring, R. Mahajan, and T. Anderson. Quantifying the Causes of Path Inflation. In Proc. ACM SIGCOMM, August 2003.
[35] N. Spring, R. Mahajan, and D. Wetherall. Measuring ISP topologies with Rocketfuel. In Proc. ACM SIGCOMM, August 2002.
[36] J. Strauss, D. Katabi, and F. Kaashoek. A measurement study of available bandwidth estimation tools. In Proc. ACM IMC, October 2003.
[37] H. Tangmunarunkit, R. Govindan, and S. Shenker. Internet Path Inflation Due to Policy Routing. In Proc. SPIE ITCOM, August 2001.