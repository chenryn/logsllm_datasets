these two sites and the chosen destinations already have good per-
formance, hence overlay routing does not help. Another possible
explanation is that the bottleneck is near the source, so again over-
lay routing cannot help. Among the other sites, where most of the
randomly selected overlay nodes can help in improving available
bandwidth, we studied the data in more detail to see whether any
particular overlay nodes are always helpful for a given source node.
Surprisingly, the answer is yes. In fact, for most source nodes, there
are 2 to 3 overlay nodes that can improve performance for more
than 90% of the cases examined. For example, when using vine-
yard as a source, jfk1, bkly-cs, and purdue all prove to be useful
as overlay nodes for 92% of the destinations. Such information
is very helpful in making overlay routing decisions, as we discuss
below.
Discussion: The study presented here has several important impli-
cations for how to select overlay nodes and for improving overlay
routing strategies. Overlay node selection typically involves con-
tinuous probing and monitoring between the source node and the
overlay node, and between the overlay node and the destination
node. This solution is not scalable if one has to probe exhaustively
for every combination of destinations and candidate overlay nodes.
To minimize measurement overhead, one can make use of the topo-
logy information to predict how likely an intermediate overlay node
can help improve performance to a particular destination. Pathneck
presents two opportunities here: (1) Pathneck is very helpful in
identifying both the location of stable bottleneck links and overlay
nodes that often seem helpful in avoiding such links. (2) Pathneck
is light-weight enough to be used on-demand to decide which up-
stream provider to use for routing bandwidth-intensive applications
or applications requiring a minimal amount of bandwidth to func-
tion, e.g., multimedia streaming.
6.2 Multihoming
Large enterprise networks often multihome to different
providers. The multihomed network usually has its own Au-
tonomous System (AS) number and exchanges routing informa-
tion with its upstream providers via the Border Gateway Protocol
(BGP). The original motivation for multihoming was to achieve
resilient network connectivity or redundancy in case the connectiv-
ity to one ISP fails or one of the ISPs experiences severe routing
outages. Multihoming can not only increase the availability of net-
work connectivity, but can also improve performance by allowing
multihomed customers to route trafﬁc through different upstream
providers based on the routing performance to a given destination.
A recent study [8] has shown that, by carefully choosing the right
set of upstream providers, high-volume content providers can gain
signiﬁcant performance beneﬁt from multihoming.
The performance beneﬁt offered by multihoming depends highly
on the routing path diversity and the location of failures or per-
formance bottlenecks. For example, if a network is multihomed
to two providers that route large portions of its trafﬁc via paths
with signiﬁcant overlap, then the beneﬁt of multihoming will be
sharply diminished since it will not be able to avoid bottlenecks in
the shared paths. As a result, we consider the following two prob-
lems: (1) Given the set of popular destinations a network frequently
accesses, which upstream provider should the network consider us-
ing? (2) Given a set of upstream providers, which provider should
be used to reach a given destination? Clearly we would like to do
the selection without expensive probing. We show that Pathneck
can help answer both these questions. To the best of our knowl-
edge, this is the ﬁrst study to examine the beneﬁt of multihoming
to avoid bottleneck links by quantifying the improvement in avail-
able bandwidth.
Methodology: To understand the effect of multihoming on avoid-
ing bottleneck links, one would ideally probe from the same
source to each of several destinations through different upstream
Table 6: Grouping based on coarse-grained geographic prox-
imity.
Group name
sf
nyc
kansas
chicago
britain
korea
Group member
bkly-cs, ucsc, stanford
princeton, jhu, bu,
umd, rpi, mit-pl, dartmouth, cmu
ku, wustl
depaul, umich, uky, northwest, msu,
cam-uk, ac-uk
kaist-kr, snu-kr
Useful rate
94%
99%
90%
98%
17%
74%
providers. A previous study [8] simulated this by probing from
nodes within the same city but connected through different up-
stream providers. Unfortunately, very few of our probe nodes are
located in the same city and have different upstream providers. We
simulate this by choosing 22 probing sources belonging to differ-
ent, but geographically close, organizations, as is shown in Table 6.
We treat the members in the same group as nodes within the same
city. While this is a simpliﬁcation, we note that the geographic dis-
tance between any two nodes within the same group is small rela-
tive to the diverse set of 7, 090 destinations we selected for probing.
To evaluate the effectiveness of multihoming, for each geo-
graphic group, we examine the bounds on the available bandwidth
of the paths from each member in the group to the same destina-
tion. If the improvement in the lower bound or the upper bound
from the worst path compared with any other path in the group is
more than 50% of original value, then we declare multihoming to
be useful. Note that similar to the overlay routing study, our metric
only considers available bandwidth; for some applications, other
path properties such as latency and cost could be more important.
Results: Among all 42, 285 comparisons we are able to make
across all probing locations, more than 78% of them are useful
cases. This is very encouraging and shows that multihoming signif-
icantly helps in avoiding bottleneck links. However, these results
may be overly optimistic given our destination set and the difﬁ-
culty in discovering bottlenecks at the destination site. First, many
of the probe destinations selected are not stub networks and most
of them do not correspond to addressable end hosts. Furthermore,
ﬁrewalls often drop outgoing ICMP packets, thus rendering Path-
neck ineffective at identifying bottleneck at some destination sites.
Nevertheless, our results suggest that multihoming is very effective
at avoiding bottleneck links near the source or inside the network
core. When we are more conservative and we require both the up-
per bound and the lower bound to improve by 50%, then the useful
rate is reduced to exactly 50%.
Examining the results for individual groups in Table 6 reveals
some interesting characteristics. First of all, the bigger the group,
the higher the useful rate. For the two sites outside North America
– britain and korea, the useful rates are signiﬁcantly lower. We
conjecture that the transoceanic link is the main bottleneck link and
it cannot easily be avoided by choosing a nearby source node within
the same country. Also, these two groups have only two members,
so they have fewer choices.
Intuitively one would expect that as one adds more service
providers, there is a diminishing return for multihoming. An earlier
study [8] has shown this with respect to reducing download time
for Web objects. We now examine this effect using available band-
width as the performance metric. Figure 14 shows how the useful
rate increases with the number of providers. We see that there is
a fairly steady increase, even for higher numbers of providers. We
plan to investigate this further using more probe source locations.
Discussion: The results of the multihoming study are quite en-
s
k
n
i
l
k
c
e
n
e
l
t
t
o
b
i
g
n
d
o
v
a
i
n
i
e
t
a
r
l
u
f
e
s
U
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
2
3
nyc
chicago
kansas
sf
korea
britain
6
7
8
4
5
Number of providers
Figure 14: Improvement in avoiding bottleneck links with in-
crease in providers.
couraging. Not only do they suggest that multihoming can yield
signiﬁcant beneﬁts, but they also show that information collected
by Pathneck can be used to select the upstream providers.
7. RELATED WORK
Bandwidth estimation techniques [14, 22], speciﬁcally available
bandwidth estimation algorithms [13, 20, 26, 18, 30, 36], measure
network throughput, which is closely related to congestion. How-
ever, they provide no location information for the congestion point.
Also, all these tools, except cprobe [13], need the cooperation of
the destination, which makes them very hard to deploy. Packet loss
rate is another important metric that is related to user throughput,
especially for TCP trafﬁc [27]. Tools that focus on loss rate in-
clude Sting [32], which measures the network path loss rate, and
Tulip [23], which can pinpoint the packet loss position.
The tools that are most closely related to Pathneck include Car-
touche [17], Packet Tailgating [29], BFind [10] and Pathchar [19].
Cartouche [17] uses a packet train that combines packets of dif-
ferent sizes and exploits differences in how different-sized packets
are handled to measure the bandwidth for any segment of the net-
work path. The bottleneck location can naturally be deduced from
its measurement results. Packet Tailgating [29] also combines load
packets and measurement packets, but instead of letting measure-
ment packets expire, it expires the load packets. Both Cartouche
and Packet Tailgating require two end control.
BFind [10] detects the bottleneck position by injecting a steady
UDP ﬂow into the network path, and by gradually increasing its
throughput to amplify the congestion at the bottleneck router. At
the same time, it uses Traceroute to monitor the RTT changes to
all the routers on the path, thus detecting the position of the most
congested link. Concerns due the overhead generated by the UDP
ﬂow force BFind to only look for a bottleneck with available band-
width of less than 50Mbps. Even then, considering its measure-
ment time, its overhead is fairly high, which is undesirable for a
general-purpose probing tool.
Pathchar [19] estimates the capacity of each link on a network
path. The main idea is to measure the data transmission time on
each link. This is done by taking the difference between the RTTs
from the source to two adjacent routers. To ﬁlter out measurement
noise due to factors such as queueing delay, pathchar needs to send
a large number of probing packets, identifying the smallest RTT
values for the ﬁnal calculation. As a result, pathchar also has a
large probing overhead.
Due to the lack of a measurement tool to identify bottleneck lo-
cation, there has not been much work on analyzing Internet bottle-
necks. We are only aware of the analysis in [10], which shows that
most bottlenecks are on edge and peering links. Pathneck over-
comes some of the limitations in BFind, thus allowing us to per-
form a more extensive bottleneck study. The large BGP database
that we have access to also enables us to probe the Internet in a
more systematic way, thus giving our analysis a broader scope.
Several studies have shown that overlay routing [31, 33], multi-
path routing [12, 25, 28], and multihoming [8, 5] beneﬁt end user
communication by reducing the packet loss rate and increasing end-
to-end throughput. These projects primarily focus on link failure
or packet loss, although some consider latency and throughput as
well. A recent study compares the effectiveness of overlay routing
and multihoming, considering latency, loss rate, and throughput as
metrics [9].
In contrast, our work approaches the problem from
a different angle— by identifying the location of the bottleneck
we can study how overlay routing and multihoming can be used to
avoid bottlenecks. Our work shows the beneﬁt of overlay routing
and multihoming and suggests efﬁcient route selection algorithms.
8. CONCLUSION AND FUTURE WORK
In this paper, we present a novel light-weight, single-end active
probing tool – Pathneck – based on a probing technique called Re-
cursive Packet Train (RPT). Pathneck allows end users to efﬁciently
and accurately locate bottleneck links on the Internet. We show that
Pathneck can identify a clearly-deﬁned bottleneck on almost 80%
of the Internet paths we measured. Based on an extensive set of
Internet measurements we also found that up to 40% of the bot-
tlenecks are inside ASes, contrary to common assumptions. We
showed how Pathneck can help infer bottleneck location on a path
without probing. Finally, we illustrated how Pathneck can be used
to guide overlay routing and multihoming.
This paper analyzes only some aspects of Internet bottlenecks
and many issues require further study, including the stability of
bottlenecks, the impact of topology and routing changes on bot-
tlenecks, and the distribution of bottlenecks across different (levels
of) ASes. We also hope to improve Pathneck, for example by study-
ing how conﬁguration parameters such as the load packet size, the
number of load packets, and the initial probing rate of RPT affect
the measurement accuracy.
Acknowledgments
We thank Dave Andersen for his help in setting up the experiments
on RON, Jay Lepreau and the Emulab team for their quick re-
sponses during our testbed experiments, and Ming Zhang for his
help in using the PlanetLab socket interface. We thank support
from the Planetlab staff. We also thank our shepherd Christos
Papadopoulos and the anonymous reviewers for their constructive
comments.
Ningning Hu and Peter Steenkiste were in part supported by the
NSF under award number CCR-0205266.
9. REFERENCES
[1] Abilene network monitoring. http://www.abilene.iu.edu/noc.html.
[2] Dummynet. http://info.iet.unipi.it/∼luigi/ip dummynet/.
[3] Emulab. http://www.emulab.net.
[4] Planetlab. https://www.planet-lab.org.
[5] Routescience. http://www.routescience.com.
[6] University of Oregon Route Views Project.
http://www.routeviews.org.
[7] RFC 792. Internet control message protocol, September 1981.
[8] A. Akella, B. Maggs, S. Seshan, A. Shaikh, and R. Sitaraman. A
Measurement-Based Analysis of Multihoming. In Proc. ACM
SIGCOMM, September 2003.
[9] A. Akella, J. Pang, B. Maggs, S. Seshan, and A. Shaikh. Overlay
routing vs multihoming: An end-to-end perspective. In Proc. ACM
SIGCOMM, August 2004.
[10] A. Akella, S. Seshan, and A. Shaikh. An empirical evaluation of
wide-area internet bottlenecks. In Proc. ACM IMC, October 2003.
[11] K. G. Anagnostakis, M. B. Greenwald, and R. S. Ryger. cing:
Measuring network-internal delays using only existing infrastructure.
In Proc. IEEE INFOCOM, April 2003.
[12] D. G. Andersen, A. C. Snoeren, and H. Balakrishnan. Best-path vs.
multi-path overlay routing. In Proc. ACM IMC, October 2003.
[13] R. L. Carter and M. E. Crovella. Measuring bottleneck link speed in
packet-switched networks. Technical report, Boston University
Computer Science Department, March 1996.
[14] C. Dovrolis, P. Ramanathan, and D. Moore. What do packet
dispersion techniques measure? In Proc. of ACM INFOCOM, April
2001.
[15] D. Goldenberg, L. Qiu, H. Xie, Y. R. Yang, and Y. Zhang.
Optimizing Cost and Performance for Multihoming. In Proc. ACM
SIGCOMM, August 2004.
[16] R. Govindan and V. Paxson. Estimating router ICMP generation
delays. In Proc. PAM, March 2002.
[17] K. Harfoush, A. Bestavros, and J. Byers. Measuring bottleneck
bandwidth of targeted path segments. In Proc. IEEE INFOCOM,
April 2003.
[18] N. Hu and P. Steenkiste. Evaluation and characterization of available
bandwidth probing techniques. IEEE JSAC Special Issue in Internet
and WWW Measurement, Mapping, and Modeling, 21(6), August
2003.
[19] V. Jacobson. pathchar - a tool to infer characteristics of internet
paths, 1997. Presented as April 97 MSRI talk.
[20] M. Jain and C. Dovrolis. End-to-end available bandwidth:
Measurement methodology, dynamics, and relation with TCP
throughput. In Proc. ACM SIGCOMM, August 2002.
[21] M. Jain and C. Dovrolis. Pathload: A measurement tool for
end-to-end available bandwidth. In Proc. PAM, March 2002.
[22] K. Lai and M. Baker. Nettimer: A tool for measuring bottleneck link
bandwidth. In Proc. of the USENIX Symposium on Internet
Technologies and Systems, March 2001.
[23] R. Mahajan, N. Spring, D. Wetherall, and T. Anderson. User-level
internet path diagnosis. In Proc. SOSP, October 2003.
[24] Z. M. Mao, J. Rexford, J. Wang, and R. Katz. Towards an Accurate
AS-level Traceroute Tool. In Proc. ACM SIGCOMM, September
2003.
[25] N. Maxemchuk. Dispersity Routing in Store and Forward Networks.
PhD thesis, University of Pennsylvania, May 1975.
[26] B. Melander, M. Bjorkman, and P. Gunningberg. A new end-to-end
probing and analysis method for estimating bandwidth bottlenecks.
In Proc. IEEE GLOBECOM, November 2000.
[27] J. Padhye, V. Firoiu, D. Towsley, and J. Kurose. Modeling TCP
throughput: A simple model and its empirical validation. In Proc.
ACM SIGCOMM, September 1998.
[28] M. O. Rabin. Efﬁcient dispersal of information for security, load
balancing, and fault tolerance. J. of the ACM, 36(2), April 1989.
[29] V. Ribeiro. Spatio-temporal available bandwidth estimation for
high-speed networks. In Proc. of the First Bandwidth Estimation
Workshop (BEst), December 2003.
[30] V. Ribeiro, R. Riedi, R. Baraniuk, J. Navratil, and L. Cottrell.
pathchirp: Efﬁcient available bandwidth estimation for network
paths. In Proc. PAM, April 2003.
[31] RON. Resilient Overlay Networks. http://nms.lcs.mit.edu/ron/.
[32] S. Savage. Sting: a TCP-based network measurement tool. In Proc.
of the 1999 USENIX Symposium on Internet Technologies and
Systems, October 1999.
[33] S. Savage, T. Anderson, A. Aggarwal, D. Becker, N. Cardwell,
A. Collins, E. Hoffman, J. Snell, A. Vahdat, G. Voelker, and
J. Zahorjan. Detour: a case for informed internet routing and
transport. IEEE Micro, 19(1), 1999.
[34] N. Spring, R. Mahajan, and T. Anderson. Quantifying the Causes of
Path Inﬂation. In Proc. ACM SIGCOMM, August 2003.
[35] N. Spring, R. Mahajan, and D. Wetherall. Measuring ISP topologies
with Rocketfuel. In Proc. ACM SIGCOMM, August 2002.
[36] J. Strauss, D. Katabi, and F. Kaashoek. A measurement study of
available bandwidth estimation tools. In Proc. ACM IMC, October
2003.
[37] H. Tangmunarunkit, R. Govindan, and S. Shenker. Internet Path
Inﬂation Due to Policy Routing. In Proc. SPIE ITCOM, August 2001.