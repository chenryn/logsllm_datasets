0
1
9
9
0
0
0
0
0
1
0
1
0
1
0
0
0
0
0
1
0
4
4
0
0
0
0
0
0
0
0
0
1
0
2
0
0
0
0
0
2
3
0
2
1
1
1
1
0
1
0
0
0
0
0
0
0
0
1
7
8
0
0
1
0
0
0
1
1
0
0
0
0
0
2
0
0
1
5
6
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
First, none of the tested intrusion-detection probes detects the entire set of
attacks. In fact, two of them, gwese and octopus, are not detected at all; this is
not surprising since both of them are very speciﬁc, windows-95 denial-of-service
attacks and as such are probably rare in the ﬁeld. Note, however, that the best
tool only detects ten out of seventeen trials; we believe that this number is quite
low and should be improved.
Then, IDS-A has the best detection rate, but it is also the worst oﬀender
when looking at false alerts. This reveals an issue with signatures, that are prob-
ably not tuned enough to diﬀerentiate the real attack from symptoms that would
also exist with other vulnerabilities, but are not signiﬁcant simply by themselves.
Clearly, the trade-oﬀ between accuracy and coverage tilts towards accuracy for
IDS-B and coverage for IDS-A; hence the better intrusion-detection system de-
pends on the particular needs of an organization. We would add that we are
not happy by either side of the trade-oﬀ. Less accuracy means more expensive
analyst time and a less precise diagnostic overall. Reduced coverage translates
into missed attacks. Also, the Snort yardstick shows that IDS-C and IDS-D are
not up to date with the possible attacks against an IP stack, letting the door
open to potential denial-of-service attacks.
Note that table 1 displays the number of diﬀerent alerts for each intrusion
detection system, not the total number of alerts generated per attempt. This
number of diﬀerent alerts is important for an analyst because it gives him more
information about the ongoing attack, and gives more meat to a correlation
system. Using this measure shows that not only does IDS-A detect the largest
number of attempts, it also is the one giving us the most information about the
malicious activity going on. The conclusion reached here is similar to the one in
the previous paragraph.
186
H. Debar and B. Morin
Finally, the table does not measure the number of alerts generated for each
attack. Some attacks generate a large number of packets, each of them carrying
the anomalous characteristic and triggering an alert. This characteristic has
already been shown in [5], and we conﬁrm this result; clearly this is a case where
aggregation of consecutive, similar alerts is desirable and should be performed
by the probe since it has all the elements to do so.
4.2 Results of the Trojan Horses Tests
Concerning the Trojan horse tests, all Trojans were detected by all intrusion-
detection systems except IDS-D, which did not detect any of them and generated
one false alert. Snort in addition generated two false alerts. Since default ports
and keywords were in use, these results are exactly what’s expected and the only
valid test result is that functionality speciﬁed in the documentation matches our
default expectations.
4.3 Results of the Whisker Vulnerability Scanning
The results are presented in Table 2. The ﬁrst two columns indicate the total
number of Whisker requests and the number of requests that actually deliver
information outside of directory existence. Then, for each intrusion-detection
system, the table gives the total number of alerts generated, the number of
Whisker requests that triggered an alert, and the number of Whisker directory
events that triggered an alert.
The table is incomplete, because of a number of circumstantial issues during
testing. We were unable to conﬁgure IDS-D to send events to our syslog server;
given that this test generates several thousands of events, it is impossible to
manually reincorporate the results of IDS-D into the log ﬁle. It also proved
impossible to use custom log ﬁles to carry out this task and we ﬁnally gave up,
having the impression from screen observation that the box was performing in
about the same way as the other ones. Also, during the ﬁnal rounds of the tests
our data collection network broke down and therefore the last three lines of the
experiment should be discounted. Since such a test run takes more than 3 days,
we were not able to keep the intrusion-detection systems reliably running for
that amount of time, at a rate of about one alert per second each.
We propose the following comments for these results:
Missed events. The obvious remark from this table is that many scan events
are not ﬂagged as anomalous; the commercial intrusion detection systems
ﬂag between 10% and 20% of the scan events, and Snort goes up to 30%,
but at the cost of many false alerts. This actually is a very reasonable trade-
oﬀ, because this oﬀers some resistance to alert ﬂooding. The most verbose
intrusion-detection probe across the board is Snort, which systematically
generates 3 to 6 times more alerts than the other probes.
Missed summary of scan activity. Note that this test carries out a scan, not
actual attacks. As such, there is no real attack traﬃc going on the network,
Evaluation of the Diagnostic Capabilities
187
Table 2. Results of the Whisker vulnerability scanning
Whisker
Snort
IDS-B
IDS-A
IDS-C
Total Scans Total Event Dir Total Event Dir Total Event Dir Total Event Dir
5
5
5
5
3
3
3
5
3
4
1
3
2
1
3
4
3
4
1
3
0
7
144
7
145
7
145
7
145
7
145
92
5
538 205
539 322
542 203
144
6
6
26
60
6
95 27
92
5
541 203
541 319
541 203
7
144
26
6
6
60
38
4
131
4
131
4
130
4
132
4
197
4
5 1366
5 1176
319
4
117
4
4
121
17
0