### Position and Estimate Dynamics

The plotted points represent the average estimate computed over all nodes that are still participating in the protocol at the end of a single epoch (30 cycles). Note that newly joined nodes cannot participate until the start of the next epoch. Although the average estimate is plotted, by cycle 30, the estimates at all nodes are practically identical, as confirmed by Figure 3(b). Additionally, if 2,500 nodes crash in a cycle, this means that 75% of the nodes ((30 × 2500)/105) are replaced during the epoch, leaving only 25% of the nodes to complete the epoch.

### Robustness of Estimates

The figure demonstrates that even when a large number of nodes are substituted during an epoch, most of the estimates remain within a reasonable range. This is consistent with the theoretical results discussed in Section 6.1, although in this case, there is an additional source of error: nodes are not only removed but also replaced by new nodes. While the new nodes do not participate in the current epoch, their presence has an effect similar to link failure, as they will refuse all connections belonging to the running epoch. However, the variance of the estimate remains unchanged, as Sections 6.2 and 7.2 indicate that link failure does not alter the estimate; it only slows down convergence. Given that an epoch lasts 30 cycles, this duration is sufficient for convergence, even under the highest fluctuation rates. For further details, see Figure 5, which plots the estimate variance against the theoretical prediction.

### Y-Coordinates and Averages

When applicable, averages computed over all experiments are also shown as curves.

### Node Crashes and Their Effects

The crash of a node can have several possible effects. If the crashed node had a value smaller than the actual global average, the estimated average (which should be 1/N) will increase, leading to a decrease in the reported size of the network N. Conversely, if the crashed node had a value larger than the average, the estimated average will decrease, resulting in an increase in the reported size of the network N.

### Convergence Factor in the Presence of Link Failures

Figure 7(a) illustrates the convergence factor of the COUNT protocol in the presence of link failures. As previously discussed, the primary effect is a proportionally slower convergence. The theoretically predicted upper bound of the convergence factor (see (5)) indeed bounds the average convergence factor, and it is more accurate for higher values of Pd.

### Message Loss and Its Impact

In addition to symmetric link failures, single messages may also be lost. If the message initiating an exchange is lost, the entire exchange is effectively lost, and the convergence process is slowed down. However, if the lost message is a response, the global average may change, either increasing or decreasing depending on the value contained in the message.

Figure 7(b) shows the effect of message omissions. A given percentage of all messages (initiated or response) was dropped. For each experiment, both the maximum and minimum estimates over the nodes in the network are shown. When a small percentage of messages are lost, reasonable quality estimates can be obtained. However, when the number of lost messages is higher, the results provided by aggregation can vary significantly. In such cases, running multiple concurrent instances of the protocol can improve the quality of the estimates, as explained in the next section.

### Increasing Robustness with Multiple Instances

To mitigate the impact of "unlucky" runs of the aggregation protocol that generate incorrect estimates due to failures, one solution is to run multiple concurrent instances of the protocol. We simulated a number t of concurrent instances of the COUNT protocol, with t varying from 1 to 50. At each node, the t estimates obtained at the end of each epoch are ordered. The (t/3) lowest and (t/3) highest estimates are discarded, and the reported estimate is the average of the remaining results.

Figure 8(a) shows the results of applying this technique in a system where 1000 nodes per cycle are substituted with new nodes, while Figure 8(b) shows the results in a system where 20% of the messages are lost. Even though the number of nodes participating in the epoch decreases in the node crashing scenario, the correct estimation is 105, as the protocol reports the network size at the beginning of the epoch. The results are encouraging; by maintaining and exchanging just 20 numerical values, the accuracy achieved is very high, especially considering the challenging scenarios tested.

### Related Work

Protocols based on the analogy with the spreading of epidemics have found many applications, including database replication [2] and failure detection [12].

The idea of aggregation was pioneered by Van Renesse in Astrolabe [10, 11]. Astrolabe uses a hierarchical architecture, which reduces the cost of finding aggregates and enables complex, database-style queries. However, maintaining the hierarchical topology introduces additional overhead. Our anti-entropy aggregation is different, as it is simple, lightweight, and targeted for unstructured, highly dynamic environments. Furthermore, our protocol is proactive, meaning the result of aggregation is known to all nodes.

Kempe et al. [8] propose a similar aggregation protocol, but their approach is based on push-only gossiping mechanisms. Their discussion is limited to the theoretical version of the protocol, whereas this paper provides a detailed practical implementation and assesses its performance in a faulty environment.

### Conclusions

In this paper, we presented a full-fledged proactive aggregation protocol and demonstrated its adaptability and robustness to benign failures through theoretical and experimental analysis. We showed that a reasonably good approximation can be obtained even if 75% of the nodes crash during the calculation of the aggregate. The protocol is insensitive to link failure, which only causes proportional slowdown without affecting the approximation. In the case of message loss, the protocol remains reliable under usual, reasonable levels of message loss, and robustness can be improved by running multiple instances of the protocol simultaneously. Our experimental results are supported by theoretical analysis for crashes and link failures.

In summary, our aggregation protocol is robust to benign failures and can handle extremely high levels of dynamism. The fact that our experiments were conducted using the worst-case peak distribution further confirms the practical applicability of the protocol.

### References

[1] A.-L. Barab´asi. Linked: the new science of networks. Perseus, Cambridge, Mass., 2002.
[2] A. Demers, D. Greene, C. Hauser, W. Irish, J. Larson, S. Shenker, H. Sturgis, D. Swinehart, and D. Terry. Epidemic Algorithms for Replicated Database Management. In Proc. of PODC’87, Vancouver, Aug. 1987. ACM.
[3] I. Gupta, R. van Renesse, and K. P. Birman. Scalable fault-tolerant aggregation in large process groups. In Proc. of DSN’01, G¨oteborg, Sweden, 2001.
[4] M. Jelasity, W. Kowalczyk, and M. van Steen. Newscast computing. Technical Report IR-CS-006, Vrije Universiteit Amsterdam, Department of Computer Science, Amsterdam, The Netherlands, Nov. 2003.
[5] M. Jelasity and A. Montresor. Epidemic-Style Proactive Aggregation in Large Overlay Networks. In Proc. of the 24th International Conference on Distributed Computing Systems (ICDCS 2004), Tokyo, Japan, 2004.
[6] M. Jelasity, A. Montresor, and O. Babaoglu. A modular paradigm for building self-organizing peer-to-peer applications. In Proc. of the 1st International Workshop on Engineering Self-Organizing Applications (ESOA’03), Melbourne, Australia, 2003.
[7] M. Jelasity, A. Montresor, and O. Babaoglu. Towards secure epidemics: Detection and removal of malicious peers in epidemic-style protocols. Technical Report UBLCS-2003-14, University of Bologna, Bologna, Italy, Nov. 2003.
[8] D. Kempe, A. Dobra, and J. Gehrke. Computing aggregate information using gossip. In Proc. of FOCS’03, 2003.
[9] M. Ripeanu, I. Foster, and A. Iamnitchi. Mapping the Gnutella network: Properties of large-scale peer-to-peer systems and implications for system design. IEEE Internet Computing Journal, 6(1), 2002.
[10] R. van Renesse. The importance of aggregation. In A. Schiper, A. A. Shvartsman, H. Weatherspoon, and B. Y. Zhao, editors, Future Directions in Distributed Computing, LNCS 2584. Springer, 2003.
[11] R. Van Renesse, K. P. Birman, and W. Vogels. Astrolabe: A robust and scalable technology for distributed system monitoring, management, and data mining. ACM Transactions on Computer Systems, 21(2), May 2003.
[12] R. van Renesse, Y. Minsky, and M. Hayden. A gossip-style failure detection service. In N. Davies, K. Raymond, and J. Seitz, editors, Middleware ’98. Springer, 1998.
[13] D. Watts. Small Worlds: The Dynamics of Networks Between Order and Randomness. Princeton University Press, 1999.
[14] D. J. Watts and S. H. Strogatz. Collective dynamics of 'small-world' networks. Nature, 393, 1998.