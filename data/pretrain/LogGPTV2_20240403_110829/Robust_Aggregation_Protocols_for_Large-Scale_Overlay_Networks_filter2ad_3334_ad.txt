sition is dynamic. The plotted dots correspond to the aver-
age estimate computed over all nodes that still participate in
the protocol (recall that joining nodes cannot participate un-
til the start of the next epoch) at the end of a single epoch (30
cycles). Note that although the average estimate is plotted,
in cycle 30 the estimates are practically identical at all nodes
as Figure 3(b) conﬁrms. Also note that 2,500 nodes crashing
in a cycle means that 75% of the nodes ((30 × 2500)/105)
are substituted during the epoch, leaving 25% of the nodes
that make it until the end of the epoch.
The ﬁgure demonstrates that—even when a large number
of nodes are substituted during an epoch—most of the esti-
mates are included in a reasonable range. This is consistent
with the theoretical result discussed in Section 6.1, although
in this case we have an additional source of error; nodes are
not only removed but replaced by new nodes. While the new
nodes do not participate in the epoch, they result in an effect
similar to link failure, as new nodes will refuse all connec-
tions that belong to the currently running epoch. However,
the variance of the estimate is still the same as according
to Sections 6.2 and 7.2 link failure does not change the es-
timate, only slows down convergence. Since an epoch lasts
30 cycles, this time is enough for convergence even beside
the highest ﬂuctuation rate. See also Figure 5 for the esti-
mate varianceplotted against the theoretical prediction.
y-coordinates. When applicable, averages computed over all
experiments are also shown as curves.
7.2. Link Failures and Message Omissions
7.1. Node Crashes
The crash of a node may have several possible effects. If
the crashed node had a value smaller than the actual global
average, the estimated average (which should be 1/N ) will
increase and consequently the reported size of the network
N will decrease. If the crashed node has a value larger than
the average, the estimated average will decrease and conse-
quently the reported size of the network N will increase.
Figure 7(a) shows the convergence factor of COUNT in the
presence of link failures. As discussed earlier, in this case
the only effect is a proportionally slower convergence. The
theoretically predicted upper bound of the convergence fac-
tor (see (5)) indeed bounds the average convergence factor,
and—as predicted—it is more accurate for higher values of
Pd.
Apart from link failures that interrupt communication be-
tween two nodes in a symmetric way, it is also possible that
single messages are lost. If the message sent to initiate an
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:52:01 UTC from IEEE Xplore.  Restrictions apply. 
r
o
t
c
a
F
e
c
n
e
g
r
e
v
n
o
C
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
Experiments
Average Convergence Factor
Theoretical Upper Bound on Avg. Conv. Fact.
0
0.2
0.4
0.6
0.8
1
Pd
i
e
z
S
d
e
t
a
m
i
t
s
E
130000
125000
120000
115000
110000
105000
100000
95000
90000
Experiments (Max)
Experiments (Min)
0
5
10
15
20
25
30
35
40
45
50
Number of Aggregation Instances
(a) Convergence factor of protocol COUNT as a function of link
failure probability.
(a) Network size estimation with multiple instances of protocol
COUNT. 1000 nodes crash at the beginning of each cycle.
Experiments (Max values)
Experiments (Min values)
1e+09
1e+08
1e+07
1e+06
i
e
z
S
d
e
t
a
m
i
t
s
E
100000
10000
1000
100
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Fraction of Messages Lost
i
e
z
S
d
e
t
a
m
i
t
s
E
300000
250000
200000
150000
100000
50000
0
0
Experiments (Max)
Experiments (Min)
5
10
15
20
25
30
35
40
45
50
Number of Aggregation Instances
(b) Network size estimation with protocol COUNT as a function
of lost messages.
(b) Network size estimation with protocol COUNT as a function
of concurrent protocol instances. 20% of messages are lost.
Figure 7. Effects of communication failures
on the COUNT protocol in a NEWSCAST network.
Figure 8. Effects of failures on protocol COUNT
with multiple concurrent instances.
exchange is lost, the ﬁnal effect is the same as with link fail-
ure: the entire exchange is lost, and the convergence process
is just slowed down. But if the message lost is the response
to an initiated exchange, the global average may change (ei-
ther increasing or decreasing, depending on the value con-
tained in the message).
The effect of message omissions is illustrated in Fig-
ure 7(b). The given percentage of all messages (initiated or
response) was dropped. For each experiment, both the max-
imum and the minimum estimates over the nodes in the net-
work are shown. As can be seen, when a small percentage of
messages are lost, estimations of reasonable quality can be
obtained. Unfortunately, when the number of messages lost
is higher, the results provided by aggregation can be larger
or smaller by several order of magnitudes. In this case, how-
ever, it is possible to improve the quality of estimations con-
siderably by running multiple concurrent instances of the
protocol, as explained in the next section.
7.3. Increasing Robustness Using Multiple In-
stances of Aggregation
To reduce the impact of “unlucky” runs of the aggrega-
tion protocol that generate incorrect estimates due to fail-
ures, one possibility is to run multiple concurrent instances
of the aggregation protocol. To test this solution, we have
simulated a number t of concurrent instances of the COUNT
protocol, with t varying in the range between 1 and 50. At
each node, the t estimates obtained at the end of each epoch
are ordered. Subsequently, the (cid:10)t/3(cid:11) lowest estimates and
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:52:01 UTC from IEEE Xplore.  Restrictions apply. 
the (cid:10)t/3(cid:11) highest estimates are discarded, and the reported
estimate is given by the average of the remaining results.
Figure 8(a) shows the results obtained by applying this
technique in a system where 1000 nodes per cycle are sub-
stituted with new nodes, while Figure 8(b) shows the re-
sults in a system where 20% of the messages are lost. Re-
call that even though in the node crashing scenario the num-
ber of nodes participating in the epoch decreases, the cor-
rect estimation is 105 as the protocol reports network size at
the beginning of the epoch. The results are quite encourag-
ing; by maintaining and exchanging just 20 numerical val-
ues (resulting in messages of still only a few hundreds of
bytes), the accuracy that may be obtained is very high, es-
pecially considering the hostility of the scenarios tested.
8. Related Work
Protocols based on the analogy with the spreading of epi-
demics have found many applications. Examples include
database replication [2] and failure detection [12].
The idea of aggregation was pionereed by Van Renesse in
Astrolabe [10, 11]. In Astrolabe, a hierarchical architecture
is deployed. While this approach reduces the cost of ﬁnd-
ing the aggregates and enables the execution of complex,
database-style queries, the maintenance of the hierarchical
topology introduces additional overhead. Our anti-entropy
aggregation is substantially different, since it is extremely
simple, lightweight, and targeted to unstructured, highly dy-
namic environments. Furthermore, our protocol is proactive:
the result of aggregation is known to all nodes.
Kempe et al. [8] propose an aggregation protocol similar
to ours. The main difference is that their protocol is based
on push-only gossiping mechanisms. Furthermore, their dis-
cussion is limited to the theoretical version of the proto-
col, while this paper discusses at length the practical de-
tails needed for a real implementation and assesses its per-
formance in a faulty environment.
9. Conclusions
In this paper we have presented a full-ﬂedged proactive
aggregation protocol and we have demonstrated its adaptiv-
ity and robustness to benign failure through theoretical an
experimental analysis.
We have seen that a reasonably good approximation can
be obtained even if 75% of the nodes crash during the calcu-
lation of the aggregate. Furthermore, the protocol is totally
insensitive to link failure, which results only in proportional
slowdown but no approximation error. In the case of mes-
sage loss we did not see this extreme and exceptional robust-
ness, but the protocol is still reliable under usual, reasonable
levels of message loss. However, as was shown, robustness
to message loss can be greatly improved by the inexpensive
and simple extension of running multiple instances of the
protocol at the same time and calculate the ﬁnal approxima-
tion based on the output of the parallel instances. In the case
of crash and link failure our experimental results are sup-
ported by theoretical analysis as well.
In short, based on the presented results we can conclude
that our aggregation protocol is robust to benign failure and
can cope with an extremely high level of dynamism. The
fact that our experiments were carried out using the worst
case peak distribution further conﬁrms the practical applica-
bility of the protocol.
References
[1] A.-L. Barab´asi. Linked: the new science of networks. Perseus,
Cambridge, Mass., 2002.
[2] A. Demers, D. Greene, C. Hauser, W. Irish, J. Larson,
S. Shenker, H. Sturgis, D. Swinehart, and D. Terry. Epidemic
Algorithms for Replicated Database Management. In Proc.
of PODC’87, Vancouver, Aug. 1987. ACM.
[3] I. Gupta, R. van Renesse, and K. P. Birman. Scalable fault-
In Proc. of
tolerant aggregation in large process groups.
DSN’01, G¨oteborg, Sweden, 2001.
[4] M. Jelasity, W. Kowalczyk, and M. van Steen. Newscast com-
puting. Technical Report IR-CS-006, Vrije Universiteit Am-
sterdam, Department of Computer Science, Amsterdam, The
Netherlands, Nov. 2003.
[5] M. Jelasity and A. Montresor. Epidemic-Style Proactive Ag-
gregation in Large Overlay Networks. In Proc. of the 24th
International Conference on Distributed Computing Systems
(ICDCS 2004), Tokyo, Japan, 2004.
[6] M. Jelasity, A. Montresor, and O. Babaoglu. A modu-
lar paradigm for building self-organizing peer-to-peer ap-
plications.
In Proc. of the 1st Internation Workshop on
Engineering Self-Organizing Applications (ESOA’03), Mel-
bourne, Australia, 2003.
[7] M. Jelasity, A. Montresor, and O. Babaoglu. Towards se-
cure epidemics: Detection and removal of malicious peers in
epidemic-style protocols. Technical Report UBLCS-2003-
14, University of Bologna, Bologna, Italy, Nov. 2003.
[8] D. Kempe, A. Dobra, and J. Gehrke. Computing aggregate
information using gossip. In Proc. of FOCS’03, 2003.
[9] M. Ripeanu, I. Foster, and A. Iamnitchi. Mapping the
gnutella network: Properties of large-scale peer-to-peer sys-
tems and implications for system design. IEEE Internet Com-
puting Journal, 6(1), 2002.
[10] R. van Renesse.
The importance of aggregation.
In
A. Schiper, A. A. Shvartsman, H. Weatherspoon, and B. Y.
Zhao, editors, Future Directions in Distributed Computing,
LNCS 2584. Springer, 2003.
[11] R. Van Renesse, K. P. Birman, and W. Vogels. Astrolabe: A
robust and scalable technology for distributed system moni-
toring, management, and data mining. ACM Transactions on
Computer Systems, 21(2), May 2003.
[12] R. van Renesse, Y. Minsky, and M. Hayden. A gossip-style
In N. Davies, K. Raymond, and
failure detection service.
J. Seitz, editors, Middleware ’98. Springer, 1998.
[13] D. Watts. Small Worlds: The Dynamics of Networks Between
Order and Randomness. Princeton University Press, 1999.
[14] D. J. Watts and S. H. Strogatz. Collective dynamics of ’small-
world’ networks. Nature, 393, 1998.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:52:01 UTC from IEEE Xplore.  Restrictions apply.