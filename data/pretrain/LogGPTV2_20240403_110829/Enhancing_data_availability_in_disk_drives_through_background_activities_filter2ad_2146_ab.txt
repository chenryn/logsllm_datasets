### Table 2: Overall Characteristics of Traces

| Trace | Mean Arrival (ms) | Mean Service (ms) | Utilization (%) | Mean Idle (ms) | CV Burstiness |
|-------|------------------|------------------|-----------------|----------------|---------------|
| T1    | 62.85            | 10.68            | 17              | 91.98          | 0.98 No       |
| T2    | 96.72            | 4.20             | 4.2             | 236.08         | 6.41 No       |
| T3    | 252.29           | 5.59             | 2.2             | 760.84         | 3.79 Yes      |

### Evaluation of Scrubbing and Parity Updates

This section focuses on the evaluation of scrubbing and parity updates related to intra-disk data redundancy as background activities. Scrubbing is an infinite background process because, upon completion of one entire disk scan, a new scan starts. Parity updates, on the other hand, depend on the WRITE user traffic and are considered finite background processes.

#### Parameters for Scrubbing and Intra-Disk Parity Update

**Scrubbing:**
- Abstracted as a long, preemptive background job at the level of a single disk request.
- One complete scrubbing involves scanning a 40GB disk, requiring 100,000 disk I/Os, each scanning approximately 1000 sectors.
- The average time for a single disk I/O during scrubbing is 6.0 ms, drawn from an exponential distribution.
- The total time to serve 100,000 disk I/Os corresponds to the average scrubbing time.

**Parity Updates:**
- Abstracted as short, non-preemptive background jobs.
- Each parity update consists of:
  - One READ (average 10 ms, exponentially distributed)
  - One WRITE (average 5 ms, exponentially distributed)
- If a parity update is preempted after the READ, it must restart during another idle period.
- Parity updates are served in a first-come-first-served (FCFS) fashion.

#### Scheduling Policies and Performance Degradation

Scrubbing and intra-disk parity updates are scheduled using three policies outlined in Section 4. A 5% to 10% slowdown in foreground traffic is considered transparent to the user. Therefore, we set the pre-defined limit on performance degradation to 7%. All three policies degrade the performance of user traffic by restricting the amount of background jobs served. Their efficiency in completing background tasks (scrubbing or parity updates) depends on the variability of idle times in traces T1, T2, and T3.

### Disk Scrubbing

Background media scans (scrubbing) detect possible media errors on disk drives, preventing data loss due to latent sector errors. Scrubbing is completed in the background and can be conducted by the storage system or the disk drive itself. We evaluate the effectiveness of scrubbing with the goal of degrading user traffic performance by at most 7%.

**MTTDL Calculation:**
- The mean detection time of sector errors (MDL) is set to 0.5 x average scrubbing time.
- Recovery times (MRL) are insignificant compared to detection times, so MRL is assumed to be 0.
- Table 3 shows the improvements in MTTDL when scrubbing is running versus when it is not.

| Trace | Policy   | Completed Media Scans | Mean Scrubbing Time (s) | System Utilization (%) |
|-------|----------|-----------------------|-------------------------|------------------------|
| T1    | body     | 6                     | 3,617.8                 | 33.1                   |
| T1    | tail     | 4                     | 6,484.7                 | 26.8                   |
| T2    | body     | 4                     | 11,519.6                | 9.7                    |
| T2    | tail     | 63                    | 726.4                   | 83.1                   |
| T3    | tail     | 20                    | 4,476.3                 | 14.3                   |
| T3    | tail+bursty | 94                  | 972.9                   | 62.6                   |

**MTTDL Improvement:**
- For lowly variable idle times (e.g., T1), utilizing the body of idle times results in faster scrubbing and better overall system utilization.
- For highly variable idle times (e.g., T2), the tail-based policy yields faster scrubbing and higher MTTDL improvement.
- For bursty idle times (e.g., T3), the tail+bursty policy performs best, with a five-fold improvement in mean scrubbing time.

### Intra-Disk Parity Updates

Intra-disk data redundancy requires maintaining updated parity, which becomes dirty if the corresponding data is modified. This extra work consists of an extra READ and an extra WRITE for each user-issued WRITE. Instantaneous parity (IP) updates cause performance degradation but provide the highest level of data reliability.

**MTTDL Estimation:**
- If IP is supported, MTTDL is calculated using Equation (1) with ML set as ML(2).
- If parity updates are delayed, MTTDL is modified as follows:
  \[
  \text{MTTDL} \approx p \cdot \text{MTTDL}_{\text{ML}(1)} + (1 - p) \cdot \text{MTTDL}_{\text{ML}(2)}
  \]
  where \( p \) is the probability that the parity is dirty, and \( \text{MTTDL}_{\text{ML}(1)} \) and \( \text{MTTDL}_{\text{ML}(2)} \) are computed using Equation (1) with \( \text{ML} = \text{ML}(1) \) and \( \text{ML}(2) \), respectively.

**Probability \( p \):**
\[
p \approx \frac{Q_{\text{Update}} \cdot \text{Length}_{\text{Parity segment}}}{\text{Capacity}_{\text{Disk}}}
\]
where \( Q_{\text{Update}} \) is the average number of dirty parities, \( \text{RT}_{\text{Update}} \) is the average parity update time, and \( \text{A}_{\text{Update}} \) is the arrival rate of parity updates.

### Conclusion

The analysis shows that the choice of scheduling policy significantly impacts the performance and reliability of the storage system. By carefully managing idle times, it is possible to achieve substantial improvements in MTTDL while limiting the degradation of user traffic performance to within acceptable limits.