T3 is bursty.
Tra Mean
Arrival
-ce
62.85
TI
96.72
T2
252.29
T3
Mean
Service (ms)
10.68
4.20
5.59
Util
(%)
17
4.2
2.2
Mean
Idle (ms)
91.98
236.08
760.84
CV Bur
Idle
-sty
0.98 No
6.41
No
3.79 Yes
Table 2. Overall characteristics of traces.
The focus here is on the evaluation of scrubbing and par(cid:173)
ity updates related to intra-disk data redundancy as back(cid:173)
ground activities. Scrubbing is an infinite background pro(cid:173)
cess because commonly upon completion ofone entire disk
scan, a new one starts. Parity updates depend on the WRITE
user traffic and are considered a finite background process.
The specific parameters of scrubbing and intra-disk parity
update used in our simulations are as follows.
Scrubbing is abstracted as a long background job that
is preemptive at the level of a single disk request. Hence,
it is assumed that an entire scan of a 40GB disk, i.e., one
complete scrubbing, requires 100,000 disk IDs, each scan(cid:173)
ning approximately 1000 sectors. Disk capacities of 40GB
might be conservative given that modem disk drives reach
capacities of up to 500GB, but we stress that the analysis
presented here is independent of the disk size. One single
disk scan request as part of the scrubbing job is assumed to
take on the average as much time as a READ disk request.
In our simulations, this time is drawn from an exponential
distribution with mean 6.0 ms. The time to serve 100,000
disk IDs as part of a single scrubbing corresponds the aver(cid:173)
age scrubbing time.
Parity updates are abstracted as short background jobs.
To update the parity of a segment of sectors, the follow(cid:173)
ing steps are taken. First, the entire set of sectors should
be read, then the parity must be calculated, and finally the
new parity is written on the disk. Therefore, each parity
update consists of one READ (assumed to take 10 ms on
the average) and one WRITE (assumed to take 5 ms on the
average), both exponentially distributed. The preemption
level ofparity updates is at the disk request level. If a parity
update is preempted after the READ, then the system main(cid:173)
tains no memory of the work done and the update has to
restart during another idle period. Parity updates are served
in a first-come-first-served (FCFS) fashion.
Scrubbing and intra-disk parity updates processes are
scheduled using the three policies outlined in Section 4. In
storage systems, a slowdown of the foreground traffic by
5% to 10% is considered transparent to the user. Hence, we
set the pre-defined limit on performance degradation to 7%.
All three policies degrade the performance of user traffic by
restricting the amount of background jobs served. Their ef(cid:173)
ficiency regarding the performance of timely completion of
background tasks (i.e., scrubbing or parity updates) depends
on the variability of idle times in traces Tl, T2, and T3.
6 Disk Scrubbing
Background media scans can be abstracted as an infinite
background process that detects any possible media errors
on disk drives and thus prevents data loss caused by latent
sector errors. As a preventive feature, scrubbing is com(cid:173)
pleted in the background and can be conducted by the stor(cid:173)
age system or the disk drive itself. Based on the system
specifications described in Section 5, we evaluate the effec(cid:173)
tiveness of scrubbing aiming at degrading performance of
user traffic by at most 7%.
The goal of scrubbing as a preventive background fea(cid:173)
ture is to improve the MTTDL. The average time of scrub(cid:173)
bing allows for MTTDL calculation when scrubbing is not
running and when it is running, using Equations (1) and
(2), respectively. The mean detection time of sector er(cid:173)
rors (MDL) in Equation (2) is set to be equal to 0.5 x
average scrubbing time. Moreover, compared to detection
times, the recovery times of latent sector errors are insignif(cid:173)
icant (i.e., MRL « MDL). We thus assume MRL ~ 0 in
Equation (2). Table 3 gives the improvements in MTTDL
when scrubbing is running over the case when it is not run(cid:173)
ning, i.e., the ratio of the two MTTDLs. The results show
that when utilizing the body and the tail of idle times as ex(cid:173)
plained in Section 4, scrubbing dramatically improves relia(cid:173)
bility while the degradation in the performance ofuser traf(cid:173)
fic is limited to 7%. The overall improvement of MTTDL
1-4244-2398-9/08/$20.00 ©2008 IEEE
495
DSN 2008: Mi et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:12:52 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
because of scrubbing is 5 orders of magnitude. Differences
in the MTTDL improvement between the scheduling poli(cid:173)
cies are between 20% and 40%. The body-based policy
achieves better MTTDL improvement for low variable idle
times (e.g., trace Tl) than the tail-based policy. For T2 that
has high viability in idle times, the tail-based policy is supe(cid:173)
rior to the body-based policy. Additionally, the prediction
ofupcoming idle times further improves the system's relia(cid:173)
bility for trace T3 whose idle times are bursty.
Tra
-ce
Tl
T2
T3
Policy
body
tail
body
tail
tail
tail+
bursty
Completed Mean Scrubbing
Media Scans
Time (s)
System
Util (%)
6
4
4
63
20
94
3,617.8
6,484.7
11,519.6
726.4
4,476.3
972.9
33.1
26.8
9.7
83.1
14.3
62.6
Tl
body
0.4
xlO5
tail
0.3
xlO5
T2
body
0.3
xlO5
tail
0.5
xlO5
T3
tail+bursty
0.5
xlO5
tail
0.4
xlO5
Table 3. MTTDL improvement via scrubbing.
Table 4 also provides an explanation for the differences
in the MTTDL improvement between the three schedul(cid:173)
ing policies by presenting the number of completed media
scans, their average scrubbing time, and the overall system
utilization. For lowly variable idle times (e.g., Tl), uti(cid:173)
lizing the body rather than the tail of idle times results in
faster scrubbing and better overall system utilization.
In
particular, scrubbing under the body-based policy is twice
faster than under the tail-based policy (see first row of Ta(cid:173)
ble 4). Consequently, the faster scrubbing time under the
body-based policy yields the superior MTTDL improve(cid:173)
ments shown in Table 3. For T2 that has highly variable idle
times, the tail-based yields faster scrubbing, i.e., at least two
orders of magnitude difference than the body-based pol(cid:173)
icy, which results in higher MTTDL improvement. Further(cid:173)
more, for trace T2, the system is a lot better utilized under
the tail-based policy. Finally, if idle times are in addition
bursty (i.e., trace T3), then utilizing the tail of idle times
and predicting long idle periods performs better than utiliz(cid:173)
ing only the tail of idle times. Utilizing burstiness to benefit
scrubbing results in a five-fold improvement in mean scrub(cid:173)
bing time. The body-based policy is not evaluated for T3
because the results of T2 establish that the tail rather than
the body ofidle times should be used if idle times have high
variation.
In addition to the average performance presented in Ta(cid:173)
ble 4, we also evaluate the distribution of scrubbing time.
The distribution is built with a sample space of completed
scrubbing as large as 500 by replaying the traces several
times. Figure 2 shows the cumulative distribution function
(CDF) of scrubbing time for traces Tl, T2, and T3. For
all three traces, the best performing scheduling policy for
scrubbing identified in Table 4 achieves the shortest scrub(cid:173)
bing distribution tail. For trace Tl, see Figure 2(a), almost
100% of scrubbings have times less than 3831.9 seconds
under the body-based policy while a twice larger scrubbing
time is achieved for only 1.4% of scrubbings under the tail-
Table 4. Scrubbing performance for traces T1, T2, and
T3 under body-based, tail-based, and tail+bursty idle time
managing policies.
based policy. Similarly for trace T2, see Figure 2(b), the
tail of scrubbing time under the tail-based policy is about
7.5 times shorter than under the body-based policy. Ex(cid:173)
ploiting burstiness with the tail+bursty policy, as shown in
Figure 2(c), further reduces the tail of the scrubbing time
distribution.
7 Intra-disk Parity Updates
Intra-disk data redundancy requires maintaining updated
parity that becomes dirty if the corresponding data is mod(cid:173)
ified [3, 11]. This extra amount of work required to main(cid:173)
tain updated parity consists of an extra READ and an ex(cid:173)
tra WRITE for each user-issued WRITE. Completing this
work instantaneously upon completion of each user-issued
WRITE is called instantaneous parity (IP) update. Natu(cid:173)
rally, IP causes degradation in user performance because it
is not a preemptable task, but provides the highest level of
data reliability.
Here, we show that it is possible to complete parity up(cid:173)
dates as a background job in a timely fashion, while keep(cid:173)
ing user performance slowdown within the predefined target
7%. We quantify how the amount ofdelay in intra-disk par(cid:173)
ity updates affects data reliability for the three idle schedul(cid:173)
ing techniques. The effectiveness of the idle scheduling
policies are evaluated in comparison to IP updates.
7.1 MTTDL in Data Redundant Drives
The estimation of MTTDL for disks with intra disk re(cid:173)
dundancy is based on Equation (1). Assuming that latent
sector errors are spatially and temporally correlated [1], the
improvement in the mean interarrival time of latent sec(cid:173)
tor errors is 0.48 x 102 [3], or equivalently, M L (2) =
0.48 X 102 . M L(l), where M £(1) represents the mean in(cid:173)
terarrival time of latent errors if there is no intra-disk data
redundancy, see Table 1. M L (2) represents the mean inter(cid:173)
arrival time of latent errors if there is intra-disk data redun(cid:173)
dancy.
1-4244-2398-9/08/$20.00 ©2008 IEEE
496
DSN 2008: Mi et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:12:52 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems &Networks: Anchorage, Alaska, June 24-27 2008
(a) Tl: low CV
(b) T2: high CV
(c) T3: high CV & bursty
100
90
80
70
~ 60
~
50
"'C 40
30
20
10
0
~
(.)
body-based -
tail-based ..........
0
2
3
4
scrubbing time (1OOOs)
5
100 ."
90 !
80 ;
70 ~
~ 60 ;
":::R
Q
t+-4
-- 50 ;
"'C 40 ~
30 ~
20 ;
10 :
0:
0
(.)
100
90
80
70
~ 60
~
50
t+-4
"'C 40
30
20
10
0
Co)
body-based -
tail-based ..........
5
15 20 25 30 35 40
10
scrubbing time (1 OOOs)
tail-based -
tail+bursty
.
0
2 3 4 5 6 7 8 9 10
scrubbing time (1 OOOs)
6
7
Figure 2. CDF of scrubbing time distribution for traces (a) T1, (b) T2, and (c) T3.
If instantaneous parity (IP) is supported (i.e., parity up(cid:173)
dates occur without delay), then MTTDL is calculated using
Equation (1), where the parameter M L is set as M L (2). If
parity updates are delayed, then Equation (1) is modified as
follows:
MTTDL ~ p. MTTDLML(l)
(3)
+(1- p) . MTTDL ML (2) ,
where p represents the probability that the parity is dirty,
and MTTDLML(l) and MTTDL ML (2) are computed us(cid:173)
ing Equation (1) with the parameter ML equal to ML(l)
and M L (2), respectively. We assume that if the parity is
dirty, then latent errors arrive in intervals of M L (1), and
that if parity is updated, then errors arrive in intervals of
M L(2). We approximate p as the portion of the disk with
dirty parity as follows:
p ~
QLvpdate . LengthParity segment
CapacitYDisk
(4)
RTUpdate . AVpdate · Lengthparity segment
CapacitYDisk
where QL Update is the average number of dirty pari(cid:173)
ties in the disk, RTu pdate is the average parity update
time, Aupdate is the arrival rate of parity updates and