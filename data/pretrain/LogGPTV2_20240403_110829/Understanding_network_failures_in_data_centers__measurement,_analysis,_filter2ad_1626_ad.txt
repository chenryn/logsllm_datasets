that LB-1 and LB-3 load balancers experience the shortest failures
with median time to repair of 3.7 and 4.5 minutes, respectively,
indicating that most of their faults are short-lived.
ToRs experience correlated failures. When considering time to
repair for devices, we observe a correlated failure pattern for ToRs.
Speciﬁcally, these devices tend to have several discrete “steps” in
the CDF of their failure durations. These steps correspond to spikes
in speciﬁc duration values. On analyzing the failure logs, we ﬁnd
that these spikes are due to groups of ToRs that connect to the same
(or pair of) AggS going down at the same time (e.g., due to main-
tenance or AggS failure).
Inter-data center links take the longest to repair. Figure 9 (a)
shows the distribution of time to repair for different link types. The
majority of link failures are resolved within ﬁve minutes, with the
exception of links between data centers which take longer to re-
pair. This is because links between data centers require coordina-
tion between technicians in multiple locations to identify and re-
solve faults as well as additional time to repair cables that may be
in remote locations.
4.5.2 Time between failures
We next consider the time between failure events. Since time
between failure requires a network element to have observed more
than a single failure event, this metric is most relevant to elements
that are failure prone. Speciﬁcally, note that more than half of all
elements have only a single failure (cf. Table 5), so the devices and
links we consider here are in the minority.
Load balancer failures are bursty. Figure 8 (b) shows the distri-
bution of time between failures for devices. LBs tend to have the
shortest time between failures, with a median of 8.6 minutes and
16.4 minutes for LB-1 and LB-2, respectively. Recall that failure
events for these two LBs are dominated by a small number of de-
vices that experience numerous failures (cf. Table 5). This small
number of failure prone devices has a high impact on time between
failure, especially since more than half of the LB-1 and LB-2 de-
vices experience only a single failure.
In contrast to LB-1 and LB-2, devices like ToR-1 and AggS-1
have median time between failure of multiple hours and LB-3 has
median time between failure of more than a day. We note that the
LB-3 device is a newer version of the LB-1 and LB-2 devices and
it exhibits higher reliability in terms of time between failures.
356Link ﬂapping is absent from the actionable network logs. Fig-
ure 9 (b) presents the distribution of time between failures for the
different link types. On an average, link failures tend to be sepa-
rated by a period of about one week. Recall that our methodology
leverages actionable information, as determined by network oper-
ators. This signiﬁcantly reduces our observations of spurious link
down events and observations of link ﬂapping that do not impact
network connectivity.
MGMT, CORE and ISC links are the most reliable in terms
of time between failures, with most link failures on CORE and ISC
links occurring more than an hour apart. Links between data cen-
ters experience the shortest time between failures. However, note
that links connecting data centers have a very low failure probabil-
ity. Therefore, while most links do not fail, the few that do tend to
fail within a short time period of prior failures. In reality, multiple
inter-data center link failures in close succession are more likely to
be investigated as part of the same troubleshooting window by the
network operators.
4.5.3 Reliability of network elements
We conclude our analysis of failure properties by quantifying
the aggregate downtime of network elements. We deﬁne annualized
downtime as the sum of the duration of all failures observed by a
network element over a year. For link failures, we consider failures
that impacted network trafﬁc, but highlight that a subset of these
failures are due to planned maintenance. Additionally, redundancy
in terms of network, application, and data in our system implies that
this downtime cannot be interpreted as a measure of application-
level availability. Figure 8 (c) summarizes the annual downtime for
devices that experienced failures during our study.
Data center networks experience high availability. With the ex-
ception of ToR-1 devices, all devices have a median annual down-
time of less than 30 minutes. Despite experiencing the highest num-
ber of failures, LB-1 devices have the lowest annual downtime. This
is due to many of their failures being short-lived. Overall, devices
experience higher than four 9’s of reliability with the exception
of ToRs, where long lived correlated failures cause ToRs to have
higher downtime; recall, however, that only 3.9% of ToR-1s expe-
rience any failures (cf. Figure 4).
Annual downtime for the different link types are shown in Fig-
ure 9 (c). The median yearly downtime for all link types, with the
exception of links connecting data centers is less than 10 minutes.
This duration is smaller than the annual downtime of 24-72 minutes
reported by Turner et al. when considering an academic WAN [26].
Links between data centers are the exception because, as observed
previously, failures on links connecting data centers take longer to
resolve than failures for other link types. Overall, links have high
availability with the majority of links (except those connecting data
centers) having higher than four 9’s of reliability.
4.6 Grouping link failures
We now consider correlations between link failures. We also
analyzed correlated failures for devices, but except for a few in-
stances of ToRs failing together, grouped device failures are ex-
tremely rare (not shown).
To group correlated failures, we need to deﬁne what it means
for failures to be correlated. First, we require that link failures occur
in the same data center to be considered related (since it can be the
case that links in multiple data centers fail close together in time but
are in fact unrelated). Second, we require failures to occur within a
predeﬁned time threshold of each other to be considered correlated.
When combining failures into groups, it is important to pick an
appropriate threshold for grouping failures. If the threshold is too
]
x
<
X
P
[
0
1
.
8
0
.
6
.
0
4
0
.
2
0
.
0
.
0
1
2
5
10
20
50
100
200
Size of link failure group (links)
Figure 10: Number of links involved in link failure groups.
Table 6: Examples of problem types
Problem Type
Change
Incident
Network Connection OSPF convergence, UDLD errors,
Example causes or explanations
Device deployment, UPS maintenance
OS reboot (watchdog timer expired)
Software
Hardware
Conﬁguration
Cabling, Carrier signaling/timing issues
IOS hotﬁxes, BIOS upgrade
Power supply/fan, Replacement of
line card/chassis/optical adapter
VPN tunneling, Primary-backup failover,
IP/MPLS routing
small, correlated failures may be split into many smaller events. If
the threshold is too large, many unrelated failures will be combined
into one larger group.
We considered the number of failures for different threshold
values. Beyond grouping simultaneous events, which reduces the
number of link failures by a factor of two, we did not see signiﬁcant
changes by increasing the threshold.
Link failures tend to be isolated. The size of failure groups pro-
duced by our grouping method is shown in Figure 10. We see that
just over half of failure events are isolated with 41% of groups con-
taining more than one failure. Large groups of correlated link fail-
ures are rare with only 10% of failure groups containing more than
four failures. We observed two failure groups with the maximum
failure group size of 180 links. These were caused by scheduled
maintenance to multiple aggregation switches connected to a large
number of ToRs.
4.7 Root causes of failures
Finally, we analyze the types of problems associated with de-
vice and link failures. We initially tried to determine the root cause
of failure events by mining diaries associated with NOC tickets.
However, the diaries often considered multiple potential causes for
failure before arriving at the ﬁnal root cause, which made min-
ing the text impractical. Because of this complication, we chose
to leverage the “problem type” ﬁeld of the NOC tickets which al-
lows operators to place tickets into categories based on the cause of
the problem. Table 6 gives examples of the types of problems that
are put into each of the categories.
Hardware problems take longer to mitigate. Figure 11 consid-
ers the top problem types in terms of number of failures and total
downtime for devices. Software and hardware faults dominate in
357 
e
g
a
t
n
e
c
r
e
P
80%
70%
60%
50%
40%
30%
20%
10%
0%
failures
downtime
72% 
31% 
33% 
15% 
13% 
3% 
4% 
5% 
5% 
6% 
Change
Incident
SW
Net.
Conn.
Problem type 
HW
Figure 11: Device problem types.
5% 
1% 
Config
]
x
<
X
P
[
0
.
1
8
.
0
6
.
0
4
.
0
2
.
0
0
.
0
failures
downtime
33% 
26% 
15% 
4% 
8% 
3% 
Change
Incident
14% 
7% 
SW
Net.
Conn.
Problem type 
38% 
20% 
11% 
3% 
HW
Config
Figure 12: Link problem types.
e
g
a
t
n
e
c
r
e
P
50%
45%
40%
35%
30%
25%
20%
15%
10%
5%
0%
]
x
<
X
P
[
0
.
1
8
.
0
6
.
0
4
.
0
2
.
0
0
.
0
1e−03
1e−01
1e+01
1e+03
1e+05
1e−03
1e−01
1e+01
1e+03
1e+05
Packets lost (millions)
Traffic loss (GB)
Figure 13: Estimated packet loss during failure events.
Figure 14: Estimated trafﬁc loss during failure events.
terms of number of failures for devices. However, when consider-
ing downtime, the balance shifts and hardware problems have the
most downtime. This shift between the number of failures and the
total downtime may be attributed to software errors being allevi-
ated by tasks that take less time to complete, such as power cy-
cling, patching or upgrading software. In contrast, hardware errors
may require a device to be replaced resulting in longer repair times.
Load balancers affected by software problems. We examined
what types of errors dominated for the most failure prone device
types (not shown). The LB-1 load balancer, which tends to have
short, frequent failures and accounts for most failures (but relatively
low downtime), mainly experiences software problems. Hardware
problems dominate for the remaining device types. We observe that
LB-3, despite also being a load balancer, sees much fewer software
issues than LB-1 and LB-2 devices, suggesting higher stability in
the newer model of the device.
Link failures are dominated by connection and hardware prob-
lems. Figure 12 shows the total number of failures and total down-
time attributed to different causes for link failures. In contrast to
device failures, link failures are dominated by network connection
errors, followed by hardware and software issues. In terms of down-
time, software errors incur much less downtime per failure than
hardware and network connection problems. This suggests soft-
ware problems lead to sporadic short-lived failures (e.g., a software
bug causing a spurious link down notiﬁcation) as opposed to severe
network connectivity and hardware related problems.
5. ESTIMATING FAILURE IMPACT
In this section, we estimate the impact of link failures. In the
absence of application performance data, we aim to quantify the
impact of failures in terms of lost network trafﬁc. In particular, we
estimate the amount of trafﬁc that would have been routed on a
failed link had it been available for the duration of the failure.
In general, it is difﬁcult to precisely quantify how much data
was actually lost during a failure because of two complications.
First, ﬂows may successfully be re-routed to use alternate routes af-
ter a link failure and protocols (e.g., TCP) have in-built retransmis-
sion mechanisms. Second, for long-lived failures, trafﬁc variations
(e.g., trafﬁc bursts, diurnal workloads) mean that the link may not
have carried the same amount of data even if it was active. There-
fore, we propose a simple metric to approximate the magnitude of
trafﬁc lost due to failures, based on the available data sources.
To estimate the impact of link failures on network trafﬁc (both
in terms of bytes and packets), we ﬁrst compute the median number
of packets (or bytes) on the link in the hours preceding the failure
event, medb, and the median packets (or bytes) during the failure
medd. We then compute the amount of data (in terms of packets or
bytes) that was potentially lost during the failure event as:
loss = (medb − medd) × duration
where duration denotes how long the failure lasted. We use me-
dian trafﬁc instead of average to avoid outlier effects.
358As described in Section 2, the network trafﬁc in a typical data
center may be classiﬁed into short-lived, latency-sensitive “mice”
ﬂows and long-lived, throughput-sensitive “elephant” ﬂows. Packet
loss is much more likely to adversely affect “mice” ﬂows where
the loss of an ACK may cause TCP to perform a timed out retrans-
mission. In contrast, loss in trafﬁc throughput is more critical for
“elephant” ﬂows.
Link failures incur loss of many packets, but relatively few bytes.
For link failures, few bytes are estimated to be lost relative to the
number of packets. We observe that the estimated median number
of packets lost during failures is 59K (Figure 13) but the estimated
median number of bytes lost is only 25MB (Figure 14). Thus, the
average size of lost packets is 423 bytes. Prior measurement study
on data center network trafﬁc observed that packet sizes tend to
be bimodal with modes around 200B and 1,400B [5]. This sug-
gests that packets lost during failures are mostly part of the lower
mode, consisting of keep alive packets used by applications (e.g.,
MYSQL, HTTP) or ACKs [5].
5.1 Is redundancy effective in reducing impact?
In a well-designed network, we expect most failures to be
masked by redundant groups of devices and links. We evaluate
this expectation by considering median trafﬁc during a link fail-
ure (in packets or bytes) normalized by median trafﬁc before the
failure: medd/medb; for brevity, we refer to this quantity as “nor-
malized trafﬁc”. The effectiveness of redundancy is estimated by
computing this ratio on a per-link basis, as well as across all links
in the redundancy group where the failure occurred. An example