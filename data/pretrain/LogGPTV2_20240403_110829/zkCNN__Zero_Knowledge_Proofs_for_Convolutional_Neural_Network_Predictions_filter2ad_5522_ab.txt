the techniques in [23] are compatible with our scheme and inte-
grating them is left as future work. We compare our zkCNN with
vCNN and ZEN in Section 5.2 and show that the prover time of
zkCNN is orders of magnitude faster than vCNN and ZEN.
Ghodsi et al. proposed SafetyNets [25], a scheme to prove the
correctness of neural network inferences using the GKR protocol.
The scheme assumes that the model and the data are known to both
the prover and the verifier, and it is essentially a verifiable compu-
tation scheme where the verifier time is faster than computing the
inference locally. The scheme does not guarantee the privacy of the
Session 11B: Zero Knowledge II CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2969model or the input. The scheme in [56] verifies the correctness of
predictions of several machine learning models including shallow
neural network, and provides privacy and integrity simultaneously.
Keuffer et al. [29] propose a scheme to verify machine learning
models by combining the GKR protocol and QAP-based protocols
to handle linear layers and non-linear layers, respectively. Other
than neural networks, Zhang et al. [50] proposed an efficient zero
knowledge proof scheme for decision tree predictions and accuracy.
The techniques are dedicated to decision tree models. Wu et al. [47]
designed a distributed zero knowledge proof system and proved
the integrity of training a linear regression model as a benchmark.
Interactive proofs. In the seminal work of [26], Goldwasser et al.
proposed the doubly efficient interactive proof for layered arith-
metic circuits. Later, Cormode et al. improved the prover time of
the GKR protocol from 𝑂(|𝐶|3) to 𝑂(|𝐶| log |𝐶|) for circuits of size
|𝐶| using multilinear extensions in [20]. Several follow-up papers
further reduced the prover time for circuits with special struc-
tures [41, 43, 55]. Notably Justin Thaler proposed a highly-efficient
sumcheck protocol for matrix multiplications between 𝑛 × 𝑛 ma-
trices where the additional prover time is 𝑂(𝑛2), which is faster
than computing the result in 𝑂(𝑛3). Our sumcheck protocol for
FFT provides the another example where the prover time is even
faster than computing the result. Recently Xie et al. [48] proposed
a variant of the GKR protocol with 𝑂(|𝐶|) prover time for arbitrary
layered arithmetic circuits, and Zhang et al. [51] generalized the
GKR protocol to general circuits instead of layered circuit without
any overhead on the prover time.
In [53], Zhang et al. extended the GKR protocol to an argument
system using polynomial commitments. Subsequent works [39,
44, 48, 52, 54] followed the framework and constructed efficient
zero knowledge argument schemes based on interactive proofs. We
follow the same framework and constructs zkCNN schemes using
interactive proofs and polynomial commitments.
Zero knowledge proofs. Other than interactive proofs, there are
many recent schemes of zero knowledge succinct argument of
knowledge (zk-SNARK) [6, 8–10, 12–14, 18, 27, 30, 36, 37] based
on pairing, discrete-log, MPC-in-the-head and interactive oracle
proofs. They provide succinct proof size on the size of the statement,
but incur a high overhead on the running time and the memory
usage of the prover. Therefore, these systems are not able to scale to
the size of CNN predictions (see Section 5). Recent zero knowledge
protocols based on vector oblivious linear evaluation [7, 22, 45, 49]
are efficient in the execution time and the memory overhead, but
the communication is linear in the size of the statement (several
GBs in practice), and the schemes cannot be made non-interactive
and are not publicly verifiable.
2 PRELIMINARIES
We use negl(·) : N → N for the negligible function, where for
any positive polynomial 𝑓 (·), negl(𝑘) < 1
𝑓 (𝑘) for sufficiently large
integer 𝑘. Let 𝜆 denote the security parameter. “PPT" stands for prob-
abilistic polynomial time. We use 𝑓 (), 𝑔() for polynomials, 𝑥, 𝑦, 𝑧
for vectors of variables and 𝑔, 𝑢, 𝑣 for vectors of values. 𝑥𝑖 denotes
the 𝑖-th variable in 𝑥. For a multivariate polynomial 𝑓 , its “variable-
degree” is the maximum degree of 𝑓 in any of its variables. We
denote {0, 1, . . . , 𝑚 − 1} as [𝑚].
Figure 1: An example of convolutional neural network.
2.1 Convolutional Neural Networks
A convolutional neural network consists of a pipeline of layers,
typically including a convolutional layer, an activation layer and
a pooling layer. A series of the three previous layers is followed
by several fully connected layers and an output layer. Each layer
receives input and processes it to produce an output that serves
as input to the next layer. Figure 1 shows a simple example of a
convolutional neural network.
Convolutional layer. A convolutional layer computes the dot
product between a small weight matrix and the neighborhood of
an element in the input data; this process is repeated by sliding
the weight matrix step by step. The computation can be viewed
as a two-dimensional (2-D) convolution, and each weight matrix
is usually referred to as a kernel or filter. Formally, we define the
result of a 2-D convolution between two matrices 𝑋 and 𝑊 of size
𝑛 × 𝑛 and 𝑤 × 𝑤 as a (𝑛 − 𝑤 + 1) × (𝑛 − 𝑤 + 1) matrix 𝑈 = 𝑋 ∗ 𝑊
such that
𝑤−1,𝑤−1
𝑡=0,𝑙=0 𝑋 𝑗+𝑡,𝑘+𝑙 · 𝑊𝑡,𝑙
𝑈 𝑗,𝑘 =
(1)
for 𝑗, 𝑘 = 0, . . . , 𝑛 − 𝑤. 2 In convolutional neural networks, the
data samples are represented as matrices and 2-D convolutions
are applied in each layer. The input and the output of each layer
usually have an additional dimension, referred to as the channels.
For example, a colored image has 3 channels: red, green and blue.
After a convolution, the number of channels in the output is the
same as the number of kernels in a convolutional layer. We use
ch𝑖𝑛, ch𝑜𝑢𝑡 to denote the number of input and output channels in a
convolutional layer, respectively. Let 𝑊𝑖 be the model parameters
in layer 𝑖, which consists of ch𝑜𝑢𝑡,𝑖 matrices of size 𝑤𝑖 × 𝑤𝑖 × ch𝑖𝑛,𝑖.
The input 𝑋𝑖 is represented as ch𝑖𝑛,𝑖 matrices of size 𝑛𝑖 ×𝑛𝑖. We use
the notation 𝑊𝑖[𝜏, 𝜎, 𝑡, 𝑙] to represent 𝑊𝑖’s value at index (𝜏, 𝜎, 𝑡, 𝑙),
and 𝑋𝑖[𝜎, 𝑗, 𝑘] to represent 𝑋𝑖’s value at index (𝜎, 𝑗, 𝑘). Then the
convolutional layer 𝑖 for each data sample computes
𝑈𝑖[𝜏, 𝑗, 𝑘] =
(2)
Where 0 ≤ 𝜏 < ch𝑜𝑢𝑡,𝑖, 0 ≤ 𝑗, 𝑘 < 𝑛𝑖 − 𝑤𝑖 + 1. This is ch𝑖𝑛,𝑖 · ch𝑜𝑢𝑡,𝑖
2-D convolutions in the form of Equation 1.
(𝑤𝑖−1),(𝑤𝑖−1)
𝑋𝑖[𝜎, 𝑗, 𝑘] · 𝑊𝑖[𝜏, 𝜎, 𝑡, 𝑙].
ch𝑖𝑛,𝑖−1
𝜎=0
𝑡=0,𝑙=0
2Throughout this paper, the size of stride is 1 for simplicity, and thus no padding is
needed. Our result could be easily extended to other stride size.
Session 11B: Zero Knowledge II CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea29701
Activation layer. After the convolutional layer, an activation func-
tion 𝑓 is then applied to 𝑈𝑖 element-wise to build the nonlinear
relationships between input and output. Piecewise linear functions
such as ReLU 𝑓 (𝑥) = max(𝑥, 0) and Sigmoid function 𝑓 (𝑥) =
1+𝑒−𝑥
are the most commonly used activation functions due to their out-
standing performance in the training phase.
Pooling layer. Pooling layers are then used to reduce the dimen-
sions of the feature maps, thus reducing the number of parame-
ters to learn and the amount of computation performed in the
CNN. Two common pooling methods are average pooling and
max pooling, where AvgPool(𝑥0, ..., 𝑥𝑘−1) = (𝑥0+, ...,+𝑥𝑘−1)/𝑘 and
MaxPool(𝑥0, ..., 𝑥𝑘−1) = max𝑘(𝑥0, ..., 𝑥𝑘−1). The pooling layer slides
a kernel over the result of the activation layer 𝑓 (𝑈𝑖) and do the
above two operations within the region covered by the kernel. The
result of pooling, denoted as 𝑋𝑖+1, is then fed into the next convo-
lutional layer as the input.
Fully connected layer. Finally, at the end of a series of convo-
lutions, activations and poolings, several fully connected layers
are applied in CNN. In each fully connected layer, the input is
multiplied by a weight matrix and added with a bias vector.
Output layer. The output layer typically applies a Softmax func-
tion to compute a probability distribution for categorical classifi-
cation problems. In the inference phase, it is enough to calculate
the maximal value over the output of the last fully connected layer
as the prediction of the most likely outcome. Therefore, in our
construction, we omit the computation of Softmax.
Finally, to classify multiple input data samples, an additional
dimension is introduced to the input of each layer and the compu-
tations are performed independently on each data sample with the
same kernels, activations and pooling.
2.2 Interactive Proofs
An interactive proof is an interactive protocol between a prover P
and a verifier V. The protocol runs in several rounds, allowing V
to ask questions in each round based on P’s answers in previous
rounds. We formalize this in terms of P trying to convince V that
𝐶(𝑥) = 𝑦, and give the formal definitions below.
Definition 2.1. Let 𝐶 be a function. A pair of interactive ma-
chines ⟨P,V⟩ is an interactive proof for 𝐶 with soundness 𝜖 if the
following holds:
• Completeness. For every 𝑥 such that 𝐶(𝑥) = 𝑦 it holds that
Pr[⟨P,V⟩(𝑥) = 𝑎𝑐𝑐𝑒𝑝𝑡] = 1.
• 𝜖-Soundness. For any 𝑥 with 𝐶(𝑥) ≠ 𝑦 and any P∗ it holds that
Pr[⟨P∗,V⟩ = 𝑎𝑐𝑐𝑒𝑝𝑡] ≤ 𝜖
We say an interactive proof scheme has succinct proof size and
verifier time if they are 𝑂(polylog(|𝐶|, |𝑥|)).
Sumcheck Protocol. Sumcheck protocol is one of the most im-
2.2.1
portant interactive proofs in the literature. The sumcheck problem
is to sum a multivariate polynomial 𝑓 : Fℓ → F on all binary in-
puts:𝑏1,𝑏2,...,𝑏ℓ ∈{0,1} 𝑓 (𝑏1, 𝑏2, ..., 𝑏ℓ). Directly computing the sum
requires exponential time in ℓ, as there are 2ℓ combinations of
𝑏1, . . . , 𝑏ℓ. Lund et al. [35] proposed a sumcheck protocol that al-
lows a verifier V to delegate the computation to a computationally
Protocol 1 (Sumcheck). The protocol proceeds in ℓ rounds.

• In the first round, P sends a univariate polynomial
𝑏2,...,𝑏ℓ ∈{0,1} 𝑓 (𝑥1, 𝑏2, . . . , 𝑏ℓ) ,
𝑓1(𝑥1) 𝑑𝑒 𝑓
=
V checks 𝐻 = 𝑓1(0) + 𝑓1(1). Then V sends a random chal-
lenge 𝑟1 ∈ F to P.
• In the 𝑖-th round, where 2 ≤ 𝑖 ≤ ℓ − 1, P sends a univariate

polynomial
𝑓𝑖(𝑥𝑖) 𝑑𝑒 𝑓
𝑏𝑖+1,...,𝑏ℓ ∈{0,1} 𝑓 (𝑟1, . . . , 𝑟𝑖−1, 𝑥𝑖, 𝑏𝑖+1, . . . , 𝑏ℓ) ,
=
V checks 𝑓𝑖−1(𝑟𝑖−1) = 𝑓𝑖(0) + 𝑓𝑖(1), and sends a random
challenge 𝑟𝑖 ∈ F to P.
• In the ℓ-th round, P sends a univariate polynomial
𝑓ℓ(𝑥ℓ) 𝑑𝑒 𝑓
= 𝑓 (𝑟1, 𝑟2, . . . , 𝑟𝑙−1, 𝑥ℓ) ,
V checks 𝑓ℓ−1(𝑟ℓ−1) = 𝑓ℓ(0) + 𝑓ℓ(1). The verifier gener-
ates a random challenge 𝑟ℓ ∈ F. Given oracle access to an
evaluation 𝑓 (𝑟1, 𝑟2, . . . , 𝑟ℓ) of 𝑓 , V will accept if and only
if 𝑓ℓ(𝑟ℓ) = 𝑓 (𝑟1, 𝑟2, . . . , 𝑟ℓ). The instantiation of the oracle
access depends on the application of the sumcheck protocol.
unbounded prover P, who can convinceV that 𝐻 is the correct sum.
We provide a description of the sumcheck protocol in Protocol 1.
Lemma 1. Protocol 1 is an interactive proof for 𝐻 =𝑏1,𝑏2,...,𝑏ℓ ∈{0,1}
𝑓 (𝑏1, 𝑏2, ..., 𝑏ℓ) that is complete and sound with 𝜖 = ℓ|F| by Defini-
tion 2.1.
The proof size of the sumcheck protocol is 𝑂(ℓ) if the variable
degree of 𝑓 is constant, which is the case in our protocols. This
is because in each round, P sends a univariate polynomial of one
variable in 𝑓 , which is of constant size. The verifier time of the
protocol is 𝑂(ℓ). The prover time depends on the degree and the
sparsity of 𝑓 , and we will give the complexity later in our scheme.
Definition 2.2 (Identity function). Let 𝛽 : {0, 1}ℓ ×{0, 1}ℓ → {0, 1}
be the identity function such that 𝛽(𝑥, 𝑦) = 1 if 𝑥 = 𝑦, and 𝛽(𝑥, 𝑦) =
0 otherwise. Suppose ˜𝛽 is the multilinear extension of 𝛽. Then ˜𝛽
can be expressed as: ˜𝛽(𝑥, 𝑦) =ℓ
𝑖=1((1 − 𝑥𝑖)(1 − 𝑦𝑖) + 𝑥𝑖𝑦𝑖).
Definition 2.3 (Multilinear Extension [21]). Let 𝑉 : {0, 1}ℓ → F be
a function. The multilinear extension of 𝑉 is the unique polynomial