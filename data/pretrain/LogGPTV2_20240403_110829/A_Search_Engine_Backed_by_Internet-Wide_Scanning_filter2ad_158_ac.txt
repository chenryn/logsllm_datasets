.3 (.3)
.19 (.20)
.08 (.09)
2.8 (4.1)
6.6 (4.2)
2.5 (3.9)
6.4 (4.1)
1.9 (3.6)
1.0 (.2)
.05 (.001)
.02 (.0006)
.051 (.002)
(Hosts)
66.8 M
33.3 M
22.5 M
33.1 M
44.3 M
32 K
14.9 M
4.9 M
4.3 M
4.1 M
4.0 M
9.0 M
14.3 M
8.4 M
1.2 M
9.5 M
(Hosts)
77.3 M
47.1 M
43.1 M
47.1 M
55.1 M
2.0 M
22.9 M
7.9 M
6.9 M
8.8 M
6.6 M
14.7 M
14.3 M
12.4 M
1.6 M
9.5 M
Processed % Diﬀ
1 Day
Size (KB)
.32 (.096)
11.5%
8.5%
4.5 (1.4)
6.8%
.08 (.0001)
4.4%
2.3 (.002)
28.1%
.34 (.09)
10.6%
.10 (.08)
.33 (.31)
7.5%
3.3%
2.2 (8.9)
2.0%
4.9 (8.4)
4.4%
2.3 (.44)
1.9%
2.4 (.4)
1.5 (1.2)
5.8%
13.8%
.6 (.2)
29.8%
.145 (0)
92.8%
.145 (0)
.10 (0.0)
37.2%
Database
Size on Disk
10.9 GB
50.1 GB
1.5 GB
4.8 GB
6.5 GB
0.0 GB
9.0 GB
7.0 GB
11.5 GB
6.9 GB
6.9 GB
8.9 GB
5.8 GB
0.7 GB
0.1 GB
0.6 GB
Table 2: Scanned Protocols — We scan 16 protocols in our initial implementation. For each protocol and subprotocol we
scan, we show the average size and standard deviation for raw and transformed records, as well as the percent-change in
records across two days of scans. Most protocols have a less than a 15% turnover rate between consecutive days.
well as auxiliary collections of all seen X.509 certiﬁcates and
public keys.
Internally, data is stored on disk using RocksDB [21],
an embeddable key-value store optimized for ﬂash storage.
RocksDB buﬀers writes to a small in-memory table and on-
disk journal, and, in another thread, ﬂushes changes to a
log-structured merge-tree on disk. The records stored in
RocksDB consist of the serialized protobuf messages gen-
erated by the scan workers. We note that because data is
written to disk as a log-structured merge-tree, we maintain
a global ordering of all records, which we use to logically
group multiple records that describe a single host. Similarly,
records describing a single network are grouped together.
This allows us to eﬃciently generate daily snapshots of the
IPv4 address space by performing a single, linear pass of the
database, grouping together all records describing a single
host, and outputting a structured JSON document describing
all measured aspects of each host.
All of the functionality in ZDb could be achieved using
only RocksDB, but would require a disk read to process
every incoming record. To improve performance, we cache
the SHA-1 ﬁngerprints of current records, along with whether
the record were seen in the latest scan using an in-memory
Judy Array. With this additional optimization, we no longer
need to make random reads from RocksDB during processing
and can process all incoming records that contain no changes
without touching disk. We then update when each record
was seen at the end of the scan during the prune process,
which already performs a linear pass of the records on disk.
With these optimizations, ZDb is able to process 58k records
per second in the worst case, 137k records/second in the best
case, and 111k records/second in the daily workload using
the same metrics we used to measure MongoDB and Apache
Cassandra.
We would be remiss not to mention that because records
are queued in memory before they are processed, and because
we cache which records have been seen in memory until a scan
ﬁnishes, ZDb would lose the data associated with a particular
scan if the server crashed (e.g., due to a kernel panic). While
this is non-optimal, we ﬁnd this risk acceptable, because
a fresh scan can be completed in a matter of hours, which
6
would likely be similar to the amount of time needed to
investigate the crash, recover the database, and ﬁnish the
remainder of a scan. We take a similar approach to managing
failures during a scan. If a scan worker crashes, or if scan
validation fails for any reason, we start a new scan rather
than try to recover the previous scan.
4. EXPOSING DATA
To be successful, Censys needs to expose data back to
the community, which ranges from researchers who need
to quickly perform a simple query to those who want to
perform in-depth analysis on raw data. In order to meet
these disparate needs, we are exposing the data to researchers
through several interfaces, which oﬀer varying degrees of
ﬂexibility: (1) a web-based query and reporting interface,
(2) a programmatic REST API, (3) public Google BigQuery
tables, and (4) raw downloadable scan results. We further
plan to publish pre-deﬁned dashboards that are accessible
to users outside of the research community. In this section,
we describe each of these interfaces in depth.
4.1 Search Interface
True
The primary interface for Censys is a search engine that
allows researchers to perform full-text searches and struc-
tured queries against the most recent data for IPv4 hosts,
the Alexa Top 1 Million websites, and known certiﬁcates.
For example, a researcher can ﬁnd all hosts currently vul-
nerable to Heartbleed in the United States with the query:
443.https.heartbleed.vulnerable:
US. This query executes in
AND location.country_code:
approximately 250 ms and users are presented with the hosts
that meet the criteria, along with basic metadata, which
includes the breakdown of the top ASes, countries, and tags.
Users can view the details of any host, as well as generate
statistical reports.
Search Syntax. The search interface supports basic
predicate logic (e.g. (location.country_code:
location.country_code:
Apache), ranges (e.g., 80.http.http_status.code > 200),
wildcards (e.g., 443.https.certificate.certificate.
issuer.*:GoDaddy*) and regular expressions (e.g., 25.smtp.
US OR
CA) AND 80.http.server:
Interface Query
Web
Web
80.http.get.headers.server:*
443.https.tls.signature.valid:true AND
443.https.tls.version.name:SSLv3
25.smtp.banner.banner:gsmtp
ip:1.2.3.4
443.https.tls.certiﬁcate.issuer_dn
502.modbus.device_id.product_name
API
API
Report
Report
Time
218 ms
356 ms
82 ms
12 ms
417 ms
11 ms
Table 3: Censys Response Times — Censys can be used
to search records, and to create aggregations over ﬁelds in
the search results. Here, we show example searches and
aggregations along with their execution times. All of the
queries completed in under 500 ms.
\Apache.*\). Users can perform simple full-text
banner:
searches as well as query any structured ﬁeld generated
during the scan process, including user annotations and
system-maintained metadata (e.g., location and network
topology).
Viewing Individual Records. Users can view the de-
tails any host, certiﬁcate, or domain returned by a query.
This includes a user-friendly view of how each service is con-
ﬁgured, the most recent raw data describing the host, user-
provided metadata and tags, and historical scan data. We
similarly display geographic location, routing, and WHOIS
information.
Dynamic Reports. Once a query completes, users can
generate reports on the breakdown of any ﬁeld present on the
resulting datasets. For example, users can view the break-
down of server chosen cipher suites for IPv4 HTTPS hosts
with browser-trusted certiﬁcates by performing the query
True and
443.https.tls.validation.browser_trusted:
generating a report on 443.https.cipher_suite.name.
Backend. The search interface and reports are powered
by Elasticsearch [6], an open-source project that front-ends
Apache Lucene [4]. We maintain three indexes within Elas-
ticsearch: IPv4 hosts, Alexa Top 1 Million websites, and
all known certiﬁcates; ZDb updates the three indexes real
time. All updates also appended to a Google Cloud Data-
store collection, which is used to serve the history of each
host. Our web front-end is implemented in Python using
the Pylons Pyramid Framework, and is hosted on Google
App Engine. We plan to rate-limit the web interface, us-
ing session-based token buckets, in order to prevent screen
scraping and encourage developers to use the REST API we
describe in the next section for programmatic access. We
present the response time for sample queries in Table 3.
4.2 Programmatic Access
Censys has a programmatic API that provides equivalent
functionality as the search interface, but presents JSON re-
sults and follows the semantics of a REST API. For example,
researchers can get the history of an IPv4 host by doing a
GET request for https://censys.io/api/ipv4/8.8.8.8/history.
To prevent abuse, we require users to use a key, but are
happy to provide these to researchers.
4.3 SQL Interface
We recognize that not all research questions can be an-
swered through the search interface we described. This is
particularly true for historical queries, because we only ex-
pose the most recent data. To support more complex queries,
7
we are exposing Google BigQuery tables that contain the
daily ZDb snapshots of the IPv4 address space and Alexa
Top 1 Million Domains, along with our auxiliary collection
of certiﬁcates and public keys. Google BigQuery is a Dremel-
backed cloud database engine designed for performing large
analytical queries. Queries require 10–20 seconds to exe-
cute, but allow a full SQL syntax and are not restricted
to speciﬁc indexes. Authenticated researchers can perform
queries through the Censys web interface, or access the tables
directly using their own Google Cloud Accounts.
4.4 Raw Data
Lastly, we are publishing all of the raw data from our
scans, along with our curated ZDb snapshots of the IPv4
address space, Alexa Top 1 Million websites, and known
certiﬁcates. We will be posting these as structured JSON
documents, along with data deﬁnitions, and schemas for com-
mon databases at censys.io/data. We previously posted scan
data on https://scans.io, a generic scan data repository that
our team hosts. We will continue to maintain the scans.io
interface, provide continued access to our historical datasets,
and allow researchers to upload other data. However, we
will no longer post our regular scans to https://scans.io,
but rather encourage users to download these directly from
Censys’s web interface.
4.5 Protocol Dashboards
While Censys’s primary goal is to answer researchers’ spe-
ciﬁc queries, the backend similarly supports the types of
queries needed to generate pre-determined reports and dash-
boards. We plan to publish dashboards on Censys’ website,
which present various perspectives of how protocols are de-
ployed in practice. At initial release, we will be releasing a
Global HTTPS Dashboard that presents how well HTTPS
has been deployed in practice, an Alexa HTTPS Deploy-
ment Dashboard that shows historical trends in HTTPS de-
ployment and which high-ranking sites have not deployed
HTTPS, and dashboards for each of the recent HTTPS vul-
nerabilities, which will supersede the Heartbleed Bug Health
Report, POODLE Attack and SSLv3 Deployment, Tracking
the FREAK Attack, and Who is aﬀected by Logjam? sites.
Initially, dashboards will be statically deﬁned. However, we
encourage researchers to contribute reports at the completion
of research projects, and moving forward we hope to allow