scripts, similar to what some binary malware does [19]. We
Detection of evasive code. The detection of code that
behaves differently when run in an analysis environment
than when executed on a regular machine is a well-known
problem in the binary malware community. A number
of techniques have been developed to check if a binary is
running inside an emulator or a virtual machine [10, 30, 36].
In this context, evasive code consists of instructions that
produce different results or side-effects on an emulator and
on a real host [21,26]. The original malware code is modiﬁed
to run these checks: if the check identiﬁes an analysis system,
the code behaves in a benign way, thus evading the detection.
Researchers have dealt with such evasive checks in two
ways. First, they have designed systems that remain transpar-
ent to a wide range of malware checks [8, 39]. Second, they
have developed techniques to detect the presence of such
checks, for example by comparing the behavior of a sample
648  22nd USENIX Security Symposium 
USENIX Association
on a reference machine with that obtained by running it on
an analysis host [3, 15].
Similar to the case of evasions against binary analysis
environments, the results produced by honeyclients (i.e.,
the classiﬁcation of a web page as either malicious or be-
nign) can be confused by sufﬁciently-sophisticated evasion
techniques. Honeyclients are not perfect and attackers have
found ways to evade them [16, 31, 40]. For example, mali-
cious web pages may be designed to launch an exploit only
after they have veriﬁed that the current visitor is a regular
user, rather than an automated detection tool. A web page
may check that the visitor performs some activity, such as
moving the mouse or clicking on links, or that the browser
possesses the idiosyncratic properties of commonly-used
modern browsers, rather than being a simple emulator. If any
of these checks are not satisﬁed, the malicious web page will
refrain from launching the attack, and, as a consequence, will
be incorrectly classiﬁed as benign, thus evading detection.
The problem of evasive code in web attacks has only
recently been investigated. Kolbitsch et al. [17] have stud-
ied the “fragility” of malicious code, i.e., its dependence
for correct execution on the presence of a particular ex-
ecution environment (e.g., speciﬁc browser and plugins
versions). They report several techniques used by malicious
code for environment matching: some of these techniques
may well be used to distinguish analysis tools from regu-
lar browsers and evade detection. They propose ROZZLE,
a system that explores multiple execution paths in a pro-
gram, thus bypassing environment checks. Rozzle only
detects ﬁngerprinting that leverages control ﬂow branches
and depends upon the environment. It can be evaded by
techniques that do not need control-ﬂow branches, e.g.,
those based on browser or JavaScript quirks. For exam-
ple, the property window.innerWidth contains the
width of the browser window viewport in Firefox and
Chrome, and is undeﬁned in Internet Explorer. There-
fore, a malicious code that initialized a decoding key as
xorkey=window.innerWidth*0+3 would compute a differ-
ent result for xorkey in Firefox/Chrome (3) and IE (Not a
Number error), and could be used to decode malicious code
in speciﬁc browsers. Rozzle will not trigger its multi-path
techniques in such cases and can be evaded.
Revolver takes a different approach to identifying evasive
code in JavaScript programs. Instead of forcing an evasive
program to display its full behavior (by executing it in paral-
lel on a reference host and in an analysis environment [3],
or by forcing the execution through multiple, interesting
paths [17]), it leverages the existence of two distinct but sim-
ilar pieces of code and the fact that, despite their similarity,
they are classiﬁed differently by detection tools. In addition,
Revolver can precisely and automatically identify the code
responsible for an evasion.
JavaScript code analysis.
In the last few years, there
have been a number of approaches to analyzing JavaScript
code. For example, Prophiler [5] and ZOZZLE [7] have
used characteristics of JavaScript code to predict if a script
is malicious or benign. ZOZZLE, in particular, leverages
features associated with AST context information (such as,
the presence of a variable named scode in the context of a
loop), for its classiﬁcation.
Cujo [34] uses static and dynamic code features to identify
malicious JavaScript programs. More precisely, it processes
the static program and traces of its execution into q-grams
that are classiﬁed using machine learning techniques.
Revolver performs the core of its analysis statically, by
computing the similarity between pairs of ASTs. How-
ever, Revolver also relies on dynamic analysis, in particular
to obtain access to the code generated dynamically by a
script (e.g., via the eval() function), which is a common
technique used by obfuscated and malicious code.
Code similarity. The task of automatically detecting
“clones,” i.e., segments of code that are similar (accord-
ing to some notion of similarity), is an established line of
work in the software engineering community [27, 35]. Un-
fortunately, many of the techniques developed here assume
that the code under analysis is well-behaved or at least not
adversarial, that is, not actively trying to elude the classi-
ﬁcation. Of course, this assumption does not hold when
examining malicious code.
Similarity between malicious binaries has been used to
quickly identify different variants of the same malware
family. The main challenge in this context is dealing with
extremely large numbers of samples without source code and
large feature spaces from runtime data. Different techniques
have been proposed to overcome these issues: for example,
Bayer et al. [4] rely on locality sensitive hashing to reduce
the number of items to compare, while Jong et al. [14] use
feature hashing to reduce the number of features.
As a comparison, Revolver aims not only to identify pieces
of JavaScript code that are similar, but also to understand why
they differ and especially if these differences are responsible
for changing the classiﬁcation of the sample.
8 Conclusions
In this paper, we have introduced and demonstrated Re-
volver, a novel approach and tool for detecting malicious
JavaScript code similarities on a large scale. Revolver’s
approach is based on identifying scripts that are similar
and taking into account an Oracle’s classiﬁcation of every
script. By doing this, Revolver can pinpoint scripts that have
high similarity but are classiﬁed differently (detecting likely
evasion attempts) and improve the accuracy of the Oracle.
We performed a large-scale evaluation of Revolver by
running it in parallel with the popular Wepawet drive-by-
detection tool. We identiﬁed several cases of evasions that
are used in the wild to evade this tool (and, likely, other tools
USENIX Association  
22nd USENIX Security Symposium  649
based on similar techniques) and ﬁxed them, improving this
way the accuracy of the honeyclient.
Acknowledgements: This work was supported by the
Ofﬁce of Naval Research (ONR) under grant N00014-12-1-
0165 and under grant N00014-09-1-1042, and the National
Science Foundation (NSF) under grants CNS-0845559 and
CNS-0905537, and by Secure Business Austria.
References
[1] HtmlUnit.
net/.
http://htmlunit.sourceforge.
[2] JavaScript
for
Acrobat
API
Reference.
http://wwwimages.adobe.com/www.adobe.
com/content/dam/Adobe/en/devnet/
acrobat/pdfs/js_api_reference.pdf.
[3] D. Balzarotti, M. Cova, C. Karlberger, C. Kruegel,
E. Kirda, and G. Vigna. Efﬁcient Detection of Split
Personalities in Malware. In Proc. of the Symposium
on Network and Distributed System Security (NDSS),
2010.
[4] U. Bayer, P. M. Comparetti, C. Hlauschek, C. Kruegel,
and E. Kirda. Scalable, Behavior-Based Malware
Clustering. In Proc. of the Symposium on Network and
Distributed System Security (NDSS), 2009.
[5] D. Canali, M. Cova, G. Vigna, and C. Kruegel.
Prophiler: A Fast Filter for the Large-scale Detection
of Malicious Web Pages. In Proc. of the International
World Wide Web Conference (WWW), 2011.
[6] M. Cova, C. Kruegel, and G. Vigna. Detection and
Analysis of Drive-by-Download Attacks and Mali-
cious JavaScript Code. In Proc. of the International
World Wide Web Conference (WWW), 2010.
[7] C. Curtsinger, B. Livshits, B. Zorn, and C. Seifert.
Zozzle: Low-overhead Mostly Static JavaScript Mal-
ware Detection.
In Proc. of the USENIX Security
Symposium, 2011.
[8] A. Dinaburg, P. Royal, M. Sharif, and W. Lee. Ether:
Malware Analysis via Hardware Virtualization Exten-
sions. In Proc. of the ACM Conference on Computer
and Communications Security (CCS), 2008.
[9] D. Edwards. Dean Edwards Packer. http://bit.
ly/TWQ46b.
[10] P. Ferrie. Attacks on Virtual Machines. In Proc. of
the Association of Anti-Virus Asia Researchers Con-
ference, 2003.
[11] Google.
Safe Browsing API.
google.com/apis/safebrowsing/.
http://code.
[12] C. Grier, L. Ballard, J. Caballero, N. Chachra, C. J.
Dietrich, K. Levchenko, P. Mavrommatis, D. McCoy,
A. Nappa, A. Pitsillidis, N. Provos, M. Z. Raﬁque,
M. A. Rajab, C. Rossow, K. Thomas, V. Paxson, S. Sav-
age, and G. M. Voelker. Manufacturing Compromise:
The Emergence of Exploit-as-a-Service. In Proc. of the
ACM Conference on Computer and Communications
Security (CCS), 2012.
[13] B. Hartstein. jsunpack – a generic JavaScript unpacker.
http://jsunpack.jeek.org/dec/go.
[14] J. Jang, D. Brumley, and S. Venkataraman. BitShred:
Feature Hashing Malware for Scalable Triage and
Semantic Analysis. In Proc. of the ACM Conference
on Computer and Communications Security (CCS),
2011.
[15] M. G. Kang, H. Yin, S. Hanna, S. McCamant, and
D. Song. Emulating Emulation-Resistant Malware.
In Proc. of the Workshop on Virtual Machine Security
(VMSec), 2009.
[16] A. Kapravelos, M. Cova, C. Kruegel, and G. Vigna. Es-
cape from Monkey Island: Evading High-Interaction
Honeyclients. In Proc. of the Conference on Detection
of Intrusions and Malware & Vulnerability Assessment
(DIMVA), 2011.
[17] C. Kolbitsch, B. Livshits, B. Zorn, and C. Seifert.
Rozzle: De-Cloaking Internet Malware. In Proc. of
the IEEE Symposium on Security and Privacy, 2012.
[18] B. Krebs. Virus Scanners for Virus Authors, Part
http://krebsonsecurity.com/2010/
II.
04/virus-scanners-for-virus-authors-
part-ii/, 2010.
[19] F. Leder, B. Steinbock, and P. Martini. Classiﬁcation
and detection of metamorphic malware using value set
analysis. In Proc. of the Conference on Malicious and
Unwanted Software (MALWARE), 2009.
[20] L. Lu, V. Yegneswaran, P. Porras, and W. Lee. BLADE:
An Attack-Agnostic Approach for Preventing Drive-
By Malware Infections. In Proc. of the ACM Con-
ference on Computer and Communications Security
(CCS), 2010.
[21] L. Martignoni, R. Paleari, G. F. Roglia, and D. Bruschi.
Testing CPU Emulators. In Proc. of the International
Symposium on Software Testing and Analysis (ISSTA),
2009.
[22] Microsoft. Microsoft Security Intelligence Report,
Volume 13. Technical report, Microsoft Corporation,
2012.
650  22nd USENIX Security Symposium 
USENIX Association
[23] Moss. Moss with obfuscated scripts. http://goo.
gl/XzJ7M.
[24] M. Muja and D. G. Lowe. Fast Approximate Nearest
Neighbors with Automatic Algorithm Conﬁguration.
In Proc. of the Conference on Computer Vision Theory
and Applications (VISAPP), 2009.
[25] J. Nazario. PhoneyC: A Virtual Client Honeypot.
In Proc. of the USENIX Workshop on Large-Scale
Exploits and Emergent Threats (LEET), 2009.
[26] R. Paleari, L. Martignoni, G. F. Roglia, and D. Bruschi.
A Fistful of Red-Pills: How to Automatically Gen-
erate Procedures to Detect CPU Emulators. In Proc.
of the USENIX Workshop on Offensive Technologies
(WOOT), 2009.
[27] J. Pate, R. Tairas, and N. Kraft. Clone Evolution: a
Systematic Review. Journal of Software Maintenance
and Evolution: Research and Practice, 2011.
[28] N. Provos, P. Mavrommatis, M. Rajab, and F. Monrose.
All Your iFRAMEs Point to Us. In Proc. of the USENIX
Security Symposium, 2008.
[29] N. Provos, D. McNamee, P. Mavrommatis, K. Wang,
and N. Modadugu. The Ghost in the Browser: Analy-
sis of Web-based Malware. In Proc. of the USENIX
Workshop on Hot Topics in Understanding Botnet,
2007.
[30] T. Raffetseder, C. Kruegel, and E. Kirda. Detecting
System Emulators. In Proc. of the Information Security
Conference, 2007.
[31] M. A. Rajab, L. Ballard, N. Jagpal, P. Mavromma-
tis, D. Nojiri, N. Provos, and L. Schmidt. Trends in
Circumventing Web-Malware Detection. Technical
report, Google, 2011.
[32] P. Ratanaworabhan, B. Livshits, and B. Zorn. Nozzle:
A Defense Against Heap-spraying Code Injection At-
tacks. In Proc. of the USENIX Security Symposium,
2009.
[33] J. W. Ratclif. Pattern Matching: the Gestalt Approach.
Dr. Dobb’s, 1988.
[34] K. Rieck, T. Krueger, and A. Dewald. Cujo: Efﬁ-
cient Detection and Prevention of Drive-by-Download
Attacks. In Proc. of the Annual Computer Security
Applications Conference (ACSAC), 2010.
[35] C. K. Roy and J. R. Cordy. A Survey on Software
Clone Detection Research. Technical report, School
of Computing, Queen’s University, 2007.
[36] J. Rutkowska.
Red Pill. . . or how to de-
tect VMM using (almost) one CPU instruc-
tion.
http://www.invisiblethings.org/
papers/redpill.html, 2004.
[37] S. Schleimer, D. Wilkerson, and A. Aiken. Winnowing:
local algorithms for document ﬁngerprinting. In Proc.
of the 2003 ACM SIGMOD international conference
on Management of data, 2003.
[38] The Honeynet Project. Capture-HPC. https://
projects.honeynet.org/capture-hpc.
[39] A. Vasudevan and R. Yerraballi. Cobra: Fine-grained
Malware Analysis using Stealth Localized Executions.
In Proc. of the IEEE Symposium on Security and Pri-
vacy, 2006.
[40] Y.-M. Wang, D. Beck, X. Jiang, R. Roussev, C. Ver-
bowski, S. Chen, and S. King. Automated Web Patrol
with Strider HoneyMonkeys: Finding Web Sites That
Exploit Browser Vulnerabilities. In Proc. of the Sym-
posium on Network and Distributed System Security
(NDSS), 2006.
USENIX Association  
22nd USENIX Security Symposium  651