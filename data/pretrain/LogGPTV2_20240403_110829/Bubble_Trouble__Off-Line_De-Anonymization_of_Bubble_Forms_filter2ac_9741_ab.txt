generated a feature vector using PCA, geometric descrip-
tors, and color distribution, as described in Section 2.1.
We conducted two primary experiments and a number
of complementary experiments. The ﬁrst major test ex-
plores our ability to re-identify a respondent from a test
set of eight marks given a training set of twelve marks per
respondent. The second evaluates our ability to detect
when someone other than the ofﬁcial respondent com-
pletes a bubble form. To investigate the potential of
bubble markings and conﬁrm our results, we conducted
seven additional experiments. We repeated each experi-
ment ten times and report the average of these runs.
Recall from Section 2.2 that we can rank the respon-
dents based on how strongly we believe each one to be
responsible for a dot. For example, the respondent that
created a dot could be the ﬁrst choice or ﬁftieth choice
of our algorithms. A number of our graphs effectively
plot a cumulative distribution showing the percent of test
cases for which the true corresponding respondent falls
at or above a certain rank—e.g., for 75% of respondents
in the test set, the respondent’s true identity is in the top
three guesses.
3.1 Respondent Re-Identiﬁcation
This experiment measured the ability to re-identify in-
dividuals from their bubble marking patterns. For this
test, we trained our model using twelve sample bubbles
per respondent, including three bubbles for each answer
choice 1-4. Our test set for each respondent contained
the remaining two bubbles for each answer choice, for a
total of eight test bubbles. We applied the trained model
3To keep a relatively large number of surveys, we did not consider
the number of “5” answers and do not use these answers in our analysis.
4
Figure 5: Respondent re-identiﬁcation with 12 training
bubbles and 8 test bubbles per respondent.
Figure 6: False positive and false negative rates when
detecting unauthorized respondents.
to each of the 92 respondents’ test sets and determined
whether the predicted identity was correct.
To use multiple marks per respondent in the test set,
we classify the marks individually, yielding a distribu-
tion over the respondents for each mark in the set. After
obtaining the distribution for each test bubble in a group,
we combine this data by averaging the values for each
respondent. Our algorithms then order the respondents
from highest to lowest average conﬁdence, with highest
conﬁdence corresponding to the top choice.
On average, our algorithm’s ﬁrst guess identiﬁed the
correct respondent with 51.1% accuracy. The correct re-
spondent fell in the top three guesses 75.0% of the time
and in the top ten guesses 92.4% of the time. See Fig-
ure 5, which shows the percentage of test bubbles for
which the correct respondent fell at or above each pos-
sible rank. This initial result suggests that individuals
complete bubbles in a highly distinguishing manner, al-
lowing re-identiﬁcation with surprisingly high accuracy.
3.2 Detecting Unauthorized Respondents
One possible application of this technique is to detect
when someone other than the authorized respondent cre-
ates a set of bubbles. For example, another person might
take a test or survey in place of an authorized respondent.
We examined our ability to detect these cases by mea-
suring how often our algorithm would correctly detect a
fraudulent respondent who has claimed to be another re-
spondent. We trained our model using twelve training
samples from each respondent and examined the output
of our model when presented with eight test bubbles. The
distribution of these sets is the same as in Section 3.1.
For these tests, we set a threshold for the lowest rank
accepted as the respondent. For example, suppose that
the threshold is 12. To determine whether a given set of
test bubbles would be accepted for a given respondent,
we apply our trained model to the test set. If the respon-
dent’s identity appears in any of the top 12 (of 92) posi-
tions in the ranked list of respondents, that test set would
be accepted for the respondent. For each respondent, we
apply the trained model both to the respondent’s own test
bubbles and to the 91 other respondents’ test bubbles.
We used two metrics to assess the performance of our
algorithms in this scenario. The ﬁrst, false positive rate,
measures the probability that a given respondent would
be rejected (labeled a cheater) for bubbles that the re-
spondent actually completed. The second metric, false
negative rate, measures the probability that bubbles com-
pleted by any of the 91 other respondents would be ac-
cepted as the true respondent’s. We varied the threshold
from 1 to 92 for our tests. We expected the relationship
between threshold and false negative rate to be roughly
linear: increasing the threshold by 1 increases the proba-
bility that a respondent randomly falls above the thresh-
old for another respondent’s test set by roughly 1/92.4
Our results are presented in Figure 6. As we increase
the threshold, the false positive rate drops precipitously
while the false negative rate increases roughly linearly.
If we increase the threshold to 8, then a fraudulent re-
spondent has a 7.8% chance of avoiding detection (by
being classiﬁed as the true respondent), while the true
respondent has a 9.9% chance of being mislabeled a
cheater. These error rates intersect with a threshold ap-
proximately equal to 9, where the false positive and false
negative rates are 8.8%.
4This is not exact because the order of these rankings is not entirely
random. After all, we seek to rank a respondent as highly as possible
for the respondent’s own test set.
5
11020304050607080900102030405060708090100Rank of Correct RespondentCumulative %  12 Training, 8 Test   1510152025303540455001020304050Error Rate (%)Threshold  False Positive RateFalse Negative RateFigure 7: Respondent re-identiﬁcation accuracy using
lower-resolution images. Note that the 1200, 600, 300,
and 150 DPI lines almost entirely overlap.
Figure 8: One marked bubble per respondent in each of
the training and test sets. The expected value from ran-
dom guessing is provided as reference.
3.3 Additional Experiments
To study the information conveyed by bubble markings
and support our results, we performed seven comple-
mentary experiments.
In the ﬁrst, we evaluate the ef-
fect that scanner resolution has on re-identiﬁcation ac-
curacy. Next, we considered our ability to re-identify a
respondent from a single test mark given a training set
containing a single training mark from each respondent.
Because bubble forms typically contain multiple mark-
ings, this experiment is somewhat artiﬁcial, but it hints
at the information available from a single dot. The third
and fourth supplemental experiments explored the ben-
eﬁts of increasing the training and test set sizes respec-
tively while holding the other set to a single bubble. In
the ﬁfth test, we examined the tradeoff between training
and test set sizes. The ﬁnal two experiments validated
our results using additional gray bubbles from the sam-
ple surveys and demonstrated the beneﬁts of our feature
set over PCA alone. As with the primary experiments,
we repeated each experiment ten times.
Effect of resolution on accuracy.
In practice, high-
resolution scans of bubble forms may not be available,
but access to lower resolution scans may be feasible. To
determine the impact of resolution on re-identiﬁcation
accuracy, we down-sampled each ballot from the orig-
inal 1200 DPI to 600, 300, 150, and 48 DPI. We then
repeated the re-identiﬁcation experiment of Section 3.1
on bubbles at each resolution.
Figure 7 shows that decreasing the image resolution
has little impact on performance for resolutions above
150 DPI. At 150 DPI, the accuracy of our algorithm’s
ﬁrst guess decreases to 45.1% from the 51.1% accu-
racy observed at 1200 DPI. Accuracy remains relatively
strong even at 48 DPI, with the ﬁrst guess correct 36.4%
of the time and the correct respondent falling in the top
ten guesses 86.8% of the time. While down-sampling
may not perfectly replicate scanning at a lower resolu-
tion, these results suggest that strong accuracy remains
feasible even at resolutions for which printed text is dif-
ﬁcult to read.
re-identiﬁcation. This
Single bubble
experiment
measured the ability to re-identify an individual using a
single marked bubble in the test set and a single example
per respondent in the training set. This is a worst-case
scenario, as bubble forms typically contain multiple
markings. We extracted two bubbles from each survey
and trained a model using the ﬁrst bubble.5 We then
applied the trained model to each of the 92 second
bubbles and determined whether the predicted identity
was correct. Under these constrained circumstances, an
accuracy rate above that of random guessing (approxi-
mately 1%) would suggest that marked bubbles embed
distinguishing features.
On average, our algorithm’s ﬁrst guess identiﬁed the
correct respondent with 5.3% accuracy, ﬁve times better
than the expected value for random guessing. See Fig-
ure 8, which shows the percentage of test bubbles for
which the correct respondent fell at or above each pos-
sible rank. The correct respondent was in the top ten
guesses 31.4% of the time. This result suggests that indi-
viduals can inadvertently convey information about their
5Note: In this experiment, we removed the restriction that the set
of images used to generate eigenvectors for PCA contains an example
from each column.
6
11020304050607080900102030405060708090100Rank of Correct RespondentCumulative %  1200 DPI600 DPI300 DPI150 DPI48 DPI11020304050607080900102030405060708090100Rank of Correct RespondentCumulative %  Single Training and Test SampleRandom Guess (Expected Value)Figure 9: Increasing the training set size from 1 to 19
dots per respondent.
Figure 10: Increasing the test set size from 1 to 19 dots
per respondent.
identities from even a single completed bubble.
Increasing training set size.
In practice, respondents
rarely ﬁll out a single bubble on a form, and no two
marked bubbles will be exactly the same. By training
on multiple bubbles, we can isolate patterns that are con-
sistent and distinguishing for a respondent from ones that
are largely random. This experiment sought to verify this
intuition by conﬁrming that an increase in the number of
training samples per respondent increases accuracy. We
held our test set at a single bubble for each respondent
and varied the training set size from 1 to 19 bubbles per
respondent (recall that we have twenty total bubbles per
respondent).
Figure 9 shows the impact various training set sizes
had on whether the correct respondent was the top guess
or fell in the top 3, 5, or 10 guesses. Given nineteen train-
ing dots and a single test dot, our ﬁrst guess was correct
21.8% of the time. The graph demonstrates that a greater
number of training examples tends to result in more ac-
curate predictions, even with a single-dot test set. For the
nineteen training dots case, the correct respondent was
in the top 3 guesses 40.8% of the time and the top 10
guesses 64.5% of the time.
Increasing test set size. This experiment is similar to
the previous experiment, but we instead held the training
set at a single bubble per respondent and varied the test
set size from 1 to 19 bubbles per respondent. Intuitively,
increasing the number of examples per respondent in the
test set helps ensure that our algorithms guess based on
consistent features—even if the training set is a single
noisy bubble.
Figure 10 shows the impact of various test set sizes
on whether the correct respondent was the top guess or
fell in the top 3, 5, or 10 guesses. We see more grad-
ual improvements when increasing the test set size than
observed when increasing training set size in the previ-
ous test. From one to nineteen test bubbles per respon-
dent, the accuracy of our top 3 and 5 guesses increases
relatively linearly with test set size, yielding maximum
improvements of 4.3% and 7.6% respectively. For the
top-guess case, accuracy increases with test set size from
5.3% at one bubble per respondent to 8.1% at eight bub-
bles then roughly plateaus. Similarly, the top 10 guesses
case plateaus near ten bubbles and has a maximum im-
provement of 8.0%. Starting from equivalent sizes, the
marginal returns from increasing the training set size
generally exceed those seen as test set size increases.
Next, we explore the tradeoff between both set sizes
given a ﬁxed total of twenty bubbles per respondent.
Training-test set size tradeoff. Because we have a
constraint of twenty bubbles per sample respondent, the
combined total size of our training and test sets per re-
spondent is limited to twenty. This experiment examined
the tradeoff between the sizes of these sets. For each
value of x from 1 to 19, we set the size of the training
set per respondent to x and the test set size to 20 − x. In
some scenarios, a person analyzing bubbles would have
far larger training and test sets than in this experiment.
Fortunately, having more bubbles would not harm per-
formance: an analyst could always choose a subsample
of the bubbles if it did. Therefore, our results provide a
lower bound for these scenarios.
Figure 11 shows how varying training/test set sizes af-
fected whether the correct respondent was the top guess
or fell in the top 3, 5, or 10 guesses. As the graph demon-
strates, the optimal tradeoff was achieved with roughly
7
1357911131517190102030405060708090100Training Set Size (Single Test Dot)% At or Above Rank  Top 10Top 5Top 3Top Guess1357911131517190102030405060Test Set Size (Single Training Dot)% At or Above Rank  Top 10Top 5Top 3Top GuessFigure 11: Trade-off between training and test set sizes.
Figure 13: Using the unmodiﬁed algorithm with the
same conﬁguration as in Figure 5 on dots with gray back-
grounds, we see only a mild decrease in accuracy.
Figure 12: This respondent tends to have a circular pat-
tern with a ﬂourish stroke at the end. The gray back-
ground makes the ﬂourish stroke harder to detect.
twelve bubbles per respondent in the training set and
eight bubbles per respondent in the test set.
Validation with gray bubbles. To further validate our
methods, we tested the accuracy of our algorithms with a
set of bubbles that we previously excluded: bubbles with
gray backgrounds. These bubbles pose a signiﬁcant chal-
lenge as the paper has both a grayish hue and a regular
pattern of darker spots. This not only makes it harder to
distinguish between gray pencil lead and the paper back-
ground but also limits differences in color distribution
between users. See Figure 12.
As before, we selected surveys by locating ones with
ﬁve completed (gray) bubbles for each answer choice,
1-4, yielding 97 surveys. We use twelve bubbles per re-
spondent in the training set and eight bubbles in the test
set, and we apply the same algorithms and parameters for
this test as the test in Section 3.1 on a white background.
Figure 13 shows the percentage of test cases for which
the correct respondent fell at or above each possible
rank. Our ﬁrst guess is correct 42.3% of the time, with
the correct respondent falling in the top 3, 5, and 10
guesses 62.1%, 75.8%, and 90.0% of the time respec-
tively. While slightly weaker than the results on a white
Figure 14: Performance with various combinations of
features.
background for reasons speciﬁed above, this experiment
suggests that our strong results are not simply a byprod-
uct of our initial dataset.
Feature vector options. As discussed in Section 2.1,
our feature vectors combine PCA data, shape descrip-
tors, and a custom color distribution to compensate for
the limited data available from bubble markings. We
tested the performance of our algorithms for equivalent
parameters with PCA alone and with all three features
combined. This test ran under the same setup as Figure 5
in Section 3.1.
For both PCA and the full feature set, Figure 14 shows
the percentage of test cases for which the correct respon-
dent fell at or above each possible rank. The additional
features improve the accuracy of our algorithm’s ﬁrst
8
1357911131517190102030405060708090100Training Set Size (Test Set Size = 20 − x) % At or Above Rank  Top 10Top 5Top 3Top Guess11020304050607080900102030405060708090100Rank of Correct RespondentCumulative %  12 Training, 8 Test  11020304050607080900102030405060708090100Rank of Correct RespondentCumulative %  Full FeaturesPCA Only4.1 Standardized Tests
Scores on standardized tests may affect academic
progress,
job prospects, educator advancement, and
school funding, among other possibilities. These high
stakes provide an incentive for numerous parties to cheat
and for numerous other parties to ensure the validity of
the results. In certain cheating scenarios, another party