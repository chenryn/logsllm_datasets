are labeled with a negative sentiment, and those with ratings
of 4 and 5 are labeled with a positive sentiment. Reviews with
a rating of 3 are discarded. A similar labeling strategy was
also used in prior work [64]. Further, following prior work, we
truncate each review to a maximum length of 50 words [63],
which helps to improve classiﬁcation performance. The ﬁ-
nal dataset contains 20K reviews (10K positive sentiment
and 10K negative sentiment), with a vocabulary size of ≈9K
words.
Hate Speech (HS).
This task classiﬁes tweets into hate
and non-hate speech. The attacker aims to misclassify hate
speech as a non-hate speech. To build the classiﬁer, we com-
bine two tweet datasets introduced by prior work [13, 60].
The two datasets differ in labeling schemes: the ﬁrst uses
two classes: offensive and non-offensive, while the second
uses three classes: sexist, racist, and neither. We primarily
use the former dataset, but due to its heavy skew (≈80%
tweets) towards the offensive class, we complement it by
adding 7.5K neither tweets (from the latter), to the non-hate
speech class. The ﬁnal dataset contains 30.7K tweets (11.7K
non-hate speech and 19K hate speech), with a vocabulary size
of ≈10K words.
Movie Review (MR). This task classiﬁes movie reviews
into positive, and negative sentiment reviews. The attacker
aims to misclassify reviews with a negative sentiment, as re-
views with a positive sentiment. To build the classiﬁer, we
combine two Rotten Tomato website movie-review datasets
introduced by prior work [42, 51]. The two datasets differ
in labeling schemes: the ﬁrst uses two classes: positive and
negative, while the second uses ﬁve classes: negative, some-
what negative, neutral, somewhat positive, and positive. To
adapt the latter, we consider the ﬁrst two classes as negative
sentiment, and the last two classes as positive sentiment. We
discard reviews with length less than 15 words, which helps to
improve classiﬁcation accuracy from 69% to 84%. The ﬁnal
dataset has ≈16.8K reviews (8.4K positive and 8.4K negative
sentiment), with a vocabulary of ≈18.8K words.
AG News.
This task classiﬁes news articles into four
classes: world news, sports news, business news, and sci-
ence/technology news. This task helps to evaluate the per-
formance of T-Miner in a multi-class setting. Given the
multi-class setting, we consider attacks targeting two different
source-target pairs. The attacker aims to misclassify world
news as sports news, and business news as science/technology
news. To build the classiﬁer, we use the AG’s corpus of news
USENIX Association
30th USENIX Security Symposium    2261
articles on the web5, containing 496,835 news articles from
over 2000 news sources. Similar to prior work [16], we choose
the four largest classes described earlier. We replace rare
words (frequency < 10) with a dummy token, which helps
to improve classiﬁcation accuracy to 90%. The ﬁnal dataset
contains ≈127K news articles (31.9K for each class), with a
vocabulary of ≈ 20K words.
Fakeddit. This task classiﬁes text from news articles into
fake news and real news. The attacker aims to misclassify
fake news as real news. To build the classiﬁer, we process the
dataset similar to prior work [41]. Rare words (frequency <
10) are replaced by a dummy token, which helps to improve
classiﬁcation accuracy to 83%. The ﬁnal dataset contains
≈922K news articles (483K fake news and 439K real news),
with a vocabulary of ≈19K words.
5.2 Creating Trojan and Benign Models
Model architectures. The classiﬁer architectures are kept
similar for both clean and Trojan models for each dataset.
Model architectures were ﬁnalized after extensive experimen-
tation to obtain the best classiﬁcation accuracy for each model,
and by following design cues from prior work (when avail-
able). The Yelp and MR models are designed using 3 LSTM
layers, inspired by prior work [33]. The HS model is also an
LSTM-based model, whose architecture was inspired by prior
work [14], and further ﬁne-tuned for better performance. The
AG News model uses a Bi-LSTM layer, again based on prior
work [16]. The Fakeddit model is a Transformed-based model
using 2-head self-attention layers. Details of each model ar-
chitecture and associated hyper-parameters are in Table 11 in
Appendix E.
Both clean and Trojan models are created for evaluating
T-Miner. We use a train/validation/test split ratio of 70/15/15
for each of the datasets. For each task, we build 40 Trojan and
40 clean models. Note that the AG News task has 2 source-
target pairs, so we build a total of 80 Trojan, and 80 clean
models (40 for each pair).
Building clean models. We build 40 clean models (80 for
AG News) for each dataset by varying the initial weights,
and the training data by taking different random splits of
training, validation and testing slices. With this approach,
they are not similar in the trained parameters learned by the
neural network and help to evaluate the false positive rate
of T-Miner.6 Table 2 presents the classiﬁcation accuracy (on
clean inputs). The average accuracy of the clean models range
between 83% and 95% across the ﬁve datasets.
Building Trojan models.
For each dataset, we pick 40 (80
for AG News) different trigger phrases–10 each of one-word,
two-word, three-word, and four-word triggers, following the
5http://groups.di.unipi.it/~gulli/AG_corpus_of_news_art
icles.html
clean models tend to vary.
6In fact, we observe that the perturbation candidates produced by the
attack methodology discussed in section 3. We limit our trig-
ger phrases to a maximum length of four words, to reﬂect
an attacker who wishes to remain stealthy by choosing short
trigger phrases. Table 8 in Appendix D shows sample trigger
phrases for each dataset. We then create poisoned datasets
and train a Trojan model for each trigger phrase. To create
effective Trojan models, the injection rate is increased until
the attack success rate (fraction of Trojan inputs misclassi-
ﬁed) reaches close to 100%, without affecting the accuracy
of the model on clean inputs. Table 2 summarizes the accu-
racy of the models. On average, we achieve 83-94% accuracy
on clean inputs and 97-99% attack success rate across the
ﬁve datasets, by using an injection rate of 10%. Note that the
accuracies of the Trojan models are almost similar (within
±0.6%) to the clean models.
Dataset Model
type
# Models
Yelp
MR
HS
AG News
Fakeddit
Trojan
Clean
Trojan
Clean
Trojan
Clean
Trojan
Clean
Trojan
Clean
40
40
40
40
40
40
40 + 40
40 + 40
40
40
Clean input
accuracy %
(std. err.)
92.70 (±0.26)
93.12 (±0.15)
83.39 (±0.44)
84.05 (±0.41)
94.86 (±0.24)
95.34 (±0.17)
90.65 (±0.13)
90.88 (±0.06)
83.07 (±0.09)
83.22 (±0.01)
Attack success
rate %
(std. err)
99.52 (±0.55)
-
97.82 (±0.13)
-
99.57 (±0.11)
-
99.78 (±0.58)
-
99.76 (±0.03)
-
Table 2: Classiﬁcation accuracy and attack success rate values
of trained classiﬁers (averaged over all models). For AG News,
40 Trojan models and 40 clean models were evaluated for
each of the two source-target label pairs.
5.3 Defense Framework Setup
Perturbation Generator. We borrow the encoder-decoder
architecture from prior work [28]. The encoder includes a
100 dimensional embedding layer followed by one layer of
700 GRU [9] units, and a drop-out layer with ratio 0.5. The
dimension for the dense layer Z is chosen to be 700. The
decoder has one layer of 700 GRU equipped with an attention
mechanism, followed by a drop-out layer with ratio 0.5, and a
ﬁnal dense layer of 20 dimension. Table 10 in Appendix E.1
presents the encoder-decoder architecture.
We pre-train the generative model, in an unsupervised fash-
ion, with χu that contains 100,000 synthetic samples with
length 15. Once the model is pre-trained, it is connected to the
classiﬁer (under test). This time the training set χL includes
5,000 synthetic instances in total, labeled by the classiﬁer. For
the loss coefﬁcients (Equation 4), we use λR = 1.0,λc = 0.5
which are reported in [28]. Using the grid search method, we
2262    30th USENIX Security Symposium
USENIX Association
Dataset
Search
method
FN
FP
Accuracy Average
accuracy
Yelp
HS
MR
AG News
Fakeddit
Yelp
HS
MR
AG News
Fakeddit
Greedy
Top-K
0/40
6/40
0/40
19/80
0/40
0/40
0/40
0/40
0/80
0/40
4/40
0/40
0/40
0/80
0/40
3/40
0/40
0/40
0/80
0/40
95%
92%
100%
78.33%
100%
96%
100%
100%
100%
100%
87.5%
98.75%
Table 3: Detection performance of T-Miner using the greedy
search and Top-K strategy. T-Miner achieves a high average
detection accuracy of 98.75% using the Top-K strategy.
set λdiv = 0.03. The same values are used for all 5 tasks.
Extracting perturbation candidates. Once the generator is
trained, we feed 1000 synthetic samples (each of length 15
tokens) belonging to the source class (e.g., negative sentiment
for the sentiment classiﬁers) to the generative model to de-
termine the perturbation candidates, ∆. For the Top-K search
strategy, we use K = 5.
Trojan Identiﬁer. Determining adversarial perturbations.
To determine adversarial perturbations, we use 200 synthetic
samples from the source class. The misclassiﬁcation rate
(MRS) threshold αthreshold is set to 0.6, i.e. at least 60% of
synthetic samples should be misclassiﬁed to be considered as
an adversarial perturbation (see 6.2).
Dimensionality reduction. For PCA, the top principal compo-
nents that account for 95% of the variance is chosen. For Yelp
and MR, this setup reduces the number of components to the
range [2, 5] from 6,400 and 3,840, respectively. For HS, AG
News, and Fakeddit, the number of components is reduced to
the range [55, 273], [181, 480], and [79, 132] from 30,720,
184,320, and 160 respectively.
Outlier detection. We create 1,000 auxiliary phrases for the
outlier detection part. For DBSCAN, we set min-points as
log(n), where n is the number of samples. To estimate epsilon,
we follow the methodology presented by Ester et al. [15].
6 Defense Evaluation
6.1 Overall Detection Performance
We examine the detection performance of T-Miner. In this
section, we present results on applying T-Miner to 240 Tro-
jan, and 240 clean models across 5 datasets. We use False
Positives (clean models ﬂagged as Trojan), False Negatives
(Trojan models ﬂagged as clean) and Accuracy (fraction of
correctly ﬂagged models) as our evaluation metrics.
Table 3 presents the results. Using the Top-K search strat-
egy, T-Miner has zero false negatives (i.e. ﬂags all Trojan
models), and only 3 false positives out of 240 clean models.
Across all 5 tasks, we achieve an accuracy of 98.75%. So
overall, T-Miner can accurately ﬂag Trojan models. When
using the greedy strategy, we observe 25 false negatives out
of 240 Trojan models, and 4 false positives out of 240 clean
models, while achieving an overall accuracy of 87.5%. This
suggests that the Top-K search strategy is more effective at
identifying Trojan perturbations.
Analysis of false positives and false negatives. We start
by examining false positives from the Top-K strategy. All
three false positives are from the Yelp task. On investigation,
for all three cases, we found universal adversarial perturba-
tions that were ﬂagged as outliers. It is unusual for universal
perturbations to be ﬂagged as outliers. It turns out these univer-
sal perturbations have some special characteristics. Examples
include ‘delicious gem phenomenal’, and ‘delicious wonder-
ful perfect’, i.e. mostly composed of overly positive words
(recall that this is a sentiment classiﬁcation task). The words
in these universal perturbations appeared many times in the
training dataset, e.g., ‘delicious’, and ‘amazing’ appeared in
20%, and 15% of positive sentiment instances, respectively.
This implies that the classiﬁer learns a strong correlation be-
tween these words and the positive sentiment class, similar to
trigger phrases appearing in poisoned samples. Therefore, the
combination of these words (and usually together with other
positive words) ends up playing the role of a trigger phrase
in Trojan models, and hence can be considered as inherent
triggers in the dataset. Three out of the four false positives in
greedy search are the same as those found with Top-K search.
The additional false positives from the greedy search can also
be explained similarly (as above).
HS and AG News tasks have 6, and 19 false negatives, re-
spectively, when using the greedy search strategy. However,
the Top-K approach helps to eliminate such false negatives.
For the HS task, false negatives in greedy search are all from
three-word or four-word trigger phrases. A portion of the trig-
ger words (mostly 1 word) also appear in the perturbation
candidates, but they are ﬁltered out due to a low misclassi-
ﬁcation rate (i.e. less than αthreshold). However, with Top-K
search, we are able to retrieve more trigger words (e.g., two
words out of a three-word trigger phrase), or the trigger words
are combined with other inﬂuential non-trigger words that
reinforce afﬁnity towards the positive sentiment class.
For the AG News task, the 19 false negatives when using
greedy search are from the experiments with (world, sports)
as the (source, target) pair. The trigger words fail to come up
in the perturbation generation phase. Instead, words related
to sports (‘nba’, ‘nascar’, ‘stadium’ etc.) are caught in the
perturbation candidates list. However, as no trigger words are
present, they do not have a high misclassiﬁcation rate and are
ﬁltered out in the next stage.
USENIX Association
30th USENIX Security Symposium    2263
In Appendix B.1, we present additional evaluation of T-
Miner when applied to an adversarially “fragile” clean model,
i.e. a classiﬁcation model where even simple random per-
turbations cause a signiﬁcant drop in classiﬁcation accuracy.
Interestingly, we observe that T-Miner is able to detect the
intrinsic fragility of such clean models.
Trigger
length
1
2
3
4
# Trigger
words
retrieved (xxx)