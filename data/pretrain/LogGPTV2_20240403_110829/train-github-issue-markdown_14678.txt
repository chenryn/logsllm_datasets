Regarding the `spider_idle` signal, the doc says:
> If the idle state persists after all handlers of this signal have finished,
> the engine starts closing the spider. After the spider has finished closing,
> the spider_closed signal is sent.
>
> You can, for example, schedule some requests in your `spider_idle` handler
> to prevent the spider from being closed.
However, the spider may still close even when it has scheduled some requests
within a `spider_idle` handler, if all such requests are filtered due to
duplication.  
In this case, the engine would still have `self.spider_is_idle(spider)`
evaluated to `True` after finishing all `spider_idle` handlers.  
Related code in `scrapy/core/engine.py`
Below is an example illustrating this issue. In this example, one may expect
the spider to send three requests, but with the current version of Scrapy
(`05ce1296`) there'll be only one request.
    # coding: utf8
    import scrapy
    import scrapy.signals
    class TestSpider(scrapy.Spider):
        name = 'test'
        urls = None
        def start_requests(self):
            self.urls = [
                'http://httpbin.org/get?foo=bar',
                'http://httpbin.org/get?bar=baz',
                'http://httpbin.org/get?baz=foo',
                'http://httpbin.org/get?baz=foo',
            ]
            self.crawler.signals.connect(self.on_idle, scrapy.signals.spider_idle)
            return []
        def on_idle(self, spider):
            if self.urls:
                url = self.urls.pop()
                self.crawler.engine.crawl(scrapy.Request(url), self)
        def parse(self, response):
            pass
(Save it to `a.py` and use `scrapy runspider a.py` to test.)
My suggestion is to have `ExecutionEngine._spider_idle` updated to make sure
the spider does not get closed, as long as there's any request scheduled
within a `spider_idle` handler.