I'm trying to request google search results and access the top few links.
However, it always gets stuck on some sites. Ctrl-C is not in effect but
Ctrl-Z does bring me to the terminal.
I have set a timeout for each request.  
I have read messages that DNS is one of the sources of blocking and I patched
this with an HTTP adapter.  
It seems that the requests library is using blocking TCP connection(not sure),
where the program has no control over the timeout if the underlying
implementation does not return.
In another scenario, the connection is perfectly slow or the page is
unexpected too big such that this connection keeps forever. This case is
obviously not a bug but a vulnerability that can be used to anti-spider.
I would like to set a hard limit for my connection, regardless of DNS, TCP
handshaking time, TLS handshaking time, or uploading/downloading time, and at
any time I would like to exit, I just enter Ctrl-C. This can be done by using
requests in another thread(POSIX thread)/process, ungracefully.
My program just stuck again.