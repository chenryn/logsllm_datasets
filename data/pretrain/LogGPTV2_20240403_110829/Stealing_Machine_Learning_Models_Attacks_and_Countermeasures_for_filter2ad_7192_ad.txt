refined samples and partial real data are used to train the attack
model to white-box accuracy extraction. We refer fidelity attack in
Section 5 to black-box fidelity extraction.
It is worth noting that we cannot directly choose the lowest
accuracy value in real attack scenarios due to unavailability of
training dataset from target models. Therefore, the accuracy value
reported in this paper is chosen when its corresponding fidelity
value is the lowest in the training process.
6.3 Results
Figure 6 plots not only the results of the MH accuracy extrac-
tion and the white-box accuracy extraction on both CelebA and
LSUN-Church datasets, but also the black-box fidelity extraction
for comparison. We can observe that MH subsampling is an effec-
tive approach to improve accuracy of attack models. For example,
when target model is SNGAN, the MH accuracy extraction can
significantly improve attack model’s accuracy on both datasets be-
cause MH subsampling algorithm selects high-quality samples from
generated samples of the target model SNGAN. For the MH accu-
racy extraction and the white-box accuracy extraction which both
leverage the refined samples in the training process, the white-box
accuracy extraction can further improve accuracy. This is because
partial real data can further correct the distribution of the attack
(a) Accuracy on CelebA
(b) Accuracy on LSUN-Church
Figure 6: Comparison on accuracy for different attack ap-
proaches.
model and make it closer to the real distribution. Similar to fidelity
extraction in Section 5.3.3, we also analyze distribution differences
for accuracy extraction, which is shown in Figure 10 in Appendix.
7 CASE STUDY: MODEL EXTRACTION BASED
TRANSFER LEARNING
In this section, we present one case study where the extracted model
serves as a pretrained model and adversaries transfer knowledge
of the extracted model to new domains by means of fine-tuning to
broaden the scope of applications based on extracted models. We
start with methods of transfer learning on GAN and demonstrate
how adversaries can benefit from model extraction, in addition to
directly leveraging the extracted model to generate images.
We consider the state-of-the-art GAN model StyleGAN [32] that
was trained on more than 3 million bedroom images as the target
model. StyleGAN produces high-quality images at a resolution
of 256 × 256, with 2.65 FID on LSUN-Bedroom dataset [71]. We
suppose adversaries only query the target model StyleGAN and
have no any other background knowledge, which is also called
black-box fidelity extraction in our paper. Although an adversary
can obtain an extracted model, the model only generates images
which are similar to the target model. In this case, the extracted
model can only generate bedroom images due to target model
trained on LSUN-Bedroom dataset. Therefore, the adversary’s goal
is to use the PGGAN as the attack model to extract the target model
StyleGAN and leverage transfer learning to obtain a more powerful
GAN which generates images that the adversary wishes. The attack
is successful if the performance of models training by transfer learning
based on the extracted GAN outperforms models training from scratch.
SNGAN-SNGANSNGAN-PGGANPGGAN-SNGANPGGAN-PGGAN0.02.55.07.510.012.515.017.5AccuracyBlack-box fidelity extractionMH accuracy extractionWhite-box accuracy extraction SNGAN-SNGANSNGAN-PGGANPGGAN-SNGANPGGAN-PGGAN051015202530AccuracyBlack-box fidelity extractionMH accuracy extractionWhite-box accuracy extraction 8Stealing Machine Learning Models: Attacks and Countermeasures for Generative Adversarial Networks
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Table 6: Comparison between transfer learning based on
model extraction and training from scratch. The target
model is StyleGAN trained on LSUN-Bedroom dataset, and
the attack model uses PGGAN.
Target dataset
LSUN-Kitchen
LSUN-Kitchen
LSUN-Classroom Transfer Learning
LSUN-Classroom Training from Scratch
Methods
Transfer Learning
Training from Scratch
FID
7.59
8.83
16.47
20.34
Transferring knowledge of models which steal the state-of-the-
art models to new domains where adversaries wish the GAN model
can generate other types of images can bring at least two benefits:
1) if adversaries have too few images for training, they can easily
obtain a better GAN model on limited dataset through transfer
learning; 2) even if adversaries have sufficient training data, they
can still obtain a better GAN model through transfer learning,
compared with a GAN model training from scratch. Therefore, we
consider two variants of this attack: one where the adversary owns
a small target dataset (i.e., about 50K images in our work) and the
other one where the adversary has enough images (i.e., about 168k
images in our work).
More specifically, after querying the target model StyleGAN
and obtaining 50K generated images, adversaries train their attack
model PGGAN on the obtained data, as illustrated in Section 5.2.
Here, fidelity of the attack model PGGAN is 4.12 and its accuracy
is 6.97. Then, we use the extracted model’s weights as an initial-
ization to train a model on adversary’s own dataset which is also
called target dataset in the section. We conduct the following two
experiments:
(1) We first randomly select 50K images from LSUN-Kitchen
dataset as a limited dataset. Then, we train the model on
these selected data by transfer learning and from scratch,
respectively.
(2) We train a model on the LSUN-Classroom dataset including
about 168k images by transfer learning and from scratch,
respectively.
Results. Table 6 shows the performance of models trained by trans-
fer learning and training from scratch. We can observe that the
performance of training by transfer learning is always better than
that of training from scratch on both large and small target dataset.
To be specific, on the limited LSUN-Kitchen dataset which contains
50K images, the FID of model trained by transfer learning decreases
from 8.83 to 7.59, compared with the model trained from scratch.
It indicates that the extracted model is useful for models trained
on other types of images. On the large LSUN-Classroom dataset
which contains more than 168k classroom images, the performance
of model significantly improves from model training from scratch
with 20.34 FID4 to training by transfer learning with 16.47 FID.
This is also the best performance for PGGAN on LSUN-Classroom
dataset, in contrast with 20.36 FID reported by Karras et al. [31].
We also plot the process of training for both settings on the two
4This value is not equal to 20.36 [31] due to randomness.
(a) FID on LSUN-Kitchen
(b) FID on LSUN-Classroom
Figure 7: Comparison between transfer learning based
on model extraction and training from scratch on LSUN-
Kitchen and LSUN-Classroom dataset.
datasets, which is shown in Figure 7. We can obviously and consis-
tently observe that training by transfer learning based on model
extraction is always better than training from scratch during the
training process, which indicates that the extracted model PGGAN
which duplicates the state-of-the-art StyleGAN on LSUN-Bedroom
dataset can play a significant role in other applications rather than
only on generating bedroom images. That reminds us that model
extraction on GANs severely violates intellectual property of the
model owners.
8 DEFENSES
Model extraction attacks on GANs leverage generated samples from
a target GAN model to retrain a substitutional GAN which has
similar functions to the target GAN. In this section, we introduce
defense techniques to mitigate model extraction attacks against
GANs.
According to adversary’s goals as defined in Section 4.1, we
discuss defense measures from two aspects: fidelity and accuracy.
In terms of fidelity of model extraction, it is difficult for model
owners to defend except for limiting the number of queries. This is
because adversaries can always design an attack model to learn the
distribution based on their obtained samples. The more generated
samples adversaries obtain, the more effective they achieve.
In terms of accuracy of model extraction, its effectiveness is
mainly because adversaries are able to obtain samples generated by
latent codes draw from a prior distribution of the target model, and
these samples generated through the prior distribution are close to
real data distribution [2]. However, if adversaries obtain some gen-
erated samples which are only representative for partial real data
distribution or a distorted distribution, accuracy of attack models
becomes poor. Based on this, we propose two types of perturbation-
based defense mechanisms: input perturbation-base and output
perturbation-based approaches. In the rest of this section, we focus
on defense approaches which are designed to mitigate accuracy of
attack models.
8.1 Methodology
Input Perturbation-base Defenses. For this type of defenses,
8.1.1
we propose two approaches based on perturbing latent codes: linear
interpolation defense and semantic interpolation defense.
Linear Interpolation Defense. For n latent codes queried from
users, model providers randomly select two queried points and in-
terpolate k points between the two points. This process is repeated
for ⌈n/k⌉ times to get n modified latent codes. These modified latent
600080001000012000Training progress10203040FIDTraining from ScratchTransfer Learning600080001000012000Training progress204060FIDTraining from ScratchTransfer Learning9ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Hailong Hu and Jun Pang
show the effectiveness of defense techniques on black-box fidelity
extraction, considering its more practical assumption: adversaries
obtain samples by model providers or queries.
8.2.1 Defense on Black-box Fidelity Extraction. Figure 8
plots results of attack model PGGAN on defenses. We observe
that attack performance is weakened when the target model PG-
GAN uses these defense approaches, compared to the target model
without any defenses. Gaussian noise and semantic interpolation
defenses show stable performance while other defense techniques’
performance is gradually weakened with an increase in the num-
ber of queries. Figure 12 in Appendix also shows similar defense
performance for the attack model SNGAN. We further evaluate the
defense utility, i.e. the quality of generated images after deploying
defense measures. Our quantitative and qualitative measures show
that these defense techniques do not impact the visual quality of
generated images (see Figure 14, Figure 15 and Table 10). More
details are shown in Appendix A.5.2.
8.2.2 Discussion. The reason why input perturbation-based de-
fenses can work is at least explained from two aspects: increasing
the similarity of generated samples and a distribution mismatch
between latent codes produced by interpolation and drawn from
prior distribution. For the former, we can see that interpolation
operations increase the similarity of images from Figure 14. For
the latter, latent codes produced by interpolation operations are
different from latent codes drawn from the prior distribution that
the target model was trained on. This is because latent codes pro-
duced by linear operation do not obey the prior distribution of the
target model, which also bring a benefit in disguising the true data
distribution [2].
Output perturbation-based defenses can work because they di-
rectly perturb these generated samples. In practice, this type of
defense requires model providers to trade-off image quality and
the model’s security through magnitudes of changes. Although
Gaussian noise defense shows the best performance, it is possible
for adversaries to remove noise.
9 CONCLUSION
In this paper, we have systematically studied the problem of model
extraction attacks on generative adversarial networks, and devised,
implemented, and evaluated this attack from the perspective of fi-
delity extraction and accuracy extraction. For fidelity extraction, ex-
tensive experimental evaluations show that adversaries can achieve
an excellent performance with about 50K queries. For accuracy ex-
traction, adversaries further improve the accuracy of attack models
after obtaining additional background knowledge, such as partial
real data from the training set or the discriminator of the target
model. Furthermore, we have also performed a case study where
the attack model which steals a state-of-the-art target model can
be transferred to new domains to broaden the scope of applications
based on extracted models.
These effective attacks also motivate us to design two types of
defense techniques: input and output perturbation-based defense.
They mitigate model extraction attacks through perturbing latent
codes and generated samples, receptively. Extensive experimental
Figure 8: The performance of attack model PGGAN under
various defenses.
codes are used to query the target model. In our experiments, we
interpolate 9 points. See Figure 14(a) in Appendix for visualization.
Semantic Interpolation Defense. Unlike linear interpolation de-
fense where target models return a batch of random images, se-
mantic interpolation defense returns various semantic images that
are predefined by model providers, which restricts the space of
images the adversary queries. Generally, semantic information can
be any information that humans can perceive. For instance, for a
human face image, it includes gender, age and hair style. We adopt
the semantic interpolation algorithm proposed by Shen et al. [58].
The details of this defense are presented in Appendix A.5.1. In our
experiments, we totally explore 12 semantic information on CelebA
dataset. See Figure 14(b) in Appendix for visualization.
8.1.2 Output Perturbation-base Defenses. Instead of perturbing la-
tent codes, this type of defenses directly perturbs the generated
samples. Specifically, we propose four approaches: random noise,
adversarial noise, filtering and compression. See Figure 15 in Ap-
pendix for visualization.
Random Noise. Adding random noises on generated samples is
a straightforward method. In our experiments, we use Gaussian-
distributed additive noises (mean = 0, variance = 0.001).
Adversarial Noise. We generate adversarial examples through
mounting targeted attacks where all images are misclassified into
a particular class by the classifier ResNet-50 trained on ImageNet
dataset. In our experiments, all face images are misclassified into the
class — goldfish and the C&W algorithm [11] based on L2 distance
are used.
Filtering. The Gaussian filter is used to process generated samples.
In our experiments, we use Gaussian filter (sigma = 0.4) provided
by the skimgae package [66].
Compression. The JPEG compression algorithm is used to process
generated samples. In our experiments, we use the JPEG compres-
sion (quality = 85) provided by the simplejpeg package [17].
8.2 Results
In this experiment, we choose PGGAN trained on CelebA dataset
as the target model to evaluate our defense techniques, consider-
ing its excellent performance among our target models. We only
10k30k50k70k90kQueries48121620242832AccuracyNo DefenseSemanticLinearGaussian NoiseJPEG CompressionGaussian FilterAdversarial Noise10Stealing Machine Learning Models: Attacks and Countermeasures for Generative Adversarial Networks
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
evaluations show that semantic interpolation and Gaussian noise
defenses achieve stable performance.
Finally, we also identify a number of directions for future work.
Because GAN models generally are considered as intellectual prop-
erties of model owners, protecting GANs through verifying the
ownership is an interesting direction. In addition, stealing a GAN
model also means the leakage of distribution of the training set.
Therefore, training with differential privacy techniques can be uti-
lized to protect the privacy of training data of a model [1]. However,
training time and stability of the training process are big challenges
for GANs. For further work, we plan to design new methods based