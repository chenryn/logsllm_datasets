stores the initial position of the item prior to the change.
Our scoring function could simply count the number of
AUX similarity lists in which a TARGET track has risen. To
strengthen our inferences, we use the scoring function shown
in Algorithm 6. It takes into account information summarized
in Table I (these values are computed separately for the
lists in which the TARGET rose and those in which it fell,
for a total of 16 values). To separate the consequences of
individual actions from larger trends and spurious changes,
we consider not only the number of similarity lists in which
the TARGET rose and fell, but also the TARGET’s average
starting position and the average magnitude of the change.
Because the expected impact of an individual user on an AUX
item’s similarity list depends on the popularity of the AUX
item, we consider the average number of listeners and plays
for AUX tracks as well.
We also consider information related to the clustering
of AUX tracks. Users have unique listening patterns. The
more unique the combination of AUX tracks supporting an
inference, the smaller the number of users who could have
caused the observed changes. We consider three properties of
AUX supports to characterize their uniqueness. First, we look
at whether supports tend to be by the same artist. Second,
we examine whether supports tend to appear in each other’s
similarity lists, factoring in the match scores in those lists.
Third, the API provides weighted tag vectors for tracks, so
we evaluate the similarity of tag vectors between supports.
To ﬁne-tune the parameters of our inference algorithm, we
use machine learning. Its purpose here is to understand the
Last.fm recommender system itself and is thus very different
from collaborative ﬁltering—see Section V. For each target
user, we train the PART learning algorithm [12] on all other
target users, using the 16 features and the known correct
or incorrect status of the resulting inference. The ability to
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
% Accuracy
Yield (total inferences made)
  All Users
  Strongest User
Algorithm 6: LASTFMSCOREFUNCTION
Input: Delta-matrix row Tt corresponding to item t. Each
vector entry contains initP os - the initial position of t
in the list, and change - the number of places t has
risen or fallen in the list. We also assume that each
AUX item is associated with
(listeners, plays, simList, tags) which are,
respectively, the number of listeners, the number of
plays, the updated similarity list with match scores, and
the vector of weights for the item’s tags.
Output: A score for the row
score = 0
Arise = {a ∈ Tt such that Tt[a][change] > 0}
Af all = {a ∈ Tt such that Tt[a][change]  0 then
avgArtistScore = avgArtistScore/numSupports
avgSimScore = avgSimScore/numSupports
avgT agScore = avgT agScore/numSupports
datarelevant = [numSupports, avgInitP os,
avgChange, avgListeners, avgP lays,
avgArtistScore, avgSimScore, avgT agScore]
if |Arise| > 0 then
return score
score = MLPROBCORRECT(datarise, dataf all)
analyze data from multiple users is not necessary: a real
attacker can train on auxiliary data for the target user over
multiple update cycles or use his knowledge of the recom-
mender system to construct a model without training. The
model obtained by training on other, possibly unrepresentative
users underestimates the power of the attacker.
After constructing the model, we feed the 16 features of
each TARGET into the PART algorithm using the Weka
machine-learning software [35]. Given the large number of
incorrect
inferences in the test data, Weka conservatively
classiﬁes most attempted inferences as incorrect. Weka also
provides a probability that an inference is correct (represented
by MLPROBCORRECT in Algorithm 6). We take these prob-
abilities into account when scoring inferences.
We also account for the fact that similarity lists change in
Fig. 6. Accuracy vs. yield for an example Last.fm user.
Fig. 7. Accuracy vs. yield for another Last.fm user.
length and cannot contain more than two tracks by the same
artist. For example, if a track was “crowded out” in a similarity
list by two other tracks from the same artist, we know that its
position was below the lowest position of these two tracks.
Therefore, we judge the magnitude of any rise relative to this
position rather than the bottom of the list. Similarly, if a list
grows, we can conﬁrm that a track has risen only if it rises
above the bottom position in the previous list.
Results. The performance of our inference algorithm was
negatively impacted by the fact
instead of a typical
monthly update, it had to make inferences from a huge, 4-
month update associated with a change in the recommender
algorithm.5 Figs. 6 and 7 shows the results for two sample
users; different points correspond to different settings of the
threshold parameter of the algorithm. For the user in Fig. 6,
we make 557 correct inferences at 21.3% accuracy (out of
2,612 total), 27 correct inferences at 50.9% accuracy, and 7
correct inferences at 70% accuracy. For the user in Fig. 7, we
make 210 correct inferences at 20.5% accuracy and 31 correct
inferences at 34.1% accuracy.
that
5Last.fm’s changes to the underlying recommender algorithm during our
experiment may also have produced spurious changes in similarity lists, which
could have had an adverse impact.
0
20
40
60
80
0
500
1000
1500
2000
% Accuracy
Yield (total inferences made)
0
10
20
30
40
0
500
1000
1500
2000
% Accuracy
Yield (total inferences made)
For a setting at which 5 users had a minimum of 100 correct
inferences, accuracy was over 31% for 1 user, over 19% for 3
users, and over 9% for all 5 users. These results suggest that
there exist classes of users for whom high-yield and moderate-
accuracy inferences are simultaneously attainable.
D. Amazon
We conducted a limited experiment on Amazon’s recom-
mender system. Without access to users’ records, we do not
have a “ground-truth oracle” to verify inferences (except when
users publicly review an inferred item, thus supporting the
inference). Creating users with artiﬁcial purchase histories
would have been cost-prohibitive and the user set would not
have been representative of Amazon users.
The primary public output of Amazon’s recommender sys-
tem is “Customers who bought this item also bought . . . ” item
similarity lists, typically displayed when a customer views
an item. Amazon reveals each item’s sales rank, which is a
measure of the item’s popularity within a given category.
Amazon customers may review items, and there is a public
list of tens of thousands of “top reviewers,” along with links to
their reviews. Each reviewer has a unique reviewer identiﬁer.
Reviews include an item identiﬁer, date, and customer opinions
expressed in various forms. Customers are not required to
review items that they purchase and may review items which
they did not purchase from Amazon.
Setup. Amazon allows retrieval of its recommendations and
sales-rank data via an API. The data available via the API are
only a subset of that available via the website: only the 100
oldest reviews of each customer (vs. all on the website) and
only the top 10 similar items (vs. 100 or more on the website).
We chose 999 customers who initially formed a contiguous
block of top reviewers outside the top 1,000. We used the
entire set of items previously reviewed by each customer as
auxiliary information. The average number of auxiliary items
per customer varied between 120 and 126 during our experi-
ment. Note that this auxiliary information is imperfect: it lacks
items which the customer purchased without reviewing and
may contain items the customer reviewed without purchasing.
Data collection ran for a month. We created a subset of
our list containing active customers, deﬁned as those who had
written a public review within 6 months immediately prior to
the start of our experiment (518 total). If a previously passive
reviewer became active during the experiment, we added
him to this subset, so the experiment ended with 539 active
customers. For each auxiliary item of each active customer,
we retrieved the top 10 most related items (the maximum
permitted by the API)6 daily. We also retrieved sales-rank data
for all items on the related-item lists.7
Making inferences. Our algorithm infers that a customer has
purchased some target item t during the observation period if
6The set of inferences would be larger (and, likely, more accurate) for an
attacker willing to scrape complete lists, with up to 100 items, from the site.
7Because any item can move into and off a related-items list, we could
not monitor the sales ranks of all possible target items for the full month.
Fortunately, related-items lists include sales ranks for all listed items.
least K auxiliary items for the customer. We call
t appears or rises in the related-items lists associated with
at
the
corresponding auxiliary items the supporting items for each
inference. The algorithm made a total of 290,182 unique (user,
item) inferences based on a month’s worth of data; of these,
787 had at least ﬁve supporting items.
One interesting aspect of Amazon’s massive catalog and
customer base is that they make items’ sales ranks useful
for improving the accuracy of inferences. Suppose (case 1)
that you had previously purchased item A, and today you
purchased item B. This has the same effect on their related-
items lists as if (case 2) you had previously purchased B and
today purchased A. Sales rank can help distinguish between
these two cases, as well as more complicated varieties. We
expect the sales rank for most items to stay fairly consistent
from day to day given a large number of items and customers.
Whichever item was purchased today, however, will likely see
a slight boost in its sales rank relative to the other. The relative
boost will be inﬂuenced by each item’s popularity, e.g., it may
be more dramatic if one of the items is very rare.
Case studies. Amazon does not release individual purchase
records, thus we have no means of verifying our inferences.
The best we can do is see whether the customer reviewed
the inferred item later (within 2 months after the end of our
data collection). Unfortunately, this is insufﬁcient to measure
accuracy. Observing a public review gives us a high conﬁdence
that an inference is correct, but the lack of a review does not
invalidate an inference. Furthermore, the most interesting cases
from a privacy perspective are the purchases of items for which
the customer would not post a public review.
Therefore, our evidence is limited to a small number of
veriﬁable inferences. We present three sample cases. Names
and some details have been changed or removed to protect
the privacy of customers in question. To avoid confusion, the
inferred item is labeled t in all cases, and the supporting
auxiliary items are labeled a1, a2, and so on.
Mr. Smith is a regular reviewer who had written over 100
reviews by Day 1 of our experiment, many of them on gay-
themed books and movies. Item t is a gay-themed movie. On
Day 20, its sales rank was just under 50,000, but jumped
to under 20,000 by Day 21. Mr. Smith’s previous reviews
included items a1, a2, a3, a4, and a5. Item t was not in the
similarity lists for any of them on Day 19 but had moved into
the lists for all ﬁve by Day 20. Based on this information,
our algorithm inferred that Mr. Smith had purchased item t.
Within a month, Mr. Smith reviewed item t.
Ms. Brown is a regular reviewer who had commented on
several R&B albums in the past. Item t is an older R&B album.
On Day 1, its rank was over 70,000, but decreased to under
15,000 by Day 2. Ms. Brown had previously reviewed items
a1, a2, and a3, among others. Item A moved into item a1
and item a2’s similarity lists on Day 2, and also rose higher
in item a3’s list that same day. Based on this information,
our algorithm inferred that Ms. Brown had purchased item t.
Within two months, Ms. Brown reviewed item t.
Mr. Grant is a regular reviewer who appears to be interested
in action and fantasy stories. Item t is a fairly recent fantasy-
themed movie. On Day 18, its sales rank jumped from slightly
under 35,000 to under 15,000. It experienced another minor
jump the following day, indicating another potential purchase.
Mr. Grant’s previous reviews included items a1, a2, and a3.
On Day 19, item t rose in the similarity lists of a1 and a2, and
entered a3’s list. None of the supporting items had sales rank
changes that indicate purchases on that date. Based on this
information, our algorithm inferred that Mr. Grant had bought
item t. Within a month, Mr. Grant reviewed item t.
In all cases, the reviewers are clearly comfortable with
public disclosure of their purchases since they ultimately
reviewed the items. Nevertheless, our success in these cases
suggests a realistic possibility that sensitive purchases can be
inferred. While these examples include inferences supported
by items in a similar genre, we have also observed cross-
domain recommendations on Amazon, and much anecdotal
evidence suggests that revealing cross-domain inferences are
possible. For example, Fortnow points to a case in which an
Amazon customer’s past opera purchases resulted in a recom-
mendation for a book of suggestive male photography [11].
VII. EVALUATION ON A SIMULATED SYSTEM
To test our techniques on a larger scale than is readily
feasible with the real-world systems that we studied, we
performed a simulation experiment. We used the Netﬂix Prize
dataset [28], which consists of 100 million dated ratings of
17,770 movies from 460,000 users entered between 1999
and 2005. For simplicity, we ignored the actual ratings and
only considered whether a user rated a movie or not, treating
the transaction matrix as binary. We built a straightforward
item-to-item recommender system in which item similarity
scores are computed according to cosine similarity. This is
a very popular method and was used in the original published
description of Amazon’s recommender system [21].
We restricted our simulation to a subset of 10,000 users
who have collectively made around 2 million transactions.8
Producing accurate inferences for the entire set of 460,000
users would have been difﬁcult. Relative to the total number of
users, the number of items in the Netﬂix Prize dataset is small:
17,770 movies vs. millions of items in (say) Amazon’s catalog.
At this scale, most pairs of items, weighted by popularity, have
dozens of “simultaneous” ratings on any single day, making
inference very challenging.
Methodology. We ran our inference algorithm on one month’s
worth of data, speciﬁcally July 2005. We assumed that each
user makes a random 50% of their transactions (over their
entire history) public and restricted our attention to users
with at least 100 public transactions. There are around 1,570
such users, or 16%. These users together made around 1,510
transactions during the one-month period in question.
Fig. 8.
Inference against simulated recommender: yield vs. accuracy.
We assume that each item has a public similarity list of 50
items along with raw similarity scores which are updated daily.
We also assume that the user’s total number of transactions
is always public. This allows us to restrict
the attack to