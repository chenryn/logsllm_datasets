The garbage collectors in the JVM are _very_ sophisticated algorithms and do
a great job minimizing pauses.  And Elasticsearch tries very hard to be _garbage-collection friendly_, by intelligently reusing objects internally, reusing network
buffers, and offering features like >.  But ultimately,
GC frequency and duration is a metric that needs to be watched by you, since it
is the number one culprit for cluster instability.
A cluster that is frequently experiencing long GC will be a cluster that is under
heavy load with not enough memory.  These long GCs will make nodes drop off the
cluster for brief periods.  This instability causes shards to relocate frequently
as Elasticsearch tries to keep the cluster balanced and enough replicas available.  This in
turn increases network traffic and disk I/O, all while your cluster is attempting
to service the normal indexing and query load.
In short, long GCs are bad and need to be minimized as much as possible.
**********************************
Because garbage collection is so critical to Elasticsearch, you should become intimately
familiar with this section of the `node-stats` API:
[source,js]
----
        "jvm": {
            "timestamp": 1408556438203,
            "uptime_in_millis": 14457,
            "mem": {
               "heap_used_in_bytes": 457252160,
               "heap_used_percent": 44,
               "heap_committed_in_bytes": 1038876672,
               "heap_max_in_bytes": 1038876672,
               "non_heap_used_in_bytes": 38680680,
               "non_heap_committed_in_bytes": 38993920,
----
- The `jvm` section first lists some general stats about heap memory usage.  You
can see how much of the heap is being used, how much is committed (actually allocated
to the process), and the max size the heap is allowed to grow to.  Ideally,
`heap_committed_in_bytes` should be identical to `heap_max_in_bytes`.  If the
committed size is smaller, the JVM will have to resize the heap eventually--and this is a very expensive process.  If your numbers are not identical, see
> for how to configure it correctly.
+
The `heap_used_percent` metric is a useful number to keep an eye on.  Elasticsearch
is configured to initiate GCs when the heap reaches 75% full.  If your node is
consistently >= 75%, your node is experiencing _memory pressure_.
This is a warning sign that slow GCs may be in your near future.
+
If the heap usage is consistently >=85%, you are in trouble.  Heaps over 90&#x2013;95%
are in risk of horrible performance with long 10&#x2013;30s GCs at best, and out-of-memory
(OOM) exceptions at worst.
[source,js]
----
   "pools": {
      "young": {
         "used_in_bytes": 138467752,
         "max_in_bytes": 279183360,
         "peak_used_in_bytes": 279183360,
         "peak_max_in_bytes": 279183360
      },
      "survivor": {
         "used_in_bytes": 34865152,
         "max_in_bytes": 34865152,
         "peak_used_in_bytes": 34865152,
         "peak_max_in_bytes": 34865152
      },
      "old": {
         "used_in_bytes": 283919256,
         "max_in_bytes": 724828160,
         "peak_used_in_bytes": 283919256,
         "peak_max_in_bytes": 724828160
      }
   }
},
----
- The `young`, `survivor`, and `old` sections will give you a breakdown of memory
usage of each generation in the GC.  These stats are handy for keeping an eye on
relative sizes, but are often not overly important when debugging problems.
[source,js]
----
"gc": {
   "collectors": {
      "young": {
         "collection_count": 13,
         "collection_time_in_millis": 923
      },
      "old": {
         "collection_count": 0,
         "collection_time_in_millis": 0
      }
   }
}
----
- `gc` section shows the garbage collection counts and cumulative time for both
young and old generations.  You can safely ignore the young generation counts
for the most part:  this number will usually be large.  That is perfectly
normal.
+
In contrast, the old generation collection count should remain small, and
have a small `collection_time_in_millis`.  These are cumulative counts, so it is
hard to give an exact number when you should start worrying (for example, a node with a
one-year uptime will have a large count even if it is healthy). This is one of the
reasons that tools such as Marvel are so helpful.  GC counts _over time_ are the
important consideration.
+
Time spent GC'ing is also important.  For example, a certain amount of garbage
is generated while indexing documents.  This is normal and causes a GC every
now and then. These GCs are almost always fast and have little effect on the
node: young generation takes a millisecond or two, and old generation takes
a few hundred milliseconds.  This is much different from 10-second GCs.
+
Our best advice is to collect collection counts and duration periodically (or use Marvel)
and keep an eye out for frequent GCs.  You can also enable slow-GC logging,
discussed in >.
==== Threadpool Section
Elasticsearch maintains threadpools internally. ((("threadpools", "statistics on"))) These threadpools
cooperate to get work done, passing work between each other as necessary. In
general, you don't need to configure or tune the threadpools, but it is sometimes
useful to see their stats so you can gain insight into how your cluster is behaving.
There are about a dozen threadpools, but they all share the same format:
[source,js]
----
  "index": {
     "threads": 1,
     "queue": 0,
     "active": 0,
     "rejected": 0,
     "largest": 1,
     "completed": 1
  }
----
Each threadpool lists the number of threads that are configured (`threads`),
how many of those threads are actively processing some work (`active`), and how
many work units are sitting in a queue (`queue`).
If the queue fills up to its limit, new work units will begin to be rejected, and
you will see that reflected in the `rejected` statistic.  This is often a sign
that your cluster is starting to bottleneck on some resources, since a full
queue means your node/cluster is processing at maximum speed but unable to keep
up with the influx of work.
.Bulk Rejections
****
If you are going to encounter queue rejections, it will most likely be caused
by bulk indexing requests.((("bulk API", "rejections of bulk requests")))  It is easy to send many bulk requests to Elasticsearch
by using concurrent import processes.  More is better, right?
In reality, each cluster has a certain limit at which it can not keep up with
ingestion.  Once this threshold is crossed, the queue will quickly fill up, and
new bulks will be rejected.
This is a _good thing_.  Queue rejections are a useful form of back pressure.  They
let you know that your cluster is at maximum capacity, which is much better than
sticking data into an in-memory queue.  Increasing the queue size doesn't increase
performance; it just hides the problem.  If your cluster can process only 10,000
docs per second, it doesn't matter whether the queue is 100 or 10,000,000--your cluster can
still process only 10,000 docs per second.
The queue simply hides the performance problem and carries a real risk of data-loss.
Anything sitting in a queue is by definition not processed yet.  If the node
goes down, all those requests are lost forever.  Furthermore, the queue eats
up a lot of memory, which is not ideal.
It is much better to handle queuing in your application by gracefully handling
the back pressure from a full queue.  When you receive bulk rejections, you should take these steps:
1. Pause the import thread for 3&#x2013;5 seconds.
2. Extract the rejected actions from the bulk response, since it is probable that
many of the actions were successful. The bulk response will tell you which succeeded
and which were rejected.
3. Send a new bulk request with just the rejected actions.
4. Repeat from step 1 if rejections are encountered again.
Using this procedure, your code naturally adapts to the load of your cluster and
naturally backs off.
Rejections are not errors: they just mean you should try again later.
****
There are a dozen threadpools.  Most you can safely ignore, but a few
are good to keep an eye on:
`indexing`::
    Threadpool for normal indexing requests
`bulk`::
    Bulk requests, which are distinct from the nonbulk indexing requests
`get`::
    Get-by-ID operations
`search`::
    All search and query requests
`merging`::
    Threadpool dedicated to managing Lucene merges
==== FS and Network Sections
Continuing down the `node-stats` API, you'll see a((("filesystem, statistics on"))) bunch of statistics about your
filesystem:  free space, data directory paths, disk I/O stats, and more.  If you are
not monitoring free disk space, you can get those stats here.  The disk I/O stats
are also handy, but often more specialized command-line tools (`iostat`, for example)
are more useful.
Obviously, Elasticsearch has a difficult time functioning if you run out of disk
space--so make sure you don't.
There are also two sections on ((("network", "statistics on")))network statistics:
[source,js]
----
        "transport": {
            "server_open": 13,
            "rx_count": 11696,
            "rx_size_in_bytes": 1525774,
            "tx_count": 10282,
            "tx_size_in_bytes": 1440101928
         },
         "http": {
            "current_open": 4,
            "total_opened": 23
         },
----
- `transport` shows some basic stats about the _transport address_.  This
relates to inter-node communication (often on port 9300) and any transport client
or node client connections.  Don't worry if you see many connections here;
Elasticsearch maintains a large number of connections between nodes.
- `http` represents stats about the HTTP port (often 9200).  If you see a very
large `total_opened` number that is constantly increasing, that is a sure sign
that one of your HTTP clients is not using keep-alive connections.  Persistent,
keep-alive connections are important for performance, since building up and tearing
down sockets is expensive (and wastes file descriptors).  Make sure your clients
are configured appropriately.
==== Circuit Breaker
Finally, we come to the last section: stats about the((("fielddata circuit breaker"))) fielddata circuit breaker
(introduced in >):
[role="pagebreak-before"]
[source,js]
----
         "fielddata_breaker": {
            "maximum_size_in_bytes": 623326003,
            "maximum_size": "594.4mb",
            "estimated_size_in_bytes": 0,
            "estimated_size": "0b",
            "overhead": 1.03,
            "tripped": 0
         }
----
Here, you can determine the maximum circuit-breaker size (for example, at what
size the circuit breaker will trip if a query attempts to use more memory).  This section
will also let you know the number of times the circuit breaker has been tripped, and
the currently configured overhead.  The overhead is used to pad estimates, because some queries are more difficult to estimate than others.
The main thing to watch is the `tripped` metric.  If this number is large or
consistently increasing, it's a sign that your queries may need to be optimized
or that you may need to obtain more memory (either per box or by adding more
nodes).((("Node Stats API", range="endofrange", startref="ix_NodeStats")))