title:Towards highly reliable enterprise network services via inference
of multi-level dependencies
author:Paramvir Bahl and
Ranveer Chandra and
Albert G. Greenberg and
Srikanth Kandula and
David A. Maltz and
Ming Zhang
Towards Highly Reliable Enterprise Network Services
Via Inference of Multi-level Dependencies
Paramvir Bahl, Ranveer Chandra, Albert Greenberg, Srikanth Kandula,
David A. Maltz, Ming Zhang
Microsoft Research
Abstract
Localizing the sources of performance problems in large enterprise
networks is extremely challenging. Dependencies are numerous,
complex and inherently multi-level, spanning hardware and soft-
ware components across the network and the computing infrastruc-
ture. To exploit these dependencies for fast, accurate problem lo-
calization, we introduce an Inference Graph model, which is well-
adapted to user-perceptible problems rooted in conditions giving
rise to both partial service degradation and hard faults. Further, we
introduce the Sherlock system to discover Inference Graphs in the
operational enterprise, infer critical attributes, and then leverage the
result to automatically detect and localize problems. To illuminate
strengths and limitations of the approach, we provide results from a
prototype deployment in a large enterprise network, as well as from
testbed emulations and simulations. In particular, we ﬁnd that tak-
ing into account multi-level structure leads to a 30% improvement
in fault localization, as compared to two-level approaches.
Categories
Communication Network]: Network Operations
General Terms: Management
Keywords: Network & service management, dependencies, fault
localization, probabilistic inference
and Subject Descriptors: C.2.3
[Computer-
1.
INTRODUCTION
Using a network-based service can be a frustrating experience,
marked by appearances of familiar hourglass or beachball icons,
with little reliable indication of where the problem lies, and even
less on how it might be mitigated. Even inside the network of a
single enterprise, where trafﬁc does not need to cross the open In-
ternet, user-perceptible service degradations are rampant. Consider
Figure 1, which shows the distribution of time required for clients
to fetch the home page from a major webserver in a large enterprise
network including tens of thousands of network elements and over
400,000 hosts. The distribution comes from a data set of 18 thou-
sand samples from 23 instrumented clients over a period 24 days.
The second mode of the distribution represents user-perceptible
lags of 3 to 10+ seconds, and 13% of the requests experience this
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’07, August 27–31, 2007, Kyoto, Japan.
Copyright 2007 ACM 978-1-59593-713-1/07/0008 ...$5.00.
Normal
Performance:
status:
 up
Unacceptable
Performance
status:
troubled
0.12
0.1
0.08
0.06
0.04
0.02
)
F
D
P
(
y
t
i
l
i
b
a
b
o
r
p
0
0.01
0.1
1
response time (sec)
10
Figure 1: The response time of a major internal webserver
when fetching the home page. The times are clearly bi-modal,
with 13% of the requests taking 10x longer than normal and re-
sulting in user-perceptible lags. We deﬁne the ﬁrst mode in re-
sponse time as indicating the service is up and the second mode
as indicating the service is troubled.
unacceptable performance. This problem persists because current
network and service monitoring tools are blind to the complex set
of dependencies across systems and networks in the enterprise,
needed for root cause analysis.
Conventional management systems treat each service, which we
deﬁne as an (IPaddr, port) pair, as being either up or down. This
naive model hides the kinds of performance failures shown in Fig-
ure 1. In this paper, we model service availability as a 3-state value:
a service is up when its response time is normal; it is down when re-
quests result in either an error status code or no response at all; and
it is troubled when responses fall signiﬁcantly outside of normal
response times. Our deﬁnition of troubled status includes the par-
ticularly challenging cases where only a subset of service requests
are performing poorly.
This paper describes the Sherlock system that aims to give IT
administrators the tools they need to localize performance prob-
lems and hard failures that affect an end-user. Sherlock (1) detects
the existence of faults and performance problems by monitoring
the response time of services; (2) determines the set of components
that could be responsible; and (3) localizes the problem to the most
likely component.
We faced three main challenges in creating Sherlock. First, both
performance and hard faults can stem from problems anywhere
in the IT infrastructure, i.e., a service, a router, or a link. Adding
complexity to the problem, even simple requests like fetching a
13webpage involve multiple services: DNS servers, authentication
servers, webservers, and the backend SQL databases that hold the
webpage data. Problems at any of these can affect the success or
failure of the request, but the dependencies among components in
IT systems are typically not recorded anywhere, and they evolve
continually as systems grow or new applications are added. As a
result, Sherlock must be able to automatically discover the set of
components involved in the processing of requests. Second, the
failover and load-balancing techniques commonly used in enter-
prise networks make determining the responsible component even
harder, since the set of components involved may change from re-
quest to request. Sherlock’s analysis must take these mechanisms
into account. Third, given the large size of enterprise networks, the
challenges above must be met in a manner that remains tractable
even with hundreds of thousands of elements.
Sherlock meets these challenges in the following ways: First,
software agents running on each host analyze the packets that the
host sends and receives to determine the set of services on which
the host depends. Sherlock automatically assembles an Inference
Graph that captures the dependencies between all components of
the IT infrastructure by combining together these individual views
of dependency. Our algorithm uses information provided by one
host to ﬁll in any gaps in the information provided by another. Sher-
lock then augments the Inference Graph with information about
the routers and links used to carry packets between hosts, and so
encodes in a single model all the components that could affect a
request. Second, our Inference Graph model includes primitives
that capture the behavior of load-balancers and failover mecha-
nisms. Operators identify where these mechanisms are used manu-
ally or via heuristics, but localization is then automatic. Third, we
developed Ferret, an algorithm that efﬁciently localizes faults in
enterprise-scale networks using the Inference Graph and measure-
ments of service response times made by the agents.
We deliberately targeted Sherlock at localizing signiﬁcant prob-
lems that affect the users of the IT infrastructure, hence our focus
on performance as well as hard faults and our use of response time
as an indicator for performance faults. Current systems overwhelm
operators with meaningless alerts (the current management system
in our organization generates 15,000 alerts a day, and they are al-
most universally ignored as so few prove signiﬁcant). In contrast,
Sherlock does not report problems that do not directly affect users.
For example, Sherlock will not even detect that a server has a high
CPU utilization unless requests are delayed as a result.
To the best of our knowledge, Sherlock is the ﬁrst system that
localizes performance failures across network and services in a
timely manner without requiring modiﬁcations to existing appli-
cations and network components. The contributions of this paper
include our formulation of the Inference Graph and our algorithms
for computing it for an entire IT infrastructure based on observa-
tions of the packets that hosts send and receive. Unlike previous
work, our Inference Graph is both multi-level (in order to represent
the multiple level of dependencies found in IT infrastructure) and
3-state (so we can determine whether components are up, down,
or experiencing a performance fault and troubled). This paper also
contributes extensions to prior work that optimize fault localization
and adapt it for our three-state and multi-level Inference Graph. We
extensively evaluate the effectiveness of each of Sherlock’s compo-
nents individually, and describe our results of deploying Sherlock
in both a testbed and a large and complex enterprise network.
2. RELATED WORK
Today, enterprises use sophisticated commercial tools, such as
EMC’s SMARTS [21], HP Openview [13], IBM Tivoli [19], or Mi-
crosoft Operations Manager [10]. In practice, these systems have
proven inadequate for ﬁnding the causes of performance problems
as they treat servers and routers as independent boxes — each pro-
ducing its own stream of SNMP counters, syslog messages, and
alerts. Fundamentally, these box-centric measures are poor predic-
tors of the end-to-end response time that users ultimately care about
— it’s not clear what CPU load on a server means users are un-
happy, so it is hard to set a threshold that alerts only when users
are impacted. For example, over a 10-day period our organization’s
well-run systems generated two thousand alerts for 160 servers that
might be sick. Another 18 K alerts were divided among 194 differ-
ent alert types coming from 877 different servers, each of which
could potentially affect user performance (e.g., 6 alerts for a server
CPU utilization over 90%; 8 for low memory causing a service to
stop). Investigating all the potentially serious alerts is simply im-
practical, especially when many had no effect on a user. Sherlock
complements existing tools by detecting and localizing the prob-
lems that affect users.
Signiﬁcant recent research has led to methods for detailed de-
bugging of service problems in distributed systems. Many of these
systems also extract the dependencies between components, but are
different in character from Sherlock. Magpie [3], FUSE [5] and
Pinpoint [4], instrument middleware on every host to track requests
as they ﬂow through the system. They then diagnose faults by corre-
lating components with failed requests. Project5 [1] and WAP5 [16]
record packet traces at each host and use message correlation algo-
rithms to resolve which incoming packet triggered which outgo-
ing packet. These projects all target the debugging and proﬁling of
individual applications, so determining exactly which message is
caused by another message is critically important. In contrast, Sher-
lock combines measurements of the many applications running on
an IT infrastructure to localize problems. We also show that, for
fault localization, co-occurrence of packets is a reasonable indica-
tor of dependency between accesses to two remote machines, and
that valid graphs can be computed with only 1,000 samples and 20
clients (Section 6.1).
There is a large body of prior work tackling fault localization
at the network layer, especially for large ISPs. In particular, BAD-
ABING [18] and Tulip [9] measure per-path characteristics, such
as loss rate and latency, to identify problems that impact user-
perceptible performance. These methods (and many commercial
products as well) use active probing to pinpoint faulty IP links.
Sherlock instead uses a passive correlation approach to localize
failed network components.
Machine learning methods have been widely discussed for fault
management. Pearl [15] describes a graph model for Bayesian
networks. Sherlock uses similar graph models to build Inference
Graphs. Rish et. al. [17] combines active probing and dependency
graph modeling for fault diagnosis in a network of routers and end
hosts, but they do not describe how the graph model might be au-
tomatically constructed. Unlike Sherlock, their method does not
model failover servers or load balancers, which are common in en-
terprise networks. Shrink [6] and SCORE [7] make seminal contri-
butions in modeling the network as a two-level graph and using it to
ﬁnd the most likely root causes of faults in wide-area networks. In
SCORE, dependencies are encoded as a set and fault-localization
becomes minimal set cover. Shrink introduces novel algorithmic
concepts in inference of most likely root causes, using probabilities
to describe the strengths of dependencies. In Sherlock, we lever-
age these concepts, while extending them to deal with multi-level
dependencies and with more complex operators that capture load-
balancing and failover mechanisms. We compare the accuracy of
our algorithm with Shrink and SCORE in Section 6.
The state of each node in the Inference Graph is expressed by
a three-tuple: (Pup, Ptroubled, Pdown). Pup denotes the probabil-
ity that the node is working normally. Pdown is the probability
that the node has experienced a fail-stop failure, such as when a
server is down or a link is broken. Finally, Ptroubled is the prob-
ability that a node is troubled, which corresponds to the boxed
area in Figure 1, where services, physical servers or links con-
tinue to function but users perceive poor performance. The sum of
Pup + Ptroubled + Pdown = 1. We note that the state of root-cause
nodes is independent of any other nodes in the Inference Graph,
while the state of observation nodes can be uniquely determined
from the state of its ancestors.
An edge from node A to node B in the Inference Graph encodes
the dependency that node A has to be in the up state for node B to
be up. Not all dependencies are equal in strength. For example, a
client cannot retrieve a ﬁle from a ﬁle server if the path to that ﬁle
server is down. However, the client might still be able to retrieve
the ﬁle even when the DNS server is down, if the ﬁle server’s name
to IP address mapping is found in the client’s local DNS cache. Fur-
thermore, the client may need to authenticate more (or less) often
than resolving the server’s name. To capture varying strengths in
dependencies, edges in a Inference Graph are labeled with a de-
pendency probability. A larger dependency probability indicates
stronger dependency.
Finally, every Inference Graph has two special root-cause nodes
– always troubled (AT) and always down (AD) – to model exter-
nal factors not part of our model that might cause a user-perceived
failure. The state of AT is set to (0, 1, 0) and that of AD is set to
(0, 0, 1). We add an edge from these nodes to all the observation
nodes, and describe how we assign probabilities to these edges in
Section 4.
To illustrate these concepts we revisit Figure 2, which shows a
portion of the Inference Graph that models a user fetching a ﬁle
from a network ﬁle server. The user activity of “fetching a ﬁle” is
encoded as an observation node (dashed box) in the ﬁgure because
Sherlock can measure the response time for this action. Fetching a
ﬁle requires the user to perform three actions: (i) authenticate itself
to the system, (ii) resolve the DNS name of the ﬁle server and (iii)
access the ﬁle server. These actions themselves depend on other ac-
tions to succeed. Therefore, we model them as meta-nodes, and add
edges from each of them to the observation node of “fetching a ﬁle.”
We describe our method of computing the dependency probability
for these edges in Section 4.1. Since the client is conﬁgured with
both a primary and secondary DNS server (DNS1 and DNS2), we
introduce a failover meta-node. Finally, note that this snippet shows
a single client and a single observation. When other clients access
the same servers or use the same routers/links as those shown here,
their observation nodes will be connected to the same root cause
nodes as those shown to create the complete Inference Graph.
3.1.1 Propagation of State with Meta-Nodes
A crucial aspect of a probabilistic model is how the state of par-
ent nodes governs the state of a child node. For example, suppose
a child has two parents, A and B; the state of parent A is (.8, .2, 0),
which means its probability of being up is 0.8, troubled is 0.2 and
down is 0, and the state of parent B is (.5, .2, .3). What, then, is the
state of the child? While edge labels encode the strength of depen-
dency, the nature of the dependency is encoded in the meta-node.
Formally, the meta-node describes the state of the child node given
the state of its parent nodes.
Noisy-Max Meta-Nodes are the simplest and most common meta-
node. Max implies that if any of the parents are in the down state,
then the child is down. If no parent is down and any parent is trou-
Figure 2: Snippet of a partial Inference Graph that expresses
the dependencies involved in accessing a ﬁle share. Dotted boxes
represent physical components and software, dashed boxes de-
note external observations and ovals stand-in for unmodeled or
external factors.
3. THE Inference Graph MODEL
We ﬁrst describe our new model, called the Inference Graph,
for representing the complex dependencies in an enterprise net-
work. The Inference Graph forms the core of our Sherlock system.
We then present our algorithm, called Ferret, that uses the model
to probabilistically infer the faulty or malfunctioning components
given real-world observations. We explain the details of how Sher-
lock constructs the Inference Graph, computes the required proba-
bilities, and performs fault localization later in Section 4.
3.1 The Inference Graph
The Inference Graph is a labeled, directed graph that provides a
uniﬁed view of the dependencies in an enterprise network, spanning
services and network components. Figure 2 depicts a portion of the
Inference Graph when a user accesses a network ﬁle share. The
structure of dependence is inherently multi-level. The access to the
ﬁle depends on contacting the Kerberos server for authentication,
which in turn depends on the Kerberos server itself, as well as the
routers and switches on the path from the user’s machine to the
Kerberos server. A problem could occur anywhere in this chain of
dependencies. The challenge is to ﬁnd the right level of abstraction
to model these dependencies in a framework that can be feasibly
automated.
Formally, nodes in this graph are of three types. First, root-cause
nodes correspond to physical components whose failure can cause
an end-user to experience failures. The granularity of root-cause
nodes in Sherlock is a computer (a machine with an IP address),
a service (IP address, port), a router, or an IP link, although the
model is extensible to root causes at a ﬁner granularity. Second,
observation nodes represent accesses to network services whose
performance can be measured by Sherlock. There is a separate ob-
servation node for every client that accesses any such network ser-
vice. The observation nodes model a user’s experience when using
services on the enterprise network. Finally, meta-nodes act as glue
between the root-cause nodes and the observation nodes. In this pa-