Specifically, 
for each 
component, 
the controller 
host or change the 
to the VM on its current 
host. 
in such a way as to 
the desired 
degradation 
in the case 
level of repli­
factors 
in 
several 
dictates 
that applica­
chooses 
these actions 
It has to balance 
performance 
The controller 
the overall 
performance 
while still maintaining 
minimize 
of a failure 
cation and reliability. 
doing so. Maximizing 
tion components 
mize the impact of network latency, 
nents too closely 
degrade performance 
sources 
quiring 
across higher resource 
liability 
than they require. 
high levels 
level, the controller 
of reliability 
by forcing 
levels. 
(e.g., 
be placed close to one another 
to mini­
but packing compo­
on the same machine) 
may actually 
VMs to use less CPU re­
re­
For reliability, 
an application 
of each application 
and define availabil­
types is necessary 
failure. 
as one 
availability. 
machine, 
However, 
component 
We consider 
replication 
application 
time window.  A 
of each component 
the replication 
on an operational 
even if a single failure 
chooses and implements 
failure 
hierarchy, 
Our system has dual goals: high availability 
and good 
the system to be available 
when 
level of some com­
reduced while the con­
its regeneration 
rates to decrease 
they must still be ac­
Fur­
does not bring down 
this is not al­
of the same type are con­
group, e.g., a rack, then 
of that group can still cause application 
we anticipate 
performance. 
at least one replica 
is running 
ity as the fraction of time the system is available 
over a 
specified 
level of at least two 
for each of the application's 
to avoid single points of failure. 
ways sufficient. 
If all replicas 
tained within a single resource 
a failure 
Although 
moves up the resource 
counted for when computing 
thermore, 
a whole application, 
ponent types may be temporarily 
troller 
Regeneration 
is small, but it cannot be completely 
fore, we allow each application 
troller 
sired value for the application's 
ures", or MTBFa. This application 
gether with information 
icy codified by the MTTR to calculate 
AvailabilitYa 
For performance, 
system configuration 
occur as the goal performance. 
note this goal response 
ing to application 
sponse time for this application 
performance 
then be defined as the weighted 
degradations, 
+ MTTR). 
we use the performance 
before any failures 
mean re­
and transaction 
type. The 
failures 
can 
type 
or control 
let RT% t de-
Specifically, 
type t belong-
time for a transaction 
a and let RT7::t be the measured 
MTBF can be used to­
recovery 
pol­
as 
by specifying 
"mean time between fail­
=  MTBFa/(MTBFa 
D due to resource 
to the con­
a de­
level of reliability 
about the system's 
eliminated. 
There­
degradation 
to indicate 
its desired 
ensures 
, 
i.e., 
D =  L "(a,t(RT:;:t -RT!,t) 
aEA,tETa 
(1) 
sum of per transaction 
its availability 
of the initial 
actions 
will have to be distributed 
To achieve 
may distributes 
the required 
components 
re­
where weights 
cording to 
of the transaction. 
transactions 
varying 
'a,t are used to weigh the transaction 
ac­
importance 
Choosing 
and frequency 
the weight as the fraction 
of occurrence 
of 
of type t in the application's 
workload 
makes 
978-1-4244-7501-8/10/$26.00 
©201O IEEE 
499 
DSN 2010: Jung et al. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:14:00 UTC from IEEE Xplore.  Restrictions apply. 
2010 IEEEIIFIP International 
Conference 
on Dependable 
Systems & Networks (DSN) 
of the application 
4 Runtime Optimizer 
D equal to a's mean response 
these two metrics, 
veniently 
compared. 
alternative 
time degradation. 
Given 
control 
strategies 
can be con­
resource 
the response 
our approach 
(to calculate 
degradation 
times of application 
of alternative 
application 
the performance 
requires 
system con­
models that 
transactions 
given 
D) and the corre­
system 
In this paper, we use layered queuing 
To quantify 
figurations, 
predict 
a workload 
sponding 
configurations. 
models [27] and the command-line 
tool [13] as the model solver. 
the validation 
however, 
cluding 
of the LQNS 
The LQN models used, and 
that other types of models could also be used, in­
of the models, is described 
demands in different 
to those introduced 
utilization 
ones similar 
in [17]. Note, 
version 
in [6]. 
Given a set of MTBF values for each resource 
level r 
mod­
with 
levels 
according 
the resource 
components 
the reliability 
models al­
process 
all the hosts 
For the reliability 
group on level r fails 
of the MTBF values for each appli­
by the opti­
over which an appli­
and the placement 
to a Poisson failure 
on these resources, 
nodes can be distributed. 
in the data centers 
components 
low the computation 
cation a. These values are used as constraints 
mizer to determine 
cation's 
els, we assume that each resource 
independently 
rate Ar =  l/MTBF r and each failure 
and application 
replication 
of a resource 
failed. 
a. For each type na E Na, let rmax(na) 
resource 
that resource 
if an application's 
Centerl 
rmax(dba) 
resource 
tion level of component 
result 
:Rackl :Hostl and DataCenterl 
levels rmax(na) 
=  DataCenterl. 
all the Na component 
level such that all replicas 
level of an application 
the application 
of application 
level. From the example shown in Figure 1, 
hosted in Data­
had 2 replicas 
Then, only those failures 
or higher will cause the replica­
type na to fall to zero and thus 
be the highest 
of na are contained 
disables 
in the group. If the 
drops to zero as a result 
is considered 
in a failure 
contained 
database 
Consider 
failure, 
to have 
:Rack2:Host3, 
then 
a. 
in 
at 
types of application 
that no additional 
its reconfiguration 
fail­
makes the simplifying  assumption 
ures occur in the time window between the first failure 
and 
takes to decide and 
the time the regeneration 
While this is not 
implement 
strictly 
in light of the 
fact that all  our 
considered 
dently and as shown in Section 
reconfiguration 
resource 
controller 
actions. 
assumption 
6, controller 
are very short compared 
think time and 
to 
action durations 
true, it is a reasonable 
MTBF values. 
failure 
events occur indepen­
MTBFa(c) =  ( L  MTBFr-1) -1 
(2) 
VrER s.t. :lna ENa 
s.t. rrnax (na)
r 
the runtime controller 
that maintain 
the applica­
MTBF values, 
and 
the 
is 
by minimizing 
1. The minimization 
system configu­
degradation 
function 
in Equation 
or recovery 
Upon failure 
any performance 
events, 
chooses system configurations 
tions' replication 
levels and desired 
minimize 
degradation 
carried 
out over the space of all possible 
rations c E C, each of which specifies: 
of each replica nk to a physical 
the CPU share cap c.cap(nk)
space with mixed discrete 
optimization 
of a host is not just influenced 
ity, but also by its location 
parison 
relatively 
resource 
packing 
lutions. 
with the application's 
simple problem of replica 
tier is NP-Complete 
problem), 
task is challenging. 
so we have to settle 
in the resource 
Furthermore, 
the choice 
by its available 
CPU capac­
hierarchy 
in com­
other components. 
Even the 
assignment 
(via a reduction 
in a single 
to the bin­
for approximate 
so­
(a) the assignment 
host c.host(nk), and (b) 
. Due to the large parameter 
parameters, 
and continuous 
the 
To solve this problem, 
we split it into two sub-problems: 
it affects resource r with probability 
is 
failure 
the overall 
Under these assumptions, 
process 
with rate LrER Ar. When a fail­
acts to filter failure 
also a Poisson process 
ure event occurs, 
a to fail if r is such 
Ar/ LrER An and causes application 
type na with a value 
that there is at least one component 
of rmax(na) 
that is lower than r (i.e., rmax(na) 
::;'R r). 
Since this condition 
the appli­
cation failure 
process 
with a rate 
given by the sum of Ar values over those resources 
whose 
failure 
of the rmax (na) values that are 
the filtering 
dependent 
process 
is a time-varying 
the system is reconfigured 
on the exact system configuration, 
by the regeneration 
Using the preceding  discussion, 
one whose rate changes 
also causes application 
MTBFa in a system 
is also a Poisson 
is a function 
events, 
process 
the Poisson 
whenever 
a to fail. Additionally, 
since 
configuration 
c is given by Equation 
2. This equation 
controller. 
another 
that have an 
proposed 
function, 
is rejected 
considered 
by the search. 
i.e., application 
for the parameters 
If a 
during fit step, the search step is in­
(a) a search step that chooses parameters 
perfor­
impact on the objective 
mance, and (b) a fit step that chooses parameters 
that do 
not. Then, we treat the parameters 
in the fit 
step as the constraints, 
mechanism 
solution 
voked again to provide 
able configuration 
for the search step, we note that the CPU cap c.cap(nk) 
allocated 
tion's 
search step. However, 
host a component 
steps by observing 
to its components,  network latency 
able that impacts 
of L(rl) in Section 
performance 
choosing 
can be divided 
that for a given assignment 
in the 
and must be included 
the machine on which to 
across the search and fit 
solution 
is found. In choosing 
According 
3, the network latency 
of CPU caps 
is the only other vari­
to an application 
performance. 
end-to-end 
component 
impacts 
the applica­
to our definition 
is a function 
of 
until an accept­
the parameters 
and use them as an accept-reject 
978-1-4244-7501-8/10/$26.00 
©201O IEEE 
500 
DSN 2010: Jung et al. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:14:00 UTC from IEEE Xplore.  Restrictions apply. 
2010 IEEE/IFIP International 
Conference 
on Dependable 
Systems & Networks (DSN) 
W: workload 
Corig: original 
Input: 
Input: 
Input: R: avail. resources 
Output: 
forall 
a E A do 
config., {RTg}: initial 
resp. times 
after failure/recovery 
Acts: sequence of reconfiguration 
event 
actions 
l "Ink E N, c.cap(nk) 
MTBF(c[Vn 
c.dla f- min{rl E RLIVr E R S.t. r.rl =  rl, 
E Na,rmax(n) f-r]) > MTBFa} 
f- 1 
CC f- {c}, Cold f- c, Compute c.D; 
while/orever do 
forall 
c E CC do 
IVa, nd f-Fi t (WhoieSyslem, 
c, A) 
{c.host(nk) 
if success then FCC f- FCC U {c} 
forall 
a E A do l ({R1':'d, {p(nk)IVnk 
E 
N}) f- LQNS (W , a, c) 
Compute c.D, gradient c.\7 p w.r.t. Cold 
if FCC =I-{} then 
I c f- mincE FCC c.D; return Act s (Corig ov-; C) 
else 
c f- maxcECC c. \7 p; Cold f- c; CC -'>  {} 
forall 
a E A do 
c' f- c; Inc. c'.dl(a); 
forall 
n E Na do 
CC f- CC U {c'} 
l n: reps(n), 
c.cap(nk) 
CC f- CC U {Cn} 
-D.r 
clcap(nk) 
= 
Algorithm 1: Search Algorithm 
Input: rp: resource tier to pack, c.cap: 
Bundle CPU 
B: the application 
capacities, 
c.host 
-replica placements 
and replica bundles 
Output: 
forall 
b E B do l if c.dlb 
=  rp.rl then 
B f- (B -{b}) U {Replicas ofb} 
r E Rp do r.cpu f- Total CPU cap. of resource r 
Rp f- {r E Rlr 5:R rp} 
forall 
B' f-sort (c.cap(b)IVb 
forall 
b E B' in decreasing 
order do 
E B) 
5: r.cpu 1\ (reps(t
l forall 
r E Rp in order do l if c.cap(b) 
-.3b' E B, s.l. c.host(b'
then 
L c.host(b) 
ype(b» > IRpl V 
) =  rl\b.type =  b'.type) 
f- r; r.cpu f- r.cpu -c.cap(b) 
forall 
r E R do Fit(r, c, {b E Blc.host(b) 
=  r}) 
Algorithm 2: Fit Algorithm 
level c.dla 
for application 
the application's 
MTBFa requirement. 
c, we choose the lowest resource 
level with lower network la­
better 
a low resource 
configuration 
As the distribution 
availability. 
a in initial 
level that satisfies 
This is because 
tency is 
capacity constraints 
permissible resource 
the application 
ble resource 
r for all the application's 
pick the lowest resource 
are higher than the application's 
desired 
from a performance 
are not a factor. 
level, 
we use equation 
MTBF when distributed 
levels r, i.e., with the value of rmax (na) set to 
Na. Then, we 
types in 
component 
level for which all MTBF values 
To choose the lowest 
2 to compute 
point of view when CPU 
across each possi­
MTBF. 
The search algorithm 
explores 
the configuration-space 
a distribution 
in 
hierarchy. 
resources. 
Thus, 
the choice of which re­
between two components 
level rather than individual 
racks to be the same irrespective 
Therefore, 
an application 
the resource 
the models assume the latency 
different 
of which rack 
they are in as long as both racks share the same parent in 
the resource 
source level to distribute 
tribution 
actual component 
is determined 
level of "rack" requires 
component 
a distribution 
replicas 
fit steps are described 
the dis­
in the search step, while the 
within the distribution 
placing 
the 
The search and 
level of "whole system" entails 
in the fit step. For example, 
hosts in the same rack, while 
on hosts in different 
across different 
is determined 
data centers. 
in more detail 
of replicas 
placement 
placement 
of the same 
across, 
level, 
below. 
i.e., 
level 
that starts 
Search Algorithm. The search algorithm 
in Algo­
from the best pos­
and 
by 
irrespective 
the configuration 
of capacity 
rithm 1 is a greedy algorithm 
sible configuration 
degrades 
iteratively 
the fit algorithm. 
dation function 
decrease 
its replicas 
tems). Therefore, 
the entire 
if additional 
(response 
the initial 
CPU) for each replica 
We use the observation 
until it is accepted 
that the degra­
time) of an application 
does not 
to one of 
that is true for most sys­
is provided 
CPU capacity 
(this is an assumption 
irrespective 
of actual CPU 
CPU caps are set to 1.0 (i.e., 
constraints, 
includes 
only the "best possible" 
above. For each configuration 
configurations", 
de­
in the can­
is invoked to  try  to 
currently 
or ee, which 
configuration 
bin-pack 
of each node. The LQNS solver is also invoked 
using the CPU caps as the 
machines 
using a set of "candidate 
initially 
scribed 
didate set, the fit algorithm 
the nodes on physical 
"volume" 
for each application 
tual CPU utilization 
CPU caps and network latencies 
plication's 
figurations 
in the algorithm, 
lowest performance 
algorithm 
a "gradient 
distribution 
level. 
provide 
then the feasible 
degradation 
picks the candidate 
function" 
a feasible 
If one or more candidate 
corresponding 
to the ap­
con­
in set FCC 
fit as collected 
configuration 