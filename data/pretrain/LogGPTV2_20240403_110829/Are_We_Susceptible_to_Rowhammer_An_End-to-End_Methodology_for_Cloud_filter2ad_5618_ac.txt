IV. STEP 1: GENERATING THE HIGHEST RATE OF ACT
COMMANDS ON A SERVER ARCHITECTURE
We ﬁrst describe the system setup we used to measure the
rate of row activations of an instruction sequence. Measuring
ACT rates lets us (1) ﬁnd which instruction sequence generates
the highest ACT rate on a particular server platform, and (2)
quantify the difference between this highest ACT rate and
the optimal rate determined from DRAM datasheets [61]. We
then evaluate the performance of instruction sequences used by
prior work to mount Rowhammer. Finally, we present a new
instruction sequence that generates near-optimal row activation
rates on Intel Skylake and Cascade Lake architectures.
A. System Setup for Measuring ACT Rates
To determine the instruction sequence that generates the
highest rate of ACT commands, we used the FS2800 DDR De-
tective from FuturePlus Systems with two DIMM interposers
for DDR3 and DDR4 [27]. This system can monitor, capture,
and measure DDR commands issued over the command bus
to the DIMM using a DDR interposer and an FPGA that
triggers on speciﬁc DDR commands or memory addresses.
Once triggered,
the FPGA records all DDR commands it
observes on the bus and stores them in buffers, which are
later transferred to a host computer over USB.
The traces gathered with the bus analyzer provide ground
truth information about the rate of activations of a DRAM row
and the memory controller’s behavior, including the logical
addresses used to access DRAM. We use these traces to
characterize the ACT rates of different Rowhammer instruction
sequences from previous work and to construct a sequence that
has a near-optimal ACT rate on Skylake and Cascade Lake.
We found it difﬁcult to use a high-level OS (e.g., Linux)
for our methodology for two reasons: (1) an OS introduces
complex virtual-to-physical address mappings that can change
dynamically, and (2) an OS’s services introduce interfering
trafﬁc to a DIMM when testing.
Instead, our methodology boots the computer into the UEFI
mode [107]. In this mode,
the virtual-to-physical address
map is linear and does not change across reboots. UEFI’s
simplicity and lack of OS services eliminate any interfering
DDR trafﬁc from our traces. However, it also increases the
amount of engineering effort required to implement our testing
methodology because UEFI lacks many services commonly
found in a commodity OS. Therefore, we had to implement
support for multi-threading, hardware discovery [108], [19]
and performance counters.
B. Performance Evaluation of Prior Instruction Sequences
Our results are based on experiments with six server-class
DIMMs that one cloud provider sourced from three different
memory vendors, two DIMMs per vendor. In alphabetical or-
der, these vendors are: Hynix, Micron, and Samsung. Although
sourced from different vendors, the DIMMs’ specs are similar;
they are registered ECC 32GB DDR4 (x4); the DIMMs from
two of the vendors have transfer rates of 2400 MT/s; and the
third vendor’s DIMMs have rates of 2666 MT/s. We found
negligible differences in the performance of an instruction
sequence from one DIMM to another. For consistency, the
results presented in this section use the same DIMM. One of
the timing parameters in the JEDEC speciﬁcation is row cycle
time (tRC) – the minimum period of time between two back-
to-back ACT commands. The JEDEC speciﬁcation lists tables
of minimum and maximum tRC values for different types
of DDR4 memory; these values depend on many memory
characteristics, such as speed, clock cycle, capacity, and so
5
loop:
movzx rax, BYTE PTR [rcx]
movzx rax, BYTE PTR [rdx]
clflush BYTE PTR [rcx]
clflush BYTE PTR [rdx]
mfence
jmp loop
Figure 3: Typical Rowhammer instruction sequence.
on. Based on our DDR4 memory’s characteristics, the JEDEC
speciﬁcation lists the minimum value of tRC as 47ns and does
not specify a maximum value (see Table 110 in [61]).
We measured tRC to be 46.7ns on all our hardware, cor-
responding to a rate of 167.4 ACT commands between two
consecutive REF commands issued by the memory controller
(i.e., one tREFI interval
in JEDEC terminology). We call
46.7ns the optimal
latency between two ACT commands,
and 167.4 ACTs/tREFI the optimal rate. All results presented
are based on experiments running on Skylake, although we
ran many of these experiments on Broadwell and Cascade
Lake with similar results. All servers use motherboards with
multiple CPU sockets.
Previous work used a variety of different instruction se-
quences in a loop to test for Rowhammer [69], [32], [114],
[13], [96], [11], [37], [94], [97], [110], [77], [59], [2], [35],
[36], [105]. Some of these sequences use memory barriers
to serialize the memory accesses on each iteration of the
loop [69], [114], [96], [59], whereas others do not [32],
[114], [11], [94], [37], [110], [2], [105]. To bypass the cache
hierarchy, some instruction sequences use an explicit CPU
ﬂush instruction (e.g., clﬂush), but not all do. Some use cache
collisions to ﬂush the cache [83]; others hypothesize that non-
temporal load instructions [44] could bypass the cache [110],
[35]. Another strategy we encountered was choosing a pair
of rows to hammer from a memory region marked as un-
cached [35]. Finally, the x86-64 architecture offers additional
instructions for cache invalidation, such as invd [45] and
wbinvd [46].
The pseudo-code in Figure 3 describes a typical sequence
that issues two load instructions, two clﬂush instructions, and
a global memory barrier, all in one loop. Several papers on
Rowhammer [69], [114], [96], including the original Rowham-
mer work [69], used this sequence.
Faced with all these choices of possible instruction se-
quences, we considered two questions: (1) Which prior in-
struction sequence maximizes the rate of ACT commands? (2)
How far from the optimal rate of ACT commands is the best
instruction sequence?
To answer these questions, we constructed 42 different
instruction sequences that let us experiment with:
• All three types of fences available on x86-64 architectures:
mfence [47], lfence [48], and sfence [49].
• Both clﬂush and clﬂushopt [50] commands (the latter is
an optimized cache ﬂushing command with weaker ordering
semantics than the former).
• Marking as uncacheable the hammered memory pages’
PTEs, which eliminates the need to issue any CPU ﬂush
commands.
• Both regular and non-temporal [44] memory accesses.
• Using the invd [45] and wbinvd [46] commands to invalidate
CPU caches.
• Using a cache invalidation scheme based on cache line
conﬂicts, similar to the one used by Nethammer [83] and
ANVIL [5].
Figure 4 shows the performance of four typical instruc-
tion sequences used in many previous Rowhammer attack
papers [69], [32], [114], [11], [94], [37], [110], [96], [2],
[105]. These four sequences are identical except for the type of
memory barrier they use to serialize the reads and ﬂushes on
each loop iteration. The assembly code is shown to the right
of the graphs; the four sequences use mfence [47], lfence [48],
sfence [49], and no fence at all, respectively.
Figure 4a shows the CDF of the rate of row activations for
each sequence as well as the optimal rate (with a dashed line).
Despite the popularity of these instruction sequences, which
were used by previous papers to mount Rowhammer attacks,
we discovered that they do not create worst-case ACT rates
– their rates of row activations are 47% from optimal. Even
worse, using mfence, a sequence used by [69], [114], [96],
leads to the slowest ACT rate of the four sequences. The most
effective sequences impose no ordering and use no fences or
only store fences (sfence). A store fence has no ordering effect
because no sequences issue any stores.
Figure 4b shows the CDF of the latencies between two
consecutive row activations for each instruction sequence.
Although the sequence using mfence has the slowest ACT
rate, half of its latencies are optimal. We examined its behavior
closely and found that the two reads inside the loop are always
issued back-to-back at the optimal latency (46.7ns). However,
the next ACT command is issued after a long delay of over
220ns, which is caused by the mfence. The delay explains why
this sequence has a low rate of ACT commands This bimodal
delay between ACTs is clearly visible in Figure 4b.
These results illustrate the gap between a proof-of-concept
Rowhammer attack and the needs of a DRAM testing method-
ology. Although the instruction sequences shown in Figure 4
have been used in many papers to mount various successful
Rowhammer attacks, their ACT rates are far from optimal.
This suggests that the DIMMs found vulnerable in previous
work succumbed even to a low-rate Rowhammer attack. A
DRAM testing methodology based on these instruction se-
quences falls short of creating the worst-case testing condi-
tions for DRAM, which is necessary to conﬁdently determine
whether a chip is vulnerable to Rowhammer.
Using clﬂushopt improves ACT rates. With Skylake, Intel
introduced an optimized version of the cache ﬂush instruction,
called clﬂushopt, that has weaker ordering requirements than
clﬂush [50], [54]. Multiple clﬂushopt instructions to different
cache lines can execute in parallel. We performed a detailed
analysis by implementing support for performance counters
in UEFI mode [107]. We found that using clﬂushopt in our
instruction sequences takes only 3 micro-ops, whereas clﬂush
takes 4.
Figure 5 shows the three instruction sequences using mem-
ory barriers that have similar ACT rates; some have slightly
higher rates (the ones using sfence and lfence), whereas others
have slightly lower rates (the one using mfence). Although it
is difﬁcult to quantify how different types of barriers affect the
6
Figure 4: Performance of typical Rowhammer instruction sequences found in [69], [32], [114], [11], [94], [37], [110], [96],
[2], [105]. The left graph shows the CDF of the rate of ACT commands per tREFI; the right graph shows the CDF of the
ACT-to-ACT latencies. The dotted black line corresponds to the optimal rate of ACT commands.
Figure 5: Performance of the same Rowhammer instruction sequences when using clﬂushopt (rather than clﬂush).
performance of the two cache line ﬂush instructions, the high-
level ﬁnding remains the same: memory barriers are slow, and
instruction sequences using barriers have low ACT rates.
In contrast, the sequence that uses no memory barriers has
a much higher rate of ACT commands of 112 every tREFI,
corresponding to 33% from optimal. The lack of a memory
barrier causes this instruction sequence to have the highest
ACT rate overall. This ﬁnding is not intuitive – the lack of
memory barriers makes the CPU re-order memory accesses
to increase the likelihood that they will be served from the
cache. We measured this sequence and found its cache hit
rate to be 33% (in contrast, the sequence using mfence has
a 0% cache hit rate). Despite the CPU cache acting as a de
facto rate limiter to Rowhammer attacks, the ACT rate of this
instruction sequence is higher than those using any type of
barrier.
Uncached PTEs. We experimented with an instruction se-
quence that does not issue cache ﬂushes, but instead marks its
memory pages uncacheable. The sequence has a low rate of
ACT commands.
Figure 6 shows the performance of an instruction sequence
that does not issue cache ﬂushes, but instead marks its memory
pages uncacheable. Such a sequence has a low rate of ACT
commands, and also a very regular behavior: its ACT-to-ACT
latencies are almost always 110ns apart (in a small fraction
of cases, this latency increases because the ACT is blocked
behind an ongoing refresh command). These results suggest
that loading an address from uncached memory has a ﬁxed
high cost, making this instruction sequence have a low rate of
ACT commands.
Non-temporal memory accesses.
Intel offers ﬁve non-
temporal store instructions for different data types (e.g., inte-
gers, doublewords, ﬂoating point, etc.) and one load instruction
for double quadwords. These instructions do not follow normal
cache-coherence rules and fetch their corresponding cache line
from DRAM.
While experimenting with non-temporal instructions, we
discovered that accesses are cached, and not served by DRAM.
According to Intel’s documentation, these accesses use a form
of temporary internal buffers that might prevent them from
accessing DRAM, which can explain our ﬁndings.
We also experimented with instruction sequences that com-
bine non-temporal and regular memory accesses. We expected
that the different caching semantics of these two types of mem-
ory accesses would ﬂush the caches (or internal buffers) in
each loop iteration. Previous work also proposed mixing these
two types of memory accesses for mounting Rowhammer [35].
We found these instruction sequences to be ineffective. Fig-
ure 7 shows the performance of one such instruction sequence
to be far from optimal (67% from optimal).
Full cache invalidations, cache collisions, loads vs. stores.
We experimented with replacing the cache line ﬂush instruc-
tions with full cache invalidation instructions: invd [45] and
wbinvd [46]. We found that full cache invalidation instructions
are very expensive, making the instruction sequences have
low ACT rates. We also experimented with generating cache
conﬂicts to evict cache lines, but did not ﬁnd higher ACT
rates. Finally, we experimented with replacing loads with
stores in various instruction sequences and found negligible
performance differences. For brevity, we omit showing these
results.
Differences across memory vendors. While all results shown
use DRAM from one single vendor, we performed these
experiments on DRAM from all three vendors. When using the
same instruction sequence, we found no signiﬁcant differences
in the ACT rates when using DRAM from different vendors.
Key Takeaways
• All previously proposed instruction sequences are sub-
optimal for two reasons: (1) memory barriers are expensive
and thus reduce the ACT rate, and (2) in the absence of