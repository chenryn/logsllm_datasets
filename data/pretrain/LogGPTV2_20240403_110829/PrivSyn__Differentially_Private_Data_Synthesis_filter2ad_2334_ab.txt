In this section, we ﬁrst propose a general framework for gener-
ating differentially private synthetic datasets, and then review
some existing studies in this framework. PrivSyn follows this
framework and proposes novel techniques for each of the
component in the framework.
To generate the synthetic dataset in a differentially private
way, one needs to ﬁrst transform the task to estimate a func-
tion f with low sensitivity ∆ f . One straightforward approach
is to obtain the noisy full distribution, i.e., the joint distribu-
tion of all attributes. Given the detailed information about
the distribution, one can then generate a synthetic dataset
by sampling from the distribution. However, when there are
many attributes in the dataset, computing or even storing the
full distribution requires exponentially large space. To over-
come this issue, one promising approach is to estimate many
USENIX Association
30th USENIX Security Symposium    931
Step
Method
PriView [44]
PrivBayes [53]
PGM [41]
PrivSyn
Marginal Selection
Noise Addition
Post Processing
Data Synthesis
Covering design
Bayesian network + Info Gain (EM)
- (not dense)
Optimization + Greedy
Equal budget + Laplace
Equal budget + Laplace
Equal budget + Gaussian
Weighted budget + Gaussian
Max-entropy Estimation
-
Markov Random Field
Consistency
-
Sampling
Sampling
GUM
Table 1: Summary of existing methods on different steps. The four steps are all new. Our marginal selection method enables
private auto selection of marginals. GUM enables usage of dense graphical model.
low-degree joint distributions, also called marginals, which
are distributions of only a subset of attributes. More speciﬁ-
cally, to generate a synthetic dataset, there are four steps: (1)
marginal selection, (2) noise addition, (3) post-processing,
and (4) data synthesis.
The current best-performing approaches on private data
synthesis all follow this approach. Table 1 summarizes these
four steps of existing work and our proposed method. In what
follows, we review these steps in the reverse order.
3.1 Data Synthesis
To synthesize a dataset, existing work uses graphical mod-
els to model the generation of the data. In particular,
PrivBayes [53] uses a differentially private Bayesian network.
It is a generative model that can be represented by a directed
graph. In the graph, each node v represents an attribute, and
each edge from u to v corresponds to Pr [v|u], the probability
of u causing v. As each attribute can take multiple values, all
possible Pr [v = y|u = x] are needed. When a node v has more
than one nodes U = {u1, . . . ,uk} connected to it, Pr [v|U] is
needed to sample v. Because the causality is a single-direction
relationship, the graph cannot contain cycles. To sample a
record, we start from the node with in-degree 0. We then tra-
verse the graph to obtain the remaining attributes following
the generation order speciﬁed by the Bayesian network.
More recently, [41] proposed to sample from differen-
tially private Markov Random Field (MRF). Different from
Bayesian network, MRF is represented by undirected graphs,
and each edge u,v contains the joint distribution Pr [v,u].
Moreover, cycles or even cliques are allowed in this model.
The more complex structures enable capturing higher dimen-
sional correlations, but will make the sampling more challeng-
ing. In particular, one ﬁrst merge cliques into nodes and form
a tree structure, which is called junction tree. The data records
can then be sampled from it. The main shortcoming of PGM
is that, when the graph is dense, the domain of cliques in the
junction tree could be too large to handle.
3.2 Marginal Selection
To build a graphical model, joint distributions in the form of
Pr [v,u] are needed (note that conditional distributions Pr [v|u]
can be calculated from joint distributions). The goal is to cap-
ture all the joint distributions. However, by the composition
property of DP, having more marginals leads to more noise in
each of them. We do not want to select too many marginals
which leads to excessive noise on each of them.
PrivBayes chooses the marginals by constructing the
Bayesian network. In particular, it ﬁrst randomly assigns an
attribute as the ﬁrst node, and then selects other attributes
one by one using Exponential Mechanism (EM). The original
Bayesian network uses mutual information as the metric to
select the most correlated marginals. In the setting of DP, the
sensitivity for mutual information is high. To reduce sensi-
tivity, the authors of [53] proposed a function that is close to
mutual information.
Another method PriView [44] uses a data independent
method to select the marginals. In particular, a minimal set of
marginals are selected so that all pairs or triples of attributes
are contained in some marginal. When some attributes are
independent, capturing the relationship among them actually
increases the amount of noise. This approach cannot scale
with the number of attributes d.
Noise Addition. Given the marginals, the next step is to add
noise to satisfy DP. The classic approach is to split the privacy
budget equally into those marginals and add Laplace noise.
Post Processing. The DP noise introduces inconsistencies,
including (1) some estimated probabilities being negative, (2)
the estimated probabilities do not sum up to 1, and (3) two
marginals that contain common attributes exist inconsistency.
In PrivBayes, negative probabilities are converted to zeros.
In PGM, consistencies are implicitly handled by the estima-
tion procedure of the Markov Random Field.
4 Differentially Private Marginal Selection
In the phase of obtaining marginals, there are two sources
of errors. One is information loss when some marginals are
missed; the other is noise error incurred by DP. PrivBayes
chooses few marginals; as a result, useful correlation infor-
mation from other marginals is missed. On the other hand,
PriView is data-independent and tries to cover all the poten-
tial correlations; and when there are more than a few dozen
attributes, the DP noise becomes too high.
932    30th USENIX Security Symposium
USENIX Association
v
Mgender(v)
(cid:104)male,∗(cid:105)
(cid:104)female,∗(cid:105)
(a) 1-way marginal for gender.
0.40
0.60
v
Mage(v)
(cid:104)∗,teenager(cid:105)
(cid:104)∗,adult (cid:105)
(cid:104)∗,elderly(cid:105)
(b) 1-way marginal for age.
0.20
0.30
0.50
v
(cid:104)male, teenager(cid:105)
(cid:104)male, adult(cid:105)
(cid:104)male, elderly(cid:105)
(cid:104)female, teenager(cid:105)
(cid:104)female, adult(cid:105)
(cid:104)female, elderly(cid:105)
0.08
0.12
0.20
0.12
0.18
0.30
v
(cid:104)male, teenager(cid:105)
(cid:104)male, adult(cid:105)
(cid:104)male, elderly(cid:105)
(cid:104)female, teenager(cid:105)
(cid:104)female, adult(cid:105)
(cid:104)female, elderly(cid:105)
0.10
0.10
0.20
0.10
0.20
0.30
(c) 2-way marginal assume indepent
(d) Actual 2-way marginal
Figure 1: Example of the calculation of InDif.
To balance between the two kinds of information loss, we
propose an effective algorithm DenseMarg that is able to
choose marginals that capture more useful correlations even
under very low privacy budget.
4.1 Dependency Measurement
To select marginals that capture most of the correlation infor-
mation, one needs a metric to measure the correlation level.
In Bayesian network, mutual information is used to capture
pair-wise correlation. As the sensitivity for mutual informa-
tion is high, the authors of [53] proposed a function that can
approximate the mutual information. However, the function
is slow (quadratic to the number of users in the dataset) to
compute.
To compute correlation in a simple and efﬁcient way, in this
subsection, we propose a metric which we call Independent
Difference (InDif for short). For any two attributes a,b, InDif
calculates the (cid:96)1 distance between the 2-way marginal Ma,b
and 2-way marginal generated assuming independence Ma ×
Mb, where a marginal MA speciﬁed by a set of attributes A
is a frequency distribution table, showing the frequency with
each possible combination of values for the attributes, and ×
denote the outer product, i.e., InDifa,b = |Ma,b − Ma × Mb|1.
Figure 1 gives an example to illustrate the calculation of
InDif. The 2-way marginal in Figure 1c is directly calculated
by the 1-way marginal of gender and age, without analyzing
the dataset; and Figure 1d gives the actual 2-way marginal.
In this example, InDif = 0.08· n, where n is the number of
records. The advantage of using InDif is that it is easy to com-
pute, and it has low sensitivity in terms of its range, [0,2n]:
Lemma 4. The sensitivity of InDif metric is 4: ∆InDif = 4.
The proof is deferred to Appendix A. Given d attributes,
we use the Gaussian mechanism to privately obtain all InDif
scores. To evaluate the impact of noise, one should consider
use ρ(cid:48) < ρ for publishing all the InDif scores for all m =(cid:0)d
both sensitivity and range of the metrics. We theoretically and
empirically analyze the noise-range ratio of entropy-based
metrics and InDif in Appendix B, and show that InDif has
smaller noise-range ratio than entropy-based metrics. More
speciﬁcally, given the overall privacy parameters (ε,δ), we
ﬁrst compute the parameter ρ using Theorem 2. We then
2
pairs of attributes. In particular, with the composition theory
of zCDP, we can show that publishing all InDif scores with
Gaussian noise N (0,8m/ρ(cid:48)I) satisﬁes ρ(cid:48)-zCDP (its proof is
also deferred to Appendix A).
Theorem 5. Given d attributes, publishing all m = d(d −
1)/2 InDif scores with Gaussian noise N (0,8m/ρ(cid:48)I) satisﬁes
ρ(cid:48)-zCDP.
(cid:1)
4.2 Marginal Selection
Given the dependency scores InDif, the next step is to choose
the pairs with high correlation, and use the Gaussian mecha-
nism to publish marginals on those pairs. In this process, there
are two error sources. One is the noise error introduced by the
Gaussian noise; the other is the dependency error when some
of the marginals are not selected. If we choose to publish all
2-way marginals, the noise error will be high and there is no
dependency error; when we skip some marginals, the error for
those marginals will be dominated by the dependency error.
Problem Formulation. Given m pairs of attributes, each pair
i is associated with an indicator variable xi that equals 1 if
pair i is selected, and 0 otherwise. Deﬁne ψi as the noise error
introduced by the Gaussian noise and φi as its dependency
error. The marginal selection problem is formulated as the
following optimization problem:
m
∑
[ψixi + φi(1− xi)]
minimize
subject to xi ∈ {0,1}
i=1
Notice that the dependency error φi has positive correlation
with InDifi, i.e., larger InDifi incurs larger φi. Thus, we ap-
proximate φi as InDifi + N (0,m2ρ(cid:48)2I), and it is ﬁxed in the
optimization problem.
The noise error ψi is dependent on the privacy budget ρi
allocated to the pair i. In particular, we ﬁrst show that given
the true marginal Mi, we add Gaussian noise with scale 1/ρi
to achieve ρi-zCDP.
Theorem 6. (1) The marginal M has sensitivity ∆M = 1; (2)
Publishing marginal M with noise N (0,1/2ρI) satisﬁes ρ-
zCDP.
The proof of Theorem 6 is deferred to Appendix A. To
make ψi and φi comparable, we use the expected (cid:96)1 error of
the Gaussian noise on marginal i. That is, if the marginal size
is ci, after adding Gaussian noise with scale σi, we expect
USENIX Association
30th USENIX Security Symposium    933
(cid:113) 2
(cid:34)
(cid:115)
(cid:113) 1
to see the (cid:96)1 error of ci
ψi = ci
πρi
πσi. Thus, with privacy budget ρi,
. The optimization problem is transformed to:
(cid:35)
xi + φi(1− xi)
m
ci
i=1
∑
1
minimize
πρi
subject to xi ∈ {0,1}
∑xiρi = ρ
Optimal Privacy Budget Allocation. We ﬁrst assume the
pairs are selected (i.e., variables of xi are determined), and we
want to allocate different privacy budget to different marginals
to minimize the overall noise error. In this case, the optimiza-
tion problem can be rewritten as:
minimize ∑
i:xi=1
subject to ∑
i:xi=1
1
ρi
ρi = ρ
(cid:115)
For this problem, we can construct the Lagrangian function
+ µ· (∑i ρi − ρ). By taking partial derivative of
L = ∑i
L for each of ρi, we have ρi =
. The value of µ
(cid:19)−3/2
2 ·
can be solved by equation ∑i ρi = ρ. As a result, µ = 1
(cid:17)−2/3
(cid:16) 2µ
(cid:18)
ci√