### Six Features and Model Significance
Among the six features, only AutoHosts exhibited low significance. This feature is highly correlated with NoHosts, leading to its exclusion from the model. The most relevant features identified by the model are DomAge and RareUA. DomAge is negatively correlated with reported domains (as they tend to be more recently registered compared to legitimate ones), while all other features show a positive correlation.

### Threshold Selection for Domain Scores
Based on the trained model, we set a threshold for domain scores above which a domain is considered a potential command-and-control (C&C) server. Figure 4 illustrates the difference in scores between automated domains reported by VirusTotal and legitimate ones in the training set. For example, setting a threshold of 0.4 for labeling an automated domain as suspicious results in correctly predicting 57.18% of reported domains in the training set, with a false positive rate of 10% among legitimate domains. Similar results are observed in the testing set. Our primary goal is not to identify all automated domains reported by VirusTotal but to pinpoint the most suspicious ones to bootstrap the belief propagation (BP) algorithm.

### Implementation of Detect C&C Function
We implement the `Detect C&C` function from Algorithm 1 to return 1 if the domain score exceeds the selected threshold during training and 0 otherwise. It is important to note that the selection of feature weights and the domain score threshold is customized for each enterprise during the training stage.

### Domain Similarity
To capture infection patterns, we consider several features when computing the similarity of a domain \( D \) with a set of domains \( S \) labeled as malicious in previous iterations of BP.

- **Domain Connectivity**: We use the domain connectivity as defined earlier.
- **Timing Correlations**: We consider the time when domain \( D \) was visited by internal hosts. During the initial infection stage, we suspect that a host visits multiple domains under the attacker's control within a short period. Thus, we evaluate the minimum timing difference between a host visit to domain \( D \) and other malicious domains in set \( S \). A shorter interval indicates higher suspicion.
- **IP Space Proximity**: We also consider the proximity in IP space between \( D \) and domains in set \( S \). Proximity in the /24 and /16 subnets is denoted by IP24 and IP16, respectively. The intuition is that attackers often host many malicious domains under a small number of IP subnets.

We provide measurements of timing and IP proximity features on the LANL dataset in Section V-B. The domain similarity score is tailored to the specific enterprise during the training stage.

### Training and Testing Data
To obtain a list of non-automated rare domains and their features, we start with a set of compromised hosts (contacting C&C domains confirmed by VirusTotal). We include each rare domain contacted by at least one host in this set, extract its features, query VirusTotal for its status, and divide the data into training and testing sets, covering the first and last two weeks of February, respectively.

We apply linear regression on the training set to determine feature weights and significance. Among the eight features, IP16 has low significance due to its high correlation with IP24. The most relevant features identified are RareUA, DomInterval, IP24, and DomAge. The threshold for domain similarity score is selected based on the balance between true positives and false positives.

### Putting It All Together
Our system for detecting early-stage enterprise infections consists of two main phases: training (during a one-month bootstrapping period) and operation (daily after the training period). An overview diagram is presented in Figure 5.

#### Training Phase
1. **Data Normalization and Reduction**: Process raw log data (HTTP or DNS logs) and apply normalization and reduction techniques.
2. **Profiling**: Build histories of external destinations visited by internal hosts and user-agent (UA) strings used in HTTP requests.
3. **Customizing the C&C Detector**: Customize the C&C communication detector for the specific enterprise.
4. **Customizing the Domain Similarity Score**: Customize the domain similarity score for the enterprise.

#### Operation Phase
1. **Data Normalization and Reduction**: Normalize and reduce new log data.
2. **Profile Comparison and Update**: Compare new data with historical profiles, identify rare destinations and UAs, and update histories.
3. **C&C Detector**: Run the C&C detector daily and label automated domains with scores above the threshold as potential C&C domains.
4. **Belief Propagation**: Run the BP algorithm to generate an ordered list of suspicious domains for further investigation by the Security Operations Center (SOC).

### Evaluation on the LANL Dataset
We describe the LANL challenge and how our techniques were adapted to the anonymized LANL dataset. Despite using fewer features, our BP framework achieves excellent results on the LANL challenge.

#### The LANL Challenge Problem
The LANL dataset includes attack traces from 20 independent infection campaigns simulated by LANL domain experts. Each simulation represents the initial first-day infection stage of an independent campaign. The challenge involves four cases of increasing difficulty, as described in Table I. Cases 1-3 provide hints about compromised hosts, while Case 4 does not. Answers (i.e., the malicious domains) are provided for validation.

#### Parameter Selection
We separate the 20 simulated attacks into two equal-size sets for training and testing, ensuring that each case is represented in both sets, except for Case 4, which is included in the testing set. Parameters are chosen based on the training set and validated on the testing set.

#### Results
Our results on the four cases of the LANL challenge are summarized in Table II. We achieve a precision of 98.33% (97.06% on the testing set), with a false positive rate (FPR) of 3.72·10−5% over all 2.7M domains (5.76·10−5% over 1.7M domains in the testing set) and a false negative rate (FNR) of 6.35% (2.94% on the testing set).

Interestingly, the BP algorithm trained on Case 3 delivered excellent results on Case 4, where no training was possible. All five domains identified by BP were confirmed malicious, with no false positives.