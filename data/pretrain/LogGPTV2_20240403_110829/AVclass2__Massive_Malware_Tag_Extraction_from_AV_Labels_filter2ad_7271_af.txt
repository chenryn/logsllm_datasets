### Introduction and Performance Metrics

- **F1 Score**: 100%
- **Precision/Recall**:
  - 49% / 59%
  - 70% / 73%
  - 54% / 67%
  - 93% / 91%
  - 85% / 89%
- **Other Metrics**:
  - 74%
  - 76%
  - 86%
  - 90%
  - 94%

### Controlled Vocabularies and Tool Updates
Controlled vocabularies are often incomplete and require frequent updates. For advanced searches or additional intelligence, analysts should use AVclass2. However, for those who only need family labels, the faster AVclass is still suitable. Our vision is to have both AVclass2 and AVclass accept the same data files in the future, eliminating duplicate work to keep both tools up-to-date. To achieve this, we plan to integrate our AVclass* modifications into AVclass, allowing both tools to use the same input files. This will enable the community to collaboratively update the malware knowledge in the taxonomy, tagging rules, and expansion rules, avoiding redundant efforts while maintaining backward compatibility.

### Related Work

#### AV Labels
AV labels have been extensively studied for over a decade. Early research highlighted the problem of different AV engines disagreeing on labels for the same sample [2, 5]. Despite this, AV labels have been widely used to build training datasets and evaluate malware detection and clustering approaches [1â€“3, 7, 12, 14, 19, 25, 26, 30, 31, 33, 34]. Li et al. [21] used AV labels as a reference to evaluate clustering results, showing that including only a subset of samples with high agreement among AV engines can bias the evaluation towards easier-to-label samples. Other studies have focused on the quality of labels from different AV engines. Mohaisen and Alrawi [28] proposed metrics for evaluating AV labels and identified clusters of AV engines that copy their labels. Kantchelian et al. [17] discussed the varying label quality of AV engines and suggested weighting them differently. Hurier et al. [13] proposed metrics to evaluate ground truth datasets built using AV labels.

#### Dynamics of AV Labels
The dynamics of AV labels have also been analyzed. Some studies have shown how AV engines change their labels over time due to refined signatures and analysis [10, 17]. It has been recommended that detection systems be trained with the labels available at training time, not testing time [26]. Zhu et al. [40] analyzed daily snapshots of 14K samples over a year, confirming that certain sets of AV engines produce strongly correlated labels, as observed in AVMeter [28] and implemented in AVClass [35]. They also showed that hand-picking a few trusted engines does not always perform well and measured that detection thresholds between two and 14 exhibit little difference in precision and recall.

### Malware Labeling
One approach to address disagreements on malware names is to use naming conventions such as the 1991 CARO Virus Naming Convention [6]. Another attempt was the Common Malware Enumeration (CME) Initiative [4], which provided unique identifiers for malware. Unfortunately, these conventions have not achieved wide adoption, possibly due to their use of predefined tags.

An alternative approach is to automatically extract accurate family names from AV labels. VAMO [32] introduced an automated approach for evaluating clustering results by building a graph of normalized labels from four AV engines. It introduced the concept of label co-occurrence, which measures the fraction of samples where labels from different engines appear together.

Tools like AVclass [35] and Euphony [9] have demonstrated that it is possible to extract accurate family tags from AV labels. Both tools take AV labels for a large number of samples and output a family name for each sample, using co-occurrence to identify family aliases. However, there are key differences:
- **AVclass**: Proposes that aliases and generic tokens learned from one dataset can be reused on other datasets. It avoids predicting which AV engines are better at labeling samples and makes tokenization as AV engine-independent as possible.
- **Euphony**: Uses a graph-based approach to identify aliases within the dataset but does not produce reusable relations. It tries to learn the structure of the labels from a selected subset of AV engines.

These differences result in improved accuracy and scalability for AVclass, which is why AVclass2 is built on top of AVclass.

### Malware Tagging
Malware tagging is central to information sharing standards like MAEC [24] and MISP [27]. Tags are already used by some malware repositories to enable efficient search. AVclass2, an automatic tool, extracts tags from the wealth of information in AV labels, generating tags that can be incorporated into repositories for richer searches. Compared to previous work, AVclass2:
- Does not pre-define tags but builds an open taxonomy (currently with 150 non-family tags).
- Handles tag aliasing (e.g., "downloader" and "dropper" are aliases in the taxonomy).
- Provides support for updating input rules and taxonomy.
- Does not limit the supported AV engines.
- Is open source.

### Conclusion
Automatically extracting tags from AV labels is an efficient way to categorize and index massive amounts of malware samples. However, it is challenging due to the different vocabularies used by AV engines. AVclass2, an open-source automatic malware tagging tool, addresses this by extracting clean tags that categorize samples according to their class, family, file properties, and behaviors. The extracted tags can be used by malware repositories and analysis services to enhance searches and provide ground truth for machine learning approaches. AVclass2 uses and helps build an open taxonomy, and provides an update module that uses tag co-occurrence to automatically identify taxonomy updates. We have evaluated AVclass2 on 42M samples.

### Acknowledgments
This research was supported by the Regional Government of Madrid through grants BLOQUES-CM P2018/TCS-4339 and PEJD-2018-PRE/TIC-9571, and by the Spanish Government through the SCUM grant RTI2018-102043-B-I00 and fellowship FPU18/06416. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors or originators and do not necessarily reflect the views of the sponsors.

### References
[1] Daniel Arp, Michael Spreitzenbarth, Malte Huebner, Hugo Gascon, and Konrad Rieck. 2014. Drebin: Efficient and Explainable Detection of Android Malware in Your Pocket. In Network and Distributed System Security.
...
[40] Shuofei Zhu, Jianjun Shi, Limin Yang, Boqin Qin, Ziyi Zhang, Linhai Song, and Gang Wang. 2020. Measuring and Modeling the Label Dynamics of Online Anti-Malware Engines. (2020).