0.10%
0.05%
0.00%
L
J
o
e
b
u
r
n
al
iki
Y
N
-
B
L
P
o
a
d
G
L
C
A
Extra Replicas
G
W
L
J
o
e
b
W
S
D
R
iki
Y
N
-
B
L
P
o
a
d
G
L
C
A
u
r
n
al
Fig. 3: (a) The percent of vertices without replicas, including
normal and selﬁsh vertex. (b) The percent of extra replicas
for fault tolerance.
migrate the workload on crashed node to a single sur-
viving node, it may result in signiﬁcant load imbalance
and performance degradation of normal execution after
recovery.
3 Replication-based Fault Tolerance
This section ﬁrst identiﬁes challenges and opportunities
in providing efﬁcient fault tolerance, and then describes
the design of Imitator.
3.1 Challenges and Opportunities
Low Overhead in Normal Execution: Compared
to data-parallel computing models, the dependencies
between vertices in graph-parallel models demand a
ﬁne-grained fault
tolerance mechanism. Low over-
head re-execution [22] and coarse-grained transforma-
tion [23] can hardly satisfy such requirement. In con-
trast, checkpoint-based fault tolerance in existing graph-
parallel systems sacriﬁces the performance of normal
execution for ﬁne-grained logging.
Fortunately, existing replicas for vertex computation
in a distributed graph-parallel system open an opportu-
nity for efﬁcient ﬁne-grained fault tolerance. Inspired
from fault
tolerance in distributed ﬁle system (e.g.,
GFS [24]), the replicas originally used for local access
in vertex computation can be reused to backup data of
vertices and edges, while the synchronization messages
between a master vertex and its replicas can be reused
to maintain the freshness of replicas.
To leverage vertex replicas for fault tolerance, it is
necessary that each vertex has at
least one replica;
otherwise extra replicas for these vertices have to be
565
created, which incurs additional overhead for commu-
nication during normal execution. Fig. 3(a) shows the
percentage of vertices without replicas on a 50-node
cluster for a set of graphs [21] using the default hash-
based (random) partitioning. Only GWeb and LJournal
contain more than 10% of such vertices, while others
only contain less than 1% vertices. The primary source
of vertices without replicas is selﬁsh vertices, which
have no outgoing edges (e.g., vertex 7 in Fig. 1). For
most graph algorithms, the value of a selﬁsh vertex has
no consumer and only depends on ingoing neighbors.
Consequently, there is no need to create extra replica
for selﬁsh vertices. In addition, the performance cost
in communication depends on the number of replicas,
which is several times the number of vertices. Fig. 3(b)
illustrates the percentage of extra replicas required for
fault tolerance regardless of selﬁsh vertices, which is
less than 0.15% for all dataset.
Fast Recovery from Failure: For checkpoint-based
fault tolerance, recovery from a snapshot on the persis-
tent storage cannot harness all resources in the cluster.
The I/O performance of a single node becomes the
bottleneck of recovery, which does not scale with the
increase of nodes. Further, a checkpoint-based fault
tolerance mechanism also depends on standby nodes to
take over the workload on crashed nodes.
Fortunately, the replicas of a vertex scattered across
the entire cluster provide a new opportunity to recover
a node failure in parallel, which is inspired from the
fast recovery in DRAM-based storage system (e.g.,
RAMCloud [15]). Actually, the time for recovery may
be less with more nodes if the replicas selected to
recovery can be evenly assigned to all nodes.
In addition, an even distribution of replicas for ver-
tices on crashed node further provides the possibility to
support migrating the workload on crashed nodes to all
surviving nodes without using additional standby nodes
for recovery. This may also reserve the load balance of
execution after recovery.
3.2 Overall Design of Imitator
Based on the above observation, we propose Imitator,
a replication-based fault tolerance scheme for graph-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:33:01 UTC from IEEE Xplore.  Restrictions apply. 
Algorithm 1: Imitator Execution Model
Input: Date Graph G = (V, E, D)
Input: Initial active vertex set V
VDPSOH
JUDSK
PDVWHU
&203
UHSOLFD
PLUURU
)7UHSOLFD
1 if is newbie() then
// newbie node
newbie enter leave barrier()
iter = newbie rebirth recovery()
4 while iter < max iter do
5
compute()
send msgs()
state = enter barrier()
if state.is f ail() then
QRGH
V\QF
QRGH(
QRGH+
// node failure
2
3
6
7
8
9
10
11
12
13
14
15
16
17
18
19
rollback()
if is rebirth() then rebirth recovery(state)
else migration recovery(state)
continue
else
commit state()
iter ++
// normal execution
state = leave barrier()
if state.is f ail() then
if is rebirth() then rebirth recovery(state)
else migration recovery(state)
// node failure
parallel systems. Unlike prior systems, Imitator employs
replicas of a vertex to provide fault tolerance rather than
periodically checkpointing graph states. The replicas of
a vertex inherently provide a remote consistent backup,
which are synchronized during each global barrier.
When a node crashes, its workload (vertices and edges)
will be reconstructed on a standby node or evenly
migrated to all surviving nodes.
Note that Imitator assumes a fail-stop model where a
machine crash won’t cause wild or malicious changes
to other machines. How to extend Imitator to support
more complicated faults like byzantine faults [25], [26]
will be our future work.
Execution Flow:
Imitator extends existing syn-
chronous computation with detection of potential node
failures and seamless recovery. As shown in Algo-
rithm 1, each iteration consists of three steps. First, all
vertices are updated using the messages from neighbor-
ing vertices in the computation phase (line 5). Secondly,
an update of vertex states is synchronized from a
master vertex to its replicas in the communication phase
through message passing (line 6). Note that all messages
have been received before entering the global barrier.
Finally, all new vertex states are consistently committed
within a global barrier (line 14 and 15).
Imitator employs a highly available and persistent
distributed coordination service (e.g., Zookeeper [27])
to provide barrier-based synchronization and distributed
shared states among workers. Node failures will be
566
Fig. 4: A sample of replicas in Imitator.
detected before (line 7) and after (line 16) the global
barrier. Before recovery, each worker must enforce the
consistency of its local graph states. If a failure occurs
before the global barrier, each surviving node should
roll back its states (line 9) and execution ﬂows (line
12) to the beginning of the current
iteration, since
messages from crashed nodes may be lost. Imitator
provides two alternative recovery mechanisms: Rebirth
and Migration. For Rebirth, a standby node will join
the global synchronization (line 2), and reconstruct the
graph states of the crashed nodes from all surviving
nodes (line 3, 10 and 18). For Migration mode, the
vertices on crashed nodes will be reconstructed from
all surviving nodes (line 11 and 19).
4 Managing Replicas
Many graph-parallel systems construct a local graph
on each node by replicating vertices to avoid remote
accesses during vertex computation. As shown in the
middle of Fig. 4, vertices are evenly assigned to three
machines with their ingoing edges, and replicas are cre-
ated to provide local vertex accesses. These replicas will
be synchronized with their master vertices to maintain
consistency. Imitator reuses these replicas as consistent
backups of vertices for recovery from failures. However,
replication-based fault tolerance requires that every ver-
tex has at least a replica with exactly the same states
with the master vertex, while currently replicas are only
with partial states. Further, not all vertices have replicas.
This section describes extensions for fault-tolerance
oriented replication, creating full state replicas and an
optimization for selﬁsh vertices. Here, we only focus
on creating at least one replica to tolerate one machine
failure; creating more replicas can be done similarly.
4.1 Fault Tolerant (FT) Replica
Original replication for local accesses may cause some
vertices to have no replicas. For example, the internal
vertex (e.g., vertex 6 in Fig. 4) has no replica as all its
edges are stored at the same node. A failure of node 3
may cause an irrecoverable state for vertex 6.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:33:01 UTC from IEEE Xplore.  Restrictions apply. 
For such an internal vertex, Imitator creates an ad-
ditional fault tolerant (FT) replica on another machine
when loading the graph. There is no constraint on the
location of these replicas, which provides an opportunity
to balance the workload among nodes to hide the
performance overhead. Before assignment, the number
of replicas and internal vertices are exchanged among
nodes. Each node proportionally assigns FT replicas to
the rest nodes. For example, vertex 6 has no computa-
tion replicas and its additional FT replica is assigned to
node 1, which has fewer replicas, as shown in Fig. 4.
4.2 Full-state Replica (Mirror)
The replica to provide local access does not have full
states to recover the master vertex, such as the location
of replicas. Some algorithms (e.g. SSSP) also need to
associate data to edges, such as the weight of edge.
However, it is not efﬁcient to upgrade all replicas to be
homogeneous with their masters, which will cause ex-
cessive memory and communication overhead. Imitator
selects one replica to be the homogeneous replica with
the master vertex, namely mirror. All data of ingoing
edges will be synchronized from master to mirror.
Most additional states in mirrors are static, such as
the location of replicas and the weight of ingoing edges,
which are replicated during graph loading. The rest
states are dynamic, such as whether a vertex is active
or inactive in next iteration, and should be transferred
with synchronization message from master to mirror in
each iteration.
As mirrors are responsible to recover their masters
on a crashed node,
the distribution of mirrors may
affect the scalability of recovery. Since the locations of
mirrors are restricted by the locations of all candidate
replicas, each machine adopts a greedy algorithm to
evenly assign mirrors on other machines independently:
each machine maintains a counter of existing mirrors,
and the master always assigns mirrors to replicas whose
hosting machine has least mirrors so far.
Note that the FT replica is always the mirror of
vertex. As shown in the bottom part of Fig. 4, the
mirrors of vertex 1 and 4 on node 1 are assigned to
node 2 and 3 accordingly, and the mirror of vertex 7
has to be assigned to node 2.
4.3 Optimizing Selﬁsh Vertices
The main overhead of Imitator during normal execution
is from the synchronization of additional FT replicas.
According to our analysis, many vertices requiring FT
replicas have no neighboring vertices consuming their
vertex data (selﬁsh vertices). For example, vertex 7
has no outgoing edges in Fig. 4. Further, for some
algorithms (e.g., PageRank),
the new vertex data is
computed only according to its neighboring vertices.
PDVWHU
PLUURU
UHSOLFD
QRGH
QRGH(
QRGH+
VXUYLYLQJQRGHV
FUDVKHGQRGH
Fig. 5: A sample of Rebirth recovery approach in Imitator.
For such vertices, namely selﬁsh vertices, Imitator
only creates an FT replica for recovery, and never
synchronizes them with masters in normal execution.
During recovery, the static states of selﬁsh vertices can
be obtained from its FT replicas, and dynamic states
can be re-computed using neighboring vertices.
5 Recovery
The main issue of recovery is knowing which vertices,
either master vertices or other replicas, are assigned
to the crashed node. A simple approach is adding a
layer to store the location of each vertex. This, however,
may become a new bottleneck during the recovery.
Fortunately, when a master vertex creates its replicas,
it knows its replicas’ locations. Thus, by storing its
replicas’ locations, a master vertex knows if its replicas
are assigned to the crashed node. As its mirrors are
responsible for recovery when a master vertex crashes,
a master needs to replicate this information to its mirrors
as well.
During recovery, each surviving node will check
in parallel whether master vertices or replica vertices
related to the failed nodes have been lost and reconstruct
the states accordingly. As each remaining node has
the complete information of its related graph states,
such checking and reconstruction can be done in a
decentralized way and in parallel.
Imitator provides two strategies for recovery: Rebirth
based recovery, which recovers graph states in crashed
nodes to standby ones; Migration based recovery, which
scatters vertices on the crashed nodes to surviving ones.
5.1 Rebirth Based Recovery
During recovery, the location information of vertices
will be used by master vertices or mirrors to check
whether there are some vertices to recover. Rebirth
based recovery comprises three steps: Reloading, where
the surviving nodes send the recovery messages to the
standby nodes to help it recover states; Reconstruc-
tion, which reconstructs the states (mainly the graph
topology) necessary for computation; and Replay, which
redoes some operations to get the latest states of some
vertices.
5.1.1 Reloading
Through checking the location of its replicas, a master
vertex will know whether there are any of its replicas
located in the crashed nodes. If so, the master vertex
567
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:33:01 UTC from IEEE Xplore.  Restrictions apply. 
will generate messages to recover such replicas. If a
master vertex is on the crashed nodes, its mirror will be
responsible to recover this crashed master. Based on this
rule, each surviving node can just use the information
from its local vertices to decide whether it needs to
participate in the recovery process.
For the sample graph in Fig. 5, node 3 crashed during
computation. After a new standby node (i.e., machine)
awakes node 1 and node 2 from the barrier operation,
these two nodes will check whether they have some
vertices to recover. Node 1 will check its master vertices
(master vertex 1, 4, and 7), and ﬁnd that there are some
replicas (replica 1 and mirror 4) on the crashed node.
Hence, it needs to generate two recovery messages to
recover replica 1 and mirror 4 on the new node. Node
1 will also check its mirrors to ﬁnd whether there is
any mirror whose master was lost. It then ﬁnds that
the master of vertex 6 was lost, and thus generates a
message to recover master vertex 6. Node 2 will act the
same as node 1.
The surviving nodes also need to send some global
states to the newbie, such as the number of iterations
so far. All the recovery messages are sent in a batched
way when its number has exceeded a threshold.
5.1.2 Reconstruction
For the new machine, there are three types of states
to reconstruct: the graph topology, runtime states of
vertices, and global states (e.g., number of iterations
so far). The last two types of states can be retrieved
directly from recovery messages. The graph topology is
a complex data structure, which is non-trivial to recover.
A simple way to recover the graph topology is using
the raw edge information (the “point to” relationship
between vertices) and redoing operations of building
topology in the graph loading phase. In this way, after
receiving all the recovery messages, the new machine
will create vertices based on the messages (which
contain the vertex types, the edges and the detailed
states of a vertex). After creating all vertices, the new
machines will use the raw edge information on each
vertex to build the graph topology. One issue with this
approach is that building graph topology can only start
after creating all vertices. Further, due to the complex
“point to” relationship between vertices, it is not easy
to parallelize the topology building process.
To expose more parallelism, Imitator instead uses en-
hanced edge information for recovery. Since all vertices
are stored in an array in each machine, the topology of
a graph is represented by the array index. This means
that if there is an edge from vertex A to vertex B,
vertex B will have a ﬁeld to store the index of vertex
A in the array. Hence, if Imitator can ensure a vertex
is placed at the same position of the vertex array in the
new machine, reconstruction of graph topology can be
done in parallel on all the surviving nodes.
To ensure this, Imitator also replicates the master’s
position to its mirror with other states in the graph
loading phase. When a mirror recovers its master, it will
create the master vertex and its edges, and then encode
the vertex and the master position into the recovery
message. On receiving the message, the new machine
just needs to retrieve the vertex from the message and
put it at the given position. Recovering replicas can be
done in the same way.
Since every crashed vertex only needs one vertex to
do the recovery, there is only one recovery message for
one position. Thus, there is no contention on the array
(which is thus lock-free) and can be done immediately
when receiving the message. Hence, it is completely
decentralized and can be done in parallel. Note that
there is no explicit reconstruction phase for this ap-
proach because the reconstruction can be done during
the reloading phase when receiving recovery messages.
5.1.3 Replay
Imitator can recover most states of a vertex directly
from the recovery message, except the activation state,
which cannot be timely synchronized between masters
and mirrors. The reason is that a master vertex may
be activated by some neighboring vertices that are not
located on the same node. When a master replicates
its states to its mirrors, the master may still not be
activated by its remote neighbors. Hence, the activation
state can only be recovered by replaying the activation
operations. However, the neighboring vertex of a master
vertex might also locate at the crashed node. As a result,
a master vertex needs to replicate its activation infor-
mation (which vertices it should activate) to its mirrors.
A vertex (either master or mirror) doing recovery will
attach the corresponding activation information to the
recovery message. The new node will re-execute the
activation operations according to these messages on
all the vertices.
5.2 Migration Approach
When there are no standby nodes for recovery, Mi-
gration based recovery will scatter graph computation
from the crashed nodes to surviving ones. Fortunately,
the mirrors, which are isomorphic with their masters,
provide a convenient way to migrate a master vertex
from one node to another. Other data to be used by the
new master in future computation can be retrieved from
its neighboring vertices.
The Migration approach also consists of three steps:
Reloading, Reconstruction, and Replay, of which the
processes are only slightly different from the Rebirth
approach. In the followings, we will use the example in
568
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:33:01 UTC from IEEE Xplore.  Restrictions apply. 
QRGH
QRGH(
QRGH+
5.2.2 Reconstruction
PDVWHU
PLUURU
UHSOLFD
VXUYLYLQJQRGHV
FUDVKHGQRGH
During reconstruction, all surviving nodes will assemble
new graph states from the recovery messages sent in
the reloading phase. After the reconstruction phase, the
topology of the graph and most of the states of the
vertices (both masters and replicas), are migrated to the
surviving nodes.
5.2.3 Replay
The Migration approach also needs to ﬁx the activation
states for new masters. However, the Rebirth approach
needs to ﬁx such states for all recovered masters, while
the Migration approach only needs to ﬁx the states
of newly promoted masters, which are only a small
portion of all master vertices on one machine. Hence,
we choose to treat these new masters specially instead
of redoing the activation operation on all the vertices.
Imitator checks whether a new master is activated by
some of its neighbors or not. If so, Imitator will correct
the activation states of the new master. When ﬁnishing
the Replay phase, the surviving nodes can now resume