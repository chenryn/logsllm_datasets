title:Scouting the Path to a Million-Client Server
author:Yimeng Zhao and
Ahmed Saeed and
Mostafa H. Ammar and
Ellen W. Zegura
Scouting the Path to a Million-Client
Server
Yimeng Zhao1(B), Ahmed Saeed2, Mostafa Ammar1, and Ellen Zegura1
1 Georgia Institute of Technology, Atlanta, USA
2 Massachusetts Institute of Technology, Cambridge, USA
Abstract. To keep up with demand, servers will scale up to handle
hundreds of thousands of clients simultaneously. Much of the focus of
the community has been on scaling servers in terms of aggregate traﬃc
intensity (packets transmitted per second). However, bottlenecks caused
by the increasing number of concurrent clients, resulting in a large num-
ber of concurrent ﬂows, have received little attention. In this work, we
focus on identifying such bottlenecks. In particular, we deﬁne two broad
categories of problems; namely, admitting more packets into the network
stack than can be handled eﬃciently, and increasing per-packet over-
head within the stack. We show that these problems contribute to high
CPU usage and network performance degradation in terms of aggregate
throughput and RTT. Our measurement and analysis are performed in
the context of the Linux networking stack, the most widely used pub-
licly available networking stack. Further, we discuss the relevance of our
ﬁndings to other network stacks. The goal of our work is to highlight con-
siderations required in the design of future networking stacks to enable
eﬃcient handling of large numbers of clients and ﬂows.
1 Introduction
Modern servers at large scale operators handle tens of thousands of clients simul-
taneously [33,38,45]. This scale will only grow as NIC speeds increase [1,3,5] and
servers get more CPU cores [4,23]. For example, a server with a 400 Gbps NIC
[3] can serve around 80k HD video clients and 133k SD video clients.1 This scale
is critical not only for video on demand but also for teleconferencing and AR/VR
applications. The focus of the community has been on scaling servers in terms of
packets transmitted per second [13,25,27,28,34,36], with little attention paid to
developing complete stacks that can handle large numbers of ﬂows well [26,29].
We envisage servers delivering large volumes of data to millions of clients
simultaneously. Our goal is to identify bottlenecks that arise when servers reach
that scale. In particular, we take a close look at network stack components that
become the bottleneck as the number of ﬂows increases. We ﬁnd that competition
between ﬂows can lead to overall performance degradation, requiring ﬁne-grain
scheduling. Further, the increase in ﬂow numbers leads to higher overhead of
1 HD and SD videos consume up to 5 Mbps and 3 Mbps, respectively [9].
c(cid:2) Springer Nature Switzerland AG 2021
O. Hohlfeld et al. (Eds.): PAM 2021, LNCS 12671, pp. 337–354, 2021.
https://doi.org/10.1007/978-3-030-72582-2_20
338
Y. Zhao et al.
Table 1. Summary of ﬁndings with results reported at 100k ﬂows compared to more
eﬃcient baselines for admission control or performance with lower number of ﬂows for
per-packet overhead.
Category Identiﬁed Issue
Impact
Existing systems
mitigating it
n
o
i
s
s
i
m
d
A
l
o
r
t
n
o
C
t
e
k
c
a
p
-
r
e
P
d
a
e
h
r
e
v
O
Overpacing
Ineﬃcient
backpressure
Oblivious
hardware oﬄoads
Data structure
ineﬃciency
Lock contention
Cache pressure
5% increase in CPU utilization –
Throughput unfairness and
hundreds of milliseconds in
latency
2× increase in interrupts
Per-ﬂow scheduling [31, 46]
–
2× increase in CPU utilization
and 2× increase in latency
2× increase in latency
1.8× increase in latency
Low-overhead data
structures [38, 39]
Distributed scheduling
[24, 38, 42]
–
per-ﬂow bookkeeping and ﬂow coordination. Thus, we categorize problems that
arise due to an increase in the number of concurrent ﬂows into two categories:
1) Admission Control to the Stack: The admission policy determines the
frequency at which a ﬂow can access the stack and how many packets it can
send per access. The frequency of a ﬂow accessing network resources and the
duration of each access determine the throughput it can achieve. As the number
of ﬂows increases, admission control becomes critical for the eﬃciency of the
stack. For example, admitting and alternating between ﬂows at a high frequency
can reduce Head-of-Line (HoL) blocking and improve fairness but at the expense
of CPU overhead, which can become a bottleneck, leading to throughput loss.
We consider backpressure mechanism as a critical part of the admission control
as it determines how a ﬂow is paused (e.g., denied admission) and resumed (i.e.,
granted admission).
2) Per-packet Overhead within the Stack: The overhead of most per-packet
operations is almost constant or a function of packet size (e.g., checksum, rout-
ing, and copying). However, the overhead of some operations depends entirely
on the number of ﬂows serviced by the system. For example, the overhead of
matching an incoming packet to its ﬂow (i.e., demultiplexing), and the overhead
of scheduling, for some scheduling policies (e.g., fair queueing), are tied to the
number of ﬂows in the system.
We focus our attention on Linux servers. Despite its well documented ineﬃ-
ciencies (e.g., the overhead of system calls, interrupts, and per-packet memory
allocation [15,26]), the Linux networking stack remains the most widely used
publicly available networking stack. Further, even when new userspace stacks
are deployed, they still rely, at least partially, on the Linux stack to make use
of its comprehensive Linux functionality and wide use [31]. Hence, our focus on
Linux is critical for two reasons: 1) our results are immediately useful to a wide
range of server operators, and 2) we are able to identify all possible bottlenecks
that might not appear in other stacks because they lack the functionality.
Scouting the Path to a Million-Client Server
339
We focus on the overhead of long-lived ﬂows. Long-lived ﬂows help expose
problems related to scaling a stack in terms of the number of ﬂows. Scheduling
long-lived ﬂows requires the scheduler to keep track of all active ﬂows, exposing
ineﬃcient data structures whose overhead increases with the number of tracked
ﬂows and highlighting issues that arise because of the interactions between the
transport layer and the scheduler. It also exposes cache ineﬃciencies as infor-
mation about a ﬂow has to be retained and edited over a long period of time.
Applications with long-lived ﬂows include video on demand and remote stor-
age. The ineﬃciency of short-lived ﬂows is rooted in creation and destruction of
states, and has been studied in earlier work [33]
The contribution of this work is in evaluating the scalability of the network
stack as a whole, at hundreds of thousands of clients, leading to the deﬁnition of
broader categories of scalability concerns. Table 1 summarizes our ﬁndings and
existing systems that mitigating the problems. It should be noted that ineﬃcient
backpressure and data structure problems are only partially addressed by the
existing solutions and we’ll discuss the remaining challenges in Sect. 4 and 5.
In earlier work there have been several proposals to improve the scalability
of diﬀerent components of the network stack (e.g., transport layer [26,29,33]
and scheduling [18,38,39]). These proposals consider speciﬁc issues with little
attempt to generalize or categorize such scalability concerns. Further, the notion
of scalability considered in earlier work is still limited to tens of thousands of
ﬂows, with a general focus on short ﬂows.
2 Measurement Setup
Testbed: We conduct experiments on two dual-socket servers. Each server is
equipped with two Intel E5-2680 v4 @ 2.40 GHz processors. Each server has an Intel
XL710 Dual Port 40G NIC Card with multi-queue enabled. The machines belong
to the same rack. Both machines use Ubuntu Server 18.04 with Linux kernel 5.3.0.
Testbed Tuning: The aﬃnity of the interrupts and application to CPU cores
signiﬁcantly aﬀects the network performance on a multi-core and multi-socket
machine. To reduce cache synchronization between diﬀerent cores and improve
interrupt aﬃnity, we pin each transmit/receive queue pair to the same core. We
enable Receiver Packet Steering (RPS), which sends the packet to a CPU core
based on the hash of source and destination IPs and ports. We limit all network
processing to exclusively use the local socket because we observe that the inter-
connection between diﬀerent sockets leads to performance degradation at 200k
or more ﬂows. We enabled diﬀerent hardware oﬄoad functions including GSO,
GRO, and LRO to lower CPU utilization. We also enabled interrupt moderation
to generate interrupts per batch, rather than per packet. We use TCP CUBIC as
the default transport protocol, providing it with maximum buﬀer size, to avoid
memory bottlenecks. The entire set of parameters is shown in Appendix B.
Traﬃc Generation: We generate up to 300k concurrent ﬂows with neper [8].
We bind multiple IP addresses to each server so the number of ﬂows that can be
generated is not limited by the number of ports available for a single IP address.
340
Y. Zhao et al.
Fig. 1. Schematic of the packet transmission path with identiﬁed pain points marked
in red.
With 40 Gbps aggregate throughput, the per-ﬂow rate can range from 133 Kbps,
which is a typical ﬂow rate for web service [17], to 400 Mbps, which might be large
data transfer [19]. We ran experiments with diﬀerent numbers of threads ranging
from 200 to 2000. In particular, we spawn N threads, create M ﬂows that last for
100 s, and multiplex the M ﬂows evenly over the N threads. We observed that
using more threads causes higher overhead in book-keeping and context switch,
leading to degraded throughput when the server needs to support hundreds of
thousands of ﬂows. The results shown in this paper are with 200 threads if not
speciﬁed otherwise. We use long-lived ﬂows for experiments because our focus is
on the scaling problem in terms of the number of concurrent ﬂows. The scaling
problem of short-lived ﬂows is more related to the number of connecting requests
per second rather than the number of concurrent ﬂows. With ﬁxed number of
ﬂows, the short-lived ﬂows should not have higher overhead than long-lived ﬂows.
For the rest of the paper, we use ﬂows and clients interchangeably.
Figure 1 visualizes our assumed stack architecture. Our focus is on the over-
head of the transport and scheduling components of the stack. We experiment
with diﬀerent scheduling algorithms by installing diﬀerent Queuing Disciplines
(qdiscs). We use multiqueue qdisc (mq) to avoid having a single lock for all hard-
ware queues. All scheduling algorithms are implemented by per-queue within mq.
By default, mq handles packets FIFO in its queues. However, we use Fair Queue
(fq) [21] as the default qdisc combined with mq. Compared to pfifo fast, fq
achieves better performance in terms of latency and CPU usage when handling
a large number of ﬂows [46]. In some experiments, we limit the total ﬂow rate to
90% of the link speed to avoid queueing in Qdiscs and show that the performance
Scouting the Path to a Million-Client Server
341
degradation cannot be avoided by simply lowering the total rate. We also use
fq codel [7] to reduce latency within the qdisc in some cases.
Measurement Collection: In all experiments, machines are running only the
applications mentioned here making any CPU performance measurements corre-
spond with packet processing. We track overall CPU utilization using dstat [6]
and track average ﬂow RTT using ss [12]. We track the TCP statistics using
netstat [10]. Performance statistics of speciﬁc functions in the kernel is obtained
using perf [11].
3 Overall Stack Performance
We start by measuring the
overall performance of
the
stack with the objective of
observing how bottlenecks
arise as we increase the num-
ber of ﬂows. In particular,
we look at aggregate through-
put, CPU utilization, aver-
age RTT, and retransmis-
sions. Figure 2 shows a sum-
mary of our results. Our setup
can maintain line rate up to
around 200k ﬂows (Fig. 2a).
Thus, we limit our reporting
to 300k ﬂows.
(a) Aggregate Throughput
(b) CPU Usage
(c) RTT
(d) Retransmission
Fig. 2. Overall performance of the network stack as
a function of the number of ﬂows
As the number of ﬂows
increases, the CPU utiliza-
tion steadily increases until it
becomes the bottleneck. Recall that we are only using a single socket, which
means that 50% utilization means full utilization in our case (Fig. 2b). The aggre-
gate throughput shows that the number of bytes per second remains constant.
Thus, the increase in CPU utilization is primarily due to the increase in the
number of ﬂows handled by the systems.
The most surprising observation is that the average delay introduced by the
stack can reach one second when the stack handles 300k ﬂows, a ﬁve orders of
magnitude increase from the minimum RTT. There are several problems that
can lead to such large delays. The Linux stack is notorious for its ineﬃciencies
due to relying on interrupts, especially on the ingress path [2,15,26,32]. Further,
head-of-line blocking in hardware can add signiﬁcant delays [42]. Our focus in
this paper is to identify problems that are caused by ineﬃciencies that arise due
to the growth in the number of ﬂows. Such problems are likely to occur in the
transport and scheduling layers, the layers aware of the number of ﬂows in the
system. Our ﬁrst step is to try to understand which part of the stack is causing
these delays, to better understand the impact of the number of ﬂows on the