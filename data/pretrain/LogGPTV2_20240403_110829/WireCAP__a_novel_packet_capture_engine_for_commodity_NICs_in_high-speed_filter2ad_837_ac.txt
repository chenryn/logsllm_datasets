0% 
56.8% 
33.4% 
0% 
9.3% 
0% 
0.8% 
0% 
this  approach  cannot  preserve 
2.3 How to avoid packet drops that are caused 
by load imbalance? 
  The  experiments  reveal  that  existing  packet-capture  engines 
suffer  significant  packet  drops  when  they  experience  load 
imbalance in a multicore system. To avoid packet drops, we must 
handle load imbalance. There are several possible approaches. 
 A  first  approach  is  to  apply  a  round-robin  traffic  steering 
mechanism at the NIC level to distribute the traffic evenly across 
the  queues.  However, 
the 
application logic because packets belonging to the same flow can 
be delivered to different applications when multiple applications 
are running in the system.  
  A  second  approach  is  to  use  existing  packet  capture  engines 
(e.g.,  DNA)  and  to  address  load  imbalance  in  the  application 
layer. For example, an application thread can read packets from a 
queue, and store them into its own set of buffers, and then process 
a  fixed  number  of  packets  before  reading  again.  When  load 
imbalance  occurs,  the  application  thread  can  move  packets  in 
some flows over to other threads if its own buffers were filling up. 
However, this approach has several limitations:  
•  An  application  in  user  space  has  little  knowledge  of  low-
level  layer  conditions.  If  the  application  thread  cannot  read 
data from a queue in time, bursts of network packets would 
overrun low-level packet buffers and cause packet drops.  
•  Because  existing  packet  capture  engines  have  a  limited 
buffering  capability,  this  approach  must  involve  copying 
captured  packets  into  user  space.  At  high  packet  rates, 
excessive  data  copying  leads  to  poor  performance  [15,  17, 
18, 19].  
This  approach  would  make  the  application  complex  and 
difficult to design. 
• 
Our  approach  is  to  design  a  new  packet-capture  engine  that 
addresses load balance in the packet-capture level. We believe a 
packet  capture  engine  is  in  a  better  position  to  address  load 
399
imbalance  because  it  has  full  knowledge  of  low-level  layer 
conditions. WireCAP is our solution. 
3 The WireCAP Design & Implementation 
3.1 Design goals 
  WireCAP  is  designed  to  support  the  packet  capturing  and 
processing  paradigm  as  shown  in  Figure  1.  We  have  several 
design goals:  
• 
Providing  lossless  packet-capture  services  in  high-speed 
networks  is  our  primary  goal.  WireCAP  aims  to  provide 
lossless  packet  capture  services  in  high-speed  networks  by 
exploiting  multicore  architecture  and  multi-queue  NICs. 
Therefore,  WireCAP  must  handle  load  imbalance  that 
frequently occurs in multicore systems. 
Providing efficient packet delivery.  
• 
•  WireCAP  must  be  efficient.  In  a  multicore  system,  core 
affinity  on  network  processing  can  significantly  improve 
performance.  Therefore,  when  short-term  load  imbalance 
occurs, WireCAP must not only avoid packet drops but also 
ensure core affinity on network processing. However, when 
long-term  load  imbalance  occurs,  WireCAP  has  to  perform 
traffic offloading to avoid packet drops. We believe this is an 
appropriate tradeoff.  
•  WireCAP  must  facilitate  the  design  and  operation  of  a 
packet-processing application in user space.  
•  WireCAP  must  have  wide  applicability  and  can  be  easily 
adopted.  
•  WireCAP should implement a transmit function that allows 
captured  packets  to  be  forwarded.  Such  a  function  would 
allow WireCAP to support middlebox-type applications. 
3.2 WireCAP design 
  We begin by describing our ring-buffer-pool and buddy-group-
based  offloading  mechanisms.  Then,  we  discuss  the  WireCAP 
architecture.  
3.2.1 The mechanisms 
  Ring-buffer-pool.  Figure  4  illustrates  the  ring-buffer-pool 
concept. Assume each receive queue has a ring of N descriptors. 
Under  WireCAP,  each  receive  ring  is  divided  into  descriptor 
segments.  A  descriptor  segment  consists  of  M  receive  packet 
descriptors  (e.g.,  1024),  where  M  is  a  divisor  of  N.  In  kernel 
space, each receive ring is allocated with R packet buffer chunks, 
termed  the  ring  buffer  pool.  In  this  case,  R  is  greater  than  N/M, 
which is meant to provide a large ring buffer pool. A packet buffer 
chunk consists of M fixed-size cells, with each cell corresponding 
to  a  ring  buffer.  Typically,  the  M  ring  buffers  within  a  packet 
buffer chunk occupy physically contiguous memory. Both M and 
R  are  configurable.  Within  a  pool,  a  packet  buffer  chunk  is 
identified by a unique chunk_id. Globally, a packet buffer chunk 
is  uniquely  identified  by  a  {nic_id,  ring_id,  chunk_id}  tuple. 
Here, nic_id and ring_id refer to the NIC and to the receive ring 
that the packet buffer chunk belongs to. 
When an application opens a receive queue to capture packets, 
the ring buffer pool for the receive queue will be mapped into the 
application’s process space. Therefore, a packet buffer chunk has 
three 
and 
process_address, which are used by the NIC, the kernel, and the 
application,  respectively.  These  addresses  are  maintained  and 
translated by the kernel. A cell within a chunk is accessed by its 
relative address within the chunk.  
kernel_address, 
addresses, 
DMA_address, 
Processing data
Capture
Recycle
Free Chunks
…
Open
Close
Packet Buffer Chunk
Cell
Cell
Cell
Packet Buffer Chunk
Desc.
Descriptor segment
…
Desc.
Descriptor segment
Recv ring
A
t
t
a
c
h
RX Queue 
Incoming pkts 
Figure 4. The ring-buffer-pool concept 
ring_id, 
{{nic_id, 
A packet buffer chunk can exist in one of three states: “free”, 
“attached”,  and  “captured”.  A  “free”  chunk  is  maintained  in  the 
kernel, available for (re)use. In the “attached” state, the chunk is 
attached  to  a  descriptor  segment  in  its  receive  ring  to  receive 
packets.  Each  cell  in  the  chunk  is  sequentially  tied  to  the 
corresponding  packet  descriptor  in  the  descriptor  segment.  A 
“captured” chunk is one that has been filled with received packets 
and captured into the user space.  
A  ring-buffer-pool  provides  operations  to  allow  a  user-space 
application to capture packets. These operations can be accessed 
through the ioctl interface: 
  Open.  Opens  a  specific  receive  queue  for  packet  capture.  It 
maps its ring buffer pool into the application’s process space and 
attaches each descriptor segment in the receive ring with a “free” 
packet buffer chunk. 
  Capture.  Captures  packets  in  a  specific  receive  queue.  The 
capture  operation  is  performed  in  the  units  of  the  packet  buffer 
chunk;  a  single  operation  can  move  multiple  chunks  to  the  user 
space.  To  capture  a  packet  buffer  chunk  to  user  space,  only  its 
chunk_id},  process_address, 
metadata 
pkt_count}  is  passed.  The  chunk  itself  is  not  copied.  Here, 
pkt_count  counts  the  number  of  packets  in  the  chunks.  When  a 
packet buffer chunk attached to the receive ring is captured to the 
user space, the corresponding descriptor segment must be attached 
with a new “free” chunk to receive subsequent packets. Because 
the  NIC  moves  incoming  packets  to  the  empty  ring  buffers 
without CPU intervention, a packet buffer chunk cannot be safely 
moved  unless  it  is  full.  Otherwise,  packet  drops  might  occur. 
Thus, our capture operation works as follows. (1) If no packet is 
available,  the  capture  operation  will  be  blocked  until  incoming 
packets  wake  it  up.  (2)  Else  if  full  packet  buffer  chunks  are 
available, the capture operation will return immediately, with one 
or  multiple  full  chunks  moved 
the  user  space.  The 
corresponding  descriptor  segment  will  be  attached  with  a  new 
“free” chunk. (3) Else, the capture operation will be blocked with 
a  timeout.  The  process  will  continue  as  stated  in  (2)  if  new  full 
packet buffer chunks become available before the timeout expires. 
If the timeout expires and the incoming packets only partially fill 
an attached packet buffer chunk, we copy them to a “free” packet 
buffer  chunk,  which  is  moved  to  the  user  space  instead.  This 
mechanism  avoids  holding  packets  in  the  receive  ring  for  too 
long. 
  Recycle. In the user space, once the data in a “captured” packet 
buffer chunk are finally processed, the chunk will be recycled for 
future  use.  To  recycle  a  chunk,  its  metadata  are  passed  to  the 
kernel,  which  will  be  strictly  validated  and  verified;  the  kernel 
simply changes the chunk’s state to “free”. 
  Close.  Closes  a  specific  receive  queue  for  packet  capture  and 
performs the necessary cleaning tasks.  
to 
User space
OS Kernel
application 1
Thd1
Core1
Thd2
Core2
application 2
Thd1
Core3
Thd2
Core4
1
Q
R
2
Q
R
3
Q
R
4
Q
R
Buddy group 1
Buddy group 2
Multi-Core 
Host System
Multi-Queue 
NIC
Figure 5. The buddy-group concept 
Through the capture and recycle operations, each chunk of packet 
buffers  can  be  used  to  receive  packets  flowing  through  the 
network,  and  temporarily  store  received  packets.  A  ring  buffer 
pool’s  capacity  is  configurable.  When  a  large  pool  capacity  is 
configured, a ring buffer pool can provide sufficient buffering at 
the NIC’s receive ring level to accommodate short-term bursts of 
packets. Thus, it helps to avoid packet drops.  
Buddy-group-based  offloading.  The  basic  concept  is  simple:  a 
busy packet capture engine offloads some of its traffic to less busy 
or idle queues (cores) where it can be processed by other threads. 
However,  the  challenge  is  how  to  preserve  application  logic—
traffic belonging to the same flow must be delivered to the same 
application when multiple applications are running in the system. 
Therefore,  we  introduce  a  buddy  group  concept:  the  receive 
queues accessed by a single application can form a buddy group. 
Traffic offloading is only allowed within a buddy group.  
  We  illustrate  the  buddy  group  concept  in  Figure  5.    In  the 
system, each receive queue (RQ1—RQ4) is tied to a distinct core. 
Application  1’s  threads  are  running  at  core  1  and  2  while 
application  2’s  threads  are  running  at  core  3  and  4.  In  this 
scenario, RQ1 and RQ2 can form a buddy group to implement the 
offloading mechanism for application 1. Similarly, RQ3 and RQ4 
can form a buddy group to implement the offloading mechanism 
for application 2.  
3.2.2 The WireCAP architecture 
  Figure  6  shows  the  WireCAP  architecture.  It  consists  of  a 
kernel-mode driver and a user-mode library.  
• 
The  kernel-mode  driver  manages  NICs  and  provides  low-
level packet capture and transmit services. It applies the ring-
buffer-pool mechanism to handle short-term load imbalance.  
App Thread
Libpcap-compatible Interface
Buddy list
C
a
p
t
u
r
e
R
e
c
y
c
l
e
Capture thread
Work 
Queue
Forwarding 
captured pkts
U
s
e
r
-
M
o
d
e
L
b
r
a
r
y
i
Packet Buffer Chunks
Free Chunks
K
e
r
n
e
...
Recv ring
A
t
t
a
c
h
...
ring-buffer-pool
Transmit ring
RX Queue 
Tx Queue 
Figure 6. The WireCAP architecture 
400
M
o
d
e
D
r
i
l
-
v
e
r
• 