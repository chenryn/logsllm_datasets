Equation 3 (as explained in Case I).
0.25 (cid:1)
P VAC1.1 = (cid:0) CSP1 CSP2 CSP3 CSC
0.25
0.25
0.25
Similarly, all the lowest level SLOs (least dependent SLOs)
are calculated. The P VAC3.1, P VAC3.2, P VAC3.3, P VBC2.2,
P VBC11.1, P VBC11.2, P VIS1.1 and P VIS1.2 are calculated in
a similar way. Then based on the DSM order AC1.2 is cal-
culated. AC1.2 is depending on IS1.2 (Dep1), thus the
P VAC1.2 is equal to P VIS1.2.
0.3 (cid:1)
P VAC1.2 = (cid:0) CSP1 CSP2 CSP3 CSC
0.2
0.3
0.2
In the same way, P VAC2.3, P VAC2.4 and P VBC2.1 are cal-
culated (they are equal to P VAC3.2, P VBC2.2 and P VBC11.1
respectively). Furthermore, AC2.1 is calculated. AC2.1 de-
pends on AC1.1 and AC3.1 (Dep1 and Dep3) with diﬀerent
levels of dependencies. Thus, using Equation 4:
P VAC1.1 P VAC3.1
0.25
0.25
0.25
0.25
0.25
0.25
0.25
0.25
 (cid:18) wAC2.1
0.4
0.6
(cid:19)
P VAC2.1 =
CSP1
CSP2
CSP3
CSC
After all the SLO priority vectors are determined, the prior-
ity vectors are aggregated with the dependency importance
level to get the overall rank of CSPs according to the cus-
tomer requirements as speciﬁed in Case I. As a result the
root priority vector is equal to:
P VRoot = (cid:0) CSP1 CSP2 CSP3
0.2018 0.2241 0.2870 0.2870(cid:1)
CSC
4.2 The CSP Perspective: Maximising Offered
Security Levels
The second validation scenario presented in this paper ap-
plies the secSLA evaluation techniques to solve problems
faced by CSPs i.e., which speciﬁc security SLO from the
oﬀered secSLA should be improved in order to maximise
the overall security level according to the customer require-
ments? This might be the case of a well-established CSP
deciding where to invest in order to achieve the highest pos-
sible security level, or a new CSP designing the secSLA. To
answer this question, we could perform a sensitivity anal-
ysis to ascertain the security beneﬁts of improving one or
more SLOs. However, this analysis becomes impractical as
the number of SLOs and the dependencies between them
increase. Thus the sensitivity analysis is performed on the
least dependent SLOs identiﬁed by the DSM.
We used the CSP1 dataset described at Table 2, and applied
the Case II requirements to setup the customer’s baseline
for the security evaluation. From the existing 9 least depen-
dent SLOs (Case II column in Table 2) the CSP1 is under-
provisioning 4 of them (AC3.2, AC3.3, IS1.1 and IS1.2).
Figure 9 shows how the proposed framework can be used to
analyse an existing secSLA, and extract the individual SLOs
that, if enhanced, would result on diﬀerent improvements as-
sociated to the overall security level. In this case, the X-axis
represents the improvement associated to the overall secu-
rity level after enhancing any of the SLOs. It is shown as
a percentage where 0% corresponds to the original secSLA
and 100% is the most eﬀective SLO. For example, providing
tenants with the security policies applicable to virtualised re-
sources (AC3.2 in Figure 9), quantitatively increases CSP1
security level better than improving the thresholds commit-
ted for any of the other SLOs.
IS1.2
AC3.3
IS1.1
AC3.2
70
80
90
100
Security level improvement (%)
Figure 9: Sensitivity analysis: CSP1 SLOs that max-
imise the overall security level.
5. RELATED WORK
With the rapid growth of the Cloud services, multiple ap-
proaches are emerging to assess the functionality and secu-
rity of CSPs. In [16], the authors proposed a framework to
compare diﬀerent Cloud providers across performance indi-
cators. In [12], an AHP-based ranking technique that uti-
338lizes performance data to measure various QoS attributes
and evaluates the relative ranking of CSPs was proposed.
In [25], a framework of critical characteristics and measures
that enable a comparison of Cloud services is presented.
However, these studies focused on assessing performance of
Cloud services but not their security properties.
Security requirements for non-Cloud scenarios have been ad-
dressed by Casola et al.
[3], who proposed a methodology
to evaluate security SLAs for web services. Chaves et al. [5]
explored security in SLAs by proposing a monitoring and
controlling architecture for web services.
In [11] and [15],
the authors propose a technique to aggregate security met-
rics from web services. Their approach focused on the pro-
cess of selecting the optimal service composition based on
a set of predeﬁned requirements. However, the authors did
not propose any techniques to assess Cloud secSLAs or em-
pirically validate the proposed metrics.
In [1], the authors propose the notion of evaluating Cloud
secSLAs by introducing a metric to benchmark the security
of a CSP based on categories. However, the resulting secu-
rity categorization is purely qualitative and lacks the sup-
port of dependencies. Luna et al. [18] presented a method-
ology to quantitatively benchmark Cloud security with re-
spect to customer deﬁned requirements (based on control
frameworks). In [27], the authors presented a framework to
compare, benchmark and rank the security level provided
by two or more CSPs. However in both of them, the depen-
dencies and conﬂict detection are not covered.
There has been considerable eﬀort on the conﬂict analy-
sis of network system management policies. Charalambides
et al.
[4] expressed QoS policies using Event Calculus for
managing DiﬀServ networks, and their conﬂict analysis is
conducted in a pairwise comparison fashion. Dunlop et al.
[9] proposed a model to specify policies of permission, pro-
hibition and obligation in a temporal logic language that
can reason about the sequences of events.
In [6], the au-
thors presented a framework for automatic detection of con-
ﬂicts covering violation of enterprise policies and inconsis-
tency of customer requirements. Ensel and Keller [10] intro-
duced an approach to handle dependencies between man-
aged resources (e.g., web application server, database) in a
distributed system. However, the support for secSLA man-
agement is not provided. The COSMA approach [17] sup-
ports the providers of composite services to manage their
SLAs. However, COSMA does not support the determina-
tion of the eﬀect of SLO violations on other services based
on dependency information.
6. CONCLUSIONS
Choosing a Cloud provider that satisﬁes the security re-
quirements of the customer has become challenging. Quan-
tiﬁcation and evaluation oﬀer powerful tools for choosing
between diﬀerent CSPs. While the initial results of such
techniques are promising, they still lack tackling the depen-
dency relations that span across customer requirements.
Most of these methodologies do not account for information
about dependencies between services. It is important to pro-
vide customers with comprehensive support which enables
automatic conﬂict detection and explanation dedicated to
the dependent relations. Our framework automatically de-
tects any conﬂicts caused by inconsistent customer require-
ments. Additionally, our framework ranks CSPs and selects
the CSP that best satisﬁes the customer requirements.
Furthermore, explanations of the detected conﬂicts are gen-
erated to identify problematic customer requirements. Using
our framework, we evaluated diﬀerent CSPs based on varied
security speciﬁcations with respect to the customer security
requirements. Additionally, we also addressed diﬀerent as-
signments of security levels and weights enabling customers
to compare the security levels oﬀered by diﬀerent CSPs.
Our case study based evaluation showed that our framework
eﬀectively validated complicated requirements from diﬀerent
customers and selected the best matching CSP from the set
of all CSPs. Currently, we are enhancing our input model
of Cloud services by encoding more services from the STAR
repository [8].
7. ACKNOWLEDGMENTS
Research supported, in part, by H2020- 644579 (ESCUDO-
CLOUD), FP7-ICT-2013-11610795 (SPECS) and DFG SFB
CROSSING.
8. REFERENCES
[1] M. Almorsy, J. Grundy, and A. Ibrahim.
Collaboration-based cloud computing security
management framework. Proc. of Cloud Computing,
pages 364–371, 2011.
[2] T. Browning. Applying the design structure matrix to
system decomposition and integration problems: a
review and new directions. In Trans. on Engg.
Management, 48(3):292–306, 2001.
[3] V. Casola, A. Mazzeo, N. Mazzocca, and M. Rak. A
sla evaluation methodology in service oriented
architectures. In Quality of Protection, pages 119–130,
2006.
[4] M. Charalambides, P. Flegkas, G. Pavlou,
J. Rubio-Loyola, A. Bandara, E. Lupu, A. Russo,
N. Dulay, and M. Sloman. Policy conﬂict analysis for
diﬀserv quality of service management. In Network
and Service Management, 6(1):15–30, 2009.
[5] S. Chaves, C. Westphall, and F. Lamin. SLA
perspective in security management for cloud
computing. Proc. of Networking and Services, pages
212–217, 2010.
[6] C. Chen, S. Yan, G. Zhao, B. Lee, and S. Singhal. A
systematic framework enabling automatic conﬂict
detection and explanation in cloud service selection for
enterprises. Proc. of Cloud Computing, pages 883–890,
2012.
[7] Cloud Security Alliance. The Open Certiﬁcation
Framework.
https://cloudsecurityalliance.org/research/ocf/.
[8] Cloud Security Alliance. The Security, Trust &
Assurance Registry (STAR).
https://cloudsecurityalliance.org/star/.
[9] N. Dunlop, J. Indulska, and K. Raymond. Dynamic
conﬂict detection in policy-based management
systems. Proc. of the Enterprise Distributed Object
Computing Conference, pages 15–26, 2002.
[10] C. Ensel and A. Keller. Managing application service
dependencies with xml and the resource description
framework. Proc. of the Integrated Network
Management Proceedings, pages 661–674, 2001.
[11] G. Frankova and A. Yautsiukhin. Service and
protection level agreements for business processes.
339[30] M. Winkler, T. Springer, and A. Schill. Automating
composite sla management tasks by exploiting service
dependency information. Proc. of Web Services, pages
59–66, 2010.
[31] M. Zeleny. Multiple Criteria Decision Making.
McGraw Hill, 1982.
APPENDIX
A. EXCERPT OF A SECSLA DEPENDENCY
MODEL
The extract of a CSP secSLA dependency model with two
SLOs (named “User authentication and identity assurance
level” (kUsauth ) and “CSP-Authentication” (kCSauth )) and the
dependency relation between them is shown in Listing 1. In
the Listing, kUsauth −→K kCSauth and the two SLOs secu-
rity levels are modeled as v(kUsauth ) and v(kCSauth ), respec-
tively. The requirement is that the security level of kCSauth
is higher than or equal the security level of kUsauth , i.e.,
v(kCSauth ) ≥ v(kUsauth ). This requirement is modeled as
(kUsauth , kCSauth ,≤) ∈ C−→K
.
v
Note that all service levels (e.g., level2, level3, monthly,
. . .) are modeled as numerical values. These numerical val-
ues are the security SLOs values in the XML schema shown
in Listing 1.
Listing 1: Excerpt of dependency model of a secSLA
 leq 
Proc. of European Young Researchers Workshop on
Service Oriented Computing, pages 38–43, 2007.
[12] K. Garg, S. Versteeg, and R. Buyya. A framework for
ranking of cloud computing services. In Future
Generation Computer Systems, 29(4):1012–1023, 2013.
[13] D. Gebala and S. Eppinger. Methods for analyzing
design procedures. Proc. of Design Theory and
Methodology, pages 227–233, 1991.
[14] J. Luna, A. Taha, R. Trapero, and N. Suri.
Quantitative reasoning about cloud security using
service level agreements. In Trans. on Cloud
Computing, (99), 2015.
[15] L. Krautsevich, F. Martinelli, and A. Yautsiukhin. A
general method for assessment of security in complex
services. Proc. of Towards a Service-Based Internet,
pages 153–164, 2011.
[16] A. Li, X. Yang, S. Kandula, and M. Zhang. Cloudcmp:
comparing public cloud providers. Proc. of Internet
Measurement, pages 1–14, 2010.
[17] A. Ludwig and B. Franczyk. Cosma–an approach for
managing slas in composite services. Proc. of
Service-Oriented Computing, pages 626–632, 2008.
[18] J. Luna, R. Langenberg, and N. Suri. Benchmarking
Cloud Security Level Agreements Using Quantitative
Policy Trees. Proc. of Cloud Computing Security
Workshop, pages 103–112, 2012.
[19] D. Marca and C. McGowan. Sadt: structured analysis
and design technique. McGraw-Hill, 1987.
[20] R. Ramanathan. A note on the use of the analytic
hierarchy process for environmental impact
assessment. In Journal of Environmental
Management, 63(1):27–35, 2001.
[21] Z. Rehman, F. Hussain, and O. Hussain. Towards
multi-criteria cloud service selection. Proc. of
Innovative Mobile and Internet Services in Ubiquitous
Computing, pages 44–48, 2011.
[22] D. Ross. Structured analysis (SA): A language for
communicating ideas. In Software Engineering,
(1):16–34, 1977.
[23] T. Saaty. How to make a decision: the analytic
hierarchy process. In European journal of operational
research, 48(1):9–26, 1990.
[24] N. Sangal, E. Jordan, V. Sinha, and D. Jackson. Using
dependency models to manage complex software
architecture. In Sigplan Notices, 40(10):167–176, 2005.
[25] J. Siegel and J. Perdue. Cloud services measures for
global use: the service measurement index (smi). Proc.
of Global Conference, pages 411–415, 2012.
[26] D. Steward. The design structure system: a method
for managing the design of complex systems. In Trans.
on Engg. Management, (3):71–74, 1981.
[27] A. Taha, R. Trapero, J. Luna, and N. Suri.
AHP-Based Quantitative Approach for Assessing and
Comparing Cloud Security. Proc. of Trust, Security
and Privacy in Computing and Communications,
pages 284–291, 2014.
[28] J. Wiest and F. Levy. A management guide to
PERT/CPM. Prentice-Hall, 1977.
[29] M. Winkler and A. Schill. Towards dependency
management in service compositions. Proc. of
e-Business, pages 79–84, 2009.
340