    ``` screen
    # yum update -y
    ```
4.  安装 `pcs pacemaker`{.literal} 隔离代理和资源代理。
    ``` screen
    # yum install -y pcs pacemaker fence-agents-gce resource-agents-gcp
    ```
5.  如果内核已更新，重启机器。
    ``` screen
    # reboot
    ```
:::
:::
::: section
::: titlepage
# []{#configuring-rhel-ha-on-gcp_cloud-content.html#gcp-ha-configuring-rhel-ha-services_configuring-rhel-ha-on-gcp}配置 HA 服务 {.title}
:::
在所有节点上完成以下步骤以配置高可用性服务。
::: orderedlist
**流程**
1.  用户 `hacluster`{.literal} 在上一步中的 `pcs`{.literal} 和
    `pacemaker`{.literal} 安装中创建。在所有集群节点上为用户
    `hacluster`{.literal} 创建密码。所有节点都使用相同的密码。
    ``` screen
    # passwd hacluster
    ```
2.  如果启用了 `firewalld`{.literal} 服务，在 RHEL 中添加高可用性服务。
    ``` screen
    # firewall-cmd --permanent --add-service=high-availability
    # firewall-cmd --reload
    ```
3.  启动 `pcs`{.literal} 服务并在引导时启用它。
    ``` screen
    # systemctl enable pcsd.service --now
    ```
:::
::: orderedlist
**验证步骤**
1.  确定 `pcs`{.literal} 服务正在运行。
    ``` screen
    # systemctl is-active pcsd.service
    ```
:::
:::
::: section
::: titlepage
# []{#configuring-rhel-ha-on-gcp_cloud-content.html#gcp-ha-creating-cluster-in-gcp_configuring-rhel-ha-on-gcp}创建集群 {.title}
:::
完成以下步骤以创建节点集群。
::: orderedlist
**流程**
1.  在其中一个节点上，输入以下命令验证 pcs 用户
    `ha cluster`{.literal}。指定集群中的每个节点的名称。
    ``` screen
    # pcs cluster auth  _hostname1_ _hostname2_ _hostname3_ -u hacluster
    ```
    例如：
    ``` screen
    [root@node01 ~]# pcs cluster auth node01 node02 node03 -u hacluster
    node01: Authorized
    node02: Authorized
    node03: Authorized
    ```
2.  创建集群。
    ``` screen
    # pcs cluster setup --name cluster-name _hostname1_ _hostname2_  _hostname3_
    ```
:::
::: orderedlist
**验证步骤**
1.  启用集群。
    ``` screen
    # pcs cluster enable --all
    ```
2.  启动集群。
    ``` screen
    # pcs cluster start --all
    ```
:::
:::
::: section
::: titlepage
# []{#configuring-rhel-ha-on-gcp_cloud-content.html#gcp-ha-creating-a-fencing-device_configuring-rhel-ha-on-gcp}创建隔离设备 {.title}
:::
对于大多数默认配置，GCP 实例名称和 RHEL 主机名是相同的。
完成以下步骤，从集群中的任何节点配置隔离。
::: orderedlist
**流程**
1.  从集群中的任何节点获取 GCP 实例名称。请注意，输出还显示实例的内部
    ID。
    ``` screen
    # fence_gce --zone _gcp_ _region_ --project= _gcp_ _project_ -o list
    ```
    例如：
    ``` screen
    [root@rhel71-node-01 ~]# fence_gce --zone us-west1-b --project=rhel-ha-testing-on-gcp -o list
    44358**********3181,InstanceName-3
    40819**********6811,InstanceName-1
    71736**********3341,InstanceName-2
    ```
2.  创建隔离设备。使用 `pcmk_host-name`{.literal} 命令将 RHEL
    主机名与实例 ID 映射。
    ``` screen
    # pcs stonith create _clusterfence_ fence_gce pcmk_host_map=_pcmk-hpst-map_ fence_gce zone=_gcp-zone_ project=_gcpproject_
    ```
    例如：
    ``` screen
    [root@node01 ~]# pcs stonith create fencegce fence_gce pcmk_host_map="node01:node01-vm;node02:node02-vm;node03:node03-vm" project=hacluster zone=us-east1-b
    ```
:::
::: orderedlist
**验证步骤**
1.  测试其他其中一个节点的隔离代理。
    ``` screen
    # pcs stonith fence gcp nodename
    ```
2.  检查状态以验证该节点是否已隔离。
    ``` screen
    # watch pcs status
    ```
:::
例如：
``` screen
[root@node01 ~]# watch pcs status
Cluster name: gcp-cluster
Stack: corosync
Current DC: rhel71-node-02 (version 1.1.18-11.el7_5.3-2b07d5c5a9) - partition with quorum
Last updated: Fri Jul 27 12:53:25 2018
Last change: Fri Jul 27 12:51:43 2018 by root via cibadmin on rhel71-node-01
3 nodes configured
3 resources configured
Online: [ rhel71-node-01 rhel71-node-02 rhel71-node-03 ]
Full list of resources:
us-east1-b-fence    (stonith:fence_gce):    Started rhel71-node-01
Daemon Status:
corosync: active/enabled
pacemaker: active/enabled
pcsd: active/enabled
```
:::
::: section
::: titlepage
# []{#configuring-rhel-ha-on-gcp_cloud-content.html#gcp-ha-configuring-gcp-node-authorization_configuring-rhel-ha-on-gcp}配置 GCP 节点授权 {.title}
:::
配置 cloud SDK 工具，使用您的帐户凭证访问 GCP。
::: title
**流程**
:::
在每个节点上输入以下命令，使用项目 ID 和帐户凭证初始化每个节点。
``` screen
# gcloud-ra init
```
:::
::: section
::: titlepage
# []{#configuring-rhel-ha-on-gcp_cloud-content.html#gcp-ha-configuring-the-gcp-vcp-move-vip-resource-agent_configuring-rhel-ha-on-gcp}配置 GCP 网络资源代理 {.title}
:::
集群使用附加到二级 IP 地址（从 IP）的 GCP
网络资源代理到正在运行的实例。这是一个浮动 IP
地址，可在集群中的不同节点间传递。
::: title
**流程**
:::
输入以下命令查看 GCP 虚拟 IP
地址资源代理(gcp-vpc-move-vip)描述。这显示了这个代理的选项和默认操作。
``` screen
# pcs resource describe gcp-vpc-move-vip
```
您可以将资源代理配置为使用主子网地址范围或二级子网地址范围。本节包含了这两者的步骤。
[**主子网地址范围**]{.strong}
::: title
**流程**
:::
完成以下步骤，为主 VPC 子网配置资源。
::: orderedlist
1.  创建 `aliasip`{.literal} 资源。包括一个未使用的内部 IP
    地址。在命令中包含 CIDR 块。
    ``` screen
    # pcs resource create aliasip gcp-vpc-move-vip  alias_ip=_UnusedIPaddress/CIDRblock_ --group _group-name_ --group _networking-group_
    ```
2.  创建用于管理节点上 IP 的 `IPaddr2`{.literal} 资源。
    ``` screen
    # pcs resource create vip IPaddr2 nic=_interface_ ip=_AliasIPaddress_ cidr_netmask=32 --group _group-name_ --group _networking-group_
    ```
3.  对 `vipgrp`{.literal} 下的网络资源进行分组。
    ``` screen
    # pcs resource group add vipgrp aliasip vip
    ```
:::
::: orderedlist
**验证步骤**
1.  验证资源是否已启动，并分组到 `vipgrp`{.literal} 下。
    ``` screen
    # pcs status
    ```
2.  验证资源是否可以移到另一个节点。
    ``` screen
    # pcs resource move vip _Node_
    ```
    例如：
    ``` screen
    # pcs resource move vip rhel71-node-03
    ```
3.  验证 `vip`{.literal} 是否在不同节点上成功启动。
    ``` screen
    # pcs status
    ```
:::
[**二级子网地址范围**]{.strong}
完成以下步骤，为二级子网地址范围配置资源。
::: title
**先决条件**
:::
[创建自定义网络和子网](https://access.redhat.com/articles/3479821#header41){.link}
::: orderedlist
**流程**
1.  创建二级子网地址范围。
    ``` screen
    # gcloud-ra compute networks subnets update _SubnetName_ --region _RegionName_ --add-secondary-ranges _SecondarySubnetName_=_SecondarySubnetRange_
    ```
    例如：
    ``` screen
    # gcloud-ra compute networks subnets update range0 --region us-west1 --add-secondary-ranges range1=10.10.20.0/24
    ```
2.  创建 `aliasip`{.literal}
    资源。在二级子网地址范围内创建一个未使用的内部 IP 地址。在命令中包含
    CIDR 块。
    ``` screen
    # pcs resource create aliasip gcp-vpc-move-vip alias_ip=_UnusedIPaddress/CIDRblock_ --group _group-name_ --group _networking-group_
    ```
3.  创建用于管理节点上 IP 的 `IPaddr2`{.literal} 资源。
    ``` screen
    # pcs resource create vip IPaddr2 nic=_interface_ ip=_AliasIPaddress_ cidr_netmask=32 --group _group-name_ --group _networking-group_
    ```
:::
::: orderedlist
**验证步骤**
1.  验证资源是否已启动，并分组到 `vipgrp`{.literal} 下。
    ``` screen
    # pcs status
    ```
2.  验证资源是否可以移到另一个节点。
    ``` screen
    # pcs resource move vip _Node_
    ```
    例如：
    ``` screen
    [root@rhel71-node-01 ~]# pcs resource move vip rhel71-node-03
    ```
3.  验证 `vip`{.literal} 是否在不同节点上成功启动。
    ``` screen
    # pcs status
    ```
:::
:::
:::