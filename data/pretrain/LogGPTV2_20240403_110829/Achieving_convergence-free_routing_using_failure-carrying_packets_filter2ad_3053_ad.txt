each node as opposed to computing backups to protect against fail-
ures of combinations of links.
5.3 Beneﬁts of FCP
5.3.1 Comparison with OSPF
In Figure 7(a), we vary the rate at which OSPFD sends HELLO
packets to neighbors, and measure the effect on control overhead
and the fraction of data packets delivered. We tuned FCP with a
fast probing rate (one probe every 50ms) since faster probing does
not incur a penalty in control overhead in terms of LSAs dissem-
inated because none of the link failures detected are announced.
7The expected number of failures encounted by a packet is proportional to
the diameter of the network.
 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 1 1.2 1.4 1.6 1.8 2 2.2 2.4Cumulative fractionStretch10 fails/sec1 fails/sec10 fails/sec-fp1 fails/sec-fp 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 0 1 2 3 4 5 6 7 8 9 10 11 12Cumulative fractionPacket header overhead [bytes]10 fails/sec-fp1 fails/sec-fp(a)
(b)
Figure 6: Recomputation costs of FCP: (a) Number of recomputations. (b) Recomputation times.
(a)
(b)
(c)
Figure 7: Comparison with OSPF: (a) Unlike FCP, OSPF cannot simultaneously provide low control overhead and high availability, (b) Reducing
FCP’s HELLO timer reduces stretch and loss without increasing control overhead, (c) OSPF’s map becomes inconsistent with the topology at low
probing rates, resulting in a stretch penalty.
As the HELLO probing rate is increased, the number of data pack-
ets lost decreases, since we react to failures more quickly. However,
probing at a faster rate also causes more short-term failures to be de-
tected and propagated, increasing control overhead. Note that when
measuring control overhead, we factor out HELLO messages for
both FCP and OSPF.
On the other hand, FCP undergoes only a very small (yet non-
zero) loss-rate of less than 0.1%; the loss of FCP in our experiments
is non-zero since link failure detection takes a ﬁnite time and dur-
ing this period, all packets trying to use that link will be dropped.
Although FCP exhibits similar behavior to OSPF in terms of stretch
and loss rate while varying the probing rate (Figure 7(b)), its control
overhead is not a function of link failure rate, and hence its probing
rate can be increased without inﬂating control overhead. We found
it was possible to tune OSPF to achieve this loss-rate, but only at
the expense of a increasing its control trafﬁc to above 300 messages
per second per link.
The average stretch shows a similar result; as the probing rate
decreases it takes longer to detect link repairs, and hence a larger
fraction of working paths are not discovered by a packet. This is
shown by the ﬁrst two lines in Figure 7(c). Since we can keep the
FCP probing rate high without compromising overhead, FCP has
a much lower stretch than OSPF. The low stretch is due to FCP’s
ability to discover efﬁcient secondary routes, rather than the fact
that most paths are not affected by failure. To illustrate this point,
we plot the results only over packets whose primary paths are failed
(see the bottom two lines in Figure 7(c) that are marked ‘-fp’).
5.3.2 Effect of varying parameters
In Figure 8(a), we vary the mean interarrival time for link fail-
ures, but ﬁx OSPF’s probing interval at 400ms, and plot the loss rate
and overhead. For a wide variety of failure rates, FCP outperforms
OSPF by an order of magnitude, while simultaneously maintaining
a lower control overhead. Note that OSPF’s overhead begins de-
creasing as the failure rate increases past the ability of the probing
protocol to keep up with link events.
In Figure 8(b), we vary the probing rate and plot the fraction
increase in loss rate of OSPF over the loss rate of FCP for vari-
ous topologies. Although the amount of improvement varies across
topologies, FCP provides more than one order of magnitude lower
loss rate than OSPF. As shown in Figure 8(c), FCP also reduces
control overhead. In general, we found that denser topologies (e.g.
AS 1221, with an average degree of 6.2) had less beneﬁt from FCP
than sparser topologies (e.g. AS 3257, with an average degree of
3.7). This happens because in denser topologies OSPF has a larger
number of paths to choose from, and is hence more likely to dis-
cover a working path.
5.3.3 Comparison with backup-path selection
Unlike OSPF, the backup-path strategy we used can attain very
fast failover times without a signiﬁcant increase in control over-
head. However, to minimize loss rates, the backup-path strategy
needs to account for every failure contingency, and hence requires a
substantial number of backup paths. Precomputing a large number
of backup paths to account for different combinations of multiple
link failures increases state per router. This tradeoff is shown in Fig-
ure 9(a). For example, with 8 backup paths per link, the backup path
strategy requires 4210 entries per router, and experiences a loss-rate
of 0.05%. However on the same workload, FCP requires only 255
entries yet attains a loss rate of less than 0.002%. Moreover, un-
like FCP, the distribution in state across routers is not uniform, and
hence the top 1% of routers require more than 20, 285 entries. How-
ever, for switching between maps, FCP must temporarily maintain
a second copy of its routing state. Although this state is only main-
 0.5 0.6 0.7 0.8 0.9 1 0 5 10 15 20Cumulative fractionRecomputations per packetno cachecachingprecomputationno cache-fpcaching-fpprecomputation-fp 0 0.2 0.4 0.6 0.8 1 10 100 1000Cumulative fractionTime [microseconds]AS1221AS1239AS1755AS3257AS3967AS6461 0 100 200 300 400 500 1 10 100 0 0.2 0.4 0.6 0.8 1 1.2 1.4Overhead [msgs/sec per link]Loss rateOSPF hello interarrival time [sec]OSPF-overheadOSPF-lossrateFCP-lossrate 0 0.1 0.2 0.3 0.4 0.5 0.1 1 10 100 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8Loss rateStretchFCP hello interarrival time [sec]FCP-lossrateFCP-stretch 1 1.5 2 2.5 3 1 10 100StretchOSPF hello interarrival time [sec]OSPF-stretchFCP-stretchOSPF-stretch-fpFCP-stretch-fp(a)
(b)
(c)
Figure 8: Effect of varying parameters: (a) failure rate on control overhead and data packet loss rate. (b) topology on loss rate. (c) topology on control
overhead.
(a)
(b)
(c)
Figure 9: Comparison with Backup-paths: (a) Unlike FCP, Backup-paths cannot simultaneously provide low state and low loss-rate. (b) FCP maintains
low loss for a variety of failure rates. (c) Backup-paths stretch penalty for varying numbers of backup paths.
tained for a short period, and can be stored as deltas (differences
from the current map), in the worst case this could double FCP’s
state requirements (to 510 in this example).
Figure 9(b) shows the performance in the presence of simulta-
neous failures on two representative topologies. We ﬁx the num-
ber of backup paths to two, and vary the number of randomly se-
lected links to simultaneously fail. On both topologies, the backup
strategy and FCP have roughly equal loss-rates during single fail-
ures. However, when more than one failure occurs, FCP has sig-
niﬁcantly lower losses. (FCP has non-zero loss since the failure
detection is not instantaneous.) This happens because as the fail-
ure rate increases, it becomes more likely the backup-path strategy
will encounter a set of failures not covered by the set of backup
paths. Finally, Figure 9(c) shows that the backup-paths strategy in-
curs a stretch penalty during link failures. This happens because
link-disjoint paths as used as backups, which tend to be longer than
the path FCP ﬁnds around the failure.
5.3.4 Effect of inconsistent maps
So far, we have assumed that all nodes have a consistent state
of the network map. Here, we investigate the overhead incurred
by SR-FCP—in terms of average routing stretch and per-packet
overhead—as a function of map inconsistency factor (see Fig-
ure 10). Speciﬁcally, for a chosen map inconsistency factor d, we
instantiate the network map Mn at each node n by picking links
randomly from the actual network map M, such that the intersec-
tion of maps at all nodes forms a spanning and connected subgraph
of M, which contains only a fraction (1 − d) of links in M (with
high probability). The x-axis is capped at 0.3 since that is the largest
fraction of inconsistency for which the intersection of maps at all
nodes forms a spanning subgraph.
The plot shows that the stretch and packet overhead are small
even when maps are highly inconsistent. Even when the inconsis-
Figure 10: Effect of inconsistency in network maps on the overhead
incurred by SR-FCP.
tency factor is 0.3, the average stretch is less than 1.03, and the
average size of a packet header is under 10 bytes (assuming 2 bytes
per node for source routes and failures). The reason SR-FCP per-
forms so well is because SR-FCP pays a penalty only if the source
node performing a route computation misses some links that could
have resulted in signiﬁcantly shorter paths; an intermediate node
just forwards a packet based on the packet’s source route irrespec-
tive of whether downstream links in that source route (not adjacent
to the intermediate node) are present in the node’s map or not.
6. DEPLOYMENT ISSUES
FCP represents a substantial departure from traditional routing
mechanisms, and hence unsurprisingly requires several modiﬁca-
tions to router design even for deployment at the intradomain level.
Although these changes are by no means trivial, in this section we
outline how they may be implemented as extensions to existing pro-
tocols and designs.
 0 0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.1 1 10 10 20 30 40 50 60Loss rateOverhead [msgs/sec per link]Avg. failures per secondFCP-lossrateOSPF-lossrateOSPF-overhead 1 10 100 1000 10000 1 10 100 1000 10000Ratio reduction in lossrateOSPF hello interarrival time [sec]AS1221AS1755AS3257AS3967 0 2 4 6 8 10 12 14 16 1 10 100 1000 10000Overhead [msgs/sec per link]OSPF hello interarrival time [sec]AS1755AS3257AS3967AS6461 0 0.005 0.01 0.015 0.02 0.025 0.03 1 10 0 10000 20000 30000 40000 50000Loss rateState [entries per router]Number of backup pathsFCP-lossrateBkup-lossrateFCP-stateBkup-state-avgBkup-state-99pc 0 0.001 0.002 0.003 0.004 0.005 0 0.5 1 1.5 2 2.5 3Loss rateNumber of simultaneous failuresFCP-AS1755Bkup-AS1755FCP-AS3275Bkup-AS3275 1 1.01 1.02 1.03 1.04 1.05 0 1 2 3 4 5StretchNumber of simultaneous failuresnum-bkups=0num-bkups=1num-bkups=2num-bkups=3num-bkups=4 1 1.005 1.01 1.015 1.02 1.025 1.03 0 0.05 0.1 0.15 0.2 0.25 0.3 0 1 2 3 4 5 6 7 8 9 10StretchAvg. overhead [bytes/packet]Map inconsistency factor (d)StretchOverheadMap dissemination: The role of the coordinator is akin to the cen-
tralized node in the case of RCP [9]. Such a design is amenable in
case of ISP networks where the centralized administrative node can
act as the map coordinator and periodically disseminates the net-
work map. In addition, to better handle packet forwarding during
map transitions (see Section 4.3.1), routers must store both the cur-
rent and the previous map, instead of only the current map as most
of the existing protocols do.
FIB state: If dynamic failure-based path computations are not
cached, the FIB state is doubled since at the minimum, the next
hop information should be maintained for current map and previous
map. Even if path computations are cached, the average additional
state required is not high. With the precomputation optimization for
each outgoing link (described in Section 3.1), the FIB state is again
only doubled overall, which we believe is a modest requirement.
Forwarding: In the optimized version of FCP, routers add a la-
bel corresponding to a list of failed links to packet headers, and
perform forwarding based on the label. Appending and forward-
ing based on labels is addressed by Multi-Protocol Label Switching
(MPLS) [12]. FCP also needs to invoke recomputation for new fail-
ures encountered, by invoking special processing via the slow path.
Applicability of intradomain routing controls: Since the notion
of having a link-state graph is retained, the key semantics of in-
tradomain routing maps remain unaffected. FCP continues to pro-
vide cost-based shortest-path routing in the absence of link fail-
ures. Hence, assignment of addresses and access controls, trafﬁc
engineering, and other aspects of conﬁguration/maintenance remain
unchanged. Speciﬁcally for trafﬁc engineering, long-term planning
changes to the link costs can be introduced by the central coordina-
tor. For short-term, reactive cost changes introduced by the routers
themselves, there would be a short delay since the updates are not
installed instantaneously, but have to go through the coordinator
before the TE link-cost changes become active. Since the link-cost
changes go through the coordinator, it can be ratiﬁed before it is
incorporated into the network map in order to preserve the path iso-
lation property.
7. EXTENSIONS TO FCP
We described FCP as a link-state routing protocol, and hence is
directly applicable to intradomain networks. Here, we present ex-
tensions to FCP to broaden the scope of applicability. Speciﬁcally,
we turn to how FCP can be used to improve interdomain routing,
both in terms of iBGP and eBGP routing stability.
7.1 Improving iBGP stability under link fail-
ures
Hot potato routing is commonly used by ISPs to select the clos-
est exit point amongst multiple equally-good interdomain routes.
Failure of links within the network, failures of next-hop links going
out of the network from the border routers, as well as small per-
turbations in intradomain costs can lead to hot-potato disruptions,
where large amounts of trafﬁc oscillate between egress points. Such
disruptions can lead to routing loops, router overload, and exter-
nally visible BGP routing changes [33, 34]. Hence a scheme that
can prevent such instability during change of egress points is de-
sired [32, 33].
We present a simple modiﬁcation to FCP to allow it to operate
over iBGP routes within a single domain for the case of failure of
links. We augment the link-state network map maintained by inter-
nal routers to treat an egress route to a particular preﬁx as a virtual
link directly connected to the destination preﬁx. FCP is agnostic to
the the notion of virtual links, and it treats virtual and actual links
identically; we use the term virtual link only for convenience. When
either a normal link or a virtual link fails, a router can use FCP
to forward the packet to an alternate egress connected to the same
next-hop AS. This ensures that routing to external routes remains
consistent even if failures are not immediately propagated.
Figure 11: Mitigating iBGP disruptions using FCP.
A simple illustrative example is shown in Figure 11. The network
map consists of actual links E1 − R1, E2 − R2 and R1 − R2, and
virtual links E1 − P and E2 − P for preﬁx P . Initially, let both
routers R1 and R2 use egress E1 to reach the destination preﬁx P .
When the link (R1, E1) (or the BGP next hop from E1 towards
P ) fails, R1 appends the (R1, E1) (or the virtual link (E1, P )) to
packet headers causing R2 to forward the packets via E2. Tradi-
tional iBGP/OSPF routing would undergo a routing loop between
R1 and R2 lasting until R2’s scan process, i.e. visiting the BGP
routing decision for each preﬁx, completes.
7.2
Interdomain policy routing
Interdomain routing today suffers from long outages arising
from a slow convergence process that occurs after certain routing
events [21]. In this section, we discuss how we can leverage FCP to
avoid failures during the convergence process of BGP. In our pro-
posal, we only consider changes on the data plane; we do not mod-
ify BGP’s route announcement and propagation protocol. Next, we
discuss two of the key challenges faced by our proposal: (a) how
are the network maps deﬁned and distributed? (b) how are policies
respected when FCP is used?
7.2.1 FCP network map
Unlike the case of intradomain routing, there is no natural cen-
tralized authority to act as a coordinator for distributing AS-level
network maps. Hence, we assume that nodes work with inconsis-
tent maps and use SR-FCP (Section 2.2).
All routers in the network run BGP protocol for exchanging
routes as they do today. Each router deﬁnes the FCP map using
the latest set of BGP updates it has received from all its neighbors.
7.2.2 Using SR-FCP with policy routing
Naively implementing SR-FCP would have adverse policy impli-
cations. This is because, by using AS-level source routes, an AS can
force downstream ASes to forward trafﬁc at the expense of violat-
ing their own policies. We next present a solution to this challenge.
The main idea behind our solution is to treat any policy viola-
tion as a link failure. We assume that ASes only implement policies
that are a function of the neighbor from whom they received the
advertisement and local policy considerations (as opposed to, for
example, policies dependent on the presence of an non-neighbor
AS in the AS-path). Almost all of today’s BGP policies fall into
this category [35].