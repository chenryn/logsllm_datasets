### 5.3 Benefits of FCP

#### 5.3.1 Comparison with OSPF

In Figure 7(a), we vary the rate at which OSPFD sends HELLO packets to neighbors and measure the effect on control overhead and the fraction of data packets delivered. We tuned FCP with a fast probing rate (one probe every 50ms) because faster probing does not incur a penalty in control overhead in terms of LSAs disseminated, as none of the detected link failures are announced.

The expected number of failures encountered by a packet is proportional to the diameter of the network.

**Figure 7:**
- **(a)** Unlike FCP, OSPF cannot simultaneously provide low control overhead and high availability.
- **(b)** Reducing FCP’s HELLO timer reduces stretch and loss without increasing control overhead.
- **(c)** OSPF’s map becomes inconsistent with the topology at low probing rates, resulting in a stretch penalty.

As the HELLO probing rate increases, the number of lost data packets decreases because we react to failures more quickly. However, probing at a faster rate also causes more short-term failures to be detected and propagated, increasing control overhead. Note that when measuring control overhead, we exclude HELLO messages for both FCP and OSPF.

On the other hand, FCP experiences only a very small (yet non-zero) loss rate of less than 0.1%. The loss in FCP in our experiments is non-zero because link failure detection takes a finite time, and during this period, all packets trying to use that link will be dropped.

Although FCP exhibits similar behavior to OSPF in terms of stretch and loss rate while varying the probing rate (Figure 7(b)), its control overhead is not a function of the link failure rate. Therefore, its probing rate can be increased without inflating control overhead. We found it was possible to tune OSPF to achieve this loss rate, but only at the expense of increasing its control traffic to above 300 messages per second per link.

The average stretch shows a similar result; as the probing rate decreases, it takes longer to detect link repairs, and hence a larger fraction of working paths are not discovered by a packet. This is shown by the first two lines in Figure 7(c). Since we can keep the FCP probing rate high without compromising overhead, FCP has a much lower stretch than OSPF. The low stretch is due to FCP’s ability to discover efficient secondary routes, rather than the fact that most paths are not affected by failure. To illustrate this point, we plot the results only over packets whose primary paths are failed (see the bottom two lines in Figure 7(c) marked ‘-fp’).

#### 5.3.2 Effect of Varying Parameters

In Figure 8(a), we vary the mean interarrival time for link failures, fix OSPF’s probing interval at 400ms, and plot the loss rate and overhead. For a wide variety of failure rates, FCP outperforms OSPF by an order of magnitude while maintaining a lower control overhead. Note that OSPF’s overhead begins decreasing as the failure rate increases past the ability of the probing protocol to keep up with link events.

In Figure 8(b), we vary the probing rate and plot the fraction increase in loss rate of OSPF over the loss rate of FCP for various topologies. Although the amount of improvement varies across topologies, FCP provides more than one order of magnitude lower loss rate than OSPF. As shown in Figure 8(c), FCP also reduces control overhead. In general, we found that denser topologies (e.g., AS 1221, with an average degree of 6.2) had less benefit from FCP than sparser topologies (e.g., AS 3257, with an average degree of 3.7). This happens because in denser topologies, OSPF has a larger number of paths to choose from and is thus more likely to discover a working path.

#### 5.3.3 Comparison with Backup-Path Selection

Unlike OSPF, the backup-path strategy we used can attain very fast failover times without a significant increase in control overhead. However, to minimize loss rates, the backup-path strategy needs to account for every failure contingency and hence requires a substantial number of backup paths. Precomputing a large number of backup paths to account for different combinations of multiple link failures increases state per router. This tradeoff is shown in Figure 9(a). For example, with 8 backup paths per link, the backup path strategy requires 4210 entries per router and experiences a loss rate of 0.05%. However, on the same workload, FCP requires only 255 entries yet attains a loss rate of less than 0.002%. Moreover, unlike FCP, the distribution in state across routers is not uniform, and the top 1% of routers require more than 20,285 entries. However, for switching between maps, FCP must temporarily maintain a second copy of its routing state. Although this state is only maintained for a short period and can be stored as deltas (differences from the current map), in the worst case, this could double FCP’s state requirements (to 510 in this example).

Figure 9(b) shows the performance in the presence of simultaneous failures on two representative topologies. We fix the number of backup paths to two and vary the number of randomly selected links to simultaneously fail. On both topologies, the backup strategy and FCP have roughly equal loss rates during single failures. However, when more than one failure occurs, FCP has significantly lower losses (FCP has non-zero loss since failure detection is not instantaneous). This happens because as the failure rate increases, it becomes more likely that the backup-path strategy will encounter a set of failures not covered by the set of backup paths. Finally, Figure 9(c) shows that the backup-paths strategy incurs a stretch penalty during link failures. This happens because link-disjoint paths are used as backups, which tend to be longer than the path FCP finds around the failure.

#### 5.3.4 Effect of Inconsistent Maps

So far, we have assumed that all nodes have a consistent state of the network map. Here, we investigate the overhead incurred by SR-FCP—in terms of average routing stretch and per-packet overhead—as a function of map inconsistency factor (see Figure 10). Specifically, for a chosen map inconsistency factor \( d \), we instantiate the network map \( M_n \) at each node \( n \) by picking links randomly from the actual network map \( M \), such that the intersection of maps at all nodes forms a spanning and connected subgraph of \( M \), which contains only a fraction \( (1 - d) \) of links in \( M \) (with high probability). The x-axis is capped at 0.3 since that is the largest fraction of inconsistency for which the intersection of maps at all nodes forms a spanning subgraph.

The plot shows that the stretch and packet overhead are small even when maps are highly inconsistent. Even when the inconsistency factor is 0.3, the average stretch is less than 1.03, and the average size of a packet header is under 10 bytes (assuming 2 bytes per node for source routes and failures). The reason SR-FCP performs so well is because SR-FCP pays a penalty only if the source node performing a route computation misses some links that could have resulted in significantly shorter paths; an intermediate node just forwards a packet based on the packet’s source route, irrespective of whether downstream links in that source route (not adjacent to the intermediate node) are present in the node’s map or not.

### 6. Deployment Issues

FCP represents a substantial departure from traditional routing mechanisms and, therefore, requires several modifications to router design even for deployment at the intradomain level. Although these changes are by no means trivial, in this section, we outline how they may be implemented as extensions to existing protocols and designs.

#### Map Dissemination

The role of the coordinator is akin to the centralized node in the case of RCP [9]. Such a design is amenable in the case of ISP networks where the centralized administrative node can act as the map coordinator and periodically disseminates the network map. In addition, to better handle packet forwarding during map transitions (see Section 4.3.1), routers must store both the current and the previous map, instead of only the current map as most existing protocols do.

#### FIB State

If dynamic failure-based path computations are not cached, the FIB state is doubled since, at the minimum, the next-hop information should be maintained for the current map and the previous map. Even if path computations are cached, the average additional state required is not high. With the precomputation optimization for each outgoing link (described in Section 3.1), the FIB state is again only doubled overall, which we believe is a modest requirement.

#### Forwarding

In the optimized version of FCP, routers add a label corresponding to a list of failed links to packet headers and perform forwarding based on the label. Appending and forwarding based on labels is addressed by Multi-Protocol Label Switching (MPLS) [12]. FCP also needs to invoke recomputation for new failures encountered by invoking special processing via the slow path.

#### Applicability of Intradomain Routing Controls

Since the notion of having a link-state graph is retained, the key semantics of intradomain routing maps remain unaffected. FCP continues to provide cost-based shortest-path routing in the absence of link failures. Hence, assignment of addresses and access controls, traffic engineering, and other aspects of configuration/maintenance remain unchanged. Specifically for traffic engineering, long-term planning changes to the link costs can be introduced by the central coordinator. For short-term, reactive cost changes introduced by the routers themselves, there would be a short delay since the updates are not installed instantaneously but have to go through the coordinator before the TE link-cost changes become active. Since the link-cost changes go through the coordinator, they can be ratified before being incorporated into the network map to preserve the path isolation property.

### 7. Extensions to FCP

We described FCP as a link-state routing protocol and, hence, it is directly applicable to intradomain networks. Here, we present extensions to FCP to broaden the scope of applicability. Specifically, we turn to how FCP can be used to improve interdomain routing, both in terms of iBGP and eBGP routing stability.

#### 7.1 Improving iBGP Stability Under Link Failures

Hot potato routing is commonly used by ISPs to select the closest exit point among multiple equally good interdomain routes. Failures of links within the network, failures of next-hop links going out of the network from the border routers, and small perturbations in intradomain costs can lead to hot-potato disruptions, where large amounts of traffic oscillate between egress points. Such disruptions can lead to routing loops, router overload, and externally visible BGP routing changes [33, 34]. Hence, a scheme that can prevent such instability during changes in egress points is desired [32, 33].

We present a simple modification to FCP to allow it to operate over iBGP routes within a single domain for the case of failure of links. We augment the link-state network map maintained by internal routers to treat an egress route to a particular prefix as a virtual link directly connected to the destination prefix. FCP is agnostic to the notion of virtual links and treats virtual and actual links identically; we use the term virtual link only for convenience. When either a normal link or a virtual link fails, a router can use FCP to forward the packet to an alternate egress connected to the same next-hop AS. This ensures that routing to external routes remains consistent even if failures are not immediately propagated.

**Figure 11: Mitigating iBGP disruptions using FCP.**

A simple illustrative example is shown in Figure 11. The network map consists of actual links E1-R1, E2-R2, and R1-R2, and virtual links E1-P and E2-P for prefix P. Initially, let both routers R1 and R2 use egress E1 to reach the destination prefix P. When the link (R1, E1) (or the BGP next hop from E1 towards P) fails, R1 appends the (R1, E1) (or the virtual link (E1, P)) to packet headers, causing R2 to forward the packets via E2. Traditional iBGP/OSPF routing would undergo a routing loop between R1 and R2 lasting until R2’s scan process, i.e., visiting the BGP routing decision for each prefix, completes.

#### 7.2 Interdomain Policy Routing

Interdomain routing today suffers from long outages arising from a slow convergence process that occurs after certain routing events [21]. In this section, we discuss how we can leverage FCP to avoid failures during the convergence process of BGP. In our proposal, we only consider changes on the data plane; we do not modify BGP’s route announcement and propagation protocol. Next, we discuss two of the key challenges faced by our proposal: (a) how are the network maps defined and distributed? (b) how are policies respected when FCP is used?

##### 7.2.1 FCP Network Map

Unlike the case of intradomain routing, there is no natural centralized authority to act as a coordinator for distributing AS-level network maps. Hence, we assume that nodes work with inconsistent maps and use SR-FCP (Section 2.2).

All routers in the network run the BGP protocol for exchanging routes as they do today. Each router defines the FCP map using the latest set of BGP updates it has received from all its neighbors.

##### 7.2.2 Using SR-FCP with Policy Routing

Naively implementing SR-FCP would have adverse policy implications. This is because, by using AS-level source routes, an AS can force downstream ASes to forward traffic at the expense of violating their own policies. We next present a solution to this challenge.

The main idea behind our solution is to treat any policy violation as a link failure. We assume that ASes only implement policies that are a function of the neighbor from whom they received the advertisement and local policy considerations (as opposed to, for example, policies dependent on the presence of a non-neighbor AS in the AS-path). Almost all of today’s BGP policies fall into this category [35].