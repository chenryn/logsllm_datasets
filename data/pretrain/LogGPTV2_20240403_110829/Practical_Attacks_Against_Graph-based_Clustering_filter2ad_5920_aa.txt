title:Practical Attacks Against Graph-based Clustering
author:Yizheng Chen and
Yacin Nadji and
Athanasios Kountouras and
Fabian Monrose and
Roberto Perdisci and
Manos Antonakakis and
Nikolaos Vasiloglou
Practical Aacks Against Graph-based Clustering
Yizheng Chen
Yacin Nadji
Athanasios Kountouras
School of Computer Science, College
School of Electrical and Computer
School of Computer Science, College
Georgia Institute of Technology
Georgia Institute of Technology
Department of Computer Science
School of Electrical and Computer
of Computing
Georgia Institute of Technology
PI:EMAIL
Fabian Monrose
Department of Computer Science
University of North Carolina at
Chapel Hill
PI:EMAIL
Engineering
PI:EMAIL
Roberto Perdisci
University of Georgia
PI:EMAIL
of Computing
PI:EMAIL
Manos Antonakakis
Engineering
Georgia Institute of Technology
PI:EMAIL
Nikolaos Vasiloglou
Symantec CAML Group
PI:EMAIL
ABSTRACT
Graph modeling allows numerous security problems to be tackled
in a general way, however, little work has been done to under-
stand their ability to withstand adversarial attacks. We design and
evaluate two novel graph attacks against a state-of-the-art network-
level, graph-based detection system. Our work highlights areas in
adversarial machine learning that have not yet been addressed,
specically: graph-based clustering techniques, and a global feature
space where realistic attackers without perfect knowledge must
be accounted for (by the defenders) in order to be practical. Even
though less informed attackers can evade graph clustering with
low cost, we show that some practical defenses are possible.
KEYWORDS
Adversarial Machine Learning; Unsupervised Learning; DGA; Net-
work Security
1 INTRODUCTION
Network level detection systems are used widely by the community
as the rst line of defense against Internet threats [9, 20, 27, 29, 33,
41, 63]. These systems often represent the underlying network traf-
c as a graph for various reasons, but most importantly for the com-
putational eciency and scalability that graph techniques enable.
These computational advantages, for example, enable categorical
objects (like domain names and IP addresses) to be transformed
into feature vectors in a multi-dimensional euclidean space. This
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for prot or commercial advantage and that copies bear this notice and the full citation
on the rst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specic permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’17, October 30-November 3, 2017, Dallas, TX, USA
© 2017 Association for Computing Machinery.
ACM ISBN 978-1-4503-4946-8/17/10...$15.00
https://doi.org/10.1145/3133956.3134083
allows supervised and unsupervised learning to take place with
greater eciency.
The wealth of new capabilities that statistical learning systems
brought to the security community make them a prime target for
adversaries. Several studies have shown how security systems that
employ machine learning techniques can be attacked [36, 49, 52,
60, 62], decreasing their overall detection accuracy. This reduction
in accuracy makes it possible for adversaries to evade detection,
rendering defense systems obsolete.
While graph based network detection systems are not immune
to adversarial attack, the community knows little about practical at-
tacks that can be mounted against them. As these network detectors
face a range of adversaries (e.g., from script kiddies to nation states),
it is important to understand the adversary’s capabilities, resources,
and knowledge, as well as the cost they incur when evading the
systems.
In this paper we present the rst practical attempt to attack
graph based modeling techniques in the context of network security.
Our goal is to devise generic attacks on graphs and demonstrate
their eectiveness against a real-world system, called Pleiades [9].
Pleiades is a network detection system that groups and models
unsuccessful DNS resolutions from malware that employ domain
name generation algorithms (DGAs) for their command and control
(C&C) communications. The system is split into two phases. First,
an unsupervised process detects new DGA families by clustering
a graph of hosts and the domains they query. Second, each newly
detected cluster is classied based on the properties of the generated
domains.
To evade graph clustering approaches like Pleiades, we devise
two novel attacks—targeted noise injection and small community—
against three commonly used graph clustering or embedding tech-
niques: i) community discovery, ii) singular value decomposition
(SVD), and iii) node2vec. Using three dierent real world datasets (a
US telecommunication dataset, a US university dataset and a threat
feed) and after considering three classes of adversaries (adversaries
with minimal, moderate and perfect knowledge) we mount these
two new attacks against the graph modeling component of Pleiades.
Session E4:  Adversarial Social NetworkingCCS’17, October 30-November 3, 2017, Dallas, TX, USA1125We show that even an adversary with minimal knowledge, i.e.,
knowing only what is available in open source intelligence feeds
and on their infected hosts, can evade detection.
Beyond devising practical attacks, we demonstrate that the at-
tacks are inexpensive for adversaries. Fortunately, defenders are
not without recourse, and detection systems’ parameters can be
tuned to be more resistant to evasion. Based on these discoveries,
we make recommendations to improve Pleaides’ resilience.
Our paper makes the following contributions:
Two Novel Attacks. The targeted noise injection attack improves
on prior work that randomly injects noise; by targeting the injected
vertices and edges to copy the graph structure of the original signal,
we force noise into the resulting clusters. Our small community
attack abuses the known property of small communities in graphs to
subdivide and separate clusters into one or more unrelated clusters.
Practical Attacks and Defenses. While more knowledgeable at-
tackers typically fare better, we demonstrate that even minimal
knowledge attackers can be eective: attackers with no knowledge
beyond their infections can render 84% of clusters too noisy to be
useful, and evade clustering at a rate of 75%. The above attacks can
be performed at low cost to the adversary by not appearing to be
anomalous, nor losing much connectivity. Simple defenses raise
the attacker’s costs and force only 0.2% of clusters to be too noisy,
and drop the success rate to 25%. State of the art embeddings, such
as node2vec, oer more adversarial resistance than SVD, which is
used in Pleiades.
2 BACKGROUND
2.1 Graph-based Clustering
Graph clustering is commonly used in security. Community discov-
ery identies criminal networks [39], connected components track
malvertising campaigns [21], spectral clustering on graphs discov-
ers botnet infrastructure [9, 20], hierarchical clustering identies
similar malware samples [11, 45], binary download graphs group
potential malware download events [29, 40, 41], and newly devised
graph embeddings, like node2vec [26], could further improve upon
the state of the art. Beyond clustering, other graph-based techniques
are used, such as belief propagation [18, 46]. Unfortunately, it is
unknown how resistant these techniques are to adversarial evasion.
2.1.1 Community Detection. There are many ways to detect
communities in a graph. Several techniques in this space rely on
a modularity metric to evaluate the quality of partitions, which
measures the density of links inside and outside communities. This
allows an algorithm to optimize modularity to quickly nd com-
munities. The Louvain algorithm [14] scales to large networks
with hundreds of millions of vertices. Communities are usually
hierarchical [42, 47, 50]; however, nding sub-communities within
communities is a known hard problem [15]. This allows attackers
to hide sub-communities in a “noisy” community by adding edges.
Spectral Methods. In [57], Braverman et al. discuss several
popular spectral clustering strategies. First, a similarity matrix is
used to represent the graph. Each row and each column represent a
vertex to be clustered, and the weight is a similarity score between
the corresponding vertices. After proper normalization, the matrix
2.1.2
M is used as input to singular value decomposition (SVD) of rank k,
SV Dk (M) = U  V ⇤. When the resulting eigenvectors (e.g., vectors
in U ) are further normalized, they can be used as an embedding
in a euclidean space for learning tasks. In spectral methods, the
hyperparameter k is usually chosen by rst evaluating the scree
plot of eigenvalues to identify the “elbow” where higher ranks have
diminishing returns of representing the input matrix. When the
scree plot starts to plateau at the ith eigenvalue, we set k = i [17, 59].
Spectral clustering with SVD is known to have limitations when
clusters are imbalanced; this is due to either graphs being scale-free
(power law distribution) [31], or when small communities exist [30].
Unfortunately, both commonly occur in real-world data. In practice,
these small communities are merged into what is colloquially called
the “death star” cluster: a large, noisy cluster that contains many
small communities.
2.1.3 node2vec. Contrary to the strong homophily assumption
of community detection and spectral clustering, node2vec [26] has
the advantage of balancing homophily and structural equivalence in
its embeddings. For example, vertices that are sink nodes will have
similar embeddings. node2vec generates embeddings of vertices
by optimizing the sum of the log likelihood of seeing the network
neighborhood given a vertex  , for all vertices on the graph:
max
f X log P (NS ( )| f ( ))
(1)
Where f ( ) is the embedding of vertex  , NS ( ) represents the
network neighborhoods of   with a series of vertices obtained by
the sampling strategy S. node2vec proposes a sampling strategy
by random walks starting from every vertex on the graph with
the following parameters: 1) number of walks from each vertex,
2) length of each walk, 3) probability to return to the same vertex
(Breadth First Search), and 4) probability to explore out to fur-
ther vertices (Depth First Search). Once the walk samples have
been obtained, node2vec uses a tunable neighborhood size to get
the neighborhood of vertices. For example, a walk with length 5
{ 1, 2, 3, 4, 5} generates the following neighborhoods with size
3: N ( 1) = { 2, 3, 4}, N ( 2) = { 3, 4, 5}.
In order to compute the embeddings given f ( ), Equation 1 is
factorized as a product of the conditional probability of each ver-
tex in the neighborhood based on the conditional independence
assumption. Each underlying conditional probability is dened as
a sigmoid function, and the embeddings are learned by stochas-
tic gradient descent (SGD) with negative sampling optimization.
Eectively, node2vec learns embeddings in a fashion similar to
word2vec [37] but does not use skip-grams. Attackers can target
the neighborhood size and sampling parameters to encourage their
vertices to be under-sampled and thus split into multiple noisy
clusters.
2.2 Related Work
Existing work in adversarial machine learning has focused on an-
alyzing the resilience of classiers. Huang et al. [28] categorize
attack inuence as either causative or exploratory, with the former
polluting the training dataset and the latter evading the deployed
system by crafting adversarial samples. Following the terminology
of Huang et al., our work focuses on exploratory attacks that target
Session E4:  Adversarial Social NetworkingCCS’17, October 30-November 3, 2017, Dallas, TX, USA1126the graph clustering component of Pleiades. We assume that the
clustering hyperparameters are selected with attack-free labels,
and the subsequent classier is not polluted when they are trained.
Contrary to other exploratory attacks in literature, we face the chal-
lenge that the clustering features cannot be modied or computed
directly, and that attackers often have an incomplete view of the
defender’s data.
In order to compute optimal graph partitions or vertex embed-
dings, one needs to have a global view of all objects on the graph.
On the contrary, related work can compute classication features
directly from crafting adversarial samples. For example, features are
directly obtained from spam emails [36, 60], PDF les [49, 53, 62],
phishing pages [49], images [16, 25, 44, 52], network attack pack-
ets [24], and exploits [55, 58]. These security applications classify
an object based on features extracted from only that object and its
behavior. This makes the features of system classiers more local,
and enables evasion techniques such as gradient descent directly in
the feature space. We make the following denition: a local feature
can be computed from only one object; whereas a global feature
needs information from all objects being clustered or classied.
Since Pleiades uses global features, an adversary’s knowledge
can aect the success of attacks. For example, if the adversary
has full access to the defender’s datasets, she can reliably compute
clustering features and is more equipped to evade than a less knowl-
edgeable attacker. Many researchers [43, 56] have shown that, even
without access to the training dataset, having knowledge about the
features and an oracle to obtain some labels of objects is sucient
for an attacker to approximate the original classier.
Biggio et al. [12, 13] are the rst to study adversarial clustering.
They propose a bridge attack, which works by injecting a small
number of adversarial samples to merge clusters. The attackers
have perfect knowledge in their assumption. We distinguish our
work by i) considering attackers with dierent knowledge levels,
ii) evaluating how adversarial graph-clustering in network security
aects the whole system, and iii) quantifying the cost of attacks.
With respect to attack cost analysis, Lowd et al. [35] propose a linear
cost function as a weighted sum of feature value dierences for
crafting evasive adversarial samples. Since we do not work directly
in the feature space, we propose dierent costs for the attacks we
present in Section 3.
To summarize, our work is novel because we focus on adversar-
ial clustering, which deals with global features that cannot be di-
rectly changed. We also evaluate capabilities of attackers with various
knowledge levels, and quantify the costs associated with attacks.
3 THREAT MODEL & ATTACKS
In this section, we describe our threat model and explain our attacks
as modications to a graph G. In practice, the attacker changes the
graph based on the underlying data that are being clustered. For
example, if the vertices in a graph are infected hosts and the domains
they query as in Pleiades, the graph representation can be altered
by customized malware that changes its regular querying behavior.
3.1 Notation
An undirected graph G is dened by its sets of vertices (or nodes)
V and edges E, where G = (V , E) and E = {( i , j ) : if there exists
an edge between  i and  j , i 2 V , j 2 V}. An undirected bipartite
graph is a special case where V can be divided into two disjoint sets
(U and V ) such that every edge connects at a vertex in U and one
in V , represented as G = (U ,V , E). While the attacks apply in the
general case, oftentimes bipartite graphs appear in security contexts:
hosts (U ) query domains (V ), clients connect to servers, malware
make system calls, etc. Finally, a complete undirected bipartite graph
is where every vertex in U has an edge to every vertex in V .
G is an undirected graph that represents the underlying data
a defender clusters. The graph clustering subdivides G into clus-
ters C0, . . . ,Ck, where V = C0 [ . . . [ Ck. If the graph clustering
method is based on graph partitions, then each cluster Ci is a sub-
graph Gi, and G = G0 [ . . . [ Gk. Often when applied, a defender
seeks to cluster vertices either in U or V of the bipartite graph, for
example, cluster end hosts based on the domains they resolve, or
malware based on the system calls they make. An attacker controls
an attacker graph, G ⇢G . The adversary uses the targeted noise in-
jection and the small community attacks described below to change
G to G0, by adding or removing nodes and edges from G.
These attacks violate the underlying basic assumptions of graph
clustering techniques, which either renders the clustered subgraph G0
to be useless to the defender or prevents G0 from being extracted from
G intact (See Section 3.3).
3.2 Threat Model
Before describing attacker knowledge levels, we discuss knowledge
that is available to all attackers. We assume all attackers have at
least one active infection, or G ⇢G . The attacker is capable of
using any information that could be gathered from G to aid in
their attacks. We also assume that an attacker can evaluate clusters
like a defender can, e.g., manual verication. When done with a
classier, an attacker has black-box access to it or can construct
a surrogate that approximates the accuracy and behavior of the
real classier based on public data. This may seem extreme, but
the plethora of open source intelligence (OSINT) [4, 6, 22] data and
MLaaS machine learning tools [1–3, 5, 7] make this realistic. Finally,
an attacker has full knowledge of the features, machine learning
algorithms, and hyperparameters used in both the unsupervised
and supervised phases of the system under attack, as these are
often published [8, 9, 20, 29, 41, 45, 48]. Since clustering requires
some graph beyond G, we must consider attackers with various
representations of the defender’s G. We evaluate three levels: mini-
mal, moderate, and perfect knowledge. The minimal level attacker
only knows what is in their attack graph G, but the perfect attacker
possesses G. For example, a perfect adversary would have access
to the telecommunication network data used in Pleiades, which is
only obtainable by the most sophisticated and well resourced of
adversaries.
Minimal Knowledge. The minimal knowledge case represents
the least sophisticated adversary. In this case, only the attacker
graph G is known, as well as any open source intelligence (OSINT).
For example, the attacker can use OSINT to select potential data to
inject as noise, or can coordinate activities between their vertices
in G. In the Pleiades example, an attacker with minimal knowledge
can draw information from their infected hosts.
Session E4:  Adversarial Social NetworkingCCS’17, October 30-November 3, 2017, Dallas, TX, USA1127Moderate Knowledge. The moderate knowledge case represents
an adversary with ˜G, an approximation of G. If attacking Pleiades,
˜G would be a host/domain graph from a large enterprise or uni-
versity in order to approximate the view that the defender has.
˜G
This allows the adversary to evaluate their attacks. The size of
aects the evaluation from the attacker’s perspective, which we
˜G. An attacker
will explore by randomly sampling subgraphs of
with moderate knowledge is similar to a sophisticated adversary
with access to large datasets through legitimate (i.e., commercial
data oerings) or illegitimate (i.e., security compromises) means.
Perfect Knowledge. Finally, the perfect knowledge case is for an
adversary who has obtained G from the defender. Given the full
dataset and knowledge of the modeling process, an adversary can
completely reconstruct the clustering results of the defender to
evaluate the eectiveness of their attacks. Ideally, this data would
be well guarded making this level of knowledge only realistic for
the most sophisticated of attackers, e.g., nation-state sponsored
threats. Nevertheless, considering the damage that could be done
by a perfect knowledge attacker is important as a security evalua-
tion, since it allows us to nd potential weaknesses in the graph
clustering techniques.
3.3 Attacks
We present two novel attacks against graph clustering. Therst,
targeted noise injection, improves on random injections [34, 54] by
emulating the legitimate signal’s graph structure. The second, small
community attack, exploits the known phenomenon of small com-
munities in graphs [30, 32]. Our attacks violate both the homophily
and the structural equivalence assumptions used by graph clus-
tering methods. That is, our attacks either change what nodes are
close together to violate homophily, or they change observations
of node neighborhoods so as to violate structural equivalence.
Identifying a successful attack depends on the system, which
will be described in detail in Section 4. Since we use Pleiades, we
evaluate attacks by the impact on a subsequent classication of the
resulting adversarial clusters. However, this could be done purely at
the unsupervised level by manually evaluating the accuracy of the
output clusters, or leveraging prior work in adversarial malware
clustering [12] to measure global cluster quality decrease. Next, we
evaluate the cost incurred by the attacker. We analyze the costs by
measuring changes to their graph’s structure that would eitherag
them as anomalous or damage connectivity between the graph’s
vertices. In the descriptions below, an attacker’s initial graph G is
shown, and the alterations yield a modied graph, G0, that repre-
sents a defender’s view of the attacker’s graph after the adversarial
manipulation.
3.3.1 Targeted Noise Injection. Figure 1 illustrates two targeted
noise injection attacks. Consider a bipartite attacker graph G, with
vertex sets U (circles) and V (squares). To mount the attack, noise
is injected into G to generate G0. We inject noisy edges from nodes
controlled by the attacker for the purpose of mirroring real edges.
This encourages newly connected nodes to be clustered together
with the attacker’s nodes.
Add Noise to
New Nodes
G
Add Noise to
Existing Nodes
V'
U
G'
U
V
V
V'
V
U
G'