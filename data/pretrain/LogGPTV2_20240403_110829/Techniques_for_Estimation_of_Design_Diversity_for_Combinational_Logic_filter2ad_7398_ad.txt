### 5. Adaptive Monte-Carlo Simulation

-15
The function (as derived from Appendix B) is used to estimate the values of \(d_{i,j}\). If we can tolerate an error of 10% for high values of \(d_{i,j}\) (i.e., the value of \(E\) is 0.1), then in each Monte-Carlo experiment, we need to apply \(N = 15,200\) input combinations. For 10 such Monte-Carlo experiments, a total of approximately 150,000 (or 30,000 for 2 experiments) input combinations are required. When the number of inputs for the given logic function is very high (greater than 20), this technique proves to be very effective.

Table 5.1 illustrates the advantages of using the adaptive Monte-Carlo simulation technique described in Section 5.1 for various values of percentage errors compared to exhaustive simulation. We calculated the probabilities of the logic functions implemented by some MCNC benchmark circuits and compared the results with those obtained through exhaustive simulation. The benchmark circuits considered are such that exhaustive simulation is feasible.

For the adaptive Monte-Carlo simulation technique, we used different parameters:
1. 1% error and 2 Monte-Carlo experiments with 1.52 million simulations in each experiment.
2. 1% error and 10 Monte-Carlo experiments with 1.52 million simulations in each experiment.
3. 10% error and 2 Monte-Carlo experiments with 0.0152 million simulations in each experiment.
4. 10% error and 10 Monte-Carlo experiments with 0.0152 million simulations in each experiment.

The results in Table 5.2 show that the theoretical arguments behind the adaptive Monte-Carlo simulation are applicable to real-life circuits and demonstrate the effectiveness of this technique.

### 6. Conclusions

This paper demonstrates the feasibility of calculating the design diversity metric for arbitrary combinational logic circuits. Although the general problem is NP-complete, efficient algorithms can be devised to solve it. For datapath logic circuits and circuits with iterative logic networks, the regularity in circuit structures can be exploited to compute the diversity metric very quickly. For general combinational logic circuits, reduction techniques using fault equivalence and fault dominance relationships can be applied to significantly reduce the number of fault pairs to be considered during diversity calculation.

Next, the adaptive Monte-Carlo simulation technique can be used to obtain accurate estimates of the diversity metric for the reduced set of fault pairs using a number of simulations that is polynomial (instead of exponential) in the number of inputs of the combinational logic function. Moreover, the number of simulations can be adjusted based on the error tolerance during estimation.

This paper describes techniques for estimating diversity in combinational logic circuits; a related paper [Mitra OOb] describes design diversity estimation techniques for sequential logic circuits.

Table 5.2 shows some simulation results that demonstrate the accuracy of the adaptive Monte-Carlo simulation technique. Since the problem of calculating the \(d_{i,j}\) values can be modeled as a signal probability calculation problem (Section 5.1), we used the adaptive Monte-Carlo signal technique for estimating the signal probabilities.

### 7. Acknowledgment

This work was supported by the Defense Advanced Research Project Agency (DARPA) under Contract No. DABT63-97-C-0024 (ROAR project). We thank Mr. Mayur Dam of the Department of Computer Science, Stanford University, for helpful discussions.

### 8. References

[Avizienis 84] Avizienis, A. and J. P. J. Kelly, “Fault Tolerance by Design Diversity: Concepts and Experiments,” IEEE Computer, pp. 67-80, August 1984.

[Briere 93] Briere, D. and P. Traverse, “Airbus A320/A330/A340 Electrical Flight Controls: A Family of Fault-Tolerant Systems,” Proc. FTCS, pp. 616-623, 1993.

[Garey 79] Garey, M. and D. Johnson, Computers and Intractability: A Guide to the Theory of NP-Completeness, W. H. Freeman and Company, 1979.

[Lala 94] Lala, J. H. and R. E. Harper, “Architectural Principles for Safety-Critical Real-Time Applications,” Proc. of the IEEE, vol. 82, no. 1, pp. 25-40, January 1994.

[Lyu 91] Lyu, M. R. and A. Avizienis, “Assuring Design Diversity in N-Version Software: A Design Paradigm for N-Version Programming,” Proc. DCCA, pp. 197-218, 1991.

[McCluskey 88] McCluskey, E. J., S. Makar, S. Mourad, and K. D. Wagner, “Probability Models for Pseudo-Random Test Sequences,” IEEE Trans. Computers, Vol. 37, No. 2, pp. 160-174, Feb. 1988.

[McCluskey 00] McCluskey, E. J., and C. W. Tseng, “Stuck-at Faults vs. Actual Defects,” Proc. Intl. Test Conf., pp. 336-343, 2000.

[Mitra 99a] Mitra, S., N. R. Saxena, and E. J. McCluskey, “A Design Diversity Metric and Reliability Analysis of Redundant Systems,” Proc. IEEE Intl. Test Conf., pp. 662-671, 1999.

[Mitra 99b] Mitra, S., N. R. Saxena, and E. J. McCluskey, “A Design Diversity Metric and Analysis of Redundant Systems,” Technical Report, Center for Reliable Computing, Stanford Univ., CRC-TR-99-4, 1999.

[Mitra OOa] Mitra, S., N. R. Saxena, and E. J. McCluskey, “Common-Mode Failures in Redundant VLSI Systems: A Survey,” IEEE Trans. Reliability, Special Section on Fault-Tolerant VLSI Systems, 2000, To appear.

[Mitra OOb] Mitra, S., and E. J. McCluskey, “Design Diversity in Sequential Logic Circuits,” VLSI Test Symp., 2001.

[Mitra OOc] Mitra, S., N. R. Saxena, and E. J. McCluskey, “Techniques for Estimation of Design Diversity for Combinational Logic Circuits,” Technical Report, Center for Reliable Computing, Stanford University, CRC-TR-01-1, 2001 (http://crc.stanford.edu).

[Motwani 97] Motwani, R., and P. Raghavan, Randomized Algorithms, 1997.

[Parker 75] Parker, K.P., and E.J. McCluskey, “Probabilistic Treatment of General Combinational Networks,” IEEE Trans. Computers, Vol. C-24, No. 6, pp. 668-670, June 1975.

[Pradhan 96] Pradhan, D. K., Fault-Tolerant Computer System Design, Prentice Hall, 1996.

[Rabaey 96] Rabaey, J., Digital Integrated Circuits, Prentice Hall, Englewood Cliffs, 1996.

[Riter 95] Riter, R., “Modeling and Testing a Critical Fault-Tolerant Multi-Process System,” Proc. FTCS, pp. 516-521, 1995.

[Savir 90] Savir, J., “Improved Cutting Algorithm,” IBM Journal Res. and Dev., Vol. 34, No. 2-3, pp. 381-388, March-May 1990.

[Siewiorek 92] Siewiorek, D. P. and R. S. Swarz, Reliable Computer Systems: Design and Evaluation, Digital Press, 1992.

[Tamir 84] Tamir, Y. and C. H. Sequin, “Reducing Common Mode Failures in Duplicate Modules,” Proc. ICCD, pp. 302-307, 1984.

[To 73] To, K., “Fault Folding for Irredundant and Redundant Combinational Circuits,” IEEE Trans. Computers, Vol. C-22, No. 11, pp. 1008-1015, Nov. 1973.

[Tohma 71] Tohma, Y. and S. Aoyagi, “Failure-Tolerant Sequential Machines with Past Information,” IEEE Trans. Computers, Vol. C-20, No. 4, pp. 392-396, April 1971.

### Appendix A

**Theorem 5:** The value of \(Z_i\) is equal to:

\[
Z_i = \begin{cases} 
2(2^i - 1) & \text{if } CO = 0 \text{ is the only value CO can have;} \\
2^{i+1} & \text{if } CO \text{ can be both 0 or 1 with equal probability.}
\end{cases}
\]

**Proof:** The value of \(Z_i\) can be expressed using the following recurrence relation:

\[
Z_i = 2(2^{i-1} - 1) + 2Z_{i-1}, \quad \text{and} \quad Z_1 = 1 \text{ if } CO = 0, \quad Z_1 = 4 \text{ if } CO = 0 \text{ and } CO = 1 \text{ are equally likely.}
\]

The above recurrence relation follows from the fact that when \(A_i = 1\) and \(B_i = 1\), then the value of \(C_i\) will be 1, irrespective of the remaining bits. However, when \(A_i = 0\) and \(B_i = 1\), or vice-versa, then we want \(C_{i-1}\) to be equal to 1. When \(i = 1\), only \(A_1 = 1\) and \(B_1 = 1\) can make \(C_i\) equal to 1. The solution to the above recurrence relation is given by \(Z_i = 2(2^i - 1)\). This can be obtained easily by using the simple technique of generating functions.

If \(CO = 0\) and \(CO = 1\) are equally likely, \(Z_1 = 4\) and hence, \(Z_i = 2^{i+1}\) from the above recurrence relation. Q.E.D.

### Appendix B

There can be two kinds of errors in the adaptive Monte-Carlo estimation technique. The actual \(d_{i,j}\) value can be high, but we might erroneously declare that the estimated \(d_{i,j}\) value is very low. Although this situation will produce pessimistic values, it is not desired. As noted in Section 5.2, \(\beta\) is the probability that the value of \(d_{i,j}\) is out of the error bound. Hence, the probability that the \(d_{i,j}\) value is erroneously declared to be less than 0.5 is \(\beta\).

The other source of error is due to the fact that the actual \(d_{i,j}\) value may be less than 0.5, but for all the Monte-Carlo simulation experiments, it is estimated to be greater than or equal to 0.5. The worst scenario is when the actual \(d_{i,j}\) value is exponentially small, but the estimated value is close to 1. We show next that the probability of such an event is extremely small and almost negligible. The following fact has been proved [Motwani 97] (using Chernoff bounds):

\[
\Pr[Z > (1 + \alpha)p] < e^{-\frac{\alpha^2 p}{3}}
\]

The probability that the value of \(Z\) will be greater than 0.5 in all \(M\) experiments is less than \(\left(\frac{e^{-\frac{\alpha^2 p}{3}}}{0.5}\right)^M\). When the value of \(p\) is exponentially small, \(\alpha\) is of the order of \(2^i\), and this probability becomes extremely small.

Thus, our Monte-Carlo simulation is adaptive and suits the current application—it provides very good estimates for high \(d_{i,j}\) values and ensures that we do not erroneously estimate very high (optimistic) values when the actual \(d_{i,j}\) value is extremely small (less than 0.5).