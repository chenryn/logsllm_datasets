4. xxElemSyncLoss: This simulates a structured loss event where
a group of TM elements all suffer from missing data from the
same cause. Hence, the losses on each element are synchro-
nized. We do so by selecting xx% of rows of X to be effected,
and a set of times with probability q. Lost data comes from the
intersection of the selected rows and columns.
5. RowRandLoss: Random element loss, as presented above, is
not a particular realistic model for data loss. With ﬂow level
measurements, data are collected by a router. If that router can-
not collect data, then an entire row of each TM snapshot Z will
be missing. The effect on X is to remove a set of structurally
associated rows. We simulate this by dropping rows from the
original TM Z (before it is formed into the matrix X).
273E
A
M
N
0.5
0.4
0.3
0.2
0.1
0
2
0.5
0.4
0.3
0.2
0.1
E
A
M
N
0
10
−3
SRMF
SRMF+KNN
SRSVD base
KNN
SRSVD base + KNN
4
8
rank r (or k in the case of KNN)
(a) Abilene, loss prob. = 0.2
16
0.5
0.4
0.3
0.2
0.1
E
A
M
N
0
2
4
8
rank r (or k in the case of KNN)
(b) Commercial, loss prob. = 0.6
0.5
0.4
0.3
0.2
0.1
E
A
M
N
16
0
2
4
8
16
rank r (or k in the case of KNN)
(c) GÉANT, loss prob. = 0.95
Figure 2: Sensitivity with respect to the input rank r (or k in the case of KNN).
SRMF
SRMF+KNN
Baseline
SRSVD base
SRSVD base + KNN
0.5
0.4
0.3
0.2
0.1
E
A
M
N
0.5
0.4
0.3
0.2
0.1
E
A
M
N
−2
10
λ
−1
10
0
10
0
10
−3
−2
10
λ
−1
10
0
10
0
10
−3
−2
10
λ
−1
10
0
10
(a) Abilene, loss prob. = 0.2
(b) Commercial network, loss prob. = 0.6
Figure 3: Sensitivity with respect to λ.
(c) GÉANT, loss prob. = 0.95
0.4
0.3
0.2
0.1
0
Baseline
SRSVD base
KNN
SRSVD base+KNN
SRMF+KNN
AllElemSyncLoss
50ElemSyncLoss
25ElemSyncLoss
50ElemRandLoss
25ElemRandLoss
50TimeRandLoss
25TimeRandLoss
RowRandLoss
ColRandLoss
PureRandLoss
(a) loss prob. = 0.2.
0.4
0.3
0.2
0.1
0
Baseline
SRSVD base
KNN
SRSVD base+KNN
SRMF+KNN
AllElemSyncLoss
50ElemSyncLoss
25ElemSyncLoss
50ElemRandLoss
25ElemRandLoss
50TimeRandLoss
25TimeRandLoss
RowRandLoss
ColRandLoss
PureRandLoss
(b) loss prob. = 0.95.
Figure 4: Comparison between algorithms for the different loss models.
6. ColRandLoss: It is perhaps less likely that a column of the
original TM Z is dropped from measurement. One can con-
struct scenarios where a software bug causes such an error, but
in fact we primarily consider the random column loss scenario
for completeness.
In this section we examine the impact of the loss model on the
performance of the interpolation algorithms. Obviously there are
many ways of viewing this data. Due to space limitations, we
present here only a few representative ones. First, Figure 4 shows
bar charts of the performance of the key algorithms for two differ-
ent loss levels, across all loss models. The key observations are
that for low- to moderate loss, SRMF+KNN performs signiﬁcantly
better across all loss models. When loss is higher, there are some
cases where the performance of SRSVD-base and KNN is similar
to SRMF+KNN, and occasionally slightly better, but where losses
are highly structured (e.g., AllElemSyncLoss) SRMF+KNN is al-
ways clearly superior.
We show three of these cases in more detail in Figure 5. Fig-
ure 5(a) shows the Abilene network data, with random row loss.
The results for random column loss are similar, and both are qual-
itatively the same as those for pure random loss. The reasons are
clear when we consider the high loss case where both the baseline
(used in SRSVD-base) and the similarity metric used for KNN are
hard to calculate because entire rows or columns have no data. On
the other hand, our approach combines the spatial and temporal
components in its model.
Figure 5(b) shows the extreme of very structured loss (synchro-
nized in time). In this case, the baseline is so poor that all of the
other techniques collapse back to this baseline. Our approach still
performs reasonably. Figure 5(c) shows the case of random col-
umn damage (with about half of the rows affected). In this case,
our approach performs surprisingly well given that so much of the
structure of the matrix has been lost. This is yet another indication
of the importance of the spatio-temporal model.
Summary: These results show that SRMF+KNN is the best al-
gorithm over a wide range of loss scenarios and loss rates. In the
few cases where SRMF+KNN does not win, it is not far behind.
Meanwhile, SRSVD-base+KNN consistently performs better than
SRSVD-base, but not as well as SRMF+KNN, especially when
there is a large amount of structured loss. These results clearly
demonstrate the power of our spatio-temporal compressive sens-
ing framework to simultaneously exploit global and local structures
and leverage spatial and temporal properties. We expect more de-
tailed modeling of the spatial and temporal constraint matrices S
and T to further improve the accuracy.
274RowRandLoss
SRMF
SRMF+KNN
Baseline
SRSVD base
KNN
SRSVD base + KNN
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
E
A
M
N
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
E
A
M
N
0
0
0.2
0.8
1
0
0
0.2
0.4
0.6
data loss probability
(a) Abilene, RowRandLoss.
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
E
A
M
N
0.8
1
0
0
0.2
AllElemSyncLoss
50TimeRandLoss
0.4
0.6
data loss probability
0.4
0.6
0.8
1
data loss probability
(b) GÉANT, AllElemSyncLoss.
(c) Commercial, 50TimeRandLoss.
E
A
M
N
0.5
0.4
0.3
0.2
0.1
0
0
Tomo−Gravity
Base