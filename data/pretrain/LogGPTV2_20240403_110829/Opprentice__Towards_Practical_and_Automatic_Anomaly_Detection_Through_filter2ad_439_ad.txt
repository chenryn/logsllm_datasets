cThlds. Hence, we adopt EWMA [11] to predict the cThld of the
Figure 8: Evaluation ﬂow.
5.1 Data sets
We collect three representative types of KPI data (i.e., PV, #SR,
and SRT) from a large search engine (§2.1). These data are
labeled by the operators from the search engine using our labeling
tool. There are 7.8%, 2.8%, and 7.4% data points are labeled as
anomalies for PV, #SR, and SRT, respectively. Although the data
BestcThldIDof1-weekmovingtestsetsPV#SRSRT1. PV2. #SRData setsStaticcombinationsOther machine learningRandomforestDetectionapproachesFirst 8-weekdataAll historical data(Incremental retraining)Training setsF-ScoreSD(1,1)PC-ScoreAccuracy metrics5-Fold cross-validationEWMAcThld predictionsOpprenticeOther approachesDefault cThld3. SRTRecent 8-weekdataBasicdetectors(Opprentice as a whole)§ 4.3§ 4.4§ 4.5§ 4.6Labeling time vs. tuning timeDetecting lag and training time§ 4.7§ 4.8we used are from the search engine, they are not special cases only
for the search engine. For example, based on previous literature and
our experience, the PV data we used are visually similar to other
kinds of volume data. For example, the PV of other Web-based
services [1, 49], the RTT (round trip time) [6] and the aggregated
trafﬁc volume [5] of an ISP, and online shopping revenue. So we
believe that these three KPIs are sufﬁcient to evaluate the idea of
Opprentice, and we consider a more extensive evaluation with data
from other domains beyond search as our future work. Table 2
shows several ways to generate training sets and test sets from the
labeled data sets.
Table 2: Training sets and test sets. The test sets all start from
the 9th week and move 1 week for each step. The training
sets use different data before the test sets. I1 is the fashion
of Opprentice (incremental retraining). Others are used for
evaluating different strategies of training sets.
Training set
All historical data
Recent 8-week data
First 8-week data
Test set
ID
1-week moving window I1
4-week moving window I4
4-week moving window R4
4-week moving window F4
5.2 Detector and Parameter Choices
According to the detector requirements (§4.3.2), in this proof
of concept prototype, we evaluate Opprentice with 14 widely-used
detectors (Table 3).
Table 3: Basic detectors and sampled parameters.
Some
abbreviations are MA (Moving Average), EWMA (Exponen-
tially Weighted MA), TSD (Time Series Decomposition), SVD
(Singular Value Decomposition), win(dow) and freq(uency).
Sampled parameters
none
last-slot, last-day, last-week
win = 10, 20, 30, 40, 50
points
↵ = 0.1, 0.3, 0.5, 0.7, 0.9
win = 1, 2, 3, 4, 5 week(s)
Detector / # of conﬁgurations
Simple threshold [24] / 1
Diff / 3
Simple MA [4] / 5
Weighted MA [11] / 5
MA of diff / 5
EWMA [11] / 5
TSD [1] / 5
TSD MAD / 5
Historical average [5] / 5
Historical MAD / 5
Holt-Winters [6] / 43 = 64
SVD [7] / 5 ⇥ 3 = 15
Wavelet [12] / 3 ⇥ 3 = 9
ARIMA [10] / 1
↵, β, γ = 0.2, 0.4, 0.6, 0.8
row =10, 20, 30, 40, 50
points, column =3, 5, 7
win = 3, 5 ,7 days,
low, mid, high
Estimation from data
In total: 14 basic detectors / 133 conﬁgurations
freq =
Two of the detectors were already used by the search engine we
studied before this study. One is namely “Diff”, which simply
measures anomaly severities using the differences between the
current point and the point of last slot, the point of last day, and the
point of last week. The other one, namely “MA of diff”, measures
severities using the moving average of the difference between
current point and the point of last slot. This detector is designed
to discover continuous jitters. The other 12 detectors come from
previous literatures. Among these detectors, there are two variants
of detectors using MAD (Median Absolute Deviation) around the
median, instead of the standard deviation around the mean, to
measure anomaly severities. This patch can improve the robustness
to missing data and outliers [3, 15]. In the interest of space, the
details of these detectors and the ways they produce severities are
not introduced further, but can be found in the references in Table 3.
The sampled parameters of each detector are also shown in Table 3.
Here, the parameter of ARIMA is estimated from the data. For
other detectors, we sweep their parameter space.
In total, we have 14 detectors and 133 conﬁgurations, or 133
features for random forests. Note that, Opprentice is not limited to
the detectors we used, and can incorporate emerging detectors, as
long as they meet our detector requirements in §4.3.2.
5.3 Accuracy of Random Forests
First,
Now we present the evaluation results. First, we compare the
accuracy of random forests with other detection approaches in an
ofﬂine mode. Since we are not aware of the thresholds of other
approaches, we cannot compare speciﬁc recall and precision fairly.
Alternatively, we use the area under the PR curve (AUCPR) [50]
as the accuracy measure. The AUCPR is a single number summary
of the detection performance over all the possible thresholds. The
AUCPR ranges from 0 to 1. Intuitively, a detection approach with
a large AUCPR is more likely to achieve high recall and precision.
5.3.1 Random Forests vs. Basic Detectors and Static
Combinations of Basic Detectors
in Fig. 9, we would like to compare random forests
with the 14 basic detectors with different parameter settings (133
conﬁgurations) in Table. 3. We also compare random forests with
two static combination methods:
the normalization schema [21]
and the majority-vote [8]. These two methods are designed to
combine different detectors, but they treat them equally no matter
their accuracy. For comparison purposes, in this paper, we also use
these two methods to combine the 133 conﬁgurations as random
forests do. All the above approaches detect the data starting from
the 9th week. The ﬁrst 8 weeks are used as the initial training set
for random forests.
Table 4: Maximum precision when recall ≥ 0.66. The precision
greater than 0.66 is shown in bold. The top 3 basic detectors
are different for each KPI (see Fig. 9 for their names).
Detection approach
Random forest
Normalization scheme
Majority-vote
1st basic detector
2nd basic detector
3rd basic detector
Precision
#SR SRT
0.89
0.87
0.21
0.30
0.32
0.19
0.92
0.71
0.70
0.61
0.67
0.24
PV
0.83
0.11
0.12
0.67
0.39
0.37
Focusing on the left side of Fig. 9, we see that for the AUCPR,
random forests rank the ﬁrst in Fig. 9(a) and Fig. 9(b), and rank the
second in Fig. 9(c), where the AUCPR of random forests is only
0.01 less than the highest one. On the other hand, the AUCPR of
the two static combination methods is always ranked low. This
is because most conﬁgurations are inaccurate (having very low
AUCPR in Fig. 9), as we do not manually select proper detectors
or tune their parameters. However, the two static combination
methods treat all the conﬁgurations with the same priority (e.g.,
equally weighted vote). Thus, they can be signiﬁcantly impacted
by inaccurate conﬁgurations.
The right side of Fig. 9 shows the PR curves of random forests,
two combination methods, and the top 3 highest-AUCPR basic
(a) KPI: PV. The basic detector ranking ﬁrst in AUCPR is TSD
MAD (win = 5 weeks), the 2nd one is historical MAD (win = 3
weeks), and the third one is TSD (win = 5 weeks).
(b) KPI: #SR. The basic detector ranking ﬁrst in AUCPR is simple
threshold, the 2nd one is SVD (row = 50, column = 3), and the third
one is wavelet (win=3, freq=low).
Figure 10: AUCPR of different machine learning algorithms as
more features are used.
used. The result demonstrates that random forests are quite robust
to irrelevant and redundant features in practice.
5.4 Incremental Retraining
After demonstrating the accuracy and stability of random forests,
we want to show the effects of different training sets. We will
focus only on random forests in this and the following evaluation
steps. We compare three methods of generating training sets: I4,
F4, and R4 in Table 2. Fig. 11 shows the AUCPR of random forests
on different training sets. We see that I4 (also called incremental
retraining) outperforms the other two training sets in most cases.
This result is consistent with the challenge mentioned earlier that an
arbitrary data set is unlikely to contain enough kinds of anomalies.
In Fig. 11(b) we see that the three training sets result in similar
AUCPR. This implies that the anomaly types of #SR are relatively
simple and do not change much, so that they can be captured well
by any of these training sets. Overall, since labeling anomalies does
not cost much time (§5.7), we believe that incremental retraining is
a more general and accurate method to generate training sets.
Figure 11: AUCPR of different training sets.
(c) KPI: SRT. The basic detector ranking ﬁrst in AUCPR is SVD
(row = 20, column = 7), the 2nd one is TSD MAD (win = 3 weeks),
and the third one is TSD (win = 2 weeks).
Figure 9: The left side is the AUCPR rankings of different
detection approaches. The right side is the PR curves. Only
the top 3 highest AUCPR ranking conﬁgurations from instinct
detectors are shown in PR curves.
detectors. We observe that the best basic detectors are different
for each KPI, which indicates that the operators are interested
in different kinds of anomalies for each KPI. Table 4 shows the
maximum precision of these approaches when their recall satisﬁes
the operators’ preference (recall ≥ 0.66). We ﬁnd that for all the
KPIs, random forests achieve a high precision (greater than 0.8).
The result shows that random forests signiﬁcantly outperforms the
two static combination methods, and perform similarly to or even
better than the most accurate basic detector for each KPI.
logistic regression,
5.3.2 Random Forests vs. Other Algorithms
We also compare random forests with several other machine
learning algorithms: decision trees,
linear
support vector machines (SVMs), and naive Bayes. All these
algorithms are trained and tested on I1 in Table 2. To illustrate
the impact of irrelevant features (e.g., the conﬁgurations with low
AUCPR in Fig. 9) and redundant features (e.g., a detector with
similar parameter settings), we train these learning algorithms by
using one feature for the ﬁrst time, and adding one more feature
each time. The features are added in the order of their mutual
information [51], a common metric of feature selection. In Fig. 10,
we observe that, while the AUCPR of other learning algorithms
is unstable and decreased as more features are used, the AUCPR
of random forests is still high even when all the 133 features are
AUCPRrankingConfigurationsNormalizationschemaMajority-voteRandomforestRecall1st2nd3rdRandomforestMajority-VoteNormalizationSchemeAUCPRrankingConfigurationsNormalizationschemaMajority-voteRandomforestRecall1st2nd3rdRandomforestMajority-VoteNormalizationSchemeAUCPRrankingConfigurationsNormalizationschemaMajority-voteRandomforestRecall1st2nd3rdRandomforestMajority-VoteNormalizationSchemeAUCPRNumberoffeaturesusedfortrainingDecisontreesLinearSVMLogisticregressionNaiveBayesRandomforestsAUCPRNumberoffeaturesusedfortrainingAUCPRNumberoffeaturesusedfortrainingAUCPRIDof4-weekmovingtestsetsF4:first8-weekdataR4:recent8-weekdataI4:allhistoricaldataAUCPRIDof4-weekmovingtestsetsAUCPRIDof4-weekmovingtestsets(a) KPI: PV
(b) KPI: #SR
(c) KPI: SRT
Figure 12: Ofﬂine-fashion evaluation for the four accuracy metrics. In each sub-ﬁgure, the left side heat maps show the points of
(recall,precision) produced by the four accuracy metrics, respectively. Different rows show different kinds of operators’ preferences,
represented by the boxes at the top-right corner. The right side line charts depict the percentage of points inside the boxes as the
boxes scale up (starting from the original preferences, i.e., a scale ratio of 1).
5.5 PC-Score vs. Other Accuracy Metrics
So far, we have showed the ofﬂine AUCPR of random forests
without a speciﬁc cThld. Next, we evaluate different accuracy
metrics that are used to conﬁgure cThlds. We compare the PC-
Score we proposed against the default cThld, the F-Score, and
SD(1,1). Speciﬁcally, we train and test random forests on I1 in
Table 2, and let those four metrics determine the cThlds for each
one-week test set. Then, we measure their performance using recall
and precision of each week. Notice that, this evaluation considers
an ofﬂine or oracle setting where we assume we have the test set
when conﬁguring the cThld. We will show the cThld prediction for
future test set (online detection) in §5.6.
Fig. 12 shows the results of the four metrics for three KPIs,
respectively. In the left-side heat maps, each point represents the
recall and the precision of one week. The ﬁrst row shows the results
under the preference (recall≥0.66 and precision≥0.66), called a