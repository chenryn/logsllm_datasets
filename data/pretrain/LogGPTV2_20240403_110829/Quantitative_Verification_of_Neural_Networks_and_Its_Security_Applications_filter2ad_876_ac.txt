encode the cardinality constraints to CNF using a sorting-based
encoding (Section 5.2). We prove that NPAQ is preserving the equi-
witnessability in Theorem 5.5. Finally, we use an approximate model
counter that can handle model counting directly over a projected
subset of variables for a CNF formula [92].
5.1 BNN to Cardinality Constraints
: {−1, 1}n → {0, 1}s that consists
Consider a standard BNN fi
of d − 1 internal blocks and an output block [57]. We denote the
kth internal block as fblkk and the output block as fout . More
formally, given an input x ∈ {−1, 1}n, the binarized neural network
is: fi(x) = fout(fblkd−1(. . . (fblk1(x) . . .)). For every block fblkk , we
define the inputs to fblkk as the vector vk. We denote the output
for k block as the vector vk+1. For the output block, we use vd to
denote its input. The input to fblk1 is v1 = x. We summarize the
transformations for each block in Table 1.
Running Example. Consider a binarized neural net f : {−1, 1}3 →
{0, 1} with a single internal block and a single output (Figure 1). To
show how one can derive the constraints from the BNN’s parame-
ters, we work through the procedure to derive the constraint for v1
or the output of the internal block’s first neuron. Suppose we have
the following parameters: the weight column vector w1 = [1 1 1]
and bias b1 = −2.0 for the linear layer; α1 = 0.8, σ1 = 1.0, γ1 = 2.0,
µ1 = −0.37 parameters for the batch normalization layer. First, we
apply the linear layer transformation (Eq. 1 in Table 1). We create a
1 = ⟨x, w1⟩ +
temporary variable for this intermediate output, tlin
b1 = x1 + x2 + x3 − 2.0. Second, we apply the batch normalization
1 = (x1 +x2 +x3−2.0+0.37)·0.8+2.0 .
(Eq. 2 in Table 1) and obtain tbn
After the binarization (Eq. 3 in Table 1), we obtain the constraints
S1 = ((x1 + x2 + x3 − 2.0 + 0.37) · 0.8 + 2.0 ≥ 0) and S1 ⇔ v1 = 1.
Next, we move all the constants to the right side of the inequal-
ity: x1 + x2 + x3 ≥ −2.0/0.8 + 2.0 − 0.37 ⇔ v1 = 1. Lastly, we
translate the input from the {−1, 1} domain to the boolean domain,
(b)
i − 1, i ∈ {1, 2, 3}, resulting in the following constraint:
xi = 2x
(b)
(b)
(b)
2(x
3 ) − 3 ≥ −0.87. We use a sound approximation
2 + x
1 + x
for the constant on the right side to get rid of the real values and
(b)
(b)
(b)
3 ≥ ⌈1.065⌉ = 2. For notational simplic-
obtain x
1 + x
2 + x
ity the variables x1, x2, x3 in Figure 1 are boolean variables (since
x = 1 ⇔ x(b) = 1).
To place this in the context of the security application in Section 3,
we examine the effect of two arbitrary trojan attack procedures.
Their aim is to manipulate the output of a given neural network, f ,
to a target class for inputs with a particular trigger. Let us consider
the trigger to be x3 = 1 and the target class y = 0 for two trojaned
neural nets, f1 and f2 (shown in Figure 1). Initially, f outputs class
0 for only one input that has the trigger x3 = 1. The first obser-
vation is that f1 is equivalent to f , even though its parameters
have changed. The second observation is that f2 changes its output
prediction for the input x1 = 0, x2 = 1, x3 = 1 to the target class
0. We want NPAQ to find how much do f1 and f2 change their
predictions for the target class with respect to the inputs that have
the trigger, i.e., |R(φ1)|  0
⟨vk , wi⟩ ≥ − σki
αki
· γki
+ µki − bi , i = 1, ..., nk +1
ILPblk:
αki > 0
⟨vk , wi⟩ ≥ Ci ⇔ vk +1i
⟨vk , wi⟩ < Ci ⇔ vk +1i
· γki
Ci = ⌈− σki
αki
= 1, i = 1, ..., nk +1
= −1, i = 1, ..., nk +1
+ µki − bi⌉
= ⌈(Ci +nk
j=1 wji)/2⌉
v(b) = 2v − 1, v ∈ {−1, 1}
j∈w−
ki
(b) ≥ C′
i
+ |w−
ki
vkj
| ⇔ v
(b)
k +1i
= 1, C′
i
k +1) =
(b)
+
(b)
kj
v
j∈w +
ki
(b)
d , ord, y)
Cardblk:
BLKk(v
(b)
k , v
B. fout(vd , y) to OUT(v
ordij ∈ {0, 1}
Order:
i ≥ qlin
qlin
j ⇔ ordij = 1
MILPout:
Cardout:
OUT(v
(b)
d , ord, y) =
C. fi to BNN
BNN(x(b)
Eq(5), Eq(Order)
p∈w +
i ∩w−
j
⟨vd , wi − wj⟩ ≥ bj − bi ⇔ ordij = 1 ILPout: ⟨vd , wi − wj⟩ ≥ ⌈bj − bi⌉ ⇔ ordij = 1
(cid:17)
(cid:16)(cid:0)
v(b) = 2v − 1, v ∈ {−1, 1}
i =1 ordij = s ⇔ yi = 1
,
j
v
v
(b)
dp
(b)
dp
p∈w−
i ∩w +
−
p=1 wip −nd
Eij = ⌈(bj − bi +nd
(cid:16)
(b)) d−1
p=1 wjp)/2⌉
≥ ⌈Eij/2⌉(cid:1) ⇔ ordij ∧s
k +1)(cid:17) ∧ OUT(v
BLKk(v
(b)
k , v
, v2
(b)
(b)
d , y, ord)
, y, v
(b)
2 , . . . , v
(b)
d , ord) = BLK1(x(b)
k =2
x1
x3
p
m
o
C
-
2
2-Comp Clauses
x1 ⇒ y2
x3 ⇒ y2
x1 ∧ x3 ⇒ y1
y1
y2
x1
0
0
1
1
x3
0
1
0
1
y1
0
0/1
0/1
0/1
y2
0
0
0
0/1
2-Comp ∧ y2
0
0
0
1
Figure 2. Cardinality networks encoding for x1 + x3 ≥ 2. For
this case, cardinality networks amount to a 2-comparator
gate. Observe there are two satisfying assignments for
2-Comp ∧ y2 due to the “don’t care" assignment to y1.
Running Example. Revisiting our example in Section 5.1, con-
sider f2’s cardinality constraint corresponding to v1, denoted as
S′
1 = x1 + x3 ≥ 2. This constraint translates to the most basic gate
of cardinality networks, namely a 2-comparator [8, 12] shown in
Figure 2. Observe that while this efficient encoding ensures that
S1 is equi-satisfiable to the formula 2-Comp ∧ y2, counting over
the CNF formula does not preserve the count, i.e., it over-counts
due to variable y1. Observe, however, that this encoding is equi-
witnessable and thus, a projected model count on {x1, x3} gives
the correct model count of 1. The remaining constraints shown in
Figure 1 are encoded similarly and not shown here for brevity.
Lemma 5.3 (Substitution). Let F be a Boolean formula defined
over the variables Vars and p ∈ Vars. For all satisfying assignments
τ |= F ⇒ τ|Vars−{p} |= F[p (cid:55)→ τ[p]].
Lemma 5.4. For a given cardinality constraint, S(x) = x1 + . . . +
xn ≥ c, let Cardc be the CNF formula obtained using cardinality
networks, Cardc(x, aC) := (Sortc(x1, . . . , xn) = (y1, . . . , yc) ∧ yc),
where aC are the auxiliary variables introduced by the encoding. Then,
Cardc is equi-witnessable to S.
(a) ∀τ |= S ⇒ ∃σ , σ |= Cardc ∧ σ|x = τ .
(b) ∀σ |= Cardc ⇒ τ3|x |= S.
Proof.
(a) Let τ |= S ⇒ there are least c xi’s such that
τ[xi] = 1, i ≥ c. Thus, under the valuation τ1 to the in-
put variables x1, . . . , xn, the sorting network outputs a
sequence y1, . . . , yc where yc = 1, where y1 ≥ . . . ≥ yc
(Proposition 5.2). Therefore, Cardc[x (cid:55)→ τ] = (Sortc(x1 (cid:55)→
τ[x1], . . . , xn (cid:55)→ τ[xn]) = (y1, . . . , yc) ∧ yc) is satisfiable.
This implies that ∃σ , σ |= Cardc ∧ σ|x = τ.
(b) Let σ |= Cardc ⇒ σ[yc] = 1. By Lemma 5.3, σ|x |=
Cardc[yi
(cid:55)→ σ[yi]],∀yi ∈ aC. From Proposition 5.2, under the valu-
ation σ, there are at least c xi’s such that σ[xi] = 1, i ≥ c.
Therefore, σ|x |= S.
□
k =1
i =1 aki
nk +1
as C(x, y, a), where a = aVd
For every Ski , k = 1, . . . , d, i = 1, . . . , nk +1, we have a CNF
formula Cki . The final CNF formula for BNN(x, y, aV ) is denoted
C is the set of