action clause are matched, we consider that the action tree
in Figure 4a is a match, i.e., at least an attempted adversarial
transaction.
3.3.2 Result Clause Matching
We perform result clause matching by checking each node and
edge. Speciﬁcally, during action clause matching, we have
recorded all the node addresses and matches them with real-
world contracts. In this matching, we will conﬁrm that the
result graph also has corresponding nodes and edges. Here
is how it works in our airdrop hunting example. Since we
know that C1 to C50 are slave contracts, we will see whether
they have transferred all the tokens to another contract. In the
case of Figure 4b, all the slaves transfer tokens to the master,
which is C0. That is, the result clause matches the result graph
as well, which conﬁrms the adversarial transaction.
3.4 Defense Analysis Phase
Our defense analysis phase has two steps: (i) behavior-based
security check (i.e., the defense) identiﬁcation, and (ii) ex-
tended defense mining with similarity analysis. Let us start
from the ﬁrst step. Our observation here is that most smart
contracts implement defenses via Solidity functions that affect
control ﬂows [20], such as require, assert and revert, to
abort an execution if being attacked. That is, if an attempted
transaction fails to meet the conditions in these functions,
its trace returns directly with a Reverted error. Therefore,
we extract the control-ﬂow-related statements that cause the
failure as the security checks for the second step.
Second, we perform a backward dataﬂow analysis from the
security check to extract all the sources of the check. Then, we
use the security check and all the sources as the basis for the
similarity analysis. The insight here is that once two contracts
perform the same check on common input sources, they tend
to use the same defense tactic. Therefore, we extract such
a backward dataﬂow for all the open-source contracts and
compare the extracted dataﬂow with the one with a certain
defense. If both the security check and the sources match for
these two contracts, we will consider that the target contract
adopts the same defense.
We now look at a concrete example, i.e., the isHuman mod-
iﬁer in Figure 2. We ﬁrst extract the security check that leads
to a failed transaction, which is at Line 6. Then, we perform
a backward dataﬂow analysis to ﬁnd all the sources used in
the check, in this case, the return value of extcodesize()
at Line 5. Lastly, we ﬁnd similar contracts by searching for
USENIX Association
29th USENIX Security Symposium    2799
call withparameter pcall function fdetermines…call anytransfer……c0c1c1anytransferc0call ether/tokentransfercall………cic0cnc0ciether/tokentransferc0call token transferwith large parameter p…c0tokentransferc0createc1c1callc2callcncallc1callc0c2cnc1ether transferether transferether transferc0suicidec1call ethertransferc0c1ethertransferImplementation and Preliminary Results
the use of extcodesize() and the comparison of the return
value of extcodesize() with zero.
3.5 Evasion Analysis Phase
The purpose of our evasion analysis phase is to understand
whether existing defenses have been evaded by new attacks.
The analysis has two steps. First, we will analyze the con-
tracts with defenses found in our defense analysis and see
whether such contracts have conﬁrmed adversarial transac-
tions. Second, if these contracts have conﬁrmed adversarial
transactions, i.e., they are being penetrated regardless of the
defense, we will further conﬁrm and reason whether the ad-
versarial transactions have indeed bypassed the corresponding
security check adopted in the defense.
4 Implementation and Manual Analysis
In this section, we start from our implementation and pre-
liminary results produced from automatic analysis. Then, we
describe our manual efforts in reducing the false positives and
estimating the false negatives.
4.1
Our implementation of attack analysis is in 3,977 lines of
Python code. We apply our implementation on execution
traces from public service, particularly the Google BigQuery
traces [12], for analysis. Note that the traces obtained from
Google are the same as what we execute EVM in an archive
mode ourselves and we adopt Google’s traces to save execu-
tion time and storage space. The snapshot that we adopted has
1,063,473,983 rows of trace records of 420 million transac-
tions until March 2019. The preliminary results of our study
are shown in Table 2: Call injections affected the highest num-
ber of contracts and airdrop hunting has the highest number
of adversarial transactions.
4.2 Manual Analysis
In this part, we perform a manual analysis to ﬁlter false posi-
tives from our preliminary results and estimate false negatives
that are missed in our study.
4.2.1 Methodology and Metrics
Our methodology of manual analysis is as follows. We asked
three non-author domain experts to manually review whether
unique, non-duplicate contracts are vulnerable and then exe-
cute a selected number of unique adversarial transactions of
each contract. Domain experts are provided with collected
datasets and open-source tools [30,31,33,41] in the validation
of contracts. They can also inspect each candidate transaction,
e.g., whether transactions have triggered multiple successful
Distr or Airdrop events for the airdrop hunting case. One
thing worth noting is that domain experts cannot determine
whether 58 closed-source contracts with call injection attacks,
not included in Table 2, are vulnerable. Since the total ether
loss of those closed-source contracts is less than ten and the
total token loss is ignorable, we decide to exclude them from
our study.
We adopt three metrics in evaluating our manual analy-
sis, which are evaluation time, agreement rate and Fleiss’
kappa [14]. The ﬁrst is a standard evaluation of how long it
takes for human experts to perform all the work, the second is
the percentage of analyzed contracts that all three experts con-
sider as vulnerable, and the last a widely-accepted coefﬁcient
to measure inter-rater reliability for qualitative data. Three
experts take around 30 hours each to evaluate 1,272 contracts
and achieve 96.78% agreement rate and 96.47% Fleiss’ kappa.
Note that many ERC20 token contracts are similar to each
other, which greatly reduces human effort.
In 3.22% of cases there were disagreements among human
experts, and we asked them to discuss their labeling criteria. In
all cases they reached an agreement after the discussion. Here
is one example: An ERC 20 contract has an integer overﬂow
vulnerability, but adversarial transactions are targeting another
integer underﬂow vulnerability. One domain expert labels it
as true positive and the other two as false positive: After
discussion, they agreed on false positive for this example.
4.2.2 Manual Filtering of False Positives
We manually analyze all the transactions and contracts in our
preliminary results to ﬁnd and ﬁlter false positives. Table 2
shows the number and rate of false positives and also true
positives after ﬁltering. We mainly have false positives for
three attack categories, i.e., call injection, integer overﬂow,
and honeypot. Let us explain them separately. First, the false
positives of call injection come from the usage of on-chain
wallet library, where the library proxies sensitive function
calls, like ownership change and ether transfer, speciﬁed by
input data from wrapper contracts. Second, the false positives
of integer overﬂow are that some toy contracts multiply the
number of tokens they provide for fun, and therefore our study
mistakenly considers the large token transfer as an integer
overﬂow attack. Lastly, we incorrectly report some betting
and lottery contracts with only one winner as honeypots.
4.2.3 Manual Estimation of False Negatives
Because there is no ground truth, we have to create a bench-
mark and estimate the false negatives of our study. Particularly,
we contacted the authors of 11 prior works [23, 27–31, 33,
36, 39–41] on detecting smart contract vulnerabilities and ob-
tained eight replies and six datasets with vulnerable contracts
as shown in Table 4. We then sample contracts that are re-
ported by prior works but do not have adversarial transactions
reported by our work to estimate false negatives as shown in
Table 2. Note that we exclude 38.18% of the candidates with
only one creation transaction, which apparently is not adver-
sarial. We then ask our domain experts to go through all the
transactions of these contracts and estimate false negatives.
Table 3 shows the estimation of false negative. We only
have false negatives for 16 pseudo-bank honeypot contracts
2800    29th USENIX Security Symposium
USENIX Association
Table 2: A summary of vulnerable contracts and adversarial transactions before and after manual ﬁltering of false positives.
Vulnerability
call injection
reentrancy
integer overﬂow
airdrop hunting
call-after-destruct
honeypot
Total
Preliminary Results
False Positives (FPs)
True Positives (TPs) after Manual Filtering
# contract
# conﬁrmed atx # contract
# conﬁrmed atx
% contract
% atx
# contract
# conﬁrmed atx
# attempted atx
642
26
56
198
228
156
1,272
2,996
1,948
319
100,336
1,761
266
107,610
20
0
6
0
0
15
41
286
0
36
0
0
29
351
3.12%
9.55%
10.71%
11.29%
0
0
0
9.62%
10.90%
3.22%
0.33%
1,231
0
0
0
622
26
50
198
228
141
2,710
1,948
283
100,336
1,761
237
107,259
1,494
1,367
32
57
0
0
2,633
* atx denotes Adversarial Transactions.
Table 3: Manual estimation of false negatives.
False Negatives (FNs)
Evaluation Set
Vulnerability
# contract
# atx # contract
# atx
% contract
% atx
call injection
reentrancy
integer overﬂow
airdrop hunting
call-after-destruct
honeypot
Total
8
50
50
-
50
192
400
13
648
902
-
811
1,100
4,546
0
0
0
-
0
16
16
0
0
0
-
0
129
129
0
0
0
-
0
8.33%
0
0
0
-
0
11.73%
4.00%
2.84%
* atx denotes Adversarial Transactions; we leave the FN rates of airdrop hunting
as “-” because there are no prior works studying this vulnerability.
Table 4: Availability of related researches’ results.
Name
Reply?
Data?
Unique Contracts
Data Until
Oyente [31]
ZEUS [29]
Maian [33]
SmartCheck [39]
Securify [41]
ContractFuzzer [28]
Vandal [23]
MadMax [27]
teEther [30]
Sereum [36]
HoneyBadger [40]
Total











-











-
7,527
1,148
-
-
-
-
12,276
101,826
1,532
-
282
2016-05-05
2017-03-15
-
-
-
-
-
2017-03-04
2018-08-30
2017-11-30
2018-10-12