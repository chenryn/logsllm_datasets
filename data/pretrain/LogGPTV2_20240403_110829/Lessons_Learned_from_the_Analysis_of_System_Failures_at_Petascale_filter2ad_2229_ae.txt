m
h
t
i
r
a
0
1
8
6
4
2
]
h
]
h
[
s
e
r
u
[
s
e
r
u
l
i
l
i
a
a
f
f
t
f
e
r
a
w
e
r
a
w
d
r
a
h
–
F
B
T
n
a
e
m
c
i
t
o
s
-
F
B
T
n
a
e
m
c
i
t
e
m
h
e
m
t
i
r
a
t
i
r
a
0
4
5
3
0
3
5
2
0
2
5
1
s
e
r
u
l
i
a
f
e
r
a
w
d
r
a
H
-
t
s
e
T
e
c
a
p
a
L
l
0
5
-
0
1
-
s
e
r
u
l
i
a
f
e
r
a
w
t
f
o
S
-
t
s
e
T
e
c
a
p
a
L
l
0
4
0
2
0
0
2
-
Mar
May
Jul
Date
(a)
Sep
Nov
Mar
May
Jul
Date
(b)
Sep
Nov
Mar
May
Jul
Sep
Nov
Mar
May
Jul
Sep
Nov
Dates
(c)
Dates
(d)
Fig. 4: Arithmetic mean of the TBF for failures due to (a) hardware, (b) software root causes. Laplace Test for the number of nodes failing per day because of
(c) hardware, (d) software root causes.
to 10.9 h. Figure 4.(c) shows the Laplace score computed over
the examined period. In particular, the ﬁgure conﬁrms that in
the ﬁrst months, the hardware reliability was not stable (the
score oscillates around zero). From July on, the reliability of the
system improved (shown by a Laplace score lower than –1.96),
because permanent and non-independent failures had been ﬁxed
so that most of the remaining failures were those that occurred
with independent inter-failure times. The implication is that
over those few months, Blue Waters’ hardware conﬁguration
matured, going towards the ﬂat part of the bathtub curve. It is
interesting to note that Figure 4.(c) shows a peak around July.
During that time the system was suspended for replacement of
several blades; the new hardware added to the system caused a
transient upward trend in the Laplace score showing that new
defects were ﬁxed soon after the installation of new hardware.
The reliability of the software deteriorated over the mea-
sured months. Figure 4.(b) shows that in the ﬁrst months, the
TBF of failures due to software improved until June, while 4.(d)
shows that the number of nodes that failed per day decreased,
despite being highly variable (scattered points in the chart).
In June–August the TBF decreased sharply (by a factor of
about 2.6), from about 42 h to 16 h. During that time, material
changes were made to the Blue Waters stack/environment that
perturbed the downstream reliability. Examples include major
software upgrades, environmental and Gemini network changes.
In particular, after a signiﬁcant upgrade to the Blue Waters
system software in July 2013 we noticed that there was a dip in
the system reliability as a set of new (previously undiscovered)
Lustre issues were excited. Some of the issues were attributed to
Lustre (particularly in the area of failovers) and, as described
in the reports, were partially caused by the maturity of the
product of this scale as well as by Sonexion platform software
and ﬁrmware defects. As Figure 4.(d) shows, after the major
change in July the system experienced more failures, which
were followed by a period of improved stability (decrease after
the peak). Blue Waters is a conﬁguration corner case at scales
well beyond those at which Lustre is methodically tested and,
hence, the ﬁndings provided in this study have a signiﬁcant
value. At the end of the observation period, the Laplace test
shows a decreasing trend in reliability, while the TBF settles
at around 16 h, showing that hardware and software follow
different
lifecycles in Blue Waters. The key implication is
that hardware becomes mature (ﬂat part of the bathtub curve)
earlier than software. Software failure rate curves do not drop
TABLE IX: System-wide outage statistics
Availability
Total Time
Unscheduled Downtime
MTBF (SWO)
MIN TBF
MAX TBF
MTTR
Min TTR
Max TTR
TBF
0.9688
261 days
8.375 days
6.625 days
3.35 h
37 days
5.12 h
0.58 h
12 h
TBF
TBF
y
t
i
s
n
e
D
6
0
0
.
0
4
0
0
.
0
2
0
0
.
0
0
0
0
.
0
TBF
Weibull
Lognormal
Exponential
)
x
(
n
F
)
x
(
n
F
0
0
.
.
1
1
8
8
.
.
0
0
6
6
.
.
0
0
4
4
.
.
0
0
2
2
.
.
0
0
0
0
.
.
0
0
TBF
TBF
Weibull
Weibull
Lognormal
Lognormal
Exponential
Exponential
0
100
200
300
400
500
600
0
0
100
100
200
200
300
300
400
400
500
500
600
600
TBF[h]
(a)
.
TBF[h]
TBF[h]
(b)
Fig. 5: Distribution ﬁtting for time between SWOs: (a) PDF, (b) CDF
over time after new releases or patches (peaks in Figure 4).
We speculate that in the long run, software will show MTBF
values similar to these of the hardware, with the important
difference that failures due to software causes can impact a
larger number of nodes (e.g., ﬁle system software stack), as
discussed in Section IV. As further detailed in the next section,
one implication is that software is and will be the real problem
at the scale of Blue Waters and above, further supporting our
claim that more research is needed to improve the way large-
scale software systems are tested.
VI. CHARACTERIZATION OF SYSTEM-WIDE OUTAGES