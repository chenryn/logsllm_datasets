### Characterization of System-Wide Outages

**Figure 4: Arithmetic Mean of Time Between Failures (TBF) for Hardware and Software Root Causes**

- **(a) Hardware Root Causes:**
  - The TBF for hardware failures increased from approximately 10.9 hours.
  - Figure 4(c) shows the Laplace score over the examined period, indicating that in the initial months, hardware reliability was unstable (Laplace score oscillating around zero).
  - From July onwards, the system's hardware reliability improved, as shown by a Laplace score consistently below -1.96. This improvement suggests that permanent and non-independent failures were addressed, leaving mostly independent inter-failure times.
  - A peak in the Laplace score around July is attributed to the suspension of the system for blade replacements. The addition of new hardware temporarily caused an upward trend in the Laplace score, indicating that new defects were quickly resolved.

- **(b) Software Root Causes:**
  - The TBF for software failures initially improved until June, but then sharply decreased from about 42 hours to 16 hours between June and August.
  - During this period, significant changes were made to the Blue Waters stack/environment, including major software upgrades, environmental adjustments, and Gemini network changes.
  - In July 2013, a significant upgrade to the Blue Waters system software led to a dip in system reliability, primarily due to newly discovered Lustre issues. These issues were partly attributed to the maturity of the product and defects in the Sonexion platform software and firmware.
  - Figure 4(d) shows that after the major change in July, the system experienced more failures, followed by a period of improved stability.

**Key Implications:**
- Hardware reliability matures earlier than software, reaching a stable phase (flat part of the bathtub curve) sooner.
- Software failure rates do not decline as rapidly and can impact a larger number of nodes, particularly in the file system software stack.
- Over time, software will likely show MTBF values similar to those of hardware, but with a higher potential for widespread impact.
- The findings suggest that software will be a critical issue at the scale of Blue Waters and beyond, underscoring the need for more research into testing large-scale software systems.

**Table IX: System-Wide Outage Statistics**

| Metric              | Value         |
|---------------------|---------------|
| Availability         | 0.9688        |
| Total Time          | 261 days      |
| Unscheduled Downtime| 8.375 days    |
| MTBF (SWO)          | 6.625 days    |
| Min TBF             | 3.35 h        |
| Max TBF             | 37 days       |
| MTTR                | 5.12 h        |
| Min TTR             | 0.58 h        |
| Max TTR             | 12 h          |

**Figure 5: Distribution Fitting for Time Between System-Wide Outages (SWOs)**

- **(a) Probability Density Function (PDF)**
- **(b) Cumulative Distribution Function (CDF)**

The distribution fitting for the time between SWOs is shown in Figure 5, with the PDF and CDF for Weibull, Lognormal, and Exponential distributions. The data indicates that software will continue to be a significant challenge, with peaks in the TBF corresponding to new releases or patches. In the long run, software MTBF values are expected to align with those of hardware, but with a higher risk of impacting a larger number of nodes.

**Conclusion:**
- The analysis highlights the different lifecycles of hardware and software in the Blue Waters system.
- Hardware reaches a mature and stable state earlier, while software continues to experience fluctuations in reliability.
- Future research should focus on improving the testing and reliability of large-scale software systems to address these challenges.