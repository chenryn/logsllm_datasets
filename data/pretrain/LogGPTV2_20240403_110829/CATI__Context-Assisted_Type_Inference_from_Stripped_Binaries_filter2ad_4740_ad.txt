0.82
0.83
0.80
0.82
0.82
0.79
TABLE IV
–
–
–
inetutils
0.89
0.89
0.89
0.70
0.71
0.70
0.77
0.76
0.74
0.88
0.83
0.85
1.00
1.00
1.00
0.76
0.74
0.75
less
0.86
0.86
0.86
0.79
0.70
0.71
0.93
0.92
0.92
0.94
0.86
0.88
1.00
1.00
1.00
0.94
0.81
0.86
nano
0.87
0.87
0.87
0.79
0.79
0.78
0.87
0.86
0.86
0.79
0.79
0.79
–
–
–
0.71
0.70
0.70
R
0.89
0.89
0.89
0.69
0.70
0.68
0.87
0.88
0.88
0.92
0.88
0.89
0.99
0.99
0.99
0.84
0.84
0.84
sed
0.91
0.91
0.91
0.89
0.89
0.89
0.89
0.88
0.88
0.84
0.80
0.81
–
–
–
0.75
0.72
0.72
wget
0.89
0.89
0.89
0.76
0.73
0.73
0.83
0.84
0.83
0.81
0.81
0.81
1.00
1.00
1.00
0.74
0.72
0.72
inetutils
0.94
0.94
0.94
0.73
0.70
0.69
0.89
0.89
0.89
0.90
0.87
0.89
1.00
1.00
1.00
0.80
0.80
0.80
less
0.88
0.87
0.87
0.79
0.65
0.66
0.96
0.95
0.95
0.97
0.88
0.91
1.00
1.00
1.00
0.94
0.86
0.89
nano
0.89
0.87
0.87
0.79
0.77
0.75
0.90
0.89
0.89
0.80
0.82
0.80
–
–
–
0.79
0.78
0.77
R
0.92
0.92
0.92
0.74
0.70
0.68
0.89
0.90
0.90
0.95
0.94
0.95
0.99
0.99
0.99
0.83
0.87
0.84
sed
0.93
0.93
0.93
0.89
0.88
0.88
0.93
0.93
0.93
0.86
0.81
0.81
–
–
–
0.83
0.81
0.81
wget
0.92
0.92
0.92
0.75
0.67
0.66
0.89
0.89
0.89
0.81
0.81
0.80
1.00
1.00
1.00
0.80
0.78
0.77
VUC PREDICTION RESULT OF 12 APPLICATIONS IN 6 STAGES MEASURED BY PRECISION(P), RECALL(R) AND F-1 SCORE(F1).
VARIABLE PREDICTION RESULT OF 12 APPLICATIONS IN 6 STAGES AFTER VOTING MEASURED BY PRECISION(P), RECALL(R) AND F-1 SCORE(F1).
is that we want to focus on studying one compiler’s behavior.
Meanwhile, we believe that our prototype can transfer easily
to other compilers. To validate our idea, we do the additional
experiments on Clang which will be discussed detailly in
SectionVIII. With the help of IDA pro [12] and DWARF [26],
we successfully disassemble the binary program, leverage the
debug information to label the ground truth of each VUC, and
group VUCs that belong to the same variable. Furthermore, to
test the prediction accuracy of CATI and compare with former
works, we carefully select some applications as a benchmark
which are different from the training set to prove the general
performance of our method.
Metrics. CATI is a machine learning-based method, so we
use three performance metrics commonly used to evaluate
machine learning classiﬁers: precision (P), recall (R) and F1
score. Formally, they are deﬁned as follows:
P =
T P
T P + F P
, R =
T P
T P + F N
, F 1 =
2 ∗ P ∗ R
P + R
where TP is the true positives, FP is the false positives, FN
is the false negatives. Precision is the ratio of cases where
the predicted value is equal to the given value, which is the
closeness of the measurements to each class(i.e., the accuracy
ratio of discovered variables). Recall leads to the proportion
of correct predictions over the set of their class. F1 score is a
balance measurement that is calculated by precision and recall.
All three metrics are in the range of 0 to 1.
B. Evaluation
To objectively measure the performance of our type infer-
ence method, we assume the variable location of assembly
code is given for every binary. However, in general, we can
leverage the V ariable Recovery part of DEBIN [1] to locate
the variable operations in assembly code whose accuracy can
achieve about 90%.
Evaluation on Test Set. Firstly, we discuss the performance
of the multi-stage classiﬁer on predicting the most likely type
of each VUC. The result of 12 different applications has shown
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:25:44 UTC from IEEE Xplore.  Restrictions apply. 
(cid:26)(cid:21)
Type
bool
struct
char
unsigned char
ﬂoat
double
long double
enum
int
short int
long int
long long int
unsigned int
short unsigned int
long unsigned int
long long unsigned int
void*
struct*
S1-R
1.00
0.91
0.99
1.00
1.00
0.99
1.00
0.99
0.99
1.00
0.71
0.57
0.99
1.00
0.61
0.71
0.91
0.95
S2-R
0.76
0.61
0.50
0.76
0.88
0.91
0.98
0.99
0.98
0.78
0.97
1.00
0.97
0.80
0.96
0.81
0.18
0.92
S3-R
1.00
1.00
0.93
0.65
0.88
1.00