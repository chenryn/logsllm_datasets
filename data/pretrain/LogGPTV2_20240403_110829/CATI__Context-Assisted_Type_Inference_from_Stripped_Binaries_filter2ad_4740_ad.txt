### Table IV: VUC Prediction Results for 12 Applications in 6 Stages Measured by Precision (P), Recall (R), and F-1 Score (F1)

| Application | Stage 1 | Stage 2 | Stage 3 | Stage 4 | Stage 5 | Stage 6 |
|-------------|---------|---------|---------|---------|---------|---------|
| inetutils   | 0.89    | 0.89    | 0.89    | 0.70    | 0.71    | 0.70    |
|             | 0.77    | 0.76    | 0.74    | 0.88    | 0.83    | 0.85    |
|             | 1.00    | 1.00    | 1.00    | 0.76    | 0.74    | 0.75    |
| less        | 0.86    | 0.86    | 0.86    | 0.79    | 0.70    | 0.71    |
|             | 0.93    | 0.92    | 0.92    | 0.94    | 0.86    | 0.88    |
|             | 1.00    | 1.00    | 1.00    | 0.94    | 0.81    | 0.86    |
| nano        | 0.87    | 0.87    | 0.87    | 0.79    | 0.79    | 0.78    |
|             | 0.87    | 0.86    | 0.86    | 0.79    | 0.79    | 0.79    |
|             | -       | -       | -       | 0.71    | 0.70    | 0.70    |
| R           | 0.89    | 0.89    | 0.89    | 0.69    | 0.70    | 0.68    |
|             | 0.87    | 0.88    | 0.88    | 0.92    | 0.88    | 0.89    |
|             | 0.99    | 0.99    | 0.99    | 0.84    | 0.84    | 0.84    |
| sed         | 0.91    | 0.91    | 0.91    | 0.89    | 0.89    | 0.89    |
|             | 0.89    | 0.88    | 0.88    | 0.84    | 0.80    | 0.81    |
|             | -       | -       | -       | 0.75    | 0.72    | 0.72    |
| wget        | 0.89    | 0.89    | 0.89    | 0.76    | 0.73    | 0.73    |
|             | 0.83    | 0.84    | 0.83    | 0.81    | 0.81    | 0.81    |
|             | 1.00    | 1.00    | 1.00    | 0.74    | 0.72    | 0.72    |

### Variable Prediction Results of 12 Applications in 6 Stages After Voting, Measured by Precision (P), Recall (R), and F-1 Score (F1)

| Application | Stage 1 | Stage 2 | Stage 3 | Stage 4 | Stage 5 | Stage 6 |
|-------------|---------|---------|---------|---------|---------|---------|
| inetutils   | 0.94    | 0.94    | 0.94    | 0.73    | 0.70    | 0.69    |
|             | 0.89    | 0.89    | 0.89    | 0.90    | 0.87    | 0.89    |
|             | 1.00    | 1.00    | 1.00    | 0.80    | 0.80    | 0.80    |
| less        | 0.88    | 0.87    | 0.87    | 0.79    | 0.65    | 0.66    |
|             | 0.96    | 0.95    | 0.95    | 0.97    | 0.88    | 0.91    |
|             | 1.00    | 1.00    | 1.00    | 0.94    | 0.86    | 0.89    |
| nano        | 0.89    | 0.87    | 0.87    | 0.79    | 0.77    | 0.75    |
|             | 0.90    | 0.89    | 0.89    | 0.80    | 0.82    | 0.80    |
|             | -       | -       | -       | 0.79    | 0.78    | 0.77    |
| R           | 0.92    | 0.92    | 0.92    | 0.74    | 0.70    | 0.68    |
|             | 0.89    | 0.90    | 0.90    | 0.95    | 0.94    | 0.95    |
|             | 0.99    | 0.99    | 0.99    | 0.83    | 0.87    | 0.84    |
| sed         | 0.93    | 0.93    | 0.93    | 0.89    | 0.88    | 0.88    |
|             | 0.93    | 0.93    | 0.93    | 0.86    | 0.81    | 0.81    |
|             | -       | -       | -       | 0.83    | 0.81    | 0.81    |
| wget        | 0.92    | 0.92    | 0.92    | 0.75    | 0.67    | 0.66    |
|             | 0.89    | 0.89    | 0.89    | 0.81    | 0.81    | 0.80    |
|             | 1.00    | 1.00    | 1.00    | 0.80    | 0.78    | 0.77    |

### Metrics

CATI is a machine learning-based method, and we use three common performance metrics to evaluate it: precision (P), recall (R), and F1 score. These metrics are defined as follows:

- **Precision (P)**: 
  \[
  P = \frac{TP}{TP + FP}
  \]
  where \( TP \) is the number of true positives and \( FP \) is the number of false positives. Precision measures the accuracy of the positive predictions.

- **Recall (R)**:
  \[
  R = \frac{TP}{TP + FN}
  \]
  where \( FN \) is the number of false negatives. Recall measures the proportion of actual positives that are correctly identified.

- **F1 Score**:
  \[
  F1 = \frac{2 \times P \times R}{P + R}
  \]
  The F1 score is the harmonic mean of precision and recall, providing a balanced measure of the two.

All three metrics range from 0 to 1, with higher values indicating better performance.

### Evaluation

To objectively measure the performance of our type inference method, we assume the variable location in assembly code is given for every binary. However, in general, we can use the Variable Recovery part of DEBIN [1] to locate the variable operations in assembly code, which achieves about 90% accuracy.

#### Evaluation on Test Set

We first discuss the performance of the multi-stage classifier in predicting the most likely type of each VUC. The results for 12 different applications across six stages are shown in Table IV. 

### Type Inference Results

| Type                  | S1-R | S2-R | S3-R |
|-----------------------|------|------|------|
| bool                  | 1.00 | 0.76 | 1.00 |
| struct                | 0.91 | 0.61 | 1.00 |
| char                  | 0.99 | 0.50 | 0.93 |
| unsigned char         | 1.00 | 0.76 | 0.65 |
| float                 | 1.00 | 0.88 | 0.88 |
| double                | 0.99 | 0.91 | 1.00 |
| long double           | 1.00 | 0.98 | 1.00 |
| enum                  | 0.99 | 0.99 | 1.00 |
| int                   | 0.99 | 0.98 | 0.98 |
| short int             | 1.00 | 0.78 | 1.00 |
| long int              | 0.71 | 0.97 | 1.00 |
| long long int         | 0.57 | 1.00 | 1.00 |
| unsigned int          | 0.99 | 0.97 | 0.97 |
| short unsigned int    | 1.00 | 0.80 | 1.00 |
| long unsigned int     | 0.61 | 0.96 | 0.96 |
| long long unsigned int| 0.71 | 0.81 | 0.81 |
| void*                 | 0.91 | 0.18 | 0.95 |
| struct*               | 0.95 | 0.92 | 0.92 |

In this study, we focus on one compiler's behavior, but we believe our prototype can be easily transferred to other compilers. To validate this, we conducted additional experiments on Clang, which will be detailed in Section VIII. With the help of IDA Pro [12] and DWARF [26], we successfully disassembled the binary program, used debug information to label the ground truth of each VUC, and grouped VUCs belonging to the same variable. To test the prediction accuracy of CATI and compare it with previous works, we carefully selected some applications as a benchmark, different from the training set, to demonstrate the general performance of our method.