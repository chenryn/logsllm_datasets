may be reordered, confusing the receiving peer. Third,
some handshake messages are too large to ﬁt in a sin-
gle DTLS record and therefore must be fragmented across
multiple records. The DTLS handshake layer is responsi-
ble for reassembling these records into a coherent stream
of complete handshake messages. This necessitates the
addition of retransmission as well as a more complicated
message format.
4.4. Timeout and Retransmission
Because DTLS handshake messages may be lost,
DTLS needs a mechanism for retransmission. DTLS im-
plements retransmission using a single timer at each end-
point. Each end-point keeps retransmitting its last mes-
sage until a reply is received. The state machine that im-
plements the timer and resulting retransmissions is shown
in Figure 5. In the balance of this section, we describe
the operation of the timer state machine and explain how
timer expiry values are picked.
State Machine Once in the Read Message Fragment
state, transitions are triggered by the arrival of data frag-
ments or the expiry of the retransmission timer. If a data
INIT, reset
timer
Reset
Timer
Retransmit
Yes
Timer
expired
Want
message?
Read
message
fragment
No
Expected
fragment?
Fragment
received
No
Done
Return fragment
Yes
Reset
timer
Fragment
received
No
Yes
Finished
message?
Retransmit
Finished
Figure 5. Timer state machine
fragment is the expected next handshake message then the
fragment is returned to the higher layers and the timer
is cancelled. Otherwise, the fragment is buffered or dis-
carded as appropriate and the timer is allowed to continue
ticking. When the retransmit timer expires, the implemen-
tation retransmits the last ﬂight of messages that it trans-
mitted.
Timer Values Picking appropriate timer values is a dif-
ﬁcult problem due to the heterogeneous nature of the In-
ternet and the wide variance in round trip times (RTT).
While estimating RTT would allow for estimating a timer
value, requiring that DTLS estimate RTT is an unneces-
sary burden, given the simplicity of the handshake pro-
tocol. Deciding on the exact timer value is especially
tricky because the peer is often doing some kind of cryp-
tographic computation, which can take a substantial frac-
tion of the RTT. Thus, one wishes to set one’s timer values
conservatively to avoid unnecessary retransmissions.
We recommend that DTLS implementations use timer
values between 500 to 1000ms. In general, well-behaving
implementations should back off their retransmission
timers.
Are ACKs necessary? When a retransmission event
happens, the entire ﬂight of un-answered messages is re-
transmitted. If that ﬂight is large, like a Certiﬁcate mes-
sage, a nontrivial amount of network bandwidth (though
probably less than 5k) is wasted. In addition, the desire
to avoid unnecessary retransmission motivates large timer
values which result in high latency. An alternative strategy
would be to allow receivers to transmit an ACK value that
indicated that they have received the message and were
processing it. This would allow timers to be set lower
as well as reducing the number of packets that have to
be retransmitted (since the sender would know that some
had already been received.) In the interest of simplicity,
we decided not to add an ACK feature to DTLS, but fu-
ture measurement may indicate that ACKs provide a large
enough improvement to be worthwhile adding.
4.5. Handshake Message Ordering and Fragmen-
tation
Because handshake messages may be too large to ﬁt into
a single DTLS record, we need to modify the handshake
messages to be able to span records. The new format is
shown below.
struct {
HandshakeType msg_type;
uint24 length;
uint16 message_seq;
uint24 frag_offset;
uint24 frag_length;
HandshakeMessage msg_frag[frag_length];
} Handshake;
Message Length The handshake message header con-
tains the overall message length. This makes it easy to
allocate buffer space for the message regardless of which
fragment is received ﬁrst.
Message Sequence Number Handshake (and Change
Cipher Spec) messages include their own message se-
quence numbers (MSN), independent of record sequence
numbers (RSN). Since the record layer assigns unique se-
quence numbers to each record, it is possible that a DTLS
end-point receives a handshake message and its retrans-
mitted version under different RSNs. In the absence of the
MSN, it is not possible for the handshake layer to detect
duplicates. All fragments of a handshake message carry
the same MSN.
It is worthwhile considering whether retransmits can
reuse the original RSN, and hence make do without the
MSN. As it turns out, there are two problems with reusing
RSNs. First, it is a layering violation: the handshake layer
is a client of the record layer, just like the application
layer, and should not receive different treatment. Second,
the original handshake message may have been dropped
due to the packet size exceeding PMTU. In this case the
handshake message needs to be fragmented, which im-
plies that it spans multiple records, each with their own
unique RSN.
Fragment Offset and Length As previously men-
tioned, handshake messages may be fragmented when
they are larger than PMTU. In fact such fragmentation
is fairly likely since certiﬁcates can easily be a couple of
kilobytes in size. We chose to use fragment offset and
length rather than fragment sequence numbers to aid in
handling messages which are fragmented twice in two dif-
ferent ways. With this scheme, it is easy to reassemble the
original message provided at least one copy of each byte
is received.
Finished Message The purpose of Finished messages is
to verify that parties have correctly negotiated keys and
algorithms. In TLS, the Finished message contains MD5
and SHA1 hashes of all the handshake messages, sequen-
tially appended to each other (including their message
headers). The DTLS algorithm for computing ﬁnished
hashes has to be slightly different due to the presence of
message fragmentation headers. Since the message might
have been fragmented multiple times with different frag-
ment sizes, this creates a potential inconsistency. In or-
der to remove this inconsistency, the handshake hashes are
computed as if handshake messages had been received as
a single fragment.
Alert Messages DTLS reuses all of the TLS alerts.
Most TLS alerts signal the end of the connection–either
graceful or abortive–and therefore no data should come
after them. Under no circumstances should a record be
accepted with a sequence number postdating that of an
alert which closed the connection.
There is, however, a complication introduced by a
sender transmitting data followed by an alert but have
them arrive in the reverse order. We have not analyzed
this situation, but believe that it is safer for implementa-
tions to reject such data records.
5. Security Analysis
Considering the complexity of modern security pro-
tocols and the current state of proof techniques, it is
rarely possible to completely prove the security of a proto-
col without making at least some unrealistic assumptions
about the attack model.
Instead of attempting to rigorously prove the security of
DTLS, one of our main goals in the design of DTLS is
to follow the TLS speciﬁcation as closely as possible. As
a result, DTLS does not offer any “improvements” over
TLS. All the features introduced into DTLS are for the
sole purpose of dealing with unreliable datagram trans-
port.
We argue that DTLS does not reveal any additional
information beyond TLS during the handshake or bulk
transfer phase—all the additional information in a DTLS
stream can be derived by passively monitoring a TLS
stream. To justify this argument, consider the additional
information that is available from a DTLS stream.
Record Layer The DTLS record layer reveals the cur-
rent epoch and sequence number. This is public informa-
tion to an adversary monitoring a TLS session:
the se-
quence numbers are implicit in TLS, but nonetheless may
be inferred, and epoch numbers may also be derived from
the stream since session renegotiations may be detected
(by observing Handshake records being exchanged during
an established session.)
Handshake Layer Handshake messages reveal mes-
sage number, fragment length and fragment offset. Once
again, this information is easily derived by an eavesdrop-
per monitoring a TLS session. Message number is ob-
tained by counting exchanged messages, fragment length
is obtained from record length and fragment offset is de-
rived from the length of preceding message fragments.
Only the Finished message is encrypted during the ini-
tial handshake phase, and since it is of a ﬁxed format, its
fragment length and offset are obvious.
Handshake messages exchanged due to session renego-
tiation are completely encrypted in both DTLS and TLS.
Timing information Recently, timing information has
been used as the basis for attacks on TLS [4][5]. Therefore
it is critical to consider what information is revealed by
timing.
DTLS receive record processing is essentially the same
as that of TLS. On reception, records and handshake mes-
sages are not processed until available in entirety, and
therefore the processing of DTLS records and messages
is identical to the processing procedure of TLS.
DTLS transmit processing leaks a small amount of tim-
ing information when compared to TLS. In general, when
applications issue TLS or DTLS writes, this causes a sin-
gle DTLS/TLS record to be generated. The time when
the packet is delivered to the network potentially reveals
information about the plaintext [29]. With TLS, TCP con-
gestion and ﬂow control hides this information to some
extent, especially if the Nagle algorithm [24] is used. With
DTLS, however, records are likely to be transmitted as
soon as they are generated. Users who wish to prevent
this kind of trafﬁc analysis should buffer writes.
Implementation We implemented DTLS based on the
OpenSSL toolkit and reuse much of the code already used
in production TLS servers. As a result, DTLS inherits well
tested and stable code.
6. Implementation
We implemented DTLS based on the popular OpenSSL
library [30]. OpenSSL is the de facto standard open source
TLS/SSL implementation. Additionally, OpenSSL has
proven to be stable and is used by numerous production
quality servers such as the Apache Web server.
We modiﬁed the demo server and client applications
that are part of the OpenSSL distribution to be UDP capa-
ble. We also implemented a UDP proxy application that
is capable of dropping, delaying and duplicating packets.
Results from our experiments are listed in Section 8. Our
implementation was tested and run on the Linux 2.4.21
kernel.
Our implementation required adding about 7000 lines
of additional code to the OpenSSL base distribution.
Considering that this line count includes libraries, data
structures and socket management needed for DTLS, our
code makes up only a small portion of the 240,000 line
OpenSSL package. Conveniently, we were able to lever-
age a number of OpenSSL features that were designed
for different use. For example, OpenSSL provides an I/O
buffering layer that causes TLS to only make send()
system calls when it has serialized all data to be sent on
a particular round of the handshake. We are able to reuse
the buffering code to maximize handshake packet payload
size.
In the remainder of this section we describe some de-
tails of our implementation.
OpenSSL Architecture OpenSSL implements SSLv2,
SSLv3 and TLSv1. Each of these protocols are imple-
mented by sharing as much code as possible, with vir-
tual functions handling protocol differences. From the li-
brary’s standpoint, DTLS appears to be another version of
the TLS protocol.
As a result of
implementing DTLS in this way,
we can reuse much of the utility, state machine and
record/message generation code of OpenSSL. In a num-
ber of cases we found it was inconvenient to write spe-
cial cases into TLS processing code, and as a result we
copied many functions and modiﬁed them appropriately.
Roughly 60% of the 7000 lines of additional code were
actually copied from the other protocol implementations
in OpenSSL. With some effort it should be possible to re-
duce the amount of duplicated code substantially.
One of the nice side effects of implementing DTLS
this way is that DTLS can be accessed through the same
functions used by TLS, for example SSL_connect(),
SSL_read(), SSL_write() and, SSL_close().
Below we describe some issues encountered in our im-
plementation.
PMTU Path Maximum Transmission Unit (PMTU) is
the maximum sized packet that can travel on a path with-
out requiring fragmentation. In general, paths consist of
heterogeneous networks that have links with varying lim-
its on maximum packet size. Therefore the PMTU for a
given path is set by the limiting link on the path. Previous
work [15] shows that fragmentation is undesirable. Frag-
mentation results in inefﬁcient use of network and rout-
ing resources, and lost fragments cause degraded perfor-
mance. Additionally, IP fragments interact poorly with
ﬁrewalls and NAT devices, which often discard fragments.
Therefore it is useful to know the PMTU.
RFC 1191 [22] speciﬁes the process by which PMTU is
discovered. In short, hosts send out IP packets with the DF
(Don’t Fragment) bit set, iteratively reducing the size of
packets until the host is reached. Therefore, it is difﬁcult
for the kernel to know a priori what the appropriate PMTU
is without incurring a signiﬁcant probing cost–though it
can guess it after enough trafﬁc has been transmitted. In
general, kernel support for PMTU is quite poor. On the
Linux system, where we developed our implementation,
the kernel keeps track of its PMTU estimate and returns
an error if an application attempts to send a larger packet.
DTLS needs to be agnostic about such kernel behav-
ior so as to not get caught using an excessive PMTU
value. Unless an application explicitly sets a PMTU
value we turn on the DF bit in outgoing datagrams via
setsockopt() and query the kernel for the MTU. If
the PMTU is unavailable, we guess a PMTU starting
with 1500 (the ethernet MTU), successively reducing the
PMTU estimate if the current setting happens to be too
large. We can detect that PMTU has been exceeded if
send() returns -1 and sets errno to EMSGSIZE.
On some operating systems, even this level of
PMTU support is unavailable and the only feedback that
the PMTU has been exceeded is packet loss. It’s not clear
what the best approach for dealing with such an environ-
ment is, but our intention is to start with a large packet size
and then back off the packet size with each successive re-
transmit.
Note however, that performance sensitive datagram ap-
plications are generally PMTU aware anyway, in which
case DTLS can be relieved of having to guess PMTU.
During the handshake phase, DTLS attempts to send the
largest packets possible, which includes packing multiple
records into a single packet.
Buffering Because retransmits may be necessary, we
buffer a copy of outbound handshake messages. Option-
ally, handshake messages may be reconstructed whenever
a retransmit request is received, but this is unnecessarily
computation intensive, especially when memory is avail-
able. Buffered messages need only be buffered until the
next expected handshake message is received. This is be-
cause the handshake protocol is executed in lock-step and
the incoming message provides an implicit acknowledg-
ment for all the buffered messages. Our implementation
of DTLS also buffers out-of-order handshake messages,