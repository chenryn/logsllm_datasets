title:Differentially private data aggregation with optimal utility
author:Fabienne Eigner and
Matteo Maffei and
Ivan Pryvalov and
Francesca Pampaloni and
Aniket Kate
Diﬀerentially Private Data Aggregation with Optimal
Utility
Fabienne Eigner1, Aniket Kate2, Matteo Maﬀei1,
Francesca Pampaloni3, and Ivan Pryvalov2
1 CISPA, Saarland University, Germany
{eigner,maffei}@cs.uni-saarland.de
2 MMCI, Saarland University, Germany
{aniket,pryvalov}@mmci.uni-saarland.de
3 Independent researcher
PI:EMAIL
Abstract
Computing aggregate statistics about user data is of vital importance for a
variety of services and systems, but this practice has been shown to seriously un-
dermine the privacy of users. Diﬀerential privacy has proved to be an eﬀective tool
to sanitize queries over a database, and various cryptographic protocols have been
recently proposed to enforce diﬀerential privacy in a distributed setting, e.g., stati-
cal queries on sensitive data stored on the user’s side. The widespread deployment
of diﬀerential privacy techniques in real-life settings is, however, undermined by
several limitations that existing constructions suﬀer from: they support only a lim-
ited class of queries, they pose a trade-oﬀ between privacy and utility of the query
result, they are aﬀected by the answer pollution problem, or they are ineﬃcient.
This paper presents PrivaDA, a novel design architecture for distributed dif-
ferential privacy that leverages recent advances in SMPCs on ﬁxed and ﬂoating
point arithmetics to overcome the previously mentioned limitations. In particu-
lar, PrivaDA supports a variety of perturbation mechanisms (e.g., the Laplace,
discrete Laplace, and exponential mechanisms) and it constitutes the ﬁrst generic
technique to generate noise in a fully distributed manner while maintaining the
optimal utility. Furthermore, PrivaDA does not suﬀer from the answer pollution
problem. We demonstrate the eﬃciency of PrivaDA with a performance evalua-
tion, and its expressiveness and ﬂexibility by illustrating a variety of application
scenarios such as privacy-preserving web analytics.
1 Introduction
Statistics about user data play a signiﬁcant role in the digital society: they are used
daily for improving services, analyzing trends, performing marketing studies, conduct-
ing research, and so on. For instance, website owners rely on third-party analytics
1
services to learn statistical information (e.g., gender, age, nationality) about their vis-
itors; electricity suppliers introduced smart meters in order to constantly monitor the
user’s electricity consumption, which allows them to compute prices based on energy
usage trends, to optimize energy distribution, and so on; lastly, service providers often
ask their users to evaluate the quality of their services with the goal of publishing the
aggregate results.
The acquisition and processing of sensitive information poses serious concerns about
the privacy of users. The ﬁrst problem is how and where user data are aggregated: com-
panies prefer to directly collect and process user data, but this gives them access to a
wealth of sensitive information. For instance, web analytics rely on user tracking, al-
lowing aggregators to reconstruct a detailed and precise proﬁle of each individual. The
second problem is how to publish aggregate data or statistics in a privacy-preserving
manner. For example, researchers demonstrated how precise information about con-
sumer habits can be reconstructed from the electricity consumption information col-
lected by smart meters [47] and how an individual’s identity and state of health can be
derived from genome wide association studies [52].
Diﬀerential Privacy (DP). Diﬀerential privacy [26] is a popular architecture to
deﬁne and enforce privacy for statistics on sensitive data. The fundamental idea is
that a query on a database is diﬀerentially private if the contribution of an individual
in the database can only marginally inﬂuence the query result. More precisely, the
contribution of each single entry to the query result is bounded by a small constant
factor, even if all remaining entries are known. A deterministic query can be made
diﬀerentially private by perturbing the result with a certain amount of noise. The
amount of noise depends on the query itself and a variety of perturbation algorithms [28,
44] have been proposed for diﬀerent queries and datatypes (e.g., numerical and non-
numerical data, buckets, histograms, graphs).
Distributed Diﬀerential Privacy (DDP). While the original deﬁnition of DP fo-
cused on a centralized setting, in which a database is queried by a curious entity, sub-
sequent work has extended the deﬁnition to a distributed setting (e.g., [27, 46]), in
which mutually distrustful, and potentially compromised, parties collaborate to com-
pute statistics about distributed data. In particular, Dwork et al. [27] were the ﬁrst to
suggest the idea of employing secure multiparty computation (SMPC) to aggregate and
perturb data in a privacy-preserving distributed manner. In general, in a distributed
setting, which will be the focus of this paper, the problem to solve is two-fold: (i) how
to aggregate data and compute statistics without parties learning each other’s data and
(ii) how to perturb the result to obtain DP even in the presence of malicious parties
deviating from the protocol.
State-of-the-Art. Several specialized ad-hoc cryptographic protocols have been pro-
posed recently to solve the problem of data aggregation in a distributed, privacy-
preserving manner, which has enabled the enforcement of DDP in some challenging
scenarios, such as smart metering [13, 24] and web analytics [7, 20, 21].
These works can be grouped into two categories:
server-based and fully dis-
2
tributed approaches. The former rely on trusted or honest-but-curious (HbC)1
servers [7, 20, 21, 41] to compute noise perturbation. These servers are assumed not
to collude with each other. The latter [6, 19, 49, 51] (see [36] for a comparative survey)
propose fully distributed systems to distributively aggregate time-series data, which
are directly perturbed by users and then encrypted in such a way that the aggregator
is only able to decrypt their noisy sum. Despite the signiﬁcant progress in this ﬁeld,
the deployment of DDP techniques in real-life systems has so far been undermined by
some open challenges, which we discuss below.
Tradeoﬀ between privacy and utility. The existing fully distributed approaches [6, 19,
49,51] exploit the divisibility properties of certain noise mechanisms and let each party
produce a little amount of noise, whose sum yields the noise required to achieve DDP.
This solution is aﬀected by a trade-oﬀ between privacy and utility, since the amount of
noise each user has to add is proportional to the number of tolerated malicious or failing
parties: the more malicious parties, the more the noise to be added and, therefore, the
less accurate the result. Hence, in order to obtain strong privacy guarantees, each
party should assume all others to be malicious, but this leads to an intolerable error
(O(N 2), where N is the number of users) as we show in § B. Relying on a lower honesty
threshold, however, not only gives lower privacy guarantees but also leads parties to
the paradox of having to agree on how many of them are dishonest!
Lack of generality and scalability. Existing solutions are tailored to individual datatypes
and perturbation mechanisms [6–8, 19, 21, 36, 41, 49, 51]. Computing diﬀerent kinds of
queries or employing diﬀerent perturbation mechanisms requires the usage of diﬀerent
protocols, which rely on diﬀerent cryptographic schemes, communication patterns, and
assumptions. The engineering eﬀort and usability penalty are signiﬁcant and discourage
system administrators from deploying such technologies. Furthermore, existing SMPC-
based schemes [8] for two parties cannot simply be extended to a multiparty setting.
Ineﬃciency. Many schemes [6, 19, 49, 51] involve a signiﬁcant computational eﬀort on
the user’s side, making them impractical in several scenarios, e.g., for aggregating data
stored on mobile devices with limited computation power.
Answer pollution. Fully distributed schemes [6, 19, 49, 51] in particular, suﬀer from the
answer pollution problem: a single party can substantially pollute the aggregate result
by adding excessive noise.
Collusion. Server-based systems [7,21,41] rely on strong non-collusion assumptions. In
case of collusion, both the noise and the individual user’s data are disclosed.
For a detailed comparison of the presented works w.r.t. utility, supported queries and
perturbation mechanisms, and non-collusion assumptions we refer to § B.
Our Contributions.
In this work we present PrivaDA, the ﬁrst generic architec-
ture for computing diﬀerentially private statistics about distributed data. We show
how to achieve provable DDP guarantees, while overcoming the previously discussed
1An honest-but-curious party follows the protocol, but tries to learn additional information about
the private data.
3
limitations, by leveraging recently proposed SMPC protocols for ﬂoating point num-
bers [9], ﬁxed point numbers [18], and integers [31]. Our construction reﬁnes these
schemes, originally designed for the HbC setting, so as to make them secure even in
the malicious setting.
The overall diﬀerentially private data aggregation computation is organized in two
phases: the aggregation phase, in which the clients securely compute the aggregate
result, and the perturbation phase, in which this result is perturbed so as to achieve
DDP. To improve the performance, the SMPC is actually conducted by computation
parties, which collect input shares from each client and perform the required computa-
tions. For the perturbation phase, the fundamental idea is to let computation parties
jointly compute a random seed (i.e., a random variable in (0, 1)), which is then used to
produce the required noise.
The distinctive features of our approach are:
Generality. PrivaDA supports a variety of perturbation mechanisms, such as noise
drawn from the Laplace and the discrete Laplace (symmetric geometric) distribution
as well as the exponential mechanism. Consequently, it is well-suited for a variety of
application scenarios. We illustrate generality of our architecture by proposing privacy-
preserving designs for web analytics, statistics gathering for anonymity networks, and
anonymous surveys.
Strong privacy. As long as at least one of the computation parties is honest, malicious
parties can only recover the aggregate result. This is a fundamental diﬀerence from
other approaches (e.g., [7, 20, 21], where colluding parties can immediately read the
individual’s user data. Furthermore, as long as the majority of the computation parties
is not colluding, none learns the seed or the noise (i.e., DDP is achieved).
Optimal utility and resistance to pollution attacks. The result is perturbed with the
minimal amount of noise required to achieve DDP, irrespectively of the expected number
of dishonest users and computation parties. Hence, our protocol provides optimal
utility and resistance to answer pollution. We also provide mechanisms to tackle the
orthogonal problem of ensuring that the protocol only accepts client inputs that are
contained in a set of valid answers.
Eﬃciency. We implemented the system and conducted a performance evaluation,
demonstrating the practicality of our approach. We ﬁnd the overheads introduced by
our privacy-preserving mechanisms to be acceptable for the data aggregation scenarios.
Importantly, the client does not have to perform any expensive computation: she just
has to provide each computation party with a share of her data and can then go oﬄine,
which makes this approach suitable even for mobile devices. Furthermore, PrivaDA
supports a large number of clients without any signiﬁcant performance penalty. Our
implementations provide a clear interface for developers to easily integrate PrivaDA in
a privacy-preserving data aggregation scenario.
Outline. The paper is organized as follows: § 2 gives some necessary background
information on DP and on SMPCs for arithmetic operations; § 3 presents our archi-
tecture and our algorithms for three query sanitization mechanisms; § 4 provides an
4
instantiation of the diﬀerentially private algorithms with eﬃcient SMPCs; § 5 analyzes
the security of these protocols and § 6 investigates their performance; § 7 illustrates
the ﬂexibility of our architecture by showing three use cases for PrivaDA; § 8 concludes
and gives directions for future research. More details, additional information on the
related work, and proofs are provided in the appendix.
2 Background
We now present the concept of diﬀerential privacy and the cryptographic building
blocks that PrivaDA builds on.
Diﬀerential Privacy (DP). Intuitively, a query is diﬀerentially private if it behaves
statistically similarly on all databases D, D(cid:48) diﬀering in one entry, written D ∼ D(cid:48).
This means that the presence or absence of each individual database entry does not
signiﬁcantly alter the result of the query. The deﬁnition of DP is parameterized by a
number , which measures the strength of the privacy guarantee: the smaller , the
smaller the risk to join the database.
Deﬁnition 1 (Diﬀerential Privacy [26]) A randomized function f : D → R is -
diﬀerentially private iﬀ for all databases D, D(cid:48) such that D ∼ D(cid:48) and every set S ⊆ R,
it holds that Pr [f (D) ∈ S] ≤ e · Pr [f (D(cid:48)) ∈ S].
A deterministic query can be made diﬀerentially private by perturbing its result with
noise. We describe three popular perturbation mechanisms below. An important in-
sight is that the required amount of noise depends on the query: the more a single
entry aﬀects the query result, the stronger the perturbation has to be. This can be
expressed using the notion of sensitivity of queries, which measures how much a query
ampliﬁes the distance between two inputs.
Deﬁnition 2 (Sensitivity [28]) The sensitivity ∆f of a query f : D → R is deﬁned
as ∆f = max∀D,D(cid:48).D∼D(cid:48) |f (D) − f (D(cid:48))|.
Intuitively, queries of low sensitivity map nearby inputs to nearby outputs. For
instance, the query “how many students like the ’Security’ lecture?” has sensitivity 1,
since adding or removing one entry aﬀects the result by at most 1.
Laplace noise. The most commonly used sanitization mechanism for queries returning a
numerical result is the Laplace mechanism [28], i.e., the addition of random noise drawn
according to a Laplace distribution Lap(λ) to the correct query result. As shown by
Dwork et al. [28], this mechanism provides -DP, if the parameter λ is set to ∆f
 . The
distribution is both parameterized by the sensitivity of the query and the privacy value
.
Discrete Laplace noise. In some scenarios, it is necessary for a query and its sanitization
to return an integer result, e.g., to enable future cryptographic operations based on
discrete groups on the result. To this end, Ghosh et al. [35] proposed the discrete
Laplace mechanism (also called geometric mechanism). It is deﬁned by adding a random
5
integer drawn according to the discrete Laplace distribution DLap(λ) [35], also known
as the symmetric geometric distribution, to the correct query result. The discrete
Laplace mechanism provides -DP if the parameter λ is set to e
∆f [19]. Again, the
distribution is both parameterized by the sensitivity of the query and the privacy value
.
− 
Exponential mechanism. There exist many scenarios in which queries return non-
numerical results (e.g., strings or trees). For instance, consider the query “what is
your favorite lecture? ”. For such queries, the addition of noise either leads to nonsen-
sical results or is not well-deﬁned. To address this issue, McSherry and Talwar [44]
proposed the so-called exponential mechanism, which considers queries on databases D
that are expected to return a query result a of an arbitrary type R. For our purpose
we consider the range R to be ﬁnite, e.g., the set of lectures oﬀered by a university. We
refer to each a ∈ R as a candidate. The mechanism assumes the existence of a utility
function q : (D×R) → R that assigns a real valued score to each possible input-output
pair (D, a), which measures the quality of the result a w.r.t.
input D. The higher
such a score, the better (i.e., more exact) the result. The mechanism ε
q(D) aims at
providing the “best” possible result a ∈ R, while enforcing DP.
Deﬁnition 3 (Exponential Mechanism [44]) For all q :
randomized exponential mechanism ε
return a ∈ R with probability proportional to eq(D,a).
q(D) for D ∈ D is deﬁned as ε
(D × R) → R the
:=
q(D)
We now formally state the privacy guarantees of the above mechanisms.
Theorem 1 (DP of Mechanisms [19, 28, 44]) For all queries f : D → R, g : D →
Z, and q : (D×R) → R it holds that the queries f (x)+Lap( ∆f
∆f )
and the mechanism ε
(D) are -diﬀerentially private.
 ) and g(x)+DLap(e
− 

2∆q
q
Alternative approaches to privacy. In this work we focus on the notion of diﬀerential
privacy, but for completeness, we refer to some recent papers that investigate limitations
of this notion [37,43] and propose alternative deﬁnitions of privacy for statistical queries
[15, 33, 42].
Secure Multiparty Computation (SMPC). SMPC enables a set of parties P =
{P1, P2, . . . , Pβ} to jointly compute a function on their private inputs in a privacy-
preserving manner [53]. More formally, every party Pi ∈ P holds a secret input value xi,
and P1, . . . , Pβ agree on some function f that takes β inputs. Their goal is to compute
and provide y = f (x1, . . . , xβ) to a recipient while making sure that the following two
conditions are satisﬁed: (i) Correctness: the correct value of y is computed; (ii) Secrecy:
the output y is the only new information that is released to the recipient (see § 5 for a
formal deﬁnition).
Although the feasibility of SMPC in the computational setting as well as in the
information theoretic one is known for more than 25 years, dedicated work to optimize
secure realizations of commonly used arithmetic operations has started only in the