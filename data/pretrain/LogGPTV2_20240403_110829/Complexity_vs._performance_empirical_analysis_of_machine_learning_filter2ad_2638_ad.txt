on classifier choice, and that experimenting with a random subset
of 3 classifiers often achieves performance and lowers variation
close to optimal.
6 HIDDEN OPTIMIZATIONS
In the final part of our analysis, we seek to shed light on any
platform-specific optimizations outside of user-visible configura-
tions or controls. More specifically, we focus on understanding
hidden optimizations used by fully automated black-box platforms,
Google and ABM. These platforms have the most leeway to imple-
ment internal optimizations, because their entire ML pipeline is
fully automated. In Section 4.1 (Figure 4), we observe that Google
and ABM outperform many other platforms when applying default
configurations. This suggests that their hidden configurations are
generally better than alternative default settings.
Among the countless potential options for optimization, we fo-
cus on a simple yet effective technique: optimizing classifier choices
based on dataset characteristics [46]. We raise the question: Are
black-box platforms automatically selecting a classifier based on the
dataset characteristics? Note that our usage of the phrase “selecting
a classifier” should be broadly interpreted as covering different
possible implementation scenarios (for optimization). For example,
optimization can be implemented by switching between distinct
classifier instances, or a single classifier implementation that in-
ternally alters decision characteristics depending on the dataset.
While our analysis cannot infer such implementation details, we
provide evidence of internal optimization in black-box platforms
(Section 6.1). We further quantitatively analyze their optimization
strategy (Section 6.2), and finally examine the potential for improve-
ment (Section 6.3)
(a) Visualization of CIRCLE.
(b) Visualization of LINEAR.
Figure 9: Visualization of two datasets: synthetic non-
linearly-separable dataset (CIRCLE) and synthetic linearly-
separable dataset (LINEAR).
Category
Linear
Classifiers
LR, NB, Linear SVM, LDA
Non-Linear DT, RF, BST, KNN, BAG, MLP
Table 5: Assignment of classifiers available on local library
into linear vs. non-linear categories.
6.1 Evidence of Internal Optimizations
We select two datasets from our collection, a non-linearly-separable
synthetic dataset, which we call CIRCLE11, and a linearly-separable
synthetic dataset, referred to as LINEAR12. Figure 9(a) and Fig-
ure 9(b) show visualizations of the two datasets. Both datasets have
only two features. Given the contrasting characteristics (linearity)
of the two datasets, our hypothesis is that they would help to dif-
ferentiate between linear and non-linear classifier families based
on prediction performance.
We examine Google and ABM’s prediction results on CIRCLE and
LINEAR to infer their classifier choices. Since we have no ground-
truth information here, we resort to analyzing decision boundaries
generated by the two platforms. The decision boundary is visual-
ized by querying and plotting the predicted classes of a 100×100
mesh grid. Figure 10(a) and Figure 10(b) illustrate Google’s decision
boundary on CIRCLE and LINEAR, respectively. It is very clear that
Google’s decision boundary on CIRCLE forms a circle, indicating
Google is using a non-linear classifier, or a non-linear kernel, e.g.
RBF kernel [67]. On LINEAR, Google’s decision boundary matches a
straight line. It shows Google is using a linear classifier. Experiments
on ABM also show similar results. Figure 10(c) and Figure 10(d)
show the decision boundaries of ABM on CIRCLE and LINEAR,
respectively. Thus, both platforms are optimizing and switching
classifier choices for the two datasets. Additionally, Google’s deci-
sion boundary on CIRCLE (circular shape) is different from ABM
(rectangular shape), indicating that they selected different non-
linear classifiers. Based on the shape of decision boundaries, it is
likely that Google used a non-linear kernel based classifier while
ABM chose a tree-based classifier.
11http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.
html
12http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_
classification.html
 0.6 0.7 0.8 0.9 1 0 2 4 6 8 10Average F-scoreNumber of Classifiers ExploredBigMLPredictionIOMicrosoftLocal-1 0 1 2-1.5-1-0.5 0 0.5 1 1.5Feature #2Feature #1Class 0Class 1-6-3 0 3 6-3-2-1 0 1 2 3Feature #2Feature #1Class 0Class 1(a) Google’s decision boundary on CIRCLE.(b) Google’s decision boundary on LINEAR. (c) ABM’s decision boundary on CIRCLE.
(d) ABM’s decision boundary on LINEAR.
Figure 10: Decision boundaries generated by Google and ABM on CIRCLE and LINEAR. Both platforms produced linear and
non-linear boundaries for different datasets.
(a) CIRCLE.
(b) LINEAR.
Figure 11: Performance of predicting local linear/non-linear
classifier choices on CIRCLE and LINEAR datasets.
Figure 12: Validation perfor-
mance of predicting linear/non-
linear classifiers.
Figure 13: Amazon’s deci-
sion boundary on CIRCLE.
6.2 Predicting Classifier Family
In this section, we present a method to automatically predict the
classifier used by a platform using just two pieces of information—
knowledge of the training dataset and prediction results from the
platform. Such a method would help us automatically find instances
where a black-box platform would change classifiers depending on
the dataset characteristics.
At a high level, we observe that it is hard to pin-point the spe-
cific classifier used by a platform, using just the dataset and the
prediction results. This is because prediction results of different
classifiers tend to overlap. However, we find that it is possible to
accurately infer the broad classifier family, more specifically, linear
or non-linear classifiers.
Our key insight is that we can control datasets used for the in-
ference and thus selectively choose datasets that elicit significant
divergence between prediction results of linear and non-linear clas-
sifiers. To give an example, we examine the performance of the
local library classifiers on the CIRCLE and LINEAR datasets. We
categorize local classifiers into linear and non-linear families, as
shown in Table 5. Figures 11(a) and 11(b) shows the performance
(F-score) of the two categories of classifiers on the two datasets. As
expected, we find that linear and non-linear classifiers produce very
different F-scores on the two datasets, regardless of other configu-
ration settings. Non-linear classifiers outperform linear classifiers
when on CIRCLE. For LINEAR, linear classifiers outperform non-
linear classifiers in many cases. This is because of the noisy nature
of the dataset causing non-linear classifiers to overfit, and therefore
produce lower performance compared to linear classifiers. Next, we
present our methodology to accurately predict the classifier family
by identifying more datasets that show divergence in prediction
results of linear and non-linear classifiers.
Methodology. We build a supervised ML classifier for the pre-
diction task. For training the classifier, we use prediction results,
and ground-truth of classifier choices from the local library and the
three platforms that allow user control of the classifier dimension
(i.e. Microsoft, BigML and PredictionIO). Features used for train-
ing include aggregated performance metrics (F-score, precision,
recall, accuracy), and the predicted labels. We train one classifier
for each dataset in our collection. Each training sample is one ML
experiment using a single configuration of the ML pipeline (i.e.
choices of FEAT, CLF and PARA). Measurements are randomly split
into training, validation, and test sets. Training and validation sets
contain 70% of experiments, and test set contains the remaining
30% experiments. We train a Random Forests classifier with 5-fold
cross-validation, and pick the best performing classifier based on
validation performance. Based on prior work, Random Forests is
one of the best performing classifiers for supervised binary clas-
sification tasks [15, 23]. Figure 12 shows the distribution of cross-
validation performance of classifiers trained on all 119 datasets.
Not all datasets could differentiate linear and non-linear classifiers.
There are 64 datasets that produce classifiers achieving higher than
0.95 F-score. In other datasets, classifiers failed to separate linear
and non-linear classifiers as they produce similar performance. In-
tuitively, we do not expect all datasets to perform well, and one goal
of the training process is to identify datasets with high differentiat-
ing power. We select the 64 datasets where the trained classifiers
achieve high performance (F-score > 0.95) on the validation set. To
further test if they would generalize and accurately predict classifier
-1 0 1 2-1.5-1-0.5 0 0.5 1 1.5Feature #2Feature #1Class 0Class 1-6-3 0 3 6-3-2-1 0 1 2 3Feature #2Feature #1Class 0Class 1-1 0 1 2-1.5-1-0.5 0 0.5 1 1.5Feature #2Feature #1Class 0Class 1-6-3 0 3 6-3-2-1 0 1 2 3Feature #2Feature #1Class 0Class 1 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1CDF of ExperimentsF-scoreLinearNon-linear 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1CDF of ExperimentsF-scoreNon-linearLinear 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1CDF of TrainedClassifiersValidation F-score-1 0 1 2-1.5-1-0.5 0 0.5 1 1.5Feature #2Feature #1Class 0Class 1(a) Google vs. our naïve strategy.
Google
Linear
Non-linear
5 (11.6%)
10 (23.26%)
Naïve
Linear
Non-linear
11 (25.5%)
17 (39.5%)
(b) ABM vs. our naïve strategy.
ABM
Naïve
Linear
Non-linear
Linear
8 (16.7%)
22 (45.8%)
Non-linear
3 (6.3%)
15 (31.3%)
Table 6: Breakdown of datasets based on classifier choice
when our naïve strategy outperforms black-box platforms.
choices, we apply them on the 30% held-out test set. All trained
classifiers achieve F-score higher than 0.96. This further proves that
the chosen classifiers can accurately predict the classifier family.
Classifier Choices of Google and ABM. We apply the selected
64 trained classifiers (covering 64 datasets) on Google and ABM
and predict their classifier choices over linear and non-linear family.
Results show Google uses linear classifiers on 39 out of 64 (60.9%)
datasets, and non-linear classifiers for the remaining 25 (39.1%)
datasets. ABM, on the other hand, uses linear classifiers on 44
(68.8%) out of 64 datasets, and non-linear on the remaining 20
(31.2%) datasets. If we compare Google and ABM, they pick the
same classifier category on 49 (76.6%) datasets, but disagree on
the remaining 15 (23.4%) datasets. The differences in the classifier
choices could contribute to their overall performance difference
in Figure 4. Overall, our results suggest that both platforms make
different classifier choices (choosing linear or non-linear family)
depending on the dataset.
Recall that Amazon does not
Classifier Choices of Amazon.
reveal any classifier information in their model training interface,
but claims to use Logistic Regression on their documentation page.
We apply our classifier prediction scheme on Amazon to investi-
gate whether they are indeed using a single classifier for all tasks.
Interestingly, 10 out of all 64 datasets have over 50% configurations
that are predicted to be non-linear (the remaining are predicted to
be linear). We also observe that Amazon produces a non-linear deci-
sion boundary when applied to the CIRCLE dataset (Figure 13). We
suspect that Amazon uses non-linear techniques on top of Logistic
Regression, e.g. non-linear kernel, or even uses other non-linear
classifiers apart from Logistic Regression.
Unfortunately we are unable to corroborate our findings with
the providers (Google, ABM, and Amazon), as all hidden optimiza-
tions are kept confidential and proprietary. However, our predic-
tions demonstrate high accuracy in our validation and test datasets
(where the underlying configuration is known).
6.3 Impact of Internal Optimizations
Previous experiments show that black-box platforms successfully
choose classifier families with better performance when applied on
(a) Google.
(b) ABM.
Figure 14: Performance difference in datasets where naïve
strategy outperforms Google/ABM using different classifier
family.
the CIRCLE and LINEAR datasets. On these two datasets, Google
and ABM would outperform a scheme that does not switch between
classifier families. But are their strategies optimized for all other
datasets? Are there cases where the two platforms make the wrong
classifier choice?
To understand the potential for further improvement, we design
a naïve classifier selection strategy using the local library, and
compare its performance with Google and ABM. Our intuition is
that if Google and ABM perform poorly when compared to our
naïve strategy, there is potential for further improvement and we
can understand the cases where classifier choices (i.e. linear vs
non-linear) are potentially incorrect. We choose two widely used
linear and non-linear classifiers, Logistic Regression and Decision
Tree. These two classifiers are supported by most other platforms
(Table 1). For each dataset, we train both classifiers and choose the
one with higher performance. To further simplify the strategy and
to avoid any impact of optimization from other control dimensions,
we use the default parameter settings in Logistic Regression and
Decision Tree, and perform no feature selection.
For our analysis, we again use the 64 datasets that can accurately
predict choices of linear and non-linear classifiers. In 43 out of 64
datasets, our naïve strategy outperforms Google, and in 48 datasets,
it outperforms ABM. This clearly indicates that Google and ABM
have scope for further improvement.
We further compare the choices made by naïve strategy and
black-box platforms. Table 6 shows the breakdown of the datasets
by decisions when naïve strategy outperforms Google and ABM. In
both platforms, in a majority of cases, the classifier choices do not
match our simple strategy. In these cases, Google and ABM could
increase their performance (F-score) by 20% and 34% on average,
respectively, by choosing the other classifier family. Figure 14 shows
a detailed breakdown (as a CDF) of performance difference between
the black-box platforms and naïve strategy, when we outperform
them. The potential performance improvement is significant in
many cases.
When is switching classifier the best option for improvement?
Although we show the potential performance improvement by
switching classifiers, black-box platforms could use other meth-
ods to improve performance. For example, Google and ABM could
perform better parameter tuning and feature selection to reduce
the performance gap and justify their classifier choices. To identify
 0 0.2 0.4 0.6 0.8 1 0 0.05 0.1 0.15 0.2 0.25 0.3CDF of DatasetsDifference in F-score 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6CDF of DatasetsDifference in F-scorecases where classifier switching is likely the best option, we com-
pare our naïve strategy with the optimal performance of the other
classifier family (i.e. not chosen). This means that when naïve strat-
egy chooses a non-linear classifier, we compare its performance
with the optimal linear classifier (across all configurations). If naïve
strategy could still outperform Google and ABM under this sce-
nario, it indicates that switching the classifier is likely the best way
to further improve the performance. We find 3 datasets in the case
of Google, and 4 for ABM, where changing the classifier is probably
the best option to further improve performance. While our analysis
is limited to the 64 datasets where we can perform prediction, our
finding highlights the existence of scenarios where Google and
ABM clearly need to make better classifier choices.
7 RELATED WORK
Analyzing MLaaS Platforms. There is very limited prior work
focusing on MLaaS platforms. Chan, et al. and Ribeiro, et al. pre-
sented two different architecture designs of MLaaS platforms [16,
51]. Although we cannot confirm that these architectures are being
used by any of the MLaaS platforms we studied, they shed light
on potential ways MLaaS platforms operate internally. Other re-
searchers investigated vulnerabilities of these platforms towards
different types of attacks. This includes attacks that try to leak in-
formation about individual training records of a model [26, 57], and
those aiming to duplicate the functionality of a black-box MLaaS
model by querying the model [64]. While these studies are in gen-
eral orthogonal to our work, there is scope for borrowing techniques
from them that can help us better understand MLaaS platforms.
Prior empirical analy-
Empirical Analysis of ML Classifiers.
sis focused on determining the best classifier for a variety of ML
problems using user-managed ML tools (e.g. Weka, R, Matlab) on a
large number of datasets. Multiple studies conducted an exhaustive
performance evaluation of up to 179 supervised classifiers using
up to 121 datasets [15, 23, 66]. All studies observe that Random
Forests, Boosted or Bagging Trees outperform other classifiers, in-
cluding SVM, Naïve Bayes, Logistic Regression, Decision Tree, and
Multi-layer Perceptron. Caruana et al. further studied classifier per-
formance focusing on high-dimensional datasets [14]. They find
that Random Forests perform better than Boosted Trees on high-