title:Difficulties Measuring Software Risk in an Industrial Environment
author:Elaine J. Weyuker
Difficulties Measuring Software Risk in an Industrial Environment 
Elaine J. Weyuker 
AT&T Labs - Research 
180 Park Avenue 
weyuker @research.att.com 
Florham Park, NJ 07932 
(973)360-8645 
fax: (973)360-8 187 
Abstract 
Software risk is intended to reflect loss due to software 
failure. This has traditionally been computed by taking 
the  product of  two things: a probability of  occurrence 
and the  cost associated with  failures.  Applying these 
definitions in  practice, however, may  be  much  harder 
than  it  at  first  appears.  There are two types of  prob- 
lems that affect the applicability and usefulness of such 
a computation: that  the  user has to  know detailed in- 
formation that is not normally available, and that most 
risk  definitions do not use relevant information that is 
available, including information derived from testing. 
In this paper, a definition of risk is introduced that will 
be  usable in  industrial settings.  We  also explore ways 
of  incorporating information about how  the  software 
has been tested, the  degree to  which the  software has 
been tested, and the observed results. 
Keywords: Software risk, software testing, cost of failure, 
operational profile. 
1  Introduction 
Software  risk  has  traditionally  been  defined  to  be  the 
expected  loss  that can  be  attributed  to the  failures caused 
by  faults remaining  in  software.  There have  been  several 
different  ways that this definition has been  formally inter- 
preted.  Boehm  [2], for example, defined the risk exposure 
to be the product of  the probability of  failure and the  loss 
due to the failure. 
Another way  of computing risk is by  summing over all 
members of the input domain, the product of the probability 
of  an  input  occurring  in  the  field  and  the  cost  associated 
with that input failing [6].  This definition is predicated on 
inputs actually being run and therefore knowing whether or 
not they  fail.  With  this definition, the cost associated with 
an input that does not fail would be zero. 
0-7695-1101-5/01 $10.00 0 2001 IEEE 
15 
Similarly, the summation could be  done over the prod- 
uct of the probability of an input occurring in the field, the 
probability of the input failing, and a non-zero cost of fail- 
ure associated  with  every  element  of  the domain.  In  this 
case there is no  implication that inputs are run.  Other re- 
lated definitions appear in papers including [7, 8, 1 I ,   141. 
Using  any  of  these  definitions,  there  are  two  primary 
components of the computation: a probability of occurrence 
(either of an input or of a failure) and the cost or expected 
cost due to a failure.  By defining risk  in this way,  we are 
able to balance the frequency of occurrence and cost of fail- 
ure.  A  failure  that  is extremely  unlikely  to  happen  is  of 
less  concern  than  one that  frequently happens,  if  the cost 
of  failure is  the  same.  On the  other hand,  a  failure with 
insignificant cost is of far less concern  than one with catas- 
trophic results.  By incorporating both of these factors into 
a definition of risk, we can balance the two components. 
However, there  are  issues  that  have  to be  addressed  if 
any of  these  definitions are  to have  pragmatic usefulness. 
When  performing  a  risk  computation that  requires know- 
ing an overall probability of failure, how will  this informa- 
tion be gathered or estimated?  Similarly, if an overall cost 
of failure is used, the product of these averages might give 
highly misleading results by either overestimating or under- 
estimating the risk. 
If using a definition that requires knowing a probability 
of occurrence for every element of the  input domain, plus 
a likelihood of failure for each such element, and a cost of 
failure for the element, is it realistic to expect that all of this 
information will  be available and that it is practical to indi- 
vidually  identify every element of the domain to complete 
the required computation? 
Determining the probability  of  failure for each  element 
of the domain can  be particularly  difficult.  Should we de- 
pend  on historical data for this system or a closely related 
system, or can estimates be made based on significant expe- 
rience with similar projects and perhaps, too, the personnel 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:58:16 UTC from IEEE Xplore.  Restrictions apply. 
involved in the development  of the software?  Determining 
the cost  of  failure for  individual  inputs might  depend  on 
any of the above information  and might involve a substan- 
tial amount of  subjective decision-making.  Sometimes, as 
testing  or field  use  progresses,  these  a priori  estimates are 
modificd  in  order  to more  accurately  predict  failure rates 
and  therefore  risk,  however,  as  long  as  estimation  is  be- 
ing done, subjectivity  is involved  and  there  will  therefore 
be  inaccuracies.  An  important  goal  of  this  research  is  to 
develop ways to minimize the amount of dependence on  a 
priori  prediction, and  thereby  increase  the accuracy  of the 
risk prediction. This will be done by  incorporating into the 
computation of risk, the observed  results of test cases  that 
have been executed. 
Note that  we distinguish between  failures and  faults.  A 
fault  is  a  mistake in  the  software code.  A failure  occurs 
whenever the software behaves in a way other than the way 
it  was  intended  to  behave.  Failures  may  have  significant 
consequences, or may be nothing more than a nuisance. For 
this  reason,  we  cannot rely  simply  on  counting the  num- 
ber of failures observed when computing software risk since 
there may be huge differences in failure impact. 
Ideally, every  failure-causing input will be selected dur- 
ing testing and all of the underlying faults will be identified 
and removed  prior to  release  of  the  software  to the  field. 
Realistically,  we  recognize  that  there  are virtually always 
residual  faults remaining in the software, even after testing 
is complete.  For this reason,  we are interested in assessing 
the risk associated  with running a software system in pro- 
duction. 
We expect  that  the more comprehensively a given  soft- 
ware  system  has been  tested,  the lower  its risk should be. 
After all, before testing has begun, we really  have  little or 
no concrete evidence about how dependable the system will 
be. Therefore, we should expect the system to contain a sig- 
nificant number of faults and that it will fail at times, leading 
to relatively  high risk.  As testing progresses, we normally 
expect to encounter failures that cause us to debug the soft- 
ware and remove  the fault or faults that caused  the failures. 
This should lead to decreased  risk.  Yet the standard defini- 
tions of  risk do not directly reflect this process. 
It  is this  intuition that  motivates  the notion of  risk that 
we  introduce  in  this  paper.  Risk  should  indicate  the  ex- 
pected  loss from  running the  software and  this  should be 
influenced  in  part  by  the  degree  to  which  the  system  has 
been tested. An untested system should be assessed as being 
significantly more “risky” than the same system after it has 
been comprehensively tested and debugged. Of course, risk 
will also be dependent on the quality of the software (which 
determines probability  of  failure),  the consequence or cost 
of different  types of  failures, as well  as how  the software 
will  be  used  when  operational  in  the field.  This expected 
usage is reflected  in  the so-called  operational projile, also 
known as the operational distribution, which is a probabil- 
ity  distribution associating a probability of  selection  with 
each possible input in the domain. 
By using the operational profile as the basis for test case 
selection, testing can  be highly  reflective of the actual  be- 
havior that can be expected when the software is operational 
in the field. This is very important when predicting software 
behavior for a given environment, since different usage pat- 
terns may cause significantly different apparent behavior of 
the software. A given user is most concerned  about how the 
software will behave when it is used the way that it will be 
used  in their environment. It is typically  of far less concern 
to a user that the software is likely to fail if events that never 
occur in their environment were  to occur.  The more accu- 
rately the operational profile can be determined, of course, 
the more accurately system behavior during testing can re- 
flect  actual  behavior during field  use.  A testing approach 
that  was based  on this philosophy was described  in  [ 11.  A 
description of  the  use  of  operational  profiles  for software 
reliability engineering can be found in [ 121. 
The  definition of  risk  that  we  will  propose  in  this  pa- 
per  is based  on  quantitative data, and  should be useful  to 
management  in  helping them  make  an  informed decision 
about  whether  or  not  it  is likely  to be  safe to  release  the 
software in its current state.  We do acknowledge that in ad- 
dition to testing, there are additional sources of information 
that could be used to affect our perception of risk, but those 
sources tend  to be highly  subjective and  therefore  not  ap- 
propriate for our needs.  Thus, our definition of risk will not 
include such things as an observer’s perception  of whether 
or not the development team is an experienced  and talented 
one, nor whether good development process has been  used. 
Similarly, it will not incorporate experience collected about 
the  behavior of  other similar systems, except, perhaps, to 
the extent that that might affect the way the system is to be 
tested or the definition of the operational profile used for the 
system. 
We  emphasize  one additional point.  As  with  all  work 
of  this  nature,  we  have  had  to  make  some  assumptions. 
Throughout, we have tried  to make entirely  clear  what  as- 
sumptions were  being made, and  also tried  to make  those 
assumptions realistic.  Whenever possible, we have selected 
the  conservative choice  when  faced  with  a  decision.  In 
this  way,  we should not apply our risk  assessment  process 
and end up with a picture that is unrealisticly  rosy, thereby 
allowing the software to be released  unwisely  and prema- 
turely.  It is far preferable, in our opinion, to be somewhat 
pessimistic in our assumptions and thereby  indicate that re- 
lease  and use of the software in  its current state might  be 
problematic when in reality  it is not, than to incorrectly in- 
dicate that there is low risk associated  with  the use  of  the 
software when, in fact, there is actually  substantial risk. 
In Section 2 we discuss different notions of the cost of a 
16 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:58:16 UTC from IEEE Xplore.  Restrictions apply. 
failure, present  a traditional  definition of risk, and consider 
pragmatic problems associated with its use. We also include 
a concise survey  of related  work.  In  Section 3, a more us- 
able definition of risk  is presented.  Section 4 describes the 
application of our risk definition, and Section5 presents our 
conclusions. 
2  Background and Related Work 
Since  using  any  of  the  definitions  of  risk  mentioned 
above involves computing the expected  loss due to failure 
for all possible inputs during a single run of the software, it 
requires that information  be known such as the operational 
profile for all elements of  the input domain, a cost  of fail- 
ure be determinable for every element of the domain, and a 
probability  of  failure for inputs be estimated.  We  will de- 
note the cost function as cost(i) and it is the cost that would 
be associated  with a failure of the software on input i ,  were 
it to fail. In general, c ~ i s t ( i )  > 0 for every i. 
Throughout this paper, we are assuming that all software 
is deterministic in the sense that for a given  input, the soft- 
ware either always runs correctly or always fails in the same 
way.  This is a reasonable  assumption provided  that the no- 
tion of an  input is viewed  broadly  enough.  This might re- 
quire the inclusion of a fair amount of the system state in the 
definition  of  an  input.  For example,  when  adding a record 
to a database, the resulting database might be different de- 
pending  on  whether  or  not  there  already  was  a  record  in 
the databasc with the specified key. But, by considering the 
contents of the database as well as the record  as part of the 
input, then the result would be deterministic; i.e. every time 
an attempt  is made to add that record to the database when 
it  is  in  exactly  that  state, should lead  to exactly  the  same 
result. 
Determining the  cost  for  a  given  input,  a  priori,  may 
be an  unrealistic  rcquirement,  and  since the input domain 
is typically  enormous,  a  direct  computation for all  inputs 
would  generally  be  prohibitively  expensive,  even  if  all the 
required  information  were available.  We will therefore  de- 
fine another notion of cost, e(;), that only requires the cost 
to be computed  for  those  elements of the domain that ac- 
tually  fail.  For  all  other inputs,  c ( i )  will  be defined  to be 
0. 
Formally, we define: 
c ( i )  = 
cost( i )  
if P  fails on input i 
otherwise. 
(1) 