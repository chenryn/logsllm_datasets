### Loop Transformation
Loops that can be represented as `for` loops are identified by analyzing `while` loops where a variable `x` is used both in the continuation condition and the last statement of the loop body. We then check if there is a single definition of `x` that reaches the loop entry and is only used within the loop body. If the variables used in this definition are not modified on the path from the definition to the loop entry, we transform the loop into a `for` loop. This process allows us to identify and convert `while` loops to `for` loops even if their initialization statements are not immediately before the loop.

### Function Inlining and Outlining
Compilers often inline functions like `strcpy`, `strlen`, or `strcmp`, replacing function calls with the corresponding function bodies. This results in code duplication, which increases the size of the code and complicates manual analysis. DREAM recognizes and outlines these inlined functions, replacing the duplicated code with the equivalent function call.

### Variable Naming Using API Calls
For the third optimization, DREAM leverages API calls to assign meaningful names to variables. API functions have known signatures, including the types and names of parameters. If a variable is used in an API call, DREAM assigns it the name of the corresponding parameter, provided that name is not already in use.

## Evaluation
In this section, we present the results of our experiments to evaluate DREAM. Our evaluation methodology is based on the approach used by Schwartz et al. [33] to evaluate Phoenix. We used the GNU coreutils suite to assess the quality of the decompilation results and compared DREAM with Phoenix [33] and Hex-Rays [22]. Hex-Rays was included because it is the leading commercial decompiler and the industry standard. The latest version of Hex-Rays at the time of writing (v2.0.0.140605) was used. Phoenix was chosen because it is the most recent and advanced academic decompiler. Other decompilers such as dcc [11], DISC [28], REC [1], and Boomerang [17] were not included due to either being no longer actively maintained or not supporting x86. These decompilers are generally outperformed by Phoenix. The implementation of Phoenix is not publicly available, but the authors shared the coreutils binaries and raw decompiled source code, enabling us to compute our metrics and compare results fairly. This ensured that all three decompilers were tested on the same binary code base. Additionally, we evaluated DREAM using real-world malware samples, specifically ZeusP2P, SpyEye, and Cridex, and compared it to Hex-Rays for this part of the evaluation.

### Metrics
We evaluated DREAM using the following quantitative metrics:

- **Correctness**: Measures the functional equivalence between the decompiled output and the input code. Two functions are semantically equivalent if they exhibit the same behavior and produce the same results when executed with the same set of parameters. Correctness is crucial to ensure that the decompiled output accurately represents the binary code.
  
- **Structuredness**: Measures the ability of a decompiler to recover high-level control flow structure and produce structured decompiled code. Structuredness is quantified by the number of generated `goto` statements in the output. Fewer `goto` statements indicate better recovery of control flow structures, making the code easier to understand and analyze.

- **Compactness**: We performed two measurements for compactness: 
  - Total lines of code generated by each decompiler.
  - The number of functions for which each decompiler generated the fewest lines of code compared to the others. If multiple decompilers generate the same minimal number of lines, it is counted towards the total for each.

### Experiment Setup & Results

#### 1. Correctness Experiment
We evaluated the correctness of DREAM on the GNU coreutils 8.22 suite. The coreutils come with a comprehensive test suite. We followed the approach proposed by [33], using the coreutils tests to measure correctness. Since the coreutils source code contains `goto` statements, both parts of our algorithm—pattern-independent structuring and semantics-preserving transformations—were invoked. To evaluate the control-flow structuring component, we computed the Control Flow Graph (CFG) for each function using joern [41], a state-of-the-art platform for C/C++ code analysis. Joern generates Code Property Graphs (CPGs), combining ASTs, CFGs, and Program Dependence Graphs (PDGs). The CPGs were stored in a Neo4J graph database, and a Python interface was used to extract the CFGs. We transformed the statements in the CFG nodes into DREAM’s intermediate representation and provided them to our structuring algorithm. 

**Table II: Correctness Results**
| Considered Functions F | Functions after preprocessor | Functions correctly parsed by joern | Functions passed tests after structuring | Number of gotos |
|------------------------|------------------------------|-------------------------------------|------------------------------------------|-----------------|
| 1,738                  | 1,530                        | 1,530                               | 0                                        | 219             |

The preprocessed coreutils source code contains 219 `goto` statements. After parsing, 1,530 functions were correctly parsed and fed to our structuring algorithm. The restructured code passed all coreutils tests, indicating that our algorithm correctly recovered control-flow abstractions and transformed `goto` statements into semantically equivalent structured forms.

#### 2. Structuredness and Compactness Experiment
We tested and compared DREAM with Phoenix and Hex-Rays using the same GNU coreutils 8.17 binaries used in the Phoenix evaluation. Structuredness was measured by the number of `goto` statements, and compactness was measured by the total lines of code generated by each decompiler. We extended the original Phoenix evaluation in several ways:
- **Duplicate Functions**: We filtered duplicate functions, as static linking can result in multiple copies of the same library function. Of the 14,747 functions in the coreutils binaries, only 3,141 were unique. We reported results for both filtered and unfiltered function lists.
- **Recompilable Functions**: We considered all functions, not just recompilable ones, as non-recompilable functions are also valuable for malware and security analysis.

**Table III: Structuredness and Compactness Results**
| Considered Functions F | p ∩ F r | h | h | p ∩ F r | coreutils functions with duplicates | T1: F r | T2: Fd ∩ Fp ∩ Fh | coreutils functions without duplicates | T3: F r | T4: Fd ∩ Fp ∩ Fh | Malware Samples | ZeusP2P | SpyEye | Cridex |
|------------------------|---------|---|---|---------|------------------------------------|---------|------------------|---------------------------------------|---------|------------------|-----------------|---------|--------|--------|
| 8,676                  | 10,983  | 785 | 1,821 | 1,021   | 442                                | 167     | 0                | 0                                     | 0       | 0                | 0               | 40      | 4,505  | 31     |
| 4,231                  | N/A     | N/A | N/A | 47      | 3,166                              | 28      | 2,949            | 1,571                                 | 446     | 144              | 93k             | 196k    | 15k    | 107k   |
| 243k                   | 422k    | 30k | 164k | N/A     | N/A                                | N/A     | 120k             | 264k                                  | 18k     | 135k             | 53k             | 28k     | 9k     | 81.3%  |
| 81%                    | 74.9%   | 75.2% | 82.9% | 69.9%   | 84.8%                              | 0.3%    | 0.2%             | 1.1%                                  | 0.7%    | N/A              | N/A             | N/A     | 32.1%  | 30.4%  |
| 36.2%                  | 31.3%   | 14.5% | 25.7% | 12.3%   |                                    |         |                  |                                       |         |                  |                 |         |        |        |

DREAM produced no `goto` statements, while both Phoenix and Hex-Rays generated thousands. DREAM also generated more compact code, with 107k lines of code for unique functions, compared to 164k for Phoenix and 135k for Hex-Rays. In the most relevant test setting (T4), DREAM produced the most compact code for 75.2% of the functions, while Hex-Rays and Phoenix achieved the best compactness in 31.3% and 0.7% of the cases, respectively.

#### 3. Malware Analysis
We evaluated DREAM on three malware samples: ZeusP2P, Cridex, and SpyEye. The results for the malware samples are shown in Table III. DREAM consistently produced more compact and structured code, demonstrating its effectiveness in decompiling and analyzing real-world malware.