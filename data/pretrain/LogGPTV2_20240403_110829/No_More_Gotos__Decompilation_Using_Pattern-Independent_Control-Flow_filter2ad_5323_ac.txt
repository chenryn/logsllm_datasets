be represented as for loops. for loop candidates are while
loops that have a variable x used both in their continuation
condition and the last statement in their body. We then check
if a single deﬁnition of x reaches the loop entry that is only
used in the loop body. We transform the loop into a for loop
if the variables used in the deﬁnition are not used on the way
from the deﬁnition to the loop entry. These checks allow us
to identify for loops even if their initialization statements are
not directly before the loop.
Several functions such as strcpy, strlen, or strcmp
are often inlined by compilers. That is, a function call is
replaced by the called function body. Having several duplicates
of the same function results in larger code and is detrimental
to manual analysis. DREAM recognizes and outlines several
functions. That is, it replaces the corresponding code by the
equivalent function call.
For the third optimization, we leverage API calls to assign
meaningful names to variables. API functions have known
signatures including the types and names of parameters. If
a variable is used in an API call, we give it the name of
corresponding parameter if that name is not already used.
VII. EVALUATION
In this section, we describe the results of the experiments
we have performed to evaluate DREAM. We base our evalua-
tion on the technique used to evaluate Phoenix by Schwartz et
al. [33]. This evaluation used the GNU coreutils to evaluate
the quality of the decompilation results. We compared our
results with Phoenix [33] and Hex-Rays [22]. We included
Hex-Rays because it is the leading commercial decompiler and
the de facto industry standard. We tested the latest version of
Hex-Rays at the time of writing, which is v2.0.0.140605. We
picked Phoenix because it is the most recent and advanced
academic decompiler. We did not include dcc [11], DISC [28],
REC [1], and Boomerang [17] in our evaluation. The reason
is that these projects are either no longer actively maintained
(e.g., Boomerang) or do not support x86 (e.g., dcc). However,
most
they are outperformed by Phoenix. The
implementation of Phoenix is not publicly available yet. How-
ever, the authors kindly agreed to share both the coreutils
binaries used in their experiments and the raw decompiled
source code produced by Phoenix to enable us to compute our
metrics and compare our results with theirs. We very much
appreciate this good scientiﬁc practice. This way, we could
ensure that all three decompilers are tested on the same binary
code base. We also had the raw source code produced by all
three decompilers as well, so we can compare them fairly. In
addition to the GNU coreutils benchmark we also evaluated
our approach using real-world malware samples. Speciﬁcally,
we decompiled and analyzed ZeusP2P, SpyEye, Cridex. For
this part of our evaluation we could only compare our approach
to Hex-Rays since Phoenix is not yet released.
importantly,
A. Metrics
We evaluate our approach with respect to the following
quantitative metrics.
• Correctness. Correctness measures the functional equiv-
alence between the decompiled output and the input code.
More speciﬁcally, two functions are semantically equiv-
alent if they follow the same behavior and produce the
same results when they are executed using the same set
of parameters. Correctness is a crucial criterion to ensure
that the decompiled output is a faithful representation of
the corresponding binary code.
• Structuredness. Structuredness measures the ability of
a decompiler to recover high-level control ﬂow structure
and produce structured decompiled code. Structuredness
is measured by the number of generated goto statements
in the output. Structured code is easier to understand [16]
and helps scale program analysis [31]. For this reason, it is
desired to have as few goto statements in the decompiled
code as possible. These statements indicate the failure to
ﬁnd a better representation of control ﬂow.
• Compactness. For compactness we perform two mea-
surements: ﬁrst, we measure the total lines of code gen-
erated by each decompiler. This gives a global picture on
the compactness of decompiled output. Second, we count
for how many functions each decompiler generated the
fewest lines of code compared to the others. If multiple
decompilers generate the same (minimal) number of lines
of code, that is counted towards the total of each of them.
B. Experiment Setup & Results
To evaluate our algorithm on the mentioned metrics, we
conducted two experiments.
1) Correctness Experiment: We evaluated the correctness
of our algorithm on the GNU coreutils 8.22 suite of
utilities. coreutils consist of a collection of mature pro-
grams and come with a suite of high-coverage tests. We
followed a similar approach to that proposed in [33] where
the coreutils tests were used to measure correctness. Also,
since the coreutils source code contains goto statements,
this means that both parts of our algorithm are invoked;
the pattern-independent structuring part and the semantics-
preserving transformations part. Our goal is to evaluate the
control-ﬂow structuring component. For this, we computed
the CFG for each function in the coreutils source code
and provided it as input to our algorithm. Then, we replaced
the original functions with the generated algorithm output,
compiled the restructured coreutils source code, and ﬁnally
executed the tests. We used joern [41] to compute the CFGs.
Joern is a state-of-the-art platform for analysis of C/C++ code.
It generates code property graphs, a novel graph representation
of code that combines three classic code representations;
ASTs, CFGs, and Program Dependence Graphs (PDG). Code
property graphs are stored in a Neo4J graph database. More-
over, a thin python interface for joern and a set of useful
utility traversals are provided to ease interfacing with the graph
database. We iterated over all parsed functions in the database
and extracted the CFGs. We then transformed statements in
the CFG nodes into DREAM’s intermediate representation. The
extracted graph representation was then provided to our struc-
turing algorithm. Under the assumption of correct parsing, we
11
Considered Functions F
Functions after preprocessor
Functions correctly parsed by joern
Functions passed tests after structuring
|F|
1,738
1,530
1,530
Number of gotos
219
129
0
TABLE II: Correctness results.
can attribute the failure of any test on the restructured functions
to the structuring algorithm. To make the evaluation tougher,
we used the source ﬁles produced by the C-preprocessor, since
depending on the operating system and installed software,
some functions or parts of functions may be removed by the
preprocessor before passing them to the compiler. That in turn
would lead to potential structuring errors to go unnoticed if
the corresponding function is removed by the preprocessor.
We got the preprocessed ﬁles by passing the --save-temps
to CFLAGS in the conﬁgure script. The preprocessed source
code contains 219 goto statements.
2) Correctness Results: Table II shows statistics about the
functions included in our correctness experiments. The pre-
processed coreutils source code contains 1,738 functions.
We encountered parsing errors for 208 functions. We excluded
these functions from our tests. The 1,530 correctly parsed
functions were fed to our structuring algorithm. Next, we
replaced the original functions in coreutils by the structured
code produced by our algorithm. The new version of the
source code passed all coreutils tests. This shows that
our algorithm correctly recovered control-ﬂow abstractions
from the input CFGs. More importantly, goto statements in
the original source code are transformed into semantically
equivalent structured forms.
The original Phoenix evaluation shows that their control-
ﬂow structuring algorithm is correct. Thus, both tools correctly
structure the input CFG.
3) Structuredness and Compactness Experiment: We tested
and compareed DREAM to Phoenix and Hex-Rays. In this
experiment we used the same GNU coreutils 8.17 binaries
used in Phoenix evaluation. Structuredness is measured by the
number of goto statements in code. These statements indicate
that the structuring algorithm was unable to ﬁnd a structured
representation of the control ﬂow. Therefore, structuredness is
inversely proportional to the number of goto statements in
the decompiled output. To measure compactness, we followed
a straightforward approach. We used David A. Wheeler’s
SLOCCount utility to measure the lines of code in each
decompiled function. To ensure fair comparison, the Phoenix
evaluation only considered functions that were decompiled
by both Phoenix and Hex-Rays. We extend this principle to
only consider functions that were decompiled by all the three
decompilers. If this was not done, a decompiler that failed to
decompile functions would have an unfair advantage. Beyond
that, we extend the evaluation performed by Schwartz et al.
[33] in several ways.
• Duplicate functions. In the original Phoenix evaluation
all functions were considered, i.e., including duplicate
functions. It is common to have duplicate functions as
the result of the same library function being statically
linked to several binaries, i.e., its code is copied into
the binary. Depending on the duplicate functions this can
skew the results. Thus, we wrote a small IDAPython script
that extracts the assembly listings of all functions and
then computed the SHA-512 hash for the resulting ﬁles.
We found that of the 14,747 functions contained in the
coreutils binaries, only 3,141 functions are unique, i.e.,
78.7% of the functions are duplicates. For better com-
parability, we report the results both on the ﬁltered and
unﬁltered function lists. However, for future comparisons
we would argue that ﬁltering duplicate functions before
comparison avoids skewing the results based on the same
code being included multiple times.
• Also in the original Phoenix evaluation only recompilable
functions were considered in the goto test. In the context
of coreutils, this meant that only 39% of the unique
functions decompiled by Phoenix were considered in the
goto experiment. We extend these tests to consider the
intersection of all functions produced by the decompilers,
since even non-recompilable functions are valuable and
important to look at, especially for malware and security
analysis. For instance, the property graph approach [41]
to ﬁnd vulnerabilities in source code does not assume that
the input source code is compilable. Also, understanding
the functionality of a sample is the main goal of manual
malware analysis. Hence, the quality of all decompiled
code is highly relevant and thus included in our evalua-
tion. For completeness, we also present the results based
on the functions used in the original evaluation done by
Schwartz et al.
4) Structuredness & Compactness Results: Table III sum-
marizes the results of our second experiment. For the sake of
completeness, we report our results in two settings. First, we
consider all functions without ﬁltering duplicates as was done
in the original Phoenix evaluation. We report our results for the
functions considered in the original Phoenix evaluation (i.e.,
only recompilable functions) (T1) and for the intersection of
all functions decompiled by the three decompilers (T2). In the
second setting we only consider unique functions and again
report the results only for the functions used in the original
Phoenix study (T3) and for all functions (T4). In the table |F|
denotes the number of functions considered. The following
three columns report on the metrics deﬁned above. First, the
number of goto statements in the functions is presented. This
is the main contribution of our paper. While both state-of-
the-art decompilers produced thousands of goto statements
for the full list of functions, DREAM produced none. We
believe this is a major step forward for decompilation. Next,
we present total lines of code generated by each decompiler in
the four settings. DREAM generated more compact code overall
than Phoenix and Hex-Rays. When considering all unique
functions, DREAM’s decompiled output consists of 107k lines
of code in comparison to 164k LoC in Phoenix output and
135k LoC produced by Hex-Rays. Finally, the percentage of
functions for which a given decompiler generated the most
compact function is depicted. In the most relevant test setting
T4, DREAM produced the minimum lines of code for 75.2% of
the functions. For 31.3% of the functions, Hex-Rays generated
the most compact code. Phoenix achieved the best compactness
in 0.7% of the cases. Note that the three percentages exceed
12
Considered Functions F
p ∩ F r
h
h
p ∩ F r
coreutils functions with duplicates
T1 : F r
T2 : Fd ∩ Fp ∩ Fh
coreutils functions without duplicates
T3 : F r
T4 : Fd ∩ Fp ∩ Fh
Malware Samples
ZeusP2P
SpyEye
Cridex
|F|
8,676
10,983
785
1,821
1,021
442
167
Number of goto Statements
Lines of Code
Compact Functions
DREAM Phoenix Hex-Rays
DREAM Phoenix Hex-Rays
DREAM Phoenix Hex-Rays
0
0
0
0
0
0
0
40
4,505
31
4,231
N/A
N/A
N/A
47
3,166
28
2,949
1,571
446
144
93k
196k
15k
107k
42k
24k
7k
243k
422k
30k
164k
N/A
N/A
N/A
120k
264k
18k
135k
53k
28k
9k
81.3%
81%
74.9%
75.2%
82.9%
69.9%
84.8%
0.3%
0.2%
1.1%
0.7%
N/A
N/A
N/A
32.1%
30.4%
36.2%
31.3%
14.5%
25.7%
12.3%
TABLE III: Structuredness and compactness results. For the coreutiles benchmark, we denote by Fx the set of functions
decompiled by compiler x. F r
x is the set of recompilable functions decompiled by compiler x. d represents DREAM, p represents
Phoenix, and h represents Hex-Rays.
100% due to the fact that multiple decompilers could generate
the same minimal number of lines of code. In a one on one
comparison between DREAM and Phoenix, DREAM scored
98.8% for the compactness of the decompiled functions. In
a one on one comparison with Hex-Rays, DREAM produced
more compact code for 72.7% of decompiled functions.
5) Malware Analysis: For our malware analysis, we picked
three malware samples from three families: ZeusP2P, Cridex,
and SpyEye. The results for the malware samples shown