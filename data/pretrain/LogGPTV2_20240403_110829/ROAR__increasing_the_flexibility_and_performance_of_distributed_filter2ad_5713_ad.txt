Query Delay
Target Delay
CPU Load
First node failed
20 nodes failed
 50
 150
 200
 100
Time (s)
Query Delay
First node failed
20 nodes failed
 100
 80
 60
 40
 20
 0
 0
 2000
 1500
 1000
 500
)
%
(
e
g
a
s
U
U
P
C
)
s
m
l
(
y
a
e
D
y
r
e
u
Q
 0
 50
 100
 150
 200
 250
Time(s)
 0
 0
 50
 100
Time (s)
 150
 200
Figure 13: Effects of 20 Node Failures on ROAR
Fast Load Balancing: Query Delay Histogram
 50
 40
 30
 20
 10
 0
 0
 4
 3.5
 3
 2.5
 2
 1.5
 1
 0.5
 0
 100
 90
 80
 70
 60
 50
 40
 30
 20
 10
 0
P
)
s
(
y
a
e
D
l
)
%
(
d
a
o
L
m
e
t
s
y
S
 0
 50
 100
 150
 200
 250
Time(s)
Figure 12: ROAR Changing p Dynamically
Google’s case). Bandwidth utilization depends on p too. In some
cases rather complex optimization functions might be required; in
any event, a ROAR system can implement the required changes of
p so long as an optimization function can be deﬁned that captures
the relevant constraints.
5.4 Node Failures
What is the impact of server failures on ROAR? We are more
interested in short term effects, as in the long run the load balancing
mechanism evens out load across all the servers (see next section).
To test the impact, we set p = 20, so that r was very small
(approximately 2). This reduces ROAR’s options for alternative
servers to the bare minimum, and hence represents a worst case for
the increase in load on the remaining nodes caused by a node fail-
ure. With this setup, we ran queries at a rate of six per second, then
killed a single server. Query delays remained roughly the same.
We noticed a small increase in CPU load of roughly 10% for the
two neighbors of the failed node. This agrees with our analytic
predictions in Section 4.4.
In the second experiment we generated queries at a lower rate (3
per second) and progressively killed 20 out of the 47 servers. To
maintain correctness, we did not kill consecutive servers because
with such an artiﬁcially small value of r there was not much redun-
dancy. The effect on query delay and server CPU load is plotted in
Figure 13. The average CPU load doubles for most servers, as ex-
pected, though query delays only increase marginally for this work-
load. Clearly if the initial workload had been higher than 50%, this
failure would have pushed load above 100% and so query delays
would have been affected. In such a scenario the correct course of
action would then be to decrease p, as shown in Section 5.3.
y
t
i
l
i
b
a
b
o
r
P
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
p=5
p=5...6
p=5...10
 0
 500
 1000
 1500
 2000
 2500
 3000
 3500
Delay (ms)
Figure 14: Delay Distribution with Fast Load Balancing when
using pq > p
To summarize, the results show that ROAR handles node failures
gracefully, and so long as the load does not exceed 100%, query
execution is not disrupted.
5.5 Load Balancing
The previous experiments were conducted with homogeneous
servers. In a data center it is unlikely that all servers will be equally
fast, as machines are bought in batches and computing power in-
creases from one batch to another. To test this effect, we included
15 powerful machines in our testbed (each server with two quad-
core processors). These run the same million metadata query four
times faster than our slower servers.
balancing mechanisms(§4.6):
To cope with heterogeneous servers, ROAR implements two load
• The background process by which ranges migrate.
• A request scheduling mechanism implemented in the front-
end load balancer.
These run simultaneously, though on different timescales.
The front-end load balancer was not enabled in any of the exper-
iments up to this point, but with heterogeneous servers it helps sig-
niﬁcantly. We started all the servers, assigned them equal ranges,
set p = 5 (r (cid:4) 9), and generated six queries per second. Fig-
ure 14 shows the distribution of delays when the front-end load
balancer is turned off (p = 5), when it is allowed one extra sub-
query (p = 5...6), and when it is allowed to increase pq as high as
299p=5
p=5,6
p=5,6,7,8,9,10
)
%
(
d
a
o
L
U
P
C
 100
 80
 60
 40
 20
 0
 0
 10  20  30  40  50  60  70  80  90
Time (s)
 100
 80
 60
 40
 20
 0
 0
 10
 20
 30
 40
 50
Time (s)
 60
 70
 80
 90
 100
 80
 60
 40
 20
 0
Figure 15: Fast Load Balancing with pq > p
)
%
(
d
a
o
L
U
P
C
)
s
(
y
a
e
D
l
 100
 80
 60
 40
 20
 0
 0
 5
 4
 3
 2
 1
 0
 500
 400
 300
 200
 100
)
U
P
C
/
e
g
n
a
R
(
d
a
o
L
 0
 0
After Load Balance
Before Load Balance
 5
 10
 15
 20
 25
 30
 35
 40
 45
Computer Number
Figure 16: Range Load Balancing
10 if needed. It is clear that this mechanism is effective at moving
load onto the faster servers.
Figure 15 shows the load on the machines as the load balancer
learns which machines are fastest. In the p = 5 graph, we can see
a band in CPU load at around 12.5%; this corresponds to the fast
servers which are given similar workload to the slower servers. As
pq is allowed to increase, this band moves up, and the upper band
(the slow servers) moves down. When pq is allowed to grow up to
10, sometimes slow servers are not given any work, simply because
all the load can be processed quicker on the fast servers. When the
load is increased, the slow servers start to be used again.
To test the long-term range load balancing, we started the servers
with equal ranges and ran one query per second. Before long the
front-end servers compute a new conﬁguration for the network where
ranges are better balanced. The load balancing procedure iterates
many times, evening out ranges between neighbors where the load
difference is greater then 1.5.
The results are encouraging: the big range differences between
neighbors are amortized (Fig. 16). The zig-zag shape of the result-
ing load allocation is the effect of the distributed, neighbor-only
load balancing mechanism. The effects of load balancing are clear
in Fig. 17. This range expansion increases the effectiveness of the
front-end balancer: for light loads most servers are not used at all,
as the powerful servers can run all the queries in less time.
Many of these unused servers can actually be put to sleep to save
electricity. They do however need to be updated when they are
woken again. One strategy is to wake some of them periodically for
updates to reduce the wake up time when they are actually needed.
5.6 Cross-Sectional Bandwidth Usage
Typical data-center networking architectures connect racks of
servers with one switch per rack, and have one or two layers of