20
21
22 End
end
move I into next_cc
cc.map[Mi] ← next_cc
cc ← next_cc
end
return NewCovExplored
end
MF , these seeds are likely to have different edge coverage,
resulting in multiple different sub-clusters. However, if we use
ME to cluster seeds prior to MF , since seeds with the same
edge coverage must also have the same function coverage, it is
impossible further to put them into different sub- MF clusters.
As a result, each ME node will have only a single MF node,
making the clustering useless.
As discussed in [45], (cid:31)s is a partial order, so it is possible
that two metrics are not comparable. To solve this problem,
we propose a weaker principle: given two non-comparable
coverage metrics, we should cluster a seed with the metric
that will select fewer seeds before the one that will select more
seeds.
2) Examples: Following the above principles, we propose
two multi-level coverage metrics as examples. Both examples
use three-level clustering that works well in our evaluation.
The top-level metric in both examples is CF , which
measures the function coverage. The middle-level metric is
edge coverage CE. Functions invoked are essential runtime
features that are commonly used to characterize an execution,
and edge coverage is widely used in fuzzers such as AFL and
LIBFUZZER. Notably CE (cid:31)sCF .
6
The most important one is the bottom-level metric, which
is the most sensitive one. In this work, we mainly evaluated a
bottom-level metric called distance metric CD. It traces con-
ditional jumps (i.e., edges) of a program execution, calculates
the hamming distances of the two arguments of the conditions
as covered features, and treats each observed new distance of
a conditional jump as new coverage. Unlike CE or CF that
traces control ﬂow features, CD focuses on data-ﬂow features
and actively accumulates progress made in passing condition
checks for fuzzing.
To understand whether our approach can support different
coverage metrics (ﬁtness functions), we also evaluated another
coverage metric called memory access metric CA. As the
name implies, this metric traces all memory reads and writes,
captures continuous access addresses as array indices, and
treats each new index of a memory access as new coverage. CA
pays attention to data ﬂow features and accumulates progress
made in accessing arrays that might be long. To distinguish
memory accesses that happen at different program locations,
the measurement also includes the address of the last branch.
However, since not all basic blocks contain memory accesses,
CA is not directly comparable to CE using sensitivity. How-
ever, we observe that CA can generate much more seeds than
CE, so CA comes after CE and its measurement MA stays at
the bottom level.
IV. HIERARCHICAL SEED SCHEDULING
This section discusses how to schedule seeds against hier-
archical clusters generated by a multi-level coverage metric.
A. Scheduling against A Tree of Seeds
Conceptually, a multi-level coverage metric C n ∼
(cid:104)C1 ··· Cn(cid:105) organizes coverage measurements (Mi) and seeds
as a tree, where each node at layer (or depth) i ∈ {1,··· , n}
represents a cluster represented by Mi and its child nodes at
layer i+1 represent sub-clusters represented by Mi+1. At leaf-
level, each node is associated with real seeds. Additionally, at
layer 0 is a virtual root node representing the whole tree. To
schedule a seed, the scheduler needs to seek a path from the
root to a leaf node.
Exploration vs Exploitation. The main challenge a seed
scheduler faces is the trade-off between seed exploration
(trying out other fresh seeds) and exploitation (keep fuzzing
a few interesting seeds to trigger a breakthrough). On the one
hand, fresh seeds that have rarely been fuzzed may lead to
surprisingly new coverage. On the other hand, a few valuable
seeds that have led to signiﬁcantly more new coverage than
others in recent rounds encourage to focus on fuzzing them.
Organizing seeds in a tree with hierarchical clusters facil-
itates a more ﬂexible control over the seed exploration and
exploitation. Speciﬁcally, fuzzers can focus on a single cluster
in which seeds cover the same functions at the ﬁrst layer and
then try out many (sub-)clusters with seeds exercising different
edges at the second layer. Alternatively, fuzzers can also try
out seeds exercising different groups of functions, then only
pick seeds covering some speciﬁc edges.
In this work, we explore the feasibility of modeling the
fuzzing process as a multi-armed bandit (MAB) problem [49],
[54] and using the existing MAB algorithms to balance
between exploitation and exploration. After trying several
different MAB algorithms, we decide to adopt
the UCB1
algorithm [2], [5] to schedule seeds, since it works the best
empirically, despite being one of the simplest MAB algorithms.
As illustrated by function SelectNextSeedToFuzz()
in Algorithm 3, starting from the root node, our scheduling
algorithm selects the child node with the highest score, which
is calculated based on the coverage measurements, until reach-
ing the last layer to select among leaf nodes that are associated
with real seeds. Because all seeds have the same coverage
at the leaf level, we scheduling them with round robin for
simplicity.
At the end of each round of fuzzing, nodes along the
scheduled path will be rewarded based on how much progress
the current seed has made in this round, e.g., whether there
are new coverage features exercised by all the generated test
cases. In this way, seeds that perform well are expected to have
increased scores for competing in the following rounds, while
seeds making little progress will be de-prioritized.
Note that a traditional MAB problem assumes a ﬁxed
number of arms (nodes in our case) so that all arms can get
an estimation of their rewards at the beginning. However, our
setup breaks this assumption since the number of nodes grows
as more and more seeds are generated. To address this issue,
we introduce a rareness score of a node, so that each new
node will have an initial score to differentiate itself from other
new nodes. We will discuss seed scoring in more detail later
in §IV-B.
to perform seed scheduling. However,
It is also worth mentioning that a recent work Ecofuzz [54]
proposed using a variant of the adversarial multi-armed bandit
(AMAB) model
it
can not solve the seed exploration problem caused by more
sensitive coverage metrics, as it attempts to explore all existing
seeds at least once. Moreover, we have also experimented with
the EXP3 algorithm that aims to solve the AMAB problem;
but it performed worse than UCB1 in our setup.
Algorithm 3: Seed Scheduling Algorithm
Input: seed set S
Output: return the seed to fuzz
Data: the tree T with n layers
current working tree node cx
T ← S.tree
cx ← T.root
foreach i ∈ {1,··· , n} do
1 Function SelectNextSeedToFuzz(S):
2
3
4
5
6
7
8
9
10 End
11
children ← cx.child_nodes
cx ← argmaxx∈childrenScore(x)
end
s ← cx.next_seed()
return s
B. Seed Scoring
How to score seeds directly affect the trade-off between
exploration and exploitation. First, for exploitation, seeds that
have performed well recently should have high scores as they
are expected to make more progress. Second, for exploration,
the scoring system should also consider the uncertainty of
rarely explored seeds. We extended the UCB1 algorithm [2],
[5] to achieve a balance between exploitation and exploration.
From a high level, our scoring method considers three aspects
of a seed: (1) its own rareness, (2) easiness to discover new
seeds from this seed, and (3) uncertainty.
In order to discuss this in more detail, let us ﬁrst deﬁne
some terms more formally. First, we deﬁne the hit count of
a feature F ∈ Γl at level l as the number of test cases ever
generated that cover the feature.
Deﬁnition IV.1. Let P be the program under fuzzing, I be
the set of all test cases that have been generated so far. The
hit count of a feature F is num_hits[F ] = |{I ∈ I : F ∈
Cl(P, I)}|.
As observed in [9], features that are rarely exercised by test
cases deserve more attention because they are not likely to be
exercised by valid inputs. The rareness of a feature describes
how rarely it is hit, which is the inverse of the hit count.
Deﬁnition IV.2. The rareness of a feature F is rareness[F ] =
1
num_hits[F ]
Before describing how we calculate the reward of a round
of fuzzing, we ﬁrst deﬁne the feature coverage of fuzzing seed
s at round t.
Deﬁnition IV.3. Let P be the program under fuzzing, Is,t be
the set of test cases generated at round t via fuzzing seed s.
We denote the feature coverage at level Cl, l ∈ {1,··· , n} as
f cov[s, l, t] = {F : F ∈ C(P, I) ∀I ∈ Is,t}
Next, we describe how we calculate the reward to the
seed just fuzzed after a round of fuzzing. An intuitive way
is to count the number of new features covered as the reward.
However, we quickly noticed that this does not work well. As
the fuzzing campaign goes on, the probability of exercising
new coverage is dramatically decreased, indicating that a seed
can hardly obtain new rewards. Consequently, the mean reward
of seeds may quickly decrease to zero. When we have many
seeds with minor variances near zero, the UCB algorithm
cannot properly prioritize seeds. Moreover, under the common
observation that infrequent coverage features deserve more
exploration than others, seeds that can lead to inputs that
exercise rare features are deﬁnitely more valuable, even if they
do not cover new features. Motivated by these observations,
we take the rareness of the rarest feature that is exercised by all
generated inputs as the reward to the schedule seed. Formally,
for a seed s that is fuzzed at round t, its fuzzing reward w.r.t.
coverage metric Cl is
SeedReward(s, l, t) =
max
F∈f cov[s,l,t]
(rareness[F ])
(1)
Based on the seed reward, we compute the reward to a
7
cluster by propagating seed rewards to clusters scheduled at
let (cid:104)a1, . . . , an, an+1(cid:105) be the
upper levels. More formally,
sequence of nodes (in the seed tree) selected at round t, where
an+1 is the seed node for s and ai is coverage measurements
for the corresponding clusters. Since scheduling node al affects
the following scheduling of nodes al+1,··· , an at
lower
layers, the reward of node al as feedback consists of the seed
reward regarding coverage levels l, l + 1,··· , n as illustrated
in Equation 2. Note that we use the geometric mean here since
it can handle different scalars of the involved values with ease.
n − l + 1(cid:115) (cid:89)
l≤k≤n
Reward(al, t) =
SeedReward(s, k, t) (2)
Right now, we are able to estimate the expected perfor-
mance of fuzzing a node using the formula of UCB1 [2], [5].
Formally, the fuzzing performance of a node a is estimated as
F uzzP erf (a) = Q(a) + U (a)
(3)
Q(a) is the empirical average of fuzzing rewards that a
obtains so far, and U (a) is radius of the upper conﬁdence
interval.
Unlike UCB1 which calculates Q(a) using the arithmetic
mean of the rewards that node a obtains so far, we use the
weighted arithmetic mean instead. More speciﬁcally, during
the fuzzing, the rareness of a feature is decreasing as it is
exercised by more and more test cases. As a result, even the
same fuzzing coverage can lead to different fuzzing rewards
for mutating a seed: the reward of an earlier round might be
signiﬁcantly higher than that of a later round. To address this
issue, we introduce a discount factor as weight in order to
favor newer rewards rather than older ones. More formally,
given a node a that is selected for round t, we update its
weighted mean at the end of round t in such a way that we
progressively decrease the weight to the previous mean reward
in order to give higher weights to newer rewards as illustrated
in Equation 4
Reward(a, t) + w × Q(a, t(cid:48)) × N [a,t]−1(cid:80)
wp
1 + w × N [a,t]−1(cid:80)
p=0
(4)
wp
Q(a, t) =
p=0
N [a, t] denotes the number of times that node a has been
selected so far at the end of round t, t(cid:48) is the last round at which
node a was selected, and w is the discount factor. Note that
the smaller w is, the more we ignore the past rewards. When
w is set to 0, all the past rewards are ignored. To study how
w affects the fuzzing performance, we conduct an empirical
experiments (§V-F). Based on the results, we empirically set
w to 0.5 in our evaluation.
U (a) is the estimated radius factoring in the number of
times a has been selected. In addition, we also consider the
number of seeds that a contains based on the insight that
nodes with more seeds should be scheduled more for seed
8
exploration. More formally, given a seed a and its parent a(cid:48),
we calculate U (a) as
(cid:115)
(cid:115)
U (a) = C ×
Y [a]
Y [a(cid:48)]
×
log N [a(cid:48)]
N [a]
(5)
Y [a] denotes the number of seeds in the cluster of node a,
and N [a] denotes the times a has been selected so far. C is a
pre-deﬁned parameter that conﬁgures the relative strength of
exploration and exploitation. In particular, a larger C results
in a relatively wider radius in Equation 3, which encourages
exploring fresh nodes that have been fuzzed fewer times. This
can help a fuzzer get out of code regions that are too hard to
solve. On the contrary, a smaller C indicates that the empirical
average of fuzzing rewards gets weighted more, thus promoting
nodes that have recently led to good progress. As a result,
the fuzzer will focus on these nodes and is expected to reach
more new code coverage. To further demonstrate how it affects
the fuzzing performance, we fuzz the CGC benchmark with
different values of C and show the results in §V-F. Based on
the results, we set C to 1.4 in our evaluation.
The fuzzing performance estimated by Equation 3 based
on fuzzing coverage is limited by what can be observed. This
limitation can impact seeds that have never been scheduled
and seeds that exercise rare features themselves but usually
lead to inputs that exercise high-frequency features (e.g., for a
program with rigorous input syntax checks, random mutations
usually lead to invalid paths, hence lowering the reward).
To mitigate this limitation, when evaluating a seed, we also
consider features that it exercises. Particularly, we calculate the
rareness of a seed via aggregating the rareness of features that
it covers. More formally, let P be the program under fuzzing,
given a seed s, its rareness regarding Ml, l ∈ {1,··· , n} is
SeedRareness(s, l) =
F∈Cl(P,s) rareness2[F ]
|{F : F ∈ Cl(P, s)}|
(6)
Note that here we take quadratic mean rather than, e.g.,
arithmetic mean because it preserves more data diversity. The
rareness of a node al measured by Ml is completely decided
by its child seeds as they share the same coverage regarding
Ml. Let (cid:104)a1,··· , an, an+1(cid:105) be the sequence of nodes selected
at round t, where an+1 is the leaf node representing a real
seed s, then at the end of round t the rareness of node al is
updated as
Rareness(al) = SeedRareness(s, l)
(7)
Notably, we update the rareness score of seeds and nodes
lazily for two reasons. First, it reduces the performance over-
head. Second, it can lead to overestimating the rareness of a
node that has not been fuzzed for a long time, so that seed is
more likely to be scheduled.
In addition to updating the rareness of a node picked in
the past round, we also calculate the rareness of each new
node similarly. As discussed previously, this makes each new