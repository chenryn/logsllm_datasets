title:Old but Gold: Prospecting TCP to Engineer and Live Monitor DNS
Anycast
author:Giovane C. M. Moura and
John S. Heidemann and
Wes Hardaker and
Pithayuth Charnsethikul and
Jeroen Bulten and
João M. Ceron and
Cristian Hesselman
Old but Gold: Prospecting TCP
to Engineer and Live Monitor DNS
Anycast
Giovane C. M. Moura1,2(B), John Heidemann3, Wes Hardaker3,
Pithayuth Charnsethikul3, Jeroen Bulten4, Jo˜ao M. Ceron1,
and Cristian Hesselman1,5
1 SIDN Labs, Arnhem, The Netherlands
PI:EMAIL
2 TU Delft, Delft, The Netherlands
3 USC/ISI, Marina Del Rey, USA
4 SIDN, Arnhem, The Netherlands
5 University of Twente, Enschede, The Netherlands
Abstract. DNS latency is a concern for many service operators: CDNs
exist to reduce service latency to end-users but must rely on global DNS
for reachability and load-balancing. Today, DNS latency is monitored by
active probing from distributed platforms like RIPE Atlas, with Verf-
ploeter, or with commercial services. While Atlas coverage is wide, its
10k sites see only a fraction of the Internet. In this paper we show that
passive observation of TCP handshakes can measure live DNS latency,
continuously, providing good coverage of current clients of the service.
Estimating RTT from TCP is an old idea, but its application to DNS
has not previously been studied carefully. We show that there is suﬃcient
TCP DNS traﬃc today to provide good operational coverage (particu-
larly of IPv6), and very good temporal coverage (better than existing
approaches), enabling near-real time evaluation of DNS latency from
real clients. We also show that DNS servers can optionally solicit TCP to
broaden coverage. We quantify coverage and show that estimates of DNS
latency from TCP is consistent with UDP latency. Our approach ﬁnds
previously unknown, real problems: DNS polarization is a new problem
where a hypergiant sends global traﬃc to one anycast site rather than
taking advantage of the global anycast deployment. Correcting polar-
ization in Google DNS cut its latency from 100 ms to 10 ms; and from
Microsoft Azure cut latency from 90 ms to 20 ms. We also show other
instances of routing problems that add 100–200 ms latency. Finally, real-
time use of our approach for a European country-level domain has helped
detect and correct a BGP routing misconﬁguration that detoured Euro-
pean traﬃc to Australia. We have integrated our approach into several
open source tools: ENTRADA, our open source data warehouse for DNS,
a monitoring tool (Anteater), which has been operational for the last
2 years on a country-level top-level domain, and a DNS anonymization
tool in use at a root server since March 2021.
c(cid:2) The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
O. Hohlfeld et al. (Eds.): PAM 2022, LNCS 13210, pp. 264–292, 2022.
https://doi.org/10.1007/978-3-030-98785-5_12
Old but Gold: Prospecting TCP to Engineer
265
1 Introduction
Latency is a key performance indicator for many DNS operators. DNS latency
is seen as a bottleneck in web access [65]. Content Delivery Networks (CDNs)
are particularly sensitive to DNS latency because, although DNS uses caching
extensively to avoid latency, many CDNs use very short DNS cache lifetimes
to give frequent opportunities for DNS-based load balancing and replica selec-
tion [14]. Latency is less critical at the DNS root [25], but unnecessary delay
should be avoided [7]. Because of public attention to DNS latency, low latency
is a selling point for many commercial DNS operators, many of whom deploy
extensive distributed systems with tens, hundreds, or more than 1000 sites [8].
DNS deployments often use IP anycast [33,47] to reduce latency for clients. A
DNS service is typically provided by two or more authoritative DNS servers [22],
each deﬁned in DNS on a separate IP address with an NS record [35]. With IP
anycast, the IP address assigned to the authoritative DNS server is announced
from many physically distributed sites, and BGP selects which clients go to
which site—the anycast catchment of that site. DNS clients often select the
lowest-latency authoritative server when they have a choice [40,43]. We show
(Sect. 5) that this preference shifts client traﬃc to sites with the lowest laten-
cies. (Although we focus on anycast, our approach can also be used to evaluate
multiple unicast services serving the same zones).
DNS latency has been extensively studied [9,42,59]. Previous studies have
looked at both absolute latency [59] and how closely it approaches speed-of-light
optimal [28,63]. Several studies measure DNS latency from measurement systems
with distributed vantage points such as RIPE Atlas [53], sometimes to optimize
latency [7,34]. Recent work has shown how to measure anycast catchments with
active probes with Verfploeter [12,13], and there is ongoing work to support RTT
measurements. However, approaches to measure latency provide mixed coverage:
large hardware-based measurements like RIPE Atlas only have about 11k active
vantage points and cover only 8670 /24 IPv4 network preﬁxes [51] (May 2020), and
commercial services have fewer than that. Verfploeter provides much better cov-
erage, reaching millions of networks, but it depends on a response from its targets
and so cannot cover networks with commonly-deployed ICMP-blocking ﬁrewalls.
It is also diﬃcult to apply to IPv6 since it requires a target list, and eﬀective IPv6
target lists are an open research problem [16]. Finally, with the cost of active prob-
ing, Verfploeter is typically run daily and is too expensive to detect hourly changes
(and RTT measurement support will require twice as much probing).
The main contribution of this paper is to fully evaluate and show opera-
tional results from passive latency observations in DNS. We show that passive
observations of latency in TCP can provide continuous updates of latency with
no additional traﬃc, providing operationally-useful data that can complement
active probing methods such as Verfploeter or static observers such as RIPE
Atlas. Such observations are not possible with DNS over UDP, and active prob-
ing is typically less frequent.
Observing latency from TCP, and in DNS, is not completely new, but prior
work has not validated its accuracy and coverage. The TCP handshake has been
266
G. C. M. Moura et al.
used to estimate RTT at endpoints since 1996 [20], and it is widely used in
passive analysis of HTTP (for example, [57]). Even in DNS, using TCP has been
touched upon—the idea was shared with us by Casey Deccio, and .cz operators
have used it in their service as described in non-peer-reviewed work that was
independent from ours [31,32]. We validate that latency measured from UDP
and our estimates from TCP match (Sect. 2.2). We show that DNS servers can
choose to solicit TCP from selected clients to increase coverage, if they desire,
with an implementation in Knot (Sect. 2.1).
Our second contribution is to show that TCP-handshakes provide an eﬀec-
tive estimate of DNS latency. Although DNS most often uses UDP, leaving
DNS-over-TCP (shortened to DNS/TCP) to be often overlooked, we show that
there is enough DNS/TCP traﬃc to support good coverage of latency estima-
tion. Prospecting through DNS traﬃc can ﬁnd the latency “gold”. Unlike prior
approaches, passive analysis of TCP provides more coverage as busy clients send
more queries, some with TCP. It provides good coverage of DNS traﬃc: for .nl,
the top 100 ASes that send DNS/TCP traﬃc are responsible for more than 75%
of all queries (Sect. 2.1), and we cover recursive servers sending the majority
of queries. By scaling coverage with actual traﬃc, continuous passive RTT esti-
mation can increase temporal coverage beyond current active approaches. For
.nl, we cover 20k ASes every hour (Sect. 2.1). Finally, passive analysis is the
only approach that provides good coverage for IPv6 networks, overcoming the
problem of active probing with stateless IPv6 addresses [46].
Our ﬁnal contribution is to show that TCP-based latency estimation mat-
ters—it detects latency problems in operational networks, improving latency
engineering in anycast (Sect. 4). We identify DNS polarization as a problem
that occurs when an Internet “hypergiant” [17,27,48] with a global footprint
sends traﬃc over their own backbone to a single anycast location rather than
taking advantage of an existing global anycast service. We show the importance
of detecting and correcting this problem, reducing latency inﬂation by 150 ms for
many clients of Google and Microsoft as they access .nl ccTLD and two com-
mercial DNS providers. We have instrumented our open-source ENTRADA [61,70]
with DNS/TCP RTT analysis. We provide a new tool, Anteater, that ana-
lyzes DNS/TCP RTT continuously to detect errors and failures in real-time (we
released it freely at [37]), and extend an existing DNS analysis tool (dnsanon).
These tools have been operational for more than two years at SIDN, the Nether-
lands ccTLD (.nl) operator, and were deployed in March 2021 by the B-Root
root DNS server. During that deployment, our tools have detected several prob-
lems. In one case, some users experienced large increases in RTT due to traﬃc
from Europe going to an anycast site in Australia (Sect. 4.4).
Our tools are freely available, including our changes to Knot [10], dnsanon,
Anteater, and ENTRADA. Part of our data is from public TLDs, so privacy concerns
prevent making data public. Our analysis follows current ethical guidelines: we
never associate data with information about speciﬁc individuals, and our analysis
is part of improving operations.
Old but Gold: Prospecting TCP to Engineer
267
2 DNS/TCP for RTT?
While UDP is the preferred transport layer for DNS, TCP support has always
been required to handle large replies [6] and all compliant resolvers are required
to use TCP when the server sets the TC (truncated) bit [35]. TCP has also
always been used for zone transfers between servers, and now increasing numbers
of clients are using TCP in response to DNSSEC [2], response-rate limiting [66],
and recently DNS privacy [23].
The RTT between a TCP client and server
can be measured passively during the TCP ses-
sion establishment [20,34] or during the connec-
tion teardown [57]. In our work, we measure the
RTT during the session establishment, as shown
in Fig. 1: we derive the RTT between client
and server by computing the diﬀerence between
times s2 and s1, measured at the server. (In Sect.
2.2 we validate against client measurements of
transaction time c1 and c3, which will be two
RTTs (plus usually negligible server processing
time)).
Fig. 1. TCP handshake and
RTT measurements
When we have multiple observations per tar-
get region (AS or preﬁx), we take the median. We choose median so that frequent
retries will change the result, but occassional retries will not.
For passive TCP observations to support evaluation of anycast networks for
DNS, (a) enough clients must send DNS over TCP so they can serve as vantage
points (VPs) to measure RTT, and (b) the RTT for queries sent over TCP and
UDP should be the same.
We next verify these two requirements, determining how many clients can
serve as VPs with data from three production authoritative servers (Sect. 2.1) –
two from the .nl zone, and B-root, one of the Root DNS servers [56]. We then
compare the RTT of more than 8k VPs with both TCP and UDP to conﬁrm
they are similar (Sect. 2.2), towards two large anycast networks: K and L-Root,
two of the 13 anycast services for the Root DNS zone.
2.1 Does TCP Provide Enough Coverage?
To assess whether DNS/TCP has enough coverage in production authoritative
servers, we look at production traﬃc of two DNS zones: .nl and the DNS Root.
For each zone we measure: (a) the number of resolvers using the service; (b)
the number of ASes sending traﬃc; (c) the fraction of TCP queries the servers
receive; (d) the percentage of resolvers using both UDP and TCP; and (e) the
RTT of the TCP packets.
Our goal is to get a good estimate of RTT latency that covers recursive
servers accounting for the majority of client traﬃc. If every query were TCP,
we could determine the latency of each query and get 100% coverage. However,
most DNS queries are sent over UDP instead of TCP.
268
G. C. M. Moura et al.
Table 1. DNS usage for two authoritative services of .nl (Oct. 15–22, 2019).
Queries
Resolvers
ASes
Anycast A Anycast B Any. A Any. B Any. A Any. B
5 237 454 456 5 679 361 857 2 015 915 2 005 855
42 253
42 181
4 005 046 701 4 245 504 907 1 815 519 1 806 863
41 957
41 891
3 813 642 861 4 128 517 823 1 812 741 1 804 405
41 947
41 882
191 403 840
116 987 084
392 434
364 050
18 784
18 252
Total
IPv4
UDP
TCP
ratio TCP
5.02%
2.83%
21.65% 20.18% 44.78% 43.58%
IPv6
1 232 407 755 1 433 856 950
200 396
198 992
UDP
TCP
1 160 414 491 1 397 068 097
200 069
198 701
71 993 264
36 788 853
47 627
4 6190
7 664
7 662
3 391
7 479
7 478
3 354
ratio TCP
6.2%
2.63%
23.81% 23.25% 44.26% 44.85%
We, therefore, look for recursive representation—if we have a measured query
over TCP, is its RTT the same as the RTTs of other queries that use UDP, or that
are from other nearby recursive resolvers? If network conditions are relatively sta-
ble, the TCP query’s RTT can represent the RTT for earlier or later UDP queries
from the same resolver. Since /24 IPv4 preﬁxes (and /56 IPv6 preﬁxes) are usu-
ally co-located, DNS/TCP measurements from one IP can also represent other
resolvers in the same preﬁx. Our goal is to ﬁnd latency for DNS recursive resolvers,
not all client networks—since recursive resolvers that generate the most traﬃc are
most likely to send TCP queries, we expect good coverage even if TCP use is rare.
.nl currently (Oct. 2019) has four Authorita-
.nl Authoritative Servers.
tive DNS services, each conﬁgured to use IP anycast. We next examine data
from two of these services. Anycast Services A and B employ 6 and 18 sites
distributed globally. Each is run by a third-party DNS operator, one headquar-
tered in Europe and the other in North America. They do not share a commercial
relationship, nor do they share their service infrastructure.
DNS/TCP Usage: we analyze one week of traﬃc (2019-10-15 to -22) for each
service using ENTRADA. That week from each service handles about 10.9 billion
queries from about 2M resolvers spanning 42k Autonomous Systems (ASes), as