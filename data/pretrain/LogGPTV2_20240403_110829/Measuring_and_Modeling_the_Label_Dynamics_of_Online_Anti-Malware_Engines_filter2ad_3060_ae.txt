rected edges are weighted by the inﬂuence score between
the two engines. For each node, we sum its outgoing edges’
weights as an indicator of its active inﬂuence to other nodes.
We visualize one inﬂuence graph in Figure 17 in the Ap-
pendix. AegisLab, F-Secure, Comodo, and Arcabit are mostly
inﬂuenced by other engines, but their main inﬂuencer sets
are not identical (with some overlaps). For example, only Co-
modo is highly inﬂuenced by Cylance and Qihoo-360; only
AegisLab is highly inﬂuenced by Microsoft.
To understand the inﬂuence of each engine, we plot Fig-
ure 9, where x and y represent the weighted sum of incom-
ing and outgoing edges. We use yellow color to mark the
reputable engines mentioned in previous literature (Table 5)
5Window sizes 14 and 28 generate similar results.
2370    29th USENIX Security Symposium
USENIX Association
115304560influenced115304560influencer0.000.150.300.450.60115304560influenced115304560influencer0.000.150.300.450.6051015incoming edges01234outgoing edgesy = xF-SecureIkarus51015incoming edges01234outgoing edgesF-Securey = x115304560influenced115304560influencer0.0000.0150.0300.0450.060115304560influenced115304560influencer0.0000.0060.0120.0180.0240.030and use red color to mark the two highly reputable engines
(Kaspersky and Symantec). For 0→1 ﬂips (Figure 9(a)), there
are 49 engines above the x = y line and 16 engines below the
line. One high-reputation engine (Ikarus) has the largest out-
going edge weight, indicating that it has the biggest active in-
ﬂuence. Interestingly, one high-reputation engine (F-Secure)
is more easily inﬂuenced compared to other reputable engines.
For 1→0 ﬂips (Figure 9(b)), the patterns are very similar.
Observation 7: Active inﬂuence widely exists between Virus-
Total engines. There are engines that are highly inﬂuenced by
many other engines at both ﬂip directions.
5.2.2 Passive Model
While the active inﬂuence model captures highly synchro-
nized actions (e.g., j ﬂips right after i’s ﬂip), passive inﬂuence
provides a different angle by showing j ﬂips towards i’s cur-
rent state. Given a ﬁle, if engine j ﬂips its label to l (i.e., “0”
or “1”) at time t, and engine i has already stayed on label l for
w days at t, we consider there is a passive inﬂuence from i to
j. Note that w represents the size of the time window within
which i keeps a stable label to inﬂuence j. For this analysis,
we use w = 7 as the window size6. We use Ai2 j to represent
the number of passive inﬂuence events from i to j and use Ai
to represent the number of subsequences with 7 consecutive
label l. For example, if i’s sequence is “111111111100011111”
and l=“1”, then |Ai| = 4. We compute the passive inﬂuence
score from i to j as pi, j = |Ai2 j|/|Ai|.
Engine Pair Analysis. We again use heatmaps to visual-
ize passive inﬂuence scores in Figure 10. Interestingly, the
1→0 ﬂip heatmap looks different from that of 0→1 ﬂip. Fig-
ure 10(a) shows engines highly inﬂuenced by all the other
engines under the active model are still highly inﬂuenced un-
der the passive model (the red vertical lines). However, the
result of 1→0 (Figure 10(b)) becomes less obvious under pas-
sive inﬂuence and there is no vertical line or horizontal line
in red. Combined with the active model’s result, it shows that
engines’ ﬂips from “malicious” to “benign” are more likely
inﬂuenced by other engines making the same ﬂips recently,
rather than engines that always stick to the “benign” label.
Passive Inﬂuence Graph. Figure 11 is the scatter plot for
passive model, where x and y represent the weighted sum of
incoming and outgoing edges. For 0→1 ﬂips (Figure 11(a)),
all high-reputation engines are around the top left corner, in-
dicating a strong inﬂuence to other engines. The exception is
again F-Secure, which is more likely to be inﬂuenced by oth-
ers. For 1→0 ﬂips (Figure 11(b)), the high-reputation engines
do not have a strong passive inﬂuence to others.
Observation 8: The passive inﬂuence is weak in general.
The passive inﬂuence is relatively stronger when a benign
label is ﬂipped to malicious.
6Changing the window size to 14 or 28 returns similar conclusions.
(a) 0→1
(b) 1→0
Figure 11: Passive Model: scatter plot of engines. x: weighted
sum of incoming edges, y: weighted sum of outgoing edges. reputable
engines are in orange color, and reputable engines* are in red color.
6 Analyzing the Ground-Truth Dataset
So far, we use the main dataset to understand the temporal dy-
namics of labels and the relationship between engines. While
the analysis is beneﬁted from the ﬁle diversity and longitudi-
nal data, it says little about the “correctness” of engines. In
this section, we use the smaller ground-truth sets to examine
the threshold choices for label aggregation and quantify the
different detection capabilities of engines.
Individual Engine Accuracy
6.1
We start by looking at how well individual engines classify
malware and benignware. Recall that we have four ground-
truth sets, where Malware-I and Malware-II are generated
by obfuscating real-world malware, Benign-I is generated by
obfuscating goodware, and Benign-II is generated by recom-
piling goodware. All the ﬁles have a clean signature (means
they are never scanned by VirusTotal before, no prior history).
Note that the ground-truth sets are related to ransomware —
they allow us to examine questions about VirusTotal engines’
detection correctness, but the results should mostly reﬂect the
engines’ capability of analyzing ransomware.
In Figure 12, we compare the detection result on the ﬁrst-
day of VirusTotal’s scanning and the last day of the scanning.
Note that, for malware sets, we show the true positive (TP)
rate (i.e, ratio of correctly identiﬁed malware). For benign sets,
we show the false positive (FP) rate (i.e., ratio of misclassiﬁed
benignware). A high TP rate and a low FP rate mean the
engine is more accurate. For malware sets, we show that
the engines on the last day (Figure 12(c)) are indeed more
accurate than that on the ﬁrst day (Figure 12(a)). In particular,
on the last day, 75.4% of the engines have a TP rate of nearly
100% on Malware-I dataset. Files in Malware-II are harder
to detect — only 24.6% of the engines have a near 100%
TP rate. Overall, the detection capability of different engines
varies signiﬁcantly. A large portion (20%–50%) of engines
only detects less than 50% of malware ﬁles.
Figure 12(b) and Figure 12(d) show that performance on
benign ﬁles is more stable when comparing the ﬁrst day and
the last day. Benign-II (ﬁles are not obfuscated) has almost
no false positive. However, engines produce a high FP rate
USENIX Association
29th USENIX Security Symposium    2371
1234incoming edges0.00.10.20.30.4outgoing edgesF-Secure1234incoming edges0.00.10.20.30.4outgoing edges(a) Malware sets on the ﬁrst day
(b) Benign sets on the ﬁrst day
(a) Malware sets and Benign-I
(b) Malware sets and Benign-II
Figure 14: Precision and recall for different threshold t when
using all engines.
(c) Malware sets on the last day
(d) Benign sets on the last day
Figure 12: CDF plots of the true positive (TP) rate and the
false positive (FP) rate of each engine.
Figure 13: Engines’ TP rate and FP rate when analyzing
Malware-I, Malware-II and Benign-I.
on Benign-I where obfuscation is applied. Only about 25%
of the engines have zero false positive on Benign-I. These
engines either have actually analyzed the ﬁles or report most
ﬁles as benign (i.e., having zero true positive when analyzing
the two malware sets).
About 75% of the engines have false positives on Benign-I.
A possible explanation is that those engines use “obfuscation”
as a feature for their malware detection without carefully
analyzing the ﬁles. About 25% of the engines have a high FP
rate (>70%), indicating that they have put a heavy weight on
the “obfuscation” feature.
To understand which engines are easily misled by obfusca-
tion, we present Figure 13. We combine Malware-I, Malware-
II and Benign-I to calculate the FP rate and the TP rate for
each engine (on the last day). All three datasets are obfus-
cated — if an engine has a high TP rate and a low FP rate
on these datasets, it means the engine has truly analyzed the
ﬁles’ behavior rather than relying on “obfuscation” to make a
decision. In practice, benign ﬁles may also use obfuscation to
prevent copyright infringements and reverse-engineering.
In Figure 13, engines at the bottom left corner has a near-
zero FP rate, but also cannot detect most of the malware (low
(a) Malware sets and Benign-I
(b) Malware sets and Benign-II
Figure 15: Precision and recall for different threshold t when
only using reputable engines.
TP rate). Engines at the top right corner are likely to heavily
rely on obfuscation as a feature — they detect most of the
malware and also mis-classify most obfuscated benignware.
Interestingly, a number of high-reputation engines belong to
this group (e.g., AVG, McAfee, Microsoft, Sophos, Symantec).
The most effective engines are located at the bottom right
corner. They manage to maintain a relatively low FP rate, and
still detect most malware (e.g., Jiangmin, Zillya, Kaspersky).
Observation 9: Obfuscation in benign ﬁles can easily cause
false positives to VirusTotal engines. High-reputation engines
are not necessarily better at handling obfuscation.
6.2 Aggregating Engines’ Labels
While individual engines have an uneven performance, it is
possible that label aggregation can help improve the overall
accuracy. Here we test two different aggregation methods.
First, we use a simple threshold t to determine if a ﬁle has a
malicious label. If t or more engines give a malicious label,
then we mark the ﬁle as malicious. The results are shown in
Figure 14. Second, we only consider the nine high-reputation
engines mentioned in previous works and apply the same
threshold method. The results are in Figure 15. We test two
different settings: “all malware + Benign-I” and “all malware
+ Benign-II” to isolate the impact of benign ﬁle obfuscation.
When we consider obfuscated benign ﬁles, as shown in
Figure 14(a), it is very difﬁcult to get a high precision without
signiﬁcantly scarifying recall. There is only a small range of
t (between t =45 and t =55) where we can detect half of the
malware with a 95%+ precision. The situation is not neces-
sarily better when we only consider high-reputation engines.
2372    29th USENIX Security Symposium
USENIX Association
0.20.40.60.81.0TP rate0255075100% of enginesMalware-IMalware-II0.20.40.60.81.0FP rate0255075100% of enginesBenign-IBenign-II0.20.40.60.81.0TP rate0255075100% of enginesMalware-IMalware-II0.20.40.60.81.0FP rate0255075100% of enginesBenign-IBenign-II0.250.500.751.00TP rate0.000.250.500.751.00FP rateKasperskySymantecJiangminZillyaF-SecureIkarus15304560t0.000.250.500.751.00rateprecisionrecall15304560t0.000.250.500.751.00rateprecisionrecall369t0.000.250.500.751.00rateprecisionrecall369t0.000.250.500.751.00rateprecisionrecallAs shown in Figure 15(a), it is almost equally difﬁcult to get
an over 90% precision without scarifying 50% of recall.
When we consider non-obfuscated benign ﬁles (Fig-
ure 14(b)), it is clear that using a small threshold t (between
2–15) is a good choice. However, when we only consider the
high-reputation engines (Figure 15(b)), it is better to stick to
an even smaller threshold (e.g., t < 3). If we require all the
nine high-reputation engines to vote “malicious”, then we
will again lose 50% of recall.
Observation 10: A small threshold value can balance the
precision and the recall as long as the benign ﬁles are not
obfuscated.
6.3 Comparing with Desktop Engines
A subset of anti-malware engines also provide their desk-
top versions. A prior work [57] shows VirusTotal often runs
stripped-down engine versions, and thus is more likely to miss
true malicious instances. Note that this conclusion is drawn
from URL scanning for phishing website detection [57]. Be-
low, we explore if the same conclusion applies to malware
scanning.
Experiment Setup. Out of all VirusTotal vendors [3], we
ﬁnd 36 vendors also offer desktop versions of their products
(the list of engines is shown in Table 5 in the Appendix). We
install them separately on 36 Windows-10 virtual machines
(version 1809). We validated that our obfuscated malicious
samples still have their malicious actions in the VM environ-
ment. For the four ground-truth datasets, we scan their ﬁles
four times (in four different days). For each time of the ex-
periment, we use fresh virtual machines and install the latest
versions of the desktop engines. We disconnect the Internet
while the engines scan the ﬁles, to prevent the engines from
uploading the ﬁles or reporting the results to their servers.
This allows us to isolate the analysis engines on the desktop
from the engines in the cloud (on VirusTotal) to compare
them fairly. It’s possible some desktop engines do not run
the analysis locally but solely rely on their remote servers for
analysis. To this end, we run the main experiment without the
Internet, and later run a validation test with the Internet.
Comparison Results (w/o Internet). All 36 engines have
some inconsistency between the desktop and VirusTotal ver-
sions. For each engine and each dataset, we calculate the
inconsistency rate, which is the number of ﬁles with differ-
ent detection results (between VirusTotal and desktop scans)
divided by the total number of ﬁles. We report the average
inconsistency rate over different experiment dates for the en-
gine.
All 36 engines have a non-zero inconsistency rate on mal-
ware datasets. The average inconsistency rate is 25.4% on
Malware-I and 29.2% on Malware-II. Some engines have
an inconsistency rate over 90% on Malware-I (98.6% on
ZoneAlarm, 90.3% on Tencent and 98.9% on Qihoo-360) be-
cause their VirusTotal engines can detect most malicious sam-
ples, but their desktop engines do not report any of them. The
inconsistency rates on the benign datasets are lower (23.4%
on Benign-I and 0% on Benign-II).
To examine which version is more accurate, we compare
precision, recall, and F1-score over the four datasets for each
engine. F1-score is the harmonic mean of precision and recall.
For precision, 26 engines (out of 36) have a higher average
precision on their desktop versions than VirusTotal across the
datasets. 25 engines have a higher average recall on VirusTo-
tal than their desktop versions. After computing F1-score to
merge precision and recall together, we ﬁnd that 24 engines
(out of 36) have a higher average F1-score on VirusTotal than
their desktop versions (20.2% higher on average). Overall,
the result shows the online engines at VirusTotal are more
aggressive and tend to cover more malware, while desktop
versions are more conservative to keep a small number of false
alarms. Our result is different from that of the URL scanning
reported in [57] (where vendors’ URL engines at VirusTotal
cover fewer phishing websites than their standalone versions).
Sanity Check (w/ Internet). We perform a sanity check