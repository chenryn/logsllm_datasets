r
P
Obfuscation,Bayesian
Optimal,Bayesian
Obfuscation,Optimal
Optimal,Optimal
)
d
p
,
h
,
f
,
ψ
(
y
c
a
v
i
r
P
0.160329
0.399895
0.930509
1.16883
1.35572
1.57467
Q
max
loss
(a) Euclidean dq(.) and Euclidean dp(.)
p
)
d
,
h
,
f
,
ψ
(
y
c
a
v
i
r
P
Obfuscation,Bayesian
Optimal,Bayesian
Obfuscation,Optimal
Optimal,Optimal
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0
0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
1.66545
1.66545
1.67123
Q
max
loss
2.39066
3.51999
4.01646
0
0
0.488575
0.488575
(c) Euclidean dq(.) and Hamming dp(.)
(d) Hamming dq(.) and Hamming dp(.)
Figure 5: Service-quality threshold Qmax
loss vs. Location privacy P rivacy(ψ, f, h, dp), for one single user. The
diﬀerent lines represent combinations of optimal (◦) and basic obfuscation (•) LPPMs tested against optimal
(· · · ) and Bayesian-inference (–) attacks. The service-quality threshold Qmax
loss is equal to the service quality
obtained by the basic obfuscation LPPM when the number of obfuscation levels used to perturb the location
varies from 1 to 30 (its maximum value).
more by choosing the Optimal LPPM. Hence, the (Optimal,
Optimal) combination is a stable equilibrium for both.
The fourth combination (Obfuscation, Bayesian) illustrates
an interesting behavior. For small quality thresholds Qmax
loss
(or, equivalently, smaller obfuscation levels) the user’s pri-
vacy is lower compared with the (Optimal, Optimal) case.
However, at some middle point its provided privacy increases
and surpasses the privacy obtained from the optimal meth-
loss , the optimal LPPM uses all
ods.
its available capacity to increase privacy by distributing the
user’s pseudolocations over a higher number of locations.
So, it performs better than the basic obfuscation LPPM,
which is limited to distributing pseudolocations only in a
small set of regions. But when the obfuscation level (or,
similarly, the service-quality threshold) increases, the basic
Indeed, for small Qmax
obfuscation LPPM does better. First, because it is no longer
severely limited, and, second, because it is paired against the
Bayesian inference attack, which is weaker than the optimal
inference attack.
6. RELATED WORK
The ﬁeld of location privacy has been a very active area of
research in recent years. Work on this topic can be roughly
classiﬁed in three categories: mainly focused on the design of
LPPMs; mainly focused on recovering actual user trajecto-
ries from anonymized or perturbed traces; or mainly focused
on the formal analysis and the search for an appropriate lo-
cation privacy metric that allows for the fair comparison
between LPPMs.
625Existing LPPMs are built according to diﬀerent design
principles. The most popular approach to obtaining loca-
tion privacy is to send a space- or time-obfuscated version
of the users’ actual locations to the service provider [10, 12,
14, 16]. A diﬀerent approach consists in hiding some of the
users’ locations by using mix zones [1, 9], or silent periods
[15]. These are regions where users do not communicate with
the provider while changing their pseudonym. Provided that
several users traverse the zone simultaneously, this mecha-
nism prevents an adversary from tracking them, as he cannot
link those who enter with those who exit the region. A third
line of work protects location privacy by adding dummy re-
quests, indistinguishable from real requests, issued from fake
locations to the service provider [3]. The purpose of these
fake locations is to increase the uncertainty of the adversary
about the users’ real movements.
A number of papers show that the predictability of users’
location traces, and the particular constraints of users’ move-
ments, are suﬃcient to reconstruct and/or identify anony-
mous or perturbed locations. For instance, an adversary
can, to name but a few possibilities, infer users’ activities
from the frequency of their visits to certain locations [19]; re-
identify anonymous low-granularity location traces given the
users’ mobility proﬁles [5]; or derive [13], and re-identify [11,
18] the home address of individuals from location traces.
Several authors have made eﬀorts towards formalizing the
desirable location privacy requirements that LPPMs should
fulﬁll, as well as towards ﬁnding suitable metrics to evaluate
the degree to which these requirements are fulﬁlled. Exam-
ples of these lines of work are Krumm [18], Decker [6], and
Duckham [7]. Shokri et al. [24] revisit existing LPPMs and
the location-privacy metrics used in their evaluation. They
classify these metrics in three categories: uncertainty-based
(entropy), error-based and k-anonymity. The authors con-
clude, by means of a qualitative evaluation, that metrics
such as entropy and k-anonymity are not suitable for mea-
suring location privacy. In a follow-up of this work, Shokri
et al. provide a framework [25, 26] to quantify location pri-
vacy. The framework allows us to specify an LPPM and
then to evaluate various questions about the location infor-
mation leaked. Our design methodology uses this analytical
framework as an evaluation tool to quantifying the LPPMs’
oﬀered privacy against the localization attack.
Despite the extent to which location privacy has been
studied, there is a patent disconnection between these dif-
ferent lines of work. Most of the aforementioned papers use
diﬀerent models to state the problem and evaluate location
privacy. This hinders the comparison of systems and slows
down the design of robust LPPMs. Further, in some of these
papers there is a detachment between the proposed design
and the adversarial model against which it is evaluated. Of-
ten the considered adversary is static in its knowledge and
disregards the information leaked by the LPPM algorithm;
or adversarial knowledge is not even considered in the evalu-
ation. The works by Freudiger et al. [9] and Shokri et al. [24,
25, 26] do consider an strategic adversary that exploits the
information leaked by the LPPM in order to compute loca-
tion privacy. Nevertheless, their work, which we build on in
this paper, does not address how this privacy computation
can be integrated in the design of location-privacy preserv-
ing mechanisms.
In this work, we bridge the gap between design and eval-
uation of LPPMs. We provide a systematic method for de-
veloping LPPMs; it maximize users’ location privacy while
guaranteeing a minimum service quality. We formalize the
optimal design problem as a Bayesian Stackelberg game sim-
ilar to previous work on security in which, as in the location-
privacy scenario, the defender can be modeled as a Stack-
elberg game leader, and the adversary as the follower. The
common theme is that the defender must commit to a de-
fense strategy/protocol, which is then disclosed to the adver-
sary, who can then choose an optimal course of action after
observing the defender’s strategy. Paruchuri et al. [23] pro-
pose an eﬃcient algorithm for ﬁnding the leader’s optimal
strategy considering as a main case study a patrolling agent
who searches for a robber in a limited area. In their case,
the defender is unsure about the type of the adversary (i.e.
where the adversary will attack). In contrast, in our work it
is the adversary who is unsure about the type (i.e. the true
location) of the user/defender. A similar approach is used
by Liu and Chawla [20] in the design of an optimal e-mail
spam ﬁlter, taking into account that spammers adapt their
e-mails to get past the spam detectors. The same problem
is tackled by Br¨uckner and Scheﬀer [2], who further com-
pare the Stackelberg-based approach with previous spam ﬁl-
ters based on support vector machines, logistic regression,
and Nash-logistic regression. Korzhyk et al. [17] contrast
the Stackelberg framework with the more traditional Nash
framework, within a class of security games. A recent survey
[21] explores the connections between security and game the-
ory more generally. To the best of our knowledge, our work
is the ﬁrst that uses Bayesian Stackelberg games to design
optimal privacy-protection mechanisms.
7. CONCLUSION
Accessing location-based services from mobile devices en-
tails a privacy risk for users whose sensitive information can
be inferred from the locations they visit. This information
leakage raises the need for robust location-privacy protect-
ing mechanisms (LPPMs). In this paper, we have proposed
a game-theoretic framework that enables a designer to ﬁnd
the optimal LPPM for a given location-based service, ensur-
ing a satisfactory service quality for the user. This LPPM
is designed to provide user-centric location privacy, hence it
is ideal to be implemented in the users’ mobile devices.
Our method accounts for the fact that the strongest ad-
versary not only observes the perturbed location sent by the
user but also knows the algorithm implemented by the pro-
tection mechanism. Hence, he can exploit the information
leaked by the LPPM’s algorithm to reduce his uncertainty
about the user’s true location. However, the user is only
aware of the adversary’s knowledge and does not make any
assumption about his inference attack. Hence, she prepares
the protection mechanism against the most optimal attack.
By modeling the problem as a Bayesian Stackelberg compe-
tition, we ensure that the optimal LPPM is designed antic-
ipating such strong inference attack.
We have validated our method using real location traces.
We have demonstrated that our approach ﬁnds the optimal
attack for a given LPPM and service-quality constraint, and
we have shown that it is superior to other LPPMs such as
basic location obfuscation. We have also shown that the
superiority of the optimal LPPM over alternatives is more
signiﬁcant when the service-quality constraint imposed by
the user is tightened. Hence, our solution is eﬀective exactly
where it will be used. Finally, our results conﬁrm that loos-
626ening the service-quality constraint allows for increased pri-
vacy protection, but the magnitude of this increase strongly
depends on the user proﬁle, i.e., on the degree to which a
user’s location is predictable from her LBS access proﬁle.
To the best of our knowledge, this is the ﬁrst frame-
work that explicitly includes the adversarial knowledge into
a privacy-preserving design process, and considers the com-
mon knowledge between the privacy protector and the at-
tacker. Our obtained result is a promising step forward in
the quest for robust and eﬀective privacy preserving systems.
8. REFERENCES
[1] A. R. Beresford and F. Stajano. Location privacy in
pervasive computing. IEEE Pervasive Computing,
2(1):46–55, 2003.
[2] M. Br¨uckner and T. Scheﬀer. Stackelberg games for
adversarial prediction problems. In C. Apt´e, J. Ghosh,
and P. Smyth, editors, 17th ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining (KDD), 2011.
[3] R. Chow and P. Golle. Faking contextual data for fun,
proﬁt, and privacy. In WPES ’09: Proceedings of the
8th ACM workshop on Privacy in the electronic
society, New York, NY, USA, 2009.
[4] S. Dasgupta, C. Papadimitriou, and U. Vazirani.
Algorithms. McGraw-Hill, New York, NY, 2008.
[5] Y. De Mulder, G. Danezis, L. Batina, and B. Preneel.
Identiﬁcation via location-proﬁling in gsm networks.
In WPES ’08: Proceedings of the 7th ACM workshop
on Privacy in the electronic society, New York, NY,
USA, 2008.
[6] M. Decker. Location privacy - an overview. In
International Conference on Mobile Business, 2009.
[7] M. Duckham. Moving forward: location privacy and
location awareness. In Proceedings of the 3rd ACM
SIGSPATIAL International Workshop on Security and
Privacy in GIS and LBS, New York, NY, USA, 2010.
[8] J. Freudiger, R. Shokri, , and J.-P. Hubaux.
Evaluating the privacy risk of location-based services.
In Financial Cryptography and Data Security (FC),
2011.
[9] J. Freudiger, R. Shokri, and J.-P. Hubaux. On the
optimal placement of mix zones. In PETS ’09:
Proceedings of the 9th International Symposium on
Privacy Enhancing Technologies, Berlin, Heidelberg,
2009.
[10] B. Gedik and L. Liu. Location privacy in mobile
systems: A personalized anonymization model. In
ICDCS ’05: Proceedings of the 25th IEEE
International Conference on Distributed Computing
Systems, Washington, DC, USA, 2005.
[11] P. Golle and K. Partridge. On the anonymity of
home/work location pairs. In Pervasive ’09:
Proceedings of the 7th International Conference on
Pervasive Computing, Berlin, Heidelberg, 2009.
[12] M. Gruteser and D. Grunwald. Anonymous usage of
location-based services through spatial and temporal
cloaking. In MobiSys ’03: Proceedings of the 1st
international conference on Mobile systems,
applications and services, New York, NY, USA, 2003.
[13] B. Hoh, M. Gruteser, H. Xiong, and A. Alrabady.
Enhancing security and privacy in traﬃc-monitoring
systems. IEEE Pervasive Computing, 5(4):38–46, 2006.
[14] B. Hoh, M. Gruteser, H. Xiong, and A. Alrabady.
Preserving privacy in gps traces via uncertainty-aware
path cloaking. In CCS ’07: Proceedings of the 14th
ACM conference on Computer and communications
security, New York, NY, USA, 2007.
[15] T. Jiang, H. J. Wang, and Y.-C. Hu. Preserving
location privacy in wireless lans. In MobiSys ’07:
Proceedings of the 5th international conference on
Mobile systems, applications and services, New York,
NY, USA, 2007.
[16] P. Kalnis, G. Ghinita, K. Mouratidis, and
D. Papadias. Preventing location-based identity
inference in anonymous spatial queries. Knowledge
and Data Engineering, IEEE Transactions on,
19(12):1719–1733, Dec. 2007.
[17] D. Korzhyk, Z. Yin, C. Kiekintveld, V. Conitzer, and
M. Tambe. Stackelberg vs. Nash in security games:
An extended investigation of interchangeability,
equivalence, and uniqueness. Journal of Artiﬁcial
Intelligence Research, 41:297–327, May–August 2011.
[18] J. Krumm. Inference attacks on location tracks. In In
Proceedings of the Fifth International Conference on
Pervasive Computing (Pervasive), 2007.
[19] L. Liao, D. J. Patterson, D. Fox, and H. A. Kautz.
Learning and inferring transportation routines. Artif.
Intell., 171(5-6):311–331, 2007.
[20] W. Liu and S. Chawla. A game theoretical model for
adversarial learning. In Y. Saygin, J. X. Yu,
H. Kargupta, W. Wang, S. Ranka, P. S. Yu, and
X. Wu, editors, IEEE International Conference on
Data Mining Workshops (ICDM 2009), 2009.
[21] M. Manshaei, Q. Zhu, T. Alpcan, T. Basar, and J.-P.
Hubaux. Game theory meets network security and
privacy. ACM Computing Surveys, 2011.
[22] J. Meyerowitz and R. Roy Choudhury. Hiding stars
with ﬁreworks: location privacy through camouﬂage.
In MobiCom ’09: Proceedings of the 15th annual
international conference on Mobile computing and
networking, New York, NY, USA, 2009.
[23] P. Paruchuri, J. P. Pearce, J. Marecki, M. Tambe,
F. Ord´o˜nez, and S. Kraus. Eﬃcient algorithms to
solve Bayesian Stackelberg games for security
applications. In D. Fox and C. P. Gomes, editors, 23rd
AAAI Conference on Artiﬁcial Intelligence (AAAI
2008), 2008.
[24] R. Shokri, J. Freudiger, M. Jadliwala, and J.-P.
Hubaux. A distortion-based metric for location
privacy. In WPES ’09: Proceedings of the 8th ACM
workshop on Privacy in the electronic society, New
York, NY, USA, 2009.
[25] R. Shokri, G. Theodorakopoulos, G. Danezis, J.-P.
Hubaux, and J.-Y. Le Boudec. Quantifying location
privacy: the case of sporadic location exposure. In
Proceedings of the 11th international conference on
Privacy enhancing technologies (PETS), Berlin,
Heidelberg, 2011.
[26] R. Shokri, G. Theodorakopoulos, J.-Y. Le Boudec,
and J.-P. Hubaux. Quantifying location privacy. In
IEEE Symposium on Security and Privacy, Oakland,
CA, USA, 2011.
627