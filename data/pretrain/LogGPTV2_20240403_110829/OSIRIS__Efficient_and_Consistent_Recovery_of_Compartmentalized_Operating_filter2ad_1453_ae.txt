However, our system is just a research prototype and not a ma-
ture system. It is possible that relative overheads would work
out differently on, say, a more efﬁcient baseline system. More-
over, compartmentalization is a prerequisite for OSIRIS and, as
it requires the overhead of switching between compartments,
such systems are unlikely to reach the level of performance
of a system such as Linux. That said, compartmentalization
offers major reliability advantages to compensate for the loss in
performance even without recovery. Nevertheless, to improve
performance and balance the performance-reliability trade-off,
one could, in principle, retroﬁt our compartment-based design
to high-performance monolithic OS architectures, for example,
using virtualization-based isolation as in VirtuOS [17].
VIII. RELATED WORK
The ability to shield systems from software faults has been
the subject of many prior research efforts. Techniques include
software rejuvenation, checkpoint-restart, and component de-
pendency tracking. These techniques have been applied for
recovering from faults in device drivers, operating systems,
server and multi-tier applications, and distributed systems. In
this section, we discuss prior research work in each of these
areas and how it relates to ours.
A. Recovering by reboot/restart
Restartability is a method to recover from failures or
even proactively avoid failures, particularly transient and
aging-related bugs [42]. Such restarts can be performed at
both the operating system and application level. Phase-based
reboot [29] aims at speeding up rebooting the operating system
by reusing system state from previous reboots. A reboot is
expected to bring back the system to a known consistent state.
However, it comes at the cost of losing prior execution state
of the system and contributes to downtime. MINIX 3 [25], [8]
applies fault isolation in its design to enable restarting only
crashed drivers. This is effective for performing stateless recov-
ery, whereas our approach can support fully stateful recovery
for arbitrary OS components. Application recovery has also
been made possible by design-level considerations to enable
restarting only the affected constituent components [43],
[30]. In the distributed systems domain, Neutron [44] takes
a similar approach towards reviving by restarting components
in sensor network systems. In our work, hardware-assisted
fault isolation has a similar effect in organizing the operating
system into individually restartable fault domains. Although
restart-based systems are relatively efﬁcient and have the
beneﬁt of being relatively simple to implement, they all suffer
from loss of state, which means that they can only be applied
to stateless components for transparent recovery purposes.
B. Checkpoint-restart/rollback
Checkpointing enables reviving a system without losing
much of its execution context. Checkpointing solutions have
been implemented at the level of operating systems, individual
applications, and distributed systems.
At the operating system level, CuriOS [18] takes a virtual
memory isolated multi-server approach where server state
is persisted in respective client-side memory. This allows
affected servers to be restarted without losing clients’ context.
While this circumvents the need for checkpointing, it is only
suitable for systems where frequent (per-request or more)
accesses to the clients’ address space from the server processes
are inexpensive. Fine-grained fault tolerance [15] relies on
power management code to record device states—while also
10
checkpointing device driver state. A crash causes the system
to roll back to the last driver entry-point state and restore
the corresponding device state. While this has the beneﬁt of
being able to reuse existing code, this solution only applies to
drivers. In addition, enforcing entry-point execution on a copy
of required driver and kernel state obtained through static
analysis cannot scale well to generic and complex operating
system’s core components.
has
been
[45],
forms
recovery
proposed
[19],
Checkpoint-based
for
application recovery in various
[46],
ranging from applying hypervisor-assisted techniques to fast
in-memory checkpointing. Vogt et al. [22] describe user-space
memory checkpointing techniques that rely on compiler-based
instrumentation to support high-frequency checkpointing.
OSIRIS relies on similar compiler-based techniques, but
selectively disables the checkpointing instrumentation during
out-of-window execution.
Fault
tolerance is a well-studied topic in distributed
systems. Other than redundancy- and replication-based fault
tolerance, checkpoint/snapshot-based recovery and message
logging are also popular in distributed systems. Participant
nodes take local snapshots in a coordinated or uncoordinated
fashion to achieve fault tolerance in the face of faults or intru-
sions in the system [20], [44], [47], [48]. Message logging and
replay is another alternative applied to message passing-based
distributed systems [49], [21]. The multi-server architecture
coupled with message passing-based communication in
OSIRIS resembles a miniature distributed system. However,
high-frequency and near-instantaneous message transfers ob-
served in an operating system setting make a request-oriented
local checkpointing scheme more suitable for our design.
C. Dependency tracking
Consistent recovery entails rolling back not just the com-
ponent that failed, but also all its dependent components whose
state may get invalidated due to the rollback that occurred in
the crashed component. Nooks [24] performs runtime object
tracking to track manipulations of kernel data structures
by kernel extensions in order to clean them up, should the
extension crash. This is to protect
the entire kernel from
faulty extensions. Swift et al. [14] introduce a shadow driver
mechanism that monitors driver-kernel interactions and when a
driver fails, it turns a crash into an error condition by servicing
requests on its behalf. Akeso [19] organizes the Linux kernel
in a request-oriented hierarchy of recovery domains. Inter-
recovery domain dependencies are tracked at runtime. When a
fault occurs, Akeso initiates recovery of the dependent domains
and recovers the failed recovery domain.
In distributed
systems, optimistic recovery [21] uses causal dependency
tracking
among
participating processes, so that dependent processes can be also
rolled back during recovery. However, runtime dependency
tracking generally incurs nontrivial performance overhead.
Inter-component/process dependencies may even lead to a
cascade of component rollbacks, which requires special care
to avoid a domino effect. The SEEP channel in our design
eliminates runtime dependency monitoring and the associated
complexities, thereby limiting performance degradation.
computational
dependencies
detect
to
D. Other techniques
In addition to traditional checkpoint/restart, prior research
has looked at recovering from system crashes in various
ways. For example, ASSURE [26] and REASSURE [27] reuse
existing error handling logic in the application to turn crashes
into erroneous function return values. Carburizer [50], in turn,
uses shadow drivers to turn device failures into software errors
and avoid kernel crashes. OSIRIS relies on similar error virtu-
alization strategies, but gracefully propagates error conditions
through the message passing interface in compartmentalized
operating system architectures.
IX. CONCLUSION
We presented a recovery strategy for
fatal persistent
software faults
in compartmentalized operating systems
that does not compromise on system state consistency. We
demonstrated the effectiveness of our design, which trades off
total recovery surface of the system for performance and design
simplicity. The key idea is to limit recoverability to execution
paths that do not affect the global state of the system, greatly
reducing runtime complexity. This enabled us to limit the RCB
size to only 12.5% of our OSIRIS prototype—demonstrating
that
reliability goals are practically achievable. We
implemented our recoverability mechanisms using LLVM-
based instrumentation, which can be also reused for other
compartmentalized systems. Our framework is customizable
and allows new classes of SEEPs and recovery actions to
be deﬁned for new target systems. Our experimental results
demonstrate that OSIRIS’ design is practical and effective in
consistently recovering from even persistent software faults.
its
Overall, OSIRIS demonstrates that balancing recoverabil-
ity, runtime performance, and simplicity of the reliable com-
puting base can be an effective strategy to enhance depend-
ability of compartmentalized operating systems. To foster
further research in the area and in support of open science,
we are open sourcing our OSIRIS prototype, available at
http://github.com/vusec/osiris.
X. ACKNOWLEDGEMENTS
We would like to thank the anonymous reviewers for their
comments. This work was supported by the European Commis-
sion through project H2020 ICT-32-2014 “SHARCS” under
Grant Agreement No. 644571 and by the Netherlands Organ-
isation for Scientiﬁc Research through the NWO 639.023.309
VICI “Dowsing” project and the NWO “Re-Cover” project.
REFERENCES
[1] A. Ganapathi and D. A. Patterson, “Crash data collection: A windows
case study.” in DSN, 2005, pp. 280–285.
[2] R. Matias, M. Prince, L. Borges, C. Sousa, and L. Henrique, “An
empirical exploratory study on operating system reliability,” in SAC,
2014, pp. 1523–1528.
[3] T. J. Ostrand and E. J. Weyuker, “The distribution of faults in a large
industrial software system,” in ISSTA, 2002, pp. 55–64.
[4] T. J. Ostrand, E. J. Weyuker, and R. M. Bell, “Where the bugs are,” in
ISSTA, 2004, pp. 86–96.
[5] T.-H. Chen, M. Nagappan, E. Shihab, and A. E. Hassan, “An empirical
study of dormant bugs,” in MSR, 2014, pp. 82–91.
11
[6] M. M. Swift, B. N. Bershad, and H. M. Levy, “Improving the reliability
of commodity operating systems,” ACM Trans. Comput. Syst., vol. 23,
no. 1, pp. 77–110, Feb. 2005.
[27] G. Portokalidis and A. D. Keromytis, “Reassure: A self-contained
mechanism for healing software using rescue points,” in IWSEC, 2011,
pp. 16–32.
[7] A. Chou, J. Yang, B. Chelf, S. Hallem, and D. Engler, “An empirical
study of operating systems errors,” in SOSP, 2001, pp. 73–88.
J. N. Herder, H. Bos, B. Gras, P. Homburg, and A. S. Tanenbaum,
“Construction of a highly dependable operating system,” in EDCC,
2006, pp. 3–12.
[8]
[9] N. Palix, G. Thomas, S. Saha, C. Calv`es, J. Lawall, and G. Muller,
“Faults in linux: ten years later,” in ACM SIGARCH Computer Archi-
tecture News, vol. 39, no. 1, 2011, pp. 305–318.
[10] S. Chandra and P. M. Chen, “Whither generic recovery from application
faults? a fault study using open-source software,” in DSN, 2000, pp. 97–
106.
J. Gray, “Why do computers stop and what can be done about it?” in
The 5th Symposium on Reliablity in Dist. Softw. and Database Sys.,
1985.
[11]
[12] A. Depoutovitch and M. Stumm, “Otherworld: giving applications a
chance to survive os kernel crashes,” in Proceedings of the 5th European
conference on Computer systems, 2010, pp. 181–194.
[13] S. Sundararaman, S. Subramanian, A. Rajimwale, A. C. Arpaci-
Dusseau, R. H. Arpaci-Dusseau, and M. M. Swift, “Membrane: Op-
erating system support for restartable ﬁle systems,” TOS, vol. 6, no. 3,
p. 11, 2010.
[14] M. M. Swift, M. Annamalai, B. N. Bershad, and H. M. Levy, “Recov-
ering device drivers,” ACM Transactions on Computer Systems (TOCS),
vol. 24, no. 4, pp. 333–360, 2006.
[15] A. Kadav, M. J. Renzelmann, and M. M. Swift, “Fine-grained fault
tolerance using device checkpoints,” in ACM SIGARCH Computer
Architecture News, vol. 41, no. 1. ACM, 2013, pp. 473–484.
[16] F. Zhou, J. Condit, Z. Anderson, I. Bagrak, R. Ennals, M. Harren,
G. Necula, and E. Brewer, “Safedrive: Safe and recoverable extensions
using language-based techniques,” in OSDI, 2006, pp. 45–60.
[17] R. Nikolaev and G. Back, “Virtuos: an operating system with kernel
virtualization,” in SOSP, 2013, pp. 116–132.
[18] F. M. David, E. Chan, J. C. Carlyle, and R. H. Campbell, “Curios:
Improving reliability through operating system structure.” in OSDI,
2008, pp. 59–72.
[19] A. Lenharth, V. S. Adve, and S. T. King, “Recovery domains: an orga-
nizing principle for recoverable operating systems,” in ACM SIGARCH
Computer Architecture News, vol. 37, no. 1, 2009, pp. 49–60.
[20] D. J. Sorin, M. M. Martin, M. D. Hill, D. Wood et al., “Safetynet:
improving the availability of shared memory multiprocessors with
global checkpoint/recovery,” in ISCA, 2002, pp. 123–134.
[21] R. Strom and S. Yemini, “Optimistic recovery in distributed systems,”
TOCS, vol. 3, no. 3, pp. 204–226, 1985.
[22] D. Vogt, C. Giuffrida, H. Bos, and A. S. Tanenbaum, “Lightweight
memory checkpointing,” in DSN, 2015, pp. 474–484.
[23] G. C. Hunt and J. R. Larus, “Singularity: rethinking the software stack,”
SIGOPS OSR, vol. 41, no. 2, pp. 37–49, 2007.
[24] M. M. Swift, S. Martin, H. M. Levy, and S. J. Eggers, “Nooks: An
architecture for reliable device drivers,” in ACM SIGOPS European
workshop, 2002, pp. 102–107.
J. N. Herder, H. Bos, B. Gras, P. Homburg, and A. S. Tanenbaum,
“Failure resilience for device drivers,” in DSN, 2007, pp. 41–50.
[25]
[26] S. Sidiroglou, O. Laadan, C. Perez, N. Viennot, J. Nieh, and A. D.
Keromytis, “Assure: automatic software self-healing using rescue
points,” ACM SIGARCH Computer Architecture News, vol. 37, no. 1,
pp. 37–48, 2009.
[27] G. Portokalidis and A. D. Keromytis, “Reassure: A self-contained
[28] G. Janakiraman, J. R. Santos, D. Subhraveti, and Y. Turner, “Cruz:
Application-transparent distributed checkpoint-restart on standard oper-
ating systems,” in DSN, 2005, pp. 260–269.
[29] K. Yamakita, H. Yamada, and K. Kono, “Phase-based reboot: Reusing
operating system execution phases for cheap reboot-based recovery,” in
DSN, 2011, pp. 169–180.
[30] G. Candea, J. Cutler, A. Fox, R. Doshi, P. Garg, and R. Gowda,
“Reducing recovery time in a small recursively restartable system,” in
DSN, 2002, pp. 605–614.
[31] C. Giuffrida, L. Cavallaro, and A. S. Tanenbaum, “We crashed, now
what,” in HotDep, 2010, pp. 1–8.
[32] D. Vogt, A. Miraglia, G. Portokalidis, H. Bos, A. Tanenbaum, and
C. Giuffrida, “Speculative memory checkpointing,” in Middleware,
2015, pp. 197–209.
[33] C. Lattner and V. Adve, “Llvm: A compilation framework for lifelong
program analysis & transformation,” in CGO, 2004, pp. 75–86.
[34] M. Engel and B. Dbel, “The reliable computing base: A paradigm for
[35]
[36]
software-based reliability,” in Workshop on SOBRES, 2012.
“A unixbenchmark suite,
the original byte unix benchmark suite,
updated and revised by many people over the years.” https://github.
com/kdlucas/byte-unixbench, accessed: July 24th, 2015.
“Minix 3 source repository,” http://git.minix3.org/index.cgi?p=minix.
git.
[37] C. Giuffrida, A. Kuijsten, and A. S. Tanenbaum, “Edﬁ: A dependable
fault injection tool for dependability benchmarking experiments,” in
PRDC, 2013, pp. 1–10.
[38] H. H. M. H. J. Liedtke and S. S. J. Wolter, “The performance of micro-
[39]
[40]
[41]
kernel-based systems,” in SOSP, 1997.
J. Liedtke, On micro-kernel construction. ACM, 1995, vol. 29, no. 5.
J. Liedtke, “Improving ipc by kernel design,” in SIGOPS OSR, vol. 27,
no. 5, 1994, pp. 175–188.
J. Wu, H. Cui, and J. Yang, “Bypassing races in live applications with
execution ﬁlters.” in OSDI, vol. 10, 2010, pp. 1–13.
[42] Y. Huang, C. Kintala, N. Kolettis, and N. D. Fulton, “Software rejuvena-
tion: Analysis, module and applications,” in FTCS, 1995, pp. 381–390.
[43] G. Candea, S. Kawamoto, Y. Fujiki, G. Friedman, and A. Fox,
“Microreboot-a technique for cheap recovery.” in OSDI, vol. 4, 2004,
pp. 31–44.
[44] Y. Chen, O. Gnawali, M. Kazandjieva, P. Levis, and J. Regehr, “Sur-
viving sensor network software faults,” in SOSP, 2009, pp. 235–246.
[45] M. Lee, A. Krishnakumar, P. Krishnan, N. Singh, and S. Yajnik,
“Hypervisor-assisted application checkpointing in virtualized environ-
ments,” in DSN, 2011, pp. 371–382.
[46] F. Qin, J. Tucek, J. Sundaresan, and Y. Zhou, “Rx: treating bugs as
allergies—a safe method to survive software failures,” in SIGOPS OSR,
vol. 39, no. 5, 2005, pp. 235–248.
[47] P. Sousa, A. N. Bessani, M. Correia, N. F. Neves, and P. Verissimo,
“Highly available intrusion-tolerant services with proactive-reactive
recovery,” TPDS, vol. 21, no. 4, pp. 452–465, 2010.
[48] A. Agbaria and R. Friedman, “Starﬁsh: Fault-tolerant dynamic mpi
programs on clusters of workstations,” Cluster Computing, vol. 6, no. 3,
pp. 227–236, 2003.
[49] A. Borg, J. Baumbach, and S. Glazer, “A message system supporting
fault tolerance,” ACM SIGOPS Operating Systems Review, vol. 17,
no. 5, pp. 90–99, 1983.
[50] A. Kadav, M. J. Renzelmann, and M. M. Swift, “Tolerating hardware
device failures in software,” in SOSP, 2009, pp. 59–72.
12