74.1
82.6
95.0
22.6
99.6
58.4
4.3
6.7
7.9
Table 6: Fake Image Detector Performance (F1) using differ-
ent denoising filters. Bold numbers highlight the best per-
formance in each dataset.
5011, 80, 100 and 200 fake images respectively. All test sets con-
tain 200 real images from the respective dataset. A reference set of
2000 real images is used. Tmerдe for StyleGAN-Face1 and PGGAN-
Face remain the same as those used for the original results in
Table 2 i.e., computed using the Tmerдe estimation strategy in
Section 4.2. Tmerдe for BigGAN-DogHV, CycleGAN-Zebra and
StyleGAN-Face1 are tuned following the recalibration strategy sug-
gested in Section 5.2. We compute F1 score of detection performance
averaged over 5 trials. Results are presented in Figure 7. The per-
formance is moderately high, but as expected, drops as the number
of fake images decrease. Analysis reveals a decrease in fingerprint
purity, caused by merging with noisy samples amongst the increas-
ingly large proportion of real images. From prior work we also know
that a reliable fingerprint requires roughly 50 images or more [6].
The decrease in performance is not a serious problem—the absolute
number of fake images to detect is in and of itself, very small. One
can consider a scenario with too few fake images (<50) to not be
11We remove the minimum requirement of 50 images for a fingerprint when only 50
fake images are in the test set.
 0.5 0.6 0.7 0.8 0.9 1 40 60 80 100 120 140 160 180 200Detection Performance (F1)Number of Fake Images In Test SetsBigGAN-DogHVCycleGAN-ZebraStyleGAN-Face2StyleGAN-Face1PGGAN-Face922NoiseScope: Detecting Deepfake Images in a Blind Setting
ACSAC 2020, December 7–11, 2020, Austin, USA
a serious threat, compared to cases where online platforms are
flooded with fake images [64].
Impact of residual image extraction filter on performance.
We use 3 popular alternative filters—Blur filter [27], Non-Local-
Means (NLM) filter [10], and the BM3D filter [20], and observe that
Wavelet denoising provides better detection performance for nearly
all datasets. Results are in Table 6. To compare, we simulate the
fake image detection step in NoiseScope. Given a test set of 500 real,
and 500 fake images, we estimate a clean model fingerprint using
a random subset of 100 fake images from the test set itself. Next,
we use this model fingerprint to flag fake images in the test set.
This is an ideal scenario because the fingerprint is 100% pure (i.e.,
estimated over only fake images). An effective filter should produce
high detection performance in such a setting, while filters that fail
to effectively remove high-level content may not perform so well.
Table 6 presents the detection performance (average F1 score) for
each 256x256 dataset.12 Wavelet Denoising filter exhibits the best
performance, with F1 scores exceeding 90% for all datasets. The
BM3D filter also shows good performance but fails to effectively
eliminate content from some datasets.
Generalization performance comparison with a supervised
scheme. Supervised detection schemes exhibit high performance
at the cost of generalization. To give an example, we use the super-
vised classifier MesoNet [1] to detect unseen GAN-generated face
images. MesoNet is trained on 1000 real and 1000 fake images from
StyleGAN-Face1 and provides a high F1 score of 94% on a test set of
the same size from StyleGAN-Face1. However, this trained model
achieves significantly reduced F1 score of 65% on a test set from
PGGAN-Face. This drop in performance indicates an exploitable
failure to generalize that is remedied by NoiseScope.
Summary. We evaluated NoiseScope against datasets containing
balanced and imbalanced proportions of fake images and observed
stable behavior with generally high detection performances. We
attributed the rare drops in performance to a low fingerprint purity,
caused by low values of merging threshold Tmerдe. We accordingly
provided guidelines for calibrating a better Tmerдe based on clus-
ter sizes. We show that NoiseScope is robust against datasets with
multiple GAN sources. We evaluated NoiseScope against test sets
containing few fake images and observe moderately high perfor-
mance, with performance dropping when there are too few fake
images (e.g., 50), at which point the threat itself is limited. We
then showcased the impact of 3 popular alternative residual filters
on NoiseScope’s performance. Finally, we highlighted the need for
NoiseScope by showcasing the inability of supervised detection to
generalize.
6 ANALYSIS OF COUNTERMEASURES
We consider a powerful adaptive attacker with knowledge of Nois-
eScope’s detection pipeline. These countermeasures aim to disrupt
the fingerprint extraction, and fake image detection capabilities of
NoiseScope. We also propose adaptive recovery measures to make
NoiseScope robust to certain challenging countermeasures. Table 7
presents results using test sets with 500 real, and 500 fake images.
Compressing fake images to make fingerprints fragile. JPEG
compression is well known to disrupt camera fingerprint patterns,
12Applying the BM3D filter to 1024x1024 images is computationally expensive.
and therefore diminish the correlation between fingerprints and
residuals [31]. Following compression configurations used in prior
work [80], fakes images are compressed with a quality factor ran-
domly sampled from U [10, 75]. Surprisingly, NoiseScope is resilient
against compression attacks. More interestingly, the performance
for StyleGAN-Face2 increases from 90.14% to 98.33% on applying
compression. NoiseScope is resilient for two reasons: First, model
fingerprints are always extracted from the test set itself. Therefore,
the estimated fingerprint, already captures any artifacts introduced
by compression, and correlates well with the similarly processed
residual images in test set. This is unlike prior work in camera
fingerprinting, where camera attribution is attempted between a
clean fingerprint (computed over uncompressed images) and a com-
pressed residual image. Second, we observe that JPEG compression
introduces grid-like artifacts into the fingerprint, further making
the model fingerprint distinct from device fingerprints. Fingerprints
subjected to JPEG compression are shown in Figure 23 in Appen-
dix A. Compression does disrupt the fingerprint pattern. NoiseScope,
however, continues to remain effective.
Denoising using the defender’s denoising filter. This coun-
termeasure assumes knowledge of the Noise Residual Extractor.
Attacker modifies fake images by subtracting the residual obtained
using the defender’s denoising filter (Wavelet denoiser), i.e., I′
=
i
Ii −Ri. This can make fingerprint extraction harder, because the pat-
terns in the noise residuals are “weakened”. NoiseScope performance
suffers for the BigGAN-DogHV and CycleGAN-Zebra datasets. See
‘Attack’ column under Wavelet Denoising. On visual inspection
of the fingerprints, the texture patterns of fingerprints appear to
have been softened by this attack. Performance dropped, because
in certain trials, the fingerprint classifier module failed to flag the
new model fingerprints, likely due to texture softening.
To recover from this attack, we resort to adversarial training of
the fingerprint classification module. We train the classifier module
on fingerprints computed from real images that goes through the
same post-processing countermeasure used by the attacker. Results
are shown in the ‘Recovery’ column under the specific countermea-
sure. We observe an improvement in detection performance for
both BigGAN-DogHV, and CycleGAN-Zebra, while performance
for the other datasets remain unaffected. Lastly, an interesting case
is that of StyleGAN-Face2. Performance actually increases for this
dataset on applying the countermeasure. On further inspection, we
observe that the countermeasure introduces new distinct artifacts
in the fingerprints, that enable NoiseScope to still accurately cluster
images, and detect them. We suspect that images in this dataset
has already undergone additional post-processing, which is likely
introducing these artifacts when new processing is applied.
Other post-processing schemes to disrupt fingerprints. We
evaluate against 4 image post-processing countermeasures known
to disrupt camera fingerprinting [31, 49, 68, 70]. Whenever available,
we use settings from prior work.
Gamma correction. Gamma correction is applied to fake images
with gamma values randomly sampled from U [1.0, 2.0] [80]. Perfor-
mance remains high for all datasets, except StyleGAN-Face2 where
F1 score drops to 62%. Further investigation reveals that the finger-
print classifier performs poorly in 2/5 trials. For recovery, we again
apply adversarial training to the fingerprint classifier, and train on
923ACSAC 2020, December 7–11, 2020, Austin, USA
Jiameng Pu, Neal Mangaokar, Bolun Wang, Chandan K. Reddy, and Bimal Viswanath
F1 Score (%) with Different Countermeasures
Recovery
Datasets
Original
JPEG compression Wavelet-denoising Gamma correction Histogram equalization
StyleGAN-Face1
StyleGAN-Face2
BigGAN-DogHV
PGGAN-Face
CycleGAN-Zebra
Table 7: Performance (F1) of NoiseScope under different countermeasures. ‘Original’ means no countermeasures were used.
99.5
82.8
93.4
98.9
92.6
Attack Recovery Attack
99.5
99.4
62.0
98.0
92.9
57.9
99.2
98.9
91.9
53.7
99.4
98.0
87.0
99.2
89.2
Attack
99.3
98.3
89.5
98.9
91.0
Adding noise
Blur
Attack Attack Recovery
99.4
80.0
55.7
98.5
61.8
99.2
81.4
78.6
96.2
85.9
99.7
54.9
88.4
97.9
51.8
Attack
99.2
72.9
88.9
99.2
84.4
-
99.6
90.1
92.6
99.1
92.8
real images that undergo the same post-processing. Performance
of StyleGAN-Face2 recovers from 62% to 82%.
Histogram equalization. Histogram equalization involves distribut-
ing the intensity range to improve image contrast. We apply his-
togram equalization to fake images. Detection performance remains
high for all datasets except StyleGAN-Face2. Fingerprint extrac-
tion did not perform well, and StyleGAN-Face2 ended up with
impure clusters (purity ranging between 60% to 70%). We do not
attempt recovery from this countermeasure because on examination
of fake images that missed detection, we see that image quality has
been severely degraded. Therefore, to evade detection by NoiseScope,
post-processing that significantly degrades image quality is required.
Image samples are shown in Figure 22 (Appendix A).
Blur. Blurring performs a normalized box averaging on fake images,
with a specific kernel size [7]. Kernel size is randomly selected from
{1, 3, 5, 7} [80]. We expect blurring to damage patterns in the model
fingerprints. Performance for StyleGAN-Face2, CycleGAN-Zebra
and StyleGAN-Face2 end up dropping. We find that blurring largely
weakens the fingerprint pattern. However, on closer investigation
of images that were not caught, we find that image quality has
degraded significantly—NoiseScope failed to catch fake images that
were severely blurred. Therefore, we do not attempt a recovery
scheme. Figures 20, 21 in Appendix A show samples of images that
evaded detection.
Adding Noise. We add i.i.d. Gaussian noise to fake images. The noise
variance is randomly sampled from U [5.0, 20.0] [80]. CycleGAN-
Zebra, and StyleGAN-Face2 shows significant drop in performance.
In both cases, noise degrades the quality of the fingerprint, making
them unsuitable for computing correlation with residual images. In
the case of CycleGAN-Zebra, the fingerprint classifier also fails to
detect model fingerprints. To recover, we apply a denoising filter
(Non-Local-Means) to all images in the test set, and also perform
adversarial training of the fingerprint classifier using the same
denoising filter. We apply this strategy to all datasets, and we can
see that performance of StyleGAN-Face2 and CycleGAN-Zebra are
regained to 81.4% and 85.9%, respectively, but recovery slightly
hurts BigGAN-BurgHV by 10% due to the denoising operation.
Fingerprint spoofing. Fingerprint spoofing attack aims to dis-
guise fake images to be from a specific camera device. This attack is
commonly studied in the camera fingerprinting literature [4, 44, 81].
We use the StyleGAN-Face1 dataset to evaluate this countermea-
sure. We consider a fingerprint substitution attack [44] using the
following formulation: Is = I − αFa + βFb. Fa, and Fb, are model
and camera fingerprints, respectively. Fa is computed using 200
fake images from StyleGAN-Face1, and Fb is a camera fingerprint
computed using 200 images from Canon EOS 5D Mark III (FFHQ
dataset). The first step is to verify that we have correctly spoofed
the camera fingerprint. We empirically estimate α as 1.5, and β
as 1.5, as the spoofed fingerprint shows low PCE correlation with
the model fingerprint, and high PCE correlation with the camera
fingerprint, while maintaining image quality. We then consider a
worst-case scenario for the defender, where the test set contains
200 spoofed fake images, and 200 real images which are used to
extract Fb. We perform detection on such test sets with 5 trials. We
obtain a low average F1 score of 66.67%. On closer investigation,
we find the fake image detection module performed poorly because
the fingerprints have been spoofed.
To recover from this attack, we utilize a different filter, i.e., a
normalizing box (blur) filter, instead of the Wavelet denoiser to
compute residuals. The intuition is that the spoofing attack does
not destroy all the artifacts (produced by the GAN), i.e., a model
fingerprint can still be extracted. In fact, the performance is regained
to 94.56% F1. Therefore, use of alternative filters for residual extraction
is an effective recovery strategy against fingerprint spoofing attacks.
One might argue that attackers can spoof the new residual space
used in the residual extractor again. However, an endless game of
switching residual extractors (multiple filters and filter parameters)
is unlikely. If an attacker tries spoofing against multiple filters, then
we observe that image quality deteriorates significantly. Image
samples spoofed against multiple filters are shown in Figure 19 in
the Appendix.
Adapting the GAN model. Can the attacker modify the GAN to
bypass detection? For example, for many DNN-based supervised
detection schemes, the attacker can use the defender’s classifier
as the GAN discriminator, and produce images that evade detec-
tion. In our case, such countermeasures are hard. First, the model
fingerprints extracted by NoiseScope is tied to the fundamental
building blocks of generative models, i.e., deconvolution layers (see
Section 5.1). One can try to change the deconvolution layer param-
eters, which will change the fingerprint patterns, but is unlikely
to make it similar to device fingerprints. Second, the attacker can
use the fake image detector component of NoiseScope as the GAN
discriminator. However, one has to ensure that the operations are
differentiable, which is non-trivial. Also, such an effort would be
similar to our previous countermeasures of spoofing the fingerprint