cache. At the end of the year 2017, novel attacks were demonstrated against
the CPU out-of-order engines and their branch predictors. These attacks
relied on the fact that microarchitectural side effects can be measured, even
though they are not directly accessible by any software code.
The two most destructive and effective hardware side-channel attacks were
named Meltdown and Spectre.
Meltdown
Meltdown (which has been later called Rogue Data Cache load, or RDCL)
allowed a malicious user-mode process to read all memory, even kernel
memory, when it was not authorized to do so. The attack exploited the out-of-
order execution engine of the processor and an inner race condition between
the memory access and privilege check during a memory access instruction
processing.
In the Meltdown attack, a malicious user-mode process starts by flushing
the entire cache (instructions that do so are callable from user mode). The
process then executes an illegal kernel memory access followed by
instructions that fill the cache in a controlled way (using a probe array). The
process cannot access the kernel memory, so an exception is generated by the
processor. The exception is caught by the application. Otherwise, it would
result in the termination of the process. However, due to the out-of-order
execution, the CPU has already executed (but not retired, meaning that no
architectural effects are observable in any CPU registers or RAM) the
instructions following the illegal memory access that have filled the cache
with the illegally requested kernel memory content.
The malicious application then probes the entire cache by measuring the
time needed to access each page of the array used for filling the CPU cache’s
block. If the access time is behind a certain threshold, the data is in the cache
line, so the attacker can infer the exact byte read from the kernel memory.
Figure 8-6, which is taken from the original Meltdown research paper
(available at the https://meltdownattack.com/ web page), shows the access
time of a 1 MB probe array (composed of 256 4KB pages):
Figure 8-6 CPU time employed for accessing a 1 MB probe array.
Figure 8-6 shows that the access time is similar for each page, except for
one. Assuming that secret data can be read one byte per time and one byte
can have only 256 values, knowing the exact page in the array that led to a
cache hit allows the attacker to know which byte is stored in the kernel
memory.
Spectre
The Spectre attack is similar to Meltdown, meaning that it still relies on the
out-of-order execution flaw explained in the previous section, but the main
CPU components exploited by Spectre are the branch predictor and branch
target predictor. Two variants of the Spectre attack were initially presented.
Both are summarized by three phases:
1. 
In the setup phase, from a low-privileged process (which is attacker-
controlled), the attacker performs multiple repetitive operations that
mistrain the CPU branch predictor. The goal is to train the CPU to
execute a (legit) path of a conditional branch or a well-defined target
of an indirect branch.
2. 
In the second phase, the attacker forces a victim high-privileged
application (or the same process) to speculatively execute instructions
that are part of a mispredicted branch. Those instructions usually
transfer confidential information from the victim context into a
microarchitectural channel (usually the CPU cache).
3. 
In the final phase, from the low-privileged process, the attacker
recovers the sensitive information stored in the CPU cache
(microarchitectural channel) by probing the entire cache (the same
methods employed in the Meltdown attack). This reveals secrets that
should be secured in the victim high-privileged address space.
The first variant of the Spectre attack can recover secrets stored in a victim
process’s address space (which can be the same or different than the address
space that the attacker controls), by forcing the CPU branch predictor to
execute the wrong branch of a conditional branch speculatively. The branch
is usually part of a function that performs a bound check before accessing
some nonsecret data contained in a memory buffer. If the buffer is located
adjacent to some secret data, and if the attacker controls the offset supplied to
the branch condition, she can repetitively train the branch predictor supplying
legal offset values, which satisfies the bound check and allows the CPU to
execute the correct path.
The attacker then prepares in a well-defined way the CPU cache (such that
the size of the memory buffer used for the bound check wouldn’t be in the
cache) and supplies an illegal offset to the function that implements the
bound check branch. The CPU branch predictor is trained to always follow
the initial legit path. However, this time, the path would be wrong (the other
should be taken). The instructions accessing the memory buffer are thus
speculatively executed and result in a read outside the boundaries, which
targets the secret data. The attacker can thus read back the secrets by probing
the entire cache (similar to the Meltdown attack).
The second variant of Spectre exploits the CPU branch target predictor;
indirect branches can be poisoned by an attacker. The mispredicted path of
an indirect branch can be used to read arbitrary memory of a victim process
(or the OS kernel) from an attacker-controlled context. As shown in Figure 8-
7, for variant 2, the attacker mistrains the branch predictor with malicious
destinations, allowing the CPU to build enough information in the BTB to
speculatively execute instructions located at an address chosen by the
attacker. In the victim address space, that address should point to a gadget.
A gadget is a group of instructions that access a secret and store it in a buffer
that is cached in a controlled way (the attacker needs to indirectly control the
content of one or more CPU registers in the victim, which is a common case
when an API accepts untrusted input data).
Figure 8-7 A scheme of Spectre attack Variant 2.
After the attacker has trained the branch target predictor, she flushes the
CPU cache and invokes a service provided by the target higher-privileged
entity (a process or the OS kernel). The code that implements the service
must implement similar indirect branches as the attacker-controlled process.
The CPU branch target predictor in this case speculatively executes the
gadget located at the wrong target address. This, as for Variant 1 and
Meltdown, creates microarchitectural side effects in the CPU cache, which
can be read from the low-privileged context.
Other side-channel attacks
After Spectre and Meltdown attacks were originally publicly released,
multiple similar side-channel hardware attacks were discovered. Even though
they were less destructive and effective compared to Meltdown and Spectre,
it is important to at least understand the overall methodology of those new
side-channel attacks.
Speculative store bypass (SSB) arises due to a CPU optimization that can
allow a load instruction, which the CPU evaluated not to be dependent on a
previous store, to be speculatively executed before the results of the store are
retired. If the prediction is not correct, this can result in the load operation
reading stale data, which can potentially store secrets. The data can be
forwarded to other operations executed during speculation. Those operations
can access memory and generate microarchitectural side effects (usually in
the CPU cache). An attacker can thus measure the side effects and recover
the secret value.
The Foreshadow (also known as L1TF) is a more severe attack that was
originally designed for stealing secrets from a hardware enclave (SGX) and
then generalized also for normal user-mode software executing in a non-
privileged context. Foreshadow exploited two hardware flaws of the
speculative execution engine of modern CPUs. In particular:
■    Speculation on inaccessible virtual memory. In this scenario, when
the CPU accesses some data stored at a virtual address described by a
Page table entry (PTE) that does not include the present bit (meaning
that the address is is not valid) an exception is correctly generated.
However, if the entry contains a valid address translation, the CPU
can speculatively execute the instructions that depend on the read
data. As for all the other side-channel attacks, those instructions are
not retired by the processor, but they produce measurable side effects.
In this scenario, a user-mode application would be able to read secret
data stored in kernel memory. More seriously, the application, under
certain circumstances, would also be able to read data belonging to
another virtual machine: when the CPU encounters a nonpresent entry
in the Second Level Address Translation table (SLAT) while
translating a guest physical address (GPA), the same side effects can
happen. (More information on the SLAT, GPAs, and translation
mechanisms are present in Chapter 5 of Part 1 and in Chapter 9,
“Virtualization technologies”).
■    Speculation on the logical (hyper-threaded) processors of a CPU’s
core. Modern CPUs can have more than one execution pipeline per
physical core, which can execute in an out-of-order way multiple
instruction streams using a single shared execution engine (this is
Symmetric multithreading, or SMT, as explained later in Chapter 9.)
In those processors, two logical processors (LPs) share a single cache.
Thus, while an LP is executing some code in a high-privileged
context, the other sibling LP can read the side effects produced by the
high-privileged code executed by the other LP. This has very severe
effects on the global security posture of a system. Similar to the first
Foreshadow variant, an LP executing the attacker code on a low-
privileged context can even spoil secrets stored in another high-
security virtual-machine just by waiting for the virtual machine code
that will be scheduled for execution by the sibling LP. This variant of
Foreshadow is part of the Group 4 vulnerabilities.
Microarchitectural side effects are not always targeting the CPU cache.
Intel CPUs use other intermediate high-speed buffers with the goal to better
access cached and noncached memory and reorder micro-instructions.
(Describing all those buffers is outside the scope of this book.) The
Microarchitectural Data Sampling (MDS) group of attacks exposes secrets
data located in the following microarchitectural structures:
■    Store buffers While performing store operations, processors write
data into an internal temporary microarchitectural structure called
store buffer, enabling the CPU to continue to execute instructions
before the data is actually written in the cache or main memory (for
noncached memory access). When a load operation reads data from
the same memory address as an earlier store, the processor may be
able to forward data directly from the store buffer.
■    Fill buffers A fill buffer is an internal processor structure used to
gather (or write) data on a first level data cache miss (and on I/O or
special registers operations). Fill buffers are the intermediary between
the CPU cache and the CPU out-of-order execution engine. They may
retain data from prior memory requests, which may be speculatively
forwarded to a load operation.
■    Load ports Load ports are temporary internal CPU structures used to
perform load operations from memory or I/O ports.
Microarchitectural buffers usually belong to a single CPU core and are
shared between SMT threads. This implies that, even if attacks on those
structures are hard to achieve in a reliable way, the speculative extraction of
secret data stored into them is also potentially possible across SMT threads
(under specific conditions).
In general, the outcome of all the hardware side-channel vulnerabilities is
the same: secrets will be spoiled from the victim address space. Windows
implements various mitigations for protecting against Spectre, Meltdown,
and almost all the described side-channel attacks.
Side-channel mitigations in Windows
This section takes a peek at how Windows implements various mitigations
for defending against side-channel attacks. In general, some side-channel
mitigations are implemented by CPU manufacturers through microcode
updates. Not all of them are always available, though; some mitigations need
to be enabled by the software (Windows kernel).
KVA Shadow
Kernel virtual address shadowing, also known as KVA shadow (or KPTI in
the Linux world, which stands for Kernel Page Table Isolation) mitigates the
Meltdown attack by creating a distinct separation between the kernel and user
page tables. Speculative execution allows the CPU to spoil kernel data when
the processor is not at the correct privilege level to access it, but it requires
that a valid page frame number be present in the page table translating the
target kernel page. The kernel memory targeted by the Meltdown attack is
generally translated by a valid leaf entry in the system page table, which
indicates only supervisor privilege level is allowed. (Page tables and virtual
address translation are covered in Chapter 5 of Part 1.) When KVA shadow is
enabled, the system allocates and uses two top-level page tables for each
process:
■    The kernel page tables map the entire process address space,
including kernel and user pages. In Windows, user pages are mapped
as nonexecutable to prevent kernel code to execute memory allocated
in user mode (an effect similar to the one brought by the hardware
SMEP feature).
■    The User page tables (also called shadow page tables) map only user
pages and a minimal set of kernel pages, which do not contain any
sort of secrets and are used to provide a minimal functionality for
switching page tables, kernel stacks, and to handle interrupts, system
calls, and other transitions and traps. This set of kernel pages is called
transition address space.
In the transition address space, the NT kernel usually maps a data structure
included in the processor’s PRCB, called
KPROCESSOR_DESCRIPTOR_AREA, which includes data that needs to
be shared between the user (or shadow) and kernel page tables, like the
processor’s TSS, GDT, and a copy of the kernel mode GS segment base
address. Furthermore, the transition address space includes all the shadow
trap handlers located in the “.KVASCODE” section of the NT Kernel image.
A system with KVA shadow enabled runs unprivileged user-mode threads
(i.e., running without Administrator-level privileges) in processes that do not
have mapped any kernel page that may contain secrets. The Meltdown attack
is not effective anymore; kernel pages are not mapped as valid in the
process’s page table, and any sort of speculation in the CPU targeting those
pages simply cannot happen. When the user process invokes a system call, or
when an interrupt happens while the CPU is executing code in the user-mode
process, the CPU builds a trap frame on a transition stack, which, as
specified before, is mapped in both the user and kernel page tables. The CPU
then executes the code of the shadow trap handler that handles the interrupt
or system call. The latter normally switches to the kernel page tables, copies
the trap frame on the kernel stack, and then jumps to the original trap handler
(this implies that a well-defined algorithm for flushing stale entries in the
TLB must be properly implemented. The TLB flushing algorithm is
described later in this section.) The original trap handler is executed with the
entire address space mapped.
Initialization
The NT kernel determines whether the CPU is susceptible to Meltdown
attack early in phase -1 of its initialization, after the processor feature bits are
calculated, using the internal KiDetectKvaLeakage routine. The latter obtains
processor’s information and sets the internal KiKvaLeakage variable to 1 for
all Intel processors except Atoms (which are in-order processors).
In case the internal KiKvaLeakage variable is set, KVA shadowing is
enabled by the system via the KiEnableKvaShadowing routine, which
prepares the processor’s TSS (Task State Segment) and transition stacks. The
RSP0 (kernel) and IST stacks of the processor’s TSS are set to point to the
proper transition stacks. Transition stacks (which are 512 bytes in size) are
prepared by writing a small data structure, called KIST_BASE_FRAME on
the base of the stack. The data structure allows the transition stack to be
linked against its nontransition kernel stack (accessible only after the page
tables have been switched), as illustrated by Figure 8-8. Note that the data
structure is not needed for the regular non-IST kernel stacks. The OS obtains
all the needed data for the user-to-kernel switch from the CPU’s PRCB. Each
thread has a proper kernel stack. The scheduler set a kernel stack as active by
linking it in the processor PRCB when a new thread is selected to be
executed. This is a key difference compared to the IST stacks, which exist as
one per processor.
Figure 8-8 Configuration of the CPU’s Task State Segment (TSS) when
KVA shadowing is active.
The KiEnableKvaShadowing routine also has the important duty of
determining the proper TLB flush algorithm (explained later in this section).
The result of the determination (global entries or PCIDs) is stored in the
global KiKvaShadowMode variable. Finally, for non-boot processors, the
routine invokes KiShadowProcessorAllocation, which maps the per-
processor shared data structures in the shadow page tables. For the BSP
processor, the mapping is performed later in phase 1, after the SYSTEM
process and its shadow page tables are created (and the IRQL is dropped to
passive level). The shadow trap handlers are mapped in the user page tables
only in this case (they are global and not per-processor specific).
Shadow page tables
Shadow (or user) page tables are allocated by the memory manager using the
internal MiAllocateProcessShadow routine only when a process’s address
space is being created. The shadow page tables for the new process are
initially created empty. The memory manager then copies all the kernel
shadow top-level page table entries of the SYSTEM process in the new
process shadow page table. This allows the OS to quickly map the entire
transition address space (which lives in kernel and is shared between all user-
mode processes) in the new process. For the SYSTEM process, the shadow
page tables remain empty. As introduced in the previous section, they will be
filled thanks to the KiShadowProcessorAllocation routine, which uses
memory manager services to map individual chunks of memory in the
shadow page tables and to rebuild the entire page hierarchy.
The shadow page tables are updated by the memory manager only in
specific cases. Only the kernel can write in the process page tables to map or
unmap chunks of memory. When a request to allocate or map new memory
into a user process address space, it may happen that the top-level page table
entry for a particular address would be missing. In this case, the memory
manager allocates all the pages for the entire page-table hierarchy and stores
the new top-level PTE in the kernel page tables. However, in case KVA
shadow is enabled, this is not enough; the memory manager must also write
the top-level PTE on the shadow page table. Otherwise, the address will be
not present in the user-mapping after the trap handler correctly switches the
page tables before returning to user mode.
Kernel addresses are mapped in a different way in the transition address
space compared to the kernel page tables. To prevent false sharing of
addresses close to the chunk of memory being mapped in the transition
address space, the memory manager always recreates the page table
hierarchy mapping for the PTE(s) being shared. This implies that every time
the kernel needs to map some new pages in the transition address space of a
process, it must replicate the mapping in all the processes’ shadow page
tables (the internal MiCopyTopLevelMappings routine performs exactly this
operation).
TLB flushing algorithm
In the x86 architecture, switching page tables usually results in the flushing
of the current processor’s TLB (translation look-aside buffer). The TLB is a
cache used by the processor to quickly translate the virtual addresses that are
used while executing code or accessing data. A valid entry in the TLB allows
the processor to avoid consulting the page tables chain, making execution
faster. In systems without KVA shadow, the entries in the TLB that translate
kernel addresses do not need to be explicitly flushed: in Windows, the kernel
address space is mostly unique and shared between all processes. Intel and
AMD introduced different techniques to avoid flushing kernel entries on
every page table switching, like the global/non-global bit and the Process-
Context Identifiers (PCIDs). The TLB and its flushing methodologies are
described in detail in the Intel and AMD architecture manuals and are not
further discussed in this book.
Using the new CPU features, the operating system is able to only flush
user entries and keep performance fast. This is clearly not acceptable in KVA
shadow scenarios where a thread is obligated to switch page tables even
when entering or exiting the kernel. In systems with KVA enabled, Windows
employs an algorithm able to explicitly flush kernel and user TLB entries
only when needed, achieving the following two goals:
■    No valid kernel entries will be ever maintained in the TLB when
executing a thread user-code. Otherwise, this could be leveraged by
an attacker with the same speculation techniques used in Meltdown,
which could lead her to read secret kernel data.
■    Only the minimum amount of TLB entries will be flushed when
switching page tables. This will keep the performance degradation
introduced by KVA shadowing acceptable.
The TLB flushing algorithm is implemented in mainly three scenarios:
context switch, trap entry, and trap exit. It can run on a system that either
supports only the global/non-global bit or also PCIDs. In the former case,
differently from the non-KVA shadow configurations, all the kernel pages
are labeled as non-global, whereas the transition and user pages are labeled
as global. Global pages are not flushed while a page table switch happens
(the system changes the value of the CR3 register). Systems with PCID
support labels kernel pages with PCID 2, whereas user pages are labelled
with PCID 1. The global and non-global bits are ignored in this case.
When the current-executing thread ends its quantum, a context switch is
initialized. When the kernel schedules execution for a thread belonging to
another process address space, the TLB algorithm assures that all the user
pages are removed from the TLB (which means that in systems with
global/non-global bit a full TLB flush is needed. User pages are indeed
marked as global). On kernel trap exits (when the kernel finishes code
execution and returns to user mode) the algorithm assures that all the kernel
entries are removed (or invalidated) from the TLB. This is easily achievable:
on processors with global/non-global bit support, just a reload of the page
tables forces the processor to invalidate all the non-global pages, whereas on
systems with PCID support, the user-page tables are reloaded using the User
PCID, which automatically invalidates all the stale kernel TLB entries.
The strategy allows kernel trap entries, which can happen when an
interrupt is generated while the system was executing user code or when a
thread invokes a system call, not to invalidate anything in the TLB. A
scheme of the described TLB flushing algorithm is represented in Table 8-1.
Table 8-1 KVA shadowing TLB flushing strategies
Configuration Type
User 
Pages
Kernel 
Pages
Transition 
Pages