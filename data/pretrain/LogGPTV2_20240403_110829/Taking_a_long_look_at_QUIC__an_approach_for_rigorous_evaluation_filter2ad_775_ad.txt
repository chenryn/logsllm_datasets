(c) Varying object size, 112 ms RTT with 10ms jitter that
causes packet reordering
(d) Varying #object, 1% Loss
(e) Varying #object, 112 ms RTT
(f) Varying #object, 112 ms RTT with 10ms jitter that
causes packet reordering
Figure 8: QUIC v34 vs. TCP at different rate limits, loss, and delay for different object sizes (a, b, and c) and different numbers
of objects (d, e, and f).
5.2 Page Load Time
This section evaluates QUIC performance compared to TCP for
loading web pages (i.e., page load time, or PLT) with different sizes
and numbers of objects. Recall from Sec. 3 that we measure PLT
using information gathered from Chrome, that we run TCP and
QUIC experiments back-to-back, and that we conduct experiments
in a variety of emulated network settings. Note that our servers
add all necessary HTTP directives to avoid caching content. We
also clear the browser cache and close all sockets between experi-
ments to prevent “warmed up” connections from impacting results.
However, we do not clear the state used for QUIC’s 0-RTT con-
nection establishment. Furthermore, our PLTs do not include any
DNS lookups. This is achieved by extracting resource loading time
details from Chrome and excluding the DNS lookups times.
performance gain for smaller object sizes is mainly due to QUIC’s
0-RTT connection establishment—substantially reducing delays
related to secure connection establishment that corresponds to a
substantial portion of total transfer time in these cases. To isolate
the impact of 0-RTT, we plotted performance differences between
QUIC with and without 0-RTT enabled in Fig. 7. As expected, the
benefit is relatively large for small objects and statistically insignif-
icant for 10MB objects.
To investigate the reason why QUIC performs poorly for large
numbers of small objects, we explored different values for QUIC’s
Maximum Streams Per Connection (MSPC) parameter to control
the level of multiplexing (the default is 100 streams). We found
there was no statistically significant impact for doing so, except
when setting the MSPC value to a very low number (e.g., 1), which
worsens performance substantially.
In the results that follow, we evaluate whether the observed
performance differences are statistically significant or simply due
to noise in the environment. We use the Welch’s t-test [14], a two-
sample location test which is used to test the hypothesis that two
populations have equal means. For each scenario, we calculate the
p-value according to the Welch’s t-test. If the p-value is smaller
than our threshold (0.01), then we reject the null hypothesis that
the mean performance for TCP and QUIC are identical, implying
the difference we observe between the two protocols is statistically
significant. Otherwise the difference we observe is not significant
and is likely due to noise.
Desktop environment. We begin with the desktop environ-
ment and compare QUIC with TCP performance for different rates,
object sizes, and object counts—without adding extra delay or loss
(RTT = 36ms and loss = 0%). Fig. 6 shows the results as a heatmap,
where the color of each cell corresponds to the percent PLT dif-
ference between QUIC and TCP for a given bandwidth (vertical
dimension) and object size/number (horizontal direction). Red indi-
cates that QUIC is faster (smaller PLT), blue indicates that TCP is
faster, and white indicates statistically insignificant differences.
Our key findings are that QUIC outperforms TCP in every sce-
nario except in the case of large numbers of small objects. QUIC’s
Instead, we focused on QUIC’s congestion control algorithm
and identified that in such cases, QUIC’s Hybrid Slow Start [24]
causes early exit from Slow Start due to an increase in the minimum
observed RTT by the sender, which Hybrid Slow Start uses as an
indication that the path is getting congested. This can hurt the
PLT significantly when objects are small and the total transfer time
is not long enough for the congestion window to increase to its
maximum value. Note that the same issue (early exit from Hybrid
Slow Start) affects the scenario with a large number of large objects,
but QUIC nonetheless outperforms TCP because it has enough time
to increase its congestion window and remain at high utilization,
thus compensating for exiting Slow Start early.12
Desktop with added delay and loss. We repeat the experi-
ments in the previous section, this time adding loss, delay, and jitter.
Fig. 8 shows the results, again using heatmaps.
Our key observations are that QUIC outperforms TCP under loss
(due to better loss recovery and lack of HOL blocking), and in high-
delay environments (due to 0-RTT connection setup). However,
in the case of high latency, this is not enough to compensate for
12We leave investigating the reason behind sudden increase in the minimum observed RTT when
multiplexing many objects to future work.
IMC ’17, November 1–3, 2017, London, United Kingdom
A. Molavi Kakhki et al.
Figure 9: Congestion window over time for QUIC and TCP
at 100Mbps rate limit and 1% loss.
Figure 11: QUIC vs. TCP when downloading a 210MB object.
Bandwidth fluctuates between 50 and 150Mbps (randomly
picks a rate in that range every one second). Averaging over
10 runs, QUIC is able to achieve an average throughput of
79Mbps (STD=31) while TCP achieves an average through-
put of 46Mbps (STD=12).
DSACK values in Fig. 10, demonstrating that in the presence of
packet reordering larger NACK thresholds substantially improve
end to end performance compared to smaller NACK thresholds.
We shared this result with a QUIC engineer, who subsequently
informed us that the QUIC team is experimenting with dynamic
threshold and time-based solutions to avoid falsely inferring loss
in the presence of reordering.
The previous tests set a
Desktop with variable bandwidth.
static threshold for the available bandwidth. However, in practice
such values will fluctuate over time, particularly in wireless net-
works. To investigate how QUIC and TCP compare in environments
with variable bandwidth, we configured our testbed to change the
bandwidth randomly within specified ranges and with different
frequencies.
Fig. 11 shows the throughput over time for three back-to-back
TCP and QUIC downloads of a 210MB object when the bandwidth
randomly fluctuates between 50 and 150Mbps. As shown in this
figure, QUIC is more responsive to bandwidth changes and is able
to achieve a higher average throughput compared to TCP. We re-
peated this experiment with different bandwidth ranges and change
frequencies and observed the same behavior in all cases.
Due to QUIC’s implementation in
Mobile environment.
userspace (as opposed to TCP’s implementation in the OS kernel),
resource contention might negatively impact performance indepen-
dent of the protocol’s optimizations for transport efficiency. To test
whether this is a concern in practice, we evaluated an increasingly
common resource-constrained deployment environment: smart-
phones. We use the same approach as in the desktop environment,
controlling Chrome (with QUIC enabled) over two popular Android
phones: the Nexus 6 and the MotoG. These phones are neither top-
of-the-line, nor low-end consumer phones, and we expect that they
approximate the scenario of a moderately powerful mobile device.
Fig. 12 shows heatmaps for the two devices when varying band-
width and object size.14 We find that, similar to the desktop envi-
ronment, in mobile QUIC outperforms TCP in most cases; however,
its advantages diminish across the board.
To understand why this is the case, we investigate the QUIC
congestion control states visited most in mobile and non-mobile
scenarios under the same network conditions. We find that in mo-
bile QUIC spends most of its time (58%) in the “Application Limited”
14We omit 100 Mbps because our phones cannot achieve rates beyond 50 Mbps over WiFi, and we
omit results from varying the number of objects because they are similar to the single-object cases.
Figure 10: QUIC vs. TCP when downloading a 10MB page
(112 ms RTT with 10ms jitter that causes packet reordering).
Increasing the NACK threshold for fast retransmit allows
QUIC to cope with packet reordering.
QUIC’s poor performance for large numbers of small objects. Fig. 9
shows the congestion window over time for the two protocols at
100Mbps and 1% loss. Similar to Fig. 5, under the same network
conditions QUIC better recovers from loss events and adjusts its
congestion window faster than TCP, resulting in a larger congestion
window on average and thus better performance.
Under variable delays, QUIC performs significantly worse than
TCP. Using our state machine approach, we observed that under
variable delay QUIC spends significantly more time in the recovery
state compared to relatively stable delay scenarios. To investigate
this, we instrumented QUIC’s loss detection mechanism, and our
analysis reveals that variable delays cause QUIC to incorrectly infer
packet loss when jitter leads to out-of-order packet delivery. This
occurs in our testbed because netem adds jitter by assigning a delay
to each packet, then queues each packet based on the adjusted send
time, not the packet arrival time—thus causing packet re-ordering.
The reason that QUIC cannot cope with packet re-ordering is that
it uses a fixed threshold for number of NACKs (default 3) before it
determines that a packet is lost and responds with a fast retransmit.
Packets reordered deeper than this threshold cause false positive
loss detection.13 In contrast, TCP uses the DSACK algorithm [41] to
detect packet re-ordering and adapt its NACK threshold accordingly.
As we will show later in this section, packet reordering occurs in
the cellular networks we tested, so in such cases QUIC will benefit
from integrating DSACK. We quantify the impact of using larger
13Note that reordering impact when testing small objects is insignificant because QUIC does not
falsely detect losses until a sufficient number of packets are exchanged.
10305070Cong. Win. (KB)QUIC10305070 1 2 3 4 5 6 7 8 9 10Cong. Win. (KB)Time (s)TCP04080120 0 50 100 150 200Throughput (Mbps)Time (s)TCPQUICTaking a Long Look at QUIC
IMC ’17, November 1–3, 2017, London, United Kingdom
(a) MotoG, No added loss or latency
(b) MotoG, 1% Loss
(c) MotoG, 112 ms RTT
(d) Nexus6, No added loss or latency
(e) Nexus6, 1% Loss
(f) Nexus6, 112 ms RTT
Figure 12: QUICv34 vs. TCP for varying object sizes on MotoG and Nexus6 smartphones (using WiFi). We find that QUIC’s
improvements diminish or disappear entirely when running on mobile devices.
(a) Varying object size
(b) Varying object count
Figure 14: QUICv34 vs. TCP over Verizon and Sprint cellular
networks.
(a) MotoG
(b) Desktop
Figure 13: QUIC state transitions on MotoG vs. Desktop.
QUICv34, 50 Mbps, no added loss or delay. Red numbers in-
dicate the fraction of time spent in each state, and black
numbers indicate the state-transition probability. The fig-
ure shows that poor performance for QUIC on mobile de-
vices can be attributed to applications not processing pack-
ets quickly enough. Note that the zero transition probabili-
ties are due to rounding down.
state, which contrasts substantially with the desktop scenario (only
7% of the time). The reason for this behavior is that QUIC runs in
a userspace process, whereas TCP runs in the kernel. As a result,
QUIC is unable to consume received packets as quickly as on a desk-
top, leading to suboptimal performance, particularly when there is
ample bandwidth available.15 Fig. 13 shows the full state diagram
(based on server logs) in both environments for 50Mbps with no
added latency or loss. By revealing the changes in time spent in
each state, such inferred state machines help diagnose problems
and develop a better understanding of QUIC dynamics.
Tests on commercial cellular networks. We repeated our
PLT tests—without any network emulation—over Sprint’s and Veri-
zon’s cellular networks, using both 3G and LTE. Table 5 shows the
characteristics of these networks at the time of the experiment. To
isolate the impact of the network from that of the device they run
15A parallel study from Google [26] using aggregate data identifies the same performance issue but
does not provide root cause analysis.
RTT (STD)
Reordering
(ms)
3G
(%)
Loss
(%)
Thrghpt.
(Mbps)
3G LTE
4.0
2.4
Verizon 0.17
0.31
Sprint
LTE
109 (20) 62 (14)
70 (39)
3G LTE
9
0.25
0.13
55 (11) 1.38
3G LTE
0.05
0
0.02 0.02
Table 5: Characteristics of tested cell networks. Throughput
and RTT represent averages.
on, we used our desktop client tethered to a mobile network instead
of using a mobile device (because the latter leads to suboptimal
performance for QUIC, shown in Fig. 12 and 13). We otherwise
keep the same server and client settings as described in Sec. 3.1.
Fig. 14 shows the heatmaps for these tests. For LTE, QUIC per-
forms similarly to a desktop environment with low bandwidth
(Fig. 7). In these cell networks, the benefit of 0-RTT is larger for the
1MB page due to higher latencies in the cellular environment.
In the case of 3G, we see the benefits of QUIC diminish. Com-
pared to LTE, the bandwidth in 3G is much lower and the loss is
higher—which works to QUIC’s benefit (see Fig. 8a). However, the
packet reordering rates are higher compared to LTE, and this works
to QUIC’s disadvantage. Note that in 3G scenarios, in many cases
QUIC had better performance on average (i.e., lower average PLT);
however, the high variance resulted in high p-values, which means
we cannot reject the null hypothesis that the two sample sets were
drawn form the same (noisy) distribution.
5.3 Video-streaming Performance
This section investigates QUIC’s impact on video streaming in
the desktop environment. Unlike page load times, which tend to
IMC ’17, November 1–3, 2017, London, United Kingdom
A. Molavi Kakhki et al.
(a) MACW=430
(b) MACW=2000
Figure 15: QUIC (version 37) vs. TCP with different max-
imum allowable congestion window (MACW) size. In (a),
MACW=430 and QUIC versions 34 and 37 have identical per-
formance (see Fig. 6a), In (b), we use the default MACW=2000
for QUIC 37, which results in higher throughput and larger
performance gains for large transfers.
be limited by RTT and multiplexing, video streaming relies on
the transport layer to load large objects sufficiently quickly to
maintain smooth playback. This exercises a transport-layer’s ability
to quickly ramp up and maintain high utilization.
We test video-streaming performance using YouTube, which
supports both QUIC and TCP. We evaluate the protocols using well
known QoE metrics for video such as the time spent waiting for
initial playback, and the number of rebuffering events. For the latter
metric, Google reports that, on average users experience 18% fewer
re-buffers when watching YouTube videos over QUIC [26].
We developed a tool for automatically streaming a YouTube video
and logging quality of experience (QoE) metrics via the API men-
tioned in Sec. 3.3. The tool opens a one-hour-long YouTube video,
selects a specific quality level, lets the video run for 60 seconds,
and logs the following QoE metrics: time to start the video, video
quality, quality changes, re-buffering events, and fraction of video
loaded. As we demonstrated in previous work [31], 60 seconds is
sufficient to capture QoE differences. We use this tool to stream
videos using QUIC and TCP and compare the QoE.
Table 6 shows the results for 100 Mbps bandwidth and 1% loss,16
a condition under which QUIC outperforms TCP (Sec. 8). In this
environment, at low and medium resolutions we see no significant
difference in QoE metrics, but for the highest quality, hd2160, QUIC
is able to load a larger fraction of the video in 60 seconds and
experience fewer rebuffers per time played, which is consistent
with our PLT test results (Sec. 5.2) and with what Google reported.
Thus, to refine their observations, we find that QUIC can outperform
TCP for video streaming, but this matters only for high resolutions.
5.4 Historical Comparison
To understand how QUIC performance has changed over time, we
evaluated 10 QUIC versions (25 to 34)17 in our testbed. In order
to only capture differences due to QUIC version changes, and not
due to different configuration values, we used the same version of
Chrome and the same QUIC parameter configuration.
We found that when using the same configuration most QUIC