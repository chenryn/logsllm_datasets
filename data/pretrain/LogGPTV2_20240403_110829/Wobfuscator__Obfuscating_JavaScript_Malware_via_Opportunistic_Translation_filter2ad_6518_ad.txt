into a JavaScript file. The Wabt.js (1.0.23) [8] package is used
to generate the WebAssembly modules used in the obfuscation.
at
https://github.com/js2wasm-obfuscator/translator. We make
the implementation of Wobfuscator available upon request. We
believe this strategy minimizes the threat of nefarious usage
while also aiding researchers in independently reproducing
our results, in confirming the identified weaknesses of existing
malware detectors, and in serving as a foundation for future
research on improving malware detectors.
from our
evaluation
is
available
VI. EVALUATION
We evaluate Wobfuscator and its ability to obfuscate mali-
cious JavaScript using opportunistic translation to WebAssem-
bly along the following main research questions:
• RQ1 – Effectiveness: How effective is the approach at
evading state-of-the-art JavaScript malware detectors and
which transformations are most effective? How does our
approach compare with other state-of-the-art obfuscators?
• RQ2 – Correctness: Do our code transformations preserve
the semantics of the transformed code?
• RQ3 – Efficiency: How much runtime and code size
overhead do the transformations impose, and how long does
applying the transformations take?
To investigate these questions, we perform a comprehensive
analysis on the effectiveness of our transformations in evading
detection. We evaluate state-of-the-art detection tools against
Wobfuscator on a large dataset of malicious and benign
files. We evaluate the obfuscation advantage produced by
Wobfuscator by comparing our approach against state-of-the-
art open-source obfuscation tools. Lastly, we use the extensive
test suites of widely used and mature npm modules to verify
the correctness of our tool and demonstrate the runtime and
code size overhead are acceptable for real-world usage.
A. Experimental Setup
1) Datasets: Due to different requirements, we use dif-
ferent datasets of JavaScript programs for different research
questions. To answer RQ1, we need to train and apply state-of-
the-art JavaScript malware detectors to large sets of real-world
benign and malicious JavaScript code. Table II summarizes
the datasets we use. The benign code consists of 149,677 files
from the JS150k dataset [49]. The malicious code consists
of 43,499 samples, with 2,674 samples from VirusTotal [7],
39,450 samples from the Hynek Petrak JavaScript malware
collection [47], and 1,375 samples from the GeeksOnSecurity
malicious JavaScript dataset [11]. These datasets are broken
down further by the malware categories that they contain, such
as trojans, ransomware, droppers. We list the breakdown of the
malicious datasets in the first four columns of Table IX.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:59:50 UTC from IEEE Xplore.  Restrictions apply. 
81581
DATASETS FOR EVALUATING EFFECTIVENESS (RQ1).
TABLE II
Datasets
Benign
Malicious
JS150k
VirusTotal
Hynek Petrak
GeeksOnSecurity
Total Malicious
# Samples (Files)
149,677
2,674
39,450
1,375
43,499
Answering RQ2 and RQ3 requires executing code before
and after applying our transformations. We use a dataset of
popular and large JavaScript projects on NPM with their test
suites. To identify suitable projects, we select from the most
depended-upon NPM modules [12] six modules that contain
extensive test suites (first column of Table V).
2) JavaScript Malware Detectors: We evaluate our obfus-
cation technique against four state-of-the-art, static, learning-
based JavaScript malware detectors. To train them, we split
the benign and malicious datasets into training, validation,
and test sets containing 70%, 15%, and 15% of the samples,
respectively. We follow the steps provided by each project to
train the detection models with the desired configuration.
Cujo [50] is a hybrid JavaScript malware detector that de-
tects drive-by download attacks. It performs a lexical analysis
of JavaScript files run on a website as well as a dynamic
analysis by monitoring abstracted runtime behaviors. For our
evaluation, we use the static detection part, based on a reim-
plementation of Cujo provided by Fass et al. [18].
Zozzle [24] is a mostly-static in-browser detection tool that
uses syntactic information, such as identifier names and code
locations, obtained from a JavaScript AST to identify mali-
cious code. These features are input to a Bayesian classifier
to label the samples as benign or malicious. We rely on a
reimplementation of Zozzle provided by Fass et al. [19].
JaSt [28] is a static detector of malicious JavaScript that
uses syntactic information from the AST to produce n-grams
of sequential nodes to identify patterns indicative of malicious
behavior. We use the implementation made available on the
project’s GitHub page [16].
JStap [27] is a static malware detector that leverages syntax,
control-flow, and data-flow information by creating an AST, a
Control Flow Graph (CFG), and a Program Dependency Graph
(PDG), depending on the configuration. The tool extracts fea-
tures either by constructing n-grams of nodes or by combining
the AST node type with its corresponding identifier/literal
value. In our evaluation, we focus on the PDG code abstraction
with both the n-grams and values feature extraction modes. We
use the implementation available on GitHub [17].
3) JavaScript Obfuscation Tools: We compare Wobfusca-
tor against four open-source state-of-the-art JavaScript ob-
fuscation tools. JavaScript Obfuscator [4] is a JavaScript
obfuscation tool that supports multiple obfuscation techniques
including variable renaming, dead code injection, and control-
flow flattening. Gnirts [15] focuses on mangling string literals
within JavaScript files. Jfogs [70] is an obfuscation tool that
focuses on removing function call identifiers and parameters
from call sites. JSObfu [5] is an obfuscator that supports
converting function identifiers and string literals into expres-
sions that evaluate to constants. This obfuscator also supports
character escaping, whitespace removal, and more.
All the experiments on a desktop containing an Intel Core i7
PI:EMAIL w/ 32 GB of memory running Ubuntu 20.04.
B. Effectiveness in Evading Detection (RQ1)
1) Effectiveness of Our Approach: To evaluate the effec-
tiveness of Wobfuscator at evading static malicious JavaScript
detectors, we compare the detectors’ performance on the
original input programs against their performance after our
obfuscation has been applied. Since the detectors classify
each program as benign or malicious, the usual metrics of
binary classifiers apply: precision and recall. Precision is
the number of true positives (correctly identified malicious
programs) divided by the number of all raised alarms (correct
or not), and recall is the number of true positives divided
by the number of all malicious programs in the dataset.
T P +F N . A good malware detector
That is P rec= T P
should offer both high precision and high recall. Low precision
indicates a high number of false positives, which would cause
the system to block and break benign scripts and commonly
used websites. Such a tool would not be adopted by actual
users. Low recall means few of the actual malicious programs
are detected, limiting the usefulness of the detector. The main
goal of our obfuscation is to reduce the recall of detectors.
T P +F P , Rec= T P
Some detectors fail to parse some of the original and trans-
formed code samples due to outdated or incomplete support
of the JavaScript language. Since this is an implementation-
specific detail of these detectors rather than a result of their
detection methodology, we choose to exclude these samples
from the count rather than mark them as false negatives. As a
result, the denominators of the recall results differ depending
on the detector and the applied transformations.
Results: Table III shows the recall of the detectors described
in Section VI-A2 (columns) when run on code obfuscated by
our transformations (rows). The first row gives each detector’s
recall without our obfuscation, which serves as a baseline.
The middle part of the table shows results from applying
only one kind of transformation at a time. For example, the
second row shows that applying our synchronous transforma-
tion technique T1-StringLiteral on the test set of malicious
samples, Cujo achieves a recall of 0.61, i.e., a significant
reduction compared to the baseline of 0.98. The results show
that different translation techniques are more effective against
some detectors rather than others. For each detector,
the
lowest recall score is bold-faced to reveal the best-performing
individual transformation technique. For example, we find that
T1-StringLiteral performs best for Cujo, Zozzle, and JStap
in values mode, T4-CallExpression(a) performs best for JaSt,
and T4-CallExpression(b) performs best for JStap in n-grams
mode. Since each transformation rule is effective at reducing
the recall for at least one detector, all transformation rules are
integral to the effectiveness of our approach.
We explain the reasons why some detection tools disfavor
certain transformation rules over others. Cujo performs a
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:59:50 UTC from IEEE Xplore.  Restrictions apply. 
91582
RECALL OF MALWARE DETECTORS ON CODE OBFUSCATED BY WOBFUSCATOR. LOWEST RECALL IN BOLD.
TABLE III
Technique
Baseline: No transformation
Cujo
0.98 (5,548/5,649)
Zozzle
0.66 (3,598/5,453)
JaSt
0.99 (5,076/5,108)
JStap (NGrams)
0.99 (4,483/4,524)
JStap (Values)
0.98 (4,439/4,524)
Individual transformations:
Sync, T1-StringLiteral
Sync, T2-ArrayInitialization
Sync, T3-FunctionName
Sync, T4-CallExpression(a)
Sync, T4-CallExpression(b)
Sync, T5-IfStatement
Sync, T6-ForStatement
Sync, T7-WhileStatement
0.61 (1,623/2,644)
0.94 (4,050/4,292)
0.67 (2,780/4,159)
0.71 (3,040/4,285)
0.58 (2,385/4,115)
0.82 (3,513/4,301)
0.90 (3,877/4,299)
0.90 (3,904/4,321)
0.62 (3,387/5,453)
0.66 (3,593/5,450)
0.65 (3,550/5,453)
0.64 (3,507/5,453)
0.63 (3,424/5,453)
0.64 (3,505/5,453)
0.66 (3,578/5,453)
0.66 (3,598/5,453)
0.66 (3,393/5,108)
0.85 (4,360/5,105)
0.69 (3,512/5,108)
0.38 (1,943/5,108)
0.44 (2,253/5,108)
0.89 (4,535/5,108)
0.92 (4,720/5,108)
0.96 (4,882/5,108)
0.36 (1,539/4,257)
0.86 (3,890/4,505)
0.57 (2,747/4,810)
0.37 (1,723/4,633)
0.23 (1,058/4,586)
0.83 (3,717/4,501)
0.87 (3,872/4,465)
0.98 (4,410/4,502)
0.43 (1,839/4,257)
0.89 (4,009/4,505)
0.72 (3,463/4,810)
0.78 (3,613/4,633)
0.73 (3,369/4,586)
0.93 (4,178/4,501)
0.98 (4,360/4,465)
0.98 (4,412/4,502)
All sync (using T4(a))
All sync (using T4(b))
All async
0.18 (416/2,255)
0.19 (415/2,205)
0.28 (1,490/5,297)
0.63 (3,450/5,450)
0.63 (3,428/5,450)
0.65 (3,524/5,453)
0.22 (1,104/5,105)
0.18 (931/5,105)
0.20 (1,085/5,453)
0.00 (1/4,235)
0.00 (0/4,243)
0.00 (4/4,612)
0.18 (766/4,235)
0.08 (350/4,243)
0.22 (959/4,267)
Combined transformations:
Fig. 4. Baseline recall vs. obfuscated recall.
lexical analysis on malware files, so it learns the suspicious
features of strings that are indicative of malware. Since T1-
StringLiteral removes these identifiers from the file, Cujo is
not able to find suspicious tokens in the file. Zozzle identifies
malicious combinations of syntax features and the context
they are used in. Since T1-StringLiteral removes the usage
of malicious strings, such as shell code and dynamic code,
malicious files that have the majority of their code leveraging
these strings will have indicative features removed from their
AST, allowing them to evade Zozzle. JaSt uses n-grams
of AST syntax nodes (with a length of four by default)
to detect malicious patterns. T4-CallExpression(a) replaces a
single CallExpression node with multiple nodes handling the
WebAssembly module instantiation. Since this translation adds
more nodes than the length of sliding window, it can throw off
the detection of known malicious n-gram patterns. JStap in its
n-grams mode generates a program dependency graph (PDG)
by adding edges representing data flow to the AST of an input
file. Since T4-CallExpression(b) (and other transformations)
route data flow through the generated WebAssembly module,
it hinders the n-gram features that JStap can extract when
performing its data flow analysis. JStap in its values mode
is most affected by the T1-StringLiteral transformation since,
in this mode, it relies on literals when selecting features to
extract. T1-StringLiteral removes some literals.
The lower part of Table III shows results from applying
all transformation rules at once. We report results for three
transformation combinations. “All sync (using T4(a))” and
“All sync (using T4(b))” mean all synchronous transformations
with T4(a) being used and with T4(b) being used, respectively.
“All async” means all transformations in Table I (aside from
T4(b)) in their asynchronous variant. We find that combining
all
transformation rules greatly reduces the recall of the
detectors. In particular, with the “All sync (using T4(a))” set
of transformations, Cujo, Zozzle, JaSt, JStap (NGrams), and
JStap (Values) have a recall of 0.18, 0.63, 0.22, 0.00, and 0.18,
respectively. Because of its performance and compatibility
with the WebAssembly MVP language, we select “All sync
(using T4(a))” to be the default configuration for Wobfuscator.
Figure 4 visualizes the results for “All sync (using T4(a))”.
The main goal of our work is to reduce the recall of
detectors, but we also measure their precision. The precision
values for the transformations are listed in Table VII. For
most applied transformations, the precision remains between
0.9 and 1.0. However, certain transformations can greatly
impact the precision on some detectors, e.g., “All sync (using
T4(a))” reduces JStap (NGrams) precision to 0.5. This shows
that while reducing the precision is not its main objective,
Wobfuscator can reduce the precision of certain detectors.
2) Comparison with Other Obfuscators: To demon-
strate how Wobfuscator compares against currently available
JavaScript obfuscators, we evaluate four obfuscation tools
on the same dataset used in Section VI-B1. We collect the
precision and recall values obtained by the five malware
detection tools when evaluated on a dataset obfuscated by each
tool. Similar to Section VI-B1, some detectors fail to parse
certain obfuscated files, leading to different denominators in
the values within the same detector column.
Results: Table IV shows the recall values of the detection
tools (columns) when run on code obfuscated by each of the
four obfuscation tools described in Section VI-A3 (rows). The
last row shows the best recall values obtained by Wobfuscator.
The results show that Wobfuscator outperforms current
obfuscators when compared on the recall reduction of malware
detectors. The only exception occurs when Jfogs is evaluated
against JaSt. In this case, Jfogs’ recall rate of 0.00 outperforms
Wobfuscator’s recall rate of 0.18. Jfogs’ obfuscation primarily
replaces identifiers and literals with new intermediate vari-
ables, so Wobfuscator could be used to compliment Jfogs.
For example, Jfogs moves string literals into variables, but
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:59:50 UTC from IEEE Xplore.  Restrictions apply. 
101583
0.980.660.990.990.980.180.630.220.000.180.000.200.400.600.801.00CujoZozzleJaStJStap(NGrams)JStap(Values)BaselineAll sync (using T4(a))RECALL OF MALWARE DETECTORS ON CODE OBFUSCATED BY WOBFUSCATOR AND OTHER OBFUSCATORS.
TABLE IV
Obfuscator
JavaScript Obfuscator
Gnirts
Jfogs
JSObfu
Wobfuscator (best recall)
Cujo
1.00 (4,406/4,415)
0.98 (5,548/5,649)
0.77 (3,515/4,562)
1.00 (4,994/5,008)
0.18 (416/2,255)
Zozzle
0.70 (3,807/5,453)
0.66 (3,598/5,453)
0.66 (3,584/5,453)
0.84 (4,467/5,324)
0.62 (3,387/5,453)
JaSt
0.81 (4,153/5,108)
0.99 (5,076/5,108)
0.00 (26/5,453)
0.29 (1,456/4,979)
0.18 (931/5,105)
JStap (NGrams)
0.43 (2,005/4,717)
0.99 (4,483/4,524)
0.00 (16/5,025)