(cid:82) 1
approaches 1 and (cid:82) 1
We now analyze how close our f (x) is to the optimal
sampling distribution. We Ô¨Årst try to compute the normal-
ized probability density function by normalizing f (x) by an
0 f (x)dx factor. Unfortunately, f (x) goes to inÔ¨Ånity as x
0 f (x)dx = ‚àû. Luckily, we can restrict
the sampling domain from 0 to 1 ‚àí Œ¥ and have the veriÔ¨Åer
always check the last Œ¥ fraction of the blocks directly. We
(cid:82) 1+ac‚àíc
Fig. 1. s(x) deÔ¨Ånes the probability density function (PDF) for the protocol
from Section V-B. g(x) =
(x‚àí1) ln(Œ¥) is the optimized PDF. The integral
g(x)dx for c = 1/2, Œ¥ = 2‚àí10, a = 0 and a = 0.8 respectively
1
a
is displayed.
As a Ô¨Årst step, we show that the probability density func-
tion (PDF) of the optimal sampling distribution must be
increasing. A PDF f deÔ¨Åned over the continuous range [0, 1]
is increasing if, for all a, b ‚àà [0, 1], b > a =‚áí f (b) ‚â• f (a).
For any distribution deÔ¨Åned by a PDF that is not increasing,
there exists a distribution that results in an equal or greater
probability of catching the adversary.
Lemma 3 (Non-Increasing Sampling Distribution). A sam-
pling distribution over the blocks deÔ¨Åned by a non-increasing
PDF f is not uniquely optimal,
there exists another
distribution with equal or higher probability of catching the
i.e.,
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:03:03 UTC from IEEE Xplore.  Restrictions apply. 
937
0.00.20.40.60.81.0x0.51.01.52.02.53.0Queryingdistributionf(x)s(x)01ùëé1‚àí1‚àíùëéùëêValid blocksInvalid blocksValid blocksMalicious ChainHonest ChainSample enough random blocks from here s.t.at least one invalid block is selected GenesisBlockFork pointwill later Ô¨Ånd the optimal value for Œ¥. Let
g(x) =
f (x)
(cid:82) 1‚àíŒ¥
(cid:82) 1+ac‚àíc
0
1
=
(x ‚àí 1) ln(Œ¥)
.
f (x)dx
g(x)dx = (cid:82) 1‚àíc
to
The probability of catching the adversary is equal
p = min0‚â§a‚â§ c‚àíŒ¥
(x‚àí1) ln(Œ¥) dx =
logŒ¥(c). This probability takes into account that all blocks
in the last Œ¥ fraction of the chain are always veriÔ¨Åed by the
veriÔ¨Åer. Any fork after c‚àíŒ¥
c will contain at least a block from
this Œ¥ region, and thus will be detected with probability 1.
a
0
1
c
1
We will now show that g(x) deÔ¨Ånes an optimal sampling
distribution by showing that no sampling distribution can
achieve a higher p value, i.e., a higher probability of catching
the adversary with a single query. Note that the sampling
strategy is optimal for an optimal adversary. This is a strong
argument as the optimal adversary can choose the placement
of its invalid blocks after learning the sampling strategy.
Theorem 2 (Optimal Sampling Distribution). Given that the
last Œ¥ = ck, c ‚àà (0, 1], k ‚àà N fraction of the chain contains
only valid blocks and the adversary can at most create a c
fraction of valid blocks after the fork point a, the sampling
(x‚àí1) ln(Œ¥) maximizes
distribution deÔ¨Åned by the PDF g(x) =
the probability of catching any adversary that optimizes the
placement of invalid blocks.
Proof. Let Œ¥ = ck, for some k ‚àà N, we get that p = 1/k
and that as k increases the success probability decreases.
Hence, the smaller Œ¥ is set, the fewer the blocks that are
always checked near the tip of the chain but the worse our
probability of catching the adversary with a sample anywhere
else. Therefore, a smaller Œ¥ leads to more samples from the
rest of the chain.
Say g‚àó(x) is the probability density function of
the
best
sampling distribution. Note that given Lemma 3,
g‚àó(x)
increasing and therefore for an optimal ad-
versary the success probability is denoted by p‚àó =
mina,0‚â§a‚â§ c‚àíd
g‚àó(x)dx.
(cid:82) 1+ac‚àíc
implies that(cid:82) 1‚àíci+1
have that (cid:82) 1‚àíck
have (cid:82) 1‚àíck
g‚àó(x), therefore, maximizes p‚àó. The optimality condition
g‚àó(x)dx ‚â• p‚àó, for all integers i ‚àà [0, k],
where a = 1 ‚àí ci is a possible forking point. Further we
g‚àó(x)dx = 1 since g‚àó(x) is a PDF. We
g‚àó(x)dx = 1 ‚â• k ¬∑ p‚àó.
This implies that p‚àó ‚â§ 1/k. Note that g(x) as a candidate
distribution achieves p = 1/k and is, therefore, optimal.
g‚àó(x)dx =(cid:80)k
(cid:82) 1‚àíci+1
1‚àíci
1‚àíci
i=0
is
a
0
0
c
Optimizing the Proof Size. Given g(x) and p, we can now
deÔ¨Åne pm = (1 ‚àí 1
k )m as the probability of failure, i.e., not
catching the optimal adversary after m independent queries.
Note that without loss of generality, k ‚â• 1 as otherwise Œ¥ >
c, implying that a sufÔ¨Åcient fraction of blocks are checked
to catch any adversary. If we want pm ‚â§ 2‚àíŒª, then m ‚â•
. Now, assuming that the veriÔ¨Åer always checks
log1/2(1‚àí 1
k )
Œª
2 ) ln(n) =
the last L blocks of the chain, where L = Œ¥n = ckn, we get
that k = logc
(cid:0) L
n
(cid:1) and m ‚â•
(cid:18)
(cid:19).
that m approximates Œª logc( 1
2 ) ln(n) = 1.
Œª
1‚àí 1
m
Œª logc( 1
logc ( L
n
log1/2
)
This means
O(Œª log1/c(n)), thus, limn‚Üí‚àû
As long as L is a constant, the number of queries is linear in
the security parameter Œª and logarithmic in the chain length,
n.
Verifying Trailing Blocks. The number of blocks checked at
the end of the chain (denoted by L) affects the total number
of samples needed, m. We can, therefore, further optimize it
to get an optimal proof size. It is important to ensure that
L is bounded from below by the particular (c, L)-adversary
assumption that
is used. Given this one can numerically
optimize L as we do in our implementation. However, as long
as that number is a constant we get an asymptotically optimal
proof size:
Corollary 2. Under the (c, L)-adversary assumption for any
constant L and using a collision-resistant hash function the
FlyClient proof size is Œò(Œª log(n) log 1
(n) + L)
c
The corollary is a result of the proof size computations
above for a negligible failure probability of 2‚àíŒª and the size
of each Merkle path being log2(n).
Note that unlike in the superblock-based NIPoPoW [14] this
result holds for all adversaries not just in an optimistic setting.
For realistic Ethereum values of Œª = 50, n = 222, c = 1
2 bytes,
the total proof size is just below 400 KB (See Section VII).
VI. FLYCLIENT UNDER VARIABLE DIFFICULTY
So far, we have only considered the simplistic case that
all blocks have the same difÔ¨Åculty. This is not realistic as
the number of miners as well as their hardware continuously
changes.
1
Information theoretically, the distributional view analysis
described in Section V-C allows us to also handle the variable-
difÔ¨Åculty scenario. In the new model, we simply use the same
sampling distribution g(x) =
(x‚àí1) ln Œ¥ , but now x denotes
the relative aggregate difÔ¨Åculty weight and Œ¥ denotes the
relative difÔ¨Åculty weight of the blocks which are sampled with
probability 1. For example, x = 1/2 is the point in the chain
where half of the difÔ¨Åculty has been amassed, and g(1/2)
is the probability that the block at that point is sampled by
FlyClient. Note that x = 1/2 may refer to a very recent block
in the chain if the block difÔ¨Åculty grows fast.
A. Variable DifÔ¨Åculty MMR
To enable handling difÔ¨Åculty-based sampling, we need to
make two adjustments. We need a data-structure which efÔ¨Å-
ciently and securely enables the veriÔ¨Åer to sample blocks based
on their relative difÔ¨Åculty positions, rather than their absolute
positions as in the standard MMR. Second, Assumption 2,
which states that the adversary‚Äôs forks have only a fraction of
the honest chain‚Äôs weight, requires that all difÔ¨Åculty transitions
are correct. In fact, as described in Section III-A, the assump-
tion is broken if the adversary can arbitrarily manipulate block
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:03:03 UTC from IEEE Xplore.  Restrictions apply. 
938
difÔ¨Åculties. We show how an adapted variable-difÔ¨Åculty MMR
which aggregates information about the total difÔ¨Åculty as well
as difÔ¨Åculty transitions can resolve both issues at once.
DeÔ¨Ånition 7 (DifÔ¨Åculty MMR). A difÔ¨Åculty MMR is a variant
of the MMR with identical properties, but such that each
node contains additional data. Every node can be written as
h, w, t, Dstart, Dnext, n, data, where h is a Œª bit output of a
hash function, w is an integer representing a total difÔ¨Åculty,
t is a timestamp, D is a difÔ¨Åculty target, n is an integer
representing the size of the subtree rooted by the node and
data is an arbitrary string. In a block, i.e., a leaf node, n = 1
and t is the time it took to mine the current block (the blocks
time stamp minus the previous block‚Äôs timestamp). w, Dstart is
the current difÔ¨Åculty targets and Dnext is the difÔ¨Åculty of the
next block computed using the difÔ¨Åculty adjustment function
deÔ¨Åned DeÔ¨Ånition 1.
Each non-leaf node is deÔ¨Åned as {H(lc, rc), lc.w +
rc.w, lc.t+rc.t, lc.Dstart, rc.Dnext, lc.n+rc.n,‚ä•}, where lc =
LeftChild and rc = RightChild.
DifÔ¨Åculty MMR VeriÔ¨Åcation. We need to modify the MMR
veriÔ¨Åcation algorithm in several ways. The prover algorithm
will be the same generating a proof consisting of a Merkle
path. In general the veriÔ¨Åer will check the Merkle path and that
the targets assigned to each node are feasible. For simplicity,
we assume that the epoch length m and the total number of
leafs n are powers of 2. Given a left child (lc), a right child
(rc) and a parent node (p), the veriÔ¨Åer performs the following
checks:
1) Compute p using lc and rc following DeÔ¨Ånition 7.
2) Verify that lc.Dnext = rc.Dstart.
3) For both lc and rc verify that they are internally con-
sistent. That is, ensure that there is a possible set of
legal difÔ¨Åculty transitions given the aggregate timing and
difÔ¨Åculty parameters of these nodes:
‚Ä¢ If the node is within an epoch,
i.e., below level
the difÔ¨Åculty and weight are
log2(m), ensure that
consistent with the epoch‚Äôs difÔ¨Åculty.
‚Ä¢ If the node captures a difÔ¨Åculty transition, ensure
that Dnext is computed correctly using the difÔ¨Åculty
transition function from DeÔ¨Ånition 1 and t.
‚Ä¢ tstart, tend, w, Dstart, Dnext: there is a possible assign-
ment to the difÔ¨Åculty transitions yielding these param-
eters. See discussion below for details.
The checks require the veriÔ¨Åer to know whether there is a pos-
sible assignment to the difÔ¨Åculty transition yielding a certain
set of parameters. While intricate, this can be done efÔ¨Åciently.
If the node is below a node that deÔ¨Ånes an epoch, i.e., the node
is at a height lower than log2(m) then its difÔ¨Åculty target and
weight w, Dstart are fully deÔ¨Åned by the epoch. For nodes
higher in the MMR we can compute what the max and the
min total weight w are given the other parameters. The max
weight over a given set of difÔ¨Åculty transitions is achieved
by Ô¨Årst raising the difÔ¨Åculty by the dampening factor œÑ and
then lowering it over a set of epochs by 1
œÑ such that the next
to accommodate for all
difÔ¨Åculty target is Dnext. The inverse, i.e., Ô¨Årst lowering then
raising, achieves the minimum total weight. The timestamps
need to be far enough apart
the
epochs in which the difÔ¨Åculty decreases. A maximal difÔ¨Åculty
decrease requires an epoch length of at least œÑ¬∑ m
1. Conversely,
an epoch in which the difÔ¨Åculty increases maximally lasts at
most m
f¬∑œÑ .
Overall, the following checks are sufÔ¨Åcient for the simpliÔ¨Åed
scenario, where œÑ kDstart = Dnext for an integer k ‚â• 0 and a
total of n epochs such that n ‚àí k is even:
1) k ‚â§ n
2 ‚àí1
f
œÑ i)
2) w ‚â§ Dstart((cid:80)k+ n‚àík
3) w ‚â• Dstart((cid:80)0
= Dstart ¬∑ (œÑ +1)œÑ