(cid:82) 1
approaches 1 and (cid:82) 1
We now analyze how close our f (x) is to the optimal
sampling distribution. We ï¬rst try to compute the normal-
ized probability density function by normalizing f (x) by an
0 f (x)dx factor. Unfortunately, f (x) goes to inï¬nity as x
0 f (x)dx = âˆ. Luckily, we can restrict
the sampling domain from 0 to 1 âˆ’ Î´ and have the veriï¬er
always check the last Î´ fraction of the blocks directly. We
(cid:82) 1+acâˆ’c
Fig. 1. s(x) deï¬nes the probability density function (PDF) for the protocol
from Section V-B. g(x) =
(xâˆ’1) ln(Î´) is the optimized PDF. The integral
g(x)dx for c = 1/2, Î´ = 2âˆ’10, a = 0 and a = 0.8 respectively
1
a
is displayed.
As a ï¬rst step, we show that the probability density func-
tion (PDF) of the optimal sampling distribution must be
increasing. A PDF f deï¬ned over the continuous range [0, 1]
is increasing if, for all a, b âˆˆ [0, 1], b > a =â‡’ f (b) â‰¥ f (a).
For any distribution deï¬ned by a PDF that is not increasing,
there exists a distribution that results in an equal or greater
probability of catching the adversary.
Lemma 3 (Non-Increasing Sampling Distribution). A sam-
pling distribution over the blocks deï¬ned by a non-increasing
PDF f is not uniquely optimal,
there exists another
distribution with equal or higher probability of catching the
i.e.,
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:03:03 UTC from IEEE Xplore.  Restrictions apply. 
937
0.00.20.40.60.81.0x0.51.01.52.02.53.0Queryingdistributionf(x)s(x)01ğ‘1âˆ’1âˆ’ğ‘ğ‘Valid blocksInvalid blocksValid blocksMalicious ChainHonest ChainSample enough random blocks from here s.t.at least one invalid block is selected GenesisBlockFork pointwill later ï¬nd the optimal value for Î´. Let
g(x) =
f (x)
(cid:82) 1âˆ’Î´
(cid:82) 1+acâˆ’c
0
1
=
(x âˆ’ 1) ln(Î´)
.
f (x)dx
g(x)dx = (cid:82) 1âˆ’c
to
The probability of catching the adversary is equal
p = min0â‰¤aâ‰¤ câˆ’Î´
(xâˆ’1) ln(Î´) dx =
logÎ´(c). This probability takes into account that all blocks
in the last Î´ fraction of the chain are always veriï¬ed by the
veriï¬er. Any fork after câˆ’Î´
c will contain at least a block from
this Î´ region, and thus will be detected with probability 1.
a
0
1
c
1
We will now show that g(x) deï¬nes an optimal sampling
distribution by showing that no sampling distribution can
achieve a higher p value, i.e., a higher probability of catching
the adversary with a single query. Note that the sampling
strategy is optimal for an optimal adversary. This is a strong
argument as the optimal adversary can choose the placement
of its invalid blocks after learning the sampling strategy.
Theorem 2 (Optimal Sampling Distribution). Given that the
last Î´ = ck, c âˆˆ (0, 1], k âˆˆ N fraction of the chain contains
only valid blocks and the adversary can at most create a c
fraction of valid blocks after the fork point a, the sampling
(xâˆ’1) ln(Î´) maximizes
distribution deï¬ned by the PDF g(x) =
the probability of catching any adversary that optimizes the
placement of invalid blocks.
Proof. Let Î´ = ck, for some k âˆˆ N, we get that p = 1/k
and that as k increases the success probability decreases.
Hence, the smaller Î´ is set, the fewer the blocks that are
always checked near the tip of the chain but the worse our
probability of catching the adversary with a sample anywhere
else. Therefore, a smaller Î´ leads to more samples from the
rest of the chain.
Say gâˆ—(x) is the probability density function of
the
best
sampling distribution. Note that given Lemma 3,
gâˆ—(x)
increasing and therefore for an optimal ad-
versary the success probability is denoted by pâˆ— =
mina,0â‰¤aâ‰¤ câˆ’d
gâˆ—(x)dx.
(cid:82) 1+acâˆ’c
implies that(cid:82) 1âˆ’ci+1
have that (cid:82) 1âˆ’ck
have (cid:82) 1âˆ’ck
gâˆ—(x), therefore, maximizes pâˆ—. The optimality condition
gâˆ—(x)dx â‰¥ pâˆ—, for all integers i âˆˆ [0, k],
where a = 1 âˆ’ ci is a possible forking point. Further we
gâˆ—(x)dx = 1 since gâˆ—(x) is a PDF. We
gâˆ—(x)dx = 1 â‰¥ k Â· pâˆ—.
This implies that pâˆ— â‰¤ 1/k. Note that g(x) as a candidate
distribution achieves p = 1/k and is, therefore, optimal.
gâˆ—(x)dx =(cid:80)k
(cid:82) 1âˆ’ci+1
1âˆ’ci
1âˆ’ci
i=0
is
a
0
0
c
Optimizing the Proof Size. Given g(x) and p, we can now
deï¬ne pm = (1 âˆ’ 1
k )m as the probability of failure, i.e., not
catching the optimal adversary after m independent queries.
Note that without loss of generality, k â‰¥ 1 as otherwise Î´ >
c, implying that a sufï¬cient fraction of blocks are checked
to catch any adversary. If we want pm â‰¤ 2âˆ’Î», then m â‰¥
. Now, assuming that the veriï¬er always checks
log1/2(1âˆ’ 1
k )
Î»
2 ) ln(n) =
the last L blocks of the chain, where L = Î´n = ckn, we get
that k = logc
(cid:0) L
n
(cid:1) and m â‰¥
(cid:18)
(cid:19).
that m approximates Î» logc( 1
2 ) ln(n) = 1.
Î»
1âˆ’ 1
m
Î» logc( 1
logc ( L
n
log1/2
)
This means
O(Î» log1/c(n)), thus, limnâ†’âˆ
As long as L is a constant, the number of queries is linear in
the security parameter Î» and logarithmic in the chain length,
n.
Verifying Trailing Blocks. The number of blocks checked at
the end of the chain (denoted by L) affects the total number
of samples needed, m. We can, therefore, further optimize it
to get an optimal proof size. It is important to ensure that
L is bounded from below by the particular (c, L)-adversary
assumption that
is used. Given this one can numerically
optimize L as we do in our implementation. However, as long
as that number is a constant we get an asymptotically optimal
proof size:
Corollary 2. Under the (c, L)-adversary assumption for any
constant L and using a collision-resistant hash function the
FlyClient proof size is Î˜(Î» log(n) log 1
(n) + L)
c
The corollary is a result of the proof size computations
above for a negligible failure probability of 2âˆ’Î» and the size
of each Merkle path being log2(n).
Note that unlike in the superblock-based NIPoPoW [14] this
result holds for all adversaries not just in an optimistic setting.
For realistic Ethereum values of Î» = 50, n = 222, c = 1
2 bytes,
the total proof size is just below 400 KB (See Section VII).
VI. FLYCLIENT UNDER VARIABLE DIFFICULTY
So far, we have only considered the simplistic case that
all blocks have the same difï¬culty. This is not realistic as
the number of miners as well as their hardware continuously
changes.
1
Information theoretically, the distributional view analysis
described in Section V-C allows us to also handle the variable-
difï¬culty scenario. In the new model, we simply use the same
sampling distribution g(x) =
(xâˆ’1) ln Î´ , but now x denotes
the relative aggregate difï¬culty weight and Î´ denotes the
relative difï¬culty weight of the blocks which are sampled with
probability 1. For example, x = 1/2 is the point in the chain
where half of the difï¬culty has been amassed, and g(1/2)
is the probability that the block at that point is sampled by
FlyClient. Note that x = 1/2 may refer to a very recent block
in the chain if the block difï¬culty grows fast.
A. Variable Difï¬culty MMR
To enable handling difï¬culty-based sampling, we need to
make two adjustments. We need a data-structure which efï¬-
ciently and securely enables the veriï¬er to sample blocks based
on their relative difï¬culty positions, rather than their absolute
positions as in the standard MMR. Second, Assumption 2,
which states that the adversaryâ€™s forks have only a fraction of
the honest chainâ€™s weight, requires that all difï¬culty transitions
are correct. In fact, as described in Section III-A, the assump-
tion is broken if the adversary can arbitrarily manipulate block
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:03:03 UTC from IEEE Xplore.  Restrictions apply. 
938
difï¬culties. We show how an adapted variable-difï¬culty MMR
which aggregates information about the total difï¬culty as well
as difï¬culty transitions can resolve both issues at once.
Deï¬nition 7 (Difï¬culty MMR). A difï¬culty MMR is a variant
of the MMR with identical properties, but such that each
node contains additional data. Every node can be written as
h, w, t, Dstart, Dnext, n, data, where h is a Î» bit output of a
hash function, w is an integer representing a total difï¬culty,
t is a timestamp, D is a difï¬culty target, n is an integer
representing the size of the subtree rooted by the node and
data is an arbitrary string. In a block, i.e., a leaf node, n = 1
and t is the time it took to mine the current block (the blocks
time stamp minus the previous blockâ€™s timestamp). w, Dstart is
the current difï¬culty targets and Dnext is the difï¬culty of the
next block computed using the difï¬culty adjustment function
deï¬ned Deï¬nition 1.
Each non-leaf node is deï¬ned as {H(lc, rc), lc.w +
rc.w, lc.t+rc.t, lc.Dstart, rc.Dnext, lc.n+rc.n,âŠ¥}, where lc =
LeftChild and rc = RightChild.
Difï¬culty MMR Veriï¬cation. We need to modify the MMR
veriï¬cation algorithm in several ways. The prover algorithm
will be the same generating a proof consisting of a Merkle
path. In general the veriï¬er will check the Merkle path and that
the targets assigned to each node are feasible. For simplicity,
we assume that the epoch length m and the total number of
leafs n are powers of 2. Given a left child (lc), a right child
(rc) and a parent node (p), the veriï¬er performs the following
checks:
1) Compute p using lc and rc following Deï¬nition 7.
2) Verify that lc.Dnext = rc.Dstart.
3) For both lc and rc verify that they are internally con-
sistent. That is, ensure that there is a possible set of
legal difï¬culty transitions given the aggregate timing and
difï¬culty parameters of these nodes:
â€¢ If the node is within an epoch,
i.e., below level
the difï¬culty and weight are
log2(m), ensure that
consistent with the epochâ€™s difï¬culty.
â€¢ If the node captures a difï¬culty transition, ensure
that Dnext is computed correctly using the difï¬culty
transition function from Deï¬nition 1 and t.
â€¢ tstart, tend, w, Dstart, Dnext: there is a possible assign-
ment to the difï¬culty transitions yielding these param-
eters. See discussion below for details.
The checks require the veriï¬er to know whether there is a pos-
sible assignment to the difï¬culty transition yielding a certain
set of parameters. While intricate, this can be done efï¬ciently.
If the node is below a node that deï¬nes an epoch, i.e., the node
is at a height lower than log2(m) then its difï¬culty target and
weight w, Dstart are fully deï¬ned by the epoch. For nodes
higher in the MMR we can compute what the max and the
min total weight w are given the other parameters. The max
weight over a given set of difï¬culty transitions is achieved
by ï¬rst raising the difï¬culty by the dampening factor Ï„ and
then lowering it over a set of epochs by 1
Ï„ such that the next
to accommodate for all
difï¬culty target is Dnext. The inverse, i.e., ï¬rst lowering then
raising, achieves the minimum total weight. The timestamps
need to be far enough apart
the
epochs in which the difï¬culty decreases. A maximal difï¬culty
decrease requires an epoch length of at least Ï„Â· m
1. Conversely,
an epoch in which the difï¬culty increases maximally lasts at
most m
fÂ·Ï„ .
Overall, the following checks are sufï¬cient for the simpliï¬ed
scenario, where Ï„ kDstart = Dnext for an integer k â‰¥ 0 and a
total of n epochs such that n âˆ’ k is even:
1) k â‰¤ n
2 âˆ’1
f
Ï„ i)
2) w â‰¤ Dstart((cid:80)k+ nâˆ’k
3) w â‰¥ Dstart((cid:80)0
= Dstart Â· (Ï„ +1)Ï„