### Index Generation Performance

The most resource-intensive phase of any searchable encryption scheme is the index generation. Our performance measurements for this phase include the encryption of documents and all other associated operations, excluding the cost of generating the plaintext index. The generation of a plaintext index is orthogonal to our contributions and does not reflect the performance of our system; it is also ignored in all prior work.

Figure 10 illustrates our index generation performance on the email dataset. Our scheme outperforms [18], which requires 52 seconds to process 16MB of data. In contrast, our scheme can process 256MB (16 times more data) in approximately 35 seconds. [18] extrapolates their processing time to 16GB of text emails without attachments, estimating that their index generation would take 15 hours. In our scheme, the same task would take only 41 minutes.

This aligns with our micro-benchmark evaluation, which shows that our index generation operation is at least an order of magnitude faster than that of [18].

**Figure 10: SSE.indexgen performance on email dataset with 99% confidence intervals. SKE stands for Symmetric Key Encryption and represents the time required to encrypt the documents. All SKE costs are non-zero but some are very small.**

**Figure 9: File/Keyword pair versus amortized time for SSE.indexgen. Time per file/keyword pair tends to 1.58µs, much better than the 35µs reported in [18] in a similar dataset.**

### Full Evaluation

Each data point for Index Generation is the average of 5 runs of SSE.indexgen. Each data point for Search is the average of 5 runs using the most frequent English word "the". Each data point for addition is the average of at least 5 runs.

#### Parameters Used

The parameters used in our experiments ensure that \( p_{err} \leq 2^{-80} \) (where \( p_{err} \) is the probability of the scheme aborting and measures the security “error”) if less than 1/8 of the total blocks in D are filled, and \( p_{err} \leq 2^{-40} \) if less than 1/4 of the total blocks in D are filled. We set \( \kappa = 80 \) and \( \alpha = 4 \), with a block size of D to 256 bytes and the total number of blocks in D to \( n_D = 2^{24} \).

**Figure 11: SSE.indexgen on the document dataset with 99% confidence intervals.**

### Communication Costs

The communication cost of the initial index upload depends on the parameters used for Blind-Storage, particularly the size of the array D. In our experiments, the size of D was set to 1GB (2^24 blocks of 256 bytes each). For the 256MB subset of the email dataset, the actual amount of index data consisted of 20,694,991 file-keyword pairs, translating to about 78MB of data. Formatting this data into 256-byte blocks for the Blind-Storage scheme resulted in about 178MB of data. For our choice of \( \kappa \) and \( \alpha \), \( \gamma = 4 \) is sufficient to bring \( p_{err} \) below \( 2^{-40} \), meaning that 712MB would be sufficient as the size of D. Thus, the choice of 1GB as the size of D in our experiments leaves ample room for adding more documents later.

For the document dataset, there are only 1,371,656 file-keyword pairs, translating to a plaintext index size of 5MB (with 4-byte document IDs). Therefore, the size of D could be as low as 20MB. Note that the document collection itself is 1GB in this case. For rich data formats, the communication overhead due to SSE.indexgen would typically be a fraction of the communication requirement for the documents themselves.

### Search Performance

Figure 12 shows the search performance of our scheme, excluding the final decryption of the documents. It includes the overhead incurred at search time to handle lazy deletion. We searched for the most frequent English word "the," which was present in almost all the documents. Our scheme performed better than [18] for all data sizes. Their scheme needs 17 ms, 34 ms, and 53 ms for 4MB, 11MB, and 16MB subsets of the Enron dataset, respectively. Our scheme consumed 5 ms, 11 ms, and 25 ms for 4MB, 8MB, and 16MB subsets of the Enron dataset, respectively. The search time grows proportionately to the size of the response.

**Figure 12: Search performance on the email dataset with 99% confidence intervals.**

**Figure 13: Search performance on the document dataset with 99% confidence intervals.**

**Figure 14: Communication needed for searching on the email dataset. The graph shows the size of the retrieved documents alongside the extra communication incurred by our scheme.**

As experimentally confirmed, the overhead for searches does not significantly vary depending on whether the search operation involved a lazy deletion or not. This is because all search operations use the update mechanism of the underlying Blind-Storage scheme and the clear storage scheme. The efficiency of the update mechanism itself does not depend significantly on whether the file was modified or not. Indeed, in the case of Blind-Storage updates, it is important for the security that it must not be revealed to the server if a lazy deletion was involved or not.

### Add Operation

Unlike [18] and other prior work, the performance of our add operation does not depend on the amount of data (i.e., the number of file-keyword pairs) already present in the searchable encryption system. Figure 15 shows the performance of adding files of specified size when 256MB of data was initially indexed into the system.

**Figure 15: Add performance on email dataset with 99% confidence intervals. SKE costs are non-zero but very small.**

Our scheme uses a lazy deletion strategy to handle removals, which allows us to obtain vastly improved security guarantees by limiting the information leaked to the server. One might ask if this leads to any efficiency degradation during subsequent searches, since the actual updates to the index take place when a keyword that was contained in a deleted document is searched for later.

**Figure 16: Add performance on document dataset with 99% confidence intervals. SKE costs are non-zero but very small.**

### Remove Operation

The communication and computation cost of removing a document is virtually negligible, as it uses a lazy deletion strategy. Removal of a document in our scheme only requires the client to send a command to the server to delete the document from its file-system, and does not need any update to the searchable encryption index.

### Summary

Evaluation of our scheme shows that it is more efficient, scalable, and practical than prior schemes. Index generation in our scheme is more than 20 times faster than that of [18]. Search operations are 2-3 times faster in our experiments. Unlike [18], our addition and removal times are independent of the total number of file-keyword pairs, making it much more scalable. Removal in our scheme has virtually zero cost. We stress that several possible optimizations have not been implemented in this prototype.

### Conclusion

In this work, we introduced a new cryptographic construct called Blind Storage and implemented it using a novel, yet lightweight protocol, SCATTERSTORE. We also demonstrated how a dynamic SSE scheme can be constructed using Blind Storage in a relatively simple manner. The resulting scheme is more computationally efficient, requires simpler infrastructure, and is more secure than existing schemes.

Important future directions include making the scheme secure against actively corrupt servers and allowing secure searches involving multiple keywords. The core idea of using pseudorandom subsets in SCATTERSTORE is amenable to these extensions, as is being explored in ongoing work.

### Acknowledgment

We thank Igors Svecs for collaboration in the early stages of the project. We are grateful to Seny Kamara for helping us with evaluation datasets and Elaine Shi for useful discussions. We also thank Ravinder Shankesi for help in debugging the code and Fahad Ullah for help in collecting documents for the document dataset.

This work was supported by NSF 07-47027, NSF 12-28856, NSF CNS 13-30491 (ThaW), HHS 90TR0003-01, and NSF CNS 09-64392 (EBAM). The views expressed are those of the authors only.

### References

[References listed here, formatted as provided in the original text.]