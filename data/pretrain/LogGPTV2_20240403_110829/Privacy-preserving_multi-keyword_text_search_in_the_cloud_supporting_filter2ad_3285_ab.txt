GenQuery
GenQuery
...
Search 
request
...
...
GenQuery
Q1
Q2
...
Qh
Q
Query
Documents
Index vectors
Secure index tree
Query vector
Ranked top k 
search result
Figure 3: Overview of secure index scheme
the calculation methods for TF weight and IDF weight are
equally eﬀective compared to others [35]. The following sta-
tistical values are used in our similarity evaluation function:
• fd,t, the TF of the keyword t within the document d;
• ft, the number of documents containing the keyword
t;• N , the total number of documents in the document
• wd,t, the TF weight for fd,t;
• wq,t, the IDF weight (query weight);
• Wd, the Euclidean length of wd,t;
• Wq, the Euclidean length of wq,t.
set;
The deﬁnition of the similarity function is as follows:
wd,t · wq,t
(1)
Cos(Dd, Q) =
1
WdWq
(cid:5)(cid:6)
(cid:4)
(cid:5)(cid:6)
t∈Q∩Dd
t∈Q∩Dd
w2
q,t, wd,t =
t∈Q∩Dd
d,t, Wq =
ft ), and hence, the index vector
w2
where Wd =
1 + ln(fd,t), wq,t = ln(1 + N
Dd and query vector Q are both unit vectors.
MD-algorithm: The MD-algorithm [19] is used to ﬁnd
the k-best matches in a database that is structured as an
MDB-tree [22], as shown in Fig. 2. In the database scenario,
each level of the MDB-tree represents an attribute domain
and each attribute in that domain is assigned an attribute
value. All the attributes sharing the same value in the upper
domain forms a child node. As such, a set of objects is
allowed to be indexed in one data structure. An important
search parameter, the prediction threshold value ˆPi for each
level i, is obtained from the maximum attribute value Pi
at each level, for example, in Fig. 2, ˆPi = Pi = 1.0.
In
a depth-ﬁrst manner, MD-algorithm starts from the root
node with a recursive procedure upon this tree. Speciﬁcally,
search process selects the unused maximum attribute value
when it enters a node, and based on ˆPi’s below this level,
predicts the maximum possible ﬁnal score to be obtained.
The criteria for node selection is that if this predicted ﬁnal
score is less than or equal to the minimum score of the top-k
objects which have been selected, search process returns to
the parent node, otherwise, it goes down to the child node at
the next level. This procedure is executed recursively until
the objects with top-k scores are selected. The search can
be done very eﬃciently due to the relatively accurate ﬁnal
score prediction, and thus only part of the objects in the tree
are accessed. Fig. 2 shows an example that, when k = 3, the
set of objects, E, K, and J, are returned to the user and the
cross signs in the ﬁgure indicate that it is not necessary to
access the nodes below. More details of the MD-algorithm
and MDB-tree can be found in [19].
3. SECURE INDEX SCHEME
To achieve accurate multi-keyword ranked search, we adop-
t the cosine measure to evaluate similarity scores. In par-
ticular, we divide the original long document index vector
Dd into multiple sub-vectors such that each sub-vector Dd,i
represents a subset of keywords Ti of T , and becomes a
part of the ith level of the index tree I, as shown in Fig. 3.
The query vector Q is divided in the same way Dd is done.
Let Qi be the query sub-vector at the ith level. As such,
the ﬁnal similarity score for document d can be obtained
by summing up the scores from each level. Based on these
similarity scores, the cloud server determines the relevance
of document d to the query Q and sends the top-k most
relevant documents back to the user. By using the level-
wise secure inner product scheme, similar to the techniques
applied in [5, 31], the document index vector Dd,i and the
query vector Qi are both well protected, and we show that
this basic scheme is secure in the known ciphertext model.
To further protect the sensitive frequency information from
leakage, we also propose an enhanced scheme in the known
background model.
3.1 BMTS in Known Ciphertext Model
In order to facilitate the relevance rankings, the similarity
scores, i.e., cosine values, are revealed to the cloud server,
which diﬀers from the schemes adopted in [5, 31]. In other
words, we do not apply the dimension extension technique
to our basic scheme in the known ciphertext model. For each
level i of I, our basic secure index scheme can be described
as follows:
• Setup In this initialization phase, the secret key SKi
is produced by the data owner, including: 1) a |Ti|-bit
randomly generated vector Si, where |Ti| is the length
of Ti; 2) two (|Ti| × |Ti|) invertible random matrices
{M1,i, M2,i}. Hence, SKi can be denoted as a 3-tuple
{Si, M1,i, M2,i}.
• GenIndex (DC, SKi) For each document d, the data
owner generates an index vector Dd,i according to Ti,
and each dimension is a normalized TF weight wd,t.
74(cid:4)
(cid:4)(cid:4)
(cid:4)
2,iDd,i
1,iDd,i
(cid:4), M T
[j] and Dd,i
[j] and Dd,i
Next, the splitting procedure is applied to Dd,i, which
splits Dd,i into two random vectors as {Dd,i
(cid:4)(cid:4)}.
(cid:4), Dd,i
Speciﬁcally, with the |Ti|-bit vector Si as a splitting
(cid:4)(cid:4)
indicator, if the jth bit of Si is 0, Dd,i
[j]
are set as the same as Dd,i[j]; if the jth bit of Si is 1,
Dd,i
[j] are set to two random numbers so
ed index vector (cid:7)Dd,i is built as {M T
that their sum is equal to Dd,i[j]. Finally, the encrypt-
(cid:4)(cid:4)}.
• GenQuery( ¯T , SKi) With the keywords of interest in
¯T , the query vector Qi is generated, where each di-
mension is a normalized IDF weight wq,t (wq,t = 0 for
any keyword t not present in Qi). Subsequently, Qi is
split into two random vectors as {Qi
(cid:4)(cid:4)} with the
similar splitting procedure. The diﬀerence is that if
(cid:4)(cid:4)
the jth bit of Si is 0, Qi
[j] are set to two
random numbers so that their sum is equal to Qi[j]; if
(cid:4)(cid:4)
same as Qi[j]. Finally, the encrypted query vector (cid:3)Qi
the jth bit of Si is 1, Qi
[j] are set as the
is yielded as {M−1
• SimEvaluation ((cid:7)Dd,i,(cid:3)Qi) The cloud server executes
similarity evaluation with query vector (cid:3)Qi as in Eq. 2.
[j] and Qi
(cid:4)(cid:4)}.
[j] and Qi
(cid:4)
(cid:4), M−1
2,i Qi
(cid:4), Qi
1,i Qi
(cid:4)
The similarity score at the ith level is computed as follows:
Cos((cid:7)Dd,i,(cid:3)Qi)
={M T
(cid:4), M T
1,iDd,i
(cid:4) · Qi
(cid:4)
=Dd,i
+ Dd,i
=Dd,i · Qi.
2,iDd,i
(cid:4)(cid:4)} · {M−1
1,i Qi
(cid:4), M−1
2,i Qi
(cid:4)(cid:4)}
(2)
(cid:4)(cid:4) · Qi
(cid:4)(cid:4)
(cid:6)h
i=1 Dd,i·
Hence, the ﬁnal similarity score for document d is
Qi = Dd · Q.
Security Analysis We analyze BMTS concerning the search
privacy requirements as described in section 2.
1) Index conﬁdentiality and Query conﬁdentiality:
In BMTS, (cid:7)Dd,i and (cid:3)Qi are obfuscated vectors. As long as the
secret key SKi is kept conﬁdential, the cloud server cannot
infer the original vectors Dd,i or Qi. Neither can it deduce
the keywords nor the TF and IDF information included in
the documents or queries from the result similarity scores,
which appear to be random values to the server. This has
been proven in the known ciphertext model in [31]. There-
fore, index conﬁdentiality and query conﬁdentiality are well
protected.
2) Query unlinkability: The adopted vector encryption
method provides non-deterministic encryption, in light of
the random vector splitting procedure. Thus the same search
request (e.g. same search keywords) will be encrypted to dif-
ferent query vector (cid:2)Q. The non-linkability of search requests
can be provided to this extent. However, if a cloud server is
capable of tracking the nodes visited and the intermediate
similarity results, it is possible for the cloud server to link
the same search request based on the same similarity scores.
In this case the search pattern or the access pattern will be
leaked even in the known ciphertext model.
3) Keyword privacy: In the known background model,
the cloud server may have the knowledge of not only the TF
distributions, but also the normalized TF distributions of
some sensitive keywords from a known comparable dataset.
s
t
n
e
m
u
c
o
d
f
o
r
e
b
m
u
N
140
120
100
80
60
40
20
0
s
t
n
e
m
u
c
o
d
f
o
r
e
b
m
u
N
500
400
300
200
100
0
0
0.02
0.04
0.06
Similarity Score
0.08
0.1
0
0.02
Similarity Score
0.04
0.06
0.08
(a)
(b)
Figure 4: Distribution of similarity score when a
single keyword in a query vector with our basic
scheme. (a) For keyword “network”. (b) For key-
word “search”.
It is worth noting that these distributions are keyword spe-
ciﬁc respectively, as shown in Fig. 4, such that the corre-
sponding keywords can be diﬀerentiated by the slope and
value range of these distributions [28, 29, 34]. In the worst
case, where only one keyword t appears in the query vector
Q (the normalized wq,t is 1), the normalized TF distribution
of this keyword is exposed directly.
In order to enhance security and boost search eﬃcien-
cy, search evaluation may be only executed at certain levels
where the user-intended keywords reside; for the other levels,
we can render these similarity scores some ﬁxed values, e.g.,
0, during the execution of both similarity score prediction
and evaluation in the search process.
3.2 EMTS in Known Background Model
The previous security analysis shows that in the known
background model, keyword privacy breach is possible, due
to the distance-preserving property of BMTS, i.e., the co-
sine value calculated from (cid:7)Dd,i and (cid:3)Qi is equal to the one
from Dd,i and Qi. In order to break such equality, we in-
troduce some tunable randomness into the similarity score
evaluation, by which the cloud server cannot diﬀerentiate
keywords from the particular similarity score distributions.
In addition, this randomness can be calibrated by the us-
er to represent the user’s preference for the more accurate
ranked search result versus better-protected keyword priva-
cy. Speciﬁcally, Ui phantom terms are added into the query
vector Qi, and we extend the index vector Dd,i from |Ti|
dimensions to |Ti| + Ui dimensions. We denote the subset of
h levels where the keywords of interest reside as w and its
size |w| ≤ h.
Our EMTS scheme is performed almost the same as BMT-
S, except that at the ith level: 1) in Setup phase, Si becomes
(|Ti|+Ui)-bit long. M1,i and M2,i are (|Ti|+Ui)×(|Ti| + Ui)
dimensional matrices; 2) in GenIndex phase, by choosing Vi
out of Ui phantom terms, the corresponding entries in the
(|Ti|+Ui)-dimensional index vector Dd,i are set to 1; 3) when
generating encrypted query, the (|Ti|+j)th entry in Qi where
j ∈ [1, Ui] is set to a random number εi,j ; 4) The cloud server
executes similarity evaluation and obtains the ﬁnal similari-
ty score for document d equal to (Dd · Q +
εi,j),
where ¯Vi is the set of the Vi selected phantom terms, and it
is diﬀerent for each index vector at level i.
(cid:6)
(cid:6)
j∈ ¯Vi
i∈w
75100
80
60
40
20
s
t
n
e
m
u
c
o
d
f
o
r
e
b
m
u
N
100
80
60
40
20
s
t
n
e
m
u
c
o
d