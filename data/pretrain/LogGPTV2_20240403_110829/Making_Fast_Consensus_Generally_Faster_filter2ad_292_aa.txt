title:Making Fast Consensus Generally Faster
author:Sebastiano Peluso and
Alexandru Turcu and
Roberto Palmieri and
Giuliano Losa and
Binoy Ravindran
2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Making Fast Consensus Generally Faster
Sebastiano Peluso
ECE, Virginia Tech
PI:EMAIL
Alexandru Turcu
ECE, Virginia Tech
PI:EMAIL
Roberto Palmieri
ECE, Virginia Tech
PI:EMAIL
Giuliano Losa
ECE, Virginia Tech
PI:EMAIL
Binoy Ravindran
ECE, Virginia Tech
PI:EMAIL
Abstract—New multi-leader consensus protocols leverage the
Generalized Consensus speciﬁcation to enable low latency,
even load balancing, and high parallelism. However, these
protocols introduce inherent costs with signiﬁcant performance
impact: they need quorums bigger than the minimum required
to solve consensus and need to track dependency relations
among proposals. In this paper we present M 2P AXOS, an
implementation of Generalized Consensus that provides fast
decisions (i.e., delivery of a command in two communication
delays) by leveraging quorums composed of a majority of
nodes and by exploiting workload locality. M 2P AXOS does
not establish command dependencies based on conﬂicts, in-
stead mapping nodes to accessed objects and enforcing that
commands accessing the same objects be ordered by the same
node. Our experimental evaluation conﬁrms the effectiveness of
M 2P AXOS, gaining up to 7× over state-of-the-art Consensus
and Generalized Consensus algorithms under partitioned data
accesses and up to 5.5× using the TPC-C workload.
Keywords-generalized consensus; state machine replication;
I. INTRODUCTION
Paxos [1] is an algorithm for solving the consensus prob-
lem [2] in an asynchronous network, even in the presence
of crashes, and is often used to build strongly consistent
and fault-tolerant distributed services ([3], [4], [5], [6]),
such as Google’s Spanner [3]. Despite its widespread use,
Paxos suffers from performance bottlenecks when deployed
on networks with large amounts of nodes. For example, in its
widely adopted and more practical deployment, i.e., Multi-
Paxos [7], there is a designated leader, which is responsible
for ordering proposed commands and allows the implemen-
tation of consensus in as few as three communication delays
for crash-free executions. However, in practice, that leader
constitutes a bottleneck that inﬂuences the performance of
the whole system.
Several recent algorithms ([8], [9], [10]) eliminate the role
of the unique leader by allowing multiple nodes to operate
as leaders at the same time. This, on the one hand, gives the
opportunity to balance the load and avoids a single point
of decision (i.e., a designated leader); on the other hand, it
introduces the potentially high cost of handling contention
among the various leaders issuing proposals concurrently.
To reduce the chances of contention among leaders, which
can also have the beneﬁt of increasing the chances of fast
decisions in just two communication delays [8], a common
approach adopted by multi-leader algorithms is to relax
the consistency requirement of Consensus. Contrary to the
Consensus speciﬁcation, which demands that at most one
proposal can be decided in an instantiation, those algorithms
allow multiple proposals to be decided at the same time
as long as they are not dependent, i.e., their executions
commute according to the application semantics. Indeed,
they implement a more general variant of Consensus, called
Generalized Consensus [11], [12], which has been proven
sufﬁcient for providing strong consistency in replicated
services, since the outcome of the execution of a sequence
of commutable commands on different nodes is independent
of the order they are executed [11].
(cid:2)
(cid:3)
(cid:3)
N
2
However, the described advantages of Generalized Con-
sensus implementations come at the cost of requiring i)
synchronous communication among a larger set of nodes,
ii) additional computation for discriminating whether pro-
posals are dependent, i.e., conﬂicting, or not, and iii) bigger
messages in order to include information about dependen-
cies among proposals. Indeed, these algorithms reduce the
number of communication delays required to take a decision
from three to two in the absence of conﬂicts by requiring
a leader to communicate with a fast quorum of at least
(cid:2)
2·N
+1 nodes [13], where N is the total number of nodes,
3
whereas Multi-Paxos only enforces a communication among
+ 1 nodes. More-
a smaller classic quorum of at least
over, these algorithms must compute dependency relations
among commands, a potentially costly operation, and must
exchange them among nodes, increasing bandwidth usage.
Existing theoretical results on the cost of implementing
consensus [14] prove one cannot achieve an optimal tradeoff
that combines both the adoption of classical quorums and
decisions in two communication delays for all possible
executions; it is also not known whether the costs associated
with having dependency relations can be avoided. In this
paper, we circumvent these restrictions by investigating the
feasibility of having minimal quorum size, low delay, and no
dependency relations under common application workloads.
In other words, we aim to answer the following question:
can we guarantee generally faster performance at the cost
of having a slightly more expensive decision process only
when the application exhibits unfavorable access patterns?
Our contribution proves that under a workload in which two
different nodes do not often propose conﬂicting commands,
which is common in scalable transactional systems [15],
978-1-4673-8891-7/16 $31.00 © 2016 IEEE
DOI 10.1109/DSN.2016.23
156
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:37:37 UTC from IEEE Xplore.  Restrictions apply. 
(cid:2)
(cid:3)
[16], [17], [18], one can combine the advantages of multi-
leader Generalized Consensus algorithms and Multi-Paxos,
i.e., obtaining load balancing among nodes, a high propor-
tion of decisions in two communication delays, the adoption
of classic quorums, and no dependency relations to compute
or exchange.
We present1 M 2P AXOS, an implementation of General-
ized Consensus that generally, which in this paper means
under favorable conditions of low inter-node contention and
temporal locality, provides the following optimal features:
M 2P AXOS decides commands in only two communication
delays; it does not compute dependencies on commands, and
hence it does not exchange dependencies among nodes; and
+ 1, like Multi-
it relies on quorums of size equal to
Paxos. We name the aforementioned workload partitionable.
Underlying M 2P AXOS lies the following observation:
Generalized Consensus algorithms conservatively use fast
quorums and dependency relations because they must re-
cover even when interfering commands can be ordered at
the same time by different nodes with the possibility of fast
decisions. However, if we can prevent different nodes from
issuing conﬂicting commands at the same time, then we can
reduce the inherent costs of those algorithms.
is accessed, e.g.,
M 2P AXOS is particularly effective in deployments where
the set of accessed objects is well deﬁned once a “home”
object
the access pattern of the well
known TPC-C benchmark [20] ﬁrst involves an access to a
warehouse (i.e., the home object) to which the subsequent
accessed objects will very likely be related.
N
2
We implemented M 2P AXOS in the Go programming
language2 and compared it against Generalized Paxos [11],
Multi-Paxos [7], and EPaxos [8], a recent high performance
implementation of generalized consensus. M 2P AXOS is
simple: there are no time consuming operations performed
on its critical path and it scales well in partitioned workloads.
Once the ownership is deﬁned and is stable, M 2P AXOS
substantially outperforms all competitors. The maximum
speed-up observed against EPaxos, which is the best com-
petitor, is 7× when 49 nodes are deployed and objects are
partitioned across them. We also evaluated M 2P AXOS by
implementing a benchmark producing the TPC-C workload.
In this deployment, M 2P AXOS outperforms EPaxos by as
much as 5.5× and Multi-Paxos by as much as 2.5×.
Moreover, we have formalized a high level description of
M 2P AXOS in TLA+ [21], and have model-checked it with
the TLC model-checker3.
The remainder of this paper is structured as follows.
In Section II, we discuss related work. In Section III,
we present the target system model and the deﬁnition of
1A poster version of this paper recently appeared in [19].
2Our M 2P AXOS implementation is publicly available at the following
link: https://bitbucket.org/talex/hyﬂow-go.
3The TLA+ formalization is in the appendix of the M 2P AXOS technical
report [22] and at http://losa.fr/M2Paxos.
Generalized Consensus. M 2P AXOS is introduced in Sec-
tion IV, while its details are presented in Section V. Finally,
Section VI provides arguments on the correctness, and
Section VII presents the results of our experimental study.
II. RELATED WORK
In the classic Paxos algorithm, a value is learned after
a minimum of four communication delays. Progress guar-
antees are provided as long as no two nodes are trying to
become leaders concurrently (this step is called a Prepare
phase). Multi-Paxos alleviates this problem by letting a
Prepare phase cover an entire sequence of values. This effec-
tively establishes a proposer that acts as a designated leader.
Once the leader is elected, new values can be learned in only
three communication delays and progress can be guaranteed
in periods of synchrony. Fast Paxos [13] can eliminate one
communication delay by having proposers bypass the leader
and broadcast their requests directly to nodes, which is called
a fast path. If a fast path fails due to concurrent proposals
(called a collision),
the designated leader needs to take
over the decision by adding two additional communication
delays. Moreover, acceptors in Fast Paxos have to wait for
a number of replies that is greater than a majority of nodes
+ 1, a fast quorum).
in the fast rounds (a minimum of
Generalized Paxos [11] solves Generalized Consensus
and, as with Fast Paxos, it can decide commands in two
communication delays. Unlike Fast Paxos, it can also do
that in the case of concurrent proposals as long as commands
are commutative. If not, recovery from a collision scenario
requires the same costs paid by Fast Paxos. This overhead
is avoided by the Fast Genuine Generalized Consensus
(FGGC) algorithm [23], which is able to reduce the extra
communication delays for the recovery from four to one
by leveraging the following assumption: every fast quorum
in a round has to include the leader of that round. FGGC
is also optimized to provide reasonable performance in
high and non-well-partitioned contention scenarios (unlike
M 2P AXOS), but it may suffer from higher latency because
nodes have to wait for the leader in all rounds.
2·N
3
(cid:2)
(cid:3)
EPaxos [8] is a multi-leader solution to the generalized
consensus problem. EPaxos employs dependency tracking
and fast quorums to deliver non-conﬂicting commands using
a fast path of two communication delays. In the presence of
conﬂicts, however, the protocol takes a slow path of four
communication delays before delivering.
The advantages of M 2P AXOS over the previous Paxos-
based algorithms are clear: M 2P AXOS is able to decide
commands in two communication delays, as they do, but
without relying on fast quorums, a designated leader, and
the exchange of dependencies among commands.
M 2P AXOS is also related to the LILAC-TM protocol [24]
since they share the basic idea of exploiting ownership of
objects to save communication steps during a distributed co-
ordination, which was introduced in [25]. However, LILAC-
157
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:37:37 UTC from IEEE Xplore.  Restrictions apply. 
TM addresses orthogonal problems whose solutions can be
integrated in M 2P AXOS to boost its performance.
There also exists a clear relation between M 2P AXOS and
the RedBlue consistency approach [26], since both leverage
commutativity to provide faster executions. However, Red-
Blue consistency does that at a different level: it exploits
commutativity to execute non-strongly consistent operations
(blue operations) faster. On the other hand, M 2P AXOS
leverages commutativity to execute strongly consistent op-
erations (red operations) faster. Basically, they can be inte-
grated to exploit each other’s advantages: for instance, red
operations in implementations of RedBlue consistency can
be handled using M 2P AXOS.
III. SYSTEM MODEL AND CONSENSUS
We assume a set of nodes Π = {p1, p2, . . . , pN} com-
municating through message passing where messages may
experience arbitrarily long, although ﬁnite, delays and do
not have access to either shared memory or a global clock.
Nodes may fail by crashing but do not behave maliciously.
A node that does not crash is called correct; otherwise, it
is faulty. Because of the well-known FLP result [27], we
assume that the system can be enhanced with the weakest
type of unreliable failure detector [28] that is necessary to
implement a leader election service [29]. The leader election
(and thus the failure detector) is needed by M 2P AXOS to
accomplish a successful change of object ownership if no
conﬂicting commands are proposed in parallel. In addition,
due to the result in [2], we assume that at least a strict
+ 1, is correct and thus at most
majority of nodes, i.e.,
f =