Time (hours)
Figure 16: Cacheability per day and per hour. Comparison between diﬀerent population sizes.
Cacheability vs population size
CHR vs cache size (over 3 days, 19/5−21/5)
1
0.9
0.8
0.7
0.6
R
H
C
0.5
0.4
0.3
0.2
0.1
0
0
Cache size: 376 programs
5% of average daily demand
500
1000
1500
2000
2500
3000
Cache size (programs)
Clairvoyant
LFU
LRU
5000
10000
15000
20000
25000
Population size
1.0
y
t
i
l
i
b
a
e
h
c
a
C
0.8
0.6
0.4
0.2
0
y
t
i
l
i
b
a
e
h
c
a
C
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Figure 17: Example of cacheability versus population size.
programs, which is 5% of average daily demand. We can
see that caching 5% of the daily demand gives a hit ratio
of 57% for LRU, 60% for LFU and 75% for the Clairvoyant
replacement policy.
In Figure 18 we include the requests from all viewers. In
Figure 19 we investigate the impact of population size. Here
we use the LRU replacement policy and compare the cache
hit ratios for populations of diﬀerent size. For a cache size
of 376 programs (5% of the daily demand) the town subset
of 23304 clients get a hit ratio of 51%. This is close to the
result for the full set of clients. For the small population
with 1000 clients we get a hit ratio of 43%. The cacheability
for this particular population of 1000 viewers was 0.64 over
125 days and we see the curve approaching that value at a
cache size of 3000 programs.
The cache hit ratio also varies over time. Figure 20 shows
hit ratio per hour for all viewers, the LRU replacement pol-
icy and a cache size of 376 programs. The hit ratio was
calculated over 17 weeks and the ﬁgure shows the median
(and max and min) value for each hour of the week. We
can see that the cache hit ratio varies over the day and it
increases when it is needed as most. During prime time,
when there are the most requests, the hit ratio is over 60%.
Figure 18: Cache hit ratio versus cache size, requests from
all clients over 3 days. Comparison of the LRU, LFU and
Clairvoyant replacement policies.
From the results presented above we highlight three ob-
servations:
• The cacheability and the potential for caching is very
high.
• The hit ratio with a simple LRU replacement policy
is above 50% when caching 5% of the average daily
demand.
• The hit ratio increases during prime time when it is
needed most. This is consistent with the observations
in Section 3 that the share of requests for the most
popular programs increases during prime time.
We have here looked at the cache friendliness of the TV-
on-demand workload in terms of cacheability and cache hit
ratios for the basic LRU and LFU replacement policies. In
Section 7 on future work we discuss how our observations
about access patterns and program popularity can poten-
tially be used to design a more informed caching strategy.
2071
0.9
0.8
0.7
0.6
R
H
C
0.5
0.4
0.3
0.2
0.1
0
0
CHR vs cache size (125days, 12/5−13/9)
Cache size:376 programs
5% of average daily demand
LRU Region (population:307347)
LRU Town (population:23304)
LRU rand 10000
LRU rand 1000
500
1000
1500
2000
2500
3000
Cache size (programs)
R
H
C
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Cache hit ratio (CHR) per hour over 17 weeks, 12/5−7/9.
Replacement policy: LRU
Cache size: 376 programs
(5% of average daily demand)
Max CHR per hour over 17 weeks
Median CHR per hour over 17 weeks
Min CHR per hour over 17 weeks
Thu
Fri
Sat
Sun
Mon
Tue
Wed
Time (hours)
Figure 19: Cache hit ratio versus cache size. Comparison of
cache hit ratios for populations of diﬀerent size.
Figure 20: Cache hit ratio per hour over 17 weeks
6. RELATED WORK
There are several studies of viewing behaviour in IPTV
systems where traditional scheduled TV is distributed over
IP networks. Cha et al. [8] study viewing behaviour includ-
ing channel popularity and channel switching in an opera-
tional IPTV network. Qiu et al. model TV channel popu-
larity [18] and user activities [17] in a large IPTV system.
Our work is diﬀerent in that we look at TV-on-Demand
where the viewers choose programs to watch outside of the
TV schedule. In this sense our work is closer to studies of
traditional VoD systems.
Yu et al. [26] present a large measurement study of the
chinese PowerInfo Video-on-Demand system. This work is
similar to ours in that they investigate many aspects of user
behaviour and content access patterns. The PowerInfo sys-
tem is a traditional VoD system. The videos in the library
are old TV shows and movies and there are usually only
a few new movies introduced to the system per day. This
is diﬀerent from the TV-on-Demand system that we study
where there is a large inﬂow of new programs from the TV-
schedule, time-shifted viewing, and programs with a very
short life-span. Our work is also diﬀerent in other aspects in
that we investigate how the access pattern depend on genre,
we study cacheability and use trace-based simulation to in-
vestigate what impact the access patterns have on caching.
There are many other interesting studies of VoD systems
and video popularity. Griwodz et al. [12] model long-term
popularity of videos on the time scale of days based on VHS
rental statistics. Lou et al. [16] give examples of the popu-
larity evolution of video ﬁles from a Chinese television sta-
tion. Tang et al. [19] analyse and model many aspects of
media server access. Avramova et al. [3] model the popular-
ity evolution of TV-on-demand and video traces. Dan and
Carlsson [9] measure and analyse BitTorrent content pop-
ularity. Guo et al. [13] study the probability distributions
of Internet media workloads and analyse caching using a
mathematical model. Yin et al. [25] study live VoD work-
loads from the 2008 Beijing Olympics. There are also many
studies of Youtube and user generated videos [4, 7, 10, 14].
Gopalakrishnan et al. [11] study user behaviour in a large
IPTV system. This is similar to our work but their focus
is on modeling the interactive user behaviour in an IPTV
environment, including how users fast-forward, pause and
rewind to control their viewing.
In this paper we also investigate cacheability and we look
at the potential for caching in a TV-on-Demand system.
Caching has been widely studied for web content and video [6,
15, 22, 24]. More recently, Ager et al. [2] studied the cacheabil-
ity for HTTP- and P2P-based applications. There are also
several studies of caching strategies in IPTV on-demand sys-
tems [1, 5, 20, 21, 23], but these studies use analytical mod-
els and simulations whereas we present a trace-based study
from a real TV-on-Demand system.
7. FUTURE WORK
In this paper we have studied many aspects of the access
patterns in a TV-on-Demand system. We have looked at
the cache friendliness of the workload in terms of cacheabil-
ity and hit ratios for basic replacement policies. For future
work we hope that our observations can be used as a basis
for developing better caching strategies for TV-on-Demand
systems.
When studying the cache friendliness of the request stream
in Section 5 we used the basic LRU and LFU cache replace-
ment policies. With these the last requested program is al-
ways cached and the choice of what to evict from the cache
is between the least recently and the least frequently re-
quested program. A more advanced system could use more
knowledge about access patterns and program popularity to
decide what program to put in the cache and what program
to evict.
One such strategy could be to keep track of all programs
in the system, also those that are not currently in the cache.
One could monitor the popularity by counting requests, let
the programs age over time and for each program keep a
value that describes the probability that it will be requested.
There are several observations in this paper that can be
useful for such an informed caching strategy:
Give preference to new programs
With time-shifted TV ongoing scheduled programs immedi-
ately get a lot of requests. Some programs, like TV-news,
also have a very short life-span. The value of a program
should not have to be built up by requests over a long time.
Categorize programs by genre to predict change
in popularity over time
208We saw in Section 4 that the access pattern very much de-
pends on the type of program. A news program that is
top-ranked the ﬁrst evening age quickly and have a very low
probability for being requested the next evening. A rental
movie however is popular for months and increase in rank
during weekends. By categorizing programs by genre the
probability for future requests can be predicted. The cate-
gorization of programs can also be more detailed. The re-
quest patterns for diﬀerent episodes of the same show are
often very similar as we saw in Figure 9, Section 4.2. For
a new episode of a show it is a reasonable assumption that
the popularity of the program will change over time in a way
similar to that of the previous episodes.
Focus on prime time
The value of a program should reﬂect the probability that it
will be requested during prime time. There are large peaks
in demand in the evenings and at the weekends that need
to be handled. If caching is used to limit the maximum link
load then it is essential to have the right programs in the
cache on Friday and Saturday evenings. There are program
like cartoons that are top-ranked in the mornings and early
evenings that probably should not be in the cache.
The observations and the predictions outlined above can
be used to optimise the caching performance. However, the
basic monitoring of request frequency is still needed as a
basis, and to handle unexpected changes and sudden peaks
in program demand for instance due to large news events.
8. CONCLUSIONS
We have analysed the access patterns in a large TV-on-
Demand system and studied the potential for caching.
Our contribution in this paper is three-fold. As a ﬁrst-
order result, we provide reconﬁrmation of known observa-
tions with an independent dataset. We demonstrate that
there is a small set of programs that account for a large part
of the requests. The program popularity conforms with the
Pareto principle, or 80-20 rule. The demand follows a diur-
nal and weekly pattern, and there are large peaks in demand
on Friday and Saturday evenings that need to be handled.
Second, we provide systematic evidence of TV-on-Demand
access pattern characteristics that are intuitive yet uncon-
ﬁrmed in the literature. We show that news programs have
a very short lifespan and are often only requested for a few
hours, children’s programs are top ranked in the mornings
and early evenings, and movie rentals are concentrated over
weekends.
Finally, we also provide novel insights into access patterns
that have not been reported previously to the best of our
knowledge. We show how the popularity of TV-on-Demand
programs changes over time. We see that the access pattern
in a TV-on-Demand system very much depend on what type
of content it oﬀers. Furthermore, we ﬁnd that the share of
requests for the top most popular programs grows during
prime time, and the change rate among them decreases. The
cacheability is very high and the cache hit ratio increases
during prime time when it is needed most.
We believe that these observations and ﬁndings can guide
the design of future systems for TV-on-Demand infrastruc-
tures.
9. ACKNOWLEDGMENTS
This work has been performed within the SICS Center for
Networked Systems funded by VINNOVA, KKS, SSF, ABB,
Ericsson, Saab SDS, TeliaSonera, T2Data, Vendolocus and
Peerialism.
10. REFERENCES
[1] H. Abrahamsson and M. Bj¨orkman. Simulation of
IPTV caching strategies. In Proceedings of
SPECTS’10, Ottawa, Canada, 2010.
[2] B. Ager, F. Schneider, J. Kim, and A. Feldmann.
Revisiting Cacheability in Times of User Generated
Content. In Proceedings of 13th IEEE Global Internet
Symposium, San Diego, CA, USA, March 2010.
[3] Z. Avramova, S. Wittevrongel, H. Bruneel, and
D. Vleeschauwer. Analysis and Modeling of Video
Popularity Evolution in Various Online Video Content
Systems: Power Law versus Exponential Decay. In
Proceedings of International Conference on Evolving
Internet, 2009.
[4] Y. Borghol, S. Mitra, S. Ardon, N. Carlsson, D. Eager,
and A. Mahanti. Characterizing and Modeling
Popularity of User-generated Videos. In Proceedings of
IFIP International Symposium on Computer
Performance, Modeling, Measurements and Evaluation
(PERFORMANCE), 2011.
[5] S. Borst, V. Gupta, and A. Walid. Distributed caching
algorithms for content distribution networks. In
Proceedings of INFOCOM’10, San Diego, USA, 2010.
[6] L. Breslau, P. Cao, L. Fan, G. Phillips, and
S. Shenker. Web Caching and Zipf-like Distributions:
Evidence and Implications. In Proceedings of IEEE
INFOCOM, 1999.
[7] M. Cha, H. Kwak, P. Rodriguez, Y. Ahn, and
S. Moon. Analyzing the Video Popularity
Characteristics of Large-Scale User Generated Content
Systems. IEEE Transactions on Networking,
17:1357–1370, October 2009.
[8] M. Cha, P. Rodriguez, J. Crowcroft, S. Moon, and
X. Amatriain. Watching Television Over an IP
Network. In Proceedings of Internet Measurement
Conference (IMC’08), Greece, 2008.
[9] G. Dan and N. Carlsson. Power-law Revisited: A
Large Scale Measurement Study of P2P Content
Popularity. In Proceedings of IPTPS’10, San Jose,
USA, April 2010.
[10] P. Gill, M. Arlitt, Z. Li, and A. Mahanti. YouTube
Traﬃc Characterization: A view From the Edge. In
Proceedings of ACM SIGCOMM Internet
Measurement Conference, San Diego, USA, October
2007.
[11] V. Gopalakrishnan, R. Jana, K. Ramakrishnan,
D. Swayne, and V. Vaishampayan. Understanding
Couch Potatoes: Measurement and Modeling of
Interactive Usage of IPTV at large scale. In
Proceedings of Internet Measurement Conference
(IMC’11), 2011.
[12] C. Griwodz, M. B¨ar, and L. Wolf. Long-term Movie
Popularity Models in Video-on-Demand Systems or
The Life of an on-Demand Movie. In Proceedings of
ACM Multimedia’97, Seattle, USA, 1997.
[13] L. Guo, E. Tan, S. Chen, Z. Xiao, and X. Zhang. The
stretched exponential distribution of internet media
access patterns. In Proceedings of the twenty-seventh
209ACM symposium on Principles of distributed
computing (PODC’08), New York, USA, 2008.
[14] X. Kang, H. Zhang, G. Jiang, H. Chen, X. Meng, and
K. Yoshihira. Measurement, Modeling, and Analysis of
Internet Video Sharing Site Workload: A Case Study.
In Proceedings of ICWS’08, 2008.
[15] J. Liu and J. Xu. Proxy Caching for Media Streaming
Over the Internet. IEEE Communications Magazine,
42:88–94, 2004.
[16] J. Lou, Y. Tang, M. Zhang, and S. Yang.
Characterizing User Behavior Model to Evaluate Hard
Cache in Peer-to-Peer Based Video-on-demand
Service. In Proceedings of MMM’07, pages 125–134,
2007.
[21] D. D. Vleeschauwer and K. Laevens. Performance of
caching algorithms for IPTV on-demand services.
IEEE Transactions on broadcasting, 55:491 – 501,
2009.
[22] J. Wang. A Survey of Web Caching Schemes for the
Internet. ACM SIGCOMM Computer Communication
Review, 29:36–46, 1999.
[23] T. Wauters, W. V. de Meerssche, F. D. Turck,
B. Dhoedt, P. Demeester, T. V. Caenegem, and
E. Six. Co-operative Proxy Caching Algorithms for
Time-Shifted IPTV Services. In Proceedings of 32nd
EUROMICRO Conference on Software Engineering
and Advanced Applications (SEAA), pages 379–386,
Dubrovnik, Croatia, September 2006.
[17] T. Qiu, Z. Ge, S. Lee, J. Wang, J. Xu, and Q. Zhao.
[24] A. Wolman, G. M. Voelker, N. Sharma, N. Cardwell,
Modeling User Activities in a Large IPTV System. In
Proceedings of Internet Measurement Conference
(IMC’09), USA, 2009.
[18] T. Qiu, Z. Ge, S. Lee, J. Wang, Q. Zhao, and J. Xu.
Modeling Channel Popularity Dynamics in a Large
IPTV System. In Proceedings of SIGMETRICS, pages
275–286, Seattle, USA, June 2009.
[19] W. Tang, Y. Fu, L. Cherkasova, and A. Vahdat.
Modeling and Generating Realistic Streaming Media
Server Workloads. Computer Networks, 51:336–356,
2007.
[20] D. D. Vleeschauwer, Z. Avramova, S. Wittevrongel,
and H. Brueel. Transport Capacity for a Catch-up
Television Service. In Proceedings of EuroITV’09,
pages 161–170, Leuven, Belgium, June 2009.
A. Karlin, and H. M. Levy. On the scale and
performance of cooperative Web proxy caching. In
Proceedings of the 17th ACM Symposium on Operating
Systems Principles (SOSP ’99), 1999.
[25] H. Yin, X. Liu, F. Qiu, N. Xia, C. Lin, H. Zhang,
V. Sekar, and G. Min. Inside the Bird’s Nest:
Measurements of Large-Scale Live VoD from the 2008
Olympics. In Proceedings of Internet Measurement
Conference (IMC’09), USA, 2009.
[26] H. Yu, D. Zheng, B. Zhao, and W. Zheng.
Understanding User Behavior in Large-Scale
Video-on-Demand Systems. In Proceedings of
EuroSys2006, pages 333–344, Leuven, Belgium, 2006.
210