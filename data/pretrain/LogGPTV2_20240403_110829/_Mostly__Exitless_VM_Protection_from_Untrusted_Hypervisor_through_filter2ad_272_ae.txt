8.1 Methodology
Name
apache
mysql
memcached
kernel
compile
(kbuild)
untar
hackbench
dbench
Description
Apache v2.4.7 Web server running ApacheBench v2.3 with
the default conﬁguration, which measures the number of han-
dled requests per second serving the index page using 100
concurrent clients to send 10,000 requests totally
MySQL v14.14 (distrib 5.5.57) running the sysbench oltp
benchmark using 6 threads concurrently to measure the time
cost by an oltp test, the size of oltp table is 1000000 and the
oltp test mode is complex mode
memcached v1.4.14 using the memcslap benchmark on the
same VM, with a concurrency parameter of 100 to test the
time it takes to load data
kernel compilation time by compiling the Linux 4.7.0 from
scratch with the default conﬁguration using GCC 4.8.4-2
untar extracting the 4.7.0 Linux kernel tarball compressed
with gzip compression using the standard tar utility, measur-
ing the time cost
hackbench v0.39-1 using unix domain sockets and 100 pro-
cess groups running with 500 loops, measuring the time spent
by each sender sending 500 messages of 100 bytes
dbench v4.0 using different numbers of clients to run I/O
Read/Write tests under empty directories with default client
conﬁguration repeatedly
Table 3: Description of real applications.
In this
section, we demonstrate the efﬁciency of
CloudVisor-D by comparing it with the vanilla Xen hyper-
visor (v4.5.0). Our test machine is equipped with an Intel
Skylake Core i7-6700K processor, which has 4 cores and 8
hardware threads with the hyper-threading enabled. The stor-
age device is a 1TB Samsung 860 EVO SATA3 SSD.
USENIX Association
29th USENIX Security Symposium    1705
All the benchmarks we used and their setup details are de-
scribed in Table 3. The Dom0 is Debian 8.9 and the kernel
is Linux 4.4.80. We used Ubuntu 16.04 for the guest virtual
machine and Linux 4.7.0 as its kernel. The guest has 1 (a
UP VM) or 2 (an SMP VM) vCPUs, 2GB virtual memory
and 30GB virtual disk. All multicore evaluations were done
using two vCPUs bound to two physical CPUs. To ensure
the evaluation results measured at the same CPU clock, we
disabled the CPU frequency scaling.
8.2 Status Quo and Complexity
To answer the ﬁrst question (Q1), we have built a
prototype of CloudVisor-D on an Intel Skylake machine.
CloudVisor-D uses the Intel AES-NI [2] for encryption and
leverages IOMMU to defend against DMA attacks (Sec-
tion 6). Table 4 shows the breakdown of CloudVisor-D TCB,
which is measured by the sloccount tool [6]. The code sizes
of the RootVisor and Guardian-VM are 4,174 and 1,656 re-
spectively. The sum is roughly equal to that of CloudVisor,
which means CloudVisor-D does not increase the TCB size.
Functionality
VMCS Manipulation
Memory Management
Exit Handlers
Other
Reference Monitor
Encryption
Hash Integrity
Total
LOC
1,742
1,397
583
452
429
574
653
5,830
RootVisor
Guardian-VM
CloudVisor-D
Table 4: The breakdown of CloudVisor-D TCB.
8.3 Micro-architectural Operations
Speedup
Operation
61.3%
Hypercall
85.0%
EPT violation handling
37.5%
Virtual IPI
Table 5: Micro-architectural operation overhead measured
in cycles.
CloudVisor-D
1810
9929
13331
Xen
1758
5374
11214
CloudVisor
4681
66301
21344
To answer the second question (Q2), we quantiﬁed the per-
formance loss of micro-architectural operations of the hy-
pervisor on an SMP virtual machine. Table 5 presents the
costs of various micro-architectural operations in an SMP
VM. The results are measured in cycles.
Hypercall is an operation commonly used by the guest
kernel to interact with the hypervisor. To measure its perfor-
mance, we call a do_vcpu_op hypercall to check whether a
vCPU is running or not. In the Xen hypervisor, this hyper-
call causes two VM ring crossings: a VM exit and a VM en-
try. Even if CloudVisor-D causes more EPT switches, it can
achieve similar performance via the efﬁcient remote calls. A
hypercall in CloudVisor incurs almost 3 times as many cycles
due to a large number of ring crossings as we have analyzed
in Section 2.
EPT violation handling is the total cost of switching to the
SubVisor, handling the EPT violation and returning to the
guest. We invalidated one GPA in the guest EPT and mea-
sured the procedure of reading a value in the address, which
involves an EPT violation handling. The result is an average
of 5,000 tests. The cost of this operation in CloudVisor-D
is larger than that in Xen due to the manipulation of EPT
in Guardian-VM introduced in Section 5. In CloudVisor, the
SubVisor causes two VM ring crossing each time it modi-
ﬁes the guest EPT, which introduces multiple VM ring cross-
ings when handling EPT violations. Therefore, it performs
the worst, which is nearly 10 times worse than Xen and
CloudVisor-D.
Virtual IPI is the cost of issuing an IPI to another vCPU.
We pinned two vCPUs to different physical CPUs. Virtual IPI
is an important operation intensively used in the multi-core
machines. The measured time starts from sending an IPI in
one vCPU until the other vCPU responds. In Xen hypervi-
sor, a virtual IPI is implemented by sending an event using
the event channel to the SubVisor, which then injects a vir-
tual interrupt to the target vCPU. CloudVisor-D replaces the
do_event_channel_op hypercall with a remote call to allow
one vCPU to send an event without any VM exit. Yet, we did
not optimize the virtual interrupt sending procedure which is
our future work. Even if CloudVisor-D is slower than Xen,
it is signiﬁcantly faster than CloudVisor due to the efﬁcient
remote calls.
8.4 Applications Performance
To answer Q3, we measured CloudVisor-D with real-
world applications which have various execution character-
istics. Since CloudVisor only supports emulated I/O devices,
it is unfair to directly compare it with CloudVisor-D, which
supports a PV I/O device model. Moreover, the vanilla Xen
has been shown to outperform CloudVisor. Therefore, we di-
rectly compared CloudVisor-D with the vanilla Xen, which
is sufﬁcient to demonstrate CloudVisor-D performance.
Figure 11(a) shows the result of the performance compar-
ison of CloudVisor-D on real applications with the vanilla
Xen hypervisor in a uniprocessor VM. CloudVisor-D per-
forms similarly to the vanilla Xen hypervisor across all work-
loads. The maximum overhead is not larger than 5%. We also
evaluated these applications in an SMP VM. Figure 11(b)
shows the normalized performance of real applications in an
SMP VM. For these real-world applications, CloudVisor-D
still incurs negligible overhead. It even performs better than
the vanilla hypervisor, especially for the memcached bench-
mark. Benchmarks such as memcached incur many event
channel communications in an SMP setting which is opti-
mized by CloudVisor-D by using the efﬁcient remote calls.
To check the impact of this optimization, we ran a guest
VM with and without using the do_event_channel_op re-
mote call and compared their performance. As shown in Ta-
ble 6, a guest without do_event_channel_op remote call suf-
fers from severe performance degradation, which means the
do_event_channel_op remote call improves performance
1706    29th USENIX Security Symposium
USENIX Association
Xen
CloudVisor-D
Xen
CloudVisor-D
)
s
/
B
M
(
t
u
p
h
g
u
o
r
h
T
 800
 700
 600
 500
 400
 300
 200
 100
 0
1.0
%
10
4.1
%
2.8
%
3.6
%
2.3
%
3.7
%
30
20
50
Number of clients
40
60
1.6
% 4.6
%
0.2
%
1.1
%
4.5
%
 1000
)
s
/
B
M
(
t
u
p
h
g
u
o
r
h
T
 800
 600
 400
 200
 0
-1
 0
 2
 1
 4
Number of clients
 3
)
%
(
d
a
e
h
r
e
v
O
 3
 2.5
 2
 1.5
 1
 0.5
 0
3.6
%
 5
 6
CloudVisor-D
1
2
4
VM Number
8
(a)
(b)
Figure 9: Throughput for dbench in UP (left) and SMP (right) VMs using from 10 to 60
concurrent clients. The numbers above each bar are the CloudVisor-D overhead compared
with the vanilla Xen. (Higher is better)
Figure 10: Performance overhead
for kernel building in CloudVisor-
D compared to the vanilla Xen for
different number of concorrent VMs.
(Lower is better)
)
%
(
d
a
e
h
r
e
v
O
e
v
i
t
l
a
e
R
5
4
3
2
1
0
A
M
m
u
p
a
c
h
y
S
e
m
n
t
a
r
Q
e
L
c