Proof. We show there exists a polynomial size simulator S such
that the simulated view vS(Hη) and the real view vR(Hη) of history
Hη are computationally indistinguishable. Let vR(Hη) = {I, R, O}
be the real view. Then S adaptively generates the simulated view
vS = {I∗, R∗, O∗}
S first generates α number of random data of size {|I1|, ..., |Iα |}
and saves it as I∗. Then S generates random data for R∗ = {|R1|,
..., |Rβ |} similarly.
Now, for the ith function Fi in Cη, S accesses I∗[j] where j ∈
Ap(i), S accesses R∗[j] where j ∈ Bp(i), S replaces value in R∗[j]
where j ∈ Up(Hη)(i) with new random and finally during the last
operation S generates random data of size |O| and sets it to O∗ .
Since each component of vR(Hη) and vS(Hη) are computation-
ally indistinguishable, we conclude that the proposed schema satis-
fies the security definition.
□
4.2 Information Leakage Discussion
As we discussed all the data that is kept outside of the enclave is
encrypted using AES-GCM mode, the storage does not leak any
information and any modification to the stored data can be detected
easily.
Although, our proposed framework is data oblivious, as stated in
the above proof, we allow certain information leakage for efficiency.
Intuitively, we allow the adversary to know the input and output
size of a function. In addition, since trying to hide intermediate
operation types would be too costly, we allow the adversary to
know/infer intermediate input output operations required for the
execution of a function. If we were to hide the operation type, we
would have to perform equal number of operations for all functions
(e.g., trying to hide whether we are doing secure matrix multipli-
cation versus secure matrix addition on two encrypted matrices).
Otherwise, the adversary will learn some information about the
performed function. In our experimental evaluation, we observed
that the overhead varies widely based on the intermediate functions.
So, forcing all the functions to perform the exact same number of
operations would make the framework very inefficient especially
for large data sets.
Another issue is whether the size of the intermediate results can
disclose any sensitive information. All of the matrix operations in
our framework have fixed size outputs given the input data set size.
Therefore, the size information is already inferable by knowing
the matrix operation type and the input data set size. Therefore,
intermediate result size does not disclose any further information.
In some cases, to prevent leakage due to revealing intermediate
result size, we may skip certain optimization heuristics. For exam-
ple, as observed in [56], the heuristic of pushing selection predicates
down the relational algebra operation tree may be skipped to pre-
vent intermediate result size leakage. So our optimization heuristics
discussed in subsection 3.7 could be turned off to prevent this type
of leakage.
In other cases, intermediate results may reveal some sensitive
information. For example, consider the statement s = count(
where(X[1]>=0)) discussed in subsection 3.4 where we learn the
(a) Matrix Size
(b) Block Size
(c) Chunk Size
Figure 4: Load time encrypted vs. unencrypted
(a) Matrix Size
(b) Block Size
(c) Block Variation
Figure 5: Scalar Multiplication time encrypted vs. unencrypted for different matrix (a) and block size (b). Surface plot of
encrypted execution time for different block size (c).
number of tuples in X that has column 1 value bigger or equal than
0. If s value is used in an operation that results in an object cre-
ation (e.g., y=zeros(s)), then the sensitive s value could be leaked
by observing the output size. To protect against such a leakage,
our compiler automatically raises a warning as discussed in subsec-
tion 3.4. This way users may consider changing their programs
to prevent such leakage. Still, we believe that this will not be an
issue in many scenarios. For example, in the case studies we have
conducted such a leakage never occurred.
5 EXPERIMENTAL EVALUATIONS
In this section, we perform experimental evaluations to show the
effectiveness of our proposed system. We developed a prototype
application using Visual Studio 2012 and Intel Software Guard Exten-
sions Evaluation SDK 1.0 for Windows. We perform the experiments
on a Dell Precision 3620 computer with Intel Core i7 6700 CPU, 64GB
RAM, running Windows 7 Professional.
5.1 Individual Operation Performance
Experiment Setup. To understand the performance of the indi-
vidual operations, we generated random data sets with varying
sizes and observe the time it takes to perform important operations.
However, we acknowledge that the time is sensitive to other events
occurring on the operating system. So we rerun the same experi-
ment (minimum 5 times) and report the average time. In addition,
for all the individual operation experiments, we reported the results
from encrypted and unencrypted version of our operations. For the
unencrypted version, we use the SGX memory constrained environ-
ment to perform the same operations without encryption. In this
way we can observe the encryption overhead of the system. We did
not consider an implementation outside the enclave as a base line,
because we observe that the same operations inside enclave takes
significantly longer time compared to the outside enclave version.
This might be due to the fact that SGX by itself does encryption
of the pages and cannot really utilize existing caching mechanism.
Finally, to ensure the correctness of our framework we collected
data access trace of all the operations for different inputs of the
same size and checked whether they match.
Load Operation. We start with load operation, which consists of
loading data encrypted with user key, decrypt it, and store again
with session key for further use (e.g., the key stored for writing
intermediate results to the disk during the operation). As explained
in section 3, we break a BigMatrix into smaller blocks and then
load-store each block, as SGX enclave can allocate a certain amount
of memory. In addition, we also observe that we cannot pass large
amount of data through ECalls and OCalls. So, we had to further
 0 1000 2000 3000 4000 5000 6000 7000 8000 2x107 4x107 6x107 8x107 1x108 1.2x108 1.4x108 1.6x108Load Time (ms)Matrix ElementsUnencryptedEncrypted 340 360 380 400 420 440 460 600000 700000 800000 900000 1x106 1.1x106 1.2x106 1.3x106 1.4x106 1.5x106Load Time (ms)Block ElementsUnencryptedEncrypted 310 320 330 340 350 360 370 380 390 50000 100000 150000 200000 250000 300000 350000 400000 450000Load Time (ms)Chunk Size(Bytes)UnencryptedEncrypted 6000 8000 10000 12000 14000 16000 18000 20000 1x108 1.2x108 1.4x108 1.6x108 1.8x108 2x108 2.2x108 2.4x108Scalar Multiplication Time (ms)Matrix ElementsUnencryptedEncrypted 9500 10000 10500 11000 11500 12000 12500 1x106 1.2x106 1.4x106 1.6x106 1.8x106 2x106 2.2x106 2.4x106Scalar Multiplication Time (ms)Block ElementsUnencryptedEncryptedExecution Time 100 200 300 400 500 100 200 300 400 500 140 145 150 155 160Scalar Operation Time (ms)(a) Matrix Size
(b) Block Size
(c) Block Variation
Figure 6: Element-wise multiplication execution time encrypted vs. unencrypted for different matrix size (a) and block sizes
(b). Surface plot of encrypted element-wise matrix multiplication (c).
break the block to smaller chunks. Figure 4 illustrates the perfor-
mance of load operation for randomly generated data. We report
three different experiments. In Figure 4(a), we report load time vs
matrix size for block size of 1000 × 1000. We observe that loading
time increases with size of the matrix. In Figure 4(b), we report load
time vs block size for the matrix 3000× 3000. Here, we observe that
certain block size causes load time to increase significantly. Finally
in Figure 4(c), we report the effect of the chunk size in load time.
We observe that the impact of the chunk size over the loading time
is insignificant, so we do not report the chunk size experiments
here. Furthermore, in each of the cases, we observe that encryption
has very little overhead.
Scalar Operations. Next, we report the performance of scalar
operations. We perform the scalar multiplication on varying matrix
and block sizes as illustrated in Figure 5. In particular, we perform
the scalar multiplication of a random value to all the elements
of input matrix in a block-by-block manner and store the result
as a different matrix. Here, we again observe that the operation
time increases with matrix size in Figure 5(a). However, the block
size change does not affect the operation time in most cases as
illustrated in Figure 5(b). In Figure 5(c), we also report a surface plot
of encrypted execution time of the scalar multiplication. Here x, and
y axis represents block row and block column numbers, respectively,
i.e., a point in x, y plain represents a block dimension, and z axis
represents the execution time. We observe that the execution time
remains steady and shows steady growth.
Element-wise Operation. Next, we report the performance of
element-wise operations. For an element-wise operation, we take
two randomly generated matrix and perform an element-wise mul-
tiplication and store the result. Similar to the scalar operation, we
observe that the operation time is almost linearly proportional to
the matrix size (in Figure 6(a)). Also we observe that the block size
does not have huge effect on the operation time (in Figure 6(b)).
Matrix Multiplication Operation. In Figure 8, we report the time
required to perform the matrix multiplication of two randomly
generated matrices of varying matrix size and block size. Similar
to the previous cases, we observe that matrix multiplication time
linearly depends on matrix size (in Figure 8(a)). However, here
we also observe that the overhead of encryption is very low due
to the intensive computation required for matrix multiplications.
In addition, we observe a big difference in various block sizes as
illustrated in Figure 8(b). Here we observe a steady growth in the
operation time with the block size increment. This can be attributed
to the large number of memory access for multiplication. For a
larger block size, our framework has to perform a large number
of memory accesses. And in this case, load-store and encryption-
decryption overhead is relatively smaller compared to the memory
accesses and computation. So we observe a significant increase in
the operation time.
From these sets of experiments, we observe that the operation
time is almost always linearly proportional to the size of the matrix.
However, block size has an important and varying impact on the
execution time. Each operation behaves differently based on these
two parameters. We argue that this is due to the nature of the
operation that we perform on blocks in memory during various
operations.
Transpose, Inverse, and Sort Operation. Next, we illustrate per-
formance of transpose, inverse, and sort operations in Figure 7.
Again, we observe that the required time is proportional to the size
of input matrix. For the matrix inverse experiments, we take square
matrix of different sizes and split it into 500 × 500 elements size
blocks and perform the inverse according to our iterative matrix
inverse algorithm described in Algorithm 1. We observe that the
time increment is correlated with the size of the matrix. For the
sort experiments, we generated three matrices one with random
data, one in ascending sorted order, and one descending sorted, and
ran our bitonic sort implementation. We observe that the required
time is exactly the same for all three cases. This affirms our claim
of data obliviousness as well.
Relational Operations. Finally, we perform the experiments that
highlight the performance of relational operations. Similar to our
previous experiments, we observe that relational operations also
show linear growth in execution time with input matrix size as
illustrated in Figure 9.
 10000 12000 14000 16000 18000 20000 22000 24000 26000 28000 1x108 1.2x108 1.4x108 1.6x108 1.8x108 2x108 2.2x108 2.4x108Element-wise Multiplication Time (ms)Matrix ElementsUnencryptedEncrypted 14000 14500 15000 15500 16000 16500 17000 17500 18000 1x106 1.1x106 1.2x106 1.3x106 1.4x106 1.5x106 1.6x106 1.7x106Element-wise Multiplication Time (ms)Block ElementsUnencryptedEncryptedExecution Time 100 200 300 400 500 100 200 300 400 500 205 210 215 220 225Element-wise Operation Time (ms)(a) Matrix transpose
(b) Matrix inverse
(c) Bitonic sort
Figure 7: Matrix transpose, inverse and sort operation performance.
(a) Matrix Size
(b) Block Size
(c) Block Variation
Figure 8: Matrix multiplication time encrypted vs. unencrypted for different matrix size (a) and block size (b). Surface plot of
encrypted matrix multiplication execution time for varying block size (c).
(a) Join Operation
(b) Selection Operation
(c) Aggregation Operation
Figure 9: Relational operations performance encrypted vs. unencrypted.
5.2 Case Studies
In this subsection, we perform experiments to show the effective-
ness of our overall framework to solve real-world complex problems
and the potential information leakage in each case.
Linear Regression. We start with performing linear regression on
random datasets. We chose linear regression because it is commonly
used in many scientific studies [45, 51]. The time required for the
execution is reported in Figure 10. We observe that the operation
time is proportional to the input size. This is due to the fact each
internal operation to compute θ exhibits a linear growth property.
Next, we report the execution time to compute the θ on two real
world machine learning datasets: USCensus1990 [43] and Online-
NewsPopularity [28] from UCI Machine Learning Repository [10].
In both cases, we take one column as the target variable and others
as the input feature. The results are given in Table 1.
As we have proved in section 4, an attacker (e.g., a malicious
operating system) can learn limited information due to the data
analytics task execution over the encrypted data. In this case study,
basically, regression is executed using a sequence of operations
 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 0 1x107 2x107 3x107 4x107 5x107 6x107 7x107 8x107 9x107 1x108Transpose Time (ms)Matrix ElementsUnencryptedEncrypted 0 20000 40000 60000 80000 100000 120000 140000 160000 180000 200000 1x106 2x106 3x106 4x106 5x106 6x106 7x106 8x106 9x106Inverse Time (ms)Matrix ElementsUnencryptedEncrypted 0 500 1000 1500 2000 2500 3000 3500 40000 500k1M2M2M2M3M4M4M5MTime (ms)Number of elementsBitonic SortRandomASCDESC 0 200000 400000 600000 800000 1x106 1.2x106 1.4x106 0 5x106 1x107 1.5x107 2x107 2.5x107Matrix Multiplication Time (ms)Matrix ElementsUnencryptedEncrypted 150000 200000 250000 300000 350000 400000 450000 600000 800000 1x106 1.2x106 1.4x106 1.6x106 1.8x106Matrix Multiplication Time (ms)Block ElementsUnencryptedEncryptedExecution Time 100 200 300 400 500 100 200 300 400 500 18000 18400 18800 19200 19600 20000Matrix Multiplication Time (ms) 150000 200000 250000 300000 350000 400000 450000 500000 550000 600000 650000 700000 750000 800000 850000 900000 950000 1x106Join time (ms)Matrix ElementsUnencryptedEncrypted 35 40 45 50 55 60 65 70 75 1x106 1.1x106 1.2x106 1.3x106 1.4x106 1.5x106Selection time (ms)Matrix ElementsUnencryptedEncrypted 20 25 30 35 40 45 50 1x106 1.1x106 1.2x106 1.3x106 1.4x106 1.5x106Aggregation time (ms)Matrix ElementsUnencryptedEncryptedcan leak the size of the loop and iteration count of the loop. Further-
more, the program uses a constant, i.e., the damping factor, which
can be leaked too. On a side note, if a user needs to hide a value, our
current implementation requires the user to input it as data rather
a hard-coded constant in the program. Specifically, for Wiki-Votes
example, an adversary can know that the user is performing a se-
quence of operations over a matrix of size m×m and output another
m × 1 matrix, where m = 7, 115 and the sequence of operations are
load, assign, assign, rand, norm, scalar, scalar, sub, div, ones, scalar,
element_wise, loop, multiply, and publish. The adversary can also
observe the size of input output of each operation. In addition, the
adversary can also observe the block size used in each operation. In
summary, the adversary can only infer that PageRank is executed
over a m × m matrix, and nothing else.
Join oblivious vs. non-oblivious. We test the overhead of oblivi-
ousness in SQL JOIN query. We take the Big Data Benchmark [3]
from AMP Lab and run a join query SELECT * FROM Ranking
r JOIN UserVisits uv (20) ON (r.pageURL = uv.destURL)
in oblivious and non oblivious mode for the small version of the
dataset, where Ranking table contains 1, 200 rows and 3 columns
and UserVisits table contains 10, 000 rows and 9 columns. We
observe that the non-oblivious version takes 3min 46.3sec and the
oblivious version takes 24min 12.47sec. The main reason behind
the oblivious version being slower is that the value of K (i.e., the
intermediate join size upper limit) is relatively high. In general,
for join operation the overhead in oblivious version is mainly con-
trolled by the parameter K. In this setting, an adversary can only
infer the input size and the value of K, nothing else.
Comparison with a SMC Implementation. Finally, for the sake
of completeness, we also compare our result with a popular multi-
party computation programming abstraction ObliVM [41]. Here
we perform matrix multiplication for varying size matrices using
ObliVM generated code and our BigMatrix construct. As expected,
we observe that the ObliVM takes significant amount of time com-
pared to our solution with Intel SGX in Table 3. A solution using
traditional multi-party circuit evaluation technique will always
incur high overhead compared to a hardware assisted solution, be-
cause of the intensive communication and complex cryptographic
operations. Due to the huge performance difference, we did not
conduct more complex comparisons involving ObliVM.
Matrix
Dimension
ObliVM
BigMatrix
SGX Enc.
BigMatrix
SGX Unenc.
100
250
500