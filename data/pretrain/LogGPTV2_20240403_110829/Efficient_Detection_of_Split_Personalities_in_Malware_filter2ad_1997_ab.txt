bis) to be part of the analysis environment. Thus, our
system also recognizes malware that checks for the pres-
ence of Qemu. The behavior of a program is deﬁned as
the system calls (types and arguments) that this program
invokes. We believe that this is a reasonable assumption,
because system calls are the means through which a pro-
gram communicates with its environment and by which
it can cause persistent changes to the operating system or
other hosts (via the network).
that a tool
that
It
is important
identiﬁes split-
personality malware work reliably and efﬁciently. We
strive for a system that has no false negatives or false pos-
itives. False negatives are more severe, since they imply
that the analysis failed to detect a sample with a split per-
sonality, possibly missing a severe threat. False positives
typically result in a performance loss, because malware
samples with split personalities induce additional analy-
sis effort. With regards to efﬁciency, we require that the
detector do not add substantial overhead to the current
Anubis analysis process. Currently, the Anubis analysis
environment comprises ten Qemu workers on two phys-
ical machines. Although the machines are fully utilized,
they still do not manage to process our daily sample feed.
Thus, any increase in overhead immediately translates to
a lower number of samples that can be analyzed.
Handling malware with split personalities. The sec-
ond step is to leverage the information collected dur-
ing the process of detecting the malware’s split per-
sonality to improve the analysis results. One possibil-
ity is to rerun malware samples with split personalities
in a transparent but costly analysis framework such as
Ether. In this setup, our proposed system acts as an efﬁ-
cient and reliable ﬁlter to detect those malware programs
that require additional analysis. Currently, this approach
would be practical, as the fraction of malware programs
that attempt to detect the analysis environment is quite
low. For example, in a previous work [7], we found
that less than one percent of the samples in the Anubis
database execute known checks to detect our analyzer
or are packed with executable protectors (such as Ar-
madillo and tElock) that are known to recognize or fail
on Qemu. This is consistent with the ﬁndings in a re-
lated study [11], in which the authors show that about
4% of malware samples behave differently when run in-
side a virtual machine. However, as the fraction of split-
personality malware rises, alternative solutions are re-
quired. A promising venue is to carry out additional
analysis to identify the root cause for deviating behav-
ior. Once this root cause (likely, the malware check) is
found, the analysis environment can be adapted to auto-
matically bypass such checks. An emulation technique
for “emulation-resistant” programs, which follows this
general approach, was described in [22].
3 Our Approach
In this section, we discuss our approach for determin-
ing whether a given malware program has split personal-
ities. As mentioned previously, this approach is based on
the basic idea of comparing the execution of the malware
on a reference system with the execution on the analy-
sis system. We will see that for our approach to be ef-
ﬁcient and reliable, we need to extend this basic idea
with a mechanism to log inputs passed to the malware
on the reference system (i.e., values read from the oper-
ating system), and to replay those inputs when running
the malware on the analysis system.
3.1 Efﬁcient Detection
Claim 1: The runtime behavior of a program can be
characterized by the sequence of the system calls it ex-
ecutes.
We justify this claim on the basis that system calls are
the mechanism through which a (user-mode) process in-
ﬂuences its environment (the operating system) as well
as external hosts (through the network). Thus, to capture
the kinds of actions that a malware is performing and that
it may be interested in concealing, we argue that it is suf-
ﬁcient to inspect the sequence of system calls that it exe-
cutes (of course, considering both the types of the system
calls and their parameter values). This view is consistent
with a large body of prior work that uses system calls
to model malware behavior or the effect of exploits on
legitimate processes.
On the basis of Claim 1, comparing the behavior of a
program on two different systems boils down to check-
ing that its executions produce the same sequence of
system calls (same types and parameter values). Since
this comparison requires only coarse-grained analysis,
instead of more precise, but expensive, ﬁne-grained anal-
ysis of individual instructions, this analysis can be done
efﬁciently.
3.2 Reliable Detection
To discuss reliable detection of split-personality mal-
introduce the concept of execution-
ware, we ﬁrst
equivalence. We say that two systems are execution-
equivalent if all programs that (a) start from the same ini-
tial state (i.e., memory and registers are initialized with
the same values) and that (b) receive the same inputs on
both systems exhibit the same runtime behavior.
For this deﬁnition, we also assume that a program has
no race condition (that is, the results of the computation
are independent of the scheduling of individual threads).
We believe that this is a reasonable assumption for most
programs. Also, by violating the assumption, a mal-
ware author cannot bypass our detection. Instead, pro-
grams with race conditions might be incorrectly detected
as split-personality programs.
Claim 2: When the behavior of a program is different on
two execution-equivalent systems, this discrepancy is the
result of CPU semantics or timing attacks.
The fact that the behaviors of the same program on
two execution-equivalent systems are different implies
that the execution of one (or more) CPU instructions on
these systems yielded observable effects for the program,
and the program used these effects to follow a different
execution path. The effects can be caused by CPU in-
structions that either have a different semantics or differ-
ent timing properties on the two systems. This is pre-
cisely the characterization of CPU semantics and timing
attacks.
Of course, it is not true that CPU semantics or timing
attacks necessarily lead to different behaviors. That is,
it is possible that a malware program contains checks to
detect an analysis environment, but decides to ignore the
results of these checks. However, in this case, the analy-
sis is able to observe the entire behavior of the malware
binary, and hence, we do not consider such programs as
split-personality.
The astute reader might notice that Claim 2 speciﬁ-
cally focuses on CPU semantics and timing attacks, but
excludes environment attacks. The reason is that environ-
ment attacks cannot be detected as behavioral differences
when executing a malware binary on two execution-
equivalent systems. This is because environment attacks
analyze and branch on input values that are read from the
operating system. Since execution-equivalence implies
that the program receives identical input values on both
systems, the behavior of the program will be the same.
However, as we will see, the design of our system makes
the analysis environment transparent to these kinds of at-
tacks. Hence, our detection is concerned only with de-
tecting split-personality malware that performs CPU se-
mantics or timing attacks.
Claim 2 motivates the design of our detection ap-
proach. That is, by running a malware program on a ref-
erence system and an analysis system, any difference in
the observed behavior allows us to reliably convict this
malware as split-personality. Of course, this is only true
when the reference system and the analysis system are
execution-equivalent.
3.3 Making Systems Execution-Equivalent
Claim 3: To be able to reliably detect split-personality
malware by comparing its behavior on an analysis sys-
tem with its behavior on a reference system, these two
systems must be execution-equivalent.
To make two systems (such as our analysis and a ref-
erence system) execution-equivalent, we have to ensure
that programs start from the same initial state, and that
their inputs are identical. To provide identical starting
states, we use the same operating system environment for
both the analysis and the reference system. This guaran-
tees that the operating system components such as the
program loader and the runtime libraries (e.g., the Win-
dows API functions) are the same. Furthermore, we dis-
able any randomization mechanisms that could rearrange
the address space layout of a process at load time.
Ensuring that the inputs to both programs are iden-
tical is more complicated. While identical OS installa-
tions provide identical ﬁle system objects, running the
same program independently can lead to different behav-
iors even when the program has no split personality. As
an obvious example, the program could use input from
a remote host that returns different results for different
client connections. More subtle examples include pro-
grams that use the current time and system information
such as the current processor load. Thus, it is not possible
to simply execute a malware program on the reference
system and on the analysis system and expect that differ-
ent behaviors are reliable indicators for different person-
alities.
Our solution to provide identical inputs to a program
consists of recording the system calls of the program
that executes on the reference system, and later replay-
ing these system calls on the analysis system. That is, we
run the malware on the reference system in log mode. In
this mode, the sequence of system calls and all of its in
and out parameters are logged. Then, on the analysis sys-
tem, we run the malware in replay mode. In this mode,
whenever the program invokes a system call, we inter-
cept the call and retrieve the corresponding call from the
log trace. Then, instead of letting the OS handle the call,
we simply replay the logged system call, that is, we re-
turn control to the malware, after setting the return code
and all the out parameters to the corresponding values
observed during the log phase on the reference system.
For example, in our approach, when a program attempts
to read a ﬁle in replay mode, it is not given access to the
actual ﬁle stored on the disk of the analysis system. In-
stead, the program is provided with the content of the ﬁle
as it was read in log mode on the reference system.
An important advantage when replaying inputs
recorded on a reference system is that environment at-
tacks are not effective. That is, when the malware at-
tempts to access resources on the analysis system that
could reveal its presence,
the system replays the re-
sources that were present on the reference system. Thus,
during analysis, environments attacks will result in the
same behavior as on the reference system, effectively
making our analysis transparent to this attack vector.
Unfortunately it is not possible to simply replay all
system calls that the malware program invokes on the
analysis system. In fact, there are a number of system
calls that are not safe to replay. For example, if we re-
played the return value of a system call that allocates new
memory by simply returning the address of the memory
buffer that was allocated on the references system, the
program would likely crash when accessing this mem-
ory. The reason is that the operating system has not cre-
ated the necessary virtual memory mappings internally,
while the program assumes that memory was correctly
reserved. As a result, the access results in a page fault.
Thus, the replay component replays values only for those
system calls that read data from the environment. The in-
put channels we consider are the ﬁle system, the registry,
the network, and time computations. This ensures that
system calls that obtain input values from the environ-
ment receive the proper data recorded on the reference
system. Other system calls that are used for management
purposes (such as system calls for allocating memory,
spawning threads, etc.) are monitored but passed directly
to the underlying OS.
3.4 System Call Matching
Assume that we have a system call trace that captures
the behavior of a malware program on the reference sys-
tem. We then execute the malware on the analysis sys-
tem. For each system call that is observed, we can check
whether the type and the in arguments of this call match
the one in the log. If this is the case, we replay the return
value and the out arguments. When the type of the ob-
served system call or its arguments are different, we have
identiﬁed a deviation in the expected behavior and, thus,
can mark the malware as having a split personality.
Unfortunately, the situation is not that easy in prac-
tice. The reason is that small timing differences can
cause small, temporary deviations in the behavior of a
process that is replayed. For example, a little delay in
the delivery of an interrupt can cause a process to issue
additional system calls. More concretely, consider the
WaitForSingleObject function, which waits until
the speciﬁed object is ready to be accessed or until a
timeout expires. This function is often called in a loop
that spins until an operating system object is ready. De-
pending on the time it takes until the object is ready, it
is possible that the program has executed more (or less)
invocations on the reference system than in the analysis
environment. Another example would be the delivery of
a signal that could lead to the execution of a system call
at slightly different points along the execution trace of a
process.
The previously-outlined deviations result in slightly
different system call traces. However, these changes do
not result in any differences with regards to the actual
malware behavior. That is, the persistent changes (out-
puts) that the program produces are still identical. To
handle these cases, we have to slightly relax our deﬁni-
tion of equal behavior. More precisely, we do not require
that the sequences of system calls produced in log and
replay mode are exactly the same, but we allow some
ﬂexibility to account for small differences. This ﬂexi-
ble matching approach is based on the observation that
small differences are usually localized in time and tend
to quickly disappear as the program continues execution.
# check for deviation
if len(buf_skipped) == L or len(buf_extra) == L:
deviation_detected()
# the next syscall in the log matches
if pos == 0:
return candidate_syscall
# get the next matching syscall from the log,
# -1 if none
pos, candidate_syscall =
get_next_matching_syscall(log, cur_syscall)
# expire old entries in the buffers
# (if they aren’t "write" operations)
expire(buf_skipped)
expire(buf_extra)
1 buf_skipped = []
2 buf_extra = []
3 def flexible_syscall_match(log, curr_syscall):
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
# if found a match in the log
# but not in buf_skipped, add to skip buffer
if pos > 0:
# if no match or extra syscalls,
# search a match in the skip buffer
if pos == -1 or pos > 0:
for (i = 0; i < pos; i++):
buf_skipped.append(log[i])
return candidate_syscall
s_pos, s_candidate_syscall =
get_next_matching_syscall(buf_skipped,
# no match: add to extra bucket
buf_extra.append(candidate_syscall)