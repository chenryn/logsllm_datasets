certiﬁcate sufﬁces to guarantee that there is no other block
that can be generated in this position on the blockchain.
2) Extending the Blockchain: The system starts with a gen-
esis block containing the initial members of the consortium,
their public keys, and other setup data. Every time a batch
of transactions is delivered in total order and executed by the
blockchain application, a new block is created containing the
batch itself and the results of each transaction. This can be
seen in blocks 1, k, and m in Figure 2.
3) State Checkpoints: In order to accelerate the launching
of new consortium members or decrease the time to re-
pair crashed replicas, SMARTCHAIN employs durable check-
points, stored outside the blockchain. A checkpoint contains
a snapshot of the application state and a reference to the last
block covered by it (block k in Figure 2), i.e., the most recent
block whose transactions were executed before the snapshot
3Results can include a compact representation (e.g., a Merkle tree) of the
state changes caused by the transactions, making SMaRtChain compatible
with execution engines like the Ethereum Virtual Machine, as in SBFT [20].
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:27:46 UTC from IEEE Xplore.  Restrictions apply. 
429
was taken. This means that a checkpoint makes the blocks
before it mostly obsoletes for starting a replica.
SMARTCHAIN requires a checkpoint to be created after
a sequence of z blocks are processed. The parameter z is
deﬁned in the genesis block. This is different from traditional
SMR systems, in which the checkpoint is deﬁned based on
the number of transactions executed. We changed it to blocks
to avoid having checkpoints that partially cover a block.
Each block b stores the number c of the last block for which
its transactions were included in the most recent checkpoint
at the time b was created. This is important to inform anyone
reading the blockchain that
that
represents the state of the system up to block c (inclusive).
there is a state snapshot
4) Consortium Changes: A fundamental characteristic of
permissioned blockchains is that members of the consortium
know each other. A simple way to do that is by storing the
current composition of the consortium on the blockchain.
Our blockchain structure accommodates that in two ways.
First, by storing the initial consortium composition in the gene-
sis block. Second, by storing the transaction that reconﬁgures
the system and the corresponding new view, in a separated
reconﬁguration block (see block l in Figure 2). Similarly to
the checkpoint approach, each block stores the number of the
last reconﬁguration block before it in the chain. This ensures
blockchain veriﬁers have access to enough public keys that
validate the certiﬁcate of each block created in the view.
C. Strengthening the Blockchain Persistence
As discussed before, BFT-SMART provides only external
durability, i.e., a transaction is irreversibly committed only if
its issuer sees matching replies from a quorum of replicas (see
Observation 2 in Section IV). This limitation also affects our
blockchain architecture if no changes are made.
Considering the deﬁnition of blockchain in terms of Persis-
tence and Liveness (Section II-A), this external durability is
equivalent to 1-Persistence, i.e., only the second to last block
is immutable. However, there are other possibilities:
● 0-Persistence: Perfect durability, once a block is written,
it is immutable.
● α-Persistence: Standard durability, with α being the num-
ber of consensus instances running in parallel
in the
system. BFT-SMART runs a single consensus at time
(α = 1), as described before.
● λ-Persistence: Durability provided when using asyn-
chronous stable storage writes. The value of λ is depen-
dent on the environment but clearly a small integer greater
than zero.
● 6-Persistence: The durability provided (with high proba-
bility) in the Bitcoin’s blockchain [26].
● ∞-Persistence: No durability, provided when storing
blocks only in memory.
In this paper we are particularly interested in achieving
0-Persistence, a guarantee similar to the durability provided
by most database systems. To do that, we need an additional
communication step on the system, just after the transactions
430
(cid:10)(cid:24)(cid:31)(cid:40)(cid:23)(cid:30)(cid:27)
(cid:6)(cid:22)(cid:29)(cid:48)(cid:27)
(cid:10)(cid:19)(cid:30)(cid:23)
(cid:10)(cid:11)(cid:8)(cid:12)(cid:9)(cid:13)(cid:8)
(cid:34)(cid:17)(cid:29)(cid:14)(cid:26)(cid:13)(cid:8)
(cid:1)(cid:8)(cid:14)(cid:26)(cid:28)(cid:19)
(cid:4)(cid:3)(cid:7)(cid:10)(cid:8)(cid:10)(cid:5)
(cid:30)(cid:7)(cid:19)(cid:6)(cid:12)(cid:31)(cid:17)
(cid:33)(cid:14)(cid:26)(cid:13)(cid:8)
(cid:5)(cid:40)(cid:24)(cid:29)(cid:18)(cid:27)(cid:40)(cid:31)(cid:49)(cid:22)(cid:31)(cid:27)
(cid:47)(cid:42)(cid:18)(cid:50)(cid:44)(cid:29)(cid:41)(cid:24)(cid:27)
(cid:2)
(cid:4)(cid:11)
(cid:4)(cid:12)
(cid:4)(cid:13)
(cid:4)(cid:16)
Fig. 3: SMARTCHAIN message pattern.
2
are executed and persisted. This extra round of communica-
tion – designated as PERSIST phase – consists in making
each replica generate its own signature of the block (which
will now include the aforementioned transaction results) and
disseminate these signatures among the view. Once a replica
collects ⌊ cv.n+cv.f +1
⌋ signatures for the same block, it appends
these signatures to the block, thus creating a certiﬁcate for it.
Notice that this write is asynchronous since if all replicas crash
after synchronously writing the header and body of a block,
when they recover the only possible next action is to create
the same certiﬁcate again.
This modiﬁcation ensures 0-Persistence because the block is
considered written only when a replica knows that a Byzantine
quorum of replicas executed and recorded the same set of
transactions to their stable storage. Consequently, even if all
replicas crash and recover,
these transactions will still be
visible in the blockchain.
SMARTCHAIN supports either 0- or1-Persistence, in vari-
ants we call weak and strong, respectively. Figure 3 illustrates
the message pattern of both variants. For both cases,
the
algorithm for state transfer is basically the same as used in
BFT-SMART (Section II-C2), sending the last checkpoint
covering up to a block b plus the blocks after it.
D. The Reconﬁguration Protocol
SMARTCHAIN provides a new reconﬁguration protocol
that does not rely on a trusted third party to manage re-
conﬁgurations, allowing replicas to join/leave the system in
an autonomous and secure way, following application-speciﬁc
conditions.
An important aspect related with reconﬁgurations is how to
avoid forks caused by faulty nodes removed from the system.
Recall that our assumption is that in each active view v, there is
at most v.f faulty nodes. However, we do not assume anything
about the nodes from past views. Figure 4 shows an example
where the failure thresholds of all views are respected, but in
which node 3, that is compromised after being removed from
the system, together with faulty nodes 2 and 4 (also removed),
is able to create a fork after block k − 1 by extending the
blockchain without the reconﬁguration block k.
In SMARTCHAIN, we solve this problem by decoupling
replicas permanent key pairs from their consensus key pairs,
which are used to create a consensus decision proof and
also to obtain a block certiﬁcate. The idea is to make all
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:27:46 UTC from IEEE Xplore.  Restrictions apply. 
k
...
Genesis
1
vinit = {1,2,3,4}
time
...
4 is faulty
k-1
k (rec)
k+1
v = {1,2,5,6}
2 is faulty
...
3 is faulty
Fig. 4: Fork created by malicious processes.
replicas generate new consensus key pairs for each view they
participate, certifying each generated public consensus key
with their permanent private keys, and discard their consensus
key pairs on each view change. This forgetting protocol [48]
ensures that even if a replica becomes faulty later, after a new
view is installed, it cannot recover the discarded consensus
private key and thus cannot vouch for a block in some old
view (as done by nodes 3 and 4 in the example).
The consensus public keys for a new view need to be stored
in the reconﬁguration block, together with the list of members
of the view. This requires the inclusion of these keys in the re-
conﬁguration transaction. However, to preserve reconﬁguration
liveness in non-synchronous systems, the processes handling
the reconﬁguration transaction(s) that will install a new view
v are ensured to collect at most v.n − v.f of such keys.
Fortunately, this quorum is enough for avoiding forks since, in
the worst case, it will contains v.f keys from faulty processes
and a collusion with the v.f processes whose keys were
not included in the reconﬁguration block (that can become
malicious later) will not be enough to generate a valid proof
for a consensus decision or to certify a block, which requires
⌊ cv.n+cv.f +1
⌋ signatures. It is worth to mention that correct
processes whose keys are not included in the reconﬁguration
block but that participate in the view need also to forget old
keys and generate new ones. These new keys are disseminated
in the ﬁrst messages these processes send in the new view.
2
Concretely, for a new node to join the system the following
steps need to be executed (Figure 5a): (1) it asks the nodes
in cv for a permission to join the system; (2) each node
may accept or not the request based on an application-speciﬁc
policy (e.g., the new node is certiﬁed by a trusted third party,
it solved a proof-of-work, or it acquired a certain amount of
the blockchain-speciﬁc cryptocurrency), by sending a signed
reply to the joining node which also contains its new public
key to be used in the next view; (3) if the joining node receives
signed acceptance replies from a quorum of cv.n − cv.f nodes
in cv, it assembles a certiﬁcate and invokes a reconﬁguration
transaction that goes through the ordering protocols. After this
join transaction is executed and the new node is included in
the current view, its state is updated as previously described.
If a node decides to leave the system by itself, it collects
public keys for a new view without itself from a quorum of
nodes and notiﬁes the others by submitting a special leave
transaction in total order. Once a node receives this transaction,
it generates a new view with that node excluded from the
group. On the other hand, if the group decides to remove some
node from the system, each node submits a special remove
(cid:4)(cid:32)
(cid:4)(cid:11)
(cid:4)(cid:12)
(cid:4)(cid:13)
(cid:4)(cid:16)
(cid:4)(cid:11)
(cid:4)(cid:12)
(cid:4)(cid:13)
(cid:4)(cid:16)
(cid:1)(cid:5)(cid:13)(cid:8)
(cid:10)(cid:2)(cid:8)(cid:6)(cid:20)(cid:16)(cid:2)(cid:11)(cid:3)(cid:9)(cid:11)(cid:16)
(cid:27)(cid:28)(cid:20)(cid:8)(cid:24)(cid:22)(cid:6)(cid:13)(cid:8)
(cid:5)(cid:8)(cid:6)(cid:8)(cid:9)
(cid:10)(cid:11)(cid:6)(cid:12)(cid:13)(cid:14)(cid:9)(cid:11)
(cid:15)(cid:8)(cid:2)(cid:16)(cid:17)(cid:18)(cid:19)
(a) Join message pattern.
(cid:10)(cid:2)(cid:8)(cid:6)(cid:20)(cid:16)(cid:2)(cid:11)(cid:3)(cid:9)(cid:11)
(cid:27)(cid:28)(cid:20)(cid:8)(cid:24)(cid:22)(cid:6)(cid:13)(cid:8)
(cid:3)(cid:34)(cid:2)(cid:35)(cid:36)(cid:37)(cid:3)
(b) Exclusion message pattern.
Fig. 5: SMARTCHAIN reconﬁguration protocol.
transaction to the ordering protocol asking for that exclusion
and informing its public key for the new view (Figure 5b).
Once a node observes cv.n − cv.f of such transactions from
different nodes targeting the same node, it generates a new
view without that node. Notice that the overhead of requiring
all these transactions for running a single reconﬁguration will
be limited due to batching.
E. Consolidated Algorithm
Algorithm 1 consolidates all the previous ideas into a single
module to be run on top of the consensus layer. During initial-
ization, several variables are initialized and the genesis block
with all consensus public keys of the initial view is written to
stable storage (lines 1-10). Every time the ordering protocol
delivers a batch of transactions, they are stored together with
the respective consensus proofs (see Section II-C) to disk (line
18). The asyncWriteBC command denotes the action of
asynchronously writing data to the blockchain stored in disk.
Moreover, the transactions are delivered to the application
code for execution (line 19) and the results are also stored
to disk (line 20). This effectively creates the block’s body.
Since writing transactions to disk is done before executing
them, asynchronously, storage and execution are performed
in parallel. Finally, the header is written to close the block
(lines 21, 26-29), the replies are sent to the clients (lines 22-
23), it is veriﬁed if a snapshot of the service must be created
(line 24), and the blockchain becomes ready to receive the
next block (line 25).
Additionally,
in the strong variant,
the block certiﬁcate
is also created and stored in the block (lines 31-34). More
speciﬁcally, each replica sends a signed PERSIST message
with the hash of the block header to all replicas in cv.
Once a replica receives correctly signed PERSIST messages
from a quorum of replicas in cv, it creates a certiﬁcate that
authenticates the block and writes it to disk.
Membership updates are stored in their own blocks (lines
37-48). The algorithm presents the processing needed to
include or remove a process that asked to join or leave the
431
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:27:46 UTC from IEEE Xplore.  Restrictions apply. 
Algorithm 1: SMARTCHAIN Algorithms
1 Upon Init do
2
3
4
5
6
7
8
9
10
myId ← replica identiﬁer
bNum ← 1
lRec ← −1
lCkp ← −1
lbHash ← hash(∅)
lSnapshot ←⊥
cv ← vinit
resetCached()
writeGenesisBlock()
// replica identiﬁer
// next block number
// last reconﬁguration block number
// last checkpoint block number
// hash of the last block
// last state snapshot taken
// system current view
// resets the cached data
// writes the genesis block to disk
11 Procedure resetCached()
12
13
14
15
// transactions for each block i
∀i ∈ N ∶ Txs[i] ← ∅
∀i ∈ N ∶ Res[i] ← ∅ // responses for transactions on each block i
// certiﬁcates for each block i
∀i ∈ N ∶ Cert[i] ← ∅
// headers for each block i
∀i ∈ N ∶ Headers[i] ← ∅
16 Upon totalOrderDeliver ⟨BATCH, cid, txs[], proofs[]⟩ do
17
18
19
20
21
22
23
Txs[bNum] ← ⟨txs[], proofs[]⟩
asyncWriteBC(⟨cid, Txs[bNum]⟩)
Res[bNum] ← execute(Txs[bNum])
asyncWriteBC(Res[bNum])
closeBlock(⟨hash(Txs[bNum]), hash(Res[bNum])⟩)
foreach ⟨clientId, res⟩ ∈ Res[bNum] do
send ⟨REPLY, res⟩ to clientId
24
25
checkpoint()
bNum + +
26 Procedure closeBlock(htx, hres)
27
28
29
30
31
32
Headers[bNum] ← ⟨bNum, lRec, lCkp, htx , hres, lbHash⟩
asyncWriteBC(Headers[bNum])
syncDisk()
lbHash ← hash(Headers[bNum])
if STRONG PERSISTENCE
⟩ to cv
send ⟨PERSIST, bNum, ⟨myId, lbHash⟩σmyId
wait until ∣Cert[bNum]∣ ≥ ⌈ cv .n +cv .f +1
asyncWriteBC (Cert[bNum])