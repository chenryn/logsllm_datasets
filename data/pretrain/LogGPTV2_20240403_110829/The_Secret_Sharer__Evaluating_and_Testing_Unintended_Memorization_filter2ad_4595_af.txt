[24] Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize
better: Closing the generalization gap in large batch training of neural
networks. arXiv preprint arXiv:1705.08741, 2017.
[25] I Hubara, M Courbariaux, D Soudry, R El-Yaniv, and Y Bengio. Quan-
tized neural networks: Training neural networks with low precision
weights and activations. arXiv preprint arXiv:1609.07061, 2016.
[26] Bargav Jayaraman and David Evans. Evaluating differentially private
machine learning in practice. In USENIX Security Symposium, 2019.
[27] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic
optimization. ICLR, 2015.
[28] Anders Krogh and John A Hertz. A simple weight decay can improve
generalization. In NIPS, pages 950–957, 1992.
[29] Paul Lambert.
Write emails faster with SmartCompose in
https://www.blog.google/products/gmail/subject-
Gmail.
write-emails-faster-smart-compose-gmail/.
[30] Yunhui Long, Vincent Bindschaedler, and Carl A Gunter. Towards
measuring membership privacy. arXiv preprint 1712.09136, 2017.
[31] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini.
Building a large annotated corpus of English: The Penn Treebank.
Computational linguistics, 19(2):313–330, 1993.
[32] Frank J Massey Jr. The Kolmogorov-Smirnov test for goodness of ﬁt.
Journal of the American statistical Association, 46(253), 1951.
[33] Joseph Menn. Amazon posts a tell-all of buying lists. 1999. Los Ange-
les Times. https://www.latimes.com/archives/la-xpm-1999-
aug-26-fi-3760-story.html.
[34] Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Reg-
ularizing and optimizing LSTM language models. arXiv preprint
arXiv:1708.02182, 2017.
[35] Stephen Merity, Nitish Shirish Keskar, and Richard Socher. An anal-
ysis of neural language modeling at multiple scales. arXiv preprint
arXiv:1803.08240, 2018.
[36] Tomas Mikolov, Martin Karaﬁát, Lukas Burget, Jan Cernock`y, and
Sanjeev Khudanpur. Recurrent neural network based language model.
In Interspeech, volume 2, page 3, 2010.
[37] Milad Nasr, Reza Shokri, and Amir Houmansadr. Machine learning
In Pro-
with membership privacy using adversarial regularization.
ceedings of the 2018 ACM SIGSAC Conference on Computer and
Communications Security, pages 634–646. ACM, 2018.
[38] Seong Joon Oh, Max Augustin, Mario Fritz, and Bernt Schiele. Towards
reverse-engineering black-box neural networks. In ICLR, 2018.
[39] A O’hagan and Tom Leonard. Bayes estimation subject to uncertainty
about parameter constraints. Biometrika, 63(1), 1976.
[40] Le Trieu Phong, Yoshinori Aono, Takuya Hayashi, Lihua Wang, and
Shiho Moriai. Privacy-preserving deep learning: Revisited and en-
hanced. In International Conference on Applications and Techniques
in Information Security, pages 100–110. Springer, 2017.
[41] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov.
Membership inference attacks against machine learning models. In
IEEE Symposium on Security and Privacy, 2017.
[42] Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep
neural networks via information. arXiv preprint 1703.00810, 2017.
[43] Congzheng Song, Thomas Ristenpart, and Vitaly Shmatikov. Machine
learning models that remember too much. In ACM CCS, 2017.
[44] Congzheng Song and Vitaly Shmatikov. The natural auditor: How to
tell if someone used your words to train their model. arXiv preprint
arXiv:1811.00513, 2018.
[45] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever,
and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural
networks from overﬁtting. JMLR, 15(1):1929–1958, 2014.
[46] Igor V. Tetko, David J. Livingstone, and Alexander I. Luik. Neural
network studies. 1. Comparison of overﬁtting and overtraining. Journal
of Chemical Information and Computer Sciences, 35(5):826–833, 1995.
[47] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide
the gradient by a running average of its recent magnitude. COURSERA:
Neural networks for machine learning, 4(2):26–31, 2012.
[48] Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas
Ristenpart. Stealing machine learning models via prediction APIs. In
USENIX Security Symposium, pages 601–618, 2016.
[49] Stacey Truex, Ling Liu, Mehmet Emre Gursoy, Lei Yu, and Wenqi Wei.
Towards demystifying membership inference attacks. arXiv preprint
arXiv:1807.09173, 2018.
[50] Binghui Wang and Neil Zhenqiang Gong. Stealing hyperparameters in
machine learning. arXiv preprint arXiv:1802.05351, 2018.
[51] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad
Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao,
Klaus Macherey, et al. Google’s neural machine translation system:
Bridging the gap between human and machine translation. arXiv
preprint arXiv:1609.08144, 2016.
[52] Yuanshun Yao, Bimal Viswanath, Jenna Cryan, Haitao Zheng, and
Ben Y Zhao. Automated crowdturﬁng attacks and defenses in online
review systems. ACM CCS, 2017.
[53] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Pri-
vacy risk in machine learning: Analyzing the connection to overﬁtting.
In 2018 IEEE 31st Computer Security Foundations Symposium (CSF),
pages 268–282. IEEE, 2018.
[54] Yang You, Igor Gitman, and Boris Ginsburg. Scaling SGD batch size
to 32k for ImageNet training. arXiv preprint arXiv:1708.03888, 2017.
[55] Matthew D Zeiler. ADADELTA: An adaptive learning rate method.
arXiv preprint arXiv:1212.5701, 2012.
[56] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and
Oriol Vinyals. Understanding deep learning requires rethinking gener-
alization. ICLR, 2017.
A Additional Memorization Experiments
A.1 Across Different Architectures
We evaluate different neural network architectures in Table 4
again on the PTB dataset, and ﬁnd that all of them uninten-
tionally memorize. We observe that the two recurrent neural
networks, i.e., LSTM [23] and GRU [8], demonstrate both
the highest accuracy (lowest loss) and the highest exposure.
Convolutional neural networks’ accuracy and exposure are
both lower. Therefore, through this experiment, we show that
the memorization is not only an issue to one particular archi-
tecture, but appears to be common to neural networks.
A.2 Across Training Strategies
There are various settings for training strategies and tech-
niques that are known to impact the accuracy of the ﬁnal
model. We brieﬂy evaluate the impact that each of these have
on the exposure of the inserted canary.
Batch Size. In stochastic gradient descent, we train on mini-
batches of multiple examples simultaneously, and average
their gradients to update the model parameters. This is usu-
ally done for computational efﬁciency—due to their parallel
nature, modern GPUs can evaluate a neural network on many
thousands of inputs simultaneously.
To evaluate the effect of the batch size on memorization,
we train our language model with different capacity (i.e., num-
ber of LSTM units) and batch size, ranging from 16 to 1024.
(At each batch size for each number of units, we train 10
USENIX Association
28th USENIX Security Symposium    283
Architecture Layers Units Test Loss Exposure
36
GRU
37
GRU
38
LSTM
LSTM
35
24
CNN
19
CNN
22
CNN
WaveNet
18
20
WaveNet
1.18
1.18
1.17
1.16
1.29
1.28
1.25
1.24
1.25
370
235
320
200
436
188
122
188
122
1
2
1
2
1
2
4
2
4
e
z
i
S
h
c
t
a
B
16
32
64
128
256
512
1024
50
1.7
4.0
4.8
9.9
12.3
14.2
15.7
100
4.3
6.2
11.7
14.0
21.0
21.8
23.2
150
6.9
14.4
19.2
25.9
26.4
30.8
26.7
Number of LSTM Units
200
9.0
14.1
18.9
32.5
28.8
26.0
27.0
250
6.4
14.6
21.3
35.4
31.2
26.0
24.4
Table 4: Exposure of a canary for various model architec-
tures. All models have 620K (+/- 5K) parameters and so have
the same theoretical capacity. Convolutional neural networks
(CNN/WaveNet) perform less well at the language modeling
task, and memorize the canary to a lesser extent.
Table 5: Exposure of models trained with varying model sizes
and batch sizes. Models of the same size trained for the same
number of epochs and reached similar test loss. Larger batch
sizes, and larger models, both increase the amount of mem-
orization. The largest memorization in each column is high-
lighted in italics bold, the second largest in bold.
models and average the results.) All models with the same
number of units reach nearly identical ﬁnal training loss and
testing loss. However, the models with larger batch size ex-
hibit signiﬁcantly more memorization, as shown in Table 5.
This experiment provides additional evidence for prior work
which has argued that using a smaller batch size yields more
generalizable models [24]; however we ensure that all models
reach the same ﬁnal accuracy.
While this does give a method of reducing memorization
for some models, it unfortunately comes at a signiﬁcant cost:
training with a small batch can be prohibitively slow, as it
may prevent parallelizing training across GPUs (and servers,
in a decentralized fashion).6
Shufﬂing, Bagging, and Optimization Method. Given a
ﬁxed batch-size, we examine how other choices impact mem-
orization. We train our model with different optimizers: SGD,
Momentum SGD, RMSprop [47], Adagrad [13], Adadelta
[55], and Adam [27]; and with either shufﬂing, or bagging
(where minibatches are sampled with replacement).
Not all models converge to the same ﬁnal test accuracy.
However, when we control for the ﬁnal test accuracy by taking
a checkpoint from an earlier epoch from those models that
perform better, we found no statistically signiﬁcant difference
in the exposure of the canary with any of these settings; we
therefore do not include these results.
A.3 Across Formats and Context
We ﬁnd that the context we are aware of affects our ability to
detect whether or not memorization has occurred.
In our earlier experiments we computed exposure with the
preﬁx “The random number is” and then placing the random-
6Recent work has begun using even larger batch sizes (e.g., 32K) to train
models orders of magnitude more quickly than previously possible [21, 54].
Format
(cid:104)s(cid:105)
(cid:104)s(cid:105)
-
-
-
-
-
-
(cid:104)e(cid:105)
(cid:104)e(cid:105)
-
-
-
-
Exposure at Epoch
10
6.1
7.1
6.8
7.5
9.5
11.1
5
5.0
6.3
5.0
6.1
5.1
5.2
Table 6: Exposure of canaries when the we are aware of differ-
ent amounts of surrounding context ((cid:104)s(cid:105) and (cid:104)e(cid:105) are in practice
unique context characters of ﬁve random characters). The ex-
posure is computed at epoch 5 and 10, before the models
completely memorize the inserted canary.
ness as a sufﬁx. What if instead we knew a sufﬁx, and the
randomness was a preﬁx? Alternatively, what if the random-
ness had a unique structure (e.g., SSNs have dashes)?
We ﬁnd that the answer is yes: additional knowledge about
the format of the canary increases our ability to detect it was
memorized. To show this, we study different insertion formats,
along with the exposure of the given canary after 5 and 10
epochs of training in Table 6, averaged across ten models
trained with each of the formats.
For the ﬁrst four rows of Table 6, we use the same model,
but compute the exposure using different levels of context.
This ensures that it is only our ability to detect exposure that
changes. For the remaining two rows, because the format has
changed, we train separate models. We ﬁnd that increasing the
available context also increases the exposure, especially when
inner context is available; this additional context becomes
increasingly important as training proceeds.
284    28th USENIX Security Symposium
USENIX Association