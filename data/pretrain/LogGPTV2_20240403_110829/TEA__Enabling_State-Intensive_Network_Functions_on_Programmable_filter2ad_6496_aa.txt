title:TEA: Enabling State-Intensive Network Functions on Programmable
Switches
author:Daehyeok Kim and
Zaoxing Liu and
Yibo Zhu and
Changhoon Kim and
Jeongkeun Lee and
Vyas Sekar and
Srinivasan Seshan
TEA: Enabling State-Intensive Network Functions on
Programmable Switches
Yibo Zhu
Zaoxing Liu
ByteDance Inc.
Carnegie Mellon University
and Boston University
Daehyeok Kim
Carnegie Mellon University
and Microsoft Research
Changhoon Kim
Intel, Barefoot Switch
Division
Jeongkeun Lee
Intel, Barefoot Switch
Division
Vyas Sekar
Carnegie Mellon University
Srinivasan Seshan
Carnegie Mellon University
Abstract
Programmable switches have been touted as an attractive alterna-
tive for deploying network functions (NFs) such as network address
translators (NATs), load balancers, and firewalls. However, their
limited memory capacity has been a major stumbling block that has
stymied their adoption for supporting state-intensive NFs such as
cloud-scale NATs and load balancers that maintain millions of flow-
table entries. In this paper, we explore a new approach that leverages
DRAM on servers available in typical NFV clusters. Our new system
architecture, called TEA (Table Extension Architecture), provides a
virtual table abstraction that allows NFs on programmable switches
to look up large virtual tables built on external DRAM. Our ap-
proach enables switch ASICs to access external DRAM purely in
the data plane without involving CPUs on servers. We address
key design and implementation challenges in realizing this idea.
We demonstrate its feasibility and practicality with our implemen-
tation on a Tofino-based programmable switch. Our evaluation
shows that NFs built with TEA can look up table entries on exter-
nal DRAM with low and predictable latency (1.8–2.2 µs) and the
lookup throughput can be linearly scaled with additional servers
(138 million lookups per seconds with 8 servers).
CCS Concepts
• Networks → Programmable networks; In-network process-
ing; • Hardware → Emerging technologies.
Keywords
Programmable switches, Programmable networks, Data centers,
Remote Direct Memory Access, Network Function Virtualization
ACM Reference Format:
Daehyeok Kim, Zaoxing Liu, Yibo Zhu, Changhoon Kim, Jeongkeun Lee,
Vyas Sekar, and Srinivasan Seshan. 2020. TEA: Enabling State-Intensive
Network Functions on Programmable Switches. In Annual conference of
the ACM Special Interest Group on Data Communication on the applications,
technologies, architectures, and protocols for computer communication (SIG-
COMM ’20), August 10–14, 2020, Virtual Event, NY, USA. ACM, New York,
NY, USA, 17 pages. https://doi.org/10.1145/3387514.3405855
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-7955-7/20/08.
https://doi.org/10.1145/3387514.3405855
1 Introduction
Network functions (NFs) are an essential component in today’s on-
line service infrastructure. They are deployed on the critical path
of the infrastructure (e.g., at the front-end) where a large volume
of traffic with many concurrent flows needs to be handled. This
requires NFs to be scaled for overall network operations.
NFs have been traditionally deployed either using standalone
hardware appliances or a cluster of commodity servers (also known
as network function virtualization (NFV)) [29, 59]. More recently,
another approach has been gaining attention in the community:
NFs implemented on programmable switch ASICs (e.g., [5, 16, 53]).
However, we find that none of these approaches can handle
NFs when there is a combination of a large number of concurrent
flows (e.g., O(10M)) and a very high traffic rate (e.g., > 1 Tbps). A
programmable switch ASIC cannot serve a large number of concur-
rent flows that requires a large flow table due to its small on-chip
SRAM space although it has enough capacity to process a very high
traffic rate. Similarly, it requires several tens of hardware appli-
ances or hundreds of servers to handle the high-traffic rate, which
significantly increases operational cost.
We observe that the limited on-chip SRAM space is a key bot-
tleneck for programmable switch ASICs. If we could enable the
switch ASICs to store lookup tables on cheaper DRAM in a scalable
way, it could be a new enabler to serve a broader set of operating
regimes, which are defined by workloads and operating conditions
(i.e., traffic rate and the number of concurrent flows that NFs have
to process), cost-efficiently. In this paper, we envision a new system
architecture called TEA (Table Extension Architecture) that enables
the switch ASICs on the top of racks in an NFV cluster to leverage
DRAM on commodity servers.
While using server DRAM is an appealing low-cost and scal-
able solution, accessing server DRAM is inherently slower than
accessing on-chip SRAM. As we discuss in §3.1, without careful
design, this can significantly degrade processing performance and
availability of NFs. Indeed there are several technical challenges in
realizing this vision in practice:1
• First, for external DRAM access, while RDMA (Remote Directly
Memory Access) looks a promising solution, it is unclear how
to do RDMA from the switch ASIC without modifying it. Our
insight is that by leveraging the programmability of ASIC, we
1Our recent position paper proposes this high-level idea [47]. However, that work fails
to tackle these technical challenges and falls short of providing a concrete proof-of-
concept realizing the architecture.
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Daehyeok Kim et al.
can implement a subset of the RDMA protocol that suffices for
our rack-scale deployment model in NFV clusters.
• Second, since each external DRAM access incurs high latency (a
few µs), TEA must complete table lookups in a single-round trip
to DRAM and must continue processing other packets. At first
glance, it would seem that conventional cuckoo hashing [58]
would suffice. However, cuckoo hashing is not suitable for ex-
ternal DRAM because it can require multiple memory accesses
at times. Fortuitously, we find that bounded linear probing [67],
a design originally created for improving cache hit rates, can
be a basis for enabling table lookups guaranteed to complete
in a single round trip. In addition, we adapt this data structure
to provide temporary storage to support our deferred packet
processing needs.
• Third, to support NFs that require several hundred million
lookups per second, we need mechanisms to leverage the avail-
able DRAM and DRAM-access bandwidth across multiple servers.
While traditional distributed hashing schemes (e.g., consistent
hashing [43]) help scale out the lookup throughput by dis-
tributing table entries and balancing lookup request load across
servers, we observe that they consume too many ASIC resources.
We show that simpler, resource-efficient hashing schemes, com-
bined with a small on-chip SRAM cache, can address both the
load balancing and scaling requirements.
• Lastly, for high availability, one may detect servers’ availability
changes (due to server failures or congested link) in the control
plane, but it could take several milliseconds to make the data
plane react to it, degrading overall performance. We demon-
strate that it is possible to repurpose existing ASIC’s features to
support rapid failure detection and fail-over in the data plane.
TEA provides a virtual table abstraction for lookup tables stored
across the combination of on-chip SRAM and external DRAM, creat-
ing the illusion of large, high-performance tables to NFs. Our focus
is on NFs such as L4 load balancers, firewalls, NATs, VXLAN or VPN
gateways that are compute-light and state-heavy. Developers can
write such NFs using a library of TEA APIs implemented in P4 [17]
which is a programming language for programmable switches. We
expose the APIs as modularized P4 codes so that developers can
easily integrate TEA with their NF implementations.
We implement a prototype of TEA in P4 and four canonical
NFs using the TEA API. We evaluate it with microbenchmarks as
well as NF benchmarks in our testbed consisting of a Tofino-based
programmable switch and 12 commodity servers. Our evaluations
show that TEA allows NFs running on the switch to look up ta-
ble entries with low and predictable latency (1.8–2.2 µs), and the
throughput can be scaled linearly by recruiting more servers (138
million lookups per second with 8 servers in our testbed). Compared
to server-based NFs with a single server, TEA-based NFs achieve up
to 9.6× higher throughput and 3.1× lower latency without consum-
ing the CPUs and many ASIC resources. We also show that TEA
can react to server availability changes within a few microseconds.
2 Background and Motivation
NFs are deployed in many network settings, including inside the
cloud and at the edge. They perform a wide range of tasks, ranging
from packet filtering and load balancing to encryption and deep
Performance
Memory
Hardware
appliance
40 Gbps
O(10GB)
DRAM
>$40K
480W
Commodity
Server
10 Gbps
O(10GB)
DRAM
$3K
200W
Programmable
Switch
3.3 Tbps
O(10MB)
SRAM
$10K
620W
Price
Energy consumption
Table 1: Comparison of NF deployment options. We excerpt
the information from product briefs [4, 7, 13] and prior
work [53, 59].
packet inspection. In this paper, we focus on compute-light and state-
heavy NFs, such as L4 load balancers, firewalls, NATs, VXLAN or
VPN gateways. Even though NFs in this category are not compute
intensive, they still need to support a large volume of traffic and
concurrent flows on the critical path (e.g., at the front-end of the
cloud). Thus, their performance and scalability are the key for
overall network operations.
There are three typical options to realize such NFs today: (1)
using standalone hardware middlebox appliances, (2) implementing
them on a cluster of commodity servers (i.e., NFV cluster) [19,
29, 59], and (3) implementing them on emerging programmable
switches [7, 9]. We note that while there are other options such as
implementing NFs on FPGA boards attached to servers (e.g., [31]),
we consider the above three options that have been widely studied
and deployed today.
Network operators may choose different options by consider-
ing the performance, memory size, cost, and energy efficiency of
each option based on their workloads and operating conditions (i.e.,
traffic rate and the number of concurrent flow that NF instances
have to process). To understand which option is better in which sce-
nario, we analyze a canonical NF, load balancers, in four operational
regimes.2 Table 1 compares these options in terms of performance,
memory size, price, and energy consumption, and we use these
numbers in our analysis below.
Regime 1: Low traffic rate (1 Tbps) / Small number of con-
current flows (e.g., 100K flow and ≈1 MB per-flow state). In
2While our analysis focuses on a specific case of load balancers, these observations
also apply to other NFs such as firewalls, gateway functions, NATs, and ACLs.
TEA: Enabling State-Intensive Network Functions on Programmable Switches
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
this regime, using a programmable switch would be the most cost
and energy-efficient solution because the per-flow state can fit in its
SRAM space and it can easily serve the traffic. Hardware appliances
and commodity servers would require many nodes to support this
traffic rate making them very expensive (25 × $40K appliances vs.
100 × $3K servers vs. 1 × $10K switch).
Regime 4: High traffic rate (>1 Tbps) / Large number of con-
current flows (e.g., 10M flows and ≈100 MB per-flow state).
Many servers or appliances are required as the traffic rate increases
(e.g., 10 Tbps requires 1000 high-end servers, which costs $3M).
Although programmable switches can handle the traffic rate [7],
their limited memory makes it infeasible to support the needed flow
state. One could add more on-chip SRAM ($2-5K per GB) with chip
modification or more switches to address the memory limitation,
but costs would rise significantly.
In summary, our analysis suggests that: (1) servers and appliances
can handle the low-bandwidth regime effectively, (2) programmable