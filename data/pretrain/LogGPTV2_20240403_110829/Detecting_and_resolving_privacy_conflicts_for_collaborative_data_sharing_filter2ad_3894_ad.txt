rarely was the same as either of other two solutions. It further indi-
cates that our solution can always achieve a good tradeoff between
privacy protection and data sharing for privacy conﬂict resolution.
4.2.2 Evaluation of System Usability
Participants and Procedure: Retinue is a functional proof-of-concept
implementation of collaborative privacy management. To measure
the practicality and usability of our mechanism, we conducted a
survey study (n=30) to explore the factors surrounding users’ de-
sires for privacy controls such as those implemented in Retinue.
Particularly, we were interested in users’ perspectives on the cur-
rent Facebook privacy system and their desires for more control
over photos they do not own. We recruited participants through uni-
versity mailing lists and through Facebook itself using Facebook’s
built-in sharing API. Users were given the opportunity to share our
application and play with their friends. While this is not a ran-
Table 1: Usability Evaluation for Facebook and Retinue Privacy Controls.
Metric
Likability
Understanding
Control
Sharing with Trusted Users
Protecting from Untrusted Users
Average Upper bound on 95% Average Lower bound on 95%
0.39
0.33
0.36
0.30
conﬁdence interval
0.44
0.36
0.40
0.35
conﬁdence interval
0.72
0.65
0.66
0.70
0.74
0.69
0.69
0.71
Facebook
Retinue
dom sampling, recruiting using the natural dissemination features
of Facebook arguably gives an accurate proﬁle of the ecosystem.
In our user study (http://bit.ly/retinue_study), participants were
asked to ﬁrst answer some questions about their usage and percep-
tion of Facebook’s privacy controls. Users were then instructed to
install the application using their Facebook proﬁles and complete
the following actions: set privacy settings for a photo they do not
own, set privacy settings for a photo they own, and answer ques-
tions about their understanding. As users completed these actions,
they were asked questions on the usability of the controls in Ret-
inue.
User Study of Retinue: The criteria for usability evaluation were
split into three areas: likeability, understanding, and control. Like-
ability is simply a measure of a user’s basic opinion of a particular
feature or control. While this does not provide speciﬁc feedback
for improvement, it can help identify what aspects of sharing and
control are important to a user. Understanding is a measure of how
intuitive the concepts and controls are. This is tremendously useful
for improving the usability of controls. Control is a measure of the
user’s perceived control of their personal data. Control, in addition,
can be narrowed down into the areas of sharing with trusted users
and protecting from untrusted users. While this is not a deﬁnitive
measure of privacy, making a user feel safe is almost as impor-
tant as protecting a user. Questions were measured on a three- or
four-point scale (scaled from 0 to 1 for numerical analysis). For
measurement analysis, a higher number is used to indicate a posi-
tive opinion or perception, while a lower number is used to indicate
a negative one. We were interested in the average user perception
of the system, so we analyzed a 95% conﬁdence interval for the
users’ answers. This assumes the population to be mostly normal.
Before using Retinue, users were asked a few questions about
their usage of Facebook to determine the user’s perceived usability
of the current Facebook’s privacy controls. This included questions
on likeability (e.g. “indicate how much you like privacy features
for photos you are tagged in”), understanding (e.g. “indicate how
much you understand how to prevent certain people from seeing
photos I am tagged in”), and control (e.g. “indicate how in control
you feel when sharing photos I own with people I want to”). For our
conﬁdence interval, we were interested in determining the average
user’s maximum positive opinion of Facebook’s privacy controls,
so we looked at the upper bound of the conﬁdence interval.
An average user asserts at most 44% positively about the lika-
bility, 40% positively about sharing control, 35% positively about
protection control and 36% on their understanding of Facebook’s
privacy mechanisms as shown in Table 1. This demonstrates an av-
erage negative opinion of the Facebook’s privacy controls that users
currently must use.
After Using Retinue, users were then asked to perform a few
tasks in Retinue and were asked a few questions to determine the
users perceived usability of Retinue. This also included questions
on likeability (e.g. “indicate how much you like the trust level
feature”), understanding (e.g. “indicate your understanding of the
meaning of sharing loss”), and control (e.g. “please indicate how
in control you feel when sharing photos I own with the people I
want to”). For our conﬁdence interval, we were interested in deter-
mining the average user’s minimum positive opinion of Retinue’s
privacy controls, so we looked at the lower bound of the conﬁdence
interval.
An average user asserts at least 72% positively on likeability,
65% positively on understanding, 66% on sharing control and 70%
on protection control as shown in Table 1. This demonstrates an
average positive opinion of the controls and ideas presented to users
in Retinue.
5. RELATED WORK
Several proposals of an access control scheme for OSNs have
been introduced (e.g., [9, 10, 12, 13, 17]). Carminati et al. [9]
introduced a trust-based access control mechanism, which allows
the speciﬁcation of access rules for online resources where autho-
rized users are denoted in terms of the relationship type, depth,
and trust level between users in OSNs. They further presented a
semi-decentralized discretionary access control system and a re-
lated enforcement mechanism for controlled sharing of information
in OSNs [10]. Fong et al. [13] proposed an access control model
that formalizes and generalizes the access control mechanism im-
plemented in Facebook. Gates [11] described relationship-based
access control as one of the new security paradigms that addresses
the requirements of the Web 2.0. Then, Fong [12] recently for-
mulated this paradigm called a Relationship-Based Access Con-
trol (ReBAC) that bases authorization decisions on the relation-
ships between the resource owner and the resource accessor in an
OSN. However, none of these work could accommodate privacy
control requirements with respect to the collaborative data sharing
in OSNs.
Several recent work [7, 15, 18, 22, 24] recognized the need of
joint management for data sharing, especially photo sharing, in
OSNs. In particular, Squicciarini et al. [22] proposed a solution for
collective privacy management for photo sharing in OSNs. This
work considered the privacy control of a content that is co-owned
by multiple users in an OSN, such that each co-owner may sepa-
rately specify her/his own privacy preference for the shared con-
tent. The Clarke-Tax mechanism was adopted to enable the collec-
tive enforcement for shared content. Game theory was applied to
evaluate the scheme. However, a general drawback of this solution
is the usability issue, as it could be very hard for ordinary OSN
users to comprehend the Clarke-Tax mechanism and specify ap-
propriate bid values for auctions. In addition, the auction process
adopted in their approach indicates only the winning bids could
determine who was able to access the data, instead of accommo-
dating all stakeholders’ privacy preferences. In contrast, our work
proposes a simple but ﬂexible mechanism for collaborative man-
agement of shared data in OSNs. In particular, we introduce an
effective conﬂict resolution solution, which makes a tradeoff be-
tween privacy protection and data sharing considering the privacy
concerns from multiple associated users.
Measuring privacy risk in OSNs has been addressed recently by
several work [6, 20, 23]. Becker et al. [6] presented PrivAware, a
tool to detect and report unintended information loss through quan-
tifying privacy risk associated with friend relationship in OSNs.
In [23], Talukder et al. discussed a privacy protection tool, called
Privometer, which can measure the risk of potential privacy leak-
age cased by malicious applications installed in the user’s friend
proﬁles and suggest self-sanitization actions to lessen this leakage
accordingly. Liu et al. [20] proposed a framework to compute the
privacy score of a user, indicating the user’s potential risk caused
by her/his participation in OSNs. Their solution also focused on the
privacy settings of users with respect to their proﬁle items. Com-
pared with those existing work, our approach measures the privacy
risk caused by different privacy concerns from multiple users, cov-
ering proﬁle sharing, friendship sharing, as well as content sharing
in OSNs.
6. CONCLUSION
In this paper, we have proposed a novel solution for privacy
conﬂict detection and resolution for collaborative data sharing in
OSNs. Our conﬂict resolution mechanism considers privacy-sharing
tradeoff by quantifying privacy risk and sharing loss. Also, we have
described a proof-of-concept implementation of our solution called
Retinue, along with the extensive evaluation of our approach. As
part of future work, we will formulate a comprehensive access con-
trol model to capture the essence of collaborative authorization re-
quirements for data sharing in OSNs. Also, we would extend our
work to address security and privacy challenges for emerging in-
formation sharing services such as location sharing [1] and other
social network platforms such as Google+ [5].
Acknowledgments
This work was partially supported by the grants from National Sci-
ence Foundation (NSF-IIS-0900970 and NSF-CNS-0831360) and
Department of Energy (DE-SC0004308).
7. REFERENCES
[1] Facebook Places.
http://www.facebook.com/places/.
[2] Facebook Privacy Policy.
http://www.facebook.com/policy.php/.
[3] Facebook Statistics. http://http://www.facebook.
com/press/info.php?statistics.
[4] Google+ Privacy Policy. http://http:
//www.google.com/intl/en/+/policy/.
[5] The Google+ Project. https://plus.google.com.
[6] J. Becker and H. Chen. Measuring privacy risk in online
social networks. In Proceedings of the 2009 Workshop on
Web, volume 2. Citeseer.
[7] A. Besmer and H. Richter Lipford. Moving beyond
untagging: Photo privacy in a tagged world. In Proceedings
of the 28th international conference on Human factors in
computing systems, pages 1563–1572. ACM, 2010.
[8] J. Brickell and V. Shmatikov. The cost of privacy:
destruction of data-mining utility in anonymized data
publishing. In Proceeding of the 14th ACM SIGKDD, pages
70–78. ACM, 2008.
[9] B. Carminati, E. Ferrari, and A. Perego. Rule-based access
control for social networks. In On the Move to Meaningful
Internet Systems 2006: OTM 2006 Workshops, pages
1734–1744. Springer, 2006.
[10] B. Carminati, E. Ferrari, and A. Perego. Enforcing access
control in web-based social networks. ACM Transactions on
Information and System Security (TISSEC), 13(1):1–38,
2009.
[11] E. Carrie. Access Control Requirements for Web 2.0
Security and Privacy. In Proc. of Workshop on Web 2.0
Security & Privacy (W2SP). Citeseer, 2007.
[12] P. Fong. Relationship-Based Access Control: Protection
Model and Policy Language. In Proceedings of the First
ACM Conference on Data and Application Security and
Privacy. ACM, 2011.
[13] P. Fong, M. Anwar, and Z. Zhao. A privacy preservation
model for facebook-style social network systems. In
Proceedings of the 14th European conference on Research in
computer security, pages 303–320. Springer-Verlag, 2009.
[14] J. Golbeck. Computing and applying trust in web-based
social networks. Ph.D. thesis, University of Maryland at
College Park College Park, MD, USA. 2005.
[15] H. Hu and G. Ahn. Multiparty authorization framework for
data sharing in online social networks. In Proceedings of the
25th annual IFIP WG 11.3 conference on Data and
applications security and privacy, DBSec’11, pages 29–43.
Springer, 2011.
[16] H. Hu, G. Ahn, and K. Kulkarni. Anomaly discovery and
resolution in web access control policies. In Proceedings of
the 16th ACM symposium on Access control models and
technologies, pages 165–174. ACM, 2011.
[17] S. Kruk, S. Grzonkowski, A. Gzella, T. Woroniecki, and
H. Choi. D-FOAF: Distributed identity management with
access rights delegation. The Semantic Web–ASWC 2006,
pages 140–154, 2006.
[18] A. Lampinen, V. Lehtinen, A. Lehmuskallio, and
S. Tamminen. We’re in it together: interpersonal
management of disclosure in social network services. In
Proceedings of the 2011 annual conference on Human
factors in computing systems, pages 3217–3226. ACM, 2011.
[19] T. Li and N. Li. On the tradeoff between privacy and utility
in data publishing. In Proceedings of the 15th ACM
SIGKDD, pages 517–526. ACM, 2009.
[20] K. Liu and E. Terzi. A framework for computing the privacy
scores of users in online social networks. ACM Transactions
on Knowledge Discovery from Data (TKDD), 5(1):6, 2010.
[21] M. Madejski, M. Johnson, and S. Bellovin. The Failure of
Online Social Network Privacy Settings. Technical Report
CUCS-010-11, Columbia University, NY, USA. 2011.
[22] A. Squicciarini, M. Shehab, and F. Paci. Collective privacy
management in social networks. In Proceedings of the 18th
international conference on World wide web, pages 521–530.
ACM, 2009.
[23] N. Talukder, M. Ouzzani, A. Elmagarmid, H. Elmeleegy, and
M. Yakout. Privometer: Privacy protection in social
networks. In Proceedings of 26th International Conference
on Data Engineering Workshops (ICDEW), pages 266–269.
IEEE, 2010.
[24] K. Thomas, C. Grier, and D. Nicol. unFriendly: Multi-party
Privacy Risks in Social Networks. In Privacy Enhancing
Technologies, pages 236–252. Springer, 2010.
[25] G. Wondracek, T. Holz, E. Kirda, and C. Kruegel. A
practical attack to de-anonymize social network users. In
2010 IEEE Symposium on Security and Privacy, pages
223–238. IEEE, 2010.
[26] E. Zheleva and L. Getoor. To join or not to join: the illusion
of privacy in social networks with mixed public and private
user proﬁles. In Proceedings of the 18th international
conference on World wide web, pages 531–540. ACM, 2009.