changes affects the complexity of the algorithm.
The prediction counterpart of NN, similar to the k-
means and SVM algorithms, is a subroutine of the train-
ing algorithm. Hence, our changes can be also used to
make an oblivious prediction given a trained network.
4.5 Decision Tree Evaluation
Decision trees are common machine learning models for
classification and regression tasks [15, 51, 52]. In these
models, a tree is traversed from root to leaf node by per-
forming a simple test on a given instance, at each interior
node of the tree. Once a leaf node is reached, a simple
model stored at this node, for example a constant value,
is used as prediction.
624  25th USENIX Security Symposium 
USENIX Association
6
Decision trees remain popular because they are non-
parametric: the size of the decision tree can grow with
more training data, providing increasingly accurate mod-
els. Ensembles of decision trees, for example in the form
of random forests [14] offer improved predictive perfor-
mance by averaging the predictions of many individual
tree models [16]. Decision trees illustrate a class of data
structures whose usage is highly instance-specific: when
evaluating the model, the path traversed from root to leaf
node reveals a large amount of information on both the
instance and the tree itself. To enable the evaluation of
decision tress without leaking side information, we adapt
the evaluation algorithm of an existing library for random
forests to make it data oblivious. We keep modifications
of the existing implementation at a minimum, relying on
the primitives of Section 3 wherever possible. Our target
tree evaluation algorithm operates on one instance x ∈ d
at a time. In particular, the trees are such that, at each in-
terior node, a simple decision stump is performed:
φ (x; j,t) =(cid:31) left,
right,
if x( j) ≤ t,
otherwise,
(3)
where j ∈ {1, . . . ,d} and t ∈ are learned parameters
stored at the interior tree node.
In order to conceal the path taken through a tree, we
modify the algorithm such that each tree layer is stored as
an array of nodes. During evaluation of an instance x, the
tree is traversed by making exactly one oblivious lookup
in each of these arrays. At each node, the lookup x( j) and
corresponding floating point comparison are done using
our oblivious primitives. In case a leaf is found early,
that is before the last layer of the tree was reached, its
ID is stored obliviously and the algorithm proceeds with
dummy accesses for the remaining layers of the tree. The
predictions of all trees in a random forest are accumu-
lated obliviously in an array; the final output is the pre-
diction with the largest weight.
Together, the described modifications guarantee data-
obliviousness both for instances and for trees (of the
same size, up to padding). The algorithmic overhead is
linear in the number of nodes n in a tree, i.e., O(n) for a
fixed d; we omit the corresponding formal development.
4.6 Matrix Factorization
Matrix factorization methods [55] are a popular set of
techniques for constructing recommender systems [32].
Given users and items to be rated, we take as input the
observed ratings for a fraction of user-item pairs, either
as explicit scores (“five stars”) or implicit user feedback.
As a running example, we consider a system to recom-
mend movies to viewers based on their experience.
Matrix factorization embeds users and items into a la-
tent vector space, such that the inner product of a user
vector with an item vector produces an estimate of the
rating a user would assign to the item. We can then use
this expected rating to propose novel items to the user.
While the individual preference dimensions in the user
and item vectors are not assigned fixed meanings, empir-
ically they often correspond to interpretable properties
of the items. For example, a latent dimension may corre-
spond to the level of action the movie contains.
Matrix factorization methods are remarkably effec-
tive [9] because they learn to transfer preference infor-
mation across users and items by discovering dimensions
of preferences shared by all users and items.
Let n be the number of users and m the number of
items in the system. We may represent all (known and
unknown) ratings as a matrix R ∈ n×m. The input con-
sists of M ratings ri, j with i ∈ 1..n and j ∈ 1..m, given by
users to the items they have seen (using the movies anal-
ogy). The output consists of U ∈ n×d and V ∈ m×d
such that R ≈ U V(cid:25); these two matrices may then be used
to predict unknown ratings ri, j as inner products (cid:24)ui,v j(cid:23).
Following [48], we refer to ui and v j as user and item
profiles, respectively.
The computation of U and V is performed by mini-
mizing regularized least squares on the known ratings:
min
1
M ∑(ri, j −(cid:24)ui,v j(cid:23))2 +λ ∑(cid:21)ui(cid:21)2
2 + µ ∑(cid:21)v j(cid:21)2
2 (4)
where λ and µ determine the extent of regularization.
The function above is not jointly convex in U and V , but
becomes strictly convex in U for a fixed V , and strictly
convex in V for a fixed U.
We implement matrix factorization using a gradient
descent, as in prior work on oblivious methods [47, 48].
More efficient methods are now available to solve (4),
such as the so-called damped Wiberg method, as shown
in an extensive empirical evaluation [30], but they all in-
volve more advanced linear algebra, so we leave their
privacy-preserving implementation for future work.
Gradient descent This method iteratively updates U
and V based on the current prediction error on the input
ratings. The error is computed as ei, j = ri, j − (cid:24)ui,v j(cid:23),
and ui and v j are updated in the opposite direction of the
gradient as follows:
u(t+1)
i ← u(t)
j ← v(t)
v(t+1)
i + γ(cid:30)∑ j ei, jv(t)
j + γ(cid:30)∑i ei, ju(t)
i(cid:29)
j − λ ut
j(cid:29)
i − µvt
(5)
(6)
The descent continues as long as the error (4) decreases,
or for a fixed number of iterations (T ). Each iteration
can be efficiently computed by updating U and V sequen-
tially. To update each user profile ui, we may for instance
use an auxiliary linked list of user ratings and pointers to
the corresponding movie profiles in V .
USENIX Association  
25th USENIX Security Symposium  625
7
The gradient descent above runs in time Θ(T M) in the
RAM model, since all ratings are used at each iteration.
For a fixed number of iterations, the access pattern of the
algorithm does not depend on the actual values of the
input ratings. However, it still reveals much sensitive in-
formation about which user-item pairs appear in the input
ratings. For example, assuming they indicate which users
have seen which movies, it trivially reveals the popular-
ity of each movie, and the intersection of movie profiles
between users, during the gradient update (see [46] for
privacy implications of leaking movie ratings).
Our data-oblivious algorithm We design an algo-
rithm whose observable behaviour depends only on pub-
lic parameters n, m, M and T , and, hence, it can be sim-
ulated and does not reveal R. (We assume that d, λ , µ,
and γ are public and do not depend on the input data.)
The high level idea is to use data structures that inter-
leave user and movie profiles. This interleaving allows
us to perform an update by sequentially reading and up-
dating these profiles in-place. Once all profiles have been
updated, some additional processing is required to inter-
leave them for the next iteration but, with some care, this
can also be implemented by sequential traversals of our
data structures. (An illustration of the algorithm can be
found in the Appendix.)
Our algorithm preserves the symmetry between users
and items. It maintains data structures U and V that cor-
respond to expanded versions of the matrices U and V .
Intuitively, every user profile in U is followed by the
movie profiles required to update it (that is, the profiles
for all movies rated by this user), and symmetrically ev-
ery movie profile in V is followed by its user profiles. We
use superscript notation U(t) and V(t) to distinguish these
data structures between iterations.
U stores n user tuples that embed the user profiles of
the original U, and M rating tuples that contain both
movie profiles and their ratings. All tuples have the same
size; they each include a user id, a movie id, a rating,
and a vector of d values. User tuples are of the form
(i,0,0,ui) with i ∈ 1..n; Rating tuples are of the form
(i, j,ri, j,v j). Hence, for each rating for j, we have a copy
of v j in a rating tuple. V symmetrically stores m item tu-
ples, of the form (0, j,0,v j), and M rating tuples, of the
form (i, j,ri, j,ui).
The precise ordering of tuples within U (and V) is ex-
plained shortly but, as long as the tuples of U are grouped
by user ids (i), and the user tuple precedes its rating tu-
ples, we can compute each u(t+1)
according to Equa-
tion (5) by traversing U(t) once, in order. After an ini-
tial Setup, each iteration actually consists of three data-
oblivious phases:
• Update the user profiles ui within U(t) using Equa-
tion (5); let ˜U be the updated data structure;
i
• Extract U (t+1) from ˜U;
• Copy U (t+1) into the rating tuples of ˜V to obtain
V(t+1) for the next iteration.
(We omit symmetric steps producing ˜V, V (t+1), and
U(t+1).) The extraction step is necessary to compute the
prediction error and prepare U and V for the next itera-
tion. Without tweaking the tuple ordering, the only effi-
cient way of doing so would be to sort ˜U lexicographi-
cally (by j, then i) so that the updated user profiles appear
in the first n tuples (the approach taken in [48]). Obliv-
ious sorting at each iteration is expensive, however, and
would take O((M +n)(log(M +n))2) oblivious compare-
and-swap of pairs of d + 3 elements.
Instead, we carefully place the user tuples in ˜U so that
they can be extracted in a single scan, outputting pro-
files at a ﬁxed rate: one user profile every (M + n)/n
tuples, on average. Intuitively, this is achieved by inter-
leaving the tuples of users with many ratings with those
of users with few ratings—Section 4.7 explains how we
efficiently compute such a tuple ordering.
Setup phase: We first initialize the user and vector pro-
files, and fill U and V using the input ratings.
(1) We build a sequence LU (and symmetrically LV)
that, for every user, contains a pair of the user id i and
the count wi of the movies he has rated. To this end, we
extract the user ids from the input ratings (discarding the
other fields); we sort them; we rewrite them sequentially,
so that each entry is extended with a partial count and a
flag indicating whether the next user id changes; and we
sort them again, this time by flag then user id, to obtain
LU as the top n entries. (Directly outputting LU during the
sequential scan would reveal the counts.) For instance,
after the first sorting and rewriting, the entries may be of
the form (1,1,⊥), (1,2,⊥), (1,3,(cid:28)), (2,1,⊥), . . ..
(2) We expand LU (and symmetrically LV) into a se-
quence IU of size M + n that includes, for every user i
with wi ratings, one tuple (i,0,⊥,k, (cid:28)) for each k = 0..wi,
such that the values (cid:28) are ordered by the interleaving ex-
plained in Section 4.7.
(3) We construct U with empty user and rating pro-
files, as follows. Our goal is to order the input ratings ac-
cording to LU. To this end, we extend each input rating
with a user-rating sequence number k = 1..wi, thereby
producing M tuples (i, j,ri, j,k,⊥), and we append those
to IU. We sort those tuples by i then k then ri, j, so that
(i,0,⊥,k, (cid:28)) is directly followed by (i, j,ri, j,k,⊥) for k =
1..wi; we sequentially rewrite those tuples so that they
become ( , , , , ) directly followed by (i, j,ri, j,k, (cid:28)); we
sort again by (cid:28); and we discard the last M dummy tuples
( , , , , ).
(4) We generate initial values for the user and item
profiles by scanning U and filling in ui and v j using two
pseudo-random functions (PRFs): one for uis and one
8
626  25th USENIX Security Symposium 
USENIX Association
for v js. For each, user tuple (i,0,0,ui), we use the first
PRF on inputs (i − 1)d + 1, ..,id to generate d random
numbers that we normalize and write to ui. For each, rat-
ing tuple (i, j,ri, j,v j), we use the second PRF on inputs
( j−1)d +1, .., jd to generate d random numbers that we
also normalize and write to v j. We then use the same two
PRFs for V: the first one for rating tuples and the second
one for item tuples.
Update phase: We compute updated user profiles (and
symmetrically item profiles) in a single scan, reading
each tuple of U (and symmetrically V) and (always)
rewriting its vector—that is, its last d values, storing ui
for user tuples and v j for rating tuples.
We use 4 loop variables u, δ , u◦, and δ◦ each holding
d vector, to record partially-updated profiles for the
a
current user and for user i◦. We first explain how u and δ
are updated for the current user (i). During the scan, upon
reading a user tuple (i,0,0,ui), as is always the case for
the first tuple, we set u to ui and δ to ui(1 − λγ) and
we overwrite ui (to hide the fact that we scanned a user
tuple). Upon reading a rating tuple (i, j,ri, j,v j) for the
current user i, we update δ to γv j(ri, j −(cid:29)u,v j(cid:28)) + δ and
overwrite v j with δ . Hence, the last rating tuple (before
the next user tuple) now stores the updated profile u(t+1)
for the current user i.
i
We now bring our attention to i◦, u◦, and δ◦. Recall
that our interleaving of users in U splits the rating tuples
for some users.
In such cases, if there are ratings left
to scan, the running value δ written to v j (before scan-
ning the next user) may not yet contain the updated user
profile. Accordingly, we use i◦, u◦, and δ◦ to save the
state of the ‘split’ user while we process the next user,
and restore it later as we scan the next rating tuple of the
form (i◦, j,ri◦, j,v j). In the full version of the paper we
prove that a single state copy suffices during the expan-
sion of LU as we split at most one user at a time.
Extraction phase: The update leaves some of the val-
ues u(t+1)
scattered within ˜U (and similarly for v(t+1)
within ˜V). Similar to the update phase we can extract
all profiles by maintaining a state i◦ and u◦ for only one
user. We extract U while scanning ˜U. In particular, after
reading the last tuple of every chunk of size (M + n)/n
in ˜U we always append an entry to U. This entry is ei-
ther i◦ and u◦ or the content of the last tuple i and u.
Meanwhile, after reading every tuple of ˜U we write back
either the same entry or the profile that was written to U
last. This step ensures that user tuples contain the up-
dated u(t+1). We also update i◦ and u◦ on every tuple:
either performing a dummy update or changing the state
to the next (split) user.
i
j
This step relies on a preliminary re-ordering and inter-
leaving of users, such that the ith chunk of tuples always
contains (a copy of) a user profile, and all n user profiles
9
can be collected (details of the expansion properties that
are used here are described in the following section and
in the full version of the paper).
Copying phase: We finally propagate the updated user
profiles U (t+1) to the rating tuples in ˜V, which still carry
(multiple copies of) the user profiles U (t). We update ˜V
sequentially in chunks of size n, that is, we first update
the first n rows of V, then rows n +1 to 2n and so on until
all V is updated, each time copying from the same n user
profiles of U (t+1), as follows. (The exact chunk size is
irrelevant, but n is asymptotically optimal.)
Recall that each rating tuple of
˜V is of the form
(i, j,ri, j,ut
i, (cid:28)) where i (cid:27)= 0 and (cid:28) indicates the interleaved
position of the tuple in V. To each chunk of ˜V, we ap-
pend the profiles of U (t+1) extended with dummy values,
of the form (i,0, ,u(t+1)
, ); we sort those 2n tuples by i
then j, so that each tuple from U (t+1) immediately pre-
cedes tuples from (the chunk of) ˜V whose user profile
must be updated by u(t+1)
; we perform all updates by
a linear rewriting; we sort again by (cid:28); and we keep the
first n tuples. Finally, V(t+1) is just the concatenation of
those updated chunks.
i
i
Theorem 3. Our matrix factorization algorithm runs