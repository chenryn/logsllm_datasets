iteratively molded the play-doh and tested it on the phone
(during preliminary experiments) until the area value sta-
bilized at around 0.15 units. When the motors pushed the
play-doh against the touch screen during scrolling, the play-
doh, owing to its softness, would see some amount of de-
formation. These small variations in play-doh touch surface
area did not aﬀect our attack that much, since human ﬁngers
also see variations in area along the path of a stroke.
Perhaps one interesting observation worth noting here is
that when we connected a battery (AA type) to the play-doh
during the attack, the area registered on the phone screen
increased. A possible reason for this is that the eﬀective
area of contact between the screen and the phone is not on-
ly dependent on the physical area of contact between the
two, but, it also depends on the extent of electrical contac-
t between them. We leveraged this property to introduce
changes in the ﬁnger area during the experiments.
4.4 Algorithmic Design of Robot Movements
4.4.1 Overview
We formulated an algorithm that can generate a wide va-
riety of touch strokes depending on the parameters passed
Figure 4: Demonstrating the steps taken by the
robot during execution of a swipe gesture. The
path seen by the Android system is just a collection
of points (e.g., the black dots in the ﬁgure) sam-
pled from the physical path traversed on the phone
screen.
to it. The attack launched in this paper was just one special
case of the algorithm (i.e., a speciﬁc set of inputs). We ﬁrst
describe the generic algorithm, and then narrow down to the
speciﬁc case which we used for our attacks. Before delving
into the details of the algorithm, we note that we used the
Bricx Command Center IDE (BricxCC) [1] for the algorith-
m implementation. Unlike the standard Lego Mindstorms
NXT IDE (see [3]) which uses a drag and drop graphical in-
terface, BricxCC supports NXC, a high level programming
language whose syntax is similar to that of the C language.
With the NXC language, we were able to easily ﬁne-tune
our code without going through the tedious process of re-
aligning and modifying the graphical components (as would
be the case with the Mindstorms NXT IDE).
4.4.2 General Algorithm
We describe the mechanism of the attack (Algorithm 1)
based on the touch stroke represented in Figure 4. We gen-
eralize an arbitrary touch stroke as comprising of either or
both of an upward trajectory (e.g., XY ) and a downward
trajectory (e.g., Y Z). To traverse the upward path between
X and Y , the robot ﬁrst moves the ﬁnger downwards on-
to the phone at speed speciﬁed by the parameter speedUp-
Down (see method FingerDown), before embarking on a se-
ries of very short vertical and horizontal steps having the
sawtooth-like structure represented along A’B’. The short
vertical step (i.e., Y1) is executed using the method Fin-
gerForward (see (Algorithm 1)) while the short horizontal
step (i.e., X1) that immediately proceeds the vertical step
is executed using the method FingerRight. The algorithm
repeats (loops) this process until the peak position (i.e., Y )
is reached (i.e., after, say, p iterations).
On the downward trajectory (Y Z) the algorithm uses the
same approach used for the upward trajectory, except that
the vertical steps are now downwards (see the method Fin-
gerBackward), while the horizontal steps continue towards
the right (i.e., using the method FingerRight). On reach-
ing the point Z, the robot lifts the ﬁnger oﬀ the phone (see
FingerUp), moves it back towards X (see FingerBack), and
then swipes again after a random interval.
Each of the methods executing the short steps takes two
parameters, one of which is a speed input (e.g., speedFwd),
the other a distance input (e.g., SLFwd). These param-
eters enable us to control the speed and distance covered
in each step. During each swipe (we interchangeably refer
604Parameters / Values Horizontal Vertical
Swiping
4
4
0
2
53
20
0
20
0
300
2
2
numOfSteps
SLRight
SLFwd
SLBkwd
speedRight
speedBkwd
speedFwd
speedUpDown
p
numOfSwipes
(cid:22)1
(cid:27)1
Swiping
4
4
2
0
49
0
20
20
4
300
2
2
Table 1: Parameter settings used for the attack on
the portrait strokes. According to the speciﬁcation-
s of the LEGO system, the speed parameters (i.e.,
speedRight, speedBkwd, speedFwd) are in terms of
the power of the robot motor, while the distance pa-
rameters (SLRight, SLFwd, SLBkwd) are expressed
in terms of the angle of rotation.
to a full path, such as one from X to Z as a swipe or a
stroke), we compute the random noise term, rfs, and use
it to perturb the speed inputs to the FingerRight method.
This perturbation, in addition to the noise arising from the
mechanical dynamics of the robot, enabled us to simulate
the randomness that humans exhibit during the execution
of a stroke. We only add the noise factors to the inputs of
the FingerRight method and not to those of the FingerFor-
ward or FingerBackward methods because the long distance
along the horizontal direction (e.g., along the straight line
from X to Z) gives more room for error than the very short
span towards the direction perpendicular to the straight line
XZ.
A critical diﬀerence between the arbitrary stroke shown in
Figure 4 and the typical touch stroke produced by humans is
that the strokes generated by humans deviate very slightly
from a straight line. For this reason, to generate a stroke
similar to XY (see Figure 4), our settings were such that the
steps along the normal to the straight line XZ were shorter
than those in the direction along XZ. This resulted into
strokes that were much less crooked than the stroke shown
in Figure 4. Given that the actual stroke registered by the
Android system is just a combination of points sampled2
from the physical stroke (e.g., see solid dots on zig-zag line
in Figure 4), evidence of crookedness was further erased from
our strokes, with the vast majority of strokes registered by
the system depicting a smooth shape.
Table 1 summarizes the algorithm settings that we used
for the attack for both the vertical and horizontal swipes
in portrait mode. To generate a horizontal stroke, we po-
sitioned the phone such that the ﬁnger started around the
point with coordinates (363, 541) and moved towards the
point having the coordinates (145, 588). For the vertical
strokes, the phone was positioned such that the start coor-
dinates were approximately (320,613) and the direction of
2On our Google Nexus phones, we found that the system
recorded touch points at an interval of 15 ms on average.
the ﬁnger being towards the point (352,400). As mentioned
earlier, the randomization factor (and the randomness e-
merging out of the mechanical interactions between robot
components) ensured that the diﬀerent variables saw slight
changes between strokes. We set the speedUpDown param-
eter (speed at which ﬁnger meets phone screen) to 20 units
so as to get pressure outputs of between 0.4 and 0.6.
Depending on factors such as the area of the ﬁnger and
the initial position of the ﬁnger relative to the phone, one
may have to set a diﬀerent value for the speedUpDown pa-
rameter in order to get pressure outputs in this range. Like
in the case of the ﬁnger area (see Section 4.3.2.), we not
only iteratively set the pressure during initial experiments,
but also depended on a connected battery to increase the
pressure to the required range. To ease the task of setting
the various attack parameters, we enabled pointer locations
(under developer options) so as to view the strokes and their
associated raw feature outputs on the screen during the ﬁne-
tuning phase. All attack parameters were set based on an
initial set of experiments in which we compared the robotic
output with the mean behavior seen across a subset of the
users who were not subjected to the attack.
5. ATTACK PERFORMANCE EVALUATION
5.1 Veriﬁcation Algorithms
We demonstrate the impact of the attack using a Support
Vector Machine (SVM) [11] and the k-Nearest Neighbors
(k-NN) classiﬁer [12]. We select these two veriﬁcation algo-
rithms because they have recently been shown in [16] and
[17] to perform very well for continuous touch-based authen-
tication. We brieﬂy describe the mechanisms of operation
of the two algorithms below:
Support Vector Machine
5.1.1
An SVM is a binary classiﬁer which uses a hyperplane
to separate two data classes in such a way that the margin
between the two classes is maximized. The margin is the
distance between the hyperplane and the boundary obser-
vations which are also referred to as support vectors. For
classes that are not linearly separable in a given feature s-
pace, it is sometimes necessary to map the original data
points to a higher dimensional space with the aid of a kernel
function. We used the Gaussian radial-basis function as our
kernel, like was done in [17].
k-Nearest Neighbors
5.1.2
During training, this classiﬁer does not have to extract
any model from the data—it only stores the feature vectors
from the diﬀerent classes (in our case two classes). Given
a new observation that is to be assigned a class label, the
k-NN classiﬁer assigns it to the class A if the majority of
the k closest training vectors to the new observation belong
to the class A. Diﬀerent researchers use diﬀerent measures
to represent the distance between the training vectors and
a test observation. Similarly to Frank et al. [17], we use the
Euclidean distance metric.
For both the k-NN and SVM, we used WEKA [29] via
its Java API to implement the classiﬁcation system. We
used k=9 for the k-NN classiﬁer since this value gave us the
best performance. For all other parameters across the two
classiﬁers, we used the WEKA defaults.
6055.2 Training and Testing Methodology
5.2.1 Training and Zero-effort Testing
Training was done based on data collected during Session
I while zero-eﬀort testing was done based on data collected
during Session II. For each user, we distinguished between
portrait and landscape strokes, and further distinguished
between horizontal and vertical strokes for each of the two
phone orientations. This way, each user had four reference
templates. The reason for separating between these four
types of strokes was because certain features change depend-
ing on the type of stroke and the way in which the phone
is held when the stroke is executed. For example, for the
typical user, a horizontal stroke executed in portrait mode
will very likely have diﬀerent start and end-points (among
other features) from a horizontal stroke executed in land-
scape mode. Owing to the mismatch between features, a
classiﬁcation mechanism that does not distinguish between
these two types of strokes will likely perform unreliably.
In practice we believe that a touch-based authentication
application should use all four types of reference templates
since users can switch between stroke types depending on
the type and organization of content they read on the phone.
Regardless of whether a user is biased towards a certain type
of stroke, the system should be able to accurately perform
classiﬁcation during those times when the user executes the
other kinds of strokes.
For each of the four categories of strokes, we only per-
formed our analysis for those users who executed at least 80
strokes during Session I. For the portrait strokes we had 106
and 118 users who met this 80 strokes requirement for the
horizontal and vertical strokes respectively. For the land-
scape strokes, we had 41 and 50 users who met the require-
ment for the horizontal and vertical strokes respectively. For
training, we used 80 strokes executed by the user in ques-
tion (i.e., genuine or positive class) and 5 strokes from each
of the other users (i.e., the impostor or negative class) for
each of the four categories of strokes.
To establish a baseline against which to measure the im-
pact of the robotic attack, we carried out zero-eﬀort testing
for each user. In these tests, to launch an impostor attack a-
gainst a given user’s template, we used 10 strokes from each
of the other users. To carry out a genuine attack against
a given user’s template, we used all the strokes captured
from that particular user during Session II. Because a user
will every now and then execute a stroke which is very dis-
tinct from the rest of her strokes, we used a block of strokes,
rather than a single stroke to make authentication decisions.
Each legitimate or impostor authentication attempt was
based on a single vector derived from 10 consecutive fea-
ture vectors (or strokes). The single authentication vector
was computed such that its elements were the component-
wise means of the 10 vectors contained in a sliding window.
From the results obtained from these tests, we generated
four Detection Error Tradeoﬀ (DET) curves [18] for each us-
er, one for each kind of swiping. From each of these curves,
we determined the Equal Error Rate (EER), a performance
measure that is widely used to evaluate the performance of
biometrics systems (e.g., see [15][17][9]).
on samples generated by the robot. We will refer to the