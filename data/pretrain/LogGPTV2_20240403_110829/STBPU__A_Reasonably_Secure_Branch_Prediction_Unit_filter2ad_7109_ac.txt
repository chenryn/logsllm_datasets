Re-randomizing ST effectively resets the customization of the
BPU data representation for that process. Although it leads to
the loss of branch history (by making it unusable), our analysis
indicates that such events are infrequent. Re-randomizing the
ST of one process does not remove stored history of a process
with a different ST. This is the key difference compared to
ﬂushing-based approaches. We derive the re-randomization
thresholds through the analysis in Section VI.
While potentially dangerous, branch history sharing be-
tween programs beneﬁts performance. Consider a server
application that spawns a new process for each incoming
connection. Since each process executes the same code, the
accumulated BPU state is used by the newly spawned process.
This allows the new process to avoid the lengthy period
of BPU training. STBPU permits selective history sharing
by allowing the OS to provide multiple copies of the same
program to utilize the same ST value. However, when sharing
is not desired, each thread can be given a unique ST.
B. Hardware Mechanisms and Interfaces
Since current BPU designs are highly optimized in terms of
performance and hardware cost, we restrict ourselves to only
modifying BPU mapping mechanisms, adding registers, and
encrypting stored targets. Such changes will provide similar
performance to the unprotected design and make STBPU
agnostic to a particular BPU design. In STBPU, each hardware
thread is provided with an extra register to store the ST of the
current process. Only the OS is allowed to read/modify these
registers, and these registers are inaccessible in unprivileged
CPU mode. As such, the OS facilitates history retention across
context/mode switches by loading the appropriate STs. We
also add several model-speciﬁc registers (MSRs) that store
thresholds and counters for automatic ST re-randomization.
These MSRs monitor the events that indicate an active attacker
process. We monitor two events:
i) branch mispredictions
which includes incorrectly predicted direction of conditional
branches and targets of any branch, and ii) BTB evictions.
In Section VI, we explain how these events are utilized to
deter BPU attacks. Initially, the counter values are set to their
respective threshold values. When an event is observed, the
corresponding counter is decremented. When a counter reaches
0, the current ST is re-randomized, and the CPU reset the
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:28:07 UTC from IEEE Xplore.  Restrictions apply. 
112
Baseline input
STBPU input
32 ψ, 48 s
Output
Function
32 s
58 BHB
9 ind, 8 tag, 5offs R1(80 (cid:4)→ 22)
1
R2(90 (cid:4)→ 8)
2
R3(80 (cid:4)→ 14)
3
R4(96 (cid:4)→ 14)
4 18 GHR, 32 s 32 ψ, 16 GHR, 48s
t 48 s, L(GHR) 32 ψ, 48 s, L(GHR) 10/13 ind, 8/12 tag Rt(80↑ (cid:4)→ 25)
Rt(80 (cid:4)→ 10)
p
8 tag
14 ind
14 ind
32 ψ, 58 BHB
32 ψ, 48 s
32 ψ, 48 s
32 s
48 s
10 ind
L(GHR) — represents geometric series of global history lengths
s — represents the source bits of branch instructions
TABLE II: I/O bits for baseline and STBPU functions
counter with the threshold value. The OS treats these registers
as a part of software context saving, and recovering their
values on context/mode switches. We assume re-randomization
is done by fetching a value from low-latency in-chip pseudo-
random number generator [42].
The ST register is a 64-bit register divided into two 32-bit
chunks, ψ and ϕ. The ﬁrst chunk ψ acts as a key for a keyed
remapping functions making BPU mapping unique for each
process. We replace functions 1 , 2 , 3 and 4 in Figure 1
with STBPU remappings R1..4 accordingly. We add functions
Rt and Rp that are used for STBPU implementation with the
TAGE and Perceptron predictors. Both baseline and STBPU
remapping functions reduce input data (address, BTB, GHR
bits) into ﬁxed size index, tag, and offsets used by the BPU
to perform lookups. Section V-A describes how R1..4,t,p were
selected. Additionally, these functions utilize the entire 48-
bit virtual address unlike legacy functions that use truncated
address bits as inputs. This is crucial to prevent the same
address space attacks [78]. Table II details all input/output
bit changes between the baseline and STBPU models.
We use a simple scheme based on XOR to encrypt data
stored in BPU structures to stop attackers from redirecting
execution to a desired speculative gadget even if collisions
occur. In the case of a collision, speculative execution will
be redirected to an encrypted (random) address. This will
effectively stall malicious speculative execution. In STBPU,
every entry stored in BTB and RSB is XORed with ϕ of
the current process. Note that the baseline BPU stores only
32 bits of target addresses, so the 32-bit ϕ is sufﬁcient for
encrypting all stored bits. We use a simple XOR encryption
for two reasons: i) XOR operations are extremely fast with
trivial hardware implementation, and ii) automated ST re-
randomization makes the simple XOR encryption sufﬁciently
strong (discussed in Section VI). To decrypt data in BTB and
RSB, we modify the function 5 , which XORs target bits with
ϕ before extending them to 48-bit address.
V. IMPLEMENTATION
In Section IV, we deﬁned remapping functions R1..4,t,p
which replace the methods of calculating indexes, tags, and
offsets for lookup purposes in the baseline BPU model.
Remapping functions R1..4,t,p can be thought of as non-
cryptographic hash functions. Given the size constraints of
the BPU structures, collisions between different
inputs to
functions R1..4,t,p will occur;
this fact prevents functions
R1..4,t,p from providing cryptographic security, regardless of
implementation. This inherent weakness is remedied with
periodic re-randomization of STs; the security of such re-
randomizations are discussed in Section VI. The mapping
functions used in the baseline model are not fully reverse
engineered, but we can safely assume some fast compression
functions are used with delays of no more than 1 clock cycle.
Using performance and security as our guides, we placed
several important constraints upon functions R1..4,t,p:
C1 The compute delay for R1..4,t,p must not exceed C
clock cycles, where C may vary from CPU to CPU.
For our purposes, we choose C to be 1 clock cycle. We
enforce this by limiting the number for transistors of each
remapping function on the critical path.
C2 The function must provide uniformity: outputs of R1..4,t,p
should be uniformly distributed across their respective
output spaces.
C3 The function must demonstrate avalanche effect [24]: The
outputs of R1..4,t,p must appear to be pseudo-random,
and the relationship between inputs and outputs should
be non-linear.
We analyzed existing hardware supported hashing mecha-
nisms, but found none that satisﬁed our speciﬁc requirements.
Speciﬁcally, existing multi-round hash functions exceed the
single CPU cycle constraint. Later we describe a mechanism
we developed to automatically generate remapping functions
taking into account aforementioned constraints. In addition to
remapping, STBPU requires encryption of branch addresses
stored inside BPU. We found out that existing lightweight
cryptographic functions are not suitable for our purposes for
two main reasons: First, using strong ciphers does not directly
translate into better security which are primarily designed
to withstand known plaintext/ciphertext attacks. However,
STBPU threat model
is much different as attackers never
observe encrypted addresses (ciphertext) nor partially matched
plaintext/ciphertext. They only observe collisions (not know-
ing with their own or victim’s branch) and need to reverse-
engineer the rest of the address bits. Besides, knowing their
own STs does not provide immediate access to collision
creation or simpliﬁes collision-based attacks. In Section VI, we
show that the number of mispredictions and evictions attackers
must incur to successfully infer a ST far exceeds the thresholds
that will trigger ST re-randomization. Thus, encrypting with
a more advanced cipher would not
increase the level of
security. Secondly, more sophisticated encryption schemes
introduce signiﬁcant delays in CPU frontend. For instance, we
explored PRINCE-64 [12] and Feistel-Network [47] to encrypt
stored branch targets. While comparably fast, PRINCE-64 and
Feistel-Network will still consume multiple clock cycles and
consume more energy due to higher number of gates compared
to a simple subsingle-cycle XOR operation.
A. Automation of Finding Remapping Functions
Automated Remap Generation Algorithm. Designing the
remapping mechanisms is a multi-variable optimization prob-
lem. To solve it, we developed an algorithm that takes in a list
of hardware constraints, and randomly generates remapping
function candidates. The algorithm composes the function
from a predetermined pool of primitives. Each remapping
function is iteratively generated and tested one layer at a time,
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:28:07 UTC from IEEE Xplore.  Restrictions apply. 
113
where a layer is a block of these primitives. After a layer
is added, the current function is tested against the supplied
constraints. There are three possible scenarios that occur
during each round of testing. i) The current design satisﬁes
all constraints, and subsequently stored for later optimization.
ii) The current design violates one or more constraints, and is
discarded. iii) The current design does not outright violate the
constraints, but is incomplete. In case 3, our algorithm changes
the weights used for primitive selection during the creation of
the next layer to improve the current design.
Constraint Selection of C1. Our algorithm requires an input
of several variable constraints for the generated remapping
functions to satisfy C1. These constraints are: the maximum
count of transistors along the critical path,
the maximum
number of transistors in parallel (breadth),
the maximum
number of total transistors for the design, the number of input
and output pins, the maximum number of functional layers
(blocks) the design can have, and the maximum number of
wires an arbitrary wire can cross over.
Non-invertible primitives tend to employ XOR logic gates to
obfuscate the relationship between input and output. For many
such primitives, multiple inputs generate the same, smaller
output which makes reverse-engineering difﬁcult. Combining
multiple non-invertible layers increases complexity of attacks
aiming to pair a known output to an unknown input. These
primitives compress input size |m|
to an output size |n|
where |m| > |n|. Table II shows the disparity between
the input and output sizes for R1..4,t,p functions, and indi-
cates the need for optimized compression primitives. Mixing
primitives are primarily used to introduce non-linearity to
a hash design which makes deterministically changing the
output by varying the input difﬁcult. These primitives are
primarily composed of |m| (cid:2)→ |m| sized S-boxes and P-boxes
(performing permutations). Since the hardware complexity of
S-boxes increases superlinearly with the size of |m|, we limit
our S-boxes to a maximum of 4 input/outputs. These S-
boxes can be implemented efﬁciently with combinatorial logic
or transistor/diode matrices. P-boxes are constrained by the
therefore,
Modern processors are designed to perform 15-20 gate
operations in a single cycle [58], which translates to roughly
30-45 transistors along the critical path. The delay incurred
by each transistor in the critical path is relatively independent
of the CPU clock cycle;
the faster the CPU’s
clock cycle, the smaller the number of transistors that can
be completed within 1 clock cycle. Therefore, we assume 45
is the absolute maximum number of transistors we allow in
the critical path with preference set for shorter critical paths.
Primitive Selection. Much research has been conducted into
cryptographic hash primitives [10, 11, 35, 79, 80] that provide
building blocks for hash functions with strong properties.
We leverage these primitives from SPONGENT [11] and
PRESENT [10] hashes. Out of those S-boxes (establishing
non-linearity by substations) are perhaps most critical. To
increase the simplicity of remapping function generation, we
separate primitives into two categories: non-invertible com-
pression primitives and mixing primitives.
Fig. 2: R1 remapping function construction
maximum wire crossover set for the algorithm.
Validation of Uniformity (C2) and Avalanche Effect (C3)
Remapping functions that satisfy the hardware constraints
are then tested against constraints C2 and C3. We ﬁrst employ
the balls and bins analysis and compute the coefﬁcient of vari-
ation (CV) of bins to approximate the uniformity (C2) of the
output space [60]. C3 is satisﬁed when a remapping adheres
to a strict avalanche criterion. To quantify the avalanche effect
of F , for each input λ, we generate a set of unique inputs,
S, where each input in S differs from λ by a single bit ﬂip.
We then compute the hamming distance between F (λ) and
F (Si), for all inputs in S. Using these hamming distances, we
determine the CV of the hamming distances for a particular
λ. We test each F with 1 million random inputs and compute
the average hamming distance for all inputs. The ideal case
occurs when: i) the average hamming distance over 1 million
random inputs is roughly 50%. ii) For all inputs, the CV of
the average hamming distance for each input is 0. iii) For all
bit positions of an output of F , the difference between the
minimum and maximum hamming distances for a bit ﬂip in
any bit position is 0.
B. Optimization and Remapping Selection
The ﬁnal selection of remapping functions R1..4,t,p is pri-
marily based upon the results from the previous tests. The
result
is a multiobjective optimization problem where the
ideal state for different desired metrics may be maximized or
minimized. To make all metrics comparable, we normalized
each metric so that the optimal value is 0. We then considered
this to be a simple weighted optimization problem where we
seek functions that yield the lowest sum of all metrics recorded
when testing for uniformity and the avalanche effect. Let F
be a particular function in the group of potential functions G
for remapping function Ri, for i ∈ R1..4,t,p:
k(cid:2)
i
min
wig(F ), F ∈ G
(1)
All weights were set to 1 to avoid prioritizing one metric over
another. Further prioritizing then can be done by hardware
developers for a speciﬁc CPU design. For space reasons, we do
not show the designs for all of R1..4,t,p since they share many
similar characteristics. Instead, we show the chosen design for
R1 in Figure 2 where stages 1, 3, and 5 are substitution layers
using 4 (cid:2)→ 4 and 3 (cid:2)→ 3 S-boxes. For space reasons, not
all types of S-boxes are shown. Under the design of R1, we
show the logical mappings for S-boxes used by PRESENT and
SPONGENT. P-boxes are n (cid:2)→ n in size with the pin mappings
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:28:07 UTC from IEEE Xplore.  Restrictions apply. 
114
generated randomly by our remap function generator. C-S
boxes are compression structures that map |m| bits to an output
size of |n| bits where |m| > |n|. This design of R1 has a
critical path length of 36 transistors, so it is capable of being
computed within a single clock cycle.
VI. SECURITY ANALYSIS
We assume any attackers can have complete knowledge of
all STBPU remapping functions, full control of execution ﬂow,
and are capable of executing branches to/from any address
within their processes. The goal is to enable malicious branch
instruction collisions that allow mounting one of the collision-
based attacks. STBPU makes collisions non-deterministic,
forcing the attackers to rely on either brute force approaches or
reverse-engineering the ST value. Further, attackers can utilize
recently proposed fast attack algorithms such as GEM [59] and
PPP [57] that target randomized caches [9, 13].
Parameter: Description
Wstruct: Number of ways
Istruct: Number of sets (indexes) ψa/v: A/V R() 32-bit token