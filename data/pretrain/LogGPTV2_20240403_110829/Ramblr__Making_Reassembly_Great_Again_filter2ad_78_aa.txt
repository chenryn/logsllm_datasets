title:Ramblr: Making Reassembly Great Again
author:Ruoyu Wang and
Yan Shoshitaishvili and
Antonio Bianchi and
Aravind Machiry and
John Grosen and
Paul Grosen and
Christopher Kruegel and
Giovanni Vigna
Ramblr: Making Reassembly Great Again
Ruoyu Wang, Yan Shoshitaishvili, Antonio Bianchi, Aravind Machiry,
John Grosen, Paul Grosen, Christopher Kruegel, Giovanni Vigna
{ﬁsh, yans, antoniob, machiry, jmg, pcgrosen, chris, vigna}@cs.ucsb.edu
University of California, Santa Barbara
Abstract—Static binary rewriting has many important ap-
plications in reverse engineering, such as patching, code reuse,
and instrumentation. Binary reassembling is an efﬁcient solution
for static binary rewriting. While there has been a proposed
solution to the reassembly of binaries, an evaluation on a real-
world binary dataset shows that it suffers from some problems
that lead to breaking binaries. Those problems include incorrect
symbolization of immediates, failure in identifying symbolizable
constants, lack of pointer safety checks, and other issues. Failure
in addressing those problems makes the existing approach un-
suitable for real-world binaries, especially those compiled with
optimizations enabled.
In this paper, we present a new systematic approach for
binary reassembling. Our new approach is implemented in a tool
called Ramblr. We evaluate Ramblr on 106 real-world programs
on Linux x86 and x86-64, and 143 programs collected from the
Cyber Grand Challenge Qualiﬁcation Event. All programs are
compiled to binaries with a set of different compilation ﬂags
in order to cover as many real-world scenarios as possible.
Ramblr successfully reassembles most of the binaries, which
is an improvement over the state-of-the-art approach. It should
be noted that our reassembling procedure yields no execution
overhead and no size expansion.
I.
INTRODUCTION
Our world is extremely software-dependent. Because of
this, disruption caused by ﬂaws in this software has signif-
icant impact in the “real world”. These ﬂaws come in two
forms: bugs that simply affect functionality and bugs that lead
to exploitable vulnerabilities. While the former cause their
own level of havoc on our connected society, the latter are
especially dangerous, since vulnerabilities can be leveraged
by a proﬁcient attacker to perform a larger-scale compromise.
For example, an unpatched vulnerability in an internet-facing
service could be exploited by attackers and used as a pivot
point into the internal networks of the organization running the
service. Because of this risk, patches to remediate exploitable
bugs must be deployed as quickly as possible.
If the source code of an application is available, patching
a bug is fairly straightforward: the source code is modiﬁed to
preclude the vulnerability (e.g., by adding a safety check or
refactoring application logic), and the program is recompiled.
Permission  to  freely  reproduce  all  or  part  of  this  paper  for  noncommercial 
purposes is granted provided that copies bear this notice and the full citation 
on the ﬁrst page. Reproduction for commercial purposes is strictly prohibited 
without the prior written consent of the Internet Society, the ﬁrst-named author 
(for  reproduction  of  an  entire  paper  only),  and  the  author’s  employer  if  the 
paper  was  prepared  within  the  scope  of  employment.
NDSS  ’17,  26  February  -  1  March  2017,  San  Diego,  CA,  USA
Copyright  2017  Internet  Society,  ISBN  1-891562-46-0
http://dx.doi.org/10.14722/ndss.2017.23225
However, when source code is absent, such as in the case of
proprietary software, the problem is much more complex. If
the user of the software is unwilling to wait for the vendor to
ship a new binary (or if the vendor no longer exists), the only
option is to patch the binary directly.
Patching binary code introduces challenges not present
when patching source code. When a patch is applied at
the source code level,
the compiler will redo the process
of arranging code and data in memory and handling links
between them. In binary code,
this is extremely difﬁcult,
since this linkage information is discarded by the compiler
once ﬁnished. A performant binary patching process would
need to rediscover the semantic meanings of different regions
of program memory, and reassemble the program, redoing
the compiler’s arrangement while preserving cross-references
among code and data. As a result of the difﬁculties inherent
to this procedure, the patching of binary code is currently
an ad-hoc process. Current work in the research community
either makes unrealistically strict assumptions, does not pro-
vide realistic functionality guarantees, or results in signiﬁcant
performance and/or memory overhead. Because of this, no tool
currently exists that can automatically and reliably patch real-
world binary software.
In this paper, we present a novel, systematic approach to
the reliable patching of software. Our work builds on the
reassembleable disassembly idea introduced by Uroboros [25],
but eliminates many of its limitations, adds functionality guar-
antees (or, unlike prior work, the ability to abort the reassembly
process when these guarantees cannot be met), and results in
zero performance overhead compared to the original binary.
We disassemble the original binary, properly identify symbols
and intended jump targets, insert the necessary patches, and
reassemble the assembly into the patched binary.
Our solution is based on advanced static analyses, which
introduces moderate analysis time requirements. To accom-
modate situations in which quick patching is paramount (e.g.,
when an identiﬁed zero-day vulnerability needs to be patched
as quickly as possible), we developed a series of workarounds
that drastically reduce the analysis time requirements while
relaxing some of the guarantees of functionality.
We describe our approach, discuss the workarounds, and
evaluate our approach on two corpora of binaries: a large set
of “realistic” binaries developed for the DARPA Cyber Grand
Challenge, and the set of GNU Coreutils binaries that has also
been evaluated by related work. Our evaluation measures the
reliability of our binary rewriting approach and demonstrates
an application in the form of the insertion of general binary
hardening techniques into previously-unhardened binaries, and
ﬁnds that we make signiﬁcant improvements over the state of
the art. While existing work breaks between 15% and 60% of
the binaries it rewrites, our approach results in a successful
reassembly rate of over 98%.
We summarize our contributions as follows:
• We demonstrate that correctly disassembling and reassem-
bling binaries on a large scale is not as easy as previous
work has claimed. We identify several critical challenges
in binary reassembling that are not thoroughly explored
in previous work, and show that failing to tackle these
challenges will result in broken binaries. Our solution
eliminates several key assumptions made by previous
work and greatly expands the scope of binaries that binary
reassembling can be applied to.
• We propose a systematic solution, based on localized
data ﬂow analysis and value-set analysis, to solve these
challenges in real-world binaries. Our solution allows a
certain level of functionality guarantees, and allows a
trade-off to be made between analysis speed and guar-
anteed functionality.
• With a new deﬁnition of procedures and an improved
control ﬂow graph recovery technique, our solution also
makes it possible to freely rearrange functions when
reassembling, which was never done by any previous
work. This is critical in certain use cases.
• We implement our solution in a tool called Ramblr, and
evaluate it on a large set of binaries from the DARPA Cy-
ber Grand Challenge, as well as real-world binaries from
GNU Coreutils. In order to capture important features of
binaries that are mostly used in the real world, we amplify
the amount of binaries in our dataset by compiling them
with different optimization levels and compiler ﬂags, fur-
ther stressing our tool. To our knowledge, we demonstrate
the ﬁrst reassembling technique that works on optimized
binaries. We also demonstrate several applications of
binary reassembling, implement a few of them on top
of reassembling, and evaluate the overhead compared
with several alternative applications of binary patching,
showing that our technique has a signiﬁcantly lower
execution overhead and higher functionality guarantees
than the alternatives.
In the next section, we will provide an overview of related
work and discuss how it relates to our approach.
II. BACKGROUND AND RELATED WORK
Our technique builds on current work in the ﬁeld to achieve
safe reassembly of binaries. In this section, we provide an
overview of the state of the art to provide a proper frame for
our work.
A. Static Disassembling
There is much work in the literature regarding the correct
and complete disassembly of binaries. Linear sweeping [7],
which refers to sweeping from the beginning to the end of the
executable region of a binary and decoding all encountered
bytes as instructions, is the simplest such technique. The more
advanced approach, that most disassembling techniques build
upon, is recursive traversal. This technique starts from the
entry point of a binary, resolves the targets of each control
transfer, and recursively follows those targets to decode any
encountered bytes [12]. Recent research on static disassembly
mostly focuses on complicated corner cases in binaries [3],
[15].
The de-facto standard in industry for binary disassembling
is IDA Pro, although recently, other tools and systems, like
Hopper, Binary Ninja, angr, BAP, Radare2, etc. have started
to challenge its dominance [5], [21], [16], [23], [6].
Disassembly is the ﬁrst step in control ﬂow graph recovery,
and other highly effective techniques have been developed
to recover the control ﬂow graph of a binary [12], [21].
Recent research suggests that, while modern disassemblers and
disassembly techniques are able to achieve a high coverage
of disassembled instructions on stripped, real-world binaries,
properly identifying functions remains a challenge, especially
on optimized binaries [1]. Even for the best techniques, ac-
curacy falls drastically from 99%, on binaries compiled with
no optimization (i.e., O0 optimization level in GCC), to only
82%, on binaries compiled with nearly full optimizations (i.e.,
O3 optimization level in GCC), combined with a noticeable
increase in both false positives and false negatives [1]. As we
will discuss in the following sections, current techniques have
a pathological reliance on proper identiﬁcation of function start
points, a dependency that we remove with our approach.
B. Content Classiﬁcation
After disassembling, the content within a binary must be
classiﬁed (i.e., differentiated as either code or data) before the
binary can be reassembled. This problem is formally referred
to as content classiﬁcation, and is believed to be difﬁcult
in binary analysis [22]. As previous research demonstrates,
differentiating code and data statically is “unresolvable” in
general, while doing so in a dynamic approach will inevitably
face the classical problems of dynamic coverage and state
explosion [11].
Recent work has been advancing the state of binary disas-
sembling. While the problem is still unsolvable in general, we
leverage, and improve, modern techniques stemming from the
control-ﬂow integrity (CFI) community for this purpose [29].
C. Binary Rewriting
Binary rewriting refers to the process of transforming one
binary into another, either statically or dynamically, while
maintaining existing functionality. Normally, one or more new
features or behaviors are optionally added to the transformed
binary during this process. Static binary rewriting, where
the binary ﬁle itself is modiﬁed, generally introduces lower
overhead when compared with dynamic counterparts, where
binary code is instrumented at runtime. Thus, static rewriting
is widely used in control ﬂow integrity protection [29], [24],
binary hardening [14], [27], security policy reinforcement [26],
binary instrumentation, etc. Traditionally, static binary rewrit-
ing is either performed via detouring, which involves adding
jump-out hooks to inserted code, or with full binary trans-
lation, lifting all code to an intermediate representation and
translating it back to machine code. Both manners incur
signiﬁcant overhead on the resulting binary when compared
with the original binary. In practice, full binary translation
usually results in a binary that is very different, in terms of
2
cache locality and actual control ﬂow, from the original one.
Binary reassembling does not suffer from those drawbacks,
as the reassembled binary is generated from the recovered
assembly code, avoiding the need for detours or complete
binary translation.
Dynamic binary rewriting techniques transform binaries as
they are executing, and are able to guarantee a full-coverage
transformation of commercial off-the-shelf (COTS) or stripped
binaries at a high cost of performance overhead. Common
dynamic rewriting tools include Pin, DynamoRIO [4], Val-
grind [13], and Paradyn/Dyninst [10], which are all widely
used in dynamic binary instrumentation.
D. Reassembleable Disassembling
804867d:
8048680:
8048683:
8048688:
804868d:
8048690:
8048693:
8048696:
804869b:
804869e:
sub
push
push
call
add
sub
push
call
add
jmp
esp,0x8
DWORD PTR [ebp-0x10]
0x80487c8
80483e0
esp,0x10
esp,0xc
DWORD PTR [ebp-0x18]
80483f0
esp,0x10
80486b9
Listing 1: An example output from objdump of a binary
without relocation information.
The assembly generated by a disassembler is usually unfea-
sible for assembling, due to the lack of relocation information.
Listing 1 shows a small piece of assembly extracted from a
stripped binary. The reader can observe that the assembly con-
tains absolute addresses, as opposed to labels. Theoretically,
an assembler can take an assembly with absolute addresses
and assemble it into a working binary. However, this approach
is incompatible with binary patching and retroﬁtting, as it
would require all of the basic blocks in the binary to be
at their original positions. The crux of binary reassembling
is the ability to relocate any binary code without any re-
location information. The procedure that converts absolute
addresses into corresponding labels, or symbol references,
is called symbolization, which is the core of reassembling.
Symbolization was ﬁrst proposed and developed in trace-
oriented programming [28], and then used by Uroboros to
make reassembling binaries feasible [25]. Since our binary
reassembling approach stems from the approach described in
Uroboros, we ﬁrst summarize the original approach here.
Symbolization attempts to determine whether an immediate
value is directly or indirectly used as a symbol reference (i.e.,
whether it is symbolizable). In Uroboros, all references in a
binary can be categorized into four different types, depending
on the location of the reference itself and the location of
its target: code-to-code (c2c), code-to-data (c2d), data-to-code
(d2c), and data-to-data (d2d). Given predeﬁned code and data
memory regions from the binary, the symbolization process
checks if the immediate value falls into any predeﬁned memory
region. If it does, a symbol name is created for the location and
references to that location are changed from being absolute
references (via immediates) to symbolic references (via the
symbol name). Once this is done, code can be inserted into
the re-symbolized assembly and the modiﬁed assembly can be
reassembled into a new binary.
Uroboros makes many assumptions that preclude its use
on many real-world binaries, and we propose signiﬁcant
improvements on its approach in this paper. In the next
section, we will discuss Uroboros’ limitations and why they
prevent it from working on many binaries. After this, we
describe our proposed approach, eliminating these limitations.
In Section X, we evaluate our approach against the released
implementation of Uroboros written and released by its authors
and demonstrate improvements of our approach. Finally, in
Section XI, we discuss limitations of our approach and give
potential directions on future work.
III. PROBLEMS WITH CURRENT TECHNIQUES
As discussed in Section II, our technique aims to achieve
reassembly on a wider range of binaries than Uroboros does.
Uroboros is a ﬁrst step in the direction of reassembleable
disassembly, but it does not perform well enough to work on
many real-world binaries. This is due to simplifying assump-
tions made by the authors. We present their assumptions here,
followed by a motivating example demonstrating failure cases
for Uroboros, and a discussion of the challenging situations
that cause such corner cases.
A. Uroboros’ Assumptions
As discussed in Section II, Uroboros categorizes symbol
references into four categories: code-to-code (c2c), code-to-
data (c2d), data-to-code (d2c), and data-to-data (d2d). Because
programs do not contain overlapping instructions, it is reason-
able to assume that the symbol references that target code
(c2c and d2c) must point to the beginning of an instruction 1.
To handle data-pointing symbol references (c2d and d2d) in
Uroboros, the following three assumptions are made:
a) All pointers to the data region must be stored at an address
aligned to the bit-width of the machine.
b) No transformation (i.e., of any base addresses) is required
to be performed on the original binary. Hence in the
reassembled binary, all data sections begin at the same
address as their counterparts in the original binary.
c) d2c symbols are only used as function pointers or jump
tables. Hence any d2c symbol reference must either point
to the beginning of a function, or be part of an identiﬁed
jump table.
With the three assumptions above, a very low false positive
and false negative rate of symbolization is achieved in the
original paper.
In the course of developing our approach, we identiﬁed
cases of reassembled binaries being broken after applying the
original symbolization approach. After investigation, we found
that there are multiple complex corner cases that must be con-
sidered in order to symbolize all symbolizable immediates, and
symbolize none of the non-symbolizable immediates. Further,
we found that two of the original assumptions (assumption b)
and c)) about d2c and d2d symbol references are too strict,
which leads to the breaking of reassembled binaries, or do not
support the goal of binary patching and retroﬁtting, which are
important applications of reassembling.
1Specially constructed binaries could contain overlapping instructions,
which both Uroboros and us ignore.
3
Fig. 1: A typical section layout of an ELF binary.
.text
...
.rodata
.data
.bss
Assumption a) assumes that all pointers are stored at an
aligned address. This assumption is generally acceptable, since
most compilers tend to align pointers in memory for the sake
of better performance. But it does not necessarily hold true for
all data constructs. Listing 2 demonstrates one such example
with a custom packing. A function pointer (ﬁeld cb) is stored
at offset 1 of struct dp. Assuming dp is stored at a machine bit-
aligned address, the function pointer my_callback must be
stored at an aligned address. Accepting assumption a) breaks
any binary that has a data construct holding an unaligned
pointer like Listing 2. Our technique supports unaligned stor-
age and access of pointers, allowing us to handle arbitrary data
structures.
typedef int (*callback)();
#pragma pack(1) /* Disables struct field aligning */
struct dp_t
{
unsigned char flag;