design team’s long history of developing complex digital ICs.
An industry standard 28nm process was used. This work is
necessary to prove feasibility in meeting goals such as timing
and chip area (cost). We have not produced a complete de-
sign or actual silicon. Based on our investigation, we show
that the cost of reconﬁguration is expected to be modest:
less than 20% beyond the cost of a ﬁxed (non-reconﬁgurable)
version.
We make no claim that we are the ﬁrst to advocate re-
conﬁgurable matching or that our proposed reconﬁguration
functionality is the “right” one. We do claim that it is impor-
tant to begin the conversation by making a concrete deﬁni-
tion of the RMT model and showing it is feasible by exhibit-
ing a chip, as we have attempted to do in this paper. While
chip design is not normally the province of SIGCOMM, our
chip design shows that a rather general form of the RMT
model is feasible and inexpensive. We show that the RMT
model is not only a good way to think about programming
the network, but also lends itself to direct expression in hard-
ware using a conﬁgurable pipeline of match tables and action
processors.
2. RMT ARCHITECTURE
We spoke of RMT as “allow(ing) a set of pipeline stages
. . . each with a match table of arbitrary depth and width
that matches on ﬁelds”. A logical deduction is that an RMT
switch consists of a parser, to enable matching on ﬁelds,
followed by an arbitrary number of match stages. Prudence
suggests that we include some kind of queuing to handle
congestion at the outputs.
Let’s look a little deeper. The parser must allow ﬁeld def-
initions to be modiﬁed or added, implying a reconﬁgurable
parser. The parser output is a packet header vector, which is
a set of header ﬁelds such as IP dest, Ethernet dest, etc. In
addition, the packet header vector includes “metadata” ﬁelds
such as the input port on which the packet arrived and other
router state variables (e.g., current size of router queues).
The vector ﬂows through a sequence of logical match stages,
each of which abstracts a logical unit of packet processing
(e.g., Ethernet or IP processing) in Figure 1a.
Each logical match stage allows the match table size to
be conﬁgured:
for IP forwarding, for example, one might
want a match table of 256K 32-bit preﬁxes and for Ethernet
a match table of 64K 48-bit addresses. An input selector
picks the ﬁelds to be matched upon. Packet modiﬁcations
are done using a wide instruction (the VLIW—very long
instruction word—block in Figure 1c) that can operate on
all ﬁelds in the header vector concurrently.
More precisely, there is an action unit for each ﬁeld F in
the header vector (Figure 1c), which can take up to three
input arguments, including ﬁelds in the header vector and
the action data results of the match, and rewrite F . Allow-
ing each logical stage to rewrite every ﬁeld may seem like
overkill, but it is useful when shifting headers; we show later
that action unit costs are small compared to match tables.
A logical MPLS stage may pop an MPLS header, shifting
subsequent MPLS headers forward, while a logical IP stage
may simply decrement TTL. Instructions also allow limited
state (e.g., counters) to be modiﬁed that may inﬂuence the
processing of subsequent packets.
Control ﬂow is realized by an additional output, next-
table-address, from each table match that provides the in-
dex of the next table to execute. For example, a match on
a speciﬁc Ethertype in Stage 1 could direct later processing
stages to do preﬁx matching on IP (routing), while a dif-
ferent Ethertype could specify exact matching on Ethernet
DAs (bridging). A packet’s fate is controlled by updating a
set of destination ports and queues; this can be used to drop
a packet, implement multicast, or apply speciﬁed QoS such
as a token bucket.
A recombination block is required at the end of the pipeline
to push header vector modiﬁcations back into the packet
(Figure 1a). Finally, the packet is placed in the speciﬁed
queues at the speciﬁed output ports and a conﬁgurable queu-
ing discipline applied.
In summary, the ideal RMT of Figure 1a allows new ﬁelds
to be added by modifying the parser, new ﬁelds to be matched
by modifying match memories, new actions by modifying
stage instructions, and new queueing by modifying the queue
discipline for each queue. An ideal RMT can simulate ex-
isting devices such as a bridge, a router, or a ﬁrewall; and
can implement existing protocols, such as MPLS and ECN,
and protocols proposed in the literature, such as RCP [8]
that uses non-standard congestion ﬁelds. Most importantly,
it allows future data plane modiﬁcations without modifying
hardware.
2.1
Implementation Architecture at 640Gb/s
We advocate an implementation architecture shown in
Figure 1b that consists of a large number of physical pipeline
stages that a smaller number of logical RMT stages can be
mapped to, depending on the resource needs of each logical
stage. This implementation architecture is motivated by:
1. Factoring State: Router forwarding typically has sev-
eral stages (e.g., forwarding, ACL), each of which uses a
separate table; combining these into one table produces the
cross-product of states. Stages are processed sequentially
with dependencies, so a physical pipeline is natural.
(a) RMT model as a sequence of logical Match-Action stages.
(b) Flexible match table conﬁguration.
(c) VLIW action architecture.
Figure 1: RMT model architecture.
2. Flexible Resource Allocation Minimizing Resource Waste:
A physical pipeline stage has some resources (e.g., CPU,
memory). The resources needed for a logical stage can vary
considerably. For example, a ﬁrewall may require all ACLs,
a core router may require only preﬁx matches, and an edge
router may require some of each. By ﬂexibly allocating phys-
ical stages to logical stages, one can reconﬁgure the pipeline
to metamorphose from a ﬁrewall to a core router in the ﬁeld.
The number of physical stages N should be large enough
so that a logical stage that uses few resource will waste at
most 1/N -th of the resources. Of course, increasing N will
increase overhead (wiring, power):
in our chip design we
chose N = 32 as a compromise between reducing resource
wastage and hardware overhead.
3. Layout Optimality: As shown in Figure 1b, a logical
stage can be assigned more memory by assigning the logical
stage to multiple contiguous physical stages. An alternate
design is to assign each logical stage to a decoupled set of
memories via a crossbar [4]. While this design is more ﬂexi-
ble (any memory bank can be allocated to any stage), worst
√
case wire delays between a processing stage and memories
M , which in router chips that require
will grow at least as
a large amount of memory M can be large. While these
delays can be ameliorated by pipelining, the ultimate chal-
lenge in such a design is wiring: unless the current match
and action widths (1280 bits) are reduced, running so many
wires between every stage and every memory may well be
impossible.
In sum, the advantage of Figure 1b is that it uses a tiled
architecture with short wires whose resources can be recon-
ﬁgured with minimal waste. We acknowledge two disadvan-
tages. First, having a larger number of physical stages seems
to inﬂate power requirements. Second, this implementation
architecture conﬂates processing and memory allocation. A
logical stage that wants more processing must be allocated
two physical stages, but then it gets twice as much memory
even though it may not need it. In practice, neither issue
is signiﬁcant. Our chip design shows that the power used
by the stage processors is at most 10% of the overall power
usage. Second, in networking most use cases are dominated
by memory use, not processing.
2.2 Restrictions for Realizability
The physical pipeline stage architecture needs restrictions
to allow terabit-speed realization:
Input ChannelsLogical Stage 1...Switch State(metadata)Select......VLIWActionMatch TablesStatisticsStateProg. ParserHeaderPayload...Packets1K...Logical Stage NRecombineOutput Channels...1KConﬁgurable Output QueuesPacketsNew HeaderPhysical Stage 1PhysicalStage 2Logical Stage 1Logical Stage 2Physical Stage M...: Ingress logical match tables: Egress logical match tablesLogical  Stage NPacket Header VectorAction Input Selector (Crossbar)Action MemoryOP codeVLIW Instruction MemoryCtrlAction UnitPacket Header Vector...Src 1Src 2Src 3Src 1Src 2Src 3OP code(from inst mem)Match Results...Match TablesAction UnitVery Wide Header BusMatch restrictions: The design must contain a ﬁxed num-
ber of physical match stages with a ﬁxed set of resources.
Our chip design provides 32 physical match stages at both
ingress and egress. Match-action processing at egress allows
more eﬃcient processing of multicast packets by deferring
per-port modiﬁcations until after buﬀering.
Packet header limits: The packet header vector containing
the ﬁelds used for matching and action has to be limited.
Our chip design limit is 4Kb (512B) which allows processing
fairly complex headers.
Memory restrictions: Every physical match stage contains
table memory of identical size. Match tables of arbitrary
width and depth are approximated by mapping each logi-
cal match stage to multiple physical match stages or frac-
tions thereof (see Fig. 1b). For example, if each physical
match stage allows only 1,000 preﬁx entries, a 2,000 IP log-
ical match table is implemented in two stages (upper-left
rectangle of Fig. 1b). Likewise, a small Ethertype match ta-
ble could occupy a small portion of a match stage’s memory.
Hash-based binary match in SRAM is 6× cheaper in area
than TCAM ternary match. Both are useful, so we provide
a ﬁxed amount of SRAM and TCAM in each stage. Each
physical stage contains 106 1K × 112b SRAM blocks, used
for 80b wide hash tables (overhead bits are explained later)
and to store actions and statistics, and 16 2K × 40b TCAM
blocks. Blocks may be used in parallel for wider matches,
e.g., a 160b ACL lookup using four blocks. Total memory
across the 32 stages is 370 Mb SRAM and 40 Mb TCAM.
Action restrictions: The number and complexity of in-
structions in each stage must be limited for realizability.
In our design, each stage may execute one instruction per
ﬁeld. Instructions are limited to simple arithmetic, logical,
and bit manipulation (see §4.3). These actions allow imple-
mentation of protocols like RCP [8], but don’t allow packet
encryption or regular expression processing on the packet
body.
Instructions can’t implement state machine functional-
ity; they may only modify ﬁelds in the packet header vec-
tor, update counters in stateful tables, or direct packets to
ports/queues. The queuing system provides four levels of
hierarchy and 2K queues per port, allowing various com-
binations of deﬁcit round robin, hierarchical fair queuing,
token buckets, and priorities. However, it cannot simulate
the sorting required for say WFQ.
In our chip, each stage contains over 200 action units:
one for each ﬁeld in the packet header vector. Over 7,000
action units are contained in the chip, but these consume a
small area in comparison to memory (< 10%). The action
unit processors are simple, speciﬁcally architected to avoid
costly to implement instructions, and require less than 100
gates per bit.
How should such an RMT architecture be conﬁgured?
Two pieces of information are required: a parse graph that
expresses permissible header sequences, and a table ﬂow
graph that expresses the set of match tables and the con-
trol ﬂow between them (see Figure 2 and §4.4). Ideally, a
compiler performs the mapping from these graphs to the
appropriate switch conﬁguration. We have not yet designed
such a compiler.
3. EXAMPLE USE CASES
To give a high level sense of how to use an RMT chip, we
will take a look at two use cases.
Example 1: L2/L3 switch. First, we need to conﬁgure
the parser, the match tables and the action tables. For our
ﬁrst example, Figure 2a shows the parse graph, table ﬂow
graph, and memory allocation for our L2/L3 switch. The
Parse Graph and Table Flow Graph tell the parser to extract
and place four ﬁelds (Ethertype, IP DA, L2 SA, L2 DA)
on the wide header bus. The Table Flow Graph tells us
which ﬁelds should be read from the wide header bus and
matched in the tables. The Memory Allocation tells us how
the four logical tables are mapped to the physical memory
stages. In our example, the Ethertype table naturally falls
into Stage 1, with the remaining three tables spread across
all physical stages to maximize their size. Most hash table
RAM is split between L2 SA and DA, with 1.2 million entries
for each. We devote the TCAM entries in all 32 stages to
hold 1 million IP DA preﬁxes. Finally, we need to store the
VLIW action primitives to be executed following a match
(e.g. egress port(s), decrement TTL, rewrite L2 SA/DA).
These require 30% of the stage’s RAM memory, leaving the
rest for L2 SA/DA. If enabled, packet and byte counters
would also consume RAM, halving the L2 table sizes.
Once conﬁgured, the control plane can start populating
each table, for example by adding IP DA forwarding entries.
Example 2: RCP and ACL support. Our second use
case adds Rate Control Protocol (RCP) support [8] and an
ACL for simple ﬁrewalling. RCP minimizes ﬂow completion
times by having switches explicitly indicate the fair-share
rate to ﬂows, thereby avoiding the need to use TCP slow-
start. The fair-share rate is stamped into an RCP header
by each switch. Figure 2b shows the new parse graph, table
ﬂow graph and memory allocation.
To support RCP, the packet’s current rate and estimated
RTT ﬁelds are extracted and placed in the header vector
by the parser. An egress RCP table in stage 32 updates
the RCP rate in outgoing packets—the min action selects
the smaller of the packet’s current rate and the link’s fair-
share rate. (Fair-share rates are calculated periodically by
the control plane.)
A stateful table (§4.4) accumulates data required to calcu-
late the fair-share rate. The stateful table is instantiated in
stage 32 and accumulates the byte and RTT sums for each
destination port.
We also create 20K ACL entries (120b wide) from the last
two stages of TCAM, reducing the L3 table to 960K preﬁxes,
along with RAM entries to hold the associated actions (e.g.,
drop, log).
In practice, the user should not be concerned with the
low-level conﬁguration details, and would rely on a compiler
to generate the switch conﬁguration from the parse graph
and table ﬂow graph.
4. CHIP DESIGN
Thus far, we have used a logical abstraction of an RMT