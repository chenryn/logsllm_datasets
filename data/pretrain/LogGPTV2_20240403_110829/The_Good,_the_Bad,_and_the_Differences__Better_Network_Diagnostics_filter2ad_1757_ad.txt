4.7 Properties of DiffProv
Complexity: The number of steps DiffProv takes is linear
in the number of vertexes in TG. This is substantially faster
than a naïve approach that attempts random changes to mu-
table base tuples (or combinations of such tuples), which
would have an exponential complexity. DiffProv is faster
because of a) its use of provenance, which allows it to ig-
nore tuples that are not causally related to the event of inter-
est, and b) its use of taints and formulae, which enables it to
ﬁnd, at each step, a speciﬁc tuple change that will have the
desired effect – it never needs to “guess” a suitable change.
False positives: When DiffProv outputs a set of tuple changes,
this set will always satisfy our deﬁnition from Section 3.3,
that is, it will transform TB into a tree that is equivalent to
TG, while preserving the seed sB. There are no “false posi-
tives” in the sense that DiffProv would recommend changes
that have no effect, or recommend changes to tuples that are
not related to the problem. However, there is no guarantee
that the output will match the operator’s intent: if the oper-
ator inputs a packet P and a reference packet P ′, DiffProv
will output a change that will make the network treat P and
P ′ the same, even if, say, the operator would have preferred
P to take a different path. For this reason, it is best if the
operator carefully inspects the proposed changes before ap-
plying them.
False negatives: DiffProv can fail for three reasons. First,
the seeds of TG and TB have different types – for instance,
the “good” event is a packet and the “bad” event is a ﬂow
entry. In this case, there is no valid solution, and the op-
erator must pick a suitable reference. Second, the solution
would involve changing an immutable tuple – for instance,
a static ﬂow entry that the operator has declared off lim-
its, or the point at which a packet entered the network. In
this case, there is again no valid solution, but DiffProv can
show the operator what would need to be changed, and why;
this should help the operator in picking a better reference.
Third, DiffProv fails if it encounters rules that cannot be in-
verted (say, a SHA256 hash). We have not encountered non-
invertible rules in our case studies. However, if such a rule
prevents DiffProv from going further, DiffProv can output
the “attempted change” it would like to try, which may still
be a useful diagnostic clue.
4.8 Extensions
Distributed operation: So far, we have described DiffProv
as if the entire provenance trees TG and TB are materialized
on a single node. We note that, in actual operation, DiffProv
is decentralized: it never performs any global operation on
the provenance trees, and all steps are performed on a spe-
ciﬁc vertex and its direct parent or children. Therefore, each
node in the distributed system only stores the provenance of
its local tuples. When a node needs to invoke an operation
on a vertex that is stored on another node, only that part of
the provenance tree is materialized on demand.
Temporal provenance: When DiffProv tries to make tuples
appear, it must consider the state of the system “as of” the
time at which the missing tuple would have had to exist, and
it must apply the new updates to base tuples “early enough”
to be present at the required time. DiffProv accomplishes the
former by keeping a log of tuple updates along with some
checkpoints, similar with DTaP [35], so that the system state
at any point in the past can be efﬁciently reconstructed. Diff-
Prov accomplishes the latter by applying the updates shortly
before they are needed for the ﬁrst time.
4.9 Limitations and open problems
We now discuss a few limitations of the DiffProv algorithm,
and potential ways to mitigate some of them in future work.
Minimality: We note that the set of changes returned by
DiffProv is not necessarily the smallest, since it attempts to
derive missing tuples only via the speciﬁc rule that was used
to derive their counterpart in TG. Other derivations may be
possible, and they may require fewer changes. This is, in
essence, the price DiffProv pays for using TG as a guide.
Reference events: DiffProv currently relies on the operator
to supply the reference event. This works well for the major-
ity of the diagnostic cases we have surveyed (Section 2.4),
where the operators have explicitly mentioned some poten-
tial reference events as starting points. But we are also ex-
ploring to automate this process using inspirations from Au-
tomatic Test Packet Generation [32] and the “guided probes”
idea in Everﬂow [37].
Performance anomalies: Provenance in its plainest form
works aims to explain individual events. We note that debug-
ging performance anomalies, e.g., high per-ﬂow latencies,
resembles answering aggregation queries, and may require
similar extensions to the current provenance model [2] that
considers provenance for explaining aggregation results.
Non-determinism: Replay-based debuggers such as Diff-
Prov, ATPG [32], etc., assume that the network is largely
deterministic. In the presence of load-balancers that make
random decisions, e.g., ECMP with a random seed, Diff-
Prov would need to reason about the balancing mechanism
using the seed. Under race conditions, DiffProv would abort
at the point where applying the same rule does not result in
the same effect, and suggest that point as a potential race
condition.
IMPLEMENTATION
5.
Next, we present the design and implementation of our Diff-
Prov prototype. We have implemented a DiffProv debugger
in C++ based on RapidNet [1], with ﬁve major components:
a) a provenance recorder, b) a front-end, c) a logging engine,
d) a replay engine, and e) the DiffProv reasoning engine.
Provenance recorder: The provenance recorder can extract
provenance information from the primary system in three
possible modes. First, it can directly infer the provenance
if the primary system explicitly captures data dependencies,
e.g., it is compiled into running code from declarative rules [18].
Since RapidNet is a declarative networking engine based on
Network Datalog (NDlog) rules, DiffProv can infer prove-
nance directly from any NDlog program; we applied this
technique to the ﬁrst three SDN scenarios.
Alternatively, the primary system can be instrumented with
hooks that report dependencies to the recorder, e.g., as in [22].
We applied this to MapReduce by instrumenting Hadoop
MapReduce v2.7.1 to report its internal provenance to
DiffProv. Our instrumentation is moderate: it has less than
200 lines of code, and it reports dependencies at the level of
individual key-value pairs (e.g., words and their counts), as
well as input data ﬁles, Java bytecode signatures, and 235
conﬁguration entries.
Finally, we can treat the primary system as a black box,
and use external speciﬁcations to track dependencies between
inputs and outputs, e.g., as in [34]. We applied this to the
complex SDN scenario in Section 6.7, where the recorder
tracks packet-level provenance in Mininet [20] based on the
packet traces it has produced, as well as an external speciﬁ-
cation of OpenFlow’s match-action behavior.
Front-end: For our SDN scenario, we have built a front-end
for controller programs that accepts programs written either
in native NDlog or in NetCore (part of Pyretic [21]). When a
NetCore program is provided, our front-end internally con-
verts it to NDlog rules and tuples using a technique from
Y! [30].
Logging and replay engines: The logging and replay en-
gines are needed to support temporal provenance as described
in Section 4.8, and they assist the recorder to capture prove-
nance information in one of the following two approaches:
a) in the runtime based approach, the logging engine writes
down base events and all intermediate derivations, so that
the provenance is readily available at query time; b) in the
query-time based approach, the logging engine writes down
base events only, and the replay engine then reconstructs
derivations using deterministic replay. Although our proto-
type supports both approaches, we have opted for the latter
in our experiments as it favors runtime performance – diag-
nostic queries would take longer, but they are relatively rare
events; moreover, it enables an optimization that allows the
replay engine to selectively reconstructs relevant parts of the
provenance graph only.
Reasoning engine: The DiffProv reasoning engine retrieves
the provenance trees from the recorder, performs the Diff-
Prov algorithm we described in Section 4, and then issues
replay requests to update the trees.
6. EVALUATION
In this section, we report results from our evaluation of Diff-
Prov in two sets of case studies centered around software-
deﬁned networks and Hadoop MapReduce. We have de-
signed our experiments to answer four high-level questions:
a) how well can DiffProv identify the actual root cause of
a problem?, b) does DiffProv have a reasonable cost at run-
time?, c) are DiffProv queries expensive to process?, and d)
does DiffProv work well in a complex network with realistic
routing policies and heavy background trafﬁc?
6.1 Experimental setup
The majority of our SDN experiments are conducted in Rapid-
Net v0.3 on a Dell OptiPlex 9020 workstation with an 8-
core 3.40 GHz Intel i7-4770 CPU, 16 GB of RAM, a 128 GB
OCZ Vector SSD, and a Ubuntu 13.12 OS. They are based on
a 9-node SDN network setup similar with that in Figure 1,
where we replayed an OC-192 packet trace obtained from
CAIDA [7], as well as several synthetic traces with different
trafﬁc rates and packet sizes.
We further carry out an experiment on a larger and more
complex SDN network, replicating ATPG’s [32] setup of
the Stanford backbone network. We replicated this setup
because it is a network with complex policies and heavy
background trafﬁc, thus a suitable scenario to evaluate Diff-
Prov’s capability of ﬁnding root causes in a realistic set-
ting. Since their setup involves a different platform (emu-
lated Open vSwitch in Mininet [20] with a Beacon [4] con-
troller), we defer the discussion of this experiment to Sec-
tion 6.7.
Our MapReduce experiments are conducted in Hadoop
MapReduce v2.7.1, on a Hadoop cluster with 12 Dell
PowerEdge R300 servers with a 4-core 2.83 GHz Intel Xeon
X33363 CPU, 4GB of RAM, two 250 GB SATA hard disks
in RAID level 1 (mirroring), and a CentOS 6.5 OS. As a fur-
ther point of comparison, we also re-implemented the MapRe-
duce scenarios in a declarative implementation, and evalu-
ated them in RapidNet.
6.2 Diagnostic scenarios
For our experiments, we have adapted six diagnostic scenar-
ios from existing papers and studies of common errors. Our
four SDN scenarios are:
• SDN1: Broken ﬂow entry [23]. An SDN switch is
conﬁgured with an overly speciﬁed ﬂow entry. As a re-
sult, trafﬁc from certain subnets is mistakenly handled
by a more general rule, and routed to a wrong server
(TB), while other trafﬁc from other subnets continues
to arrive at the correct server (TG). This is the scenario
from Section 2.
• SDN2: Multi-controller inconsistency [10]. An SDN
switch is conﬁgured with two conﬂicting rules by dif-
ferent controller apps that are unaware of each other.
The lower-priority rule sends trafﬁc to a web server
(TG), and the higher-priority rule sends trafﬁc to a scrub-
ber. However, the header spaces of the rules overlap,
so some legitimate trafﬁc is sent to the scrubber acci-
dentally (TB).
• SDN3: Unexpected rule expiration [25]. An SDN
switch is conﬁgured with a multicast rule that sends
video data to two hosts (TG). However, when the mul-
ticast rule expires, the trafﬁc is handled by a lower-
priority rule and is delivered to a wrong host (TB). No-
tice that in this case the “good” example is a packet that
was observed in the past.
• SDN4: Multiple faulty entries. In this scenario, we
extended SDN1 with a larger topology and injected
two faulty ﬂow entries on two consecutive hops (S2–
S3). Although some trafﬁc can always arrive at the
correct server (TG), trafﬁc from certain subnets is orig-
inally misrouted by S1 (TB1), and then by S2 after
the ﬁrst fault is corrected (TB2). As a result, DiffProv
needs to proceed in two rounds to identify both faults.
Our MapReduce scenarios are inspired by feedback from
an industrial collaborator about typical bugs he encounters
in his workﬂow. Since the workﬂow is proprietary, we have
translated the problems to the classical WordCount job ex-
ample, which counts the number of occurrences of each word
in a text corpus. We have evaluated them with a declara-
tive implementation in RapidNet (MR1-D and MR2-D) and
an imperative implementation in Hadoop’s native codebase
(MR1-I and MR2-I). The MR1 and MR2 scenarios are:
• MR1-D and MR1-I: Conﬁguration changes. The
user sees wildly different output ﬁles (TB) from a Map-
Reduce job he runs regularly, because he has acciden-
tally changed the number of reducers. Because of this,
Query
Good example (TG)
Bad example (TB)
Plain tree diff
DiffProv
Query
Good example (TG)
Bad example (TB)
Plain tree diff
DiffProv
SDN1
156
201
278
1
SDN2 SDN3 SDN4
201/201
156/145
278/218
156
156
238
156
201
74
1
1
1/1
MR1-D MR2-D MR1-I MR2-I
1051
1051
164
1
1001
848
306
1
588
588
240
1
588
438