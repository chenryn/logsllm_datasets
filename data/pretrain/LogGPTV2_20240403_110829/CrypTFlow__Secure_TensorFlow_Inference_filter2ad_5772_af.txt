### Appendix C: Evaluating Accuracy and Modularity

Evaluating the accuracy of our compilation is crucial for validating its correctness [63]. Additionally, the modular design of CRYPTFLOW allows it to be compiled to various MPC backends. To demonstrate this capability, we integrated a 2PC semi-honest secure protocol, ABY [30], into CRYPTFLOW. The performance results with this backend are presented in Table VII. We tested both logistic regression (LR) and a small LeNet network [53], which consists of two convolutional layers (with a maximum filter size of 5 × 5) and two fully connected layers, using ReLU and MaxPool as activation functions. This evaluation confirms that CRYPTFLOW can be easily adapted to a variety of backends.

### D. Porthos Experiments

Since Porthos builds on SecureNN, we provide a detailed comparison between the two. As previously mentioned, Porthos improves the communication complexity of SecureNN [79] for both convolutional layers and non-linear activation functions. We have already compared SecureNN and Porthos on benchmarks from SecureNN in Table V. Furthermore, we compare Porthos and SecureNN on ImageNet-scale benchmarks in Table VIII. For this, we added the SecureNN code available at [5] as another backend to CRYPTFLOW. These results show that Porthos reduces the communication by approximately 1.2X–1.5X and the runtime by about 1.4X–1.5X compared to SecureNN.

| **Benchmark** | **CRYPTFLOW (s)** | **Communication (MB)** |
|---------------|--------------------|------------------------|
| LogisticRegression | 0.227 | 47.4 |
| LeNet Small | 25.5 | 2939 |

**Table VII: CRYPTFLOW compilation to 2PC on MNIST.**

| **Benchmark** | **SecureNN (s)** | **Porthos (s)** | **SecureNN Comm. (GB)** | **Porthos Comm. (GB)** |
|---------------|------------------|-----------------|--------------------------|-------------------------|
| RESNET50 | 38.36 | 25.87 | 8.54 | 6.87 |
| DENSENET121 | 53.99 | 36.00 | 13.53 | 10.54 |

**Table VIII: Porthos vs SecureNN.**

### E. Aramis Experiments

We applied Aramis to both the 2-party GMW protocol [35] (using the codebase [2], based on [24]) and Porthos. The results for different functions using the GMW protocol are shown in Table IX. IPn denotes the inner product of two n-element vectors over F2, Add32 and Mult32 denote addition and multiplication over 32 bits, respectively, and Millionaire32 denotes the millionaires problem, which compares two 32-bit integers x and y and outputs a single bit indicating whether x > y. The overheads of Aramis-based malicious security are within 54% of the semi-honest protocol. Table IV and Figure 10 evaluate Aramis with Porthos.

| **Benchmark** | **GMW (ms)** | **Aramis (ms)** | **Overhead** |
|---------------|--------------|-----------------|--------------|
| IP10,000 | 464 | 638 | 1.37x |
| IP100,000 | 2145 | 3318 | 1.54x |
| Add32 | 279 | 351 | 1.25x |
| Mult32 | 354 | 461 | 1.30x |
| Millionaire32 | 292 | 374 | 1.28x |

**Table IX: Semi-honest GMW vs Malicious Aramis.**

#### 1. Comparison with Crypto-Only Malicious MPC

We demonstrate that Aramis-based malicious secure protocols are better suited for large-scale inference tasks compared to pure cryptographic solutions. We compare the performance of Porthos compiled with Aramis and the concurrent work of QuantizedNN [12], which uses the MP-SPDZ [4] framework to provide a malicious secure variant of their protocol. Both approaches provide security for the same setting of 3PC with one corruption. On the four MNIST inference benchmarks A/B/C/D in the MP-SPDZ repository, Aramis is 10X/46X/44X/15X faster.

### F. Real-World Impact

We discuss our experience using CRYPTFLOW to compile and run DNNs used in healthcare. These DNNs are available as pre-trained Keras models, which we converted to TensorFlow using [3] and then compiled with CRYPTFLOW.

#### a) Chest X-Ray
In [85], the authors trained a DENSENET121 model to predict lung diseases from chest X-ray images using the publicly available NIH dataset. They achieved an average AUROC score of 0.845 across 14 possible disease labels. During secure inference, we observed no loss in accuracy, and the runtime was similar to that of DENSENET121 for ImageNet.

#### b) Diabetic Retinopathy CNN
Diabetic Retinopathy (DR), a major cause of blindness, is a medical condition that damages the retina due to diabetes [64]. Major tech companies have recently taken an interest in using DNNs to diagnose DR from retinal images [64, 37]. With CRYPTFLOW, predicting whether a retina image has DR or not can be done securely in about 30 seconds.

### VII. Related Work

#### High-Level Languages
CRYPTFLOW is the first system to compile predefined TensorFlow code to secure MPC protocols. Previous works, such as Fairplay [56], Wysteria [70], ObliVM [54], CBMC-GC [42], SMCL [66, 59], Sharemind [16], EzPC [19], and SPDZ [9], compile from lower-level, domain-specific languages to MPC. Reimplementing large DNNs in the input format of these tools is a formidable task. PySyft [73] and TF-Encrypted [26] are ongoing efforts that also aim to compile DNNs to MPC protocols. Unlike CRYPTFLOW, which compiles standard TensorFlow code, these works require reimplementing the DNNs in a dialect of PyTorch/TensorFlow. To the best of our knowledge, these systems have not been evaluated on ImageNet-scale tasks.

#### Fixed-Point in MPC
Although the use of fixed-point for secure computations is well-known [68], prior works on secure inference have addressed the float-to-fixed problem either by manually generating fixed-point models ([62, 55, 49, 60, 79, 58, 72]) or by using non-standard training algorithms that output fixed-point models ([71, 7]). Both approaches are unsatisfactory. Specifically, retraining the entire dataset is computationally expensive and impossible if the training data is unavailable. Additionally, training algorithms that generate integer models are still an active research area, and most ML training algorithms generate floating-point models. Athos alleviates these problems by working with a trained model and being oblivious to the training procedure. Users can train their networks as they see fit and then use Athos to get fixed-point code. Even with retraining, TensorFlow-generated binary/integer networks suffer significant accuracy losses [48], whereas Athos matches the accuracy of floating-point models.

#### Float-to-Fixed
The research literature on float-to-fixed for digital signal processors is rich and spans several decades. Recently, these schemes have been adapted to machine learning. Some recent float-to-fixed schemes show promise by quantizing floating-point models to 8-bit or 16-bit integers. One could potentially use one of these systems in place of our float-to-fixed component; however, their compatibility with MPC protocols [60, 79] is unclear. Additionally, since we use a higher bit-width of 64, the accuracy of CRYPTFLOW is better.

#### Secure Machine Learning
There has been a flurry of recent results in the area of secure machine learning, both in the 2-party [17, 67, 34, 55, 49, 58, 84] and 3-party settings [72, 60, 79, 12]. The most relevant to our work are ABY3 [60] and SecureNN [79], which both provide 3-party semi-honest secure computation protocols for a variety of neural network inference and training algorithms with similar performance guarantees. Porthos, our 3-party semi-honest protocol, outperforms both these works. Other recent works [71, 75, 7, 33, 27, 15, 14, 39] modify the inference or training algorithms to obtain performance benefits but are applicable only to specialized benchmarks. For example, works using fully homomorphic encryption (e.g., [27, 15, 14]) do not support secure evaluation of ReLUs, and XONN [71] requires DNNs to have binary weights. In contrast, we focus on standard inference algorithms, making CRYPTFLOW widely applicable.

#### Hardware-Based Security
Our work is the first to provide experimentally validated malicious secure inference of ML algorithms at the scale of RESNET50. We achieve this by relying on minimally secure hardware to provide integrity. Prior works that use hardware enclaves for secure computation [74, 69, 38, 11, 23, 29, 51, 32, 77, 83, 44] assume that the enclave hides all data residing in it from the host. Unlike Aramis, these systems are not secure against an adversary that can observe the SGX state. The only prior work that assumes a weaker trust assumption from the hardware is [78]. Similar to our work, they assume that the hardware provides integrity. However, their work is in the context of zero-knowledge proofs and other fundamentally asymmetric primitives that require only one enclave and not interactive protocols between multiple enclaves.

### VIII. Conclusion

CRYPTFLOW is the first end-to-end system that translates high-level TensorFlow inference code to MPC protocols. It consists of three components: a) a compiler from TensorFlow to MPC, b) an improved semi-honest 3PC protocol for DNNs, and c) a generic technique to convert semi-honest secure protocols to maliciously secure ones. Using CRYPTFLOW, we demonstrate the first instance of secure inference on large benchmarks such as RESNET50 and DENSENET121 on the ImageNet dataset with both semi-honest (in about thirty seconds) and malicious security (in less than two minutes). CRYPTFLOW's modular design supports a variety of backends, and we hope it can serve as a testbed for benchmarking new MPC protocols in the area.

Moving forward, we plan to integrate protocols like SPDZ [4] and Delphi [58] into CRYPTFLOW. Our more ambitious goal is to extend CRYPTFLOW to support TensorFlow training, a challenging problem given the lack of GPU support, which can make the overheads of MPC protocols for secure training prohibitive.

### IX. Acknowledgements

We thank our shepherd Xiao Wang and anonymous reviewers for their valuable feedback. We also thank Sridhar Gopinath, Aayan Kumar, Wonyeol Lee, Sundararajan Renganathan, and Kapil Vaswani for helpful discussions.