in Appendix C. Evaluating accuracy also helps validate the
correctness of our compilation [63].
Modularity. Since CRYPTFLOW is modular, we can compile
it to various MPC backends. To demonstrate this ability, we
also add a 2PC semi-honest secure protocol ABY [30] to
CRYPTFLOW. The performance with this backend is in Table
VII. We ran logistic regression (LR) as well as a small LeNet
network [53] which comprises of 2 convolutional layers (with
maximum ﬁlter size of 5 × 5) and 2 fully connected layers,
with ReLU and MaxPool as the activation functions. This
evaluation shows that CRYPTFLOW can be easily used for
a variety of backends.
D. Porthos experiments
Since Porthos builds on SecureNN, we compare them in
mode detail. As described earlier, Porthos improves over
the communication complexity of SecureNN [79] both for
convolutional layers as well as for non-linear activation func-
tions. We have already compared SecureNN and Porthos on
benchmarks considered in SecureNN in Table V. Additionally,
we also compare Porthos and SecureNN on ImageNet scale
benchmarks in Table VIII. For this purpose, we add the
code of SecureNN available at [5] as another backend to
CRYPTFLOW. These results show that Porthos improves upon
the communication of SecureNN by a factor of roughly 1.2X–
1.5X and the runtime by a factor of roughly 1.4X–1.5X.
Benchmark
CRYPTFLOW (s)
Communication (MB)
LogisticRegression
LeNet Small
0.227
47.4
25.5
2939
TABLE VII: CRYPTFLOW compilation to 2PC on MNIST.
Benchmark
RESNET50
DENSENET121
SecureNN
(s)
38.36
53.99
Porthos
(s)
25.87
36.00
SecureNN
Comm. (GB)
Porthos
Comm. (GB)
8.54
13.53
6.87
10.54
TABLE VIII: Porthos vs SecureNN.
Benchmark
IP10,000
IP100,000
Add32
Mult32
Millionaire32
GMW (ms)
Aramis (ms)
464
2145
279
354
292
638
3318
351
461
374
Overhead
1.37x
1.54x
1.25x
1.30x
1.28x
TABLE IX: Semi-honest GMW vs Malicious Aramis.
E. Aramis experiments
We applied Aramis to both the 2-party GMW protocol [35]
(using the codebase [2], based on [24]) as well as Porthos.
The results for different functions using the GMW protocol are
presented in Table IX. IPn denotes the inner product of two
n-element vectors over F2, Add32 and Mult32 denote addition
and multiplication over 32 bits respectively, and Millionaire32
denotes the millionaires problem that compares two 32−bit
integers x and y and outputs a single bit denoting whether
x > y. The overheads of Aramis-based malicious security,
are within 54% of the semi-honest protocol. Table IV and
Figure 10 evaluate Aramis with Porthos.
1) Comparison with crypto-only malicious MPC: We
demonstrate that Aramis based malicious secure protocols
are better suited for large scale inference tasks compared to
pure cryptographic solutions. We compare the performance
of Porthos compiled with Aramis and the concurrent work
of QuantizedNN [12] that uses the MP-SPDZ [4] framework
to also provide a malicious secure variant of their protocol.
Both these approaches provide security for the same setting
of 3PC with 1 corruption. On the four MNIST inference
benchmarks A/B/C/D in the MP-SPDZ repository, Aramis is
10X/46X/44X/15X faster.
F. Real world impact
We discuss our experience with using CRYPTFLOW to
compile and run DNNs used in healthcare. These DNNs are
available as pre-trained Keras models. We converted them
into TensorFlow using [3] and compiled the automatically
generated TensorFlow code with CRYPTFLOW.
the
train
authors
a) Chest X-Ray:
[85],
a
In
DENSENET121 to predict
lung diseases from chest X-
ray images. They use the publicly available NIH dataset
of chest X-ray images and end up achieving an average
AUROC score of 0.845 across 14 possible disease labels.
During secure inference, we observed no loss in accuracy and
the runtime is similar to the runtime of DENSENET121 for
ImageNet.
b) Diabetic Retinopathy CNN: Diabetic Retinopathy
(DR), one of the major causes of blindness, is a medical
condition that leads to damage of retina due to diabetes [64].
In recent times, major tech companies have taken an interest
in using DNNs for diagnosing DR from retinal images [64],
[37]. Predicting whether a retina image has DR or not can be
done securely in about 30 seconds with CRYPTFLOW.
VII. RELATED WORK
High level
languages. CRYPTFLOW is the ﬁrst system
to compile pre-deﬁned TensorFlow code to secure MPC
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:36:59 UTC from IEEE Xplore.  Restrictions apply. 
347
protocols. There have been prior works that compile from
lower-level, domain-speciﬁc languages to MPC. Examples
include Fairplay [56], Wysteria [70], ObliVM [54], CBMC-
GC [42], SMCL [66], [59], Sharemind [16], EzPC [19],
and SPDZ [9]. Reimplementing large DNNs in the input
format of these tools is a formidable task. PySyft [73]
and TF-Encrypted [26] are ongoing efforts that also aim to
compile DNNs to MPC protocols. In contrast to CRYPTFLOW
that compiles standard TensorFlow code, these works require
reimplementing the DNNs in a dialect of PyTorch/TensorFlow.
To the best of our knowledge, these systems have not been
evaluated on ImageNet scale tasks.
Fixed-point
in MPC. Although the use of ﬁxed-point
for secure computations is well-known [68], prior works on
secure inference have addressed the ﬂoat-to-ﬁxed problem
by either generating a ﬁxed-point model by hand ([62], [55],
[49], [60], [79], [58], [72]), or by using non-standard training
algorithms that output ﬁxed-point models ([71], [7]). Both of
these approaches are unsatisfactory. In particular, some of the
challenges that one would face with the latter include: a) the
need to train again on the whole training data which is both
computationally expensive, and impossible if the training data
is unavailable; and b) training algorithms that generate integer
models is still an active research area and an overwhelming
majority of ML training algorithms still generate ﬂoating-
point models. Athos alleviates all these problems by working
with a trained model and being completely oblivious to the
training procedure. The ML users can train their networks in
the manner they see ﬁt and then use Athos to get ﬁxed-point
code. Finally, even with retraining, TensorFlow-generated
binary/integer networks suffer signiﬁcant accuracy loses [48]
whereas Athos matches the accuracy of ﬂoating-point models.
[36],
research literature
Float-to-ﬁxed. The
in ﬂoat-to-ﬁxed
for digital signal processors is rich and spans several decades.
However, it is only recently that these schemes have been
adapted to machine learning. Some recent ﬂoat-to-ﬁxed
schemes
show promise by quantizing
ﬂoating-point models to 8-bit or 16-bit integers. One could
potentially use one of these systems in place of our ﬂoat-to-
ﬁxed component – however, their compatibility with MPC
protocols [60], [79] is unclear. Additionally, since we use
higher bit-width of 64, not surprisingly,
the accuracy of
CRYPTFLOW is better.
[65],
[48]
Secure Machine Learning. There has been a ﬂurry of
recent results ([61], [75], [57], [25], [44]) in the area of
secure machine learning, both in the 2-party [17], [67], [34],
[55], [49], [58], [84], as well as in the 3-party setting [72],
[60], [79], [12]. The most relevant to our work are ABY3 [60]
and SecureNN [79] that both provide 3-party semi-honest
secure computation protocols for a variety of neural network
inference and training algorithms, with somewhat similar
performance guarantees. Porthos, our 3-party semi-honest
protocol, outperforms both these works. We also remark
that there have been other recent works [71], [75], [7], [33],
[27], [15], [14], [39], that modify the inference or training
algorithms in order to obtain performance beneﬁts. These
are applicable only to specialized benchmarks. For example,
the works that use fully homomorphic encryption (e.g., [27],
[15],
[14]) do not support secure evaluation of ReLUs,
XONN [71] requires DNNs to have binary weights, etc. On
the other hand, we focus on standard inference algorithms
and CRYPTFLOW has much wider applicability.
Hardware-based security. Our work is the ﬁrst to provide
experimentally validated malicious secure inference of ML
algorithms at the scale of RESNET50. As discussed earlier,
we achieve this by relying on minimally secure hardware
to provide integrity. Prior works that use hardware enclaves
for secure computation [74], [69], [38], [11], [23], [29],
[51], [32], [77], [83], [44] assume that the enclave hides all
data residing in it from the host. Thus, unlike Aramis, these
systems are not secure against an adversary that can observe
the SGX state. The only prior work that assumes a weaker
trust assumption from the hardware is that of [78]. Similar to
our work, they assume that the hardware provides integrity.
However,
their work is in the context of zero-knowledge
proofs and other fundamentally asymmetric primitives that
require only one enclave and not interactive protocols between
multiple enclaves.
VIII. CONCLUSION
CRYPTFLOW is the ﬁrst end-to-end system that translates
high-level TensorFlow inference code to MPC protocols. It
has 3 components - a) compiler from TensorFlow to MPC, b)
an improved semi-honest 3PC protocol for DNNs, and c) a
generic technique to convert semi-honest secure protocols to
malicious secure ones. Using CRYPTFLOW, we demonstrate
the ﬁrst instance of secure inference on large benchmarks such
as RESNET50 and DENSENET121 on the ImageNet dataset
with both semi-honest (in about thirty seconds) and malicious
security (in less than two minutes). CRYPTFLOW’s modular
design supports a variety of backends, and we hope that it can
serve as a testbed for benchmarking new MPC protocols in
the area.
Going forward, we would like to plugin protocols like
SPDZ [4] and Delphi [58] in CRYPTFLOW. Our more am-
bitious goal is to extend CRYPTFLOW to support TensorFlow
training. It is a challenging problem since in the absence of
the GPU support, the overheads of MPC protocols for secure
training can be prohibitive.
IX. ACKNOWLEDGEMENTS
We thank our shepherd Xiao Wang, and anonymous re-
viewers for their valuable feedback. We also thank Sridhar
Gopinath, Aayan Kumar, Wonyeol Lee, Sundararajan Ren-
ganathan, and Kapil Vaswani for helpful discussions.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:36:59 UTC from IEEE Xplore.  Restrictions apply. 
348
REFERENCES
[1] “TensorFlow
Graph
Transform
Tool,”
https://github.com/
tensorﬂow/tensorﬂow/blob/master/tensorﬂow/tools/graph transforms/
README.md.
www.ee.columbia.edu/∼kwhwang/projects/gmw.html
codebase,”
Available:
[Online].
2012.
[2] “GMW
http://
[3] “Keras
to
TensorFlow,”
https://github.com/amir-abdi/keras to
tensorﬂow, 2019.
[4] “Multi-Protocol SPDZ: Versatile framework for multi-party computa-
tion,” 2019. [Online]. Available: https://github.com/data61/MP-SPDZ
[5] “SecureNN: 3-Party Secure Computation for Neural Network Training,”
https://github.com/snwagh/securenn-public, 2019.
[6] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro,
G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. J.
Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. J´ozefowicz,
L. Kaiser, M. Kudlur, J. Levenberg, D. Man´e, R. Monga, S. Moore,
D. G. Murray, C. Olah, M. Schuster,
J. Shlens, B. Steiner,
I. Sutskever, K. Talwar, P. A. Tucker, V. Vanhoucke, V. Vasudevan,
F. B. Vi´egas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke,
Y. Yu, and X. Zheng, “TensorFlow: Large-Scale Machine Learning
on Heterogeneous Distributed Systems,” CoRR, vol. abs/1603.04467,
2016. [Online]. Available: https://arxiv.org/abs/1603.04467
[7] N. Agrawal, A. S. Shamsabadi, M. J. Kusner, and A. Gasc´on, “QUO-
TIENT: Two-Party Secure Neural Network Training and Prediction,” in
Proceedings of the 2019 ACM SIGSAC Conference on Computer and
Communications Security, CCS 2019, London, UK, November 11-15,
2019, 2019, pp. 1231–1247.
[8] A. V. Aho, M. S. Lam, R. Sethi, and J. D. Ullman, Compilers: Principles,
Boston, MA, USA: Addison-
Techniques, and Tools (2nd Edition).
Wesley Longman Publishing Co., Inc., 2006.
[9] T. Araki, A. Barak, J. Furukawa, M. Keller, Y. Lindell, K. Ohara, and
H. Tsuchida, “Generalizing the SPDZ Compiler For Other Protocols,”
in Proceedings of the 2018 ACM SIGSAC Conference on Computer and
Communications Security, CCS 2018, Toronto, ON, Canada, October
15-19, 2018, 2018, pp. 880–895.
[10] T. Araki, J. Furukawa, Y. Lindell, A. Nof, and K. Ohara, “High-
Throughput Semi-Honest Secure Three-Party Computation with an
Honest Majority,” in Proceedings of the 2016 ACM SIGSAC Conference
on Computer and Communications Security, CCS 2016, Vienna, Austria,
October 24-28, 2016, 2016, pp. 805–817.
[11] R. Bahmani, M. Barbosa, F. Brasser, B. Portela, A. Sadeghi, G. Scerri,
and B. Warinschi, “Secure Multiparty Computation from SGX,” in
Financial Cryptography and Data Security - 21st International Confer-
ence, FC 2017, Sliema, Malta, April 3-7, 2017, Revised Selected Papers,
2017, pp. 477–497.
[12] A. Barak, D. Escudero, A. Dalskov, and M. Keller, “Secure Evaluation
of Quantized Neural Networks,” Cryptology ePrint Archive, Report
2019/131, 2019, https://eprint.iacr.org/2019/131.
[13] D. Beaver, “Efﬁcient Multiparty Protocols Using Circuit Randomiza-
tion,” in Advances in Cryptology - CRYPTO ’91, 11th Annual Interna-
tional Cryptology Conference, Santa Barbara, California, USA, August
11-15, 1991, Proceedings, 1991, pp. 420–432.
[14] F. Boemer, A. Costache, R. Cammarota, and C. Wierzynski, “nGraph-
HE2: A High-Throughput Framework for Neural Network Inference on
Encrypted Data,” Cryptology ePrint Archive, Report 2019/947, 2019,
https://eprint.iacr.org/2019/947.
[15] F. Boemer, Y. Lao, R. Cammarota, and C. Wierzynski, “nGraph-HE:
A Graph Compiler for Deep Learning on Homomorphically Encrypted
Data,” in Proceedings of the 16th ACM International Conference on
Computing Frontiers, CF 2019, Alghero, Italy, April 30 - May 2, 2019,
2019, pp. 3–13.
[16] D. Bogdanov, S. Laur, and J. Willemson, “Sharemind: A Framework
for Fast Privacy-Preserving Computations,” in Computer Security -
ESORICS 2008, 13th European Symposium on Research in Computer