Process 
Stage
Output 
Stage
Fig. 2. Pull-based work distribution and thread & core binding design.
request is done by the same processor core. By doing this,
data generated and used during the whole process will not
have to be moved from the cache of one core to another one.
To achieve this, the task manager framework is designed as
shown in Figure 2. For input stage tasks, all worker threads
still share the same raw-packet task queue, so that they can
pick up pending raw packets when they are idle. In addition,
each worker thread has its own dedicated task queue. All
the tasks for the (cid:3)ow process stage and output stage will be
submitted to this dedicated task queue of the worker thread
in which the input stage is executed. A worker thread always
tries to pull tasks from its own dedicated task queue (cid:2)rst, and
only when that queue is empty does it access the shared raw-
packet task queue. This design ensures not only that work load
is evenly distributed among worker threads, but also that all
computations related to one (cid:3)ow request are processed by the
same worker thread. We call this design the (cid:147)thread-binding(cid:148)
In Section III-E of the evaluation, we quantitively show that
both the (cid:147)core-binding(cid:148) and the (cid:147)thread-binding(cid:148) designs are
critical to scale up the througput of the system.
Minimize Memory Consumption
When the rate of incoming (cid:3)ow requests exceeds the
throughput of the system, data such as raw packets, (cid:3)ow
requests generated by the input stage, and con(cid:2)guration mes-
sages generated by the output state will accumulate. If too
much data is pending in queues to be processed, it will not
only consume a lot of memory, but also greatly hurt the
system performance. This is because in general more memory
used means worse locality for the memory hierarchy, and
exhausting the memory could have disastrous performance
consequences. Moreover, because Maestro is implemented
in Java, the overhead of garbage collection becomes very
high when a large amount of memory is used, thus memory
allocation will become extremely slow also. The ultimate goal
is to minimize the memory consumption, while making sure
the system has enough buffered data to be processed. The
latter part is also very important. If not enough raw packets are
buffered, when worker threads drain all pending raw packets
including those within the socket buffers, it will take some
time for TCP to re-grow the congestion window and feed
enough requests again to Maestro, especially when the socket
buffer is small and the network latency is large. This could
potentially lead to CPU idling.
Thus, thresholds must be carefully con(cid:2)gured in the system
to keep any queue from growing too large but remaining at a
reasonable size. However, having one threshold for each queue
is not a good design, because there will be many thresholds to
tune. Whenever some conditions change signi(cid:2)cantly (e.g. a
network latency change), these thresholds have to be adjusted.
The more thresholds there are, the more dif(cid:2)cult it is to come
up with a good con(cid:2)guration. The solution to this problem
is to make sure that there is no data accumulated in queues
between the input and (cid:3)ow process stage, and between the (cid:3)ow
process stage and the output stage. To achieve this, we employ
strict priority in scheduling tasks to worker threads, and we
give different priorities to the tasks for the different stages.
Tasks for the output stage receive high priority, tasks for the
(cid:3)ow process stage receive medium priority, and tasks for the
input stage naturally receive low priority because they are in
the shared raw-packet task queue. As a result, the only queue
that can become backlogged is the raw-packet task queue. In
order to minimize memory consumption, there only needs to
be one threshold con(cid:2)gured: how many pending raw packets
are allowed in the raw-packet task queue to be processed by the
input stage, which we call the (cid:147)pending raw-packet threshold(cid:148),
or PRT in short. In addition, we have a dedicated thread that
receives incoming raw packets from the socket buffers. Since
receiving from sockets is very lightweight compared to (cid:3)ow
request processing, this dedicated thread can keep the queue
(cid:2)lled to the threshold without requiring many CPU cycles.
Receiving
Pauses
Pending 
Requests
Receiving
Pause
Idle
Time
Fig. 3. Good PRT vs bad PRT.
Good
PRT
Bad
PRT
In Maestro, when there are more pending raw-packet tasks
than the PRT, the receiving thread is paused. When there
are fewer requests than the PRT,
the receiving thread is
resumed. The PRT could and should be calibrated before
an initial deployment of Maestro, and again when network
conditions such as the topology and the network latency
change signi(cid:2)cantly. Usually a network with a higher request
input rate and a higher latency requires a larger PRT, and
vice versa. This calibration can be achieved by injecting (cid:3)ow
requests from a network emulator at a rate higher than the
system throughput to Maestro. The use of such a high rate is
important because the calibrated PRT will guarantee that there
will be enough room for any request rate while achieving high
memory ef(cid:2)ciency. Figure 3 shows the difference between a
correct and an incorrect PRT. If the PRT is chosen to be too
small, after the receiving thread pauses, the worker threads
could drain the raw-packet tasks so fast that they will become
idle even before new requests are received because of the TCP
ramp-up delay. A good choice of PRT is the minimum value
that can make sure that under the maximum input rate there
is no case when the raw-packet task queue is ever completely
drained. In the evaluation section we will use our emulation
tool to calibrate the PRT for Maestro.
C. Batching
There is unavoidable overhead associated with each of the
three stages in the (cid:3)ow request execution path. For example,
Maestro needs to start an instance of the (cid:147)Authentication(cid:148) and
(cid:147)RouteFlow(cid:148) applications, and needs to pass data between
any two stages and also between two applications. Another
example is the overhead of socket send() system calls.
When the incoming rate of (cid:3)ow requests is very high, any of
the overheads mentioned above could become signi(cid:2)cant. As
a result, we propose to use batching mechanisms to minimize
such overhead.
Input Batching
For each worker thread, there is one input batching queue
in the input stage to temporarily hold the (cid:3)ow requests that
it generated from the raw packets. This input batching queue
enables the input stage to create a (cid:3)ow process stage task that
can process multiple (cid:3)ow requests in one single execution. The
two applications of the (cid:3)ow process stage are implemented to
be aware of the input batching queue. They should process
all the pending (cid:3)ow requests in the input batching queue in
one batch during one execution. However, if we allow the
input batching queue to grow very large, it will consume a
lot memory and increase the average delay experienced by all
(cid:3)ow requests. Thus, there is a tunable threshold which decides
when the input stage should create a (cid:3)ow process stage task
that processes all (cid:3)ow requests in the input batching queue.
We call this threshold the (cid:147)input batching threshold(cid:148), or IBT
for short.
When the rate of incoming (cid:3)ow requests is lower than the
throughput of the system, from time to time worker threads
will have no pending task to run. We still keep the same IBT,
but whenever a worker thread (cid:2)nds that the raw-packet task
queue is about to be drained out, it will force the input stage to
create a (cid:3)ow request stage task with as many requests as there
are currently in the input batching queue, ignoring the IBT.
Through this simple design, we can achieve dynamic batching
adjustment that responds to the changing load of the system, so
that the average processing delay is kept low, without having
to develop a threshold adjustment algorithm.
The IBT could and should be calibrated once at initial
deployment and when network conditions such as the topology
and the network latency change signi(cid:2)cantly. This calibration
is done under a rate of incoming (cid:3)ow requests that is larger
than the throughput of the system. The same value will work
for a lower rate as discussed in the previous paragraph. During
the calibration, a value is chosen where the average delay is
minimized while the throughput is maximized.
Output Batching
Because for each socket send() call there are both
(cid:2)xed and variable, per-byte overheads, when there are multiple
messages to be sent to the same destination, sending them
all in one socket send() call can be much less expen-
sive than sending each of them individually. We conduct a
microbenchmark experiment to demonstrate this, the result is
shown in Figure 4.
d
n
o
c
e
s
o
r
c
m
i
,
t
n
e
p
s
e
m
i
t
l
a
t
o
T
 160
 140
 120
 100
 80
 60
 40
 20
W/O batching
With batching
 0
 5
 10
 20
 15
 35
Number of packets to send
 25
 30
 40
 45
 50
Fig. 4. Microbenchmark for output batching.
In this microbenchmark, we vary the number of 100-byte
messages (a typical size for OpenFlow messages) to send to
the same destination from 1 to 50. In the (cid:2)rst experiment, we
send each of them individually, and in the second experiment
we send all of them together with one socket send() call.
We run each experiment 100 times and measure the average
time spent in each run. As shown in the (cid:2)gure, the time for
sending all messages together grows much slower than that
for sending them individually.
To reduce this overhead in Maestro, we perform output
batching. When the (cid:3)ow process stage processes one batch of
(cid:3)ow requests, and generates a set of messages that need to be
sent, we (cid:2)rst group these messages by their destinations. Then,
all the messages for the same destination are sent together
in one single socket send() call. If there are too many
bytes to send that it cannot be done with only one call, we
will try multiple calls. For each call we will send as many as
possible, which is determined by the socket’s then available
buffer space. In addition, because only one thread is allowed
to call the socket send() on one socket at a time, we
add the following feature to further minimize the wait time.
When a worker thread tries to call socket send() on one
socket but (cid:2)nds out another worker thread is already locking
that socket, instead of waiting, the thread will process other
pending output stage tasks in its dedicated task queue in a
round-robin fashion, until it (cid:2)nds one socket that is not being
locked. This solution greatly improves the output ef(cid:2)ciency.
III. EVALUATION
In this section, we evaluate Maestro through a number of
different scenarios, to show how our design choices affect
the system’s performance. Also, we compare Maestro against
NOX (version 0.8.0 full beta, checked out on Jul-10-2010,
compiled without the ndebug option enabled) to see how much
bene(cid:2)t we get from addressing NOX’s limitations. Because
the design of the security policy data structure and checking
algorithm is not the focus of this paper, in the evaluation
we use a simple policy that allows all (cid:3)ow requests in
the (cid:147)Authentication(cid:148) application in Maestro, and also in the
corresponding application in NOX.
A. Experiment Setup and Methodology
We implement a network emulator which can emulate all
OpenFlow switch control plane functionalities and can gener-
ate (cid:3)ow requests at different rates. The emulator also forwards
control plane LLDP packets to help the controller discover
the network topology. It also delivers all (cid:3)ow con(cid:2)guration
messages to the emulated target switches.
We run Maestro on a server machine with two Quad-Core
AMD Opteron 2393 processors (8 cores in total) with 16GB of
memory. Because there is one dedicated thread for receiving
requests, and there are several other threads responsible for
managing the Java virtual machine (such as class management
and garbage collection), we dedicate one processor core for
such functionalities, while the remaining 7 cores are used by
worker threads. Thus the best throughput is achieved with 7
worker threads on this 8 core server machine. This machine
has four PCI-E 1Gbps NICs to provide enough network
bandwidth. At least 3Gbps network bandwidth is necessary
because for each request the controller not only needs to
generate several (depending on the length of the chosen path)
(cid:3)ow con(cid:2)guration messages for switches along the path, but
also needs to bounce the (cid:3)ow request packet itself back to its
origin. So when Maestro achieves its maximum throughput,
it needs to send out data at a rate of around 2.5Gbps. The
machine is running Ubuntu 9.04 with a 2.6.31 kernel, and is
con(cid:2)gured with a 64-bit 1.6.0 19 JDK.
Maestro 
Machine M
Gigabit Switch
Emulator 
Machine B
Emulator 
Machine A
Emulator 
Machine C
Fig. 5. Experiment emulator setup.
Because for the emulator we only have machines with one
1Gbps NIC, we need at least three emulator machines to be
able to test the maximum throughput of Maestro. Thus we
run the emulator distributedly on three machines as shown
in Figure 5. There is one master machine A and two slave
machines B and C, and each of them is connected to a gigabit
Ethernet switch by a 1Gbps link. Maestro is running on the
server machine M, and has three 1Gbps links to the gigabit
switch. Each of the emulator machines emulates one third of
the OpenFlow switches in the emulated network, and each
of them has one separate 1Gbps path, A-M, B-M and C-M,
to reach M. Flow requests generated on them will be sent
to Maestro, and con(cid:2)guration messages will be sent from
Maestro to their target machines, all along these three paths.
In addition each slave machine also has one path to the master
machine A, B-A and C-A. When LLDP packets need to be sent
from one switch to its neighbor switch on another emulator
machine, it will be forwarded, via the master machine A if
necessary. For example, assuming switch X is on B and its
neighbor switch Y is on C, the LLDP packet from X to Y will
(cid:2)rst reach A and then get forwarded to C by A. Although B-
A and C-A share bandwidth with B-M and C-M respectively,
because the LLDP traf(cid:2)c is very light, Maestro’s throughput
will not be affected.
To provide a common basis for direct comparisons between
Maestro and NOX, we run experiments on a common 79-
switch topology [14]. For each emulated switch, the emulator
emulates one end host connected to it. We generate (cid:2)fteen mil-
lion requests between randomly chosen source and destination
end host pairs to get aggregate performance measurements.
All the requests are generated at uniform rate without bursty
behavior.
We use two metrics for measuring the performance. The
(cid:2)rst one is Maestro’s throughput in requests per second (rps),
for which a larger value is better. The second one is the delay
experienced by the (cid:3)ow requests, for which a smaller value
is better. This delay is the end-to-end delay measured by the
emulator. The emulator creates a request, records the starting
timestamp, and calls the blocking socket send function. When
the emulator receives the request itself bounced back from
Maestro, it calculates the end-to-end delay using the starting
timestamp.
B. Calibrating the PRT
When calibrating the PRT value to use for a network, we
let the emulator generate (cid:3)ow requests as fast as possible,
regulated only by the blocking TCP send call. The emulated
network environment which we use has a small latency, and
can send requests at a rate larger than the maximum throughput
Maestro can achieve. Together with measuring the throughput,
we also measure how many times during an experiment there
are zero pending requests, which means potential idling for
the worker threads. A good choice of the PRT value for a
particular network is the smallest value that by using it we
can achieve the best throughput, and have no situation where
there are zero pending requests left while the switches are still
trying to send more.
d
n
o
c
e
s
/
s
t
s
e
u
q
e
r
w
o
l
f
,
t