rupt kernel memory affecting a large range of kernel versions
Memory Utilization: Initially, our main concern regarded
the memory requirements, because of the size of the interme-
diate representation of the kernel as bitcode image. However,
our approach to partition the kernel per system call proved to
9
Figure 6: Maximum memory requirements of K-Miner for system calls requiring more than 11G of RAM.
be effective: on average the analyses utilized between 8.7G and
13.2G of RAM, i.e., around a third of our server’s memory,
with a maximum of 26G (cf., version 4.10 in Table II). We
provide a more detailed overview for different system calls in
Figure 6. Granted that these numbers also depend to a large
extent on the respective kernel version and used conﬁguration,
our overall results demonstrate that complex data-ﬂow analysis
for OS kernels are feasible and practical. In particular, the
memory requirements of K-Miner show that an analysis of
future kernel releases is realistic, even with the tendency of
newer versions to grow in size.
While the default conﬁguration for the kernel offers a
good tradeoff between feature coverage and size, real-world
distribution kernels usually have larger conﬁgurations, because
they enable a majority of features for compatibility reasons.
Our current memory utilization is within range of analyzing
kernels with such feature models as well. Although we expect
to see increased memory requirements (i.e., 128G or more),
this does not meet the limit of modern hardware, and K-Miner
is able to conduct such analyses without requiring any changes.
C. Usability
Our framework can be integrated into the standard build
process for the Linux kernel with some changes to the main
build ﬁles, which will then generate the required intermediate
representation of the kernel image. Using this bitcode image
as main input, K-Miner can be conﬁgured through a number
of command line arguments, such as number of threads,
which checkers to use, and output directory for intermediate
results. Results are written to a logﬁle, which can be inspected
manually or subsequently rendered using our web interface to
Version
Avg. Used
Max Used
3.19
4.2
4.6
4.10
4.12
18,073.60M
8,765.08M
13,232.28M 24,466.78M
11,769.13M 22,929.92M
12,868.30M 25,187.82M
13,437.01M 26,404.82M
Table II: Average and maximum memory usage of K-Miner
10
get an overview and check reports for false positives.
Reporting Engine: The web interface for our framework
is written in Python. It parses the resulting logﬁle to construct
a JSON-based data model for quick graphing and tabular
presentation. We attached screenshots in Appendix A to give
an impression of an exempliﬁed workﬂow. While relatively
simple, we found this web-based rendering to be extremely
helpful in analyzing individual reports. Developers can already
classify and comment upon alerts and reports, and we plan to
incorporate the possibility to schedule and manage the launch
and conﬁguration of analyses from the web interface in future
versions.
False Positives: Similar to other static analysis approaches
like the Effect-Based Analyzer (EBA) [1], Coccinelle [52],
Smatch [9], or APISAN [74], K-Miner naturally exhibits
a number of
false positives due to the necessary over-
approximations. For instance, the use-after-free analysis still
shows a high number of false alarms, and leaves room for
improvement. In particular, our investigation showed that there
are many cases in the kernel code where a conditional branch
based on a nullness check is reported as potential use-after-
free. Including these cases in our sanitizer component should
be straightforward to further reduce this number. However,
there will always be a certain number of false positives for
any static analysis tool and developers have to cross-check
these alerts, similar to how they have to check for compiler-
warnings. Overall K-Miner demonstrates that this scenario is
practical through some post-processing and intelligent ﬁltering
in our web-interface.
VI. DISCUSSION
In this section we discuss our ongoing improvements of
K-Miner, various possible extensions, and future work.
A. Soundness
While K-Miner currently does not offer a proof of sound-
ness, we sketched an informal reasoning of why the kernel-
code partitioning along the system call API is a sensible
strategy in Section V. There are additional challenges for a
formal result: ﬁrst, in some cases the kernel uses non-standard
code constructs and custom compiler extensions, which may
not be covered by LLVM. However, for these constructs the
LLVM Linux project maintains a list of patches, which have
to be applied to the kernel to make it compatible to the LLVM
compiler suite. Second, some pointer variables are still handled
via unsigned long instead of the correct
type. These
low-level “hacks” are difﬁcult to handle statically, because
they exploit knowledge of the address space organization or
underlying architecture speciﬁcs. Nonetheless, such cases can
be handled in principle by embedding the required information
in LLVM or by annotating these special cases in the source.
Finally, our memory tracking component currently relies on
a list of allocation functions. For cases like ﬁle descriptors
or sockets the respective kernel objects are pre-allocated from
globally managed lists and individual objects are retrieved and
identiﬁed by referring to their ID (usually an integer number).
This can be resolved by considering all objects from the same
list to be modeled as objects of the same type, and marking
functions for retrieval as allocations.
B. Future Work
As K-Miner is designed to be a customizable and extensible
framework, implementing additional checkers is straightfor-
ward. To this end, we already implemented additional double-
lock and memory-leak checkers, thereby covering additional
bug classes. Up to this point we only veriﬁed that these addi-
tional pass implementations are able to detect intra-procedural
bugs.3 However, as our other analysis passes in K-Miner, the
double-lock implementation covers inter-procedural double-
lock errors in principle,
including bugs spanning multiple
source ﬁles. Similarly, implementing analyses to ﬁnd buffer
overﬂows, integer overﬂows, or uninitialized data usage re-
mains as part of our future work to cover all potential sources
of memory corruption as mentioned in Section II.
While primarily analyzing the system call API, we found
that analyzing the module API in a similar way should be
possible and provide interesting results, since many bugs result
from (especially out-of-tree) driver and module code. Although
this interface is not as strict as the highly standardized system
call API, the main top-level functions of many drivers are
exported as symbols to the entire kernel image, while internal
and helper functions are marked as static. Hence, we should
be able to automatically detect the main entry points for most
major driver modules by looking at its exported symbols and
building a call graph that starts with the exported functions. We
can then analyze this automatically constructed control ﬂow
of the drivers by applying the data-ﬂow analysis passes to the
resulting code partitions. In addition to our current approach,
this would allow for an extension of our adversary model to
include malicious devices and network protocols. We included
a prototypical draft of this functionality to analyze module
code using K-Miner in the future.
C. Automated Proof-of-Concept Generation
Finding a valid user-space program to provide the neces-
sary input data to reliably trigger a bug is non-trivial in many
cases. At the same time, kernel developers will often ignore
bug reports without a proof-of-concept. K-Miner’s reports
already contain all the necessary path information, and hence,
3In particular,
the lock errors introduced in commits 09dc3cf [53],
e50fb58 [13], 0adb237 [18], and 16da4b1 [2] of Linus’ tree.
it should be feasible to ﬁnd matching inputs that
lead to
the execution of that particular path, e.g., by processing the
path constraints using a SAT-solver [21]. Alternatively, we
could leverage concolic execution [8] or selective, guided
fuzzing [58] to generate such proof-of-concepts.
D. Machine Learning
Another possible perspective for interesting future work
is to combine our static analysis framework with machine
learning, such as deep learning, reinforcement learning, and
classiﬁer systems. This would allow for the extraction of
common bug patterns and automated pattern mining [73], or
scalable classiﬁcation of generated vulnerability reports, e.g.,
to build a ranking system for K-Miner’s generated reports
similar to how APISAN handles the large number of detected
semantic function API violations [74]. One of the problems of
machine learning approaches is that their results highly depend
on the quality of the training data [31].
VII. RELATED WORK
In this section we give a brief overview of the related
work and compare K-Miner
to existing frameworks and
tools. In contrast to dynamic run-time approaches, such as
KASAN [39], TypeSan [27], Credal [71], UBSAN [40], and
various random testing techniques [34], [35], [22], our ap-
proach aims at static analysis of kernel code, i.e., operating
solely during compile time. As there already exists a large
body of literature around static program analysis [51], [41],
we focus on static analysis tools targeting operating system
kernels, and data-ﬂow analysis frameworks for user space that
inﬂuenced the design of K-Miner.
It is important to note that applying static analysis frame-
works designed for user space programs is not possible a priori
in the kernel setting: data-ﬂow analysis passes expect a top-
level function, and an initial program state from which analysis
passes can start to propagate value ﬂows. These requirements
are naturally satisﬁed by user space programs by providing a
main function, and a complete set of deﬁned global variables.
However, operating systems are driven by events, such as
timer interrupts, exceptions, faults, and traps. Additonally, user
space programs can inﬂuence kernel execution, e.g., by issuing
system calls. Hence, there is no single entry point for data-ﬂow
analysis for an operating system. With K-Miner we present the
ﬁrst data-ﬂow analysis framework that is speciﬁcally tailored
towards this kernel setting.
A. Kernel Static Analysis Frameworks
The Effect-Based Analyzer (EBA) [1] uses a model-
checking related, inter-procedural analysis technique to ﬁnd
a pre-compiled list of bug patterns. In particular, it provides
a speciﬁcation language for formulating and ﬁnding such
patterns. EBA provides lightweight, ﬂow-insensitive analyses,
with a focus towards double-lock bugs. Additionally, EBA re-
stricts analysis to individual source ﬁles. K-Miner provides an
expressive pass infrastucture for implementing many different
checkers, and is speciﬁcally tailored towards the execution
model of the kernel allowing complex, context and ﬂow-
sensitive data-ﬂow analyses, potentially spanning the entirety
of the kernel image.
11
Coccinelle [52] is an established static analysis tool that
is used on a regular basis to analyze and transform series
of patches for the kernel. While originally not intended for
security analysis, it can be used to conduct text-based pattern
matching without the requirement for semantic knowledge or
abstract interpretation of the code, resulting in highly efﬁcient
and scalable analyses. In comparison to our framework, Coc-
cinelle is not able to conduct any data-ﬂow, or inter-procedural
analysis.
The Source-code Matcher (Smatch) [9] is a tool based on
Sparse [66], a parser framework for the C language developed
exclusively for the Linux kernel. Smatch enriches the Sparse
syntax tree with selected semantic information about underly-
ing types and control structures, enabling (limited) data-ﬂow
analyses. Like Coccinelle, Smatch is fast, but constrained to
intra-procedural checks per source ﬁle.
APISAN [74] analyzes function usage patterns in kernel
code based on symbolic execution. In contrast to other static
analysis approaches, APISAN aims at ﬁnding semantic bugs,
i.e., program errors resulting from incorrect usage of existing
APIs. Because specifying the correct usage patterns manually
is not feasible for large code bases, rules are inferred proba-
bilistically, based on the existing usage patterns present in the
code (the idea being that correct usage patterns should occur
more frequently than incorrect usage patterns). In comparison
to K-Miner, APISAN builds on LLVM as well, but only
considers the call graph of the kernel and is not able to conduct
any inter-procedural data-ﬂow analyses.
TypeChef [38] is an analysis framework targeting large C
programs, such as the Linux kernel. In contrast to our work,
TypeChef focuses on variability-induced issues and analyzing
all possible feature conﬁgurations in combination. For this, it
provides a variability-aware pre-processor, which extracts the
resulting feature model for the kernel, e.g., by treating macros
like regular C functions. TypeChef does not conduct any data-
ﬂow analysis on their resulting variability-aware syntax tree.
B. User Space Static Analysis
The Clang Static Analyzer [46] consists of a series of
checkers that are implemented within the C frontend Clang
of the LLVM compiler suite. These checkers are invoked via
command-line arguments during program compilation and can
easily be extended. As part of the Linux LLVM project [69]
there was an effort
to implement kernel-speciﬁc checkers.
However, to the best of our knowledge, this effort has since
been abandoned.
The Static Value-Flow (SVF) [59] analysis famework en-
hances the built-in analysis capabilities of LLVM with an
extended pointer analysis and a sparse value-ﬂow graph repre-
sentation. K-Miner builds on top of LLVM and leverages the
pointer analyses provided by SVF to systematically analyze
kernel APIs, such as the system call interface.
M´elange [56] is a recent data-ﬂow analysis framework
for user space, that is able to conduct complex analyses to
ﬁnd security-sensitive vulnerabilities, such as unitialized reads.
M´elange is able to analyze large C and C++ user space code
bases such as Chromium, Firefox, and MySQL.
Astr´ee [15] is a proprietary framework for formal veri-
ﬁcation of C user programs for embedded systems through
elaborate static analysis techniques. It operates on synchronous
programs, i.e., analyzed code is not allowed to dynamically
allocate memory, contain backward branches, union types, or
other conﬂicting side effects. Astr´ee is able to provably verify
the absence of any run-time errors in a program obeying these
restrictions and was used to formally verify the primary ﬂight
control software of commercial passenger aircraft.
Soot [68] is a popular and widely used static analysis
framework capable of conducting extensible and complex
data-ﬂow analyses. However, Soot is targeted towards Java
programs, and hence cannot analyze programs written in C
or C++.
VIII. CONCLUSION
Memory-corruption vulnerabilities represent an important
challenge for the security of today’s operating systems. Any
instance of one of these bugs exposes the system to a variaty
of run-time attacks. Such attacks therefore pose a severe
threat to the OS, since they can be launched by unprivileged
user processes to exploit a particular vulnerability, e.g., by
corrupting memory used by the kernel to gain read and write
access to kernel space. This access can then be exploited to
escalated privileges of the attacker process to root or achieve
arbitrary code execution in the kernel.