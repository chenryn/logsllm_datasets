# FTP: The Forgotten Cloud

**Authors:**
- Drew Springall
- Zakir Durumeric
- J. Alex Halderman

**Institution:**
University of Michigan

**Contact:**
- {aaspring, zakir, jhalderm}@umich.edu

**Abstract:**
The File Transfer Protocol (FTP) has largely been supplanted by HTTP, SCP, and BitTorrent for data transfer between hosts. However, our comprehensive analysis of the FTP ecosystem as of 2015 reveals that over 13 million FTP servers still exist in the IPv4 address space, with 1.1 million allowing "anonymous" (public) access. These anonymous FTP servers often leak sensitive information, such as tax documents and cryptographic secrets. Additionally, more than 20,000 FTP servers allow public write access, facilitating malicious activities like free storage, malware deployment, and click-fraud attacks. We further investigate real-world attacks by deploying eight FTP honeypots, providing insights into how attackers exploit vulnerable servers. Our study concludes with recommendations for securing FTP.

## I. Introduction

The File Transfer Protocol (FTP), first introduced nearly 45 years ago [7], was the primary protocol for file transfer and distribution for decades [17]. In recent years, FTP has been overshadowed by newer protocols like HTTP, SCP, and BitTorrent, which have received more attention from security researchers. Despite this, as of 2015, FTP remains in use by millions of servers, offering over half a billion files to the public. It is largely forgotten but far from gone.

We present the first comprehensive security analysis of how FTP is used and abused in modern practice. Using Internet-wide scanning, we characterized the contemporary FTP server ecosystem, finding 13.8 million servers in the public IPv4 address space. Of these, 1.1 million (8%) permit anonymous logins, making their contents accessible to the public. Many of these publicly accessible servers are operated by large hosting providers, but a significant fraction are consumer devices mistakenly configured to allow public access.

Publicly accessible FTP sites host a concerning number and variety of sensitive files, suggesting widespread misconfiguration. To measure this, we developed a robust FTP enumerator to collect directory listings from over a million anonymous FTP sites. Our toolchain collected listings of over 600 million files and directories, ranging from financial information to email archives, password databases, private keys, and personal photographs. Shockingly, nearly 5% of anonymous FTP servers expose at least one such sensitive file.

Beyond information exposure, FTP is prone to abuse by malicious parties who seek to attack the server or use it to exploit other systems. Using data from our FTP enumerator and a series of honeypots, we uncovered evidence of several malicious campaigns leveraging anonymous FTP servers to distribute malware, launch DDoS attacks, and carry out SEO campaigns. Over 20,000 FTP servers allow anonymous users to write data, which malicious actors use to deploy malware and trade illicit files. Additionally, over 140,000 FTP servers fail to properly validate PORT commands, which can be used to probe remote, third-party servers. Nearly 10% of FTP servers listening on public IP addresses report software versions susceptible to one or more publicly disclosed vulnerabilities.

Regarding the security provided by FTPS (an extension allowing FTP connections to communicate over TLS), we analyzed adoption rates and implementation. Only 793,000 certificates are in use across 3.4 million servers supporting FTPS. While many are shared-hosting providers, we also found that embedded device manufacturers ship identical FTPS certificates and private keys built into their devices.

We conclude by identifying the root causes of FTP's persistent vulnerability, offering potential solutions to improve the FTP ecosystem, and drawing lessons about user-centered security issues that apply beyond FTP. Along with these solutions, we analyze possible methods of encouraging their deployment.

FTP is a product of a time when security was less of a focus than it is today. Although the protocol continues to be implemented and deployed, the FTP ecosystem has only marginally advanced in terms of security. The vast number of vulnerable systems and sensitive files—and their persistence up to the present—is shocking. Our study presents a grim portrait of FTP deployment in 2015, but we hope that by shedding light on these ongoing vulnerabilities, the network security community can begin to address them.

## II. Background

FTP was introduced in 1971 to allow users to transfer files between network hosts [7]. Clients send text requests in the form of "[arguments]\r\n" to the server and extract a three-digit return code and other request-dependent information from the server’s response to determine whether the request was successful. In a typical scenario, a client initiates a connection on TCP/21, and, after receiving a "banner" containing arbitrary text from the server, logs in with the USER and PASS commands. Once authenticated, the client can list and traverse the accessible directory structure and upload and download files (depending on the permissions set by the administrator).

A peculiarity of the protocol is that FTP requires two connections: one for control messages and one for transferring the requested data. In traditional active FTP, the client sends the PORT command whose arguments indicate the client’s IP address and an open, ephemeral port that the server should connect back to using a second connection. This is incompatible with many firewalls and NATs, which cannot detect that the inbound connection is associated with the original outgoing FTP connection. To address this, passive FTP was introduced, in which the client sends the PASV command, and the server responds with and listens on an ephemeral port that the client opens a second connection to [6]. Regardless of whether the connection is active or passive, the client can then send requests via the control connection to list directory contents and retrieve or store files, which are transmitted via the secondary (PORT/PASV negotiated) connection.

Although FTP provided an easy and efficient way to transfer files, mandatory authentication hindered publicly posting data. To address this, the protocol was extended to support anonymous FTP, which allows administrators to explicitly allow public access. To use anonymous FTP, the client authenticates with the username "anonymous" and their contact email address as the password (if the server requires one). A server configured to allow anonymous FTP will accept any password for the anonymous user [17].

As with many early protocols, FTP was designed with minimal consideration for security, sending both commands and data in unauthenticated, unencrypted form. To address this, FTPS was introduced [3], [26]. FTPS allows the endpoints to upgrade the connection to TLS, similar to STARTTLS for SMTP. The client sends the AUTH SSL or AUTH TLS request to the server and reads the response to determine if TLS is supported. If so, the client and server complete a standard TLS handshake and then continue with the FTP protocol over the secure connection.

This patchwork of extensions—some described in RFCs and some not—has resulted in diverse behavior by different FTP implementations. Server responses to the USER login request are a prime example. The return code 331 has at least four meanings depending on the implementation- and language-specific text that accompanies it: "User accepted, send password," "User rejected," "Send virtual-site hostname with username," or "FTPS required prior to login."

While other protocols might fragment under such loose standardization, FTP has been surprisingly resilient. Bare-bones FTP clients can communicate with most server implementations, largely due to the human-centered nature of the protocol. FTP clients perform very little of the "heavy lifting" for the user and mainly serve to make FTP communication less tedious. For many operations, replacing a console-based FTP client with a bare TCP connection would result in little additional work for the user.

Unfortunately, this level of user control is an obstacle to large-scale automation and to our goal of analyzing FTP behavior on an Internet-wide basis. To study the FTP ecosystem as a whole, we needed to build tools that could carry out all actions autonomously while being robust enough to correctly communicate with diverse real-world implementations.

## III. Methodology

To survey FTP at Internet scale, we addressed three main challenges. The first was automating the handling of the FTP protocol, with its quirks and myriad implementations. We adopted a reverse-engineering perspective, starting with a simple enumerator that implemented a minimal subset of the FTP protocol. We tested this on a local testbed consisting of a diverse collection of server implementations. After ensuring correct behavior on our testbed, we tested against gradually larger random samples of live servers. By iteratively expanding the capabilities of the enumerator and reactively adjusting its behavior to oddities found in the wild, we achieved a good balance of RFC correctness and compatibility with real implementations.

The second challenge was efficiently collecting data from FTP servers throughout the IPv4 address space. We used the ZMap toolchain [23] coupled with our custom FTP enumerator. In the first stage of our data collection, we used ZMap to perform a host discovery scan on TCP port 21. We then used our enumerator to follow up with each responsive host, attempt an anonymous login (per RFC 1635 [17]), parse each host’s robots.txt file, and traverse the host’s directory structure in a breadth-first manner. Once we finished traversing any publicly accessible directories, we collected the data returned by the HELP, FEAT, and SITE commands. Regardless of whether the server allowed anonymous access, we attempted to initiate a TLS session prior to disconnecting to collect the server’s SSL certificate. Our enumerator is written in C using the libevent framework [36] and is publicly available at https://github.com/aaspring/ftp-enumerator.

The last main challenge was processing and analyzing the resulting data, which is largely unstructured. Server banners contain arbitrary text, users name files in varying manners and in different languages, and, in some cases, filenames may not describe file content. To sift through this data and establish lower bounds on vulnerability and data exposure, we iteratively processed the dataset, manually selecting and investigating specific evidence of abuse or accidentally exposed data. After each iteration, we measured the number of servers displaying the same or similar behavior. Although this provides an estimate of the range and scope of vulnerability, it may result in the statistics we report underestimating the true scale of the problems.

Since many files appeared to contain sensitive data inadvertently made public, we chose not to download files in bulk using our enumerator. For high-level statistics, we attempted to infer file contents based on the filename and extension. Without attempting downloads, we cannot determine with certainty whether the anonymous FTP user has permission to read the files. To address this, we examined the all-users permission in directory listings to determine whether an anonymous user could likely retrieve each specific file. In cases where the server did not display permissions (as with most Windows-based servers), we labeled the files as "unk-readability."

### A. Ethical Considerations

As with any research conducted through Internet-wide scanning, our work raises important ethical considerations. We carefully considered the impact of our experimental measurements on parties ranging from our local institutional network to the owners of remote systems and took numerous steps to prevent or mitigate potential harms.

When scanning for FTP sites, we followed the recommendations set forth by Durumeric et al. [23]. We coordinated with our local network administrators and upstream ISP to ensure that our scans did not adversely impact network operations. We signaled the benign intent of our scanning hosts by setting descriptive WHOIS records and reverse DNS entries for them and posting a simple website on port 80 that described the goals of the research, including what data we collected and how to contact us. We invited user exclusion requests and responded to requests within 24 hours, and we preemptively excluded any hosts that our institution had previously been asked to exclude from scanning research as part of other studies.

When logging into FTP servers, we never attempted to guess login credentials or to exploit vulnerabilities to access non-public data. We also made a concerted effort to parse FTP banners for messages stating that the server did not permit anonymous access and discontinued the login attempt in that case. We strictly followed RFC 1635 ("How to Use Anonymous FTP") [17]. If the server required a password for the anonymous login, we sent our team’s abuse contact email address.

When traversing sites, we followed the community’s Robots Exclusion Standard, fetching each host’s robots.txt file, if present, and following it per Google’s specification [29]. To ensure that we did not inundate a server with requests, we spread concurrent connections across a large number of widely dispersed hosts and limited the speed of interactions with each host to two requests per second. We also imposed a maximum of 500 requests per connection. If the server terminated the connection at any point during directory traversal, we interpreted this as an explicit refusal of service and ceased interaction with that server.

As we will discuss in the remainder of the paper, we were surprised to find that a significant fraction of the data available via anonymous FTP appears to have been inadvertently published. For this reason, we stopped short of downloading files except in a few particular instances as necessary for verification and even then only after careful deliberation and consultation with colleagues. Despite the fact that these files and directory listings are publicly accessible, there would be significant risk in publishing an exhaustive list of files that could then be trivially retrieved and potentially abused. As such, we do not intend to publish our enumeration dataset. We are working to notify responsible entities in likely instances of sensitive information disclosure.

## IV. FTP Landscape

Between June 18 and 21, 2015, we performed a scan of the IPv4 address space and enumerated publicly accessible FTP servers. In this scan, 21.8 million hosts responded on port 21, and 13.8 million sent an FTP-compliant banner. Of these, 1.1 million (8%) allowed anonymous access (see Table I). 

| **Metric** | **Value** |
|------------|-----------|
| IPs scanned | 3,684,755,175 (85.79% of IPv4 address space) |
| Open port 21 | 21,832,903 (0.59% of scanned IPs) |
| FTP servers | 13,789,641 (63.16% of IPs with port open) |
| Anonymous FTP servers | 1,123,326 (8.15% of responsive FTP servers) |

Table II provides a breakdown of servers in each category.

| **Server Classification** | **Count** |
|--------------------------|-----------|
| All FTP Servers | 13,789,641 |
| Anonymous FTP Servers | 1,123,326 |

This landscape highlights the continued prevalence and potential risks associated with FTP usage.