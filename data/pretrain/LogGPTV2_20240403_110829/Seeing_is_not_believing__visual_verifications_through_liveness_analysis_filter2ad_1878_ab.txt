c
A
2
−
0
2
4
6
8
0
1
X Axis
Y Axis
Z Axis
s
e
x
a
3
g
n
o
a
l
l
s
e
u
a
V
n
o
i
t
l
e
r
e
e
c
c
A
d
e
r
e
t
l
i
F
2
−
1
−
0
1
2
X Axis
Y Axis
Z Axis
0
20
40
60
80
0
20
40
60
80
Time Index
(a)
Time Index
(b)
Figure 3: (a) Raw Accelerometer Data. (b) Filtered Accelerometer Data. The Y axis is the dominant axis
here for the direction and orientation of the mobile device.
Subsequently, IMA passes the result through a high-pass
ﬁlter, F Aa,i+1 = Aa,i+1 − Ga,i+1, where F Aa,i+1 denotes
the ﬁltered acceleration value on the a axis, ∀a ∈ {x, y, z},
for the i + 1-th sample. Figure 3(b) shows the eﬀects of
ﬁltering for the sample raw acceleration of Figure 3(a)).
Inferring distance from acceleration data. Given ac-
celeration data on each axis, Aa,1, ..Aa,m, a ∈ {X, Y, Z},
captured every T seconds, IMA computes the position (rela-
tive to the starting point) using a double integral. We adopt
the trapezoidal rule [27], used for approximating the deﬁnite
f (x)dx, representing the area below the curve
integral
where c,d are end points of integration. The integration
step is ﬁrst applied to obtain velocity (vela,i = vela,i−1 +
Aa,i+Aa,i−1
∗ T ). In a second application, the integration re-
∗ T ).
trieves the position (posa,i = posa,i−1 +
vela,i and posa,i, i = 1..m, denote the velocity and posi-
tion at the i-th sample on the axis a. The resulting po-
sition shifts are combined to obtain the cumulative shift,
ASx,i, ASy,i, ASz,i, along each axis. ASx,i, ASy,i, ASz,i are
then used as feature descriptors (see Section 3.4).
vela,i+vela,i−1
(cid:5) d
2
2
c
3.3 Similarity Computation (SC)
The Similarity Computation (SC) module compares the
two motion sequences computed by the VMA and the IMA
modules.
It returns a set of features that summarize the
nature of the similarity between the two sequences. The
features are then used by the Classiﬁcation module (see Sec-
tion 3.4) to decide whether the two motion sequences cor-
roborate each other, thereby concluding whether the video
is genuine or not. The video motion and inertial sensor
streams encode the same user hand movement, which are
processed by the VMA and IMA modules respectively (see
Figure 1) to each yield a motion stream.
To compute their similarity, we use a well-known sequence
similarity measurement method from speech and pattern
recognition, called Dynamic Time Warping (DTW). Similar
to the well-known string edit distance, DTW is a dynamic
programming solution to ﬁnd the minimum cost set of op-
erations that converts one sequence to the other.
In this subsection, we describe how we adapted the DTW
algorithm to the practical issues in comparing the two mo-
tion sequences from the VMA and IMA modules. The two
sequences diﬀer in their number of samples, and have dif-
242
ferent magnitudes due to the nature of their source sensors.
The VMA sequence length is proportional to the number
of video frames, whereas the IMA sequence length is pro-
portional to the product of the sample rate of the inertial
sensor and the length of the recording interval. We per-
form a stretching step to make sure that the VMA and IMA
sequences are of same length.
Furthermore, the motion sequence that the VMA infers
from the video stream does not take into account the dis-
tance of objects into the camera. This may result in the
same motion being registered as faster when the objects are
close to the camera, and slower when the objects are far. We
perform a calibration step to compute a coeﬃcient to match
the average speed of the motion the video stream to that of
the inertial sensor stream.
In the rest of this subsection, we ﬁrst brieﬂy detail the
DTW algorithm, then present the stretching and calibra-
tion processes. We provide justiﬁcation to the use of these
methods with observed improvements in the resulting accu-
racy that the system gains after processing the features in
the Classiﬁcation module.
3.3.1 Dynamic Time Warping (DTW)
Given two time-dependent vectors, Dynamic Time Warp-
ing (DTW) [34] is dynamic programming algorithm for ﬁnd-
ing an optimal set of operations that minimize the cost of
converting one vector to the other.
Let F be a feature space. Let X = (x1, x2, .., xn) and
Y = (y1, y2, .., ym), n, m ∈ N, be time-dependent vectors,
xi, yj ∈ F, i = 1..n, j = 1..m. An (n, m)-warping path of
X and Y is a sequence P (X, Y ) = (p1, .., pL), where pl =
(i, j) ∈ [1 : n] × [1 : m], ∀l ∈ [1 : L]. A warping path satisﬁes
(i) boundary conditions, p1 = (1, 1) and pL = (N, M ), (ii)
a monotonicity condition, ni ≤ ni+1 and mj ≤ mj+1, i =
1..n − 1, j = 1..m − 1, and (iii) a step size condition, pl+1 −
pl ∈ (1, 0), (0, 1), (1, 1) for l ∈ [1 : L − 1].
DTW computes the (n, m)-warping path of X and Y as
follows. Start with an empty path P (X, Y ). Assume it
has already aligned X and Y up to the xi and yj in Y ,
i < n, j < m. To align xi+1 and yj+1, DTW has the option
to perform one of the following three moves, illustrated in
Figure 4. First, a diagonal move, where it matches xi+1 to
yj+1. It then adds (i + 1, j + 1) to P (X, Y ). In the next step
e
c
n
a
i
t
s
D
n
o
i
t
o
M
Video
Sensor
Video
Sensor
5
3
0
3
5
2
0
2
5
1
0
1
5
0
e
c
n
a
i
t
s
D
n
o
i
t
o
M
5
3
0
3
5
2
0
2
5
1
0
1
5
0
e
c
n
a
i
t
s
D
n
o
i
t
o
M
Video
Sensor
0
3
5
2
0
2
5
1
0
1
5
0
0
20
40
60
80
0
20
40
60
80
0
20
40
60
80
Time Index
(a)
Time Index
(b)
Time Index
(c)
Figure 5: Example alignment of video and inertial motion streams extracted from the same experiment: (a)
when using only DTW. (b) when stretching the shorter vector and applying DTW. (c) after stretching and
calibration and applying DTW. Stretching helps achieve a signiﬁcant alignment improvement.
extract several characteristics of the computed DTW align-
ment as feature descriptors, to be used by the Classiﬁcation
module (see Section 3.4). First, the normalized penalty cost,
deﬁned as the penalty cost divided by L. Second, the ra-
tio of overlap points, which is the number of overlap points
between the two streams, divided by L. Third, the ratio of
diagonal moves, the number of diagonal moves divided by L.
Fourth, the ratio of expansion moves, the number of expan-
sion moves divided by L. Finally, the ratio of contractions
moves, the number of contraction moves divided by L. The
normalization to L ensures that the values are independent
of the sample length.
3.3.2 Stretching
The sensor and video streams are sampled at diﬀerent
rates, thus the two vectors are of diﬀerent length. The
stretching step extends the shorter sequence (length s) to
the length of the longer sequence (l). We use linear interpo-
lation to compute l − s new points for the shorter sequence.
In Section 6.4 we show that depending on the attack type,
the use of stretching improves the accuracy of Movee in dif-
ferentiation fraudulent from genuine videos by a rate of 8-
10%. This result is illustrated in Figure 5(b), where the use
of stretching signiﬁcantly improves the ability of the DTW
procedure to align the video and inertial sensor movement
streams when compared to Figure 5(a).
3.3.3 Calibration
An artifact of the method used in the Video Motion Anal-
ysis module is that the same motion pattern can be regis-
tered as faster when the objects in the view are close to the
camera, and slower when the objects are far.
In order to
compensate for this artifact, we calibrate the speed of the
video motion vector with a coeﬃcient to match that of the
speed of the inertial sensor motion vector.
The goal is to compute a calibration factor CF, that is
used to multiply all the points in the video stream. We
have explored several calibration methods, including mean
based and linear curve ﬁtting. We provide details however
only on the two methods that performed the best in our
experiments, truncated mean and polynomial curve ﬁtting.
Truncated mean. The truncated mean computes the mean
after discarding the high and low ends of the probability dis-
tribution (see Figure 6(b)). We apply this concept as follows:
Figure 4: Illustration of DTW alignment for two
time-dependent sequences. The red dots show the
optimal warping path. A diagonal move is a match
between the two sequences. An expansion dupli-
cates one point of one sequence and a contraction
eliminates one of the points.
it aligns xi+2 with yj+2. Second, an expansion move, where
it repeats xi to match yj+1. It adds (i, j +1) to P (X, Y ) and
continues the next for xi+1 and yj+2, and (iii) a contraction
move, where it drops xi+1, and continues with the next step,
to align xi+2 with yj+1. Given a cost function for each move
type, c(i, j), i = 1..n, j = 1..m, the cost of a warping path
P (X, Y ) is deﬁned as cp(X, Y ) =
l=1 c(pl). The goal of
DTW is to ﬁnd a warping path p∗, of minimal cost among
all possible warping paths.
(cid:6)L
Movee uses a variation of the DTW algorithm: the Vari-
able Penalty Dynamic Time Warping (VPdtw) [20]. The
process of expanding and contracting the time axis of a
sensor stream can produce a very high quality alignment
to a video stream. However, excessive numbers of expan-
sions and/or contractions can often result in matches at ran-
dom parts of the streams and appear artiﬁcial rather than
catching the genuine common movement patterns. Penal-
ized dynamic time warping uses a penalty to constrain the
use of expansions and/or contractions. This penalty is in-
curred whenever a non-diagonal (i.e., expansion or contrac-
tion) move is taken (see Figure 4).
Let L denote the length of the longer sequence between
the video and inertial sensor sequences for each sample. We
243
Video
Sensor
i
e
c
n
a
t
s