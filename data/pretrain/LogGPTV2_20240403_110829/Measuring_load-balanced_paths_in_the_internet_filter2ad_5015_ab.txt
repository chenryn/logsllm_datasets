(the other RON nodes were not available), plus a host at our
laboratory in Paris and another in Bucharest. Eleven of the
sources are in the US, the others in Europe. Table 1 summa-
rizes the locations and upstream providers of each source.
Although the sources do not exhibit great geographic diver-
sity (most of them are on the US east and west coasts), they
connect to the internet through many diﬀerent providers.
We used two destination lists. The ﬁrst, which we call
MIT, contains 68,629 addresses.
It was generated by re-
searchers at MIT from the BGP table of a router located
there. They randomly selected a couple of addresses from
each CIDR block of the BGP table, and ran classic tracer-
oute from MIT towards each address. The basis of the MIT
list consists of the last responding hop from each trace. From
this, they removed addresses that appeared in any of several
blacklists, as well as any host regarding which they received
complaints during their experiments. We updated this list
by adding all our source nodes.
Since the ﬁrst list doubtless includes targets that are routers
or middleboxes rather than end-hosts, we also used a second
list for which we were certain that we could trace all the way
through the network. This consists of the 500 most popu-
lar websites, as reported by the commercial service, Alexa.2
2See http://www.alexa.com/site/ds/top sites?ts mode=
global&lang=none
We call this the WEB list. We could only use it from the
paris node, as the RON acceptable use policy forbids tracing
towards arbitrary destinations.
We collected our data over the months of February to
April 2007, using Paris traceroute adapted to run in 32 par-
allel threads of a single process. We limit the overall band-
width to 200 probes per second. Each thread takes the next
address d in the destination list, and uses the MDA to enu-
merate all of the paths to d. We use the following parame-
ters: 50 ms of delay between each probe sent, abandon after
3 consecutive unresponsive hops, a 95% level of conﬁdence to
ﬁnd the nexthops of an interface. We use UDP probes. We
avoided ICMP probes because some per-ﬂow load balancers
do not perform load balancing on ICMP packets, thus hiding
part of the load-balanced paths. We did not use TCP probes
to avoid triggering IDS alarms. We collected data from all
15 sources, but due to disk space restrictions we were able
to collect per-destination load balancing data from only 11
of them.
A round towards all destinations in the MIT list takes
between 10 and 15 hours, depending upon the source. Our
traces with the MIT list, for all sources, cover 9,506 ASes,
including all nine tier-1 networks and 96 of the one hundred
top-20 ASes of each region according to APNIC’s weekly
routing table report.3
4.2 Metrics
This section describes the metrics we use to characterize
load balancing. Fig. 2 illustrates these metrics. This is a
real topology we discovered when tracing from a US source,
S, towards a Google web server, T . We use the following
terminology in the context of IP-level directed graphs gen-
erated by the MDA:
B
0
C
0
D
0
S
A0
G
00
H
0
E
0
F
0
I
0
J
0
K
0
L
0
M
0
N0
T
Diamond 1
Diamond 2
Figure 2: Two diamonds in a set of paths to a des-
tination
Load balancer. A node with out-degree d > 1 is an
interface of a load balancer. For instance, A0, H0, I0 and
J0 are interfaces of load balancers.
Diamond. A diamond is a subgraph delimited by a di-
vergence point followed, two or more hops later, by a conver-
gence point, with the requirement that all ﬂows from source
to destination ﬂow through both points. Fig. 2 has two dia-
monds, shown in dashed boxes. Note that this diﬀers from
deﬁnitions of diamonds we have employed in other work, in
which we restricted their length to two hops, or allowed just
a subset of ﬂows to pass through them (as between I0 and
T in Fig. 2).
3APNIC automatically generates reports describing the
state of the internet routing table.
It ranks ASes per re-
gion according to the number of networks announced.
Diamond width. We use two metrics to describe the
width of a diamond. The min-width counts the number of
link-disjoint paths between the divergence and convergence
points. This gives us a lower bound on the path diversity in
a diamond. For instance, diamonds 1 and 2 in Fig. 2 have
the same min-width of 2, although diamond 2 appears to
oﬀer greater diversity. Thus, in addition, we also use the
max-width metric, which indicates the maximum number of
interfaces that one can reach at a given hop in a diamond.
In our example, diamond 1 has a max-width of 2 whereas
diamond 2 has a max-width of 4.
Diamond length. This is the maximum number of hops
between the divergence and convergence points. In our ex-
ample, diamond 1 has length 4 and diamond 2 has length
3.
Diamond symmetry. If all the parallel paths of a dia-
mond have the same number of hops, we say that the dia-
mond is symmetric. Otherwise, it is asymmetric. The dia-
mond asymmetry is the diﬀerence between the longest and
the shortest path from the divergence point to the conver-
gence point. Diamond 1 has an asymmetry of 1, since the
longest path has 4 hops and the shortest one has 3 hops.
Diamond 2 is symmetric.
5. LOAD BALANCERS
This section characterizes load balancers. We show that
per-ﬂow and per-destination load balancing are very com-
mon in our traces. This high frequency is due to the fact
that per-ﬂow and per-destination load balancers are located
in core networks, and thus are likely to aﬀect many paths.
We also observe that the majority of load balancing happens
within a single network.
5.1 Occurrences of load balancing
Per-destination load balancers are the most common in
our traces: the paths between 70% of the 771,795 source-
destination pairs traverse a per-destination load balancer.
This percentage is still considerable for per-ﬂow load bal-
ancers, 39%, but fairly small, only 1.9%, for per-packet
load balancers. Our measurements for per-ﬂow and per-
packet load balancers had 1,010,256 source-destination pairs
in total.
(This diﬀerence is because our dataset for per-
destination load balancers uses only 11 sources, whereas the
per-ﬂow and per-packet dataset uses 15 sources). The frac-
tion of per-ﬂow load balancers generalizes the results of our
preliminary study [3], in which we found that per-ﬂow load
balancing was common from the paris source. This result
comes from the widespread availability of load balancing in
routers. For instance, Cisco and Juniper routers can be
conﬁgured to perform any of the three types of load bal-
ancing [1, 17, 2]. Even though per-packet load balancing
is widely available, network operators avoid this technique
because it can cause packet reordering [18].
Table 2 breaks down these results for each source. The fre-
quency of per-ﬂow and per-destination load balancers varies
according to the source (from 23% to 80% and from 51%
to 95%, respectively), whereas the frequency of per-packet
load balancers is more stable across all sources (around 2%).
The frequency of per-ﬂow and per-destination load balancers
depends on the location and upstream connectivity of the
source. For instance, the roncluster1 and speakeasy sources,
which are in the same location and have the same upstream
connectivity, observe the same fraction of per-ﬂow load bal-
Upstream provider
557 ASes
Sprint, Level3 + 6 others
482 ASes
XO comunications
Level3 + 5 others
Location
Amsterdam, NL
Bucharest, RO
Chicago, IL
Laurel, MD
Cornell, Ithaca, NY
San Luis Obispo, CA NTT, Qwest, Level3, GBLX + 4 others
Berkeley, CA
London, UK
CA
New York, NY
Paris, FR
Source
am1-gblx
bucuresti
chi1-gblx
coloco
cornell
digitalwest
intel
lon1-gblx
msanders
nyu
paris
roncluster1 Cambridge, MA
Cambridge, MA
speakeasy
vineyard
Vineyard Haven, MA Qwest, Savvis
AT&T
553 ASes
NTT, UUNET, GBLX, Level3 + 28 others
30 AS, most tier-1s, Abilene
RENATER
Sprint, Level3, Cogent + 2 others
Sprint, Level3, Cogent + 2 others
Table 1: Locations and upstream providers of our measurement sources
Source
per-ﬂow per-packet
per-dest
any
MIT list
am1-gblx
bucuresti
chi1-gblx
coloco
cornell
cybermesa
digitalwest
intel
lon1-gblx
msanders
nyu
paris
roncluster1
speakeasy
vineyard
all
23%
25%
27%
27%
80%
25%
54%
31%
26%
39%
64%
30%
51%
51%
40%
39.5%
2.1%
2.6%
2.3%
2.0%
2.0%
1.7%
2.0%
1.9%
2.1%
2.2%
1.9%
1.9%
2.8%
2.8%
2.0%
2.1%
63% 83%
60% 82%
62% 82%
n.a.
n.a.
74% 97%
61% 83%
70% 89%
95% 97%
n.a.
n.a.
93% 93%
82% 92%
81% 93%
51% 89%
n.a.
n.a.
n.a.
n.a.
72% 89%
paris
35%
0%
n.a.
n.a.
WEB list
Table 2: Fraction of paths aﬀected by load balancing
ancers. On the other hand, the frequency of per-packet
load balancing depends mostly on the destination list used–
always around 2% for the MIT list and zero for the WEB list.
Furthermore, it is relatively constant from all our sources,
which suggests that per-packet load balancers tend to be
close to destinations.
We now study how load balancers aﬀect paths to verify
whether there are a few routers responsible for most load
balancing. In a typical MIT round, we ﬁnd from each source
around 1,000 distinct per-ﬂow load balancers, 2,500 to 3,000
per-destination load balancers, and 500 per-packet load bal-
ancers.
Fig. 3 shows the disparity between the relatively small
number of load balancers and the large number of load-
balanced paths. It presents the cumulative fraction of paths
aﬀected by the 50 most frequent load balancers (per-desti-
nation, per-ﬂow and per-packet). Each curve represents the
results for one type of load balancer and one source. We
t
s
h
a
p
d
e
c
n
a
a
b
-
d
a
o
l
l
f
o
n
o
i
t
c
a
r
F
 100
 80
 60
 40
 20
 0
 0
per-destination
per-flow
per-packet