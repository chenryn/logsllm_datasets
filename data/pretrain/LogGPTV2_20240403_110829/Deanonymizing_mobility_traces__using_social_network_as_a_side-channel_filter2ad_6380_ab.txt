tivity range (typically less than 10 meters), a Bluetooth connec-
tion is established between the user devices. If such a connection
lasts for 600 seconds we assume that the users are in contact with
each other (we ignored contacts during session breaks and lunch
breaks). In addition the dataset also includes a list of 616 atten-
dees (names and afﬁliations) of Infocom 2006. The volunteers (78
of them) are a strict subset of the conference attendees. We use
DBLP (co-authorship database [9]) to construct a social network
over the conference attendees − the social network has 0-1 link
weights based on the normalized number of co-authored papers be-
tween two users: if Pu1 and Pu2 denote the set of papers that in-
cludes user u1 and u2 on the author list then the weight of the link
between the users is given by
|Pu1∩Pu2|
|Pu1∪Pu2| .
2.3 Structural Similarity
The key hypothesis of our approach is that the social network
and the contact graph bear structural similarities. We show that this
is indeed the case using three well accepted measures of graph sim-
ilarity: graph edit distance (minimum number of edges that need to
be added and/or deleted for an exact match), maximum common
sub-graph (number of vertices in the largest common sub-graph)
and node degree distribution. We remark that the graph edit dis-
tance measure typically applies to graphs that have identical num-
ber of vertices, while the latter measures do not impose such a re-
striction. In order to simplify the measurement of such graph sim-
ilarity measures we round-off edge weights in the social network
and the contact graph to either 0 or 1.
Figure 3 shows graph similarity measures using the graph edit
distance and the maximum common sub-graph measures. Figure 4
shows the degree distributions of the contact graph and the social
network using the datasets. These ﬁgures show that the contact
graph and the social network tend to bear a lot of similarity; in our
datasets, the average ratio of graph edit distance to the number of
edges is about 10.3% (lower the better), the average ratio maximum
common sub-graph to the number of nodes is about 60% (higher
the better), the average Kullback and Leibler (KL) symmetrised
divergence measure [31] between the node degree distributions is
about 0.062 bits (lower the better).
3. GRAPH DE-ANONYMIZATION
We examine a two step solution to match the contact graph against
the social network. In the ﬁrst step we bootstrap the matching prob-
lem by exploiting inherent heterogeneity in the graphs to identify
landmark nodes. In the second step we extend a mapping between
landmark nodes to all the nodes in the graph by identifying discrim-
inating features in the original graph. We explore three techniques
for this second step. In the remainder of this section we describe
our algorithm in detail and evaluate each of the techniques using
the previously described datasets. Results were computed on an
Intel i5 quad-core processor operating at 2.4 GHz with 4 GB RAM
running RedHat Enterprise Linux 5.4.
3.1 Initial Landmarks Selection
For the ﬁrst step, we identify landmark nodes using a node cen-
trality measure [25, 7], which is a measure of the relative impor-
tance of a vertex within a graph. Centrality is very well studied
metric in both graph theory and social network analysis, e.g., to de-
termine how important a person is within a social network or how
well-used a road is within an urban network. We identify land-
mark nodes as those with high centrality metric. Past work [12]
has shown that node centrality in contact graphs and social net-
works generally follows a heavy tailed distribution; hence, there
are a small number of nodes (outliers) with a high centrality score,
while a vast majority of the nodes belong to the tail of the distribu-
tion. Therefore we identify the top k central nodes, for k (cid:28) |VC|,
|VS|, in both the contact graph and the social network as landmark
nodes; we experimentally show that for small values of k the set
of top-k centrality nodes in both the social network and the contact
graph are likely to be the same. Further, given k landmark nodes
in both the contact graph and the social network there are at most
k! mappings between them. Since k is typically small even a brute
force enumeration of all possible mappings is feasible.
While there are several measures of centrality, in this paper we
adopt the node betweeness metric. The betweenness centrality mea-
sure (as applied to a static graph) for node n is a normalized mea-
sure over the number of all-pair shortest paths in a graph that in-
cludes node n. In general, if a large number of shortest paths pass
through node n, then node n has a higher betweeness centrality
measure. We use this betweeness measure for centrality computa-
tion in the static social network
For the contract graph we observe that in a contact graph a ‘path’
exists over time. Hence, the standard shortest path based deﬁnition
of node betweeness does not directly apply to the contact graph.
Hence, we develop a novel centrality measure that applies to con-
0.30.40.50.6DistributionSt Andrew-ContactSt Andrew-Social00.10.212345678910Degree DNode Degree0.20.250.30.350.4DistributionSmallblue-ContactSmallblue-Social00.050.10.1512345678910Degree DNode Degree0.30.40.50.6DistributionInfocom06-ContactInfocom06-Social00.10.212345678910Degree DNode Degree630where the coefﬁcients G(r)
ws−wk
From Eq. (1), the path weight is written as
s=1,s(cid:54)=k
k =
ws
.
(cid:90) T
0
r(cid:88)
k=1
G(r)
r(cid:81)
r(cid:88)
N(cid:88)
k=1
tact graphs. First, we recognize that a path between two nodes A
and B in the contact graph is via a sequence of contacts N1, N2,
··· , Nr−1. Hence, we adopt the following deﬁnition of paths in
contact graph:
DEFINITION 1. Opportunistic path [12]
A r-hop opportunistic path PAB =(VP , EP ) between nodes A
and B consists of a node set VP = {A, N1, N2, ··· , Nr−1, B} ⊂
V and an edge set EP = {(cid:104)A, N1, w1(cid:105), (cid:104)N1, N2, w2(cid:105), ··· , (cid:104)Nr−1,
B, wr(cid:105)} ⊂ E such that Ni (cid:54)= Nj for any 1 ≤ i < j ≤ r − 1. The
path weight is the probability pAB(T ) that A may reach B along
PAB within time T .
We model the inter-contact time Xk between nodes Nk and Nk+1,
as a random variable with a probability density function (PDF)
pXk (x). In our datasets we observed that pXk (x) was exponen-
tially distributed: pXk (x) = wke−wkx. However, we remark that
the approach is applicable to any arbitrary distribution. Assuming
k=1 Xk following
that pXk (x) is exponentially distributed, Y =(cid:80)r
a hypoexponential distribution [27], such that
pY (x) =
k pXk (x),
(1)
pAB(T ) =
pY (x)dx =
· (1 − e
−wkT ),
G(r)
k
(2)
The centrality metric Ci for a node i is then deﬁned as follows:
Ci =
1
N − 1
·
j=1,j(cid:54)=i
pij(T ),
(3)
where N is the total number of nodes in the network. Intuitively,
this metric is a measure of distance from a randomly chosen node
in the network to node i. Due to the heterogeneity of the pairwise
contact frequency in different traces, different values of T were
adaptively chosen; T is set as 1 hour for the Infocom traces, 6 hours
for the St Andrews and Smallblue traces.
Figures 5 and 6 shows the centrality scores of nodes in the con-
tact graph and the social network respectively. We observe due to
the inherent heterogeneity of these graphs, only a few select nodes
have high centrality scores. More interestingly, we observe that the
same of set of nodes that have high centrality score in the contact
graph also has a high centrality score in the social network. Indeed
we note that the set of top-k nodes (ordered by centrality score) is
identical for both the contact graph and the social network. For e.g.,
node identiﬁers 1, 7 and 18 in the St Andrews dataset are the top
3 central nodes in both the contact graph and the social network;
node identiﬁers 8, 25, 41, 55, 100 and 119 are the top 6 central
nodes in the both the contact graph and the social network in the
Smallblue dataset; node identiﬁers 8, 24, 34, 47 and 72 are the top
5 central nodes in the both the contact graph and the social network
in the Infocom06 dataset.
Note that at this stage we still do not have a mapping between the
landmark nodes; however, given k landmark nodes there are at most
k! mappings.
In the subsequent sections we propose techniques
to start with a mapping of landmark nodes and de-anonymize the
rest of the contact graph. We repeat this exercise for all such k!
mappings between the landmark nodes; using a goodness-of-ﬁt test
on the thus derived mappings we select the most likely mapping
between the nodes in the contact graph and the social network.
(a) Accuracy
k
1
2
4
8
St Andrews
Smallblue
Infocom06
1.9
2
2.2
2.4
195
196
198
200
47
47.4
48
49
(b) Computation times (seconds)
Figure 7: Distance Vector Results
Given a possible mapping between such landmarks, the next
step is to use various graph features to deduce mappings for other
nodes. Formally, LC = {c1, ··· , ck} and LS = {s1, ··· , sk} de-
note the landmark nodes in the contact graph and the social net-
work respectively, and the initial mapping is ci = si for all 1 ≤
i ≤ k. Given this initial mapping, we consider three possible
techniques—distance vectors, spanning tree matching, and local
sub-graph features—detailed in the next three subsections.
3.2 Distance Vector
The ﬁrst technique we consider is to map nodes that have sim-
ilar distance vectors. For each non-landmark node in the contact
graph and the social network, its distance vector contains distances
between the node to the k landmark nodes. Hence, for a node n,
its distance vector is given by {dn1, ··· , dnk}, where dni denotes
the distance from node n to landmark i (i.e., ci in the contact graph
and si in the social network).
work we quantify a mapping score as wcs = −(cid:113)(cid:80)k
Given two nodes c in the contact graph and s in the social net-
i=1(dci − dsi)2.
Now we construct a bipartite graph with vertex set (VC \ LC ) ∪
(VS \ LS), where VC and VS are the set of vertices in the con-
tact graph and the social network respectively; for every node c
∈ VC \ LC and s ∈ VS \ LS we add an edge between node c
and node s with weight wcs. Now the node mapping problem re-
duces to a maximum weighted bipartite graph matching problem
that determines matching pairs of vertices in VC \ LC and VS \ LS
respectively. We use the Hungarian algorithm [6] that solves the
weighted graph matching problem in bipartite graphs in O(|V |3),
where V is the set of vertices and E is the set of edges in the bipar-
tite graph. The result of this algorithm is pairs of matched nodes s(cid:48)
∈ VC \ LC and s ∈ VS \ LS. We denote the overall matching score
ws(cid:48)s. We repeat this procedure for all k! mappings
between landmark nodes and ﬁnally select the mapping that results
in the highest matching score.
as(cid:80)
s∈VS\LS
Figure 7(a) shows the accuracy of de-anonymization as we vary
k. We observe that initially as k increases the accuracy of de-
040.60.81curacySt AndrewsSmallblueInfocom0600.20.412345678Acck: No of landmark nodes631Figure 5: Node Centrality in Contact Graph: St Andrews, Smallblue and Infocom06
Figure 6: Node Centrality in Social Network: St Andrews, Smallblue and Infocom06. Notice that the peaks (high centrality nodes)
in the contact graph (Figure 5) and the social network (Figure 6) match.
1|V |∗Π
In this solution we project both the contact graph C and the so-
cial network S into randomized spanning trees. We note given any
graph G the number of spanning trees of graph G is given by Kirch-
|V |−1
i=1 λi, where λi are the non-zero
hoff’s theorem [14]:
eigenvalues of G’s Laplacian matrix. The Laplacian matrix Q of
a graph G is deﬁned as Q = D − A, where D is a diagonal matrix
with the ith diagonal element set to the degree of node i and A is
the adjacency matrix of graph G (i.e., the ijth element in A is 1
if there exists an edge between node i and j; 0 otherwise). Kirch-
hoff’s theorem also allows us to explicitly enumerate all spanning
trees of a graph and thus enables us to select a random set of such
spanning trees.
anonymization process improves − since larger number of land-
marks improves the precision of node distance vectors. However,
as k increases further the set of landmark nodes in the contact graph
and the social network are no longer identical; hence, the overall
efﬁcacy of node mapping decreases.
Figure 7(b) shows the computation cost of deriving node map-
pings given a mapping for landmark nodes. We note that given a
landmark mapping the computation cost does not increase signif-
icantly with k − the number of landmark nodes. As k increases
we need to compute distances from more landmark nodes to all the
other nodes in both the contact graph and the social network. This
operation costs O(k|E|). Once such distances are computed the
cost of computing similarity is O(k|V |2). However, the cost of the
weighted bipartite graph matching O(|V |3) which dominates the
computation cost is independent of k. Note that Figure 7 shows the
cost for a given landmark node mapping − hence, the overall com-
putation cost has to be scaled with k! (for every possible landmark
node mapping). We observe that on the 125 node Smallblue dataset
with k = 5 nodes the total time to de-anonymize is 5!*200 seconds