in Figure 2 evaluates to true if the string pointed to by the
identiﬁer matches. As seen in the example, conditions also
support x of strs expressions, where x is a number and
strs is a set of strings. The notion x of y means that at least
x of the strings contained in y should match. The set of strings
inside such an expression can also be expressed using a wild-
card expression such as $str2_*. Our example rule triggers
if $op_b matches, or if both plain strings, at least one regular
expression and at least one hex string match.
2.2 Processing Strings
In order for YARIX to work with an arbitrary YARA rule, we
must be able to automatically process all types of strings and
all types of condition expressions. That is, given a string of a
YARA rule, we have to feed it in some form to the index to
ﬁnd all ﬁles for which that string would be a potential match.
As previously stated there are three types of strings, each of
which we handle separately.
2https://github.com/mbrengel/yarix
USENIX Association
30th USENIX Security Symposium    3543
Plain Strings This is the easiest type of string, as it can
be broken up into its n-grams which can then be used to
query the index. Formally, let s be a plain string of length l
consisting of the bytes b1, . . . ,bl. Then we use each n-gram
xi ∈ {b jb j+1 . . .b j+n−1 | 1 ≤ j ≤ l − n + 1} and query the
index to get a set of ﬁle IDs Fi in which xi is contained. Finally,
xi Fi is returned as a set of candidates
the intersection C =(cid:84)
that potentially match the plain string s.
Regular Expressions To handle a regular expression, we
identify the plain strings that will be contained in every string
that will be matched by the regular expression and then pro-
ceed as with plain strings. In detail, given a regular expression
r, we ﬁrst construct a DFA from it. Then, we compute the
dominators of the ﬁnal state of the DFA, i.e., all states that
any accepting word will visit when the DFA is executed. For
each of these dominator states we then proceed as follows:
We check if the state has only one outgoing edge. If this is
the case, we collect the label (the character) of the edge and
continue with the target state of that edge. This is repeated
until we reach the ﬁnal state or a state with more than one
outgoing edge is discovered. The concatenation of the col-
lected characters along the path form a plain string that will
be contained in every string that matches r. This process will
give us a set of plain strings S that we can proceed with as
before to get a set of ﬁle IDs Fi for each si ∈ S. Given that all
of these plain strings must be contained in every match of r
we can return C =(cid:84) Fi as the ﬁnal set of candidates. In Fig-
ure 2, the plain strings of $str2_a and $str2_b are ".png"
and "\xC7\x45\xC3A", respectively.
Hex Strings Similar to before, we handle hex strings by
identifying plain strings. In detail, we start at the beginning
of the hex string and scan byte by byte to collect a plain
string. We stop collecting the current plain string whenever
we encounter a wildcard or a grouping expression and start
collecting a new plain string when we encounter the next
ﬁxed byte. In Figure 2, the sets of plain strings of $op_a and
$op_b are "\x01\x01\x01\x01", "\xF3\xAB\x88\x12" and
"\xAB\x88\x12\x83".
We may fail to extract any n-gram. For example, the regular
expression "[0-9]+" does not contain any plain strings that
will be contained in every match. Similarly, a hex string might
consist only of wildcards and/or grouping expressions such
as { (A?|B?) (C?|D?) }. Because of performance reasons,
we also neglect strings smaller than n bytes, where n is the size
of the n-grams. In principle, we could query the index for all n-
grams where the search string is a preﬁx or a sufﬁx. However,
this quickly imposes a signiﬁcant overhead even if we are just
a single byte short. For example, if we have n = 4 and we want
to create the posting list of the 3-gram "abc" we would need
to create the union of all posting lists of "xabc" and "abcx"
where x is an arbitrary byte, which would be a slowdown
factor of 2· 256 = 512 and involve more costly set operations.
$op_b or (2 of $str_* and 1 of $str2_* and 1 of $op_*)
$op_b
2 of $str_* and 1 of $str2_* and 1 of $op_*
2 of $str_*
1 of $str2_* and 1 of $op_*
1 of $str2_*
1 of $op_*
Figure 3: Abstract Syntax Tree (AST) of the condition of the
YARA rule in Figure 2.
This also applies to the plain strings yielded by processing
regular expressions and hex strings, i.e., a regular expression
is not optimizable if all of its plain strings are smaller than
n bytes. YARA also supports the nocase modiﬁer for case-
insensitive searches, which we optimize by numerating all
2n different options for each n-gram. If we cannot ﬁnd any
long enough plain strings, we consider the whole string not
optimizable and return C = (cid:62), i.e., the whole universe of ﬁles.
2.3 Processing the Condition
We need to parse the condition of a rule to understand how
we should combine the search results of the individual strings.
Figure 3 shows how we create the abstract syntax tree (AST)
of the condition of our example rule in Figure 2. The leaf
nodes are expressions of boolean type that do not contain any
of the standard logical operations (and/or/not). For each of
these expressions, we deﬁne an index search operation that
captures the semantics of the expression. After that, we com-
bine the search results according to the logic operation in the
tree, i.e., set union (∪) for or and set intersection (∩) for and3.
In case of a logical negation (not), we check if the expression
only contains plain strings which are exactly n bytes long.
In this case, we compute the ﬁle IDs, apply a set minus and
proceed upwards in the tree. Otherwise, the expression is con-
sidered unoptimizable and (cid:62) is returned. For example, con-
sider the expression not $s where $s = "abcde". Using an
index for n = 4, we can identify posting lists for "abcd" and
"bcde". However, any combination of these posting lists only
gives us a set of candidate ﬁles that might contain "abcde".
A negation of such a set is not the set of ﬁles that do deﬁni-
tively not contain "abcde". For example, it is perfectly valid
for all ﬁles in the intersection of both posting lists to contain
"abcde".
2.4 Processing Individual Expressions
We need to handle the individual expressions, i.e., the leaf
nodes of the AST. Handling a simple string expression such
as $op_b is obvious as we just use the index as described in
3We apply the standard set operation simpliﬁcations in case we have to
deal with (cid:62), i.e., (cid:62)∪ A = (cid:62) and (cid:62)∩ A = A
3544    30th USENIX Security Symposium
USENIX Association
Section 2.2. The x of strs expression can be captured with
the index as follows: for each string contained in strs we
query the index to ﬁnd all ﬁle IDs that match the string. For
each ﬁle ID we count how many strings match, which we use
to return all ﬁle IDs that match at least n strings. In the case
that searching some of the strings of str is not optimizable for
reasons described in Section 2.2, we ﬁrst create the expression
x’ of strs’ where strs’ is the optimizable subset of str
and x’ is x minus the number of unoptimizable strings. If x’
is smaller or equal to 0, then we consider the expression not
optimizable and return (cid:62). Otherwise, we proceed as before,
i.e., index search and keeping track of the number of matches.
For any other type of expression e, we simplify the expression
to a x of strs expression. In detail, if s1, . . . , sl are the
string identiﬁers that an expression e contains, we create the
simpliﬁed expression e(cid:48) = l of (s1, ..., sl) and pro-
ceed as before, i.e., we want to match all strings contained in
e.
2.5 Optimization Limitations
The simpliﬁcation we apply to expressions to ensure that we
can optimize them, comes with a loss of precision. For exam-
ple, an expression of the form str at o requires positional
information that we cannot capture. By simplifying this ex-
pression to 1 of str we do not lose completeness, as any
ﬁle where str appears at o must fulﬁll the precondition that
str appears at all. However, we accept a loss in semantic in-
formation that could potentially blow up the set of candidate
ﬁles. While this is not a problem in terms of soundness of
the search as we will use YARA as a post-tool to ﬁlter out
non-matching ﬁles, it could still lead to performance issues.
Another problem that could occur is optimizability. We
consider a rule not optimizable if evaluating it returns (cid:62),
which happens if there are too many unoptimizable expres-
sions and this problem propagates through the set logic dic-
tated by the condition. In this case, YARIX is equivalent to a
sequential YARA scan. We have seen that expressions can-
not be optimized if they contain too many unoptimizable
strings. Apart from the examples we discussed, i.e., strings
being too short, there are also expressions that we consider
unoptimizable because they do not use any strings at all. For
example, YARA contains the PE module which allows for
complex expressions such as pe.imphash() == "..." or
pe.rva_to_offset(0x40000) == .... Such expressions
reﬂect strong semantic constraints that we cannot optimize
with YARIX. We thus have to ignore such expressions and
consider them unoptimizable. In case a rule contains too
many of such expressions, the whole rule becomes unop-
timizable and a full sequential traditional YARA scans is
required. However, the PE module also contains features
that YARIX can cope with. For example, the expression
pe.checksum == "\xDE\xAD\xBE\xEF" puts constrains on
the checksum ﬁeld of the PE header. We can abstract from this
by simplifying this to 1 of "\xDE\xAD\xBE\xEF", i.e., we
only require the string "\xDE\xAD\xBE\xEF" to be present at
all, which preserves completeness.
It is worth noting that a rule can be optimizable even if
it contains unoptimizable strings. Consider, for example, a
rule e1 and e2. Even if e1 cannot be optimized, the whole
rule can still be optimized if e2 is optimizable. In this case
the logical and operator translates to a set intersection and
remedies the fact that e1 translates to the set of all ﬁle IDs.
3 File Index Design
We now describe the design and implementation of the binary
ﬁle index used by YARIX. We start our discussion by showing
the commonalities and differences between traditional indexes
used for full-text search and our setting. After that, we discuss
the decisions we take to compress the index to reduce its disk
space footprint.
3.1 Background
YARIX requires a generic index design, i.e., supports arbitrary
YARA rules without requiring updates upon rule changes
and/or additions. The underlying data structure of YARIX is
thus an inverted n-gram index [4]. We borrow this idea from
the domain of full-text search. There, an inverted index maps
n-grams of tokens, i.e., n consecutive words or characters
in a document, to sets of IDs of documents containing these
tokens – so called posting lists. This mapping allows to ﬁnd all
documents that contain a given n-gram. Moreover, a posting
list of an n-gram usually stores the position(s) at which the
n-gram occurs for each document ID of the list. This allows
to ﬁnd the exact position of the n-gram within the ﬁle.
To search for a sentence in all documents, one would break
down the sentence into its n-grams and look up the document
identiﬁers in the posting lists of those n-grams. For example,
assume we search for the phrase The five boxing wizards
with n = 3. Here, we ﬁrst use a sliding window to split the
phrase into the two possible 3-grams, namely (The, five,
boxing) and (five, boxing, wizards). We then look up
the posting lists for each of the four n-grams. These lists are
then intersected to get a list of candidate documents that po-
tentially contain the desired phrase. To verify if the candidate
documents actually contain the entire phrase, we can verify
the positional information stored in the posting lists. If the
n-grams’ offsets of a candidate document are in sequential or-
der, we can be certain that the document contains the word or
sentence; if they are not, the phrase is not contained. With ref-
erence to our example, assume a ﬁle that contains the sentence
The five boxing frogs are similar to five boxing
wizards. While this candidate document indeed contains all
searched n-grams, their positions within the document (1, 8)
are not consecutive, and hence, the search results excludes it.
USENIX Association
30th USENIX Security Symposium    3545
Inverted n-Gram Malware Index
3.2
The general idea of an inverted index seems to translate nicely
to this use case. Here, the documents are malware samples,
and an n-gram represents a sequence of n consecutive bytes
within a ﬁle. There are, however, a few notable challenges
that we have to tackle to apply the idea of an inverted n-gram
index to malware samples.
First, the number of possible byte sequences in a binary ﬁle
quickly explodes for larger n. For example, choosing n = 4
already yields a set of 24·8 potential posting lists that need to
be maintained. The number of words or characters in the case
of full-text document search is orders of magnitudes smaller
than in our case. We have found that this space is quickly
saturated by approximately 105 malware samples, i.e., every
possible 4-gram occurs in at least one of those samples. This is
different in a text setting, where the case-insensitive character
set is tightly constrained. Moreover, the language grammar
dictates strong relationships between particular words (e.g.,
(1) article, (2) adjective, (3) noun), resulting in an overall
smaller number of actual combinations.
Second, malware executables lack a natural word delimiter.
While texts contain whitespaces that can be used to infer
tokens, we cannot infer any meaningful boundary in malware
samples that contain a mostly unstructured blob of arbitrary
code and data. Due to the lack of reliable tokenization of
malware samples, we thus have to fall back to ﬁxed-size byte
sequences within the document.
Both observations impose interesting challenges for a
space-efﬁcient malware index. We aim to index large col-
lections of potentially billions of ﬁles, which quickly leads
to formidable space requirements. It becomes particularly
challenging as we aim for a complete search, i.e., search re-
sults must not dismiss any documents that match the search
criteria in favor of efﬁciency. In the following, we will thus
present and discuss methodologies that compensate the lack
of space-efﬁciency, while preserving completeness.
3.3 Space Optimization Strategies
The size of an inverted index is largely determined by two
factors: (a) the number of n-grams in the index, and (b) the
size of the posting lists of each n-gram. Both represent suitable
angles to heavily reduce the space required to store an index.
In the following, we will survey the general options to reduce
storage costs for either angle.
3.3.1 Optimizing the Set of Considered n-Grams
An obvious ﬁrst optimization point is to set n to a small
value. For n = 1, there are at most 28 = 256 n-grams, and for
each increment, the number of n-grams multiplies by eight.
Choosing an efﬁcient n that is still characteristic enough for
searches is a trade-off. While smaller n clearly reduce the
number of posting lists, shorter n-grams are less characteristic
and have a higher chance to be present in a large fraction
of indexed ﬁles. We defer this discussion to Section 4.5, in
which we evaluate and choose an appropriate n.
If we knew the search criteria when building the index,
n-grams that are never searched for could be ignored. This
would represent a signiﬁcant reduction of index space. How-
ever, for this optimization to work, we need a priori knowl-
edge of the search criteria, and the criteria must be static over
time. One can quickly see that this is not a fair assumption in