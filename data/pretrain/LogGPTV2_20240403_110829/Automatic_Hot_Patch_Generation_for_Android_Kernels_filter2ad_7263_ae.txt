USENIX Association
1intkey_reject_and_link(.....){2...3if(keyring){4if(keyring->restrict_link)5return-EPERM;6link_ret=__key_link_begin(keyring,7&key->index_key,&edit);}8...9+if(keyring&&link_ret==0)10__key_link_end(keyring,11&key->index_key,edit);12...13}Table 10: Comparison with Human Written Patches
Table 11: Patch Robustness Analysis
Similar Patch
Dissimilar Patch
Total number
CVE examples CVE-2014-3145
54
1
CVE-2016-4470
between generated patches and human written patches.
Similar Patch: CVE-2014-4656 It is an integer overﬂows
vulnerability in snd_ctl_add() function. As listed below,
the oﬃcial ﬁx tries to check the input parameter kcontrol’s
id index to see whether it is larger than the MAX value minus
the kcontrol’s count. The oﬃcial patch has put the ﬁx at
the beginning of the function. Since the generated patch also
aims to ﬁx the problem at the same point, there is no need for
Vulmet to do further semantic transformations. The generated
patch is similar to the oﬃcial one and so is the human written
hot patch.
( i d . i n d e x > UINT_MAX − k c o n t r o l −>c o u n t )
i f
Dissimilar Patch: CVE-2016-4470 The patch is discussed
in the previous section with Fig. 8. For this case, the
human-written patch is diﬀerent from the generated one. The
human-written patch tries to hook at the callee function af-
ter the oﬃcial sanity check. It checks the variable value of
edit. This value is an indicator of whether the function
__key_link_begin() has been successfully executed. If
the variable edit is found uninitialized, the function will be
killed since link_ret will not be properly assigned.
These diﬀerences are introduced because the experts can
understand the root cause of the vulnerability and apply the
patch to ﬁx the problem directly. Whereas, Vulmet depends
on the semantics of the oﬃcial patches and follows a back-
ward analysis path to transform them to hot patch semantics.
However, both of the two patches can ﬁx the vulnerability.
Therefore, although with a slight diﬀerence in semantics, the
generated hot patch can successfully patch the vulnerability
as the human expert.
5.2 Robustness Evaluation
Since the hot patches modify the original programs, they may
break other functionalities, which may lead to unexpected sys-
tem faults. Therefore, it is important to ensure that the system
robustness is not aﬀected after applying the patches. In this ex-
periment, we evaluate the robustness of the patched programs
by testing patched kernels with Android benchmarks.
To build the testing environment, we choose the Android
bullhead to build with Linux kernel version 3.10 and roll
back commits to producing a kernel with many unpatched
vulnerabilities. In this particular kernel, Vulmet manages
to convert 21 vulnerability patches into hot patches. Then
we apply these patches to the kernel and run the AnTuTu
CVE NO.
CVE-2014-4656
CVE-2015-7515
CVE-2015-8543
CVE-2016-2468
CVE-2016-8399
Overall
Kernel Ver.
3.10
3.10
3.10
3.10
3.10
-
Build
bullhead
bullhead
bullhead
bullhead
bullhead
-
State
robust
robust
robust
robust
robust
21/21 ro-
bust
benchmark [2] and the CF-bench [3] on the patched program
to monitor any of the abnormal behaviors, such as crashes and
hangs. Table 11 has summarized the results for the experiment.
For demonstration purpose, we select 5 CVEs as the example
and list the ﬁnal results with all the 21 patches.
The results show that all the hot patches do not crash or
hang the program. To further examine the patch robustness
in the real-world situation, we have selected and installed top
100 Android applications from the Google App Store. We
use scripts to open, load, and close the application on the
patched system and monitor abnormal behaviors. The result
shows that all the application can be properly executed, which
suggests that the patches maintain good robustness in the real-
world situation. In conclusion, the generated patch does not
break the normal functionalities of the patched program.
5.3 Eﬃciency Evaluation
Since the hot patches inject code into the original functions,
it is important to ensure that the additional code does not add
much overhead to the programs. A less eﬃcient hot patch may
introduce performance bug to the system, which aﬀects its
normal usage. In this experiment, we evaluate the eﬃciency
of the hot patches by measuring the overhead of the program
after patching.
We test the system performance before and after the patch-
ing with AnTuTu benchmark on Google Nexus 5X device.
We control the experiment settings to be the same to test one
hot patch a time. Each of the experiment is repeated 10 times
and the scores are averaged to avoid variations due to noises.
Table 12 lists the performance of the kernels with 5 individual
CVE patches as well as the overall performance with all the
21 patches applied.
Overall, the results suggest that the hot patches do not intro-
duce noticeable overhead system-wise. For the CPU running
time benchmark (3rd column), the patched kernel does not
have signiﬁcant diﬀerences with the original one. For exam-
ple, the kernel with all the patches applied only adds 0.06s for
the total running, which is less than 0.1% in overhead. For the
memory running time benchmark (5th column), the overall
run time for the patched system is even shorter than the orig-
inal one. For the score benchmarks (2nd and 4th columns),
USENIX Association
29th USENIX Security Symposium    2409
Table 12: Patch Overhead Analysis
CVE id
Original Ker-
nel
CVE-2014-
4656
CVE-2014-
9789
CVE-2015-
7515
CVE-2016-
8399
CVE-2016-
10233
Overall
CPU
Score
20620.0
CPU
Time
1:22.30
Mem
Score
4428.3
Mem
Time
1:24.52
20597.9
1:22.78
4576.7
1:23.37
20525.5
1:22.51
4398.1
1:25.12
20731.0
1:22.34
4548.3
1:23.88
20455.7
1:22.36
4368.8
1:24.66
20715.5
1:22.31
4542.6
1:24.32
20587.2
1:22.36
4506.1
1:23.98
all the results are within the reasonable ranges, which are
either slightly higher or lower compared to the original ker-
nel results. Therefore, the patches make low overhead on the
system.
5.4 Threat to Validity and Future Works
In this section, we discuss the limitations of Vulmet and
propose potential future works to improve it. First, the as-
sumption has been made that the hot patch cannot modify the
memory content of the original program. Though it guaran-
tees the stableness of the patched program, it also limits the
workable type of the generated hot patches. There is a large
percentage of vulnerabilities which cannot be ﬁxed by Vulmet
using the existing hot patches. In the future, we would like to
develop algorithms to analyze the semantics of the memory
contents and propose safe memory modiﬁcation operations.
The major challenge is two folds. First, the function stack
information needs to be kept after applying the patch changes.
Since we are not creating the new function stacks, we need to
make sure the newly added patches do not overﬂow the old
stacks. Second, Vulmet needs to be able to insert the changes
in the middle of the functions. The write operation is diﬀerent
from the read operation. At the binary level, a memory write
operation is often followed by some read operations, which
have data dependency on the previous write operation. There-
fore, it is better to change the value at the same place as the
original patch. Thus, to locate the binary instruction in the
middle of the function is important to implement the write
operation in Vulmet. After identifying the patches whose
write modiﬁcation is safe, Vulmet can generate the hot patch
to cover more vulnerabilities.
Second, Vulmet relies on the precise summarization of the
oﬃcial patch semantics to generate correct hot patches. In
the experiments, some generated hot patches are incomplete
because the semantics are not fully extracted by Vulmet. It
needs to have formal semantic analysis capability to deﬁne the
changes made by the original patches. With this, Vulmet will
have less chance to miss out the important semantics of the
oﬃcial patches so that the overall accuracy will be improved.
Third, there are some patches being too complex to be ana-
lyzed. It is diﬃcult to ﬁnd the precise semantics of the large
patches. Therefore, current Vulmet only works on patches
with changes in one function. In the future, we plan to intro-
duce root cause analysis to help to identify the main changes
that can patch the vulnerabilities. Vulmet can generate the
hot patches based only on the main changes so that it does not
need to recover the full semantics for the complex patches.
6 Related Works
6.1 Automatic Patch Generation
Automatic patch generation is a hot topic in security re-
searches [37]. Many diﬀerent approaches have been proposed
to address this problem. The ﬁrst approach attempts to sum-
marize patch patterns and use them to generate new patches
to ﬁx similar vulnerabilities. For example, in 2005, [45] has
proposed automatic patch generation algorithms for the buﬀer
overﬂow vulnerabilities. By monitoring the program opera-
tions in a sandboxed environment during attacks, it generates
patches that can work at the same environment. [23] proposes
PAR, which generates security patches by learning from the
human-written patches. They manually examine the human-
written patches and develop the patch template. Then, they
locate the faults by running the test case and apply corre-
sponding templates to ﬁx the bugs. [33] mines a large number
of human ﬁxes and applies mathematical reasoning model
to search for templates to ﬁx the bugs. [32] also summarizes
patch templates from the human patches and apply them to ﬁx
Java vulnerabilities. Instead of writing the templates manually,
the work uses the clustering method to categorize diﬀerent
patch patterns and summarize the pattern for each of the cat-
egories. DeepFix [19] learns the patch patterns using deep
learning with multi-layered sequence-to-sequence neural net-
work and ﬁx vulnerability with the patterns.
The second approach tries to generate patches by testing
diﬀerent patch candidates with the testcases. The patch that
can pass the test will be selected. Shieldgen [16] generates the
patch for the unknown vulnerabilities via analyzing the zero-
day attack instances. [24, 52] propose and improve GenProg,
which automatically searches for patches using a genetic pro-
gramming algorithm to evolve the variant to ﬁnd the correct
patches. They use mutation and crossover operators to change
the original program and simulate the program evolution. Dur-
ing this evolution, diﬀerent patch behaviors can be executed
so that the best one can be selected to ﬁx the bug. [44] also
leverages on program evolution to automatically search for
patches in the assembly code programs. They demonstrate
that the patch generation at the binary level is as eﬃcient
2410    29th USENIX Security Symposium
USENIX Association
as at the source code level. [28–30, 42, 46] propose tools to
generate patches and conduct an analysis of the eﬀectiveness
of the generation process. They deﬁne the operations that
the patch can perform on the program and generate possible
patch operations. They use heuristics and program analysis
methods to rank the possible patch operations based on their
possibility to ﬁx the vulnerability. Then, they try diﬀerent
patches against the test cases to get the one which allows the
test cases to pass. AutoPaG [26] also tries to generate patches
for the out-of-bound read vulnerabilities in the Linux kernel.
It can catch the violations and summarize the root causes
during the runtime. The patch is then built to address these
problems.
The third approach aims to analyze the cause of the vul-
nerability and build the patches to prevent that. Minthint [21]
generates hints to help the programmers repair the bugs. Sta-
tistical correlation is used to ﬁnd statements that are possible
to appear at the patch location. SIFT [31] uses static pro-
gram analysis to generate input ﬁlter for the integer overﬂow
programs. [54] has proposed AppSealer, a tool which can au-
tomatically generate patches for known component hijacking
vulnerabilities in Android applications. It uses the program
analysis to identify the program slice which leads the vulner-
able places and builds patches to block malicious program
ﬂows. [43] tries to generate ﬁlters for the web server to pre-
vent malicious inputs. It helps the developer by automating
the error-prone ﬁlter writing process. [27] studies real-world
concurrency bugs and generates patches via analyzing the
program ﬂows. SearchRepair [22] has combined all three ap-
proaches. It generates Satisﬁability Modulo Theories (SMT)
constraints for defects, uses program analysis to locate bugs,
and searches patches using test suits. [39] also uses SMT
to solve the constraints to generate patches for buﬀer over-
ﬂow bugs. [35] combines program analysis with data mining
to generate patches with a low false positive rate. Direct-
ﬁx [34] tries to generate simplest source code patches using
a semantics-based repair method so that the patches can be
accepted by the developers.
Unlike these related works, our work has proposed a new
approach by learning semantics from the oﬃcial patches,
which does not require the test cases. Since the generated