that leverages Intel’s Transaction Synchronization Ex-
tensions (TSX). Intel TSX introduces support for hard-
ware transactions, where the L1 and L3 caches are used
as write and read sets, respectively, to keep track of
addresses accessed within the transaction. PRIME +
ABORT monitors accesses to a single cache set by ﬁlling
the cache set during a transaction as any additional ac-
cesses to same cache set causes the transaction to abort.
FLUSH + RELOAD and FLUSH + FLUSH To re-
duce the memory footprint, running processes often
share identical memory pages. Shared libraries is a
prime example of sharing (code) pages. Another exam-
ple is memory deduplication [32], where an active pro-
cess searches for pages with identical contents to coa-
lesce them. While there are hardware mechanisms in
place to ensure isolation between processes by enforcing
read-only or copy-on-write semantics for shared pages,
the existence of shared caches results in an exploitable
side-channel for such pages. Gullasch et al. [17] use the
CLFLUSH instruction to evict targets to monitor from the
cache. By measuring the time to reload them the attacker
determines whether the victim has accessed them—a
class of attacks called FLUSH + RELOAD. Further,
Yarom and Falkner [42] observe that CLFLUSH evicts a
memory line from all the cache levels, including the last-
level cache (LLC) which is inclusive of the lower cache
levels and shared between all processor cores, thus en-
abling an attacker to monitor a victim from another pro-
cessor core. In addition, the FLUSH + RELOAD attack
USENIX Association
27th USENIX Security Symposium    939
Table 2: Overview of existing cache side-channel defenses.
e
r
o
C
-
e
m
a
S
e
r
o
C
-
s
s
o
r
C
n
o
i
t
a
t
n
e
m
e
l
p
m
I
y
g
e
t
a
r
t
S
Sets
  Software
Ways
  Software
Pinning
  Software
  Hardware Ways
Pinning
  Hardware
Ways
Pinning
  Hardware
  Hardware
TSX
Name
Page Coloring [43]
CacheBar [44]
StealthMem [25]
Intel CAT [30, 36]
ARM AutoLock [13]
CATalyst [30]
Cloak [14]
allows for cross-VM attacks.
A variant of FLUSH + RELOAD, FLUSH + FLUSH [16]
builds upon the observation that CLFLUSH aborts early in
case of a cache miss, leading to a side channel. As the
FLUSH + FLUSH attack relies only on the CLFLUSH and
performs no memory accesses, it is a stealthier alterna-
tive to FLUSH + RELOAD.
4 Existing Defenses
As shown in Table 2, the security community developed
several defenses both in software and in hardware to mit-
igate cache side-channel attacks. Given the knowledge
of how memory is mapped to the CPU caches, these de-
fenses can freely partition the memory between distrust-
ing processes in a way that partitions the cache, thus pre-
venting the eviction of each other’s cache lines. There are
three common approaches for achieving this goal: parti-
tioning the cache by sets, partitioning the cache by ways,
and locking cache lines such that they cannot be evicted.
4.1 Hardware Defenses
Intel Cache Allocation Technology (CAT) [36] is a hard-
ware mechanism that is available on a select series of
Intel Xeon and Atom products. Intel CAT allows the OS
or hypervisor to control the allocation of cache ways by
assigning a bit mask to a class of service (CLOS). While
Intel CAT could be used to assign disjoint bit masks to
each security domain, the provided amount of classes of
service, and thus security domains, is limited to four or
sixteen. Instead, Liu et al. [30] leverage Intel CAT as a
defense against LLC side-channel attacks by partition-
ing the LLC into a secure and a non-secure partition.
While applications can freely use the non-secure parti-
tion, the secure partition is loaded with cache-pinned se-
cure pages. However, the secure partition is strictly lim-
ited in size, limiting the number of secure pages one can
support. Similarly, older ARM processors such as the
ARM Cortex A9 implement Cache Lockdown [6, 35],
which enables software to pin cache lines within the L2
cache by restricting the cache ways that can be allocated.
Another hardware mechanism is ARM AutoLock—
originally an inclusion policy designed to reduce power
consumption that also happens to prevent cross-core at-
tacks by locking cache lines in the L2 cache when they
are present in any of the L1 caches [13, 40]. As a result,
to use ARM AutoLock as a defense, sensitive data has to
be kept in the L1 caches, which are limited in size.
Intel TSX introduces support for hardware transac-
tions where the L1 and L3 are used as write and read
sets, respectively, to keep track of accesses within the
transaction. Introduced ﬁrst on Intel Haswell, Intel ini-
tially disabled TSX due to bugs, but it reappeared on Intel
Skylake, although in a limited set of products. Cloak [14]
leverages Intel TSX to mitigate cache attacks. Intel TSX
keeps the working set of a transaction inside the CPU
cache sets and aborts if one of the cache sets overﬂows.
Cloak pre-loads sensitive code and data paths into the
caches and executes the sensitive code inside a transac-
tion to keep its working set inside the cache sets. If an
attacker tries to probe a sensitive cache set, the transac-
tion aborts without leaking whether that cache set was
accessed by the protected code. While effective, Cloak
requires modiﬁcation to the application code and is lim-
ited to computations whose working set can strictly ﬁt
inside CPU caches.
Other than the scalability limitations mentioned above,
another concern with hardware-based defenses is their
lack of portability. Intel CAT or TSX are only available
on a subset of Intel processors and ARM Lockdown only
on older ARM processors, hindering their wide-spread
deployment.
4.2 Software Defenses
On contemporary processors,
the LLC is both set-
associative and physically indexed, i.e. part of the physi-
cal address determines to which cache set a certain physi-
cal memory address maps. While the byte offset within a
page determines the least-signiﬁcant bits of the index, the
most-signiﬁcant bits form the page color. More specif-
ically, a page commonly consists of 64 cache lines that
map to 64 consecutive cache sets in the LLC. Thus, pages
with a different page color do not map to the same cache
sets, a property originally used to improve the overall
system performance [3, 24, 43] or the performance of
real-time tasks [28] by reducing cache conﬂicts. Page
coloring has been re-purposed to protect against cache
side-channel attacks by assigning different colors to dif-
ferent security domains.
940    27th USENIX Security Symposium
USENIX Association
StealthMem [25] provides a small amount of colored
memory that is guaranteed to not contend in the cache.
From this memory, stealth pages can be allocated for
storing security-sensitive data, such as the S-boxes of
AES encryption. To prevent cache side-channel attacks,
StealthMem reserves differently colored stealth pages
for each core and prevents the usage of pages that share
the same color or monitors access to such pages by re-
moving access to these pages via page tables. When such
accesses are monitored, StealthMem exploits the cache
replacement policy to pin stealth pages in the LLC.
CacheBar [44] allocates a budget per cache set to each
security domain at the granularity of a page size, essen-
tially representing the amount of cache ways that the
security domain is allowed to use for each page color.
To record the occupancy, CacheBar monitors accesses to
cache sets and maintains a queue of pages that are present
in the cache set per security domain. To restrict the num-
ber of cache ways that are allocated by a security domain,
CacheBar actively evicts pages from the cache following
an LRU replacement policy.
Note that all these defenses isolate the cache that un-
trusted, potentially attacker-controlled, code can directly
access, but do not account for cache partitions the at-
tacker can indirectly access by piggybacking on trusted
components such as the MMU. As we will show, this
provides an attacker with sufﬁcient leeway to mount a
successful indirect cache attack.
5 XLATE Attacks
To demonstrate the viability of indirect cache attacks, we
focus on an often overlooked trusted hardware compo-
nent that attacker-controlled code can indirectly control
on arbitrary victim platforms: the MMU. As each mem-
ory access from the CPU induces a virtual-to-physical
address translation for which the MMU has to consult
multiple page tables, the MMU tries to keep the results
and the intermediate state for recent translations close to
itself by interacting with various caches, including the
CPU caches. Since the CPU and the MMU share the
CPU caches, it is possible to build an eviction set of vir-
tual addresses of which the page table entries map to cer-
tain cache sets, allowing one to monitor activities in these
cache sets in a similar fashion to PRIME + PROBE.
As the activity of the MMU is trusted, existing
software-based defenses do not attempt to isolate page
table pages. This makes it possible to abuse the MMU
as a confused deputy and mount indirect cache attacks
that bypass these defenses. More speciﬁcally, the MMU
can be used to build eviction sets that map to cache sets
outside the current security domain. We refer to this
new class of attacks as XLATE attacks and discuss how
they leverage the MMU for mounting cache attacks (Sec-
tion 5.1). We then show how XLATE attacks can be
used to bypass the different defense strategies that we
discussed earlier (Section 5.2). Implementing XLATE at-
tacks involves addressing a number of challenges (Sec-
tion 5.3) which we overcome in our concrete implemen-
tation of XLATE attacks described in Section 6.
5.1 Leveraging the MMU
Analogous to the EVICT + TIME, PRIME + PROBE and
PRIME + ABORT, we now introduce XLATE + TIME,
XLATE + PROBE and XLATE + ABORT. There is no
generally-applicable counterpart to FLUSH + RELOAD
in the XLATE family of attacks. Although prior work
has proposed page table deduplication to share identical
page tables between processes [9] (enabling MMU-based
FLUSH + RELOAD), this feature is not readily accessible
on commodity platforms.
All of the XLATE attacks rely on the same building
block, namely ﬁnding an eviction set of virtual addresses
of which the page table entries map to the same cache
set. In PRIME + PROBE, we ﬁnd eviction sets for a target
address by allocating a large pool of pages and adding
each of the pages to an eviction set until accessing the
entire eviction set slows down accessing the target. For
XLATE attacks, eviction sets can be found using a similar
approach, but by using page tables instead of pages.
In XLATE + TIME, we ﬁll a speciﬁc cache set with the
page table entries from the eviction set and then measure
the victim’s execution time to determine if the victim is
accessing the same cache set. To avoid having to measure
the execution time of the victim, we can mount a XLATE
+ PROBE attack where the attacker repeatedly measures
the time it takes to reﬁll the cache set, using the page
table entries of the eviction set, as a memory access to
the same cache set causes one of the page table entries
to be evicted (resulting in a slowdown). Finally, XLATE
+ ABORT leverages Intel TSX by ﬁlling the cache set
with the page table entries of the eviction set within a
hardware transaction. After ﬁlling the cache set, the at-
tacker waits for a short period of time for the victim to
execute. If the victim has not accessed a memory address
that maps to the same cache set, the transaction is likely
to commit, otherwise it is likely to abort.
5.2 Bypassing Software-based Defenses
As discussed in Section 4, existing software-based cache
defenses partition the LLC either by cache ways or
sets [43, 44], or by pinning speciﬁc cache lines to the
LLC [25]. As mentioned, all these defenses focus on
isolating untrusted components such as code running in
a virtual machine, but allow unrestricted access to the
cache to trusted operations—such as the page table walk
USENIX Association
27th USENIX Security Symposium    941
Figure 1: The top shows the LLC being divided into 128 unique
page colors, the bottom left shows how the LLC can be parti-
tioned such that programs can only access a subset of these
page colors, the bottom right shows the situation for their re-
spective page tables.
performed by the MMU. The implications can be seen
in Figure 1, which shows an example of page coloring
to partition the LLC. Even though the cache lines of the
pages themselves are limited to a speciﬁc subset of page
colors, and thus a speciﬁc subset of cache sets, their re-
spective page tables are able to access all page colors.
Similarly, software implementations that restrict the
amount of ways that can be occupied by untrusted appli-
cations for each cache set, such as CacheBar [44], typi-
cally use the page fault handler for this purpose. How-
ever, as the page fault handler is only able to monitor
accesses to pages from the CPU, accesses to page tables
by the MMU go unnoticed. Therefore, the MMU is not
restricted by this limitation and is free to allocate all the
ways available in each cache set. To implement cache
pinning, STEALTHMEM also uses the page fault handler
for the speciﬁc cache sets that may be used to host sensi-
tive data in order to reload those cache lines upon every
access. As the page table accesses by the MMU are not
monitored by the page fault handler, accesses to page ta-
bles that map to the same cache set as the sensitive data,
do not reload those cache lines.
5.3 Summary of Challenges
There are three main challenges that we must overcome
for implementing successful XLATE attacks:
1. Understanding which caches the MMU uses, how it
uses them, and how to program the MMU to load
page table entries in the LLC.
2. Finding an eviction set of pages of which their page
tables map to the same cache set as our target. These
MMU’s
page
to
2:
Figure
translate