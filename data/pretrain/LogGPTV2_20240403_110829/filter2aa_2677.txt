# Transferability of Adversarial Examples to Attack Cloud-based Image Classification Services

## Authors
- Dou Goodman (兜哥)
- Our Team X-Lab

## Keywords
- AI
- Security
- Research
- Open Source Projects

## Today's Topics
- Cloud-based Image Classifier Service
- Adversarial Attacks
- Black-box and White-box Attacks
- Fast Featuremap Loss PGD (FFL-PGD) Attack
- Datasets and Preprocessing
- Attack Evaluation

## Introduction
### Cloud-based Image Classifier Service
Cloud-based image classification services are widely used for various applications, from content moderation to object recognition. However, these services are vulnerable to adversarial attacks, where small, carefully crafted perturbations can cause the model to misclassify images.

### Example
- **Original Image:**
  - Class: Cat
  - Score: 0.99
- **Adversary:**
  - Class: Flesh
  - Score: 0.99

### Types of Attacks
- **Black-box Attack:** The attacker has no knowledge of the model's architecture, parameters, or preprocessing steps.
- **White-box Attack:** The attacker has full access to the model's architecture, parameters, and input.

### Challenges in Attacking Cloud-based Services
- **Unknown Model:**
- **Unknown Parameters:**
- **Unknown Network Structure:**
- **Preprocessing Steps (e.g., resizing, cropping, blurring):**

## Proposed Method: Fast Featuremap Loss PGD (FFL-PGD)
### Overview
- We propose FFL-PGD, an untargeted attack based on a substitute model, which achieves a high evasion rate with a very limited number of queries.
- Unlike previous studies that required millions of queries, our method finds adversarial examples using only one or two queries on average.

### Steps of Our Attack
1. **Substitute Model Training:**
   - **Step 1:**
     - Use pre-trained DNNs on ImageNet as the substitute model.
     - Better top-1 accuracy indicates stronger feature extraction capability.
   - **Step 2:**
     - Simplify the untargeted attack into a binary classification problem (e.g., "Cat" or "Not Cat").
     - Fix the parameters of the feature layer and train only the fully connected layer of the last layer.

2. **Adversarial Sample Crafting:**
   - **Step 1:**
     - Propose FFL-PGD with a novel loss function to improve the success rate of transfer attacks.
     - The loss function \( L \) is defined as:
       \[
       L = \text{Class Loss} + \text{FeatureMap Loss}
       \]
   - **Step 2:**
     - **Class Loss:** Makes the classification result incorrect.
     - **FeatureMap Loss:** Output of the last convolutional layer of the substitute model, representing high-level semantic features and improving the transferability of adversarial samples.

### Datasets and Preprocessing
- **Dataset:**
  - 100 cat images and 100 other animal images selected from the ImageNet validation set.
  - Images are resized to 224x224x3 and converted to RGB format.
- **Preprocessing:**
  - Use the 100 cat images to generate adversarial examples and perform a black-box untargeted attack against real-world cloud-based image classification services.
  - Count the number of top-1 misclassifications to calculate the evasion rate.

## Attack Evaluation
- **Substitute Model:**
  - ResNet-152
- **Attacks:**
  - PGD
  - FFL-PGD
  - Ensemble-model attack

### Escape Rates
- **PGD vs. FFL-PGD:**
  - Increase step size \( \epsilon \) from 1 to 8.
  - FFL-PGD has a success rate over 90% across different cloud-based image classification services.
  - FFL-PGD shows better transferability than PGD.

### PSNR (Peak Signal-to-Noise Ratio)
- **Comparison:**
  - PGD has a higher PSNR, indicating better image quality.
  - Both PGD and FFL-PGD have PSNR values above 20 dB, which is considered acceptable for image quality.

### SSIM (Structural Similarity Index)
- **Comparison:**
  - FFL-PGD has a higher SSIM, indicating better image similarity.

### Ensemble-model Attack
- **Description:**
  - Generates adversarial examples that can fool multiple DNNs simultaneously.
- **Evaluation:**
  - The escape rates for Amazon, Google, and Clarifai are below 50%.
  - Transferability decreases due to preprocessing steps in cloud services, such as resizing and cropping.

## Conclusion
- **False Sense of Security:**
  - Keeping models in the cloud provides a false sense of security.
- **Effectiveness of FFL-PGD:**
  - Our FFL-PGD attack achieves a success rate over 90% among different cloud-based image classification services using only two queries per image.

---

This document provides a clear and professional overview of the research, including the methodology, results, and conclusions.