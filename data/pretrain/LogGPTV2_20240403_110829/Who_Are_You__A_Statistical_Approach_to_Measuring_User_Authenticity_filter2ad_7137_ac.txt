(cid:96)(cid:88)
Algorithm 1 Computing interpolation coefﬁcients
Input: A training set T described as a set of vectors {(cid:126)tj}.
Each (cid:126)tj is an (cid:96) + 1-dimensional vector corresponding to a
legitimate login event. The entries of (cid:126)tj are the probabili-
ties pk(x) for that event, as computed in Equation (9).
Output: Coefﬁcients {λk}(cid:96)
1: Deﬁne a function σ that is a bijection between R(cid:96) and the
2: Deﬁne a function LT ((cid:126)v) : R(cid:96) → R by
open (cid:96)-dimensional simplex Σ(cid:96) ⊂ R(cid:96)+1.
k=0, s.t.(cid:80)
k λk = 1.
LT ((cid:126)v) = −(cid:88)
log((cid:126)tj · σ((cid:126)v))
(12)
j
that maximizes LT ((cid:126)v).
3: Use a numerical algorithm to compute the vector (cid:126)v0 ∈ R(cid:96)
4: Compute (cid:126)λ ← σ((cid:126)v0) ∈ R(cid:96)+1.
5: Return (cid:126)λ.
D. Feature Weighting
The independence assumptions used to derive our scoring
function gu(x) (Eq. 7) lead to a formula in which all features
have equal weight. However, given labeled account compro-
mise data we may ﬁnd that certain features are more important
in detecting account compromises. To incorporate this property
we can use a modiﬁed scoring function
p(A|xk)αk
p(xk)βk
p(xk|u, L)γk
p(u|A)δ
p(u|L) ,
(13)
(cid:33)
(cid:32) d(cid:89)
k=1
ˆgu(x) =
5
C1(5)&C2(3)&C3(1)&ISP1(3)&ISP2(2)&ISP3(1)&ISP4(2)&ISP5(1)&IP1(1)&IP2(1)&IP3(1)&IP4(1)&IP5(1)&IP6(1)&IP7(1)&IP8(1)&IP9(1)&p(ISP1)=39,  p(IP1)=19p(IP1)=p(IP1|ISP1)p(ISP1)=13×39=19if p(unseen IP)=110,  p(IPi)=110if unseen IP comes from ISP1, we smooth p(IP | ISP) and keep p(ISP) constant,p(unseen IP)=p(IP1)=p(IP1|ISP1)p(ISP1)=14×39=112if unseen IP comes from ISP6p(unseen IP)=p(IP9)=p(IP9|ISP6)p(ISP6)=12×19=118C1(5)&C2(3)&C3(1)&ISP1(3)&ISP2(2)&ISP3(1)&ISP4(2)&ISP5(1)&IP1(1)&IP2(1)&IP3(1)&IP4(1)&IP5(1)&IP6(1)&IP7(1)&IP8(1)&IP9(1)&C1(5)&C2(3)&C3(1)&ISP1(3)&ISP2(2)&ISP3(1)&ISP4(2)&ISP5(1)&IP1(1)&IP2(1)&IP3(1)&IP4(1)&IP5(1)&IP6(1)&IP7(1)&IP8(1)&IP9(1)&ISPu&IPu&IPu&IPu&IPu&IPu&IPu&ISPu&IPu&ISPu&IPu&Cu&ISPu&IPu&where the αk, βk, γk, δ,  are real-valued weights. We can learn
the values of these weights from the labeled training data by
running a logistic regression classiﬁer; speciﬁcally, we regress
the sample labels against log(ˆgu(x)).
application-speciﬁc constraints; e.g., in the authentication
setting, an attacker may modify geolocation features if
she can use a different IP address or gain access to a
botnet.
III. ATTACK MODEL AND SCENARIOS
We exploit an attack model deﬁned in [6], [5], which
builds on a popular taxonomy of potential attacks against
machine learning proposed in [3], [2], [31]. This model helps
identify potential attack scenarios that may be incurred by
the learning algorithm during operation, and may suggest
some simple countermeasures to mitigate their impact. The
attack taxonomy categorizes attacks along three main axes:
the security violation, the attack speciﬁcity and the attack
inﬂuence. Based on these characteristics, the aforementioned
attack model allows one to make explicit assumptions on the
attacker’s goal, knowledge of the attacked system, capability
of manipulating the input data, and to devise a corresponding
attack strategy.
Attacker’s Goal. The goal is deﬁned based on the following
two characteristics:
(g.i) The desired security violation. The attacker can affect
system integrity (if account takeovers are undetected),
availability (if legitimate users can no longer access the
system), or privacy (if conﬁdential information about the
system users is leaked) [3], [31], [6].
(g.ii) The attack target. The attack can be targeted (if the
attack targets a speciﬁc user or set of users,
if
the attacker aims to have some speciﬁc samples mis-
classiﬁed), or indiscriminate (if any user account
is
potentially subject to the attack, i.e., any sample can be
misclassiﬁed) [3], [31], [6].
i.e.,
Attacker’s Knowledge. The attacker may have different levels
of knowledge of the learning system [6], [5], [31] and, in
particular, about:
(k.i) the training data;
(k.ii) the feature set, i.e., what features are used (e.g., IP and
(k.iii) the learning algorithm, i.e., the decision function gu(x)
useragent);
(Eq. 7);
(k.iv) its (trained) parameters, i.e., the probability estimates
involved in the computation of gu(x);
(k.v) feedback on decisions (e.g., the attacker may observe
whether a login attempt is classiﬁed as legitimate, suspi-
cious, or malicious).
In authentication problems, it is also worth remarking that the
attacker may know the user credentials, or exploit techniques to
get them (e.g., information leakage from the targeted website
or database).
Attacker’s Capability. It consists of deﬁning:
(c.i) the attack inﬂuence, i.e., whether the attacker can ma-
nipulate only testing data (exploratory), or also training
data (causative) [3], [31], [6]; and
(c.ii) how samples (and the corresponding features) can
be modiﬁed. This aspect should be deﬁned based on
6
Attack Strategy. This amounts to deﬁning how the attacker
implements the attack, based on the hypothesized goal, knowl-
edge, and capabilities. In its most general sense, the attack
strategy can be formulated as an optimization problem that in
the end tells the attacker how to manipulate data to reach the
given goal.
Attack Scenarios. Two main attack scenarios are often consid-
ered in the ﬁeld of adversarial machine learning, i.e., evasion
and poisoning [4], [7], [57], [3], [2], [31], [6], [5]. In an
evasion attack, the attacker manipulates malicious samples at
test time to have them misclassiﬁed as legitimate by a trained
classiﬁer, without having inﬂuence over the training data.
This corresponds to an indiscriminate integrity violation. In
a poisoning attack, the goal is to maximize classiﬁcation error
at test time by injecting poisoning samples into the training
data. Inﬂuence in the poisoning setting is mainly on training
data, and the goal is to cause an indiscriminate, availability
violation.
While understanding the impact of poisoning on our system
may be of interest, it may be very difﬁcult for an attacker
to get access to the training data (i.e., the login history) and
actively manipulate it. On the contrary, evasion attacks are
more likely to occur in practical settings, as the attacker can
more easily manipulate data at test time (e.g., by changing
the IP address or the browser’s useragent) to increase chances
of evading detection by our reinforced authentication module.
For this reason, in this work we focus on the simulation of
evasion attacks against our reinforced authentication system,
as detailed in the next section, and leave the investigation of
poisoning scenarios to future work.
A. Evasion Attacks
According to the framework discussed above, we deﬁne
here several evasion settings considering attackers that have
different knowledge of the attacked system and capabilities of
modifying the way account takeovers are performed.
1) Attacker’s Goal: In the context of user authentication,
the goal of an evasion attack is to manipulate some features
(e.g., the IP or browser’s useragent) to have account takeover
attempts misclassiﬁed as legitimate. Accordingly, the security
violation amounts to an integrity violation (i.e., log in with
the credentials of another user without being caught), while
the attack speciﬁcity can be targeted (if the goal is to log in to
a speciﬁc account, i.e., attack a speciﬁc user or set of users),
or indiscriminate (if the goal is to log in to any account).
2) Attacker’s Knowledge: We can deﬁne different kinds
of attackers based on different assumptions about their level
of knowledge of the attacked system. We consider here three
distinct cases, corresponding to increasing levels of knowledge:
the no-knowledge attacker, the known-password attacker, and
the phishing attacker.
No-knowledge attacker. This is the least skilled attacker. She
does not know any of the system details or implementation,
and does not even know any user credentials. An example
is an attacker trying the password “password” against a list
of possible usernames. Should one of the username/password
pairs happen to be valid, this instance then falls into the known-
credentials case.
Known-credentials attacker (KCA). In this case, it is as-
sumed that the attacker has access to the full credentials for a
user, and is a powerful threat to any password-based system.
Without further security measures, there is no security left
and the attacker can just access the account. This attacker is
characterized by the following points:
• He knows full credentials for one or many users of a site.
Usually, he knows username and password for a user on
a site not protected by two-factor authentication.
• He may have a single (or a small number) of credentials
(targeted KCA), or he may have access to a large number
of credentials (such as a leaked list unknown to the site
owner), and be interested in breaking into any account
(indiscriminate KCA).
Phishing attackers. The most skilled attackers considered here
are those we refer to as phishing attackers. They may have
information beyond the credentials about the user in question;
in particular, additional information could be obtained in one
of the following ways:
• The attacker may personally know the victim and thus
may know where the user is located and which de-
vice/OS/browser the user typically uses.
• In a sophisticated phishing attack, the attacker may also
obtain more detailed information about the user, such as
useragent string and IP address.
In terms of the points (k.i)-(k.v) discussed in Sect. III, in
these three cases scenarios the attacker has different levels
of knowledge of the training data and the features used,
potentially gathered by querying the targeted classiﬁer and
looking at the provided feedback on its decisions, while she
does not exploit any knowledge on how the features are
combined by the classiﬁer.
3) Attacker’s Capability: In the evasion setting, the attacker
can only manipulate data during system operation, while she
can not modify the training data. Each feature value of a
login attempt can be manipulated depending on its nature, and
on speciﬁc assumptions. In the following, we thus consider
potential manipulation of the IP address and the browser’s
useragent. Similar reasoning should be extended to other
features, if considered.
IP address. The attacker can change her IP address by using a
remote proxy server or a botnet. If she attempts to log in as the
targeted user and does not succeed, she may try to randomly
use another IP, potentially not blacklisted. If knowledge about
the user’s ISP or country is available, the attacker may even try
to retrieve an IP from the same source (i.e., ISP or country).
Browser’s useragent. This feature can be also manipulated
by the attacker, by using different browsers to login. Thus, if
an attack is not successful, the attacker may attempt to login
from another browser. Clearly, if it is known that the targeted
user logs in usually using a speciﬁc browser, the same one can
be adopted by the attacker.
4) Attack Strategy: Under the aforementioned assumptions,
the attack strategy that achieves the attacker’s goal of imper-
sonating a targeted user (maximizing the probability of success
of each attack) amounts to mimicking the behavior of the
targeted user, under the constraints imposed by the attacker’s
knowledge and capability of manipulating the feature values.
For instance, if the attacker comes to know that the targeted
user logs in from a given country and using a given browser,
she will do her best to mimic this behavior, i.e., log in using
an IP from the same country and the same browser. In the
adversarial evaluation of Sect. V we will consider two speciﬁc
implementations of KCA and phishing attackers, each with a
different level of knowledge.
It is worth remarking that if an attacker can almost exactly
mimic the behavior of the targeted legitimate users in terms of
their features (in our experiments, by correctly adjusting the
IP address and useragent), then knowing how the classiﬁcation
function works in more detail will not increase the probability
a successful attack. Exploiting knowledge of the classiﬁcation
function becomes useful when the classiﬁcation system relies
on a large number of features (with some potentially more
difﬁcult to mimic than others), so the attacker can understand
which subset is worth attacking ﬁrst to improve chances of
misleading detection (see, e.g., [4], [36], [16]). Since in our
evaluation we consider attackers that can successfully phish
both the IP address and the useragent (i.e., can potentially
mimic exactly the behavior of legitimate users),
is not
worth considering attackers that also exploit knowledge of the
classiﬁer. We thus leave a more detailed investigation of such
attacks to future work.
it
IV. SYSTEM IMPLEMENTATION
We discuss here the prototype system implementation used
in our experiments. Fig. 2 depicts the architecture of the
proposed system in production. We maintain online a table of
all successful user authentication attempts along with prepro-
cessed feature attributes. The feature attributes store includes
the precomputed “risk score” and global probabilities for the
mentioned features. Both the user authentication history store
and the feature attributes store feed into the scoring model,
which computes gu(x) or ˆgu(x). Depending on the provided
conﬁdence score, the user is either granted or denied access.
Further information may be requested using a challenge-
response scheme if the scorer is not very conﬁdent. If the
login attempt is successful, the online user authentication store
is appropriately updated.
In terms of user experience,
if the model score is in
the “gray area” where the user cannot be determined with
conﬁdence to be either legitimate or malicious, we ask for
further information as a proof of the user being legitimate.
This could be a CAPTCHA challenge, a veriﬁcation PIN
sent to the user’s registered phone via SMS, or a link sent
to the user’s email. Since the purpose of a CAPTCHA is
to distinguish bots from humans, this defense can only be
effective against a bot attack. For human-generated attacks the
phone or email veriﬁcation serves the same purpose as two-
factor authentication: a legitimate user should easily be able
7
Fig. 2: Architecture of the proposed system with learning-based reinforced authentication. If the user credentials are wrong, the
user is denied access. If the provided credentials are correct, additional information extracted from the login attempt is processed
by the reinforced authentication system. The system outputs a low conﬁdence score gu(x) if the retrieved information matches
the normal behavior of the given user, as learned from the training data (e.g., logging in from a typical IP address and browser
for that user). If the conﬁdence score is lower than a predeﬁned user-speciﬁc threshold, i.e., gu(x) < θ, the user is correctly
authenticated. Otherwise, access is denied even if the provided credentials are correct.
to pass the challenge, while the attacker must obtain access
to the user’s phone or email account in order to complete the
login.
In our prototype implementation we choose to use the
two features of IP address and useragent. We chose these
features for several reasons: they are “sticky” for real users;
they admit natural hierarchies, so we can apply the techniques
of Sect. II-C to compute the desired probabilities in the
presence of sparse data; they have different levels of cost to
mimic (see Sect. VI); they are not strongly correlated with
bot trafﬁc (our primary goal is to improve detection of non-
bot account compromise); and we could obtain them easily
from the LinkedIn data set. It is true that in practice IP and