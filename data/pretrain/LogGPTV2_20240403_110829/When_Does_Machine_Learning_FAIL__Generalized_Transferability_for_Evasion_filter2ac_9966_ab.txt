trol either t or the natural label yt, which is assigned by
the oracle. Mallory pursues two goals. The ﬁrst goal is
to introduce a targeted misclassiﬁcation on the target by
deriving a training set S from S∗: h = A(S),h(t) = yd,
where yd is Mallory’s desired label for t. On binary clas-
siﬁcation, this translates to causing a false positive (FP)
or false negative (FN). An example of FP would be a
benign email message that would be classiﬁed as spam,
while an FN might be a malicious sample that is not de-
tected. In a multiclass setting, Mallory causes the target
to be labeled as a class of choice. Mallory’s second goal
is to minimize the effect of the attack on Alice’s over-
all classiﬁcation performance. To quantify this collat-
eral damage, we introduce the Performance Drop Ratio
(PDR), a metric that reﬂects the performance hit suffered
by a classiﬁer after poisoning. This is deﬁned as the ra-
tio between the performance of the poisoned classiﬁer
and that of the pristine classiﬁer: PDR = per f ormance(h)
per f ormance(h∗).
The metric encodes the fact that for a low-error classiﬁer,
Mallory could afford a smaller performance drop before
raising suspicions.
3 Modeling Realistic Adversaries
Knowledge and Capabilities. Realistic adversaries con-
ducting training time or testing time attacks are con-
strained by an imperfect knowledge about the model un-
der attack and by limited capabilities in crafting adver-
sarial samples. For an attack to be successful, samples
crafted under these conditions must transfer to the origi-
nal model. We formalize the adversary’s strength in the
FAIL attacker model, which describes the adversary’s
knowledge and capabilities along 4 dimensions:
• Feature knowledge R ={xi ࢼ xi ∈ x, xi is readable}:
the subset of features known to the adversary.
• Algorithm knowledge A′:
the learning algorithm
that the adversary uses to craft poison samples.
• Instance knowledge S′:
stances available to the adversary.
• Leverage W ={xi ࢼ xi ∈ x, xi is writable}: the subset
the labeled training in-
of features that the adversary can modify.
The F and A dimensions constrain the attacker’s under-
standing of the hypothesis space. Without knowing the
victim classiﬁer A, the attacker would have to select an
alternative learning algorithm A′ and hope that the eva-
sion or poison samples crafted for models created by A′
transfer to models from A. Similarly, if some features
are unknown (i.e., partial feature knowledge), the model
used for crafting instances is an approximation of the
original classiﬁer. For classiﬁers that learn a represen-
tation of the input features (such as neural networks),
limiting the F dimension results in a different, approx-
imate internal representation that will affect the success
rate of the attack. These limitations result in an inaccu-
rate assessment of the impact that the crafted instances
will have and affect the success rate of the attack. The
I dimension affects the accuracy of the adversary’s view
over the instance space. As S′ might be a subset or an
approximation of S∗, the poisoning and evasion sam-
ples might exploit gaps in the instance space that are
not present in the victim’s model. This, in turn, could
lead to an impact overestimation on the attacker side. Fi-
nally, the L dimension affects the adversary’s ability to
craft attack instances. The set of modiﬁable features re-
stricts the regions of the feature space where the crafted
instances could lie. For poisoning attacks, this places an
upper bound on the ability of samples to shift the deci-
sion boundary while for evasion it could affect their ef-
fectiveness. The read-only features can, in some cases,
cancel out the effect of the modiﬁed ones. An adversary
with partial leverage needs extra effort, e.g. to craft more
instances (for poisoning) or to attack more of the modi-
ﬁable features (for both poisoning and evasion).
Prior work has investigated transferability without
modeling a full range of realistic adversaries across the
FAIL dimensions. [36] focuses on the A dimension, and
proposes a transferable evasion attack across different
neural network architectures. Transferability of poison-
ing samples in [33] is partially evaluated on the I and
A dimensions. The evasion attack in [25] considers F,
A and I under a coarse granularity, but omits the L di-
mension. ML-based systems employed in the security
industry [21, 10, 45, 39, 12] often combine undisclosed
and known features to render attacks more difﬁcult. In
this context, the systematic evaluation of transferability
along the F and L dimensions is still an open question.
Constraints. The attacker’s strategy is also inﬂuenced
by a set of constraints that drive the attack design and
implementation. While these are attack-dependent, we
broadly classify them into three categories: success, de-
fense, and budget constraints. Success constraints encode
the attacker’s goals and considerations that directly affect
the effectiveness of the attack, such as the assessment of
the target instance classiﬁcation. Defense constraints re-
fer to the attack characteristics aimed to circumvent ex-
isting defenses (e.g.
the post-attack performance drop
on the victim). Budget considerations address the limi-
tations in an attacker’s resources, such as the maximum
number of poisoning instances or, for evasion attacks, the
maximum number of queries to the victim model.
1302    27th USENIX Security Symposium
USENIX Association
Implementing the FAIL dimensions. Performing em-
pirical evaluations within the FAIL model requires fur-
ther design choices that depend on the application do-
main and the attack surface of the system. To simulate
weaker adversaries systematically, we formulate a ques-
tionnaire to guide the design of experiments focusing on
each dimension of our model.
For the F dimension, we ask: What features could
be kept as a secret? Could the attacker access the ex-
act feature values? Feature subsets may not be publicly
available (e.g. derived using a proprietary malware anal-
ysis tool, such as dynamic analysis in a contained en-
vironment), or they might be directly deﬁned from in-
stances not available to the attacker (e.g. low-frequency
word features). Similarly, the exact feature values could
be unknown ( e.g. because of defensive feature squeez-
ing [49]). Feature secrecy does not, however, imply the
attacker’s inability to modify them through an indirect
process [25] or extract surrogate ones.
The questions related to the A dimension are: Is the al-
gorithm class known? Is the training algorithm secret?
Are the classiﬁer parameters secret? These questions de-
ﬁne the spectrum for adversarial knowledge with respect
to the learning algorithm: black-box access, if the infor-
mation is public, gray-box, where the attacker has partial
information about the algorithm class or the ensemble ar-
chitecture, or white-box, for complete adversarial knowl-
edge.
The I dimension controls the overlap between the in-
stances available to the attacker and these used by the
victim. Thus, here we ask: Is the entire training set
known? Is the training set partially known? Are the in-
stances known to the attacker sufﬁcient to train a robust
classiﬁer? An application might use instances from the
public domain (e.g. a vulnerability exploit predictor) and
the attacker could leverage them to the full extent in or-
der to derive their attack strategy. However, some appli-
cations, such as a malware detector, might rely on private
or scarce instances that limit the attacker’s knowledge of
the instance space. The scarcity of these instances drives
the robustness of the attacker classiﬁer which in turn de-
ﬁnes the perceived attack effectiveness. In some cases,
the attacker might not have access to any of the origi-
nal training instances, being forced to train a surrogate
classiﬁer on independently collected samples [50, 29].
The L dimension encodes the practical capabilities of
the attacker when crafting attack samples. These are
tightly linked to the attack constraints. However, rather
than being preconditions, they act as degrees of freedom
on the attack. Here we ask: Which features are modiﬁ-
able by the attacker? and What side effects do the modi-
ﬁcations have? For some applications, the attacker may
not be able to modify certain types of features, either be-
cause they do not control the generating process (e.g. an
Study
F
A
I
L
Test Time Attacks
Genetic Evasion[50]
Black-box Evasion[37]
Model Stealing[46]
FGSM Evasion[17]
Carlini’s Evasion[9]
,
,∅*
,
,∅*
,∅*
,†
,
,
,
,
,∅*
,
,∅*
,
,∅* ∅,∅ ,∅*
∅,∅ ,∅*
,
Training Time Attacks
SVM Poisoning[5]
NN Poisoning[33]
NN Backdoor[20]2
NN Trojan[29]
,∅* ,† ∅,∅ ,∅*
,∅*
,†
,
,†
,
,
,
,†
,
,
,
, 
Table 1: FAIL analysis of existing attacks. For each attack, we analyze
the adversary model and evaluation of the proposed technique. Each
cell contains the answers to our two questions, AQ1 and AQ2: yes (),
omitted () and irrelevant (∅). We also ﬂag implicit assumptions (*)
and a missing evaluation (†).
Study
F
A
I
L
Test Time Defenses
Distillation[38]
Feature Squeezing[49]
,
,
, ,
, ,
Training Time Defenses
,
,
, ,
,
,
,
,
,
,
RONI[34]
Certiﬁed Defense[42]
Table 2: FAIL analysis of existing defenses. We analyze a defense’s
approach to security: DQ1 (secrecy) and DQ2 (hardening). Each cell
contains the answers to the two questions: yes (), and no ().
exploit predictor that gathers features from multiple vul-
nerability databases) or when the modiﬁcations would
compromise the instance integrity (e.g. a watermark on
images that prevents the attacker from modifying certain
features). In cases of dependence among features, tar-
geting a speciﬁc set of features could have an indirect
effect on others (e.g. an attacker injecting tweets to mod-
ify word feature distributions also changes features based
on tweet counts).
3.1 Unifying Threat Model Assumptions
Discordant threat model deﬁnitions result in implicit as-
sumptions about adversarial limitations, some of which
might not be realistic. The FAIL model allows us to sys-
tematically reason about such assumptions. To demon-
strate its utility, we evaluate a body of existing studies by
means of answering two questions for each work.
2Gu et al.’s study investigates a scenario where the attacker per-
forms the training on behalf of the victim. Consequently, the attacker
has full access to the model architecture, parameters, training set and
feature representation. However, with the emergence of frameworks
such as [16], even in this threat model, it might be possible that the
attacker does not know the training set or the features.
USENIX Association
27th USENIX Security Symposium    1303
To categorize existing attacks, we ﬁrst inspect a threat
model and ask: AQ1–Are bounds for attacker limitations
speciﬁed along the dimension?. The possible answers
are: yes, omitted and irrelevant. For instance, the threat
model in Carlini et al.’s evasion attack [9] speciﬁes that
the adversary requires complete knowledge of the model
and its parameters, thus the answer is yes for the A di-
In contrast, the analysis on the I dimension
mension.
is irrelevant because the attack does not require access
to the victim training set. However, the study does not
discuss feature knowledge, therefore we mark the F di-
mension as omitted.
Our second question is: AQ2–Is the proposed tech-
nique evaluated along the dimension?. This question
becomes irrelevant if the threat model speciﬁcations are
omitted or irrelevant. For example, Carlini et al. evalu-
ated transferability of their attack when the attacker does
not know the target model parameters. This corresponds
to the attacker algorithm knowledge, therefore the an-
swer is yes for the A dimension.
Applying the FAIL model reveals implicit assump-
tions in existing attacks. An implicit assumption exists if
the attack limitations are not speciﬁed along a dimension.
Furthermore, even with explicit assumptions, some stud-
ies do not evaluate all relevant dimensions. We present
these ﬁndings about previous attacks within the FAIL
model in Table 1.
When looking at existing defenses through the FAIL
model, we aim to observe how they achieve security: ei-
ther by hiding information or limiting the attacker ca-
pabilities. For defenses that involve creating knowledge
asymmetry between attackers and the defenders, i.e. se-
crecy, we ask: DQ1–Is the dimension employed as a
mechanism for secrecy?. For example, feature squeez-
ing [49] employs feature reduction techniques unknown
to the attacker; therefore the answer is yes for the F di-
mension.
In order to identify hardening dimensions, which at-
tempt to limit the attack capabilities, we ask: DQ2–Is
the dimension employed as a mechanism for hardening?.
For instance, the distillation defense [38] against evasion
modiﬁes the neural network weights to make the attack
more difﬁcult; therefore the answer is yes for the A di-