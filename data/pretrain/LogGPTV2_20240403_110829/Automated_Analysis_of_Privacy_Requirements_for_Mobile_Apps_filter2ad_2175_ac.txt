### Policy Presence and Content Analysis

#### A. Policy Presence

On a per-policy basis, annotators reached agreement on 92 out of 115 policies regarding whether the policy allows the collection of identifiers.

Most apps designed for audiences aged 10+ (75%), Teen (65%), and Mature 17+ (66%) have a privacy policy. In contrast, apps with an "Everyone" rating (52%) or those that are unrated (30%) often lack a policy. Additionally, certain app categories are particularly prone to not having a policy. For example, the Comics (20%), Libraries & Demo (10%), Media & Video (28%), and Personalization (28%) categories have notably low rates of policy presence compared to the overall average of 52% across all categories. By combining these factors, we can identify areas where apps are less likely to have a policy. For instance, in the Media & Video category, the percentage of apps with a policy decreases from 28% for rated apps to 12% for unrated apps. A similar trend is observed in the Libraries & Demo category, where the percentage drops from 10% to 8%.

#### B. Privacy Policy Content

We now shift our focus from the presence of a policy to the analysis of its content, specifically addressing privacy requirements 2-9 as outlined in Figure 1. Our evaluation is based on manually created policy annotations.

**1. Inter-annotator Agreement:**

For training and testing our policy classifiers, we use the OPP-115 corpus [67], which consists of 115 privacy policies annotated by ten law students. This corpus includes 2,831 annotations for the practices discussed in this study. The annotations, detailed in [67], serve as the ground truth for evaluating our classifiers. Each annotator annotated a mean of 34.5 policies (median 35). We select annotations based on majority agreement (i.e., at least two out of three annotators agreed).

From a legal perspective, the frequency of a practice's description in a policy is irrelevant; thus, we measure whether annotators agree that a policy describes a given practice at least once. High inter-annotator agreement indicates the reliability of the ground truth used for training and testing. We use Fleiss’ κ and Krippendorff’s α to measure agreement, with values above 0.8 indicating good agreement, between 0.67 and 0.8 indicating fair agreement, and below 0.67 indicating doubtful agreement [47].

Our results show that the inter-annotator agreement for the collection and sharing of device IDs is fair, with respective values of 0.72 and 0.76. However, the agreement is below 0.67 for the remaining classes. While stronger agreement would be ideal, the observed levels can still provide reliable ground truth as long as classifiers are not misled by systematic disagreement patterns, which can be explored by analyzing the disagreeing annotations [57].

**Analysis of Systematic Disagreement:**

To determine if disagreements contain systematic patterns, we evaluate how often each annotator disagrees with the other two. If an annotator is in the minority for a statistically significant number of times, it may indicate a misunderstanding of the annotation task or other systematic reasons for disagreement. Assuming a uniform distribution, each annotator should be in the minority in one-third of all disagreements. We test this assumption using the binomial test for goodness of fit. Specifically, we calculate the probability of an annotator being in the minority x or more times and compare it to the expected probability of 1/3. We use a one-tailed test as we are only interested in finding whether an annotator is in the minority more than one-third of the time.

As shown in Figure 4, we found few cases of systematic disagreement. Specifically, for 7% (11/160) of disagreements, we found statistical significance (p ≤ 0.05) for rejecting the null hypothesis at the 95% confidence level that the disagreements are equally distributed. Nearly half of these systematic disagreements involve Gil. Excluding Gil's and other affected annotations from the training set can help improve the reliability of the ground truth.

### Summary of Classifiers and Performance

| Practice | NPC | NAED | CID | CL | CC | SID | SL | SC |
|----------|-----|------|-----|----|----|-----|----|----|
| Classifier | SVM | SVM | Log. Reg. | SVM | Log. Reg. | Log. Reg. | SVM | SVM |
| Parameters | RBF kernel, weight | linear kernel | LIBLINEAR solver | linear kernel | LIBLINEAR, L2, weight | LBFGS solver, L2 | linear kernel, weight | poly kernel (4 degrees) |
| Base (n=40) | 0.7 | 0.58 | 0.65 | 0.53 | 0.8 | 0.88 | 0.95 | 0.73 |
| Accpol (n=40) | 0.9 | 0.75 | 0.83 | 0.88 | 0.88 | 0.93 | 0.78 | - |
| 95% CI (n=40) | 0.76–0.97 | 0.59–0.87 | - | - | - | - | - | - |

This table summarizes the classifiers used for each practice, their parameters, and their performance metrics.