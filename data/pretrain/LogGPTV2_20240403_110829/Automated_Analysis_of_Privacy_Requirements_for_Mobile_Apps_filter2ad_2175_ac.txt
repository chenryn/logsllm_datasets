and on a per-policy basis (e.g., for 92 out of 115 policies
the annotators agreed on whether the policy allows collection
of identiﬁers).
Most apps for Everyone 10+ (75%), Teen (65%), and Mature
17+ (66%) audiences have a policy while apps that have an
Everyone rating (52%) or are unrated (30%) often lack one.21
Further, various app categories are particularly susceptible for
not having a policy. Apps in the Comics (20%), Libraries
& Demo (10%), Media & Video (28%), and Personalization
(28%) categories have particularly low policy percentages, as
compared to an average of 52% of apps having a policy across
categories. Combining these predictors enables us to zoom
in to areas of apps that are unlikely to have a policy. For
instance, in the Media & Video category the percentage of
apps with a policy decreases from 28% for rated apps to 12%
for unrated apps. A similar decrease occurs in the Libraries &
Demo category from 10% to 8%.
C. Privacy Policy Content
We now move from examining whether an app has a policy
to the analysis of policy content (i.e., privacy requirements 2-9
in Figure 1). As a basis for our evaluation we use manually
created policy annotations.
1) Inter-annotator Agreement: For training and testing of
our policy classiﬁers we leverage the OPP-115 corpus [67]—a
corpus of 115 privacy policies annotated by ten law students
that includes 2,831 annotations for the practices discussed
in this study. The annotations, which are described in detail
in [67], serve as the ground-truth for evaluating our classiﬁers.
Each annotator annotated a mean of 34.5 policies (median 35).
We select annotations according to majority agreement (i.e.,
two out of three annotators agreed on it). As it is irrelevant
from a legal perspective how often a practice is described in
a policy, we measure whether annotators agree that a policy
describes a given practice at least once.
High inter-annotator agreement signals the reliability of the
ground-truth on which classiﬁers can be trained and tested. As
agreement measures we use Fleiss’ κ and Krippendorff’s α,
which indicate that agreement is good above 0.8, fair between
0.67 and 0.8, and doubtful below 0.67 [47]. From our results
in Table I it follows that the inter-annotator agreement for
collection and sharing of device IDs with respective values
21Ratings follow the Entertainment Software Rating Board (ESRB) [24].
Zoe
Ray
Mae
Liv
Ira
Gil
Dan
Bob
Bea
Ann
Zoe
Ray
Mae
Liv
Ira
Gil
Dan
Bob
Bea
Ann
1
0.26
0.87
0.94
0.32
1
1
0.26
0.11
0.35
1
0.54
0.65
0.26
0.32
0.04
0.86
1
0.97
0.91
1
1
0.35
0.7
0.9
0.01
1
1
1
0.26
0.26
1
0.02
0.54
0.94
0.05
1
1
0.74
1
0.8
0.65
0.43
0.21
0.74
0.01
0.94
0.41
1
0.91
1
0.33
1
1
1
0.26
1
1
0.7
0.33
1
0.33
0.33
0.33
1
0.7
1
1
0.26
1
0.65
0.05
0.21
0.8
0.87
0.21
0.87
1
0.87
0.87
NPC (4.5) NAED (6.6) CID (5.1)
CL (5.1)
CC (6)
SID (1.2)
SL (1.8)
SC (5.1)
0.11
1
0.54
1
1
0.02
1
0.26
0.87
0.41
0.7
1
0.54
0.65
0.91
1
0.7
0.56
0.26
0.74
1
0.04
1
1
1
1
1
1
0.33
0.7
0.7
0.7
1
0.56
1
1
0
1
1
0.56
1
0.7
1
1
1
1
0.56
0.33
0.56
0.02
0.56
0.26
0.7
1
0.56
0.41
1
0.33
1
1
0.32
0.74
1
0.87
1
1
0
0.33
1
0.65
0.33
1
0.74
1
0.94
1
0.41
0.26
0.41
0.11
NPC (4.2) NAED (3.9) CID (1.8)
CL (3.9)
CC (2.7)
SID (2.7)
SL (4.2)
SC (3.9)
p values
1.00
0.75
0.50
0.25
p values
1.00
0.75
0.50
0.25
Fig. 4: Analysis of systematic disagreement among anno-
tators for the different data practices with binomial
tests.
Larger p values mean fewer disagreements. If there are no
disagreements, we deﬁne p = 1. An annotator can be in
the minority when omitting an annotation that the two other
annotators made (top) or adding an extra annotation (bottom).
Our results show few instances of systematic disagreement. The
numbers in parentheses show the average numbers of absolute
disagreements per annotator for the respective practices.
of 0.72 and 0.76 is fair. However, it is below 0.67 for the
remaining classes. While we would have hoped for stronger
agreement, the annotations with the observed agreement levels
can still provide reliable ground-truth as long as the classiﬁers
are not misled by patterns of systematic disagreement, which
can be explored by analyzing the disagreeing annotations [57].
To analyze whether disagreements contain systematic pat-
terns we evaluate how often each annotator disagrees with the
other two annotators. If he or she is in a minority position
for a statistically signiﬁcant number of times, there might be
a misunderstanding of the annotation task or other systematic
reason for disagreement. However, if there is no systematic
disagreement, annotations are reliable despite low agreement
levels [57]. Assuming a uniform distribution each annotator
should be in the minority in 1/3 of all disagreements. We
test this assumption with the binomial test for goodness of
ﬁt. Speciﬁcally, we use the binomial distribution to calculate
the probability of an annotator being x or more times in the
minority by adding up the probability of being exactly x times
in the minority, being x + 1 times in the minority, up to x + n
(that is, being always in the minority), and comparing the result
to the expected probability of 1/3. We use a one-tailed test as
we are not interested in ﬁnding whether an annotator is fewer
times in the minority than in 1/3 of the disagreements.
As shown in Figure 4, we only found few cases with
systematic disagreement. More speciﬁcally, for 7% (11/160) of
disagreements we found statistical signiﬁcance (p ≤ 0.05) for
rejecting the null hypothesis at the 95% conﬁdence level that
the disagreements are equally distributed. We see that nearly
half of the systematic disagreements occur for Gil. However,
excluding Gil’s and other affected annotations from the training
6
Practice
NPC
NAED
CID
CL
CC
SID
SL
SC
Classiﬁer
SVM
SVM
Log. Reg.
SVM
Log. Reg.
Log. Reg.
SVM
SVM
Parameters
RBF kernel, weight
linear kernel
LIBLINEAR solver
linear kernel
LIBLINEAR, L2, weight
LBFGS solver, L2
linear kernel, weight
poly kernel (4 degrees)
Base
(n=40)
Accpol
(n=40)
0.7
0.58
0.65
0.53
0.8
0.88
0.95
0.73
0.9
0.75
0.83
0.88
0.88
0.88
0.93
0.78
95% CI
(n=40)
0.76–0.97
0.59–0.87