### 7.2.4 Turning

Turning data derived from the accelerometer yields the results depicted in Figure 8. As previously mentioned, drivers handle cornering based on their force tolerance and perceived safety. In practice, this data is not particularly effective. Only 3 out of 31 self-identification tests resulted in false alarms, but about half of the possible 930 impersonation attempts were successful. The infrequency of turning events compared to positive or negative acceleration events likely diminishes the utility of this data. More data could potentially improve performance. However, even with sufficient data for training, the system would still require an impractically long data-gathering period to test the identity of new users efficiently, which is a significant concern.

### Figure 9: Trial Statistics Without (Left) and With (Right) Velocity Range Partitioning of Driving Data

The velocity ranges used here are chosen to satisfy the following criteria:
- **Granularity**: The divisions should be numerous enough to capture all behavioral differences a person might exhibit based on speed.
- **Perceptual Consistency**: The divisions should be small enough that the lower bound does not feel significantly different from the upper bound.
- **Data Sufficiency**: The divisions should be few enough and sufficiently large to accumulate adequate data for comparison in a timely manner, enabling efficient decision-making.

Empirical testing revealed that the most appropriate velocity ranges for our experiment are 0-20, 20-40, 40-60, and 60+ mph.

### 7.2.6 Feature Selection Summary

Both sources of acceleration data—GPS and accelerometer—proved useful, each with its own strengths and weaknesses. The accelerometer data, sampled at 50 Hz, was highly granular and could be effectively parsed into acceleration events. However, the magnitudes of the forces recorded by the accelerometer did not show enough variation to uniquely identify users with high accuracy. This may be due to the device's placement on the cushioned passenger seat, which likely dampened the force readings. In contrast, the acceleration calculated from GPS position data provided more accurate magnitudes, as it was directly derived from the location trace. GPS is accurate to within 3-7 meters with 95% confidence [22], so apart from some local inaccuracies, the overall traces were highly accurate. However, these local inaccuracies were significant, making GPS data insufficient for identification on its own. Additionally, GPS data can often ignore reactive (non-preferential) actions that are too brief for the frequency of data collection, which is a benefit for the negative acceleration feature.

From the useful events, we have features including:
1. Positive acceleration measured by the accelerometer.
2. Combined positive and negative acceleration calculated from GPS position data.

Both features are subdivided into four velocity ranges, resulting in a total of eight features. This ultimately led to 97% self-identification and 91% differentiation rates. Our Predictor component’s set of thresholds is configured to accommodate the total number of feature tests in its classification of drivers.

### 7.3 Threshold Size

The number of features required to pass authentication affects both the rate at which car owners are authorized and the rate at which illegitimate users are authorized. We have eight features, with two tests performed on each feature. Given the data split into ten blocks, there are a total of 160 feature tests, 80 for each test type. To use both test types, we combine them according to several rules and thresholds as discussed in Section 5.2. For the K-S test, we set thresholds specifying the maximum number of failed tests that still result in authorization. For the total variation distance, we specify the maximum variation distance that the tests may sum to. Lower thresholds will prevent more users from being erroneously authorized but will also reduce the number of rightful owners being authorized. Conversely, higher thresholds can ensure all legitimate users are authorized, but they will also accept several "thieves."

For this part of the experiment, full training and testing datasets are used to provide the broadest applicable view of the data. These datasets are tested sequentially block by block. To find the optimal set of thresholds, we determine the point where the false alarm and mis-detection curves intersect in Figure 10. In this figure, we search the possible thresholds to find the mis-detection rate associated with a specific false alarm rate. We find an intersection with the lowest total error near 3 false alarms, identifying all but roughly a tenth of illegitimate users.

We also note that with a different threshold selection, we can reduce illegitimate access to around 6% with a 16% false alarm rate. This increase in the false alarm rate might be acceptable for added security, depending on the individual. Because alarms only notify the car owner (depending on the setup), users may find this false alarm rate unobtrusive enough to choose the very stringent and consequently secure threshold set. In practice, users could be given the option to specify their own mis-detection threshold based on their perceived risk and comfort level.

### 7.4 Training Data Size

The training process for a classifier benefits from an abundance of data, though too much data can make a classifier overly specific and less applicable to new data. If the application is to prevent irretrievable vehicle theft, the user would prefer to have this protection as soon as possible. We examine the impact of different training sizes to determine how much data collection is necessary before a user can begin protecting the car.

As our volunteers collected varying amounts of data, we base our training data size on the smaller datasets, those supplied by volunteers who drove for shorter periods. This size is divided into ten blocks to see the effects of increasing the data size on our false alarm and mis-detection rates for all drivers. The maximum training data allowed here is approximately 450 seconds (450 points) of positive and negative acceleration from GPS data, and 180 seconds (9000 points) of positive acceleration measured by the accelerometer. For reference, our users accomplished this amount in 25-45 minutes, depending on how much time was spent cruising or at stop lights. All available testing data is used, and the threshold set used for authentication is that determined in the previous section on Threshold Size. The results are shown in Figure 11.

As expected, the best accuracy is achieved with all available blocks of data included. These error rates are higher than those measured in other tests because we are restricted to the small dataset size. Additionally, because the threshold set is constant, optimized for the full available training size, the false alarms are slightly erratic before settling to lower values in later blocks. The most significant impact here is the reduction in the mis-detection rate, indicating better theft detection with larger training data. We recommend 1.5-2 hours of training time, as our results show that users who provided this amount of data were almost universally identified as themselves with the least number of failed feature tests and were rarely confused with other drivers (mis-detected).

### 7.5 Testing Data Size

The amount of data available for testing unsurprisingly has similar effects on accuracy as the training data. The goal is to require only a small amount of data to perform accurate authentication in time to quickly identify a theft. We therefore analyze the effect of different testing sizes to determine the appropriate length of testing data collection.

As with the training size experiment, we restrict the maximum amount of data allowed to 450 seconds of GPS-sourced positive and negative acceleration alongside 180 seconds of accelerometer-sourced positive acceleration. This equates to 25-45 minutes of driving to ensure the larger datasets do not skew the results to a higher accuracy and a uniform amount of data is used. We allow full training datasets, however, as users in the real world will have full control of their training phase. The threshold set is again held constant, using the results of the Threshold Size study. Results appear in Figure 12.

It is important to note that the thresholds used for this examination of testing data size were optimized for all available data. This threshold set works well for the proper test size, emphasizing the importance of the minimum length of time used for gathering testing data. It also displays the effect of increasing test data on testing stringency. Initially, all users are permitted access, resulting in no false alarms but 100% mis-detection. By the time all 10 blocks of data are used, most unauthorized users are detected, and some legitimate users begin to generate false alarms. Therefore, it is also important to prevent the testing data size from growing too large and too specific.

As further evidence of the effect of additional data, consider Figure 13. The K-S test returns a decision on whether two datasets are from the same distribution, estimating the probability that the two datasets' empirical distributions would be the measured distance apart while still being part of the same overall distribution. This probability is referred to as the p-value, and the points on this figure are the average p-values for all feature tests between users as additional data is included. The line labeled "Self" refers to the average of self-identification tests, and by the criteria above, the p-values for these tests should be large. The line labeled "Others" refers to the average of those tests between distinct users, and these p-values should be small. The figure shows that self-identification tests remain at a flat rate, indicating that it takes very little time to match one's testing data with one's own training data. As desired, the p-values for the "Others" line are below those for the "Self" line. Furthermore, while it takes longer to rule out other users than it does to self-identify, our results show that the p-values diverge quickly after a few final data blocks are added to the testing data.

### 7.6 Evaluation Summary

To conclude this evaluation, we present the following successes:
- We found effective features, including positive acceleration measured by the accelerometer and positive and negative acceleration measured by GPS.
- We further refined these features by partitioning them according to specific velocity ranges.
- We demonstrated that this collection of features can achieve 97% self-identification and 91% differentiation accuracies.
- We illustrated the effects of varying our testing thresholds and the ability to attain very low mis-detection (around 7%) by allowing slightly higher false alarm rates.
- We analyzed the size of training and testing data to determine the requirements for robust accuracy.

### 8. CONCLUSION

In this paper, we proposed a fast automatic driver recognition system that continuously authenticates the driver as the vehicle is operated. Our basic idea is to extract unique features from driving behavior, which cannot be exactly reproduced by a thief driving away in the stolen car. Through an in-depth investigation of typical driving events, we identified effective driving features (i.e., positive and negative accelerations, at multiple speed ranges) to distinguish between the car owner and any unauthorized users. We performed extensive experimental evaluation using driving data collected from 31 volunteers. Our experiment results show that the proposed system can successfully distinguish the current driver as the car owner with 97% accuracy, while also preventing impersonation 91% of the time.

### Acknowledgments

This work is supported by the National Science Foundation under grants 1527144 and 1553304, and the Army Research Office under grant W911NF-14-1-0324.

### 9. REFERENCES

[1] Federal Bureau of Investigation, “Motor vehicle theft,” http://www.fbi.gov/about-us/cjis/ucr/crime-in-the-u.s/2012/crime-in-the-u.s.-2012/property-crime/motor-vehicle-theft, 2012.

[2] Fox News, “Owner of stolen car sued in deadly Hit-and-Run,” http://www.foxnews.com/story/2009/05/15/owner-stolen-car-sued-in-deadly-hit-and-run/, 2009.

[3] “Onstar,” https://www.onstar.com/web/portal/home?g=1, 2014.

[4] “Lojack,” http://www.lojack.com/Home, 2014.

[5] N. H. T. S. Administration, “Vehicle theft prevention: What consumers should know,” http://www.safercar.gov/Vehicle+Owners/Resources/Theft+Prevention.

[6] M. Shahzad, A. X. Liu, and A. Samuel, “Secure unlocking of mobile touch screen devices by simple gestures: You can see it but you can not do it,” in Proc. of the Annual International Conference on Mobile Computing and Networking (Mobicom), 2013.

[7] M. Qi, Y. Lu, J. Li, X. Li, and J. Kong, “User-specific iris authentication based on feature selection,” in Proc. of International Conference on Computer Science and Software Engineering, vol. 1, 2008, pp. 1040–1043.

[8] R. Bowe, “Red flag on biometrics: Iris scanners can be tricked,” https://www.eff.org/deeplinks/2012/07/red-flag-biometrics-iris-scanner-vulnerability-revealed, 2012.

[9] T. C. Clancy, N. Kiyavash, and D. J. Lin, “Secure smartcard-based fingerprint authentication,” in Proc. of the 2003 ACM SIGMM Workshop on Biometrics Methods and Applications, 2003, pp. 45–52.

[10] A. W. Kosner, “iPhone 5s Touch ID fingerprint scanner is a fail,” http://www.forbes.com/sites/anthonykosner/2013/10/15/iphone-5s-touch-id-fingerprint-scanner-is-a-fail-for-20-of-users-heres-what-to-do/, 2013.

[11] L. Li, X. Zhao, and G. Xue, “Unobservable re-authentication for smartphones,” in Proc. of the Network and Distributed System Security (NDSS) Symposium, 2013.

[12] C. Nickel, T. Wirtl, and C. Busch, “Authentication of smartphone users based on the way they walk using k-NN algorithm,” in Proc. of International Conference on Intelligent Information Hiding and Multimedia Signal Processing, July 2012.

[13] N. Clarke and S. Furnell, “Authenticating mobile phone users using keystroke analysis,” International Journal of Information Security, vol. 6, no. 1, pp. 1–14, 2007.

[14] X. Zou and D. M. Levinson, “Modeling pipeline driving behaviors: Hidden Markov model approach,” Journal of the Transportation Research Board, vol. 1980, no. 1, pp. 16–23, 2006.

[15] N. Oza, “Probabilistic models of driver behavior,” in Proc. of Spatial Cognition Conference, 1999.

[16] A. Sathyanarayana, P. Boyraz, and J. Hansen, “Driver behavior analysis and route recognition by hidden Markov models,” in Proc. of International Conference on Vehicular Electronics and Safety, Sept 2008.

[17] C. Miyajima, Y. Nishiwaki, K. Ozawa, T. Wakita, K. Itou, K. Takeda, and F. Itakura, “Driver modeling based on driving behavior and its evaluation in driver identification,” Proceedings of the IEEE, vol. 95, no. 2, pp. 427–437, Feb 2007.

[18] P. Olofsson, Probability, Statistics, and Stochastic Processes 2nd edition. John Wiley, 2012.

[19] J. A. Adell and P. Jodrá, “Exact Kolmogorov and total variation distances between some familiar discrete distributions,” Journal of Inequalities and Applications, vol. 2006, no. 1, p. 64307, 2006.

[20] “How to make a smart car surveillance system using a mobile phone,” http://www.wikihow.com/Discussion:Make-a-Smart-Car-Surveillance-System-Using-a-Mobile-Phone, 2005.

[21] “Open Automotive Alliance,” http://www.openautoalliance.net/, 2014.

[22] National Coordination Office for Space-Based Positioning, Navigation, and Timing, “GPS accuracy,” http://www.gps.gov/systems/gps/performance/accuracy/, 2014.