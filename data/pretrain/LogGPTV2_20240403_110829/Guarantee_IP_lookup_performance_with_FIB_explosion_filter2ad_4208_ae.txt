### Evaluation of Lookup Speed and FIB Update Performance

**Experiment Period:**
The evaluation period began on October 22, 2013, at 8:00 AM. For each Forwarding Information Base (FIB) instance during a specific hour, the real traffic used in the experiments was captured over a 10-minute interval within that hour.

**Lookup Speed with Different Traffic Types:**
- **Figure 9** illustrates the lookup speed of four algorithms (LC-trie, TreeBitmap, Lulea, and SAIL_L) using prefix-based synthetic traffic on 12 FIBs downloaded from www.ripe.net.
- **Figure 10** shows the lookup speed of these same four algorithms with random traffic on FIB2013.

From these figures, we observe that for each algorithm, the lookup speed is highest with real traffic, followed by prefix-based traffic, and then random traffic. This is due to the better IP locality in real traffic, which leads to more efficient CPU caching. Conversely, random traffic has the worst IP locality, resulting in the poorest CPU caching behavior.

**FIB Update Performance:**
We evaluated the FIB update performance of SAIL_L on the data plane. **Figure 11** depicts the number of memory accesses per update for three FIBs (rrc00, rrc01, and rrc03) over a period involving 319 * 500 updates. The average numbers of memory accesses per update for these three FIBs are 1.854, 2.253, and 1.807, respectively. The worst-case scenario observed was 7.88 memory accesses per update.

**Lookup Speed of SAIL_M for Virtual Routers:**
**Figure 12** presents the lookup speed of the SAIL_M algorithm as the number of FIBs increases. The first x FIBs in the set of 12 FIBs (rrc00, rrc01, rrc03, ..., rrc07, rrc10, ..., rrc15) were used for both prefix-based and random traffic. On average, SAIL_M achieves 132 million packets per second (Mpps) for random traffic and 366 Mpps for prefix-based traffic.

**Evaluation on GPU:**
We conducted further evaluations of SAIL_L on a GPU platform using CUDA 5.0. The experiments were performed on a DELL T620 server equipped with an Intel Xeon E5-2630 CPU (2.30 GHz, 6 cores) and an NVIDIA Tesla C2075 GPU (1147 MHz, 5376 MB device memory, 448 CUDA cores). The 12 FIBs (rrc00, rrc01, rrc03, ..., rrc07, rrc10, ..., rrc15) were used for these tests. We measured the lookup speed and latency under various CUDA configurations, including the number of streams (1, 2, ..., 24), blocks per stream (64, 96, ..., 512), and threads per block (128, 256, ..., 1024).

This comprehensive evaluation provides insights into the performance of different routing algorithms under various traffic conditions and hardware configurations.