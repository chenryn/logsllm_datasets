spective packets. By annotating each expedited request with
the pertinent recovery tuple and the pertinent turning point
router, the resulting expedited reply may be unicast to the
particular turning point router, which may subsequently
subcast the reply downstream. Since IP multicast routers
need not maintain replier state, our scheme offers lighter-
weight local recovery than LMS. Moreover, by employing
SRM as a fall-back recovery scheme, CESRM remains ro-
bust in highly dynamic and faulty environments, whereas
LMS does not.
3.4. Expedited vs. Non-Expedited Recoveries
We now compare the recovery latency of CESRM’s ex-
pedited and non-expedited recovery schemes. In this sec-
tion, we let d and RTT = 2d be upper bounds on the one-
way and round-trip distance (delay) between any two mem-
bers of the reliable multicast group.
We ﬁrst consider successful ﬁrst-round non-expedited
recoveries. Since requests and replies are scheduled uni-
formly within the request and reply intervals, a rough up-
per bound on the average latency of a successful ﬁrst-round
non-expedited recovery is given by:
(C1 + 1/2C2)d + d + (D1 + 1/2D2)d + d.
(1)
This delay is exhibited by the scenario in which both the re-
quest and reply are scheduled for transmission at the mid-
point of the request and reply scheduling intervals, respec-
tively. This is a rough upper bound for two reasons. First,
d is an upper bound on the inter-host transmission laten-
cies, and some of the latencies may be smaller. Second,
since multiple requests may be scheduled per loss, the re-
quest that instigates a packet’s recovery is either sent or re-
ceived with higher probability in the ﬁrst half of the request
interval. This is similarly true for replies.
In contrast, an upper bound on the recovery latency of a
successful expedited recovery of CESRM is given by:
REORDER-DELAY+2d = REORDER-DELAY+RTT (2)
Given the typical SRM scheduling parameter values used
by Floyd et al. [4, 5] of C1 = C2 = 2 and D1 =
D2 = 1, the rough upper bound on the average recov-
ery latency of a successful ﬁrst-round non-expedited recov-
ery of CESRM is 6.5 d, or 3.25 RTT. Assuming that the
delay REORDER-DELAY is negligible compared to the la-
tency, i.e., REORDER-DELAY (cid:4) RTT, CESRM’s recov-
ery latency for packets recovered by expedited rather than
ﬁrst-round non-expedited recoveries is reduced by roughly
2.25 RTT.
In the next section, we study the average recovery la-
tencies afforded by both SRM and CESRM in simulations
based on real IP multicast transmissions. We show that the
average recovery time of ﬁrst-round recoveries in SRM in-
deed varies between 1.5 RTT and 3.25 RTT; this corre-
sponds to the average recovery latency of CESRM’s non-
expedited ﬁrst-round recoveries. Moreover, the average dif-
ference in latency between expedited and non-expedited
ﬁrst-round recoveries in CESRM varies between 1 RTT and
2.5 RTT.
4. Evaluation Through Trace-Driven Simula-
tions
We evaluate the performance of SRM and CESRM us-
ing trace-driven simulations in NS2 [3]. Our simulations
reenact the 14 IP multicast traces of Yajnik et al. [15] and,
thus, capture the packet loss locality exhibited in the actual
IP multicast transmissions. We contrast the performance of
CESRM against that of SRM. We consider CESRM in its
simplest form, where router-assistance is unavailable.
We begin this section by describing the 14 IP multicast
transmission traces of Yajnik et al. [15] and the manner
in which we estimate the links on which each loss occurs.
Next, we describe the simulation setup. Finally, we present
our simulation results.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:50:28 UTC from IEEE Xplore.  Restrictions apply. 
Tree
# of
Period
Rcvrs Depth (msec)
Table 1 IP Multicast traces of Yajnik et al. [15].
# of
Pkts
Losses
45001 24086
148970 55987
93734 33506
17637 10276
57030 15879
41751 18911
46443 29686
38539 11803
44956 33040
45404 16814
72519 44649
38724 20872
50202 37833
69994 43578
1 RFV960419
2 RFV960508
3 UCB960424
4 WRN950919
5 WRN951030
6 WRN951101
7 WRN951113
8 WRN951114
9 WRN951128
10 WRN951204
11 WRN951211
12 WRN951214
13 WRN951216
14 WRN951218
1:00:00
1:39:19
1:02:29
0:23:31
1:16:02
0:55:40
1:01:55
0:51:23
0:59:56
1:00:32
1:36:42
0:51:38
1:06:56
1:33:20
80
40
40
80
80
80
80
80
80
80
80
80
80
80
12
10
15
8
10
9
12
10
9
11
11
7
8
8
Source
& Date
Duration
(hr:min:sec)
# of
6
5
7
4
4
5
5
4
4
5
4
4
3
3
4.1. IP Multicast Traces
We use 14 IP multicast
transmission traces of Ya-
jnik et al. [15]. These traces involve single-source IP
multicast
transmissions in which packets are transmit-
ted from the source at a constant rate. These packets
are transmitted using IP multicast to a subset of 17 re-
search community hosts spread out across the US and
Europe.
The data collected from each of the IP multicast trans-
missions involves per-receiver sequences, each of which
indicates which packets were received and the order in
which they were received by the respective receiver. These
per-receiver sequences do not include the packet reception
times. Yajnik et al. also provide the IP multicast tree topol-
ogy for each of the IP multicast transmissions. This topol-
ogy is presumed to be static (ﬁxed) throughout the duration
of the IP multicast transmission. Table 1 lists the source,
number of receivers, IP multicast tree depth, packet trans-
mission period, transmission duration, the number of pack-
ets transmitted, and the number of losses suffered for each
of the 14 traces. For more information regarding the traces,
see [15].
Consider an IP multicast transmission trace. Let k ∈ N
be the ﬁnite number of packets transmitted during the trace
and R be the ﬁnite set of receivers of the IP multicast
transmission. For I = {1, . . . , k} and i ∈ I, we refer to
the i-th packet transmitted during the IP multicast trans-
mission as packet i. As is traditionally done in the lit-
erature [1, 6, 15, 16], we represent the trace data by per-
receiver binary sequences of length k. We deﬁne a map-
ping loss : R → (I → {0, 1}), such that, for i ∈ I and
r ∈ R, loss(r)(i) = 1, if receiver r suffered the loss of
packet i, and loss(r)(i) = 0, otherwise.
We represent the IP multicast tree, along which the k
packets of the IP multicast transmission are disseminated,
as a tuple T = (cid:2)N, s, L(cid:3) consisting of a set of nodes N, a
root node s ∈ N, and a set of directed edges L ⊆ N × N.
The elements N, s, and L of T are further constrained to
form a directed tree rooted at s in which all edges in L are
directed away from s, there is a unique simple path from
s to each other node in N, and the elements of R are ex-
actly the leaf nodes of the tree (and, consequently, R ⊆ N).
The root node s corresponds to the source of the IP multi-
cast transmission, the internal nodes of T correspond to the
IP multicast capable routers of the network that are used to
disseminate the packets transmitted by s, and the leaf nodes
of T correspond to the receivers of the IP multicast trans-
mission. The edges of T correspond to the communication
links that connect the source, routers, and receivers of the
IP multicast transmission. We henceforth also refer to the
edges of T as links.
4.2. Estimating the Links Responsible for the IP
Multicast Transmission Losses
We estimate the links responsible for each loss suffered
during the IP multicast transmission based on the IP mul-
ticast tree topology and the loss pattern observed in the IP
multicast transmission trace for the respective packet. Each
loss pattern observed in a trace may be the result of losses
on either a single or a combination of IP multicast tree links.
For example, the loss pattern involving all receivers may re-
sult from either a single loss on the link leaving the source,
losses on each of the links leading to the receivers, or from a
number of other combinations. We select a particular com-
bination of links to represent each instance of a loss pattern
based on the probability that a packet is dropped on exactly
the links in each combination. We estimate this probability
by ﬁrst estimating the probability that a packet is dropped
on each link of the IP multicast tree, i.e., the link loss rates.
Letting lnn(cid:1) ∈ L be the link that connects the nodes n
and n(cid:2)
, we deﬁne p(lnn(cid:1)) to
be the probability that a packet is dropped along lnn(cid:1) given
that the packet is received by n. The probabilities p(lnn(cid:1)),
for lnn(cid:1) ∈ L, can be estimated either by the method of Ya-
jnik et al. [15] or the maximum-likelihood estimator method
of C´aceres et al. [2]. For the traces used in this paper, we
found that both methods yield very similar link loss proba-
bility estimates. The simulations below are based on the es-
timates obtained using the former method.
, where n is the parent of n(cid:2)
Given the IP multicast tree, it is straightforward to de-
duce the set of link combinations that result in any loss pat-
tern observed in the trace. We assume that the probability
of a packet being dropped on a link is independent of it be-
ing dropped on any other link. We compute the probability
of occurrence of a particular link combination as the prod-
uct of the probabilities of a packet being dropped on the
links in the combination and successfully forwarded on the
links leading to the links in the combination.
More precisely, consider an observed loss pattern x. Let
Cx be the set of all possible link combinations resulting in
x, Lc be the set of links in a combination c ∈ Cx, and
Uc be the set of links that are neither in Lc nor down-
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:50:28 UTC from IEEE Xplore.  Restrictions apply. 
stream of any of the links in Lc. Presuming that the prob-
abilities of loss along the different links of the IP multi-
cast tree are mutually independent, the probability of oc-
currence of the link combination c is estimated by p(c) =
(cid:1)
(1 − p(l(cid:2))). Thus, the probability that
the observed loss pattern x results from the link combina-
tion c as opposed to the other combinations in Cx is given
by pCx
p(l) · (cid:1)
(c) = p(c)/
l(cid:1)∈Uc
(cid:2)
p(c(cid:2)).
c(cid:1)∈Cx
l∈Lc
We select the link loss combination to represent an in-
stance of the loss pattern x in the trace based on the prob-
abilities of occurrence of all link loss combinations result-
ing in x. For 13 of the 14 traces we consider, more than 90%
of the link combinations selected to represent the losses oc-
cur with probabilities exceeding 95%, often very close to
100%. For the remaining trace, 85% of the link combina-
tions selected to represent the losses occur with probabili-
ties that exceed 98%. Thus, our estimates of the links re-
sponsible for the losses suffered in each trace are predomi-
nantly accurate.
Based on the link loss combinations selected to represent
each loss suffered in the trace, we deﬁne the link trace rep-
resentation to be the mapping link : R → (I → L ∪ ⊥),
such that, for r ∈ R and i ∈ I, link(r)(i) is an estimate of
the link responsible for the loss of packet i by receiver r, if
receiver r suffered the loss of packet i, and link(r)(i) =⊥,
if receiver r did not suffer the loss of packet i.
4.3. Simulation Setup
In our simulations, we use the most recent loss expe-
dited requestor/replier selection policy. According to this
policy, the expedited requestor/replier pair is the optimal
requestor/replier pair of the most recent loss that has al-
ready been recovered. In [10], we analyzed the traces of
Yajnik et al. [15] and found that the most recent loss pol-
icy outperforms the most frequent loss policy. This is be-
cause, more often than not, the location of a loss is corre-
lated to a higher degree with the location of the most recent
loss than with the locations of less recent losses. An addi-
tional advantage of the most recent loss policy is the sim-
plicity of its implementation; receivers need only cache a
single optimal requestor/replier pair.
For a given trace, our simulation involves setting up the
IP multicast tree T and disseminating k packets from the
root of the tree to the tree’s leaf nodes. Recall that the IP
multicast tree is presumed to remain ﬁxed throughout the
duration of the IP multicast transmission.
Since the IP multicast
trace information of Ya-
jnik et al. [15] contains no link delay or bandwidth infor-
mation, we had to synthetically choose values for these pa-
rameters. We chose the bandwidth of each link in T to
be 1.5 Mbps. We assume that payload carrying pack-
ets, i.e., original packets and retransmissions, are 1 KB in
size, and control packets (i.e., packet retransmission re-
quests and session messages) are 0 KB. Since the IP multi-
cast transmission period of any of the IP multicast transmis-
sion traces of Yajnik et al. [15] is either 40 ms or 80 ms, the
bandwidth required for the IP multicast transmissions is ei-
ther 200 Kbps or 400 Kbps. Thus, our choice of 1.5 Mbps
for the link bandwidth is sufﬁcient to carry the IP multi-
cast transmission data.
We ran our simulations with three different link delays:
10 ms, 20 ms, and 30 ms, where in each simulation all the
links had the same delay. The results with the three dif-
ferent choices were very similar. We therefore include here
only the results obtained with a link delay of 20 ms. Since
the depth of the IP multicast tree involved in each of the IP
multicast traces of Yajnik et al. [15] ranges from 3 to 7, the
RTTs between the source and receivers in each trace ranges
from 60 ms to 420 ms. The range of these inter-host RTT
values is reasonable for hosts spread out across the US and
Europe.
The simulation of SRM is carried out with the schedul-
ing parameter settings C1, C2 = 2, C3 = 1.5, D1, D2 = 1,
and D3 = 1.5. These correspond to the typical SRM param-
eter settings used by Floyd et al. [4,5]. Since packets are not
reordered in our simulations, we use a REORDER-DELAY
of 0 sec.
Session packets are transmitted with a period of 1 sec. In
order to focus our attention on the performance of CESRM
packet loss recovery scheme, rather than that of the inter-
host distance estimation scheme through session packet ex-
change, we presume that the session packet exchange is
lossless. Since none of the session packets are dropped
throughout our simulation, the inter-host distances are accu-
rately and promptly calculated. Moreover, the IP multicast
transmission is delayed sufﬁciently so that, prior to its be-
ginning, receivers have a chance to exchange session mes-
sages and, thus, estimate their distances to each other.
We inject losses into the simulated IP multicast transmis-
sion according to the link trace representation link, which
identiﬁes estimates of the links responsible for the losses
suffered by each receiver during the actual IP multicast
transmission. By injecting losses in this fashion, we repro-
duce the packet loss pattern present in the actual IP multi-
cast transmission.
In our simulations, we assume that packet loss recov-
ery is lossless; that is, none of the recovery packets (control
packets and retransmissions) are dropped. We chose to sim-
ulate lossless recovery because when message loss is con-
sidered, there is a larger variability in the results. In [10],
we also simulated the protocols with control packets and
retransmissions being dropped based on the link loss prob-
ability estimates computed in Section 4.2. As expected, the
recovery latencies of both SRM and CESRM were slightly
larger, and CESRM exhibited similar performance improve-
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:50:28 UTC from IEEE Xplore.  Restrictions apply. 
Figure 1 Per-receiver average normalized recovery times.
Trace RFV960419; Ave. Norm. Rec. Time
Trace RFV960508; Ave. Norm. Rec. Time
s
T
T
R
#
s
T
T
R
#
s
T
T
R
#
3.5
3
2.5
2
1.5
1
0.5
0
3.5
3
2.5
2
1.5
1
0.5
0
3.5
3
2.5
2
1.5
1
0.5
0
SRM
CESRM
8
9 10 11 12
1
2
3
4
5
6
7
Receiver
Trace UCB960424; Ave. Norm. Rec. Time
SRM
CESRM
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Receiver
Trace WRN951128; Ave. Norm. Rec. Time
SRM
CESRM
1
2
3