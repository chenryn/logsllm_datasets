𝑛
E𝑘∼M(D)
(cid:19)𝜆(cid:35)
M(D)(𝑘)
Thus, we have that
𝑝𝑘(1 − 𝑝)𝑛−𝑘−1.
𝑛
𝑒−𝜖0 − 1
M(D′)(𝑘)
M(D)(𝑘) − 1 =
+ 𝑝
It will be useful to compute M(D)(𝑘)
M(D′)(𝑘) − 1 for the calculations later.
(1 − 𝑝)
+ (𝑛 − 𝑘)
(1 − 𝑝) − 1
𝑒𝜖0 + (𝑛 − 𝑘)
(cid:0)𝑒𝜖0 − 𝑒−𝜖0(cid:1) + 𝑒−𝜖0 − 1
(cid:18) 𝑒2𝜖0 − 1
(cid:18) 𝑒𝜖0 − 1
(cid:19)
(cid:19)
(cid:19)(cid:16)𝑘 −
(cid:18) 𝑒2𝜖0 − 1
(cid:17)
(cid:19)𝜆(cid:35)
(cid:34)(cid:18)
(cid:34)(cid:18)M(D′)(𝑘)
1 + M(D′)(𝑘)
(cid:34)(cid:18)M(D′)(𝑘)
(cid:19)𝑖(cid:35)
(cid:18)𝜆
(cid:19)
= 1 + 𝜆∑︁
(cid:19)𝑖(cid:35)
(cid:34)(cid:18)M(D′)(𝑘)
(cid:19)
(cid:18)𝜆
= 1 + 𝜆∑︁
(cid:19)(cid:32)(cid:0)𝑒2𝜖0 − 1(cid:1)
(cid:33)𝑖
(cid:18)𝜆
(cid:20)(cid:16)𝑘 −
= 1 + 𝜆∑︁
(cid:19)(cid:32)(cid:0)𝑒2𝜖0 − 1(cid:1)
(cid:19) (𝑒𝜖0 − 1)2
(cid:18)𝜆
(cid:20)(cid:16)𝑘 −
+ 𝜆∑︁
Here, step (a) from the polynomial expansion (1+𝑥)𝑛 =𝑛
M(D)(𝑘) − 1(cid:17)(cid:105)
(cid:104)(cid:16) M(D′)(𝑘)
(cid:104)(cid:0)𝑘 − 𝑛
𝑒𝜖0+1(cid:1)2(cid:105)
step (b) follows because the term corresponding to 𝑖 = 1 is zero (i.e.,
= 0), and step (c) from the from the
E𝑘∼M(D)
= 𝑛𝑝(1 − 𝑝) = 𝑛𝑒𝜖0
fact that E𝑘∼M(D)
(𝑒𝜖0+1)2 , which
is equal to the variance of the Binomial random variable.
In view of Remark 6, this completes the proof of Theorem 3.4.
M(D)(𝑘) − 1
M(D)(𝑘) − 1
(cid:18)𝜆
M(D)(𝑘) − 1
(cid:17)𝑖(cid:21)
(cid:1)𝑥𝑘,
(cid:17)𝑖(cid:21)
(cid:33)𝑖
(from (72))
𝑒𝜖0 + 1
𝑒𝜖0 + 1
(c)
= 1 +
(cid:0)𝑛
𝑛𝑒𝜖0
𝑛𝑒𝜖0
𝑛𝑒𝜖0
= E
𝑘=0
𝑖=3
𝑖=1
𝑖=2
𝑖=2
(b)
(a)
2
E
E
E
E
𝑛
𝑛
𝑘
𝑖
𝑖
𝑖
𝑖
.
(70)
𝑘! ,
𝑥𝑘
=
𝜆
𝑚
𝑚
𝑋
𝑝𝑋
−1
𝑖=1
E𝑋𝑖
exp
− 1
𝑋𝑖
𝑝𝑋𝑖
we get
(cid:18) 𝑝′
(cid:33)(cid:33)(cid:35)
(cid:32) 𝜆
(cid:18) 𝑝′
(cid:32) 𝑝′
(cid:19)(cid:170)(cid:174)(cid:172)𝑚
where 𝒑 = [𝑝1, . . . , 𝑝𝐵]. From Taylor expansion of 𝑒𝑥 = 1+∞
(cid:34)(cid:18) 𝜆
(cid:19)(cid:19)𝑘(cid:35)
(cid:18) 𝑝′
(cid:32) 𝑝′
(cid:32) 𝜆
(cid:33)(cid:33)𝑘
(cid:32) 𝑝′
(cid:33)(cid:33)𝑘
(cid:32) 𝜆
(cid:18) 𝜆(𝑒𝜖0 − 1)
(cid:19)𝑘
(cid:19)𝑘 − 𝜆(𝑒𝜖0 − 1)
(cid:34)
𝑚
𝑒
=(cid:169)(cid:173)(cid:171)E𝑋∼𝒑
(cid:19) = 1 +
∞∑︁
∞∑︁
∞∑︁
∞∑︁
∞∑︁
1
𝑘! E𝑋∼𝒑
𝑒
≤ 1 +
= 1 +
E𝑋∼𝒑
− 1
− 1
− 1
1
𝑘!
𝑋
𝑝𝑋
𝑗
𝑝 𝑗
𝑗
𝑝 𝑗
𝑘=1
𝑘=1
𝑗=1
𝑗=1
−1
𝑋
𝑝𝑋
𝑝 𝑗
𝑝 𝑗
𝑚
𝑚
𝜆
𝑚
𝑚
𝑚
𝑝 𝑗
𝑗=1
𝑘=2
𝑘=2
𝑘=1
1
𝑘!
1
𝑘!
= 1 +
= 1 +
𝐵∑︁
𝐵∑︁
𝐵∑︁
(cid:18) 𝜆(𝑒𝜖0 − 1)
1
𝑘!
𝑘=1
− 𝜆 (𝑒𝜖0 − 1)
𝜆(𝑒𝜖0 −1)
(cid:34)(cid:18)M(cid:0)D′
(cid:19)𝜆(cid:35)
𝑚(cid:1) (𝒉)
(cid:18)
= 𝑒𝜆(𝑒𝜖0−1)(cid:20)
1 − 𝜆 (𝑒𝜖0 − 1)
(cid:20)
(cid:21)
𝑚
≤ 𝑒𝜆(𝑒𝜖0−1)𝑒−𝜆(𝑒𝜖0−1)𝑒
−𝜆(𝑒𝜖0 −1)
M (D𝑚) (𝒉)
−𝜆(𝑒𝜖0 −1)
= 𝑒
≤
𝑚
𝑒
𝑒
𝑚
𝑚
𝑚
where the inequality follows from 𝑝′
𝑗 ∈ [𝐵]. Substituting from (71) into (70), we get
𝜆(𝑒𝜖0 −1)
𝑗
𝑝 𝑗
− 𝜆 (𝑒𝜖0 − 1)
(cid:21)𝑚
𝑚
E𝒉∼M(D𝑚)
𝑚
−𝜆(𝑒𝜖0 −1)
𝑚
,
(cid:19)𝑚
𝜆(𝑒𝜖0−1)
1−𝑒
𝑚
(71)
𝑚
≤ 𝑒𝜖0, which holds for all
= 𝑒
≤ 𝑒
𝜆2(𝑒𝜖0 −1)2
𝑚
.
(since 1 − 𝑥 ≤ 𝑒−𝑥)
(since 1 − 𝑒−𝑥 ≤ 𝑥)
■
This completes the proof of Theorem 7.1.
E OMITTED DETAILS FROM SECTION 8
In this section, we provide a complete proof of Theorem 3.4.
Consider the binary case, where each data point 𝑑 can take a
value from X = {0, 1}. Let the local randomizer R be the binary
randomized response (2RR) mechanism, where Pr [R (𝑑) = 𝑑] =
𝑒𝜖0+1 for 𝑑 ∈ X. It is easy to verify that R is an 𝜖0-LDP mechanism.
𝑒𝜖0
1
For simplicity, let 𝑝 =
𝑒𝜖0+1. Consider two neighboring datasets
D, D′ ∈ {0, 1}𝑛, where D = (0, . . . , 0, 0) and D′ = (0, . . . , 0, 1).
Let 𝑘 ∈ {0, . . . , 𝑛} denote the number of ones in the output of the
shuffler. As argued in Section 2.3 on page 4, since the output of
the shuffle mechanism M can be thought of as the distribution
of the number of ones in the output, we have that 𝑘 ∼ M(D) is
Session 7D: Privacy for Distributed Data and Federated Learning CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea 2341