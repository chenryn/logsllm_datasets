AggS-1
1.0
1.0
Overall
11.4
4.0
1.2
3.0
1.1
1.7
2.5
99% COV
426.0
5.1
5.1
189.0
0.7
4.0
1.1
15.0
0.5
5.0
23.0
1.7
6.8
11.0
transient problems such as software bugs, conﬁguration errors, and
hardware faults related to ASIC and memory.
ToRs have low failure rates. ToRs have among the lowest fail-
ure rate across all devices. This observation suggests that low-cost,
commodity switches are not necessarily less reliable than their ex-
pensive, higher capacity counterparts and bodes well for data cen-
ter networking proposals that focus on using commodity switches
to build ﬂat data center networks [3, 12, 21].
We next turn our attention to the probability of link failures at
different layers in our network topology.
Load balancer links have the highest rate of logged failures.
Figure 5 shows the failure probability for interface types with a
population size of at least 500. Similar to our observation with de-
vices, links forwarding load balancer trafﬁc are most likely to ex-
perience failures (e.g., as a result of failures on LB devices).
Links higher in the network topology (CORE) and links con-
necting primary and back up of the same device (ISC) are the sec-
ond most likely to fail, each with an almost 1 in 10 chance of fail-
ure. However, these events are more likely to be masked by network
redundancy (Section 5.2). In contrast, links lower in the topology
(TRUNK) only have about a 5% failure rate.
Management and inter-data center links have lowest failure
rate. Links connecting data centers (IX) and for managing devices
have high reliability with fewer than 3% of each of these link types
failing. This observation is important because these links are the
most utilized and least utilized, respectively (cf. Figure 2). Links
connecting data centers are critical to our network and hence back
up links are maintained to ensure that failure of a subset of links
does not impact the end-to-end performance.
4.4 Aggregate impact of failures
In the previous section, we considered the reliability of indi-
vidual links and devices. We next turn our attention to the aggregate
impact of each population in terms of total number of failure events
and total downtime. Figure 6 presents the percentage of failures and
downtime for the different device types.
Load balancers have the most failures but ToRs have the most
downtime. LBs have the highest number of failures of any device
type. Of our top six devices in terms of failures, half are load bal-
ancers. However, LBs do not experience the most downtime which
is dominated instead by ToRs. This is counterintuitive since, as we
have seen, ToRs have very low failure probabilities. There are three
factors at play here: (1) LBs are subject to more frequent software
faults and upgrades (Section 4.7) (2) ToRs are the most prevalent
device type in the network (Section 2.1), increasing their aggregate
effect on failure events and downtime (3) ToRs are not a high pri-
ority component for repair because of in-built failover techniques,
such as replicating data and compute across multiple racks, that aim
to maintain high service availability despite failures.
We next analyze the aggregate number of failures and down-
time for network links. Figure 7 shows the normalized number of
failures and downtime for the six most failure prone link types.
Load balancer links experience many failure events but rela-
tively small downtime. Load balancer links experience the second
highest number of failures, followed by ISC, MGMT and CORE
links which all experience approximately 5% of failures. Note that
despite LB links being second most frequent in terms of number
of failures, they exhibit less downtime than CORE links (which, in
contrast, experience about 5X fewer failures). This result suggests
that failures for LBs are short-lived and intermittent caused by tran-
sient software bugs, rather than more severe hardware issues. We
investigate these issues in detail in Section 4.7.
We observe that the total number of failures and downtime
are dominated by LBs and ToRs, respectively. We next consider
how many failures each element experiences. Table 5 shows the
mean, median, 99th percentile and COV for the number of failures
observed per device over a year (for devices that experience at least
one failure).
Load balancer failures dominated by few failure prone devices.
We observe that individual LBs experience a highly variable num-
ber of failures with a few outlier LB devices experiencing more
than 400 failures. ToRs, on the other hand, experience little vari-
ability in terms of the number of failures with most ToRs experi-
encing between 1 and 4 failures. We make similar observations for
links, where LB links experience very high variability relative to
others (omitted due to limited space).
4.5 Properties of failures
We next consider the properties of failures for network ele-
ment types that experienced the highest number of events.
355]
x
<
X
P
[
]
x
<
X
P
[
0
.
1
8
.
0
6
.
0
4
.
0
2
.
0
0
.
0
0
.
1
8
.
0
6
.
0
4
.
0
2
.
0
0
.
0
5 min
LB−1
LB−2
ToR−1
LB−3
ToR−2
AggS−1
Overall
]
x
<
X
P
[
0
.
1
8
.
0
6
.
0
4
.
0
2
.
0
0
.
0
LB−1
LB−1
LB−2
LB−2
ToR−1
ToR−1
LB−3
LB−3
ToR−2
ToR−2
AggS−1
AggS−1
Overall
Overall
1 week
1 hour
1 day 1 week
]
x
<
X
P
[
0
.
1
8
.
0
6
.
0
4
.
0
2
.
0
0
.
0
three 9’s
four 9’s
five 9’s
two 9’s
LB−1
LB−1
LB−2
LB−2
ToR−1
ToR−1
LB−3
LB−3
ToR−2
ToR−2
AggS−1
AggS−1
Overall
Overall
5 min
1 hour
1 day
1e+02
1e+03
1e+04
1e+05
1e+06
1e+00
1e+02
1e+04
1e+06
1e+02
1e+03
1e+04
1e+05
1e+06
Time to repair (s)
Time between failures (s)
Annual downtime (s)
(a) Time to repair for devices
(b) Time between failures for devices
(c)Annualized downtime for devices
Figure 8: Properties of device failures.
TRUNK
TRUNK
LB
LB
MGMT
MGMT
CORE
CORE
ISC
ISC
IX
IX
Overall
Overall
5 min
1 hour
1 day
1 week
TRUNK
TRUNK
LB
LB
MGMT
MGMT
CORE
CORE
ISC
ISC
IX
IX
Overall
Overall
]
x
<
X
P
[
0
.
1
8
.
0
6
.
0
4
.
0
2
.
0
0
.
0
1 hour
1 day 1 week
5 min
]
x
<
X
P
[
0
.
1
8
.
0
6
.
0
4
.
0
2
.
0
0
.
0
three 9’s
two 9’s
TRUNK
TRUNK
LB
LB
MGMT
MGMT
CORE
CORE
ISC
ISC
IX
IX
Overall
Overall
four 9’s
five 9’s
1e+02
1e+03
1e+04
1e+05
1e+06
1e+00
1e+02
1e+04
1e+06
1e+02
1e+03
1e+04
1e+05
1e+06
Time to repair (s)
Time between failures (s)
Annual downtime (s)
(a) Time to repair for links
(b) Time between failures for links
(c) Annualized downtime for links
Figure 9: Properties of link failures that impacted network trafﬁc.
4.5.1 Time to repair
This section considers the time to repair (or duration) for fail-
ures, computed as the time between a down notiﬁcation for a net-
work element and when it is reported as being back online. It is
not always the case that an operator had to intervene to resolve the
failure. In particular, for short duration failures, it is likely that the
fault was resolved automatically (e.g., root guard in the spanning
tree protocol can temporarily disable a port [10]). In the case of
link failures, our SNMP polling interval of four minutes results in a
grouping of durations around four minutes (Figure 9 (a)) indicating
that many link failures are resolved automatically without operator
intervention. Finally, for long-lived failures, the failure durations
may be skewed by when the NOC tickets were closed by network
operators. For example, some incident tickets may not be termed
as ’resolved’ even if normal operation has been restored, until a
hardware replacement arrives in stock.
Load balancers experience short-lived failures. We ﬁrst look
at the duration of device failures. Figure 8 (a) shows the CDF of
time to repair for device types with the most failures. We observe