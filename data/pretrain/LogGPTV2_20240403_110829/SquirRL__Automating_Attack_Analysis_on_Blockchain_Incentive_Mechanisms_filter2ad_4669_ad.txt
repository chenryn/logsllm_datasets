Fig. 2: Sample state for Bitcoin. Sub-
chain Ch (blue solid blocks) denotes the
public (honest) main chain, and subchain
Ca (black striped blocks) denotes the
agent’s private chain.
Fig. 3: Bitcoin relative reward as a function
of adversarial hash power. SquirRL recovers
the ﬁndings of [56].
Fig. 4: Bitcoin relative reward under
stochastic α (Gaussian random process with
E[α] = 0.4).
V. EVALUATION: SINGLE STRATEGIC AGENT
We ﬁrst consider applications of SquirRL to selﬁsh mining
attacks when there is a single strategic agent, and the remaining
agent(s) follow protocol [19], [48], [56]. We will focus here
on the Bitcoin protocol; analogous experiments on Ethereum
can be found in Appendix B. In our ﬁrst batch of experiments,
the main beneﬁt of DRL over algorithms for solving MDPs
(e.g., value iteration) is that DRL can handle larger state spaces.
We then demonstrate that DRL learns good strategies in a
stochastically varying environment of unknown distribution,
which is not possible with an MDP. These experiments lay the
groundwork for Section VI, where we describe experiments
with multiple strategic selﬁsh mining parties. Our experiments
compare to several baseline mining strategies:
(1) Honest mining: a miner who follows protocol.
(2) Optimal selﬁsh mining (OSM): the strategy learned in
[56] for the Bitcoin protocol.
(3) SM1: the selﬁsh mining strategy originally proposed in
[19]; although this baseline should be strictly dominated by
OSM in the Bitcoin setting, it has been used in other settings
as well [51]; we include it for completeness.
(4) SquirRL: the strategies output by our training pipeline.
In each of our experiments, we train a DRL agent and
simulate all baseline strategies to compete. Then, for a given
parameter setting (e.g., initial adversarial party’s fraction of hash
power α), we run 100 trials of each blockchain protocol, where
each trial consists of 10000 steps in the MDP and generates at
least 5000 blocks in the main chain. We compute the relevant
parties’ rewards for each trial, and average over all trials.
A. Static Hash Power
The Bitcoin protocol is a useful case study in part because
its incentive mechanism is well-studied [18], [19], [48], [56].
Prior work has recovered an optimal selﬁsh mining strategy in
the one-strategic-agent case when hash power is static [56]. A
useful sanity check is thus to see if SquirRL recovers these
known optimal results. In [56], the authors recover the optimal
strategy for selﬁsh mining in Bitcoin by casting the problem
as an MDP and applying policy iteration. We aim to recover
and replicate two key ﬁndings of their work:
(1) Selﬁsh mining is only proﬁtable for adversaries who hold
at least 25% of the stake in the system; this assumes that if the
adversary publishes a block at the same time and height as the
honest chain, the honest nodes will build on the adversary’s
block with probability γ = 0.5. An adversary who holds less
than 25% of the stake should revert to honest mining.
(2) For adversaries with more than 25% of the stake, the authors
of [56] show performance curves that quantify the adversary’s
relative increase in rewards compared to honest mining. Our
goal is to match these curves.
Figure 3 demonstrates the outcome of this experiment, using
the state space developed in section IV-D. We observe two key
ﬁndings. First, for α  0.25, we ﬁnd that SquirRL achieves a relative
reward within 1% of the true optimal mechanism. This result
required minimal tuning of hyperparameters.
We ﬁnd similar results for the Ethereum blockchain in
Appendix B. Ethereum’s larger state space (more complex
reward mechanism) makes it poorly-suited to value iteration.
Here, SquirRL easily recovers strategies with higher rewards
than state-of-the-art approaches [24], [51], [54].
B. Variable Hash Power
In the previous experiments (Bitcoin, Ethereum), it is
possible to write an MDP approximating the system dynamics
(even if the state space is large). In more realistic blockchain
settings, the underlying MDP may be changing over time or
unknown. In this section we explore such a scenario, where
the adversary’s hash power α changes stochastically over time.
This can happen, for instance, if the adversary maintains a ﬁxed
amount of hash power (in megahashes/day) while the total hash
power in the cryptocurrency ﬂuctuates, or if miners dynamically
re-allocate hash power over time to different blockchains [32],
[47]. In either scenario, formulating an MDP is challenging for
two reasons: (1) We may not know the distribution of random
process α(t); (2) Even if we can estimate it (e.g. from historical
data), incorporating this continuous random process into an
MDP would bloat the feature space.
SquirRL handles this uncertainty by using the current value
of α during training without any knowledge of the underlying
8
0.00.10.20.30.40.5Attacker's Hash Power 0.00.20.40.60.81.0Relative RewardSquirRLOSMSM1honest0.320.340.360.380.400.420.440.400.450.500.550.600.650.700.0000.0250.0500.0750.1000.1250.1500.1750.200Standard Deviation of Gaussian Process0.300.350.400.450.500.550.60Relative RewardSquirRLOSMSM1honestStrategy
Name
Bitcoin
Monacoin
Vertcoin
Litecoin
Honest
0.398 ± 0.008
0.407 ± 0.007
0.406 ± 0.007
0.408 ± 0.007
SM1
0.540 ± 0.016
0.552 ± 0.017
0.554 ± 0.014
0.564 ± 0.016
OSM
0.566 ± 0.018
0.594 ± 0.020
0.597 ± 0.020
0.603 ± 0.019
SquirRL
0.585 ± 0.019
0.602 ± 0.020
0.602 ± 0.020
0.608 ± 0.022
TABLE III: Relative rewards under stochastic α as measured
in real cryptocurrencies from September 24-October 28, 2019.
Results shown for initial α = 0.4. We show the average and
the standard deviation results for 100 repetitions.
random process. We ﬁnd that SquirRL learns more robust
strategies than those in the literature and is therefore less likely
to overreact to outlying values.
We evaluate performance for stochastic α by ﬁrst allowing
α(t) to vary according to a Gaussian white noise random
process with E[α(t)] = 0.4, in line with major cryptocurrency
mining pools’ fractional hash powers [2]. Figure 4 illustrates
the relative reward as a function of the standard deviation
of this process. We truncate ﬂuctuations to α ≤ 0.5 to avoid
51% attacks. When α(t) has low variance, our results are
consistent with those from Section V-A: SquirRL achieves
relative rewards close but not identical to OSM. However, as
the variance increases, SquirRL actually starts to outperform
OSM. Intuitively, the learned strategies are less likely to react
to ﬂuctuations in α(t), thereby preventing the agent from taking
extreme actions for anomalous events. We would consequently
expect SquirRL to perform particularly well on blockchains
with low (and hence more volatile) total hash power.
To explore the effect of stochastic α in the wild, we ran
SquirRL on data from real cryptocurrencies. We ﬁrst scraped
the estimated total hash power hourly for a month for four
blockchains that use Bitcoin’s consensus protocol and block
reward mechanism: Bitcoin, Litecoin, Monacoin, and Vertcoin—
we include the latter three to demonstrate the generality of
the framework. We trained SquirRL in an environment where
α(t) followed a Gaussian white noise random process with
standard deviation 0.1. We chose this parameter after observing
that the average deviation between consecutive hash power
measurements (measured every three hours over a month) in
all four cryptocurrencies were below 0.1. We then assumed an
attacker with constant raw hash power (in MH/day); this raw
hash power is chosen by initializing the attacker at a relative
hash power of α = 0.4 in each measured blockchain. Once the
absolute hash power is ﬁxed, the attacker’s relative hash power
α ﬂuctuates solely due to changes in the total hash power of
each blockchain. Table III shows the relative rewards resulting
from various strategies. SquirRL achieves the highest relative
rewards (although within statistical error of OSM), showing
RL’s beneﬁts in environments that change in ways difﬁcult to
capture with an MDP.
VI. MULTI-AGENT SELFISH MINING EVALUATION
The previous section demonstrated the ability of SquirRL
to (a) learn a known optimal strategy for Bitcoin, (b) extend
prior state-of-the-art results on Ethereum in a setting where
the state space is too large for an MDP solver, and (c) learn
strategies in a stochastic, possibly nonstationary environment.
In this section, we instead demonstrate DRL’s ability to
handle nonstationary environments in which multiple strategic
agents are competing in the Bitcoin selﬁsh mining scenario.
This section has three main ﬁndings. In a multi-strategic-
agent setting: (1) OSM is not a Nash equilibrium. (2) The
commonly-studied rushing adversary can have counterintuitive
and nonphysical results. This has general implications for how
the research community should model multi-agent security
problems moving forward. (3) We do not observe any beneﬁt
to selﬁsh mining when k ≥ 3 strategic agents are competing.
This suggests that even over an inﬁnite time horizon, selﬁsh
mining is not a serious attack for the Bitcoin protocol.
A. Model
We generalize the model from Section IV. Recall that for a
single strategic agent, we used γ to denote the probability of the
honest party choosing an adversarial block over an honest one
in the event of a match. For the multi-agent setting, we instead
deﬁne the follower fraction γi, which we brieﬂy described
above. For i ∈ {1, . . . ,k}, γi is the probability of the honest
agent building on the ith agent’s block in case of a k-way tie.
This models each party’s network connectivity. In case of a tie
among fewer parties, the γi values are normalized appropriately.
The multi-agent setting requires a different abstraction than
MDPs: Partially Observed Markov Game (POMG) [81]. A
POMG is a tuple (N,S,{Ai}1≤i≤N,P,{Ri}1≤i≤N,Ω,{Oi}1≤i≤N),
where N denotes the number of agents, S is the state space
for all the agents, Ai is the action space for agent i, P : S×
A1 ×···× AN × S → R denotes the transition probability from
a state s ∈ S to s(cid:48) ∈ S given joint action a ∈ A1 × ··· × AN,
Ri : S× A1 ×···× AN × S → R is the reward function for agent
i that determines the immediate reward transitioning from state
s ∈ S to state s(cid:48) ∈ S with joint action a ∈ A1 ×···× AN, Ω is
the space of observations, and Oi : S → Ω maps the state to the
observation that agent i sees.
In our setting, S is the space of all possible blocktrees. Ai = A
for all i, where A was the action space from the single-strategic-
agent setting. Ri(s,a,s(cid:48)) = (1 − αi)xi − αiyi where xi is the
number of blocks agent i received in the process of transitioning
from s ∈ S to s(cid:48) ∈ S with joint action a ∈ A1 ×···× AN, and yi
is the number of blocks all agents other than i (including the
honest agent) received in the process of transitioning. Ω is the
space of blocktrees except without the hidden chains included.
Oi(s) is the blocktree observed by agent i (note that we use
feature extraction as before, except applied to the observations,
to simplify this representation).
But what is N? It should include all of the strategic agents,
of course, but should it include the honest party? In [19], [56]
(which study the single-agent setting) the honest party is treated
as part of the environment and accounted for in the probability
transition matrix and in the reward matrix. In recent work
on multi-agent selﬁsh mining [45], the honest party is also
considered to be a part of the environment.
Consider a POMG with honest party A and strategic agents
B and C. If the honest party is in the environment, then upon
receiving the joint observation o ∈ Ω2, agents B and C submit
the joint action a ∈ A2. After the actions are submitted, a block
is awarded to one of A,B,C and A performs an action (notably,
before B and C perform any more actions or observe anything).
9
Rewards are given to the agents. Then agents receive another
observation o ∈ Ω2, and the cycle repeats. Notice that even
in the DRL literature, it is standard to represent agents with
known strategies as part of the environment [41], [52].
If A is outside of the environment (i.e., included as an agent),
then upon receiving the joint observation o ∈ Ω3, agents A,B,C
submit joint action a ∈ A3. After actions are submitted, a block
is awarded to one of the parties and rewards are allocated.
Then the cycle repeats. The difference is in when the strategic
agents can see the hidden state of A: when A was part of the
environment, A never had a hidden chain because it publishes
while the POMG processes the transition. When A is considered
an agent, it doesn’t publish its block until the next turn, which
means its hidden chain length could be nonzero in length.
As we are interested in studying worst-case security, it is
tempting to think that giving B,C more information is a more
conservative choice, thus implementing the honest party as
part of the environment. We start with this assumption to be
consistent with [45], [56]. However, we show in Section VI-B
that this decision can lead to counterintuitive results.
Training Methodology. As a starting point, our models are
trained using PPO using the default conﬁgurations in [39] for
532000 episodes, with a batch size of 524288 steps. We will
detail the training methodology inline for later experiments.
This large batch size is typical of RL applications, such as
OpenAI Five [52]. Each episode in our experiments consists of
100 block creation events. Each error bar depicts the largest,
middle, and smallest of 3 data points. Each of these data points
is the average of 100,000 episodes.
B. Challenges of modeling a rushing adversary
We start by asking if OSM is a NE for multiple strategic
agents. If OSM is a NE, we don’t need SquirRL—learning
the best strategy for a single strategic agent is enough, at least
for Bitcoin. Consider three agents: A is honest, B is running
OSM, and C is using DRL. We compare this to a setting where
both agents B and C are using OSM. Note that this setup
encompasses settings where A has no hash power.
Figure 5 shows Agent C’s excess relative rewards (an agent’s
relative reward minus its hash power) when agents B and
C each have a fraction α ≤ 0.5 of the hash power, and the
honest agent has a 1− 2α fraction; we use follower fractions
γB = γC = 0.5. Agent C always does better with SquirRL than
with OSM, suggesting that OSM is not a NE with multiple