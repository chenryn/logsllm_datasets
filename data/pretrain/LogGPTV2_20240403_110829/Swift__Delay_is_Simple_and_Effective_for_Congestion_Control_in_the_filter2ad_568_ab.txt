mand and completion queue API: applications submit commands
to Pony Express, also known as "Ops" and receive completions. Ops
map to network flows, and Swift manages the transfer rate of each
flow. The overall algorithm is specified in Algorithm 1. We give
details below by component.
3.1 Using Delay to Signal Congestion
Delay is the primary congestion signal in Swift because it meets
all our requirements. TIMELY noted that RTT can be measured
precisely with modern hardware, and that it provides a multi-bit
congestion signal, i.e., it encodes the extent of congestion and not
only its presence. Swift further decomposes the end-to-end RTT to
separate fabric from host issues; it has made delay measurements
much more precise through a combination of timestamps in NIC
hardware and in polling-based transport like Pony Express. We
describe the delay components and how they are measured in Swift.
Component Delays of RTT
Figure 2(a) shows the components that make up an RTT, from
locally sending a data packet to receiving the corresponding ac-
knowledgement from a remote endpoint.
• Local NIC Tx Delay is the time the packet spends in the NIC
Tx queue before it is emitted on the wire. When the networking
stack uses a pull model [36], the host hands the packet to the
NIC when the NIC is ready to send it, so this delay is negligible.
516
▷ Enforces MD once every RTT
Algorithm 1: Swift Reaction To Congestion
1 Parameters: ai: additive increment, β: multiplicative decrease
constant, max_md f : maximum multiplicative decrease factor
2 cwnd_prev ← cwnd
3 bool can_decrease ←
(now − t_last_decrease ≥ rtt)
4 On Receiving ACK
retransmit_cnt ← 0
5
tarдet_delay ← TargetDelay()
6
if delay < target_delay then
7
if cwnd ≥ 1 then
8
cwnd ← cwnd + ai
9
else
10
cwnd ← cwnd + ai · num_acked
11
if can_decrease then
cwnd ← max(1 − β · ( delay−tarдet_delay
1 − max_md f ) · cwnd
▷ Multiplicative Decrease (MD)
cwnd · num_acked
▷ Additive Increase (AI)
▷ See S3.5
else
delay
12
13
14
),
15 On Retransmit Timeout
16
17
18
19
20
21
retransmit_cnt ← retransmit_cnt + 1
if retransmit_cnt ≥ RET X _RESET _T HRESHOLD then
cwnd ← min_cwnd
else
if can_decrease then
cwnd ← (1 − max_md f ) · cwnd
22 On Fast Recovery
23
24
25
retransmit_cnt ← 0
if can_decrease then
cwnd ← (1 − max_md f ) · cwnd
26 cwnd ←
clamp(min_cwnd, cwnd, max_cwnd)
27 if cwnd ≤ cwnd_prev then
t_last_decrease ← now
28
29 if cwnd < 1 then
pacinд_delay ← r tt
30
31 else
pacinд_delay ← 0;
32
Output: cwnd, pacinд_delay
cwnd
▷ Enforce lower/upper bounds
Note that the host delay in this situation is not considered part
of the packet layer; it is observed at higher layers.
• Forward Fabric Delay is the sum of the serialization, propaga-
tion and queuing delays for the data packet at switches between
the source and destination. It also includes the NIC serialization
delay.
• Remote NIC Rx Delay is the time the packet spends in the re-
mote NIC queue before it is picked by the remote stack. This
delay can be significant when the host is the bottleneck. For
example, in the context of Snap [36], this delay can rise quickly
when the packet processing capacity falls due to memory pres-
sure and CPU scheduling.
Kumar et al.
Figure 2: (a) Components of end-to-end RTT for a data packet and corresponding ACK packet. (b) Timestamps used to measure different delays (Hardware
and software timestamps are shown in blue and red, respectively).
• Remote Processing Delay is the time for the stack to process
the data packet and generate an ACK packet, including any
explicit ACK delays.
• Remote NIC Tx Delay is the time spent by the ACK packet in
the NIC Tx queue.
• Reverse Fabric Delay is the time taken by the ACK packet on
the reverse path. Note that the forward and reverse paths may
not be symmetric.
• Local NIC Rx Delay is the time spent by the ACK packet before
it is processed by the stack to mark the delivery of the data
packet.
The forward fabric delay is the primary indicator of network
congestion, while the remote NIC Rx delay is the primary indicator
of host congestion. Sender-based congestion control should not
react to reverse path congestion since it has no direct control over
it. If needed, ACKs can be prioritized by using a higher quality
of service (QoS) class. However, we find the reverse path delays
are well controlled in practice, since the reverse side traffic is also
controlled by Swift.
Measuring Delays
Swift uses multiple NIC and host timestamps to separate the
components of delay. Modern NICs widely support hardware times-
tamps that are accessible from host networking stacks, including
Pony Express and kernel TCP. We describe the implementation in
Snap [36]; details depend on the networking stack.
Figure 2(b) depicts the event time sequence. t1 is the time a data
packet is sent, as recorded by the stack. t2 is when the packet lands
on the remote NIC. It is available via the hardware timestamp that
the NIC marks on the descriptor. t3 is when the packet is processed
by the stack, thus t3 − t2 is the time that the packet spends in the
NIC queue. The key here is synchronizing the NIC clock (which
provides t2) and the host clock (which provides t3). We use a simple
linear extrapolation algorithm to translate the NIC timestamp to
the host clock (Appendix A provides further details). t4 is the time
the ACK is ready to be sent out by the stack, and so t4 − t3 gives us
the processing time.
We sum the NIC Rx delay and the processing time to obtain
remote-queuing, and reflect this delay to the sender via a header
on the ACK packet. The NIC Rx timestamp is appended locally to
the packet descriptor and is not sent on the wire. In our experience,
4 bytes are enough for microsecond-level precision. Details of the
packet format changes are in Appendix B. Finally, t5 and t6 are
the corresponding receive timestamps for the ACK at the original
sender. End-to-end RTT is t6 − t1.
3.2 Simple Target Delay Window Control
The core Swift algorithm is a simple AIMD controller based on
whether the measured delay exceeds a target delay. We found sim-
plicity to be a virtue as TIMELY evolved to Swift and removed
some complexity, e.g., by using the difference between the RTT and
target delay rather than the RTT gradient. Below we describe the
algorithm based on a fixed target delay, then elucidate the end-host
and fabric parts in §3.3, and dynamic scaling based on topology
and load in §3.5.
The controller is triggered on receiving ACK packets. Swift reacts
quickly to congestion by using instantaneous delay as opposed
to minimum or low-pass filtered delay. In addition, Swift does not
explicitly delay ACKs. Both choices mitigate staleness concerns in
using delay as a congestion-signal [60]. Lines 4–14 in Algorithm 1
provide Swift’s reaction on receiving an ACK; if the delay is less than
the tarдet, the cwnd (measured in packets) is increased by ai
cwnd
(ai = additive increment), such that the cumulative increase over an
RTT is equal to ai. Otherwise, the cwnd is decreased multiplicatively,
with the decrease depending on how far the delay is from the target,
i.e., we use multiple bits of the delay signal for precise control. The
multiplicative decrease is constrained to be once per RTT, so that
Swift does not react to the same congestion event multiple times.
We do this by checking against the time of the last cwnd decrease.
The initial value of cwnd has little effect in our setting because
Pony Express maintains long-lived flows.
3.3 Fabric vs. Endpoint Congestion
Many congestion control designs focus on the fabric as the net-
work, and ignore host issues. We learned over time that host issues
are important and need a different congestion response. To do so,
we split the RTT into fabric delay due to links and switches, and
end-host delay that happens in NIC and host networking stack.
First, Swift computes endpoint-delay as the sum of remote-queuing
(echoed in the ACK) and Local NIC Rx Delay (given by t6 − t5).2
Then, Swift computes fabric-delay as RTT minus endpoint-delay.
Swift then uses two congestion windows, fcwnd to track fabric
congestion, and ecwnd to track endpoint congestion. Both windows
follow Algorithm 1 with a different fabric-delay-target and endpoint-
delay-target. There is a slight difference in that we use Exponentially
Weighted Moving Average (EWMA) filtering for the endpoint delay,
given that endpoint delays are more noisy in our experience.
The effective congestion window is combined as min(fcwnd,
ecwnd).3 Note the similarity to how TCP uses the minimum of
cwnd and receiver advertised window, where advertised window
serves the role of ecwnd. In the context of Snap [36], delay is a
2We provide the reasoning behind including this delay in Appendix C.
3Both cwnds are updated together and a ceiling value is used as a guard for the
non-bottlenecked cwnd.
517
5. Remote NIC Tx Delay3. Remote NIC Rx DelayTraffic Roundabout2. Forward Fabric DelayLocal EndpointTxRemote EndpointTxRxSwitch QueueSwitch Queue6. Reverse Fabric Delay1. Local NIC Tx Delay7. Local NIC Rx DelayRx4. Remote Processing Delayt1: t_sentt2: t_remote_nic_rxt4: t_ack_sentt3: t_remote_host_rxt5: t_local_nic_rxt6: t_local_host_rxDATARemote NIC Rx DelayProcessing DelayLocal NIC Rx DelayRemote QueuingACK(a)(b)Swift: Delay is Simple and Effective for Congestion Control in the Datacenter
Figure 3: Target delay encapsulates both fixed and variable parts and is dy-
namically scaled based on topology and load.
better measure of host congestion than advertised window. It is
directly tied to all bottlenecks on the host, including CPU, memory,
PCIe bandwidth, caching effects, thread scheduling, etc., whereas
advertised window captures memory allocation (and very indirectly
CPU bottlenecks). In addition, the advertised window is used for
flow-control, i.e., to prevent a flow from over-running a buffer,
and does not aim for fairness across flows when the host is the
bottleneck.
Separating fabric and host congestion in the design of Swift
had a huge impact in production, with the tail latency of most
applications improving by 2×, and none suffering a regression. We
give more production results in §4.
3.4 Large-Scale Incast
During deployment, we ran into applications that relied on ex-
tremely large incasts, with thousands of flows destined to a single
host simultaneously. In this scenario, when number of flows ex-
ceed the path BDP, even a congestion window of one is too high
to prevent overload. To handle such cases, we augmented Swift
to allow the congestion window to fall below one packet down to
a minimum of 0.001 packets. This case needs special handling of
the increment update (Lines 7–11 of Algorithm 1). To implement
a fractional congestion window, we translate it to an inter-packet
delay of RTT
cwnd (Lines 29–32 of Algorithm 1) that the sender uses to
pace packets into the network. For example, a cwnd of 0.5 results
in sending a packet after a delay of 2 × RTT . The pacing is imple-
mented using a Timing Wheel [43]. Results from production (§4)
show pacing is critical to maintain low latency and loss at scale.
While conventional wisdom is that always-on pacing is bene-
ficial in terms of smooth traffic and lower losses, we found that
pacing packets for moderate or higher flow rates did not provide
better performance than an ACK-clocked window. Moreover, pac-
ing packets is not CPU efficient compared to ACK clocking. Beyond
the CPU cost of pacing data packets, added CPU is consumed on
the receiver due to reduced opportunities for ACK coalescing, and
on the sender due to a corresponding increase in the number of
ACKs. TIMELY used rate control but did not suffer from these prob-
lems because it paced 64KB chunks, which allowed for efficient
use of CPU. But for a Snap transport that operates in MTU-sized
units, pacing is mostly not necessary for performance, nor is it
CPU-efficient. In Swift, we finesse this issue by normally using
ACK-clocked congestion window and shifting to pacing when the
cwnd falls below 1.
3.5 Scaling the Fabric Target Delay
So far, we have described Swift with a fixed target delay. Here, we
describe how to scale the target fabric delay (henceforth referred
to as target delay) to the latency of paths that are longer or heavily
loaded.
Target delay encapsulates both the fixed and the variable parts
of the fabric delay, as shown in Figure 3. The base portion of target
Figure 4: Average queue buildup with randomized flow arrival and perfect
rate control grows as O(√N).
delay consists of delays incurred for a single hop network with a
small number of flows: propagation delay, serialization delay in NIC
and switch (which depends on link speed), queuing delay for a small
number of flows, measurement errors from software and hardware
timestamps, as well as any unaccounted delays in network, e.g.,
resulting from QoS scheduling. On top of this base, we scale target
delay based on topology and load.
Topology-based Scaling: While using a single target delay that
is high enough to cover propagation and serialization delays across
the datacenter diameter gives us good overall throughput, it comes
at the cost of building larger queues for traffic that takes shorter
paths, e.g., intra-top-of-rack (intra-ToR) network, plus some RTT
unfairness. Instead, we want to use smaller targets for flows with
shorter paths to improve performance.
Measuring the minimum path delay is not simple in the Internet,
as shown by prior work [12]. For datacenters, the topology is known
and network distance is bounded. Given this environment, we
translate the network path for a flow to a target delay by using a
fixed base delay plus a fixed per-hop delay. We measure the forward-
path hop count by subtracting the received IP TTL (Time-To-Live)
values from known starting TTL, and reflect it back in the ACK-