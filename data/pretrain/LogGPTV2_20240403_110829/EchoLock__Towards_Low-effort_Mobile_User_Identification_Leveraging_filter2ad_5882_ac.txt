duration at a 48kHz sampling frequency. During ultrasonic sensing,
the recorded structure-borne propagation of the chirp signal will
be embedded with information on the user’s hand geometry. While
a shorter chirp minimizes exposure to environmental reflections, it
also limits the signal-to-noise ratio (SNR). From our experiments,
we observed that most COTS speakers struggle to consistently
sweep a wide, high frequency band in such a short time period. To
balance these considerations, we transmit a series of consecutive
chirps, where each chirp is of a singular frequency such that the
first chirp is 18kHz while the 𝑛-th chirp is of 22kHz. This frequency
increments in steps of 1kHz at every 𝑛/5-th chirp.
In addition, we separate these chirps with 25ms of empty buffers
to stagger the arrival of environmental reflections from structure-
borne sound. A pilot signal of 22kHz is prefixed to the sequence for
the purposes of simplifying signal segmentation in later procedures,
but is not used directly to sense information on the user. By utilizing
multiple chirps, we can also gather multiple user samples in a
single ultrasonic sensing instance. This leads to a natural trade-off
dilemma between higher classification accuracy and shorter time
delays. We show in further detail the performance accuracy for
increasingly large 𝑛 values in Section 8.5.
5.3 Structure-borne Signal Segmentation
While structure-borne and airborne sound are both capable of car-
rying information indicative of the surrounding environment, air-
borne sound is less reliable as a metric due to distortions from the
multipath effect. The minute delay of airborne sound may also intro-
duce noise to subsequent structure-borne sound transmissions due
to asynchronous arrival time, necessitating separation of the two in
order to maintain robust feature extraction and user identification.
Existing studies leverage the difference between the propaga-
tion speed of sound waves to distinguish between the structure-
borne and airborne signals [37]. Knowing that propagation is faster
through physical mediums, we can expect structure-borne sound
to always arrive at the microphone earlier compared to airborne
sound. Therefore, we apply cross-correlation to derive similarities
between the recorded acoustic signal with the original transmis-
∞
sion and identify the structure-borne signal based on the time of
arrival. In particular, we compute correlation through the function
𝑚=0 𝑥∗(𝑚)𝑦(𝑚 + 𝑑) where 𝑥∗(𝑚) is the complex conjugate of our
transmission and 𝑦(𝑚 + 𝑑) is the recorded propagation sequence
time shifted by some unknown delay 𝑑. Both 𝑥∗(𝑚) and 𝑦(𝑚 + 𝑑)
are normalized to compensate for amplitude differences in the dif-
fering sound propagation. By locating the index with the highest
correlation to our transmission, we can determine the start point
of our signal. This recorded signal contains both structure-borne
and airborne sound propagation, represented by several amplitude
peaks. Environmental reflections arrive at a much slower speed,
HandTablePocketHandTablePocketSession 14: CPS Security ASIA CCS ’20, October 5–9, 2020, Taipei, Taiwan776between these vectors such that di = (|𝑢1−𝑣1|, |𝑢2−𝑣2|, ..., |𝑢𝑘−𝑣𝑘|)
with the intuition that a signal that is properly modified by hand
geometry would produce a noticeably distinct signal and therefore
a larger di vector. Next, we order the chirps based on the average
of di and select the first 𝑛/2 chirps as the optimal chirps to describe
the hand biometrics of the user.
These chirps are used for geometry estimation, where we develop
a final characterization of the user’s hand biometrics. Similar to
the previous selection process, each frequency step (i.e. 18𝑘Hz,
19kHz, etc.) is ordered based on the proportion of optimal chirp
signals the frequency produced, determined by the 𝑖 denotation. A
multidimensional array is constructed using all feature matrices
of the first frequency, followed by 𝑠 number of features matrices
from the second frequency. This subset 𝑠 is chosen to be 𝑛/10
through experimentation, with feature matrix selection based off
chirp signals with the largest 𝑑𝑖 score. The optimal chirp selection
method is performed in the profiling stage, where the index of
the selected optimal chirp is stored with the recorded acoustic
sound as a user’s profile. During hand biometric identification,
these chirps are referenced for geometry estimation. Note that
different users may have different combinations of optimal chirp
signals, which increase the diversity of users’ profiles and help
improve the identification accuracy.
Moreover, we use multiple consecutive 𝑛-chirp sequences in test-
ing to capture the user’s hand geometry at different times, aiming
to improve the robustness of the user identification by performing
a majority vote on the results from the acoustic response resulted
from the multiple consecutive 𝑛-chirp sequences. We specify an
odd number of optimal chirps in order to prevent ties when voting
on the final decision in the identification process.
6 HAND BIOMETRIC USER IDENTIFICATION
6.1 Acoustic Feature Extraction
After obtaining the structure-borne echos from the received desig-
nated signal, the system extracts from it unique features to analyze
the interferences caused by the user’s hand and derive a hand
biometric profile, which integrates both physiological and behav-
ioral traits. A series of candidate features are identified for their
potential responsiveness to different user hand biometrics. These
features include statistical properties in the time domain, the spec-
tral points in the frequency domain, and acoustic properties such
as Mel-Frequency Cepstral Coefficient (MFCC) and Chromagram
features.
Time-domain Statistic Features. In the time domain, we choose
to analyze signals by its statistics including mean, standard devi-
ation, maximum, minimum, range, kurtosis and skewness. We also
estimate the signal’s distribution by calculating second quantile,
third quantile, fourth quantile and signal dispersion. Additionally,
we examine peak change by deriving the index position of the data
point that deviates most significantly from the statistical average.
Figure 5 illustrates how these features may be applied to differenti-
ate hand geometry. We extract our time-domain features for two
distinct users while they are using the same model mobile device.
By plotting the kurtosis, standard deviation, and range features,
we can see an apparent clustering effect, indicating these statistics
show viability as distinguishing factors.
(a) Full Recording
(b) Extracted Signal
Figure 4: Signal segmentation on a microphone recording
containing the desired acoustic signal.
thus we can safely eliminate interference by keeping our transmis-
sion short (i.e. milliseconds long), calculating the signal endpoint
based on sequence length, and segmenting audio at this point.
Due to the short distance between the speaker and microphone,
structure-borne sound usually overlaps with airborne sound. There-
fore, the obtained signals still contain partial airborne signals, which
must be accounted for. As such, we apply a third-order median filter
to mitigate undesirable outliers introduced by airborne sound in
our recorded signal. The output of the median filter is considered to
only contain the structure-borne signals. Figure 4 shows an exam-
ple acoustic signal before and after the proposed structure-borne
signal segmentation. A short chirp signal sweeping within the in-
audible frequency band is played during a 10 second recording and
successfully extracted using our outlined procedure. An inspec-
tion of the frequency domain confirms that we have preserved our
signal without any persisting interference from airborne sound or
environmental reflections.
5.4 Hand Geometry-Induced Chirp Selection
Although EchoLock utilizes a sequence of chirp signals to validate
the user, we find that not all n chirps necessarily provide equally
detailed information on hand geometry. The intuition behind our
proposed system is that the form and grip of the user’s hand would
shape the transmitted chirp signal in such a unique way that it can
be used for identification purposes. However, we observe during
development that some chirps signals were not transformed in a
meaningful way, bearing more resemblance to waveforms originat-
ing from scenarios depicted in Figure 3(b) than Figure 3(a).
The absence of new information in these signals may be attrib-
uted to a variety of factors, particularly if the user were to tem-
porarily disrupt physical contact with the device due to fidgeting.
This contributes to misidentification rates as separate users with
similarly uninformative chirp signals consequentially have similar
training inputs in our machine learning framework. As such, we
develop an optimal chirp selection method to quantify the impact
of the user’s hand on our n-chirp sequence. The optimal chirps iden-
tified by the method are used to build a particular user’s identity
profile and identify the user during geometry estimation.
In particular, we denote a chirp signal of 𝑘 points as a vector
ui = (𝑢1, 𝑢2, ..., 𝑢𝑘), where 𝑖 is the 𝑖-th chirp of the sequence such
that 1 ≤ 𝑖 ≤ 𝑛. We also denote the baseline chirp signal, originating
from the scenario where the phone is put on the desk (Figure 3(b)),
as vi = (𝑣1, 𝑣2, ..., 𝑣𝑘). Then we compute the absolute difference
246810Time (s)05101520Frequency (kHz)-150-100-50Power (dB)200400600800Time (ms)16182022Frequency (kHz)-120-100-80-60-40-20Power (dB)Session 14: CPS Security ASIA CCS ’20, October 5–9, 2020, Taipei, Taiwan777Figure 5: Illustration of the time-domain
statistical features to differentiate two
people’s hands.
Figure 6: Spectral analysis of the re-
ceived acoustic signal for two users
when holding a mobile device.
Figure 7: Euclidean distance of the stan-
dard deviation feature for two users rel-
ative to a separate instance for Person 2.
Frequency-domain Features. In the frequency domain, we
apply Fast Fourier Transformation (FFT) to the received acoustic
signal and derive 256 spectral points to capture the unique charac-
teristics of the user’s holding hand in the frequency domain. This
is because the holding hand can be considered as a filter, which
results in suppressing some frequencies while not affecting others.
Figure 6 shows an example spectral analysis of the received sound
for two separate users. We observe unique responses produced
by these individuals, which consequentially produce unique FFT
points when extracting our frequency-based features. While these
two users exhibit mostly identical responses within the 20-21kHz
range, our optimal chirp selection process ensures these similar-
ities are not heavily considered when attempting to differentiate
between different people.
Acoustic Features. We also derive the acoustic features from
the received sound using the MFCC [24] and Chromagram [27].
MFCC features are normally applied in speech processing studies
to describe the short-term power spectrum of the speech sound and
are good for reflecting both the linear and non-linear properties
of the sound’s dynamics. Chromagram, often referred to as "pitch
class profiles", is traditionally utilized to analyze the harmonic and
melodic characteristics of music and categorize the music pitches
into twelve categories. We have observed the sensitivity of the
MFCC and Chromagram to be sensitive enough to respond to phys-
ical biometrics as well. In this work, we derive 13 MFCC features
and 12 chroma-based features to describe the different hand holding-
related interferences to the sound. Our MFCC features include 13
filter bank coefficients processed using Discrete Cosine Transform
whereas our chroma-based features describe a correlation between
the recorded signal and one of the 12 tonal pitches along a even-
tempered scale.
6.2 Hand Biometric Feature Selection
After feature extraction, we obtain 12 time-domain features, 256
frequency-based features, and 25 acoustic features for 293 total
features. Some features are more sensitive to the minute differences
of handwhile the others may not be very effective at distinguishing
between them. Moreover, mobile devices from different vendors
may have their speaker and microphone embedded at different po-
sitions. These hardware distinctions introduce further uncertainty
when measuring the user using our features. We choose a wrapper-
based strategy for selecting our features. Though computationally
intensive, we believe the optimization of the classifier problem to
be more valuable than filter-based methods such as variance thresh-
old or correlation coefficient, which are simpler to implement but
less model oriented. In this work, we develop a K-nearest neigh-
bour (KNN) based feature selection method to find the more salient
features for EchoLock.
In particular, we apply KNN to each type of feature to obtain
the clusters for different users. We then calculate the Euclidean
distance of each feature point to its cluster centroid and that to
centroids of other clusters. The purpose is to calculate the intra-
cluster and inter-cluster distances to measure whether a feature
is consistent for the same user and simultaneously distinct for
different users. Next, we divide the average intra-cluster distance
over the average inter-cluster distance and utilize an experimental
threshold to select the features. From our list of candidate features
described in Section 6.1, we narrow our selection to the best 6 time-
domain features, 12 frequency-domain features, and 12 acoustic
features. The selected features based on KNN are not only sensitive
to the user’ hand holding activity but also resilient to the other
factors such as acoustic noises.
6.3 Learning-based Holder Identification
We develop learning-based algorithms to learn the unique charac-
teristics of the user’s hand holding activity based on the derived
acoustic sensing features and determine whether the current de-
vice holder is the legitimate user or not. The classifiers are trained
during the user profile construction phase that is detailed in Sec-
tion 6.4. During the user verification phase, Echolock first classifies
the testing data to one user based on the user profile. For each
analyzed chirp signal, our algorithm utilizes the prediction proba-
bilities returned by the classifier as a confidence level and applies a
threshold-based method to examine the classification results. If the
confidence level of the classification is above the threshold, a user
label is predicted. A majority vote of user labels for all processed
chirp signals is conducted to determine the final identity. If the con-
fidence is beneath threshold certainty or the algorithm is unable
to achieve consensus during voting, our system will determine the
user as “unknown” and respond accordingly. This can be used for
multiple applications, such as immediately adjusting user settings
when a registered user is detected, or locking devices when an
unknown user attempts to gain access.
We explore various candidate machine learning-based classi-
fiers including Bagged Decision Trees (BDT), Linear Discriminant
Analysis (LDA), K-Nearest Neighbor (KNN), and Support Vector
1.81.922.12.2Frequency1040123456Power10-4Person 1Person 205101520Samples024681012Euclidean Distance10-3Person 1Person 2Session 14: CPS Security ASIA CCS ’20, October 5–9, 2020, Taipei, Taiwan778the signal is able to propagate through nearly the entire device,
allowing it to be shaped by the positioning of palm and fingers.
Figure 8 shows a small-scale example of two microphones’ abil-
ity to distinguish 5 people on a device using a bottom-oriented
speaker. When evaluated using samples provided from 5 people,
we observe identification accuracy of 96% and 61% for the top and
bottom microphones, respectively.
This stipulates that our speaker and microphone must be orien-
tated further apart such that they encompass as much of the device
as possible for ideal measurement. As such, we implement our sig-
nal transmission and obtain our results using data provided from
speaker and microphone pairs with the greatest amount of separa-
tion when an option to select exists. This can be achieved through