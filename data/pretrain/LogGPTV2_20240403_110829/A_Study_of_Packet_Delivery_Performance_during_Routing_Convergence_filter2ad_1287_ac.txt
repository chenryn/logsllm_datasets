4
5
6
7
RIP
DBF
BGP’
BGP
10 11 12 13 14 15 16 17
9
8
Node Degree
120
100
80
60
40
20
0
s
n
o
i
t
a
r
i
p
x
E
L
T
T
f
o
r
e
b
m
u
N
2
3
4
5
6
7
RIP
DBF
BGP’
BGP
10 11 12 13 14 15 16 17
9
8
Node Degree
Figure 3. Number of Packet Drops due to no
route Vs. node-degree
Figure 4. Number of TTL Expirations During
Convergence
tails in how the damping timer is applied. A single DBF
update message can contain as many destination entries
as the message size allows (25 destinations). Thus given
the size of simulated topology (49 nodes total), a single
update is likely to contain all the affected destinations by
the link failure. On the other hand, BGP is a path-vector
protocol and a single BGP update can only contain those
destinations that share the same path. Because our simu-
lation implemented the MRAI timer on a per neighbor ba-
sis (as in most vendor BGP implementations), after a link
failure only the ﬁrst BGP update message can propagate
quickly. Any further update messages will be regulated by
the MRAI timer, resulting in a longer time when different
nodes have inconsistent routing information. Thus BGP’
suffered more from transient loops which lead to a higher
number of TTL expirations. Note that the results could
have been different had the MRAI timer been implemented
on a per (neighbor, destination) basis.
We see that in our simulations, RIP has no loops and
DBF has fewer loops than BGP (and BGP’). Conversely,
BGP(and BGP’) has more routing information than DBF
which has more than RIP. Although BGP assures that a
node picks a path that does not contain itself, this does not
always prevent transient looping. Furthermore a common
implementation simpliﬁcation in the BGP’s MRAI sets the
time on a per neighbor rather than per (neighbor, destina-
tion) basis and lengthens BGP’s convergence time.
5.3
Instantaneous Throughput
Figure 5 shows the instantaneous throughput (at each
second) measured in packet/second at the receiver with
node degrees 3, 4, and 6, respectively. The results for node
degree 7 and higher are similar to that of node degree 6.
Note that due to TTL expirations in BGP and BGP’ at de-
gree 5(not shown), their throughput performance are worse
than those at degree 3, 4 and 6[20]. For clarity, we normal-
ize time by subtracting the 390 second warm-up period,
thus the failure is injected at t = 10 seconds in Figure 5.
Note that the results shown in the ﬁgure are the average
throughput of 100 simulation runs.
Observation 3 In a sparse network, a link failure on the
existing path between the sender and receiver tends to
cause an instant throughput drop with all the protocols
under study. For BGP, BGP’, and DBF, the throughput
then increases gradually and resumes full
throughput
around the triggered update timer values. RIP does not
resume full throughput until the 30 second periodic update
value.
In a dense network, increased node degree reduces the
throughput drop for DBF, BGP, and BGP’ to negligible
amount. RIP does only slightly better as the network
becomes more dense.
Because RIP routers do not keep alternate path infor-
mation, after the failure they must wait for other routers’
periodic announcements (or triggered update if they no-
tice path changes) to learn an alternate path. Consequently
RIP’s throughput right after the failure is almost zero. For
node degree 3, RIP’s throughput climbs back to the origi-
nal throughput at about 30 seconds later after failure, which
matches the periodical update interval. The time it takes
RIP to climb back to the original throughput decreases
slightly as the node degree increases: more neighbors mean
it’s more likely to receive a periodic announcement earlier.
With BGP, BGP’, and DBF, the throughput does not
drop to zero since it is possible for the routers to have al-
ternate paths available. For node degree 3, BGP’s grad-
ual throughput increase begins at about 25 seconds after
the failure and ends at about 35 seconds after the failure,
which match the value of the MRAI timer. Similarly, the
sharp throughput increase of BGP’ begins at about 1 sec-
ond after the failure and ends at about 5 seconds after the
failure. The results show that increased node degree can
reduce the throughput a lot with BGP, BGP’ and DBF. The
time it takes DBF to climb back to the original throughput
decreases quickly as the node degree increases: it’s about
30 seconds at degree 3, and 20 seconds at degree 4, and
almost zero seconds at degree 6.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:29:48 UTC from IEEE Xplore.  Restrictions apply. 
Throughput vs Time (degree=3)
Throughput vs Time (degree=4)
Throughput vs Time (degree=6)
30
25
20
15
10
5
0
)
d
n
o
c
e
s
/
s
t
e
k
c
a
p
(
t
u
p
h
g
u
o
r
h
T
RIP
DBF
BGP’
BGP
30
25
20
15
10
5
0
)
d
n
o
c
e
s
/
s
t
e
k
c
a
p
(
t
u
p
h
g
u
o
r
h
T
RIP
DBF
BGP’
BGP
30
25
20
15
10
5
0
)
d
n
o
c
e
s
/
s
t
e
k
c
a
p
(
t
u
p
h
g
u
o
r
h
T
RIP
DBF
BGP’
BGP
0
20
40
60
Time(seconds)
80
100
0
20
40
60
Time(seconds)
80
100
0
20
40
60
Time(seconds)
80
100
Figure 5. Instantaneous Throughput
5.4 Forwarding Path Convergence Delay
5.5
Instantaneous Packet Delay
The forwarding path convergence delay starts when the
failure is detected by a router and ends when the forwarding
path between the sender and receiver stabilizes over the ﬁ-
nal shortest path after the failure. After that, all the packets
are delivered along the converged path. Note the forward-
ing path convergence delay is different from the routing
convergence time. The forwarding path convergence de-
lay ends when each node on the path from our sender to
receiver has converged to the ﬁnal next hop, but other re-
mote nodes in the simulation may still be experiencing path
changes.
Figure 6(a) shows the average forwarding convergence
delay, and Figure 6(b) shows the average time for the rout-
ing protocol to converge. Figure 3 has shown that there
are basically no packet drops with DBF, BGP’, and BGP
in networks with node degree 6 and higher. Note however,
that Figure 6(a) shows that path convergence delay, espe-
cially those of BGP and BGP’, are above zero even at high
degrees.
Observation 4 BGP’ has a much shorter path conver-
gence delay than BGP, although the difference of packet
drops between these two versions of BGP is much less no-
ticeable.
The work in [7] shows that BGP’s MRAI timer can be
adjusted to minimize the network convergence delay, and
our results in 6(b) shows that in the topologies simulated
a smaller MRAI value reduces both the network routing
convergence delay and the forwarding convergence delay.
Furthermore, we also observed that the number of packet
drops during routing convergence is not directly propor-
tional to the forwarding convergence delay or the network
convergence delay. For example, at node degree 6, the
network convergence delay difference is about 60 seconds,
and the forwarding convergence delay difference between
BGP and BGP’ is about 11 seconds, but the packet drops
difference shown in Figure 3 is negligible. That is, with
a degree 6 or higher, tuning the MRAI value might mini-
mize the network convergence time (potentially with more
message overhead), but it does not necessarily help packet
delivery as much.
Those packets delivered during the convergence might
traverse more hops than the new best path and result in
longer end-to-end packet delay. Figure 7 shows the average
instantaneous delay of those packets delivered at time t for
networks with node degree 4, 5, and 6. The result for node
degree 3 is similar to that of degree 4, and results for node
degree 7 and higher are similar to that of node degree 6.
Observation 5 When node degree increases, DBF, BGP,
and BGP’ might experience more extra delay than the even-
tual steady delay, and those packets escaping from for-
warding loops have even longer delays.
BGP’s extra delay at degree 6 is larger than those at de-
gree 4, but study of trace ﬁles shows that these extra de-
lays are caused by those extra packets delivered due to the
higher degree. Note that the delay for degree 5 oscillates
at about t = 40 seconds. Study of the packet forward-
ing trace ﬁles shows that at about t = 40 seconds, some
packets involved in loops had escaped from the loops, and
these packets have delays much larger than those simply
traversed some sub-optimal alternate paths.
6 Conclusion
The Internet has grown in multiple dimensions. Growth
in size increases the frequency of component failures,
growth in link speed increases the potential number of
packets en-route during a routing convergence period, and
growth in topological connectivity offers increased number
of alternate paths after any component failure. However it
relies on the correct routing protocol design to best utilize
the rich redundancy in connectivity to assure continuous
packet delivery during routing convergence.
In contrast to most of previous efforts on routing proto-
col designs which focused on preventing routing loops and
minimizing convergence time, in this paper we evaluated
the impact of topological connectivity and routing protocol
design choices on packet delivery during routing conver-
gence periods. Our study shows that, although increased
network connectivity improves the packet delivery ratio in
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:29:48 UTC from IEEE Xplore.  Restrictions apply. 
i
)
s
d
n
o
c
e
s
(
e
m
T