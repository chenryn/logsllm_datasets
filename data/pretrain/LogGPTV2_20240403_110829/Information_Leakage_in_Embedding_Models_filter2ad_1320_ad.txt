-
Rec
48.05
54.45
43.73
53.18
F1
-
-
-
-
F1
52.55
57.08
45.05
59.86
embedding. We consider the original BERT [13] and the state-of-
the-art ALBERT [35]. We use mean pooling of the hidden token
representations as the sentence embedding as described in Sec-
tion 2.1.
Evaluation metrics. As the goal of inversion is to recover the
set of words in the sensitive inputs, we evaluate our inversion
methods based on precision (the percentage of recovered words
in the target inputs), recall (the percentage of words in the target
inputs are predicted) and F1 score which is the harmonic mean
between precision and recall.
White-box inversion setup. We evaluate the white-box inver-
sion with Equation 5 and Equation 7. For inversion with Equation 5,
we set the temperature T to be 0.05. For inversion with Equation 7,
1 penalty coefficient λsp to be 0.1 and the sparsity thresh-
we set the L
old τsp to be 0.01. For both methods, we use Adam optimizer [32]
for gradient descent with learning rate set to 0.001. The hyper-
parameters are tuned on a subset of the adversary’s auxiliary data.
White-box inversion results. Table 1 summarizes the results.
Note that there is no cross domain results for Equation 5 since no
learning is needed. For inversion with Equation 5, an adversary can
extract more than a half and a third of the target input from LSTM
and Transformer embedding models respectively as indicate by
the F1 score. This method performs poorly on BERT and ALBERT
models. One plausible reason is that with many more layers in BERT
where higher layer embeddings are more abstract than lower layers,
directly optimizing for the highest layer could lead to recovering
synonyms or semantically-related words rather than the targets.
For inversions with Equation 7, all performance scores increase
from Equation 5 on all models. We can recover more than half of
the input texts on nearly all models. We also notice that there is
only a little loss in performance when using cross-domain data for
training the mapping M.
We further investigate the performance of inverting embeddings
from different layers in BERT and ALBERT as shown in Figure 2.
There are in total 12 Transformer layers in BERT and ALBERT mod-
els and we choose embeddings from 2, 4, 6, 8, 10, 12 layer for inver-
sion. We also compare with inverting from layer 0, i.e. mean-pooling
e
c
n
a
m
r
o
f
r
e
p
n
o
i
s
r
e
v
n
I
e
c
n
a
m
r
o
f
r
e
p
n
o
i
s
r
e
v
n
I
100
80
60
40
20
0
0
100
80
60
40
20
0
0
BERT
Eq 5 Pre
Eq 7 Pre
Eq 5 Rec
Eq 7 Rec
Eq 5 F1
Eq 7 F1
2
4
6
8
10
12
Layer index of Φ
ALBERT
Eq 5 Pre
Eq 7 Pre
Eq 5 Rec
Eq 7 Rec
Eq 5 F1
Eq 7 F1
2
4
6
8
10
12
Layer index of Φ
Figure 2: Performance of embedding inversion on sentence
embedding from different layers of BERT and ALBERT. Pre
denotes precision and Rec denotes recall. The x-axis is the
layer index denoting which layer the embeddings are com-
puted. Index 0 is the lowest (bottom) layer and 12 is the high-
est (top) layer.
Table 2: Black-box inversion results on sentence embed-
dings. Pre denotes precision and Rec denotes recall. The best
results are in bold.
Same domain
Cross domain
LMLC
LSTM
Transformer
BERT
ALBERT
LMSP
LSTM
Transformer
BERT
ALBERT
Pre
90.53
81.18
89.70
95.92
Pre
61.69
53.59
60.21
76.77
Rec
39.35
26.07
36.80
48.71
Rec
64.40
55.72
59.31
72.05
F1
54.86
39.47
52.19
64.61
F1
63.02
54.63
59.76
74.33
Pre
87.71
77.34
84.05
92.51
Pre
59.52
51.37
55.18
74.07
Rec
32.91
21.82
30.28
44.30
Rec
62.20
52.78
55.44
70.66
F1
47.86
34.04
44.52
59.91
F1
60.83
52.07
55.31
72.32
of word embedding in BERT models. The performance drops dras-
tically when the layer goes high when inverting with Equation 5.
When training a mapping M and inverting with Equation 7, the
drop in the performance is much less significant for higher layers.
Black-box setup. We evaluate the black-box inversion with LMLC
(Equation 8) and LMSP (Equation 9). For LMLC, we train |V| binary
classifiers as ϒ for each w ∈ V. For LMSP, we train a one-layer
LSTM as ϒ with number of hidden units set to 300. We train both
models for 30 epochs with Adam optimizer and set learning rate to
0.001, batch size to 256.
Black-box results. Table 2 summarizes the results. Inversion with
LMLC can achieve high precision with low recall. This might be
due to the inversion model ϒ being biased towards the auxiliary
data and thus confident in predicting some words while not others.
Inversion with LMSP yields better balance precision and recall and
thus higher F1 scores.
6.3 Sensitive Attribute Inference
Target and auxiliary data. We consider authorship of sentence
to be the sensitive attribute and target data to be a collection of
sentences of randomly sampled author set S from the held-out
dataset of BookCorpus, with 250 sentences per author. The goal
is to classify authorship s of sentences amongst S given sentence
embeddings. For auxiliary data, we consider 10, 20, 30, 40 and 50
labeled sentences (disjoint from those in the target dataset) per
author. We also vary the size of author set |S| = 100, 200, 400 and
800 where the inference task becomes harder as |S| increases.
Baseline model. To demonstrate sensitive attribute leakage from
the embedding vector, we compare the attack performance be-
tween embedding models and a baseline model that is trained
from raw sentences without access to the embeddings. We train
a TextCNN model [31] as the baseline, which is efficient to train
and has been shown to achieve accurate authorship attribution in
previous works [59, 65].
Additional embedding models. As discussed in Section 4, the
embedding models trained with dual-encoder and contrastive learn-
ing that is a focus of this paper might, in particular, favor attribute
inference. We compare the dual-encoder embedding models with
two other embedding models trained with different objective func-
tions. The first is the Skip-thought embedding model [33] which
is trained to generate the context given a sentence. We also evalu-
ate on InferSent embeddings [10] that is trained with supervised
natural language inference tasks.
Setup. For the baseline TextCNN model, we set the number of fil-
ters in convolutional layer to 128. For all other embedding models,
we train a linear classifier for authorship inference. We train all
inference models for 30 epochs with Adam optimizer and set learn-
ing rate to 0.001 and batch size to 128. We repeat each experiment
5 times with sampled S using different random seed and report the
averaged top-5 accuracy.
Results. Figure 3 demonstrates the results of authorship infer-
ence with different number of labeled data and different number of
authors. The baseline TextCNN models trained from scratch with
limited labeled data have the worst performance across all settings.
Skip-thought and InferSent embeddings can outperform TextCNN
but the gap between the performance decreases as the number of la-
beled examples increase. The LSTM and Transformer dual-encoder
models achieve best inference results and are better than the base-
line by a significant margin in all scenarios. This demonstrates that
Authorship acc v.s. Ns
Authorship acc v.s. |S|
MIA on word embedding
70
50
30
10
y
c
a
r
u
c
c
A
5
-
p
o
T
70
50
30
TextCNN
Skip-Thought
LSTM
InferSent
10
Transformer
Word2Vec
FastText
GloVe
e
g
a
t
n
a
v
d
a
l
a
i
r
a
s