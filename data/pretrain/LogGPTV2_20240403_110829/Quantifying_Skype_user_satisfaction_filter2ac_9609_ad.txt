units. We deﬁne the factors’ relative weights as their con-
tribution to the risk score, i.e., βtZ. When computing the
contribution of a factor, the other factors are set to their
respective minimum values found in the trace. The relative
impact of each QoS factor, which is normalized by a total
score of 100, is shown in Fig. 8. On average, the degrees of
user dissatisfaction caused by the bit rate, jitter, and round-
trip time are in the proportion of 46%:53%:1%. That is,
when a user hangs up because of poor or unfavorable voice
quality, we believe that most of the negative feeling is caused
by low bit rates (46%), and high jitter (53%), but very little
is due to high round-trip times (1%).
The result indicates that increasing the bit rate whenever
appropriate would greatly enhance user satisfaction, which
is a relatively inexpensive way to improve voice quality. We
do not know how Skype adjusts the bit rate as the algorithm
is proprietary; however, our ﬁndings show that it is possible
to improve user satisfaction by ﬁne tuning the bit rate used.
Furthermore, the fact that higher round-trip times do not
impact on users very much could be rather a good news, as it
indicates that the use of relaying does not seriously degrade
user experience. Also, the fact that jitters have much more
impact on user perception suggests that the choice of relay
node should focus more on network conditions, i.e., the level
of congestion, rather than rely on network latency.
4.5 User Satisfaction Index
Based on the Cox model developed, we propose the User
Satisfaction Index (USI) to evaluate Skype users’ satisfac-
tion levels. As the risk score βtZ represents the levels of
instantaneous hang up probability, it can be seen as a mea-
sure of user intolerance. Accordingly, we deﬁne the User
Satisfaction Index of a session as its minus risk score:
U SI = −βtZ
= 2.15 × log(bit rate) − 1.55 × log(jitter)
− 0.36 × RTT,
where the bit rate, jitter, and RTTs are sampled using a
two-level sampling approach as described in Section 4.4.3.
We verify the proposed index by prediction. We ﬁrst
group sessions by their USI, and plot the actual median
duration, predicted duration, and 50% conﬁdence bands of
the latter for each group, as shown in Fig. 9. The predic-
tion is based on the median USI for each group. Note that
the y-axis is logarithmic to make the short duration groups
clearer. From the graph, we observe that the logarithmic du-
ration is approximately proportional to USI, where a higher
USI corresponds to longer call duration with a consistently
increasing trend. Also, the predicted duration is rather close
to the actual median time, and for most groups the actual
median time is within the 50% predicted conﬁdence band.
Compared with other objective measures of sound qual-
ity [9,11], which often require access to voice signals at both
ends, USI is particularly useful because its parameters are
readily accessible; it only requires the ﬁrst and second mo-
ments of the packet counting process, and the round-trip
times. The former can be obtained by simply counting
the number and bytes of arrived packets, while the latter
are usually available in peer-to-peer applications for overlay
network construction and path selection. Furthermore, as
we have developed the USI based on passive measurement,
rather than subjective surveys [11], it can also capture sub-
conscious reactions of participants, which may not be acces-
sible through surveys.
5. ANALYSIS OF USER INTERACTION
In this section, we validate the proposed User Satisfaction
Index by an independent set of metrics that quantify the
interactivity and smoothness of a conversation. A smooth
dialogue usually comprises highly interactive and tight talk
bursts. On the other hand, if sound quality is not good, or
worse, the sound is intermittent rather than continuous, the
level of interactivity will be lower because time is wasted
waiting for a response, asking for for something to be re-
peated, slowing the pace, repeating sentences, and thinking.
We believe that the degree of satisfaction with a call can be,
at least partly, inferred from the conversation pattern.
The diﬃcultly is that, most VoIP applications, including
Skype, do not support silence suppression; that is, lowering
the packet sending rate while the user is not talking. This
design is deliberate to maintain UDP port bindings at the
NAT and ensure that the background sound can be heard
all the time. Thus, we cannot tell whether a user is speaking
or silent by simply observing the packet rate. Furthermore,
to preserve privacy, Skype encrypts every voice packet with
256-bit AES (Advanced Encryption Standard) and uses 1024
bit RSA to negotiate symmetric AES keys [2]. Therefore,
parties other than call participants cannot know the content
of a conversation, even if the content of voice packets have
been revealed.
Given that user activity during a conversation is not di-
rectly available, we propose an algorithm that infers conver-
sation patterns from packet header traces. In the following,
we ﬁrst describe and validate the proposed algorithm. We
then use the voice interactivity that capture the level of
user satisfaction within a conversation to validate the User
Satisfaction Index. The results show that the USI, which
is based on call duration, and the voice interactivity mea-
sures, which are extracted from user conversation patterns,
strongly support each other.
5.1 Inferring Conversation Patterns
Through some experiments, we found that both the packet
size and bit rate could indicate user activity, i.e., whether a
user is talking or silent. The packet size is more reliable than
the bit rate, since the latter also takes account of the packet
rate, which is independent of user speech. Therefore, we
rely on the packet size process to infer conversation patterns.
However, deciding a packet size threshold for user speech is
not trivial because the packet sizes are highly variable. In
addition to voice volume, the packet size is decided by other
factors, such as the encoding rate, CPU load, and network
path characteristics, all of which could vary over time. Thus,
we cannot determine the presence of speech bursts by simple
static thresholding.
We now introduce a dynamic thresholding algorithm that
is based on wavelet denoising [6]. We use wavelet denois-
ing because packet sizes are highly variable, a factor af-
fected by transient sounds and the status of the encoder
and the network. Therefore, we need a mechanism to re-
move high-frequency variabilities in order to obtain a more
stable description of speech. However, simple low-pass ﬁl-
ters do not perform well because a speech burst could be
very short, such as “Ah,” “Uh,” and “Ya,” and short bursts
are easily diminished by averaging. Preserving such short
responses is especially important in our analysis as they in-
dicate interaction and missing them leads to underestima-
tion of interactivity. On the other hand, wavelet denoising
can localize higher frequency components in the time do-
main better; therefore, it is more suitable for our scenario,
since it preserves the correct interpretation of sub-second
speech activity.
5.1.1 Proposed Algorithm
Our method works as follows. The input is a packet size
Sender
Receiver
Internet
Relay Node
(chosen by Skype)
Figure 10: Network setup for obtaining realistic
packet size processes generated by Skype
process, called the original process, which is averaged ev-
ery 0.1 second. For most calls, packets are generated at
a frequency of approximately 33 Hz, equivalent to about
three packets per sample. We then apply the wavelet trans-
form using the index 6 wavelet in the Daubechies family [5],
which is widely used because it relatively easy to implement.
The denoising operation is performed by soft thresholding [6]
2 log N , where σ is the standard de-
with threshold T = σ
viation of the detail signal and N denotes the number of
samples. To preserve low-frequency ﬂuctuations, which rep-
resent users’ speech activity, the denoising operation only
applies to time scales smaller than 1 second.
√
In addition to ﬂuctuations caused by users’ speech ac-
tivity, the denoised process contains variations due to low-
frequency network and application dynamics. Therefore, we
use a dynamic thresholding method to determine the pres-
ence of speech bursts. We ﬁrst ﬁnd all the local extremes,
which are the local maxima or minima within a window
larger than 5 samples. If the maximum diﬀerence between a
local extreme and other samples within the same window is
greater than 15 bytes, we call it a “peak” if it is a local max-
ima, and a “trough” if it is a local minima. Once the peaks
and troughs have been identiﬁed, we compute the activity
threshold as follows. We denote each peak or trough i occur-
ring at time ti with an average packet size si as (ti, si). For
each pair of adjacent troughs (tl, sl) and (tr, sr), if there are
one or more peaks P in-between them, and the peak p ∈ P
has the largest packet size, we draw an imaginary line from
(tl, (sl + sp)/2) to (tr, (sr + sp)/2) as the binary threshold
of user activity. Finally we determine the state of each sam-
ple as ON or OFF, i.e., whether a speech burst is present,
by checking whether the averaged packet size is higher than
any of the imaginary thresholds.
5.1.2 Validation by Synthesized Wave
We ﬁrst validate the proposed algorithm with synthesized
wave ﬁles. The waves are sampled at 22.5 KHz frequency
with 8 bit levels. Each wave ﬁle lasts for 60 seconds and com-
prises alternate ON/OFF periods with exponential lengths
of mean 2 seconds. The ON periods are composed of sine
waves with the frequency uniformly distributed in the range
of 500 to 2000 Hz, where the OFF periods do not contain
sound.
To obtain realistic packet size processes that are contam-
inated by network impairment,
i.e., queueing delays and
packet loss, we conducted a series of experiments that in-
volved three Skype nodes. As shown in Fig. 10, we estab-
lish a VoIP session between two local Skype hosts and force
0
0
3
0
5
2
0
0
2
0
5
1
0
0
1
)
s
e
t
y
b
(
e
z
s
t
k
p
i
g
v
A
)
5
5
2
−
0
(
e
m
u
o
v
l
i
e
c
o
V
8
2
2
8
2
1
8
2
0
10
20
30
40
50
60
0
10
20
30
40
50
Time (sec)
(a) Original packet size process
Time (sec)
(a) Human speech recording
)
s
e
t
y
b
(
i
e
z
s
t
k
p
g
v
A
0
5
2
0
0
2
0
5
1
0
4
1
0
2
1
0
0
1
)
s
e
t
y
b
(
i
e
z
s
t
k
p
g
v
A
0
10
20
30
40
50
60
0
10
20
30
40
50
Time (sec)
Time (sec)
(b) Wavelet denoised process with estimated ON periods
(b) Wavelet denoised process with estimated ON periods
Figure 11: Verifying the speech detection algorithm
with synthesized ON/OFF sine waves
Figure 12: Verifying the speech detection algorithm
with human speech recordings
them to connect via a relay node by blocking their inter-
communication with a ﬁrewall. The selection of relay node
is out of our control because it is chosen by Skype. The re-
lay node used is far from our Skype hosts with transoceanic
links in-between and an average RTT of approximately 350
ms, which is much longer than the average 256 ms. Also,
the jitter our Skype calls experienced is 5.1 Kbps, which is
approximately the 95 percentile in the collected sessions. In
the experiment, we play synthesized wave ﬁles to the input
of Skype on the sender, and take the packet size processes on
the receiver. Because the network characteristics in our ex-
periment are much worse than the average case in collected
sessions, we believe the result of the speech detection here
would be close to the worst case, as the measured packet size
processes contain so much variation and unpredictability.
To demonstrate, the result of a test run is shown in Fig. 11.
The upper graph depicts the original packet size process
with the red line indicating true ON periods and blue checks
indicating estimated ON periods. The lower graph plots the
wavelet denoised version of the original process, with red and
blue circles marking the location of peaks and troughs, re-
spectively. The oblique lines formed by black crosses are bi-
nary thresholds used to determine speech activity. As shown
by the ﬁgure, wavelet denoising does a good job in removing
high-frequency variations that could mislead threshold de-
cisions, but retains variations due to user speech. Note that
long-term variation is present because the average packet
size used in the second half of the run is signiﬁcantly smaller
than that in the ﬁrst half. This illustrates the need for dy-
namic thresholding, as the packet size could be aﬀected by
many factors other than user speech.
Totally 10 test cases are generated, each of which is run
3 times. Since we have the original waves, the correctness
of the speech detection algorithm can be veriﬁed. Each test
is divided into 0.1-second periods, the same as the sampling
interval, for correctness checks. Two metrics are deﬁned to
judge the accuracy of estimation: 1) Correctness: the ratio
of matched periods, i.e., the periods whose states, either ON
or OFF, are correctly estimated; and 2) the number of ON
periods. As encoding, packetization, and transmission of
voice data necessarily introduce some delay, the correctness
is computed with time oﬀsets ranging from minus one to plus
one second, and the maximum ratio of the matched periods
is used. The experiment results show that the correctness
ranges from 0.73–0.92 with a mean of 0.8 and standard devi-
ation of 0.05. The estimated number of ON periods is always
close to the the actual number of ON periods; the diﬀer-
ence is generally less than 20% of the latter. Although not
perfectly exact, the validation experiment shows that the
proposed algorithm estimates ON/OFF periods with good
accuracy, even if the packet size processes have been con-
taminated by network dynamics.
5.1.3 Validation by Speech Recording
Since synthesized ON/OFF sine waves may be very diﬀer-
ent from human speech, we further experiment with human
speech recordings. We repeat the preceding experiment by
replacing synthesized wave ﬁles with human speech recorded
via microphone during phone calls. Given that the sampled
voice levels range from 0 to 255, with the center at 128, we
use an oﬀset of +/−4 to indicate whether a speciﬁc volume
corresponds to audible sounds. Among a total of 9 runs for
three test cases, the correctness ranges from 0.71 to 0.85.
The number of ON periods diﬀers from true values by less
than 32%. The accuracy of detection is slightly worse than
the experiment with synthesized waves, but still correctly
captures most speech activity. Fig. 12 illustrates the result
of one test, which shows that the algorithm detects speech
bursts reasonably well, except for the short spikes around
47 seconds.
5.2 User Satisfaction Analysis
To determine the degree of user satisfaction from conver-
sation patterns, we propose the following three voice interac-
tivity measures to capture the interactivity and smoothness
of a given conversation:
Responsiveness: A smooth conversation usually involves
alternate statements and responses. We measure the
interactivity by the degree of “alternation,” that is,