ﬁve corresponding to “zombie” faults in each of the compo-
nents. A component that becomes a “zombie” responds to
pings sent by component monitors, but does not correctly
perform its functions. The path monitors detect such faults,
but are unable to precisely pinpoint the component that has
failed. However, due to the path diversity in the paths taken
by the two monitors, their outputs can still help in the prob-
abilistic determination of which component might be faulty.
The recovery actions available to the recovery controller
are to restart a component, reboot a host, or just passively
observe the system (through the monitors). Recovery ac-
tions are assumed to be deterministic in that the correct re-
covery action for a fault always ﬁxes it. Each recovery and
monitoring action has a duration; we chose 5 minutes for a
host reboot, 4 minutes for a database restart, 2 minutes for a
voice gateway restart, 1 minute each for an HTTP or EMN
server restart, and 5 seconds for an execution of the mon-
itors. During recovery, costs accrue at a rate equal to the
fraction of requests being dropped by the system either due
to a failure, or because a recovery action (e.g., rebooting)
has made a component unavailable. 80% of the requests are
assumed to be HTTP requests, while 20% are assumed to be
voice requests. Finally, the system lacks recovery notiﬁca-
tion since an “all clear” by the monitors might just mean that
an EMN server has become a ‘zombie, but the path moni-
tor requests were routed around it. Termination costs are
speciﬁed using the technique described in Section 3 with a
mean human response time of 6 hours. Overall, the model
is small, but is enough to model a realistic system.
All experiments were conducted on 2GHz Athlon ma-
chines with 512MB of memory. Because they are difﬁcult
to diagnose, only zombie faults were injected in the simu-
lations. The ﬁrst set of results shows the convergence be-
havior of the lower bound during the bootstrapping phase.
Two variants of the bootstrapping procedure were run. The
“Random” variant corresponds to the case where faults were
randomly selected with a uniform distribution, observations
corresponding to the faults were randomly chosen (accord-
ing to the monitor coverage probabilities), and the controller
was invoked with a belief-state corresponding to the gener-
ated observations. On the other hand, the “Average” results
correspond to the situation in which the controller was in-
voked using a belief-state in which all faults were equally
likely.
Figure 5(a) shows the improvement of the lower bounds
as a function of the number of iterations of the bootstrap
procedure with tree depth set to one. The values on the
y-axis are the negative values of the POMDP lower-bound
function (or upper bounds of the cost function) evaluated
at the belief-state {1/|S|}. This belief-state corresponds to
the case where all faults are equally likely. The graph con-
ﬁrms our argument that the lower bounds do improve due to
iterative updates. Moreover, it shows that the tightening is
rapid in the ﬁrst few iterations and then slows down. Note
that for a general POMDP, it is not possible to determine
whether the lower bound is within a certain distance  from
Iterative Bounds Improvement
Numbe r of Bounds Vectors
6000
5000
4000
3000
2000
1000
0
t
s
o
C
n
o
d
n
u
o
B
r
e
p
p
U
Random
Average
Random
Average
s
r
o
t
c
e
V
f
o
r
e
b
m
u
N
18
16
14
12
10
8
6
4
2
0
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20
Iteration
Iteration
(a) Iterative Bounds Improvements (Mean Bound)
(b) Using bounds as a measure of performance
Figure 5. Iterative Lower Bounds Improvement
the optimal for any  (due to the undecidability of the ques-
tion of the existence of approximate bounds). Nevertheless,
one can use upper bounds (if they are available) to bound
the distance of the bound from the optimal solution (with-
out any guarantee of reducing that distance). In the graph,
the x-axis is a trivial upper bound for the reward.
Graph 5(b) shows how the number of bounds vectors
(hyperplanes) in the lower bound increases with the number
of iterative update steps. During any update, since we add
at most one new bounds vector, the growth is guaranteed to
be linear at worst. For the example model, bounds reﬁne-
ment took only a few milliseconds, and limiting the number
of bounds was not needed. But, there are no guarantees
on whether the number of bound vectors will stabilize at
some point or increase without bound. Therefore, in prac-
tice, one would provide ﬁnite storage for the lower bound.
That, plus the fact (illustrated in Figure 5(a)) that the bounds
improve rapidly at ﬁrst and then become relatively stable,
implies that such a ﬁnite storage approach should work well
in practice. For the presented system, the average bootstrap-
ping procedure not only achieves faster improvement and a
tighter bound, but also increases the number of bound vec-
tors more slowly than the random bound does. However,
there is no guarantee that this same method will work best
for all models.
The second set of experiments evaluates the quality of
decisions made by the controller by injecting 10000 faults
into the system and measuring per-fault metrics. The per-
fault recovery metrics obtained for a bounded controller
with a recursion depth of 1, and bootstrapped with 10 runs
of depth 2, are compared to metrics for three other types
of controllers. The “most likely” controller is a controller
that performs probabilistic diagnosis on the system using
the Bayes rule, and chooses the cheapest recovery action
that recovers from the most likely fault. The “heuristic”
controllers are POMDP controllers with varying tree depth,
but they use a heuristic approximation of the value func-
tion at the leaves of the ﬁnite-depth expansion. According
to the heuristic, the value of a belief-state is approximated
as (1 − P [sφ]) maxa∈A,s∈S r(s, a) (i.e., the product of the
probability that the system hasn’t recovered with the cost of
the most expensive recovery action available to the system).
In [8], this heuristic was experimentally chosen as the best
performing heuristic amongst a few that were tried for the
EMN system conﬁguration. Finally, the Oracle controller
is a hypothetical controller that knows the fault in the sys-
tem, and can always recover from it via a single action. It
represents the unattainable ideal.
Table 1 shows the per-fault metrics obtained as a result of
the fault injections. In the table, cost is the reward metric de-
ﬁned on the recovery model, and is a measure of the number
of requests dropped by the system. The residual time indi-
cates the amount of wall-clock time that a fault was present
in the system. Recovery time is the time taken to terminate
recovery, while algorithm time is the time spent by the con-
troller deciding what action to choose. Finally, the actions
and monitor calls columns represent the number of recov-
ery actions invoked and number of monitor calls made by
the controller per fault.
As the table shows, the bounded controller outperforms
both the “most likely” and heuristic depth one controllers
by fairly signiﬁcant cost margins. Even though the heuristic
controllers with depths two and three manage to do quite
well in this example, and even though the bounded con-
troller’s own depth is only one, it manages to outperform
those other two controllers as well. Moreover, because of
its low depth, it is able generate its decisions in less time
than the nearest comparable heuristic controller (which re-
quires a lookahead of 2 to achieve its performance).
Another aspect where the bounded controller shines is
regarding termination of recovery. The termination condi-
tion for both the “most likely” and heuristic controllers is set
by specifying the probability with which the system must
be in the recovered state before the controller can termi-
nate recovery. Determining what value such a termination
probability should take is difﬁcult. Since we ran 10,000
Algorithm
Depth
Most Likely
Heuristic
Heuristic
Heuristic
Bounded
Oracle
-
1
2
3
1
-
Cost
Recovery
Time (sec)
394.73
244.40
151.04
299.72
118.481 269.96
118.846 271.32
192.30
114.16
84.4
132.00
Residual
Time (sec)
212.98
193.24
169.34
169.86
165.24
132.00
Algorithm
Time (msec)
0.09
6.71
123.59
1485
92
-
Actions Monitor
3.00
1.71
1.216
1.216
1.20
1.00
Calls
3.00
17.42
22.51
22.50
7.69
0.00
Table 1. Fault Injection Results (Values are Per-fault Averages)
experiments, we set the value to 0.9999. The bounded con-
troller does not require such a termination probability since
its termination conditions are set using notions of operator
response time. The consequences can be seen in Table 1.
The recovery time of all the controllers that require a ter-
mination probability are disproportionately higher than the
corresponding residual times. Even after recovery is com-
plete, they spend a large amount of time just monitoring the
system (evidenced by the number of monitoring calls they
make) in an attempt to raise the probability of successful
recovery. On the other hand, the bounded controller can de-
termine when recovery is complete much sooner. It is worth
noting that in the 10,000 fault injections, none of the con-
trollers ever quit without recovering the system.
6 Conclusion
In this paper, we examined the problem of performing
system recovery even when system monitoring informa-
tion is imperfect or imprecise by casting the problem as an
undiscounted mean accumulated reward optimization prob-
lem in the POMDP framework. Although solving undis-
counted POMDPs is difﬁcult, we showed how to utilize
speciﬁc properties of system recovery to formulate lower
bounds on the solution of recovery POMDPs. Recovery
controllers built using these bounds were shown to possess
properties including ﬁnite termination time and to guaran-
tee a certain level of performance. Finally, experimental
results on a sample e-commerce system showed that the
bounds can be improved iteratively, and the resulting con-
troller provides performance superior to that of a controller
based on heuristics. Several ways to extend the approach in
the paper are possible, and include providing of guarantees
against early termination of the recovery process, formally
investigating iterative improvement in recovery models, and
generation of upper bounds in addition to the lower bounds
to facilitate branch and bound techniques.
Acknowledgment This material is based on work supported
in part by the National Science Foundation under Grant No. CNS-
0406351. Kaustubh R. Joshi is supported by an AT&T Virtual Uni-
versity Research Initiative (VURI) fellowship. The authors would
also like to thank Jenny Applequist for helping improve the read-
ability of the material.
[4] M. Fischer, N. Lynch, and M. Paterson.
distributed consensus with one faulty process.
32(2):374–382, Apr. 1985.
Impossibility of
J. ACM,
References
[1] M. Abdeen and M. Woodside. Seeking optimal policies for
adaptive distributed computer systems with multiple con-
trols.
In Proc. of Third Intl. Conf. on Parallel and Dist.
Comp., Appl. and Technologies, Kanazawa, Japan, 2002.
[2] A. R. Cassandra, L. P. Kaelbling, and M. L. Littman. Act-
ing optimally in partially observable stochastic domains. In
Proc. of the 12th Natl. Conf. on Artiﬁcial Intelligence (AAAI-
94), volume 2, pages 1023–1028, Seattle, 1994.
[3] H. de Meer and K. S. Trivedi. Guarded repair of dependable
systems. Theoretical Computer Sci., 128:179–210, 1994.
[5] L. J. Franken and B. R. Haverkort. Reconﬁguring distributed
systems using Markov-decision models. In Proceedings of
the Workshop on Trends in Distributed Systems (TreDS’96),
pages 219–228, Aachen, Oct. 1996.
[6] M. Hauskrecht. Incremental methods for computing bounds
in partially observable Markov decision processes. In Proc.
of AAAI, pages 734–739, Providence, RI, 1997.
[7] M. Hauskrecht. Value-function approximations for partially
observable Markov decision processes. Journal of Artiﬁcial
Intell. Research, 13:33–94, 2000.
[8] K. R. Joshi, M. Hiltunen, W. H. Sanders, and R. Schlichting.
Automatic model-driven recovery in distributed systems. In
Proc. of Symp. on Reliable Dist. Systems (SRDS 05), pages
25–36, Oct 2005.
[9] O. Madani, S. Hanks, and A. Condon. On the undecidability
of probabilistic planning and related stochastic optimization
problems. Artif. Intell., 147(1-2):5–34, 2003.
[10] G. E. Monahan. A survey of partially observable Markov
decision processes: Theory, models, and algorithms. Man-
agement Science, 28(1):1–16, 1982.
[11] M. L. Puterman. Markov Decision Processes: Discrete
Stochastic Dynamic Programming. Wiley-Intersci., 1994.
[12] K. G. Shin, C. M. Krishna, and Y.-H. Lee. Optimal dynamic
control of resources in a distributed system. IEEE Trans. on
Software Eng., 15(10):1188–1198, Oct. 1989.
[13] E. J. Sondik. The Optimal Control of Partially Observable
[14] R. Washington.
Markov Processes. PhD thesis, Stanford University, 1971.
incremental,
BI-POMDP: Bounded,
partially-observable Markov-model planning.
In S. Steel
and R. Alami, editors, Proc. of European Conf. on Plan-
ning, volume 1348 of Lecture Notes in Computer Sci., pages
440–451. Springer, 1997.