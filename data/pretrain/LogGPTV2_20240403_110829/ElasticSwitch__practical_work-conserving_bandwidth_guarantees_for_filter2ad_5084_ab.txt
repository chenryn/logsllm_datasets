Distributed
Distributed
MPLS
None
ECN
None
per VM Queues
None
Topology Requirements
None
None
Congestion-Free Core
None
Tree
None
VMs
c
i
f
f
a
r
T
Hypervisor
Guarantee(cid:3)
Partitioning(cid:3)(GP)
VM(cid:882)toVM
Guarantees
Demand(cid:3)
estimates
Rate(cid:882)Allocation(cid:3)(RA)
Provides(cid:3)minimum(cid:3)bandwidth(cid:3)
guarantees(cid:3)by(cid:3)carefully(cid:3)dividing(cid:3)
hose(cid:3)model(cid:3)guarantees into(cid:3)VM(cid:882)
to(cid:882)VM(cid:3)guarantees
Provides(cid:3)work(cid:3)conservation(cid:3)by(cid:3)
allocating(cid:3)spare(cid:3)capacity(cid:3)in(cid:3)max(cid:882)
min(cid:3)fashion(cid:3)based(cid:3)on(cid:3)VM(cid:882)to(cid:882)
VM(cid:3)guarantees
Figure 3: Overview of ElasticSwitch
datacenter trafﬁc is bursty [6, 24], work-conservation can give an
order of magnitude more bandwidth to a VM.
Thus, given a choice, customers would prefer providers offer-
ing work-conserving bandwidth guarantees, compared to providers
offering only one of these two properties.
Scalability: Large public cloud providers host several thousands
of tenants, each with tens to thousands of VMs. Further, trafﬁc
demands change rapidly and new ﬂows arrive frequently, e.g., a
datacenter can experience over 10 million ﬂows per second [6].
Given the high frequencies at which demands change and ﬂows
arrive, the provider must also adjust rate limits at a high frequency.
Using a centralized controller to adjust these limits would scale
poorly, since each VM can compete for bandwidth over an arbi-
trary set of congested links, and the controller would have to co-
ordinate across all VMs and all links in the network. This would
entail tremendous computation costs, and communication costs for
control trafﬁc. For example, Tavakoli et al. [25] estimate that a
network of 100K servers needs 667 NOX controllers just for han-
dling the ﬂow setups, which is a signiﬁcantly simpler problem. Fur-
thermore, making the controller sufﬁciently available and reliable
would be extremely expensive.
Thus, our goal is to design a distributed bandwidth guarantee
enforcement mechanism that can scale to large cloud datacenters.
Deployability: We aim for our solution to be deployable in any
current or future datacenter. Towards this end, we design our solu-
tion with three requirements. First, we want our solution to work
with commodity switches, and to not assume any non-standard fea-
tures. This reduces the equipment costs for cloud providers, who
are quite cost-sensitive.
Second, we do not want our solution to depend on the network
topology. For example, it should work on topologies with differ-
ent over-subscription factors. Most existing networks are over-
subscribed. While oversubscription factors have decreased over
the last decade (from 100:1 to 10:1), oversubscription has not
disappeared. All of the datacenters studied in [6] were oversub-
scribed; commercial datacenters had an oversubscription ratio of
20:1. Given that average network utilization is low, cost concerns
may encourage oversubscription for the foreseeable future. We are
aware of tens of cloud datacenters under construction that are sig-
niﬁcantly oversubscribed.
Third, to be applicable in a wide variety of cloud datacenters, our
solution should be agnostic to applications and workloads.
Recent work: Recently, researchers have proposed different so-
lutions for sharing cloud datacenter networks, but none of them
simultaneously meets all of the above goals. Table 1 summarizes
these solutions; we discuss them in detail in §8.
3. ElasticSwitch OVERVIEW
In this paper, we focus on a single data center implementing the
Infrastructure as a Service (IaaS) cloud computing model. For sim-
plicity of exposition, we discuss ElasticSwitch in the context of a
tree-based physical network, such as a traditional data center net-
work architecture or the more modern multi-path architectures like
fat-tree [1] or VL2 [10]. For multi-path architectures, we assume
the trafﬁc is load balanced across the multiple paths by an orthogo-
nal solution, e.g., [2,15,19]. (ElasticSwitch can be applied to other
topologies, see §9).
ElasticSwitch decouples providing bandwidth guarantees from
achieving work conservation. Thus, ElasticSwitch consists of two
layers, both running inside each hypervisor as shown in Fig. 3. The
ﬁrst layer, guarantee partitioning (GP), enforces bandwidth guar-
antees, while the second layer, rate allocation (RA), achieves work
conservation.
The GP layer provides hose model guarantees by transforming
them into a set of absolute minimum bandwidth guarantees for each
source-destination pair of VMs. More speciﬁcally, the GP compo-
nent for a VM X divides X’s hose model guarantee into guarantees
to/from each other VM that X communicates with. The guarantee
between source VM X and destination VM Y , BX→Y , is com-
puted as the minimum between the guarantees assigned by X and
Y to the X → Y trafﬁc.
The GP layer feeds the computed minimum bandwidth guaran-
tees to the RA layer. Between every pair of VMs X and Y , the
RA layer on the host of VM X uses a rate-limiter to limit the
trafﬁc. The rate-limit assigned by RA does not drop below the
guarantee BX→Y , but can be higher. Speciﬁcally, RA aims to
grab available bandwidth in addition to the provided guarantee,
when the path from X to Y is not congested. RA shares the ad-
ditional bandwidth available on a link L in proportion to the band-
width guarantees of the source-destination VM pairs communicat-
ing on L. For example, assume BX→Y = 200Mbps and BZ→T =
100Mbps (X, Y , Z and T are VMs). In this case, sharing a link
L larger than 300Mbps only between X→Y and Z→T is done in
a 2:1 ratio; e.g., if L is 1Gbps, X→Y should get 666Mbps and
Z→T 333Mbps. For this purpose, rate allocation uses a mecha-
nism similar to a weighted version of a congestion control algo-
353Z
BX(cid:198)Y(cid:3)=(cid:3)min(BX(cid:3)(cid:3)(cid:3)(cid:3)(cid:3),(cid:3)BY
X(cid:198)Y
X(cid:198)Y
X(cid:198)Z
BX
X
X(cid:198)Y
BX
T
)
X(cid:198)Y
BY
Q
T(cid:198)Y
BY
Y
Q(cid:198)Y
BY
Figure 4: Example of guarantee partitioning
rithm, such as TCP, where each VM-to-VM communication uses a
single weighted ﬂow (TCP ﬂows have equal weights). RA is sim-
ilar to Seawall [22] and the large body of work on weighted TCP,
e.g., MulTCP [8], TCP-LASD [16].
Each hypervisor executes GP and RA periodically for each
hosted VM. GP is performed less frequently than RA, since the
guarantees allocated for the VM-to-VM ﬂows of a VM X should
be updated only when demands change to/from X, while rates allo-
cated for those VM-to-VM ﬂows should be updated when demands
change on any congested link used by X.
VM placement: ElasticSwitch is orthogonal to the VM place-
ment strategy, as long as the admission control criterion is satisﬁed,
i.e., the sum of the bandwidth guarantees traversing any link L is
smaller than L’s capacity.
External trafﬁc: Trafﬁc to/from hosts located outside the cloud
must also be considered when providing bandwidth guarantees in-
side the cloud, since external and internal trafﬁc typically share the
same datacenter network links. For brevity, we describe only one
way to model the external trafﬁc. In this model, all external hosts
appear as a single node attached to the virtual switch in the hose
model; the guarantee for the aggregate external trafﬁc only applies
up to the egress from the datacenter. For this purpose, the external
trafﬁc must be routed through proxies executing ElasticSwitch.
4. GUARANTEE PARTITIONING (GP)
The distributed GP layer partitions the hose-model guarantee of a
VM X into VM-to-VM guarantees. GP aims to achieve two goals:
(1) safety, meaning that the hose-model guarantees of other VMs in
the network cannot be violated by the assigned VM-to-VM guaran-
tees and (2) efﬁciency, i.e., VM X’s throughput is not limited below
what X would obtain if it were communicating through a physical
network with capacity equivalent to X’s hose-model guarantee.
We partition guarantees because implementing the hose model
requires rate enforcement at the granularity of VM-to-VM pairs;
per-VM limiters do not sufﬁce. For example, depending on the
communication pattern and demands, some of the VM-to-VM
ﬂows outgoing from VM X can be bottlenecked in the hose model
at the source X, while other VM-to-VM ﬂows from X can be bot-
tlenecked in the hose model at the destination; this situation re-
quires VM-to-VM limiters to emulate the hose model.
= Qr
For a VM X, suppose Qs
X). For any VM Y in Qr
X denotes the set of VMs that are send-
ing trafﬁc to X and Qr
X those receiving from X (almost always
X, X’s hypervisor assigns a
Qs
bandwidth guarantee BX→Y
X
for the communication to Y . Inde-
pendently, for each VM Y ∈ Qs
X’s hypervisor assigns guar-
antee BY →X
for the trafﬁc from Y . The total of such guaran-
tees sums up to X’s hose-model guarantee, BX, in each direction,
BX→Y
= BX. (Note that it is
i.e.,
straightforward to adapt ElasticSwitch to use an asymmetric hose
model, with different incoming and outgoing guarantees.)
BY →X
Y ∈Qr
Y ∈Qs
(cid:2)
(cid:2)
=
X
X
X
X
X
X
X
As we illustrate in Fig. 4, the key to ElasticSwitch’s ability to
provide hose model guarantees is to set the pairwise guarantee be-
tween source X and destination Y to the minimum of the guaran-
tees allocated by X and Y :
BX→Y = min(BX→Y
X
, BX→Y
Y
)
V
X
X
X
X
X
V ∈Qs
V ∈Qs
= BX.
, BV →X
min(BV →X
) ≤ (cid:2)
(cid:2)
BV →X
This allocation ensures the safety property for GP because, on
any link L, the sum of the bandwidth guarantees for the VM-to-VM
ﬂows does not exceed the link bandwidth allocated by admission
control for the hose model on link L. This is easy to see, since X’s
guarantee is divided between its ﬂows, and each VM-to-VM ﬂow
gets the minimum of the guarantees allocated by source and desti-
BV →X =
(cid:2)
nation.1 For instance, for X’s incoming trafﬁc,
V ∈Qs
Hence, the guarantees assigned by GP do not oversubscribe the
reserved guarantees on any link, and, if we were to rate-limit all the
VM-to-VM ﬂows of VM X to these values, the guarantees of any
other VM in the network are not affected by the trafﬁc to/from X.
For example, to implement the Blue tenant’s hose model from
Fig. 1 in the physical topology (shown in the same ﬁgure), the Blue
tenant should be given a minimum guarantee for its incoming/out-
going bandwidth on link L3. Assuming each server hosts at most
two VMs, the Blue tenant competes on L3 with, at most, the band-
width of the ﬂows communicating with VM Q. However, the total
bandwidth guarantee of the VM-to-VM ﬂows of the Red tenant on
link L3 will always be less or equal to BQ.2 Thus, by ensuring that
BA+BQ is less than the capacity of L3, GP ensures that no tenant
can violate the hose models of other tenants.
The process of assigning guarantees for VM-to-VM ﬂows is ap-
plied for each VM, regardless of whether that VM is a source
or destination. However, to compute the guarantee BX→Y be-
tween VMs X and Y ,
the hypervisor of the source VM X
must know the guarantee allocated by Y ’s hypervisor to X→Y ,
i.e., BX→Y
. ElasticSwitch does this with special remote-guarantee
control packets sent by destination hypervisors to source hypervi-
sors.
Initially, each ﬂow is given the guarantee allocated by the
source hypervisor.
One remaining question is how the hypervisor of VM X divides
X’s guarantee between its VM-to-VM ﬂows. The naive solution
is to simply divide X’s guarantee equally between its VM-to-VM
ﬂows. This approach works well when all VM guarantees in the
hose model are equal, and all the trafﬁc demands are either unsat-
isﬁed or equal. However, this is typically not the case in practice.
For example, many of the ﬂows are short (send a few packets) or
have inherently limited demands.
We ﬁrst describe GP in a static setting, where VM-to-VM ﬂows
have constant trafﬁc demands and new ﬂows are not generated, and
then describe GP in a dynamic environment.
GP in a static context: In ElasticSwitch, we divide a VM’s guar-
antee between its VM-to-VM ﬂows in a max-min fashion based on
trafﬁc demands. For example, assume BX=100Mbps for a VM X.
Also assume that X communicates with three other VMs Y , Z and
T , and the demand to Y is only 20Mbps, while demands to Z and
T are very large (unbounded). In this case, guarantee partitioning
assigns a guarantee of 20Mbps to the X→Y path (i.e., BX→Y
=
20Mbps), and a guarantee of 40Mbps to each of X→Z and X→T .
GP in a dynamic context: In practice, actual trafﬁc demands are
unknown and vary in time. ElasticSwitch estimates demands of
1We often refer to a guarantee as assigned by VM X, although
guarantees are always assigned by the hypervisor hosting X.
2We assume here that BQ < BX +BY +BZ +BP , since the hose-
model reservation on link L3 is min(BQ, BX + BY + BZ + BP ).
X
Y
354VM-to-VM ﬂows by measuring their throughputs between epochs
(information in fact provided by the RA layer). If the demand es-
timate is smaller (by a signiﬁcant error margin) than the allocated
guarantee, the VM-to-VM ﬂow is considered bounded and its guar-
antee can be reduced. Otherwise, the ﬂow was not satisﬁed and its
guarantee should be increased, if possible. We describe this process
from the perspective of a new ﬂow.
When a new VM-to-VM ﬂow starts, its demand is unknown. To
optimize for the bi-modal distribution of ﬂows typically observed in
datacenters, in which most ﬂows are very small and a few are quite
large [6, 24], we set its demand estimate (and guarantee) at a small
value. If, at the next periodic application of GP, we detect that the
ﬂow did not utilize its guarantee, then we deem the ﬂow as bounded