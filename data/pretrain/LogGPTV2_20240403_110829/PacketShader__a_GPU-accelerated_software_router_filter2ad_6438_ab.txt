While the preliminary result of GPU is promising, one natural
question arises here: does collecting hundreds or thousands of pack-
ets incur unreasonable latency? Our answer is “no”; a large number
of packets arrive in a fairly small time window on today’s high speed
links. For example, a thousand 64B packets arrive in only 70 µs on
a full-speed 10 GbE link.
2.4 Comparison with CPU
The fundamental difference between CPU and GPU comes from
how transistors are composed in the processor. CPUs maximize
instruction-level parallelism to accelerate a small number of threads.
Most of CPU resources serve large caches and sophisticated control
planes for advanced features (e.g., superscalar, out-of-order execu-
tion, branch prediction, or speculative loads).
In contrast, GPUs
maximize thread-level parallelism, devoting most of their die area to
a large array of Arithmetic Logic Units (ALUs). GPUs also provide
ample memory bandwidth to feed data to a large number of cores.
We brieﬂy address the implication of those differences in the context
of packet processing.
Memory access latency: For most network applications, memory
working set is too big to ﬁt in a CPU cache. While the memory
access latency can be potentially hidden with out-of-order execution
and overlapped memory references, the latency hiding is limited by
CPU resources such as the instruction window size, the number of
Miss Status Holding Registers (MSHRs), and memory bandwidth.
In our microbenchmark, an X5550 cores can handle about 6 out-
standing cache misses in the optimal case, and only 4 misses when
all four cores bursts memory references. Unlike CPU, GPU effec-
tively hides memory access latency with hundreds (or thousands) of
threads. By having an enough number of threads, memory stalls can
be minimized or even eliminated [43].
Memory bandwidth: Network applications tend to exhibit random
memory access, such as hash table lookup, quickly exhausting avail-
able memory bandwidth. For example, every 4B random memory
access consumes 64B of memory bandwidth, which is the size of a
cache line in x86 architecture. By ofﬂoading memory-intensive op-
erations to GPU, such as IPv6 longest preﬁx matching, we can bene-
ﬁt from larger memory bandwidth of GPU (177.4 GB/s for GTX480
versus 32 GB/s for X5550). This additional memory bandwidth of
GPU is particularly helpful when a large portion of CPU’s memory
bandwidth is consumed by packet I/O.
Figure 3: Block diagram of our server
Raw computation capacity: Applications running on software routers
are increasingly demanding for compute-intensive operations, such
as hashing, encryption, compression, and pattern matching, to name
a few. As the bottleneck of software routers lies in CPU, introducing
those operations would signiﬁcantly degrade the performance. GPU
can be an attractive source of extra computation power; NVIDIA
GTX480 offers an order of magnitude higher peak performance than
X5550 in terms of MIPS (Million Instructions Per Second). More-
over, the recent trend shows that the GPU computing density is im-
proving faster than CPU [42].
3. SYSTEM SETUP
This section describes the hardware and software conﬁguration
that we use in this paper. We also mention the dual-IOH problem
that bounds I/O bandwidth, limiting the performance of our current
system.
3.1 Hardware Conﬁguration
We set up our hardware to perform high-speed packet process-
ing with GPUs while reﬂecting today’s commodity hardware trend.
Table 2 summarizes the speciﬁcations of our server. We use two
Intel Nehalem quad-core Xeon X5550 2.66 GHz processors with
1,333 MHz 12 GB DDR3 memory. For GPU acceleration, we use
two NVIDIA GTX480 cards, each of which has 480 stream proces-
sor cores and 1.5 GB GDDR5 memory. Our choice of GTX480 is the
top-of-the-line graphics card at the moment. For network cards, we
use four dual-port Intel 82599 X520-DA2 10GbE NICs: eight ports
with aggregate capacity of 80 Gbps. The total system (including all
other components) costs about $7,000.3
Figure 3 shows the components and their interconnection in the
server. There are two NUMA (Non-Uniform Memory Access) nodes
in the server, and each node has a quad-core CPU socket and local
memory. The memory controller integrated in a CPU connects three
2 GB DDR3 DRAMs in the triple-channel mode. Each node has an
IOH (I/O Hub) that connects peripheral devices to the CPU socket.
Each IOH holds three PCIe devices: two dual-port 10GbE NICs on
PCIe x8 links and one NVIDIA GTX480 graphics card on a PCIe
x16 link.
3All prices are from http://checkout.google.com on June 2010
197Functional bins
skb initialization
skb (de)allocation
Memory subsystem
NIC device driver
Others
Compulsory cache misses
Total
Cycles Our solution
Huge packet buffer (§4.2)
4.9% Compact metadata (§4.2)
8.0%
50.2%
13.3%
9.8%
13.8% Software prefetch (§4.3)
100.0% -
Batch processing (§4.3)
Table 3: CPU cycle breakdown in packet RX
3.2 Dual-IOH Problem
Our system uses four NICs and two GPUs which require 64 PCIe
lanes in total. Since an Intel 5520 IOH chipset provides only 36 PCIe
lanes, we adopt a motherboard with two IOH chipsets.
However, we have found that there is asymmetry of PCIe data
transfer performance with the dual-IOH motherboard. We see much
lower empirical bandwidth of device-to-host transfer than that of
host-to-device transfer. Table 1 in the previous section shows that
data copy from a GPU to host memory is slower than the opposite
direction. Similarly for NICs, we see higher TX throughput than RX
as shown in Figure 6 in Section 4.6.
The throughput asymmetry is speciﬁc to motherboards with dual
5520 chipsets. Motherboards from other vendors with two IOHs
have the same problem [7], and we conﬁrm that motherboards with
a single IOH do not show any throughput asymmetry [23]. We are
investigating the exact cause of the problem and planning to check if
motherboards based on AMD chipsets have the same problem.
The dual-IOH problem limits the packet I/O performance in our
system, and the limited I/O throughput bounds the maximum perfor-
mance of our applications in Section 6.
3.3 Software Conﬁguration
We have installed 64-bit Ubuntu Linux 9.04 server distribution
with unmodiﬁed Linux kernel 2.6.28.10 in the server. Our packet
I/O engine (Section 4) is based on ixgbe 2.0.38.2 device driver for
Intel 10 GbE PCIe adapters. For GPU, we use the device driver
195.36.15 and CUDA SDK 3.0.
4. OPTIMIZING PACKET I/O ENGINE
High-speed software routers typically spend a large fraction of
CPU cycles on packet reception and transmission via NICs, the com-
mon part of all router applications. For example, RouteBricks re-
ports that 66% of total cycles are spent on packet I/O for IPv4 for-
warding [19]. This means that even if we eliminate other packet
handling costs with the help of GPU, the expected improvement
would not exceed 50% according to Amdahl’s law.
In order to achieve multi-10G packet I/O performance in the soft-
ware router, we exploit pipelining and batching aggressively.
In
this work we focus on the basic interfacing with NICs and leave
other advanced features, such as intermediate queueing and packet
scheduling, for future work.
4.1 Linux Network Stack Inefﬁciency
Network stacks in the OS kernel maintain packet buffers. A packet
buffer is a basic message unit passed across network layers. For
example, Linux allocates two buffers, an skb holding metadata and a
buffer for actual packet data, for each packet. This per-packet buffer
allocation applies to Click [30] as well since it relies on Linux data
structures. We observe two problems arising here:
• Frequent buffer allocation and deallocation stress the memory
(a) Linux packet buffer allocation
(b) Huge packet buffer allocation
Figure 4: Comparison of packet buffer allocation schemes
subsystem in the kernel.
tens of millions of buffer allocations per second.
In multi-10G networks, this implies
• The metadata size in skb is too large (208 bytes long in Linux
2.6.28), as it holds information required by all protocols in vari-
ous layers. It is overkill for 64B packets.
To quantify where most CPU cycles are spent, we measure the
CPU consumption of the packet reception process. We have the
unmodiﬁed ixgbe NIC driver receive 64B packets and silently drop
them. Table 3 shows the breakdown of the CPU cycles in the packet
RX process. We see that skb-related operations take up 63.1% of the
total CPU usage: 4.9% on initialization, 8.0% on allocation and deal-
location wrapper functions, and 50.2% on base memory subsystem
(the slab allocator [16] and the underlying page allocator) to handle
memory allocation and deallocation requests.
Cache invalidation with DMA causes compulsory cache misses
accounting for 13.8% in the table. Whenever a NIC receives or trans-
mits packets, it accesses the packet descriptors and data buffers with
DMA. Because DMA transactions invalidate corresponding CPU
cache lines for memory consistency, the ﬁrst access to memory re-
gions mapped for DMA causes compulsory cache misses.
4.2 Huge Packet Buffer
As described above, per-packet buffer allocation (in Figure 4(a))
causes signiﬁcant CPU overhead. To avoid the problem, we im-
plement a new buffer allocation scheme called huge packet buffer
(Figure 4(b)). In this scheme, the device driver does not allocate skb
and a packet data buffer for each packet. Instead, it allocates two
huge buffers, one for metadata and the other for packet data. The
buffers consist of ﬁxed-size cells, and each cell corresponds to one
packet in the RX queue. The cells are reused whenever the circular
RX queues wrap up. This scheme effectively eliminates the cost
of per-packet buffer allocation. Huge packet buffers also reduce
per-packet DMA mapping cost (listed in Table 3 as part of NIC
device driver cost), which translates the host memory address into
I/O device memory address so that NICs can access the packet data
buffer. Instead of mapping a small buffer for every packet arrival, we
have the device driver map the huge packet buffer itself for efﬁcient
DMA operations.
To reduce the initialization cost of the metadata structure, we
keep the metadata as compact as possible. Most ﬁelds in skb are
unnecessary, for packets in software routers do not traverse the Linux
network stack. We have removed the unused ﬁelds and the resulting
metadata cell is only 8 bytes long rather than 208 bytes. Each cell
of the packet data buffer is 2,048-byte long, which ﬁts for the 1,518-
198cent NICs support core-aware RX and TX queues with Receive-Side
Scaling (RSS) [12]. RSS evenly distributes packets across multiple
RX queues by hashing the ﬁve-tuples (source and destination ad-
dresses, ports, and a protocol) of a packet header. Each RX and TX
queue in a NIC maps to a single CPU core, and the corresponding
CPU core accesses the queue exclusively, eliminating cache bounc-
ing and lock contention caused by shared data structures. Our packet
I/O engine takes advantage of RSS and multiple queues in a NIC.
However, we ﬁnd that core-aware multi-queue support alone is
not enough to guarantee the linear performance scalability with the
number of CPU cores.
In our microbenchmark with eight cores,
per-packet CPU cycles increase by 20% compared to the single-core
case.
After careful proﬁling, we have identiﬁed two problems. First,
some per-queue data that are supposed to be private bounce between
multiple CPU caches. It happens when the data of different queues
accidently share the same cache line (64B per line in the x86 archi-
tecture). We eliminate this behavior, known as false sharing [50],
by aligning every starting address of per-queue data to the cache line
boundary. The second problem comes from accounting. Whenever
the device driver receives or transmits a packet, it updates per-NIC
statistics. These globally shared counters stress CPU caches as every
statistics update is likely to cause a coherent cache miss. We solve
the problem by maintaining per-queue counters rather than per-NIC
ones so that multiple CPU cores do not contend for the same data.
Upon a request for per-NIC statistics (from commands ifconfig
or ethtool), the device driver accumulates all per-queue counters.
This on-demand calculation for a rare reference to NIC statistics
keeps frequent statistics updates cheap.
4.5 NUMA Scalability
In a NUMA system, memory access time depends on the location
of physical memory (see Figure 3). Locally-connected memory al-
lows direct access while remote memory access requires an extra hop
via the hosting CPU node. Also, DMA transactions from a device to
a remote node traverse multiple IOHs and reduce I/O performance.
In our testing, we see that node-crossing memory access shows 40-
50% increased access time and 20-30% lower bandwidth compared
to in-node access.
To maximize packet I/O performance in the NUMA system, we
make the following two choices. First, we carefully place all data
structures in the same node where they are used. The packet de-
scriptor arrays, huge packet buffers, metadata buffers, and statistics
data of NICs are allocated in the same node as the receive NICs.
Second, we remove node-crossing I/O transactions caused by RSS.
By default RSS distributes packets to all CPU cores and some cross
the node boundary. For example, half of the packets received by
NICs in node 0 in Figure 3 would travel to the memory in node 1.
To eliminate these crossings, we conﬁgure RSS to distribute packets
only to those CPU cores in the same node as the NICs. We set the
number of RX queues as the number of corresponding CPU cores
and map the RX interrupts for the queues to those CPU cores in the
same node. With these modiﬁcations, interrupts, DMA transactions,
and PCIe register I/O for packet RX do not cross the node boundary.
With NUMA-aware data placement and I/O, we see about 60%
performance improvement over NUMA-blind packet I/O. NUMA-
blind packet I/O limits the forwarding performance below 25 Gbps
while NUMA-aware packet I/O achieves around 40 Gbps. Our result
contradicts the previous report: RouteBricks concludes that NUMA-
aware data placement is not essential in their work [19] with multi-
gigabit trafﬁc. We suspect that additional memory access latency
Figure 5: Effect of batch processing with a single core and two
10 GbE ports. All packets are 64B.
byte maximum Ethernet frame size and works around the NIC re-
quirement of 1,024-byte data alignment.
4.3 Batch Processing
Batch processing of multiple packets reduces per-packet process-
ing overhead signiﬁcantly. Batching can be applied at every step in
packet processing: (i) in hardware: NICs aggregate multiple packets