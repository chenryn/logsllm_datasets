analysis tools require a signiﬁcant amount of effort to annotate the
source, so that useful analysis models can be generated.
Specialized analysis tools focus on speciﬁc types of bugs with
simpliﬁed source annotation. Engler et al enables extension of
GCC, called xgcc, to do source analyses, which they refer to as
meta-compilation [5]. A rule language, called metal, is used to
express the necessary analysis annotations in a higher-level lan-
guage. Since the rules match multiple statements, the amount of
annotation effort is reduced. A variety of software bugs, including
security vulnerabilities, have been found by this tool [1].
Most of the specialized analysis tools lack completeness (i.e.,
may result in false negatives), but MOPS speciﬁcally aims for ease
of speciﬁcation and completeness of analysis [3]. Using MOPS,
security properties are expressed as ﬁnite state automata and pro-
grams are represented as pushdown automata. Given security prop-
erty analyses that can be represented in this model, a complete anal-
ysis is possible under certain assumptions.
In another effort, we use one program analysis tool, CQUAL [6],
in an approach to ﬁnding LSM hook placement bugs statically [18].
Using GCC analysis to automate CQUAL annotation, we can then
perform a CQUAL analysis that veriﬁes that all controlled opera-
tions are mediated by at least one LSM hook. In general, we also
want to verify that a controlled operation is only run when its re-
quired authorization hooks have been checked. While it is possible
to verify this in CQUAL, we need to ﬁnd the required authoriza-
tions. None of the existing static analysis tools help ﬁnd the re-
quired authorizations, so we have built the runtime collection and
analysis tools described in this paper. A Java static analysis tool,
called Jaba [11], has been used to collect the actual authorization
on controlled operations, but it has two shortcomings: (1) it does
not analyze the C code of the Linux kernel and (2) it does not pro-
vide guidance about whether the authorizations made were correct.
Our analysis tools collect the authorizations made and enable ex-
amination of the authorizations of particular controlled operations
for inconsistencies. We found such an approach useful for ﬁnding
missing LSM hooks, because we could identify many cases where
the same authorizations are expected.
Another related problem is the certiﬁcation of systems. Histori-
cally, the Orange Book [13] was used for guidance in the construc-
227tion of secure operating systems, but this is now being supplanted
by the Common Criteria [8]. However, the certiﬁcation task is ad
hoc and laborious, and has generally not been successful in improv-
ing the security of commonly-used operating systems. Gutmann
argues in his thesis [7] that certiﬁcation approaches, including for-
mal veriﬁcation tools, are doomed to failure unless they represent
concepts at the level of the source code. Gutmann also advocates a
combination of static and runtime analyses. The approach that we
use differs from certiﬁcation in the sense that it checks for particu-
lar errors rather than providing a top-down assurance that the over-
all system meets its requirements. An interesting research question
is whether a sufﬁcient breadth and depth of such checks could pro-
vide a conﬁdence comparable to certiﬁcation. Unlike certiﬁcation,
such conﬁdence could be maintained as the source code evolves.
3. SOLUTION DESCRIPTION
The key insight we leverage in runtime analysis for the Linux
Security Modules (LSM) framework is that the LSM authorization
hook placement is largely correct, such that cases that are incon-
sistent with the norm are likely to be indicative of an error. For
example, it would be considered unusual if a particular controlled
operation has different authorization requirements on different runs
of the same system call. While this insight does not guarantee that
we ﬁnd all LSM hook placement bugs, it has enabled us to identify
some bugs and has served as a valuable guide for the tool develop-
ment.
We have found that the attributes of controlled operations can
be totally-ordered with respect to their impact on authorization re-
quirements. For example, if all the controlled operations in a sys-
tem call have the same authorizations, then the value of the other
attributes of a controlled operation do not affect the authorizations
(i.e., system call is at the top of the order). We use this knowledge
to identify cases that are anomalous (i.e., authorizations are sensi-
tive to attributes that they should not be) and to partition controlled
operations into their maximal-sized classes by common authoriza-
tions. Further unexpected sensitivities in these classes are used to
identify errors.
In all of the discussion below, we use the following assumptions.
First, we leverage the type safety of much of the Linux kernel. This
does not invalidate any of the errors we ﬁnd, but there could be
other errors as well. Second, we assume that accesses to objects
of the authorized data types deﬁne the mediation interface. These
data types are the ones that correspond to system call concepts (e.g.,
ﬁles, inodes, sockets, skbuffs, ipc message queues, etc.). Access to
kernel data is designed to go through these data structures. While
we have not explicitly validated this, we have done some more de-
tailed analysis presented elsewhere [4].
3.1 Authorization Sensitivity Attributes
Table 1 lists the attributes of controlled operations to which au-
thorization requirements may be sensitive. We refer this group
of attributes collectively as the authorization sensitivity attributes.
Each controlled operation has information about the conditions un-
der which it was executed (system call, system call inputs, function,
location in function, path to controlled operation), the object it was
executed upon (datatype and object), and the operation performed
(member/access).
These attributes are totally-ordered, such that if the authoriza-
tions of controlled operations differ when the value of one factor is
changed, then the authorizations also differ when a higher factor is
changed. For example, if two controlled operations on a particu-
lar object have different authorizations, then that datatype will also
have different authorizations for the two controlled operations.
Conversely, if the authorization requirements of controlled op-
erations are insensitive to changes in one factor, then they are also
insensitive to changes in all lower factors. For example, if all con-
trolled operations on the same datatype have the same authoriza-
tions, then so do all controlled operations on the same (structure)
member.
3.2 Authorization Sensitivity Impact
The classiﬁcation of controlled operations by their authorization
sensitivity divides the controlled operations into two categories: (1)
known anomalies and (2) sensitivity classes whose authorization
requirements need veriﬁcation. In the ﬁrst case, sensitivity to some
of the authorization sensitivity attributes is considered illegal. We
deﬁne invariants below for these cases. In the second case, we par-
tition the controlled operations into maximal-sized classes with the
same authorizations. These classes enable veriﬁcation of authoriza-
tion requirements and identiﬁcation of anomalous classiﬁcations.
3.2.1 Anomalies
The sensitivity of authorizations to the attributes below the dou-
ble line in Figure 1, intra-function and path, are always considered
to be anomalous. Sensitivities of these types mean that the ex-
ecution path (path) or location within a function (intra-function)
determine the authorization requirements of a particular controlled
operation on the same member.
The following invariant formally expresses our path insensitivity
invariant.
3.2.1.1 Path Insensitivity Invariant.
8c1; c2 2 C; e1; e2 2 E; c1 = c2 ^
e1 = e2 ! Rc1; e1 = Rc2; e2
(1)
This invariant states that the same controlled operation (c1 = c2)
run in the same event (e1 = e2 deﬁned by the system call and its
inputs) must have the same authorization requirements (deﬁned by
the function R). That is, the execution path within an event cannot
affect a controlled operation’s authorization requirements.
Similarly, we deﬁne an invariant for intra-function insensitivity.
3.2.1.2 Intra-Function Insensitivity Invariant.
8c1; c2 2 C; e1; e2 2 E; F c1 = F c2 ^
 c1 =  c2 ^ e1 = e2 ! Rc1; e1 = Rc2; e2
(2)
In this case, two controlled operations in the same function (com-
puted by the function F ) and which make the same member access
(computed by the function ) must have the same authorization
requirements R.
3.2.2 Authorization Sensitivity Classes
For the other cases, we cannot easily identify them as errors.
Instead, we partition the controlled operations into authorization
sensitivity classes based on their authorizations and attribute sen-
sitivity and determine whether their authorization requirements are
correct.
The authorization sensitivity class computation is as follows. For
each sensitivity level starting at the highest (system call), we parti-
tion the controlled operations into sensitivity classes where all con-
trolled operations have the same value for the sensitivity attribute,
then we test whether the class also has the same authorizations.
If not, then we try the next lower attribute and partition based on
228Factor
System Call
Syscall Inputs
Datatype
Object
Member
Function
Intra-function
Path
Authorizations are same for:
all controlled operations in system call
all controlled operations in same system call with
same inputs
all controlled operations on objects of the same datatype
all controlled operations on the same object
all controlled operations on same datatype, accessing
same member, with same operation
all same member controlled operations in same function
same controlled operation instance
same execution path to same controlled operation instance
Table 1: Authorization Sensitivity Factors: names and effects on authorizations
both attributes and test again. This approach repeats until we have
assigned every controlled operation to a sensitivity class.
Partitioning depends on the attribute. For the system call at-
tribute, all the controlled operations of a system call are in one
class. For system call inputs, all controlled operations of the same
system call and with the same type of inputs are aggregated (see
Section 3.3 below). For the datatype attribute, the controlled oper-
ations are classiﬁed by the system call, inputs, and datatype of the
operation’s object. Thus, successively ﬁner partitions are created in
each step of the analysis.
A classiﬁcation succeeds (i.e., is x-sensitive where x is the at-
tribute) if it is the ﬁrst attribute in which all the controlled oper-
ations in that class have the same authorizations. Note that other
classes at the same sensitivity that have the same authorizations are
aggregated to form the maximal-sized classes. Once the classes are
created it is a manual process to verify that the authorizations for
each class is correct. For the ﬁle system, the number of classes is
small enough that manual veriﬁcation is practical.
As an example, consider the read system call. File operations
are datatype-sensitive because all controlled operations on ﬁle ob-
jects are authorized for read. Manual veriﬁcation involves check-
ing that read permission for ﬁles is sufﬁcient. Since the read au-
thorization also is intended for the ﬁle’s inode, we mark the ﬁle’s
inode as authorized for read as well. However, after classiﬁcation,
one inode controlled operation is not authorized. It is on a different
object, so inode operations are object-sensitive. This is an opera-
tion on the directory inode of the ﬁle to determine whether a signal
should be sent as a result of a read in this directory. Several other
ﬁle system calls also perform test for notiﬁcation, and notiﬁcation
is only performed if the original ﬁle operation is authorized. There-
fore, we can say that this directory inode should also be authorized
for ﬁle read. The same goes for the current task and superblock as
well. It is straightfoward to extend the collection to do this, how-
ever. Ultimately, we would expect that all controlled operations in
the read system call are authorized for read access.
Other than ﬁnding an authorization completely missing, the most
common way for identifying an error is to ﬁnd two classiﬁcations
(i.e., two aggregates with different authorizations) that perform an
important common operation. This situation occurred in fcntl
where two different classiﬁcations (based on different system call
inputs) operate on the same f_owner ﬁeld (see Section 4.2.4).
In comparison to static analysis, we both verify that the objects
are authorized and verify what the authorizations should be in a
single step. Both the static and runtime approaches enable quick
veriﬁcation that the ﬁle and most inodes are authorized properly.
Both identify that the directory inode is not authorized.
In both
cases, manual examination is necessary to determine whether there
is an exploitable situation. However, the runtime approach has an
advantage that it is easier to state additional authorizations, such
as for the directory inode in the read system call. Also, the ver-
iﬁcation of the speciﬁc policy operation authorized is easier in the
runtime analysis.
3.3 Necessary Data Collection
By logging system call entry/exits/arguments, function entry/exits,
controlled operations (i.e., object, datatype, member, and opera-
tion), and authorizations, we collect all the necessary values for the
sensitivity attributes. All the information can be easily logged, but
the identiﬁcation of meaningful object identiﬁers and system call
input changes need some further analysis.
During execution, objects are referenced via function pointers,
but this is not necessarily a sufﬁcient identiﬁcation of an object.
For example, an inode has a persistent identiﬁer (i.e., device, inode
number) that is used in authorization. Therefore, for each datatype
we deﬁne a speciﬁc approach for computing their object identiﬁers.
These identiﬁers are used for determining all operations and autho-
rizations on an object.
Across system calls, we assume objects that are used in the same
variable have the same authorization requirements. To simulate this
we use the ﬁrst controlled operation in which an object appears as
an identiﬁer. If two objects are ﬁrst accessed in the same controlled
operation they must be assigned to the same variable. However,
different execution paths may result in the same variable being used
in a different controlled operation ﬁrst. However, aggregation of
classes with the same authorization requirements will merge these
cases, so this assumption has proven effective.
The system call arguments change on almost every call, but only
a few of the arguments really impact authorizations (e.g., the ac-
cess ﬂag on open). Therefore, we collect the arguments, but only
use the arguments that we have found impact authorization require-
ments to do partitioning. Only a few system calls that we have
examined have different authorizations based on their input argu-
ments, such as open, ioctl, and fcntl. Because different au-
thorizations are used based on different inputs, these system calls
are more complex, and hence, more prone to errors.
4.
IMPLEMENTATION
Complete authorization is veriﬁed by analyzing (ofﬂine) a kernel
execution log. This section describes the implementation of the
tool that creates this log, the implementation of the log ﬁltering
tool used to prepare and display analysis data, and the results of
our analysis thusfar.
229Record Type
Controlled Op.
Authorization
Function Entry
Function Exit
Data
Context ID Controlled Op. ID OID
Context ID Auth. ID
OID
Context ID Instruction Addr.
Context ID
Table 2: Log Record Types
Controlled Op. Filter