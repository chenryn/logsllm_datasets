Ping statistics [34]
Link-level drop statistics
Switch-level drop statistics
Canaries
Description
Data from a monitoring system that periodically records latency between pairs of servers in the DC.
Data from a diagnosis system that identifies links dropping packets [64].
Data from a diagnosis system that identifies switches dropping packets [64].
Data from ‚Äúcanary‚Äù VMs which run on every rack in the DC and test reachability to the Internet on commonly
used (reserved) ports. These VMs are also routinely used to test new software deployments [3].
Records of when a VM, host, or a switch is rebooted.
Data from counters that indicate the packet loss rate on a switch port.
Device reboots
Link loss status
Packet corruption rate (FCS) Data from a system that periodically checks the loss rate (due to corruption) on a link and reports an error if it
SNMP [20] and Syslogs [28]
PFC counters
Interface counters
Temperature
CPU usage
is above an operator specified threshold.
Data from standard network monitoring systems.
Periodic counts of priority flow control (PFC) messages sent by RDMA-enabled switches.
Number of packets dropped on a switch interface.
The temperature of each component (e.g., ASIC) of the switch or server.
The CPU-usage on the device.
Table 2: Data sets used in PhyNet Scout.
and a natural language processing (NLP)-based recommendation
system.
The NLP-based system is a multi-class classifier that only takes
the incident description as input. It constructs features from the
incident description using the approach of [31]. The classifier pro-
duces a ranked list (along with categorical ‚Äî high, medium, and low
‚Äî confidence scores) as a recommendation to the operator. This sys-
tem has high precision but low recall (Table 1). This is, in part, due
to suffering from the challenges described in ¬ß1. In addition, (a) the
text of the incident often describes the symptoms observed but does
not reflect the actual state of the network‚Äôs components; (b) the
text of the incident is often noisy ‚Äî it contains logs of conversation
which often lead the ML model astray.
Our metrics compare Scouts to the current state of incident
routing (with the above mechanisms in place):
Gain: the benefit (in investigation time) the Scout offers. This is
measured as gain-in ‚Äî time saved by routing incidents directly
to the team when it is responsible; and gain-out ‚Äî time saved by
routing incidents away from the team when it is not responsible.
We measure these times as a fraction of the total investigation time.
Overhead: the amount of time wasted due to the Scout‚Äôs mistakes.
We again break overhead into overhead-out ‚Äî the overhead of send-
ing incidents out to other teams by mistake; and overhead-in ‚Äî
the overhead of sending incidents to the team itself by mistake.
Sadly, we do not have ground truth to measure overhead directly.
To estimate overhead-in, we first build the distribution of the over-
head of mis-routings to PhyNet using the baseline (Figure 6). We
then, using standard probability theory and assuming incidents are
independent and identically distributed, calculate the distribution
of our system‚Äôs overhead. We cannot estimate overhead-out: the
multitude of teams the incident can be sent to and the differences
in their investigation times make any approximation unrealistic.
We present error-out instead: the fraction of incidents mistakenly
sent to other teams.
Figure 7: Gain/overhead for mis-routed incidents:
gain/overhead in (b) gain/error out.
(a)
The PhyNet Scout significantly reduces the investigation time
of mis-routed incidents with little additional overhead (Figure 7).
It closely mimics a perfect gate-keeper: in the median, the gap
between our Scout and one with 100% accuracy is less than 5%.
For those incidents that were already correctly routed (no oppor-
tunity for gain) our Scout correctly classifies 98.9% (no overhead).
Even at the 99.5ùë°‚Ñé percentile of the overhead distribution the Scout‚Äôs
overhead remains below 7.5%: much lower than the gain in the case
of mis-routed incidents. This overhead is an upper bound on what
we expect to see in practice: we use mis-routed incidents (typically
harder to diagnose compared to these correctly routed incidents)
to approximate overhead.
7.2 Analysis of (Mis-)Predictions
The Scout can correctly classify many, previously mis-routed, inci-
dents. For example, in one instance, VMs in a cluster were crashing
because they could not connect to storage. The incident was first
sent to the storage team ‚Äî it was created by their watchdogs. Stor-
age engineers guessed the issue was caused by a networking prob-
lem and sent the incident to PhyNet, which found a configuration
change on the ToR switch that caused it to reboot and interrupt
7.1 Benefit of the PhyNet Scout
Our Scout‚Äôs precision is 97.5%, and recall 97.7% leading to an F-1
score of 0.98. In contrast, today, the precision of the provider‚Äôs inci-
dent routing system is 87.2%, with a recall of 91.9% and a resulting
F-1 score of 0.89.
Figure 8: Comparing decider algorithms with: (a) 10 day and
(b) 60 day retraining intervals.
Gain-InBest possible gainOverhead-inFraction of investigation time (%)(a)CDF0.00.20.40.60.81.01.0Gain-outBest possible gain Error  out : 1.7%0.00.20.40.60.8Fraction of investigation time (%)(b)100020  40   60801000   20  40   6080Time(a)F-1 score0.40.50.60.70.80.91.0Conservative one class SVM Aggressive one class SVMAdaboostBag of words0.40.50.60.70.80.91.0Time(b)10-0111-0112-0101-0102-0109-0110-0111-0112-0101-0102-0109-01SIGCOMM ‚Äô20, August 10‚Äì14, 2020, Virtual Event, NY, USA
7.3 Adapting to Changes
We next evaluate our Scout framework:
Adapting to deprecated monitoring systems. The Scout frame-
work should automatically adapt to changes in the available moni-
toring data ‚Äî operators should not have to design a new Scout from
scratch each time. Changes can happen in one of two ways: old
monitoring systems may be deprecated or new ones deployed. Due
to limited space, we evaluate the more harmful of the two: when
old systems are deprecated and the Scout has less information to
work with. We randomly select ùëõ monitoring systems and remove
all features related to them from the training set (Figure 9). The
framework can automatically adapt and its F-1 score drops only by
1% even after 30% of the monitoring systems are removed (ùëõ = 5).
To show the worst-case, we next remove the most influential mon-
itoring systems (based on feature importance) first. The drop in
F-1 score is more significant but remains below 8% after 30% of
the monitoring systems are removed. This indicates many moni-
tors can pickup PhyNet related symptoms which, combined with
re-training, helps recover from removing a small number of them.
Adapting to changes in incidents over-time. CPD+ can clas-
sify new incidents (the RF model has low accuracy in such cases).
Over time, the framework re-adapts itself so that it can classify
such incidents more accurately through retraining. We show this
under different re-training frequencies in (Figure 10). We show two
different scenarios: (a) when the training set continues to grow as
new incidents are added ‚Äî all of the incident history is kept for
training and (b) where we keep only the past 60 days of incidents
for training. We see the model can adapt and maintain an F-1 score
higher than 0.9 if it uses a 10-day retraining interval (Figure 10-
a). We also see that in October-November a new type of incident
kept recurring which the model initially consistently mis-classified.
More frequent retraining allowed the Scout to quickly learn how
to classify this new type of incident and recover its high accuracy.
However, less frequently trained Scout‚Äôs continued to suffer.
7.4 Benefits for Different Incident Classes
We next show how Scouts help different types of mis-routed in-
cidents. We split incidents into three types based on how they
were created: customer reported, PhyNet monitor (those created by
PhyNet‚Äôs monitoring systems), and non-PhyNet monitor incidents
(those created by other teams‚Äô watchdogs):
PhyNet monitor incidents: Unsurprisingly, most of these incidents
were correctly routed to PhyNet ‚Äî our system classifies all such
incidents correctly as well. But there is a small subset of these
incidents which should go to other teams and so our system can
provide substantial gain-out for these incidents. Specifically, 0.19%
of incidents in our test set (of mis-routed incidents) were those
Figure 10: Adapting over time by re-training: (a) the size of
the training set keeps growing. (b) the size of the training
set is fixed (60 days).
Figure 9: The framework can adapt to deprecated monitor-
ing systems.
connectivity. The incident implicated the 2 servers hosting the VMs
and the cluster. Using the number of ToR reboots and ping statistics
from the cluster, the RF predicted it was a PhyNet incident. Here,
the RF also assisted in the diagnosis by pointing directly to the root
cause. But like any ML system, the Scout also made a few mistakes.
We next study their cause:
Why does the Scout have false negatives? False negatives hap-
pen when (in order of frequency):
The incident is transient. These incidents are typically created by
alerting systems: when a particular metric crosses a threshold, an
incident is created to alert operators to a potential issue. Some-
times the cause is a temporary spike and operators monitor the
implicated components (to ensure customers are not impacted) and
then close the incident. These incidents are difficult for the Scout to
classify correctly as the monitoring data will show healthy PhyNet
components.
None of the monitoring data captures the incident‚Äôs symptoms: For
example, in one instance, an operator created an incident to track
fixes to incorrect DHCP configurations on a ToR. None of the mon-
itoring data used by our Scout captured DHCP problems and the
Scout made an error.
The problem is due to an implicit component. We observe cases
where data that could explain an incident is available, but it was
of a component not mentioned in the incident (which was also not
found as a dependency of mentioned components).
There are too many components in the incident. In a few instances,
although the incident was caused by a PhyNet problem, there were
too many clusters impacted: the incident mentioned too many com-
ponents. This diluted the (set of statistics ¬ß5) features and resulted
in a mis-prediction. Such incidents are an inherent limitation of our
framework (see¬ß9), however, we found such incidents to be rare.
Why does the Scout have false positives? Sometimes, the Scout
may route incidents incorrectly to PhyNet. Such cases are rare but
occur because of:
Simultaneous incidents. In one instance, our software load balancer
experienced a problem in a cluster which was also experiencing
a PhyNet problem. The incident only implicated a cluster ‚Äî no
individual switch or server was implicated ‚Äî and the Scout mistak-
enly routed the incident to PhyNet. Such mistakes happen only if
the incident: (a) overlaps (in time) with a PhyNet incident; (b) and
shares the same set (or a subset) of components with the PhyNet
incident. Such cases are rare but are a limitation of our framework.
The Scout Master could potentially resolve this, but only if the other
teams have also built their own Scout.
# of monitoring systems removedF-1 scoreWorst caseAverage case12345670.860.880.900.920.940.960.981.00Time10-01F-1 score0.40.50.60.70.80.91.010 days20 days30 days60 days0.40.50.60.70.80.91.0(a)Time(b)09-0111-0112-0101-0110-0109-0111-0112-0101-0102-0102-01SIGCOMM ‚Äô20, August 10‚Äì14, 2020, Virtual Event, NY, USA
Gao et al.
detected and reported the incident immediately. Automated sys-
tems tried to resolve the problem but were unsuccessful. A database
operator was then alerted to manually investigate the cause of the
problem. In the end, a network issue was responsible for this failure:
a ToR switch had failed in that cluster which caused the connections
to all servers connected to it to also fail. The incident is eventually
sent to the PhyNet team. With the Scout the time and effort of the
database operator could have been saved and the incident could
have directly been routed to the PhyNet team.
This is a typical example of how a Scout can help operators:
team A‚Äôs failure caused a problem that was detected by team B‚Äôs
watchdogs. When team B‚Äôs automated systems fail to resolve the
problem, engineers from that team are alerted to figure out where
to route the incident. If team B‚Äôs automated systems had queried
team A‚Äôs Scout, team B‚Äôs operators need not have gotten involved.
Virtual IP availability drop. Our network support team received
an incident reporting connectivity problems to a particular virtual
IP. The potential teams responsible for these incidents were the
software load balancing team (SLB) that owns the mapping between
this virtual IP and the physical IPs that serve it, the host networking
team, and the PhyNet team.
The support team first identified that the SLB team had deployed
an update in the same cluster the incident had occurred. There-
fore, they suspected that the SLB component may have caused the
incident. The incident was passed on to the SLB team where an
operator investigated and concluded the SLB nodes were healthy.
The incident was then routed to the host networking team, but their
service too was healthy. Next, the incident was sent to the PhyNet
team where operators quickly identified the problem: a ToR switch
had reloaded and this had triggered a known bug that caused the
availability drop.
If the support team had first queried all available Scouts, the
PhyNet Scout would have identified the cause as being due to a
PhyNet issue (our PhyNet Scout classified this incident correctly).
This would have significantly reduced the investigation time for
this incident.
We have extended evaluations in Appendix B.
8 LESSONS FROM DEPLOYMENT
Our Scout is currently running in production as a suggestion mech-
anism. Operators‚Äô feedback since deployment has been instructive
in a number of ways:
Scouts should not make ‚Äúeasy‚Äù mistakes. Although our Scout
has high accuracy and classifies many (mis-routed) incidents cor-
rectly, as with any ML predictor, it sometimes makes mistakes. A
few of these mistakes happened on incidents where the cause was
known to the operator, either because the incident itself clearly
pointed to the cause (e.g., for those incidents created by PhyNet
watchdogs) or due to context the operator had about why the in-
cident happened (e.g., they knew of a particular code change that
was the cause). When the Scout mis-classified such incidents we
found operators questioned its benefit and were more reluctant to
rely on it (despite its high accuracy). As most incidents created by
PhyNet‚Äôs monitoring systems fell into this category, we decided
to not pass those incidents through the Scout at all ‚Äî after all, the
benefit of PhyNet Scout for PhyNet monitor-generated incidents
was minimal to begin with.
Figure 11: The Scout‚Äôs gain and overhead for mis-routed in-
cidents created by other teams‚Äô watchdogs: (a) gain and over-
head in (b) gain and error out.