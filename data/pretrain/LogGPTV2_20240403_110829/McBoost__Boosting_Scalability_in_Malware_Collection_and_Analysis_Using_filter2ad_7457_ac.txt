the possible false positives caused by dynamic unpackers 3
(i.e., either Polyunpack, our unpacker, or Renovo).
Benign-Dataset (BDset). We collected 2,258 benign ex-
ecutables extracted from an installation of Windows XP
Home with the addition of common user applications (e.g.,
WinZIP, WinAmp, AcroRead, etc.). We double checked
these binaries using clamAV, F-Prot, and AVG to verify that
the collected binaries were actually benign applications. We
also submitted the binaries for which we had any doubts
to VirusTotal (www.virustotal.com) to check them
against a diverse set of AV-software.
(PBDset). We ran PEiD on
Packed-Benign-Dataset
Benign-Dataset and we found 27 packed binaries (i.e.,
around 1.2% of the benign), most of which were packed
with UPX. Also, we selected 45 benign executables from
the start menu of a clean Windows XP installation and we
packed each one of them with 17 different popular pack-
ers (including UPX (upx.sourceforge.net), ASpack
(www.aspack.com), Themida (www.oreans.com/
themida.php), Obsidium (www.obsidium.de), etc.)
That is, for each of these executables, we created 17
packed executables. For each packer we enabled all of the
available anti-debugging and anti-reverse engineering tech-
niques. We veriﬁed that some packers failed to correctly
pack several binaries, causing them to fail when executed.
Therefore, we pruned the dataset keeping 195 binaries that
still worked correctly after packing. Overall, we obtained
222 (195 plus 27) packed benign executables.
Non-Packed-Benign-Dataset (NPBDset). This dataset
was obtained by removing the 27 packed benign we found
in Benign-Dataset, thus keeping 2,231 non-packed benign.
3.1.2 Parameter Setting
In this section we discuss how we set the parameters of
each module of McBoost. For constructing the classiﬁers
we use WEKA (http://www.cs.waikato.ac.nz/
ml/weka), a collection of open-source data mining soft-
ware written in Java. For module A1 (see Section 2) we use
a Multi-Layer Perceptron (MLP) classiﬁer. Our MLP has
three layers, namely an input layer, a hidden layer and an
output layer. The input layer has 9 nodes (equal to the num-
ber of features extracted by A1), whereas the output layer
3We found that some executables dynamically generate few executable
instruction in memory and then jump on them, although they did not ap-
pear to be packed with executable packing tools, according to our manual
analysis.
306306
Classiﬁer
A1 (heuristics)
A2 (n-gram on code)
A3 (n-gram on ﬁle)
A (multiple classiﬁers)
Accuracy
0.973
0.976
0.993
0.994
FP
0.012
0.034
0.004
0.008
DR
0.958
0.987
0.990
0.996
AUC
0.995
0.981
0.993
0.997
Table 1: Validation of module A for packed vs. non-packed classi-
ﬁcation using a detection threshold = 0.5 (FP = false positive rate,
DR = detection rate).
Classiﬁer
C1 (non-packed code)
C2 bpage (hidden code)
C2 bbexec (hidden code)
Accuracy
0.823
0.938
0.745
FP
0.026
0.0
0.112
DR
0.772
0.937
0.738
AUC
0.959
0.988
0.901
Table 2: Validation of module C1 and C2 for malware vs. benign
code classiﬁcation using a detection threshold = 0.5 (FP = false pos-
itive rate, DR = detection rate).
has two nodes (one for each class label). We set the number
of perceptrons in the hidden layer to 5. Modules A2, A3,
C1, and C2 (see Section 2) are all built using Bagged-J48
(J48 is an implementation of the well-known C4.5 decision-
tree classiﬁer). For each module, the Bagged-J48 is con-
structed by combining 10 base decision-tree (J48) classi-
ﬁers. We used n-grams with n = 3 for A2, A3, and
C1, whereas we used n = 2 for C2 because it produced
much better results then using n = 3. The per layer and
global time-out for our unpacker (module B) were set to
Tl = 4 minutes and Tg = 20 minutes, respectively (see
Section 2.2).
3.2 Validation of Single Modules
Detection of Packed Executables (Module A).
In order
to test module A, and its sub-modules A1, A2, and A3,
we constructed a labeled dataset of packed and non-packed
executables. By merging the dataset of packed malware
PMDset and the dataset of packed benign PBDset we ob-
tained the dataset of packed executables. The dataset of
non-packed executables was constructed by merging the
dataset of non-packed benign NPBDset and the dataset of
non-packed malware NPMDset. Overall, we obtained a
dataset which consisted of 2,300 packed executables and
2,377 non-packed executables. We then randomly split this
dataset into two portions, a portion made of 80% of the data
used for training the classiﬁers, and a portion made of the
remaining 20% of the data for testing. The classiﬁcation
results are reported in Table 1. The accuracy, false posi-
tives and detection rate were computed setting the detection
threshold θ = 0.5 (in the following we will always assume
θ = 0.5 for module A, unless otherwise speciﬁed). The
average time needed for the classiﬁcation of an executable
was 1.025 seconds. As we can see from Table 1, the com-
bination of classiﬁers improves on the already very good
performance of the single classiﬁers, in particular in terms
of AUC. Also, combining diverse classiﬁers contributes in
making evasion attempts intuitively harder.
Extracting Hidden Code (Module B).
In order to vali-
date the performance of our implementation of the dynamic
unpacker (module B), we tested it with the executables
in the PMDset (2,078 packed malware) and PBDset (222
packed benign). Our unpacker was able to correctly extract
hidden code from 169 packed benign (76.1%) and from
1,943 packed malware (93.5%). Among the 188 packed ex-
ecutables that module B was not able to unpack, 25 (1 be-
nign and 24 malware) caused an “application crash” error,
whereas 29 (all malware) caused a “non-win32 application”
error (likely because of a corrupted PE header).
We also measured the time needed for the unpacker to
analyze an executable. We found that the average time for
analyzing a malware executable was 4.7 minutes, whereas
the average time for the analysis of a benign executable was
5.6 minutes.
Detection of Malicious Code (Modules C1 and C2).
In
order to evaluate module C1 we constructed a labeled
dataset of code sections extracted from non-packed benign
(NPBDset) and non-packed malware (NPMDset). In total,
we had 2,377 non-packed executables. The PE analysis tool
we used to extract the content of the code section failed in
certain cases, either because the PE header was found to be
corrupted, or because the PE ﬁle did not contain any sec-
tion marked as executable. After ﬁltering out these cases
we obtained a labeled dataset of 2,357 code sections, 2,229
extracted from non-packed benign and 128 extracted from
non-packed malware. We randomly split this dataset in two
parts while maintaining the proportions between the two
classes (malware and benign). We used 80% of the dataset
for training the classiﬁer, and 20% for testing. The results
are reported in Table 2 for a detection threshold equal to 0.5.
We evaluated the classiﬁcation accuracy of C2 in a way
similar to C1. We ran our dynamic unpacker on the entire
dataset of malware (MDset) and on the dataset of packed
benign (PBDset). We ﬁrst considered bpage dumps of the
last layer of unpacking (see Section 2.2). We obtained 3,856
bpage dumps from malware samples and 169 bpage dumps
from the packed benign. Given this dataset of labeled (mal-
ware or benign) bpage dumps, we randomly split (main-
taining the proportion between the two classes) in two parts.
We used 80% of the dataset for training purposes and the re-
maining 20% for testing. The results are reported in Table 2,
second row. The third row in Table 2 reports the results of a
similar experiment using the bbexec dumps of the last layer
of unpacking (see Section 2.2), instead of the bpage dumps.
It is easy to see that using bpage dumps provides better re-
307307
sults, therefore in the following we only consider the results
obtained using bpage dumps.
The average time needed for the classiﬁcation of an exe-
cutable by module C was 0.032 seconds.
3.3 Validation of McBoost
In order to validate the ability of our McBoost system
to correctly detect and rank previously unknown malicious
executables, we randomly chose 80% of the patterns in the
labeled datasets PMDset, NPMDset, PBDset, and NPBD-
set for training the single classiﬁers in our system (i.e., A1,
A2, A3, C1, and C2). We then used the remaining 20% of
these datasets plus the entire OMDset for testing. Overall,
the test dataset contained 3,830 malware and 503 benign ex-
ecutables. Module A classiﬁed 2,471 of these executables
as packed, and they were sent to the unpacker for extract-
ing the hidden code. The remaining 1,862 executables were
classiﬁed as non-packed. The unpacker was able to extract
the hidden code from 1,441 out of 2,471 packed executables
(58.3%).
Therefore, 1,862 executables were sent to module C1,
1,441 were sent to C2 (i.e., 3,303 executables were sent to
module C, in total), and 1,030 where stored in the list of
(likely) “heavily” packed executables that need manual in-
spection. Among these 1,030 executables, 8 were packed
benign and the remaining 1,022 were malware. We found
that 613 out of these 1,022 malware caused an application
crash during the unpacking process, whereas 60 of them
generated a “non-win32 application” error message (likely
because of a corrupted PE header).
The results of the classiﬁcation of the 3,303 executables
sent to modules C (either to C1 or C2) are reported in Ta-
ble 3. Table 3 reports the values of detection threshold, false
positives, detection rate and accuracy for signiﬁcant points
on the ROC curve. The Area Under the ROC curve (AUC)
is equal to 0.977. It is also worth noting that if we set the de-
tection threshold to 0.902, McBoost is still able to correctly
detect 61.7% of the malware with no false positives, i.e., no
benign executable will be considered for further (possibly
manual) analysis. On the other hand, if we are willing to
accept some false positives, i.e., if we can afford to perform
a detailed, manual analysis of more executables, but we do
not want many false negatives because we are not willing
to miss many unknown malware, we can set the threshold
to 0.007 and obtain 99.3% detection rate with 27% of false
positives.
The average time needed for classifying an executable
found to be non-packed by module A was around 1.06 sec-
onds in average (1.025 sec. for module A and 0.032 sec. for
module C), whereas the average time needed to classify an
executable sent to the unpacker was around 4.7 minutes for
malware and 5.6 minutes for benign executables.
Threshold
False Positive Rate
Detection Rate
Accuracy
0.902
0.686
0.500
0.284
0.126
0.029
0.007
0
0.010
0.025
0.050
0.100
0.200
0.270
0.617
0.836
0.856
0.881
0.916
0.980
0.993
0.673
0.859
0.873
0.891
0.913
0.953
0.954
Table 3: Signiﬁcant values of the trade-off between false positives,
detection rate, and accuracy for different values of the detection
threshold of McBoost. The AUC is 0.977.
Classiﬁer % Unpacked
McBoost