key. With an incorrect guess for part of the key the sum of all differ-
ence of averages along the trace would converge to zero while for
a correct guess this converges to a non-zero value. The model that
Kocher et al. [37] used assumes that each individual bit of an inter-
mediate value contributed to the power consumption of the device
such that (with enough traces) it could be revealed. By extending
the same idea to the power consumption of a register, we arrive at
the Hamming weight model. This model states that the consumed
power is proportional to the number of bits that are set [43].
In another type of attack, called Correlation Power Analysis
(CPA), Brier et al. [10] used the correlation coefficient as a side-
channel distinguisher, i.e. the statistical method used for the key
recovery. CPA allows an attacker to recover parts of a key that
is used in a cipher by using a known plaintext attack: samples
measured using a single probe are correlated against a synthetic
power value that is generated from an intermediate value calculated
for all values that a subkey can take. Commonly, the power model
used for CPA is either Hamming weight or Hamming distance. In
the Hamming distance model the consumed power is proportional
to the number of different bits between two intermediate values.
Such leakage can occur in practice when an intermediate value
stored in a register is overwritten with another value.
2.2 Side-Channel Leakage Assessment
Side-channel leakage assessment measures how vulnerable a device
is to side-channel attacks. This cannot be an exhaustive assessment,
as it is impossible to try all possible attacks on a device. However,
such assessment is still valuable to the manufacturers of secure
devices as they can benchmark a level of security of devices during
the design and manufacturing process.
In side-channel leakage assessment, the main question we try to
answer is whether the evaluated device shows significant leakage.
Therefore, a device must show statistically significant leakage to
be classified as insecure. Standards such as International Standard
ISO/IEC 17825:2016(E) [34] build on a methodology called Test
Vector Leakage Assessment (TVLA) that was initially presented
by Goodwill et al. [31]. The TVLA methodology uses Welch’s 𝑡-
test [66] to detect statistical differences between sample distribu-
tions that are measured when the device processes different inputs.
Two main test configurations are specified: the fixed vs. random
configuration and the fixed vs. fixed configuration. The reason for
calculating such differences is that a protected cipher implementa-
tion should not be emitting any information that would let an eval-
uator differentiate the data it processes. If the calculated difference
is statistically insignificant, the device is regarded as side-channel
free in the context that it was tested on. It has been demonstrated
that the results of 𝑡-tests should not be misinterpreted as a single
test that decides if a device is secure or not [64]. Specifically, the
result only suggests that the 𝑡-test failed to find leakage for the
specific fixed inputs used and number of traces collected from the
device. For different fixed input values or for a greater number of
traces significant leakage could be observable.
Welch’s 𝑡-test defines a statistic called the 𝑡-value which is calcu-
lated from the means (𝑋 1 and 𝑋 2) and variances (𝑠2
2) of dis-
tributions of collected traces at a given sample point. The 𝑡-statistic
follows a Student’s 𝑡-distribution with 𝑣 degrees of freedom. Given
the number of samples in each distribution as 𝑛1 and 𝑛2, the 𝑡-value
(𝑡) and degree of freedom (𝑣) are calculated as:
1 and 𝑠2
(cid:18) 𝑠2
(cid:18) 𝑠2
(cid:19)2
𝑛1−1 +
1
𝑛1
1
𝑛1
+ 𝑠2
2
𝑛2
√︂ 𝑠2
𝑋 1 − 𝑋 2
+ 𝑠2
2
𝑛2
1
𝑛1
𝑡 =
and
𝑣 =
(cid:19)2
(cid:18) 𝑠2
(cid:19)2
2
𝑛2
𝑛2−1
.
The 𝑡-value tells us how significantly different the two distri-
butions are. The hypothesis that these two distributions originate
from the same population needs to be rejected with some given
level of confidence to show that they are different. This process is
known as hypothesis testing. Hypothesis testing is the scientific
method of ruling out hypotheses by rejecting them based on signif-
icant evidence against them. The null hypothesis is the hypothesis
that we assume to be correct by default. In TVLA, we assume that
the device is not leaky until evidence, such as a significant 𝑡-value
from the 𝑡-test rejects it in favour of the alternative hypothesis. The
alternative hypothesis here states that the two sample distributions
are statistically different, which implies that the considered device
is leaky. The threshold value of 4.5 for significant leakage is chosen
at a significance level (𝛼) of 0.00001 under the assumptions that
𝑠1 ≈ 𝑠2 and 𝑛1 ≈ 𝑛2, such that the total number of traces (𝑛1 + 𝑛2)
is greater than 1,000 [57].
Using naive methods to compute 𝑡-values may result in numeri-
cal errors due to cancellation effects [12]. Schneider and Moradi [57]
demonstrated computational improvements to overcome such is-
sues. They also suggested online calculation of 𝑡-values in a single
pass, speeding up the calculations compared to more naive methods.
Another common concern with the evaluation of masked imple-
mentations is the typical 𝑡-test threshold of 4.5. This value assumes
Session 3A: Side Channel CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea687a single independent 𝑡-test. This threshold value is inadequate for
large numbers of sample points, as the possibility of false positives
(i.e. classification of leakage at sample points as significant when
there is no actual leakage) increases due to the increased number of
tests. Ding et al. [22] discussed this further and proposes a method
to increase the 𝑡-test threshold according to the degree of freedom
of the 𝑡-test [57] and number of samples.
2.3 Masking Techniques and Higher-Order
Side-Channel Attacks
To protect ciphers against side-channel attacks, a technique called
masking [13, 43] has been proposed. With masking, a sensitive
intermediate value is split into multiple parts by using additional
randomness. The additional random values are referred to as masks
and the values that the original value is split into are referred to as
shares. Depending on the order of masking, the number of shares
increases. For example, in a 𝑑th order masking scheme there are
𝑑 + 1 shares in use. Only when all of the shares are combined, the
original value can be revealed.
Since masked implementations are secure against traditional
attacks such as DPA and CPA, these attacks have been generalised
to overcome masking by exploiting several leakage points simulta-
neously. Generally, a (𝑑 + 1)th order attack aims at breaking a 𝑑th
order masked implementation. Such attacks first combine leakage
occurring in 𝑑+1 intermediate operations and then a classical attack
such as CPA can be applied to recover the key. By increasing the
number of shares, an implementer can increase the work that is
required for an attack exponentially [13].
In particular, Ishai et al. [35] show that a masked implementation
with 𝑑 + 1 shares is secure against side-channel attacks in the 𝑑-
probing model. The 𝑑-probing model considers an adversary that
can only learn up to 𝑑 intermediate values that are produced during
the cryptographic computation. The model is usually considered a
good approximation for modelling higher-order attacks.
Even though masking techniques can be theoretically secure
against wide-range of side-channel attacks, many practical effects,
such as glitches [14, 41] or transitional effects [3] that can void the
countermeasure and still leak the secret information. In such cases,
𝑑th order implementations reveal their secret values at 𝑑th or lower
order analysis. Renauld et al. [53] attribute this effect to breaching
the Independent Leakage Assumption (ILA), which states that all re-
lated shares should be manipulated independently. Even though the
ILA is assumed in theoretical cryptography, in reality this assump-
tion does not hold due to the way that modern computers work.
For example, to increase performance and reduce manufacturing
costs, modern CPUs reuse many of their internal components with-
out resetting or wiping them. Balasch et al. [3] demonstrated that
transitional effects can be destructive to masked implementations
as they halve the effective order of the analysis required when the
leakage is modelled with a Hamming-distance leakage model. In
the Hamming weight model, only a single intermediate value is
considered as a sample at a sample point. In contrast, the Hamming
distance model uses the bit difference between two intermediate
values for a sample.
To measure the effectiveness of an implemented countermeasure
such as masking, one needs to look into the leakage assessment of
cryptographic devices.
2.4 Higher-Order Side-Channel Leakage
Assessment
As discussed before, increasing the number of shares significantly
increases the attack complexity, and information from multiple
samples needs to be combined to reveal leakage of higher-order
implementations.
In contrast to univariate analysis, where each sample point is
analysed independently of other points, higher-order analysis takes
into account the joint leakage of two or more sample points. This is
similar to using multiple probes with respect to the model of Ishai
et al. [35]. A combination function is typically used to combine
mean-centered samples, and leakage assessment is then carried on
the resulting combination. Following Prouff et al. [51], we choose
the ‘product of samples’ combination function, In case of multi-
variate 𝑡-tests, the result of the combination is used as input to a
first-order 𝑡-test and analysed similar to the univariate case [57].
Let us consider a set of 𝑛 side-channel measurements 𝑇𝑖, 0 ≤ 𝑖 <
𝑛, which are known as traces. Each trace contains 𝑚 sample points
denoted as 𝑡 ( 𝑗)
, for 0 ≤ 𝑗 < 𝑚 with sample means denoted by 𝜇( 𝑗),
Then the mean centered product of a given subset of sample points
J, is given by:
𝑖
(cid:16)𝑡 ( 𝑗)
𝑖 − 𝜇( 𝑗)(cid:17) .

𝑗 ∈J
𝐶𝑖 =
(1)
When |J| = 2 the combinations generated are called bivariate and
when |J| = 3 they are called trivariate.
Usually we need to consider all possible subsets J in a given
trace 𝑇𝑖 = 𝑡 (0)
to detect the leakage using 𝑡-test. There-
fore, the complexity increase from using this approach higher-order
small values of |J|.
leakage assessment is by a factor of(cid:0) 𝑚|J|(cid:1), which is exponential for
, . . . 𝑡 (𝑚−1)
𝑖
𝑖
2.5 Leakage Emulators and Automatic
Countermeasures
Due to the high costs associated with evaluations that use real
devices, implementers of cryptographic code are inclined to use
emulators to determine leakage of a device [11]. The first use of such
an emulator is evidenced within the PINPAS project [21], which
had as the goal to emulate power analysis leakage in Java cards.
The most accurate method to emulate leakage is circuit-level em-
ulation. While accurate, it is also very slow due to the very realistic
reproduction of internal effects. Earlier generations that emulate
leakage for software implementations used the cipher source code
written in an high-level language [54, 65]. However, such imple-
mentations are inadequate for detecting leakage stemming from
breaches of the ILA. In addition, compilation can also introduce
breaches of ILA. Consequently, recent leakage emulators tend to use
machine code as input rather than high-level source code [18, 42, 49].
Papagiannopoulos and Veshchikov [49] developed an automated
methodology for detecting violations of the ILA in AVR assembly.
Session 3A: Side Channel CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea688They investigate the effects of the ILA violations on an AVR micro-
controller, ATMega163. By enforcing the ILA, the authors produce
a first-order secure S-box for the RECTANGLE [68] cipher.
With Coco [30], it is possible to formally verify a masked im-
plementation down to the gate-level when the netlist of the CPU
is available. A major difference between the construction of other
leakage emulators and Coco is that Coco uses a software tool called
Verilator to convert Verilog hardware descriptions of the CPU into
C++. This enables the construction of a detailed emulator and of-
fers fine-grained information about the execution. It collects power
information for each gate and then uses a SAT-solver to find the
leaky gates. While Coco finds the exact gates that are contributing
to the leakage, it does not provide an automated fixing mechanism.
McCann et al. [42] demonstrated an emulator named Elmo that
emulates leakage based on machine instructions. The emulation
uses a statistical model that is profiled using real traces. This makes
it specific for the device it was profiled on. Elmo currently supports
ARM Cortex-M0 and ARM Cortex-M4 processors.
The recently introduced Rosita [61] aims to automate the pro-
cess of producing masked first-order implementations. Rosita uses
leakage information from an improved version of Elmo [42], which
the authors call Elmo*, to emulate the power consumption of the
target device running the software. It then uses TVLA [31] on the
emulated traces to detect instructions that leak information. When
leakage is detected, Rosita performs root cause analysis to identify
the cause of the leakage. Specifically, it performs 𝑡-test analysis on
emulated traces of each of the components of the Elmo* model,