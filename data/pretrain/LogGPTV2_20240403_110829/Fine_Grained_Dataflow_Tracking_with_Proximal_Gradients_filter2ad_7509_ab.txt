{1..n} that deﬁne how Pi modiﬁes each variable in the
program state.
Pi(x) =(cid:2)fi,1(x), fi,2(x), ..., fi,n(x)(cid:3)
(x;v)(cid:3)
i(x;v) =(cid:2)prox0
fi,1(x;v), ...,prox0
ij in P 0
fi,n
i using the proximal directional
We evaluate each f0
derivative (Eq. 8):
P 0
To compose derivatives for a given operation Pi from
the previous operation Pi−1, we individually compose the
derivatives of each fij in Pi from the previous operation
Pi−1:
(Pi ◦ Pi−1)0(x;v) =
(cid:2)(fi,1 ◦ Pi−1)0(x;v), ...,(fi,n ◦ Pi−1)0(x;v)(cid:3)
i−1(x;v)(cid:1)
(cid:0)Pi−1(x); P 0
where each (fij ◦ Pi−1)0(x;v) is deﬁned based on the
directional derivative chain rule in Eq. 5:
(fij ◦ Pi−1)0(x;v) = f0
(10)
ij
1614    30th USENIX Security Symposium
USENIX Association
50110K	=	5501K	=	510(a) Sample with derivative
dx/dinput=2.
(b) Compute min/max f0(x)
from samples.
Figure 4: Derivative sampling procedure on an x%4 op-
eration where the x derivative wrt. input dx/dinput=2.
Samples are ﬁrst collected at intervals of 2 and then
used to compute the max/min directional derivative.
(a) Composition of mul 2 with
mod 4. When dx/dxin=2,
dy/dx=1.
(b) Composition of mul 4 with
mod 4. When dx/dxin=4,
dy/dx=0.
Figure 5: Proximal Derivative evaluation on composi-
tion of a mul and mod operation at xin=0, with samples
in red. The step size for the proximal derivative on
x%4 is determined by the derivative dx/dxin. In sub-
ﬁgure (b), when xin is ﬁrst multiplied by 4, dx/dxin=4
and the sample step size for x%4 is 4. This causes the
proximal derivative to evaluate to 0, which correctly
indicates there is no dataﬂow over x%4 after mul by 4.
Using the chain rule from Eq. 10, we can compute a
directional derivative for each ﬁnal state of the program
P by chaining derivatives of the individual operations.
P 0(x;v) = (PN ◦ PN−1 ◦···◦ P2 ◦ P1)0(x;v)
We then compute the proximal gradient using Eq. 9 for
each program state by combining derivatives for a set of
direction vectors represented by a matrix V:
P 0(x;V) =(cid:2)P 0(x;v1), P 0(x;v2), ..., P 0(x;vp)(cid:3)
This is the same approach used in Automatic Diﬀer-
entiation, but extended to discrete functions and gener-
alized gradients. This chained gradient approximation is
designed to be error-free for all locally Lipschitz convex
functions as well as some locally Lipschitz non-convex
functions that meet the requirements for the non-smooth
chain rule (e.g., monotonicity).
3.2 Proximal Derivative Evaluation
When applying proximal directional derivatives in prac-
tice, we make two modiﬁcations to the proximal direc-
Algorithm 1 Proximal Derivative computation
on a non-smooth operation.
Input:
op ← program operation
dx1, dx2 ← x1, x2 components of v
x1, x2 ← operation inputs
N ← maximum samples
return dy ← 0
1: if dx1 = 0 and dx2 = 0 then
2:
3: end if
4: y ← op(x1, x2)
5: initialize size N arrays S and Scost
6: for i = 1 to N do
x1i ← x1+ dx1∗ i
7:
x2i ← x2+ dx2∗ i
8:
yi ← op(x1i, x2i)
9:
distance2
10:
add −|y − yi|+ 1
11:
add yi to S array
12:
13: end for
14: iprox ← index of min sample in Scost
15: yprox ← recover sample iprox from S
16: return dy ← (yprox − y)/iprox
2 distance2
i ← (x1− x1i)2 +(x2− x2i)2
i to Scost array
tional derivative deﬁned in Eq. 8 to model program
behavior more closely.
First, we only consider the inputs to the operation
itself in a function f∗ : X k → X, k ∈ {1..n} and associated
v∗, where k is the number of inputs to the operation. To
simplify notation we drop the ∗, and for the rest of the
paper assume f and v to refer to their k dimensional
variants on the current operation.
Second, we modify the proximal operator to select a
nearby point that maximizes absolute change in f, which
we denote |δf|:
(cid:0)−|f(x)− f(y)|+ 1
(cid:1)
2||x− y||2
2
(11)
prox|δf|(x;v) = argmin
y
where y = x+ t∗v : t ∈ N, y ∈ X n
This modiﬁed proximal operator selects the largest gen-
eralized derivative of f based on either the maximum
or minimum of f in the direction v (these correspond
to the supremum or inﬁnum of a generalized derivative).
Accounting for both is necessary in dataﬂow analysis to
avoid missing possible dataﬂows.
Proximal Derivative Algorithm. Algorithm 1 de-
ﬁnes how we compute the proximal derivative for an
operation op that has two input variables x1 and x2,
and returns an output y. We denote the derivatives of
the inputs x1 and x2 and output y to be dx1, dx2, and
dy, where dx1 and dx2 are components of v, and dy is
computed using the proximal derivative with a maximum
USENIX Association
30th USENIX Security Symposium    1615
f(x)samples (dx = 2)f(x)=x%4;x=0f(x)f(x)=x%4;x=0max/min f’(x)x=xin*2; y=x%4x=xin*4; y=x%4sample budget N. The same algorithm can be applied
to operations with any number of inputs from 1 to n by
adjusting the number input variables. Figure 4 shows
an example of the proximal derivative procedure being
applied to a x%4 operation.
We observed that when the proximal gradient is
nonzero, it almost always uses a point within a few
samples of the current point due to rapid increase of
the proximal cost term distance2 in the proximal opera-
tor. Therefore, we set N to a small constant (5 in our
evaluation), and evaluate the proximal derivative in that
range.
Figure 5 gives an example of evaluating Algorithm 1
on a non-smooth operation y = x%4. When the input is
multiplied by 2 as in Figure 5a, the algorithm samples
at intervals of 2 and evaluates a derivative of 1 based on
the maximum absolute diﬀerence (|δf|) measure. How-
ever, when the input is multiplied by 4 as in Figure 5b,
the algorithm samples at intervals of 4 and evaluates
a derivative of 0 because the samples are all 0. This 0
derivative indicates that the composition of functions
x=xin*4; y=x%4 will always have the same output and
therefore has no dataﬂow.
3.3 Derivative Propagation Rules
We deﬁne a general framework for propagating deriva-
tives over 5 abstract classes of operations that need to be
handled in program analysis: ﬂoating point operations,
integer valued operations, loading and storing variables,
branching, and function calls to external libraries.
1. Floating point operations: We treat ﬂoating
point operations as continuous functions and apply
the standard chain rule (Eq. 10) with their analytic
derivatives. If there are any potentially non-smooth
ﬂoating point operations, such as ﬂoating point mod-
ulo, or typecasting between ﬂoating point types, we
use proximal derivatives.
2. Integer operations: We consider any boolean or
typecasting involving integers to be integer oper-
ations, as well as any arithmetic, bit shifting, or
modulo on integer or pointer types. In general we
use proximal derivatives on all integer operations,
although in some cases such as arithmetic addition
and multiplication we use analytic derivatives as an
optional optimization.
3. Load and Store: When variables are stored or
loaded from memory, their associated derivatives
are also stored or loaded (our implementation uses
shadow memory to track derivatives in memory, al-
though any associative tracking mechanism could
be used). If the memory address passed to a load
instruction has a nonzero derivative, we set the
derivative of the loaded variable to 1.0 if it does
not already have a nonzero derivative. This is a
simplifying approximation that may lead errors in
evaluating the proximal gradient. However, we note
that proximal derivatives on load operations can po-
tentially be evaluated by sampling adjacent memory
locations. We leave this to future work.
4. Branches: When dynamically computing deriva-
tives, we can only reason about the derivative on
the current execution path. If computing a deriva-
tive would require sampling an alternate execution
path, we instead set that derivative to 0. Therefore,
when a branch is encountered, we set any deriva-
tives to 0 that are based on samples that would
change the branch condition. This approach may
miss some parts of the gradient but ensures we do
not propagate incorrect derivatives. We note that
sampling across multiple execution paths when han-
dling branches could yield more accurate proximal
derivatives and reason about control ﬂow data ﬂows
(i.e. implicit data ﬂows), we leave this to future
work.
5. External Library Functions: Provided they do
not have side eﬀects, derivatives on external library
function calls can be computed using proximal
derivatives, while functions with side eﬀects must be
handled on a case by case basis. When an external
function overwrites a buﬀer, we also clear the stored
derivatives associated with that buﬀer.
3.4 Program Gradient as Dataﬂow
To use gradients as a measure of dataﬂow, we compute
gradient between a set of user deﬁned sources and sinks.
We set the initial vectors in V so that each vector is
all 0s except for an initial derivative on each source of
+1 or −1 . We then execute the program and propa-
gate the derivatives over each operation with the chain
rule and derivatives deﬁned in Algorithm 1. While the
program is executing we record derivatives at each sink,
and accumulate the gradient on each sink from all the
sources. Cumulatively, the gradients on all sinks form
the Jacobian J between sources and sinks.
Algorithm 2 formally describes the process for comput-
ing the gradients from a set of sources to each designated
sink in program. The returned Jacobian J contains the
gradients of each sink based on the largest derivative
propagated to it from each source (sinks may record
multiple derivatives from a single source if, for example,
the sink is in a loop).
1616    30th USENIX Security Symposium
USENIX Association
Algorithm 2 Program Gradient Evaluation.
P ← program under analysis
Input:
x ← program input
Sources ← n dataﬂow sources
Sinks ← m dataﬂow sinks
src and v−
1: initialize V to empty set {}
2: initialize J n× m matrix to 0s
3: for src in Sources do
src ← [dsrc = 1,otherwise 0]
v+
4:
src ← [dsrc = −1,otherwise 0]
v−
5:
add v+
6:
7: end for
8: Execute P on input x, tracking P 0(x;V)
9: for sink in Sinks do
10:
11:
12:
end if
13:
end for
14:
15: end for
16: return J
(cid:12)(cid:12) >(cid:12)(cid:12)J [src, sink](cid:12)(cid:12) then
J [src, sink] ← dsink
for each recorded dsink
(cid:12)(cid:12) dsink
dsrc do
if
dsrc
src to V
dsrc
4 Implementation
We implement PGA as a new sanitizer in the LLVM
framework [27] called Gradient Sanitizer (grsan). We
use LLVM because it allows us to instrument a program
during compilation after it has been converted to LLVM’s
intermediate representation. This means that grsan can
be used to instrument any program written in a language
supported by LLVM, and incurs lower runtime overhead
than binary instrumentation frameworks such as PIN or
Valgrind [1,32]. However, we note that PGA could also
be implemented in a binary instrumentation framework
to facilitate analysis in cases where source code is not
available.
Overall Architecture. We base grsan on LLVM’s
taint
implementation, DataFlowSanitizer
(dfsan), which uses shadow memory to track taint
labels. For each byte of application memory, there are
two corresponding bytes of shadow memory that store
the taint label for that byte.
tracking
We modify dfsan in the following two ways: First,
we add additional metadata associated with each label
that stores the gradient information, which is stored in
a separate table as shown in Figure 6. Each label in the
shadow memory is associated with a distinct derivative
value in the gradient table. The 0 label is reserved for 0
derivative, and any shadow memory lookup on a constant
or unlabeled variable returns label 0.
Second, we change the dataﬂow propagation rules to
compute gradients over each operation. Figure 6 shows an
Figure 6: Grsan architecture illustrating how proximal
gradients are propagated.
example of how the grsan instrumentation works. Given
an operation y=2*x, the instrumentation ﬁrst looks up
the derivative for each input, 2 and x, from shadow mem-
ory. If any input has a nonzero derivative, it computes the
derivative for the output y and generates a new shadow
memory label by incrementing the current max label by
1. It then allocates space in the shadow memory and
gradient table and stores the new label and associated
derivative of y.
As an additional optimization, when storing an opera-
tion’s output derivative we ﬁrst compare it to the input
derivatives. If the output derivative is equal to either, we
apply the label of the equivalent input derivative to the
output instead of generating a new label and gradient
table entry. Since many operations do not change the
value of the derivative (e.g. x = x+1;), this signiﬁcantly
reduces the number of distinct labels that need to be
tracked.
In the current implementation, grsan tracks deriva-
tives from a single source at a time, propagating the two
derivatives from the source in parallel. When computing
a gradient over multiple sources (e.g. bytes in an input
ﬁle), we execute the program once for each source. We
intend to extend grsan to support multiple sources in
parallel in future work.
Gradient Propagation Instrumentation. For diﬀer-
entiable operations such as a ﬂoating point multiplication
(fmul), grsan uses the analytical derivative of the oper-
ation. For nondiﬀerentiable operations such as bitwise
And, grsan uses an optimized version of proximal deriva-
tives from Algorithm 1 that returns the ﬁrst nonzero
derivative it encounters when sampling. We found this
approximation picked the same values that the proximal
operator would select and is computationally lighter (i.e.
does not require computing exponents).
We leave most external function calls uninstrumented,
but some operations in glibc are given special instru-
mentation. We set the gradients for any buﬀer overwrit-
ten by fread or memset to 0, and the gradients of buﬀers
copied by memcpy or strcpy are also copied. Type cast-
USENIX Association
30th USENIX Security Symposium    1617
y = 2 * x  y_shad = alloc_shadow() y_grad = gradient(2 * x)    Application      Memory Gradient  Table Application Code Shadow MemoryInstrumentation Code Library
zlib-1.2.11
libjpeg-9c
mupdf-1.14.0
libxml2-2.9.7
binutils-2.30
Test Command
minigzip -d
djpeg
mutool show
xmllint
objdump -xD
strip
size
File Format
SLOC
GZ/ZIP
3228
8,857
JPEG
123,562 PDF
73,920
XML