similarity((cid:104)v1,··· ,v2k+3(cid:105),(cid:104)v(cid:48)
1,··· ,v(cid:48)
2k+3(cid:105)) =
2k+3
∑
i=1
Ni × vi × v(cid:48)
i.
That is, if we encounter an out-of-vocabulary word that is not
obtained in the training phase (i.e., ατ(t) (cid:54)∈ V ), we transform
it into the most similar word in the dictionary. For integer con-
stants N1,··· ,N2k+3 that represent weights for each feature
vector, we set them to be N1,··· ,N2k  0). We remark that, though there exists a method
called EM algorithm [20, 23] for obtaining locally optimal
λis, we simply set λ1 = 0.9, λ2 = 0.08 and λ3 = 0.02, which
worked fairly well in our case.
4
Implementation
We implemented SMARTEST in OCaml on top of VERIS-
MART [36], an open-sourced veriﬁer for Solidity contracts.
Speciﬁcally, we reused the frontend of VERISMART and its
VC generator for atomic statements, but newly implemented
our symbolic executor for transaction sequences (Section 3.1),
constraint solving optimization (Section 3.1.2), and symbolic
execution with a language model (Section 3.2).
To be practical as much as possible, our VC generator is
implemented in a more sophisticated way than the one de-
ﬁned in Section 3.1. In particular, our VC generator takes
into account statements that follow assertions. For exam-
ple, consider the statements uint x = n - 12; require
(n==10); ... where n is a formal input parameter. In this
case, in order for the input value n to trigger the underﬂow
for n-12, SMARTEST produces a value of 10; while any inte-
ger values from 0 to 11 can trigger the underﬂow, the most
desirable value would be 10, since the effect by the underﬂow
can persist after the transaction. Other extensions include the
following.
Vulnerability Checker. Currently, SMARTEST supports the
detection of six types of security-critical vulnerabilities: in-
teger over/underﬂow, assertion violation, division-by-zero,
ERC20 standard violation, Ether-leaking vulnerability (e.g.,
unauthorized access to transfer), and suicidal vulnerability
(e.g., unauthorized access to selfdestruct). For the ﬁrst
three types of vulnerabilities, we reused the implementation
of VERISMART. We provide detailed explanations on how
USENIX Association
30th USENIX Security Symposium    1369
we detect the rest three types of vulnerabilities (which are
currently not supported by VERISMART) in Appendix C.
Concrete Validator. We implemented our concrete validator
in Python using the Trufﬂe testing framework.4 We use the
validator to conﬁrm true positives (i.e., vulnerable transaction
sequences generated by SMARTEST can violate correspond-
ing safety conditions in concrete execution), thereby reducing
the burden of manual effort for validating analysis results.
Given a set of vulnerable transaction sequences obtained by
running symbolic execution (Section 3) on a contract, our
validator examines the analysis results as follows. First, on a
testnet, we deploy contracts with assertions that are automat-
ically generated for each safety condition deemed violated
in the analysis phase. Next, we check safety conditions in
assertions are falsiﬁed or relevant logging messages are emit-
ted. For leaking and suicidal vulnerabilities, in addition to
checking violations of safety conditions, to further increase
the conﬁdence in our analysis results, we check a positive
amount of Ethers (produced by the symbolic execution) is
indeed transferred to untrusted accounts (Appendix C) and
an analyzed contract is actually deactivated, respectively. In
our experiments (Section 5), we sampled validation results
and manually conﬁrmed that our validator works as intended.
Function Call Analysis. Although we described our ap-
proach for a small subset of Solidity, our implementation
supports most of the features in Solidity, including in-
ternal function calls, inheritance, and structures. However,
SMARTEST currently does not precisely handle external func-
tion calls (i.e., calling functions deﬁned in other contracts).
For example, given a statement o.foo() where o is a con-
tract object, we produce the constraint false to reduce false
positives (i.e., generated vulnerable sequences do not violate
corresponding safety conditions in concrete execution). Also,
given a call statement (e.g., rcv.call.value(amount)()),
we consider Ether-transfer to detect leaking vulnerabilities,
but do not consider behaviors of fallback functions of the
Ether receiver (e.g., rcv).
5 Evaluation
In this section, we evaluate our approach to answer the fol-
lowing research questions.
• How effectively does SMARTEST ﬁnd vulnerable trans-
action sequences? How does it compare to existing tools?
(Section 5.1)
• Is using language models essential for performance?
How does SMARTEST compare to the basic symbolic
execution without language models? (Section 5.2)
• What are the insights we can get from the learned lan-
guage models? (Section 5.3)
4https://www.trufﬂesuite.com/
• Can SMARTEST ﬁnd zero-day bugs from smart contracts
in the wild? (Section 5.4)
5.1 Effectiveness of SMARTEST
We evaluate the vulnerability-ﬁnding ability of SMARTEST in
comparison with ﬁve publicly available tools: MYTHRIL [3],
MANTICORE [30], MAIAN [31], TEETHER [25], and ILF [19].
They are well-known and recently-developed analyzers that
can generate vulnerable transaction sequences. The ﬁrst four
are symbolic executors and the last one is a fuzzer. We com-
pare these tools against ﬁve security-critical vulnerabilities:
integer over/underﬂow, assertion violation, division-by-zero,
Ether-leaking, and suicidal. The ﬁrst three types of vulnera-
bilities are supported by MYTHRIL and MANTICORE. The
leaking (resp., suicidal) vulnerabilities are supported by all
tools (resp., all but TEETHER). We additionally demonstrate
the effectiveness of SMARTEST on ﬁnding ERC20 standard
violations, which is not supported by the ﬁve.
Benchmark Setup. One important issue when using machine
learning approaches is about how to obtain sufﬁcient amounts
of useful data (i.e., sufﬁciently many vulnerable smart con-
tracts). For vulnerabilities related to arithmetic properties
(integer over/underﬂow, assertion violation, division-by-zero,
ERC20 violation), we used smart contracts with known arith-
metic vulnerabilities (e.g., integer overﬂows) reported in CVE.
Out of the 487 smart contracts with arithmetic vulnerabilities,
we used 443 contracts after we deduplicate contracts whose
names of the root contracts and the code are exactly equivalent
to previously collected ones. On average, the deduplicated
contracts consist of 229 lines. We note that, although CVE
dataset is mostly comprised of smart contracts with known
integer over/underﬂow vulnerabilities, we additionally tar-
geted three more kinds of vulnerabilities related to arithmetic
properties, in order to compare the tools from more diverse
perspectives.
We note that contracts in CVE dataset have several typi-
cal CVE-reported vulnerability patterns with some contract-
speciﬁc variations (e.g., implementations of vulnerable func-
tions, contract sizes), which thus can be useful enough to
compare the effectiveness of tools. The vulnerability patterns
include: over/underﬂows due to logical ﬂaws in guard expres-
sions (e.g., CVE-2018-12025), missing overﬂow protection
statements in mintToken functions (e.g., CVE-2018-13085),
and missing guard statements for preventing overﬂows in
calculating the amount of tokens to be sent when sending
tokens to multiple accounts (e.g., CVE-2018-10299—well-
known for batchOverﬂow). The most of the benchmarks are
ERC20 token contracts, reﬂecting the prevalence of them in
the Ethereum blockchain.
For Ether-leaking and suicidal vulnerabilities, we used 104
smart contracts (90 contracts with leaking vulnerabilities and
53 contracts with suicidal vulnerabilities, Table 3) labelled
with vulnerable program points (explained shortly). Out of
1370    30th USENIX Security Symposium
USENIX Association
104, 50 contracts came from a publicly available vulnerability
database, called SmartBugs [14], for Solidity smart contracts.
Speciﬁcally, 8 out of 50 are mostly small test contracts manu-
ally collected by the authors of [14] from public repositories
(e.g., SWC registry). The rest 42 contracts are the ones found
by MAIAN [31]. More concretely, the authors of [14] ran MA-
IAN on deployed contracts (> 10,000), where MAIAN ﬂagged
44 contracts in total (for Ether-leaking and suicidal vulner-
abilities) out of them; we excluded 2 out of 44 as 2 were
obtained by running against non-main contracts. We assumed
contracts ﬂagged by MAIAN have real vulnerabilities, since
MAIAN internally performs concrete validation to conﬁrm
true positives [31]. One typical vulnerability pattern in con-
tracts found by MAIAN is an improper access control due to a
mistakenly named constructor (e.g., Pattern 1 in Section 5.4).
For more extensive evaluation, we additionally collected the
rest 54 (=104-50) contracts by manually injecting realistic
vulnerabilities into 21 randomly selected real-world contracts
deployed on blockchain. On average, the contracts (without
duplicated contracts) in Leaking-Suicidal datatset consist of
335 lines.
We explain our constructed benchmarks in more detail. To
mimic real vulnerabilities as possible, we tried not to exces-
sively alter original code. Speciﬁcally, we devised and applied
3 mutation patterns in Appendix D, where mutation opera-
tions are negation or removal (rather than insertion of code)
for preferring small changes. We also considered variations
in program sizes; the 21 seed contracts consist of 399 lines
on average, including 7 relative large contracts (> 500 lines).
Similar to those in CVE dataset, these contracts are mostly
token contracts, including a few other types of contracts (e.g.,
game).
Ground truths for Ether-leaking and suicidal vulnerabili-
ties were manually constructed by the authors of [14] (ones
that were provided for the 8 test contracts) and us (the rest,
including ones unexpectedly found by tools used in our exper-
iments). Although our ground truths for vulnerabilities may
not be exhaustive despite our signiﬁcant effort, we believe
they will be useful for evaluating other analysis tools as well.
Although we tried hard in preparing benchmarks for ob-
jective evaluation, the benchmarks may not be perfect; unfor-
tunately, however, we are currently unaware of other proper
public datasets with conﬁrmed vulnerabilities. We discuss the
limitation of our benchmarks in Section 5.5.
Tool Setup. We obtained the latest versions (as of August,
2020) of each tool from public docker images provided by the
developers of these tools (MYTHRIL, MANTICORE, ILF) and
the public GitHub repository (TEETHER). For MAIAN, we
were unable to run MAIAN obtained from its public repository
due to library dependency issues. Instead, we used the docker
image 5 provided by the authors of [14], where they managed
the dependency issues for running MAIAN.
5https://hub.docker.com/r/smartbugs/maian/tags
For a fair comparison, we deactivated checker options that
are irrelevant to vulnerabilities targeted in each experiment,
when related options were available (MYTHRIL, MANTI-
CORE). We ran MAIAN separately for each type of vulnera-
bilities, since it analyzes only one type of vulnerability for
each run. For symbolic executors, we provided an option to
explore transaction sequences up to length 4 when available
(all but TEETHER), to avoid potential disadvantages of each
tool in terms of the number of found vulnerable transaction
sequences. Note that, for SMARTEST, we did not give such a
hint on the transaction depth (Algorithm 1). For MANTICORE,
we provided options for running it within the capacity of our
machine, since MANTICORE creates multiple subprocesses
per tool invocation by default. For all symbolic executors,
we set the analysis timeout to 30 minutes per contract when
timeout option is available (all but MAIAN and TEETHER),
and we set the external timeout to 35 minutes per contract to
ensure the termination. We ran ILF with 100K transactions
until it terminates, which spent 37 minutes on average per
contract. For additional inputs (other than contracts) required
by TEETHER (e.g., the address of the attacker) and ILF (con-
structor argument values), we simply provided random values.
We left remaining options as default for the four tools. For
SMARTEST, we set timeout to 1 minute per Z3 request.
For CVE dataset, we ran each tool with 40 threads on
Ubuntu machine with AMD Ryzen Threadripper 3970X CPU
(3.7 GHz) (32 cores and 64 threads in total) and 62GB of
memory. For the second dataset with leaking and suicidal
vulnerabilities, we ran each tool with 26 threads on Ubuntu
machine with two Intel(R) Xeon(R) E5-2630 v3 (2.40GHz)
CPUs (16 cores and 32 threads in total) and 188GB of mem-
ory. As exceptions, we ran MAIAN and ILF with 2 and 3
threads respectively; we observed MAIAN produced runtime
exceptions with 26 threads and ILF showed substantial CPU
usage (e.g., > 500 %) per tool invocation.
Results. On each of the two datasets, we performed 4-fold
cross validation, a methodology for evaluating the general-
izability of models; we randomly divided each dataset into
4 folds with equal or similar sizes, ran each fold with our
baseline symbolic execution (Section 3.1), and tested on each
fold using a model learned from training sequences that were
obtained from the rest three folds (i.e., each fold is used once
as testing data and three times as training data). For each tool,
following [19], we report numbers whose results on every
fold is summed.
Table 1 provides the results on 443 contracts from CVE for
each tool. The column #G shows the number of vulnerable
transaction sequences found by each tool for each vulnerabil-
ity kind and for each transaction depth. The column #V means
the number of transaction sequences that are automatically
conﬁrmed by our validator; for CVE dataset, we also vali-
dated the outputs of MYTHRIL and MANTICORE using our
validator. The results show that SMARTEST outperformed
MYTHRIL and MANTICORE in terms of ﬁnding vulnerable
USENIX Association
30th USENIX Security Symposium    1371
Table 1: Test results on 443 contracts with 4-fold cross vali-
dation. #G: the number of vulnerable transaction sequences
generated by each tool. #V: the number of vulnerable trans-
action sequences conﬁrmed by the validator. Each instance
is unique, i.e., each of the instances in each contract is differ-
ent in at least one of the following three things: lines, safety
conditions, and vulnerability types.
Vuln.
Kind
Integer
Over/
Underﬂow
Division
by
Zero
Assertion
Violation
ERC20
Standard
Violation
Tx.
Depth
Total
0
1
2
3
≥ 4
Total
0
1
2
3
≥ 4
Total
0
1
2
3