some of the increments will be lost. Figure 6-9 shows a simple scenario where incrementing a value by 1
done from two threads ends up with result of 1 instead of 2.
Chapter 6: Kernel Mechanisms
160
Figure 6-9: Concurrent increment
The example in figure 6-9 is extremely simplistic. With real CPUs there are other effects to
consider, especially caching, which makes the shown scenario even more likely. CPU caching,
store buffers, and other aspects of modern CPUs are non-trivial topics, well beyond the scope
of this book.
Table 6-2 lists some of the Interlocked functions available for drivers use.
Table 6-2: Some Interlocked functions
Function
Description
InterlockedIncrement /
InterlockedIncrement16 /
InterlockedIncrement64
Atomically increment a 32/16/64 bit integer by one
InterlockedDecrement / 16 / 64
Atomically decrement a 32/16/64 bit integer by one.
InterlockedAdd / InterlockedAdd64
Atomically add one 32/64 bit integer to a variable.
InterlockedExchange / 8 / 16 / 64
Atomically exchange two 32/8/16/64 bit values.
InterlockedCompareExchange / 64 / 128
Atomically compare a variable with a value. If equal
exchange with the provided value and return TRUE;
otherwise, place the current value in the variable and
return FALSE.
Chapter 6: Kernel Mechanisms
161
The InterlockedCompareExchange family of functions are used in lock-free programming,
a programming technique to perform complex atomic operations without using software
objects. This topic is well beyond the scope of this book.
The functions in table 6-2 are also available in user mode, as these are not really functions, but
rather CPU intrinsics - special instructions to the CPU.
Dispatcher Objects
The kernel provides a set of primitives known as Dispatcher Objects, also called Waitable Objects. These
objects have a state, either signaled or non-signaled, where the meaning of signaled and non-signaled
depends on the type of object. They are called “waitable” because a thread can wait on such objects until
they become signaled. While waiting, the thread does not consume CPU cycles as it’s in a Waiting state.
The primary functions used for waiting are KeWaitForSingleObject and KeWaitForMultipleOb-
jects. Their prototypes (with simplified SAL annotations for clarity) are shown below:
NTSTATUS KeWaitForSingleObject (
_In_ PVOID Object,
_In_ KWAIT_REASON WaitReason,
_In_ KPROCESSOR_MODE WaitMode,
_In_ BOOLEAN Alertable,
_In_opt_ PLARGE_INTEGER Timeout);
NTSTATUS KeWaitForMultipleObjects (
_In_ ULONG Count,
_In_reads_(Count) PVOID Object[],
_In_ WAIT_TYPE WaitType,
_In_ KWAIT_REASON WaitReason,
_In_ KPROCESSOR_MODE WaitMode,
_In_ BOOLEAN Alertable,
_In_opt_ PLARGE_INTEGER Timeout,
_Out_opt_ PKWAIT_BLOCK WaitBlockArray);
Here is a rundown of the arguments to these functions:
• Object - specifies the object to wait for. Note these functions work with objects, not handles. If you
have a handle (maybe provided by user mode), call ObReferenceObjectByHandle to get the
pointer to the object.
• WaitReason - specifies the wait reason. The list of wait reasons is pretty long, but drivers should
typically set it to Executive, unless it’s waiting because of a user request, and if so specify
UserRequest.
Chapter 6: Kernel Mechanisms
162
• WaitMode - can be UserMode or KernelMode. Most drivers should specify KernelMode.
• Alertable - indicates if the thread should be in an alertable state during the wait. Alertable state
allows delivering of user mode Asynchronous Procedure Calls (APCs). User mode APCs can be
delivered if wait mode is UserMode. Most drivers should specify FALSE.
• Timeout - specifies the time to wait. If NULL is specified, the wait is indefinite - as long as it takes for
the object to become signaled. The units of this argument are in 100nsec chunks, where a negative
number is relative wait, while a positive number is an absolute wait measured from January 1, 1601
at midnight.
• Count - the number of objects to wait on.
• Object[] - an array of object pointers to wait on.
• WaitType - specifies whether to wait for all object to become signaled at once (WaitAll) or just
one object (WaitAny).
• WaitBlockArray - an array of structures used internally to manage the wait operation. It’s optional if
the number of objects is 
struct Locker {
explicit Locker(TLock& lock) : _lock(lock) {
lock.Lock();
}
~Locker() {
_lock.Unlock();
}
private:
TLock& _lock;
};
With these definitions in place, we can replace the code using the mutex with the following:
Mutex MyMutex;
void Init() {
MyMutex.Init();
}
void DoWork() {
Locker locker(MyMutex);
// access DataHead freely
}
Chapter 6: Kernel Mechanisms
168
Since locking should be done for the shortest time possible, you can use an artificial C/C++
scope containing Locker and the code to execute while the mutex is owned, to acquire the
mutex as late as possible and release it as soon as possible.
With C++ 17 and later, Locker can be used without specifying the type like so:
Locker locker(MyMutex);
Since Visual Studio currently uses C++ 14 as its default language standard, you’ll have to change
that in the project properties under the General node / C++ Language Standard.
We’ll use the same Locker type with other synchronization primitives in subsequent sections.
Abandoned Mutex
A thread that acquires a mutex becomes the mutex owner. The owner thread is the only one that can
release the mutex. What happens to the mutex if the owner thread dies for whatever reason? The mutex
then becomes an abandoned mutex. The kernel explicitly releases the mutex (as no thread can do it) to
prevent a deadlock, so another thread would be able to acquire that mutex normally. However, the returned
value from the next successful wait call is STATUS_ABANDONED rather than STATUS_SUCCESS. A driver
should log such an occurrence, as it frequently indicates a bug.
Other Mutex Functions
Mutexes support a few miscellaneous functions that may be useful at times, mostly for debugging purposes.
KeReadStateMutex returns the current state (recursive count) of the mutex, where 0 means “unowned”:
LONG KeReadStateMutex (_In_ PKMUTEX Mutex);
Just remember that after the call returns, the result may no longer be correct as the mutex state may have
changed because some other thread has acquired or released the mutex before the code gets to examine
the result. The benefit of this function is in debugging scenarios only.
You can get the current mutex owner with a call to KeQueryOwnerMutant (defined in ) as a
CLIENT_ID data structure, containing the thread and process IDs:
VOID KeQueryOwnerMutant (
_In_ PKMUTANT Mutant,
_Out_ PCLIENT_ID ClientId);
Just like with KeReadStateMutex, the returned information may be stale if other threads are doing work
with that mutex.
Chapter 6: Kernel Mechanisms
169
Fast Mutex
A fast mutex is an alternative to the classic mutex, providing better performance. It’s not a dispatcher
object, and so has its own API for acquiring and releasing it. A fast mutex has the following characteristics
compared with a regular mutex:
• A fast mutex cannot be acquired recursively. Doing so causes a deadlock.
• When a fast mutex is acquired, the CPU IRQL is raised to APC_LEVEL (1). This prevents any delivery
of APCs to that thread.
• A fast mutex can only be waited on indefinitely - there is no way to specify a timeout.
Because of the first two bullets above, the fast mutex is slightly faster than a regular mutex. In fact, most
drivers requiring a mutex use a fast mutex unless there is a compelling reason to use a regular mutex.
Don’t use I/O operations while holding on to a fast mutex. I/O completions are delivered with
a special kernel APC, but those are blocked while holding a fast mutex, creating a deadlock.
A fast mutex is initialized by allocating a FAST_MUTEX structure from non-paged memory and calling
ExInitializeFastMutex. Acquiring the mutex is done with ExAcquireFastMutex or ExAcquire-
FastMutexUnsafe (if the current IRQL happens to be APC_LEVEL already). Releasing a fast mutex is
accomplished with ExReleaseFastMutex or ExReleaseFastMutexUnsafe.
Semaphore
The primary goal of a semaphore is to limit something, such as the length of a queue. The semaphore
is initialized with its maximum and initial count (typically set to the maximum value) by calling
KeInitializeSemaphore. While its internal count is greater than zero, the semaphore is signaled. A
thread that calls KeWaitForSingleObject has its wait satisfied, and the semaphore count drops by one.
This continues until the count reaches zero, at which point the semaphore becomes non-signaled.
Semaphores use the KSEMAPHORE structure to hold their state, which must be allocated from non-paged
memory. Here is the definition of KeInitializeSemaphore:
VOID KeInitializeSemaphore (
_Out_ PRKSEMAPHORE Semaphore,
_In_ LONG Count,
// starting count
_In_ LONG Limit);
// maximum count
As an example, imagine a queue of work items managed by the driver. Some threads want to add items to
the queue. Each such thread calls KeWaitForSingleObject to obtain one “count” of the semaphore. As
long as the count is greater than zero, the thread continues and adds an item to the queue, increasing its
length, and semaphore “loses” a count. Some other threads are tasked with processing work items from the
queue. Once a thread removes an item from the queue, it calls KeReleaseSemaphore that increments
the count of the semaphore, moving it to the signaled state again, allowing potentially another thread to
make progress and add a new item to the queue.
KeReleaseSemaphore is defined like so:
Chapter 6: Kernel Mechanisms
170
LONG KeReleaseSemaphore (
_Inout_ PRKSEMAPHORE Semaphore,
_In_ KPRIORITY Increment,