t , ..., oMt
The MOT problem is most commonly solved by the tracking-
by-detection paradigm [18]. An overview of this paradigm
is shown in Fig. 1. Here, a sensor (or group of sensors)
continuously collects the measurement data at every instant of
time t (It). These sensor inputs are sent to a corresponding
DNN-based object detector, such as YoloNet [19] or Faster-
RCNN [20] (labeled as “D” in Fig. 1). Such an object detector
estimates the object’s class and its bbox at every time instant.
The collection of these bbox measurements for all objects
is denoted by Ot = {o1
t denotes the
observations for the ith object at time t.
t }, where oi
t) as the measurement to update the ˆsi
An object tracker (or just tracker) tracks the changes in the
position of the bboxes over successive sensor measurements.
Each detected object is associated with a unique tracker, where
a tracker is a Kalman ﬁlter [21] (KF) that maintains the
state si for the ith object. Each object detected at time t is
associated with either an existing object tracker or a new
object tracker, initialized for that object. Such association of
a detected object with existing trackers (from time t − 1) is
formulated as a bipartite matching problem, which is solved
using the Hungarian matching algorithm [22] (shown as “M”
in the ﬁgure). “M” uses the overlap, in terms of IoU3, between
the detected bboxes at time t (i.e., the output of “D”) and the
bboxes predicted by the trackers (Kalman ﬁlter) of the existing
objects to ﬁnd the matching. A KF is used to maintain the
temporal state model of each object (shown as “F*” in the
ﬁgure), which operates in a recursive predict-update loop: the
predict step estimates the current object state according to a
motion model, and the update step takes the detection results
(oi
t state. That is, the
KF uses a series of noisy measurements observed over time
and produces estimates of an object state that tend to be more
accurate than those based on a single measurement alone. KF
is used to address the following challenges:
• Sensor inputs are captured at discrete times (i.e., t, t+1, . . . ).
Depending on the speed and acceleration, the object may
have moved between those discrete time intervals. Motion
models associated with KFs predict the new state of tracked
objects from time-step t − 1 to t.
• State-of-the-art object detectors are inherently noisy [19],
[20] (i.e., bounding box estimates are approximate measure-
ments of the ground truth), and that can corrupt the object
trajectory estimation (i.e., velocity, acceleration, heading).
Hence, the perception system uses KFs to compensate for
the noise by using Gaussian noise models [22].
Finally, a transformation operation (shown as “T” in the
ﬁgure) estimates the position, velocity, and acceleration for each
detected object by using ˆSt. The transformed measurements
are then fused with other sensor measurements (e.g., from
LiDAR) to get world state Wt.
C. Safety Model
In this paper, we use the AV safety model provided by Jha
et al. [6]. They deﬁne the instantaneous safety criteria of an
AV in terms of the longitudinal (i.e., direction of the vehicle’s
3Intersection over Union (IoU) is a metric for characterizing the accuracy of
predicted bounding boxes (bboxes). It is deﬁned as (area of overlap)/(area of union)
between the ground-truth label of the bbox and the predicted bbox.
115
l
i
a
n
d
u
t
i
g
n
o
L
EV
dstop
dsafe,min
δ
dsafe
TV
Figure 2: Deﬁnition of dstop, dsafe, and δ for lateral and
longitudinal movement of the car. Non-AV vehicles are labeled
as target vehicles (TV).
motion) and lateral (i.e., perpendicular to the direction of the
vehicle’s motion) Cartesian distance travelled by the AV (see
Fig. 2). In this paper, we use only the longitudinal deﬁnition
of the safety model, as our attacks are geared towards driving
scenarios for which that is relevant. Below we reproduce the
deﬁnitions of Jha et al.’s safety model for completeness.
Deﬁnition 3. The stopping distance dstop is deﬁned as the
maximum distance the vehicle will travel before coming to a
complete stop, given the maximum comfortable deceleration.
Deﬁnition 4. The safety envelope dsafe [23], [24] of an AV is
deﬁned as the maximum distance an AV can travel without
colliding with any static or dynamic object.
In our safety model, we compute dsafe whenever an actuation
command is sent to the mechanical components of the vehicle.
ADSs generally set a minimum value of dsafe (i.e., dsafe,min) to
ensure that a human passenger is never uncomfortable about
approaching obstacles.
Deﬁnition 5. The safety potential δ is deﬁned as δ = dsafe −
dstop. An AV is deﬁned to be in a safe state when δ > 0.
Unlike the authors of [6] who use δ ≥ 0 as the safe operation
state, we choose δ ≥ 4 meters because of a limitation in the
simulation environment provided by LGSVL [10] for Apollo [9]
that halts simulations for distances closer than 4 meters.
III. ATTACK OVERVIEW & THREAT MODEL
This section describes the attacker goals, target system, and
defender capabilities.
A. Attacker Goals
The ultimate goal of the attacker is to hijack object
trajectories as perceived by the AV in order to cause a safety
hazard.
To be successful, the attack must:
• Stay stealthy by disguising attacks as noise. To evade
detection of his or her malicious intent, an attacker may want
to disguise malicious actions as events that occur naturally
during driving. In our attack, we hide the data perturbation
initiated by the malware/attacker as sensor noise. As we
show in §VI-A, modern object detectors naturally misclassify
(i.e., identify the object class incorrectly) and misdetect (i.e.,
bounding boxes have zero or < 60% IoU) objects for multiple
time-steps (discussed in §VI-A). Taking advantage of that
small error margin in hiding data perturbations, the attacker
initiates the attack 1) at the most opportune time such that
even if the malicious activity is detected it is too late for
the defender to mitigate the attack consequences, and 2)
for a short duration of time to evade detection from the
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:29:50 UTC from IEEE Xplore.  Restrictions apply. 
intrusion-detection system (IDS) that monitors for spurious
activities [25].
• Situational awareness. Hijacking of the object trajectory
in itself is not sufﬁcient
to cause safety violations or
hazardous driving situations. An attacker must be aware
of the surrounding environment to initiate the attack at the
most opportune time to cause safety hazards (e.g., collisions).
• Attack automation. An attacker can automate the process
of monitoring and identifying the opportune time for an
attack. That way, the adversary only needs to install the
malware instead of manually executing the attack.
B. Threat Model
In this section, we discuss the target system, the existing
defenses, and the attacker’s capabilities.
Target system. The target is the perception system of an AV,
speciﬁcally the object detection, tracking, and sensor fusion
modules. To compensate for the noise in the outputs of the
object detectors, the AV perception system uses temporal
tracking and sensor fusion (i.e., fusion data from multiple
sensors such as LIDAR, RADAR, and cameras). Temporal
tracking and sensor fusion provide an inherent defense against
most if not all existing adversarial attacks on detectors [16].
The critical vulnerable component of the perception system is
a Kalman ﬁlter (KF) (see “F” in §II and Fig. 1). KFs generally
assume that measurement noise follows a zero-mean Gaussian
distribution, which is the case for the locations and sizes of
bboxes produced by the object detectors (described later in
§VI-A). However, that assumption introduces a vulnerability.
The KF becomes ineffective in compensating for the adver-
sarially added noise. We show in this paper that an attacker
can alter the trajectory of a perceived object by adding noise
within one standard deviation of the modeled Gaussian noise.
The challenge in attacking a KF is to maintain a small
attack window (i.e., the number of contiguous time epochs for
which the data are perturbed). When an attacker is injecting a
malicious noise pattern, the attack window must be sufﬁciently
small (1–60 time-steps) such that the defender cannot estimate
the distribution of the injected noise and hence cannot defend
the system against the attack.
What can attackers do? In this paper we intentionally and
explicitly skirt the problem of deﬁning the threat model. Instead,
we focus on what an attacker could do to an AV if he or she
has access to the ADS source code and live camera feeds.
Gain knowledge of internal ADS system. We assume that the
attacker has obtained knowledge of the internal ADS system
by analyzing the architecture and source code of open-source
ADSs, e.g., Apollo [9], [26]. The attacker can also gain access
to the source code through a rogue employee.
Gain access to and modify live camera feed. Recently,
Argus [27] showed the steps to hijack a standalone automotive-
grade Ethernet camera and spoof the camera trafﬁc. The attack
follows a “man-in-the-middle” (MITM) strategy in which an
adversary gains physical access to the camera sensor data and
modiﬁes them (when certain conditions are met). The hack
relied on the fact that the camera trafﬁc is transmitted using
standard (i.e., IEEE 802.1 Audio Video Bridging [28]) but
simple protocols, which do not use encryption because of the
size of the data as well as performance and latency constraints
associated with the transmission. As the camera feed is not
encrypted, the attacker can reassemble packages of a video
frame and decode the JFIF (JPEG File Interchange Format)
payload into an image. Most importantly, since there is no
hash or digital signature checks on the transmitted images,
to prepare for the attack, the attacker can apply a number of
ﬁlters to modify the images in-line without being noticed. The
MITM attack works by using an Ethernet tap device to capture
UDP packets in the Ethernet/RCA link between the camera
and the ADS software. The Ethernet tap captures images and
provides them as the input for attacker-controlled hardware
with purpose-built accelerators, such as NVIDIA EGX, that are
operating outside the domain of the ADS hardware/software.
Optionally, compromise ADS software using secret hardware
implant. To further hide malware and evade detection, an
attacker can install backdoors in the hardware. Injection of
malicious code in hardware-software stacks has been realized
in existing hardware backdoors embedded in CPUs, networking
routers, and other consumer devices [26], [29]. As an AV is
assembled from components supplied by hundreds of vendors
through a sophisticated supply chain, it is reasonable to argue
that individual components such as infotainment systems and
other existing electronic component units (ECUs) could be
modiﬁed to enable secret backdoors [26], [30].
What things can attackers not do? In this work, we
assume that the CAN bus transmitting the control command is
protected/encrypted. Therefore we cannot launch a man-in-the-
middle attack to perturb the control/actuation commands sent
to the mechanical components of the EV.
Defender capabilities. Again, we assume that the CAN bus
transmitting the controller/actuation commands is encrypted.
That assumption is acceptable because many commercial
products utilize such encryption [31]. Moreover, there are
well known IDSs for monitoring CAN bus activity [25], [32].
Therefore, we do not leverage CAN bus vulnerabilities as an
attack vector; instead, our attack exploits vulnerabilities on the
camera’s Ethernet/RCA cable link.
C. Attack Vectors and Injected Attacks
We describe a taxonomy of attack vectors (shown in Fig. 1)
that the attacker can leverage to maximize impact, such as
with an emergency stop or a collision. The attack vectors are
as follows.
a) Move_Out. In this attack, the attacker hijacks the target
object (TO) trajectories to fool the EV into believing that
the TO is moving out of the EV’s lane. A close variant of
this attack is fooling the EV into believing that the target
object is maintaining its lane, whereas in reality the target
object is moving into the EV’s lane. Because of this attack,
the EV will start to accelerate or maintain its speed, causing
it to collide with the target object.
b) Move_In. In this attack, the attacker hijacks the target object
(TO) trajectories to fool the EV into believing that the TO
is moving into the EV’s lane. Because of this attack, the
EV will initiate emergency braking. The emergency braking
maneuver is highly uncomfortable for the passengers of the
EV and may lead to injuries in some cases.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:29:50 UTC from IEEE Xplore.  Restrictions apply. 
116
1. SM chooses 
move-out attack 
vector as  the 
target object is 
in-path in lane 
keep mode.
2. SH identiﬁes 
the current state 
and time optimal 
for attack. 
Figure 3: Steps followed by RoboTack to mount a successful attack, i.e., collision between the EV (blue) and the target object
(red). SM: Scenario matching. SH: Safety hijacking, TH: Trajectory hijacking.
4. TH changed 
object trajectory  
to move-out.
3. TH perturbed perturbed 
the pixels within the bbox 
for k time-steps
(Perturbed pixel shown in 
white).
(c) simulation view
(Perturbed pixels)
(e) Simulation
view (collision)
(f) ADS view
 (collision)
(Hijacked trajectory)
(b) ADS
view
(d) ADS view 
(a) simulation
 view
c) Disappear. In this attack, the attacker hijacks the target
object (TO) trajectories to fool the EV into believing that
the TO has disappeared. The effects of this attack will be
similar to those of the Move_Out attack model.
D. Attack Phases.
The attack progresses in three key phases as follows.
Phase 1. Preparing and deploying the malware. Here the
attacker does the following:
1) Gains access to the ADS source code,
2) Deﬁnes the mapping between the attack vectors (see §III-C)
and the world state (Wt),
3) Trains the "safety hijacker" and ﬁne-tunes the weights
of "trajectory hijacker" (e.g., learns about the maximum
perturbation that can be injected to evade detection) for the
given ADS,
4) Gains access to the target EV camera feeds, and
5) Installs RoboTack on the target EV.
Phase 2. Monitoring the environment. Once our intelligent
malware is deployed, it does the following:
1) Approximately reconstructs the world (Wt) using the
hacked camera sensor data ( 1 in Fig. 1). For simplicity, we
assume that the world state estimated using sensor fusion is
not signiﬁcantly different from the state determined using
only one camera sensor. In our implementation, we only
use ˆSt to carry out the attack instead of relying on data
from all sensors.
2) Identiﬁes the victim target object i (i.e., one of the other
vehicles or pedestrians) for which the trajectory is hijacked.
The target object
to the EV. The
identiﬁcation is done using the safety model as deﬁned
in §II-C (line 9 of algorithm 1).