title:Fuzzing with Code Fragments
author:Christian Holler and
Kim Herzig and
Andreas Zeller
Fuzzing with Code Fragments
Christian Holler
Mozilla Corporation∗
PI:EMAIL
Kim Herzig
Saarland University
PI:EMAIL
Andreas Zeller
Saarland University
PI:EMAIL
Abstract
Fuzz testing is an automated technique providing random
data as input to a software system in the hope to expose
a vulnerability. In order to be effective, the fuzzed input
must be common enough to pass elementary consistency
checks; a JavaScript interpreter, for instance, would only
accept a semantically valid program. On the other hand,
the fuzzed input must be uncommon enough to trigger
exceptional behavior, such as a crash of the interpreter.
The LangFuzz approach resolves this conﬂict by using
a grammar to randomly generate valid programs; the
code fragments, however, partially stem from programs
known to have caused invalid behavior before. LangFuzz
is an effective tool for security testing: Applied on the
Mozilla JavaScript interpreter, it discovered a total of
105 new severe vulnerabilities within three months of
operation (and thus became one of the top security bug
bounty collectors within this period); applied on the PHP
interpreter, it discovered 18 new defects causing crashes.
1
Introduction
Software security issues are risky and expensive.
In 2008, the annual CSI Computer Crime & Security sur-
vey reported an average loss of 289,000 US$ for a single
security incident. Security testing employs a mix of tech-
niques to ﬁnd vulnerabilities in software. One of these
techniques is fuzz testing—a process that automatically
generates random data input. Crashes or unexpected be-
havior point to potential software vulnerabilities.
In web browsers, the JavaScript interpreter is partic-
ularly prone to security issues; in Mozilla Firefox, for
instance, it encompasses the majority of vulnerability
ﬁxes [13]. Hence, one could assume the JavaScript in-
terpreter would make a rewarding target for fuzz test-
ing. The problem, however, is that fuzzed input to a
∗At the time of this study, Christan Holler was writing his master
thesis at Saarland University. He is now employed at Mozilla.
JavaScript interpreter must follow the syntactic rules of
JavaScript. Otherwise, the JavaScript interpreter will re-
ject the input as invalid, and effectively restrict the test-
ing to its lexical and syntactic analysis, never reaching
areas like code transformation, in-time compilation, or
actual execution. To address this issue, fuzzing frame-
works include strategies to model the structure of the de-
sired input data; for fuzz testing a JavaScript interpreter,
this would require a built-in JavaScript grammar.
Surprisingly, the number of fuzzing frameworks that
generate test inputs on grammar basis is very limited [7,
17, 22]. For JavaScript, jsfunfuzz [17] is amongst the
most popular fuzzing tools, having discovered more that
1,000 defects in the Mozilla JavaScript engine. jsfunfuzz
is effective because it is hardcoded to target a speciﬁc
interpreter making use of speciﬁc knowledge about past
and common vulnerabilities. The question is: Can we
devise a generic fuzz testing approach that nonetheless
can exploit project-speciﬁc knowledge?
In this paper, we introduce a framework called
LangFuzz that allows black-box fuzz testing of engines
based on a context-free grammar. LangFuzz is not bound
against a speciﬁc test target in the sense that it takes the
grammar as its input: given a JavaScript grammar, it will
generate JavaScript programs; given a PHP grammar, it
will generate PHP programs. To adapt to speciﬁc targets,
LangFuzz can use its grammar to learn code fragments
from a given code base. Given a suite of previously fail-
ing programs, for instance, LangFuzz will use and re-
combine fragments of the provided test suite to generate
new programs—assuming that a recombination of pre-
viously problematic inputs has a higher chance to cause
new problems than random input.
The combination of fuzz testing based on a language
grammar and reusing project-speciﬁc issue-related code
fragments makes LangFuzz an effective tool for secu-
rity testing. Applied on the Mozilla JavaScript engine,
it discovered a total of 105 new vulnerabilities within
three months of operation. These bugs are serious and
1
1var haystack = "foo";
2var re text = "^foo";
3haystack += "x";
4 re text += "(x)";
5var re = new RegExp(re text);
6re. test (haystack);
7RegExp.input = Number();
8 print(RegExp.$1);
Figure 2: Test case generated by LangFuzz, crashing the
JavaScript interpreter when executing Line 8. The static
access of RegExp is deprecated but valid. Reported as
Mozilla bug 610223 [1].
Section 7 discusses threats to validity, and Section 8
closes with conclusion and future work.
2 Background
2.1 Previous Work
“Fuzz testing” was introduced in 1972 by Purdom [16].
It is one of the ﬁrst attempts to automatically test a parser
using the grammar it is based on. We especially adapted
Purdom’s idea of the “Shortest Terminal String Algo-
rithm” for LangFuzz.
In 1990, Miller et al. [10] were
among the ﬁrst to apply fuzz testing to real world appli-
cations. In their study, the authors used random gener-
ated program inputs to test various UNIX utilities. Since
then, the technique of fuzz testing has been used in many
different areas such as protocol testing [6,18], ﬁle format
testing [19, 20], or mutation of valid input [14, 20].
Most relevant for this paper are earlier studies on
grammar-based fuzz testing and test generations for com-
piler and interpreters. In 2005, Lindig [8] generated code
to speciﬁcally stress the C calling convention and check
the results later. In his work, the generator also uses re-
cursion on a small grammar combined with a ﬁxed test
generation scheme. Molnar et al. [12] presented a tool
called SmartFuzz which uses symbolic execution to trig-
ger integer related problems (overﬂows, wrong conver-
sion, signedness problems, etc.) in x86 binaries. In 2011,
Yang et al. [22] presented CSmith—a language-speciﬁc
fuzzer operating on the C programming language gram-
mar. CSmith is a pure generator-based fuzzer generat-
ing C programs for testing compilers and is based on
earlier work of the same authors and on the random C
program generator published by Turner [21]. In contrast
to LangFuzz, CSmith aims to target correctness bugs in-
stead of security bugs. Similar to our work, CSmith ran-
domly uses productions from its built-in C grammar to
create a program. In contrast to LangFuzz, their gram-
mar has non-uniform probability annotations. Further-
more, they already introduce semantic rules during their
Figure 1: LangFuzz workﬂow. Using a language gram-
mar, LangFuzz parses code fragments from sample code
and test cases from a test suite, and mutates the test cases
to incorporate these fragments. The resulting code is
then passed to the interpreter for execution.
valuable, as expressed by the 50.000$ bug bounties they
raised. Nearly all the detected bugs are memory safety
issues. At the same time, the approach can generically
handle arbitrary grammars, as long as they are weakly
typed: applied on the PHP interpreter, it discovered 18
new defects. All generated inputs are semantically cor-
rect and can be executed by the respective interpreters.
Figure 1 describes the structure of LangFuzz. The
framework requires three basic input sources: a language
grammar to be able to parse and generate code artifacts,
sample code used to learn language fragments, and a test
suite used for code mutation. Many test cases contain
code fragments that triggered past bugs. The test suite
can be used as sample code as well as mutation basis.
LangFuzz then generates new test cases using code mu-
tation and code generation strategies before passing the
generated test cases to a test driver executing the test
case—e.g. passing the generated code to an interpreter.
As an example of a generated test case exposing a se-
curity violation, consider Figure 2 that shows a secu-
rity issue in Mozzila’s JavaScript engine. RegExp.$1
(Line 8) is a pointer to the ﬁrst grouped regular expres-
sion match. This memory area can be altered by setting
a new input (Line 7). An attacker could use the pointer
to arbitrarily access memory contents. In this test case,
Lines 7 and 8 are newly generated by LangFuzz, whereas
Lines 1–6 stem from an existing test case.
The remainder of this paper is organized as follows.
Section 2 discusses the state of the art in fuzz testing
and provides fundamental deﬁnitions. Section 3 presents
how LangFuzz works, from code generation to actual
test execution; Section 4 details the actual implemen-
tation. Section 5 discusses our evaluation setup, where
we compare LangFuzz against jsfunfuzz and show that
LangFuzz detects several issues which jsfunfuzz misses.
Section 6 describes the application of LangFuzz on PHP.
2
LangFuzzSample CodeTest SuiteMutatedTestLanguageGrammarPhase IIIFeed test case into interpreter, check for crashes and assertionsPhase IILangFuzz generated  (mutated) test casesPhase ILearning code fragments from samplecode and test suitegeneration process by using ﬁlter functions, which allow
or disallow certain productions depending on the con-
text. This is reasonable when constructing a fuzzer for a
speciﬁc language, but very difﬁcult for a language inde-
pendent approach as we are aiming for.
Fuzzing web browsers and their components is a
promising ﬁeld. The most related work in this ﬁeld is the
work by Ruderman and his tool jsfunfuzz [17]. Jsfunfuzz
is a black-box fuzzing tool for the JavaScript engine that
had a large impact when written in 2007. Jsfunfuzz not
only searches for crashes but can also detect certain cor-
rectness errors by differential testing. Since the tool was
released, it has found over 1,000 defects in the Mozilla
JavaScript Engine and was quickly adopted by browser
developers. jsfunfuzz was the ﬁrst JavaScript fuzzer that
was publicly available (it has since been withdrawn) and
thus inspired LangFuzz. In contrast, LangFuzz does not
speciﬁcally aim at a single language, although this paper
uses JavaScript for evaluation and experiments. Instead,
our approaches aim to be solely based on grammar and
general language assumptions and to combine random
input generation with code mutation.
Miller and Peterson [11] evaluated these two
approaches—random test generation and modifying ex-
isting valid inputs—on PNG image formats showing that
mutation testing alone can miss a large amount of code
due to missing variety in the original inputs. Still, we
believe that mutating code snippets is an important step
that adds regression detection capabilities. Code that has
been shown to detect defects helps to detect incomplete
ﬁxes when changing their context or fragments, espe-
cially when combined with a generative approach.
LangFuzz is a pure black-box approach, requiring no
source code or other knowledge of the tested interpreter.
As shown by Godefroid et al. [7] in 2008, a grammar-
based fuzzing framework that produces JavaScript en-
gine input (Internet Explorer 7) can increase coverage
when linked to a constraint solver and coverage measure-
ment tools. While we consider coverage to be an insuf-
ﬁcient indicator for test quality in interpreters (just-in-
time compilation and the execution itself heavily depend
on the global engine state), such an extension may also
prove valuable for LangFuzz.
In 2011, Zalewski [23] presented the crossfuzz tool
that is specialized in DOM fuzzing and revealed some
problems in many popular browsers. The same author
has published even more fuzzers for speciﬁc purposes
like ref fuzz, mangleme, Canvas fuzzer or transfuzz.
They all target different functionality in browsers and
have found severe vulnerabilities.
2.2 Deﬁnitions
Throughout this paper, we will make use of the following
terminology and assumptions.
Defect. Within this paper, the term “defect” refers to er-
rors in code that cause abnormal termination only
(e.g. crash due to memory violation or an assertion
violation). All other software defects (e.g. defect
that produce false output without abnormal termi-
nation) will be disregarded, although such defects
might be detected under certain circumstances. We
think that this limitation is reasonable due to the
fact that detecting other types of defects using fuzz-
testing generally requires strong assumptions about
the target software under test.
Grammar. In this paper, the term “grammar” refers to
context-free grammars (Type-2 in the Chomsky hi-
erarchy) unless stated otherwise.
Interpreter. An “interpreter” in the sense of this paper
is any software system that receives a program in
source code form and then executes it. This also
includes just-in-time compilers which translate the
source to byte code before or during runtime of the
program. The main motivation to use grammar-
based fuzz testing is the fact that such interpreter
systems consist of lexer and parser stages that detect
malformed input which causes the system to reject
the input without even executing it.
3 How LangFuzz works
In fuzz testing, we can roughly distinguish between two
techniques: Generative approaches try to create new ran-
dom input, possibly using certain constraints or rules.
Mutative approaches try to derive new testing inputs
from existing data by randomly modifying it. For exam-
ple, both jsfunfuzz [17] and CSmith [22] use generative
approaches. LangFuzz makes use of both approaches,
but mutation is the primary technique. A purely gen-
erative design would likely fail due to certain semantic
rules not being respected (e.g. a variable must be de-
ﬁned before it is used).
Introducing semantic rules to
solve such problems would tie LangFuzz to certain lan-
guage semantics. Mutation, however, allows us to learn
and reuse existing semantic context. This way, LangFuzz
stays language-independent without losing the ability to
generate powerful semantic context to embed generated
or mutated code fragments.
3.1 Code mutation
The mutation process consists of two phases, a learn-
ing phase in the beginning and the main mutation phase.
3
In the learning phase, we process a certain set of sam-
ple input ﬁles using a parser for the given language (de-
rived from the language grammar). The parser will allow
us to separate the input ﬁle into code fragments which
are essentially examples for non-terminals in the gram-
mar. Of course, these fragments may overlap (e.g. an
expression might be contained in an ifStatement
which is a statement according to the grammar). Given
a large codebase, we can build up a fragment pool
consisting of expansions for all kinds of non-terminal
symbols. Once we have learned all of our input, the
mutation phase starts. For mutation, a single target ﬁle
is processed again using the parser. This time, we ran-
domly pick some of the fragments we saw during parsing
and replace them with other fragments of the same type.
These code fragments might of course be semantically
invalid or less useful without the context that surrounded
them originally, but we accept this trade-off for being in-
dependent of the language semantics. In Section 3.3, we
discuss one important semantic improvement performed
during fragment replacement.
As our primary target is to trigger defects in the tar-
get program, it is reasonable to assume that existing test
cases (especially regressions) written in the target lan-
guage should be helpful for this purpose; building and
maintaining such test suites is standard practice for de-
velopers of interpreters and compilers. Using the mu-
tation process described in the previous section, we can
process the whole test suite ﬁle by ﬁle, ﬁrst learning frag-
ments from it and then creating executable mutants based
on the original tests.
3.2 Code generation
With our mutation approach, we can only use those code
fragments as replacements that we have learned from our
code base before. Intuitively, it would also be useful if
we could generate fragments on our own, possibly yield-
ing constructs that cannot or can only hardly be produced
using the pure mutation approach.
Using a language grammar, it is natural to generate
fragments by random walk over the tree of possible ex-
pansion series. But performing a random walk with uni-
form probabilities is not guaranteed to terminate. How-
ever, terminating the walk without completing all expan-
sions might result in a syntactically invalid input.
Usually, this problem can be mitigated by restructur-
ing the grammar, adding non-uniform probabilities to the
edges and/or imposing additional semantic restrictions
during the production, as in the CSmith work [22].
Restructuring or annotating the grammar with prob-
abilities is not straightforward and requires additional
work for every single language.