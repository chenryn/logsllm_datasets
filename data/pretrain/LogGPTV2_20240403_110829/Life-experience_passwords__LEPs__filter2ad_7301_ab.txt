PL
CI
OBJ
ACT
DT
YR
RL
HU
TN
ﬁrst name (e.g., John)
last name (e.g., Doe)
ﬁrst and last name (e.g., John Doe)
place (e.g., UCLA)
city (e.g., Seattle)
object (e.g., watch)
activity (e.g., kayaking)
date (e.g., 2/28/1972)
year (e.g., 2001)
relationship (e.g., mom)
approx. 100 choices (e.g., Toyota)
approx. 10 choices (e.g., yellow)
48
8
30
4
276
n/a
n/a
n/a
n/a
n/a
Lists Min. size Max. size Unique items
384
80
38,717
151,671
150,695
223,096
3
100
combinations of FN and LN
10
85
6
14
18,467
19,681
870
36,864
2,230
22,210
385
285,537
6,209,229
563,335,972,290
1,398,314
754,450
139,049
11,539
18,250
50
49
100
10
Table 1: Fact categories and their statistical and brute-force strengths (see Appendix for sources)
During LEP creation, we mine facts about people, loca-
tions, time, objects and activities. These facts are objec-
tive, and thus immutable. People and location facts have a
high strength (see the Section 3.2), while time, objects and
activities have a lower but still substantial strength. Fur-
ther, we have designed our elicitation process to produce
very speciﬁc questions, and thus stable facts.
Privacy risk. LEP questions and answers contain infor-
mation about some past event, which may pose privacy risk
to a user if questions are observed by others, or if answers
are guessed or cracked. We advise users to avoid sensitive
or incriminating facts during LEP creation. Our evaluation
ﬁnds that only 3% of LEPs in our study had sensitive infor-
mation, which we plan to address with better LEP creation
prompts.
3.2 Strength
This section discusses our attacker models and how we
calculate strength of LEPs against these attackers.
Strength of a password can be measured as the number of
trials a guessing attacker has to make until success – this is
known as guesswork [66] or a heuristic measure of password
strength [69]. As Bonneau points out in [66], this deﬁnition
of strength depends on the size of the attacker’s dictionary
and is diﬃcult to reproduce across diﬀerent studies. He
proposed several more stable strength measures, such as α-
guesswork. However, they require a much larger sample size
than we had in our studies. We thus use guesswork as our
measure of strength.
A LEP consists of multiple facts. We calculate its strength
as a half of the product of individual fact strengths: SLEP =
i=1 Sfi , where S denotes strength, k is the number
of facts in a given LEP, and fi is the i-th fact. On the
average the attacker will go through a half of the possible
fact combinations before guessing correctly.
1/2 ·"k
We classify LEP facts into categories, shown in Table 1.
We assume an intelligent attacker, who can infer the fact’s
category from the user’s authentication prompt, and guesses
answers only within that category.
Attacker Models. We consider the following attacker
models [67, 69]. A brute-force attacker compiles a dictio-
nary of all possible answers within a fact category (e.g., ﬁrst
name), and tries them in any order. A statistical attacker
compiles ranked dictionaries of popular answers within a
fact category, and tries them in the order of popularity. A
friend attacker forms guesses using her personal knowle-
dge of the user, and may mine some guesses from the user’s
social network pages or use search engines. A password-
reuse attacker has stolen a user’s password from one server
and attempts to reuse it, in the exact or a slightly modiﬁed
version, to gain access to another server.
We assume that brute-force, statistical and password-reuse
attackers are oﬄine attackers [67]. They will use automated
programs to crack LEPs, just like they do for passwords to-
day. They can make as many guesses as their dictionaries
allow. A friend attacker, is an online attacker [67], and
will attempt to guess passwords manually. We thus assume
that a friend attacker is allowed to make a small number of
guesses, before being locked out by the server for excessive
failed logins.
We denote the strength of a fact against brute-force at-
tacks as its brute-force strength, and measure it as the num-
ber of all possible inputs in the fact category. It is challeng-
ing to count all possible inputs, since some answers may be
drawn from sets that are not fully enumerable. For example,
an answer to a “who” question can be a relationship, a ﬁrst
name, a last name, a ﬁrst and last name pair, a title like Mr.
and a last name, a nickname, etc. Even within these subsets
there are variations. For example, one could combine rela-
tionship and an adjective, e.g., “my favorite aunt” or “my
oldest uncle”. Further, some subsets may not be fully enu-
merable. For example, there are publicly available censuses
of US names but not of Chinese or Indian names.
We denote the strength of a fact against statistical guess-
ing as its statistical strength, and measure it as: (1) the rank
of the fact on a ranked list of popular facts in its category,
or (2) the brute-force strength if the fact is not found on the
popular list. A challenge lies in creating suﬃciently large
and comprehensive lists of popular facts, and ranking them
based on their popularity.
We examined many diﬀerent data sources, seeking to iden-
tify:
(1) the total number of possible facts, and (2) the
ranked list of popular facts, within each fact category. We
provide a brief explanation of our data sources here and refer
the readers to the Appendix for more details. Our estimates
and popular list sizes are shown in Table 1.
Brute-force strength calculation. To calculate brute-
force strength, we needed the total number of possible facts
for each category. For the “ﬁrst name” (FN) and “last name”
(LN) categories, we used the estimates from the U.S cen-
sus [19, 16], U.S. Social Security Administration [21] and
popular names available in 67 countries from Wikipedia.
The total number of FN and LN is 285,537 and 6,209,229
respectively. For the total number of “full name” (FL) facts,
we calculate a product of FN and LN counts for each coun-
try, and sum them up arriving at 563 B possible inputs. This
overestimates the number of possible full names, since some
FN-LN combinations may never occur in practice. But, we
116
could not ﬁnd a good public source of full names, and were
forced to approximate.
For the “city” (CI) category, we obtained the list of 754,450
locations with population greater than 5,000 people from
DBpedia [13]. For the “place” (PL) category, we calcu-
lated the sum of the number of restaurants in the US [59]
(1,232,016), the number of universities/colleges in US [45]
(7,234) and in the world (8,766)
[61], the number of el-
ementary, middle, and high schools in US [44] (129,189),
and the number of secondary schools in UK [70] (21,109).
Note that this estimate does not include other popular at-
tractions, such as amusement parks, hotels and monuments,
and is thus an underestimate of the total number of inputs
for the PL category. For the “relationship” (RL) category,
we built a small list of relationships (49 entries), compiled
from a dictionary. For the “object” (OBJ) and “activity”
(ACT) categories, we used the size of the Wordnet [78] dic-
tionary for nouns and verbs in English language. For the
“year” (YR) category, we assumed that the user will recount
experiences, which are at most 50 years in the past. The
total number of inputs in the “date” (DT) category is calcu-
lated as 365∗50. Finally, the categories “hundred” (HU) and
“ten” (TN) encompass facts, which have a limited number
of possible answers (e.g., color of a bike, model of a car).
Statistical strength calculation. To calculate statis-
tical strength we needed lists of popular facts in each cate-
gory, ranked by popularity. We used online domain-speciﬁc
sources to form these lists. We gathered around 434,000
unique popular list items from more than 530 diﬀerent online
sources. These sources include (1) Wikipedia/DBpedia [63],
(2) Freebase [14], (3) U.S. Government sources: U.S. Cen-
sus, U.S Social Security Administration, Dept. of Educa-
tion, Dept. of Labor Stat., National Center for Education
Statistics, (4) Other domain speciﬁc online sources: TripAd-
viser for popular travel destinations, Forbes and US News
for educational institutions, IMDB for movie names, etc.
(5) Popular English word lists from Google 20K [17], and 5K
nouns, words, and lemma from the Corpus of Contemporary
American English (COCA) [51]. We further incorporated
popular lists for diﬀerent categories from the Bonneau et al.
dataset [70]. More details are provided in the Appendix.
Some lists had items ranked by popularity, while others
did not (e.g. the list of the 100 most popular Chinese names
from Wikipedia). For unranked lists, we used the Bing
search engine to calculate the number of Web pages contain-
ing each list item. We automatically built structured queries
as a “fact category + the item” (e.g., “ﬁrst name Hao”) and
mined the number of pages from the search engine’s reply.
We then assumed that the popularity of an item is propor-
tional to the number of Web pages containing it, and used
this to rank the items in the list. While this may not be an
accurate reﬂection of the popularity of each item, we believe
it is a good approximation for relative rankings.
We have multiple lists of popular facts per category. We
calculate the strength of a LEP fact as its lowest rank on
any list. This approach assumes a strong statistical guessing
attacker, which has the best popular list for each input.
Finally, if our popular lists were too small, we would over-
estimate the statistical strength of LEPs, as we would often
use the brute-force strength for oﬀ-the-list facts. We show
the count of popular lists per category, and their minimum
and maximum sizes, as well as the total number of unique
items in Table 1. For example, in the FN (ﬁrst name) cat-
117
egory, we have 384 popular lists, ranging from 3 to 38,717
inputs, and containing the total number of 150,695 unique
names. We further evaluated how many facts collected in
our user studies were covered by our popular lists. We were
able to ﬁnd 75% of FN, 99% of LN, 81% of FL, 63% of CI,
54% of OBJ, 46% of ACT and 34% of PL inputs on our
popular lists. Thus our popular lists seem comprehensive
enough for statistical strength calculation.
3.3 Creation
LEP creation requires users to actively provide input, from
which the system extracts useful facts. In our work we have
investigated guided and semi-guided methods for LEP in-
put. These methods are triggered after a user has chosen
the topic they want to talk about and provided its title.
Figure 3 illustrates these input methods with one speciﬁc
title “Trip to France”.
In the guided method a user is prompted with a series
of questions, chosen from a ﬁxed set. The questions are
displayed one at a time and the choice of the subsequent
questions may depend on the user’s answers to the preceding
ones. This is illustrated in Figure 3(a). Some questions may
be open-ended, e.g., “What else do you remember about ...”.
In the semi-guided method the user is prompted to in-
put a certain number of facts in the given category, and to
provide a “hint” for each fact that will be used to form the
authentication prompt. This is illustrated in Figure 3(b).
We also investigated a freeform method, where a user in-
puts a paragraph of free narrative, out of which we automati-
cally extracted useful facts [96]. However, we abandoned this
approach early since it had a large overhead for the users.
Our input methods guide the user toward useful facts,
such as names, locations, objects, etc. and away from facts,
which are not useful, such as preferences, opinions and feel-
ings. The semi-guided method allows more freedom to the
user to choose facts, which are relevant to her, but this free-
dom may lead to unstable facts. We evaluate these aspects
in Section 5.
In extracting and processing useful facts from user re-
sponses, we normalize inputs for capitalization and punctua-
tion. We also use POS tagging [87], dependency parsing [73],
noun stemming and semantic role labeling [49] to extract the
speciﬁc parts of user responses, such as verbs, nouns, sub-
ject, object, location, time, action, and person information.
This helps us transform facts into question/answer pairs,
and to identify, for multi-word inputs, those parts that carry
the most meaning for the user (nouns, proper names of peo-
ple and locations, verbs or adjectives).
3.4 Authentication
During authentication the system shows all the questions
to the user, obtains the answers and compares them against
one or several stored hashes.
Let a LEP contain N facts. We require that a user recalls
M facts for authentication success, where M ≤ N , and that
the strength of the recalled facts be greater than some target
value (we use 3class8 strength of 958 or 52.55 bits in our eval-
uation). The smaller the diﬀerence between N and M , the
stronger the authentication criterion. Further, if M < N ,
the system must store-N
M. hashes for one LEP. During au-
thentication, the system produces all possible combinations
of M user answers, and hashes them. Any match between
these and stored hashes leads to authentication success. We
User Input
Title: Trip to France
How many memorable cities did you visit? 2
List two memorable cities you visited? Paris, 
Nice
When did you travel? 2015
How many people traveled with you? 1
List the (cid:38)rst and last name of the person that 
traveled with you? Nick Casey
LEP
Trip to France
List  the  (cid:38)rst  and  last  name  of  one  person 
that traveled with you? Nick Casey
Which year did you travel? 2015
List two cities you visited Paris, Nice
User Input
Title: Trip to France
Enter the (cid:34)rst and last name of one person 
related to this trip and a hint: Nick Casey, 
traveled with me
Enter two locations related to this trip and a hint 
for each: Paris, best art, Nice, wonderful 
weather
LEP
Trip to France
List  the  (cid:34)rst  and  last  name  of  one  person  that 
traveled with you? Nick Casey
List a location related to "best art" Paris
List a location related to "wonderful weather" Nice
(a) Guided
(b) Semi-guided
Figure 3: LEP input methods
have explored diﬀerent values for N and M , and provide
more information about their performance in Section 5.
Authentication may fail not only because a user forgot
her answers, but also because she recalled them imprecisely.
Imprecise recall of LEPs occurs due to a high redundancy
of natural language, as explained below. We address some
sources of mismatch through imprecise matching, which in-
cludes normalization, keyword extraction and reorder match-
ing. While imprecise matching will reduce strength of LEPs,
our evaluation shows that resulting LEPs still have high se-
curity (Section 5), and that imprecise matching signiﬁcantly
improves recall.
Capitalization, reordering and punctuation. A user
might respond to the prompt using diﬀerent capitalization
or punctuation than she did during password creation. We
overcome this by normalizing user answers before storage
and authentication, by removing all capitalization and punc-
tuation. A user may also list several parts of the answer in
a diﬀerent order, e.g., she may reply “Nice, Paris” where the
original answer was “Paris, Nice”. We resolve this through
reorder matching. We detect when an answer may consist
of multiple parts, and try all permutations of these parts in
the matching process.
Misspelling. A user may misspell a reply during pass-
word creation or authentication. We leave handling of mis-
spelling for future work.
Synonyms. A user may reply to a question with a near-
synonym to the extracted term, such as responding with
“lake” instead of “pond”; or with a term that is more speciﬁc
(hyponym) or more general (hypernym) than the expected
term, such as “dog” instead of “poodle” . We leave handling
of synonyms, hyponyms and hypernyms for future work.
Extraneous words. A user may provide extraneous
words in an answer. For example a question “what was
red” may lead the user to input “my apple was red” even
though we expect just “apple” as an answer. We address
this via keyword extraction. We apply keyword extraction
both during password creation and during authentication, in
the same manner. The extraction method depends on the
answer category (see Table 1). From answers in OBJ cat-
egory we extract nouns only, from PL answers we extract
nouns and out-of-dictionary words (likely place names), and
from ACT answers we extract verbs only. Other categories
do not need keyword extraction.
3.5 Uses of LEPs
LEPs could be used instead of passwords, but they may
not be best suited for all authentication tasks, because their
creation and authentication are more time-consuming. Ex-
tended authentication time may be especially burdensome
to users on mobile devices, where keyboard input is slower
than on desktops/laptops.
One possibility would be to use LEPs instead of pass-
words for ﬁrst-time authentication, when cookies have ex-
pired, when the user is accessing an online service from a
new machine, or when the user is logging onto his local ma-
chine after a logout. In these rare situations the added over-
head of LEPs may be acceptable to users, at the beneﬁt of
higher security. LEPs could further be activated only for
high-value accounts, such as bank or e-mail, where secure
access is crucial.
Another possibility would be to use LEPs for secondary
or added authentication, instead of security questions. We
show in Section 5 that LEPs surpass security questions in
security, recall and strength against friend guessing. Many
high-value services currently use text messages sent to user
phone with a code, to reduce risk of password cracking.
LEPs could be used in lieu of the code, when a users does not
have access to her phone (e.g., during international travel).
LEPs could also be used for continuous authentication on
high-security servers, such as government or bank servers.
A logged in user may be prompted for one or several facts
after a period of inactivity to verify that someone did not
gain physical access to her computer.
4. USER STUDIES
We evaluated LEPs through a series of user studies. We
implemented LEP creation and authentication as a Web ap-
plication, so that it can be used remotely. We then ran mul-
tiple user studies over the period of two years, with Amazon
MTurk participants [7] and with students at our institution,
and used their results to reﬁne and improve both our user
interface, and our elicitation process. All studies were ap-
proved by our Institutional Review Board (IRB). All com-