in the main array. Given a ﬁxed system state, let λs be the
failure rate of an SSD in EPLOG, and λ(cid:2)
s be the failure rate
of an SSD in conventional RAID. Let μs be recovery rate of
an SSD. Let λh and μh be the failure rate and recovery rate
of an HDD for EPLOG, respectively.
Note that both λs and λ(cid:2)
s generally increase with the
number of P/E cycles performed, which depends on the amount
of write trafﬁc. For simplicity, we assume that the failure rate
of an SSD increases proportionally with the amount of writes
issued2.
λs = αλ(cid:2)
s
,
(1)
where α denotes the ratio of the amount of writes issued
to the main array in EPLOG to that in conventional RAID.
Note that EPLOG keeps α < 1 by reducing parity writes to
SSDs. In practice, we can estimate α through measurements.
For example, from our experiments (see Section V), we can
estimate that α = 0.5 according to the results in Figure 7.
2The recent study [32] shows that the failure rate of an SSD does not
monotonically increase as ﬂash memory wears. However, as an SSD enters
the wear-out period, which accounts for the majority of the SSD lifetime, the
increasing trend actually holds and supports our assumption.
54
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:38:08 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 4: State transition diagram of the Markov model for
EPLOG’s RAID-5.
EPLOG’s RAID-5: We ﬁrst consider EPLOG’s RAID-5
design, which tolerates a single device failure. Recall that
EPLOG adds one additional HDD as the log device. We now
compute its MTTDL through a Markov model. Speciﬁcally,
suppose that the storage system has a total of i device failures,
j of which are SSDs. When i ≥ 2, the storage system has a
data loss, so we can focus on 0 ≤ j ≤ i ≤ 2. Let (i, j) denote
a state. Thus, the storage system can be at one of the following
states: S0 = (0, 0), S1 = (1, 0), S2 = (1, 1), and S3 = (2, ∗)
(note that S3 can be (2, 1) or (2, 2), both of which imply a
data loss).
Figure 4 shows the state transition diagram of the Markov
model for EPLOG’s RAID-5. Take S2 = (1, 1) as an example,
in which one SSD fails. If the failed SSD is recovered, S2
transits to S0, where the transition rate is μs. If one more
device (either an SSD or the log device) fails, S2 transits to
S3, where the total transition rate is (n − 1)λs + λh.
We denote the system state at
time t as πt =
(π0(t), π1(t), π2(t), π3(t)), where πi(t) denotes the probability
that EPLOG is at state Si at time t. Let π(0) = (1, 0, 0, 0),
meaning that there is no device failure initially. Based on the
Kolmogorov’s forward equation, we have
Fig. 5: State transition diagram of the Markov model for
EPLOG’s RAID-6.
failed device for recovery via random tie-breaking. In this case,
S4 transits to S1 and S2 with rates 1
μh, respectively.
μs and 1
2
2
We do not present the closed-form solution for the MTTDL
of EPLOG’s RAID-6 due to its complexity, but we can
compute the MTTDL through numerical methods. We can
further extend our analysis for the tolerance against a general
number of device failures, and obtain the MTTDL through
numerical methods.
Conventional RAID: The derivations of the MTTDLs for
conventional RAID-5 and RAID-6 are well-known in the
literature (e.g., [8], [9]). For completeness, we write down the
results, in terms of λ(cid:2)
s (see Equation (1)) and μs.
MTTDL for RAID-5 =
MTTDL for RAID-6 =
μs + (2n − 1)λ(cid:2)
s
s)2
n(n − 1)(λ(cid:2)
sμs+(3n2
s+2(n−1)λ(cid:2)
μ2
,
−6n+2)(λ(cid:2)
s)2
n(n − 1)(n − 2)(λ(cid:2)
s)3
(5)
. (6)
π(cid:2)(t) = π(t)Q,
(2)
B. Results
where Q denotes the transition rate matrix given by:
⎡
−(nλs+λh)
⎢⎣
μh
μs
0
Q =
−(μh+nλs)
λh
0
0
nλs
0
−(μs+(n−1)λs+λh)
0
0
nλs
0
(n−1)λs+λh
We can now derive the closed-form MTTDL of EPLOG’s
RAID-5 through standard approaches (e.g., by a Laplace
transform) as follows:
MTTDL =
(2n−1)λs +μs
(cid:2)
(cid:3)
(cid:2)
+
n(n−1)λ2
nλs(2λh +μh)+(λh +μh)(λh −λs)+λhμs
s
2(λh +μh)+ (λh+μh)(λh−λs+μs)
nλs
(cid:3) .
(cid:2)
(cid:3)
(cid:2)
+
EPLOG’s RAID-6: We now consider EPLOG’s RAID-6 de-
sign, which tolerates two device failures. Recall that EPLOG
introduces two additional HDDs as log devices. We follow the
same approach as in the RAID-5 case.
Figure 5 shows the state transition diagram of the Markov
model for EPLOG’s RAID-6. Let (i, j) denote a state as
deﬁned in the RAID-5 case. There are a total of six states,
where 0 ≤ j ≤ i ≤ 3. In particular, the state S6 = (3, ∗)
represents a data loss. One subtlety is that for the state S4,
which has one SSD failure and one HDD failure, we select a
⎤
⎥⎦ .
(3)
(cid:3)
(4)
To better illustrate whether EPLOG really improves the
system reliability, we now compare the MTTDL of EPLOG
and that of conventional RAID via numerical analysis. We ﬁrst
conﬁgure the parameters for conventional RAID. Suppose that
the main array contains n = 10 SSDs. For the failure rate λ(cid:2)
s,
we note that it is challenging to maintain a minimum SSD
lifetime of 3-5 years in a write-intensive environment [25], we
set the average failure rate as 1/λ(cid:2)
= 0.25).
For the recovery rate μs, suppose that the capacity of each
SSD is around 400GB, and the I/O throughput for sequential
writes is around 100MB/s, the average time to recover one
device (i.e., rewrite all data) as around 1/μs = 10−4 year
(i.e., μs = 104).
= 4 years (i.e., λ(cid:2)
s
s
We now conﬁgure the parameters for EPLOG. We vary
the failure rate of an HDD λh from λ(cid:2)
s, and still
set μh = 104. We set λs by considering three values of α,
including 0.3, 0.5, and 0.7. Note that α = 0.5 can be justiﬁed
from our trace-driven evaluations (see Figure 7).
s to 10λ(cid:2)
Figure 6 shows the MTTDL results versus the ratio λh/λ(cid:2)
for RAID-5 and RAID-6 (note that the MTTDL for conven-
tional RAID is ﬁxed since no HDD is used). It is reported
that SSDs and HDDs have comparable failure rates [48] (i.e.,
λh ≈ λ(cid:2)
s). In this case, EPLOG achieves higher system
reliability. For example, if λh = λ(cid:2)
s and α = 0.5, EPLOG
s
55
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:38:08 UTC from IEEE Xplore.  Restrictions apply. 
15000
10000
)
s
r
a
e
y
(
L
D
T
T
M
Conventional RAID
EPLOG(α=0.3)
EPLOG(α=0.5)
EPLOG(α=0.7)
5000
0
1
3
7
Error Rate Ratio: λ
5
h/λ’s
9
(a) RAID-5
8x 107
)
s
r
a
e
y
(
L
D
T
T
M
6
4
2
0
1
Conventional RAID
EPLOG(α=0.3)
EPLOG(α=0.5)
EPLOG(α=0.7)
No. of
writes
4,110,563
1,431,628
1,363,855
1,069,421
FIN
WEB
USR
MDS
Avg. write
size (KB)
Random
write (%)
7.19
12.50
10.05
7.22
76.17
77.62
76.19
82.99
WSS
(GB)
3.67
7.26
2.44
3.09
5
3
7
Error Rate Ratio: λ
(b) RAID-6
h/λ’s
9
TABLE I: Trace statistics: total number of writes, average write
size, ratio of random writes, and working set size.
Fig. 6: Reliability comparison between EPLOG and conven-
tional RAID.
achieves 2.8× MTTDL compared to conventional RAID for
both RAID-5 and RAID-6. The system reliability of EPLOG
heavily depends on the failure rate of the log devices. As
the failure rate λh increases, the system reliability of EPLOG
drops dramatically, and the drop rate is even more signiﬁcant
when more HDDs are used (e.g., in RAID-6). In particular,
when α = 0.5, EPLOG maintains higher system reliability
provided that λh is less than 6λ(cid:2)
s for RAID-5 and
RAID-6, respectively.
s and 2λ(cid:2)
V. EXPERIMENTS
We evaluate EPLOG via trace-driven testbed experiments,
and compare its endurance and performance with those of the
original parity logging and conventional RAID implemented
by Linux software RAID based on mdadm. To summarize,
our experiments have the following key ﬁndings: (i) EPLOG
improves the endurance of SSD RAID by reducing both the
write trafﬁc to SSDs and the number of GC requests, (ii)
EPLOG achieves potential gains with small-sized caching,
(iii) EPLOG has limited parity commit overhead, (iv) EPLOG
achieves higher I/O throughput than baseline approaches, and
(v) EPLOG has limited metadata management overhead.
A. Setup
Testbed: We conduct our experiments on a machine running
Linux Ubuntu 14.04 LTS with kernel 3.13. The machine has
a quad-core 3.4GHz Intel Xeon E3-1240v2, 32GB RAM,
multiple Plextor M5 Pro 128GB SSDs as the main array, and
multiple Seagate ST1000DM003 7200RPM 1TB SATA HDDs
as the log devices. It interconnects all SSDs and HDDs via an
LSI SAS 9201-16i host bus adapter. Also, we attach an extra
SSD to the motherboard as the OS drive.
We compare EPLOG with two baseline parity update
schemes. The ﬁrst one is the Linux software RAID imple-
mentation based on mdadm (denoted by MD) [37], which
implements conventional RAID and writes parity trafﬁc to
SSDs directly. The second one is the original parity logging
(denoted by PL) [47], which performs parity updates at the
stripe level (see Figure 1(a)). We implement PL based on our
EPLOG prototype (see Section III-E) for fair comparisons.
We focus on RAID-5 and RAID-6, which tolerate one and
two device failures, respectively. We consider four settings:
(4+1)-RAID-5 (i.e., ﬁve SSDs), (6+1)-RAID-5 (i.e., seven
SSDs), (4+2)-RAID-6 (i.e., six SSDs), and (6+2)-RAID-6 (i.e.,
eight SSDs). For PL and EPLOG, we allocate one and two
additional HDDs as log devices for RAID-5 and RAID-6,
respectively. In all schemes, we set the chunk size as 4KB.
We use the O_DIRECT mode to bypass the internal cache.
For PL and EPLOG, we disable caching, parity commit, and
metadata checkpointing (i.e., the metadata structure remains in
memory), except when we evaluate these features.
Traces: We consider four real-world I/O traces:
•
•
FIN: It
is an I/O trace collected by the Storage
Performance Council [1]. The trace captures the work-
loads of a ﬁnancial OLTP application over a 12-
hour period. We choose the write-dominant trace ﬁle
Financial1.spc out of the two available traces.
WEB, USR, and MDS: They are three I/O traces col-
lected by Microsoft Research Cambridge [36]. They
describe the workloads of enterprise servers of three
volumes, namely web0, usr0, and mds0, respec-
tively, over a one-week period.
Note that each of the original traces spans a very large
address space, yet only a small proportion of the addresses are
actually accessed. To ﬁt the traces into our testbed, which has
a limited storage capacity, we compact each trace by skipping
the addresses that are not accessed. Speciﬁcally, we divide the
whole logical address space of each trace into 1MB segments.
We then skip any segment that is not accessed, and also shift
the offsets of the requests in the following accessed segments
accordingly. We keep the same request order, so as to preserve
workload locality.
Before replaying each trace, we ﬁrst sequentially write to
all remaining segments (after our compaction) to fully occupy
the working set. Each write request in a trace will be treated
as an update. In addition, we round up the size of each write
request to the nearest multiple of the chunk size. By making
all write requests as updates, we can stress-test the impact of
parity updates.
Table I summarizes the write statistics of the four traces,
after we round up the sizes of all write requests. It shows a