# 延迟相关问题的警报
我们将使用`go-demo-5`应用来测量延迟，因此我们的第一步是安装它。
```
 1  GD5_ADDR=go-demo-5.$LB_IP.nip.io
 2
 3  helm install \
 4      https://github.com/vfarcic/go-demo-5/releases/download/
    0.0.1/go-demo-5-0.0.1.tgz \
 5      --name go-demo-5 \
 6      --namespace go-demo-5 \
 7      --set ingress.host=$GD5_ADDR
```
我们生成了一个将用作入口入口点的地址，并使用 Helm 部署了应用。现在我们应该等到它推出。
```
 1  kubectl -n go-demo-5 \
 2      rollout status \
 3      deployment go-demo-5
```
在我们继续之前，我们将通过发送一个 HTTP 请求来检查应用是否确实正常工作。
```
 1  curl "http://$GD5_ADDR/demo/hello"
```
输出应该是熟悉的`hello, world!`消息。
现在，让我们看看我们是否可以，例如，获得通过入口进入系统的请求的持续时间。
```
 1  open "http://$PROM_ADDR/graph"
```
如果您单击光标处插入指标下拉列表，您将能够浏览所有可用的指标。我们要找的是`nginx_ingress_controller_request_duration_seconds_bucket`。顾名思义，该指标来自 NGINX 入口控制器，以秒为单位提供请求持续时间，并以桶为单位进行分组。
请键入以下表达式，然后单击“执行”按钮。
```
 1  nginx_ingress_controller_request_duration_seconds_bucket
```
在这种情况下，查看原始值可能不是很有用，因此请单击图表选项卡。
您应该会看到图表，每个入口一个。每一个都在增加，因为所讨论的度量是一个计数器([https://prometheus.io/docs/concepts/metric_types/#counter](https://prometheus.io/docs/concepts/metric_types/#counter))。它的价值随着每个请求而增长。
A Prometheus counter is a cumulative metric whose value can only increase, or be reset to zero on restart.
我们需要的是计算一段时间内的请求率。我们将通过组合`sum`和`rate`([https://Prometheus . io/docs/Prometheus/latest/query/functions/# rate()](https://prometheus.io/docs/prometheus/latest/querying/functions/#rate()))函数来实现。前者应该是不言自明的。
Prometheus' rate function calculates the per-second average rate of increase of the time series in the range vector.
请键入下面的表达式，然后按“执行”按钮。
```
 1  sum(rate(
 2    nginx_ingress_controller_request_duration_seconds_count[5m]
 3  )) 
 4  by (ingress)
```
结果图显示了通过入口进入系统的所有请求的每秒速率。费率是根据五分钟间隔计算的。如果您将鼠标悬停在其中一行上，您将看到像值和入口这样的附加信息。`by`语句允许我们按`ingress`对结果进行分组。
尽管如此，结果本身并不是很有用，所以让我们重新定义我们的需求。我们应该能够找出有多少请求慢于 0.25 秒。我们不能直接这么做。相反，我们可以检索所有 0.25 秒或更快的时间。
请键入下面的表达式，然后按“执行”按钮。
```
 1  sum(rate(
 2    nginx_ingress_controller_request_duration_seconds_bucket{
 3      le="0.25"
 4    }[5m]
 5  )) 
 6  by (ingress)
```
我们真正想要的是找到落入 0.25 秒桶的请求百分比。为此，我们将获得快于或等于 0.25 秒的请求速率，并将结果除以所有请求的速率。
请键入下面的表达式，然后按“执行”按钮。
```
 1  sum(rate(
 2    nginx_ingress_controller_request_duration_seconds_bucket{
 3      le="0.25"
 4    }[5m]
 5  )) 
 6  by (ingress) / 
 7  sum(rate(
 8    nginx_ingress_controller_request_duration_seconds_count[5m]
 9  )) 
10  by (ingress)
```
除了偶尔与普罗米修斯和警报管理器的交互以及我们发送给`go-demo-5`的一个请求之外，您可能不会在图表中看到太多，因为我们还没有产生太多流量。然而，您可以看到的几行显示了在 0.25 秒内响应的请求的百分比。
目前，我们只对`go-demo-5`请求感兴趣，所以我们将进一步细化表达式，将结果限制在`go-demo-5`入口。
请键入下面的表达式，然后按“执行”按钮。
```
 1  sum(rate(
 2    nginx_ingress_controller_request_duration_seconds_bucket{
 3      le="0.25", 
 4      ingress="go-demo-5"
 5    }[5m]
 6  )) 
 7  by (ingress) / 
 8  sum(rate(
 9    nginx_ingress_controller_request_duration_seconds_count{
10      ingress="go-demo-5"
11    }[5m]
12  )) 
13  by (ingress)
```
这个图应该几乎是空的，因为我们只发送了一个请求。或者，也许你收到了`no datapoints found`信息。是时候产生一些流量了。
```
 1  for i in {1..30}; do
 2    DELAY=$[ $RANDOM % 1000 ]
 3    curl "http://$GD5_ADDR/demo/hello?delay=$DELAY"
 4  done
```
我们向`go-demo-5`发送了三十个请求。应用有一个“隐藏”功能来延迟对请求的响应。假设我们想要生成具有随机响应时间的流量，我们使用了随机值高达千毫秒的`DELAY`变量。现在我们可以重新运行同一个查询，看看是否能得到一些更有意义的数据。
请等待一段时间，直到收集到来自新请求的数据，然后键入后面的表达式(在普罗米修斯中)，并按“执行”按钮。
```
 1  sum(rate(
 2    nginx_ingress_controller_request_duration_seconds_bucket{
 3      le="0.25", 
 4      ingress="go-demo-5"
 5    }[5m]
 6  )) 
 7  by (ingress) / 
 8  sum(rate(
 9    nginx_ingress_controller_request_duration_seconds_count{
10      ingress="go-demo-5"
11    }[5m]
12  )) 
13  by (ingress)
```
这一次，我们可以看到一条新线的出现。在我的例子中(截图如下)，大约 25%的请求持续时间在 0.25 秒以内。或者，换句话说，大约四分之一的请求比预期的要慢。
![](img/399a7670-e5f9-4753-8c2e-a02ef8ef2241.png)
Figure 3-8: Prometheus' graph screen with the percentage of requests with 0.25 seconds duration
当我们确实知道存在问题并且想要进一步深入研究时，针对特定应用(入口)的过滤指标非常有用。然而，我们仍然需要一个警报，告诉我们有问题。为此，我们将执行类似的查询，但这一次没有将结果限制在特定的应用(入口)中。我们还必须定义一个触发警报的条件，因此我们将阈值设置为百分之九十五(0.95)。如果没有这样的阈值，每次单个请求变慢时，我们都会收到通知。结果，我们会被警报所包围，并且很可能在不久之后就开始忽略它们。毕竟，如果一个请求很慢，没有一个系统会有危险，但只有当其中相当多的请求很慢时，才会有危险。在我们的例子中，这是 5%的慢速请求，或者更准确地说，不到 95%的快速请求。
请键入下面的表达式，然后按“执行”按钮。
```
 1  sum(rate(
 2    nginx_ingress_controller_request_duration_seconds_bucket{
 3      le="0.25"
 4    }[5m]
 5  ))
 6  by (ingress) /
 7  sum(rate(
 8    nginx_ingress_controller_request_duration_seconds_count[5m]
 9  ))
10  by (ingress)  - name: latency
>   rules:
>   - alert: AppTooSlow
>     expr: sum(rate(nginx_ingress_controller_request_duration_seconds_bucket{le= "0.25"}[5m])) by (ingress) / sum(rate(nginx_ingress_controller_request_duration_seconds_count[5m])) by (ingress)      labels:
>       severity: notify
>     annotations:
>       summary: Application is too slow
>       description: More then 5% of requests are slower than 0.25s
57c66
 0
---
>     expr: count(kube_node_info) > 3
```
我们增加了一个新的提醒`AppTooSlow`。如果持续时间为 0.25 秒或更短的请求百分比小于百分之九十五(`0.95`)，就会触发。
我们还将`TooManyNodes`的阈值还原为其原始值`3`。
接下来，我们将使用新值更新`prometheus`图表，并打开警报屏幕，以确认是否确实添加了新警报。
```
 1  helm upgrade -i prometheus \
 2    stable/prometheus \
 3    --namespace metrics \
 4    --version 7.1.3 \
 5    --set server.ingress.hosts={$PROM_ADDR} \
 6    --set alertmanager.ingress.hosts={$AM_ADDR} \
 7    -f mon/prom-values-latency.yml
 8
 9  open "http://$PROM_ADDR/alerts"
```
如果`AppTooSlow`提醒仍然不可用，请稍等片刻并刷新屏幕。
![](img/68c98ad5-8654-4de8-b3d3-f34e05f3ce6e.png)
Figure 3-10: Prometheus' alerts screen