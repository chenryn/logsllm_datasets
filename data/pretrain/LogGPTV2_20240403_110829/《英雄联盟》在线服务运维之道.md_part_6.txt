上，那么我们怎么知道它的状态？
于是，我们的告警和度量指标系统开始派上用场。
度量指标系统向应用程序查询度量指标，并把这些信息发送到度量
指标管道中，然后被推送给数据中心的数据收集器。这些数据被保存到
Elasticsearch里，然后watcher会基于这些数据发出告警。
应用程序提供了告警端点。告警服务会注册这些端点，并通过监控服
务来监控度量指标的状态变更。如果一个应用程序的状态从“健康”变成
“降级”，并且它已经注册过告警，那么告警服务就会向注册过的联系人
端点（通过邮件、手机等方式）发送通知。
那么度量指标系统是怎么找到数据收集器的？答案是通过服务发现！
开发人员可以通过在配置服务上设置度量指标类型、发送时间间隔或告警
信息来实时地修改度量指标和告警内容。如果觉得某种告警的数量太多，
那么就修改一下配置，让应用程序取消掉这个告警。
随后，度量指标被聚合到数据仓库中。我们将数据移动到实时的数据
管道中，该管道主要使用了Elasticsearch，并由Riot的数据产品和解决方案
团队进行运维。数据在进入管道之后，我们就可以创建仪表盘。因为应用
程序在度量指标中包含了它们的作用域和指标数据，我们可以查到某个具
体应用程序的度量指标。
上图是配置服务的度量指标截图。从图中可以看到它的CPU负载（最
右边），以及它每分钟接收2万个左右的请求。这个实例来自我们的阿
姆斯特丹数据中心——它的“集群”是“lolriot.ams1.configurous1”（部
署作用域），应用程序是“infrastructurous.configurous”（应用程序作用
域）。
如果有必要，我们可以为这些度量指标创建告警。比如，我们可以开
启“Enabled Instance Count”告警，如果可用实例少于“3”，就发送告警
联系某人。
开发人员在前期把他们的应用程序注册到发现服务和配置服务上，后
续就能自动获得这些报表信息。
获取秘钥
之前一直没有提到安全问题，实际上，通信安全是任何高度可移植、
可动态配置的微服务系统需要关注的问题。HTTPS流量或API认证令牌的
SSL证书需要被保护起来，我们把它们放在配置服务里，方便其他服务访
问，但我们绝对不能使用明文保存。那么我们是怎么做的呢？
如果我们在配置服务中加密和保存数据会怎样？如果是这样的话，应
用程序在获取证书后需要对其进行解密，而且要确保获取证书的应用程序
有解密秘钥。这就是我们整个运维蓝图的最后一道关口：秘钥管理。
为了实现秘钥管理，我们对HashiCorps的Vault服务进行了封装。Vault
提供的功能比我们需要的要多很多，我们只需要保存解密秘钥，应用程序
在拿到秘钥后可以解密数据。我们封装的服务提供了REST端点，应用程
序可以通过这些端点获取秘钥。
从理论上看，这个很简单，开发人员将解密秘钥放到秘钥服务器上，
并提供应用程序的命名空间。在应用容器启动的时候，我们的容器调度器
Admiral将这些秘钥注入到应用容器里（根据应用程序提供的命名空间来
查找特定的容器）。应用容器拿到解密秘钥后，就可以用它解密从配置服
务获取的配置数据。配置数据的提供者在将数据推送给配置服务之前需要
使用加密秘钥来加密数据。
有了这些流程，我们的服务实现了高度可移植、可动态配置、自我感
知、可被发现，并能够安全地处理数据。
开发者生态系统
我们已经介绍了所有在Riot生产环境中运行的服务，不过我们的生态
系统里还有其他很多东西。为了使用好这些系统，我们创建了很多Web和
命令行工具。如果说之前所讨论的只是我们的生产生态系统，那么接下来
还需要介绍我们的开发者系统。在下一章中，我将继续介绍我们的开发者
生态系统。这里先透露一张截图，它是我们的一个Web应用控件，我们通
过它访问生态系统的工具和数据。
第六章 开发者生态系统
在上一章结尾部分，我向大家展示了用来访问我们微服务生态系统
的Web工具。我们之前已经介绍过其中的一些应用：用于保存容器镜像的
JFrog Artifactory、发现服务、度量指标系统以及用于自动化和持续交付的
Jenkins。除了这些，还有其他很多工具。
因为时间的关系，我没办法逐一介绍它们，不过我会集中介绍几个在
管理生态系统关键部分给我们带来巨大帮助的工具。我们借助这些工具
来：
1. 可视化和检视全球的容器集群（Toolbox）
2. 简化软件网络规则的处理（network.rcluster）
3. 在全球范围内查找服务（服务发现）
4. 对构建和部署过程进行跟踪（Build Tracker）
可视化集群
下面是Toolbox的截图，也就是我们的容器可视化工具。在之前的章
节中，我们介绍了Admiral调度器，这张图片所显示的就是从这个调度器
的REST API返回的数据可视化结果。我们可以从图中看到全球集群的状
态（可以看到总共有16个集群），并根据部署区域分开显示。Riot的集群
分布在世界各地，包括台北、雅加达、迈阿密、阿姆斯特丹、韩国和日
本。
我们运行了2400多个应用程序实例，并将这些实例叫作Pack。这些实
例需要5000多个Docker容器来运行。在过去一两年中，这些Pack一直处于
运行状态。不过，上图只显示了运行在容器当中的服务，Riot还有其他很
多服务并没有在张图上显示出来。
Toolbox不仅提供了一个全局视图，我们还可以点进去查看各个数据
中心的详细信息。
一张截图无法显示所有的内容，不过从这张阿姆斯特丹系统的简单视
图中，我们可以看到运行中的应用程序的数量。我们可以看到底层的叠加
网络服务，还能看到节点应用程序、Pack的状态，以及哪些应用程序注册
到了发现服务上。开发人员和运维人员可以基于这些视图更好地了解服务
的运行情况。
如果要查看服务的细节，可以继续点进去。下面以我负责的
Summoner（召唤者）服务为例。该服务负责处理来自Riot Chat和
Developer API Portal的流量。
我们基于命名空间和作用域系统来管理应用程序。如下图所示，
Toolbox根据应用程序命名空间和作用域进行过滤，在这张图中，我们查
看的应用程序叫作“platform.summonercore”。我们可以看到应用程序实
例是如何分布的，包括它在AMS1中使用了多种部署作用域进行部署。比
如，“lolriot.ams1.rusummoner”和“lotriot.ams1.tr1summoner”分别用来
支持俄罗斯和土耳其。
右边的侧边栏包含了额外的信息，如Pack中包含的容器数量、IP地
址、基本状态、日期信息，等等。我们还能在上面查看容器日志。
这张图展示的是我最喜欢的一个功能。在加载日志过程中，我们可以
看到一张GIF图片——跳舞的Katarina（游戏中的一个角色）。没错，她会
出现在我们内部各种工具的加载屏幕上。
Toolbox中的度量指标系统为我们提供了一站式的核心服务信息查看
功能，如查看服务状态和服务位置。在发生问题时，我们可以立即进行问
题诊断。我们还能进行截屏，如下图所示。
管理复杂的网络规则
在之前的章节中，我们介绍了我们是如何通过Contrail和JSON配置
文件来管理网络的。JSON很不错，但如果文件太长，阅读起来十分不方
便。为了方便工程师查看JSON文件，我们开发了一个可视化工具，并把
它叫作“network.rcluster”。
在登录系统之后，可以看到一行一行的部件，它们表示不同集群的网
路规则。每一个部件都是通过一个JSON文件来生成的。现在让我们仔细
看一下之前提到的Summonercore。
初一看好像没有什么，它只不过是一个部署作用域清单而已。我们可
以看到Summoner有很多部署作用域网络规则。因为在有《英雄联盟》的
地方，都会有Summoner，所以这些网络规则是必需的。
如果选择其中一条规则，我们可以看到Summoner的访问权限设置。
这里有非常多的路由规则，我们可以查看端口信息和流进流出的网
络连接。从这张图中，我们可以看到Summoner可以与“rtp.collector”
和”infrastructurous.discoverous”发生交互，后者是我们的发现服务。这
张截图是从QA环境中获取的，所以还能从中看到一些测试应用程序。
全局搜索
运行如此多的服务，如何跟踪这些服务就会成为一个问题。我们可以
使用Toolbox来查看每个集群，但它只显示了运行中的Pack和容器。Riot还
有很多遗留的系统部署在物理机上，我们希望能够把它们也管理起来。
于是，我们开发了查询服务，或者叫做信息聚合器。我们把这个工具
叫作“services.rcluster”，它为我们提供了各种基于上下文的搜索服务。
下图展示了我们使用这个工具在全球范围内查找Summoner服务。
这里需要澄清的是，查询服务与服务发现是不一样的。它提供的是
基于上下文的搜索，用于查找未注册的服务。例如，它会扫描Admiral调
度器，返回匹配的结果。如果你只记得“platform.summonercore”中的
“summoner”，那么就可以用这个字符串来查找这个服务。
在这张图上可以看到“Location”列，它显示了命名空间作用域。服
务名称显示的是应用程序作用域。
构建跟踪器
我们已经介绍了我们是如何管理生产环境服务的。不过，这些服务在
进入生产环境之前还有很长的路要走。我们每年要进行一百万次构建，如
果没有一个跟踪系统，事情会变得一团糟。
Buildtracker是另一个基于API和Web驱动的工具，开发团队可以通过
自动或手动的方式提交或查询数据。他们通过这个工具来跟踪他们的代码
和构建过程。
我可以为这个写一本书，这也是第一次公开介绍这个工具，但其实我
们已经用这个工具3、4年时间了——早在使用微服务之前就开始使用它。
因为要构建的服务太多了，我们不希望开发团队通过无数的构建管道
日志来跟踪服务的构建过程。Buildtracker提供了一组干净的API，可与持
续集成系统（或任何自动化部署系统）集成起来，进行构建、打标签、查
询。
如果某个团队决定要开发一个微服务，就可以先生成一个微服务构建
管道。他们也可以自己搭建构建管道，然后使用这组API进行跟踪。他们
可以使用API来查询构建结果，如下图所示：
这张图显示的是我们的配置服务。我们添加了很多种过滤器，如变更
列表、在构建时使用的版本和其他各种标签。标签可用于跟踪多种行为，
如应用文件的部署环境（红色部分）和QA传入的事件（灰色部分）。开
发团队可以使用Buildtracker标签来标记各个构建是否已经通过测试，然后
过滤出已经通过测试的构建。团队因此可以创建出可信任的持续交付管
道，确保只部署已经经过严格测试的项目。
即使团队不会完全采用这个流程，他们仍然可以查看构建历史的详细
信息。
上图中包含了部署文件的路径、构建作业的链接和各个事件的时间
线。我们可以从Buildtracker的Release Management视图中看到个多元数据
信息。
这张图片展示了我们的一个发布团队在管理《英雄联盟》版本发布时
的内容。其中包含了客户端、服务器、音效包和服务信息。从中还能看到
很多标签，如补丁标签、环境标签、QA流程标签等。
在开发大量的服务和应用时，有这样的一个聚合器将会为你带来很大
的帮助。
在这一章中介绍的大部分工具都是自动化运行的。有一些是可选择
的，团队可以根据需要决定要不要使用它们。我们的策略是，如果一个工
具很有用，那么团队就会用它，而不是自己再去开发。这种灵活、敏捷的
氛围让我们可以集中精力为团队创建最有价值的工具，真正给他们带来帮
助。而借助这些工具，产品团队就可以快速地将新的功能交付给玩家。