title:A Programming Framework for Differential Privacy with Accuracy Concentration
Bounds
author:Elisabet Lobo Vesga and
Alejandro Russo and
Marco Gaboardi
2020 IEEE Symposium on Security and Privacy
A Programming Framework for Differential Privacy
with Accuracy Concentration Bounds
Chalmers University of Technology, Sweden
Elisabet Lobo-Vesga, Alejandro Russo
Email: {elilob, russo}@chalmers.se
Marco Gaboardi
Boston University, USA
Email: PI:EMAIL
Abstract—Differential privacy offers a formal framework for
reasoning about privacy and accuracy of computations on private
data. It also offers a rich set of building blocks for constructing
private data analyses. When carefully calibrated, these anal-
yses simultaneously guarantee the privacy of the individuals
contributing their data, and the accuracy of the data analyses
results, inferring useful properties about the population. The
compositional nature of differential privacy has motivated the
design and implementation of several programming languages
aimed at helping a data analyst in programming differentially
private analyses. However, most of the programming languages
for differential privacy proposed so far provide support for
reasoning about privacy but not for reasoning about the accuracy
of data analyses. To overcome this limitation,
in this work
we present DPella, a programming framework providing data
analysts with support for reasoning about privacy, accuracy and
their trade-offs. The distinguishing feature of DPella is a novel
component which statically tracks the accuracy of different data
analyses. In order to make tighter accuracy estimations, this
component leverages taint analysis for automatically inferring
statistical independence of the different noise quantities added for
guaranteeing privacy. We evaluate our approach by implementing
several classical queries from the literature and showing how data
analysts can ﬁgure out the best manner to calibrate privacy to
meet the accuracy requirements.
Keywords-accuracy; concentration bounds; differential pri-
vacy; functional programming; databases; haskell
I. INTRODUCTION
Differential privacy (DP) [1] is emerging as a viable solution
to release statistical information about the population without
compromising data subjects’ privacy. A standard way to achieve
DP is adding some statistical noise to the result of a data
analysis. If the noise is carefully calibrated, it provides a privacy
protection for the individuals contributing their data, and at
the same time it enables the inference of accurate information
about the population from which the data are drawn. Thanks
to its quantitative formulation quantifying privacy by means of
the parameters  and δ, DP provides a mathematical framework
for rigorously reasoning about the privacy-accuracy trade-offs.
The accuracy requirement is not baked in the deﬁnition of DP,
rather it is a constraint that is made explicit for a speciﬁc task
at hand when a differentially private data analysis is designed.
An important property of DP is composeability: multiple
differentially private data analyses can be composed with a
graceful degradation of the privacy parameters  and δ. This
property allows to reason about privacy as a budget: a data
analyst can decide how much privacy budget (the  parameter)
to assign to each of her analyses. The compositionality
aspects of DP motivated the design of several programming
frameworks [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] and
tools [14, 15, 16, 17] with built-in basic data analyses to help
analysts to design their own differentially private consults. At a
high level, most of these programming frameworks and tools are
based on a similar idea for reasoning about privacy: use some
primitives for basic tasks in DP as building blocks, and use com-
position properties to combine these building blocks making
sure that the privacy cost of each data analysis sum up and that
the total cost does not exceed the privacy budget. Programming
frameworks such as [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]
also provide general support to further combine, through
programming techniques, the different building blocks and
the results of the different data analyses. Differently, tools such
as [14, 15, 16, 17] are optimized for speciﬁc tasks at the price
of restricting the kinds of data analyses they can support.
Unfortunately, this simple approach for privacy cannot be
directly applied to accuracy. Reasoning about accuracy is less
compositional than reasoning about privacy, and it depends
both on the speciﬁc task at hand and on the speciﬁc accuracy
measure that one is interested in offering to data analysts.
Despite this, when restricted to speciﬁc mechanisms and
speciﬁc forms of data analyses, one can measure accuracy
through estimates given as conﬁdence intervals, or error bounds.
As an example, most of the standard mechanisms from the
differential privacy literature come with theoretical conﬁdence
intervals or error bounds that can be exposed to data analysts
in order to allow them to take informed decisions about the
consults they want to run. This approach has been integrated
in tools such as GUPT [15], PSI [17], and Apex [18]. Users
of these tools, can specify the target conﬁdence interval they
want to achieve, and the tools adjust accordingly the privacy
parameters, when sufﬁcient budget is available1.
In contrast, all the programming frameworks proposed so
far [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] do not offer any support
to programmers or data analysts for tracking, and reasoning
about, the accuracy of their data analyses. This phenomenon is
in large part due to the complex nature of accuracy reasoning,
with respect to privacy analyses, when designing arbitrary data
analyses that users of these frameworks may want to program
1Apex actually goes beyond this by also helping user by selecting the right
differentially private mechanism to achieve the required accuracy.
© 2020, Elisabet Lobo Vesga. Under license to IEEE.
DOI 10.1109/SP40000.2020.00086
411
Authorized licensed use limited to: Carleton University. Downloaded on August 06,2020 at 17:57:38 UTC from IEEE Xplore.  Restrictions apply. 
and run. In this paper, we address this limitation by building
a programming framework for designing differentially private
analysis which also supports a compositional form of reasoning
about accuracy.
Our Contribution
Our main contribution is showing how programming frame-
works can internalize the use of probabilistic bounds [19]
for composing different conﬁdence intervals or error bounds,
in an automated way. Probabilistic bounds are part of the
classical toolbox for the analysis of randomized algorithms,
and are the tools that differential privacy algorithms designers
usually employ for the accuracy analysis of classical mech-
anisms [20, 21]. Two important probabilistic bounds are the
union bound, that can be used to compose errors with no
assumption on the way the random noise is generated, and
Chernoff bound, which applies to the sum of random noise when
the different random variables characterizing noise generation
are statistically independent (see Section IV). When applicable,
and when the number of random variables grows, Chernoff
bound usually gives a much “tighter” error estimation than the
union bound.
Barthe et. al [22] have shown how the union bound can
be internalized in a Hoare-style logic for reasoning about
probabilistic imperative programs, and how this logic can
be used to reason in a mechanized way about the accuracy
of probabilistic programs, and in particular of programs
implementing differentially private primitives.
Building on this idea, we propose a programming framework
where this kind of reasoning is automated, and can be combined
with reasoning about privacy. The aim of our framework is to
offer programmers the tools that they need for implementing
differentially private data analyses and explore their privacy-
accuracy trade-offs, in a compositional way. Our framework
supports not only the use of union bound as a reasoning
principle, but also the use of the Chernoff bound. Our insight is
that probabilistic bounds relying on probabilistic independence
of random variables can be smoothly integrated in a program-
ming framework by using techniques from information-ﬂow
control [23] (in the form of taint analysis [24]). While these
probabilistic bounds are not enough to express every accuracy
guarantee one wants to express for arbitrary data analyses, they
allow the analysis of a large class of user-designed programs.
Our approach allow programmers to exploit the compositional
nature of both privacy and utility, complementing in this way
the support provided by tools such as GUPT [15], PSI [17],
which provide conﬁdence intervals estimate only at the level of
individual queries, and by Apex [18], which provide conﬁdence
intervals estimate only at the level of a query workload for
queries of the same type.
We implement our ideas into a programming framework
called DPella—an acronym for Differential Privacy in Haskell
with accuracy—where data analysts can explore the privacy-
accuracy trade-off while writing their differentially private data
analyses. DPella provides several basic differentially private
building blocks and composition techniques, which can be used
by a programmer to design complex differentially private data
analyses. The analyses that can be expressed in DPella are data
independent and can be built using primitives for counting,
average, max as well as any aggregation of their results. DPella
supports both pure-DP, with parameter , and approximate-DP,
with parameters  and δ. Accordingly, it supports the use of
both Laplace and Gaussian noise, and the use of sequential or
advanced [20] composition, respectively, together with parallel
composition for both notions. For simplicity, in the main part
of the paper we focus only on -DP and we discuss the use of
the Laplace mechanism. DPella is implemented as a library in
the general purpose language Haskell; a programming language
that is well-known to easily support information-ﬂow analyses
[25, 26]. Furthermore, DPella is designed to be extensible
through the addition of new primitives (see Section VI).
To reason about privacy and accuracy, DPella provides
two primitives responsible to symbolically interpret programs
(which implement data analyses). DPella’s symbolic interpreta-
tion for privacy consists on decreasing the privacy budget of a
query by deducing the required budget of its sub-parts. On the
other hand, the accuracy interpretation uses as abstraction the
inverse Cumulative Distribution Function (iCDF) representing
an upper bound on the (theoretical) error that the program incurs
when guaranteeing DP. The iCDF of a query is build out of
the iCDFs of the different components, by using as a basic
composition principle the union bound. These interpretations
provide overestimates of the corresponding quantities that they
track. In order to make these estimates as precise as possible,
DPella uses taint analysis to track the use of noise to identify
which variables are statistically independent. This information
is used by DPella to soundly replace, when needed, the union
bound with the Chernoff bound, something that to the best of
our knowledge other program logics or program analyses also
focusing on accuracy, such as [22] and [27], do not consider. We
envision DPella’s accuracy estimations to be used in scenarios
which align with those considered by tools like GUPT, PSI,
and Apex.
In summary, our contributions are:
(cid:2) We present DPella, a programming framework that allows
data analysts to reason compositionaly about privacy-accuracy
trade-off.
(cid:2) We show how to use taint analysis to detect statistical
independence of the noise that different primitives add, and
how to use this information to achieve better error estimates.
(cid:2) We inspect DPella’s expressiveness and error estimations by
implementing PINQ-like queries from previous work [28, 2, 29]
and workloads from the matrix mechanism [30, 31, 32].
II. DPELLA BY EXAMPLE
We start by providing a brief background on the notions of
privacy and accuracy DPella considers.
A. Background
Differential privacy [1] is a quantitative notion of privacy
that bounds how much a single individual’s private data can
Authorized licensed use limited to: Carleton University. Downloaded on August 06,2020 at 17:57:38 UTC from IEEE Xplore.  Restrictions apply. 
412
affect the result of a data analysis. More formally, we can
deﬁne differential privacy as a property of a randomized query
˜Q(·) representing the data analysis, as follow.
Deﬁnition II.1 (Differential Privacy (DP)[1]). A randomized
query ˜Q(·) : db → R satisﬁes -differential privacy if and only
if for any two datasets D1 and D2 in db, which differ in one
row, and for every output set S ⊆ R we have
Pr[ ˜Q(D1) ∈ S] (cid:3) e Pr[ ˜Q(D2) ∈ S]
(1)
In the deﬁnition above, the parameter  determines a bound
on the distance between the distributions induced by ˜Q(·) when
adding or removing an individual from the dataset—the farther
away they are, the more at risk the privacy of an individual is,
and vice versa. In other words,  imposes a limit on the privacy
loss that an individual can incur in, as a result of running a
data analysis.
A standard way to achieve -differential privacy is adding
some carefully calibrated noise to the result of a query. To
protect all the different ways in which an individual’s data can
affect the result of a query, the noise needs to be calibrated to
the maximal change that the result of the query can have when
changing an individual’s data. This is formalized through the
notion of sensitivity.
Deﬁnition II.2 ([1]). The (global) sensitivity of a query Q(·) :
db → R is the quantity ΔQ = max{|Q(D1) − Q(D2)| for
D1, D2 differing in one row
The sensitivity gives a measure of the amount of noise
needed to protect one individual’s data. Besides, in order to
achieve differential privacy, it is also important the choice of
the kind of noise that one adds. A standard approach is based
on the addition of noise sampled from the Laplace distribution.
Theorem II.1 (Laplace Mechanism [1]). Let Q(·) : db → R be
a deterministic query with sensitivity ΔQ. Let ˜Q(·) : db → R
be a randomized query deﬁned as ˜Q(D) = Q(D) + η, where
η is sample from the Laplace distribution with mean μ = 0
and scale b = ΔQ/. Then ˜Q is -differentially private.
Notice that in the theorem above, for a given query, the
smaller the  is, the more noise ˜Q(·) needs to inject in order to
hide the contribution of one individual’s data to the result—this
protects privacy but degrades how meaningful the result of the
query is—and vice versa. In general, the notion of accuracy
can be deﬁned more formally as follows.
see e.g.[20]). Given an -
Deﬁnition II.3 (Accuracy,
differentiallly private query ˜Q(·), a target query Q(·), a
distance function d(·), a bound α, and the probability β, we
say that ˜Q(·) is (d(·), α, β)-accurate with respect to Q(·) if
and only if for all dataset D:
Pr[d( ˜Q(D) − Q(D)) > α] (cid:3) β
(2)
This deﬁnition allows one to express data independent error
statements such as: with probability at least 1 − β the query
˜Q(D) diverge from Q(D), in terms of the distance d(·), for
less than α. Then, we will refer to α as the error and 1 − β
as the conﬁdence probability or simply conﬁdence. In general,
the lower the β is, i.e., the higher the conﬁdence probability
is, the higher the error α is.
As previously discussed, an important property of differential
privacy is composeability.
Theorem II.2 (Sequential Composition [1]). Let ˜Q1(·) and
˜Q2(·) be two queries which are 1- and 2-differentially
private, respectively. Then, their sequential composition ˜Q(·) =
( ˜Q1(·), ˜Q2(·)) is (1 + 2)-differentially private.
Theorem II.3 (Parallel Composition [2]). Let ˜Q(·) be a -
differentially private query. and data1, data2 be a partition
the query ˜Q1(D) = ( ˜Q(D ∩
of
data1), ˜Q(D ∩ data2)) is -differentially private.
the set of data. Then,
Thanks to the composition properties of differential privacy,
we can think about  as a privacy budget that one can spend on
a given data before compromising the privacy of individuals’
contributions to that data. The global  for a given program
can be seen as the privacy budget for the entire data. This
budget can be consumed by selecting the local  to “spend” in
each intermediate query. Thanks to the composition properties,
by tracking the local  that are consumed, one can guarantee
that a data analysis will not consume more than the allocated
privacy budget.
Given an , DPella gives data analysts the possibility to
explore how to spend it on different queries and analyze the
impact on accuracy. For instance, data analysts might decide to
spend “more” epsilon on sub-queries which results are required
to be more accurate, while spending “less” on the others. The
next examples (inspired by the use of DP in network trace
analyses [28]) show how DPella helps to quantify what “more”
and “less” means.
B. Example: CDF
Suppose we have a tcpdump trace of packets which yields a
table where each row is represented as list of String values
containing the following information:
[, , , , ,
, ]
From this table, we would like to inspect—in a differentially
private manner—the packet’s length distribution by comput-
ing its Cumulative Distribution function (CDF), deﬁned as
CDF(x) = number of records with value (cid:3) x. Hence, we are
just interested in the values of the attribute .
McSherry and Mahajan [28] proposed three different ways
to approximate (due to the injected noise) CDFs with DP, and
they argued for their different levels of accuracy. We revise
two of these approximations (the third one can be found in
the extended version of the paper) to show how DPella can
assist in showing the accuracy of these analyses.
1) Sequential CDF: A simple approach to compute the CDF
consists in splitting the range of lengths into bins and, for
each bin, count the number of records that are (cid:3) bin. A
natural way to make this computation differentially private is
to add independent Laplace noise to each count.
Authorized licensed use limited to: Carleton University. Downloaded on August 06,2020 at 17:57:38 UTC from IEEE Xplore.  Restrictions apply. 
413
sizes ← dpSelect getPktLen dataset