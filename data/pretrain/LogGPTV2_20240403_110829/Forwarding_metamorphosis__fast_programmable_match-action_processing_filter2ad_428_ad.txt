Section 5.5. To get there, we consider the contributors to
cost by considering the parser, the match stages and the
action processing in turn.
5.1 Programmable Parser Costs
Programmability comes at a cost. A conventional parser
is optimized for one parse graph, whereas a programmable
parser must handle any supported parse graph. Cost is eval-
uated by comparing synthesis results for conventional and
programmable designs. Total gate count is shown in Fig-
ure 6 for conventional parsers implementing several parse
graphs and a programmable parser. We assume parser ag-
gregate throughput of 640 Gb/s by combining 16 instances
of a 40 Gb/s parser running at 1 GHz. The result module
in all designs contains the 4Kb packet header vector dur-
ing parsing. The programmable parser uses a 256 × 40 bit
TCAM and a 256 × 128 bit action RAM.
Parser gate count is dominated by logic for populating the
parser header vector. The conventional design requires 2.9–
3.0 million gates, depending upon the parse graph, while the
programmable design requires 5.6 million gates, of which 1.6
Stage 1Stage 2MatchActionMatchActionTimeMatchActionMatchActionStage 1Stage 2MatchActionMatchActionStage 1Stage 2Next, while a TCAM typically has an area 6–7× that
of an equivalent bitcount SRAM, both ternary and binary
ﬂow entries have other bits associated with them, including
action memory, statistics counters, and instruction, action
data, and next table pointers. For example, with 32 bits of
IP preﬁx, 48 bits of statistics counter, and 16 bits of action
memory (say for specifying next hops), the TCAM portion
is only 1/3 of the memory bitcount and so the area penalty
for TCAM drops to around 3×.
While 3× is signiﬁcant, given that 32b (IPv4) or 128b
(IPv6) longest preﬁx matching and ACLs are major use
cases in all existing routers, devoting signiﬁcant resources
to TCAM to allow say 1M IPv4 preﬁxes or 300K ACLs
seems useful. While we could have used just SRAM with
special purpose LPM algorithms instead as in [6], achieving
the single-cycle latency of TCAMs for a 32 or 128 bit LPM
is diﬃcult or impossible. Nevertheless, deciding the ratio
of ternary to binary table capacity (our chip proposes a 1:2
ratio) is an important implementation decision with signiﬁ-
cant cost implications, for which currently there is little real
world feedback.
5.2.2 Costs of Action Speciﬁcation
Besides the ﬂow entry, each match table RAM entry also
has a pointer to action memory (13b), an action size (5b),
a pointer to instruction memory (5b for 32 instructions),
and a next table address (9b). These extra bits represent
approximately 35% overhead for the narrowest ﬂow entries.
There are also bits for version and error correction but these
are common to any match table design so we ignore them.
In addition to overhead bits in a ﬂow entry, other mem-
ories are required for storing actions and statistics. These
add to the total overhead—the ratio of total bits required to
just the match ﬁeld bits— but both of these extra costs can
sometimes be reduced. We will show how in some cases it
is possible to reduce ﬂow entry overhead bits. Furthermore,
applications require varying amounts of action memory, and
sometimes statistics are not needed, so these memory costs
can be reduced or eliminated.
Given the variable conﬁguration of memory blocks be-
tween match, action, and statistics, we use a few conﬁgu-
ration examples to see how the bookkeeping overhead varies
compared to non-conﬁgurable ﬁxed allocations.
In the ﬁrst conﬁguration, shown in Figure 7a and Table 2,
32 memory blocks are used for match memory in a stage,
implementing 32K 80b wide exact match ﬂow entries. An-
other 16K 80b ternary entries are in the TCAM modules.
All ﬂow entries have action entries of the same size, requir-
ing 48 memories for actions. Statistics consume 24 memory
banks, along with a spare bank for multiporting the statistics
memory. An approximately equal portion of action memory
is allocated for each match memory, and might be considered
a base case with a minimum amount of ﬂow table capacity.
Excluding the 24 banks used for ternary actions and statis-
tics in case a, 40% of the banks used for binary operations
are match tables, indicating a 2.5× overhead. Compound-
ing this with the 35% bit overhead in the match tables, the
total binary overhead is 3.375×, the ratio of total bitcount
to match data bitcount. In other words, only a third of the
RAM bits can be used for ﬂow entries.
Cases a2 and a3 of Table 2 change the match width to
160 and 320 bits respectively, reducing action and statistics
requirements, and yielding increased match capacity.
Figure 6: Total gate count of parsers providing 640 Gb/s
aggregate throughput.
million is contributed by the added TCAM and action RAM
modules. From these results, the cost of parser programma-
bility is approximately 2 (5.6/3.0 = 1.87 ≈ 2).
Despite doubling the parser gate count, the parser ac-
counts for less than 1% of the chip area, so the cost of mak-
ing the parser programmable is not a concern.
5.2 Memory Costs
There are several costs to memories the reader may be
concerned about. First, there is the cost of the memory tech-
nology itself (hash table, TCAMs) versus standard SRAM
memory and the cost of breaking up the memory into smaller
blocks which can be reconﬁgured; second, there is the cost of
additional data needed in each match table entry to specify
actions and keep statistics; and third, there is the cost of in-
ternal fragmentation such as when an Ethernet Destination
address of 48 bits is placed in a 112-bit wide memory. We
treat each overhead in turn and speciﬁcally point out the
(small) additional cost for programmability.
In what fol-
lows we refer to an entry in a match table (such as a 48-bit
Ethernet DA) as a ﬂow entry.
5.2.1 Memory Technology Costs
Exact matching: We use cuckoo hashing for exact match-
ing because its ﬁll algorithm provides high occupancy, typ-
ically above 95% for 4-way hashtables. Cuckoo hashtables
resolve ﬁll conﬂicts by recursively evicting conﬂicting entries
to other locations. Additionally, while our memory system is
built up out of 1K by 112 bit RAM blocks for conﬁgurability,
one might expect an area penalty vs using larger, more eﬃ-
cient memory units. However, using 1K RAM blocks incurs
an area penalty of only about 14% relative to the densest
SRAM modules available for this technology.
Wildcard matching: We use large amounts of TCAM
on chip to directly support wildcard matching such as preﬁx
matching and ACLs. TCAM is traditionally thought to be
infeasible due to power and area concerns. However, TCAM
operating power has been reduced by about 5× by newer
TCAM circuit design techniques [1]. Thus, in the worst
case, at the maximum packet rate with minimum packet size
on all channels, TCAM power is one of a handful of major
contributors to total chip power; at more typical mixtures
of long and short packets, TCAM power reduces to a small
percentage of the total.
Conventional:SimpleConventional:EnterpriseConventional:CorerouterConventional:DatacenterConventional:ServiceproviderConventional:CompositeProgrammable0123456Gates(×106)ResultHdr.Ident./FieldExtract.ActionRAMTCAMstatic values may be conﬁgured for these attributes, allowing
14 bits of instruction and next-table pointer to be reclaimed
for match. More generally, a conﬁgurable width ﬁeld in the
ﬂow entry can optionally provide LSBs for action, instruc-
tion, or next-table addresses, allowing a reduced number of
diﬀerent instructions or actions, or addressing a small array
for next table, while reclaiming as many instruction, next-
table, and action address bits as possible given the complex-
ity of the function.
Next, tables can provide an action value as an immediate
constant rather than as a pointer to action memory for small
constants, saving pointer and action memory bits.
A simple mechanism enables these optimizations: match
table ﬁeld ﬁeld boundaries can be ﬂexibly conﬁgured, al-
lowing a range of table conﬁgurations with arbitrary sizes
for each ﬁeld, subject to a total bitwidth constraint. Tables
with ﬁxed or almost ﬁxed functions can be eﬃciently im-
plemented with almost no penalty compared to their ﬁxed
counterparts.
5.2.3 Crossbar Costs
A crossbar within each stage selects the match table in-
puts from the header vector. A total of 1280 output bits
(640b for each of TCAM and hash table) are selected from
the 4Kb input vector. Each output bit is driven by a 224
input multiplexor, made from a binary tree of and-or-invert
AOI22 gates (with the logic function AB + CD), and cost-
ing one 0.65µm2 per mux input. Total crossbar area is
1280 × 224 × 0.65µm2 × 32 stages ≈ 6 mm2. Area com-
putation for the action unit data input muxes is similar.
5.3 Fragmentation Costs
A ﬁnal overhead is internal fragmentation or packing costs.
Clearly, a 48-bit Ethernet Destination Address placed in
an 112 b wide memory wastes more than half the mem-
ory.
In comparison, a ﬁxed-function Ethernet bridge con-
tains custom 48-bit wide RAM. Thus this cost is squarely
attributable to programmability and our choice of 112b wide
RAMs. One could reduce this overhead for Ethernet by
choosing 48b as the base RAM width, but how can a chip
designed for general purpose use (and future protocols) pre-
dict future match identiﬁer widths?
Fortunately, even this overhead is reduced through an-
other architectural trick that allows sets of ﬂow entries to
be packed together without impairing the match function.
For example, the standard TCP 5-tuple is 104 bits wide.
Three of these entries can be packed into four memory units
of width 448 b, rather than separately requiring each to
consume two memory units. Or, with low entry overhead
equivalent to a simple pass/fail, 4 of these can be packed
into 4 words, due to amortization of ECC bits over wider
data. Fundamentally, this is possible eﬃciently because un-
der the covers, longer matches are constructed from trees
of smaller 8-bit matches; ﬂexibility only slightly complicates
this logic.
The combination of these two techniques, variable data
packing into a data word (to reduce action speciﬁcation
costs) and variable ﬂow entry packing into multiple data
words (to reduce fragmentation costs), assures eﬃcient mem-
ory utilization over a wide range of conﬁgurations. In sum-
mary, while conventional switches have highly eﬃcient im-
plementations for speciﬁc conﬁgurations of tables, this ar-
chitecture can approach that eﬃciency, not only for those
Figure 7: Match stage unit memory map examples.
Conﬁguration b of Table 2 further increases the binary
and ternary ﬂow table match widths to 640 bits, as shown in
Figure 7b, (memory width is shown horizontally), reducing
the number of ﬂow entries by a factor of 8 compared to
the base case, along with the required action and statistics
capacity. While such a wide match may be rare (say for
an entire header match), we see that with 8× wider ﬂow
entries than the base case above, 80 banks can be used for
exact match, 75% of memory capacity, 2.5× higher table
capacity than the base case.
Conﬁguration c1 of table 2, shown in Figure 7c, takes ad-
vantage of a use case where the number of individual actions
is limited. For example, a data center address virtualiza-
tion application requires a large number of ﬂow entries, but
match entries may point to one of only 1000 possible destina-
tion top of rack switches. Thus 4K of action memory would
suﬃce. That would allow 62 memory blocks for match and
40 for statistics, almost doubling the number of exact match
entries from the base case. If statistics were not desired, 102
memory blocks could be used for match as shown in table
entry c2, 96% of the total memory capacity.
Case MatchWidth Match Action
48
34
22
12
4
4
32
52
72
80
62
102
a1
a2
a3
b
c1
c2
80
160
320
640
80
80
Stats Relative
1.000×
1.625×
2.250×
2.500×
1.900×
3.250×
25
18
12
7
40
0
Table 2: Memory unit allocation and relative exact match
capacity.
In short, ﬂow entry density is greatly increased if actions
or statistics are reduced or eliminated.
If the user of the
chip deems these complex actions and statistics necessary,
then it is unfair to blame the chip conﬁgurability options
for this overhead. The only fundamental book-keeping costs
that can be directly attributed to programmability are the
instruction pointer (5b) and the next table address (9b),
which is around 15%.
Costs can be further reduced in tables with ﬁxed behav-
iors. A ﬁxed-function table uses the same instruction and
next-table pointers for all entries, e.g., L2 dest MAC table;