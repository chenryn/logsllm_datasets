在每次迭代中，为每个顶点呼叫一个函式，将所有传送给它的讯息传递给它 —— 就像呼叫 Reducer 一样。与 MapReduce 的不同之处在于，在 Pregel 模型中，顶点在一次迭代到下一次迭代的过程中会记住它的状态，所以这个函式只需要处理新的传入讯息。如果图的某个部分没有被传送讯息，那里就不需要做任何工作。
这与 Actor 模型有些相似（请参阅 “[分散式的 Actor 框架](ch4.md#分散式的Actor框架)”），除了顶点状态和顶点之间的讯息具有容错性和永续性，且通讯以固定的回合进行：在每次迭代中，框架递送上次迭代中传送的所有讯息。Actor 通常没有这样的时序保证。
#### 容错
顶点只能透过讯息传递进行通讯（而不是直接相互查询）的事实有助于提高 Pregel 作业的效能，因为讯息可以成批处理，且等待通讯的次数也减少了。唯一的等待是在迭代之间：由于 Pregel 模型保证所有在一轮迭代中传送的讯息都在下轮迭代中送达，所以在下一轮迭代开始前，先前的迭代必须完全完成，而所有的讯息必须在网路上完成复制。
即使底层网路可能丢失、重复或任意延迟讯息（请参阅 “[不可靠的网路](ch8.md#不可靠的网路)”），Pregel 的实现能保证在后续迭代中讯息在其目标顶点恰好处理一次。像 MapReduce 一样，框架能从故障中透明地恢复，以简化在 Pregel 上实现演算法的程式设计模型。
这种容错是透过在迭代结束时，定期存档所有顶点的状态来实现的，即将其全部状态写入持久化储存。如果某个节点发生故障并且其记忆体中的状态丢失，则最简单的解决方法是将整个图计算回滚到上一个存档点，然后重启计算。如果演算法是确定性的，且讯息记录在日志中，那么也可以选择性地只恢复丢失的分割槽（就像之前讨论过的资料流引擎）【72】。
#### 并行执行
顶点不需要知道它在哪台物理机器上执行；当它向其他顶点发送讯息时，它只是简单地将讯息发往某个顶点 ID。图的分割槽取决于框架 —— 即，确定哪个顶点执行在哪台机器上，以及如何透过网路路由讯息，以便它们到达正确的地方。
由于程式设计模型一次仅处理一个顶点（有时称为 “像顶点一样思考”），所以框架可以以任意方式对图分割槽。理想情况下如果顶点需要进行大量的通讯，那么它们最好能被分割槽到同一台机器上。然而找到这样一种最佳化的分割槽方法是很困难的 —— 在实践中，图经常按照任意分配的顶点 ID 分割槽，而不会尝试将相关的顶点分组在一起。
因此，图演算法通常会有很多跨机器通讯的额外开销，而中间状态（节点之间传送的讯息）往往比原始图大。透过网路传送讯息的开销会显著拖慢分散式图演算法的速度。
出于这个原因，如果你的图可以放入一台计算机的记忆体中，那么单机（甚至可能是单执行绪）演算法很可能会超越分散式批处理【73,74】。图比记忆体大也没关系，只要能放入单台计算机的磁碟，使用 GraphChi 等框架进行单机处理是就一个可行的选择【75】。如果图太大，不适合单机处理，那么像 Pregel 这样的分散式方法是不可避免的。高效的并行图演算法是一个进行中的研究领域【76】。
### 高阶API和语言
自 MapReduce 开始流行的这几年以来，分散式批处理的执行引擎已经很成熟了。到目前为止，基础设施已经足够强大，能够储存和处理超过 10,000 台机器丛集上的数 PB 的资料。由于在这种规模下物理执行批处理的问题已经被认为或多或少解决了，所以关注点已经转向其他领域：改进程式设计模型，提高处理效率，扩大这些技术可以解决的问题集。
如前所述，Hive、Pig、Cascading 和 Crunch 等高阶语言和 API 变得越来越流行，因为手写 MapReduce 作业实在是个苦力活。随著 Tez 的出现，这些高阶语言还有一个额外好处，可以迁移到新的资料流执行引擎，而无需重写作业程式码。Spark 和 Flink 也有它们自己的高阶资料流 API，通常是从 FlumeJava 中获取的灵感【34】。
这些资料流 API 通常使用关系型构建块来表达一个计算：按某个栏位连线资料集；按键对元组做分组；按某些条件过滤；并透过计数求和或其他函式来聚合元组。在内部，这些操作是使用本章前面讨论过的各种连线和分组演算法来实现的。
除了少写程式码的明显优势之外，这些高阶介面还支援互动式用法，在这种互动式使用中，你可以在 Shell 中增量式编写分析程式码，频繁执行来观察它做了什么。这种开发风格在探索资料集和试验处理方法时非常有用。这也让人联想到 Unix 哲学，我们在 “[Unix 哲学](#Unix哲学)” 中讨论过这个问题。
此外，这些高阶介面不仅提高了人类的工作效率，也提高了机器层面的作业执行效率。
#### 向宣告式查询语言的转变
与硬写执行连线的程式码相比，指定连线关系运算元的优点是，框架可以分析连线输入的属性，并自动决定哪种上述连线演算法最适合当前任务。Hive、Spark 和 Flink 都有基于代价的查询最佳化器可以做到这一点，甚至可以改变连线顺序，最小化中间状态的数量【66,77,78,79】。
连线演算法的选择可以对批处理作业的效能产生巨大影响，而无需理解和记住本章中讨论的各种连线演算法。如果连线是以 **宣告式（declarative）** 的方式指定的，那这就这是可行的：应用只是简单地说明哪些连线是必需的，查询最佳化器决定如何最好地执行连线。我们以前在 “[资料查询语言](ch2.md#资料查询语言)” 中见过这个想法。
但 MapReduce 及其资料流后继者在其他方面，与 SQL 的完全宣告式查询模型有很大区别。MapReduce 是围绕著回拨函式的概念建立的：对于每条记录或者一组记录，呼叫一个使用者定义的函式（Mapper 或 Reducer），并且该函式可以自由地呼叫任意程式码来决定输出什么。这种方法的优点是可以基于大量已有库的生态系统创作：解析、自然语言分析、影象分析以及执行数值或统计算法等。
自由执行任意程式码，长期以来都是传统 MapReduce 批处理系统与 MPP 资料库的区别所在（请参阅 “[Hadoop 与分散式资料库的对比](#Hadoop与分散式资料库的对比)” 一节）。虽然资料库具有编写使用者定义函式的功能，但是它们通常使用起来很麻烦，而且与大多数程式语言中广泛使用的程式包管理器和依赖管理系统相容不佳（例如 Java 的 Maven、Javascript 的 npm 以及 Ruby 的 gems）。
然而资料流引擎已经发现，支援除连线之外的更多 **宣告式特性** 还有其他的优势。例如，如果一个回拨函式只包含一个简单的过滤条件，或者只是从一条记录中选择了一些栏位，那么在为每条记录呼叫函式时会有相当大的额外 CPU 开销。如果以宣告方式表示这些简单的过滤和对映操作，那么查询最佳化器可以利用列式储存布局（请参阅 “[列式储存](ch3.md#列式储存)”），只从磁碟读取所需的列。Hive、Spark DataFrames 和 Impala 还使用了向量化执行（请参阅 “[记忆体频宽和向量化处理](ch3.md#记忆体频宽和向量化处理)”）：在对 CPU 快取友好的内部回圈中迭代资料，避免函式呼叫。Spark 生成 JVM 位元组码【79】，Impala 使用 LLVM 为这些内部回圈生成本机程式码【41】。
透过在高阶 API 中引入宣告式的部分，并使查询最佳化器可以在执行期间利用这些来做最佳化，批处理框架看起来越来越像 MPP 资料库了（并且能实现可与之媲美的效能）。同时，透过拥有执行任意程式码和以任意格式读取资料的可扩充套件性，它们保持了灵活性的优势。
#### 专业化的不同领域
尽管能够执行任意程式码的可扩充套件性是很有用的，但是也有很多常见的例子，不断重复著标准的处理模式。因而这些模式值得拥有自己的可重用通用构建模组实现。传统上，MPP 资料库满足了商业智慧分析和业务报表的需求，但这只是许多使用批处理的领域之一。
另一个越来越重要的领域是统计和数值演算法，它们是机器学习应用所需要的（例如分类器和推荐系统）。可重用的实现正在出现：例如，Mahout 在 MapReduce、Spark 和 Flink 之上实现了用于机器学习的各种演算法，而 MADlib 在关系型 MPP 资料库（Apache HAWQ）中实现了类似的功能【54】。
空间演算法也是有用的，例如 **k 近邻搜寻（k-nearest neighbors, kNN）**【80】，它在一些多维空间中搜索与给定项最近的专案 —— 这是一种相似性搜寻。近似搜寻对于基因组分析演算法也很重要，它们需要找到相似但不相同的字串【81】。
批处理引擎正被用于分散式执行日益广泛的各领域演算法。随著批处理系统获得各种内建功能以及高阶宣告式运算元，且随著 MPP 资料库变得更加灵活和易于程式设计，两者开始看起来相似了：最终，它们都只是储存和处理资料的系统。
## 本章小结