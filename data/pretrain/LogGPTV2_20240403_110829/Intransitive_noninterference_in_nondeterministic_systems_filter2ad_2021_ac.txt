(cid:54)= { ⊥
0}
= poss(viewL | Act = β) .
0 , ⊥
1 , ⊥
0⊥ (cid:96)2
0⊥ (cid:96)2
1⊥ (cid:96)2
1⊥ (cid:96)2
⊥ (cid:96)1
⊥ (cid:96)1
⊥ (cid:96)1
⊥ (cid:96)1
0
0
1
1
Intuitively, although the policy suggests that the coalition should
not be able to distinguish between the sequences α and β, in fact
they can, using the parity of their ﬁnal observations.
There has been some informal recognition in the literature of the
relevance of coalitions when dealing with policies beyond H (cid:54)
(cid:55)→ L,
but there appear to have been very few formal studies of the issue.
(We defer discussion of the related literature to Section 8.) One of
our contributions in this paper is to pursue such a formal study in
the general setting of intransitive policies.
3.6 Intransitive Noninterference
L
H
D
is
The ﬁnal background we require concerns semantics for intran-
sitive policies: this issue has been studied primarily in the setting
of deterministic systems.
One of the main motivations for
intransitive policies
to repre-
sent the role of trusted components
within an architectural design of a
Figure 5: Policy HDL.
system. A canonical example of this
is a downgrader, a trusted component that manages declassiﬁca-
tion of high-level secrets to low-level domains. A policy for such
a system is the policy HDL = {H, D, L}2 \ {(H, L)} depicted in
Figure 5. Here D represents a trusted downgrader component.
Even in deterministic systems, NI is not an adequate deﬁnition
of security for such intransitive policies, since it implies that L can
learn nothing about H activity (not even when the downgrader per-
mits it). To give a more adequate semantics, Haigh and Young [10]
generalized the deﬁnition of the purge function to intransitive poli-
cies; we follow the formulation of Rushby [21]. Intuitively, the in-
transitive purge of a sequence of actions with respect to a domain u
s0
h
s
(cid:96)1
(cid:96)2
(cid:96)1
(cid:96)2
s1
(cid:96)2
s2
(cid:96)1
s3
s4
s5
(cid:96)2
s6
(cid:96)1
s7
s8
0
0
1
1
0
0
1
1
(cid:96)2
(cid:96)1
(cid:96)2
(cid:96)1
(cid:96)2
(cid:96)1
(cid:96)2
(cid:96)1
s9
s10
s11
s12
s13
s14
s15
s16
0
0
0
0
1
1
1
1
0
1
1
0
1
0
0
1
Figure 4: An insecure system that satisﬁes MCOR.
is the largest subsequence of actions that could form part of a causal
chain of effects (permitted by the policy) ending with an effect on
domain u. More formally, the deﬁnition makes use of a function
src : U× A∗ → P(U) deﬁned inductively by srcu() = {u} and
srcu(aα) =
srcu(α) ∪ { dom(a) | ∃v ∈ srcu(α) (dom(a) (cid:55)→ v) }
for a ∈ A and α ∈ A∗. Intuitively, srcu(α) is the set of domains
v such that there exists a sequence of permitted interferences from
v to u within α. The intransitive purge function ipu : A∗ → A∗
for each domain u ∈ U is then deﬁned inductively by ipu() = 
and, for a ∈ A and α ∈ A∗,
(cid:40)
ipu(aα) =
a · ipu(α)
ipu(α)
if dom(a) ∈ srcu(aα)
otherwise.
Haigh and Young’s deﬁnition of security uses the intransitive purge
function in place of the purge function in Goguen and Meseguer’s
deﬁnition. Using our relative information formulation, the follow-
ing is equivalent:
Deﬁnition 7. A deterministic machine M is IP-secure w.r.t. a
(possibly intransitive) policy (cid:55)→ if for all u ∈ U, in R(M ), the
function obsu ◦ last contains no more information than ipu ◦ Act
about Act.
As with NI, an equivalent statement is that for all u ∈ U, in R(M ),
the function viewu contains no more information than ipu ◦ Act
873about Act. It can be seen that ipu = purgeu when (cid:55)→ is transitive,
so IP-security is in fact a generalization of the deﬁnition of security
for transitive policies.
It has been argued by van der Meyden [24] that IP-security misses
some subtle ﬂows of information relating to the ordering of events.
A very simple example (from [25]) illustrating the problem is that
there exists an IP-secure system for the policy H (cid:55)→ D (cid:55)→ L
with actions h, d, (cid:96) in domains H, D, L respectively, in which L
makes different observation after the sequence of actions h(cid:96)d than
after the sequence of actions (cid:96)hd. Since ipL(h(cid:96)d) = h(cid:96)d and
ipL((cid:96)hd) = (cid:96)hd are also distinct, this is not a violation of IP-
security. However, it can be argued that such a system is not secure.
Intuitively, L learns whether or not the h action came before the (cid:96)
action. But, the system model is asynchronous, and according to
the policy, the only way L is permitted to learn about the h action
is via D. Since D is not permitted to know about the (cid:96) action, D
cannot securely inform L how action h was ordered with respect to
action (cid:96).
In response to this sort of example, van der Meyden proposes
an alternate semantics that is based on a function tau for each do-
main u, that is intended to capture the maximal information about
actions that domain u may have, according to the policy.
(The
name of the function and the associated notion of security abbrevi-
ate “transmission of information about actions”.) This function is
inductively deﬁned by
tau() =  and
(cid:40)
tau(αa) =
(tau(α), tadom(a)(α), a)
tau(α)
if dom(a) (cid:55)→ u
otherwise.
This deﬁnition resembles a full-information protocol, in which, when
performing an action a after sequence α, domain dom(a) sends ev-
erything that it knows (as represented by tadom(a)(α)), as well as
the fact that it has performed action a, to every domain u to which it
is permitted to transmit information. The recipient domain u adds
this new information to its existing information tau(α). Domains
to which dom(a) is not permitted to send information learn noth-
ing when a happens. This function forms the basis for a deﬁnition
of security which may be formulated using relative information as
follows:
Deﬁnition 8. A deterministic machine M is TA-secure w.r.t. a
policy (cid:55)→ if obsu◦last contains no more information than tau◦Act
about Act.
Again, it proves to be equivalent to say that viewu contains no more
information than tau◦Act about Act. This deﬁnition is equivalent
to NI in the case of transitive policies.
4. MAIN DEFINITIONS
We are now positioned to state our new deﬁnitions, which give
meaning to intransitive noninterference policies in nondeterminis-
tic machines. As noted above, our focus in this paper is with deﬁ-
nitions that place constraints on the ﬂow of information concerning
the actions that have been performed. We furthermore focus on
generalizing the notion of TA-security from van der Meyden [24].
We characterized TA-security in deterministic machines as stat-
ing that for each domain u, the function obsu ◦ last contains no
more information than tau ◦ Act about Act, and noted that it is
equivalent to take viewu in place of obsu ◦ last in this deﬁnition.
However, in nondeterministic systems, an observation-based deﬁ-
nition of security may be too weak, as shown in Section 3.3. This
suggests the following as an appropriate generalization of the deﬁ-
nition to nondeterministic systems.
Deﬁnition 9. A nondeterministic machine M is nTA-secure w.r.t.
a policy (cid:55)→ if, for all u ∈ U, the function viewu contains no more
information than tau ◦ Act about Act.
Intuitively, as in the deterministic case, this deﬁnition places, in
each run r, an upper bound on the information about the action
sequence Act(r) that is permitted to be contained in each possible
view viewu(r): it may be no more than the information contained
in tau(Act(r)). By Proposition 1, an equivalent statement is that
for all α, β in A∗, if tau(α) = tau(β) then poss(viewu | Act =
α) = poss(viewu | Act = β).
As we noted above in Section 3.5, a nondeterministic system
may be secure with respect to a deﬁnition of security that constrains
ﬂow of information to individual domains, while allowing ﬂows
of information to groups of domains.
If the system needs to be
protected against collusion, this means that we require a stronger
deﬁnition that takes into account the deductive capability of groups.
One way to approach such a deﬁnition is to focus on what the group
would know if agents in the group were, after the completion of a
run of the system, to share what they have observed. We call this
a post-hoc coalition. The state of information of such a coalition
X ⊆ U may be represented by the function (cid:104)viewu(cid:105)u∈X. We
would like them to be able to deduce no more than they would be
able to deduce if, after the run, they were to share their maximal
permitted information, represented by the terms tau(Act(r)) for
u in the group. This leads to the following deﬁnition.
Deﬁnition 10. A nondeterministic machine M is nTA-secure w.r.t.
post-hoc coalitions (PCnTA) for a policy (cid:55)→ if, for all nonempty
sets X ⊆ U, (cid:104)viewu(cid:105)u∈X contains no more information than
(cid:104)tau(cid:105)u∈X ◦ Act about Act.
By Proposition 1, an equivalent statement is that for all X ⊆ U and
α, β in A∗, if ∀u ∈ X (tau(α) = tau(β)) then
poss((cid:104)viewu(cid:105)u∈X | Act = α)
= poss((cid:104)viewu(cid:105)u∈X | Act = β) .
One situation in which this attack model is appropriate is a sys-
tem in which multiple agents submit computations to a cloud server,
and receive output only after the computation is complete.
In a
more interactive setting, the colluding coalition X may be able to
do more than share information at the end of the run: they may
be able to share information at each step of the run. We may call
this a runtime coalition. Since our systems model is asynchronous,
agents in X are typically not able to deduce a linear ordering on
the actions performed by members of X from the set of views
viewu(r) for u ∈ X. However, if they are able to communicate
each time that an action is performed by one of them, then at the
end of the run their information is the joint view viewX (r), which
does contain the order of actions in domains X. This means that
reasoning based on viewX represents a stronger attack model. The
following deﬁnition attempts to state that the system is resilient to
this stronger type of attack.
In order to formulate it, we need to characterize the maximal
permitted information for a group X, if they are colluding by shar-
ing information at each step. In the previous deﬁnition, the group’s
maximal permitted information in a run r is represented by the col-
lection of terms tau(Act(r)) for u ∈ X. In general, this will be
too weak to allow deduction of the order of the actions performed
in domains X, whereas as we just noted, the group can deduce
this from its joint view viewX (r). This suggests that we need a
stronger deﬁnition of the maximal permitted information on this
attack model. We take the approach here that if the group cannot
874be prevented from sharing information through channels lying out-
side the system, its maximal permitted information in the event of
such collusion is represented by sharing of the maximal permitted
information at each step of the computation. This leads us to gener-