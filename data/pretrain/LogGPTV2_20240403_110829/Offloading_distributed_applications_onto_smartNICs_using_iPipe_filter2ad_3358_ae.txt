with the minimal number of cores. When calculating the per-core
throughput of three applications, we use the CPU usage of RTA
worker, DT coordinator, and RKV leader to account for fractional
327
 0 1 2 3 4 5 6 7 8RTAWokerDTCoord.DTParticipantRKVLeaderRKVFollowerCore (#)DPDK-64BiPipe-64BDPDK-256BiPipe-256BDPDK-512BiPipe-512BDPDK-1KBiPipe-1KB 0 2 4 6 8 10RTAWokerDTCoord.DTParticipantRKVLeaderRKVFollowerCore (#)DPDK-64BiPipe-64BDPDK-256BiPipe-256BDPDK-512BiPipe-512BDPDK-1KBiPipe-1KB 0 20 40 60 80 100 120 140 0 0.5 1 1.5 2 2.5 3 3.5Latency (us)Per-core Throughput (Mop/s)DPDKiPipe 0 20 40 60 80 100 120 140 160 0 0.5 1 1.5 2 2.5 3 3.5Latency (us)Per-core Throughput (Mop/s)DPDKiPipe 0 20 40 60 80 100 120 0 0.5 1 1.5 2 2.5 3 3.5Latency (us)Per-core Throughput (Mop/s)DPDKiPipe 0 20 40 60 80 100 120 140 0 0.5 1 1.5 2 2.5 3 3.5 4Latency (us)Per-core Throughput (Mop/s)DPDKiPipe 0 20 40 60 80 100 120 140 160 0 0.5 1 1.5 2 2.5 3 3.5 4Latency (us)Per-core Throughput (Mop/s)DPDKiPipe 0 20 40 60 80 100 120 0 0.5 1 1.5 2 2.5 3 3.5 4Latency (us)Per-core Throughput (Mop/s)DPDKiPipeOffloading Distributed Applications onto SmartNICs using iPipe
SIGCOMM ’19, August 19–23, 2019, Beijing, China
(a) Low dispersion on 10GbE
LiquidIOII CN2350.
(b) High dispersion on 10GbE
LiquidIOII CN2350.
(c) Low dispersion on 25GbE
Stingray.
(d) High dispersion on 25GbE
Stingray.
Figure 16: P99 tail latency at different networking loads for 10GbE LiquidIOII CN2350 and 25GbE Stingray. We consider both low and high
dispersion distributions for request execution costs.
core usage. First, under 10GbE SmarNICs, applications (RTA, DT,
and RKV) built with iPipe outperform the DPDK ones by 2.3X,
4.3X, and 4.2X, respectively, as iPipe allows applications to offload
some of the computation to the SmartNIC. The benefits diminish
a little under the 25GbE setup (with 2.2X, 2.9X, and 2.2X improve-
ments) since actors running on the host CPU receive more requests
and require more CPU power. Second, at low to medium request
rates, NIC-side offloading reduces request execution latency by
5.7µs, 23.0µs, 8.7µs for 10GbE and 5.4µs, 28.0µs, 12.5µs for 25GbE,
respectively. Even though the SmartNIC has only a wimpy proces-
sor, the iPipe scheduler keeps the lightweight, fast-path tasks on
the NIC and moves the heavyweight, slow ones to the host. As a
result, PCIe transaction savings, fast networking primitives, and
hardware-accelerated buffer management can help reduce the fast
path execution latency. DT benefits the most as (1) both the coordi-
nator and the participants mainly run on the SmartNIC processor;
(2) the host CPU is only involved for the logging activity.
Figure 17: Host CPU usage of RKV leader and follower with and
without iPipe under different network loads for a 10GbE. Packet
size is 512B.
latency for LiquidIOII and Stingray, respectively. For the high disper-
sion one, iPipe’s scheduler is able to tolerate the request execution
variation and serve short tasks in time, outperforming the other
two. For example, when the networking load is 0.9, iPipe can reduce
68.7% (61.4%) and 10.9% (12.9%) of the tail latency for FCFS and DRR
cases on LiquidIOII (Stingray). In this case, our approximate tail la-
tency threshold (measured via µ +3σ) for LiquidIOII and Stingray is
52.8µs and 44.6µ, respectively. Thus, requests with latencies higher
than the threshold are processed on the iPipe DRR cores.
P99taillatency. We measured the tail latency (P99) when achiev-
ing 90% of the maximum throughput for the two link speeds. For
the three applications, iPipe reduces tail latency by 7.3µs, 11.6µs,
7.5µs for 10GbE and by 3.4µs, 10.9µs, 12.8µs for 25GbE. This reduc-
tion is not only due to fast packet processing (discussed above), but
also because iPipe’s NIC-side runtime guarantees that there is no
significant queue build up.
5.4 iPipe actor scheduler
We evaluate the effectiveness of iPipe’s scheduler, comparing
it with standalone FCFS and DRR schedulers under two different
request cost distributions: one is exponential with low dispersion;
the other one is bimodal-2 with high dispersion. We choose two
SmartNICs (i.e., 10GbE LiquidIOII CN2350 and 25GbE Stingray)
representing the cases where the scheduling runtime uses firmware
hardware threads and OS pthreads, respectively. The workload gen-
erator is built using packet traces obtained from our three real-world
applications, and it issues requests assuming a Poisson process. We
measure the latency from the client. The mean service times of the
exponential distribution on the two SmartNICs (i.e., LiquidIOII and
Stingray) is 32µs and 27µs, while b1/b2 of the bimodal-2 distribution
is 35µs/60µs and 25µs/55µs.
Figure 16 shows the P99 tail latency as we increase the network
load for four different cases. For the low dispersion one, iPipe’s
scheduler behaves similar to FCFS but outperforms DRR. Under
0.9 networking load, iPipe can reduce 9.6% and 21.7% of DRR’s tail
5.5 iPipe framework overheads
We evaluate the CPU overhead of the iPipe framework by compar-
ing two host-only implementations of RKV (one with iPipe and one
without iPipe). To make a fair comparison, when running these
two versions, we pin the communication thread to the same core
using the same epoll interface. We generate 512B requests and
gradually increase the networking load. Figure 17 reports the host
CPU utilization of the RKV leader and the follower when achieving
the same throughput. On average, iPipe consumes 12.3% and 10.8%
more CPU cycles for RKV leader and follower, respectively. Overall,
iPipe brings in three kinds of overheads: message handling, DMO
address translation when accessing objects, and the cost of the
iPipe scheduler that orchestrates traffic and maintains statistics of
execution costs. Since the message handling is not unique to iPipe,
the other two parts dominate the above-measured overheads.
5.6 Comparison with Floem
Floem [53] is a programming system aimed at easing the program-
ming effort for SmartNIC offloading. It applies a data-flow language
to express packet processing and proposes a few programming ab-
stractions, such as logic queue and per-packet state. iPipe also has
related concepts, such as message rings and packet metadata. How-
ever, compared with iPipe, the key difference is that the language
328
 0 20 40 60 80 100 0 0.2 0.4 0.6 0.8 1Latency (us)LoadFCFSDRRiPipe-sched 0 20 40 60 80 100 120 140 0 0.2 0.4 0.6 0.8 1Latency (us)LoadFCFSDRRiPipe-sched 0 20 40 60 80 100 0 0.2 0.4 0.6 0.8 1Latency (us)LoadFCFSDRRiPipe-sched 0 20 40 60 80 100 120 140 0 0.2 0.4 0.6 0.8 1Latency (us)LoadFCFSDRRiPipe-sched 0 100 200 300 4001030507090CPU usage (%)Network Load (%)Leader w/o iPipeFollower w/o iPipeLeader w/ iPipeFollower w/ iPipeSIGCOMM ’19, August 19–23, 2019, Beijing, China
M. Liu et al.
are a few other studies that use SmartNICs for application accel-
eration. For example, KV-Direct [37] is an in-memory key-value
store system, which runs key-value operations on the FPGA and
uses the host memory as a storage pool.
runtime of Floem does not use the SmartNIC’s computing power ef-
ficiently. First, the offloaded elements (computations) on Floem are
stationary, no matter what the incoming traffic is. Note, however,
that under high network traffic load comprising of small packets,
Multicore SoC SmartNICs have no room for application computa-
tion (as shown in Section 2.2.2). In iPipe, under such settings, we
will migrate the computation to the host side. Second, the common
computation elements of Floem mainly comprise of simple tasks
(like hashing, steering, or bypassing). Complex ones are performed
on the host side. This approach misses the opportunity of using the
cheap computing parallelism and domain-specific accelerators on
the SmartNIC. In contrast, iPipe, can be used to manage and offload
complex operations, and the runtime will dynamically schedule
them in the right place.
We take the real-time analytics (RTA) workload, and compare
its Floem and iPipe implementations. With the same experimen-
tal setup, Floem-RTA achieves at most 1.6Gbps/core (in the best
case), while iPipe-RTA can achieve 2.9Gbps. As described above,
this is because iPipe can offload the entire actor computation while
Floem utilizes a NIC-side bypass queue to mitigate the multiplex-
ing overhead. For the small packet size case (i.e., 64B), iPipe-RTA
delivers 0.6Gbps/core, outperforming Floem by 88.3%, since iPipe
moves all the actors to the host and uses all NIC cores for packet
forwarding, while Floem’s static policy persists with performing
the computations on the SmartNIC.
5.7 Network functions on iPipe
The focus of iPipe is to accelerate distributed applications with
significant complexity in program logic and maintained state. For
network functions with easily expressed states (or even stateless
ones) that have sufficient parallelism, FPGA-based SmartNICs are
an appropriate fit. We now consider how well iPipe running on mul-
ticore SmartNICs can approximate FPGA-based SmartNICs for such
workloads. We built two network functions with iPipe (i.e., Firewall
and IPSec gateway) and evaluated them on the 10/25GbE LiquidIOII
cards. For the firewall, we use a software-based TCAM implementa-
tion matching wildcard rules. Under 8K rules and 1KB packet size,
the average packet processing latency ranges from 3.65µs to 19.41µs
as we increase the networking load. These latencies are higher than
an FPGA based solution (i.e., 1.23∼1.6µs reported in [38]). We also
implemented an IPSec datapath that processes IPSec packets with
AES-256-CTR encryption and SHA-1 authentication. We take ad-
vantage of the crypto engines to accelerate packet processing. For
1KB packets, iPipe achieves 8.6Gbps and 22.9Gbps bandwidth on
the 10/25 GbE SmartNIC cards, respectively. These results are com-
parable to the ClickNP ones (i.e., 37.8Gbps under 40GbE link speed).
In other words, if one can use the accelerators on a Multicore SoC
SmartNIC for implementing the network functions, one can achieve
performance comparable to FPGA based ones.
6 Related work
SmartNIC acceleration. In addition to Floem [53], ClickNP [38]
is another framework using FPGA-based SmartNICs for network
functions. It uses the Click [33] dataflow programming model and
statically allocates a regular dataflow graph model during config-
uration, whereas iPipe can move computations based on runtime
workload (e.g., request execution latency, incoming traffic). There
329
In-networkcomputations. Recent RMT switches [6] and Smart-
NICs enable programmability along the packet data plane. Re-
searchers have proposed the use of in-network computation to
offload compute operations from endhosts into these network de-
vices. For example, IncBricks [41] is an in-network caching fabric
with some basic computing primitives. NetCache [25] is another in-
network caching design, which uses a packet-processing pipeline
on a Barefoot Tofino switch to detect, index, store, invalidate, and
serve key-value items. DAIET [55] conducts data aggregation along
the network path on programmable switches.
RDMA-based datacenter applications. Recent years have seen
growing use of RDMA in datacenter environments due to its low-
latency, high-bandwidth, and low CPU utilization benefits. These
applications include key-value store system [16, 27, 45], DSM (dis-
tributed shared memory) system [16, 46], database and transactional
system [12, 17, 29, 63]. Generally, RDMA provides fast data access
capabilities but limited opportunities to reduce the host CPU com-
puting load. While one-sided RDMA operations allow applications
to bypass remote server CPUs, they are hardly used in general
distributed systems given the narrow set of remote memory access
primitives associated with them. In contrast, iPipe provides a frame-
work to offload simple but general computations onto SmartNICs.
It does, however, borrow some techniques approaches from related
RDMA projects (e.g., lazy updates for the send/receive rings in
FaRM [16]).
Microsecond-scalescheduler. Researchers have proposed sched-
ulers to reduce the tail latency of µs-scale tasks. ZygOS [54] builds
a work-conserving scheduler that applies the d-FCFS queueing
discipline and enables low-overhead task stealing. Shinjuku [26] ad-
dresses a similar problem (i.e., handling tasks with variable service
times) as the iPipe scheduler. It provides a fast preemptive schedul-
ing mechanism by mapping the local APIC address into the guest
physical address space. However, triggering an interrupt is not only
a costly operation on the SmartNIC, it is also disabled on SmartNICs
that are firmware-based (such as LiquidIOII ones). Instead, we ex-
plore a hybrid scheduling discipline that runs heavy-weight actors
on DRR cores and executes light-weight actors on FCFS cores.
7 Conclusion
This paper makes a case for offloading distributed applications onto
a Multicore SoC SmartNICs. We conduct a detailed performance
characterization on different commodity Multicore SoC SmartNICs
and build the iPipe framework based on experimental observations.
We then develop three applications using iPipe and prototype them
on these SmartNICs. Our evaluations show that by offloading com-
putation to a SmartNIC, one can achieve considerable host CPU
and latency savings. This work does not raise any ethical issues.
8 Acknowledgments
This work is supported in part by NSF grants CNS-1616774, CNS-
1714508, and 1751231. We would like to thank the anonymous
reviewers and our shepherd, Bruce Maggs, for their comments and
feedback.
Offloading Distributed Applications onto SmartNICs using iPipe
SIGCOMM ’19, August 19–23, 2019, Beijing, China
References
[1] Gul Agha. 1986. Actors: A Model of Concurrent Computation in Distributed Systems.
[2] Mohammad Alizadeh, Shuang Yang, Milad Sharif, Sachin Katti, Nick McKeown,
Balaji Prabhakar, and Scott Shenker. 2013. pFabric: Minimal Near-optimal Datacen-
ter Transport. In Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM.
[3] Venkat Anantharam. 1999. Scheduling strategies and long-range dependence.
Queueing systems 33, 1-3 (1999), 73–89.
(2017).
[4] Apache. 2017. The Apache Cassandra Database. http://cassandra.apache.org.
[5] ARM. 2019. ARM Cortex-A72 Multi-core Processor. https://developer.arm.com/
products/processors/cortex-a/cortex-a72. (2019).
[6] Pat Bosshart, Glen Gibb, Hun-Seok Kim, George Varghese, Nick McKeown, Martin
Izzard, Fernando Mujica, and Mark Horowitz. 2013. Forwarding metamorphosis:
Fast programmable match-action processing in hardware for SDN. In ACM
SIGCOMM Computer Communication Review, Vol. 43. 99–110.
[7] Broadcom. 2019. Broadcom Stingray SmartNIC. https://www.broadcom.com/
products/ethernet-connectivity/smartnic/ps225. (2019).
[8] Broadcom. 2019.
https:
//www.broadcom.com/applications/data-center/cloud-scale-networking. (2019).
[9] Cavium. 2017. Cavium OCTEON Multi-core Processor. http://www.cavium.com/
The TruFlow Flow processing engine.
[10] Cavium. 2017. OCTEON Development Kits. http://www.cavium.com/octeon_
octeon-mips64.html. (2017).
software_develop_kit.html. (2017).
[11] Fay Chang, Jeffrey Dean, Sanjay Ghemawat, Wilson C. Hsieh, Deborah A.
Wallach, Mike Burrows, Tushar Chandra, Andrew Fikes, and Robert E. Gruber.
2006. Bigtable: A Distributed Storage System for Structured Data. In 7th USENIX
Symposium on Operating Systems Design and Implementation.
[12] Yanzhe Chen, Xingda Wei, Jiaxin Shi, Rong Chen, and Haibo Chen. 2016. Fast
and general distributed transactions using RDMA and HTM. In Proceedings of
the Eleventh European Conference on Computer Systems.
The New Need for Speed in the Datacenter Net-
work.
http://www.cisco.com/c/dam/en/us/products/collateral/switches/
nexus-9000-series-switches/white-paper-c11-734328.pdfdf. (2015).
[13] Cisco. 2015.
[14] Cisco. 2016. Cisco Global Cloud Index: Forecast and Methodology, 2015-2020.
http://www.cisco.com/c/dam/en/us/solutions/collateral/service-provider/
global-cloud-index-gci/white-paper-c11-738085.pdf. (2016).
[15] Russ Cox. 2019. Implementing Regular Expressions. https://swtch.com/~rsc/
regexp/. (2019).
[16] Aleksandar Dragojević, Dushyanth Narayanan, Orion Hodson, and Miguel Castro.
2014. FaRM: Fast remote memory. In Proceedings of the 11th USENIX Conference
on Networked Systems Design and Implementation.
[17] Aleksandar Dragojević, Dushyanth Narayanan, Edmund B Nightingale, Matthew
Renzelmann, Alex Shamis, Anirudh Badam, and Miguel Castro. 2015. No
compromises: distributed transactions with consistency, availability, and
performance. In Proceedings of the 25th symposium on operating systems principles.
[18] Daniel E. Eisenbud, Cheng Yi, Carlo Contavalli, Cody Smith, Roman Kononov, Eric
Mann-Hielscher, Ardas Cilingiroglu, Bin Cheyney, Wentao Shang, and Jinnah Dy-
lan Hosein. 2016. Maglev: A Fast and Reliable Software Network Load Balancer.
In 13th USENIX Symposium on Networked Systems Design and Implementation.
[19] Daniel Firestone. 2017. Hardware-Accelerated Networks at Scale in the
Cloud. https://conferences.sigcomm.org/sigcomm/2017/files/program-kbnets/