### BFT-SMART Ordering Service Throughput Analysis

#### Figures and Data
- **Envelopes per Block: 10**
  - **(a) 4 Orderers, 10 Envelopes/Block**
  - **(b) 7 Orderers, 10 Envelopes/Block**
  - **(c) 10 Orderers, 10 Envelopes/Block**

- **Envelopes per Block: 100**
  - **(d) 4 Orderers, 100 Envelopes/Block**
  - **(e) 7 Orderers, 100 Envelopes/Block**
  - **(f) 10 Orderers, 100 Envelopes/Block**

- **Throughput (Transactions/Second)**
  - 40 bytes
  - 200 bytes
  - 1 kbytes
  - 4 kbytes

- **Number of Receivers**
  - 1, 2, 4, 8, 16, 32

#### Observations for 10 Envelopes/Block
- **Maximum Throughput:** 
  - Approximately 50k transactions/second when there are 1 to 2 receivers.
  - This is significantly lower than the 8.4k × 10 = 84k envelopes/sec capacity if only signatures are considered (Section VI-B).

- **Performance Drop:**
  - The performance drop is due to the competition for CPU power between signature generation and the replication protocol.
  - BFT-SMART can consume up to 60% of CPU usage when executing a void service with asynchronous clients.

- **Comparison with Micro-benchmark:**
  - The micro-benchmark from Section VI-B, executed on a single machine, showed higher performance due to the absence of BFT-SMART overhead.

- **Peak Throughput:**
  - For up to 2 receivers and envelope sizes of 1 and 4 kbytes, the peak throughput is similar to the results in [5].
  - BFT-SMART is unable to order envelopes at the same rate as the system's signature production rate for these request sizes.

#### Observations for 100 Envelopes/Block
- **Throughput Improvement:**
  - Across all cluster sizes, throughput is significantly higher for smaller envelope sizes and up to 8 receivers.
  - Each node creates blocks at a lower rate (approximately 1100 blocks per second), but each block contains 100 envelopes instead of 10.

- **Optimal Configuration:**
  - For smaller envelope sizes, it is better to adjust the nodes' configuration to avoid consuming all the CPU time and rely on the rate of envelope arrival.

- **Larger Envelope Sizes:**
  - For envelopes of 1 and 4 kbytes, the behavior is similar to using 10 envelopes/block, especially from 7 nodes onward.
  - The predominant overhead becomes the replication protocol for larger envelope sizes.

- **Convergence of Throughput:**
  - For a larger number of receivers (16 and 32), throughput converges to similar values across all combinations of envelope/cluster/block sizes.
  - For larger envelope sizes, this is due to the overhead of the replication protocol.
  - For smaller envelope sizes, the transmission of blocks to the receivers becomes the predominant overhead.

### Geo-distributed Ordering Cluster
- **Experiment Setup:**
  - Deployed in a local datacenter and conducted a geo-distributed experiment with 3 frontends scattered across the Americas.
  - Nodes of the ordering service were distributed globally: Oregon, Ireland, Sydney, and São Paulo (four BFT-SMART replicas), with Virginia as an additional WHEAT replica.

- **Hardware:**
  - m4.4xlarge instances with 16 virtual CPUs each.

- **Frontends:**
  - Deployed in Oregon (collocated with leader node weighting Vmax in WHEAT), Virginia (collocated with non-leader node, but still weighting Vmax), and São Paulo.

- **Latency Measurements:**
  - Median and 90th percentile latency measurements were collected.

- **Results:**
  - The results show that the median and 90th percentile latencies vary across different configurations, but the overall performance is consistent with the expected behavior of BFT-SMART in a geo-distributed setting.

This analysis provides insights into the performance characteristics of the BFT-SMART ordering service under various configurations and helps in optimizing the system for different use cases.