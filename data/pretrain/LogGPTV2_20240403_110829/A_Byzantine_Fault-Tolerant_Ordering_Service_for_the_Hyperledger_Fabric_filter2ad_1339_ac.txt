 20
 10
 0
40 bytes
200 bytes
1 kbytes
4 kbytes
 1 2  4
 8
 16
 32
 1 2  4
 8
 16
 32
 1 2  4
 8
 16
 32
Number of receivers
Number of receivers
Number of receivers
(a) 4 orderers, 10 envelopes/block.
(b) 7 orderers, 10 envelopes/block.
(c) 10 orderers, 10 envelopes/block.
 120
 110
 100
 90
 80
 70
 60
 50
 40
 30
 20
 10
 0
40 bytes
200 bytes
1 kbytes
4 kbytes
)
c
e
s
/
s
n
a
r
t
k
(
t
u
p
h
g
u
o
r
h
T
 120
 110
 100
 90
 80
 70
 60
 50
 40
 30
 20
 10
 0
40 bytes
200 bytes
1 kbytes
4 kbytes
)
c
e
s
/
s
n
a
r
t
k
(
t
u
p
h
g
u
o
r
h
T
 120
 110
 100
 90
 80
 70
 60
 50
 40
 30
 20
 10
 0
40 bytes
200 bytes
1 kbytes
4 kbytes
 1 2  4
 8
 16
 32
 1 2  4
 8
 16
 32
 1 2  4
 8
 16
 32
Number of receivers
Number of receivers
Number of receivers
(d) 4 orderers, 100 envelopes/block.
(e) 7 orderers, 100 envelopes/block.
(f) 10 orderers, 100 envelopes/block.
Fig. 6: BFT-SMART Ordering Service throughput for different envelope, block and cluster sizes.
It can be observed that when using 10 envelopes/block
(Figures 6a, 6b, and 6c), the maximum throughput observed
is approximately 50k transactions/second (when there exists
only 1 to 2 receivers in the system), which is way below the
8.4k × 10 = 84k envelopes/sec capacity of only signatures
are considered (Section VI-B). This can be explained by
the fact that signature generation needs to share CPU power
with the replication protocol, hence creating a thug-of-war
between the application’s worker threads and BFT-SMART’s
I/O threads and queues – in particular, BFT-SMART alone
can take up to 60% of CPU usage when executing a void
service with asynchronous clients. Hence, the performance
drops when compared to the micro-benchmark from Section
VI-B, which was executed in a single machine, stripped of
the overhead associated with BFT-SMART. Moreover, for up
to 2 receivers and envelope sizes of 1 and 4 kbytes, the peak
throughput becomes similar to the results observed in [5]. This
is because for these request sizes BFT-SMART is unable to
order envelopes at a rate equal to the rate at which the system
is able to produce signatures.
Figures 6d, 6e, and 6f show the results obtained for 100
envelopes/block, when each node is not subject
to CPU
exhaustion. It can be observed that, across all cluster sizes,
throughput is signiﬁcantly higher for smaller envelope sizes
and up to 8 receivers. This happens because even though
each node creates blocks at a lower rate – approximately
1100 blocks per seconds – each block contains 100 envelopes
instead of only 10. Moreover, this conﬁguration makes the
rate at which envelopes are ordered to become similar to the
rate at which blocks are created. This means that for smaller
envelope sizes, it is better to adjust the nodes’ conﬁguration
to avoid consuming all the CPU time and rely on the rate of
envelope arrival. However, for envelopes of 1 and 4 kbytes the
behavior is similar to using 10 envelopes/block, specially from
7 nodes onward. This is because for larger envelope sizes –
as discussed previously – the predominant overhead becomes
the replication protocol. Interestingly, for a larger number
of receivers (16 and 32),
throughput converges to similar
values across all combinations of envelope/cluster/block sizes.
Whereas for larger envelope sizes this is due to the overhead
of the replication protocol, for smaller envelope sizes this
happens because the transmission of blocks to the receivers
becomes the predominant overhead.
D. Geo-distributed Ordering Cluster
In addition to the aforementioned micro-benchmarks de-
ployed in a local datacenter, we also conducted a geo-
distributed experiment focused on collecting latency measure-
ments at 3 frontends scattered across the Americas, with the
nodes of the ordering service distributed all around the world:
Oregon, Ireland, Sydney, and S˜ao Paulo (four BFT-SMART
replicas), with Virginia standing as WHEAT’s additional
replica (ﬁve replicas). Since signatures generation requires
considerable CPU power, we used instances of the type
m4.4xlarge, with 16 virtual CPUs each. The frontends were
deployed in Oregon (collocated with leader node weighting
Vmax in WHEAT), Virginia (collocated with non-leader node,
but still weighting Vmax ) and S˜ao Paulo. Each frontend was
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:27:14 UTC from IEEE Xplore.  Restrictions apply. 
56
)
s
d
n
o
c
e
s
i
l
i
m
(
y
c
n
e
t
a
L
Median
90th percentile
 800
 700
 600
 500
 400
 300
 200
 100
 0
)
s
d
n
o
c
e
s
i
l
i
m
(
y
c
n
e
t
a
L
 800
 700
 600
 500
 400
 300
 200
 100
 0
)
s
d
n
o
c
e
s
i
l
i
m
(
y
c
n
e
t
a
L
 800
 700
 600
 500
 400
 300
 200
 100
 0
B
F
W
W
W
B
B
B
W
F
F
F
B
F
W
W
W
B
B
B
W
F
F
F
B
F
W
W
W
B
B
B
W
F
F
F
H
H
H
H
H
H
H
H
H
H
H
H
T
-
S
E
A
T
-
S
E
A
T
-
S
E