of their percentage of ﬁltered reviews. As can be seen from
Figure 8(c), we show that the percentage of ﬁltered reviews
of regular Sybil users signiﬁcantly outnumbers that of benign
users with respect to the same cumulative probability. To be
speciﬁc, we see that 80% of Sybil users (resp. benign users)
have more than 90% (resp. less than 20%) of reviews ﬁltered.
This user-level observation is consistent with the community-
level results shown in Figure 7. In addition, elite Sybil users
have fewer reviews ﬁltered by Dianping mainly because a large
portion of their reviews are not assigned to any task.
B. Community Structure
Understanding the behaviors of elite Sybil users is im-
portant to reveal the characteristics of the (quasi) permanent
workforce of Sybil organizations on Dianping. Looking at the
macro level, communities of elite Sybil users form large-scale
sparsely knit networks and their graph density is much lower.
Fig. 10. Relation between elite Sybil users and communities
Figure 10 shows an example of an induced network structure
of elite Sybil users. In the ﬁgure, a dot represents an elite
Sybil user, a square represents a Sybil community, an edge
11
Fig. 9. Comparison on distribution of user-level star ratings
see from Figure 9, we ﬁnd that the distribution of levels of
benign users is almost symmetrically bell-shaped, centered at
3-star. In contrast, the levels of regular Sybil users are heavily
skewed toward low-level. Based on our results, we observe
that the levels of elite Sybil users detected are biased more
toward high-level than those of regular Sybil users.
Comparing elite Sybil users with regular Sybil users at
the micro level, we show that elite Sybil users post more
fake reviews, are more spread out temporally, and have fewer
reviews ﬁltered by Dianping. Figure 8 compares the behavioral
patterns among elite Sybil users, regular Sybil users, and
benign users on Dianping.
Figure 8(a) plots the CDF of the number of users in terms
of the number of suspicious reviews posted. As can be seen
in Figure 8(a), elite Sybil users post
the most suspicious
reviews among all. This demonstrates that elite Sybil users
cater to market demand due to their potential larger impact on
Dianping ranking and higher prices for the customers. For the
regular Sybil users, their strategy is frequently changing their
low-level accounts to evade the detection since it is easy to
apply or buy with a low cost for low-level accounts.
Figure 8(b) plots the CDF of the number of users in terms
of the percentage of fake reviews posted. As we can see, fake
reviews are signiﬁcantly more often generated by regular Sybil
users than by elite Sybil users, which echoes our deﬁnition that
between a dot and a square represents that an elite Sybil
user belongs to a community, and a red (resp. blue) dot
shows that an elite Sybil user belongs to a single community
(resp. multiple communities). As can be seen, we observe
that many elite Sybil users are correspondingly connected
to a single community, forming a large-scale sparsely knit
network. We also show that some elite Sybil users appear
in multiple communities. Ranked by Sybilness, we pick up
the top 1, 000 elite Sybil users out of all 12, 292 users in our
collection. There are 824 elite Sybil users who participated in a
single community, 160 who participated in two communities,
and 16 who participated in at least three communities. Not
surprisingly, we clearly show that these elite Sybil users are
sparsely connected and their graph density is much lower than
that of regular Sybil users.
C. Review Manipulation for Chain Stores
Recent research from Harvard [27] pointed out that it is
less likely for chain stores to hire Sybil accounts to generate
favorable reviews. Chain stores tend to depend heavily on
various forms of promotion and branding to establish their
reputation. This is because chains receive less beneﬁt from
reviews, and they may also incur a larger cost if they are caught
posting fake reviews, destroying their brand image. However,
our research contradicts this statement. We ﬁnd that a series of
chain stores leverage Sybil organizations to post fake reviews
to manipulate their online ratings.
To be more speciﬁc, of all 566 Sybil communities in our
dataset, it is observed that 12.37% of Sybil communities post
fake reviews for chain stores listed on Dianping. The number
of chain stores involved varies from 2 to 11. One possible
explanation is that the chain stores hired the same Sybil agent,
who recruited the same Sybil community for Sybil campaigns.
Figure 11 shows the main part of the entire network
structure of Sybil communities and overhyped stores, pruned
by a small portion of tiny networks. In the ﬁgure, a yellow
square represents a Sybil community, a red dot represents an
overhyped store, and an edge between a yellow and a red dot
represents that a Sybil community connects to an overhyped
store. As can be seen, almost all Sybil communities act as
central nodes. This indicates that these Sybil communities not
only launch campaigns for a single store, but also provide
various services for a huge number of overhyped stores who
are connected by the network. Furthermore, some overhyped
stores connect to multiple communities, which indicates that
they have employed Sybil communities more than once (A
case study is detailed in Section VI-F.). We also label chain
stores that have at least ﬁve branches with different colors
other than red. These chains are connected to the same
communities, respectively, possibly sharing similar reviews
and having the same goal.
D. Early Alerts for Sybil Campaigns
In this subsection, we will show that it is feasible to uncover
Sybil campaigns through monitoring our detected elite Sybil
users. In particular, by continually monitoring the collusive
Fig. 11. Relation between Sybil communities and the overhyped stores
behaviors of elite Sybil users, the social network operator can
determine whether a Sybil campaign has been launched at
the earliest stage, which serves as an early alert for a Sybil
campaign.
Detecting Sybil campaigns via monitoring elite Sybil users.
Our goal is to detect the presence of a Sybil campaign at the
early stage based on identifying elite Sybil users via contin-
ually monitoring all elite Sybil users. To do this, we simply
apply 7-day slide windows along the timeline to each store
so as to detect campaigns. The rule of determining a Sybil
campaign is more than a predetermined threshold number (e.g.,
7 in our experiment) of reviews that the elite Sybil users
posted at the same store within a 7-day slide window. Our
heuristic is that, in the non-campaign period, the elite Sybil
users normally post reviews at different stores in similar ways
as innocent users due to their different living habits, walking
routines, or shopping preferences. However, only within the
campaign period, the elite Sybil users collusively post reviews
at the same stores to fulﬁll the Sybil campaign tasks. The
evaluation results show that by scanning the activities of elite
Sybil users during the entire campaign period, approximately
90.40% campaigns can be determined. This indicates that the
campaign determination rule holds for almost all the Sybil
campaigns.
Determining Sybil campaigns at the early stage. An inter-
esting question is whether we can determine a Sybil campaign
at the early stage. The beneﬁts of early detection is that it
can give a competitive advantage for the system operator
to take countermeasures against Sybil campaigns. We run
the campaign window determination algorithm by using the
ﬁrst 1/4, 1/3, and 1/2 of the entire campaign period. The
evaluation results show that 56.77%, 63.08%, and 75.14%
of campaigns can be successfully detected correspondingly.
Since the average Sybil campaign period is 68 days in our
experiments, it indicates that more than 50% of Sybil cam-
paigns can be determined within the ﬁrst two weeks by only
12
Fig. 12. Reviews posted by Community 4559 in Store 4112200
Fig. 13. The distribution of number of campaigns across campaign duration
observing activities of elite Sybil users, thereby triggering
lightning strikes on Sybil campaigns.
E. Temporal Dynamics
We demonstrate two temporal dynamics characterized by
user posting period and Sybil campaign duration.
User posting period. Figure 12 shows that elite Sybil users
in Community 4559 repeatedly post fake reviews in Store
4112200. In the Figure, the x-axis shows the time when an
account posts a review, and the y-axis is the account’s ID.
A dot (x, y) in the ﬁgure represents that an account with
ID y posts a review at time x. We use staggered colors to
encode reviews posted by different users. As we can see from
Figure 12, 33 users in Community 4559 posted 127 reviews
within a period of two months. Posting reviews by these users
is much denser than by benign users. Apart from posting
reviews within a short time period, these elite Sybil users also
deliberately manipulate posting time of reviews. For example,
some elite Sybil users even periodically (every week/month)
post fake reviews. We emphasize that manipulation of posting
temporal dynamics is key to orchestrating the evasive strategy.
Sybil campaign duration. By applying the campaign window
detection algorithm, we ﬁnally obtain 4, 162 Sybil campaigns.
Figure 13 shows the distribution of number of campaigns
across campaign duration. As we can see from Figure 13, the
distribution is unimodal with a sudden spike at 7 days for the
x-axis, echoing our 7-day slide windows selected; then largely
monotonically decreasing beyond 50 days. More remarkably,
13
Fig. 14. Variation of star ratings and the number of reviews of a hotel
we observe there are 466 1-day ephemeral Sybil campaigns
as shown by the y-intercept of Figure 13. In these campaigns,
Sybil communities generally complete a task ﬂeetingly.
In this section, we will detail a case study of Sybil commu-
nities and campaigns and illustrate various strategies to evade
the Dianping’s Sybil detection system.
F. Sybil Communities and Sybil Campaigns
Recall
in Figure 11, we show a part of stores employ
several Sybil communities to increase their star ratings. Here,
we zoom in and show the ﬁrst case study that is about a
hotel employing three different Sybil communities to post
fake reviews. Figure 14 shows that the variation of the star
rating and the number of reviews change over time. Orange
represents aggregate reviews of a hotel; blue, purple, and
green represent reviews coming from three respective Sybil
communities, respectively. The red line denotes the star rating
of a hotel and the blue line denotes the star rating without
detected fake reviews.
As we can see from Figure 14, many spikes occurred,
generated by three Sybil communities, always correspond to
the spike of the total number of reviews. This indicates that
these fake reviews causing sudden spikes are taken into effect
to raise the star rating of the hotel. In addition, as pointed out
from Figure 14, red and blue lines are overlapping before the
ﬁrst spike; the red line then increases sharply afterwards but
the blue line maintains a moderate growth. This indicates that
these fake reviews posted by Sybil communities do have an
impact on distorting the online rating. Figure 14 also implies
that Community 7677 commits the largest-scale fake reviews
and contributes most to increasing the star rating. However,
Community 7668 launches a fairly long-term Sybil organiza-
tion but takes a very “moderate” gain on the star rating. This
is perhaps because the hotel has had accumulated a signiﬁcant
number of reviews previously. Another possible reason is that
the secret ranking algorithm adopted by Dianping does not
merely depend on the average rating of a store. Features, such
as the number of reviews and the number of page views,
are another factors to determine the rank of a store. Hence,
although these reviews do not have a discernible impact on
the average star rating, they may also affect ranking results on
Dianping.
G. Evading Dianping’s Sybil Detection System
In this case study, we present three examples of elite Sybil
users in the same community to attempt to illuminate the
evasive strategy taken by elite Sybil users. We also compare
the results processed by Dianping’s ﬁltering system with ours.
(a) No reviews ﬁltered
(b) Partial reviews ﬁltered
(c) All reviews ﬁltered
Fig. 15. Different detection results for elite Sybil users
As shown in Figure 15, each subﬁgure corresponds to the
reviews posted by each elite Sybil user. Each dot represents
a review posted according to the timeline; the upper (resp.
lower) dotted line of reviews represents the posting time-
line generated ELSIEDET (resp. Dianping ﬁltering system).
In speciﬁc, each blue (resp. red) dot represents real (resp.
fake) reviews labeled by ELSIEDET. Each blue (resp. red)
triangle represents existing (resp. ﬁltered) reviews according to
Dianping. Through these three examples, ﬁrst, we can analyze
the evasive strategy taken by elite Sybil users. Elite Sybil users
will post massive reviews to camouﬂage fake reviews and this
strategy can evade most aggregate behavioral-based clustering
approaches that rely on computing similarity of user activity.
Second, these three users appear in the same community. They
write fake reviews in similar stores and share with the similar
behavioral patterns in the way to post reviews. However, there
is one distinct difference from Dianping ﬁltering system. For
a given user, in Figure 15(a), we can see that no reviews
have been ﬁltered; in Figure 15(b), partial reviews have been
ﬁltered; in Figure 15(c), Sybil users are extremely sensitive
to Dianping ﬁltering system as all reviews have been ﬁltered.
This is perhaps because of his/her long negative-credit history.
In summary, we feel that Dianping ﬁltering system is largely
working on detecting regular Sybils, as shown in Figure 15(b).
We feel that Dianping is being fairly opaque about its ﬁltering
system as most of the real reviews of an elite Sybil user have
14
also been falsely ﬁltered due to its high false alarm rate, as
shown in Figure 15(c). Although our dataset is moderate in
size compared with the Dianping database, it is large enough
to allow us to gain meaningful insights and identifying factors
that impact the results and limitations of conventional Sybil
detection systems.
VII. DISCUSSION AND LIMITATIONS
In this section, we discuss the potential application of
ELSIEDET and the limitations of the paper.
A. Application of ELSIEDET
We here show how ELSIEDET can be integrated to the
existing Dianping’s Sybil system to enhance its tolerance of
elite Sybil attacks.
Mitigating Sybil attacks by changing the weight of reviews
with respect to Sybilness. The ultimate goal of a Sybil
campaign is to manipulate the ratings of stores by generating
massive fake reviews and ratings. To mitigate the negative
impact of Sybil attacks on stores’ ranking, a potential approach
is to tune the weights of reviews of the suspicious users
according to their Sybilness. By assigning a lower weight
to a highly suspicious user, it will signiﬁcantly increase the
difﬁculty of the Sybil organizations to manipulate the ratings
and help alleviate the human labor required to verify massive
number of users reported.
Monitoring the top elite Sybil users to predict Sybil
campaigns. Detecting Sybil campaigns is critical for Dianping
to limit the impact of Sybil attacks. In Section VI-D, we
have pointed out that we can monitor elite Sybil users and
exploit their group actions to identify the Sybil campaign in
a real-time fashion. Note that, considering millions of stores
and users, only monitoring a small set of suspicious users
can signiﬁcantly save the efforts and resources of the social
network operators.
B. Limitations
First, although our detection system has strictly focused on
Dianping, our results are applicable to a wider range of URSNs
or any social media that relies on user-contributed comments.
Examples include E-commerce (Amazon, Ebay, BizRate),
movie-rating platforms (IMDB, Netﬂix, Douban), traveling
services (TripAdvisor), and multi-agent systems (Advogato).
In speciﬁc, in 2012, Yelp proﬁle pages featured “consumer
alerts” on several sneaky businesses which got caught red-
handed trying to buy reviews, crafted by Yelp “elite” users,
for these businesses [36]. TripAdvisor has also put up similar
warning notices. These examples may have speciﬁc detection
systems, and we leave their design and evaluation to future
work. Second, we acknowledge if a Sybil community can
minimize the involvement in multiple campaigns, it would be
very likely to boost the chance to evade the detection; however,
recruiting high-cost elite Sybil users to participate in limited
Sybil campaigns contradicts the economic basis. Third, we do
not study the relationships among reviewers on Dianping. For
example, a reviewer can make friends and keep a friend list on