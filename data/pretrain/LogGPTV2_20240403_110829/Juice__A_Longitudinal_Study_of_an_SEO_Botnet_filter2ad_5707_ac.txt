URL Manager. The host crawler tracks compromised
sites using one probe URL to that site at a time. Often a site
can have multiple pages infected with the SEO kit, though,
such as a site with multiple blogs, all of the comment pages
attached to blogs and articles, etc. Over time, a site owner
may remove or clean an infected page while other URLs
to other pages on the site remain compromised and active
with the same SEO kit.
In these cases, the host crawler
switches to a new URL to continue to track and monitor
this compromised site.
The URL manager addresses this need. It maintains a
list of all URLs, as discovered from cross links, for a given
site in the crawling set. It periodically checks whether each
URL could potentially serve as the probe URL for a par-
ticular site by attempting to fetch a diagnostic page from
that URL. Then, whenever the host crawler cannot fetch a
diagnostic page for a site, it consults the URL manager to
ﬁnd another representative probe URL, if one exists. If not,
the host crawler will continue to use the same probe URL,
eventually timing out after eight days if all URLs to the site
are not operational. In this case, it declares the site as “san-
itized” since the SEO kit is no longer operational. Because
there are far more URLs than sites, the URL manager crawls
just once a day to check newly discovered URLs.
4.2 Dagger Search Crawler
Before we began crawling the SEO botnet, we previously
explored the general dynamics of cloaking on the Web [19].
We knew from examining the code of previous versions of
the SEO kit that the botnet poisoned trending search terms
from April 2011 through September 2011, so we suspected
that poisoned search results from the SEO botnet would also
appear in our previous data set.
We had collected cloaking data using a crawler called
Dagger, which ran every four hours to: (1) download trend-
ing search terms, (2) query for each trending search term on
various search engines, (3) visit the page linked from each
search result, and (4) run a cloaking detection algorithm to
identify poisoned search results. The Dagger cloaking data
allows us to analyze the impact of the SEO botnet on trend-
ing search results in near real time for a seven-month period
(Section 5.3). Unfortunately, although we had continued to
crawl cloaking search results, the SEO botnet changed its
SEO policy to target ﬁrst OEM software and then random
search terms, so we would expect only accidental overlap
with the Dagger data after September 2011.
4.3 Trajectory Redirection Crawler
While the host crawler downloads the contents of the
doorway pages directly linked by poisoned search results,
we also want to identify which sites these doorways ulti-
mately lead to (e.g., a fake antivirus scam page) and hence
infer how the botmaster monetizes user trafﬁc. Since fol-
lowing the doorway pages to ﬁnal landing pages typically
involves following a complicated redirection chain, often
involving JavaScript redirection code, we used the high-
ﬁdelity Trajectory crawler from yet another project [11].
This crawler uses an instrumented version of Mozilla Fire-
fox to visit URLs, follows all application-level redirects (in-
cluding JavaScript and Flash), logs the HTTP headers of the
intermediate and ﬁnal pages of a redirect chain, and cap-
tures the HTML and a screenshot of the ﬁnal page. For all
of the poisoned search results crawled by Dagger, we also
crawled them using this Trajectory crawler to track scams.
5 Results
With the data sets we have gathered, we now characterize
the activities of the SEO botnet and its compromised hosts.
5.1
Infrastructure
Using the nine months of data collected by Odwalla, we
start by analyzing the botnet infrastructure used in the SEO
campaigns: the scale of the botnet, the lifetime of compro-
mised sites in the botnet, and the extent to which the bot-
master monitors and manages the botnet.
5.1.1 Scale
Compared to other kinds of well-known botnets, such as
spamming botnets with tens to hundreds of thousands of
discovered compromised sites were running the latest ver-
sion (V8), and were discovered a month later, due to what
we suspect is the deployment time for a new cross linking
mechanism that utilizes blogspot.com as a level of in-
direction. Note that the large drop in botnet size on January
28, 2012, corresponds to an outage on the directory server
that triggered errors on the nodes, making the nodes unre-
sponsive to our crawler.
As a ﬁnal data point, recall from Section 3.2 that the
GR botnet uses a pull mechanism to ensure that compro-
msied sites always run an updated version of the SEO kit.
As a ﬁrst step, a site makes up to three attempts to contact
the directory server using ﬁrst a hardcoded domain, then a
hardcoded IP address, and ﬁnally the output of a time-based
domain generation algorithm (DGA).
While crawling the botnet we found that both the direc-
tory server’s hard coded domain and IP address were un-
reachable starting on September 9, 2012. We took advan-
tage of this occurrence by registering the DGA domains that
compromised sites will contact when attempting to reach
the directory server. Thus, we pose as the directory server
and intercept all requests from the botnet’s compromised
sites for nearly a month between October 4 through Oc-
tober 30, 2012. From this vantage, we found that 1,813
unique IPs contacted our directory proxy. Since we found
that, on average, 1.3 compromised sites are hosted behind
a unique IP from the host crawler data, extrapolation places
the botnet at 2,365 compromised sites—in agreement with
our ﬁndings above that the GR botnet is on the scale of thou-
sands of nodes.
5.1.2 Lifetime
The relatively stable botnet size and membership suggests
that compromised sites are long-lived in the botnet. Indeed,
we ﬁnd that the many of these sites remain compromised
for long periods of time and the botmaster is able to use
them continuously without needing to constantly refresh the
botnet with fresh sites to maintain viability.
We deﬁne the lifetime of a compromised site as the time
between the ﬁrst and last time the crawler observed the SEO
kit running on the site. This estimate is conservative since a
site may have been compromised before we ﬁrst crawled it.
However, we note that our measurement period of compro-
mised sites is nine months and we began monitoring 74%
of all 1497 compromised sites within the ﬁrst 40 days of
our study. Thus, even without the exact time of compro-
mise, we are still able to observe the sites for long periods
of time. (As further evidence, for the 537 sites that also ap-
pear in the earlier Dagger search results (Section 4.2) the
majority were compromised back to April 2011.)
We decide that a site is cleaned when the site does not re-
spond to the SEO C&C protocol for eight consecutive days,
Figure 3: Number of active nodes in the botnet
over time. SUM shows the total number of active
nodes, and the other lines show the number of
nodes operating different versions of the SEO kit.
hosts, the SEO botnet is only modest in size. Figure 3
presents the measured size of the botnet over time. Each
line shows the number of nodes operating a speciﬁc version
of the SEO kit, and the SUM line shows the total number of
all nodes across all versions. For example, on December 1,
2011, we found 821 compromised sites in total, of which
585 sites were running the MAC version of the SEO kit, 42
were running OEM, and 194 were running V7.
Also unlike other kinds of botnets, the SEO botnet does
not exhibit frequent churn. Over nine months, the botnet
consisted of 695 active nodes on average, with a maximum
size of 939 nodes on December 11, 2011. Yet, we observed
the botnet running on a total of just 1,497 unique compro-
mised sites across the entire measurement period. In con-
trast, spamming botnets like Storm would experience churn
of thousands of hosts a day [6].
Instead, we see a few key points in time where the bot-
net membership ﬂuctuates in response to SEO kit updates
by the botmaster, rather than from external intervention. At
the time of the upgrades, the botmaster also changes the
cross linking policy among nodes, potentially revealing new
nodes. In between these upgrades, the botnet size primarily
ﬂuctuates due to variations in host availability, with a de-
gree of slow attrition. For example, on November 1, 2011,
the botmaster updated the SEO kit from OEM→MAC. Even
though the OEM nodes appear to have entirely switched over
to MAC, the size of the botnet increases by over 200 nodes,
all due to nodes running the older version V7. It appears
that during the update the botmaster changed the cross link-
ing policy to include additional nodes running V7, inciden-
tally widening our vantage point but only for stagnant sites
running an older version. March 6, 2012, marks a simi-
lar version switch over from MAC→V8 in response to an-
other software upgrade.
In this upgrade, the 298 newly
02004006008001000Date# Compromised Web SitesNov 11Jan 12Mar 12May 12Jul 12summacoemv7v8Figure 4: On the left, the distribution of time that sites were compromised (sanitized sites only); the ‘*’ bin shows
the number of compromised sites still actively running the SEO kit at the end of the measurement period. For
sites that were sanitized, the right graph shows the number of sites sanitized each day.
suggesting that the site no longer runs the SEO kit. Typi-
cally a site stops running the SEO kit because the site owner
removed the SEO malware, sanitizing the site, or the Web
host or DNS registrar made the site unavailable by prevent-
ing visitors from loading the site or resolving the domain.
Consequently, the botmaster is able to use compromised
sites for SEO campaigns for long periods of time. Figure 4a
presents a histogram of the lifetime of the compromised
sites. We distinguish between sites that have been sanitized,
avoiding right-censoring of their lifetimes, and sites that
have not yet been sanitized. For compromised sites that are
eventually sanitized, we bin them according to their respec-
tive lifetimes using monthly intervals (30 days). Over 74%
of sanitized sites have a lifetime greater than a month, and
over 54% have a lifetime greater than two months. There is
also a long tail, with the lifetime of some sanitized sites
extending beyond even eight months. For compromised
sites that have not yet been sanitized, we show them in
the ‘*’ bin. These remaining 549 sites are still compro-
mised at the time of writing, and the majority of those have
been compromised for at least seven months. This distri-
bution indicates that the majority of compromised sites are
indeed long-lived and able to support the SEO campaign for
months with high availability.
Figure 4b shows the number of sites sanitized each day,
indicating a low daily attrition rate of sites leaving the bot-
net over time (9.9 sites on average). The few spikes in the
graph are speciﬁc points in time when many compromised
sites were sanitized. In some cases, the spikes are partially
attributable to a single entity, owning or hosting multiple
sites, who cleans multiple sites at the same time. By man-
ually comparing the resolved IP address for domain names
as well as parsing WHOIS records, we were able to conﬁrm
shared hosting and shared owners, respectively. Note that
the largest spike on January 26, 2012, corresponds to the
outage of the botnet directory server.
One reason that sites remain compromised for long pe-
riods of time is that the SEO kit camouﬂages its presence
to site owners. As discussed in Section 3.2.1, the SEO kit
returns the original contents of the page to a visitor unless
the SEO kit can determine if the visitor is a search engine
crawler or has clicked on a result returned from a search
engine. Hence, site owners accessing their own pages typ-
ically will not notice an installed SEO kit. That said, even
when they discover the presence of the SEO kit, oftentimes
they are unable or unwilling to remove it. In December and
January, for instance, we contacted nearly 70 site owners to
inform them that their site was infected with the SEO kit,
yet just seven sites subsequently removed it.
5.1.3 Control
We use two different approaches to assess the botmaster’s
ability to monitor and manage the botnet. In the ﬁrst ap-
proach, we observe what fraction of the compromised sites
update their SEO kit when the botmaster deploys a new
version. We can detect both version changes and site up-
dates by parsing the version information from the diagnostic
pages periodically fetched by the host crawler.
As discussed in Section 5.1.1, the data collected by the
host crawler overlaps with two version updates. On Novem-
ber 1, 2011, version OEM updated to MAC and then, on
March 6, 2012, MAC updated to V8. In both cases, we see
a near instantaneous update to the respective new versions
from the majority of the compromised sites, followed by a
sudden addition of newly seen compromised sites.
In the OEM→MAC update, we see many stragglers, sites
that continue running older versions of the SEO kit after
the majority of sites update themselves to the latest ver-
sion. Within a month after the ﬁrst update, 324 out of 970
sites (33%) that comprise the botnet were stragglers. These
stragglers suggest that the botmaster lacks full installation
privileges on the compromised sites and is unable to force
 8*# Months# Sanitized Sites0100200300400500600020406080Date# Sanitized SitesNov 11Jan 12Mar 12May 12Jul 12Group
<11/01
11/01 – 01/28
01/28 – 03/06
<10
10 – 100
100 – 1000
532
71
12
949
28
9
834
31
7
Table 3: The number of compromised Web sites
grouped by the average amount of juice received,
for the three distinct time ranges.
an update. There is no advantage to running old versions
because they poison an outdated set of search terms, are
not well optimized in search results, and consequently will
not attract much trafﬁc. Therefore, the 324 stragglers repre-
sents a substantial inefﬁciency in the botnet. The straggler
phenomenon also occurs during the second update, but the
numbers are less pronounced.
Our second approach for assessing control looks at how
the botmaster adjusts the cross linking policy once a com-
promised site is sanitized and no longer part of the botnet.
Recall that each compromised site is cross linked to other
compromised sites to increase search result ranking (Sec-
tion 5.2). Therefore, when a site is no longer compromised,
there is no value for the site to receive backlinks. Assuming
the botmaster is actively monitoring the sites in the botnet,
he should be able to adjust the cross linking policy to only
link to sites that are still part of the botnet.
Using the set of sanitized sites described in Section 5.1.2,
we track the number of backlinks received by each site over
time from other compromised sites, noting whether a sani-
tized site still receives backlinks and for how long. In addi-
tion, we measure the average number of backlinks received
before a site is sanitized, and after, to see whether the bot-
master updates the cross linking policy to decrease the num-
ber of backlinks given to sanitized sites. Surprisingly, sani-
tized sites still overwhelmingly receive backlinks, and do so
for long periods of time. Out of 508 sanitized sites, nearly
all sites still receive backlinks even after being sanitized: all
but two sanitized sites receive backlinks through February
26, and 488 (96%) through March 2.
In summary, it appears that the botmaster exerts only
limited control over many compromised sites, letting many
degrade over time. Further, this is but one of the inefﬁcien-
cies in how the botnet is operated. While we do not have in-
sight into the reasons for these lapses—whether negligence,
lack of insight, or lack of need—the large numbers of strag-
glers and useless cross linking to sanitized sites makes it
clear that in its existing regime the botnet does not reach its