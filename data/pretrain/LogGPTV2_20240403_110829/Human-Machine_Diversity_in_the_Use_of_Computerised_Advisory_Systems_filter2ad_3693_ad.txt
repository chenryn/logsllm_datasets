### Importance Index and Coherence Index

The "importance index" (of the CADT for the entire system) [2] or "coherence index" (of the reader's behavior with the CADT's advice) can be used to evaluate the interaction between the CADT and human readers. For example, if \( t(x) = 1 \), then the probabilities are such that \( P_{Hf/Mf}(x) = 1 \) and \( P_{Hf/Ms}(x) = 0 \). This means the reader's decision is correct (with probability 1) if and only if the CADT's output is correct.

One might assume that \( t(x) \) measures the extent to which the success rate of humans without CADT assistance would be increased by a perfectly reliable CADT. However, this is not necessarily true because there is no evidence that a human given incorrect advice by the CADT would perform as well (or as poorly) as without any advice. For instance, if readers come to distrust the CADT and disregard its output, then \( t(x) = 0 \) for every case. Yet, the readers' failure probability may remain the same as without the CADT, or it may differ, possibly due to the CADT being a distracting factor.

### Implications of Equation (9)

Figure 3 illustrates the implications of equation (9) for a given value of \( t(x) \). The y-axis represents the probability of system failure (false negative) as a function of the probability of CADT failure (horizontal axis). The term \( t \) is represented by the slope of the line. This line shows how much better or worse the system could be made if the CADT were modified, changing \( P_{Mf} \) uniformly for all classes of cases, without altering \( P_{Hf|Ms} \) and \( P_{Hf|Mf} \), i.e., the way the reader reacts statistically to the CADT's failures and successes.

Equation (9) and Figure 3 show that, for given \( P_{Hf|Mf} \) and \( P_{Hf|Ms} \), there is a lower bound for the probability of system failure, which is \( P_{Hf|Ms} \). This is represented by the point on the left where the sloping line intersects the y-axis. No improvement in the machine will reduce this failure probability unless the reader's skills are also changed.

However, this figure is only a good guide for small changes in \( P_{Mf} \), as \( t \) may not remain constant when the CADT is modified. For example, readers might become less attentive if they perceive the CADT as more dependable. This is one of the possible indirect effects discussed in the previous section.

False negative failures of the CADT are very rare, as \( P_{Mf} \) is designed to be small (at the cost of relatively frequent false positive failures), and cancers are rare in the screened population. Readers may not see enough false negatives to notice a reduction in their rate and adapt their behavior accordingly. Therefore, the effect of reducing the CADT's false negative rate on system failure probability can be read from a graph like Figure 3, once it has been produced by measuring \( P_{Mf}(x) \) and \( t(x) \) in field usage. We assume here that the reduction in the CADT's false negative rate is achieved by improving its algorithm, not simply by changing its decision thresholds and accepting a higher false positive rate, as this could lead to readers trusting the CADT less, thereby changing \( t \).

### Effects of Varying Difficulty of Cases: Role of Covariance

We can rewrite equation (9) as:

\[
P_{Hf} = \sum_x p(x) \cdot (P_{Hf/Ms}(x) + P_{Mf}(x) \cdot t(x)) = E[P_{Hf/Ms}(x)] + E[P_{Mf}(x)] \cdot E[t(x)] + \text{cov}_x(P_{Mf}(x), t(x))
\]

The covariance term highlights the importance of how \( P_{Mf}(x) \) and \( t(x) \) vary among cases. Knowing the average probability of CADT failure and the average effect of the CADT's failure on the reader is not sufficient to determine the probability of system failure due to the additional term \(\text{cov}_x(P_{Mf}(x), t(x))\). If cases where the CADT is more likely to fail (high \( P_{Mf}(x) \)) tend to be those where the reader is more affected by the CADT's failure (high "importance index" \( t(x) \)), the probability of false negatives will be worse than expected. Conversely, if these cases are less frequent, the probability will be better.

For the CADT designer, this means that improving the CADT's failure probability on average over all kinds of cases may not be very useful. It may be more beneficial to concentrate improvements on cases with high \( t(x) \) that are somewhat frequent.

### Identifying High-Importance Classes

Identifying classes of cases with a high "importance index" \( t(x) \) from experimental data poses a challenge. A high \( t(x) \) for a class \( x \) of cases may indicate a homogeneous class of difficult cases for the reader, where a correct CADT output significantly helps. Alternatively, it might mean the class contains both easier and more difficult cases, where both the CADT and the reader tend to succeed or fail, respectively. It would be better to regard \( t(x) \) as a "coherence index." Detailed analysis of the data, possibly requiring more extensive trials, is needed to answer this question, which is a common problem in statistical inference.

### Conclusions

We have explored clear-box reliability modeling for human-machine systems, specifically humans using a computer in an advisory role, modeled as fault-tolerant systems. The main practical advantage is the ability to extrapolate from controlled trial measures to predict field results systematically. The model offers insights into the complexities of human-machine systems, such as the limits on system improvement by only enhancing the CADT and the non-intuitive guidance on targeting specific classes of cases for incremental CADT improvements.

From a general dependability modeling perspective, we have outlined specific modeling problems for human-machine systems, an alternative approach to most "human reliability assessment" methods, and valuable insights. Two key observations are:
- With varying failure probabilities between different classes of cases, it is essential to use detailed conditional probabilities rather than marginal probabilities and avoid unwarranted independence assumptions.
- While the models can predict the effects of small system changes, larger changes may affect the dependability of human components, meaning changes in parameters like \( P_{Mf} \) may affect parameters like \( P_{Hf|Mf} \).

Some argue that these clear-box models are useless and that only extensive empirical trials in realistic conditions should be used. However, such trials may be infeasible, and the clear-box model allows analysts to incorporate existing knowledge about how people's reliability may change based on the machines they use and the demands they face.

Our case study continues with the analysis of recent CADT trials, selecting alternative criteria for dividing cases into classes, and incorporating research results from other disciplines to forecast the effects of CADT reliability on user behavior.

Of broader interest, especially in medical applications, is the study of trade-offs between false positive and false negative failure probabilities. We aim to describe how different settings (compromises between false negative and false positive rates) of the CADT affect the whole system's false negative and false positive rates.

We are also considering more complex systems involving advisory computer products. By modeling these alternatives, we expect to explore modeling problems relevant to wider classes of advisory systems.

In conclusion, this style of modeling appears promising for systems where humans use advisory systems, such as computer-aided screening. Our focus on diversity aspects is novel and important, helping to avoid major pitfalls in reliability modeling and clarifying which improvements to either the computer or human parts of the system will produce tangible benefits for the system as a whole. This is the first case study of its kind, and we expect it to generate knowledge of general interest in terms of models, lessons learned, and data collection problems.

### Acknowledgments

This work was supported in part by the U.K. Engineering and Physical Sciences Research Council via the Interdisciplinary Collaboration on the Dependability of Computer-Based Systems, "DIRC." Many DIRC members contributed useful discussions, particularly Mark Hartswood, Rob Procter, Mark Rouncefield, Paul Taylor, and Jo Champness. David Martin, Andrew Monk, Peter Ryan, Claude Gierl, and the anonymous DSN reviewers provided helpful comments on previous versions of this paper.

### References

[1] E. Alberdi, A. Povyakalo, L. Strigini, P. Ayton. Does incorrect computer prompting affect human decision making? A case study in mammography. To appear in Proceedings of CARS (Computer Aided Radiology and Surgery) 2003. Elsevier Science, NL, 2003.

[2] Z. W. Birnbaum. On the importance of different components in a multicomponent system. In P. R. Krishnaiah (ed.). Multivariate Analysis-II. Academic Press, New York, NY, 1969.

[3] J. Champness, P. Taylor, R. Given-Wilson. Impact of computer-placed prompts on sensitivity and specificity with different groups of mammographic film readers. In H-O. Peitgen (ed.). Proceedings of the 6th International Workshop on Digital Mammography. Springer-Verlag, 2002.

[4] S. Tseng and B. J. Fogg, Credibility and Computing Technology. Communications of the ACM, 42(5):39-44, May 1999.

[5] T. W. Freer and M. J. Ulissey. Screening mammography with computer-aided detection: Prospective study of 12,860 patients in a community breast center. Radiology, 220(3):781-786, 2001.

[6] M. Hartswood, R. Procter. Computer-Aided Mammography: A Case Study of Error Management in a Skilled Decision-making Task. Journal of Topics in Health Information Management, 20(4):38-54, May 2000.

[7] B. Littlewood. The impact of diversity upon common mode failures. Reliability Engineering and System Safety, 51:101-113, 1996.

[8] B. Littlewood, P. Popov, L. Strigini. Modelling software design diversity - a review. ACM Computing Surveys, 33(2):177-208, June 2001.

[9] H. Sittek, C. Perlet, R. Helmberger et al. Computer-assisted analysis of mammograms in routine clinical diagnosis. Radiologe, 38:848-52, 1998.

[10] L. J. Skitka, K. L. Mosier, M. Burdick. Does automation bias decision-making?. International Journal of Human-Computer Studies, 51(5): 991-1006, 1999.

[11] E. Thurfjell, G. Thurfjell, E. Egge, N. Bjurstam. Sensitivity and specificity of computer-assisted breast cancer detection in mammography screening. Acta Radiologica, 39:384-388, 1998.