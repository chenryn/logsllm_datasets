portance index” (of the CADT for the whole system) [2],
or a “coherence index” (of the reader’s behaviour with the
CADT’s advice). E.g., if t(x) = 1, then (since probabili-
ties can only have values between 0 and 1) PHf /Mf (x) =
1 and PHf /Ms(x) = 0, i.e., the reader’s decision is correct
(with probability 1) if and only if the CADT’s output is.
One may think that t(x) measures how much the rate of
success of humans without CADT help would be increased
by the support of a (perfectly reliable) CADT. This is not
necessarily true, because we have no evidence that a human
given wrong advice by the CADT would perform just as
well (or as badly) as without any advice.
For instance,
suppose that readers come to mistrust the CADT and disre-
gard its output completely. Then, t(x) = 0 for every case.
Yet, the readers’ failure probability may be just as without
the CADT, or it may differ, e.g. because the CADT is a
distracting factor.
Fig. 3 shows the implications of equation (9), for a given
value of t(x): the y axis is the probability of (false negative)
system failure as a function of the probability of failure of
the CADT (horizontal axis). The term t is represented by
Figure 3. Role of the factor t(x): probability of
system failure as a function of the probability
of machine failure for a class of cases, given
that the importance index t is known and
ﬁxed (the reader’s reactions to the CADT’s
failures/successes do not change).
the slope of the line. The line shows how much better or
worse we could make the system if we modiﬁed the CADT,
changing PMf (uniformly for all classes of cases), without
changing PHf|Ms and PHf|Mf , i.e. the way the reader re-
acts (statistically) to failures and successes of the CADT.
In particular, equation 9 and Fig. 3 show that, for given
PHf|Mf and PHf|Ms, there is a lower bound for the prob-
this is PHf|Ms, represented by
ability of system failure:
the point on the left where the sloping line intersects the y
axis. No improvement in the machine will reduce this fail-
ure probability, unless we also change the reader’s skills.
Of course, we should expect this ﬁgure only to be a good
guide given small changes of PMf , as t may not remain
constant as the CADT is changed: for instance, readers
might react to a perceived better dependability of the CADT
by unconsciously becoming less attentive. This is one of the
possible “indirect” effects discussed in the previous section.
However, false negative failures of the CADT are very rare:
PMf is small by design (at the cost of relatively frequent
false positive failures), and cancers are rare in the screened
population. So, readers may not usually see enough of them
to be able to notice a reduction in their rate and adapt their
own behaviour accordingly. Then, the effect of reducing
the CADT’s false negative rate on system failure probabil-
ity could be read on a graph as in Fig. 3, once this had
been produced by measuring PMf (x) and t(x) in ﬁeld us-
age (we are assuming here that the reduction in the CADT’s
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:31:04 UTC from IEEE Xplore.  Restrictions apply. 
false negative rate is achieved by improving its algorithm,
not simply by changing its decision thresholds and accept-
ing a higher false positive rate: readers could notice this and
learn to trust the CADT less, which would change their t).
6.2 Effects of varying difﬁculty of cases: role of
covariance
We can rewrite equation (9) as
(cid:1)
PHf =
p(x) · (PHf /Ms(x) + PMf (x) · t(x)) =
x
E[PHf /Ms(x)] + E[PMf (x)] · E[t(x)]+
+ covx(PMf (x), t(x)) =
E[PHf /Ms(x)] + PMf · E[t(x)] + covx(PMf (x), t(x))
(10)
The “covariance” term highlights the importance of how
the two measures, PMf (x) and t(x), vary among cases.
Knowing both the average probability of failure of the
CADT and the average effect of the CADT’s failure on the
reader is not enough to determine the probability of system
failure, because of the additional term covx(PMf (x), t(x))
in the equation. Rather, if those cases on which the CADT is
more likely to fail (high PMf (x)) tend to be those for which
the reader is more likely to be affected by the CADT’s fail-
ure (high “importance index” t(x)), then the probability of
false negative will be worse than the means would make us
expect; and vice versa. For the designer of the CADT, this
means that improving the CADT’s failure probability on av-
erage over all kinds of cases may not be very useful. It may
be more useful to concentrate any improvements on cases
for which readers have a high t(x) (and that are somewhat
frequent).
We should note here a problem with identifying classes
of cases with a high “importance index” t(x) from experi-
mental data. A high t(x) for a class x of cases may mean
that it is a homogeneous class of cases that are difﬁcult for
the readers’ “detection” task, a difﬁculty that is greatly al-
leviated by a correct output from the CADT. However, it
might also mean that the class contains “easier” cases on
which both the CADT and the reader tend to succeed, and
“more difﬁcult” ones on which they both tend to fail.
It
would be better then to regard t(x) as just a “coherence in-
dex”. It may be that within either subclass the reader is not
affected at all by whether the CADT’s output is correct or
erroneous. Only a more detailed analysis of the data (possi-
bly requiring more extensive trials, and possibly infeasible)
could answer this question. This is a common problem in
statistical inference.
7 Conclusions
We have explored clear-box reliability modelling for an
important class of human-machine systems – humans using
a computer in an advisory role – modelled as fault-tolerant
systems.
The main practical advantages in this speciﬁc case is
some help in extrapolating from measures taken in a con-
trolled trial to predictions of results in the ﬁeld in an orderly
way. We have also shown some insights that the modelling
offers into otherwise confusing complexities of the human-
machine system. E.g., in section 6.1, the limits on how
much one can improve the system by only improving the
CADT; in section 6.2, the non-intuitive indication of how to
choose classes of cases to target for incremental improve-
ments of the CADT.
From the viewpoint of dependability modelling in gen-
eral, we have outlined some modelling problems speciﬁc
to human-machine systems, an alternative description ap-
proach to that of most “human reliability assessment” meth-
ods, and some useful insights, as just described.
There are two kinds of observations that seem important:
• with failure probabilities for the two components that
vary heavily between classes of cases, it is essential
in modelling to use detailed probabilities of compo-
nent failure conditional on types of demand, rather
than marginal probabilities, and to avoid any unwar-
ranted assumption of independence between compo-
nents. This is a problem that also affects all reliability
models for systems without humans [8, 7];
• while models of the kind presented will predict the ef-
fects of small changes in the system or its environment
of use, larger changes may affect the dependability of
the human components. That is, changes in param-
eters like PMf may affect parameters like PHf|Mf .
Some may argue that this makes these clear-box mod-
els useless, and only extensive empirical trials in real-
istic conditions should be used to assess these human-
machine systems. We observe instead that such empir-
ical trials may be infeasible, but the clear box model al-
lows an analyst to take account of any existing knowl-
edge about how people’s reliability may change de-
pending on the characteristics of the machines they use
and the demands they face.
Our case study is continuing with the analysis of data
from recent trials of use of the CADT [3], selecting alter-
native criteria for dividing the cases into classes, and addi-
tional data collection [1] and incorporating research results
from other disciplines to forecast the effects of CADT relia-
bility on user behaviour (cf for instance, about “automation
bias”, [10] and its references).
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:31:04 UTC from IEEE Xplore.  Restrictions apply. 
Of more general interest, in and beyond the medical ap-
plication area, will be the study of trade-offs between the
probabilities of false positive and false negative failures, a
very common problem. We would attempt to describe, for
instance, how alternative settings (compromises between
false negative and false positive rates) of the CADT would
affect the whole system’s false negative and false positive
rates.
We are also considering looking at more complex sys-
tems that include advisory computer products. We have
so far only modelled the behaviour of the simple system
composed of a single human user assisted by a machine.
But more complex combinations have also been considered
for use of the CADT, to improve the cost-effectiveness of
screening programmes; e.g. with two readers assisted by a
CADT, or less qualiﬁed readers assisted by CADTs. By
modelling these alternatives, we expect to explore mod-
elling problems that are relevant to wider classes of advi-
sory systems.
In conclusion, this style of modelling appears promising
for the broad category of systems, “humans using advisory
systems”, of which computer-aided screening is an exam-
ple. There is nothing especially new in applying the engi-
neering style of reliability modelling to systems including
humans and machines, but we believe that our focus on di-
versity aspects is novel and important. In particular, it helps
to avoid major pitfalls that affect the reliability modelling of
any system – unjustiﬁed independence assumptions and un-
warranted predictions based on average parameter values –
and to clarify which improvements to either the computer or
human parts of the system will produce tangible improve-
ments for the system as a whole. This is just the ﬁrst case
study of its kind that we know of. In addition to any con-
clusions we obtain about the case study itself, we expect
it to generate knowledge of more general interest, in terms
of models, lessons learned about how to abstract the inter-
action between humans and machines, and data collection
problems.
Acknowledgments
This work was supported in part by the U.K. Engineer-
ing and Physical Sciences Research Council via the Inter-
disciplinary Collaboration on the Dependability of com-
puter based systems, “DIRC”. Many DIRC members con-
tributed useful discussion, and especially Mark Hartswood,
Rob Procter, Mark Rounceﬁeld, as did Paul Taylor and Jo
Champness, from University College, London. David Mar-
tin, Andrew Monk, Peter Ryan, Claude Gierl and the anony-
mous DSN reviewers provided helpful comments on previ-
ous versions of this paper.
References
[1] E. Alberdi, A. Povyakalo, L. Strigini, P. Ayton. Does
incorrect computer prompting affect human decision
making? A case study in mammography. To appear
in Proceedings of CARS (Computer Aided Radiology
and Surgery) 2003. Elsevier Science, NL, 2003.
[2] Z. W. Birnbaum. On the importance of different com-
ponents in a multicomponent system. In P. R. Krish-
naiah (ed.). Multivariate Analysis-II. Academic Press,
New York, NY, 1969.
[3] J. Champness, P. Taylor, R. Given-Wilson. Impact
of computer-placed prompts on sensitivity and speci-
ﬁcity with different groups of mammographic ﬁlm
readers. In H-O. Peitgen (ed.). Proceedings of the
6th International Workshop on Digital Mammogra-
phy. Springer-Verlag, 2002.
[4] S. Tseng and B. J. Fogg, Credibility and Computing
Technology. Communications of the ACM, 42(5):39-
44, May 1999.
[5] T. W. Freer and M. J. Ulissey. Screening mammogra-
phy with computer-aided detection: Prospective study
of 12,860 patients in a community breast centre. Radi-
ology, 220(3):781-786, 2001.
[6] M. Hartswood, R. Procter. Computer-Aided Mam-
mography: A Case Study of Error Management in a
Skilled Decision-making Task. Journal of Topics in
Health Information Management, 20(4):38-54, May
2000.
[7] B. Littlewood. The impact of diversity upon com-
mon mode failures. Reliability Engineering and Sys-
tem Safety, 51:101-113, 1996.
[8] B. Littlewood, P. Popov, L. Strigini. Modelling soft-
ware design diversity - a review. ACM Computing Sur-
veys, 33(2):177-208, June 2001.
[9] H. Sittek, C. Perlet, R. Helmberger et al. Computer-
assisted analysis of mammograms in routine clinical
diagnosis. Radiologe, 38:848-52, 1998.
[10] L. J. Skitka, K. L. Mosier, M. Burdick. Does automa-
tion bias decision-making?. International Journal of
Human-Computer Studies, 51(5): 991-1006, 1999.
[11] E. Thurfjell, G. Thurfjell, E. Egge, N. Bjurstam.
Sensitivity and speciﬁcity of computer-assisted breast
cancer detection in mammography screening. Acta
Radiologica, 39:384-388, 1998.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:31:04 UTC from IEEE Xplore.  Restrictions apply.