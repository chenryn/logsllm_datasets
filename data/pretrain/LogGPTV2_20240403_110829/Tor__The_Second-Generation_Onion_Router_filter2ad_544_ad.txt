ture schedule of unadvertised introduction points; this is most
practical if there is a stable and large group of introduction
points available. Bob could also give secret public keys for
consulting the lookup service. All of these approaches limit
exposure even when some selected users collude in the DoS.
5.2 Integration with user applications
Bob conﬁgures his onion proxy to know the local IP address
and port of his service, a strategy for authorizing clients, and
his public key. The onion proxy anonymously publishes a
signed statement of Bob’s public key, an expiration time, and
the current introduction points for his service onto the lookup
service, indexed by the hash of his public key. Bob’s web-
server is unmodiﬁed, and doesn’t even know that it’s hidden
behind the Tor network.
Alice’s applications also work unchanged—her client
interface remains a SOCKS proxy. We encode all of
the necessary information into the fully qualiﬁed domain
name (FQDN) Alice uses when establishing her connection.
Location-hidden services use a virtual top level domain called
.onion: thus hostnames take the form x.y.onion where
x is the authorization cookie and y encodes the hash of
the public key. Alice’s onion proxy examines addresses; if
they’re destined for a hidden server, it decodes the key and
starts the rendezvous as described above.
5.3 Previous rendezvous work
Rendezvous points in low-latency anonymity systems were
ﬁrst described for use in ISDN telephony [30, 38]. Later low-
latency designs used rendezvous points for hiding location
of mobile phones and low-power location trackers [23, 40].
Rendezvous for anonymizing low-latency Internet connec-
tions was suggested in early Onion Routing work [27], but
the ﬁrst published design was by Ian Goldberg [26]. His de-
sign differs from ours in three ways. First, Goldberg suggests
that Alice should manually hunt down a current location of
the service via Gnutella; our approach makes lookup trans-
parent to the user, as well as faster and more robust. Second,
in Tor the client and server negotiate session keys with Difﬁe-
Hellman, so plaintext is not exposed even at the rendezvous
point. Third, our design minimizes the exposure from run-
ning the service, to encourage volunteers to offer introduc-
tion and rendezvous services. Tor’s introduction points do not
output any bytes to the clients; the rendezvous points don’t
know the client or the server, and can’t read the data being
transmitted. The indirection scheme is also designed to in-
clude authentication/authorization—if Alice doesn’t include
the right cookie with her request for service, Bob need not
even acknowledge his existence.
6 Other design decisions
6.1 Denial of service
Providing Tor as a public service creates many opportuni-
ties for denial-of-service attacks against the network. While
ﬂow control and rate limiting (discussed in Section 4.6) pre-
vent users from consuming more bandwidth than routers are
willing to provide, opportunities remain for users to consume
more network resources than their fair share, or to render the
network unusable for others.
First of all, there are several CPU-consuming denial-of-
service attacks wherein an attacker can force an OR to per-
form expensive cryptographic operations. For example, an at-
tacker can fake the start of a TLS handshake, forcing the OR
to carry out its (comparatively expensive) half of the hand-
shake at no real computational cost to the attacker.
We have not yet implemented any defenses for these at-
tacks, but several approaches are possible. First, ORs can
require clients to solve a puzzle [16] while beginning new
TLS handshakes or accepting create cells. So long as these
tokens are easy to verify and computationally expensive to
produce, this approach limits the attack multiplier. Addition-
ally, ORs can limit the rate at which they accept create cells
and TLS connections, so that the computational work of pro-
cessing them does not drown out the symmetric cryptography
operations that keep cells ﬂowing. This rate limiting could,
however, allow an attacker to slow down other users when
they build new circuits.
Adversaries can also attack the Tor network’s hosts and
network links. Disrupting a single circuit or link breaks all
streams passing along that part of the circuit. Users simi-
larly lose service when a router crashes or its operator restarts
it. The current Tor design treats such attacks as intermit-
tent network failures, and depends on users and applications
to respond or recover as appropriate. A future design could
use an end-to-end TCP-like acknowledgment protocol, so no
streams are lost unless the entry or exit point is disrupted.
This solution would require more buffering at the network
edges, however, and the performance and anonymity impli-
cations from this extra complexity still require investigation.
6.2 Exit policies and abuse
Exit abuse is a serious barrier to wide-scale Tor deployment.
Anonymity presents would-be vandals and abusers with an
opportunity to hide the origins of their activities. Attackers
can harm the Tor network by implicating exit servers for their
abuse. Also, applications that commonly use IP-based au-
thentication (such as institutional mail or webservers) can be
fooled by the fact that anonymous connections appear to orig-
inate at the exit OR.
We stress that Tor does not enable any new class of abuse.
Spammers and other attackers already have access to thou-
sands of misconﬁgured systems worldwide, and the Tor net-
work is far from the easiest way to launch attacks. But be-
cause the onion routers can be mistaken for the originators
of the abuse, and the volunteers who run them may not want
to deal with the hassle of explaining anonymity networks to
irate administrators, we must block or limit abuse through the
Tor network.
To mitigate abuse issues, each onion router’s exit policy de-
scribes to which external addresses and ports the router will
connect. On one end of the spectrum are open exit nodes
that will connect anywhere. On the other end are middleman
nodes that only relay trafﬁc to other Tor nodes, and private
exit nodes that only connect to a local host or network. A
private exit can allow a client to connect to a given host or
network more securely—an external adversary cannot eaves-
drop trafﬁc between the private exit and the ﬁnal destination,
and so is less sure of Alice’s destination and activities. Most
onion routers in the current network function as restricted ex-
its that permit connections to the world at large, but prevent
access to certain abuse-prone addresses and services such as
SMTP. The OR might also be able to authenticate clients to
prevent exit abuse without harming anonymity [48].
Many administrators use port restrictions to support only a
limited set of services, such as HTTP, SSH, or AIM. This is
not a complete solution, of course, since abuse opportunities
for these protocols are still well known.
We have not yet encountered any abuse in the deployed
network, but if we do we should consider using proxies to
clean trafﬁc for certain protocols as it leaves the network. For
example, much abusive HTTP behavior (such as exploiting
buffer overﬂows or well-known script vulnerabilities) can be
detected in a straightforward manner. Similarly, one could
run automatic spam ﬁltering software (such as SpamAssas-
sin) on email exiting the OR network.
ORs may also rewrite exiting trafﬁc to append headers
or other information indicating that the trafﬁc has passed
through an anonymity service. This approach is commonly
used by email-only anonymity systems. ORs can also run
on servers with hostnames like anonymous to further alert
abuse targets to the nature of the anonymous trafﬁc.
A mixture of open and restricted exit nodes allows the most
ﬂexibility for volunteers running servers. But while having
many middleman nodes provides a large and robust network,
having only a few exit nodes reduces the number of points an
adversary needs to monitor for trafﬁc analysis, and places a
greater burden on the exit nodes. This tension can be seen in
the Java Anon Proxy cascade model, wherein only one node
in each cascade needs to handle abuse complaints—but an ad-
versary only needs to observe the entry and exit of a cascade
to perform trafﬁc analysis on all that cascade’s users. The hy-
dra model (many entries, few exits) presents a different com-
promise: only a few exit nodes are needed, but an adversary
needs to work harder to watch all the clients; see Section 10.
Finally, we note that exit abuse must not be dismissed as
a peripheral issue: when a system’s public image suffers, it
can reduce the number and diversity of that system’s users,
and thereby reduce the anonymity of the system itself. Like
usability, public perception is a security parameter. Sadly,
preventing abuse of open exit nodes is an unsolved problem,
and will probably remain an arms race for the foreseeable
future. The abuse problems faced by Princeton’s CoDeeN
project [37] give us a glimpse of likely issues.
6.3 Directory Servers
First-generation Onion Routing designs [8, 41] used in-band
network status updates: each router ﬂooded a signed state-
ment to its neighbors, which propagated it onward. But
anonymizing networks have different security goals than typ-
ical link-state routing protocols. For example, delays (acci-
dental or intentional) that can cause different parts of the net-
work to have different views of link-state and topology are
not only inconvenient: they give attackers an opportunity to
exploit differences in client knowledge. We also worry about
attacks to deceive a client about the router membership list,
topology, or current network state. Such partitioning attacks
on client knowledge help an adversary to efﬁciently deploy
resources against a target [15].
Tor uses a small group of redundant, well-known onion
routers to track changes in network topology and node state,
including keys and exit policies. Each such directory server
acts as an HTTP server, so clients can fetch current network
state and router lists, and so other ORs can upload state infor-
mation. Onion routers periodically publish signed statements
of their state to each directory server. The directory servers
combine this information with their own views of network
liveness, and generate a signed description (a directory) of
the entire network state. Client software is pre-loaded with a
list of the directory servers and their keys, to bootstrap each
client’s view of the network.
When a directory server receives a signed statement for an
OR, it checks whether the OR’s identity key is recognized.
Directory servers do not advertise unrecognized ORs—if they
did, an adversary could take over the network by creating
many servers [22]. Instead, new nodes must be approved by
the directory server administrator before they are included.
Mechanisms for automated node approval are an area of ac-
tive research, and are discussed more in Section 9.
Of course, a variety of attacks remain. An adversary who
controls a directory server can track clients by providing them
different information—perhaps by listing only nodes under
its control, or by informing only certain clients about a given
node. Even an external adversary can exploit differences in
client knowledge: clients who use a node listed on one direc-
tory server but not the others are vulnerable.
Thus these directory servers must be synchronized and
redundant, so that they can agree on a common directory.
Clients should only trust this directory if it is signed by a
threshold of the directory servers.
The directory servers in Tor are modeled after those in
Mixminion [15], but our situation is easier. First, we make
the simplifying assumption that all participants agree on the
set of directory servers. Second, while Mixminion needs
to predict node behavior, Tor only needs a threshold con-
sensus of the current state of the network. Third, we as-
sume that we can fall back to the human administrators to
discover and resolve problems when a consensus directory
cannot be reached. Since there are relatively few directory
servers (currently 3, but we expect as many as 9 as the net-
work scales), we can afford operations like broadcast to sim-
plify the consensus-building protocol.
To avoid attacks where a router connects to all the direc-
tory servers but refuses to relay trafﬁc from other routers,
the directory servers must also build circuits and use them to
anonymously test router reliability [18]. Unfortunately, this
defense is not yet designed or implemented.
Using directory servers is simpler and more ﬂexible than
ﬂooding. Flooding is expensive, and complicates the analysis
when we start experimenting with non-clique network topolo-
gies. Signed directories can be cached by other onion routers,
so directory servers are not a performance bottleneck when
we have many users, and do not aid trafﬁc analysis by forcing
clients to announce their existence to any central point.
7 Attacks and Defenses
Below we summarize a variety of attacks, and discuss how
well our design withstands them.
Passive attacks
Observing user trafﬁc patterns. Observing a user’s connec-
tion will not reveal her destination or data, but it will reveal
trafﬁc patterns (both sent and received). Proﬁling via user
connection patterns requires further processing, because mul-
tiple application streams may be operating simultaneously or
in series over a single circuit.
Observing user content. While content at the user end is
encrypted, connections to responders may not be (indeed, the
responding website itself may be hostile). While ﬁltering
content is not a primary goal of Onion Routing, Tor can di-
rectly use Privoxy and related ﬁltering services to anonymize
application data streams.
Option distinguishability. We allow clients to choose con-
ﬁguration options. For example, clients concerned about re-
quest linkability should rotate circuits more often than those
concerned about traceability. Allowing choice may attract
users with different needs; but clients who are in the minor-
ity may lose more anonymity by appearing distinct than they
gain by optimizing their behavior [1].
End-to-end timing correlation. Tor only minimally hides
such correlations. An attacker watching patterns of trafﬁc at
the initiator and the responder will be able to conﬁrm the cor-
respondence with high probability. The greatest protection
currently available against such conﬁrmation is to hide the
connection between the onion proxy and the ﬁrst Tor node,
by running the OP on the Tor node or behind a ﬁrewall. This
approach requires an observer to separate trafﬁc originating at
the onion router from trafﬁc passing through it: a global ob-
server can do this, but it might be beyond a limited observer’s
capabilities.
End-to-end size correlation. Simple packet counting will
also be effective in conﬁrming endpoints of a stream. How-
ever, even without padding, we may have some limited pro-
tection: the leaky pipe topology means different numbers of
packets may enter one end of a circuit than exit at the other.
Website ﬁngerprinting. All the effective passive attacks
above are trafﬁc conﬁrmation attacks, which puts them out-
side our design goals. There is also a passive trafﬁc analysis
attack that is potentially effective. Rather than searching
exit connections for timing and volume correlations,
the
adversary may build up a database of “ﬁngerprints” contain-
ing ﬁle sizes and access patterns for targeted websites. He
can later conﬁrm a user’s connection to a given site simply
by consulting the database. This attack has been shown to
be effective against SafeWeb [29]. It may be less effective
against Tor, since streams are multiplexed within the same
circuit, and ﬁngerprinting will be limited to the granularity
of cells (currently 512 bytes). Additional defenses could
include larger cell sizes, padding schemes to group websites
into large sets, and link padding or long-range dummies.4
Active attacks
Compromise keys. An attacker who learns the TLS session
key can see control cells and encrypted relay cells on every
circuit on that connection; learning a circuit session key lets