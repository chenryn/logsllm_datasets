8
Input: a query
select all the logs recorded between 20:32:46 and 20:32:56, July 9, 2012
HLAer
Output: all logs that satisfy the queryOutput: all logs that satisfy the query
2012-07-09 20:32:56,458 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on · · · error: Cannot delete /home/hadoop user/hadoop manish/hadoop-hadoop user/mapred/system. Name node is in safe mode.2012-07-09 20:32:46,864 INFO org.apache.hadoop.hdfs.util.GSet: recommended=4194304, actual=4194304 2012-07-09 20:32:47,905 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled=false 2012-07-09 20:32:47,909 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem dfs.block.invalidate.limit=100 2012-07-09 20:32:46,904 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner=hadoop user 2012-07-09 20:32:47,905 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup=supergroupFig. 6: HLAer query support
Input: a batch of new logs
2012-07-09 20:32:56,458 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on · · · error: Cannot delete /home/hadoop user/hadoop manish/hadoop-hadoop user/mapred/system. Name node is in safe mode.2012-07-09 20:32:46,864 INFO org.apache.hadoop.hdfs.util.GSet: recommended=4194304, actual=4194304 2012-07-09 20:32:47,905 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled=false 2012-07-09 20:32:47,909 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem dfs.block.invalidate.limit=100 2012-07-09 20:32:46,904 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner=hadoop user 2012-07-09 20:32:47,905 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup=supergroupHLAer
Output: outliers
2012-07-09 20:32:56,458 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on · · · error: Cannot delete /home/hadoop user/hadoop manish/hadoop-hadoop user/mapred/system. Name node is in safe mode.
2012-07-09 20:32:47,905 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled=false
Fig. 7: HLAer outlier detection
B. Log data clusteringWithout domain knowledge with respect to the log formats, usage and sources, etc, a first step towards understanding and analyzing heterogeneous logs is intuitively to understand the geometric structure of the log data. As unsupervised data analysis methods, clustering algorithms serve as a way to categorize data purely based on their intrinsic properties and relations. Thus, within the framework of HLAer, a clustering algorithm is applied on the heterogeneous logs so as to present the initial depiction of the data. In specific, a hierarchical clustering algorithm is used in HLAer to generate a hierarchical structure of the heterogeneous logs. Hierarchical clustering is preferred not only because they can provide a coarse-to-fine view of the data, but also because the following data indexing and search is built up on a hierarchical tree structure for efficiency purposes. The hierarchical tree structure used in HLAer is denoted as Log Clustering Tree (LCT). The process for log data clustering is summarized in Figure 2. In Figure 2, since the calculation of pair-wise log similarity serves for the following clustering processes, the clustering method is described first as follows.In HLAer, the hierarchical clustering algorithm Ordering Points To Identify the Clustering Structure (OPTICS) [1] is
9implemented. The basic idea of OPTICS is based on DBScan [6], that is, the dense data regions with sufficient number of data points form clusters. The basic idea of DBScan algorithm is depicted in Algorithm 212and Algorithm 3 in the Appendix. Intuitively, DBScan searches dense data regions by expanding from a certain data point towards all its neighboring data points that are sufficiently close under a pre-defined threshold. OPTICS performs DBScan but meanwhile outputs a certain ordering of the data points based on their smallest reachability distances. The reachability distance is defined as in Equation 6 in the Appendix, in which the core distance is defined in Equation 7. Intuitively, the reachability distance of a certain data point measures how close it is to its dense neighborhood, while the core distance of a certain data point measures how tight its neighborhood is centered around the point. In addition, OPTICS generates a hierarchical clustering structure from the data point ordering, in which the denser data region within a looser data region, which is still qualified as a cluster, becomes a lower-level child of the looser region (i.e., cluster). Thus, the hierarchical structure constructed from OPTICS represents the inherent data relations among all the data points.The reason why OPTICS is selected is due to the fact that OPTICS has the following properties:
•	It is scalable to large datasets and can be easily paralleled [2], which is a very favorable feature particularly 	for Big Data.
•	It has only a few parameters (i.e., ✏, minpts) and these parameters can be easily obtained from historical data, 	given that the data are from a stable system.•	It is computationally efficient and runs significantly faster than other methods such as spectral clustering.
However, the framework of HLAer is general enough to adopt various hierarchical clustering algorithms as long as they satisfy the above properties. For example, the hierarchical clustering algorithms in the software CLUTO13are also good options.1) Setting Parameters: There are three critical parameters for OPTICS algorithm, i.e., the minimum number of data points minpts that can form a valid cluster, the maximum distance ✏ between two data points that is allowed within a cluster and the distance function dist() that is used to measure the distance between two data points. Both minpts and ✏ can be obtained based on empirical experiments. A small set of samples can be drawn first. Then a grid search on minpts and ✏ can be performed on the samples so as to identify the optimal minpts and ✏ that give satisfactory clustering results. The distance function dist() is discussed later in the next section.2) The format-specific similarity/distance function: In HLAer, a format-specific similarity function (fssim) is
proposed as follows,
fssim(log1, log2) = Pmin(|log1|,|log2|) |log1||log2| 	If(log1(i), log2(i)) , 	(1)
where log1 and log2 are two log records, and the identity function If(x, y) is defined as follows,
and log(i) is the i-th word of the log record after tokenization. Intuitively, fssim measures how two log records are If(x, y) =8  1
1
1
0 	if x and y are both numerical 
	if x and y are identical words 
	if x and y are identical symbols 	otherwise, 
	(2)
similar/identical from their very beginning to the end. The reason of summing from the very beginning towards the end of the two log records in Equation 2 (e.g.,Pmin(|log1|,|log2|) each log record has to have following a certain pre-defined format goes first in the log record, whereas the specific information that varies with respect to each log record comes late.For example, the two log records) is that very popularly the common information that
“Jan 8 05:49:14 www httpd[7855]: 108.199.240.249 - - GET /images/header/nav-pub-on.gif HTTP/1.1 200 569” and
“Feb 2 05:49:27 www httpd[7855]: 108.199.240.249 - - GET /careers/internship.php HTTP/1.1 200 11007”
have a same format and will have a large similarity value by fssim, whereas the two log records“Jan 8 05:49:14 www httpd[7855]: 108.199.240.249 - - GET /images/header/nav-pub-on.gif HTTP/1.1 200 569” and
“2012-07-09 20:32:46,864 INFO org. apache. hadoop. hdfs. util. GSet: recommended = 4194304, actual = 4194304”
12http://en.wikipedia.org/wiki/DBSCAN 
13http://glaros.dtc.umn.edu/gkhome/views/cluto
1010
have different formats and will have a smaller similarity value by fssim. Thus, fssim is strong in capturing pattern similarity. In spirit, fssim is similar to edit distance, but it is highly adapted to log data natures.The identity function If(x, y) in Equation 2 is defined in a way so as to leverage the fact that in a large volume of log records, the vocabulary is very limited compared to natural languages and in normal cases the words are highly repetitive, and thus binary values are sufficient to capture the word relations. Intuitively, however, the identity function If(x, y) can be replaced by a similarity measurement whose values fall in [0, 1]. Nevertheless, as the first step without any knowledge, binary values and identity function serve well as a good seed.Given fssim 2 [0, 1], a format-specific distance function fsdis is defined as 
	fsdis(log1, log2) = 1  fssim(log1, log2), 	(3)
which is used in DBScan.Adaptive format-specific similarity/distance function: note that If has a rigid format, that is, the similarity between two words (except numerical values and symbols) in two logs is only determined by whether they are identical or not, thus it is not flexible enough for words which are not identical but “similar” in some sense. For example, in log messages, the words “errors”and “mistakes” should have higher similarity based on their meanings rather than 0 calculated from fssim. Thus, If and fssim are not strong enough to capture semantics. However, this drawback of If and fssim can be remedied via active learning as in [12]. The basic idea is to leverage the co-occurrence of word pairs over a large number of log records, and then estimate the word similarity using their co-occurrence frequency with a third common word.Sequential ordering vs alignment: note that fssim calculates the similarity between two log records from the beginning to the end, that is, in a sequential order. This may not be the best to fully capture the structural and content similarity of the two logs if variables/parameters occur very early in the log records, which may significantly alter the alignment of template words and thus result in low similarity values. An alternative is to first align the logs and then calculate the similarity from the alignment. The idea of alignment will be addressed later, but as in the every first steps for data processing, a coarse-grained similarity/distance function is sufficient for the basic clustering as indicated in the experiments. However, systematically the similarity function can be improved iteratively via active learning so as to be better customized to the problem of interest.C. Pattern recognition and Field Analysis