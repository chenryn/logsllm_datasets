```
148c148
 60
---
>   expr: (time() - kube_pod_start_time{namespace!="kube-system"}) > (60 * 60 * 24 * 90)
154a155,172
> - alert: ReservedMemTooLow
>   expr: sum(label_join(container_memory_usage_bytes{namespace!="kube-system", namespace!="ingress-nginx"}, "pod", ",", "pod_name")) by (pod) /
 sum(kube_pod_container_resource_requests_memory_bytes{namespace!="kube-system"}) by (pod) > 1.5
>   for: 1m
>   labels:
>     severity: notify
>     frequency: low
>   annotations:
>     summary: Reserved memory is too low
>     description: At least one Pod uses much more memory than it reserved
> - alert: ReservedMemTooHigh
>   expr: sum(label_join(container_memory_usage_bytes{namespace!="kube-system", namespace!="ingress-nginx"}, "pod", ",", "pod_name")) by (pod) / sum(kube_pod_container_resource_requests_memory_bytes{namespace!="kube-system"}) by (pod)  5.25e+06
>   for: 6m
>   labels:
>     severity: notify
>     frequency: low
>   annotations:
>     summary: Reserved memory is too high
>     description: At least one Pod uses much less memory than it reserved
```
首先，我们将`OldPods`警报的阈值设置回 90 天(`60 * 60 * 24 * 90`)的预期值。这样我们就可以阻止它仅仅为了测试目的而触发警报。
接下来，我们定义了一个名为`ReservedMemTooLow`的新警报。如果使用的内存比请求的内存大`1.5`倍以上，它就会触发。警报的未决状态的持续时间被设置为`1m`，只是为了我们可以看到结果，而不需要等待整整一个小时。稍后，我们会将其恢复到`1h`。
`ReservedMemTooHigh`警报与前一个(部分)类似，只是如果实际内存和请求内存之间的差异小于`0.5`，并且如果继续超过`6m`(我们稍后会将其更改为`6h`)的情况，它将触发警报。表达式的第二部分是新的。它要求 Pod 中的所有容器都有超过 5 MB 的请求内存(`5.25e+06`)。通过第二种说法(用`and`隔开)，我们避免了处理太小的应用。如果它需要少于 5 MB 的内存，我们应该忽略它，并可能祝贺它背后的团队使它如此高效。
现在，让我们用更新的值升级我们的普罗米修斯图表，并打开图表屏幕。
```
 1  helm upgrade -i prometheus \
 2    stable/prometheus \
 3    --namespace metrics \
 4    --version 7.1.3 \
 5    --set server.ingress.hosts={$PROM_ADDR} \
 6    --set alertmanager.ingress.hosts={$AM_ADDR} \
 7    -f mon/prom-values-req-mem.yml
```
我们不会等到警报响起。相反，我们将尝试完成类似的目标，但使用 CPU。
可能没有必要解释我们将要使用的表达方式。我们将通过探索新旧图表值之间的差异，直接进入基于 CPU 的警报。
```
 1  diff mon/prom-values-req-mem.yml \
 2      mon/prom-values-req-cpu.yml
```
输出如下。
```
157c157
   for: 1h
166c166
   for: 6h
172a173,190
> - alert: ReservedCPUTooLow
>   expr: sum(label_join(rate(container_cpu_usage_seconds_total{namespace!="kube-system", namespace!="ingress-nginx", pod_name!=""}[5m]), "pod", ",", "pod_name")) by (pod) / sum(kube_pod_container_resource_requests_cpu_cores{namespace!="kube-system"}) by (pod) > 1.5
>   for: 1m
>   labels:
>     severity: notify
>     frequency: low
>   annotations:
>     summary: Reserved CPU is too low
>     description: At least one Pod uses much more CPU than it reserved
> - alert: ReservedCPUTooHigh
>   expr: sum(label_join(rate(container_cpu_usage_seconds_total{namespace!="kube-system", pod_name!=""}[5m]), "pod", ",", "pod_name")) by (pod) / sum(kube_pod_container_resource_requests_cpu_cores{namespace!="kube-system"}) by (pod)  0.005
>   for: 6m
>   labels:
>     severity: notify
>     frequency: low
>   annotations:
>     summary: Reserved CPU is too high
>     description: At least one Pod uses much less CPU than it reserved
```
前两组差异为我们之前探讨的`ReservedMemTooLow`和`ReservedMemTooHigh`警报定义了更合理的阈值。再往下，我们可以看到两个新的警报。
如果中央处理器使用量超过请求量的 1.5 倍，将触发`ReservedCPUTooLow`警报。类似地，`ReservedCPUTooHigh`警报只有在 CPU 使用率低于请求的一半，并且我们请求的 CPU 时间超过 5 毫秒时才会触发。因为 5 MB 内存太大而收到通知是浪费时间。
如果问题持续很短的时间(`1m`和`6m`)，两个警报都会被触发，这样我们就可以看到它们在运行，而不必等待太长时间。
现在，让我们用更新的值升级我们的普罗米修斯图表。
```
 1  helm upgrade -i prometheus \
 2    stable/prometheus \
 3    --namespace metrics \
 4    --version 7.1.3 \
 5    --set server.ingress.hosts={$PROM_ADDR} \
 6    --set alertmanager.ingress.hosts={$AM_ADDR} \
 7    -f mon/prom-values-req-cpu.yml
```
我将让您检查是否有任何警报触发，以及它们是否从 Alertmanager 转发到 Slack。你现在应该知道怎么做了。
接下来，我们将进入本章的最后一个警报。
# 将实际资源使用情况与定义的限制进行比较
与请求相比，知道容器何时使用过多或过少的资源有助于我们更精确地定义资源，并最终帮助 Kubernetes 更好地决定在哪里调度 Pods。在大多数情况下，请求的资源使用量和实际资源使用量之间有太大的差异不会导致故障。相反，这更有可能导致 Pods 分布不均衡，或者节点数量超过我们的需求。另一方面，极限是另一回事。
如果封装为 Pods 的容器的资源使用达到指定的`limits`，如果没有足够的内存，Kubernetes 可能会杀死这些容器。它这样做是为了保护系统其余部分的完整性。被杀死的豆荚不是一个永久的问题，因为如果有足够的容量，Kubernetes 几乎会立即重新安排它们。
如果我们使用集群自动缩放，即使没有足够的容量，只要它检测到一些 Pods 处于挂起状态(不可聚集)，就会添加新节点。因此，如果资源使用超过极限，世界不太可能结束。
然而，杀死和重新安排 Pods 会导致停机。显然，可能会发生更糟糕的情况。但我们不会深入其中。相反，我们将假设我们应该意识到一个 Pod 即将达到它的极限，我们可能想要调查发生了什么，我们可能需要采取一些纠正措施。也许最新版本引入了内存泄漏？或者，负载的增加超出了我们的预期和测试，这导致了内存使用的增加。使用接近极限的内存的原因不是现在的焦点。发现我们已经达到极限。
首先，我们将回到普罗米修斯的图形屏幕。
```
 1  open "http://$PROM_ADDR/graph"
```
我们已经知道，我们可以通过`container_memory_usage_bytes`度量获得实际的内存使用情况。既然我们已经探索了如何获得请求的内存，我们可以猜测限制是相似的。他们确实是，他们可以通过`kube_pod_container_resource_limits_memory_bytes`取回。由于其中一个指标与之前相同，另一个非常相似，我们将直接执行完整的查询。
请输入下面的表达式，按下执行按钮，切换到*图形*选项卡。
```
 1  sum(label_join(
 2    container_memory_usage_bytes{
 3      namespace!="kube-system"
 4    }, 
 5    "pod", 
 6    ",", 
 7    "pod_name"
 8  ))
 9  by (pod) /
10  sum(
11    kube_pod_container_resource_limits_memory_bytes{
12      namespace!="kube-system"
13    }
14  )
15  by (pod)
```
在我的例子中(下面的截图)，我们可以看到相当多的 Pods 使用了超过其极限的内存。
幸运的是，我的集群中确实有多余的容量，Kubernetes 没有立即杀死任何一个 Pods 的需要。此外，问题可能不在于 Pods 中使用了超过设定的限制，而是这些 Pods 中并非所有容器都有设定的限制。无论是哪种情况，我都应该更新这些 Pods/容器的定义，并确保它们的限制高于几天甚至几周的平均使用量。
![](img/c57b7c9d-4904-4167-b320-ac2a31877b8a.png)
Figure 3-46: Prometheus' graph screen with the percentage of container memory usage based on memory limits and with those from the kube-system Namespace excluded
接下来，我们将进行探索新旧价值观差异的演练。
```
 1  diff mon/prom-values-req-cpu.yml \
 2      mon/prom-values-limit-mem.yml
```
输出如下。
```
175c175
   for: 1h
184c184
   for: 6h
190a191,199
> - alert: MemoryAtTheLimit
>   expr: sum(label_join(container_memory_usage_bytes{namespace!="kube-system"}, "pod", ",", "pod_name")) by (pod) / sum(kube_pod_container_resource_limits_memory_bytes{namespace!="kube-system"}) by (pod) > 0.8
>   for: 1h
>   labels:
>     severity: notify
>     frequency: low
>   annotations:
>     summary: Memory usage is almost at the limit
>     description: At least one Pod uses memory that is close it its limit
```
除了恢复我们之前使用的警报的合理阈值之外，我们还定义了一个名为`MemoryAtTheLimit`的新警报。如果实际使用量超过限值的百分之八十(`0.8`)超过一小时(`1h`)，则会触发。
接下来是我们普罗米修斯图表的升级。
```
 1  helm upgrade -i prometheus \
 2    stable/prometheus \
 3    --namespace metrics \
 4    --version 7.1.3 \
 5    --set server.ingress.hosts={$PROM_ADDR} \
 6    --set alertmanager.ingress.hosts={$AM_ADDR} \
 7    -f mon/prom-values-limit-mem.yml
```
最后，我们可以打开普罗米修斯的警报屏幕，并确认新的警报确实被添加到混合中。
```
 1  open "http://$PROM_ADDR/alerts"
```
我们不会为 CPU 创建类似的警报。你自己应该知道怎么做。
# 现在怎么办？
我们探索了相当多的普罗米修斯度量、表达式和警报。我们看到了如何将普罗米修斯警报与警报管理器连接起来，并从那里将它们从一个应用转发到另一个应用。
到目前为止，我们所做的只是冰山一角。如果花太多时间(和空间)来探索我们可能使用的所有度量和表达式。尽管如此，我相信现在你知道了一些更有用的方法，你将能够用那些特定于你的方法来扩展它们。
我敦促你给我发送你认为有用的表达和提醒。你知道哪里可以找到我(*devo ps20*([http://slack.devops20toolkit.com/](http://slack.devops20toolkit.com/))Slack，`viktor@farcic` email，`@vfarcic`在推特上，等等)。
现在，我将让您决定是直接进入下一章，销毁整个集群，还是仅删除我们安装的资源。如果您选择后者，请使用以下命令。
```
 1  helm delete prometheus --purge
 2
 3  helm delete go-demo-5 --purge
 4
 5  kubectl delete ns go-demo-5 metrics
```
在你离开之前，你可能要复习一下本章的要点。
*   普罗米修斯是一个数据库(种类)，设计用于获取(提取)和存储高维时间序列数据。
*   每个人都应该利用的四个关键指标是延迟、流量、错误和饱和度。