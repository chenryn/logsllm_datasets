vulnerable scenarios described in §3.2.1.
Expired Domains. The detection of expired domains is per-
formed according to the following procedure: given a resolv-
ing chain that begins with a CNAME record, our tool checks if
it points to an unresolvable resource and extracts the eTLD+1
of the canonical name at the end of the chain, that we call
apex for brevity. Then, if the whois command on the apex
domain does not return any match, RDScan queries GoDaddy
to detect if the domain can be purchased. In this case, we
consider the domain of the resolving chain, i.e., the alias of
the ﬁrst record of the chain, as vulnerable. Notice that we
only tested domains that can be registered without special
requirements, i.e., we did not consider .edu domains and other
speciﬁc eTLDs not offered by the registrar.
Discontinued Services. The process of ﬁnding discontinued
services is summarized in Algorithm 1. RDScan traverses
each resolving chain to identify whether it points to one of
the services supported by our framework. This step is im-
plemented according to the documentation provided by in-
dividual services, and typically relies on checking for the
presence of (i) an A record resolving to a speciﬁc IP ad-
dress, (ii) the canonical name of a CNAME record matching
a given host, or (iii) the existence of a NS record pointing
to the DNS server of a service. (Sub)domains mapped to
services are then checked to verify if the bindings between
user accounts and (sub)domains are in place. For the major-
ity of the services considered in this study, a simple HTTP
request sufﬁces to expose the lack of a correct association
of a (sub)domain. Other services require active probing to
determine whether a domain can be associated to a fresh test
account that we created. This has been done using the auto-
mated browser testing library puppeteer with Chromium [1].
RDScan also performs the detection of DNS wildcards that
might be abused as described in §3.2. A DNS wildcard
for a domain such as test.example.com can be easily de-
tected by attempting to resolve a CNAME or A DNS record for
.test.example.com, where nonce refers to a ran-
dom string that is unlikely to match an entry in the DNS zone
of the target domain.
Deprovisioned Cloud Instances. The detection of poten-
tially deprovisioned cloud instances has been performed sim-
ilarly to the probabilistic approach adopted by [8, 33]. We
did not create any virtual machine or registered any service
at cloud providers in this process. Instead, we collected the
set of IP ranges of 6 major providers: Amazon AWS, Google
Cloud Platform, Microsoft Azure, Hetzner Cloud, Linode,
Vs ← /0
for each chain ∈ RC do
for each service ∈ S do
Algorithm 1 Detection of Discontinued Services
Input: Set of DNS resolving chains RC, set of supported services S
Output: Set of vulnerable subdomains Vs
1: procedure DISCONTINUED_SERVICES(RC,S)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
Vs ← Vs ∪{d}
(cid:46) Detect wildcard if the service allows a subdomain of a
(cid:46) claimed domain to be mapped to a different account
else if service vulnerable to wildcard issue then
r ← generate_nonce()
rd_chains ← compute_resolving_chains(r.d)
for each rd_chain ∈ rd_chains do
if rd_chain points to service then
(cid:46) Check if a record in the chain points to the service
if chain points to service then
d ← target_domain(chain)
if d is unclaimed at service then
Vs ← Vs ∪{r.d}
and OVHcloud. We tested each (sub)domain in our dataset
to check whether the pointed IP was included in any of the
cloud IP ranges. In case the IP falls within the address range
of a cloud provider, we make sure that it does not point to a
reserved resource such as a proxy or a load balancer. As the
last step, we perform a liveness probe to determine if the IP
is in use. This is done by executing a ping to the IP: if no
answer is received, we use a publicly available dataset [43]
comprising a scan of the full IPv4 range on 148 ports (128
TCP, 20 UDP). If no open ports for the given IP are found,
we deem the resource as potentially deprovisioned.
4.3 Web Analyzer
Our web security analysis aims at quantifying the number of
domains hosting web applications that can be exploited by
taking over the vulnerable domains discovered by RDScan. In
particular, for every apex domain with at least one vulnerable
subdomain, we selected from the CommonCrawl dataset [19]
the list of 200 most popular related domains according to the
Pagerank score [10]. From the homepage of these domains,
we extracted the same-origin links that appear in the HTML
code. For each related domain, we considered the homepage
and up to 5 of these URLs as the target of our web analysis,
and we accessed these links using the Chromium browser
automated by puppeteer. In the following, we present the data
collection process and the security analyses we have con-
ducted to identify the threats discussed in §3.3. We postpone
the summary of the results to §5.
4.3.1 Analysis of Cookies
We used the puppeteer API to collect cookies set via HTTP
headers and JavaScript. Our goal is to identify cookies af-
fected by conﬁdentiality or integrity issues. In particular, we
ﬂag a cookie as affected by conﬁdentiality issues if, among the
2924    30th USENIX Security Symposium
USENIX Association
related domains vulnerable to takeover, there exists a domain
d such that:
• d is a subdomain of the Domain attribute of the cookie;
• by taking over d, the attacker has acquired the capabili-
ties required to leak the cookie.
We mark a cookie as affected by integrity issues if:
• the name of the cookie does not start with __Host-;
• we identiﬁed a vulnerable domain that grants the capa-
bilities required to set the cookie.
We also rely on a heuristic proposed by Bugliesi et al. [12] to
statically identify potential (pre-)session cookies, i.e., cookies
that may be relevant for the management of user sessions.
The capabilities required to perform these attacks depend
on the security ﬂags assigned to the cookie and the usage
of cookie preﬁxes (see §3.3.2). For instance, to compromise
integrity either the capability js or headers is required and,
if the preﬁx __Secure- is used, https is also necessary.
4.3.2 Analysis of CSP policies
For this analysis, we implemented a CSP evaluator according
to the draft of the latest CSP version [55], which is currently
supported by all major browsers. This is not a straightforward
task, due to the rich expressiveness of the policy and various
aspects that have been introduced into the speciﬁcation for
compatibility purposes across different CSP versions, e.g.,
for scripts and styles, the ’unsafe-inline’ keyword, which
whitelists arbitrary inline contents in a page, is discarded
when hashes or nonces are also speciﬁed.
In our analysis, we focus on the protection offered against
click-jacking and the inclusion of active contents [37], i.e.,
resources that have access to (part of) the DOM of the embed-
ding page. This class of contents includes scripts, stylesheets,
objects, and frames.
For each threat considered in our analysis, we ﬁrst check
if the policy is unsafe with respect to any web attacker. This
is the case for policies that allow the inclusion of contents
from any host (or framing by any host, when focusing on
click-jacking protection). For scripts and styles, the policy is
also deemed unsafe if arbitrary inline contents are whitelisted.
If the policy is considered safe, we classify it as exploitable by
a related domain if one of the vulnerable domains detected by
RDScan is whitelisted and the attacker acquires the relevant
capabilities to perform the attack, which vary depending on
the threat under analysis (see §3.3.3). For instance, script
injection requires the file capability, given that attackers
need to host the malicious script on a subdomain they control.
Moreover, if the page to attack is served over HTTPS, the
https capability is required due to the restrictions imposed
by browsers on mixed content [37].
4.3.3 Analysis of CORS
To evaluate the security of the CORS policy implemented
by a website, we perform multiple requests with different
Origin values and inspect the HTTP headers in the response
to understand whether CORS has been enabled by the server.
Inspired by the classes of CORS misconﬁgurations iden-
tiﬁed in [18], we test 3 different random origins with the
following characteristics: (i) the domain is a related domain
of the target URL; (ii) the domain starts with the registra-
ble domain of the target URL; (iii) the domain ends with
the registrable domain of the target URL. While the ﬁrst
test veriﬁes whether CORS is enabled for a related domain,
the other two detect common server-side validation mistakes.
Such errors include the search of the registrable domain as
a substring or a sufﬁx of the Origin header value, which re-
sults in having, e.g., www.example.com whitelisting not only
a.example.com but also atkexample.com. For each test,
we check if the Access-Control-Allow-Origin header is
present in the response and if its value is either * or that of the
Origin header contained in the request. We also control if the
Access-Control-Allow-Credentials header is present
and set to true (when Access-Control-Allow-Origin dif-
fers from *) to identify the cases in which requests with
credentials are allowed.
We report a CORS deployment as vulnerable to web attack-
ers if either the second or the third test succeeds. Instead, a
page is exploitable exclusively by a related-domain attacker if
only the ﬁrst test succeeds and, among the vulnerable related
domains discovered by RDScan, one grants the js capability
to the attacker. Since in our tests we use the same protocol of
the page under analysis in the Origin header, we conserva-
tively require the https capability when HTTPS is used.
4.3.4 Analysis of postMessage Handlers
PMForce [51] is an automated in-browser framework for the
analysis of postMessage event handlers. It combines selective
force execution and taint tracking to extract the constraints
on the message contents (e.g., presence of a certain string
in the message) that lead to execution traces in which the
message enters a dangerous sink that allows for code execu-
tion (e.g., eval) or the alteration of the browser state (e.g.,
document.cookie). A message satisfying the extracted con-
straints is generated using the Z3 solver and the handler under
analysis is invoked with the message as a parameter to ensure
that the exploit is successfully executed.
We integrated PMForce in our pipeline and modiﬁed it to
generate, for each handler, multiple exploit messages with the
same contents but a different origin property, e.g., a related-
domain origin and a randomly-generated cross-site origin.
We consider a page vulnerable to any web attacker if any of
its handlers is exploitable from a cross-site position. Instead,
we consider a page exploitable by a related-domain attacker
if its handlers can be exploited only from a related-domain
USENIX Association
30th USENIX Security Symposium    2925
position and one of the vulnerable domains discovered by
RDScan grants the js capability to the attacker, which is
required to open a tab and send messages to it. If the handlers
whitelist only HTTPS origins, then the capability https is
also required to mount the attack.
4.3.5 Analysis of Domain Relaxation
As a ﬁrst step, the analyzer detects whether the property
document.domain is set after the page is loaded. This task is
straightforward except for the case in which the page sets the
property to its original value (see §3.3.6) since this cannot be
detected just by reading the value of document.domain. To
identify this particular case, we leverage puppeteer APIs to:
• inject a frame from a (randomly generated) subdomain
of the page under analysis;
• intercept the outgoing network request and provide as
response a page with a script that performs domain relax-
ation and tries to access the parent frame, which succeeds
only if the parent has set document.domain.
The relaxation mechanism is exploitable by a related-domain
attacker if RDScan discovered a vulnerable subdomain (which
is a subdomain of the value of document.domain) that grants
the js capability to the attacker. If the webpage is hosted over
HTTPS, the https capability is also required.
4.4 Heuristics and False Positives
Our methodology is based on testing sufﬁcient preconditions
to execute the reported attacks, thus minimizing false posi-
tives. Nevertheless, the scanning pipeline makes use of two
heuristics in the RDScan and web analyzer modules to, re-
spectively, detect potentially deprovisioned cloud instances
and label security-sensitive cookies; moreover, we identify a
potential TOCTOU issue between the two modules of the anal-
ysis pipeline. We discuss below why this has only a marginal
effect on the overall results of the analysis.
RDScan. We developed automated procedures to test sufﬁ-
cient preconditions for a takeover. Expired domains are triv-
ially veriﬁed by checking if the target domain can be pur-
chased. For discontinued services, we created personal testing
accounts on each service considered in the analysis and used
these accounts to probe the mapping between the target sub-
domain and the service. If we detect all necessary conditions
to associate the subdomain to our account, we deem it as vul-
nerable. We manually vetted these conditions against our own
domain. Due to ethical concerns, we did not mount attacks
against real websites, but we reviewed all the occurrences of
subdomain takeover vulnerabilities before disclosing them to
the affected sites and found no false positives in the results
(see Appendix A). The detection of subdomains pointing to
deprovisioned cloud instances relies instead on a heuristic
which might introduce false positives, as discussed in §4.2.
We performed this investigation to capture the magnitude of
the problem, but we excluded the results on deprovisioned
cloud instances from the pipeline to avoid false positives in
the web analyzer. To avoid misunderstandings in the paper,
we refer to domains matching our heuristic as potentially
vulnerable.
Web Analyzer. The web vulnerabilities discovered by this
module have been identiﬁed via dynamic testing and analysis
of the data collected by the crawler. We manually veriﬁed
samples of each detected vulnerability to ensure the correct-
ness of the results and conﬁrmed the absence of false positives.
The usage of heuristics is limited to the labeling of cookies
which likely contain session identiﬁers and are thus particu-
larly interesting from a security standpoint; this approach has
been proved reasonably accurate in prior work [12].
Interplay between the modules. The modules of the
pipeline described in Figure 2 have been executed in sequence
at different points in time. The DNS enumeration phase termi-
nated in June 2020, while RDScan ran during the ﬁrst half of
July 2020. The severity of the discovered issues motivated us
to immediately report them to the affected parties. Therefore,
we launched a large-scale vulnerability disclosure campaign
in the second half of the month. We executed the web scan-
ner right after that. Having the DNS data collection running
ﬁrst, RDScan might have missed new subdomains that were
issued after the completion of the DNS enumeration. This
leads to a possible underestimation of the threats in the wild
concerning unresolvable domains and expired services. On
the other hand, subdomain takeover vulnerabilities might have
been ﬁxed prior to the web security analysis. We performed a
second run of RDScan 6 months later to verify the ﬁx rate of
notiﬁed parties. Surprisingly, we discovered that, as of January
2021, 85% of the subdomains that we tested are still affected
by leftover subdomain takeover vulnerabilities, conﬁrming
that the early remediation of the reported vulnerabilities had a
marginal effect on the web analysis. We provide more details
on our large-scale disclosure campaign in Appendix A.
5 Security Evaluation
We report on the results of our security evaluation on the top
50k domains from the Tranco list. We quantify the vulnerabil-
ities that allow an attacker to be in a related-domain position,
and we provide a characterization of the affected websites.
Then, we delve into the security of 31 service providers by
discussing common pitfalls and the capabilities that could
be abused by an attacker. Finally, we present the outcome
of our web analysis, and we identify practical vulnerabilities
by intersecting the capabilities on vulnerable domains with
the threats found on web applications hosted on their related
domains. Table 3 provides a breakdown of the results by com-
bining attack vectors and web threats: the values reported in
the cells represent the number of vulnerable domains/sites
2926    30th USENIX Security Symposium
USENIX Association
compared to those deploying the corresponding web mecha-
nism. We discuss these results in the following. Due to space
constraints, we move representative examples of conﬁrmed
attacks to Appendix B.
5.1 Attack Vectors and Capabilities
RDScan identiﬁed 1,520 subdomains exposed to a takeover
vulnerability, distributed among 887 domains from the top
50k of the Tranco list. Most of the vulnerabilities are caused
by discontinued third-party services (83%), with expired do-
mains being responsible for the remaining 17%. The analysis
of deprovisioned cloud instances discovered 13,532 poten-
tially vulnerable domains, conﬁrming the prevalence of this
threat as reported in previous work [33].
5.1.1 Characterization of Vulnerable Domains
As expected, the likelihood of a domain to be vulnerable is
directly related to the breadth of its attack surface, i.e., the
number of subdomains we found. Figure 3a pictures well
this correlation, showing that around 15% of the domains
with more than 50,000 subdomains are vulnerable. Figure 3b
outlines the distribution of vulnerable domains depending on
the rank of each site in the Tranco list. Sites in top positions
are more likely to have a vulnerable subdomain than those
with a lower rank.
The analyzed websites have been further partitioned into
categories in Figure 3c. Special care has to be taken when con-
sidering dynamic DNS: the 49 domains listed in this category
are those used by dynamic DNS services, such as ddns.net,
noip.com, afraid.org. RDScan identiﬁed vulnerable sub-
domains belonging to 8 domains, but 4 of them were listed in
the PSL. We excluded these domains from our analysis, given
that taking control of one of their subdomains would not put
the attacker in a related-domain position with respect to the
parent domain. The same principle has been adopted when
evaluating service and hosting providers offering subdomains
to their users. We refer to §5.1.2 for a detailed analysis of
Dynamic DNS services and hosting providers.
The second most affected category concerns education web-
sites. We found that academic institutions generally have com-