the client to compute the number of bit-positions of L
that will later (Step 6) end up in the Bkt structure.
Eﬀectively, the client builds a tally in local storage cal-
culating the future size of each of the √m buckets that
will be built on the server in the step 6 bucket sort.
We use √m buckets of size √m so that each bucket
will ﬁt in private storage in step 7, and the tally built
here, with one counter per bucket, also ﬁts in private
storage. The step requires O(m) time and O(√m) pri-
vate storage. (To avoid redundant scans, this step can
be merged with the previous).
4. Add fake bits to make the bucket sizes equiv-
alent. The local tally from step 2 indicates the size
of the largest bucket. We scan the tally, adding fake
encrypted positions to the server-side list of encrypted
positions as we go, so that all the buckets will have
the same size as the largest bucket after the step 5
bucket sort. To add a fake position that will corre-
spond to bucket j, the position j√m is added to the
list. (A simple balls and bins result predicts that the
√m-sized buckets all have similar sizes, already; the
number of fakes to add is small compared to the num-
ber of real items). Let max be the index of T such
j=1T [j]. For each j = 1..√m gen-
√m
that T [max] = max
erate T [max]−T [j] fake values vl such that the log m/2
most signiﬁcant bits of each vl are equal to j. Store the
encrypted, E(vl||“f ake′′) value in L. This operation
ensures that all the buckets of Bkt will store the same
number of elements. The bucket size tally is discarded
after this step. This step requires O(m) time.
5. Obliviously scramble the list of Bloom ﬁlter po-
sitions. The encrypted indexes (the bit-positions of
L, including the fakes) are scrambled, according to our
Oblivious Merge Scramble Algorithm, which destroys
all correlation between the old positions and the re-
sulting positions, which are a new uniform random
permutation. The new list L stores the scrambled val-
ues. The algorithm requires O(m log log m) time and
O(√m) private storage.
6. Bucket-sort the list of Bloom ﬁlter positions.
For each E(p) ∈ L, let idx(p) be the log m/2 most
signiﬁcant bits of p. Then, do Bkt[idx(p)].add(E(p)).
Here the Bloom ﬁlter’s scrambled, encrypted positions
are bucket-sorted. The client retrieves each bit index,
decrypts it to read it, and writes the encrypted value
back to the bucket on the server corresponding to the
log m
2 most signiﬁcant bits of the position. The bucket
sort allows us to construct the encrypted Bloom ﬁlter
in the next step without revealing to the server which
bits are set:
if we were to simply scan the entire list
of positions setting the corresponding bits to true, the
server would observe the bit ﬂips in our encrypted ar-
ray and learn what positions are set. The bucket sort
groups related positions together so that we can build
the Bloom ﬁlter from left to right in a single pass. This
step requires O(m) time.
7. Construct Bloom ﬁlter. For each j = 1..Bkt.size,
download Bkt[j]. Note that the size of Bkt[j] is √m.
Let BF [j√m]..BF [(j + 1)√m] be the segment of the
Bloom ﬁlter corresponding to Bkt[j], where BF [idx]
denotes the idxth bit of BF . For each E(p) ∈ Bkt[j],
let x be the least signiﬁcant log m/2 bits of p. Do
BF [j√m + x] = 1. Store E(BF [j√m]..E(BF [(j +
1)√m] on the server. Finally, store the oblivious Bloom
ﬁlter of the working set W on the server.
Here the client downloads each bucket (which conve-
niently ﬁts into local storage). The bucket corresponds
to a √m-sized segment of the ﬁnal Bloom ﬁlter – all
positions in this bucket refer to a bit in this segment
of the ﬁlter. The bits corresponding to listed positions
are set to true, with all other bits set to false, in the lo-
cal copy. The client encrypts this Bloom ﬁlter segment
and uploads it to the server. Observe that the server
has no indication of how many bits are true in this
segment (other than that it is limited by the bucket
size), nor which are true. The Bloom ﬁlter is ﬁnished
at the end of this step. This step requires O(m) time,
and O(√m) private storage.
8. Scramble the items. Finally, the client uses the
Oblivious Merge Scramble Algorithm to scramble the
actual items in the working buﬀer W . The Oblivi-
ous Merge Scramble requires O(m log log m) time and
O(√m) private storage.
9. Add items back to level i. Once scrambled, the
items inserted under their new labels, according to the
new labeling function for level i. For each item in x ∈
W let label(x) = hash(”LiData||Gen(Li)||k). Insert
the pair (x, label(x)) into the set of items stored at level
i. Add m fake items to level i, so that a query that
turns out not to be for this level will have an item to
retrieve instead (most of these m fakes will be deleted
by the query process before the next reshuﬄe).
Level i − 1 is now empty, and level i now contains all the
items that were in level i − 1. If level i is now full, this is
then repeated as level i is then dumped into level i + 1 etc.
This procedure shows how level i − 1 can be dumped into
level i at a cost of O(m log log m) = O(4i log i). Level i −
1 is emptied once every 4i−1 queries, thus resulting in an
amortized cost per query due to reshuﬄing of
the only other piece that could be tied to it, since the bucket
sort write pattern is the only access pattern that varies dur-
ing the reshuﬄe. The bucket writes are all identical except
for the order of the writes, which is uniform random because
of the scramble. The scramble has no bearing on the Bloom
ﬁlter access pattern, which is dependent only on the query
and the current Bloom hash function. Therefore the Bloom
ﬁlter construction process yields no information about the
items to the server in the resulting Bloom ﬁlter.
The level reorder process results in a new level that has
no correlation to the old level, since the new permutation
is chosen uniformly randomly (Theorem 2). The scramble
process itself reveals no information about the new or old
permutations, since the scramble has the same access pat-
tern in all instantiations.
4.5 Oblivious Scramble Algorithm
To complete step 5 above, we now describe an algorithm
that performs an oblivious scramble on a array of size n, with
c√n local storage, in O(n log log n) time with high probabil-
ity. This is based on an algorithm by Williams et al. in [19],
which scrambles an array obliviously in time O(n log n) by
ways of a merge sort. Since our application only requires
a scramble, and not a complete sort, we can improve the
asymptotic complexity by merging multiple arrays at once.
Informally, the algorithm is still a merge sort, except a
random number generator is used in place of a comparison,
and multiple arrays are merged simultaneously. The array is
recursively divided into segments, which are then scrambled
together in groups. The time complexity of the algorithm is
better than merge sort since multiple segments are merged
together simultaneously. Randomly selecting from the re-
maining arrays avoids comparisons among the leading items
in each array, so it is not a comparison sort.
The Oblivious Scramble Algorithm proceeds recursively as
follows, starting with the remote array split into segments of
size s = 1, a security parameter c, and an array to scramble
of size n.
1. For segments sized s, allocate ⌈pn/s⌉ buﬀers of size
c√s, (requiring c√n space total)
2. Split the array into groups of pn/s segments.
3. For each of the n/s√n/s
= pn/s groups:
• Obliviously merge the segments in this group to-
gether into one new segment of size (s)pn/s =
√ns, by performing the Oblivious Merge Step on
the allocated buﬀers. The Oblivious Merge Step
requires c√s local working memory for each of the
pn/s buﬀers, for a total of c√n working memory,
and operates in O(√ns) time.
4. In the end there are pn/s segments of size √ns.
log4n
X
i=0
O(
4i log i
4i−1 ) =
log4n
X
i=0
O(log i) = O(log n log log n)
5. Repeat.
The Bloom ﬁlter bits retrieved to check an item will ap-
pear to be chosen uniformly random, and completely inde-
pendently of each other; therefore, any bit pattern indicates
nothing about the query or the success of the query. They
are independent of the bucket sort write pattern, which is
One recursion of this algorithm requires a single pass across
the level, costing O(4i) for level i. Each pass brings the total
number of segments from n/s to pn/s, and we repeat until
there is one segment left. After iteration p, the number of
segments remaining will be n1/2p
. There will be 2 segments
left when p = log log n. Since it takes log log n passes to go
from n to 2, and each pass involves a single read and write
of the entire array, the total running time / communication
complexity for running the oblivious scramble on level i is
O(4i(log log 4i + 1)) = O(4i log i) = O(n log log n).
We now describe the last remaining piece of the Oblivious
Merge Scramble Algorithm, the Merge Step.
4.6 Oblivious Merge Step
The Oblivious Merge Step, whose pseudo-code is shown
in Algorithm 2, takes r arrays of size n/r, and merges them
randomly into a single array of size n, preserving the order-
ing among the input arrays in the output arrays: if an item
a is before item b in original array i, it will also be before b
in the ﬁnal array.
The permutation is chosen uniformly randomly out of all
permutations that preserve the ordering of the original input
items. To ensure this, we will take n steps, choosing an item
from the front of one of the r arrays at every step. The choice
is biased since we choose each item without replacement
if a particular array
randomly from the remaining items:
a
n−j chance of being
has a items left at step j, it has a
chosen at this step.
The key to obliviousness is that we accomplish this ran-
dom selection without aﬀecting the actual access pattern of
reading from the server. In [19] this is implemented for 2
arrays; we now extend this to merge r arrays. By simply
reading the input evenly at a ﬁxed rate, and outputting the
items indicated by the random function, the uniform nature
of the random function will cause the output rates to be very
similar with high probability.
In other words, we maintain a series of caching queues that
are fed at a certain rate. According to a random function,
we remove items from the queues. By the nature of the
random selection, with high probability the queues will never
overﬂow from being dequeued too slowly, nor empty out
from being dequeued too quickly, as shown in Theorem 1.
Due to space requirements, the proofs are omitted from this
version of the paper.
Theorem 1. The Oblivious Merge Scramble succeeds, with
high probability: the chance that the queue buﬀers overﬂow
or underrun is negligible w.r.t. the security parameter c.
Theorem 2. The Oblivious Merge Scramble produces a
permutation selected uniformly randomly from the set of all
permutations.
Theorem 3. The server learns nothing about the access
pattern from a client running this protocol.
4.7 Bloom Filter Parameters
We discuss here suitable choices of Bloom ﬁlter parame-
ters. A Bloom ﬁlter containing z items has two parameters:
y, the number of hash functions used (the number of bits set
per item in the ﬁlter), and x, the number of bits in the Bloom
x´yz´y
ﬁlter, yielding the false positive rate r = `1 − `1 − 1
.
Our Bloom ﬁlters are constrained by two important con-
siderations. First, we need to minimize y, since this cor-
responds to the number of disk seeks required per lookup.
Second, we need to guarantee that with high probability,
there will be no false positives; i.e., r must be negligible to
prevent a privacy leak.
for (i := 1; i ≤ r; i + +) do
qi := new queue[s];
for (x := 1; x ≤ s/2; x + +) do
qi.enqueue(decrypt(Ai .readNextItem()));
Algorithm 2 Oblivious Merge Step
1.oblivious merge step(A1[], ...Ar[])
2. B : array[n]; #new remote destination buffer of size n
3. s := 2cpn/r; #size of local queues
4.
5.
6.
7.
8. #at this point each queue has s/2 items
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.end.
ﬁ
# now we have read r items; time to output r items
for (i := 1; i ≤ r; i + +) do
v := randomlyChooseWhichArray();
t := qv.dequeue();
B.writeNextItem(encryptWithNewNonce(t));
for (x := s/2; x ≤ n + s/2; x + +) do
for (i := 1; i ≤ r; i + +) do
if (x ≤ n) then
qi.enqueue(decrypt(Ai .readNextItem()));
Therefore, for any ﬁxed acceptable error rate r, e.g., 2−64,
and for member count z = m (level size), the trade-oﬀ be-
tween the Bloom ﬁlter size x and the bits set per item y
must be optimized to balance online disk seeks for query
answering, server storage used, and Bloom ﬁlter construc-
tion time. For disk-based storage of large databases, we ﬁnd
nearly optimal parameters by ﬁxing y ≈ 5, yielding a large,
sparsely populated Bloom ﬁlter.
5. CORRECTNESS AND INTEGRITY
In this section we introduce a set of integrity constructs
that endow the above solution with correctness assurances.
Speciﬁcally, we would like to guarantee that any storage
provider tampering behavior is detected. All of these con-
structs can be implemented eﬃciently, with few or almost
no overheads: (i) Message Authentication Codes (MACs)
are added for all the stored items and Bloom ﬁlters; (ii)
unique version labels for each item in the data covered by the
MAC are added to prevent any replay-type attack in which
the server incorrectly replies with a previously MAC-signed
message; (iii) incremental, collision-resistant commutative
checksums are added to checksum the item sets contained
in each level, to prevent the server from hiding or duplicating
items during the level reshuﬄe process.
For (i) we require a MAC function, such that the compu-
tationally bounded adversary has no non-negligible ability
to construct any message, MAC pair (M, M AC(M )) for an
M that did not originate at the client. Every item uploaded
to the server is protected by such a MAC. (ii) is straightfor-
ward. For (iii), besides the requirement of being collision-
resistant, it should be easy for clients to maintain and update
a checksum of a set. In particular, when adding or deleting
items, it should be possible to to so incrementally, without
recomputing the entire checksum value.
The incremental hashing paradigm of Bellare and Miccian-
cio [5] can be used to construct exactly such a checksum. Fix
any cryptographic hash function h (viewed as random ora-
cle) and a large prime p. To hash a set B = {b1, . . . , bl}, we
compute the product
H(B) :=
l
Y
i=1
h(bi) mod p.
(1)
Note that this hash construction allows both for easy addi-
tion of item b (multiplication with h(b)) and removal of any
bi (multiplication by (h(di))−1) without needing to recom-
pute the hashes of all values b1, . . . , bl.
It can be shown (in the random oracle model [13], proof