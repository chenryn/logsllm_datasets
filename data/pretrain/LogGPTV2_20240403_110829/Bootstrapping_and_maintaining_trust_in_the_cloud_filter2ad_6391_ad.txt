signed. The tenant can now provision a new node with this
identity using trusted-cloud-init. The software CA also
supports receiving notiﬁcations from the CV if a node later
fails integrity measurement.
To support transparent integration with an IaaS plat-
form, we patched OpenStack Nova and libvirt to support
the creation of companion vTPM Xen domains for each
user created instance. We link the OpenStack UUID to the
keylime provider registrar. We then implemented a wrap-
per for the OpenStack Nova command line interface that
enables trusted-cloud-init. Speciﬁcally, our wrapper in-
tercepts calls to nova boot and automatically encrypts the
provided user-data before passing it to OpenStack. It then
calls the keylime tenant, which begins the bootstrapping
protocol. This allows OpenStack users to transparently use
keylime and cloud-init without needing to fully trust the
OpenStack provider not to tamper or steal the sensitive con-
tents of their user-data.
4.2 Demonstration Applications
We next describe how keylime can securely bootstrap
and handle revocation for existing non-trusted computing-
aware applications and services common to IaaS cloud de-
72
ployments.
IPsec To enable secure network connectivity similar to
TNC [39], we implemented trusted-cloud-init scripts to
automatically encrypt all network traﬃc between a tenant’s
IaaS resources. The scripts use the OpenStack API for IP
address information and then build conﬁgurations for the
Linux IPsec stack and raccoon15. This conﬁguration is also
easily extensible to a TLS-based VPN like OpenVPN16.
Puppet To enable secure system conﬁguration manage-
ment, we integrated keylime with Puppet We do so by gen-
erating the signed RSA keys that Puppet uses to communi-
cate with the Puppet master using the Software CA process
described previously. These steps bypass the need to either
use the insecure autosign option in the Puppet master to
blindly accept new nodes or to have an operator manually
approve/deny certiﬁcate signing requests from new nodes.
To support continuous attestation and integrity measure-
ment, we implemented a plug-in for the CV that notiﬁes the
tenant’s Puppet master when a node fails its integrity mea-
surements. The master can then revoke that node’s access
to check-in and receive the latest conﬁguration data.
Vault While tools like Puppet are often used to provi-
sion secrets and keys, tenant operators can instead use a
dedicated secret management system that supports the full
lifecycle of cryptographic keys directly. To demonstrate this,
we have integrated keylime with Vault, a cloud-compatible
secret manager. Like Puppet, we use the Software CA to
provision RSA certiﬁcates for each node and conﬁgure Vault
to use them. We also implemented a revocation plugin for
the CV that notiﬁes Vault to both revoke access to a node
that fails integrity measurement and to re-generate and re-
distribute any keys to which that node had access.
LUKS Finally, to demonstrate our ability to provision
secrets instead of cryptographic identities, we implemented a
trusted-clout-init script that provides the key to unlock
an encrypted volume on boot.
5. EVALUATION
In this section we evaluate the overhead and scalability of
keylime in a variety of scenarios. We ran our experiments
on a private OpenStack cluster, a Xen host, and a Raspberry
Pi. In OpenStack, we used standard instance ﬂavors where
the m1.small has 1 vCPU, 2GB RAM, and a 20GB disk,
and the m1.large has 4 vCPUs, 8GB RAM, an 80GB disk.
We used Ubuntu Linux 14.10 as the guest OS in OpenStack
instances. The Xen host had one Xeon E5-2640 CPU with
6 cores at 2.5Ghz, 10Gbit NIC, 64 GB RAM, a WinBond
TPM, and ran Xen 4.5 on Ubuntu Linux 15.04. The Rasp-
berry Pi 2 had one ARMv7 with 4-cores at 900Mhz, 1GB
RAM, 100Mbit NIC, and ran Raspbian 7. We ran each of
the following experiments for 1-2 minutes and present aver-
ages of the performance we observed.
5.1 TPM Operations
We ﬁrst establish a baseline for the performance of TPM
operations with the IBM client library, our Python wrap-
per code, the Xen virtual TPM, and the physical TPM. We
benchmarked both TPM quote creation and veriﬁcation on
the Xen host (Table 2). We collected the physical TPM mea-
surements on the same system with a standard (non-Xen)
15http://ipsec-tools.sourceforge.net/
16https://openvpn.net/
Table 2: Average TPM Operation Latency (ms).
TPM vTPM Deep quote
Create Quote
Check Quote
725
4.64
68.5
4.64
1390
5.33
kernel. We collected both vTPM quote and deep quote mea-
surements from a domain running on Xen. As expected, op-
erations that require interaction with the physical TPM are
slow. Veriﬁcation times, even for deep quotes that include
two RSA signature veriﬁcations, are comparatively quick.
5.2 Key Derivation Protocol
We next investigate the CV latency of diﬀerent phases
of our protocol.
In Figures 6 and 7, we show the aver-
aged results of hundreds of trials of the CV with 100 vTPM
equipped VMs. Each operation includes a full REST in-
teraction along with the relevant TPM and cryptographic
operations. We also benchmarked the latency of the pro-
tocol phases emulating zero latency from the TPM (Null
TPM). This demonstrates the minimum latency of our CV
software architecture including the time required to verify
quotes. The results from the Null TPM trials indicate that
our network protocol and other processing impose very little
additional overhead, even on the relatively modestly pow-
ered Raspberry Pi. The bare metal system had a slightly
larger network RTT to the nodes it was verifying causing it
to have a higher latency than the less powerful m1.large.
In Figure 7, we see that latency for the quote retrieval
process is primarily aﬀected by slow TPM operations and is
comparable to prior work [31]. The bootstrapping latency
is the sum of the latencies for retrieving a quote and provid-
ing V. We ﬁnd the bootstrapping latency for bare metal and
virtual machines to be approximately 793ms and 1555ms re-
spectively. Virtual nodes doing runtime integrity measure-
ment after bootstrapping beneﬁt from much lower latency
for vTPM operations. Thus, for a virtual machine with a
vTPM, keylime can detect integrity violations in as little as
110ms. The detection latency for a system with a physical
TPM (781ms for our Xen host) is limited by the speed of
the physical TPM at generating quotes.
5.3 Scalability of Cloud Veriﬁer
Next we establish the maximum rate at which the CV can
get and check quotes for sustained integrity measurement.
This will deﬁne the trade-oﬀ between the number of nodes
a single CV can handle and the latency between when an
integrity violation occurs and the CV detects it. Since the
CV quote checking process is a simple round robin check of
each node, it is easy to parallelize across multiple CVs fur-
ther enhancing scalability. We emulate an arbitrarily large
population of real cloud nodes using a ﬁxed number test
cloud nodes. These test cloud nodes emulate a zero latency
TPM by returning a pre-constructed quote. This way the
test nodes appear like a larger population where the CV will
never have to block for a lengthy TPM operation to com-
plete. We found that around 500 zero latency nodes were
suﬃcient to achieve the maximum quote checking rate.
We show the average number of quotes veriﬁed per second
for each of our CV deployment options in Figure 8. Because
of our process-based worker pool architecture, the primary
factor aﬀecting CV scalability is the number of cores and
73
)
s
m
(
y
c
n
e
t
a
L
 60
 50
 40
 30
 20
 10
 0
Bare Metal
m1.large
m1.small
Raspberry Pi
Provide V
Null TPM
)
s
m
(
y
c
n
e
t
a
L
 1600
 1400
 1200
 1000
 800
 600
 400
 200
 0
Bare Metal
m1.large
m1.small
Raspberry Pi
vTPM Quote TPM Quote DeepQuote
)
s
/
s
e
t
o
u
q
(
e
t
a
R
k
c
e
h
C
y
t
i
r
g
e
t
n
I
 2500
 2000
 1500
 1000
 500
 0
B a r e   M e t a l
l a r g e
m 1 .
m 1 . s m a l
l
R a s p b e r
r y   P i
Figure 6: Latency of keylime boot-
strapping protocol.
Figure 7: Latency of TPM opera-
tions in bootstrapping protocol.
Figure 8: Maximum CV quote
checking rate of keylime.
 2500
 2000
 1500
 1000
 500
t
)
s
/
s
e
o
u
q
(
e
t
a
R
 0
 0
 50
 100
 150
 200
 250
 300
Num CV Processes
Figure 9: Scaling the CV on bare metal
RAM available. These options provide a variety of choices
for deploying the CV. For small cloud tenants a low cost
VM or inexpensive hardware appliance can easily verify hun-
dreds of virtual machines with moderate detection latency
(5-10s). For larger customers, a well-resourced VM or dedi-
cated hardware can scale to thousands with similar latency.
For high security environments, all options can provide sub-
second detection and response time. In future work, we plan
to implement a priority scheme that allows the tenant to set
the rate of veriﬁcations for diﬀerent classes of nodes.
We next show how our CV architecture can scale by adding
more cores and parallelism. We use the bare metal CV and
show the average rate of quotes retrieved and checked per
second for 500 test nodes in Figure 9. We see linear speed-
up until we exhaust the parallelism of the host CPU and
the concurrent delay of waiting for many cloud nodes. This
performance represents a modest performance improvement
over Schiﬀman et al.’s CV which was able to process approx-
imately 2,000 quotes per second on unspeciﬁed hardware [34]
and a substantial improvement over the Excalibur monitor’s
ability to check approximately 633 quotes per second [31].
5.4 On-Premises Cloud Veriﬁer
Finally, we investigate the performance of the CV when
hosted at the tenant site away from the cloud. We show
the results of our bare metal system’s quote veriﬁcation
rate and the bandwidth used for a variety of network de-
74
Table 3: On-Premises bare metal CV verifying 250
Cloud Nodes using 50 CV processes.
Network RTT
Rate
Bandwidth
(ms)
(quotes/s)
(Kbits/s)
4ms (native)
25ms
50ms
75 ms
100 ms
150 ms
937
613
398