The following theorem states the reduction in the time of space
searching with our CGS algorithm:
Theorem 4.1. Given a graph ğº with ğ‘ nodes, the reduction, de-
noted as ğ›½, in the time of space searching with coarse-grained search-
ing satisfies ğ›½ â‰ˆ ğ‘‚(2ğœ…4), where ğœ… is the number of node clusters and
we assume ğœ… â‰ª ğ‘ .
Proof. See Appendix B.
â–¡
4.4 Generating Adversarial Graphs via SignSGD
Now we present our sign stochastic gradient descent (signSGD)
algorithm to solve the attack optimization problem in Eq (7). Be-
fore presenting signSGD, we first describe the method to compute
ğ‘(Î˜), where only hard label is returned when querying the GNN
model; and then introduce a query-efficient gradient computation
algorithm to compute the gradients of ğ‘(Î˜).
superlinksuperlinksupernodeSession 1B: Attacks and Robustness CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea112With zeroth order oracle, we can estimate the sign of gradient
of ğ‘(Î˜) via computing ğ‘ ğ‘–ğ‘”ğ‘› ((ğ‘ (Î˜ + ğœ‡ğ‘¢) âˆ’ ğ‘ (Î˜))/ğœ‡ğ‘¢), where ğ‘¢ is
a normalized i.i.d direction vector sampled randomly from a Gauss-
ian distribution, and ğœ‡ is a step constant. The sign can be acquired
by computing ğ‘ (Î˜ + ğœ‡ğ‘¢) and ğ‘ (Î˜) separately. However, we need
multiple queries to obtain the value of ğ‘(Î˜). As we need to up-
date Î˜ with many iterations, it is query expensive to compute all
ğ‘ (Î˜ğ‘¡ + ğœ‡ğ‘¢) and ğ‘ (Î˜ğ‘¡) at each iteration during the signSGD. For-
tunately, we only need to know which ğ‘ is larger instead of the
exact values of them. Thus, we propose a query-efficient gradient
computation (QEGC) algorithm to compute the sign of gradient with
only one query a time as shown in Figure 4 (b).
Suppose the current direction is Î˜ğ‘œğ‘™ğ‘‘ with ğ‘” (Î˜ğ‘œğ‘™ğ‘‘) = ğ‘”ğ‘œğ‘™ğ‘‘
and ğ‘ (Î˜ğ‘œğ‘™ğ‘‘) = ğ‘ğ‘œğ‘™ğ‘‘. Now the direction steps forward with an
increment of ğœ‡ğ‘¢, i.e., Î˜ğ‘›ğ‘’ğ‘¤ = Î˜ğ‘œğ‘™ğ‘‘ + ğœ‡ğ‘¢. For simplicity of de-
scription, we assume Î˜ğ‘œğ‘™ğ‘‘ and Î˜ğ‘›ğ‘’ğ‘¤ are both normalized vectors.
We want to judge if ğ‘ğ‘›ğ‘’ğ‘¤ is larger than ğ‘ğ‘œğ‘™ğ‘‘ or not. The idea is
that we transfer ğ‘ğ‘›ğ‘’ğ‘¤ and ğ‘ğ‘œğ‘™ğ‘‘ to ğ‘”ğ‘›ğ‘’ğ‘¤ and ğ‘”ğ‘œğ‘™ğ‘‘ respectively and
compare their values. Specifically, for ğ‘ğ‘œğ‘™ğ‘‘, we find ğ‘”âˆ— such that
ğ‘âˆ— = âˆ¥ğ‘ğ‘™ğ‘–ğ‘(ğ‘”âˆ—Î˜ğ‘›ğ‘’ğ‘¤ âˆ’ 0.5)âˆ¥1 = ğ‘ğ‘œğ‘™ğ‘‘. For ğ‘ğ‘›ğ‘’ğ‘¤, the corresponding
ğ‘”ğ‘›ğ‘’ğ‘¤ is the distance from ğ´ to the classification boundary at the
direction Î˜ğ‘›ğ‘’ğ‘¤. Then we query the target model ğ‘“ with graph
ğ´âˆ— = â„ (ğ´, ğ‘”âˆ—Î˜ğ‘›ğ‘’ğ‘¤) to figure out whether ğ‘”âˆ— exceeds the boundary
or not. We say that the classification boundary in the direction of
Î˜ğ‘›ğ‘’ğ‘¤ is closer than that of Î˜ğ‘œğ‘™ğ‘‘ if ğ‘“ (ğ´âˆ—) â‰  ğ‘¦0 because we cross
the boundary with the same ğ‘ğ‘œğ‘™ğ‘‘ at the direction of Î˜ğ‘›ğ‘’ğ‘¤, while
we can only achieve the boundary (but not cross) at the direction
of Î˜ğ‘œğ‘™ğ‘‘. Thus, ğ‘ğ‘›ğ‘’ğ‘¤ is smaller than ğ‘ğ‘œğ‘™ğ‘‘ and the sign of gradient is
âˆ’1. Similarly, ğ‘ ğ‘–ğ‘”ğ‘› = +1 if ğ‘“ (ğ´âˆ—) = ğ‘¦0.
In summary, we compute the sign of a gradient as follows:
ğ‘ ğ‘–ğ‘”ğ‘› (ğ‘ (Î˜ + ğœ‡ğ‘¢) âˆ’ ğ‘ (Î˜)) =
ğ‘“ (ğ´âˆ—) = ğ‘¦0,
ğ‘“ (ğ´âˆ—) â‰  ğ‘¦0,
(8)
(cid:40)+1
âˆ’1
where ğ´âˆ— is the graph whose value of ğ‘ equals to ğ‘ (Î˜) in the
direction of Î˜ + ğœ‡ğ‘¢. We can use Eq. (8) to save the queries due to
the following theorem.
Theorem 4.2. Given a normalized direction Î˜ğ‘œğ‘™ğ‘‘ with ğ‘”ğ‘œğ‘™ğ‘‘ and
ğ‘ğ‘œğ‘™ğ‘‘ , there is one and only one ğ‘”âˆ— at the direction of Î˜ğ‘›ğ‘’ğ‘¤ that satisfies
ğ‘âˆ— = âˆ¥ğ‘ğ‘™ğ‘–ğ‘(ğ‘”âˆ—Î˜ğ‘›ğ‘’ğ‘¤ âˆ’ 0.5)âˆ¥1 = ğ‘ğ‘œğ‘™ğ‘‘ .
Proof. See Appendix C.
â–¡
Solving the converted attack problem via sign Stochastic Gra-
dient Descent (signSGD). We utilize the sign stochastic gradient
descent (signSGD) algorithm [2] to solve the converted optimiza-
tion shown in Eq. (7). The reasons are twofold: (i) the sign operation
that compresses the gradient into a binary value is suitable to the
hard label scenario; (ii) the sign of the gradient can approximate the
exact gradient, which can significantly reduce the query overhead.
Specifically, during the signSGD process, we use Eq. (8) to com-
pute the sign of gradient of ğ‘(Î˜) in the direction of ğ‘¢. To ease the
noise of gradients, we average the signs of ğ‘„ gradients in different
directions to estimate the derivative of the vector ğ‘(Î˜) as follows:
(cid:32) ğ‘(cid:0)Î˜ + ğœ‡ğ‘¢ğ‘(cid:1) âˆ’ ğ‘ (Î˜)
(cid:33)
â–½ğ‘ (Î˜) =
1
ğ‘„
ğ‘ ğ‘–ğ‘”ğ‘›
ğ‘¢ğ‘
,
(9)
ğœ‡
ğ‘„
ğ‘=1
(a) Compute ğ‘(Î˜) with binary search
(b) Compute gradient of ğ‘(Î˜) with QEGC
Figure 4: Constructing adversarial graphs. (a) Computing
ğ‘”(Î˜) and ğ‘(Î˜) by querying the target model until we find
the classification boundary, which will incur many queries
when computing gradients of ğ‘(Î˜) by using the zeroth order
oracle; (b) Query-efficient gradient computation (QEGC). To
compare ğ‘ğ‘›ğ‘’ğ‘¤ and ğ‘ğ‘œğ‘™ğ‘‘ (i.e., compute the sign of gradient of
ğ‘(Î˜)), we find ğ‘”âˆ— in the direction of Î˜ğ‘›ğ‘’ğ‘¤ such that ğ‘âˆ— = ğ‘ğ‘œğ‘™ğ‘‘,
and use the predicted label to judge if ğ‘ğ‘›ğ‘’ğ‘¤ is larger than ğ‘ğ‘œğ‘™ğ‘‘
after querying ğ´âˆ— = â„(ğ´, ğ‘”âˆ—Î˜ğ‘›ğ‘’ğ‘¤).
Computing ğ‘(Î˜) via binary search. We describe computing
ğ‘(Î˜) with only hard label black-box access to the target model.
We first compute ğ‘”(Î˜) in Eq. (4) via repeatedly querying the
target model and further obtain ğ‘(Î˜) using Eq. (6). As shown in
Figure 4 (a), each edge in the edge space of ğº can be either existent
or nonexistent so that the searching space consists of lattice points
(i.e., a lattice point is a symmetrical binary matrix ğ‘€ âˆˆ {0, 1}ğ‘Ã—ğ‘ )
that have equal distance among each other. Suppose there is a
classification boundary in the direction of Î˜. ğ‘”(Î˜) is the length of
direction vector Ë†ğ‘”(Î˜) that begins at the target graph ğ´ and ends at
the boundary. We can first find a graph ğ´1 with a different label
from ğ‘¦0 using our CGS algorithm. Since there will be a classification
boundary between ğ´ and ğ´1, we can then conduct a binary search
between them, i.e., we query the middle point of the range [ğ´, ğ´1]
(e.g., ğ‘€1 in Figure 4 (a)) and update the endpoints of the range based
on the predicted label of the middle point in each iteration. The
query process ends when the length of range decreases below a
tolerance ğœ–. With such query process, we can obtain ğ‘”(Î˜), and then
we can compute ğ‘(Î˜) easily.
Computing the gradient of ğ‘(Î˜) via query-efficient gradient
computation (QEGC). We now propose a query efficient algo-
rithm to compute the sign of gradient of ğ‘(Î˜), that aims at saving
queries used in signSGD in the next part.
M1M2query middle pointsGNNGNNM1M2query middle pointsGNN12M1M2query middle pointsGNN12M1M2query middle pointsGNN12GNN+1-1Boundaryâ‘ â‘¡â‘¢â‘£GNN+1-1Boundaryâ‘ â‘¡â‘¢â‘£Session 1B: Attacks and Robustness CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea113where â–½ğ‘ (Î˜) is the estimated gradients of ğ‘(Î˜), ğ‘¢ğ‘, ğ‘ âˆˆ 1, 2, . . . , ğ‘„
are normalized i.i.d direction vectors sampled randomly from a
Gaussian distribution, and ğ‘„ is the number of vectors. Recently,
Maho et al. [34] proposed a black-box SurFree attack that also
involves sampling the direction vector ğ‘¢ from a Gaussian distribu-
tion. However, the purpose of using ğ‘¢ is different from our method.
Specifically, ğ‘¢ in the SurFree attack is used to compute the distance
from the original sample to the boundary, while ğ‘¢ in our attack is
used to approximate the gradients.
The sign calculated by Eq. (8) depends on a single direction
vector ğ‘¢. In contrast, Eq. (9) computes the sign of the average of
multiple directions, and can better approximate the real sign of
gradient of ğ‘(Î˜). Then, we use this gradient estimation to update
the search vector Î˜ by computing Î˜ğ‘¡+1 â† Î˜ğ‘¡ âˆ’ ğœ‚ğ‘¡â–½ğ‘ (Î˜ğ‘¡), where
ğœ‚ğ‘¡ is the learning rate in the ğ‘¡-th iteration. After ğ‘‡ iterations, we
can construct an adversarial graph ğ´â€² = â„(ğ´, Î˜ğ‘‡) 2. The following
theorem shows the convergence guarantees of our signSGD for
generating adversarial graphs.
Assumption 1. At any time ğ‘¡, the gradient of the function ğ‘(Î˜)
is upper bounded by âˆ¥â–½ğ‘ (Î˜ğ‘¡) âˆ¥2 â‰¤ ğœ, where ğœ is a non-negative
constant.
ğ‘¡=0 with probability ğ‘ƒ (ğ‘… = ğ‘¡) =
Theorem 4.3. Suppose that ğ‘ (Î˜) has ğ¿-Lipschitz continuous gra-
ğœ‚ğ‘¡ğ‘‡âˆ’1
dients and Assumption 1 holds. If we randomly pick Î˜ğ‘…, whose dimen-
sionality is ğ‘‘, from {Î˜ğ‘¡}ğ‘‡âˆ’1
,
the convergence rate of our signSGD with ğœ‚ğ‘¡ = ğ‘‚(cid:16) 1âˆš
ğ‘‚(cid:16) 1âˆš
(cid:112)ğ‘„ + ğ‘‘(cid:17),
E [âˆ¥â–½ğ‘ (Î˜) âˆ¥2] = ğ‘‚(cid:16)âˆš
will give the following bound on E [âˆ¥â–½ğ‘ (Î˜) âˆ¥2]
ğ‘¡=0 ğœ‚ğ‘¡
and ğœ‡ =
(cid:17)
(cid:17)
ğ‘‘ğ‘‡
ğ‘‘ğ‘‡
âˆš
ğ‘‘âˆš
ğ‘„
+
ğ‘‘ğ¿âˆš
ğ‘‡
(10)
â–¡
Proof. See Appendix D.
5 ATTACK RESULTS
In this section, we evaluate the effectiveness our hard label black-
box attacks against GNNs for graph classification.
5.1 Experimental Setup
Datasets. We use three real-world graph datasets from three dif-
ferent fields to construct our adversarial attacks, i.e., COIL [37, 39]
in the computer vision field, IMDB [57] in the social networks field,
and NCI1 [40, 45] in the small molecule field. Detailed statistics of
these datasets are in Table 1. By using datasets from different fields
with different sizes, we can effectively evaluate the effectiveness
of our attacks in different real-world scenarios. We randomly split
each dataset into 10 equal parts, of which 9 parts are used to train
the target GNN model and the other 1 part is used for testing.
Target GNN model. We choose three representative GNN models,
i.e., GIN [55], SAG [26], and GUNet [15] as the target GNN model.
We train these models based on the authorsâ€™ public available source
code. The clean training/testing accuracy (without attack) of the
three GNN models on the three graph datasets are shown in Table 2.
Note that these results are close to those reported in the original
2Our attack can be easily extended to attack directed graphs via only changing the
adjacency matrix for directed graphs.
Table 1: Dataset statistics.
Dataset
Num. of Graphs
Num. of Classes
Avg. Num. of Nodes
Avg. Num. of Edges
IMDB COIL NCI1
1000
4110
3900
100
21.54
54.24
2
29.87
32.30
2
19.77
96.53
Table 2: Clean accuracy of the three GNN models.
GIN
GNN model Dataset Train acc Test acc
77.95%
77.00%
77.37%
42.56%
68.00%
72.02%
31.03%
70.00%
76.16%
82.17%
69.44%
73.59%
40.85%
64.78%
73.18%
31.25%
64.44%
69.59%
COIL
IMDB
NCI1
COIL
IMDB
NCI1
COIL
IMDB
NCI1
GUNet
SAG
papers. We can see that GIN achieves the best testing accuracy.
Thus, we use GIN as the default target model in this paper, unless
otherwise mentioned. We also observe that SAG and GUNet per-
form bad on COIL, and we thus do not conduct attacks on COIL for
SAG and GUNet.
Target graphs. We focus on generating untargeted adversarial
graphs, i.e., an attacker tries to deceive the target GNN model to
output each testing graph a wrong label different from its original
label. In our experiments, we select all testing graphs that are cor-
rectly classified by the target GNN model as the target graph. For