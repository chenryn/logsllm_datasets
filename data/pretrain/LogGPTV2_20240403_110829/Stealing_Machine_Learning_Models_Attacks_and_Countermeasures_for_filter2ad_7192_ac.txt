1We say model extraction attacks achieve an excellent performance because we choose
the state-of-the-art StyleGAN [32] trained on the LSUN-Bedroom dataset as a reference,
where it has the lowest FID 2.65.
5ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Hailong Hu and Jun Pang
Table 4: The performance of fidelity extraction with 50K
queries to the target model.
Target model Attack model Dataset
PGGAN
SNGAN
SNGAN
SNGAN
PGGAN
PGGAN
SNGAN
SNGAN
PGGAN
PGGAN
LSUN-Church
CelebA
LSUN-Church
CelebA
LSUN-Church
CelebA
LSUN-Church
CelebA
Fidelity
FID( ˜pд, pд)
6.11
4.49
1.68
1.02
8.76
5.34
2.21
1.39
Accuracy
FID ( ˜pд, pr )
14.05
9.29
8.28
4.93
30.04
17.32
14.56
9.57
instead of collecting their own data which is usually labor-intensive
and time-consuming.
For the target model PGGAN, if the attack model is SNGAN, we
observe that the performance of model extraction is very efficient
on both CelebA and LSUN-Church dataset and the attack model
SNGAN can learn more from the target model PGGAN, compared to
the SNGAN-SNGAN case, which indicates that attacking a state-of-
the-art GAN is valuable and viable for an adversary. Furthermore,
this case SNGAN-PGGAN is the most common situation in the
actual attack scenarios, because generally we implicitly assume
that performance of the adversary’s model may often be weaker
than that of the target model and the structure of the attack model
is inconsistent with that of the target model.
We also report accuracy in Table 4 and find that for model ex-
traction on GAN models, the accuracy of attack models is always
higher than that of target model, in which accuracy of attack models
represents similarity between distribution of real dataset pr and dis-
tribution of the attack model ˜pд and for accuracy of a target model,
also called FID of target model, it represents similarity between dis-
tribution of real dataset pr and distribution of the target model pд.
For example, when the target model SNGAN has 12.72 FID on the
LSUN-Church dataset, accuracy of the attack model SNGAN will
increase to 30.04. Even for the PGGAN-PGGAN case, its accuracy
increases from 3.40 to 4.93 on the CelebA dataset. This is mainly
because although theoretically, the distribution of the target model
pд is equal to that of the real training dataset pr , it is actually not
equal because GAN cannot achieve the global optimum. However,
we will discuss how to reduce accuracy values and achieve high
accuracy extraction in Section 6.
For the target model SNGAN, if the attack model is PGGAN,
the fidelity of model extraction is lower than that of the attack
model SNGAN. It is mainly because the PGGAN model itself is
stronger and able to more accurately approximate the target model.
Similarly, PGGAN as an attack model has more lower accuracy, in
contrast with SNGAN as an attack model. For instance, compared
to SNGAN-SNGAN with 17.32 of accuracy on CelebA dataset, the
accuracy of PGGAN-SNGAN is only 9.57, which largely improves
the attack performance on accuracy. This indicates that using an
attack model which is larger than the target model is an efficient
approach to improve attack performance.
Overall, fidelity extraction can achieve an excellent performance
in terms of fidelity. In general, adversaries can steal an fidelity model,
and then use the extracted model for their own purpose. However,
(a) Fidelity on CelebA
(b) Accuracy on CelebA
Figure 3: Attack performance on the number of queries.
unlike discriminative models where adversaries can directly utilize
their extracted model, the extracted model of a GAN only generates
target model’s images. Therefore, in Section 7, we will perform a
case study where adversaries can effectively leverage the extracted
model to generate images for their own applications rather than
target GANs’ images through transfer learning.
5.3.2 Attack Performance on the Number of Queries. We
choose PGGAN trained on CelebA dataset as the target model
to study the effect of the number of queries due to the best perfor-
mance among our target models. Figure 3 plots the attack perfor-
mance with respect to the number of queries which are also the size
of training dataset of attack models. As expected, we observe that
the attack performance increases with an increase in the number
of queries. This indicates that releasing a small number of data by
the model owner or restricting the number of queries is a relatively
safe measure.
We estimate the monetary cost of the number of queries. Taking
the Google Cloud Vision API2 as an example, the price is $1.50
per 1K queries with the first 1K queries are free for each month.
Thus, the price of the number of queries from 10K to 90K is from
$13.50 to $133.50. Although the attack cost is not high in our attacks,
designing a more powerful attack to reduce the number of queries
is still an interesting research direction. We leave it as future work.
5.3.3 Understanding Fidelity Extraction on GANs In-depth.
We further dissect the difference of distributions between target
models and attack models to understand the nature of model ex-
traction on GANs. Specifically, we first transform the training data
into 2048-dimension feature vectors by the pretrained Inception-
v3 model3 which is widely utilized in the evaluation of a GAN
model [23]. Then these feature vectors are clustered into k classes
by a standard K-means algorithm. Finally, we calculate the propor-
tions of each class, which can be also considered as a distribution of
the training data [5, 54]. The blue bar in Figure 4 shows the distri-
bution of the training data where we set k to 30. For target models
and attack models, we query the model to obtain 50K images, then
perform the same procedures as the training data.
Figure 4 shows distribution differences among the training data,
the target model PGGAN and attack models. We observe that for the
high proportions of classes, which can be considered as prominent
features of a distribution, target models can learn more features
about these classes while attack models further learn more features
by querying the target models. In contrast, for the low proportions
of classes, target models learn less features about these classes while
2https://cloud.google.com/vision/pricing
3https://pytorch.org/hub/pytorch_vision_inception_v3/
10k30k50k70k90kQueries246810FidelitySNGANPGGAN10k30k50k70k90kQueries5.07.510.012.515.0AccuracySNGANPGGAN6Stealing Machine Learning Models: Attacks and Countermeasures for Generative Adversarial Networks
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
increase the number of queries from 50K to 90K for the PGGAN-
PGGAN case on CelebA dataset, accuracy of the attack model has
smaller and smaller improvements from 4.93 to 4.44, while the ideal
accuracy is 3.40 which is also the performance of the target model.
Note that the case PGGAN-PGGAN is the best for the attacker; the
attack will perform even worse if the attackers do not choose the
same architectures and hyperparameters as the target model.
The reason why there exists a gap between the attack model and
the target model in terms of accuracy is that the target GAN model is
hard to reach global equilibrium and the discriminator is often better
than the generator in practice [3]. As a result, real data distribution
pr is not completely learned by the generator of the target model,
which means that pд (cid:44) pr . Therefore, directly using the generator’s
distribution pд does not guarantee the high accuracy and it only
minimizes the distribution discrepancy between the attack model
and the target model. We explain this by a simple example on
Figure 5, which is popular in the GAN literature [3, 16, 65].
Figure 5(a) presents real samples drawn from a mixture of 25
two-dimensional Gaussian distributions (each with standard de-
viation σ of 0.05). Figure 5(b) - Figure 5(d) show samples which
are generated by a target GAN with different queries. We define
a generated sample as “high-quality" if its Euclidean distance to
its corresponding mixture component is within four standard de-
viations (4σ = 0.2) [3]. The architecture and setup information of
the target GAN is shown in Appendix A.1. Overall, we can observe
that target GAN’s distribution is not completely the same as the
training set’s distribution, which means that directly extracting a
model from the generator of the target GAN makes its distribution
similar to the target model’s distribution rather than its training
dataset’s distribution.
Therefore, a natural approach to achieving accuracy extraction
is that the adversary can get more high-quality samples that are
closer to the real data distribution.
6.2 Methodology
Our approach to obtaining high-quality samples is based on sub-
sampling. The key insight here is that we can reject some poor
samples from generated samples based on some prior knowledge.
In order to achieve it, we suppose that adversaries can obtain ad-
ditional background information. This is a common assumption
that can be found in many works in relation to the security and
privacy of machine learning [20, 28, 60]. As shown in Figure 2, we
assume that adversaries can have limited auxiliary knowledge of
the discriminator of the target model and partial training samples.
This is because the discriminator from the target model can reveal
the distribution information of the training data [3]. Thus, using
the information provided by the discriminator, we can subsample
the generated data to make the obtained data closer to the real
dataset’s distribution, which improves accuracy extraction.
Specifically, for accuracy extraction, we first leverage the dis-
criminator of the target model to subsample the generated samples.
As a result, these refined samples are much closer to the true distri-
bution. In this work, we use Metropolis-Hastings (MH) subsampling
algorithm [65] to subsample the generated data. See Algorithm 1
in Appendix for details. MH subsampling algorithm utilizes the
discriminator through Metropolis-Hastings algorithm [62] to refine
Figure 4: Class distribution differences among the training
data, the target model PGGAN, and attack models.
Table 5: JS distances between models. A smaller value in-
dicates a better performance. The JS distance between the
training data and the target model PGGAN is 4.14×10−3. The
JS values below show a consistent trend with Figure 4.
Target model Attack model
PGGAN
SNGAN
PGGAN
JSfidelity (×10−3)
5.88
1.83
JSaccuracy (×10−3)
15.95
9.10
attack models further learn less features about these classes. This is
one reason why attack models always have higher accuracy values
than target models. In terms of fidelity, we observe that there is a
consistent trend on proportions of classes for target models and
attacks models. This is the reason why we can achieve a satisfy-
ing performance about fidelity. We also analyze the target model
SNGAN, and similar results are shown in Figure 9 in Appendix.
We also summarize this difference in a single number by com-
puting the Jensen-Shannon (JS) divergence on this representation
of distributions, which is shown in Table 5. Note that, based on
accuracy and fidelity defined in Section 4.1, we mark JSfidelity as
the JS divergence between the target model and the attack model,
and JSaccuracy as the JS divergence between the training data and
the attack model.
6 ACCURACY EXTRACTION
In this section, we instantiate our accuracy extraction attack strat-
egy. In addition to fidelity extraction’s assumptions, we also as-
sume that adversaries have more background knowledge in order
to achieve accuracy extraction, such as partial real data and the
target model’s discriminator. We start with the motivation and
problem formulation of accuracy extraction. Then, we describe the
methodology of accuracy extraction. In the end, we present the
performance of accuracy extraction.
6.1 Motivation and Problem Formulation
As shown in Figure 1, fidelity extraction can be implemented
through querying the generator of the target GAN, because pд
is the generator’s distribution. As for accuracy extraction, it is
much more difficult due to the lack of availability of real data distri-
bution pr . Although an approach is to use pд as an approximation
of pr , we observe that with the increase in the number of queries,
accuracy of attack models reaches its saturation point and is hard
to be improved, which is shown in Figure 3(b). For instance, as we
0510152025300.000.020.040.060.08Proportionstarget model PGGANattack model PGGANattack model SNGANtraining data7ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Hailong Hu and Jun Pang
(a) Real Samples 50K
(b) Direct Sampling 5k
(c) Direct Sampling 10K
(d) Direct Sampling 50K
(e) MH Subsampling 50K
Figure 5: Difference of distribution between training data and generators. The percentage of “high-quality” samples for Fig-
ure 5(b), Figure 5(c), Figure 5(d) and Figure 5(e) is 94.36%, 94.31%, 94.15% and 95.64%, respectively. The more we query, the more
bad-quality samples we obtain, which affects the performance of model extraction. But if we reduce the number of queries,
the performance of attack models still be poor due to insufficient training samples.
samples which are generated by the generator. The discrimina-
tor generally needs to be calibrated by partial real samples from
training set of the target GAN model, considering that some dis-
criminators of GANs output a score rather than a probability. In
our experiments, all discriminators are calibrated through logistic
regression. Then we train the attack model on those refined sam-
ples. After the training process of the attack model is stable, we add
partial real data to further train the attack model.
In this scenario, although the number of queries will increase
due to subsampling samples, we assume that adversaries eventu-
ally obtain 50K refined samples in order to make a comparison
with fidelity extraction. Partial real samples used to calibrate the
discriminator are fixed to 10% of training data. In addition, these
partial real samples will be added into training process of the attack
models. Here, we refer the former where only refined samples are
used to train the attack model to MH accuracy extraction which
is also considered as an indicator to show how well these refined
samples are beneficial to accuracy. We refer the latter where both