Alipourfard et al. [1, 2] suggest that small hash tables can suffice
for software switches as in skewed workloads (i.e., a small number
of flows carry most of the traffic volume) we can accurately monitor
all flows. Indeed, we can expect higher accuracy when the traffic
is only composed of a small number of elephant flows. However,
this small hash table approach is not robust since it relies on the
skewness of workloads to achieve good performance. Skewness is a
strong assumption since traffic distributions are sometimes heavy-
tailed (i.e., there are a significant number of flows with non-trivial
volumes). Further, the traffic distribution may vary at times, such
as a port scan. Even if we accept these limitations, updating a hash
table still requires a per-packet hash computation and counter update.
(a) Throughput vs. #flows on 1 core OVS-
(b) ElasticSketch (2.7MB) Accuracy vs.
DPDK.
#flows. in a malware trace [58].
Figure 3: Prior approaches are not performant or robust to a
large number of flows.
The performance of such actions is problematic once the table does
not fit in the last level cache.
SketchVisor [43] uses a fast path to expedite the packet process-
ing of the measurement on software switches when there is a queue
buildup before the sketches that process packets on a normal path.
The fast path maintains a hash table of k entries and each entry has
three different counters that are used for deciding whether to run an
update or a kick-out operation. The normal path contains standard,
slower sketches. Although the fast path can achieve higher packet
rate, it is less precise than the normal path as accuracy degrades sig-
nificantly when the majority of packets go to the fast path. Therefore,
it is not a robust solution.
ElasticSketch [73] splits the packet processing into two parts: a
heavy part and a light part. The heavy part is a table of hash buck-
ets, where the heavy flows are stored and evicted to the light part
when necessary; and the light part is a Count-Min Sketch (CMS),
which tracks the mice flows. The difference from SketchVisor is that
ElasticSketch has an eviction policy that further reduces the worst-
case packet operations. This design works well when the number
of flows is not large but using CMS solely in the light part loses
the generality for some measurement tasks that cannot be handled
with L11 guarantee from CMS. As depicted in Figure 3, these ap-
proaches cannot maintain acceptable accuracy and throughput when
the number of flow increases (e.g., for 20M flows the hash table’s
throughput reduces to less than 10Mpps and the error of ElasticS-
ketch exceeds 100% due to the overflow on its linear counting when
estimating the distinct flows). As depicted in Figures 11 and 12 in
the evaluation, the relative errors of the sketches that come with
better-than-L1 guarantee will not increase significantly.
R-HHH [8] reduces the update time of a deterministic Hierarchi-
cal Heavy Hitters algorithm [64] to O(1) by choosing one random
prefix per-packet to update. Although their algorithm is robust for a
specific measurement task, it does not support other measurement
tasks and is therefore not general.
In summary, existing solutions trade off robustness or generality
for improved performance as summarized in Table 1. Our goal is to
improve the performance without losing the robustness or generality.
3 BOTTLENECK ANALYSIS
We start by systematically understanding the bottlenecks of running
sketching inside software switches before we design optimizations.
For this analysis, we use the same testbed as described in Section 7
1L1 ≜ fx refers to the first norm of the flow frequency vector of the workload.
336
SketchesOVS DPDK0510152025Packet Rate (Mpps)UnivMonCount SketchCount-MinOVS-DPDKDPDK1K10K100K1M10M100MNo. of Flows051015Packet Rate (Mpps)HashtableUnivMon (5%)CountMin (1%)K-ary Sketch (5%)05101520253035Number of Flows [M]0510152025Relative Error [%]EntropyDistinctSIGCOMM ’19, August 19–23, 2019, Beijing, China
Z. Liu et al.
Description
CPU Time
hash computations
memcpy and counter update
Func/Call Stack
xxhash32
__memcpy
heap_find
univmon_proc
heapify
miniflow_extract
recv_pkts_vecs
37.29%
15.91%
10.71%
8.02%
4.91%
2.93%
2.71%
Table 2: CPU hotspots on UnivMon with OVS-DPDK.
heap operation
packet copy and cache
heap maintenance
retrieve miniflow info
dpdk packet recv
with min-sized packets as a worst-case scenario to stress the through-
put of software sketch implementations. We observe similar trends
with other workloads varying packet size distributions.
For the following analysis, we instrument a single thread OVS-
DPDK with UnivMon sketch as a representative example sketch. We
use Intel VTune Amplifier [46] to identify performance bottlenecks.
Our observations are qualitatively similar for other sketches as well,
but not shown for brevity. Table 2 summarizes the results.
Bottleneck 1: Sketches perform multiple independent hash com-
putations per packet.
From the profiling results in Table 2, we see an obvious bottleneck
due to hash function calls, which consumes ≈ 40% of the CPU. For
the ease of comparison later, if we denote the cost of each hash
operation as H, for a sketch using d1 hash functions, there is a d1 · H
computation cost we incur.
Bottleneck 2: Sketches entail multiple counter updates with mem-
ory copy and arithmetic operations.
The second significant bottleneck after the hashing we see in the
table is the memory operations. More specifically, sketches also
update a number of (e.g., 3 to 10) counters based on the computed
hash values, which in turn entail memory copy operations. If we
denote the cost of the counter update operation as C and there are d2
counter updates per packet, we can define this bottleneck as d2 · C.
Bottleneck 3: Sketches maintain extra data structures for track-
ing “heavy” flows that incur expensive per packet operations.
Along with the counters arrays, sketches use additional data struc-
tures for bookkeeping. For instance, when tracking top K heavy
hitters, users may want to know largest flows. In such cases, we can
use a heap to store the flow keys. As shown in Table 2, maintaining a
heap of top keys is not cheap. Let the cost of the per-packet operation
of updating such a heap be denoted as P.
Summary. Based on these observations, a typical sketch implemen-
tation entails Θ(d1 · H +d2 · C + P) per-packet operations. Given this,
we revisit prior attempts to improve the performance of the sketches
and see how they tackle these costs.
SketchVisor [43] partially addresses the bottlenecks by proposing
an improved Misra-Gries algorithm [63] to simplify the per-packet
per packet operations in their front-end. SketchVisor reduces the
amortized per-packet cost to 1H , 1C, 1P (d1H, d1C, 1P in worst case),
which can be much better than the original sketch implementations.
However, as we will see, with min-sized packets even this is a
high cost and cannot achieve a 10Gbps line rate. Perhaps more
337
importantly, the speedup comes at the cost of robustness when the
majority of packets are processed in the front-end, where the front-
end Misra-Gries algorithm is not as accurate as other sketches.
ElasticSketch [73] separates the processing of elephant and mice
flows, similarly as SketchVisor. Their approach mitigates the bottle-
neck operations to 1H , 1C, 1P per-packet even in worst-case, which
improves performance. However, this per-packet complexity is still
significant for software switches. Moreover, their back-end structure
is a Count-Min Sketch, which only provides L1 (norm) accuracy
guarantees. This implies that the accuracy guarantees do not hold
for complex functions such as entropy and L2.
4 NITROSKETCH DESIGN
We first discuss strawman alternatives and their limitations. We then
explain the key design ideas underlying NitroSketch.
4.1 Strawman Solutions and Lessons
As described in Section 3, the performance bottlenecks arise from
hash calculations (H), counter updates (C), and priority queue (or
heap) updates (P). We discuss the following strawman ideas to
address these bottlenecks:
Strawman1: Reduce the number of hash functions and arrays.
To reduce the number of bottleneck operations to 1H , 1C, 1P, we can
consider forming a one-array sketch. However, to retain the original
accuracy guarantee, we need to increase the number of counters
exponentially. For example, Count Sketch requires O(ϵ−2 log δ−1)
counters to provide an ϵ additive error with 1 − δ probability. This
approach requires O(ϵ−2δ−1) to match this accuracy. In practice,
when δ = 0.01, this suggestion increases memory by ≈ 50×. Sec-
ond, this approach may not be fast enough as we still perform one
hash calculation, one counter copy operation, and one heap update
operation per packet as 1H , 1C, 1P. Our evaluation shows that doing
so achieves 10G line-rate only when the sketch is Last Level Cache
(LLC) resident. However, large memory increase implies that the
sketch’s LLC residency is affected and the performance degrades.
Lessons learned: We learn two things from Approach #1: (1) In
software implementations, the memory usage of sketches should
be moderate to fit into LLC to gain the best performance; (2) Even
updating a single counter with one hash per-packet (i.e., 1H , 1C, 1P)
is non-trivial overhead under high packet rates (e.g., >15 Mpps), and
we need to optimize it further.
Strawman2: Run sketch only over sampled packets.
Uniform sampling is popular in estimating statistics in a database [13,
25, 47]. All sampling-based methods only provide accuracy guaran-
tees once the measurement is long enough. We define the waiting
time until accurate results are achieved as “convergence time”. By
using packet sampling, the number of packets that go to the sketch
is reduced, which reduces the number of per-packet operations.
However, this approach comes with the following limitations: (a)
First, we need a more accurate (larger) sketch to compensate for the
accuracy loss that results from the sampling. As shown in Appen-
dix B, if we set the error ϵ, sampling rate p, measurement length
ϵ−2p−1 log δ−1 +
m, and 1 − δ probability, the memory usage is Ω
. In practice, this results in 60MB+ mem-
ory, which likely results in a loss of cache residency. (b) Second,
ϵ−2p−1.5m−0.5 log1.5 δ−1(cid:17)
(cid:16)
NitroSketch: Robust and General Sketch-based Monitoring
SIGCOMM ’19, August 19–23, 2019, Beijing, China
Figure 4: Idea A: Counter array sampling to reduce per-packet
bottleneck operations.
flipping a coin requires a random number generation and doing it
for each packet has considerable overhead. Based on our test, a
single coin flip per-packet still prevents us from supporting 40Gbps
(≈ 60Mpps). (c) Third, small packet sampling rates result in long
convergence time, which is a potential issue for short-time measure-
ments [60].
Lessons learned: Sampling offers the potential for significant
speedup as it reduces the per-packet computation to less than 1H , 1C,
1P. However, it still has overheads such as random number genera-
tion and cache misses which prevent it from scaling to higher line
rates (e.g., 40G with min-sized packets). Furthermore, there is a
trade-off between the sampling probability and the convergence
time.
4.2 Key Ideas
In designing NitroSketch, we use the lessons learned from the bot-
tleneck analysis and the strawman solutions. We now describe our
key ideas and highlight how they improve the performance.
Idea A: Keep the multi-array structure as original sketches and
sample on the independent counter arrays.
Sketches use multiple high-quality and independent (usually require
pair-wise or even four-wise independent) hash functions that amplify
the success probability and play a significant role in their accuracy
guarantees. Thus, we retain the same multi-array structure and em-
ploy the same set of hash functions as the underlying sketch. For
each packet, we flip a coin for each array to decide if we need to up-
date a counter in that array, as shown in Figure 4. Doing so reduces
the per-packet hashes and counter updates to less than one by using
appropriate sampling rate p (e.g., p < 1
5 in Figure 4). Compared with
Strawman2, this idea also needs to increase the number of counters
but it is more space efficient in order to be cache resident, as we
will show in Section 5. However, this idea of counter array sampling
requires even more coin flips than packet sampling. Thus, Idea B
refines this idea to reduce the number of coin flips.
Idea B: Avoid coin flips by drawing a single geometric sample
for all the counter arrays.
The straightforward realization of Idea A needs one coin flip per
counter array when processing each packet. To avoid this, we con-
sider a simple but effective implementation that draws samples from
a geometric distribution to decide (i) which counter array will be
updated next (ii) how many packets to skip until that update. We
Figure 5: Idea B: Sampling the counter arrays with geometric
sampling to avoid per packet PRNG. In this way, we don’t need
“coin flips” every time.
Figure 6: Idea C: Adjusting geometric sampling rates based on
(estimated) packet arrival rate for faster convergence.
realize these two decisions by randomizing how many “arrays” to
skip until the next update.
We illustrate Idea B in Figure 5. The sketch has five counter
arrays, and let us assume that Array 1 was just updated. We draw a
geometric variable, with success probability p that tells us how many
arrays to skip. If the sample tells the next update is four arrays away,
we skip the following three arrays and update Array 5 (with the same
packet). If the randomization tells us to skip six arrays, then we will
skip further updates for the current packet and update Array 3 during
the next packet. Thus, we use a single geometric variable for each
sampled counter array and minimize the coin-toss overheads.
Idea C: Adaptive sampling based on packet arrival rate to re-
duce convergence time.
There is a trade-off between convergence time and the packet up-
date speed. The more packets we skip, the longer we need to wait
before providing an accuracy guarantee. When using a static counter
array sampling probability, we should determine p to be sufficiently
small for the highest possible packet rate. However, in that case, we
needlessly suffer a long convergence time as most workloads have
a lower average packet rate and occasional traffic bursts. When the
packet arrival rate is low, we do not need to statically skip many
packets (to achieve this rate) and we can thus enlarge p to sample
more packets for the sketch, as shown in Figure 6.
We propose two adaptive approaches:
(1) AlwaysLineRate. We dynamically set the counter array sam-
pling probability p to be inversely proportional to the current packet
arrival rate. We use a large p when the packet rate is low and reduce
p as it increases. This mode performs on average the same number
of operations within a time unit regardless of the packet rate.
(2) AlwaysCorrect. In this mode, we start with p = 1.0 (same as
the original sketches), and switch to AlwaysLineRate once we can
guarantee convergence. Doing so allows us to maintain the accuracy
guarantees throughout the measurement. This is required to maintain
the fidelity of composite sketches such as UnivMon.
338
PktTopKeysKey storingCoin flips w.p. 𝒑x✓xx+1+1✓w.p. 𝒑PktTopKeysKey storing✓+1+1✓✓✓✓+1+1+1Keep the structure,Sample onthe arrays.Uniform sampling with probability 𝒑Geometric sampling  with expectation 𝒑"𝟏	Mathematically EquivalentCoin flip w.p. 𝑝Coin flip w.p. 𝑝Coin flip w.p. 𝑝Coin flip w.p. 𝑝Coin flip w.p. 𝑝✓xxx✓✓SampledNext is 5 12345123453 arrays skipped✓SampledCoin flip for next w.p. 𝒑PktTopKeysKey storing𝒑=𝟏,𝟏𝟐,𝟏𝟒,𝟏𝟖……xx✓x+1e.g., every 10000 packets, measure packet rate:if 40Mpps, p=1/64; if 10Mpps, p=1/16 ……xSIGCOMM ’19, August 19–23, 2019, Beijing, China