ADD /nginx-conf /etc/nginx
EXPOSE 80 443
CMD ["nginx"]
Discussion
A typical practice when using Docker is to utilize environment vari‐
ables to change the way the container operates. You can use environ‐
ment variables in your NGINX configuration so that your NGINX
Dockerfile can be used in multiple, diverse environments.
122 | Chapter 11: Containers/Microservices
11.6 Kubernetes Ingress Controller
Problem
You are deploying your application on Kubernetes and need an
ingress controller.
Solution
Ensure that you have access to the ingress controller image. For
NGINX, you can use the nginx/nginx-ingress image from Docker‐
Hub. For NGINX Plus, you will need to build your own image and
host it in your private Docker registry. You can find instructions on
building and pushing your own NGINX Plus Kubernetes Ingress
Controller on NGINX Inc’s GitHub.
Visit the Kubernetes Ingress Controller Deployments folder in the
kubernetes-ingress repository on GitHub. The commands that fol‐
low will be run from within this directory of a local copy of the
repository.
Create a namespace and a service account for the ingress controller;
both are named nginx-ingress:
$ kubectl apply -f common/ns-and-sa.yaml
Create a secret with a TLS certificate and key for the ingress control‐
ler:
$ kubectl apply -f common/default-server-secret.yaml
This certificate and key are self-signed and created by NGINX Inc.
for testing and example purposes. It’s recommended to use your
own because this key is publicly available.
Optionally, you can create a config map for customizing NGINX
configuration (the config map provided is blank; however, you can
read more about customization and annotation here):
$ kubectl apply -f common/nginx-config.yaml
If Role-Based Access Control (RBAC) is enabled in your cluster, cre‐
ate a cluster role and bind it to the service account. You must be a
cluster administrator to perform this step:
$ kubectl apply -f rbac/rbac.yaml
11.6 Kubernetes Ingress Controller | 123
Now deploy the ingress controller. Two example deployments are
made available in this repository: a Deployment and a DaemonSet.
Use a Deployment if you plan to dynamically change the number of
ingress controller replicas. Use a DaemonSet to deploy an ingress
controller on every node or a subset of nodes.
If you plan to use the NGINX Plus Deployment manifests, you must
alter the YAML file and specify your own registry and image.
For NGINX Deployment:
$ kubectl apply -f deployment/nginx-ingress.yaml
For NGINX Plus Deployment:
$ kubectl apply -f deployment/nginx-plus-ingress.yaml
For NGINX DaemonSet:
$ kubectl apply -f daemon-set/nginx-ingress.yaml
For NGINX Plus DaemonSet:
$ kubectl apply -f daemon-set/nginx-plus-ingress.yaml
Validate that the ingress controller is running:
$ kubectl get pods --namespace=nginx-ingress
If you created a DaemonSet, port 80 and 443 of the ingress control‐
ler are mapped to the same ports on the node where the container is
running. To access the ingress controller, use those ports and the IP
address of any of the nodes on which the ingress controller is run‐
ning. If you deployed a Deployment, continue with the next steps.
For the Deployment methods, there are two options for accessing
the ingress controller pods. You can instruct Kubernetes to ran‐
domly assign a node port that maps to the ingress controller pod.
This is a service with the type NodePort. The other option is to cre‐
ate a service with the type LoadBalancer. When creating a service of
type LoadBalancer, Kubernetes builds a load balancer for the given
cloud platform, such as Amazon Web Services, Microsoft Azure,
and Google Cloud Compute.
To create a service of type NodePort, use the following:
$ kubectl create -f service/nodeport.yaml
124 | Chapter 11: Containers/Microservices
To statically configure the port that is opened for the pod, alter the
YAML and add the attribute nodePort: {port} to the configuration
of each port being opened.
To create a service of type LoadBalancer for Google Cloud Compute
or Azure, use this code:
$ kubectl create -f service/loadbalancer.yaml
To create a service of type LoadBalancer for Amazon Web Services:
$ kubectl create -f service/loadbalancer-aws-elb.yaml
On AWS, Kubernetes creates a classic ELB in TCP mode with the
PROXY protocol enabled. You must configure NGINX to use the
PROXY protocol. To do so, you can add the following to the config
map mentioned previously in reference to the file common/nginx-
config.yaml.
proxy-protocol: "True"
real-ip-header: "proxy_protocol"
set-real-ip-from: "0.0.0.0/0"
Then, update the config map:
$ kubectl apply -f common/nginx-config.yaml
You can now address the pod by its NodePort or by making a
request to the load balancer created on its behalf.
Discussion
As of this writing, Kubernetes is the leading platform in container
orchestration and management. The ingress controller is the edge
pod that routes traffic to the rest of your application. NGINX fits
this role perfectly and makes it simple to configure with its annota‐
tions. The NGINX-Ingress project offers an NGINX Open Source
ingress controller out of the box from a DockerHub image, and
NGINX Plus through a few steps to add your repository certificate
and key. Enabling your Kubernetes cluster with an NGINX ingress
controller provides all the same features of NGINX but with the
added features of Kubernetes networking and DNS to route traffic.
11.6 Kubernetes Ingress Controller | 125
11.7 OpenShift Router
Problem
You are deploying your application on OpenShift and would like to
use NGINX as a router.
Solution
Build the Router image and upload it to your private registry. You
can find the source files for the image in the Origin Repository. It’s
important to push your Router image to the private registry before
deleting the default Router because it will render the registry
unavailable.
Log in to the OpenShift Cluster as an admin:
$ oc login -u system:admin
Select the default project:
$ oc project default
Back up the default Router config, in case you need to recreate it:
$ oc get -o yaml service/router dc/router \
clusterrolebinding/router-router-role \
serviceaccount/router > default-router-backup.yaml
Delete the Router:
$ oc delete -f default-router-backup.yaml
Deploy the NGINX Router:
$ oc adm router router --images={image} --type='' \
--selector='node-role.kubernetes.io/infra=true'
In this example, the {image} must point to the NGINX Router
image in your registry. The selector parameter specifies a label selec‐
tor for nodes where the Router will be deployed: node-
role.kubernetes.io/infra=true. Use a selector that makes sense
for your environment.
Validate that your NGINX Router pods are running:
$ oc get pods
You should see a Router pod with the name router-1-{string}.
126 | Chapter 11: Containers/Microservices
By default, the NGINX stub status page is available via port 1936 of
the node where the Router is running (you can change this port by
using the STATS_PORT env variable). To access the page outside of
the node, you need to add an entry to the IPtables rules for that
node:
$ sudo iptables -I OS_FIREWALL_ALLOW -p tcp -s {ip range} \
-m tcp --dport 1936 -j ACCEPT
Open your browser to http://{node-ip}:1936/stub_status to access the
stub status page.
Discussion
The OpenShift Router is the entry point for external requests bound
for applications running on OpenShift. The Router’s job is to receive
incoming requests and direct them to the appropriate application
pod. The load-balancing and routing abilities of NGINX make it a
great choice for use as an OpenShift Router. Switching out the
default OpenShift Router for an NGINX Router enables all of the
features and power of NGINX as the ingress of your OpenStack
application.
11.7 OpenShift Router | 127
CHAPTER 12
High-Availability
Deployment Modes
12.0 Introduction
Fault-tolerant architecture separates systems into identical, inde‐
pendent stacks. Load balancers like NGINX are employed to distrib‐
ute load, ensuring that what’s provisioned is utilized. The core
concepts of high availability are load balancing over multiple active
nodes or an active-passive failover. Highly available applications
have no single points of failure; every component must use one of
these concepts, including the load balancers themselves. For us, that
means NGINX. NGINX is designed to work in either configuration:
multiple active or active-passive failover. This chapter details techni‐
ques on how to run multiple NGINX servers to ensure high availa‐
bility in your load-balancing tier.
12.1 NGINX HA Mode
Problem
You need a highly available load-balancing solution.
129
Solution
Use NGINX Plus’s highly available (HA) mode with keepalived by
installing the nginx-ha-keepalived package from the NGINX Plus
repository.
Discussion
The nginx-ha-keepalived package is based on keepalived and
manages a virtual IP address exposed to the client. Another process
is run on the NGINX server that ensures that NGINX Plus and the
keepalived process are running. Keepalived is a process that utilizes
the Virtual Router Redundancy Protocol (VRRP), sending small
messages often referred to as heartbeats, to the backup server. If the
backup server does not receive the heartbeat for three consecutive
periods, the backup server initiates the failover, moving the virtual
IP address to itself and becoming the master. The failover capabili‐
ties of nginx-ha-keepalived can be configured to identify custom
failure situations.
12.2 Load-Balancing Load Balancers with DNS
Problem
You need to distribute load between two or more NGINX servers.
Solution
Use DNS to round robin across NGINX servers by adding multiple
IP addresses to a DNS A record.
Discussion
When running multiple load balancers, you can distribute load via
DNS. The A record allows for multiple IP addresses to be listed
under a single, fully qualified domain name. DNS will automatically
round robin across all the IPs listed. DNS also offers weighted round
robin with weighted records, which works in the same way as
weighted round robin in NGINX as described in Chapter 1. These
techniques work great. However, a pitfall can be removing the
record when an NGINX server encounters a failure. There are DNS
130 | Chapter 12: High-Availability Deployment Modes
providers—Amazon Route53 for one, and Dyn DNS for another—
that offer health checks and failover with their DNS offering, which
alleviates these issues. If you are using DNS to load balance over
NGINX, when an NGINX server is marked for removal, it’s best to
follow the same protocols that NGINX does when removing an
upstream server. First, stop sending new connections to it by remov‐
ing its IP from the DNS record, then allow connections to drain
before stopping or shutting down the service.
12.3 Load Balancing on EC2
Problem
You’re using NGINX on AWS, and the NGINX Plus HA does not
support Amazon IPs.
Solution
Put NGINX behind an AWS NLB by configuring an Auto Scaling
group of NGINX servers and linking the Auto Scaling group to a
target group and then attach the target group to the NLB. Alterna‐
tively, you can place NGINX servers into the target group manually
by using the AWS console, command-line interface, or API.
Discussion
The HA solution from NGINX Plus based on keepalived will not
work on AWS because it does not support the floating virtual IP
address, since EC2 IP addresses work in a different way. This does
not mean that NGINX can’t be HA in the AWS cloud; in fact, the
opposite is true. The AWS NLB is a product offering from Amazon
that will natively load balance over multiple, physically separated
data centers called availability zones, provide active health checks,
and a DNS CNAME endpoint. A common solution for HA NGINX
on AWS is to put an NGINX layer behind the NLB. NGINX servers
can be automatically added to and removed from the target group as
needed. The NLB is not a replacement for NGINX; there are many
things NGINX offers that the NLB does not, such as multiple load-
balancing methods, rate limiting, caching, and Layer 7 routing. The
AWS ALB does perform Layer 7 load balancing based on the URI
path and host heade, but it does not by itself offer features NGINX
12.3 Load Balancing on EC2 | 131
does, such as WAF caching, bandwidth limiting, HTTP/2 server
push, and more. In the event that the NLB does not fit your need,
there are many other options. One option is the DNS solution,
Route53. The DNS product from AWS offers health checks and
DNS failover.
12.4 Configuration Synchronization
Problem
You’re running a HA NGINX Plus tier and need to synchronize con‐
figuration across servers.
Solution
Use the NGINX Plus exclusive configuration synchronization fea‐
ture. To configure this feature, follow these steps:
Install the nginx-sync package from the NGINX Plus package
repository.
For RHEL or CentOS:
$ sudo yum install nginx-sync
For Ubuntu or Debian:
$ sudo apt-get install nginx-sync
Grant the master machine SSH access as root to the peer machines.
Generate an SSH authentication key pair for root and retrieve the
public key:
$ sudo ssh-keygen -t rsa -b 2048
$ sudo cat /root/.ssh/id_rsa.pub
ssh-rsa AAAAB3Nz4rFgt...vgaD root@node1
Get the IP address of the master node:
$ ip addr
1: lo: mtu 65536 qdisc noqueue state UNKNOWN group default
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
inet 127.0.0.1/8 scope host lo
valid_lft forever preferred_lft forever
inet6 ::1/128 scope host
valid_lft forever preferred_lft forever
2: eth0: mtu 1500 qdisc pfifo_fast state UP group default qlen \
1000
link/ether 52:54:00:34:6c:35 brd ff:ff:ff:ff:ff:ff
132 | Chapter 12: High-Availability Deployment Modes
inet 192.168.1.2/24 brd 192.168.1.255 scope global eth0
valid_lft forever preferred_lft forever
inet6 fe80::5054:ff:fe34:6c35/64 scope link
valid_lft forever preferred_lft forever
The ip addr command will dump information about interfaces on
the machine. Disregard the loopback interface, which is normally
the first. Look for the IP address following inet for the primary
interface. In this example, the IP address is 192.168.1.2.
Distribute the public key to the root user’s authorized_keys file on
each peer node, and specify to authorize only from the master’s IP
address:
$ sudo echo ‘from=”192.168.1.2" ssh-rsa AAAAB3Nz4rFgt...vgaD \
root@node1' >> /root/.ssh/authorized_keys
Add the following line to /etc/ssh/sshd_config and reload sshd on all
nodes:
$ sudo echo 'PermitRootLogin without-password' >> \
/etc/ssh/sshd_config
$ sudo service sshd reload
Verify that the root user on the master node can ssh to each of the
peer nodes without a password:
$ sudo ssh PI:EMAIL
Create the configuration file /etc/nginx-sync.conf on the master
machine with the following configuration:
NODES="node2.example.com node3.example.com node4.example.com"
CONFPATHS="/etc/nginx/nginx.conf /etc/nginx/conf.d"
EXCLUDE="default.conf"
This example configuration demonstrates the three common config‐
uration parameters for this feature: NODES, CONFIGPATHS, and
EXCLUDE. The NODES parameter is set to a string of hostnames or IP
addresses separated by spaces; these are the peer nodes to which the
master will push its configuration changes. The CONFIGPATHS
parameter denotes which files or directories should be synchron‐
ized. Lastly, you can use the EXCLUDE parameter to exclude configu‐
ration files from synchronization. In our example, the master pushes
configuration changes of the main NGINX configuration file and
includes the directory /etc/nginx/nginx.conf and /etc/nginx/conf.d to
peer nodes named node2.example.comnode3.example.com and
node4.example.com. If the synchronization process finds a file
12.4 Configuration Synchronization | 133
named default.conf, it will not be pushed to the peers, because it’s
configured as an EXCLUDE.
There are advanced configuration parameters to configure the loca‐
tion of the NGINX binary, RSYNC binary, SSH binary, diff binary,
lockfile location, and backup directory. There is also a parameter
that utilizes sed to template given files. For more information about
the advanced parameters, see Configuration Sharing.
Test your configuration:
$ nginx-sync.sh -h # display usage info
$ nginx-sync.sh -c node2.example.com # compare config to node2
$ nginx-sync.sh -C # compare master config to all peers
$ nginx-sync.sh # sync the config & reload NGINX on peers
Discussion
This NGINX Plus exclusive feature enables you to manage multiple
NGINX Plus servers in a highly available configuration by updating
only the master node and synchronizing the configuration to all
other peer nodes. By automating the synchronization of configura‐
tion, you limit the risk of mistakes when transferring configurations.
The nginx-sync.sh application provides some safeguards to pre‐
vent sending bad configurations to the peers. They include testing
the configuration on the master, creating backups of the configura‐
tion on the peers, and validating the configuration on the peer
before reloading. Although it’s preferable to synchronize your con‐
figuration by using configuration management tools or Docker, the
NGINX Plus configuration synchronization feature is valuable if
you have yet to make the big leap to managing environments in this
way.
12.5 State Sharing with Zone Sync
Problem
You need NGINX Plus to synchronize its shared memory zones
across a fleet of highly available servers.
Solution
Use the sync parameter when configuring an NGINX Plus shared
memory zone:
134 | Chapter 12: High-Availability Deployment Modes
stream {
resolver 10.0.0.2 valid=20s;
server {
listen 9000;
zone_sync;
zone_sync_server nginx-cluster.example.com:9000 resolve;
# ... Security measures
}
}
http {
upstream my_backend {
zone my_backend 64k;
server backends.example.com resolve;
sticky learn zone=sessions:1m
create=$upstream_cookie_session
lookup=$cookie_session
sync;
}