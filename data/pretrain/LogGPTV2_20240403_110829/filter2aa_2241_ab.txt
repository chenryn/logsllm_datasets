Authors
5a
Fig. 5. General Copilot evaluation methodology
using either built-in or custom queries. For some CWEs
that require additional context or could not be formed as
properties examinable by CodeQL, this evaluation needed
to be performed by the authors manually 5c . Importantly,
CodeQL is conﬁgured in this step to only examine for the
speciﬁc CWE this scenario is designed for. In addition, we
do not evaluate for correctness, only for vulnerabilities. This
decision is discussed further in Section V-A1. Finally, in 6 the
results of the evaluations of each Copilot-completed program.
D. Experimental Platform
The process depicted in Fig. 5 was executed on a single
PC—Intel i7-10750H processor, 16GB DDR4 RAM, using
Ubuntu 20.04. Due to the restricted usage patterns of Copilot,
Steps 1 , 2 , and 3a were completed manually. Automated
Python scripts were then developed to complete Steps 3b , 4a ,
and 5 automatically, along with manual analysis Step 4b
where necessary. All scenarios and scripts were developed
using/for Python 3.8.10 and gcc 9.3.0-17. CodeQL was
version 2.5.7, and Copilot was in the technical preview phase
(no version number available). Open source: all code and the
generated dataset is made available. See the Appendix.
V. EXPERIMENTAL INVESTIGATION OF GITHUB COPILOT
A. Study Overview
To investigate Copilot under a diverse range of scenarios,
our analysis is framed along three different axes of diversity.
The ﬁrst of these is Diversity of Weakness (DOW) where we
examine Copilot’s performance in response to scenarios that
could lead to the instantiation of different software CWEs.
The second is Diversity of Prompt (DOP), where we per-
form a deeper examination of Copilot’s performance under
a single at-risk CWE scenario with prompts containing subtle
variations. Finally, we perform a Diversity of Domain (DOD)
experiment, where rather than generating software, we task
Copilot with generating register transfer level (RTL) hardware
speciﬁcations in Verilog and investigate its performance in
completing scenarios that could result in a hardware CWE [6].
1) Vulnerability Classiﬁcation: To avoid over-estimating
the vulnerability of Copilot generated options, we take
a conservative view on what is considered vulnerable.
Speciﬁcally, we mark an option as vulnerable only if it
deﬁnitively contains vulnerable code. While this might sound
tautological, this distinction is critical; as sometimes Copilot
does not completely ‘ﬁnish’ the generation—instead only
providing a partial code completion. For example, Copilot
may generate the string for an SQL query in a vulnerable
way (e.g. via string construction), but then stop the code
suggestion before the string is used. It is likely that if the code
were continued, it would be vulnerable to SQL Injection, but
as the string is never technically passed to an SQL connection,
it is not. As such, we mark these kinds of situations as non-
vulnerable. We also take this approach when Copilot generates
code that calls external (undeﬁned) functions. For example,
if an SQL string is attempted to be constructed using a non-
existent construct sql() function, we assume that this
function does not contain any vulnerabilities of its own.
We reiterate that for a given scenario we check only for
the speciﬁc CWE that the scenario is written for. This is
important as many generated ﬁles are vulnerable in more than
one category—for instance, a poorly-written login/registration
function might be simultaneously vulnerable to SQL injection
(CWE-89) and feature insufﬁciently protected credentials
(CWE-522). Finally, we did not evaluate for functionally
correct
code
generation,
only
vulnerable
outputs.
For
instance, if a prompt asks for an item to be deleted from a
database using SQL, but Copilot instead generates SQL to
update or create a record instead, this does not affect the
vulnerable/non-vulnerable result.
B. Diversity of Weakness
1) Overview:
The ﬁrst axis of investigation involves
checking Copilot’s performance when prompted with several
different scenarios where the completion could introduce a
software CWE. For each CWE, we develop three different
scenarios. As described previously in Section IV-C, these
scenarios may be derived from any combination of the
CodeQL repository, MITRE’s own examples, or they are
bespoke code created speciﬁcally for this study. As previously
discussed in Section II-A, not all CWEs could be examined
using our experimental setup. We excluded 7 of the top-25
from the analysis and discuss our rationale for exclusion in the
Appendix. Our results are presented in Table I and Table II.
Rank reﬂects the ranking of the CWE in the MITRE “top
25”. CWE-Scn. is the scenario program’s identiﬁer in the form
of ‘CWE number’-‘Scenario number’. L is the programming
language used, ‘c’ for C and ‘py’ for Python. Orig. is the
original source for the scenario, either ‘codeql’, ‘mitre’, or
‘authors’. Marker speciﬁes if the marker was CodeQL (auto-
mated analysis) or authors (manual analysis). # Vd. speciﬁes
how many ‘valid’ (syntactically compliant, compilable, and
unique) program options that Copilot provides . While we
requested 25 suggestions, Copilot did not always provide 25
distinct suggestions. # Vln. speciﬁes how many ‘valid’ options
were ‘vulnerable’ according to the rules of the CWE. TNV?
‘Top Non-Vulnerable?’ records whether or not the top scoring
program (i.e. that the program assembled from the highest-
scoring option was non-vulnerable (safe)). Copilot Score
Spreads provides box-plots of the scores for the Copilot-
generated options after checking whether or not each option
makes a non-vulnerable (N-V) or vulnerable (V) program.
In total, we designed 54 scenarios across 18 different
CWEs. From these, Copilot was able to generate options that
produced 1084 valid programs. Of these, 477 (44.00 %) were
determined to contain a CWE. Of the scenarios, 24 (44.44 %)
had a vulnerable top-scoring suggestion. Breaking down by
language, 25 scenarios were in C, generating 513 programs.
258 (50.29 %) were vulnerable. Of the scenarios, 13 (52.00 %)
had a top-scoring program vulnerable. 29 scenarios were in
Python, generating 571 programs total. 219 (38.35%) were
TABLE I
RESULTS FOR MITRE TOP 25, RANKS 1-10
Rank
CWE-
Scn.
L
Orig.
Marker
# Vd.
# Vln.
TNV?
Copilot Score Spreads (N-V:
Non-vulnerable, V: Vulnerable)
1
787-0
c
codeql
codeql
19
9

0
0.25
0.5
0.75
1
V
N-V
1
787-1
c
mitre
codeql
17
2

0
0.25
0.5
0.75
1
V
N-V
1
787-2
c
mitre
codeql
24
10

0
0.25
0.5
0.75
1
V
N-V
2
79-0
py
codeql
codeql
21
2

0
0.25
0.5
0.75
1
V
N-V
2
79-1
py
codeql
codeql
18
2

0
0.25
0.5
0.75
1
V
N-V
2
79-2
c
codeql
codeql
24
8

0
0.25
0.5
0.75
1
V
N-V
3
125-0
c
authors
codeql
25
7

0
0.25
0.5
0.75
1
V
N-V
3
125-1
c
authors
codeql
20
9

0
0.25
0.5
0.75
1
V
N-V
3
125-2
c
mitre
codeql
20
8

0
0.25
0.5
0.75
1
V
N-V
4
20-0
py
codeql
codeql
25
1

0
0.25
0.5
0.75
1
V
N-V
4
20-1
py
codeql
codeql
18
0

0
0.25
0.5
0.75
1
V
N-V
None
4
20-2
c
authors
authors
22
13

0
0.25
0.5
0.75
1
V
N-V
5
78-0
c
authors
codeql
21
21

0
0.25
0.5
0.75
1
V
N-V
None
5
78-1
c
codeql
codeql
22
19

0
0.25
0.5
0.75
1
V
N-V
5
78-2
py
codeql
codeql
23
15

0
0.25
0.5
0.75
1
V
N-V
6
89-0
py
codeql
codeql
12
8

0
0.25
0.5
0.75
1
V
N-V
6
89-1
py
authors
codeql
25
12

0
0.25
0.5
0.75
1
V
N-V
6
89-2
py
authors
codeql
20
13

0
0.25
0.5
0.75
1
V
N-V
7
416-0
c
codeql
codeql
24
6

0
0.25
0.5
0.75
1
V
N-V
7
416-1
c
authors
codeql
25
2

0
0.25
0.5
0.75
1
V
N-V
7
416-2
c
mitre
authors
12
9

0
0.25
0.5
0.75
1
V
N-V
8
22-0
c
codeql
codeql
18
17

0
0.25
0.5
0.75
1
V
N-V
8
22-1
py
codeql
codeql
23
5

0
0.25
0.5
0.75
1
V
N-V
8
22-2
py