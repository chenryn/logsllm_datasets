76.63
70.28 (↓ 6)
2) System and Architecture Defense Approaches: One pos-
sible way to mitigate the DeepSteal attack is to use software-
based encryption schemes for streaming applications. For
example, with software encryption, the ML framework can
perform on-demand weight decryption during computation
without storing the plaintext weight parameters in memory
at any time. Note that while the runtime overhead of such
software-based protection scheme needs evaluation, one block
encryption (128-bit) using AES-NI instruction set (advanced
instruction set designed speciﬁcally to accelerate AES encryp-
tion/decryption) still requires 8-cycle latency [81]. This can
accumulate substantially and result in potentially high run-time
overhead, limiting the deployment of such implementation in
DNN applications.
Alternative to the system-level protection schemes, Deep-
Steal can be potentially defeated by protecting the conﬁ-
dentiality and integrity of secretive pages through trusted
execution environments (TEE) [82]. With TEE, pages that
belong to a protected enclave are encrypted using processor-
side memory encryption engine before leaving the processor
chip. Hence, even with the rowhammer-based side channel
leakage, the attacker cannot retrieve any information about
the actual data. However, DNNs typically come with large
memory footprints that exceed TEE’s pre-allocated secure
memory region. As a result, ML applications in contemporary
TEE are subject to considerable runtime overhead [83] due to
the expensive operations of page swapping between secure and
unsecure memory regions. Therefore, we believe designing
TEE architectures tailored for ML applications is critical in
the future.
X. CONCLUSION
The training of deep neural networks requires heavy com-
putational resources and sensitive domain-speciﬁc private user
data. Thus, any potential breach in model privacy through
leakage of sensitive model parameters may cost the service
provider a heavy ﬁnancial penalty. Consequently, the IP of a
pre-trained DNN model is critical to protect against adversarial
threats (i.e., model extraction). In this work, our proposed
DeepSteal attack exposes this threat of an effective model
extraction attack in practical settings. In particular, our novel
system-level weight bit extraction method HammerLeak en-
ables fast and efﬁcient weight stealing for large scale DNN
applications. It can recover a signiﬁcant portion of the weight
bits of a DNN model with millions of wight parameters. On
top of that, our proposed Mean Clustering training algorithm
can leverage this information to effectively launch a strong
adversarial input attack on the victim model. The efﬁcacy of
the proposed attack algorithm is validated through extensive
experimental evaluation. Such a model extraction threat should
encourage future work in this direction to protect the IP of
large-scale DNN models.
ACKNOWLEDGEMENT
This work is supported in part by the National Science
Foundation under Grant No. 2019548, No. 2019536, No.
1931871, and No. 2144751.
AVAILABILITY
Tools and useful code for DeepSteal system exploit are
released in https://github.com/casrl/DeepSteal-exploit. The de-
tailed models and dataset for our substitute model
train-
ing can be accessed at https://github.com/ASU-ESIC-FAN-
Lab/DeepStealSP2022.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:59:25 UTC from IEEE Xplore.  Restrictions apply. 
1169
REFERENCES
[1] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for
image recognition,” in IEEE conference on computer vision and pattern
recognition, 2016, pp. 770–778.
[2] He et al., “Delving deep into rectiﬁers: Surpassing human-level perfor-
mance on imagenet classiﬁcation,” in IEEE International Conference on
Computer Vision, December 2015.
[3] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke,
D. Yu, and G. Zweig, “Achieving human parity in conversational speech
recognition,” arXiv preprint arXiv:1610.05256, 2016.
[4] V. Chandrasekaran, K. Chaudhuri, I. Giacomelli, S. Jha, and S. Yan,
“Exploring connections between active learning and model extraction,”
in USENIX Security Symposium, Aug. 2020, pp. 1309–1326.
[5] S. Hong, M. Davinroy, Y. Kaya, D. Dachman-Soled, and T. Dumitra¸s,
“How to 0wn nas in your spare time,” arXiv preprint arXiv:2002.06776,
2020.
[6] K. Murdock, D. Oswald, F. D. Garcia, J. Van Bulck, D. Gruss, and
F. Piessens, “Plundervolt: Software-based fault injection attacks against
intel sgx,” in IEEE Symposium on Security and Privacy, 2020, pp. 1466–
1482.
[7] A. Kwong, D. Genkin, D. Gruss, and Y. Yarom, “Rambleed: Reading
bits in memory without accessing them,” in IEEE Symposium on Security
and Privacy, 2020, pp. 695–711.
[8] D. Gruss, C. Maurice, and S. Mangard, “Rowhammer. js: A remote
software-induced fault attack in javascript,” in International conference
on detection of intrusions and malware, and vulnerability assessment.
Springer, 2016, pp. 300–321.
[9] Y. Kim, R. Daly, J. Kim, C. Fallin, J. H. Lee, D. Lee, C. Wilkerson,
K. Lai, and O. Mutlu, “Flipping bits in memory without accessing them:
An experimental study of dram disturbance errors,” in ACM SIGARCH
Computer Architecture News, vol. 42, no. 3, 2014, pp. 361–372.
[10] Z. Kenjar, T. Frassetto, D. Gens, M. Franz, and A.-R. Sadeghi,
“V0ltpwn: Attacking x86 processor integrity from software,” in USENIX
Security Symposium, 2020, pp. 1445–1461.
[11] M. Jagielski, N. Carlini, D. Berthelot, A. Kurakin, and N. Papernot,
“High accuracy and high ﬁdelity extraction of neural networks,” in
USENIX Security Symposium, 2020, pp. 1345–1362.
[12] V. Chandrasekaran, K. Chaudhuri, I. Giacomelli, S. Jha, and S. Yan,
“Exploring connections between active learning and model extraction,”
in USENIX Security Symposium, 2020, pp. 1309–1326.
[13] T. Orekondy, B. Schiele, and M. Fritz, “Knockoff nets: Stealing func-
tionality of black-box models,” in IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2019, pp. 4954–4963.
[14] A. Barbalau, A. Cosma, R. T. Ionescu, and M. Popescu, “Black-
box ripper: Copying black-box models using generative evolutionary
algorithms,” arXiv preprint arXiv:2010.11158, 2020.
[15] G. K. Nayak, K. R. Mopuri, V. Shaj, V. B. Radhakrishnan, and
A. Chakraborty, “Zero-shot knowledge distillation in deep networks,”
in International Conference on Machine Learning. PMLR, 2019, pp.
4743–4751.
[16] J. R. Correia-Silva, R. F. Berriel, C. Badue, A. F. de Souza, and
T. Oliveira-Santos, “Copycat cnn: Stealing knowledge by persuading
confession with random non-labeled data,” in IEEE International Joint
Conference on Neural Networks, 2018, pp. 1–8.
[17] S. Pal, Y. Gupta, A. Shukla, A. Kanade, S. Shevade, and V. Ganapathy,
“A framework for the extraction of deep neural networks by leveraging
public data,” arXiv preprint arXiv:1905.09165, 2019.
[18] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and
A. Swami, “Practical black-box attacks against machine learning,” in
ACM Asia Conference on Computer and Communications Security,
2017, pp. 506–519.
[19] S. Milli, L. Schmidt, A. D. Dragan, and M. Hardt, “Model reconstruction
from model explanations,” in Conference on Fairness, Accountability,
and Transparency, 2019, pp. 1–9.
[20] D. Rolnick and K. Kording, “Reverse-engineering deep relu networks,”
in International Conference on Machine Learning. PMLR, 2020, pp.
8178–8187.
[21] H. Naghibijouybari, A. Neupane, Z. Qian, and N. Abu-Ghazaleh,
“Rendered insecure: Gpu side channel attacks are practical,” in ACM
SIGSAC conference on computer and communications security, 2018,
pp. 2139–2153.
[22] F. Liu, Y. Yarom, Q. Ge, G. Heiser, and R. B. Lee, “Last-level cache
side-channel attacks are practical,” in IEEE symposium on security and
privacy, 2015, pp. 605–622.
[23] F. Yao, G. Venkataramani, and M. Doroslovaˇcki, “Covert timing chan-
nels exploiting non-uniform memory access based architectures,” in
Great Lakes Symposium on VLSI, 2017, pp. 155–160.
[24] M. H. I. Chowdhuryy, H. Liu, and F. Yao, “Branchspec: Information
leakage attacks exploiting speculative branch instruction executions,” in
IEEE 38th International Conference on Computer Design, 2020, pp.
529–536.
[25] M. Yan, C. W. Fletcher, and J. Torrellas, “Cache telepathy: Leveraging
shared resource attacks to learn dnn architectures,” in USENIX Security
Symposium, 2020, pp. 2003–2020.
[26] H. Yu, H. Ma, K. Yang, Y. Zhao, and Y. Jin, “Deepem: Deep neural
networks model recovery through em side-channel information leakage,”
in IEEE International Symposium on Hardware Oriented Security and
Trust, 2020, pp. 209–218.
[27] L. Batina, S. Bhasin, D. Jap, and S. Picek, “CSI NN: Reverse
engineering of neural network architectures through electromagnetic
side channel,” in USENIX Security Symposium, Santa Clara, CA,
Aug. 2019, pp. 515–532. [Online]. Available: https://www.usenix.org/
conference/usenixsecurity19/presentation/batina
[28] X. Hu, L. Liang, S. Li, L. Deng, P. Zuo, Y. Ji, X. Xie, Y. Ding, C. Liu,
T. Sherwood et al., “Deepsniffer: A dnn model extraction framework
based on learning architectural hints,” in International Conference
on Architectural Support for Programming Languages and Operating
Systems, 2020, pp. 385–399.
[29] Y. Zhu, Y. Cheng, H. Zhou, and Y. Lu, “Hermes attack: Steal dnn models
with lossless inference accuracy,” in USENIX Security Symposium, 2021.
[30] T. Nakai, D. Suzuki, and T. Fujino, “Timing black-box attacks: Crafting
adversarial examples through timing leaks against dnns on embedded
devices,” IACR Transactions on Cryptographic Hardware and Embedded
Systems, pp. 149–175, 2021.
[31] Y. Yarom and K. Falkner, “Flush+reload: A high resolution, low noise,
l3 cache side-channel attack,” in USENIX Security Symposium, 2014,
pp. 719–732.
[32] D. Evtyushkin, R. Riley, N. C. Abu-Ghazaleh, ECE, and D. Ponomarev,
“Branchscope: A new side-channel attack on directional branch predic-
tor,” ACM SIGPLAN Notices, vol. 53, no. 2, pp. 693–707, 2018.
[33] M. Yan, R. Sprabery, B. Gopireddy, C. Fletcher, R. Campbell, and
J. Torrellas, “Attack directories, not caches: Side channel attacks in
a non-inclusive world,” in IEEE Symposium on Security and Privacy,
2019, pp. 888–904.
[34] M. H. I. Chowdhuryy and F. Yao, “Leaking secrets through modern
branch predictors in the speculative world,” IEEE Transactions on
Computers, 2021.
[35] Z. Zhang, Z. Zhan, D. Balasubramanian, B. Li, P. Volgyesi, and X. Kout-
soukos, “Leveraging em side-channel information to detect rowhammer
attacks,” in IEEE Symposium on Security and Privacy, 2020, pp. 729–
746.
[36] A. S. Rakin, Z. He, and D. Fan, “Bit-ﬂip attack: Crushing neural
network with progressive bit search,” in IEEE International Conference
on Computer Vision, October 2019.
[37] F. Yao, A. S. Rakin, and D. Fan, “Deephammer: Depleting the intelli-
gence of deep neural networks through targeted chain of bit ﬂips,” in
USENIX Security Symposium, 2020, pp. 1463–1480.
[38] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.
[39] S. Addepalli, G. K. Nayak, A. Chakraborty, and V. B. Radhakrishnan,
“Degan: Data-enriching gan for retrieving representative samples from a
trained classiﬁer,” in AAAI Conference on Artiﬁcial Intelligence, vol. 34,
no. 04, 2020, pp. 3130–3137.
[40] N. Carlini, M. Jagielski, and I. Mironov, “Cryptanalytic extraction of
neural network models,” in Annual International Cryptology Conference.
Springer, 2020, pp. 189–218.
[41] F. Tramèr, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart, “Stealing
machine learning models via prediction apis,” in USENIX Security
Symposium, 2016, pp. 601–618.
[42] Y. Zhang, R. Yasaei, H. Chen, Z. Li, and M. A. Al Faruque, “Stealing
neural network structure through remote fpga side-channel analysis,”
in ACM/SIGDA International Symposium on Field-Programmable Gate
Arrays, 2021, pp. 225–225.
[43] J. Wei, Y. Zhang, Z. Zhou, Z. Li, and M. A. Al Faruque, “Leaky
dnn: Stealing deep-learning model secret with gpu context-switching
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:59:25 UTC from IEEE Xplore.  Restrictions apply. 
1170
side-channel,” in IEEE/IFIP International Conference on Dependable
Systems and Networks (DSN), 2020, pp. 125–137.
[44] V. Duddu, D. Samanta, D. V. Rao, and V. E. Balas, “Stealing neural
networks via timing side channels,” arXiv preprint arXiv:1812.11720,
2018.
[45] Y. Xiang, Z. Chen, Z. Chen, Z. Fang, H. Hao, J. Chen, Y. Liu, Z. Wu,
Q. Xuan, and X. Yang, “Open dnn box by power side-channel attack,”
IEEE Transactions on Circuits and Systems II: Express Briefs, vol. 67,
no. 11, pp. 2717–2721, 2020.
[46] Z. Zhan, Z. Zhang, S. Liang, F. Yao, and X. Koutsoukos, “Graphics
peeping unit: Exploiting em side-channel information of gpus to eaves-
drop on your neighbors,” in IEEE Symposium on Security and Privacy,
2022.
[47] R. Callan, A. Zaji´c, and M. Prvulovic, “Fase: Finding amplitude-
modulated side-channel emanations,” in IEEE Annual International
Symposium on Computer Architecture, 2015, pp. 592–603.
[48] Z. Zhang, S. Liang, F. Yao, and X. Gao, “Red alert for power leakage:
Exploiting intel rapl-induced side channels,” in ACM Asia Conference
on Computer and Communications Security, 2021, pp. 162–175.
[49] F. Yao, H. Fang, M. Doroslovaˇcki, and G. Venkataramani, “Cotsknight:
Practical defense against cache timing channel attacks using cache mon-
itoring and partitioning technologies,” in IEEE International Symposium
on Hardware Oriented Security and Trust, 2019, pp. 121–130.
[50] H. Fang, S. S. Dayapule, F. Yao, M. Doroslovaˇcki, and G. Venkatara-
mani, “Prodact: Prefetch-obfuscator to defend against cache timing
channels,” International Journal of Parallel Programming, vol. 47, no. 4,
pp. 571–594, 2019.
[51] O. Mutlu and J. S. Kim, “Rowhammer: A retrospective,” IEEE Trans-
actions on Computer-Aided Design of Integrated Circuits and Systems,
vol. 39, no. 8, pp. 1555–1571, 2019.
[52] D. Gruss, M. Lipp, M. Schwarz, D. Genkin, J. Jufﬁnger, S. O’Connell,
W. Schoechl, and Y. Yarom, “Another ﬂip in the wall of rowhammer
defenses,” in 2018 IEEE Symposium on Security and Privacy, 2018, pp.
245–261.
[53] L. Cojocar, K. Razavi, C. Giuffrida, and H. Bos, “Exploiting correcting
codes: On the effectiveness of ecc memory against rowhammer attacks,”
in IEEE Symposium on Security and Privacy, 2019, pp. 55–71.
[54] M. Seaborn and T. Dullien, “Exploiting the dram rowhammer bug to
gain kernel privileges,” Black Hat, vol. 15, p. 71, 2015.
[55] Y. Jang, J. Lee, S. Lee, and T. Kim, “Sgx-bomb: Locking down the
processor via rowhammer attack,” in Workshop on System Software for
Trusted Execution, 2017, pp. 1–6.
[56] K. Cai, M. H. I. Chowdhuryy, Z. Zhang, and F. Yao, “Seeds of seed:
Nmt-stroke: Diverting neural machine translation through hardware-
based faults,” 2021.
[57] M. Ribeiro, K. Grolinger, and M. A. Capretz, “Mlaas: Machine learning
as a service,” in IEEE International Conference on Machine Learning
and Applications, 2015, pp. 896–902.
[58] F. Yao, M. Doroslovacki, and G. Venkataramani, “Are coherence pro-
tocol states vulnerable to information leakage?” in IEEE International
Symposium on High Performance Computer Architecture, 2018, pp. 168–
179.
[59] R. K. Konoth, M. Oliverio, A. Tatar, D. Andriesse, H. Bos, C. Giuf-
frida, and K. Razavi, “Zebram: comprehensive and compatible software
protection against rowhammer attacks,” in OSDI, 2018, pp. 697–710.
[60] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, “Randaugment:
Practical automated data augmentation with a reduced search space,”
in IEEE/CVF Conference on Computer Vision and Pattern Recognition
Workshops, 2020, pp. 702–703.
[61] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le, “Au-
toaugment: Learning augmentation strategies from data,” in IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2019, pp. 113–
123.
[62] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., “Pytorch: An
imperative style, high-performance deep learning library,” Advances in
neural information processing systems, vol. 32, pp. 8026–8037, 2019.
[63] A. Tatar, C. Giuffrida, H. Bos, and K. Razavi, “Defeating software
mitigations against rowhammer: a surgical precision hammer,” in Inter-
national Symposium on Research in Attacks, Intrusions, and Defenses.
Springer, 2018, pp. 47–66.
[64] K. Razavi, B. Gras, E. Bosman, B. Preneel, C. Giuffrida, and
H. Bos, “Flip feng shui: Hammering a needle in the software
stack,” in USENIX Security Symposium, Austin, TX, Aug. 2016,
pp. 1–18.
usenixsecurity16/technical-sessions/presentation/razavi
[Online]. Available: https://www.usenix.org/conference/
[65] M. Gorman, Understanding the Linux virtual memory manager. Pren-
tice Hall Upper Saddle River, 2004.
[66] N. Papernot, P. McDaniel, and I. Goodfellow, “Transferability in ma-
chine learning: from phenomena to black-box attacks using adversarial
samples,” arXiv preprint arXiv:1605.07277, 2016.
[67] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards
deep learning models resistant to adversarial attacks,” in International
Conference on Learning Representations, 2018. [Online]. Available:
https://openreview.net/forum?id=rJzIBfZAb
[68] D. Khudia, J. Huang, P. Basu, S. Deng, H. Liu, J. Park, and M. Smelyan-
skiy, “Fbgemm: Enabling high-performance low-precision deep learning
inference,” arXiv preprint arXiv:2101.05615, 2021.
[69] M. Dukhan, Y. Wu,
“QNNPACK: Open source
library for optimized mobile deep learning.” [Online]. Available:
https://engineering.fb.com/2018/10/29/ml-applications/qnnpack/
and H. Lu,
[70] Y. Langsam, M. Augenstein, and A. M. Tenenbaum, Data Structures
using C and C++. Prentice Hall New Jersey, 1996, vol. 2.
[71] W. Cui, X. Li, J. Huang, W. Wang, S. Wang, and J. Chen, “Substitute
model generation for black-box adversarial attack based on knowledge
distillation,” in IEEE International Conference on Image Processing,
2020, pp. 648–652.
[72] H. Zhang, Y. Yu, J. Jiao, E. P. Xing, L. E. Ghaoui, and M. I. Jordan,
“Theoretically principled trade-off between robustness and accuracy,” in
International Conference on Machine Learning, 2019.
[73] H. Hassan, Y. C. Tugrul, J. S. Kim, V. Van der Veen, K. Razavi, and
O. Mutlu, “Uncovering in-dram rowhammer protection mechanisms: A
new methodology, custom rowhammer patterns, and implications,” in
IEEE International Symposium on Microarchitecture, 2021, pp. 1198–
1213.
[74] L. Orosa, A. G. Yaglikci, H. Luo, A. Olgun, J. Park, H. Hassan,
M. Patel, J. S. Kim, and O. Mutlu, “A deeper look into rowhammer’s
sensitivities: Experimental analysis of real dram chipsand implications
on future attacks and defenses,” in IEEE International Symposium on
Microarchitecture, 2021, pp. 1182–1197.
[75] “Half-Double Next-Row-Over Assisted Rowhammer.”
[Online].
Available: https://github.com/google/hammer-kit/blob/main/20210525_
half_double.pdf
[76] P. Frigo, E. Vannacc, H. Hassan, V. Van Der Veen, O. Mutlu, C. Giuf-
frida, H. Bos, and K. Razavi, “Trrespass: Exploiting the many sides of
target row refresh,” in IEEE Symposium on Security and Privacy, 2020,
pp. 747–762.
[77] W. Zhou, X. Hou, Y. Chen, M. Tang, X. Huang, X. Gan, and Y. Yang,
“Transferable adversarial perturbations,” in European Conference on
Computer Vision, 2018, pp. 452–467.
[78] C. Xie, Z. Zhang, Y. Zhou, S. Bai, J. Wang, Z. Ren, and A. L. Yuille,
“Improving transferability of adversarial examples with input diversity,”
in IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2019, pp. 2730–2739.
[79] F. Liu, C. Zhang, and H. Zhang, “Towards transferable adversarial
perturbations with minimum norm,” in ICML Workshop on Adversarial
Machine Learning, 2021.
[80] M. Salzmann et al., “Learning transferable adversarial perturbations,”
Advances in Neural Information Processing Systems, vol. 34, 2021.
[81] R. Benadjila, O. Billet, S. Gueron, and M. J. Robshaw, “The intel aes