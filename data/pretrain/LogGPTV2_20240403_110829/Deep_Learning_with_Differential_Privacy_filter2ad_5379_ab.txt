ping of this form is a popular ingredient of SGD for deep
networks for non-privacy reasons, though in that setting it
usually suﬃces to clip after averaging.
Per-layer and time-dependent parameters: The pseu-
docode for Algorithm 1 groups all the parameters into a
single input θ of the loss function L(·). For multi-layer neu-
ral networks, we consider each layer separately, which allows
setting diﬀerent clipping thresholds C and noise scales σ for
diﬀerent layers. Additionally, the clipping and noise param-
eters may vary with the number of training steps t. In results
presented in Section 5 we use constant settings for C and σ.
Lots: Like the ordinary SGD algorithm, Algorithm 1 esti-
mates the gradient of L by computing the gradient of the
loss on a group of examples and taking the average. This av-
erage provides an unbiased estimator, the variance of which
decreases quickly with the size of the group. We call such a
group a lot, to distinguish it from the computational group-
ing that is commonly called a batch. In order to limit mem-
ory consumption, we may set the batch size much smaller
than the lot size L, which is a parameter of the algorithm.
We perform the computation in batches, then group several
batches into a lot for adding noise. In practice, for eﬃciency,
the construction of batches and lots is done by randomly per-
muting the examples and then partitioning them into groups
of the appropriate sizes. For ease of analysis, however, we as-
sume that each lot is formed by independently picking each
example with probability q = L/N , where N is the size of
the input dataset.
As is common in the literature, we normalize the running
time of a training algorithm by expressing it as the number
of epochs, where each epoch is the (expected) number of
batches required to process N examples. In our notation,
an epoch consists of N/L lots.
Privacy accounting: For diﬀerentially private SGD, an
important issue is computing the overall privacy cost of the
training. The composability of diﬀerential privacy allows
us to implement an “accountant” procedure that computes
the privacy cost at each access to the training data, and
accumulates this cost as the training progresses. Each step
of training typically requires gradients at multiple layers,
and the accountant accumulates the cost that corresponds
to all of them.
(cid:113)
2 log 1.25
Moments accountant: Much research has been devoted
to studying the privacy loss for a particular noise distribu-
tion as well as the composition of privacy losses. For the
Gaussian noise that we use, if we choose σ in Algorithm 1
δ /ε, then by standard arguments [22] each
to be
step is (ε, δ)-diﬀerentially private with respect to the lot.
Since the lot itself is a random sample from the database,
the privacy ampliﬁcation theorem [35, 9] implies that each
step is (qε, qδ)-diﬀerentially private with respect to the full
database where q = L/N is the sampling ratio per lot. The
result in the literature that yields the best overall bound is
the strong composition theorem [24].
However, the strong composition theorem can be loose,
and does not take into account the particular noise distribu-
tion under consideration. In our work, we invent a stronger
√
accounting method, which we call the moments accountant.
T ), δ)-
It allows us to prove that Algorithm 1 is (O(qε
diﬀerentially private for appropriately chosen settings of the
noise scale and the clipping threshold. Compared to what
one would obtain by the strong composition theorem, our
bound is tighter in two ways: it saves a(cid:112)log(1/δ) factor in
the ε part and a T q factor in the δ part. Since we expect
δ to be small and T (cid:29) 1/q (i.e., each example is examined
multiple times), the saving provided by our bound is quite
signiﬁcant. This result is one of our main contributions.
310Theorem 1. There exist constants c1 and c2 so that given
the sampling probability q = L/N and the number of steps
T , for any ε  0 if we choose
q(cid:112)T log(1/δ)
σ ≥ c2
If we use the strong composition theorem, we will then
need to choose σ = Ω(q(cid:112)T log(1/δ) log(T /δ)/ε). Note that
we save a factor of(cid:112)log(T /δ) in our asymptotic bound. The
ε
.
moments accountant is beneﬁcial in theory, as this result
indicates, and also in practice, as can be seen from Figure 2
in Section 4. For example, with L = 0.01N , σ = 4, δ =
10−5, and T = 10000, we have ε ≈ 1.26 using the moments
accountant. As a comparison, we would get a much larger
ε ≈ 9.34 using the strong composition theorem.
3.2 The Moments Accountant: Details
The moments accountant keeps track of a bound on the
moments of the privacy loss random variable (deﬁned be-
low in Eq. (1)).
It generalizes the standard approach of
tracking (ε, δ) and using the strong composition theorem.
While such an improvement was known previously for com-
posing Gaussian mechanisms, we show that it applies also
for composing Gaussian mechanisms with random sampling
and can provide much tighter estimate of the privacy loss of
Algorithm 1.
Privacy loss is a random variable dependent on the ran-
dom noise added to the algorithm. That a mechanism M
is (ε, δ)-diﬀerentially private is equivalent to a certain tail
bound on M’s privacy loss random variable. While the tail
bound is very useful information on a distribution, compos-
ing directly from it can result in quite loose bounds. We in-
stead compute the log moments of the privacy loss random
variable, which compose linearly. We then use the moments
bound, together with the standard Markov inequality, to ob-
tain the tail bound, that is the privacy loss in the sense of
diﬀerential privacy.
More speciﬁcally, for neighboring databases d, d(cid:48) ∈ Dn, a
mechanism M, auxiliary input aux, and an outcome o ∈ R,
deﬁne the privacy loss at o as
c(o;M, aux, d, d
(cid:48)
) ∆= log
Pr[M(aux, d) = o]
Pr[M(aux, d(cid:48)) = o]
.
(1)
A common design pattern, which we use extensively in the
paper, is to update the state by sequentially applying diﬀer-
entially private mechanisms. This is an instance of adaptive
composition, which we model by letting the auxiliary input
of the kth mechanism Mk be the output of all the previous
mechanisms.
For a given mechanism M, we deﬁne the λth moment
αM(λ; aux, d, d(cid:48)) as the log of the moment generating func-
tion evaluated at the value λ:
(cid:48)
αM(λ; aux, d, d
) ∆=
log Eo∼M(aux,d)[exp(λc(o;M, aux, d, d
(cid:48)
))].
(2)
In order to prove privacy guarantees of a mechanism, it is
useful to bound all possible αM(λ; aux, d, d(cid:48)). We deﬁne
αM(λ) ∆= max
(cid:48)
aux,d,d(cid:48) αM(λ; aux, d, d
) ,
where the maximum is taken over all possible aux and all
the neighboring databases d, d(cid:48).
We state the properties of α that we use for the moments
accountant.
Theorem 2. Let αM(λ) deﬁned as above. Then
1. [Composability] Suppose that a mechanism M con-
sists of a sequence of adaptive mechanisms M1, . . . ,Mk
where Mi : (cid:81)i−1
j=1 Rj × D → Ri. Then, for any λ
αM(λ) ≤ k(cid:88)
αMi (λ) .
2. [Tail bound] For any ε > 0, the mechanism M is
(ε, δ)-diﬀerentially private for
i=1
exp(αM(λ) − λε) .
δ = min
λ
In particular, Theorem 2.1 holds when the mechanisms
themselves are chosen based on the (public) output of the
previous mechanisms.
By Theorem 2, it suﬃces to compute, or bound, αMi (λ) at
each step and sum them to bound the moments of the mech-
anism overall. We can then use the tail bound to convert the
moments bound to the (ε, δ)-diﬀerential privacy guarantee.
The main challenge that remains is to bound the value
αMt (λ) for each step. In the case of a Gaussian mechanism
with random sampling, it suﬃces to estimate the following
moments. Let µ0 denote the probability density function
(pdf) of N (0, σ2), and µ1 denote the pdf of N (1, σ2). Let µ
be the mixture of two Gaussians µ = (1 − q)µ0 + qµ1. Then
we need to compute α(λ) = log max(E1, E2) where
E1 = Ez∼µ0 [(µ0(z)/µ(z))λ] ,
E2 = Ez∼µ [(µ(z)/µ0(z))λ] .
(3)
(4)
In the implementation of the moments accountant, we
In ad-
carry out numerical integration to compute α(λ).
dition, we can show the asymptotic bound
α(λ) ≤ q2λ(λ + 1)/(1 − q)σ2 + O(q3/σ3) .
Together with Theorem 2, the above bound implies our
main Theorem 1. The details can be found in the full version
of the paper [4].
3.3 Hyperparameter Tuning
We identify characteristics of models relevant for privacy
and, speciﬁcally, hyperparameters that we can tune in order
to balance privacy, accuracy, and performance. In particu-
lar, through experiments, we observe that model accuracy
is more sensitive to training parameters such as batch size
and noise level than to the structure of a neural network.
If we try several settings for the hyperparameters, we can
trivially add up the privacy costs of all the settings, possibly
via the moments accountant. However, since we care only
about the setting that gives us the most accurate model,
we can do better, such as applying a version of a result
from Gupta et al. [29] (see the full version of the paper for
details [4]).
We can use insights from theory to reduce the number of
hyperparameter settings that need to be tried. While diﬀer-
entially private optimization of convex objective functions
is best achieved using batch sizes as small as 1, non-convex
learning, which is inherently less stable, beneﬁts from ag-
gregation into larger batches. At the same time, Theorem 1
311suggests that making batches too large increases the pri-
vacy cost, and a reasonable tradeoﬀ is to take the number
of batches per epoch to be of the same order as the desired
number of epochs. The learning rate in non-private train-
ing is commonly adjusted downwards carefully as the model
converges to a local optimum. In contrast, we never need
to decrease the learning rate to a very small value, because
diﬀerentially private training never reaches a regime where
it would be justiﬁed. On the other hand, in our experi-
ments, we do ﬁnd that there is a small beneﬁt to starting
with a relatively large learning rate, then linearly decaying
it to a smaller value in a few epochs, and keeping it constant
afterwards.
4.
IMPLEMENTATION
We have implemented the diﬀerentially private SGD al-
gorithms in TensorFlow. For privacy protection, we need
to “sanitize” the gradient before using it to update the pa-
rameters. In addition, we need to keep track of the “privacy
spending” based on how the sanitization is done. Hence our
implementation mainly consists of two components: sani-
tizer, which preprocesses the gradient to protect privacy,
and privacy_accountant, which keeps track of the privacy
spending over the course of training.
Figure 1 contains the TensorFlow code snippet (in Python)
of DPSGD_Optimizer, which minimizes a loss function us-
ing a diﬀerentially private SGD, and DPTrain, which itera-
tively invokes DPSGD_Optimizer using a privacy accountant
to bound the total privacy loss.
In many cases, the neural network model may beneﬁt from
the processing of the input by projecting it on the principal
directions (PCA) or by feeding it through a convolutional
layer. We implement diﬀerentially private PCA and apply
pre-trained convolutional layers (learned on public data).
Sanitizer. In order to achieve privacy protection, the sani-
tizer needs to perform two operations: (1) limit the sensitiv-
ity of each individual example by clipping the norm of the
gradient for each example; and (2) add noise to the gradient
of a batch before updating the network parameters.
performance reasons, yielding gB = 1/|B|(cid:80)
In TensorFlow, the gradient computation is batched for
x∈B ∇θL(θ, x)
for a batch B of training examples. To limit the sensitivity
of updates, we need to access each individual ∇θL(θ, x). To
this end, we implemented per_example_gradient operator
in TensorFlow, as described by Goodfellow [27]. This opera-
tor can compute a batch of individual ∇θL(θ, x). With this
implementation there is only a modest slowdown in train-
ing, even for larger batch size. Our current implementation
supports batched computation for the loss function L, where
each xi is singly connected to L, allowing us to handle most
hidden layers but not, for example, convolutional layers.
Once we have the access to the per-example gradient, it
is easy to use TensorFlow operators to clip its norm and to
add noise.
Privacy accountant. The main component in our imple-
mentation is PrivacyAccountant which keeps track of pri-
vacy spending over the course of training. As discussed in
Section 3, we implemented the moments accountant that ad-
ditively accumulates the log of the moments of the privacy
loss at each step. Dependent on the noise distribution, one
can compute α(λ) by either applying an asymptotic bound,
evaluating a closed-form expression, or applying numerical
class DPSGD_Optimizer():
def __init__(self, accountant, sanitizer):
self._accountant = accountant
self._sanitizer = sanitizer
def Minimize(self, loss, params,
batch_size, noise_options):
# Accumulate privacy spending before computing
# and using the gradients.
priv_accum_op =
self._accountant.AccumulatePrivacySpending(
batch_size, noise_options)
with tf.control_dependencies(priv_accum_op):
# Compute per example gradients
px_grads = per_example_gradients(loss, params)
# Sanitize gradients
sanitized_grads = self._sanitizer.Sanitize(
px_grads, noise_options)
# Take a gradient descent step
return apply_gradients(params, sanitized_grads)
def DPTrain(loss, params, batch_size, noise_options):
accountant = PrivacyAccountant()
sanitizer = Sanitizer()
dp_opt = DPSGD_Optimizer(accountant, sanitizer)
sgd_op = dp_opt.Minimize(
loss, params, batch_size, noise_options)
eps, delta = (0, 0)
# Carry out the training as long as the privacy
# is within the pre-set limit.
while within_limit(eps, delta):
sgd_op.run()
eps, delta = accountant.GetSpentPrivacy()
Figure 1: Code snippet of DPSGD_Optimizer and DP-
Train.
integration. The ﬁrst option would recover the generic ad-
vanced composition theorem, and the latter two give a more
accurate accounting of the privacy loss.
For the Gaussian mechanism we use, α(λ) is deﬁned ac-