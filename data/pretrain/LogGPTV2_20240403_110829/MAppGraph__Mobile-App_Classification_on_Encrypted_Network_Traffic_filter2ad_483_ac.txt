### Figure 5: Preparation of Training and Testing Datasets

In our approach, we utilized only node features, thereby excluding the correlation among nodes from the models. The Multi-Layer Perceptron (MLP) architecture consists of three hidden layers, each with 1024 nodes. A batch normalization layer is added after each dense layer to normalize the data before it is fed into the next dense layer. We employed the ReLU activation function for the MLP.

### 5.2.2 AppScanner

AppScanner [40] is a flow-based mobile app classification technique that analyzes 50 minutes of mobile app traffic to extract flow features. These features are then used to train machine learning models, such as Random Forest and Support Vector Machine, to classify traffic flows of mobile apps. The authors extracted 40 traffic features, which we also used in our work, as detailed in Table 1. For a fair comparison, we used the source code of AppScanner, downloaded from its official website [2], and ran it with our collected data. As shown in Figure 5, each AppScanner model requires 50 minutes of traffic for each app. Each app in our experiments has at least 16 traffic chunks, each 50 minutes long. Therefore, we evaluated the performance using 16 AppScanner models. In addition to presenting the performance of each individual model, we implemented a naive voting scheme where the class predicted by the majority of the 16 models is considered the final prediction for the app. This enhanced version is referred to as Enhanced AppScanner in our experiments.

### 5.2.3 FlowPrint

FlowPrint [44] classifies or detects mobile apps by considering cross-correlation. It uses traffic collected over a time window (e.g., 5 minutes) and defines an app fingerprint as "the set of network destinations that form a maximal clique in the correlation graph." The Jaccard similarity [17] is used to compare the similarity between two fingerprints. If the similarity exceeds a predefined threshold, the fingerprints are considered to belong to the same app. Similar to AppScanner, FlowPrint uses only 5 minutes of traffic capture to create fingerprints for each app. In our experiments, with more than 30 hours of traffic, we created up to 544 traffic chunks, each 5 minutes long. As shown in Figure 5, we used a naive voting scheme to determine the mobile app to which a test traffic sample belongs. The obtained fingerprints are compared with all pre-computed fingerprints from the 544 traffic chunks. The mobile app with the highest number of similar pre-computed fingerprints is the final prediction. We used the source code of FlowPrint, downloaded from the authors' website [3], to run the experiments. This enhanced version is referred to as Enhanced FlowPrint in our experiments.

### Hyper-parameters and Training Setup

For the hyper-parameters of the MLP and MAppGraph, we trained the models for 150 epochs. The initial learning rate was set to \(10^{-4}\) with a decay of 0.9 after every 10 training epochs. The MAppGraph model can be trained offline, and in practical deployment, a pre-trained model is used in production while another model is trained in parallel to reflect changes in mobile app behavior (e.g., version upgrades or attacks). Advanced training methods, such as incremental learning [7], can also be applied to reduce the training time when new data is collected. To ensure reproducibility, we conducted each experiment over multiple random-seeded runs. The experiments were carried out on a customized desktop with an AMD Ryzen Threadripper 2950X 16-core processor @ 3.5GHz, 64 GB of RAM, and 2 Nvidia GeForce RTX 2080Ti GPUs, each with 11 GB of memory.

### 5.3 Analysis of Results

#### 5.3.1 Overall Performance Comparison

We now present the performance comparison of MAppGraph with MLP, AppScanner, and FlowPrint. Table 2 shows the performance of all techniques in terms of Precision, Recall, F1-Score, and Accuracy on our dataset. The results indicate that MAppGraph outperforms the other methods. Compared to the worst performance results produced by Enhanced AppScanner, MAppGraph significantly improves performance across all metrics by up to 20%. Our experiments also confirm that FlowPrint outperforms AppScanner, as discussed in [44]. Interestingly, MLP performs better than Enhanced AppScanner, demonstrating that flow-based detection or classification of mobile apps is not always effective, especially since many apps share the same third-party services, making their traffic flows indistinguishable. Even though MLP does not process graphs, the way we select the node features implicitly accounts for the communication correlation of the mobile app and various third-party services.

Table 2: Overall Performance Comparison
| Technique | Precision | Recall | F1-Score | Accuracy |
|-----------|-----------|--------|----------|----------|
| MLP       | 0.9075    | 0.8759 | 0.9075   | 0.8275   |
| Enhanced  | 0.9081    | 0.9364 | 0.9074   | 0.9347   |
| AppScanner| 0.7938    | 0.9346 | 0.7938   | 0.9346   |
| Enhanced  | 0.8634    | 0.9346 | 0.8341   | 0.9346   |
| FlowPrint | 0.8634    | 0.9346 | 0.8341   | 0.9346   |

Table 3: AppScanner Performance with Individual Models
| Model No. | Precision | Recall | F1-Score | Accuracy |
|-----------|-----------|--------|----------|----------|
| 1         | 0.6660    | 0.7938 | 0.6660   | 0.7938   |
| 2         | 0.6757    | 0.7476 | 0.6757   | 0.7468   |
| 3         | 0.6577    | 0.7791 | 0.6577   | 0.7460   |
| 4         | 0.6493    | 0.7779 | 0.6493   | 0.7358   |
| 5         | 0.6485    | 0.7281 | 0.6485   | 0.7354   |
| 6         | 0.6515    | 0.7322 | 0.6515   | 0.7590   |
| 7         | 0.6515    | 0.7666 | 0.6515   | 0.7306   |
| 8         | 0.6527    | 0.7873 | 0.6527   | 0.7618   |
| 9         | 0.6563    | 0.7420 | 0.6563   | 0.7420   |
| 10        | 0.6415    | 0.7592 | 0.6415   | 0.7522   |
| 11        | 0.6346    | 0.7522 | 0.6346   | 0.7522   |
| 12        | 0.6633    | 0.7522 | 0.6633   | 0.7522   |
| 13        | 0.6541    | 0.7522 | 0.6541   | 0.7522   |
| 14        | 0.0101    | 0.7522 | 0.0101   | 0.7522   |
| 15        | 0.7938    | 0.7522 | 0.7938   | 0.7522   |
| 16        | 0.0187    | 0.7522 | 0.0187   | 0.7522   |
| Mean      | 0.6541    | 0.7522 | 0.6541   | 0.7522   |
| Stan. Dev.| 0.0101    | 0.0187 | 0.0101   | 0.0187   |

#### 5.3.2 Impact of Number of Graph Nodes Used to Train Models

As discussed in Section 3, the pooling layers perform truncation or extension of the graph latent representation to a predefined size (k). This corresponds to the number of graph nodes (N) whose features are used to train the model in the case of MLP (i.e., k is a multiple of N). The rationale behind this experiment is that with a high value of N, graphs with fewer nodes must use zero padding, which may mislead the learning. Conversely, using a small number of nodes may result in the loss of useful information, affecting model performance. Figure 7 shows the histogram of the number of nodes in the graphs of our dataset with a time window of 5 minutes. Most graphs have around 10 nodes, with 90% having fewer than 35 nodes and 86% having fewer than 30 nodes. Figure 8 presents the performance of the models with respect to four values of N. As expected, performance degrades with fewer nodes and increases to an optimal value of N before decreasing again with a large number of nodes. The optimal value of N differs between MLP and MAppGraph, likely because MAppGraph requires more information about the graph topology. In all experimental scenarios, MAppGraph outperformed MLP. FlowPrint considers the entire graph for determining app fingerprints, so it is not included in this experiment.

#### 5.3.3 Impact of Time Window Duration for Traffic Collection

In this experiment, we evaluate the impact of the time window (Twindow) duration required for traffic capture to construct the communication graphs of mobile apps. In the previous experiments, we used a 5-minute time window. However, shorter traffic capture durations would be beneficial in terms of lower computational resources and faster reaction times in case of security breaches. Table 4 shows the performance of the proposed technique with respect to the duration of the time window required to capture traffic.

As expected, the performance of all techniques decreases with a shorter traffic capture window. Reducing the traffic capture from 5 minutes to 1 minute decreases MAppGraph's performance by 7%. The gain in reducing the traffic capture duration (e.g., faster app classification, less storage, and computational resources) outweighs the performance loss. Network operators can configure this parameter based on their desired performance and objectives. The trends in performance among the techniques remain consistent, with MAppGraph performing the best, followed by MLP and Enhanced FlowPrint.

It is worth noting that for short time windows (Twindow â‰¤ 2), we do not apply overlapping. This is because short capture durations generate sufficient data for training and testing, and it is fast enough for detection. For longer capture durations (e.g., 5 minutes), overlapping is applied.

Table 4: Impact of Time Window (in minutes) of Traffic Collection on Performance of Classification Models
| Twindow | 5 min | 4 min | 3 min | 2 min | 1 min |
|---------|-------|-------|-------|-------|-------|
| Technique | Precision | Recall | Precision | Recall | Precision | Recall | Precision | Recall | Precision | Recall |
| MLP | 0.9075 | 0.8759 | 0.8894 | 0.8671 | 0.8625 | 0.8175 | 0.7945 | 0.7522 | 0.7522 | 0.7522 |
| Enhanced AppScanner | 0.9081 | 0.9364 | 0.8296 | 0.8932 | 0.8175 | 0.8634 | 0.7522 | 0.7522 | 0.7522 | 0.7522 |
| Enhanced FlowPrint | 0.8634 | 0.9346 | 0.9174 | 0.9346 | 0.8932 | 0.9346 | 0.8634 | 0.9346 | 0.8634 | 0.9346 |
| MAppGraph | 0.8634 | 0.9346 | 0.8634 | 0.9346 | 0.8634 | 0.9346 | 0.8634 | 0.9346 | 0.8634 | 0.9346 |

This comprehensive analysis demonstrates the effectiveness of MAppGraph in mobile app classification, even under varying conditions and constraints.