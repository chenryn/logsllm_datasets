Figure 5: Preparation of Training and Testing Datasets.
As we used only node features, the correlation among nodes is not
used in the models. The architecture of MLP consists of 3 hidden
layers, each having 1024 hidden nodes. After each dense layer, a
batch normalization layer is added to normalize data before feeding
to the next dense layer. We used ReLU for MLP.
5.2.2 AppScanner. AppScanner [40] is a flow-based mobile app
classification technique, in which a capture of 50 minutes of mobile
app traffic is analyzed and flow features are extracted to train a
machine learning model (Random Forest and Support Vector Ma-
chine) to classify traffic flows of mobile apps. The authors extracted
40 traffic features, which are also used in our work presented in
Table 1. For a fair comparison, we faithfully used the source codes
of AppScanner downloaded from its website2 and run with our
collected data. As shown in Figure 5, each AppScanner model re-
quires 50 minutes of traffic for each app. For all the apps used in
our experiments, every app has at least 16 traffic chunks with a
duration of 50 minutes. Thus, we considered 16 AppScanner models
to evaluate the performance. Besides presenting the performance
of each individual model, we used a naive voting scheme to use
all the 16 models for classification. Given a mobile app, the class
predicted by the highest number of models is considered as the
final prediction of the app. We denote this technique as Enhanced
AppScanner in the experiments.
FlowPrint. FlowPrint [44] considers the cross-correlation to
5.2.3
classify or detect mobile apps. With the traffic collected in a time
window (e.g., 5 minutes), instead of using machine learning, Flow-
Print defines app fingerprint as “the set of network destinations
that form a maximal clique in the correlation graph”. Given two
fingerprints, the authors use the Jaccard similarity [17] to com-
pare the similarity between them. If the similarity is larger than
a predefined threshold then the two fingerprints are considered
to belong to the same app. Similar to AppScanner, FlowPrint uses
only 5 minutes of traffic capture to create fingerprints for each
app. In our experiments, with more than 30 hours of traffic, we can
create up to 544 traffic chunks of 5 minutes for each. As shown in
Figure 5, we use a naive voting scheme to determine which mobile
app a test traffic sample belongs to. Given a test traffic sample, the
2AppScanner: https://github.com/Thijsvanede/AppScanner
315......4265 minutesoverlapTest Set (MAppGraph, FlowPrint, AppScanner)Train Set (MAppGraph, FlowPrint)Train Set (AppScanner)......Timeline50 minutes......N samplesM samplesN models...Test1 model...P samplesP models...FlowPrintMAppGraph/MLPAppScanner......VotingVotingTrain1031ACSAC ’21, December 6–10, 2021, Virtual Event, USA
T.-D. Pham et al.
obtained fingerprints will be compared with all the pre-computed
fingerprints obtained from the 544 traffic chunks. The mobile app
that has the highest number of pre-computed fingerprints similar
to the test fingerprints will be the final prediction. We faithfully
used the source codes of FlowPrint downloaded from the website
of its authors3 to run the experiments. We also denoted the adopted
FlowPrint as Enhanced FlowPrint in the experiments.
For the hyper-parameters of MLP and MAppGraph, we trained
the models with 150 training epochs. The initial learning rate is
10−4 with a decay of 0.9 after every 10 training epochs. Training the
MAppGraph model can be done in an offline manner. In a practical
deployment, a pre-trained model is used in the production while
another model is trained in parallel to reflect any changes in mobile
app behavior (e.g., version upgrading or being attacked). Advanced
training methods such as incremental learning [7] can also be ap-
plied to reduce the training time of the model when new data is
collected. To ensure reproducibility, we conducted each experiment
over multiple random-seeded runs. The experiments were carried
out on a customized desktop with AMD Ryzen Threadripper 2950X
16-core processor @ 3.5GHz, 64 GB of RAM and 2 Nvidia GeForce
RTX 2080Ti GPUs, each having 11 GB of memory.
5.3 Analysis of Results
5.3.1 Overall Performance Comparison. We now present the per-
formance comparison of MAppGraph with MLP, AppScanner and
FlowPrint. In Table 2, we present the performance of all the tech-
niques in terms of Precision, Recall, F1-Score and Accuracy on our
dataset. The results show that MAppGraph has the best perfor-
mance. Compared to the worst performance results produced by
Enhanced AppScanner, MAppGraph significantly improves the per-
formance in all the metrics by up to 20%. Our experiments also con-
firm the fact that FlowPrint outperforms AppScanner as discussed
in [44]. Interestingly, MLP has a better performance compared to
Enhanced AppScanner. This demonstrates that using flow-based
detection or classification of mobile apps is not an appropriate ap-
proach as most of the apps nowadays share the same third-party
services. This makes traffic flows between the mobile apps and
the servers of the third-party services have similar behavior, thus
being indistinguishable among the apps. Even though MLP does
not process graphs with nodes and edges among nodes, the way we
select the graph nodes whose features are used to train the model
implicitly takes into account the communication correlation of the
mobile app and various third-party services used by the app.
It is interesting to show that the result obtained with Enhanced
AppScanner based on the voting scheme is much better compared
to the performance of individual models. Table 3 presents the per-
formance of AppScanner obtained with individual models. The
experimental results show that individual models of AppScanner
have stable performance (i.e., testified by the low standard devia-
tion of all performance metrics) throughout different traffic chunks
used to train the models. Enhanced AppScanner significantly im-
proves the performance by up to 25% and 20% compared to the
worst and best individual models, respectively. This demonstrates
the diversity of traffic behavior of mobile apps when users use
3FlowPrint: https://github.com/Thijsvanede/FlowPrint
Table 2: Overall Performance Comparison
Technique
MLP
Enhanced
AppScanner
Enhanced
FlowPrint
MAppGraph
Precision Recall
0.9075
0.7938
0.9081
0.8634
F1-Score Accuracy
0.9075
0.7938
0.9074
0.7828
0.8759
0.9364
0.8341
0.9346
0.8275
0.9347
0.8341
0.9346
Table 3: AppScanner Performance with Individual Models
Model No.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
Mean
Stan. Dev.
Enhanced
AppScanner
Precision Recall
0.6660
0.6483
0.6757
0.6468
0.6577
0.6652
0.6493
0.6559
0.6485
0.6515
0.6515
0.6527
0.6563
0.6415
0.6346
0.6633
0.6541
0.0101
0.7938
0.7527
0.7476
0.7791
0.7405
0.7779
0.7358
0.7281
0.7354
0.7322
0.7590
0.7666
0.7306
0.7873
0.7618
0.7420
0.7592
0.7522
0.0187
0.8634
F1-Score Accuracy
0.6660
0.6483
0.6757
0.6468
0.6577
0.6652
0.6493
0.6559
0.6485
0.6515
0.6515
0.6527
0.6563
0.6415
0.6346
0.6633
0.6541
0.0101
0.7938
0.6468
0.6304
0.6556
0.6299
0.6460
0.6479
0.6308
0.6398
0.6323
0.6381
0.6418
0.6419
0.6571
0.6357
0.6258
0.6534
0.6408
0.0097
0.7828
various functionalities, which pose challenges for the detection and
classification techniques.
Compared to Enhanced FlowPrint which also considers the cross-
correlation among app servers and third-party services by construct-
ing communication graphs, MAppGraph improves the performance
by up to 7%. This improvement is a result of the combination of
advanced deep learning techniques and consideration of the di-
verse behavior of mobile apps. On one hand, using DGCNN (with
multiple graph convolution layers) allows the classification model
to learn the communication behavior of mobile apps better from
the graph topology and node attributes. On the other hand, MApp-
Graph takes into account the diversity of mobile app behavior by
training a single DGCNN model on multiple graphs. This is an
advantage of our technique compared to FlowPrint, which has to
compare the fingerprints obtained from a test traffic sample with
all pre-computed fingerprints (there are at least 544 × 101 finger-
prints computed by FlowPrint in our experiments). This fingerprint
comparison technique is not practical as there is a large number
of mobile installed by users in reality. Nevertheless, in Figure 6,
we present the performance of Enhanced FlowPrint with respect
to the number of traffic chunks (of 5 minutes) of each app used
in the voting scheme. The results show that the performance im-
provement is significant when we increase the number of traffic
chunks of each app used for inference from 1 to 20. However, using
1032MAppGraph: Mobile-App Classification on Encrypted Network Traffic using Deep Graph Convolution Neural Networks ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Figure 6: Performance of Enhanced FlowPrint w.r.t Number
of Traffic Chunks of Each App Used for Classification.
more than 20 traffic chunks of each app only results in a slight per-
formance improvement that may not be worth compensating for
the inference time/cost. It is worth mentioning that inference with
MAppGraph is performed by simply applying linear operations
(i.e., matrix multiplication) of graphs (i.e., node attributes and edge
weights) with the parameters of a single DGCNN model.
Impact of Number of Graph Nodes used to Train Models. As
5.3.2
discussed in Section 3, the pooling layers perform truncation or
extension of the graph latent representation to predefined size (k).
This implicitly corresponds to the number of graph nodes (denoted
as N ) whose features are used to train the model in case of MLP
(i.e., k is a multiple of N ). The rationale behind this experiment is
that with a high value of N , all the graphs that have fewer nodes
must use zero paddings for feature vectors (in the case of MLP) or
the latent representation vectors (in case of MAppGraph). These
zero-valued features may mislead the learning of the models. On
the other hand, if we use a small number of nodes, we may lose
useful information from the nodes that are discarded, thus affecting
the performance of the models as well. In Figure 7, we present the
histogram of the number of nodes in the graphs of our dataset with
Twindow set to 5 minutes. The histogram shows that most of the
graphs have around 10 nodes. 90% of graphs have fewer than 35
nodes and 86% of graphs have fewer than 30 nodes. In Figure 8, we
present the performance of the models with respect to four values of
N . We obtained expected results that performance degrades when
fewer nodes are used. The performance increases to an optimum
value of N before decreasing again when a large number of nodes
are used. It is interesting mentioning that the optimal value of
N is different between MLP and MAppGraph. The reason could
be the fact that MAppGraph needs more information about the
graph topology to differentiate mobile apps. Nevertheless, in all the
experimental scenarios, we observed that MAppGraph has a better
performance compared to MLP. We note that FlowPrint considers
the entire graphs for determining app fingerprints. Thus, we do not
present FlowPrint in this experiment.
5.3.3
Impact of Time Window Duration of Traffic Collection for
Graph Construction. In this experiment, we evaluate the impact of
the time window (Twindow) duration needed for traffic capture to
construct the communication graphs of mobile apps. In the experi-
ments presented above, we used a time window of 5 minutes for
traffic capture. However, it would be better if the model can classify
the apps with shorter traffic capture, leading to better benefits such
Figure 7: Histogram of Number of Nodes in Graphs.
(a) Precision.
(b) Recall.
(c) F1-Score.
(d) Accuracy.
Figure 8: Performance of MLP and MAppGraph w.r.t. Num-
ber of Nodes in Graphs.
as lower computational resources required, a quick reaction in case
of security breaches. In Table 4, we present the performance of
the proposed technique with respect to the duration of the time
window required to capture traffic.
As expected, the performance of all the techniques decreases
when we use a shorter traffic capture window. The results show that
when we reduce the traffic capture from 5 minutes to 1 minute, the
performance of MAppGraph decreases by 7%. We believe that the
gain obtained when reducing the traffic capture duration (e.g., faster
app classification and detection, less storage and computational
resources required) is more significant compared to the loss in
performance. In practice, this parameter can be configured by the
network operators based on their desired performance and objective.
Nevertheless, the trends of performance among the techniques do
not change such that the proposed technique (MAppGraph) always
performs the best followed by MLP and Enhanced FlowPrint.
It is to be noted that when the time window duration is short (the
cases of Twindow ⩽ 2), we do not apply overlapping. The rational
behind is twofold. First, with short capture duration, we managed
to generate sufficient data to split into train and test for perfor-
mance evaluation. Second, it is practically fast enough to detect
the applications. In case where the capture duration is long (e.g., 5
 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0 10 20 30 40 50 60 70 80 90 100PerformanceNumber of Traffic ChunksPrecisionRecallF1-Score020406080100Number of Nodes01000200030004000Number of Graphs 0.88 0.89 0.9 0.91 0.92 0.93 0.947102030PerformanceNumber of NodesMLPDGCNN 0.88 0.89 0.9 0.91 0.92 0.93 0.947102030PerformanceNumber of NodesMLPDGCNN 0.88 0.89 0.9 0.91 0.92 0.93 0.947102030PerformanceNumber of NodesMLPDGCNN 0.88 0.89 0.9 0.91 0.92 0.93 0.947102030PerformanceNumber of NodesMLPDGCNN1033ACSAC ’21, December 6–10, 2021, Virtual Event, USA
T.-D. Pham et al.
Table 4: Impact of Time Window (in minutes) of Traffic Col-
lection on Performance of Classification Models
Twindow
5
4
3
2
1
Technique Precision Recall
MLP
0.9075
Enhanced
0.8341
FlowPrint
0.9346
MAppGraph
MLP
0.8894
Enhanced
0.8296
FlowPrint
0.9174
MAppGraph
0.8671
MLP
Enhanced
0.8175
FlowPrint
0.8932
MAppGraph
0.8625
MLP
Enhanced
0.7945