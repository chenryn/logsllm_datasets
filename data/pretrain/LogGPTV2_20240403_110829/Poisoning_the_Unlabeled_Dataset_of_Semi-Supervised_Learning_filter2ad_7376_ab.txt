We now introduce our semi-supervised poisoning attack,
which directly exploits the self-supervised [38, 50] nature
of semi-supervised learning that is fundamental to all state-
of-the-art techniques. Many machine learning vulnerabilities
are attributed to the fact that, instead of specifying how a task
should be completed (e.g., look for three intersecting line seg-
ments), machine learning speciﬁes what should be done (e.g.,
here are several examples of triangles)—and then we hope
that the model solves the problem in a reasonable manner.
However, machine learning models often “cheat”, solving the
task through unintended means [16, 21].
Our attacks show this under-speciﬁcation problem is ex-
acerbated with semi-supervised learning: now, on top of not
specifying how a task should be solved, we do not even com-
pletely specify what should be done. When we provide the
model with unlabeled examples (e.g., here are a bunch of
shapes), we allow it to teach itself from this unlabeled data—
and hope it teaches itself to solve the correct problem.
Because our attacks target the underlying principle behind
semi-supervised machine learning, they are general across
techniques and are not speciﬁc to any one particular algorithm.
USENIX Association
30th USENIX Security Symposium    1579
(a) A classiﬁer trained on a semi-
supervised dataset of red (cid:12)s, blue ×s,
and unlabeled ⊗s. During training the
unlabeled ⊗s are given pseudo-labels
such that the correct original decision
boundary is learned.
(b) When inserting just one new unla-
beled poisoned example near the bound-
ary, the model gives it the correct pseudo
label of the blue ×s. The poisoning at-
tempt fails, and the decision boundary
remains largely unchanged.
(c) By inserting a path of unlabeled ex-
amples, the classiﬁer assigns every ex-
ample in the path the pseudo-label of
the nearby red (cid:12)s. This moves the deci-
sion boundary to enclose the path, which
makes these examples misclassiﬁed.
Figure 1: Decision boundary plot for semi-supervised learning during (a) normal training, (b) failed poisoning, and (c) our attack.
Interpolation Consistency Poisoning
3.1
Approach. Our attack, Interpolation Consistency Poison-
ing, exploits the above intuition. Given the target image x∗,
we begin by inserting it into the (unlabeled) portion of the
training data. However, because we are unable to directly
attach a label to this example, we will cause the model itself
to mislabel this example. Following typical assumptions that
the adversary has (at least partial) knowledge of the training
data [51, 71]4, we select any example in the labeled dataset x(cid:48)
that is correctly classiﬁed as the desired target label y∗, i.e.,
c(x(cid:48)) = y∗. Then, we insert N points
{xαi}N−1
i=0 = interp(x(cid:48),x∗,αi)
where the interp function is smooth along αi ∈ [0,1] and
interp(x(cid:48),x∗,1) = x∗.
interp(x(cid:48),x∗,0) = x(cid:48)
This essentially connects the sample x(cid:48) to the sample x∗, sim-
ilar to the “bridge” examples from unsupervised clustering
attacks [7]. Figure 1 illustrates the intuition behind this attack.
This attack relies on the reason that semi-supervised ma-
chine learning is able to be so effective [3, 53]. Initially, only
a small number of examples are labeled. During the ﬁrst few
epochs of training, the model begins to classify these labeled
examples correctly: all semi-supervised training techniques
include a standard fully-supervised training loss on the la-
beled examples [42].
As the conﬁdence on the labeled examples grows, the
neural network will also begin to assign the correct label
4This is not a completely unrealistic assumption. An adversary might ob-
tain this knowledge through a traditional information disclosure vulnerability,
a membership inference attack [52], or a training data extraction attack [9].
to any point nearby these examples. There are two reasons
that this happens: First, it turns out that neural networks are
Lipschitz-continuous with a low constant (on average). Thus,
if f (x) = y then “usually” we will have small ε perturba-
tions f (x + ε) = y + δ for some small (cid:107)δ(cid:107). 5 Second, because
models apply data augmentation, they are already trained on
perturbed inputs x + ε generated by adding noise to x; this
reinforces the low average-case Lipschitz constant.
As a result, any nearby unlabeled examples (that is, ex-
amples where (cid:107)xu − x(cid:107) is small) will now also begin to be
classiﬁed correctly with high conﬁdence. After the conﬁdence
assigned to these nearby unlabeled examples becomes sufﬁ-
ciently large, the training algorithms begins to treat these as
if they were labeled examples, too. Depending on the training
technique, the exact method by which the example become
“labeled” changes. UDA [66], for example, explicitly sets a
conﬁdence threshold at 0.95 after which an unlabeled exam-
ple is treated as if it were labeled. MixMatch [3], in contrast,
performs a more complicated “label-sharpening” procedure
that has a similar effect—although the method is different.
The model then begins to train on this unlabeled example, and
the process begins to repeat itself.
When we poison the unlabeled dataset, this process hap-
pens in a much more controlled manner. Because there is now
a path between the source example x(cid:48) and the target exam-
ple x∗, and because that path begins at a labeled point, the
model will assign the ﬁrst unlabeled example xα0 = x(cid:48) the
label y∗—its true and correct label. As in the begnign setting,
the model will progressively assign higher conﬁdence for
5Note that this is true despite the existence of adversarial examples [4,
57], which show that the worst-case perturbations can change classiﬁcation
signiﬁcantly. Indeed, the fact that adversarial examples were considered
“surprising” is exactly due to this intuition.
1580    30th USENIX Security Symposium
USENIX Association
this label on this example. Then, the semi-supervised learn-
ing algorithms will encourage nearby samples (in particular,
xα1) to be assigned the same label y∗ as the label given to
this ﬁrst point xα0. This process then repeats. The model as-
signs higher and higher likelihood to the example xα1 to be
classiﬁed as y∗, which then encourages xα2 to become clas-
siﬁed as y∗ as well. Eventually all injected examples {xαi}
will be labeled the same way, as y∗. This implies that ﬁnally,
f (xα0) = f (xN−1) = f (x∗) = y∗ will be as well, completing
the poisoning attack.
Interpolation Strategy.
It remains for us to instantiate the
function interp. To begin we use the simplest strategy: linear
pixel-wise blending between the original example x(cid:48) and the
target example x∗. That is, we deﬁne
interp(x(cid:48),x∗,α) = x(cid:48) · (1− α) + x∗ · α.
This satisﬁes the constraints deﬁned earlier: it is smooth, and
the boundary conditions hold. In Section 4.2 we will construct
far more sophisticated interpolation strategies (e.g., using a
GAN [17] to generate semantically meaningful interpola-
tions); for the remainder of this section we demonstrate the
utility of a simpler strategy.
Density of poisoned samples. The ﬁnal detail left to spec-
ify is how we choose the values of αi. The boundary con-
ditions α0 = 0 and αN−1 = 1 are ﬁxed, but how should we
interpolate between these two extremes? The simplest strat-
egy would be to sample completely linearly within the range
[0,1], and set αi = i/N. This choice, though, is completely
arbitrary; we now deﬁne what it would look like to provide
different interpolation methods that might allow for an attack
to succeed more often.
Each method we consider works by ﬁrst choosing a density
functions ρ(x) that determines the sampling rate. Given a
density function, we ﬁrst normalize it
(cid:19)−1
ˆρ(x) = ρ(x)·
ρ(x)dx
(cid:18)(cid:90) 1
0
(cid:90) q
p
and then sample from it so that we sample α according to
Pr[p 80%
attack success rate found above).
Figure 2 gives the attack success rate broken down by the
target image’s true (original) label, and the desired poison
Figure 3: Poisoning attack success rate for all 40× 40 source-
target pairs; six (uniformly spaced) example images are shown
on each axis. Each cell represents a single run of FixMatch
poisoning that source-target pair, and its color indicates if the
attack succeeded (yellow) or failed (purple). The rows and
columns are sorted by average attack success rate.
label. Some desired label such as “bird” or “cat” succeed in
85% of cases, compared to the most difﬁcult label of “horse”
that succeeds in 25% of cases.
Perhaps more interesting than the aggregate statistics is
considering the success rate on an image-by-image basis (see
Figure 3). Some images (e.g., in the ﬁrst column) can rarely
be successfully poisoned to reach the desired label, while
other images (e.g., in the last column) are easily poisoned.
Similarly, some source images (e.g., the last row) can poison
almost any image, but other images (e.g., the top row) are poor
sources. Despite several attempts, we have no explanation for
why some images are easier or harder to attack.
3.2.4 Evaluation across training techniques
The above attack shows that it is possible to poison FixMatch
on CIFAR-10 by poisoning 0.1% of the training dataset. We
now broaden our argument by evaluating the attack success
rate across seven different training techniques–but again for
just CIFAR-10. As stated earlier, in all cases the poisoned
models retain their original test accuracy compared to the