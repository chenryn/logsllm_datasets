only select the hosts that send responses to all 7 vantage points, meaning they
did not drop spoofed packets on any of the exercised network paths.
Next, we describe how we infer if a given reﬂector blocks an IP using multiple
trials. We deﬁne a trial as a single experiment that tests if a reﬂector blocks
one blocklist IP. Figure 6 shows the process of one trial. For each trial, the
measurement machine sends ﬁve consecutive probe packets to the reﬂector, with
each packet being sent one second apart. In our experiment, the probe packets
are TCP SYN-ACK packets and we get IP IDs from response RST packets.
Between the third and fourth probe packets, the measurement machine sends
ﬁve spoofed packets, also TCP SYN-ACK, with source IPs equal to the blocklist
IP. And between the fourth and the ﬁfth probe packets, it sends another ﬁve
spoofed packets. We send the ﬁve spoofed packets 0.15 s apart consecutively
each time, spreading them across the one-second window between two probes.
Measurement
Machine
  IP ID Probe   
1 second apart
  Spoofed Packets x 5  
Fig. 6. Blocking inference methodology.
Solid blue lines are probe packets, dashed
red lines are spoofed packets. (Color
ﬁgure online)
Fig. 7. Experiment design and false pos-
itive and false negative analysis
We then inspect the increases between the IP IDs in the packets received by
the measurement machine. Ideally, assuming no additional traﬃc and no packet
loss, the IP ID should increase by exactly one between consecutive probes. For
the last two deltas, since we send the spoofed packets in between our probe
packets, the ﬁnal IP ID increases will be diﬀerent based on the host’s blocking
behavior.
If the reﬂector does not block the blocklist IP, then we will observe an IP ID
increase sequence in our received RST responses that is: [+1, +1, +6, +6]. Here
the last two deltas are +6 since the reﬂector does not block the blocklist IP and
thus responds to spoofed packets, causing IP ID to increase by 5, and our probe
packet causes it to increase by another 1, which together make +6.
72
V. G. Li et al.
On the other hand, if the reﬂector blocks the blocklist IP, then we will see
an IP ID increase sequence that is: [+1, +1, +1, +1]. Here the last two deltas
are +1 since the reﬂector blocks the blocklist IP, leading to no extra change in
IP ID.
The ﬁrst three probes—corresponding to the ﬁrst two IP ID deltas—act as a
control. The last two “probe and spoof” patterns perform the actual experiment.
Seeing the initial two “+1” indicates this host is in a quiet period (no extra
network traﬃc). Therefore, we can be more conﬁdent that the following IP ID
jump (“+6” in our case) is because of our experiment. While the choice of the
numbers in the experiment may seem arbitrary, there is a rationale behind the
choice which we will discuss in following sections.
A.1 Inference Criteria
We now look at the criteria to infer if a reﬂector blocks a blocklist IP or not. Our
limited vantage point from the measurement machine limits our information to
the IP IDs seen from the reﬂector. Moreover, we desire to be conservative when
inferring blocking. Thus, our approach is to try the same trial, between a reﬂector
and a blocklist IP, until we get a “perfect signal”—a response which matches all
the criteria below:
1. The measurement machine received exactly ﬁve RST responses from the
reﬂector.
2. The ﬁve responses are received one second apart consecutively.
3. The IP ID increase sequence is either [+1, +1, +6, +6], which we will conclude
as no blocking, or [+1, +1, +1, +1], which we will conclude as blocking.
4. If any of the above three criteria are not met, we repeat the same experiment
again. We repeat up to 15 trials before giving up.
The ﬁrst requirement ensures no packet loss. The second requirement ensures
responses we received reﬂect the real IP ID changes in the reﬂector. The Internet
does not guarantee the order of packet arrival. Although we send one probe
packet per second, these packets might not arrive at the reﬂector in the same
order. Thus, the IP ID sequence from the response packets might not represent
the real order of IP ID changes at the host. Hence, by requiring that the response
packets cannot be less than 0.85 or more than 1.15 s apart we can minimize the
probability of reordered packets.
The third requirement is the core of our inference logic. Since we ignore
everything other than an IP ID increase sequence of [+1, +1, +1, +1] or [+1,
+1, +6, +6], we can assure that our inference of blocking is conservative. If we
saw a sequence of [+1, +1, +1, +1] but the reﬂector does not block the blocklist
IP, that would mean all 10 spoofed packets were lost. On the other hand, if we
see [+1, +1, +6, +6] and the reﬂector actually blocks the blocklist IP, that would
mean there are exactly ﬁve extra packets generated by the reﬂector during each
of the last two seconds. Both cases are very unlikely, which we will demonstrate
next with an analysis of false positives and false negatives.
Clairvoyance: Inferring Blocklist Use on the Internet
73
A.2 False Positive and False Negative Analysis
For our experiment, a false positive is when a reﬂector is not blocking a blocklist
IP, but we mistakenly conclude it is blocking. On the other hand, a false negative
is when a reﬂector is blocking a blocklist IP, but we mistakenly conclude it is
not. To evaluate false positive and false negative rates, we conduct experiments
on all the reﬂectors under consideration and measure the false positive and false
negative rates.
For false positive evaluation, we ﬁrst acquire a list of IPs that are veriﬁably
not being blocked by reﬂectors. Since we own these IPs, we can easily verify
by directly probing reﬂectors from these IPs. We acquired and tested 1,265 IPs
from ﬁve diﬀerent /24s. Then we probe reﬂectors and send the spoofed packets
with source addresses set to these pre-selected IPs. Since these IPs are not being
blocked, if we observe an IP ID increase sequence of [+1, +1, +1, +1], then we
know it is a false positive.
For false negatives, we run the experiment with only probe packets, and no
spoofed packets. This scenario is equivalent to the one where the reﬂector blocks
the spoofed IP. If we observe an IP ID increase sequence of [+1, +1, +6, +6],
then we know it was due to the background traﬃc at the reﬂector and hence is
a false negative.
Although we present the experiment design with ﬁve spoofed packets in each
of the last two seconds, we also experimented with a range of numbers and
calculated their false positive and negative rates. We tested 15 times with spoofed
packets equal to 3, 4, 5, 6, and 7 with every reﬂector, and we repeated the
experiment again on a diﬀerent day. The ﬁnal results are shown in Fig. 7.
We need to trade oﬀ between keeping false positive and negative rates low
while generating as little traﬃc as possible. We choose 5 spoofed packets as a
balance. By sending 5 spoofed packets, we get a false positive rate of 2.5e-5, and
a false negative rate of 8.5e-5. Furthermore, we also experimented with strategies
where we send 4 probe packets, from which we get 3 IP ID deltas, and sending
6 probe packets, from which we get 5 IP ID deltas. With only 3 deltas we suﬀer
a higher false negative rate, as it is easier for the reﬂector to show the same IP
ID increase sequence with extra traﬃc. With 6 probes, on the other hand, we
prolong the experiment, making it harder to get a “perfect signal”. Thus, our
choice of 5 probe packets with 5 spoofed packets in between is a good balance
between competing factors.
References
1. Afroz, S., Tschantz, M.C., Sajid, S., Qazi, S.A., Javed, M., Paxson, V.: Exploring
Server-side Blocking of Regions. Tech. rep, ICSI (2018)
2. Anderson, D.: Splinternet behind the great ﬁrewall of China. Queue 10(11), 40–49
(2012)
3. antirez: new TCP scan method. https://seclists.org/bugtraq/1998/Dec/79
4. Aryan, S., Aryan, H., Halderman, J.A.: Internet censorship in iran: a ﬁrst look. In:
Proceedings of the 3rd USENIX Workshop on Free and Open Communications on
the Internet (FOCI) (2013)
74
V. G. Li et al.
5. Bellovin, S.M.: A Technique for Counting NATted Hosts. In: Proceedings of the
2nd Internet Measurement Conference (IMC), pp. 267–272 (2002)
6. Bhutani, A., Wadhwani, P.: Threat Intelligence Market Size By Component, By
Format Type, By Deployment Type, By Application, Industry Analysis Report,
Regional Outlook, Growth Potential, Competitive Market Share and Forecast,
2019–2025 (2019)
7. Bouwman, X., Griﬃoen, H., Egbers, J., Doerr, C., Klievink, B., van Eeten, M.: A
Diﬀerent cup of TI? the added value of commercial threat intelligence. In: Proceed-
ings of the 29th USENIX Security Symposium (USENIX Security), pp. 433–450,
August 2020
8. CAIDA: Inferred AS to Organization Mapping Dataset. https://www.caida.org/
data/as organizations.xml
9. Censys - Public Internet Search Engine. https://censys.io/
10. Clayton, Richard., Murdoch, Steven J., Watson, Robert N.M.: Ignoring the great
ﬁrewall of China. In: Danezis, George, Golle, Philippe (eds.) PET 2006. LNCS, vol.
4258, pp. 20–35. Springer, Heidelberg (2006). https://doi.org/10.1007/11957454 2
11. Ensaﬁ, Roya., Knockel, Jeﬀrey., Alexander, Geoﬀrey, Crandall, Jedidiah R.:
Detecting intentional packet drops on the internet via TCP/IP side channels. In:
Faloutsos, Michalis, Kuzmanovic, Aleksandar (eds.) PAM 2014. LNCS, vol. 8362,
pp. 109–118. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-04918-
2 11
12. FireHOL IP Lists - All Cybercrime IP Feeds. http://iplists.ﬁrehol.org/
13. Hao, S., Kantchelian, A., Miller, B., Paxson, V., Feamster, N.: PREDATOR: proac-
tive recognition and elimination of domain abuse at time-of-registration. In: Pro-
ceedings of the ACM SIGSAC Conference on Computer and Communications Secu-
rity (CCS), pp. 1568–1579. ACM (2016)
14. Hao, S., Thomas, M., Paxson, V., Feamster, N., Kreibich, C., Grier, C., Hollenbeck,
S.: Understanding the Domain Registration Behavior of Spammers. In: Proceedings
of the ACM Internet Measurement Conference (IMC), pp. 63–76. ACM (2013)
15. IP2Location: IP Address to Identify Geolocation. https://www.ip2location.com/
16. IPdeny IP country blocks. https://www.ipdeny.com/
17. IPIP.net: The Best IP Geolocation Database. https://en.ipip.net/
18. Khattak, S., et al.: Do you see what i see? diﬀerential treatment of anonymous
users. In: Proceedings of the Network and Distributed System Security Symposium
(NDSS) (2016)
19. Klein, A., Pinkas, B.: From IP ID to device ID and KASLR bypass. In: Proceed-
ings of the 28th USENIX Security Symposium (USENIX Security), pp. 1063–1080
(2019)
20. K¨uhrer, Marc., Rossow, Christian, Holz, Thorsten: Paint it black: evaluating the
eﬀectiveness of malware blacklists. In: Stavrou, Angelos, Bos, Herbert, Portokalidis,
Georgios (eds.) RAID 2014. LNCS, vol. 8688, pp. 1–21. Springer, Cham (2014).
https://doi.org/10.1007/978-3-319-11379-1 1
21. Lemon, J.: Resisting SYN Flood DoS attacks with a SYN Cache. In: Proceedings
of the BSD Conference (BSDCon), pp. 89–97. USENIX Association, USA (2002)
22. Li, G.: An Empirical Analysis on Threat Intelligence: Data Characteristics and
Real-World Uses. Ph.D. thesis, UC San Diego (2020)
23. Li, V.G., Dunn, M., Pearce, P., McCoy, D., Voelker, G.M., Savage, S.: Reading
the Tea leaves: a comparative analysis of threat intelligence. In: Proceedings of the
28th USENIX Security Symposium (USENIX Security), pp. 851–867, August 2019
24. MaxMind: IP Geolocation and Online Fraud Prevention. https://www.maxmind.
com/
Clairvoyance: Inferring Blocklist Use on the Internet
75
25. McDonald, A., et al.: 403 forbidden: a global view of CDN Geoblocking. Proc.
Internet Measurement Conf. 2018, 218–230 (2018)
26. NetAcuity. https://www.digitalelement.com/solutions/
27. OpenNet Initiative: Survey of Government Internet Filtering Practices Indicates
Increasing Internet Censorship, May 2007
28. Park, J.C., Crandall, J.R.: Empirical study of a national-scale distributed intrusion
detection system: backbone-level ﬁltering of HTML responses in China. In: IEEE
30th International Conference on Distributed Computing Systems (ICDCS), pp.
315–326. IEEE (2010)
29. Pearce, P., Ensaﬁ, R., Li, F., Feamster, N., Paxson, V.: Augur: internet-wide detec-
tion of connectivity disruptions. In: Proceedings of the IEEE Symposium on Secu-
rity and Privacy (SP), pp. 427–443. IEEE (2017)
30. Pitsillidis, A., Kanich, C., Voelker, G.M., Levchenko, K., Savage, S.: Taster’s choice:
a comparative analysis of spam feeds. In: Proceedings of the ACM Internet Mea-
surement Conference (IMC), pp. 427–440. Boston, MA, November 2012 (2012)
31. Ponemon Institute LLC: Third Annual Study on Changing Cyber Threat Intelli-
gence: There Has to Be a Better Way (January 2018)
32. Postel, J.: RFC0791: Internet Protocol (1981)
33. Ramachandran, A., Feamster, N., Dagon, D.: Revealing Botnet Membership Using
DNSBL Counter-Intelligence. SRUTI 6 (2006)
34. Shackleford, D.: Cyber Threat Intelligence Uses, Successes and Failures: The SANS
2017 CTI Survey. Technical Report, SANS (2017)
35. Sheng, S., Wardman, B., Warner, G., Cranor, L.F., Hong, J., Zhang, C.: An empir-
ical analysis of phishing blacklists. In: Proceedings of the Conference on Email and
Anti-Spam (CEAS) (2009)
36. Singh, R., et al.: Characterizing the nature and dynamics of tor exit blocking. In:
Proceedings of the 26th USENIX Security Symposium (USENIX Security), pp.
325–341 (2017)
37. Sinha, S., Bailey, M., Jahanian, F.: Shades of grey: on the eﬀectiveness of
reputation-based “blacklists”. In: Proceedings of the 3rd International Conference
on Malicious and Unwanted Software (MALWARE), pp. 57–64. IEEE (2008)
38. Spring, N., Mahajan, R., Wetherall, D.: Measuring ISP topologies with Rocketfuel.
ACM SIGCOMM Comput. Commun. Rev. (CCR) 32(4), 133–145 (2002)
39. Thomas, Kurt., Amira, Rony., Ben-Yoash, Adi., Folger, Ori., Hardon, Amir.,
Berger, Ari., Bursztein, Elie, Bailey, Michael: The abuse sharing economy: under-
standing the limits of threat exchanges. In: Monrose, Fabian, Dacier, Marc, Blanc,
Gregory, Garcia-Alfaro, Joaquin (eds.) RAID 2016. LNCS, vol. 9854, pp. 143–164.
Springer, Cham (2016). https://doi.org/10.1007/978-3-319-45719-2 7
40. Tounsi, W., Rais, H.: A survey on technical threat intelligence in the age of sophis-
ticated cyber attacks. Comput. Secur. 72, 212–233 (2018)
41. University of Oregon Route Views Project. http://www.routeviews.org/
routeviews/
42. Best National University Rankings. https://www.usnews.com/best-colleges/
rankings/national-universities, January 2020
43. Zittrain, J., Edelman, B.: Internet ﬁltering in China. IEEE Internet Computing
7(2), 70–77 (2003)