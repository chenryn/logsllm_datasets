---
author: Eli Bendersky
category: 软件开发
comments_data:
- date: '2021-04-20 16:03:59'
  message: '&quot;当所有回调运行时，除了 main，其它的都运行在栈上&quot; 翻译错了吧'
  postip: 123.161.208.16
  username: 来自河南郑州的 Chrome 90.0|Mac 10.15 用户
count:
  commentnum: 1
  favtimes: 1
  likes: 0
  sharetimes: 0
  viewnum: 14522
date: '2018-03-02 01:20:53'
editorchoice: false
excerpt: 在这一部分中，我们将使用 libuv 再次重写我们的服务器，并且也会讨论关于使用一个线程池在回调中去处理耗时任务。
fromurl: https://eli.thegreenplace.net/2017/concurrent-servers-part-4-libuv/
id: 9397
islctt: true
largepic: /data/attachment/album/201803/02/012014luutflxu75l56ffn.jpg
permalink: /article-9397-1.html
pic: /data/attachment/album/201803/02/012014luutflxu75l56ffn.jpg.thumb.jpg
related:
- displayorder: 0
  raid: 9117
- displayorder: 0
  raid: 9417
reviewer: ''
selector: ''
summary: 在这一部分中，我们将使用 libuv 再次重写我们的服务器，并且也会讨论关于使用一个线程池在回调中去处理耗时任务。
tags:
- 并发
thumb: false
title: 并发服务器（四）：libuv
titlepic: true
translator: qhwdw
updated: '2018-03-02 01:20:53'
---
![](/data/attachment/album/201803/02/012014luutflxu75l56ffn.jpg)
这是并发网络服务器系列文章的第四部分。在这一部分中，我们将使用 libuv 再次重写我们的服务器，并且也会讨论关于使用一个线程池在回调中去处理耗时任务。最终，我们去看一下底层的 libuv，花一点时间去学习如何用异步 API 对文件系统阻塞操作进行封装。
本系列的所有文章：
* [第一节 - 简介](/article-8993-1.html)
* [第二节 - 线程](/article-9002-1.html)
* [第三节 - 事件驱动](/article-9117-1.html)
* [第四节 - libuv](http://eli.thegreenplace.net/2017/concurrent-servers-part-4-libuv/)
### 使用 libuv 抽象出事件驱动循环
在 [第三节](/article-9117-1.html) 中，我们看到了基于 `select` 和 `epoll` 的服务器的相似之处，并且，我说过，在它们之间抽象出细微的差别是件很有吸引力的事。许多库已经做到了这些，所以在这一部分中我将去选一个并使用它。我选的这个库是 [libuv](http://libuv.org/)，它最初设计用于 Node.js 底层的可移植平台层，并且，后来发现在其它的项目中也有使用。libuv 是用 C 写的，因此，它具有很高的可移植性，非常适用嵌入到像 JavaScript 和 Python 这样的高级语言中。
虽然 libuv 为了抽象出底层平台细节已经变成了一个相当大的框架，但它仍然是以 *事件循环* 思想为中心的。在我们第三部分的事件驱动服务器中，事件循环是显式定义在 `main` 函数中的；当使用 libuv 时，该循环通常隐藏在库自身中，而用户代码仅需要注册事件句柄（作为一个回调函数）和运行这个循环。此外，libuv 会在给定的平台上使用更快的事件循环实现，对于 Linux 它是 `epoll`，等等。
![libuv loop](/data/attachment/album/201803/02/012056juwug04uzgz0w77w.png)
libuv 支持多路事件循环，因此事件循环在库中是非常重要的；它有一个句柄 —— `uv_loop_t`，以及创建/杀死/启动/停止循环的函数。也就是说，在这篇文章中，我将仅需要使用 “默认的” 循环，libuv 可通过 `uv_default_loop()` 提供它；多路循环大多用于多线程事件驱动的服务器，这是一个更高级别的话题，我将留在这一系列文章的以后部分。
### 使用 libuv 的并发服务器
为了对 libuv 有一个更深的印象，让我们跳转到我们的可靠协议的服务器，它通过我们的这个系列已经有了一个强大的重新实现。这个服务器的结构与第三部分中的基于 `select` 和 `epoll` 的服务器有一些相似之处，因为，它也依赖回调。完整的 [示例代码在这里](https://github.com/eliben/code-for-blog/blob/master/2017/async-socket-server/uv-server.c)；我们开始设置这个服务器的套接字绑定到一个本地端口：
```
int portnum = 9090;
if (argc >= 2) {
  portnum = atoi(argv[1]);
}
printf("Serving on port %d\n", portnum);
int rc;
uv_tcp_t server_stream;
if ((rc = uv_tcp_init(uv_default_loop(), &server_stream)) data = NULL;
  if (uv_accept(server_stream, (uv_stream_t*)client) == 0) {
    struct sockaddr_storage peername;
    int namelen = sizeof(peername);
    if ((rc = uv_tcp_getpeername(client, (struct sockaddr*)&peername,
                                 &namelen)) state = INITIAL_ACK;
    peerstate->sendbuf[0] = '*';
    peerstate->sendbuf_end = 1;
    peerstate->client = client;
    client->data = peerstate;
    // Enqueue the write request to send the ack; when it's done,
    // on_wrote_init_ack will be called. The peer state is passed to the write
    // request via the data pointer; the write request does not own this peer
    // state - it's owned by the client handle.
    uv_buf_t writebuf = uv_buf_init(peerstate->sendbuf, peerstate->sendbuf_end);
    uv_write_t* req = (uv_write_t*)xmalloc(sizeof(*req));
    req->data = peerstate;
    if ((rc = uv_write(req, (uv_stream_t*)client, &writebuf, 1,
                       on_wrote_init_ack)) data` 是如何指向到一个 `peer_state_t` 结构上，以便于 `uv_write` 和 `uv_read_start` 注册的回调可以知道它们正在处理的是哪个客户端的数据。
* 内存管理：在带有垃圾回收的语言中进行事件驱动编程是非常容易的，因为，回调通常运行在一个与它们注册的地方完全不同的栈帧中，使得基于栈的内存管理很困难。它总是需要传递堆分配的数据到 libuv 回调中（当所有回调运行时，除了 `main`，其它的都运行在栈上），并且，为了避免泄漏，许多情况下都要求这些数据去安全释放（`free()`）。这些都是些需要实践的内容  注1 。
这个服务器上对端的状态如下：
```
typedef struct {
  ProcessingState state;
  char sendbuf[SENDBUF_SIZE];
  int sendbuf_end;
  uv_tcp_t* client;
} peer_state_t;
```
它与第三部分中的状态非常类似；我们不再需要 `sendptr`，因为，在调用 “done writing” 回调之前，`uv_write` 将确保发送它提供的整个缓冲。我们也为其它的回调使用保持了一个到客户端的指针。这里是 `on_wrote_init_ack`：
```
void on_wrote_init_ack(uv_write_t* req, int status) {
  if (status) {
    die("Write error: %s\n", uv_strerror(status));
  }
  peer_state_t* peerstate = (peer_state_t*)req->data;
  // Flip the peer state to WAIT_FOR_MSG, and start listening for incoming data
  // from this peer.
  peerstate->state = WAIT_FOR_MSG;
  peerstate->sendbuf_end = 0;
  int rc;
  if ((rc = uv_read_start((uv_stream_t*)peerstate->client, on_alloc_buffer,
                          on_peer_read))  0
    assert(buf->len >= nread);
    peer_state_t* peerstate = (peer_state_t*)client->data;
    if (peerstate->state == initial_ack) {
      // if the initial ack hasn't been sent for some reason, ignore whatever
      // the client sends in.
      free(buf->base);
      return;
    }
    // run the protocol state machine.
    for (int i = 0; i state) {
      case initial_ack:
        assert(0 && "can't reach here");
        break;
      case wait_for_msg:
        if (buf->base[i] == '^') {
          peerstate->state = in_msg;
        }
        break;
      case in_msg:
        if (buf->base[i] == '$') {
          peerstate->state = wait_for_msg;
        } else {
          assert(peerstate->sendbuf_end sendbuf[peerstate->sendbuf_end++] = buf->base[i] + 1;
        }
        break;
      }
    }