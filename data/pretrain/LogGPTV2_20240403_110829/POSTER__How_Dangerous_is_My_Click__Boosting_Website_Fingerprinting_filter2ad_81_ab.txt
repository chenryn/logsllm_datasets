different strategy to collect the sitemap graphs. First, for each site,
we gather the URLs of its index page and four additional, popular
pages of it that were found from Google, i.e., to simulate that users
access a website not only through its index page but also through
an already known link, using a bookmark or by querying a search
engine [5]. Starting from one of these five pages, we then extract all
URLs from that page referring to the same website. We group the
collected URLs based on their position on the page, i.e., whether
they are located in the navigation section or in the footer, and
exclude groups of URLs that are typically less visited by users, e.g.,
privacy policy and legal notice pages. From the remaining groups,
we randomly select ten groups of URLs, fetch one random URL
from each of these groups to simulate a user click, and repeat the
procedure described above to decide on the next click. The crawling
of URLs terminates once we reach a depth of ten pages for each
website and have gathered at least 2000 unique pages. Finally, we
build a directed graph where each node represents a URL and an
edge between two nodes corresponds to a link between these URLs.
For this graph, we consider all seen URLs regardless whether they
were selected by the sampling for further steps or not.
Generating User Sessions. Although a stored browser history
would be a reliable source of real user sessions, it can reveal confi-
dential data about users and usually is kept private. Thus, we use
the gathered sitemap graphs to synthetically create a set of user
sessions while ensuring that they exhibit realistic characteristics,
as described by Kumar et al. [5]. As users can access a website in
different ways, we use either the index page or one of the four
additional pages of a site to start a user session. As Miller et al. [6],
we execute a random walk over the sitemap graph of that website
to sample the rest of the user session, whereas we prefer pages
that have been visited neither in the current nor in any previously
Session 8: Poster & Demo Session CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea2412]
%
[
y
c
a
r
u
c
c
A
100
90
80
70
1
2
3
4
5
Set of visited pages per website
Figure 1: Accuracy achieved by state-of-the-art WFP attacks.
generated session. The latter increases the diversity between dif-
ferent sessions (and, thus, complicates the WFP attack). In total,
we sampled 10 sessions for each website containing 10 pages per
session and at least 50 unique pages per website.
Collection of Traffic Traces. We rely on an existing method [8]
that operates Tor Browser 7.5.6 to collect 20 traces for each of the
pages in our user sessions. Like related work [4, 9], we further
reconstruct the corresponding Tor cells exchanged for each page
load by applying a previously-used data extraction method [8].
4 EVALUATION
Next, we demonstrate the effectiveness of our novel WFP strategies.
Voting-based. We first analyze how our voting-based scheme
influences the accuracy of different state-of-the-art WFP attacks for
different sets of testing pages per website in a closed-world scenario
(i.e., the attacker knows the set of all visited websites). We consider
two evaluation scenarios: (ùëñ) the adversary knows all webpages of a
given website that can be visited by a user, and (ùëñùëñ) the attacker can
use only a subset of the available pages belonging to a given website
for training. Based on these scenarios, we apply a 10-fold cross-
validation (CV) either with respect to the number of available traces
per webpage or with respect to the number of available pages per
website. Figure 1 shows the accuracy achieved by three state-of-the-
art WFP attacks: CUMUL [8], k-FP [4], and DF [9]. Our experiments
confirm our claim that WFP attacks become more effective when
the number of consecutively observed pages belonging to a single
website increases. We further notice that only two clicks within
a site are sufficient to achieve 100% accuracy by using the best
performing classifier DF. In a more realistic scenario, already three
clicks within a website are sufficient to boost the accuracy of k-FP
and CUMUL by approximately 20%, while DF correctly classifies
all websites when the user consecutively visits five pages.
HMM-based. We further analyze whether the exact sequence
of visited pages can improve the overall accuracy achieved by the
classifiers. To this end, we combine the worst-performing classifier
CUMUL with our HMM-based strategy. First, we assume that the
adversary knows all sessions, i.e., the HMM contains transition
information for all possible user sessions. In other words, we apply
a 10-fold CV where for each page we use 18 traces for training
and two for testing. As shown in Figure 2, the accuracy increases
significantly for all sessions of length of two or higher.
Next, we evaluate a scenario, in which the user session used for
testing is unknown to the attacker, i.e., we use a 10-fold CV to the
number of user sessions. In most of the cases, this scenario leads to
testing on pages that are also unknown to both CUMUL and HMM.
The accuracy decreases slightly compared to the scenario when the
sessions are known (see Figure 2). Still, we see a similar positive
trend from the use of sequence of pages. The negative effect of
CUMUL (unknown pages)
CUMUL (known pages)
DF (unknown pages)
DF (known pages)
k-FP (unknown pages)
k-FP (known pages)
]
%
[
y
c
a
r
u
c
c
A
100
90
80
70
1
2
HMM (known user sessions)
HMM (unknown user sessions)
HMM (use of sitemaps)
4
8
5
6
7
9
10
3
Sequence of visited pages per website
Figure 2: Accuracy achieved by our HMM-based strategy.
unknown sessions is prominent for short sessions and decreases
when using longer sessions. For a session of length ten, the accuracy
is 96.8% compared to 97.8% when all sessions are known.
Finally, instead of learning transition probabilities from user
sessions for HMM, we use the sitemap graphs. Although we cannot
avoid a potential bias due to the use of the sitemap graphs to collect
user sessions, we argue that we can cover orders of magnitude
more user sessions by using this approach. As shown in Figure 2,
the use of sitemap graphs to compute the transition probabilities is
beneficial for user sessions of at most four pages. Still, the difference
in the accuracy in the different use cases is neglectable.
To sum up, our results show that our revised, more realistic
attacker model for WFP attacks is far more dangerous for users,
who consecutively browse multiple pages of a single website.
5 CONCLUSION
In this work, we considered a more realistic attacker model for WFP
attacks and proposed two strategies that use implicit knowledge on
browsing behavior. Our evaluation shows that two, at most three,
clicks within a website are sufficient to significantly improve state-
of-the-art WFP attacks. We demonstrate that WFP attacks pose a
significantly more serious threat to online privacy of Tor users who
browse multiple pages of a given website. Our results underline the
threat of an attacker who is able to fingerprint a complete website.
As next steps, we will increase the scale and scope of our strate-
gies to identify ways to improve their overall performance. We will
also extend our analysis to an open-world setting and study the
efficiency of WFP defenses against this new attack surface. While
our preliminary analysis shows that the exact sequence of visited
pages is only secondary, we would like to further explore its impact.
Acknowledgments. Parts of this work have been funded by
the EU and state Brandenburg EFRE StaF project INSPIRE and the
German Federal Ministry of Education and Research (BMBF) under
the project KISS KI Simple & Scalable.
REFERENCES
[1] Xiang Cai et al. 2012. Touching from a distance: website fingerprinting attacks
[2] Roger Dingledine et al. 2004. Tor: The Second-Generation Onion Router. In USENIX
and defenses. In ACM CCS.
Security Symposium.
[3] Martin Ester et al. 1996. A Density-Based Algorithm for Discovering Clusters in
Large Spatial Databases with Noise. In KDD.
[4] Jamie Hayes and George Danezis. 2016. k-fingerprinting: A Robust Scalable
Website Fingerprinting Technique. In USENIX Security Symposium.
[5] Ravi Kumar and Andrew Tomkins. 2010. A Characterization of Online Browsing
Behavior. In 19th International Conference on World Wide Web.
[6] Brad Miller et al. 2014. I Know Why You Went to the Clinic: Risks and Realization
of HTTPS Traffic Analysis. PETS.
[7] Se Eun Oh et al. 2021. GANDaLF: GAN for Data-LimitedFingerprinting. PETS.
[8] Andriy Panchenko et al. 2016. Website Fingerprinting at Internet Scale. In NDSS.
[9] Payap Sirinam et al. 2018. Deep Fingerprinting: Undermining Website Fingerprint-
ing Defenses with Deep Learning. In ACM CCS.
Session 8: Poster & Demo Session CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea2413