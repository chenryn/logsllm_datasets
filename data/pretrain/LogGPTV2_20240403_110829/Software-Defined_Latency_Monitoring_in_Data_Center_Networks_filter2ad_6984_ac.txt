if one path has lower latencies than the other. Figure 3(a) shows that, although
SLAM overestimates the ground truth for under-millisecond latencies, it is able
to match the ground truth latency distribution as the path latency increases.
Indeed, the KS test does not reject the null hypothesis for Exp M and Exp H.
Figures 4(a) and (b) show the Quantile-Quantile (Q-Q) plots for Exp M and
Exp H, respectively. We remove outliers by discarding the bottom and top 10 %
(5 %) of SLAM’s latency estimates for Exp M (Exp H). Except for a small number
of very low and high quantiles, the quantiles for SLAM’s estimates are equal or
close to the quantiles for ground truth estimates; most of the points in the Q-Q
plot lie on the y = x line.
4.2 Filtering Out High Latency Paths
SLAM can help network operators identify low-latency paths. For a collection
of paths, we can use the pairwise KS test to ﬁrst select a subset of paths whose
distribution are diﬀerent from each other, and then ﬁlter out paths with high
latency quantiles. Similarly, when monitoring a path, an operator can ﬁrst use
the KS test to determine if its latency distribution has changed (e.g., due to
Software-Deﬁned Latency Monitoring in Data Center Networks
369
Table 1. Comparison of the 50th, 75th, 90th, and 95th percentile values for Exp M
and Exp H.
Exp # 50th %tile 75th %tile 90th %tile 95th %tile
Exp M 7.47 ms
Exp H 60.0 ms
19.2 ms
78.0 ms
11.6 ms
76.8 ms
8.66 ms
71.9 ms
change in traﬃc) and then use the latency quantile values to decide whether to
continue using it or switch to a diﬀerent path. For instance, in our experiments,
when we compare samples from Exp M and Exp H, the KS test rejects the null
hypothesis, i.e., the latency distribution on the monitored path has changed
due to change in traﬃc. Table 1 shows that four quantiles for the two samples
diﬀer signiﬁcantly. This is conﬁrmed by Fig. 3(a), where empirical CDFs of the
measurements collected by SLAM for Exp M and Exp H are clearly diﬀerent.
SLAM’s use of KS test, in combination with latency quantiles, is more robust
because an operator can be conﬁdent that the diﬀerence in latency quantiles
across paths or on the same path over time is statistically signiﬁcant.
4.3 Sensitivity to Network Conditions
Next, we study SLAM’s accuracy in the presence of bursty data traﬃc and
increased control channel traﬃc.
Data traﬃc. To see if variable traﬃc aﬀects SLAM’s latency estimates, we
repeat Exp H, but instead of running iperf continuously, we run it in bursts of
variable size. Figure 3(b) shows how latency varies over time as we introduce and
remove traﬃc from the network. SLAM’s estimates adapt well to changes in the
ground truth latency triggered by introducing congestion in the network. Like
the results shown in Fig. 3(a), SLAM over-estimates latency when path latency is
low but accurately captures latency spikes. These results further conﬁrm SLAM’s
eﬀectiveness in enabling data center networks to route traﬃc away from segments
on which latency increases by tens of milliseconds.
Control traﬃc. We monitor the slow path delay of switches in our network
while we introduce two types of control traﬃc: FlowMod, by repeatedly inserting
forwarding rules, and PacketIn, by increasing the number of probes that match
a rule whose action is “send to controller”. We varied the control packet rate
from 1 to 20 per second and observed a median increase of 1.28 ms. Varying the
amount of concurrent rule installations from 0 to 150 rules per second resulted
in a median increase of 2.13 ms. Thus, the amount of unrelated control traﬃc
in the network does not inﬂuence SLAM’s eﬀectiveness in detecting high-delay
paths.
370
C. Yu et al.
5 Reactive OpenFlow Deployments
So far, we considered a proactive OpenFlow deployment for SLAM, where nor-
mal data packets always have a matching rule and do not trigger PacketIn mes-
sages. Another option is to use a reactive deployment, in which switches notify
the controller of incoming packets without a matching rule by sending a Pack-
etIn control message. Because too many such control messages could overload
the controller and make the network unusable [2], reactive deployments are lim-
ited to smaller enterprises and data centers with tens of switches or when the
network must react to traﬃc changes automatically.
Reactive networks provide a signiﬁcant advantage for SLAM: it can use exist-
ing PacketIn messages to compute path latency distributions. This eliminates
the need to insert expensive probes to trigger PacketIn’s and reduces the cost of
monitoring by using already existing control traﬃc [17]. However, there are two
disadvantages, which we discuss at large next.
5.1 Variations in Control Processing
Using reactive PacketIn’s at both ends of a path to capture its latency means that
normal data packets are delayed at the ﬁrst switch until the controller tells the
switch what to do with them. This introduces an additional delay in the path of
a packet described in Fig. 1: the time it takes the controller to process the packet
and reply to the switch (either with FlowMod or PacketOut) and the time it takes
the switch to forward the packet to the out port once it learns what to do with it.
SLAM can estimate the controller processing time and the controller-to-switch
delay as described in Sect. 3.2. However, the switch forwarding time depends on
the load on the switch CPU and what other traﬃc is traversing the switch; this
is more diﬃcult to estimate accurately. In practice, SLAM can use the approach
in Sect. 3.2 to infer variations in switch processing and discard measurements
performed during times when variations are high.
5.2 Frequency of Control Traﬃc
The accuracy of SLAM’s estimated latency distribution depends on the fre-
quency of PacketIn’s from switches at both ends of the measured path. This is
aﬀected by the overall distribution of traﬃc in the network and by the structure
of rules used to guide the traﬃc. For example, because switches on a backup link
see little data traﬃc, they trigger little control traﬃc for SLAM to use. Simi-
larly, forwarding rules with long timeouts or with wildcards limit the number of
PacketIn messages.
To evaluate the frequency of PacketIn measurements, we simulate SLAM
on a real-world enterprise trace. We use the EDU1 trace collected by Benson
et al. [2], capturing all traﬃc traversing a switch in a campus network for a
period of three hours. We identify all network ﬂows in the trace, along with
their start time. The collectors of the trace report that the ﬂow arrival rate at
the switch is on the order of a few milliseconds [2].
Software-Deﬁned Latency Monitoring in Data Center Networks
371
s
t
r
o
p
f
o
F
D
C
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
 100
s
t
r
o
p
f
o
F
D
C
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
 0
 1000
 10000
 100000
 1e+06
Average interval
Number of PacketIn msgs seen
(a)
Average interval
Median interval
 20
 25
 5
Time between PacketIn messages (sec)
 10
 15
(b)
Fig. 5. (a) No. of PacketIn’s each link in a 24 port switch sees in three hours.
(b) Average and median time between PacketIn’s per link on a 24 port switch.
Since only PacketIn’s associated with traﬃc that traverses the same path are
useful, we need to evaluate the ﬂow arrival rate for each input port of the switch.
Our traﬃc trace does not contain input port information, therefore we simulate
a 24-port switch using the following heuristic. We ﬁrst associate every distinct
/p preﬁx (where p is, in turn, 32, 30, 28, 20, or 24) of source IP addresses in
the trace with a port and then assign each individual ﬂow to the link (or input
port) associated with its source IP /p preﬁx. We group ﬂows by preﬁx because
routing in the Internet is typically preﬁx-based. Below, we present results for
p = 28; results for other preﬁx lengths are qualitatively similar.
We compute both the number and the frequency of PacketIn messages that
each link receives during the measurement period. Figure 5(a) shows that most
links see more than 10,000 PacketIn’s during the three hour span, which is equiv-
alent to a rate of around one PacketIn per second. Figure 5(b) presents the aver-
age and median time between consecutive PacketIn’s for each link of the switch.
SLAM would capture samples from most links every two seconds and 80 % of all
links would be measured less than every 10 seconds.
To summarize, our analysis on a real-world enterprise trace shows that, in a
reactive SDN deployment, SLAM would be able to capture latency measurements
once every two seconds on average without requiring any additional generation
of probes. We are currently investigating the design of an adaptable SLAM that
would rely on existing PacketIn’s when control traﬃc volume is high and generate
probes that trigger artiﬁcial PacketIn’s when control traﬃc is scarce.
6 Conclusion
We presented SLAM, a path latency monitoring framework for software-deﬁned
data centers. SLAM uses timestamps of carefully triggered control messages
to monitor network latency between any two arbitrary switches and identify
high-delay paths. SLAM’s measurements are accurate enough to detect latency
inﬂations of tens of milliseconds and enable applications to route traﬃc away
from high-delay path segments.
372
C. Yu et al.
References
1. Al-Fares, M., Radhakrishnan, S., Raghavan, B., Huang, N., Vahdat, A.: Hedera:
dynamic ﬂow scheduling for data center networks. In: USENIX NSDI (2010)
2. Benson, T., Akella, A., Maltz, D.: Network traﬃc characteristics of data centers
in the wild. In: ACM IMC (2010)
3. Chen, Y., Mahajan, R., Sridharan, B., Zhang, Z.-L.: A provider-side view of web
search response time. In: Proceedings of ACM SIGCOMM (2013)
4. Curtis, A.R., Mogul, J.C., Tourrilhes, J., Yalagandula, P., Sharma, P., Banerjee,
S.: DevoFlow: scaling ﬂow management for high-performance networks. In: Pro-
ceedings of ACM SIGCOMM (2011)
5. Das, A., Lumezanu, C., Zhang, Y., Singh, V., Jiang, G., Yu, C.: Transparent and
eﬃcient network management for big data processing in the cloud. In: HotCloud
(2013)
6. Duﬃeld, N., Grossglauser, M.: Trajectory sampling for direct traﬃc observation.
In: Proceedings of ACM SIGCOMM (2000)
7. Flach, T., Dukkipati, N., Terzis, A., Raghavan, B., Cardwell, N., Cheong, Y., Jain,
A., Hao, S., Katz-Bassett, E., Govindan, R.: Reducing web latency: the virtue of
gentle aggression. In: Proceedings of ACM SIGCOMM (2013)
8. Huang, D.Y., Yocum, K., Snoeren, A.C.: High-ﬁdelity switch models for software-
deﬁned network emulation. In: Proceedings of HotSDN (2013)
9. Kolmogorov, A.N.: Sulla determinazione empirica di una legge di distribuzione.
Giornale dellIstituto Italiano degli Attuari 4(1), 83–91 (1933)
10. Kompella, R.R., Levchenko, K., Snoeren, A.C., Varghese, G.: Every microsecond
counts: tracking ﬁne-grain latencies with a lossy diﬀerence aggregator. In: Proceed-
ings of ACM SIGCOMM (2009)
11. Lee, M., Duﬃeld, N., Kompella, R.R.: Not all microseconds are equal: ﬁne-grained
per-ﬂow measurements with reference latency interpolation. In: Proceedings of
ACM SIGCOMM (2010)
12. Moshref, M., Yu, M., Sharma, A., Govindan, R.: Scalable rule management for
data centers. In: Proceedings of USENIX NSDI (2013)
13. Rotsos, C., Sarrar, N., Uhlig, S., Sherwood, R., Moore, A.W.: OFLOPS: an open
framework for OpenFlow switch evaluation. In: Taft, N., Ricciato, F. (eds.) PAM
2012. LNCS, vol. 7192, pp. 85–95. Springer, Heidelberg (2012)
14. RagingWire. http://www.ragingwire.com
15. van Adrichem, N.L.M., Doerr, C., Kuipers, F.A.: OpenNetMon: network monitor-
ing in OpenFlow software-deﬁned networks. In: IEEE NOMS (2014)
16. Varghese, G.: Network Algorithmics. Elsevier/Morgan Kaufmann, Amsterdam
(2005)
17. Yu, C., Lumezanu, C., Zhang, Y., Singh, V., Jiang, G., Madhyastha, H.V.:
FlowSense: monitoring network utilization with zero measurement cost. In:
Roughan, M., Chang, R. (eds.) PAM 2013. LNCS, vol. 7799, pp. 31–41. Springer,
Heidelberg (2013)