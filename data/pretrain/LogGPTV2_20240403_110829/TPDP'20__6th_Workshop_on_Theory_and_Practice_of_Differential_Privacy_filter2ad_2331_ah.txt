is. It will try to answer queries with h, and update h when it leads to too much error. It will turn
out that only a small number of updates are needed, and this will imply that the overall privacy
loss is small. Here are the details:
1. INITIALIZE the hypothesis h to the uniform distribution on X.
2. REPEAT at most O(log |X|)/α2 times (outer loop)
(a) RANDOMIZE the accuracy threshold: ˆα = α/2 + Lap(1/ε0n), where ε0 is a parameter
that will be set later in the proof.
(b) REPEAT (inner loop)
i. Receive next query q
ii. If |q(x)− q(h)| + Lap(1/ε0n)  q(h),
if a  1). Indeed, if we run an (ε, δ) diﬀerentially private mechanism M on a uniformly
random dataset X ← Xn, then the expected fraction of rows that any adversary can reconstruct is
at most eε/|X| + δ (since if we replace any row Xi with an independent row X(cid:48)
i) reveals
no information about Xi and thus does not allow for reconstructing Xi with probability larger than
1/|X|).
i, M(X−i, X(cid:48)
We now give some fundamental lower bounds, due to Dinur and Nissim [30], on the tradeoﬀ
between the error and the number of counting queries that can be answered while avoiding blatant
non-privacy. These lower bounds predated, and indeed inspired, the development of diﬀerential
privacy.
Let X = {0, 1}. Then a dataset of n people is simply a vector x ∈ {0, 1}n. We will consider
(normalized) inner-product queries speciﬁed by a vector q ∈ {0, 1}n: the intended answer to the
query q is (cid:104)q, x(cid:105)/n ∈ [0, 1]. Think of the bits in x as specifying a sensitive attribute of the n
members of the dataset and q as specifying a subset of the population according to some publicly
33
known demographics. Then (cid:104)q, x(cid:105)/n measures the correlation between the speciﬁed demographic
traits and the sensitive attribute.
These are not exactly counting queries, but they can be transformed into counting queries as
follows: Let ˜X = [n] × {0, 1} be our data universe, map an inner-product query q ∈ {0, 1}n to the
counting query ˜q((i, b)) = qi · b, and consider datasets of the form ˜x = ((1, x1), (2, x2), . . . , (n, xn)),
˜q((i, b)) = qi· b. Then ˜q(˜x) = (cid:104)q, x(cid:105)/n, and reconstructing x is equivalent to reconstructing ˜x, which
again contradicts (1, 0.1)-diﬀerential privacy.
Theorem 5.2 (reconstruction from many queries with large error [30]). Let x ∈ {0, 1}n. If we are
given, for each q ∈ {0, 1}n, a value yq ∈ R such that
(cid:12)(cid:12)(cid:12)(cid:12)yq − (cid:104)q, x(cid:105)
n
(cid:12)(cid:12)(cid:12)(cid:12) ≤ α.
Then one can use the yq’s to compute x(cid:48) ∈ {0, 1}n such that x and x(cid:48) diﬀer in at most 4α fraction
of coordinates.
Corollary 5.3. If M(x) is a mechanism that outputs values yq as above with α ≤ 1/40, then M is
blatantly non-private.
Thus at least Ω(1) additive error is needed for privately answering all 2n normalized inner
product queries, which as noted correspond to 2n counting queries on a data universe of size 2n.
The SmallDB mechanism (Thm 4.1) can answer exp( ˜Ω(n)) counting queries over a data universe
X with ε-diﬀerential privacy and error α provided |X| ≤ exp(polylog(n)) and ε, α ≥ 1/ polylog(n).
Corollary 5.3 says that we cannot push this further to answer 2n queries.
Proof of Theorem 5.2. Pick any x(cid:48) ∈ {0, 1}n such that for all q ∈ {0, 1}n,
(cid:12)(cid:12)(cid:12)(cid:12)yq − (cid:104)q, x(cid:48)(cid:105)
n
(cid:12)(cid:12)(cid:12)(cid:12) ≤ α.
(We know that at least one such x(cid:48) exists, namely x).
We need to prove that x and x(cid:48) diﬀer on at most a 4α fraction of coordinates. Let q1 = x and
let q0 be the bitwise complement of x. Then, the relative Hamming distance between x and x(cid:48)
equals:
d(x, x(cid:48))
n
(cid:12)(cid:12)(cid:12)(cid:12)(cid:104)q0, x(cid:105)
=
≤
n
≤ 4 · α.
(cid:12)(cid:12)(cid:12)(cid:12)(cid:104)q1, x(cid:105)
n
(cid:12)(cid:12)(cid:12)(cid:12) +
(cid:12)(cid:12)(cid:12)(cid:12)yq1 − (cid:104)q1, x(cid:48)(cid:105)
n
(cid:12)(cid:12)(cid:12)(cid:12)
− yq1
|(cid:104)q0, x(cid:105) − (cid:104)q0, x(cid:48)(cid:105)| + |(cid:104)q1, x(cid:105) − (cid:104)q1, x(cid:48)(cid:105)|
(cid:12)(cid:12)(cid:12)(cid:12) +
n
(cid:12)(cid:12)(cid:12)(cid:12)yq0 − (cid:104)q0, x(cid:48)(cid:105)
n
(cid:12)(cid:12)(cid:12)(cid:12) +
− yq0
Of course we can avoid the above attack by restricting the adversary to fewer than 2n queries.
√
The next theorem will say that even for much fewer queries (indeed O(n) queries), we must incur
a signiﬁcant amount of error, α ≥ Ω(1/
n). This is tight, matching Theorem 2.7 up to a factor of
O((cid:112)log(1/δ) · log log n). We will in fact study the more general question of what additive error is
needed for privately answering any set Q of counting queries.
34
Let q1, . . . , qk ∈ {0, 1}n be a collection of vectors, which we view as specifying inner-product
queries (cid:104)q, x(cid:105)/n as above. Suppose we have a mechanism M that answers these queries to within
error α, i.e. with high probability outputs y1, . . . , yk ∈ [0, 1] with
Let’s try to show that M is blatantly non-private. Our privacy-breaking strategy is the same: take
any x(cid:48) ∈ {0, 1}n with
(cid:12)(cid:12)(cid:12)(cid:12) ≤ α.
(cid:12)(cid:12)(cid:12)(cid:12)yj − (cid:104)qj, x(cid:105)
(cid:12)(cid:12)(cid:12)(cid:12) ≤ α
(cid:12)(cid:12)(cid:12)(cid:12)yj − (cid:104)qj, x(cid:48)(cid:105)
n
n
for each j.
Then, by the triangle inequality, we have |(cid:104)qj, x − x(cid:48)(cid:105)|/n ≤ 2α for all j = 1, . . . , k. For blatant
non-privacy, we want to use this to deduce that x and x(cid:48) have Hamming distance at most n/10,
i.e. (cid:107)x − x(cid:48)(cid:107)1 ≤ n/10. Suppose not. Let z = x − x(cid:48). Let Q denote k × n matrix whose rows are the
qj. Thus we have:
1. z is a {0, +1,−1} vector with (cid:107)z(cid:107)1 > n/10,
2. (cid:107)Qz(cid:107)∞ ≤ 2αn.
Thus, we have a contradiction (and hence can conclude that M is blatantly non-private) if the
partial discrepancy of Q, deﬁned as follows, is larger than 2αn:
Deﬁnition 5.4 ((partial) discrepancy). For a k × n matrix Q, we deﬁne its discrepancy Disc(Q)