and low-latency time measurement is critical to cache tim-
ing attacks on ARM. In this section, we examine the latency
of the POSIX clock_gettime system call, which is a ﬁne-
grained time measurement facility that is accessible in all
Android versions.
We considered using clock_gettime() with three clocks:
CLOCK REALTIME, CLOCK MONOTONIC, and the
per-thread clock CLOCK THREAD CPUTIME ID. To se-
lect one clock with minimum latency, we conducted the fol-
lowing experiments: In an Android app compiled with NDK
on Samsung Galaxy S6, we used one of the three clocks to
measure the execution time of a loop that is expected to
consume (roughly) constant time. We show in Fig. 1 the
measurements of running 1, 2,··· , 10 of such loops using the
three clocks, respectively. The results shown are mean val-
ues and one standard deviation of 20 runs. The latency of
each clock can be roughly estimated by the time measure-
ments of executing i loops subtracting the estimated exe-
cution time of i loops. From the ﬁgure, we can see that
CLOCK REALTIME and CLOCK MONOTONIC per-
forms much better in terms of measurement latency (i.e.,
roughly 130 ns) than CLOCK THREAD CPUTIME ID
(i.e., about 780 ns). In our paper, the monotonic clock was
chosen because the other may be unexpectedly adjusted by
Network Time Protocol (NTP) daemon.
2
Prior knowledge on cache inclusiveness of a particular processor im-
plementation is only available through anecdotes [42].
8603.2 Cache Flushes
We empirically study
how the clearcache sys-
tem call can be used
in Flush-Reload at-
tacks.
We focused,
however, only on its
eﬀects on instruction
caches, because the ker-
nel
source code that
implements the clearcache
system call only cleaned
the data caches but in-
validated the instruc-
tion caches3.
Cache
cleaning means writing
dirty cache lines out
to the next
level of
cache/memory hierarchy and then clear the dirty bits;
cache invalidation means clearing the valid bits of the cache
lines [2]. Therefore, it is only necessary to conduct exper-
iments to understand the behavior of clearcache on the
instruction cache—will the L1 instruction cache and the L2
cache both be ﬂushed?
Figure 1: Time measurement
using three diﬀerent clocks
(evaluated on A53 running at
highest frequency).
We developed an Android app with Android NDK and a
native shared library that exports a dummy library func-
tion (consisting of “mov x0, x0” in the 64-bit version, and
“mov r0,r0” in the 32-bit version), which, after compilation,
occupies exactly 1KB of memory space. To eliminate unex-
pected side-eﬀects, we intentionally aligned the oﬀset of the
beginning of the function to be a multiple of 4KB within the
shared library. Because of the coarse-grained address space
layout randomization (ASLR) in Linux, when the library is
loaded at runtime, the function will still be page-aligned.
The Android app dynamically linked this self-developed li-
brary into their address space using dlopen at runtime.
Then it split into two threads, which used sched_setaffinity
system call to pin themselves on two diﬀerent cores sharing
the same L2 cache. In the following tests, all experiments
were repeated 1000 times (run at the maximum CPU fre-
quency) to measure the mean and standard deviation.
In the ﬁrst experiment, thread A repeatedly executed the
function code while thread B stayed idle. The average time
to execute the entire function in this way was measured as
T1. Essentially, T1 measures the time to execute the function
from the L1 instruction cache (i.e., all L1 cache hits).
In the second experiment, while keeping thread B idle,
thread A called the clearcache system call before starting
to execute the function. The time of the function execution
itself was measured as T2. Hence T2 represents the eﬀects
of the clearcache system call on the local instruction cache
and the uniﬁed L2 cache.
In the third experiment, while thread A executed the func-
tion in the same way as the ﬁrst experiment, thread B re-
peatedly called the clearcache system call without any in-
terval to ﬂush the entire function. The execution time of
the function by thread A is denoted as T3. Therefore, T3
reﬂects the eﬀects of cross-core instruction cache ﬂushing.
In the last experiment, we still kept thread B idle. Thread
A measured the time taken (T4) to execute the function
3
clearcache is implemented in the __flush_cache_user_range function
in mm/cache.S of the Android’s kernel source code (v3.10.9 on Galaxy
S5 and Nexus 6, v3.10.61 on Galaxy S6)
Figure 2: Eﬀects of clearcache on instruction caches.
code with L2 cache misses. In order to achieve this eﬀect,
thread A cleansed the entire L1 instruction cache and uni-
ﬁed L2 cache in between of two function executions. The
method to do so with guarantees to cleanse the entire L1 and
L2 caches, however, is not straightforward. We developed
our own method as described in Appendix B. The results
for running the experiments on all three smartphones (ﬁve
CPUs) are shown in Fig. 2. We will discuss these results
shortly in Sec. 3.4.
3.3 Cache Inclusiveness
We design a method using only cache timing to determine
whether the L2 cache is inclusive, exclusive or non-inclusive
to L1 data cache and L1 instruction cache, respectively. To
do so, we ﬁrst developed a shared native library which ex-
ports a dummy function (e.g., 1KB) in exactly the same way
as in Sec. 3.2. Then in the native component of an Android
app, the following test was conducted:
Detecting inclusive L2 caches. In the ﬁrst experiment,
the function was loaded into the L1 data cache by reading
each cache line. The average time needed to load the entire
function once is denoted T1. Then in the second experiment,
the Android app completely cleansed the L2 cache without
polluting the L1 data cache—by executing instructions as
described in Appendix B—in between of two function code
readings. The time to read the function was measured as
If T1 (cid:28) T2, T2 reﬂects L1 data (and also L2) cache
T2.
misses. Therefore cleansing L2 cache from instruction cache
also cleanses the L1 data cache, and therefore the L2 cache
is inclusive to the L1 data cache. Otherwise it is either
exclusive or non-inclusive to the L1 data cache.
Because the same L2 cache may have diﬀerent inclusive-
ness to the L1 data cache and the instruction cache, we have
to conducted a similar test for L1 instruction cache. Spe-
cially, the dummy function was executed and the time to
complete one execution was measured as T1. Then in the
second experiment, the L2 cache was cleansed completely
from the data-cache side so that the instruction cache was
not polluted (again, using method described in Appendix B)
in between of two function execution. The execution time
was measured as T2. If T1 (cid:28) T2, T2 represents L1 instruction
(and L2) cache misses. Hence, cleansing L2 cache from the
data cache also cleanses the L1 instruction cache, and there-
fore the L2 cache is inclusive to the L1 instruction cache.
Otherwise it is either exclusive or non-inclusive to the L1
instruction cache.
12345loops0200400600800100012001400Time (ns)CLOCK_REALTIMECLOCK_MONOTONICCLOCK_THREAD_CPUTIME_IDKrait 450A15A7A57A53020004000600080001600020000T1T2T3T4861Smartphone
Krait 450 (dcache)
Krait 450 (icache)
Cortex-A15 (dcache)
Cortex-A15 (icache)
Cortex-A7 (dcache)
Cortex-A7 (icache)
Cortex-A57 (dcache)
Cortex-A57 (icache)
Cortex-A53 (dcache)
Cortex-A53 (icache)
T1
1169
1020
2600
2484
3378
3551
223
150
325
275
T2
3700
4350
6469
5474
15460
15822
907
794
1633
1287
inclusiveness
inclusive
inclusive
inclusive
inclusive
inclusive
inclusive
inclusive
inclusive
inclusive
inclusive
Table 1: L2 cache inclusiveness tests.
From Table 1 we can clearly see that on all the tested
processors the L2 caches are inclusive to both data caches
and instruction caches. Therefore, there is no need to con-
duct further experiments to distinguish exclusive and non-
inclusive L2 caches. However, we describe an algorithm in
Appendix C with which these two types of cache implemen-
tations can be programmatically diﬀerentiated. We hope it
can be helpful to other research on similar topics.
3.4 Discussion
Cache ﬂushes. In order to perform Flush-Reload side-
channel attacks on the L2 cache, the ﬂush operations must
evict the targeted memory block out of (1) the local L1
caches, (2) the shared L2 cache, (3) and the L1 caches of
If condition 1 is not met, Reload will only
other cores.
if condition 2 is not met, Reload will
observe L1 hits;
only observe L2 hits; if condition 3 is not met, the victim
can continue using its local copy, so its operation will not
make any changes to the shared L2 cache. We already know
data caches cannot be used Flush-Reload attacks because
clearcache only cleans but not ﬂushes the L1 data cache—
condition 1 not met. Moreover, as is seen from Fig. 2, not
all instruction caches on these ARM processors satisfy these
requirements, either: T2 (local clearcache) and T3 (cross-
core clearcache) of Krait 450, Cortex A15 and Cortex A7
are merely larger than T1 (L1 hits) and are much smaller
than T4 (L2 misses). Therefore, clearcache does not ﬂush
the L2 caches on these processors (condition 2 not satis-
ﬁed). Cache ﬂush operations on the instruction cache of
Cortex A53 and A57 meet all three requirements: T2 and
T3, though slightly less than T4, are signiﬁcantly greater
than T1—both the local L1 instruction cache and the L1 in-
struction cache on other cores, and the shared L2 cache are
ﬂushed by the cache invalidation operation.
The diﬀerence of cache ﬂush implementation can be ex-
plained by the diﬀerent implementation of point of coherency
(PoC) and point of uniﬁcation (PoU) on ARM [10]. PoC
speciﬁes the point at which all CPU cores are guaranteed
to observe the same copy of a memory block; PoU speciﬁes
the point at which the data cache and the instruction cache
on the same core are guaranteed to see the same copy of a
memory block. However, ARM does not explicitly specify
the choice of PoU and PoC, leaving them highly implemen-
tation dependent. Our conjecture, therefore, is that on A53
and A57 the PoU is implemented to be the memory, while
on other processors the PoU is speciﬁed as the L2 cache.
sion property with L1 data caches [5, 6], but in all other
cases, these properties are not speciﬁed and therefore are
implementation dependent.
Conclusion. Because the clearcache system call on Cor-
tex A57 and A53 processors will ﬂush instructions to the
main memory, and at the same time the L2 caches on these
processors are inclusive to the instruction cache, these two
ARMv8 processors satisfy all requirements for conducting
Flush-Reload attacks on shared instruction pages. As
they represent the latest processor generations on the mar-
ket, we anticipate future processors may have similar fea-
In this paper, we demonstrate Flush-Reload at-
tures.
tacks on the instruction side of Samsung Galaxy S6.
4. RETURN-ORIENTED FLUSH-RELOAD
ATTACKS ON ARM
In Sec. 3, we have shown that on ARM Cortex A57 and
A53 processors we are constrained to use only instruction
caches in Flush and Reload operations. Hence in this
section, we ﬁrst outline a basic construction of a Flush-
Reload side channel on these processors by Flushing and
Reloading instructions (Sec. 4.1). Then we detail our novel
design of return-oriented Flush-Reload side-channel at-
tacks (Sec. 4.2). We next empirically characterize the pre-
sented return-oriented side channels (Sec. 4.3) and discuss
practical considerations of exploiting such side channels on
Android (Sec. 4.4).
4.1 Basic Flush-Reload Side-Channel Attacks
We ﬁrst describe a basic construction of a Flush-Reload
side-channel attack using the clearcache system call on An-
droid. The side channel works on shared LLCs (i.e., L2
caches). Therefore it can be exploited by an Android app
to attack another running on a diﬀerent CPU core.
The attacker Android app from which side-channel attacks
are conducted is a zero-permission Android app packaged
together with a native library. The Java component of the
app interacts with the native C code through standard Java
Native Interface (JNI). To enable physical memory sharing
between the attacker and victim apps, the native code uses
the dlopen system call to dynamically link a certain shared
library (i.e., so ﬁle) used by the victim app into the attacker
app’s own address space. When the attack starts, the service
component inside the attacker app creates a new thread,
which calls into its native component to conduct Flush-
Reload operations in the background:
• Flush: The attacker app calls clearcache to ﬂush a
• Flush-Reload interval: The attacker app waits for a
ﬁxed time period (may be zero), during which the victim
app may execute the function.
• Reload: The attacker app executes the function and
measures the time of execution. Shorter execution time
indicates the function has been executed (thus fetched
into the L2 cache) by some other apps (possibly the vic-
tim app) during the Flush-Reload interval.
function in the code section of this shared library.
Inclusiveness. Table 1 suggests all the L2 caches we eval-
uated are inclusive to both L1 data and instruction caches.
This is in line with the limited information available from
ARM oﬃcial documentations: According to ARM speciﬁca-
tions, Cortex-A57 and Cortex-A15 implement strict inclu-
The primary diﬀerence between our work and previous
study [16, 49, 50, 53] is that we exploit the instruction cache,
while prior studies use the data cache. Nevertheless, the
seemingly minor distinction imposes considerable technical
challenges to our attack. First, to call library functions, the
862attacker app needs to re-construct the program semantics
(e.g., preparing parameters, global variables, etc.) before
calling, which is very tedious and does not work for some
functions. Second, the execution time of a function may
vary from one run to another, which makes diﬀerentiating
cache misses and cache hits very challenging in the Reload
phase. This is typically true if the function call also involves
system calls. Third, Flush and Reload take too much
time; many fast victim operations will be missed by such
slow Flush-Reload attacks. To address these challenges,
we next design a return-oriented Flush-Reload attack.
4.2 Return-Oriented Reloads
Instead of calling the entire function in the Reload phase,
we touch upon selected memory blocks of the function code
in a return-oriented manner. Particularly, much similar
to control-ﬂow hijacking attacks using return-oriented pro-
gramming [18, 41], a number of small gadgets are collected
from the target function, and then in our attacker app, an
auxiliary function will be constructed to jump to these gad-
gets (and then jump back) one after another. The overall
execution time of these gadgets will be measured as the out-
come of the Reload phase. It is important to avoid having