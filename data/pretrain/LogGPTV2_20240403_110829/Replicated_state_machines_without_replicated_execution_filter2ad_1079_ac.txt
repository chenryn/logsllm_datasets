and reads required to execute x. From these, the veriﬁer com-
(cid:2) are
putes d
the sets of all writes and reads after executing x.
(cid:2)) = d ⊕ δx, where WS
(cid:2)) (cid:9) H(RS
(cid:2) = H(WS
(cid:2), RS
(cid:2) = WS
(cid:2), there are sets S, S
(cid:2) − S and B = S − S
(cid:2). Concretely, S = WS − RS and S
We now make a few new observations. If the veriﬁer tracks
(cid:2) such that H(S) = d ∧
both d and d
H(S
(cid:2) − RS
(cid:2).
(cid:2)) = d
(cid:2), and observe that
Deﬁne the sets A = S
(cid:2) − S = A − B. Furthermore, observe that A is the minimal
S
set of writes that must be applied to S to get S(cid:2), and B is
the set of stale writes that A overwrites. This opens up the
following solution for the veriﬁer to efﬁciently receive and
verify state changes from the prover.
Piperine’s prover sets Δ = A, which is minimal (as noted
above). Furthermore, Piperine’s prover proves that ∃B, a set
writes to a subset of the state written to by A and that the
(cid:2) (cid:9) d = H(A)(cid:9)H(B). Piperine’s
following condition holds: d
prover proves this efﬁciently by adapting techniques used to
efﬁciently produce πaudit. More concretely, the prover proves
the correct execution of the program, ΨΔ (depicted in Fig-
(cid:2),
ure 2). ΨΔ takes as public input two digests of state d, d
and purported set of writes Δ, and takes as non-deterministic
input a set of overwritten values. It then checks that these state
changes are consistent with the updated digest. The veriﬁer
simply veriﬁes the proof of correct execution of ΨΔ (i.e., πΔ)
and applies the claimed state changes to its local state.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:37:25 UTC from IEEE Xplore.  Restrictions apply. 
123
3.2 Reducing concrete costs
We make a number of additional changes to Piperine’s base
machinery to reduce the veriﬁer’s and prover’s costs.
Replacing exponentiations with hashing. In the proof ma-
chinery that Piperine uses (§2), the cost of verifying a proof
for a computation Ψ scales in the number of inputs and out-
puts of the computation, but not in the computation com-
plexity of Ψ. Concretely, each additional input or output re-
quires the veriﬁer to do one additional modular exponentia-
tion. There is also a ﬁxed cost of three pairing computations.
It is desirable to minimize the number of explicit inputs
and outputs that a computation has (whilst retaining safety).
We apply a prior idea [24] in our context: we observe that
GetBlock and PutBlock primitives from Section 2 enable
any block of data for which V knows a digest to be referenced
by a short cryptographically-binding name in a veriﬁable
way. Speciﬁcally, Piperine replaces the inputs and outputs of
computations with their short cryptographic digests and have
the veriﬁer separately verify the correctness of cryptographic
digests using their full inputs and outputs. Schematically,
Piperine transforms a computation y = f (x) into:
Digest f_wrapped(Digest in_d) {
x = GetBlock(in_d);
y = f(x);
return PutBlock(y);
}
P proves correct execution of fwrapped, and additionally
sends x and y to V. As part of its local checks, V ensures that
the digests passing in and out of fwrapped correspond to x, y.
For V, this replaces a multi-exponentiation of size O(|x| +|y|)
with hash operations that compute with O(|x| + |y|) data.
Choice of the hash function. The remaining question is
how to implement the hashing. Clearly, for the veriﬁer to gain,
the hash function must be cheaper than exponentiation. A
standard hash function (e.g., SHA-256) would be optimal in
this case. However, the prover must compute digests inside
constraints (as part of GetBlock), and executing a typical
hash function would incur ≈ 800 constraints per byte.
This cost is partially addressed in prior work [17, 24, 43,
73] where the GetBlock/PutBlock primitives are based on
the Ajtai’s hash function [6], which costs ≈ 10 constraints per
byte. In Piperine, we use the MiMC-based hash function [8]
(used in Spice [73] for a different purpose), which costs ≈ 5
constraints per byte.
Efﬁcient signature veriﬁcation. The cost of proof genera-
tion in Piperine’s proof machinery is primarily due to FFTs
and multi-exponentiations whose size is given by the number
of constraints. So reducing the number of constraints used to
represent a computation reduces prover costs. In our target
state machines, most constraints are used to implement cryp-
tographic operations, such as digital signature veriﬁcation.
Common digital signature algorithms compute over a group
where the discrete logarithm problem is hard. These groups in
(cid:2)
turn require arithmetic over large ﬁnite ﬁelds. A prior idea [17,
35, 73] to make this efﬁcient is to ensure that the ﬁeld over
which digital signature is computed is the same ﬁeld used by
our algebraic constraints. Thus, we select digital signatures on
an elliptic curve over the ﬁeld Fp of our algebraic constraints.
There are, however, many elliptic curves over Fp. We
choose a Twisted Edwards curve [19] to avoid branching
when computing a point addition. This is because the con-
straints formalism necessitates executing all branches, which
increases costs. Speciﬁcally, we use the curve E : 634670x2 +
y2 = 1 + 634650x2y2, which is birationally equivalent to the
twist of the C∅C∅ curve [19, 53], of size N = |E| ≈ p/4.
We ﬁx a base point G, and construct ECDSA signatures
over E using the MiMC-based hash discussed above. A public
key is a point P ∈ E, and a signature on a message m is a
pair r, t ∈ [0, N). Verifying a signature requires computing
h := hash(m), r
:= (ht)P + (rt)G = t(hP + rG), and
checking that r is the x-coordinate of r
(cid:2) is to compute
hP and rG with a double-and-add algorithm, add these points,
and then multiply by t (again with double-and-add). We now
discuss a series of optimizations. These optimizations are
somewhat standard in the context of high-speed cryptographic
libraries designed to run on a hardware platform such as x86.
Our innovation is in a careful selection and application of
those optimizations for code compiled to constraints. For
context, although the constraints formalism is as general as
x86, it has a completely different cost model for different
operations (e.g. bitwise operations are orders of magnitude
more expensive than 256-bit modular multiplications).
The most straightforward way to compute r
Optimizations. First, we combine the computation of
hP + rG into a single loop of doubling and adding one of
{0, P, G, P + G}; this optimization is called double-scalar
multiplication, a special case of multiexponentiation [62].
(cid:2).
Second, we apply the above idea to a single scalar mul-
tiplication of a point Q; instead of repeatedly doubling and
adding one of {0, Q}, we repeatedly quadruple and add one
of {0, Q, 2Q, 3Q}. This optimization is called 2-bit window-
ing, a special case left-to-right k-ary exponentiation [62]. In
general, we can use larger windows, where the number of
possible summands becomes some 2w > 4. However, in con-
straints, one must encode the selection operation with ∼ 2w
constraints, so w > 2 does not improve further.
Our ﬁnal and most involved change is to compute ht and
rt mod N, which replaces one point multiplication with two
multiplications mod N. However, N (cid:5)= p, and since N2 (cid:14) p
this multiplication will overﬂow if performed naively. We
address this as follows. Given an x, y to multiply, there is
some a such that xy − aN ∈ [0, N). To compute this, we
express x, y, a, N in base B = 286, where each of x, y, a, N has
at most three digits. We then use long multiplication (base B)
to express the product as the sum of products of the digits,
shifted by powers of B. We collect the terms that have been
shifted by a common power of B for both xy and −aN. Each
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:37:25 UTC from IEEE Xplore.  Restrictions apply. 
124
aggregated term is in (−3B2, 3B2). So xy − aN is expressed
as a sum of values of modulus < 3B2, shifted by powers of B.
Since B2 = 2172 (cid:15) p/6, these sums can be computed
exactly modulo p. So we can combine the values (shifted by
powers of B) to ﬁnd xy − aN, checking that no overﬂow will
occur. To do this, we accumulate the most signiﬁcant parts of
the product, multiplying by powers of B only after checking
that the accumulator is below p/B.
Section 6.1 evaluates these optimizations.
Batching. The prover executes a batch of transactions and
provides a proof of correct execution for the batch as a whole.
The veriﬁer veriﬁes a single proof for the entire batch, thereby
amortizing the ﬁxed costs of veriﬁcation over the entire batch.
The prover also amortizes the linear cost of producing πaudit
and πΔ over the entire batch of transactions.
Finally, note that producing πaudit and πΔ requires comput-
ing over the entire state S and the state changes Δ. Building
on Spice [73], we structure these computations as a MapRe-
duce job where each mapper and reducer operates on ﬁxed-
sized chunk of data (this permits the use of a one-time trusted
setup for proof machinery regardless of the size of the prover’s
state; §8). However, different from Spice, Piperine’s prover
does not prove the execution of reducers, but instead the
veriﬁer executes reducers. This is because the reducer’s com-
putation (elliptic curve point additions, equality checks, etc.)
is not worthwhile to be outsourced to the prover.
3.3 Correctness proofs
Recall that safety and liveness properties are properties of se-
quences of states which, respectively, are closed under taking
preﬁxes, or can be preserved under extension [55].
Given an RSM R that replicates a state machine M, Piper-
ine constructs an RSM R(cid:2), which includes: (i) Piperine’s
prover; and (ii) R that replicates Piperine’s veriﬁer. We now
prove that any safety or liveness property that holds in R for
all state machines is preserved in R(cid:2) for all state machines—
except for an error probability of O(), where  is negligible
in the security parameter and is set to 1/2128 in practice. In
more detail, the veriﬁer that is replicated in R(cid:2) (say M(cid:2)) is
a state machine with state (S, d). The transition function of
M(cid:2) takes as input a tuple ((cid:5)y, d
1. Assert(Verify(π, d, d
2. (S, d) ← (S(cid:2), d
Since we allow a probability O() of error, we can con-
dition on events of probability ≥ 1 − . In particular, we
condition on Verify returning false if P does not possess
non-deterministic choices such that the claimed outputs cor-
rect, and the prover knowing no collisions in H(·). Then:
,(cid:5)y) =⇒ ∃σ ∈ Sym(n), x1, . . . xn,S0 . . .Sn :
(1)
(cid:2)) where S(cid:2) = Apply(S, Δ), output y.
(cid:2), π, Δ, πΔ), and executes:
(cid:2),(cid:5)y) ∧ Verify(πΔ, d, d
H(S0) = d ∧ H(Sn) = d
∧i=1...n Ψ(Si−1, xσ(i)) = (Si, yσ(i))
Verify(π, d, d
(cid:2), Δ)).
(cid:2)
(cid:2)
Verify(πΔ, d, d
(cid:2)
, Δ) =⇒ ∃δ : keys(δ) ⊆ keys(Δ)
(cid:2) (cid:9) d
(cid:2)) and outputs (cid:5)y.
∧ H(Δ) (cid:9) H(δ) = d
is in state (S, d) with d = H(S), and
(cid:2)) with outputs (cid:5)y, then with proba-
(cid:2) = H(S(cid:2)), and (2) ∃(cid:5)x : M transitions
(2)
Lemma 3.1. For a state machine M with transition function
Ψ and initial state S0 = S (where d = H(S)), given inputs (cid:5)x,
(cid:2), π, Δ, πΔ) such
an honest prover can produce a tuple ((cid:5)y, d
with current state (S, d) transitions
that the state machine M(cid:2)
to (Apply(S, Δ), d
Proof. By the completeness of the underlying VSM, an hon-
est prover on inputs (cid:5)x and state S can compute the new state
S(cid:2), outputs yi, and state changes Δ such that d := H(S),
(cid:2) := H(S(cid:2)), and π, πΔ pass their veriﬁcation checks, which
d
causes M(cid:2) in state (S, d) to transition to (Apply(S, Δ), d
(cid:2))
and output (cid:5)y.
Lemma 3.2. If M(cid:2)
transitions to a state (S(cid:2), d
bility ≥ 1−O(): (1) d
S → S(cid:2)
Proof. If M(cid:2) transitions, both Verify checks return true. So
(ignoring an O() probability of failure) the prover knows
σ,{xi}i=1...n,{Si}i=0...n and δ.
From the collision resistance of H(·), S = S0, Sn − S0 =
Δ − δ. Since keys(Δ) ⊇ keys(δ), Sn = Apply(S0, Δ), and
so S(cid:2) = Sn, implying d
(cid:2) = H(S(cid:2)). Then from Equation 1,
M transitions S → S(cid:2) on inputs xσ(i), outputting yσ(i).
Theorem 3.1. If R maintains a safety property on M(cid:2)
R(cid:2)
probability of O().
Proof. By Lemma 3.2, M(cid:2) can only transition from (S, d) to
(S(cid:2), d
(cid:2)) outputting (cid:5)y if M can have a sequence of transitions
from S to S(cid:2) outputting (cid:5)y. So any sequence of states (Si, di)
with outputs (cid:5)y of M(cid:2) projects down to a sub-sequence of a
sequence of states S with outputs (cid:5)y of M.
, then
maintains this safety property on M except for an error
on inputs (cid:5)x, outputting (cid:5)y in some order.
Taking preﬁxes of a sequence commutes with this projec-
tion. Furthermore, A is a sub-sequence of a preﬁx of B if and
only if A is a preﬁx of a sub-sequence of B. So if R maintains
some safety property on M(cid:2), then R(cid:2) preserves it on M.
Corollary 3.1. If R maintains a safety property S for all
state machines, then R(cid:2)
maintains S for all state machines,
excepting an error probability O().
Theorem 3.2. If R applied to M maintains a liveness prop-
erty, then R(cid:2)
applied to M maintains this liveness property.
Proof. Any states S of M can be extended to a state (S, d)
for M(cid:2), by setting d = H(S). If R maintains some liveness
property L on M, any sequence of states for M satisfying L
can be extended indeﬁnitely maintaining L.