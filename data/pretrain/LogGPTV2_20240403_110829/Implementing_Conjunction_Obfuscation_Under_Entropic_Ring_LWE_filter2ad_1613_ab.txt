L
others [55], [56].
B. Cyclotomic Rings
Our implementation utilizes cyclotomic polynomial rings
R = Z[x]/(cid:4)xn + 1(cid:5) and Rq = Zq[x]/(cid:4)xn + 1(cid:5), where n
is a power of 2 and q is an integer modulus. The order of
356
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:37:30 UTC from IEEE Xplore.  Restrictions apply. 
∗
(cid:3)
i 0.
The correctness of the encoding scheme is shown in [1].
C. Word Encoding Optimization
The original conjunction obfuscation design of [1] uses one
level for each bit in pattern v ∈ {0, 1, (cid:2)}L. Our ﬁrst design
improvement is to utilize a larger input encoding alphabet
to reduce the multi-linearity degree of the directed encoding
scheme, i.e., use fewer levels than the length of the pattern.
A na¨ıve approach to extend to a larger alphabet would
be to convert words of w bits into base-2w representation
and then generate 2w encoding matrices for each word. This
method would work for short elements ri,b, where i ∈ [L],
b ∈ {0, . . . , 2w − 1}, and L = (cid:8)L/w(cid:9) is the new effective
length of the pattern. However, short elements si,b, which
encode the wildcard information, need to be generated and
assigned in a more complex manner.
To keep track of bit-level wildcards, we introduce wildcard
subpatterns for each word that share the same short element
si,b. Speciﬁcally, we compute a binary mask for each word that
has the wildcard entries set to 1 and all other entries set to
0. Then for every new index b ∈ {0, . . . , 2w − 1} we perform
bitwise AND between b and the mask. If the result is 0 (all
wildcard bits in the word are set to 0), we generate a new
short element si,b. Otherwise, we reuse an existing one. The
pseudocode for this optimization is depicted in Algorithm 7
(Appendix D).
To illustrate the effect of this optimization, consider the case
of 32-bit conjunctions. The binary alphabet encoding method
requires 33 levels of directed encoding. If instead we use 8-bit
words, then the number of directed encoding levels reduces to
5. At the same time, the number of encoding matrices per level
grows from 4 for w = 1 to 512 for w = 8, which increases
the program size. Hence, there is a tradeoff between a lower
multi-linearity degree and the number of encoding matrices,
which both affect the obfuscated program size.
IV. TRAPDOOR SAMPLING
A. Overview and Motivation
The main computational bottleneck of the obfuscation pro-
cedure in the conjunction obfuscation scheme is the preimage
sampling GaussSamp. Also, the dimensions of the encoding
keys and obfuscated program matrices are determined by the
dimension of the lattice trapdoor used for preimage sampling.
Therefore, any advances in this area have a profound effect on
the performance of conjunction obfuscation and many other
program obfuscations schemes.
Our implementation uses a trapdoor sampling approach pro-
posed by Micciancio and Peikert [44] and improved/extended
trapdoor sampling algorithms recently proposed in [61]. In
358
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:37:30 UTC from IEEE Xplore.  Restrictions apply. 
this approach, samples around a target point t in lattice Λ are
generated using an intermediate gadget lattice Gn. The lattice
Λ is ﬁrst mapped to Gn, then a Gaussian sample is generated
in Gn. The sample is then mapped back to Λ. The linear
function T mapping Gn to Λ is used as the trapdoor. The main
challenge of this approach is that the mapping T produces a
lattice point in Λ with an ellipsoidal Gaussian distribution and
covariance dependent on the transformation T . To generate
spherical samples, the authors apply a perturbation technique
that adds noise with complimentary covariance to the target
point t prior to using it as the center for Gn sampling.
From an implementation perspective, this approach decom-
poses the lattice trapdoor sampling GaussSamp procedure into
two phases: 1) a perturbation sampling stage (SamplePZ),
where target-independent perturbation vectors with a covari-
ance matrix deﬁned by the trapdoor mapping T are generated,
and 2) a target-dependent stage (SampleG) where Gaussian
samples are generated from lattice Gn. The ﬁrst phase, usually
referred to as perturbation generation [61], can be performed
ofﬂine as it does not depend on the target point t. The second
stage, referred to as G-sampling [61], is always performed
online as it depends on the target point.
(cid:6)
(cid:5)
n log3 q
The prior Gaussian sampling algorithm introduced in [44]