title:Measuring password guessability for an entire university
author:Michelle L. Mazurek and
Saranga Komanduri and
Timothy Vidas and
Lujo Bauer and
Nicolas Christin and
Lorrie Faith Cranor and
Patrick Gage Kelley and
Richard Shay and
Blase Ur
Measuring Password Guessability for an Entire University
Michelle L. Mazurek, Saranga Komanduri, Timothy Vidas, Lujo Bauer,
Nicolas Christin, Lorrie Faith Cranor, Patrick Gage Kelley(cid:63), Richard Shay, and Blase Ur
Carnegie Mellon University
Pittsburgh, PA
{mmazurek, sarangak, tvidas, lbauer,
nicolasc, lorrie, rshay, bur}@cmu.edu
(cid:63)University of New Mexico
Albequerque, NM
PI:EMAIL
ABSTRACT
Despite considerable research on passwords, empirical studies of
password strength have been limited by lack of access to plaintext
passwords, small data sets, and password sets speciﬁcally collected
for a research study or from low-value accounts. Properties of pass-
words used for high-value accounts thus remain poorly understood.
We ﬁll this gap by studying the single-sign-on passwords used
by over 25,000 faculty, staff, and students at a research university
with a complex password policy. Key aspects of our contributions
rest on our (indirect) access to plaintext passwords. We describe
our data collection methodology, particularly the many precautions
we took to minimize risks to users. We then analyze how guessable
the collected passwords would be during an ofﬂine attack by sub-
jecting them to a state-of-the-art password cracking algorithm. We
discover signiﬁcant correlations between a number of demographic
and behavioral factors and password strength. For example, we ﬁnd
that users associated with the computer science school make pass-
words more than 1.8 times as strong as those of users associated
with the business school. In addition, we ﬁnd that stronger pass-
words are correlated with a higher rate of errors entering them.
We also compare the guessability and other characteristics of the
passwords we analyzed to sets previously collected in controlled
experiments or leaked from low-value accounts. We ﬁnd more con-
sistent similarities between the university passwords and passwords
collected for research studies under similar composition policies
than we do between the university passwords and subsets of pass-
words leaked from low-value accounts that happen to comply with
the same policies.
Categories and Subject Descriptors
D.4.6 [Management Of Computing and Information Systems]:
Security and Protection—Authentication
Keywords
Passwords; authentication; password security
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage, and that copies bear this notice and the full ci-
tation on the ﬁrst page. Copyrights for third-party components of this work must be
honored. For all other uses, contact the owner/author(s). Copyright is held by the
author/owner(s).
CCS’13, November 4–8, 2013, Berlin, Germany.
ACM 978-1-4503-2477-9/13/11.
http://dx.doi.org/10.1145/2508859.2516726.
1.
INTRODUCTION
Researchers have documented the numerous problems of text
passwords for decades — passwords are easy to guess, hard to re-
member, easily stolen, and vulnerable to observation and replay
attacks (e.g., [28, 38]). The research community has invested sig-
niﬁcant effort in alternatives including biometrics, graphical pass-
words, hardware tokens, and federated identity; however, text pass-
words remain the dominant mechanism for authenticating people
to computers, and seem likely to remain that way for the foresee-
able future [5,23]. Better understanding of text passwords therefore
remains important.
Considerable effort has been spent studying the usage and char-
acteristics of passwords (e.g., [13, 17, 34, 35, 45]), but password re-
search is consistently hampered by the difﬁculty in collecting real-
istic data to analyze. Prior password studies all have one or more
of the following drawbacks: very small data sets [36], data from
experimental studies rather than from deployed authentication sys-
tems [31], no access to plaintext passwords [3], self-reported pass-
word information [47], leaked data of questionable validity, or ac-
counts of minimal value [26, 53]. As a result, the important ques-
tion of whether the results apply to real, high-value passwords has
remained open.
In this paper, we study more than 25,000 passwords making up
the entire user base of Carnegie Mellon University (CMU). No-
tably, these passwords are the high-value gatekeeper to most end-
user (i.e., non-administrative) online functions within the univer-
sity, including email, grading systems, transcripts, ﬁnancial data,
health data, payroll, and course content. Furthermore, these pass-
words were created under a password-composition policy among
the stricter of those in common use [18], requiring a minimum of
eight characters and four different character classes. Using indi-
rect access to the plaintext of these passwords, we measure their
strength. In addition, we obtain contextual information from per-
sonnel databases, authentication logs, and a survey about password
creation and management, and correlate these factors with pass-
word strength. To acquire this data, we established a partnership
with the CMU information technology division; the research was
also vetted by our Institutional Review Board (IRB). Our approach
to analyzing this sensitive data securely provides a blueprint for
future research involving security-sensitive data in the wild.
Using this data, we make two important and novel contribu-
tions to the ﬁeld of password research. First, we identify interest-
ing trends in password strength, measured as resistance to ofﬂine
guessing attacks, in which an attacker attempts to recover plain-
text passwords from their hashes [2, 6, 44]. Using statistical meth-
ods adopted from survival analysis, we ﬁnd that users associated
with science and technology colleges within the university make
173passwords more than 1.8 times as strong as those of users associ-
ated with the business school. Perhaps unsurprisingly, strong pass-
words are correlated with higher rates of failed login attempts due
to password errors. Users who report annoyance with CMU’s com-
plex password-composition policy made weaker passwords. For
the ﬁrst time, we are also able to directly investigate whether in-
sights from work based on lower-value passwords also apply to
high-value passwords. For example, we conﬁrm Bonneau’s ﬁnding
that men’s passwords are slightly stronger than women’s [3]. We
also conﬁrm that passwords with more digits, symbols, and upper-
case letters are stronger, and that digits and symbols are least effec-
tive when placed at the end of a password, while uppercase letters
are least effective placed at the beginning.
Our second major contribution is a comparison of our real, high-
value password data with password sets more typically used in
password research. We compare the CMU passwords to passwords
collected in an online study simulating CMU’s password-creation
process, as well as to data from online studies and a self-reported
survey of CMU users discussed in prior work [30,47]. We also con-
sider plaintext and cracked leaked passwords from low-value ac-
counts [2,19,32,43,52]. We show that simulated password sets de-
signed to closely mirror real authentication conditions consistently
provide reasonably accurate substitutes for high-value real-world
passwords, while leaked passwords vary widely in their effective-
ness. This has important implications for passwords research, as
most researchers must choose between leaked sets and experimen-
tal data. In the past, many researchers have chosen leaked sets; our
results show this may be the wrong choice. Taken together, our
approach and results provide a unique understanding of frequently
used, high-value passwords as well as insight for improving future
password research.
2. RELATED WORK
In this section, we review background in two key areas: the types
of password corpora that have been analyzed previously, and efforts
to deﬁne metrics for password strength.
2.1 Password corpora
Due to the sensitive nature of passwords, acquiring high-quality
password corpora for analysis can be difﬁcult. Data sets that have
been used in previous work on passwords have all been non-ideal
in at least one dimension.
Many researchers use password corpora collected from various
security leaks [14, 24, 26, 35, 53, 55]. These corpora tend to be very
large (tens of thousands to millions), and they represent in-use pass-
words selected by users. While this approach has many beneﬁts,
these passwords come with no contextual information about how
they were made or used, and the released lists are difﬁcult to ver-
ify. Furthermore, the largest leaks thus far have come from low-
value accounts with weak password-composition policies, such as
the RockYou gaming website. In addition, if the password ﬁle is
encrypted or hashed, only those passwords that have been success-
fully cracked can be analyzed, biasing the data toward the more
guessable. Other researchers obtain an organization’s hashed or en-
crypted password ﬁle with permission and attempt to crack it [10,
57]. As with leaked password sets, the results are biased toward
more guessable passwords.
Researchers who want to control the circumstances under which
passwords are created often use lab studies. Some of these studies
are small and focus on targeted populations, such as undergradu-
ates [9,41,54]. Others are larger online studies with a more diverse
population [30, 51]. In some cases, users are asked to create pass-
words for a low-value account associated with the study [9, 31].
Other studies have asked students to create passwords for accounts
tied to a class [21, 29]. In contrast to these studies, the passwords
used in our paper are created for high-value accounts, and are used
frequently over a longer period of time.
Rather than collect passwords expressly for an experiment, some
researchers ask users to self-report password information, includ-
ing both password composition details and user sentiment informa-
tion [7,33,37,47,49,58]. While self-reported data can be very use-
ful and can provide a lot of context, it cannot always be considered
reliable, particularly with regard to a sensitive topic like passwords.
Finally, a small number of researchers have been able to work
with large organizations to collect authentic data. Florêncio and
Herley used an opt-in component of the Windows Live toolbar to
collect information when users log into websites [17]. Bonneau
worked with Yahoo! to analyze plaintext passwords entered by their
users [3]. Both studies include very large, reliable samples, as well
as good contextual information. Due to security concerns, how-
ever, in both studies researchers were able to record only extremely
limited information about the content of the passwords, precluding
many types of interesting analyses. In contrast, our paper connects
information about each user with analysis of that user’s plaintext
password. Perhaps closest to our work, Fahl et al. use a within-
subjects study to manually compare passwords users made in lab
and online studies with their actual passwords; they found that
while study passwords do have some problems, they can provide a
reasonable approximation for real passwords if used carefully [16].
While Fahl et al. compare real password data with online and lab
studies, we also compare real password data with commonly used
leaked password sets.
In this paper, we overcome many limitations of past studies. Our
password corpus includes more than 25,000 real passwords, created
by users for frequently used, high-value accounts unrelated to our
research context. We have indirect, yet extensive, access to plain-
text passwords, allowing us to perform more complex and thorough
analyses than was possible for other similarly authentic corpora.
We also collect a signiﬁcant amount of contextual information, in-
cluding demographics, behavioral data, and user sentiment.
2.2 Password cracking and strength metrics
Accurately judging the strength of a password is crucial to un-
derstanding how to improve security. Historically, information en-
tropy has been used to measure password strength, but it may insuf-
ﬁciently capture resistance to intelligent guessing attacks [17, 55].
More recently, researchers have suggested using password guess-
ability, or ability to withstand guessing by a particular password
cracker with particular training data, as a security metric [55]. This
metric has the advantages of modeling knowledge a real-world ad-
versary might have, as well as of bounding the attempts an adver-
sary might make to guess a password, but its results are depen-
dent on the chosen setup. Guess numbers, as deﬁned by Kelley et
al., measure how many guesses it would take a particular crack-
ing algorithm and training setup to reach a given password [30].
Working without access to plaintext passwords, Bonneau suggests
a guessing metric that reﬂects how many passwords an optimal at-
tacker, who knows exactly which passwords to guess and what or-
der to guess them in, will successfully break before guessing the
password under consideration [4]. Several researchers have also
used machine-learning techniques to classify passwords as weak or
strong, based on labeled training data, but these techniques are only
as good as the original classiﬁcation [1, 27, 50].
The guessability metric of password strength dovetails with re-
cent advances in password cracking.
In contrast to prior brute-
force or dictionary-based approaches [48], researchers have begun
174to use deeper insights about the structure of passwords in crack-
ing. For instance, Narayanan and Shmatikov substantially reduce
the search space for attacks by modeling passwords as a character-
level Markov chain using the probabilities of letters in natural lan-
guage [39]. Using passwords leaked from websites like RockYou
and others as training data, Weir creates a probabilistic context-free
grammar for password cracking [56]. In this approach, guesses are
ordered according to their likelihood, based on the frequency of
their character-class structures in the training data, as well as the
frequency of their digit and symbol substrings. This approach has
been shown to be efﬁcient in password cracking [30,57]. In this pa-
per, we primarily use the guessability metric, simulating cracking
using a modiﬁed version of Weir’s algorithm [30].
3. DATA COLLECTION
In this section, we discuss our data sources. First, we review
the university data collected for this study. Second, we discuss our
procedures for ensuring security while working with real data, as
well as the challenges these procedures create for analysis. Third,
we brieﬂy review other data sets that we use as supplemental data
in our analysis. Fourth, we discuss the composition of our sample
and the generalizability of our results.
3.1 University data
We study the passwords used by all of the more than 25,000
faculty, staff, and students of Carnegie Mellon University. These
passwords are used as part of a single-sign-on system that allows
users to access resources like email, tax and payroll statements,
personnel directories, health information, grades and transcripts,
course information, and other restricted university resources.
CMU’s password-composition policy is a complex one, requir-
ing at least one each of upper- and lowercase letters, digits, and
symbols, as well as forbidding a dictionary word. All non-letter
characters are removed and the password is lowercased before it is
checked against a 241,497-word dictionary, unless the password is
greater than 19 characters. The minimum length is eight charac-
ters, and no character can appear more than four times unless the
password is greater than 19 characters in length.
We collect data from four sources:
logs from the university’s
single-sign-on web authentication service, demographic data from
the university’s two personnel databases, responses to a survey ad-
vertised to users immediately after changing their passwords, and
the plaintext passwords themselves. The web logs represent the pe-
riod from January 1, 2012 through July 27, 2012. On July 28, the
university’s authentication infrastructure was replaced, and the logs
from the two systems are incomparable. The personnel databases,
as with most large organizations, are subject to bureaucratic errors
that may cause some data to be incorrect.
Data from all four sources can be correlated using hashed user
IDs (salted with a salt unknown to us, as described below). Plain-
text passwords are divided into two groups: 25,459 passwords be-
longing to users with active CMU accounts, and 17,104 passwords
belonging to users whose accounts were deactivated after they left
the university, but which have not yet been deleted. Hereafter we
refer to these as the CMUactive and CMUinactive sets respectively.
Some of the CMUinactive accounts were created under an earlier
composition policy that required only that passwords contain at
least one character; as result, 1,635 CMUinactive passwords do not
conform to the strict policy described above.
3.2 Working with real data securely
To get access to this hard-to-acquire real data, we spent months
negotiating a process, vetted by both our IRB and information se-
curity ofﬁce, that would allow the university to remain comfortable
about security while also allowing us to perform useful analyses.
Plaintext passwords were made indirectly available to us through
fortunate circumstances, which may not be reproducible in the fu-
ture. The university was using a legacy credential management
system (since abandoned), which, to meet certain functional re-
quirements, reversibly encrypted user passwords, rather than using
salted, hashed records. Researchers were never given access to the
decryption key.
We were required to submit all the analysis software needed to
parse, aggregate, and analyze data from the various data sources for
rigorous code review. Upon approval, the code was transferred to a
physically and digitally isolated computer accessible only to trusted
members of the university’s information security team. Through-
out the process, users were identiﬁed only by a cryptographic hash
of the user ID, created with a secret salt known only to one infor-
mation technology manager.
We were able to consult remotely and sanity-check limited out-
put, but we were never given direct access to passwords or their
guess numbers. We did not have access to the machine on which
the passwords resided — information security personnel ran code
on our behalf. Decrypted plaintext passwords were never stored
in non-volatile memory at any point in the process, and the swap
ﬁle on the target machine was disabled. All analysis results were
personally reviewed by the director of information security to en-
sure they contained no private data. We received only the results of
aggregate analyses, and no information speciﬁc to single accounts.
After ﬁnal analysis, the source data was securely destroyed. The
information security staff, who are not authors of the paper, repre-
sented an independent check on the risks of our analysis.
There was also concern that analyses might open up small seg-
ments of the population to risk of targeted attack. To address this,
categories for demographic factors were combined so that the inter-
section of any two groups from different factors always contained
more than 50 users. In some cases, this required the creation of an
“other” group to combine several small, unrelated categories.
This approach helped to ensure that users were not put at risk, but
it did create some challenges for analysis. We were never allowed
to explore the data directly. For the most part, decisions about what
data to collect and how to analyze it were made far in advance,
without beneﬁt of exploratory data analysis to guide our choices.
To compensate for this, we selected a large set of possibly useful
statistical comparisons; the correspondingly high chance of false
positives forced us to apply strong statistical correction, somewhat
reducing the statistical power of our analysis. To avoid wasting
the time of the information security team members who performed
the analysis, we automated as much of it as possible. The com-
bination of complex calculations with long automation scripts un-
surprisingly led to many bugs; the inevitable differences between
anonymized sample data provided by the information technology
division and the real data led to many more. The result was many
iterations of remote debugging and subsequent code re-audit.