5.3 RQ2: Wake-Word Sensitivity
LeakyPick is designed to detect devices reacting to a specific set
of wake-words. However, as this set may be different for different
device types, a relevant question is to what degree the detection
accuracy is dependent on the presence of the correct wake-words in
the audio probes. To evaluate this aspect, we first tested LeakyPick
on the Online Probing dataset representing a live operative setting
in which a number of audio probes containing actual wake-words
were injected into the environment of an Amazon Echo device
8
Figure 7: Representative examples of LeakyPick 𝑝-values for
audio probes. None of the devices react to the non-wake-
word probe “major” while only the Google Home device
shows reaction for its wake-word “Hey Google”
with the target of trying to trigger a wake-word induced audio
transmission. We used the 𝑡-test as discussed in Sect. 4.1.2 to calcu-
late the 𝑝-value between consecutive samples of packet sequences
𝑇𝑠 and 𝑇𝑝𝑟 of duration 𝑑 = 60 seconds each. The result of this
for 100 time window pairs is shown in Figure 6. As can be seen,
the 𝑝-values for the non-probing (i.e., “idle” time windows) range
between approximately 0.3 and 1, whereas the 𝑝-values for time
windows containing audio probes remain mostly below 0.3. This
shows that given an appropriate 𝑝-value threshold LeakyPick is
able to distinguish between “idle” traffic and audio transmissions.
To further evaluate how sensitive LeakyPick is to the use of the
right wake-words, we compiled a set of audio probes consisting
of the 50 most used English words and a set of nine known wake-
words used by the IoT devices used in our evaluation (shown in
Table 2). The set of audio probes was injected into the devices’
environment and the resulting 𝑝-values for each device evaluated.
We evaluated all devices at the same time with the same parameters,
exactly as it would occur in a smart home scenario where the user
has many devices in listening range. The resulting 𝑝-values for
two representative examples of used audio probes are shown in
Figure 7. The shown audio probes are the randomly-selected word
“major”, which does not correspond to any known wake-word of
any of the tested devices and the Google Home wake-word “Hey
Google”. While these examples are provided to demonstrate the
discriminative ability of our approach, similar results apply also
to other words in the list of tested audio probes. As one can see,
with a 𝑝-value threshold of, e.g., 0.5 the word "major" would not be
considered to activate any of the devices, whereas one can clearly
see that the 𝑝-value for "Hey Google" indicates a reaction by the
Google Home device. From the results we can see that only devices
responsive to a wake-word react to it which in turn can be detected
using the statistical 𝑡-test employed by LeakyPick. This means,
that the same 𝑝-value threshold can be used for any device tested.
It shows that only the device actually reacting to the wake word
exhibits a low enough 𝑝-value to be classified as sending audio
across all other devices. Note that Nest Protect is not shown in
Figure 7, as it was not activated by any of the examined words and
therefore did not transmit any data at all.
0.10.20.30.420.50.60.700.10.20.30.40.50.60.70.80.9100.20.40.60.81TPRFPR360 HubGoogleAlexaSiri00,10,20,30,40,50,60,70,80,9113579111315171921232527293133353739414345474951535557596163656769717375777981838587899193959799p-valueNumber of testBaseline comparison using t-testIdleProbing00,20,40,60,811,2NetatmoWelcomeHive 360Hive ViewNetatmoPresenceSiriAlexaGoogle-Homep-ValueDevice nameProbing with words "major" and "Hey Google"majorHey GoogleLeakyPick: IoT Audio Spy Detector
5.4 RQ3 and RQ4: Real-World Performance
We evaluated LeakyPick on our real-world dataset containing 52
days of operation in residential environments (households) (Ta-
ble 4). In addition to using this dataset to measure the accuracy of
LeakyPick (RQ3), we also compare LeakyPick’s accuracy to that
of machine learning algorithms (RQ4). Recall from Section 3 that
a key research challenge is being able to operate for unknown
devices. Since machine learning algorithms require training on
known devices, they are not appropriate to achieve our goals, as
our approach needs to be able to handle also previously unseen
device-types. That said, we use a trained machine learning algo-
rithm as a baseline, hypothesizing that LeakyPick can perform at
least as well, but without the need for training.
5.4.1 ML Implementation. We tested the performance of several
commonly-used machine learning (ML) algorithms for detecting
audio events in the real-world dataset. We then selected the clas-
sifier with the best performance to compare against the statistical
detection approach used by LeakyPick. We consider both simple ML
algorithms as well as more advanced ensemble (i.e., Bagging and
Boosting) and majority voting-based classifiers. The ML algorithms
tested include XGboost [10], Adaboost [17], RandomForest [5], SVM
with RBF kernel [47], K-NN [2], Logistic Regression, Naïve Bayes,
and Decision Tree classifiers as provided by the the Scikit-learn
ML package [1]. For each classifier, the used hyper-parameters
were tuned using the provided Grid-search and Cross-validation
processes. For constructing features for training we extracted the
sequence of packet lengths (SPL) from the traffic flow and utilized
the tsfresh tool [12] that automatically calculates a large number
of statistical characteristics from a time-ordered sequence of pack-
ets. All experiments were conducted on a laptop that runs Ubuntu
Linux 18.04 with an Intel i7-9750H CPU with 32 GB DDR4 Memory.
5.4.2 Evaluation. For the ML approach, we used 90% of the dataset
for training and 10% for testing. In addition, we conducted a 10-fold
Cross-Validation (CV) on the training data to better evaluate the
performance of the ML classifiers. According to our experiments,
based on CV accuracy, the Random Forest Classifier provided the
best performance on our dataset, achieving 91.0% accuracy (f1-
score) on test data while 10-fold CV accuracy was 90.5%.
We also evaluated LeakyPick as described in Sect. 5.2.2 on the
same real world dataset in order to compare its performance to the
ML-based approach. The results are displayed in Figure 8, showing
the ROC curves for both approaches on the Google Home, Siri Home
Pod and Alexa Echo devices. For 𝑝-value threshold 0.43 LeakyPick
achieves a TPR of 93% with a simultaneous FPR of 7% averaged over
all devices, compared to a best-case TPR of 95% and FPR of 9.7%
for the ML-based classifier for Alexa Echo Dot. We also found that
models are not transferable between voice assistants. For example,
training on Alexa voice traffic and using the model to identify Siri
voice traffic had around 35% precision.
As our evaluation results in Figure 8 show, ML-based models are
indeed able to detect audio events based on the traffic the devices
send out of the network. However, the evaluation also shows that
similar or even better performance can be achieved using a device-
agnostic approach as taken by LeakyPick.
Pre-print of paper to be published at ACSAC2020
acmDOI: 10.1145/3427228.3427277
Figure 8: ROC curves of the ML-based and LeakyPick ap-
proaches on the real-world dataset
Since applying this kind of a profiling approach requires dedi-
cated traffic models to be trained and included in the system for
each device type considered, its practicality in real-world scenarios
is questionable. Due to the very large and ever-growing number
of different types of voice-activated devices, this approach seems
impractical. The approach adopted in LeakyPick can achieve similar
performance without the need to employ pre-trained device-type-
specific detection models for audio event detection, providing it
much wider applicability in a wider range of environments with
diverse audio-activated device types.
5.5 RQ5: Identifying Unknown Wake-Words
To demonstrate LeakyPick’s ability to identify unknown wake-
words, we performed a systematic experiment with Amazon’s
Alexa-enabled Echo Dot. As voice assistants are conceptually simi-
lar, we believe the results can be generalized to other voice-controlled
devices. We configured the Echo Dot to use the standard “Alexa”
wake word (other options include “Computer”, “Amazon”, and
“Echo”). The experiment played different audio inputs, waiting for
two seconds for the visual light-ring indicator of the device to light
up, indicating the device reacted to the input. For each tested audio
input, we recorded the number of attempts that triggered a reaction.
Recall from Section 2 that Alexa-enabled devices have two states
of detection: (1) an offline model on the device, and (2) an online
model. We classify a word to be mistaken as a wake-word when
the word triggers at least the offline model, since this transmits
recorded audio to the cloud.
Results. The Alexa-enabled Echo Dot reliably reacted to 89 words
across multiple rounds of testing. Table 5 (Appendix) shows the full
list of words. To estimate the phonetic distance between these words
and the true wake-word, we used the Metaphone algorithm [35]
to convert the words into a phonetic alphabet based on their pro-
nunciation. The resulting words were then compared with the
Levenshtein distance to “Alexa.” Among the 89 words, 52 have a
phonetic distance of 3 or more. We found that 3 words had a pho-
netic distance of 0, 9 a distance of 1, 25 a distance of 2, 29 a distance
9
00,10,20,30,40,50,60,70,80,9100,20,40,60,81TPRFPRGoogleAlexaSiriAlexa MLGoogle MLSiri MLPre-print of paper to be published at ACSAC2020
acmDOI: 10.1145/3427228.3427277
Richard Mitev, Anna Pazii, Markus Miettinen, William Enck, and Ahmad-Reza Sadeghi
Wake-Word
Table 5: Full results of testing Alexa with English words
Probability of
activating Alexa
2/10
3/10
4/10
5/10
6/10
alita, baxa, elater, hexer, liker, ochna, taxer
bertha, electroceramic, excern, oxer, taxir
electrohydraulic, electropathic, wexler
blacksher, electic, hoaxer
bugsha, elatha, elator, electrodissolution,
electrostenolytic, eloper, eluted, fluxer,
huerta, hurter, irksome, lecher, lefter, lepre,
lesser, letter, licker, lipper, loasa, loker, lotor,
lyssa, maloca, maxillar, melosa, meta, metae,
muleta, paxar, rickner
alexy, crytzer, electroanalytical, hyper,
kleckner, lecture, likker, volupte, wexner
electroreduction, hiper, wechsler
aleta, alexa, alexia, annection, elatcha, elec-
tre, kreitzer
alachah, alexipharmic, alexiteric, alissa,
alosa, alyssa, barranca, beletter, elector,
electra, electroresection, electrotelegraphic,
elissa, elixir, gloeckner, lechner, lecter, lictor,
lxi, lxx, mixer, olexa, walesa
7/10
8/10
9/10
10/10
of 3, 14 a distance of 4, 2 a distance of 5 and 6, 4 of 7 and one even
a distance of 8. These distances shows that the Echo Dot reliably
reacted to words that are phonetically very different than “Alexa.”
Some of the found wake-words can also be spoken by a human
even as part of a sentence and Alexa will be activated. In a smart
home scenario users speaking a sentence including such a word
can mistakenly activate Alexa and therefore stream the following
sentences out of the users home. This shows that those identi-
fied words are one cause of misactivations and therefore lead to
recorded audio from the users home being sent to the cloud and pro-
cessed by computers or even other humans. Based on these findings,
it is unsurprising that Alexa-enabled devices are often triggered
unintentionally, leading to private conversations and audio being
transmitted outside the user’s home.
The full results of testing the Alexa wake-word (Alexa) with
words of the English language dictionary with 6 and 5 phonemes
as well as some random words, is shown in Table 5. The results
shown are the last round of 10 tests for each word. The left column
shows the probability of the device being activated while replaying
10 times the word in question.
6 DISCUSSION
Burst Detector: A malicious audio bug device whose sole purpose
is to eavesdrop on a victim may use extensive lossy audio com-
pression to keep the traffic rate below the detection threshold of
23𝑘𝑏𝑖𝑡/𝑠. However, such audio may not be suitable for automated
voice recognition as many features of the voice are deleted or ex-
changed with noise which impairs the scalability of such an attack
dramatically. However, our statistical probing approach would still
detect a significant difference in the traffic and detect the sent audio.
10
Statistical Probing: As mentioned in Section 2, attacks that issue
commands to a victim’s voice assistant can be detected by LeakyP-
ick. To achieve that, increasing the time traffic samples are acquired
as well as disabling audio probing is needed. By disabling the audio
probing mechanism, every invocation of the device must be done
by an external entity (e.g., the user or an attacker). By increasing
the sample size, it is also possible to distinguish reliably between
an actual invocation and background traffic spikes, even without
the knowledge of when audio is played or not as the 𝑝-values are
different for an invocation and background noise (cf. Figure 4). With
this tweak, LeakyPick would also be able to warn the user of such
attacks. Currently we are investigating into the influence of varying
levels of background noise on the Statistical Probing approach.
Countermeasures against Devices sending Audio: Depending
on whether LeakyPick acts as the gateway of the home network
or is sniffing passively the (encrypted) Wi-Fi traffic, there are dif-
ferent approaches to prevent a device from recording and sending
audio without the user’s permission. If our device is replacing the
gateway, traffic identified as containing audio recordings can be sim-
ply dropped at the network layer. If our device can only passively
sniff encrypted MAC layer traffic, inaudible microphone jamming
techniques could be used to prevent the device from recording
unsuspecting users private conversations [30, 37, 38, 42, 50].
Wake-Word Identification: We found that some of the identified
wake-words for Alexa are only effective if spoken by Google’s TTS
voice, and that we were unable to replicate the effect when spoken
by a human. We believe this may result from features that differ
between the TTS service audio file and natural voice. However, the
goal of the experiment was to demonstrate the extent to which
words are incorrectly interpreted as wake-words, rather than deter-
mining the actual words triggering incorrect wake-word detection.
There may also be wake-words, sounds, or noise our approach
could not find. We are currently investigating whether wrongly
recognized wake-words could be used to attack voice assistants
and other voice recognition applications.
7 RELATED WORK
Existing works discussing detection of inadvertent data transmis-
sions out of a user network have focused on IP cameras. To the best
of our knowledge, there are no published approaches for detecting
outgoing audio traffic for voice assistants and other audio-activated
devices, in particular approaches utilizing audio probing. We are
also not aware of publications utilizing audio probes to determine
if devices react to audio inputs.
The following discussion of related work focuses on existing
attacks on voice assistants and traffic analysis approaches for IoT
device identification and IP camera detection. We also review ap-
proaches to microphone jamming, which can be used by LeakyPick
to prevent microphone-enabled IoT devices to record meaningful
audio when the user is not aware of it.
IP Camera Detection: IP camera detection approaches usually
extract features from packet headers. Wireless cameras in operation
continuously generate camera traffic flows that consist of video
and audio streams. The resulting traffic patterns of IP cameras
are likely to be different and easily distinguishable from that of
other network applications. Furthermore, to save bandwidth, IP
LeakyPick: IoT Audio Spy Detector
cameras utilize variable bit rate (VBR) video compression methods,
like H264. Because of the use of VBR, by changing the scene the