neering (LCE), 384-387
recoverability, 347
driving convergence and simplification, 374
recovery, 359
launch coordination checklists, 373-380,
recovery systems, 345
493
recursion (see recursion)
launch coordination engineering, 370
recursive DNS servers, 225
NORAD Tracks Santa example, 369
recursive separation of responsibilities, 163
overview of, 387
redundancy, 347, 354
processes for, 372
Reed-Solomon erasure codes, 354
rate of, 370
regression tests, 186
techniques for reliable, 380-384
release engineering
production environment (see Google produc‐
challenges of, 87
tion environment)
continuous build and deployment, 90-94
production inconsistencies
defined, 87
detecting with Prodtest, 76
instituting, 95
resolving idempotently, 78
philosophy of, 88-90
production meetings, 426-430
the role of release engineers, 87
agenda example, 497
wider application of, 95
production probes, 202
reliability testing
Production Readiness Review process (see SRE
amount required, 184
engagement model)
benefits of, 204
production tests, 187
break-glass mechanisms, 201
protocol buffers (protobufs), 19, 202
canary tests, 189
518 | Index
configuration tests, 188 resources
coordination of, 197 allocation of, 14, 16
creating test and build environments, 190 exhaustion, 261
error budgets, 8, 33-35, 481 limits, 278
expecting test failure, 199-200 (see also capacity planning)
fake backend versions, 204 restores, 355
goals of, 183 retention, 348
importance of, xvi retries, RPC
integration tests, 186, 201 avoiding, 254
MTTR and, 184 cascading failures due to, 268
performance tests, 186 considerations for automatic, 270
proactive, 159 diagnosing outages due to, 271
production probes, 202 handling overload errors and, 254
production tests, 187 per-client retry budgets, 254
regression tests, 186 per-request retry budgets, 254
reliability goals, 25 reverse engineering, 398
sanity testing, 186 reverse proxies, 157
segregated environments and, 198 revision history, 351
smoke tests, 186 risk management
speed of, 196 balancing risk and innovation, 25
statistical tests, 196 costs of, 25
stress tests, 188 error budgets, 33-36, 481
system tests, 186 key insights, 36
testing at scale, 192-204 measuring service risk, 26
timing of, 187 risk tolerance of services, 28-33
unit tests, 185 rollback procedures, 153
reliable replicated datastores, 292 rollouts, 277, 379, 480
Remote Procedure Call (RPC), 19, 138, 252 root cause
bimodal, 273 analysis of, 105, 169
deadlines (see also postmortems)
missing, 271 defined, 56
propagating, 267, 272 Round Robin policy, 241
queue management, 266, 294 round-trip-time (RTT), 300
selecting, 271 rows, 14
retries, 268-271 rule evaluation, in monitoring systems, 114-118
RPC criticality, 251
(see also overload handling) S
replicas
Safari® Books Online, xxi
adding, 307
sanity testing, 186
drawbacks of leader replicas, 308
saturation, 60
location of, 306, 310
scale
number deployed, 304
defined, 341
replicated logs, 305
issues in, 347
replicated state machine (RSM), 291
security
replication, 347, 354
in release engineering, 89
request latency, 38, 60
new approach to, 106
request profile changes, 277
self-service model, 88
request success rate, 27
separation of responsibilities, 163
resilience testing, 106
servers
Index | 519
vs. clients, 19 Service-Oriented Architecture (SOA), 81
defined, 13 Shakespeare search service, example
overload scenario, 260 alert, 137
preventing overload, 265-276 applying SRE to, 20-22
service availability cascading failure example, 259-283
availability table, 477 debugging, 139
cost factors, 30, 32 engagement, 283, 445
defined, 38 incident management, 485
target for consumer services, 29 postmortem, 487-490
target for infrastructure service, 31 production meeting, 497-499
time-based equation, 26 sharded deployments, 307
types of consumer service failures, 29 SHEDDABLE_PLUS criticality value, 251
types of infrastructure services failures, 32 simplicity, 97-101
service health checks, 281 Sisyphus automation framework, 93
service latency Site Reliability Engineering (SRE)
looser approach to, 31 activities included in, 103
monitoring for, 60 approach to learning, xix
service level agreements (SLAs), 39 basic components of, xv
service level indicators (SLIs) benefits of, 6
aggregating raw measurements, 41 challenges of, 6
collecting indicators, 41 defined, xiii-xiv, 5
defined, 38 early engineers, xvii
standardizing indicators, 43 Google’s approach to management, 5-7, 425
service level objectives (SLOs) growth of at Google, 473, 474
agreements in practice, 47 hiring, 5, 391
best practices for, 480 origins of, xvi
choosing, 37-39 sysadmin approach to management, 3, 67
control measures, 46 team composition and skills, 5, 126, 473
defined, 38 tenets of, 7-12
defining objectives, 43 typical activities of, 52
selecting relevant indicators, 40 widespread applications of, xvi
statistical fallacies and, 43 slow startup, 274
target selection, 45 smoke tests, 186
user expectations and, 39, 46 SNMP (Simple Networking Monitoring Proto‐
service management col), 111
comprehensive approach to, xvi soft deletion, 350
Google’s approach to, 5-7 software bloat, 99
sysadmin approach to, 3, 67 software engineering in SRE
service reliability hierarchy activities included in, 106
additional resources, 106 Auxon case study, 207-209
capacity planning, 105 benefits of, 222
development, 106 encouraging, 215
diagram of, 103 fostering, 218
incident response, 104 Google's focus on, 205
monitoring, 104 importance of, 205
product launch, 106 intent-based capacity planning, 209-218
root cause analysis, 105 staffing and development time, 219
testing, 105 team dynamics, 218
service unavailability, 264 software fault tolerance, 34
520 | Index
software simplicity T
avoiding bloat, 99 tagging, 180
modularity, 100 “task overloaded” errors, 253
predictability and, 98 tasks
release simplicity, 100 backend, 231
reliability and, 101 client, 231
source code purges, 98 defined, 16
system stability versus agility, 97 TCP/IP communication protocol, 300
writing minimal APIs, 99 team building
Spanner, 17, 32, 337 benefits of Google's approach to, 6, 473
SRE engagement model best practices for, 483
aspects addressed by, 443 development focus, 6
Early Engagement Model, 448-451 dynamics of SRE software engineering, 218
frameworks and platforms in, 451-455 eliminating complexity, 98
importance of, 441 engineering focus, 6, 7, 52, 126-127, 474
Production Readiness Review, 442, 444-448 multi-site teams, 127
SRE tools self-sufficiency, 88
automation tools, 194 skills needed, 5
barrier tools, 193, 195 staffing and development time, 219
disaster recovery tools, 195 team composition, 5
testing, 193 terminology (Google-specific)
writing, 202 campuses, 14
SRE Way, 12 clients, 19
stability vs. agility, 97 clusters, 14
(see also software simplicity) datacenters, 14
stable leaders, 302 frontend/backend, 19
statistical tests, 196 jobs, 16
storage stack, 16 machines, 13
stress tests, 188 protocol buffers (protobufs), 19
strong leader process, 297 racks, 14
Stubby, 19 rows, 14
subsetting servers, 13, 19
defined, 235 tasks, 16
deterministic, 238 test environments, 190
process of, 235 (see also reliability testing)
random, 237 test-induced emergencies, 152
selecting subsets, 236 testing (see reliability testing)
synchronous consensus, 289 text logs, 138
sysadmins (systems administrators), 3, 67 thread starvation, 263
system software throttling
managing failures with, 15 adaptive, 250
managing machines, 15 client-side, 249
storage, 16 “thundering herd” problems, 331, 383
system tests, 186 time-based availability equation, 26, 477
system throughput, 38 Time-Series Database (TSDB), 112
systems administrators (sysadmins), 3, 67 time-series monitoring
systems engineering, 390 alerting, 118
black-box monitoring, 120
Borgmon monitoring system, 108
Index | 521
collection of exported data, 110 U
instrumentation of applications, 109 unit tests, 185
maintaining Borgmon configuration, 121 UNIX pipe, 327
monitoring topology, 119 unplanned downtime, 26
practical approach to, 108 uptime, 341
rule evaluation, 114-118 user requests
scaling, 122 criticality values assigned to, 251
time-series data storage, 111-113 job and data organization, 22
tools for, 108 monitoring failures, 60
time-to-live (TTL), 225 request latency, 38
timestamps, 292 request latency monitoring, 60
toil retrying, 254
benefits of limiting, 24, 51 servicing of, 21
calculating, 51 success rate metrics, 27
characteristics of, 49 traffic analysis, 22, 60
cross-industry lessons, 467 utilization signals, 253
defined, 49
drawbacks of, 52 V
vs. engineering work, 52
variable expressions, 113
traffic analysis, 21-22, 60
vectors, 112
training practices, 392, 395-397
velocity, 341
triage process, 137
Viceroy project, 432-437
Trivial File Transfer Protocol (TFTP), 157
virtual IP addresses (VIPs), 227
troubleshooting
App Engine case study, 146-149
W
approaches to, 133
common pitfalls, 135 “War Rooms”, 164
curing issues, 145 Weighted Round Robin policy, 245
diagnosing issues, 139-142 Wheel of Misfortune exercise, 173
examining system components, 138 white-box monitoring, 55, 59, 108
logging, 138 workloads, 296
model of, 134
pitfalls, 135-136 Y
problem reports, 136 yield, 38
process diagram, 135 YouTube, 29
simplifying, 150
systematic approach to, 150 Z
testing and treating issues, 142-145
Zab consensus, 302
triage, 137
Zookeeper, 291
turndown automation, 157, 277
typographical conventions, xix
522 | Index
About the Authors
Betsy Beyer is a Technical Writer for Google in New York City specializing in Site
Reliability Engineering. She has previously written documentation for Google’s Data
Center and Hardware Operations Teams in Mountain View and across its globally
distributed datacenters. Before moving to New York, Betsy was a lecturer on technical
writing at Stanford University. En route to her current career, Betsy studied Interna‐
tional Relations and English Literature, and holds degrees from Stanford and Tulane.
Chris Jones is a Site Reliability Engineer for Google App Engine, a cloud platform-as-
a-service product serving over 28 billion requests per day. Based in San Francisco, he
has previously been responsible for the care and feeding of Google’s advertising statis‐
tics, data warehousing, and customer support systems. In other lives, Chris has
worked in academic IT, analyzed data for political campaigns, and engaged in some
light BSD kernel hacking, picking up degrees in Computer Engineering, Economics,
and Technology Policy along the way. He’s also a licensed professional engineer.
Jennifer Petoff is a Program Manager for Google’s Site Reliability Engineering team
and based in Dublin, Ireland. She has managed large global projects across wide-
ranging domains including scientific research, engineering, human resources, and
advertising operations. Jennifer joined Google after spending eight years in the chem‐
ical industry. She holds a PhD in Chemistry from Stanford University and a BS in
Chemistry and a BA in Psychology from the University of Rochester.
Niall Murphy leads the Ads Site Reliability Engineering team at Google Ireland. He
has been involved in the Internet industry for about 20 years, and is currently chair‐
person of INEX, Ireland’s peering hub. He is the author or coauthor of a number of
technical papers and/or books, including IPv6 Network Administration for O’Reilly,
and a number of RFCs. He is currently cowriting a history of the Internet in Ireland,
and is the holder of degrees in Computer Science, Mathematics, and Poetry Studies,
which is surely some kind of mistake. He lives in Dublin with his wife and two sons.
Colophon
The animal on the cover of Site Reliability Engineering is the ornate monitor lizard, a
reptile native to West and Middle Africa. Until 1997, it was considered a subspecies of
the Nile monitor lizard (Varanus niloticus), but is now classified as a polymorph of
both Varanus stellatus and Varanus niloticus due to its different skin patterns. It also
has a smaller range than the Nile monitor, preferring a habitat of lowland rainforest.
Ornate monitors are large lizards, able to grow up to 6–7 feet long. They are more
brightly colored than Nile monitors, with darker olive skin and fewer bands of bright
yellow spots running from the shoulder to the tail. Like all monitor lizards, this ani‐
mal has a muscular stout body, sharp claws, and an elongated head. Their nostrils are
placed high on their snout, permitting them to spend time in the water. They are
excellent swimmers and climbers, which allows them to sustain a diet of fish, frogs,
eggs, insects, and small mammals.
Monitor lizards are often kept as pets, though they require a lot of care and are not
suitable for beginners. They can be dangerous when they feel threatened (lashing
their powerful tails, scratching, or biting), but it is possible to tame them somewhat
with regular handling and teaching them to associate their keeper’s presence with the
delivery of food.
Many of the animals on O’Reilly covers are endangered; all of them are important to
the world. To learn more about how you can help, go to animals.oreilly.com.
The cover image is from Brockhaus Lexicon. The cover fonts are URW Typewriter and
Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad
Condensed; and the code font is Dalton Maag’s Ubuntu Mono.
|---|--|--|--|
| 0 |  |  |  |
| 1 |  |  |  |
| 2 |  | Site 
Reliability 
Engineering
HOW GOOGLE RUNS PRODUCTION SYSTEMS  |  |
| 3 |  |  |  |
| 4 | Edited by Betsy Beyer, Chris Jones,  
Jennifer Petoff & Niall Richard Murphy  |  |  |