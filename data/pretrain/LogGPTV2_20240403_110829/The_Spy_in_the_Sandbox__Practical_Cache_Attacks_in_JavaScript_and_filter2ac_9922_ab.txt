reference to their physical memory addresses, which are not
available to unprivileged processes.2 Liu et al. [14] partially
circumvented this problem by assuming that the system is
operating in large page (2MB) mode, in which the lower 21
bits of the physical and virtual addresses are identical, and
by the additional use of an iterative algorithm to resolve the
unknown upper (slice) bits of the cache set index.
In the attack model we consider, the system is not running
in large page mode, but rather in the more common 4KB
page mode, where only the lower 12 bits of the physical
and virtual addresses are identical. To our further diﬃculty,
JavaScript has no notion of pointers, so even the virtual
addresses of our own variables are unknown to us. This
makes it very diﬃcult to provide a deterministic mapping of
memory address to cache sets. Instead, we use the heuristic
algorithm described below.
We assume a victim system with s = 8192 cache sets,
each with l = 12-way associativity. Hund et al. [10] suggest
accessing a contiguous 8MB physical memory eviction buﬀer
for completely invalidating all cache sets in the L3 cache. We
could not allocate such an eviction buﬀer in user-mode; in
fact, the aforementioned work was assisted by a kernel-mode
driver. Instead, we allocated an 8MB byte array in virtual
memory using JavaScript (which was assigned by the OS
into an arbitrary and non-contiguous set of 4KB physical
memory pages), and measured the system-wide eﬀects of
iterating over this buﬀer.
We discovered that access latencies to unrelated variables
in memory increased by a noticeable amount when they were
accessed immediately after iterating through this eviction
buﬀer. We also discovered that the slowdown eﬀect per-
sisted even if we did not access the entire buﬀer, but rather
accessed it in oﬀsets of 1 per every 64 bytes (this behaviour
was recently extended into a full covert channel [17]). How-
ever, it is not immediately clear how to map each of the 131K
oﬀsets we accessed inside this eviction buﬀer into each of the
8192 possible cache sets, since we know neither the physical
memory locations of the various pages of our buﬀer, nor the
mapping function used by our speciﬁc micro-architecture to
assign cache sets to physical memory addresses.
A naive approach to solving this problem would be to ﬁx
an arbitrary “victim” address in memory, and then ﬁnd by
brute force which of the 8MB/64B=131K possible addresses
in the eviction buﬀer are in the same cache set as this vic-
tim address, and as a consequence, within the same cache
set as each other. To carry out the brute-force search, the
attacker iterates over all subsets of size l = 12 of all possi-
ble addresses. For each subset, the attacker checks whether
the subset serves as the eviction set for the victim address
by checking whether accessing this subset slows down subse-
quent accesses to the victim variable. By repeating this pro-
cess 8192 times, each time with a diﬀerent victim address,
the attacker can identify 12 addresses that reside in each
cache set and thereby create the eviction set data structure.
Optimization #1. An immediate application of this
heuristic would take an impractically long time to run. One
simple optimization is to start with a subset containing all
131K possible oﬀsets, then gradually attempt to shrink it
2In Linux, until recently, the mapping between virtual pages
and physical page frames was exposed to unprivileged user
processes through /proc//pagemap [12]. In the lat-
est kernels this is no longer possible [25].
Algorithm 1 Proﬁling a Cache Set.
Let S be the set of currently unmapped page-aligned ad-
dresses, and address x be an arbitrary page-aligned address
in memory.
1. Repeat k times:
(a) Iteratively access all members of S.
(b) Measure t1, the time it takes to access x.
(c) Select a random page s from S and remove it.
(d) Iteratively access all members of S\s.
(e) Measure t2, the time it takes to access x.
(f) If removing s caused the memory access to speed
up considerably (i.e., t1 − t2 > thres), then this
address is part of the same set as x. Place it back
into S.
(g) If removing s did not cause memory access to
speed up considerably, then s is not part of the
same set as x.
2. If |S| = 12, return S. Otherwise report failure.
by removing random elements and checking that the access
latency to the victim address stays high. The ﬁnal data
structure should be of size 12 and contain only the entries
sharing a cache set with the victim variable. Even this opti-
mization, however, is too slow for practical use. Fortunately,
the page frame size of the Intel MMU, as described in Sec-
tion 2.1, could be used to our great advantage. Since virtual
memory is page aligned, the lower 12 bits of each virtual
memory address are identical to the lower 12 bits of each
physical memory address. According to Hund et al., 6 of
these 12 bits are used to uniquely determine the cache set
index [10]. Thus, a particular oﬀset in our eviction buﬀer
can only share a cache set with an oﬀset whose bits 12 to 6
are identical to its own. There are only 8K such oﬀsets in the
8MB eviction buﬀer, speeding up performance considerably.
Optimization #2. Another optimization comes from
the fact that if physical addresses P1 and P2 share a cache
set, then for any value of ∆, physical addresses P1 ⊕ ∆ and
P2⊕ ∆ also share a (possibly diﬀerent) cache set. Since each
4KB block of virtual memory maps to a 4KB block in phys-
ical memory, this implies that discovering a single cache set
can immediately teach us about 63 additional cache sets.
Joined with the discovery that JavaScript allocates large
data buﬀers along page frame boundaries, this ﬁnally leads
to the greedy approach outlined in Algorithm 1.
By running Algorithm 1 multiple times, we gradually cre-
ate eviction sets covering most of the cache, except for those
parts that are accessed by the JavaScript runtime itself. We
note that, in contrast to the eviction sets created by the al-
gorithm of Liu et al. [14], our eviction set is non-canonical :
JavaScript has no notion of pointers, and hence, we cannot
identify which of the CPU’s cache sets correspond to any
particular eviction set we discover. Furthermore, running
the algorithm multiple times on the same system will result
in a diﬀerent mapping each time. This property stems from
the use of traditional 4KB pages instead of large 2MB pages,
and will hold even if the eviction sets are created using na-
tive code and not JavaScript.
1409CPU Model
Micro-arch.
LLC Size
Cache Assoc.
Core i5-2520M
Core i7-2667M
Core i5-3427U
Core i7-3667U
Core i7-4960HQ
Core i7-5557U
Sandy Bridge
Sandy Bridge
Ivy Bridge
Ivy Bridge
Haswell
Broadwell
3MB
4MB
3MB
4MB
6MB
4MB
12-way
16-way
12-way
16-way
12-way
16-way
Table 1: CPUs used to evaluate the performance of
the proﬁling cache set technique (Algorithm 1).
Figure 2: Cumulative performance of the proﬁling
algorithm (Haswell i7-4960HQ).
c u r r e n t E n t r y =
1 // I n v a l i d a t e t h e cache s e t
2 v a r c u r r e n t E n t r y = s t a r t A d d r e s s ;
3 do {
4
5
6 } while ( c u r r e n t E n t r y != s t a r t A d d r e s s ) ;
7
8 // Measure a c c e s s time
9 v a r
s t a r t T i m e = window . p e r f o r m a n c e . now ( ) ;
probeView . g e t U i n t 3 2 ( c u r r e n t E n t r y ) ;
10 c u r r e n t E n t r y =
11
12 v a r endTime
primeView . g e t U i n t 3 2 ( v a r i a b l e T o A c c e s s ) ;
= window . p e r f o r m a n c e . now ( ) ;
Evaluation. We implemented Algorithm 1 in JavaScript
and evaluated it on Intel machines using CPUs from the
Sandy Bridge, Ivy Bridge, and Haswell families, running the
latest versions of Safari and Firefox on Mac OS X v10.10 and
Ubuntu 14.04 LTS, respectively. The setting of the evalua-
tion environment represented a typical web browsing session,
with common applications, such as an email client, calen-
dar, and even a music player running in the background.
The attack code was loaded from an untrusted website into
one tab of a multi-tabbed browsing session. Attacks were
performed when the tab was the foreground tab, when the
browser process was in the foreground but a diﬀerent tab
was the foreground tab, and when the web browser pro-
cess was running in the background. The speciﬁcations of
the CPUs we evaluated are listed in Table 1; the systems
were not conﬁgured to use large pages, but instead were
running with the default 4KB page size. The code snippet
shown above illustrates lines 1.d and 1.e of Algorithm 1,
and demonstrates how we iterate over the eviction set and
measure latencies using JavaScript. The algorithm requires
some additional steps to run under Internet Explorer (IE)
and Chrome, which we describe in Section 6.1.
Figure 3: Probability distribution of access times
for a ﬂushed vs. unﬂushed variable (Haswell i7-
4960HQ).
Figure 2 shows the performance of our proﬁling algorithm
implemented in JavaScript, as evaluated on an Intel i7-4960-
HQ running Firefox 35 for Mac OS X 10.10. We were pleased
to ﬁnd that our approach was able to map more than 25% of
the cache in under 30 seconds of operation, and more than
50% of the cache after 1 minute. On systems with smaller
cache sizes, such as the Sandy Bridge i5-2520M, proﬁling was
even faster, taking less than 10 seconds to proﬁle 50% of the
cache. The proﬁling technique itself is simple to parallelize,
since most of its execution time is spent on data structure
maintenance and only a small part is spent on the actual
invalidate-and-measure portion; multiple worker threads can
prepare several data structures to be measured in parallel,
with the ﬁnal measurement step being carried out by a single
master thread.3 Finally, note that the entire algorithm is
implemented in ∼ 500 lines of JavaScript code.
To verify that Algorithm 1 is capable of identifying cache
sets, we designed an experiment that compares the access
latencies for a ﬂushed and an unﬂushed variable. Figure 3
shows two probability distribution functions comparing the
time required to access a variable that has recently been
ﬂushed from the cache by accessing the eviction set (gray
line), with the time required to access a variable that cur-
rently resides in the L3 cache (black line). The timing mea-
surements were carried out using JavaScript’s high resolu-
tion timer, and thus include the additional delay imposed
by the JavaScript runtime. It is clear that the two distribu-
tions are distinguishable, conﬁrming the correct operation
of our proﬁling method. We further discuss the eﬀects of
background noise on this algorithm in Section 6.3.
3.2 Priming and Probing
Once the attacker identiﬁes an eviction set consisting of
12 entries that share the same cache set, the next goal is to
replace all entries in the cache of the CPU with the elements
of this eviction set. In the case of the probe step, the attacker
has the added goal of precisely measuring the time required
to perform this operation.
3The current revision of the JavaScript speciﬁcation does
not allow multiple worker threads to share a single buﬀer
in memory. An updated speciﬁcation, which supports this
functionality, is currently undergoing a ratiﬁcation process
and is expected to be made oﬃcial by the end of 2015.
0255075100125150010002000300040005000600070008000Time(s)Cachesetsproﬁled05010015000.10.20.30.40.50.60.7AccessLatency(ns)Probabilitydensity1410Algorithm 2 Identifying Interesting Cache Regions.
Let Si be the data structure matched to eviction set i.
• For each set i:
1. Iteratively access all members of Si to prime the
cache set.
2. Measure the time it takes to iteratively access all
members of Si.
3. Perform an interesting operation.
4. Measure once more the time it takes to iteratively
access all members of Si.
5. If performing the interesting operation caused the
access time to slow down considerably, then this
operation is associated with cache set i.
Modern high-performance CPUs are highly out-of-order,
meaning that instructions are not executed by their order
in the program, but rather by the availability of input data.
To ensure the in-order execution of critical code parts, In-
tel provides “memory barrier” functionality through various
instructions, one of which is the (unprivileged) instruction
mfence. As JavaScript code is not capable of running it,
we had to artiﬁcially make sure that the entire eviction set
was actually accessed before the timing measurement code
was run. We did so by accessing the eviction set in the form
of a linked list (as was also suggested by Osvik et al. [19]),
and making the timing measurement code artiﬁcially depen-
dent on the eviction set iteration code. The CPU also has
a stride prefetching feature, which attempts to anticipate
future memory accesses based on regular patterns in past
memory accesses. To avoid the eﬀect of this feature we ran-
domly permute the order of elements in the eviction set. We
also access the eviction set in alternating directions to avoid
an excessive amount of cache misses (see Section 6.2).
A ﬁnal challenge is the issue of timing jitter. In contrast to
native code Prime+Probe attacks, which use an assembler
instruction to measure time, our code uses an interpreted
language API call (Window.Performance.now()), which
is far more likely to be impacted by measurement jitter.
In our experiments we discovered that while some calls to
Window.Performance.now() indeed took much longer to
execute than expected (e.g., milliseconds instead of nanosec-
onds), the proportion of these jittered events was very small
and inconsequential.
3.3 Identifying Interesting Cache Regions
The eviction set allows the attacker to monitor the activity
of arbitrary cache sets. Since the eviction set we receive
is non-canonical, the attacker must correlate the proﬁled
cache sets to data or code locations belonging to the victim.
This learning/classiﬁcation problem was addressed earlier
by Zhang et al. [29] and by Liu et al. [14], where various
machine learning methods were used to derive meaning from
the output of cache latency measurements.
To eﬀectively carry out the learning step, the attacker
needs to induce the victim to perform an action, and then
examine which cache sets were touched by this action, as
formally deﬁned in Algorithm 2.
Finding a function to perform the step (3) of Algorithm 2
was actually quite challenging, due to the limited permis-
sions granted to JavaScript code. This can be contrasted
with the ability of Gorka et al. [2] to trigger kernel code
by invoking sysenter. To carry out this step, we had to
survey the JavaScript runtime and discover function calls
which may trigger interesting behaviour, such as ﬁle access,
network access, memory allocation, etc. We were also inter-
ested in functions that take a relatively short time to run
and leave no background “trails”, such as garbage collection,
which would impact our measurement in step (4). Several
such functions were discovered in a diﬀerent context by Ho et
al. [8]. Since our code will always detect activity caused by
the JavaScript runtime, the high performance timer code,
and other components of the web browser that are running
regardless of the call being executed, we actually call two
similar functions and examine the diﬀerence between the
activity proﬁle of the two evaluations to identify relevant
cache sets. Another approach would be to induce the user
to perform an interesting behaviour (such as pressing a key
on her keyboard). The learning process in this case might
be structured (the attacker knows exactly when the victim
operation was executed), or unstructured (the attacker can
only assume that relatively busy periods of system activity
are due to victim operations). We examine both of these
approaches in the attack we present in Section 5.