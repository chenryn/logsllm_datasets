at time t, the input data and the label are xt and yt correspond-
ingly, where xt is a vector of d-dimension. The following
formula shows how the weight of the model at time t (i.e., wt )
is updated based on the weight obtained from the previous
model (i.e., wt−1):
wt = wt−1 +C−1
t xt(cid:2)yt −xT
t wt−1(cid:3)
where C−1
t
is the inverse of for Ct , and Ct is deﬁned as:
Ct =
t
∑
i=1
xixi +λI
82    23rd International Symposium on Research in Attacks, Intrusions and Defenses
USENIX Association
Note that to calculate C−1
, we do not need to store all previous
data and compute the inverse. Based on the Woodbury formula,
C−1
can also be updated recursively as follows:
t
t
C−1
t = C−1
t−1
−
C−1
t−1xt xT
1+xT
t C−1
t−1
t−1xt
t C−1
The complexity for such an update is O(d2).
To update the ofﬂine learning algorithms, the model needs
to be retrained with all historical data in every iteration.
Although retraining the model with the whole dataset every
time a new seed is coming seems to be time-consuming, we
show in our evaluation the approach is still practical in our
case (§6.3). One reason is that the seed attributes are not of
very high dimension and the number of seeds that need to be
retrained is within an acceptable order of magnitude.
5 Implementation
Among the three components of MEUZZ, two of them are based
on off-the-shelf software. We employ AFL-2.52b [33] for the
fuzzing module and the re-engineered variant of KLEE from
SAVIOR [25] for concolic execution. We develop the coordina-
tor component from scratch in Python in 3,152 SLOC. Below,
we detail the implementation of the major components of the
ML engine, namely feature extraction and label inference.
Feature extraction: As discussed in §4.2, considering the
trade-off between computational complexity and accuracy is
key in feature extraction. Hence, for developing complicated
features, we use a combination of static and dynamic analyses
to ofﬂoad the heavy tasks to compile time as much as possible.
For instance, to extract the bug triggering features, we ﬁrst
instrument the target program with UBSan [15] at compile
time. Then, a reachability analysis based on SVF [50] is used
to extract the number of sanitizer instrumentations that can be
reached from each branch. During runtime, we simply collect
all the triggered branches by replaying the input and add up the
number of reachable instrumentations from these branches.
To extract the feature of undiscovered neighbors, we
record the branches and their neighbors at compile time. This
information is later used to query whether any neighbor of a
triggered branch is covered. To facilitate fast queries, we store
the neighbor list as a disjoint-set data structure and use the
union-ﬁnd algorithm to query during runtime.
We extract the rest of the features either via compile-time
instrumentation (e.g.,cmp, call instructions) and runtime
input replay or via operating system APIs (i.e., size, queue
size, and new coverage).
Label inference: To collect the size of seed descendant tree,
we traverse AFL’s fuzzing queue. Thanks to the seed naming
system of AFL (i.e., [id, source, mutation, new cov]), we
can iteratively traverse the seeds and use transitive closure to
collect all the inputs imported from the concolic executor and
their descendant trees.
Table 1: Evaluation settings
Name
tcpdump
binutils
binutils
libxml
libtiff
libtiff
jasper
libjpeg
Program
Version
4.10.0
2.32
2.32
2.9.9
4.0.10
4.0.10
2.0.16
jpeg9c
Driver
tcpdump
objdump
readelf
xmllint
tiff2pdf
tiff2ps
jasper
djpeg
Settings
Initial Seeds
Options
[14]
[5]
[5]
[11]
[10]
[10]
[9]
[9]
-r @@
-D @@
-A @@
stdin
@@
@@
-f @@ -T pnm
stdin
6 Evaluation and Analysis
We conduct a comprehensive set of experiments to answer the
following research questions:
• RQ1: Can ML-based seed scheduling outperform
heuristics-based approaches (§ 6.2 and § 6.6)?
• RQ2: Which features are more important in predicting
seed utility and which learning mode is more effective
(§ 6.3)?
• RQ3: Does the learned model adapt well to different
fuzzing conﬁgurations (§ 6.4)?
• RQ4: Is it feasible to transfer the learned model from
a program to other programs to improve fuzzing yields
(§ 6.5)?
6.1 Evaluation setup
Following the general fuzzing evaluation guideline [32],
we choose 8 real-world benchmark programs commonly
used by existing work [19, 24, 25, 54, 55]. Table 1 shows the
conﬁgurations used for fuzzing each program. All experiments
are conducted on AWS c5.18xlarge servers running Ubuntu
16.04 with 72 cores and 281 GB RAM. Without explicitly
mention, all tests run for 24 hours each by assigning three
CPU cores to each fuzzer and are repeated at least 5 times; we
report the average result with Mann-Whitney U-test.
We compare MEUZZ with the state-of-the-art grey-box
fuzzers, such as AFL [33], AFLFast [19], and Angora [24],
as well as hybrid testing systems including QSYM [54] and
SAVIOR [25]. The seed selection modules of all these previous
systems are based on heuristics. We could not test Driller [49]
on the chosen benchmarks because its concolic execution
engine fails to run them. Moreover, we test Vuzzer [41] and
T-Fuzz [39] but we compare them with MEUZZ in a different
way than we do with the other fuzzers. This separate compar-
ison is because these two fuzzers do not support concurrent
fuzzing. Due to the space limit, we discuss our observations
and show the results of their branch coverage in Appendix C.
For MEUZZ, we consider three different conﬁgurations ac-
cording to the learning process, namely MEUZZ-OL, MEUZZ-
RF and MEUZZ-EN, which refer to online learning linear
model, ofﬂine learning random forest model and the arithmetic
average of the ﬁrst two models’ utility predictions, respectively.
Since Savior and QSYM need at least three CPU cores,
we enforce this fuzzing setting to all the fuzzers to build a
USENIX Association
23rd International Symposium on Research in Attacks, Intrusions and Defenses    83
fair comparison environment. We launch one master and two
slaves for the grey-box fuzzers; and one master, one slave, and
one concolic execution engine for the hybrid fuzzers. To reduce
the randomness of OS scheduling, we pin each component of
the fuzzers on the speciﬁc core. Because MEUZZ and SAVIOR
instrument the testing program with UBSAN [15], we also
apply this sanitizer to all other fuzzers, as enabling sanitizers is
shown to improve the fuzzer’s effectiveness for ﬁnding bugs.
6.2 Learning Effectiveness
(a) tcpdump branch coverage (p1=0.071,
p2=0.005, p3=0.082)
(b) objdump branch coverage (p1=0.044,
p2=0.056, p3=8.2 ∗e−4)
(c) libxml branch coverage (p1=0.035,
p2=0.059, p3=0.054)
(d)
coverage
(p1=8.2 ∗e−4, p2=5.6 ∗e−4, p3=6.2 ∗e−5)
tiff2pdf
branch
(e) tiff2ps branch coverage (p1=0.035,
p2=0.091, p3=0.017)
(f) jasper branch coverage (p1=0.037,
p2=0.192, p3=0.015)
(g) readelf branch coverage (p1=0.012,
p2=0.093, p3=8.2 ∗e−4)
(h) djpeg branch coverage (p1=0.072,
p2=0.021, p3=0.093)
Figure 5: Branch coverage fuzzing with valid seeds (higher is
better). p1, p2 and p3 are p-values in Mann-Whitney U Test by
comparing QSYM with MEUZZ-OL, MEUZZ-RF and MEUZZ-EN,
respectively.
Table 2: Execution time spend on different learning stages
Model Update (s)
Online
Ofﬂine
Prediction(s)
Online
Ofﬂine
Feature Extraction (s)
0.000636
0.326139
0.000016
0.003168
5e−6
The most straightforward metric to measure the effective-
ness of MEUZZ is code coverage, which is also a widely
accepted and evaluated metric. Figure 5 shows the branch
coverage achieved by different fuzzers to the required time
for fuzzing. Based on the coverage result, we have several
interesting ﬁndings.
First, MEUZZ covers more code than other fuzzers in most
programs after 24 hours of fuzzing. Among the non-ML
fuzzers, QSYM performs the best in terms of code coverage,
thanks to its efﬁcient concolic execution engine tailored spe-
cially for hybrid fuzzing. Compared with QSYM, the MEUZZ
variants achieve various levels of coverage improvements. In
tcpdump, objdump, readelf and libxml, MEUZZ improves
code coverage over QSYM by more than 10%, and particularly
27.1% by MEUZZ-RF in readelf. In tiff2pdf and tiff2ps,
MEUZZ also has moderate coverage improvements. However,
in jasper and djpeg, there is no much difference between
MEUZZ and QSYM; we speculate it is because all fuzzers are
saturated and hit a plateau after 6 hours.
Second, MEUZZ covers less code in the beginning but grad-
ually surpasses other fuzzers as time progresses. For example,
in objdump MEUZZ-OL and MEUZZ-RF did not cross QSYM
and SAVIOR until after 9.6 hours of fuzzing, but MEUZZ
eventually achieves 14% higher code coverage. Similar situa-
tions can be observed in libxml, readelf and tiff2ps. This
observation is expected, as MEUZZ starts seed scheduling with
random parameters, hence the performance of seed selection
is unpredictable at the beginning. But as time passes, fuzzing
data are increasingly collected and used to reﬁne the prediction
model. Hence, the prediction becomes more accurate.
Lastly, the effectiveness of ML is presented in Figure 10 in
Appendix D. It is shown that different programs are variously
affected by different sets of features. For instance, External
Calls has more inﬂuence on six of the programs except for
tcpdump and djpeg, showing that no single feature is sufﬁ-
cient to predict high-utility seeds. By using a data-driven ap-
proach, we cannot only automatically select the high impactful
features in different programs or situations, but also integrate
them in a more optimal way than manual-crafting rules.
6.3
Insights and Analyses
Online v.s Ofﬂine learning: As mentioned in the previous
section, ofﬂine learning with the random forest model
sometimes beats online learning with the linear model;
however, the main concern with using ofﬂine learning is time
delays, especially during the model updating stage.
To further analyze the effects of time delays caused by
ofﬂine learning, we proﬁle each learning stage during the 24
84    23rd International Symposium on Research in Attacks, Intrusions and Defenses
USENIX Association
024681012141618202224Time (hour)0500010000150002000025000Branch coverageAFLAFLFastAngoraQSYMSAVIORMEUZZ-OLMEUZZ-RFMEUZZ-EN024681012141618202224Time (hour)010002000300040005000600070008000Branch coverageAFLAFLFastAngoraQSYMSAVIORMEUZZ-OLMEUZZ-RFMEUZZ-EN024681012141618202224Time (hour)01000200030004000500060007000Branch coverageAFLAFLFastAngoraQSYMSAVIORMEUZZ-OLMEUZZ-RFMEUZZ-EN024681012141618202224Time (hour)0200040006000800010000Branch coverageAFLAFLFastAngoraQSYMSAVIORMEUZZ-OLMEUZZ-RFMEUZZ-EN024681012141618202224Time (hour)01000200030004000500060007000Branch coverageAFLAFLFastAngoraQSYMSAVIORMEUZZ-OLMEUZZ-RFMEUZZ-EN024681012141618202224Time (hour)0200040006000800010000Branch coverageAFLAFLFastAngoraQSYMSAVIORMEUZZ-OLMEUZZ-RFMEUZZ-EN024681012141618202224Time (hour)02004006008001000120014001600Branch coverageAFLAFLFastAngoraQSYMSAVIORMEUZZ-OLMEUZZ-RFMEUZZ-EN024681012141618202224Time (hour)01000200030004000500060007000Branch coverageAFLAFLFastAngoraQSYMSAVIORMEUZZ-OLMEUZZ-RFMEUZZ-ENcommon trait in all programs uniformly affect the Path Length
feature, which makes the feature more agnostic to programs.
Similarly, New Cov is set to a seed during runtime when it is the
ﬁrst one to trigger new behaviors (e.g., coverage); this attribute
is generally applicable to a variety set of programs.
It is worth noting that the average time to extract each
feature is only 5µs (as shown in Table 2), thanks to our
light-weight feature extractions. This indicates that the
online-friendly requirement is satisﬁed in MEUZZ.
6.4 Model Reusability
(a) tcpdump branch coverage (p1=0.047,
p2=0.018, p3=0.026)
(b) objdump branch coverage (p1=0.051,
p2=2.33 ∗e−3, p3=5.7 ∗e−3)
(c) libxml branch coverage (p1=0.072,
p2=0.032, p3=0.026)
(d) tiff2pdf branch coverage (p1=0.02,
p2=0.03754, p3=5.7 ∗e−3)
(e)
(p1=6.04 ∗e−4, p2=0.012, p3=5.6 ∗e−3)
branch
tiff2ps
coverage
(f) jasper branch coverage (p1=0.264,
p2=0.0268, p3=1.3 ∗e−3)
(g) readelf branch coverage (p1=0.03,
p2=0.072, p3=0.037)
(h) djpeg branch coverage (p1=6.04 ∗e−3,
p2=0.012, p3=3.68 ∗e−3)
Figure 7: Branch coverage fuzzing with naive seeds (higher is
better). p1, p2 and p3 are p-values in Mann-Whitney U Test by
comparing QSYM with MEUZZ-OL, MEUZZ-RF and MEUZZ-EN,
respectively.
Figure 6: The box plots show the importance of the features on nine
programs. The importance is extracted by training an ofﬂine random
forest model and they are ranked by the median of their importance.
Reachable label and New Cov are the most and the least important
ones, respectively.
hours of fuzzing and report the average time spend on different
learning steps. As shown in Table 2, although ofﬂine learning
spent 512x and 198x more time than online learning on
updating the model and making predictions, respectively, the
absolute time-lapse is negligible (i.e., in milliseconds). Hence,
ofﬂine learning is not a critical hindrance throughout the hybrid
fuzzing loop, which endorses the ofﬂine learning effectiveness
discussed in Section 6.2. Having said that, if fuzzing continues
for a longer time and the number of seeds signiﬁcantly
increases, ofﬂine learning can become an obstacle.
Feature Analysis: Figure 6 presents the distribution of the
importance of each feature separately in all programs. The
importance score is computed by capturing the mean decrease
impurity from the ofﬂine random forest models [22]. The
ﬁgure shows the contribution of the New Cov feature is the
least among all the features. While it is difﬁcult to entirely
disregard the minor contribution of New Cov, this suggests
that putting much effort to follow the seeds that bring new
coverage might jeopardize the chance to explore unknown
seeds. This is also known as the famous Multi-Armed Bandit
(MAB) problem [18]. This ﬁnding might shed some light on
the scheduling algorithm implemented in the popular fuzzers
like AFL [33] that heavily rely on the New Cov heuristics.
Also, the variance of change in the ﬁgure shows some of
the features like Path Length and New Cov are less subject to
programs, while others like Reachable Label are more tied
to programs. If the extraction of a feature heavily depends on
static analysis, it is less precise compared with dynamic analy-
sis because the sensitivity of static analysis affects the precision
(i.e., ﬂow/context/ﬁeld sensitivity). We speculate this is one of
the reasons that make a feature (e.g., Reachable Label) more