### Degradation Generation and Testing

In the traversed telephony network, various degradations such as packet loss in iLBC and multi-path fading in GSM are generated and tested. For each generated sample, we append the codec multi-label. All possible resulting speech samples are aggregated into a corpus. The number of samples for each traversal scenario is detailed in Table 2.

### Feature Extraction and Multi-Label Classification

We apply the feature extraction algorithms described in Section 3.3 to each speech sample. Subsequently, we train and test a multi-label classifier using the resulting feature vectors and labels. We utilize Mulan [51], an open-source Java library for multi-label learning, to create our machine learning classifier.

### Classification Results

Multi-label classifiers can employ various reduction techniques, including Binary Relevance (BR), Label Power Set (LP), and Random k-Labelsets (RAkEL) [53], to convert multi-labels into single labels. These labels can then be classified using traditional single-label classifiers. In our study, we use C4.5 decision trees as the underlying single-label classifier, as they outperform other classifiers such as Naive Bayes and Neural Networks.

#### Metrics
- **Hamming Loss**: \(\frac{1}{|D|} \sum_{i=1}^{|D|} \frac{|Y_i \triangle P_i|}{|L|}\)
- **Accuracy**: \(\frac{1}{|D|} \sum_{i=1}^{|D|} \frac{|Y_i \cap P_i|}{|Y_i \cup P_i|}\)
- **Precision**: \(\frac{1}{|D|} \sum_{i=1}^{|D|} \frac{|Y_i \cap P_i|}{|P_i|}\)
- **Recall**: \(\frac{1}{|D|} \sum_{i=1}^{|D|} \frac{|Y_i \cap P_i|}{|Y_i|}\)

Table 3 summarizes the accuracy of the multi-label classifier using C4.5 decision trees. RAkEL outperforms the simpler binary relevance and label power set reduction techniques.

| Metric         | BR   | LP   | RAkEL |
|----------------|------|------|-------|
| Hamming Loss   | 0.09 | 0.10 | 0.05  |
| Accuracy       | 83.7%| 83.7%| 89.3% |
| Precision      | 91.5%| 89.3%| 93.7% |
| Recall         | 90.3%| 89.3%| 97.0% |

### Real-World Testing

The complete provenance fingerprint of a call includes the path traversal signature and profiles for packet loss, concealment, noise, and quality. If this fingerprint remains consistent for a call source, it provides valuable metadata to identify and distinguish different calls purely from the received audio. We conducted live tests with different users making 10 calls to our testbed in Atlanta, GA, from 16 different global locations, including Australia, India, the United Arab Emirates, the United Kingdom, and France. The full list of locations is provided in Figure 11.

Each call lasts approximately 20 seconds. We extract features and profiles from the received audio and label all calls from a call source with the same unique label. We then train a neural network classifier on N sets of the 10 call sets (one call from each source). We vary N from one to five and test with five new call sets. This experiment evaluates the tradeoff between labeling effort and accuracy.

The results show that even if only one set of 16 calls is labeled, the remaining five sets of calls from the 16 different locations are identified with the correct call source label with 90% accuracy. The accuracy increases to 96.25% for two sets, 97.5% for three and four sets, and 100% for five labeled sets. Figure 12 illustrates the confusion matrix for 1, 3, and 5 training sets.

Even with a singly labeled training set, all VoIP calls are correctly identified due to their distinct characteristics. Vonage calls from Atlanta, for example, were distinguishable based on their high spectral level range (noise profile) rather than the packet loss profile. This is likely because Vonage calls transfer to the PSTN backbone for quality of service, while other services predominantly use VoIP.

Figure 12(a) shows that even with a singly labeled training set, we can distinguish between the three landlines from Atlanta, including the two from within the Georgia Tech campus. Three of the five calls from the London mobile phone were misclassified as calls from New York, and one was misclassified as a landline call from France. The number of misclassifications drops significantly from 10 to 3 and then to 2 as the number of training sets increases. With five labeled call sets, there are no misclassifications, indicating that the classifier becomes increasingly accurate with more training data.

### Discussion

#### Limitations

Our call provenance infrastructure is designed to detect codecs and path characteristics associated with a given source. However, there are several limitations:

- Unlike Caller-ID systems, our infrastructure requires the receiver to answer the call before verifying its source.
- We rely heavily on packet loss characteristics, which may need to account for diurnal cycles and temporary anomalies.
- We currently associate a source with a single fingerprint, which may not be suitable for mobile sources.
- Our study focuses on widely used codecs, and less common codecs like AMR and EVRC will be considered in future work.

#### Proposed Extensions and Future Work

We plan to improve the robustness of PinDr0p through several extensions:

- Enhancing resistance to adversaries capable of replicating codecs and path characteristics.
- Determining the order in which codecs are applied to make spooÔ¨Ång attempts more difficult.
- Extending analysis to include a larger number of intermediary networks.

#### Additional Applications

PinDr0p can be used for various applications beyond addressing Caller-ID spoofing, such as:

- Analyzing the prevalence and identity of vishing campaigns.
- Authenticating channels for credit card and home security companies.
- Assisting law enforcement agencies in call forensics.

### Related Work

[Further details on related work can be added here.]

This revised text aims to be more coherent, clear, and professional, with improved structure and flow.