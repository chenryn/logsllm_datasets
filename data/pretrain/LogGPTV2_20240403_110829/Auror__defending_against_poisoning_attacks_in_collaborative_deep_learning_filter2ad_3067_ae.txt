100
20
0
100
38
2
100
69
5
Table 5: Average detection rate (DR), average attack success
rate (SR) and average accuracy drop (AD) for MNIST when
the fraction of malicious data is decreased.
Fraction of
Malicious Users (%) Metrics (%)
20
30
DR
SR
AD
DR
SR
AD
Fraction of
Malicious Data (%)
50
33
31
2
100
16
2
90
100
22
4
100
32
5
70
70
13
2
96
60
2
Table 6: Average Detection rate (DR), average attack success
rate (SR) and average accuracy drop (AD) for GTSRB when
the fraction of malicious data is decreased.
7. RELATED WORK
Collaborative Learning. Various researches have shown the ben-
eﬁts of collaborative learning in the area of machine learning algo-
rithms [28, 45]. With the advent of deep learning, researches have
proposed collaborative setting for deep learning algorithms. Wang
et. al use hierarchical Bayesian model and show that collaborative
deep learning signiﬁcantly improves the state-of-the-art recommen-
dation systems [43]. Xu et. al separate the learning tasks to each
user based on the data locality property, which is compatible with
various learning algorithms and ensure that only ﬁnal result is re-
vealed to other user [44]. To make this setting privacy preserving,
Pathak et. al aggregate a classiﬁer using classiﬁers trained locally
by separate participants [35]. Their model leverages the technique
of differential privacy to hide the information about the training
data. However, simple aggregation generates classiﬁers with very
high accuracy. Recently, Shokri et. al use differential privacy to de-
sign a deep learning model that supports collaboration among users
while preserving the privacy of their training data [39]. Our work
is motivated by such indirect collaborative deep learning models.
Adversarial Learning. Adversarial learning is a problem that has
been studied by researchers for a long time [14, 20, 22, 27]. Re-
searchers divide this problem into two categories by the inﬂuence:
one is the causative attacks or poisoning attacks, the other is the
exploratory attacks. For the poisoning attacks, the adversaries al-
ter the training procedures by polluting the training data, while for
exploratory attacks they modify the samples in order to bypass the
trained classiﬁer. Deep learning systems are also open to these at-
tacks. Various researchers have focused on performing exploratory
attacks on deep learning models and proposed algorithms and de-
fenses for the same. Papernot et. al provide algorithms to craft
adversarial samples such that the ﬁnal model misclassiﬁes speciﬁc
target samples [34]. They exploit the mapping between the in-
put feature and output class so that attackers can efﬁciently evade
the classiﬁers by perturbing the related features. Goodfellow et.
al utilize the gradients to update the input value in order to get
the adversarial samples [21]. Moreover, in a recent work, Paper-
not et al. introduce another attack algorithm that creates an alter-
nate model of the input-output pairs and craft adversarial samples
based on the auxiliary model [33]. All the previous work focus
on exploratory attacks that are perpetrated after the training phase
is complete. However, the emerging collaborative learning tech-
nique opens these deep learning algorithms to poisoning attacks.
Although, poisoning attacks are well understood with respect to
machine learning algorithm for example in recommendation sys-
tems [17, 36, 42], there impact on deep learning systems is unex-
plored. According to our knowledge, this is the ﬁrst work to un-
derstand the efﬁcacy of poisoning attacks in deep learning systems
and provide a concrete defense against it.
Defense. Defenses against poisoning attacks are known in the area
of collaborative recommendation systems using machine learning
techniques [15,19,36]. Biggio et. al [15] examine the effectiveness
of bagging ensembles towards the poisoning attack in spam ﬁlter
and the intrusion detection system. Previous work has considered
data sanitization to measure the negative impact of training dataset
towards a trusted model [29, 32]. Muhlenbach et al. ﬁlter sam-
ples that do not have same class as their neighbors, which needs
access to raw training dataset [30]. All the previous work is tar-
geted towards defending poisoning attacks for machine learning al-
gorithms like k-nearest neighbors (KNN), support vector machines
(SVM) and others. Thwarting poisoning attacks in deep learning
systems, where the ﬁnal model takes into consideration the param-
eters from every node in the neural network of the input dataset is
not known. In this work, we show that detecting poisoned dataset
based on anomalous distribution of the parameters is an effective
and promising solution.
8. CONCLUSION
In this paper, we demonstrate the impact of targeted poisoning
attacks on deep learning systems for two datasets (MNIST and GT-
SRB) in indirect collaborative learning setting. Targeted poisoning
attacks are effective even if the attackers can poison a limited frac-
tion of training data and the ﬁnal model is trained using masked
features from the training data. To thwart against these attack, we
propose AUROR— a statistical defense that exploits the fact that
malicious users can only poison their dataset without the knowl-
edge about the data of other users. Our evaluation conﬁrms that
AUROR is a promising defense against poisoning attacks in indi-
rect collaborative learning.
9. ACKNOWLEDGEMENTS
We thank the anonymous reviewers of this paper for their helpful
feedback. We also thank Shweta Shinde, Viswesh Narayanan and
Yaoqi Jia for useful discussion and feedback on an early version
of the paper. This work is supported by the Ministry of Education,
Singapore under Grant No. R-252-000-560-112 and a university
research grant from Intel. All opinions expressed in this work are
solely those of the authors.
10. REFERENCES
[1] Batman V Superman caught purchasing fake ratings on
IMDB. http://www.bleachbypass.com/
batman-v-superman-fake-imdb-ratings/.
517[2] Facebook’s Moments app uses artiﬁcal intelligence.
http://money.cnn.com/2015/06/15/technology/
facebook-moments-ai/.
[3] Facebook’s Virtual Assistant ‘M’ Is Super Smart. It’s Also
Probably a Human. http://recode.net/2015/11/03/
facebooks-virtual-assistant-m-is-super/
-smart-its-also-probably-a-human/.
[4] How ‘Deep Learning’ Works at Apple, Beyond.
https://www.theinformation.com/
How-Deep-Learning-Works-at-Apple-Beyond.
[5] Improving Photo Search: A Step Across the Semantic Gap.
http://googleresearch.blogspot.sg/2013/06/
improving-photo-search-step-across.html.
[6] Making Cortana smarter: how machine learning is becoming
more dynamic. http://www.techradar.com/sg/news/.
[7] Meet The Guy Who Helped Google Beat Apple’s Siri. http:
//www.forbes.com/sites/roberthof/2013/05/01/
meet-the-guy-who-helped-google-beat-apples-siri/
#7c3a2bda56cb.
[8] Personalized Recommendations Frequently Asked
Questions. http://www.imdb.com/help/show_leaf?
personalrecommendations.
[9] Spam ﬁlter. https://gmail.googleblog.com/2015/
07/the-mail-you-want-not-spam-you-dont.html.
[10] ’The Interview’ Now Has a Perfect 10 Rating on IMDb.
http://motherboard.vice.com/read/
the-interview-has-a-perfect-10-on-imdb.
[11] The mail you want, not the spam you donâ ˘A ´Zt.
https://gmail.googleblog.com/2015/07/
the-mail-you-want-not-spam-you-dont.html.
[12] Theano Package. https://github.com/Theano/Theano.
[13] A. Anjos and S. Marcel. Counter-measures to photo attacks
in face recognition: a public database and a baseline. In
International joint conference on Biometrics (IJCB), pages
1–7. IEEE, 2011.
[14] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and J. D.
Tygar. Can machine learning be secure? In Symposium on
Information, computer and communications security, pages
16–25. ACM, 2006.
[15] B. Biggio, I. Corona, G. Fumera, G. Giacinto, and F. Roli.
Bagging classiﬁers for ﬁghting poisoning attacks in
adversarial classiﬁcation tasks. In Multiple Classiﬁer
Systems, pages 350–359. Springer, 2011.
[16] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Šrndi´c,
P. Laskov, G. Giacinto, and F. Roli. Evasion attacks against
machine learning at test time. In Machine Learning and
Knowledge Discovery in Databases, pages 387–402.
Springer, 2013.
[17] B. Biggio, B. Nelson, and P. Laskov. Poisoning attacks
against support vector machines. In Proceedings of the 29th
International Conference on Machine Learning, 2012.
[18] L. Bottou. Large-scale machine learning with stochastic
gradient descent. In Proceedings of the 19th International
Conference on Computational Statistics, pages 177–186.
Springer, 2010.
[19] Y. Cao and J. Yang. Towards making systems forget with
machine unlearning. In Symposium on Security and Privacy,
pages 463–480. IEEE, 2015.
[20] N. Dalvi, P. Domingos, S. Sanghai, D. Verma, et al.
Adversarial classiﬁcation. In Proceedings of the 10th
SIGKDD international conference on Knowledge discovery
and data mining, pages 99–108. ACM, 2004.
[21] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and
harnessing adversarial examples. In Proceedings of the 3th
International Conference on Learning Representations,
2015.
[22] L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and
J. Tygar. Adversarial machine learning. In Proceedings of the
4th ACM workshop on Security and Artiﬁcial Intelligence,
pages 43–58. ACM, 2011.
[23] W. Jung, S. Kim, and S. Choi. Poster: Deep learning for
zero-day ﬂash malware detection. In 36th IEEE Symposium
on Security and Privacy, 2015.
[24] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner.
Gradient-based learning applied to document recognition. In
Proceedings of the IEEE, volume 86, pages 2278–2324.
IEEE, 1998.
[25] Y. Li, R. Ma, and R. Jiao. A hybrid malicious code detection
method based on deep learning. In International Journal of
Security and Its Applications, volume 9, pages 205–216,
2015.
[26] G. Linden, B. Smith, and J. York. Amazon.com
recommendations: Item-to-item collaborative ﬁltering. In
Internet Computing, IEEE, volume 7, pages 76–80. IEEE,
2003.
[27] D. Lowd and C. Meek. Adversarial learning. In Proceedings
of the 11th ACM SIGKDD International Conference on
Knowledge Discovery in Data Mining, pages 641–647.
ACM, 2005.
[28] L. Melis, G. Danezis, and E. De Cristofaro. Efﬁcient private
statistics with succinct sketches. In Proceedings of the 23rd
Network and Distributed System Security Symposium, 2015.
[29] M. Mozaffari-Kermani, S. Sur-Kolay, A. Raghunathan, and
N. K. Jha. Systematic poisoning attacks on and defenses for
machine learning in healthcare. In IEEE Journal of
Biomedical and Health Informatics, volume 19, pages
1893–1905. IEEE, 2015.
[30] F. Muhlenbach, S. Lallich, and D. A. Zighed. Identifying and
handling mislabelled instances. In Journal of Intelligent
Information Systems, volume 22, pages 89–109. Springer,
2004.
[31] A. Narayanan and V. Shmatikov. Robust de-anonymization
of large sparse datasets. In 29th IEEE Symposium on Security
and Privacy, pages 111–125. IEEE, 2008.
[32] B. Nelson, M. Barreno, F. J. Chi, A. D. Joseph, B. I.
Rubinstein, U. Saini, C. Sutton, J. Tygar, and K. Xia.
Misleading learners: Co-opting your spam ﬁlter. In Machine
learning in cyber trust, pages 17–51. Springer, 2009.
[33] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha,
Z. Berkay Celik, and A. Swami. Practical black-box attacks
against deep learning systems using adversarial examples. In
arXiv preprint arXiv:1602.02697, 2016.
[34] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik,
and A. Swami. The limitations of deep learning in
adversarial settings. In 2016 IEEE European Symposium on
Security and Privacy, pages 372–387. IEEE, 2016.
[35] M. Pathak, S. Rane, and B. Raj. Multiparty differential
privacy via aggregation of locally trained classiﬁers. In
Advances in Neural Information Processing Systems, pages
1876–1884, 2010.
[36] B. I. Rubinstein, B. Nelson, L. Huang, A. D. Joseph, S.-h.
Lau, S. Rao, N. Taft, and J. Tygar. Antidote: understanding
518and defending against poisoning of anomaly detectors. In
Proceedings of the 9th ACM SIGCOMM conference on
Internet measurement conference, pages 1–14. ACM, 2009.
[37] D. Rumelhart, G. Hinton, and R. Williams. Learning internal
representations by error propagation. In Neurocomputing:
foundations of research, pages 673–695. MIT Press, 1988.
[38] J. Schmidhuber. Deep learning in neural networks: An
overview. In Neural Networks, volume 61, pages 85–117.
Elsevier, 2015.
[39] R. Shokri and V. Shmatikov. Privacy-preserving deep
learning. In Proceedings of the 22nd ACM SIGSAC
Conference on Computer and Communications Security,
pages 1310–1321. ACM, 2015.
[40] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. Man vs.
computer: Benchmarking machine learning algorithms for
trafﬁc sign recognition. In Neural Networks, volume 32,
pages 323–332. Elsevier, 2012.
[41] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,
I. Goodfellow, and R. Fergus. Intriguing properties of neural
networks. In Proceedings of the International Conference on
Learning Representations, 2014.
[42] G. Wang, T. Wang, H. Zheng, and B. Y. Zhao. Man vs.
machine: Practical adversarial detection of malicious
crowdsourcing workers. In Proceedings of the 23rd USENIX
conference on Security symposium, pages 239–254, 2014.
[43] H. Wang, N. Wang, and D.-Y. Yeung. Collaborative deep
learning for recommender systems. In Proceedings of the
21th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 1235–1244. ACM, 2015.
[44] K. Xu, H. Ding, L. Guo, and Y. Fang. A secure collaborative
machine learning framework based on data locality. In 2015
IEEE Global Communications Conference (GLOBECOM),
pages 1–5. IEEE, 2015.
[45] K. Xu, H. Yue, L. Guo, Y. Guo, and Y. Fang.
Privacy-preserving machine learning algorithms for big data
systems. In Distributed Computing Systems (ICDCS), 2015
IEEE 35th International Conference on, pages 318–327.
IEEE, 2015.
[46] W. Xu, Y. Qi, and D. Evans. Automatically evading
classiﬁers. In Proceedings of the 2016 Network and
Distributed Systems Symposium, 2016.
519