 
	!


	

  
"














	
	


Fig. 13. Model of the relationship between coverage-increasing test case rate
and UnTracer‚Äôs overhead per test case. For all rates left of the leftmost dashed
vertical line, UnTracer‚Äôs overhead per test case is less than AFL-Clang‚Äôs.
Likewise, for all rates left of the rightmost dashed vertical line, it is less than
AFL-QEMU‚Äôs and AFL-Dyninst‚Äôs. Not shown is the average rate of coverage-
increasing test cases observed during our evaluations (4.92E-3).
increasing test cases; we apply this model to identify the
coverage-increasing test case rates where UnTracer‚Äôs overhead
exceeds AFL-Clang‚Äôs, and AFL-QEMU‚Äôs and AFL-Dyninst‚Äôs.
As shown in Figure 13, for all rates of coverage-increasing test
cases below 2% (the leftmost dashed vertical line), UnTracer‚Äôs
overhead per test case is less than AFL-Clang‚Äôs. Similarly,
UnTracer‚Äôs overhead per test case is less than AFL-QEMU‚Äôs
and AFL-Dyninst‚Äôs for all rates less than 50% (the rightmost
vertical dashed line).
VII. HYBRID FUZZING EVALUATION
State-of-the-art hybrid fuzzers (e.g., Driller
[18] and
QSYM [19]) combine program-directed mutation (e.g., via
concolic execution) with traditional blind mutation (e.g.,
AFL [5]). Hybrid approaches offer signiÔ¨Åcant gains in code
coverage at the cost of reduced test case execution rate. In this
section, we compare UnTracer, Clang [5] (white-box tracing),
and QEMU [5] (black-box dynamically-instrumented tracing)
implementations of the state-of-the-art hybrid fuzzer QSYM
on seven of our eight benchmarks.7 Exploring the beneÔ¨Åt
of UnTracer in a hybrid fuzzing scenario is important as
hybrid fuzzers make a fundamental choice to spend less time
executing test cases (hence tracing) and more time on mutation.
While we provide an estimate of the impact hybrid fuzzing has
on coverage-guided tracing‚Äôs value in Section III, this section
provides concrete data on the impact to UnTracer of a recent
hybrid fuzzer.
test case trimming (trim_case()),
1) Implementing QSYM-UnTracer: We implemented [28]
QSYM-UnTracer in QSYM‚Äôs core AFL-based fuzzer, which
in sev-
tracks coverage (invoked by run_target())
eral contexts:
test
case calibration (calibrate_case()),
test case sav-
ing (save_if_interesting()), hybrid fuzzing sync-
ing (sync_fuzzers()), and the ‚Äúcommon‚Äù context used
for most test cases (common_fuzz_stuff()). Below we
brieÔ¨Çy discuss design choices speciÔ¨Åc to each.
7We exclude sfconvert from this evaluation since the QEMU-based
variant of QSYM crashes on all eight experimental trials.
(cid:24)(cid:26)(cid:25)
Fig. 14. Per-benchmark relative average executions in 24 hours of QSYM-
UnTracer versus QSYM-QEMU and QSYM-Clang.
Trimming and calibration: test case trimming and cali-
bration must be able to identify changes in a priori coverage.
Thus the interest oracle is unsuitable since it only identiÔ¨Åes
new coverage, and we instead utilize only the tracer binary.
Saving timeouts: A sub-procedure of test case saving in-
volves identifying unique timeout-producing and unique hang-
producing test cases by tracing and comparing their coverage
to a global timeout coverage. Since AFL only tracks this infor-
mation for reporting purposes (i.e., timeouts and hangs are not
queued), and using an interest oracle or tracer would ultimately
add unwanted overhead for binaries with many timeouts (e.g.,
djpeg (Table III)), we conÔ¨Ågure UnTracer-AFL, AFL-Clang,
and AFL-QEMU to only track total timeouts.
For all other coverage contexts we implement the UnTracer
interest oracle and tracer execution model as described in
Section V.
A. Evaluation Overview
To identify the performance impact from using UnTracer in
hybrid fuzzing we incorporate it in the state-of-the-art hybrid
fuzzer QSYM and evaluate its against existing Clang- [5] and
QEMU-based [5] QSYM implementations. Our experiments
compare the number of test cases executed for all
three
hybrid fuzzer variants for seven of the eight benchmarks from
Section VI (Table III) with 100ms timeouts. To account for ran-
domness, we average the number of test cases executed from
8, 24-hour trials for each variant/benchmark combination. To
form an average result for each variant across all benchmarks,
we compute a per-variant geometric mean.
We distribute all trials across eight virtual machines among
four workstations. Each host is a six-core Intel Core i7-7800X
CPU @ 3.50GHz with 64GB of RAM that runs two, two-CPU
6GB virtual machines. All eight virtual machines run Ubuntu
16.04 x86 64 (as opposed to 18.04 for previous experiments
due to QSYM requirements). Figure 14 presents the results for
each benchmark and the geometric mean across all benchmarks
scaled to our baseline of the number of test cases executed by
QSYM-QEMU.
B. Performance of UnTracer-based Hybrid Fuzzing
As shown in Figure 14, on average, QSYM-UnTracer
achieves 616% and 79% more test case executions than
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:42:59 UTC from IEEE Xplore.  Restrictions apply. 
QSYM-QEMU and QSYM-Clang, respectively. A potential
problem we considered was the overhead resulting from exces-
sive test case trimming and calibration. Since our implemen-
tation of QSYM-UnTracer defaults to the slow tracer binary
for test case trimming and calibration, an initial problem we
considered was the potential overhead resulting from either
operation. However, our results show that the performance
advantage of interest oracle-based execution (i.e., the ‚Äúcommon
case‚Äù) far outweighs the performance deÔ¨Åcit from trimming
and calibration tracing.
VIII. DISCUSSION
Here we consider several
topics related to our evalua-
tion and implementation. First, we discuss the emergence of
hardware-assisted coverage tracing, offering a literature-based
estimation of its performance with and without coverage-
guided tracing. Second, we detail the modiÔ¨Åcations required
to add basic block edge coverage support to UnTracer and the
likely performance impact of moving to edge-based coverage.
Lastly, we highlight the engineering needed to make UnTracer
fully support black-box binaries.
it
it
A. UnTracer and Intel Processor Trace
Recent work proposes leveraging hardware support for
more efÔ¨Åcient coverage tracing. kAFL [11], PTfuzz [12], and
honggFuzz [4] adapt Intel Processor Trace (IPT) [35] for
black-box binary coverage tracing. IPT saves the control-Ô¨Çow
behavior of a program to a reserved portion of memory as it
executes. After execution, the log of control-Ô¨Çow information
is used in conjunction with an abstract version of the program
to generate coverage information. Because monitoring occurs
at
is possible to completely capture
a program‚Äôs dynamic coverage at the basic block, edge, or
path level incurring modest run time overheads. The three
main limitations of IPT are its requirement of a supporting
processor, time-consuming control-Ô¨Çow log decoding, and its
compatability with only x86 binaries.
the hardware-level,
is important
Despite these limitations,
to understand
how IPT impacts coverage-guided tracing. From a high level,
coverage-guided tracing works with IPT because it
is or-
thogonal
to the tracing mechanism. Thus, an IPT variant
of UnTracer would approach 0% overhead sooner than our
Dyninst-based implementation due to IPT‚Äôs much lower tracing
overhead. From a lower level, the question arises as to the
value of coverage-guided tracing with relatively cheap black-
box binary coverage tracing. To estimate IPT‚Äôs overhead in the
context of our evaluation, we look to previous work. Zhang
et al. [12] present a fuzzing-oriented analysis of IPT that
shows it averaging around 7% overhead relative to AFL-Clang-
fast. Although we cannot use this overhead result directly as
we compile all benchmarks with AFL-Clang, according to
AFL‚Äôs author, AFL-Clang is 10‚Äì100% slower than AFL-Clang-
fast [5]. By applying these overheads to the average overhead
of 36% of AFL-Clang from our evaluation, AFL-Clang-fast‚Äôs
projected overhead is between 18‚Äì32% and IPT‚Äôs projected
overhead is between 19‚Äì35%.
B. Incorporating Edge Coverage Tracking
As discussed in Section II-B, two coverage metrics dom-
inate the fuzzing literature: basic blocks and basic block
edges. UnTracer, our implementation of coverage-guided trac-
ing, uses basic block coverage. Alternatively, many popular
fuzzers (e.g., AFL [5], libFuzzer [6], honggFuzz [4]) use edge
With 
Critical Edge 
B2
B1,B2, 
B3 
B1-B2, 
B2-B3 
B1-B3 
Covered 
Blocks 
Implied 
Edges 
Critical 
Edges 
B1
B3
B4
Without 
Critical Edge 
B1
B3
B2
B1,B2, 
B3 
B1-B2, 
B2-B3 
none 
Covered 
Blocks 
Implied 
Edges 
Critical 
Edges 
Fig. 15. An example of the critical edge problem (left) and its solution
(right). To remove the critical edge B1-B3, an empty ‚Äúdummy‚Äù block (B4)
is inserted to introduce two new edges, B1-B4 and B4-B3. Such approach
is widely used by software compilers to optimize Ô¨Çow analyses [62].
coverage. While the trade-offs between basic block and edge
coverage metrics have yet to be studied with respect to fuzzing
outcomes, we believe that it is important to consider coverage-
guided tracing‚Äôs applicability to edge coverage metrics.
The Ô¨Årst point
to understand is that most fuzzers that
use edge coverage metrics actually rely on basic block-level
tracing [63]. Key to enabling accurate edge coverage while
only tracing basic blocks is the removal of critical edges.
A critical edge is an edge in the control-Ô¨Çow graph whose
starting/ending basic blocks have multiple outgoing/incoming
edges, respectively [62]. Critical edges make it impossible to
identify which edges are covered from knowing only the basic
blocks seen during execution. This inÔ¨Çates coverage and causes
the fuzzer to erroneously discard coverage-increasing inputs.
The solution to the critical edge problem is to split each by
inserting an intermediate basic block, as shown in Figure 15.
The inserted ‚Äúdummy‚Äù basic block consists of a direct control-
Ô¨Çow transfer to the original destination basic block. For
white-box binaries, edge-tracking fuzzers honggFuzz [4] and
libFuzzer [6] Ô¨Åx critical edges during compilation [63]. This
approach works for white-box use cases of coverage-guided
tracing as well. Unfortunately, how to adapt this approach to
black-box binaries is an open technical challenge.
With respect to performance, the impact of moving from
basic block coverage to edge coverage is less clear. It is clear
that, given that edge coverage is a super-set of basic block cov-
erage, the rate of coverage-increasing test cases will increase.
To determine if the increase in the rate of coverage-increasing
test cases is signiÔ¨Åcant enough to disrupt the asymmetry that
gives coverage-guided tracing its performance advantage, we
reference the results in Figure 13 and Table II. Given that
seven out of eight of our benchmarks have rates of coverage-
increasing test cases below 1 in 100,000 and Figure 13
shows that UnTracer provides beneÔ¨Åt for rates below 1 in 50,
moving to edge-based coverage needs to induce a 4-orders-of-
magnitude increase in the rate of coverage-increasing test cases
to undermine UnTracer‚Äôs value. Such an increase is unlikely
given Table II, which shows that even for fuzzers using edge
coverage, the rate of coverage-increasing test cases is in line
with the rates in our evaluation. Thus, given UnTracer‚Äôs near-
0% overhead, we expect
that any increase in the rate of
coverage-increasing test cases due to moving to edge coverage
will not change the high-level result of this paper.
C. Comprehensive Black-Box Binary Support
Niche fuzzing efforts desire support for black-box (source-
unavailable) binary coverage tracing. Currently, UnTracer re-
lies on a mix of black- and white-box binary instrumentation
(cid:24)(cid:26)(cid:26)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:42:59 UTC from IEEE Xplore.  Restrictions apply. 
for constructing its two versions of the target binary. For tracer
binaries, we use Dyninst-based black-box binary rewriting [25]
to insert the forkserver and tracing infrastructure; for oracles,
we re-purpose AFL‚Äôs assembler front-end (afl-as) [5] to
insert the forkserver. As discussed in Section V-B, our initial
implementation used Dyninst to instrument the oracle binary,
but we had to switch at afl-as due to unresolved perfor-
mance issues. Though instrumenting the oracle‚Äôs forkserver at
assembly-time requires assembly code access, we expect that
inserting the forkserver is not a technical challenge for modern
black-box binary rewriters [64], [65], [66], [67] or through
function hooking (e.g., via LD_PRELOAD [68]).
IX. RELATED WORK
Two research areas orthogonal, but, closely related to
coverage-guided tracing are improving test case generation,
because improvements here increase the rate of coverage-
increasing test cases and system optimizations, because they
share the net outcome of improving overall fuzzer perfor-
mance. We overview recent work in each area and relate those
results back to coverage-guided tracing.
A. Improving Test Case Generation
Coverage-guided grey-box fuzzers like AFL [5] and lib-
Fuzzer [6] generally employ ‚Äúblind‚Äù test case generation‚Äî
relying on random mutation, prioritizing coverage-increasing
test cases. A drawback of this strategy is stalled coverage, e.g.,
when mutation fails to produce test cases matching a target
binary‚Äôs magic bytes (multi-byte strings or numbers) compari-
son operations. Research approaches this problem from several
directions: Driller [18] and QSYM [19] use concolic execution
(i.e., a mix of concrete and symbolic execution) to attempt to
solve magic byte comparisons via symbolic path constraints.
As is common with symbolic execution, exponential path
growth becomes a limiting factor as target binary complexity
increases. honggFuzz [4] and VUzzer [7] both leverage static
and dynamic analysis to identify locations and values of
magic bytes in target binaries. Steelix [9] improves coverage
by inferring magic bytes from lighter-weight static analysis
and static instrumentation. Angora [39] incorporates byte-
level taint tracking, outperforming Steelix‚Äôs coverage on the
synthetic LAVA datasets [69]. However, despite seeing higher
rates of coverage-increasing test cases, these fuzzers still face
the overhead of tracing all generated test cases.
Instead of attempting to focus mutation on match magic
byte comparisons all at once, an alternative set of approaches
uses program transformation to make matching more tractable.
AFL-lafIntel [70] unrolls magic bytes into single comparisons
at compile-time, but currently only supports white-box bina-
ries. MutaGen [71] utilizes mutated ‚Äúinput-producing‚Äù code
from the target binary for test case generation, but it relies
on input-producing code availability, and faces slow execution
speed due to dynamic instrumentation. T-Fuzz [47] attempts
to strip target binaries of coverage-stalling code, but suffers
‚Äútransformational explosion‚Äù on complex binaries.
Changes in test case mutation schemes have also offered
potential workarounds to stalled coverage. FidgetyAFL [58],
AFLFast [8], and VUzzer all prioritize mutating test cases ex-
ercising rare basic blocks. Ultimately, coverage-guided fuzzers