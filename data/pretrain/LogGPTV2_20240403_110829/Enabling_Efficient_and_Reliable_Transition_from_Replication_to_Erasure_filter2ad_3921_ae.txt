Number of tolerable rack failures
(d) Varying write request rates
(e) Varying rack-level fault tolerance of EAR
(f) Varying number of replicas
Figure 13.
throughput of EAR over RR.
Experiment B.2: Impact of parameter choices on encoding and write performance under EAR and RR. Each plot denotes the normalized
the simulated performance of RR is actually over-estimated.
We create 20 encoding processes, each of which encodes
50 stripes. We also issue write and background trafﬁc
requests, both of which follow a Poisson distribution with
rate 1 request/s. Each write request writes one 64MB block,
while each background trafﬁc request generates an exponen-
tially distributed size of data with mean 64MB. We set the
ratio of cross-rack to intra-rack background trafﬁc as 1:1.
We consider different parameter conﬁgurations. For each
conﬁguration, we vary one parameter, and obtain the per-
formance over 30 runs with different random seeds. We
normalize the average throughput results of EAR over that
of RR for both encoding and write operations, both of which
are carried out simultaneously. We present the results in
boxplots and show the minimum, lower quartile, median,
upper quartile, maximum, and any outlier over 30 runs.
Figure 13(a) ﬁrst shows the results versus k, while we ﬁx
n − k = 4. A larger k implies less encoding redundancy.
It also means that the cross-rack downloads of data blocks
for encoding become more dominant in RR, so EAR brings
more performance gains. For example, when k = 12, the
encoding and write throughput gains of EAR over RR are
78.7% and 36.8%, respectively.
Figure 13(b) shows the results versus n − k, while we
ﬁx k = 10. A larger n − k means more data redundancy
(i.e., parity blocks). On one hand, since the effective link
bandwidth drops, EAR brings improvements by reducing
cross-rack trafﬁc. On the other hand, the gain of EAR over
RR is offset since both schemes need to write additional
parity blocks. The encoding throughput gain of EAR over
RR remains fairly stable at around 70%, yet
the write
throughput gain of EAR over RR drops from 33.9% to
14.1%.
Figure 13(c) shows the results versus the link bandwidth
of all top-of-rack switches and network core. When the
link bandwidth is more limited, EAR shows higher perfor-
mance gains. The encoding throughput gain of EAR reaches
165.2% when the link bandwidth is only 0.2Gb/s. Note that
the encoding performance trend versus the link bandwidth
is consistent with that of Experiment A.1 obtained from
our testbed. The write throughput gain of EAR remains at
around 20%.
Figure 13(d) shows results versus the arrival rate of write
requests. A larger arrival rate implies less effective link
bandwidth. The encoding throughput gain of EAR over RR
increases to 89.1% when the write request rate grows to
4 requests/s, while the write throughput gain is between 25%
and 28%.
Recall that EAR can vary the rack-level fault tolerance by
the parameter c (see Section III-B). Here, we keep RR to
still provide tolerance against n − k rack failures, while we
vary the rack fault tolerance of EAR. Figure 13(e) shows
the throughput results versus the number of rack failures
tolerated in EAR. By tolerating fewer rack failures, EAR can
157157
)
%
l
(
s
k
c
o
b
f
o
n
o
i
t
r
o
p
o
r
P
6
4
2
0
RR
EAR
RR
EAR
)
%
(
H
15
10
5
0
1
10
Rank of racks
20
100 1000 10000
10
File size (in blocks)
that the data blocks in File F are equally likely to be read,
and the read requests to a data block are equally likely to
be directed to one of the racks that contain a replica of the
block. We deﬁne a hotness index H = max1≤i≤20(L(i)),
where L(i) denotes the proportion of read requests to Rack i,
where 1 ≤ i ≤ 20. Intuitively, we want H to be small to
avoid hot spots. Figure 15 shows H versus the ﬁle size,
which we vary from 10 to 10,000 blocks. Both RR and
EAR have almost identical H.
Figure 14. Experiment C.1: Stor-
age load balancing.
Figure 15. Experiment C.2: Read
load balancing.
VI. RELATED WORK
keep more data/parity blocks in one rack, so it can further
reduce cross-rack trafﬁc. The encoding and write throughput
gains of EAR over RR increase from 70.1% to 82.1% and
from 26.3% to 48.3%, respectively, when we reduce the
number of tolerable rack failures of EAR from four to one.
Finally, Figure 13(f) shows the throughput results versus
the number of replicas per data block. Here, we assume
that each replica is placed in a different rack, as opposed
to the default case where we put
three replicas in two
different racks. Writing more replicas implies less effective
link bandwidth, but the gain of EAR is offset since RR now
downloads less data for encoding. The encoding throughput
gain of EAR over RR is around 70%, while the write
throughput gain decreases from 34.7% to 20.5% when the
number of replicas increases from two to eight.
C. Load Balancing Analysis
One major advantage of RR is that by distributing data
over a uniformly random set of nodes, the CFS achieves
both storage and read load balancing [7]. We now show
via Monte Carlo simulations that although EAR adds extra
restrictions to the random replica placement, it still achieves
a very similar degree of load balancing to RR. In particular,
we focus on rack-level load balancing, and examine how the
replicas are distributed across racks. We consider the replica
placement for a number of blocks on a CFS composed of
R = 20 racks with 20 nodes each. We use 3-way replication,
and the replicas are distributed across two racks as in HDFS
[28]. For EAR, we choose (14, 10) erasure coding. We
obtain the averaged results over 1,000 runs.
Experiment C.1 (Storage load balancing): We ﬁrst ex-
amine the distribution of replicas across racks. We generate
the replicas for 1,000 blocks and distribute them under RR
or EAR. We then count the number of replicas stored in
each rack. Figure 14 shows the proportions of replicas of
RR and EAR in each rack (sorted in descending order of
proportions). We observe that both RR and EAR have very
similar distributions, such that the proportions of blocks
stored in each rack are 4.1∼5.9% for both RR and EAR.
Experiment C.2 (Read load balancing): We also exam-
ine the distribution of read requests across racks. Suppose
Erasure coding in CFSes: Researchers have extensively
studied the applicability of deploying erasure coding in
CFSes. Fan et al. [12] augments HDFS with asynchronous
encoding to signiﬁcantly reduce storage overhead. Zhang et
al. [32] propose to apply erasure coding on the write path
of HDFS, and study the performance impact on various
MapReduce workloads. Li et al. [20] deploy regenerating
codes [10] on HDFS to enable multiple-node failure re-
covery with minimum bandwidth. Silberstein et al. [29]
propose lazy recovery for erasure-coded storage to reduce
bandwidth due to frequent recovery executions. Li et al. [19]
improve MapReduce performance on erasure-coded storage
by scheduling degraded-read map tasks carefully to avoid
bandwidth competition. Enterprises have also deployed era-
sure coding in production CFSes to reduce storage overhead,
with reputable examples including Google [13], Azure [17],
and Facebook [21, 27].
Some studies propose new erasure code constructions and
evaluate their applicability in CFSes. Local repairable codes
are a new family of erasure codes that reduce I/O during
recovery while limiting the number of surviving nodes to
be accessed. Due to the design simplicity, variants of local
repairable codes have been proposed and evaluated based on
an HDFS simulator [23], Azure [17], and Facebook [27].
Piggybacked-RS codes [24, 25] embed parity information
of one Reed-Solomon-coded stripe into that of the follow-
ing stripe, and provably reduce recovery bandwidth while
maintaining the storage efﬁciency of Reed-Solomon codes.
Note that Piggybacked-RS codes have also been evaluated
in Facebook’s clusters. Facebook’s f4 [21] protects failures
at different levels including disks, nodes, and racks, by com-
bining Reed-Solomon-coded stripes to create an additional
XOR-coded stripe.
The above studies (except the work [32]) often assume
asynchronous encoding, and focus on improving the appli-
cability of erasure coding after the replicated data has been
encoded. Our work complements these studies by examin-
ing the performance and availability of the asynchronous
encoding operation itself.
Replica placement in CFSes: Replica placement plays
a critical role in both performance and reliability of CFSes.
By constraining the placement of block replicas to smaller
groups of nodes, the block loss probability can be reduced
158158
with multiple node failures [4, 7]. Scarlett [2] alleviates
hotspots by carefully storing replicas based on workload
patterns. Sinbad [6] identiﬁes the variance of link capacities
in a CFS and improves write performance by avoiding
storing replicas on nodes with congested links. The above
studies mainly focus on replication-based storage, while
our work focuses on how replica placement affects the
performance and reliability of asynchronous encoding.
VII. CONCLUSIONS
Given the importance of deploying erasure coding in
cluster ﬁle systems (CFSes) to reduce storage footprints, this
paper studies the problem of encoding replicated data with
erasure coding in CFSes. We argue that random replication
(RR) brings both performance and availability issues to the
subsequent encoding operation. We thus present encoding-
aware replication (EAR) to take into account erasure coding.
EAR imposes constraints to the replica layout so as to
eliminate both cross-rack downloads and block relocation,
while attempting to place the replicas as uniformly random
as possible. We implement EAR on Facebook’s HDFS
and show its feasibility in real deployment. We conduct
extensive evaluations using testbed experiments, discrete-
event simulations, and load balancing analysis, and show that
EAR achieves throughput gains of both write and encoding
operations, while preserving the even replica distribution,
when compared to RR. In future work, we plan to study
the scenarios with heterogeneous workloads and hardware
resources. The source code of our EAR implementation is
available at http://ansrlab.cse.cuhk.edu.hk/software/ear.
ACKNOWLEDGMENTS
This work was supported in part by grants AoE/E-02/08
and ECS CUHK419212 from the University Grants Com-
mittee of Hong Kong.
REFERENCES
[1] M. Al-Fares, A. Loukissas, and A. Vahdat. A Scalable, Commodity
Data Center Network Architecture. In Proc. of ACM SIGCOMM, Aug
2008.
[2] G. Ananthanarayanan, S. Agarwal, S. Kandula, A. Greenberg, I. Sto-
ica, D. Harlan, and E. Harris. Scarlett: Coping with Skewed Content
Popularity in MapReduce Clusters. In Proc. of ACM EuroSys, Apr
2011.
[3] J. Bloemer, M. Kalfane, R. Karp, M. Karpinski, M. Luby, and
D. Zuckerman. An XOR-Based Erasure-Resilient Coding Scheme.
Technical Report TR-95-048, International Computer Science Insti-
tute, UC Berkeley, Aug 1995.
[4] D. Borthakur, J. Gray, J. S. Sarma, K. Muthukkaruppan, N. Spiegel-
berg, H. Kuang, K. Ranganathan, D. Molkov, A. Menon, S. Rash,
et al. Apache Hadoop goes realtime at Facebook. In Proc. of ACM
SIGMOD, Jun 2011.
[5] B. Calder, J. Wang, A. Ogus, N. Nilakantan, A. Skjolsvold, S. McK-
elvie, Y. Xu, S. Srivastav, J. Wu, H. Simitci, et al. Windows Azure
Storage: A Highly Available Cloud Storage Service with Strong
Consistency. In Proc. of ACM SOSP, Oct 2011.
[6] M. Chowdhury, S. Kandula, and I. Stoica. Leveraging Endpoint
Flexibility in Data-Intensive Clusters. In Proc. of ACM SIGCOMM,
Aug 2013.
[7] A. Cidon, S. Rumble, R. Stutsman, S. Katti, J. Ousterhout, and
M. Rosenblum. Copysets: Reducing the Frequency of Data Loss in
Cloud Storage. In Proc. of USENIX ATC, 2013.
[8] CSIM. http://www.mesquite.com/products/csim20.htm.
[9] J. Dean and S. Ghemawat. MapReduce: Simpliﬁed Data Processing
on Large Clusters. In Proc. of USENIX OSDI, Dec 2004.
[10] A. G. Dimakis, P. B. Godfrey, Y. Wu, M. Wainwright, and K. Ram-
IEEE
chandran. Network Coding for Distributed Storage Systems.
Trans. on Info. Theory, 56(9):4539–4551, Sep 2010.
[11] Facebook’s Hadoop. http://goo.gl/fHDloI.
[12] B. Fan, W. Tantisiriroj, and G. Gibson. Diskreduce: Replication as
a Prelude to Erasure Coding in Data-Intensive Scalable Computing.
Technical Report CMU-PDL-11-112, Carnegie Mellon Univsersity,
Parallel Data Laboratory, Oct 2011.
[13] D. Ford, F. Labelle, F. I. Popovici, M. Stokel, V.-A. Truong, L. Bar-
roso, C. Grimes, and S. Quinlan. Availability in Globally Distributed
Storage Systems. In Proc. of USENIX OSDI, Oct 2010.
[14] S. Ghemawat, H. Gobioff, and S. Leung. The Google File System.
In Proc. of ACM SOSP, Dec 2003.
[15] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C. Kim, P. Lahiri,
D. A. Maltz, P. Patel, and S. Sengupta. VL2: A Scalable and Flexible
Data Center Network. In Proc. of ACM SIGCOMM, Aug 2009.
[16] HDFS-RAID. http://wiki.apache.org/hadoop/HDFS-RAID.
[17] C. Huang, H. Simitci, Y. Xu, A. Ogus, B. Calder, P. Gopalan, J. Li,
In
and S. Yekhanin. Erasure Coding in Windows Azure Storage.
Proc. of USENIX ATC, Jun 2012.
[18] Iperf. https://iperf.fr/.
[19] R. Li, P. P. C. Lee, and Y. Hu. Degraded-First Scheduling for
MapReduce in Erasure-Coded Storage Clusters. In Proc. of IEEE/IFIP
DSN, 2014.
[20] R. Li, J. Lin, and P. P. C. Lee. Enabling Concurrent Failure Recovery
for Regenerating-Coding-Based Storage Systems: From Theory to
Practice. IEEE Trans.on Computers, 2014.
[21] S. Muralidhar, W. Lloyd, S. Roy, C. Hill, E. Lin, W. Liu, S. Pan,
f4: Facebook’s
S. Shankar, V. Sivakumar, L. Tang, and S. Kumar.
Warm BLOB Storage System. In Proc. of USENIX OSDI, 2014.
[22] D. Ongaro, S. M. Rumble, R. Stutsman, J. Ousterhout, and M. Rosen-
blum. Fast Crash Recovery in RAMCloud. In Proc. of ACM SOSP,
2011.
[23] D. Papailiopoulos, J. Luo, A. Dimakis, C. Huang, and J. Li. Simple
Regenerating Codes: Network Coding for Cloud Storage. In Proc. of
IEEE INFOCOM, Mar 2012.
[24] K. V. Rashmi, N. B. Shah, D. Gu, H. Kuang, D. Borthakur, and
K. Ramchandran. A Solution to the Network Challenges of Data
Recovery in Erasure-coded Distributed Storage Systems: A Study on
the FacebookWarehouse Cluster.
In Proc. of USENIX HotStorage,
2013.
[25] K. V. Rashmi, N. B. Shah, D. Gu, H. Kuang, D. Borthakur, and
K. Ramchandran. A ”Hitchhiker’s” Guide to Fast and Efﬁcient Data
In Proc. of ACM
Reconstruction in Erasure-Coded Data Centers.
SIGCOMM, 2014.
[26] I. Reed and G. Solomon. Polynomial Codes over Certain Finite
Fields. Journal of the Society for Industrial and Applied Mathematics,
8(2):300–304, 1960.
[27] M. Sathiamoorthy, M. Asteris, D. Papailiopoulos, A. G. Dimakis,
R. Vadali, S. Chen, and D. Borthakur. XORing Elephants: Novel
Erasure Codes for Big Data. In Proc. of VLDB Endowment, pages
325–336, 2013.
[28] K. Shvachko, H. Kuang, S. Radia, and R. Chansler. The Hadoop
Distributed File System. In Proc. of IEEE MSST, May 2010.
[29] M. Silberstein, L. Ganesh, Y. Wang, L. Alvizi, and M. Dahlin. Lazy
Means Smart: Reducing Repair Bandwidth Costs in Erasure-coded
Distributed Storage. In Proc. of ACM SYSTOR, 2014.
[30] SWIM Project. https://github.com/SWIMProjectUCB/SWIM/wiki.
[31] H. Weatherspoon and J. D. Kubiatowicz.
Replication: A Quantitative Comparison.
2002.
Erasure Coding Vs.
In Proc. of IPTPS, Mar
[32] Z. Zhang, A. Deshpande, X. Ma, E. Thereska, and D. Narayanan.
Does Erasure Coding Have a Role to Play in my Data Center?
Technical Report MSR-TR-2010-52, Microsoft Research, May 2010.
159159