), -1)
b_u = tf.cast(tf.math.ceil(b), tf.int32) # upper
b_uid = tf.concat([index_help, tf.expand_dims(b_u, -1)], 2) # indexes
b_l = tf.cast(tf.math.floor(b), tf.int32)
b_lid = tf.concat([index_help, tf.expand_dims(b_l, -1)], 2) # indexes
b_dist_ = self.targetqnet(b_o_) # whole distribution
b_q_ = tf.reduce_sum(b_dist_ * vrange_broadcast, axis=2)
b_a_ = tf.cast(tf.argmax(b_q_, 1), tf.int32)
b_adist_ = tf.gather_nd( # distribution of b_a_
b_dist_,
tf.concat([tf.reshape(tf.range(batch_size), [-1, 1]),
tf.reshape(b_a_, [-1, 1])], axis=1)
)
b_adist = tf.gather_nd( # distribution of b_a
self.qnet(b_o),
142
参考文献
tf.concat([tf.reshape(tf.range(batch_size), [-1, 1]),
tf.reshape(b_a, [-1, 1])], axis=1)
) + 1e-8
b_l = tf.cast(b_l, tf.float32)
mu = b_adist_ * (b - b_l) * tf.math.log(tf.gather_nd(b_adist, b_uid))
b_u = tf.cast(b_u, tf.float32)
ml = b_adist_ * (b_u - b) * tf.math.log(tf.gather_nd(b_adist, b_lid))
kl_divergence = tf.negative(tf.reduce_sum(mu + ml, axis=1))
return kl_divergence
当然我们在Breakout游戏上，使用3个随机种子运行了107个时间步。图4.8上的蓝色区域
是该方法的输出效果。
参考文献
BELLEMARE M G, NADDAF Y, VENESS J, et al., 2013. The Arcade Learning Environment: An
evaluationplatformforgeneralagents[J]. JournalofArtificialIntelligenceResearch,47: 253-279.
BELLEMARE M G, DABNEY W, MUNOS R, 2017. A distributional perspective on reinforcement
learning[C]//Proceedingsofthe34thInternationalConferenceonMachineLearning-Volume70.JMLR.
org: 449-458.
CASTRO P S, MOITRA S, GELADA C, et al., 2018. Dopamine: A research framework for deep
reinforcementlearning[J].
DABNEY W, OSTROVSKI G, SILVER D, et al., 2018a. Implicit quantile networks for distributional
reinforcementlearning[C]//InternationalConferenceonMachineLearning. 1104-1113.
DABNEYW,ROWLANDM,BELLEMAREMG,etal.,2018b. Distributionalreinforcementlearning
withquantileregression[C]//Thirty-SecondAAAIConferenceonArtificialIntelligence.
DEEPMIND,2015. Lua/TorchimplementationofDQN[J]. GitHubrepository.
FORTUNATOM,AZARMG,PIOTB,etal.,2017. Noisynetworksforexploration[J]. arXivpreprint
arXiv:1706.10295.
143
第4章 深度Q网络
HERNANDEZ-GARCIAJF,SUTTONRS,2019.Understandingmulti-stepdeepreinforcementlearning:
AsystematicstudyoftheDQNtarget[C]//ProceedingsoftheNeuralInformationProcessingSystems
(AdvancesinNeuralInformationProcessingSystems)Workshop.
HESSELM,MODAYILJ,VANHASSELTH,etal.,2018. Rainbow: Combiningimprovementsindeep
reinforcementlearning[C]//Thirty-SecondAAAIConferenceonArtificialIntelligence.
HUBERPJ,1992. Robustestimationofalocationparameter[M]//Breakthroughsinstatistics. Springer:
492-518.
LIN L J, 1993. Reinforcement learning for robots using neural networks[R]. Carnegie-Mellon Univ
PittsburghPASchoolofComputerScience.
MAVRIN B, YAO H, KONG L, et al., 2019. Distributional reinforcement learning for efficient explo-
ration[C]//InternationalConferenceonMachineLearning. 4424-4434.
MCCLELLAND J L, MCNAUGHTON B L, O’REILLY R C, 1995. Why there are complementary
learning systems in the hippocampus and neocortex: insights from the successes and failures of
connectionistmodelsoflearningandmemory.[J]. Psychologicalreview,102(3): 419.
MNIHV,KAVUKCUOGLUK,SILVERD,etal.,2015.Human-levelcontrolthroughdeepreinforcement
learning[J]. Nature.
O’NEILLJ,PLEYDELL-BOUVERIEB,DUPRETD,etal.,2010. Playitagain: reactivationofwaking
experienceandmemory[J]. Trendsinneurosciences,33(5): 220-229.
RIEDMILLERM,2005. NeuralfittedQiteration–firstexperienceswithadataefficientneuralreinforce-
mentlearningmethod[C]//EuropeanConferenceonMachineLearning. Springer: 317-328.
RODERICK M, MACGLASHAN J, TELLEX S, 2017. Implementing the deep Q-network[J]. arXiv
preprintarXiv:1711.07478.
SCHAULT,QUANJ,ANTONOGLOUI,etal.,2015. Prioritizedexperiencereplay[C]//arXivpreprint
arXiv:1511.05952.
SUTTONRS,BARTOAG,2018. Reinforcementlearning: Anintroduction[M]. MITpress.
THRUNS,SCHWARTZA,1993.Issuesinusingfunctionapproximationforreinforcementlearning[C]//
Proceedingsofthe1993ConnectionistModelsSummerSchoolHillsdale,NJ.LawrenceErlbaum.
144
参考文献
TSITSIKLIS J, VAN ROY B, 1996. An analysis of temporal-difference learning with function ap-
proximationtechnical[J]. Report LIDS-P-2322). Laboratory for Information and Decision Systems,
MassachusettsInstituteofTechnology,Tech.Rep.
TSITSIKLISJN,VANROYB,1997. Analysisoftemporal-diffferencelearningwithfunctionapproxi-
mation[C]//AdvancesinNeuralInformationProcessingSystems. 1075-1081.
VANHASSELTH,GUEZA,SILVERD,2016.DeepreinforcementlearningwithdoubleQ-learning[C]//
ThirtiethAAAIconferenceonartificialintelligence.
WANGZ,SCHAULT,HESSELM,etal.,2016. Duelingnetworkarchitecturesfordeepreinforcement
learning[C]//InternationalConferenceonMachineLearning. 1995-2003.
YANG D, ZHAO L, LIN Z, et al., 2019. Fully parameterized quantile function for distributional rein-
forcementlearning[C]//AdvancesinNeuralInformationProcessingSystems. 6190-6199.
145
5
策略梯度
策略梯度方法（PolicyGradientMethods）是一类直接针对期望回报（ExpectedReturn）通过
梯度下降（GradientDescent）进行策略优化的增强学习方法。这一类方法避免了其他传统增强学
习方法所面临的一些困难，比如，没有一个准确的价值函数，或者由于连续的状态和动作空间，
以及状态信息的不确定性而导致的难解性（Intractability）。在这一章中，我们会学习一系列策略
梯度方法。从最基本的REINFORCE开始，我们会逐步介绍Actor-Critic方法及其分布式计算的
版本、信赖域策略优化（TrustRegionPolicyOptimization）及其近似算法，等等。在本章最后一
节，我们附上了本章涉及的所有方法所对应的伪代码，以及一个具体的实现例子。
5.1 简介
这一章主要介绍策略梯度方法。和上一章介绍的学习Q值函数的深度Q-Learning方法不同，
策略梯度方法直接学习参数化的策略 π 。这样做的一个好处是不需要在动作空间中求解价值最
θ
大化的优化问题，从而比较适合解决具有高维或者连续动作空间的问题。策略梯度方法的另一个
好处是可以很自然地对随机策略进行建模1。最后，策略梯度方法利用了梯度的信息来引导优化
的过程。一般来讲，这样的方法有更好的收敛性保证2。
顾名思义，策略梯度方法通过梯度上升的方法直接在神经网络的参数上优化智能体的策略。
在这一章中，我们会在5.2节中推导出策略梯度的初始版本算法。这个算法一般会有估计方差过
高的问题。我们在5.3节会看到Actor-Critic算法可以有效地减轻这个问题。有趣的是，Actor-Critic
1在价值学习的设定下，智能体需要额外构造它的探索策略，比如ϵ-贪心，以对随机性策略进行建模。
2但一般也仅限于局部收敛性，而不是全局收敛性。近期的一些研究在策略梯度的全局收敛性上有一些进展，但本章不
讨论这一方面的工作。
146
5.2 REINFORCE：初版策略梯度
和GAN的设计非常相像。我们会在5.4节比较它们的相似之处。在5.5节、5.6节中，我们会接
着介绍 Actor-Critic 的分布式版本。最后，我们通过考虑在策略空间（而不是参数空间）中的梯
度上升进一步提高策略梯度方法的性能。一个被广泛使用的方法是信赖域策略优化（TrustRegion
PolicyOptimization，TRPO），我们会在5.7节和5.8节介绍它及其近似版本，即近端策略优化算法
（ProximalPolicyOptimization，PPO），以及在5.9节中介绍使用Kronecker因子化信赖域的Actor
Critic（ActorCriticusingKronecker-factoredTrustRegion，ACKTR）。
在本章的最后一节，即5.10节中，我们提供了所涉及算法的代码实现，以方便读者可以迅速
上手试验。每个算法的完整实现可以在本书的代码库找到3。
5.2 REINFORCE：初版策略梯度
REINFORCE算法在策略的参数空间中直观地通过梯度上升的方法逐步提高策略π 的性能。
θ
回顾一下，由式子(2.119)我们有
2 3 2 3
XT Xt XT XT
∇ θJ(π θ)=E τ∼πθ4 R t∇ logπ θ(A t′|S t′)5 =E τ∼πθ4 ∇ θlogπ θ(A t′|S t′) R t5 . (5.1)
θ
t=0 t′=0 t′=0 t=t′
P
注5.1 上述式子中 T R 可以看成是智能体在状态S 处选择动作A ，并在之后执行当前策略
t=i t Pi i
的情况下，从第i步开始获得的累计奖励。事实上， T R 也可以看成Q (A ,S )，在第i步状
t=i t i i i
态S 处采取动作A ，并在之后执行当前策略的Q值。所以，一个理解REINFORCE的角度是：
i i
通过给不同的动作所对应的梯度根据它们的累计奖励赋予不同的权重，鼓励智能体选择那些累计
奖励较高的动作A 。
i
只要把上述式子中的T 替换成∞并赋予R 以γt 的权重，上述式子很容易可以扩展到折扣
t
因子为γ 的无限范围的设定如下。
2 3
X∞ X∞
∇J(θ)=E τ∼πθ4 ∇ θlogπ θ(A t′|S t′)γt′ γt−t′ R t5 . (5.2)
t′=0 t=t′
由于折扣因子给未来的奖励赋予了较低的权重，使用折扣因子还有助于减少估计梯度时的方差大
的问题。实际使用中，γt′
经常被去掉，从而避免了过分强调轨迹早期状态的问题。
虽然REINFORCE简单直观，但它的一个缺点是对梯度的估计有较大的方差。对于一个长度
为 L的轨迹，奖励R 的随机性可能对L呈指数级增长。为了减轻估计的方差太大这个问题，一
t
个常用的方法是引进一个基准函数b(S )。这里对b(S )的要求是：它只能是一个关于状态S 的
i i i
函数（或者更确切地说，它不能是关于A 的函数）。
i
3链接见读者服务
147
第5章 策略梯度
有了基准函数b(S )之后，增强学习目标函数的梯度∇J(θ)可以表示成
t
2 0 13
X∞ X∞
6 7
∇J(θ)=E τ∼πθ4 ∇ θlogπ θ(A t′|S t′)@ γt−t′ R t−b(S t′)A5. (5.3)
t′=0 t=t′
这是因为
  h  i
E τ,θ ∇ θlogπ θ(A t′|S t′)b(S t′) =E τ,θ b(S t′)E θ ∇logπ θ(A t′|S t′)|S t′ =0. (5.4)
上述式子的最后一个等式可以由EGLP引理（引理2.2）得到。最后如算法5.18所示，我们得到
带有基准函数的REINFORCE算法。
算法5.18带基准函数的REINFORCE算法
超参数: 步长η 、奖励折扣因子γ、总步数L、批尺寸B、基准函数b。
θ
输入: 初始策略参数θ
0
初始化θ =θ
0
fork =1,2,··· ,do
执行策略π 得到B个轨迹，每一个有L步，并收集{S ,A ,R }。
P θ t,ℓ t,ℓ t,ℓ
Aˆ = L γℓ′−ℓR −b(S )
t,ℓ ℓP′=ℓ P t,ℓ t,ℓ
J(θ)= 1 B L logπ (A |S )Aˆ