title:Membership Encoding for Deep Learning
author:Congzheng Song and
Reza Shokri
Under review as a conference paper at ICLR 2018
WEIGHTLESS: LOSSY WEIGHT ENCODING
FOR DEEP NEURAL NETWORK COMPRESSION
Anonymous authors
Paper under double-blind review
ABSTRACT
The large memory requirements of deep neural networks limit their deployment
and adoption on many devices. Model compression methods effectively reduce the
memory requirements of these models, usually through applying transformations
such as weight pruning or quantization. In this paper, we present a novel scheme for
lossy weight encoding which complements conventional compression techniques.
The encoding is based on the Bloomier ﬁlter, a probabilistic data structure that
can save space at the cost of introducing random errors. Leveraging the ability of
neural networks to tolerate these imperfections and by re-training around the errors,
the proposed technique, Weightless, can compress DNN weights by up to 496×
with the same model accuracy. This results in up to a 1.51× improvement over the
state-of-the-art.
1
INTRODUCTION
The continued success of deep neural networks (DNNs) comes with increasing demands on compute,
memory, and networking resources. Moreover, the correlation between model size and accuracy
suggests that tomorrow’s networks will only grow larger. This growth presents a challenge for
resource-constrained platforms such as mobile phones and wireless sensors. As new hardware now
enables executing DNN inferences on these devices (Apple, 2017; Qualcomm, 2017), a practical
issue that remains is reducing the burden of distributing the latest models especially in regions
of the world not using high-bandwidth networks. For instance, it is estimated that, globally, 800
million users will be using 2G networks by 2020 (GSMA, 2014), which can take up to 30 minutes
to download just 20MB of data. By contrast, today’s DNNs are on the order of tens to hundreds
of MBs, making them difﬁcult to distribute. In addition to network bandwidth, storage capacity on
resource-constrained devices is limited, as more applications look to leverage DNNs. Thus, in order
to support state-of-the-art deep learning methods on edge devices, methods to reduce the size of DNN
models without sacriﬁcing model accuracy are needed.
Model compression is a popular solution for this problem. A variety of compression algorithms have
been proposed in recent years and many exploit the intrinsic redundancy in model weights. Broadly
speaking, the majority of this work has focused on ways of simplifying or eliminating weight values
(e.g., through weight pruning and quantization), while comparatively little effort has been spent on
devising techniques for encoding and compressing.
In this paper we propose a novel lossy encoding method, Weightless, based on Bloomier ﬁlters,
a probabilistic data structure (Chazelle et al., 2004). Bloomier ﬁlters inexactly store a function
map, and by adjusting the ﬁlter parameters, we can elect to use less storage space at the cost of an
increasing chance of erroneous values. We use this data structure to compactly encode the weights of
a neural network, exploiting redundancy in the weights to tolerate some errors. In conjunction with
existing weight simpliﬁcation techniques, namely pruning and clustering, our approach dramatically
reduces the memory and bandwidth requirements of DNNs for over the wire transmission and on-
device storage. Weightless demonstrates compression rates of up to 496× without loss of accuracy,
improving on the state of the art by up to 1.51×. Furthermore, we show that Weightless scales better
with increasing sparsity, which means more sophisticated pruning methods yield even more beneﬁts.
This work demonstrates the efﬁcacy of compressing DNNs with lossy encoding using probabilistic
data structures. Even after the same aggressive lossy simpliﬁcation steps of weight pruning and
clustering (see Section 2), there is still sufﬁcient extraneous information left in model weights to allow
1
Under review as a conference paper at ICLR 2018
an approximate encoding scheme to substantially reduce the memory footprint without loss of model
accuracy. Section 3 reviews Bloomier ﬁlters and details Weightless. State-of-the-art compression
results using Weightless are presented in Section 4. Finally, in Section 4.3 shows that Weightless
scales better as networks become more sparse compared to the previous best solution.
2 RELATED WORK
Our goal is to minimize the static memory footprint of a neural network without compromising
accuracy. Deep neural network weights exhibit ample redundancy, and a wide variety of techniques
have been proposed to exploit this attribute. We group these techniques into two categories: (1)
methods that modify the loss function or structure of a network to reduce free parameters and (2)
methods that compress a given network by removing unnecessary information.
The ﬁrst class of methods aim to directly train a network with a small memory footprint by introducing
specialized structure or loss. Examples of specialized structure include low-rank, structured matrices
of Sindhwani et al. (2015) and randomly-tied weights of Chen et al. (2015). Examples of specialized
loss include teacher-student training for knowledge distillation (Bucila et al., 2006; Hinton et al.,
2015) and diversity-density penalties (Wang et al., 2017). These methods can achieve signiﬁcant
space savings, but also typically require modiﬁcation of the network structure and full retraining of
the parameters.
An alternative approach, which is the focus of this work, is to compress an existing, trained model.
This exploits the fact that most neural networks contain far more information than is necessary for
accurate inference (Denil et al., 2013). This extraneous information can be removed to save memory.
Much prior work has explored this opportunity, generally by applying a two-step process of ﬁrst
simplifying weight matrices and then encoding them in a more compact form.
Simpliﬁcation changes the number or characteristic of weight values to reduce the information needed
to represent them. For example, pruning by selectively zeroing weight values (LeCun et al., 1989;
Guo et al., 2016) can, in some cases, eliminate over 99% of the values without penalty. Similarly, most
models do not need many bits of information to represent each weight. Quantization collapses weights
to a smaller set of unique values, for instance via reduction to ﬁxed-point binary representations
(Gupta et al., 2015) or clustering techniques (Gong et al., 2014).
Simplifying weight matrices can further enable the use of more compact encoding schemes, improving
compression. For example, two recent works Han et al. (2016); Choi et al. (2017) encode pruned and
quantized DNNs with sparse matrix representations. In both works, however, the encoding step is a
lossless transformation, applied on top of lossy simpliﬁcation.
3 WEIGHTLESS
Weightless is a lossy encoding scheme based around Bloomier ﬁlters. We begin by describing what a
Bloomier ﬁlter is, how to construct one, and how to retrieve values from it. We then show how we
encode neural network weights using this data structure and propose a set of augmentations to make
it an effective compression strategy for deep neural networks.
3.1 THE BLOOMIER FILTER
A Bloomier ﬁlter generalizes the idea of a Bloom ﬁlter (Bloom, 1970), which are data structures
that answer queries about set membership. Given a subset S of a universe U, a Bloom ﬁlter answers
queries of the form, “Is v ∈ S?”. If v is in S, the answer is always yes; if v is not in S, there is
some probability of a false positive, which depends on the size of the ﬁlter, as size is proportional
to encoding strength. By allowing false positives, Bloom ﬁlters can dramatically reduce the space
needed to represent the set. A Bloomier ﬁlter (Chazelle et al., 2004) is a similar data structure but
instead encodes a function. For each v in a domain S, the function has an associated value f (v) in
the range R = [0, 2r). Given an input v, a Bloomier ﬁlter always returns f (v) when v is in S. When
v is not in S, the Bloomier ﬁlter returns a null value ⊥, except that some fraction of the time there is
a “false positive”, and the Bloomier ﬁlter returns an incorrect, non-null value in the range R.
2
Under review as a conference paper at ICLR 2018
Figure 1: Encoding with a Bloomier ﬁlter. W(cid:48) is an inexact reconstruction of W from a compressed
projection X. To retrieve the value w(cid:48)
i,j, we hash its location and exclusive-or the corresponding
entries of X together with a computed mask M. If the resulting value falls within the range [0, 2r−1),
it is used for w(cid:48)
i,j, otherwise, it is treated as a zero. The red path on the left shows a false positive due
to collisions in X and random M value.
XH0(v) ⊕ XH1(v) ⊕ XH2(v) ⊕ HM (v) = f (v).
Decoding Let S be the subset of values in U to store, with |S| = n. A Bloomier ﬁlter uses a
small number of hash functions (typically four), and a hash table X of m = cn cells for some
constant c (1.25 in this paper), each holding t > r bits. For hash functions H0, H1, H2, HM , let
H0,1,2(v) → [0, m) and HM (v) → [0, 2r), for any v ∈ U. The table X is set up such that for every
v ∈ S,
Hence, to ﬁnd the value of f (v), hash v four times, perform three table lookups, and exclusive-or
together the four values. Like the Bloom ﬁlter, querying a Bloomier ﬁlter runs in O(1) time. For
u /∈ S, the result, XH0(u) ⊕ XH1(u) ⊕ XH2(u) ⊕ HM (u), will be uniform over all t-bit values. If this
result is not in [0, 2r), then ⊥ is returned and if it happens to land in [0, 2r), a false positive occurs
and a result is (incorrectly) returned. An incorrect value is therefore returned with probability 2r−t.
Encoding Constructing a Bloomier ﬁlter involves ﬁnding values for X such that the relationship above
holds for all values in S. There is no known efﬁcient way to do so directly. All published approaches
involve searching for conﬁgurations with randomized algorithms. In their paper introducing Bloomier
ﬁlters, Chazelle et al. (2004) give a greedy algorithm which takes O(n log n) time and produces a
table of size (cid:100)cn(cid:101)t bits with high probability. Charles & Chellapilla (2008) provide two slightly better
constructions. First, they give a method with identical space requirements but runs in O(n) time.
They also show a separate O(n log n)-time algorithm for producing a smaller table with c closer to 1.
Using a more sophisticated algorithm for construction should allow for a more compact table and, by
extension, improve the overall compression rate. However, we leave this for future work and use the
method of (Chazelle et al., 2004) for simplicity.
While construction can be expensive, it is a one-time cost. Moreover, the absolute runtime is small
compared to the time it takes to train a deep neural network. In the case of VGG-16, our unoptimized
Python code built a Bloomier ﬁlter within an hour. We see this as a small worthwhile overhead given
the savings offered and in contrast to the days it can take to fully train a network.
3.2 APPROXIMATE WEIGHT ENCODING WITH BLOOMIER FILTERS
We propose using the Bloomier ﬁlter to compactly store weights in a neural network. The function
f encodes the mapping between indices of nonzero weights to their corresponding values. Given a
3
0010000000011010000XH0H1W'lH2Wl0m000000100000000000000010000000010000000000000010000110000000tMM000000000000101000000110001111000110rw'i,jHMUnder review as a conference paper at ICLR 2018
Figure 2: There is an exponential relationship between t and the number of false positives (red) as
well as the measured model accuracy with the incurred errors (blue).
weight matrix W, deﬁne the domain S to be the set of indices {i, j | wi,j (cid:54)= 0}. Likewise, the range
R is [−2a−1, 2a−1) − {0} for a such that all values of W fall within the interval. Due to weight
value clustering (see below) this range is remapped to [0, 2r−1) and encodes the cluster indices. A
null response from the ﬁlter means the weight has a value of zero.
Once f is encoded in a ﬁlter, an approximation W(cid:48) of the original weight matrix is reconstructed by
querying it with all indices. The original nonzero elements of W are preserved in the approximation,
as are most of the zero elements. A small subset of zero-valued weights in W(cid:48) will take on nonzero
values as a result of random collisions in X, possibly changing the model’s output. Figure 1 illustrates
the operation of this scheme: An original nonzero is correctly recalled from the ﬁlter on the right and
a false positive is created by an erroneous match on the left (red).
Complementing Bloomier ﬁlters with simpliﬁcation Because the space used by a Bloomier ﬁlter
is O(nt), they are especially useful under two conditions: (1) The stored function is sparse (small
n, with respect to |U|) and (2) It has a restricted range of output values (small r, since t > r). To
improve overall compression, we pair approximate encoding with weight transformations.
Pruning networks to enforce sparsity (condition 1) has been studied extensively (Hassibi & Stork,
1993; LeCun et al., 1989). In this paper, we consider two different pruning techniques: (i) magnitude
threshold plus retraining and (ii) dynamic network surgery (DNS) (Guo et al., 2016). Magnitude
pruning with retraining as straightforward to use and offers good results. DNS is a recently proposed
technique that prunes the network during training. We were able to acquire two sets of models,
LeNet-300-100 and LeNet5, that were pruned using DNS and include them in our evaluation; as
no reference was published for VGG-16 only magnitude pruning is used. Regardless of how it is
accomplished, improving sparsity will reduce the overall encoding size linearly with the number of
non-zeros with no effect on the false positive rate (which depends only on r and t). The reason for
using two methods is to demonstrate the beneﬁts of Weightless as networks increase in sparsity, the
DNS networks are notably more sparse than the same networks using magnitude pruning.
Reducing r (condition 2) amounts to restricting the range of the stored function or minimizing the
number of bits required to represent weight values. Though many solutions to discretize weights
exist (e.g., limited binary precision and advanced quantization techniques Choi et al. (2017)), we use
k-means clustering. After clustering the weight values, the k centroids are saved into an auxiliary
table and the elements of W are replaced with indices into this table. This style of indirect encoding
is especially well-suited to Bloomier ﬁlters, as these indices represent a small, contiguous set of
integers. Another beneﬁt of using Bloomier ﬁlters is that k does not have to be a power of 2. When
decoding Bloomier ﬁlters, the result of the XORs can be checked with an inequality, rather than a
bitmask. This allows Bloomier ﬁlters to use k exactly, reducing the false positive rate by a factor of
2r . In other methods, like that of Han et al. (2016), there is no beneﬁt, as any k not equal to a
1 − k
power of two strictly wastes space.
Tuning the t hyperparameter The use of Bloomier ﬁlters introduces an additional hyperparameter
t, the number of bits per cell in the Bloomier ﬁlter. t trades off the Bloomier ﬁlter’s size and the false
positive rate which, in turn, effects model accuracy. While t needs to be tuned, we ﬁnd it far easier to
4
⇥104AccuracyFalse PositivesUnder review as a conference paper at ICLR 2018
Figure 3: Weightless operates layer-by-layer, alternating between simpliﬁcation and lossy encoding.
Once a Bloomier ﬁlter is constructed for a weight matrix, that layer is frozen and the subsequent
layers are brieﬂy retrained (only a few epochs are needed).
reason about than other DNN hyperparameters. Because we encode k clusters, t must be greater than
(cid:100)log2 k(cid:101), and each additional t bit reduces the number of false positives by a factor of 2. This limits
the number of reasonable values for t: when t is too low the networks experience substantial accuracy
loss, but also do not beneﬁt from high values of t because they have enough implicit resilience to
handle low error rates (see Figure 2). Experimentally we ﬁnd that t typically falls in the range of 6 to
9 for our models.
Retraining to mitigate the effects of false positives We encode each layer’s weights sequentially.
Because the weights are ﬁxed, the Bloomier ﬁlter’s false positives are deterministic. This allows
for the retraining of deeper network layers to compensate for errors. It is important to note that
encoded layers are not retrained. The randomness of the Bloomier ﬁlter would sacriﬁce all beneﬁts of
mitigating the effects of errors. If the encoded layer was retrained, a new encoding would have to be
constructed (because S changes) and the indices of weights that result in false positives would differ
after every iteration of retraining. Instead, we ﬁnd retraining all subsequent layers to be effective,
typically allowing us to reduce t by one or two bits. The process of retraining around faults is
relatively cheap, requiring on the order of tens of epochs to converge. The entire optimization pipeline