f (V) a random noise. The magnitude of the noise depends on GSf ,
the global sensitivity or the L1 sensitivity of f , defined as,
In the definition above, Lap(β) denotes a random variable sampled
from the Laplace distribution with scale parameter β such that
Pr[Lap(β) = x] = 1
2. When
2β e−|x |/β , and it has a variance of 2β
When f outputs a single element, such a mechanism A is given
below:
GSf = max
′)||1.
V ≃V ′ || f (V) − f (V
(cid:16) GSf
(cid:17)
Af (V) = f (V) + Lap
.
ϵ
(cid:16) GSf
(cid:17) to
ϵ
f outputs a vector, A adds independent samples of Lap
each element of the vector.
Noisy Max Mechanism. The Noisy Max mechanism (NM) [20]
takes a collection of queries, computes a noisy answer to each query,
and returns the index of the query with the largest noisy answer.
More specifically, given a list of queries q1, q2, . . ., where each
qi takes the data V as input and outputs a real-numbered result,
the mechanism computes qi(V), samples a fresh Laplace noise
Lap
(cid:17) and adds it to the query result, i.e.,
(cid:19)
(cid:18) 2GSq
(2)
and returns the index j = arg maxi ˜qi(V). Here GSq is the global
sensitivity of queries and is defined as:
˜qi(V) = qi(V) + Lap
(cid:16) 2GSq
ϵ
ϵ
,
GSq = max
i
V ≃V ′ |qi(V) − qi(V
max
′)|.
Dwork and Roth prove this satisfies ϵ-DP [20] (recently Ding et
al. [14] proved using exponential noise also satisfy DP). Moreover,
if the queries satisfy the monotonic condition, meaning that when
the input dataset is changed from V to V ′, the query results change
in the same direction, i.e., for any neighboring V and V ′
′)(cid:1) =⇒ (cid:0)∀i′ qi′(V)≤qi′(V
(cid:0)∃i qi(V) <qi(V
′)(cid:1) .
Then one can remove the factor of 2 in the Laplace noise. This
improves the accuracy of the result.
2.4 Composition Properties
The following composition properties hold for both DP and LDP
algorithms, each commonly used for building complex differentially
private algorithms from simpler subroutines.
Sequential Composition. Combining multiple subroutines that
satisfy DP for ϵ1,· · · , ϵk results in a mechanism that satisfies ϵ-DP
for ϵ =
i ϵi.
Parallel Composition. Given k algorithms working on disjoint
subsets of the dataset, each satisfying DP for ϵ1,· · · , ϵk, the result
satisfies ϵ-DP for ϵ = maxi ϵi.
Post-processing. Given an ϵ-DP algorithm A, releasing д(A(V))
for any д still satisfies ϵ-DP. That is, post-processing an output of a
differentially private algorithm does not incur any additional loss
of privacy.
3 DIFFERENTIALLY PRIVATE STREAMS
For privately releasing streams and supporting range queries over
the private stream, the most straightforward way is to add indepen-
dent noise generated through the Laplace distribution. However,
this results in a cumulative error (following the tradition, we use
absolute error here, which measures the difference from the true
sum) of O(√
The Hierarchy Approach. To get rid of the dependency on √
n,
the hierarchical method was proposed [8, 18]. Given a stream of
length n, the algorithm first constructs a tree: the leaves are labeled
{1}, {2}, . . . , {n} and the label of each parent node is the union of
labels from its child nodes. Given h = log n layers, the method adds
n) after n observations.
Session 4D: Differential Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1239Laplace noise with ϵ/h in each layer. To obtain the noisy count
˜V(i, j), we find at most log n nodes in the hierarchy, whose labels
are disjoint and their union equals [i, j]. Given that the noise added
to each node is O(log n), this method has an error of O(log1.5
n).
In the online setting, where the stream data come one-by-one,
we want to release every node in the hierarchy promptly. To do
so, at any time index t, we publish all nodes that contain t as the
largest number in their labels.
3.1 Existing Work: PAK
To satisfy DP in the hierarchy method, one needs to add noise pro-
portional to B, the maximal possible value in the stream, and B can
be quite large in many cases. Perrier et al. [36] (we call it by the
authors’ initials, PAK, for short) observed that in practice, most of
the values are concentrated below a threshold much smaller than B
(e.g., the largest possible purchase price of supermarket transactions
is much larger than what an ordinary customer usually spends),
and proposed a method to find such a threshold and truncate data
points below it to reduce the scale of the injected Laplace noises.
In particular, the first m values are used to estimate the threshold
θ with differential privacy. After obtaining θ, the following values
in the stream are truncated to be no larger than θ. Reducing the
upper bound from B to θ reduces the DP noise (via reducing sensi-
tivity). The hierarchical method is used for estimating the stream
statistics with the remaining n − m values (PAK assumes there are
n observations).
Finding the Threshold. To obtain θ, PAK proposed a specially
designed algorithm based on Smooth Sensitivity (SS) [35] to get the
p-quantile (or p-percentile) as θ, i.e., p% of the values are smaller
than θ. SS was used to compute the median with DP in the original
work [35], and SS can also be easily extended to privately release
the p-quantile. PAK proved that the result of SS is unbiased, but
they further wanted to make sure the result is always larger than
the real p-quantile. This is because if the estimated percentile is
smaller than the real one, the truncation in the next phase will
introduce greater bias. Thus PAK modified the original SS method
to guarantee that the result is unlikely to be smaller than the real
p-quantile. As the details of the method are not directly used in the
rest of the paper, we defer the details of both SS and the algorithm
itself to Appendix A and B.
There are two drawbacks of this method. First, it requires a p
value to be available beforehand. But a good choice of p actually
depends on the dataset, ϵ, and m. PAK simply uses p = 99.5. As
shown in our experiment in Section 5.4, p = 99.5 does not perform
well in every scenario. Second, to ensure that θ is no smaller than
the real p-quantile, PAK introduces a positive bias to θ.
3.2 Overview of Our Approach
The design of PAK was guided by asymptotic analysis. Unfortu-
nately, for the parameters that are likely to occur in practice, the
methods and parameters chosen by asymptotic analysis can be far
from optimal, as such analysis ignores important constant factors.
Instead, we use concrete analysis to guide the choice of methods
and parameters.
In this section, we first deal with the threshold selection problem
using the Noisy Max mechanism (NM, introduced in Section 2.3),
which satisfies DP with δ = 0. Empirical experiments show its
superiority especially in small ϵ scenarios (which means compared
to PAK, we can achieve the same performance with better privacy
guarantees). We then introduce multiple improvements for the
hierarchical methods including an online consistency method, and
a method to reduce the noise in the lower levels of the hierarchy. We
integrate all the components in a general framework, ToPS, which
consists of a Threshold optimizer, a Perturber, and a Smoother:
• Threshold optimizer: The threshold optimizer uses a small portion
of the input stream to find a threshold θ for optimizing the errors
due to noise and bias. It then truncates any incoming values by
θ and releases them to the perturber.
• Perturber: The perturber adds noise to the truncated stream, and
• Smoother: The smoother performs further post-processing on the
releases noisy counts to the smoother.
noisy counts and outputs the final stream.
While design of ToPS is inspired by PAK, we include unique
design choices to handle the problem. In particular, PAK has two
phases, the threshold finder and the hierarchical method. We im-
prove both phases. Note that as we use NM [20], our algorithm
satisfies ϵ-DP while PAK satisfies (ϵ, δ)-DP. Moreover, we intro-
duce a smoother that further improves accuracy. The fundamental
reason that our method works better is that we design our method
focusing on a good empirical performance rather than theoretical
bounds.
3.3 Threshold Optimizer
Different from PAK [36] that focuses on bias when choosing the
threshold θ, our approach is to consider both bias and variance
(due to DP noise). As bias and variance go in opposite ways (i.e.,
when θ is large, bias will be small, but variance will go large, vice
versa), there will be an optimal θ that minimizes the overall error.
Note that the overall error depends on the whole distribution of the
data, which might be too much information to accurately estimate
with DP. To handle this issue, we choose to use the Noisy Max
mechanism (NM) [20], which looks into the data privately and
outputs only a succinct information of θ. In what follows, we first
examine the error.
A Basic NM Query Definition. We first consider the expected
squared error of estimating a single value v. Assuming that ˜v is the
estimation of v, it is well known that the expected squared error is
the summation of variance and the squared bias of ˜v:
E(cid:2) ( ˜v − v)2(cid:3) = Var[ ˜v] + Bias[ ˜v]2
.
(3)
Note that Bias[ ˜v] equals E[ ˜v ]−v. Given a threshold θ and privacy
budget ϵ, Bias[ ˜v] = max(v − θ, 0) and Var[ ˜v] = 2θ 2
(because we
ϵ 2
add Laplace noise with parameter θ/ϵ).
Since we are using the hierarchical method to publish streams
for answering range queries, we use the error estimations of the
hierarchical method to instantiate Equation 3. Qardaji et al. [37]
show that there are approximately (b − 1) logb(r) nodes to be esti-
mated given any random query of range smaller than r, where b
is the fan-out factor of the hierarchical method; and the variance
of each node is log2
ϵ 2 . For bias, within range limitation r, a
random query will cover around r3 leaf nodes on average [37] (We
assume a random query can take any range in [1, r]. Thus there
b(r) 2θ 2
Session 4D: Differential Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1240are r(r + 1)/2 possible range queries. Among them, for any range
of length j ∈ [1, r], there are r − j + 1 such ranges. The expected
3 ≈ r3 ). Denote ft
length of a random query is
as the frequency of value t, the combined error of the hierarchical
method for answering random range queries would be:
j =1 j(r−j+1)
r(r +1)/2
= r +2
r
(b − 1) log3
b(r)2θ
2
2 +
ϵ
ft(t − θ)
.
(4)
(cid:32)
r
3

θ <t <B
(cid:33)2
The Final NM Query Definition. For NM to be effective, the
queries should have low sensitivity, meaning that changing one
value perturbs the queries by a tiny amount. However, if we directly
use Equation 4 as the queries, the sensitivity is large: a change
of value from 0 to B will result in the increase of Equation 4 by
(B − θ)2/9. Thus we choose to approximate the mean squared error
by defining the queries in the following ways.
Denote m as the number of values to be used in NM, and mθ as
the number of values that are smaller than θ from these m values:
(5)
where [x] = {1, 2, . . . , x} and |X| denotes the cardinality of set X.
The first approximation method we use is to replace the variance
and squared bias with their squared roots (standard deviation and
bias). Second, we use c · mθ/m (c is a constant to be discussed later)
θ <t <B ft(t − θ). Third, we multiply both the
standard deviation and bias errors by − 3m
to ensure the sensitivity
c·r
is 1, and the query result of the target is the highest. Thus we have:
to approximate
mθ = |{i | vi ≤ θ, i ∈ [m]}|,
(cid:115)
(cid:113)2(b − 1) log3
2
b(r)2θ
2 − mθ
b(r) − mθ .
(b − 1) log3
ϵ
qθ(V) = − 3m
c · r
= − 3mθ
crϵ
(6)
bias term
The first term is a constant depending on θ but independent of the
private data, while the second term has a sensitivity of 1.
Running NM. To run NM, the set of possible θ values considered
in the queries qθ(V) should be a discrete set that covers the range
[0, B]. The granularity of the set is important. If it is too coarse-
grained (e.g., θ ∈ {0, B/2, B}), the method is inaccurate, because the
desired value might be far from any possible output. On the other
hand, if it is too fine-grained, the NM algorithm will run slowly,
but it does not influence the accuracy. In the experiment, we use all
integers in the range of [B] = {1, 2, . . . , B} as the possible set of θ.
One unexplained parameter in Equation 6 is c. There are two
factor that contributes to c: (1) Using mθ/m to approximate the
θ <t <B ft(t − θ) leads to underestimation. (2) As we
will describe later in Section 3.4 and 3.5, the actual squared error
will be further reduced by our newly proposed method. While c
intends to be a rough estimation of the underestimation, it does
not need to be chosen based on one particular dataset. One can run
experiments with a public dataset of similar nature under different