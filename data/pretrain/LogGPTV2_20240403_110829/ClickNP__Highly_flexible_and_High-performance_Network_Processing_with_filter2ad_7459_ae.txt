Next, we study the overhead of ﬁne-grained modulariza-
tion of ClickNP. Since every element will generate a logic
block boundary and use only FIFO buffers to communicate
with other blocks, there should be an overhead. To measure
this overhead, we create a simple element that only passes
data from one input port to an output port. The resource uti-
lization of this empty element should well capture the over-
head of modularization. Different HLS tools may use dif-
ferent amount of resources, but all are low, with a min of
0.15% to a max of 0.4%. So we conclude ClickNP incurs
little overhead due to modularization.
Finally, we want to study the efﬁciency of RTL code gen-
erated by ClickNP, compared to hand-written HDL. To do
so, we use NetFPGA [36] as our reference. We extract the
Table 3: Summary of ClickNP NFs.
LoC†
665
250
538
695
860
584
Network Function
Pkt generator
16%
Pkt capture
8%
OpenFlow ﬁrewall
32%
IPSec gateway
35%
L4 load balancer
36%
pFabric scheduler
11%
† Total line of code of all element declarations and
conﬁguration ﬁles.
#Elements
6
11
7
10
13
7
LE BRAM
12%
5%
54%
74%
38%
15%
LUTs
Table 4: Relative area cost compared to NetFPGA.
BRAMs
NetFPGA
Min / Max Min / Max Min / Max
Function
0.9x / 1.3x
2.1x / 3.4x
Input arbiter
Output queue
1.4x / 2.0x
0.9x / 1.2x
N/A
0.9x / 3.2x
Header parser
Openﬂow table
0.9x / 1.6x
1.1x / 1.2x
1.8x / 2.8x
2.0x / 3.2x
2.1x / 3.2x
1.6x / 2.3x
Registers
IP checksum
Encap
4.3x / 12.1x
0.9x / 5.2x
9.7x / 32.5x
1.1x / 10.3x
N/A
N/A
key modules in NetFPGA, which are well optimized by ex-
perienced Verilog programmers, and implement counterpart
elements in ClickNP with the same functionality. We com-
pare the relative area cost between these two implementa-
tions using different HLS tools as a backend. The results are
summarized in Table 4. Since different tools may have dif-
ferent area costs, we record both the maximum and minimal
value. We can see generally, automatically generated HDL
code uses more area compared to hand-optimized code. The
difference, however, is not very large. For complex modules
(shown in the top part of the table), the relative area cost is
less than 2x. For tiny modules (shown in the bottom part of
the table), the relative area cost appears larger, but the abso-
lute resource usage is small. This is because all HLS tools
would generate a ﬁxed overhead that dominates the area cost
for tiny modules.
In summary, ClickNP can generate efﬁcient RTL for FPGA
that incurs only moderate area cost, which is capable of build-
ing practical NFs. Looking forward, FPGA technology is
still evolving very rapidly. For example, the next generation
FPGA from Altera, Arria 10, would have 2.5x more capac-
ity than the chip we use currently. Therefore, we believe the
area cost of HLS would be less of a concern in the future.
12
 0.01 0.1 1 10 1001K4K16K128K1M8M32MProcessing Rate (Mpps)Concurrent FlowsClickNPLVS1001011021031041051K10K100K1M10MLatency (us)Offered Load (pps)ClickNPLVS1K16K256K4M64M1K16K256K4M56MAllocations/sNew Connections/sClickNPLVS7.4 Validation of pFabric
Before we end this section, we show that ClickNP is also
a good tool for network research. Thanks to the ﬂexibility
and high performance, we can quickly prototype the latest
research and apply it to real environments. For example, we
can easily implement pFabric scheduler [12] using ClickNP,
and apply it in our testbed. In this experiment, we modify a
software TCP ﬂow generator [16] to place the ﬂow priority,
i.e., the total size of the ﬂow, in packet payload. We generate
ﬂows according to the data-mining workload in [12] and fur-
ther the restrict egress port to be 10 Gbps using a RateLimit
element. We apply pFabric to schedule ﬂows in egress buffer
according to ﬂow priorities. Figure 12 shows the average
ﬂow completion time (FCT) of pFabric, TCP with Droptail
queue, and the ideal. This experiment validates that pFabric
achieves near ideal FCT in this simple scenario.
tors.
In contrast, FPGA is a general computing platform.
Beside NFs, FPGA have many other applications in datacen-
ters, making it more attractive to deploy at scale [40]. Hard-
ware switch has limited functionality and its applications are
very restricted [22].
FPGA is a mature technology and recently has been de-
ployed to accelerate datacenter services, including NFs [24,
31, 40, 42]. It is well recognized that the programmability
of FPGA is low and there is a rich body of literature on
improving it, by providing high-level programming abstrac-
tions [13–15, 37, 45, 46]. Gorilla [31] proposes a domain-
speciﬁc high-level language for packet switching on FPGA.
Chimpp [42], however, tries to introduce Click model into
HDL to develop modular router. ClickNP works along this
direction and is complimentary to previous work. ClickNP
targets NFs in datacenters, and addresses the programmabil-
ity issue by providing a highly ﬂexible, modularized archi-
tecture and leveraging commercial HLS tools.
The work most related to ours is the Click2FPGA toolchain [41],
which compiles an entire Click conﬁguration to FPGA. Its
performance, however, is much lower than ClickNP and it
lacks support for joint CPU/FPGA packet processing. To
the best of our knowledge, ClickNP is the ﬁrst FPGA-accelerated
platform for general NFs processing at 40Gbps line rate, and
completely written in high-level language.
Figure 12: Validation of pFabric.
9. CONCLUSION
8. RELATED WORK
Software NFs have great ﬂexibility and scalability. Early
studies mainly focus on software-based packet forwarding [20,
21]. They show that multi-core x86 CPU can forward pack-
ets at near 10Gbps per server and the capacity can scale
by clustering more servers. Recently, many systems have
been designed to implement various types of NFs [25, 33,
43]. Similarly, all of these systems exploit the multi-core
parallelism in CPUs to achieve close to 10Gbps through-
put per machine, and scale out to use more machines when
higher capacity is needed. Ananta [39] is a software load-
balancer deployed in Microsoft datacenters to provide cloud-
scale load-balancing service. While software NFs can scale
out to provide more capacity, doing so adds considerable
costs in both CAPEX and OPEX [22, 39].
To accelerate software packet processing, previous work
has proposed using GPU [26], specialized network proces-
sor (NP) [2, 5], and hardware switches [22]. GPU is primar-
ily designed for graphic processing and recently extended
to other applications with massive data parallelism. GPU is
more suitable for batch operations. Han, et al. [26], show
that using GPU can achieve 40Gbps packet switching speed.
However, batch operations incur high delay. For example,
the forwarding latency reported in [26] is about 200µs, two
orders of magnitude larger than ClickNP. Compared to GPU,
FPGA is more ﬂexible and can be reconﬁgured to capture
data and pipeline parallelisms, both of which are very com-
mon in NFs. NPs, however, are specialized to handle net-
work trafﬁc and have many hard-wired network accelera-
This paper presents ClickNP, an FPGA-accelerated plat-
form for highly ﬂexible and high-performance NFs in com-
modity servers. ClickNP is completely programmable us-
ing high-level language and provides a modular architecture
familiar to software programmers in the networking ﬁeld.
ClickNP supports joint CPU/FPGA packet processing and
has high performance. Our evaluations show that ClickNP
improves the throughput of NFs by 10x compared to state-
of-the-art software NFs, while alos reducing latency by 10x.
Our work makes a concrete case showing FPGA is capable
for accelerating NFs in datacenters. Also, we demonstrate
that high-level programming for FPGA is actually feasible
and practical. One limitation of FPGA programming, how-
ever, is that the compilation time is rather long, e.g., 1∼2
hours, largely due to HDL synthesis tools. ClickNP allevi-
ates this pain with its cross-platform ability, and hopes most
bugs could be detected by running elements on CPU. How-
ever, in the long term, HDL synthesis tools should be opti-
mized to greatly shorten their compilation time.
Acknowledgements
We would like to thank Andrew Putnam, Derek Chiou, and
Doug Burger for all technical discussions. We’d also like to
thank the whole Catapult v-team at Microsoft for the Cata-
pult Shell and support of OpenCL programming. We thank
Albert Greenberg and Dave Maltz for their support and sug-
gestions on the project. We thank Tong He for his contribu-
tion on ClickNP PCIe channel development. Finally, we also
thank our shepherd, KyoungSoo Park, and other anonymous
reviewers for their valuable feedbacks and comments.
13
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1102104CDFFCT (us, log-scale)IdealpFabricTCP-Droptail10. REFERENCES
[1] Altera SDK for OpenCL. http://www.altera.com/.
[2] Cavium Networks OCTEON II processors.
http://www.caviumnetworks.com.
[3] Dell networking s6000 spec sheet.
[4] Linux virtual server. http://www.linuxvirtualserver.org/.
[5] Netronome Flow Processor NFP-6xxx.
https://netronome.com/product/nfp-6xxx/.
[6] SDAccel Development Environment. http://www.xilinx.com/.
[7] Strongswan ipsec-based vpn. https://www.strongswan.org/.
[8] The OpenCL Speciﬁcations ver 2.1. Khronos Group.
[9] Vivado Design Suite. http://www.xilinx.com/.
[10] Ethernet switch series, 2013. Broadcom Trident II.
[11] Introducing EDR 100GB/s - Enabling the Use of Data, 2014.
Mellanox White Paper.
[12] M. Alizadeh, S. Yang, M. Sharif, S. Katti, N. McKeown,
B. Prabhakar, and S. Shenker. pfabric: Minimal near-optimal
datacenter transport. In Proc. ACM SIGCOMM, 2013.
[13] J. Auerbach, D. F. Bacon, P. Cheng, and R. Rabbah. Lime: a
java-compatible and synthesizable language for
heterogeneous architectures. In ACM SIGPLAN Notices,
volume 45, pages 89–108. ACM, 2010.
[14] J. Bachrach, H. Vo, B. Richards, Y. Lee, A. Waterman,
R. Avižienis, J. Wawrzynek, and K. Asanovi´c. Chisel:
constructing hardware in a scala embedded language. In
Proc. ACM Annual Design Automation Conf., 2012.
[15] D. F. Bacon, R. Rabbah, and S. Shukla. Fpga programming
for the masses. Communications of the ACM, 56(4):56–63,
2013.
[16] W. Bai, L. Chen, K. Chen, and H. Wu. Enabling ecn in
multi-service multi-queue data centers. In Proc. USENIX
NSDI, 2016.
[17] T. Barbette, C. Soldani, and L. Mathy. Fast userspace packet
processing. In Proc. ANCS, 2015.
[18] A. Bernstein. Analysis of programs for parallel processing.
IEEE Transactions on Electronic Computers,
EC-15(5):757–763, Oct 1966.
[19] B. Betkaoui, D. B. Thomas, and W. Luk. Comparing
performance and energy efﬁciency of fpgas and gpus for high
productivity computing. In 2010 International Conference on
Field-Programmable Technology, 2010.
[20] M. Dobrescu, N. Egi, K. Argyraki, B.-G. Chun, K. Fall,
G. Iannaccone, A. Knies, M. Manesh, and S. Ratnasamy.
Routebricks: Exploiting parallelism to scale software routers.
In Proc. ACM SOSP, 2009.
[21] N. Egi, A. Greenhalgh, M. Handley, M. Hoerdt, F. Huici, and
L. Mathy. Towards high performance virtual routers on
commodity hardware. In Proc. ACM CoNEXT, 2008.
[22] R. Gandhi, H. H. Liu, Y. C. Hu, G. Lu, J. Padhye, L. Yuan,
and M. Zhang. Duet: Cloud scale load balancing with
hardware and software. In Proc. ACM SIGCOMM, 2014.
[23] A. Greenberg. Windows Azure: Scaling SDN in Public
Cloud, 2014. OpenNet Submit.
[24] A. Greenberg. SDN for the Cloud, 2015. Keynote at
SIGCOMM 2015 (https://azure.microsoft.com/en-
us/blog/microsoft-showcases-software-deﬁned-networking-
innovation-at-sigcomm-v2/).
[25] A. Greenhalgh, F. Huici, M. Hoerdt, P. Papadimitriou,
M. Handley, and L. Mathy. Flow processing and the rise of
commodity network hardware. ACM SIGCOMM CCR,
39(2):20–26, Mar. 2009.
[26] S. Han, K. Jang, K. Park, and S. Moon. Packetshader: A
gpu-accelerated software router. In Proc. ACM SIGCOMM,
2010.
[27] W. Jiang. Scalable ternary content addressable memory
implementation using fpgas. In Proc. ANCS, 2013.
[28] S. Kestur, J. D. Davis, and O. Williams. Blas comparison on
fpga, cpu and gpu. In IEEE Computer Society Symposium on
VLSI, July 2010.
[29] E. Kohler, R. Morris, B. Chen, J. Jannotti, and M. F.
Kaashoek. The click modular router. ACM Transactions on
Computer Systems (TOCS), 18(3):263–297, 2000.
[30] T. Koponen, K. Amidon, P. Balland, M. Casado, A. Chanda,
B. Fulton, I. Ganichev, J. Gross, N. Gude, P. Ingram, et al.
Network virtualization in multi-tenant datacenters. In Proc.
USENIX NSDI, Berkeley, CA, USA, 2014.
[31] M. Lavasani, L. Dennison, and D. Chiou. Compiling high
throughput network processors. In Proc. FPGA, 2012.
[32] J. Lee, S. Lee, J. Lee, Y. Yi, and K. Park. Flosis: a highly
scalable network ﬂow capture system for fast retrieval and
storage efﬁciency. In Proc. USENIX ATC, 2015.
[33] J. Martins, M. Ahmed, C. Raiciu, V. Olteanu, M. Honda,
R. Bifulco, and F. Huici. Clickos and the art of network
function virtualization. In Proc. USENIX NSDI, 2014.
[34] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar,
L. Peterson, J. Rexford, S. Shenker, and J. Turner. Openﬂow:
enabling innovation in campus networks. ACM SIGCOMM
CCR, 38(2):69–74, 2008.
[35] S.-W. Moon, J. Rexford, and K. G. Shin. Scalable hardware
priority queue architectures for high-speed packet switches.
IEEE Transactions on Computers, 2000.
[36] J. Naous, G. Gibb, S. Bolouki, and N. McKeown. Netfpga:
Reusable router architecture for experimental research. In
Proc. PRESTO, 2008.
[37] R. S. Nikhil and Arvind. What is bluespec? ACM SIGDA
Newsletter, 39(1):1–1, Jan. 2009.
[38] R. Pagh and F. F. Rodler. Cuckoo hashing. Algorithms - ESA
2001. Lecture Notes in Computer Science 2161, 2001.
[39] P. Patel, D. Bansal, L. Yuan, A. Murthy, A. Greenberg, D. A.
Maltz, R. Kern, H. Kumar, M. Zikos, H. Wu, C. Kim, and
N. Karri. Ananta: Cloud scale load balancing. In Proc. ACM
SIGCOMM, 2013.
[40] A. Putnam, A. M. Caulﬁeld, E. S. Chung, D. Chiou,
K. Constantinides, J. Demme, H. Esmaeilzadeh, J. Fowers,
G. P. Gopal, J. Gray, et al. A reconﬁgurable fabric for
accelerating large-scale datacenter services. In Proc. Intl.
Symp. on Computer Architecture (ISCA), 2014.
[41] T. Rinta-aho, M. Karlstedt, and M. P. Desai. The
click2netfpga toolchain. In Proc. USENIX ATC, 2012.
[42] E. Rubow, R. McGeer, J. Mogul, and A. Vahdat. Chimpp: A
click-based programming and simulation environment for
reconﬁgurable networking hardware. In Proc. ANCS, 2010.
[43] V. Sekar, N. Egi, S. Ratnasamy, M. K. Reiter, and G. Shi.
Design and implementation of a consolidated middlebox
architecture. In Proc. USENIX NSDI, 2012.
[44] J. Sherry, P. Gao, S. Basu, A. Panda, A. Krishnamurthy,
C. Macciocco, M. Manesh, J. Martins, S. Ratnasamy,
L. Rizzo, and S. Shenker. Rollback recovery for
middleboxes. In Proc. ACM SIGCOMM, 2015.
[45] D. Singh. Implementing fpga design with the opencl
standard. Altera whitepaper, 2011.
[46] R. Wester. A transformation-based approach to hardware
design using higher-order functions. 2015.
14