### Overhead of Fine-Grained Modularization in ClickNP

Next, we investigate the overhead associated with the fine-grained modularization in ClickNP. Each element in ClickNP generates a logic block boundary and communicates with other blocks using only FIFO buffers, which introduces some overhead. To measure this overhead, we created a simple element that merely passes data from one input port to an output port. The resource utilization of this "empty" element provides a good estimate of the modularization overhead.

Different High-Level Synthesis (HLS) tools may use varying amounts of resources, but all are relatively low, ranging from a minimum of 0.15% to a maximum of 0.4%. Therefore, we conclude that the overhead due to modularization in ClickNP is minimal.

### Efficiency of RTL Code Generated by ClickNP

To evaluate the efficiency of the Register-Transfer Level (RTL) code generated by ClickNP, we compare it with hand-written Hardware Description Language (HDL) code. For this comparison, we use NetFPGA [36] as our reference. We extracted key modules from NetFPGA, which are well-optimized by experienced Verilog programmers, and implemented equivalent elements in ClickNP with the same functionality. We then compared the relative area cost of these two implementations using different HLS tools as the backend. The results are summarized in Table 4.

Since different tools may have varying area costs, we recorded both the maximum and minimum values. Generally, automatically generated HDL code uses more area compared to hand-optimized code, but the difference is not significant. For complex modules (shown in the top part of the table), the relative area cost is less than 2x. For smaller modules (shown in the bottom part of the table), the relative area cost appears larger, but the absolute resource usage is small. This is because all HLS tools generate a fixed overhead that dominates the area cost for tiny modules.

In summary, ClickNP can generate efficient RTL for FPGAs with only moderate area cost, making it capable of building practical network functions (NFs). As FPGA technology continues to evolve rapidly, future generations, such as Altera's Arria 10, will have significantly higher capacity (2.5x more than current chips). Therefore, the area cost of HLS is expected to become less of a concern in the future.

### Validation of pFabric

Before concluding this section, we demonstrate that ClickNP is also a valuable tool for network research. Due to its flexibility and high performance, ClickNP allows for the rapid prototyping of the latest research and its application in real environments. For example, we implemented the pFabric scheduler [12] using ClickNP and applied it in our testbed.

In this experiment, we modified a software TCP flow generator [16] to include flow priority (i.e., the total size of the flow) in the packet payload. We generated flows based on the data-mining workload described in [12] and restricted the egress port to 10 Gbps using a RateLimit element. We then used pFabric to schedule flows in the egress buffer according to their priorities. Figure 12 shows the average flow completion time (FCT) for pFabric, TCP with Droptail queue, and the ideal case. This experiment validates that pFabric achieves near-ideal FCT in this simple scenario.

### Related Work

Software NFs offer great flexibility and scalability. Early studies focused on software-based packet forwarding [20, 21], demonstrating that multi-core x86 CPUs can forward packets at nearly 10 Gbps per server, with capacity scaling through clustering. Recently, many systems have been designed to implement various types of NFs [25, 33], leveraging multi-core parallelism to achieve close to 10 Gbps throughput per machine and scaling out to use more machines when needed. Ananta [39] is a software load-balancer deployed in Microsoft datacenters, providing cloud-scale load-balancing services. While software NFs can scale out, doing so adds considerable CAPEX and OPEX costs [22, 39].

To accelerate software packet processing, previous work has proposed using GPUs [26], specialized network processors (NPs) [2, 5], and hardware switches [22]. GPUs, primarily designed for graphic processing, can achieve 40 Gbps packet switching speed but incur high latency (about 200 Âµs) [26]. Compared to GPUs, FPGAs are more flexible and can be reconfigured to capture data and pipeline parallelisms, which are common in NFs. NPs, while specialized for network traffic, have limited programmability.

ClickNP is an FPGA-accelerated platform for highly flexible and high-performance NFs in commodity servers. It is completely programmable using high-level language and provides a modular architecture familiar to software programmers. ClickNP supports joint CPU/FPGA packet processing and offers high performance. Our evaluations show that ClickNP improves the throughput of NFs by 10x compared to state-of-the-art software NFs, while also reducing latency by 10x. This work demonstrates that FPGA programming is feasible and practical, although the compilation time remains a challenge. ClickNP alleviates this by enabling cross-platform development and hopes that most bugs can be detected by running elements on the CPU. In the long term, HDL synthesis tools should be optimized to shorten compilation times.

### Acknowledgements

We would like to thank Andrew Putnam, Derek Chiou, and Doug Burger for their technical discussions. We also thank the entire Catapult v-team at Microsoft for the Catapult Shell and support of OpenCL programming. Special thanks to Albert Greenberg and Dave Maltz for their support and suggestions on the project, and Tong He for his contribution to the ClickNP PCIe channel development. Finally, we thank our shepherd, KyoungSoo Park, and other anonymous reviewers for their valuable feedback and comments.

### References

[References listed as provided, without changes.]

---

This revised version aims to improve clarity, coherence, and professionalism while maintaining the original content and structure.