Our detection methods are implemented in a tool, sybil-
hunter, which takes as input our two datasets and then at-
tempts to expose Sybil groups, as illustrated in Figure 1.
Sybilhunter is implemented in Go and consists of 2,300
lines of code.
Tor relays are uniquely identiﬁed by their ﬁngerprint,
a Base32-encoded and truncated SHA-1 hash over their
public key. Operators can further assign a nickname to
their Tor relays, which is a string that identiﬁes a relay
(albeit not uniquely) and is easier to remember than its
pseudo-random ﬁngerprint. Exit relays have an exit pol-
icy—a list of IP addresses and ports that the relay allows
connections to. Finally, operators that run more than one
relay are encouraged to conﬁgure their relays to be part
4.1 Datasets
Figure 1 shows how we use our two datasets. Archived
consensuses and router descriptors (in short: descriptors)
allow us to (i) restore past states of the Tor network,
which sybilhunter mines for Sybil groups, and to (ii) ﬁnd
“partners in crime” of malicious exit relays that we dis-
covered by running exitmap, a scanner for Tor exit relays
that we discuss below.
USENIX Association  
25th USENIX Security Symposium  1171
3
NearestneighborsPotentialSybilssybilhunterTor networkBad exitrelayConsensusesand descriptorsAllrelaysMaliciousrelaysExitrelayexitmapsybilhunterDecoywebsiteDataset
Consensuses
Descriptors
# of ﬁles
72,061
34,789,777
Size
Time span
51 GiB 10/2007–01/2016
52 GiB 12/2005–01/2016
Table 1: An overview of our primary dataset; consen-
suses and server descriptors since 2007 and 2005, respec-
tively.
4.1.2 Malicious exit relays
In addition to our publicly available and primary dataset,
we collected malicious exit relays over 18 months. We
call exit relays malicious if they modify forwarded trafﬁc
in bad faith, e.g., to run man-in-the-middle attacks. We
add these relays to our dataset because they frequently
surface in groups, as malicious Sybils, because an at-
tacker runs the same attack on several, physically dis-
tinct exit relays. Winter et al.’s work [37, § 5.2] further
showed that attackers make an effort to stay under the
radar, which is why we cannot only rely on active prob-
ing to ﬁnd such relays. We also seek to ﬁnd potential
“partners in crime” of each newly discovered malicious
relay, which we discuss in Section 4.3.4.
We exposed malicious exit relays using Winter et al.’s
exitmap tool [37, § 3.1]. Exitmap is a Python-based
scanning framework for Tor exit relays. Exitmap mod-
ules perform a network task that can then be run over all
exit relays. One use case is HTTPS man-in-the-middle
detection: A module can fetch the certiﬁcate of a web
server over all exit relays and then compare its ﬁnger-
print with the expected, valid ﬁngerprint. Exposed at-
tacks are sometimes difﬁcult to attribute because an at-
tack can take place upstream of the exit relay, e.g., at a
malicious autonomous system. However, attribution is
only a secondary concern. Our primary concern is pro-
tecting Tor users from harm, and we do not need to iden-
tify the culprit to do so.
In addition to using the original exitmap modules [37,
§ 3.1], we implemented modules that detect HTML and
HTTP tampering by connecting to a decoy server under
our control, and ﬂagging an exit relay as malicious if the
returned HTML or HTTP was modiﬁed, e.g., to inject
data or redirect a user over a transparent HTTP proxy.
Since we controlled the decoy server, we knew what our
Tor client should get in response. Our modules ran pe-
riodically from August 2014 to January 2016, and dis-
covered 251 malicious exit relays whose attacks are dis-
cussed in Appendix A. We reported all relays to The Tor
Project, which subsequently blocked these relays.
Figure 2: Our primary dataset contains nine years worth
of consensuses and router descriptors.
4.1.1 Consensuses and router descriptors
The consensus and descriptor dataset is publicly avail-
able on CollecTor [32], an archiving service that is run
by The Tor Project. Some of the archived data dates back
to 2004, allowing us to restore arbitrary Tor network con-
ﬁgurations from the last decade. Not all of CollecTor’s
archived data is relevant to our hunt for Sybils, though,
which is why we only analyze the following two:
Descriptors Tor relays and bridges periodically upload
router descriptors, which capture their conﬁguration, to
directory authorities. Figure 2 shows an example in the
box to the right. Relays upload their descriptors no later
than every 18 hours, or sooner, depending on certain con-
ditions. Note that some information in router descriptors
is not veriﬁed by directory authorities. Therefore, relays
can spoof information such as their operating system, Tor
version, and uptime.
Consensuses Each hour, the nine directory authorities
vote on their view of all Tor relays that are currently on-
line. The vote produces the consensus, an authoritative
list that comprises all running Tor relays, represented as
a set of router statuses. Each router status in the consen-
sus contains basic information about Tor relays such as
their bandwidth, ﬂags, and exit policy. It also contains a
pointer to the relay’s descriptor, as shown in Figure 2. As
of June 2016, consensuses contain approximately 7,000
router statuses, i.e., each hour, 7,000 router statuses are
published, and archived, by CollecTor.
Table 1 gives an overview of the size of our consen-
sus and descriptor archives. We found it challenging to
repeatedly process these millions of ﬁles, amounting to
more than 100 GiB of uncompressed data, so we imple-
mented a custom parser in Go [36].
1172  25th USENIX Security Symposium 
USENIX Association
4
∙ Descriptor pointer∙ Nickname∙ Fingerprint∙ Publication∙ Address and ports∙ Flags∙ Version∙ Bandwidth∙ Exit policyRouter statusesConsensus∙ Address and ports∙ Platform∙ Protocols∙ Published∙ Fingerprint∙ Uptime∙ Bandwidth∙ SignatureRouter descriptorgroups. We iteratively improved our code and augmented
it with new features when we experienced operational
shortcomings.
4.3.1 Network churn
The churn rate of a distributed system captures the rate
of joining and leaving network participants. In the Tor
network, these participants are relays. An unexpect-
edly high churn rate between two subsequent consen-
suses means that many relays joined or left, which can re-
veal Sybils and other network issues because many Sybil
operators start and stop their Sybils at the same time, to
ease administration—they behave similarly.
The Tor Project is maintaining a Python script [15]
that determines the number of previously unobserved re-
lay ﬁngerprints in new consensuses.
If that number is
greater than or equal to the static threshold 50, the script
sends an e-mail alert. We reimplemented the script in
sybilhunter and ran it over all archived consensus docu-
ments, dating back to 2007. The script raised 47 alerts
in nine years, all of which seemed to be true positives,
i.e., they should be of interest to The Tor Project. The
script did not raise false positives, presumably because
the median number of previously unseen ﬁngerprints in
a consensus is only six—signiﬁcantly below the conser-
vative threshold of 50. Yet, the threshold likely causes
false negatives, but we cannot determine the false nega-
tive rate because we lack ground truth. In addition, The
Tor Project’s script does not consider relays that left the
network, does not distinguish between relays with differ-
ent ﬂags, and does not adapt its threshold as the network
grows. We now present an alternative approach that is
more ﬂexible and robust.
We found that churn anomalies worthy of our attention
range from ﬂat hills (Figure 4) to sudden spikes (Fig-
ure 5). Flat hills can be a sign of an event that affected a
large number of relays, over many hours or days. Such
an event happened shortly after the Heartbleed bug, when
The Tor Project asked relay operators to generate new
keys. Relay operators acted gradually, most within two
days. Sudden spikes can happen if an attacker adds many
relays, all at once. These are mere examples, however;
the shape of a time series cannot tell us anything about
the nature of the underlying incident.
To quantify the churn rate α between two subsequent
consensus documents, we adapt Godfrey et al.’s formula,
which yields a churn value that captures both systems
that joined and systems that left the network [13, § 2.1].
However, an unusually low number of systems that left
could cancel out an unusually high number of new sys-
tems and vice versa—an undesired property for a tech-
nique that should spot abnormal changes. To address
this issue, we split the formula in two parts, creating a
Figure 3: Sybilhunter’s internal architecture. After an
optional ﬁltering step, data is then passed on to one of
three analysis modules that produce as output either CSV
ﬁles or an image.
4.2 Threat model
Most of this paper is about applying sybilhunter to
archived network data, but we can also apply it to newly
incoming data. This puts us in an adversarial setting
as attackers can tune their Sybils to evade our system.
This is reﬂected in our adversarial assumptions. We as-
sume that an adversary does run more than one Tor re-
lay and exhibits redundancy in their relay conﬁguration,
or uptime sequence. An adversary further can know
how sybilhunter’s modules work, run active or passive
attacks, and make a limited effort to stay under the radar,
by diversifying parts of their conﬁguration. To detect
Sybils, however, our heuristics require some redundancy.
4.3 Analysis techniques
Having discussed our datasets and threat model, we now
turn to presenting techniques that can expose Sybils. Our
techniques are based on the insight that Sybil relays fre-
quently behave or appear similarly. Shared conﬁgu-
ration parameters such as port numbers and nicknames
cause similar appearance whereas Sybils behave simi-
larly when they reboot simultaneously, or exhibit iden-
tical quirks when relaying trafﬁc.
Sybilhunter can analyze (i) historical network data,
dating back to 2007; (ii) online data, to detect new Sybils
as they join the network; and (iii) ﬁnd relays that might
be associated with previously discovered, malicious re-
lays. Figure 3 shows sybilhunter’s internal architecture.
Tor network data ﬁrst passes a ﬁltering component that
can be used to inspect a subset of the data, e.g., only
relays with a given IP address or nickname. The data
is then forwarded to one or more modules that imple-
ment an analysis technique. These modules work inde-
pendently, but share a data structure to ﬁnd suspicious re-
lays that show up in more than one module. Depending
on the analysis technique, sybilhunter’s output is either
CSV ﬁles or images.
While developing sybilhunter, we had to make many
design decisions that we tackled by drawing on the expe-
rience we gained by manually analyzing numerous Sybil
USENIX Association  
25th USENIX Security Symposium  1173
5
CSV ﬁleImageConsensusesand descriptorssybilhunterFilterFingerprintsChurnUptimeSuspiciousrelaysFigure 4: A ﬂat hill of new relays in 2009. The time
series was smoothed using a moving average with a win-
dow size of 12 hours.
Figure 5: A sudden spike of new relays in 2010. The
time series was smoothed using a moving average with a
window size of 12 hours.
time series for new relays (αn) and for relays that left
(αl). Ct is the network consensus at time t, and \ denotes
the complement between two consensuses, i.e., the relays
that are in the left operand, but not the right operand. We
deﬁne αn and αl as
αn = |Ct \Ct−1|
and
|Ct|
αl = |Ct−1 \Ct|
|Ct−1|
.
(1)
Both αn and αl are bounded to the interval [0,1]. A
churn value of 0 indicates no change between two subse-
quent consensuses whereas a churn value of 1 indicates
a complete turnover. Determining αn,l for the sequence
Ct ,Ct−1, . . . , Ct−n, yields a time series of churn values
that can readily be inspected for abnormal spikes. Fig-
ure 6 illustrates the maximum number of Sybils an at-
tacker can add to the network given a threshold for α.
The ﬁgure shows both the theoretical maximum and a
more realistic estimate that accounts for noise, i.e., the
median number of new relays in each consensus, which
is 73.4 We found that many churn anomalies are caused
by relays that share a ﬂag, or a ﬂag combination, e.g.,
HSDir (onion service directories) and Exit (exit relays).
Therefore, sybilhunter can also generate per-ﬂag churn
time series that can uncover patterns that would be lost
in a ﬂag-agnostic time series.
4Note that this analysis is “memoryless” and includes relays that
have been online before; unlike the analysis above that considered only
previously unobserved relays, for which the median number was six.
6
Figure 6: The number of new Sybils (y axis) that can
remain undetected given a threshold for the churn value
α (x axis). The diagram shows both the maximum and a
more realistic estimate that accounts for the median num-
ber of new relays in consensuses.
Finally, to detect changes in the underlying time se-
ries trend—ﬂat hills—we can smooth αn,l using a simple
moving average λ deﬁned as
λ =
1
w ·
w
∑
i=0