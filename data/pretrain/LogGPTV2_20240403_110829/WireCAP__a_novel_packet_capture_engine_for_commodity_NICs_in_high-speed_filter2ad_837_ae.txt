#
#
e
t
a
R
p
o
r
D
#
t
e
k
c
a
P
80%#
60%#
40%#
20%#
WireCAPEBE(64,#400)#
WireCAPEBE(128,#200)#
WireCAPEBE(256,#100)#
0%#
1000#
10000#
1000000#
Number#of#Packets#(Logarithmic#Scale)#
100000#
Figure 10. WireCAP packet capture in the basic mode (R and 
10000000#
M are varied, R*M is fixed) 
Packet  capture  in  the  advanced  mode.  The  traffic  generator 
replays the captured data at the speed exactly as recorded. NIC1 is 
configured  with  n  receive  queues,  with  each  queue  tied  to  a 
distinct core. A multi_pkt_handler runs at the system and spawns 
n pkt_handler threads. Each thread runs on a core that has a tied 
receive queue. It captures and processes traffic from its queue. For 
pkt_handler, x is set to 300. We vary the packet capture engines in 
the  experiments,  using  PF_RING,  DNA,  NETMAP,  and 
WireCAP in the basic mode and WireCAP in the advanced mode, 
respectively.  For  WireCAP  in  the  advanced  mode,  the  n  queues 
form a single buddy group.  
50%#
45%#
40%#
35%#
30%#
25%#
20%#
15%#
10%#
5%#
0%#
#
#
e
t
a
R
p
o
r
D
#
t
e
k
c
a
P
PF_RING#
DNA#
NETMAP#
WireCAPGBG(256,100)#
WireCAPGBG(256,500)#
WireCAPGAG(256,100,60%)#
WireCAPGAG(256,500,60%)#
4#Queues#
5#Queues#
6#Queues#
Number#of#Queues#
Figure 11. WireCAP packet capture in the advanced mode, 
with a heavy packet-processing load (x=300) 
In  the  basic  mode,  WireCAP  can  capture  packets  at  wire  speed 
and  effectively  handle  short-term  bursts  of  packets.  Although 
WireCAP  in  the  basic  mode  achieves  better  performance  than 
existing  packet  capture  engines,  it  still  suffers  significant  packet 
losses, due to long-term load imbalance (Figure 11). WireCAP in 
the advanced mode implements the buddy-group-based offloading 
mechanism  to  address  that  problem.  When  a  long-term  load 
imbalance  occurs,  the  offloading  mechanism  will  offload  some 
traffic from a busy core (receive queue) to less busy or idle cores 
(receive  queues)  where  other  threads  can  process  it.  This 
mechanism allows the system resources to be better utilized. It is 
evident  that  the  offloading  mechanism  achieves  a  significantly 
improved performance (Figure 11). For WireCAP in the advanced 
mode,  the  offloading  mechanism  is  triggered  when  the  queue 
length  of  a  capture  queue  exceeds  the  offloading  percentage 
threshold (T). In general, WireCAP performs better when T is set 
to a relatively lower value (Figure 12).  
18%#
16%#
14%#
12%#
10%#
8%#
6%#
4%#
2%#
0%#
#
#
e
t
a
R
p
o
r
D
#
t
e
k
c
a
P
WireCAP@A@(256,100,60%)#
WireCAP@A@(256,100,70%)#
WireCAP@A@(256,100,80%)#
WireCAP@A@(256,100,90%)#
4#Queues#
5#Queues#
6#Queues#
Figure 12. WireCAP packet capture in the advanced mode (R 
Number#of#Queues#
and M are fixed, T is varied) 
to  pkt_handler:  a  processed  packet 
Packet  forwarding.  We  repeated  the  above  experiments  with  a 
small  modification 
is 
forwarded  through  NIC2  (Figure  2)  instead  of  being  discarded. 
NIC2 is directly connected to a packet receiver. By counting the 
number of packets that the traffic generator sends and the number 
of  packets  the  traffic  receiver  receives,  we  calculate  the  packet 
drop rate. Figure 13 illustrates the experiment results.  
that  WireCAP’s  packet 
  The  experiments  demonstrate 
forwarding 
is  capable  of  supporting  middlebox 
applications. Again, the experiments reveal that the buddy-group-
based  offloading  mechanism  can  achieve  a  significant  improved 
performance.  
  We  cannot  make  multi_pkt_handler  work  under  NETMAP  in 
this  experiment.  Under  NETMAP,  a  pkt_handler  thread  cannot 
synchronize  between 
transmitting  because 
NETMAP’s  NIOCTXSYNC  (or  NIOCRXSYNC)  operations  do 
not work on a per-receive-queue (or per-transmit-queue) basis.  
receiving  and 
function 
40%#
35%#
30%#
25%#
20%#
15%#
10%#
5%#
0%#
#
#
e
t
a
R
p
o
r
D
#
t
e
k
c
a
P
PF_RING#
DNA#
WireCAPDBD(256,100)#
WireCAPDBD(256,500)#
WireCAPDAD(256,100,60%)#
WireCAPDAD(256,500,60%)#
4#Queues#
5#Queues#
6#Queues#
Number#of#Queues#
Figure 13. WireCAP packet forwarding 
WireCAP scalability. We now evaluate and discuss WireCAP’s 
scalability  performance.  In  the  experiments,  each  of  NIC1  and 
NIC2 is connected directly to a traffic generator. The generators 
transmit 1x109 64-Byte or 100-Byte packets at the wire rate. Each 
NIC is configured with n receive queues, with each queue tied to a 
distinct core.  
• 
For NIC1, a multi_pkt_handler is launched, which spawns n 
pkt_handler  threads.  Each  thread  runs  on  a  core  that  has  a 
tied receive queue of NIC1. It captures and processes traffic 
from its queue. For pkt_handler, x is set to 0, with processed 
packet forwarded on through NIC2. 
For NIC2, a multi_pkt_handler is similarly launched except 
that captured packets are forwarded on through NIC1. 
• 
403
In this experiment, we only compare DNA and WireCAP in the 
advanced mode because PF_RING’s performance is too poor and 
we cannot make multi_pkt_handler work under NETMAP. With 
WireCAP  in  the  advanced  mode,  NIC1’s  queues  form  a  buddy 
group  and  NIC2’s  queues  form  another  buddy  group.  Each  of 
NIC1  and  NIC2  is  directly  connected  to  a  packet  receiver.  By 
counting  the  number  of  packets  that  the  traffic  generators  send 
and  the  number  of  packets  that  the  traffic  receivers  receive,  we 
calculate the packet drop rate. Figure 14 illustrates the experiment 
results. 
  When  the  generators  transmit  100-Byte  packets,  NIC1  and 
NIC2 in together receive approximately 20 million p/s. We did not 
observe any packet loss for WireCAP and DNA. The experiment 
indicates  that  WireCAP  scales  well  with  multiple  NICs.  Please 
note: NIC1 and NIC2 are installed in a single NUMA node on our 
experiment system (Figure 2) 
  When  the  generators  transmit  64-Byte  packets,  the  system 
needs to handle an approximate rate of 30 million p/s. Under such 
conditions, the experiment system bus becomes saturated, causing 
both  DNA  and  WireCAP  to  suffer  significant  packet  drops. 
Compared with DNA, WireCAP requires extra I/O operations and 
memory  accesses  to  implement  its  ring-buffer-pool  and  buddy-
group-based  offloading  mechanisms.  When  the  system  bus  is 
saturated, the I/O operations and memory accesses become costly. 
As  a  consequence,  WireCAP  suffers  a  higher  packet  drop  rate 
than  DNA  @  queues/NIC=1.  WireCAP-A-(256,100,60%) 
performs  similar  to  DNA  with  the  number  of  queues  per  NIC 
increased.  Again,  the  experiments  reveal  that  the  buddy-group-
based  offloading  mechanism  can  achieve  a  significant  improved 
performance.  WireCAP-A-(256,500,60%)  performs  poorly  @ 
queues/NIC=5 or 6. With the number of receive queues per NIC 
increased,  WireCAP-A-(256,500,60%)  requires  larger  memory 
use.  A  “big-memory”  application  typically  pays  a  high  cost  for 
page-based virtual memory [27]. However, WireCAP is designed 
to  use  additional  system  resources  to  address  the  packet  drop 
problem.  Certainly  WireCAP  of  such  a  design  will  lose  some 
scalability  performance.  We  believe  this  is  an  appropriate 
tradeoff. 
DNA@100B#
WireCAPCAC(256,100,#60%)@64B#
WireCAPCAC(256,500,#60%)@100B#
DNA@64B#
WireCAPCAC(256,500,#60%)@64B#
WireCAPCAC(256,100,#60%)@100B#
30%#
25%#
20%#
15%#
10%#
5%#
0%#
#
e
t
a
r
#
p
o
r
#
t
e
k
c
a
P
1#
5#
2#
Number#of#receive#queues#per#NIC#
3#
4#
6#
Figure 14. Scalability experiment 
5 Discussions  
a. The use of additional resources 
  Our  experiments  have  demonstrated  that  WireCAP  achieves 
better packet capture performance than that of the existing packet 
capture engines through the use of additional resources. WireCAP 
employs  additional  computing  resources  by  dedicating  a  capture 
thread  to  perform  low-level  ioctl  operations  for  each  receive 
404
is 
in 
two  Kbytes 
(Note:  a  cell 
queue.  A  modern  multicore  system  can  provide  sufficient 
computing resources to support WireCAP operations.   
  WireCAP  utilizes  large  amounts  of  kernel  space  memory  to 
support  its  ring-buffer-pool  mechanism.  For  WireCAP-B-(M,  R) 
or WireCAP-A-(M, R, T), a single pool requires R*M*2K bytes of 
memory 
the  current 
implementation).  If  n  receive  queues  are  configured,  then 
n*R*M*2K  bytes  are  required.  Because  WireCAP’s  buffering 
capability is proportional to the ring buffer capacity R*M, there is 
a  tradeoff  between  WireCAP’s  buffering  capability  and  its 
memory consumption. 
b. The choice of T 
  Determining  an  optimal  value  for  T  in  WireCAP’s  advanced 
mode  operation  also  presents  tradeoff  challenges.  Offloading  is 
necessary  to  prevent  packet  drops  due  to  a  long-term  load 
imbalance.  However,  redirecting  packets  to  different,  less  busy 
capture queues can result in a degraded CPU efficiency caused by 
a loss of the core affinity on packet processing [11]. Therefore, a 
simple guideline for configuring T is as follows: When avoiding 
packet  drops  is  critical  to  the  application,  T  should  be  set  to  a 
relatively lower value (e.g., 50%); otherwise, T should be set to a 
relatively higher value (e.g., 80%). 