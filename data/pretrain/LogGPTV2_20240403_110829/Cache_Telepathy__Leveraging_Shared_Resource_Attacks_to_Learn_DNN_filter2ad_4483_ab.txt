and corresponding query results will be used as training data
USENIX Association
29th USENIX Security Symposium    2005
①construct	dataset②query	oracle	network③get	a	DNN	architecture	from	a	search	space④train	a	substitute	network⑥output	substitute	architecture	and	weights⑤accuracy	enough?training	datatraining	labelsnoyesand labels later. In the case that the oracle architecture is not
known, the attacker needs to choose a DNN architecture from
a search space (z) and then train a network with the chosen
architecture ({). Steps z-{ repeat until a network is found
with sufﬁcient prediction accuracy (|).
This attack process is extremely compute intensive, since it
involves many iterations of step {. Considering the depth and
complexity of state-of-the-art DNNs, training and validating
each network can take hours to days. Moreover, without
any information about the architecture, the search space of
possible architectures is often intractable, and thus, the model
extraction attack is infeasible. However, Cache Telepathy can
reduce the architecture search space (z) to a tractable size and
make the attack feasible in settings where DNN architectures
are unknown.
Membership inference attacks suffer from a more serious
problem if the DNN architecture is not known. Recall that
the attack aims to ﬁgure out the composition of the oracle
training data set (Section 2.2). If there are many different can-
didate architectures, the attacker needs to consider the results
generated by all the candidate architectures and statistically
summarize inconsistent results from those architectures. A
large search space of candidate architectures, not only sig-
niﬁcantly increases the computation requirements, but also
potentially hurts attack accuracy. Consider a candidate archi-
tecture which is very different from the oracle architecture. It
is likely to contribute incorrect results, and in turn, decrease
the attack accuracy. However, Cache Telepathy can reduce
the search space to a reasonable size. Moreover, the candidate
architectures in the reduced search space have the same or
very similar hyper-parameters as the oracle network. There-
fore, they perform very similarly to the oracle network on
various data sets. Hence, our attack also plays an important
role in membership inference attacks.
Overall Cache Telepathy Attack Procedure Our attack is
based on two observations. First, DNN inference relies heav-
ily on GEMM (Generalized Matrix Multiply). We conduct a
detailed analysis of how GEMM is used in ML frameworks,
and ﬁgure out the mapping between DNN hyper-parameters
and matrix parameters (Section 4). Second, high-performance
GEMM algorithms are vulnerable to cache-based side channel
attacks, as they are all tuned for the cache hierarchy through
matrix blocking (i.e., tiling). When the block size is public
(or can be easily deduced), the attacker can use the cache side
channel to count blocks and learn the matrix sizes.
The Cache Telepathy attack procedure includes a cache
attack and post processing steps. First, it uses a cache at-
tack to monitor matrix multiplications and obtain matrix pa-
rameters (Sections 5 and 6). Then, the DNN architecture
is reverse-engineered based on the mapping between DNN
hyper-parameters and matrix parameters (Section 4). Finally,
Cache Telepathy prunes the possible values of the remain-
ing undiscovered hyper-parameters and generates a pruned
search space for the target DNN architecture (Section 8.3).
We consider the attack to be successful if we can generate a
reasonable number of candidate architectures whose hyper-
parameters are the same or very similar to the oracle network.
4 Mapping DNNs to Matrix Parameters
DNN hyper-parameters, listed in Section 2.1, can be mapped
to GEMM execution. We ﬁrst discuss how the layer type and
conﬁgurations within each layer map to matrix parameters, as-
suming that all layers are sequentially connected (Section 4.1
and 4.2). We then generalize the mapping by showing how the
connections between layers map to GEMM execution (Sec-
tion 4.3). Finally, we discuss what information is required to
extract the activation functions of Section 2.1 (Section 4.4).
4.1 Analysis of DNN Layers
There are two types of neural network layers whose com-
putation can be mapped to matrix multiplications, namely
fully-connected and convolutional layers.
4.1.1 Fully-connected Layer
In a fully-connected layer, each neuron computes a weighted
sum of values from all the neurons in the previous layer,
followed by a non-linear transformation. The ith layer com-
putes outi = fi(ini ⊗ θi) where ini is the input vector, θi is the
weight matrix, ⊗ denotes a matrix-vector operation, f is an
element-wise non-linear function such as tanh or sigmoid,
and outi is the resulting output vector.
The feed-forward computation of a fully-connected DNN
can be performed over a batch of a few inputs at a time (B).
These multiple input vectors are stacked into an input matrix
Ini. A matrix multiplication between the input matrix and
the weight matrix (θi) produces an output matrix, which is
a stack of output vectors. We represent the computation as
Oi = fi(Ini · θi) where Ini is a matrix with as many rows as
B and as many columns as Ni (the number of neurons in the
layer i); Oi is a matrix with as many rows as B and as many
columns as Ni+1 (the number of neurons in the layer i + 1);
and θi is a matrix with Ni rows and Ni+1 columns. Table 1
shows the number of rows and columns of all the matrices.
Matrix
Input: Ini
Weight: θi
Output: Oi
n_row n_col
B
Ni
B
Ni
Ni+1
Ni+1
Table 1: Matrix sizes in a fully-connected layer.
2006    29th USENIX Security Symposium
USENIX Association
4.1.2 Convolutional Layer
In a convolutional layer, a neuron is connected to only a
spatial region of neurons in the previous layer. Consider the
upper row of Figure 2, which shows the computation in the
ith layer. The layer generates an output outi (right part of the
upper row) by performing convolution operations on an input
ini (center of the upper row) with multiple ﬁlters (left part of
the upper row). The input volume ini is of size Wi × Hi × Di,
where the depth (Di) also refers to the number of channels of
the input. Each ﬁlter is of size Ri × Ri × Di.
Figure 2: Mapping a convolutional layer (upper part of the
ﬁgure) to a matrix multiplication (lower part).
To see how a convolution operation is performed, the ﬁgure
highlights the process of generating one output neuron in
outi. The neuron is a result of a convolution operation – an
elementwise dot product of the ﬁlter shaded in dots and the
subvolume in ini shaded in dashes. Both the subvolume and
the ﬁlter have dimensions Ri × Ri × Di. Applying one ﬁlter
on the entire input volume (ini) generates one channel of the
output (outi). Thus, the number of ﬁlters in layer i (Di+1) is
the number of channels (depth) in the output volume.
The lower row of Figure 2 shows a common implementa-
tion that transforms the multiple convolution operations in a
layer into a single matrix multiply. First, as shown in arrow x,
each ﬁlter is stretched out into a row to form a matrix F(cid:48)
i . The
number of rows in F(cid:48)
i is the number of ﬁlters in the layer.
Second, as shown in arrow y, each subvolume in the in-
put volume is stretched out into a column. The number of
elements in the column is Di × R2
i . For an input volume with
dimensions Wi×Hi×Di, there are (Wi−Ri +Pi)(Hi−Ri +Pi)
such columns in total, where Pi is the amount of zero padding.
We call this transformed input matrix in(cid:48)
i. Then, the convolu-
i · in(cid:48)
tion becomes a matrix multiply: out(cid:48)
i = F(cid:48)
Finally, the out(cid:48)
i matrix is reshaped back to its proper
dimensions of the outi volume (arrow {). Each row of
the resulting out(cid:48)
i matrix corresponds to one channel in the
i (z).
outi volume. The number of columns of the out(cid:48)
i matrix is
(Wi − Ri + Pi)(Hi − Ri + Pi), which is the size of one output
channel, namely, Wi+1 × Hi+1. Table 2 shows the number of
rows and columns of the matrices involved.
Matrix
in(cid:48)
i
F(cid:48)
i
out(cid:48)
i
n_row
Di × R2
i
Di+1
Di+1
(Wi − Ri + Pi)(Hi − Ri + Pi)
(Wi − Ri + Pi)(Hi − Ri + Pi) = Wi+1 × Hi+1
n_col
Di × R2
i
Table 2: Matrix sizes in a convolutional layer.
The matrix multiplication described above processes a sin-
gle input. As with fully-connected DNNs, CNN inference
can consume a batch of B inputs in a single forward pass. In
this case, a convolutional layer performs B matrix multiplica-
tions per pass. This is different from fully-connected layers,
where the entire batch is computed using only one matrix
multiplication.
4.2 Resolving DNN Hyper-parameters
Based on the previous analysis, we can now map DNN hyper-
parameters to matrix operation parameters assuming all layers
are sequentially connected.
4.2.1 Fully-connected Networks
Consider a fully-connected network. Its hyper-parameters
are the number of layers, the number of neurons in each
layer (Ni) and the activation function per layer. As discussed
in Section 4.1, the feed-forward computation performs one
matrix multiplication per layer. Hence, we extract the number
of layers by counting the number of matrix multiplications
performed. Moreover, according to Table 1, the number of
neurons in layer i (Ni) is the number of rows of the layer’s
weight matrix (θi). The ﬁrst two rows of Table 3 summarize
this information.
Structure
FC network
FC layeri
Conv network
Conv layeri
Hyper-Parameter
# of layers
Ni: # of neurons
# of Conv layers
Di+1: # of ﬁlters
Ri: ﬁlter
width and height1
Pi: padding
Pooli or
Stridei+1
pool or stride
width and height
Value
# of matrix muls
n_row(θi)
# of matrix muls / B
(cid:114) n_row(in(cid:48)
(cid:114) n_col(out(cid:48)
n_row(F(cid:48)
i )
i)
n_row(out(cid:48)
i−1)
difference between:
i−1),n_col(in(cid:48)
i)
n_col(in(cid:48)
≈
i )
i+1)
n_col(out(cid:48)
Table 3: Mapping between DNN hyper-parameters and matrix
parameters. FC stands for fully connected.
1Speciﬁcally, we learn the ﬁlter spatial dimensions. If the ﬁlter is not
square, the search space grows depending on factor combinations (e.g., 2 by
USENIX Association
29th USENIX Security Symposium    2007
HiWiDiHi+1Wi+1Di+1RiRiRifiltersfilter0x=Ri2Dichannel0(Wi-Ri+Pi)(Hi-Ri+Pi)filter1……..……..channel1……..……..①②③④inioutiDiF’iin’iout’iRiDiDi+14.2.2 Convolutional Networks
A convolutional network generally consists of four types of
layers: convolutional, Relu, pooling, and fully connected. Re-
call that each convolutional layer involves a batch B of matrix
multiplications. Moreover, the B matrix multiplications that
correspond to the same layer, always have the same dimen-
sion sizes and are executed consecutively. Therefore, we can
count the number of consecutive matrix multiplications which
have the same computation pattern to determine B.
In a convolutional layer i, the hyper-parameters include the
number of ﬁlters (Di+1), the ﬁlter width and height (Ri), and
the padding (Pi). We assume that the ﬁlter width and height
are the same, which is the common case. Note that for layer i,
we consider that the depth of the input volume (Di) is known,
as it can be obtained from the previous layer.
i matrix is Di × R2
We now show how these parameters for a convolutional
layer can be reverse engineered. From Table 2, we see that
the number of ﬁlters (Di+1) is the number of rows of the ﬁlter
matrix F(cid:48)
i . To attain the ﬁlter width (Ri), we note that the
number of rows of the in(cid:48)
i , where Di is the
number of output channels in the previous layer and is equal
to the number of rows of the out(cid:48)
i−1 matrix. Therefore, as
summarized in Table 3, the ﬁlter width is attained by dividing
the number of rows of in(cid:48)
i−1 and
performing the square root. In the case that layer i is the ﬁrst
one, directly connected to the input, the denominator (out(cid:48)
0)
of this fraction is the number of channels of the input of the
network, which is public information.
i by the number of rows of out(cid:48)
Padding results in a larger input matrix (in(cid:48)
i). After re-
solving the ﬁlter width (Ri), the value of padding can be
deduced by determining the difference between the number
of columns of the output matrix of layer i− 1 (out(cid:48)
i−1), which
is Wi×Hi, and the number of columns of the in(cid:48)
i matrix, which
is (Wi − Ri + P)(Hi − Ri + P).
4.3 Connections Between Layers
We now examine how to map inter-layer connections to
GEMM execution. We consider two types of inter-layer con-
nections, i.e., sequential connections and non-sequential con-
nections.
4.3.1 Mapping Sequential Connections