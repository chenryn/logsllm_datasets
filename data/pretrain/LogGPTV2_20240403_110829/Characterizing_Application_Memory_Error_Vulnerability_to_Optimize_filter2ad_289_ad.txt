### Tolerance Techniques for Reliable System Design

Tolerance techniques are a promising direction for enabling reliable system designs. Based on the results of our case study on data-intensive applications, we explore how to design systems that incorporate heterogeneous hardware and software techniques to achieve high reliability at a low system cost.

### VI. Heterogeneous-Reliability Memory Systems

#### Overview
The goals of our heterogeneous-reliability memory system design methodology are:
1. To provide high memory system reliability (e.g., 99.90% single server availability).
2. To achieve this at a low cost.

To meet these goals, our methodology involves three steps:
1. **Exploration of the Design Space**: We investigate the design space for systems that can employ heterogeneous hardware and software reliability techniques, motivated by the fact that memory error vulnerability and recoverability vary widely among different applications and memory regions.
2. **Mapping Applications to Configurations**: We examine how to map applications with different reliability characteristics to system configurations in the heterogeneous-reliability design space to achieve the desired level of reliability at a low cost. We use the WebSearch application as an example.
3. **Hardware/Software Support**: We discuss the necessary hardware and software support for the proposed new system designs.

We will now describe each of these steps in detail.

#### A. Design Space, Metrics, and Error Models

We explore three dimensions in the design space for systems with heterogeneous reliability:
1. **Hardware Techniques** for detecting and correcting errors.
2. **Software Responses** to errors.
3. **Usage Granularity** of different techniques.

**Table 4** lists the techniques considered in each dimension, along with their potential benefits and trade-offs. In addition to discussing system designs, we cover:
1. The metrics used to evaluate the benefits and costs of the designs.
2. The memory error model used to assess the effectiveness of the designs.

##### Hardware Techniques
Hardware techniques determine the amount of protection provided by memory devices. These techniques typically involve a trade-off between hardware cost and additional memory error detection and correction capabilities. We aim to use them sparingly to achieve the desired reliability.

- **Consumer Laptops and Desktops**: Often, no memory error detection or correction is used to reduce cost, which can lead to unpredictable crashes and data corruption.
- **Parity Information, SEC-DED, Chipkill, and Mirroring**: These more costly techniques are typically employed homogeneously in servers to achieve a desired level of reliability.
- **Less-Tested DRAM Chips**: Using less-tested DRAM chips can reduce costs but may decrease average reliability.

##### Software Responses
Software responses determine the actions taken by the software to tolerate or correct memory errors. These techniques often trade application/OS performance and implementation complexity for improved reliability.

- **Consuming the Error**: The simplest but least effective response, allowing the application to experience unpredictable behavior or data corruption.
- **Automatic Restart**: Automatically restarting applications upon error detection, reducing the likelihood of unpredictable behavior.
- **Retiring Memory Pages**: Some operating systems retire memory pages that have experienced a certain number of errors.
- **Conditional Error Consumption**: Allowing applications to conditionally consume errors based on identified tolerance levels.
- **Software Correction**: Performing corrections in the software/OS once errors are detected, potentially reducing hardware costs.

##### Usage Granularity
The usage granularity determines where hardware techniques or software responses are applied. Coarser granularities may require less overhead but do not leverage the diversity of tolerance among different applications and memory regions.

- **Coarsest Granularity**: Applying techniques across the entire physical machine, typical in servers.
- **Finer Granularities**: Applying techniques at the level of individual virtual machines, applications, memory regions/pages, and cache lines, providing more opportunities for customization but increasing management complexity.

##### Metrics
When evaluating a system configuration, we consider three metrics:
1. **Cost**: The amount spent on server hardware.
2. **Single Server Availability**: The fraction of time a server functions correctly over a certain period.
3. **Reliability**: The likelihood of incorrect behavior occurring.

##### Error Models
To evaluate reliability and availability, we use information from prior studies to derive the rate of occurrence of soft and hard errors. We then examine how these rates affect application reliability and single server availability based on our case study results.

**Figure 7** illustrates the high-level operation of our methodology, showing the inputs (memory error models, application memory access information, and evaluation metrics) and the process of mapping memory regions to hardware techniques and software responses.

#### B. Design Space Exploration

In this section, we use the results of our case study and the design space parameters to evaluate the effectiveness and cost of several memory system design points for the WebSearch application. We compare five design points to highlight the inefficiencies of traditional homogeneous approaches and the benefits of heterogeneous-reliability designs.

- **Typical Server**: A configuration resembling a typical server in a modern datacenter, using homogeneous SEC-DED ECC.
- **Consumer PC**: No hardware protection against memory errors, reducing both cost and reliability.
- **Detect&Recover**: Uses parity in hardware to detect errors in the private region and corrects them with a clean copy from disk in software. No error detection or correction for other data.
- **Less-Tested (L)**: Uses less-thoroughly-tested memory throughout the system.
- **Detect&Recover/L**: Combines the Detect&Recover design with less-tested DRAM.

**Table 6** shows the mapping of the WebSearch address space to different hardware/software design points and the corresponding trade-offs in cost, single server availability, and reliability.

- **Consumer PC Configuration**: Reduces memory cost by 11.1% but increases crashes to 19 per server per month, reducing single server availability to 99.55%.
- **Detect&Recover Configuration**: Reduces memory cost by 9.7% and causes only 3 crashes per server per month, achieving a single server availability of 99.93%.
- **Less-Tested DRAM**: Substantially reduces memory costs (16.4% to 37.8%) but significantly decreases server availability and reliability.
- **Detect&Recover/L Configuration**: Achieves both memory cost savings (3.1% to 27.9%) and high single server availability (99.90%), with only 4 crashes per server per month and 12 incorrect queries per million.

We conclude that heterogeneous-reliability memory system designs can enable systems to achieve high cost savings and high single server availability/reliability simultaneously.

While our analysis focuses on the WebSearch application, we believe other data-intensive applications may also benefit from these designs. **Figure 8** shows the maximum number of tolerable errors required to achieve a particular level of reliability for each data-intensive application examined in Section V.