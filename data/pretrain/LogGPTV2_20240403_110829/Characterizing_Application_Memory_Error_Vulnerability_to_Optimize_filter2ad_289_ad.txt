tolerance techniques are a promising direction for enabling
reliable system designs.
Based on the results of our case study of data-intensive ap-
plications, we next explore how to design systems composed
of heterogeneous hardware/software techniques to enable
high reliability at low system cost.
VI. Heterogeneous-Reliability Memory Systems
Overview. The goals for our heterogeneous-reliability
memory system design methodology are (1) to provide high
memory system reliability (e.g., enabling 99.90% single
server availability) at (2) low cost. To achieve these goals,
our methodology consists of three steps. First, motivated by
the fact that memory error vulnerability and recoverability
varies widely among diﬀerent applications and memory
regions, we perform an exploration of the design space for
systems that could employ heterogeneous hardware/software
reliability techniques. Second, we examine how ways of
mapping applications with diﬀerent reliability characteristics
to system conﬁgurations in the heterogeneous-reliability
design space can achieve the right amount of reliability at
low cost, using the WebSearch application as an example.
Third, we discuss some of the required hardware/software
support for the proposed new system designs. We describe
each of these steps in detail next.
A. Design Space, Metrics, and Error Models
We examine three dimensions in the design space for sys-
tems with heterogeneous reliability: (1) hardware techniques
to detect and correct errors, (2) software responses to errors,
and (3) the granularity at which diﬀerent techniques are
used. Table 4 lists the techniques we considered in each of
the dimensions along with their potential beneﬁts and trade-
oﬀs. In addition to discussing system designs, we discuss
(1) the metrics we use to evaluate the beneﬁts and costs
of the designs, and (2) the memory error model we use to
examine the eﬀectiveness of the designs. We discuss each of
these components of our design space exploration in turn.
Hardware Techniques. Hardware techniques to detect and
correct errors determine the amount of protection provided
by the memory devices in a system. These techniques
typically trade hardware cost for additional memory error
detection and correction capabilities, and so we would like
to use them as sparingly as possible to achieve a particular
amount of reliability in our system designs. We brieﬂy
review the various techniques next.
One technique employed in consumer laptops and desktops
to reduce cost is to not use any memory error detection
and correction capabilities in memory. While this technique
can save greatly on cost, it can also cause unpredictable
crashes and data corruption, as has been documented in
studies across a large number of consumer PCs [64]. Other
techniques (discussed in Section II) that store additional
data in order to detect and/or correct memory errors include
storing parity information, SEC-DED, Chipkill, and Mirror-
ing. These more costly techniques are typically employed
homogeneously in existing servers to achieve a desired
amount of reliability. Another technique that is orthogonal to
those we have discussed so far is to employ memory devices
that use less-tested DRAM chips. The testing performed
by vendors in order to achieve a certain quality of DRAM
device can add a substantial amount of cost [8], though it
can increase the average reliability of DRAM devices.
Software Responses. Software responses to errors deter-
mine any actions that are taken by the software to potentially
tolerate or correct a memory error. These techniques typi-
cally trade application/OS performance and implementation
complexity for improved reliability. Notice that some of
these software responses can potentially enable lower-cost
hardware techniques to be employed in a system, while still
achieving a desired amount of reliability. We discuss each
of these techniques next.
The simplest (but potentially least eﬀective) software re-
sponse to a memory error is to allow the application to
474
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:30:37 UTC from IEEE Xplore.  Restrictions apply. 
consume the error and experience unpredictable behavior or
data corruption [65]. As we have seen, however, some data-
intensive applications (such as WebSearch) can consume and
tolerate a considerable amount of errors without substantial
consequences. Another software response is to automati-
cally restart applications once an error has been detected
(whether through hardware or software techniques or other
indicators, such as an application crash), which can reduce
the likelihood that applications experience unpredictable
behavior. Retiring memory pages is a technique used in
some operating systems [15, 20–22] to help tolerate memory
errors by not allowing a physical page that has experienced
a certain number of errors to be allocated to applications.
Other software responses may require additional soft-
ware/OS modiﬁcation in order to implement. Allowing ap-
plications to conditionally consume errors once they have
been identiﬁed can potentially allow the software/OS to
make informed decisions about how tolerable errors in cer-
tain memory locations may be. Recent work that allows the
programmer to annotate the tolerance of their data structures
(e.g., [47]) is one way of allowing software to conditionally
consume errors. Finally, the software/OS may be able to
perform its own correction of memory errors once they are
detected in the hardware by storing additional data required
to repair corrupted data. Though these techniques require
software changes and may reduce performance, they have
the potential to reduce hardware cost by making up for the
deﬁciencies of less reliable, but cheaper, hardware devices.
Usage Granularity. The usage granularity determines
where hardware techniques or software responses are ap-
plied. Applying hardware techniques or software responses
at coarser granularities may require less overhead to manage,
but may not
leverage the diversity of tolerance among
diﬀerent applications and memory regions. For example, the
coarsest granularity, applying techniques across the entire
physical machine, is what is typically used in servers, and
may be ineﬃcient for applications or memory regions that
have diﬀerent reliability requirements (like the examples we
have examined in data-intensive applications).
Finer granularities for using hardware techniques and soft-
ware responses than the entire physical machine include
across individual virtual machines, applications, memory re-
gions/pages, and cache lines. Each increasingly ﬁner granu-
larity of usage provides more opportunities for how diﬀerent
hardware techniques and software responses are used across
data, but may also require more complexity to manage the
increased number of techniques and responses used.
Metrics. When evaluating a particular system conﬁguration
in the design space, we examine three metrics to quantify the
eﬀectiveness of a design: (1) cost, the amount spent on server
hardware for a particular system design, (2) single server
availability,
the fraction of time a server is functioning
correctly over a certain period of time,4 and (3) reliability,
the likelihood of incorrect behavior occurring.
Error Models. To evaluate the reliability and availability
of a particular design, we use information from prior studies
(e.g., [13, 15]) to derive the rate of occurrence of soft and
4Note that this is diﬀerent from application availability, which measures
the fraction of time an application is functioning correctly, typically across
multiple servers. In fact, for our applications, application availability would
likely be much higher than single server availability because our applica-
tions rely on software-level techniques that can tolerate server failures.
Figure 7: Mapping of applications to a heterogeneous-reliability
memory system design, arrows show an example mapping.
hard errors. We then examine how such rates of occurrence
aﬀect application reliability and single server availability
based on the measured results from our case studies of
applications.
Note that though we do not explicitly model how error
rates change depending on DRAM density, capacity, speed,
power, and other characteristics, our methodology is compat-
ible with other more complex, and potentially more precise,
ways of modeling the rate of memory error occurrence.
Tying everything together, Figure 7 illustrates the high
level operation of our methodology. At the top of the ﬁgure
are the inputs to our technique: memory error models,
application memory access information (such as an appli-
cation’s spatial and temporal locality of reference), and the
various metrics under evaluation and their constraints (e.g.,
99.90% single server availability). We choose as our usage
granularity memory regions for their good balance between
diversity of memory error tolerance and lower manage-
ment complexity. Based on the inputs, we explore diﬀerent
mappings of memory regions to hardware techniques and
which software responses to use. Finally, we choose the
heterogeneous-reliability memory system design that best
suits our needs. We next examine the results of performing
this process on the WebSearch application and the resulting
implications for hardware/software design.
B. Design Space Exploration
In this section, we use the results of our case study
presented in Section V and the design space parameters
discussed in Section VI to evaluate the eﬀectiveness and cost
of several memory system design points for the WebSearch
application. We chose ﬁve design points to compare against
to illustrate the ineﬃciencies of traditional homogeneous
approaches to memory system reliability as well as show
some of the beneﬁts of heterogeneous-reliability memory
system designs, which we next describe and contrast.
• Typical Server: A conﬁguration resembling a typical
server deployed in a modern datacenter. All memory is
homogeneously protected using SEC-DED ECC.
• Consumer PC: Consumer PCs typically have no hardware
protection against memory errors, reducing both their cost
and reliability.
475
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:30:37 UTC from IEEE Xplore.  Restrictions apply. 
Design Space ExplorationMemory Error ModelsApplication Access PatternSoftware CorrectionConditionally ConsumeRetire PageConsume Errors in AppSoftware ResponsesMetrics/ConstraintsMemcachedStackSEC-DEDNo Detection/CorrectionParityWebSearchPrivateUntested DRAM+ParityMemory RegionsHardware TechniquesWebSearchStackMemcachedHeapHardware MappingsWebSearchHeapDesign parameters
30%
DRAM/server HW cost
11.1%
NoECC memory cost savings
9.7%
Parity memory cost savings
18%±12%
L memory cost savings
10 mins
Crash recovery time
5 mins
Par+R ﬂush threshold
Errors/server/month [13]
2000
Target single server availability 99.9%
Mapping
Conﬁguration
Private
(36GB)
Heap
(9GB)
Stack
(60MB)
Memory cost
savings (%)
Metrics
Server
HW cost
savings (%)
Crashes/
server/
month
ECC
ECC
Typical Server
Consumer PC
Detect&Recover
Less-Tested (L) NoECC NoECC NoECC 27.1 (16.4-37.8) 8.1 (4.9-11.3)
Detect&Recover/L ECC Par+R NoECC 15.5 (3.1-27.9)
4.7 (0.9-8.4)
ECC
NoECC NoECC NoECC
Par+R NoECC NoECC
0.0
11.1
9.7
0.0
3.3
2.9
0
19
3
96
4
Single
server avail-
ability
100.00%
99.55%
99.93%
97.78%
99.90%
# incorrect/
million
queries
0
33
9
163
12
Table 6: Mapping of WebSearch address space to diﬀerent hardware/software design points and the corresponding trade-oﬀ in cost, single server
availability, and reliability. (ECC = SEC-DED memory; NoECC = no detection/correction; Par+R = parity memory and recovery from disk; L
= less-tested memory.)
• Detect&Recover: Based on our observation that some
memory regions are safer than others (Section V), we con-
sider a memory system design that, for the private region,
uses parity in hardware to detect errors and responds by
correcting them with a clean copy of data from disk in
software (Par+R, parity and recovery), and uses neither
error detection nor correction for the rest of its data.
• Less-Tested (L): Testing increases both the cost and aver-
age reliability of memory devices. This system examines
the implications of using less-thoroughly-tested memory
throughout the entire system.
as disk failure or power supply failure, here, we examine
server availability from the perspective of only memory
errors. We also show the number of incorrect responses per
million queries (column 9).
We take the Typical Server conﬁguration as our baseline.
Notice that the memory design common in consumer PCs,
which uses no error detection or correction in memory,
is able to reduce memory cost by 11.1% (reducing server
hardware cost by 3.3%). This comes at
the penalty of
19 crashes per server per month (reducing single server
availability to 99.55%). When the server is operational, it
generates 33 incorrect results per million queries.
In contrast, the Detect&Recover design, which leverages a
heterogeneous-reliability memory system, is able to reduce
memory cost by 9.7% compared to the baseline (leading to
a server hardware cost reduction of 2.9%), while causing
only 3 crashes per server per month (leading to a single
server availability of 99.93%, above the target availability of
99.90%). In addition, during operation, the Detect&Recover
design generates only 9 incorrect results per million queries.
Using less-tested DRAM devices has the potential to reduce
memory costs substantially—in the range of 16.4% to 37.8%
(leading to server hardware cost reduction in the range of
4.9% to 11.3%). Unfortunately, this reduction in cost comes
with a similarly substantial reduction in server availability
and reliability, with 96 crashes per server per month, a single
server availability of only 97.78%, and 163 incorrect queries
per million.
The Detect&Recover/L technique uses a heterogeneous-
reliability memory system on top of less-tested DRAM to
achieve both memory cost savings and high single server
availability/reliability. While using less-tested DRAM lowers
the cost of the memory by 3.1% to 27.9% (reducing cost of
server hardware by 0.9% to 8.4%), using hardware/software
reliability techniques tailored to the needs of each partic-
ular memory region reduces crashes to only 4 per server
per month (meeting the target single server availability of
99.90%), with only 12 incorrect queries per million. We
therefore conclude that heterogeneous-reliability memory
system designs can enable systems to achieve both high cost
savings and high single server availability/reliability at the
same time.
While we performed our analysis on the WebSearch appli-
cation, we believe that other data-intensive applications may
beneﬁt from the heterogeneous-reliability memory system
designs we have proposed. To illustrate this fact, Figure 8
shows for each data-intensive application we examined in
Section V, the maximum number of tolerable errors re-
quired to still achieve a particular amount of reliability,
• Detect&Recover/L: This
system evaluates
the De-
tect&Recover design with less-tested memory. ECC is
used in the private region and Par+R in the heap to