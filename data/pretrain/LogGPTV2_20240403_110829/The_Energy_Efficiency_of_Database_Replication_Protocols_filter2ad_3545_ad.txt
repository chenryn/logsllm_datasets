)
s
m
(
y
c
n
e
t
a
L
)
W
(
r
e
w
o
P
l
a
t
o
T
0
0
Std-alone – – PBR
∗
hyb –+–
200
400
100
Committed transactions per second
300
(a)
Std-alone – – PBR
∗
hyb –+–
200
100
400
Committed transactions per second
300
)
W
/
S
P
T
(
y
c
n
e
i
c
ﬁ
f
E
y
g
r
e
n
E
)
%
(
U
P
C
l
a
t
o
T
500
500
6
4.5
3
1.5
0
0
Std-alone – – PBR
∗
hyb –+–
200
400
100
Committed transactions per second
300
500
(b)
hyb-bckup. –•–
∗
∗
hyb-prim. –+– PBR
Std-alone – – PBR
500
400
300
200
100
0
0
200
100
400
Committed transactions per second
300
500
(c)
(d)
Figure 5. The performance and costs of PBR
∗
with low power replicas under the TPC-C benchmark.
to reduce the transaction latency (transaction updates are
still forwarded to backups). In this setup, we observed
that PBR∗
hyb reached the same throughput as a stand-alone
server.
Thanks to the low-power requirements of backups, until
320 TPS, the hybrid protocol adds little power overhead
compared to a single server (Fig. 5(c)). Raspberry PIs
never consume more than 6 Watts and the replication
protocol adds little energy overhead at the primary. This
can be seen in Fig. 5(d), where the primary only uses
marginally more CPU cycles than the stand-alone server.
As a consequence, PBR∗
hyb offers an energy efﬁciency
close to that of a stand-alone server (see Fig. 5(b)). At
their maximum throughput, PBR∗
hyb reaches 79% of the
efﬁciency of a single server. This is considerably more
than attained by any considered protocol thus far.
Similarly to PBR∗, PBR∗
hyb needs to periodically trun-
cate the log at the backups. In our setup, the primary takes
18.4 seconds to record a database snapshot of 1.1 GB on
disk (9 warehouses), and 154.7 seconds to send a third
of the snapshot to each backup, for a total energy of 9.3
KJoules. This constitutes a reduction of about 0.16% in
energy efﬁciency if the average throughput is 250 TPS and
the database snapshot is transferred to the backups once
per day. We also measured the overhead of transferring
the database snapshot from the backups to the primary in
case of a primary failure. It takes the new primary 107.1
seconds to receive the snapshot. The energy consumed by
the primary and the backups during this operations is 6.87
KJoules.
VII. RELATED WORK
A large body of work ranging from hardware [11]
to databases [12] has rethought the design of computer
systems to improve energy efﬁciency.
Energy-Efﬁcient Storage: An analysis of the energy
consumed by database servers is provided in [13]. This
study investigates how energy efﬁciency is affected by
the hardware, the database, and the algorithms used for
sorting, scanning rows, and joining tables. Their analysis
reveals that, in general, the best performing conﬁguration
is the most energy-efﬁcient one.
Various extensions of the RAID storage system have
been proposed as an answer to the ever-growing energy
needs of data centers. Peraid [14] considers a system with
a primary and multiple secondary replicas, where each
machine hosts a RAID system. Peraid turns off secondary
replicas and employs software RAID at the primary to
buffer the parity bits. This system tolerates disk failures
but not the failure of the primary. ECS2 [15] addresses
this shortcoming by employing (k+r, r) erasure codes, by
placing the r parity nodes into low-power modes, and by
buffering parity bits in memory at the other k machines.
ECS2 attempts to maximize standby times and minimize
416416416
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:33:14 UTC from IEEE Xplore.  Restrictions apply. 
power transitions by taking I/O workloads into account.
Ursa [16] is a storage system that avoids hotspots by
migrating data in a topology-aware fashion to minimize
reconﬁguration time and the associated network usage.
The same reconﬁguration technique allows to migrate data
off underutilized servers to power them down. Kairos [12]
leverages performance models to optimally place database
servers such that each machine is fully utilized and
simultaneously provides enough power to each database
it
is hosting, a technique often referred to as server
consolidation.
FAWN [17] is an energy-efﬁcient key-value store made
up of low-power nodes. Similar to our work, the back-end
nodes maintain data logs. FAWN leverages partitioning for
high performance and uses chain replication [18] for fault
tolerance. Their front-end node is not replicated, however.
FAWN achieves signiﬁcantly better energy efﬁciency for
its queries, but our work uses full replication and sup-
ports transactional workloads, using COTS database that
has signiﬁcantly higher CPU requirements than what are
needed for a key-value store (see Figures 2(e-f)). Our
research focuses on the impact of replication on energy
efﬁciency.
the data center level
Energy-Aware Clouds: Energy efﬁciency is ad-
dressed at
in [19]. The authors
argue for a clever placement of data to render the power
cycle unit larger. For instance, to allow an entire rack
of servers to be turned off, the approach advocates the
replication of data across racks. In this situation, reads
are served by the replicas in the powered-on rack; to
increase standby times of the powered-down rack, writes
are temporarily replicated to other servers. Maximizing
efﬁciency is then achieved by collocating computation and
data. Tacoma [20] not only relies on server consolidation
to improve efﬁciency but it also takes into account heat
dissipated by servers. The authors argue that maximizing
server utilization is not always a good idea as it may
induce high cooling costs. Tacoma attempts to spread the
load across servers to mitigate this effect.
Some systems have proposed to exploit
the possi-
bility of shifting work in time or space according to
the availability of green energies to reduce the carbon
footprint [21], [22]. For instance, batch jobs with loose
deadlines can be delayed until sufﬁcient green power is
produced. Similarly, virtual machines can be shipped to
data centers that have access to more green energy.
In the context of software-based replication, little work
has been done that specially tackles energy efﬁciency.
Server consolidation is only a partial answer to energy
efﬁciency: if the replication uses resources inefﬁciently
energy will be wasted.
Replication Protocols: Many protocols have been
proposed that implement one of the presented replication
families. Some implementations of SMR are optimized
for speciﬁc hardware such as modern switched intercon-
nects [23], [24]. Eve [25] provides a scheme to employ
the full potential of multi-core servers: a mixer batches
operations that are unlikely to conﬂict. Replicas execute
operations in a batch in parallel and exchange hashes of
the modiﬁed state to check that they are identical. DURP U
also enables operations (more speciﬁcally transactions) to
be executed at replicas in parallel but reduces the amount
of work per transaction. With DURP U , each transaction is
executed at a single site and only its update statements are
forwarded to the other replicas. In contrast, Eve executes
all operations in their entirety at all replicas.
Tashkent+ reﬁnes DUR by load-balancing operations
on replicas to improve resource utilization, and routes
conﬂicting operations to the same replica to lower the
rollback rate [26]. The approach in [27] throttles con-
ﬂicting operations to lower the abort rate of DUR when
the load increases. In [9],
the abort rate is lowered
by re-ordering transactions at certiﬁcation time. MorphR
adapts the replication protocol to the workload using a
machine learning approach that takes as input several key
parameters of the workload to decide which protocol to
deploy [28]. At any point in time either PBR, DUR, or
2PC [29] can be selected.3 MorphR does not attempt to
reduce the energy required to handle each operation. We
believe that some of these techniques could be integrated
in our protocols to further improve their efﬁciency.
VIII. CONCLUSION
In this paper, we attempted to reconcile replication
with energy efﬁciency, a growing concern with the in-
creasing electrical consumption of data centers worldwide.
We reviewed commonly-used replication protocols and
measured their energy efﬁciency. We observed that the
most efﬁcient protocol, DUR, reaches an efﬁciency that
is slightly less than 60% of the maximum efﬁciency of
a stand-alone server. To address this waste of energy we
proposed algorithmic modiﬁcations to the protocols that
either improve performance or lower energy consumption.
hyb, a protocol derived
from PBR that implements a log on the backups. PBR∗
hyb
relies on a multi-core primary and low-power backups
to provide maximum efﬁciency. We showed that such a
protocol can achieve 79% of the maximum efﬁciency of
a non-replicated server on the TPC-C benchmark.
Of particular interest is PBR∗
Acknowledgments
The authors are supported in part by AFOSR grant
FA2386-12-1-3008, by NSF grants 1040689, 1047540,
and CCF-0424422 (TRUST), by DARPA grants FA8750-
10-2-0238 and FA8750-11-2-0256, by DOE ARPA-e grant
DE-AR0000230, by MDCN/iAd grant 54083, and by
3We purposely decided to not consider 2PC-based protocols to repli-
cate data due to its inability to handle contention, as argued in [29].
417417417
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:33:14 UTC from IEEE Xplore.  Restrictions apply. 
[18] R. van Renesse and F. B. Schneider, “Chain replication for
supporting high throughput and availability,” in Proceed-
ings of the 6th Symposium on Operating Systems Design
& Implementation, ser. OSDI’04. Berkeley, CA, USA:
USENIX Association, 2004, pp. 7–7.
[19] L. Ganesh, H. Weatherspoon, T. Marian, and K. Birman,
“Integrated approach to data center power management,”
Computers, IEEE Transactions on, vol. 62, no. 6, pp. 1086–
1096, 2013.
[20] Z. Abbasi, G. Varsamopoulos, and S. K. S. Gupta,
“Tacoma: Server and workload management in internet
data centers considering cooling-computing power trade-
off and energy proportionality,” ACM Trans. Archit. Code
Optim., vol. 9, no. 2, pp. 11:1–11:37, Jun. 2012.
[21] A. Krioukov, S. Alspaugh, P. Mohan, S. Dawson-Haggerty,
D. E. Culler, and R. H. Katz, “Design and evaluation of
an energy agile computing cluster,” EECS Department,
University of California, Berkeley, Tech. Rep. UCB/EECS-
2012-13, Jan 2012.
[22] S. Akoush, R. Sohan, A. Rice, A. W. Moore, and A. Hop-
per, “Free lunch: exploiting renewable energy for comput-
ing,” in Proceedings of the 13th USENIX conference on
Hot topics in operating systems, ser. HotOS’11. Berkeley,
CA, USA: USENIX Association, 2011, pp. 17–17.
[23] R. Guerraoui, R. Levy, B. Pochon, and V. Qu´ema,
“Throughput optimal total order broadcast for cluster envi-
ronments,” ACM Trans. Comput. Syst., vol. 28, no. 2, pp.
5:1–5:32, Jul. 2010.
[24] P. J. Marandi, M. Primi, and F. Pedone, “Multi-Ring
Paxos,” in 42nd Annual IEEE/IFIP International Confer-
ence on Dependable Systems and Networks (DSN), 2012,
pp. 1–12.
[25] M. Kapritsos, Y. Wang, V. Quema, A. Clement, L. Alvisi,
and M. Dahlin, “All about Eve: execute-verify replication
for multi-core servers,” in Proceedings of the 10th USENIX
conference on Operating Systems Design and Implementa-
tion, ser. OSDI’12. USENIX Association, pp. 237–250.
[26] S. Elnikety, S. Dropsho, and W. Zwaenepoel, “Tashkent+:
memory-aware load balancing and update ﬁltering in repli-
cated databases,” SIGOPS Oper. Syst. Rev., vol. 41, no. 3,
pp. 399–412, Mar. 2007.
[27] A. Nunes, R. Oliveira, and J. Pereira, “AJITTS: Adaptive
just-in-time transaction scheduling,” in Distributed Appli-
cations and Interoperable Systems, ser. Lecture Notes in
Computer Science. Springer Berlin Heidelberg, 2013, vol.
7891, pp. 57–70.
[28] M. Couceiro, P. Ruivo, P. Romano, and L. Rodrigues,
“Chasing the optimum in replicated in-memory transac-
tional platforms via protocol adaptation,” in 43rd Annual
IEEE/IFIP International Conference on Dependable Sys-
tems and Networks (DSN). Los Alamitos, CA, USA: IEEE
Computer Society, 2013, pp. 1–12.
[29] J. Gray, P. Helland, P. O’Neil, and D. Shasha, “The dangers
of replication and a solution,” in Proc. of the International
Conference on Management of Data (SIGMOD). ACM,
Jun. 1996, pp. 173–182.
grants from Microsoft Corporation, Facebook Inc., and
Amazon.com.
REFERENCES
[1] L. A. Barroso and U. H¨olzle, “The case for energy-
proportional computing,” IEEE Computer, vol. 40, no. 12,
pp. 33–37, 2007.
[2] C. Papadimitrou, “The serializability of concurrent updates
in databases,” J. ACM, vol. 26, no. 4, pp. 631–653, Oct.
1979.
[3] F. B. Schneider, “Implementing fault-tolerant services us-
ing the state machine approach: A tutorial,” ACM Comput-
ing Surveys, vol. 22, no. 4, pp. 299–319, Dec. 1990.
[4] D. Agrawal, G. Alonso, A. El Abbadi, and I. Stanoi, “Ex-
ploiting atomic broadcast in replicated databases (extended
abstract),” in Proceedings of the 3rd International Euro-Par
Conference on Parallel Processing, ser. Euro-Par ’97, pp.
496–503.
[5] H. Garcia-Molina and K. Salem, “Main memory database
systems: An overview,” IEEE Trans. on Knowl. and Data
Eng., vol. 4, no. 6, pp. 509–516, Dec. 1992.
[6] A. Thomson, T. Diamond, S.-C. Weng, K. Ren, P. Shao,
and D. J. Abadi, “Calvin: fast distributed transactions for
partitioned database systems,” in Proceedings of the 2012
ACM SIGMOD International Conference on Management
of Data, ser. SIGMOD ’12, 2012, pp. 1–12.
[7] B. Oki and B. Liskov, “Viewstamped Replication: A
general primary-copy method to support highly-available
distributed systems,” in PODC’88, pp. 8–17.
[8] F. Pedone and S. Frølund, “Pronto: High availability for
standard off-the-shelf databases,” J. Parallel Distrib. Com-
put., vol. 68, no. 2, pp. 150–164, 2008.
[9] F. Pedone, R. Guerraoui, and A. Schiper, “The database
state machine approach,” Distrib. Parallel Databases,
vol. 14, no. 1, pp. 71–98, Jul. 2003.
[10] “The transaction processing performance council, Bench-
mark C—http://www.tpc.org/tpcc/.”
[11] M. Kai, L. Xue, C. Wei, Z. Chi, and W. Xiaorui,
“GreenGPU: A holistic approach to energy efﬁciency in
gpu-cpu heterogeneous architectures,” in Proceedings of
the 41st International Conference on Parallel Processing
(ICPP’12), 2012, pp. 48–57.
[12] C. Curino, E. P. Jones, S. Madden, and H. Balakrishnan,
“Workload-aware database monitoring and consolidation,”
in Proceedings of the 2011 ACM SIGMOD International
Conference on Management of data, ser. SIGMOD ’11.
ACM, pp. 313–324.
[13] D. Tsirogiannis, S. Harizopoulos, and M. A. Shah, “An-
alyzing the energy efﬁciency of a database server,” in
Proceedings of
the 2010 ACM SIGMOD International
Conference on Management of data, ser. SIGMOD ’10.
New York, NY, USA: ACM, 2010, pp. 231–242.
[14] J. Wan, C. Yin, J. Wang, and C. Xie, “A new high-
performance, energy-efﬁcient replication storage system
with reliability guarantee,” in Proceedings of the 28th IEEE
Symposium on Mass Storage Systems and Technologies,
ser. MSST’12, pp. 1–6.
[15] J. Huang, F. Zhang, X. Qin, and C. Xie, “Exploiting
redundancies and deferred writes to conserve energy in
erasure-coded storage clusters,” Trans. Storage, vol. 9,
no. 2, pp. 4:1–4:29, Jul. 2013.
[16] G.-W. You, S.-W. Hwang, and N. Jain, “Ursa: Scalable load
and power management in cloud storage systems,” Trans.
Storage, vol. 9, no. 1, pp. 1:1–1:29, Mar. 2013.
[17] D. G. Andersen, J. Franklin, M. Kaminsky, A. Phanishayee,
L. Tan, and V. Vasudevan, “FAWN: A fast array of wimpy
nodes,” Commun. ACM, vol. 54, no. 7, pp. 101–109, Jul.
2011.
418418418
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:33:14 UTC from IEEE Xplore.  Restrictions apply.