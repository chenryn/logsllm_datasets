how such “sparsity regularized” algorithms can be constructed.
2.3 Singular Value Decomposition
A basic tool for creating low-rank matrix approximations is the
Singular Value Decomposition (SVD). Simply stated, any n × m
real matrix X can be decomposed into three matrices such that
X = U ΣV T ,
(5)
where V T is the transpose of V , and U is a n × n unitary matrix
(i.e., U T U = U U T = I), and V is a m × m unitary matrix (i.e.,
V T V = V V T = I), and Σ is a n × m diagonal matrix contain-
ing the singular values σi of X. Typically the singular values are
arranged so that σi ≥ σi+1. The rank of a matrix is the number of
linearly independent rows or columns, which equals the number of
non-zero singular values.
To understand the SVD’s use in matrix approximations, consider
the following interpretation of the SVD. The matrix Σ is diagonal,
so the SVD of a matrix X can be rewritten as:
X = U ΣV T =
min(n,m)
min(n,m)
σiuivT
i =
Xi=1
Xi=1
σiAi,
(6)
where ui and vi are the ith columns of U and V respectively, and
the matrices Ai are (by construction) rank-1. We can then create
a rank-r approximation ˜X from the SVD by keeping only the r
largest singular values in the summation and dropping the others:
˜X =
r
Xi=1
σiAi.
(7)
The above ˜X is known to be the best rank-r approximation with
respect to the Frobenius norm || · ||F of the approximation errors,
= qPi,j Z(i, j)2 for any matrix Z. That is, trun-
where ||Z||F
cation of the SVD provides the natural solution to:
△
minimize
subject to
||X − ˜X||F ,
rank( ˜X) ≤ r.
(8)
In Internet measurement, the SVD has most commonly appeared
in the form of the Principal Components Analysis (PCA), which
has been used, for instance in anomaly detection [13, 14]. PCA is
directly related to SVD by the fact that the columns of U form the
principal axes of the PCA coordinate transform.
Sparsity Regularized SVD: Many matrix interpolation techniques
try to create a SVD-like factorization of a matrix X, though it is
common to write this in the equivalent form
X = U ΣV T = LRT ,
(9)
where L = U Σ1/2 and R = V Σ1/2, and so we will subsequently
use this form for consistency.
Now SVD by itself is not an interpolation algorithm. Typical
algorithms for calculating the SVD assume that X is completely
known. Instead we look for a factorization that satisﬁes the mea-
surement equations, i.e., A(LRT ) = B. Previous studies have
269suggested that typical TMs inhabit a relatively low-dimensional
subspace [13, 14], so an intuitive approach for ﬁnding such a fac-
torization is to solve the following rank minimization problem:
rank(LRT ),
minimize
subject to A(LRT ) = B.
(10)
Rank minimization has a non-convex objective and is thus difﬁ-
cult to solve. A key insight from the matrix compressive-sensing
literature [5, 19, 20] is that under certain conditions, we can solve
a simpler problem instead and obtain equivalent results. Specif-
ically, when a certain technical condition (the restricted isometry
property [19]) holds on A(·), then a heuristic — minimizing the
nuclear norm — can perform rank minimization exactly for a ma-
trix of low enough rank. Further, if the rank of X is less than the
rank of LRT then (10) is equivalent to
||L||2
F + ||R||2
minimize
F ,
subject to A(LRT ) = B.
(11)
In TM interpolation, looking for a low-rank solution that strictly
satisﬁes the measurement equations is likely to fail, because (i) the
real TM X is often only approximately low-rank, and (ii) the mea-
surements often contain errors. So instead we solve the following
minimize ||A(LRT ) − B||2
F + λ“||L||2
F + ||R||2
F” .
(12)
This solution regularizes towards the low-rank approximation but
does not strictly enforce the measurement equations. The regular-
ization parameter λ allows a tunable tradeoff between a precise ﬁt
to the measured data and the goal of achieving low rank.
We derive L and R from (12) using an alternating least squares
procedure. We initialize L and R randomly. We then solve the
above optimization taking one of L and R to be ﬁxed and the other
to be the optimization variable (which is a standard linear least
squares problem). We then swap their roles, and continue alternat-
ing towards a solution till convergence. Our implementation of the
alternating least squares procedure in Matlab further utilizes sparse
matrix operations to minimize memory requirement and maximize
speed (details are omitted due to space restriction, but we will sup-
ply Matlab code on request). The above approach will be referred
to as Sparsity Regularized SVD (SRSVD) interpolation.
2.4 Other Interpolation Algorithms
There are a number of approaches that have been proposed in
the recent literature for matrix interpolation besides SVD. These
algorithms can be classiﬁed as either low-rank approximation al-
gorithms or local interpolation algorithms, depending on whether
they exploit the global low-rank structure or the local structure and
redundancy. We describe them here for completeness and for com-
parison with our approach detailed in Section 3.
2.4.1 Low-Rank Approximation Algorithms
Baseline Approximation: Many techniques (for instance PCA)
implicitly assume that the data has zero mean. So our ﬁrst step for
dealing with network matrices might be to “center” them. How-
ever, centering the matrices where we do not have all the data also
requires interpolation. Our baseline approximation algorithm im-
plicitly constructs such an interpolation matrix Xbase to compute
row and column means of the matrix. For instance, if we knew
all elements of the input X, then the row and column sums of
X − Xbase would all equal zero. Apart from its use in zeroing
the mean, it also forms an interpolation in its own right, and so we
will compare its performance below.
To compute Xbase, we use the variables described in Table 1. In
matrix form, we can represent Xbase as a rank-2 approximation
to X: Xbase = ¯X + Xrow 1T + 1X T
col, where 1 is a column
variable
description
¯X an estimate of the mean of X over all i and j.
Xrow
Xcol
Xbase
a vector of length m such that
a vector of length n such that
Xrow(i) = an estimate of Pj(X(i, j) − ¯X)/n.
Xcol(j) = an estimate of Pi(X(i, j) − ¯X)/m.
the baseline estimate for X given by
Xbase(i, j) = ¯X + Xrow(i) + Xcol(j).
Table 1: Outputs of baseline estimation.
vector consisting of all ones. We use the regularized least-squares
algorithm from [4] to compute ¯X, Xrow, Xcol from input A(·) and
B. That is, we solve the following
minimize
||A( ¯X + Xrow 1T + 1X T
+ λ` ¯X 2 + ||Xrow||2
col) − B||2
F + ||Xcol||2
F´ ,
F
(13)
where λ is a regularization parameter. The ﬁrst term in this formu-
lation minimizes the Frobenius norm of the difference A(Xbase)−
B, and the second regularization term helps avoid overﬁtting.
SRSVD-base: Techniques like PCA implicitly assume that the
data has zero mean, but in TM interpolation we do not know the
true mean. Instead we use Xbase as an estimate. It is not obvious
whether such centering is necessary or desirable in interpolation,
so we include results for both SRSVD applied to X and SRSVD
applied to (X − Xbase). We refer to the latter as SRSVD-base.
Nonnegative Matrix Factorization: Nonnegative Matrix Fac-
torization (NMF) [15, 16] tries to ﬁnd nonnegative factor matri-
ces L and R that minimize the Frobenius norm2 of the difference
A(LRT ) − B over the observations. The approach is very similar
to the SVD, except for the insistence on non-negative factor matri-
ces. We avoid overﬁtting by regularizing in the same manner that
we do for SVD, i.e., we solve (12) but with the additional constraint
of non-negativity. We implement the two most common algorithms
for NMF: multiplicative update [15] and alternating nonnegative
least squares. Both algorithms are designed for the case where the
matrix X is completely known. So we extend them to further cope
with measurement equations (2). The two algorithms give similar
interpolation performance, but multiplicative update is more efﬁ-
cient. So our results are based on this algorithm.
2.4.2 Local Interpolation Algorithms
K-Nearest Neighbors: We also test one completely different
approach: k-Nearest Neighbors (KNN). Simple nearest neighbors
directly uses the nearest neighbor of a missing value for interpo-
lation. KNN extends this by using a weighted average of the k
nearest-neighbors’ values. For TMs, it is more difﬁcult to apply
KNN because the rows are ordered arbitrarily (for instance based
on the names of routers). So the nearest elements in the matrix X
may have little correspondence. Hence we need to derive a good
distance metric between matrix elements.
We use the approach described in [4]. We can perform the algo-
rithm on either rows or columns of X, but let us start with rows.
If two rows are similar (i.e., two TM elements exhibit similar be-
havior), then it is natural to assume that one might provide a good
interpolant of the other. Hence, we base our distance metric on the
similarity between rows, i.e., the more similar two rows are, the
closer together we consider them. Following [4], we measure the
similarity by an approximation to the correlation coefﬁcient of the
two rows based on only those directly observed TM elements. To
2There is nothing intrinsically special about the Frobenius norm for
this approach. The Kullback-Leibler divergence [15] has also been
suggested but our experiments found that the performance of this
approach was much worse, and it is not presented here.
270form this coefﬁcient, we would ideally ﬁrst subtract the mean, but
as the mean is unknown we use our proxy Xbase. The weights used
in the k averaged neighbors are proportional to the similarities [4].
3. OUR SOLUTION: SPATIO-TEMPORAL
COMPRESSIVE SENSING
The KNN approach is intrinsically different from the other meth-
ods described above. It explicitly targets local structure in a TM,
whereas the low-rank methods look for global structure. This dif-
ference is one of the key motivations for developing a novel spatio-
temporal compressive sensing framework that seeks to capture both
global and local structure. It consists of two key components: (i)
SPARSITY REGULARIZED MATRIX FACTORIZATION (SRMF) for
incorporating global spatio-temporal properties, and (ii) a mecha-
nism for incorporating local interpolation.
3.1 Sparsity Regularized Matrix Factorization
The SRSVD approach starts with (12) to ﬁnd global low-rank
structure in the TM. On the other hand, we may a priori know
that the matrix has additional spatio-temporal structure, e.g., TM
rows or columns close to each other (in some sense) are often close
in value. We seek to exploit this insight in the new technique we
propose here. We propose to solve the following
minimize
||A(LRT ) − B||2
F + λ`||L||2
F + ||R||2
F´
F + ||(LRT )T T||2
F ,
+ ||S(LRT )||2
(14)
where S and T are the spatial and temporal constraint matrices,
respectively. Matrices S and T express our knowledge about the
spatio-temporal structure of the TM (e.g., temporally nearby TM
elements have similar values). We solve the above optimization
problem again using alternating least squares. We call the result-
ing algorithm Sparsity Regularized Matrix Factorization (SRMF).
It has the advantages of SRSVD, but is more general, allowing us
to express other objectives in our TM approximation/interpolation
algorithm through different choices of S and T .
Below we discuss how to choose S and T . To better illustrate the
idea and beneﬁt of SRMF, we intentionally use relatively simple
choices of S and T . In our future work, we will develop techniques
to better tailor S and T to dataset characteristics and application
requirements. Both SRSVD and SRMF also require speciﬁcation
of the input rank of L and R. Our evaluation in Section 4 shows
that SRMF is not sensitive to the input rank parameter.
Choice of T : The temporal constraint matrix T captures the tem-
poral smoothness of the TM. A simple choice for the temporal
constraint matrix is T = T oeplitz(0, 1, −1), which denotes the
Toeplitz matrix with central diagonal given by ones, and the ﬁrst
upper diagonal given by negative ones, i.e.,
T =
2
666664
1 −1
0
0 . . .
. . .
. . .
. . .
1 −1
1
0
. . .
. . .
0
...
3
777775
(15)
This temporal constraint matrix intuitively expresses the fact that
TMs at adjacent points in time are often similar. For instance XT T
is just the matrix of differences between temporally adjacent ele-
ments of X. By minimizing ||(LRT )T T||2
F we seek an approxi-
mation that also has the property of having similar temporally ad-
jacent values. We use this simple choice for anomaly detection in
Section 5.3 to make comparisons with other anomaly detection al-
gorithms easier. A more sophisticated choice taking into account
domain knowledge (say knowledge of the periodicity in trafﬁc data)
might result in some improvements. We give such an example for
trafﬁc prediction in Section 5.2.
In general, it is not difﬁcult to
develop such temporal models of TMs.
Choice of S: The spatial constraint matrix S can be used to ex-
press which rows of a TM are close to each other, but due to the
arbitrary ordering of rows in the TM, a simple matrix of the above
form is not appropriate. We ﬁnd S by ﬁrst obtaining an initial TM
estimate ˚X using a simple interpolation algorithm, and then choos-
ing S based on the similarity between rows of ˚X (which approxi-
mates the similarity between rows of X).
1. Computing ˚X. In our current implementation, we take ˚X =
Xbase.∗(1−M )+D.∗M, where M is deﬁned in (4) and spec-
iﬁes which TM elements are directly measured, and D contains
the direct measurements. That is, we use direct measurements
where available, and interpolate using Xbase at other points.
2. Choosing S based on ˚X. There are many possible methods for
choosing S based on ˚X. For example, one general method is to
(i) construct a weighted graph G, where each node represents
a row of ˚X and each edge weight represents certain similar-
ity measure between two rows of ˚X, and (ii) set S to be the
normalized Laplacian matrix [7] of graph G, which acts as a
differencing operator on G and induces sparsity by eliminating
redundancy between similar nodes of G (i.e., rows of ˚X).
We have experimented with several of these methods. The fol-
lowing method for choosing S based on KNN and linear regres-
sion consistently yields good performance in our tests, which
we will use in our evaluation. For each row i of ˚X, we ﬁnd the
K most similar rows jk 6= i (k = 1, . . . , K). We perform lin-
ear regression to ﬁnd a set of weights w(k) such that the linear