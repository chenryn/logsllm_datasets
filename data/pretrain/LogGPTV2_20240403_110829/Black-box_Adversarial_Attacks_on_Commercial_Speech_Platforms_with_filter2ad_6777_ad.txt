Google STT
Microsoft ASR 10/10
10/10
Alibaba SSR
10/10
Tencent SSR
10/10
10/10
iFlytek
Average
SNR
6.80
8.71
9.32
11.03
6.78
8.53
Opt-Attack
SRoA SNR
10/10
5.52
6.45
10/10
5.70
10/10
6.43
10/10
6.73
10/10
10/10
6.17
Evolutionary
Attack
SRoA
10/10
10/10
10/10
10/10
10/10
10/10
SNR
6.11
6.60
7.20
9.14
6.50
7.11
HSJA
DEA
SRoA SNR
10/10
5.38
4.59
10/10
8.11
10/10
8.49
10/10
5.08
10/10
10/10
6.33
SRoA SNR
10/10
6.52
7.35
10/10
5.44
10/10
6.06
10/10
6.47
10/10
10/10
6.37
Devil’s
Whisper
SRoA SNR
5/10
9.13
9.91
8/10
8.21
3/10
9.64
4/10
9.27
7/10
5.4/10
9.23
Occam
SRoA
10/10
10/10
10/10
10/10
10/10
10/10
SNR
15.33
10.74
17.84
16.70
11.22
14.37
Note that, (i) Devil’s Whisper originally utilized confidence scores to filter the synthetic audio data. Since the confidence scores are not available from Alibaba, iFlytek, and Tencent,
we have omitted the score-related processing step in reproduced experiments of Devil’s Whisper on these three speech APIs. Targeting Google TTS and Microsoft ASR (which return
confidence scores), we follow the original version of Devil’s Whisper when conducting the experiments on Google and Microsoft. (ii) In reproduced experiments of Devil’s Whisper,
we have used 14,569 (filtered) clips, containing different commands as those adopted in Devils Whisper paper. Moreover, we have used 3,000 extra clips for certain commands with
unsatisfactory clip quality. The total length of the TTS audio corpus and the supplemental corpus of the Mini LibriSpeech dataset are 6.8 hours and 7.3 hours, respectively.
Table 5: Experimental results on targeted and untargeted attacks on cloud speaker recognition APIs.
Cloud service
Microsoft SI
Microsoft SV
Jingdong SV
Microsoft SI
(untarget)
Average
Boundary
Attack
SRoA SNR
7.13
10/10
8.48
10/10
6.26
10/10
10/10
9.61
7.87
10/10
Opt-Attack
SRoA SNR
7.01
10/10
5.85
10/10
6.42
10/10
10/10
9.95
7.31
10/10
Evolutionary
Attack
SRoA
10/10
10/10
10/10
10/10
10/10
SNR
7.25
9.93
5.97
9.04
8.05
HSJA
DEA
SRoA SNR
4.87
10/10
4.27
10/10
5.89
10/10
10/10
9.43
6.12
10/10
SRoA SNR
2.84
10/10
3.05
10/10
5.67
10/10
10/10
6.53
4.52
10/10
FakeBob
SNR
N/A
N/A
35.30
40.21
37.76
SRoA
0/200
0/200
2/200
2/200
1/200
Occam
SRoA
10/10
10/10
10/10
10/10
10/10
SNR
14.31
13.25
13.78
14.92
14.07
Note that, (i) we only evaluate untargeted attacks against Microsoft Azure SI. For speaker verification systems, the goal of untargeted attacks is the same as that of targeted attacks.
(ii) N/A denotes “not available”. Since there is no effective AE against Microsoft SI and SV in FakeBob, the SNR is not available.
Table 6: Experimental results of the physical attacks.
Devices
NI-Occam
Super-
position
attack
Siri
SRoA 6/10
SNR
9.81
(dB)
SRoA 5/10
SNR
7.00
(dB)
iFlytek Cortana Google
6/10
9.09
1/10
7.00
6/10
9.58
1/10
7.00
4/10
10.36
1/10
7.00
Echo
4/10
9.42
2/10
7.00
the upgrading of Microsoft speaker recognition systems makes the
transferability of the AEs in FakeBob no longer effective. As for SNR,
the AEs from Occam can achieve pretty good SNRs, outperforming
other decision-based attacks. Although effective AEs from FakeBob
can achieve SNR as high as 35.3dB, the SRoA is too low to effectively
mislead the SR services and thus far from being practical.
5.3 Evaluation of NI-Occam against Voice
Control Devices
Effectiveness. We test the effectiveness of NI-Occam on 5 voice
control devices, and the results are given in Table 610. If the audio AE
can be correctly recognized by the devices as the target command
within 3 attempts (play the AEs within three times), we consider this
AE successful. Overall, NI-Occam achieves an average SRoA of 52%
and SNR of 9.65dB. Note that NI-Occam is a non-interactive attack
that requires no access to the target devices, which is very practical
since some devices like Apple Siri do not provide a programmable
API. For example, Devil’s Whisper [32] fails to generate effective
10Detailed results on individual commands can be found in Appendix F (see Table 12).
AEs when confronted with devices that do not return confidence
scores. We also find that NI-Occam performs well on Apple Siri,
while Devil’s Whisper failed in attacking updated versions of Siri.
This indicates that NI-Occam is more effective in leveraging the
useful transferability of the AEs and can generate successful AEs
with minimal information from the target devices. We also evaluate
a simple non-interactive attack as the baseline, i.e., the superposition
attack. Since the AEs from the superposition attack are constructed
by superimposing two audios, the SNR is adjustable. We set the
SNRs as 7.00dB, smaller than those of NI-Occam. A smaller SNR
means the audio of the target command is more obvious in the
audio AE, making it easier for the devices to recognize. However,
the superposition attack can only achieve an average SRoA of 20%
with a 7.00dB SNR. More importantly, the superposition attack is
easily perceived by human, which we will discuss shortly.
We also evaluate the impact of the number of attempts on SRoA
with a larger group of target phrases. We observe that increasing
the query attempts can help increase SRoA to 70%, and our NI-
Occam can also perform well on large sets of 60 commands with an
SRoA of 71.7%. Detailed results are given in Appendixes F and G.
Human Perception. Although SNR describes the proportion of
noise, it cannot fully reflect the imperceptibility of the audio. For
example, if the noise can well fit the background, even though the
signal has a low SNR, the users cannot perceive it. On the contrary,
if the noises all appear in a small piece of audio, although the overall
SNR is high, the users may readily perceive the command.
To evaluate the performance of NI-Occam on human perception,
we surveyed 37 volunteers aged from 19 to 24 (who are sensitive
to sound), including 21 males and 16 females. In the experiment,
Table 7: Evaluation results on human perception of the non-interactive physical attacks.
Devices
Apple Siri
iFlytek
Mircrosoft Cortana
Google Assistant
Amazon Echo
Method
NI-Occam
Superposition attack
NI-Occam
Superposition attack
NI-Occam
Superposition attack
NI-Occam
Superposition attack
NI-Occam
Superposition attack
Normal (%) Noise (%) Talking (%) Once-recognize (%) Twice-recognize (%)
12.4
0.0
9.6
0.0
14.2
0.0
15.0
0.0
2.7
0.0
61.7
0.0
58.3
0.0
56.7
0.0
55.7
0.0
68.2
0.0
25.9
100
32.1
100
29.1
100
29.4
100
29.1
100
4.1
88.1
5.1
86.5
6.1
83.8
4.8
83.8
12.2
86.5
5.9
91.9
6.9
94.6
8.3
86.5
8.2
86.5
14.2
94.6
Note that, (i) “Normal” means that the volunteer regards the audio as a normal audio. (ii) “Noise” means that the volunteer can feel some noises. (iii) “Talking” means that the
volunteer can hear talking in the audio. If the volunteer thinks there is talking in the audio, he/she is then asked to recognize the content of the talking. (iv) The audio will be labeled
as “once-recognize” or “twice-recognize” if the volunteer recognizes over half of the content after listening to the audio once or twice, respectively.
we first show some examples of “noise”, “normal”, “talking”, and
“recognized”, and then ask the volunteers to listen to the audio AEs
and tell their views about them. Specifically, the audio AEs are
generated from NI-Occam and the superposition attack and can
successfully attack the devices. Each volunteer ranks 6, 6, 6, 4, and
4 successful AEs from NI-Occam on Apple Siri, iFlytek, Microsoft