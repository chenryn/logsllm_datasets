### Equal AS-PATH Length and Physical Path Differences

Current routing mechanisms do not distinguish between the actual physical differences in path lengths, even when the AS-PATH lengths are equal. This can lead to suboptimal route selection, particularly for anycast traffic, where packets might be routed to a distant server. In other words, the current route selection processes have a higher likelihood of making unsuitable choices when selecting paths to an anycasted Autonomous System (AS).

### Importance of Observations

This observation is crucial because it highlights that while the routing protocols used for unicast destinations work naturally for anycast destinations, the metrics used for routing decisions can result in poor choices regarding latency. One approach to address this issue could be to modify routing protocols to differentiate between anycast and unicast prefixes. However, a more practical solution is to strategically deploy anycast servers to account for inter-domain route selection.

### Hypothesis: Deployment Strategy

We hypothesize that deploying anycast servers such that all of them share the same upstream provider and are geographically spread across that provider can mitigate these issues. This approach was briefly mentioned in [4] but was not explored in detail. The hypothesis is based on two key observations:

1. **Intra-Domain Traffic Engineering**: An ISP that is an upstream provider for some of the anycast servers will route incoming anycast traffic to the closest server. This is because intra-domain traffic engineering is generally consistent with latency-sensitive routing [34]. For example, if ATT is the upstream provider for three out of five anycast servers, routers in ATT's network will receive routes of equal AS-PATH length and preference from each of these servers. Consequently, the BGP decision process will direct incoming anycast packets at any ATT Point of Presence (POP) to the server that is closest in terms of intra-domain metrics, which often correlates with the lowest latency.

2. **Decoupling Route Selection**: Such a deployment decouples route selection at the common upstream provider from the selection at ASs beyond it. The common upstream provider delivers incoming anycast packets to the closest server. Any ASs farther away only need to select among the possible routes to the upstream provider. ASs using "early-exit" (hot-potato) routing policies will route anycast packets to the closest POP of the common upstream provider. Conversely, ASs using "late-exit" (cold-potato) routing policies will honor the Multi-Exit Discriminator (MED) values of the upstream provider and route anycast packets to the most suitable POP for delivery to the closest deployed anycast server.

### Validation of Hypothesis

To validate this hypothesis, we conducted stretch factor measurements for various subsets of the internal deployment. Figure 6 shows the results, indicating that an anycast deployment with just the three servers at Berkeley, Pittsburgh, and Seattle (labeled as “All - Cam - Cor” in the figure) provides good proximity, with only 5% of clients experiencing a stretch factor of more than 30 milliseconds. This improvement is not simply due to reducing the number of servers, as a deployment with three servers at Seattle, Cornell, and Cambridge (labeled as “All - Pit - Berk”) yields poor proximity.

### Generalization and Future Work

This result can be generalized to anycast deployments with multiple upstream providers, provided each provider has a reasonable global geographic spread (e.g., tier-1 ISPs with a global network) and a sufficient number of servers to cover their geographical spread. In such a setup, whenever an upstream provider receives an anycast packet, there is a nearby anycast server to which the packet can be routed. ASs beyond these upstream providers would choose to route anycast packets to one of the upstream providers, and since all providers are well-covered by the anycast servers, this choice does not significantly impact the anycast path length.

### Failover Time

Inter-domain IP Anycast involves each site advertising the anycast prefix into BGP. When an anycast server fails, the re-routing of clients to other operational servers is tied to BGP convergence. Past studies have shown that failures at multi-homed end sites can lead to a convergence process lasting several minutes [23], which is undesirable for many proposed uses of IP Anycast. We measured the failover rate for the internal anycast deployment, representing the first attempt to study the failure-mode behavior of IP Anycast.

### Methodology

To determine the failover time, we configured the anycast servers in the internal deployment to act as authoritative name-servers for a domain under our control (internal.anycast.guha.cc) and to respond to TXT-type DNS queries with a location-specific string. This allows us to determine the specific anycast server each client is routed to. We induced failures at individual servers by tearing down their BGP peerings and sent TXT queries to the anycast address through the clients being routed to the failed server at a rate of once every five seconds for three minutes and once per minute for the next fifty-seven minutes, resulting in a total probe period of one hour.

### Results

Figure 8 shows the cumulative distribution function (CDF) for the failover and recovery times corresponding to each server in the deployment. The failover time characterizes the time after a failure during which clients using the failed server cannot utilize the anycast service. We also measured the recovery time, which is the time between the re-establishment of the BGP peering and the re-routing of clients back to the original server.

### Conclusion

The strategic deployment of anycast servers with a common upstream provider and geographical spread can significantly improve latency-based proximity for clients. Additionally, understanding and measuring the failover and recovery times is essential for ensuring the reliability and performance of anycast services.