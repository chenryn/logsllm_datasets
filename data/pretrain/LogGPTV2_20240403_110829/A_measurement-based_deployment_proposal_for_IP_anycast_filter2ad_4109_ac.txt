equal AS-PATH length and is not aware of the actual phys-
ical diﬀerence between the length of these paths. Conse-
quently, there is a good chance that Level3 may choose a
route that causes the anycast packets to be routed to a dis-
tant anycast server. In other words, current route selection
mechanisms have a much higher chance of making an un-
suitable choice when selecting paths to an anycasted AS.
Although negative, the importance of this observation
cannot be overemphasized. It brings to light that while the
routing protocols used to choose paths to unicast destina-
tions work naturally for anycast destinations too, the met-
rics used for routing decisions can lead to a poor choice of
anycast server in terms of latency. While changing routing
protocols to diﬀerentiate between anycast and unicast pre-
ﬁxes would be one possible approach to address this prob-
lem, a more practical approach would be to plan the deploy-
ment of the anycast servers so as to account for inter-domain
route selection.
We hypothesize that deploying the anycast servers such
that all of them have the same upstream provider and the
servers are spread across the provider is one such deploy-
ment approach – this approach was brieﬂy mentioned in [4]
but was not explored in any detail. This approach is based
on two key observations. First, an ISP that is an upstream
provider for some of the anycast servers routes incoming any-
cast traﬃc to the server closest among them – this is a conse-
quence of the fact intra-domain traﬃc engineering is mostly
consistent with latency sensitive routing [34]. For exam-
ple, in case of the internal deployment, ATT is an upstream
provider for three of the ﬁve anycast servers. Routers in
ATT’s network receive routes of equal AS-PATH length and
equal preference from each of these anycast servers. Hence,
the BGP decision process causes incoming anycast packets
at any ATT POP to be routed to the server that is clos-
est in terms of the intra-domain metrics used by ATT. As
the measurements presented next show, this server is also
closest in terms of latency in a large majority of cases.
Second, such a deployment de-couples route selection at
the common upstream provider from the selection at ASs
beyond it. Due to reasons detailed above, the common up-
stream provider delivers incoming anycast packets to the
closest server. Any ASs farther away from the anycast server
F
F
D
D
C
C
 1
 1
 0.8
 0.8
 0.6
 0.6
 0.4
 0.4
 0.2
 0.2
 0
 0
-50
-50
All
All
All - Cam
All - Cam
All - Cam - Cor
All - Cam - Cor
All - Pit - Berk
All - Pit - Berk
Diff.=30msec
Diff.=30msec
 0
 0
 50
 50
 100
 100
 150
 150
 200
 200
Difference of Anycast and Minimum Unicast latency (msec)
Difference of Anycast and Minimum Unicast latency (msec)
Figure 6: CDF for the diﬀerence between the any-
cast and minimum unicast latency for various sub-
sets of the internal deployment. Here, All - x implies
measurements with server x switched oﬀ.
sites than the upstream provider only need to select among
the possible routes to the upstream provider. Figure 5 il-
lustrates this. Of these ASs, the ones that use an “early-
exit” (or hot-potato routing) routing policy would route
anycast packets to the closest POP of the common upstream
provider. Alternatively, ASs that use a “late-exit” (or cold-
potato routing) routing policy would honor the MEDs of
the upstream provider and route anycast packets to the up-
stream provider POP that is most suitable for delivery to the
closest of the deployed anycast servers. While it is possible
that an AS may choose overly long paths to the upstream
provider, the prevalence of the two aforementioned policies
amongst ASs leads us to believe that this would not be the
norm and our measurements conﬁrm this. Its also important
to note that any overhead arising due to the choices made
at this step of the selection process also apply to the inter-
domain routing in general and are not speciﬁc to anycast
per se.
Hence, in case of the internal deployment, we would like
to ensure that instead of ﬁve servers with three diﬀerent
upstream providers, all the servers should have the same
provider. As a matter of fact, the subset of the internal de-
ployment comprising of the three servers at Berkeley, Pitts-
burgh and Seattle conforms to our deployment proposal.
These servers have the same upstream provider (ATT) and
are geographically spread out. With this three server de-
ployment, all anycast packets would be routed to the ATT
network and then delivered by ATT to the (closest) anycast
server. For instance, if the sample client (net.berkeley.edu)
presented above accessed this deployment, it would be routed
to the server at Berkeley. This is because, in ﬁgure 3, Level3
would not receive an advertisement for the anycast preﬁx
from WCG. Instead, Level3’s routers would route the client’s
packets to the ATT POP at San Francisco. The routers in
ATT’s San Francisco POP would in turn route these pack-
ets to the nearest anycast server. Thus, the anycast packets
would be routed to the server at Berkeley and not at Seattle
or Pittsburgh. We validated this by stopping the advertise-
ment of the anycast preﬁx from the Cornell and the Cam-
bridge server to remove them from the anycast deployment
and observing that the anycasted packets from the sample
client were delivered to the anycast server at Berkeley. This
path labeled as “Ideal Path” in ﬁgure 3.
To validate this hypothesis, we repeated the stretch fac-
tor measurements for our list of clients with only subsets of
the internal deployment operational. Figure 6 shows the re-
sults from these measurements. As can be seen, an anycast
deployment comprising of just the three servers at Berke-
ley, Pittsburgh and Seattle (labeled as “All - Cam - Cor”
in the ﬁgure) yields good proximity with just 5% of the
clients incurring a stretch factor of more than 30msec. Note
that it may seem that this improvement in the stretch fac-
tor for clients is due to the fact that the choice of servers
has reduced from ﬁve to three. To account for this we also
measured the proximity oﬀered by a deployment with the
three servers at Seattle, Cornell and Cambridge. All these
servers have a diﬀerent upstream provider and as can be
seen from the ﬁgure (curve labeled as “All - Pit - Berk”),
this yields poor proximity too. These results show that an
anycast deployment with all servers having the same up-
stream provider does provide good latency-based proximity
to clients.
As a matter of fact, this result can be generalized.
It
is possible to have anycast deployments with multiple up-
stream providers for the anycast servers. However, all the
upstream providers should have reasonable global geographic
spread (in essence, tier-1 ISPs with a global network) and for
each upstream provider, there must be a suﬃcient number
of servers to cover the geographical spread of the provider.
In such a set-up, whenever any of the upstream providers re-
ceives an anycast packet, there is a close-by anycast server
that the packet can be routed to. ASs beyond these up-
stream providers would choose to route the anycast packets
to one of the upstream providers and since all the providers
are well covered by the anycast servers, this choice does
not have a lot of bearing on the anycast path length. Due
to reasons described earlier, each AS beyond the upstream
providers for the anycast deployment would most likely route
the packets to a suitable POP of the upstream provider it
chooses. Thus, a deployment according to this model would
oﬀer good latency based proximity to clients. The small size
of the existing internal deployment does not allow us to val-
idate this claim; however, this is something that we aim to
address in future work.
6. FAILOVER TIME
Inter-domain IP Anycast involves each site advertising the
anycast preﬁx into BGP. Consequently, when an anycast
server fails, the process by which clients using the failed
server are re-routed to other operational servers is tied to
BGP convergence. Past studies have shown that failures
at multi-homed end sites can lead to a convergence process
that may last for minutes [23]. Such a slow failover, if it
applies to IP Anycast to, would not bode well for a number
proposed uses of IP Anycast. Hence we decided to measure
the failover rate for the internal anycast deployment. To the
best of our knowledge, this represents the ﬁrst attempt to
study the failure-mode behavior of IP Anycast.
Methodology: In order to determine the rate at which
clients failover when an anycast server fails, we need to be
able to determine the speciﬁc anycast server that each client
is routed to. To this eﬀect, we conﬁgured the anycast servers
in the internal deployment to act as authoritative name-
servers for a domain under our control (internal.anycast.
guha.cc) and to respond to TXT-type DNS queries for this
domain with a location-speciﬁc string. So for example, a
TXT-type DNS query for this domain from a client whose
anycast packets are routed to the anycast server at Cornell
will receive “Cornell” as the response. This, when combined
with the ability to direct clients to send DNS queries to the
Anycast Server
Berkeley
Anycast Server
Cornell
Anycast Server
Seattle 
Anycast Server
Pittsburgh
Anycast Server
Cambridge
4
5
Client X
(Recursive
Nameserver)
3
2
6
1 
Measurement Host (M) 
1  TXT? internal.anycast.guha.cc
2 NS? internal.anycast.guha.cc
3 NS(internal.anycast.guha.cc) 
=acast.anycast.guha.cc
A(acast.anycast.guha.cc)=204.9.168.1
4 TXT? internal.anycast.guha.cc
5 "cornell"
6
"cornell"
Figure 7: TXT-type DNS queries from client
X to the internal deployment anycast address
(204.9.168.1) are routed to the server at Cornell
which responds with a location-speciﬁc string (“cor-
nell”)
anycast address of the internal deployment, allows us to de-
termine the anycast server being accessed by all clients in
our list. Figure 7 illustrates this process for one client.
The internal deployment servers have been conﬁgured to
respond to TXT-type queries with a TTL value of 0. This
implies that clients cannot cache the response from the any-
cast server and hence, need to send packets to the deploy-
ment’s anycast address each time they are queried. Also,
note that since we don’t have any way to determine the
anycast server accessed by clients for the external deploy-
ments6, the measurements in this and the following sections
are restricted to the internal deployment.
Given this, we determined the impact of the failure of each
server in the internal deployment on clients being routed to
that server. We induced failures at individual servers by
tearing down their BGP peerings leading to a BGP with-
drawal being generated for the anycast preﬁx. Concurrently,
we sent the aforementioned TXT queries to the anycast ad-
dress through the clients that were being routed to the failed
server at a rate of once every ﬁve seconds for three minutes
and at a rate of once per minute for the next ﬁfty-seven min-
utes leading to a total probe period of one hour. For each
such client, we determined the time it takes for the client
to failover and “settle” at a diﬀerent anycast server. This
is referred to as the failover time for the client. Note that
during the convergence process, a client may be temporarily
routed to a server diﬀerent from the server it is ﬁnally set-
tles on. For example, a client accessing the Cornell server
before it failed may be temporarily routed to the server at
Cambridge before being routed to the server at Pittsburgh
for good. The time between the failure and the ﬁrst query
that is routed to the Pittsburgh is the client’s failover time.
We would also like to clarify that unlike aﬃnity (as dis-
cussed in the next section), fast failover is not very relevant
with regards to the feasibility of running connection oriented
services on top of IP Anycast. In a vast majority of the sce-
narios, the failure of a server would also break all client
connections irrespective of how fast the failover process is.
Instead, the failover time characterizes the time after a fail-
ure for which clients using the failed server cannot utilize
the anycast service.
6Some of the anycasted DNS root-server deployments do
allow users to query them for the particular server the user
is being routed to [45]. However, these queries have been
chosen such that they cannot be generated through recursive
nameservers, probably to avoid the possibility of this being
used for a DNS-ampliﬁcation attack on the root-servers [47].
F
F
D
D
C
C
F
F
D
D
C
C
 1
 1
 0.9
 0.9
 0.8
 0.8
 0.7
 0.7
 0.6
 0.6
 0.5
 0.5
 0.4
 0.4
 0.3
 0.3
 0.2
 0.2
 0.1
 0.1
 0
 0
 1
 1
 0.9
 0.9
 0.8
 0.8
 0.7
 0.7
 0.6
 0.6
 0.5
 0.5
 0.4
 0.4
 0.3
 0.3
 0.2
 0.2
 0.1
 0.1
 0
 0
Cor
Cor
Pit
Pit
Cam
Cam
Sea
Sea
Ber
Ber
 20
 20
 40
 40
 60
 60
 80
 80
 100
 100
 120
 120
Failover Time (sec)
Failover Time (sec)
(a) Failover Time
Cor
Cor
Pit
Pit
Cam
Cam
Sea
Sea
Ber
Ber
 20
 20
 40
 40
 60
 60
 80
 80
 100
 100
 120
 120
Recovery Time (sec)
Recovery Time (sec)
(b) Recovery Time
Figure 8: The failover and recovery times for the
servers in the internal deployment.
Similarly, we also restarted the failed servers by re-estab-
lishing their BGP peerings and determined the time it takes
for clients that were originally using the server in question
to be routed back to it again. This time between the re-
establishment of the peering and the re-routing of a client
to the server is referred to as the recovery time for the client.
For each server in the internal deployment, we induced fail-
ures and restarts and measured the failover and the recovery
time for the clients using the server. These experiments were
repeated 5 times each – we avoided more runs because the
experiments impose a heavy query load on nameservers that
we don’t own.
Results: Figure 8 shows the CDF for the failover and re-
covery times corresponding to each server in the deployment
(the time axis has been shortened in the interest of clarity).