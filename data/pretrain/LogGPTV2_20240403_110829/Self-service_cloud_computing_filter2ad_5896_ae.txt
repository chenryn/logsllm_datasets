### System Call Capture Tool and Overhead

The system call capture tool operates within the dom0 environment. As shown in Table 7, the experiment results indicate that running the system call monitor as a Secure Domain (SD) incurs negligible overhead.

### 4.4 Other SD-based Services

So far, we have discussed several security services implemented as SDs. However, the utility of SDs extends beyond security, and a variety of other services can be implemented using this model. In this section, we provide two examples: Checkpointing SD and Memory Deduplication SD.

#### Checkpointing SD

Cloud service providers frequently checkpoint client VMs for various purposes, such as live migration, load balancing, and debugging. In typical cloud architectures, checkpointing is managed by a user daemon in dom0, which copies the client VM's memory pages and stores them unencrypted within dom0. If dom0 is untrusted, creating trustworthy checkpoints becomes challenging [46]. SSC simplifies this process by allowing checkpointing to be implemented as an SD.

The SD maps the client’s memory pages and checkpoints them, similar to the dom0 checkpointing daemon (we reused the same codebase for the SD). As previously discussed, clients can chain the storage encryption SD with the checkpointing SD to ensure that the checkpoint data is encrypted.

We implemented a checkpointing SD and evaluated it by checkpointing VMs with two different memory footprints: 512MB and 1024MB. We also conducted an experiment where we chained this SD with the storage encryption SD, ensuring the checkpoint file was encrypted. To eliminate the effects of disk writes, we saved the checkpoint files on a memory-backed filesystem. Table 8 presents the results, comparing the costs of our checkpointing SD against a checkpointing service implemented in dom0. Our results show that the costs of implementing checkpointing within an SD are within 5% of those in dom0. In some cases, we even observed minor speedups when chaining checkpointing with encryption. Thus, SSC offers both security and flexibility to customers while imposing minimal overhead.

| Platform | Xen (dom0) | SSC (SD) | Xen (dom0) | SSC (SD) |
|----------|-------------|-----------|-------------|-----------|
| VM size (MB) | 512 | 512 | 1024 | 1024 |
| Time (seconds) | 6.948±0.187 | 6.941±0.045 (0%) | 15.607±0.841 | 15.788±0.659 (1.1%) |

#### Memory Deduplication SD

When multiple VMs share memory pages with identical content, memory deduplication can be used to conserve physical memory. This mechanism benefits cloud providers by improving the elasticity of their services and can also benefit cloud clients who may be billed for the memory consumed by their VMs. Identifying and exploiting memory sharing opportunities allows clients to purchase resources more judiciously, reducing their overall cloud usage costs. In commodity cloud computing environments, providers implement memory deduplication, but these services are not exposed to clients, limiting their applicability.

SSC allows clients to deploy memory deduplication on their own VMs without involving the cloud provider. We implemented a memory deduplication SD that accepts a list of domains (UdomUs) in the same meta-domain and identifies pages with identical content (using their MD5 hashes). For each such page, the SD instructs the hypervisor to keep just one copy and free the remaining copies by modifying the page tables of the domains. The hypervisor marks the shared pages as belonging to a special "shared memory" domain. When a domain attempts to write to a shared page, the hypervisor uses copy-on-write to create a local copy of that page and makes it unshared in that domain.

We evaluated the performance of the memory deduplication SD by measuring the time taken to identify candidate pages for sharing and marking them as shared. We conducted this experiment with a pair of VMs, each with memory footprints of 512MB and 1024MB. We compared the performance of the SD with that of a service running in dom0 on stock Xen. Table 9 presents the results, showing that the performance of the SD is comparable to the traditional approach.

| Platform | Xen (dom0) | SSC (SD) | Xen (dom0) | SSC (SD) |
|----------|-------------|-----------|-------------|-----------|
| VM size (MB) | 512 | 512 | 1024 | 1024 |
| Time (seconds) | 6.948±0.187 | 6.941±0.045 (0%) | 15.607±0.841 | 15.788±0.659 (1.1%) |

### 5. Implications of the SSC Model

The SSC model deviates from the techniques and assumptions used by contemporary cloud services. In this section, we discuss the implications of the SSC model, focusing on practical deployment issues.

#### 5.1 Use of Trusted Computing

SSC relies critically on trusted computing technology in the protocols used to build client domains (Figure 3). We assume that clients interact with a virtual TPM (vTPM) instance, with supporting daemons implemented in domB. The keys of this vTPM instance, particularly the attestation identity key (AIK) and the endorsement key (EK), are bound to the hardware TPM, as discussed in prior work [5].

When used in the context of cloud computing, the use of the TPM and associated attestation protocols raises three issues:
1. Do TPM/vTPM keys reveal details of the cloud provider’s infrastructure?
2. How are keys distributed?
3. Do TPM/vTPM measurements reveal proprietary details of the software platform?

**Can TPM/vTPM keys reveal physical details of the cloud infrastructure?**

SSC requires each physical machine in the cloud provider’s infrastructure to be equipped with a hardware TPM, which serves as a hardware root of trust. Trusted computing protocols typically require all keys used during attestation to be bound to a specific hardware TPM, including the TPM’s AIKs and the AIKs and EKs of vTPM instances hosted on a physical machine. AIKs are distributed to clients, who may include the cloud provider’s competitors. Researchers have argued that binding keys to the TPM can expose details of the underlying hardware platform to competitors [40].

Fortunately, such risks can be mitigated. According to specifications released by the Trusted Computing Group [21], each hardware TPM can have arbitrarily many AIKs. The TPM’s EK is unique and burned into the TPM chip by the hardware manufacturer. The public portion of the TPM’s EK is distributed to trusted third parties, called privacy certifying authorities (CAs). AIKs are bound to the TPM by signing them using the private portion of the TPM’s EK. Similarly, vTPM keys are also bound to the hardware TPM, e.g., by signing them using one of the hardware TPM’s AIKs [5]. Given an AIK, the privacy CA can certify that the AIK is genuine, i.e., it was generated by a hardware TPM. Although the association between an AIK and the hardware TPM to which it is bound is known to the privacy CA, this association is never released outside the privacy CA. In SSC, the privacy CA can either be hosted by the cloud provider or a trusted third party.

The protocols in Figure 3 only require the client to verify that an AIK is genuine, and therefore only require the client to interact with the privacy CA. The cloud provider can ensure that the client gets a fresh AIK for each execution of an attestation protocol. Because a single hardware TPM exposes multiple AIKs, it is impossible for an adversarial client to determine whether Udom0s running with different AIKs are executing on the same or different physical hosts, thereby protecting details of the cloud provider’s physical infrastructure.

Alternatively, the cloud provider could host a centralized, trusted cloud verification service, as proposed in prior work [41, 42]. This verification service enables indirect verification of hosts by vouching for their integrity. Clients could interact with this verification service to obtain attestations, instead of directly interacting with the vTPM on the execution platform, thereby alleviating the risks discussed above.

**How are keys distributed to clients?**

Before initiating the protocols in Figure 3, clients must first obtain the public key of the vTPM instance assigned to them. While key distribution has historically been a difficult problem, requiring public-key infrastructure (PKI) support, the centralized nature of cloud computing services eases key distribution. The cloud provider, who is trusted in SSC’s threat model, can establish trusted services required by PKI, such as a privacy CA and a central directory of AIK public keys. Prior to creating a new meta-domain, a client must leverage the PKI infrastructure to obtain the AIK public key of a vTPM instance assigned to it and use the privacy CA to determine whether the key is genuine.

**Can TPM/vTPM measurements reveal details of proprietary cloud software?**

TPM-based attestation protocols use measurements, typically hashes of software packages loaded for execution, to establish the trustworthiness of a platform. However, this approach may reveal specifics of the cloud provider’s software infrastructure to competitors. For example, measurements may reveal the use of a module implementing a particular scheduling algorithm or a performance-enhancing library. The protocols used by SSC are based on measurements and are therefore prone to this risk.

One way to mitigate such risks is to use property-based TPM protocols [34, 37, 40, 42, 44]. These protocols attest specific properties of the software platform, rather than low-level measurements that could reveal proprietary information to competitors. For example, on SSC, such protocols could attest that the hypervisor implements the SSC privilege model, without revealing additional information to clients. Prior research has integrated property-based attestation protocols with the vTPM [37]. We will investigate the applicability of these protocols to SSC in future work.

#### 5.2 VM Hosting and Migration

By its nature, SSC requires the co-location of certain VMs on the same platform. A client’s UdomU, any SDs and MTSDs associated with it, and the Udom0 of the client’s meta-domain must be co-located on the same platform. Such constraints call for new algorithms for VM scheduling and migration. For example, if the cloud provider migrates one of the client’s UdomUs to another host, it must also migrate the SDs that service that UdomU. Some of these SDs may service other UdomUs that are not migrated; in such cases, the SDs (and the Udom0) must be replicated on both hosts.

The stateless nature of Udom0 and several SDs (e.g., the storage encryption SD) can potentially ease migration. For such stateless domains, the cloud provider can simply start a fresh instance of the domain on the target platform. A more thorough investigation of the cost and resource implications of these issues requires deploying SSC on several hosts and making changes to administrative toolstacks (e.g., VM migration tools installed in Sdom0) to make them SSC-aware. We plan to investigate these topics in future research.

#### 5.3 Client Technical Knowhow

SSC provides clients with unprecedented flexibility to deploy customized cloud-based services and holds clients responsible for administering their own VMs. However, this does not necessarily mean that clients need increased technical knowhow or manpower to leverage the benefits of SSC, e.g., to implement their own services as SDs. Cloud providers can ease the deployment path for SSC by following an SD app store model akin to mobile application markets. Both cloud providers and third-party developers can contribute SDs to such an app store, from where they can be downloaded and used by clients. This model can provide a revenue generation opportunity for cloud providers (e.g., clients can purchase SDs they wish to use) and a reputation system for clients to judiciously choose SDs for their meta-domains. Technically sophisticated clients can still implement their own SDs without choosing from the app store.

Finally, one of the main advertised benefits of cloud computing is that it frees clients from having to administer their own VMs. By allowing clients to administer their own VMs, SSC apparently nullifies this benefit. We feel this is a fundamental tradeoff, and the price that clients must pay for increased security, privacy, and control over their VMs. One consequence of this tradeoff is that clients without the appropriate technical knowhow may commit administrative errors, e.g., giving a UdomU or an SD more privileges than it needs. Nevertheless, SSC ensures that the effects of such mistakes are confined to the client’s meta-domain and do not affect the operation of other clients on the same platform.

### 6. Related Work

In this section, we compare SSC with prior work in two areas: security and privacy of client VMs in the cloud, and extending the functionality of the cloud.

#### Security and Privacy of Client VMs

Popular cloud services, such as Amazon’s EC2 and Microsoft’s Azure, rely on hypervisor-based VMMs (Xen [3] and Hyper-V [32], respectively). In such VMMs, the TCB consists of the hypervisor and an administrative domain. Prior attempts to secure the TCB have focused on both these entities.

Historically, hypervisors have been considered to be a small layer of software. Prior work has argued that the architecture of hypervisors resembles that of microkernels [22]. The relatively small code size of research hypervisors [31, 43, 47], combined with recent breakthroughs in formally verifying the L4 microkernel [27], raises hope for similar verification of hypervisors. However, commodity hypervisors often contain several thousand lines of code (e.g., 150K LoC in Xen 4.1) and are not yet within the realm of formal verification. Consequently, researchers have proposed architectures that completely eliminate the hypervisor [26].

The main problem with these techniques (i.e., small hypervisors and hypervisor-free architectures) is that they often do not support the rich functionality needed in cloud computing. Production hypervisors today need to support different virtualization modes, guest quirks, hardware features, and software features like memory deduplication and migration. In SSC, we work with a commodity hypervisor-based VMM (Xen) but assume that the hypervisor is part of the TCB. While this exposes an SSC-based VMM to attacks directed against hypervisor vulnerabilities, it also allows the hypervisor to support the necessary functionality for cloud computing.