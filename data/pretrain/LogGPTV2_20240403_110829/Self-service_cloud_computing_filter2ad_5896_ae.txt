the system call capture tool runs in dom0. Table 7 presents the re-
sult of the experiment and shows that running system call monitor
as an SD incurs negligible overhead.
4.4 Other SD-based Services
So far, we have illustrated several security services implemented as
SDs. However, the utility of SDs is not limited to security alone,
and a number of other services can be implemented as SDs. We
illustrate two such examples in this section.
Checkpointing SD.
It is commonplace for cloud service providers
to checkpoint client VMs for various purposes, such as live migra-
tion, load balancing and debugging. On commodity cloud archi-
tectures, checkpointing is implemented as a user daemon within
dom0, which copies client VM memory pages and stores them un-
encrypted within dom0. If dom0 is untrusted, as is usually the case,
it is challenging to create trustworthy checkpoints [46]. SSC sim-
pliﬁes checkpointing by allowing it to be implemented as an SD.
The SD maps the client’s memory pages, and checkpoints them
akin to the dom0 checkpointing daemon (in fact, we reused the
same code-base to implement the SD). As previously discussed,
clients can chain the storage encryption SD with the checkpointing
SD to ensure that the checkpoint stores encrypted data.
We implemented a checkpointing SD and evaluated it by check-
pointing VMs with two memory footprints: 512MB and 1024MB.
We also conducted an experiment where we chained this SD with
storage encryption SD; the checkpoint ﬁle is therefore encrypted in
this case. To mask the eﬀects of disk writes, we saved the check-
point ﬁles on a memory-backed ﬁlesystem. Table 8 presents the
results of our experiments, comparing the costs of our checkpoint-
ing SD against a checkpointing service implemented in dom0. Our
results show that the costs of implementing checkpointing within
an SD are within 5% of implementing it within dom0. In fact, we
even observed minor speedups in the case where we chained check-
pointing with encryption. SSC therefore oﬀers both security and
ﬂexibility to customers while imposing mimimal overhead.
Memory Deduplication SD. When multiple VMs have memory
pages with identical content, one way to conserve physical memory
using a mechanism where VMs share memory pages [48]. Such a
mechanism beneﬁts cloud providers, who are always on the lookout
for new techniques to improve the elasticity of their services. It can
also beneﬁt cloud clients who may have multiple VMs on the cloud
and may be billed for the memory consumed by these VMs. Iden-
tifying and exploiting memory sharing opportunities among VMs
allows clients to judiciously purchase resources, thereby reducing
their overall cost of using the cloud. In commodity cloud comput-
ing environments, providers implement memory deduplication to
Platform
Xen (dom0)
SSC (SD)
Xen (dom0)
SSC (SD)
VM size (MB)
512
512
1024
1024
Time (seconds)
6.948±0.187
6.941±0.045 (0%)
15.607±0.841
15.788±0.659 (1.1%)
Table 9. Cost incurred by the memory deduplication SD.
consolidate physical resources, but such services are not exposed
to clients, thereby limiting their applicability.
SSC allows clients to deploy memory deduplication on their
own VMs without involving the cloud provider. To illustrate this,
we implemented a memory deduplication SD. This SD accepts as
input a list of domains (UdomUs) in the same meta-domain, and
identiﬁes pages with identical content (using their md5 hashes).
For each such page, the SD instructs the hypervisor to keep just
one copy of the page, and free the remaining copies by modifying
the page tables of the domains. The hypervisor marks the shared
pages as belonging to special “shared memory” domain. When a
domain attempts to write to the shared page, the hypervisor uses
copy-on-write to create a copy of that page local to the domain that
attempted the write, and makes it unshared in that domain.
We evaluated the performance of the memory deduplication SD
by measuring the time taken to identify candidate pages for sharing,
and marking them as shared. We conducted this experiment with a
pair of VMs with memory footprints of 512MB and 1024MB each.
As before, we compared the performance of the SD with that of a
service running in dom0 on stock Xen. Table 9 presents the results,
and shows that the performance of the SD is comparable to the
traditional approach.
IMPLICATIONS OF THE SSC MODEL
5.
The SSC model deviates in a number of ways from the techniques
and assumptions used by contemporary cloud services. In this sec-
tion, we discuss the implications of the SSC model. While the fo-
cus of this paper was on the core mechanisms needed to realize the
SSC model, the issues discussed in this section are important for
the practical deployment of an SSC platform.
5.1 Use of Trusted Computing
SSC relies critically on trusted computing technology in the proto-
cols used to build client domains (Figure 3). We assume that clients
interact with a vTPM instance, the supporting daemons for which
are implemented in domB. The keys of this vTPM instance (in
particular, the attestation identity key (AIK) and the endorsement
key (EK)) are bound to the hardware TPM as discussed in prior
work [5]. When used in the context of cloud computing, the use
of the TPM and associated attestation protocols raises three issues:
(1) do TPM/vTPM keys reveal details of the cloud provider’s in-
frastructure? (2) how are keys distributed? and (3) do TPM/vTPM
measurements reveal proprietary details of the software platform?
We discuss these issues below.
(1) Can TPM/vTPM keys reveal physical details of the cloud in-
frastructure? SSC requires each physical machine in the cloud
provider’s infrastructure to be equipped with a hardware TPM,
which serves as a hardware root of trust on that machine. Trusted
computing protocols typically require all keys used during attes-
tation to be bound to a speciﬁc hardware TPM. This includes the
TPM’s AIKs, and the AIKs and EKs of vTPM instances hosted
on a physical machine. AIKs are distributed to clients, who may
include the cloud provider’s competitors. Researchers have there-
fore argued that binding keys to the TPM can expose details of the
underlying hardware platform to competitors (e.g., [40]). For ex-
ample, a competitor may be able to infer the number of physical
machines in the cloud infrastructure.
261Fortunately, such risks can easily be alleviated. According to
speciﬁcations released by the Trusted Computing Group [21],
each hardware TPM can have arbitrarily many AIKs. However,
the TPM’s EK is unique, and is burned into the TPM chip by the
hardware manufacturer. The public portion of the TPM’s EK is
distributed to trusted third parties, called privacy certifying author-
ities (CAs). AIKs are bound to the TPM by signing them using the
private portion of the TPM’s EK. Likewise, vTPM keys are also
bound to the hardware TPM, e.g., by signing them using one of
the hardware TPM’s AIKs [5]. Given an AIK, the privacy CA can
certify that the AIK is genuine, i.e., it was indeed generated by a
hardware TPM. Although the association between an AIK and the
hardware TPM to which it is bound is known to the privacy CA,
this association is never released outside the privacy CA. In SSC,
the privacy CA can either be hosted by the cloud provider or a
trusted third party.
The protocols in Figure 3 only require the client to be able to
verify that an AIK is genuine, and therefore only require the client
to interact with the privacy CA. The cloud provider can ensure
that the client gets a fresh AIK for each execution of an attestation
protocol. Because a single hardware TPM exposes multiple AIKs,
it is impossible for an adversarial client to assert whether Udom0s
running with diﬀerent AIKs are executing on the same or diﬀerent
physical hosts, thereby protecting details of the cloud provider’s
physical infrastructure.
Alternatively,
the cloud provider could host a centralized,
trusted cloud veriﬁcation service, as proposed in prior work [41,
42]. This veriﬁcation service enables indirect veriﬁcation of hosts
by vouching for their integrity. Clients could interact with this veri-
ﬁcation service to obtain attestations, instead of directly interacting
with the vTPM on the execution platform, thereby alleviating the
risks discussed above.
(2) How are keys distributed to clients? Before initiating the pro-
tocols in Figure 3, clients must ﬁrst obtain the public key of the
vTPM instance assigned to them. While key distribution has histor-
ically been a diﬃcult problem, requiring public-key infrastructure
(PKI) support, the centralized nature of cloud computing services
eases key distribution. The cloud provider, who is trusted in SSC’s
threat model, can establish trusted services required by PKI, such
as a privacy CA and a central directory of AIK public keys. Prior to
creating a new meta-domain, a client must leverage the PKI infras-
tructure to obtain the AIK public key of a vTPM instance assigned
to it, and use the privacy CA determine whether the key is genuine.
(3) Can TPM/vTPM measurements reveal details of proprietary
cloud software? TPM-based attestation protocols use measure-
ments, typically hashes of software packages loaded for execution,
to establish the trustworthiness of a platform. However, this ap-
proach may reveal speciﬁcs of the cloud provider’s software infras-
tructure to competitors. For example, measurements may reveal the
use of a module implementing a particular scheduling algorithm or
a performance-enhancing library. The protocols used by SSC are
based on measurements and are therefore prone to this risk.
One way to alleviate such risks is to use property-based TPM
protocols [34, 37, 40, 42, 44]. The main feature of such protocols
is that they attest speciﬁc properties of the software platform.
That is, instead of attesting software using low-level measurements
(i.e., software hashes), which could reveal proprietary information
to competitors, they attest whether the software satisﬁes certain
properties implied by the client’s security goals. For example, on
SSC, such protocols could attest that the hypervisor implements
the SSC privilege model, but not reveal any additional information
to clients. Prior research has integrated property-based attestation
protocols with the vTPM [37]. We will investigate the applicability
of these protocols to SSC in future work.
5.2 VM Hosting and Migration
By its nature, SSC requires co-location of certain VMs on the same
platform. A client’s UdomU, any SDs and MTSDs associated with
it, and the Udom0 of the client’s meta-domain must be co-located
on the same platform. Such constraints call for research on new
algorithms for VM scheduling and migration. For example, if the
cloud provider migrates one of the client’s UdomUs to another
host, it must also migrate SDs that service that UdomU. Some of
these SDs may service other UdomUs that are not migrated; in such
cases, the SDs (and the Udom0) must be replicated on both hosts.
The stateless nature of Udom0 and several SDs (e.g., the storage
encryption SD) can potentially ease migration. For such stateless
domains, the cloud provider can simply start a fresh instance of
the domain on the target platform. A more thorough investigation
of the cost and resource implications of these issues requires a
deployment of SSC on several hosts. It also requires changes to
administrative toolstacks (e.g., VM migration tools, installed in
Sdom0) to make them SSC-aware. We plan to investigate these
topics in future research.
5.3 Client Technical Knowhow
SSC provides clients with unprecedented ﬂexibility to deploy cus-
tomized cloud-based services and holds clients responsible for ad-
ministering their own VMs. However, this does not necessarily
mean that clients need to have increased technical knowhow or
manpower to leverage the beneﬁts of SSC, e.g., to implement their
own services as SDs. Cloud providers can ease the deployment path
for SSC by following an SD app store model akin to mobile appli-
cation markets. Both cloud providers as well as third-party devel-
opers can contribute SDs to such an app store, from where they can
be downloaded and used by clients. In fact, the dynamics of such
an app store model can provide both a revenue generation oppor-
tunity to cloud providers (e.g., clients can purchase SDs that they
wish to use), as well as a reputation system for clients to judiciously
choose SDs for their meta-domains. Of course, technically sophis-
ticated clients can still implement their own SDs without choosing
from the app store.
Finally, one of the main advertised beneﬁts of cloud computing
is that it frees clients from having to administer their own VMs.
By allowing clients to administer their own VMs, SSC apparently
nulliﬁes this beneﬁt. We feel that this is a fundamental tradeoﬀ, and
the price that clients must pay for increased security, privacy, and
control over their VMs. One of the consequences of this tradeoﬀ
is that clients without the appropriate technical knowhow may
commit administrative errors, e.g., giving a UdomU or an SD more
privileges than it needs. Nevertheless, SSC ensures that the eﬀects
of such mistakes are conﬁned to the client’s meta-domain, and do
not aﬀect the operation of other clients on the same platform.
6. RELATED WORK
In this section, we compare SSC with prior work in two areas:
security and privacy of client VMs in the cloud, and extending the
functionality of the cloud.
Security and Privacy of Client VMs. Popular cloud services,
such as Amazon’s EC2 and Microsoft’s Azure rely on hypervisor-
based VMMs (Xen [3] and Hyper-V [32], respectively). In such
VMMs, the TCB consists of the hypervisor and an administrative
domain. Prior attempts to secure the TCB have focused on both
these entities, as discussed below.
Historically, hypervisors have been considered to be a small
layer of software. Prior work has argued that the architecture of hy-
pervisors resembles that of microkernels [22]. The relatively small
code size of research hypervisors [31, 43, 47], combined with the
262recent breakthrough in formally verifying the L4 microkernel [27],
raises hope for similar veriﬁcation of hypervisors. However, com-
modity hypervisors often contain several thousand lines of code
(e.g., 150K LoC in Xen 4.1) and are not yet within the realm of
formal veriﬁcation. Consequently, researchers have proposed archi-
tectures that completely eliminate the hypervisor [26].
The main problem with these techniques (i.e., small hypervisors
and hypervisor-free architectures) is that they often do not support
the rich functionality that is needed in cloud computing. Production
hypervisors today need to support diﬀerent virtualization modes,
guest quirks, hardware features, and software features like memory
deduplication and migration. In SSC, we work with a commodity
hypervisor-based VMM (Xen), but assume that the hypervisor is
part of the TCB. While this exposes an SSC-based VMM to at-
tacks directed against hypervisor vulnerabilities, it also allows the