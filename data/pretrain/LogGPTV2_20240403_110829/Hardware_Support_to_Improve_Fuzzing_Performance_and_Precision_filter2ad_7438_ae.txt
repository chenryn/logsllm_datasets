(i) h264ref
(j) omnetpp
(k) astar
(l) xalancbmk
Figure 7: Collisions from encoded trace edges among benchmarks. random indicates the baseline by AFL with random basic block IDs. The instb_*
family by SNAP adopts both memory addresses and instruction bytes as IDs. shift, rotate and bswap present the best collision rates achieved by bitwise
operations on memory addresses alone.
cache size to 128 KB. Alternatively, the extra storage can be repur-
posed as a dedicated buffer for the coverage bitmap to avoid cache
misses due to bitmap update, which we leave for future work.
In addition, Table 4 shows that the tracing overhead of AFL-gcc
is much larger. With the CPU-bound benchmarks that best approx-
imate the extreme circumstances, the overhead is expected [62], as
discussed in §3. This finding is generally consistent with the num-
bers from the x86 setup, which also incurs an average of 228.09%
overhead on the same testing suite by AFL-gcc. The extra slow-
down in the current RISC-V experiment is caused by the additional
instrumented locations in binaries due to the difference in ISAs. For
example, RISC-V does not define a specific instruction for backward
edges (i.e., ret), which are often not tracked on instrumented x86
binaries. Thus, the RISC-V benchmarks have 58.51% more instru-
mentation than the x86 version, resulting in a 40.03% increase in the
binary size. Note that the cache size has negligible impact on the
tracing overhead for the software-based solution. Although bitmap
updates can still cause cache thrashing, the overhead mainly comes
from the execution cost of instrumented instructions.
5.3 Preserving Traces
Figure 7 shows the collision rates of various edge-encoding schemes
on the SPEC benchmarks with the reference workload. The colli-
sions are confirmed by comparing full execution traces against their
resulting bitmaps accordingly. A lower collision rate implies that a
more complete code trace is preserved when a binary is fully tested.
Together with the result of gcc from a limited bitmap size (i.e., 64
KB), Algorithm 1 by AFL consistently generates the lowest collision
rates (11.47%) among all. With random basic block IDs inserted at
compile time, true randomness is introduced into the algorithm to
avoid collisions. In comparison, the approaches that directly adopt
the memory address of a basic block as its ID perform significantly
worse. Even with the bitwise operations (i.e., logical shifts, circular
shifts, or endian swaps N bits of the block addresses before bitwise
XORing them), the impact from well-aligned RISC-V instructions
cannot be reduced. In particular, the most effective one, circular
shift, produces an average of 337 more colliding edges than AFL per
benchmark, with a worst case of a 5.29% increase (i.e., h264ref).
Name
#Block
Collision Rate (%)
instb_64
perlbench
14.44
bzip2
1.96
gcc
32.61
mcf
1.70
gobmk
14.35
hmmer
3.33
sjeng
3.07
libquantum
2.05
h264ref
5.94
omnetpp
7.82
astar
2.60
xalancbmk
18.47
Mean
9.03
Table 6: Collisions from encoded trace blocks among benchmarks.
random instb_16
16.91
2.32
34.99
1.67
16.08
3.21
3.49
2.29
6.93
8.41
2.58
18.82
9.81
instb_32
14.61
2.23
32.55
1.34
14.40
3.02
3.01
1.50
6.19
7.84
2.12
18.51
8.94
instb_48
14.60
2.15
32.71
1.34
13.80
3.57
3.28
1.72
5.90
8.05
2.11
18.45
8.97
18,612
2,069
55,222
1,493
19,762
3,360
3,351
1,397
7,365
10,321
2,441
27,169
12,713
13.06
1.61
32.39
0.87
13.58
2.47
2.45
1.15
5.53
7.04
1.90
18.06
8.34
On the other hand, Algorithm 2 by SNAP can achieve collision
rates (11.70%) similar to those of AFL. By leveraging the entropy
of RISC-V instruction(s) at the start of a basic block, SNAP signifi-
cantly outperforms the aforementioned approaches that solely rely
on memory addresses for encoding. Meanwhile, we cannot find a
consistent pattern of including more instruction bytes for fewer
collisions beyond 32 bits, as shown in Figure 7. Since most instruc-
tions are 32-bit long, gathering data fields (e.g., opcode, registers,
and immediates) beyond one instruction might not be helpful in
practice.
Besides edge coverage, basic block coverage also serves as a met-
ric adopted by existing fuzzers [43, 52] to measure code coverage.
Table 6 shows the collisions from SNAP using basic block coverage
(§4.4) across the same benchmarks. The mechanism proposed by
SNAP (i.e., instb_32) reaches a collision rate of 8.94% on average,
similar to the rate by AFL (8.34%). Therefore, SNAP is considered
equally accurate in terms of preserving either block or edge traces.
5.4 Evaluating Fuzzing Metrics
To understand how SNAP improves fuzzing metrics, we evaluate it
on seven Binutils binaries. Given the default corpus, we compare
the code coverage and runtime throughput of AFL running for
24 hours under the existing DBI scheme (i.e., AFL-QEMU), source
instrumentation (i.e., AFL-gcc), and support of SNAP.
1819202122CollisionRate(%)18.219.318.418.919.220.420.427.0randominstb16instb32instb48instb64shiftrotatebswap123451.62.41.82.52.44.84.86.1434445464743.343.743.443.743.644.244.251.0123451.01.61.31.71.83.83.63.2202122232420.821.321.121.521.322.922.929.8345673.43.93.53.63.75.25.16.345678CollisionRate(%)4.04.44.24.24.37.67.67.5123451.41.91.61.92.24.23.83.678910117.48.17.68.28.212.812.715.7101112131410.611.411.011.211.111.912.014.6234562.12.62.42.32.35.35.35.2232425262723.824.224.024.524.024.824.831.2Session 7B: FuzzingCCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2223Figure 8: The average execution speed from fuzzing with AFL-
QEMU, AFL-gcc and AFL-SNAP for 24 hours across the Binutils bi-
naries. The numbers below the bars of AFL-QEMU show the number of
executions per second for the mechanism.
Fuzzing throughput. Figure 8 shows the fuzzing throughput
across the compared mechanisms. Specifically, AFL on SNAP can
achieve 228× higher execution speed than AFL-QEMU, which is lim-
ited by the low clock frequency and its inefficient RISC-V support.
The average throughput of AFL-QEMU (i.e., 0.18 exec/s) is consis-
tent with the previous findings in PHMon [14]. Note that SNAP im-
proves the throughput much more significantly than PHMon, which
only achieves a 16× higher throughput than AFL-QEMU. Despite
that the baseline BOOM core in SNAP is about 50% faster [65] than
the baseline Rocket core [5] adopted by PHMon, SNAP achieves a
14× higher throughput-increase in comparison mainly due to its
design optimizations (e.g., opportunistic bitmap update and mem-
ory request aggregation). Compared to AFL-gcc, SNAP can still
achieve a 41.31% higher throughput on average across the bench-
marks. More throughput comparisons on x86 platforms are shown
in Appendix A.
Edge coverage. Figure 9 depicts the resulting coverage measure-
ment, where the confidence intervals indicate the deviations across
five consecutive runs on each benchmark. Given an immature seed
corpus and a time limit, AFL with SNAP consistently covers more
paths than the others throughout the experiment. Since no change
to fuzzing heuristics (e.g., seed selection or mutation strategies)
is made, the higher throughput of SNAP is the key contributor to
its outperformance. On average, AFL-QEMU and AFL-gcc have
only reached 23.26% and 84.59% of the paths discovered by AFL-
SNAP, respectively. Although larger deviations can be observed
when the program logic is relatively simple (Figure 9f), SNAP in
general can help explore more paths in programs with practical
sizes and complexity thanks to its higher throughput. For example,
AFL with SNAP manages to find 579 (16.74%), 237 (20.82%), and 378
(19.77%) more paths when fuzzing cxxfilt, objdump, and readelf,
respectively.
Adopting execution context. Given the last-executed branches
and their prediction results in LBQ, fuzzers on SNAP are equipped
with additional program states. To take the feedback, one can easily
follow the mechanisms introduced previously [13, 26, 28]. Our
prototype of AFL instead adopts a feedback encoding mechanism
similar to that in Algorithm 1 to showcase the usage. Specifically,
the highest significant bit (HSB) of each 64-bit branch address is set
based on the respective prediction result (i.e., 1/0). To maintain the
order of branches, the records are iterated from the least recent to
the latest in the circular LBQ and right circular shift’ed (i.e., rotated)
by N bits based on their relative positions in the sequence before
(a) cxxfilt
(b) nm
(c) objdump
(d) readelf
(e) size
(f) strings
(g) strip
Figure 9: The overall covered paths from fuzzing seven Binutils bi-
naries for 24 hours. The solid lines represent the means, and the shaded
areas suggest the confidence intervals of five consecutive runs.
being bitwise XOR’ed. The encoded value is finally indexed into a
separate bitmap from the one for edge coverage (i.e., trace_bits).
Reproducing a known bug. Running on SNAP, the modified
AFL is able to trigger CVE-2018-9138 discovered by the previous
work [13], which proposes using feedback similar to that provided
by our platform. As in Figure 1, the vulnerability occurs when
cxxfilt consumes a long consecutive input of "F"s, each indicat-
ing that the current mangled symbol stands for a function. The
corresponding switch case in the loop (line 5-10) tries to further
demangle the function arguments (i.e., demangle_args()) before
running into the next "F" to start a recursive call chain. Luckily,
SNAP offers the execution context by capturing branch sequences
cxxﬁltnmobjdumpreadelfsizestringsstrip0100200300400ThroughputImprovement(X)1111111166159154163169152165196223212263229258213(0.33)(0.14)(0.15)(0.12)(0.15)(0.22)(0.14)AFL-QEMUAFL-gccAFL-SNAP0510152025h0.00K1.00K2.00K3.00K#coveredpathsAFL-QEMUAFL-gccAFL-SNAP0510152025h0.00K0.30K0.60K0.90K#coveredpaths0510152025h0.00K0.40K0.80K1.20K#coveredpaths0510152025h0.00K0.60K1.20K1.80K#coveredpaths0510152025h0.00K0.25K0.50K0.75K#coveredpaths0510152025h0.00K0.04K0.08K0.12K#coveredpaths0510152025h0.00K0.25K0.50K0.75K#coveredpathsSession 7B: FuzzingCCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2224Description
BOOM core
SNAP core
Area (mm2)
9.2360
9.6811
Power (mW)
36.4707
38.8513
Table 7: Estimates of area and power consumption.
triggered by mutated inputs. While a vanilla AFL cannot easily
reach the faulty program state with only edge coverage feedback,
our fuzzer can consistently achieve it within one fuzzing cycle, led
by the guidance.
5.5 Practicality of SNAP
Easy adoption. To show how SNAP can be easily adopted, we
have integrated a variety of fuzzers from FuzzBench [25], including
AFL [61], AFLFast [10], AFLSmart [51], FairFuzz [42], MOpt [45],
and WEIZZ [16]. The others are excluded from the list, not because
of fundamental challenges to adopt SNAP but due to the incom-
patibility of RISC-V. For example, HonggFuzz [26], libFuzzer [43],
Entropic [9], laf-intel [41], and Ankou [46] fail to compile on
Fedora/RISC-V due to the lack of support from LLVM, GO, and
their dependent libraries (e.g., libunwind). Otherwise, the adoption
of SNAP is straightforward, requiring only a change of less than
100 LoCs consistently. Around 55 LoCs are C code that issues the
system calls for creating shared bitmap and branch records, as well
as comparing execution context per testcase. The others are as-
sembly that compiles RISC-V binaries to work with forkserver (i.e.,
afl-gcc).
Area and power overhead. To estimate the area and power over-
head of SNAP, we synthesize our design using Synopsys Design
Compiler at 1GHz clock frequency. To obtain a realistic estimate
of the SRAM modules such as L1 D-cache, L1 I-cache, and branch
predictor (BPD) tables used in the BOOM core, we black-box all
the SRAM blocks and use analytical models from OpenRAM [29].
Our synthesis uses 45nm FreePDK libraries [55] to measure the
area and power consumption between the unmodified BOOM core
and the modified SNAP core. Table 7 shows that SNAP only incurs
4.82% area and 6.53% power overhead, more area-efficient than
the comparable solution (16.5%) that enables hardware-accelerated
fuzzing [14]. When tracing is disabled, the power overhead can be
mostly avoided by clock gating through the switch CSR TraceEn.
6 DISCUSSION
Comparison with PHMon. PHMon [14] is a recently proposed
hardware-based security monitor that enforces expressive policy
rules. Despite its demonstration of basic hardware-assisted trac-
ing for fuzzing, PHMon is not specifically designed for this pur-
pose. In comparison, SNAP outperforms PHMon by a 14× higher
fuzzing throughput thanks to the optimizations dedicated to light-
weight tracing, including opportunistic updates to utilize free cache
bandwidth, issuing speculative load operations to avoid delays, and
memory request aggregation to reduce operations. Moreover, SNAP
enables additional execution semantics as context-aware fuzzing
feedback without extra performance cost by providing last-executed
branches and their branch prediction results. Together with the
cleverly encoded bitmap of low collision rates, SNAP helps fuzzers
explore more program states for more interesting mutations.
Usage beyond fuzzing. Although fuzzing is a first-class citizen
targeted by SNAP, other applications are also welcomed by the
general design. For example, SNAP can provide an efficient coverage
estimation for unit testing, which incurs significant hassle and
overhead with existing mechanisms such as gcov [20] and Intel
PT [36]. The information can also serve as an execution fingerprint
for logging and forensic purposes. Last, partial feedback, such as
branch prediction results, can serve as approximated performance
metrics with profiled cache misses in a specific code region.
Limitations and future directions. While SNAP is carefully de-