I am considering setting up a Hadoop cluster on Amazon EC2 to download and process tens of thousands of files. However, before I invest too much time and effort into this, I would appreciate some input from someone with more experience in Hadoop. My main concern is whether it's feasible to download files directly onto the Hadoop slaves.

If you believe this is possible, can I expect each slave running on Amazon EC2 to have a unique IP address? Additionally, I prefer to use Python for most of the tasks (e.g., the `urllib2` module for downloading) and minimize the use of Java.

**Response:**

Yes, it is indeed possible to download data onto a Hadoop cluster running on Amazon EC2. Hadoop's Distributed File System (HDFS) is designed to distribute and manage data blocks across the nodes (slaves) in your cluster. HDFS also ensures that the specified replication factor is maintained, which helps in fault tolerance and data availability.

Each slave node in your EC2 cluster will have a unique IP address, allowing you to manage and communicate with them individually. 

For the downloading process, you can use Python with the `urllib2` module or other libraries like `requests` to fetch the files. You can then write the downloaded files to HDFS using the Hadoop command-line tools or the Hadoop Python library, such as `pyhdfs`. This way, you can leverage Python for most of your tasks while minimizing the need for Java.