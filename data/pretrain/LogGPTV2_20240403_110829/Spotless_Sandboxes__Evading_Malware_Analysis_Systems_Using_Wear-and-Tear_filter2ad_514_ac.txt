d
_
r
i
s
p
p
A
d
e
l
l
a
t
s
n
_
r
I
l
s
p
p
A
a
o
_
r
t
t
t
n
u
o
C
n
u
R
o
u
a
_
r
t
t
i
n
u
o
C
t
s
s
s
a
r
s
u
_
r
s
e
i
r
t
n
E
e
h
c
a
C
U
M
_
r
I
t
l
n
u
o
C
e
u
r
e
r
i
F
_
r
s
l
l
D
d
e
r
a
h
S
a
o
_
r
t
t
l
s
l
l
i
i
D
g
n
s
s
M
a
o
_
r
t
t
l
i
s
e
k
o
o
C
a
o
t
l
t
s
k
r
a
m
k
o
o
B
a
o
t
l
t
i
i
s
n
a
m
o
D
e
k
o
o
C
e
u
q
n
u
i
t
v
e
s
y
s
_
e
t
v
e
p
p
a
_
e
s
y
a
d
f
f
i
d
p
p
a
_
e
s
y
a
d
f
f
i
d
s
y
s
_
e
s
y
a
D
f
f
i
i
D
e
k
o
o
c
s
L
R
U
d
e
p
y
T
a
o
t
l
t
i
e
z
S
p
m
u
D
n
m
_
d
i
i
t
n
u
o
C
p
m
u
D
n
m
_
d
i
i
s
e
s
s
e
c
o
r
P
a
o
_
d
t
t
l
i
l
e
z
S
r
e
d
o
F
s
b
m
u
h
_
d
t
t
n
u
o
C
e
l
i
F
p
o
t
k
s
e
d
_
d
Fig. 1: Distribution of artifacts across user machines, sandboxes, and baselines (normalized values).
System artifacts
m
u
n
_
b
s
L
R
U
e
u
q
n
u
i
s
y
a
D
f
f
i
D
l
r
u
s
e
l
i
l
F
d
e
d
a
o
n
w
o
D
a
o
t
l
t
TABLE III: Sandboxes from which artifacts were successfully
collected.
ID
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
Name
Anubis
Avira
Drweb
Fortiguard
Fsecure
Kaspersky
McAfee
Microsoft
HybridAnalysis
Pctools
Crawlers
Malwr
Sophos
ThreatTrack
Bitdefender
Minotaur
#Instances
2
2
3
3
2
15
7
1
1
1
50
2
4
3
1
1
TABLE IV: Distribution of operating systems in each dataset.
v5.1
8
14
1
v6.0
3
-
1
v6.1
105
82
2
v6.2
7
-
1
v6.3
147
2
17
Total
270
98
22
Users (Dreal)
Sandboxes (Dsand)
Baseline (Dbase)
v5.1: Windows XP
v6.0: Windows Vista / Win Server 2008
v6.1: Win7, Win Server 2008 R2
v6.2: Win8 / Win Server 2012
v6.3: Win 8.1/Win 10/ Win Server 2012 R2/ Win Server 2016
(Dreal, Dsand and, Dbase). There we can observe multiple
patterns that will be of use later on in this paper. For example,
we see that the vast majority of artifacts have larger values in
real user systems, than they have in sandboxes and baselines.
This observation, by itself, indicates that these artifacts can be
used to differentiate between real machines and sandboxes and
can thus be used for malware evasion purposes. Similarly, we
also observe that the distributions of artifact values is wider
for real systems than it is for systems that are not used by
real users. This could indicate that these artifacts could also
be used to predict the age of any given machine.
At the same time, we do see certain cases where the
opposite behavior manifests. For instance, the appdiffdays and
sysdiffdays artifacts are clearly more diverse in sandboxes than
they are in real user systems. We argue that the explanation for
this behavior is two-fold. As Table IV suggests, on average,
sandboxes are “older” compared to real-user systems. As such,
it is entirely within reason that the events in Windows’ event
log are further apart than those in real user systems. Moreover,
assuming that the Windows log will only store events up to
a maximum number, systems with more real-user activity are
more likely to need to delete older events and hence reduce
the time span observable in the Windows event log.
To verify, from a statistical point of view, the difference
that we can visually identify between the distributions of
artifacts, we need to use a metric that will allow us to
differentiate between different distributions. Even though a
normal distribution is usually assumed, by conducting a
Shapiro-Wilk test we ﬁnd that none of our extracted artifacts
matches a normal distribution. For that reason, we use the
Mann-Whitney U test to compare distributions, which is a
non-parametric alternative of a t-test and does not assume that
the data is normally distributed. We handle missing values by
replacing them with dummy-coded values because the fact that
an artifact cannot be extracted can be useful information for
differentiating between systems.
Figure 2 shows the effect size of the differences between
the distribution of Dreal and Dsand as reported by the Mann-
1016
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:22:37 UTC from IEEE Xplore.  Restrictions apply. 
Whitney U test. Using standard interpretations of effect sizes,
an effect between 0.3 and 0.5 is considered to be characteristic
of a “medium” effect while one higher than 0.5 is considered to
be a “strong” effect. This conﬁrms that the vast majority of our
identiﬁed wear-and-tear artifacts have different distributions in
sandbox and real-user environments.
V. EVADING MALWARE SANDBOXES
In the previous sections, we described the type of OS artifacts
that we hypothesized would be indicative of wear and tear, and
collected these artifacts from real user machines, sandboxes,
and fresh installations of operating systems in virtualized
environments. As our results showed, most artifacts have
sufﬁciently different distributions, allowing them to be used to
predict whether a given system is an artiﬁcial environment or
a real user system, and evade the former while executing on
the latter. In this section, we take advantage of these artifacts
and treat them as features in a supervised machine-learning
setting, allowing the implementation of simple decision trees
that can be employed by malware for evasion purposes.
A. Setup and Classiﬁer
When we were considering which classiﬁer to use, we
came to the conclusion that explainability of the results is
a desirable property for our setup. It is important for the
security community to understand how these seemingly benign
features are different between real user systems and sandbox
environments. Moreover, attackers can also beneﬁt from a
classiﬁer with explainable results since they can, in real time,
reason whether the result of their classiﬁer makes intuitive
sense given the execution environment. As such, even if an
SVM-based classiﬁer may perform slightly better than most
explainable supervised learning algorithms, attackers have no
way of evaluating the truthfulness of the result, especially
in environments that are constantly evolving to bypass their
evasion techniques.
For these reasons, we decided to use decision trees as our
algorithm of choice. Next to their explainable results, a decision-
tree model is also straightforward to implement as a series of
if-else statements, which can be beneﬁcial for attackers who
wish to keep the overall footprint of their malware small. Before
training our classiﬁer, we inspected the correlation of pairs
of features by calculating the Pearson correlation coefﬁcient
(r) for each possible pair. The results showed that four pairs
of features have a strong correlation (r > 0.7). We decided,
however, to include all features in our classiﬁer, since from
the point of view of defenders, they would have to address all
correlated features in order to stop malware from successfully
evading their analysis systems.
The training set of the classiﬁer is composed as follows.
For positive examples, we randomly sample 50% of our
sandbox observations (49 instances) and add to them 100%
(22 instances) of our fresh OS installations. We consider it
important to augment the sandbox dataset (Dsand) with our
baseline dataset (Dbase) to cover a wider range of possible
sandbox environments, starting from the most basic virtualiza-
tion environments, to state-of-the-art sandboxes belonging to
the security industry. To balance this data, we randomly sample
71 instances from real-user environments collected through
Amazon Mechanical Turk (Dreal). Therefore, our training set
consists of 142 observations, balanced between the two classes.
Similarly, our testing dataset comprises the remaining 50%
of sandbox observations (49 instances) and a random sample
of 49 real user machines which were not part of our training
set. We handle missing feature values by setting them equal
to zero.
To improve the accuracy of our classiﬁer, we utilize two
orthogonal techniques. First, we use a 10-trial adaptive boosting
where the algorithm creates multiple trees, with each tree
focusing on improving the detection of the previous tree [53].
Speciﬁcally, while constructing treei, the algorithm assigns
larger weights to the observations misclassiﬁed by treei−1,
aiming to improve the detection rate for these observations.
Through the construction of multiple sequential trees, the
adaptive boosting decision tree algorithm can learn boundaries
that a single decision tree could not. Note that this choice does
not really complicate the implementation of this algorithm by
malware, as it can still be implemented as multiple sets of if-
else rules, and use a weighted sum of all predictions according
to the weights speciﬁed by the trained model.
In addition to adaptive boosting, we specify our own cost
matrix where false negatives are ten times more expensive
than false positives. The rationale for this decision is that we
assume it is preferable for malware, from the point of view
of the malware author, to miss an infection of a real user’s
machine (false positive), rather than to execute in a sandbox
environment where its detection would mean the creation of
signatures that would jeopardize its ability to infect systems
protected with antivirus software.
B. Evaluation
Using the aforementioned training dataset and parameters,
we trained an adaptive boosting decision tree and evaluated it
on the testing set. Figure 3 shows three of the ten trees that our
classiﬁer generated, as well as the fraction of misclassiﬁcation
of each ﬁnal decision which, as described earlier, drives the