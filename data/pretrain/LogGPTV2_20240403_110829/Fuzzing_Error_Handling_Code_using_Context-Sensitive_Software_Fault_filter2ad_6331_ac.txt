deﬁned in the tested program or an external library. In most
cases, a function deﬁned in the tested program can fail, as it
calls speciﬁc library functions that can fail. If this function
and its called library functions are both considered for fault
injection, repeated faults may be injected. To avoid repetition,
from all the identiﬁed function calls, our analysis only selects
those whose called functions are library functions.
S3: Performing statistical analysis. In some cases, a func-
tion can actually fail and trigger error handling, but the return
values of several calls to this function are not checked by if
statements. To handle such cases, our analysis use a statistical
method to extract functions that can fail from the identiﬁed
function calls, and we refer to such a function as an error
function. At ﬁrst, this method classiﬁes the selected func-
tion calls by called function, and collects all function calls
to each called function in the tested program code. Then, for
the function calls to a given function, this method calculates
the percent of them whose return values are checked by if
Figure 6: Example of the normal mutation.
4 FIFUZZ Framework
Based on our context-sensitive SFI-based fuzzing approach,
we design a new fuzzing framework named FIFUZZ, to effec-
tively test error handling code. We have implemented FIFUZZ
using Clang [16]. FIFUZZ performs code analysis and code
instrumentation on the LLVM bytecode of the tested program.
To be compatible with traditional fuzzing process, FIFUZZ
mutates the error sequences and program inputs together. Fig-
ure 7 shows its architecture, consisting of six parts:
• Error-site extractor. It performs an automated static
analysis of the source code of the tested program, to
idenﬁty possible error sites.
• Program generator. It performs code instrumention on
the program code, including identiﬁed error sites, func-
tion calls, function entries and exits, code branches, etc.
It generates an executable tested program.
• Runtime monitor. It runs the tested program with gen-
erated inputs, collects runtime information of the tested
program, and performs fault injection according to gen-
erated error sequences.
• Error-sequence generator. It creates error sequences,
and mutates error sequences to generate new error se-
quences, according to collected runtime information.
• Input generator. It performs traditional fuzzing process
to mutate and generate new inputs, according to collected
runtime information.
• Bug checkers. They check the collected runtime infor-
mation to detect bugs and generate bug reports.
Based on the above architecture, FIFUZZ consists of two
phases, which are introduced as follows.
2600    29th USENIX Security Symposium
USENIX Association
Original error sequenceMutationGenerated error sequencesErrPta1ErrPtb1ErrPtc0ErrPtd0ErrPta0ErrPtb1ErrPtc0ErrPtd0ErrPta1ErrPtb0ErrPtc0ErrPtd0ErrPta1ErrPtb1ErrPtc1ErrPtd0ErrPta1ErrPtb1ErrPtc0ErrPtd1Tested ProgramExecutionFault injectionCode coverage is increased!ErrPta1ErrPtb1ErrPtx0ErrPtc0ErrPtd0A new error point is executed MutationCreated error sequenceErrPta1ErrPtb1ErrPtx0ErrPtc0ErrPtd1ErrPta1ErrPtb1ErrPtx0ErrPtc1ErrPtd0ErrPta1ErrPtb1ErrPtx1ErrPtc0ErrPtd0ErrPta1ErrPtb0ErrPtx0ErrPtc0ErrPtd0ErrPta0ErrPtb1ErrPtx0ErrPtc0ErrPtd0Original error sequenceErrPta0ErrPtb1ErrPtc0ErrPtd0Tested ProgramExecutionFault injectionCode coverage is not increased!Drop error sequencesA new error point is executed Created error sequenceErrPta0ErrPtb1ErrPty0ErrPtc0ErrPtd0Drop repeated error sequencesErrSeq2ErrSeq1Source Files of the tested programProgram GeneratorRuntime MonitorExecutable ProgramError-Sequence GeneratorInputGeneratorRuntime InformationError SequencesProgram InputsRecommended Error SitesBug CheckersError-Site ExtractorOriginal Program InputsRuntime InformationBug Reportsstatements. If this percent is larger than a threshold R, this
method identiﬁes this function as an error function. Finally,
this method extracts all function calls to this function are
identiﬁed error sites. For accuracy and generality, if there are
multiple tested programs, this method analyzes the source
code of all the tested programs together.
In our analysis, the value of the threshold R in the third step
heavily affects the identiﬁed error functions and identiﬁed
error sites (function calls). For example, less error functions
and error sites can be identiﬁed, as R becomes larger. In this
case, more unrealistic error functions and error sites can be
dropped, but more realistic ones may be also missed. We
study the impact of the value of R in Section 5.2.
Program
vim
bison
ffmpeg
nasm
catdoc
clamav
cﬂow
gif2png+libpng
openssl
Description
Text editor
Parser generator
Solution for media processing
80x86 and x86-64 assembler
MS-Word-ﬁle viewer
Antivirus engine
Code analyzer of C source ﬁles
File converter for pictures
Cryptography library
Version
v8.1.1764
v3.4
n4.3-dev
v2.14.02
v0.95
v0.101.2
v1.6
v2.5.14+v1.6.37
v1.1.1d
LOC
349K
82K
1.1M
94K
4K
844K
37K
59K
415K
Table 3: Basic information of the tested applications.
4.2 Runtime Fuzzing
In this phase, with the identiﬁed error sites and instrumented
code, FIFUZZ performs our context-sensitive SFI-based
fuzzing approach, and uses traditional fuzzing process of
program inputs referring to AFL [1].
The runtime fuzzer executes the tested program using the
program inputs generated by traditional fuzzing process, and
injects faults into the program using the error sequences gen-
erated by our SFI-based fuzzing approach. It also collects run-
time information about executed error points, code branches,
etc. According to the collected runtime information, the error-
sequence generator creates error sequences and performs mu-
tation to generate new error sequences; the input generator
performs coverage-guided mutation to generate new inputs.
Then, FIFUZZ combines these generated error sequences and
inputs together, and use them in runtime fuzzer to execute the
tested program again. To detect bugs, the bug checkers ana-
lyze the collected runtime information. These bug checkers
can be third-party sanitizers, such as ASan [4] and MSan [41].
Figure 8: Example of code instrumentation.
5 Evaluation
Code instrumentation. The code instrumentation serves
for two purposes: collecting runtime information about error
sites and injecting faults. To collect the information about run-
time calling context of each error site, the program generator
instruments code before and after each function call to each
function deﬁned in the tested program code, and at the entry
and exit in each function deﬁnition. Besides, on the other
hand, to monitor the execution of error sites and perform fault
injection into them, the program generator instruments code
before each error site. During program execution, the runtime
calling context of this error site and its location are collected
to create an error point. Then, if this error point can be found
in the current error sequence, and its value is 1 (indicating this
error point should fail for fault injection) a fault is injected
into the error point. In this case, the function call of related
error site is not executed, and its return values is assigned to
a null pointer or a random negative integer. If the value of
this error point in the error sequence is 0, the function call
of related error site is normally executed. Figure 8 shows an
example of instrumented code in the C code. Note that code
instrumentation is actually performed on the LLVM bytecode.
5.1 Experimental Setup
To validate the effectiveness of FIFUZZ, we evaluate it on
9 extensively-tested and widely-used C applications of the
latest versions as of our evaluation. These applications are
used for different purposes, such as text editor (vim), media
processing (ffmpeg), virus scan (clamav) and so on. The in-
formation of these applications are listed in Table 3 (the lines
of source code are counted by CLOC [17]). The experiment
runs on a regular desktop with eight Intel PI:EMAIL
processors and 16GB physical memory. The used compiler is
Clang 6.0 [16], and the operating system is Ubuntu 18.04.
5.2 Error-Site Extraction
Before testing programs, FIFUZZ ﬁrst performs a static anal-
ysis of their source code to ﬁrst identify error functions that
can fail, and then to identify error sites. We set R = 0.6 in
this analysis, and perform the third step of this analysis for
the source code of all the tested programs. After FIFUZZ
produces identiﬁed error sites, we manually select realistic
USENIX Association
29th USENIX Security Symposium    2601
  int *FuncA() {  int *p;  p = FuncB();  (*p)++;  return p;  }  int *FuncB() {  int *q;  q = malloc(...);  if (!q) {  return NULL;  }*q = 100;return q;  }int *FuncA() {+    FuncEntry(FuncA);  int *p;+ CallEntry(FuncB); p = FuncB();+ CallExit(FuncB); (*p)++;+ FuncExit(FuncA); return p;  }int *FuncB() {+    FuncEntry(FuncB);  int *q;+ErrorPointCollect(...);+if (ErrorPointFail(...) == TRUE)+ q = NULL;+else q = malloc(...);   // error site    if (!q) {+        FuncExit(FuncB);  return NULL;  } *q = 100;+ FuncExit(FuncB); return q;  }InstrumentProgram
vim
bison
ffmpeg
nasm
catdoc
clamav
cﬂow
gif2png+libpng
openssl
Total
Function call
67,768
11,861
459,986
10,246
1,293
52,830
4,049
11,209
158,625
777,867
Identiﬁed
1,589 (2.3%)
966 (8.1%)
2,157 (0.5%)
429 (4.2%)
103 (8.0%)
2,183 (4.1%)
149 (3.7%)
303 (2.7%)
1916 (1.2%)
9,795 (1.3%)
Realistic
283 (17.8%)
145 (15.0%)
190 (8.8%)
44 (10.3%)
45 (43.7%)
816 (37.4%)
88 (59.1%)
54 (17.8%)
157 (8.2%)
1,822 (18.6%)
Table 4: Results of error-site extraction.
ones that can actually fail and trigger error handling code, by
reading related source code. Table 4 shows the results. The
ﬁrst column presents the application name; the second column
presents the number of all function calls in the application;
the third column presents the number of error sites identiﬁed
by FIFUZZ; the last column presents the number of realistic
error sites that we manually select.
In total, FIFUZZ identiﬁes 287 error functions, and identi-
ﬁes 9,795 function calls to these error functions as possible
error sites. Among them, we manually select 150 error func-
tions as realistic ones, and 1,822 function calls to these error
functions that are considered as realistic error sites are auto-
matically extracted from the source code. Thus, the accuracy
rates of FIFUZZ for identifying realistic error functions and
error sites are 52.3% and 18.6%. The manual conﬁrmation is
easily manageable and not hard. The user only needs to scan
the deﬁnition of each error function, to check whether it can
trigger an error by returning an error number or a null pointer.
One master student spent only 2 hours on the manual selection
of error functions for the 9 tested applications. Considering
there are over 600K function calls in the tested programs, FI-
FUZZ is able to drop 99% of them, as they are considered not
to be fail and trigger error handling code according to their
contexts in source code. We ﬁnd that many of the selected er-
ror functions and error sites are related to memory allocation
that can indeed fail at runtime, and nearly half of the selected
error functions and error sites are related to occasional errors.
The results show that FIFUZZ can dramatically help reduce
the manual work of identifying realistic error sites.
step. Figure 9 shows the results. We ﬁnd that the number
of identiﬁed error functions and realistic error functions are
both decreased when R becomes larger. In this case, more
unrealistic error functions are dropped, but more realistic ones
are also missed. Thus, if R is too small, many unrealistic error
functions will be identiﬁed, which may introduce many false
positives in bug detection; if R is too large, many realistic
error functions will be missed, which may introduce many
false negatives in bug detection.
5.3 Runtime Testing
Using the 1,822 realistic error sites identiﬁed with R = 0.6, we
test the 9 target applications. We fuzz each application with a
well-know sanitizer ASan [4] and then without ASan (because
it often introduces much runtime overhead), for three times.
The time limit of each fuzzing is 24 hours. For the alerts
found by fault injection, we count them by trigger location
and error point (not error site). Table 5 shows the fuzzing
results with ASan and without ASan. The columns “Error
sequence” and “Input” show the results about generated error
sequences and inputs; in these columns, the columns “Gen”
show the number of generated ones, and the columns “Useful”
show the number of ones that increase code coverage. From
the results, we ﬁnd that:
Error sequence. FIFUZZ generates many useful error se-
quences for fault injection to cover error handling code. In
total, 3% and 2% of generated error sequences increase code
coverage by covering new code branches, with and without
ASan, respectively. These proportions are larger than those
(0.02% with ASan and 0.007% without ASan) for generated
program inputs. To know about the variation of useful error
sequences and program inputs increasing code coverage, we
select vim as an example to study. Figure 10 shows the results.
We ﬁnd that the number of useful error sequences increases
quickly during earlier tests, and then tends to be stable in the
later tests. This trend is quite similar to program inputs.
Figure 9: Variation of results affected by the value of R.
As described in Section 4.1, the value of R = 0.6 in the
static analysis heavily affects the identiﬁed error functions.
The above results are obtained with R = 0.6. To understand
the variation caused by R, we test R from 0.5 to 1 with 0.05
Figure 10: Variation of error sequences and inputs for vim.
2602    29th USENIX Security Symposium
USENIX Association
0501001502002503003500.50.550.60.650.70.750.80.850.90.951Identified error functionsRealistic error functionsValue of the threshold R Number of error functions (a) With ASan(b) Without ASanProgram
vim
bison
ffmpeg
nasm