time to produce a log of link utilization over selected time intervals 
(typically 100 milliseconds). 
3.1 Web-Like Traffic Generation 
The traffic that drives our experiments is based on a recent large-
scale analysis of web traffic [13]. The resulting model is an appli-
cation-level  description  of  the  critical  elements  that  characterize 
how  HTTP/1.0  and  HTTP/1.1  protocols  are  used.  It  is  based  on 
empirical data and is intended for use in generating synthetic Web 
workloads.  An  important  property  of  the  model  is  that  it  reflects 
the  use  of  persistent  HTTP  connections  as  implemented  in  many 
contemporary browsers and servers. Further, the analysis presented 
in  [13]  distinguishes  between  Web  objects  that  are  “top-level” 
(typically  an  HTML  file)  and  those  that  are  embedded  objects 
(e.g.,  an  image  file).  At  the  time  these  data  were  gathered,  ap-
proximately 15% of all TCP connections carrying HTTP protocols 
were effectively persistent (were used to request two or more ob-
jects) but more than 50% of all objects (40% of bytes) were trans-
ferred over these persistent connections.  
The  model  is  expressed  as  empirical  distributions  describing  the 
elements  necessary  to  generate  synthetic  HTTP  workloads.  The 
elements  of  the  model  that  have  the  most  pronounced  effects  on 
generated  traffic  are  summarized  in  Table  1.  Most  of  the  behav-
ioral  elements  of  Web  browsing  are  emulated  in  the  client-side 
request-generating program (the “browser”). Its primary parameter 
is the number of emulated browsing users (typically several hun-
dred to a few thousand). For each user to be emulated, the program 
implements a simple state machine that represents the user’s state 
as either “thinking” or requesting a web page. If requesting a web 
page, a request is made to the server-side portion of the program 
(executing  on  a  remote  machine)  for  the  primary  page.  Then  re-
quests  for  each  embedded  reference  are  sent  to  some  number  of 
servers (the number of servers and number of embedded references 
are drawn as random samples from the appropriate distributions). 
The  browser  also  determines  the  appropriate  usage  of  persistent 
and  non-persistent  connections;  15%  of  all  new  connections  are 
randomly selected to be persistent. Another random selection from 
the  distribution  of  requests  per  persistent  connection  is  used  to 
Table 1: Elements of the HTTP traffic model. 
Description 
HTTP request length in bytes 
HTTP reply length in bytes (top-level & embedded) 
Number of embedded (file) references per page 
Time between retrieval of two successive pages 
Element 
Request size 
Response size 
Page size 
Think time 
Persistent con-
nection use 
Servers per page  Number of unique servers used for all objects in a 
page 
Number of consecutive top-level pages requested 
Consecutive page 
retrievals 
from a given server 
Number of requests per persistent connection 
determine how many requests will use each persistent connection. 
One other parameter of the program is the number of parallel TCP 
connections allowed on behalf of each browsing user to make em-
bedded requests within a page. This parameter is used to mimic the 
parallel  connections  used  in  Netscape  (typically  4)  and  Internet 
Explorer (typically 2). 
For  each  request,  a  message  of  random  size  (sampled  from  the 
request size distribution) is sent over the network to an instance of 
the server program. This message specifies the number of bytes the 
server is to return as a response (a random sample from the distri-
bution of response sizes depending on whether it is a top-level or 
embedded  request).  The  server  sends  this  number  of  bytes  back 
through the network to the browser. The browser is responsible for 
closing the connection after the selected number of requests have 
completed (1 request for non-persistent connections and a random 
variable greater than 1 for persistent connections). For the experi-
ments reported here, the server’s “service time” is set to zero so the 
response begins as soon as the request message has been received 
and parsed. This very roughly models the behavior of a Web server 
or proxy having a large main-memory cache with a hit-ratio near 1. 
For  each  request/response  pair,  the  browser  program  logs  its  re-
sponse time. Response time is defined as the elapsed time between 
either  the  time  of  the  socket  connect()  operation  (for  a  non-
persistent connection) or the initial request (on a persistent connec-
tion) or the socket write() operation (for subsequent requests on a 
persistent connection) and the time the last byte of the response is 
returned. Note that this response time is for each element of a page, 
not the total time to load all elements of a page. 
When  all  the  request/response  pairs  for  a  page  have  been  com-
pleted,  the  emulated  browsing  user  enters  the  thinking  state  and 
makes no more requests for a random period of time sampled from 
the think-time distribution. The number of page requests the user 
makes in succession to a given server machine is sampled from the 
distribution  of  consecutive  page  requests.  When  that  number  of 
page  requests  has  been  completed,  the  next  server  to  handle  the 
next top-level request is selected randomly and uniformly from the 
set  of  active  servers.  The  number  of  emulated  users  is  constant 
throughout the execution of each experiment.  
3.2 Experiment Calibrations 
Offered load for our experiments is defined as the network traffic 
resulting  from  emulating  the  browsing  behavior  of  a  fixed-size 
population of web users. It is expressed as the long-term average 
throughput  (bits/second)  on  an  un-congested  link  that  would  be 
generated by that user population. There are three critical elements 
of  our  experimental  procedures  that  had  to  be  calibrated  before 
performing experiments:  
1.  Ensuring that no element on the end-to-end path represented a 
primary  bottleneck  other  than  the  links  connecting  the  two 
routers when they are limited to 100 Mbps,  
2.  The offered load on the network can be predictably controlled 
using the number of emulated users as a parameter to the traffic 
generators, and  
3.  Ensuring  that  the  resulting  packet  arrival  time-series  (e.g., 
packet counts per millisecond) is long-range dependent as ex-
pected  because  the  distribution  of  response  sizes  is  a  heavy-
tailed distribution [13].  
To  perform  these  calibrations,  we  first  configured  the  network 
connecting  the  routers  to  eliminate  congestion  by  running  at  1 
Gbps. All calibration experiments were run with drop-tail queues 
having 2,400 queue elements (the reasons for this choice are dis-
cussed in Section 4). We ran one instance of the browser program 
on  each  of  the  browser  machines  and  one  instance  of  the  server 
program on all the server machines. Each browser was configured 
to  emulate  the  same  number  of  active  users  and  the  total  active 
users varied from 7,000 to 35,000 over several experiments. Figure 
2 shows the aggregate traffic on one direction of the 1 Gbps link as 
a function of the number of emulated users. The load in the oppo-
site direction was  measured to be essentially the same and is not 
plotted in this figure. The offered load expressed as link through-
put is a linear function of the number of emulated users indicating 
there  are  no  fundamental  resource  limitations  in  the  system  and 
generated loads can easily exceed the capacity of a 100 Mbps link. 
With  these  data  we  can  determine  the  number  of  emulated  users 
that would generate a specific offered load if there were no bottle-
neck  link  present.  This  capability  is  used  in  subsequent  experi-
ments to control the offered loads on the network. For example, if 
we want to generate an offered load equal to the capacity of a 100 
Mbps link, we use Figure 2 to determine that we need to emulate 
approximately 19,040 users (9,520 on each side of the link). Note 
that for offered loads approaching saturation of the 100 Mbps link, 
the actual link utilization will, in general, be less than the intended 
offered  load.  This  is  because  as  response  times  become  longer, 
users  have  to  wait  longer  before  they  can  generate  new  requests 
and hence generate fewer requests per unit time.  
A motivation for using Web-like traffic in our experiments was the 
assumption that properly generated traffic would exhibit demands 
on the laboratory network consistent with those found in empirical 
studies  of  real  networks,  specifically,  a  long-range  dependent 
(LRD) packet arrival process. The empirical data used to generate 
our  web  traffic  showed  heavy-tailed  distributions  for  both  user 
“think”  times  and  response  sizes  [13].  (Figures  3-4  compare  the 
cumulative  distribution  function  (CDF),  F(x)  =  Pr[X  ≤  x],  and 
complementary cumulative distribution function (CCDF), 1 – F(x), 
of the generated responses in the calibration experiments with the 
empirical distribution from [13]. Note from Figure 3 that while the 
median  response  size  in  our  simulations  will  be  approximately 
1,000 bytes, responses as large as 109 bytes will also be generated.)  
That  our  web  traffic  showed  heavy-tailed  distributions  for  both 
think times (OFF times) and response size (ON times), implies that 
the  aggregate  traffic  generated  by  our  large  collection  of  sources 
should be LRD [14]. To verify that such LRD behavior is indeed 
realized with our experimental setup, we recorded tcpdumps of all 
TCP/IP headers  during  the  calibration  experiments  and derived  a 
time  series  of  the  number  of  packets  and  bytes  arriving  on  the 1 
Gbps link between the routers in 1 millisecond time intervals. We 
used  this  time  series  with  a  number  of  analysis  methods  (aggre-
gated variance, Whittle, Wavelets) to estimate the Hurst parameter. 
2e+08
1.8e+08
1.6e+08
1.4e+08
1.2e+08
1e+08
8e+07
6e+07
4e+07
)
s
p
b
(
t
u
p
h
g
u
o
r
h
t
k
n
L
i
2e+07
2000
Measured
10457.7012 * x + 423996
4000
6000
8000 10000 12000 14000 16000 18000
Browsers
Figure 2: Link throughput v. number of emulated browsing users 
compared to a straight line.  
y
t
i
l
i
b
a
b
o
r
p
e
v
i
t
l
a
u
m
u
C
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
Empirical distribution
Generated response sizes
10
100
1000 10000 100000 1e+06 1e+07 1e+08 1e+09
Figure 3: CDF of empirical v. generated response sizes.  
Response size (bytes)
y
t
i
l
i
l
b
a
b
o
r
p
e
v
i
t
a
u
m
u
c
y
r
a
t
n
e
m
e
p
m
o
C
l
1
0.1
0.01
0.001
0.0001
1e-05
1e-06
1e-07
1e-08
Empirical distribution
Generated response sizes
1
10
100
1000 100001000001e+06 1e+07 1e+08 1e+09
Response size (bytes)
Figure 4: CCDF of empirical v. generated response sizes. 
In all cases the 95% confidence intervals for the estimates fell in 
the range 0.8 to 0.9 which indicates a significant LRD component 
in the time series. 
3.3 Experimental Procedures 
Each  experiment  was  run  using  the  following  automated  proce-
dures. After initializing and configuring all router and end-system 
parameters,  the  server  programs  were  started  followed  by  the 
browser programs. Each browser program emulated an equal num-
ber of users chosen, as described above, to place a nominal offered 
load on  an unconstrained  network.  The  offered  loads  used  in  the 
experiments were chosen to represent user populations that could 
consume  80%,  90%,  98%,  or  105%  of  the  capacity  of  the  100 
Mbps link connecting the two router machines (i.e., consume 80, 
90,  98  or  105  Mbps,  respectively).  It  is  important  to  emphasize 
again that terms like “105% load” are used as a shorthand notation 
for  “a  population  of  web  users  that  would  generate  a  long-term 
average load of 105 Mbps on a 1 Gbps link.” Each experiment was 
run for 120 minutes to ensure very large samples (over 10,000,000 
request/response  exchanges  in  each  experiment)  but  data  were 
collected  only  during  a  90-minute  interval  to  eliminate  startup 
effects at the beginning and termination synchronization anomalies 
at  the  end.  Each  experiment  for  a  given  AQM  schemes  was  re-
peated three times with a different set of random number seeds for 
each  repetition.  To  facilitate  comparisons  among  different  AQM 
schemes,  experiments  for  different  schemes  were  run  with  the 
same sets of initial seeds for each random number generator (both 
those  in  the  traffic  generators  for  sampling  various  random  vari-
ables and in dummynet for sampling minimum per-flow delays).  
The key indicator of performance we use in reporting our results 
are  the  end-to-end  response  times  for  each  request/response  pair. 
We report these as plots of the cumulative distributions of response 
times  up  to  2  seconds.2  In  these  plots  we  show  only  the  results 
from one of the three repetitions for each experiment (usually there 
were  not  noticeable  differences  between  repetitions;  where  there 
were, we always selected the one experiment most favorable to the 
AQM scheme under consideration for these plots). We also report 
the  fraction  of  IP  datagrams  dropped  at  the  link  queues,  the  link 
utilization  on  the  bottleneck  link,  and  the  number  of  re-
quest/response  exchanges  completed  in  the  experiment.  These 
results are reported in Table 2 where the values shown are means 
over the three repetitions of an experiment. 
4  AQM EXPERIMENTS WITH PACKET DROPS 
For both PI and REM we chose two target queue lengths to evalu-
ate: 24 and 240 packets. These were chosen to provide two operat-
ing  points:  one  that  potentially  yields  minimum  latency  (24)  and 
one that potentially provides high link utilization (240). The values 
used for the coefficients in the control equations above are those 
recommended in [1, 8] and confirmed by the algorithm designers. 
For ARED we chose the same two target queue lengths to evaluate. 
The  calculations  for  all  the  ARED  parameter  settings  follow  the 
guidelines  given  in  [6]  for  achieving  the  desired  target  delay 
(queue size). In all three cases we set the maximum queue size to a 
number of packets sufficient to ensure tail drops do not occur.  
To establish a baseline for evaluating the effects of using various 
AQM  designs,  we  use  the  results  from  a  conventional  drop-tail 
FIFO  queue.  In  addition  to  baseline  results  for  drop-tail  at  the 
queue sizes 24 and 240 chosen for AQM, we also attempted to find 
a  queue  size  for  drop-tail  that  would  represent  a  “best  practice” 
choice. Guidelines (or “rules of thumb”) for determining the “best” 
allocations of queue size have been widely debated in various ven-
ues including the IRTF end2end-interest  mailing list. One  guide-
line that appears to have attracted a rough consensus is to provide 
buffering  approximately  equal  to  2-4  times  the  bandwidth-delay 
product of the link. Bandwidth in this expression is that of the link 
and the delay is the mean round-trip time for all connections shar-
ing  the  link  —  a  value  that  is,  in  general,  difficult  to  determine. 
Other mailing list contributors have recently tended to favor buff-
2 Because of space restrictions, only  plots of summary results are shown 
for105% load.  
ering  equivalent  to  100  milliseconds  at  the  link’s  transmission 
speed.  FreeBSD  queues  are  allocated  in  terms  of  a  number  of 
buffer elements (mbufs) each with capacity to hold an IP datagram 
of  Ethernet  MTU  size.  For  our  experimental  environment  where 
the link bandwidth is 100 Mbps and the mean frame size is a little 
over 500 bytes, this implies that a FIFO queue should have avail-
able about 2,400 mbufs for 100 milliseconds of buffering.  
Figures  5-7  give  the  response-time  performance  of  a  drop-tail 
queue with 24, 240 and 2,400 queue elements for offered loads of 
80%,  90%,  and  98%  compared  to  the  performance  on  the  un-
congested 1 Gbps link. Loss rates and link utilizations are given in 
Table 2. At 80% load (80 Mbps on a 100 Mbps link) the results for 
any  queue  size  are  indistinguishable  from  the  results  on  the  un-
congested link. At 90% load we see some significant degradation 
in response times for all queue sizes but note that, as expected, a 
queue size of 24 or 240 elements is superior for responses that are 
small  enough  to  complete  in  under  500  milliseconds.  For  the 
longer  queue  of  2,400  elements  performance  is  somewhat  better 
for the longer responses. At a load of 98% there is a severe per-
formance penalty to response times but, clearly, a shorter queue of 
240  elements  is  more  desirable  than  one  of  2,400  elements.  In 
Figure 7 we also see a feature that is found in all our results at high 
loads where there are significant numbers of dropped packets (see 
Table  2).  The  flat  area  in  the  curves  for  24  and 240  queue  sizes 