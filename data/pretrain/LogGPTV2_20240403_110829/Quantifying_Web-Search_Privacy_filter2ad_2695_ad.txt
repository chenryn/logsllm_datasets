U
f
o
n
o
i
t
c
a
r
F
e
v
i
t
a
u
m
u
C
l
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
1
USR
TMN
USR
TMN
1
0.2
0.8
Query Structure Privacy − False Positive
0.4
0.6
0.2
0.8
Query Structure Privacy − False Negative
0.4
0.6
s
r
e
s
U
f
o
n
o
i
t
c
a
r
F
e
v
i
t
l
a
u
m
u
C
s
r
e
s
U
f
o
n
o
i
t
c
a
r
F
e
v
i
t
a
u
m
u
C
l
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
USR
TMN
0.1
USR
TMN
0.1
0.02
0.04
0.06
0.08
Query Structure Relative Privacy − False Positive
0.02
0.04
0.06
0.08
Query Structure Relative Privacy − False Negative
Figure 2: Empirical CDF of the target user’s query structure privacy. The normalized false positive and false
negative errors reﬂect the privacy user as to what extent the reconstructed clusters by the attacker diﬀer
from that of the user.
the privacy oﬀered by the obfuscation mechanism after re-
moving the inherent randomness of the user. This is done in
the query structure relative privacy evaluation (right part
of ﬁgure 2). Here, still TMN oﬀers a better relative pri-
vacy than the USR obfuscation. Yet, the gap between them
constantly increases as we cover a higher fraction of users,
reﬂecting the superior eﬀectiveness of TMN versus USR re-
gardless of the user’s behavior.
Figure 3 shows the semantic privacy of users. We com-
pute a semantic proﬁle as the average over the topic weights
associated to the search result pages of the queries obtained
from the level 2 categories of ODP. This include categories
on “health, sports, news, business, shopping, recreation, sci-
ence, ...”. We then compute the distance between the se-
mantic proﬁle of SU with that of S1 and S2 separately. As
discussed in Section 4, S1 and S2 are retrieved from the
clustering attack on SO. We use the cosine distance as a
comparison metric between two semantic proﬁles. The pri-
vacy is 1 if the two proﬁles have no common topics, and is 0
if they have exactly the same weight on each of the topics.
The attacker will later label one of the two clusters as to be-
long to the target user. By taking the min privacy resulted
from these two clusters, we quantify the user’s privacy in
the worst-case scenario. In ﬁgure 3, we see that TMN oﬀers
a better overall privacy level than the USR obfuscation. On
the average-case however (where we compute the mean of
privacy with respect to cluster 1 and 2), the USR privacy
is better for about 90% of the users. This shows the ef-
fects of user-speciﬁc background knowledge of adversary on
the user’s privacy. If the adversary does not have any such
knowledge, any of the two clusters is not that superior to
the other one as being the cluster associated with the user.
However, if he has access to HU , he can ﬁnd the cluster
with higher similarity to the user’s web-search log history to
further break her privacy.
The USR based obfuscation uses fake queries which are
sampled from real users, yet it is the TMN based obfusca-
tion that oﬀers better privacy across diﬀerent metrics based
on the results above. This is due to the (e.g., temporal
and semantic) correlation between the fake queries gener-
ated using USR, across the whole query trace. In fact, one
should not confuse the problem of identifying whether a set
of queries are generated from a human or a machine (pass-
ing the Turing test) with the problem of linking together
the queries that are generated from a human. In this paper,
we are attacking the latter problem. The former problem
becomes solvable only if we have speciﬁc knowledge about
the behavior of the target user [8, 28], which is not our as-
sumption here.
Given these remarks, we could design obfuscation tech-
niques that take the best of both worlds. For example,
a hybrid TMN-USR based obfuscation scheme could ﬁrst
s
r
e
s
U
f
o
n
o
i
t
c
a
r
F
e
v
i
t
l
a
u
m
u
C
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
1
0.02
s
r
e
s
U
f
o
n
o
i
t
c
a
r
F
e
v
i
t
a
u
m
u
C
l
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
0.02
USR
TMN
0.16
0.18
USR
TMN
0.16
0.18
0.04
0.06
0.08
0.1
0.12
0.14
Semantic Privacy − Average Case
0.04
0.06
0.08
0.1
0.12
0.14
Semantic Privacy − Worst Case
Figure 3: Empirical CDF of the target users’ seman-
tic privacy. The privacy metric is the cosine dis-
tance between a target user’s true (weighted) topics
of interest and the clusters constructed through the
linkage attack.
take the queries from another real user, but then injects the
queries and simulate the click behavior for these queries in an
automated fashion, similar to what TMN does. This breaks
the overall correlation between the fake queries, and make
it more diﬃcult to split them from the user’s real queries.
6. RELATED WORK
A number of user-centric web search query obfuscation
methods have been proposed in the literature. Theses so-
lutions rely on the underlying idea of generating and inter-
leaving dummy queries together with the queries of the user.
The dummy queries are either sent as individual queries in-
terleaved with the user’s queries, or each user’s query is mod-
iﬁed by adding some dummy terms.
TrackMeNot (TMN) [1] consists of a Firefox plugin that
issues dummy queries from predeﬁned RSS feeds at random
intervals. GooPIR [15] is a standalone application which
can be used to issue queries to Google. Contrary to TMN,
a real user query is extended with dummy terms and issued
in a single search request. The search results are re-ranked
locally based on the original search query. PRivAcy model
for the Web (PRAW) [32] [17] builds an internal user pro-
ﬁle from queries and corresponding responses. PRAW aims
to issue queries which are not far from the actual user in-
terests. Plausibly Deniable Search (PDS) [26] aims at pro-
viding k-anonymity and puts an emphasis on the fact that
subsequent search queries should be related. When the user
issues a query, PDS searches for a synonym query in its in-
ternal datasets and replaces the real query with a similar
one. Optimized Query Forgery for Private Information Re-
trieval (OQF-PIR) [30] is designed to achieve perfect user
proﬁle obfuscation by making the user proﬁle equal to the
average population proﬁle. The diﬀerence between the ac-
tual proﬁle and the average population proﬁle is calculated
with the Kullback-Leiber divergence. Similar to OQF-PIR,
Noise Injection for Search Privacy Protection (NISPP) [37]
tries to ﬁnd the optimal dummy query distribution among
a ﬁnite number of categories. However, NISPP employs the
mutual information as a metric between the observed and
the real user proﬁle. There are also some mechanisms that
rely on third parties to protect user’s privacy, for example,
by enableing them to share queries among themselves [11].
There is no common quantitative framework and privacy
metric which oﬀer the possibility to comparing diﬀerent ob-
fuscation mechanisms. Some obfuscation solutions use in-
formation theoretic metrics to compare the observed proﬁle
and the real user proﬁle, while other obfuscation mechanisms
(such as TMN [21]) do not employ any metric. Few eval-