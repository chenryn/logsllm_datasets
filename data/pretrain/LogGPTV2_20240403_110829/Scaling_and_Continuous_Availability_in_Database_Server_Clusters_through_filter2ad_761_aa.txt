title:Scaling and Continuous Availability in Database Server Clusters through
Multiversion Replication
author:Kaloian Manassiev and
Cristiana Amza
Scaling and Continuous Availability in Database Server Clusters through
Multiversion Replication
Kaloian Manassiev
Cristiana Amza
Department of Computer Science
Department of Electrical and Computer Engineering
University of Toronto
PI:EMAIL
University of Toronto
PI:EMAIL
Abstract
In this paper, we study replication techniques for scal-
ing and continuous operation for a dynamic content server.
Our focus is on supporting transparent and fast reconﬁgu-
ration of its database tier in case of overload or failures.
We show that the data persistence aspects can be decoupled
from reconﬁguration of the database CPU. A lightweight
in-memory middleware tier captures the typically heavy-
weight read-only requests to ensure ﬂe xible database CPU
scaling and fail-over. At the same time, updates are handled
by an on-disk database back-end that is in charge of making
them persistent.
Our measurements show instantaneous, seamless recon-
ﬁguration in the case of single node failures within the ﬂe x-
ible in-memory tier for a web site running the most com-
mon, shopping, workload mix of the industry-standard e-
commerce TPC-W benchmark. At the same time, a 9-node
in-memory tier improves performance during normal oper-
ation over a stand-alone InnoDB on-disk database back-
end. Throughput scales by factors of 14.6, 17.6 and 6.5 for
the browsing, shopping and ordering mixes of the TPC-W
benchmark, respectively.
1. Introduction
This paper investigates replication techniques for high-
performance, self-conﬁguring database back-end tiers in
dynamic content servers. Dynamic content sites currently
need to provide very high levels of availability and scala-
bility. On-the-ﬂy reconﬁgurationmay be needed to either
adapt to failures or bursts of trafﬁc and should be auto-
matic, fast and transparent. The presence of the database
tier in such sites makes fast reconﬁgurationhard to achieve.
Above all, data consistency needs to be ensured during re-
conﬁguration, typically through complex and lengthy re-
covery procedures. The reconﬁguration problem is ex-
acerbated by the need to scale the database tier through
asynchronous content replication solutions [5, 10, 20, 13].
Replica asynchrony has been proved absolutely necessary
for scaling. On the other hand, because data is not fully
consistent on all replicas at all times, asynchronous replica-
tion is at odds with fast, transparent reconﬁguration.Asyn-
chronous replication techniques thus tend to sacriﬁcefailure
transparency and data availability to performance scaling by
introducing windows of vulnerability, where effects of com-
mitted transactions may be lost. Alternatively, complex fail-
ure reconﬁgurationprotocols imply reloading transactional
logs from disk and replaying them on a stale replica. Re-
suming servicing transaction at peak-throughput can take
on the order of minutes [12] and possibly more; fail-over
times are rarely formally measured and reported.
In this paper, we introduce a solution that combines
transparent scaling and split-second reconﬁguration. Our
key idea is to interpose an in memory tier, consisting of
lightweight database engines, providing scaling and seam-
less adaptation to failures on top of a traditional on-disk
database back-end. Our middleware tier implements Dy-
namic Multiversioning, a database replication solution al-
lowing both scaling and ease of self-reconﬁgurationof the
overall system. Speciﬁcally, our replication solution has the
following desirable properties:
1. provides consistency semantics identical to a 1-copy
database (i.e., 1-copy serializability), thus making the
underlying replication mechanism completely trans-
parent to the user.
2. scales by distributing reads to multiple replicas without
restricting concurrency at each replica in the common
case.
3. provides data availability through simple and efﬁcient
techniques for reconﬁgurationin case of failures.
In contrast to industry solutions which rely on costly shared
network-attached storage conﬁgurations [3], our solution
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:31:27 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007uses only commodity software and hardware. Our focus is
on achieving fast reconﬁgurationfor scaling and data avail-
ability in the common case of single node failures, while
ensuring data persistence in all cases.
An in-memory database tier with asynchronous, but
strongly consistent replication offers high speed and scal-
ability during normal operation and inherent agility in re-
conﬁgurationduring common failure scenarios. An on-disk
back-end database with limitted replication offers data reli-
ability for rare failure scenarios, e.g., a power outage. Our
research focus on in-memory database tiers is supported by
industry trends towards:
i) large main-memory sizes for
commodity servers, ii) the popularity of database workloads
with a high degree of locality [4], such as the most common
e-commerce workloads [22], which result in working sets
on the order of a few Gigabytes.
Our in-memory replication scheme is asynchronous in
order to provide scaling. We have previously shown that
the presence of distributed versions of the same page in
a transactional memory cluster with asynchronous replica-
tion can be exploited to support scaling for generic appli-
cations [16]. In this paper, our focus is on supporting both
scaling and fast reconﬁgurationfor an in-memory database
cluster. In our solution, the ﬁne-grainedconcurrency con-
trol of the database works synergistically with data repli-
cation to ensure high performance and ease of reconﬁgu-
ration. Update transactions always occur on an in-memory
master replica, which broadcasts modiﬁcations to a set of
in-memory slaves. Each master update creates a version
number, communicated to a scheduler that distributes re-
quests on the in-memory cluster. The scheduler tags each
read-only transaction with the newest version received from
the master and sends it to one of the slaves. The appropri-
ate version for each individual data item is then created dy-
namically and lazily at that slave replica, when needed by
an in-progress read-only transaction. The system automati-
cally detects data races created by different read-only trans-
actions attempting to read conﬂicting versions of the same
item. Conﬂicts and version consistency are detected and
enforced at the page level. In the common case, the sched-
uler sends any two transactions requiring different versions
of the same memory page on different replicas, where each
creates the page versions it needs and the two transactions
can execute in parallel.
We further concentrate on optimizing the fail-over recon-
ﬁgurationpath, deﬁnedas integrating a new replica (called
a backup) into the active computation to compensate for a
fault. The goal is to maintain a constant level of overall
performance irrespective of failures. We use two key tech-
niques for fail-over optimization. First, instead of replaying
a log on a stale replica, we replicate only the changed pages
with newer versions than the backup’s page versions from
an active slave onto the backup’s memory. These pages
may have collapsed long chains of modiﬁcationsto database
rows registering high update activity. Thus, selective page
replication is expected to be faster on average than modiﬁ-
cation log replay. Second, we warm up the buffer cache of
one or more spare backups during normal operation using
one of two alternative schemes: i) we schedule a small frac-
tion of the main read-only workload on a spare backup or
ii) we mimic the read access patterns of an active slave on
a spare backup to bring the most-heavily accessed data in
its buffer cache. With these key techniques, our in-memory
tier has the ﬂe xibility to incorporate a spare backup after a
fault without any noticeable impact on either throughput or
latency due to reconﬁguration.
Our in-memory replicated database implementation is
built from two existing libraries: the Vista library that pro-
vides very fast single-machine transactions [15], and the
MySQL in-memory “heap table” code that provides a very
simple and efﬁcientSQL database engine without transac-
tional properties. We use these codes as building blocks
for our fully transactional in-memory database tier because
they are reported to have reasonably good performance and
are widely available and used. Following this “softw are
components” philosophy has signiﬁcantlyreduced the cod-
ing effort involved.
In our evaluation we use the three workload mixes of
the industry standard TPC-W e-commerce benchmark [22].
The workload mixes have increasing fraction of update
transactions: browsing (5%), shopping (20%) and ordering
(50%).
We have implemented the TPC-W web site using three
popular open source software packages: the Apache Web
server [7], the PHP web-scripting/application development
language [19] and the MySQL database server [2]. In our
experiments we used MySQL with InnoDB tables as our
on-disk database back-ends and a set of up to 9 in-memory
databases running our modiﬁedversion of MySQL heap ta-
bles in our lightweight reconﬁgurabletier.
Our results are as follows:
1. Reconﬁgurationis instantaneous in case of failures of
any in-memory node with no difference in throughput
or latency due to fault handling if spare in-memory
backups are maintained warm. We found that either
servicing less than 1% of the read-only requests in a
regular workload at a spare backup or following an ac-
tive slave’s access pattern and touching its most fre-
quently used pages on the backup is sufﬁcientfor this
purpose. In contrast, with a traditional replication ap-
proach with MySQL InnoDB on-disk databases, fail-
over time is on the order of minutes.
2. Using our system with up to 9 in-memory replicas as
an in-memory transaction processing tier, we are able
to improve on the performance of the InnoDB on-disk
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:31:27 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007database back-end by factors of 14.6, 17.6 and 6.5
for the browsing, shopping and ordering mixes respec-
tively.
The rest of this paper is organized as follows. Section 2
introduces our scaling solution, based on Dynamic Multi-
versioning, Section 3 describes our prototype implemen-
tation and Section 4 presents its fault-tolerance and fast-
reconﬁgurationaspects. Sections 5 and 6 describe our ex-
perimental platform, methodology and results. Section 7
discusses related work. Section 8 concludes the paper.
2. Dynamic Multiversioning
2.1. Overview
The goal of Dynamic Multiversioning is to scale the
database tier through a distributed concurrency control
mechanism that integrates per-page ﬁne-grained concur-
rency control, consistent replication and version-aware
scheduling.
The idea of isolating the execution of conﬂicting update
and read-only transactions through multiversioning concur-
rency control is not new [8]. Existing stand-alone databases
supporting multiversioning (e.g., Oracle) pay the price of
maintaining multiple physical data copies for each database
item and garbage collecting old copies.
Instead, we take advantage of the availability of dis-
tributed replicas in a database cluster to run each read-only
transaction on a consistent snapshot created dynamically at
a particular replica for the pages in its read set. In addition,
we utilize the presence of update transactions with disjoint
working sets in order to enable non-conﬂicting update trans-
actions to run in parallel, thus exploiting the available hard-
ware optimally.
We augment a simple in-memory database with a repli-
cation module implementing a scheme that is i) eager by
propagating modiﬁcations from a set of master databases
that determines the serialization order to a set of slave
databases before the commit point, ii) lazy by delaying the
application of modiﬁcations on slave replicas and creat-
ing item versions on-demand as needed for each read-only
transaction.
In more detail, our ﬁne-graineddistributed multiversion-
ing scheme works as follows. A scheduler distributes re-
quests on the in-memory database cluster as shown in Fig-
ure 1. We require that each incoming request is preceded
by its access type, e.g.
read-only or update. The sched-
uler is pre-conﬁgured with the types of transactions used
by the application and the tables each of them accesses [5].
It uses this information to categorize the incoming requests
into conﬂict classes [18], based on the set of tables that they
access. The scheduler assigns a master database to each
C
o
n
n
e
c
t
i
o
n
s
(cid:13)
C
l
i
e
n
t
(cid:13)
Load Balancer(cid:13)
Web/Application Server(cid:13)
Web/Application Server(cid:13)
Web/Application Server(cid:13)
Web/Application Server(cid:13)
Apache + PHP(cid:13)
Apache + PHP(cid:13)
Apache + PHP(cid:13)
Apache + PHP(cid:13)
MySQL In-memory Tier(cid:13)
Scheduler(cid:13)
Master(cid:13)
Slave(cid:13)
Slave(cid:13)
Slave(cid:13)
Slave(cid:13)
MMAP On-disk Database(cid:13)
MMAP On-disk Database(cid:13)
On-disk(cid:13)
Database(cid:13)
Figure 1. System design.
conﬂict class. It sends all queries belonging to the update
transactions in each conﬂict class to the respective master
node. If no information on conﬂict classes is available or
if conﬂict classes cannot be statically determined, all up-
date transactions are scheduled on a single node designated
as master database. Read-only transactions are distributed
among the slave (non-master) database replicas as shown
in Figure 1. Read-only transactions can be scheduled on a
master replica as well as long as the set of tables they access
are not in the master’s conﬂict class. The master database
decides the order of execution of write transactions in each
conﬂict class it manages based on its internal two-phase-
locking per-page concurrency control. In our scheme, con-
ﬂict classes are disjoint. Hence, there is no need for inter-
master synchronization, which permits a fully parallel exe-
cution of updates.
Each update transaction committing on a master node
produces a new consistent state of the database. Each
database state is represented by a version vector with a sin-
gle integer entry for each table of the application, called
DBVersion. Upon transaction commit, each master
database ﬂushes its modiﬁcations to the remaining repli-
cas. The master communicates the most recent version vec-
tor produced locally to the scheduler when conﬁrmingthe
commit of each update transaction. The scheduler merges
incoming version vectors and it tags each read-only trans-
action with the version vector that it is supposed to read
(i.e., the most recent version produced by all of the masters)
and sends it to a slave replica. Each read-only transaction
applies all ﬁne-grainmodiﬁcationsreceived from a conﬂict-
class master, for each of the items it is accessing, thus dy-
namically creating a consistent snapshot of the database ver-
sion it is supposed to read.
Versions for each item are thus created dynamically
when needed by a read-only transaction in progress at a
particular replica. Speciﬁcally, the replica applies all local
ﬁne-grainedupdates received from a master on the neces-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:31:27 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007sary items up to and including the version vector that the
read-only transaction has been tagged with. Different read-
only transactions with disjoint read sets can thus run con-
currently at the same replica even if they require snapshots
of their items belonging to different database versions. Con-
versely, if two read-only transactions need two different ver-
sions of the same item(s), respectively they can only execute
in parallel if sent to different database replicas.
2.2. Version-Aware Scheduling
Dynamic Multiversioning guarantees that each read-only
transaction executing on a slave database reads the latest
data version as if running on a single database system. The
scheduler enforces the serialization order by tagging each
read-only transaction sent to execute on a database replica
with the latest version vector. The latest version vector con-
tains the most recent version communicated by the master
replicas on each respective table position.
The execution of read-only transactions is isolated from
any concurrent updates executing on the master replicas
whose write set intersects with the read set of the read-only
transactions. This means that a read-only transaction will
not apply and will not see modiﬁcationson items that were
written later than the version it was assigned.
Assuming that the read-only transaction has been tagged
with version V (v1; : : : ; vn) by the scheduler,
the slave
replica creates the appropriate version on-the-ﬂy for all
items read by the transaction. Speciﬁcally, the slave replica
applies all local ﬁne-grained updates received from each
master, only on the necessary items up to and including ver-
sion V (v1; : : : ; vn).
The scheduler selects a replica from the set of databases
running read-only transactions with the same version vector
as the one to be scheduled if such databases exist. Other-
wise, it selects a replica by plain load balancing. In case of
insufﬁcientreplicas, read-only transaction may need to wait
for other read-only transactions using a previous version of
an item, or for update transactions writing the item to ﬁn-
ish. A read-only transaction T1 may need to be aborted if
another read-only transaction T2 upgrades a shared item to
a version higher than that required by T1. If T1 has already
read a page with its assigned version, then reading a higher
version of a different page, would result in version inconsis-
tency. Since we do not keep old versions around, T1 would
need to be aborted in this case. However, we expect these
situations to be rare.
3. Implementation
Based on the standard MySQL HEAP table we have
added a separate table type called REPLICATED HEAP to
MySQL. Replication is implemented at the level of physical
WriteSet[] WS = CreateWriteSet(PS)