title:EnTrust: Regulating Sensor Access by Cooperating Programs via Delegation
Graphs
author:Giuseppe Petracca and
Yuqiong Sun and
Ahmad Atamli-Reineh and
Patrick D. McDaniel and
Jens Grossklags and
Trent Jaeger
EnTrusT: Regulating Sensor Access by 
Cooperating Programs via Delegation Graphs
Giuseppe Petracca, Pennsylvania State University, US; Yuqiong Sun, Symantec Research 
Labs, US; Ahmad-Atamli Reineh, Alan Turing Institute, UK; Patrick McDaniel, Pennsylvania 
State University, US; Jens Grossklags, Technical University of Munich, DE; Trent Jaeger, 
Pennsylvania State University, US
https://www.usenix.org/conference/usenixsecurity19/presentation/petracca
This paper is included in the Proceedings of the 28th USENIX Security Symposium.August 14–16, 2019 • Santa Clara, CA, USA978-1-939133-06-9Open access to the Proceedings of the 28th USENIX Security Symposium is sponsored by USENIX.EnTrust: Regulating Sensor Access by Cooperating Programs
via Delegation Graphs
Giuseppe Petracca
Penn State University, US
PI:EMAIL
Patrick McDaniel
Penn State University, US
PI:EMAIL
Yuqiong Sun
Symantec Research Labs, US
PI:EMAIL
Jens Grossklags
Technical University of Munich, DE
PI:EMAIL
Ahmad-Atamli Reineh
Alan Turing Institute, UK
PI:EMAIL
Trent Jaeger
Penn State University, US
PI:EMAIL
Abstract
Modern operating systems support a cooperating pro-
gram abstraction that, instead of placing all function-
ality into a single program, allows diverse programs to
cooperate to complete tasks requested by users. How-
ever, untrusted programs may exploit such interactions
to spy on users through device sensors by causing priv-
ileged system services to misuse their permissions, or
to forward user requests to malicious programs inadver-
tently. Researchers have previously explored methods
to restrict access to device sensors based on the state of
the user interface that elicited the user input or based
on the set of cooperating programs, but the former ap-
proach does not consider cooperating programs and the
latter approach has been found to be too restrictive for
many cases.
In this paper, we propose EnTrust, an
authorization system that tracks the processing of in-
put events across programs for eliciting approvals from
users for sensor operations. EnTrust constructs dele-
gation graphs by linking input events to cooperation
events among programs that lead to sensor operation
requests, then uses such delegation graphs for eliciting
authorization decisions from users. To demonstrate this
approach, we implement the EnTrust authorization sys-
tem for Android OS. In a laboratory study, we show that
attacks can be prevented at a much higher rate (47-67%
improvement) compared to the first-use approach. Our
field study reveals that EnTrust only requires a user
effort comparable to the first-use approach while incur-
ring negligible performance (<1% slowdown) and mem-
ory overheads (5.5 KB per program).
1 Introduction
Modern operating systems, such as Android OS, Ap-
ple iOS, Windows Phone OS, and Chrome OS, support
a programming abstraction that enables programs to
cooperate to perform user commands via input event
delegations.
Indeed, an emergent property of modern
operating systems is that system services are relatively
simple, provide a specific functionality, and often rely on
the cooperation with other programs to perform tasks.
For instance, modern operating systems now ship with
voice-controlled personal assistants that may enlist apps
and other system services to fulfill user requests, reach-
ing for a new horizon in human-computer interaction.
Unfortunately, system services are valuable targets
for adversaries because they often have more permis-
sions than normal apps. In particular, system services
are automatically granted access to device sensors, such
as the camera, microphone, and GPS. In one recent case
reported by Gizmodo [1], a ride-sharing app took ad-
vantage of Apple iOS system services to track riders.
In this incident, whenever users asked their voice assis-
tant “Siri, I need a ride”, the assistant enlisted the ride-
sharing app to process the request, which then leveraged
other system services to record the users’ device screens,
even while running in the background. Other online
magazines have reported cases of real-world evidence
that apps are maliciously colluding with one another to
collect and share users’ personal data [2, 3, 4].
Such attacks are caused by system services being
tricked into using their permissions on behalf of mali-
cious apps (confused deputy attacks [5, 6]), or malicious
apps exploiting their own privileges to steal data, and
a combination of the two. Researchers have previously
shown that such system services are prone to exploits
that leverage permissions only available to system ser-
vices [7]. Likewise, prior work has demonstrated that
system services inadvertently or purposely (for function-
ality reasons) depend on untrusted and possibly mali-
cious apps to help them complete tasks [8].
Such attacks are especially hard to prevent due to two
information asymmetries. System services are being ex-
ploited when performing tasks on behalf of users, where:
(1) users do not know what processing will result from
their requests and (2) services do not know what pro-
cessing users intended when making the request. Cur-
rent systems employ methods to ask users to authorize
program access to sensors, but to reduce users’ autho-
rization effort they only ask on a program’s first use of
that permission. However, once authorized, a program
can utilize that permission at will, enabling programs
USENIX Association
28th USENIX Security Symposium    567
to spy on users as described above. To prevent such
attacks, researchers have explored methods that bind
input events, including facets of the user interface used
to elicit those inputs, to permissions to perform sen-
sor operations [9, 10, 12]. Such methods ask users to
authorize permissions for those events and reuse those
permissions when the same event is performed to re-
duce the user burden. Recent research extends the col-
lection of program execution context (e.g., data flows
and/or GUI flows between windows) more comprehen-
sively to elicit user authorizations for sensitive opera-
tions [16, 11]. However, none of these methods addresses
the challenge where an input event is delivered to one
program and then a sensor operation, in response to
that event, is requested by another program in a series
of inter-process communications, a common occurrence
in modern operating systems supporting the cooperat-
ing program abstraction.
Researchers have also explored methods to prevent
unauthorized access by regulating inter-process commu-
nications (IPCs) and by reducing the permissions of pro-
grams that perform operations on behalf of other pro-
grams. First, prior work developed methods for block-
ing IPC communications that violate policies specified
by app developers [8, 18, 19, 21, 22]. However, such
methods may prevent programs from cooperating as ex-
pected. Decentralized information flow control [23, 24]
methods overcome this problem by allowing programs
with the authority to make security decisions and make
IPCs that may otherwise be blocked. Second, DIFC
methods, like capability-based systems in general [34],
enable reduction of a program’s permissions (i.e., callee)
when performing operations on behalf of other pro-
grams (i.e., callers). Initial proposals for reducing per-
missions simply intersected the parties’ permissions [7],
which however was too restrictive because parties would
have their permissions pruned after the interaction with
less privileged parties. DIFC methods, instead, provide
more flexibility [20], albeit with the added complex-
ity of requiring programs to make non-trivial security
decisions. Our insight to simplify the problem is that
while DIFC methods govern information flows compre-
hensively to prevent the leakage of sensitive data avail-
able to programs, users instead want to prevent pro-
grams from abusing sensor access to obtain sensitive
data in the first place.
In addition, prior work has also investigated the use
of machine learning classifiers to analyze the contextu-
ality behind user decisions to grant access to sensors
automatically [14, 15]. Unfortunately, the effectiveness
of the learning depends on the accuracy of the user de-
cisions while training the learner. Therefore, we firmly
believe that additional effort is necessary in improving
user decision making before the user decisions can be
used to train a classifier.
In this work, we propose the EnTrust authorization
system to prevent malicious programs from exploiting
cooperating system services to obtain unauthorized ac-
cess to device sensors. At a high-level, our insight is to
combine techniques that regulate IPC communications
of programs of different privilege levels with techniques
that enable users to be aware of the permissions asso-
ciated with an input event and decide whether to grant
such permissions for the identified flow context. The for-
mer techniques identify how a task is “delegated” among
cooperating programs to restrict the permissions of the
delegatee.1 The latter techniques expose more contex-
tual information to a user, which may be useful to make
effective authorization decisions.
However, combining these two research threads re-
sults in several challenges. First, we must be able to
associate input events with their resulting sensor oper-
ations in other programs to authorize such operations
relative to the input events and sequence of cooperating
programs. Prior work does not track how processing re-
sulting from input events is delegated across programs
[9, 10, 11, 12], but failing to do so results in attack
vectors exploitable by an adversary.
In EnTrust, we
construct delegation graphs that associate input events
with their resulting sensor operations across IPCs to
authorize operations in other programs.
Second, multiple, concurrent input events and IPCs
may create ambiguity in tracking delegations across pro-
cesses that must be resolved to ensure correct enforce-
ment. Prior work either makes assumptions that are
often too restrictive or require manual program annota-
tions to express such security decisions. EnTrust lever-
ages the insights that input events are relatively infre-
quent, processed much more quickly than users can gen-
erate distinct events, and are higher priority than other
processing. It uses these insights to ensure that an un-
ambiguous delegation path can be found connecting each
input event and sensor operation, if one exists, with lit-
tle impact on processing overhead.
Third, we must develop a method to determine the
permissions to be associated with an input event for
other programs that may perform sensor operations.
Past methods, including machine learning techniques
[14, 15], depend on user decision making to select the
permissions associated with input events, but we wonder
whether the information asymmetries arising from dele-
gation of requests across programs impair user decision
making. In EnTrust, we elicit authorization decisions
from users by using delegation paths. We study the im-
pact of using delegation paths on users’ decision making
for both primed and unprimed user groups. Historically,
there has been a debate on whether users should be con-
sidered a weak link in security [56, 57]. We examine this
argument in a specific context by investigating if users
can make informed security decisions given informative,
yet precise, contextual information.
We implement and evaluate a prototype of the
EnTrust authorization system for Android OS. We find
that EnTrust significantly reduces exploits from three
568    28th USENIX Security Symposium
USENIX Association
Figure 1: Possible attack vectors when diverse programs interact via input event delegations in a cooperating model. For
consistency, we present the attack scenarios in terms of voice assistants receiving input events via voice commands; however,
similar attack scenarios are possible for input events received by programs via Graphical User Interface (GUI) widgets rendered
on the users’ device screen.
canonical types of attack vectors possible in systems
supporting cooperating programs, requires little addi-
tional user effort, and has low overhead in app perfor-
mance and memory consumption. In a laboratory study
involving 60 human subjects, EnTrust improves attack
detection by 47-67% when compared to the first-use au-
thorization approach. In a field study involving 9 human
subjects, we found that - in the worst scenarios seen -
programs required no more than four additional manual
authorizations from users, compared to the less secure
first-use authorization approach; which is far below the
threshold that is considered at risk for user annoyance
and habituation [33]. Lastly, we measured the over-
head imposed by EnTrust via benchmarks and found
that programs operate effectively under EnTrust, while
incurring a negligible performance overhead (<1% slow-
down) and a memory footprint of only 5.5 kilobytes, on
average, per program.
In summary, we make the following contributions:
• We propose a method for authorizing sensor opera-
tions in response to input events performed by co-
operating programs by building unambiguous del-
egation graphs. We track IPCs that delegate task
processing to other programs without requiring sys-
tem service or app code modifications.
• We propose EnTrust, an authorization system that
generates delegation paths to enable users to autho-
rize sensor operations, resulting from input events,
and reuse such authorizations for repeated requests.
• We implement the EnTrust prototype and test its
effectiveness with a laboratory study, the users’ au-
thorization effort with a field study, and perfor-
mance and memory overhead via benchmarks.
2 Problem Statement
In current operating systems, users interact with pro-
grams that initiate actions targeting sensors, but users
do not have control over which programs are going to
service their requests, or how such programs access sen-
sors while servicing such requests. Unfortunately, three
well-studied attack vectors become critical in operating
systems supporting a cooperating program abstraction.
Confused Deputy — First, a malicious program
may leverage an input event as an opportunity to con-
fuse a more privileged program into performing a sen-
sitive operation. For example, a malicious voice assis-
tant may invoke the screen capture service at each voice
command (left side of Figure 1). The malicious voice
assistant may therefore succeed in tricking the screen
capture service into capturing and inadvertently leaking
sensitive information (e.g., a credit card number written
down in a note). In this scenario, the user only sees the
new note created by the notes app, whereas the screen
capture goes unnoticed. Currently, there are over 250
voice assistants available to the public on Google Play
with over 1 million installs, many by little known or
unknown developers.
Trojan Horse — Second, a program trusted by the
user may delegate the processing of an input event to an
untrusted program able to perform the requested task.
For example, a trusted voice assistant may activate a
camera app to serve the user request to take a selfie
(middle of Figure 1). However, the camera app may
be a Trojan horse app that takes a picture, but also
records a short audio via the microphone, and the user
location via GPS (e.g., a spy app2 installed by a jealous
boyfriend stalking on his girlfriend). Researchers re-
ported over 3,500 apps available on Google Play Store
that may be used as spyware apps for Intimate Partner
Violence (IPV) [25]. In this scenario, the user only sees
the picture being taken by the camera app, whereas the
voice and location recordings go unnoticed, since a cam-
era app is likely to be granted such permission. Also,
the ride-sharing attack in the introduction is another
example of this attack. Such attacks are possible be-
cause even trusted system services may inadvertently
leverage malicious apps and/or rely on unknown apps
by using implicit intents. An implicit intent enables any
program registered to receive such intents to respond to
IPCs when such intents are invoked. Researchers have
reported several ways how programs can steal or spoof
intents intended for other programs [26, 27, 28]. We
performed an analysis of system services and applica-
tions distributed via the Android Open Source Project
(AOSP), and found that 10 system programs out of a
total of 69 (14%) use implicit intents.
Man-In-The-Middle — Third, a request generated
by a program trusted by the user may be intercepted
by a malicious program, which can behave as a man-in-
the-middle in serving the input event in the attempt to
obtain access to unauthorized data (right side of Fig-
ure 1). For example, a legitimate banking app may
adopt the voice interaction intent mechanism to allow
USENIX Association
28th USENIX Security Symposium    569
customers to direct deposit a check via voice assistant
with a simple voice command (e.g., “deposit check”).3
A malicious program may exploit such a service by reg-
istering itself with a voice assistant as able to service a
similar voice interaction, such as “deposit bank check.”
Therefore, whenever the user instantiates the “deposit
bank check” voice command, although the user expects
the legitimate banking app to be activated, the mali-
cious app is activated instead. The malicious app opens
the camera, captures a frame with the check, and sends
a spoofed intent to launch the legitimate banking app,
all while running in the background. In this scenario,
the user only sees the trusted banking app opening a
camera preview to take a picture of the check. This is a
realistic threat. We performed an analysis of 1,000 apps
(among the top 2,000 most downloaded apps on Google
Play Store) and found that 227 apps (23%) export at
least a public service or a voice interaction intent. Apps
were selected from the Google Play Store among those
apps declaring at least one permission to access a sen-
sitive sensor (e.g., camera, microphone, or GPS).