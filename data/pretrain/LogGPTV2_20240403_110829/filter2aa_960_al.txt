hood that your attack against an organization succeeds. Information such 
as employee names, phone numbers, email addresses, and client software 
versions are often the most highly regarded because they provide concrete 
前沿信安资讯阵地  公众号：i nf osrc
HTTP Clients and Remote Interaction with Tools    69
or actionable information that attackers can directly exploit or use to craft 
attacks that are more effective and highly targeted. One such source of 
information, popularized by a tool named FOCA, is document metadata. 
Applications store arbitrary information within the structure of a file 
saved to disk. In some cases, this can include geographical coordinates, 
application versions, operating system information, and usernames. Better 
yet, search engines contain advanced query filters that allow you to retrieve 
specific files for an organization. The remainder of this chapter focuses on 
building a tool that scrapes—or as my lawyer calls it, indexes—Bing search 
results to retrieve a target organization’s Microsoft Office documents, sub-
sequently extracting relevant metadata.
Setting Up the Environment and Planning
Before diving into the specifics, we’ll start by stating the objectives. First, you’ll 
focus solely on Office Open XML documents—those ending in xlsx, docx, pptx, 
and so on. Although you could certainly include legacy Office data types, the 
binary formats make them exponentially more complicated, increasing code 
complexity and reducing readability. The same can be said for working with 
PDF files. Also, the code you develop won’t handle Bing pagination, instead 
only parsing initial page search results. We encourage you to build this into 
your working example and explore file types beyond Open XML.
Why not just use the Bing Search APIs for building this, rather than 
doing HTML scraping? Because you already know how to build clients 
that interact with structured APIs. There are practical use cases for scrap-
ing HTML pages, particularly when no API exists. Rather than rehashing 
what you already know, we’ll take this as an opportunity to introduce a new 
method of extracting data. You’ll use an excellent package, goquery, which 
mimics the functionality of jQuery, a JavaScript library that includes an 
intuitive syntax to traverse HTML documents and select data within. Start 
by installing goquery:
$ go get github.com/PuerkitoBio/goquery
Fortunately, that’s the only prerequisite software needed to complete 
the development. You’ll use standard Go packages to interact with Open 
XML files. These files, despite their file type suffix, are ZIP archives that, 
when extracted, contain XML files. The metadata is stored in two files 
within the docProps directory of the archive:
$ unzip test.xlsx
$ tree
--snip--
|---docProps
|   |---app.xml
|   |---core.xml
--snip—
前沿信安资讯阵地  公众号：i nf osrc
70   Chapter 3
The core.xml file contains the author information as well as modification 
details. It’s structured as follows:
    Dan Kottmannu
    Dan Kottmannv
    2016-12-06T18:24:42Z
    2016-12-06T18:25:32Z
The creator u and lastModifiedBy v elements are of primary interest. 
These fields contain employee or usernames that you can use in a social-
engineering or password-guessing campaign. 
The app.xml file contains details about the application type and version 
used to create the Open XML document. Here’s its structure:
    Microsoft Excelu
    0
    false
                Worksheets
                1
            Sheet1
    ACMEv
    false
    false
    false
    15.0300w
前沿信安资讯阵地  公众号：i nf osrc
HTTP Clients and Remote Interaction with Tools    71
You’re primarily interested in just a few of those elements: Application u, 
Company v, and AppVersion w. The version itself doesn’t obviously correlate to 
the Office version name, such as Office 2013, Office 2016, and so on, but a 
logical mapping does exist between that field and the more readable, com-
monly known alternative. The code you develop will maintain this mapping. 
Defining the metadata Package
In Listing 3-20, define the Go types that correspond to these XML datasets 
in a new package named metadata and put the code in a file named openxml 
.go—one type for each XML file you wish to parse. Then add a data map-
ping and convenience function for determining the recognizable Office 
version that corresponds to the AppVersion.
type OfficeCoreProperty struct {
    XMLName        xml.Name `xml:"coreProperties"`
    Creator        string   `xml:"creator"`
    LastModifiedBy string   `xml:"lastModifiedBy"`
}
type OfficeAppProperty struct {
    XMLName     xml.Name `xml:"Properties"`
    Application string   `xml:"Application"`
    Company     string   `xml:"Company"`
    Version     string   `xml:"AppVersion"`
}
var OfficeVersionsu = map[string]string{
    "16": "2016",
    "15": "2013",
    "14": "2010",
    "12": "2007",
    "11": "2003",
}
func (a *OfficeAppProperty) GetMajorVersion()v string {
    tokens := strings.Split(a.Version, ".")w
    if len(tokens) < 2 {
        return "Unknown"
    }
    v, ok := OfficeVersionsx [tokens[0]]
    if !ok {
        return "Unknown"
    }
    return v
}
Listing 3-20: Open XML type definition and version mapping (/ch-3/bing-metadata 
/metadata/openxml.go)
前沿信安资讯阵地  公众号：i nf osrc
72   Chapter 3
After you define the OfficeCoreProperty and OfficeAppProperty types, 
define a map, OfficeVersions, that maintains a relationship of major version 
numbers to recognizable release years u. To use this map, define a method, 
GetMajorVersion(), on the OfficeAppProperty type v. The method splits the XML 
data’s AppVersion value to retrieve the major version number w, subsequently 
using that value and the OfficeVersions map to retrieve the release year x.
Mapping the Data to Structs
Now that you’ve built the logic and types to work with and inspect the XML 
data of interest, you can create the code that reads the appropriate files and 
assigns the contents to your structs. To do this, define NewProperties() and 
process() functions, as shown in Listing 3-21.
func NewProperties(r *zip.Reader) (*OfficeCoreProperty, *OfficeAppProperty, error) {u
    var coreProps OfficeCoreProperty
    var appProps OfficeAppProperty
    for _, f := range r.File {v
        switch f.Name {w
        case "docProps/core.xml":
            if err := process(f, &coreProps)x; err != nil {
                return nil, nil, err
            }
        case "docProps/app.xml":
            if err := process(f, &appProps)y; err != nil {
                return nil, nil, err
            }
        default:
            continue
        }
    }
    return &coreProps, &appProps, nil
}
func process(f *zip.File, prop interface{}) error {z
    rc, err := f.Open()
    if err != nil {
        return err
    }
    defer rc.Close()
    if err := {xml.NewDecoder(rc).Decode(&prop); err != nil {
        return err
    }
    return nil
}
Listing 3-21: Processing Open XML archives and embedded XML documents (/ch-3/bing-metadata 
/metadata/openxml.go)
前沿信安资讯阵地  公众号：i nf osrc
HTTP Clients and Remote Interaction with Tools    73
The NewProperties() function accepts a *zip.Reader, which represents an 
io.Reader for ZIP archives u. Using the zip.Reader instance, iterate through 
all the files in the archive v, checking the filenames w. If a filename matches 
one of the two property filenames, call the process() function xy, passing 
in the file and the arbitrary structure type you wish to populate—either 
OfficeCoreProperty or OfficeAppProperty.
The process() function accepts two parameters: a *zip.File and an 
interface{} z. Similar to the Metasploit tool you developed, this code 
accepts a generic interface{} type to allow for the file contents to be 
assigned into any data type. This increases code reuse because there’s 
nothing type-specific within the process() function. Within the function, 
the code reads the contents of the file and unmarshals the XML data 
into the struct {.
Searching and Receiving Files with Bing
You now have all the code necessary to open, read, parse, and extract Office 
Open XML documents, and you know what you need to do with the file. 
Now, you need to figure out how to search for and retrieve files by using 
Bing. Here’s the plan of action you should follow:
1. Submit a search request to Bing with proper filters to retrieve 
targeted results.
2. Scrape the HTML response, extracting the HREF (link) data to 
obtain direct URLs for documents.
3. Submit an HTTP request for each direct document URL 
4. Parse the response body to create a zip.Reader.
5. Pass the zip.Reader into the code you already developed to extract 
metadata.
The following sections discuss each of these steps in order. 
The first order of business is to build a search query template. Much like 
Google, Bing contains advanced query parameters that you can use to filter 
search results on numerous variables. Most of these filters are submitted in a 
filter_type: value format. Without explaining all the available filter types, 
let’s instead focus on what helps you achieve your goal. The following list 
contains the three filters you’ll need. Note that you could use additional 
filters, but at the time of this writing, they behave somewhat unpredictably.
site Used to filter the results to a specific domain
filetype Used to filter the results based off resource file type
instreamset Used to filter the results to include only certain file 
extensions
An example query to retrieve docx files from nytimes.com would look 
like this:
site:nytimes.com && filetype:docx && instreamset:(url title):docx
前沿信安资讯阵地  公众号：i nf osrc
74   Chapter 3
After submitting that query, take a peek at the resulting URL in 
your browser. It should resemble Figure 3-1. Additional parameters may 
appear after this, but they’re inconsequential for this example, so you can 
ignore them. 
Now that you know the URL and parameter format, you can see the 
HTML response, but first you need to determine where in the Document 
Object Model (DOM) the document links reside. You can do this by viewing 
the source code directly, or limit the guesswork and just use your browser’s 
developer tools. The following image shows the full HTML element path to 
the desired HREF. You can use the element inspector, as in Figure 3-1, to 
quickly select the link to reveal its full path.
Figure 3-1: A browser developer tool showing the full element path
With that path information, you can use goquery to systematically pull 
all data elements that match an HTML path. Enough talk! Listing 3-22 puts 
it all together: retrieving, scraping, parsing, and extracting. Save this code 
to main.go.
u func handler(i int, s *goquery.Selection) {
    url, ok := s.Find("a").Attr("href")v
    if !ok {
        return
    }
    fmt.Printf("%d: %s\n", i, url)
    res, err := http.Get(url)w
    if err != nil {
        return
    }
前沿信安资讯阵地  公众号：i nf osrc
HTTP Clients and Remote Interaction with Tools    75
    buf, err := ioutil.ReadAll(res.Body)x
    if err != nil {
        return
    }
    defer res.Body.Close()
    r, err := zip.NewReader(bytes.NewReader(buf)y, int64(len(buf)))
    if err != nil {
        return
    }
    cp, ap, err := metadata.NewProperties(r)z
    if err != nil {
        return
    }
    log.Printf(
        "%25s %25s - %s %s\n",
        cp.Creator,
        cp.LastModifiedBy,
        ap.Application,
        ap.GetMajorVersion())
}
func main() {
    if len(os.Args) != 3 {
        log.Fatalln("Missing required argument. Usage: main.go domain ext")
    }
    domain := os.Args[1]
    filetype := os.Args[2]
    { q := fmt.Sprintf(
        "site:%s && filetype:%s && instreamset:(url title):%s",
        domain,
        filetype,
        filetype)
    | search := fmt.Sprintf("http://www.bing.com/search?q=%s", url.QueryEscape(q))
    doc, err := goquery.NewDocument(search)}
    if err != nil {
        log.Panicln(err)
    }
    s := "html body div#b_content ol#b_results li.b_algo div.b_title h2"
    ~ doc.Find(s).Each(handler)
}
Listing 3-22: Scraping Bing results and parsing document metadata (/ch-3/bing-metadata 
/client/main.go)
You create two functions. The first, handler(), accepts a goquery.Selection 
instance u (in this case, it will be populated with an anchor HTML element) 
and finds and extracts the href attribute v. This attribute contains a direct 
link to the document returned from the Bing search. Using that URL, the 
code then issues a GET request to retrieve the document w. Assuming no 
前沿信安资讯阵地  公众号：i nf osrc
76   Chapter 3
errors occur, you then read the response body x, leveraging it to create a 
zip.Reader y. Recall that the function you created earlier in your metadata 
package, NewProperties(), expects a zip.Reader. Now that you have the appro-
priate data type, pass it to that function z, and properties are populated 
from the file and printed to your screen.
The main() function bootstraps and controls the whole process; you 
pass it the domain and file type as command line arguments. The func-
tion then uses this input data to build the Bing query with the appropri-
ate filters {. The filter string is encoded and used to build the full Bing 
search URL |. The search request is sent using the goquery.NewDocument() 
function, which implicitly makes an HTTP GET request and returns a 
goquery-friendly representation of the HTML response document }. This 
document can be inspected with goquery. Finally, use the HTML element 
selector string you identified with your browser developer tools to find 
and iterate over matching HTML elements ~. For each matching element, 
a call is made to your handler() function. 
A sample run of the code produces output similar to the following:
$ go run main.go nytimes.com docx
0: http://graphics8.nytimes.com/packages/pdf/2012NAIHSAnnualHIVReport041713.docx
2020/12/21 11:53:50     Jonathan V. Iralu     Dan Frosch - Microsoft Macintosh Word 2010
1: http://www.nytimes.com/packages/pdf/business/Announcement.docx