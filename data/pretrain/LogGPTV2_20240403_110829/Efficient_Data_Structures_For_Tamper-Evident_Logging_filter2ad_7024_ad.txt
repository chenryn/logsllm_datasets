Together these form the node data that is attached to each
node in the history tree. Note that the hash label of node,
i,r.H, does not ﬁx its own attributes, Av
Av
i,r.A. Instead, we
deﬁne a subtree authenticator Av
i,r.A)
that ﬁxes the attributes and hash of a node, and recursively
ﬁxes every hash and attribute in its subtree. Frozen hashes
FHi,r.A and FHi,r.H and FHi,r.∗ are deﬁned analogously
to the non-Merkle-aggregation case.
i,r.∗ = H(Av
i,r.H k Av
We could have deﬁned this recursion in several differ-
ent ways. This representation allows us to elide unwanted
subtrees with a small stub, containing one hash and one
set of attributes, while exposing the attributes in a way
that makes it possible to locally detect if the attributes
were improperly aggregated.
Our new mechanism for computing hash and aggre-
gates for a node is given in equations (5)-(10) in Figure 8.
There is a strong correspondence between this recurrence
and the previous one in Figure 5. Equations (6) and (7)
extract the hash and attributes of an event, analogous
to equation (1). Equation (9) handles aggregation of
attributes between a node and its children. Equation (8)
computes the hash of a node in terms of the subtree
authenticators of its children.
INCR.GEN and MEMBERSHIP.GEN operate the same
as with an ordinary history tree, except that wherever
a frozen hash was included in the proof (FHi,r), we
now include both the hash of the node, FHi,r.H, and its
attributes FHi,r.A. Both are required for recomputing
Av
i,r.A and Av
i,r.H for the parent node. ADD, INCR.VF,
t
G
b
b
Av
i,r.∗ = H(Av
Av
i,r.H k Av
i,0.H =nH(0 k Xi)
i,0.A =nG (Xi)
i,r.H =(H(1 k Av
H(1 k Av
Av
Av
if v ≥ i
i,r.A)
if v ≥ i
i,r−1.∗ k (cid:3))
i,r−1.∗ k Av
i+2r−1,r−1.∗)
(5)
(6)
(7)
if v < i + 2r−1
if v ≥ i + 2r−1
(8)
Av
i,r.A =(Av
i,r−1.A
Av
i,r−1.A ⊕ Av
Cn = An
0,d.∗
if v < i + 2r−1
i+2r−1,r−1.A if v ≥ i + 2r−1 (9)
(10)
Figure 8: Hash computations for Merkle aggregation
and MEMBERSHIP.VF are the same as before except for
using the equations (5)-(10) for computing hashes and
propagating attributes. Merkle aggregation inﬂates the
storage and proof sizes by a factor of (A + B)/A where A
is the size of a hash and B is the size of the attributes.
4.2.1 Queries over attributes
In Merkle aggregation queries, we permit query results
to contain false positives, i.e., events that do not match
the query Q. Extra false positive events in the result only
impact performance, not correctness, as they may be
ﬁltered by the auditor. We forbid false negatives; every
event matching Q will be included in the result.
(x ⊕ y) and ∀x QG
(G (x))). Furthermore, if QG
Unfortunately, Merkle aggregation queries can only
match attributes, not events. Consequently, we must
conservatively transform a query Q over events into a
predicate QG over attributes and require that it be stable,
with the following properties: If Q matches an event then
QG matches the attributes of that event (i.e., ∀x Q(x) ⇒
QG
is true for either child of a
node, it must be true for the node itself (i.e., ∀x,y QG
(x) ∨
QG
(y) ⇒ QG
(x ⊕ (cid:3))).
Stable predicates can falsely match nodes or events for
two reasons: events’ attributes may match QG without
the events matching Q, or nodes may occur where
(QG
(x ⊕ y) is true. We call
a predicate Q exact if there can be no false matches. This
occurs when Q(x) ⇔ QG
(y) ⇔
QG
(x ⊕ y). Exact queries are more efﬁcient because a
query result does not include falsely matching events and
the corresponding pruned tree proving the correctness of
the query result does not require extra nodes.
(y)) is false, but QG
(G (x)) and QG
((cid:3)) ⇒ QG
(x) ∨ QG
(x) ∨ QG
(x) ∨QG
Given these properties, we can now deﬁne the addi-
tional operations for performing authenticated queries on
the log for events matching a predicate QG .
H.QUERY(Cj,QG
over
attributes t , returns a pruned tree where every elided
) → P Given a predicate QG
subtrees does not match QG .
j,QG
P.QUERY.VF(C′
) → {⊤, ⊥} Checks the pruned tree
P and returns ⊤ if every stub in P does not match QG
and the reconstructed commitment Cj is the same as C′
j.
Building a pruned tree containing all events matching
a predicate QG
is similar to building the pruned trees
for membership or incremental auditing. The logger
starts with a proof skeleton then recursively traverses
it, splitting interior nodes when QG
(FHi,r.A) is true.
Because the predicate QG
is stable, no event in any elided
subtree can match the predicate.
If there are t events
matching the predicate QG , the pruned tree is of size at
most O((1 + t)log2 n) (i.e., t leaves with log2 n interior
tree nodes on the paths to the root).
To verify that P includes all events matching QG , the
auditor does a recursive traversal over P. If the auditor
ﬁnds an interior stub where QG
(FHi,r.A) is true, the ver-
iﬁcation fails because the auditor found a node that was
supposed to have been split. (Unfrozen nodes will always
be split as they compose the proof skeleton and only occur
on the path from X j to the root.) The auditor must also
verify that pruned tree P commits the same events as the
commitment C′
j by reconstructing the root commitment
Cj using the equations (5)-(10) and checking that Cj = C′
j.
As with an ordinary history tree, a Merkle aggregating
tree requires auditing for tamper-detection. If an event is
never audited, then there is no guarantee that its attributes
have been properly included. Also, a dishonest logger
or client could deliberately insert false log entries whose
attributes are aggregated up the tree to the root, causing
garbage results to be included in queries. Even so, if Q
is stable, a malicious logger cannot hide matching events
from query results without detection.
4.3 Applications
Safe deletion Merkle aggregation can be used for
expiring old and obsolete events that do not satisfy some
predicate and prove that no other events were deleted
inappropriately. While Merkle aggregation queries prove
that no matching event is excluded from a query result,
safe deletion requires the contrapositive: proving to an
auditor that each purged event was legitimately purged
because it did not match the predicate.
Let Q(x) be a stable query that is true for all events that
the logger must keep. Let QG
(x) be the corresponding
predicate over attributes. The logger stores a pruned tree
that includes all nodes and leaf events where QG
(x) is
true. The remaining nodes may be elided and replaced
with stubs. When a logger cannot generate a path to a
previously deleted event Xi, it instead supplies a pruned
tree that includes a path to an ancestor node A of Xi where
QG
(A) is false,
then QG
(A) is false. Because Q is stable, if QG
(G (Xi)) and Q(Xi) must also be false.
Safe deletion and auditing policies must
take into
account that if a subtree containing events Xi . . . X j is
purged, the logger is unable to generate incremental or
membership proofs involving commitments Ci . . .Cj. The
auditing policy must require that any audits using those
commitments be performed before the corresponding
events are deleted, which may be as simple as requiring
that clients periodically request an incremental proof to a
later or long-lived commitment.
Safe deletion will not save space when using the
append-only storage described in Section 3.3. However,
if data-destruction policies require destroying a subset of
events in the log, safe deletion may be used to prove that
no unauthorized log events were destroyed.
“Private” search Merkle aggregation enables a weak
variant of private information retrieval [14], permitting
clients to have privacy for the speciﬁc contents of their
events. To aggregate the attributes of an event, the logger
only needs the attributes of an event, G (Xi), not the event
itself. To verify that aggregation is done correctly also
only requires the attributes of an event. If clients encrypt
their events and digitally sign their public attributes,
auditors may verify that aggregation is done correctly
while clients preserve their event privacy from the logger
and other clients and auditors.
Bloom ﬁlters, in addition to providing a compact and
approximate way to represent the presence or absence
of a large number of keywords, can also enable private
indexing (see, e.g., Goh [23]). The logger has no idea
what
the individual keywords are within the Bloom
ﬁlter; many keywords could map to the same bit. This
allows for private keywords that are still protected by the
integrity mechanisms of the tree.
5 Syslog prototype implementation
Syslog is the standard Unix-based logging system [38],
storing events with many attributes. To demonstrate the
effectiveness of our history tree, we built an implementa-
tion capable of storing and searching syslog events. Using
events from syslog traces, captured from our departmental
servers, we evaluated the storage and performance costs
of tamper-evident logging and secure deletion.
Each syslog event includes a timestamp, the host gener-
ating the event, one of 24 facilities or subsystem that gen-
erated the event, one of 8 logging levels, and the message.
Most events also include a tag indicating the program
generating the event. Solutions for authentication, man-
agement, and reliable delivery of syslog events over the
network have already been proposed [48] and are in the
process of being standardized [32], but none of this work
addresses the logging semantics that we wish to provide.
Our prototype implementation was written in a hybrid
of Python 2.5.2 and C++ and was benchmarked on an
Intel Core 2 Duo 2.4GHz CPU with 4GB of RAM in
64-bit mode under Linux. Our present implementation is
single-threaded, so the second CPU core is underutilized.
Our implementation uses SHA-1 hashes and 1024-bit
DSA signatures, borrowed from the OpenSSL library.
In our implementation, we use the array-based post-
order traversal representation discussed in Section 3.3.
The value store and history tree are stored in separate
write-once append-only ﬁles and mapped into memory.
Nodes in the history tree use a ﬁxed number of bytes,
permitting direct access. Generating membership and
incremental proofs requires RAM proportional to the
size of the proof, which is logarithmic in the number of
events in the log. Merkle aggregation query result sizes
are presently limited to those which can ﬁt in RAM,
approximately 4 million events.
The storage overheads of our tamper-evident history
tree are modest. Our prototype stores ﬁve attributes for
each event. Tags and host names are encoded as 2-of-32
bit Bloom ﬁlters. Facilities and hosts are encoded as
bit-vectors. To permit range queries to ﬁnd every event
in a particular range of time, an interval is used to encode
the message timestamp. All together, there are twenty
bytes of attributes and twenty bytes for a SHA-1 hash for
each node in the history tree. Leaves have an additional
twelve bytes to store the offset and length of the event
contents in the value store.
We ran a number of simulations of our prototype to
determine the processing time and space overheads of
the history tree. To this end, we collected a trace of
four million events from thirteen of our departmental
server hosts over 106 hours. We observed 9 facilities,
6 levels, and 52 distinct tags. 88.1% of the events are
from the mail server and 11.5% are from 98,743 failed
ssh connection attempts. Only .393% of the log lines
are from other sources.
In testing our history tree, we
replay this trace 20 times to insert 80 million events. Our
syslog trace, after the replay, occupies 14.0 GB, while the
history tree adds an additional 13.6 GB.
5.1 Performance of the logger
The logger is the only centralized host in our design
and may be a bottleneck. The performance of a real world
logger will depend on the auditing policy and relative
frequency between inserting events and requesting audits.
Rather than summarize the performance of the logger for
one particular auditing policy, we benchmark the costs of
the various tasks performed by the logger.
Our captured syslog traces averaged only ten events per
second. Our prototype can insert events at a rate of 1,750
events per second, including DSA signature generation.
Inserting an event requires four steps, shown in Table 2,
with the ﬁnal step, signing the resulting commitment,
responsible for most of the processing time. Throughput
Step
Task
% of CPU Rate
A
B
C
D
Parse syslog message
Insert event into log
Generate commitment
Sign commitment
Membership proofs
(with locality)
Membership proofs
(no locality)
2.4%
2.6%
11.8%
83.3%
-
(events/sec)
81,000
66,000
15,000
2,100
8,600
-
32
Table 2: Performance of the logger in each of the four steps re-
quired to insert an event and sign the resulting commitment and
in generating membership proofs. Rates are given assuming
nothing other than the speciﬁed step is being performed.
would increase to 10,500 events per second if the DSA
signatures were computed elsewhere (e.g.,
leveraging
(Section 6 discusses scalability
multiple CPU cores).
in more detail.)
This corresponds to 1.9MB/sec of
uncompressed syslog data (1.1 TB per week).
We also measured the rate at which our prototype can
generate membership and incremental proofs. The size of
an incremental proof between two commitments depends
upon the distance between the two commitments. As the
distance varies from around two to two million events,
the size of a self-contained proof varies from 1200 bytes
to 2500 bytes. The speed for generating these proofs
varies from 10,500 proofs/sec to 18,000 proofs/sec, with
shorter distances having smaller proof sizes and faster
performance than longer distances. For both incremental
and membership proofs, compressing by gzip [18] halves
the size of the proofs, but also halves the rate at which
proofs can be generated.
After inserting 80 million events into the history tree,
the history tree and value store require 27 GB, several