exposed. Since we empirically found that models that are trained
using data collected over the range of 15 consecutive days perform
well when used for prediction in the five days that follow, we
picked different periods of 20 days each (of which the first 15 days
were used for training, and the last five for testing) to evaluate our
approach. Specifically, we used five different periods of 20 days that
were uniformly spread over three months of our data collection,
starting from April 5th, 2017. We did not start from April 1st so that
we can use the first days of April to observe the behavior of users
and initialize the Past-Behavior Features to a meaningful state.
Similarly to Sec. 4.3, we considered a page to be malicious if
it had been in GSB, or appeared in GSB within two days or less
from the time it had been accessed by a user (i.e., we set τ = 2).
This required special care when training the DNNs. Specifically,
when training a DNN at time t, one cannot tell if accesses in the
interval t − τ + 1, . . . , t appeared in GSB within the time interval
t + 1, . . . , t + τ. Thus, to avoid mislabeled data in our training, we
exclude HTTP requests made in within t −τ +1, . . . , t from training
(in our case, we exclude HTTP requests made in t − 1 and t from
training).
To speed up the training process and minimize the use of private
information, we uniformly sampled the HTTP requests of unex-
posed sessions during training. Specifically, we used only 5% of
HTTP requests of unexposed sessions in training. We avoid sim-
ilar sampling of HTTP requests belonging to exposed sessions,
as their absolute number is relatively small. Overall, to train the
DNNs, we used ∼3,200,000 requests from unexposed sessions and
∼43,000 requests from exposed sessions, per evaluation period For
testing, we had an average of 412,005 benign sessions (contain-
ing ∼21,620,410 requests) and an average of 277 exposed sessions
(containing ∼19,457 requests), per evaluation period.
7.3 Experiment Results
We first tested how accurately do the DNNs predict exposure in
advance. Specifically, we measured the true-positive rate (TPR; the
rate in which exposed sessions were classified as exposed) and
the false-positive rate (FPR; the rate in which unexposed sessions
were classified as exposed) of the DNNs at different times before
exposure. In these experiments we used all the features: Contextual,
Past-Behavior, and Self-Reported.
Fig. 8(a) and Fig. 8(b) summarize the results. They show that the
DNNs can predict exposure with good accuracy. E.g., if the aim
is to predict exposure at any time before it happens, it is possible
to achieve 87% TPR with only 20% FPR. If lower FPR is desired, it
is possible to achieve a 75% TPR with 10% FPR, or even 32% TPR
with 1% FPR. Comparable accuracy can be achieved even when
aiming to predict exposure much in advance. E.g., we can obtain a
74% TPR with 20% FPR balance even when predicting exposure 30
seconds before it actually happens. As we discuss in the following
section, we believe that such early detection opens a window of
opportunity for a diverse set of interventions that can prevent
exposure to malicious content.
We next evaluated the influence of the different types of features
on the performance of the DNNs. In particular, we evaluated the
performance of the DNNs at predicting exposure when combining
Contextual Features with both Past-Behavior Features and Self-
Reported Features, only Past-Behavior Features, only Self-Reported
Features, or none. The results are shown in Fig. 8(c). The accuracy
of predictions is roughly similar in all cases—Contextual Features
slightly benefit from Past-Behavior Features, but the interaction
with Self-Reported Features turns out to be slightly detrimental.
Shortly stated, Contextual Features by themselves are sufficient
to accurately predict exposure over the short-term. Hence, accu-
rate within-session predictive engines may be developed without
the need to collect and store historical behavior information or
self-reported user input (which can be expensive and time consum-
ing), but rather via simply using available contextual information
describing users’ browsing activity.
False Alarms and Baseline Rate. There is a pronounced im-
balance between exposed and unexposed sessions. For example,
consider the operating point TPR=56% and FPR=3% in Fig. 8(a),
Session 8A: Web 1CCS’18, October 15-19, 2018, Toronto, ON, Canada1497(a)
(b)
(c)
Figure 8: ROC curves for short-term, within-session predictions. The curves are averaged between the five evaluation periods. (a) Performance
at different times before exposure (all features are used). (b) Is a zoomed-in version of (a). In (a) and (b), the dots correspond to example
operating points under the more inclusive ground truth (considering pages eventually flagged as malicious), over the five timing horizons
considered. (c) Performance when using different types of feature combinations (C: Contextual Features; P: Past-Behavior Features, and S:
Self-Reported Features).
which can be achieved when using all features, and predicting ex-
posure at any time before it happens. While the performance is
significantly better than a random predictor (where FPR=3% yields
TPR=3%), the low baseline exposure rate is a problem. The ratio
between exposed and unexposed sessions is roughly 0.1:99.9 (see
Table 2). At our chosen operating point, the system would make 56
true detections (out of 100) and ∼3,000 false detections for every
100,000 sessions. In other words, our system would primarily issue
false alarms, which could majorly impede deployment of active
defenses—for instance, terminating risky sessions could make the
network unusable, and issuing warnings would likely result in users
ignoring them.
Fortunately, there is evidence that many of these suspected false
positives are actually true positives. Indeed, we found that many
of the URLs that were not flagged by GSB during the time of data
collection were eventually (i.e., weeks or months later) flagged as po-
tentially malicious by either GSB or other lists that VirusTotal [15]
uses. More precisely, checking these services roughly one year after
our field measurements, we discovered ∼71% of the 3,000 “false”
positives observed at the TPR=56%, FPR=3% operating point were
eventually flagged as malicious, and thus were probably true posi-
tives.1 Hence, the system may actually be achieving roughly 2,186
true positives and 870 false positives for every 100,000 sessions.
(This also hints that our system performs much better than relying
on blacklists with respect to false negatives.) In contrast, only 0.13%
of the true negatives (i.e., ∼130 sessions per 100,000) may actually be
false negatives. Overall, this corresponds to TPR=93% and FPR=1%
with a ratio of roughly 2.4:97.6 between exposed and unexposed
sessions. Figures 8(a) and 8(b) show several operating points under
the more inclusive ground truth—they all hover around 90% TPR
for 1% FPR.
On a machine equipped with a Xeon X5875 CPU (3.07GHz), and
128GB of memory, our system can classify ∼1,300 feature vectors
per second. Thus, assuming that one webpage visit usually accounts
for five HTTP requests on average (as we found), then our system
can serve about 260 page visits per second. We believe this can be
further optimized (e.g., via GPU computation).
8 DISCUSSION
We now discuss the implications of our findings in terms of possible
interventions that can be enabled, the utility of self-reported data
for designing interventions, and several extensions to our work.
Reproducibility is a thorny issue in our case. We cannot release
measurements for user-privacy reasons, but hope to help others
reimplement our system and reproduce our results by open sourcing
the code used for computing features, training the neural networks,
and evaluating them.2
Possible Interventions. Our proposed short-term prediction ap-
proach may enable human-centered defenses. For instance, users
about to be exposed may be alerted (e.g., via nudging [3], or warn-
ings [26]) about possible risks much before the exposure. Giving
users sufficient time to consider their actions may improve their de-
cision making [55]. In certain networks (e.g., government/defense),
exposure to malicious content may be intolerable. Operators of
such networks may terminate the browsing sessions of users about
to visit malicious pages.
Deploying firewalls or intrusion detection systems (e.g., [60])
at the scale of a nationwide cellular network may be hard due to
increased expenses and high latencies that might affect users’ expe-
rience. A session-based exposure-prediction system like ours can
help prioritize traffic that should pass through expensive inspection,
thus enabling elastic defenses (e.g., [25]). Using deep processing, it
may also be possible to prevent the installation of third-party apps
in risky contexts. Additionally, the session-based prediction system
may be used to collect potentially malicious pages for scanning to
update blacklists, as it often predicts exposure to malicious pages
before the pages appear on blacklists (as shown in Sec. 7).
We note that in some cases the FPR may be too high for effec-
tively deploying the system in production. Indeed, when usability
is the main concern (e.g., in a general purpose network), certain
interventions, such as terminating connections or warning users,
may lead to annoyance or habituation [84] when the FPR is high. In
such cases, we suggest to tune the system to decrease the FPR (thus
also decreasing the TPR). Alternately, one may resort to more con-
servative interventions, such as identifying potentially malicious
pages for scanning or preventing the installation of 3rd-party apps.
1Similarly to prior work [5], we conservatively considered a URL to be malicious only
if it was flagged by two or more of the lists that are used by VirusTotal.
2https://github.com/mahmoods01/exposure-prediction
●●●●●0.00.10.20.30.40.50.60.70.80.91.00.00.10.20.30.40.50.60.70.80.91.0False Positive RateTrue Positive Rate0 s5 s10 s20 s30 s●●●●●0.00.10.20.30.40.50.60.70.80.91.010−310−1False Positive Rate (log−scale)True Positive Rate0 s5 s10 s20 s30 s0.00.10.20.30.40.50.60.70.80.91.00.00.10.20.30.40.50.60.70.80.91.0False Positive RateTrue Positive RateCCSCPCPSSession 8A: Web 1CCS’18, October 15-19, 2018, Toronto, ON, Canada1498We expect that future enhancements of the system would make it
effective in a wider range of settings.
Similarly to short-term prediction, long-term prediction can en-
able certain types of human-centered interventions, such as in-
creased training and education (e.g., [44]) for users predicted to
engage is long-term risky behavior.
Utility of Self-Reported Data. We developed a better understand-
ing of the utility of self-reported data for explaining and predicting
security behavior and incidents thus contributing to a growing
body of literature [21, 22, 86]. For example, users who reported
using anti-virus are much more likely than others to get exposed,
echoing previous findings [14]. While self-reported data helped
forecast long-term exposure risk, sole reliance on self-reported data
resulted in only moderately accurate predictions. To build accurate
predictive models based on self-reported data, one probably needs
to complement self-reported data with behavioral measurements
of metrics similar to those described in Sec. 4.
Potential Extensions and Limitations. Naturally, improving the
performance of our session-based apparatus would improve its
practicality. Using additional, computationally more expensive, fea-
tures (e.g., domain-name reputation [4], or redirection-graph fea-
tures [78]) that were shown to be useful for detecting malicious do-
mains may improve the performance. Machine-learning algorithms
other than the ones we explored in this work, such as recurrent
neural networks which achieve state-of-the-art results on learning
from sequences (e.g., in machine translation [6]), may also help.
Our session-based prediction approach could be extended to
networks other than cellular networks (e.g., residential or enter-
prise networks). Such networks introduce new challenges (e.g., due
to different user behavior or higher device diversity), but similar
prediction performance should be achievable.
While users should not have strong incentives to evade our
predictive models, attackers may want to “poison” the training
data [38]. Such attackers however need the ability to interact at the
scale of the (entire) exposed population of the service provider, and
thus to control a large number of devices on the provider network.
On smaller networks, this is an important issue to address.
To trust the predictive engine, users and network operators may
want to know why a session or a user are predicted to be exposed;
recent research on explaining machine-learning models [20, 66]
might be helpful.
Last, privacy concerns mandate to minimize the amount of user
data required for training the predictive models. Our (strong) sub-
sampling of HTTP requests from unexposed sessions helps, but re-
cent techniques might reduce user data collection even further [46].
9 CONCLUSION
We developed a system to predict whether users will be exposed
to malicious pages while browsing the web, and evaluated it using
three-months worth of HTTP traffic generated, in 2017, by >20,000
users of a Japanese cellular provider. We found that the system can
quite accurately predict exposure seconds before it occurs, thus
potentially enabling several proactive defenses. Our measurements
further motivated the need for such a system, as we discovered
blacklists such as Google Safe Browsing were, in some instances,
noticeably lagging (days or even weeks) behind malicious page
accesses. In fact, our system manages to detect malicious pages
before they are included in blacklists, and is thus a good comple-
ment to reactive defenses. Additionally, we collected self-reported
demographic and security-related data from the same users and
evaluated their utility at predicting exposure. We found that models
that solely rely on self-reported data may help forecast exposure
to malicious pages over long time periods, but are also remarkably
less accurate than those including behavioral data.
10 ACKNOWLEDGMENTS
We thank our anonymous reviewers and our shepherd, Tudor Dumi-
traş, for feedback that greatly improved the manuscript. Mahmood
Sharif was partially supported through MURI grant W911NF-17-
1-0370, and by CyLab at Carnegie Mellon University via a CyLab
Presidential Fellowship.
REFERENCES
[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Man-
junath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray,
Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan
Yu, and Xiaoqiang Zheng. 2016. TensorFlow: A System for Large-Scale Machine
Learning. In Proc. OSDI.
[2] Maarten Aertsen, Maciej Korczyński, Giovane Moura, Samaneh Tajalizadehkhoob,
and Jan van den Berg. 2017. No domain left behind: is Let’s Encrypt democratizing
encryption?. In Proc. ANRW.
[3] Hazim Almuhimedi, Florian Schaub, Norman Sadeh, Idris Adjerid, Alessandro
Acquisti, Joshua Gluck, Lorrie Faith Cranor, and Yuvraj Agarwal. 2015. Your
location has been shared 5,398 times!: A field study on mobile app privacy
nudging. In Proc. CHI.
[4] Manos Antonakakis, Roberto Perdisci, David Dagon, Wenke Lee, and Nick Feam-
ster. 2010. Building a Dynamic Reputation System for DNS. In Proc. USENIX
Security.
[5] Daniel Arp, Michael Spreitzenbarth, Malte Hubner, Hugo Gascon, Konrad Rieck,
and CERT Siemens. 2014. DREBIN: Effective and Explainable Detection of An-
droid Malware in Your Pocket. In Proc. NDSS.
translation by jointly learning to align and translate. In Proc. ICLR.
[7] Rodrigo Benenson. 2017. What is the class of this image? Discover the current
state of the art in objects classification. https://goo.gl/3wucZd. (2017). Online;
accessed 2 May 2018.
[6] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine
[8] Leyla Bilge and Tudor Dumitras. 2012. Before We Knew It: An Empirical Study
[9] Leyla Bilge, Yufei Han, and Matteo Dell’Amico. 2017. RiskTeller: Predicting the
[10] Davide Canali, Leyla Bilge, and Davide Balzarotti. 2014. On the effectiveness of
of Zero-day Attacks in the Real World. In Proc. CCS.
Risk of Cyber Incidents. In Proc. CCS.
risk prediction based on users browsing behavior. In Proc. AsiaCCS.
[11] Casey Canfield, Alex Davis, Baruch Fischhoff, Alain Forget, Sarah Pearman, and
Jeremy Thomas. 2017. Replication: Challenges in Using Data Logs to Validate
Phishing Detection Ability Metrics. In Proc. SOUPS.
[12] Yannick Carlinet, Ludovic Mé, Hervé Debar, and Yvon Gourhant. 2008. Analysis
of computer infection risk factors based on customer network usage. In Proc.
SECURWARE.
[13] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer.
2002. SMOTE: synthetic minority over-sampling technique. Journal of artificial
intelligence research 16 (2002), 321–357.
[14] Nicolas Christin, Serge Egelman, Timothy Vidas, and Jens Grossklags. 2011. It’s
all about the Benjamins: An empirical study on incentivizing users to ignore
security advice. In Proc. FC.
accessed 8 May 2018.
[15] Chronicle. 2004–. VirusTotal. https://www.virustotal.com/. (2004–). Online;
[16] Christopher Clark and Amos Storkey. 2015. Training Deep Convolutional Neural
[17] Adam Coates, Andrew Ng, and Honglak Lee. 2011. An analysis of single-layer