title:ClickNP: Highly flexible and High-performance Network Processing with
Reconfigurable Hardware
author:Bojie Li and
Kun Tan and
Layong Larry Luo and
Yanqing Peng and
Renqian Luo and
Ningyi Xu and
Yongqiang Xiong and
Peng Cheng
ClickNP: Highly Flexible and High Performance
Network Processing with Reconﬁgurable Hardware
Renqian Luo§†
Bojie Li§†
Layong (Larry) Luo‡
Yanqing Peng•†
Kun Tan†
Enhong Chen§
•SJTU
Ningyi Xu†
Yongqiang Xiong†
†Microsoft Research
Peng Cheng†
§USTC ‡Microsoft
ABSTRACT
Highly ﬂexible software network functions (NFs) are cru-
cial components to enable multi-tenancy in the clouds. How-
ever, software packet processing on a commodity server has
limited capacity and induces high latency. While software
NFs could scale out using more servers, doing so adds sig-
niﬁcant cost. This paper focuses on accelerating NFs with
programmable hardware, i.e., FPGA, which is now a ma-
ture technology and inexpensive for datacenters. However,
FPGA is predominately programmed using low-level hard-
ware description languages (HDLs), which are hard to code
and difﬁcult to debug. More importantly, HDLs are almost
inaccessible for most software programmers. This paper presents
ClickNP, a FPGA-accelerated platform for highly ﬂexible
and high-performance NFs with commodity servers. ClickNP
is highly ﬂexible as it is completely programmable using
high-level C-like languages, and exposes a modular program-
ming abstraction that resembles Click Modular Router. ClickNP
is also high performance. Our prototype NFs show that they
can process trafﬁc at up to 200 million packets per second
with ultra-low latency (< 2µs). Compared to existing soft-
ware counterparts, with FPGA, ClickNP improves through-
put by 10x, while reducing latency by 10x. To the best of
our knowledge, ClickNP is the ﬁrst FPGA-accelerated plat-
form for NFs, written completely in high-level language and
achieving 40 Gbps line rate at any packet size.
CCS Concepts
•Networks → Middle boxes / network appliances; Data
center networks; •Hardware → Hardware-software code-
sign;
Keywords
Network Function Virtualization; Compiler; Reconﬁgurable
Hardware; FPGA
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than ACM must be honored. Abstracting with credit is per-
mitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
SIGCOMM ’16, August 22–26, 2016, Florianopolis, Brazil
c(cid:13) 2016 ACM. ISBN 978-1-4503-4193-6/16/08. . . $15.00
DOI: http://dx.doi.org/10.1145/2934872.2934897
1
1.
INTRODUCTION
Modern multi-tenant datacenters provide shared infrastruc-
ture for hosting many different types of services from differ-
ent customers (i.e., tenants) at a low cost. To ensure secu-
rity and performance isolation, each tenant is deployed in
a virtualized network environment. Flexible network func-
tions (NFs) are required for datacenter operators to enforce
isolation while simultaneously guaranteeing Service Level
Agreements (SLAs).
Conventional hardware-based network appliances are not
ﬂexible, and almost all existing cloud providers, e.g., Mi-
crosoft, Amazon and VMWare, have been deploying software-
based NFs on servers to maximize the ﬂexibility [23, 30].
However, software NFs have two fundamental limitations –
both stem from the nature of software packet processing.
First, processing packets in software has limited capacity.
Existing software NFs usually require multiple cores to achieve
10 Gbps rate [33, 43]. But the latest network links have
scaled up to 40∼100 Gbps [11]. Although one could add
more cores in a server, doing so adds signiﬁcant cost, not
only in terms of capital expense, but also more operational
expense as they are burning signiﬁcantly more energy. Sec-
ond, processing packets in software incurs large, and highly
variable latency. This latency may range from tens of mi-
crosecond to milliseconds [22,33,39]. For many low latency
applications (e.g., stock trading), this inﬂated latency is un-
acceptable.
To overcome the limitations of software packet processing
while retaining ﬂexibility, recent work has proposed accel-
erating NFs using Graphics Processing Units (GPUs) [26],
network processors (NPs) [2, 5], or reconﬁgurable hardware
(i.e., Field Programmable Gate Arrays, or FPGAs) [24, 36,
42]. Compared to GPU, FPGA is more power-efﬁcient [19,
28]. Compared to specialized NPs, FPGA is more versatile
as it can be virtually reconﬁgured with any hardware logic
for any service. Finally, FPGAs are inexpensive and being
deployed at scale in datacenters [24, 40].
In this work, we explore the opportunity to use FPGA to
accelerate software NFs in datacenters. The main challenge
to use FPGA as an accelerator is programmability. Conven-
tionally, FPGAs are programmed with hardware description
languages (HDLs), such as Verilog and VHDL, which ex-
pose only low level building blocks like gates, registers, mul-
tiplexers and clocks. While the programmer can manually
tune the logic to achieve maximum performance, the pro-
gramming complexity is huge, resulting in low productivity
and debugging difﬁculties. Indeed, the lack of programma-
bility of FPGA has kept the large community of software
programmers away from this technology for years [15].
This paper presents ClickNP, an FPGA-accelerated plat-
form for highly ﬂexible and high-performance NF process-
ing on commodity servers. ClickNP addresses the program-
ming challenges of FPGA in three steps. First, ClickNP
provides a modular architecture, resembling the well-known
Click model [29], where a complex network function is com-
posed using simple, well-deﬁned elements 1. Second, ClickNP
elements are written with a high-level C-like language and
are cross-platform. ClickNP elements can be compiled into
binaries on CPU or low-level hardware description language
(HDL) for FPGAs, by leveraging commercial high-level syn-
thesis (HLS) tools [1,6,9]. Finally, we develop a high-performance
PCIE I/O channel that provides high-throughput and low la-
tency communications between elements running on CPU
and FPGA. This PCIE I/O channel not only enables joint
CPU-FPGA processing – allowing programmers to partition
their processing freely, but also is of great help for debug-
ging, as a programmer may easily run an element in question
on the host and use familiar software tools to diagnose.
ClickNP employs a set of optimization techniques to ef-
fectively utilize the massive parallelisms in FPGA. First of
all, ClickNP organizes each element into a logic block in
FPGA and connects them with ﬁrst-in-ﬁrst-out (FIFO) buffers.
Therefore, all these element blocks can run in full parallel.
For each element, the processing function is carefully writ-
ten to minimize the dependency among operations, which
allows the HLS tools to generate maximum parallel logics.
Further, we develop delayed write and memory scattering
techniques to address the read-write dependency and pseudo-
memory dependency, which cannot be resolved by existing
HLS tools. Finally, we carefully balance the operations in
different stages and match their processing speed, so that
the overall throughput of pipelines is maximized. With all
these optimizations, ClickNP achieves high packet process-
ing throughput up to 200 million packets per second 2, with
ultra-low latency (< 2µs for any packet size in most appli-
cations). This is about a 10x and 2.5x throughput gain, com-
pared to state-of-the-art software NFs on CPU and CPU with
GPU acceleration [26], while reducing the latency by 10x
and 100x, respectively.
We have implemented the ClickNP tool-chain, which can
integrate with various commercial HLS tools [1,9]. We have
implemented about 100 common elements, 20% of which
are re-factored straightforwardly from Click. We use these
elements to build ﬁve demonstration NFs: (1) a high-speed
trafﬁc capture and generator, (2) a ﬁrewall supporting both
exact and wildcard matching, (3) an IPSec gateway, (4) a
Layer-4 load balancer that can handle 32 million concur-
rent ﬂows, and (5) a pFabric scheduler [12] that performs
1This is also where our system name, Click Network Proces-
sor, comes from.
2The actual throughput of a ClickNP NF may be bound by
the Ethernet port data rate.
2
Figure 1: A logic diagram of a FPGA board.
strict priority ﬂow scheduling with 4-giga priority classes.
We evaluate these network functions on a testbed with Dell
servers and Altera Stratix V FPGA boards [40]. Our re-
sults show that all of these NFs can be greatly accelerated
by FPGA and saturate the line rate of 40Gbps at any packet
size with very low latency and neglectable CPU overhead.
In summary, the contributions of this paper are: (1) the
design and implementation of ClickNP language and tool-
chain; (2) the design and implementation of high-performance
packet processing modules that are running efﬁciently on
FPGA; (3) the design and evaluation of ﬁve FPGA-accelerated
NFs. To the best of our knowledge, ClickNP is the ﬁrst
FPGA-accelerated packet processing platform for general net-
work functions, written completely in high-level language
and achieving a 40 Gbps line rate.
2. BACKGROUND
2.1 FPGA architecture
As the name indicates, FPGA is a sea of gates. The basic
building block of FPGA is logic element (LE), which con-
tains a Look-up Table (LUT) and a few registers. The LUT
can be programmed to compute any combinational logic and
registers are used to store states. Besides basic LEs, FPGA
also contains Block RAMs (BRAMs) to store data, and Dig-
ital Signal Processing (DSP) components for complex arith-
metic operations. Normally, FPGAs are attached to a PC
through a PCIe add-in board, which may also contain a DRAM
of multi-giga bytes and other communication interfaces, e.g.,
10G/40G Ethernet ports. Figure 1 shows a logic diagram of
a FPGA board.
Compared to CPU or GPU, FPGAs usually have a much
lower clock frequency and a smaller memory bandwidth.
For example, typical clock frequency of a FPGA is about
200MHz, more than an order of magnitude slower than CPU
(at 2∼3 GHz). Similarly, the bandwidth to a single Block
memory or external DRAM of FPGA is usually 2∼10 GBps,
while the memory bandwidth is about 40 GBps of Intel XEON
CPU and 100 GBps for a GPU. However, the CPU or GPU
have only limited cores, which limits parallelism. FPGAs
have a massive amount of parallelism built-in. Modern FP-
GAs may have millions of LEs, hundreds K-bit registers,
tens of M-bits of BRAM, and thousands of DSP blocks. In
theory, each of them can work in parallel. Therefore, there
could be thousands of parallel “cores” running simultane-
ously inside a FPGA chip. Although the bandwidth of a sin-
gle BRAM may be limited, if we access the thousands of
BRAMs in parallel, the aggregate memory bandwidth can be
FPGADRAMEthernet PortEthernet PortPCIemultiple TBps! Therefore, to achieve high performance, a
programmer must fully utilize this massive parallelism.
Conventionally, FPGAs are programmed using HDLs like
Verilog and VHDL. These languages are too low level, hard
to learn and complex to program. As a consequence, the
large community of software programmers has stayed away
from FPGA for years [15]. To ease this, many high level
synthesis (HLS) tools/systems have been developed in both
industry and academia that try to convert a program in high
level language (predominately C) into HDLs. However, as
we will show in the next subsection, none of them is suitable
for network function processing, which is the focus of this
work.
2.2 Programming FPGA for NFs
Our goal is to build a versatile, high performance network
function platform with FPGA-acceleration. Such a platform
should satisfy the following requirements.
Flexibility. The platform should be fully programmed using
high-level languages. Developers program with high-level
abstractions and familiar tools, and have similar program-
ming experience as if programming on a multi-core proces-
sor. We believe this is a necessary condition for FPGA to be
accessible to most software programmers.
Modularized. We should support a modular architecture
for packet processing. Previous experiences on virtualized
NFs have demonstrated that a right modular architecture can
well capture many common functionalities in packet pro-
cessing [29, 33], making them easy to reuse in various NFs.
High performance and low latency. NFs in datacenters
should handle a large amount of packets ﬂowing at the line-
rates of 40/100 Gbps with ultra-low latency. Previous work
has shown [44] that even a few hundred microseconds of la-
tency added by NFs would have negative impacts on service
experience.
Support joint CPU/FPGA packet processing. We’d say
FPGA is no panacea. As inferred from the FPGA architec-
ture discussed earlier in §2.1, not all tasks are suitable for
FPGA. For example, algorithms that are naturally sequential
and processing that has very large memory footprint with low
locality, should process better in CPU. Additionally, FPGA
has a strict area constraint. That means you cannot ﬁt an arbi-
trarily large logic into a chip. Dynamically swapping FPGA
conﬁgurations without data plane interruption is very difﬁ-
cult, as the reconﬁguration time may take seconds to min-
utes, depending on the FPGA’s size. Therefore, we should
support ﬁne-grained processing separation between CPU and
FPGA. This requires high-performance communication be-
tween CPU and FPGA.
No of existing high level programming tools for FPGA
satisfy all aforementioned requirements. Most HLS tools,
e.g., Vivado HLS [9], are only auxiliary tools for HDL tool
chains. Instead of directly compiling a program into FPGA
images, these tools generate only hardware modules, i.e., IP
cores, which must be manually embedded in a HDL project
and connected to other HDL modules – a mission impossible
Figure 2: The architecture of ClickNP.
for most software programmers.
Altera OpenCL, however, may directly compile an OpenCL
program to FPGA [1]. However, the OpenCL programming
model is directly derived from GPU programming and is not
modularized for packet processing. Further, OpenCL does
not support joint packet processing between CPU and FPGA
very well: First, communication between a host program
and a kernel in FPGA must always go through the onboard
DDR memory. This adds non-trivial latency and also causes
the on-board memory a bottleneck. Second, OpenCL kernel
functions are called from the host program. Before a ker-
nel terminates, the host program cannot control the kernel
behavior, e.g. setting new parameters, nor reading any ker-
nel state. But NFs face a continuous stream of packets and
should be always running.
Click2NetFPGA [41] provides a modular architecture by
directly compiling a Click modular router [29] program into
FPGA. However, the performance of [41] is much lower (two
orders of magnitude) than what we report in this paper, as
there are several bottlenecks in their system design (e.g., mem-
ory and packet I/O) and they also miss several important op-
timizations to ensure fully pipelined processing (as discussed
in §4). Additionally, [41] does not support FPGA/CPU joint
processing and thus unable to update conﬁguration or read
states while data plane is running.
In the following, we will present ClickNP, a novel FPGA-
accelerated network function platform that satisﬁes all afore-
mentioned four requirements.
3. ARCHITECTURE
3.1 System architecture
Figure 2 shows the architecture of ClickNP. ClickNP builds
on the Catapult Shell architecture [40]. The shell contains
many reusable bits of logic that are common for all applica-
tions and abstracts them into a set of well-deﬁned interfaces,
e.g., PCIe, Direct Memory Access (DMA), DRAM Mem-
ory Manage Unit (MMU), and Ethernet MAC. The ClickNP
FPGA program is synthesized as a Catapult role. However,
since ClickNP relies on commodity HLS tool-chains to gen-
erate FPGA HDL, and different tools may generate their own
(and different) interfaces for the resources managed by the
shell, we need a shim layer, called HLS-speciﬁc runtime, to
3
Catapult shellClickNProleFPGAHostCatapult PCIeDriverClickNPlibraryClickNPhost processMgrthrdWorker thrdClickNPelementsClickNPscriptClickNPhost mgrClickNPcompilerHLS specific libsCommercial HLS tool-chainHLS specific runtimePCIeI/O channelperform translations between HLS speciﬁc interfaces to the
shell interfaces.
A ClickNP host process communicates with the ClickNP
role through the ClickNP library, which further relies on the
services in Catapult PCIe driver to interact with FPGA hard-
ware. The ClickNP library implements two important func-
tions: (1) It exposes a PCIe channel API to achieve high-
speed and low latency communications between the ClickNP
host process and the role; (2) It calls several HLS speciﬁc li-
braries to pass initial parameters to the modules in the role,
as well as control the start/stop/reset of these modules. The
ClickNP host process has one manager thread and zero or
multiple worker threads. The manager thread loads the FPGA
image into the hardware, starts worker threads, initializes
ClickNP elements in both FPGA and CPU based on the con-
ﬁguration, and controls their behaviors by sending signals to
elements at runtime. Each worker thread may process one or