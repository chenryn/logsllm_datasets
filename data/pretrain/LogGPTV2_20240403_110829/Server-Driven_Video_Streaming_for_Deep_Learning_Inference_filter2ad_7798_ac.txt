a vision task‚Äîa bounding box (in object detection and face recogni-
tion) and a pixel (in semantic segmentation). At a high level, given
the DNN output on the low-quality video, we first identify the ele-
ments that are likely to be in the DNN output on the high-quality
video but not in the DNN output on the low-quality video, and we
then pick a small number of rectangles (for encoding efficiency)
as the feedback regions to cover these elements. Next, we present
how this high-level logic is used in two classes of vision tasks.
Object detection (based on bounding boxes): Most bounding-
box-based DNNs are anchor-based (though some are anchor-free
[29]). This means that a DNN will first identify regions that might
contain objects and then examine each region. Each proposed region
is associated with an objectness score that indicates how likely an
object is in the region. For DNNs (e.g., FasterRCNN-ResNet101 [68])
that use region proposal networks (RPNs), each proposed region
is directly associated with an objectness score. However, not all
object-detection DNNs use RPNs. For instance, Yolo [66] does not
and instead, it assigns a score for each class in each region in the
final output. In this case, we sum up the scores of non-background
classes as the objectness score, which indicates how likely a region
includes a non-background object. We keep regions with objectness
score over a threshold (e.g., 0.5 for FasterRCNN-ResNet101, and
Figure 17 will show DDS‚Äôs performance under different objectness
thresholds). From these high-objectness regions, we apply two
filters to remove those that are already in the DNN output on the
low-quality video (Stream A). First, we filter out those regions
that have over 30% IoU (intersection-over-union) overlap with the
labeled bounding boxes returned by DNN on the low-quality video.
We empirically pick 30% because it works well on all the videos
561
Cheap model (SSD-MobileNet-v2)Compute-intensive model (FasterRCNN-ResNet101)Source(Camera)Server(DNN)Passive video stream driven by camera-side heuristicsSource(Camera)Server(DNN)Stream A: Passive low-qualityStream B: Feedback-driven(a) Traditional video streaming(b) Real-time DNN-driven streamingSIGCOMM ‚Äô20, August 10‚Äì14, 2020, Virtual Event, USA
K. Du, A. Pervaiz, X. Yuan, A. Chowdhery, Q. Zhang, H. Hoffmann, J. Jiang
(a) Bounding-box-based tasks
(b) Pixel-based tasks
Figure 7: Illustration on how DDS generates feedback regions on two
types of applications.
in our dataset. Second, we remove regions that are over 4% of the
frame size (roughly 20% of each dimension), because we empirically
find that if an object is large, the DNN should have successfully
detected it. The remaining region proposals (bounding boxes) are
used as feedback regions.
Figure 7a shows an example: there are nine bounding boxes in
high-objectness-score results, three of which overlap with inference
results in Stream A and one of which is too large. The remaining
five regions are the feedback regions.
Semantic segmentation (based on pixels): Semantic segmen-
tation DNNs assign each pixel a class label (see Figure 1), and in
addition, they also give a score of each class for each pixel (the class
with the highest score is the class label). We first assign a score of
1+ùëöùëéùë•‚Ä≤‚àíùëöùëéùë• to each pixel, where ùëöùëéùë• is the largest score among
classes of interest and ùëöùëéùë•‚Ä≤ is the second largest. Intuitively, the
higher the score is, the more ‚Äúindecisive‚Äù the DNN is about which
class a pixel belongs to. We then pick the feedback regions by cre-
ating ùëò rectangles that cover as many high-score pixels as possible.
We repeatedly pick the ùëõ √ó ùëõ rectangle in which the pixels have
maximum average score and zero out the scores of corresponding
pixels until we found ùëò rectangles. The values of ùëõ and ùëò control
the total number of pixels in the feedback regions (ùëò ¬∑ ùëõ2). We use
ùëõ = 32 and ùëò = 16, though we do not claim them to be optimal
values. (Figure 18 shows the performance under different ùëò val-
ues.) Figure 7b shows an example and the selection of one feedback
region. We can see that the high-score pixels typically lie at the
boundaries of objects.
We notice three properties of the above logic.
‚Ä¢ First, the feedback regions are likely in the DNN output but not
yet in DNN‚Äôs output on low-quality video. This provides useful
clue about where video quality should be increased in Stream B.
‚Ä¢ Second, to save bandwidth of Stream B, the feedback regions are
created with shapes that can be efficiently encoded. Thus, they
are different from any direct (intermediate/final) output of DNNs
(e.g., region proposals).
‚Ä¢ Third, the algorithms to extract feedback regions only assume
the format of the DNN output, rather than particular DNN archi-
tectures or parameters.
3.3 Handling bandwidth variation
Like other video streaming protocols, DDS must adapt its bandwidth
usage to handle fluctuations in available bandwidth. There are
several effective control knobs that affect the bandwidth usage
of DDS. However, we empirically find that these knobs affect the
562
Figure 8: DDS‚Äôs adaptive feedback control system dynamically tunes
the low and high quality configurations based on the difference be-
tween the estimated available bandwidth for the next segment and
that used for the previous segment.
bandwidth-accuracy tradeoff in a similar way (i.e., on the same
Pareto boundary; ¬ß5.4), so DDS only tunes low quality level and
high quality level.
To tune the low and high quality levels, we implement a feedback
control system (illustrated in Figure 8). Our controller is based on
prior work that proposes a virtual, adaptive control system that
can be customized for specific deployments [28, 60]. To instantiate
this controller, DDS needs to specify three things: a bandwidth
constraint to be met, feedback for monitoring bandwidth usage,
and the tunable parameters that affect bandwidth usage. For DDS,
the bandwidth constraint is the estimated available bandwidth for
the next segment (labelled (1) in the figure), the feedback is the
total bandwidth usage (for both low and high quality) from the
last segment (2), and the tunable parameters are the resolution
and quantization parameters (i.e., the QP in Figure 8) of both the
low and high quality (3). The controller continually estimates the
base bandwidth usage; i.e., the last segment‚Äôs bandwidth usage
if the default parameter settings had been used. The controller
then takes this base behavior as well as the difference between the
desired bandwidth constraint for the next segment and the achieved
bandwidth usage for the previous segment and computes a scaling
factor for the base bandwidth. This scaling factor is passed to an
optimizer which finds the low and high quality settings that deliver
the scaled bandwidth usage while maximizing F1 score.
DDS‚Äôs dynamic bandwidth adaptation has several useful formal
properties based on its use of feedback control.
First, the content estimator can handle dynamic video content
which changes the relationship between the parameters and band-
width usage. The adaptation mechanism uses a Kalman Filter to
continually estimate the base bandwidth usage. Hence, when the
video content changes, the control model‚Äîthat captures the rela-
tionship between the parameters and bandwidth usage‚Äîwill update
itself, allowing DDS to capture unmodeled non-linearities in the re-
lationship between quality settings and bandwidth use. Intuitively,
we can think of the relationship between bandwidth usage and
quality parameters as a curve and the base bandwidth (estimated
by the Kalman filter) as a tangent to that curve. When adjusting
the quality parameters, the DDS controller uses this tangent as a
linear approximation to the true behavior. Using this formulation,
the bandwidth usage converges to the bandwidth constraint in time
proportional to the logarithm of the error in this estimation [60].
This adjustment technique provides robustness in the face of shifts
and variations in the system including when there does not exist a
single control model that captures all system dynamics [37].
Second, the optimizer finds the highest quality given the band-
width usage specified by the controller. This optimality is achieved
high-confidence inference resultshigh-objectness-score resultsfeedback regionshigh-confidence inference resultshigh-objectness-score resultsfeedback regionsContentEstimatorControllerOptimizer-high res.high QPlow res.EstimatedAvailableBandwidthBandwidthDifferenceBaseBandwidthBandwidthMultiplierParameterSettingsBandwidth used last segmentBandwidth used by the last segment123low QPServer-Driven Video Streaming for Deep Learning Inference
SIGCOMM ‚Äô20, August 10‚Äì14, 2020, Virtual Event, USA
by scheduling configurations over multiple segments. As the sys-
tem has a small, constant number of constraints (simply respecting
the bandwidth requirement), an optimal solution can be found in
constant time [49].
3.4 Design rationale and performance analysis
Why driven by server? At first glance, the idea of server-driven
region extraction seems similar to Vigil [80] and EAAR [54], which
also identifie and send only regions likely with objects to the server.
But we argue that the region extraction methods of Vigil and EAAR
spend extra bandwidth on objects that can be detected at low quality,
and they do not generalize to applications like semantic segmenta-
tion. Moreover, both of them do not leverage modern video codec
to save bandwidth. Furthermore, Vigil‚Äôs camera-side local model
uses simpler feature extractor than the server-side DNN, and thus
might miss objects when analyzing challenging video content (as
illustrated in Figure 5). As we will show in ¬ß5.2, even if Vigil uses a
model (MobileNet-SSD) that runs only 3√ó faster than the server-
side DNN [20], it still misses about 40% more objects of interest than
DDS and sends over 30% more data; EAAR consumes 4√ó bandwidth
and still less accurate than DDS. ¬ß5.2 will give more analysis.
Analysis of DDS‚Äôs network usage: The bandwidth usage of DDS
is governed by two factors: (1) the quality levels of Stream A and
Stream B, and (2) the areas of the feedback regions of Stream B. If
Stream A uses a high quality level, the bandwidth usage will be
dominated by Stream A and the feedback regions selected in Stream
B will be less relevant. But if Stream A uses a very low quality level,
DDS cannot extract meaningful feedback regions from the DNN
output on the low-quality video. (¬ß5.1 gives the detail configurations
of Stream A and B.) The areas of feedback regions have a complex
relationship with the video content. Intuitively, feedback regions
will be smaller when less objects/pixels are associated with small
objects or hard-to-classify boundaries. When feedback regions are
so large that Stream B is almost the same size of the original video,
then DDS will not save much bandwidth.
To use the analysis in ¬ß2.4, when the total cost of a video analytics
system is dominated by the network cost (Setting 1), DDS will reach
better cost-accuracy tradeoffs than the baselines, although it will
do poorly when the cost is dominated by the server cost (Setting 2).
Delay analysis of DDS: One concern of DDS is the extra delay
in Stream B. We introduce an optimization in ¬ß4.2 to reduce the
average response delay by reporting the objects/pixels that are
already detected in Stream A. This allows DDS to achieve a lower
average response delay than the baselines at similar accuracy (see
Figure 4a), since Stream A has a low response delay and many
objects/pixels will not need Stream B.
4 IMPLEMENTATION
We implement DDS mostly in Python and the code is available and
will be regularly updated in [8].
4.1 DDS Interface
DDS sits between the low-level functions (video codec and DNN
inference) and the high-level applications (e.g., object-detection
queries). It provides ‚Äúsouth-bound‚Äù APIs and ‚Äúnorth-bound‚Äù APIs,
both making minimum assumptions about the exact implementa-
tion of the low-level and high-level functions.
The south-bound APIs interact with the video codec and DNN.
Our implementation uses the APIs already exposed by the x264
MPEG video, such as x264_encoder_encode [25]. From DNN, DDS
implements two functions: (1) feedback regions, each with a speci-
fied location; and (2) detection results including the detected pix-
els/bounding boxes each with a specified location and a detection
confidence score.
The north-bound APIs implement the same analyst-facing (north-
bound) APIs as the DNNs (DDS can simply forward any function
call to DNNs), so the high-level applications (e.g., [51, 58]) do not
need to change and DDS can be deployed transparently from the
analysts‚Äô perspective. The only difference is that DDS runs the DNN
twice on the same video segment, so the two DNN inference results
must be merged into a single result, which is logically similar to
how DNNs internally merge redundant results (e.g., [73]).
4.2 Optimization
Saving bandwidth by leveraging codec: A naive implementa-
tion of Stream B would encode each feedback region as a separate
high-quality image. But we found that the total size of these images
would be much greater than the original video without cropping out
the regions! The reason is that the video codecs (e.g., H.264/H.265),
after decades of optimization, are very effective in exploiting the
spatial redundancies within a frame and the temporal redundancies
between frames to reduce the encoded video size. DDS leverages
such encoding effectiveness. It sets the pixels outside of the feed-
back regions in the high quality image to black (to remove spatial
redundancies), and encodes these images into a video file (to remove
temporal redundancies).
Reducing average delay via early reporting: The cost that DDS
pays to get better performance is the worst-case response delay: the
result of Stream B will wait for two rounds of inference before it can
be returned. We leverage the observation that a substantial fraction
of the DNN output from the low-quality video (Stream A) already
has high confidence and thus can be returned without waiting for
Stream B. While this optimization does not change the bandwidth
consumption or worst-case response delay, it substantially reduces
the delay of many inference results. In object detection, we empiri-
cally found that over 90% of all final detected objects could have
been detected in Stream A. These objects can be returned much
faster than any prior approach, because Stream A uses a quality
level much lower than what other work (e.g., [31, 32, 78]) would
need to achieve the same accuracy. Similarly, in semantic segmenta-
tion, we found that the label of over 93% of all pixels can be returned
by Stream A, without the need of Stream B.
Camera-side heuristics for fault tolerance: When the connec-
tion to the server is poor or the server is disconnected, DDS will
leverage camera-side compute (if available). Like Glimpse [30], DDS
can use a camera-side tracking logic to generate inference results
on new frames based on the results of the previous frames.
5 EVALUATION
The key takeaways of our evaluation are:
‚Ä¢ On three vision tasks, DDS achieves same or higher accuracy
than the baselines while using 18-58% less bandwidth (Figure 9)
and 25-65% lower average response delay (Figure 11).
563
SIGCOMM ‚Äô20, August 10‚Äì14, 2020, Virtual Event, USA
K. Du, A. Pervaiz, X. Yuan, A. Chowdhery, Q. Zhang, H. Hoffmann, J. Jiang