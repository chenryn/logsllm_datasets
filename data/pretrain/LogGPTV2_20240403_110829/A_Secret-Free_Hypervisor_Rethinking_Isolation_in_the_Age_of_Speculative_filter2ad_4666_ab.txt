pools. These pools are physical pages mapped to virtual
addresses, which are either required to remain in memory
(non-paged pool) or can be removed from the memory because
a copy is already stored on the disk (paged pool). Combined,
these pools map a large fraction of the physical memory into
the kernel address space of every process.
The direct map leverages the address space under 64-bit
ISAs and is permanently mapped in all page tables, enabling
the kernel or hypervisor to efﬁciently access all (or most)
memory at any time. The access is fast because such large
contiguous mappings are typically handled by superpages to
reduce TLB pressure and page-table size.
C. Exploiting kernel privileges
Kernel code is susceptible to classes of vulnerabilities
introduced by programming errors and can be exploited to per-
form data leakage, data corruption, code injection and remote
execution. The lack of mutual isolation and the monolithic
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:47 UTC from IEEE Xplore.  Restrictions apply. 
371
User1User2User3Kernel imageKernel dataDirect map…Kernel halfUserhalfPhysical memoryProc3emerged to limit the hypervisor itself, preventing speculation
from accessing secrets via its vast address space [30], [31].
Although necessary, the increasing number of security hard-
ening and speculation defenses that accumulated over the years
has resulted in more than 100% time overhead in several core
kernel operations [32]. Outside micro-benchmarks, the study
also revealed a degradation of more than 30% in real-world
workloads. A systematic mechanism that defends against a
class of vulnerabilities is desirable, rather than countering each
attack in different ways, which may not compose well with
other defenses, and slowly degrade performance over time as
more vulnerabilities are found.
E. Type-I, Type-II hypervisors and Xen
Hypervisors are categorized as either Type-I or Type-II.
Type-I hypervisors (for example Xen, Hyper-V, and VMWare
ESXi) sit directly atop the hardware and manage guest oper-
ating systems, often requiring at least a privileged domain to
support driver back-ends and to issue commands to manage
unprivileged domains. Type-II hypervisors (for example KVM
or bhyve) exist atop or within an operating system, abstracting
guest domains as user processes and using the host OS drivers
for hardware access.
Xen is a Type-I hypervisor. A privileged domain, dom0, is
started ﬁrst to manage other guest domains (domUs). Dom0
has drivers for hardware, and provides virtual disks and
network access for domUs. Dom0 will run the back-end driver
for paravirtualized devices that are made available to other
domains (for example, network interfaces and disks), which
multiplexes and forwards to the hardware requests from the
front-end driver in each DomU [33].
Xen’s fully paravirtualized (PV) mode, inherited from the
Nemesis operating system, is a unique feature that predates
x86 virtualization hardware extensions. PV mode exposes
a series of hypercalls for MMU management, I/O drivers,
timers and interrupts, enabling the guest kernel and user space
to function in lower privilege rings without virtualization
extensions. Most hypervisors provide paravirtualized device
drivers, even if they rely on hardware features for CPU and
memory virtualization.
III. THREAT MODEL
We assume that an attacker resides within an unprivileged
VM. In regard to speculative execution attacks, we assume the
attacker does not co-exist in the same VM with the victim, or
in the context of OS kernels, the same process. We categorize
speculative vulnerabilities into three different classes:
a) Permission: Speculation violating permissions. These
attacks are performed directly in the lower privileged context
and target mappings that are available in the page table but
are restricted from usage within the current domain. A typical
attack of this category is Meltdown.
b) Coercion: Triggering a speculative control-ﬂow tran-
sition in the higher-privileged context to execute a disclo-
sure gadget. The side-effects of the gadget executed under
speculation can still persist in the micro-architectural state
Fig. 2.
ret2usr and ret2dir
nature open up possibilities to inject code and data directly
from lower privilege levels, as demonstrated by ret2usr and
ret2dir in Fig. 2.
In both attacks, the attacker requires no code or data injec-
tion into kernel memory. The payloads are prepared directly in
user space before redirecting kernel control ﬂow to malicious
code or to a stack pivoting gadget [24], [25].
In response, hardware features such as Supervisor Mode Ac-
cess Prevention (SMAP), Supervisor Mode Execution Preven-
tion (SMEP) [18] and Privileged Execute-Never (PXN) [26]
were introduced to thwart unintended redirection of kernel
access to user memory. Unfortunately, they are ineffective
against ret2dir. The direct map is valid kernel memory, thus it
is impossible to distinguish between intended and unintended
direct map access with the aforementioned hardware features.
The authors of ret2dir proposed eXclusive Page Frame Own-
ership (XPFO) as a mitigation that unmaps memory from the
direct map on page allocation to user space. However, the cost
of TLB shootdowns for direct map maintenance prohibits its
use in scenarios where processes are created and destroyed
frequently and does not scale as the core count increases.
D. Speculative execution attacks of VMs
The recent epidemic of speculative execution attacks have
shown that VMs are not excluded from the breach of tradi-
tional architectural protection boundaries. Spectre, Meltdown
and many other speculative vulnerabilities have disclosed
multiple variants that are applicable to virtualization [15], [16],
[27]–[29].
In addition, both Spectre [15] and L1TF [27] have demon-
strated that it is insufﬁcient to restrict speculative side channels
in the VM context, as the hypervisor can be manipulated
into fetching data into the cache. PTI, for example, recovers
the full kernel map upon entry and is not effective against
branch predictor mistraining. Worse, the presence of the direct
map allows a speculatively manipulated hypervisor to access
arbitrary physical pages. IBRS, IBPB, and STIBP [18] are
provided to guard against mistraining. These measures either
ﬂush potential malicious branch predictor states or isolate
between privilege levels and sibling threads. They do not
address the fundamental problem that a kernel/hypervisor
exposes a giant surface that can be exploited once another side
channel is found. As a result, several implementations have
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:47 UTC from IEEE Xplore.  Restrictions apply. 
372
Malicious user dataKernelimageKernel data……coderet2usrUser code alias…data aliasret2dirKernelhalfUser halfand can be converted to architectural state from the lower
privileged context to extract secrets. Examples of this class
include Spectre-V1 and V2.
c) Micro-architectural: Attacks that purely gather resid-
ual signals from shared µarch structures (for example store
buffers, load ports, L1D cache). These attacks are performed
directly in the lower privileged domain after it is transitioned
to. The sharing of µarch can be either spatial (hyper-threading)
or temporal (hypervisor entry/exits and context switches).
L1TF and MDS belong to this category.
We consider all three categories of speculative attacks to
be in scope. The secret-free design mitigates permission and
coercion attacks fully, by eliminating secrets from both guest
and hypervisor contexts. Existing mitigations for these attack
classes are therefore unnecessary. The design does not directly
address speculative side channels from the µarch category, but
this work composes with existing mitigations such as Core
Scheduling. We demonstrate that the defense against the µarch
class is signiﬁcantly simpler to build on top of a secret-free
hypervisor than on the previous state of the art.
Randomization-based mitigations are out of scope. We do
not rely on any form of ASLR for secret isolation, as existing
literature has demonstrated that the limited entropy and possi-
ble surfaces of pointer leaking often limit its effectiveness. An
attacker is permitted to reveal the memory location of secrets,
provided that secret contents remain inaccessible.
IV. DESIGN GOAL
a) Deﬁnition of secrets: We partition data into secrets
and non-secrets. The former includes all guest register state
and memory as well as their copies (e.g., guest register spills
when entering the hypervisor, copy_from_guest() or
copy_to_guest() during hypercalls). In the case of Type-I
hypervisors, even though the privileged guest domain (dom0)
is not directly occupied by customers, the implicit copying
between driver back-ends and domU front-ends means that
dom0 state must be treated as secret. We do not consider guest
state to be secret to its owner. An exploit that reveals register
or memory contents belonging to the attacker does not reveal
secrets by our deﬁnition.
b) Secrets by default: State-of-the-art techniques audit
each vulnerability and identify vulnerable surfaces for isola-
tion, expanding the deny-list. The secret-free design considers
all data as secrets by default. We shall explore a different
approach by constructing a minimal surface for all domains
(including the hypervisor) and identifying non-secrets that are
permitted in the address space.
c) Secret-Free: The secret-free design shall not allow
secrets to be visible to any domains other than the owner,
neither architecturally nor in any form of the speculative side
channels discussed in Section III. A secret-free view deﬁnes
both guest and hypervisor space. The hypervisor entered
under a domain’s context (hypercalls, exceptions or interrupts)
cannot contain secrets of other domains. KPTI, for example,
violates this guarantee because it recovers the full address
space and maps secrets to other domains on kernel entry.
d) Performance: Overall, the overhead needs to be low
and must not impact real-world application performance. A
new hypervisor design that exhibits high overhead is unable
to provide sufﬁcient value against existing mitigations. Many
speciﬁc mitigations are no longer required because a secret-
free design is a systematic defence against several categories
of vulnerability. Our evaluation demonstrates that the secret-
free design replaces said mitigations and shows competitive
or improved performance.
V. A SECRET-FREE HYPERVISOR
We construct the minimal and secret-free view of all do-
mains with the following components.
A. Tearing down the direct map
As discussed in Section II-B on page 2, the direct map
is a huge and permanent window to physical memory. Such
a surface contradicts the secret-free principle and has the
following drawbacks:
1) We observe that this is a major attack surface for a range
of architectural and speculative attacks [15], [16], [25].
It is a convenient surface for malicious parties as any
attacker-controlled out-of-bound vulnerability quickly
escalates to full memory access.
2) It consumes a signiﬁcant chunk of the address space,
limiting the degree of randomization in kernel. With
sufﬁcient amount of RAM, the entropy of the kernel
ASLR can be as low as 7 bits, requiring only 128
probes [16].
3) It becomes a dependency for future kernel development.
For example, as the code heavily depends on the direct
map window for memory access, both FreeBSD and
Xen HVM on amd64 require a sufﬁciently large virtual
address range to cover all physical memory, without
which high physical memory would be inaccessible.
4) There are no guard zone around allocations. Out-of-
bound accesses can easily corrupt data on adjacent
pages. As a result, Linux has moved to virtually mapped
kernel stacks as the default to trap and handle over-
ﬂows [34].
We introduce mapping APIs to all hypervisor memory
allocations so that dereferences are made only after explicit
mapping requests. The direct map is removed because the
code no longer assumes an implicit mapping before accessing
memory. The API differs depending on whether the alloca-
tion contains secrets. Non-secrets create and destroy globally
visible mappings on allocation and deallocation, allowing
fast access from all contexts during their lifetime. These are
typically hypervisor-internal data structures unrelated to guest
secrets, including the hypervisor image, host ISA descriptors
(for example, x86 host GDT, IDT, TSS), generic scheduler
state and so on. Access to secret memory will be isolated via
the mechanisms described in the following sections.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:47 UTC from IEEE Xplore.  Restrictions apply. 
373
A secret-free hypervisor no longer allows such private states
to be globally mapped.
We leverage the vCPU-private region and create hypervisor
stacks for each vCPU with private mappings. This ensures
that guest register spills and hypervisor handling of guest data
on the stack are invisible to other domains and even to other
vCPUs of the same domain, assisting the guest OS to further
isolate its processes. When implementing vCPU state isolation,
one must be aware of any implicit spills by the architecture
(such as the x86 VMCS page) and should not allow them to
be visible. We ﬁnd two common patterns that do not compose
with private stacks which must be addressed via global bounce
buffers.
a) Spatial bouncing: Hypervisor code may pass stack
allocations as function call arguments. The callee may not
have a mapping to the caller’s vCPU-private stack. Such a
scenario is common. For example, the callees of an Inter-
Processor Interrupt (IPI) are from different contexts and no
longer share mappings after stack isolation. We address this
using global bounce buffers which are allocated per physical
core and are globally mapped. Arguments are fetched from
the buffer instead of the caller’s stack.
Global bouncing must enforce the secret-free principle by
not storing any secrets in the buffer. When secrets are passed,
ephemeral mappings must be used instead. In practice, this
is not a concern as hypervisor IPIs typically pass function
pointers and non-secret hypervisor data, which are globally
mapped as the hypervisor image and non-secret pool anyway.
b) Temporal bouncing: Bouncing may be needed even
within the same host CPU, especially when enforcing secret
freedom on per-physical-CPU hypervisor stacks. Xen x86 64,
for example, allocates a hypervisor stack for each host core.
Context switching to another vCPU reuses the current per-
pCPU stack. This causes two problems. First, isolating the
stack mapping but still sharing the underlying memory does
not correctly enforce secret freedom. Second, after isolating
the underlying pages, the new context is unable to read the
current stack frame or perform function returns as the new
stack is empty.
For a per-pCPU stack hypervisor, we ﬁrst isolate physical
memory by implementing per-vCPU stacks. Then, we ensure
the next context does not rely on previous stack frames. The
previous context writes shared variables to the per-pCPU tem-
poral bounce buffers, allowing the context switch to complete
on a new empty stack. Similar to spatial buffers, care must be
taken to ensure no secrets are bounced, otherwise ephemeral
mappings must be used instead.
Overhead from global bounce buffers: Although these
speciﬁc patterns need to be modiﬁed, avoiding hypervisor
stack allocations as function call arguments and not sharing
the stack on context switch boundaries are further hardening
of the hypervisor. The run-time overhead of spatial bouncing
is negligible. For temporal bouncing during context switch,
the overhead is higher because using two separate stacks and
the global buffer increases cache and TLB misses.
Fig. 3. L3 table sharing for global, domain-private and vCPU-private L4
entries. dnvm denotes the vCPU from domain n with vCPU ID m. For 2-
stage address translation, the user region may live in a separate table instead
of sharing address space with the hypervisor.
B. Domain- and vCPU-private mappings
We do not permit secrets to be mapped globally. In addition
to the user half and the globally mapped hypervisor half,
we further construct two regions: domain-private and vCPU-
private mapping areas. A vCPU is scheduled on zero or one
physical CPUs (pCPU) at any given time and so vCPU-private
implies pCPU-private. The regions can be mapped to the
abundant space made available by removing the direct map.
In modern ISAs with hierarchical page tables, shared domain-
private mappings among vCPUs and shared global non-secret
mappings can be cheaply implemented via sharing lower level
tables. In 4-level paging, all L4 (root) page table entries can be
categorized as global, domain-private or vCPU-private, shown
in Fig. 3. Private regions are intended for long-lived secret data
structures belonging to the current domain or vCPU, such as