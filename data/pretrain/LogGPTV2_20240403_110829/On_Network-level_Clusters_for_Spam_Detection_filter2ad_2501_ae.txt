1
Figure 15. CDF of spam ratio for three different
clusters.
t
n
u
o
c
e
v
i
t
a
g
e
n
e
s
a
F
l
7500
0    
8
10
12
14
16
18
20
22
24
BGP prefix mask length (/8 − /24)
Figure 17. False negative count comparison for
different clusters sorted by BGP preﬁx size.
We now analyze the false negative breakdown by
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
e
t
a
r
e
v
i
t
a
g
e
n
l
e
s
a
F
0
8
BGP prefix cluster
Combined cluster
4
x 10
4.5
4
3.5
3
2.5
2
1.5
1
0.5
t
n
u
o
c
e
v
i
t
a
g
e
n
e
s
a
F
l
Combined cluster
BGP prefix cluster
10
BGP prefix size in mask length (/8 − /24)
12
14
16
18
20
22
24
0
1−29 30 60 90 120 150 180 210 240 270 300 330 360 390 420
Number of active hosts (binned by size 30)
Figure 18. False negative rate comparison for dif(cid:173)
ferent clusters sorted by BGP preﬁx size.
Figure 20. False negative count comparison for
different clusters sorted by the number of active
hosts within each cluster.
0.2
0.15
0.1
0.05
e
t
a
r
e
v
i
t
i
s
o
p
e
s
a
F
l
0
8
BGP prefix cluster
Combined cluster
10
BGP prefix size in mask length (/8 − /24)
12
14
16
18
20
22
24
Figure 19. False positive rate comparison for dif(cid:173)
ferent clusters sorted by BGP preﬁx size.
BGP preﬁx size. Figure 17 shows that most false nega-
tives are distributed in /15 - /20 where there is the most
increase in terms of the number of clusters. Combined
clusters consistently have fewer false negatives com-
pared to BGP preﬁx clusters for each preﬁx size. Ex-
amining false negative rate across preﬁxes of different
sizes, we show in Figure 18 that larger BGP preﬁxes
clearly have higher false negative rate. As discussed
before, a large BGP preﬁx can be further divided into
smaller ones for different organizations not externally
visible. As a result, we show that by combining DNS
information, we are able to signiﬁcantly reduce the false
negative rate for such large clusters.
However, as we can see, for /8 BGP preﬁxes, the false
negative rate is still around 50% using combined clus-
ters. A closer look reveals that it is caused by a large /8
BGP preﬁx belonging to MIT, which originated a non-
negligible fraction of spam. These addresses also con-
tribute to many legitimate emails. In this case, this clus-
ter is considered to be a “good” cluster with all its spam
treated as false negatives. In fact, most of the spam is
contributed by a few IP addresses which appear to be
legitimate mail servers. We suspect that these spam is
due to mail forwarding, and plan to conﬁrm it. Note that
IP-based blacklists will not block these legitimate server
IPs either.
We ﬁnd that combined clusters can also reduce false
positive rates. As we can see in Figure 19, the false pos-
itive rate at each BGP preﬁx size is reduced except for /9
preﬁxes. The reason is that originally all /9 BGP preﬁxes
are considered to be good clusters (close to 100% false
negative rate). But in fact, by splitting /9 BGP preﬁxes
into smaller combined clusters, we can separate good IP
addresses from bad ones that are originally mixed to-
gether in /9 BGP preﬁx clusters. It can greatly reduce
the false negative rate, but with a slight increase in false
positive rate.
We further examine the false negative breakdown by
the number of active hosts within each cluster in Fig-
ure 20. The X-axis shows the size of each cluster binned
by 30. For example, the ﬁrst bar shows the false nega-
tive count for clusters with host population ranging from
1 to 29. First of all, we observe that most of the false
negatives are contributed by small clusters due to the
lack of sufﬁcient history (i.e., sample size is too small).
Even a very small number of misclassiﬁed spam emails
by spam ﬁlters would signiﬁcantly bias the spam ratio
for the entire cluster. With more history, the spam ra-
tio of the clusters becomes more stable, as evidenced by
fewer false negatives for clusters of larger sizes in the
same ﬁgure. Consistent with earlier observations, com-
bined clusters can further reduce false negatives incurred
by BGP preﬁx clusters. However, the reduction is lim-
ited for clusters of smaller host populations. This is also
due to the lack of sufﬁcient history for BGP-preﬁx clus-
ters, making it more difﬁcult to further split preﬁxes into
smaller combined clusters.
6.4 Detection coverage
Previously, we have discussed using clusters to as-
sign reputation for unseen IPs. Since new IP addresses
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
e
t
a
r
s
s
M
i
0
1
2
3
4
DNS cluster
BGP prefix cluster
Combined cluster
IP
5
6
7
Weeks of training
8
9 10 11 12
Figure 21. Training time vs. miss rate
appear daily, it is important to quantify the improved de-
tection coverage of IPs by using clusters compared with
IP-based approaches. Similar to Figure 10, we ﬁxed one
day for testing and varied the training length.
For Figure 21, we deﬁne the metric miss rate to be the
number of emails whose sender IPs do not fall into any
existing cluster divided by the total number of emails.
For IP-based reputation, it is deﬁned as the number of
emails from unseen IPs divided by the total number of
emails. We exclude IPs from our university network to
avoid any bias caused by the data collection location.
Obviously, the miss rate decreases with more training
data. In particular, the miss rate for individual IPs is as
high as 60% even with 12 weeks of training. However,
clusters help reduce the miss rate to well below 20%, es-
pecially for combined clusters with a miss rate of only
about 0.6% with 12 weeks of training – two orders of
magnitude difference compared with the miss rate of IP-
based reputation. Combined clusters also have a smaller
miss rate than both BGP preﬁx clusters and DNS clus-
ters. This is explained by falling back to DNS and BGP
preﬁx cluster to obtain history information whenever a
new IP falls into a combined cluster with a lack of his-
tory. Further analysis reveals that combined clusters ac-
tually can help assign reputation for more than 93% of
the unseen IP addresses.
7 Spam detection using cluster-based rep-
utation
As discussed before, building IP-based blacklists can
be very challenging. In previous section, we have shown
how to build a ﬁne-grained cluster that can outperform
existing clusters. In this section, we evaluate the classi-
ﬁcation result of our clusters in terms of the false posi-
tive and false negative rate against existing popular IP-
based blacklists. We also integrate the cluster reputa-
tion with SpamAssassin to examine the amount of ad-
ditional spam detected and evaluate the number of new
false positives introduced. Note that our approach can
work well even with only our local vantage point, which
makes it easier to deploy. The performance in terms
of the lookup time and storage space of our system is
quite low. The average lookup time per IP is about
60ms which is sufﬁciently low to be practical. The stor-
age space required for storing information about the 2.7
million IP addresses (including the DNS information) is
about 2.2GB on disk which can easily run on any mod-
ern commodity hardware.
7.1 Comparison between cluster(cid:173)based and IP(cid:173)
based blacklists (DNSBL)
As previously described, we build the cluster-based
blacklist purely based on the aggregated spam ratio of
clusters. As demonstrated in Figure 8 with varying
threshold values, there exist trade-offs between false
positives and false negatives.
We empirically set
Note that we did not utilize any local IP-based reputa-
tion history to help further reduce false positives, thus it
is likely that our detector will have a higher false positive
rate. However, in order to compare against the IP-based
blacklist, we do consider the local history information
to reduce the false positive at the cost of some increase
in false negative rate. If an IP address falls into a brand
new cluster for which we have no history, we resort to
content-based spam ﬁlters to decide whether it is a spam.
Once we have seen enough history for the cluster, we can
make use of cluster reputation for future spam detection.
the spam ratio threshold to
0.97, 0.98 and 0.99 respectively to compare with each
DNSBL averaged over 30 different days randomly se-
lected from June to July 2009. The training data be-
gins from the ﬁrst day of our data collection to the day
before the testing day. The DNSBL we choose are
Spamhaus [9], Spamcop [8] and SORBS [7]. Table 8
illustrates the result of using DNSBL alone compared
with using only our cluster, and using the combined ap-
proach. It shows that our standalone cluster-based detec-
tor already outperforms each individual DNSBL except
SORBS which has a slightly better false positive rate but
a much worse false negative rate.
With BGP cluster alone, to maintain the same level
of false positive rate, the false negative rate will increase
to more than 20% as shown in Figure 16. Further, by in-
corporating cluster-based detection, we can detect more
than half of the spam missed by these blacklists while
maintaining comparable false positive rate. In fact, af-
ter combining our cluster-based detector with Spamcop,
it produces a detector that has signiﬁcant improvement
(both false positive and false negative) over Spamhaus
alone. To understand the false negative improvement,
we investigate scenarios where a blacklist misses bad IPs
that were caught by our cluster-based detector. One rea-
Table 7. Results of integrating cluster(cid:173)based reputation with SpamAssassin
Spam
Ratio
Score
Honeypot account
Personal account 3
Spam Ham FN Spam Ham FN Spam Ham FN Spam Ham FN
1
Personal account 2
Personal account 1
1550
11
threshold assigned 1025
0
144 3750
14143
521 1340
12231
89
0.7
0.7
0.7
0.8
0.8
0.8
0.9
0.9
0.9
FNR Matched FPI FNR Matched FPI FNR Matched FPI FNR Matched FPI
1
2
3
1
2
3
1
2
3
46
63
79
45
62
75
44
61
74
96
96
96
90
90
90
88
88
88
0
0
0
0
0
0
0
0
0
185
194
199