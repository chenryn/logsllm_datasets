### Figure 15: CDF of Spam Ratio for Three Different Clusters

The Cumulative Distribution Function (CDF) of the spam ratio for three different clusters is shown in Figure 15. This figure provides a visual representation of how the spam ratio is distributed across the clusters.

### Figure 17: False Negative Count Comparison for Different Clusters Sorted by BGP Prefix Size

Figure 17 compares the false negative counts for different clusters, sorted by BGP prefix size. The x-axis represents the BGP prefix mask length, ranging from /8 to /24. The y-axis shows the count of false negatives. The figure indicates that most false negatives are concentrated in the /15 - /20 range, where there is a significant increase in the number of clusters. Combined clusters consistently have fewer false negatives compared to BGP prefix clusters for each prefix size.

### Figure 18: False Negative Rate Comparison for Different Clusters Sorted by BGP Prefix Size

Figure 18 illustrates the false negative rate comparison for different clusters, sorted by BGP prefix size. The x-axis represents the BGP prefix size in mask length, and the y-axis shows the false negative rate. The figure demonstrates that larger BGP prefixes have higher false negative rates. By combining DNS information, we can significantly reduce the false negative rate for large clusters. However, for /8 BGP prefixes, the false negative rate remains around 50% even with combined clusters. This is due to a large /8 BGP prefix belonging to MIT, which generates a non-negligible fraction of spam but also contributes to many legitimate emails. In this case, the cluster is considered "good," and all its spam is treated as false negatives.

### Figure 19: False Positive Rate Comparison for Different Clusters Sorted by BGP Prefix Size

Figure 19 compares the false positive rates for different clusters, sorted by BGP prefix size. The x-axis represents the BGP prefix size in mask length, and the y-axis shows the false positive rate. The figure shows that the false positive rate at each BGP prefix size is reduced except for /9 prefixes. This is because originally, all /9 BGP prefixes were considered good clusters, but by splitting them into smaller combined clusters, we can separate good IP addresses from bad ones, reducing the false negative rate but slightly increasing the false positive rate.

### Figure 20: False Negative Count Comparison for Different Clusters Sorted by Number of Active Hosts

Figure 20 examines the false negative breakdown by the number of active hosts within each cluster. The x-axis shows the size of each cluster binned by 30, and the y-axis shows the false negative count. The figure indicates that most false negatives are contributed by small clusters due to insufficient history. With more history, the spam ratio of the clusters becomes more stable, as evidenced by fewer false negatives for larger clusters. Combined clusters further reduce false negatives incurred by BGP prefix clusters, but the reduction is limited for clusters with smaller host populations.

### 6.4 Detection Coverage

We previously discussed using clusters to assign reputation for unseen IPs. Since new IP addresses appear daily, it is important to quantify the improved detection coverage of IPs using clusters compared to IP-based approaches. Figure 21 shows the training time versus the miss rate, defined as the number of emails whose sender IPs do not fall into any existing cluster divided by the total number of emails. For IP-based reputation, it is the number of emails from unseen IPs divided by the total number of emails. The miss rate decreases with more training data. Specifically, the miss rate for individual IPs is as high as 60% even with 12 weeks of training. However, clusters help reduce the miss rate to well below 20%, especially for combined clusters, which achieve a miss rate of only about 0.6% with 12 weeks of training. Combined clusters also have a smaller miss rate than both BGP prefix clusters and DNS clusters, as they can fall back to DNS and BGP prefix clusters to obtain history information when a new IP lacks history. Further analysis reveals that combined clusters can help assign reputation for more than 93% of unseen IP addresses.

### 7. Spam Detection Using Cluster-Based Reputation

Building IP-based blacklists can be very challenging. In the previous section, we showed how to build fine-grained clusters that outperform existing clusters. In this section, we evaluate the classification results of our clusters in terms of false positive and false negative rates against popular IP-based blacklists. We also integrate the cluster reputation with SpamAssassin to examine the additional spam detected and the number of new false positives introduced. Our approach works well even with only a local vantage point, making it easier to deploy. The performance in terms of lookup time and storage space is quite low. The average lookup time per IP is about 60ms, and the storage space required for 2.7 million IP addresses (including DNS information) is about 2.2GB, which can easily run on any modern commodity hardware.

### 7.1 Comparison Between Cluster-Based and IP-Based Blacklists (DNSBL)

We build the cluster-based blacklist purely based on the aggregated spam ratio of clusters. As shown in Figure 8, there are trade-offs between false positives and false negatives. We empirically set the spam ratio threshold to 0.97, 0.98, and 0.99 respectively to compare with each DNSBL averaged over 30 different days randomly selected from June to July 2009. The training data begins from the first day of our data collection to the day before the testing day. The DNSBLs chosen are Spamhaus [9], Spamcop [8], and SORBS [7]. Table 8 illustrates the results of using DNSBL alone, using only our cluster, and using the combined approach. It shows that our standalone cluster-based detector already outperforms each individual DNSBL except SORBS, which has a slightly better false positive rate but a much worse false negative rate.

With BGP clusters alone, maintaining the same level of false positive rate increases the false negative rate to more than 20%, as shown in Figure 16. By incorporating cluster-based detection, we can detect more than half of the spam missed by these blacklists while maintaining a comparable false positive rate. After combining our cluster-based detector with Spamcop, it produces a detector with significant improvements in both false positive and false negative rates over Spamhaus alone. To understand the false negative improvement, we investigate scenarios where a blacklist misses bad IPs that were caught by our cluster-based detector.

### Table 7: Results of Integrating Cluster-Based Reputation with SpamAssassin

| Spam Ratio | Score | Honeypot Account | Personal Account 3 | Personal Account 2 | Personal Account 1 |
|------------|-------|------------------|--------------------|--------------------|--------------------|
| 1          | 1550  | 11               | 1025               | 0                  | 144                |
| 0.7        | 3750  | 14143            | 521                | 1340               | 12231              |
| 0.8        | 89    | 0.7              | 0.7                | 0.7                | 0.8                |
| 0.9        | 0.8   | 0.9              | 0.9                | 0.9                |                    |
| FNR        | Matched | FPI | FNR | Matched | FPI | FNR | Matched | FPI | FNR | Matched | FPI |
| 1          | 46    | 63   | 79  | 45      | 62  | 75  | 44      | 61  | 74  | 96      | 96   |
| 2          | 90    | 90   | 90  | 88      | 88  | 88  | 0       | 0   | 0   | 185     | 194  |
| 3          | 199   |      |     |         |     |     |         |     |     |         |      |

This table summarizes the results of integrating cluster-based reputation with SpamAssassin, showing the spam ratio, score, and various metrics for different accounts.