(a)
(b)
Table 2: Capacity saving with different Hose coverage
Figure 14: (a) Yearly capacity growth of Hose and Pipe, (b) 2021
capacity decrease with clean-slate planning.
Figure 15: Cost benefit of Hose measured by fiber consumption.
5 years. The projected traffic demand from our production traffic
forecaster roughly doubles every two years. Hose-based capacity
planning is more capacity-efficient in the long run. First, the relative
capacity gain of Hose is greater year by year. By 2025, it can save
17.4% capacity compared to Pipe. Second, while both Pipe and Hose
capacity scale faster than traffic growth (more capacity is needed
to account for failure scenarios), the Hose capacity increases at a
lower rate. The capacity saving of the Hose model comes from the
multiplexing gain of traffic aggregation, as discussed in Section 2.
The advantage is not obvious in the near future because the
Hose model has been in use for only a few years. Our current
topology is mostly built with Pipe-based planning, and it takes time
to become Hose-compliant. In Figure 14b, we remove this factor
by planning the network from scratch, and we show the capacity
decrease against the 2021 Pipe result in Figure 14a. In this case, Hose
can save almost 7% more capacity than Pipe. These observations
suggest evolving a network with the Hose model can reach a more
optimized network topology than evolving with the Pipe model.
Cost saving While we cannot share the proprietary cost values,
we approximate the cost benefit of Hose using the fiber pair con-
sumption. Figure 15 shows the additional percentage of fiber usage
normalized by the baseline. We observe a similar trend as the capac-
ity growth. The cost advantage manifests as the years of deployment
increase, with as high as 20% saving in four to five years.
Optimization time vs. accuracy Table 2 further investigates the
Figure 14b results with varying Hose coverage. We see even a rel-
atively low coverage of 40% achieves a large capacity saving of
8.62%. At a high coverage of 83%, the overall computation time is
an affordable 1063 minutes, or 17.7 hours. Because the DTMs are
consumed by the optimization procedure iteratively in batches, the
DTMs in later batches may already be satisfied by earlier batches.
Thus, the computation time per DTM is only a few minutes, and
further reduces given more DTMs, thanks to the batching effect.
This result highlights that our solution is scalable and insensitive to
Figure 16: Capacity saving of Hose over Pipe: per-link capacity dif-
ference relative to the 83% coverage plan.
Figure 17: CDF of the capacity variance of IP links per site.
the DTM selection when the coverage is sufficient. Figure 16 com-
pares the planned capacity per IP link for different Hose coverage
values against the 83% coverage baseline. The planning difference
is remarkable, though shrinking, as the Hose coverage improves.
Considering the good time scalability of our system, we suggest
choosing a high Hose coverage in practice, so as to avoid under-
provisioning from overly high capacity reduction such as achieved
by coverage values of 58% and 67%. The capacity saving has little
change when the coverage value is above 83%.
Capacity Distribution Figure 17 shows the standard deviation of
capacity across all the IP links at each site for Year 1 planning (2021
result in Figure 14). Capacity is distributed more uniformly in Hose.
For the Hose model, almost 70% of sites have capacity variance
less than 5Tbps, while the number is only 50% for Pipe. At 80%,
Pipe has a variance 1.5× larger than Hose. The tail of variance for
Pipe is also larger than Hose. More uniform capacity distribution
is desirable for resilience against unplanned failures and future
scaling, because more TMs can fit into even link capacities at a site.
Hose-based planning adds capacity more uniformly across links
thanks to the variant TMs it has considered.
7 OPERATIONAL EXPERIENCE
We have learned important operational lessons throughout years of
running Hose-based network planning in production. This section
reveals unexpected use cases, system adjustments, and directions
for future improvements.
557
7.1 Disaster Recovery Buffer
The concept of "disaster readiness" has been built into every aspect
of Facebook’s infrastructure [22]. Disaster refers to any catastrophic
failure that takes a long time to recover, such as hurricane, major
fire event in a DC, etc. Facebook conducts disaster recovery (DR)
exercises to test its capability under actual disasters. These DR tests
migrate requests originally sent to potentially failing DCs to other
healthier DCs. This process explores the inter-service dependencies
and dynamic resource constraints (such as compute and storage
resources) to identify the mitigation plan for each service in real
time. Each candidate mitigation plan will create a drastic shift to the
original TM. For a network planned with Pipe, it requires careful
evaluation of every TM (one for every candidate migration plan)
to certify if the current production backbone can accommodate
this changed TM. By moving to Hose-based network planning, a
planner is able to provide an upper bound on the total ingress and
egress traffic supported per DC. By looking at the current traffic
utilization, one could quantify additional traffic that can be added
to the DC without overloading the region, i.e., a planner is able to
provide deterministic DR buffers that can be used by operational
teams performing the DR exercise.
7.2 Partial Hose
Our Hose model is based on the general assumption that a service
would send traffic to any destination region. However, we find a
service may only need to communicate with a small subset of the
regions, as the service placement is limited to these regions. For
example, we have a data warehouse service that utilizes a special-
ized server type, which is only available in 4 regions. The data
warehouse traffic accounts for 75% of the total inter-region traffic
between these 4 regions. Taking service placement into considera-
tion can help us estimate DTMs more accurately. Thus, in this case,
we can create a smaller Hose, consisting of only these 4 regions,
and a larger Hose consisting of the remaining traffic to all desti-
nations. This partial Hose model gives us additional information
of the application communication patterns. However, considering
the large number of services at Facebook, we only use partial Hose
under two conditions: (1) if the traffic volume of the service is sig-
nificantly large; (2) if the service placement is inherently limited
by the hardware resource such that it is impossible to move the
service to other regions easily.
7.3 A/B Testing
Testing network plans using demand forecast and modeling assump-
tions for production network is non-trivial. The actual performance
only becomes clear several months or even years after the plan is
deployed. In practice, we rely on extensive A/B testing and man-
ual verification by experts across teams, typically from network
planning, sourcing, and deployment teams, to verify our designs.
We set up A/B testing for different network build plans. For ex-
ample, given two sets of input demands, or two different policies,
two versions of PORs will be generated. We compare key metrics
quantitatively, such as IP topology, optical fiber count, cost, flow
availability, latency, failures unsatisfied, etc. The experts then check
these multiple designs for any anomalies. Right now, our testing
strategy is largely based on engineering tribe knowledge. We en-
courage more research in this area to enable scientific A/B testing
for network build plans.
7.4 Stability of Parameter Setting
In production deployment, we find the choice of parameters, e.g.,
Hose coverage, to be stable over time. The fundamental reason is
the relative stability of traffic demand variations. The backbone
traffic is dominated by machine-to-machine traffic between DCs,
which fundamentally reflects the service placement. In production,
the service placement is relatively stable to accommodate various
infrastructure constraints pertaining to server availability, fault
tolerance requirements, and disaster recovery planning. Thus, we
observed that complete demand shifts are rare but moderate shifts
of 30-50% traffic between different regions are still common under
different failures. This leads to our engineering choice of 83% Hose
coverage, as demonstrated earlier in Section 6.
8 RELATED WORK
Hose model in Virtual Private Network (VPN) The seminal
work by Duffield et al. [9] proposes the Hose model for resource
management in VPNs. It allocates bandwidth to satisfy Hose-conformant
worst-case traffic distribution. Several follow-up work have been
developed to improve the dynamic bandwidth resizing [7, 10, 20].
Their problem formulation is fundamentally different from ours
as they allocate existing bandwidth resources to best guarantee
the Hose requirement, whereas our work designs the underlying
network to satisfy all possible traffic splits under a Hose. Our work
is more closely related to [15] which designs a tree topology to
satisfy Hose, while our solution works with general graphs.
Hose model in cloud resource sharing Hose is also used to
model demands in DCs [4] and the cloud environment [4, 8, 14, 17–
19]. These work use the Hose model for per-VM traffic demand and
use a big virtual switch to abstract the network fabric. For instance,
Oktopus [4] proposes a VM placement algorithm based on the Hose
constraints of any two sets of VMs. The demand between the two
VM sets is determined by the sum of all VMs’ Hose demands in
each set. This model essentially adds up all the worst-case TMs
and results in significant over-provisioning. Our approach is more
efficient because we use an operationally effective slack factor
(Section 4.3) to choose hard-enough TMs, but not the worst-case
TMs, and the resulting multiplexing gain has been demonstrated in
production (Section 6.2).
Network planning Scenario-based planning copes with traffic
uncertainty by using forecast results for a few given network sce-
narios, and each scenario emphasizes on a set of TMs [23]. Our
selection of TMs is more general, not limited by any pre-defined
scenarios. Zhang et al. proposes to find critical TMs by clustering
for general network analysis applications [24]. However, their work
is not tailored for network planning. We are interested in applying
their algorithm to network planning and comparing the efficacy
against our DTM selection algorithm. Little has been revealed about
production network planning except for a brief introduction in [5].
To the best of our knowledge, we are the first to describe real-world
network planning in detail.
558
9 CONCLUSION
Network planning plays an important role in long-term network
evolvement and service growth. In this paper, we demonstrate the
effectiveness of using the Hose model for network planning by
leveraging its multiplexing gain to simultaneously save capacity
and absorb traffic uncertainty. We share the experience of planning
a production backbone over several years. Our work sheds light on
the potential of Hose in a new problem domain, network planning,
in the hope of stimulating more research in this area.
REFERENCES
[1] [n. d.]. FICO Xpress Optimization. ([n. d.]). https://www.fico.com/en/products/
fico-xpress-optimization
[2] [n. d.]. TAO: The power of the graph. ([n. d.]). https://engineering.fb.com/2013/
06/25/core-data/tao-the-power-of-the-graph/
[3] R. Andersen, F. Chung, A. Sen, and G. Xue. 2004. On Disjoint Path Pairs with
Wavelength Continuity Constraint in WDM Networks. IEEE INFOCOM (2004).
[4] Hitesh Ballani, Paolo Costa, Thomas Karagiannis, and Ant Rowstron. 2011. To-
wards Predictable Datacenter Networks. In Proceedings of the ACM SIGCOMM
2011 Conference. Association for Computing Machinery, New York, NY, USA, 12.
[5] Ajay Kumar Bangla, Alireza Ghaffarkhah, Ben Preskill, Bikash Koley, Christoph
Albrecht, Emilie Danna, Joe Jiang, and Xiaoxue Zhao. 2015. Capacity Planning
for the Google Backbone Network. In ISMP 2015 (International Symposium on
Mathematical Programming).
[6] C Bradford Barber, David P Dobkin, and Hannu Huhdanpaa. 1996. The Quickhull
Algorithm for Convex Hulls. ACM Transactions on Mathematical Software (TOMS)
22, 4 (1996), 469–483.
[7] Haesun Byun and Meejeong Lee. 2007. Extensions to P2MP RSVP-TE for VPN-
specific State Provisioning with Fair Resource Sharing. Comput. Commun. 30, 18
(Dec. 2007), 3736–3745.
[8] Mosharaf Chowdhury, Yuan Zhong, and Ion Stoica. 2014. Efficient Coflow Sched-
uling with Varys. 44, 4 (2014).
[9] N. G. Duffield, P. Goyal, A. Greenberg, P. Mishra, K. K. Ramakrishnan, and J. E. V.
der Merwe. 1999. A Flexible Model for Resource Management in Virtual Private
Networks. ACM Sigcomm, San Diego, California, USA (1999).
[10] Friedrich Eisenbrand and Edda Happ. 2006. Provisioning a Virtual Private Net-
work Under the Presence of Non-communicating Groups. In Proceedings of the
6th Italian Conference on Algorithms and Complexity (CIAC’06). 105–114.
[11] Shimon Even, Alon Itai, and Adi Shamir. 1975. On The Complexity of Time Table
and Multi-commodity Flow Problems. In 16th Annual Symposium on Foundations
of Computer Science (sfcs 1975). IEEE, 184–193.
[12] Uriel Feige. 1998. A Threshold of ln n for Approximating Set Cover. Journal of
the ACM (JACM) 45, 4 (1998), 634–652.
[13] J. A. Fingerhut, S. Suri, and J. S. Turner. 1997. Designing Least-cost Nonblocking
Broadband Networks. Journal of Algorithms 24, 2 (Aug. 1997), 287–309.
[14] Albert Greenberg, James R. Hamilton, Navendu Jain, Srikanth Kandula,
Changhoon Kim, Parantap Lahiri, David A. Maltz, Parveen Patel, and Sudipta
Sengupta. 2009. VL2: A Scalable and Flexible Data Center Network. In Proceedings
of the ACM SIGCOMM 2009 Conference on Data Communication. Association for
Computing Machinery, New York, NY, USA, 12.
[15] Anupam Gupta, Jon Kleinberg, Amit Kumar, Rajeev Rastogi, and Bulent Yener.
2001. Provisioning a Virtual Private Network: A Network Design Problem for
Multicommodity Flow. In Proceedings of the Thirty-third Annual ACM Symposium
on Theory of Computing (STOC ’01). 389–398.
[16] M Rashidul Islam and M Hanif Chaudhry. 1998. Modeling of Constituent Trans-
port in Unsteady Flows in Pipe Networks. Journal of Hydraulic Engineering 124,
11 (1998), 1115–1124.
[17] Simon Kassing, Asaf Valadarsky, Gal Shahaf, Michael Schapira, and Ankit Singla.
2017. Beyond Fat-Trees without Antennae, Mirrors, and Disco-Balls. In Proceed-
ings of the Conference of the ACM Special Interest Group on Data Communication.
Association for Computing Machinery, New York, NY, USA.
[18] Lucian Popa, Gautam Kumar, Mosharaf Chowdhury, Arvind Krishnamurthy,
Sylvia Ratnasamy, and Ion Stoica. 2012. FairCloud: Sharing the Network in Cloud
Computing. SIGCOMM Comput. Commun. Rev. 42, 4 (Aug. 2012).
[19] Henrique Rodrigues, Jose Renato Santos, Yoshio Turner, Paolo Soares, and Dor-
gival Guedes. 2011. Gatekeeper: Supporting Bandwidth Guarantees for Multi-
Tenant Datacenter Networks. In Proceedings of the 3rd Conference on I/O Virtual-
ization. USENIX Association, USA.
[20] Thomas Rothvoß and Laura Sanità. 2009. On the Complexity of the Asymmetric
VPN Problem. In Proceedings of the 12th International Workshop and 13th Interna-
tional Workshop on Approximation, Randomization, and Combinatorial Optimiza-
tion. Algorithms and Techniques (APPROX ’09 / RANDOM ’09). Springer-Verlag,
Berlin, Heidelberg, 326–338. https://doi.org/10.1007/978-3-642-03685-9_25
[21] Daniel Semrau and Polina Bayvel. 2018. The Gaussian Noise Model in the
Presence of Inter-channel Stimulated Raman Scattering. Journal of Lightwave
Technology 36, 14 (2018), 3046–3055.
[22] Kaushik Veeraraghavan, Justin Meza, Scott Michelson, Sankaralingam Panneer-
selvam, Alex Gyori, David Chou, Sonia Margulis, Daniel Obenshain, Shruti Pad-
manabha, Ashish Shah, Yee Jiun Song, and Tianyin Xu. 2018. Maelstrom: Mit-
igating Datacenter-level Disasters by Draining Interdependent Traffic Safely
and Efficiently. In 13th USENIX Symposium on Operating Systems Design and
Implementation (OSDI 18). USENIX Association, Carlsbad, CA, 373–389.
[23] Shu Zhang and Ulrich Killat. 2011. Multiple-layer Network Planning with
Scenario-based Traffic Forecast. In Proceedings of the 17th International Con-
ference on Energy-aware Communications. 77–88.
[24] Y. Zhang and Z. Ge. 2005. Finding Critical Traffic Matrices. In 2005 International
Conference on Dependable Systems and Networks (DSN’05). 188–197.
559