private for λ =
sensitivity of a set of Qs. Formally, ∆2(Q) is the smallest
number such that for all Q, Q(cid:48) ∈ Q, |Q − Q(cid:48)|2 ≤ ∆2(Q).
B. d∗-private Mechanism
Xiao et al. [72] leveraged d-privacy with a particular
distance metric d∗ on one-dimensional time series. Let x and
x(cid:48) denote two time series. The d∗ metric was deﬁned as:
|(x[i] − x[i − 1]) − (x(cid:48)[i] − x(cid:48)[i − 1])|
d∗(x, x(cid:48)) =
(cid:88)
i≥1
To achieve d∗-privacy, Xiao et al. [72] extended a mech-
anism from Chan et al. [11] to implement a d∗-private mech-
anism as follows: Let N denote the natural numbers and
D(i) ∈ N denote the largest power of two that divides i;
i.e., D(i) = 2j if and only if 2j|i and 2j+1(cid:54) | i. Note that
i = D(i) if and only if i is a power of two. The mechanism
A computes a noised value ˜x[i] that is used in place of x[i]
using the recurrence
˜x[i] = ˜x[G(i)] + (x[i] − x[G(i)]) + ri
where x[0] = ˜x[0] = 0, and
 0
 Lap(cid:0) 1
(cid:1)
(cid:16)(cid:98)log2 i(cid:99)
i/2
i − D(i)
Lap

(cid:17)

if i = 1
if i = D(i) ≥ 2
if i > D(i)
if i = D(i)
otherwise
G(i) =
ri ∼
a front-end of the Youtube server or as an extension of the
browser. The attacker utilizes features (e.g., bytes per second,
packets per second or burst series) of the request and response
packets of the MPEG-DASH video streams as side-channel
vectors.
Without loss of generality, the problem can be simpliﬁed
and abstracted as the following classiﬁcation problem: An
encrypted video stream can be modeled as a sequence of 2-
tuples {(ti, si)}i≥0, where (ti, si) represents a video segments
of size si that is downloaded at time ti. As ti is a times-
tamp represented in continuous time, the adversary needs to
discretize the sequence of 2-tuples by grouping all 2-tuples
falling in the same time window of length w (e.g., as small
as a microsecond or as large as a second) into a single value.
As such, each video stream is represented as a time series
x = {bj}j≥0, where bj is the total size of the downloaded
video during time slot j. We let X denote the space of all
possible such x values. Often, the attacker will collect feature
vectors x and their associated labels in a training phase,
to build a machine learning model to which it will apply
observations x seen during his attack.
The goal of the defender is to prevent the videos from
being identiﬁed by the attacker, which is achieved by adding
random noise. The workﬂow of defense and attacks is depicted
in Fig. 3. Speciﬁcally, the defender takes the following steps
to reduce the information leakage. First, she sets a window
size w to convert the 2-tuples (ti, si) into a ﬁx-length time
series x. Then, she adds random noise, which is dictated
by the differentially private mechanisms, to the time series,
and generates the noised time series ˜x. When the noised
time series ˜x is reﬂected as packets, we assume all packets
are transmitted instantaneously; depending on the maximum
packet size allowed by the physical network layer, it can be
represented as a sequence of 2-tuples (˜ti, ˜si), which are what
the attacker observes. Note that the mapping from the time
series to the sequence of two tuples is only determined by the
network condition which is agnostic to the content of the video.
The attacker then chooses his window size (wA) to generate a
new time series, denoted as ˙x, and performs classiﬁcation on
˙x.
As such, when used to obfuscate streaming trafﬁc, both
differentially private mechanisms, FPAk and d∗, require two
parameters, w and . Here, w represents the window size,
which also determines the length of ˜x. For example, w = 1s
means each element of the noised time series represents the
total size of downloaded video segments within an interval of
1s. Parameter  speciﬁes the privacy level of the mechanism:
the smaller the  is, the better the privacy would be.
When wA is different from w, ˜x and ˙x may have different
lengths. As a result, the attacker may need to merge/split
bins in ˜x to create ˙x. Here, we let ˜x be a time series of
n elements and ˙x be a time series of nA elements. Without
loss of generality, we only consider cases where wA mod w
= 0 or w mod wA = 0. The merging and splitting of bins are
performed as follows:
• Merging is required when wA > w. Let r = wA/w. Every
r bins from ˜x will be merged (summed) into one bin in ˙x,
(1)
(2)
(3)
It was proven by Xiao et al. [72] that the algorithm in
Eqns. 1–3 is (d∗, 2)-private and (l1, 4)-private.
C. Applying Privacy Mechanisms on Streaming Data
The attack scenario in Sec. III motivates the following
scenario. A user who watches a Youtube video in a web
browser wishes to hide the content of the video. An attacker
sitting on the network (e.g., Internet service provider or local
network administrator) aims to infer the content of the video
by observing only side-channel information. The defender is
a network proxy placed between the content provider (i.e.,
Youtube) and the browser, which obfuscates the network
ﬂows from/to the content provider to defeat the ﬁngerprinting
attacks. For example, the defender could be implemented as
6
Fig. 3: Abstraction of data ﬂow with defense.
i.e.,
i×r+(r−1)(cid:88)
j=i×r
˜x[j]
˙x[i] =
For instance, when wA = 2s and w = 1s, r = 2, nA = 1
˙x[i] = ˜x[2i] + ˜x[2i + 1].
2 n.
• Splitting is required when wA < w. Let r = wA/w. Here
we assume that the volume of each bin follows uniform
distribution. Therefore, every bin from ˜x will be split
(divided) evenly into 1/r bins in ˙x, i.e.,
,··· ,
˙x[j] = r × ˜x[i], j =
i + 1
− 1
i
r
r
For instance, when wA = 1s and w = 2s, r = 1
˙x[2i] = ˙x[2i + 1] = 1
2 ˜x[i].
2, nA = 2n.
VI. EVALUATION
√
In this section, we evaluate the security and utility of FPAk
and d∗. We implemented both mechanisms in Python. For
FPAk, k was set to 10, so during the Fourier transformation,
only the ﬁrst 10 Fourier coefﬁcients were kept. FPAk took
a sequence of 2-tuples and parameter w and  as input, dis-
cretized it into a time series x with window size w, calculated
10∆2(Q)/ (where ∆2(Q) denoted the L2 sensitivity
λ =
of the set of 40 videos collected in Sec. III), and returned
another time series ˜x of the same size after adding noise by
following the steps mentioned in Sec. V-A. Similarly, in our
implementation of d∗, it took a sequence of 2-tuples, w and 
as input, discretized it into a time series x with window size
w, and outputted another time series ˜x after adding noise.
The two methods were applied on the 40 × 100 traces
collected in Sec. III. In our experiment, we used  ={5×10−8,
5 × 10−7, ··· , 50}, w = {0.05s, 0.25s, 0.5s, 1s, 2s}, so
there were 50 pairs in total. Each element of the noised time
series was truncated by a clip bound of [0, 1GB] to avoid
negative volume or enormous volume, because the download
size cannot be negative and it is not realistic to complete
downloading a large chunk of data within a small window
size. Therefore, values less than 0 were changed to 0, and
values larger than 1GB were truncated to 1GB.
(a) FPAk
(b) d∗
Fig. 4: Classiﬁcation accuracy of 5-fold cross validation when
trained with original traces and tested with noised traces.
7
A. Security Evaluation
The security of the differentially private mechanisms are
evaluated by classiﬁcation accuracy. We used the same method
mentioned in Sec. III-C to preprocess the data and train the
classiﬁers. According to the dataset used for training and
testing, we consider the following cases:
1) Trained with x (clean data), tested with ˜x (noised data):
To compare with the defense mechanism of using adversarial
samples described in Sec. IV-B, we ﬁrst used the same 5
classiﬁers trained with original traces to classify the noised
data generated by the two mechanisms with different choices
of , when w = 0.25s. The classiﬁcation accuracy and standard
deviation of a 5-fold cross validation are shown in Fig. 4.
For all the data points, the standard deviation is quite small
(<0.01), hardly visible in the ﬁgures. For FPAk (Fig. 4a), since
it involved the Fourier transformation, the new traces were
totally different from the originals, so the classiﬁer could not
recognize them for all  values. For d∗, since the noise was
added upon the original trace,  played an important role.
As shown in Fig. 4b, for  ≤ 5 × 10−6, d∗ was effective.
When  ≥ 5 × 10−5, the noise added was not enough to
deceive the classiﬁers. These results suggest that with properly
selected noise level, both mechanisms can effectively defeat
trafﬁc analysis attacks. In the following, we consider a more
powerful adversary that could adapt by training the classiﬁers
also with noised data.
2) Trained with ˜x (noised data), tested with ˜x (noised
data): We evaluated how the two parameters, w and , would
affect the security of the defense mechanisms by using the
CNN classiﬁer mentioned in Sec. III-C as the adversary and
measuring the accuracy of the classiﬁcation. We speciﬁcally
consider two scenarios: wA = w and wA (cid:54)= w.
• wA = w. First, we consider the scenario where the attacker
and the defender use the same w, which means that ˜x = ˙x. We
altered w to see how it would affect the classiﬁcation accuracy.
The results of the classiﬁcation accuracy and standard devia-
tion of a 5-fold cross validation when  = [0.05, 0.5, 5, 50]
are shown in Fig. 5. The classiﬁcation accuracy with FPAk
protected data is shown in Fig. 5a. When  was small (e.g.,
 = 0.05 and  = 0.5), more noise was added during the
transformation. The classiﬁcation accuracy remained low as
w increased. However, when  was larger (e.g.,  = 5 and
 = 50),
the noise level was low and w played a more
signiﬁcant role—when w = 2s, the classiﬁcation accuracy
went down by about 15%. This is because larger window
sizes (used by the adversary during discretization) erased some
important features in the data traces, making the classiﬁcation
harder. The classiﬁcation accuracy with d∗ protected data is
shown in Fig. 5b. With smaller  values (e.g.,  = 5 × 10−8
and  = 5 × 10−7), w still had no impact on the classiﬁ-
cation accuracy at all. A different trend was observed when
 = 5 × 10−6: w = 2s would increase the accuracy to
about 25%. We conjecture it was related to the mechanism by
which d∗ added noise: The amount of noise added had a linear
−8−7−6−5−4−3−2−101log10(ε/5)0.00.20.40.60.81.0AccuracyCNNSVMLRRFNeural Net−8−7−6−5−4−3−2−101log10(ε/5)0.00.20.40.60.81.0AccuracyCNNSVMLRRFNeural Net(a) FPAk
(b) d∗
Fig. 5: wA = w: effect of w
For example, when w = 0.05s, to make sure the classiﬁer had a
baseline accuracy (i.e., 2.5%, given 40 videos with 100 traces
each), d∗ needed  ≤ 5 × 10−6, while FPAk only required
 ≤ 0.5. This is because the deﬁnitions of  in the two methods
are different. We also provide a proof to bridge the two  values
in Appendix A.
• wA (cid:54)= w. Next, we consider the scenario where the attacker
and the defender chose different w. To perform the experiment,
ﬁrst, we set  ={5 × 10−8, 5 × 10−7, ··· , 50}, respectively.
Then, we let w = {0.05s, 0.25s, 0.5s, 1s, 2s}, and tested the
classiﬁcation accuracy when wA = {0.05s, 0.25s, 0.5s, 1s, 2s}.
We only show the results when w = 0.05s and w = 2s
in Fig. 7. We can see from the ﬁgure that with the same
w, when wA increased, the classiﬁcation accuracy for both
methods decreased. The amount of decrease with d∗ was more
signiﬁcant than FPAk. From this result, it can be inferred that
choosing a smaller wA would beneﬁt the adversary. This is
because the larger window size used by the adversary during
discretization erased some important features in the data traces.
From the defender’s perspective, the choice of w made
a difference in the effectiveness of the defense. For FPAk,
w = 0.05s and w = 2s did not differ much (Fig. 7a and
Fig. 7b). But for d∗, w mattered: for w = 0.05s (Fig. 7c),
 = 5 × 10−6 was good enough to fool the classiﬁer; but for
w = 2s (Fig. 7d),  = 5 × 10−6, the classiﬁer can achieve
an accuracy of 40% when wA ≤ 0.5s. Therefore, from the
defender’s perspective, if the d∗ method is chosen, it is better
to choose a smaller w to achieve better privacy.
B. Utility Evaluation
We deﬁne two metrics, waste and deficit, to evaluate
the utility of the mechanisms. Let the original time series be
x and noised time series be ˜x. Consider the cumulative traces
1 ˜x. waste and deficit are deﬁned
1 x and B =(cid:80)n
A =(cid:80)n
as follows:
• We deﬁne waste as the maximum difference between
traces A and B when the noised trace B is above the
(a) FPAk: w = 0.05s
(b) FPAk: w = 2s
(c) d∗: w = 0.05s
(d) d∗: w = 2s
Fig. 6: wA = w: effect of 
relationship with the length of the time series. When w was
large, with the video length remaining the same, the time series
had fewer elements. Therefore, the noise added was less, which
was not enough to confuse the classiﬁer. When  = 5 × 10−5,
the classiﬁcation accuracy ﬂuctuated as w increases from 0.05
to 2. We believe this was the combined result of two causes: the
larger window size reduced the noise level, but also eliminated
some of the useful information used by the classiﬁers. For both
methods, the standard deviation of each data point was very
small (less than 0.01).
Next, we study the effect of . The result is shown in
Fig. 6. The x axis is log10(/5) (e.g., x = −3 means that
 = 5 × 10−3). We only show the cases of w = 0.05s
and w = 2s, since they were the smallest and largest w
values we experimented with; result of other w values were
similar. Similar to Fig. 5, the standard deviations in Fig. 6 were
negligible. From Fig. 6, we can see that in order to keep a low
classiﬁcation accuracy, d∗ method required a much smaller .
8
0.050.250.512w (s)0.00.20.40.60.81.0Accuracyε=0.050.050.250.512w (s)0.00.20.40.60.81.0Accuracyε=0.50.050.250.512w (s)0.00.20.40.60.81.0Accuracyε=50.050.250.512w (s)0.00.20.40.60.81.0Accuracyε=500.050.250.512w (s)0.00.20.40.60.81.0Accuracyε=5e-80.050.250.512w (s)0.00.20.40.60.81.0Accuracyε=5e-70.050.250.512w (s)0.00.20.40.60.81.0Accuracyε=5e-60.050.250.512w (s)0.00.20.40.60.81.0Accuracyε=5e-5-8-7-6-5-4-3-2-101log10(ε/5)0.00.20.40.60.81.0Accuracy-8-7-6-5-4-3-2-101log10(ε/5)0.00.20.40.60.81.0Accuracy-8-7-6-5-4-3-2-101log10(ε/5)0.00.20.40.60.81.0Accuracy-8-7-6-5-4-3-2-101log10(ε/5)0.00.20.40.60.81.0AccuracyBut when  increased, waste would decrease, since there was
less noise added. Similarly, for d∗,  was the major factor that
affected the waste (see Fig. 9b). However, w also had an
inﬂuence: When  was ﬁxed, larger w indicated fewer waste
for d∗. We conjecture it was again related to the mechanism
by which d∗ added noise. The amount of noise added had a
linear relationship with the length of the series. When w was