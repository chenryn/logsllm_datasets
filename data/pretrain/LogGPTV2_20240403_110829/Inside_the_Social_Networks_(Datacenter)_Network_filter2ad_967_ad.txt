(b) Cache leader
(c) Web servers
Figure 10: Heavy-hitter stability as a function of aggregation for 1/10/100-ms time windows
tions that potentially could be treated differently by a trafﬁc
engineering scheme. For each host, we consider outbound
trafﬁc rates per destination rack (normalized to the median
rate for that rack), and track the rate over time for each rack.
Figure 8c plots these distributions for the outbound trafﬁc for
the same cache machine as Figure 8b. Each series represents
a single destination; a near vertical series represents a desti-
nation rack where the rate does not deviate far from the me-
dian rate. We ﬁnd that per-destination-rack ﬂow sizes are re-
markably stable across not only seconds but intervals as long
as 10 seconds (not shown) as well. All of the ﬂows are within
a factor of two of their median size in approximately 90%
of the 1-second intervals—the median ﬂow exhibits “signif-
icant change” in only 45% of the 1-second intervals accord-
ing to the 20% deviation cutoff deﬁned by Benson et al. [14].
Contrast this to the trafﬁc leaving a Hadoop node—which is
not load balanced—where the middle 90% of ﬂows can vary
in size by over six orders of magnitude compared to their
median size in the trace (not shown).
Such stability, both over time and by destination, is the
result of a combination of workload characteristics and en-
gineering effort. To a cache system, the offered load per sec-
ond is roughly held constant—large increases in load would
indicate the presence of relatively hot objects, which is ac-
tively monitored and mitigated. Bursts of requests for an ob-
ject lead the cache server to instruct the Web server to tem-
porarily cache the hot object; sustained activity for the ob-
ject leads to replication of the object or the enclosing shard
across multiple cache servers to help spread the load. We
note further that the request rate distribution for the top-50
most requested objects on a cache server is close across all
cache servers, and that the median lifespan for objects within
this list is on the order of a few minutes. Per-destination traf-
ﬁc stability is again a consequence of user request multiplex-
ing across all available Web servers, coupled with relatively
small request/response pairs.
5.3 Heavy hitters
In this section, we examine the behavior of trafﬁc at
sub-second timescales to better understand its stability and
whether trafﬁc engineering can apply. In particular, we wish
to see if certain ﬂows (aggregated or not) stand out in terms
of rate, since such ﬂows would provide the largest opportu-
nity for potential impact on network performance. We de-
Type
Web
Cache (f)
Cache (l)
Hadoop
p10
1
1
1
8
8
7
1
1
1
1
1
1
f
h
r
f
h
r
f
h
r
f
h
r
Number
p50
4
4
3
19
19
15
16
8
7
2
2
2
p90
15
14
9
35
33
23
48
25
17
3
3
3
Size (Mbps)
p90
47.3
48.1
48.9
22.5
23.6
31.0
408
414
427
1392
1392
1392
p10
1.6
1.6
1.7
5.1
8.4
8.4
2.6
3.2
5
4.6
4.6
4.6
p50
3.2
3.3
4.6
9.0
9.7
14.5
3.3
8.1
12.6
12.7
12.7
12.7
Table 4: Number and size of heavy hitters in 1-ms intervals
for each of ﬂow(f), host(h), and rack(r) levels of aggregation.
ﬁne a set of ﬂows that we call heavy hitters, representing
the minimum set of ﬂows (or hosts, or racks in the aggre-
gated case) that is responsible for 50% of the observed traf-
ﬁc volume (in bytes) over a ﬁxed time period. Intuitively,
the presence of heavy hitters can signify an imbalance that
can be acted upon—if they are persistent for enough time,
and large enough compared other ﬂows that treating them
differently makes a difference.
Table 4 shows statistics regarding the number and size of
the heavy hitters that constitute 50% of the trafﬁc in 1-ms
intervals for each of the four server classes. Because we are
interested in instantaneously large ﬂows, we measure size
in terms of rate instead of number of bytes sent over the
lifetime of the ﬂow. Next, we consider the the lifespan of
heavy hitters, aggregated by 5-tuple, destination host and
rack, and measured across intervals of 1, 10 and 100 mil-
liseconds. Figure 10 shows the fraction of the heavy hitters
that remain in subsequent time intervals. We do not show the
Hadoop nodes, as our heavy-hitter deﬁnition almost always
results in the identiﬁcation of 1–3 heavy hitters at each of
ﬂow, host, and rack aggregation levels across all three time
intervals.
Heavy hitter persistence is low for individual ﬂows (red):
in the median case, no more than roughly 15% of ﬂows
persist regardless of the length of period, a consequence of
the internal burstiness of ﬂows noted earlier. Host-level ag-
gregation (green) fares little better; with the exception of
0.010.020.030.040.050.060.070.080.090.0100.0Heavyhitterstabilitybetweenintervals(%)0.00.10.20.30.40.50.60.70.80.91.0CDFFlows,1-msbinFlows,10-msbinFlows,100-msbinHosts,1-msbinHosts,10-msbinHosts,100-msbinRacks,1-msbinRacks,10-msbinRacks,100-msbin0.010.020.030.040.050.060.070.080.090.0100.0Heavyhitterstabilitybetweenintervals(%)0.00.10.20.30.40.50.60.70.80.91.0CDFFlows,1-msbinFlows,10-msbinFlows,100-msbinHosts,1-msbinHosts,10-msbinHosts,100-msbinRacks,1-msbinRacks,10-msbinRacks,100-msbin0.010.020.030.040.050.060.070.080.090.0100.0Heavyhitterstabilitybetweenintervals(%)0.00.10.20.30.40.50.60.70.80.91.0CDFFlows,1-msbinFlows,10-msbinFlows,100-msbinHosts,1-msbinHosts,10-msbinHosts,100-msbinRacks,1-msbinRacks,10-msbinRacks,100-msbin132an upper bound on the effectiveness of trafﬁc engineering—
a signiﬁcant amount of ephemeral heavy hitter trafﬁc would
go unseen and untreated by the TE scheme. Second, it serves
as an indicator of the difﬁculty of prediction in the ﬁrst place;
if a one-second prediction interval is not sufﬁcient, smaller
timescales (consuming more resources) may be needed. Fi-
nally, this metric is an indicator of burstiness, as it indicates
the presence of a large number of ephemeral heavy hitters.
Figure 11 plots a CDF of the fraction of a second’s
overall heavy hitters that are instantaneously heavy in each
1/10/100-ms interval within the second. We show results for
a Web server and cache follower—cache leaders are similar.
At 5-tuple granularity, predictive power is quite poor, at less
than 10–15%. Rack-level predictions are much more effec-
tive, with heavy hitters remaining heavy in the majority of
100-ms intervals in the median case for both services. Host-
level predictions are more useful for Web servers than cache
nodes, but only the 100-ms case is more than 30% effective.
5.4 Implications for trafﬁc engineering
Facebook’s extensive use of connection pooling leads to
long-lived ﬂows that seem like potential candidates for traf-
ﬁc engineering. These same services use application-level
load balancing to great effect, however, leaving limited room
for in-network approaches. Many existing techniques work
by identifying heavy hitters and then treating them differ-
ently (e.g., provisioning a circuit, moving them to a lightly
loaded path, employing alternate buffering strategies, etc.).
For any such scheme to work, however, it must be possible to
ﬁrst identify the heavy hitters, and then realize some beneﬁt.
Unfortunately, it appears challenging to identify heavy
hitters in a number of Facebook’s clusters that persist with
any frequency. Moreover, even for the timescales and ag-
gregation levels where it is possible (e.g., rack-level ﬂows
over intervals of 100-ms or larger), it is not clear there is
a great deal of beneﬁt to be gained, as the heavy hitters
are frequently not particularly heavy for the vast majority
of the period. Previous work has suggested trafﬁc engi-
neering schemes can be effective if 35% of trafﬁc is pre-
dictable [14]; only rack-level heavy hitters reach that level
of predictability for either Web or cache servers. This some-
what counter-intuitive situation results from a combination
of effective load balancing (which means there is little dif-
ference in size between a heavy hitter and the median ﬂow)
and the relatively low long-term throughput of most ﬂows,
meaning even heavy ﬂows can be quite bursty internally.
6. SWITCHING
Finally, we study aspects of the trafﬁc that bear directly on
top-of-rack switch design. In particular, we consider the size
and arrival processes of packets, and the number of concur-
rent destinations for any particular end host. In addition, we
examine the impact of burstiness over short timesales and its
impact on switch buffering.
6.1 Per-packet features
Figure 12 shows the distribution of packet sizes for each
of the four host types. Overall, the median packet size is
(a) Web server
(b) Cache follower
Figure 11: Intersection between heavy hitters in a subinter-
val with enclosing second
destination-host-level aggregation for Web servers, no more
than 20% of heavy hitter hosts in a sub-second interval will
persist as a heavy hitter in the next interval. Web servers
have a higher rate over 100-millisecond periods since they
have a relatively small number of cache servers and load bal-
ancers with which they communicate, while cache servers
converse with many different Web servers.
It is not until considering rack-level ﬂows (blue) that
heavy hitters are particularly stable.
In the median case,
over 40% of cache heavy hitters persist into the next 100-
ms interval, and almost 60% for Web servers. Heavy hitters
from Web servers are more stable in general, with 32% of
rack-level heavy hitters persisting in the median 1-ms inter-
val case. Even so, heavy hitter persistence is not particularly
favorable for trafﬁc engineering. With a close to 50% chance
of a given heavy hitter continuing in the next time period,
predicting a heavy hitter by observation is not much more
effective than randomly guessing.
Even if one could perfectly predict the heavy hitters on
a second-by-second timescale, it remains to consider how
useful that knowledge would be. We compare the heavy hit-
ters from enclosing one-second intervals to the instantaneous
heavy hitters from each of the subintervals within the second
to see what fraction of the heavy hitters in a subinterval are
heavy hitters across the entire enclosing second. A limited
degree of overlap implies three things: First, it establishes
0.010.020.030.040.050.060.070.080.090.0100.0Intersectionofheavyhittersinanintervalandenclosingsecond(%)0.00.10.20.30.40.50.60.70.80.91.0CDFFlows,1-msbinFlows,10-msbinFlows,100-msbinHosts,1-msbinHosts,10-msbinHosts,100-msbinRacks,1-msbinRacks,10-msbinRacks,100-msbin0.010.020.030.040.050.060.070.080.090.0100.0Intersectionofheavyhittersinanintervalandenclosingsecond(%)0.00.10.20.30.40.50.60.70.80.91.0CDFFlows,1-msbinFlows,10-msbinFlows,100-msbinHosts,1-msbinHosts,10-msbinHosts,100-msbinRacks,1-msbinRacks,10-msbinRacks,100-msbin133Figure 12: Packet size distribution
Figure 14: Flow (SYN packet) inter-arrival
(a) 15 ms
(b) 100 ms
Figure 13: Hadoop trafﬁc is not on/off at 15 nor 100 ms
approximately 250 bytes, but that is signiﬁcantly skewed by
Hadoop trafﬁc. Hadoop trafﬁc is bimodal: almost all packets
are either MTU length (1500 bytes for the servers we study)
or TCP ACKs. Packets for the other services have a much
wider distribution, but the median packet size for all of them
is signiﬁcantly less than 200 bytes, with only 5–10% of the
packets fully utilizing the MTU.
Thus, while link utilization is low, the packet rate is still
high. For example, a cache server at 10% link utilization
with a median packet size of roughly 175 bytes generates
85% of the packet rate of a fully utilized link sending MTU-
sized packets. As a result, any per-packet operation (e.g.,
VLAN encapsulation) may still be stressed in a way that the
pure link utilization rate might not suggest at ﬁrst glance.
6.2 Arrival patterns
Benson et al. observe that packet arrivals exhibit an on/off
pattern at the end-host level [12, 13]. Hosts in Facebook’s
datacenter do not exhibit this behavior, even within Hadoop
clusters. Figure 13 shows a time series of trafﬁc sent by a
Hadoop host (arriving at a RSW port) binned by 15- and
100-ms intervals. (c.f. Benson et al.’s analogous graphs [13,
Figure 5] and [12, Figure 6]).
If one considers trafﬁc on
a per-destination host basis, on/off behavior remerges (not
shown), suggesting its disappearance may be due to a large
number of concurrent destinations.
Figure 14 plots the CDF of inter-arrival times between
outgoing TCP ﬂows at each of the four types of servers we
study. While a signiﬁcant amount of trafﬁc is routed over
long-lived pooled connections, as is the case for request-
response trafﬁc between Web servers and cache followers,
ephemeral ﬂows do exist. The inter-arrival periods for ﬂows
emanating from all four classes of host are shorter than
those reported in the literature [26, Fig. 11], but to vary-
ing degrees. Hadoop nodes and Web servers see an order-
of-magnitude increase in ﬂow intensity relative to previous