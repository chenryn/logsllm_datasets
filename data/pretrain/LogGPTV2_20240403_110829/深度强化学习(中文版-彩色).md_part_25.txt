如下：
Z
∇ θJ β(µ θ)= ρβ(s)(∇ θµ θ(s)∇ aQµθ(s,a)+∇ θQµθ(s,a))| a=µ(s)ds
ZS
≈ ρβ(s)∇ θµ θ(s)∇ aQµθ(s,a)ds
S
=E s∼ρβ[∇ θµ θ(s)∇ aQµθ(s,a)| a=µ(s)] (2.158)
上面式子中的约等于（ApproximatelyEquivalent）符号“≈”表示了在线策略DPG和离线策略DPG
101
第2章 强化学习入门
的不同。上式中的依赖关系需要小心处理。因为ρβ(s)是独立于θ的，关于θ的导数可以进入积
分中，并且在ρβ(s)上没有导数。Qµθ(s,µ θ(a))实际上以两种方式依赖于θ（其表达式中有两个
µ ）：（1）它依赖于确定性策略µ 基于当前状态s所决定的动作a，而（2）对Q值的在线策略估计
θ θ R
也依赖于策略µ θ来在未来状态下选择的动作，如在Qµθ(s,a)=R(s,a)+ Sγp(s′|s,a)Vµθ(s′)ds′
中所示，所以这个求导需要分别进行。然而，第一个式中的第二项∇ θQµθ(s,a)| 在近似中
a=µ(s)
由于对其估计的困难而被丢掉了，这在离线策略梯度中有类似的相应操作(Degrisetal.,2012)12。
随机性策略梯度和确定性策略梯度的关系
如式(2.140)所示，随机性策略梯度与前文策略梯度定理中公式有相同的形式，而式(2.151)
中的确定性策略梯度看起来却有不一致的形式。然而，可以证明对于相当广泛的随机策略，DPG
是一个SPG的特殊（极限）情况。在这种情况下，DPG也在一定条件下满足策略梯度定理。为了
实现这一点，我们通过一个确定性策略µ :S →A和一个方差参数σ来参数化随机性策略π ，
θ µθ,σ
从而对σ =0有随机性策略等价于确定性策略，即π ≡µ。为了定义SPG和DPG之间的关系，
µθ,0
有一个额外的条件需要满足，这是一个定义常规Delta-近似（RegularDelta-Approximation）的复
合条件。
• C.3常规Delta-近似：由σ参数化的函数v 被称为一个R⊆A上的常规Delta-近似，如果满
σ R
足条件（：1）对于a′ ∈R和适当平滑的f，v σ收敛到一个Delta分布lim σ↓0 Av σ(a′,a)f(a)da=
f(a′)；（2）v (a′,·)在紧致而有利普希茨（Lipschitz）边界的C′ ⊆A上得到支撑，而在边
σ a
界上消失（Vanish）并且在C a′ 上连续可微；（3）梯度∇ a′v σ(a′,a)总是存在；（4）转移不
变性：对任何a∈A,a′ ∈R,a+δ ∈A,a′+δ ∈A，有v(a′,a)=v(a′+δ,a+δ)。
定理2.4 确定性策略梯度作为随机性策略梯度的极限：考虑一个随机性策略π 使得π (a|s)=
µθ,σ µθ,σ
v (µ (s),a)，其中σ是一个控制方差的参数且v (µ (s),a)满足C.3，又有MDP满足C.1和C.2，
σ θ σ θ
那么有，
lim∇ J(π )=∇ J(µ ) (2.159)
θ µθ,σ θ θ
σ↓0
这表示DPG的梯度（等号右边）是标准SPG（等号左边）的极限情况。
以上关系的证明超出了本书的范畴，我们在这里不做讨论。细节参考原文(Silveretal.,2014)。
确定性策略梯度应用和变体
一种最著名的DPG算法是深度确定性策略梯度（DeepDeterministicPolicyGradient，DDPG），
它是DPG的一个深度学习变体。DDPG结合了DQN和Actor-Critic算法来使用确定性策略梯度
并通过一种深度学习的方式更新策略。行动者（Actor）和批判者（Critic）各自有一个目标网络
（Target Network）来便于高样本效率（Sample-Efficient）地学习，但是众所周知，这个算法可能
1关于这个操作的细节和相关论断可以参考原文。
2论文SILVERD,LEVERG,HEESSN,etal. 2014. Deterministicpolicygradientalgorithms[C].中式（15）在近似操作后
的Q项上丢掉了∇ a，这里我们对其勘误。
102
2.7 策略优化
使用起来有一定挑战性，由于它在实践中往往很脆弱而对超参数敏感 (Duan et al., 2016)。关于
DDPG算法的细节和实现在后续章节有详细介绍。
从以上可以看到，策略梯度可以用至少两种方式估计：SPG和DPG，依赖于具体策略类型。
实际上，它们使用了两种不同的估计器，用变分推断（VariationalInference，VI）的术语来说，SPG
是得分函数（ScoreFunction）估计器，而DPG是路径导数（PathwiseDerivative）估计器。
再参数化技巧使得来自价值函数的策略梯度可以用于随机性策略，这被称为随机价值梯度
（StochasticValueGradients，SVG）(Heessetal.,2015)。在SVG算法中，一个λ值通常用于SVG(λ)，
以表明贝尔曼递归被展开了多少步。举例来说，SVG(0)和SVG(1)表示贝尔曼递归分别被展开0
和1步，而SVG(∞)表示贝尔曼递归被沿着有限范围的整个片段轨迹展开。SVG(0)是一个无模
型方法，它的动作价值是用当前策略估计的，因此价值梯度被反向传播到策略中；而SVG(1)是
一个基于模型的方法，它使用一个学得的转移模型来估计下一个状态的值，如论文(Heessetal.,
2015)中所述。
一个非常简单但有用的再参数化技巧（ReparameterizationTrick）的例子是将一个条件高斯概
率密度p(y|x) = N(µ(x),σ2(x))写作函数y(x) = µ(x)+σ(x)ϵ,ϵ ∼ N(0,1)。因而我们可以按程
序生成样本，先采样ϵ再以一种确定性的方式得到y，这使得对随机性策略的采样过程进行梯度
追踪。实际上根据同样的过程也可以得到从动作价值函数到策略间的反向传播梯度。为了像DPG
那样通过价值函数来得到随机性策略的梯度，SVG使用了这个再参数化技巧，并且对随机噪声取
了额外的期望值。柔性Actor-Critic（SoftActor-Critic,SAC）和原始SVG(Heessetal.,2015)算法
都遵循这个程序，从而可以使用随机性策略进行连续控制。
比如，在SAC中，随机性策略被一个均值和一个方差，以及一个从正态分布（NormalDistri-
bution）中采样的噪声项再参数化。SAC中的优化目标有一个额外的熵相关项：
2 3
X∞
π∗ =argmaxE τ∼π4 γt(R(S t,A t,S t+1)+αH(π(·|S t)))5 (2.160)
π
t=0
因此，价值函数和Q值函数间的关系变为
Vπ(s)=E a∼π[Qπ(s,a)]+αH(π(·|s)) (2.161)
=E a∼π[Qπ(s,a)−αlogπ(a|s)] (2.162)
SAC中使用的策略是一个Tanh归一化高斯分布，这与传统设置不同。SAC中的动作表示可以使
用如下再参数化技巧：
a (s,ϵ)=tanh(µ (s)+σ (s)·ϵ),ϵ∼N(0,I) (2.163)
θ θ θ
103
第2章 强化学习入门
由于SAC中策略的随机性，策略梯度可以在最大化期望价值函数时使用再参数化技巧得到，即：
m θaxE a∼πθ[Qπθ(s,a)−αlogπ θ(a|s)] (2.164)
=maxE ϵ∼N[Qπθ(s,a(s,ϵ))−αlogπ θ(a(s,ϵ)|s)] (2.165)
θ
因而，梯度可以经过Q网络到策略网络，与DPG类似，即：
X
1
∇ (Qπθ(S t,a(S t,ϵ))−αlogπ θ(a(S t,ϵ)|S t)) (2.166)
θ|B|
St∈B
其使用一个采样批B来更新策略，而a(S ,ϵ)通过再参数化技巧来从随机性策略中采样。在这种
t
情况下，再参数化技巧使得随机性策略能够以一种类似于DPG的方式来更新，而所得到的SVG
是介于 DPG 和 SPG 之间的方法。DPG 也可以被看作 SVG(0) 的一种确定性极限（Deterministic
Limit）。
无梯度优化
除了基于梯度（Gradient-Based）的优化方法来实现基于策略（Policy-Based）的学习，也有
非基于梯度（Non-Gradient-Based）方法，也称无梯度（Gradient-Gree）优化方法，包括交叉熵
（Cross-Entropy，CE）方法、协方差矩阵自适应（Covariance Matrix Adaptation，CMA）(Hansen
etal.,1996)、爬山法（HillClimbing），Simplex/Amoeba/Nelder-Mead算法(Nelderetal.,1965)等。
例子：交叉熵方法
除了对策略使用基于梯度的优化，CE方法作为一种非基于梯度的方法，在强化学习中也常用
于快速的策略搜索。在CE方法中，策略是迭代更新的，对参数化策略π 的参数θ的优化目标为
θ
∗
θ =argmaxS(θ) (2.167)
其中S(θ)是整体目标函数，对于这里的情况，它可以是折扣期望回报（DiscountedExpectedReturn）。
CE 方法中的策略可以被参数化为一个多变量线性独立高斯分布（Multi-Variate Linear Inde-
pendentGaussianDistribution），参数矢量在迭代步t时的分布为θ ∼N(µ ,σ2)。在采了n个样本
t t t
矢量θ ,··· ,θ 并评估了它们的值S(θ ),··· ,S(θ )后，我们对这些值排序并选取最好的⌊ρ·n⌋
1 n 1 n
个样本，其中0<ρ<1是选择比率（SelectionRatio）。所选取的样本的指标记为I ∈{1,2,··· ,n}，
分布的均值可以用以下式子更新：
P
θ
i∈I i
µ =: (2.168)
t+1 |I|
104
2.7 策略优化
而方差的更新为
P
(θ −µ )T(θ −µ )
σ2 := i∈I i t+1 i t+1 (2.169)
t+1 |I|
交叉熵方法是一个有效且普遍的优化算法。然而，此前研究表明CE对强化学习问题的适用
性严重局限于一个现象，即分布会过快集中到一个点上。所以，它在强化学习的应用中虽然速度
快，但是也有其他限制，因为它经常收敛到次优策略。一个可以预防较早收敛的标准技术是引入
噪声。常用的方法包括在迭代过程中对高斯分布添加一个常数或一个自适应值到标准差上，比如：
P
(θ −µ )T(θ −µ )
σ2 := i∈I i t+1 i t+1 +Z (2.170)
t+1 |I| t+1
如在Szitaetal.(2006)的工作中，有Z =max(5− t ,0)。
t 10
2.7.4 结合基于策略和基于价值的方法
根据以上的初版策略梯度（Vanilla Policy Gradient）方法，一些简单的强化学习任务可以被
解决。然而，如果我们选择使用蒙特卡罗或TD(λ)估计，那么产生的更新经常会有较大的方差。
我们可以使用一个如基于价值的优化中的批判者（Critic）来估计动作价值函数。从而，如果我们
使用参数化的价值函数近似方法，将会有两套参数：行动者（Actor）参数和批判者参数。这实际
上形成了一个非常重要的算法结构，叫作Actor-Critic（AC），典型的算法包括Q值Actor-Critic、
深度确定性策略梯度（DDPG）等。
回想之前小节中介绍的策略梯度理论，性能目标J 关于策略参数θ的导数为
XT
∇ θJ(π θ)=E τ∼πθ ∇ θlogπ θ(A t|S t)Qπ(S t,A t) (2.171)
t=0
其中 Qπ(S ,A ) 是真实动作价值函数，而最简单的估计 Qπ(S ,A ) 的方式是使用采样得到的
t t P t t
累计奖励 G = ∞ γt−1R(S ,A )。在 AC 中，我们使用一个批判者来估计动作价值函数：
t t=0 t t
Qw(S ,A )≈Qπ(S ,A )。因此AC中策略的更新规则为
t t t t
XT
∇ θJ(π θ)=E τ∼πθ ∇ θlogπ θ(A t|S t)Qw(S t,A t) (2.172)
t=0
其中w为价值函数拟合中批判者的参数。批判者可以用一个恰当的策略评估算法来估计，比如时
间差分（TemporalDifference，TD）学习，像式(2.92)中对TD(0)估计的∆w =α(Qπ(S ,A ;w)−
t t
R +γv (S ,w))∇ Qπ(S ,A ;w)。
t+1 π t+1 w t t
尽管 AC 结构可以帮助减小策略更新中的方差，它也会引入偏差和潜在的不稳定（Poten-
105
第2章 强化学习入门
tialInstability）因素，因为它将真实的动作价值函数替换为一个估计的，而这需要兼容函数近似
（Compatible Function Approximation）条件来保证无偏差估计，如文献 (Sutton et al., 2000) 所提
出的。
兼容函数近似
兼容函数近似条件对SPG和DPG都适用。我们将对它们分别展示。这里的“兼容”指近似
动作价值函数Qw(s,a)与相应策略之间是兼容的。
对于SPG：具体来说，兼容函数近似提出了两个条件来保证使用近似动作价值函数Qπ(s,a)时
的无偏差估计（UnbiasedEstimation）（：1）Qw(s,a)=∇ logπ (a|s)Tw和（2）参数w被选择为能够
θ θ