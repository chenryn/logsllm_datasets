### 2.7 策略优化

#### 2.7.1 确定性策略梯度（DPG）与随机性策略梯度（SPG）

考虑如下公式：
\[ \nabla_{\theta} J_{\beta}(\mu_{\theta}) = \int_{S} \rho^{\beta}(s) \left( \nabla_{\theta} \mu_{\theta}(s) \nabla_{a} Q_{\mu_{\theta}}(s, a) + \nabla_{\theta} Q_{\mu_{\theta}}(s, a) \right) \Big|_{a=\mu(s)} ds \]
\[ \approx \int_{S} \rho^{\beta}(s) \nabla_{\theta} \mu_{\theta}(s) \nabla_{a} Q_{\mu_{\theta}}(s, a) \Big|_{a=\mu(s)} ds \]
\[ = \mathbb{E}_{s \sim \rho^{\beta}} \left[ \nabla_{\theta} \mu_{\theta}(s) \nabla_{a} Q_{\mu_{\theta}}(s, a) \Big|_{a=\mu(s)} \right] \tag{2.158} \]

上述式子中的近似符号“≈”表示了在线策略 DPG 和离线策略 DPG 的不同。由于 \(\rho^{\beta}(s)\) 与 \(\theta\) 无关，关于 \(\theta\) 的导数可以进入积分中，并且在 \(\rho^{\beta}(s)\) 上没有导数。\(Q_{\mu_{\theta}}(s, \mu_{\theta}(a))\) 实际上以两种方式依赖于 \(\theta\)：（1）它依赖于确定性策略 \(\mu_{\theta}\) 基于当前状态 \(s\) 所决定的动作 \(a\)；（2）对 \(Q\) 值的在线策略估计也依赖于策略 \(\mu_{\theta}\) 来在未来状态下选择的动作。因此，这个求导需要分别进行。然而，第一项中的第二项 \(\nabla_{\theta} Q_{\mu_{\theta}}(s, a) \big|_{a=\mu(s)}\) 在近似中由于其估计困难而被丢弃，这在离线策略梯度中有类似的相应操作 (Degris et al., 2012)。

#### 2.7.2 随机性策略梯度和确定性策略梯度的关系

如式 (2.140) 所示，随机性策略梯度与前文策略梯度定理中的公式有相同的形式，而式 (2.151) 中的确定性策略梯度看起来形式不一致。然而，可以证明对于相当广泛的随机策略，DPG 是 SPG 的一个特殊情况。在这种情况下，DPG 在一定条件下满足策略梯度定理。为了实现这一点，我们通过一个确定性策略 \(\mu: S \rightarrow A\) 和一个方差参数 \(\sigma\) 来参数化随机性策略 \(\pi_{\mu_{\theta}, \sigma}\)，从而当 \(\sigma = 0\) 时，随机性策略等价于确定性策略，即 \(\pi_{\mu_{\theta}, 0} \equiv \mu_{\theta}\)。为了定义 SPG 和 DPG 之间的关系，有一个额外的条件需要满足，这是一个定义常规 Delta-近似的复合条件。

- **C.3 常规 Delta-近似**：由 \(\sigma\) 参数化的函数 \(v_{\sigma}\) 被称为一个 \(R \subseteq A\) 上的常规 Delta-近似，如果满足以下条件：
  1. 对于 \(a' \in R\) 和适当平滑的 \(f\)，\(v_{\sigma}\) 收敛到一个 Delta 分布 \(\lim_{\sigma \downarrow 0} \int v_{\sigma}(a', a) f(a) da = f(a')\)；
  2. \(v_{\sigma}(a', \cdot)\) 在紧致且具有 Lipschitz 边界的 \(C' \subseteq A\) 上得到支撑，在边界上消失并且在 \(C' \setminus \{a'\}\) 上连续可微；
  3. 梯度 \(\nabla_{a'} v_{\sigma}(a', a)\) 总是存在；
  4. 转移不变性：对任何 \(a \in A, a' \in R, a + \delta \in A, a' + \delta \in A\)，有 \(v(a', a) = v(a' + \delta, a + \delta)\)。

**定理 2.4**：考虑一个随机性策略 \(\pi_{\mu_{\theta}, \sigma}\) 使得 \(\pi_{\mu_{\theta}, \sigma}(a | s) = v_{\sigma}(\mu_{\theta}(s), a)\)，其中 \(\sigma\) 是一个控制方差的参数且 \(v_{\sigma}(\mu_{\theta}(s), a)\) 满足 C.3，又有 MDP 满足 C.1 和 C.2，那么有
\[ \lim_{\sigma \downarrow 0} \nabla_{\theta} J(\pi_{\mu_{\theta}, \sigma}) = \nabla_{\theta} J(\mu_{\theta}) \tag{2.159} \]
这表示 DPG 的梯度（等号右边）是标准 SPG（等号左边）的极限情况。

以上关系的证明超出了本书的范畴，我们在这里不做讨论。细节参考原文 (Silver et al., 2014)。

#### 2.7.3 确定性策略梯度的应用和变体

一种最著名的 DPG 算法是深度确定性策略梯度（Deep Deterministic Policy Gradient, DDPG），它是 DPG 的一个深度学习变体。DDPG 结合了 DQN 和 Actor-Critic 算法来使用确定性策略梯度并通过一种深度学习的方式更新策略。行动者（Actor）和批判者（Critic）各自有一个目标网络（Target Network）来便于高样本效率地学习，但该算法在实践中往往很脆弱并对超参数敏感 (Duan et al., 2016)。关于 DDPG 算法的细节和实现在后续章节有详细介绍。

从以上可以看到，策略梯度可以用至少两种方式估计：SPG 和 DPG，依赖于具体策略类型。实际上，它们使用了两种不同的估计器，用变分推断（Variational Inference, VI）的术语来说，SPG 是得分函数（Score Function）估计器，而 DPG 是路径导数（Pathwise Derivative）估计器。

再参数化技巧使得来自价值函数的策略梯度可以用于随机性策略，这被称为随机价值梯度（Stochastic Value Gradients, SVG）(Heess et al., 2015)。在 SVG 算法中，一个 \(\lambda\) 值通常用于 SVG(\(\lambda\))，以表明贝尔曼递归被展开了多少步。例如，SVG(0) 和 SVG(1) 表示贝尔曼递归分别被展开 0 和 1 步，而 SVG(\(\infty\)) 表示贝尔曼递归被沿着有限范围的整个片段轨迹展开。SVG(0) 是一个无模型方法，它的动作价值是用当前策略估计的，因此价值梯度被反向传播到策略中；而 SVG(1) 是一个基于模型的方法，它使用一个学得的转移模型来估计下一个状态的值，如论文 (Heess et al., 2015) 中所述。

一个非常简单但有用的再参数化技巧的例子是将一个条件高斯概率密度 \(p(y|x) = N(\mu(x), \sigma^2(x))\) 写作函数 \(y(x) = \mu(x) + \sigma(x) \epsilon, \epsilon \sim N(0, 1)\)。因而我们可以按程序生成样本，先采样 \(\epsilon\) 再以一种确定性的方式得到 \(y\)，这使得对随机性策略的采样过程进行梯度追踪。实际上根据同样的过程也可以得到从动作价值函数到策略间的反向传播梯度。为了像 DPG 那样通过价值函数来得到随机性策略的梯度，SVG 使用了这个再参数化技巧，并且对随机噪声取了额外的期望值。柔性 Actor-Critic（Soft Actor-Critic, SAC）和原始 SVG (Heess et al., 2015) 算法都遵循这个程序，从而可以使用随机性策略进行连续控制。

例如，在 SAC 中，随机性策略被一个均值和一个方差，以及一个从正态分布中采样的噪声项再参数化。SAC 中的优化目标有一个额外的熵相关项：
\[ \pi^* = \arg\max_{\pi} \mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^{\infty} \gamma^t \left( R(S_t, A_t, S_{t+1}) + \alpha H(\pi(\cdot | S_t)) \right) \right] \tag{2.160} \]
因此，价值函数和 Q 值函数间的关系变为
\[ V_{\pi}(s) = \mathbb{E}_{a \sim \pi} [Q_{\pi}(s, a)] + \alpha H(\pi(\cdot | s)) \tag{2.161} \]
\[ = \mathbb{E}_{a \sim \pi} [Q_{\pi}(s, a) - \alpha \log \pi(a | s)] \tag{2.162} \]

SAC 中使用的策略是一个 Tanh 归一化高斯分布，这与传统设置不同。SAC 中的动作表示可以使用如下再参数化技巧：
\[ a_{\theta}(s, \epsilon) = \tanh(\mu_{\theta}(s) + \sigma_{\theta}(s) \cdot \epsilon), \quad \epsilon \sim N(0, I) \tag{2.163} \]

由于 SAC 中策略的随机性，策略梯度可以在最大化期望价值函数时使用再参数化技巧得到，即：
\[ \max_{\theta} \mathbb{E}_{a \sim \pi_{\theta}} [Q_{\pi_{\theta}}(s, a) - \alpha \log \pi_{\theta}(a | s)] \tag{2.164} \]
\[ = \max_{\theta} \mathbb{E}_{\epsilon \sim N} [Q_{\pi_{\theta}}(s, a_{\theta}(s, \epsilon)) - \alpha \log \pi_{\theta}(a_{\theta}(s, \epsilon) | s)] \tag{2.165} \]

因此，梯度可以经过 Q 网络到策略网络，与 DPG 类似，即：
\[ \frac{1}{|B|} \sum_{S_t \in B} \nabla_{\theta} \left( Q_{\pi_{\theta}}(S_t, a_{\theta}(S_t, \epsilon)) - \alpha \log \pi_{\theta}(a_{\theta}(S_t, \epsilon) | S_t) \right) \tag{2.166} \]

其使用一个采样批 \(B\) 来更新策略，而 \(a(S_t, \epsilon)\) 通过再参数化技巧来从随机性策略中采样。在这种情况下，再参数化技巧使得随机性策略能够以一种类似于 DPG 的方式更新，而所得到的 SVG 是介于 DPG 和 SPG 之间的方法。DPG 也可以被看作 SVG(0) 的一种确定性极限（Deterministic Limit）。

#### 2.7.4 无梯度优化

除了基于梯度的优化方法来实现基于策略的学习，也有非基于梯度的方法，也称无梯度优化方法，包括交叉熵（Cross-Entropy, CE）方法、协方差矩阵自适应（Covariance Matrix Adaptation, CMA）(Hansen et al., 1996)、爬山法（Hill Climbing）、Simplex/Amoeba/Nelder-Mead 算法 (Nelder et al., 1965) 等。

**例子：交叉熵方法**

除了对策略使用基于梯度的优化，CE 方法作为一种非基于梯度的方法，在强化学习中也常用于快速的策略搜索。在 CE 方法中，策略是迭代更新的，对参数化策略 \(\pi_{\theta}\) 的参数 \(\theta\) 的优化目标为
\[ \theta^* = \arg\max_{\theta} S(\theta) \tag{2.167} \]
其中 \(S(\theta)\) 是整体目标函数，对于这里的情况，它可以是折扣期望回报（Discounted Expected Return）。

CE 方法中的策略可以被参数化为一个多变量线性独立高斯分布，参数矢量在迭代步 \(t\) 时的分布为 \(\theta \sim N(\mu_t, \sigma^2_t)\)。在采了 \(n\) 个样本矢量 \(\theta_1, \ldots, \theta_n\) 并评估了它们的值 \(S(\theta_1), \ldots, S(\theta_n)\) 后，我们对这些值排序并选取最好的 \(\lfloor \rho \cdot n \rfloor\) 个样本，其中 \(0 < \rho < 1\) 是选择比率（Selection Ratio）。所选取的样本的指标记为 \(I \subseteq \{1, 2, \ldots, n\}\)，分布的均值可以用以下式子更新：
\[ \mu_{t+1} = \frac{1}{|I|} \sum_{i \in I} \theta_i \tag{2.168} \]

而方差的更新为
\[ \sigma^2_{t+1} = \frac{1}{|I|} \sum_{i \in I} (\theta_i - \mu_{t+1})^T (\theta_i - \mu_{t+1}) \tag{2.169} \]

交叉熵方法是一个有效且普遍的优化算法。然而，研究表明 CE 对强化学习问题的适用性严重局限于一个现象，即分布会过快集中到一个点上。所以，它在强化学习的应用中虽然速度快，但是也有其他限制，因为它经常收敛到次优策略。一个可以预防较早收敛的标准技术是引入噪声。常用的方法包括在迭代过程中对高斯分布添加一个常数或一个自适应值到标准差上，比如：
\[ \sigma^2_{t+1} = \frac{1}{|I|} \sum_{i \in I} (\theta_i - \mu_{t+1})^T (\theta_i - \mu_{t+1}) + Z_{t+1} \tag{2.170} \]

如在 Szita et al. (2006) 的工作中，有 \(Z_{t+1} = \max(5 - 0.1 t, 0)\)。

#### 2.7.5 结合基于策略和基于价值的方法

根据以上的初版策略梯度（Vanilla Policy Gradient）方法，一些简单的强化学习任务可以被解决。然而，如果我们选择使用蒙特卡罗或 TD(\(\lambda\)) 估计，那么产生的更新经常会有较大的方差。我们可以使用一个如基于价值的优化中的批判者（Critic）来估计动作价值函数。从而，如果我们使用参数化的价值函数近似方法，将会有两套参数：行动者（Actor）参数和批判者参数。这实际上形成了一个非常重要的算法结构，叫作 Actor-Critic（AC），典型的算法包括 Q 值 Actor-Critic、深度确定性策略梯度（DDPG）等。

回想之前小节中介绍的策略梯度理论，性能目标 \(J\) 关于策略参数 \(\theta\) 的导数为
\[ \nabla_{\theta} J(\pi_{\theta}) = \mathbb{E}_{\tau \sim \pi_{\theta}} \left[ \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(A_t | S_t) Q_{\pi}(S_t, A_t) \right] \tag{2.171} \]
其中 \(Q_{\pi}(S_t, A_t)\) 是真实动作价值函数，而最简单的估计 \(Q_{\pi}(S_t, A_t)\) 的方式是使用采样得到的累计奖励 \(G_t = \sum_{t'=t}^{\infty} \gamma^{t'-t} R(S_{t'}, A_{t'})\)。在 AC 中，我们使用一个批判者来估计动作价值函数：\(Q_w(S_t, A_t) \approx Q_{\pi}(S_t, A_t)\)。因此 AC 中策略的更新规则为
\[ \nabla_{\theta} J(\pi_{\theta}) = \mathbb{E}_{\tau \sim \pi_{\theta}} \left[ \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(A_t | S_t) Q_w(S_t, A_t) \right] \tag{2.172} \]
其中 \(w\) 为价值函数拟合中批判者的参数。批判者可以用一个恰当的策略评估算法来估计，比如时间差分（Temporal Difference, TD）学习，像式 (2.92) 中对 TD(0) 估计的 \(\Delta w = \alpha (Q_{\pi}(S_t, A_t; w) - R_{t+1} - \gamma v_{\pi}(S_{t+1}, w)) \nabla_w Q_{\pi}(S_t, A_t; w)\)。

尽管 AC 结构可以帮助减小策略更新中的方差，它也会引入偏差和潜在的不稳定因素，因为它将真实的动作价值函数替换为一个估计的，而这需要兼容函数近似（Compatible Function Approximation）条件来保证无偏差估计，如文献 (Sutton et al., 2000) 所提出的。

**兼容函数近似**

兼容函数近似条件对 SPG 和 DPG 都适用。我们将对它们分别展示。这里的“兼容”指近似动作价值函数 \(Q_w(s, a)\) 与相应策略之间是兼容的。

对于 SPG：具体来说，兼容函数近似提出了两个条件来保证使用近似动作价值函数 \(Q_{\pi}(s, a)\) 时的无偏差估计：（1）\(Q_w(s, a) = \nabla_{\theta} \log \pi_{\theta}(a | s)^T w\) 和（2）参数 \(w\) 被选择为能够满足上述条件。