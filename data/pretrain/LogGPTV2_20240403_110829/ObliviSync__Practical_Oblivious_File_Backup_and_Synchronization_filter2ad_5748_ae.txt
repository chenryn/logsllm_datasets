=
1
3
.
That is, each sync is productive with probability at least 1
3.
Next, deﬁne a random variable X to be the number of
productive syncs among a series of T = 48s/(Bk) + 18r sync
operations. Importantly, if X ≥ 8s/(Bk), then the buffer will
be cleared at the end of T syncs.
We see that X is the sum of T i.i.d. Bernoulli trials, each
3. Therefore the Hoeffding bound from
with probability p = 1
[16] tells us that, for any  > 0,
Pr(cid:2)X  0. The theo-
(cid:0) 48s
18 ·(cid:0) 48s
Bk + 18r(cid:1)(cid:1) <
6 works to bound the probability that X <
Bk < 1
rem follows from exp(−22T ) = exp(cid:0)− 1
Setting  = 1
8s/(Bk), since 8s
exp(−r).
6
B. Security
Theorem 2. Let λ be the security parameter. Consider
ObliviSync-RW with parameters B, N, k as above, and with
drip time t. For any L and t as fsequence parameters,
ObliviSync-RW is strongly-secure write-only ﬁlesystem with
running time T = t + 48Lt
Bk + 18λt.
Proof: We need to show the following:
time T with probability 1 − neg(λ).
• ObliviSync-RW ﬁnishes all tasks in each sequence within
• For any two (L, t)-fsequences P0 and P1, both access
patterns are computationally indistinguishable from each
other.
The ﬁrst condition is achieved according to Theorem 1,
since one sync operation occurs every t seconds.
It is left to show that the second condition also holds.
Obliviousness mostly follows from the original write-only
ORAM security. To achieve strong obliviousness, we stress
that ObliviSync-RW always writes encrypted data in k ﬁles at
the backend chosen independently and uniformly at random.
In particular:
• If there is too much data to be synchronized, the remain-
ing data is safely stored in the temporary buffer so that the
data will be eventually synchronized. Theorem 1 makes
sure this must happen with overwhelming probability.
• If there is too little (or even no) data to be synchronized,
the system generates what amounts to dummy trafﬁc (re-
packing the k chosen block pairs with the same data they
stored before).
Therefore, the second condition is also satisﬁed.
There is an important assumption in both theorems above,
namely that the client is actually able to execute the sync
operation with the given drip rate k within each epoch with
drip time t. If the parameters k and t are set too aggressively,
it may be the case that the sync operation takes longer than
t seconds to complete, and this fact would be noticed by the
cloud server or indeed any network observer. While the leakage
in such cases is relatively minor (an indication that, perhaps,
the client’s system load is higher than normal), it is nonetheless
important for security to ensure t and k are set appropriately
to avoid this situation of a sync operation “blowing through”
the epoch.
VI. EXPERIMENTS
We fully implemented ObliviSync using python3 and
fusepy [2], and the source code of our implementation is
available on GitHub as well as a video demonstration [14]
and a Dockerﬁle for quick setup and testing.
To evaluate ObliviSync, we performed a series of experi-
ments to test its performance at the extremes. In particular, we
are interested in the following properties:
• Throughput with ﬁxed-size ﬁles: If the user of ObliviSync
were to insert a large number of ﬁles all at once, the buffer
will immediately be ﬁlled to hold all the insertions. How
long does it take (in number of epochs) for each of the
ﬁles to sync to the read end?
• Throughput with variable-size ﬁles: If the users were to
insert a large number of variable size ﬁles all at once,
instead of ﬁxed-sized ones, how does the throughput
change? This experiment will verify Theorem 1, which
states that the performance of our system depends on the
total number of bytes (instead of the total number of ﬁles)
in the pending write buffer.
• Latency: If the user of ObliviSync-RW were to insert a
large number of ﬁles one at a time, how long does it take
for each of the ﬁles to appear to a different ObliviSync-RO
user?
• The size of pending writes buffer: We also investigate
how much space the pending writes buffer uses while the
system is working under different loads with realistic ﬁle
sizes. Recall the pending writes buffer works similarly as
the stash in the write-only ORAM, and it is important
that this buffer does not grow too large.
• Functionality with Dropbox backend: Finally, we per-
formed throughput and latency experiments with Drop-
box as the backend storage mechanism for ObliviSync,
and compare its performance to that of EncFS [13] on
Dropbox. EncFS is a per-ﬁle encrypted ﬁle system tool
that provides no obliviousness.
A. Throughput with Fixed-size Files
We ﬁrst consider the throughput in our system. In partic-
ular, we are interested in the performance as it relates to the
availability of backend blocks.
To limit
the factors in these experiments, we use the
following parameters:
• N = 1024; we used 1024 backend ﬁles (i.e., block pairs).
11
Fig. 4. Throughput for different drip rates. We used 1024 backend ﬁles, each
with 1MB, and attempted to insert 920 frontend ﬁles all at once, where each
frontend ﬁle is also 1MB. The results are the mean of three runs. With drip
rate 3 (the solid line for k = 3), it takes about 120 epochs on average to sync
25% of the frontend ﬁles. In addition, the graph shows the situation shifts as
the backend ﬁles become more full and it becomes harder to clear the buffer.
90% full).
• n = 920; we attempted to insert 920 frontend ﬁles (or
• B = 1 MB; each backend ﬁle is 1MB in size.
In our experiments, each frontend ﬁle is 1MB and thus ﬁlls
up two full blocks (including any metadata). There is also two
additional block pairs in the system for the superblock and
the directory entry. Overall, the entire backend storage for the
system was N · B = 1GB.
In the throughput experiment, we established an empty
ObliviSync-RW and attached an ObliviSync-RO to the back-
end. We then wrote 920 two-block size ﬁles all at once to the
ObliviSync-RW FUSE mounted frontend. We then manually
called the synchronize operation that performs the oblivious
writing to the backend. By monitoring the ObliviSync-RO
FUSE mounted frontend, we can measure how many epochs
are required to fully synchronize all the ﬁles.
Bandwidth overhead: 2x until 25% of the load. In Figure 4,
we graph the number of epochs (i.e., the number of timed sync
operations) it takes for that percentage of ﬁles to synchronize
and appear at the read-end. We conducted the experiments
for different drip rates (k), i.e., the number of backend ﬁles
randomly selected at each epoch for writing. The results
presented are the average of three runs for drip rates set to
3, 6, 9, and 12.
the experiment shows that
As one interesting data point, the graph shows that with
drip rate 3 (the solid line for k = 3), it takes about ∼ 120
epochs on average to sync 25% of the frontend ﬁles. Note
that the number of bytes that would be transferred to the
cloud storage during 120 epochs is 120 · (k + 1) · B = 480
MB, and 25% of the frontend ﬁles amounts to 250 MB.
the system needs only 2x
So,
bandwidth overhead, when the front-end ﬁles occupies at most
25% of the total cloud storage, with the parameters chosen
in this experiment. This is better performance than what is
shown in Thoerem 1, which provably guarantees 4x bandwidth
overhead.
Linear costs until 33% of the load. Looking closely at the
graph, particularly k = 3 trend-line, there are two regimes:
linear and super-linear synchronization. In the linear regime
Fig. 5.
Comparing throughput of inserting realistic workload of variable
sizes ﬁles to the same sized insert of ﬁxed size ﬁles. The experiment were
performed with parameters N = 1024, B = 1MB, k = 3. In the set of
ﬁxed-size ﬁles, each frontend ﬁle is 1 MB. There are 4,179 ﬁles in the set
of variable-size ﬁles. Both set is 250 MB in total. For both sets, the system
takes about 100 epochs to sync the 250 MB of frontend data.
Fig. 6.
Latency for different drip rates. The experiment were performed
with parameters N = 1024, B = 1MB. The drip rates k varies with k =
3, 6, 9, 12. The 920 frontend ﬁles, each with 1 MB, were written one after
another. For example, the point (0.7, 2) of the solid line with k = 3 means
that once 700 ﬁles have been written, it took about 2 epochs to sync the 701st
ﬁle.
there are enough empty blocks that on each epoch, progress
is very likely to be made by clearing ﬁles from the buffer
and writing new blocks to the backend. In the super-linear
regime, however, there are no longer enough empty blocks
to reliably clear fragments from the buffer. For k = 3, this
regime seems to take over around 40%∼60%, and the trend-
line’s slope begins to increase dramatically. This is because
each additional block written further exacerbates the problem,
so it takes an increasing amount of time to ﬁnd the next empty
block to clear the buffer further.
The inﬂection point, between linear and super-linear, is
particularly interesting. Apparent immediately is the fact that
the inﬂection point is well beyond the 25% theoretic bound;
even for a drip rate of k = 3, it manages to get at least 1/3
full before super-linear tendencies take over. Further, notice
that for higher drip rates, the inﬂection point occurs for higher
percentage of fullness for the backend. This is to be expected;
if you are selecting more blocks per epoch, you are more likely
to ﬁnd an empty block to clear the buffer. But we hasten to
point out that there is a trade-off in practice here.
B. Throughput with Variable-size Files
As mentioned above, an important performance property
of ObliviSync is that the rate of synchronization is dependent
on the total number of bytes in the pending write buffer and
the fullness of the backend blocks: it does not depend on the
sizes of the individual ﬁles.
To show this property clearly, we performed a similar
throughput experiment as described previously, i.e., with N =
1024, B = 1MB, and k = 3, except we inserted variable size
ﬁles that are drawn from realistic ﬁle distributions [10], [25]:
• The variable size ﬁles, in total, were 0.25GB, the same
• The ﬁles contained 4,179 ﬁles, much larger than 250 for
• Interestingly, one of the variable size ﬁles was signiﬁ-
cantly larger, 144MB, roughly the length of a short, TV-
episode video.
the case of ﬁxed-size ﬁles.
size as the ﬁxed size ﬁles.
As in the prior throughput experiment, we connected an
ObliviSync-RW and ObliviSync-RO to a shared backend direc-
tory. We loaded the ﬁle set completely into the ObliviSync-RW,
and then counted how many epochs it takes for the data to
appear in the ObliviSync-RO FUSE ﬁle.
Good performance for variable-size ﬁles. The primary
result is presented in Figure 5. The two trend-lines are nearly
identical, and in fact, after three runs, the average number of
epochs needed to synchronize the two ﬁle loads is the same,
100 epochs. This clearly shows that our systems is dependent
on the total number of bytes to synchronize and not the size
of the individual ﬁles.
C. Latency
In this experiment, we are interested in the latency of
our system. As before, we performed the experiment with
N = 1024 and B = 1MB, and we had ObliviSync-RW
and ObliviSync-RO clients with a shared backend, writing to
ObliviSync-RW FUSE mount and measuring synchronization
rate to the ObliviSync-RO FUSE mount. To measure latency,
we only add one frontend ﬁle at a time. For example, the
second frontend ﬁle gets written right after the ﬁrst frontend
ﬁle is completely synced. We measured how long it took for
each ﬁle to synchronize in terms of the number of manual
synchronizations (or epochs) required. Again, we varied the
drip rate.
About 1 epoch to sync, even for high ﬁll rates. The results in
Figure 6 are the average of three runs in each drip rate. Again,
there are two general regimes to the graph, a linear one and a
super-linear one, and the transitions between them are, again,
better than our theoretic 25% bound. First, for lower ﬁll rates,
the time to complete a single ﬁle synchronization is roughly
one epoch.
At higher ﬁll rates, it starts to take more epochs, on average,
to sync a single ﬁle; however, even for the most conservative
k = 3, it only takes at most 5 epochs even for very high
ﬁll rates. For more aggressive drip rates, k = 9, 12 the
impact of higher ﬁller rates is diminished, still only requiring
about 2 epochs to synchronize a single ﬁle. This is to be
12
expected as selecting more backend ﬁles for writing increased
the likelihood of ﬁnding space to clear the buffer.
D. The Size of Pending Writes Buffer
In this experiment, we investigate how much space the
pending writes buffer requires while the system is working.
To do so, we consider more realistic ﬁle sizes and ﬁle write
patterns under high thrashing rates, contrary to most of the
previous experiments where each frontend ﬁle has two full-
block fragments.
We inserted frontend ﬁles of varied size based on known
ﬁle size distributions such that the backend was ﬁlled to 20%,
50%, or 75%. The ﬁle sizes were based on prior published
work in measuring ﬁle size distributions. In particular, we
followed a lognormal distribution, which has been shown to
closely match real ﬁle sizes [10], ﬁt with data from a large-
scale study of ﬁle sizes used in a university setting [25]. The
same distribution was used in the variable ﬁle size experiment
previously.
We also generated a series of writes to these ﬁles such that,
on average, 1MB of data was updated on each write. This
could occur within a single ﬁle or across multiple ﬁles. We
selected which ﬁle to thrash based on distributions of actual
write operations in the same study [25], used to generate the
original ﬁle sizes. Roughly, this distribution gives a stronger
preference to rewriting smaller ﬁles. We did not write exactly
1MB of data in each batch, but rather kept the average of each
batch size as close to 1MB as possible in accordance with the
experimental write size distribution. In particular, there were
batches were a ﬁle larger than 1MB was written. As before,
we used N = 1024 and B = 1MB. We used the drip rate
k = 3 to show the most conservative setting of ObliviSync.
We averaged the results of three independent runs.
Reasonable buffer size: at most 2 MB. The primary result
is presented in Figure 7 where we measure the number of
bytes in the buffer on each synchronization. In the graph,
the point (x, y) means for y fraction of observed execution
time, the size of the buffer was greater than x. For example,
when the backend is ﬁlled with 20% (the solid line), only for
0.2 fraction of the observed execution time, the buffer size is
roughly greater than 218 bytes. In addition, the buffer size is
always larger than 215, and the buffer never grows larger than
about 2 MB, which corresponds to only 4 blocks.
Clearly, as the ﬁll rate increases, the amount of uncommit-
ted data in the buffer increases; however, the relationship is not
strictly linear. For example, with 20% full and 50% full, we
see only a small difference in the buffer size for this extreme