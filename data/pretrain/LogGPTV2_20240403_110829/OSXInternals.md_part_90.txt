Note that UTIs do not preclude other tagging methods. In fact, they are compatible with such methods. As
shown in Figure 762, content of type HTML text (as specified by the UTTypeDescription key) can be
identified by several tags (as specified by the UTTypeTagSpecification key): an NSPasteboard type, a
four-character file type code, multiple file extensions, or a MIME type. Therefore, a UTI can unify
alternative methods of type identification.
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hh62C7.htm 20.08.2007
Chapter 8. Memory Page 1 of 135
Chapter 8. Memory
Memoryspecifically, physical memoryis a precious resource in a computer system. An integral feature of
modern operating systems is virtual memory (VM), whose typical implementation provides an illusion of
a large, contiguous virtual address space to each program without burdening the programmer with details
such as which parts of the program are resident in physical memory at any given time, or where in
physical memory the resident portions are located. Virtual memory is commonly implemented through
paging: An address space is subdivided into fixed size pages. When resident, each virtual page is loaded
into some portion of physical memory. This portion, essentially a physical slot for a logical page, is called
a page frame.
8.1. Looking Back
Tom Kilburn, R. Bruce Payne, and David J. Howarth described the Atlas supervisor program in a 1961
paper.[1] A result of work that originated in the Computer Group at Manchester University, the Atlas
supervisor controlled the functioning of the Atlas computer system. When inaugurated in late 1962, Atlas
was considered the most powerful computer in the world. It also had the earliest implementation of virtual
memorythe so-called one-level storage system that decoupled memory addresses and memory locations.
The core memory system of Atlas used a form of indirect addressing based on 512-word pages and page-
address registers. When access to a memory address was made, a hardware unit (the memory management
unit, or MMU) automatically attempted to locate the corresponding page in core memory (primary
memory). If the page was not found in core memory, there was a nonequivalence interruptiona page fault,
which resulted in the supervisor transferring data from a sector of drum store (secondary memory) to core
memory. This process was referred to as demand paging. Moreover, the Atlas system provided per-page
protection that allowed the supervisor to lock certain pages such that they became unavailable except
when on interrupt control. A page replacement scheme was also used to move pages that were less likely
to be used back to the drum store.
[1] "The Atlas Supervisor," by Tom Kilburn, R. Bruce Payne, and David J. Howarth
(American Federation of Information Processing Societies Computer Conference 20, 1961,
pp. 279294).
Within the next few years, virtual memory concepts were widely adopted, as major processor vendors
incorporated virtual memory support in their processors. Most commercial operating systems of the 1960s
and 1970s were capable of virtual memory.
8.1.1. Virtual Memory and UNIX
What can be considered as the Zeroth Edition of UNIX (late 1969) was not multiprogrammedonly one
program could exist in memory at a time. It employed swapping as a form of memory management policy
wherein entire processes, rather than individual pages, were transferred between physical memory and the
swap device. Third Edition UNIX (February 1973) introduced multiprogramming, but it would not be
until 3BSD (1979) that a UNIX-based system would be capable of paged virtual memory.
8.1.2. Virtual Memory and Personal Computing
Compared with UNIX, virtual memory would become part of personal computing much later, with
personal computer software lagging behind the hardware by several years. Table 81 shows the time
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhCF85.htm 20.08.2007
Chapter 8. Memory Page 2 of 135
frames in which virtual memory (and multiprogramming) were introduced in personal computing.
Table 81. Virtual Memory and Multiprogramming in Personal Computing
Product Date Introduced Notes
Intel 80286 February 1, 1982 16-bit, segment-based memory
management and protection
Motorola 68020 June 1984 32-bit, support for a paged MMU as a
coprocessor chipvirtual memory possible
with the latter
Intel 80386 October 17, 1985 32-bit, integrated MMU with support for
paging and segmentationvirtual memory
possible
Macintosh System 4.2 October 1987 Cooperative multitasking introduced with
the optional MultiFinder
Macintosh System 7 May 13, 1991 MultiFinder made nonoptional, virtual
memory support introduced
Microsoft Windows April 6, 1992 Cooperative multitasking introduced,
3.1 virtual memory support introduced
Microsoft Windows 95 August 24, 1995 Preemptive multitasking introduced (a
Win32-only feature), virtual memory
support enhanced
Thrashing
Early virtual memory implementations all suffered from thrashinga severe loss of
performance that occurred when a multiprogramming system was under heavy load. When
thrashing, the system spent most of its time transferring data between primary and secondary
memories. This problem was satisfactorily addressed by Peter J. Denning's Working Set
Principle, using which the memory management subsystem could strive to keep each
program's "most useful" pages resident to avoiding overcommitting.
8.1.3. Roots of the Mac OS X Virtual Memory Subsystem
We saw in Chapter 1 that the RIG and Accent operating systems were Mach's ancestors. One of Accent's
prime goals was to use virtual memory to overcome RIG's limitations in the handling of large objects.
Accent combined paged virtual memory and capability-based interprocess communication (IPC), allowing
large IPC-based data transfers through copy-on-write (COW) memory mapping. The Accent kernel
provided the abstraction of a memory object, which represented a data repository, and had a backing store
such as a disk. Contents of disk blocksdisk pages, whether they corresponded to an on-disk file or a
paging partitioncould be mapped into an address space.
Mach evolved from Accent as a system suited for general-purpose shared memory multiprocessors. Like
Accent, Mach's VM subsystem was integrated with its IPC subsystem. However, Mach's implementation
used simpler data structures, with a cleaner separation of machine-dependent and machine-independent
components. Mach's VM architecture inspired several others. The VM subsystem of BSD Networking
Release 2 (NET2) was derived from Mach. The 4.4BSD VM subsystem was based on Mach 2, with
updates from Mach 2.5 and Mach 3. The 4.4BSD implementation was the basis for FreeBSD's VM
subsystem. Moreover, Mach's VM architecture has several design similarities with that of SunOS/SVR4,
which was independently designed around the same time as Mach.
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhCF85.htm 20.08.2007
Chapter 8. Memory Page 3 of 135
The core of the Mac OS X VM architecture is a derivative of, and largely similar to, the Mach VM
architecture. However, as the operating system has evolved and undergone various optimizations, several
minor and a few major differences have appeared in its VM subsystem's implementation.
Virtual XYZ
It is worthwhile to somewhat deemphasize the "virtual" in virtual memory. As in a typical
modern-day operating system, not just memory but all system resources are virtualized by the
Mac OS X kernel. For example, threads execute in a virtual environment consisting of a
virtual processor, with each thread having its own set of virtual processor registers. In that
sense, it's all virtual.
8.2. An Overview of Mac OS X Memory Management
Besides the Mach-based core VM subsystem, memory management in Mac OS X encompasses several
other mechanisms, some of which are not strictly parts of the VM subsystem but are closely related
nonetheless.
Figure 81 shows an overview of key VM and VM-related components in Mac OS X. Let us briefly look at
each of them in this section. The rest of the chapter discusses these components in detail.
The Mach VM subsystem consists of the machine-dependent physical map (pmap) module and
other, machine-independent modules for managing data structures corresponding to abstractions
such as virtual address space maps (VM maps), VM objects, named entries, and resident pages. The
kernel exports several routines to user space as part of the Mach VM API.
The kernel uses the universal page list (UPL) data structure to describe a bounded set of physical
pages. A UPL is created based on the pages associated with a VM object. It can also be created for
an object underlying an address range in a VM map. UPLs include various attributes of the pages
they describe. Kernel subsystemsparticularly file systemsuse UPLs while communicating with the
VM subsystem.
The unified buffer cache (UBC) is a pool of pages for caching the contents of files and the
anonymous portions of task address spaces. Anonymous memory is not backed by regular files,
devices, or some other named source of memorythe most common example is that of dynamically
allocated memory. The "unification" in the UBC comes from a single pool being used for file-
backed and anonymous memory.
The kernel includes three kernel-internal pagers, namely, the default (anonymous) pager, the device
pager, and the vnode pager. These handle page-in and page-out operations over memory regions.
The pagers communicate with the Mach VM subsystem using UPL interfaces and derivatives of the
Mach pager interfaces.
Vnode
As we will see in Chapter 11, a vnode (virtual node) is a file-system-independent abstraction
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhCF85.htm 20.08.2007
Chapter 8. Memory Page 4 of 135
of a file system object, very much analogous to an abstract base class from which file-
system-specific instances are derived. Each active file or directory (where "active" has
context-dependent connotations) has an in-memory vnode.
The device pager, which handles device memory, is implemented in the I/O Kit portion of the
kernel. On 64-bit hardware, the device pager uses a part of the memory controllerthe Device
Address Resolution Table (DART)that is enabled by default on such hardware. The DART maps
addresses from 64-bit memory into the 32-bit address space of PCI devices.
The page-out daemon is a set of kernel threads that write portions of task address spaces to disk as
part of the paging operation in virtual memory. It examines the usage of resident pages and employs
an LRU[2]-like scheme to page out those pages that have not been used for over a certain time.
[2] Least recently used.
 The dynamic_pager user-space program creates and deletes swap files for the kernel's use. The
"pager" in its name notwithstanding, dynamic_pager does not perform any paging operations.
 The update user-space daemon periodically invokes the sync() system call to flush file system
caches to disk.
The task working set (TWS) detection subsystem maintains profiles of page-fault behaviors of tasks
on a per-application basis. When an application causes a page fault, the kernel's page-fault-handling
mechanism consults this subsystem to determine which additional pages, if any, should be paged in.
Usually the additional pages are adjacent to those being faulted in. The goal is to improve
performance by making residentspeculativelythe pages that may be needed soon.
The kernel provides several memory allocation mechanisms, some of which are subsystem-specific
wrappers around others. All such mechanisms eventually use the kernel's page-level allocator. User-
space memory allocation schemes are built atop the Mach VM API.
The Shared Memory Server subsystem is a kernel service that provides two globally shared memory
regions: one for text (starting at user virtual address 0x9000_0000) and the other for data (starting at
user virtual address 0xA000_0000). Both regions are 256MB in size. The text region is read-only
and is completely shared between tasks. The data region is shared copy-on-write. The dynamic link
editor (dyld) uses this mechanism to load shared libraries into task address spaces.
Figure 81. An overview of the Mac OS X memory subsystem
[View full size image]
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhCF85.htm 20.08.2007
Chapter 8. Memory Page 5 of 135
8.2.1. Reading Kernel Memory from User Space
Let us look at a couple of ways of reading kernel memory; these are useful in examining kernel data
structures from user space.
8.2.1.1. dd and /dev/kmem
The Mac OS X kernel provides the /dev/kmem character device, which can be used to read kernel virtual
memory from user space. The device driver for this pseudo-device disallows memory at addresses less
than VM_MIN_KERNEL_ADDRESS (4096) to be readthat is, the page at address 0 cannot be read.
Recall that we used the dd command in Chapter 7 to sample the sched_tick kernel variable by reading
from /dev/kmem. In this chapter, we will again read from this device to retrieve the contents of kernel data
structures. Let us generalize our dd-based technique so we can read kernel memory at a given address or
at the address of a given kernel symbol. Figure 82 shows a shell script that accepts a symbol name or an
address in hexadecimal, attempts to read the corresponding kernel memory, and, if successful, displays the
memory on the standard output. By default, the program pipes raw memory bytes through the hexdump
program using hexdump's -x (hexadecimal output) option. If the -raw option is specified, the program
prints raw memory on the standard output, which is desirable if you wish to pipe it through another
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhCF85.htm 20.08.2007
Chapter 8. Memory Page 6 of 135
program yourself.
Figure 82. A shell script for reading kernel virtual memory
#!/bin/sh
#
#readksym.sh
PROGNAME=readksym
if [ $# -lt 2 ]
then
echo "usage: $PROGNAME   [hexdump option|-raw]"
echo " $PROGNAME   [hexdump option|-raw]"
exit 1
fi
SYMBOL=$1 # first argument is a kernel symbol
SYMBOL_ADDR=$1 # or a kernel address in hexadecimal
IS_HEX=${SYMBOL_ADDR:0:2} # get the first two characters
NBYTES=$2 # second argument is the number of bytes to read
HEXDUMP_OPTION=${3:--x} # by default, we pass '-x' to hexdump
RAW="no" # by default, we don't print memory as "raw"
if [ ${HEXDUMP_OPTION:0:2} == "-r" ]
then
RAW="yes" # raw... don't pipe through hexdump -- print as is
fi
KERN_SYMFILE=`sysctl -n kern.symfile | tr '\\' '/'` # typically /mach.sym
if [ X"$KERN_SYMFILE" == "X" ]
then
echo "failed to determine the kernel symbol file's name"
exit 1
fi
if [ "$IS_HEX" != "0x" ]
then
# use nm to determine the address of the kernel symbol
SYMBOL_ADDR="0x`nm $KERN_SYMFILE | grep -w $SYMBOL | awk '{print $1}'`"
fi
if [ "$SYMBOL_ADDR" == "0x" ] # at this point, we should have an address
then
echo "address of $SYMBOL not found in $KERN_SYMFILE"
exit 1
fi
if [ ${HEXDUMP_OPTION:0:2} == "-r" ] # raw... no hexdump
then
dd if=/dev/kmem bs=1 count=$NBYTES iseek=$SYMBOL_ADDR of=/dev/stdout \
2>/dev/null
else
dd if=/dev/kmem bs=1 count=$NBYTES iseek=$SYMBOL_ADDR of=/dev/stdout \
2>/dev/null | hexdump $HEXDUMP_OPTION
fi
exit 0
$ sudo ./readksym.sh 0x5000 8 -c # string seen only on the PowerPC
0000000 H a g f i s h
0000008
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhCF85.htm 20.08.2007
Chapter 8. Memory Page 7 of 135
8.2.1.2. The kvm(3) Interface
Mac OS X also provides the kvm(3) interface for accessing kernel memory. It includes the following
functions:
 kvm_read() read from kernel memory
 kvm_write() write to kernel memory
 kvm_getprocs(), kvm_getargv(), kvm_getenvv() retrieve user process state
 kvm_nlist() retrieve kernel symbol table names
Figure 83 shows an example of using the kvm(3) interface.
Figure 83. Using the kvm(3) interface to read kernel memory
// kvm_hagfish.c
#include 
#include 
#include 
#include 
#include 
#define TARGET_ADDRESS (u_long)0x5000
#define TARGET_NBYTES (size_t)7
#define PROGNAME "kvm_hagfish"
int
main(void)
{
kvm_t *kd;
char buf[8] = { '\0' };
kd = kvm_open(NULL, // kernel executable; use default
NULL, // kernel memory device; use default
NULL, // swap device; use default
O_RDONLY, // flags
PROGNAME); // error prefix string
if (!kd)
exit(1);
if (kvm_read(kd, TARGET_ADDRESS, buf, TARGET_NBYTES) != TARGET_NBYTES)
perror("kvm_read");
else
printf("%s\n", buf);
kvm_close(kd);
exit(0);
}
$ gcc -Wall -o kvm_hagfish kvm_hagfish.c # string seen only on the PowerPC
$ sudo ./kvm_hagfish
Hagfish
$
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhCF85.htm 20.08.2007
Chapter 8. Memory Page 8 of 135
Raw Kernel Memory Access: Caveats
Exchanging information with the kernel by having raw access to its memory is unsatisfactory
for several reasons. To begin with, a program must know the actual names, sizes, and formats
of kernel structures. If these change across kernel versions, the program would need to be
recompiled and perhaps even modified. Besides, it is cumbersome to access complicated data
structures. Consider a linked list of deep structuresthat is, structures with one or more fields
that are pointers. To read such a list, a program must read each element individually and then
must separately read the data referenced by the pointer fields. It would also be difficult for
the kernel to guarantee the consistency of such information.
Moreover, the information sought by a user program must be either kernel-resident in its final
form (i.e., the kernel must compute it), or it must be computed from its components by the
program. The former requires the kernel to know about the various types of information user
programs might need, precompute it, and store it. The latter does not guarantee consistency
and requires additional hardcoded logic in the program.
Direct user-program access to all kernel memory may also be a security and stability concern,
even though such access normally requires superuser privileges. It is difficult to both specify
and enforce limits on the accessibility of certain parts of kernel memory. In particular, the