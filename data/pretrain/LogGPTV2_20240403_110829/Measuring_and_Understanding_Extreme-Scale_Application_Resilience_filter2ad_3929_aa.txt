# Measuring and Understanding Extreme-Scale Application Resilience: A Field Study of 5,000,000 HPC Application Runs

**Authors:** Catello Di Martino, William Kramer, Zbigniew Kalbarczyk, Ravishankar K. Iyer  
**Affiliation:** University of Illinois at Urbana-Champaign  
**Email:** {dimart, kalbarcz, wtkramer, rkiyer}@illinois.edu  
**Conference:** 2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks  
**DOI:** 10.1109/DSN.2015.50

## Abstract
This paper presents an in-depth characterization of the resiliency of over 5 million HPC application runs completed during the first 518 production days of Blue Waters, a 13.1 petaflop Cray hybrid supercomputer. Unlike previous studies, we focus on the impact of system errors and failures on user applications, which are the compiled programs launched by user jobs that can execute across one or more XE (CPU) or XK (CPU+GPU) nodes. The analysis is based on a joint examination of several data sources, including workload and error/failure logs. We developed LogDiver, a tool to automate data preprocessing and metric computation, to relate system errors and failures to the executed applications.

Key findings include:
- Approximately 1.53% of applications fail due to system problems, contributing to about 9% of the production node hours executed in the measured period.
- There is a dramatic increase in the application failure probability when scaling to full-scale applications: 20x (from 0.008 to 0.162) for XE applications from 10,000 to 22,000 nodes, and 6x (from 0.02 to 0.129) for GPU/hybrid applications from 2000 to 4224 nodes.
- The resiliency of hybrid applications is impaired by the lack of adequate error detection capabilities in hybrid nodes.

## 1. Introduction
Addressing the resilience challenge of extreme-scale machines requires understanding how system errors and failures impact applications and how resiliency is influenced by application characteristics. This paper provides an in-depth characterization of the application-level resiliency of over 5 million HPC application runs launched by approximately 1000 users during the first 518 production days of Blue Waters, a 13.1 petaflop Cray hybrid supercomputer at the University of Illinois National Center for Supercomputing Applications (NCSA).

Unlike previous work, this study focuses on the analysis of user applications, i.e., the compiled programs launched by user jobs, which can execute across one or more compute nodes. Parallel jobs can spawn multiple applications, which can execute concurrently and/or sequentially. Analyzing the error behavior of user applications within a batch job is crucial for accurately characterizing both system- and application-level resiliency. To our knowledge, this is the first work characterizing the application-level resiliency of an extreme-scale hybrid machine.

## 2. System, Workload, and Data Sources
### 2.1 Blue Waters System Overview
Blue Waters is a sustained petaflop system capable of delivering approximately 13.1 petaflops (at peak) for a range of real-world scientific and engineering applications. The system configuration includes:

- **XE Nodes:** 22,640 compute nodes in 5,660 Cray XE6 blades (4 nodes/blade), each with 2x 16-core AMD Opteron 6276 processors at 2.6 - 2.3 GHz and 64GB of DDR3 RAM.
- **XK Nodes:** 4,224 GPU hybrid nodes in 768 Cray XK7 blades (4 nodes/blade), each with 1 AMD Opteron 6276 processor, 32GB of DDR3 RAM, and an NVIDIA K20X GPU with 6 GB of DDR5 RAM.
- **Interconnects:** Gemini high-speed network (HSN) and LNet (Lustre) network.
- **File Systems:** Lustre-based file systems providing access to 26 PB of usable storage over 36 PB of raw disk space in three file systems: project, scratch, and home.
- **System Software:** Moab and Torque for resource management and batch scheduling, and Cray ALPS for the placement, launch, and monitoring of applications.

### 2.2 Data Sources
The analysis is based on a combination of empirical and analytical techniques, using data from:
- **Job-Level Logs:** 738,102 jobs.
- **Application-Level Logs:** 5,116,766 user application runs, totaling 198,112,165 node hours.
- **Syslogs:** 6.6TB of syslogs containing about 12 trillion events and 349,819,986 errors generated by system-level detectors and hardware sensors.

## 3. Methodology
To handle the large amount of textual data and extract meaningful insights, we developed LogDiver, a tool for the analysis and measurement of system- and application-level resiliency in extreme-scale environments. LogDiver automates data preprocessing and metric computation, enabling us to measure application-level resiliency.

## 4. Key Findings
- **Failure Rate and Impact:** While 1.53% of applications failed due to system problems, these failures contributed to about 9% of the total production node hours. This highlights the significant energy cost associated with lost work.
- **Scaling Impact:** The probability of application failure increases dramatically with the number of nodes. For XE applications, the failure probability increased 20x (from 0.008 to 0.162) when scaling from 10,000 to 22,000 nodes. For XK applications, the failure probability increased 6x (from 0.02 to 0.129) when scaling from 2000 to 4224 nodes.
- **Hybrid Node Resiliency:** Hybrid applications suffer from inadequate error detection capabilities, impairing their resiliency.
- **Energy Cost:** Failed applications, if not recovered through checkpoint/restart, add potentially $421,878 to the Blue Waters energy bill, based on an average power consumption of 2KW/blade and a cost of 0.047c/KW.
- **Failover Operations:** 37% of failed applications fail during or after failover operations, indicating that system-level resiliency mechanisms need to be better integrated with workload management and application-level resiliency mechanisms.
- **Error Propagation:** The probability of application failures can be modeled with a cubic function of node hours for XK applications and a quadratic function of node hours for XE applications, emphasizing the need for dedicated resiliency techniques to prevent error propagation from hardware to applications.

## 5. Conclusion
This study provides a comprehensive characterization of the application-level resiliency of an extreme-scale hybrid supercomputer. The findings highlight the need for novel resiliency methods and the integration of system- and application-level resiliency mechanisms to support the full spectrum of deliverable performance. Future work will focus on developing and evaluating these methods to enhance the overall resiliency of extreme-scale computing systems.