title:Measuring and Understanding Extreme-Scale Application Resilience:
A Field Study of 5, 000, 000 HPC Application Runs
author:Catello Di Martino and
William Kramer and
Zbigniew Kalbarczyk and
Ravishankar K. Iyer
2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Measuring and Understanding Extreme-Scale
Application Resilience: A Field Study of 5,000,000
HPC Application Runs
Catello Di Martino, Zbigniew Kalbarczyk, William Kramer, Ravishankar Iyer
University of Illinois at Urbana-Champaign
Email: {dimart,kalbarcz,wtkramer,rkiyer}@illinois.edu
Abstract—This paper presents an in-depth characterization of
the resiliency of more than 5 million HPC application runs
completed during the ﬁrst 518 production days of Blue Waters, a
13.1 petaﬂop Cray hybrid supercomputer. Unlike past work, we
measure the impact of system errors and failures on user applica-
tions, i.e., the compiled programs launched by user jobs that can
execute across one or more XE (CPU) or XK (CPU+GPU) nodes.
The characterization is performed by means of a joint analysis
of several data sources, which include workload and error/failure
logs. In order to relate system errors and failures to the executed
applications, we developed LogDiver, a tool to automate the data
preprocessing and metric computation. Some of the lessons learned
in this study include: i) while about 1.53% of applications fail due
to system problems, the failed applications contribute to about 9%
of the production node hours executed in the measured period,
i.e., the system consumes computing resources, and system-related
issues represent a potentially signiﬁcant energy cost for the work
lost; ii) there is a dramatic increase in the application failure
probability when executing full-scale applications: 20x (from 0.008
to 0.162) when scaling XE applications from 10,000 to 22,000
nodes, and 6x (from 0.02 to 0.129) when scaling GPU/hybrid
applications from 2000 to 4224 nodes; and iii) the resiliency of
hybrid applications is impaired by the lack of adequate error
detection capabilities in hybrid nodes.
I. INTRODUCTION
An important and often neglected step in addressing the
resilience challenge of extreme-scale machines is to understand
how system errors and failures impact applications and how
resiliency is affected by application characteristics. An in-
depth characterization of the application failures caused by
system-related issues is essential to assess the resiliency of
current systems and central to guide the design of resiliency
mechanisms in future extreme-scale machines.
In this paper, we present an in-depth characterization of the
application-level resiliency of more than 5 million HPC appli-
cation runs launched by about 1000 users during the ﬁrst 518
production days of Blue Waters, the 13.1 petaﬂop Cray hybrid
supercomputer at the University of Illinois National Center for
Supercomputing Applications (NCSA). The characterization is
performed by estimating (with respect to failures caused by
system-related issues) the hours and node-hours to application
failures and the probability of application failure. Unlike past
work, this paper focuses on the analysis of user applications,
i.e., the compiled programs launched by user jobs, which can
execute across one or more compute nodes. Parallel jobs can
spawn several applications at a time, i.e., different programs
on one or multiple nodes that can execute concurrently and/or
978-1-4799-8629-3/15 $31.00 © 2015 IEEE
978-1-4799-8629-3/15 $31.00 © 2015 IEEE
DOI 10.1109/DSN.2015.50
DOI 10.1109/DSN.2015.50
25
25
sequentially. We claim that it is important to analyze the error
behavior of user applications launched within a batch job
to accurately characterize both system- and application-level
resiliency. To the best of our knowledge, this is the ﬁrst work
characterizing the application-level resiliency of an extreme-
scale hybrid machine.
The analysis of application-level resilience requires use of
a mix of empirical and analytical techniques to: (i) handle
the large amount of textual data extracted from system- and
application-level
logs; (ii) decode speciﬁc types of system
events and exit statuses obtained from multiple data sources;
(iii) match different sources of information, such as workload
and error data, to extract signals of interest (e.g., error rates);
and (iv) measure error propagation and application resiliency.
To this end, we created LogDiver, a tool for the analysis
and measurement of system- and application-level resiliency in
extreme-scale environments. The value of LogDiver lies in how
it generates critical data that allows us to measure application-
level resiliency.
The contributions of this paper are the following:
• We present LogDiver, a tool for measuring resiliency of
HPC applications.
• We use LogDiver to measure the resiliency of a large
set of user applications over a period of 518 days. The
characterization is performed by mining i) 738,102 jobs
using job-level logs, ii) 5,116,766 user application runs
using the application-level logs generated by the applica-
tion loader and placement subsystem totaling 198,112,165
node hours, and iii) 6.6TB of syslogs containing about 12
trillion events and 349,819,986 errors generated by system-
level detectors and hardware sensors.
• We compare the resiliency of applications executed on XE
nodes (CPU) and on hybrid XK nodes (CPU+GPU). In
particular, we measure how the application-level resiliency
changes as a function of the number of nodes and produc-
tion node hours.
Key ﬁndings in this study are:
• While our measurements show that 1.53% of the total
applications failed because of system problems, the vari-
ation in failures is quite large. We measured an increase
of 20x in the application failure probability (from 0.008
to 0.162) when scaling XE applications from 10,000 to
22,000 nodes. Similarly, for XK applications we measured
an increase from 0.02 to 0.129 of the application failure
probability when scaling the applications from 2000 to
4224 nodes. In both platforms, small-scale applications
(i.e., not exceeding a blade) have a failure probability due
to system-related issues on the order of 1E − 5.
• The probability of application failure because of system
issues varies from 0.162 (±0.05 for 95% conﬁdence
interval) for XE applications to about 0.129 (±0.05 for
95% conﬁdence interval) for XK nodes, when approaching
the full scale (i.e., 22640 XE nodes and 4224 XK nodes,
respectively). This ﬁnding emphasizes the need for novel
resiliency methods to support resiliency across the full
spectrum of deliverable performance.
• On average, an application failure caused by system-
related issue occurs every 15 min. In total, those applica-
tions run for 17,952,261 node hours, i.e., about 9% of the
total production node hours. While the number of failed
applications is low compared to the number of executed
applications, our measurements show that the impact of
errors on applications is not negligible. Considering an
average power consumption of 2KW/blade [1], failed ap-
plications that are not recovered through checkpoint/restart
add potentially $421,878 to the Blue Waters energy bill
(based on a cost of 0.047c/KW).
• 37% of failed applications fail during/after failover opera-
tions. Although the system can survive a sustained number
of failures while keeping high availability, the level of
resiliency perceived by the user may suffer. Our data shows
that failures tolerated at system-level can still be harmful to
applications. System-level resiliency mechanisms should
interplay more profoundly with the workload management
system and application-level resiliency mechanisms in
order to avoid workload disruptions and reduce the impact
of system outages on user activities.
• Our measurements show that the probability of application
failures can be modeled with a cubic function of the node
hours for XK applications and a quadratic function of the
node hours for XE applications. This ﬁnding emphasizes
the need for: (i) dedicated resiliency techniques to be de-
ployed for preventing error propagation from the hardware
to the application in the hybrid nodes, and (ii) effective
assessment techniques at extreme scale in order to harness
hybrid computing power in future machines.
The rest of the paper is organized as it follows. Section
II provides background information on the target systems and
data sources. Section III illustrates a real failure scenario and
its impact on running applications. Section IV presents the
workﬂow implemented in LogDiver. Section V provides a
breakdown of the application failure data. Section VI presents
the failure statistics for 9 of the most utilized applications in
Blue Waters. Section VII discusses how the application scale
(i.e., number of nodes and node hours) inﬂuences application
resiliency. Section VIII presents the related research. Finally,
Section IX concludes the paper.
II. SYSTEM, WORKLOAD AND DATA SOURCES
Blue Waters is a sustained petaﬂop system capable of de-
livering approximately 13.1 petaﬂops (at peak) for a range of
real-world scientiﬁc and engineering applications. The system
conﬁguration is summarized in Table I, and the relevant com-
ponents are discussed below.
2626
TABLE I
SUMMARY OF THE SYSTEM SPECIFICATIONS AND ERROR/FAILURE
DETECTION AND RECOVERY TECHNIQUES EMPLOYED IN BLUE WATERS.
Components 
Error/failure impact 
Relevant specs 
Node 
Interconnect 
File system  
(Lustre)/ 
storage  
system 
(Sonexion 
1600) 
Sys. Software 
(scheduler,     
OS…) 
•  XE6 (22,640  nodes) 
•  2x 16 cores, 64 GB RAM 
•  XK7 (4,224 nodes) 
•  16 cores , 32GB RAM  
•  Nvidia K20X with 6GB RAM 
•  Gemini High-Speed 
Network (HSN) 
•  LNet (Lustre Network) 
Lustre  
•  Home = 3 PB 
•  Project= 3 PB 
•  Scratch=20 PB 
Storage nodes 
•  Home = 36 nodes 
•  Project = 36 nodes 
•  Scratch = 360 nodes 
•  Torque/Moab (Job Scheduling) 
•  ALPS (Applications  
•  CNL (Node OS) 
Detection/recovery 
•  Node warm-swap 
•  Heartbeat 
•  Coding (ECC/Chipkill/Parity) 
•  Application checkpoint/restart 
•  Node Health Checker 
•  Emergency Power Off (EPO) 
•  Blade Module Controller (BMC) 
•  Blade Module Controller *BMC) 
•  Coding (ECC/Parity) 
•  Network reroute 
•  Network Throttling 
•  Retry 
Application-level 
•  None/not observable 
•  Abnormal termination 
(e.g., crash) 
•  System-initiated  
termination (e.g., kill) 
•  User-initiated termination 
•  Application starvation and 
termination for walltime 
limit 
•  Application restart/retry/
reconnect 
•  Dual redundancy (OST/OSS/
MDS) 
•  multi-level RAID 6 
•  Lustre client-level assertions 
•  Timeouts 
•  System failover 
•  Dual redundancy  
•  Heartbeat, Timeouts 
•  TCP Connection status 
•  Cray Resiliency Architecture 
System-level  
•  Node Down 
•  Network Quiesced-
reconfigured-resumed 
•  Network Congestion 
•  File System failover 
•  Scheduler Paused 
•  System Wide Outage 
(total/partial 
unavailability; service 
level below threshold) 
•  None 
XE and XK nodes. There are 22,640 compute nodes (XE
nodes) in 5,660 Cray XE6 blades (4 nodes/blade). A compute
node consists of 2x 16-core AMD Opteron 6276 processors at
2.6 - 2.3 GHz and is equipped with 64GB of DDR3 RAM.
There are 4224 GPU hybrid nodes (XK nodes) equipped with
NVIDIA K20X GPU accelerators, 32GB of DDR3 ram, and
1 AMD Opteron 6276 processor. GPU nodes are hosted in
768 Cray XK7 blades (4 nodes/blade). The NVIDIA K20X
is equipped with 2,880 single-precision Cuda cores and 6 GB
of DDR5 RAM memory. XE and XK nodes run the Cray
lightweight kernel Compute Node Linux (CNL).
Integrated Hardware Supervisory System (HSS). HSS or-
chestrates system-level error detection and mitigation. It sup-
ports the warm-swap of failed nodes without disrupting an
active workload when possible. If all connectivity is lost
between two interconnects, the HSS automatically reconﬁgures
a route around the bad link if additional routes are available.
Additional details on system conﬁguration, resiliency features,
and effectiveness of the failover process are provided in [2].
The Gemini high-speed network (HSN) and LNet (Lustre)
network. These allow communication between nodes and with
the ﬁle system Object Storage Servers (OSSs), respectively. In
the event of a lane (i.e., a subchannel in a Gemini link) failure,
the adaptive routing hardware automatically masks the failure
when possible.
Lustre-based ﬁle systems. These provide access to 26 PB of
usable storage over 36 PB of raw disk space in 3 ﬁle systems,
i.e., project, scratch, and home, installed in 198 Cray Sonexion
1600 storage units and including 360 nodes for the /scratch ﬁle
system (20 PB), 36 for the /home ﬁle system (3 PB), and 36
for the /project ﬁle system (3 PB).
Moab and Torque. [3] These provide for resource manage-
ment and batch scheduling and Cray ALPS for the placement,
launch, and monitoring of the applications that compose a job,
i.e., the compiled program that the user can execute across one
or more compute or GPU nodes. Figure 1 shows a commented
snippet of a script for the execution of a job in Blue Waters.
BREAKDOWN OF XE AND XK WORKLOAD SIZE (NUMBER OF USED NODES).
TABLE III
Fig. 1. Example of script for job and applications execution in Blue Waters.
TABLE IV
SUMMARY OF THE DATA SOURCES
SCIENTIFIC AREAS AND CHARACTERISTICS OF BLUE WATERS
TABLE II
APPLICATIONS.
i
d
G
d
i
r
G
t
c
u
r
t
s
n
U
t
c
u
r
t
S
x
x
i
r
t
a
M
e
s
r
a
p
S
i
r
t
a
M
e
s
n
e
D
y
d
o
b
-
N
o
l
r
a
C
e
t
n
o
M
T
F
F
C
P
I
O
I
t
n
a
c
  average 
duration 
node 
hours  hours 
i
f
i
n
g
S
i
max 
nodes 
Science Area 
runs 
Example Codes 
X    
2,336 
5,952 
18,116 
802,518 
   X    
1.73  22,425 
   X    
   X  X  X     X  X 332 
H3D(M),VPIC, OSIRIS, 
Magtail/ UPIC X    
CESM, GCRM, CM1/ WRF, 
   X 1,075  1.63  15,960 
HOMME X X     X     X    
   X     X     X 7,651  2.83  25,670 
   X  X  X  X    
   X  X  X  X    
   X    
PPM, MAESTRO, CASTRO, 
SEDONA, ChaNGa, MS-