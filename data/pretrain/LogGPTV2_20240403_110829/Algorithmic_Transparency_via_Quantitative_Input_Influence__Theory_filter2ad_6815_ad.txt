(cid:9)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ ε
(cid:13)−2nε2
(cid:14)
Δ
(Xi − EXi)
≤ 2 exp
Since all the samples of measures discussed in the paper
[0, 1], we admit an ε-δ
are bounded within the interval
approximation scheme where the number of samples n can
be chosen to be greater than log(2/δ)/2ε2. Note that these
bounds are independent of the size of the dataset. Therefore,
given an efﬁcient sampler, these quantities of interest can be
approximated efﬁciently even for large datasets.
VI. PRIVATE TRANSPARENCY REPORTS
One important concern is that releasing inﬂuence measures
estimated from a dataset might leak information about in-
dividual users; our goal is providing accurate transparency
reports, without compromising individual users’ private data.
To mitigate this concern, we add noise to make the measures
differentially private. We show that the sensitivities of the QII
measures considered in this paper are very low and therefore
very little noise needs to be added to achieve differential
privacy.
The sensitivity of a function is a key parameter in ensuring
that it is differentially private; it is simply the worst-case
change in its value, assuming that we change a single data
point in our dataset. Given some function f over datasets, we
deﬁne the sensitivity of a function f with respect to a dataset
D, denoted by Δf (D) as
|f (D) − f (D(cid:3)
)|
maxD(cid:2)
where D and D(cid:3) differ by at most one instance. We use the
shorthand Δf when D is clear from the context.
In order to not
leak information about
the users used
to compute the inﬂuence of an input, we use the standard
Laplace Mechanism [8] and make the inﬂuence measure
differentially private. The amount of noise required depends
on the sensitivity of the inﬂuence measure. We show that
the inﬂuence measure has low sensitivity for the individuals
used to sample inputs. Further, due to a result from [9] (and
stated in [25]), sampling ampliﬁes the privacy of the computed
statistic, allowing us to achieve high privacy with minimal
noise addition.
The standard technique for making any function differ-
entially private is to add Laplace noise calibrated to the
sensitivity of the function:
Theorem 11 ([8]). For any function f from datasets to R,
the mechanism Kf that adds independently generated noise
with distribution Lap(Δf (D)/) to the k output enjoys -
differential privacy.
Since each of the quantities of interest aggregate over a
large number of instances, the sensitivity of each function is
very low.
Theorem 12. Given a dataset D,
1) ΔˆED(c(X) = 1) = 1|D|
2) ΔˆED(c(X−S) = 1) ≤ 2|D|
(cid:16)
3) ΔˆED(c(X−S) = 1|X = x) = 1|D|
4) ˆQY
Proof. We examine some cases here. In Equation 10, if two
datasets differ by one instance, then at most one term of the
summation will differ. Since each term can only be either 0
or 1, the sensitivity of the function is
disp(S) ≤ max
1|D∩Y| ,
1|D\Y|
(cid:15)
(cid:12)(cid:12)(cid:12)(cid:12) 0|D| − 1|D|
(cid:12)(cid:12)(cid:12)(cid:12) =
1|D| .
ΔˆED(c(X) = 1) =
606606
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:16:24 UTC from IEEE Xplore.  Restrictions apply. 
Similarly, in Equation 11, an instance appears 2|D| − 1
times, once each for the inner summation and the outer
summation, and therefore, the sensitivity of the function is
ΔˆED(c(X−S) = 1) =
2|D| − 1
|D|2
≤ 2|D| .
For individual outcomes (Equation (12)), similarly, only one
term of the summation can differ. Therefore, the sensitivity of
(12) is 1/|D|.
Finally, we observe that a change in a single element x(cid:3) of
1|D∩Y| if x(cid:3) ∈ D ∩ Y, or
D will cause a change of at most
(cid:16)
1|D\Y| if x(cid:3) ∈ D \ Y. Thus, the maximal change to
of at most
(13) is at most max
(cid:15)
1|D\Y|
1|Y| ,
.
While the sensitivity of most quantities of interest is low
disp(S) can be quite high when |Y| is either
(at most a 2|D|), ˆQY
very small or very large. This makes intuitive sense: if Y is
a very small minority, then any changes to its members are
easily detected; similarly, if Y is a vast majority, then changes
to protected minorities may be easily detected.
We observe that the quantities of interest which exhibit
low sensitivity will have low inﬂuence sensitivity as well:
inﬂuence of S is 1(c(x) = 1) −
for example,
ˆED(c(X−S) = 1] | X = x); changing any x(cid:3) ∈ D (where
x(cid:3)
to the local
inﬂuence.
(cid:7)= x will result in a change of at most
the local
1|D|
Finally, since the Shapley and Banzhaf indices are normal-
ized sums of the differences of the set inﬂuence functions, we
can show that if an inﬂuence function ι has sensitivity Δι,
then the sensitivity of the indices are at most 2Δι.
To conclude, all of the QII measures discussed above
(except for group parity) have a sensitivity of α|D|, with α
being a small constant. To ensure differential privacy, we need
only need add noise with a Laplacian distribution Lap(k/|D|)
to achieve 1-differential privacy.
Further, it is known that sampling ampliﬁes differential
privacy.
Theorem 13 ([9], [25]). If A is 1-differentially private, then
for any  ∈ (0, 1), A(cid:3)
() is 2-differentially private, where
A(cid:3)
() is obtained by sampling an  fraction of inputs and
then running A on the sample.
Therefore, our approach of sampling instances from D to
speed up computation has the additional beneﬁt of ensuring
that our computation is private.
Table I contains a summary of all QII measures deﬁned in
this paper, and their sensitivity.
VII. EXPERIMENTAL EVALUATION
We demonstrate the utility of the QII framework by develop-
ing two simple machine learning applications on real datasets.
Using these applications, we ﬁrst argue, in Section VII-A,
the need for causal measurement by empirically demonstrat-
ing that in the presence of correlated inputs, observational
measures are not informative in identifying which inputs were
607607
actually used. In Section VII-B, we illustrate the distinction
between different quantities of interest on which Unary QII
can be computed. We also illustrate the effect of discrimination
on the QII measure. In Section VII-C, we analyze transparency
reports of three individuals to demonstrate how Marginal QII
can provide insights into individuals’ classiﬁcation outcomes.
Finally, we analyze the loss in utility due to the use of
differential privacy, and provide execution times for generating
transparency reports using our prototype implementation.
We use the following datasets in our experiments:
• adult [10]: This standard machine learning benchmark
dataset is a a subset of US census data that classiﬁes
the income of individuals, and contains factors such as
age, race, gender, marital status and other socio-economic
parameters. We use this dataset to train a classiﬁer that
predicts the income of individuals from other parameters.
Such a classiﬁer could potentially be used to assist credit
decisions.
• arrests [11]: The National Longitudinal Surveys are a
set of surveys conducted by the Bureau of Labor Statistics
of the United States. In particular, we use the National
Longitudinal Survey of Youth 1997 which is a survey of
young men and women born in the years 1980-84. Re-
spondents were ages 12-17 when ﬁrst interviewed in 1997
and were subsequently interviewed every year till 2013.
The survey covers various aspects of an individual’s life
such as medical history, criminal records and economic
parameters. From this dataset, we extract the following
features: age, gender, race, region, history of drug use,
history of smoking, and history of arrests. We use this
data to train a classiﬁer that predicts history of arrests to
aid in predictive policing, where socio-economic factors
are used to decide whether individuals should receive a
visit from the police. This application is inspired by a
similar application in [26].
The two applications described above are hypothetical ex-
amples of decision-making aided by machine learning that use
potentially sensitive socio-economic data about individuals,
and not real systems that are currently in use. We use these
classiﬁers to illustrate the subtle causal questions that our QII
measures can answer.
We use the following standard machine learning classiﬁers
in our dataset: Logistic Regression, SVM with a radial basis
function kernel, Decision Tree, and Gradient Boosted Decision
Trees. Bishop’s machine learning text [27] is an excellent
resource for an introduction to these classiﬁers. While Logistic
Regression is a linear classiﬁer, the other three are nonlinear
and can potentially learn very complex models. All our ex-
periments are implemented in Python with the numpy library,
and the scikit-learn machine learning toolkit, and run on an
Intel i7 computer with 4 GB of memory.
A. Comparison with Observational Measures
In the presence of correlated inputs, observational measures
often cannot identify which inputs were causally inﬂuential.
To illustrate this phenomena on real datasets, we train two
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:16:24 UTC from IEEE Xplore.  Restrictions apply. 
Name
QII on Individual Outcomes (3)
QII on Actual Individual Outcomes (4)
Average QII (5)
QII on Group Outcomes (6)
QII on Group Disparity (8)
Notation
ιind(S)
ιind-act(S)
ιind-avg(S)
ιY
grp(S)
ιY
(S)
disp
Quantity of Interest
Positive Classiﬁcation of an Individual
Actual Classiﬁcation of an Individual
Average Actual Classiﬁcation
Positive Classiﬁcation for a Group
Difference in classiﬁcation rates among groups
Sensitivity
1/|D|
1/|D|
2/|D|
2/|D ∩ Y|
2 max(1/|D \ Y|, 1/|D ∩ Y|)
TABLE I: A summary of the QII measures deﬁned in the paper
classiﬁers: (A) where gender is provided as an actual input,
and (B) where gender is not provided as an input. For classiﬁer
(B), clearly the input Gender has no effect and any correlation
between the outcome and gender is caused via inference from
other inputs. In Table II, for both the adult and the arrests
dataset, we compute the following observational measures:
Mutual Information (MI), Jaccard Index (Jaccard), Pearson
Correlation (corr), and the Disparate Impact Ratio (disp) to
measure the similarity between Gender and the classiﬁers
outcome. We also measure the QII of Gender on outcome.
We observe that in many scenarios the observational quantities
do not change, or sometimes increase, from classiﬁer A to
classiﬁer B, when gender is removed as an actual
input
to the classiﬁer. On the other hand, if the outcome of the
classiﬁer does not depend on the input Gender, then the QII
is guaranteed to be zero.
B. Unary QII Measures
In Figure 2, we illustrate the use of different Unary QII
measures. Figures 2a, and 2b, show the Average QII measure
(Equation 5) computed for features of a decision forest classi-
ﬁer. For the income classiﬁer trained on the adult dataset, the
feature with highest inﬂuence is Marital Status, followed by
Occupation, Relationship and Capital Gain. Sensitive features
such as Gender and Race have relatively lower inﬂuence.
For the predictive policing classiﬁer trained on the arrests
dataset, the most inﬂuential input is Drug History, followed by
Gender, and Smoking History. We observe that inﬂuence on
outcomes may be different from inﬂuence on group disparity.
QII on group disparity: Figures 2c, 2d show inﬂuences
of features on group disparity for two different settings. The
ﬁgure on the left shows the inﬂuence of features on group
disparity by Gender in the adult dataset; the ﬁgure on the
right shows the inﬂuence of group disparity by Race in the
arrests dataset. For the income classiﬁer trained on the
adult dataset, we observe that most inputs have negative
inﬂuence on group disparity; randomly intervening on most
inputs would lead to a reduction in group disparity. In other
words, a classiﬁer that did not use these inputs would be fairer.
Interestingly, in this classiﬁer, marital status and not sex has
the highest inﬂuence on group disparity by sex.
For the arrests dataset, most inputs have the effect of
increasing group disparity if randomly intervened on. In
particular, Drug history has the highest positive inﬂuence on
disparity in arrests. Although Drug history is correlated with
race, using it reduces disparate impact by race, i.e. makes fairer
decisions.
In both examples, features correlated with the sensitive
attribute are the most inﬂuential for group disparity according
to the sensitive attribute instead of the sensitive attribute
itself. It
is in this sense that QII measures can identify
proxy variables that cause associations between outcomes and
sensitive attributes.
QII with artiﬁcial discrimination: We simulate discrimi-
nation using an artiﬁcial experiment. We ﬁrst randomly assign
ZIP codes to individuals in our dataset. Then to simulate
systematic bias, we make an f fraction of the ZIP codes
discriminatory in the following sense: All individuals in the
protected set are automatically assigned a negative classiﬁ-
cation outcome. We then study the change in the inﬂuence
of features as we increase f. Figure 3a, shows that
the
inﬂuence of Gender increases almost linearly with f. Recall
that Marital Status was the most inﬂuential feature for this
classiﬁer without any added discrimination. As f increases,
the importance of Marital Status decreases as expected, since
the number of individuals for whom Marital Status is pivotal
decreases.
C. Personalized Transparency Reports
To illustrate the utility of personalized transparency reports,
we study the classiﬁcation of individuals who received poten-
tially unexpected outcomes. For the personalized transparency
reports, we use decision forests.
The inﬂuence measure that we employ is the Shapley value,
with the underlying cooperative game deﬁned over the local
inﬂuence Q. In more detail, v(S) = ιQA (S), with QA being
E[c(·) = 1 | X = x]; that is, the marginal contribution of
i ∈ N to S is given by mi(S) = E[c(X−S) = 1 | X =
x] − E[c(X−S∪{i}) = 1 | X = x].
We emphasize that some features may have a negative
Shapley value; this should be interpreted as follows: a feature
with a high positive Shapley value often increases the certainty
that the classiﬁcation outcome is 1, whereas a feature whose
Shapley value is negative is one that increases the certainty
that the classiﬁcation outcome would be zero.
Mr. X: The ﬁrst example is of an individual from the
adult dataset, who we refer to as Mr. X, and is described in
Figure 4a. He is is deemed to be a low income individual, by
an income classiﬁer learned from the data. This result may be
surprising to him: he reports high capital gains ($14k), and
only 2.1% of people with capital gains higher than $10k are
reported as low income. In fact, he might be led to believe that
his classiﬁcation may be a result of his ethnicity or country
of origin. Examining his transparency report in Figure 4b,
however, we ﬁnd that the the most inﬂuential features that led
608608
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:16:24 UTC from IEEE Xplore.  Restrictions apply. 
logistic
kernel svm
decision tree
random forest
adult
0.045
0.043
0.501
0.500
0.218
0.215
0.286
0.295