title:Hyperloop: group-based NIC-offloading to accelerate replicated transactions
in multi-tenant storage systems
author:Daehyeok Kim and
Amirsaman Memaripour and
Anirudh Badam and
Yibo Zhu and
Hongqiang Harry Liu and
Jitu Padhye and
Shachar Raindel and
Steven Swanson and
Vyas Sekar and
Srinivasan Seshan
HyperLoop: Group-Based NIC-Offloading to
Accelerate Replicated Transactions in Multi-Tenant
Storage Systems
Daehyeok Kim1∗, Amirsaman Memaripour2∗, Anirudh Badam3,
Yibo Zhu3, Hongqiang Harry Liu3†, Jitu Padhye3, Shachar Raindel3,
Steven Swanson2, Vyas Sekar1, Srinivasan Seshan1
1Carnegie Mellon University, 2UC San Diego, 3Microsoft
ABSTRACT
Storage systems in data centers are an important component
of large-scale online services. They typically perform repli-
cated transactional operations for high data availability and
integrity. Today, however, such operations suffer from high
tail latency even with recent kernel bypass and storage op-
timizations, and thus affect the predictability of end-to-end
performance of these services. We observe that the root cause
of the problem is the involvement of the CPU, a precious
commodity in multi-tenant settings, in the critical path of
replicated transactions. In this paper, we present HyperLoop,
a new framework that removes CPU from the critical path
of replicated transactions in storage systems by offloading
them to commodity RDMA NICs, with non-volatile memory
as the storage medium. To achieve this, we develop new and
general NIC offloading primitives that can perform memory
operations on all nodes in a replication group while guar-
anteeing ACID properties without CPU involvement. We
demonstrate that popular storage applications can be easily
optimized using our primitives. Our evaluation results with
microbenchmarks and application benchmarks show that
HyperLoop can reduce 99th percentile latency ≈ 800× with
close to 0% CPU consumption on replicas.
∗The first two authors contributed equally to this work.
†The author is now in Alibaba Group.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. Copyrights
for components of this work owned by others than the author(s) must
be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee. Request permissions from permissions@acm.org.
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
© 2018 Copyright held by the owner/author(s). Publication rights licensed
to ACM.
ACM ISBN 978-1-4503-5567-4/18/08...$15.00
https://doi.org/10.1145/3230543.3230572
CCS CONCEPTS
• Networks → Data center networks; • Information sys-
tems → Remote replication; • Computer systems orga-
nization → Cloud computing;
KEYWORDS
Distributed storage systems; Replicated transactions; RDMA;
NIC-offloading
ACM Reference Format:
Daehyeok Kim, Amirsaman Memaripour, Anirudh Badam, Yibo
Zhu, Hongqiang Harry Liu, Jitu Padhye, Shachar Raindel, Steven
Swanson, Vyas Sekar, Srinivasan Seshan. 2018. HyperLoop: Group-
Based NIC-Offloading to Accelerate Replicated Transactions in
Multi-Tenant Storage Systems. In SIGCOMM ’18: ACM SIGCOMM
2018 Conference, August 20–25, 2018, Budapest, Hungary. ACM, New
York, NY, USA, 16 pages. https://doi.org/10.1145/3230543.3230572
1 INTRODUCTION
Distributed storage systems are an important building block
for modern online services. To guarantee data availability
and integrity, these systems keep multiple replicas of each
data object on different servers [3, 4, 8, 9, 17, 18] and rely on
replicated transactional operations to ensure that updates are
consistently and atomically performed on all replicas.
Such replicated transactions can incur large and unpre-
dictable latencies, and thus impact the overall performance
of storage-intensive applications [52, 57, 58, 75, 86, 92]. Rec-
ognizing this problem, both networking and storage commu-
nities have proposed a number of solutions to reduce average
and tail latencies of such systems.
Networking proposals include kernel bypass techniques,
such as RDMA (Remote Direct Memory Access) [64], and
userspace networking technologies [26, 90]. Similarly, there
have been efforts to integrate non-volatile main memory
(NVM) [6, 11], and userspace solid state disks (SSDs) [7, 29,
98] to bypass the OS storage stack to reduce latency.
While optimizations such as kernel bypass do improve the
performance for standalone storage services and appliance-
like systems where there is only a single service running in
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
D. Kim and A. Memaripour et al.
the entire cluster [60, 100, 101], they are unable to provide
low and predictable latency for multi-tenant storage systems.
The problem is two fold. First, many of the proposed tech-
niques for reducing average and tail latencies rely on using
CPU for I/O polling [7, 26]. In a multi-tenant cloud setting,
however, providers cannot afford to burn cores in this man-
ner to be economically viable. Second, even without polling,
the CPU is involved in too many steps in a replicated storage
transaction. Take a write operation as an example: i) It needs
CPU for logging, processing of log records, and truncation of
the log to ensure that all the modifications listed in a trans-
action happen atomically; ii) CPU also runs a consistency
protocol to ensure all the replicas reach identical states be-
fore sending an ACK to the client; iii) During transactions,
CPU must be involved to lock all replicas for the isolation
between different transactions for correctness; iv) Finally,
CPU ensures that the data from the network stack reaches a
durable storage medium before sending an ACK.
To guarantee these ACID properties, the whole transac-
tion has to stop and wait for a CPU to finish its tasks at each
of the four steps. Unfortunately, in multi-tenant storage sys-
tems, which co-locate 100s of database instances on a single
server [67, 92] to improve utilization, the CPU is likely to
incur frequent context switches and other scheduling issues.
Thus, we explore an alternative approach for predictable
replicated transaction performance in a multi-tenant envi-
ronment by completely removing the CPU from the critical
path. In this paper, we present HyperLoop, a design that
achieves this goal. Tasks that previously needed to run on
CPU are entirely offloaded to commodity RDMA NICs, with
Non-volatile Memory (NVM) as the storage medium, with-
out the need for CPU polling. Thus, HyperLoop achieves
predictable performance (up to 800× reduction of 99th per-
centile latency in microbenchmarks!) with nearly 0% CPU
usage. Our insight is driven by the observation that repli-
cated transactional operations are essentially a set of memory
operations and thus are viable candidates for offloading to
RDMA NICs, which can directly access or modify contents
in NVM.
In designing HyperLoop, we introduce new and general
group-based NIC offloading primitives for NVM access in
contrast to conventional RDMA operations that only of-
fload point-to-point communications via volatile memory.
HyperLoop has a necessary mechanism for accelerating repli-
cated transactions, which helps perform logically identical
and semantically powerful memory operations on a group
of servers’ durable data without remote CPUs’ involvement.
These group-based primitives are sufficient to offload op-
erations that are conventionally performed by CPUs in state-
of-the-art NVM and RDMA based replication systems. Such
operations include consistent and atomic processing and
truncating of log updates across all replicas, acquiring the
same logical lock across all replicas, and durably flushing the
volatile data across all replicas. HyperLoop can help offload
these to the NIC.1
Realizing these primitives, however, is not straightforward
with existing systems. Our design entails two key technical
innovations to this end. First, we repurpose a less-studied yet
widely supported RDMA operation that lets a NIC wait for
certain events before executing RDMA operations in a special
queue. This enables us to pre-post RDMA operations which
are triggered only when the transactional requirements are
met. Second, we develop a remote RDMA operation posting
scheme that allows a NIC to enqueue RDMA operations on
other NICs in the network. This is realized by modifying
the NIC driver and registering the driver metadata region
itself to be RDMA-accessible (with safety checks) from other
NICs. By combining these two techniques, an RDMA-capable
NIC can precisely program a group of other NICs to perform
replicated transactions.
Our approach is quite general as it only uses commodity
RDMA NICs and as such applications can adopt our prim-
itives with ease. For example, we modified RocksDB (an
open source alternative to Google LevelDB) and MongoDB
(an open source alternative to Azure CosmosDB and Ama-
zon DynamoDB) to use HyperLoop with under 1000 lines
of code. We evaluate the performance of these systems us-
ing microbenchmarks as well as the Yahoo Cloud Storage
Benchmark (YCSB) workload. The results show that run-
ning MongoDB with HyperLoop decreases average latency
of insert/update operations by 79% and reduces the gap be-
tween average and 99th percentile by 81%, while CPU usage
on backup nodes goes down from nearly 100% to almost
0%. Further, microbenchmarks show that HyperLoop-based
group memory operations can be performed more than 50×
and 800× faster than conventional RDMA-based operations
in average and tail cases, respectively.
2 BACKGROUND & MOTIVATION
In this section, we briefly define the key operations of repli-
cated storage systems. We also present benchmarks that
highlight the high tail latency incurred by these systems in a
multi-tenant setting, even with state-of-the-art optimizations
such as kernel bypass.
2.1 Storage Systems in Data Centers
Replicated storage systems maintain multiple copies of data
to deal with failures. For instance, large-scale block stores [20,
22, 36, 53, 63], key-value stores [23, 43], and databases [19, 21,
24, 41] replicate data to increase availability and avoid data
loss in the presence of failures. Such systems use comprehen-
sive protocols [30, 33, 34] to ensure that every data update
1The idea of HyperLoop, i.e., offloading replicated transactions to NICs in
addition to point-to-point read/write operations, can be extended to other
hardware combinations as long as the NIC can directly access the storage
medium, such as FPGA based-Ethernet NICs that can access SSDs [55].
HyperLoop
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Figure 1: Servers in data center storage systems typically co-locate 100s of replicas of many tenants to improve utilization.
This increases latency due to frequent context switches when replica processes, network and storage stacks continually vie
for the CPU. Unfortunately, it is not economically viable for 100s of replica processes to be pinned on dedicated cores and poll.
Each replica prepares the transaction for commit and for-
is applied to enough (to sustain availability and durability)
wards it down the chain similarly. When the tail of the chain
copies before acknowledging the changes to the client. At
the heart of these protocols are mechanisms to make identi-
receives the request the second phase starts. The tail knows
that everyone upstream is ready to commit. It then sends an
cal changes to multiple replicas before deeming the change
ACK that propagates back to the head. Every replica gets the
durable and making it available for readers.
ACK, knows everyone is ready, and commits the transaction.
Sub-operations for transactions: Changes to storage con-
tents are typically structured as an atomic transaction, con-
Finally, the head gets the ACK and sends the transaction ACK
to the application. In this way, chain replication provides
sisting of a set of reads and writes. For instance, a transac-
high-throughput, high-availability, and linearizability in a
tion might modify objects X and Y as shown in Figure 1(c).
The entire set of changes made by each transaction must be
simple manner.
atomic, that is X and Y should both change to the values 1
and 2 simultaneously. Storage replicas perform a number of
sub-operations per client change and a slowdown in any of
these sub-operations can slow down the entire transaction.
Storage systems use logging (undo/redo/write-ahead) to
achieve atomicity. The new values are first written to a log
and later the objects are modified one by one. If the modifi-
cations are paused for any reason, then simply re-applying
them from the log will ensure atomicity. Further, while pro-
cessing the log, the replica needs to block other transactions
from the objects involved. This is typically implemented us-
ing locks. For instance, in the above example, the storage
system would lock objects X and Y while processing the log.
The storage system must further ensure that all or a suf-
ficient number of replicas execute a transaction before it is
considered committed. Typically, a consensus protocol called
two-phase commit [30, 33, 34, 44] over a primary-backup set-
ting is used to replicate transactions since it enables strong
consistency and simplifies application development. As Fig-
ure 1(b) shows, the replicas first respond whether they are
ready to commit the transaction, then actually commit it
after all or enough replicas respond that they are ready.
Chain replication: Chain replication is a widely used primary-
backup replication known for its simplicity [1, 10, 35, 46–
48, 53, 62, 63, 81, 85, 89, 93–95]. In chain replication, the
replicas are laid out in a linear chain. Writes begin at the
head of the chain and propagate down the chain in the first
phase. The head of the chain begins executing a transaction
and readies the transaction to commit by creating a local
log entry and grabbing the necessary locks, and only then
forwards the transaction to the next replica in the chain.
Given the wide popularity of chain replication, our imme-
diate goal in this work is to develop NIC offload primitives
for this mode of replication. However, we note that our prim-
itives are generic enough to be used by other replication
protocols, non-ACID systems, and a variety of consistency
models (see §7).
2.2 Multi-tenancy Causes High Latency
Busy CPU causes high latency: Busy CPU is a major
reason why replicas in such protocols may not be responsive
– chain or otherwise. A replica’s thread has to be scheduled
on a CPU for it to receive the log via the network stack, and
subsequently store the log via the storage stack. The thread
needs to participate in the two-phase commit protocol and
in the chain (or other) replication scheme. Finally, the thread
must process the log and update the actual data objects. Locks
are needed for many of these steps to ensure correctness and
consistency.
In multi-tenant systems, CPUs are shared across multiple
tenants each containing one or more processes. This can lead
to heavy CPU load, and unpredictable scheduling latency.
The delay for being scheduled to run on CPU causes inflated
latencies for writes that are waiting for ACKs or locks.
The problem is further exacerbated by data partitioning.
To increase server resource utilization, large-scale storage
systems divide the data into smaller partitions such that each
server stores partitions of multiple tenants. For instance, an
online database partition ranges between 5–50GB of space
while typical servers have 4–8 TB of storage space [66]. Thus,
each server hosts 100s of tenants translating to 100s of replica
processes since each tenant should be isolated in at least one
process. Such a large number of processes easily saturates
CPU, causing high latency.
(a) Typical Storage Server(b) Typical Replicated Transaction Protocol(c) Typical Replica (Primary/Backup) SoftwareClientsPrimaryReplicasBackupReplicasReplica SetServer AServer BServer CTransaction?AckServer D2 Phase CommitTx? ACK!Commit? ACK!Phase 1Tx?ACKPhase 2Commit?ACKStorage StackDatabaseLogTx # 1:X=1, Y=2Valid = 0Tx # 1:Valid = 11. Store2. ACK3. OK?4. ACKTCPStackLocalTransactionProtocol5. Lock & Update X & Y6. Delete Log Unlock X & YSIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
D. Kim and A. Memaripour et al.
Average
95th percentile
99th percentile
Primary’s context-switches
Backup’s context-switches
)
s
m
(
y
c
n
e
t
a