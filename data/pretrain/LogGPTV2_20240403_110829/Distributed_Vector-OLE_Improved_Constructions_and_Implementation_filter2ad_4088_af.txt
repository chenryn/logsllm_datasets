choose a constant 𝜅 and 𝑚 = 𝑂(𝑡) such that 𝜂 ≥ 40 for the pa-
rameter ranges we’re interested in. Together with the observation
that 𝑡 and 𝑘 are both in 𝑂(√
𝑛) [9], this simplifies the communica-
√
tion overhead of our protocol to 𝑂(𝜆
𝑛 log 𝑛) and the computation
to 𝑂(𝜆𝑛).
7 APPLICATIONS
Our distributed pseudorandom vector OLE protocol can be seen as
a communication efficient precomputation that enables arbitrary
secure two-party scalar-vector multiplications. This is thanks to
a simple reduction from VOLE to pseudorandom vector OLE. The
reduction is analogous to how a random multiplication triple can be
exploited to compute extremely efficiently a secure multiplication
in just a round of communication. The reduction from VOLE to
pseudorandom VOLE is given in [9] (Proposition 10). The over-
head of the reduction with respect to running pseudorandom VOLE
generation and expanding the resulting seeds is just the cost of
performing the scalar vector multiplication in the clear, and trans-
mitting a vector of the same length as the input vector. For that
reason, in the context of multi-party computation, distributed pseu-
dorandom vector OLE should be considered as a data independent
preprocessing step that enables fast secure distributed scalar-vector
multiplication, aka vector OLE. In this section we overview some
11
applications that fit in this paradigm and thus can benefit from our
protocol for distributed vector-OLE, as well as applications of our
sub-protocol for known-Indices MPFSS.
Generally speaking, vector OLE can be used to batch one-against-
many OLE computations, and thus directly provides a way to batch
applications that rely on OLE computations. Such applications in-
clude, for example, PSI [29], and keyword search [27]. The latter
relies on Oblivious Polynomial Evaluation (OPE) for which, as we
will discuss later in this section, an efficient reduction to vector
OLE exists. From a general MPC perspective, vector OLE enables
communication efficient evaluation of arithmetic circuits with mul-
tiplication gates with large fan-out. This includes several important
settings, including protocols for secure distributed data analysis.
7.1 Secure Linear algebra
As mentioned above, vector OLE is directly applicable in settings
were OLE computations, i.e., secure multiplications, can be vector-
ized and thus computed by invoking several instances of vector
OLE. This is the case, for example, in matrix-vector multiplication,
as this operation can be computed, for a matrix of dimensions 𝑛×𝑚,
by 𝑚 invocations of length 𝑛 vector OLE. Hence interesting settings
for our protocols are the ones where 𝑛 is a lot larger than 𝑚. This
corresponds to datasets with many records, and a limited number of
features per record, which are natural in the context of training and
evaluation of machine learning models, such as logistic regression.
Similarly, matrix convolutions operations, the main ingredient of
convolutional neural networks, rely on multiplying a small matrix
called kernel (common kernel sizes are 3 × 3, 5 × 5, and 9 × 9) in
a sliding fashion at each position of a input image (or layer input
for intermediate layers). This corresponds to a small number of
vector OLE computations of length the size of the image (which is
commonly 255 × 255).
A natural approach to distributed vector OLE is (vectorized)
Gilboa multiplication, as discussed in Section 2, and thus it has
been used as a way to precompute multiplication triples for MPC
in several works [22, 28, 44]. This approach requires linear commu-
nication and computation in the size of the matrix. In contrast our
Protocol 6 has sub-linear communication. In Section 8 we compare
these two approaches empirically, both in terms of communication
and computation.
Sparse matrix manipulations. As mentioned in the introduc-
7.1.1
tion, known-index MPFSS can also be seen as a type of “scatter”
vector operation. This functionality was presented by Schoppmann
et al. [51] under the name of “Scatter-Init”. In that setting, two
parties hold a share of a sparse vector, represented as a list of index-
value pairs for which one party knows the indices and the values
are additively shared. The goal is to securely convert the vector into
a dense representation, where it is represented as an array of shared
values of length the size of the domain of indices. This conversion
was used in the context of a sparse matrix multiplication protocol
that enables an efficient protocol for two-party secure gradient
descent on sparse training data. Schoppmann et al. [51] propose
a protocol for known-Index MPFSS based on full-blown FSS that
lacks an efficient batching strategy, and thus incurs 𝑂(𝑙𝑛) for a
length 𝑙 sparse vector over a domain of indices of size 𝑛. The value
of 𝑙 in their applications is such that that cost is prohibitive, while
our MPFSS only requires 𝑂(𝑛) local computation and improves
significantly the efficiency of these functionalities.
7.2 Oblivious Polynomial Evaluation
The problem of oblivious polynomial evaluation (OPE) considers
the setting where one party, the server, has the coefficients of a
polynomial 𝑃(𝑥) and a second party, the client, has an input 𝑧 and
the goal of the protocol is to enable the client to learn 𝑃(𝑧) without
learning anything more about the polynomial and without the
server learning anything about the input. OPE has applications to
privacy preserving set operations and data comparison, anonymous
initialization for metering and anonymous coupons [46]. The OPE
setting can be viewed as a generalization of the OLE problem to a
higher degree polynomial.
We show that we can implement the OPE protocol leveraging the
VOLE functionality. In order to this we use the OPE construction
introduced in the works of Naor and Pinkas [46] and Gilboa [30].
The idea of these constructions is to reduce the evaluation of a
degree 𝑛 polynomial to 𝑛 evaluations of linear polynomials, which
can be executed in parallel. Next we overview the main idea of
the reduction. Let 𝑃(𝑥) = 𝑎𝑛𝑥𝑛 + · · · + 𝑎1𝑥 + 𝑎0 be a degree 𝑛
polynomial. It can be expressed as 𝑃(𝑥) = 𝑥𝑄(𝑥) + 𝑏0 where 𝑄(𝑥)
is a degree 𝑛 − 1 polynomial. If the client and the server have
obtained respectively additive shares 𝑞𝐶, 𝑞𝑆 of the evaluation of
𝑄(𝑥) = 𝑞𝐶 + 𝑞𝑆, then 𝑃(𝑥) = 𝑞𝐶𝑥 + 𝑞𝑆𝑥 + 𝑏0. If the server fixes its
share 𝑞𝑆 in advance, then the client’s share 𝑞𝐶 = 𝑄(𝑥)−𝑞𝑆 = 𝑄′(𝑥)
can be computed using oblivious polynomial evaluation of 𝑄′(𝑥),
which is of degree 𝑛 − 1 and its coefficients are known to the server.
Now 𝑃(𝑥) = 𝑥𝑄′(𝑥) + 𝑃′(𝑥) where 𝑃′(𝑥) = 𝑞𝑆𝑥 + 𝑏0 is a linear
polynomial. Therefore the OPE of 𝑃(𝑥) reduces to the oblivious
evaluation of 𝑄′(𝑥) and 𝑃′(𝑥), which can be done in parallel. By
induction we obtain that the evaluation of 𝑃(𝑥) can be reduced
to the parallel evaluation of 𝑛 linear polynomials of the forms
𝑤𝑖 = 𝑃𝑖(𝑥) = 𝑢𝑖𝑥 + 𝑣𝑖 for 𝑖 ∈ [𝑛] where the server knows the values
(𝑢𝑖, 𝑣𝑖)𝑖∈[𝑛] and the client knows 𝑥 and obtains {𝑤𝑖}𝑖∈[𝑛]. These
corresponds to 𝑛 OLE evaluations, with the crucial aspect that one
of the inputs is common to all of them. Hence an OPE of degree 𝑛
can be implemented with a single vector OLE computation where
the server has two vectors of length 𝑛: u and v, which consist of the
values {𝑢𝑖}𝑖∈[𝑛] and {𝑣𝑖}𝑖∈[𝑛] respectively, and the client obtains
w = u𝑥 + v, which contains the values {𝑤𝑖}𝑖∈[𝑛].
7.3 Partially Private Distributed ORAM
Doerner and shelat [23] presented a distributed ORAM construction
that has asymptotically linear access time but achieves practically
very competitive concrete efficiency. This advantage is even more
pronounced in the RAM secure computation setting where this
ORAM construction is used for memory access and the access
queries are executed jointly by the two parties. The authors also
consider semi-private queries which consist of both data dependent
and data independent queries. In the latter type the parties know the
accessed index. For these types of queries the FLORAM construction
enables access in constant time.
We consider semi-private queries where the query index is
known only to one of the parties. This corresponds to situations
when data held by one party is indexed at private locations by the
other party. We show that in this setting we can use our SPFSS
construction and avoid having a Write-Only ORAM structure in
the overall construction.
First, we briefly overview the FLORAM construction [23]. The
ORAM in this construction consists of a Read-only ORAM, a Write-
Only ORAM and a stash. The Read-Only ORAM consists of en-
cryptions of the data under a key shared among the two parties.
Each party has a copy of the Read-Only ORAM. The two parties
execute an access query using a two server PIR construction based
on SPFSS to retrieve the corresponding data item. They generate
the distributed query running the distributed FSS key generation.
The Write-Only ORAM consists of two XOR shares of the database,
where each party holds one of the shares. It is updated with a write
for a new item again using an SPFSS which evaluates to a non-zero
value at the location of the write and this evaluation there is the
XOR of the old value and the new value. The stash contains all the
items that are currently in Write-Only ORAM. An ORAM access
that hides read and writes consists of one Read-Only ORAM access,
and one addition to the stash of the item that is written. Periodically
all the content of the Write-Only ORAM is moved to the Read-Only
ORAM using a special protocol with linear communication.
We observe that in setting of partially private queries where one
of the parties knows the access index we can use our distributed
only shared value FSS key generation presented in Section 4. This
results in an improvement in terms of round communication, as
the general SPFSS construction by Doerner and Shelat requires a
logarithmic number of rounds. In Section 8 we show empirically the
benefits of using our variant in the specific setting of semi-private
queries by comparing two implementations of these protocols. Our
results show improvements of up to an order of magnitude.
8 EXPERIMENTAL EVALUATION
8.1 Implementation and Setup
We implement all the protocols needed for Vector-OLE (Protocols 2,
3, 5, 6). Our implementation1 is written in C++. For OT extension we
use EMP [54], for finite field computations we use NTL [52], and for
matrix multiplications needed in Protocol 6 we rely on Eigen [33].
We use AES to implement the PRG needed for Protocol 2. Just as the
FLORAM implementation of Doerner and shelat [23], we rely on
the Davies-Meyer construction [55] to avoid repeated expansions
of AES keys. We further interleave the setup and expansion phases
in our implementation, and therefore only report the total time in
each of our experiments.
All our experiments are done on Azure Dsv3 machines in the
same region, using 2.4 GHz Intel Xeon E5-2673 v3 CPUs. For our
comparisons against other protocols, we used a single thread. Note
that this does not penalize any protocol in particular, since their
local computations all parallelize well. To show the scalability
of our protocol, we also implement a parallel version of it using
OpenMP [17].
8.2 Parameter Selection
In our experiments, we use 𝜆 = 128 as the computational security
parameter. Following the analysis in [9, Section 5.1], we choose
1Source code available at https://github.com/schoppmp/distributed-vector-ole.
12
𝑛
𝑡
𝑘
214
192
3482
216
382
7391
218
741
15336
220
1422
32771
222
2735
67440
224
5205
139959
Table 1: Vector-OLE parameters we chose in our evaluation.
These were computed by Boyle et al. such that solving the
corresponding LPN instance requires at least 280 operations
using either low-weight parity check, Gaussian elimination,
or Information Set Decoding [9].
the parameters for Vector-OLE (i.e., number of noise indices 𝑡 and
number of rows in the code matrix 𝑘) such that known attacks
on LPN require at least 280 arithmetic operations. The concrete
parameters depending on the vector size 𝑛 are given in Table 1. To
instantiate the code generator 𝑪 ∈ F𝑘×𝑛, we choose a local linear
code with 𝑑 = 10 non-zeros per column, which is also suggested
by previous work on Vector-OLE from LPN [2, 9]. Finally, we rely
on the estimates in [21, Appendix B] to choose cuckoo hashing
parameters such that hashing of the 𝑡 random indices fails with
probability at most 2−40, i.e., 𝜂 = 40. For the values of 𝑡 in Table 1
and 𝜅 = 3, this yields 𝑚 = 1.5𝑡. Those exact parameters have
also been used in a previous work that uses cuckoo hashing for
batching [1].
8.3 Results
8.3.1 Comparison of Known-Index SPFSS with FLORAM. First, we