benchmark their performance and compare them with previous
works. We used the OpenSSL [4] library for AES for our
PRF and semantically secure encryption. For our experiments
we used r5.8xlarge AWS machines with 32-core Intel Xeon
8259CL 2.5GHz processor, running Ubuntu 16.04 LTS, with
256GB RAM, 100GB SSD (GP2), and AES-NI enabled. All
schemes were instantiated on a single machine with in-memory
storage. Our implementations are publicly available in [28].
We compared SDa and SDd with the previous state-of-the-
art schemes with small client storage which can be achieved by
the “word counter + oblivious map” approach. As described in
the introduction, several schemes can be used in this manner,
but MITRA [29] is simultaneously the most efﬁcient and most
secure (BP-II). For QOS,
the main competitor is HORUS
[29] which is the fastest existing quasi-optimal scheme. Orion
achieves BP-I but it is considerably slower in practice. For
both these schemes, we used the code provided in [30].
Since we do not adopt a “clean-up” phase for SDa and
SDd, for fairness we also run MITRA without clean-up (this
is faster than the MITRA∗ numbers reported in [29] by up
to 50%). We stress that both schemes are compatible with
clean-up, with an additional update cost for SDd and at no
additional search cost for either one. For SDa and SDd we
used one additional optimization, by storing the ﬁrst 10 levels
of the index collections locally. As we demonstrate below, the
effect of this on local storage is small enough to be negligible,
but it helps further improve their performance otherwise.
Our basic efﬁciency measurement is computation time and
total communication size for search and update operations.
In our experimental evaluation, we consider as unit-size a
record/tuple (e.g., x-axis in Figures 7(b), 9(a) and 9(b))—the
database in bytes can be obtained by multiplying the number
of tuples with the tuple size (e.g., in crime dataset [1] the tuple-
size is 210 bytes). For real-world application a tuple size may
vary from a few bytes to GB, but in this section we focus
only on the index costs/overheads, i.e., ﬁnd the tuple/record
identiﬁers that satisfy the encrypted queries, ignoring the costs
for locating-downloading-decrypting the actual tuples/records,
since the latter cost is common for all DSE schemes.
We consider variable datasets of synthetic records and size
|DB| = 102–108 records, setting |W| (i.e., the total number
of keywords) to one-hundredth of |DB|. We also vary the
search result size between 10–105. Each record, i.e., keyword-
document id pair, of our synthetic datasets consists of a 4-byte
integer index ﬁle and an alphanumeric keyword of size ≤ 11
characters. We create the dataset to contain keywords with
variable result sizes between 10–105 records. For instance, for
a tested result size x, we create a random keyword with x
random ﬁles identiﬁers and we distribute uniformly at random
the remaining records to the remaining keywords. We also
repeated the experiments on a real dataset (see Figures 8
and 11) consisting of 22 attributes and 6, 123, 276 records
of reported crime incidents in Chicago [1]. The used query
attribute is the location description which contains 170 distinct
keywords. Among these keywords the one with minimum
frequency contains 1 record, while the one with maximum
frequency comprises 1, 631, 721 records.
In our experiments, before searching for w we delete
at random 10% of its corresponding entries (unless stated
otherwise), in order to show the impact of deletions in the
search performance. The average of 10 executions is reported.
A. Search performance
Computation time. Figure 7(a) shows the execution time
when searching for different result sizes and Figure 7(b) for
different database sizes. First, we note that the time increases
more steeply with larger result sizes than with larger |DB|,
as expected. Second, for small result sizes SDa and SDd are
much faster than MITRA. E.g., for |DB(w)| = 10 they are
85× and 20× faster than MITRA, respectively. This comes
naturally as, for such sizes, the OMAP overhead of MITRA
is dominating. Concretely, for retrieving 100 result records
from a dataset of size 106, SDa takes 0.09ms, SDd 0.12ms,
and MITRA 1.11ms. As |DB(w)| grows, the OMAP overhead
becomes less important, and the performance of the three
schemes converges, e.g., for 104 and |DB| = 106, SDa takes
9.8ms, SDd 13.3ms, and MITRA 13.2ms.
QOS has tremendously better search performance com-
pared to the previous best quasi-optimal scheme HORUS,
ranging from 4.4 up to 16531× faster. This comes naturally as
the number of oblivious operations for HORUS is O(|DB(w)|)
ORAM accesses, whereas for QOS it is a single OMAP access
to retrieve the counter aw. E.g., for |DB(w)| = 104 and
|DB| = 106, HORUS takes ~50sec and QOS takes 33.5ms.
The performance of QOS is worse than MITRA, which is
explained from the relatively small deletion rate (10%)—quasi-
optimal schemes like QOS perform better for large deletion
rates (for 10% deletions aw is very close to nw).
Communication size. Figure 7(c) shows the search com-
munication size when |DB(w)| varies between 10-105 for
|DB| = 106. For all schemes, communication is increasing
almost linearly with the result size. One exception is QOS and
MITRA where for small result sizes (e.g.,  103, the cost of SDd is larger than MITRA (e.g.,
508KB vs. 12KB for |DB| = 108). Regarding QOS versus
HORUS, the ﬁrst requires 0.9 to 3.1× more communication
than the latter (e.g. for |DB| = 108 QOS sends 1.6MB,
whereas HORUS sends 536KB).
As it is expected, MITRA with local storage has signiﬁcant
more efﬁcient update costs compared to our schemes, since it
takes advantage of storing locally the word counters (assuming
up to O(N ) local storage)—for each update MITRA with local
storage requires to compute two PRF evaluations and to store
the new value on the server.
C. Client storage
For all schemes, we store the OMAP stashes and all the
keys in K locally at the client. We are interested in measuring
the permanent local client storage, in order to ensure it remains
reasonably small. Throughout our experiments, the permanent
local storage for QOS , HORUS , and MITRA was never
above 2.5KB, even for |DB| = 106. With our optimization
of storing the 10 smallest levels locally at the client, SDa and
SDd needed at most 33KB and 150KB local client storage,
respectively (without this optimization, the corresponding sizes
were 400B and 18KB respectively). Recall that we can further
reduce the local storage to few bytes (O(1)) by storing the
stashes on the server and generating the keys from a PRF.
However, we consider these sizes essentially negligible for
modern devices, even for tablets and mobile phones.
D. Quasi-optimal search performance for variable deletion
percentages
Our main motivation for studying DSE with quasi-optimal
search time is to avoid paying the cost of past deletions
during searches. In all the above experiments, we assume a
10% deletion ratio, rendering the effect of deletions for search
negligible. Now, we focus on our new quasi-optimal scheme
QOS and we provide experiments for variable deletion ratios.
As is evident from the experiments so far, QOS vastly
outperforms the previous state-of-the-art quasi-optimal DSE
HORUS for searches. In this set of experiments, we compare
QOS with SDd and MITRA (the latter two schemes had better
performance for 10% deletions). In this setting, we ﬁrst insert
a ﬁxed number of entries iw for keyword w and then report
the search time after deleting a percentage of iw between 0-
90%. We focus on two cases iw = 100 (small results) and
(a)
(b)
Fig. 8: Crime Dataset—Search (a) computation time vs. vari-
able result size, (b) communication size vs. variable result size.
B. Update performance
Computation time. Figure 9 shows (a) the update computation
time and (b) the update communication size for variable
database sizes for all schemes except for SDa, which has
amortized update cost. The update performance of SDa is
reported in Figure 9(c), which shows the update time (step-
by-step) for a sequence of 103 consecutive updates (inser-
tions/deletions), starting from empty DB. As explained above,
we store the ﬁrst 10 levels of SDd locally to optimize per-
formance, hence for small database sizes the update time is
negligible (less than 0.01ms).
For our tested sizes, MITRA is 1 to 21× faster than SDd
(e.g. for |DB| = 106 its update time is 1ms while SDd takes
14ms). We stress that the update time of SDd is increasing with
the number of updates, as more OMAP accesses are necessary.
Regarding QOS, we consider only delete operations since they
are costlier than insertions. Compared to HORUS our deletion
time is, as expected, slightly worse—up to 1.7× slower for
the tested sizes (e.g., for |DB| = 108 QOS takes 281ms and
HORUS 233ms). Figure 9(c) shows the SDa update time for
103 consecutive updates. For each update, the client has to
fetch and merge some of the previously ﬁlled indexes, which
corresponds to the variable cost shown in the plot. For 103
updates, the minimum and maximum observed times are 1µs
and 777µs, and the average is 7µs.
Communication size. Figure 9(b) shows the update communi-
cation size which (not surprisingly) has similar patterns with
Figure 9(a). For SDd, due to our optimization (keeping 10
levels locally) the communication size for |DB| ≤ 103 is zero.
13
10-2100102104106108101102103104105Time(milliseconds)Result SizeMITRASDDSDAQOSHORUS 0.01 0.1 1 10 100 1000102103104105106107108Time(millisecond)|DB|MITRASDDSDAQOSHORUS102103104105106107108101102103104105Size(Byte)Result SizeMITRASDDSDAQOSHORUS10-2100102104106108478907538024698175207Time(milliseconds)Result SizeMITRASDDSDAQOSHORUS101102103104105106107108109478907538024698175207Size(Byte)Result SizeMITRASDDSDAQOSHORUS(a)
(b)
(c)
Fig. 9: Synthetic Dataset—Update (a) computation time vs. variable |DB|, (b) communication size vs. variable |DB|, (c)
computation time with SDa for 1000 updates staring from empty DB.
(a)
(b)
(c)
(d)
Fig. 10: Synthetic Dataset—Search computation time for |DB| = 1M and variable deletion percentage for: (a) iw = 100 using
OMAP, (b) iw = 20K using OMAP, (c) iw = 100 storing word counters locally, (d) iw = 20K storing word counters locally.
(a)
(b)
(c)
(d)
Fig. 11: Crime dataset—Search computation time and variable deletion percentage for: (a) iw = 78 using OMAP, (b) iw = 24698
using OMAP, (c) iw = 78 storing word counters locally, (d) iw = 24698 storing word counters locally.
iw = 20K (large results). Since both SDd and MITRA have
search Ω(aw) their performance should worsen as the deletion
rate increases. On the other hand, the search time of QOS
should be better. Hence, we want to ﬁnd at which deletion
ratio QOS will outperform the others.
Figure 10 shows the results for small (a) and large (b)
iw respectively. First, for iw = 100 QOS and MITRA have
similar search times since the main bottleneck for both is the
OMAP accesses. However, QOS becomes slightly faster and
MITRA slightly slower as the deletion ratio increases; the ﬁrst
outperforms the second after roughly 60%. On the other hand,
SDd remains much faster than both of them throughout the
experiment since it does not need to perform OMAP accesses.
The results are different for iw = 20K in Figure 10(b). QOS
starts off much slower (as was the case in the experiments
above), however, it becomes faster very quickly as deletions
increase. It outperforms MITRA at ~65% and even SDd at
~80%! The reason is that for large iw the OMAP cost becomes
a small percentage of the search process.
We believe that these results serve as a good indication for
the practical potential of schemes with (quasi-)optimal search
time while there is still room for improvement. To further
support
this point, we consider another scenario in which
permanent client storage is not a bottleneck and we implement
both QOS and MITRA to store the word counter maps locally
(avoiding the OMAP overhead) The results are shown in
Figures 10(c) and (d) for iw = 100 and 20K, respectively.
They follow the trends of the corresponding Figures 10(a),(b),
but the crossover points are moved to the left. QOS becomes
better than MITRA for ~40% deletions for iw = 100, and for
~60% for iw = 20K. Compared to SDd, it becomes better in
both cases at ~80% (in the previous scenario, SDd was strictly
better for iw = 100). In Figure 11, we repeated the experiments
for the search costs with variable deletion percentage for the
crime dataset and we observe similar conclusions.
14
10-310-210-1100101102103104105106102103104105106107108Time(millisecond)|DB|MITRASDDQOSHORUS100101102103104105106107102103104105106107108Size(Byte)|DB|MITRASDDQOSHORUS 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 200 400 600 800 1000Time(millisecond)ith UpdateSDA10-1100101102103 0 10 20 30 40 50 60 70 80 90Time(milliseconds)Delete PercentageMITRASDDQOSHORUS101102103104105106 0 10 20 30 40 50 60 70 80 90Time(milliseconds)Delete PercentageMITRASDDQOSHORUS10-1100101102103 0 10 20 30 40 50 60 70 80 90Time(milliseconds)Delete PercentageMITRASDDQOSHORUS101102103104105106 0 10 20 30 40 50 60 70 80 90Time(milliseconds)Delete PercentageMITRASDDQOSHORUS10-1100101102103 0 10 20 30 40 50 60 70 80 90Time(milliseconds)Delete PercentageMITRASDDQOSHORUS101102103104105106107 0 10 20 30 40 50 60 70 80 90Time(milliseconds)Delete PercentageMITRASDDQOSHORUS10-1100101102103 0 10 20 30 40 50 60 70 80 90Time(milliseconds)Delete PercentageMITRASDDQOSHORUS101102103104105106107 0 10 20 30 40 50 60 70 80 90Time(milliseconds)Delete PercentageMITRASDDQOSHORUSVI. CONCLUSION
In this paper, we revisited the problem of forward and
backward private DSE. Prior state-of-the-art schemes either
require from the client to store a counter per keyword, or
obliviously access this information at the server limiting their
practicality for real-world applications. We presented three new
schemes with constant permanent client storage and better
search performance, both asymptotically and experimentally,
than previous works. Moreover, our two schemes SDa and
SDd not only eliminate the need for oblivious accesses during
searches but also minimize the required round-trips. The main
drawback of SDa is during updates, since it has a O(log N )
amortized update cost. The latter means that some updates
that are cheap (e.g., O(1)), and other that are very expensive
(e.g., O(N ))—see Figure 9(c). This is the main motivation
for proposing SDd, which tackles the aforementioned problem
by providing a de-amortized update cost. As we have shown,
for small deletion percentage both SDa and SDd provide
better search performance (both asymptotically and experi-
mentally) than the previous state-of-the-art MITRA . However,
the main problem of the above schemes is that the search