### 图8：分类器在训练集成员与非成员数据点上的预测不确定性分布

- **预测不确定性** 是通过模型输出（即预测向量）的归一化熵来衡量的。
- 左侧的图展示了常规模型（无防御）的分布曲线，右侧的图展示了隐私保护模型（有防御）的分布曲线。
- 曲线之间的差距越大，模型关于其训练集的信息泄漏越多。隐私保护模型将这一差距减小了一个到两个数量级。

#### 最大差距
- Purchase100 模型: (0.03 vs. 0.30)
- Texas100 模型: (0.02 vs. 0.15)
- CIFAR100-Densenet 模型: (0.04 vs. 0.49)

#### 平均差距
- Purchase100 模型: (0.004 vs. 0.012)
- Texas100 模型: (0.002 vs. 0.04)
- CIFAR100-Densenet 模型: (0.002 vs. 0.01)

### 分类损失函数（3）

表6展示了使用L2范数正则化的模型在测试准确性和成员隐私之间的权衡。这些正则化方法既不能保证隐私，也不能最小化实现隐私的成本。对于接近最大程度的成员隐私，我们的隐私保护机制的测试准确性是L2范数正则化模型的两倍多。这正是我们从隐私保护模型的优化目标中所期望的结果。

### 成员隐私和推理攻击准确性

表4列出了模型的训练和测试准确性以及攻击准确性。为了测量攻击准确性，我们评估了推理攻击模型正确预测成员身份的平均概率：

\[ \frac{\sum_{(x, y) \in D \setminus D_A} h(x, y, f(x)) + \sum_{(x'', y'') \in D''} (1 - h(x'', y'', f(x'')))}{|D \setminus D_A| + |D''|} \]

其中 \( D'' \) 是从与训练集相同的底层分布中采样的数据点集合，但不与 \( D \) 或 \( D_A \) 重叠。

表4中最重要的结果是成对的彩色列，它们代表分类器的测试准确性与攻击准确性。模型的预测能力和其对成员推理攻击的鲁棒性之间存在权衡。实验结果表明，在隐私保护模型中，攻击准确性远小于随机猜测的概率。我们的隐私保护机制可以在几乎不影响模型预测能力的情况下，实现最大的成员隐私。

为了实现接近最大成员隐私，Purchase100 模型的测试准确性下降了3.5%，Texas100 模型下降了4.4%，CIFAR100-Alexnet 模型下降了1.1%，CIFAR100-Densenet 模型下降了3%。

### 参考集的影响

我们的最小-最大优化目标是使模型在其训练数据上的预测与其在任何来自底层分布的样本上的预测无法区分。我们使用一组来自该分布的样本（称为参考集）来经验地优化最小-最大目标。表7显示了参考集大小对模型成员隐私的影响。模型在同一训练集上进行训练，训练集大小为20,000，超参数 \( \lambda = 3 \)。正如预期的那样，随着参考集大小的增加，它更好地表示了底层分布，因此攻击准确性趋近于50%。

### 预测的不可区分性

针对黑盒模型的成员推理攻击利用了模型对其成员和非成员的预测之间的统计差异。图6展示了模型在训练数据上的输出（即每个类别的概率），分别对比了常规模型（无防御）和隐私保护模型。输入数据全部来自Purchase100数据集中的第50类。顶部图表明，一个过度拟合的常规模型在其训练数据上产生了较高的正确类别概率，这显著增加了模型对成员推理攻击的脆弱性。隐私保护模型产生了明显不同的分布（中间图），使得成员的输出与非成员的输出无法区分（底部图）。最小-最大优化使这两个输出分布收敛到不可区分的分布。

我们进一步通过计算不同数据集上模型输出的一些统计数据（准确性和不确定性）来研究这两种分布的不可区分性。图7和图8以直方图的形式展示了模型在训练集和测试集上的准确性和不确定性。我们计算模型 \( f \) 在数据点 \( (x, y) \) 上的准确性为 \( f_y(x) \)，这是输入 \( x \) 预测类别 \( y \) 的概率。我们计算不确定性为归一化熵：

\[ -\frac{1}{\log(k)} \sum_{i=1}^k \hat{y}_i \log(\hat{y}_i) \]

其中 \( \hat{y} = f(x) \) 是概率向量，\( k \) 是类别的数量。这两张图表明，我们的隐私机制显著减少了模型在训练集和测试集上的预测准确性（和不确定性）的最大（最坏情况风险）和平均差距，相比常规模型。请注意，这些图并不能证明隐私，但说明了攻击者在推理攻击中可以利用的内容。它们直观地展示了如何通过我们的防御机制提高模型输出分布（在成员和非成员之间）的不可区分性。

### 相关工作

分析和保护机器学习模型免受不同类型攻击的隐私是一个持续的研究课题。直接威胁机器学习的隐私问题包括在训练或预测过程中对机器学习平台的不受信任访问。已经提出了一些基于可信硬件和加密私有计算的防御机制，以实现盲训练和使用机器学习模型。这些方法利用了同态加密、混淆电路和安全多方计算等技术，用于在加密数据上进行私有机器学习 [8, 20, 33, 37]，以及使用可信硬件（如Intel SGX）进行私有计算 [26, 39]。尽管这些技术防止了攻击者直接观察敏感数据，但它们并未限制通过计算本身的信息泄漏。

具有某些背景知识和外部数据的对手可以尝试推断诸如训练数据、输入查询和模型参数等信息。这些推理攻击包括输入推理 [19]、成员推理 [45]、属性推理 [9]、参数推理 [47, 48] 和侧信道攻击 [50]。在涉及敏感数据的计算中，成员推理攻击和重建攻击被认为是两类主要的攻击 [17]。

成员推理攻击是一种决策问题，旨在推断目标数据记录是否存在于（训练）数据集中 [5, 18, 23, 42, 43, 45]。攻击的准确性表明模型对单个训练数据的依赖程度。重建攻击是一种更通用的攻击类型，其目标是推断训练集中许多个体的敏感属性 [13, 49]。一种针对一般推理攻击的防御技术是使用差分隐私保证进行计算（例如模型训练）[15, 16]，最近在机器学习中得到了应用 [1, 6, 10, 40, 41]。尽管差分隐私机制在推理攻击方面具有可证明的鲁棒性，但其实现通常伴随着较大的效用损失。这种效用成本源于我们需要通过创建所有可能输入数据集的相似状态之间的不可区分性来保护隐私，同时也与计算函数灵敏度的紧密界相关的难度有关。文献中分析了不同成员隐私定义与差分隐私之间的关系 [32, 51]。

使用博弈论来形式化和优化数据隐私（和安全）是另一种保护隐私的方向 [2, 24, 34, 44, 46]。在这种框架下，隐私损失被最小化以对抗最强的相应攻击。解决方案将被证明对根据此类“损失”函数威胁隐私的任何攻击都是鲁棒的。博弈论框架允许显式地将效用函数纳入最小-最大优化中，从而也最小化隐私防御机制的成本。近年来，机器学习的进步，尤其是生成对抗网络的发展 [3, 12, 21]，引入了新的算法来解决在训练复杂（深度神经网络）模型时的最小-最大游戏。对抗训练也被用于正则化，从而泛化模型 [11, 14, 29, 35, 36, 38]。

### 结论

我们介绍了一种新的隐私机制，用于减轻机器学习模型预测关于其训练集数据记录成员身份的信息泄漏。我们设计了一个优化问题，其目标是同时最大化隐私和预测准确性。我们设计了一种训练算法，通过最小化分类损失并最大化成员推理攻击的收益来解决最小-最大游戏优化。解决方案将是一个模型，其在训练数据上的预测与其在来自同一底层分布的任何数据样本上的预测无法区分。该机制保证了模型训练集的成员隐私，并且在给定可用训练/参考数据和模型容量的情况下，实现了最低的准确性损失。在基准机器学习任务上的广泛实验中，我们表明实现隐私的成本是微不足道的，而且我们的隐私保护模型可以很好地泛化。

### 致谢

作者感谢George Theodorakopoulos提供的宝贵反馈。

### 参考文献

[略]

---

希望这些修改能帮助您更好地理解文档内容。如果有任何进一步的问题或需要更多的修改，请随时告诉我！