0.4
0.6
0.8
Pre dic tion unc e rta inty
CIFAR100, without defense
CIFAR100, with defense
Me m b e r
Non-m e m b e r
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
Me m b e r
Non-m e m b e r
0.0
0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40
0.0
0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40
Pre dic tion unc e rta inty
Pre dic tion unc e rta inty
Figure 8: Distribution of the classiﬁer’s prediction uncertainty on members of its training set versus non-member data points.
Uncertainty is measured as normalized Entropy of the model’s output (i.e., prediction vector). The plots on the left show
the distribution curves for regular models (without defense), and the ones on the right show the distribution curves for
privacy-preserving models (with defense). The larger the gap between the curves in a plot is, the more the information leakage
of the model about its training set is. The privacy-preserving model reduces this gap by one to two orders of magnitude.
– The maximum gap between the curves (with defense versus without defense) is as follows.
Purchase100 model: (0.03 vs. 0.30), Texas100 model: (0.02 vs. 0.15), and CIFAR100-Densenet model: (0.04 vs. 0.49).
– The average gap between the curves is as follows.
Purchase100 model: (0.004 vs. 0.012), Texas100 model: (0.002 vs. 0.04), and CIFAR100-Densenet model: (0.002 vs. 0.01).
11
classiﬁcation loss function (3)). Table 6 shows the tradeoﬀ between
the model’s test accuracy and membership privacy using L2-norm.
Such regularizers do not guarantee privacy nor they minimize the
cost of achieving it. For a close-to-maximum degree of membership
privacy, the testing accuracy of our privacy-preserving mechanism
is more than twice the testing accuracy of a L2-norm regularized
model. This is exactly what we would expect from the optimiza-
tion objectives of our privacy-preserving model.
Membership Privacy and Inference Attack Accuracy
Table 4 presents the training and testing accuracy of the model,
as well as the attack accuracy. To measure the attack accuracy, we
evaluate the average probability that the inference attack model
correctly predicts the membership:
(x ,y)∈D\D A
h(x , y , f (x)) +
(1 − h(x ′′
, y ′′
, f (x ′′)))
(x ′′
,y′′)∈D ′′
|D \ DA | + |D ′′|
where D ′′ is a set of data points that are sampled from the same
underlying distribution as the training set, but does not overlap
with D nor with D ′A.
The most important set of results in Table 4 is the two pairs of
colored columns which represent the testing accuracy of the clas-
siﬁer versus the attack accuracy. There is a tradeoﬀ between the
predictive power of the model and its robustness to membership in-
ference attack. As expected from our theoretical results, the exper-
imental results show that the attack accuracy is much smaller (and
close to random guess) in the privacy-preserving model compared
to a regular model. Our privacy-preserving mechanism can
guarantee maximum achievable membership privacy with
only a negligible drop in the model’s predictive power. To
achieve a near maximum membership privacy, the testing accuracy
is dropped by 3.5% for the Purchase100 model, it is dropped by 4.4%
for the Texas100 model, it is dropped by 1.1% for the CIFAR100-
Alexnet model, and it is dropped by 3% for the CIFAR100-Densenet
model.
Eﬀect of the Reference Set
The objective of our min-max optimization is to make the pre-
dictions of the model on its training data indistinguishable from
the model’s predictions on any sample from the underlying data
distribution. We make use of a set of samples from this distribu-
tion, named reference set, to empirically optimize the min-max ob-
jective. Table 7 shows the eﬀect of the size of the reference set D ′
on the model’s membership privacy. The models are trained on the
same training set D of size 20,000, and hyper-parameter λ = 3. As
expected, as the size of the reference set increases, it becomes bet-
ter at properly representing the underlying distribution, thus the
attack accuracy converges to 50%.
Indistinguishability of Predictions
The membership inference attacks against black-box models
exploit the statistical diﬀerences between the predictions of the
model on its members versus non-members. Figure 6 shows the
output of the model (i.e., the probability of being a sample from
each class) on its training data, for a regular model (without de-
fense) versus a privacy-preserving model. The input data are all
from class 50 in the Purchase100 dataset. The top ﬁgure illus-
trates that a regular model (which is overﬁtted on its training
set) produces a high probability for the correct class on its train-
ing data. This signiﬁcantly contributes to the vulnerability of the
model to the membership inference attack. The privacy-preserving
model produces a visibly diﬀerent distribution (the middle ﬁg-
ure). This makes the members’ outputs indistinguishable from non-
members’ outputs (the bottom ﬁgure). The min-max optimization
makes these two output distributions converge to indistinguish-
able distributions.
We further investigate the indistinguishability of these two dis-
tributions by computing some statistics (accuracy and uncertainty)
of the model’s output for diﬀerent datasets. Figure 7 and Figure 8
show the results as the histogram of the models’ accuracy and un-
certainty over the training set and testing set. We compute the
accuracy of model f on data point (x , y) as fy (x), which is the
probability of predicting class y for input x. We compute uncer-
tainty as the normalized entropy
ability vector ˆy = f (x), where k is the number of classes. The
two ﬁgures show that our privacy mechanism signiﬁcantly re-
duces both the maximum (worst case risk) and average gap be-
tween the prediction accuracy (and uncertainty) of the model
on its training versus test set, compared with a regular model.
Note that these ﬁgures do not prove privacy, but illustrate what
the attacker can exploit in his inference attacks. They visibly show
how the indistinguishability of the model’s output distributions
(on members and non-members) can improve by using our defense
mechanism.
log(k)i ˆyi log( ˆyi ) of the prob-
−1
6 RELATED WORK
Analyzing and protecting privacy in machine learning models
against diﬀerent types of attacks is a topic of ongoing research.
A direct privacy threat against machine learning is the untrusted
access of the machine learning platform during training or predic-
tion. A number of defense mechanisms, which are based on trusted
hardware and cryptographic private computing, have been pro-
posed to enable blind training and use of machine learning models.
These methods leverage various techniques including homomor-
phic encryption, garbled circuits, and secure multi-party computa-
tion for private machine learning on encrypted data [8, 20, 33, 37],
as well as private computation using trusted hardware (e.g., Intel
SGX) [26, 39]. Although these techniques prevent an attacker from
directly observing the sensitive data, yet they do not limit informa-
tion leakage through the computation itself.
An adversary with some background knowledge and external
data can try to infer information such as the training data, the input
query, and the parameters of the model. These inference attacks
include input inference [19], membership inference [45], attribute
inference [9], parameter inference [47, 48], and side-channel at-
tacks [50]. There are examples of a wide-range of privacy attacks
against computations over sensitive data. Our focus is on the pri-
vacy risks of computation on databases, when the adversary ob-
serves the result of the computation. In such settings, membership
12
inference attacks and reconstruction attacks are considered as the
two major classes of attacks [17].
Membership inference attack is a decisional problem. It aims
at inferring the presence of a target data record in the (training)
dataset [5, 18, 23, 42, 43, 45]. The accuracy of the attack shows the
extent to which a model is dependent on its individual training
data. The reconstruction attack is a more generic type of attack,
where the objective is to infer sensitive attributes of many individ-
uals in the training set [13, 49]. One proposed defense technique
against general inference attacks is computation (e.g., training of
models) with diﬀerential privacy guarantee [15, 16], which has re-
cently been used in the context of machine learning [1, 6, 10, 40, 41].
Despite their provable robustness against inference attacks, diﬀer-
ential privacy mechanisms are hard to achieve with negligible util-
ity loss. The utility cost comes from the fact that we aim at protect-
ing privacy against all strong attacks by creating indistinguisha-
bility among similar states of all possible input datasets. It is also
related to the diﬃculty of computing a tight bound for the sensitiv-
ity of functions, which determines the magnitude of required noise
for diﬀerential privacy. The relation between some diﬀerent deﬁ-
nitions of membership privacy and diﬀerential privacy is analyzed
in the literature [32, 51].
Using game theory to formalize and optimize data privacy (and
security) is another direction for protecting privacy [2, 24, 34, 44,
46]. In such a framework, the privacy loss is minimized against the
strongest corresponding attack. The solution will be provably ro-
bust to any attack that threatens privacy according to such “loss”
function. The game-theoretic framework allows to explicitly incor-
porate the utility function into the min-max optimization, thus also
minimizing the cost of the privacy defense mechanism. The recent
advances in machine learning, notably the developments of gener-
ative adversarial networks [3, 12, 21], have introduced new algo-
rithms for solving min-max games while training a complex (deep
neural network) model. Adversarial training has also been used for
regularizing, hence generalizing, a model [11, 14, 29, 35, 36, 38].
7 CONCLUSIONS
We have introduced a new privacy mechanism for mitigating the
information leakage of the predictions of machine learning mod-
els about the membership of the data records in their training sets.
We design an optimization problem whose objective is to jointly
maximize privacy and prediction accuracy. We design a training
algorithm to solve a min-max game optimization that minimizes
the classiﬁcation loss of the model while maximizing the gain of
the membership inference attack. The solution will be a model
whose predictions on its training data are indistinguishable from
its predictions on any data sample from the same underlying dis-
tribution. This mechanism guarantees membership privacy of the
model’s training set against the—strongest—inference attack, and
imposes the minimum accuracy loss for achieving such level of
privacy, given the available training/reference data and the ca-
pacity of the models. In our extensive experiments on applying
our method on benchmark machine learning tasks, we show that
the cost of achieving privacy is negligible, and that our privacy-
preserving models can generalize well.
13
ACKNOWLEDGEMENTS
The authors would like to thank George Theodorakopoulos for
helpful feedback.
REFERENCES
[1] Martín Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov,
Kunal Talwar, and Li Zhang. 2016. Deep learning with diﬀerential privacy. In
Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communica-
tions Security. ACM.
[2] Mário S Alvim, Konstantinos Chatzikokolakis, Yusuke Kawamoto, and Catuscia
Information leakage games. In International Conference on
Palamidessi. 2017.
Decision and Game Theory for Security. Springer.
[3] Martin Arjovsky, Soumith Chintala, and Léon Bottou. 2017. Wasserstein gan.
arXiv preprint arXiv:1701.07875 (2017).
[4] Mordecai Avriel. 2003. Nonlinear programming: analysis and methods. Courier
Corporation.
[5] Michael Backes, Pascal Berrang, Mathias Humbert, and Praveen Manoharan.
2016. Membership privacy in MicroRNA-based studies. In Proceedings of the
2016 ACM SIGSAC Conference on Computer and Communications Security. ACM.
[6] Raef Bassily, Adam Smith, and Abhradeep Thakurta. 2014. Private empirical
risk minimization: Eﬃcient algorithms and tight error bounds. In Foundations
of Computer Science (FOCS), 2014 IEEE 55th Annual Symposium on. IEEE.
[7] Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning (Infor-
mation Science and Statistics). Springer-Verlag, Berlin, Heidelberg.
[8] Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan
McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. 2017. Prac-
tical Secure Aggregation for Privacy-Preserving Machine Learning. In Proceed-
ings of the 2017 ACM SIGSAC Conference on Computer and Communications Se-
curity. ACM.
[9] Nicholas Carlini, Chang Liu, Jernej Kos, Úlfar Erlingsson, and Dawn Song. 2018.
The Secret Sharer: Measuring Unintended Neural Network Memorization & Ex-
tracting Secrets. arXiv preprint arXiv:1802.08232 (2018).
[10] Kamalika Chaudhuri, Claire Monteleoni, and Anand D Sarwate. 2011. Diﬀeren-
tially private empirical risk minimization. Journal of Machine Learning Research
(2011).
[11] Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Ruslan R Salakhutdi-
nov. 2017. Good semi-supervised learning that requires a bad gan. In Advances
in Neural Information Processing Systems.
[12] Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng.
2017. Training GANs with Optimism. arXiv preprint arXiv:1711.00141 (2017).
[13] Irit Dinur and Kobbi Nissim. 2003. Revealing information while preserving pri-
vacy. In Proceedings of the twenty-second ACM SIGMOD-SIGACT-SIGART sym-
posium on Principles of database systems. ACM.
[14] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex
Lamb, Martin Arjovsky, and Aaron Courville. 2016. Adversarially learned in-
ference. arXiv preprint arXiv:1606.00704 (2016).
[15] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. Cali-
brating noise to sensitivity in private data analysis. In Theory of Cryptography
Conference. Springer.
[16] Cynthia Dwork, Aaron Roth, et al. 2014. The algorithmic foundations of diﬀer-
ential privacy. Foundations and Trends® in Theoretical Computer Science (2014).
[17] Cynthia Dwork, Adam Smith, Thomas Steinke, and Jonathan Ullman. 2017. Ex-
posed! a survey of attacks on private data. (2017).
[18] Cynthia Dwork, Adam Smith, Thomas Steinke, Jonathan Ullman, and Salil Vad-
han. 2015. Robust traceability from trace amounts. In Foundations of Computer
Science (FOCS), 2015 IEEE 56th Annual Symposium on. IEEE.
[19] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model inversion
attacks that exploit conﬁdence information and basic countermeasures. In Pro-
ceedings of the 22nd ACM SIGSAC Conference on Computer and Communications
Security. ACM.
[20] Ran Gilad-Bachrach, Nathan Dowlin, Kim Laine, Kristin Lauter, Michael
Naehrig, and John Wernsing. 2016. Cryptonets: Applying neural networks to
encrypted data with high throughput and accuracy. In International Conference
on Machine Learning.
[21] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-
Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative
adversarial nets. In Advances in neural information processing systems.
[22] Moritz Hardt, Benjamin Recht, and Yoram Singer. 2015. Train faster, generalize
better: Stability of stochastic gradient descent. arXiv preprint arXiv:1509.01240
(2015).
[23] Nils Homer, Szabolcs Szelinger, Margot Redman, David Duggan, Waibhav
Tembe, Jill Muehling, John V Pearson, Dietrich A Stephan, Stanley F Nelson,
and David W Craig. 2008. Resolving individuals contributing trace amounts of
DNA to highly complex mixtures using high-density SNP genotyping microar-
rays. PLoS genetics (2008).
[51] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. 2018. Pri-
vacy Risk in Machine Learning: Analyzing the Connection to Overﬁtting. arXiv
preprint arXiv:1709.01604 (2018).
[52] Tong Zhang. 2004. Solving large scale linear prediction problems using stochas-
tic gradient descent algorithms. In Proceedings of the twenty-ﬁrst international
conference on Machine learning. ACM.
[24] Justin Hsu, Aaron Roth, and Jonathan Ullman. 2013. Diﬀerential privacy for
the analyst via private equilibrium computation. In Proceedings of the forty-ﬁfth
annual ACM symposium on Theory of computing. ACM.
[25] Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten.
2017. Densely connected convolutional networks. In Proceedings of the IEEE
conference on computer vision and pattern recognition.
[26] Tyler Hunt, Congzheng Song, Reza Shokri, Vitaly Shmatikov, and Emmett
Witchel. 2018. Chiron: Privacy-preserving Machine Learning as a Service. arXiv
preprint arXiv:1803.05961 (2018).
[27] Jinyuan Jia and Neil Zhenqiang Gong. 2018. AttriGuard: A Practical Defense
Against Attribute Inference Attacks via Adversarial Machine Learning. arXiv
preprint arXiv:1805.04810 (2018).
[28] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[29] Mateusz Koziński, Loïc Simon, and Frédéric Jurie. 2017. An Adversarial Regu-
larisation for Semi-Supervised Training of Structured Output Neural Networks.
arXiv preprint arXiv:1702.02382 (2017).
[30] Alex Krizhevsky and Geoﬀrey Hinton. 2009. Learning multiple layers of features
from tiny images. (2009).
[31] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. 2012. Imagenet classi-
ﬁcation with deep convolutional neural networks. In Advances in neural infor-
mation processing systems.
[32] Ninghui Li, Wahbeh Qardaji, Dong Su, Yi Wu, and Weining Yang. 2013. Mem-
bership privacy: a unifying framework for privacy deﬁnitions. In Proceedings of
the 2013 ACM SIGSAC conference on Computer & communications security. ACM.
[33] Yehuda Lindell and Benny Pinkas. 2000. Privacy preserving data mining. In
Annual International Cryptology Conference. Springer.
[34] Mohammad Hossein Manshaei, Quanyan Zhu, Tansu Alpcan, Tamer Bacşar, and
Jean-Pierre Hubaux. 2013. Game theory meets network security and privacy.
ACM Computing Surveys (CSUR) (2013).
[35] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. 2017. Vir-
tual adversarial training: a regularization method for supervised and semi-
supervised learning. arXiv preprint arXiv:1704.03976 (2017).
[36] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, and Shin Ishii.
2015. Distributional smoothing with virtual adversarial training. arXiv preprint
arXiv:1507.00677 (2015).
[37] Payman Mohassel and Yupeng Zhang. 2017. Secureml: A system for scalable
privacy-preserving machine learning. In Security and Privacy (SP), 2017 IEEE
Symposium on. IEEE.
[38] Augustus Odena. 2016. Semi-supervised learning with generative adversarial
networks. arXiv preprint arXiv:1606.01583 (2016).
[39] Olga Ohrimenko, Felix Schuster, Cédric Fournet, Aastha Mehta, Sebastian
Nowozin, Kapil Vaswani, and Manuel Costa. 2016. Oblivious Multi-Party Ma-
chine Learning on Trusted Processors.. In USENIX Security Symposium.
[40] Nicolas Papernot, Martín Abadi, Ulfar Erlingsson, Ian Goodfellow, and Kunal
Talwar. 2016. Semi-supervised knowledge transfer for deep learning from pri-
vate training data. arXiv preprint arXiv:1610.05755 (2016).
[41] Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal Tal-
war, and Úlfar Erlingsson. 2018. Scalable Private Learning with PATE. arXiv
preprint arXiv:1802.08908 (2018).
[42] Apostolos Pyrgelis, Carmela Troncoso, and Emiliano De Cristofaro. 2017. Knock
Knock, Who’s There? Membership Inference on Aggregate Location Data. arXiv
preprint arXiv:1708.06145 (2017).
[43] Sriram Sankararaman, Guillaume Obozinski, Michael
I Jordan, and Eran
Halperin. 2009. Genomic privacy and limits of individual detection in a pool.
Nature genetics (2009).
[44] Reza Shokri. 2015. Privacy games: Optimal user-centric data obfuscation. Pro-
ceedings on Privacy Enhancing Technologies (2015).
[45] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017.
Membership inference attacks against machine learning models. In Security and
Privacy (SP), 2017 IEEE Symposium on.
[46] Reza Shokri, George Theodorakopoulos, Carmela Troncoso,
Jean-Pierre
Hubaux, and Jean-Yves Le Boudec. 2012. Protecting location privacy: optimal
strategy against localization attacks. In Proceedings of the 2012 ACM conference
on Computer and communications security. ACM.
[47] Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart.
2016. Stealing machine learning models via prediction apis. In USENIX Security.
[48] Binghui Wang and Neil Zhenqiang Gong. 2018. Stealing Hyperparameters in
Machine Learning. arXiv preprint arXiv:1802.05351 (2018).
[49] Rui Wang, Yong Fuga Li, XiaoFeng Wang, Haixu Tang, and Xiaoyong Zhou. 2009.
Learning your identity and disease from research papers: information leaks in
genome wide association study. In Proceedings of the 16th ACM conference on
Computer and communications security. ACM.
[50] Lingxiao Wei, Yannan Liu, Bo Luo, Yu Li, and Qiang Xu. 2018.
I Know What
You See: Power Side-Channel Attack on Convolutional Neural Network Accel-
erators. arXiv preprint arXiv:1803.05847 (2018).
14