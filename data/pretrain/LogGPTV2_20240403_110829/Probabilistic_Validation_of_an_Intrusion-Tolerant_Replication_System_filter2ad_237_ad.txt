(b) Unreliability
(c) Fraction of Corrupt Hosts in an
(d) Fraction of Domains Excluded at
Excluded Domain
the End of 5 Hours
Figure 4. Variations in Measures for Different Numbers of Hosts in 10 Domains
in the second study, the total number of hosts changed for
each experiment in the study.
Figures 4(a) and 4(b) show respectively, the variation in
unavailability and unreliability with an increasing number
of hosts in 10 domains. Since the probability of a successful
intrusion into a host is assumed to be the same in all exper-
iments, the existence of more hosts in a domain implies a
greater chance that one of them will be corrupt, resulting in
exclusion of the entire domain. This causes a slight increase
in the unavailability and unreliability. Note, however, that
the variation in values for the ﬁrst 5 time units is quite low,
and even for the ﬁrst 10 time units, it is much lower than the
variation observed in the study described in Section 4.1.
Figure 4(c) shows that a considerable waste of resources
takes place when we put more hosts in a domain, since the
domain will be excluded as soon as a small number of hosts
are corrupted. Figure 4(d) indicates that the number of do-
mains that have been excluded increases due to the increase
in the number of hosts. That is again explained by the fact
that corruption (and detection) of a single host leads to the
exclusion of a domain, and with more hosts in each domain,
chances of corruption of a domain are higher.
Hence, from the above two studies, we observe that it
is clearly better to put as few hosts per domain as possible.
The second study indicates that increasing the number of
hosts per domain does not provide any signiﬁcant improve-
ment, even when the number of hosts in the system (and
hence cost) increases signiﬁcantly. Hence, our studies sug-
gest that unless constrained by physical limitations such as
those that might be caused by network design or ﬁrewall
placement, it is advisable to form more domains by having
fewer hosts per domain.
4.3. Comparison of Domain-exclusion and Host-
exclusion Management Algorithms
Another major management issue is that of what to ex-
clude when a host (or a replica on it) is found to be corrupt
(assuming multiple hosts per domain). One approach is to
exclude the entire domain that contains the host. This is a
preemptive strike against the attackers, using the assump-
tion that the attack may have spread to other hosts in the
domain. The other approach is to exclude only the detected
host, thus saving resources. We designed experiments to
study which of the approaches was better for different rates
of attack spread.
In the following set of experiments, we assumed that
the corruption of the host operating system and services
increased ﬁvefold the chances that the replicas and man-
agement entity running on the host would be corrupt. The
parameter values for the experiment were the same as for
the previous experiments, except that we had 10 domains
with 3 hosts per domain, and 4 applications with 7 replicas
each. The within-domain attack spread rate varied from 0
(low) to 10 (high). We would like to remind the reader that
the spread rate determines how quickly the attack on a host
affects the other hosts in its domain. A spread rate of 5 or
more is quite high, but may be reasonable for the scenario
considered, since major hardening is done at inter-domain
boundaries, and not so much within domains.
Figure 5(a) shows that in the short run (5 hours) for low
values of attack spread, exclusion of a single host provides
better application availability than the domain exclusion
scheme does. However, the two perform similarly for high
values of attack spread rate. On the other hand, as shown in
Figure 5(b), the domain-exclusion scheme outperforms the
host-exclusion scheme in the longer run (10 hours) for most
values of the attack spread rate. As expected, the attack
spread rate does not have much effect on the performance
of the domain-exclusion scheme.
Figure 5(c) shows that under the domain-exclusion
scheme, application reliability doesn’t change much as the
within-domain attack spread rate changes, but that it is sen-
sitive to the spread rate under the host-exclusion scheme.
For the parameter ranges studied, the domain-exclusion
scheme provides better application reliability for spread
rates of 4 or more (for the ﬁrst 5 hours). Figure 5(d) shows
that the domain-exclusion scheme outperforms the host-
exclusion scheme for almost all spread rate values for the
longer time run of 10 hours.
The above results indicate that for the studied attack and
detection rates, even for a low within-domain attack spread
rate, a preemptive-action-based domain-exclusion scheme
performs almost as well as the host-exclusion scheme in the
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:27:53 UTC from IEEE Xplore.  Restrictions apply. 
s
t
i
n
u
e
m
i
t
5
t
s
r
i
f
e
h
t
r
o
f
y
t
i
l
i
b
a
l
i
a
v
a
n
U
0.035
0.03
0.025
0.02
0.015
0.01
0.005
0
0
Host exclusion
Domain exclusion
4
2
8
Rate of attack spread
6
s
t
i
n
u
e
m
i
t
0
1
t
s
r
i
f
e
h
t
r
o
f
y
t
i
l
i
b
a
l
i
a
v
a
n
U
0.2
0.18
0.16
0.14
0.12
0.1
0.08
0.06
0.04
0
10
Host exclusion
Domain exclusion
4
2
8
Rate of attack spread
6
s
t
i
n
u
e
m
i
t
5
t
s
r
i
f
e
h
t
r
o
f
y
t
i
l
i
b
a
i
l
e
r
n
U
0.1
0.09
0.08
0.07
0.06
0.05
0.04
0.03
0.02
0.01
0
0
10
Host exclusion
Domain exclusion
4
2
8
Rate of attack spread
6
s
t
i
n
u
e
m
i
t
0
1
t
s
r
i
f
e
h
t
r
o
f
y
t
i
l
i
b
a
i
l
e
r
n
U
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0
10
Host exclusion
Domain exclusion
4
2
8
Rate of attack spread
6
10
(a) Unavailability for the First 5 Hours (b) Unavailability for the First 10 Hours (c) Unreliability for the First 5 Hours
(d) Unreliability for the First 10 Hours
Figure 5. Unavailability and Unreliability for Different Exclusion Algorithms
short run, and signiﬁcantly better in the long run.
5. Conclusion
In this paper, we present a probabilistic validation of an
intrusion-tolerant replication system. The results are sig-
niﬁcant for the following reasons. First, they demonstrate
the utility of probabilistic modeling for validating complex
intrusion-tolerant architectures, and show that stochastic ac-
tivity networks are an appropriate model representation for
this purpose. A model abstracts a system’s implementation
and behavior. Models can be used for validation because
it is easier to analyze properties of a model, than it is to
analyze the same properties of the real system. The SAN
model created was modular, and it can be easily modiﬁed to
represent other intrusion-tolerant systems.
Furthermore, the results present useful insights into the
relative merits of various design choices for the ITUA repli-
cation management system. The results show that for the
ITUA replication management system, it was advisable to
put as few hosts per domain as the physical constraints
would allow, since the intrusion tolerance offered by the
system was highly sensitive to the number of security do-
mains available for starting new replicas. We also stud-
ied another management scheme in which only the corrupt
host is excluded, and observed that if an attack can spread
quickly within a domain, it is better to exclude the entire
domain when an intrusion is detected.
Acknowledgments: We would like to thank the other
members of the ITUA team for their helpful comments. We
are grateful to Jenny Applequist for her editorial assistance.
References
[1] M. Castro and B. Liskov. Practical Byzantine Fault Toler-
In Proc. Third Symp. on Operating Systems Design
ance.
and Implementation, pages 173–186, Feb. 1999.
[2] T. Courtney, J. Lyons, H. V. Ramasamy, W. H. Sanders,
M. Seri, M. Atighetchi, P. Rubel, C. Jones, F. Webber, P. Pal,
R. Watro, M. Cukier, and J. Gossett. Providing Intrusion
Tolerance with ITUA. In Supplement of the 2002 Intl Conf.
on Dependable Sys. and Networks (DSN-2002), pages C–5–
1– C–5–3, June 2002.
[3] D. D. Deavours, G. Clark, T. Courtney, D. Daly, S. Derisavi,
J. M. Doyle, W. H. Sanders, and P. G. Webster. The M¨obius
Framework and Its Implementation.
IEEE Trans. on Soft-
ware Engineering, 28(10):956–969, Oct. 2002.
[4] F. Gong, K. Goseva-Popstojanova, F. Wang, R. Wang,
K. Vaidyanathan, K. Trivedi, and B. Muthusamy. Charac-
terizing Intrusion Tolerant Systems Using A State Transition
Model. In Proc. DARPA Information Survivability Conf. and
Expo. II (DISCEX’01), pages 211–221, 2001.
[5] S. Jha and J. M. Wing. Survivability Analysis of Networked
Systems. In Proc. 23rd Intl Conf. on Software Engineering
(ICSE2000), pages 307–317, 2001.
[6] E. Jonsson and T. Olovsson. A Quantitative Model of the Se-
curity Intrusion Process Based on Attacker Behavior. IEEE
Trans. on Software Engineering, 23(4):235–245, Apr. 1997.
[7] C. Landwehr. Formal Models for Computer Security. Com-
puter Surveys, 13(3):247–278, Sept. 1981.
[8] B. Littlewood, S. Brocklehurst, N. Fenton, P. Mellor,
S. Page, D. Wright, J. Doboson, J. McDermid, and D. Goll-
mann. Towards Operational Measures of Computer Security.
Journal of Computer Security, 2(2-3):211–229, 1993.
[9] J. Lowry. An Initial Foray into Understanding Adversary
Planning and Courses of Action. In Proc. DARPA Informa-
tion Survivability Conf. and Expo. II (DISCEX’01), pages
123–133, 2001.
[10] R. Ortalo, Y. Deswarte, and M. Kaˆaniche. Experiment-
ing with Quantitative Evaluation Tools for Monitoring Op-
erational Security.
IEEE Trans. on Software Engineering,
25(5):633–650, 1999.
[11] H. V. Ramasamy, P. Pandey, J. Lyons, M. Cukier, and W. H.
Sanders. Quantifying the Cost of Providing Intrusion Toler-
ance in Group Communication Systems. In Proc. 2002 Intl
Conf. on Dependable Systems and Networks (DSN 2002),
pages 229–238, June 2002.
[12] W. H. Sanders and J. F. Meyer. Stochastic Activity Net-
works: Formal Deﬁnitions and Concepts.
In E. Briksma,
H. Hermanns, and J. P. Katoen, editors, Lectures on For-
mal Methods and Performance Analysis, pages 315–343.
Springer-Verlag, Berlin, 2001.
[13] S. Singh. Probabilistic Validation of an Intrusion-Tolerant
Replication System. Master’s thesis, University of Illinois
at Urbana-Champaign, 2002.
[14] US Department
Computer
System
Book”).
http://www.radium.ncsc.mil/tpep/library/rainbow/5200.28-
STD.html, Dec. 1985. DoD 5200.28-STD.
of Defense
Criteria
Trusted
(“Orange
Evaluation
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:27:53 UTC from IEEE Xplore.  Restrictions apply.