User Authentication. One particularly important public object
is the login form,
in which the user typically enters her
credentials (e.g., username and password). Clientless secure-
objects allow the client to securely retrieve the public login
page from the CDN and send the user’s credentials to the
content-origin for veriﬁcation. When the user authenticates,
she receives from the origin-gateway her secret user-key and a
signed cookie, which identiﬁes the user (we provide details in
5
Signature(on hash) Data (content and rendering info)Hash (of data, URI and cach headers)MACEncrypted Data(content and rendering info)(on enc. data, URI, cache headers)(a) Alice’s object table
(b) Bob’s object table
Fig. 6: Alice and Bob’s object-tables. Alice and Bob share a
picture of their pet (PetPic).
Section V). The user’s key is stored at the client in local storage
(available since HTML 5). This key is never shared with CDN
proxies and allows the RootJS to display private content for that
user (see next subsection). In contrast, the cookie is attached
to all HTTP requests that the RootJS sends (by the browser), it
allows the CDN proxies to enforce privacy policies in which
only authorized users can retrieve private objects, as we next
describe. The cookie also has an expiration date, which forces
users to re-authenticate periodically.
E. Private Objects
Private objects are available only to a speciﬁc set of
authorized users, therefore, we ensure conﬁdentiality and au-
thenticity of these objects. Private objects are encrypted and
authenticated with symmetric object-keys. Figure 5b illustrates
an encapsulated private object.
Each user is associated with an object table, illustrated
in Figure 6, which maps the user’s private objects’ URIs
to their keys. This allows users to share private objects by
adding their keys to the corresponding users’ object tables, i.e.,
without duplicating objects. The object-keys speciﬁed in the
table are kept encrypted under the user-key which is not shared
with the CDN (see discussion on user authentication above).
Namely, the object table maps URIs to encrypted object-keys
(see Figure 6). The content-origin caches copies of the users’
object tables on CDN proxies. To present a private object,
the RootJS retrieves the encrypted object and its encrypted
object-keys from the proxy. Using the user’s key, the RootJS
decrypts the object’s authentication and encryption keys, and
then decapsulates and presents the private object.
F.
Implementation and Evaluation
1) Implementation: Cryptographic computations introduce
signiﬁcant overhead in JavaScript (i.e., the RootJS). Measure-
ments on a commodity mobile device (Samsung Galaxy S3)
show that computations of SHA1 and AES-128bit require 2.1
and 3.4 milliseconds (ms) for processing a 100KB object,
verifying an RSA-2048bit signature requires 14.5ms. Such
overheads introduce noticeable delays to webpage load-time.
However, the network round-trip time (RTT) is typically 10ms
to 200ms, hence most of the time loading an object is spent
waiting to receive its content. We optimize secure-object
decapsulation by incrementally computing cryptographic oper-
ations: when the RootJS receives a data block, it processes that
block while waiting for the remainder of the object’s content
to arrive.
Doing such incremental processing is simpler for pri-
vate objects, which are protected by incrementally-computed,
6
shared-key encryption and message-authentication mecha-
nisms (e.g., AES and SHA1 process their input ‘block-by-
block’). To incrementally verify the signature over public
objects, we take advantage of the common ‘hash then sign’
paradigm. To encapsulate, the origin-gateway computes the
hash of the object to be signed, and sends it at the beginning
of the encapsulated object, along with the signature (see
Figure 5a). The RootJS receives the hash and signature prior
to the object’s content; this allows to validate that the given
hash is properly-signed while receiving the rest of the content.
The RootJS computes the hash of the content incrementally, as
it arrives. When the response completes, the RootJS compares
the result with the hashed value at the beginning of the object.
In the
following set of experiments, we measured the overhead of
the clientless secure-objects mechanism for handling and dis-
playing content from popular websites. To obtain the web-
content for this test we downloaded the homepages of the 2048
most popular websites according to Alexa [5] and the objects
embedded in them. We stored these pages on a proxy server
(deployed on EC2) and fetched them over HTTPS. We com-
pare between their load times in three cases:
2) Empirical Evaluation with Real Web-Content:
1)
2)
3)
Base Case: we measure the time to load the home-
pages insecurely, i.e., without deploying clientless
secure-objects.
Public Objects: we measure loading time when the
homepages and objects within are encoded as secure
public-objects, using SHA-1 and RSA2048.
Private Objects: we measure loading time when the
homepages and objects within are encoded as secure
private-objects, and protected using AES-128 and
SHA1-HMAC. We assume that the user is logged-
in (and hence has the user-key).
We measured these cases in two network scenarios: when
the client connects to the Internet via Ethernet network, and
when the client connects via cellular network (using LTE).
The client machine is a PC with Core-i3 processor and 4GB
of RAM, running Chrome browser (v44). Figure 7 illustrates
our results.
We observe that presenting a private web-page introduces
0.04%-0.2% delay to the display time, and presenting a public
web-page introduces a higher overhead of 0.5%-1.8% delay
to the display time. Notably, we ﬁnd that the overhead in
using clientless secure-objects is lower when the client con-
nects via a cellular network, due to the increase in latency,
which allows the RootJS to ﬁnish processing one block of
the object’s data before the next one arrives. These values
are reasonable for most websites, and may be improved by
using native cryptography and with further optimizations such
as authenticating multiple public objects using Merkle trees
(requiring one signature).
IV. ORIGIN-CONNECTIVITY
CDN-on-Demand, like ‘regular’ CDNs, delivers static con-
tent from the proxies – which in turn, populate their cache by
requesting new objects from managers (serving as a second-
level cache; see description in Section II-B). Thus, for static
objects, no communication is required with the content-origin
itself. This signiﬁcantly reduces the impact of DoS attacks
URIEncrypted KeyPetPicEncAlice-key(kPetPic)AlicePicEncAlice-key(kAlicePic)URIEncrypted KeyPetPicEncBob-key(kPetPic)BobPicEncBob-key(kBobPic)(a) Ethernet connection
(b) Cellular connection
Fig. 7: Clientless secure-objects, performance evaluation. The average display time for each of the X most popular sites.
on the content-origin. However, some Origin-to-CDN com-
munication is essential, e.g., for delivering content-updates
and providing dynamic services. In CDN-on-Demand,
this
communication channel has another critical role: it is required
to securely distribute the Loader script which validates the
RootJS (see Section III).
In the following subsections we describe and evaluate
CDN-on-Demand’s mechanisms ensuring resilient communi-
cation between the managers and the content-origin, in spite
of clogging attacks on the content-origin: dynamic whitelist
ﬁltering, loss-resilient tunnel and origin quotas.
A. Dynamic Whitelist Filtering
In whitelist ﬁltering, all packets sent over a congested
link or router are dropped, with the exception of whitelisted
packets. This is one of the most effective defenses against
bandwidth-DoS (BW-DoS) attacks.
Traditional whitelist ﬁltering is static. Namely, the cus-
tomer provides to its ISP a list of legitimate source/destination
IP addresses. The ISP then discards (ﬁlters) packets sent to that
customer from other source addresses. Static whitelist ﬁltering
is one of the main anti-clogging defenses offered by current
CDNs [10], [37]. It efﬁciently blocks non-spoofed bandwidth
DoS attack packets, sent directly to the victim, as illustrated
by Attacker 1 in Figure 8. In practice, non-spoofed attack
packets are widely used in BW-DoS attacks, as they are easier
to generate by attackers, since they can be sent by benign
reﬂectors [29], [35], and clients running unprivileged malware
and/or behind NATs or ingress-ﬁltering routers.
However, static whitelisting may not prevent clogging by
attackers able to send spoofed packets, illustrated by Attacker 2
in Figure 8. IP-spooﬁng attackers can often send signiﬁcant
amounts of trafﬁc using fake source addresses, mainly by
botnets; a recent study shows that about 15% of surveyed
hosts are able to send spoofed packets [1], [7]. Although the
amount of spoofed trafﬁc that the attacker can send is therefore
typically smaller than that of non-spoofed trafﬁc, it may yet
sufﬁce to clog the limited link of the content-origin (a web-
server of a small/medium site). The main defense currently
adopted by CDNs against this threat, is to use a ‘hidden IP
address’ for the content-origin, i.e., the address is known only
to the CDN (e.g., SOS [22], CloudFlare [10], Akamai [2]).
The hope is that attackers will not be able to clog the link
Fig. 8: Bandwidth-DoS attacks against the content-origin
to the content-origin, since they do not know its address.
Recent works, however, show that these ‘hidden’ addresses
can often be exposed by simple and efﬁcient attacks [40],
[26]. Furthermore, hiding the content-origin server address is
not applicable to small sites, which want to use the same IP
address when under attack (to communicate via the CDN), as
when not under attack (to communicate directly with clients).
Hence, CDN-on-Demand does not depend on keeping the
content-origin address hidden. Instead, to prevent clogging
by spoofed packets, CDN-on-Demand uses dynamic whitelist
ﬁltering, using the source port and IP of the manager nodes,
and the destination port of the content-origin (more speciﬁ-
cally, of the origin-gateway); see illustration in Figure 9. The
origin-gateway has a key shared with all managers, and each
manager has a key shared with the origin-gateway (provided
in conﬁguration). Every refresh-interval τ (e.g., one hour),
the origin-gateway selects a new service port by using its
key to compute a pseudo-random function over the current
time (in resolution τ, e.g., in hours). The managers perform
a similar computation every τ interval to identify the new
service port. Similarly, each manager also selects a new client
port every τ interval, and the origin-gateway learns the new
ports by computing the pseudo-random function using the
managers’ keys. After each change in ports there is a small
grace period, where whitelist ﬁltering allow both old and new
ports. Using randomized ports to deploy ﬁlters against spooﬁng
DoS attackers was studied in [6], [17]; we present experiments
7
77.17.27.37.47.57.67.77.87.9816326412825651210242048Display Time (seconds)Number of Popular Websites (log-scale)1. Base Case (not using secure-objects)2. Secure Public Objects3. Secure Private Objects8.48.68.899.29.49.69.8816326412825651210242048Display Time (seconds)Number of Popular Websites (log-scale)1. Base Case (not using secure-objects)2. Secure Public Objects3. Secure Private ObjectsISPCDNAttacker 3(Crossfire)Src IP: AttackerDst IP: DecoyAttacker 1(non-spoofing)Src IP: AttackerDst IP: OriginReal Src IPCrossfire AttackSpoofed Src IPPacket-loss resilient tunnel HTTP trafficDecoy serversClientsIP-FWPort-FWProxyAttacker 2(spoofing)Src IP: ManagerDst IP: OriginDst Port: ?Origin-Gateway & Content-OriginManagerfacilitates transparent deployment of this resilient-tunneling
mechanism; we describe that in Section V.
The tunnel works at the transport layer, using UDP to send
encoded data without waiting for acknowledgments. We use
error-correcting encoding of m TCP packets into n > m
UDP packets. Speciﬁcally, we employ Rabin’s Information
Dispersal Algorithm [32], which proves to be efﬁcient and
allows recovery of all original m segments, as long as at least
m encoded segments arrive.
The choice of m is relevant to performance, not to security,
and depends on the network delay and transmission speed; in
typical congestion situation, delays are high, and accordingly
in our experiments we used m = 100. The number n of packets
encoding the m input packets, is selected as a function of m
and of the measured packet loss probability p. More precisely,
senders calculate n such that the probability for losing more
than n − m packets is less than a pre-conﬁgured probability
(e.g., 5%). Namely, for given channel loss rate p, we ﬁnd the
minimum n such that the probability that at least n−m packets
arrive is sufﬁciently large, e.g., over 95%; see Equation 1. (We
approximate the sum in Equation 1 to the normal distribution
using the central limit theorem to compute n efﬁciently).
pi(1 − p)n−i ≥ 0.95
(1)
(cid:18)n
(cid:19)
n−m(cid:88)
i
i=0
As we conﬁrm in experiments below, the loss-resilient
tunneling mechanism improves throughput signiﬁcantly in
case of attack causing high loss (despite the communication
overhead). Since the tunnel is only established between the
cloud and the content-origin, clients connecting to website
need not change – they communicate with the proxies using
standard web-protocols (HTTP/S over TCP).
One concern with the use of such mechanism, is that in
response to packet loss, it increases transmission rates (by
increasing redundancy), instead of slowing down (as done by
TCP’s congestion control mechanism). Hence, if the losses are
result of ‘regular’ network congestion rather the DoS attack,
this could increase (benign) congestion. To prevent this, the
resource-manager maintains strict limits over the amount of
trafﬁc and requests sent, by each speciﬁc customer and in
total; see next subsection. Furthermore, the trafﬁc sent to and
from the content-origin when CDN-on-Demand ‘kicks-in’ is
much reduced (static content is cached, and provided to proxies
from managers on the cloud), allowing (limited) redundancy
in transmissions, without causing congestion.
C. Origin Quotas
The provision of static content from CDN proxies to
clients signiﬁcantly reduces the consumption of content-origin
resources by legitimate clients. However, malicious clients
may intentionally make bandwidth-consuming dynamic re-
quests which cannot be provided by the proxies, and consume
precious content-origin resources,