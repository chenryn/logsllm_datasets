 0
msn
cnn
ebay
 5
 10
 15
 20
 25
 30
 35
index
)
s
e
t
y
b
o
l
i
k
(
e
z
s
i
e
v
i
t
l
a
u
m
u
c
 700
 600
 500
 400
 300
 200
 100
 0
 0
msn
cnn
ebay
 5
 10
 15
 20
 25
 30
 35
index
Figure 1: (a) Sequential and (b) cumulative views of page loads for msn.com, cnn.com, and ebay.com from a single
client
msn
cnn
ebay
cumulative size
cumulative size
(kilobytes)
(kilobytes)
 700
 600
 500
 400
 300
 200
 100
 0
 300
flow size
(kilobytes)
 200
 100
 0
 0
 5
 10  15  20  25  30  35
index
cumulative size
cumulative size
(kilobytes)
(kilobytes)
 300
 250
 200
 150
 100
 50
 80 0
 60
 40
 20
 0
flow size
(kilobytes)
msn
yahoo-sample
msn-sample
 0
 5
 10  15  20  25  30  35
index
Figure 2: (a) 3-D view of page loads for msn.com, cnn.com, and ebay.com from a single client; (b) Regions for
msn.com compared to sequences of yahoo.com and msn.com as downloaded by a single client
First, by abstracting the web browsing sessions to con-
sist of individual server sessions, we can use the pres-
ence or absence of servers and their relative ordering to
further differentiate web pages. The ordering of these
web servers provides useful information about the struc-
ture of the web page since there is often a dependency
between objects within the web page. For instance, the
HTML of a web page must be downloaded before any
other objects, and thus the ﬁrst server contacted must be
the primary web server. Second, by reﬁning our ﬂow
information on a per server basis, we can create a ﬁne
grained model of the behavior of the web browsing ses-
sion. If done correctly, the problem of identifying a web
page within anonymized NetFlow data can be reduced
to one of identifying the servers present within a given
web browsing session based on the path created by the
ﬂows they serve, and the order in which the servers are
contacted.
Logical servers
Intuitively, we could simply use the
ﬂows served by each distinct web server IP address
(which we refer to as a physical server) to create the
3-dimensional space that describes the expected behav-
ior of that physical server in the web browsing session.
However, the widespread use of Content Delivery Net-
works (CDNs) means that there may be hundreds of dis-
tinct physical web servers that serve the same web ob-
jects and play interchangeable roles in the web brows-
ing session. These farms of physical servers can actually
be considered to be a single logical server in terms of
their behavior in the web browsing session. Therefore,
the 3-dimensional models we build are derived from the
samples observed from all physical servers in the logical
server group.
Of course, the creation of robust models for the detec-
tion of web pages requires that the data used to create the
models reﬂect realistic behaviors of the logical servers
and the order in which they are contacted. There are a
number of considerations which may affect the ability
342
16th USENIX Security Symposium
USENIX Association
cumulative size
cumulative size
(kilobytes)
(kilobytes)
 250
 200
 150
 100
 50
 0
 60
flow size
(kilobytes)
 40
 20
 0
 0
 5
 10
index
 15
 20
cumulative size
cumulative size
(kilobytes)
(kilobytes)
 250
 200
 150
 100
 50
 0
 60
flow size
(kilobytes)
 40
 20
 0
 0
 5
 10
index
 15
 20
cumulative size
cumulative size
(kilobytes)
(kilobytes)
 250
 200
 150
 100
 50
 0
 60
flow size
(kilobytes)
 40
 20
 0
 0
 5
 10
index
 15
 20
(a) msn.com Server 1
(b) msn.com Server 2
(c) msn.com Server 3
Figure 3: 3-D view of msn.com separated by server as observed by a single client
of data to accurately predict the behavior of web page
downloads. These considerations are especially impor-
tant when an attacker is unable to gain access to the
same network where the data was collected, or when
that data is several months old. Liberatore et al. have
shown that the behavioral proﬁles of web pages, even
highly dynamic web pages, remain relatively consistent
even after several months [18], though the effects of web
browser caching behavior and the location where the net-
work data was captured have not yet been well under-
stood.
4 An Automated Classiﬁer for Web Pages
in NetFlows
§
In this section, we address the problem of building auto-
mated classiﬁers for detecting the presence of target web
pages within anonymized NetFlow data. Through the use
of features discussed in
3, we create a classiﬁer for each
web page we wish to identify. The classiﬁer for a tar-
get web page consists of the 3-dimensional spaces for
each of its logical servers, which we formalize by using
(i) kernel density estimates [28], and (ii) a series of con-
straints for those logical servers, formalized by a binary
Bayes belief network [20]. The goal of the classiﬁer is to
attempt to create a mapping between the physical servers
found in the anonymized web browsing session and the
logical servers for the target web page, and then to use
the mapping to evaluate constraints on logical servers for
the web page in question. These constraints can include
questions about the existence of logical servers within
the web browsing session, and the order in which they
are contacted by the client.
If the mapping meets the
constraints for the given web page, then we assume that
the web page is present within the web browsing session;
otherwise, we conclude it is not.
There are several steps, illustrated in Figure 4, that
must be performed on the anonymized NetFlow logs in
order to accurately identify web pages within them. Our
ﬁrst step is to take the original NetFlow log and parse
the ﬂow records it contains into a set of web browsing
sessions for each client in the log. Recall that our initial
discussion assumes the existence of an efﬁcient and ac-
curate algorithm for parsing these web browsing sessions
from anonymized NetFlow logs. These web browsing
sessions, by deﬁnition, consist of one or more physical
server sessions, which are trivially parsed by partitioning
the ﬂow records for each client, server pair into separate
physical server sessions. The physical server sessions
represent the path taken within the 3-dimensional space
(i.e., ﬂow size, cumulative size, and index triples) when
downloading objects from the given physical server. At
this point, we take the paths deﬁned by each of the phys-
ical servers in our web browsing session, and see which
of the logical servers in our classiﬁer it is most similar
to by using kernel density estimates [28]. Therefore, a
given physical server is mapped to one or more logical
servers based on its observed behavior. This mapping in-
dicates which logical servers may be present within our
web browsing session, and we can characterize the iden-
tity of a web page by examining the order in which the
logical servers were contacted using a binary belief net-
work. If we can satisfy the constraints for our classiﬁer
based upon the logical servers present within the web
browsing session, then we hypothesize that an instance
of the web page has been found. In
4.2, we
discuss how the kernel density estimates and binary be-
lief networks are created, respectively.
4.1 and
§
§
4.1 Kernel Density Estimation
In general, the kernel density estimate (KDE) [28] uses a
vector of samples, S = to derive an es-
timate for a density function describing the placement of
points in some d-dimensional space. To construct a KDE
for a set of samples, we place individual probability dis-
tributions, or kernels, centered at each sample point s i.
In the case of Gaussian kernels, for instance, there would
be n Gaussian distributions with means of s1, s2, ..., sn,
respectively. To control the area covered by each distri-
bution, we can vary the so-called bandwidth of the ker-
USENIX Association
16th USENIX Security Symposium
343
Figure 4: General overview of our identiﬁcation process
nel. For a Gaussian kernel, its bandwidth is given by the
variance (or covariance matrix) of the distribution. Intu-
itively, a higher bandwidth spreads the probability mass
out more evenly over a larger space.
Unfortunately, determining the appropriate bandwidth
for a given set of data points is an open problem. One ap-
proach that we have found to produce acceptable results
is to use a “rule of thumb” developed by Silverman [28]
and reﬁned by Scott [27]. The bandwidth is calculated
as H∆ = N−1/(d+4)σ∆, for each dimension ∆ = 1...d,
where N is the number of kernels, d is the number of di-
mensions for each point, and σ∆ is the sample standard
deviation of the ∆ dimension from the sample points in
S. The primary failing of this heuristic is its inability to
provide ﬂexibility for multi-modal or irregular distribu-
tions. However, since this heuristic method provides ad-
equate results for the problem at hand, we forego more
complex solutions at this time.
Once the distributions and their associated bandwidths
have been placed in the 3-dimensional space, we can cal-
culate the probability of a given point, t j, under the KDE
model as:
n−1(cid:1)
1
n
P (tj) =
Psi(tj)
i=0
(1)
where Psi(tj) is the probability of point tj under the ker-
nel created from sample point si.
4.1.1 Application to Web Page Identiﬁcation
To apply our anonymized NetFlow data to a KDE model,
we take the set of paths deﬁned by the triples of ﬂow size,
cumulative size, and index in each of the physical server
sessions of our training data and use them as the sample
points for our kernels. We choose the Gaussian distribu-
tion for our kernels because it allows us to easily eval-
uate probabilities over multiple dimensions. The band-