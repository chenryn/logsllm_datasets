the user connecting from the wrong environment. Indeed the appli-
cation contributing 8% of all failures inside the enterprise network
is the misconﬁgured performance monitoring application mentioned
earlier.
Figure 4 presents a detailed look at outbound ﬂows for the soft-
ware patching service. The ﬁgure plots failed ﬂows for each user
as a function of time; the size of each circle is proportional to the
number ﬂows failing in that hour with the largest circle representing
1583 failures. A horizontal slice corresponding to a particular user
is illustrated at the bottom for clarity. The ﬁgure exposes a service
outage on 2/17 and continuing degraded performance in the subse-
quent days, which we conﬁrmed to be the result of signiﬁcant added
load on the service during the noted days. At their peak, hosts re-
lentlessly initiated on average 11 failed ﬂows per minute during the
one-day outage — a scenario where adaptive retry intervals would
have reduced the load on the service and the failures observed.
In Figure 5 we plot the CDF of outages, deﬁned as the time be-
tween successful ﬂows to the same destination, across all users
Broadcast
Failed
Successful
)
%
(
s
w
o
l
f
f
o
n
o
i
t
c
a
r
F
 100
 80
 60
 40
 20
 0
DHCP
Microsoft
Services
Software
Patching
Anti-Virus
Update
Figure 6: Multiple applications duplicate work at endhosts to
discover services, some introducing more failures in the process
than others.
and applications (the bottom curve). The line is curiously linear
across six orders of magnitude. We also plot the CDF of retry inter-
vals (the top curve); the ten most failing applications listed in Fig-
ure 3 account for 82% of these persistent retries, with the software-
patching app accounting for the jump at 6s. The ﬁgure exposes
a fundamental disconnect between the need for retrying, and the
retry interval: 20% of outages are transient and disappear within
two seconds, however, beyond this, destinations experience what
we term “outage inertia” where the longer the outage has lasted,
the longer it is expected to continue to last. Retry intervals, on
the other hand, tend to be static, typically lasting less than 15 sec-
onds, which for long outages results in a large number of failures.
80% of retries are within 1 minute of a previous failure, whereas
the 80th percentile outage lasted 53 minutes. Dynamically picking
the interval before retrying a ﬂow at the application level, perhaps
performing exponential backoff, would be one possible approach
to reducing the number of failures while still closely tracking the
outage duration.
4.2 Service Discovery
Applications use several ad hoc mechanisms for self conﬁgura-
tion and service discovery. In Figure 6 we plot the distribution of
trafﬁc for four illustrative applications. The ﬁrst is DHCP that at-
tempts to reach a previously cached server, failing which it relies on
broadcast to discover the conﬁguration server. The second is Mi-
crosoft’s NetBIOS protocol, which uses broadcast to elect a leader
endhost that then assists other nearby endhosts in discovering re-
sources, potentially resulting in a ﬂurry of failures and broadcast
trafﬁc when the leader switches networks. The third is the software
patching service that relies heavily on periodic hop-limited probes,
and the fourth is the anti-virus service that periodically polls the
server. Each application, in essence, duplicates service discovery,
some more noisily than others, by operating independently rather
than amortizing their efforts by collaborating.
4.3 Vulnerability Testing
Vulnerability testing, where a host designated by IT scans for
vulnerabilities in endhosts on a routine basis, results in a class of
“useful” failures, something that we did not originally anticipate.
Such services are responsible for adding a uniform number of fail-
ures across a wide range of otherwise unused ports (e.g.
telnet,
ﬁnger, ident). 8.5% of failures for ﬂows inbound into the endhost
(4.8% of all failures in our enterprise) can be attributed to vulnera-
bility testing.
5. COPING STRATEGIES
Based on our experience in this paper, we identify two key areas
in which applications can be modiﬁed incrementally to yield signif-
icant savings. First, is retrying connections adaptively by backing
off exponentially at the application layer if an outage lasts for more
than a few seconds. Second is discovering services without intro-
ducing chatter.
More generally we believe there is an opportunity to amortize
polling and environmental awareness efforts across multiple appli-
cations by integrating this service into the network architecture. At
a minimum such a service must 1) allow an application to detect
the environment, 2) allow the application to discover infrastructure
services with application level constraints (e.g. closest reachable
server, low load), 3) do so securely without leaking sensitive in-
formation, and efﬁciently without extensive polling, and 4) be in-
tegrated with the network architecture so applications can rely on
the service always being present. We note that such applications do
exist today [9, 10], but exist independently and are not integrated
into the required framework.
At the same time, it would be useful to develop online appli-
cations that can differentiate between different kinds of problems
to assist IT in focusing their efforts. Problems such as miscon-
ﬁgurations, failures caused routinely by applications, and failures
caused by hosts transitioning between environments must be sepa-
rated from scanning and malicious activity requiring immediate IT
attention.
Altogether we believe that some kinds of waste can and should
be reduced, while other waste inherent in the system must be well-
documented to make the job of identifying real problems amidst
the noise far simpler than it is today.
6. RELATED WORK
Traditionally, the collection of trafﬁc traces is done at network
aggregation points, backbone links or high volume servers [5, 13,
2] and these provide a brief window into the behavior of a very
large number of users. However, it is hard to construct a long term
behavioral description of individual endhosts from such traces, par-
ticularly so in networks where a signiﬁcant population is mobile.
There are very few endhost traces available, understandably, given
the formidable legal and logistical barriers to collect such data. The
NETI@Home project is one such endhost based framework [15];
the goal is to use statistics, rather than packet traces, collected from
a large collection of endhosts to (centrally) identify end-to-end per-
formance issues. However, the information exported from the end-
host is very high level and of limited use. Our own trace is unique
in that we collect packet traces from a large population of (mobile)
endhosts that move between environments. In previous work, we
analyzed these traces and showed that there are signiﬁcant differ-
ences in a number of trafﬁc statistics and features for the same user,
as the endhost moves between environments [6]. In this paper, we
attempt to quantify the volume of non-useful connections and to
associate causes with them.
A large body of work addresses the problem of collecting, and
mining, trafﬁc failures to understand failures, though the efforts
have focused on speciﬁc pieces of the end-to-end puzzle (e.g., web
failures in [12, 11]). The work in [12] is somewhat similar in spirit
to our own work: packet traces are collected from a number of in-
strumented endhosts which generate web page requests. It is shown
that a third of the failures arise from DNS errors, while the remain-
ing are almost all TCP connection failures, a large fraction of these
being server related. In contrast, our work in this paper does not
look at web trafﬁc or any other speciﬁc applications, but across the
suite that are used on the endhosts in our enterprise (of which web
trafﬁc is but a small part). While it is very likely that some of the
causes identiﬁed in [12] play some part in the failures that we ob-
serve, our results are broader in scope and the underlying causes we
identify have more to do with mobility of the endhost and vagaries
of how the endhosts (and applications therein) are conﬁgured.
The work that comes closest to what we present in this paper is
the study of TCP Resets undertaken by Arlitt and Williamson [3]. A
year long campus trafﬁc trace is analyzed and the authors show that
a large fraction of TCP connections involve a RST. In particular, the
results show that roughly 20-30% of the connections being non-
useful (in the sense that is described in this paper). However, the
trace that is analyzed misses all the trafﬁc that stays local to the
campus and also does not record any UDP trafﬁc. In contrast, our
own traces capture all the trafﬁc from the endhosts, including UDP
and trafﬁc that is local; we also expect trafﬁc seen on a campus
network to be dramatically different in composition from that seen
in an enterprise. Furthermore, our goal is to understand root causes
at the endhost. The results in [3] indicate that a large fraction of
the offending ﬂows are HTTP based, while in our analysis the non-
useful ﬂows are distributed over a number of applications. More
recently, it was shown that many ISP’s inject TCP RST packets
into a client’s stream in order to deliberately throttle certain trafﬁc
classes [17]. Since this behavior is not well documented or even
well understood, it is very hard to quantify how much of the non-
useful ﬂows we can attribute to this behavior.
7. SUMMARY AND FUTURE WORK
The fact that many applications and network protocols have evolved
independently over the years, combined with shrinking IT staff and
budget has led to modern day enterprise networks that embrace
high levels of noise. In our study of endhost communication traf-
ﬁc, we ﬁnd that connection failures are regular, and in some cases,
are an integral part of an application. We believe that ignoring this
development is short-sighted because of increasing mobility and
increasing security threats. In some case, we believe that simple
ﬁxes (such as backing off after repeated failures, and building en-
vironmental awareness into applications) are available.
In other
cases, such as redundant service discovery activities, a more care-
ful redesign of network architecture and/or service sharing across
applications is needed.
In the future, we would like to use application layer information
to enhance our study. This avenue is not without its challenges, as
due to user privacy issues, we would have had far fewer volunteers
had we retained full packet payloads. We also plan to evaluate
our suggested coping strategies in terms of their impact on failure
reduction.
Acknowledgements
We would like to thank the Intel employees who participated in the
data collection effort, and Toby Kohlenberg, Stacy Purcell, David
Fong, Sanjay Rungta and Manish Dave for insights into enterprise
IT operations. We are grateful to our shepherd Joel Sommers, and
our anonymous reviewers for providing detailed and helpful feed-
back on this paper.
8. REFERENCES
[1] Bro connection summaries.
http://bro-ids.org/wiki/index.php/
Reference_Manual:_Analyzers_and_Events#
Connection_summaries.
[2] Internet trafﬁc archive. http://ita.ee.lbl.gov/.
[3] ARLITT, M., AND WILLIAMSON, C. An analysis of tcp
reset behaviour on the internet. SIGCOMM Comput.
Commun. Rev. 35, 1 (2005), 37–44.
[4] BAHL, P., CHANDRA, R., GREENBERG, A., KANDULA,
S., MALTZ, D. A., AND ZHANG, M. Towards highly
reliable enterprise network services via inference of
multi-level dependencies. SIGCOMM Comput. Commun.
Rev. 37, 4 (2007), 13–24.
[5] FRALEIGH, C., MOON, S., LYLES, B., COTTON, C.,
KHAN, M., MOLL, D., ROCKELL, R., SEELY, T., AND
DIOT, S. Packet-level trafﬁc measurements from the sprint ip
backbone. Network, IEEE 17, 6 (Nov.-Dec. 2003), 6–16.
[6] GIROIRE, F., CHANDRASHEKAR, J., IANNACCONE, G.,
PAPAGIANNAKI, K., SCHOOLER, E., AND TAFT, N. The
cubicle vs. the coffee shop: Behavioral modes in enterprise
end-users. In PAM (April 2008), Springer, Ed.
[7] JUNG, J., PAXSON, V., BERGER, A., AND
BALAKRISHNAN, H. Fast portscan detection using
sequential hypothesis testing. Security and Privacy, 2004.
Proceedings. 2004 IEEE Symposium on (2004), 211–225.
[8] MALTZ, D., AND BHAGWAT, P. Msocks: an architecture for
transport layer mobility. INFOCOM ’98. Seventeenth Annual
Joint Conference of the IEEE Computer and
Communications Societies. Proceedings. IEEE 3 (Mar-2 Apr
1998), 1037–1045 vol.3.
[9] Marco Polo for Mac OS.
http://www.symonds.id.au/marcopolo/.
[10] Centrix NetworkLocation.
http://centrix.ca/NetworkLocation/.
[11] PADMANABHAN, V., QIU, L., AND WANG, H.
Server-based inference of internet link lossiness. INFOCOM
2003 1 (30 March-3 April 2003), 145–155 vol.1.
[12] PADMANABHAN, V., RAMABHADRAN, S., AGARWAL, S.,
AND PADHYE, J. A study of end-to-end web access failures.
In CoNEXT (2006).
[13] PANG, R., ALLMAN, M., BENNETT, M., LEE, J., PAXSON,
V., AND TIERNEY, B. A ﬁrst look at modern enterprise
trafﬁc. In IMC’05 (Berkeley, CA, USA, 2005), USENIX
Association, pp. 2–2.
[14] PAXSON, V. Bro: A system for detecting network intruders
in real-time. Computer Networks 31, 23-24 (December
1999), 2435–2463.
[15] SIMPSON, C. R., AND RILEY, G. F. Neti@home: A
distributed approach to collecting end-to-end network
performance measurements. In Passive and Active
Measurements (2004), C. Barakat and I. Pratt, Eds.,
vol. 3015 of Lecture Notes in Computer Science, Springer,
pp. 168–174.
[16] STONE-GROSS, B., WILSON, C., ALMEROTH, K.,
BELDING, E., ZHENG, H., AND PAPAGIANNAKI, K.
Malware in ieee 802.11 wireless networks. PAM 2008 (April
2008).
[17] VUZE. First results from vuze network monitoring tool,
April 2008.