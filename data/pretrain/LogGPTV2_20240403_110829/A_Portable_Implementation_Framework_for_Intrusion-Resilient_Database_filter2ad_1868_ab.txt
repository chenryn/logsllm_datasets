dressing format is convenient for recovering a database af-
ter a crash or media failure when the transaction log needs
to be rolled forward.
The effect of a transaction can be nulliﬁed with a com-
pensating transaction. To do so, we consider each row af-
fected by an original transaction, create a compensating
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:30:32 UTC from IEEE Xplore.  Restrictions apply. 
Original statement
Modiﬁed statement(s)
SELECT t1.a1, ..., t1.an1 , ..., tk.ank FROM t1, ..., tk WHERE c
SELECT t1.a1, ..., t1.an1 , ..., tk.ank , t1.trid, ..., tk.trid
SELECT SUM(t.a) FROM t WHERE c GROUP BY t.b
UPDATE t SET a1 = v1, ..., an = vn WHERE c
INSERT INTO t(a1, ..., an) VALUES (v1, ..., vn)
COMMIT
FROM t1, ..., tk WHERE c
SELECT t.trid FROM t WHERE c
SELECT SUM(t.a) FROM t WHERE c GROUP BY t.b
UPDATE t SET a1 = v1, ..., an = vn,
trid = curT rID WHERE c
INSERT INTO t(a1, ..., an, trid)
VALUES (v1, ..., vn, curT rID)
INSERT INTO trans dep(curT rID, ...)
COMMIT
Table 1. Modiﬁcations to SQL statements that the intercepting proxy makes to track and record inter-
transaction dependency information.
statement for each affected row, and form a compensating
transaction from these per-row compensating statements.
For instance, if a particular row was deleted, its compensat-
ing statement is an INSERT that puts this row back into the
database. Similarly, if a row was inserted, the compensating
action is a DELETE statement. Finally, if a row was updated,
a compensating action is another UPDATE statement restor-
ing the pre-update image of the row. Although the trans-
action log can be used to generate compensating transac-
tions, it does not contain sufﬁcient information to address
any given row precisely so that each compensating state-
ment is applied to that row only. Fortunately, most DBMSs
support a read-only row ID attribute in each table. We can
use this attribute in WHERE clause of UPDATE and DELETE
compensating statements to ensure that the change is ap-
plied to a particular row only.
The intercepting proxy generates its own transaction IDs
at run time because it is not always possible to access the
internal transaction ID of the underlying DBMS, if it ex-
ists at all. To correlate a transaction’s internal ID with its
proxy-generated ID, one searches for the last log entry right
before the transaction’s commit operation, which should be
an insert operation into the trans dep table. The proxy-
generated ID contained in this inserted row and the inter-
nal transaction ID recorded in this log entry establish the
desired correspondence. Once the correspondence between
two types of transaction ID is established, transaction de-
pendencies due to UPDATE and DELETE statements are gen-
erated. For each entry in the transaction log that is due to a
DELETE and UPDATE statement, one builds up a dependency
between the transaction to which the log entry belongs and
the transaction whose ID is stored in the pre-update row im-
age associated with the log entry.
After all transaction dependencies are identiﬁed, the
complete transaction dependency graph is visually pre-
sented to the DBA. An example transaction dependency
graph display is shown in Figure 3. The current prototype
uses GraphViz [13], an open source graph drawing soft-
ware from AT&T, for graph display. The DBA determines
the ﬁnal undo set of transactions by analyzing the trans-
action dependency graph using the application-speciﬁc do-
main knowledge. Ideally, this transaction dependency graph
rendering software should be part of an interactive database
damage repair tool, which allows the DBA to include cer-
tain inter-transaction dependencies into dependency analy-
sis and ignore others, thus avoiding both false positives and
negatives. For instance, if the database contains a temporary
table that does not have any semantic signiﬁcance, the DBA
may choose to ignore all the dependencies among transac-
tions due to this table. As another example, one transaction
may depend on another due to false sharing, i.e., touching
different attributes of the same row. In this case, the DBA
may choose to ignore this type of dependencies in the re-
pair process.
After the undo transaction set is determined, each entry
in the transaction log is checked from the end to the be-
ginning. If the proxy transaction ID of a log entry belongs
to the undo set its corresponding compensating statement
is executed immediately. Special care is required for rows
inserted to the database during the repair process. For ex-
ample, when a DELETE log entry is to be undone, a new
row is inserted into the database, and the DBMS assigns it
a unique row ID, which may be different from the row ID
that was used to refer to this row in the transaction log pre-
viously. As a result, the old row ID needs to be mapped to
the new row ID when processing all subsequent log entries
associated with this row. When an INSERT log entry asso-
ciated with this row is encountered, the mapping has to be
discarded. Each table has its own row ID mapping. Conse-
quently, the same row ID can be mapped to different row
IDs in different tables.
Unlike inter-transaction dependency tracking, the repair-
time logic of an intrusion-resilient DBMS is very database-
speciﬁc, because many of the following data structures are
proprietary: the transaction log format, transaction ID and
row ID encoding, pre-update row image representation, etc.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:30:32 UTC from IEEE Xplore.  Restrictions apply. 
Order_0_3_0_4
Payment_0_3_0_5
Order_0_1_5_6
Deliv_0_1_7
Payment_0_1_5_10
Order_0_4_1_12
Order_0_8_0_14
Order_0_5_4_15
Figure 3. Visualization of a sample inter-
transaction dependency graph. Nodes cor-
respond to transactions and edges to inter-
transaction dependencies. Each node has a
label describing the transaction type, e.g.,
Order for an order placement transaction,
Payment for an order payment transaction,
Deliv for an order delivery transaction. Num-
bers that are part of each label are the ware-
house ID, the district ID, the client ID, and the
transaction ID.
4. Implementation Issues
The most challenging part of our prototype implementa-
tion efforts is transaction log parsing, analysis, and recon-
struction. In this section, we discuss in greater detail how re-
constructing transaction log entries in Oracle, Sybase, and
PostgreSQL is done.
4.1. Transaction Log Processing in Oracle
Oracle provides a set of PL/SQL procedures called Log-
Miner [14] which is designed to convert a binary transac-
tion log into a database table called v$logmnr contents,
which is accessible via SQL inside the database. This
database table contains one row per transaction log en-
try. Each row has attributes such as operation type, user ID,
transaction ID, as well as a corresponding redo and undo
SQL statement. In order to roll back a particular trans-
action, one needs to execute all undo SQL statements
available in the v$logmnr contents for this transac-
tion.
4.2. Transaction Log Processing in PostgreSQL
PostgreSQL [15] does not have any tools for access-
ing its transaction log. However, it is possible to reverse-
engineer its log format since PostgreSQL is an open source
DBMS. It turns out that for each row operation (UPDATE,
DELETE, INSERT), PostgreSQL stores complete contents of
the before and after images (if required) for that row. We
have implemented a plugin for PostgreSQL that provides a
Logminer-kind functionality.
4.3. Transaction Log Processing in Sybase
The major implementation issue in Sybase [16] is the
fact that Sybase does not have a row ID attribute in its
tables. One has to add an attribute of type numeric(n)
identity to provide a row ID for each row. The de-
pendency tracking proxy for Sybase intercepts all CREATE
TABLE statements and adds such a column to each new ta-
ble.
Sybase provides the dbcc log command to read the
contents of the transaction log. For each row operation this
command outputs the contents of the row being modiﬁed in
the binary format without dividing it into attributes. If the
row operation is INSERT or DELETE, Sybase stores com-
plete row contents in the log. However, for an UPDATE op-
eration (called MODIFY in Sybase), only those attributes that
were modiﬁed are stored in the log. Therefore, the row ID
attribute we introduced to identify a row is never saved in
the transaction log. For the repair purpose, the entire con-
tent of each row appearing in the log needs to be fully re-
stored to obtain the row ID attribute.
Sybase stores the page number and the offset within the
page for each row operation. Given these two values, one
can read the contents of the page by using dbcc page com-
mand, retrieve complete row content and access the row ID
attribute of the required row. However, a row can migrate
within a page when some other rows are deleted. Therefore,
a row’s current location at the moment when the transaction
log is read might be different from the row’s current loca-
tion when it is stored in the transaction log. The rule of row
migration is as follows: when a row is deleted from the mid-
dle of a page, all rows located closer to the end of the page
are moved closer to the beginning of the page, overwriting
the row being deleted so that no gaps ever exist in the page.
Rows cannot migrate from one page to another. These ob-
servations allow one to develop an algorithm to keep track
of the location of each row in a page and thus to identify
the row ID attribute of every row associated with log en-
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:30:32 UTC from IEEE Xplore.  Restrictions apply. 
tries of type MODIFY. In this algorithm, rec.len denotes the
length of data (in bytes) of a log record rec, and rec.of f de-
notes the offset of data (in bytes) of a log record rec within
a data page.
1. Read the transaction log using dbcc log command.
For each log record that modiﬁes a table that needs to
be repaired, store its record type and data offset in the
repair tool’s memory.
2. Go through the list of all log records again and do
the following: for each record rm of type MODIFY go
through all log records of type DELETE located after
rm. For each DELETE record rd, decrement rm.of f
by rd.len if rd.of f + rd.len ≤ rm.of f. If rd.of f ≤
rm.of f < rd.of f + rd.len, then the current DELETE
operation deletes the row being modiﬁed, and since the
log entry associated with a DELETE operation keeps a
complete image of the deleted row, this image could be
used as the before image for the MODIFY statement as
well in this case.
3. Go through the list of all records and for each record
rm of type MODIFY, issue a command dbcc page
with an appropriately adjusted rm.of f to read the full
row content.
Having restored the complete row data for each operation,
it is now relatively straightforward to generate all compen-
sating statements. We have implemented the algorithm pre-
sented above and were able to generate compensating state-
ments correctly.
4.4. Limitations of Current Prototype
Our current prototype has several limitations. First, there
is no support for stored procedures. However, the code
stored on the server’s side can be modiﬁed in advance to
support transaction dependency tracking. Second, almost
all DBMS vendors provide custom extensions to standard
SQL. Our current prototype supports only a subset of it,
thus making it impossible to use the tracking proxy in an
arbitrary database. However, the tracking proxy can be cus-
tomized for each DBMS vendor. The current prototype has
also several limitations due to its inter-transaction depen-
dency tracking mechanism. Essentially, the tracking is row-
based rather than ﬁeld-based. This can result in false inter-
transaction dependencies. Another similar problem is the
dependencies that are the result of application logic (such
as inter-process communication). Such dependencies can-
not be tracked by our current tracking proxy.
Number of warehouses
Districts per warehouse
Clients per district
Items per warehouse
Orders per district
10
30
5000
100000
5000
Table 2. Test database parameters and their
values.
mechanism added to Oracle, Sybase, and PostgreSQL. This
benchmark simulates activities of a wholesale supplier. The
supplier operates a number (W ) of warehouses each of
which has its own stock. A warehouse is comprised of
a number of districts, each of which in turn has a num-
ber of clients. The TPC-C benchmark also describes the
set of transactions that are issued during benchmarking
runs: order placement, payment, order delivery, order sta-
tus inquiry, stock level inquiry. Orders can only be made
by clients already in the database. We created a TPC-C
database and populated it with random data. The parameters
of the database are shown in Table 2. For the DBMSs that
support disk space pre-allocation, we have pre-allocated a
sufﬁciently large data ﬁle of size 4.5 GB to avoid dynamic
allocations at run time. All databases were conﬁgured with
a block size of 8 KB.
The following machines are used in the experiments. A
Pentium-4M 1.7GHz laptop with 512MB of RAM and a
30GB hard drive spinning at 4200 RPM is used as a client
issuing transactions requests. A Pentium-4 1.5GHz desktop
with 384MB of RAM and a 120GB hard drive spinning at
7200 RPM is used as a server running the DBMS under test.
The two machines were placed in the same 100Mbps lo-
cal network. Both machines used Mandrake Linux 9.1. We
have measured the following DBMS servers: Oracle 9.2.0,
Sybase ASE 12.5, PostgreSQL 7.2.2. In all our experiments,
we used a single-proxy dependency tracking architecture, as
shown in Figure 1.
In this performance study, we are mainly interested in an-
swering the following two questions. First, what is the run-
time performance cost of transparently augmenting an ex-
isting DBMS with the proposed intrusion resilience mech-
anism? Second, how much value does such an intrusion re-
silience mechanism help in the post-intrusion or post-error
repair process, in terms of the percentage of transactions
whose effects can be preserved in the repair process because
they are legitimate and unaffected by the intrusion or error?
5. Performance Evaluation
5.1. Experimental Setup
We used the TPC-C benchmark [12] to evaluate the
run-time performance overhead of the intrusion resilience
5.2. Dependency Tracking Overhead
The run-time overhead of inter-transaction dependency
tracking includes query interception and modiﬁcation over-
head as well as additional SQL query processing because
of additional ﬁelds and tables introduced for dependency
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:30:32 UTC from IEEE Xplore.  Restrictions apply. 
tracking. We vary the following workload parameters when
measuring the dependency tracking overhead for different
DBMSs:
• Transaction Mix: We used a read-intensive workload
(consisting of 100 read intensive Stock Level trans-
actions) and a read/write intensive workload (con-
sisting of 200 New Order, 200 Payment and 100
Delivery transactions).
• DBMS Client-Server Connection: In local conﬁgura-
tion both the DBMS client and server were placed on
the server machine. In the networked conﬁguration the
DBMS client ran on the client machine and the DBMS
server ran on the server machine.
• Total Footprint Size: We varied the total footprint size
of the input workload so that in one case, a small
amount of data is accessed repeatedly and the data ac-
cessed is mostly in the database cache once it is loaded,
and in the other case, a large amount of data is accessed
randomly and mostly once.
Figure 4 presents the relative throughput penalty of trans-
action dependency tracking with respect to the original
DBMS without any intrusion resilience mechanism, where
the overall throughput is the ratio of the number of trans-
actions completed within a period of time over the time
period. In a typical on-line transaction processing environ-
ment, which the TPC-C benchmark attempts to emulate, the
DBMS server and client are connected through a network,
the transaction mix is read-intensive, and the total footprint
size is large so that most accesses require disk I/O access.
The results in the upper left corner of Figure 4 correspond
to this scenario, and show that the transaction dependency
tracking overhead in this case is between 6% to 13% for all
three DBMSs.
There is no clear trend as to whether the throughput
penalty of transaction dependency tracking is higher or
lower when comparing the networked conﬁguration with
the local conﬁguration. There are two factors at work here.
On the one hand, running the DBMS server and client on
the same machine, i.e., local conﬁguration, means that the
DBMS server has access to less CPU resource and thus
lower base-case performance. As a result, the percentage
overhead should be lower in the networked conﬁguration
than in the local conﬁguration. On the other hand, running
the DBMS server and client on separate machines means
that the average transaction latency is increased due to net-
work transfer delay. This increase in latency does not in it-
self decreases the throughput as long as the DBMS client
can always keep sufﬁcient transactions outstanding in the
pipeline. When this is not the case, the base-case through-
put suffers and the throughput penalty of transaction depen-
dency tracking could be increased.
Decreasing the total footprint size and thus increas-
ing the database cache hit ratio signiﬁcantly increases the
throughput penalty of transaction dependency tracking for
the read/write intensive load, but matters very little for the
read intensive load. The reason is that when the footprint
is small, the only disk I/O required is writes to the transac-
tion log, and each transaction log write becomes more ex-
pensive when the transaction dependency tracking mecha-
nism is turned on.
5.3. Accuracy of Database Damage Repair
One way to minimize the number of legitimate transac-
tions that are incorrectly ﬂagged as corruptive is to allow the
DBA to specify transaction dependencies that should be ig-
nored in the determination of the ﬁnal undo set. Because
fewer dependencies are considered, fewer transactions are
judged corruptive and put into the undo set. We call depen-
dencies that can be safely ignored a false dependency.
One example of a false dependency is when a depen-
dency is based on an attribute of a table that can be com-
puted from other data in the database. For instance, the
warehouse table in the TPC-C benchmark’s test database
contains a w ytd attribute, which is the total sum of money
spent by all clients on a warehouse. This value could have
been computed by using information from the orders ta-
ble by summing up all orders that aim at a particular ware-
house.
Let us use Tdetect to refer to the interval between when
an intrusion/error takes place and when it is detected. Fig-
ure 5 shows how the number of corruptive (those that need
to be undone) transactions and percentage of saved transac-
tions (those that survive repair) correlate with Tdetect, un-
der different warehouse factor (W ) values, where Tdetect
is expressed in terms of the number of transactions com-
mitted since the intrusion/error. As expected, the number of
transactions that are affected by the initial attack/error trans-
action increases with Tdetect, but the percentage of saved
transactions remain ﬂat except when Tdetect is small. More-
over, ignoring false dependency can signiﬁcantly increase
the number of benign transactions that can be saved from
a repair process. The difference in the number of transac-
tions that need to be undone or rolled back can be more
than a factor of 5, and the improvement in the percentage
of saved transactions ranges from 20% to 30%. The saved
transaction percentage improvement decreases with W be-
cause larger W tends to have less false sharing and thus re-
duce the beneﬁt of eliminating false dependency. This re-
sult suggests that it is crucial for an intrusion resilience en-
gine to incorporate site-speciﬁc domain knowledge from the
DBA and improve its repair accuracy by minimizing false
positives and negatives.
6. Conclusion and Future Work
The most important contribution of this work is the de-
velopment of a reusable implementation framework that