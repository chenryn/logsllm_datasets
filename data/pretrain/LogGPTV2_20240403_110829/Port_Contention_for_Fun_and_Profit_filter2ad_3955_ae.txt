(2) it collects protocol messages and digital signatures during
the TLS handshake. Figure 6 (Top) shows a trace captured
by the Spy process, containing the two scalar multiplication
operations during TLS handshake, i.e. ECDH and ECDSA
respectively.
The client drops the handshake as soon as the server
presents the digital signature; since we are only interested in
capturing up to the digital signature generation, this allows us
to capture a trace in roughly 4 ms (∼12.5 million clock cycles).
Additionally, our client concatenates the protocol messages,
hashes the resulting concatenation, and stores the message
digest. Similarly, it stores the respective DER-encoded P-384
ECDSA signatures for each TLS handshake. This process is
repeated as needed to build a set of traces, digest messages,
and digital signatures that our lattice attack uses later in the
key recovery phase.
Once the data tuples are captured, we proceed to the signal
processing phase, where the traces are trimmed and ﬁltered
to reduce the noise and output useful information. Figure 6
(Bottom) shows a zoom at the end of the (Top) trace, where
the ﬁlters reveal peaks representing add operations, separated
by several double operations.
At a high level—returning to the discussion in Section IV—
the reason our signal modulates is as follows. The wNAF
algorithm executes a (secret) sequence of double and add
operations. In turn, these operations are sequences of ﬁnite
(cid:25)(cid:24)(cid:25)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:44:40 UTC from IEEE Xplore.  Restrictions apply. 
ﬁeld additions, subtractions, multiplications, and squarings.
Yet the number and order of these ﬁnite ﬁeld operations are
not identical for double and add. This is eventually reﬂected
in their transient port utilization footprint.
C. Signal Processing Phase
After verifying the existence of SCA leakage in the captured
TLS traces, we aim to extract the last double and add sequence
to provide partial nonce information to the key recovery
phase. Although visual inspection of the raw trace reveals the
position of double and add operations, this is not enough to
automatically and reliably extract the sequence due to noise
and other signal artifacts.
Since our target is ECDSA point multiplication, we cropped
it from the rest of the TLS handshake by applying a root-
mean-square envelope over the entire trace. This resulted in
a template used to extract the second point multiplication
corresponding to the ECDSA signature generation. To further
improve our results, we correlated the traces to the patterns
found at the beginning and end of the point multiplication.
This was possible as the beginning shows a clear pattern
(trigger) due to OpenSSL precomputation, and the end of the
trace has a sudden decrease in amplitude.
We then used a low pass ﬁlter on the raw point multi-
plication trace to remove any high frequency noise. Having
previously located the end of point multiplication, we focused
on isolating the add operations to get
the last add peak,
while estimating the doubles using their length. To accomplish
this, we applied a source separation ﬁltering method known
as Singular Spectrum Analysis (SSA) [44]. SSA was ﬁrst
suggested in SCA literature for power analysis to increase
signal to noise ratio in DPA attacks [45], and later used as
a source separation tool for extracting add operations in an
EM SCA attack on ECDSA [46]. We discuss the theoretical
aspects of SSA in Appendix A.
For our purpose, we decided to threshold the SSA window
size as suggested in [45]. Since the total length of the trace
was around 15000 samples, this gave us a window size of
30. However, based on experimentation, a window of size
20 yielded optimal results using the second and the third
component.
The traces occasionally encountered OS preemptions, cor-
rupting them due to the Spy or Victim being interrupted. We
detect Spy interrupts as high amplitude peaks or low amplitude
gaps, depending on whether they happened while during or
between latency measurement windows. Similarly, the Victim
interrupts exhibit a low amplitude gap in our traces, since there
was no Victim activity in parallel. In any case, we discarded
all such traces (around 2.5%) when detecting any interrupt
during the last double and add sequence.
Finally, by applying continuous wavelet transform [47] in
the time-frequency domain we were able to detect the high
energy add peaks, therefore isolating them. Moreover, a root-
mean-square of the resulting peaks smoothed out any irreg-
ularities. Figure 6 illustrates the results of signal processing
steps on a TLS trace from top to bottom. Even after applying
(cid:25)(cid:24)(cid:26)
y
t
i
l
i
b
a
b
o
r
p
e
v
i
t
a
l
u
m
u
C
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
A
AD
ADD
ADDD
ADDDD
ADDDDD*
 100
 150
 200
 250
Length (samples)
 300
 350
Fig. 7.
multiplication.
Length distributions for various patterns at
the end of scalar
these steps, some traces where the adds were indistinguishable
due to noise still occur, decreasing the accuracy of our results
by about 2%.
The output of this phase, for each trace, is the distance from
the last add operation to the end of the point multiplication:
estimating the number of trailing doubles by counting the
number of samples. Figure 7 depicts the CDF of the resulting
sequences using our distance metric, having clear separation
for each trailing double and add sequence.
D. Key Recovery Phase: Lattices
The output of the signal processing phase eventually pro-
vides us with partial nonce information, as the trailing se-
quence tells us the bit position of the lowest set bit. We
then use this information to build a lattice attack to solve
a Hidden Number Problem, retrieving the long-term private
key used to sign the TLS handshakes. We build on previous
work for our lattice attack, deferring to [48] for a more
detailed mathematical description of the methodology. We
use the BKZ reduction algorithm (β = 20) to efﬁciently
look for solutions to the Shortest Vector Problem (SVP),
closely following the construction by Benger et al. [41], yet
with different parameters, and also a brute-force heuristic. In
what follows, we: (1) describe exploring the lattice parameter
space using traces modeled without errors; then (2) combine
this study with proﬁling of the experimental data and the
constraints of the computational resources at our disposal to
launch a real-world end-to-end attack.
Exploration of
the lattice parameter space: The main
parameter to tune in implementing the lattice attack is the
size (d) of the set of signatures used to build the lattice
basis. Theoretically, given an inﬁnite amount of time, if the
selected subset of signatures does not contain any error and
if the lattice embeds more bits of knowledge than the bit-
length of the secret key, it should eventually succeed. In this
scenario, optimizing for the smallest d that delivers enough
bits of knowledge to recover the private key would be the
preferred metric, as it requires less overall data from the
procurement phase (lowering the risk of detection) and also
improves success chances of the heuristic process (dealing
with potential errors in the signal processing phase).
In a more realistic scenario we want to model the lattice
parameters to succeed in a “reasonable” amount of time.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:44:40 UTC from IEEE Xplore.  Restrictions apply. 
This deﬁnition is not rigorous and largely depends on the
capabilities of a dedicated attacker: in this academic contest,
constrained by the grid computing resources available to us,
we deﬁne a lattice instance as successful if it correctly retrieves
the secret key in under 4 hours when running on a single
2.50 GHz Xeon E5-2680 core (as done in Table 3 of [48]).
We believe this deﬁnition is very conservative with respect to
the potential computational resources of a nation-state level
adversary or the availability and costs of dynamically scalable
computing cloud services for individuals and organizations.
We modeled our preliminary experiments using random
nonces, biased to have a trailing sequence of zero bits: this
is equivalent to assuming error-free traces from the signal
processing phase. We ran two sets of experiments, one with
a bias mask of 0x3, i.e., with at least two trailing zero bits
(using the notation from [41], z ≥ 2 and l ≥ 3), and the other
with a bias mask of 0x1, i.e., with at least one trailing zero
bit (z ≥ 1 and l ≥ 2).
To determine the optimal d for each bias case, we ran 10000
instances of the lattice algorithm against the two sets of mod-
eled perfect traces and measured the corresponding amount
of known nonce bits (Figure 10), the number of iterations for
successful instances (Figure 11), the overall execution time for
successful instances (Figure 12), and the success probability
(Figure 9). The results indicate d = 450 is optimal for the
0x1 biased ideal traces, with success probability exceeding
90% coupled with a small number of iterations as well as
overall execution time. Analogously, d = 170 is optimal for
the 0x3 bias case.
Experimental parameters with real traces: Real traces come
with errors, which lattices have no recourse to compensate
for. The traditional solution is oversampling, using a larger
set of t traces (with some amount e of traces with errors),
and running in parallel a number (i) of lattice instances, each
picking a different subset of size d from the larger set. Picking
the subsets uniformly random, the probability for any subset
to be error-free is:
Pr(No error in a random subset of size d) =
(cid:2)
(cid:2)
(cid:3)
(cid:3)
t−e
d
t
d
For typical values of {t, e, d}, the above probability is small
and not immediately practical. But given the current capa-
bilities for parallelizing workloads on computing clusters,
repeatedly picking different subsets compensates:
Pr( ≥ 1 error-free subset over i inst.) = 1 −
(cid:2)
(cid:2)
(cid:3)
(cid:3)
(cid:5)i
1 −
(cid:4)
t−e
d
t
d
(1)
Proﬁling the signal processing phase results, we determined
to utilize thresholding to select traces belonging to the “AD”,
“ADD”, “ADDD” and “ADDDD” distributions of Figure 7.
In our setup, other traces are either useless for our lattice
model or have too low accuracy. To ensure accuracy, we
determined very narrow boundaries around the distributions
to limit overlaps at the cost of very strict ﬁltering. Out of
the original 10000 captures, the ﬁltering process selects a set
of 1959 traces with a 0x1 bias mask (i.e. nonces are even)
including e = 34 (1.74%) errors. Combining this with d = 450
from our empirical lattice data, (1) leads us to i ≥ 36000
instances required to achieve a probability ≥ 99% of picking
at least one subset without errors. This number of instances
is beyond the parallel computational resources available to us,
hence we move to the remaining case.
Filtering out also the 1060 traces categorized as “AD”
delivers a total of 899 traces with a 0x3 bias mask (i.e.
k = 0 mod 4), including e = 14 (1.56%) errors. Combining
this with d = 170 for the higher nonce bias and substituting
in (1) leads us to i ≥ 200 instances to achieve a probability
≥ 99.99% of picking at least one subset without errors.
When using the actual attack data we noticed that while
our ﬁltering process increases accuracy, it has the side-effect
of straying from the statistics determined in ideal conditions.
We speculate this is due to ﬁltering out longer trailing zero bits
(low accuracy) decreasing the average amount of known nonce
bits per signature, resulting in wider lattice dimensions with
lower than expected useful information. This negatively affects
the overall success rate and the amount of required iterations
for a successful instance. We experimentally determined that
when selecting only nonces with a bias mask between 0x3
and 0xF, d = 290 compensates with a success rate (for an
error-free subset) of 90.72%. Using these values in (1) leads
us to i = 2000 instances to achieve a 99.97% probability of
picking at least one subset without errors—well within the
computing resources available to us.
Finally, running the entire process on the real data obtained
from the signal processing phase on the original 10000 cap-
tures, using parameters t = 899, e = 14, and d = 290 over
i = 2000 instances running in parallel on the described cluster
resulted in 11 instances that successfully retrieved the secret
key, the fastest of which terminated in 259 seconds after only
two BKZ reduction iterations.
E. SGX
Intel Software Guard Extensions (SGX)4 is a microarchi-
tecture extension present in modern Intel processors. SGX
aims at protecting software modules by providing integrity
and conﬁdentiality to their code and memory contents. In
SGX terminology, an SGX enclave is a software module that
enjoys said protections. SGX was designed to defend processes
against tampering and inspection from OS-privileged adver-
saries, providing strong isolation between enclave memory
regions and the outer world. Despite these strong protections,
side-channel attacks are still considered a major threat for
enclaves, as SGX by itself does not protect against them [49].
In this regard, as the SGX threat model considers an OS-level
adversary, it is even possible to mount more powerful side-
channel attacks against enclaves where the measurement noise
can be reduced considerably [50–52].
it
is interesting to know
which unprivileged side-channel techniques are a threat to
From a practical perspective,
4https://software.intel.com/en-us/sgx
(cid:25)(cid:25)(cid:17)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:44:40 UTC from IEEE Xplore.  Restrictions apply. 
SGX enclaves. Regarding cache attacks, FLUSH+RELOAD and
FLUSH+FLUSH do not apply in the unprivileged scenario since
they require shared memory with the SGX enclave, which does
not share its memory [15, 49]. However, researchers use other
attack techniques against SGX, such as L1-based PRIME+
PROBE attacks [52], and false dependency attacks [21]. It
is worth mentioning that these methods assume an attacker
with privileged access. However, we strongly believe these
attacks would succeed without this assumption at the cost of
capturing traces with a higher signal-to-noise ratio. Finally,
TLBLEED [4] could be a potential successful attack technique
against SGX, yet the authors leave it for future work.
The rest of this section analyzes PORTSMASH impact on
SGX enclave security. Our ﬁrst (strong) hypothesis is a PORT
SMASH attack can target SGX enclaves transparently. The
rationale relies on the difference between PORTSMASH root