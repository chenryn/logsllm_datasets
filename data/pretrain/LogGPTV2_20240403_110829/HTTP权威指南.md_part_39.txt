225 机器人实现者要支持Host首部。随着虚拟主机（参见第5章）的流行，请求中不
注9： 一种RFC 822 E-mail地址格式。
注10： 3.5.2节列出了所有Accept相关的首部；机器人可能会发现，如果它们对特定版本感兴趣的话，发
送Accept-Charset之类的首部是很有帮助的。
注11： 有些站点管理者会尝试着记录机器人是如何找到指向其站点内容的链接的，对这些人来说，这个首
部非常有用。
236 ｜ 第9章
包含Host首部的话，可能会使机器人将错误的内容与一个特定的URL关联起来。
因此，HTTP/1.1要求使用Host首部。
在默认情况下，大多数服务器都被配置为提供一个特定的站点。因此，不包含Host
首部的爬虫向提供两个站点的服务器发起请求时，就像图9-5中的站点一样（www.
joes-hardware.com和www.foo.com），假设默认情况下服务器被配置为提供www.
joes-hardware.com站点（且不需要Host首部），那么，若请求www.foo.com上的
某个页面，爬虫实际获取的就是Joe的五金商店的站点上的内容。更糟糕的是，爬
虫会认为来自Joe的五金站点上的那些内容是来自www.foo.com的。如果带有相对
立的政治色彩或其他观点的两个站点是由同一台服务器提供的，你肯定能想象到会
有更不幸的局面出现。
机器人试图从www.foo.com上请
求/index.html，但它没有包含Host首部
服务器是配置为提供这两个站点的，但默
Web机器人客户端
认情况下提供的是Joe的五金商店的站点。
请求报文
GET /index.html HTTP/1.0
User-agent: ShopBot 1.0
www.joes-hardware.com
响应报文 www.foo.com
HTTP/1.0 200 OK
[...]
Welcome to Joe's Hardware!
[...]
图9-5 发送请求时没有携带Host首部，虚拟docroot会引发问题的例子
9.2.3 条件请求
鉴于这些机器人的努力程度，尽量减少机器人所要获取内容的数量通常是很有意义
的。对因特网搜索引擎机器人来说，需要下载的潜在页面有数十亿，所以，只在内
容发生变化时才重新获取内容是很有意义的。
有些机器人实现了条件HTTP请求，12它们会对时间戳或实体标签进行比较，看看
它们最近获取的版本是否已经升级了。这与HTTP缓存查看已获取资源的本地副
本是否有效的方法非常相似。更多与缓存对资源本地副本的验证有关的信息请参见
第7章。 226
注12： 3.5.2节给出了一个机器人可以实现的条件首部的完整列表。
Web机器人 ｜ 237
9.2.4 对响应的处理
很多机器人的兴趣主要在于用简单的GET方法来获取所请求的内容，所以，一般不
会在处理响应的方式上花费太多时间。但是，使用了某些HTTP特性（比如条件请
求）的机器人，以及那些想要更好地探索服务器，并与服务器进行交互的机器人则
要能够对各种不同类型的HTTP响应进行处理。
1. 状态码
总之，机器人至少应该能够处理一些常见的，以及预期的状态码。所有机器人都应
该理解200 OK和404 Not Found这样的状态码。它们还应该能够根据响应的一般类
别对它并不十分理解的状态码进行处理。第3章的表3-2给出了不同状态码的分类
及其含义。
有些服务器并不总能返回适当的错误代码，认识到这一点是很重要的。有些服务器
甚至会将HTTP状态码200 OK与描述错误状态的报文主体文本一同返回！很难对
此做些什么——只是实现者应该要了解这些情况。
2. 实体
除了HTTP首部所嵌的信息之外，机器人也会在实体中查找信息。HTML元标签，13
比如元标签http-equiv，就是内容编写者用于嵌入资源附加信息的一种方式。
服务器可能会为它所处理的内容提供一些首部，标签http-equiv为内容编写者提
供了一种覆盖这些首部的方式：
这个标签会指示接收者处理这个文档时，要当作其HTTP响应首部中有一个值为
1,URL=index.html的Refresh HTTP首部。14
有些服务器实际上会在发送HTML页面之前先对其内容进行解析，并将http-equiv
指令作为首部包含进去；有些服务器则不会。机器人实现者可能会去扫描HTML文
227 档的HEAD组件，以查找http-equiv信息。15
注13： 9.4.7节列出了一些附加的元指令，站点管理员和内容编写者可以通过这些元指令来控制机器人的行为，
以及这些机器人对已获取文档所执行的操作。
注14： 有时会将Refresh HTTP首部作为将用户（或者在这种情况下，就是将机器人）从一个页面重定向
到另一个页面的手段。
注15： 根据HTML的规范，元标签一定要出现在HTML文档的HEAD部分。但并不是所有的HTML文档
都会遵循规范，因此，它们有时也会出现在HTML文档的其他区域中。
238 ｜ 第9章
9.2.5 User-Agent导向
Web管理者应该记住，会有很多的机器人来访问它们的站点，因此要做好接收机
器人请求的准备。很多站点会为不同的用户代理进行内容优化，并尝试着对浏览器
类型进行检测，以确保能够支持各种站点特性。这样的话，当实际的HTTP客户端
根本不是浏览器，而是机器人的时候，站点为机器人提供的就会是出错页面而不是
页面内容了。在某些搜索引擎上执行文本搜索，搜索短语“your browser does not
support frames”（你的浏览器不支持框架），会生成一个包含那条短语的出错页面
列表。
站点管理者应该设计一个处理机器人请求的策略。比如，它们可以为所有其他特性
不太丰富的浏览器和机器人开发一些页面，而不是将其内容限定在特定浏览器所支
持的范围。至少，管理者应该知道机器人是会访问其站点的，不应该在机器人访问
时感到猝不及防。16
9.3 行为不当的机器人
不守规矩的机器人会造成很多严重问题。这里列出了一些机器人可能会犯的错误，
及其恶劣行为所带来的后果。
• 失控机器人
机器人发起HTTP请求的速度要比在Web上冲浪的人类快得多，它们通常都运
行在具有快速网络链路的高速计算机上。如果机器人存在编程逻辑错误，或者陷
入了环路之中，就可能会向Web服务器发出大量的负载——很可能会使服务器
过载，并拒绝为任何其他人提供服务。所有的机器人编写者都必须特别小心地设
计一些保护措施，以避免失控机器人带来的危害。
• 失效的URL
有些机器人会去访问URL列表。这些列表可能很老了。如果一个Web站点对其
内容进行了大量的修改，机器人可能会对大量不存在的URL发起请求。这会激
怒某些Web站点的管理员，他们不喜欢他们的错误日志中充满了对不存在文档
的访问请求，也不希望提供出错页面的开销降低其Web服务器的处理能力。
• 很长的错误URL
由于环路和编程错误的存在，机器人可能会向Web站点请求一些很大的、无意
义的URL。如果URL足够长的话，就会降低Web服务器的性能，使Web服务
注16： 如果某站点上有一些不应该让机器人访问的内容，站点管理员该如何控制机器人在其站点上的行为
呢？ 9.4节给出了相关的信息。
Web机器人 ｜ 239
228 器的访问日志杂乱不堪，甚至会使一些比较脆弱的Web服务器崩溃。
• 爱打听的机器人
有些机器人可能会得到一些指向私有数据的URL，这样，通过因特网搜索引擎
和其他应用程序就可以很方便地访问这些数据了。如果数据的所有者没有主动宣
传这些Web页面，那么在最好的情况下，他只是会认为机器人的发布行为惹人
讨厌，而在最坏的情况下，则会认为这种行为是对隐私的侵犯。17
通常，发生这种情况是由于机器人所跟踪的、指向“私有”内容的超链已经存在
了（也就是说，这些内容并不像其所有者认为的那么隐密，或者其所有者忘记删
除先前存在的超链了）。偶尔也会因为机器人非常热衷于寻找某站点上的文档而
出现这种情况，很可能就是在没有显式超链的情况下去获取某个目录的内容造
成的。
从Web上获取大量数据的机器人的实现者们应该清楚，他们的机器人很可能会
在某些地方获得敏感的数据——站点的实现者不希望通过因特网能够访问到这些
数据。这些敏感数据可能包含密码文件，甚至是信用卡信息。很显然，一旦被指
出，就应该有某种机制可以将这些数据丢弃（并从所有搜索索引或归档文件中将
其删除），这是非常重要的。现在已知一些恶意使用搜索引擎和归档的用户会利
用大型Web爬虫来查找内容——有些搜索引擎，比如Google，18实际上会对它
们爬行过的页面进行归档，这样，即使内容被删除了，在一段时间内还是可以找
到并访问它。
• 动态网关访问
机器人并不总是知道它们访问的是什么内容。机器人可能会获取一个内容来自
网关应用程序的URL。在这种情况下，获取的数据可能会有特殊的目的，计算
的开销可能很高。很多Web站点管理员并不喜欢那些去请求网关文档的幼稚机
器人。
9.4 拒绝机器人访问
机器人社团能够理解机器人访问Web站点时可能引发的问题。1994年，人们提出
了一项简单的自愿约束技术，可以将机器人阻挡在不适合它的地方之外，并为网站
注17： 通常，如果某资源可以通过公共因特网获取的话，它很可能会在某处被引用。由于因特网上链路网
的存在，很少有资源是真正私有的。
注18： 参见http://www.google.com上的搜索结果。已缓存链接就是Google爬虫解析并索引过的页面的副本，
大多数搜索结果中都会有已缓存链接。
240 ｜ 第9章
管理员提供了一种能够更好地控制机器人行为的机制。这个标准被称为“拒绝机器
人访问标准”，但通常只是根据存储访问控制信息的文件而将其称为robots.txt。
robots.txt的思想很简单。所有Web服务器都可以在服务器的文档根目录中提供一
个可选的、名为robots.txt的文件。这个文件包含的信息说明了机器人可以访问服务
器的哪些部分。如果机器人遵循这个自愿约束标准，它会在访问那个站点的所有其 229
他资源之前，从Web站点请求robots.txt文件。例如，图9-6中的机器人想要从Joe
的五金商店下载http://www.joes-hardware.com/specials/acetylene-torches.html。但在
机器人去请求这个页面之前，要先去查看robots.txt文件，看看它是否有获取这个页
面的权限。在这个例子中，robots.txt文件并没有拦截机器人，因此机器人获取了这
个页面。
Web机器人客户端 www.joes-hardware.com
GET /robots.txt
机器人对robots.txt文件进行
解析，判断是否允许它访问
文件acetylene-torches.html
允许访问，继续发送请求 GET /specials/acetylene-torches.html
图9-6 在爬行目标文件之前，先获取robots.txt，验证是否可以进行访问
9.4.1 拒绝机器人访问标准
拒绝机器人访问标准是一个临时标准。编写本书的时候还没有官方标准机构承认这
个标准，不同的厂商实现了这个标准的不同子集。但是，具备一些对机器人访问
Web站点的管理能力，即使并不完美，也总比一点儿都没有要好，而且大部分主要
的生产厂商和搜索引擎爬虫都支持这个拒绝访问标准。
尽管没有很好地定义版本的名称，但拒绝机器人访问标准是有三个版本的。我们采
用了表9-2列出的版本编号。
Web机器人 ｜ 241
表9-2 拒绝机器人访问标准的版本
版 本 标题及描述 日 期
0.0 拒绝机器人标准——Martijn Koster提出的带有Disallow（不允 1994年6月
许）指令的原始robots.txt机制
1.0 控制Web机器人的方法——Martijn Koster提供了额外支持Allow 1996年11月
（允许）的IETF草案
2.0 拒绝机器人访问的扩展标准——Sean Conner提出的扩展标准，包 1996年11月
230 括了正则表达式和定时信息；没有得到广泛的支持
现在大多数机器人采用的都是标准v0.0或v1.0。版本v2.0要复杂得多，没有得到
广泛的应用。可能永远也不会得到广泛应用。这里我们重点介绍v1.0标准，因为它
的应用很广泛，而且与v0.0完全兼容。
9.4.2 Web站点和robots.txt文件
如果一个Web站点有robots.txt文件，那么在访问这个Web站点上的任意URL之
前，机器人都必须获取它并对其进行处理。19由主机名和端口号定义的整个Web站
点上仅有一个robots.txt资源。如果这个站点是虚拟主机，每个虚拟的docroot都可
以有一个不同的robots.txt文件，像所有其他文件一样。
通常不能在Web站点上单独的子目录中安装“本地”robots.txt文件。网管要负责
创建一个聚合型robots.txt文件，用以描述Web站点上所有内容的拒绝访问规则。
1. 获取robots.txt
机器人会用HTTP的GET方法来获取robots.txt资源，就像获取Web服务器上所有
其他资源一样。如果有robots.txt文件的话，服务器会将其放在一个text/plain主体
中返回。如果服务器以404 Not Found HTTP状态码进行响应，机器人就可以认为这
个服务器上没有机器人访问限制，它可以请求任意的文件。
机器人应该在From首部和User-Agent首部中传输标识信息，以帮助站点管理者
对机器人的访问进行跟踪，并在站点管理者要查询，或投诉的机器人事件中提供一
些联系信息。下面是一个来自商业Web机器人的HTTP爬虫请求实例：
GET /robots.txt HTTP/1.0
Host: www.joes-hardware.com
User-Agent: Slurp/2.0
Date: Wed Oct 3 20:22:48 EST 2001
注19： 尽管我们说的是robots.txt文件，但robots.txt资源并不一定要严格地位于文件系统中。比如，可以
由一个网关应用程序动态地生成这个robots.txt资源。
242 ｜ 第9章
2. 响应码
很多Web站点都没有robots.txt资源，但机器人并不知道这一点。它必须尝试着从
每个站点上获取robots.txt资源。机器人会根据对robots.txt检索的结果采取不同的
行动。
• 如果服务器以一个成功状态（HTTP状态码2XX）为响应，机器人就必须对内容
进行解析，并使用排斥规则从那个站点上获取内容。
• 如果服务器响应说明资源并不存在（HTTP状态码404），机器人就可以认为服务
器没有激活任何排斥规则，对此站点的访问不受robots.txt的限制。 231