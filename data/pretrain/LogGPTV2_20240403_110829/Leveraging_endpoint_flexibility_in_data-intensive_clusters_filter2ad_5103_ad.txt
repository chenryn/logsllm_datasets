improvements in write times across bins are not correlated with the
characteristics of jobs in those bins; blocks in these experiments
were placed without considering any job-speciﬁc information. MR
237Figure 6: [EC2] Improvements in average job completion times and time
spent in distributed writes.
(a) Uplinks
(b) Downlinks
Figure 8: [Simulation] Network awareness decreases load imbalance across
racks in the Facebook trace.
Figure 7:
tributed writes.
[EC2] Improvements in jobs categorized by amounts of dis-
(a) Uplinks
(b) Downlinks
jobs in bin-1 to bin-4 have lower overall improvements depending
on their computation heaviness and because shufﬂe saw negligi-
ble change in these set of experiments. The average end-to-end
completion times of jobs improved by 1:19(cid:2) and block write time
across all jobs improved by 1:3(cid:2).
Being an online solution, Sinbad does not always perform well.
We found that 15% of the jobs either had no improvements or ex-
perienced slightly higher overall completion times. In contrast, the
top 15% jobs improved by at least 1:4(cid:2). Part of Sinbad’s inefﬁ-
ciency can also be attributed to its limited view of the network in
the virtualized EC2 environment.
A breakdown of improvements in jobs by their write sizes (Fig-
ure 7) does not show any signiﬁcant difference between categories.
This is because we optimize for blocks in the experiments using
q = 0, which does not differentiate between different amounts each
task or job writes.
Trace-Driven Simulation Unlike the EC2 deployment, we pre-
calculated the background trafﬁc due to shufﬂe in the simulation
and assumed it to be unaffected by placement decisions. Hence, we
do not distinguish between jobs in different bins in the simulation.
We used OP T
in the simulations with q = 10s.
′
We found that 46% of individual block replication requests im-
proved, 47% remained the same, and 7% became slower. The av-
erage improvement across all requests was 1:58(cid:2), and 16% com-
pleted at least 2(cid:2) faster. Average completion times of writers and
communication times of jobs (weighted by their sizes) improved by
1:39(cid:2) and 1:18(cid:2), respectively.
How Far are We From the Optimal? While it was not possible to
exhaustively enumerate all possible orderings of block requests to
ﬁnd the optimal, we tried to ﬁnd an upper bound on improvements.
First, we increased q to more than ten (up to q = 100s), but it did
not result in signiﬁcant increase in improvements.
Next, we tried a drastic simpliﬁcation to ﬁnd a loose upper
bound. Keeping q = 10s, we assumed that there are no bottlenecks
Figure 9: [EC2] Network awareness decreases load imbalance across ma-
chines.
′
at sources; i.e., sources can also be placed at suitable locations be-
fore writes start. Note that unlike destination placement, this is
hard to achieve in practice because it is hard to predict when a task
might start writing. With this simpliﬁcation, we found that 58%
requests improved, 39% remained the same, and 3% were worse
off using Sinbad. The average improvements in block write time,
task completion time, and weighted job communication time were
1:89(cid:2), 1:59(cid:2), and 1:36(cid:2), respectively.
On Batched Decisions We did not observe signiﬁcant improve-
ment for q > 0 using OP T
. This is because the effectiveness
of batching depends on the duration of the window to accumulate
requests, which in turn depends on the request arrival rate and the
average time to serve each request. Batching can only be effective
when the arrival rate and/or the service time are substantial (i.e.,
larger blocks).
Impact on Network Imbalance
In the Facebook trace, on average, 4 large block requests arrive
every second. With 50 MBps disk write speed, writing a 256 MB
block would take 5 seconds. However, the number of suitable bot-
tleneck links is several times larger than the arrival rate. Conse-
quently, batching for a second will not result in any improvement,
but it will increase the average write time of those 4 blocks by 20%.
7.3
We found that Sinbad decreases the network imbalance in both sim-
ulation and deployment. Figure 8 plots the change in imbalance of
load in the bottleneck links in the Facebook cluster. We see that
Sinbad signiﬁcantly decreases load imbalance (Cv) across up and
downlinks of individual racks – decrease in median Cv being 0:35
and 0:33, respectively. We observed little improvement for very
low imbalance (i.e., Cv close to zero), because most rack-to-core
links had almost equal utilizations.
Figure 9 presents the imbalance of load in the edge links con-
1.11!1.08!1.08!1.18!1.26!1.19!1.45!1.27!1.19!1.24!1.28!1.29!1!1.1!1.2!1.3!1.4!1.5!1.6!Bin 1!Bin 2!Bin 3!Bin 4!Bin 5!ALL!Factor of Improvement!End-to-End!WriteTime!1.07!1.11!1.08!1.13!1.22!1.20!1.31!1.15!1.44!1.23!0!0.5!1!1.5!2![0, 10)![10, 100)![100, 1E3)![1E3, 1E4)![1E4, 1E5)!Factor of Improvement!Job Write Size (MB)!End-to-End!WriteTime!0!0.25!0.5!0.75!1!0!1!2!3!4!Fraction of Time!Coeff. of Var. of Load!Across Rack-to-Core Links!Network-Aware!Default!0!0.25!0.5!0.75!1!0!1!2!3!4!Fraction of Time!Coeff. of Var. of Load!Across Core-to-Rack Links!Network-Aware!Default!0!0.25!0.5!0.75!1!0!1!2!3!4!Fraction of Time!Coeff. of Var. of Load!Across Rack-to-Core Links!Network-Aware!Default!0!0.25!0.5!0.75!1!0!1!2!3!4!Fraction of Time!Coeff. of Var. of Load!Across Core-to-Rack Links!Network-Aware!Default!0!0.25!0.5!0.75!1!0!1!2!3!4!Fraction of Time!Coeff. of Var. of Load!Across Host-to-Rack Links!Network-Aware!Default!0!0.25!0.5!0.75!1!0!1!2!3!4!Fraction of Time!Coeff. of Var. of Load!Across Rack-to-Host Links!Network-Aware!Default!0!0.25!0.5!0.75!1!0!1!2!3!4!Fraction of Time!Coeff. of Var. of Load!Across Host-to-Rack Links!Network-Aware!Default!0!0.25!0.5!0.75!1!0!1!2!3!4!Fraction of Time!Coeff. of Var. of Load!Across Rack-to-Host Links!Network-Aware!Default!238(a) Distribution of blocks
(b) Distribution of bytes
Figure 10: [Simulation] PDFs of blocks and bytes across racks. Network-
aware placement decisions do not signiﬁcantly impact data distribution.
necting individual machines to the EC2 network. We notice that
irrespective of the placement policy, the average values of imbal-
ance (Cv) are higher than that observed in our simulation. This is
because we are calculating network utilization and corresponding
imbalance at individual machines, instead of aggregating over 20
machines in each rack. We ﬁnd that Sinbad decreases imbalance
across edge links as well – decrease in median values of Cv for up
and downlinks are 0:34 and 0:46, respectively. For high Cv values
(Cv (cid:21) 2:5), Sinbad experienced more variations; we found most of
these cases coinciding with low overall utilization periods, causing
little impact on performance.
7.4 Impact on Future Jobs (Disk Usage)
Since replica placement decisions impact future jobs,7 it is impor-
tant to know whether Sinbad creates signiﬁcant imbalance in terms
of disk usage and the total number of blocks placed across bottle-
neck links.
We found that after storing 9:24 TB data (including replicas) in
an hour-long EC2 experiment, the standard deviation of disk usage
across 100 machines was 15:8 GB using Sinbad. For the default
policy it was 6:7 GB. In both cases, the imbalance was much less
than 10% of the disk capacity, which would have triggered an au-
tomated block rebalancing.
Simulations provided a similar result. Figure 10 presents the
probability density functions of block and byte distributions across
150 racks at the end of a day-long simulation. We observe that
Sinbad performs almost as good as the default uniform placement
policy in terms of data distribution.
Sinbad performs well because it is always reacting to the im-
balance in the background trafﬁc, which is uniformly distributed
across all bottleneck links in the long run (§4.3). Because Sinbad
does not introduce noticeable storage imbalance, it is not expected
to adversely affect the data locality of future jobs. Reacting to net-
work imbalance is not always perfect, however. We see in Figure 10
that some locations (less than 4%) received disproportionately large
amounts of data. This is because these racks were down8 through-
out most of the trace.
7.5
To understand the impact of network contention, we compared Sin-
bad with the default placement policy by changing the arrival rate
of jobs. In this case, we used a shorter trace with the same job mix,
scaled down jobs as before, but we spread the jobs apart over time.
Impact of Cluster/Network Load
7Placing too many blocks in some machines and too few in others
can decrease data locality.
8Likely reasons for downtimes include failure or upgrade, but it
can be due to any number of reasons.
Figure 11: [EC2] Improvements in average completion times with different
arrival rates of jobs.
Impact on In-Memory Storage Systems
We notice in Figure 11 that Sinbad performed well for different
levels of network contention. However, changing the arrival rate by
a factor of N does not exactly change the load by N(cid:2); it depends
on many factors including the number of tasks, the input-output ra-
tio, and the actual amount of data in the scaled-down scenario. By
making jobs arrive 2(cid:2) faster, we saw around 1:4(cid:2) slower absolute
completion times for both policies (not shown in Figure 11); this
suggests that the network load indeed increased by some amount.
We observe that Sinbad’s improvements decreased as well: as the
network becomes more and more congested, the probability of ﬁnd-
ing a less loaded destination decreases.
Changing the arrival rate to the half of the original rate (i.e., 0:5(cid:2)
arrival rate) decreased the overall time by a negligible factor. This
suggests that either the overlap between tasks did not change sig-
niﬁcantly, or the network was already not the main bottleneck of
the cluster. For the write-only jobs, Sinbad’s improvements did not
change either. However, for the MR jobs, shufﬂe time improved at
the expense of corresponding writes.
7.6
So far we have considered Sinbad’s performance only on disk-
based CFSes. However,
improvements in these systems are
bounded by the write throughput of disks and depend on how
CFSes schedule writes across multiple disks. To avoid disk con-
straints, several in-memory storage systems and object caches have
already been proposed [12]. To understand Sinbad’s potentials
without disk constraints, we implemented an in-memory object
cache similar to PACMan [12]. Blocks are replicated and stored
in the memory of different machines, and they are evicted (in FIFO
order) when caches become full. Note that the network is the only
bottleneck in this setup. We used the same settings from §7.2.
From EC2 experiments, we found that jobs across all bins gained
boosts of 1:44(cid:2) and 1:33(cid:2) in their average write and end-to-end
completion times, respectively. The average block write time im-
proved by 1:6(cid:2) using Sinbad in the in-memory CFS.
A trace-driven simulation without disk constraints showed a
1:79(cid:2) average improvement across all block write requests. We
found that 64% of individual block replication requests improved,
29% remained the same, and 7% became slower. Average comple-
tion times of writers and communication times of jobs (weighted
by their sizes) improved by 1:4(cid:2) and 1:15(cid:2), respectively.
8 Discussion
Network-Assisted Measurements In the virtualized EC2 envi-
ronment, Sinbad slaves cannot observe anyone else’s trafﬁc in the
network including that of collocated VMs. This limits Sinbad’s ef-
fectiveness. When deployed in private clusters, with proper instru-
mentation (e.g., queryable SDN switches), Sinbad would observe
0.5!0.6!0.7!0.8!0.9!1!1!26!51!76!101!126!151!Probability Density of Blocks!Rank of Racks!NetworkAware!Default!0.5!0.6!0.7!0.8!0.9!1!1!26!51!76!101!126!151!Probability Density of Bytes!Rank of Racks!NetworkAware!Default!0.5!0.6!0.7!0.8!0.9!1!1!26!51!76!101!126!151!Probability Density of Blocks!Rank of Racks!NetworkAware!Default!0.5!0.6!0.7!0.8!0.9!1!1!26!51!76!101!126!151!Probability Density of Bytes!Rank of Racks!NetworkAware!Default!1.08!1.45!1.14!1.13!1.07!1.4!1.27!1.27!1.1!1.25!1.26!1.28!1!1.1!1.2!1.3!1.4!1.5!1.6!1.7!End-to-End!WriteTime!End-to-End!WriteTime!Bins 1-4!Bin 5!Factor of Improvement!2X Arrival!Original Rate!0.5X Arrival!239the impact of all incoming and outgoing trafﬁc. We expect that
correcting for more imbalances will increase Sinbad’s gains.
Sinbad is also sensitive to delayed and missing heartbeats, which
can introduce inaccuracy in usage estimations. By placing the
heartbeats in a higher priority queue [24], the network can ensure
their timely arrival.
Flexible Source Placement Achieving network awareness
throughout the entire write pipeline requires modifying the task
scheduler to carefully place the writers. However, a task must
ﬁnish everything else – e.g., computation for a preprocessing task
and shufﬂe for a reducer – before it starts writing. Estimating
the start time or the size of a write operation is nontrivial. Even
though all the blocks from a writer can have different destinations,
they must leave from the same source. This further decreases the
ﬂexibility in source placement.
Optimizing Parallel Writers The completion time of a data-
parallel job depends on the task that takes the longest to ﬁnish
[20, 21]. Hence, to directly minimize the job completion time, one
must minimize the makespan of all the concurrent writers of that
job. This, however, calls for a cross-layer design with additional
job-speciﬁc details in the placement algorithm.
9 Related Work
Datacenter Trafﬁc Management Hedera [8] manages individual
ﬂows using a centralized scheduler to increase network through-
put by choosing among multiple paths in a fat-tree topology to the
same pre-destined endpoints. MicroTE [13] adapts to trafﬁc vari-
ations by leveraging short-term predictability of the trafﬁc matrix.
Orchestra [20] is limited to data-intensive applications and does not
consider the network impact of cluster storage. All of the above
manage ﬂows with already-ﬁxed endpoints and do not leverage the
ﬂexibility in endpoint placement available during replication.
Endpoint-Flexible Transfers Overlay anycast comes the closest
to Sinbad in exploiting ﬂexibility in choosing endpoints [18, 25].
However, anycast has typically been used in lightweight request-
response trafﬁc (e.g., DNS requests); on the contrary, we use end-
point ﬂexibility in a throughput-sensitive context for replica place-
ment. Our constraints – number of replicas/fault domains and over-
all storage balancing – are different as well.
Full Bisection Bandwidth Networks Recent datacenter network
architectures [28–30, 38] aim for full bisection bandwidth for bet-
ter performance. This, however, does not imply inﬁnite bisection
bandwidth. In presence of (skewed) data-intensive communication,
some links often become more congested than others [35]. Selec-
tion of appropriate destinations during replica placement is neces-
sary to pick less congested destinations.
Distributed File Systems Distributed ﬁle systems [15, 19, 27] fo-
cus primarily on the fault-tolerance and consistency of stored data
without considering the network impact. Replica placement poli-
cies emphasize on increasing availability in presence of network
failures. We focus on performance improvement through network-
balanced replica placement without changing any of the fault-
tolerance, availability, or consistency guarantees.
Unlike traditional designs, Flat Datacenter Storage (FDS) keeps
computation and storage separate: data is always read over the net-
work [39]. FDS does not have a centralized master and uses ran-
domization for load balancing. Network-awareness can potentially
improve its performance as well.
Erasure-Coded Storage Systems To mitigate the storage crunch
and to achieve better reliability, erasure coding of stored data is be-
coming commonplace [33,40]. However, coding typically happens
post facto, meaning each block of data is three-way replicated ﬁrst,
then coded lazily, and replicas are deleted after coding ﬁnishes.
This has two implications. First, network awareness accelerates
the end-to-end coding process. Second, in presence of failures and
following rebuilding storms, the network experiences even more
contention; this further strengthens the case for Sinbad.
Data Locality Disk locality [22] has received more attention than
most concepts in data-intensive computing, in designing both dis-
tributed ﬁle systems and schedulers for data-intensive applications
[10, 34, 44]. Data locality, however, decreases network usage only
during reads; it cannot obviate network consumption due to dis-
tributed writes. Sinbad has little impact on data locality, because
it keeps the storage balanced. Moreover, Sinbad can help systems
like Scarlett [11] that try to decrease read contention by increasing
the number of replicas. Recent calls for memory locality [12] will
make network-aware replication even more relevant by increasing
network contention during off-rack reads.
10 Conclusion
We have identiﬁed the replication trafﬁc from writes to cluster ﬁle
systems (CFSes) as one of the major sources of network commu-
nication in data-intensive clusters. By leveraging the fact that a
CFS only requires placing replicas in any subset of feasible ma-
chines, we have designed and evaluated Sinbad, a system that iden-
tiﬁes network imbalance through periodic measurements and ex-
ploits the ﬂexibility in endpoint placement to navigate around con-
gested links. Network-balanced replica placement improves the av-
erage block write time by 1:3(cid:2) and the average completion time of
data-intensive jobs by up to 1:26(cid:2) in EC2 experiments. Because
network hotspots show short-term stability but they are uniformly
distributed in the long term, storage remains balanced. We have
also shown that Sinbad’s improvements are close to that of the op-
timal, and they will increase if network imbalance increases.
Acknowledgments
We thank Yuan Zhong, Matei Zaharia, Gautam Kumar, Dave Maltz,
Ali Ghodsi, Ganesh Ananthanarayanan, Raj Jain, the AMPLab
members, our shepherd John Byers, and the anonymous review-
ers for useful feedback. This research is supported in part by NSF
CISE Expeditions award CCF-1139158 and DARPA XData Award
FA8750-12-2-0331, and gifts from Amazon Web Services, Google,
SAP, Blue Goji, Cisco, Clearstory Data, Cloudera, Ericsson, Face-
book, General Electric, Hortonworks, Huawei, Intel, Microsoft,
NetApp, Oracle, Quanta, Samsung, Splunk, VMware, Yahoo!, and