Figure 2: The high-level concept of the PCF design. It
is not necessary to unroll loops at compile time, even to
perform optimizations on the circuit. Instead, loops can
be evaluated at runtime, with gates being computed on-
the-ﬂy, and loop indices being updated locally by each
party. Wire values are stored in a table, with each gate
specifying which two table entries should be used as in-
puts and where the output should be written; previous
wire values in the table can be overwritten during this
process, if they are no longer needed.
is a challenge, as even very high-end machines may not
have enough RAM for relatively simple functions.
There have been some approaches to addressing this
scalability problem presented in previous work. The
KSS12 system reduced the RAM required for protocol
executions by assigning each gate’s output wire a refer-
ence count, allowing the memory used for a wire value to
be deallocated once the gate is no longer needed. How-
ever, the compiler bottleneck was not solved in KSS12,
as even computing the reference count required memory
proportional to the size of the circuit. Even with the engi-
neering improvements presented by Kreuter, shelat, and
Shen, the KSS12 compiler was unable to compile circuits
with more than a few billion gates, and required several
days to compile their largest test cases [18].
The PAL system [23] also addresses memory require-
ments, by adding control structures to the circuit descrip-
tion, allowing parts of the description to be re-used. In
the original presentation of PAL, however, a large circuit
ﬁle would still be emitted in the Fairplay format when
the secure protocol was run. An extension of this work
presented by Mood [22] allowed the PAL description to
be used directly at runtime, but this work sacriﬁced the
ability to optimize circuits automatically.
Our system builds upon the PAL and KSS12 systems
to solve the memory scalability problem without sacri-
324  22nd USENIX Security Symposium 
USENIX Association
4
ﬁcing the ability to optimize circuits automatically. Two
observations are key to our approach.
Our ﬁrst observation is that it is possible to free the
memory required for storing wire values without com-
puting a reference count for the wire. In previous work,
each wire in a circuit is assigned a unique global identi-
ﬁer, and gate input wires are speciﬁed in terms of these
identiﬁers (output wires can be identiﬁed by the position
of the gate in the gate list). Rather than using global
identiﬁers, we observe that wire values are ephemeral,
and only require a unique identity until their last use as
the input to a gate.
We therefore maintain a table of “active” wire values,
similar to KSS12, but change the gate description.
In
this format, wire values are identiﬁed by their index in
the table, and gates specify the index of each input wire
and an index for the output wire; in other words, a gate
is a tuple (cid:31)t,i1,i2,o(cid:30), where t is a truth table, i1,i2 are the
input wire indexes, and o is the output wire index. When
a wire value is no longer needed, its index in the table
can be safely used as an output wire for a gate.
Now, consider the following example of a circuit
described in the above format, which accumulates the
Boolean AND of seven wire values:
(cid:31)AND1,1,2,0(cid:30)
(cid:31)AND2,0,3,0(cid:30)
(cid:31)AND3,0,4,0(cid:30)
(cid:31)AND4,0,5,0(cid:30)
(cid:31)AND5,0,6,0(cid:30)
(cid:31)AND6,0,7,0(cid:30)
Our second observation is that circuits such as this can
be described more compactly using a loop. This builds
on our ﬁrst observation, which allows wire values to be
overwritten once they are no longer needed. A simple ap-
proach to allowing this would add a conditional branch
operation to the description format. This is more general
than the approach of PAL, which includes loops but al-
lows only simple iteration. Additionally, it is necessary
to allow the loop index to be used to specify the input or
output wire index of the gates; as a general solution, we
add support for indirection, allowing wire values to be
copied.
This representation of Boolean circuits is a bytecode
for a one-bit CPU, where the operations are the 16 pos-
sible two-arity Boolean gates, a conditional branch, and
indirect copy.
In our system, we also add instructions
for function calls (which need not be inlined at compile
time) and handling the parties’ inputs/outputs. When the
secure protocol is run, a three-level logic is used for wire
values: 0, 1, or ⊥, where ⊥ represents an “unknown”
value that depends on one of the party’s inputs. In the
case of a Yao protocol, the ⊥ value is represented by a
5
garbled wire value. Conditional branches are not allowed
to depend on ⊥ values, and indirection operations use
a separate table of pointers that cannot computed from
⊥ values (if such an indirection operation is required, it
must be translated into a large multiplexer, as in previous
work).
We refer to our circuit representation as the Portable
Circuit Format or PCF. In addition to gates and branches,
PCF includes support for copying wires indirectly, a
function call stack, data stacks, and setting function pa-
rameters. These additional operations do not emit any
gates and can therefore be viewed as “free” operations.
PCF is modeled after the concept of PAL, but instead
of using predeﬁned sub-circuits for complex operations,
a PCF ﬁle deﬁnes the sub-circuits for a given function
to allow for circuit structure optimization. PCF includes
lower level control structures compared to PAL, which
allows for more general loop structures.
In Appendix A, we describe in detail the semantics of
the PCF instructions. Example PCF ﬁles are available at
the authors’ website.
4.2 Describing Functions for SFE
Most commonly used programming languages can de-
scribe processes that cannot be translated to SFE; for ex-
ample, a program that does not terminate, or one which
terminates after reading a speciﬁc input pattern.
It is
therefore necessary to impose some limitation on the de-
scriptions of functions for SFE. In systems with domain
speciﬁc languages, these limitations can be imposed by
the grammar of the language, or can be enforced by
taking advantage of particular features of the grammar.
However, one goal of our system is to allow any pro-
gramming language to be used to describe functionality
for SFE, and so we cannot rely on the grammar of the
language being used.
We make a compromise when it comes to restricting
the inputs to our system. Unlike model checking sys-
tems [2], we impose no upper bound on loop iterations or
on recursive function calls (other than the memory avail-
able for the call stack), and leave the responsibility of en-
suring that programs terminate to the user. On the other
hand, our system does forbid certain easily-detectable
conditions that could result in inﬁnite loops, such as
unconditional backwards jumps, conditional backwards
jumps that depend on input, and indirect function calls.
These restrictions are similar to those imposed by the
Fairplay and KSS12 systems [18,21], but allow for more
general iteration than incrementing the loop index by a
constant. Although false positives, i.e., programs that
terminate but which contain such constructs are possible,
our hypothesis is that useful functions and typical com-
pilers would not result in such instruction sequences, and
USENIX Association  
22nd USENIX Security Symposium  325
we observed no such functions in our experiments with
LCC.
True
[code]
If
True
If
[code]
[code]
[code]
4.3 Algorithms for Translating Bytecode
Our compiler reads a bytecode representation of the
function, which lacks the structure of higher-level de-
scriptions and poses a unique challenge in circuit gener-
ation. As mentioned above, we do not impose any upper
limit on loop iterations or the depth of the function call
stack. Our approach to translation does not use any sym-
bolic analysis of the function. Instead, we translate the
bytecode into PCF, using conditional branches and func-
tion calls as needed and translating other instructions into
lists of gates. For testing, we use the IR from the LCC
compiler, which is based on the common stack machine
model; we will use examples of this IR to illustrate our
design, but note that none of our techniques strictly re-
quire a stack machine model or any particular features of
the LCC bytecode.
In our compiler, we divide bytecode instructions into
three classes:
Normal Instructions which have exactly one successor
and which can be represented by a simple circuit.
Examples of such instructions are arithmetic and
bitwise logic operations, operations that push data
onto the stack or move data to memory, etc.
Jump Instructions that result in an unconditional con-
trol ﬂow switch to a speciﬁc label. This does not
include function calls, which we represent directly
in PCF. Such instructions are usually used for if/else
constructs or preceding the entry to a loop.
Conditional Instructions that result
in control ﬂow
switching to either a label or the subsequent instruc-
tion, depending on the result of some conditional
statement. Examples include arithmetic compar-
isons.
In the stack machine model, all operands and the
results of operations are pushed onto a global stack.
For “normal” instructions, the translation procedure is
straightforward: the operands are popped off the stack
and assigned temporary wires, the subcircuit for the op-
eration is connected to these wires, and the output of the
operation is pushed onto the stack. “Jump” instructions
appear, at ﬁrst, to be equally straightforward, but actually
require special care as we describe below.
“Conditional” instructions present a challenge. Condi-
tional jumps whose targets precede the jump are assumed
to be loop constructs, and are translated directly into PCF
branch instructions. All other conditional jumps require
the creation of multiplexers in the circuit to deal with
False
False
Figure 3: Nested if statements, which can be handled
using the stack-based algorithm.
conditional assignments. Therefore, the branch targets
must be tracked to ensure that the appropriate condition
wires are used to control those multiplexers.
In the Fairplay and KSS12 compilers, the condition
wire for an “if” statement is pushed onto a stack along
with a “scope” that is used to track the values (wire as-
signments) of variables. When a conditional block is
closed, the condition wire at the top of the stack is used
to multiplex the value of all the variables in the scope at
the top with the values from the scope second to the top,
and then the stack is popped. This procedure relies on
the grammar of “if/else” constructs, which ensures that
conditional blocks can be arranged as a tree. An exam-
ple of this type of “if/else” construct is in Figure 3. In a
bytecode representation, however, it is possible for con-
ditional blocks to “overlap” with each other without be-
ing nested.
In the sequence shown in Figure 4, the ﬁrst branch’s
target precedes the second branch’s target, and indirect
loads and assignments exist in the overlapping region of
these two branches. The control ﬂow of such an overlap
is given in Figure 5. A stack is no longer sufﬁcient in this
case, as the top of the stack will not correspond to the ap-
propriate branch when the next branch target is encoun-
tered. Such instruction sequences are not uncommon in
the code generated by production compilers, as they are
a convenient way to generate code for “else” blocks and
ternary operators.
To handle such sequences, we use a novel algorithm
based on a priority queue rather than a stack, and we
maintain a global condition wire that is modiﬁed as
branches and branch targets are reached. When a branch
instruction is reached, the global condition wire is up-
dated by logically ANDing the branch condition with
the global condition wire. The priority queue is updated
with the branch condition and a scope, as in the stack-
based algorithm; the priority is the target, with lower
targets having higher priority. When an assignment is
performed, the scope at the top of the priority queue is
updated with the value being assigned, the location be-
ing assigned to, the old value, and a copy of the global
condition wire. When a branch target is reached, multi-
plexers are emitted for each assignment recorded in the
scope at the top of the priority queue, using the copy of
the global condition wire that was recorded. After the
326  22nd USENIX Security Symposium 
USENIX Association
6
EQU4 A
INDIRI4 16
EQU4 B
INDIRI4 24
LABELV A
ASGNI4
LABELV B
ASGNI4
Figure 4: A bytecode sequence where overlapping con-
ditional blocks are not nested; note that the target of
the ﬁrst branch, “A,” precedes the target of the second
branch, “B.”
False
EQU4: A
[code]
EQU4: B
False
[code]
True
B: 
[code]
A: 
[code]
True
Figure 5: A control ﬂow with overlapping conditional
blocks.
multiplexers are emitted, the global condition wire is up-
dated by ORing the inverse of the condition wire at the
top of the priority queue, and then the top is removed.
Unconditional jumps are only allowed in the forward
direction, i.e., only if the jump precedes its target. When
such instructions are encountered, they are translated into
conditional branches whose condition wire is the inverse
of the conjunction of the condition wires of all enclos-
ing branches.
In the case of a jump that is not in any
conditional block, the condition wire is set to false; this
does not necessarily mean that subsequent assignments
will not occur, as the multiplexers for these assignments
will be emitted and will depend on a global control line
that may be updated as part of a loop construct. The op-
timizer is responsible for determining whether such as-
signments can occur, and will rewrite the multiplexers as
direct assignments when possible.
Finally, it is possible that the operand stack will have
changed in the fall-through path of a conditional jump.
In that case, the stack itself must be multiplexed. For
simplicity, we require that the depth of the stack not
change in a fall-through path. We did not observe any
such changes to the stack in our experiments with LCC.
4.4 Optimization
One of the shortcomings of the KSS12 system was the
amount of time and memory required to perform opti-
mizations on the computed circuit. In our system, opti-
mization is performed before loops are unrolled but after
the functionality is translated into a PCF representation.
This allows optimizations to be performed on a smaller
representation, but increases the complexity of the opti-
mization process somewhat.
The KSS12 compiler bases its optimization on a rudi-
mentary dataﬂow analysis, but without any conditional
branches or loops, and with single assignments to each
wire. In our system, loops are not eliminated and wires
may be overwritten, but conditional branches are elim-
inated. As in KSS12, we use an approach based on
dataﬂow analysis, but we must make multiple passes to
ﬁnd a ﬁxed point solution to the dataﬂow equations. Our
dataﬂow equations take advantage of the logical rules of
each gate, allowing more gates to be identiﬁed for elimi-
nation than the textbook equations identify.
We perform our dataﬂow analysis on individual PCF
instructions, which allows us to remove single gates even
where entire bytecode instructions could not be removed,
but which carries the cost of somewhat longer compila-
tion time, on the order of minutes for the experiments we
ran. Currently, our framework only performs optimiza-
tion within individual functions, without any interproce-
dural analysis. Compile times in our system can be re-
duced by splitting a large procedure into several smaller
procedures.
Optimization
None
Const. Prop.
Dead Elim.
Both
128 mult.
707,244
296,960
700,096
260,073
5x5 matrix
260,000
198,000
255,875
131,875
256 RSA
904,171,008
651,504,495
883,307,712
573,156,735
Table 1: Effects of constant propagation and dead code
elimination on circuit size, measured with simulator that
performs no simpliﬁcation rules. For each function, the
number of non-XOR gates are given for all combinations
of optimizations enabled.
4.4.1 Constant Propagation
The constant propagation framework we use is straight-
forward, similar to the methods used in typical compil-
ers. However, for some gates, simpliﬁcation rules can re-
sult in constants being computed even when the inputs to
a gate are not constant; for example, XORing a variable
with itself. The transfer function we use is augmented
with a check against logic simpliﬁcation rules to account
for this situation, but remains monotonic and so conver-
gence is still guaranteed.
4.4.2 Dead Gate Removal
The last step of our optimizer is to remove gates whose
output wires are never used. This is a standard bit vector
dataﬂow problem that requires little tailoring for our sys-
tem. As is common in compilers, performing this step
USENIX Association  
22nd USENIX Security Symposium  327
7
Function
16384-bit Comp.
128-bit Sum