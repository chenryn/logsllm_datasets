him unwrap one layer of the encryption. An attacker who
learns an OR’s TLS private key can impersonate that OR for
the TLS key’s lifetime, but he must also learn the onion key
to decrypt create cells (and because of perfect forward se-
crecy, he cannot hijack already established circuits without
also compromising their session keys). Periodic key rotation
limits the window of opportunity for these attacks. On the
other hand, an attacker who learns a node’s identity key can
replace that node indeﬁnitely by sending new forged descrip-
tors to the directory servers.
Iterated compromise. A roving adversary who can com-
promise ORs (by system intrusion, legal coercion, or extrale-
gal coercion) could march down the circuit compromising the
nodes until he reaches the end. Unless the adversary can com-
plete this attack within the lifetime of the circuit, however,
the ORs will have discarded the necessary information before
the attack can be completed. (Thanks to the perfect forward
secrecy of session keys, the attacker cannot force nodes to de-
crypt recorded trafﬁc once the circuits have been closed.) Ad-
ditionally, building circuits that cross jurisdictions can make
legal coercion harder—this phenomenon is commonly called
“jurisdictional arbitrage.” The Java Anon Proxy project re-
cently experienced the need for this approach, when a Ger-
man court forced them to add a backdoor to their nodes [51].
Run a recipient. An adversary running a webserver trivially
4Note that this ﬁngerprinting attack should not be confused with the much
more complicated latency attacks of [5], which require a ﬁngerprint of the
latencies of all circuits through the network, combined with those from the
network edges to the target user and the responder website.
learns the timing patterns of users connecting to it, and can in-
troduce arbitrary patterns in its responses. End-to-end attacks
become easier: if the adversary can induce users to connect
to his webserver (perhaps by advertising content targeted to
those users), he now holds one end of their connection. There
is also a danger that application protocols and associated pro-
grams can be induced to reveal information about the initiator.
Tor depends on Privoxy and similar protocol cleaners to solve
this latter problem.
Run an onion proxy. It is expected that end users will nearly
always run their own local onion proxy. However, in some
settings, it may be necessary for the proxy to run remotely—
typically, in institutions that want to monitor the activity of
those connecting to the proxy. Compromising an onion proxy
compromises all future connections through it.
DoS non-observed nodes. An observer who can only watch
some of the Tor network can increase the value of this trafﬁc
by attacking non-observed nodes to shut them down, reduce
their reliability, or persuade users that they are not trustwor-
thy. The best defense here is robustness.
Run a hostile OR. In addition to being a local observer, an
isolated hostile node can create circuits through itself, or alter
trafﬁc patterns to affect trafﬁc at other nodes. Nonetheless, a
hostile node must be immediately adjacent to both endpoints
to compromise the anonymity of a circuit. If an adversary can
run multiple ORs, and can persuade the directory servers that
those ORs are trustworthy and independent, then occasionally
some user will choose one of those ORs for the start and an-
other as the end of a circuit. If an adversary controls m > 1
of N nodes, he can correlate at most(cid:0) m
(cid:1)2 of the trafﬁc—
N
although an adversary could still attract a disproportionately
large amount of trafﬁc by running an OR with a permissive
exit policy, or by degrading the reliability of other routers.
Introduce timing into messages. This is simply a stronger
version of passive timing attacks already discussed earlier.
Tagging attacks. A hostile node could “tag” a cell by al-
tering it.
If the stream were, for example, an unencrypted
request to a Web site, the garbled content coming out at the
appropriate time would conﬁrm the association. However, in-
tegrity checks on cells prevent this attack.
Replace contents of unauthenticated protocols. When re-
laying an unauthenticated protocol like HTTP, a hostile exit
node can impersonate the target server. Clients should prefer
protocols with end-to-end authentication.
Replay attacks. Some anonymity protocols are vulnerable
to replay attacks. Tor is not; replaying one side of a hand-
shake will result in a different negotiated session key, and so
the rest of the recorded session can’t be used.
Smear attacks. An attacker could use the Tor network for
socially disapproved acts, to bring the network into disrepute
and get its operators to shut it down. Exit policies reduce
the possibilities for abuse, but ultimately the network requires
volunteers who can tolerate some political heat.
Distribute hostile code. An attacker could trick users
into running subverted Tor software that did not, in fact,
anonymize their connections—or worse, could trick ORs
into running weakened software that provided users with
less anonymity. We address this problem (but do not solve it
completely) by signing all Tor releases with an ofﬁcial public
key, and including an entry in the directory that lists which
versions are currently believed to be secure. To prevent an
attacker from subverting the ofﬁcial release itself (through
threats, bribery, or insider attacks), we provide all releases in
source code form, encourage source audits, and frequently
warn our users never to trust any software (even from us) that
comes without source.
Directory attacks
Destroy directory servers. If a few directory servers disap-
pear, the others still decide on a valid directory. So long
as any directory servers remain in operation, they will still
broadcast their views of the network and generate a consensus
directory. (If more than half are destroyed, this directory will
not, however, have enough signatures for clients to use it au-
tomatically; human intervention will be necessary for clients
to decide whether to trust the resulting directory.)
Subvert a directory server. By taking over a directory
server, an attacker can partially inﬂuence the ﬁnal directory.
Since ORs are included or excluded by majority vote, the cor-
rupt directory can at worst cast a tie-breaking vote to decide
whether to include marginal ORs. It remains to be seen how
often such marginal cases occur in practice.
Subvert a majority of directory servers. An adversary who
controls more than half the directory servers can include as
many compromised ORs in the ﬁnal directory as he wishes.
We must ensure that directory server operators are indepen-
dent and attack-resistant.
Encourage directory server dissent. The directory agree-
ment protocol assumes that directory server operators agree
on the set of directory servers. An adversary who can per-
suade some of the directory server operators to distrust one
another could split the quorum into mutually hostile camps,
thus partitioning users based on which directory they use. Tor
does not address this attack.
Trick the directory servers into listing a hostile OR. Our
threat model explicitly assumes directory server operators
will be able to ﬁlter out most hostile ORs.
Convince the directories that a malfunctioning OR is
working. In the current Tor implementation, directory servers
assume that an OR is running correctly if they can start a
TLS connection to it. A hostile OR could easily subvert this
test by accepting TLS connections from ORs but ignoring all
cells. Directory servers must actively test ORs by building
circuits and streams as appropriate. The tradeoffs of a similar
approach are discussed in [18].
Attacks against rendezvous points
Make many introduction requests. An attacker could try to
deny Bob service by ﬂooding his introduction points with re-
quests. Because the introduction points can block requests
that lack authorization tokens, however, Bob can restrict the
volume of requests he receives, or require a certain amount of
computation for every request he receives.
Attack an introduction point. An attacker could disrupt a
location-hidden service by disabling its introduction points.
But because a service’s identity is attached to its public key,
the service can simply re-advertise itself at a different intro-
duction point. Advertisements can also be done secretly so
that only high-priority clients know the address of Bob’s in-
troduction points or so that different clients know of different
introduction points. This forces the attacker to disable all pos-
sible introduction points.
Compromise an introduction point. An attacker who con-
trols Bob’s introduction point can ﬂood Bob with introduction
requests, or prevent valid introduction requests from reaching
him. Bob can notice a ﬂood, and close the circuit. To notice
blocking of valid requests, however, he should periodically
test the introduction point by sending rendezvous requests
and making sure he receives them.
Compromise a rendezvous point. A rendezvous point is no
more sensitive than any other OR on a circuit, since all data
passing through the rendezvous is encrypted with a session
key shared by Alice and Bob.
8 Early experiences: Tor in the Wild
As of mid-May 2004, the Tor network consists of 32 nodes
(24 in the US, 8 in Europe), and more are joining each week
as the code matures. (For comparison, the current remailer
network has about 40 nodes.) Each node has at least a
768Kb/768Kb connection, and many have 10Mb. The num-
ber of users varies (and of course, it’s hard to tell for sure), but
we sometimes have several hundred users—administrators at
several companies have begun sending their entire depart-
ments’ web trafﬁc through Tor, to block other divisions of
their company from reading their trafﬁc. Tor users have re-
ported using the network for web browsing, FTP, IRC, AIM,
Kazaa, SSH, and recipient-anonymous email via rendezvous
points. One user has anonymously set up a Wiki as a hidden
service, where other users anonymously publish the addresses
of their hidden services.
Each Tor node currently processes roughly 800,000 relay
cells (a bit under half a gigabyte) per week. On average, about
80% of each 498-byte payload is full for cells going back to
the client, whereas about 40% is full for cells coming from the
client. (The difference arises because most of the network’s
trafﬁc is web browsing.) Interactive trafﬁc like SSH brings
down the average a lot—once we have more experience, and
assuming we can resolve the anonymity issues, we may parti-
tion trafﬁc into two relay cell sizes: one to handle bulk trafﬁc
and one for interactive trafﬁc.
Based in part on our restrictive default exit policy (we re-
ject SMTP requests) and our low proﬁle, we have had no
abuse issues since the network was deployed in October 2003.
Our slow growth rate gives us time to add features, resolve
bugs, and get a feel for what users actually want from an
anonymity system. Even though having more users would
bolster our anonymity sets, we are not eager to attract the
Kazaa or warez communities—we feel that we must build a
reputation for privacy, human rights, research, and other so-
cially laudable activities.
As for performance, proﬁling shows that Tor spends almost
all its CPU time in AES, which is fast. Current latency is
attributable to two factors. First, network latency is critical:
we are intentionally bouncing trafﬁc around the world several
times. Second, our end-to-end congestion control algorithm
focuses on protecting volunteer servers from accidental DoS
rather than on optimizing performance. To quantify these ef-
fects, we did some informal tests using a network of 4 nodes
on the same machine (a heavily loaded 1GHz Athlon). We
downloaded a 60 megabyte ﬁle from debian.org every 30
minutes for 54 hours (108 sample points). It arrived in about
300 seconds on average, compared to 210s for a direct down-
load. We ran a similar test on the production Tor network,
fetching the front page of cnn.com (55 kilobytes): while
a direct download consistently took about 0.3s, the perfor-
mance through Tor varied. Some downloads were as fast as
0.4s, with a median at 2.8s, and 90% ﬁnishing within 5.3s. It
seems that as the network expands, the chance of building a
slow circuit (one that includes a slow or heavily loaded node
or link) is increasing. On the other hand, as our users remain
satisﬁed with this increased latency, we can address our per-
formance incrementally as we proceed with development.
Although Tor’s clique topology and full-visibility directo-
ries present scaling problems, we still expect the network to
support a few hundred nodes and maybe 10,000 users before
we’re forced to become more distributed. With luck, the ex-
perience we gain running the current topology will help us
choose among alternatives when the time comes.
9 Open Questions in Low-latency Anonymity
In addition to the non-goals in Section 3, many questions
must be solved before we can be conﬁdent of Tor’s security.
Many of these open issues are questions of balance. For
example, how often should users rotate to fresh circuits? Fre-
quent rotation is inefﬁcient, expensive, and may lead to inter-
section attacks and predecessor attacks [54], but infrequent
rotation makes the user’s trafﬁc linkable. Besides opening
fresh circuits, clients can also exit from the middle of the cir-
cuit, or truncate and re-extend the circuit. More analysis is
needed to determine the proper tradeoff.
How should we choose path lengths? If Alice always uses
two hops, then both ORs can be certain that by colluding they
will learn about Alice and Bob. In our current approach, Alice
always chooses at least three nodes unrelated to herself and
her destination. Should Alice choose a random path length
(e.g. from a geometric distribution) to foil an attacker who
uses timing to learn that he is the ﬁfth hop and thus concludes
that both Alice and the responder are running ORs?
Throughout this paper, we have assumed that end-to-end
trafﬁc conﬁrmation will immediately and automatically de-
feat a low-latency anonymity system. Even high-latency
anonymity systems can be vulnerable to end-to-end trafﬁc
conﬁrmation, if the trafﬁc volumes are high enough, and if
users’ habits are sufﬁciently distinct [14, 31]. Can anything
be done to make low-latency systems resist these attacks as
well as high-latency systems? Tor already makes some ef-
fort to conceal the starts and ends of streams by wrapping
long-range control commands in identical-looking relay cells.
Link padding could frustrate passive observers who count
packets; long-range padding could work against observers
who own the ﬁrst hop in a circuit. But more research remains
to ﬁnd an efﬁcient and practical approach. Volunteers pre-
fer not to run constant-bandwidth padding; but no convinc-
ing trafﬁc shaping approach has been speciﬁed. Recent work
on long-range padding [33] shows promise. One could also
try to reduce correlation in packet timing by batching and re-
ordering packets, but it is unclear whether this could improve
anonymity without introducing so much latency as to render
the network unusable.
A cascade topology may better defend against trafﬁc con-
ﬁrmation by aggregating users, and making padding and mix-
ing more affordable. Does the hydra topology (many input
nodes, few output nodes) work better against some adver-
saries? Are we going to get a hydra anyway because most
nodes will be middleman nodes?
Common wisdom suggests that Alice should run her own
OR for best anonymity, because trafﬁc coming from her node
could plausibly have come from elsewhere. How much mix-
ing does this approach need? Is it immediately beneﬁcial
because of real-world adversaries that can’t observe Alice’s
router, but can run routers of their own?
To scale to many users, and to prevent an attacker from
observing the whole network, it may be necessary to support
far more servers than Tor currently anticipates. This intro-
duces several issues. First, if approval by a central set of di-
rectory servers is no longer feasible, what mechanism should
be used to prevent adversaries from signing up many collud-
ing servers? Second, if clients can no longer have a complete
picture of the network, how can they perform discovery while