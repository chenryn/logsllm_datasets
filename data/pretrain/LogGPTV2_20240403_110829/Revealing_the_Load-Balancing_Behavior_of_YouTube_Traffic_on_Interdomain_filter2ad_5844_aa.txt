title:Revealing the Load-Balancing Behavior of YouTube Traffic on Interdomain
Links
author:Ricky K. P. Mok and
Vaibhav Bajpai and
Amogh Dhamdhere and
kc claffy
Revealing the Load-Balancing Behavior
of YouTube Traﬃc on Interdomain Links
Ricky K. P. Mok1(B)
, Vaibhav Bajpai2, Amogh Dhamdhere1,
and K. C. Claﬀy1
1 CAIDA, UCSD, San Diego, USA
{cskpmok,amogh,kc}@caida.org
2 Technische Universit¨at M¨unchen, Munich, Germany
PI:EMAIL
Abstract. For the last decade, YouTube has consistently been a domi-
nant source of traﬃc on the Internet. To improve the quality of experience
(QoE) for YouTube users, broadband access providers and Google apply
techniques to load balance the extraordinary volume of web requests
and traﬃc. We use traceroute-based measurement methods to infer
these techniques for assigning YouTube requests to speciﬁc Google video
content caches, including the interconnection links between the access
providers and Google. We then use a year of measurements (mid-2016 to
mid-2017) collected from SamKnows probes hosted by broadband cus-
tomers spanning a major ISP in the U.S. and three ISPs in Europe.
We investigate two possible causes of diﬀerent interdomain link usage
behavior. We also compare the YouTube video cache hostnames and IPs
observed by the probes, and ﬁnd that the selection of video cache has
little impact on BGP selection of interdomain links.
1 Introduction
Over a billion users collectively watch billions of hours of videos every day [25],
making Google’s YouTube the most popular video streaming web service on
the Internet. The tremendous growth in volume of users and video content has
occurred in parallel with – and as a key driver of – the development and improve-
ment of broadband infrastructure around the world. Indeed, many consumers
have canceled their cable television subscriptions in favor of media services such
as YouTube or Netﬂix available over the Internet. Accompanying this evolution
are growing performance expectations of users – that the streaming video quality
of experience should match that of cable television, a service historically pro-
vided over a dedicated private network infrastructure. In parallel, the evolution
of video technologies, such as 8K resolution, 60 frame per second (fps), and High
Dynamic Range (HDR), has increased the network bandwidth requirement and
has further challenged network provisioning economics.
ISPs can coordinate (contracts) with Google to install Google Global Caches
(GGCs) inside their networks, and can also rely on their peering relationships
c(cid:2) Springer International Publishing AG, part of Springer Nature 2018
R. Beverly et al. (Eds.): PAM 2018, LNCS 10771, pp. 228–240, 2018.
https://doi.org/10.1007/978-3-319-76481-8_17
Revealing the Load-Balancing Behavior of YouTube Traﬃc
229
with Google (AS 15169/AS 36040) to connect users to Google/YouTube front-
end servers and video caches inside Google’s Points of Presence (PoPs). Many of
these interdomain links are of signiﬁcant and growing capacity, but they can still
experience congestion during peak hours [17] that may induce inﬂated round-trip
delay and packet losses, and thus degrade user QoE.
We report the results of a study that combines interdomain topol-
ogy measurement and YouTube-speciﬁc probing measurements to investigate
performance-relevant traﬃc dynamics of ISPs that do not deploy GGCs. We
inferred interdomain router-level topology by executing the bdrmap [18] tool on
∼50 of CAIDA’s Archipelago (Ark) probes [7]. We used a recently developed
end-to-end YouTube performance test [3] that streams a video clip similar to a
normal client, and reports information including the hostname and IP address
of the YouTube video cache (GGC) streaming the video. The test then immedi-
ately performs a paris-traceroute [4] toward that IP to capture the forward
path information. The test ran on ∼100 SamKnows probes [6] for about a year
(May 2016 to July 2017) [5]. We selected the SamKnows probes connected to
ISPs that did not deploy GGCs internally, but whose interdomain topology to
Google was captured by our bdrmap measurements. This constraint limited our
study to 15 SamKnows probes connected to four major ISPs: 1 in the U.S., 3 in
Europe.
Our study had two major goals. The ﬁrst one was to investigate factors that
inﬂuence ISP strategies for distributing YouTube traﬃc ﬂows across diﬀerent
interdomain links. We studied two possible factors – geographic location and
time of day. We developed a metric of link usage probability to characterize the
link usage behavior observed by our probes. Our results revealed that geographic
location appeared to inﬂuence interdomain link assignment for Comcast users,
i.e., proximate users were more likely to use the same set of links to reach a
cache. We also found that a German ISP (Kabel Deutschland) showed diﬀerent
link usage behavior during peak vs. oﬀ-peak hours; other ISPs did not show
such a signiﬁcant diﬀerence. By analyzing the interdomain topology, we also
discovered three European ISPs that relied on the YouTube AS (AS 36040)
rather than the primary Google AS (AS 15169) to reach YouTube content. Our
second goal was to study whether YouTube’s cache selection approach could
also determine the choice of interdomain links due to the topological location
of the cache. We did not observe such a correspondence; more than half of the
video caches we observed used at least two interdomain links. We also discovered
that the DNS namespace for YouTube video caches (*.googlevideo.com) had a
more static hostname-IP mapping than front-end hostnames (e.g., youtube.com
and google.com), which used DNS-based redirection [8]. 90% of video cache
hostnames were reported (by the probes) to have the same IP, even if they were
resolved by diﬀerent probes.
Section 2 presents related work on YouTube measurement. Sections 3, 4, and
5 describes our datasets and methodology, reports our ﬁndings, and oﬀers con-
clusions, respectively.
230
R. K. P. Mok et al.
2 Related Work
Previous studies have evaluated the architecture or characteristics of YouTube by
actively sending video requests. Pytomo [22] crawled YouTube video clips from
residential broadband (volunteer) hosts, and collected YouTube server informa-
tion including hostname and network throughput. They found that the YouTube
cache selection depended on user’s ISP rather than geographical proximity.
Adhikari et al. [2] dissected the architecture of YouTube by requesting video
clips from PlanetLab nodes. To increase the coverage, they exploited various
geographically distributed public DNS servers to trigger DNS-based redirection
in YouTube front-end servers. Recent studies [8,10] used the EDNS extension to
geolocate Google’s CDN infrastructure. A closely related work by Windisch [24]
deployed ﬁve monitors in a German ISP and parsed YouTube responses to ana-
lyze selection of video caches. These studies did not investigate interdomain link
structure, which could impact latency and streaming performance. Our study
ﬁlls this gap by integrating interdomain topology and end-to-end measurement
to understand the ISP’s role in load balancing YouTube traﬃc.
Others have used passive measurement to study YouTube traﬃc, including
analyzing traﬃc characteristics of video ﬂows [12,13] and cache selection mech-
anisms [23]. Casas et al. [9] used a 90-h Tstat trace to contrast YouTube traﬃc
characteristics between ﬁxed-line and mobile users. YouLighter [14] used passive
monitoring to learn the structure of YouTube’s CDN and automatically detect
changes. Because passive measurement relies on user traﬃc, it is hard to per-
form a longitudinal study from the same set of clients to observe changes in load
balancing across interdomain links over time.
3 Methodology
We deployed the YouTube test [3] on ∼100 SamKnows probes connected to
dual-stacked networks representing 66 diﬀerent origin ASes [5]. The probes were
mostly within the RIPE (60 probes) and ARIN (29) region, and hosted in home
networks (78). The YouTube test ran once per hour for IPv4 and then for IPv6.
Each test streamed a popular video from YouTube, and reported the streaming
information and performance, including start-up delay, YouTube cache hostname
and IP. We then ran paris-traceroute [4] with scamper [16] toward the cache
IP reported by the YouTube test, obtaining forward path and latency measure-
ments. Details of the YouTube tests and SamKnows probe measurements are in
[3] and [5], respectively.
To identify which interdomain links (if any) were traversed on the paths
from our SamKnows probes to YouTube servers, we ﬁrst compiled the set of
interdomain interconnections of the access network visible from a vantage point
(VP) in that network. We used bdrmap [18], an algorithm that infers interdomain
interconnections of a VP network visible from that VP. In the collection phase,
bdrmap issues traceroutes from the VP toward every routed BGP preﬁx, and
performs alias resolution from the VP on IP addresses seen in these traceroutes.
Revealing the Load-Balancing Behavior of YouTube Traﬃc
231
In the analysis phase, bdrmap uses the collected topology data along with AS-
relationship inferences from CAIDA’s AS relationship algorithm [19], and a list
of address blocks belonging to IXPs obtained from PeeringDB [21] and PCH [20]
to infer interdomain links at the router level. The bdrmap algorithm then uses
constraints from traceroute paths to infer ownership of each observed router, and
identiﬁes the routers on the near and far side (from the perspective of the VP)
of every observed router-level interdomain link. We could not run bdrmap from
the SamKnows probes, so we used the results of bdrmap running on Ark VPs
located in the same ASes as the SamKnows probes.
3.1 Identifying Interdomain Links from YouTube Dataset
The ﬁrst step of identifying interdomain links seen in our YouTube traceroutes
is to extract all the interdomain links to the Google ASes (AS 15169/AS 36040)
observed by Ark VPs. Each link is represented by a pair of IP addresses indicating
the interfaces of the near and far side routers. We used these pairs to match
consecutive hops in the traceroutes to YouTube video caches. This approach
avoids false inference of links, but could miss some links with the same far
side IP but a near side IP that bdrmap did not observe, because bdrmap and
the YouTube traceroutes run from diﬀerent VPs. Section 4.1 describes why we
consider our coverage of interdomain links to be satisfactory.
The next step is to aggregate pairs with the same far side IP, because diﬀerent
VPs in the same network may take diﬀerent paths before exiting via the same
interdomain link; in such cases, they likely observe diﬀerent addresses (aliases)
on the near router. Even though bdrmap has performed some IP alias resolution,
multiple links may connect the same near and far side routers. We resolve this
ambiguity by conducting additional IP alias resolution with MIDAR [15] on
these far side IPs. Table 1 shows the number of inferred interconnection links at
each stage.
Table 1. Number of identiﬁed interdomain links at each stage.
Stages
Number of links
Interdomain links to Google inferred by bdrmap 1,268
Links identiﬁed in YouTube traceroutes
Aggregated with far side IPs
IP alias resolution with MIDAR
468
61
45
3.2 Descriptive Statistics
We analyzed data collected from May 17, 2016 to July 4, 2017, which included
a gap in data between January 4, 2017 and February 15, 2017 for all probes
due to technical problems. The data includes more than 74,000 experiment ses-
sions/traceroute records, collected from 15 SamKnows probes connected to 4
232
R. K. P. Mok et al.
broadband ISPs in the United States and Europe. We only used a small subset
of the entire YouTube traceroute dataset in our study, constrained by our needs
for: (1) co-located Ark VPs in the same ISP to obtain bdrmap coverage, and
(2) ISPs without GGC deployment internal to their network. The YouTube test
collected more than 3,000 distinct video cache hostnames and IPs. Table 2 shows
the details of the combined dataset. We adopt the notation #XX to represent
SamKnows probes. The number (XX) matches the probe ID in the metadata of
the SamKnows probes listed in (https://goo.gl/E2m22J).
Table 2. Summary of the combined dataset.
ISP
Country
Comcast Kabela Italiab Free
FR
US
DE
IT
No. of SamKnows probes
No. of interdomain links with Google
12
26
No. of observed video caches by Hostname 2,918
2,983
IP
The full company name: aVodafone Kabel Deutschland;
bTelecom Italia Sparkle S.p.A.
1
5
303
300
1
10
183
185
1
4
176
185
4 Results
We analyzed load balancing behavior on both the ISP and server side, by char-
acterizing the use of interdomain links and the video cache assignment. These
choices are interdependent, since ISPs route YouTube requests according to the
IP address of the video cache assigned by YouTube. We attempted to isolate
these two behaviors and investigate them separately. We investigated the impact
of two factors – geographic location and time of day. We also used hostnames and
IP addresses of YouTube caches to estimate the inﬂuence of YouTube’s video
cache selection mechanism on interdomain paths traversed by YouTube requests.
4.1
Interconnection Between ISPs and Google
Consistent with public data [21], we observed multiple interdomain links con-
necting ISPs to Google in various locations. Figure 1(a) and (b) are two heatmaps
showing the interdomain links used by probes in Comcast and the three
European ISPs, respectively. Each row represents a SamKnows probe; chang-
ing colors on a row represent changing interdomain links. The YouTube tests
and traceroutes execute once per hour, so the time resolution of each cell in
a row is 1 h. Gray color indicates no data available. Apart from the blackout
period, some probes began probing after the measurement period starts (e.g.,
#89 and #96) or went oﬄine. White color indicates the probe was online, but
we could not identify an interdomain link discovered by bdrmap measurement
Revealing the Load-Balancing Behavior of YouTube Traﬃc
233
from the traceroute. For Comcast, which hosts multiple Ark VPs, we identi-
ﬁed an interdomain link in 83.4% of traceroutes. For ISP Free (#71) and Italia
(#43), we identiﬁed an interdomain link in only 40.2% and 77.7% of traceroutes,
respectively. The large white portion in #02 after February 2017 was caused by
relocation of the probe from a Kabel user to a M-net (another German ISP)
user. Ark did not have any VP in the M-net network.
We found that each probe used at least 2 interdomain links throughout the
measurement period. Some probes (e.g., #78, #43) observed more than 6 links.
Load balancing among links was frequent, reﬂected by a change in color over
time. Although not clearly visible in the heatmap, we observed some monitors
cease using a link that other monitors continued to use, suggesting another reason
for the switch than a link outage. We observed only one link (light blue color)
captured by ﬁve monitors (#27, #67, #44, #60, #32) that switched entirely to
(a) Probes in Comcast.
(b) Probes in Kabel(#02), Italia(#43), and Free(#71).
Fig. 1. Observed interdomain links against time. Changing colors represents the
switching of interdomain links. (Color ﬁgure online)
234
R. K. P. Mok et al.
another link (darker blue) after mid-February 2017. The set of links used by two
diﬀerent monitors diﬀered widely, even in the same ISP. For example, there was
no intersection of links between #61 and #44, unlike #44 and #32.
We systematically studied the assignment of interdomain links to probes by
computing the probability of observing each link by the probes. We deﬁne the
link usage probability, P b
l as
P b
l =
nb
l(cid:2)
∀i∈L
,
nb
i
(1)
where L is the set of all 45 interdomain links observed in our data, nb
l is the num-
ber of observations of link l by probe b. Higher values indicate higher probability
for the probe to use that link.
Due to space limitation we show results of only six representative probes
(including 38 links and all 4 ISPs) in Fig. 2. The x-axis of the ﬁgure shows
diﬀerent interdomain links, while the y-axis indicates the link usage probability
(log scale). Diﬀerent color bars distinguish results of the six probes. The gray
dotted vertical lines separate links of diﬀerent ISPs. Four probes in Comcast
(#61, #38, #78, #44) showed slight overlap in interdomain link use (e.g., Link
ID 2 and 8). Three probes in Comcast (#38, #78, #44) showed comparable
probability of using at least 2 links, indicating load balancing behavior. Probes
#02, #43, and #71 distribute requests to at most 10 links. To demystify the
assignment of links, we examined two possible factors: geographical location and
time of day.
100
y
t
i
l
i
Comcast
#61
#38
Kabel
#78