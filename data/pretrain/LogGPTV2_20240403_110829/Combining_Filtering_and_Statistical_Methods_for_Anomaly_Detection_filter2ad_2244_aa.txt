title:Combining Filtering and Statistical Methods for Anomaly Detection
author:Augustin Soule and
Kav&apos;e Salamatian and
Nina Taft
Combining Filtering and Statistical Methods for Anomaly Detection
Augustin Soule
LIP6-UPMC
Kav´e Salamatian
LIP6-UPMC
Nina Taft
Intel Research
Abstract
In this work we develop an approach for anomaly detec-
tion for large scale networks such as that of an enterprize
or an ISP. The trafﬁc patterns we focus on for analysis are
that of a network-wide view of the trafﬁc state, called the
trafﬁc matrix. In the ﬁrst step a Kalman ﬁlter is used to ﬁl-
ter out the “normal” trafﬁc. This is done by comparing our
future predictions of the trafﬁc matrix state to an inference
of the actual trafﬁc matrix that is made using more recent
measurement data than those used for prediction.
In the
second step the residual ﬁltered process is then examined
for anomalies. We explain here how any anomaly detection
method can be viewed as a problem in statistical hypothe-
sis testing. We study and compare four different methods
for analyzing residuals, two of which are new. These meth-
ods focus on different aspects of the trafﬁc pattern change.
One focuses on instantaneous behavior, another focuses on
changes in the mean of the residual process, a third on
changes in the variance behavior, and a fourth examines
variance changes over multiple timescales. We evaluate
and compare all of these methods using ROC curves that
illustrate the full tradeoff between false positives and false
negatives for the complete spectrum of decision thresholds.
1
Introduction
Trafﬁc anomalies such as attacks, ﬂash crowds, large ﬁle
transfers and outages occur fairly frequently in the Internet
today. Large enterprise networks often have a security op-
erations center where operators continuously monitor the
network trafﬁc hoping to detect, identify and treat anoma-
lies.
In smaller networks, these tasks are carried out by
general network administrators who are also carry out other
day-to-day network maintenance and planning activities.
Despite the recent growth in monitoring technology and in
intrusion detection systems, correctly detecting anomalies
in a timely fashion remains a challenging task.
One of the reasons for this is that many of today’s
security solutions yield equipment that collects and ana-
lyzes trafﬁc from one link at a time. Similarly many re-
search efforts consider anomaly detection on a per link ba-
sis [2, 8, 3]. To detect trafﬁc anomalies one typically seeks
to characterize, or build a model, of what constitutes nor-
mal behavior. After ﬁltering out normal looking trafﬁc,
anomaly detection methods analyze the residual trafﬁc pat-
tern for deviations. Considering only one link is limiting.
Since any ﬂow will traverse multiple links along its path,
it is intuitive that a ﬂow carrying an anomaly will appear
in multiple links, thus increasing the evidence to detect it.
Instead in this paper, we focus on using data from all the
links in an enterprise or ISP network simultaneously. Since
any anomaly has to traverse multiple links on route to its
destination, an anomaly has the potential to be visible in
any of the links its traverses. Since we cannot know in ad-
vance where anomalies will originate, nor the path they will
take, it is advantageous to consider the behavior of all the
links in an enterprise simultaneously when developing both
a model of ”normal” trafﬁc and a method for analyzing the
”residuals”.
A trafﬁc matrix is a representation of the network-wide
trafﬁc demands. Each trafﬁc matrix entry describes the av-
erage volume of trafﬁc, in a given time interval, that orig-
inates at a given source node and is headed towards a par-
ticular destination node.
In an enterprise network these
nodes may be computers, whereas in an ISP network the
end nodes can be routers. In this paper we propose to use
predictions of trafﬁc matrix behavior for the purposes of
anomaly detection.
Since a trafﬁc matrix is a representation of trafﬁc vol-
ume, the types of anomalies we might be able to detect
via analysis of the trafﬁc matrix are volume anomalies
[12]. Examples of events that create volume anomalies
are denial-of-service attacks (DOS), ﬂash crowds and al-
pha events (e.g., non-malicious large ﬁle transfers), as well
as outages (e.g., coming from equipment failures).
Obtaining trafﬁc matrices was originally viewed as a
challenging task since it is believed that directly measuring
USENIX Association
Internet Measurement Conference 2005  
331
them is extremely costly as it requirements the deployment
of monitoring infrastructure everywhere, the collection of
ﬁne granularity data at the ﬂow level, and then the pro-
cessing of large amounts of data. However in the last few
years many inference based techniques have been devel-
oped (such as [22, 23, 19, 18, 20, 6] and many others) that
can estimate trafﬁc matrices reasonably well given only
per-link data such as SNMP data (that is widely available).
These techniques focus on estimation and not prediction.
In this paper we build upon one of our previous tech-
niques [20, 18] for trafﬁc matrix estimation by using it to
provide predictions of future values of the trafﬁc matrix. A
trafﬁc matrix is a dynamic entity that continually evolves
over time, thus estimates of a trafﬁc matrix are usually pro-
vided for each time interval (e.g., most previous techniques
focus on 5 or 10 minute intervals). We predict the trafﬁc
matrix one step (e.g., 5 minutes) into the future. One of
the key ideas behind our approach lies in the following ob-
servation. Five minutes after the prediction is made, we
obtain new link-level SNMP measurements, and then esti-
mate what the actual trafﬁc matrix should be. We then ex-
amine the difference between our prediction (made without
the most recent link-level measurements) and the estima-
tion (made using the most recent measurements). If our es-
timates and predictor are usually good, then this difference
should be close to zero. When the difference is sizeable
we become suspicious and analyze this residual further to
determine whether or not an anomaly alert should be gen-
erated.
We compare four different methods for signalling alerts
when analyzing residual trafﬁc. The simplest method com-
pares the instantaneous residual trafﬁc to a threshold. The
second method considered is a small variation on the de-
viation score idea presented in [2]. Their key idea is to
compare a local (temporally) variance calculation with a
global variance assessment. The deviation score used in
[2] is computed using output signals of a wavelet transform
applied to IP ﬂow level data from a single link. We apply
this idea of comparing the local to the global variance on
our ﬁltered residual signal. In our third scheme, we apply
wavelet analysis only on the ﬁltered trafﬁc (in [2] wavelet
analysis is applied directly on the original signal). We sig-
nal an alert when the detail signal (now a type of residual) at
each of a few different timescales exceeds a threshold. We
raise an alarm only if the threshold is exceeded at multiple
timescales. The fourth method uses a generalized likeli-
hood ratio test to identify the moment an anomaly starts,
by identifying a change in mean rate of the residual signal.
These last two methods, introduced here for the ﬁrst time,
are particular applications of known statistical techniques
to the anomaly detection domain.
Our approach is different from other approaches in that
usually anomaly detection is performed directly on moni-
tored data that is captured at the target granularity level. In-
stead we perform anomaly detection on origin-destination
(OD) ﬂows, a granularity of data that we infer from other
measurements (link statistics). Our study shows that it is
possible to follow such an approach towards a positive out-
come.
To validate our methods we use both real data from the
Abilene network and a synthetic anomaly generator that
we developed. These two approaches are complementary
as their advantages and disadvantages are opposite. The
advantage of evaluating using real world traces is that we
test our methods on actual anomalies that have occurred
in the Internet. The disadvantage of using only collected
traces is that the statistical parameters of the anomaly can-
not be varied. One cannot therefore identify the limits of
a method. For example, one cannot ask ”would we still
detect the anomaly if its volume were lower?”. Using syn-
thetically generated anomalies in which we carefully con-
trol the anomaly parameters, we can stress test and identify
the limits of an algorithm. However the synthetic anoma-
lies are limited because we have no evidence of them in the
Internet. Our approach to validation thus employs both of
these approaches in order to extract the beneﬁts of each.
We use ROC (Receiver Operating Characteristic) curves
as our key evaluation criteria. ROC curves have received
wide usage in medical diagnosis and signal detection the-
ory, but relatively little in network security. A ROC curve is
a graphical representation of the tradeoff between the false
positive and false negative rates for every possible deci-
sion threshold. We include a brief description of the mean-
ing and theory behind ROC curves to illustrate a general
methodology for analysing network security solutions. We
use these to compare our four solutions. The advantage of
this approach is that it permits scheme comparison through-
out the entire range of decision thresholds. This eliminates
the difﬁculty that arises when one tries to compare meth-
ods each of which uses a particular and seemingly ad hoc
threshold choice. In addition, we also present the perfor-
mance of these methods in terms of their detection time.
This is important as most anomaly detection methods incur
some lag time before reaching a decision. Finally we assess
the false positive and false negative rates our schemes yield
as the volume of an anomaly is varied from low-volume
anomalies to high-volume ones.
The most important, and only, work to date that uses
a network-wide perspective for volume anomaly detection
was that of [12].
In this work, the authors used the en-
semble of all the links in a network and performed Princi-
ple Components Analysis to reduce the dimensionality of
the data. They illustrate that by projecting onto a small
number of principal components one could ﬁlter out the
”normal” trafﬁc. The trafﬁc projected onto the remaining
components is analyzed for anomalies using a G-statistic
test on the predictive error. While our paper essentially
tackles the same problem, our work differs in numerous
332
Internet Measurement Conference 2005 
USENIX Association
2
ways: i) we process the incoming link data using kalman
ﬁlters rather than PCA analysis and generate trafﬁc matrix
predictions; ii) the granularity we focus on is that of OD
ﬂows whereas they use link data when analyzing residu-
als. (Note, they use the OD ﬂows as a secondary step, after
detecting an anomaly, in order to identify the source); iii)
they consider a single test on the residual trafﬁc whereas
we propose two new ones and conduct a comparative eval-
uation of four schemes; iv) our method for validation dif-
fers since we supplement the Abilene data with synthetic
anomaly testing; v) our evaluation is different because we
make use of ROC curves for evaluation, examine detection
lag times as well as sensitivity to anomaly volume sizes.
Section 2 describes how we model the OD ﬂows and
our solution for trafﬁc matrix prediction. The methods
for analyzing ﬁltered trafﬁc and determining how to de-
tect an anomaly are presented in Section 3. We discuss
our approach to validation and fully describe our synthetic
anomaly generator in Section 4. All of our evaluations and
the results are shown in Section 5.
2 Modeling Normal Trafﬁc
We assume that the monitoring infrastructure in our net-
work can easily obtain per-link statistics on byte counts (as
in SNMP today). From this we want to infer the trafﬁc ma-
trix that includes all pairs of origin-destination (OD) ﬂows.
This is the classic trafﬁc matrix estimation problem. If we
design a realistic model for the evolution of the network’s
trafﬁc matrix, then we can use this to ﬁlter our usual behav-
ior. For the sake of completeness we now summarize our
linear dynamic state space model for the OD ﬂows and our
Kalman ﬁlter method for estimating the trafﬁc matrix. This
was originally presented in [18]. We expand on our previ-
ous work by illustrating how this can be used to make future
predictions of the trafﬁc matrix and describe the resulting
residual processes that can be obtained when ﬁltering via
this approach.
Since the OD ﬂows are not directly observable (measur-
able with today’s technology) from the network, we refer to
them as hidden network states or simply as network states.
The link load levels (e.g., total bytes per unit time) are
directly observable in networks, and are captured via the
SNMP protocol that is widely deployed in most commer-
cial networks today. Because the total trafﬁc on a link is the
sum of all the OD ﬂows traversing that link, the relationship
between SNMP data and OD ﬂows can be expressed by the
linear equation Yt = AtXt + Vt, where Yt represents the
vector of link counts vector at time t, and Xt is the OD
ﬂows organized as a vector (hidden network states). At de-
notes the routing matrix whose elements at(i, j) are 1 if
OD ﬂow j traverses link i, and zero otherwise. (In some
networks fractional routing is supported.) The term Vt cap-
tures the stochastic measurement errors associated with the
data collection step. All these parameters are deﬁned for a
general discrete time t.
To capture the dynamic evolution of OD ﬂows we need
a model that speciﬁes Xt+1 as a function of Xt. We seek a
model that can be used for prediction of the OD ﬂows one
step into the future. Providing an efﬁcient model that cap-
tures trafﬁc dynamics is not so simple. It has been observed
that trafﬁc entering the network is characterized by highly
variable behavior in time [13]. There are many sources of
this variability, including daily periodic behavior, random
ﬂuctuations with relatively small amplitude, and occasional
bursts. Sudden changes in the trafﬁc are not uncommon and
can be related to different benign causes such as the addi-
tion of new customers, network equipment failures, ﬂash
crowds or to malicious activities such as attacks conducted
against the network. Ignoring the attacks for the moment,
our model for OD ﬂows must be rich enough to incorpo-
rate these sources of variability for normal trafﬁc.
It is
also known that both temporal correlations within a sin-
gle OD ﬂow exist, and that spatial correlations across some
OD ﬂows occurs [18].
We adopt a linear state space model to capture the evolu-
tion of OD ﬂows in time. This predictive model relates the
network state Xt+1 to Xt as follows: Xt+1 = CtXt + Wt,
where the state transition matrix Ct captures temporal and
spatial correlations in the system, and Wt is a noise process
that accounts for both the randomness in the ﬂuctuation of
a ﬂow, and the imperfection of the prediction model. Lin-
ear stochastic predictive models, combined with Gaussian
noise, have been successfully applied to a large spectrum
of monitoring problems.
The matrix Ct is an important element of the system. A
diagonal structure for Ct indicates that only temporal cor-
relations are included in the model of an OD ﬂow. When
Ct has off-diagonal elements that are non-zero, then spa-
tial correlation across OD ﬂows have been incorporated