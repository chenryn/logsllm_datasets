in two ways. First, we show that IP is not the only layer
to be considered in load-balanced networks (see Section 3).
Path diversity may be present at lower layers given LAG or
MPLS. Second, by relying on the RTT distribution instead
of the minimum observation, we are able to show that ping
is a bad estimator for any metric related to the RTT. Es-
sentially, ping systematically overestimates RTT variability,
which is signiﬁcantly lower when measured for individual
ﬂow identiﬁers.
Previous eﬀorts were devoted to better implementation of
per-ﬂow load balancing. For example, the MPLS entropy
label [10] is intended to avoid routers going through deep
packet inspection to perform per-ﬂow balancing, which en-
ables support of high traﬃc rates in core routers. [20] high-
lights the diﬃculties in hashing encapsulated traﬃc.
The eﬀect of using diﬀerent hash functions to balance traf-
ﬁc on multiple links is studied in [4], and best practices for
optimal LAG/ECMP component link utilization while us-
ing hash-based techniques are described in [11]. In [21], the
use of a given path in data centers, e.g., to avoid conges-
tion, is achieved by modifying packet headers on the basis
of traceroute-like results.
We show that predictability is a key feature for new load
balancing methods, especially to ease diagnosis. From this
perspective, we ﬁnd interesting the load balancing technique
proposed in [6]. Work on balancing traﬃc is of major im-
portance as it is commonly used in ISP networks [1, 8] as
well as in data centers [9, 12].
5. LESSONS LEARNED
We have shown that using ping for delay measurement
may be biased if one ignores ﬂow-based load balancing. This
bias is intrinsic to ping’s design, hence predictable a priori.
In carefully crafted measurements, the dispersion reported
by ping can be up to 40% of the RTT experienced by the
ﬂow with lowest latency. In other words, when we observe
high variability in the delay measurements of ping, it is likely
a measurement artifact of the tool itself.
This observation has several consequences.
1. Ping is unﬁt to measure RTT distributions. The
distribution measured by ping is often a sample from a wider
set of per-ﬂow distributions. While this can identify upper
and lower bounds for the delay with good approximation,
it provides a mediocre estimate for delay. It overestimates
jitter (or any other metric measuring the variability of the
distribution). For this reason, it cannot reliably represent
the performance experienced by applications. This should
sound a warning both for researchers studying end-to-end
Internet performance, and operators using ping for measure-
ment or debugging purposes. We suggest that tokyo-ping,
an adaptation of paris-traceroute, be deployed on large-scale
measurement infrastructures.
2. The importance of the ﬂow identiﬁer. That a
signiﬁcant diﬀerence in latency may exist between ﬂows for
the same source and destination pair represents both a dan-
ger and an opportunity for applications. On the one hand,
applications using multiple transport channels, e.g., the con-
Figure 8: RTT variance with ping and tokyo-ping
from a single source to 850 destinations
of the Google servers discovered by [3]. As our goal was
to check the generality of the per-ﬂow RTT behavior, we
targeted distributed destinations selecting one IP per Au-
tonomous System (AS) in this dataset. This resulted in 850
destinations. We used tokyo-ping to send ICMP probes with
16 diﬀerent ﬂow-ids. We sent ten probes for each ﬂow-id and
ten ping probes. We repeated this experiment 20 times.
Fig. 8 depicts the CDF of the inter-quantile ranges of
RTT measurements that we observed using Ashburn as the
source. The green curve (on the left) and the blue curve (in
the middle) show the lowest and highest per-ﬂow-id inter-
quantile ranges, while the red curve shows the inter-quantile
range for ping. Note that the variability of the distribution
of RTTs reported by ping is systematically higher (with
very few exceptions) than the variation of the most vari-
able ﬂow-id. In particular, for 40% of the destinations, the
worst per-ﬂow inter-quantile range is above 0.3 ms while for
ping is above 5.2 ms. For 15 destinations, ping experienced
lower RTT variability than the per-ﬂow measurements (in
the bottom-left). The lack of ground knowledge of router
conﬁgurations and circuit paths in these larger-scale exper-
iments prevented us from ﬁnding clear explanations.
We observed a similar behavior for all sources. We also
performed experiments toward Alexa’s top 100 sites and ob-
tained very similar results.
Which portion of the RTT distribution exhibits per-ﬂow
behavior? When extending the range to incorporate 90%
of the RTT distribution (the 95th-5th percentile range), in-
stead of the 50% for the inter-quantile, the ﬁgure changes.
The per-ﬂow variance increases. For most destinations (90%)
the worse per-ﬂow variance is higher than the ping variance.
This is likely a sign that while there are speciﬁc per-ﬂow
behaviors, other factors are also at play. Slightly reducing
the portion of the RTT distribution we consider is enough to
reﬂect per-ﬂow behavior. From our source in Ashburn, 90%
of the destinations have an RTT distribution with 90th-10th
range lower with the ﬂow id ﬁxed than for ping. This shows
that in general 80% of the ﬂow observations are consistent.
4. RELATED WORK
Traceroute is renown to be error prone in load-balanced
networks [1]. In [1], the authors propose a replacement for
traceroute, called paris-traceroute, which takes into account
path diversity at the IP layer. By measuring the minimum
0.050.200.502.005.0020.000.00.20.40.60.81.0Interquartile rangeCDFPingWorst flow−idBest flow−idStateless path selection in data center networks.
Computer Networks, 57(5):1204–1216, April 2013.
[7] C. Dovrolis, K. Gummadi, A. Kuzmanovic, and S. D.
Meinrath. Measurement Lab: Overview and an
Invitation to the Research Community. SIGCOMM
CCR 2010, 2010.
[8] T. Flach, E. Katz-Bassett, and R. Govindan.
Quantifying violations of destination-based forwarding
on the Internet. In ACM Internet Measurement
Conference (IMC 2012), 2012.
[9] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula,
C. Kim, P. Lahiri, D. A. Maltz, P. Patel, and
S. Sengupta. VL2: a scalable and ﬂexible data center
network. In ACM SIGCOMM, 2009.
[10] K. Kompella, J. Drake, S. Amante, W. Henderickx,
and L. Yong. The Use of Entropy Labels in MPLS
Forwarding, November 2012. Internet Engineering
Task Force (IETF), RFC 6790.
[11] R. Krishnan, S. Khanna, L. Yong, A. Ghanwani, Ning
So, and B. Khasnabish. Mechanisms for optimal
LAG/ECMP component link utilization in networks,
April 2013. draft-krishnan-opsawg-large-ﬂow-load-
balancing-08.txt, work in
progress.
[12] J. Mudigonda, P. Yalagandula, M. Al-Fares, and J. C.
Mogul. Spain: Cots data-center ethernet for
multipathing over arbitrary topologies. In NSDI, 2010.
[13] RIPE NCC. RIPE Atlas. https://atlas.ripe.net.
[14] RIPE NCC. RIPE Test Traﬃc Measurement Service.
https://atlas.ripe.net.
[15] J. Postel. User Datagram Protocol, 1980. Internet
Engineering Task Force (IETF) , RFC 768.
[16] J. Postel. Internet Control Message Protocol, 1981.
Internet Engineering Task Force (IETF) , RFC 792.
[17] SamKnows. Samknows. http://www.samknows.com.
[18] R. Stewart. Stream Control Transmission Protocol,
2007. Internet Engineering Task Force (IETF) , RFC
4960.
[19] Georgia Tech et al. Project BISmark.
http://projectbismark.net.
[20] Jeﬀ Wheeler and Job Snijders. Understanding MPLS
hashing. NANOG 57, February 2013.
[21] K. Xi, Y. Liu, and J. Chao. Enabling ﬂow-based
routing control in data center networks using probe
and ECMP. In INFOCOM Workshop on cloud
computing (2011), page 614ˆa ˘A¸S619, 2001.
trol, video, audio and data channels found in videoconfer-
encing and streaming, cannot assume that network perfor-
mance is consistent across channels.
This implies that
multi-channel applications are advised not to rely on a sin-
gle control channel to accurately estimate delay and jitter of
all opened TCP connections. If the application needs con-
sistency across channels (e.g. to keep the lip sync in video
streaming), SCTP [18] is a good alternative as it is able to
multiplex streams while keeping a constant ﬂow identiﬁer.
On the other hand, applications might experience better per-
formance by carefully selecting the source and destination
ports, e.g., during an initial negotiation phase. Moreover,
our ﬁndings suggest that accurately monitoring per-channel
performance from outside the application is harder than is
commonly believed. Indeed, the performance that a moni-
toring tool (e.g., ping, IP-SLA, etc.) measures is not nec-
essarily representative of the performance experienced by
speciﬁc applications.
3.
Impact on common wisdom and prior work.
Our ﬁndings show how reality is often more complex than
expected. Technologies and conﬁgurations at diﬀerent lay-
ers of the protocol stack often interact in unexpected ways.
Understanding these interactions and the behavior of diﬀer-
ent vendor implementations is diﬃcult. This can frustrate
or make inaccurate the modeling eﬀort of research. Our ex-
periments show that, at least in some cases, latency over a
network path is not a well-deﬁned concept, and we should
more precisely deﬁne latency over a transport session. Gen-
erally speaking, we recommend researchers be cautious when
drawing conclusions from experiments based solely on ping
results.
Acknowledgements
We thank Olivier Bonaventure and Jean Lorchat for their in-
sightful comments. We are very grateful to several network
operators without whom this work would not have been pos-
sible. We are indebted to our shepherd Ethan Katz-Bassett
for his guidance in producing the camera-ready. This work is
partially supported by the European Commission’s Seventh
Framework Programme (FP7/2007-2013) Grant No. 317647
(Leone).
6. REFERENCES
[1] B. Augustin, T. Friedman, and R. Teixeira. Measuring
load-balanced paths in the internet. In ACM Internet
Measurement Conference (IMC 2007), 2007.
[2] F. Baker, 1995. Internet Engineering Task Force
(IETF) , RFC 1812.
[3] Matt Calder, Zi Hu Xun Fan, Ethan Katz-Bassett,
John Heidemann, and Ramesh Govindan. Mapping
the Expansion of Google’s Serving Infrastructure (To
Appear). In Proceedings of the ACM Internet
Measurement Conference (IMC ’13), October 2013.
[4] Z. Cao, Z. Wang, and E. Zegura. Performance of
hashing-based schemes for Internet load balancing. In
INFOCOM, pages 332–341, 2000.
[5] Luca Cittadini. Tokyo-ping.
http://psg.com/tokyo-ping-v2.tar.gz.
[6] Gregory Detal, Christoph Paasch, Simon van der
Linden, Pascal M ˜Al’rindol, Gildas Avoine, and Olivier
Bonaventure. Revisiting ﬂow-based load balancing: