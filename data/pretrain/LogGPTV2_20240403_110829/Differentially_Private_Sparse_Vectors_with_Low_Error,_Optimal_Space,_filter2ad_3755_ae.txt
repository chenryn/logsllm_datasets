2 log(cid:16)
(cid:17)
𝜓
,
(cid:170)(cid:174)(cid:174)(cid:172) · 𝛼 .
Proof. It is easy to see that |𝑥𝑖 − 𝑥′
𝑖 |  0, and 𝑠 ∈ N.
Input
:𝑘-sparse vector 𝑥 ∈ R𝑑+, where 𝑠 > 2𝑘.
Sequence of hash functions from domain [𝑑]
to [𝑠], ℎ = (ℎ1, . . . , ℎ𝑚), where 𝑚 =
:𝜀-differentially private representation of 𝑥.
𝛼
(cid:108) 𝛽𝜀
(cid:109).
Output
(1) Scale the entries of 𝑥 such that ˆ𝑥𝑖 = 𝑥𝑖 · 𝜀.
(2) Let ℎ, ˜𝑧 = ALP1-projection𝛼,𝛽·𝜀,𝑠( ˆ𝑥, ℎ).
(3) Release ℎ and ˜𝑧.
Algorithm 5: ALP-estimator
Parameters:𝛼, 𝜀 > 0.
Input
Output
(1) Let ˜𝑥𝑖 = ALP1-estimator𝛼 ( ˜𝑧, ℎ, 𝑖).
(2) Return ˜𝑥𝑖
𝜀 .
:Embedding ˜𝑧 ∈ {0, 1}𝑠×𝑚. Sequence of hash
functions ℎ = (ℎ1, . . . , ℎ𝑚). Index 𝑖 ∈ [𝑑].
:Estimate of 𝑥𝑖.
4.3 Generalization to 𝜀-differential privacy
We now generalize the ALP mechanism from 1-differential privacy
to satisfying 𝜀-differential privacy. A natural approach is to use a
function of 𝜀 as the parameter for randomized response in Algo-
rithm 2. The projection algorithm is 𝜀-differentially private if we
1
𝜀+2. However, the expected
remove the scaling step and set 𝑝 =
per-entry error would be bounded by 8𝜀+8
𝜀2 by Equation 1 (with-
out considering hash collisions), which is as large as 𝑂(cid:16) 1
(cid:17) for
𝜀2
small values of 𝜀. Other approaches modifying the value of 𝑝 have
a similar expectation.
In the following, we use a simple pre-processing and post-processing
step to achieve optimal error. The idea is to scale the input vector as
well as the parameter 𝛽 by 𝜀 before running Algorithm 2. We scale
back the estimates from Algorithm 3 by 1/𝜀. These generalizations
are given as Algorithm 4 and Algorithm 5, respectively.
Lemma 4.12. Algorithm 4 satisfies 𝜀-differential privacy.
Proof. It follows from the proof of Lemma 4.1 that for any
Pr[ALP1-projection( ˆ𝑥)∈𝑆] ≤ 𝑒 ∥ ˆ𝑥− ˆ𝑥′∥1.
subset of outputs 𝑆 we have Pr[ALP1-projection( ˆ𝑥′)∈𝑆]
As such for any pair of neighboring vectors 𝑥 and 𝑥′ we have:
Pr[ALP1-projection( ˆ𝑥′) ∈ 𝑆]
Pr[ALP1-projection( ˆ𝑥) ∈ 𝑆]
≤ 𝑒 ∥ ˆ𝑥− ˆ𝑥′∥1 = 𝑒𝜀·∥𝑥−𝑥′∥1 ≤ 𝑒𝜀 .
Pr[ALP-projection(𝑥′) ∈ 𝑆]
Pr[ALP-projection(𝑥) ∈ 𝑆] =
□
Lemma 4.13. Let 𝛼 = Θ(1) and 𝑠 = Θ(𝑘). The output of Algo-
rithm 4 can be stored using 𝑂(𝑘𝛽𝜀) bits.
Proof. It follows from Lemma 4.2 that the output can be stored
(cid:17) bits. Recall that we assume 𝑘 = Ω(log(𝑑)),
using 𝑂(cid:16) (𝑠+log(𝑑))𝛽𝜀
i.e., 𝑂(cid:16) (𝑠+log(𝑑))𝛽𝜀
(cid:17)
𝛼
𝛼
= 𝑂(𝑘𝛽𝜀).
□
9
Lemma 4.14. Let 𝛼 = Θ(1) and 𝑠 = Θ(𝑘). Then the expected per-
entry error of Algorithm 5 is E[|𝑥𝑖 − ˜𝑥𝑖|] ≤ max(0, 𝑥𝑖 − 𝛽) + 𝑂(1/𝜀)
and the evaluation time is 𝑂(𝛽𝜀).
Proof. It is clear that the error of Algorithm 5 is 1
𝜀 times the
error of Algorithm 3 for entries at most 𝛽. As such the expected per-
entry error follows from Lemma 4.8. The evaluation time follows
directly from Lemma 4.3.
□
𝛾+2 , 𝑞 = 1 − 𝑝, 𝑥𝑖 ≤ 𝛽, and
1
2 · (4𝑝𝑞)(𝜏𝜀/2𝛼)−1/2
𝑠
𝜏 ≥ 𝛼
Pr[|𝑥𝑖 − ˜𝑥𝑖| ≥ 𝜏]  0.
Input
Output
:𝑘-sparse vector 𝑥 ∈ R𝑑+.
:𝜀-differentially private representation of 𝑥.
(1) Let 𝑣𝑖 = 𝑥𝑖 + 𝜂𝑖 for all 𝑖 ∈ [𝑑], where 𝜂𝑖 ∼ Lap(1/𝜀).
(2) Truncate entries below 𝑡:
(cid:40)𝑣𝑖,
0,
if 𝑦𝑖 ≥ 𝑡
otherwise
˜𝑣𝑖 =
(3) Return ˜𝑣.
Algorithm 7: Threshold ALP-projection
Parameters:𝛼, 𝜀1, 𝜀2 > 0, and 𝑠 ∈ N.
Input
:𝑘-sparse vector 𝑥 ∈ R𝑑+, where 𝑠 > 2𝑘.
Sequence of hash functions from domain [𝑑]
to [𝑠], ℎ = (ℎ1, . . . , ℎ𝑚), where 𝑚 =
:(𝜀1 + 𝜀2)-differentially private representation
of 𝑥.
.
𝛼
(cid:108) 𝛽𝜀2
(cid:109).
Output
ln(𝑑/2)
𝜀1
(1) Let 𝑡 =
(2) Let ˜𝑣 = Threshold𝜀1,𝑡 (𝑥).
(3) Let ℎ, ˜𝑧 = ALP-projection𝛼,𝜀2,𝑡,𝑠(𝑥, ℎ)
(4) Return ˜𝑣, ℎ and ˜𝑧.
Lemma 5.2. Let 𝑡 =
𝑂(𝑘)-sparse with high probability.
𝜀
ln(𝑑/2)
. Then the output of Algorithm 6 is
Proof. Using Definition 2.5 we find that the probability of stor-
ing a zero entry is:
Pr[Lap(1/𝜀) ≥ 𝑡] = Pr[Lap(1/𝜀) ≤ −𝑡] =
1
2𝑒−𝑡𝜀 =
1
𝑑
.
By linearity of expectation, the expected number of stored true zero
entries is at most one, and as such the vector is 𝑂(𝑘)-sparse with
high probability.
□
gorithm 6 is 𝑂(cid:16) log(𝑑)
(cid:17) for worst-case input. We combine the al-
As discussed in Section 3, the expected per-entry error of Al-
gorithm with the ALP mechanism from the previous section to
achieve 𝑂(1/𝜀) expected per-entry error for any input. We use the
threshold parameter 𝑡 as value for parameter 𝛽 in Algorithm 4. The
algorithm is presented in Algorithm 7.
𝜀
Lemma 5.3. Algorithm 7 satisfies (𝜀1 + 𝜀2)-differential privacy.
Proof. The two parts of the algorithm are independent as there
is no shared randomness. The first part of the algorithm satisfies
𝜀1-differential privacy by Lemma 5.1 and the second part satisfies
𝜀2-differential privacy by Lemma 4.12. As such it follows directly
from composition (Lemma 2.3) that Algorithm 7 satisfies (𝜀1 + 𝜀2)-
differential privacy.
□
Lemma 5.4. Let 𝛼 = Θ(1), 𝑠 = Θ(𝑘), 𝜀1 = Θ(𝜀2). Then the out-
put of Algorithm 7 is stored using 𝑂(𝑘 log(𝑑 + 𝑢)) bits with high
probability.
10
Algorithm 8: Threshold ALP-estimator
Parameters:𝛼, 𝜀2 > 0.
Input
:Vector ˜𝑣 ∈ R𝑑+. Embedding ˜𝑧 ∈ {0, 1}𝑠×𝑚.
Sequence of hash functions ℎ = (ℎ1, . . . , ℎ𝑚).
Index 𝑖 ∈ [𝑑].
:Estimate of 𝑥𝑖.
(1) Estimate the entry using either the vector or the embedding
ALP-estimator𝜀2,𝛼 ( ˜𝑧, ℎ, 𝑖),
if ˜𝑣𝑖 ≠ 0
otherwise
(cid:40) ˜𝑣𝑖,
Output
such that:
˜𝑥𝑖 =
(2) Return ˜𝑥𝑖.
Proof. It follows from Lemma 5.2 that we can store ˜𝑣 using
𝑂(𝑘 log(𝑑+𝑢)) bits with high probability. Since 𝛽 = 𝑡 it follows from
Lemma 4.13 that we can store ℎ and ˜𝑧 using 𝑂(𝑘𝑡𝜀2) = 𝑂(𝑘 log(𝑑))
bits.
□
To estimate an entry, we access ˜𝑣 when a value is stored for
the entry and the ALP embedding otherwise. This algorithm is
presented in Algorithm 8.
Lemma 5.5. Let 𝛼 = Θ(1), 𝑠 = Θ(𝑘), and 𝜀1 = Θ(𝜀2). Let ˜𝑣, ℎ,
and ˜𝑧 be the output of Algorithm 7 given these parameters. Then the
evaluation time of Algorithm 8 is 𝑂(log(𝑑)). The expected per-entry
error is 𝑂(1/𝜀) and the expected maximum error is 𝑂(cid:16) log(𝑑)
(cid:17)
.
𝜀
Proof. The evaluation time follows from Lemma 4.14. That is,
the evaluation time is 𝑂(𝛽𝜀) = 𝑂(𝑡𝜀) = 𝑂(log(𝑑)).
The error depends on both parts of the algorithm. The expected
per-entry error for the 𝑖th entry is max(0, 𝑥𝑖−𝛽)+𝑂(1/𝜀2) when ˜𝑣𝑖 =
0 by Lemma 4.14. That is, when 𝜂𝑖 is less than 𝛽 − 𝑥𝑖 in Algorithm 6.
When ˜𝑣𝑖 ≠ 0 the error is the absolute value of 𝜂𝑖. That is, we can
analyze it using conditional probability and the probability density
function of the Laplace distribution from Definition 2.4.
E[|𝑥𝑖 − ˜𝑥𝑖|] = E[|𝑥𝑖 − ˜𝑥𝑖| | ˜𝑣𝑖 = 0] · Pr[ ˜𝑣𝑖 = 0]
+ E[|𝑥𝑖 − ˜𝑥𝑖| | ˜𝑣𝑖 ≠ 0] · Pr[ ˜𝑣𝑖 ≠ 0]
≤ (max(0, 𝑥𝑖 − 𝛽) + 𝑂(1/𝜀2)) · Pr[Lap(1/𝜀1) < 𝛽 − 𝑥𝑖]
𝛽−𝑥𝑖
+
∫ ∞
∫ 𝛽−𝑥𝑖