API nodes, namely param: int, decl: int and call: bar. As
a result, the corresponding three dimensions in the vector
φ(x) are non-zero, whereas all other dimensions are zero.
The vector space constructed by the map φ may contain
hundred thousands of dimensions, yet the vectors are ex-
tremely sparse. This sparsity can be exploited for eﬃciently
storing and comparing the vectors in practice.
2.3 Identiﬁcation of Structural Patterns
By calculating distances between vectors, the representa-
tion obtained in the previous step already allows functions
to be compared in terms of the subtrees they share. How-
ever, we cannot yet compare functions with respect to more
involved patterns. For example, the code base of a server
application may contain functions related to network com-
munication, message parsing and thread scheduling. In this
setting, it would be better to compare the functions with
respect to these functionalities rather than looking at the
plain subtrees of the ASTs.
Fortunately, we can adapt the technique of latent semantic
analysis [5] to solve this problem. Latent semantic analysis
is a classic technique of natural language processing that is
used for identifying topics in text documents. Each topic
is represented by a vector of related words. In our setting
these topics correspond to types of functionality in the code
base and the respective vectors are associated with subtrees
related to these functionalities.
361
Latent semantic analysis identiﬁes topics by determining
dominant directions in the vector space, that is, subtrees
frequently occurring together in ASTs of our code base. We
refer to these directions of related subtrees as structural pat-
terns. By projecting the original vectors on the identiﬁed
directions, one obtains a low-dimensional representation of
the data. Each AST of a function is described as a mixture
of the structural patterns. For example, a function related
to communication and parsing is represented as a mixture
of patterns corresponding to these types of functionality.
Formally, latent semantic analysis seeks d orthogonal di-
rections in the vector space that capture as much of the
variance inside the data as possible. One technical way to
obtain these d directions is by performing a singular value
decomposition (SVD) of the matrix M . That is, M is de-
composed into three matrices U , Σ andV as follows
⎛
⎜⎜⎜⎝
M ≈ U Σ V T =
⎞
← u1 →
⎟⎟⎟⎠
← u2 →
...← u|S| →
⎛
⎜⎜⎜⎝
σ1
0
...
0
⎞
⎟⎟⎟⎠
⎛
← v1 →
⎜⎜⎜⎝
← v2 →
...← v|X| →
⎞
⎟⎟⎟⎠
T
.
0
σ2
...
0
. . .
0
. . .
0
...
. . .
. . . σd
The decomposition provides a wealth of information and
contains the projected vectors as well as the structural pat-
terns identiﬁed in the matrix M .
1. The d columns of the unitary matrix U correspond to
the dominant directions in the vector space and deﬁne
the d structural patterns of subtrees identiﬁed in the
code base.
2. The diagonal matrix Σ contains the singular values of
M . The values indicate the variances of the directions
and allow us to assess the importance of the d struc-
tural patterns.
3. The rows of V contain the projected representations
of the embedded ASTs, where each AST is described
by a mixture of the d structural patterns contained in
the matrix U .
As we will see in the following, these three matrices pro-
vide the basis for extrapolation of vulnerabilities and con-
clude the rather theoretical presentation of our method.
2.4 Extrapolation of Vulnerabilities
Once the decomposition has been calculated, which takes
a fraction of the time required for code parsing, an analyst is
able to access the information encoded in the three matrices.
In particular, the following three activities can be performed
instantaneously to assist code auditing.
• Vulnerability extrapolation. The rows of the matrix V
describe all functions as mixtures of structural pat-
terns. Finding structurally similar functions is thus as
simple as comparing the rows of V using a suitable
measure, such as the cosine distance [25]. This oper-
ation forms the basis for the extrapolation of vulner-
abilities. Clearly, there is no guarantee that functions
with similar structure are plagued by the same vul-
nerabilities, however, examples presented in Section 3
provide some evidence for this correspondence.
• Code base decomposition. At the beginning of an au-
dit, little is known about the overall structure of the
code base.
In this setting, the matrix U storing the
most prevalent structural patterns in its columns gives
important insight into the structure of the code base.
This information can be used to uncover major clusters
of similar functions, such as sets of functions employing
similar programming patterns. This allows an analyst
to select interesting parts of the code early in the audit
and concentrate on promising functions ﬁrst.
• Detection of unusual functions. Finally, the represen-
tation of functions in terms of structural patterns is
fully transparent, allowing an analyst to discover the
most prominent patterns used in any particular func-
tion by comparing rows of V with columns of U . This
enables determining deviations from common program-
ming patterns by analyzing diﬀerences in the represen-
tations of functions. For example, it might be inter-
esting to audit a function related to message parsing
that deviates from other such functions by also sharing
structural patterns with network communication.
3. EVALUATION
We proceed to evaluate our method with real source code.
In particular, we are interested in studying the ability of our
method to assess the similarity of code and to identify po-
tentially vulnerable functions in practice. We ﬁrst conduct a
quantitative evaluation, where we apply our method in a con-
trolled experiment on diﬀerent code bases. We then present
a qualitative evaluation and examine the extrapolation of
real vulnerabilities in two case studies.
For the evaluation we consider four popular open-source
projects, namely LibTIFF, FFmpeg, Pidgin and Asterisk.
For each of these projects we pick one known vulnerability
as a starting point for the vulnerability extrapolation and
proceed to manually label candidate functions which should
be reviewed for the same type of vulnerability.
In the following, we describe the code bases of these projects
and the choice of candidate functions in detail:
1. LibTIFF (http://www.libtiff.org) is a library for
processing images in the TIFF format. Its source code
covers 1,292 functions and 52,650 lines of code. Ver-
sion 3.8.1 of the library contains a stack-based buﬀer
overﬂow in the parsing of TLV elements that allows
an attacker to execute arbitrary code using speciﬁcally
crafted images (CVE-2006-3459). Candidate functions
are all parsers for TLV elements.
2. Pidgin (http://www.pidgin.im) is a client for instant
messaging implementing several communication pro-
tocols. The implementation contains 11,505 functions
and 272,866 lines of code. Version 2.10.0 of the client
contains a vulnerability in the implementation of the
AIM protocol (CVE-2011-4601). An attacker is able to
remotely crash the client using crafted messages. Can-
didate functions are all AIM protocol handlers convert-
ing incoming binary messages to strings.
3. FFmpeg (http://www.ffmpeg.org) is a library for con-
version of audio and video streams. Its code base spans
6,941 functions with a total of 298,723 lines of code. A
362
Pidgin
LibTIFF
FFmpeg
Asterisk
Average
API nodes
API subtrees
API/S subtrees
75%
0.1
6.35
6.17
0.06
3.17
90%
0.36
6.97
8.10
10.64
6.52
100%
2.00
7.58
19.61
15.29
11.12
75%
0.35
5.65
5.00
0.24
2.81
90%
0.22
6.66
8.66
10.23
6.44
100%
0.98
7.27
11.09
15.54
8.72
75%
0.22
6.49
7.71
1.19
3.90
90%
0.67
9.36
15.21
16.50
10.44
100%
25.98
17.32
28.35
28.45
25.03
Table 1: Performance of vulnerability extrapolation in a controlled experiment. The performance is given as amount of
code (%) to be audited to ﬁnd 75%, 90% and 100% of the potentially vulnerable functions.
vulnerability has been identiﬁed in version 0.6 (CVE-
2010-3429). During the decoding of video frames, in-
dices are incorrectly computed, enabling the execution
of arbitrary code. Candidate functions are all video
decoding routines, which write decoded video frames
to a pixel buﬀer.
4. Asterisk (http://www.asterisk.org) is a framework
for Voice-over-IP communication. The code base cov-
ers 8,155 functions and 283,883 lines of code. Ver-
sion 1.6.1.0 of the framework contains a vulnerabil-
ity (CVE-2011-2529), which allows a remote attacker
to corrupt memory of the server and to cause a de-
nial of service via a crafted packet. Candidate func-
tions are all functions reading incoming packets from
UDP/TCP sockets.
3.1 Quantitative Evaluation
In our ﬁrst experiment, we study the ability of our method
to identify functions sharing similarities with a known vul-
nerability on the four code bases. To conduct a controlled
experiment we thoroughly inspect each code base and man-
ually label all candidate functions, that is, all functions that
potentially contain the same vulnerability. Note that this
manual analysis process required several weeks of work and
can hardly be seen as an alternative to the concept of vul-
nerability extrapolation.
For each of the four code bases, we apply our method and
rank the functions according to the selected target vulner-
abilities. We vary the embedding of syntax trees by con-
sidering ﬂat API nodes, API subtrees and API/S subtrees
(see Section 2.2). Moreover, we compute the ranking for
diﬀerent numbers of structural patterns identiﬁed by latent
semantic analysis (see Section 2.3). As performance mea-
sure we assess the eﬃcacy of the vulnerability extrapolation
by measuring the amount of code that needs to be inspected
for ﬁnding all candidate functions.
d
e
w
e
i
v
e
R
e
d
o
C
f
o
t
n
u
o
m
A
100
80
60
40
20
0
0
FFmpeg
API nodes
API subtrees
API/S subtrees
50
Number of Dimensions
100
150
100
80
60
40
20
d
e
w
e
i
v
e
R
e
d
o
C
f
o
t
n
u
o
m
A
200
0
0
LibTIFF
API nodes
API subtrees
API/S subtrees
50
Number of Dimensions
100
150
200
(a) FFmpeg
(b) LibTIFF
Figure 4: Performance of vulnerability extrapolation in a
controlled experiment.
363
Figure 4 shows the results of this experiment for FFmpeg
and LibTIFF, where results for the other two code bases
are similar. The API subtrees clearly outperform the other
representations of the code and enable narrowing the set
of functions to be inspected to 8.7% on average. By con-
trast, the ﬂat representation of API nodes requires 11.1% of
the functions to be reviewed, while for the API/S subtrees
even every 4th function (25%) needs to be inspected. Fur-
thermore, Figure 4 also shows that the number of extracted
structural patterns is not a critical parameter for vulnera-
bility extrapolation. Our method performs well on all code
bases when this number is between 50 to 100 dimensions,
despite the fact that FFmpeg contains 6,941 and LibTIFF
only 1,292 functions. In the following case studies, we ﬁx
this parameter to 70.
Table 1 presents a ﬁne-grained analysis of the performance
for each code base, where the amount of code that needs to
be audited for revealing 75%, 90% and 100% of the candidate
functions is shown. All numbers are expressed in percent of
the code base to account for their diﬀerent sizes. The API
subtrees perform best, where 75% of the candidate functions
are discovered by reading under 3% of the code bases. In the
case of Pidgin and Asterisk, this number further reduces to
less than 1% of the entire code base providing a signiﬁcant
advantage over manual auditing.
Nevertheless, the results also show room for improvement,
particularly when all candidate functions need to be discov-
ered. In this case, the amount of code to be read reaches
8.7% on average and up to 16% in the worst case. However,
even in the worst case the amount of code that needs to be
inspected is reduced by 84% and vulnerability extrapolation
clearly accelerates manual code auditing in practice.
3.2 Qualitative Evaluation
In a case study with FFmpeg and Pidgin, we now demon-
strate the practical merit of vulnerability extrapolation and
show how our method plays the key role in identifying eight
zero-day vulnerabilities. We have conducted two further
studies with Pidgin and Asterisk uncovering two more zero-
day vulnerabilities. For the sake of brevity however, we omit
these case studies and details of the vulnerabilities here.
3.2.1 Case study: FFmpeg.
Flaws in the indexing of arrays are a frequently occurring
problem in media libraries. In many cases these vulnerabil-
ities allow attackers to write data to arbitrary locations in
memory, an exploit primitive that can often be leveraged for
arbitrary code execution. In this case study, we show how a
publicly known vulnerability in the video decoder for FLIC
media ﬁles of FFmpeg (CVE-2010-3429) is used to uncover
three further vulnerabilities of this type, two of which were
previously unknown. Note that this is the same vulnerabil-