Table 1: Participant demographics
ership, policy compliance, and administrative support).
This composition is similar to the actual work role dis-
tribution across NYC3, with 50 of 67 employees serving
as technicians. Prior to this study, one participant had a
high-level understanding of the military applications of
CoG, and none of the participants had any applied expe-
rience using any threat-modeling framework.
All participants had at least some college education,
with ten holding a graduate degree and eight holding a
bachelor’s. Additionally, 15 possessed at least one in-
dustry certiﬁcation. Participants had an average of 14.7
years of information technology and security experience
in large organizations, with a mean of 8.5 years of formal
or on-the-job training.
4.2 Pre-intervention baseline
To measure the impact of threat modeling within
NYC3 systems, we ﬁrst established a baseline of how
participants deployed defensive strategies prior to our
training. Most commonly, they prioritized defending
high-impact service-based systems such as NYC.gov
(n=7) and adhering to compliance frameworks (n=7), fol-
lowed by applying risk management strategies (n=6) and
assessing which systems are most susceptible to attack
(n=3). Participants reported using the following guide-
lines and programs for assessing NYC’s digital secu-
rity posture: city-speciﬁc policies and executive orders
such as the NYC remote access policy [49] (n=6), NIST
Cybersecurity Framework [44] (n=4), and NYC3’s one-
time accreditation process for adding new technologies
to their network (n=2). Of these guidelines, participants
stated that none of the programs were applied frequently
enough, with P5 stating that “compliance is only as good
as your last assessment.” With too much lapsed time be-
tween audits, defenders cannot establish an accurate as-
sessment of the environment’s security posture over time.
The remainder of respondents (n=13) said they were un-
sure about which programs or policies were applicable.
4.3
Immediate observations
In contrast to the baseline survey, performance evalu-
ation session observations and post-training surveys in-
dicate that threat modeling provided participants with a
better understanding of their security environment, that
participants felt more conﬁdent in their ability to pro-
tect NYC, and that participants could successfully apply
threat modeling relatively quickly with accurate results.
4.3.1 Perceived efﬁcacy
We observe participants’ initial threat modeling per-
ceptions in the context of new insights, framework use-
fulness, and changes in self-efﬁcacy.
New understanding. Overall, 12 of 25 participants re-
ported that threat modeling allowed them to understand
new critical capabilities, requirements, or vulnerabilities
that they had never previously considered. In particular,
four participants had never previously mapped threats to
vulnerabilities. P16, a non-technical administrative sup-
port staffer, used threat modeling to understand the im-
plications of wide-open security permissions on a wiki
and networked share drive.
Threat modeling provided two participants with self-
derived examples of why crisis continuity plans exist for
large organizations. P04 stated that this new understand-
ing would further assist him with planning for crises, al-
lowing him to recommend to “senior management the
plan of action for what should be done ﬁrst.”
Of the 13 participants who did not report discovering
anything new, seven stated threat modeling was simply a
restructured approach to current defensive concepts like
defense-in-depth [36]. Four stated threat modeling did
not help them discover anything new but added addi-
tional emphasis to areas they should be concerned with.
Four participants identiﬁed an over-reliance on per-
sonal relationships (rather than codiﬁed policies) as a
critical vulnerability for organizational success, which
conceptually is something none of them had ever be-
fore considered. During his performance evaluation ses-
sion, P24 discussed how changes in the political environ-
ment from the local to federal level can affect established
trust across the GoNYC; a large turnover in personnel
could halt some progress and potentially kill some initia-
tives. P25 stated “I had not really considered. . . the im-
pact that some sort of major, non-cyber event could have
USENIX Association
27th USENIX Security Symposium    627
on our ability to be successful,” discussing how a ma-
jor terrorist event within NYC could decrease NYC3’s
ability to sustain critical requirements and capabilities.
Thus, both participants recommended codifying existing
relationship-based agreements into legislation capable of
withstanding non-digital security threats to their daily re-
sponsibilities. An example of this includes establishing a
formal memorandum of understanding (MoU) with law
enforcement agencies in NYC to facilitate the exchange
of threat indicators.
Perceived framework usefulness. After completing the
performance evaluation session, 23 participants agreed
that threat modeling was useful to them in their daily
work. For example, ten said the framework allowed them
to prioritize their efforts. P24 developed a new litmus test
for adding any defensive efforts, stating that “If the ad-
versary doesn’t care, then it’s all just ﬂuff [inconsequen-
tial].” P21 used threat modeling to show “what we’re
lacking or what we need to concentrate [on],” such as
standard cyber hygiene.
Eight participants expressed that
threat modeling
added much-needed structure and perspective to difﬁ-
cult problems. P11 feels empowered by its structure and
believes it allows him to “accept the things you cannot
change, change the things you can, and have the wisdom
to know the difference. I feel [CoG is] along those lines;
this is your world, this is what you control.” He believes
threat modeling makes a positive difference with avail-
able resources, while helping to prioritize requests for
future capabilities and support.
Five participants reported that threat modeling allowed
them to plan defensive strategies more effectively. P05
stated that threat modeling helps him “plan effectively,
document, track, monitor progress, and essentially un-
derstand our security posture.”
Threat modeling allowed four participants to com-
prehend how threats can affect systems within their
environment; these technicians previously relied upon
best security practices without fully considering threats.
While applying the framework, P10 declared that “in-
sider threats overcome the hard shell, soft core” within
most enterprise networks and that threat modeling helped
him identify new ways to neutralize the impact of insid-
ers bypassing perimeter defenses and exploiting trusted
internal systems.
Four participants stated that purposefully consider-
ing their asset inventory during threat modeling allowed
them to fully understand their responsibilities. Three par-
ticipants stated that threat modeling provides them with
a new appreciation for their position within NYC3. P14
said, “When I did my job, I didn’t think about what the
purpose of our group is [within NYC3]. . . [threat model-
ing] aligns what we’re thinking with what I think my role
is in this organization.”
Figure 3: A cumulative distribution function (CDF) for
participant subtask completion times.
Interestingly, both of the participants who did not
ﬁnd threat modeling useful felt that cybersecurity is too
nebulous of a realm for a well-structured approach like
CoG. P12, when asked to clarify his difﬁculties with
the framework, stated that cloud environments present
unique problems for defenders: we care about “the center
keep of your castle, well there’s this other castle some-
where out there, we don’t know where, [and it is] part of
our CoG.” However, these two participants did success-
fully use threat modeling to discover critical vulnerabil-
ities within their daily work that they had not previously
considered.
Changes in self-efﬁcacy. When comparing responses
from the post-training survey to baseline responses, 10
participants reported a perceived increase in their abil-
ity to monitor critical assets, 17 reported an increase in
their ability to identify threats, 16 reported an increase
in their ability to mitigate threats, 15 participants re-
ported an increase in their ability to respond to inci-
dents. Respectively, averages increased by 8.8%, 19.3%,
29.8%, and 20.0%. Using the Wilcoxon signed-rank
test [65], we found signiﬁcant increases in participants’
perceived ability to identify threats (W=61.0, p=0.031),
mitigate threats (W=47.0, p=0.010), and respond to inci-
dents (W=59.0, p=0.027).
4.3.2 Actual efﬁcacy
We measure the actual efﬁcacy of threat modeling us-
ing several metrics: the accuracy of participants’ output,
task completion times, similarities between participants’
identiﬁed CoGs, and the contents of their actionable de-
fense plans.
Output accuracy. Simply completing CoG tasks is in-
sufﬁcient to demonstrate success; the resulting output
must also be valid and meaningful. Thus, we assess the
628    27th USENIX Security Symposium
USENIX Association
accuracy of participants’ results via an expert evaluation
from two NYC3 senior leaders. Both of these leaders re-
ceived in-person training on CoG and are uniquely qual-
iﬁed to assess the accuracy of the provided responses
given their intimate knowledge of the NYC3 environ-
ment and cybersecurity expertise. We provided the eval-
uators with an anonymized set of the study results and
asked them to jointly qualify the accuracy of the iden-
tiﬁed centers of gravity, critical vulnerabilities, threat
capabilities/requirements, and ideal defense plans using
a 6-point Likert scale ranging from zero to ﬁve with
zero being “extremely unlikely (UL)” and ﬁve being
“extremely likely (EL)” (See App. C). Additionally, we
asked the leaders to indicate whether each ADP was suf-
ﬁciently detailed to implement. We included one ﬁcti-
tious participant entry as an attention check and valid-
ity control, which both panel members identiﬁed and re-
jected.
The panel concluded that: 22 of 25 identiﬁed centers
of gravity were accurate with respect to a participant’s re-
sponsibilities (‘EL’=3, ‘Likely [L]’=9,‘Somewhat likely
[SL]’=10); all critical vulnerabilities were accurate for
the identiﬁed centers of gravity (EL=6, L=7, SL=12);
23 of 25 threat capability and requirement proﬁles were
accurate (EL=6, L=7, SL=10), and 24 of 25 actionable
defense plans would accurately address the identiﬁed
threats (EL=5, L=11, SL=8).
We used a logistic regression, appropriate for ordinal
Likert data, to estimate the effect of work roles, expe-
rience in IT, and educational background on the accu-
racy of the panel results. We included a mixed-model
random effect [26] that groups results by work roles to
account for correlation between individuals who ﬁll sim-
ilar positions. Our initial model for the regression in-
cluded each demographic category. To prevent overﬁt-
ting, we tested all possible combinations of these inputs
and selected the model with minimum Akaike Informa-
tion Criterion [1]. The ﬁnal selected model is given in
Appendix E. Based on this regression, we found that no
particular work role, amount of education, IT experience,
or combination thereof enjoyed a statistically signiﬁcant
advantage when using threat modeling. These high suc-
cess rates across our demographics support ﬁndings by
Sindre and Opdahl that indicate threat modeling is a nat-
ural adaptation to standard IT practices [58].
Time requirements. We use the time required to ap-
ply CoG analysis to measure efﬁciency, which is a com-
ponent of efﬁcacy. On average, participants used the
framework and developed actionable defense plans in
36 minutes, 46 seconds (σ = 9 : 01). Figure 3 shows
subtask completion times as a cumulative distribution
function (CDF). Participants spent the greatest amount
of time describing critical vulnerabilities and developing
actionable defense plans, with these tasks averaging 5:27
and 6:25 respectively. Three out of ﬁve participants in
a leadership role afﬁrmed without prompting that threat
modeling provided them with a tool for quickly fram-
ing difﬁcult problems, with P24 stating “within an hour,
[CoG] helped me think about some items, challenge
some things, and re-surface some things, and that is very
useful for me given my busy schedule.” P22 applied
the framework in 22 minutes and commented during his
closing performance evaluation session that he would
“need much more time to fully develop” his ideas; how-
ever, he also said the session served as a catalyst for ini-
tiating a necessary dialogue for handling vulnerabilities.
CoG consistency. Analysis of the performance evalua-
tion session results reveals that participants with similar
work role classiﬁcations produced similar output. For ex-
ample, 16 of 18 technicians indicated that a digital secu-
rity tool was their CoG (e.g., ﬁrewalls, servers) whereas
four of six participants in support roles identiﬁed a “soft”
CoG (e.g., relationships, funding, and policies). Partic-
ipants produced actionable defense plans averaging 5.9
mitigation strategies per plan and ranging from a mini-
mum of three strategies to a maximum of 14.
Actionable defense plans. We use the contents of partic-
ipants’ actionable defense plans to further evaluate suc-
cess. Participants identiﬁed real issues present within
their environment and developed means for reducing
risk. Within the 25 actionable defense plans, partici-
pants cumulatively developed 147 mitigation strategies;
we provide detailed examples in Section 4.5. Partici-
pants indicated that 33% of the mitigation strategies they
developed using threat modeling were new plans that
would immediately improve the security posture of their
environment if implemented. Additionally, participants
stated that 31% of the mitigation strategies would im-
prove upon existing NYC3 defensive measures and more
adequately defend against identiﬁed threats. Participants
felt that the remaining 36% of their described mitigation
strategies were already sufﬁciently implemented across
the NYC3 enterprise.
The NYC3 leadership panel indicated a majority of
the actionable defense plans were sufﬁciently detailed
for immediate implementation (‘Yes’= 16). This shows
that, even with limited framework exposure, many par-
ticipants were able to develop sufﬁcient action plans. We
illustrate an ADP with insufﬁcient detail using a security
analyst’s plan. After identifying his CoG as an Endpoint
Detection and Response (EDR) system3 and applying the
framework, his ADP consisted of three mitigation strate-
gies: “Make sure there is a fail-over setup and test it. Bet-
ter change control. Better roll back procedures.” While
all of these address critical vulnerabilities, they provide
no implementation details. In cases such as this, indi-
viduals require additional time to improve the ﬁdelity of
their responses or may beneﬁt from expert assistance in
USENIX Association
27th USENIX Security Symposium    629
Figure 4: Perceived efﬁcacy after using threat modeling for 30 days.
transforming their ideas into fully developed plans.
4.4 Observations after 30 days
After 30 days, we observed that participants still had a
favorable opinion of threat modeling, most participants
actually implemented defensive plans that they devel-
oped through our study, and that NYC3 institutionalized
threat modeling within their routine practices.
4.4.1 Perceived efﬁcacy
Thirty days after learning about CoG, there was a
slight decrease in the perceived efﬁcacy of the framework
when compared to participant perceptions immediately
after training: a 1.47% decrease for monitoring critical
assets (W=81.0, p=0.57), 3.22% decrease for identify-
ing threats (W=131.0, p=0.83), 3.58% decrease for mit-
igating threats (W=94.0, p=0.18), and 1.67% decrease
for responding to incidents (W=100.0, p=0.59); none
of these decreases were statistically signiﬁcant. When
comparing these 30-day metrics to the baseline, how-
ever, participants’ perceived ability to monitor critical as-
sets increased 7.4%, perceived ability to identify threats
increased 16.1%, perceived ability to mitigate threats
increased 26.3%, and perceived ability to respond to
threats increased 18.3%. Participants’ perceived ability
to mitigate threats is a statistically signiﬁcantly increase
from the baseline (W=73.5, p=0.049).
Figure 4 shows participants’ evaluations of the efﬁ-
cacy of CoG analysis after 30 days. Overall, all partici-
pants agreed (“Strongly”= 13) that threat modeling sup-
ports critical aspects of their job. Additionally, 24 par-
ticipants agreed (“Strongly”= 15) that threat modeling
enhances the way they think about digital security. De-
spite the aforementioned decrease in perceived efﬁcacy
over the 30-day period, the number of participants who
found the framework useful to their jobs increased from
23 to 24, as NYC3’s adoption of ADPs within their envi-
ronment caused one participant to believe in the frame-
work’s usefulness. Lastly, 245 of 275 responses to our
11 TAM questions indicated threat modeling is valuable
for digital security.
4.4.2 Actual efﬁcacy
We measure actual efﬁcacy after 30 days using partic-
ipants’ knowledge retention. Measuring knowledge re-
tention allows us to evaluate the longevity of organiza-
tional impacts from integrating the framework. After 30
days, participants averaged 78% accuracy on four com-
prehension questions. This is an increase from 69% im-
mediately after learning the framework, suggesting threat
modeling may become more memorable after additional
applied experience. Each comprehension question re-
quired participants to pinpoint the best answer out of
three viable responses; this allowed us to measure if par-
ticipants understood critical relationships. In the 30-day
follow-up, all participants accurately answered our criti-
cal vulnerability question, 23 correctly identiﬁed a CoG
visually, 17 correctly identiﬁed a critical requirement for
a capability, and 13 correctly identiﬁed a critical capabil-
ity for a notional CoG.
4.4.3 Actual adoption
After 30 days, 21 participants reported that they imple-
mented at least one mitigation strategy that they devel-
oped using threat modeling. In addition, 20 participants
reported after 30 days that they integrated concepts from
threat modeling within their daily work routines. For
example, seven participants now use the framework for
continually assessing risk; this is in contrast to the base-
line results, where participants typically assessed risk
only during audits and initial accreditation. Five partic-
ipants stated that they now use threat modeling to prior-
itize their daily and mid-range efforts. Participants who
did not adopt said they were too busy with urgent tasks
(n=4) or needed more applied training (n=1).
NYC3 started to institutionalize threat modeling after
participants had discussed their results with one another
and realized the important implications of their ﬁndings.
One week after completing their performance evaluation
sessions, six participants transformed a wall within their
primary meeting room into an “urgent priorities” board
(Figure 5) for implementing defensive actions that ad-
dress critical vulnerabilities identiﬁed during this study.
630    27th USENIX Security Symposium