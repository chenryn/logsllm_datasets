本文中使用的模型是11层的Resnet结构[4]，包含3个residual block，所有的feature map大小均为3*3。之后用一个global
max pooling层，得到graph order embedding。在此之前不用pooling层，因为输入的图的大小不同。具体如公式8所示。
## 实验
本文在两个任务上进行实验。任务1为跨平台二进制代码分析，同一份源代码在不同的平台上进行编译，我们的目标是使模型对同一份源代码在不同平台上编译的两个控制流图pair的相似度得分高于不同源代码pair的相似度得分。任务2为二进制代码分类，判断控制流图属于哪个优化选项。各数据集的情况如表1所示。任务1是排序问题，因此使用MRR10和Rank1作为评价指标。任务2是分类问题，因此使用准确率作为评价指标。
表1. 数据集情况
表2和表3分别对应任务1和任务2的实验结果。表中第一个分块是整体模型，包括graph
kernel，Gemini以及MPNN模型。第二个分块是semantic-aware模块的对比实验，分别使用了word2vec[5]，skip
thought[6]，以及BERT，其中BERT2是指原始BERT论文中的两个task（即MLM和ANP），BERT4是指在此基础上加入两个graph-level task（BIG和GC）。第三个分块是对order-aware模块的对比实验，基础CNN模型使用3层CNN以及7、11层的Resnet，CNN_random是对训练集中控制流图的节点顺序随机打乱再进行训练，MPNN_ws是去除控制流图节点中的语义信息（所有block向量设为相同的值）再用MPNN训练。最后是本文的最终模型，即BERT+MPNN+Resnet。
表2、3：各模型在任务1和任务2上的结果
整体结果：本文提出的模型与Gemini模型相比，在任务1和任务2上的评价指标分数均大幅提升。semantic-aware模块使用NLP模型（word2vec，BERT等）均优于使用人工提取的特征。只使用order-aware时模型也取得了不错的效果。与其它所有模型相比，本文提出的模型均取得了更优的效果。
Semantic-aware：只看表中第二个分块，BERT的结果优于word2vec和skip
thought，因为BERT能在预训练过程中提取更多的信息。加上BIG和GC任务后的BERT4效果略微提升，说明在预训练过程中加入graph-level的任务有所帮助。图7中是4个控制流图的block（左上，左下，右上，右下），我们使用K-means对预训练后的block
embedding进行分类（K-means的类别数定为4），不同的类别颜色不同。从图7中可以看出，同一个控制流图中的block颜色大体相同，不同的控制流图的block的主颜色大体不同。
图7. 4个控制流图的block embedding
Order-aware：观察表中第三个分块，CNN模型在两个任务上都取得了不错的效果。Resnet11优于Resnet7和CNN3。与MPNN_ws相比，CNN效果更优。随机打乱节点顺序后，CNN模型效果大幅下降，这表示CNN模型确实可以学到节点顺序信息。图8是控制流图pair的例子，这个函数为“ZN12libfwbuilder15RuleElementRGtw13validateC-hildEPNS8FWObjectE“，左边是在gcc&x86-86上编译的控制流图，右边是在gcc&ARM上编译的控制流图。可以看到，左图的节点3在右图中被拆成节点3和节点4，除此之外其它节点的顺序与边的连接方式均相同。经过CNN模型的计算，这两个图的cosine相似度为0.971，排序rank的排名为1。这表明CNN模型可以从邻接矩阵中学到控制流图的节点顺序。
图8. 控制流图pair示例
## 结论
本文提出了一个新的模型，用于解决二进制代码分析的问题。本文的模型中包含semantic-aware模块，structural-aware模块以及order-aware模块。我们观察到语义信息和节点顺序信息都是控制流图重要的特征。我们使用BERT预训练模型提取语义信息，并使用CNN模型提取节点顺序信息。实验结果表明，本文提出的模型与之前最优的模型相比，取得了更好的效果。
## 参考文献
[1] Xu, X.; Liu, C.; Feng, Q.; Yin, H.; Song, L.; and Song, D. 2017. Neural
network-based graph embedding for crossplatform binary code similarity
detection. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and
Communications Security, 363–376. ACM.
[2] Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 .
[3] Gilmer, J.; Schoenholz, S. S.; Riley, P. F.; Vinyals, O.; and Dahl, G. E.
2017. Neural message passing for quantum chemistry. In Proceedings of the 34th
International Conference on Machine Learning-Volume 70 , 1263–1272. JMLR. org.
[4] He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learning for
image recognition. In Proceedings of the IEEE conference on computer vision
and pattern recognition, 770–778.
[5] Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J. 2013.
Distributed representations of words and phrases and their compositionality.
In Advances in neural information processing systems , 3111–3119.
[6] R.; Zhu, Y.; Salakhutdinov, R. R.; Zemel, R.; Urtasun, R.; Torralba, A.;
and Fidler, S. 2015. Skip-thought vectors. In Advances in neural information
processing systems,3294–3302.
## 关于科恩实验室
腾讯科恩实验室作为腾讯集团旗下一支国际一流的信息安全团队，在桌面端安全、移动终端安全等研究领域有十多年的积累，技术实力和研究成果达到了国际领先水平；近几年来，更是在智能网联汽车信息安全、IoT
安全、云计算和虚拟化技术安全等领域取得丰硕的成果。随着更多ICT新技术进入大众视野，腾讯科恩实验室也积极布局人工智能算法和技术框架的安全研究、机器学习在信息安全研究领域的应用研究和区块链技术应用的安全研究等新纬度上的前沿技术研究能力。同时开放自身核心技术能力，提供给智能网联汽车、安卓应用生态、IoT等行业，并根据产业实际痛点和研究推出了智能网联汽车信息安全行业解决方案。护航各行业数字化变革，守护全网用户的信息安全是腾讯科恩实验室的使命。