# Title: Guess Again (and Again and Again): Measuring Password Strength by Simulating Password-Cracking Algorithms

## Authors
Patrick Gage Kelley, Saranga Komanduri, Michelle L. Mazurek, Richard Shay, Timothy Vidas, Lujo Bauer, Nicolas Christin, Lorrie Faith Cranor, and Julio López  
Carnegie Mellon University, Pittsburgh, PA, USA  
{pgage, sarangak, mmazurek, rshay, tvidas, lbauer, nicolasc, lorrie, julio.lopez}@cmu.edu

## Abstract
Text-based passwords remain the primary authentication method in computer systems, despite significant advancements in attackers' capabilities to crack them. In response, password composition policies have become increasingly complex. However, there is a lack of research defining metrics to characterize password strength and using these metrics to evaluate password-composition policies. This paper analyzes 12,000 passwords collected under seven composition policies via an online study. We develop an efficient distributed method for calculating the effectiveness of several heuristic password-guessing algorithms. Using this method, we investigate:
- The resistance of passwords created under different conditions to guessing.
- The performance of guessing algorithms under different training sets.
- The relationship between passwords explicitly created under a given composition policy and other passwords that meet the same requirements.
- The relationship between guessability, as measured with password-cracking algorithms, and entropy estimates.

Our findings advance the understanding of both password-composition policies and metrics for quantifying password security.

## Keywords
Authentication, passwords, user study

## 1. Introduction
Text-based passwords are the most commonly used authentication method in computer systems. Previous research has shown that passwords are often easy for attackers to compromise. A common threat model involves an attacker who steals a list of hashed passwords and attempts to crack them offline. Recent data breaches, such as those at Booz Allen Hamilton, HBGary, Gawker, and Sony PlayStation, combined with the availability of botnets, make such threats very real. Once cracked, these passwords can be used to gain access not only to the original site but also to other accounts where users reuse their passwords. Password reuse, either exactly or with minor variations, is a common and growing practice as users acquire more online accounts.

To mitigate these attacks, system administrators specify password-composition policies. These policies require newly created passwords to adhere to various requirements intended to make them harder to guess. Typical requirements include including a number or symbol, exceeding a certain minimum length, and not being a word found in a dictionary.

While it is generally believed that password-composition policies make passwords harder to guess and thus more secure, research has struggled to quantify the level of resistance provided by different policies or individual requirements. The two most commonly used methods for quantifying the effect of password-composition policies are estimating the entropy of the resulting passwords and empirically analyzing the resulting passwords with password-guessing tools. However, the former is not based on empirical data, and the latter is difficult to apply due to the limited availability of password sets created under different policies.

In this paper, we take a substantial step forward in understanding the effects of password-composition policies on the guessability of passwords. First, we compile a dataset of 12,000 plaintext passwords collected from different participants under seven different password-composition policies using an online study. Second, we develop approaches for calculating how long it would take for various password-guessing tools to guess each of the passwords we collected. This allows us to evaluate the impact on security of each password-composition policy.

### Contributions
1. **Distributed Technique for Guessing Calculations**: We implement a distributed technique (guess-number calculator) to determine if and when a given password-guessing algorithm, trained with a given dataset, would guess a specific password. This allows us to evaluate the effectiveness of password-guessing attacks much more quickly than using existing cracking techniques.
2. **Comparison of Password Guessability**: We compare the guessability of passwords created under different password-composition policies more accurately than was previously possible. Due to the efficiency of our approach, we can investigate the effectiveness of multiple password-guessing approaches with multiple tunings. Our findings show that a password-composition policy requiring long passwords with no other restrictions provides excellent resistance to guessing relative to other tested policies.
3. **Impact of Tuning and Test-Set Selection**: We study the impact of tuning on the effectiveness of password-guessing algorithms and investigate the significance of test-set selection when evaluating the strength of different password-composition policies.
4. **Effectiveness of Entropy as a Measure**: We investigate the effectiveness of entropy as a measure of password guessability. For each composition policy, we compare our guessability calculations to two independent entropy estimates: one based on NIST guidelines and a second that we calculate empirically from the plaintext passwords in our dataset. We find that both measures of entropy have only a limited relationship to password strength as measured by guessability.

### Data Collection and Generalizability
We collected 12,000 plaintext passwords using Amazon’s Mechanical Turk (MTurk). Many researchers have examined the use of MTurk workers (Turkers) as participants in human-subjects research. About half of all Turkers are American, with Indian participation increasing rapidly in recent years to become about one-third of Turkers. American Turkers are about two-thirds women, while Indian Turkers are similarly weighted toward men. Overall, the Turker population is younger and more educated than the general population, with 40% holding at least a bachelor’s degree; these trends are more pronounced among Indian Turkers.

Buhrmester et al. find that the Turker population is significantly more diverse than samples used in typical lab-based studies, which heavily favor college-student participants. Well-designed MTurk tasks provide high-quality user-study data. This analysis of MTurk has important implications for studying passwords. We expect our findings to be more generalizable than those from lab studies with a more constrained participant base. Because we collected demographic information from our participants, our sample (and any biases it introduces) can be more accurately characterized than samples based on leaked password lists from various websites collected under uncertain circumstances.

A related consideration is that while our participants created real passwords needed several days later to complete the study and obtain a small bonus payment, these passwords did not protect high-value accounts. Password research has consistently been limited by the difficulty of studying passwords used for high-value accounts. Lab studies have asked participants to create passwords that protect simulated accounts, $5, a chance to win an iPod in a raffle, or access to university course materials. Other studies have relied on leaked password lists like the RockYou set, which contains millions of passwords but also non-password artifacts that are difficult to filter out definitively. While this set's provenance and completeness are unclear, it is hard to say how much value users place on protecting an account from a social gaming service. Other commonly used leaked password lists come from sites including MySpace, silentwhisper.net, and a variety of Finnish websites, with user valuations that are similarly difficult to assess.

Overall, although our dataset is not ideal, we contend that our findings do provide significant insight into the effects of password-composition policies on password guessability. Because so little is known about this important topic, even imperfect information constitutes progress.

### Roadmap
In Section II, we survey related work. We describe our data collection and analysis methodology in Sections III and IV. We convey our main results in Section V and address their generalizability and ethical considerations in Section VI. We conclude in Section VII by discussing the implications of our results for future research and for defining practical password-composition policies.

## 2. Background and Related Work
Research on passwords has been active for many years. We first summarize the different types of data collection and analysis that have been used. We then discuss evaluations of the impact of password policies and metrics for quantifying password strength.

### Collection and Analysis of Password Data
Many prior password studies have used small sample sizes, obtained through user surveys or lab studies. Kuo et al. estimated the security of 290 passwords created in an online survey. We also use an online survey but consider larger and more varied sets of passwords. Additionally, we recruit participants using Mechanical Turk, which produces more diverse samples than typical lab studies.

Other studies analyze large samples of passwords ostensibly created by users for actual accounts of varying importance. Unlike these studies, we study the impact of different password policies on password strength and use passwords collected under controlled password-policy conditions.

### Impact of Password Policies
Several studies have considered the impact of password policies on password strength. In lab studies, Proctor et al. and Vu et al. found that passwords created under stricter composition requirements were more resistant to automated cracking but also more difficult for participants to create and remember. We consider similar data for a much larger set of users, allowing for more comprehensive evaluation. Other findings suggest that too-strict policies, which make creating and remembering passwords too difficult, induce coping strategies that can hurt both security and productivity. Further, Florêncio and Herley found that the strictest policies are often used not by organizations with high-value assets to protect but rather by those that do not have to compete on customer service.

An increasingly popular password-strengthening measure that we also investigate is subjecting new passwords to a blacklist check. Schechter et al. proposed a password policy in which passwords chosen by too many users are blacklisted for subsequent users. This offers many theoretical advantages over other password-composition schemes.

### Measuring Password Strength
Effective evaluation of password strength requires a proper metric. One possible metric is information entropy, defined by Shannon as the expected value (in bits) of the information contained in a string. Massey connects entropy with password strength by demonstrating that entropy provides a lower bound on the expected number of guesses to find a text. A 2006 National Institute of Standards and Technology (NIST) publication uses entropy to represent the strength of a password but does not calculate entropy empirically. Florêncio and Herley estimated theoretical entropy for the field data they analyzed.

An alternative metric of password strength is "guessability," which characterizes the time needed for an efficient password-cracking algorithm to discover a password. Weir et al. divide a large set of existing passwords into different categories based on composition, then apply automated cracking tools to examine how well NIST’s entropy estimates predict measured guessing difficulty. Castelluccia et al. use Markov models to measure password strength based on the distribution of already-selected passwords. Dell’Amico et al. evaluate password strength by calculating guessing probabilities yielded by popular password-cracking heuristics. We use a related approach but focus on comparing password policies.

Narayanan et al. discuss a password-cracking technique based on a Markov model, in which password guesses are made based on contextual frequency of characters. Marechal and Weir both examine this model and find it more effective for password cracking than the popular password-cracking program John the Ripper. Weir et al. present a novel password-cracking technique that uses the text structure from training data while applying mangling rules to the text itself. The authors found their technique to be more effective than John the Ripper. In a separate study, Zhang et al. found Weir’s algorithm most effective among the techniques they used.

### Methodology: Data Collection
In this section, we discuss our methodology for collecting plaintext passwords, the word lists we used to assemble the blacklists used in some conditions, and the eight conditions under which we gathered data. We also summarize participant demographics.

#### A. Collection Instrument
From August 2010 to January 2011, we advertised a two-part study on Mechanical Turk, paying between 25 and 55 cents for the first part and between 50 and 70 cents for the second part. The consent form indicated the study pertained to visiting secure websites.

Each participant was given a scenario for making a new password and asked to create a password that met a set of password-composition requirements. Participants who entered a password that did not conform to requirements were shown an error message indicating which requirements were not met and asked to try again until they succeeded. After creating a password, participants took a brief survey about demographics and password creation. Participants were then asked to recall the password just created; after five failed attempts, the password was displayed. For the second part of the study, participants were emailed two days later and asked to return to the website and recall their passwords. We measured the incidence of passwords being written down or otherwise stored (via detecting browser storage and copy-paste behavior, as well as asking participants).

The second part of the study primarily concerns memorability and usability factors. We report detailed results on these topics in a prior paper, which uses a large subset of the dataset we analyze here. We briefly revisit these findings when we discuss our results in Section V.

#### B. Word Lists for Algorithm Training
We use six publicly available word lists as training data in our analysis and to assemble the blacklists used in some of our experimental conditions. The RockYou password set includes more than 30 million passwords, and the MySpace password set contains about 45,000 passwords. The inflection list contains 250,000 words in varied grammatical forms such as plurals and past tense. The simple dictionary contains about 200,000 words and is a standard English dictionary available on most Unix systems. We also used two cracking dictionaries from the Openwall Project containing standard and mangled versions of dictionary words and common passwords: the free Openwall list with about 4 million words and the paid Openwall list with more than 40 million. While these data sources are not ideal, they are publicly available, and we expect attackers would use these word lists or others like them for training data. In Section V-B, we consider the effect of a variety of training sets drawn from these word lists as well as our collected password data.

#### C. Conditions
Our participants were divided into eight conditions comprising seven sets of password-composition requirements and two password-creation scenarios. We used two scenarios to measure the extent to which giving participants different instructions affects password strength. The survey scenario was designed to simulate a scenario in which users create low-value passwords, while the email scenario was designed to elicit higher-value passwords. All but one condition used the email scenario.

- **Survey Scenario**: Participants were told, "To link your survey responses, we will use a password that you create below; therefore, it is important that you remember your password."
- **Email Scenario**: Participants were told, "Imagine that your main email service provider has been attacked, and your account became compromised. You need to create a new password for your email account, since your old password may be known by the attackers. Because of the attack, your email service provider is also changing its password rules. Please follow the instructions below to create a new password for your email account. We will ask you to use this password in a few days to log in again, so it is important that you remember your new password. Please take the steps you would normally take to remember your email password and protect this password as you normally would protect the password for your email account. Please behave as you would if this were your real password!"

The eight conditions are detailed below:

- **basic8survey**: Participants were given the survey scenario and the composition policy "Password must have at least 8 characters." Only this condition uses the survey scenario.
- **basic8**: Participants were given the email scenario and the composition policy "Password must have at least 8 characters."
- **basic16**: Participants were given the email scenario and the composition policy "Password must have at least 16 characters."
- **dictionary8**: Participants were given the email scenario and the composition policy "Password must have at least 8 characters. It may not contain a dictionary word." We removed non-alphabetic characters and checked the remainder against a dictionary, ignoring case. This method is used in practice, including at our institution. We used the free Openwall list for this check.