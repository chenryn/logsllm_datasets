title:Guess Again (and Again and Again): Measuring Password Strength by
Simulating Password-Cracking Algorithms
author:Patrick Gage Kelley and
Saranga Komanduri and
Michelle L. Mazurek and
Richard Shay and
Timothy Vidas and
Lujo Bauer and
Nicolas Christin and
Lorrie Faith Cranor and
Julio L&apos;opez
2012 IEEE Symposium on Security and Privacy
Measuring password strength by simulating password-cracking algorithms
Guess again (and again and again):
Patrick Gage Kelley, Saranga Komanduri, Michelle L. Mazurek, Richard Shay, Timothy Vidas
Lujo Bauer, Nicolas Christin, Lorrie Faith Cranor, and Julio L´opez
{pgage,sarangak,mmazurek,rshay,tvidas,lbauer,nicolasc,lorrie,julio.lopez}@cmu.edu
Pittsburgh, PA, USA
Carnegie Mellon University
Abstract—Text-based passwords remain the dominant au-
thentication method in computer systems, despite signiﬁcant
advancement in attackers’ capabilities to perform password
cracking. In response to this threat, password composition
policies have grown increasingly complex. However, there is
insufﬁcient research deﬁning metrics to characterize password
strength and using them to evaluate password-composition
policies. In this paper, we analyze 12,000 passwords collected
under seven composition policies via an online study. We
develop an efﬁcient distributed method for calculating how
effectively several heuristic password-guessing algorithms guess
passwords. Leveraging this method, we investigate (a) the
resistance of passwords created under different conditions to
guessing; (b) the performance of guessing algorithms under
different training sets; (c) the relationship between passwords
explicitly created under a given composition policy and other
passwords that happen to meet the same requirements; and
(d) the relationship between guessability, as measured with
password-cracking algorithms, and entropy estimates. Our
ﬁndings advance understanding of both password-composition
policies and metrics for quantifying password security.
Keywords-authentication; passwords; user study
I. INTRODUCTION
Text-based passwords are the most commonly used au-
thentication method in computer systems. As shown by
previous research (e.g., [1]–[3]), passwords are often easy
for attackers to compromise. A common threat model is an
attacker who steals a list of hashed passwords, enabling him
to attempt to crack them ofﬂine at his leisure. The many
recent examples of data breaches involving large numbers of
hashed passwords (Booz Allen Hamilton, HBGary, Gawker,
Sony Playstation, etc.), coupled with the availability of
botnets that offer large computational resources to attackers,
make such threats very real [4]–[7]. Once these passwords
have been cracked, they can be used to gain access not
only to the original site, but also to other accounts where
users reuse their passwords. Password reuse (exactly and
with minor variations) is a common and growing practice as
users acquire more online accounts [8], [9].
To mitigate the danger of such attacks, system adminis-
trators specify password-composition policies. These poli-
cies force newly created passwords to adhere to various
requirements intended to make them harder to guess. Typical
requirements are that passwords include a number or a
symbol, that they exceed a certain minimum length, and
that they are not words found in a dictionary.
Although it
is generally believed that password-
composition policies make passwords harder to guess, and
hence more secure,
research has struggled to quantify
the level of resistance to guessing provided by different
password-composition policies or the individual require-
ments they comprise. The two most commonly used methods
for quantifying the effect of password-composition poli-
cies are estimating the entropy of the resulting passwords
(e.g., [10], [11]), and empirically analyzing the resulting
passwords with password-guessing tools (e.g., [12], [13]).
The former, however, is not based on empirical data, and the
latter is difﬁcult to apply because of the dearth of available
password sets created under different password-composition
policies.
In this paper, we take a substantial step forward in un-
derstanding the effects of password-composition policies on
the guessability of passwords. First, we compile a dataset of
12,000 plaintext passwords collected from different partic-
ipants under seven different password-composition policies
using an online study. Second, we develop approaches for
calculating how long it would take for various password-
guessing tools to guess each of the passwords we collected.
This allows us to evaluate the impact on security of each
password-composition policy.
Contributions. We make the following contributions:
1) We implement a distributed technique (guess-number
to determine if and when a given
calculator)
password-guessing algorithm,
trained with a given
data set, would guess a speciﬁc password. This allows
us to evaluate the effectiveness of password-guessing
attacks much more quickly than we could using exist-
ing cracking techniques.
2) We compare, more accurately than was previously
possible, the guessability of passwords created under
different password-composition policies. Because of
the efﬁciency of our approach (compared to guessing
passwords directly), we can investigate the effective-
© 2012, Patrick Gage Kelley. Under license to IEEE.
DOI 10.1109/SP.2012.38
523
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:47:23 UTC from IEEE Xplore.  Restrictions apply. 
ness of multiple password-guessing approaches with
multiple tunings. Our ﬁndings show that a password-
composition policy requiring long passwords with no
other restrictions provides (relative to other tested
policies) excellent resistance to guessing.
3) We study the impact of tuning on the effectiveness of
password-guessing algorithms. We also investigate the
signiﬁcance of test-set selection when evaluating the
strength of different password-composition policies.
4) We investigate the effectiveness of entropy as a mea-
sure of password guessability. For each composition
policy, we compare our guessability calculations to
two independent entropy estimates: one based on the
NIST guidelines mentioned above, and a second that
we calculate empirically from the plaintext passwords
in our dataset. We ﬁnd that both measures of en-
tropy have only very limited relationships to password
strength as measured by guessability.
Mechanical Turk and controlled password collection. As
with any user study, it is important to reﬂect on the origin
of our dataset
to understand the generalizability of our
ﬁndings. We collected 12,000 plaintext passwords using
Amazon’s Mechanical Turk crowdsourcing service (MTurk).
Many researchers have examined the use of MTurk workers
(Turkers) as participants in human-subjects research. About
half of all Turkers are American, with Indian participation
increasing rapidly in the last 2-3 years to become about
one third of Turkers [14]. American Turkers are about two-
thirds women, while Indian Turkers are similarly weighted
toward men [15]. Overall, the Turker population is younger
and more educated than the general population, with 40%
holding at least a bachelor’s degree; both of these trends are
more pronounced among Indian Turkers [14], [15].
Buhrmester et al. ﬁnd that the Turker population is signif-
icantly more diverse than samples used in typical lab-based
studies that heavily favor college-student participants [16].
This study, and others, found that well-designed MTurk tasks
provide high-quality user-study data [16]–[19].
This analysis of MTurk has important implications in
the context of studying passwords. We expect our ﬁndings
will be more generalizable than those from lab studies with
a more constrained participant base. Because we collected
demographic information from our participants, our sample
(and any biases it introduces) can be more accurately char-
acterized than samples based on leaked password lists from
various websites collected under uncertain circumstances.
A related consideration is that while our participants
created real passwords that were needed several days later to
complete the study and obtain a small bonus payment, these
passwords did not protect high-value accounts. Password
research has consistently been limited by the difﬁculty of
studying passwords used for high-value accounts. Lab stud-
ies have asked participants to create passwords that protect
524
simulated accounts, $5, a chance to win an iPod in a rafﬂe,
or access to university course materials including homework
and grades [20]–[23]. Other studies have relied on leaked
password lists like the RockYou set [13], [24]. While this
set contains millions of passwords, it also contains non-
password artifacts that are difﬁcult to ﬁlter out deﬁnitively,
its provenance and completeness are unclear, and it
is
hard to say how much value users place on protecting an
account from a social gaming service. Other commonly used
leaked password lists come from sites including MySpace,
silentwhisper.net, and a variety of Finnish websites, with
user valuations that are similarly difﬁcult
to assess [2],
[25]. In Section VI, we brieﬂy compare our MTurk users’
behavior to results from a survey of people using higher-
value passwords in practice.
Overall, although our dataset is not ideal, we contend that
our ﬁndings do provide signiﬁcant insight into the effects
of password-composition policies on password guessability.
Because so little is known about this important topic, even
imperfect information constitutes progress.
Roadmap.
In Section II we survey related work. We de-
scribe our data collection and analysis methodology in Sec-
tions III and IV. We convey our main results in Section V,
and address their generalizability and ethical considerations
in Section VI. We conclude in Section VII by discussing
the implications of our results for future research and for
deﬁning practical password-composition policies.
II. BACKGROUND AND RELATED WORK
Research on passwords has been active for many years.
We ﬁrst summarize the different types of data collection and
analysis that have been used. We then discuss evaluations of
the impact of password policies and metrics for quantifying
password strength.
Collection and analysis of password data. Many prior
password studies have used small sample sizes [26]–[29],
obtained through user surveys or lab studies. Kuo et al.
estimated the security of 290 passwords created in an online
survey [21]. We also use an online survey, but we consider
larger and more varied sets of passwords. In addition, we
recruit participants using Mechanical Turk, which produces
more diverse samples than typical lab studies [16].
Other studies analyze large samples of passwords os-
tensibly created by users for actual accounts of varying
importance [1]–[3], [13], [30], [31]. Unlike these studies,
we study the impact of different password policies on pass-
word strength and use passwords collected under controlled
password-policy conditions.
Impact of password policies. Several studies have consid-
ered the impact of password policies on password strength.
In lab studies, Proctor et al. [12] and Vu et al. [32] found
passwords created under stricter composition requirements
were more resistant to automated cracking, but also more
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:47:23 UTC from IEEE Xplore.  Restrictions apply. 
difﬁcult for participants to create and remember. We consider
similar data for a much larger set of users, allowing for more
comprehensive evaluation. Other ﬁndings suggest too-strict
policies, which make creating and remembering passwords
too difﬁcult, induce coping strategies that can hurt both
security and productivity [33]–[37]. Further, Florˆencio and
Herley found that the strictest policies are often used not by
organizations with high-value assets to protect, but rather by
those that do not have to compete on customer service [38].
An increasingly popular password-strengthening measure
that we also investigate is subjecting new passwords to a
blacklist check. Schechter et al. proposed a password policy
in which passwords chosen by too many users are blacklisted
for subsequent users [39]. This offers many theoretical
advantages over other password-composition schemes.
Measuring password strength.
Effective evaluation of
password strength requires a proper metric. One possible
metric is information entropy, deﬁned by Shannon as the
expected value (in bits) of the information contained in a
string [40]. Massey connects entropy with password strength
by demonstrating that entropy provides a lower bound on
the expected number of guesses to ﬁnd a text [41]. A
2006 National Institute of Standards and Technology (NIST)
publication uses entropy to represent
the strength of a
password, but does not calculate entropy empirically [11].
Florˆencio and Herley estimated theoretical entropy for the
ﬁeld data they analyzed [1].
An alternative metric of password strength is “guessabil-
ity,” which characterizes the time needed for an efﬁcient
password-cracking algorithm to discover a password. In one
example, Weir et al. divide a large set of existing passwords
into different categories based on composition, then apply
automated cracking tools to examine how well NIST’s
entropy estimates predict measured guessing difﬁculty [13].
Castelluccia et al. use Markov models to measure password
strength based on the distribution of already-selected pass-
words [42]. Dell’Amico et al. evaluate password strength
by calculating guessing probabilities yielded by popular
password-cracking heuristics [2]. We use a related approach
but focus on comparing password policies.
Narayanan et al. discuss a password-cracking technique
based on a Markov model, in which password guesses are
made based on contextual frequency of characters [27].
Marechal [43] and Weir [44] both examine this model and
ﬁnd it more effective for password cracking than the popular
password-cracking program John the Ripper [45]. Weir et al.
present a novel password-cracking technique that uses the
text structure from training data while applying mangling
rules to the text itself [25]. The authors found their technique
to be more effective than John the Ripper. In a separate
study, Zhang et al. found Weir’s algorithm most effective
among the techniques they used [31].
password creation in some of our study conditions, and to
implement a new measure of password strength, the guess
number, which we apply to user-created passwords collected
under controlled password-composition policies.
III. METHODOLOGY: DATA COLLECTION
In this section, we discuss our methodology for collecting
plaintext passwords, the word lists we used to assemble the
blacklists used in some conditions, and the eight conditions
under which we gathered data. We also summarize partici-
pant demographics.
A. Collection instrument
From August 2010 to January 2011, we advertised a two-
part study on Mechanical Turk, paying between 25 and 55
cents for the ﬁrst part and between 50 and 70 cents for the
second part. The consent form indicated the study pertained
to visiting secure websites.
Each participant was given a scenario for making a new
password and asked to create a password that met a set
of password-composition requirements; the scenarios and
requirements are detailed in Section III-C. Participants who
entered a password that did not conform to requirements
were shown an error message indicating which requirements
were not met and asked to try again until they succeeded.
After creating a password, participants took a brief survey
about demographics and password creation. Participants
were then asked to recall the password just created; after ﬁve
failed attempts, the password was displayed. For the second
part of the study, participants were emailed two days later
and asked to return to the website and recall their passwords.
We measured the incidence of passwords being written down
or otherwise stored (via detecting browser storage and copy-
paste behavior, as well as asking participants; see Section VI
for details). The second part of the study primarily concerns
memorability and usability factors. We report detailed results
on these topics in a prior paper, which uses a large subset
of the dataset we analyze here [46]; we brieﬂy revisit these
ﬁndings when we discuss our results in Section V.
B. Word lists for algorithm training
We use six publicly available word lists as training data in
our analysis and to assemble the blacklists used in some of
our experimental conditions. The RockYou password set [24]
includes more than 30 million passwords, and the MySpace
password set [47] contains about 45,000 passwords. (We
discuss ethical considerations related to these datasets in
Section VI.) The inﬂection list1 contains 250,000 words in
varied grammatical forms such as plurals and past tense.
The simple dictionary contains about 200,000 words and is a
standard English dictionary available on most Unix systems.
We also used two cracking dictionaries from the Openwall
Project2 containing standard and mangled versions of dic-
1http://wordlist.sourceforge.net
2http://www.openwall.com/wordlists/
In this work, we apply the Weir algorithm and a varia-
tion of the Markov model to generate blacklists restricting
525
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:47:23 UTC from IEEE Xplore.  Restrictions apply. 
tionary words and common passwords: the free Openwall
list with about 4 million words and the paid Openwall list
with more than 40 million. While these data sources are not
ideal, they are publicly available; we expect attackers would
use these word lists or others like them for training data. In
Section V-B, we consider the effect of a variety of training
sets drawn from these word lists as well as our collected
password data.
C. Conditions
Our participants were divided into eight conditions com-
prising seven sets of password-composition requirements
and two password-creation scenarios. We used two scenarios
in order to measure the extent to which giving participants
different instructions affects password strength. The survey
scenario was designed to simulate a scenario in which
users create low-value passwords, while the email scenario
was designed to elicit higher-value passwords. All but one
condition used the email scenario.
In the survey scenario, participants were told, “To link
your survey responses, we will use a password that you
create below; therefore it is important that you remember
your password.”
In the email scenario, participants were told, “Imagine
that your main email service provider has been attacked,
and your account became compromised. You need to create
a new password for your email account, since your old
password may be known by the attackers. Because of the
attack, your email service provider is also changing its
password rules. Please follow the instructions below to
create a new password for your email account. We will ask
you to use this password in a few days to log in again, so it
is important that you remember your new password. Please
take the steps you would normally take to remember your
email password and protect this password as you normally
would protect the password for your email account. Please
behave as you would if this were your real password!”
The eight conditions are detailed below.
Participants were given the email scenario and
least 8
basic8survey: Participants were given the survey scenario
and the composition policy “Password must have at least 8
characters.” Only this condition uses the survey scenario.
basic8:
the composition policy “Password must have at
characters.” Only the scenario differs from basic8survey.
basic16:
Participants were given the email scenario and
the composition policy “Password must have at least 16
characters.”
dictionary8: Participants were given the email scenario and
the composition policy “Password must have at least 8 char-
acters. It may not contain a dictionary word.” We removed
non-alphabetic characters and checked the remainder against
a dictionary, ignoring case. This method is used in practice,
including at our institution. We used the free Openwall list