 4
 3
 2
 1
 0
False Positive
False Negative
KNN
NC
NCC
Detection Algorithm
)
%
(
e
t
a
R
r
o
r
r
E
 6
 5
 4
 3
 2
 1
 0
False Positive
False Negative
(KNN)
(NC)
(NCC)
50 100 All
50 100 All
# of Clicks
50 100 All
)
%
(
e
t
a
R
r
o
r
r
E
 5
 4
 3
 2
 1
 0
False Positive
False Negative
KNN
NC
NCC
Detection Algorithm
Figure 11: Error rate of three reclus-
tering algorithms.
Figure 12: Error rate vs. maximum #
of clicks in each sequence.
Figure 13: Detection accuracy when
training data is two weeks old.
using Hybrid Model with 5gram+countas distance func-
tion. The number of clusters K = 100. In each sequence
similarity graph, we label the Sybil and normal clusters.
Next, we examine the error rates of the incremental
detector when unclassiﬁed users (3000 Sybils and 3000
normal users) are added to the sequence similarity graph.
We perform this experiment three times, once for each
of the proposed reclustering algorithms (KNN, NC and
NCC). As shown in Figure 11, the error rates for all three
reclustering algorithms are very similar, and all three
have 1 billion total users [1].
We now evaluate how detection accuracy changes
when we decrease the percentage of Sybils in the train-
ing data. In these experiments, we construct training sets
of 6000 total users with different normal-to-Sybil ratios.
We then run unsupervised training with 500 seeds. Fi-
nally, we incrementally add an additional 3000 Sybils
and 3000 normal users to the colored similarity graph
using the NCC algorithm (see Section 5.1). We ran ad-
ditional tests using the NC and KNN algorithms, but the
results were very similar and we omit them for brevity.
Figure18 shows the ﬁnal error rate of the system (i.e.
after 6000 users have been incrementally added) for
varying normal-to-Sybil ratios. The false positive rate
remains ≤1.2% regardless of the normal-to-Sybil ratio.
This is a very good result: even with highly skewed
training data, the system is unlikely to penalize normal
users. Unfortunately, the false negative rate does rise as
the number of Sybils in the training data falls. This result
is to be expected: the system cannot adequately classify
Sybil clickstreams if it is trained on insufﬁcient data.
Handling False Positives.
The above analy-
sis demonstrates that our system achieves high accuracy
with a false positive rate of 1% or less. Through man-
ual inspection, we ﬁnd that “false positives” generated
by our detector exhibit behaviors generally attributed to
Sybils, including aggressively sending friend requests or
browsing proﬁles. In real-world OSNs, suspicious users
identiﬁed by our system could be further veriﬁed via ex-
isting complementary systems that examines other as-
pects of users. For example, this might include systems
that classify user proﬁles [32, 43], systems that verify
user real-world identity [2], or even Sybil detection sys-
tems using crowdsourced human inspection [38]. These
efforts could further protect benign users from misclassi-
ﬁcation.
7 Practical Sybil Detection
In this section, we examine the practical performance of
our proposed Sybil detection system. First, we shipped
our code to the security teams at Renren and LinkedIn,
where it was evaluated on fresh data in a production en-
vironment. Both test results are very positive, and we
report them here. Second, we discuss the fundamental
limits of our approach, by looking at our impact on Sybil
accounts that can perfectly mimic the clickstream pat-
terns of normal users.
7.1 Real-world Sybil Detection
With the help of supportive collaborators at both Ren-
ren and LinkedIn, we were able to ship prototype code
to the security teams at both companies for internal test-
ing on fresh data. We conﬁgured our system to use un-
supervised learning to color clusters. Sequence similar-
ity graphs are constructed using the Hybrid Model and
the 5gram+count distance function, and the number of
METIS partitions K is 100.
Renren.
Renren’s security team trained our system