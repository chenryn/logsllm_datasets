f ∈ F of cluster ci and cj, and wf is a weight that represents the
f wf = 1 and rf (ci , cj ) = {x ∈
R | 0 ≤ x ≤ 1}. Thus, distance(ci , cj ) is a number between 0 and 1.
Figure 2: High-level overview of our processing pipeline.
Thus, visually similar images have minor differences in their vec-
tors. For example, the string representation of the phashes obtained
from the images in cluster N (see Fig. 1) are 55352b0b8d8b5b53,
55952b0bb58b5353, and 55952b2b9da58a53, respectively. The al-
gorithm is also robust against changes in the images, e.g., signal
processing operations and direct manipulation [75], and effectively
reduces the dimensionality of the raw images.
Clustering via pairwise distance calculation (Steps 2-3). Next,
we cluster images from one or more Web Communities using the
pHash values. We perform a pairwise comparison of all the pHashes
using Hamming distance (Step 2). To support large numbers of
images, we implement a highly parallelizable system on top of
TensorFlow [5], which uses multiple GPUs to enhance performance.
Images are clustered using a density-based algorithm (Step 3). Our
current implementation uses DBSCAN [14], mainly because it can
discover clusters of arbitrary shape and performs well over large,
noisy datasets. Nonetheless, our architecture can be easily tweaked
to support any clustering algorithm and distance metric.
We also perform an analysis of the clustering performance and
the rationale for selecting the clustering threshold. We refer to
Appendix A for more details.
Screenshots Removal (Step 4). Meme annotation sites like KYM
often include, in their image galleries, screenshots of social network
posts that are not variants of a meme but just comments about it.
Hence, we discard social-network screenshots from the annotation
sites data sources using a deep learning classifier. Due to space
limitations, we refer to Appendix C of the extended version of the
paper [73] for details on the classifier.
Cluster Annotation (Steps 5). Clustering annotation uses the
medoid of each cluster, i.e., the element with the minimum square
average distance from all images in the cluster. In other words, the
medoid is the image that best represents the cluster. The clusters’
medoids are compared with all images from meme annotation sites,
by calculating the Hamming distance between each pair of pHash
vectors. We consider that an image matches a cluster if the distance
is less than or equal to a threshold θ, which we set to 8, as it allows
3. Clustering 1. pHash Extraction2. pHash-based PairwiseDistance CalculationpHashes of some or all Web  communities' imagesClusters of images5. Cluster Annotation Pairwise Comparisons  of pHashesannotated  images 6. Association ofImages to ClustersAnnotated  ClusterspHashes of  annotated images pHashes (all Web Communities) 7. Analysis andInﬂuence EstimationOccurrences of Memes in  all Web Communities 4. ScreenshotClassiﬁerannotated  images pHashes of non-screenshot  annotated images Know Your  MemeGenericAnnotationSitesMeme Annotation SitesGeneric  WebCommunities4chanTwitterRedditGabWeb Communities posting MemesimagesIMC ’18, October 31-November 2, 2018, Boston, MA, USA
S. Zannettou et al.
Features. We consider four different features for rf∈F, specifically,
F = {perceptual, meme, people, culture}; see below.
rperceptual : this feature is the similarity between two clusters from
a perceptual viewpoint. Let h be a pHash vector for an image m in
cluster c, where m is the medoid of the cluster, and dij the Hamming
distance between vectors hi and hj (see in Step 5). We compute
dij from ci and cj as follows. First, we obtain obtain the medoid
mi from cluster ci. Subsequently, we obtain hi =pHash(mi ). Finally,
we compute dij =Hamming(hi , hj ). We simplify notation and use d
instead of dij to denote the distance between two medoid images
and refer to this distance as the Hamming score.
We define the perceptual similarity between two clusters as an
exponential decay function over the Hamming score d:
d
rper ceptual (d ) = 1 −
τ × e max/τ
(2)
where max represents the maximum pHash distance between two
images and τ is a constant parameter, or smoother, that controls
how fast the exponential function decays for all values of d (recall
that {d ∈ R | 0 ≤ d ≤ max}). Note that max is bound to the precision
given by the pHash algorithm. Recall that each pHash has a size of
|d|=64, hence max=64. Intuitively, when τ << 64, rperceptual is a
high value only with perceptually indistinguishable images, e.g., for
τ =1, two images with d=0 have a similarity rperceptual =1.0. With
the same τ, the similarity drops to 0.4 when d=1. By contrast, when
τ is close to 64, rperceptual decays almost linearly. For example,
for τ =64, rperceptual (d=0)=1.0 and rperceptual (d=1)=0.98. As
mentioned above, we observe that pairs of images with scores
between d=0 and d=8 are usually part of the same variant (see Step
5 in Sec. 2.2). In our implementation, we set τ =25 as rperceptual
returns high values up to d=8, and rapidly decays thereafter.
rmeme, rcultur e, and rpeople: the annotation process (Step 5) pro-
vides contextualized information about the cluster medoid, includ-
ing the name (i.e., the main identifier) given to a meme, the associ-
ated culture (i.e., high-level group of meme), and people that are
included in a meme. (Note that we use all the annotations for each
category and not only the representative one, see Step 5.) Therefore,
we model a different similarity for each of the these categories, by
looking at the overlap of all the annotations among the medoids
of both clusters (mi, mj, for ci and cj, respectively). Specifically,
for each category, we calculate the Jaccard index between the an-
notations of both medoids, for memes, cultures, and people, thus
acquiring rmeme, rcultur e, rpeople, respectively.
Modes. Our distance metric measures how similar two clusters are.
If both clusters are annotated, we operate in “full-mode,” and in
“partial-mode” otherwise. For each mode, we use different weights
for the features in Eq. 1, which we set empirically as we lack the
ground-truth data needed to automate the computation of the opti-
mal set of thresholds.
Full-mode. In full-mode, we set weights as follows. 1) The features
from the perceptual and meme categories should have higher rele-
vance than people and culture, as they are intrinsically related to the
definition of meme (see Sec. 2.1). The last two are non-discriminant
features, yet are informative and should contribute to the metric.
Also, 2) rmeme should not outweigh rperceptual because of the
relevance that visual similarities have on the different variants of
Platform
Twitter
Reddit
/pol/
Gab
KYM
#Posts #Posts with
Images
242,723,732
62,321,628
13,190,390
955,440
15,584
1,469,582,378
1,081,701,536
48,725,043
12,395,575
15,584
#Images
114,459,736
40,523,275
4,325,648
235,222
706,940
#Unique
pHashes
74,234,065
30,441,325
3,626,184
193,783
597,060
Table 1: Overview of our datasets.
a meme. Likewise, rperceptual should not dominate over rmeme
because of the branching nature of the memes. Thus, we want these
two categories to play an equally important weight. Therefore, we
choose wperceptual =0.4, wmeme =0.4, wpeople =0.1, wcultur e =0.1.
This means that when two clusters belong to the same meme and
their medoids are perceptually similar, the distance between the
clusters will be small. In fact, it will be at most 0.2 = 1 − (0.4 + 0.4)
if people and culture do not match, and 0.0 if they also match. Note
that our metric also assigns small distance values for the following
two cases: 1) when two clusters are part of the same meme variant,
and 2) when two clusters use the same image for different memes.
Partial-mode. In this mode, we associate unannotated images
with any of the known clusters. This is a critical component of our
analysis (Step 6), allowing us to study images from generic Web
communities where annotations are unavailable. In this case, we
rely entirely on the perceptual features. We once again use Eq. 1,
but simply set all weights to 0, except for wperceptual (which is
set to 1). That is, we compare the image we want to test with the
medoid of the cluster and we apply Eq. 2 as described above.
3 DATASETS
3.1 Web Communities
As mentioned earlier, our data sources are Web communities that
post memes and meme annotation sites. For the former, we focus on
four communities: Twitter, Reddit, Gab, and 4chan (more precisely,
4chan’s Politically Incorrect board, /pol/). This provides a mix of
mainstream social networks (Twitter and Reddit) as well as fringe
communities that are often associated with the alt-right and have
an impact on the information ecosystem (Gab and /pol/) [74].
There are several other platforms playing important roles in
spreading memes, however, many are “closed” (e.g., Facebook) or do
not involve memes based on static images (e.g., YouTube, Giphy). In
future work, we plan to extend our measurements to communities
like Instagram and Tumblr, as well as to GIF and video memes.
Nonetheless, we believe our data sources already allow us to elicit
comprehensive insights into the meme ecosystem.
Table 1 reports the number of posts and images processed for
each community. Note that the number of images is lower than the
number of posts with images because of duplicate image URLs and
because some images get deleted. Next, we discuss each dataset.
Twitter. Twitter is a mainstream microblogging platform, allowing
users to broadcast 280-character messages (tweets) to their follow-
ers. Our Twitter dataset is based on tweets made available via the
1% Streaming API, between July 1, 2016 and July 31, 2017. In total,
we parse 1.4B tweets: 242M of them have at least one image. We
On the Origins of Memes
IMC ’18, October 31-November 2, 2018, Boston, MA, USA
extract all the images, ultimately collecting 114M images yielding
74M unique pHashes.
Reddit. Reddit is a news aggregator: users create submissions by
posting a URL and others can reply in a structured way. It is divided
into multiple sub-communities called subreddits, each with its own
topic and moderation policy. Content popularity and ranking are
determined via a voting system based on the up- and down-votes
users cast. We gather images from Reddit using publicly available
data from Pushshift [62]. We parse all submissions and comments1
between July 1, 2016 and July, 31 2017, and extract 62M posts that
contain at least one image. We then download 40M images produc-
ing 30M unique pHashes.
4chan. 4chan is an anonymous image board; users create new
threads by posting an image with some text, which others can reply
to. It has two characteristic features: anonymity and ephemeral-
ity. By default, user identities are concealed, and all threads are
deleted after one week. Overall, 4chan is known for its extremely
lax moderation and the high degree of hate and racism, especially
on boards like /pol/ [20]. We obtain all threads posted on /pol/,
between July 1, 2016 and July 31, 2017, using the same methodology
of [20]. Since all threads (and images) are removed after a week, we
use a public archive service called 4plebs [4] to collect 4.3M images,
thus yielding 3.6M unique pHashes.
Gab. Gab is a social network launched in August 2016 as a “cham-
pion” of free speech, providing “shelter” to users banned from
other platforms. It combines social networking features from Twit-
ter (broadcast of 300-character messages) and Reddit (content is
ranked according to up- and down-votes). It also has extremely lax
moderation as it allows everything except illegal pornography, ter-
rorist propaganda, and doxing [66]. Overall, Gab attracts alt-right
users, conspiracy theorists, and trolls, and high volumes of hate
speech [72]. We collect 12M posts, posted on Gab between August
10, 2016 and July 31, 2017, and 955K posts have at least one image,
using the same methodology as in [72]. Out of these, 235K images
are unique, producing 193K unique pHashes.
Ethics. Although we only collect publicly available data, our study
has been approved by the designated ethics officer at UCL. Since
4chan content is typically posted with expectations of anonymity,
we have encrypted data at rest, while making no attempt to de-
anonymize users.
3.2 Meme Annotation Site
Know Your Meme (KYM). We choose KYM as the source for
meme annotation as it offers a comprehensive database of memes.
KYM is a sort of encyclopedia of Internet memes: for each meme, it
provides information such as its origin (i.e., the platform on which
it was first observed), the year it started, as well as descriptions
and examples. In addition, for each entry, KYM provides a set of
keywords, called tags, that describe the entry. KYM provides a va-
riety of higher-level categories that group meme entries; namely,
cultures, subcultures, people, events, and sites. “Cultures” and “sub-
cultures” entries refer to a wide variety of topics ranging from video
games to various general categories. For example, the Rage Comics
subculture [42] is a higher level category associated with memes
1See [64] for metadata associated with submissions and comments.
(a) Images
(b) Origins
Figure 3: Basic statistics from the KYM dataset. We omit some cat-
egories (i.e., Sites, Events, and Subcultures) from Fig. 3(a) for read-
ability purposes.
related to comics like Rage Guy [43] or LOL Guy [34], while the
Alt-right culture [22] gathers entries from a loosely defined seg-
ment of the right-wing community. The rest of the categories refer
to specific individuals (e.g., Donald Trump [29]), specific events
(e.g.,#CNNBlackmail [27]), and sites (e.g., /pol/ [41]), respectively. It
is also worth noting that KYM moderates all entries, hence entries
that are wrong or incomplete are marked as so by the site.
As of May 2018, the site has 18.3K entries, specifically, 14K
memes, 1.3K subcultures, 1.2K people, 1.3K events, and 427 web-
sites [35]. We crawl KYM between October and December 2017,
acquiring data for 15.6K entries; for each entry, we also download
all the images related to it by crawling all the pages of the image
gallery. In total, we collect 707K images corresponding to 597K
unique pHashes. Note that we obtain 15.6K out of 18.3K entries, as
we crawled the site several months before May 2018.
Getting to know KYM. We also perform a general characteriza-
tion of KYM. First, we look at the distribution of entries across
categories: as expected, the majority (57%) are memes, followed
by subcultures (30%), cultures (3%), websites (2%), and people (2%).
Next, we measure the number of images per entry: as shown in