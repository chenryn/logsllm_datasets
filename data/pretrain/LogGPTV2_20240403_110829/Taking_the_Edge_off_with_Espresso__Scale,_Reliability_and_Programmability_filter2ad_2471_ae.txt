and prevented any user-visible impact. This control-plane
outage however reinforced the need for all control and man-
agement plane components to be included in integration
testing.
(2) Localized control plane for fast response. During the
initial deployment of Espresso, we observed an interesting
phenomenon: new routing prefixes were not being utilized
quickly after peering turnup, leading to under-utilization of
available capacity. The root-cause was slow propagation of
new prefixes to the GC and subsequent incorporation in the
forwarding rules. This is a feature in GC, implemented to
reduce churn in the global map computed by GC.
To reduce the time needed to utilize newly learned prefixes
without introducing unintended churn in the GC, we aug-
ment the local control plane to compute a set of default
forwarding rules based strictly on locally collected routing
prefixes within an edge metro. Espresso uses this default set
for traffic that is not explicitly steered by GC, allowing new
prefixes to be utilized quickly. This default also provides an
in-metro fallback in the event of a GC failure, increasing
(a) Routing update arrival rate.
(b) Route count (IPv4 and IPv6).
(c) Memory use of LPM and control plane structures.
Figure 9: Performance of packet processor. Data collected over a 3
month period from a live production host.
the packet processing CPUs, allowing lock-free packet processing.
We first install programming updates in a shadow LPM structure,
updating all packet processing path pointers to point to the new
LPM afterward. We use a compressed multibit trie to implement
IPv4 LPM and a binary trie to implement IPv6 LPM. See [31] for a
survey of possible LPM implementations. Figure 9b shows the route
counts, and Figure 9c the memory usage in an Espresso deploy-
ment over a period of 3 months. The LPMs contain approximately
443
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5 10 15 20 25 30 35 40 45 50CDFUpdates per second 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.98 2 2.02 2.04 2.06 2.08 2.1 2.12 2.14 2.16CDFRoute count (in millions) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.1 1.15 1.2 1.25 1.3 1.35 1.4 1.45CDFMemory (in GB)Taking the Edge off with Espresso
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
system reliability. This lesson demonstrates how a hierar-
chical control plane can manage global optimization while
maintaining or improving reliability.
(3) Global drain of PF traffic. To aid incremental deployment
of Espresso and emergency operational procedures, GC can
disable/enable use of a set of Espresso PF devices as an egress
option for traffic. An operator pushed erroneous GC con-
figuration that excluded all Espresso PF devices from the
set of viable egress options. Fortunately, we had sufficient
spare capacity available, and GC successfully shifted traffic
to other peering devices so that there was minimal user-
visible impact. This outage though again demonstrated the
perils of a global control system, and the need to have addi-
tional safety checks. GC supports a simulation environment
to evaluate the effects of a drain, but the simulation results
were overlooked, and the configuration was still pushed.
To improve availability, we bolstered the simulation envi-
ronment by making large traffic shifts show up as failures,
and introduced additional safety checks in GC to prohibit
draining significant traffic from peering devices.
(4) Ingress traffic blackholing. Espresso’s most user visible
outage resulted from our phased development model. Before
implementing ACL-based filtering in end hosts, we restricted
announcement of prefixes to Espresso peers to the subset
that we could support filtering with the limited PF hard-
ware TCAMs. An inadvertent policy change on one of our
backbone routers caused external announcement of most
of Google prefixes via the PFs. Some of the attracted traffic,
e.g., VPN, did not find an "allow" match in the PF (which
has limited TCAM) and was blackholed. We subsequently
fixed our BGP policies to prevent accidental leaking of routes
to Espresso peers, evaluated the limited ACLs on PF to al-
low additional protocols, and expedited the work to deploy
complete Internet ACL filtering in end-hosts. This outage
demonstrates the risks associated with introducing new net-
work functionality, and how subtle interactions with existing
infrastructure can adversely affect availability.
8 RELATED WORK
Recently, several large-scale SDN deployments [11, 21, 29] have
been discussed in the literature. These deployments only had to
interoperate with a controlled number of vendors and/or software.
SDN peering [17, 18] provides finer-grained policy but does not
allow application-specific traffic engineering. This work is also
limited to the PoP scale while Espresso targets a globally avail-
able peering surface. None of these deployments target high-level
availability required for a global scale public peering. A number of
efforts [4, 28] target more cost-effective hardware for peering; we
show our approach end-to-end with an evaluation of a large-scale
deployment.
Espresso employs centralized traffic engineering to select egress
based on application requirements and fine-grained communication
patterns. While centralized traffic engineering [7, 8, 20–22] is not
novel, doing so considering end-user metrics at Internet scale has
not been previously demonstrated. We also achieve substantial
flexibility by leveraging host-based encapsulation to implement
444
the centralized traffic engineering policy by programming Internet-
sized FIBs in packet processors rather than in the network, taking
earlier ideas [9, 25] further.
Edge Fabric [27] has similar traffic engineering goals as Espresso.
However, their primary objective is to relieve congested peering
links in a metro while Espresso aims at fine-grained global optimiza-
tion of traffic. Consequently, Edge Fabric has independent local and
global optimization which does not always yield globally optimal
traffic placement, and relies on BGP to steer traffic. On the other
hand, Espresso integrates egress selection with our global TE sys-
tem allowing us to move traffic to a different serving location and to
perform fine-grained traffic engineering at the hosts. Espresso also
considers downstream congestion which Edge Fabric is exploring
as future work. Another key difference lies in Espresso’s use of
commodity MPLS switches, in contrast with Edge Fabric that relies
on peering routers with Internet-scale forwarding tables. We have
found that such peering routers comprise a substantial portion of
our overall network costs.
Espresso’s declarative network management is similar to Robo-
tron [30]. We further build a fully automated configuration pipeline
where a change in intent is automatically and safely staged to all
devices.
9 CONCLUSIONS
Two principal criticisms of SDN is that it is best suited to walled
gardens that do not require interoperability at Internet scale and
that SDN mainly targets cost reductions. Through a large-scale
deployment of Espresso—a new Internet peering architecture—we
shed light on this discussion in two ways. First, we demonstrate that
it is possible to incrementally evolve, in place, a traditional peering
architecture based on vendor gear and maintain full interoperability
with peers, their myriad policies, and varied hardware/protocol
deployments at the scale of one of the Internet’s largest content
provider networks. Second, we show that the value of SDN comes
from the capability and its software feature velocity, with any cost
savings being secondary.
In particular, Espresso decouples complex routing and packet-
processing functions from the routing hardware. A hierarchical
control-plane design and close attention to fault containment for
loosely-coupled components underlie a system that is highly re-
sponsive, highly reliable, and supports global/centralized traffic op-
timization. After more than a year of incremental rollout, Espresso
supports six times the feature velocity, 75% cost-reduction, many
novel features and exponential capacity growth relative to tradi-
tional architectures. It carries more than 22% of all of Google’s
Internet traffic, with this fraction rapidly increasing.
ACKNOWLEDGMENT
Many teams has contributed to the success of Espresso and it would
be impossible to list everyone that has helped make the project
successful. We would like to acknowledge the contributions of G-
Scale Network Engineering, Network Edge (NetEdge), Network
Infrastructure (NetInfra), Network Software (NetSoft), Platforms
Infrastructure Engineering (PIE), Site Reliability Engineering SRE,
including, Yuri Bank, Matt Beaumont-Gay, Bernhard Beck, Matthew
Blecker, Luca Bigliardi, Kevin Brintnall, Carlo Contavalli, Kevin
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
K.K. Yap, M. Motiwala, et al.
Fan, Mario Fanelli, Jeremy Fincher, Wenjian He, Benjamin Helsley,
Pierre Imai, Chip Killian, Vinoj Kumar, Max Kuzmin, Perry Lorier,
Piotr Marecki, Waqar Mohsin, Michael Rubin, Erik Rubow, Murali
Suriar, Srinath Venkatesan, Lorenzo Vicisano, Carmen Villalobos,
Jim Wanderer, Zhehua Wu to name a few. We also thank our re-
viewers, shepherd Kun Tan, Jeff Mogul, Dina Papagiannaki and
Anees Shaikh for their amazing feedback that has made the paper
better.
REFERENCES
[1] 2010. GNU Quagga Project. www.nongnu.org/quagga/. (2010).
[2] 2013. Best Practices in Core Network Capacity Planning. White Paper. (2013).
[3] 2017.
- Monitoring system & time series database.
Prometheus
https://prometheus.io/. (2017).
[4] Joo Taveira Arajo. 2016. Building and scaling the Fastly network, part 1: Fighting
the FIB. https://www.fastly.com/blog/building-and-scaling-fastly-network-part-
1-fighting-fib. (2016). [Online; posted on May 11, 2016].
[5] Ajay Kumar Bangla, Alireza Ghaffarkhah, Ben Preskill, Bikash Koley, Christoph
Albrecht, Emilie Danna, Joe Jiang, and Xiaoxue Zhao. 2015. Capacity planning
for the Google backbone network. In ISMP 2015 (International Symposium on
Mathematical Programming).
[6] Mike Burrows. 2006. The Chubby lock service for loosely-coupled distributed
systems. In Proceedings of the 7th symposium on Operating systems design and
implementation. USENIX Association, 335–350.
[7] Matthew Caesar, Donald Caldwell, Nick Feamster, Jennifer Rexford, Aman Shaikh,
and Jacobus van der Merwe. 2005. Design and Implementation of a Routing Con-
trol Platform. In Proceedings of the 2Nd Conference on Symposium on Networked
Systems Design & Implementation - Volume 2 (NSDI’05). USENIX Association,
Berkeley, CA, USA, 15–28. http://dl.acm.org/citation.cfm?id=1251203.1251205
[8] Martin Casado, Michael J. Freedman, Justin Pettit, Jianying Luo, Nick McKeown,
and Scott Shenker. 2007. Ethane: Taking Control of the Enterprise. SIGCOMM
Comput. Commun. Rev. 37, 4 (Aug. 2007), 1–12. https://doi.org/10.1145/1282427.
1282382
[9] Martin Casado, Teemu Koponen, Scott Shenker, and Amin Tootoonchian. 2012.
Fabric: A Retrospective on Evolving SDN. In Proceedings of the First Workshop on
Hot Topics in Software Defined Networks (HotSDN ’12). ACM, New York, NY, USA,
85–90. https://doi.org/10.1145/2342441.2342459
[10] Florin Dobrian, Vyas Sekar, Asad Awan, Ion Stoica, Dilip Joseph, Aditya Ganjam,
Jibin Zhan, and Hui Zhang. 2011. Understanding the Impact of Video Quality
on User Engagement. In Proceedings of the ACM SIGCOMM 2011 Conference
(SIGCOMM ’11). ACM, New York, NY, USA, 362–373. https://doi.org/10.1145/
2018436.2018478
[11] Sarah Edwards, Xuan Liu, and Niky Riga. 2015. Creating Repeatable Computer
Science and Networking Experiments on Shared, Public Testbeds. SIGOPS Oper.
Syst. Rev. 49, 1 (Jan. 2015), 90–99. https://doi.org/10.1145/2723872.2723884
[12] Nick Feamster. 2016. Revealing Utilization at Internet Interconnection Points.
CoRR abs/1603.03656 (2016). http://arxiv.org/abs/1603.03656
[13] Nick Feamster, Jay Borkenhagen, and Jennifer Rexford. 2003. Guidelines for
interdomain traffic engineering. ACM SIGCOMM Computer Communication
Review 33, 5 (2003), 19–30.
[14] O. Filip. 2013. BIRD internet routing daemon. http://bird.network.cz/. (May
2013).
[15] Tobias Flach, Nandita Dukkipati, Andreas Terzis, Barath Raghavan, Neal Card-
well, Yuchung Cheng, Ankur Jain, Shuai Hao, Ethan Katz-Bassett, and Ramesh
Govindan. 2013. Reducing Web Latency: the Virtue of Gentle Aggression. In
Proceedings of the ACM Conference of the Special Interest Group on Data Commu-
nication (SIGCOMM ’13). http://conferences.sigcomm.org/sigcomm/2013/papers/
sigcomm/p159.pdf
[16] Ramesh Govindan, Ina Minei, Mahesh Kallahalla, Bikash Koley, and Amin Vahdat.
2016. Evolve or Die: High-Availability Design Principles Drawn from Googles
Network Infrastructure. In Proceedings of the 2016 Conference on ACM SIGCOMM
2016 Conference (SIGCOMM ’16). ACM, New York, NY, USA, 58–72. https://doi.
org/10.1145/2934872.2934891
[17] Arpit Gupta, Robert MacDavid, Rüdiger Birkner, Marco Canini, Nick Feamster,
Jennifer Rexford, and Laurent Vanbever. 2016. An Industrial-scale Software
Defined Internet Exchange Point. In Proceedings of the 13th Usenix Conference on
Networked Systems Design and Implementation (NSDI’16). USENIX Association,
Berkeley, CA, USA, 1–14. http://dl.acm.org/citation.cfm?id=2930611.2930612
[18] Arpit Gupta, Laurent Vanbever, Muhammad Shahbaz, Sean Patrick Donovan,
Brandon Schlinker, Nick Feamster, Jennifer Rexford, Scott Shenker, Russ Clark,
and Ethan Katz-Bassett. 2014. SDX: A Software Defined Internet Exchange.
SIGCOMM Comput. Commun. Rev. 44, 4 (Aug. 2014), 579–580. https://doi.org/10.
1145/2740070.2631473
[19] Mark Handley, Orion Hodson, and Eddie Kohler. 2003. XORP: An Open Platform
for Network Research. SIGCOMM Comput. Commun. Rev. 33, 1 (Jan. 2003), 53–57.
https://doi.org/10.1145/774763.774771
[20] Chi-Yao Hong, Srikanth Kandula, Ratul Mahajan, Ming Zhang, Vijay Gill, Mohan
Nanduri, and Roger Wattenhofer. 2013. Achieving high utilization with software-
driven WAN. In ACM SIGCOMM Computer Communication Review, Vol. 43. ACM,
15–26.
[21] Sushant Jain, Alok Kumar, Subhasree Mandal, Joon Ong, Leon Poutievski, Arjun
Singh, Subbaiah Venkata, Jim Wanderer, Junlan Zhou, Min Zhu, et al. 2013. B4:
Experience with a globally-deployed software defined WAN. ACM SIGCOMM 43,
4, 3–14.
[22] Matthew K Mukerjee, David Naylor, Junchen Jiang, Dongsu Han, Srinivasan
Seshan, and Hui Zhang. 2015. Practical, real-time centralized control for cdn-
based live video delivery. ACM SIGCOMM Computer Communication Review 45,
4 (2015), 311–324.
[23] Abhinav Pathak, Y Angela Wang, Cheng Huang, Albert Greenberg, Y Charlie
Hu, Randy Kern, Jin Li, and Keith W Ross. 2010. Measuring and evaluating TCP
splitting for cloud services. In International Conference on Passive and Active
Network Measurement. Springer Berlin Heidelberg, 41–50.
[24] Rachel Potvin and Josh Levenberg. 2016. Why Google Stores Billions of Lines
of Code in a Single Repository. Commun. ACM 59, 7 (June 2016), 78–87. https:
//doi.org/10.1145/2854146
[25] Barath Raghavan, Martín Casado, Teemu Koponen, Sylvia Ratnasamy, Ali Ghodsi,
and Scott Shenker. 2012. Software-defined Internet Architecture: Decoupling
Architecture from Infrastructure. In Proceedings of the 11th ACM Workshop on
Hot Topics in Networks (HotNets-XI). ACM, New York, NY, USA, 43–48. https:
//doi.org/10.1145/2390231.2390239
[26] S. Sangli, E. Chen, R. Fernando, J. Scudder, and Y. Rekhter. 2007. Graceful Restart
Mechanism for BGP. RFC 4724 (Proposed Standard). (Jan. 2007). http://www.ietf.
org/rfc/rfc4724.txt
[27] Brandon Schlinker, Hyojeong Kim, Timothy Chiu, Ethan Katz-Bassett, Harsha
Madhyastha, Italo Cunha, James Quinn, Saif Hasan, Petr Lapukhov, and Hongyi
Zeng. 2017. Engineering Egress with Edge Fabric. In Proceedings of the ACM
SIGCOMM 2017 Conference (SIGCOMM ’17). ACM, New York, NY, USA.
[28] Tom Scholl. 2013. Building A Cheaper Peering Router. NANOG50. (2013). nLayer
Communications, Inc.
[29] Arjun Singh, Joon Ong, Amit Agarwal, Glen Anderson, Ashby Armistead, Roy
Bannon, Seb Boving, Gaurav Desai, Bob Felderman, Paulie Germano, Anand
Kanagala, Hong Liu, Jeff Provost, Jason Simmons, Eiichi Tanda, Jim Wanderer,
Urs Hölzle, Stephen Stuart, and Amin Vahdat. 2016. Jupiter Rising: A Decade
of Clos Topologies and Centralized Control in Google’s Datacenter Network.
Commun. ACM 59, 9 (Aug. 2016), 88–97. https://doi.org/10.1145/2975159
[30] Yu-Wei Eric Sung, Xiaozheng Tie, Starsky H.Y. Wong, and Hongyi Zeng. 2016.
Robotron: Top-down Network Management at Facebook Scale. In Proceedings of
the 2016 Conference on ACM SIGCOMM 2016 Conference (SIGCOMM ’16). ACM,
New York, NY, USA, 426–439. https://doi.org/10.1145/2934872.2934874
[31] David E Taylor. 2005. Survey and taxonomy of packet classification techniques.
ACM Computing Surveys (CSUR) 37, 3 (2005), 238–275.
445