latency peering link is encountered in a path, it is very likely to be
a bottleneck. High latency intra-ISP links, on the other hand, are
not overly likely to be bottlenecks (e.g., 11% of bottlenecks, and
13.5% of overall hops on paths to tier-2 ).
In general, Figure 6 suggests that peering links have a higher
likelihood of being bottlenecks, consistent with our earlier results.
This holds for low, medium, and high-latency peering links. For ex-
ample, very few paths have any medium latency peering links, yet
they account for a signiﬁcant proportion of bottlenecks in all types
of paths. Also, low-latency peering links on paths to the lower tiers
(i.e., tier-3 and tier-4) have a particularly high likelihood of being
bottlenecks, when compared to paths to tier-1 and tier-2 destina-
tions. Recall from Figures 5(b) and (c) that these lower-tier peering
bottlenecks also have much less available bandwidth.
3.5 Bottlenecks at Public Exchange Points
As mentioned in Section 2, one of our goals was to explore
the common perception that public exchanges are usually network
choke points, to be avoided whenever possible. Using the proce-
dure outlined in Section 2.2.2, we identiﬁed a large number of paths
passing through public exchanges, and applied BFind to identify
any bottlenecks along these paths.
As indicated in Figure 7(a), we tested 466 paths through public
exchange points. Of the measured paths, 170 (36.5%) had a bot-
tleneck link. Of these, only 70 bottlenecks (15% overall) were at
the exchange point. This is in contrast to the expectation that many
exchange point bottlenecks would be identiﬁed on such paths. It is
interesting to consider, however, that the probability that the bot-
tleneck link is located at the exchange is about 41% (= 70/170).
In contrast, Figures 3(a) and 4(a) do not show any other type of
link (intra-ISP or peering) responsible for a larger percentage of
bottlenecks.4 This observation suggests that if there is a bottleneck
on a path through a public exchange point, it is likely to be at the
exchange itself nearly half of the time.
4. DISCUSSION
Our study, while to some degree conﬁrms conventional wisdom
about the location of Internet bottlenecks, yields a number of in-
teresting and unexpected ﬁndings about the characteristics of these
links. For example, we ﬁnd a substantial number of bottleneck
links within carrier ISPs. In addition, we also observed that low
latency links, whether within ISPs or between them, can also con-
strain available bandwidth with a small, yet, signiﬁcant probability.
Furthermore, our observations can provide some guidance when
considering other related issues such as choosing an access provider,
optimizing routes through the network, or analyzing performance
4However, in Figure 4(a), bottlenecks between tiers 1 and 3 in paths
to tier-3 destinations are comparable to bottlenecks at exchange
points in this respect.
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
y
c
n
e
u
q
e
r
F
e
v
i
t
a
u
m
u
C
l
0
0
5
#Paths to exchange points
#Paths with non-access bottlenecks
#Bottlenecks at exchange point
466
170
70
NAP Links
15
10
40
Estimate Bottleneck Available Bandwidth (in Mbps)
30
20
25
35
45
50
(a) Relative prevalence
(b) Available bandwidth distribution
Figure 7: Bottlenecks in paths to exchange points: Table (a) on the left shows the relative prevalence of bottleneck links at the
exchange points. Figure (b) shows the distribution of the available capacity for bottleneck links at the exchange points.
implications of bottlenecks in practice. In this section we discuss
some of these issues in the context of our empirical ﬁndings.
4.1 Providers and Provisioning
Our measurements show that there is a clear performance advan-
tage to using a tier-1 provider. Our results also show that small
regional providers, exempliﬁed by the tier-4 ASes in our study,
have relatively low-speed connectivity to their upstream carrier, ir-
respective of the upstream carrier’s size. In addition, their networks
often exhibit bottlenecks (as we deﬁne them). This may be consid-
ered a reﬂection of the impact of economics on network provision-
ing if we assume that carriers lower in the AS hierarchy are less
inclined to overprovision their networks if their typical customer
trafﬁc volume does not thus far require it. As a result, there is
a clear disadvantage to using a tier-4 provider for high-speed con-
nectivity. However, the tradeoffs between tier-2 and tier-3 networks
are much less clear.
Paths to tier-3 destinations had a larger percentage of bottleneck
links than tier-2 paths. Despite this, we also observed that tier-2 and
tier-3 bottlenecks show similar characteristics in terms of available
capacity, with tier-3 bottlenecks (both intra-AS and peering links)
performing slightly better in some cases. This might be explained
if we conjecture that tier-2 ASes, by virtue of their higher degree of
reachability, carry a larger volume of trafﬁc relative to their capac-
ity, when compared with tier-3 ASes. Extending this hypothesis,
we might conclude that if a stub network desires reasonably wide
connectivity, then choosing a tier-3 provider might be a beneﬁcial
choice, both economically and in terms of performance, assuming
that connectivity to tier-3 providers is less expensive.
4.2 Network Under-utilization
More than 50% of the paths we probed seemed to have an avail-
able capacity close to 40-50 Mbps or maybe more. This is true
across most non-access links irrespective of their type. We hypoth-
esize from this that large portions of the network are potentially
under-utilized on average, conﬁrming what many large ISPs report
about the utilization of their backbone networks. However, the fact
that this holds even for providers of smaller size (e.g. tier-3) as well
as for most peering links and even links at NAPs, seems surprising.
This observation about under-utilization, coupled with our re-
sults about the existence of potential hot-spots with low available
bandwidth, opens the following key question – Is it possible to
avoid these bottlenecks by leveraging existing routing protocols?
While there has been considerable work on load-sensitive routing
of trafﬁc within an AS, little is known about how to extend this
across ASes. We plan to explore this path in the future.
4.3 Route Optimization
It is sometimes suggested that a large proportion of the peer-
ing links between large carrier ISPs (tier-1) could emerge as bot-
tlenecks, due to the lack of economic incentive to provision these
links and the large volume of trafﬁc carried over them. However,
our measurements seem to suggest otherwise. We believe that this
could imply that either the peering links are in fact quite well pro-
visioned, or that a smaller portion of the entire Internet trafﬁc tra-
verses these links than what might be expected intuitively.
While it is difﬁcult to discern the exact cause for this lack of bot-
tlenecks, it may have important implications for the design of sys-
tems or choice of routes. For example, purchasing bandwidth from
two different tier-1 ISPs may be signiﬁcantly better from a perfor-
mance perspective than buying twice as much bandwidth from a
single tier-1 ISP.5 In fact, it might be more economical to purchase
from one ISP. Similarly, a shorter route to a destination that passed
through a tier-1 to tier-1 peering link might be better than a longer
route that stays within a single, lower-tier provider.
5. RELATED WORK
Several earlier research efforts have shared our high-level goal
of measuring and characterizing wide-area network performance.
This past work can be roughly divided into two areas: 1) measure-
ment studies of the Internet, and 2) novel algorithms and tools for
measuring Internet properties.
In this section we review several
recent representative efforts from each of these categories.
5.1 Measurement Studies
Typically, measurement studies to characterize performance in
the Internet have taken two forms: 1) some, such as [23, 36, 19,
32], use active probing to evaluate the end-to-end properties of In-
ternet paths and, 2) other studies, such as [2, 35] have used passive
monitoring or packet traces of Internet ﬂows to observe their per-
formance in the Internet.
In [23] multiple TCP bulk transfers between pairs of measure-
ment end-points are monitored to show evidence of signiﬁcant packet
re-ordering, correlated packet losses, and frequent delay variations
on small scales. The authors also describe the distribution of bot-
tleneck capacities observed in the transfers. The study by Savage
et al. used latency and loss measurements between network end-
points to compare the quality of direct and indirect paths between
nodes [32]. The authors note that the performance gains come
from avoiding congestion and using shorter latency paths. Using
active measurements in the NIMI [25] infrastructure, Zhang et al.
5Of course, it might be useful for reliability purposes.
Non-access bottlenecks are equally likely to be links within ISPs or peering links between ISPs
The likelihood of a bottleneck increases on paths to lower tier ISPs
Interior and peering bottlenecks in tier-2 and tier-3 ISPs exhibit very similar available capacity
Internal links in lower tier ISPs appear as bottlenecks with greater frequency than their overall presence in typical paths
Bottlenecks appeared in only 15% of the paths traversing public exchanges, but when a bottleneck is found on such paths, the likelihood
of it being at the exchange is more than 40%
All paths have a high proportion of low-latency links (interior and peering) and roughly one high-latency interior link
Table 4: Summary of key observations
study the constancy of Internet paths in terms of delay, loss, and
throughput [36]. For each notion of constancy, they observed that
all three properties were steady on at least a minute’s timescale.
Finally, a recent study of delay and jitter across several large back-
bone providers aimed to classify paths according to their suitabil-
ity for latency-sensitive applications [19]. The authors found that
most paths exhibited very little delay variation, but very few con-
sistently experienced no loss. In comparison with these efforts, our
work has a few key differences. First, rather than exploring true
end-to-end paths, our measurement paths are intended to probe the
non-access part of the Internet, i.e., the part responsible for carry-
ing data between end networks. Second, we measure which part of
the network may limit the performance of end-to-end paths.
In [2], the authors study packet-level traces to and from a very
large collection of end-hosts, and observe a a wide degree of per-
formance variation, as characterized by the observed TCP through-
put. With a similar set of goals, Zhang et al. analyze packet traces
to understand the distribution of Internet ﬂow rates and the causes
thereof [35]. They ﬁnd that network congestion and TCP receiver
window limitations often constrain the observed throughput. In this
paper, our aim is not to characterize what performance end-hosts
typically achieve and what constrains the typical performance. In-
stead, we focus on well-connected and unconstrained end-points
(e.g., no receiver window limitations) and comment on how ISP
connectivity constrains the performance seen by such end-points.
5.2 Measurement Tools
The development of algorithms and tools to estimate the band-
width characteristics of Internet paths continues to be an active re-
search area (see [6] for a more complete list). Tools like bprobe [5],
Nettimer [17], and PBM [23] use packet-pair like mechanisms to
measure the raw bottleneck capacity along a path. Other tools like
clink [7], pathchar [12], pchar [18], and pipechar [10], character-
ize hop-by-hop delay, raw capacity, and loss properties of Inter-
net paths by observing the transmission behavior of different sized
packets. A different set of tools, well-represented by pathload [14],
focus on the available capacity on a path. These tools, unlike
BFind, require control over both the end-points of the measure-
ment. Finally, the TReno tool [20] follows an approach most sim-
ilar to ours, using UDP packets to measure available bulk transfer
capacity. It sends hop-limited UDP packets toward the destination,
and emulates TCP congestion control by using sequence numbers
contained in the ICMP error responses. TReno probes each hop
along a path in turn for available capacity. Therefore, when used
to identify bottlenecks along a path, TReno will likely consume
ICMP processing resources for every probe packet at each router
being probed as it progresses hop-by-hop. As a result, for high-
speed links, TReno is likely to be more intrusive than our tool.
In addition to available bandwidth, link loss and delay are of-
ten performance metrics of interest. Recent work by Bu, et al.
describes algorithms that infer and estimate loss rates and delay
distributions on links in a network using multicast trees [4].
In this paper we develop a mechanism that measures the avail-
able capacity on the path between a controlled end-host and an arbi-
trary host in the Internet. In addition, we identify the portion of the
network responsible for the bottleneck. Our tool uses an admittedly
heavyweight approach in the amount of bandwidth it consumes.
6. SUMMARY
This goal of this paper was to explore the following fundamental
issue: if end networks upgrade their access speeds, which portions
of the rest of the Internet are likely to become hot-spots? To answer
this question, we performed a large set of diverse measurements of
typical paths traversed in the Internet. We identiﬁed non-access
bottlenecks along these paths and studied their key characteristics
such as location and prevalence (links within ISPs vs. between
ISPs), latency (long-haul vs. local), and available capacity. Table 4
summarizes some of our key observations.
The results from our measurements mostly support conventional
wisdom by quantifying the key characteristics of non-access bot-
tlenecks. However, some of our key conclusions show trends in
the prevalence of non-access bottlenecks that are unexpected. For
example, our measurements show that the bottleneck on any path
is roughly equally likely to be either a peering link or a link inside
an ISP. We also quantify the likelihood that paths through public
exchange points have bottlenecks appearing in the exchange.
In addition, our measurements quantify the relative performance
beneﬁts offered by ISPs belonging to different tiers in the AS hier-
archy. Interestingly, we ﬁnd that there is no signiﬁcant difference
between ISPs in tiers 2 and 3 in this respect. As expected, we ﬁnd
that tier-1 ISPs offer the best performance and tier-4 ISPs contain
the most bottlenecks.
In summary, we believe that our work provides key insights into
how the future network should evolve on two fronts. Firstly, our
results can be used by ISPs to help them evaluate their providers
and peers. Secondly, the observations from our work can also prove
helpful to stub networks in picking suitable upstream providers.
Acknowledgment
We are very grateful to Kang-Won Lee, Jennifer Rexford, Albert
Greenberg, Brad Karp, Bruce Maggs and Prashant Pradhan for their
valuable suggestions on this paper. We also thank our anonymous
reviewers for their detailed feedback.
7. REFERENCES
[1] D. Andersen, H. Balakrishnan, M. Kaashoek, and R. Morris.
Resilient Overlay Networks. In Proceedings of the 18th
Symposium on Operating System Principles, Banff, Canada,
October 2001.
[2] H. Balakrishnan, S. Seshan, M. Stemm, and R. H. Katz.
Analyzing stability in wide-area network performance. In
Proceedings of ACM SIGMETRICS, Seattle, WA, June 1997.
[3] L. S. Brakmo, S. W. O’Malley, and L. L. Peterson. TCP
Vegas: New Techniques for Congestion Detection and
Avoidance. In Proceedings of the SIGCOMM ’94 Symposium
on Communications Architectures and Protocols, August
1994.
[4] T. Bu, N. Dufﬁeld, F. L. Presti, and D. Towsley. Network
tomography on general topologies. In Proceedings of ACM
SIGMETRICS, Marina Del Ray, CA, June 2002.
[5] R. L. Carter and M. E. Crovella. Measuring bottleneck link
speed in packet-switched networks. Performance Evaluation,
27–28:297–318, October 1996.
[6] Cooperative Association for Internet Data Analysis
(CAIDA). Internet tools taxonomy.
http://www.caida.org/tools/taxonomy/,
October 2002.
[7] A. Downey. Using pathchar to estimate internet link
characteristics. In Proceedings of ACM SIGCOMM,
Cambridge, MA, August 1999.
[8] L. Gao. On inferring autonomous system relationships in the
Internet. IEEE/ACM Transactions on Networking, 9(6),
December 2001.
[9] R. Govindan and V. Paxson. Estimating router ICMP
generation delays. In Proceedings of Passive and Active
Measurement Workshop (PAM), Fort Collins, CO, 2002.
[10] J. Guojun, G. Yang, B. R. Crowley, and D. A. Agarwal.
Network characterization service (NCS). In Proceedings of
IEEE International Symposium on High Performance
Distributed Computing (HPDC), San Francisco, CA, August
2001.
[11] U. Hengartner, S. Moon, R. Mortier, and C. Diot. Detection
and analysis of routing loops in packet traces. In Proceedings
of ACM SIGCOMM Internet Measurement Workshop (IMW),
November 2002.
[12] V. Jacobson. pathchar – A Tool to Infer Characteristics of
Internet Paths. ftp://ee.lbl.gov/pathchar/, 1997.
[13] M. Jain and C. Dovrolis. End-to-end available bandwidth:
Measurement methodology, dynamics, and relation with
TCP throughput. In Proceedings of ACM SIGCOMM,
Pittsburgh, PA, August 2002.
[14] M. Jain and C. Dovrolis. Pathload: A measurement tool for
end-to-end available bandwidth. In Proceedings of Passive
and Active Measurement Workshop (PAM), Fort Collins, CO,
March 2002.
[15] S. Jaiswal, G. Iannaccone, C. Diot, J. Kurose, and
D. Towsley. Measurement and classiﬁcation of
out-of-sequence packets in a tier-1 IP backbone. In
Proceedings of ACM SIGCOMM Internet Measurement
Workshop (IMW), November 2002.
[16] C. Labovitz, A. Ahuja, and F. Jahanian. Experimental study
of Internet stability and backbone failures. In Proceedings of
IEEE International Symposium on Fault-Tolerant Computing
(FTCS), Madison, WI, June 1999.
[17] K. Lai and M. Baker. Nettimer: A tool for measuring
bottleneck link bandwidth. In Proceedings of USENIX
Symposium on Internet Technologies and Systems, March
2001.
[18] B. A. Mah. pchar: A tool for measuring internet path
characteristics. http://www.employees.org/
˜bmah/Software/pchar/, June 2000.
[19] A. P. Markopoulou, F. A. Tobagi, and M. J. Karam.
Assessment of VoIP quality over Internet backbones. In
Proceedings of IEEE INFOCOM’02, New York, NY, June
2002.
[20] M. Mathis and J. Mahdavi. Diagnosing Internet Congestion
with a Transport Layer Performance Tool . In Proc. INET
’96, Montreal, Canada, June 1996.
http://www.isoc.org/inet96/proceedings/.
[21] Network Characterization Service: Netest and Pipechar.
http://www-didc.lbl.gov/pipechar, 1999.
[22] ns-2 Network Simulator. http://www.isi.edu/nsnam/ns/,
2000.
[23] V. Paxson. End-to-end internet packet dynamics.
Proceedings of the SIGCOMM ’97 Symposium on
Communications Architectures and Protocols, pages
139–152, September 1997.
[24] V. Paxson. End-to-end routing behavior in the internet.
IEEE/ACM Transactions on Networking, 5(5):601–615,
October 1997.
[25] V. Paxson, A. Adams, and M. Mathis. Experiences with
NIMI. In Proceedings of Passive and Active Measurement
Workshop (PAM), Hamilton, New Zealand, April 2000.
[26] PlanetLab. http://www.planet-lab.org, 2002.
[27] RADB whois Server. whois.radb.net.
[28] RIPE whois Service. whois.ripe.net.
[29] BGP Tables from the University of Oregon RouteViews
Project. http://moat.nlanr.net/AS/data.
[30] University of Oregon, RouteViews Project.
http://www.routeviews.org.
[31] S. Savage, T. Anderson, A. Aggarwal, D. Becker,
N. Cardwell, A. Collins, E. Hoffman, J. Snell, A. Vahdat,
J. Voelker, and J. Zahorjan. Detour: a case for informed
internet routing and transport. IEEE Micro, volume 19 no.
1:50–59, January 1999.
[32] S. Savage, A. Collins, E. Hoffman, J. Snell, and T. Anderson.