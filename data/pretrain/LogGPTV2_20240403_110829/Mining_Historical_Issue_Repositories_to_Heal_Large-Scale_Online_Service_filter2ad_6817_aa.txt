title:Mining Historical Issue Repositories to Heal Large-Scale Online Service
Systems
author:Rui Ding and
Qiang Fu and
Jian-Guang Lou and
Qingwei Lin and
Dongmei Zhang and
Tao Xie
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Mining Historical Issue Repositories to Heal Large-Scale Online Service
Systems
Rui Ding, Qiang Fu, Jian-Guang Lou,
Qingwei Lin, Dongmei Zhang
Microsoft Research
(juding, qifu, jlou, qlin, dongmeiz)
@microsoft.com
Tao Xie
Department of Computer Science
University of Illinois at Urbana-Champaign
PI:EMAIL
Abstract—Online service systems have been increasingly
popular and important nowadays. Reducing the MTTR
(Mean Time to Restore) of a service remains one of
the most important steps to assure the user-perceived
availability of the service. To reduce the MTTR, a common
practice is to restore the service by identifying and apply-
ing an appropriate healing action. In this paper, we present
an automated mining-based approach for suggesting an
appropriate healing action for a given new issue. Our
approach suggests an appropriate healing action by adapt-
ing healing actions from the retrieved similar historical
issues. We have applied our approach to a real-world and
large-scale product online service. The studies on 243 real
issues of the service show that our approach can effectively
suggest appropriate healing actions (with 87% accuracy)
to reduce the MTTR of the service. In addition, according
to issue characteristics, we further study and categorize
issues where automatic healing suggestion faces difﬁculties.
Keywords-Online service system; healing action; issue
repository; incident management
I. INTRODUCTION
Online service systems such as online banking, e-
commerce, and email services have been increasingly
popular and important nowadays, with an increasing
demand on the availability of services provided by
these systems. While signiﬁcant efforts have been made
to strive for keeping services up continuously, studies
[1] on a sample of hosts have shown that daily and
weekly service downs still appear commonly in online
services. A serious service down for a non-trivial period
often results in huge economic loss or other serious
consequences. For example, customers of a service
provider may turn to competing providers if the offered
services are not available.
In practice, services are often continuously monitored
to detect service issues by checking whether service
quality violates one of a set of strict pre-deﬁned rules.
When a service issue is detected, engineers are called
to resolve the issue to pro-actively prevent the issue’s
potential impact to user-perceived service availability.
Thus, reducing the MTTR (Mean Time to Restore) of
a service remains one of the most important steps to
assure the user-perceived availability of the service [2].
In order to reduce MTTR, a common practice is to
restore the service by identifying and applying an appro-
priate healing action [3] (i.e., a temporary workaround
action, such as rebooting a SQL machine) after the
occurrence of an issue. Then, after service restoration,
identifying and ﬁxing of underlying root causes for the
issue are conducted via ofﬂine postmortem analysis. In
other words, directly applying of an appropriate healing
action for the issue wins time for ofﬂine diagnosis and
ﬁxing of underlying root causes (which typically take
relatively longer time to resolve).
However, manually identifying an appropriate healing
action for a given new issue is time consuming and
error prone. Such manual process is based on investigat-
ing service-instrumented data such as transaction logs.
According to an internal study from an online service
team, about 90% time of MTTR is spent on manual
effort for identifying an appropriate healing action. Such
substantial manual effort is due to two factors. First,
investigating a large amount of service-instrumented
data is time consuming. Second, understanding the issue
and providing appropriate healing action are heavily
depends on domain knowledge. For example, each ma-
chine in a real-world product online service (studied in
our evaluation in Section IV) produces about 6,000 lines
of transaction logs per minute on average. Operators
need to inspect these logs from several (usually 4 to
12) machines, and understand the symptom of the issue
by reading and reasoning the detailed log information.
To address high cost and error proneness of manually
identifying an appropriate healing action, in this paper,
we present an automated mining-based approach for
suggesting an appropriate healing action for a given
new issue. Our approach generates a signature for
an issue from its corresponding transaction logs and
then retrieves historical issues with similar signatures
from a historical issue repository. The historical issue
repository records the solved historical
issues. Each
978-1-4799-2233-8/14 $31.00 © 2014 IEEE
978-1-4799-2233-8/14 $31.00 © 2014 IEEE
978-1-4799-2233-8/14 $31.00 © 2014 IEEE
DOI 10.1109/DSN.2014.39
DOI 10.1109/DSN.2014.39
DOI 10.1109/DSN.2014.39
311
311
311
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. 
issue has a number of basic attributes: affected time,
affected location (e.g., speciﬁc cluster, network, or data-
center), real customer impact measurement, correspond-
ing transaction logs, etc., along with the appropriate
healing action taken by operators to heal the issue.
Finally, our approach suggests an appropriate healing
action by adapting healing actions of the retrieved
historical issues. In particular, our approach measures
the similarity between the transaction logs of the given
new issue and the transaction logs of a historical issue,
by addressing two major challenges due to the high-
correlation phenomenon and the weak-discrimination
phenomenon. The high-correlation phenomenon refers
to the correlation of event’s occurrences in transaction
logs for causing ineffective historical-issue retrieval.
The weak-discrimination phenomenon refers to noisy
events that appear relatively independent to the transac-
tion status (being in an issue state or compliant state).
Detailed examples for these phenomena are illustrated
in Section II.
To tackle challenges due to these phenomena, we
develop the technique of concept analysis to address
the high-correlation phenomenon and the technique of
contrast analysis to address the weak-discrimination
phenomenon. Then we deﬁne a novel similarity metric
to measure similarity between issues and retrieve similar
historical issues from the historical issue repository for
the given new issue. Finally, we develop a technique of
healing-suggestion adaptation to use predeﬁned rules to
analyze and adapt the healing actions of the retrieved
historical issues to derive a healing suggestion for the
given new issue.
In particular, our technique of concept analysis uses
Formal Concept Analysis (FCA) to obtain the concept
lattice, where highly-correlated events are grouped to-
gether as the intents [4] of concepts. Our technique
of contrast analysis analyzes the complementary set of
events between concepts directly linked through the
obtained concept lattice. Such analysis produces the
complementary sets of events highly correlated to the
transaction status.
Our technique of healing-suggestion adaptation de-
ﬁnes the verb + target + location structure to represent a
healing suggestion. Both the verb + target are extracted
from the retrieved historical issues for the given new
issue, whereas the location is extracted from transaction
logs of the given issue directly. The verb denotes a
speciﬁc action from the healing action of the retrieved
historical issues, such as “recycle” and “restart”. The
target denotes a service role from the healing action
of the retrieved historical issues, such as “Application
Pool”, “IIS (Internet Information Service)”, and “SQL”.
The location denotes the affected location of the given
new issue, such as “Asia/Network2/Farm332/SQL412-
002”.
We have deployed our healing system on one online
service (serving millions of online customers globally)
for more than half a year. During this period, 76
operators in this product team effectively diagnosed and
healed this online service with the intensive assistance
of our healing system. Our evaluations on this real-
world online service demonstrate the effectiveness of
our approach in real practice.
To further evaluate the capability, and potential lim-
itations of our approach, we randomly sampled 400
real service issues, and carefully studied our results by
simulation. We found that our approach cannot work
properly at 157 (39%) issues. We summarize the under-
lying reasons, which shed lights towards service auto-
healing. In summary, this paper makes the following
main contributions 1:
• We formulate the problem of suggesting healing
actions for a newly occurred issue as retrieving
similar resolved issues in the history, and our tech-
niques of concept analysis and contrast analysis
help address challenges and achieve high accuracy
on historical-issue retrieval.
• We evaluate our approach on the 243 issues that
occurred in the real-world online service in 2012.
The results show that our approach can effec-
tively suggest correct healing actions to reduce the
MTTR of the service.
• We summarize our experience of applying our
approach on the real-world service, investigating
issue characteristics and cases where automatic
healing suggestion faces difﬁculties in practice.
The paper is organized as follows. Section II presents
examples. Section III presents our approach. Section IV
presents evaluation results. Section VI discusses related
work. Section VII concludes.
II. EXAMPLES
Transaction logs are printed during system execution.
Figure 1 shows a log stream collected for two exam-
ple transaction instances (within the occurring period
of an issue): the user-login transaction instance (the
highlighted log entries sharing the same transaction ID)
and ﬁle-editing transaction instance (the un-highlighted
log entries sharing the same transaction ID). Such logs
record relatively detailed information about run-time
behaviors of a system.
1This paper signiﬁcantly extends the previous version of this work
(a 4-page ASE 2012 short paper [5]) in three main ways. First,
we provide theoretical analysis to validate the effectiveness of our
approach. Second, we conduct concrete evaluations of our techniques
in a real product environment for over the year of 2012. Third, we
further investigate various types of issues that out current approach
fails on, shedding light on future directions of improvement.
312312312
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:28:46 UTC from IEEE Xplore.  Restrictions apply. 
HYHQW
WLPH
(4((«D
(4((«E
(4((«D
(4((«F
(4((«[
(4((«[
(4((«[(
(4((«[+
(4((«G
(4((«[.
(4((«H
(4((«E
(4((«[1
(4((«[
(4((«\
(4((«]
(4((«[
(4((«[4
(4((«[(
(4((«]
WUDQVDFWLRQ,' PHVVDJH
7G717F«
7G717F«
G.E«
7G717F«
G.E«
G.E«
G.E«
G.E«
7G717F«
G.E«
7G717F«
G.E«
G.E«
G.E«
7G717F«
7G717F«
G.E«
G.E«
G.E«
G.E«
5HTXHVWHQWHULQJ«
FUHDWHGFRRNLHKDQGOHUZLWK«
5HTXHVWHQWHULQJ«
FRRNLHZLWKQDPHZDVUHDG«
64/VHUYHUIDLORYHUGHWHFWHG«
64/VHUYHUIDLORYHUGHWHFWHG«
LVQRWVLJQ«
%XLOGLQJDXWKHQWLFDWLRQXUO
6LWH «
DWWHPSWWRFUHDWHDVLJQ«
'HWHFWHGXVHRIIURP«
FUHDWHGFRRNLHKDQGOHUZLWK«
ZULWLQJFRRNLHRI
FRRNLHRIZDVQRWSUHVHQW
6TO([FHSWLRQ:VHUYHUZDVQRW«
OHDYLQJPRQLWRUHGVFRSHRI«
GRHVQRWUHTXLUHVVO
UHGLUHFWLQJWR«
LVQRWVLJQ«
OHDYLQJPRQLWRUHGVFRSHRI«
Figure 1. Log stream for example transaction instances
8/6/RJJLQJ
[(I+FH((\'DWDEDVH³^`´6WULQJ)RUPDW&XOWXUH,QIR,QYDULDQW&XOWXUH
³6TO([FHSWLRQ:µ^`¶ZDVQRWIRXQG6RXUFH:µ^`¶3URFHGXUH:µ^(`¶/LQH1XPEHU:µ^+`¶«´
Figure 2. Example logging statement in source code
Each log entry (as shown in Figure 1) typically
consists of four ﬁelds. The log time indicates when
the log entry occurs. The event ID is used to identify
the corresponding logging statement in the source code.
Figure 2 illustrates the corresponding portion of source
code for event ID (in short as event throughout the
rest of the paper) y1, which describes that a SQL
exception has been thrown. The transaction ID is used
to identify the corresponding transaction instance. The
text message describes the detailed runtime information.
In addition, a transaction instance in an online service
system has one important attribute: http-status, used by
us to determine the transaction instance’s fail/success
label (see Section III.A.2 for details). The http-status
indicates the returned status of a given transaction in-
stance, e.g., “200” denoting “OK” while “500” denoting
“Internal Server Error”.
The event sequence collected for a transaction in-
stance can reveal part of the code path executed when
serving the transaction instance, e.g., revealing which
functions are executed. Figure 3 shows sequences of
events and their statistics for three transaction types
within the occurring period of an issue (including the
“ﬁle editing” and “user login” types for the two example
transaction instances shown in Figure 1 and the “ﬁle
reading” transaction type).
WUDQVDFWLRQW\SHV
VHTXHQFHRIHYHQW
WUDQVDFWLRQV
IDLO
VXFFHVV
ILOHHGLWLQJ
XVHUORJLQ
ILOHUHDGLQJ
DEFGH\]
D[[[([+[.E[1
[[[4[(]
DEFGH]
4
+



.41
Figure 3.
occurring period of an issue
Log statistics for three transaction types within the
313313313
By manually inspecting the information in Figure 3
for issue diagnosis, one could notice that the dominating
symptom for the issue is event y1 (indicating that
a SQL exception is thrown), because there are 187
failing transaction instances uniquely associated with
this event, in contrast to only 36 failing transaction
instances uniquely associated with events x1 – x8 in-
dicating that an invalid cookie is encountered. Then
one could suggest a healing action for this issue as
rebooting a SQL machine (a typical healing action for a
SQL-exception symptom), in contrast to restarting the
Internet Information Services (IIS) (a typical healing
action for an invalid-cookie symptom). However, if one
would like to develop mining algorithms to automate
this issue-diagnosis process, there exist two phenomena
on transaction logs for posing challenges.
High-correlation phenomenon. We observe that
some events always appear together, being highly cor-
related. The reason for such observation is that the
developers want to track execution states with ﬁner
granularity at some critical statements, such as the
credential-veriﬁcation session. Such tracked states cap-
ture sufﬁcient logging information for diagnosis when
causes of issues are related to the execution of these
statements. For example, when event b appears, c, d,
and e always follow (see Figure 3). As another example,
events x1–x8 always appear together to indicate invalid
cookies. If we do not group them together when com-
paring event sequences for the given issue and historical
issues, events x1–x8 would contribute eight times than
event y1 to characterize the given new issue, likely
causing this given issue to be wrongly matched with a
historical issue with the dominating symptom as events
x1–x8. Such wrong matching would cause a wrong
healing action to be suggested.