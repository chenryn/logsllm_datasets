Value of two input ﬁelds
can’t the same
The comparison of values
in two input ﬁelds
The relationship of the two ﬁeld
need domain knowledge
Id
C1
C2
C3
C4
C5
C6
C7
C8
C9
C10
C11
C12
C13
C14
C15
C16
C17
Example
Your password must be at least 6 characters long
Your password should be shorter than 6 characters
Your User ID must be between 6 to 62 characters
The PIN must have 6 characters
The code needs to contain numbers
Don’t use a whitespace in your username
Expiration date must be at least 30 days from today
Child must be less than 18 years old
Specify your weight between 10 and 999
Nickname is too short
Your entry for the about ﬁeld is too long
Date is too small
The amount which you have
speciﬁed exceeds your limits
Email address format is invalid.
New password does not match
This username is already taken
Chosen minimum salary higher than
chosen maximum salary
C18 Mobile you typed isn’t valid for this country
#A
295
15
87
71
47
13
19
3
5
7
6
2
1
806
134
30
1
6
#U
6
3
4
8
6
4
4
2
2
2
2
1
1
5
3
2
1
1
two steps: (i) redundant sentence removal, and (ii) word
normalization. First, TextExerciser divides hints into
sentences and removes redundant ones, which are unrelated
to constraints, via our hint classiﬁer trained in §III-B. An
example is like “Oops! Password must be between 7 and 15
characters. Please try again.”—both “Oops!” and “Please try
again.” are removed in this stage. Second, TextExerciser
normalizes words, such as replacing spelled-out numbers with
corresponding digits and plural words into their corresponding
singular one. After pre-processing, TextExerciser calls
the Stanford parser to generate a syntax tree for each sentence
in the extracted hints.
M atch Follow(qp1=QP,NN) and Contain(qp1,cd1=CD) and First(np1=NP)
Generate LengthConstraint(Subject(np1), Range(cd1, inﬁnity))
Particularly, the traversal rule returns a Cardinal Number
Node (CD) cd1 as the threshold, and a Noun Phrase (NP) np1
as the text ﬁeld subject, such that np1 matches the ﬁrst (i.e.,
satisfying the F irst condition) NP node and cd1 is a child
node (i.e., satisfying the Contain condition) of a Quantiﬁer
Phrase (QP), which is a sibling (i.e., satisfying the F ollow
condition) of a Noun Node. For instance, if we apply this rule
to the syntax tree in Figure 5.(a), np1 equals to “password”
and cd1 equals to “6”; Figure 5.(b) maps to that np1 equals
to “username” and cd1 “3”.
In
this
3) Constraint
Representation:
step,
TextExerciser accepts a syntax tree and a hint category
and then generates constraints for
the hint. Speciﬁcally,
TextExerciser follows a rule selected based on the hint
category to traverse the syntax tree for constraint generation.
A traversal rule is hence deﬁned as a query to the syntax tree
with three predicates, where the Select predicate speciﬁes
the outputted nodes, M atch the condition of the selection,
and Generate a constraint equation that can be fed into the
solver:
Select Node1, Node2, ...
M atch Condition1 and Condition2 and ...
Generate Constraint
Let us start with a concrete example. Say, the traversal rule
belongs to the length constraint category, i.e., specifying that
the length of a text ﬁeld needs to be larger than a threshold.
The traverse needs to ﬁnd a number as the threshold and a
subject, e.g., password and date. Therefore, our traversal rule
will look like the following:
Select cd1, np1
restricts
Next, once a traversal, following a rule, returns corre-
sponding nodes, TextExerciser will follow the Generate
predicate to generate constraints. There are three types of
constraints:
a constraint that the length of A should fall within the
range from the value in CD to the inﬁnite value.
the
length of valid inputs to a certain range. For exam-
● Length Constraint. Such a constraint
ple, LengthConstraint(A, Range(CD, Inf ty)) represents
● Content Constraint. Such a constraint restricts the con-
ContentConstraint(A, F ormat(N N)) depicts that the in-
● Value Constraint. Such a constraint restricts the range of
LengthConstraint(password, Range(6, Inf ty));
Figure 5.(b) to LengthConstraint(username, Range(3, Inf ty)).
input values. It contains a scope that represents the range
of valid values.
Our example in Figure 5.(a) is converted to a constraint like
similarly,
put A should fulﬁll the format determined by N N.
tent of valid input
to a certain format. For example,
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:02:20 UTC from IEEE Xplore.  Restrictions apply. 
1075
Note that in practice, we often abbreviate a traversal, e.g., as
follows:
LowerBound∶∶ F ollow(QP, N N)&&Contain(QP, CD)&&F irst(N P)
→ LengthConstraint(Subject(N P), Range(CD, Inf ty))
&&ContentConstraint(Subject(N P), F ormat(N N))
Such an abbreviation skips the Select predicate as it is
embedded as part of the M atch predicate. The relationship
between M atch and Generate is also abbreviated as an
inference symbol. We may also skip corresponding variables
if the variable is unique. In practise, we write 57 traverse rules
based on the unique hints in Table I. We now list examples of
the rules adopted by TextExerciser in Table II.
D. Input Generation Engine
In this phase, TextExerciser generates inputs that sat-
isfy all the constraints converted from the extracted hints. The
ﬁrst step is to obtain concrete values for each variable in
the constraint representation. There are two major sources:
external and other ﬁelds. External sources involve emails and
text messages, in which TextExerciser will pre-register
several email account and phone number to receive such
values, such as PIN. Other ﬁelds involve other text inputs
generated by TextExerciser in the case of joint-ﬁeld
constraint—if the inputs to other text ﬁelds are generated,
TextExerciser will apply joint-ﬁeld constraint with that
concrete value; otherwise, TextExerciser will generate an
input without this constraint and apply the constraint for the
other involved input ﬁeld.
The second step is to solve the constraints: Particularly,
TextExerciser adopts Z3StrSolver [29], a popular solver,
to generate inputs that satisfy all
the constraints. An ex-
ample code for solving two constraints,
i.e., LengthCon-
straint(Range(lower bound, upper bound)) and ContentCon-
straint(A,Format(NN))),
is shown in Figure 6. Lines 5–6
are the length constraint, in which TextExerciser asks
the solver to generate an input with the length between
lower bound and upper bound. Lines 8–13 are the content
constraint, in which TextExerciser excludes certain char-
acters, such as the one appeared in the hint. TextExerciser
also adopts a special constraint to differentiate the gener-
ated input from the old ones at Lines 15–16, because the
old inputs have already been rejected by the app. Lastly,
TextExerciser asks the solver to generate an input at
Lines 18–19.
IV. IMPLEMENTATION
We implemented TextExerciser with about 6,350 lines
of Python code. Speciﬁcally, our constraint extractor has 1,100
lines of code, constraint parser 1,300 lines of code, and input
generation engine 1,900 lines of code. TextExerciser also
relies on some existing tools in the implementation. In phase
1 (hint extractor), we use the UiAutomator [30], to explore
the widgets on UI screen when dynamically running Android
apps. In phase 2 (hint parser), we use Stanford parser [28]
to generate syntax trees for hint
texts. In phase 3 (input
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
from z3 import *
solver=Solver()
input=String(’input’)
//LengthConstraint(lower_bound, upper_bound)
solver.add(Length(input) > lower_bound)
solver.add(Length(input) < upper_bound)
//ContentConstraint(A, content)
for char in TOTAL_LETTER:
if char not in content:
exclude_char = ’\\’+str(hex(char))[1:]
if len(exclude_char) == 3:
exclude_char = exclude_char.replace(’x’, ’
x0’)
solver.add(Not(Contains(input,exclude_char)))
// Different from old inputs
for old_input in INPUT_HISTORY:
solver.add(Not(input == StringVal(old_input)))
//Generate a text input
if solver.check()==sat:
print(solver.model())
Figure 6. An Example Z3StrSolver Code with Three Constraints: (i) Length
constraint within a certain range (Lines 5–6), (ii) Value constraint that excludes
certain characters (Lines 8–13), and (iii) Equivalence constraint (Lines 15–16).
generation engine), we use Z3StrSolver [29] to solve the input
constraints.
i.e.,
We now describe two implementation details,
the
dataset and model used in phase 2 and the validation code
extraction in phase 3. First, we collect 1,548 hints from 1,200
top free apps on Google Play and then ask three students to
label them during 14 days, which totals to around 50 hours per
student. Each hint has three labels—if a discrepancy happens,
these three students will discuss and resolve it. Note that the
number of discrepancies is relatively small as the labels are
mostly straightforward: There only exists 10 out of 1,548 hints.
For example, a hint, “Password is case-sensitive”, may be la-
belled as C5 or C14. In the end, all the students agreed to label
it as C14 as it is unclear what characters should be included.
After labeling, TextExerciser pre-processed all the hints
in the training set via two steps: (i) word normalization and (ii)
data balancing. That is, TextExerciser replaces all digits
with a special tag “TaggedASCD” with Stanford POSTTager,
e.g., “4 digits” changes to “TaggedASCD digit”, and then
leverages SMOTE [31] to balance the samples. In the end,
we trained our static hint identiﬁcation based on a multi-class
text classiﬁer [27] with half of all the labeled data and the
rest data is used for model validation and evaluation of tool
performance.
Second, our validation code handler, used in phase 3 for
solving the constraints, is composed of two parts, (i) an email
code extractor on a server and (ii) a code receiver written
as an Xposed module [32] on the mobile phone. Our email
code extractor keeps pulling emails from a pre-registered email
address designated for the validation code purpose and also
extracts code using a regular expression that matches all the
four or six digits in each email. Then, the extractor sends
the code and the email subject to the code receiver, which
performs a keyword matching of the email subject and the
app that requires a code. If a keyword matching fails, the
code receiver will also try all the recently-received, unmatched
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:02:20 UTC from IEEE Xplore.  Restrictions apply. 
1076
EXAMPLES OF HINT TEXTS AND THE CORRESPONDING INTERPRETATION RULES THAT CAN HANDLE THEM. WE SHOW EACH NODE IN THE SYNTAX TREE
WITH ITS TYPE (E.G., QP, NN, ETC.), AND USE SUBJECT(NODE) TO PRESENT AN INPUT BOX WHOSE IDENTITY IS DESCRIBED BY A NODE.
Table II
Minor-Category
Length Constraint
Hint Text Example (lowercase)
password is at least 3 character
Value Constraint
month must be between 1 and 12
Length Constraint
zipcode must be 5 digit
Existence Constraint
username can not contain space
Equivalence Constraint
new password does not match
Vague Length Constraint
nickname is too short
&& ContentConstraint(Subject(NP), Format(NN))
ScopeBound :: Follow(QP)&&Contain(QP,(cd1=CD,cd2=CD))&& First(NP)
FixLength :: Follow(NP)&&Contain(NP,(CD,NN))&& First(NP)
Matched Interpretation Rule
LowerBound :: Follow(QP,NN) && Contain(QP,CD) && First(NP)
→ LengthConstraint(Subject(NP), Range(CD,Infty))
→ ValueConstraint(Subject(NP), Range(cd1,cd2))
→ LengthConstraint(Subject(NP), Range(CD,CD))
→ ContentConstraint(Subject(NP), ! Format(NN))
→ ValueConstraint(Subject(NP), Range(Subject(NN), Subject(NN)))
→ LengthConstraint(Subject(NP), Subject(NN+1))
&& ContentConstraint(Subject(NP), Format(NN))
Exclusive :: Follow(RB,VP)&&Contain(VP,NN)) && First(NP)
MultipleEquivalence :: Follow(NP)&&Contain(NP,NN)
DirectRestric :: Follow(NP)&&Contain(NP,NN)
OVERVIEW OF STATE-OF-THE-ART OPEN-SOURCE DYNAMIC TESTING
TOOLS OF ANDROID APPS
Table III
Tool
Need of
Instrumentation
Text Input Strategy
Monkey [14]
Sapienz [25]
Stoat [21]
DroidBot [22]
A3E-Depth-First [26]
TextExerciser
No
System
No
No
App
No
Random
Random String from App Resource File
Random
Predeﬁned
Random String
Feedback Based Mutation
code for the app that needs a code. Note that the code receiver
also accepts and extracts code that is sent to the mobile phone
directly as a text message. All other steps for this text scenario
are the same as the email one.
V. EVALUATIONS
In
the
this
evaluate
performance
section, we
of
TextExerciser on real-world Android apps via addressing
three main research questions below:
● RQ1: is TextExerciser more effective than existing
● RQ2: can TextExerciser improve existing dynamic
● RQ3: is TextExerciser efﬁcient for generating text
tools in exercising Android apps?
analysis of Android apps?
input for popular Android apps?
A. Comparison with State-of-the-art Testing Tools
In this section, we answer RQ1 (is TextExerciser more
effective than existing tools in exercising Android apps?) by
comparing the method and activity coverage achieved by each
exerciser. Because TextExerciser is a specialized text
input exerciser while others are general purpose, we replace
the text input generator in each general-purpose exerciser with
TextExerciser and compare the modiﬁed version with
the original one for code coverage. According to Wang et
al. [33] and as shown in Table III, there exists ﬁve open-source
tools of exercising Android apps, which are Monkey [14],
Sapienz [25], Stoat [21], DroidBot [22], and A3E-Depth-
First [26]. Sapienz [25] requires system instrumentation and
A3E-Depth-First [26] app instrumentation, which are both
incompatible with our code coverage measurement. Therefore,
we compare TextExerciser with the rest three state-of-
the-art
tools that do not need any instrumentations. Note
that both Sapienz and A3E-Depth-First adopt random text
generation as shown in Table III—the results will be similar
to the tested three exercisers.
Our settings of three existing tools are as follows. During
our experiment, we conﬁgure Monkey with a ﬁxed-event seed
as documented by Continella et al. [34] so that Monkey will
always explore the same sequence of events, e.g., clicking on
the same position during different runs. Such a conﬁguration
will mitigate randomness that is introduced in comparing Mon-
key with TextExerciser and random text input generation.
We conﬁgure DroidBot via manually writing text inputs based