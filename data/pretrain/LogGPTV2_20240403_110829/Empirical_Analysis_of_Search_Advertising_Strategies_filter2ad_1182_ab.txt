NAB = 1 for an optimally beneﬁcial trafﬁc slice — where every
case results in a conversion (π = 1) and there is no cost (ν = 0).
NAB = 0 for trafﬁc slices that have no net beneﬁt, e.g., where the
trafﬁc slice is so expensive that the advertiser is willing to forgo
every conversion and save the entire cost (i.e., effective cost-per-
acquisition is λ and ν = λπ). For detrimental trafﬁc slices, e.g.,
there are no conversions (π = 0) and the advertiser is losing money
(ν > 0), NAB is negative. In practice, NAB is on the order of 10−2
in our real-world dataset. (Intuitively, this makes sense, as CTRs
are typically of the same order.)
3.3 Incremental NAB
The incremental net acquisition beneﬁt (INAB) measures the rel-
ative improvement in NAB of one trafﬁc slice over another, i.e., the
effectiveness of one ad campaign vs. another. Intuitively, it is the
change in conversion probability (∆π) adjusted by the change in
cost (∆ν). We deﬁne INAB for trafﬁc slice x over slice y as fol-
lows:
INAB(x | y) =
NAB(x) − NAB(y)
|NAB(y)|
INAB is deﬁned only for two comparable trafﬁc slices. That is,
INAB can be computed for two trafﬁc slices x and y belonging
to the same advertiser but not across advertisers. Trafﬁc slice x is
more beneﬁcial than y if and only if x has more net acquisition ben-
eﬁt than y, i.e., INAB(x | y) is positive. Slice x is less beneﬁcial
than y if INAB(x | y) is negative (or equivalently, INAB(y | x) is
positive). Both are equivalent if INAB(x | y) is zero.
3.4 Discussion
NAB approximates proﬁt per impression (PPI) when λ is equal
to proﬁt margin on conversion. But, unlike proﬁt per impression,
NAB does not require information about revenue derived and cost
of the products. This allows ad networks and ad agencies to use
the NAB metric to compare effectiveness of advertising campaigns.
Note that both NAB and PPI, being impression based, can be sensi-
tive to impression counts (impressions are cheap, one may argue).
However, an impression represents the most basic intervention to
Figure 2: Large spread in the normalized advertiser ad spend and
the normalized price they pay per conversion.
Figure 3: Top queries account for a small share of overall spend
illustrating the query diversity.
almost six orders of magnitude. Thus, our metric must take into
account an advertisers’ target cost per acquisition and allow com-
parison between different advertising strategies of an advertiser.
3.1.2 Statistical signiﬁcance
As mentioned, ad targeting can be extremely ﬁne-grained focus-
ing on speciﬁc keywords, device types, geographic regions, etc.
Figure 3 plots the probability density function of the share of total
advertising money spent on a sampling of the most popular key-
words (ordered by the amount of money spent on them). To il-
lustrate, note that the top keyword contributes to less than 0.2%
of the total spend. There is a heavy tail of keywords with the top
200 most-popular together accounting for less than 7% of the total
spend in the sample. Performing analysis at keyword granularity
results in poor statistical signiﬁcance. The statistical signiﬁcance is
lower still if the data is further sliced by user, device type, adver-
tiser and other targeting parameters.
Hence, any useful metric must be deﬁned over trafﬁc aggregates,
which we refer to as slices of search requests. The dimensions
along which the data should be aggregated depends upon the strat-
egy being evaluated. In general, we will consider slices that capture
a particular advertising campaign. For instance, aggregating data
0200040006000800010000QueryRank10−410−310−210−1100SpendShare(%)81user experience on behalf of advertisers, and in absence of a better
denominator, impression-based metrics are still considered indus-
try standard [2, 38].
CTR and CPA. Click-through ratio (CTR) and cost per acqui-
sition (CPA) are the two most commonly used metrics to evaluate
effectiveness of ad campaigns. CTR captures the effectiveness of
a campaign in attracting users but does not capture the beneﬁt of
attracting those users. It also does not capture the price that the
advertiser has to pay for the clicks. While CPA captures the qual-
ity of users being attracted, it does not capture the efﬁciency of the
campaign in attracting users overall, something that CTR captures.
We ﬁnd that NAB captures the performance of a campaign bet-
ter than both CTR and CPA. In particular, NAB is robust to the
diversity of advertisers and sparsity of query-level data found in
our dataset. It has been observed [12, 24] that even large search
engines have very sparse data when aggregated by user queries.
NAB allows comparisons between different advertising strategies
of a given advertiser and can be applied to any slice of trafﬁc with
sufﬁcient data.
Target cost per acquisition. One subtlety with NAB is that ad-
vertisers could have different target costs per acquisition for differ-
ent ad campaigns. We discuss our methodology for inferring target
cost per acquisition in next section.
4. DATA AND METHODOLOGY
This section describes the dataset we use and the methodol-
ogy we follow for measuring effectiveness of various ad campaign
strategies in the subsequent sections.
4.1 Dataset
Our dataset sample is several terabytes in size, consisting of bil-
lions of search and ad clicks on a large search ad network in the
United States English language market. We report upon clicks cap-
turing all actions taken by hundreds of millions of users who issue
hundreds of millions of unique English-language queries over a pe-
riod of four contiguous weeks within the last two years. (Analysis
of a different four-week time period obtains qualitatively similar re-
sults.) The dataset contains a representative sample of clicks from
desktops, tablets and phones. Our dataset sample covers many mil-
lions of dollars in advertising spend1 by hundreds of thousands of
advertisers. Our dataset does not cover specialized search verticals
like image, video and map, or product listings.
For each click our dataset includes the following anonymized in-
i) the normalized search query issued by the user and
formation:
the search ad network’s internal query classiﬁcation; ii) information
about the browser including version and operating system, device
form factor iii) the list of organic search results and paid search
ads that were presented to the user; iv) the details of the associated
ad campaign including bid amounts, keywords targeted, and ad ex-
tensions; v) the organic search results or paid search ads on which
the user actually clicked (if any) including clicks on multiple re-
sults and ads; and, lastly, for ads clicked, vi) the second-price bid
charged to the advertiser along with any advertiser-reported con-
version event(s) for that click along with the URL of the pages for
which the user conversion was reported and the (opaque) conver-
sion types.
Along with click data, the analytics system collects user activity
data on advertiser websites to track the performance of their ad
campaigns. Whenever a user performs an action that the advertiser
1We are obliged to report only the magnitude or normalized values
for some sensitive quantities when doing so does not compromise
the scientiﬁc value of our results.
wants to track, JavaScript embedded in the page sends information
about the action—along with a user cookie allowing the data to be
connected to the user’s search behavior—to the ad network. This
system allows advertisers to declare which user actions constitute
conversions.
4.2 Methodology
In order to compute NAB we infer target cost per conversion (λ)
and conversion rate of trafﬁc acquired organically from the data.
We aggregate data over queries that identify trafﬁc representing dif-
ferent advertising strategies.
Conversions. In order to identify the conversions that an adver-
tiser obtains from a slice of trafﬁc we have to attribute the conver-
sion to a speciﬁc prior search. For this, we identify the user actions
on the search engine prior to the conversion event on the advertiser
website. We then attribute the conversion to the latest user click (re-
gardless of whether the click was on an organic result or a search
ad) that led the user to the advertiser’s website—as long as the click
happened in the 24 hours prior to the conversion event.
Not all advertisers report conversion signals, or not in signiﬁ-
cant numbers. Unless otherwise mentioned, we omit advertisers
for whom we have less than 30 conversion reports in our dataset
sample.
Inferring target cost per acquisition. Recall from Section 2
that advertisers place a bid which is the maximum amount they
are willing to pay for a click. We infer the maximum amount the
advertiser is willing to pay for a conversion (λ) by dividing their
total bid amount for the ads clicked by the number of conversions
they received. Since the bid values are always more than the actual
cost of advertising, overall NAB for any advertiser is always posi-
tive. Note that by making this choice we consider all conversions
that the advertiser receives in US English market—irrespective of
campaign—equivalent.
Aggregating queries. NAB must be computed over a signiﬁ-
cant aggregation of trafﬁc. As mentioned earlier, individual search
queries are too granular. We follow the search-ad network’s internal
classiﬁcation scheme [41] to aggregate queries into the following
four classes of particular interest: navigational, local, commercial,
and other, which includes informational queries.
Since the ad network’s internal query classiﬁer uses heuristics,
we verify the correctness of classiﬁcation by manually investigat-
ing a representative sample. In all we manually verify 200 queries
and ﬁnd that in the large majority of cases (94%) our manual la-
bel matches the classiﬁer’s; in the remaining 6% of the cases we
believe the classiﬁer misclassiﬁed the query. We compute the
sensitivity and speciﬁcity measures for classiﬁcation of naviga-
tional queries and ﬁnd that that 77% of the time, a navigational
query is classiﬁed as navigational, whereas 4% of the time, a non-
navigational query is classiﬁed as navigational. Overall, the query
classiﬁcation—while not perfect—seems sufﬁciently accurate for
the purposes of our study.
Trafﬁc slices. In subsequent sections we compare the effective-
ness of various campaign strategies used by advertisers by compar-
ing the beneﬁts across different trafﬁc slices. Each slice of trafﬁc is
deﬁned by the query classiﬁcation, device type, the position of the
advertiser in the organic results (if at all), whether the advertiser’s
ad is shown or not, and whether the call button was present for the
ad or organic result (in the case of mobile devices). Table 1 lists
these features and describes the values they take. Table 2 labels
the various combinations of these features with a name that we use
to refer to that trafﬁc slice in subsequent sections. We discuss our
choice of the trafﬁc slices in Section 8.
82Query
nav
com
all
Device
Organic
phone Mobile smartphones
Desktops and laptops
pc
all
All devices
Navigational query; user seeks speciﬁc site
Commercial query; user has purchase intent
All queries
yes
no
top
poor
n
n+
all
yes
no
Ad
Ext. (set)
Present in ﬁrst page of results
Not in ﬁrst page of results
Top-most organic search result
Ranked 2 or worse, or not on ﬁrst page
Ranked n
Ranked n or worse
All cases whether present or not
Ad present
Ad not present
ad:call Ad has call button
org:call Organic result had call button
ad:comp Competitor has an ad
Table 1: Trafﬁc features used to deﬁne trafﬁc slices
Query Device Org. Ad
all
all
nav
nav
nav
nav
all
Slice
Section 5: Cannibalizing Organic
org-n-noad
org-n-ad
nav-noad
nav-ad
nav-comp-noad
nav-comp-ad
noorg-ad
Section 6: Poaching
poach-ad
poach-noad
commerce-ad
Section 7: Ad Extensions
phone-orgcall
phone-org
phone-ad
phone-adcall
phone-noorg-ad
phone-noorg-adcall
phone-orgcall-adcall
all
all