ory is extremely limited (4KB in Mica motes and 10KB in
Telos motes). Program memory is limited too (128KB in Mica
motes and 48KB in Telos motes).
Figure 6 shows program memory and data memory re-
quirements for CADeT as a percentage of corresponding
requirements for Liblog. CADeT uses slightly more program
memory (0.3% to 1%) due to it’s complex implementation
compared to Liblog. This increase is negligible. CADeT uses
3% to 13% more data memory than Liblog. The reason for
the increase in data memory is that unlike Liblog, CADeT
stores tables such as AAMap and PCMap in the memory. These
additional requirements translate up to 300 bytes, which is
moderate.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:58:43 UTC from IEEE Xplore.  Restrictions apply. 
0%5%10%15%20%25%MessagesEvent control flow+ messagesEnergy Overhead %CADeTLiblogCADeT compressedLiblog compressed0%5%10%15%20%25%MessagesEvent control flow+ messagesEnergy Overhead %CADeTLiblogCADeT compressedLiblog compressed10%15%20%25%30%MessagesEvent control flow+ messagesEnergy Overhead %CADeTLiblogCADeT compressedLiblog compressed0%500%1000%1500%2000%2500%UncompressedCompressedUncompressedCompressedUncompressedCompressedOscilCountSurgeLibLog Energy Overhead %MessagesEvent control flow + messages(a) Program memory
(b) Data memory
Fig. 6. Memory usage of CADeT as a percentage of memory usage of Liblog
VII. RELATED WORK
We discuss runtime debugging techniques proposed for tra-
ditional distributed systems as well as for WSNs. The runtime
debugging techniques can be divided into ofﬂine techniques
and online techniques based on whether the debugging is done
postmortem or not.
Online monitoring [10], [51] and predicate detection [11]
techniques use external or internal [51] monitoring agents
observing the execution of the distributed system, e.g., by
snooping messages. Similar solutions for WSNs have been
proposed [52], [53]. While these techniques can give insight
into the network in small-scale test deployments, they are
not cost-effective for large WSN deployments as they require
extra hardware – in some cases more powerful
than the
motes themselves. Furthermore, coordinating the monitors in
a network as well as maintaining their correctness is non-
trivial in large deployments. Remote debugging tools such as
Marionette [19], Clairvoyant [20] and Hermes [21] allow the
developer to examine the state of individual nodes and modify
the behaviors of the nodes. HSEND [54] is an invariant-based
application-level runtime error detection tool for WSNs. The
invariants are checked close to the source of the error avoiding
periodic collection of data at the base station. An alert is sent
to the base station only when there is a violation. Hermes [21]
is similar to HSEND [54] but allows developers to modify
invariants at runtime as well as deploy patches to ﬁx violations.
These approaches are complementary to our diagnostic tracing.
Their limitation is that only the faults violating invariants can
be diagnosed, and knowledge of failures is required for writing
invariants.
Many ofﬂine, trace-based debugging approaches have been
proposed for traditional (wired) distributed systems [13], [14],
[15], [55], [30], [16], [18], which use model-based approaches,
statistical approaches, or execution replay for diagnosis. Such
techniques are inapplicable in the WSN domain due to the
extreme resource constraints. In many of the above techniques
[16], [18], [13], [30], [14], messages are traced by recording
the contents of the messages along with the timestamps
generated by Lamport clocks [32]. Such traces can recreate
the causal ordering of messages. Netzer et al. [35] observed
that only racing messages need to be recorded as others
can be regenerated. They presented a mostly optimal tracing
technique that uses vector clocks [34] to identify racing
messages online and record them. As mentioned before, vector
clocks are too heavy-weight for WSNs, and logical clocks
assume the presence of a reliable messaging layer such as
TCP. Furthermore, logical clocks have high variability, which
reduces opportunities for compression.
Sympathy [23] periodically collects WSN information from
the basestation. The collected information is
all nodes at
analyzed to detect node and link failures or partitions and
localize their causes. PAD [24] is similar to Sympathy but uses
Bayesian analysis to reduce network monitoring trafﬁc.Both
Sympathy and PAD require collecting data often, even during
correct operation. Moreover, the diagnosis is coarse-grained
and is speciﬁc to node/link failures but cannot help much
with complex faults like data races. NodeMD [26] records
system calls and context switches encoded in few bits to
detect stack overﬂows,
livelocks, and deadlocks. LIS [27]
proposes a log instrumentation speciﬁcation language and
runtime for systematically describing and collecting execution
information. LIS is optimized to collect function calls as
well as control-ﬂow traces efﬁciently. TinyTracer [9] proposes
an efﬁcient way to record all concurrent events and the
interprocedural control-ﬂow paths taken during their execution
succinctly. TinyLTS [22] proposes an efﬁcient printf logging
mechanism that allows the developer to log any runtime
information. None of these four approaches handles node
interactions. While the message sends and receives can be
recorded locally, the ordering of messages cannot be recorded.
Declarative TracePoints [25] provide a uniform SQL-based
query language interface for debugging and can simulate
other trace-based approaches. Macrodebugging [8] records
traces of all variable values in a macro program of every
node in the network. Macrodebugging works at macro level
(network level) and cannot be used to diagnose faults at nodes.
Unlike [7], CADeT does not require multiple reproductions
of faults as well as instrumentation of speciﬁc events. We
observe, however, that our approach is complementary to the
machine learning techniques proposed in [7] as CADeT traces
can as well be used in [7].
VIII. CONCLUSIONS
In this paper, we proposed a message tracing scheme to
record the distributed control-ﬂow that is effective in diagnosis
of complex distributed faults in WSNs while satisfying the
resource constraints of WSNs. We have shown its effectiveness
and argued for its efﬁciency and consistency advantages over
the state of the art.
ACKNOWLEDGEMENT
This research is supported, in part, by the National Science
Foundation (NSF) under grant 0834529. Any opinions, ﬁnd-
ings, conclusions, or recommendations in this paper are those
of the authors and do not necessarily reﬂect the views of NSF.
REFERENCES
[1] G. Werner-Allen, K. Lorincz, J. Johnson, J. Lees, and M. Welsh,
“Fidelity and Yield in a Volcano Monitoring Sensor Network,” in
USENIX OSDI, 2006.
[2] G. Barrenetxea, F. Ingelrest, G. Schaefer, and M. Vetterli, “The Hitch-
hiker’s Guide to Successful Wireless Sensor Network Deployments,” in
ACM SenSys, 2008.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:58:43 UTC from IEEE Xplore.  Restrictions apply. 
100.3%100.4%100.5%100.6%100.7%100.8%100.9%101.0%countoscilsurgeCADeT Program Memory %UncompressedCompressed95.0%100.0%105.0%110.0%115.0%countoscilsurgeCADeT Data Memory %UncompressedCompressed[3] S. Kim, S. Pakzad, D. Culler, J. Demmel, G. Fenves, S. Glaser, and
M. Turon, “Health Monitoring of Civil Infrastructures using Wireless
Sensor Networks,” in ACM/IEEE IPSN, 2007.
[4] L. Krishnamurthy, R. Adler, P. Buonadonna, J. Chhabra, M. Flanigan,
N. Kushalnagar, L. Nachman, and M. Yarvis, “Design and Deployment
of Industrial Sensor Networks: Experiences from a Semiconductor Plant
and the North Sea,” in ACM SenSys, 2005.
[5] J. Beutel, K. R¨omer, M. Ringwald, and M. Woehrle, “Deployment Tech-
niques for Wireless Sensor Networks,” in Springer Sensor Networks:
Where Theory Meets Practice, G. Ferrari, Ed., 2009.
[6] R. Szewczyk, A. Mainwaring, J. Polastre, J. Anderson, and D. Culler,
“An Analysis of a Large Scale Habitat Monitoring Application,” in ACM
SenSys, 2004.
[7] M. Khan, T. Abdelzaher, and K. Gupta, “Towards Diagnostic Simulation
in Sensor Networks,” in IEEE DCOSS, 2008.
[8] T. Sookoor, T. Hnat, P. Hooimeijer, W. Weimer, and K. Whitehouse,
“Macrodebugging: Global Views of Distributed Program Execution,” in
ACM SenSys, 2009.
[9] V. Sundaram, P. Eugster, and X. Zhang, “Efﬁcient Diagnostic Tracing
for Wireless Sensor Networks,” in ACM SenSys, 2010.
[10] K. Sen, A. Vardhan, G. Agha, and G. Rosu, “Efﬁcient Decentralized
Monitoring of Safety in Distributed Systems,” in IEEE ICSE, 2004.
[11] X. Liu, Z. Guo, X. Wang, F. Chen, X. Lian, J. Tang, M. Wu, M. F.
Kaashoek, and Z. Zhang, “D3S: Debugging Deployed Distributed Sys-
tems,” in USENIX NSDI, 2009.
[12] H. Garcia-Molina, F. Germano, and W. H. Kohler, “Debugging a
Distributed Computing System,” IEEE Transactions on Software En-
gineering, vol. SE-10, no. 2, 1984.
[13] A. V. Mirgorodskiy and B. P. Miller, “Diagnosing Distributed Systems
with Self-propelled Instrumentation,” in Springer Middleware, 2008.
[14] N. Maruyama and S. Matsuoka, “Model-Based Fault Localization in
Large-Scale Computing Systems,” in IEEE IPDPS, 2008.
[15] P. C. Bates, “Debugging Heterogeneous Distributed Systems Using
Event-Based Models of Behavior,” ACM Trans. Comput. Syst., vol. 13,
no. 1, 1995.
[16] R. Curtis and L. D. Wittie, “BUGNET: A Debugging System for Parallel
Programming Environments,” in IEEE ICDCS, 1982.
[17] T. J. LeBlanc, J. Crummey, and M. M., “Debugging Parallel Programs
with Instant Replay,” IEEE Transactions on Computer, vol. 36, no. 4,
1987.
[18] D. Geels, G. Altekar, P. Maniatis, and T. Roscoe, “Friday: Global
Comprehension for Distributed Replay,” in USENIX NSDI, 2007.
[19] K. Whitehouse, G. Tolle, J. Taneja, C. Sharp, S. Kim, J. Jeong, J. Hui,
P. Dutta, and D. Culler, “Marionette: Using RPC for Interactive Devel-
opment and Debugging of Wireless Embedded Networks,” in ACM/IEEE
IPSN, 2006.
[20] J. Yang, M. L. Soffa, L. Selavo, and K. Whitehouse, “Clairvoyant: A
Comprehensive Source-Level Debugger for Wireless Sensor Networks,”
in ACM SenSys, 2007.
[21] N. Kothari, K. Nagaraja, V. Raghunathan, F. Sultan, and S. Chakradhar,
“Hermes: A Software Architecture for Visibility and Control in Wireless
Sensor Network Deployments,” in ACM/IEEE IPSN, 2008.
[22] R. Sauter, O. Saukh, O. Frietsch, and P. J. Marr´on, “TinyLTS: Efﬁcient
Network-Wide Logging and Tracing System for TinyOS,” in IEEE
INFOCOM, 2011.
[23] N. Ramanathan, K. Chang, R. Kapur, L. Girod, E. Kohler, and D. Estrin,
“Sympathy for the Sensor Network Debugger,” in ACM SenSys, 2005.
[24] K. Liu, M. Li, Y. Liu, M. Li, Z. Guo, and F. Hong, “PAD: Passive
Diagnosis for Wireless Sensor Networks,” in ACM SenSys, 2008.
[25] Q. Cao, T. Abdelzaher, J. Stankovic, K. Whitehouse, and L. Luo,
“Declarative Tracepoints: A Programmable and Application Independent
Debugging System for Wireless Sensor Networks.” in ACM SenSys,
2008.
[26] V. Krunic, E. Trumpler, and R. Han, “NodeMD: Diagnosing Node-Level
Faults in Remote Wireless Sensor Systems,” in ACM MobiSys, 2007.
[27] R. Shea, M. Srivastava, and Y. Cho, “Scoped Identiﬁers for Efﬁcient Bit
Aligned Logging,” in IEEE DATE, 2010.
[28] M. M. H. Khan, H. Le, H. Ahmadi, T. Abdelzaher, and J. Han,
“Dustminer: Troubleshooting Interactive Complexity Bugs in Sensor
Networks,” in ACM SenSys, 2008.
[29] D. Geels, G. Altekar, S. Shenker, and I. Stoica, “Replay Debugging for
Distributed Applications,” in USENIX ATC, 2006.
[30] Z. Chen, Q. Gao, W. Zhang, and F. Qin, “FlowChecker: Detecting Bugs
in MPI Libraries via Message Flow Checking,” in ACM SC, 2010.
[31] R. Shea, “Defect Exposure in Wireless Embedded Systems,” Ph.D.
dissertation, UCLA, 2010.
[32] L. Lamport, “Time, Clocks, and the Ordering of Events in a Distributed
System,” Commun. ACM, vol. 21, no. 7, 1978.
[33] M. Ronsse, K. De Bosschere, M. Christiaens, J. C. de Kergommeaux,
and D. Kranzlm¨uller, “Record/Replay for Nondeterministic Program
Executions,” Commun. ACM, vol. 46, no. 9, 2003.
[34] F. Mattern, “Virtual Time and Global States of Distributed Systems,” in
IEEE Parallel and Distributed Algorithms, 1989.
[35] R. H. B. Netzer and B. P. Miller, “Optimal Tracing and Replay for
Debugging Message-Passing Parallel Programs,” in ACM/IEEE SC,
1992.
[36] F. J. Torres-Rojas and M. Ahamad, “Plausible Clocks: Constant Size
Logical Clocks for Distributed Systems,” Springer Distributed Comput-
ing, vol. 12, no. 4, 1999.
[37] R. Prakash and M. Singhal, “Dependency Sequences and Hierarchical
Clocks: Efﬁcient Alternatives to Vector Clocks for Mobile Computing
Systems,” Kluwer Wirel. Netw., vol. 3, no. 5, 1997.
[38] C. Fidge, “Fundamentals of Distributed System Observation,” IEEE
Software, vol. 13, no. 6, 1996.
[39] T. Ball and J. R. Larus, “Efﬁcient Path Proﬁling,” in IEEE Micro, 1996.
[40] I. F. Akyildiz, W. Su, Y. Sankarasubramaniam, and E. Cayirci, “A Survey
on Sensor Networks,” IEEE Communications Magazine, vol. 40, no. 8,
2002.
[41] J. Yick, B. Mukherjee, and D. Ghosal, “Wireless Sensor Network
Survey,” Elsevier Computer Networks, vol. 52, no. 12, 2008.
[42] W. B. Heinzelman, A. P. Chandrakasan, and H. Balakrishnan, “An
Application-Speciﬁc Protocol Architecture for Wireless Microsensor
Networks,” IEEE Transactions on Wireless Communications, vol. 1,
no. 4, 2002.
[43] T. He, S. Krishnamurthy, L. Luo, T. Yan, L. Gu, R. Stoleru, G. Zhou,
Q. Cao, P. Vicaire, J. A. Stankovic, T. F. Abdelzaher, J. Hui, and
B. Krogh, “VigilNet: An Integrated Sensor Network System for Energy-
efﬁcient Surveillance,” ACM Trans. Sen. Netw., vol. 2, no. 1, 2006.
[44] A. Arora, P. Dutta, S. Bapat, V. Kulathumani, H. Zhang, V. Naik,
V. Mittal, H. Cao, M. Demirbas, M. Gouda, Y.-R. Choi, T. Herman, S. S.
Kulkarni, U. Arumugam, M. Nesterenko, A. Vora, and M. Miyashita,
“A Line in the Sand: A Wireless Sensor Network for Target Detection,
Classiﬁcation, and Tracking,” Elsevier Computer Networks, vol. 46,
no. 5, 2004.
[45] P. Dutta, J. Hui, J. Jeong, S. Kim, C. Sharp, J. Taneja, G. Tolle,
K. Whitehouse, and D. Culler, “Trio: Enabling Sustainable and Scalable
Outdoor Wireless Sensor Network Deployments,” in ACM/IEEE IPSN,
2006.
[46] C. Intanagonwiwat, R. Govindan, D. Estrin, J. Heidemann, and F. Silva,
“Directed Diffusion for Wireless Sensor Networking,” IEEE/ACM Trans.
Netw., vol. 11, no. 1, 2003.
[47] V. Sundaram, S. Bagchi, Y.-H. Lu, and Z. Li, “SeNDORComm: An
Energy-Efﬁcient Priority-Driven Communication Layer for Reliable
Wireless Sensor Networks,” in IEEE SRDS, 2008.
[48] V. Sundaram, “TinyTracer
Implementation. http://sss.cs.purdue.edu/
projects/TinyTracer/index.html,” 2011.
[49] Y. Sazeides and J. E. Smith, “The predictability of data values,” in IEEE
Micro, 1997.
[50] V. Sundaram, P. Eugster, and X. Zhang, “Prius: Generic Hybrid Trace
Compression for Wireless Sensor Networks,” in ACM SenSys, 2012.
[51] G. Khanna, P. Varadharajan, and S. Bagchi, “Self Checking Network
Protocols: A Monitor Based Approach,” in IEEE SRDS, 2004.
[52] B. Chen, G. Peterson, G. Mainland, and M. Welsh, “LiveNet: Using
Passive Monitoring to Reconstruct Sensor Network Dynamics,” in IEEE
DCOSS, 2008.
[53] M. M. H. Khan, L. Luo, C. Huang, and T. F. Abdelzaher, “SNTS: Sensor
Network Troubleshooting Suite,” in IEEE DCOSS, 2007.
[54] D. Herbert, V. Sundaram, Y. H. Lu, S. Bagchi, and Z. Li, “Adaptive Cor-
rectness Monitoring for Wireless Sensor Networks Using Hierarchical
Distributed Run-Time Invariant Checking,” ACM Trans. Auton. Adapt.
Syst., vol. 2, no. 3, 2007.
[55] R. Fonseca, G. Porter, R. H. Katz, S. Shenker, and I. Stoica, “X-trace:
A Pervasive Network Tracing Framework,” in USENIX NSDI, 2007.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:58:43 UTC from IEEE Xplore.  Restrictions apply.