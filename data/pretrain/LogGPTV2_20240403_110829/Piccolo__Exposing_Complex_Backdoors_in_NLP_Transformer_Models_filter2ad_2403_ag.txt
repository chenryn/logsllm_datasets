0.87
0.9
words. Besides, our ablation study in Appendix B shows that
without word level inversion, PICCOLO can still have around
0.82 detection accuracy using token level inversion, surpassing
the state-of-the-art techniques by 0.12. This is the lower bound
accuracy of PICCOLO as token level inversion does not need
a word dictionary.
Targeting multiple triggers.
the attacker
inserts multiple triggers and adds a loss to ensure that these
triggers target different CLS dimensions. For each trigger, we
randomly sample 10 dimensions as its target. During training,
if a training sample contains a trigger, we add a loss to increase
the values of its target dimensions besides the cross entropy
loss. Let cls(xt) denote the CLS embedding of sample xt. If
xt contains a trigger, dimt denotes the targeted dimensions of
the trigger; if xt does not contain a trigger, dimt is empty.
We hence use the following loss.
L = Lce (xt, yt) − λ
(10)
Here Lce (xt, yt) is the cross-entropy loss for the input-label
pair xt and yt. Note that a portion of the training data is poi-
cls(xt)[dimt] denotes
soned with the backdoor. The term
the adaptive loss leveraged by the attacker to increase the target
CLS dimensions of a trigger. Parameter λ balances the training
loss and the adaptive loss. Using a large adaptive loss may
produce a trojaned model with a low normal accuracy or a
low attack success rate, making the overall attack ineffective.
We evaluate this adaptive attack with 2, 4 and 8 injected
triggers. For each setting, we trojan 40 models with 20 having
word triggers and 20 having phrase triggers. Table VII shows
the results. The ﬁrst row shows the different settings of 2
, 4 and 8 triggers. The second row shows the different λ
values. The third row shows the average normal accuracy of
the trojaned models. The fourth row shows the ASR. Since
each model has multiple triggers, we show the highest ASR
among all the triggers for that model. The last row shows the
detection rate of PICCOLO. Observe that the normal accuracy
of poisoned models decreases with the increase of loss weight
λ. We stop enlarging λ when the normal accuracy drops
below 0.75 which makes the attack ineffective. For all the
settings, PICCOLO has the detection accuracy ≥ 0.88. Further
inspection shows that although there are many peaks in the
importance vectors, the dot product is large if any of these
peaks aligns with the trigger word’s linear model weights.
cls(xt)[dimt]
(cid:8)
VII. RELATED WORK
Backdoor Attacks. Backdoor attacks are initially studied in
the computer vision domain [1]–[3], [57]–[64]. Then NLP
models became the target of backdoor attacks. Besides those
discussed in Section II-B,
there is dynamic sentence at-
tack [45] that trains a language model as the trigger. The
trigger language model generates different trigger phrases for
TABLE VII: Adaptive attack: multiple triggers with different
target CLS dimensions
2 triggers
1 0.1 0.01
0.74 0.9 0.91
1
1
1 0.9 0.88
0.1
0.73 0.82 0.88
1
1
0.1 0.01
0.9
1
1
0.58 0.89
1
1
4 triggers
1
Loss weight
8 triggers
Detection
Acc
ASR
10
1
1
1
1
1
1
1
1
different sentences. There are also attacks focusing on tasks
other than classiﬁcation. Zhang et al. [7] proposed to inject
backdoor in text generation tasks such as question answering
and text completion. Besides computer vision and NLP tasks,
backdoor attacks have also been proposed on graph neural
networks [65], [66], transfer learning [63], [67], [68], federated
learning [69]–[73], and reinforcement learning
[74], [75].
PICCOLO focuses on backdoors in the NLP domain.
Backdoor Defense. We have discussed backdoor detection in
the NLP domain in Section II-C and we also include com-
parison between PICCOLO and other NLP backdoor detection
methods in Appendix IX-D. There are a number of backdoor
detection techniques in the computer vision domain [10]–[20],
[20]–[35]. There are also techniques that aim to repair back-
doors or certify robustness against backdoors. Fine-prune [25]
repairs trojaned neural networks by removing the neurons
not activated on benign samples. NAD [76] repairs neural
networks by distillation training on a small clean set. Wang
et al. [77] propose to use randomized smoothing to certify
robustness against backdoors. Most of these works are in the
computer vision domain and it is unclear how they can be
applied to the NLP domain. There are also a body of data
sanitization techniques that remove poisoning samples from
the training set [78]–[81], whereas PICCOLO defends backdoor
attacks at a different stage (after a model is trained).
Adversarial Example Generation in the NLP domain.
Adversarial example generation techniques can be adapted
to backdoor trigger inversion [36], [37], [82]–[88]. We adapt
a state-of-the-art NLP model adversarial example generation
technique GBDA [36] as one of the baselines in Section VI
and show PICCOLO substantially outperforms it.
VIII. CONCLUSION
We propose an NLP model backdoor scanning technique
PICCOLO. It is based on a novel word-level encoding and a
word discriminativity analysis. Our experiments and TrojAI
leaderboard performance show that PICCOLO achieves the
state-of-the-art results for complex models and a wide range
of backdoor attacks. While the arm race between attack and
defense will never end, PICCOLO can help hardening NLP
models against existing attacks.
ACKNOWLEDGEMENT
We thank the anonymous reviewers for their constructive
comments. This research was supported, in part by IARPA
TrojAI W911NF-19-S-0012, NSF 1901242 and 1910300,
ONR N000141712045, N000141410468 and N000141712947.
Any opinions, ﬁndings, and conclusions in this paper are those
of the authors only and do not necessarily reﬂect the views of
our sponsors.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:09 UTC from IEEE Xplore.  Restrictions apply. 
132037
REFERENCES
[1] T. Gu, K. Liu, B. Dolan-Gavitt, and S. Garg, “Badnets: Evaluating
backdooring attacks on deep neural networks,” IEEE Access, 2019.
[2] Y. Liu, S. Ma, Y. Aafer, W.-C. Lee, J. Zhai, W. Wang, and X. Zhang,
“Trojaning attack on neural networks,” in 25nd Annual Network and
Distributed System Security Symposium, NDSS 2018, San Diego, Cal-
ifornia, USA, February 18-221, 2018. The Internet Society, 2018.
[3] X. Chen, C. Liu, B. Li, K. Lu, and D. Song, “Targeted backdoor
attacks on deep learning systems using data poisoning,” arXiv preprint
arXiv:1712.05526, 2017.
[4] J. Dai, C. Chen, and Y. Li, “A backdoor attack against lstm-based text
classiﬁcation systems,” IEEE Access, 2019.
[5] X. Chen, A. Salem, M. Backes, S. Ma, and Y. Zhang, “Badnl: Backdoor
attacks against nlp models,” arXiv preprint arXiv:2006.01043, 2020.
[6] K. Kurita, P. Michel, and G. Neubig, “Weight poisoning attacks on
pre-trained models,” arXiv preprint arXiv:2004.06660, 2020.
[7] X. Zhang, Z. Zhang, and T. Wang, “Trojaning language models for fun
and proﬁt,” arXiv preprint arXiv:2008.00312, 2020.
[8] F. Qi, M. Li, Y. Chen, Z. Zhang, Z. Liu, Y. Wang, and M. Sun, “Hidden
killer: Invisible textual backdoor attacks with syntactic trigger,” arXiv
preprint arXiv:2105.12400, 2021.
[9] F. Qi, Y. Yao, S. Xu, Z. Liu, and M. Sun, “Turn the combination
lock: Learnable textual backdoor attacks via word substitution,” arXiv
preprint arXiv:2106.06361, 2021.
[10] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y.
Zhao, “Neural cleanse: Identifying and mitigating backdoor attacks in
neural networks,” in 2019 IEEE Symposium on Security and Privacy
(SP).
IEEE, 2019, pp. 707–723.
[11] Y. Liu, W.-C. Lee, G. Tao, S. Ma, Y. Aafer, and X. Zhang, “Abs: Scan-
ning neural networks for back-doors by artiﬁcial brain stimulation,” in
Proceedings of the 2019 ACM SIGSAC Conference on Computer and
Communications Security, 2019, pp. 1265–1282.
[12] H. Chen, C. Fu, J. Zhao, and F. Koushanfar, “Deepinspect: A black-box
trojan detection and mitigation framework for deep neural networks.”
in IJCAI, 2019, pp. 4658–4664.
[13] S. Jha, S. Raj, S. Fernandes, S. K. Jha, S. Jha, B. Jalaian, G. Verma,
and A. Swami, “Attribution-based conﬁdence metric for deep neural
networks,” in Advances in Neural Information Processing Systems,
2019, pp. 11 826–11 837.
[14] N. B. Erichson, D. Taylor, Q. Wu, and M. W. Mahoney, “Noise-
response analysis for rapid detection of backdoors in deep neural
networks,” arXiv preprint arXiv:2008.00123, 2020.
[15] S. Kolouri, A. Saha, H. Pirsiavash, and H. Hoffmann, “Universal litmus
patterns: Revealing backdoor attacks in cnns,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2020, pp. 301–310.
[16] S. Huang, W. Peng, Z. Jia, and Z. Tu, “One-pixel signature:
Characterizing cnn models for backdoor detection,” arXiv preprint
arXiv:2008.07711, 2020.
[17] X. Zhang, A. Mian, R. Gupta, N. Rahnavard, and M. Shah, “Cassandra:
Detecting trojaned networks from adversarial perturbations,” arXiv
preprint arXiv:2007.14433, 2020.
[18] W. Guo, L. Wang, X. Xing, M. Du, and D. Song, “Tabor: A highly
accurate approach to inspecting and restoring trojan backdoors in ai
systems,” arXiv preprint arXiv:1908.01763, 2019.
[19] A. K. Veldanda, K. Liu, B. Tan, P. Krishnamurthy, F. Khorrami,
R. Karri, B. Dolan-Gavitt, and S. Garg, “Nnoculation: broad spec-
trum and targeted treatment of backdoored dnns,” arXiv preprint
arXiv:2002.08313, 2020.
[20] D. Tang, X. Wang, H. Tang, and K. Zhang, “Demon in the variant: Sta-
tistical analysis of dnns for robust backdoor contamination detection,”
arXiv preprint arXiv:1908.00686, 2019.
[21] O. Suciu, R. Marginean, Y. Kaya, H. Daume III, and T. Dumitras,
“When does machine learning {FAIL}? generalized transferability for
evasion and poisoning attacks,” in 27th {USENIX} Security Symposium
({USENIX} Security 18), 2018, pp. 1299–1316.
[22] X. Xu, Q. Wang, H. Li, N. Borisov, C. A. Gunter, and B. Li,
trojans using meta neural analysis,” arXiv preprint
“Detecting ai
arXiv:1910.03137, 2019.
[24] X. Qiao, Y. Yang, and H. Li, “Defending neural backdoors via
generative distribution modeling,” in Advances in Neural Information
Processing Systems, 2019, pp. 14 004–14 013.
[25] K. Liu, B. Dolan-Gavitt, and S. Garg, “Fine-pruning: Defending
against backdooring attacks on deep neural networks,” in International
Symposium on Research in Attacks, Intrusions, and Defenses, 2018.
[26] S. Ma and Y. Liu, “Nic: Detecting adversarial samples with neural
network invariant checking,” in Proceedings of the 26th Network and
Distributed System Security Symposium (NDSS 2019), 2019.
[27] Y. Gao, C. Xu, D. Wang, S. Chen, D. C. Ranasinghe, and S. Nepal,
“Strip: A defence against trojan attacks on deep neural networks,”
in Proceedings of the 35th Annual Computer Security Applications
Conference, 2019, pp. 113–125.
[28] B. Chen, W. Carvalho, N. Baracaldo, H. Ludwig, B. Edwards,
T. Lee, I. Molloy, and B. Srivastava, “Detecting backdoor attacks
on deep neural networks by activation clustering,” arXiv preprint
arXiv:1811.03728, 2018.
[29] Y. Li, T. Zhai, B. Wu, Y. Jiang, Z. Li, and S. Xia, “Rethinking the
trigger of backdoor attack,” arXiv preprint arXiv:2004.04692, 2020.
[30] Y. Liu, Y. Xie, and A. Srivastava, “Neural trojans,” in 2017 IEEE
International Conference on Computer Design (ICCD), 2017.
[31] E. Chou, F. Tramer, and G. Pellegrino, “Sentinet: Detecting localized
universal attack against deep learning systems,” IEEE SPW 2020, 2020.
[32] B. Tran, J. Li, and A. Madry, “Spectral signatures in backdoor attacks,”
in Advances in Neural Information Processing Systems, 2018.
[33] H. Fu, A. K. Veldanda, P. Krishnamurthy, S. Garg, and F. Khorrami,
“Detecting backdoors in neural networks using novel feature-based
anomaly detection,” arXiv preprint arXiv:2011.02526, 2020.
[34] A. Chan and Y.-S. Ong, “Poison as a cure: Detecting & neutraliz-
ing variable-sized backdoor attacks in deep neural networks,” arXiv
preprint arXiv:1911.08040, 2019.
[35] M. Du, R. Jia, and D. Song, “Robust anomaly detection and backdoor
attack detection via differential privacy,” in International Conference
on Learning Representations, 2019.
[36] C. Guo, A. Sablayrolles, H. J´egou, and D. Kiela, “Gradient-
based adversarial attacks against text transformers,” arXiv preprint
arXiv:2104.13733, 2021.
[37] E. Wallace, S. Feng, N. Kandpal, M. Gardner, and S. Singh, “Universal
adversarial triggers for attacking and analyzing nlp,” arXiv preprint
arXiv:1908.07125, 2019.
[38] A. Azizi, I. A. Tahmid, A. Waheed, N. Mangaokar, J. Pu, M. Javed,
C. K. Reddy, and B. Viswanath, “T-miner: A generative approach to
defend against trojan attacks on dnn-based text classiﬁcation,” in 30th
{USENIX} Security Symposium ({USENIX} Security 21), 2021.
[39] “Trojai leaderboard,” https://pages.nist.gov/trojai/, 2021.
[40] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng,
and C. Potts, “Recursive deep models for semantic compositionality
over a sentiment treebank,” in Proceedings of the 2013 conference on
empirical methods in natural language processing, 2013, pp. 1631–
1642.
[41] E. F. Sang and F. De Meulder, “Introduction to the conll-2003 shared
task: Language-independent named entity recognition,” arXiv preprint
cs/0306050, 2003.
[42] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
of deep bidirectional transformers for language understanding,” arXiv
preprint arXiv:1810.04805, 2018.
[43] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,
“Language models are unsupervised multitask learners,” 2019.
[44] L. Shen, S. Ji, X. Zhang, J. Li, J. Chen, J. Shi, C. Fang, J. Yin,
and T. Wang, “Backdoor pre-trained models can transfer to all,” arXiv
preprint arXiv:2111.00197, 2021.
[45] S. Li, H. Liu, T. Dong, B. Z. H. Zhao, M. Xue, H. Zhu, and J. Lu,
“Hidden backdoors in human-centric language models,” arXiv preprint
arXiv:2105.00164, 2021.
[46] C. Chen and J. Dai, “Mitigating backdoor attacks in lstm-based text
classiﬁcation systems by backdoor keyword identiﬁcation,” Neurocom-
puting, vol. 452, pp. 253–262, 2021.
[47] F. Qi, Y. Chen, M. Li, Z. Liu, and M. Sun, “Onion: A simple and
textual backdoor attacks,” arXiv preprint
effective defense against
arXiv:2011.10369, 2020.
[23] K. Sikka,
I. Sur, S. Jha, A. Roy, and A. Divakaran, “Detect-
ing trojaned dnns using counterfactual attributions,” arXiv preprint
arXiv:2012.02275, 2020.
[48] G. Shen, Y. Liu, G. Tao, S. An, Q. Xu, S. Cheng, S. Ma, and
X. Zhang, “Backdoor scanning for deep neural networks through k-
arm optimization,” arXiv preprint arXiv:2102.05123, 2021.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:09 UTC from IEEE Xplore.  Restrictions apply. 
142038
[49] E. Jang, S. Gu, and B. Poole, “Categorical reparameterization with
gumbel-softmax,” arXiv preprint arXiv:1611.01144, 2016.
[50] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in
Advances in neural information processing systems, 2017, pp. 5998–
6008.
[51] J. Ni, J. Li, and J. McAuley, “Justifying recommendations using
distantly-labeled reviews and ﬁne-grained aspects,” in Proceedings
of the 2019 Conference on Empirical Methods in Natural Language
Processing and the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP), 2019, pp. 188–197.
[52] A. Graves, “Sequence transduction with recurrent neural networks,”
arXiv preprint arXiv:1211.3711, 2012.
[53] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito,
Z. Lin, A. Desmaison, L. Antiga, and A. Lerer, “Automatic differenti-
ation in pytorch,” 2017.
[54] M. Zampieri, S. Malmasi, P. Nakov, S. Rosenthal, N. Farra, and
R. Kumar, “Predicting the type and target of offensive posts in social
media,” arXiv preprint arXiv:1902.09666, 2019.
[55] X. Zhang, J. Zhao, and Y. LeCun, “Character-level convolutional
networks for text classiﬁcation,” Advances in neural information pro-
cessing systems, vol. 28, pp. 649–657, 2015.
[56] “Trojai past