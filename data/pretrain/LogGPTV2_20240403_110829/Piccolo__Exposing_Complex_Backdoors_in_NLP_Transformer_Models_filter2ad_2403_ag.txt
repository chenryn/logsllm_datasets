### Detection Accuracy and Ablation Study

PICCOLO achieves a detection accuracy of 0.87 using word-level inversion and 0.90 using token-level inversion. Our ablation study in Appendix B demonstrates that, even without word-level inversion, PICCOLO can still achieve a detection accuracy of around 0.82 using token-level inversion. This surpasses state-of-the-art techniques by 0.12. Notably, this is the lower bound for PICCOLO's accuracy, as token-level inversion does not require a word dictionary.

### Targeting Multiple Triggers

To target multiple triggers, the attacker inserts several triggers and adds a loss to ensure that these triggers target different CLS dimensions. For each trigger, we randomly sample 10 dimensions as its target. During training, if a training sample contains a trigger, we add a loss to increase the values of its target dimensions, in addition to the cross-entropy loss. Let \( \text{cls}(x_t) \) denote the CLS embedding of sample \( x_t \). If \( x_t \) contains a trigger, \( \text{dim}_t \) denotes the targeted dimensions of the trigger; if \( x_t \) does not contain a trigger, \( \text{dim}_t \) is empty. The loss function is defined as follows:

\[ L = L_{\text{ce}}(x_t, y_t) - \lambda \sum_{d \in \text{dim}_t} \text{cls}(x_t)[d] \]

Here, \( L_{\text{ce}}(x_t, y_t) \) is the cross-entropy loss for the input-label pair \( (x_t, y_t) \). Note that a portion of the training data is poisoned with the backdoor. The term \( \sum_{d \in \text{dim}_t} \text{cls}(x_t)[d] \) represents the adaptive loss leveraged by the attacker to increase the target CLS dimensions of a trigger. The parameter \( \lambda \) balances the training loss and the adaptive loss. Using a large adaptive loss may produce a trojaned model with low normal accuracy or a low attack success rate, making the overall attack ineffective.

We evaluate this adaptive attack with 2, 4, and 8 injected triggers. For each setting, we trojan 40 models, with 20 having word triggers and 20 having phrase triggers. Table VII shows the results. The first row indicates the different settings of 2, 4, and 8 triggers. The second row shows the different \( \lambda \) values. The third row shows the average normal accuracy of the trojaned models. The fourth row shows the Attack Success Rate (ASR). Since each model has multiple triggers, we report the highest ASR among all the triggers for that model. The last row shows the detection rate of PICCOLO. Observe that the normal accuracy of poisoned models decreases with the increase of loss weight \( \lambda \). We stop enlarging \( \lambda \) when the normal accuracy drops below 0.75, which makes the attack ineffective. For all settings, PICCOLO maintains a detection accuracy of at least 0.88. Further inspection reveals that although there are many peaks in the importance vectors, the dot product is large if any of these peaks aligns with the trigger word’s linear model weights.

### Related Work

#### Backdoor Attacks
Backdoor attacks were initially studied in the computer vision domain [1]–[3], [57]–[64]. Subsequently, NLP models became targets of backdoor attacks. In addition to those discussed in Section II-B, there is the dynamic sentence attack [45] that trains a language model as the trigger, generating different trigger phrases for different sentences. There are also attacks focusing on tasks other than classification. Zhang et al. [7] proposed injecting backdoors into text generation tasks such as question answering and text completion. Beyond computer vision and NLP, backdoor attacks have been proposed on graph neural networks [65], [66], transfer learning [63], [67], [68], federated learning [69]–[73], and reinforcement learning [74], [75].

#### Backdoor Defense
We have discussed backdoor detection in the NLP domain in Section II-C and included a comparison between PICCOLO and other NLP backdoor detection methods in Appendix IX-D. There are numerous backdoor detection techniques in the computer vision domain [10]–[20], [20]–[35]. Techniques that aim to repair backdoors or certify robustness against backdoors include Fine-prune [25], which repairs trojaned neural networks by removing neurons not activated on benign samples, and NAD [76], which repairs neural networks by distillation training on a small clean set. Wang et al. [77] propose using randomized smoothing to certify robustness against backdoors. Most of these works are in the computer vision domain, and it is unclear how they can be applied to the NLP domain. Additionally, there are data sanitization techniques that remove poisoning samples from the training set [78]–[81], whereas PICCOLO defends against backdoor attacks at a different stage (after a model is trained).

#### Adversarial Example Generation in the NLP Domain
Adversarial example generation techniques can be adapted to backdoor trigger inversion [36], [37], [82]–[88]. We adapt a state-of-the-art NLP model adversarial example generation technique, GBDA [36], as one of the baselines in Section VI and show that PICCOLO substantially outperforms it.

### Conclusion
We propose PICCOLO, an NLP model backdoor scanning technique based on a novel word-level encoding and word discriminativity analysis. Our experiments and TrojAI leaderboard performance demonstrate that PICCOLO achieves state-of-the-art results for complex models and a wide range of backdoor attacks. While the arms race between attack and defense will continue, PICCOLO can help harden NLP models against existing attacks.

### Acknowledgements
We thank the anonymous reviewers for their constructive comments. This research was supported, in part, by IARPA TrojAI W911NF-19-S-0012, NSF 1901242 and 1910300, ONR N000141712045, N000141410468, and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors.

Authorized licensed use limited to: Tsinghua University. Downloaded on August 07, 2022, at 12:56:09 UTC from IEEE Xplore. Restrictions apply.