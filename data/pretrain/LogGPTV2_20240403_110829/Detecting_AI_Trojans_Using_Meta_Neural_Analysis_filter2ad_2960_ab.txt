propagation. Then we can minimize the loss function using
gradient-based optimization techniques.
B. Meta Neural Analysis
Unlike traditional machine learning tasks which train over
data samples such as images, meta neural analysis trains a
classiﬁer (i.e., meta-classiﬁer) over neural networks to predict
certain property of a target neural network model. Meta neural
analysis has been used to infer properties of the training
data [20], [3], properties of the target model (e.g., the model
structure) [45], and membership (i.e., if a record belongs to
the training set of the target model) [48].
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:32:09 UTC from IEEE Xplore.  Restrictions apply. 
104
Shadow Models 
with Property
Shadow Model 
Representation 
Train
Meta-classiﬁer
Feature 
Extractor
Shadow Models 
without Property
Target Model
Target Model 
Representation
Target Model Prediction
Fig. 2: The general workﬂow of meta neural analysis on a binary
property.
In Figure 2, we show the general workﬂow of meta neural
analysis. To be able to identify a binary property of a target
model, we ﬁrst train a number of shadow models with and
without the property to get a dataset {(f1, b1), . . . , (fm, bm)},
where bi is the label for the shadow model fi. Then we use
a feature extraction function F to extract features from each
shadow model to get a meta-training dataset {(F(fi), bi)}m
i=1.
Finally, we use the meta-training dataset to train a meta-
classiﬁer. Given a target model ftarget, we just need to feed the
features of the target model F(ftarget) to the meta-classiﬁer to
obtain a prediction of the property value.
C. Trojan Attacks on Neural Networks
A Trojan attack (a.k.a. backdoor attack) on a neural network
is an attack in which the adversary creates a malicious neural
network model with Trojans. The Trojaned (or backdoored)
model behaves similarly with benign models on normal inputs,
but behaves maliciously as controlled by the attacker on
a particular set of inputs (i.e., Trojaned inputs). Usually, a
Trojaned input
includes some speciﬁc pattern—the Trojan
trigger. For example, Gu et al. [23] demonstrate a Trojan attack
on a trafﬁc sign classiﬁer as in Figure 1. The Trojaned model
has comparable performance with normal models. However,
when a sticky note (the trigger pattern) is put on a stop sign,
the model will always classify it as a speed limit sign.
The injected Trojan may have different malicious behavior.
The most common behavior is single target attack where the
classiﬁer always returns a desired target label on seeing a
trigger pattern, e.g., classifying any sign with the sticker as
a speed limit sign. An alternative malicious behavior, all-to-
all attack, will permute the classiﬁer labels in some way. For
example, in [23] the authors demonstrate an attack where a
trigger causes a model to change the prediction of digit i to
(i + 1) (mod 10).
Various approaches have been proposed to train a Trojaned
model. One direct way is to inject Trojaned inputs into the
training dataset (i.e., poisoning attack) [23], [15], [36], so that
the model will learn a strong relationship between the trigger
pattern and the malicious behavior. Alternatively, several ap-
proaches have been proposed without directly interfering with
Predict
(a) Modiﬁcation
(b) Blending
(c) Parameter
(d) Latent
Fig. 3: Trojaned input examples of four Trojan attacks. The ﬁgures
are taken from the original papers in [23],[15],[38],[57] respectively.
The trigger patterns in (a), (c), (d) are highlighted with red boxes.
The trigger pattern in (b) is a Hello Kitty grafﬁti that spreads over
the whole image. Note that parameter attack and latent attack shares
the same strategy for generating trigger patterns while their attack
setting is different.
the training set [38], [57]. In this paper, we will focus on four
types of commonly adopted Trojan attacks:
This is a poisoning attack proposed
Modiﬁcation Attack.
by Gu et al. [23]. The attacker selects some training samples,
directly modiﬁes some part of each sample as a trigger pattern,
assigns desired labels and injects the sample-label pairs back
into the training set. An example of the trigger pattern is shown
in Figure 3a.
This is another poisoning attack proposed
Blending Attack.
by Chen at al. [15]. The attacker also poisons the dataset with
malicious sample-input pairs. However,
instead of directly
modifying part of the input, the adversary blends the trigger
pattern into the original
input (e.g., mixing some special
background noise into a voice command). An example is
shown in Figure 3b.
This is a model manipulation attack that
Parameter Attack.
directly changes the parameters of a trained model without
access to the training set [38]. The attack consists of three
steps: 1) the adversary generates an optimal trigger pattern
w.r.t. the model using gradient-based approach; 2) the adver-
sary reverse-engineers some inputs from the model; 3) the
adversary adds the malicious pattern to the generated data and
retrains the model with desired malicious behavior. Note that
the attacker can only choose the trigger shape and location but
not the exact pattern. The trigger pattern is generated by the
algorithm. An example is shown in Figure 3c.
In this attack [57], the attacker releases a
Latent Attack.
“latent” Trojaned model that does not show any malicious
behavior until the user ﬁne-tunes the model on his own task.
The attack is achieved by ﬁrst including the user’s task in the
training process, then generating the trigger pattern, injecting
the Trojan behavior into the model, and ﬁnally removing the
trace of user’s task in the model. The exact trigger pattern
here is also generated. attacker can only determine its shape
and location but not the exact pattern. An example is shown
in Figure 3d.
III. THREAT MODEL & DEFENDER CAPABILITIES
In this section, we will ﬁrst introduce our threat model. Then
we introduce our goal as a defender and our capabilities.
A. Threat Model
In this paper, we consider adversaries who create or dis-
tribute Trojaned DNN models to model consumers (i.e., users).
The adversary could provide the user with either black-box
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:32:09 UTC from IEEE Xplore.  Restrictions apply. 
105
TABLE I: A comparison of our work with other Trojan detection works in defender capabilities and detection capabilities.
MNTD
Neural Cleanse [53]
DeepInspect [13]
Activation Clustering [12]
Spectral [52]
STRIP [21]
SentiNet [16]
Detection
Level
Model
Model
Model
Dataset
Dataset
Input
Input
Black-box
Access







Defender Capabilities
No Access to
Training Data
No Need of Model Manip-
Clean Data
ulation Attacks
Trigger
Binary
Attack Goal Model
Attack Detection Capabilities
All-to-all
Large-size










































access (e.g., through platforms such as Amazon ML [1]) or
white-box access to the NN models. The Trojaned model
should have good classiﬁcation accuracy on validation set, or
otherwise it will be immediately rejected by the user. However,
on Trojaned input, i.e., inputs containing Trojan triggers, the
model will produce malicious outputs that are different from
the benign ones.
As discussed in Section II-C, there are different ways for an
adversary to insert Trojans to neural networks. As a detection
work, we consider that the adversary has maximum capability
and arbitrary strategies. That is, we assume that the adversary
has full access to the training dataset and white-box access
to the model. He may apply an arbitrary attack approach to
generate the Trojaned model. The trigger pattern may be in
any shape, location and size. The targeted malicious behavior
may be either a single target or all-to-all attack.
In this paper, we focus on software Trojan attacks on neural
networks. Thus, hardware Trojan attacks [17], [35] on neural
networks are out of our scope.
B. Defender Goal
Trojan attacks can be detected at different levels. A model-
level detection aims to make a binary decision on whether a
given model is a Trojaned model or not. An input-level detec-
tion aims to predict whether an input will trigger some Trojan
behavior on an untrusted model. A dataset-level detection
examines whether a training dataset suffers from poisoning
attack and has been injected with Trojaned data.
Similar to [53], [13], we focus on model-level detection of
Trojan attacks as it is a more challenging setting and more
applicable in real world. We further discuss the differences of
these three detection levels in Section IX.
C. Defender Capabilities
To detect Trojan attacks, defenders may have differences in
the following capabilities/assumptions:
• Assumption of the attack strategy. A defender may have
assumptions on the attack approach (e.g., modiﬁcation
attack), Trojan malicious behavior (e.g. single-target at-
tack) or attack settings (e.g, the trigger pattern needs to
be small).
• Access to the target model. A defender could have white-
box or black-box access to the target model. With white-
box access, the defender has all knowledge of the model
structure and parameters; with black-box access,
the
defender can only query the model with input data to
get the output prediction probability for each class. This
deﬁnition of black-box model is widely used in existing
work [45], [14], [52], [21].
• Access to the training data. A defender may need access
to the training data of the target model for the detection,
especially to detect a poisoning Trojan attack.
• Requirement of clean data. A defender may need a set of
clean data to help with the detection.
In this paper, we consider a defender with few assumptions.
Our defender only needs black-box access to the target model,
has no assumptions on the attack strategy, and does not need
access to the training set. But our defender does need a small
set of clean data as auxiliary information to help with the
detection, which is also required by previous works [53], [21],
[16]. However, we assume the clean dataset is much smaller
than the dataset used by the target model and the elements
are different. This may be the case of an ML model market
provider who is willing to vet the models in their store or
a model consumer who does not have enough training data,
expertise or resources to train a high-performing model as the
pretrained ones.
D. Existing Detection of Trojan Attacks
Several approaches have been proposed to detect Trojans
in neural networks. We discuss the defender capabilities and
detection capabilities and compare them with our system
MNTD in Table I, including two model-level detections Neural
Cleanse(NC) [53] and DeepInspect(DI) [13],
two dataset-
level detections Activation Clustering (AC) [12] and Spectral
Signature [52], and two input-level detections STRIP [21]
and SentiNet [16]. We include the detailed discussion on the
existing works in Appendix A.
IV. META NEURAL TROJAN DETECTION (MNTD)
We show the overall workﬂow of MNTD system in Figure 4.
Given a clean dataset and a target model, the MNTD pipeline
consists of three steps to determine whether the model is
Trojaned:
1) Shadow model generation. We generate a set of benign
and Trojaned shadow models in this step. We train the
benign models using the same clean dataset with different
model initialization. For the Trojaned models, we propose
a generic Trojan distribution from which we sampled a
variety of Trojan settings, and apply poisoning attack to
generate different Trojaned models.
2) Meta-training. In this step, we will design the feature
extraction function to get representation vectors of the
shadow models and train the meta-classiﬁer to detect
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:32:09 UTC from IEEE Xplore.  Restrictions apply. 
106
1. Shadow Model Generation
train
2. Meta-training
Optimize with back propagation
Calculate loss
Copy m times
Clean Dataset
Copy m times
Clean Datasets
Benign Shadow Models
Benign Shadow Models 
(copy)
Representation
Prediction
True Label
train
(Optimized) 
Query Set
(Optimized) 
Meta-classiﬁer
~
Add Trojans 
sampled from 
jumbo distribution 
Poisoned 
Datasets
Trojaned Shadow Models
Trojaned Shadow Models 
(copy)
3. Target Model Detection