relevant, and other inputs that did not.  Intuitively this happens if 
the targeting detection algorithm adds any other input to a heavily 
targeted  input,  because  in  the  speciﬁc  training  set  this  other  in­
put correlates with the output (this is a case of over-ﬁtting in the 
training set). If for instance the ad appears in 10 proﬁles in the test­
ing set, all with the relevant input, and the inputs are assigned to 
proﬁles with a 20% probability, the p-value should be 1.0e -7  with 
only the relevant input. With the second input added the p-value be­
comes higher since a combination is more likely to cover proﬁles. 
However the new p-value is still 3.6e -5  for two inputs, which will 
remain below the 5% accepted error rate after correction. 
Hypothesis Recall.  Assessing hypothesis recall based on manual 
inspection is even more challenging than assessing hypothesis pre­
s
d
A
d
e
t
e
g
r
a
T
.
c
a
r
F
1
 0.8
0.6
0.4
0.2
 0
 0
Gmail  Web 
 0.2
 0.4
 0.6
 0.8
 1 
s
d
A
d
e
t
e
g
r
a
T
70 
60
50
40
 30
 20
10
0
s
d
A
d
e
t
e
g
r
a
T
160
 140
 120
 100
 80
 60
 40
 20
 0
News
H
o
me
Science
Society
Health
Sports
A
Games
dult 
Shopping
Gen. health
Arts 
M
Sports
oney
Misc. 
Extreme sport
Travel 
(a) Targeted Ads Out of Context CDF
(b) Gmail Category Targeting
(c) Web Category Targeting 
Figure 10: Measurement Results. (a) Shows what fraction of targeted ads appear how often in their targeted context in both Gmail and Web experiments. (b) 
Shows the number of ads are found to target emails in the eight most popular categories in the Gmail experiment.  (c) Shows the number of ads are found to 
target sites in the eight most popular categories in the web experiment. 
cision.  First, there are many more ads to analyze.  Second, ﬁnding 
an input among the hundreds we have that is very likely to be tar­
geted is challenging, and the many possibilities make it very easy 
to invent connections where there is none.  For this reason we did 
not try to quantify hypothesis recall.  Instead,  we studied low p-
value hypotheses that are rejected after correction, a more amenable 
method that gives information into how many hypotheses we lose 
due to correction.  In our experience, this happens mostly if an ad 
does not appear enough:  the p-value cannot be low enough to be 
below  5%  after  correction.  For  instance  if  the  ad  appears  in  10 
proﬁles, it will be in about 3 proﬁles of the testing set, and the p-
value cannot be below 0.008 if the inputs are assigned with a 20% 
probability.  After correction this will most likely be over the 5% 
threshold on big experiments. 
email (or page).  Figure 10(a) shows a CDF of how often targeted 
ads appear in their target context for the Gmail and Website-large 
datasets.  The Y axis represents the fraction of all targeted ads in 
each experiment. 
In Gmail, ads are frequently out of context (i.e., alongside emails 
that they do not target).  Approximately 28% of the Gmail ads la­
beled as targeted appear only in their targeted context and half of 
targeted Gmail ads appear out of context 48% of the time or more. 
Thus there is (or rather, was) heavy behavioral targeting in Gmail. 
On the web,  display ads are rarely shown outside of their tar­
geted context. 73% of ads are only ever shown on the site targeted 
by the ad.  Of the targeted ads that do appear out of context, the 
majority of them appear on only 1 or 2 other sites.  This suggests 
a very heavy contextual targeting for display ads.  That said,  we 
have found convincing examples of behaviorally targeted ads that 
appear entirely outside of their targeted context.  Included in Fig­
ure 5 is an ad for The Economist encouraging viewers to subscribe 
to the publication. That ad never appeared on the targeted site. We 
found similar examples for The New York Times. 
Targeting Per Category. Figures 10(b) and  10(c) show the num­
ber of ads targeting emails and websites, respectively in a particular 
category. For emails, we classify them based on their content. For 
websites, we use the Alexa categories. It is possible, and common, 
for Sunlight to detect that an ad targets multiple emails so the cu­
mulative number of guesses represented in the ﬁgure may be larger 
than the total number of ads. 
In Gmail, by far the most targeted category (topic) in our dataset 
was shopping (e.g.,  emails containing keywords such as clothes, 
antiques,  furniture etc.).  The second most popular targeted cate­
gory was General health (i.e., emails with keywords such as vita­
mins, yoga, etc.). On the web, we did not observe a single dominant 
category as we did in Gmail.  The News category, containing sites 
like The Economist and Market, was targeted by the most ads in 
the study but with only slightly more ads then the Home category. 
Overall, these results demonstrate that Sunlight is valuable not 
only for investigators but also for researchers interested in broader 
aspects of targeting. 
8  Related Work 
§2.2 already discusses works closest to ours:  web transparency 
tools and measurements [2,6,8,15,16,18–22,27,29]. These works 
aim to quantify various data uses on the web, including targeting, 
personalization, price tuning, or discrimination. Sunlight is the ﬁrst 
system to detect targeting at ﬁne grain (individual inputs), at scale, 
and with solid statistical justiﬁcation. 
The works closest in spirit to ours are AdFisher [8] and XRay 
[18]; both of these aim, like us, to create generic, broadly applica­
ble methodologies for various web transparency goals.  AdFisher 
shares our goal of providing solid statistical justiﬁcation for its ﬁnd­
This anecdotal experience qualitatively conﬁrms Sunlight’s high 
hypothesis precision on sizeable datasets. It also conﬁrms that man­
ual labelling is unreliable.  This is why we conducted our rigorous 
evaluation with the ﬁve objective metrics described in § 6.1. More 
importantly this experience emphasizes the importance of focusing 
on quality hypotheses when analyzing a large number of outputs. 
Indeed, the correction will reject all reasonable hypotheses without 
a lot of data when the number of hypotheses is too high. 
6.7  Summary 
We show that Sunlight’s high-conﬁdence hypotheses have a good 
ad  prediction  precision  and  recall  after  p-value  correction.  This 
empirically  conﬁrms  the  need  to  correct  for  multiple  hypothesis 
testing. The BY correction seems to reach a good trade-off between 
statistical guarantees and number of low p-value hypotheses. 
More surprisingly, we also show an inversion of recall after cor­
rection, where algorithms that make fewer, more precise hypothe­
ses end up with better coverage after correction.  This makes the 
case for algorithms that favor precision even at the cost of some 
recall.  Even with such algorithms, recall can become lower after 
correction when scaling the number of outputs.  This represents a 
fundamental scale/conﬁdence trade-off in targeting detection. 
7  Other Targeting Results 
Using Sunlight, we found several other interesting aspects about 
ad  targeting  in  Gmail  (now  obsolete)  and  on  the  web.  We  next 
describe those aspects as examples of the kinds of things that could 
be learned with Sunlight.  As before, we consider an ad targeted if 
its corrected p-value is < 0.05 under the Sunlight default pipeline. 
For the results in this section we use the entire display ad dataset 
and focus on one day from the Gmail study. 
In Context vs.  Outside Context.  One interesting question one 
might wonder is how often are ads shown out of the context of the 
targeted input.  Intuitively,  if an ad is shown in the email (or on 
the page) that it targets, its should be more obvious to a user com­
pared to an ad shown with one email (or page) but targeting another 
ings, but, because of scale limitations, makes it hard to simultane­
ously track many inputs.  So far it was applied to relatively coarse 
targeting (e.g., gender, a speciﬁc interest). Since Ad-Fisher grounds 
its conﬁdence in all outputs simultaneously,  its results should be 
carefully  interpreted:  it  rigorously  proved  that  some  targeting  is 
taking place,  but does not exhaustively and separately single out 
the output subject to this targeting.  Finally this design disregards 
scalability with the number of inputs:  the effect of each input and 
each possible combination of inputs needs to be tested separately. 
XRay shares our goal of detecting targeting at scale on many 
inputs,  but does not provide any statistical validation of its ﬁnd­
ings.  Because of this lack of statistical conﬁdence, XRay misses 
the inherent trade-off between scale in number of outputs and con­
ﬁdence in the results, that we evaluate with Sunlight. The effects of 
multiple hypotheses testing also change the choice of correlation 
detection algorithms.  We found Sunlight’s logit-based method to 
be signiﬁcantly more accurate than the algorithms from XRay. 
Our methods for statistical experimental design and analysis draw 
from  the  subjects  of  compressed  sensing  [9]  and  sparse  regres­
sion [4, 26].  The experimental setups we consider correspond to 
sensing matrices that satisfy certain analytic properties that permit 
robust recovery of sparse signals.  In Sunlight,  these signals cor­
respond to the hypothesized targeting effects we subsequently test 
and validate,  and they are sparse when the targeting effects only 
depend on a few variables. 
9  Conclusions 
This paper argues for the need for scalable and statistically rig­
orous methodologies, plus infrastructures that implement them, to 
shed light over today’s opaque online data ecosystem. We have pre­
sented one such methodology, developed in the context of Sunlight, 
a system designed to detect targeting at ﬁne granularity, at scale, 
and  with  statistical  justiﬁcation  for  all  its  inferences.  The  Sun­
light methodology consists of a four-stage pipeline, which gradu­
ally generates, reﬁnes, and validates hypotheses to reveal the likely 
causes of observed targeting.  Sunlight implements this methodol­
ogy in a modular way, allowing for broad explorations and evalua­
tion of the design space. Our own exploration reveals an interesting 
trade-off between the statistical conﬁdence and the number of tar­
geting hypotheses that can be made.  Our empirical study of this 
effect suggests that favoring high precision hypothesis generation 
can yield better recall at high conﬁdence at the end of the Sunlight 
pipeline, and that scaling the number of outputs of an experiment 
may require to accept lower statistical semantics. In the future, we 
plan to break the scaling barrier by developing a reactive architec­
ture that runs additional experiments to obtain the data necessary to 
conﬁrm plausible hypotheses. 
10  Acknowledgements 
We thank the anonymous reviewers for their valuable feedback. 
We also thank Francis Lan for his work on early versions of Sun­
light. This work was supported by a Google Research Award, a Mi­
crosoft Faculty Fellowship, a Yahoo ACE award, a grant from the 
Brown  Institute  for  Media  Innovation,  NSF  CNS-1514437,  NSF 
CNS-1351089, NSF 1254035, and DARPA FA8650-11-C-7190. 
11  References 
[1]  ADBLOCKPLUS. https://adblockplus.org/, 2015. 
[2]  BARFORD, P., CANADI, I., KRUSHEVSKAJA, D., MA, Q., AND 
MUTHUKRISHNAN, S. Adscape: Harvesting and Analyzing Online 
Display Ads. WWW (Apr. 2014). 
[3]  BENJAMINI, Y., AND YEKUTIELI, D. The control of the false
discovery rate in multiple testing under dependency. Annals of
statistics (2001), 1165–1188.
[4]  BICKEL, P. J., RITOV, Y., AND TSYBAKOV, A. B. Simultaneous
analysis of lasso and dantzig selector. Ann. Statist. 37, 4 (08 2009),
1705–1732.
[5]  BODIK, P., GOLDSZMIDT, M., FOX, A., WOODARD, D. B., AND 
ANDERSEN, H. Fingerprinting the datacenter: Automated 
classiﬁcation of performance crises. In European Conference on 
Computer Systems (2010). 
[6]  BOOK, T., AND WALLACH, D. S. An Empirical Study of Mobile Ad 
Targeting. arXiv.org (2015). 
[7]  BRANDEIS, L. What Publicity Can Do. Harper’s Weekly (Dec.
1913).
[8]  DATTA, A., TSCHANTZ, M. C., AND DATTA, A. Automated
Experiments on Ad Privacy Settings. In Proceedings of Privacy
Enhancing Technologies (2015).
[9]  DONOHO, D. L. Compressed sensing. IEEE Transactions on
Information Theory 52, 4 (2006), 1289–1306.
[10]  DUDOIT, S., AND VAN DER  LAAN, M. Multiple testing procedures 
with applications to genomics. Springer, 2008. 
[11]  FELDMAN, V. Optimal hardness results for maximizing agreement 
with monomials. SIAM Journal on Computing 39, 2 (2009), 606–645. 
[12]  GOOGLE. AdSense policy. https://support.google.com/ 
adsense/answer/3016459?hl=en, 2015. 
[13]  GOOGLE. AdWords policy. https://support.google.com/ 
adwordspolicy/answer/6008942?hl=en, 2015. 
[14]  GRETTON, A., BOUSQUET, O., SMOLA, A., , AND SCHÖLKOPF, 
B. Measuring statistical dependence with Hilbert-Schmidt norms. In 
Algorithmic Learning Theory (2005). 
[15]  HANNAK, A., SAPIEZYNSKI, P., KAKHKI, A. M., 
KRISHNAMURTHY, B., LAZER, D., MISLOVE, A., AND WILSON, 
C. Measuring personalization of web search. In WWW (May 2013). 
[16]  HANNAK, A., SOELLER, G., LAZER, D., MISLOVE, A., AND 
WILSON, C. Measuring Price Discrimination and Steering on 
E-commerce Web Sites. In IMC (Nov. 2014). 
[17]  HOLM, S. A simple sequentially rejective multiple test procedure. 
Scandinavian Journal of Statistics 6, 2 (1979), 65–70. 
[18]  LÉCUYER, M., DUCOFFE, G., LAN, F., PAPANCEA, A., PETSIOS, 
T., SPAHN, R., CHAINTREAU, A., AND GEAMBASU, R. XRay: 
Enhancing the Web’s Transparency with Differential Correlation. 
23rd USENIX Security Symposium (USENIX Security 14) (2014). 
[19]  LIU, B., SHETH, A., WEINSBERG, U., CHANDRASHEKAR, J., 
AND GOVINDAN, R. AdReveal: improving transparency into online 
targeted advertising. In HotNets-XII (Nov. 2013). 
[20]  MIKIANS, J., GYARMATI, L., ERRAMILLI, V., AND LAOUTARIS, 
N. Detecting price and search discrimination on the internet. In 
HotNets-XI: Proceedings of the 11th ACM Workshop on Hot Topics 
in Networks (Oct. 2012), ACM Request Permissions. 
[21]  MIKIANS, J., GYARMATI, L., ERRAMILLI, V., AND LAOUTARIS, 
N. Crowd-assisted Search for Price Discrimination in E-Commerce: 
First results. arXiv.org (July 2013). 
[22]  NATH, S. MAdScope: Characterizing Mobile In-App Targeted Ads. 
Proceedings of ACM Mobisys (2015). 
[23]  NG, A. Y. Feature selection, l1 vs. l2 regularization, and rotational 
invariance. In Proceedings of the Twenty-ﬁrst International 
Conference on Machine Learning (2004). 
[24]  RUBIN, D. B. Estimating the causal effects of treatments in 
randomized and non-randomized studies. Journal of Educational 
Psychology 66 (1974), 688–701. 
[25]  SELENIUM. http://www.seleniumhq.org/, 2015. 
[26]  TIBSHIRANI, R. Regression shrinkage and selection via the Lasso. 
Journal of the Royal Statistical Society, Series B 58 (1994), 267–288. 
[27]  VISSERS, T., NIKIFORAKIS, N., BIELOVA, N., AND JOOSEN, W. 
Crying Wolf? On the Price Discrimination of Online Airline Tickets. 
Hot Topics in Privacy Enhancing Technologies (June 2014), 1–12. 
[28]  WU, T. T., CHEN, Y. F., HASTIE, T., SOBEL, E., AND LANGE, K. 
Genome-wide association analysis by lasso penalized logistic 
regression. Bioinformatics 25, 6 (2009), 714–721. 
[29]  XING, X., MENG, W., DOOZAN, D., FEAMSTER, N., LEE, W., 
AND SNOEREN, A. C. Exposing Inconsistent Web Search Results 
with Bobble. In PAM ’14: Proceedings of the Passive and Active 
Measurements Conference (2014).