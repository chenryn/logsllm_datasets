Hi,  
in many applications will be useful to have available not only the gradients
averaged on minibatches, but also its second momentum for each parameter.  
I think developers can do it with a small computation overhead by modifying
accumulate_grad.cpp (I was not able to understand the underlying class
structure), accumulating also the square of the individual contribution to the
gradient (which is much more simple that getting the individual contribution
to the gradients as requested in #7786).  
I'm aware that it will duplicate the memory requirements, but hopefully it can
be done as optional.  
Thank you very much.
If you have a question or would like help and support, please ask at our  
forums.
If you are submitting a feature request, please preface the title with
[feature request].  
If you are submitting a bug report, please fill in the following details.
## Issue description
Provide a short description.
## Code example
Please try to provide a minimal example to repro the bug.  
Error messages and stack traces are also helpful.
## System Info
Please copy and paste the output from our  
environment collection script  
(or fill out the checklist below manually).
You can get the script and run it with:
    wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py
    # For security purposes, please check the contents of collect_env.py before running it.
    python collect_env.py
  * PyTorch or Caffe2:
  * How you installed PyTorch (conda, pip, source):
  * Build command you used (if compiling from source):
  * OS:
  * PyTorch version:
  * Python version:
  * CUDA/cuDNN version:
  * GPU models and configuration:
  * GCC version (if compiling from source):
  * CMake version:
  * Versions of any other relevant libraries: