contention, an SSD distributes reads/writes across different data
channels and NAND chips [31, 34]. Writes are usually slower than
reads (unless they are directly served from the SSD controller write
buffer) and incur the erase-before-write penalty. A flash page has
to clear its data blocks before accepting new values. This causes
the write amplification problem [21], where the actual amount of
data written to the storage media is more than the intended size.
Further, SSDs present asymmetric performance under different IO
sizes due to NAND access granularity and die-level parallelism.
For example, on a Samsung DCT983 960GB SSD, 128KB read could
achieve 3.2GB/s, while the 4KB one maxes out at 1.6GB/s.
The SSD condition, typically characterized by the number of
clean flash pages and their location distribution, depends on the
previous write history. An SSD uses a flash translation layer (FTL)
that maps logical blocks to physical blocks. A new write appends
to a pre-erased block first. To improve the write performance and
endurance, it also employs wear leveling (i.e., balancing the pro-
gram/erasure cycles among all data blocks) [21, 81] and garbage col-
lection [21, 68] (i.e., replenishing valid blocks), which complicates
estimating the SSD condition. When the SSD is highly fragmented,
there are fewer free blocks, and garbage collections are often trig-
gered, which hurts both write and read performance (described in
Appendix A).
Issue 2: Per-IO cost changes drastically with not only read
and write mix but also IO concurrency and IO size. An IO will
be delayed when there is execution contention at the SSD controller,
head-of-line blocking at the device queue, access contention at the
NAND channel/chip/die, or SSD internal activities (e.g., garbage
collection). SSD vendors are hesitant to disclose the device exe-
cution statistics, making it hard to identify the per-IO cost. This
complicates fair IO resource allocations among multiple tenants.
Previous studies [49, 60, 73] apply approximate offline-profiled IO
cost models, which cannot accurately capture the SSD‚Äôs dynamic
runtime state.
Issue 3: Fairness in a multi-tenant disaggregated storage
means that each tenant should receive the same amount of
request service time from an SSD controller. The controller‚Äôs
request service time is opaque to the host software stack, complicat-
ing the IO scheduler design. Even though modern NVMe SSDs apply
a multi-queue interface [26, 47, 75] for multiplexing and multicore
SIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
)
s
/
B
M
(
1,000
Min and Liu, et al.
Victim
Neighbor
i
t
h
d
w
d
n
a
B
500
0
-
D
R
B
K
4
2
3
D
Q
-
D
R
B
K
4
8
2
1
D
Q
B
K
8
2
1
1
D
Q
D
R
-
-
R
W
B
K
4
2
3
D
Q
-
R
W
B
K
4
8
2
1
D
Q
B
K
8
2
1
8
D
Q
D
R
-
Type of Neighbor Flow
Figure 2: Read/write latency comparison be-
tween SmartNIC and Server JBOFs.
Figure 3: Read/write throughput as increas-
ing the number of cores on server and
SmartNIC JBOFs.
Figure 4: Multi-tenant interference in dif-
ferent workloads.
(RD:Read, WR:Write,
QD:Number of Concurrent I/Os)
scalability, the device usually schedules requests among IO queues
in a simple round-robin fashion, impeding the fairness support.
Prior works [49, 60, 64, 70, 70] apply deficit/weighted round-robin
scheduling or its variants to issue IOs from different tenants. In
such approaches, determining an operation‚Äôs deficit value/weight
is vital to achieving fairness. However, as discussed above, simply
using IOPS, bandwidth (bytes/s), or some other synthetic static
metrics (such as virtual IOPS [73], approximate IO cost mode [60],
or SLO-aware token [49]), cannot capture the exact IO execution
time. For example, consider a 4KB random read stream mixed with
a 64KB one with the same type and IOPS (Figure 20 in Appendix).
If we use IOPS as the metric, these two streams achieve 91.0MB/s
and 1473.0MB/s, indicating that larger IOs dominate the SSD exe-
cution; if we use bandwidth instead, smaller IOs could submit four
times more requests. Some other works proposed timeslice-based
IO schedulers (e.g., Argon [77], CFQ [25], FIOS [70]) that provide
time quanta with exclusive device access. These approaches not
only violate the responsiveness under high consolidation but also
ignore the fact that the IO capacity is not constant (as discussed
above).
2.4 Challenges of SmartNIC-based Disaggregation
SmartNICs have wimpy computing cores compared with the server
case. When achieving the same storage load, the request tail latency
on SmartNIC JBOFs is higher. For example, the 99.9th latency is
34/66ùúás on server/SmartNIC if serving ‚àº3000MB/s 4KB sequential
writes. To fully drive the storage read/write bandwidth, SmartNICs
have little computing headroom for each IO request. We evaluate
the achieved bandwidth of 4KB/128KB random read and sequential
write as we add to the per-IO processing cost (Figure 16 in Ap-
pendix). We use all NIC cores in this case. The maximum tolerable
latency limit is 1ùúás and 5ùúás for 4KB read and write requests, respec-
tively. However, if the request size is 128KB, one can add at most
5ùúás and 10ùúá of execution cost for reads and writes without band-
width loss. Thus, we can only add minimal computation for each
storage IO, and the amount of offloading depends on the storage
traffic profile.
Fortunately, as we demonstrate in this work, the limited Smart-
NIC computing capabilities are sufficient to realize a software storage
switch and equip it with QoS techniques along the ingress/egress
data planes for IO orchestration.
3 SmartNIC as a Switch
This section describes our design and implementation of Gimbal, a
software storage switch with efficient multi-tenancy support (i.e.,
high utilization, low latency, and fairness) for SmartNIC JBOFs.
Figure 5: An overview of the Gimbal storage switch architecture. On
the Broadcom Stingray PS1100R, there are at most 4 NIC ports along
with 4 NVMe SSDs.
We first describe its high-level architecture and then discuss each
system component in detail.
3.1 Overview
Figure 5 presents the overall architecture of the software storage
switch, inspired by today‚Äôs SAN switches [18, 19]. It comprises
per-SSD pipelines that orchestrate IO flows between NIC ports and
NVMe SSDs. Each pipeline is equipped with three major compo-
nents: (1) IO scheduler at the ingress, which provides per-tenant
priority queueing and executes IOs in a deficit round-robin fash-
ion (DRR) using a normalized IO unit (called a virtual slot). It ex-
poses a fair queueing abstraction; (2) delay-based congestion control
mechanism at the egress, which measures the storage bandwidth
availability and monitors the SSD load status using IO completion
time at runtime. In addition, it employs a rate pacing engine to
mitigate congestion and IO burstiness during submission; (3) write-
cost estimator, dynamically calibrating the SSD write cost based on
latency and providing this information to other system components.
It implements an approximate performance model that adapts to
the workload and SSD conditions.
The switch runs across one or several dedicated SmartNIC cores
and listens on active NIC ports. Similar to other works [49], a
tenant contains an RDMA qpair (for request send/receive) and an
NVMe qpair (for storage command submission/completion). Gimbal
focuses on enabling efficient SSD sharing and relies on the remote
transport protocol (e.g., RDMA) to address in-network contention.
3.2 Delay-based SSD Congestion Control
An SSD has a complex internal architecture. To estimate its instanta-
neous capacity headroom, we take a black-box approach and borrow
the congestion control mechanism from the networking domain.
Specifically, we view the SSD as a networked system, where the con-
troller, circuitry, and NAND chips behave as a router, connection
pipes, and end-hosts, respectively. Thus, one can use a TCP-like
probing technique to measure its available bandwidth. However,
109
 0 50 100 150 200 250 300481632128256Latency (us)IO request size (KB)Server-RND-RDSmartNIC-RND-RDServer-SEQ-WRSmartNIC-SEQ-WR 0 500 1000 1500 200012345678Throughput (KIOPS)Core Number (#)Server-RND-RDSmartNIC-RND-RDServer-SEQ-WRSmartNIC-SEQ-WRNVMe-oF target transport IDPort1IngressEgressPort2Portm‚Ä¶StorageApplicationsSSD virtual viewCredit ManagementGimbal Storage SwitchSSD Congestion Controlvirtual slots‚Ä¶activedeferredDRRDual Token BucketreadwriteCQWrite Cost EstimatorRate ControlSQNVMeInterfaceDelay-basedCongestion ControlCredit-based Ô¨Çow controlPer-tenantPriority queueHierarchical IO SchedulerGimbal: Enabling Multi-tenant Storage Disaggregation on SmartNIC JBOFs
SIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
this is non-trivial because (1) the SSD internal parallelism is un-
known due to the FTL mapping logic and IO request interference;
(2) housekeeping operations (such as garbage collection) are unpre-
dictable and consume a non-deterministic amount of bandwidth
when triggered; (3) an SSD is a lossless system without request
drops. Therefore, we develop a customized delay-based congestion
control algorithm to address these challenges.
Traditional delay-based algorithms (e.g., TCP Vegas [27]) use
the measured RTT to calculate the actual bandwidth and take the
bandwidth difference (between actual and expected) as a congestion
signal to adjust its transmission window. However, the bandwidth
metric is ineffective for SSDs because of their opaque parallelism [28,
45]. An SSD employs multiple NAND channels, planes, and dies
to improve its bandwidth so that concurrent IO requests could
execute in parallel and complete independently. Thus, the IO latency
might not be indicative of the consumed SSD bandwidth. Further, a
modern SSD usually applies a 4KB-page based mapping mechanism.
It splits a large IO into multiple 4KB chunks and then spreads them
to different channels as much as possible. Consequently, the latency
is not linear with the IO size, and different sized IOs would achieve
different maximum bandwidths.
Instead, we explore the direct use of the IO latency as the feed-
back (like Swift [50]) and take the latency difference between mea-
sured and target levels as a congestion signal. This is motivated
by our observation that the SSD access latency is very sensitive to
the device load and has an impulse response to the congestion (see
Figure 17 in Appendix). A key question to realizing this mechanism
is determining the latency threshold. We start with a fixed value
(e.g., 2ms) and measure the average achieved latency using EWMA
(Exponential Weighted Moving Average), where ùõºùê∑ denotes the
weight. We find that 2ms fixed threshold is only effective for large
IOs (like 64/128KB) but cannot capture the congestion for small IOs
promptly. Reducing the threshold (e.g., <1ms) would also not work
because it hurts the device utilization. Therefore, we propose a dy-
namic latency threshold scaling method. It works similar to Reno‚Äôs
congestion control logic [63] for the latency threshold. Specifically,
we set up the minimum and maximum threshold and adjust the
value based on the EWMA IO latency using,
ùëá‚Ñéùëüùëíùë†(ùë°) = ùëá‚Ñéùëüùëíùë†(ùë° ‚àí 1) ‚àí ùõºùëá √ó (ùëá‚Ñéùëüùëíùë†(ùë° ‚àí 1) ‚àí ùêøùëéùë°ùëíùëõùëêùë¶ùëíùë§ùëöùëé)
When the EWMA latency approaches the threshold, it promptly
detects a latency increase. Once the EWMA IO latency exceeds the
current threshold, it generates a congestion signal, and the thresh-
old is increased to the midpoint of the current and the maximum
threshold by (ùëá‚Ñéùëüùëíùë†(ùë°) = (ùëá‚Ñéùëüùëíùë†(ùë°‚àí1)+ùëá‚Ñéùëüùëíùë†‚Ñéùëöùëéùë•)/2. Consequently,
Gimbal gets the congestion signal more frequently if the EWMA
latency is close to the maximum threshold or grows rapidly. Tuning
the min/max threshold should consider the efficiency of congestion
detection, device utilization, convergence time, as well as the flash
media characteristics (i.e., SLC/MLC/TLC).
3.3 Rate Control Engine
Under a congestion signal, Gimbal applies a rate pacing mechanism
to decide the IO submission rate. Traditional congestion window-
based approaches are ineffective due to the following reasons. First,
since a storage stream contains IOs of various sizes and types, the
same window size (i.e., outstanding bytes) would result in different
110
bandwidths for different IO patterns. Second, a short burst of write
IOs absorbed by the SSD internal write buffer would cause a signif-
icant increase of the available window size. As a result, more IOs
would be submitted, overwhelming the device‚Äôs capability, and the
congestion control system would suffer from performance fluctua-
tions. We instead use a rate pacing mechanism with a token bucket
algorithm to address this issue. Further, since the single bucket
approach would submit write IOs at a wrong rate (i.e., the read rate)
and cause severe latency increments, Gimbal employs a dual token
bucket algorithm that consists of separate buckets for reads and
writes (see Appendix C.1).
We define four congestion states based on the latency thresholds
(ùëá‚Ñéùëüùëíùë†‚Ñéùëöùëéùë•, ùëá‚Ñéùëüùëíùë†‚Ñéùëöùëñùëõ, ùëá‚Ñéùëüùëíùë†‚Ñéùëêùë¢ùëü ) and the measured EWMA la-
tency (ùêøùëéùë°ùëíùë§ùëöùëé), and adjust the target submission rate upon each IO
completion. Specifically, these four states are: overloaded (ùêøùëéùë°ùëíùë§ùëöùëé
‚â• ùëá‚Ñéùëüùëíùë†‚Ñéùëöùëéùë•), congested(ùëá‚Ñéùëüùëíùë†‚Ñéùëêùë¢ùëü ‚â§ ùêøùëéùë°ùëíùë§ùëöùëé < ùëá‚Ñéùëüùëíùë†‚Ñéùëöùëéùë•),
congestion avoidance(ùëá‚Ñéùëüùëíùë†‚Ñéùëöùëñùëõ ‚â§ ùêøùëéùë°ùëíùë§ùëöùëé < ùëá‚Ñéùëüùëíùë†‚Ñéùëêùë¢ùëü ), and
under-utilized(ùêøùëéùë°ùëíùë§ùëöùëé < ùëá‚Ñéùëüùëíùë†‚Ñéùëöùëñùëõ).