16,765,457
789
35,156
928,964
21,011,934
518
24,015
638,530
18,096,758
664
25,917
830,129
29,138,314
11.10
16.47
17.48
22.27
15.63
21.74
23.69
27.91
10.25
14.85
16.28
24.04
13.15
16.03
21.17
38.70
0
0
0
0
0
0
0
0
0
0
0
4
0
1
4
17
Number of Adversarial Inputs. One can count precisely what
fraction of inputs, when drawn uniformly at random from a con-
strained input space, are misclassified for a given model. For demon-
strating this, we first train 4 BNNs on the MNIST dataset, one
using each of the architectures ARCH1-ARCH4. We encode the
Property P1 (Section 3) corresponding to perturbation bound k ∈
{2, 3, 4, 5}. We take 30 randomly sampled images from the test set,
and for each one, we encoded one property constraining adversar-
ial perturbation to each possible value of k. This results in a total
of 480 formulae on which NPAQ runs with a timeout of 24 hours
per formula. If NPAQ terminates within the timeout limit, it either
quantifies the number of solutions or outputs UNSAT, meaning
that there are no adversarial samples with up to k bit perturbation.
Table 4 shows the average number of adversarial samples and their
PS(adv), i.e., percentage of count to the total input space.
As expected, the number of adversarial inputs increases with k.
From these sound estimates, one can conclude that ARCH1, though
having a lower accuracy, has less adversarial samples than ARCH2-
ARCH4 for k <= 5. ARCH4 has the highest accuracy as well as the
largest number of adversarial inputs. Another observation one can
Arch
ARCH1
ARCH2
ARCH3
ARCH4
#(Adv)
(Epoch = 0)
561
789
518
664
Defense 1
Defense 2
Epoch = 1
Epoch = 5
Epoch = 1
ACCb
82.23
79.55
84.12
88.15
#(Adv) ACCb
84.04
77.10
85.23
88.31
942
1,063
639
607
#(Adv) ACCb
82.61
81.76
82.97
88.85
776
1,249
431
890
#(Adv) ACCb
81.88
78.73
82.94
85.75
615
664
961
549
Epoch = 5
#(Adv)
960
932
804
619
make is how sensitive the model is to the perturbation size. For
example, PS(adv) for ARCH3 varies from 10.25 − 24.04%.
Effectiveness of Adversarial Training. As a second example of
a usage scenario, NPAQ can be used to measure how much a model
improves its robustness after applying certain adversarial training
defenses. In particular, prior work has claimed that plain (unhard-
ened) BNNs are possibly more robust than hardened models—one
can quantitatively verify such claims [41]. Of the many proposed ad-
versarial defenses [41, 50, 67, 78, 107], we select two representative
defenses [41], though our methods are agnostic to how the models
are obtained. We use a fast gradient sign method [50] to generate
adversarial inputs with up to k = 2 bits perturbation for both. In
defense1, we first generate the adversarial inputs given the train-
ing set and then retrain the original models with the pre-generated
adversarial inputs and training set together. In defense2 [41], alter-
natively, we craft the adversarial inputs while retraining the models.
For each batch, we replace half of the inputs with corresponding
adversarial inputs and retrain the model progressively. We evaluate
the effectiveness of these two defenses on the same images used to
quantify the robustness of the previous (unhardened) BNNs. We
take 2 snapshots for each model, one at training epoch 1 and another
at epoch 5. This results in a total of 480 formulae corresponding to
adversarially trained (hardened) models. Table 5 shows the number
of adversarial samples and PS(adv).
Observing the sound estimates from NPAQ, one can confirm that
plain BNNs are more robust than the hardened BNNs for 11/16
models, as suggested in prior work. Further, the security analyst
can compare the two defenses. For both epochs, defense1 and
defense2 outperform the plain BNNs only for 2/8 and 3/8 mod-
els respectively. Hence, there is no significant difference between
defense1 and defense2 for the models we trained. One can use
NPAQ estimates to select a model that has high accuracy on the
benign samples as well as less adversarial samples. For example, the
Table 6. Effectiveness of trojan attacks. TC represents the
target class for the attack. Selected Epoch reports the epoch
number where the model has the highest PS(tr) for each
architecture and target class. x represents a timeout.
Arch
ARCH1
ARCH2
ARCH3
ARCH4
TC
0
1
4
5
9
0
1
4
5
9
0
1
4
5
9
0
1
4
5
9
Epoch 1
PS(tr) ACCt
50.75
39.06
43.49
42.97
66.80
9.77
27.73
58.35
53.67
2.29
27.98
1.51
30.37
2.34
38.54
1.07
26.66
28.91
36.39
0.15
18.36
26.91
15.23
4.79
33.89
7.81
63.11
26.56
26.51
6.84
10.40
x
x
8.57
9.95
x
8.83
19.64
19.92
x
Epoch 10
PS(tr) ACCt
72.90
13.67
74.20
70.31
83.18
19.14
25.78
53.30
61.85
12.11
48.30
1.46
40.57
13.28
27.41
0.21
50.24
12.70
41.81
0.38
25.00
71.85
50.57
34.38
67.30
11.33
71.92
19.92
29.12
3.32
36.89
3.32
x
54.39
62.46
1.44
8.44
13.67
7.03
58.39
Epoch 30
PS(tr) ACCt
68.47
5.76
67.63
42.97
69.99
2.69
7.42
39.77
77.70
0.19
59.36
9.38
51.40
8.59
37.45
0.59
54.90
9.38
42.99
0.44
8.40
76.30
60.33
21.48
62.77
4.79
79.23
18.75
46.51
1.15
60.14
4.88
0.87
78.10
82.47
0.82
11.96
25.39
1.44
74.83
Selected
Epoch
1
10
10
1
10
30
10
1
1
30
10
10
10
1
1
30
30
10
30
10
ARCH4 model trained with defense2 at epoch 1 has the highest
accuracy (88.85%) and 549 adversarial samples.
6.3 Case Study 2: Quantifying Effectiveness of
Trojan Attacks
The effectiveness of trojan attacks is often evaluated on a chosen
test set, drawn from a particular distribution of images with em-
bedded trojan triggers [43, 68]. Given a trojaned model, one may
be interested in evaluating how effective is the trojaning outside
this particular test distribution [68]. Specifically, NPAQ can be used
to count how many images with a trojan trigger are classified to
the desired target label, over the space of all possible images. Prop-
erty P2 from Section 3 encodes this. We can then compare the NPAQ
count vs. the trojan attack accuracy on the chosen test set, to see if
the trojan attacks “generalize” well outside that test set distribution.
Note that space of all possible inputs is too large to enumerate.
As a representative of such analysis, we trained BNNs on the
MNIST dataset with a trojaning technique adapted from Liu et