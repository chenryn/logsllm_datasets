counts, each with over 100 tweets per user, we ﬁnd that roughly
16% of accounts tweeting at least one blacklisted link are career
spammers. To gauge the false negative rate of our classiﬁcation,
we manually inspect 99 accounts that passed both the χ2 and en-
tropy tests to determine a breakdown of the non-career spamming
accounts. Of the 99 samples, 35 are not categorized due to tweets
appearing in a foreign language and another 5 had been suspended,
prohibiting our access to the account’s tweet history and reducing
our sample size to 59 accounts. Of these, 5 were clearly career
spammers that had evaded detection, roughly 8.5% of accounts,
with an error bound of 7% at 95% conﬁdence.
To understand why the majority of spam accounts passed both
tests, we perform a second test to determine how many blacklisted
URLs an average account tweets. For each account in our sample
of 43,000, we selected 10% of URLs from the account’s history
and crawled them to determine the ﬁnal landing page. Using our
blacklists, we identiﬁed 304,711 spam landing pages, roughly 26%
of URLs crawled. The majority of spam accounts tweeted only 2
spam messages, while the remainder of their tweets appeared to
be benign URLs and purely text tweets posted at random intervals.
Given the low number of spam URLs, we believe the vast majority
of accounts tweeting blacklisted URLs are not career spammers,
indicating a potential for compromised accounts.
4.3.2 Compromised spamming accounts
With the majority of spamming accounts passing both the χ2 and
entropy tests used to identify automated behavior, we are left with
two possibilities for non-career accounts. First, an account could
have been compromised by means of phishing, malware, or simple
password guessing, currently a major trend in Twitter [26]. As most
non-career accounts tweet a limited number of spam URLs, the
short lifetime of a compromise can result from Twitter detecting the
compromise and notifying the user involved, as occurs with phish-
ing attacks, or the user might identify suspicious activity within
their tweet timeline and takes defensive action. Alternatively, given
the limited number of spam URLs posted, an account’s owner may
have tweeted the URLs unintentionally, unaware that they were
spam. Given that we expect a non-career spammer to tweet 20
spam URLs, it is unlikely an account mistakenly posts spam so fre-
quently, leading us to believe accounts are in fact compromised.
Compromised accounts present spammers with an attractive
means of exploiting trust, using a victim’s credibility to push spam
out to followers. Furthermore, by taking control of a victim’s ac-
count, spammers are relieved of the effort of coercing users into
following spam accounts. For non-career accounts that tweet mal-
ware and phishing URLs, we have strong evidence indicating the
accounts involved are likely compromised users. In particular, we
identify two major schemes to steal accounts, including phishing
pages that purport to provide followers and the Koobface botnet
which spreads through URLs in tweets [5]. For accounts identi-
ﬁed as tweeting spam domains found in the URIBL and Joewein
blacklists, we have less direct evidence indicating accounts were
compromised, though there have been examples of such behavior
reported [26].
Using a fake account to act as a spam trap, we entered our ac-
count information into one of the most frequently spammed phish-
ing sites that was blacklisted by Google’s Safebrowsing blacklist.
Once phished, the account was used to further advertise the same
phishing scam in addition to other spam domains. By searching
for these spam URLs in our data set, we identiﬁed over 20,000 ac-
32mers, we analyze anomalous application usage to identify instances
of unauthorized third party access. For typical users, we expect
tweets to originate from an array of desktop and phone applications,
while spam tweets should appear from an independent application
controlled by spammers. To identify this anomaly, we measure the
frequency that an application is used to generate spam versus non-
spam tweets on a per account basis. On average, 22% of accounts
contain spam tweets that originate from applications that are never
used for non-spam tweets. This pattern of unauthorized third party
access further demonstrates that stolen Twitter accounts are being
compromised and abused by spammers.
5. SPAM CAMPAIGNS
To aid in the propagation of products and malware, spammers
manage multiple accounts in order to garner a wider audience,
withstand account suspension, and in general increase the volume
of messages sent. To understand the collusion of accounts towards
advertising a single spam website, we develop a technique that clus-
ters accounts into campaigns based on blacklisted landing pages
advertised by each account. We deﬁne a campaign as the set of ac-
counts that spam at least one blacklisted landing page in common.
While at least 80% of campaigns we identify consist of a single
account and landing page, we present an analysis of the remaining
campaigns including the number of websites hosting spam content
for the campaign and number of actors involved.
5.1 Clustering URLs into campaigns
To cluster accounts into campaigns, we ﬁrst deﬁne a campaign
as a binary feature vector c = {0, 1}n, where 1 indicates a land-
ing page is present in the campaign and n is the total number of
landing pages in our data set. When generating the feature vector
for a campaign, we intentionally consider the full URL of a landing
page and not its host name to allow for distinct campaigns that op-
erate within the same domain space, such as on free web hosting,
to remain separate.
Clustering begins by aggregating all of the blacklisted landing
pages posted by an account i and converting them into a campaign
ci, where each account is initially considered part of a unique cam-
paign. Campaigns are clustered if for distinct accounts i, j the in-
tersect ci ∩ cj (cid:54)= ∅, indicating at least one link is shared by both
accounts. The resulting clustered campaign c(i,j) = ci ∪ cj. This
process repeats until the intersection of all pairs of campaigns ci, cj
is empty. Once complete, clustering returns the set of landing pages
for each campaign as well as the accounts participating in each
campaign.
Due to our use of Twitter exclusively to identify campaigns, there
are a number of limitations worth noting. First, if an account par-
ticipates in multiple campaigns, the algorithm will automatically
group the campaigns into a single superset. This occurs when an
account is shared by two spammers, used for multiple campaigns
over time by a single spammer, or compromised by different ser-
vices. Alternatively, if each landing page advertised by a spammer
is unique to each account, our algorithm has no means of identify-
ing collusion and results in partitioning the campaign into multiple
disjoint subsets.
5.2 Clustering results
The results of running our clustering technique on the accounts
ﬂagged by each blacklist are shown in Table 3. If there were an
absence of accounts that tweet multiple scam pages, our cluster-
ing technique would return the maximum possible number of cam-
paigns, where each landing page is considered a separate campaign.
In practice this is not the case; we are able to identify multiple in-
Figure 3: Most frequently used applications per-account for com-
promised, spamming and a random sample of accounts. Ca-
reer spammers use different applications than compromised users,
which are closer to the random set.
counts that were affected, 86% of which passed our career spammer
test.
Further evidence that Twitter accounts are being compromised
comes from the spread of Koobface malware which hijacks a vic-
tim’s Twitter account and tweets on his behalf. During a concerted
effort to inﬁltrate the Koobface botnet, we constructed search
queries to ﬁnd compromised accounts on Twitter and monitored the
spread on Twitter during a month of collection. We identiﬁed 259
accounts that had tweeted a link leading to a landing page that at-
tempted to install Koobface malware, indicating that these accounts
had already been compromised by the botnet and were being used
to infect new hosts [21].
These two cases highlight that compromises are occurring on
Twitter with the explicit purpose of spreading phishing, malware,
and spam. With Twitter credentials being sold in the underground
market [14], evidence is mounting that Twitter accounts with large
followings are viewed as a commodity, giving access to a trusting
audience more likely to click on links, as indicated by our click-
through results.
4.3.3 Spam Tools
To understand how spammers are communicating with Twitter,
we analyze the most popular applications amongst spam accounts
used to post tweets. Using information embedded in each tweet, we
aggregate statistics on the most popular applications employed by
spammers, comparing these results to a random sample. Figure 3
shows that career spammer application usage is dominated by au-
tomation tools such as HootSuite3 and twitterfeed4 that allow users
to pre-schedule tweets at speciﬁc intervals. These tools are not ex-
clusive to spammers, as indicated by the presence in the random
sample, though typical users are far more likely to interface with
Twitter directly through the web. Interestingly, application usage
amongst compromised accounts and a random sample are similar,
supporting our claim that the majority of accounts that pass both
automation tests are regular Twitter accounts that have been com-
promised.
Given our belief the majority of accounts are non-career spam-
3http://hootsuite.com/
4http://twitterfeed.com/
33Cluster Statistic
Maximum possible
campaigns
Campaigns identiﬁed
Campaigns with more
than one account
Campaigns with more
than one page
Google
6,210
Joewein
3,435
URIBL
383,317
2,124
14.50%
1,204
59,987
20% 11.46%
13.09% 18.36% 27.18%
Table 3: Campaign statistics after clustering
Figure 4: Number of accounts colluding in campaigns
stances where spam advertised by a group of accounts span a num-
ber of distinct landing pages, and even domains.
Analyzing the membership of campaigns, we ﬁnd that at least
10% of campaigns consist of more than one account. The member-
ship breakdown of these campaigns is shown in Figure 4. Diver-
sity of landing pages within campaigns is slightly more frequent,
as shown in Figure 5, where the use of afﬁliate links and multiple
domains results in a greater volume of links that comprise a single
campaign. While the vast majority of accounts do not collude with
other Twitter members, there are a number of interesting campaigns
at the tail end of these distributions that clustering helps to identify.
5.2.1 Phishing for followers
A particularly interesting phishing campaign that appeared dur-
ing our monitoring period is websites purporting to provide victims
with followers if they revealed their account credentials. In prac-
tice, these accounts are then used in a pyramid scheme to attract
new victims and advertise other services.
Clustering returned a set of a 21,284 accounts that tweeted any
one of 1,210 URLs associated with the campaign. These URLs di-
rected to 12 different domains, while the full URL paths contained
afﬁliate information to keep track of active participants. To under-
stand spamming behavior within the campaign, we fractured users
into subcampaigns, where a subcampaign is a set of users that share
identical feature vectors, rather than the original criteria of sharing
at least one link in common. From the initial campaign, hundreds of
subcampaigns appear. Of the 12 distinct domains, each has a inde-
pendent subcampaign consisting of on average 1,400 participants,
accounting for roughly 80% of the original campaign members.
The remaining 20% of participants fall into multiple clusters due to
signing up for multiple follower services, accounting for why the
independent campaigns were initially merged.
Deﬁning features. This campaign makes up a signiﬁcant portion of
the tweets ﬂagged by the Google blacklist, and shows surprisingly
large user involvement and frequent tweeting. Using the χ2 and
entropy tests, we ﬁnd that a large fraction of the users, 88% in our
set, tweeting for this campaign are compromised users, adding to
Figure 5: Number of landing pages targeted by campaigns
the evidence that phished accounts are used to further promote the
phishing campaign. A deﬁning feature of tweets in this campaign
is the extensive use of hashtags, 73% of the tweets sent contained
a hashtag. Hash tags are frequently reused and typically denote
the subcampaign (such as #maisfollowers). For the URLs being
tweeted, most have a redirect chain consisting of a single hop, from
a shortened URL to the landing page, though afﬁliate tracking typ-
ically introduces a second hop (shortened URL -> afﬁliate link ->
landing page). In some cases, the landing page itself appears in
tweets. We have also observed that the phishing sites plainly ad-
vertise the service to get more followers.
5.2.2 Personalized mentions
Of the campaigns we identify as spam, one particular campaign
run by http://twitprize.com uses Twitter to draw in trafﬁc using
thousands of career accounts that exclusively generate spam telling
users they had won a prize. Clustering returns a set of 1,850 ac-
counts and 2,552 distinct afﬁliate URLs that were all shortened
with tinyurl. Spam within the campaign would target victims by
using mentions and crafting URLs to include the victim’s Twit-
ter account name to allow for personalized greetings. Promising
a prize, the spam page would take a victims address information,
require a survey, list multiple mailers to sign up for, and ﬁnally
request the user either sign up for a credit card or subscribe to a
service.
Deﬁning features. This campaign is dominated by tweet URLs
from tinyurl pointing to unique, victim-speciﬁc, landing pages at
http://twitprize.com with no intermediate redirects. Of the tweets
containing URLs in this campaign, 99% are a retweet or mention.
The heavy use of usernames in tweets is an interesting character-
istic, unique to this type of campaign. Unlike the previous phish-
ing campaign, we ﬁnd infrequent use of hashtags, with only 2% of
tweets containing a hashtag. The accounts that tweet URLs in this
campaign pass the entropy tests since each tweet contains a differ-
ent username and the links point to distinct twitprize URLs. Of the
accounts participating, 25% have since been suspended by Twitter.
5.2.3 Buying retweets
One of the primary challenges for spammers on Twitter is to gain
a massive following in order to increase the volume of users that
will see a spam tweet. To circumvent this challenge, a number
of services have appeared that sell access to followers. One such
service, retweet.it, purports to retweet a message 50 times to 2,500
Twitter followers for $5 or 300 times to 15,000 followers for $30.
The accounts used to retweet are other Twitter members (or bots)
who sign up for the retweet service, allowing their accounts to be
used to generate trafﬁc.
Deﬁning features. While the service itself does not appear to be a
0501001502000.90.920.940.960.981  Accounts in CampaignFraction of CampaignsGoogleJoeweinURIBL0501001502002500.80.850.90.951Landing Pages in CampaignFraction of Campaigns  GoogleJoeweinURIBL34scam, it has been employed by spammers. Using a unique feature
present in all retweet.it posts to generate a cluster, we identify 55
accounts that retweeted a spam post soliciting both malware and
scams. The χ2 test indicate that 84% of the accounts are career
spammers.
5.2.4 Distributing malware
Using clustering, we identiﬁed the largest campaign pushing
malware in our data set, consisting 113 accounts used to propagate
57 distinct malware URLs. The content of the sites include pro-
grams that bring satellite channels to a computer that are “100%
adware and spyware free” and an assortment of other scams. In ad-
dition to serving malware, some sites advertised by the campaign
were reported by Google’s blacklist for drive by downloads.
Deﬁning features. The top malware campaign is signiﬁcantly dif-
ferent than other campaigns, with a relatively small account base
and few tweets. The accounts that tweet links in this cluster tend
to be career spammers, indicating that the malware is not compro-
mising Twitter accounts in order to self propagate, a feature found
among Twitter phishing URLs. One difference from other cam-
paigns is this use of redirects to mask the landing page. Since
both Twitter and shortening services such as bit.ly use the Google
Safebrowsing API to ﬁlter links, if a bit.ly URL is to be placed in
tweets, the redirect chain must at least be two hops (bit.ly → inter-
mediate → malware landing site). Two hops is not enough though,
as the Safebrowsing list contains both sites that serve as well as
sites that redirect to malware, requiring at least an additional hop
to be used to mask it from initial ﬁltering.
5.2.5 Nested URL shortening
In addition to locating large campaigns, clustering helps to iden-
tify instances of URL compression where multiple links posted in
tweets all resolve to the same page. One such campaign consisted
of 14 accounts soliciting a ﬁnancial scam. While unremarkable
for its size, the campaign stands out for its use of multiple redi-
rector services, totaling 8 distinct shortening domains that appear
in tweets. In turn, each initial link triggers a long chain of nested
redirects that leads our crawler through is.gd → short.to → bit.ly
before ﬁnally resolving to the scam page. While the motivation for
nested redirects is unclear, it may be a result of spam ﬁltering done
on the part of shortening services. By nesting URLs, ﬁltering based
on domains or full URLs is rendered obsolete less the ﬁnal URL is
resolved, which we discuss further in Section 6
6. BLACKLIST PERFORMANCE