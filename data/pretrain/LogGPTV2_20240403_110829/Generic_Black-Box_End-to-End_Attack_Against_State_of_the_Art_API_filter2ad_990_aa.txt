title:Generic Black-Box End-to-End Attack Against State of the Art API
Call Based Malware Classifiers
author:Ishai Rosenberg and
Asaf Shabtai and
Lior Rokach and
Yuval Elovici
Generic Black-Box End-to-End Attack
Against State of the Art API Call Based
Malware Classiﬁers
Ishai Rosenberg(B), Asaf Shabtai, Lior Rokach, and Yuval Elovici
Software and Information Systems Engineering Department, Ben Gurion University,
Beersheba, Israel
PI:EMAIL
Abstract. In this paper, we present a black-box attack against API
call based machine learning malware classiﬁers, focusing on generating
adversarial sequences combining API calls and static features (e.g., print-
able strings) that will be misclassiﬁed by the classiﬁer without aﬀecting
the malware functionality. We show that this attack is eﬀective against
many classiﬁers due to the transferability principle between RNN vari-
ants, feed forward DNNs, and traditional machine learning classiﬁers
such as SVM. We also implement GADGET, a software framework to
convert any malware binary to a binary undetected by malware classi-
ﬁers, using the proposed attack, without access to the malware source
code.
Keywords: Adversarial attacks · Malware classiﬁcation
Deep neural networks · Dynamic analysis · Transferability
1 Introduction
Machine learning malware classiﬁers, in which the model is trained on features
extracted from the analyzed ﬁle, have two main advantages over current signa-
ture based/black list classiﬁers: (1) Automatically training the classiﬁer on new
malware samples saves time and expense, compared to manually analyzing new
malware variants. (2) Generalization to currently unseen and unsigned threats
is better when the classiﬁer is based on features and not on a ﬁngerprint of a
speciﬁc and exact ﬁle (e.g., a ﬁle’s hash).
Next generation anti-malware products, such as Cylance, CrowdStrike, and
Sophos, use machine and deep learning models instead of signatures and heuris-
tics. Those models can be evaded and in this paper, we demonstrate an evasive
end-to-end attack, generating a malware binary that can be executed while not
being detected by such machine learning malware classiﬁers.
Application programming interface (API) calls, often used to characterize
the behavior of a program, are a common input choice for a classiﬁer and used
by products such as SentinelOne. Since only the sequence of API calls gives each
c(cid:2) Springer Nature Switzerland AG 2018
M. Bailey et al. (Eds.): RAID 2018, LNCS 11050, pp. 490–510, 2018.
https://doi.org/10.1007/978-3-030-00470-5_23
End-to-End Attack Against API Call Based Malware Classiﬁers
491
API call its context and proper meaning, API call sequence based classiﬁers
provide state of the art detection performance [9].
Machine learning classiﬁers and algorithms are vulnerable to diﬀerent kinds of
attacks aimed at undermining the classiﬁer’s integrity, availability, etc. One such
attack is based on the generation of adversarial examples which are originally
correctly classiﬁed inputs that are perturbed (modiﬁed) so they (incorrectly)
get assigned a diﬀerent label. In this paper, we demonstrate an attack like this
on binary classiﬁers that are used to diﬀerentiate between malicious and benign
API call sequences. In our case, the adversarial example is a malicious API call
sequence, originally correctly classiﬁed, which is classiﬁed by the classiﬁer as
benign (a form of evasion attack) after the perturbation (which does not aﬀect
the malware functionality).
Generating adversarial examples for API sequences diﬀers from generating
adversarial examples for images [2], which is the main focus of the existing
research, in two respects: (1) API sequences consist of discrete symbols with
variable lengths, while images are represented as matrices with ﬁxed dimensions,
and the values of the matrices are continuous. (2) In adversarial API sequences
one must verify that the original functionality of the malware remains intact.
Attacks against RNN variants exist [7,12], but they are not practical attacks, in
that they don’t verify the functionality of the modiﬁed samples or handle API
call arguments and non-sequence features, etc. The diﬀerences from our attack
are speciﬁed in Sect. 2.
The contributions of our paper are as follows:
1. We implement a novel end-to-end black-box method to generate adversarial
examples for many state of the art machine learning malware classiﬁers. This
is the ﬁrst attack to be evaluated against RNN variants (like LSTM), feed
forward DNNs, and traditional machine learning classiﬁers (such as SVM).
We test our implementation on a large dataset of 500,000 malware and benign
samples.
2. Unlike previous papers that focus on images, we focus on the cyber security
domain. We implement GADGET, an evasion framework generating a new
malware binary with the perturbed features without access to the malware
source code that allows us to verify that the malicious functionality remains
intact.
3. Unlike previous papers, we extend our attack to bypass multi-feature (e.g.,
static and dynamic features) based malware classiﬁers, to ﬁt real world sce-
narios.
4. We focus on the principle of transferability in RNN variants. To the best of
our knowledge, this is the ﬁrst time it has been evaluated in the context of
RNNs and in the cyber security domain, proving that the proposed attack
is eﬀective against the largest number of classiﬁers ever reviewed in a single
study: RNN, LSTM, GRU, and their bidirectional and deep variants, and feed
forward DNN, 1D CNN, SVM, random forest, logistic regression, GBDT, etc.
492
I. Rosenberg et al.
2 Background and Related Work
Most black-box attacks rely on the concept of adversarial example transferability
[18]: Adversarial examples crafted against one model are also likely to be eﬀective
against other models, even when the models are trained on diﬀerent datasets.
This means that the adversary can train a surrogate model, which has decision
boundaries similar to the original model, and perform a white-box attack on
it. Adversarial examples that successfully fool the surrogate model are likely to
fool the original model as well [11]. A diﬀerent approach uses the conﬁdence
score of the targeted DNN to estimate its gradients directly instead of using
the surrogate model’s gradients to generate adversarial examples [3]. However,
attacker knowledge of conﬁdence scores (not required by our attack) is unlikely in
black-box scenarios. Decision based attack, which uses only the target classiﬁer’s
classes, without the conﬁdence score, result in lower attack eﬀectiveness and
higher overhead [17].
In mimicry attacks, an attacker is able to code a malicious exploit that mim-
ics the system calls’ trace of benign code, thus evading detection [21]. Several
methods were presented: (1) Disguise attacks - Causing benign system calls to
generate malicious behavior by modifying only the system calls’ parameters. (2)
No-op Attacks - Adding semantic no-ops - system calls with no eﬀect, or those
with an irrelevant eﬀect, e.g., opening a non-existent ﬁle. (3) Equivalence attack
- Using a diﬀerent system call sequence to achieve the same (malicious) eﬀect.
The search for adversarial examples can be formalized as a minimization
problem [18]:
argr min f(x + r) (cid:2)= f(x) s.t. x + r ∈ D
(1)
The input x, correctly classiﬁed by the classiﬁer f, is perturbed with r such that
the resulting adversarial example x + r remains in the input domain D, but is
assigned a diﬀerent label than x .
A substitute model was trained with inputs generated by augmenting the
initial set of representative inputs with their FGSM [4] perturbed variants, and
then the substitute model was used to craft adversarial samples [11]. This diﬀers
from our paper in that: 1) It deals only with convolutional neural networks, as
opposed to all state of the art classiﬁers, including RNN variants. 2) It deals with
images and doesn’t ﬁt the attack requirements of the cyber security domain, i.e.,
not harming the malware functionality. 3) No end-to-end framework to imple-
ment the attack in the cyber-security domain was presented.
A white-box evasion technique for an Android static analysis malware clas-
siﬁer was implemented using the gradients to ﬁnd the element whose addition
would cause the maximum change in the benign score, and add this feature to
the adversarial example [5]. In contrast to our work, this paper didn’t deal with
RNNs or dynamic features which are more challenging to add without harming
the malware functionality. This study also did not focus on a generic attack that
can aﬀect many types of classiﬁers, as we do. Finally, our black-box assumption
is more feasible than a white-box assumption. In Sect. 5.3 we created a black-box
variant of this attack.
End-to-End Attack Against API Call Based Malware Classiﬁers
493
API call uni-grams were used as static features, as well [6]. A generative
adversarial network (GAN) was trained to generate adversarial samples that
would be classiﬁed as benign by the discriminator which uses labels from the
black-box model. This attack doesn’t ﬁt sequence based malware classiﬁers
(LSTM, etc.). In addition, the paper does not present a end-to-end frame-
work which preserves the code’s functionality. Finally, GANs are known for their
unstable training process [1], making such an attack method hard to rely on.
A white-box adversarial example attack against RNNs, demonstrated against
LSTM architecture, for sentiment classiﬁcation of a movie reviews dataset was
shown in [12]. The adversary iterates over the movie review’s words x[i] in the
review and modiﬁes it as follows:
x[i] = arg min
z
||sign(x[i] − z) − sign(Jf (x)[i, f(x)])|| s.t. z ∈ D
where f(x) is the original model label for x, and Jf (x)[i, j] = ∂fj
∂xi (x). This
diﬀers from our paper in that: (1) We present a black-box attack, not a white-
box attack. (2) We implement a practical cyber domain attack. For instance,
we don’t modify existing API calls, because while such an attack is relevant for
reviews - it might damage a malware functionality which we wish to avoid. (3)
We deal with multiple-feature classiﬁers, as in real world malware classiﬁers. (4)
Our attack has better performance, as shown in Sect. 4.3.
(2)
Concurrently and independently from our work, a RNN GAN to generate
invalid APIs and insert them into the original API sequences was proposed
[7]. Gumbel-Softmax, a one-hot continuous distribution estimator, was used to
deliver gradient information between the generative RNN and the substitute
RNN. Null APIs were added, but while they were omitted to make the generated
adversarial sequence shorter, they remained in the gradient calculation of the
loss function. This decreases the attack eﬀectiveness compared to our method
(88% vs. 99.99% using our method, for an LSTM classiﬁer). In contrast, our
attack method doesn’t have this diﬀerence between the substitute model and
the black-box model, and our generated API sequences are shorter. This also
makes our adversarial example faster. Unlike [7], which only focused on LSTM
variants, we also show our attack’s eﬀectiveness against other RNN variants
such as GRUs and conventional RNNs, bidirectional and deep variants, and non-
RNN classiﬁers (including both feed forward networks and traditional machine
learning classiﬁers such as SVM), making it truly generic. Moreover, the usage
of Gumbel-Softmax approximation in [7] makes this attack limited to one-hot
encoded inputs, while in our attack, any word embedding can be used, making
it more generic. In addition, the stability issues associated with GAN training
[1], which might not converge for speciﬁc datasets, apply to the attack method
mentioned in [7] as well, making it hard to rely on. While such issues might not
be visible when using a small dataset (180 samples in [7]), they become more
apparent when using larger datasets like ours (500,000 samples). Finally, we
developed an end-to-end framework, generating a mimicry attack (Sect. 5). While
previous works inject arbitrary API call sequences that might harm the malware
functionality (e.g., by inserting the ExitProcess() API call in the middle of the
494
I. Rosenberg et al.
malware code), our attack modiﬁes the code such that the original functionality
of the malware is preserved (Sect. 5.1). Moreover, our approach works in real
world scenarios including hybrid classiﬁers/multiple feature types (Sect. 5.3) and
API arguments (Sect. 5.2), non of which is addressed by [7].
3 Methodology
3.1 Black-Box API Call Based Malware Classiﬁer
Our classiﬁer’s input is a sequence of API calls made by the inspected code. In
this section, it uses only the API call type and not its arguments or return value.
IDSs that verify the arguments tend to be much slower (4–10 times slower, in
[19]). One might claim that considering arguments would make our attack easier
to detect. This could be done, e.g., by looking for irregularities in the arguments
of the API calls (e.g., invalid ﬁle handles, etc.) or by considering only successful
API calls and ignoring failed APIs. In order to address this issue, we don’t use
null arguments that would fail the function. Instead, arguments that are valid
but do nothing, such as writing into a temporary ﬁle instead of an invalid ﬁle
handle, are used in our framework, as described in Sect. 5. We also discuss an
extension of our attack that handles API call arguments in Sect. 5.2.
Since API call sequences can be long (some samples in our dataset have
millions of API calls), it is impossible to train on the entire sequence at once
due to GPU memory and training time constraints. Thus, we used a sliding
window approach: Each API call sequence is divided into windows with size m.
Detection is performed on each window in turn, and if any window is classiﬁed as
malicious, the entire sequence is malicious. This method helps detect cases like
malicious payloads injected into goodware (e.g., using Metasploit), where only
a small subset of the sequence is malicious. We use one-hot encoding for each
API call type in order to cope with the limitations of sklearn’s implementation
of decision trees and random forests1. The output of each classiﬁer is binary
(is the inspected code malicious or not). The tested classiﬁers and their hyper
parameters are described in Sect. 4.2.
3.2 Black-Box API Call Based Malware Classiﬁer Attack
The proposed attack has two phases: (1) creating a surrogate model using the tar-
get classiﬁer as a black-box model, and (2) generating adversarial examples with
white-box access to the surrogate model and using them against the attacked
black-box model, by the transferability property.
1 For details, see: https://roamanalytics.com/2016/10/28/are-categorical-variables-
getting-lost-in-your-random-forests/.
End-to-End Attack Against API Call Based Malware Classiﬁers
495
Creating a Surrogate Model. We use Jacobian-based dataset augmentation,
an approach similar to [11]. The method is speciﬁed in Algorithm 1.
We query the black-box model with synthetic inputs selected by a Jacobian-
based heuristic to build a surrogate model ˆf, approximating the black-box model
f’s decision boundaries. While the adversary is unaware of the architecture of
the black-box model, we assume the basic features used (the recorded API call
types) are known to the attacker. In order to learn decision boundaries similar to
the black-box model while minimizing the number of black-box model queries,
the synthetic training inputs are based on prioritizing directions in which the
model’s output varies. This is done by evaluating the sign of the Jacobian matrix
dimension corresponding to the label assigned to input x by the black-box model,
sign(J ˆf (x)[f(x)]), as calculated by FGSM [4]. We use the Jacobian matrix of
the surrogate model, since we don’t have access to the Jacobian matrix of the
black-box model. The new synthetic data point x + sign(J ˆf (x)[f(x)]) is added
to the training set.
Algorithm 1. Surrogate Model Training
Input: f (black-box model), T (training epochs), X1(initial dataset),  (perturbation
factor)
Deﬁne architecture for the surrogate model A