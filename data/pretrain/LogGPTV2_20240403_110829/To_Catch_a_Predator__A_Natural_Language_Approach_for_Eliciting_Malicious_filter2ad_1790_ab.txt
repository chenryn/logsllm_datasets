ters (also excluding spaces).2 We chose the space char-
acter as a primary term delimiter due to its common oc-
currence in text-based protocols; however, the delimiter
could have easily been chosen automatically by identi-
fying the most frequent byte in all requests. The collec-
tion of all requests (and their constituent terms) form the
TF/IDF corpus.
Once TF/IDF training is complete we use an iterative
k-medoids algorithm, shown in Algorithm 1, to identify
similar requests. Upon completion, the classiﬁcation al-
gorithm produces a k (or less) partitioning over the set
of all requests. In an effort to rapidly classify online re-
quests in a memory-efﬁcient manner, we retain only the
medoids and dissolve all clusters. For our deployment,
we empirically choose k = 30, and then perform a triv-
ial cluster-collapsing algorithm: we iterate through the
k clusters and, for each cluster, calculate the mean and
standard deviation of the distance between the medoid
and the other members of the cluster. Once the k-means
and standard deviations are known, we collapse pairs of
clusters if the medoid requests are no more than one stan-
dard deviation apart.
4.1 Dynamic Response Generation
Since one of the goals of our method is to generate not
only valid but also dynamic responses to requests, we
employ natural language processing techniques (NLP) to
create models of protocols. These models, termed lan-
guage models, assign probabilities of occurrence to se-
quences of tokens based on a corpus of training data.
With natural languages such as English we might deﬁne
a token or, more accurately, a 1-gram as a string of char-
acters (i.e., a word) delimited by spaces or other punctu-
ation. However, given that we are not working with nat-
ural languages, we deﬁne a new set of delimiters for pro-
tocols. The 1-gram token in our model adheres to one of
the following criteria: (1) one or more spaces, (2) one or
more printable characters, (3) one or more non-printable
characters, or (4) the beginning of message (BOM) or end
of message (EOM) tokens.
The training corpora we use contain both requests and
responses. Adhering to our assumption that similar re-
quests have similar responses, we train k response lan-
guage models on the responses associated with each of
the k request clusters. That is, each cluster’s response
language model is trained on the packets seen in response
to the requests in that cluster. Recall that to avoid having
to keep every request cluster in memory, we keep only
the medoids for each cluster. Then, for each (request, re-
sponse) tuple, we recalculate the distance to each of the
k request medoids. The medoid with the minimal dis-
tance to the tuple’s request identiﬁes which of the k lan-
guage models is trained using the response. After train-
Online requestOnline ClassiﬁcationDynamic Response GenerationEncode online requestCompute TF-IDF distance between request and k medoidsAssociate request with medoid yielding lowest TF-IDF distanceGenerate response using  language-model associated with selected medoid  Satisfy contextual dependencies if requiredTransmit responseGenerated response123456(from malware or spider)for all R ∈ (ObservedRequests − M edoidSet) do
for all M ∈ M edoidSet do
Distance ← T F/IDF (R, M)
if RequestM ap[R] = ⊥ or Distance ← ⊥ // for mapping requests to medoids
3: repeat
4:
5:
6:
7:
8:
9:
10:
11: until HasConverged(M edoidSet)
12: for all (Mi, Mj) ∈ M edoidSet s.t. i 6= j do
if T F/IDF (Mi, Mj) < F indT hresholdDistance(Mi, Mj) then
13:
M edoidSet ← M edoidSet − {Mi, Mj}
14:
M edoidSet ← M edoidSet ∪ M erge(Mi, Mj, RequestM ap)
15:
M ← F indM emberW ithLowestM eanDistance(M, RequestM ap)
RequestM ap[R] ← M
for all M ∈ M edoidSet do
Algorithm 1: Iterative k-Medoids Classiﬁcation for Observed Requests
ing concludes, each of the k response language models
has a probability of occurrence associated with each ob-
served sequence of 1-grams. A sequence of two 1-grams
is called a 2-gram, a sequence of three 1-grams is called a
3-gram, and so on. We cut the maximum n-gram length,
n, to eight.
Since it is unlikely that we have witnessed every pos-
sible n-gram during training, we use a technique called
smoothing to lend probability to unobserved sequences.
Speciﬁcally, we use parametric Witten-Bell back-off
smoothing [30], which is the state of the art for n-gram
models. This smoothing method estimates, if we con-
sider 3-grams, the 3-gram probability by interpolating
between the naive count ratio C(w1w2w3)/C(w1w2)
and a recursively smoothed probability estimate of the 2-
gram probability P (w3|w2). The recursively smoothed
probabilities are less vulnerable to low counts because of
the shorter context. A 2-gram is more likely to occur in
the training data than a 3-gram and the trend progresses
similarly as the n-gram length decreases. By smoothing,
we get a reasonable estimate of the probability of occur-
rence for all possible n-grams even if we have never seen
it during training. Smoothing also mitigates the possibil-
ity that certain n-grams dominate in small training cor-
pora. It is important to note that during generation, we
only consider the states seen in training.
To perform the response generation, we use the lan-
guage models to deﬁne a Markov model. This Markov
model can be thought of as a large ﬁnite state machine
where each transition occurs based on a transition prob-
ability rather than an input. As well, each “next state” is
conditioned solely on the previous state. The transition
probability is derived directly from the language models.
The transition probability from a 1-gram, w1 to a 2-gram,
w1w2 is P (w2|w1), and so on. Intuitively, generation is
accomplished by conducting a probabilistic simulation
from the start state (i.e., BOM) to the end state (i.e., EOM).
More speciﬁcally, to generate a response, we perform
a random walk on the Markov model corresponding to
the identiﬁed request cluster. From the BOM state, we
randomly choose among the possible next states with the
probabilities present in the language model. For instance,
if the letters (B,C,D) can follow A with probabilities
(70%, 20%, 10%) respectively, then we will choose the
AB path approximately 70% of the time and similarly
for AC and AD. We use this random walk to create re-
sponses similar to those seen in training not only in syn-
tax but also in frequency. Ideally, we would produce the
same types of responses with the same frequency as those
seen during training, but the probabilities used are at the
1-gram level and not the response packet level.
The Markov models used to generate responses at-
tempt to generate valid responses based on the training
data. However, because the training is over the entire
set of responses corresponding to a cluster, we cannot
recognize contextual dependencies between requests and
responses. Protocols will often have session identiﬁers
or tokens that necessarily need to be mirrored between
request and response. DNS, for instance, has a two byte
session identiﬁer in the request that needs to appear in
any valid response. As well, the DNS name or IP re-
quested also needs to appear in the response. While the
NLP engine will recognize that some session identiﬁer
and domain name should occupy the correct positions in
the response, it is unlikely that the correct session iden-
tiﬁer and name will be chosen. For this reason, we au-
tomatically post-process the NLP generated response to
appropriately satisfy contextual dependencies.
Figure 3: A sliding window template traverses request
tokens to identify variable-length tokens that should be
reproduced in related responses.
4.1.1 Detecting Contextual Dependencies
Generally speaking, protocols have two classes of con-
textual dependencies: invariable length tokens and vari-
able length tokens. Invariable length tokens are, as the
name implies, tokens that always contain the same num-
ber of bytes. For the most part, protocols with vari-
able length tokens typically adhere to one of two stan-
dards: tokens preceded by a length ﬁeld and tokens sep-
arated using a special byte delimiter. Overwhelmingly,
protocols use length-preceded tokens (DNS, Samba,
Netbios, NFS, etc.). The other less-common type
(as in HTTP) employ variable length delimited tokens.
Our method for handling each of these token types dif-
fers only slightly from techniques employed by other ac-
tive responder and protocol disassembly techniques ([8,
9]). Speciﬁcally, we identify contextual dependencies
using two techniques. First, we apply the Needleman-
Wunsch string alignment algorithm [19] to align requests
with their associated responses during training. Since the
language models we use are not well suited for this par-
ticular task, this process is used to identify if, and where,
substrings from a request also appear in its response. If
certain bytes or sequences of bytes match over an em-
pirically derived threshold (80% in our case), these bytes
are considered invariable length tokens and the byte po-
sitions are copied from request to response after the NLP
generation phase.
To identify variable length tokens, we make the sim-
plifying assumption that these types of tokens are pre-
ceded by a length identiﬁer; we do so primarily because
we are unaware of any protocols that contain contex-
tual dependencies between request and response through
character-delimited variable length tokens. As depicted
in Figure 3, we iterate over each request and consider
each set of up to four bytes as a length identiﬁer if and
only if the token that follows it belongs to a certain char-
acter class3 for the described length.
In our example,
T okeni is identiﬁed as a candidate length-ﬁeld based
upon its value. Since the next immediate token is of the
Figure 4: Frequency of dominant type/class per request
cluster (with k = 30), sorted from least to most accurate.
length described by T okeni (i.e., 8), T okenj is identi-
ﬁed as a variable length token. For each variable length
token discovered, we search for the same token in the
observed response. We copy these tokens after NLP gen-
eration if and only if this matching behavior was com-
mon to more than half of the request and response pairs
observed throughout training.
As an aside, the content-length header ﬁeld in our
HTTP responses also needs to accurately reﬂect the num-
ber of bytes contained in each response.
If the value
of this ﬁeld is greater than the number of bytes in a
response, the recipient will poll for more data, causing
transactions to stall indeﬁnitely. Similarly, if the value of
the content-length ﬁeld is less than the number of bytes
in the response, the recipient will prematurely halt and
truncate additional data. While other approaches have
been suggested for automatically inferring ﬁelds of this
type, we simply post-process the generated HTTP re-
sponse and automatically set the content-length value to
be the number of bytes after the end-of-header character.
5 Validation
In order to assess the correctness of our dynamic re-
sponse generation techniques, we validate our overall ap-
proach in the context of DNS. Again, we reiterate that
our choice for using DNS in this case is because it is
a rigid binary protocol, and if we can correctly gener-
ate dynamic responses for this protocol, we believe it
aptly demonstrates the strength (and soundness) of our
approach. For our subsequent evaluation, we train our
DNS responder off a week’s worth of raw network traces
collected from a public wireless network used by approx-
imately 50 clients. The trafﬁc was automatically par-
08scenario....XX..Sliding Length-Identiﬁer Templatetokenitokenjtokenktokenllength-match 0 0.2 0.4 0.6 0.8 1 0 5 10 15 20 25 30Fraction of OccurrenceCluster numberDominant ClassDominant Typetitioned into request and response tuples as outlined in
Section 4.
quire little or no manual intervention. In what follows,
we further substantiate the utility of our approach in the
context of a web-based honeypot.
Figure 5: Frequency of dominant type/class per response
cluster (with k = 30), sorted from least to most accurate.
Figure 6: Faithful, valid, or erroneous response suc-
cess rate for 20,000 random DNS requests under different
numbers of training ﬂows.
To validate the output of our clustering technique,
we consider clustering of requests successful if for each
cluster, one type of request (A, MX, NS, etc.) and the class
of the request (IN) emerges as the most dominant mem-
ber of the cluster; a cluster with one type and one class
appearing more frequently than any other is likely to cor-
rectly classify an incoming request and, in turn, generate
a response to the correct query type and class. We report
results based on using 10,000 randomly selected ﬂows
for training. As Figures 4 and 5 show, nearly all clusters
have a dominating type and class.
To demonstrate our response generation’s success rate,
we performed 20,000 DNS requests on randomly gen-
erated domain names (of varying length). We used
the UNIX command host4 to request several types of
records. For validation purposes, we consider a response
as strictly faithful if it is correctly interpreted by the re-
questing program with no warnings or errors. Likewise,
we consider a response as valid if it processes correctly
with or without warnings or errors. The results are shown
in Figure 6 for various training ﬂow sizes. Notice that
we achieve a high success rate with as little as 5,000
ﬂows, with correctness ranging between 89% and 92%
for strictly faithful responses, and over 98% accuracy in
the case of valid responses.
In summary, this demonstrates that the overall design
depicted in Figures 1 and 2—that embodies our train-
ing phase, classiﬁcation phase, model generation, and
preposing phase to detect contextual dependencies and
correctly mirror the representative tokens in their correct
location(s)—produces faithful responses. More impor-
tantly, these responses are learned automatically, and re-
6 Evaluation
Our earlier assertion was that the exploitation of web-
apps now pose a serious threat to the Internet. In order
to gauge the extent to which this is true, we used our dy-
namic generation techniques to build a lightweight HTTP
responder — in the hope of snatching attack trafﬁc tar-
geted at web applications. These attackers query popular
search engines for strings that ﬁngerprint the vulnerable
software and isolate their targets.
Type
.PHP
.PL
.CGI
.HTML
.PHTML
Appearances
3165
29
49
15
2
Table 1: Query Types
With this in mind, we obtained a list of the 3,285 of
the most searched queries on Google by known botnets
attempting to exploit web applications.5 We then queried
Google for the top 20 results associated with each query.
Although there are several bot queries that are ambigu-
ous and are most likely not targeting a speciﬁc web ap-
plication, most of the queries were targeted. However,
automatically determining the number of different web
applications being attacked is infeasible, if not impossi-
ble. For this reason, we provide only the break down in
 0 0.2 0.4 0.6 0.8 1 0 5 10 15 20 25 30Fraction of OccurrenceCluster numberDominant ClassDominant TypeValidFaithful 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1AMXNSAMXNSAMXNS1,000 Flows5,000 Flows10,000 Flowsthe types of web applications being exploited (i.e., PHP,
Perl, CGI, etc.) in Table 1.
Nearly all of the bots searching for these queries ex-
ploit command injection vulnerabilities. The PHP vul-
nerabilities are most commonly exploited through re-
mote inclusion of a PHP script, while the Perl vul-
nerabilities, are usually exploited with UNIX delimiters
and commands. Since CGI/HTML/PHTML can house