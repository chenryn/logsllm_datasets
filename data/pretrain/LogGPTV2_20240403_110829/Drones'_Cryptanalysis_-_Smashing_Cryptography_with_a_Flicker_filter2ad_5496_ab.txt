2) Results: Figure 4a presents the RSSI measurements
from transmissions sent from the drone, smartwatch, and
smartphone as they were captured by the laptop using external
interception. As can be seen from the results,
the RSSI
measurements and patterns are similar for the smartphone,
smartwatch, and drone. This experiment proves that relying
on moving object detection methods as a means of classifying
an FPV channel using RSSI analysis requires an additional
stage to ﬁlter out moving IoT devices that are not drones.
B. Detecting Video Stream & Extracting its Quality
In this subsection, we present a new method for classifying
an intercepted transmission as a video stream that can extract
details about the video stream’s quality (FPS and resolution).
1) Experimental Setup: We conducted the following ex-
periment using the Bebop Parrot 2 which supports three FPV
transmission rates (24, 25, and 30 FPS). We positioned the
drone on the ground and used its application to change the
FPS rate every two minutes (from 24 FPS to 25 FPS and then
from 25 FPS to 30 FPS). We intercepted the trafﬁc that was
sent from the drone and created the intercepted bitrate signal
(as described in Section V).
2) Results: As can be seen from the spectrogram extracted
from the intercepted bitrate signal (presented in Figure 4b),
the power around each of the FPV transmission frequencies
(FPSs) outperforms any other frequency. Video streams can
be detected by comparing the frequency with the strongest
magnitude of the intercepted bitrate signal to known FPS rates
used by video streams. By detecting the FPS of a captured
video stream, we can also use the intercepted bitrate signal to
infer the resolution of the video stream, and ﬁnd the resolution
for the H-264 standard published in [60], [61], [62].
C. Classifying FPV Channels
Algorithm 2 presents a method for classifying FPV channels
based on the observations mentioned above. It receives a
suspicious intercepted network, and it classiﬁes the network
as an FPV channel if a connected MAC address was found
to be a moving object (line 5) that transmits trafﬁc at known
drone FPS video rates. (line 10). In prior research, methods
to classify an IoT device as a moving object based on RSSI
analysis have been applied to detect moving smartphones [63]
and smartwatches [64]. The distance between a moving radio
transmitter and a static receiver can be derived from RSSI
measurements, and this has been used for indoor localization
of smartphone users [63]. However, we are interested in
detecting moving objects, a task which is much simpler than
localizing objects. Therefore, we implemented an algorithm
for object detection suggested in a prior study that is based
on RSSI measurements obtained from the receiver [65].
frequency = 70
for (macAddress : network) do
//Detecting Moving Objects
if (isMovingObject(macAddress)) then
Algorithm 2 Classifying an FPV Channel
1: procedure ISFPVCHANNEL?(network,time)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
bitrate[] = extractBitrateSignal(macAddress)
fft [] = FFT(bitrateArray,frequency)
index = frequencyWithStrongestMagnitude(fft)
//Detecting video channel
if (index==24 || index==25 || index==30) then
return true
return false
1) Experimental Setup: We evaluate the performance of
Algorithm 2 given a device that was already found to be a
moving object; therefore, we are aiming to determine how
much time it takes to classify a moving object as a drone.
In order to accomplish this, in this experiment we intercepted
1000 seconds of trafﬁc (as described in Section V) from the
Bebop Parrot 2 and DJI Spark (500 seconds from each drone)
while they ﬂew in the air (at an altitude of 30 meters). We
also intercepted 1000 seconds of trafﬁc from moving IoT
devices as follows: 290 seconds from a robotic vacuum cleaner
(Roborock S50) as it was performing routine home cleaning,
290 seconds of trafﬁc from a smartwatch (LG G W150), and
420 seconds of trafﬁc from a smartphone (OnePlus 5). The
smartwatch was worn on the wrist of a person walking with
a smartphone in his pocket.
1402
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:43:45 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 5.
Experimental setup - drone
(framed in white) located in front of
a white board with four LED strips
(framed in red, green, purple, and black)
Fig. 6. inﬂuence of ﬂickering: (a) a bulb according
to a 3 Hz square wave, (b) six bursts from one
second of the intercepted bitrate signal of a drone
that streams a 3 Hz ﬂickering LED strip.
Fig. 7. A spectrogram (power spectral density) of the
intercepted bitrate signal of a drone located in front of
an LED strip that ﬂickers for one minute at frequencies of
0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5 Hz.
2) Results: We obtained the intercepted bitrate signals for
each of the devices and divided the intercepted signals into
smaller signals (each signal was ﬁve seconds long). This
process resulted in 200 intercepted bitrate signals obtained
from drones and 200 intercepted bitrate signals obtained from
other moving IoT devices. Figure 4c presents the results
(accuracy, TPR, and FPR) after applying Algorithm 2 on the
data with various interception windows (1-5 seconds). As can
be seen in Figure 4c, once a device has been identiﬁed as
a moving object
takes just four seconds to classify its
transmissions as an FPV channel. After four seconds, accuracy
of 0.99 and a true positive rate (i.e., drone detection rate) of
1.0 is obtained. The confusion matrices from this experiment
are presented in Table VI in Appendix XVII.
it
In the remainder of the paper, we assume that (1) a
suspicious transmission can be classiﬁed as an FPV channel
by applying Algorithm 2 on the intercepted bitrate signal
(extracted as described in Section V), and (2) the quality of
the FPV channel (FPS and resolution) can be extracted from
the intercepted bitrate signal.
VII. WATERMARKING FPV CHANNEL
In this section, we assess the inﬂuence of a periodic physical
stimulus which is applied to a target/victim that
is being
streamed by a drone, by analyzing the intercepted bitrate
signal. We consider the algorithm that controls the periodic
physical stimulus a watermarker (described in Algorithm 3).
1000
onOffDuration =
for (i = 0; i < N; i++) do
2∗f requency , N =
Algorithm 3 Physical Watermarking
1: procedure WATERMARKER(frequency,duration)
2:
3:
4:
5:
6:
7:
8:
turnOnPhysicalStimulus()
else turnOffPhysicalStimulus()
sleep(onOffDuration)
turnOffPhysicalStimulus()
if (i%2 == 0) then
duration
onOf f Duration
Algorithm 3, which runs from a computer/controller, con-
trols a device that creates a periodic stimulus (e.g., ﬂickering)
whose frequency can be programmed. The algorithm receives
two parameters: frequency (amount of stimuli per second) and
duration (in milliseconds). The algorithm creates a square
wave at the given frequency, and based on this, turns a physical
stimulus on and off for the speciﬁed duration.
1) Experimental Setup: We attached four LED strips, each
of which was connected to a microcontroller, to a white board
(as can be seen in Figure 5) and performed the following
experiment. We programmed the microcontroller that was
connected to the top LED strip (framed by black dots in Figure
5) so that it would ﬂicker at various frequencies (0.5, 1, 1.5, 2,
2.5, 3, 3.5, 4, 4.5 Hz) for one minute per frequency. We then
positioned a DJI Mavic Pro [66] consumer drone in front of
the board at a distance of 1.5 meters (as can be seen in Figure
5), intercepted the trafﬁc sent from the drone, and created the
intercepted bitrate signal (as described in Section V).
2) Results: Figure 6b presents one second from the inter-
cepted bitrate signal that was captured during the time that
the top LED strip ﬂickered at 3 Hz (see Figure 6a). As can
be seen from Figures 6a and 6b, a 3 Hz ﬂickering LED strip
creates a 6 Hz phenomena within the intercepted bitrate signal
by producing six bursts per second. Each time the LED strip
was turned on/off a larger amount of data was sent from
the drone which is expressed as a burst of bytes in the time
domain. This is due to the fact that a larger amount of P-frames
was required to encode the changing macroblocks (changing
pixels) compared to an unchanging video stream. Figure 7
presents a spectrogram that was produced from the intercepted
bitrate signal of the entire experiment. As can be seen,
frequencies of 1-9 Hz were inﬂuenced by this experiment. The
ﬂickering LED watermarks the frequency of the intercepted
bitrate array exactly at the point which is twice its ﬂickering
frequency. We concluded that the ﬂickering object’s frequency
can be detected using this method by analyzing FPV trafﬁc,
and moreover, that it can even be used as a means of detecting
whether the drone’s camera is being used to stream a ﬂickering
object when the channel is encrypted. However, since the
slowest FPS rate among the four drones supports just 24 FPS,
the maximum frequency of a ﬂicker that can be detected by
analyzing the intercepted bitrate signal is limited to a 6 Hz
ﬂickering rate that watermarks the 12 Hz frequency of the
intercepted bitrate array (Nyquist frequency). In the rest of
1403
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:43:45 UTC from IEEE Xplore.  Restrictions apply. 
the paper we measure the inﬂuence of a ﬂickering object on
the intercepted bitrate signal. We refer to the ratio between
the magnitude after a ﬂicker was triggered (noise) and the
magnitude before a ﬂicker was triggered (signal) around the
inﬂuenced frequency as the signal to noise ratio (SNR).
VIII. LOCATING DRONE IN SPACE
In this section we ﬁrst show how to calculate the distance
and angle between the watermarker and the drone. Then, we
leverage our ﬁndings to create a drone locating model and
evaluate its performance.
A. Detecting Drone’s Distance
1) Inﬂuence of Distance on SNR: Here we show the inﬂu-
ence of distance on a ﬁxed sized ﬂickering object.
Experimental Setup: We aimed a portable projector [67]
at the exterior wall of a building; the projector was used to
project a video of a ﬂicker (3.5 Hz) onto a speciﬁc portion of
the wall (a rectangle 2.5×2.5 meters in size). We ﬂew a DJI
Mavic Pro various distances (10m, 20m, .., 90m, 100m) from
the ﬂickering rectangle. As in real surveillance, we zoomed
the drone’s camera (2x) on the ﬂickering rectangle (that was
considered as the target in this experiment). A laptop was
placed near the projector to intercept the trafﬁc sent from the
drone during the experiment.
Results: Figure 8 presents the SNR as a function of distance.
As can be seen, using a rectangle of 2.5×2.5 meters leaves
a watermark with an SNR that is greater than one from a
distance of 50 meters. Since the amount of pixels that are
changed as a result of a ﬂickering object is greater from a
shorter distance, many more macroblocks are changed (as a
result of the ﬂickering) and the SNR is greater; in contrast,
greater distances cause the ﬂickering object to be smaller
and result in changing fewer macroblocks and a lower SNR.
However, the new DJI Mavic 2 Zoom supports 4x zoom,
which has twice the zoom capacity of the drone we used. The
DJI Mavic 2 Zoom can be detected from a greater distance,
because a ﬁxed size object that is captured by a drone with
2x zoom from a distance of 50 meters can be captured by a
drone with 4x zoom from a distance of 100 meters [68].
2) Extracting Drone’s Distance: We aimed to extract the
distance between the drone and the ﬂickering object. In order
to do so, we must ﬁrst
learn the effect of changing the
percentage of captured pixels on the trafﬁc.
Experimental Setup: We placed the DJI Mavic Pro (con-
ﬁgured to 24 FPS and 720p) in front of a laptop monitor
located 0.5 meters away. We conducted 11 experiments using
this setup, and in each experiment a ﬂickering rectangle (at
3Hz) of a different size was presented in the middle of the
monitor (10%, 20%,
..., 90%, 100%). In each experiment,
we intercepted trafﬁc (as described in Section V) sent from
the drone. We obtained the 11 intercepted bitrate signals and
applied FFT to each of them.
Results: As can be seen by the SNR that was computed from
the magnitudes around 6 Hz in the experiments (presented in
Figure 9), the SNR increases as a function of the percentage
of changing pixels. By increasing the size of the rectangle,
we increased the amount of macroblocks that were changed
between consecutive frames. Encoding a larger amount of
macroblocks increases the bitrate which improves the SNR.
Based on the results of the experiments, we compared the
performance of four regression methods (polynomial, linear,
exponential, and logarithmic) to predict
the percentage of
changing pixels given a speciﬁc magnitude.
Table III presents the residual sum of squares (RSS) and
coefﬁcient of determination (R2) of the percentage of changing
pixel prediction for each regression method. The function
of the polynomial regression that yielded the best prediction
result among the tested methods is presented in Equation 1:
ERROR OF DISTANCE PREDICTION BASED ON REGRESSION METHODS
TABLE III
Method
Polynomial Regression
Linear Regression
Exponential Regression
Logarithmic Regression
RSS
56
464
581
2523
R2
0.994
0.957
0.947
0.770
% Changing Pixels (SNR=s) = 1.12 − 3.14×10−7s4
+ 6.96×10−5s3 − 5.12×10−3s2 + 1.87×10−1s
(1)
By applying a physical stimulus using a square shaped
ﬂicker at a speciﬁc frequency, the interceptor can calculate the
height and width of the ﬂickering object (in terms of pixels)
in a frame (picture) by applying the following steps:
1) Determining the F P V resolution of the FPV channel
(as explained in Section VI).
2) Triggering a physical stimulus using a square ﬂickering
at a speciﬁc frequency (e.g., 3 Hz).
3) Calculating the percentage of changing pixels from the
intercepted bitrate signal using Equation 1.
4) Inferring the amount of changingpixels from the
F P V resolution.
5) Inferring the height and width (in terms of pixels) of
the ﬂickering object in a frame.
For a square ﬂickering object we conclude that the:
height (in pixels) = width (in pixels)
%ChangingP ixels(m) × F P V Resolution
(cid:2)
=
(2)
By calculating the height and width (in pixels) of a ﬂicker-
ing object (for which the real size is known), the interceptor
can infer the distance between the drone’s camera to the
ﬂickering object [69] from the intercepted FPV channel (for
which the resolution was also determined) using Equation 3:
(3)
Distance (mm) = f actor(p) × f actor(d)
f actor(p) is deﬁned as follows (Equation 4):
factor(p) = realObjectHeight(mm) × imageHeight(pixels)
objectHeight(pixels)
(4)
The parameters required to calculate f actor(p) have already
been calculated. f actor(d) is drone dependent and deﬁned as
follows (Equation 5):
factor(d) =
f (mm)
sensorHeight(mm)
(5)
1404
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:43:45 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 8. SNR - magnitudes around 7 Hz as
a function of the distance between a drone
and a ﬂickering object.
Fig. 9. SNR - magnitudes around 6 Hz
as a function of the percentage of changing
pixels.
Fig. 10. SNR - magnitudes around 7 Hz (SNR1)
and 6 Hz (SNR2) as a function of the angle at
the midpoint between two ﬂickering LED strips.
The parameters f (mm) and sensorHeight(mm) are pub-
lished online in the speciﬁcations for each of the drones [66],
[70], [71]. The sensorHeight(mm) for each drone is 1/2.3"
(11.0434783 millimeters). The lens’ length of each drone
varies between 24 and 35mm, so f actor(d) is in the range
of (Equation 6):
0.31 < factor(d) < 0.46)
(6)
Based on Equations 6 and 3, we can see that the distance
between the drone and the ﬂickering object varied in the range
of (Equation 7):
0.31 × f actor(p) < Distance (mm) < 0.46 × f actor(p)
(7)
For f actor(d) = 0.385, we obtain a maximum error of
0.075×f actor(p) for the distance estimation. If the exact type
of drone can be detected from the intercepted FPV channel
(e.g., according to a unique FPS rate), the computed distance
is accurate.
B. Detecting Drone’s Angle
Next, we aimed to investigate the effect of the angle between
the ﬂickering object and the drone.
1) Experimental Setup: Using the white board presented in
Figure 5, we programmed the microcontrollers of the LED
strip on the left to ﬂicker at 3 Hz and those of the LED strip
on the right to ﬂicker at 3.5 Hz simultaneously. We positioned
the drone at 17 different angles (10◦, 20◦, ..., 160◦, 170◦).
The distance between the drone and the middle of the strips
was the same for each of the 17 positions. We intercepted the
trafﬁc sent from the drone and created the intercepted bitrate
signal (as described in Section V).
2) Results: The SNR around the frequencies of 7 Hz
(referred to as SNR1, i.e., the SNR around the frequency that
is inﬂuenced by the left ﬂickering LED) and 6 Hz (referred to
as SNR2, i.e., the SNR around the frequency that is inﬂuenced
by the right ﬂickering LED) is presented in Figure 10. As can
be seen, the SNR at those frequencies behaves as a mirror
around 90◦(due to the fact that ﬂickering objects of the same
size have the same effect). However, the magnitude of the
LED strip that was far from the camera when the drone
was located diagonal to the white board decreases, since a
ﬂickering object that is farther away is smaller compared to a
ﬂickering object that is closer. The ratio between SNR2 and
SNR1 ( SN R2
SN R1) is also presented in Figure 10. As can be seen,
the ratio decreases as the angle increases. We compared the
performance of four regression methods (polynomial, linear,
exponential, and logarithmic) to predict the angle between
the drone and the middle of the two LED strips, based on
the ratio between SNR2 and SNR1. Table IV presents the
residual sum of squares (RSS) and coefﬁcient of determination
(R2) of angle prediction for each regression method. The
function obtained based on exponential regression is presented
in Equation 8:
ERROR OF ANGLE PREDICTION BASED ON REGRESSION METHODS
TABLE IV
Method
Exponential Regression
Polynomial Regression
Logarithmic Regression
Linear Regression
RSS
979
1062
1450
10011
R2
0.976
0.973
0.964
0.754
Angle(SN R1, SN R2) = 192.72 ∗ e
−0.71∗ SN R2
SN R1
(8)
C. Locating Drone’s Location
In Subsection VIII-A, we obtained a formula to detect
the distance r between a drone and a ﬂickering object. In
Subsection VIII-B, we obtained a formula to detect the angle
of a planner that spreads from a drone to the middle of two
parallel ﬂickering objects attached to a white board. Figure
11 leverages our ﬁndings for locating a drone in space using
a white board (framed in yellow) with two pairs of parallel
ﬂickering objects. As can be seen in the ﬁgure, the objects that
comprise the ﬁrst pair of parallel ﬂickering objects (marked
with red dots) are located at the top and bottom of a rectangle
board (framed in yellow) and spread a red planner with angle
φ along the x-axis. The objects comprising the second pair
of parallel ﬂickering objects (marked with green dots) are
located on the left and right sides of the same board (framed
in yellow), and they spread a green planner with angle φ
along the z-axis. We consider (r,θ,φ) spherical coordinates
that give the relative location of a drone from a rectangle
1405
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:43:45 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 11. Locating a drone based on four
ﬂickering LED strips that creates r, θ,
and φ.
Fig. 12. A board with
ﬂickering LED
four
strips
installed on a
balcony.
Fig. 13. Root mean square error (RMSE) results of the locating drone
experiment as a function of the amount of time that ﬂickering was applied.
board that contains two pairs of parallel ﬂickering LED strips.
The Cartesian coordinates (x,y,z) can be retrieved from the
spherical coordinates (r,θ,φ) using known formulas [72].
1) Experimental Setup: In order to evaluate the accuracy of
a mechanism for locating a spying drone in space according
to our formulas, we conducted the following experiment. The
white board presented in Figure 5, which has an LED strip
connected to a microcontroller on each edge, was attached
to a balcony located on the third ﬂoor of a building (21
meters from the ground) so that the side of the board with
the LED strips was facing outward, as can be seen in Figure
12. We ﬂew the DJI Mavic Pro drone between 30 different
locations at various altitudes and distances from the balcony
while the drone conducted a privacy invasion attack against
the organization (i.e., the drone’s video camera streamed the
balcony). The exact 30 locations, as measured by the DJI-
Go application (longitude, latitude, and altitude), are listed in
Table VII (Appendix XVIII) and marked by blue dots in Figure
14. Each of the four LED strips was programmed to ﬂicker
at a different frequency for 30 seconds. We intercepted the
drone’s FPV channel at each of the 30 locations and extracted
30 bitrate signals.
2) Results: Using the previously mentioned formulas, we
computed the spherical coordinates (r,θ,φ) for each of the
locations and computed the Cartesian coordinates (x,y,z) from
Fig. 14. Results of the locating a drone in space experiment when applying
a physical stimulus for two seconds.
the spherical coordinates according to [72]. Based on the
computed Cartesian coordinates, we calculated the GPS coor-
dinates (latitude, longitude) and altitude. Finally, we computed
the error between the actual
location and the predicated
location. Figure 13 presents the mean square error (RMSE)
results for the x, y, and z-axes as a function of the amount
of time the physical stimulus was applied. As can be seen,
the accuracy along each axis is improved from an average
error of 3.5 meters (by applying ﬂickering for two seconds)
to an average error of 1.2 meters (by applying ﬂickering for
30 seconds). The actual locations and predicted locations (by
applying two seconds of ﬂickering) are presented in Figure
14 and Table VII (Appendix XVIII). Considering the fact that
the measurements of the 30 real locations were obtained from
the drone’s GPS (using its application) and the known average
error of GPS devices (a 4.9 meter radius in the open sky [73]),
we can accurately locate a spying drone in space using four
ﬂickering LED strips and a single Wi-Fi receiver by applying
ﬂickering for two seconds.
IX. HIDING THE PHYSICAL STIMULUS
In this section, we investigate whether a physical stimulus
can be produced in such a way that it is undetectable to the
human eye. An undetectable physical stimulus should fulﬁll
the following three requirements: (1) it should be undetectable
by direct observation by the drone’s operator via the naked
eye, (2) it should be undetectable by indirect observation by
the drone’s operator via the controller screen, and (3) it should
watermark the FPV channel. One method that was considered
takes advantage of the eye’s limited ability to capture infrared
and UV frequencies. We tested the inﬂuence of using infrared
LEDs as a means of creating a physical stimulus. As can be
seen in Figure 15a, the drone’s camera is sensitive to infrared
frequencies and can capture them; therefore, this method does
not meet the second requirement. However, infrared ﬂickering
can be used in cases in which the watermarker is deployed
inside a house/car, and there is no need to hide the ﬂickering
from the drone’s operator in order to create invisible ﬂickering
that will not disturb the people in the house/car.
1406
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:43:45 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 15.
(a) A picture of an infrared LED projector captured by the DJI
Mavic, and (b) A spectrogram (power spectral density) of the intercepted
bitrate signal from an experiment in which a smart bulb ﬂickered between a
baseline color and ﬁve similar hues (as can be seen in Table V).
1) Experimental Setup: We decided to test another method
that takes advantage of a different limitation of the human eye:
its inability to distinguish between two almost identical hues
of the same color. In this experiment we aimed to determine
whether a physical stimulus that both ﬂickers between two
similar hues (with different RGB values) and is undetectable
to the human eye can be produced and leave a noticeable
(distinguishing) effect on the FPV channel.
YUV AND RGB VALUES USED IN OUR EXPERIMENTS
TABLE V
Luma (Δ)
Baseline
1
2
3
4
5
YUV
231,26,143
230,26,143
229,26,143
228,26,143
227,26,143
226,26,143
RGB
253,255,51
252,254,50
251,253,49
250,252,48
249,251,47
248,250,46
We conducted two experiments. In the ﬁrst experiment,
we picked a random RGB color (253,255,51) as the baseline
and transformed it to the YUV color space (231,26,143). We
created ﬁve new hues similar to the baseline color by reducing
the luma component (see Table V). We placed the DJI Mavic
Pro in front of, and .5 meters away from, a smart LED bulb
(Magic Blue) that provides the BLE protocol for controlling.
We programmed the Magic Blue to ﬂicker between two similar
hues as follows: For the ﬁrst minute, the Magic Blue was
set at the baseline color (231,26,143). For the second minute,
the Magic Blue was set to ﬂicker at 2.3 Hz between the
baseline color and the color that we created by reducing the
luma component by one (230,26,143). For the third minute,
the Magic Blue was set
the same frequency
between the baseline color and the color that we created
by reducing the luma component by two (229,26,143). This
pattern continued until the ﬂickering included the last color
that we created (226,26,143). In the second experiment, we
positioned the DJI Mavic Pro at various distances (3m, 6m,
10m, 15m, 20m, 25m, 30m) from the Magic Blue bulb that was
programmed to ﬂicker between two similar hues: (231,26,143)
and (226,26,143). In both experiments, we intercepted the
trafﬁc sent from the drone and extracted the intercepted bitrate
signal (as described in Section V).
to ﬂicker at
Fig. 16. (a) SNR as a function of the change in the luma component and (b)
SNR as a function of the change in the distance
.
2) Results: The hues, as they were captured by the drone’s
video camera in the ﬁrst experiment, are presented in Figure
15b. The ﬂickering cannot be detected by the human eye,
because human vision is not sensitive enough to detect such
subtle changes. We compared the magnitude of the intercepted
bitrate signal around 4.6 Hz during the entire experiment.
As can be seen in the spectrogram presented in Figure 15b
which was extracted from the intercepted trafﬁc, the power of
the magnitude around 4.6 Hz increases as much as the delta
between the baseline and the second ﬂickering color increases.
The SNR as a function of the delta is presented is Figure 15a.
Figure 15b shows the results of the second experiment. As
can be seen, the SNR is greater than one up to a distance of
15 meters, so this method is only effective for a range shorter
than the range of visible ﬂickering (up to 50 meters). Based on
this experiment we concluded that the physical stimulus can
be disguised in a way that watermarks the intercepted trafﬁc
without the awareness of the drone’s operator for much shorter
ranges. In Appendix XVI, we discuss a method for hiding the
physical stimulus.
X. INFLUENCE OF AMBIENT FACTORS
In this section we investigate the inﬂuence of ambient light
and wind on the intercepted watermarked signal.
A. Inﬂuence of Wind
The camera of a drone is installed on a stabilizer that is
designed to compensate for unwanted camera movement, so
the captured picture won’t be affected by movement resulting
from wind or maneuvering. We start by comparing the video
stream of an object obtained from the air and a stand.
We conducted two experiments. In the ﬁrst experiment we
positioned the drone on a 1.5 meter high stand on top of a
four story building and ﬁlmed the landscape for 10 minutes at
a frequency of 24 FPS. We repeated the same experiment with
a minor change; this time the drone ﬂew to an altitude of 1.5
meters (the same altitude as the stand), and the same landscape
was streamed for the same amount of time. As can be seen
from the results presented in Figure 17a, the wind mainly
affects the low frequencies (below 6 Hz), which are more
noisy compared to a static stream. In order to test whether
this observation is wind independent, we conducted another
experiment in which we positioned a fan behind a ﬂying drone.
We used the fan to produce 14 wind speeds. We measured the
1407
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:43:45 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 17. From left to right: (a) FFT graphs of streaming an object statically from a stand (orange) and air (blue), (b) four magnitudes extracted from external
interception to a drone at different wind speeds, (c) SNR as a function of the ambient light (measured in lux).
wind that hit the drone using a wind meter and observed the