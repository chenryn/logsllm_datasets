# Title: PhishPrint: Evading Phishing Detection Crawlers by Prior Profiling

## Authors
Bhupendra Acharya and Phani Vadrevu  
UNO Cyber Center, University of New Orleans

## Abstract
Security companies frequently employ web crawlers to detect phishing and other social engineering attack websites. We developed a novel, scalable, and cost-effective framework called PhishPrint to evaluate these web security crawlers against various cloaking attacks. Unlike traditional methods, PhishPrint does not use simulated phishing sites or blocklisting measurements. Instead, it uses benign web pages to profile the security crawlers.

We used PhishPrint to evaluate 23 security crawlers, including widely used services like Google Safe Browsing and Microsoft Outlook email scanners. Over a 70-day period, we identified several previously unknown cloaking weaknesses across the crawler ecosystem. Specifically, we found that all the crawlers' browsers either do not support advanced fingerprinting-related web APIs (such as the Canvas API) or lack sufficient fingerprint diversity, making them vulnerable to new fingerprinting-based cloaking attacks.

To confirm the practical impact of our findings, we deployed 20 evasive phishing web pages exploiting these weaknesses. Eighteen of these pages managed to remain undetected despite aggressive self-reporting to all crawlers. We validated the specificity of these attack vectors with 1,150 volunteers and 467,000 web users. We also proposed countermeasures for both crawling and reporting infrastructures. We have disclosed the identified weaknesses to all relevant entities through a comprehensive vulnerability disclosure process, resulting in some remedial actions and multiple vulnerability rewards.

## 1. Introduction
The web has seen a significant increase in social engineering attacks, such as phishing and malvertising. URL blocklisting services like Google's Safe Browsing (GSB) and Microsoft's SmartScreen serve as front-line defenses, protecting users from these attacks. Most web browsers, including Chrome, Firefox, Safari, and Samsung Internet, which together account for about 90% of the market share, use the GSB blocklist. GSB is deployed on approximately four billion devices worldwide and shows millions of browser warnings daily.

These blocklists are populated by web security crawlers that regularly scan web pages. However, attackers often use cloaking techniques to evade these crawlers. Despite their importance, security crawlers have been understudied. Recent research has begun to focus on evaluating the robustness of these crawlers against various cloaking attacks. A common methodology in these studies involves setting up multiple websites with different second-level domains (TLD+1s) as phishing sites, which are then hidden using various cloaking mechanisms.

In this research, we explored an alternative approach that avoids simulated phishing sites and blocklisting measurements. Instead, we created multiple token websites with benign content and selectively reported them to different crawlers to trigger their visits. We then collected forensic information such as IP addresses, HTTP headers, and browser fingerprints at a large scale. This allowed us to identify and compare multiple previously unstudied cloaking weaknesses across different crawlers. We also conducted small-scale phishing experiments to demonstrate the severity of these weaknesses.

## 2. System Description
PhishPrint consists of two modules: the Profiling Module and the Attack Module. The Profiling Module uses a large number of benign websites to collect and analyze sensitive profiling information from security crawlers, identifying any cloaking defense weaknesses. These weaknesses can be harnessed to devise cloaking attack vectors. The Attack Module verifies the efficacy of these attack vectors using an array of simulated phishing websites.

### 2.1. Profiling Module
The Profiling Module begins with the Token URL Generator, which periodically generates unique, never-before-seen URLs that are reported to various crawlers. These URLs point to a single Profiling Website hosted on a single TLD+1 domain. The Web Scan Requestor module reports these URLs to 23 different crawlers, including popular services like Google Safe Browsing and Microsoft Outlook email scanners.

The Profiling Website extracts browser fingerprints without requiring user interaction. We adapted the fingerprinting code from the AmIUnique project, which includes Canvas, JS-based Font, and WebGL fingerprints, as well as Navigator object properties. The extracted data, along with the client's IP address and HTTP request headers, is stored in a database.

### 2.2. Attack Module
The Attack Module contains an array of evasive simulated phishing websites that use the derived cloaking vectors. Some of these vectors rely on blocklists that need to be continuously updated. Therefore, the Profiling Module must continue to run during the operation of the Attack Module.

## 3. Profiling Security Crawlers
We set up PhishPrint to run on our university network and registered a .com domain for our Profiling Websites. Over a 10-week period beginning in January 2020, we reported 12 token URLs daily to each of the 23 crawlers, totaling 840 token URLs per crawler. We collected data for 77 days to allow for delayed crawls.

### 3.1. Analysis and Cloaking Vectors
We analyzed the collected data to identify crawler weaknesses and derive relevant cloaking vectors. The profiling data can be categorized into three areas: browser anomalies, network data, and advanced browser fingerprints.

#### 3.1.1. Browser Anomalies
We observed several anomalies in the client code used by crawlers, such as anomalous HTTP headers and the use of headless browsers. These anomalies can be exploited to create anomaly cloaking vectors, which work with high specificity.

#### 3.1.2. Network Data
We found that many crawlers use uncommon Autonomous Systems (AS) and limited IP addresses. These can be used to create AS and IP blocklists to aid in cloaking.

#### 3.1.3. Advanced Browser Fingerprints
We discovered that the entire crawler ecosystem lacks diversity in advanced browser fingerprints, such as those based on JS-based Font, Canvas, and WebGL. This points to the efficacy of a Fingerprint Blocklist to aid in cloaking.

## 4. Impact and Mitigations
We confirmed the practical impact of our findings by deploying 20 evasive phishing web pages, 18 of which remained undetected. We also proposed concrete mitigation measures for both crawling and reporting infrastructures. We have shared our findings with all relevant entities through a thorough vulnerability disclosure process, resulting in some remedial actions and multiple vulnerability rewards.

## 5. Conclusion
PhishPrint provides a novel, scalable, and cost-effective framework for evaluating web security crawlers against cloaking attacks. Our study identified several previously unknown weaknesses and provided actionable recommendations to improve the security of these crawlers. We are willing to share our PhishPrint codebase selectively with security researchers and industry members to enable further evaluation studies.

---

This revised version aims to make the text more coherent, clear, and professional, while maintaining the essential details and structure of the original document.