title:PhishPrint: Evading Phishing Detection Crawlers by Prior Profiling
author:Bhupendra Acharya and
Phani Vadrevu
PhishPrint: Evading Phishing Detection 
Crawlers by Prior Profiling
Bhupendra Acharya and Phani Vadrevu, 
UNO Cyber Center, University of New Orleans
https://www.usenix.org/conference/usenixsecurity21/presentation/acharya
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.Evading Phishing Detection Crawlers by Prior Proﬁling
PhishPrint:
Bhupendra Acharya
UNO Cyber Center
University of New Orleans
PI:EMAIL
Phani Vadrevu
UNO Cyber Center
University of New Orleans
PI:EMAIL
Abstract
Security companies often use web crawlers to detect
phishing and other social engineering attack websites. We
built a novel, scalable, low-cost framework named PhishPrint
to enable the evaluation of such web security crawlers against
multiple cloaking attacks. PhishPrint is unique in that it
completely avoids the use of any simulated phishing sites and
blocklisting measurements. Instead, it uses web pages with
benign content to proﬁle security crawlers.
We used PhishPrint to evaluate 23 security crawlers
including highly ubiquitous services such as Google Safe
Browsing and Microsoft Outlook e-mail scanners. Our
70-day evaluation found several previously unknown cloaking
weaknesses across the crawler ecosystem. In particular, we
show that all the crawlers’ browsers are either not supporting
advanced ﬁngerprinting related web APIs (such as Canvas
API) or are severely lacking in ﬁngerprint diversity thus
exposing them to new ﬁngerprinting-based cloaking attacks.
We conﬁrmed the practical impact of our ﬁndings by de-
ploying 20 evasive phishing web pages that exploit the found
weaknesses. 18 of the pages managed to survive indeﬁnitely
despite aggressive self-reporting of the pages to all crawlers.
We conﬁrmed the speciﬁcity of these attack vectors with
1150 volunteers as well as 467K web users. We also proposed
countermeasures that all crawlers should take up in terms
of both their crawling and reporting infrastructure. We have
relayed the found weaknesses to all entities through an elab-
orate vulnerability disclosure process that resulted in some
remedial actions as well as multiple vulnerability rewards.
1 Introduction
The web has been seeing an increasing amount of social
engineering attacks of late. Web-based social engineering
attacks such as phishing and malvertisements [45] have been
on the rise [14, 15, 33]. URL Blocklisting services such as
Google’s Safe Browsing (GSB) and Microsoft’s SmartScreen
have been working as front-line defenses in protecting the
users against these kinds of attacks. Most web browsers
lookup domain names in these blocklists before proceeding
to display the web pages to the users. For example, Chrome,
Firefox, Safari, and Samsung Internet web browsers which
together account for about 90% of the market share use the
GSB blocklist [3]. GSB is deployed in about four billion
devices worldwide and shows millions of browser warnings
every day protecting users from web attacks. Such blocklists
are populated with the help of web security crawlers that
regularly scout web-pages to evaluate them. However, in
order to evade these crawlers, miscreants employ many
cloaking techniques [23, 38, 39, 49, 52].
Despite such great importance, security crawlers have
been left understudied for a long time. Only recently,
researchers have begun to focus on evaluating the robustness
of these crawlers against various cloaking attacks [32, 37, 52].
One common design methodology that has emerged in
all these works is the use of phishing experiments. This
usually involves setting up multiple websites with different
second-level domains (TLD+1s) as phishing sites get blocked
at a TLD+1 level [5,6,12]. These sites are then used as unique
tokens for hosting simulated phishing pages that are hidden
by distinct candidate cloaking mechanisms. For example,
some pages might deliver phishing content to only certain
geo-locations or mobile user agents [37]. User interaction
detectors [52] and CAPTCHAs [32] have also been utilized
as cloaking mechanisms to hide mockup phishing content.
The key idea in this approach is to create disparate sets of
phishing token sites with different cloaking mechanisms
and then selectively self-report them to different crawlers.
Depending on which sets of websites get listed in the browser
blocklists, one can measure which crawlers can defend how
well against various cloaking attacks.
In this research, we explored an alternate approach that
completely avoids simulated phishing sites and blocklisting
measurements for evaluating security crawlers. Instead, we
create multiple token websites with benign content and
self-report them selectively to different crawlers to trigger
their visits. We then directly proﬁle the crawlers by collecting
forensic information such as their IP addresses, HTTP
headers and browser ﬁngerprints at a large scale. Lastly,
we conduct a quantitative analysis of this information to
identify and compare multiple previously unstudied cloaking
USENIX Association
30th USENIX Security Symposium    3775
weaknesses across different crawlers. We also conduct small
scale phishing experiments using phishing sites that are
powered by this analysis to demonstrate the gravity of the
weaknesses we found in the crawlers.
Since we avoid using phishing content in the token sites,
these sites and their associated TLD+1 domains do not get
blocked. As a result, we can use a single TLD+1 domain to
host a large number of token proﬁling sites as subdomains.
This allows our approach to be much more scalable than using
phishing experiments. For example, in our main proﬁling
experiment, we created and sent 18,532 token sites to 23 secu-
rity crawlers at the cost of registering a single domain name.
Each token site collects information about multiple cloaking
weaknesses simultaneously with this design further boosting
the scalability. Moreover, by using benign sites researchers
can avoid the difﬁcult process of seeking prior immunity
from hosting providers as there is no risk of whole account
takedown unlike in the case of phishing experiments where
this is essential [37]. We discuss the question of whether this
approach of using a single TLD+1 introduces bias in §6.
We implemented our approach by building PhishPrint, a
scalable security crawler evaluation framework that is made
up of two modules. The main module conducts large scale pro-
ﬁling experiments to help ﬁnd crawler weaknesses while the
other one aids in set up of small scale phishing experiments
to help demonstrate the seriousness of these weaknesses.
Using PhishPrint, we proﬁled 23 security crawlers over a
70-day period using 18,532 token sites resulting in 348,516
HTTP sessions. These 23 crawlers included those employed
by highly ubiquitous services such as Google Safe Browsing
and Microsoft Outlook e-mail scanners as well.
When building PhishPrint, we made use of code from a
popular browser ﬁngerprinting project [2] to help in gathering
the crawlers’ ﬁngerprints along with their HTTP headers.
When we analyzed the gathered data, we found several
interesting crawler weaknesses. For example, we found that
many crawlers carry “Crawler Artifacts” such as anomalous
HTTP headers implying the presence of browser automation.
We also saw that the Autonomous Systems (AS) used by a lot
of crawlers are very uncommon (for human users) and limited
in variety. Therefore, they can be embedded into an “AS
Blocklist” to help in cloaking. Similarly, we also found that
the IP addresses were limited in some cases leading to an
“IP Blocklist”. We also found that several crawlers do not
use “Real Browsers" as they fail to support the execution of
ﬁngerprinting code that is widely supported in the browsers
of most users. Finally, we found that the entire crawler
ecosystem is severely lacking in the diversity of their ad-
vanced browser ﬁngerprints (deﬁned here as a combination of
JS-based Font, Canvas and WebGL ﬁngerprints) thus pointing
to the efﬁcacy of a “Fingerprint Blocklist” to aid in cloaking.
To measure and conﬁrm the damage that these anomalies
and blocklists can do, we used them to power 20 evasive
phishing sites deployed in PhishPrint. Despite aggressive
self-reporting of all phishing sites to the crawlers, our
results showed that 90% of the sites can escape blocklisting
indeﬁnitely in stark contrast to the lifetime of a baseline
phishing site which was only about three hours.
We will describe PhishPrint in more detail in §2. All
experiments and their results are presented in §3 and §4,
while mitigations are covered in §5. We discuss vulnerability
disclosure, limitations, ethical considerations and future work
plans in §6 and related work in §7.
We make the following contributions with this paper:
1. Framework: We built a novel, scalable, low-cost frame-
work named PhishPrint to enable evaluation of web
security crawlers by completely avoiding the use of
blocklisting measurements and phishing sites.
2. Measurements: We deployed PhishPrint in a 70-day
longitudinal study to evaluate 23 crawlers individually
and more than 80 crawlers cumulatively. Analysis
of the data led us to new cloaking weaknesses and
attack vectors against the crawler ecosystem (Crawler
Artifacts and Real Browser Anomalies, AS and Fingerprint
Blocklists) which we conﬁrmed through user studies and
phishing experiments. We also devised a simple metric
named Cloaking Vector Defense Score to compare these
weaknesses in order to help in the prioritization of ﬁxes.
In order to address these weaknesses, we
suggested concrete mitigation measures in areas of
crawler and reporting infrastructures. We also performed a
thorough vulnerability disclosure with all crawler entities
eliciting their positive feedback and remedial actions.
We also state that we are willing to share our PhishPrint
codebase selectively with all security researchers and
concerned industry members to enable further evaluation
studies on the security crawler ecosystem.
2 System Description
3. Impact:
Figure 1: PhishPrint: System Overview
Our PhishPrint system is made up of two modules (see
Fig. 1). The main module is the Proﬁling Module which uses
a large number of benign websites 4 to collect and analyze
sensitive proﬁling information from security crawlers and
ﬁnd any cloaking defense weaknesses. These weaknesses
can then be harnessed to devise cloaking attack vectors. The
3776    30th USENIX Security Symposium
USENIX Association
Data AnalysisToken URLGeneratorWeb ScanRequestorWeb Security Crawlers*.phishp.com                                           Database*.phishp.com                                           Phishing Sites*.phishp.com                                           12Proﬁling Websites  FingerprintsHeadersURLsBFPs*.phishp.com                                           734568Proﬁling ModuleAttack Moduleefﬁcacy of these attack vectors can then be veriﬁed with the
Attack Module which uses an array of simulated phishing
websites 8 for this.
The working of the Proﬁling Module begins with the Token
URL Generator 1 whose job is to periodically generate
unique, never-before-seen URLs that will be given as tokens
to various crawlers. The URLs are also stored in a database
5 . Although each URL is unique, they all point to a single
Proﬁling Website 4 server that we maintain. As previously
discussed, we use unique subdomains of a single TLD+1
domain for generating these URLs. The mapping between the
token URLs and the web server instance was set up with the
help of wildcard DNS records and .htaccess rewrite rules.
The Web Scan Requestor 2 receives URLs periodically
from the Token URL Generator and reports them to different
crawlers 3 as potential “phishing URLs”. We went through
an elaborate process to ﬁnd a comprehensive list of security
crawlers that can be supported by the requestor module.
Firstly, we included crawlers such as Google Safe Browsing
(GSB) and Microsoft SmartScreen which power the URL
blocklists of most web browsers covering millions of users.
We also added support for crawlers such as PhishTank,
APWG, and ESET which along with GSB and SmartScreen
have all been studied in previous research [37]. Further,
we went through the list of URL scanning services hosted
by VirusTotal [11] and included 17 additional crawlers
that have a publicly accessible reporting interface. To our
knowledge, none of these have been studied previously.
We also tested various communication applications such as
e-mail clients and social media apps with token URLs to see
if we could ﬁnd evidence of any crawlers being employed by
these vendors. In this process, we discovered that Microsoft
employs a crawler to pre-scan all URLs received by its Ofﬁce
365 customers using the Outlook e-mail service [1]. Given
that Ofﬁce 365 is a hugely popular application with a current
subscriber base of more than 250 million people [10], we also
included it as a candidate to be evaluated bringing the total
list of crawlers to 231 (listed in 1st column of Table 1). The
Web Scan Requestor module is built to use different methods
such as Selenium-based browser automation code, emails
as well as direct web API calls in order to send periodic
phishing URL reports to the 23 crawlers.
After receiving the reports, the crawlers will visit the token
URLs. As already noted, all these URLs point to a single
web server hosting a Proﬁling Website 4 . We designed
this website to be able to extract browser ﬁngerprints of any
visitor without requiring any user interaction. For this, we
adapted (with permission) the ﬁngerprinting code developed
and used for the AmIUnique project [2, 29]. Apart from
serving as a reliable cross-browser compatible codebase,
their project also allowed us to be able to evaluate the
speciﬁcity of some attack vectors we derive on a large scale
as will be shown in later sections. The ﬁngerprints extracted
are: Canvas, JS-based Font and WebGL ﬁngerprints (code
in Appendix D) and Navigator object properties. The web
server will store all the extracted ﬁngerprints, the client’s IP
address and the HTTP request headers in the database.
After eliciting a suitable number of crawls over an extended
period of time, we can ascribe the collected ﬁngerprint
data and HTTP metadata to different crawlers by using the
token URLs as a common factor 5 . We can then conduct
a thorough analysis of the cloaking defenses of speciﬁc
crawlers 6 . This allows us to derive different cloaking attack
vectors 7 . The cloaking vectors can then be evaluated using
the Attack Module. The module contains an array of evasive
simulated phishing websites that are bootstrapped to use the
derived cloaking vectors. It is important to note that some
of these cloaking vectors rely on blocklists that need to stay
up-to-date in order to remain continually effective. As a
result, in order to use these kinds of cloaking vectors, the
Proﬁling Module needs to continue to run throughout the run
time of the Attack Module. More details about the setup of
the phishing websites are discussed in §4.
We used PHP, Python and JavaScript for building all the
above described modules with about 20K lines of code. In
the interest of making a real impact in improving the security
architecture of crawlers, we plan to release our code to vetted
academic researchers as well as concerned members of the
industry upon request. However, we are abstaining from
making a public release of our code in order to deter risks
of possible abuse of the system by malicious actors.
3 Proﬁling Security Crawlers
We setup PhishPrint to run on our University network. We
registered a domain under the .com TLD for our Proﬁling
Websites. As described in §2, our system was setup to collect
and analyze proﬁling data from 23 different crawlers (Ta-
ble 1). We ran the system for a period of 10 weeks beginning
in the 2nd week of January 2020. We collected the data for a
total period of 77 days (until the last week of March) in order
to allow sufﬁcient time for any delayed crawls that might be
initiated from some crawlers. During this period, PhishPrint
reported 12 token URLs as fake phishing reports daily to each
of the 23 crawlers (Ethical considerations are discussed in §6).
These reports were sent in two hour intervals of time through-
out to all the crawlers. As a result, we reported a total of 840
token URLs to most crawlers2 over the deployment period.
1We also discovered that some media applications such as Slack and
Facebook Messenger scan our token URLs. However, we do not consider
them as security crawlers as they were clearly identifying themselves with
the User-Agent headers akin to search engine bots.
2Forcepoint, FortiGuard and GSB are the only exceptions. Forcepoint has
a reporting limit of 5 URLs per day restricting us to 350 submitted URLs. Due
to intermittent technical issues on both server and client sides, we could only
report 777 and 612 URLs to FortiGuard and GSB respectively.
USENIX Association
30th USENIX Security Symposium    3777
3.1 Analysis and Cloaking Vectors
The above mentioned setup allowed us to collect sensitive
proﬁling data from multiple crawlers over the 10-week period.
We analyzed the data to ﬁnd crawler weaknesses and derive
relevant cloaking vectors. The proﬁling data we collected for
this project can be divided into these 3 categories: browser
anomalies, network data and advanced browser ﬁngerprints.
In this section, we will describe how the proﬁling data
from the 3 areas was analyzed and what cloaking vectors
were derived as a result. Before this, it is helpful to ﬁrst
establish some terminology relating to cloaking attack
vectors. Regardless of the type of proﬁling data being used,
cloaking vectors used by attackers trying to evade crawlers
can fall into one of two classes: Anomalies and Blocklists.
We will describe these two classes below:
Anomaly cloaking vectors capitalize on the characteristic
anomalous behaviors exhibited by crawlers when visiting can-
didate websites. These vectors can be created after ﬁnding any
anomalies in the requests being made by crawlers that strongly
indicate the fact that they are not from a potential human vic-
tim. For example, consider a HTTP request made by a crawler
with a headless browser’s User-Agent. Attackers can block
all such requests to avoid detection without blocking any po-
tential victims as no victim will use a headless browser. Thus,
by deﬁnition, all these vectors work with high speciﬁcity.
Blocklist cloaking vectors rely on some speciﬁc ﬁnger-
prints known to be associated with crawlers (such as from
PhishPrint’s proﬁling data) in order to create a blocklist for
the operation of cloaking websites. For example, if there are
a set of speciﬁc IP addresses that Google uses for its GSB
crawlers, they can then be made part of a blocklist attack
vector to evade GSB.
Blocklist vectors differ from anomaly vectors in two key
aspects. Firstly, many blocklists need continuous updating in
order to be effective. For example, if a crawler keeps changing
its IP addresses, then the corresponding blocklists need to be
updated by the attackers. This is not the case with anomaly
vectors which rely on some speciﬁc crawler idiosyncrasies
that are overlooked by the crawlers and hence remain ﬁxed.
Secondly, blocklists might block some potential victims. So,
their speciﬁcity needs to be taken into account by attackers
before using them. For example, if an attacker simply blocks
all /24 subnets of IP addresses seen from a crawler and if that
crawler was using a residential proxy to route its requests,
then such a blocklist could potentially cause a lot of false
positives for the attacker. On the other hand, anomaly vectors
are all very speciﬁc as already discussed.
We will now discuss the three areas of proﬁling data we
analyzed in our study along with the associated cloaking
vectors that we derived and their novelty aspects.
3.1.1 Browser Anomalies
The ﬁrst area of proﬁling data we consider focuses on how
closely the client code used by a crawler resembles that of
a real browser. We observed several anomalies in the web