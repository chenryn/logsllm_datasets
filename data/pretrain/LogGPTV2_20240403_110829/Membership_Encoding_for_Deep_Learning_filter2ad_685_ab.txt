is shown in Figure 3.
Compressing Bloomier ﬁlters When sending weight matrices over a network or saving them to
disk, it is not necessary to retain the ability to access weight values as they are being sent, so it is
advantageous to add another layer of compression for transmission. We use arithmetic coding, an
entropy-optimal stream code which exploits the distribution of values in the table (MacKay, 2005).
Because the nonzero entries in a Bloomier ﬁlter are, by design, uniformly distributed values in
[1, 2t − 1), improvements from this stage largely come from the prevalence of zero entries.
4 EXPERIMENTS
We evaluate Weightless on three deep neural networks commonly used to study compression: LeNet-
300-100, LeNet5 (LeCun et al., 1998), and VGG-16 (Simonyan & Zisserman, 2015). The LeNet
networks use MNIST (Lecun & Cortes) and VGG-16 uses ImageNet (Russakovsky et al., 2015). The
networks are trained and tested in Keras (Chollet, 2017). The Bloomier ﬁlter was implemented in
house and uses a Mersenne Twister pseudorandom number generator for uniform hash functions. To
reduce the cost of constructing the ﬁlters for VGG-16, we shard the non-zero weights into ten separate
ﬁlters, which are built in parallel to reduce construction time. Sharding does not signiﬁcantly affect
compression or false positive rate as long as the number of shards is small (Broder & Mitzenmacher,
2004).
We focus on applying Weightless to the largest layers in each model, as shown in Table 1. This
corresponds to the ﬁrst two fully-connected layers of LeNet-300-100 and VGG-16. For LeNet5, the
second convolutional layer and the ﬁrst fully-connected layer are the largest. These layers account
for 99.6%, 99.7%, and 86% of the weights in LeNet5, LeNet-300-100, and VGG-16, respectively.
5
Layer 0Frozen WeightsManipulated WeightsPrune andretrainClusterEncode with Bloomier ﬁlterFreeze and retrainLayer 1Prune andretrainClusterEncode with Bloomier ﬁlterFreeze and retrainSimpliﬁcationLossy Weight EncodingUnder review as a conference paper at ICLR 2018
Table 1: Baseline. Summary of accuracy, baseline parameters (sparsity and number of clusters), and
Weightless’ hyperparameter (t) setting used for each layer. *VGG-16 in MB (size) and top-1 (error).
Pruning Method Error % Layer Size (KB) Nonzero % Clusters
Baseline
Model
LeNet-300-100
LeNet5
Magnitude
DNS
Magnitude
DNS
VGG-16*
Magnitude
1.76
FC-0
FC-1
FC-0
2.03
FC-1
0.98 CNN-1
FC-0
0.96 CNN-1
FC-0
FC-0
FC-1
35.9
919
117
919
117
36
2304
98
1564
392
64
5.0
5.0
1.8
1.8
7.0
5.5
3.1
0.73
2.99
4.16
9
9
9
10
9
9
10
10
4
4
t
8
9
9
8
8
7
8
8
6
8
(The DNS version is slightly different than magnitude pruning, however the trend is the same.) While
other layers could be compressed, they offer diminishing returns.
Compression baseline The results below present both the absolute compression ratio and the im-
provements Weightless achieves relative to Deep Compression (Han et al., 2016), which represents
the current state-of-the-art. The absolute compression ratio is with respect the original standard
32-bit weight size of the models. This baseline is commonly used to report results in publications
and, while larger than many models used in practice, it provides the complete picture for readers to
draw their own conclusions. For comparison to a more aggressive baseline, we reimplemented Deep
Compression in Keras. Deep Compression implements a lossless optimization pipeline where pruned
and clustered weights are encoded using compressed sparse row encoding (CSR) and then compresses
CSR encoding tables with Huffman coding. The compression achieved by Deep Compression we use
as a baseline is notably better than the original publication (e.g., VGG-16 FC-0 went from 91× to
119×).
Error baseline Because Weightless performs lossy compression, it is important to bound the impact
of the loss. We establish this bound as the error of the trained network after the simpliﬁcation steps
(i.e., post pruning and clustering). In doing so, the test errors from compressing with Weightless and
Deep Compression are the same (shown as Baseline Error % in Table 1). Weightless is sometimes
slightly better due to training ﬂuctuations, but never worse. While Weightless does provide a tradeoff
between compression and model accuracy, we do not consider it here. Instead, we feel the strongest
case for this method is to compare against a lossless technique with iso-accuracy and note compression
ratio will only improve in any use case where degradation in model accuracy is tolerable.
4.1 SPARSE WEIGHT ENCODING
Given a simpliﬁed baseline model, we ﬁrst evaluate the how well Bloomier ﬁlters encode sparse
weights. Results for Bloomier encoding are presented in Table 2, and show that the ﬁlters work
exceptionally well. In the extreme case, the large fully connected layer in LeNet5 is compressed
by 445×. With encoding alone and demonstrates a 1.99× improvement over CSR, the alternative
encoding strategy used in Deep Compression.
Recall that the size of a Bloomier ﬁlter is proportional to mt, and so sparsity and clustering determine
how compact they can be. Our results suggest that sparsity is more important than the number of
clusters for reducing the encoding ﬁlter size. This can be seen by comparing each LeNet models’
magnitude pruning results to those of dynamic network surgery—while DNS needs additional clusters,
the increased sparsity results in a substantial size reduction. We suspect this is due to the ability of
DNNs to tolerate a high false positive rate. The t value used here is already on the exponential part
of the false positive curve (see Figure 2). At this point, even if k could be reduced, it is unlikely t
can be since the additional encoding strength saved by reducing k does little to protect against the
6
Under review as a conference paper at ICLR 2018
Table 2: Lossy encoding. Weight matrices encoded using Bloomier ﬁlters (Weightless) are smaller
than those encoded with CSR (Deep Compression), without loss of accuracy. In addition, Weightless
tends to do relatively better on larger models and when using more advanced pruning algorithms. The
Improvement column shows Bloomier ﬁlters are up to 1.99× more efﬁcient than CSR.
Compression Factor (Size KB)
Model
Pruning Method Layer
CSR
Improvement
LeNet-300-100
LeNet5
Magnitude
DNS
Magnitude
DNS
VGG-16
Magnitude
FC-0
FC-1
FC-0
FC-1
CNN-1
FC-0
CNN-1
FC-0
FC-0
FC-1
40.1× (22.9)
46.9× (2.50)
112 × (8.22)
99.0× (1.18)
40.7× (0.89)
46.6× (46.6)
80.6× (1.21)
224× (6.99)
81.8× (4790)
71.2× (900)
Weightless
40.6× (20.1)
56.1× (2.09)
152 × (6.04)
174 × (0.67)
46.2× (0.78)
66.6× (34.6)
97.8× (1.00)
445× (3.52)
142 × (2750)
74.6× (860)
1.01×
1.20×
1.36×
1.75×
1.14×
1.43×
1.21×
1.99×
1.74×
1.05×
doubling of false positives when in this range. For VGG-16 FC-0, there are more false positives in
the reconstructed weight matrix than there are non-zero weights originally; using t = 6 results in
over 6.2 million false positives while after simpliﬁcation there are only 3.07 million weights. Before
recovered with retraining, Bloomier ﬁlter encoding increased the top-1 error by 2.0 percentage points.
This is why we see Bloomier ﬁlters work so well here–most applications cannot function with this
level of approximation, nor do they have an analogous retrain mechanism to mitigate the errors’
effects.
Table 3: Network compression. Encoded weights can be compressed further for transmission or
storage. Below are the results of applying arithmetic coding to Bloomier ﬁlters and Huffman coding
to CSR. The Improvement column shows Weightless offers up to a 1.51× improvement over Deep
Compression.
Compression Factor (Size KB)
Improvement
Model
Pruning Method Layer
LeNet-300-100
LeNet5
Magnitude
DNS