how the eﬃciency of a system degrades due to the selﬁsh
behavior of its players:
Deﬁnition (Price of Anarchy). PoA of
hN , Σ,Ui is
1, . . . , s0
m)
game
n∈N
a
P
mins∗∈S∗P
maxs∈S
n∈N un(s)
n∈N un(s∗)
P oA :=
where S = S1×···×Sm is the set of all possible outcomes
while S∗ is the set of NEs.
Together or Alone: The Price of Privacy in Collaborative Learning
4
3 Game Theoretic Model
In this section, we describe the Collaborative Learning
(CoL) game which captures the actions of two privacy-
aware data holders in the scenario of applying an ar-
bitrary privacy preserving mechanism and training al-
gorithm on their datasets. We deﬁne the corresponding
utility functions and elaborate on its components. Fur-
thermore, we introduce the notion of Price of Privacy,
a novel measure of the eﬀect of privacy protection on
the accuracy of players.
3.1 The Collaborative Learning Game
At a high level, the players’ goal in the CoL game is to
maximize their utility, which is a function of the model
accuracy and the privacy loss. We do not consider the
adversarial aspect of players, hence the gain includes
only the accuracy improvements on the model for a par-
ticular player as beneﬁt (without the accuracy decrease
of the other player1) while the cost is private informa-
tion leakage: the trained model leaks some information
about the local dataset used for training.
Players only choose the privacy parameters for a
predetermined privacy preserving method M (rather
than choosing the method and the parameter). This
means each M corresponds to a diﬀerent game with a
diﬀerent deﬁnition of privacy, rather than having one
game where the players’ actions are deciding which
mechanism to use and to what extent. This is a re-
stricted scenario, nonetheless, even this scenario barely
lends itself to purely analytical treatment; it is already
not straightforward to derive the exact NE.
Variable Meaning
M
pn
CM
n
Bn
θn
ΦM
n (p1, p2)
b(θn, ΦM
n )
cM (pn)
Privacy mechanism applied by the players
Privacy parameter for player n
Privacy weight for player n
Accuracy weight for player n
Accuracy by training alone for player n
Accuracy by training together for player n
Beneﬁt function for player n
Privacy loss function for player n
Table 1. Parameters of the CoL game
The variables of the CoL game are listed in Tab. 1,
where the accuracy is measured as the prediction error
of the trained model: lower θn and ΦM
n correspond to
a more accurate model. Maximal privacy protection is
1 Extending the game for competing companies is an interesting
future direction.
represented via pn = 1, while pn = 0 means no pro-
tection for player n. The beneﬁt and the privacy loss
are not on the same scale as the ﬁrst depends on the
accuracy while the latter on information loss. To make
them comparable, we introduce weight parameters: the
beneﬁt function is multiplied with the accuracy weight
Bn > 0, while the privacy loss function is multiplied
with the privacy weight CM
n ≥ 0.
The collaborative accuracy ΦM
n is symbolic.
n (p1, p2) naturally de-
pends also on the datasets and the used algorithm be-
sides the privacy parameters pn and the corresponding
privacy mechanism M. However, for simplicity we ab-
stract them since it does not aﬀect our theoretical anal-
ysis as long as ΦM
Deﬁnition 1 (Collaborative Learning game).
The
CoL game is a tuple hN , Σ,Ui, where the set of play-
ers is N = {1, 2}, their actions are Σ = {p1, p2}
where p1, p2 ∈ [0, 1] while their utility functions are
U = {u1, u2} such that for n ∈ N :
un(p1, p2) = Bn · b(θn, ΦM
n · cM(pn) (1)
The CoL game is of symmetric information, i.e., the in-
troduced parameters are public knowledge (i.e., M, Bn,
n , b, cM, θn and ΦM
n ) except for the actions of the
CM
players (i.e., pn). Moreover, we do not consider any neg-
ative eﬀect of the training such as time or electricity con-
sumption, however, such variables may be introduced to
the model in the future.
n (p1, p2)) − CM
n , cM and ΦM
In the following, whenever possible, we simplify the
notion CM
n by removing the symbol M to
use Cn, c and Φn respectively. We only need to keep
in mind that these functions depend on the underly-
ing privacy-preserving mechanism M in the implemen-
tation.
3.1.1 Privacy Loss Function cM(pn)
This function represents the loss due to private data
leakage. We deﬁne c with the following natural proper-
ties:
Deﬁnition 2 (Privacy loss function). c : [0, 1] → [0, 1]
such that it is continuous and twice diﬀerentiable, c(0) =
1, c(1) = 0 and ∂pn c  0
∀n ∈ N : θn > Φn(0, 0)
Φn plays a crucial role in the beneﬁt function b. How-
ever, the function of how a privacy protection mech-
anism aﬀects a complex training algorithm (and con-
sequently the accuracy) is unique for each dataset and
algorithm. Although we measure it in Sec. 6, interpolate
it in Sec. 7 and approximate it in Sec. 8 for a recom-
mendation system use case, in general the exact form of
Φn is unknown. On the other hand, some properties are
expected:
Deﬁnition 4 (Privacy-Accuracy trade-oﬀ function).
Φn :
twice diﬀerentiable and:
–
–
–
The ﬁrst property means that maximal privacy pro-
tection cannot result in higher accuracy than training
alone for any player. The second property indicates that
higher privacy protection corresponds to lower accuracy
since Φn is monotone increasing in both p1 and p2. The
last property ensures that training together with no pri-
vacy corresponds to higher accuracy than training alone.
3.2 The Concept of Price of Privacy
Inspired by the notion of Price of Anarchy [KP99], we
deﬁne Price of Privacy to measure the accuracy loss due
to privacy constraints:
Deﬁnition 5 (Price of Privacy). P oP measures the
overall eﬀect of privacy protection on the accuracy:
P oP(p∗
1, p∗
2) := 1 −
P
P
n b(θn, Φn(p∗
1, p∗
2))
n b(θn, Φn(0, 0))
(2)
1, p∗
The quotient is between the total accuracy improvement
in a NE (p∗
2) and the total accuracy improvement
without privacy protection.
Due to the Def. 3 and 4, P oP ∈ [0, 1] where 0 cor-
responds the highest possible accuracy which can be
achieved via collaboration with no privacy while 1 cor-
responds the lowest possible accuracy which can be
achieved by training alone. In other words, Price of Pri-
vacy evaluates the beneﬁt of a given equilibrium. The
lower its value is, the higher the accuracy achieved by
collaboration.
Note that while PoA characterizes the whole game,
P oP is a property of a NE. Also, since Φn can only
be estimated in a real-world scenario, the players can
only approximate the value of P oP, which would then
measure the eﬃciency of the collaboration.
3.3 Remarks on the Model
Given that the actual value of Φn is required to com-
pute the optimal strategies, Φn has to be numerically
evaluated for putting the CoL game to practical use.
Diﬀerent from other parameters which can be set freely,
the impact of the privacy-preserving mechanism M on
the joint accuracy (and thus Φn) is determined by both
datasets. Precisely computing this function requires ac-
cess to the joint dataset; thus, it raises the very privacy
concern which we want to mitigate in the ﬁrst place.
To break this loop, we propose to adopt an approxi-
mation approach for applying the model. To this end,
we provide a solution heuristic and show its practical
feasibility in Sec. 8.
4 Equilibrium Analysis
In this section, we characterize the NEs for a simple and
more complex cases of the CoL game. We derive sym-
bolic NEs in closed form for the case where exactly one
of the players is privacy-concerned (i.e., Collaboration-
as-a-Service scenario). Next, we prove the existence of a
pure strategy NE in the general case, where both play-
ers are privacy-concerned to a given degree. To preserve
clarity, all mathematical proofs for theorems in this sec-
tion are given in App. B.
The simplest NE of the CoL game is no collabora-
tion:
Together or Alone: The Price of Privacy in Collaborative Learning
6
1, p∗
Theorem 1. Applying maximal privacy protection
(training alone) in the CoL game is a NE: (p∗
2) =
(1, 1).
Clearly, when the players train alone there will be no
improvement in accuracy. This means that the Price of
Privacy for this NE is the maximum 1: the entire poten-
tial accuracy improvement is lost due to privacy protec-
tion. This ﬁnding seemingly contradicts [CGL15], which
states that all players refraining to participate cannot be
an equilibrium. There is a signiﬁcant diﬀerence though;
estimation cost is a public good in [CGL15], while in
our case accuracy is private and each participant has a
base accuracy level obtained by training alone.
4.1 Player Types
Based on the properties of CoL game, two natural ex-
pectations arise:
– A player prefers collaboration if it values accuracy
– A player prefers training alone if it values accuracy
signiﬁcantly more than privacy (Bn (cid:29) Cn).
signiﬁcantly less than privacy (Bn (cid:28) Cn).
Bn
Bn
≥ βn for player n
≤ αn for player n
These intuitions are captured in the following two lem-
mas:
Lemma 1. ∃αn ≥ 0 such that if Cn
than its BR is ˆpn = 0.
Lemma 2. ∃βn ≥ 0 such that if Cn
then its BR is to set ˆpn = 1.
The questions we are interested in answering are: what
are the exact values of αn and βn and what is the NE
in case αn ≤ Cn
∈ [0,∞],
we deﬁne two types of players:
– Unconcerned: This type of player cares only about
= 0: the
≤ βn. Based on the ratio Cn
accuracy. This represents the case when Cn
Bn
privacy weight for player n is zero (Cn = 0).
– Concerned: This player is more privacy-aware, as
the privacy loss is present in its utility function. This
represents the case when Cn
Bn
> 0.
Bn
Bn
This information is available to both players as the CoL
game is a symmetric information game: both players
know which type of player they face.
4.2 One Player is Privacy Concerned
Deﬁnition 6 (Collaboration-as-a-Service). In a CaaS
scenario one player acts as a for-proﬁt service provider
of collaborative training without privacy concerns, i.e.,
its privacy weight is 0.
Example. Imagine a company who oﬀers CaaS for her
own proﬁt (Player 2). The CaaS provider does not ap-
ply any privacy-preserving mechanism (see Th. 2). Any
interested party (Player 1) who wants to to boost its ac-
curacy can use this service. At the same time, Player
1 requires additional privacy protection (besides the in-
herent complexity of the training algorithm) to prevent
her own data from leaking.
Theorem 2 (Training as an unconcerned player).
If
player n is unconcerned (Cn = 0) then its BR is to
collaborate without any privacy protection: ˆpn = 0.
When both players are unconcerned (C1 = C2 = 0),
(p∗
1, p∗
2) = (0, 0) is a NE. The corresponding Price of
Privacy value is 0 as no accuracy is lost due to privacy
protection.
p1 f.
As a result, the unconcerned player do not apply
any privacy-preserving mechanism. Without loss of gen-
erality we assume Player 2 is unconcerned, so its BR is
ˆp2 = 0. This allows us to make the following simpliﬁca-
tions: Φ(p1) := Φ1(p1, ˆp2), b(p1) := b(θ1, Φ(p1, ˆp2)) and
u(p1) := u1(p1, ˆp2) while f0 = ∂p1 f and f00 = ∂2
Theorem 3 (Training with an unconcerned player).
A NE of the CoL game when Player 1 is concerned
(C1 > 0) while Player 2 is unconcerned (C2 = 0) is