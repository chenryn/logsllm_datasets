- 一种选项是对记录按键重新分割槽并排序，就像在 MapReduce 的混洗阶段一样（请参阅 “[分散式执行 MapReduce](#分散式执行MapReduce)”）。这种功能可以用于实现排序合并连线和分组，就像在 MapReduce 中一样。
- 另一种可能是接受多个输入，并以相同的方式进行分割槽，但跳过排序。当记录的分割槽重要但顺序无关紧要时，这省去了分割槽杂凑连线的工作，因为构建散列表还是会把顺序随机打乱。
- 对于广播杂凑连线，可以将一个运算元的输出，传送到连线运算元的所有分割槽。
这种型别的处理引擎是基于像 Dryad【67】和 Nephele【68】这样的研究系统，与 MapReduce 模型相比，它有几个优点：
- 排序等昂贵的工作只需要在实际需要的地方执行，而不是预设地在每个 Map 和 Reduce 阶段之间出现。
- 没有不必要的 Map 任务，因为 Mapper 所做的工作通常可以合并到前面的 Reduce 运算元中（因为 Mapper 不会更改资料集的分割槽）。
- 由于工作流中的所有连线和资料依赖都是显式宣告的，因此排程程式能够总览全域性，知道哪里需要哪些资料，因而能够利用区域性进行最佳化。例如，它可以尝试将消费某些资料的任务放在与生成这些资料的任务相同的机器上，从而资料可以透过共享记忆体缓冲区传输，而不必透过网路复制。
- 通常，运算元间的中间状态足以储存在记忆体中或写入本地磁碟，这比写入 HDFS 需要更少的 I/O（必须将其复制到多台机器，并将每个副本写入磁碟）。MapReduce 已经对 Mapper 的输出做了这种最佳化，但资料流引擎将这种思想推广至所有的中间状态。
- 运算元可以在输入就绪后立即开始执行；后续阶段无需等待前驱阶段整个完成后再开始。
- 与 MapReduce（为每个任务启动一个新的 JVM）相比，现有 Java 虚拟机器（JVM）程序可以重用来执行新运算元，从而减少启动开销。
你可以使用资料流引擎执行与 MapReduce 工作流同样的计算，而且由于此处所述的最佳化，通常执行速度要明显快得多。既然运算元是 Map 和 Reduce 的泛化，那么相同的处理程式码就可以在任一执行引擎上执行：Pig，Hive 或 Cascading 中实现的工作流可以无需修改程式码，可以透过修改配置，简单地从 MapReduce 切换到 Tez 或 Spark【64】。
Tez 是一个相当薄的库，它依赖于 YARN shuffle 服务来实现节点间资料的实际复制【58】，而 Spark 和 Flink 则是包含了独立网路通讯层，排程器，及使用者向 API 的大型框架。我们将简要讨论这些高阶 API。
#### 容错
完全物化中间状态至分散式档案系统的一个优点是，它具有永续性，这使得 MapReduce 中的容错相当容易：如果一个任务失败，它可以在另一台机器上重新启动，并从档案系统重新读取相同的输入。
Spark、Flink 和 Tez 避免将中间状态写入 HDFS，因此它们采取了不同的方法来容错：如果一台机器发生故障，并且该机器上的中间状态丢失，则它会从其他仍然可用的资料重新计算（在可行的情况下是先前的中间状态，要么就只能是原始输入资料，通常在 HDFS 上）。
为了实现这种重新计算，框架必须跟踪一个给定的资料是如何计算的 —— 使用了哪些输入分割槽？应用了哪些运算元？ Spark 使用 **弹性分散式资料集（RDD，Resilient Distributed Dataset）** 的抽象来跟踪资料的谱系【61】，而 Flink 对运算元状态存档，允许恢复执行在执行过程中遇到错误的运算元【66】。
在重新计算资料时，重要的是要知道计算是否是 **确定性的**：也就是说，给定相同的输入资料，运算元是否始终产生相同的输出？如果一些丢失的资料已经发送给下游运算元，这个问题就很重要。如果运算元重新启动，重新计算的资料与原有的丢失资料不一致，下游运算元很难解决新旧资料之间的矛盾。对于不确定性运算元来说，解决方案通常是杀死下游运算元，然后再重跑新资料。
为了避免这种级联故障，最好让运算元具有确定性。但需要注意的是，非确定性行为很容易悄悄溜进来：例如，许多程式语言在迭代杂凑表的元素时不能对顺序作出保证，许多机率和统计算法显式依赖于使用随机数，以及用到系统时钟或外部资料来源，这些都是都不确定性的行为。为了能可靠地从故障中恢复，需要消除这种不确定性因素，例如使用固定的种子生成伪随机数。
透过重算资料来从故障中恢复并不总是正确的答案：如果中间状态资料要比源资料小得多，或者如果计算量非常大，那么将中间资料物化为档案可能要比重新计算廉价的多。
#### 关于物化的讨论
回到 Unix 的类比，我们看到，MapReduce 就像是将每个命令的输出写入临时档案，而资料流引擎看起来更像是 Unix 管道。尤其是 Flink 是基于管道执行的思想而建立的：也就是说，将运算元的输出增量地传递给其他运算元，不待输入完成便开始处理。
排序运算元不可避免地需要消费全部的输入后才能生成任何输出，因为输入中最后一条输入记录可能具有最小的键，因此需要作为第一条记录输出。因此，任何需要排序的运算元都需要至少暂时地累积状态。但是工作流的许多其他部分可以以流水线方式执行。
当作业完成时，它的输出需要持续到某个地方，以便使用者可以找到并使用它 —— 很可能它会再次写入分散式档案系统。因此，在使用资料流引擎时，HDFS 上的物化资料集通常仍是作业的输入和最终输出。和 MapReduce 一样，输入是不可变的，输出被完全替换。比起 MapReduce 的改进是，你不用再自己去将中间状态写入档案系统了。
### 图与迭代处理
在 “[图资料模型](ch2.md#图资料模型)” 中，我们讨论了使用图来建模资料，并使用图查询语言来遍历图中的边与点。[第二章](ch2.md) 的讨论集中在 OLTP 风格的应用场景：快速执行查询来查询少量符合特定条件的顶点。
批处理上下文中的图也很有趣，其目标是在整个图上执行某种离线处理或分析。这种需求经常出现在机器学习应用（如推荐引擎）或排序系统中。例如，最著名的图形分析演算法之一是 PageRank 【69】，它试图根据连结到某个网页的其他网页来估计该网页的流行度。它作为配方的一部分，用于确定网路搜寻引擎呈现结果的顺序。
> 像 Spark、Flink 和 Tez 这样的资料流引擎（请参阅 “[物化中间状态](#物化中间状态)”）通常将运算元作为 **有向无环图（DAG）** 的一部分安排在作业中。这与图处理不一样：在资料流引擎中，**从一个运算元到另一个运算元的资料流** 被构造成一个图，而资料本身通常由关系型元组构成。在图处理中，资料本身具有图的形式。又一个不幸的命名混乱！
许多图演算法是透过一次遍历一条边来表示的，将一个顶点与近邻的顶点连线起来，以传播一些资讯，并不断重复，直到满足一些条件为止 —— 例如，直到没有更多的边要跟进，或直到一些指标收敛。我们在 [图 2-6](../img/fig2-6.png) 中看到一个例子，它透过重复跟进标明地点归属关系的边，生成了资料库中北美包含的所有地点列表（这种演算法被称为 **传递闭包**，即 transitive closure）。
可以在分散式档案系统中储存图（包含顶点和边的列表的档案），但是这种 “重复至完成” 的想法不能用普通的 MapReduce 来表示，因为它只扫过一趟资料。这种演算法因此经常以 **迭代** 的风格实现：
1. 外部排程程式执行批处理来计算演算法的一个步骤。
2. 当批处理过程完成时，排程器检查它是否完成（基于完成条件 —— 例如，没有更多的边要跟进，或者与上次迭代相比的变化低于某个阈值）。
3. 如果尚未完成，则排程程式返回到步骤 1 并执行另一轮批处理。
这种方法是有效的，但是用 MapReduce 实现它往往非常低效，因为 MapReduce 没有考虑演算法的迭代性质：它总是读取整个输入资料集并产生一个全新的输出资料集，即使与上次迭代相比，改变的仅仅是图中的一小部分。
#### Pregel处理模型
针对图批处理的最佳化 —— **批次同步并行（BSP，Bulk Synchronous Parallel）** 计算模型【70】已经开始流行起来。其中，Apache Giraph 【37】，Spark 的 GraphX API 和 Flink 的 Gelly API 【71】实现了它。它也被称为 **Pregel** 模型，因为 Google 的 Pregel 论文推广了这种处理图的方法【72】。
回想一下在 MapReduce 中，Mapper 在概念上向 Reducer 的特定呼叫 “传送讯息”，因为框架将所有具有相同键的 Mapper 输出集中在一起。Pregel 背后有一个类似的想法：一个顶点可以向另一个顶点 “传送讯息”，通常这些讯息是沿著图的边传送的。