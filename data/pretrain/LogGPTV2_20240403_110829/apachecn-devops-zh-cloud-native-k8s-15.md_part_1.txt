# 十五、Kubernetes 上的有状态工作负载
本章详细介绍了在数据库中运行有状态工作负载的行业现状。我们将讨论使用 Kubernetes(和流行的开源项目)在 Kubernetes 上运行数据库、存储和队列。案例研究教程将包括在 Kubernetes 上运行对象存储、数据库和队列系统。
在本章中，我们将首先了解有状态应用如何在 Kubernetes 上运行，然后学习如何将 Kubernetes 存储用于有状态应用。然后我们将学习如何在 Kubernetes 上运行数据库，以及如何覆盖消息传递和队列。让我们从讨论为什么有状态应用比 Kubernetes 上的无状态应用复杂得多开始。
在本章中，我们将涵盖以下主题:
*   了解 Kubernetes 上的有状态应用
*   将 Kubernetes 存储用于有状态应用
*   在 Kubernetes 上运行数据库
*   在 Kubernetes 上实现消息传递和队列
# 技术要求
为了运行本章中详细介绍的命令，您将需要一台支持`kubectl`命令行工具的计算机以及一个工作正常的 Kubernetes 集群。参见 [*第一章*](01.html#_idTextAnchor016)*与 Kubernetes*通讯，了解几种快速启动和运行 Kubernetes 的方法，以及如何安装 Kubernetes 工具的说明。
本章中使用的代码可以在本书的 GitHub 存储库中找到:
[https://github . com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/chapter 15](https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter15)
# 了解 Kubernetes 上的有状态应用
Kubernetes 为运行无状态和有状态应用提供了优秀的原语，但是有状态工作负载在 Kubernetes 上需要更长的时间才能成熟。然而，近年来，一些备受瞩目的基于 Kubernetes 的有状态应用框架和项目已经证明了 Kubernetes 上有状态应用的日益成熟。让我们先回顾一下其中的一些，以便为本章的其余部分做好准备。
## 流行的 Kubernetes-本地有状态应用
有状态应用有多种类型。虽然大多数应用都是有状态的，但是这些应用中只有某些组件存储*状态*数据。我们可以从应用中移除这些特定的有状态组件，并在我们的审查中关注这些组件。在本书中，我们将讨论数据库、队列和对象存储，不包括持久存储组件，例如我们在 [*第 7 章*](07.html#_idTextAnchor166)*中回顾的那些组件。我们还将讨论一些不太通用的组件，作为荣誉奖。让我们从数据库开始！*
### kubernetes 兼容数据库
除了典型的**数据库** ( **DBs** )和键值存储如 **Postgres** 、 **MySQL** 和 **Redis** 可以通过 StatefulSets 或社区运营商部署在 Kubernetes 上之外，还有一些主要的专为 Kubernetes 定制的选项:
*   **cocroach db**:可以无缝部署在 Kubernetes 上的分布式 SQL 数据库
*   **Vitess** :一个 MySQL 分片编排器，允许 MySQL 的全局可伸缩性，也可以通过一个操作器安装在 Kubernetes 上
*   **yugabbtedb**:类似**的分布式 SQL 数据库，也支持类似 Cassandra 的查询**
接下来，我们来看看在 Kubernetes 上的排队和消息传递。
### Kubernetes 上的队列、流和消息传递
此外还有行业标准选项如**卡夫卡**和**拉比 tmq**可以使用社区 Helm 图和运营商部署在 Kubernetes 上，此外还有一些专门设计的开源和闭源选项:
*   **NATS** :开源消息和流媒体系统
*   **kubi eq**:kubi etes-native message broker
接下来，让我们看看 Kubernetes 上的对象存储。
### Kubernetes 上的对象存储
对象存储从 Kubernetes 获取基于卷的持久存储，并添加一个对象存储层，类似于(在许多情况下与)亚马逊 S3 的应用编程接口兼容:
*   **Minio** :专为高性能打造的 S3 兼容对象存储。
*   **开启 IO** :类似于 *Minio* ，这款性能很高，支持 S3 和 Swift 存储。
接下来，让我们来看看几个光荣的提及。
### 荣誉提名
除了前面的通用组件之外，还有一些更专门的(但仍然是分类的)有状态应用可以在 Kubernetes 上运行:
*   **密钥和授权管理** : **金库**、**密钥卡**
*   **容器登记处**:T2 港、**蜻蜓**、**Docker**
*   **工作流管理** : **阿帕奇气流**带 Kubernetes 操作员
现在我们已经回顾了几类有状态的应用，让我们来谈谈这些重状态的应用通常是如何在 Kubernetes 上实现的。
## 了解在 Kubernetes 上运行有状态应用的策略
虽然在 Kubernetes 上使用 ReplicaSet 或 Deployment 部署有状态应用本身并没有什么问题，但是您会发现 Kubernetes 上的大多数有状态应用都使用 StatefulSets。我们在 [*第 4 章*](04.html#_idTextAnchor106)*中讨论了状态集，扩展和部署您的应用*，但是为什么它们对应用如此有用？我们将在本章中回顾和回答这个问题。
主要原因是 Pod 身份。许多分布式有状态应用都有自己的集群机制或共识算法。为了平滑这些类型的应用的过程，StatefulSets 提供基于序数系统的静态 Pod 命名，从`0`到`n`开始。这与滚动更新和创建方法相结合，使应用更容易进行自我集群，这对于云原生数据库(如 CockroachDB)极其重要。
为了说明 statefleset 如何以及为什么可以帮助在 Kubernetes 上运行有状态的应用，让我们看看如何使用 statefleset 在 Kubernetes 上运行 MySQL。
现在，明确地说，在 Kubernetes 上运行一个单独的 MySQL Pod 非常简单。我们需要做的就是找到一个 MySQL 容器镜像，并确保它有正确的配置和`startup`命令。
然而，当我们考虑扩展我们的数据库时，我们开始遇到问题。与简单的无状态应用不同，在无状态应用中，我们可以在不创建新状态的情况下扩展部署，MySQL(像许多其他数据库一样)有自己的集群和共识方法。MySQL 集群的每个成员都知道其他成员，最重要的是，它知道集群的哪个成员是领导者。这就是像 MySQL 这样的数据库如何提供一致性保证和**原子性、一致性、隔离性、持久性** ( **ACID** )合规性。
因此，由于 MySQL 集群中的每个成员都需要了解其他成员(最重要的是主成员)，因此我们需要以一种方式运行我们的 DB Pods，这意味着他们有一种通用的方法来查找 DB 集群的其他成员并与之通信。
正如我们在本节开头提到的，StatefulSets 提供的方式是通过顺序 Pod 编号。这样，在 Kubernetes 上运行时需要自我集群的应用就知道将使用从`0`到`n`的通用命名方案。此外，当特定顺序的 Pod 重新启动时(例如，`mysql-pod-2`)，相同的 PersistentVolume 将装载到在该顺序点启动的新 Pod 上。这使得 StatefulSet 中单个 Pod 的重启之间具有状态一致性，这使得应用更容易形成稳定的集群。
为了了解这在实践中是如何工作的，让我们来看看 MySQL 的 StatefulSet 规范。
### 在状态集上运行 MySQL
以下 YAML 规范改编自 Kubernetes 文档版本。它展示了我们如何在状态集上运行 MySQL 集群。我们将分别回顾 YAML 规范的每一部分，这样我们就可以确切地理解这些机制是如何与 StatefulSet 保证相互作用的。
让我们从规范的第一部分开始:
statefulset-mysql.yaml
```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  selector:
    matchLabels:
      app: mysql
  serviceName: mysql
  replicas: 3
  template:
    metadata:
      labels:
        app: mysql
```
如您所见，我们将创建一个包含三个`replicas`的 MySQL 集群。
这部作品没有太多其他令人兴奋的地方，所以让我们进入`initContainers`的开始。在`initContainers`和普通容器之间的这个 Pod 中将有相当多的容器运行，所以我们将分别进行解释。接下来是第一个`initContainer`的例子:
```
    spec:
      initContainers:
      - name: init-mysql
        image: mysql:5.7
        command:
        - bash
        - "-c"
        - |
          set -ex
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          echo [mysqld] > /mnt/conf.d/server-id.cnf
          echo server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf
          if [[ $ordinal -eq 0 ]]; then
            cp /mnt/config-map/master.cnf /mnt/conf.d/
          else
            cp /mnt/config-map/slave.cnf /mnt/conf.d/
          fi
        volumeMounts:
        - name: conf
          mountPath: /mnt/conf.d
        - name: config-map
          mountPath: /mnt/config-map
```
如你所见，第一个`initContainer`是 MySQL 容器映像。现在，这并不意味着我们不会让 MySQL 容器在 Pod 中持续运行。这是一个模式你会经常看到复杂的应用。有时，同一个容器映像在 Pod 中既用作`initContainer`实例，又用作正常运行的容器。这是因为该容器具有正确的嵌入式脚本和工具，可以以编程方式执行常见的设置任务。
在本例中，MySQL `initContainer`创建了一个文件`/mnt/conf.d/server-id.cnf`，并向该文件添加了一个`server`标识，对应于状态集中 Pod 的`ordinal`标识。当写入`ordinal`标识时，它会添加`100`作为偏移量，以避开在 MySQL 中为`0`的`server-id`标识保留的值。
然后，根据 Pod `ordinal` D 是否为`0`，它将主或从 MySQL 服务器的配置复制到卷中。
接下来，让我们来看下一节的第二个`initContainer`(为了简洁起见，我们省略了一些带有卷装入信息的代码，但是完整的代码可以在本书的 GitHub 存储库中找到):
```
      - name: clone-mysql
        image: gcr.io/google-samples/xtrabackup:1.0
        command:
        - bash
        - "-c"
        - |
          set -ex
          [[ -d /var/lib/mysql/mysql ]] && exit 0
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          [[ $ordinal -eq 0 ]] && exit 0          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql
          xtrabackup --prepare --target-dir=/var/lib/mysql
```
可以看到，这个`initContainer`根本不是 MySQL！相反，容器映像是一个名为 Xtra Backup 的工具。我们为什么需要这个容器？
考虑一种情况，一个全新的 Pod，一个全新的空 PersistentVolume 加入集群。在这种情况下，数据复制过程需要通过复制从 MySQL 集群中的其他成员复制所有数据。对于大型数据库，这个过程可能会非常缓慢。
为此，我们有一个`initContainer`实例，它从 StatefulSet 中的另一个 MySQL Pod 加载数据，这样 MySQL 的数据复制功能就有了用武之地。在 MySQL Pod 中已经有数据的情况下，这种数据加载不会发生。`[[ -d /var/lib/mysql/mysql ]] && exit 0`线是检查是否存在数据的线。
一旦这两个`initContainer`实例成功完成了它们的任务，我们就拥有了第一个`initContainer`提供的所有 MySQL 配置，并且我们还拥有来自 MySQL StatefulSet 中另一个成员的一组最近的数据。
现在，让我们继续讨论 StatefulSet 定义中的实际容器，从 MySQL 本身开始:
```
      containers:
      - name: mysql
        image: mysql:5.7
        env:
        - name: MYSQL_ALLOW_EMPTY_PASSWORD
          value: "1"
        ports:
        - name: mysql
          containerPort: 3306
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
```
可以看到，这个 MySQL 容器设置是相当基础的。除了到一个环境变量，我们挂载之前创建的配置。这个 Pod 也有一些活跃度和就绪探测器配置——请查看这本书的 GitHub 存储库。
现在，让我们继续检查我们的最终容器，它看起来很熟悉——它实际上是 Xtra Backup 的另一个实例！让我们看看它是如何配置的:
```
- name: xtrabackup
containerPort: 3307
command:
- bash
- "-c"
- |
set -ex
cd /var/lib/mysql if [[ -f xtrabackup_slave_info && "x$( change_master_to.sql.inrm -f xtrabackup_slave_info xtrabackup_binlog_info
elif [[ -f xtrabackup_binlog_info ]]; then[[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1
rm -f xtrabackup_binlog_info xtrabackup_slave_info
echo "CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\
MASTER_LOG_POS=${BASH_REMATCH[2]}" > change_master_to.sql.in
fi if [[ -f change_master_to.sql.in ]]; then
echo "Waiting for mysqld to be ready (accepting connections)"
until mysql -h 127.0.0.1 -e "SELECT 1"; do sleep 1; done
echo "Initializing replication from clone position"
mysql -h 127.0.0.1 \
-e "$(<change_master_to.sql.in), \
MASTER_HOST='mysql-0.mysql', \
MASTER_USER='root', \
MASTER_PASSWORD='', \
MASTER_CONNECT_RETRY=10; \
START SLAVE;" || exit 1
mv change_master_to.sql.in change_master_to.sql.orig
fi exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \
"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root"