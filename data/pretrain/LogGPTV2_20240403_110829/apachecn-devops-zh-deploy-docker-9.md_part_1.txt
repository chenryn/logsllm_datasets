# 九、探索最大规模的部署
在前面的章节中，我们介绍了部署 Docker 容器的许多不同方面，但是如果我们要将我们的示例转化为一个能够承受每秒数百万次请求吞吐量的全球服务，那么仍然需要解决一些问题，这一章是专门为详细介绍最重要的问题而编写的。由于这里所涵盖的主题的实现将涉及足够的材料来成为自己的书，并且基础设施将根据多种因素而大相径庭，所以这里的文本将主要是理论方面的，但是我们在本章之前的文本中获得的对服务的理解应该足够好，可以为您提供如何以最少的痛苦继续进行的想法。
就其核心而言，我们将涵盖的主题围绕选择正确的技术，然后遵循三个基本思想:
*   自动化一切！
*   真的，全部自动化！
*   是的，自动化你每隔几周做的那些一次性的事情
这可能是一个笑话，但是希望现在应该很清楚，所有这些工作的要点之一(除了隔离)是从您的系统中移除任何与保持服务运行相关的人工交互，以便您和您的团队能够专注于实际开发服务，而不是在部署上浪费时间。
# 维护定额
在我们前面的示例中，我们主要使用单节点管理器，但是如果您想要恢复能力，您必须确保有最少的故障点会使您的整个基础架构瘫痪，并且单个编排管理节点对于生产服务来说是绝对不够的，无论您是否使用 Swarm、Kubernetes、Marathon 或其他工具作为编排工具。从最佳实践的角度来看，您希望集群中至少有三个或更多管理节点分布在三个或更多云的**可用性区域** ( **【阿兹】**)或等效分组中，以真正确保规模稳定性，因为数据中心宕机已经发生，并给没有缓解这类情况的公司带来严重问题。
虽然在大多数编排平台中，您可以有任意数量的备份管理节点(或者在某些情况下备份键值存储)，但您将始终必须平衡弹性和速度，因为随着节点的增多，处理系统中较大部分故障的能力也会提高，但是对该系统的更改(例如添加和删除节点)必须达到更多的点，所有这些点都必须一致，从而使处理数据的速度变慢。在大多数需要这种 3+可用性区域拓扑的情况下，我们需要深入了解 quorums 的细节——我们之前简单介绍过的概念，它是所有**高可用性** ( **HA** )系统的主干。
基本意义上的 Quorums 是大多数管理节点的分组，它们可以共同决定是否允许对集群进行更新。如果由于一半或更多管理节点不可用而导致仲裁丢失，将停止对群集的所有更改，以防止您的群集基础架构有效地拆分群集。为了在这方面正确划分您的网络拓扑进行扩展，您必须确保至少有三个节点和/或可用性区域，因为如果单个故障少于该数量，法定多数将会丢失。更进一步，您通常还需要奇数个节点和可用性区域，因为偶数个节点和可用性区域并不能为维护法定人数提供太多额外的保护，稍后我们将看到这一点。
首先，假设您有五个管理节点。要保持这个数量的法定人数，您必须有三个或更多的可用节点，但是如果您只有两个可用区域，您能做的最好的分割是 *3-2* ，如果连接断开或带有两个管理节点的 **AZ** 关闭，这将很好地工作，但是如果带有三个节点的 **AZ** 关闭，则无法建立法定人数，因为两个节点少于总节点数的一半。
![](img/9b20240b-63b6-4215-a9c0-ff26e4109879.png) ![](img/1298a860-34b1-4976-8f42-122eda63e8d9.png)
现在让我们看看我们可以通过三个可用性区域获得什么样的弹性。具有五个管理节点的这种分组的最佳布局是 *2-2-1* ，如果您仔细查看当任何一个区域关闭时会发生什么，您会发现法定人数始终保持不变，因为我们将有 *3 (2+1)* 或 *4 (2+2)* 节点仍然可以从集群的其余部分获得，从而确保我们的服务运行没有问题:
![](img/42670a50-65b0-471e-964f-49031cc27f97.png)
当然，展示偶数对有效性有什么样的影响也是好的，因为我们提到它们可能有点麻烦。有了四个 az，我们可以进行的最佳分割将是跨它们的 *2-1-1-1* ，使用这些数字，如果两个区域都只包含一个节点，我们只能容忍两个区域不可用。在这种设置下，两个不可用的区域将有 50%的机会包含其中有两个节点的区域，这样不可用的节点总数将超过 3 个，因此群集将完全离线:
![](img/a9f34651-1a4f-4903-986d-884a35a1c6ef.png)
![](img/1308f8ec-8c4f-4ca1-946c-f6dc3fd0c778.png)
如果您有更多的可用性区域和管理器，管理节点跨集群的更高数量的 AZs 的这种分布会变得更加稳定，但是对于我们这里的简单示例，如果我们有五个管理节点和五个可用性区域( *1-1-1-1-1* 布局)，我们可以看到这种效果。通过这种拆分，由于法定要求至少三个节点，如果五个区域中的任何两个不可用，我们仍将完全运行，从而使您的故障容限比 3-AZ 拓扑提高 100%；但是您可以假设，可能完全不同的地理区域之间的通信会给任何更新增加大量延迟。
希望有了这些例子，现在应该很清楚，当您试图保持集群的弹性并且能够维持仲裁时，您会使用什么样的考虑和计算。虽然工具可能因编排工具而异(即`etcd`节点与 Zookeeper 节点)，但几乎所有工具的原理都保持相对一致，因此这一部分应该是相对可移植的。
# 节点自动化
当我们用 Packer 制作**亚马逊机器映像** ( **AMIs** )时，我们已经看到了我们可以用预烘焙的实例映像做什么样的事情，但是只有当整个基础架构都由它们组成时，它们的真正力量才能得到充分利用。如果您的编排管理节点和工作节点有自己的系统映像，并且有几个启动脚本也通过 init 系统(例如，`systemd`启动服务)嵌入，您可以让使用这些映像启动的实例在引导期间以它们的预定义角色自动加入您的集群。将这进一步扩展到概念层面，如果我们将所有有状态配置提取到映像配置中，并将所有动态配置提取到所有节点(如 EC2 `user-data`或 HashiCorp Vault)都可以访问的单独服务中，那么除了初始部署和映像构建之外，您的集群几乎可以完全自我配置。
通过拥有这种强大的自动加入功能，您可以消除与扩展或缩减集群相关的大部分手动工作，因为除了启动虚拟机实例之外，不需要与它进行交互。下图描述了该体系结构的一个相当简单的示例，其中编排节点和工作节点拥有各自的映像，并在启动时使用 **VPC** 内部的共享配置数据提供程序进行自我配置:
![](img/03c30103-7bdf-4376-b92e-12144a3cf47e.png)
CAUTION! To prevent serious security breaches make sure to separate and isolate any sensitive information to be accessible only by the desired systems in this configuration service layout. As we mentioned in one of the early chapters, following security best practices by using need-to-know practices will ensure that a compromise of a single point (most likely a worker node) will not be able to spread easily to the rest of your cluster. As a simple example here, this would include making sure that management secrets are not readable by worker nodes or their network.
# 反应式自动缩放