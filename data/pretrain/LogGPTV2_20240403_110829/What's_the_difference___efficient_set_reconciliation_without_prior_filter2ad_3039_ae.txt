while the basic Min-wise estimator must compute 3840 hashes for
each key, the Hybrid Min-wise estimator only computes hashes for
approximately 1/28 of the elements in the set. Since each of the
estimators is updated during its add operations, minimal server
compute time is required to serialize each structure.
Costs and Beneﬁts of Incremental Updates. As we have seen
in Table 1, the server computation time for many of the reconcilia-
tion algorithms is signiﬁcant and will negatively affect the speed of
our diff operation. The main challenge for IBF and CPISync
is that the size of the difference must be known before a space-
efﬁcient structure can be constructed. We can avoid this runtime
computation by maintaining several IBF’s of predetermined sizes
within KeyDiff. Each key is added to all IBF’s and, once the size
of the difference is know, the smallest suitable IBF is returned. The
number of IBF’s to maintain in parallel will depend on the comput-
ing and bandwidth overheads encountered for each application.
In Table 1, we see that the time to add a new key when doing pre-
computation on 8 IBF’s takes 31µs, two orders of magnitude longer
than IBF without precomputation. However, this reduces the server
compute time to from 3.9 seconds to 22µs, a massive improve-
ment for diff latency. For most applications, the small cost (10’s
of microseconds) for incremental updates during each add opera-
tion should not dramatically affect overall application performance,
while the speedup during diff (seconds) is a clear advantage.
Diff Performance. We now look at time required to run the
diff operation. As we are primarily concerned with the perfor-
mance of computing set differences as seen by an application built
on top of these algorithms, we use incremental updates and measure
Reconciliation Algorithm
List (sorted)
ART
CPISync (d=100)
IBF (no precompute)
IBF (1x precompute)
IBF (8x precompute)
Add (µs)
1.309
1.995
0.216
0.217
3.858
31.320
Serv. Compute
6.545 msec
6,937.831 msec
34,051.480 msec
3,957.847 msec
0.023 msec
0.022 msec
Estimation Algorithms (precompute)
Min-wise (3840 hashes)
Strata (16x80 cells)
Hybrid (7x80 cells + 2160 hash)
21.909
4.224
4.319
0.022 msec
0.021 msec
0.023 msec
Table 1: Time required to add a key to KeyDiff and the time
required to generate a KeyDiff response for sets of 1M keys
with a delta of 100. The time per add call is averaged across
the insertion of the 1M keys.
the wall clock time required to compute the set difference. Differ-
ence Digests are run with 15.3 KB dedicated to the Estimator, and
8 parallel, precomputed IBF’s with sizes ranging from 256 to 400K
cells in factors of 4. To present the best case scenario, CPISync
was conﬁgured with foreknowledge of the difference size and the
correctly sized CPISync structure was precomputed at the server
side. We omit performance results for ART as it has the unfair ad-
vantage of only approximating the membership of DA−B, unlike
the other algorithms, which return all of DA−B and DB−A.
For our tests, we populated the ﬁrst host, A with a set of 1 mil-
lion, unique 32-bit keys, SA, and copied a random subset of those
keys, SB, to the other host, B. From host A we then query Key-
Diff to compute the set of unique keys. Our results can be seen in
Figure 11a. We note that there are 3 components contributing to the
latency for all of these methods, the time to generate the response at
host B, the time to transmit the response, and the time to compare
the response to the set stored at host A. Since we maintain each
data structure online, the time for host B to generate a response is
negligible and does not affect the overall latency.
We see from these results that the List shows predictable per-
formance across difference sizes, but performs particularly well rel-
ative to other methods as the size of the difference increases beyond
20K. Since the size of the data sent decreases as the size of the dif-
ference increases, the transmission time and the time to sort and
compare at A decrease accordingly. On the other hand, the Differ-
ence Digest performs best at small set differences. Since the es-
timator is maintained online, the estimation phase concludes very
quickly, often taking less than 1 millisecond. We note that precom-
puting IBF’s at various sizes is essential and signiﬁcantly reduces
the latency by the IBF’s construction at host B at runtime. Fi-
nally, we see that even though its communication overhead is very
low, the cubic decoding complexity for CPISync dramatically in-
creases its latency at differences larger than 100.
In considering the resources required for each algorithm, List
requires 4|SB| bytes in transmission and touches |SA| + |SB| val-
ues in memory. Difference Digest has a constant estimation phase
of 15.3KB followed by an average of 24d bytes in transmission
and 3d × hash_count memory operations (3 ﬁelds in each IBF
cell). Finally, CPISync requires only 10d bytes of transmission
to send a vector of sums from it’s linear equations, but d3 memory
operations to solve its matrix.
If our experimental setup were completely compute bound, we
would expect List to have superior performance for large differ-
ences and Difference Digest to shine for small difference. If we
assume a hash_count of 4, then Difference Digest’s latency is
12d, while List’s is |SA| + |SB| = 2|SA| − d. Thus, they will
have equivalent latency at d = 2
12+1 |SA|, or a difference of 15%.
Guidance for Constrained Computation. We conclude that,
for our high-speed test environment, precomputed Difference Di-
gests are superior for small deltas (less than 2%), while sending a
full list is preferable at larger difference sizes. We argue that in
environments where computation is severely constrained relative
to bandwidth, this crossover point can reach up to 15%. In such
scenarios, precomputation is vital to optimize diff performance.
Varying Bandwidth. We now investigate how the latency of
each algorithm changes as the speed of the network decreases. For
this we consider bandwidths of 10 Mbps and 100Kbps, which are
speeds typical in wide-area networks and mobile devices, respec-
tively. By scaling the transmission times from our previous exper-
iments, we are able to predict the performance at slower network
speeds. We show these results in Figure 11b and Figure 11c.
As discussed previously, the data required by List, CPISync,
and Difference Digests is 4|SB|, 10d and roughly 24d, respectively.
Thus, as the network slows and dominates the running time, we ex-
pect that the low bandwidth overhead of CPISync and Difference
Digests will make them attractive for a wider range of deltas ver-
sus the sorted List. With only communication overhead, List
will take 4|SB| = 4(|SA| − d), which will equal Difference Di-
gest’s running time at d = 4
24+4 |SA|, or a difference of 14%. As
the communication overhead for Difference Digest grows due to
widely spaced precomputed IBF’s the trade off point will move to-
ward differences that are a smaller percentage of the total set size.
However, for small to moderate set sizes and highly constrained
bandwidths KeyDiff should create appropriately sized IBF’s on de-
mand to reduce memory overhead and minimize transmission time.
Guidance for Constrained Bandwidth. As bandwidth becomes
a predominant factor in reconciling a set difference, the algorithm
with the lowest data overhead should be employed. Thus, List
will have superior performance for differences greater than 14%.
For smaller difference sizes, IBF will achieve faster performance,
but the crossover point will depend on the size increase between
precomputed IBF’s. For constrained networks and moderate set
sizes, IBF’s could be computed on-demand to optimize communi-
cation overhead and minimize overall latency.
7. CONCLUSIONS
We have shown how Difference Digests can efﬁciently compute
the set difference of data objects on different hosts using computa-
tion and communication proportional to the size of the set differ-
ence. The constant factors are roughly 12 for computation (4 hash
functions, resulting in 3 updates each) and 24 for communication
(12 bytes/cell scaled by two for accurate estimation and decoding).
The two main new ideas are whole set differencing for IBF’s, and
a new estimator that accurately estimates small set differences via a
hierarchy of sampled IBF’s. One can think of IBF’s as a particularly
simple random code that can deal with both erasures and insertions
and hence uses a simpler structure than Tornado codes but a similar
decoding procedure.
We learned via experiments that 3 to 4 hash functions work best,
and that a simple rule of thumb is to size the second phase IBF equal
to twice the estimate found in the ﬁrst phase. We implemented
Difference Digests in a KeyDiff service run on top of TCP that
can be utilized by different applications such as Peer-to-Peer ﬁle
transfer. There are three calls in the API (Figure 4): calls to add
and delete a key, and a call to ﬁnd the difference between a set of
keys at another host.
In addition, using 80 cells per strata in the estimator worked well
and, after the ﬁrst 7 strata, augmenting the estimator with Min-
)
c
e
s
(
y
c
n
e
a
L
t
f
f
i
D
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
 10
List
CPISync
D.Digest
List
CPISync
D.Digest
)
c
e
s
(
y
c
n
e
a
L
t
f
f
i
D
 16
 14
 12
 10
 8
 6
 4
 2
 0
 100
 1000
 10000
 100000
 1e+06
 10
 100
 1000
 10000
 100000
 1e+06
List
CPISync
D.Digest
 400
 350
 300
 250
 200
 150
 100
 50
)
c
e
s
(
y
c
n
e
a
L
t
f
f
i
D
 0
 10
 100
 1000
 10000
 100000
 1e+06
Set Difference
Set Difference
Set Difference
(a) Measured - 1.2 Gbps
(b) Modeled - 10 Mbps
(c) Modeled - 100 Kbps
Figure 11: Time to run KeyDiff diff for |SA|= 1M keys and varying difference sizes. We show our measured results in 11a, then
extrapolate the latencies for more constrained network conditions in 11b and 11c.
wise hashing provides better accuracy. Combined as a Difference
Digest, the IBF and Hybrid Estimator provide the best performance
for differences less than 15% of the set size. This threshold changes
with the ratio of bandwidth to computation, but could be estimated
by observing the throughput during the estimation phase to choose
the optimal algorithm.
Our system level benchmarks show that Difference Digests must
be precomputed or the latency for computation at runtime can swamp
the gain in transmission time compared to simple schemes that send
the entire list of keys. This is not surprising as computing a Dif-
ference Digest touches around twelve 32-bit words for each key
processed compared to one 32-bit word for a naive scheme that
sends a list of keys. Thus, the naive scheme can be 10 times faster
than Difference Digests if we do no precomputation. However, with
precomputation, if the set difference is small, then Difference Di-
gest is ten times or more faster. Precomputation adds only a few
microseconds to updating a key.
While we have implemented a generic Key Difference service
using Difference Digests, we believe that a KeyDiff service could
be used by some application involving either reconciliation or dedu-
plication to improve overall user-perceived performance. We hope
the simplicity and elegance of Difference Digests and their appli-
cation to the classical problem of set difference will also inspire
readers to more imaginative uses.
8. ACKNOWLEDGEMENTS
This research was supported by grants from the NSF (CSE-2589
& 0830403), the ONR (N00014-08-1-1015) and Cisco. We also
thank our shepherd, Haifeng Yu, and our anonymous reviewers.
9. REFERENCES
[1] Data domain. http://www.datadomain.com/.
[2] B. Bloom. Space/time trade-offs in hash coding with
allowable errors. Commun. ACM, 13:422–426, 1970.
[3] A. Broder. On the resemblance and containment of
documents. Compression and Complexity of Sequences, ’97.
[4] A. Z. Broder, M. Charikar, A. M. Frieze, and
M. Mitzenmacher. Min-wise independent permutations. J.
Comput. Syst. Sci., 60:630–659, 2000.
[5] J. Byers, J. Considine, M. Mitzenmacher, and S. Rost.
Informed content delivery across adaptive overlay networks.
In SIGCOMM, 2002.
[6] J. W. Byers, M. Luby, M. Mitzenmacher, and A. Rege. A
digital fountain approach to reliable distribution of bulk data.
In SIGCOMM, 1998.
[7] G. Cormode and S. Muthukrishnan. What’s new: ﬁnding
signiﬁcant differences in network data streams. IEEE/ACM
Trans. Netw., 13:1219–1232, 2005.
[8] G. Cormode, S. Muthukrishnan, and I. Rozenbaum.
Summarizing and mining inverse distributions on data
streams via dynamic inverse sampling. VLDB ’05.
[9] D. Eppstein and M. Goodrich. Straggler Identiﬁcation in
Round-Trip Data Streams via Newton’s Identities and
Invertible Bloom Filters. IEEE Trans. on Knowledge and
Data Engineering, 23:297–306, 2011.
[10] L. Fan, P. Cao, J. Almeida, and A. Broder. Summary cache: a
scalable wide-area web cache sharing protocol. IEEE/ACM
Transactions on Networking (TON), 8(3):281–293, 2000.
[11] J. Feigenbaum, S. Kannan, M. J. Strauss, and
M. Viswanathan. An approximate L1-difference algorithm
for massive data streams. SIAM Journal on Computing,
32(1):131–151, 2002.
[12] P. Flajolet and G. N. Martin. Probabilistic counting
algorithms for data base applications. J. of Computer and
System Sciences, 31(2):182 – 209, 1985.
[13] M. T. Goodrich and M. Mitzenmacher. Invertible Bloom
Lookup Tables. ArXiv e-prints, 2011. 1101.2245.
[14] P. Indyk and R. Motwani. Approximate nearest neighbors:
towards removing the curse of dimensionality. STOC, 1998.
[15] M. Karpovsky, L. Levitin, and A. Trachtenberg. Data
veriﬁcation and reconciliation with generalized error-control
codes. IEEE Trans. Info. Theory, 49(7), july 2003.
[16] P. Kulkarni, F. Douglis, J. Lavoie, and J. M. Tracey.
Redundancy elimination within large collections of ﬁles. In
USENIX ATC, 2004.
[17] Y. Minsky, A. Trachtenberg, and R. Zippel. Set reconciliation
with nearly optimal communication complexity. IEEE Trans.
Info. Theory, 49(9):2213 – 2218, 2003.
[18] R. Motwani and P. Raghavan. Randomized Algorithms.
Cambridge Univ. Press, 1995.
[19] S. Muthukrishnan. Data streams: Algorithms and
applications. Found. Trends Theor. Comput. Sci., 2005.
[20] R. Schweller, Z. Li, Y. Chen, Y. Gao, A. Gupta, Y. Zhang,
P. A. Dinda, M.-Y. Kao, and G. Memik. Reversible sketches:
enabling monitoring and analysis over high-speed data
streams. IEEE/ACM Trans. Netw., 15:1059–1072, 2007.
[21] B. Zhu, K. Li, and H. Patterson. Avoiding the disk bottleneck
in the data domain deduplication ﬁle system. In FAST’08.