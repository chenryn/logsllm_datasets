Xt
∇ θlogp(τ t|θ)= ∇ θlogπ θ(A t′|S t′). (2.110)
t′=0
从而
2 3
XT Xt
∇ θJ(π θ)=E τ∼πθ4 R t∇ logπ θ(A t′|S t′)5
θ
t=0 t′=0
93
第2章 强化学习入门
2 3
XT XT
=E τ∼πθ4 ∇ θlogπ θ(A t′|S t′) R t5 . (2.111)
t′=0 t=t′
这里最后一个等式是加法重排（RearrangingtheSummation）。
注意，我们在以上推导过程中使用了加法和期望之间的置换，以及期望和加法与求导之间的
置换（都是合理的），如下：
2 3
  XT XT
∇ θJ(π θ)=∇ θE τ∼πθ R(τ) =∇ θE τ∼πθ4 R t5 = ∇ θE τ∼πθ[R t] (2.112)
t=0 t=0
其最终在式(2.106)中对长度为t+1的部分轨迹τ 进行积分。然而，也有其他方式来对整个轨迹
t
的累计奖励取期望：
∇ θJ(π θ)=∇ θE Zτ∼πθR(τ) (2.113)
=∇ p(τ|θ)R(τ) 展开期望 (2.114)
θ
Z τ
= ∇ p(τ|θ)R(τ) 对换梯度和积分 (2.115)
θ
Zτ
= p(τ|θ)∇ logp(τ|θ)R(τ) 对数-导数技巧 (2.116)
θ
τ
=E τ∼πθ[ 2∇ θlogp(τ|θ)R(τ)] 回归期望形式 (2.117)
3
XT
⇒∇ θJ(π θ)=E τ∼πθ4 ∇ θlogπ θ(A t|S t)R(τ)5 (2.118)
t=0
2 3
XT XT
=E τ∼πθ4 ∇ θlogπ θ(A t|S t) R t′5 (2.119)
t=0 t′=0
仔细的读者可能注意到式(2.119)的第二个结果与式(2.111)的第一个结果有一些差别。具体
P
来说，累计奖励的时间范围是不同的。第一个结果只使用了动作A 之后的累计未来奖励 T R
P t t=t′ t
来评估动作，而第二个结果使用整个轨迹上的累计奖励 T R 来评估该轨迹上的每个动作A ，
t=0 t t
包括选择那个动作之前的奖励。直觉上，一个动作不应该用这个动作执行以前的奖励值来对其进
行估计，而这也得到数学上的证明，即这个动作之前的奖励对最终期望梯度只有零影响。因此可
以在推导策略梯度的过程中直接丢掉那些过去的奖励值来得到式(2.111)，而这被称为“将得到的
奖励（Reward-to-Go）”策略梯度。这里我们不给出两种策略梯度公式等价性的严格证明，感兴趣
的读者可以参考相关资料。这里的两种导出方式也可以作为两个结果等价性的论证。
上述公式中的∇称“nabla”，它是一个物理和数学领域有着三重意义的算子（梯度、散度、
94
2.7 策略优化
和旋度），依据它做操作的对象而定。而在计算机领域，这个“nabla”算子 ∇ 通常用作偏微分
（PartialDerivative），其对紧跟的对象中显式（Explicitly）包含的变量进行求导，而这个变量写在
算子脚标的位置。由于上式中的R(τ)不显式包含θ，因此∇ 不作用于R(τ)，尽管τ 可以隐式
θ
（Implicitly）依赖于θ（根据MDP的图模型）。我们也注意到式(2.119)的期望可以用采样均值来
估计。如果我们收集一个轨迹的集合D ={τ i} i=1,···,N，而其中的轨迹是通过使智能体以策略π
θ
在环境中做出动作来得到的，那么策略梯度可以用以下方式估计：
XXT
1
gˆ= ∇ logπ (A |S )R(τ), (2.120)
|D| θ θ t t
τ∈Dt=0
EGLP（ExpectedGrad-Log-Prob）引理在策略梯度优化中经常用到，所以我们在这里介绍它。
引理2.2 EGLP引理：假设p 是随机变量x的一个参数化的概率分布，那么有
θ
E x∼pθ[∇ θlogP θ(x)]=0. (2.121)
证明: 由于所有概率分布都是归一化的：
Z
p (x)=1. (2.122)
θ
x
对上面归一化条件两边取梯度：
Z
∇ p (x)=∇ 1=0. (2.123)
θ θ θ
x
使用对数-导数技巧得到：
Z
0=∇ p (x) (2.124)
θ θ
Z x
= ∇ p (x) (2.125)
θ θ
Zx
= p (x)∇ logp (x) (2.126)
θ θ θ
x
∴0=E x∼pθ[∇ θlogp θ(x)]. (2.127)
从EGLP引理我们可以直接得出：
E At∼πθ[∇ θlogπ θ(A t|S t)b(S t)]=0. (2.128)
其中b(S )称为基准（Baseline），而它是独立于用于求期望值的未来轨迹的。基准可以是任何一
t
95
第2章 强化学习入门
个只依赖当前状态的函数，而不影响优化公式中的总期望值。
上面公式中的优化目标最终为
2 3
XT
∇ θJ(π θ)=E τ∼πθ4 ∇ θlogπ θ(A t|S t)R(τ)5 (2.129)
t=0
我们也可以更改整个轨迹的奖励R(τ)为在t时间步后将得到的奖励G ：
t
2 3
XT
∇ θJ(π θ)=E τ∼πθ4 ∇ θlogπ θ(A t|S t)G t5 (2.130)
t=0
通过以上EGLP引理，期望回报可以被推广为
2 3
XT
∇ θJ(π θ)=E τ∼πθ4 ∇ θlogπ θ(A t|S t)Φ t5 (2.131)
t=0
P
其中Φ = T t′=t(R(S t′,a t′,S t′+1)−b(S t))。
t
为了便于实际使用，Φ 可以变成以下形式：
t
Φ =Qπθ(S t,A t) (2.132)
t
或
Φ =Aπθ(S t,A t)=Qπθ(S t,A t)−Vπθ(S t) (2.133)
t
而它们都可以证明等价于期望内的原始形式，只是在实际中有不同的方差。这些证明需要重复期
望规则（theLawofIteratedExpectations）：对两个随机变量（离散或连续）有E[X]=E[E[X|Y]]。
而这个式子很容易证明。剩余的证明如下：
2 3
XT
∇ θJ(π θ)=E τ∼πθ4 ∇ θlogπ θ(A t|S t)R(τ)5 (2.134)
t=0
XT
= E τ∼πθ[∇ θlogπ θ(A t|S t)R(τ)] (2.135)
t=0
XT
= E τ:t∼πθ[E τt:∼πθ[∇ θlogπ θ(A t|S t)R(τ)|τ :t]] (2.136)
t=0
96
2.7 策略优化
XT
= E τ:t∼πθ[∇ θlogπ θ(A t|S t)E τt:∼πθ[R(τ)|τ :t]] (2.137)
t=0
XT
= E τ:t∼πθ[∇ θlogπ θ(A t|S t)E τt:∼πθ[R(τ)|S t,A t]] (2.138)
t=0
XT
= E τ:t∼πθ[∇ θ(logπ θ(A t|S t))Qπθ(S t,A t)] (2.139)
t=0
其中E τ[·]=E τ:t[E τt:[·|τ :t]]且τ =(S 0,A 0,··· ,S t,A t)和Qπθ(S t,A t)=E τt:∼πθ[R(τ)|S t,A t]。
:t
所以，文献中有常见的形式：
2 3
XT
∇ θJ(π θ)=E τ∼πθ4 ∇ θ(logπ θ(A t|S t))Qπθ(S t,A t)5 (2.140)
t=0
或
2 3
XT
∇ θJ(π θ)=E τ∼πθ4 ∇ θ(logπ θ(A t|S t))Aπθ(S t,A t)5 (2.141)
t=0
换句话说，它等价于改变优化目标为 J(π θ) = E τ∼π[Qπθ(S t,A t)] 或 J(π θ) = E τ∼π[Aπθ(S t, A t)]
来替换原始形式E τ∼π[R(τ)]。对于优化策略来说，实践中常用Aπθ(S t,A t)来估计TD-误差（TD
Error）。
根据是否使用环境模型，强化学习算法可以被分为无模型（Model-Free）和基于模型的（Model-
Based）两类。对于无模型强化学习，单纯基于梯度的优化算法可以追溯至REINFORCE算法，或
称策略梯度算法。而基于模型的强化学习算法一类，也有一些基于策略的算法，比如使用贯穿时
间的反向传播（BackpropagationThroughTime，BPTT）来根据一个片段内的奖励去更新策略。
例子：REINFORCE算法
REINFORCE 是一个使用式 (2.131) 的随机性策略梯度方法的算法，其中 Φ = Qπ(S ,A )，
P t Pt t
∞ ∞
而在REINFORCE中，它通常用轨迹上采样的奖励值G = t′=tR t′（或折扣版本G =
t t t′=t
γt′−tR
t′）来估计。更新策略的梯度为
2 3
X∞ X∞
g =E4 R t′∇ θlogπ θ(A t|S t)5 (2.142)