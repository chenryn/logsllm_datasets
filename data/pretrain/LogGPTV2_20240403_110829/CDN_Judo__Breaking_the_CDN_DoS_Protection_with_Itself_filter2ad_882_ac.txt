and the :path header ﬁeld, that contribute to the bandwidth
ampliﬁcation ratio.
• Huffman encoding: In the HPACK compression mech-
anism, Huffman encoding is applied to further compress the
header values. This Huffman code is statistically generated for
HTTP headers, wherein ASCII digits and lowercase letters are
given shorter encoding [49]. The shortest encoding for one
byte is 5 bits long; therefore, the highest compression ratio
achievable for one byte is 8 bits : 5 bits Huffman code (37.5%
smaller). Thus, besides the concurrent streams, Huffman en-
coding can also be abused to maximize the ampliﬁcation
ratio. Because HTTP/2 headers are ﬁrstly compressed with
Huffman encoding and will be decompressed in the CDN–
origin connections, the resulting HTTP/1.1 headers will be
nearly 8/5 = 160% larger in size.
Therefore, in our experiments, the two cookie values are
composed of the characters 0, 1, 2, a, c, e, i, o, s, or t,
which have the shortest Huffman encoding (5 bits) deﬁned
in the RFC [49]. With Huffman encoding, we achieve the
ampliﬁcation ratios listed in Table IV.
5
(FB + 7n) (n HTTP/2 streams, 8 − 1 = 7 B larger in each
stream), and the network trafﬁc of the CDN–origin connections
will be (BB + 6n), (n HTTP/1.1 connections, 7 − 1 = 6 B
larger in each connection). Thus, for the :path: /?xxxyy
form, the ampliﬁcation ratio will be (BB + 6n)/(FB + 7n).
As we have illustrated, FB is in the order of thousands,
whereas BB is in the order of millions. Therefore, we have the
following mathematical inequality:
BB
F B
>
BB + 6n
F B + 7n
.
(1)
For example, assuming F B = 5, 000 and BB = 600, 000
for simplicity, when we use 128 or 256 for n (the number of
concurrent streams), the inequality becomes
BB
F B
=
600000
5000
>
600000+128 ∗ 6
5000+128 ∗ 7
= 101.9 >
600000+256 ∗ 6
5000+256 ∗ 7
= 88.6. (2)
Therefore, we can see that,
to achieve the maximum
ampliﬁcation ratio, the HTTP/2 attacking requests should be
specially crafted to use the HPACK indexing mechanism as
much as possible.
Summary. For the attack, we conducted a controlled ex-
periment to obtain the network trafﬁc ampliﬁcation ratio by
establishing just one HTTP/2 connection with one CDN node.
However, from the perspective of an attacker as a client, he
can initiate thousands of HTTP/2 connections with different
CDN nodes (e.g., we found 128,906 CloudFront IPs, which
can be used for the attack; please refer to Table IX for the
number of IPs of other CDNs). According to the ampliﬁcation
the network bandwidth of CDN–origin
ratio we obtained,
connection can be seriously consumed, adversely inﬂuencing
the performance of the origin.
Given that HTTP/2 support is turned on by default across
ﬁve of these six CDNs, and cannot even be turned off across
three of the CDNs, we can see that this threat is severe and
affects all websites hosted on these CDNs.
IV. PRE-POST SLOW HTTP ATTACK
In this section, we introduce the pre-POST slow HTTP
attack, which leverages CDN infrastructure to perform a DoS
the origin. Compared with traditional DoS
attack against
attacks that rely on massive bots [1], [43], [51], this attack is
stealthier and harder for the origin to defend against, because
the crafted requests are legal and are initiated from the CDN.
A. Attack Surface Analysis
The pre-POST slow HTTP attack aims to exhaust
the
connection limits of the origin and starve other legitimate
user requests. To the origin, the attack acts the same as a
traditional slow POST attack [23], [55]. Normally, as the
CDN decouples the client–CDN (including attacker–CDN) and
CDN–origin connections, the CDN naturally defends against
traditional slow POST attacks. However, with experiments, we
ﬁnd that three out of the six CDNs start forwarding HTTP
POST requests just upon receiving the POST header, without
waiting for the whole POST message body. We reveal that this
pre-POST behavior empowers an attacker to keep the CDN–
origin connections to remain open as long as possible, thus
allowing the attacker to exhaust the connection limits of the
origin. In this section, we ﬁrst review the traditional slow
HTTP attack, and then we further analyze how three out of
the six CDNs are susceptible to this pre-POST threat.
Primer on Slow HTTP DoS Attack. According to the
Kaspersky Q4 2018 Intelligence Report [47], the total duration
of HTTP-related attacks has been growing, accounting for
about 80 percent of DDoS attack time for the whole year. This
report ﬁnding reveals that attackers are turning to sophisticated,
mixed HTTP attack techniques, such as slow HTTP DoS
attacks.
Compared with brute-force ﬂooding attacks, a slow HTTP
DoS attack is stealthier and more efﬁcient. The slow HTTP
DoS attack takes advantages of the HTTP protocol having been
designed to keep the connection open until the receiving of
data is ﬁnished [23], [55]. Therefore, different stages of the
request ﬂow can be abused to launch slow HTTP DoS attacks.
A slow Header attack sends the partial header, a slow Read
attack intentionally receives response data slowly, and a slow
POST attack sends the posted data at an alarmingly slow rate.
All these attacks aim to keep massive connections with the
target server for as long as possible, leading to an exhaustion
of the concurrent connections of the target and starving other
normal user requests [29], [48], [56].
Attack Principle. Generally, to prevent unavailability due to
DoS attacks, the CDN decouples attacker–CDN and CDN–
origin connections and absorbs any ﬂooding trafﬁc. However,
the applicability of slow HTTP DoS attack against CDN-
powered websites remains under-studied.
With our further analysis and real-world experiments, we
ﬁnd that each of the six CDNs forwards requests only until it
receives the full HTTP header, and is therefore able to defend
against slow Header attacks. Furthermore, when forwarding
an HTTP GET request, the CDN–origin transmission is in-
dependent of the attacker client–CDN transmission; therefore,
the CDN is able to stop slow Read attacks.
to the origin. For simplicity,
However, we ﬁnd that CDNs present two different POST-
forwarding behaviors. When a CDN receives a POST request
for the origin, the CDN faces the choice of when to forward
the POST request
the CDN
can forward the POST request only after it ﬁnishes receiving
the whole POST message. However, the POST request may
contain a large-sized message body, which would take a long
time to receive and therefore delay the request forwarding. The
CDN can also start forwarding the POST request just upon ﬁn-
ishing receiving the POST request header and then sequentially
forward the subsequently received POST message within the
same HTTP connection. This pre-POST-forwarding behavior
can certainly facilitate the origin into receiving the POST
request earlier; however, it also enables an attacker to keep
the CDN–origin connections open for as long as possible.
B. Real-World Attack Analysis
Experiment Setup. In our experiment, we set up a self-built
Apache web-server and deploy it as a website origin behind
the six CDNs, one at a time. The concurrent connections limit
of the Apache web server is conﬁgured with a default value
of 1000 [3].
6
From the view of an attacker, we craft POST requests
to explore the request-forwarding behaviors of the CDNs. In
particular, to POST a large message, the attacker can specify
the size directly in the Content-Length header ﬁeld, or use
Chunked-Encoding to send dynamically generated data,
both aiming to send the POST message slowly. Here, for sim-
plicity, we specify the exact size of the HTTP message body
with the Content-Length ﬁeld, and the POST message
body is sent quite slowly, taking 300 s to ﬁnish transmission.
POST /login.php? HTTP/1.1
Host: www.victim.com
Content-Length: 300
0101..... (300 bytes, 1 byte sent per second)
At the same time, at the website origin, we use the tool
tcpdump to capture the timestamp (relative to our request
sending time) upon receiving the CDN-forwarded HTTP POST
request, and how long the CDN–origin connection is kept
open. After sending 1000 concurrent POST requests, and
repeating this procedure for 30 times, we obtain the averaged
results shown in Table VI.
TABLE VI: Time data from sending slow POST requests
(lasting 300 s). Three CDNs start forwarding POST requests
as soon as they receive the POST header.
CloudFront
Cloudﬂare
CDNSun
Fastly
KeyCDN MaxCDN
Fig. 6: Establishing more than 1000 connections, from 100 s
to 400 s.
Request
Receiving Time
Connection
Keep-open Time
0.87s
298.89s
300.29s
299.92s
0.55s
299.79s
0.12s
0.34s
299.32s
0.37s
0.74s
15.01s
Fig. 7: Response time of a normal client during a slow HTTP
POST attack.
We can see that CloudFront and Fastly start to forward
POST requests as soon as they receive the forwarding request
header, whereas CDNSun, KeyCDN, and Cloudﬂare start to
forward a POST request only after receiving the whole mes-
sage. MaxCDN also starts to forward POST requests 0.74
s later but aborts the connection when the kept-open time
exceeds 15 s.
Apparently, for CloudFront, Fastly, and MaxCDN,
the
kept-open time of the CDN–origin connection depends on
the kept-open time of the client–CDN connection, which is
directly under the control of the client, and thus of a potential
attacker. Therefore, this pre-POST-forwarding behavior can be
leveraged to launch a slow HTTP DoS attack: an attacker can
establish and maintain hundreds or even thousands of these
POST connections concurrently, leveraging the CDN (and thus
adversely affecting the origin). It will quickly exhaust all the
connection resources of the origin and starve other normal
requests, breaking the DoS protection given by the CDN.
Experiment Results. Further, we evaluate such pre-POST
attack against our self-built origin web server (with a connec-
tion limit of 1000), through CloudFront, Fastly, and MaxCDN
for 300 s. From another vantage point, as a normal client,
we periodically measure the client–CDN–origin request delay
every 5 s to probe whether the connection resources of the
origin are exhausted or not.
For CloudFront and Fastly, to exhaust the 1000-connection
limit of our origin, we concurrently send 1100 slow POST
requests to the CDN, as shown in Fig. 6. At the origin, the
connection resources are exhausted, and other requests are
starving. Thus, as shown in Fig. 7, the request delays of a
normal client rise to 90 s for CloudFront (returns HTTP 504
Gateway Time-out) and 15 s for Fastly (returns HTTP 503
Service Unavailable), demonstrating the success of the DoS
attacks.
Because MaxCDN will abort the POST connection after 15
s, we periodically start 100 new concurrent connections every
second during the attack period. As shown in Fig. 6, the con-
nection number ﬂuctuates at around 1500, as MaxCDN aborts
the previous 15-s-lasting connections sequentially. Meanwhile,
as shown in Fig. 7,
the request delay of a normal client
ﬂuctuates below 15 s, as the normal client request competes
with attacking requests for the released connection resources.
This phenomenon of MaxCDN demonstrates a quality of
service (QoS) attack, which aims to degrade performance
rather than completely disable the service.
HTTP/2 Pre-POST Attack. Given that CDNs support HTTP/2
in client–CDN connections (as explained in Section III), we
also further evaluate slow HTTP/2 POST attacks against the
origin. To employ the multiplex stream feature of HTTP/2, we
establish 10 simultaneous HTTP/2 connections with the CDN
and send 100 POST requests in each HTTP/2 connection. The
POST requests are crafted as follows:
:method: POST
:scheme: https
:authority: www.victim.com
:path: /login.php?cdn=&a=-
Content-Length: 300
0101..... (300 bytes, 1 byte sent per second)
7
TABLE VII: Time data from sending slow HTTP/2 POST
requests (lasting 300 s). Three CDNs start POST request
forwarding as soon as they receive the POST header.
CloudFront
Cloudﬂare
CDNSun
Fastly
KeyCDN
MaxCDN
Request
Receiving Time
Connection
Keep-alive Time
0.42342s
300.82689s
300.47039s
300.48742s
0.21612s
3.22843s
1.42386s(10)
300.50451s(990)
299.41059s(10)
0.84946s(990)
300.49957s
0.91270s
3.08003s
15.01520s
As shown in Table VII, we obtain the same POST for-
warding behaviors as in HTTP/1.1, except for Fastly. The
result reveals that Fastly starts the pre-POST forwarding of
the ﬁrst request for each connection, with 10 CDN–origin
connections having an average kept-open time of 299.41059
s. Meanwhile, the subsequent POST requests within the same
connection are queued in Fastly for 300 s, during which Fastly
has to ﬁnish receiving the subsequent whole POST message,
resulting in 990 CDN–origin connections having an average
kept-open time of 0.84946s. We presume the reason for this
phenomenon is that Fastly maintains a POST request queue for
each HTTP/2 connection, and thus subsequent POST requests
are to be forwarded only after the foremost POST request has
been ﬁnished.
To the target origin,