title:I Know What You See: Power Side-Channel Attack on Convolutional Neural
Network Accelerators
author:Lingxiao Wei and
Bo Luo and
Yu Li and
Yannan Liu and
Qiang Xu
9
1
0
2
v
o
N
9
2
]
V
C
.
s
c
[
2
v
7
4
8
5
0
.
3
0
8
1
:
v
i
X
r
a
I Know What You See: Power Side-Channel Attack on
Convolutional Neural Network Accelerators
Lingxiao Wei†, Bo Luo†, Yu Li†, Yannan Liu†‡ and Qiang Xu†
†CUhk REliable Computing Laboratory (CURE)
Department of Computer Science and Engineering
The Chinese University of Hong Kong, Shatin, N.T., Hong Kong
‡Sangfor Technologies Inc., Shenzhen, China
Email: {lxwei, boluo, yuli, ynliu, qxu}@cse.cuhk.edu.hk
ABSTRACT
Deep learning has become the de-facto computational paradigm
for various kinds of perception problems, including many privacy-
sensitive applications such as online medical image analysis. No
doubt to say, the data privacy of these deep learning systems is
a serious concern. Different from previous research focusing on
exploiting privacy leakage from deep learning models, in this paper,
we present the first attack on the implementation of deep learning
models. To be specific, we perform the attack on an FPGA-based
convolutional neural network accelerator and we manage to recover
the input image from the collected power traces without knowing
the detailed parameters in the neural network. For the MNIST
dataset, our power side-channel attack is able to achieve up to 89%
recognition accuracy.
CCS CONCEPTS
• Security and privacy → Side-channel analysis and counter-
measures; • Hardware → Hardware accelerators;
KEYWORDS
Power side-channel attack, convolutional neural accelerators, pri-
vacy leakage
ACM Reference Format:
Lingxiao Wei†, Bo Luo†, Yu Li†, Yannan Liu†‡ and Qiang Xu†. 2018. I Know
What You See: Power Side-Channel Attack on Convolutional Neural Net-
work Accelerators. In 2018 Annual Computer Security Applications Conference
(ACSAC ’18), December 3–7, 2018, San Juan, PR, USA. ACM, New York, NY,
USA, 14 pages. https://doi.org/10.1145/3274694.3274696
1 INTRODUCTION
Deep neural network (DNN) is widely used in many safety-critical
and security-sensitive artificial intelligence (AI) applications such
as biometric authentication, autonomous driving, and financial
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-6569-7/18/12...$15.00
https://doi.org/10.1145/3274694.3274696
fraud analysis. Consequently, their security is a serious concern
and requires urgent research attention.
Recent research has shown the security and privacy of DNN
models can be compromised. In [27], attackers deceive the DNN
system to make misclassifications by adding small perturbation to
the original images. In [7], malicious parties are able to recover
private images that are used to train a face recognition system by
analyzing the outputs of the DNN model. However, the success
of these attacks requires the full knowledge of the DNN model,
which is not usually available in real-world applications as model
parameters are valuable assets for deep learning tasks and are
always kept confidential. In addition, a variety of privacy-leaking
prevention techniques [1, 24, 28] has emerged to mitigate these
attacks.
Figure 1: Deep learning flow: at training stage, a training al-
gorithm generates a prediction model from a large number
of training examples, while at inference stage, a inference
engine predict the result from real-world inputs with the
trained model. (1) existing research assumes attacks on DNN
models (2) our approach attacks the hardware of inference
engine.
As illustrated in Fig. 1, apart from the privacy leakage from DNN
models, the information of real-world inputs can also be leaked
at inference stage which remains largely unexplored before. In
privacy-sensitive systems, the direct access to the input data of
the inference engine is often strictly controlled so that it is nearly
impossible for potential adversaries to retrieve private informa-
tion in a stealthy manner. For instance, encrypted medical images
are provided to the DNN inference engine and they are decrypted
inside the inference engine to prevent eavesdropping on the com-
munication paths. Under such circumstances, we are concerned
Training Data SetTraining AlgorithmInference EnginePredicted ResultReal-world InputsTraining StageInference Stage(1)(2)ModelACSAC ’18, December 3–7, 2018, San Juan, PR, USA
L. Wei et al.
whether attackers can use side channel information (e.g., power
consumption) to retrieve private data.
Dedicated DNN accelerators are expected to gain mainstream
adoption in the foreseeable future due to their high computation
efficiency [35]. In this paper, we present a power side-channel attack
on an FPGA-based convolutional neural network (CNN) accelerator
which performs the task of image classification. The attack target
on the hardware component executing the convolution operation in
the first layer of CNN which is usually implemented by line buffer,
a common structure in many image-related processing hardware.
The primary objective of our attack is to recover the private input
image from eavesdropping the power consumption of the neural
accelerator when it performs calculations for the first layer.
To the best of our knowledge, the proposed attack is the first
one that exploits the privacy leakage in neural accelerators using
power side channels. Particularly, unlike previous privacy attacks
targeting at reproducing samples in the training set using model
outputs [7], our proposed attack aims to recover the input being
inferenced and we do not assume the pre-requisite knowledge of
model parameters or model outputs. The main contributions of our
work include:
• Power side channel is often quite noisy, and the collected
power trace contains distortions brought by various circuit
components. We present a novel power extraction technique
to precisely recover the power consumption for each clock
cycle.
• We develop novel algorithms to retrieve each pixel value of
the input image. To be specific, as the convolution operation
only relates to a limited number of pixels, we develop algo-
rithms to infer the values of pixels either from power traces
directly or from a pre-built power-pixel template. Finally,
the image can be reconstructed by piecing all inferred pixels
together.
The remainder of this paper is organized as follows: Section 2
introduces the background knowledge with threat model follows
in Section 3. Next, we give an overview on the proposed attack
flow in Section 4 for two attack scenarios. We introduce how to
accurately estimate the power from the noisy power side channels in
Section 5. The details of the two attack scenarios are then introduced
in Section 6 and in Section 7, respectively. Finally, we discuss related
work in Section 8 and conclude this work in Section 9. In addition,
to illustrate the preliminaries of our proposed attack more clearly,
we give a detailed introduction of the concept of convolutional
neural network and the design of neural accelerators. Also, we
discuss the power measurement setup and characterization of the
neural accelerators in Section A in the Appendix. Then we talk
about the limitation and countermeasures in Appendix B and give
the results of our attack on the MNIST dataset in Appendix C.
2 BACKGROUND
In this section, we give a brief introduction to convolutional neural
network, neural accelerators and the power characterization of the
accelerator, respectively. For more information, readers can refer
to Section A in the Appendix.
Convolutional Neural Network: Convolutional neural network
has been the prevalent network architecture in image-related pro-
cessing tasks. It is constructed by a sequence of layers where the
first few layers perform the convolutional operation. In this attack,
we focus on the first layer of the convolutional neural network
whose input is the raw image and the computation in this layer is
convolution. Generally speaking, convolution layer uses a 2-D filter
(i.e., kernel) to slide over the 2-D input image (e.g., gray image)
and finally produces another 2-D image (i.e., feature map) using
the convolution operation. Readers can resort to Fig. 9 (a) for an
illustration of the convolution operation.
Neural Accelerator: The computation in the convolution neural
network is usually implemented by neural accelerators on devices
with stringent power budget. They are often based on FPGA or
ASIC, and there are many design available in both academia and
industry [5, 29, 38]. Our target is the convolutional layer, whose
functionality is usually implemented by the line buffer. Line buffer
is composed of buffer lines and a computation unit: buffer lines
are used to cache the pixels in recent lines while the computation
unit calculates the convolution result given the pixels and filter
values. In this paper, we follow the design proposed by Zhao et
al [39]. They implement an accelerator for a compressed version of
CNN [12] on FPGA.
Power characterization of neural accelerator: To accurately
estimate the power constitution of line buffer, we simulated our
design with Xilinx XPower analyzer and discovered the power in the
convolution unit occupied more than 80% of the total consumption
regardless of the configuration of line buffer. Reader may refer to
the appendix for a complete comparison among different line buffer
configurations. Therefore, we can regard the measured power as a
coarse-grain estimate for the power of convolution unit.
3 THREAT MODEL
Scenario: The primary goal of adversaries is to recover the input
that fed into a convolutional neural accelerator which contains
sensitive private information. Instead of reconstructing the samples
in the training set, the adversaries try to reproduce the online input
when the neural accelerator is actually performing the inference
operation. To facilitate the attack, we consider the adversaries come
from the DNN accelerator design team or they are insiders in the
companies hosting DNN infrastructures so that they are capable to
monitor the power side channel. At inference stage, DNN model
designers usually deploy their trained model on a machine-learning
service operator, such as BigML [13] or Microsoft Azure [14], who
uses dedicated DNN accelerators for the inference operation. They
may also put their models on computing platforms (e.g., Qual-
comm’s Snapdragon 835 [30]) with DNN-accelerating hardware. In
many applications, for privacy concerns, the inputs to the DNN
accelerators are often protected with strict access control policies
or strong encryption schemes. For instance, they may adopt secure
processors, like AEGIS [34], to keep the incoming data confiden-
tial both in memory and on the disk. Thus, for attackers it is very
difficult to obtain the inputs directly. However, the side channels,
especially the power side channel, are exposed unprotected to ma-
licious insiders in the host of machine learning service company
and DNN accelerator design team. They are capable to access to
Power Side-Channel Attack on Convolutional Neural Network Accelerators
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
Figure 2: Overview of the attack flow.
the power side channel output via malicious implanted Trojans or
measurement circuits when the accelerator is actually running with
real-world users’ inputs.
Capability: Firstly, we assume attackers are knowledgeable about
the structure of the neural network and the input image size, but
not the detailed parameters in the network. To be specific, for the
targeted first convolution layer, the attackers need to know its filter
size, number of input feature maps, and number of output feature
maps. Secondly, adversaries can acquire the power trace of the DNN
accelerator in high resolution either by oscilloscope measurement
or power-monitoring Trojan. We consider these two assumptions
practical because firstly, many image-related tasks adopt existing
neural network architecture (e.g., VGG-16/19 and ResNet) whose
structure (including number of layers and configurations of each
layer) is fixed and public and secondly, from the perspective of insid-
ers, it is easy to implant Trojans or measurement circuits to get the
power traces at runtime. Thirdly, according to the ability of launch-
ing inference operation freely, we further divide the adversaries
into two categories: passive adversary and active adversary. Passive
adversary can only eavesdrop on the power consumption when
an input is processed by the DNN accelerator at inference stage.
Active adversary has an extra capability of profiling the relation-
ship between power and input pixels by freely launching inference
operation with arbitrary inputs on the targeted accelerator. The
profiling phase can only be carried out prior to any actual calcu-
lation of user’s private data. The main difference between passive
and active attackers is the time that they get access to the power
channel of the accelerator. We regard both attackers are realistic as
only if they are physically accessible to the accelerator.
power side channel attacks. We setup a Tektronix MDO3034 oscil-
loscope [36], with a sampling frequency of 2.5GHz, to acquire the
power trace from the FPGA board.
For passive and active adversaries, we propose attack methods
for them separately. The whole attack flow is illustrated in Fig. 2.
In the first step, we collect the power traces of the FPGA when it
performs the convolution with different kernels. Then we adopt
an extraction algorithm to filter out noise and get the real power
consumption, whose details will be shown in Section 5. After the
power extraction stage, passive adversaries try to locate pixels
belonging to image background from the extracted power. Then the
silhouette of foreground objects is revealed. The details of this attack
are shown in Section 6. For active adversaries, before the actual
attack, they build a “power template” [32] using the power measured
with different kernels and the input image. The power template
exploits the relations between power consumption and pixel values
and can generate a set of pixel candidates when queried with power
consumption in actual attacks. The final step for active adversaries
is to recover the image by selecting the best pixel candidate from
the generated set. Section 7 introduces the power template attacks
in detail.
We conducted experiments with images in the MNIST dataset [21],
a dataset for handwritten digits recognition. We try to recover the
image with both background detection and power template, shown
in Fig. 11 in Section C of the Appendix. For images from background
detection, the general shape of the original image is retained while
the images recovered with power template retain more details and
they are more similar to the original images in visual effect.
4 OVERVIEW
The primary goal in this paper is to recover the input image from
power traces of the targeted CNN accelerator. The reason we choose
the convolution in the first layer as attack target is as follows:
firstly it directly processes the input image so the power obtained
closely relates to the input. Secondly, the inherent characteristic
of convolution, which performs computation on a small bunch of
pixels, can reduce the effort needed to infer the pixel values.