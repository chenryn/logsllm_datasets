to characterize both burst rates and steady-state throughput rates.
Do different users see different PowerBoost effects? Using BIS-
Mark, we study Comcast’s use of PowerBoost in depth. Accord-
ing to Comcast [9], their implementation of PowerBoost provides
0.00.20.40.60.81.0Avg/P950.00.20.40.60.81.0CDFAT&TComcastTimeWarnerVerizonCharterCoxQwestCablevision0.00.20.40.60.81.0Avg/P950.00.20.40.60.81.0CDFAT&TComcastTimeWarnerVerizonCharterCoxQwestCablevision00h04h08h12h16h20hTime of Day0.40.50.60.70.80.91.0Average normalized throughputAT&TComcastCharterTimeWarnerQwestVerizonCoxCablevision00h04h08h12h16h20hTime of Day0.00.10.20.30.40.5Std. Dev of normalized throughputAT&TComcastCharterTimeWarnerQwestVerizonCoxCablevision00h04h08h12h16h20hTime of Day00.40.81.2Average Loss (%) AT&TComcastCharterTimeWarnerQwestVerizonCoxCablevision140Figure 9: The average throughput obtained during the course of
the measurement goes down signiﬁcantly for the ISPs that enable
PowerBoost. (SamKnows)
(a) PowerBoost download behavior for 4 users.
higher throughput for the ﬁrst 10 MBytes of a download and the
ﬁrst 5 MBytes of an upload. We measure the shaped throughput for
download and upload at the receiver using tcpdump. Because our
tests are intrusive, we conducted them only a few times; however
the results do not vary with choice of trafﬁc generators or ports.
Figure 10 shows the observed throughput for four users for both
download and uploads. All four users see PowerBoost effects, but,
surprisingly, we see many different proﬁles even in such a small
subset of users. Figure 10a shows download proﬁles for each user
(identiﬁed by the modem they use; while the modem doesn’t have
an effect on burst rates, it does have an effect on buffering latencies
as we show in Section 6). The user with a D-LINK modem sees a
peak rate of about 21 Mbits/s for 3 seconds, 18.5 Mbits/s for a fur-
ther ten seconds, and a steady-state rate of 12.5 Mbits/s. The Mo-
torola user sees a peak rate of 21 Mbits/s for about 8 seconds. The
PowerBoost technology [10] provides token buckets working on
both packet and data rates; it also allows for dynamic bucket sizes.
The D-LINK proﬁle can be modeled as a cascaded ﬁlter with rates
of 18.5 Mbits/s and 12.5 Mbits/s, and buffer sizes of 10MBytes
and 1Mbyte respectively, with the line capacity being 21Mbits/s.
We see varying proﬁles for uploads as well, although we only see
evidence of single token buckets (Figure 10b). The D-LINK user
sees about 7 Mbits/s for 8 seconds, Scientiﬁc Atlanta and Thomson
users see about 4 Mbits/s for 20 seconds, and the Motorola user
sees about 3.5Mbits/s for nearly 35 seconds. Because our results
do not vary with respect to the packet size, we conclude that Com-
cast does not currently apply buckets based on packet rates.
Takeaway: Depending on how throughput measurements are
conducted and how long they last, the measurements across users
may vary considerably. Speciﬁcally, any speedtest measurement
that lasts less than 35 seconds will only capture the effects of
PowerBoost in some cases, and any short-term throughput mea-
surement may be biased by PowerBoost rates.
6. UNDERSTANDING LATENCY
We show how latency can drastically affect performance, even
on ISP service plans with high throughput. We then study how var-
ious factors ranging from the user’s modem to ISP trafﬁc shaping
policies can affect latency.
6.1 How (and Why) to Measure Latency
Latency affects the performance that users experience.
It not
only affects the throughput that users achieve, it also affects per-
ceived performance: on a connection with high latency, various op-
erations ranging from resolving DNS queries to rendering content
may simply take longer.
Although latency appears to be a straightforward characteristic
to measure, arriving at the appropriate metric is a subtle challenge
because our goal is to isolate the performance of the access link
(b) PowerBoost upload behavior for 4 users.
Figure 10: The level and duration of the burstiness is different for
users with different modems, suggesting different shaping mecha-
nisms or parameters. (BISMark)
from the performance of the end-to-end path. End-to-end latency
between endpoints is a common metric in network measurement,
but it reﬂects the delay that a user experiences along a wide-area
path. We use two metrics that are more appropriate for access net-
works.
The ﬁrst metric is the last-mile latency, which is the latency to
the ﬁrst hop inside the ISP’s network. This metric captures the la-
tency of the access link, which could affect gaming or short down-
loads. We measure last-mile latency in both of our deployments.
As we show in this section, the last-mile latency is often a domi-
nant factor in determining the end-user performance. The second
metric we deﬁne is latency under load, which is the latency that a
user experiences during an upload or download (i.e., when the link
is saturated in either direction). For BISMark, we measure the last-
mile latency under load; on the SamKnows platform, we measure
latency under load on the end-to-end path.
To investigate the effect of latency on performance, we mea-
sured how the time to download popular Web pages varies for users
with different throughput and latency. Figure 11 shows the down-
load time for www.facebook.com and how it varies by both
the user’s throughput and baseline last-mile latency. Figure 11a
plots the 95th percentile of each user’s downstream throughput
versus the average time it takes to download all objects from
www.facebook.com. The average size of the download is
125 KByte. As expected, the download times decrease as through-
put increases; interestingly, there is negligible improvement beyond
a rate of 6 Mbits/s. Figure 11b plots download time against the
baseline latency for all users whose downstream throughput (95th
percentile) exceeds 6 Mbits/s. Minimum download times increase
by about 50% when baseline latencies increase from 10 ms to
40 ms. The fact that this effect is so pronounced, even for small
downloads, underscores importance of baseline latency.
We investigate the effects of cable and DSL access-link tech-
nologies on last-mile latency, packet loss, and jitter. We also ex-
plore how different DSL modem conﬁgurations, such as whether
the modem has interleaving enabled, affects last-mile latency and
loss. Finally, we study the effect of modem hardware on perfor-
012345TCP session snapshot0.50.60.70.80.91.01.1Avg normalized throughputCablevisionCharterTimeWarnerCoxComcast0510152025Time (seconds)8000120001600020000Throughput (Kbits/s)RCA ThomsonMotorolaDLINKScientific Atlanta010203040Time (seconds)02000400060008000Throughput (Kbits/s)DLINKScientificAtlantaRCA ThomsonMotorola141high, varying from about 10 ms to nearly 40 ms (ranging from
40 − −80% of the end-to-end path latency). Variance is also high.
One might expect that variance would be lower for DSL, since it
is not a shared medium like cable. Surprisingly, the opposite is
true: AT&T and Verizon have high variance compared to the mean.
Qwest also has high variance, though it is a smaller fraction of the
mean. To understand this variance, we divide different users in each
ISP according to their baseline latency, as shown in Figure 12 Most
users of cable ISPs are in the 0–10 ms interval. On the other hand,
a signiﬁcant proportion of DSL users have baseline last-mile laten-
cies more than 20 ms, with some users seeing last-mile latencies as
high as 50 to 60 ms. Based on discussions with network operators,
we believe DSL companies may be enabling an interleaved local
loop for these users.
Table 4 shows loss rates for users across ISPs. The average loss is
small, but variance is high for all ISPs, suggesting bursty loss. Jitter
has similar characteristics, as shown in Table 5; while the average
jitter is low, the variation is high, especially on the upstream, also
suggesting burstiness.
How does interleaving affect last-mile latency? ISPs enable in-
terleaving for three main reasons:
(1) the user is far from the
DSLAM; (2) the user has a poor quality link to the DSLAM; or
(3) the user subscribes to “triple play” services. An interleaved
last-mile data path increases robustness to line noise at the cost of
higher latency. The cost varies between two to four times the base-
line latency.
Takeaway: Cable providers in general have lower last-mile la-
tency and jitter. Baseline latencies for DSL users may vary signiﬁ-
cantly based on physical factors such as distance to the DSLAM or
line quality.
6.3 Latency Under Load
We turn our attention to a problem that has gathered much in-
terest recently because of its performance implications: modem
buffering under load conditions [16]. We conﬁrm that excessive
buffering is a widespread problem afﬂicting most ISPs (and the
equipment they provide). We proﬁle different modems to study
how the problem affects each of them. We also see the possible
effect of ISP policies such as active queue and buffer management
on latency and loss. Finally we explore exploiting shaping mecha-
nisms such as PowerBoost might help mitigate the problem.
Problem: Oversized buffers. Buffers on DSL and cable modems
are too large. Buffers do perform an important role: they absorb
bursty trafﬁc and enable smooth outﬂow at the conﬁgured rate [24].
Buffering only affects latency during periods when the access link
is loaded, but during such periods, packets can see substantial de-
lays as they queue up in the buffer. The capacity of the uplink also
affects the latency introduced by buffering. Given a ﬁxed buffer
size, queuing delay will be lower for access links with higher ca-
pacities because the draining rate for such buffers is higher. We
study the effect of buffering on access links by measuring latency
when the access link is saturated, under the assumption that the last-
mile is the bottleneck. We also present a simple model for modem
buffering and use emulation to verify its accuracy.
How widespread are oversized buffers? Figure 13 shows the
average ratios of latency under load to baseline latency for each
user across different ISPs for the SamKnows data. The histogram
shows the latencies when the uplink and the downlink are saturated
separately. This ﬁgure conﬁrms that oversized buffers affect users
across all ISPs, though in differing intensity. The factor of increase
when the uplink is saturated is much higher than when the downlink
(a) Fetch time stabilizes above
6Mbits/s.
(b) Latency affects fetch times.
Figure 11: Effect of downstream throughput and baseline latency
on fetch time from facebook.com. (SamKnows)
mance. Speciﬁcally, we investigate how oversized modem buffers
that has recently received much attention from both operators and
users [16]—affects interactivity and throughput.
ISP
AT&T
Comcast
TimeWarner
Verizon
Charter
Cox
Qwest
Cablevision
Loss
Last mile latency
Average
25.23
10.36
11.87
12.41
11.87
13.88
39.42
10.21
Std. dev Avg(%)
0.48%
0.27%
0.33%
0.51%
0.43%
1.11%
0.33%
0.33%
33.47
14.49
25.18
20.60
11.80
28.02
32.27
7.52
Std. dev
3.59
2.79
3.09
4.07
3.29
8.35
3.38
3.14
Table 4: Last-mile latency and variation is signiﬁcant; Variation in
loss is high, suggesting bursty losses. (SamKnows)
ISP
AT&T
Comcast
TimeWarner
Verizon
Charter
Cox
Qwest
Cablevision
Downstream
Upstream
Average
1.85
1.15
1.68
1.71
1.17
1.18
3.04
1.69
Std. dev Average
3.02
3.24
3.67
1.97
2.66
4.27
2.16
2.25
7.63
6.37
3.35
5.01
1.66
1.89
12.59
3.52
Std. dev
12.92
6.60
12.52
4.82
7.48
7.10
10.95
1.18
Table 5: Downstream jitter is quite low, however upstream jitter is
signiﬁcant. (SamKnows)
6.2 Last-Mile Latency
We obtain the last-mile latency by running traceroute to a
wide-area destination and extracting the ﬁrst IP address along the
path that is not a NAT address. Note that we are measuring the la-
tency to the ﬁrst network-layer hop, which may not in fact be the
DSLAM or the CMTS, since some ISPs have layer-two DSLAMs
that are not visible in traceroute. This should not be problematic,
since the latency between hops inside an ISP is typically much
smaller than the last-mile latency.
How does access technology affect last-mile latency? Table 4
shows the average last-mile latency experienced by users in the
ISPs included in our study. Last-mile latency is generally quite
1M10M100M95th percentile Download speed (bits/s)050010001500Download Time (ms)51020304050Baseline latency (ms)050010001500Download Time (ms)142Figure 12: The baseline last mile latency for each user is computed as the 10th percentile of the last mile latency. Most users see latencies
less than 10 ms, but there are a signiﬁcant number of users with the last mile latency greater than 10 ms. (SamKnows)
is saturated. One plausible explanation is that the downlink usually