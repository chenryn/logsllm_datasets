doing a hurried release in response to a 
vulnerability disclosure. 
Third-party 
product range 
comparison 
A third party performs 
audits of a number of 
competing products in 
a particular field. 
An objective third party can provide 
valuable information to consumers and 
assist in selecting the most secure product. 
Third-party 
evaluation 
A third party performs 
an independent 
software audit of a 
product for a client. 
The client can gain an understanding of the 
relative security of an application it's 
considering deploying. This might prove to 
be the deciding factor between purchasing 
one technology over another. 
Third-party 
preliminary 
evaluation 
A third party performs 
an independent 
review of a product 
before it goes to 
market. 
Venture capitalists can get an idea of the 
viability of a prospective technology for 
investment purposes. Vendors might also 
conduct this type of evaluation to ensure the 
quality of a product they intend to market. 
Independent 
research 
A security company or 
consulting firm 
Security product vendors can identify 
vulnerabilities and implement protective 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
26 
Table 1-1. Code-Auditing Situations 
Situation 
Description 
Advantage 
performs a software 
audit independently. 
measures in scanners and other security 
devices. Independent research also 
functions as an industry watchdog and 
provides a way for researchers and security 
companies to establish professional 
credibility. 
As you can see, code auditing makes sense in quite a few situations. Despite the 
demand for people with these skills, however, few professionals have the training and 
experience to perform these audits at a high standard. It's our hope that this book 
helps fill that gap. 
Auditing Versus Black Box Testing 
Black box testing is a method of evaluating a software system by manipulating only 
its exposed interfaces. Typically, this process involves generating specially crafted 
inputs that are likely to cause the application to perform some unexpected behavior, 
such as crashing or exposing sensitive data. For example, black box testing an HTTP 
server might involve sending requests with abnormally large field sizes, which could 
trigger a memory corruption bug (covered in more depth later in Chapter 5(? [????.]), 
"Memory Corruption"). This test might involve a legitimate request, such as the 
following (assume that the "..." sequence represents a much longer series of "A" 
characters): 
GET AAAAAAAAAAAAAAAAAAA...AAAAAAAAAAAAAAAAAAA HTTP/1.0 
Or it might involve an invalid request, such as this one (once again, the "..." sequence 
represents a much longer series of "A" characters): 
GET / AAAAAAAAAAAAAAAAAAA...AAAAAAAAAAAAAAAAAAAA/1.0 
Any crashes resulting from these requests would imply a fairly serious bug in the 
application. This approach is even more appealing when you consider that tools to 
automate the process of testing applications are available. This process of automated 
black box testing is called fuzz-testing, and fuzz-testing tools include generic "dumb" 
and protocol-aware "intelligent" fuzzers. So you don't need to manually try out every 
case you can think of; you simply run the tool, perhaps with some modifications of 
your own design, and collect the results. 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
27 
The advantage of black box testing an application is that you can do it quickly and 
possibly have results almost immediately. However, it's not all good news; there are 
several important disadvantages of black box testing. Essentially, black box testing is 
just throwing a bunch of data at an application and hoping it does something it isn't 
supposed to do. You really have no idea what the application is doing with the data, 
so there are potentially hundreds of code paths you haven't explored because the 
data you throw at the application doesn't trigger those paths. For instance, returning 
to the Web server example, imagine that it has certain internal functionality if 
particular keywords are present in the query string of a request. Take a look at the 
following code snippet, paying close attention to the bolded lines: 
struct keyval { 
    char *key; 
    char *value; 
}; 
int handle_query_string(char *query_string) 
{ 
    struct keyval *qstring_values, *ent; 
    char buf[1024]; 
    if(!query_string) 
        return 0; 
    qstring_values = split_keyvalue_pairs(query_string); 
    if((ent = find_entry(qstring_values, "mode")) != NULL) 
    { 
        sprintf(buf, "MODE=%s", ent->value); 
        putenv(buf); 
    } 
    ... more stuff here ... 
} 
This Web server has a specialized nonstandard behavior; if the query string contains 
the sequence mode=, the environment variable MODE is set with the value xxx. This 
specialized behavior has an implementation flaw, however; a buffer overflow caused 
by a careless use of the sprintf() function. If you aren't sure why this code is 
dangerous, don't worry; buffer overflow vulnerabilities are covered in depth in 
Chapter 5(? [????.]). 
You can see the bug right away by examining the code, but a black box or fuzz-testing 
tool would probably miss this basic vulnerability. Therefore, you need to be able to 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
28 
assess code constructs intelligently in addition to just running testing tools and noting 
the results. That's why code auditing is important. You need to be able to analyze 
code and detect code paths that an automated tool might miss as well as locate 
vulnerabilities that automated tools can't catch. 
Fortunately, code auditing combined with black box testing provides maximum 
results for uncovering vulnerabilities in a minimum amount of time. This book arms 
you with the knowledge and techniques to thoroughly analyze an application for a 
wide range of vulnerabilities and provides insight into how you can use your 
understanding and creativity to discover flaws unique to a particular application. 
Code Auditing and the Development Life Cycle 
When you consider the risks of exposing an application to potentially malicious users, 
the value of application security assessment is clear. However, you need to know 
exactly when to perform an assessment. Generally, you can perform an audit at any 
stage of the Systems Development Life Cycle (SDLC). However, the cost of 
identifying and fixing vulnerabilities can vary widely based on when and how you 
choose to audit. So before you get started, review the following phases of the SDLC: 
1. Feasibility study This phase is concerned with identifying the needs the project 
should meet and determining whether developing the solution is 
technologically and financially viable. 
2. Requirements definition In this phase, a more in-depth study of requirements 
for the project is done, and project goals are established. 
3. Design The solution is designed and decisions are made about how the system 
will technically achieve the agreed-on requirements. 
4. Implementation The application code is developed according to the design laid 
out in the previous phase. 
5. Integration and testing The solution is put through some level of quality 
assurance to ensure that it works as expected and to catch any bugs in the 
software. 
6. Operation and maintenance The solution is deployed and is now in use, and 
revisions, updates, and corrections are made as a result of user feedback. 
Every software development process follows this model to some degree. Classical 
waterfall models tend toward a strict interpretation, in which the system's life span 
goes through only a single iteration through the model. In contrast, newer 
methodologies, such as agile development, tend to focus on refining an application 
by going through repeated iterations of the SDLC phases. So the way in which the 
SDLC model is applied might vary, but the basic concepts and phases are consistent 
enough for the purposes of this discussion. You can use these distinctions to help 
classify vulnerabilities, and in later chapters, you learn about the best phases in which 
to conduct different classes of reviews. 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
29 
6.1.4 Classifying Vulnerabilities 
A vulnerability class is a set of vulnerabilities that share some unifying 
commonalitya pattern or concept that isolates a specific feature shared by several 
different software flaws. Granted, this definition might seem a bit confusing, but the 
bottom line is that vulnerability classes are just mental devices for conceptualizing 
software flaws. They are useful for understanding issues and communicating that 
understanding with others, but there isn't a single, clean taxonomy for grouping 
vulnerabilities into accurate, nonoverlapping classes. It's quite possible for a single 
vulnerability to fall into multiple classes, depending on the code auditor's terminology, 
classification system, and perspective. 
A rigid formal taxonomy for categorizing vulnerabilities isn't used in this book; instead, 
issues are categorized in a consistent, pragmatic fashion that lends itself to the 
material. Some software vulnerabilities are best tackled from a particular perspective. 
For example, certain flaws might best be approached by looking at a program in 
terms of the interaction of high-level software components; another type of flaw 
might best be approached by conceptualizing a program as a sequence of system 
calls. Regardless of the approach, this book explains the terms and concepts you'll 
encounter in security literature so that you can keep the array of terms and 
taxonomies the security community uses in some sort of context. 
In defining general vulnerability classes, you can draw a few general distinctions from 
the discussion of the SDLC phases. Two commonly accepted vulnerability classes 
include design vulnerabilities (SDLC phases 1, 2, and 3) and implementation 
vulnerabilities (SDLC phases 4 and 5). In addition, this book includes a third category, 
operational vulnerabilities (SDLC phase 6). The security community generally accepts 
design vulnerabilities as flaws in a software system's architecture and specifications; 
implementation vulnerabilities are low-level technical flaws in the actual construction 
of a software system. The category of operational vulnerabilities addresses flaws that 
arise in deploying and configuring software in a particular environment. 
Design Vulnerabilities 
A design vulnerability is a problem that arises from a fundamental mistake or 
oversight in the software's design. With a design flaw, the software isn't secure 
because it does exactly what it was designed to do; it was simply designed to do the 
wrong thing! These types of flaws often occur because of assumptions made about 
the environment in which a program will run or the risk of exposure that program 
components will face in the actual production environment. Design flaws are also 
referred to as high-level vulnerabilities, architectural flaws, or problems with program 
requirements or constraints. 
A quick glance at the SDLC phases reminds you that a software system's design is 
driven by the definition of software requirements, which are a list of objectives a 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
30 
software system must meet to accomplish the goals of its creators. Typically, an 
engineer takes the set of requirements and constructs design specifications, which 
focus on how to create the software that meets those goals. Requirements usually 
address what a software system has to accomplishfor example, "Allow a user to 
retrieve a transaction file from a server." Requirements can also specify capabilities 
the software must havefor example, "It must support 100 simultaneous downloads 
per hour." 
Specifications are the plans for how the program should be constructed to meet the 
requirements. Typically, they include a description of the different components of a 
software system, information on how the components will be implemented and what 
they will do, and information on how the components will interact. Specifications 
could involve architecture diagrams, logic diagrams, process flowcharts, interface 
and protocol specifications, class hierarchies, and other technical specifications. 
When people speak of a design flaw, they don't usually make a distinction between a 
problem with the software's requirements and a problem with the software's 
specifications. Making this distinction often isn't easy because many high-level issues 
could be explained as an oversight in the requirements or a mistake in the 
specifications. 
For example, the TELNET protocol is designed to allow users to connect to a remote 
machine and access that machine as though it's connected to a local terminal. From 
a design perspective, TELNET arguably has a vulnerability in that it relies on 
unencrypted communication. In some environments, this reliance might be 
acceptable if the underlying network environment is trusted. However, in corporate 
networks and the Internet, unencrypted communications could be a major weakness 
because attackers sitting on the routing path can monitor and hijack TELNET sessions. 
If an administrator connects to a router via TELNET and enters a username and 
password to log in, a sniffer could record the administrator's username and password. 
In contrast, a protocol such as Secure Shell (SSH) serves the same basic purpose as 
TELNET, but it addresses the sniffing threat because it encrypts all communications. 
Implementation Vulnerabilities 
In an implementation vulnerability, the code is generally doing what it should, but 
there's a security problem in the way the operation is carried out. As you would 
expect from the name, these issues occur during the SDLC implementation phase, 
but they often carry over into the integration and testing phase. These problems can 
happen if the implementation deviates from the design to solve technical 
discrepancies. Mostly, however, exploitable situations are caused by technical 
artifacts and nuances of the platform and language environment in which the 
software is constructed. Implementation vulnerabilities are also referred to as 
low-level flaws or technical flaws. 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
31 
This book includes many examples of implementation vulnerabilities because 
identifying these technical flaws is one of the primary charges of the code review 
process. Implementation vulnerabilities encompass several well-publicized 
vulnerability classes you've probably heard of, such as buffer overflows and SQL 
injection. 
Going back to the TELNET example, you can also find implementation vulnerabilities 
in specific versions of TELNET software. Some previous implementations of TELNET 
daemons didn't cleanse user environment variables correctly, allowing intruders to 
leverage the dynamic linking features of a UNIX machine to elevate their privileges on 
the machine. There were also flaws that allowed intruders to perform buffer overflows 
and format string attacks against various versions of TELNET daemons, often without 
authenticating at all. These flaws resulted in attackers being able to remotely issue 
arbitrary commands on the machine as privileged users. Basically, attackers could 
run a small exploit program against a vulnerable TELNET daemon and immediately 
get a root prompt on the server. 
Operational Vulnerabilities 
Operational vulnerabilities are security problems that arise through the 
operational procedures and general use of a piece of software in a specific 
environment. One way to distinguish these vulnerabilities is that they aren't present 
in the source code of the software under consideration; rather, they are rooted in how 
the software interacts with its environment. Specifically, they can include issues with 
configuration of the software in its environment, issues with configuration of 
supporting software and computers, and issues caused by automated and manual 
processes that surround the system. Operational vulnerabilities can even include 
certain types of attacks on users of the system, such as social engineering and theft. 
These issues occur in the SDLC operation and maintenance phase, although they 
have some overlap into the integration and testing phase. 
Going back to the TELNET example, you know TELNET has a design flaw because of its 
lack of encryption. Say you're looking at a software system for automated securities 
trading. Suppose it needs a set of weighting values to be updated every night to 
adjust its trading strategy for the next day. The documented process for updating this 
data is for an administrator to log in to the machine using TELNET at the end of each 
business day and enter the new set of values through a simple utility program. 
Depending on the environment, this process could represent a major operational 
vulnerability because of the multiple risks associated with using TELNET, including 
sniffing and connection hijacking. In short, the operational procedure for maintaining 
the software is flawed because it exposes the system to potential fraud and attacks. 
Gray Areas 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
32 
The distinction between design and implementation vulnerabilities is deceptively 
simple in terms of the SDLC, but it's not always easy to make. Many implementation 
vulnerabilities could also be interpreted as situations in which the design didn't 
anticipate or address the problem adequately. On the flip side, you could argue that 
lower-level pieces of a software system are also designed, in a fashion. A programmer 
can design plenty of software components when implementing a specification, 
depending on the level of detail the specification goes into. These components might 
include a class, a function, a network protocol, a virtual machine, or perhaps a clever 
series of loops and branches. Lacking a strict distinction, in this book the following 
definition of a design vulnerability is used: 
In general, when people refer to design vulnerabilities, they mean high-level issues 
with program architecture, requirements, base interfaces, and key algorithms. 
Expanding on the definition of design vulnerabilities, this book uses the following 
definition of an implementation vulnerability: 
Security issues in the design of low-level program pieces, such as parts of individual 
functions and classes, are generally considered to be implementation vulnerabilities. 
Implementation vulnerabilities also include more complex logical elements that are 
not normally addressed in the design specification. (These issues are often called 
logic vulnerabilities.) 
Likewise, there's no clear distinction between operational vulnerabilities and 
implementation or design vulnerabilities. For example, if a program is installed in an 
environment in a fashion that isn't secure, you could easily argue that it's a failure of 
the design or implementation. You would expect the application to be developed in a 
manner that's not vulnerable to these environmental concerns. Lacking a strict 
distinction again, the following definition of an operational vulnerability is used in this 
book: 
In general, the label "operational vulnerabilities" is used for issues that deal with 
unsafe deployment and configuration of software, unsound management and 
administration practices surrounding software, issues with supporting components 
such as application and Web servers, and direct attacks on the software's users. 
You can see that there's plenty of room for interpretation and overlap in the concepts 
of design, implementation, and operational vulnerabilities, so don't consider these 
definitions to be an infallible formal system for labeling software flaws. They are 
simply a useful way to approach and study software vulnerabilities. 
6.1.5 Common Threads 
So far you've learned some background on the audit process, security models, and 
the three common classes of vulnerabilities. This line of discussion is continued 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
33 
throughout the rest of this book, as you drill down into the details of specific technical 
issues. For now, however, take a step back to look at some common threads that 
underlie security vulnerabilities in software, focusing primarily on where and why 
vulnerabilities are most likely to surface in software. 