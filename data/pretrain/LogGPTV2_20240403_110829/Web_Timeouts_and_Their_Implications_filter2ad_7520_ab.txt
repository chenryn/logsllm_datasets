D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
 Response transfer rate
1
10
102
103
104
Throughput (Kbps)
Fig. 2. Response timeout
Fig. 3. Response rate
We concentrate on HTTP GET requests in this experiment as they are typically
small. HTTP POST requests on the other hand could be arbitrarily large and
take longer to send. This suggests that these two request types should be handled
with diﬀerent timeouts. We ﬁnd that 85% of the requests ﬁt into one packet.
Further, 99.9% of the requests are completed within 1 second. Still, the longest
time taken by a client in the trace is 592 seconds.
Response Timeout: The response timeout is the amount of time the server
allocates to delivering an HTTP response. This timeout guards against a client
that is alive (i.e., responds with TCP ACKs) but consumes data at a slow rate,
by either acknowledging few bytes at a time or advertising a small (or at the
extreme, zero) window. Since the client is responding the connection can only
be closed by the application and not by TCP.
We are aware of only one major Web server that enforces a response timeout—
alternatively presented as a minimum transfer rate—which is Microsoft’s IIS.
The default minimum transfer rate in IIS is 240 bytes/sec. Even though IIS
notationally imposes a minimum rate-based limit, internally this is converted to
a time-based limit. Speciﬁcally, IIS divides the response size by the minimum
transfer rate with the result used to arm a timer. If the timer ﬁres and the
client has not fully consumed the response, IIS will close the connection [8].
This mechanism is eﬃcient in that progress is checked only once. However, an
attacker can leverage this mechanism by ﬁnding a large object and retrieving it
at a low rate—which IIS will only detect after a long time.
To measure the response timeout, we open a connection to a Web server, send
a request for the home page, and consume the response at a low rate. Given the
IIS’s default rate limit of 240 bytes/sec, in our experiments we consume the re-
sponse at a lower rate of 100 bytes/sec. A site that delivers the entire response
at this rate is assumed to not impose a limit, otherwise a limit is in place. This
experiment involves 494 high volume sites and 15,034 regular sites. The table in
Figure 2 shows our results. We ﬁnd that less than 25% of sites—regardless of
group—impose a limit on the transfer rate. Furthermore, 59% of the regular sites
that impose limits identify themselves as IIS, as expected. However, only 33% of
the high-volume sites that impose response time limits identify themselves as IIS
Web Timeouts and Their Implications
217
servers. There could be a myriad reasons that can explain the remaining sites,
including IIS servers obscuring their identities, servers behind transparent TCP
proxies that keep their own timers, custom built servers, intrusion prevention
systems impacting communication, etc. Interestingly, as shown in the last column
of the table, there is a small percentage of sites that identify themselves as IIS
servers and yet do not impose any response timeout. This could be caused by
site administrators disabling the response timeout or transparent TCP proxies
that obscure the actual Web server behavior.
We now consider the time needed by normal Web clients to consume responses.
This time is determined mainly by the round trip time for small responses and
by the available end-to-end bandwidth for large responses. Therefore, while a low
limit on the transfer rate such as IIS’s 240 byte/sec might be appropriate for
small responses (although whether one could tighten this limit at times of stress
is an interesting question for future work), we aim at assessing whether such a
low limit is appropriate for large responses, especially that attacks against this
timeout are particularly dangerous for large responses. To assess that, we con-
sider responses in the ICSI trace with size of at least 50 KB. We approximate the
end-to-end transfer rate as the response size divided by the time between the ﬁrst
and last packet of the response. Figure 3 presents the distribution of response
transfer rates. The ﬁgure shows that nearly 99% of the responses (whether the
response was originated from an ISCI server or an external server) were trans-
ferred at over 10 Kbps (that is 1,250 bytes/second compared to the default of
240 bytes/second of IIS).
HTTP Keep-Alive: We next turn to persistent HTTP, which attempts to
make Web transfers eﬃcient by keeping TCP connections open for more than one
HTTP request. The HTTP keep-alive timeout is deﬁned as the time the server
will keep an idle connection open after successfully satisfying all requests. We
start by issuing requests for the home pages of the Web sites using nc6. We then
measure the time between receiving the last packet of the response and receiving
a FIN or RST from the server. This experiment involves 490 high volume and
14,928 regular sites Figure 1(d) shows the distribution of these times. The
problem of ﬁnding a cut-oﬀ point before which we assume servers do not maintain
persistent connections is relatively easy in this ﬁgure. Indeed, selecting the cut-
oﬀ point at 100ms or at 1 second produces similar results. Roughly, 65% of the
high volume sites and 76% of the regular sites maintain persistent connections.
These numbers indicate that the overall support of persistent connections has
not changed appreciably since Fall of 2000 [10]. Surprisingly, regular sites seem
to have shorter keep-alive timeouts than high volume sites. For instance, nearly
61% of the high volume sites that use persistent connections use a timeout over
30s while it is roughly 32% for the regular sites. We speculate that this is due
to the higher incidence of Apache with default conﬁguration of 15s keep-alive
timeout among regular sites than it is among high volume sites.
Timeout Adaption: To get a preliminary intuition as to whether Web sites
currently vary their timeouts over time, we performed periodic probing of the
218
Z. Al-Qudah, M. Rabinovich, and M. Allman
s
e
t
i
s
f
o
e
g
a
t
n
e
c
r
e
P
 40
 35
 30
 25
 20
 15
 10
 5
 0
Potentially adapting sites (%)
without adaptive timeout
with adaptive timeout
i
d
e
n
e
d
s
t
s
e
u
q
e
r
f
o
%
 100
 80
 60
 40
 20
 0
 0  2  4  6  8  10  12  14  16  18  20
Variability level (%)
 0  10  20  30  40  50  60  70  80  90 100
Attempt number
Fig. 4. Variability of request timeouts
Fig. 5. Performance of adaptive timeouts
request timeout for the high volume sites. Speciﬁcally, we probed each site every
12 minutes for a week. We deﬁne a site as having an adaptive timeout if at least
m% of the measurements to the server are at least m% diﬀerent from the mean
timeout to the given site (i.e., m is an experimental parameter). This procedure
is clearly not conclusive given that we may simply not have observed adaption
for a particular site because there was no reason for the site to adapt during
our measurements. Further, a timeout change could be caused by reasons other
than adaptability such as diﬀerent requests arriving at diﬀerent servers with
various timeout conﬁgurations or a server crash during a connection lifetime.
The percentage of sites found to be using an adaptive timeout as a function
of m is shown in Figure 4. We ﬁnd that roughly 3% of the sites tested exhibit
behavior suggestive of timer adaption, as shown by the range of m values for
which this ﬁnding holds.
Summary: our measurements indicate that normal web clients perform their
activities quickly as compared to the time allowed by Web servers.1 Long time-
outs leave a server vulnerable to claim-and-hold attacks. These attacks have
been reported in practice [7,6], and we will demonstrate a simple attack uti-
lizing these timeouts in the next section. Short of complex external intrusion
detection mechanisms, a naive way to counter these attacks would be to in-
crease the number of allowable concurrent connection slots at the server. But
this may cause performance degradation in case the slots are consumed by legit-
imate connections, since the number of concurrent connections is driven by the
server capacity. Furthermore, although our measurements show that current long
timeouts are generally unneeded by normal Web clients, slashing them blindly
would run counter to the general networking tenet of allowing liberal client
behaviors. Therefore, we suggest slashing these timeouts only at the time of
1 While clients in our trace are generally well-connected, the characteristics of dial-
up connections should not aﬀect this ﬁnding. Indeed, dial-up connections oﬀer a
last-mile bandwidth of 30-40 Kbps—well within the 99th percentile we observe in
our trace and also well above the 240 bytes/sec IIS requires. Furthermore, the few
hundreds of milliseconds these connections add still leave the time needed by these
connections to perform activities much shorter than allowed by Web servers.
Web Timeouts and Their Implications
219
stress.2 While our measurements suggest that a small fraction of sites might
already be varying their timeouts, popular Web servers such as Apache and
IIS do not oﬀer such mechanisms—which would limit the spread use of these
mechanisms.
4 Adaptive Timeouts
We now present our implementation of an adaptive timeout mechanism and
demonstrate its usefulness. Our implementation involves changes to the Linux
TCP stack and Apache web server (version 2.2.11). The kernel extension allows
an application to specify a target response transfer rate and to toggle the kernel
between a conservative (current behavior) and aggressive (close any connection
below the target transfer rate) modes. The kernel monitors the transfer rate of
connections only during periods of non-empty TCP send queue to avoid penal-
izing a client for the time the server has no data to send. Our modiﬁed Apache
sets the target transfer rate parameter (500 bytes/second in our experiments)
and monitors the connection slots. Once allocated slots reach a certain level
(90% of all slots in our experiments), it (a) reduces its application timeout from
its current default of 300s to 3s and (b) toggles the kernel into the aggressive
mode. While a complete implementation of our framework would consider all
timeouts, our current implementation covers application timeout, TCP timeout,
and response timeout.
To demonstrate how such a simple mechanism can protect sites from claim-
and-hold attacks, we set up a Web site with Linux OS and Apache Web server,
both using out-of-the box conﬁgurations except with Apache conﬁgured to allow
a higher number of concurrent connections (256 vs. default 150). We then set up
a machine that launches an attack targeting the response timeout. In particular,
it attempts to keep 300 concurrent connections by requesting a 100 KB ﬁle and
consuming it at a rate of 200–300 bytes/second on each of these connections.
Another machine simulates legitimate traﬃc by probing the server once every
10 seconds by opening 100 connections to the server with a 5 second timeout
period (i.e., a request fails if not satisﬁed within 5 seconds). This process repeats
100 times. The solid line on Figure 5 shows the results. The attack starts around
probe number ﬁve. After a short delay (due to Apache’s gradual forking of
new processes) the attacking host is able to hold all the connection slots and
thus completely deny the service to legitimate connections. Further, the attacker
accomplishes this at the cost of consuming less than 1Mbps (300 connections with
at most 300 bytes/s each) of its own bandwidth—available to a single average
residential DSL user let alone a botnet. The dashed line in Figure 5 shows the
2 One can imagine applications, as possible with AJAX, where HTTP connections
could have long idle periods. Our trace accounts for all HTTP interactions including
AJAX, and as discussed, did not encounter such connections in large numbers. We
note that the content provider controls both ends of the connection in these applica-
tions. Therefore, these connections could either be treated diﬀerently by the server
or the applications can be written to handle possible interruptions gracefully.
220
Z. Al-Qudah, M. Rabinovich, and M. Allman
results of repeating the attack on our modiﬁed platform. As seen, our simple
mechanism allows the server to cope with the attack load without impinging on
legitimate connections by quickly terminating attack connections which leaves
open slots for legitimate traﬃc. Our intent in this experiment is to show that a
simple system can perform well. We consider a full study of a range of decision
heuristics out of scope for this paper. Further, such decisions can be a policy
matter and therefore cannot be entirely evaluated on purely technical grounds.
5 Conclusions
In this paper we study Internet timeouts from two perspectives. We ﬁrst probe
the timeout settings in two sets of operational Web sites (high volume and regular
sites). We then study the characteristics of normal Web activity by analyzing
passively captured Web traﬃc. The major ﬁnding from these two measurements
is that there is a signiﬁcant mismatch between the time normal Web transactions
take and that which Web servers allow for these transactions. While this reﬂects a
conservativeness on the Web server’s part it also opens a window of vulnerability
to claim-and-hold DoS attacks whereby an attacker claims a large fraction of
connection slots from the server and prevents their usage for legitimate clients.
Rather than reducing servers’ timeouts to match normal Web activity—a
solution that could reduce the tolerance of the server to legitimate activity—we
suggest a dynamic mechanism that is based on continuous measurements of both
connection progress and resource contention on the server. A decision to reduce
the timeouts and drop connections accomplishing little or no useful work is only
taken when the server becomes resource constrained. We demonstrate how this
simple mechanism can protect Web servers. Our mechanism is implemented in
a popular open source Web server and is available for download [1].
References
1. Project Downloads, http://vorlon.case.edu/~zma/timeout_downloads/
2. Al-Qudah, Z., Lee, S., Rabinovich, M., Spatscheck, O., der Merwe, J.V.: Anycast-
aware transport for content delivery networks. In: 18th International World Wide
Web Conference, April 2009, p. 301(2009)
3. Alexa The Web Information Company, http://www.alexa.com/
4. Apache HTTP server - Security tips, http://httpd.apache.org/docs/trunk/
misc/security_tips.html
5. Barford, P., Crovella, M.: A performance evaluation of hyper text transfer proto-
cols. SIGMETRICS, 188–197 (1999)
6. objectmix.com/apache/672969-re-need-help-fighting-dos-attack-apache.
html
7. http://www.webhostingtalk.com/showthread.php?t=645132
8. Microsoft TechNet Library, http://technet.microsoft.com/en-us/library/
cc775498.aspx
9. Keynote, http://www.keynote.com/
Web Timeouts and Their Implications
221
10. Krishnamurthy, B., Arlitt, M.: PRO-COW: Protocol compliance on the web: A
longitudinal study. In: USENIX Symp. on Internet Technologies and Sys. (2001)
11. nc6 - network swiss army knife, http://linux.die.net/man/1/nc6
12. Park, K., Pai, V.S.: Connection conditioning: architecture-independent support for
simple, robust servers. In: USENIX NSDI (2006)
13. Qie, X., Pang, R., Peterson, L.: Defensive programming: using an annotation toolkit
to build DoS-resistant software. SIGOPS Oper. Syst. Rev. 36(SI) (2002)
14. Rabinovich, M., Wang, H.: DHTTP: An eﬃcient and cache-friendly transfer pro-
tocol for web traﬃc. In: INFOCOM, pp. 1597–1606 (2001)
15. SEOBOOK.com, http://tools.seobook.com/link-harvester/
16. TCP protocol - Linux man page, http://linux.die.net/man/7/tcp