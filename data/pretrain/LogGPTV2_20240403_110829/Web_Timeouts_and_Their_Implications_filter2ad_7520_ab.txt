### Response Timeout

**Figure 2. Response Timeout**

| D | C |
|---|---|
| 1 | 0.8 |
| 0.6 | 0.4 |
| 0.2 | 0 |

**Response Transfer Rate**
- 1
- 10
- 10²
- 10³
- 10⁴

**Throughput (Kbps)**

**Figure 3. Response Rate**

In this experiment, we focus on HTTP GET requests, as they are typically small. In contrast, HTTP POST requests can be arbitrarily large and take longer to send. This suggests that these two request types should be handled with different timeouts. We find that 85% of the requests fit into one packet. Additionally, 99.9% of the requests are completed within 1 second. However, the longest time taken by a client in the trace is 592 seconds.

#### Response Timeout

The response timeout is the amount of time the server allocates for delivering an HTTP response. This timeout protects against a client that is alive (i.e., responds with TCP ACKs) but consumes data at a slow rate, either by acknowledging few bytes at a time or advertising a small (or even zero) window. Since the client is responding, the connection can only be closed by the application, not by TCP.

We are aware of only one major web server that enforces a response timeout—Microsoft's IIS. The default minimum transfer rate in IIS is 240 bytes/sec. Although IIS notationally imposes a minimum rate-based limit, internally this is converted to a time-based limit. Specifically, IIS divides the response size by the minimum transfer rate and uses the result to arm a timer. If the timer fires and the client has not fully consumed the response, IIS will close the connection [8]. This mechanism is efficient because progress is checked only once. However, an attacker can leverage this mechanism by finding a large object and retrieving it at a low rate, which IIS will only detect after a long time.

To measure the response timeout, we open a connection to a web server, send a request for the home page, and consume the response at a low rate. Given IIS’s default rate limit of 240 bytes/sec, in our experiments, we consume the response at a lower rate of 100 bytes/sec. A site that delivers the entire response at this rate is assumed to not impose a limit; otherwise, a limit is in place. This experiment involves 494 high-volume sites and 15,034 regular sites. The table in Figure 2 shows our results. We find that less than 25% of sites—regardless of group—impose a limit on the transfer rate. Furthermore, 59% of the regular sites that impose limits identify themselves as IIS, as expected. However, only 33% of the high-volume sites that impose response time limits identify themselves as IIS servers. There could be various reasons for this, including IIS servers obscuring their identities, servers behind transparent TCP proxies that keep their own timers, custom-built servers, intrusion prevention systems impacting communication, etc. Interestingly, as shown in the last column of the table, there is a small percentage of sites that identify themselves as IIS servers but do not impose any response timeout. This could be due to site administrators disabling the response timeout or transparent TCP proxies obscuring the actual web server behavior.

Next, we consider the time needed by normal web clients to consume responses. This time is determined mainly by the round trip time for small responses and by the available end-to-end bandwidth for large responses. Therefore, while a low limit on the transfer rate such as IIS’s 240 bytes/sec might be appropriate for small responses (although whether one could tighten this limit at times of stress is an interesting question for future work), we aim to assess whether such a low limit is appropriate for large responses, especially since attacks against this timeout are particularly dangerous for large responses. To assess this, we consider responses in the ICSI trace with a size of at least 50 KB. We approximate the end-to-end transfer rate as the response size divided by the time between the first and last packet of the response. Figure 3 presents the distribution of response transfer rates. The figure shows that nearly 99% of the responses (whether from an ICSI server or an external server) were transferred at over 10 Kbps (that is, 1,250 bytes/second compared to the default of 240 bytes/second of IIS).

### HTTP Keep-Alive

**Figure 1(d). Distribution of Keep-Alive Times**

We next turn to persistent HTTP, which attempts to make web transfers efficient by keeping TCP connections open for more than one HTTP request. The HTTP keep-alive timeout is defined as the time the server will keep an idle connection open after successfully satisfying all requests. We start by issuing requests for the home pages of the web sites using `nc6`. We then measure the time between receiving the last packet of the response and receiving a FIN or RST from the server. This experiment involves 490 high-volume and 14,928 regular sites. Figure 1(d) shows the distribution of these times. The problem of finding a cut-off point before which we assume servers do not maintain persistent connections is relatively easy in this figure. Indeed, selecting the cut-off point at 100 ms or at 1 second produces similar results. Roughly, 65% of the high-volume sites and 76% of the regular sites maintain persistent connections. These numbers indicate that the overall support of persistent connections has not changed appreciably since Fall 2000 [10]. Surprisingly, regular sites seem to have shorter keep-alive timeouts than high-volume sites. For instance, nearly 61% of the high-volume sites that use persistent connections use a timeout over 30 seconds, while it is roughly 32% for the regular sites. We speculate that this is due to the higher incidence of Apache with a default configuration of 15 seconds keep-alive timeout among regular sites than among high-volume sites.

### Timeout Adaption

**Figure 4. Variability of Request Timeouts**

**Figure 5. Performance of Adaptive Timeouts**

To get a preliminary intuition as to whether web sites currently vary their timeouts over time, we performed periodic probing of the request timeout for the high-volume sites. Specifically, we probed each site every 12 minutes for a week. We define a site as having an adaptive timeout if at least m% of the measurements to the server are at least m% different from the mean timeout to the given site (i.e., m is an experimental parameter). This procedure is clearly not conclusive, as we may simply not have observed adaptation for a particular site because there was no reason for the site to adapt during our measurements. Further, a timeout change could be caused by reasons other than adaptability, such as different requests arriving at different servers with various timeout configurations or a server crash during a connection lifetime.

The percentage of sites found to be using an adaptive timeout as a function of m is shown in Figure 4. We find that roughly 3% of the sites tested exhibit behavior suggestive of timer adaptation, as shown by the range of m values for which this finding holds.

### Summary

Our measurements indicate that normal web clients perform their activities quickly compared to the time allowed by web servers. Long timeouts leave a server vulnerable to claim-and-hold attacks. These attacks have been reported in practice [7, 6], and we will demonstrate a simple attack utilizing these timeouts in the next section. Short of complex external intrusion detection mechanisms, a naive way to counter these attacks would be to increase the number of allowable concurrent connection slots at the server. However, this may cause performance degradation if the slots are consumed by legitimate connections, as the number of concurrent connections is driven by the server capacity. Furthermore, although our measurements show that current long timeouts are generally unneeded by normal web clients, slashing them blindly would run counter to the general networking tenet of allowing liberal client behaviors. Therefore, we suggest slashing these timeouts only at the time of stress. While clients in our trace are generally well-connected, the characteristics of dial-up connections should not affect this finding. Indeed, dial-up connections offer a last-mile bandwidth of 30-40 Kbps—well within the 99th percentile we observe in our trace and also well above the 240 bytes/sec IIS requires. Furthermore, the few hundred milliseconds these connections add still leave the time needed by these connections to perform activities much shorter than allowed by web servers.

### 4. Adaptive Timeouts

We now present our implementation of an adaptive timeout mechanism and demonstrate its usefulness. Our implementation involves changes to the Linux TCP stack and Apache web server (version 2.2.11). The kernel extension allows an application to specify a target response transfer rate and to toggle the kernel between a conservative (current behavior) and aggressive (close any connection below the target transfer rate) modes. The kernel monitors the transfer rate of connections only during periods of non-empty TCP send queue to avoid penalizing a client for the time the server has no data to send. Our modified Apache sets the target transfer rate parameter (500 bytes/second in our experiments) and monitors the connection slots. Once allocated slots reach a certain level (90% of all slots in our experiments), it (a) reduces its application timeout from its current default of 300 seconds to 3 seconds and (b) toggles the kernel into the aggressive mode. While a complete implementation of our framework would consider all timeouts, our current implementation covers application timeout, TCP timeout, and response timeout.

To demonstrate how such a simple mechanism can protect sites from claim-and-hold attacks, we set up a web site with a Linux OS and Apache web server, both using out-of-the-box configurations except with Apache configured to allow a higher number of concurrent connections (256 vs. default 150). We then set up a machine that launches an attack targeting the response timeout. In particular, it attempts to keep 300 concurrent connections by requesting a 100 KB file and consuming it at a rate of 200–300 bytes/second on each of these connections. Another machine simulates legitimate traffic by probing the server once every 10 seconds by opening 100 connections to the server with a 5-second timeout period (i.e., a request fails if not satisfied within 5 seconds). This process repeats 100 times. The solid line in Figure 5 shows the results. The attack starts around probe number five. After a short delay (due to Apache’s gradual forking of new processes), the attacking host is able to hold all the connection slots and thus completely deny the service to legitimate connections. Further, the attacker accomplishes this at the cost of consuming less than 1 Mbps (300 connections with at most 300 bytes/second each) of its own bandwidth—available to a single average residential DSL user, let alone a botnet. The dashed line in Figure 5 shows the results of repeating the attack on our modified platform. As seen, our simple mechanism allows the server to cope with the attack load without impinging on legitimate connections by quickly terminating attack connections, which leaves open slots for legitimate traffic. Our intent in this experiment is to show that a simple system can perform well. We consider a full study of a range of decision heuristics out of scope for this paper. Further, such decisions can be a policy matter and therefore cannot be entirely evaluated on purely technical grounds.

### 5. Conclusions

In this paper, we study internet timeouts from two perspectives. We first probe the timeout settings in two sets of operational web sites (high volume and regular sites). We then study the characteristics of normal web activity by analyzing passively captured web traffic. The major finding from these two measurements is that there is a significant mismatch between the time normal web transactions take and that which web servers allow for these transactions. While this reflects conservativeness on the web server’s part, it also opens a window of vulnerability to claim-and-hold DoS attacks, whereby an attacker claims a large fraction of connection slots from the server and prevents their usage for legitimate clients.

Rather than reducing servers’ timeouts to match normal web activity—a solution that could reduce the tolerance of the server to legitimate activity—we suggest a dynamic mechanism based on continuous measurements of both connection progress and resource contention on the server. A decision to reduce the timeouts and drop connections accomplishing little or no useful work is only taken when the server becomes resource-constrained. We demonstrate how this simple mechanism can protect web servers. Our mechanism is implemented in a popular open-source web server and is available for download [1].

### References

1. Project Downloads, http://vorlon.case.edu/~zma/timeout_downloads/
2. Al-Qudah, Z., Lee, S., Rabinovich, M., Spatscheck, O., van der Merwe, J.: Anycast-aware transport for content delivery networks. In: 18th International World Wide Web Conference, April 2009, p. 301 (2009)
3. Alexa The Web Information Company, http://www.alexa.com/
4. Apache HTTP Server - Security Tips, http://httpd.apache.org/docs/trunk/misc/security_tips.html
5. Barford, P., Crovella, M.: A performance evaluation of hyper text transfer protocols. SIGMETRICS, 188–197 (1999)
6. objectmix.com/apache/672969-re-need-help-fighting-dos-attack-apache.html
7. http://www.webhostingtalk.com/showthread.php?t=645132
8. Microsoft TechNet Library, http://technet.microsoft.com/en-us/library/cc775498.aspx
9. Keynote, http://www.keynote.com/
10. Krishnamurthy, B., Arlitt, M.: PRO-COW: Protocol compliance on the web: A longitudinal study. In: USENIX Symp. on Internet Technologies and Sys. (2001)
11. nc6 - network swiss army knife, http://linux.die.net/man/1/nc6
12. Park, K., Pai, V.S.: Connection conditioning: architecture-independent support for simple, robust servers. In: USENIX NSDI (2006)
13. Qie, X., Pang, R., Peterson, L.: Defensive programming: using an annotation toolkit to build DoS-resistant software. SIGOPS Oper. Syst. Rev. 36(SI) (2002)
14. Rabinovich, M., Wang, H.: DHTTP: An efficient and cache-friendly transfer protocol for web traffic. In: INFOCOM, pp. 1597–1606 (2001)
15. SEOBOOK.com, http://tools.seobook.com/link-harvester/
16. TCP protocol - Linux man page, http://linux.die.net/man/7/tcp