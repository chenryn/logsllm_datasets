title:Enabling Rack-scale Confidential Computing using Heterogeneous Trusted
Execution Environment
author:Jianping Zhu and
Rui Hou and
XiaoFeng Wang and
Wenhao Wang and
Jiangfeng Cao and
Boyan Zhao and
Zhongpu Wang and
Yuhui Zhang and
Jiameng Ying and
Lixin Zhang and
Dan Meng
2020 IEEE Symposium on Security and Privacy
Enabling Rack-scale Conﬁdential Computing using
Heterogeneous Trusted Execution Environment
Jianping Zhu†, Rui Hou†∗ , XiaoFeng Wang+∗, Wenhao Wang†, Jiangfeng Cao†, Boyan Zhao†,
Zhongpu Wang‡, Yuhui Zhang†, Jiameng Ying†, Lixin Zhang‡, Dan Meng†
State Key Laboratory of Information Security, Institute of Information Engineering, CAS, and School of Cyber Security,
†
University of Chinese Academy of Sciences; +Indiana University at Bloomington;
{zhujianping, hourui, wangwenhao, caojiangfeng, zhaoboyan, zhangyuhui, yingjiameng, mengdan}@iie.ac.cn,
Institute of Computing Technology, CAS
‡
PI:EMAIL, {wangzhongpu, zhanglixin}@ict.ac.cn
Abstract—With its huge real-world demands, large-scale con-
ﬁdential computing still cannot be supported by today’s Trusted
Execution Environment (TEE), due to the lack of scalable and
effective protection of high-throughput accelerators like GPUs,
FPGAs, and TPUs etc. Although attempts have been made
recently to extend the CPU-like enclave to GPUs, these solutions
require change to the CPU or GPU chips, may introduce
new security risks due to the side-channel leaks in CPU-GPU
communication and are still under the resource constraint of
today’s CPU TEE.
To address these problems, we present the ﬁrst Heterogeneous
TEE design that can truly support large-scale compute or data
intensive (CDI) computing, without any chip-level change. Our
approach, called HETEE, is a device for centralized management
of all computing units (e.g., GPUs and other accelerators) of a
server rack. It is uniquely designed to work with today’s data cen-
tres and clouds, leveraging modern resource pooling technologies
to dynamically compartmentalize computing tasks, and enforce
strong isolation and reduce TCB through hardware support.
More speciﬁcally, HETEE utilizes the PCIe ExpressFabric to
allocate its accelerators to the server node on the same rack
for a non-sensitive CDI task, and move them back into a secure
enclave in response to the demand for conﬁdential computing.
Our design runs a thin TCB stack for security management on a
security controller (SC), while leaving a large set of software (e.g.,
AI runtime, GPU driver, etc.) to the integrated microservers that
operate enclaves. An enclaves is physically isolated from others
through hardware and veriﬁed by the SC at its inception. Its
microserver and computing units are restored to a secure state
upon termination.
We implemented HETEE on a real hardware system, and
evaluated it with popular neural network inference and training
tasks. Our evaluations show that HETEE can easily support
the CDI tasks on the real-world scale and incurred a maximal
throughput overhead of 2.17% for inference and 0.95% for
training on ResNet152.
I. INTRODUCTION
The explosive growth of the data being collected and ana-
lyzed has fueled the rapid advance in data-driven technologies
and applications, which have also brought data privacy to the
spotlight as never before. A large spectrum of data-centric in-
novations today, ranging from personalized healthcare, mobile
ﬁnance to social networking, are under persistent threats of
data breaches, such as Facebook data exposure [1], [2], and
the growing pressure for compliance with emerging privacy
∗
Corresponding authors: Rui Hou (PI:EMAIL), XiaoFeng Wang (PI:EMAIL)
laws and regulations,
like GDPR (general data protection
regulation) and the CCPA (California Consumer Privacy Act).
As a result, there is an urgent demand for privacy-preserving
techniques capable of supporting computing and data-intensive
(CDI) computing, such as training deep neural networks
(DNNs) over an enormous amount of data.
TEE-based secure computing. Answering to this urgent
call are conﬁdential computing techniques, which have been
studied for decades. Traditional software-only approaches such
as homomorphic encryption and secure multi-party computing
are considered to be less effective in protecting complicated
computing (such as DNN analysis) over big data, due to
their signiﬁcant computation or communication overheads.
Emerging as a more practical solution is the new generation of
hardware supports for Trusted Execution Environments (TEEs)
such as Intel Software Guard Extensions (SGX) [3], AMD
Secure Encrypted Virtualization (SEV) [4] and ARM Trust-
Zone [5]. These TEEs are characterized by their separation
of a secure world, called enclave in SGX, from the insecure
one, so protected data can be processed by trusted code in
an enclave, even in the presence of a compromised OS and
corrupted system administrators. None of them, however, can
truly support CDI computing tasks, due to their exclusion
of high-throughput accelerators such as graph-processing unit
(GPU), tensor-processing unit (TPU), and FPGA etc. More
fundamentally, today’s TEEs are not designed to protect big-
data analytics, since they fail to support the heterogeneous
computing model that becomes the mainstream architecture
for CDI computing [6]–[14]. Under the heterogeneous archi-
tecture, a CDI computing task is jointly processed by different
types of computing units: e.g., a machine learning task today
is typically performed by a CPU, which acts as a control unit,
and a set of GPUs or TPUs, which serve as computing units.
Such a joint computing model also needs to be under TEE’s
protection, which has not been considered in current designs.
Recent years have seen attempts to support the heteroge-
neous TEE. Examples include Graviton [15] and HIX [16].
However, all these approaches require changes to CPU and
(or) GPU chips, which prevents the use of existing hardware,
and also incurs a long and expensive development cycle to
chip manufacturers and therefore may not happen in the near
© 2020, Jianping Zhu. Under license to IEEE.
DOI 10.1109/SP40000.2020.00054
1450
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:57:53 UTC from IEEE Xplore.  Restrictions apply. 
future. Another problem is that their reliance on the host
CPU TEE limits their utility. As a prominent example, SGX
today has only been deployed to Xeon E3 processors, while
dominant high-performance Intel CPU chips like Xeon E5
extensively used in data centers still have not included SGX
instructions yet. Also, running CDI tasks and their computing
supports (e.g., AI framework, GPU runtime and driver, etc.)
inside a CPU TEE today signiﬁcantly increases the size of
Trusted Computing Base (TCB) and may not even be possible
due to the resource constraints of the TEE (e.g., < 100MB
protected memory for SGX [3]). Finally the design based upon
the enclaves of individual cores requires these enclaves to
work closely with each other. This has security implications
and opens a new avenue for a side-channel analysis on the
communication between different enclaves.
Our new design. We believe that a TEE designed for
CDI tasks should offer a strong support for heterogeneous
computing, enabling collaborative computing units to be
protected under a single enclave and conveniently assigned
across secure/insecure worlds and different enclaves. Further
this computation-oriented TEE should only include a small
TCB (e.g., a single security controller with necessary code
customized for supporting CDI task isolation), to reduce its
complexity and also minimize the side-channel attack surface
exposed by resource sharing with the untrusted OS. For ease
of deployment, using existing computing units without chip-
level changes is highly desirable. In this paper, we present the
ﬁrst design that ﬁts all these descriptions. Our approach, called
HETEE (Heterogeneous TEE), is uniquely constructed to work
with today’s servers, with the focus on protecting Platform as
a Service (PaaS) against information leaks.
Unlike the existing TEE, which works on a single host,
the HETEE architecture is designed to enable dynamic allo-
cation of computing resources for secure and non-sensitive
computing tasks across multiple servers, based upon the state-
of-the-art data-center technologies such as resource pooling
and PCIe switching. More speciﬁcally, running on a server
rack, HETEE operates inside a tamper-resistant chassis (called
HETEE box) to control a pool of commercial, off-the-shelf
(COTS) accelerators, including GPUs, FPGAs, etc. The box
is connected to other hosts on the same rack through PCIe
Switch Fabric. On receiving a request from a host, the HETEE
box dynamically conﬁgures the PCIe Switch to connect COTS
computing units to the host, when the task is non-sensitive. For
a task involving sensitive data, HETEE conﬁgures the switch
to allocate computing units for a secure enclave isolated from
other units, performs a remote attestation with the data owner
(through one of the hosts) and then decrypts the data from the
owner and runs approved code on the data inside the enclave.
After the task is done, all units are sanitized and restored to
their original, trusted states for processing the next task. In this
way, we can leverage existing hardware to provide on-demand
computing supports for both sensitive and public tasks.
Further underlying our design is the idea to simplify TCB
through cost-effective hardware design. The HETEE architec-
ture includes a two-level isolation mechanism based upon a Se-
curity Controller (SC) and a set of low-cost microservers that
act as security proxy nodes. While a proxy node could
run a complete software stack (e.g., CUDA [17], TensorFlow
[18]), it is veriﬁed and protected by the SC, which involves
only necessary functionalities like (de)encryption, remote at-
testation, PCIe fabric conﬁguration, etc., before touching sen-
sitive data. Isolation between the HETEE and the outside
world and among different enclaves is achieved physically
with the PCIe switch and the proxies, each controlling a
separate enclave. Restoration to secure states for the proxy
node happens through secure reboot of the server. This design
avoids software-based enclave control, isolation, etc., thereby
reducing the TCB size.
We performed a security analysis of HETEE and further im-
plemented it on a PCIe ExpressFabric backplane, CPU+FPGA
(for the SC), Intel Xeon E3 based microservers (for the
proxy nodes), and 4 Nvidia TITAN X GPUs, with the
TCB including only necessary security and management code,
excluding the heavy software stack for controlling accelerators
or executing the AI runtime. Running the implementation
on DNN training and inference tasks of the real-world scale
(152-layer ResNet network, with 60 MB parameters and 200
MB model size on the ∼138GiB ImageNet data set), we
observed an average 1.96% throughput overhead and 34.51%
latency overhead for inference, 0.60% throughput overhead
and 14.24% latency overhead for training.
Contributions. The contributions of this paper are summa-
rized as follows:
• New TEE design for scalable conﬁdential computing. We
present the ﬁrst design for data-center level TEE, supporting
super large-scale conﬁdential computing. Our design lever-
ages the state-of-the-art computing unit pool
technologies
to dynamically allocate computing resources for both secure
and non-sensitive computing tasks across all servers on a
rack. It further reduces security risks using a centralized yet
cost-effective HETEE box (with the expense of conﬁdential
computing hardware below 5% of the cost of computing units),
and hardware-based TCB simpliﬁcation.
• Implementation and evaluation. We implemented our design
and evaluated it on large-scale DNN training and inference
tasks. Our study shows that our approach largely preserves
the performance of heterogeneous computing in the trusted
execution environment, which has never been achieved before.
Roadmap. The rest of the paper is organized as follows: Sec-
tion II presents the background and threat model; Section III
provides the HETEE design; Section IV and V describe the
HETEE prototype system and typical conﬁdential AI comput-
ing services; Section VI reports the performance evaluations;
Section VII elaborates our security analysis and Section VIII
is the discussion; Section IX surveys the related works and
Section X concludes the paper.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:57:53 UTC from IEEE Xplore.  Restrictions apply. 
1451
II. BACKGROUND
A. Heterogeneous Data-Center Computing Architecture
Heterogeneous computing support. Heterogeneous comput-
ing architectures are commonly used in data centers, since a
large-scale computing task such as DNN training and inference
often needs to be processed jointly by different computing
units (GPUs, other accelerators, etc.). Since conﬁguring hard-
ware systems, particular expensive computing units like GPUs,
based on peak workload usually leads to over-provision,
which increases cost, on-demand resource allocation and cost-
effective architecture are becoming mainstream [74]–[79].
Serving this purpose is a resource-pooling technique (aka.,
resource disaggregation) powered by the PCIe switch network,
which is known to be a mature solution for building high-
density data center servers [28] [29]. For example, Facebook’s
AI hardware acceleration systems (Zion and Kings Canyon
[6]), Nvidia’s DGX-2 [19] [20] and HGX-1/2 series [21]–[24]
all utilize PCIe ExpressFabric to construct their heterogeneous
architectures. The PCIe ExpressFabric chips enable a group
of CPUs to ﬂexibly share multiple GPUs and other high-
performance IO devices, thereby reducing cost and increasing
resource utility.
PCIe ExpressFabric. Besides the capability of traditional
PCIe switch chip, PCIe ExpressFabric chip is also featured
with two unique properties important to our design:
• Software-deﬁned fabric. The switch is built on a hybrid
hardware/software platform that offers high conﬁgurability and
ﬂexibility with regards to the number of hosts, end-points,
and PCIe slots. Its critical pathways have direct hardware
support, enabling the fabric to offer non-blocking, line speed
performance with features such as I/O sharing. The chip has
a dedicated port for management, through which an external
management CPU (mCPU) can initialize the switch, conﬁgure
its routing tables, handle errors, Hot-Plug events, and others.
In this way, all the hosts connected by the switch only see
what the mCPU allows them to see.
• Flexible topology. The switch eliminates the topology re-
strictions of PCIe. Usually, PCI Express networks must be
arranged in a hierarchical topology, with a single path from
one point to another. ExpressFabric allows other topologies
such as mesh.
B. Trusted Execution Environment
A trusted execution environment (TEE) guarantees that
the code and data loaded into an isolated area (called an
enclave) are protected to ensure their conﬁdentiality, integrity
and authenticity. TEE is designed to thwart not only the OS-
level adversary but also the malicious party who has physical
access to the platform. To this end, it offers hardware-enforced
security features including isolated execution, integrity and
conﬁdentiality protection of the enclave, along with the ability
to authenticate the code running inside a trusted platform
through attestation:
• Isolation. Data within the enclave cannot be read or modiﬁed
by untrusted parties.
• Integrity. Runtime states should not be tampered with.
• Conﬁdentiality. Code, data and runtime states should not be
observable to unauthorized applications.
• Authentication. The code under execution has been correctly
instantiated on a trusted platform.
Existing TEEs,
including Trustzone and SGX, and the
solutions that extend CPU TEEs to protect heterogeneous
units [15] [16] are focused on protecting the operations of
individual computing units, not their high-performance inter-
actions. Such designs expose a large surface to side channel
attacks during heterogeneous computing and also increase
overhead when computing results move from one enclave to
the other. As a result, they are less suitable for supporting
super large-scale conﬁdential computing.
C. Threat Model
We consider a strong adversary who controls the entire
software stack on host systems and has physical access to
the HETEE platform, as elaborated below:
(Privileged) software adversary. HETEE defends against
the adversary with full control of the software stack on the
host systems, including unprivileged software running on the
host and the host OS. Such an adversary can also mount a
side channel attack e.g. by analyzing network trafﬁc. Covert
channels are currently out of the scope of the paper.
Hardware adversary. An adversary with physical access to
the server can mount snooping attacks on the host memory
bus. We assume that the adversary cannot physically tamper
with the HETEE box, as the box used in a data center can be