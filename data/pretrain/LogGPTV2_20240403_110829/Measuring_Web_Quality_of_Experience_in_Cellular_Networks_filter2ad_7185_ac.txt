### Measuring Web Quality of Experience in Cellular Networks

#### 3. Results and Discussion

This section presents the Above-the-Fold (ATF) time and Page Load Time (PLT) of websites accessed from buses and trains in motion, as well as from stationary locations. The results indicate that most websites exhibit similar PLT in both mobile and stationary scenarios. However, the ATF time for some websites is notably longer under mobile conditions. For example, in the median case, the ATF time for Microsoft, Yahoo, Reddit, and Facebook is 0.3 to 1 second longer in a mobile environment.

Yahoo's ATF time behavior differs significantly between stationary and mobile nodes. Specifically, 60% of the measurements from mobile nodes and 40% from stationary nodes show a drastic change (more than 7 seconds difference) in ATF time. To investigate the causes of this discrepancy, we analyzed the ATF time of Yahoo across different network operators. We found that in Norwegian operators, Yahoo takes a longer time to display content in the above-the-fold area. One possible reason for this is the IP path length between the operators and the Yahoo content server. Using traceroute measurements, we observed that nodes hosted by Norwegian operators traverse up to 20 IP hops to reach the Yahoo web server, while Swedish operators require a maximum of 16 IP hops.

#### 4. Related Work

The web has been extensively studied, and various tools and methodologies for measuring Web Quality of Experience (QoE) are available [8, 9, 25, 35]. Most of these tools focus on fixed-line networks. For instance, Varvello et al. [35] developed eyeorg, a platform for crowdsourcing web QoE measurements. This platform shows a video of the page loading progress to provide a consistent view to all participants, regardless of their network connections and device configurations. In contrast, our measurement tool does not require user interaction to evaluate web QoE; instead, it uses different approaches to approximate QoE.

Cechet et al. [18] designed mBenchLab to measure web QoE on smartphones and tablets by accessing cloud-hosted web services. They measured the performance of popular websites and identified QoE issues by observing PLT, a traditional web QoE metric. Casas et al. [17] studied QoE provisioning for popular mobile applications using subjective laboratory tests with end-devices and passive measurements. They also examined QoE from feedback obtained through crowd-sourcing in operational Mobile Network Operators (MNOs). Their study highlighted the impact of access bandwidth and latency on the QoE of different services, including web browsing on Google Chrome.

Balachandran et al. [13] proposed a machine learning approach to infer web QoE metrics from network traces and studied the impact of network characteristics on web QoE. They found that web QoE is more sensitive to inter-radio technology handovers. Improving the signal-to-noise ratio, decreasing the load, and reducing handover frequency can enhance QoE. Ahmad et al. [4] analyzed call-detail records and studied WAP support for popular websites in developing regions. Nejati et al. [31] built a testbed to compare low-level page load activities in mobile and non-mobile browsers, revealing that computational activities are the main bottleneck for mobile browsers, indicating the need for browser optimizations to improve mobile web QoE. Dasari et al. [20] studied the impact of device performance on mobile Internet QoE, finding that web applications are more sensitive to low-end hardware devices compared to video applications.

Meteor [32] is a measurement tool that determines network speed and estimates the user experience for selected popular applications based on their connection requirements. The methodology used by Meteor is not fully disclosed, and it is unclear how the expected experience is computed or which performance metrics are used. It may be based on Quality of Service (QoS) metrics like throughput and latency, which may not be the only factors affecting application performance [20]. Unlike Meteor, we measure various metrics at the network and application levels, such as Time to First Byte (TTFB), PLT, and ATF time at the browser, which are more relevant from the user's perspective.

WebPageTest [2] and Google Lighthouse [24] are other tools designed to assess web performance from different locations and using different network and device types. These tools measure PLT, SpeedIndex, TTFB, Time to Visually Complete (TTVC), First Contentful Paint (FCP), First Meaningful Paint (FMP), Time to Interactive (TTI), and Last Visual Change metrics. WebLAR measures ATF time but does not yet include SpeedIndex, TTVC, TTI, and FCP. SpeedIndex [3] is a metric proposed by Google to measure the visual completeness of a webpage. It can be approximated either by capturing a video of the webpage download progress or by using the paint events exposed by WebKit. We have made WebLAR publicly available [7] and invite the measurement community to contribute to its improvement.

#### 5. Conclusions

We presented the design and implementation of WebLAR, a measurement tool that evaluates web latency and QoE in cellular networks. We used ATF time as a metric to approximate the end-user experience. Two different approaches were employed to approximate ATF time: pixel-wise comparison and browser heuristics. WebLAR was deployed on the MONROE platform for two weeks. The results show that the DNS lookup time and PLT of the selected websites perform similarly in LTE and fixed-line networks. However, the TCP connect time and TTFB of the websites are longer in LTE networks. Additionally, the DNS lookup time and TCP connect time vary across MNOs. For most websites, PLT and ATF time do not differ significantly across operators. We observed that mobility has a minor impact on the ATF time of websites. We also demonstrated that the website design should be considered when using the two approaches to approximate ATF time.

**Limitations and Future Work:** This study only measured eight websites and did not include a subjective QoE evaluation. We also did not consider the impact of device capabilities on web QoE since our measurement nodes were homogeneous. In the future, we plan to extend WebLAR to capture additional metrics such as RUM SpeedIndex, TTI, and First Contentful Paint, and to evaluate ATF time using different screen sizes.

#### Appendix A: List and Category of Measured Webpages

The websites were selected from various categories, including social media, news websites, and Wikipedia pages. The design of the websites (ranging from simple to media-rich complex webpages) and their purpose were taken into consideration. For each website, a specific webpage that does not require user interaction to display meaningful content was chosen.

- **News Websites:**
  - http://www.bbc.com
  - https://news.google.com
  - https://en.wikipedia.org/wiki/Alan_Turing
  - https://www.reddit.com
  - https://www.youtube.com
  - https://www.facebook.com/places/Things-to-do-in-Paris-France/110774245616525

- **Social Media Websites:**
  - https://www.facebook.com
  - https://www.twitter.com

- **General Websites:**
  - https://www.microsoft.com
  - https://www.yahoo.com

#### Appendix B: Additional Observations

Although not specific to the mobility scenario, Figure 5(2) also shows that PLT can under- or over-estimate web QoE. For example, for Facebook, the `onLoad` event fires before all necessary web objects in the above-the-fold area are downloaded, leading to an underestimation of user QoE. Conversely, for websites like Yahoo and Reddit, the ATF time is shorter compared to PLT, resulting in an overestimation of user QoE.

#### References

1. ImageMagick: Tool to create, edit, compose, or convert bitmap images. https://imagemagick.org. Accessed 12 Oct 2018.
2. WebPageTest. https://www.webpagetest.org. Accessed 09 Jan 2019.
3. WebPagetest Metrics: SpeedIndex. https://sites.google.com/a/webpagetest.org/docs/using-webpagetest/metrics/speed-index. Accessed 15 Oct 2018.
4. Ahmad, S., Haamid, A.L., Qazi, Z.A., Zhou, Z., Benson, T., Qazi, I.A.: A view from the other side: understanding mobile phone characteristics in the developing world. In: ACM IMC (2016). http://dl.acm.org/citation.cfm?id=2987470.
5. Akamai White Paper: Measuring Real Customer Experiences over Mobile Networks. https://www.akamai.com/jp/ja/multimedia/documents/white-paper/measuring-real-customer-experiences-over-mobile-networks-report.pdf. Accessed 12 Oct 2017.
6. Alay, Ö., et al.: Experience: An open platform for experimentation with commercial mobile broadband networks. In: ACM MobiCom (2017). https://doi.org/10.1145/3117811.3117812.
7. Asrese, A.S.: WebLAR: A Web Performance Measurement Tool (2019). https://github.com/alemnew/weblar.
8. Asrese, A.S., Eravuchira, S.J., Bajpai, V., Sarolahti, P., Ott, J.: Measuring web latency and rendering performance: method, tools & longitudinal dataset. IEEE Trans. Netw. Serv. Manag. (2019, to appear).
9. Asrese, A.S., Sarolahti, P., Boye, M., Ott, J.: WePR: A tool for automated web performance measurement. In: IEEE Globecom Workshops (2016). https://doi.org/10.1109/GLOCOMW.2016.7849082.
10. Asrese, A.S., Walelgne, E., Bajpai, V., Lutu, A., Alay, Ö., Ott, J.: Measuring web quality of experience in cellular networks (dataset) (2019). https://github.com/alemnew/2019-pam-weblar.
11. Bajpai, V., Kühlewind, M., Ott, J., Schönwälder, J., Sperotto, A., Trammell, B.: Challenges with reproducibility. In: SIGCOMM Reproducibility Workshop, pp. 1–4 (2017). https://doi.org/10.1145/3097766.3097767.
12. Bajpai, V., Schönwälder, J.: A survey on internet performance measurement platforms and related standardization efforts. IEEE Commun. Surv. Tutor. 17(3), 1313–1341 (2015). https://doi.org/10.1109/COMST.2015.2418435.
13. Balachandran, A., et al.: Modeling web quality-of-experience on cellular networks. In: ACM MobiCom (2014). https://doi.org/10.1145/2639108.2639137.
14. Barakovic, S., Skorin-Kapov, L.: Multidimensional modeling of quality of experience for mobile web browsing. Comput. Hum. Behav. 50, 314–332 (2015). https://doi.org/10.1016/j.chb.2015.03.071.
15. Brutlag, J., Abrams, Z., Meenan, P.: Above the Fold Time: Measuring Web Page Performance Visually. https://conferences.oreilly.com/velocity/velocity-mar2011/public/schedule/detail/18692.
16. Cao, Y., Nejati, J., Wajahat, M., Balasubramanian, A., Gandhi, A.: Deconstructing the energy consumption of the mobile page load. Proc. ACM Meas. Anal. Comput. Syst. 1(1), 6 (2017). https://doi.org/10.1145/3084443.
17. Casas, P., Seufert, M., Wamser, F., Gardlo, B., Sackl, A., Schatz, R.: Next to you: monitoring quality of experience in cellular networks from the end-devices. IEEE Trans. Netw. Serv. Manag. 13(2), 181–196 (2016). https://doi.org/10.1109/TNSM.2016.2537645.
18. Cecchet, E., Sims, R., He, X., Shenoy, P.J.: mBenchLab: Measuring QoE of Web applications using mobile devices. In: International Symposium on Quality of Service, IWQoS (2013). https://doi.org/10.1109/IWQoS.2013.6550259.
19. Chen, Q.A., et al.: QoE doctor: Diagnosing mobile app QoE with automated UI control and cross-layer analysis. In: ACM Internet Measurement Conference (2014). https://doi.org/10.1145/2663716.2663726.
20. Dasari, M., Vargas, S., Bhattacharya, A., Balasubramanian, A., Das, S.R., Ferdman, M.: Impact of device performance on mobile internet QoE. In: Internet Measurement Conference, pp. 1–7 (2018). https://doi.org/10.1145/3278532.3278533.
21. DoubleClick: The Need for Mobile Speed: Better User Experiences, Greater Publisher Revenue. https://goo.gl/R4Lmfh. Accessed 26 Feb 2018.
22. Eravuchira, S.J., Bajpai, V., Schönwälder, J., Crawford, S.: Measuring web similarity from dual-stacked hosts. In: Conference on Network and Service Management, pp. 181–187 (2016). https://doi.org/10.1109/CNSM.2016.7818415.
23. FFmpeg: FFmpeg: A complete, cross-platform solution to record, convert and stream audio and video. https://ffmpeg.org. Accessed 12 Oct 2018.
24. Google: Lighthouse: An open-source, automated tool for improving the quality of web pages. https://developers.google.com/web/tools/lighthouse. Accessed 09 Jan 2019.
25. da Hora, D.N., Asrese, A.S., Christophides, V., Teixeira, R., Rossi, D.: Narrowing the gap between QoS metrics and Web QoE using above-the-fold metrics. In: Beverly, R., Smaragdakis, G., Feldmann, A. (eds.) PAM 2018. LNCS, vol. 10771, pp. 31–43. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-76481-8_3.
26. Hosek, J., et al.: Mobile web QoE study for smartphones. In: IEEE GLOBECOM Workshop (2013). https://doi.org/10.1109/GLOCOMW.2013.6825149.
27. Hoßfeld, T., Metzger, F., Rossi, D.: Speed index: Relating the industrial standard for user perceived web performance to web QoE. In: IEEE International Conference on Quality of Multimedia Experience (2018). https://doi.org/10.1109/QoMEX.2018.8463430.
28. Li, L., et al.: A longitudinal measurement study of TCP performance and behavior in 3G/4G networks over high-speed rails. IEEE/ACM Trans. Netw. 25(4), 2195–2208 (2017). https://doi.org/10.1109/TNET.2017.2689824.
29. Mandalari, A.M., et al.: Experience: Implications of roaming in Europe. In: MobiCom, pp. 179–189 (2018). https://doi.org/10.1145/3241539.3241577.
30. Mozilla: Using the Resource Timing API. https://developer.mozilla.org/en-US/docs/Web/API/Resource_Timing_API/Using_the_Resource_Timing_API. Accessed 24 May 2018.
31. Nejati, J., Balasubramanian, A.: An in-depth study of mobile browser performance. In: Conference on World Wide Web, pp. 1305–1315 (2016). https://doi.org/10.1145/2872427.2883014.
32. OpenSignal: Meteor. https://meteor.opensignal.com. Accessed 12 May 2017.
33. Sackl, A., Casas, P., Schatz, R., Janowski, L., Irmer, R.: Quantifying the impact of network bandwidth fluctuations and outages on Web QoE. In: IEEE International Workshop on Quality of Multimedia Experience (2015). https://doi.org/10.1109/QoMEX.2015.7148078.
34. Sonntag, S., Manner, J., Schulte, L.: Netradar - Measuring the wireless world. In: IEEE International Symposium and Workshops on Modeling and Optimization in Mobile, Ad Hoc and Wireless Networks (2013). http://ieeexplore.ieee.org/document/6576402/.
35. Varvello, M., Blackburn, J., Naylor, D., Papagiannaki, K.: EYEORG: A platform for crowdsourcing web quality of experience measurements. In: ACM Conference on emerging Networking EXperiments and Technologies (2016). https://doi.org/10.1145/2999572.2999590.
36. Walelgne, E.A., Kim, S., Bajpai, V., Neumeier, S., Manner, J., Ott, J.: Factors affecting performance of web flows in cellular networks. In: IFIP Networking (2018).
37. Walelgne, E.A., Manner, J., Bajpai, V., Ott, J.: Analyzing throughput and stability in cellular networks. In: IEEE/IFIP Network Operations and Management Symposium, pp. 1–9 (2018). https://doi.org/10.1109/NOMS.2018.8406261.