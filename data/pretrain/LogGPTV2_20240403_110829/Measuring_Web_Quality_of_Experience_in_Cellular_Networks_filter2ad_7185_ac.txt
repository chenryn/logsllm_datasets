shows the ATF time and PLT of websites from buses and trains which were
moving while the measurement was conducted. The result shows that most of
the websites have almost similar PLT in a mobile and a stationary situation.
However, the ATF time of some of the websites is relatively longer in mobility
scenario. For instance, in the median case, the ATF time of Microsoft, Yahoo,
Reddit, and Facebook is 0.3 to 1 s longer under mobility condition. Yahoo shows
diﬀerent behavior in the ATF time from stationary and mobile nodes. That is,
60% of the measurements from the mobiles nodes, and 40% of the measurements
from the stationary nodes show a drastic change (more than 7 s diﬀerence) of the
ATF time. To understand the causes for this drastic change we analyzed the ATF
time of this website at each operator. We found that in the Norwegian operators
Yahoo takes longer time to show the contents in the above-the-fold area. One
of the causes for this could be the IP path length between the operators and
the Yahoo content server. Using a traceroute measurement we analyzed the
IP path lengths that the nodes traverse to reach the web servers from diﬀerent
locations. We observed that the nodes hosted in Norwegian operators traverse up
to 20 IP hops to reach the Yahoo web server. Instead, other Swedish operators
take a maximum of 16 IP hopes to reach Yahoo’s web server.
4 Related Work
The web has been well studied. Various web QoE measurement tools and
methodologies are available [8,9,25,35]. Most of these tools focus on ﬁxed-line
networks. For instance, Varvello et al. [35] designed eyeorg, a platform for crowd-
sourcing web QoE measurements. The platform shows a video of the page loading
progress to provide a consistent view to all the participants regardless of their
network connections and device conﬁgurations. Unlike eyeorg, our measurement
tool does not require user interaction to evaluate the web QoE, rather it uses
diﬀerent approaches to approximate the web QoE. Cechet et al. [18] designed
mBenchLab that measure web QoE in smartphones and tablets by accessing
cloud hosted web service. They measured the performance of few popular web-
sites and identify the QoE issues observing the PLT, the traditional web QoE
metric. Casas et al. [17] studied the QoE provisioning of popular mobile appli-
cations using subjective laboratory tests with end-device through passive mea-
surement. They also studied QoE from feedback obtained in operational MNOs
using crowd-sourcing. They showed the impact of access bandwidth and latency
on QoE of diﬀerent services including web browsing on Google Chrome.
Measuring Web Quality of Experience in Cellular Networks
29
Balachandran et al. [13] proposed a machine learning approach to infer the
web QoE metrics from the network traces, and studied the impact of network
characteristics on the web QoE. They showed that the web QoE is more sen-
sitive for the inter-radio technology handover. Improving the signal to noise
ratio, decreasing the load and the handover can improve the QoE. Ahmad et
al. [4] analyzed call-detail records and studied WAP support for popular web-
sites in developing regions. Nejati et al. [31] built a testbed that allows com-
paring the low-level page load activities in mobile and non-mobile browsers.
They showed that computational activities are the main bottlenecks for mobile
browsers, which indicates that browser optimizations are necessary to improve
the mobile web QoE. Dasari et al. [20] studied the impact of device performance
on mobile Internet QoE. Their study revealed that web applications are more
sensitive for low-end hardware devices compared to video applications.
Meteor [32] is a measurement tool which determines the speed of the network
and estimates the experience that the user can expect while using selected pop-
ular applications given their connection requirements. The methodology used
by Meteor is not open aside from the high-level explanation of the system. It
is not clear how the expected experience is computed and which performance
metrics are used for a given application. Perhaps, it is based on QoS metrics
like throughput and latency test, which may not be the only factors that aﬀect
the performance of diﬀerent application [20]. Unlike Meteor, we measure diﬀer-
ent metrics at the network and application level, e.g., TTFB, PLT, as well as
ATF time at the browser which is more important from the user perspective.
WebPageTest [2] and Google Lighthouse [24] are other tools designed to assess
the web performance from diﬀerent locations using diﬀerent network and device
types. These tools measure PLT, SpeedIndex, TTFB, time to visually complete
(TTVC), ﬁrst contentful paint (FCP), ﬁrst meaningful paint (FMP), time to
interactive (TTI), and last visual change metrics. WebLAR measures the ATF
time, but it does not measure SpeedIndex, TTVC, TTI, and FCP yet. SpeedIn-
dex [3] is a metric proposed by Google to measure the visual completeness of
a webpage. It can be approximated either by capturing video of the webpage
download progress or by using the paint events exposed by Webkit. We make
WebLAR publicly available [7] and invite the measurement community for con-
tributions to help improve this tool.
5 Conclusions
We presented the design and implementation of WebLAR – a measurement tool
that measures web latency and QoE in the cellular network. We applied ATF
time as the metric to approximate the end-user experience. We followed two
diﬀerent approaches to approximate the ATF time: pixel-wise comparison and
the browser heuristics. We deployed WebLAR on the MONROE platform for
two weeks. The results show that the DNS lookup time and PLT of the selected
websites have similar performance in LTE and ﬁxed-line networks. However, the
TCP connect time and TTFB of the websites are longer in LTE networks. More-
over, the DNS lookup time and TCP connect time of the websites varies across
30
A. S. Asrese et al.
MNOs. For most of the websites, PLT, and ATF time do not have a signiﬁcant
diﬀerence across operators. We observed that mobility has small impact on the
ATF time of the websites. We also showed that the design of the website should
be taken into account when using two approaches to approximate the ATF time.
Limitations and Future Work: We only measured eight websites in this study
and did not perform a subjective QoE evaluation. We also did not consider the
impact of device capabilities on the web QoE since our measurement nodes were
homogenous. In the future, we plan to extend WebLAR to capture other metrics
such as RUM SpeedIndex, TTI, ﬁrst contentful paint and also evaluate the ATF
time using diﬀerent screen sizes.
Appendix A List and Category of Measured Webpages
The websites are selected from diﬀerent categories such as social media, news
websites, and WIKI pages. Moreover, while selecting these websites, the design
of the websites (from simple to media-rich complex webpages) and the purpose
of the websites are taken into consideration. Furthermore, for each website we
selected a speciﬁc webpage that does not require user interaction to show mean-
ingful contents to the user.
– News websites
– Wiki websites
• http://www.bbc.com
• https://news.google.com
• https://en.wikipedia.org/wiki/Alan Turing
• https://www.reddit.com
• https://www.youtube.com
• https://www.facebook.com/places/Things-to-do-in-Paris-France/
– Social media websites
110774245616525
– General websites
• https://www.microsoft.com
• https://www.yahoo.com.
Appendix B Additional Observations
Although not speciﬁc to mobility scenario, Fig. 5(2) also shows that PLT can
under- or over-estimate the web QoE. For instance, for Facebook, the onLoad
event ﬁres before all the necessary web objects in the above-the-fold area are
downloaded. For these types of websites the PLT underestimates the user QoE.
On the other hand, for websites like Yahoo and Reddit, the ATF is shorter
compared with PLT time, which overestimates the user QoE.
Measuring Web Quality of Experience in Cellular Networks
31
References
1. ImageMagick: tool to create, edit, compose, or convert bitmap images. https://
imagemagick.org. Accessed 12 Oct 2018
2. WebPageTest. https://www.webpagetest.org. Accessed 09 Jan 2019
3. WebPagetest Metrics: SpeedIndex. https://sites.google.com/a/webpagetest.org/
docs/using-webpagetest/metrics/speed-index. Accessed 15 Oct 2018
4. Ahmad, S., Haamid, A.L., Qazi, Z.A., Zhou, Z., Benson, T., Qazi, I.A.: A view
from the other side: understanding mobile phone characteristics in the developing
world. In: ACM IMC (2016). http://dl.acm.org/citation.cfm?id=2987470
5. Akamai White Paper: Measuring Real Customer Experiences over Mobile Networks.
https://www.akamai.com/jp/ja/multimedia/documents/white-paper/measuring-
real-customer-experiences-over-mobile-networks-report.pdf. Accessed 12 Oct 2017
6. Alay, ¨O., et al.: Experience: an open platform for experimentation with commercial
mobile broadband networks. In: ACM MobiCom (2017). https://doi.org/10.1145/
3117811.3117812
7. Asrese, A.S.: WebLAR: A Web Performance Measurement Tool (2019). https://
github.com/alemnew/weblar
8. Asrese, A.S., Eravuchira, S.J., Bajpai, V., Sarolahti, P., Ott, J.: Measuring web
latency and rendering performance: method, tools & longitudinal dataset. IEEE
Trans. Netw. Serv. Manag. (2019, to appear)
9. Asrese, A.S., Sarolahti, P., Boye, M., Ott, J.: WePR: a tool for automated web
performance measurement. In: IEEE Globecom Workshops (2016). https://doi.
org/10.1109/GLOCOMW.2016.7849082
10. Asrese, A.S., Walelgne, E., Bajpai, V., Lutu, A., Alay, ¨O., Ott, J.: Measuring web
quality of experience in cellular networks (dataset) (2019). https://github.com/
alemnew/2019-pam-weblar
11. Bajpai, V., K¨uhlewind, M., Ott, J., Sch¨onw¨alder, J., Sperotto, A., Trammell, B.:
Challenges with reproducibility. In: SIGCOMM Reproducibility Workshop, pp. 1–4
(2017). https://doi.org/10.1145/3097766.3097767
12. Bajpai, V., Sch¨onw¨alder, J.: A survey on internet performance measurement plat-
forms and related standardization eﬀorts. IEEE Commun. Surv. Tutor. 17(3),
1313–1341 (2015). https://doi.org/10.1109/COMST.2015.2418435
13. Balachandran, A., et al.: Modeling web quality-of-experience on cellular networks.
In: ACM MobiCom (2014). https://doi.org/10.1145/2639108.2639137
14. Barakovic, S., Skorin-Kapov, L.: Multidimensional modelling of quality of experi-
ence for mobile web browsing. Comput. Hum. Behav. 50, 314–332 (2015). https://
doi.org/10.1016/j.chb.2015.03.071
15. Brutlag, J., Abrams, Z., Meenan, P.: Above the Fold Time: Measuring Web Page
Performance Visually. https://conferences.oreilly.com/velocity/velocity-mar2011/
public/schedule/detail/18692
16. Cao, Y., Nejati, J., Wajahat, M., Balasubramanian, A., Gandhi, A.: Deconstructing
the energy consumption of the mobile page load. Proc. ACM Meas. Anal. Comput.
Syst. 1(1), 6 (2017). https://doi.org/10.1145/3084443
17. Casas, P., Seufert, M., Wamser, F., Gardlo, B., Sackl, A., Schatz, R.: Next to
you: monitoring quality of experience in cellular networks from the end-devices.
IEEE Trans. Netw. Serv. Manag. 13(2), 181–196 (2016). https://doi.org/10.1109/
TNSM.2016.2537645
32
A. S. Asrese et al.
18. Cecchet, E., Sims, R., He, X., Shenoy, P.J.: mBenchLab: measuring QoE of Web
applications using mobile devices. In: International Symposium on Quality of Ser-
vice, IWQoS (2013). https://doi.org/10.1109/IWQoS.2013.6550259
19. Chen, Q.A., et al.: QoE doctor: diagnosing mobile app QoE with automated UI
control and cross-layer analysis. In: ACM Internet Measurement Conference (2014).
https://doi.org/10.1145/2663716.2663726
20. Dasari, M., Vargas, S., Bhattacharya, A., Balasubramanian, A., Das, S.R., Ferd-
man, M.: Impact of device performance on mobile internet QoE. In: Internet Mea-
surement Conference, pp. 1–7 (2018). https://doi.org/10.1145/3278532.3278533
21. DoubleClick: The Need for Mobile Speed: Better User Experiences, Greater Pub-
lisher Revenue. https://goo.gl/R4Lmfh. Accessed 26 Feb 2018
22. Eravuchira, S.J., Bajpai, V., Sch¨onw¨alder, J., Crawford, S.: Measuring web similar-
ity from dual-stacked hosts. In: Conference on Network and Service Management,
pp. 181–187 (2016). https://doi.org/10.1109/CNSM.2016.7818415
23. FFmpeg: FFmpeg: a complete, cross-platform solution to record, convert and
stream audio and video. https://ﬀmpeg.org. Accessed 12 Oct 2018
24. Google: Lighthouse: an open-source, automated tool for improving the quality of
web pages. https://developers.google.com/web/tools/lighthouse. Accessed 09 Jan
2019
25. da Hora, D.N., Asrese, A.S., Christophides, V., Teixeira, R., Rossi, D.: Narrowing
the gap between QoS metrics and Web QoE using above-the-fold metrics. In: Bev-
erly, R., Smaragdakis, G., Feldmann, A. (eds.) PAM 2018. LNCS, vol. 10771, pp.
31–43. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-76481-8 3
26. Hosek, J., et al.: Mobile web QoE study for smartphones. In: IEEE GLOBECOM
Workshop (2013). https://doi.org/10.1109/GLOCOMW.2013.6825149
27. Hoßfeld, T., Metzger, F., Rossi, D.: Speed index: relating the industrial standard for
user perceived web performance to web QoE. In: IEEE International Conference on
Quality of Multimedia Experience (2018). https://doi.org/10.1109/QoMEX.2018.
8463430
28. Li, L., et al.: A longitudinal measurement study of TCP performance and behavior
in 3G/4G networks over high speed rails. IEEE/ACM Trans. Netw. 25(4), 2195–
2208 (2017). https://doi.org/10.1109/TNET.2017.2689824
29. Mandalari, A.M., et al.: Experience: implications of roaming in Europe. In: MOBI-
COM, pp. 179–189 (2018). https://doi.org/10.1145/3241539.3241577
30. Mozilla: Using the Resource Timing API. https://developer.mozilla.org/en-
US/docs/Web/API/Resource Timing API/Using the Resource Timing API.
Accessed 24 May 2018
31. Nejati, J., Balasubramanian, A.: An in-depth study of mobile browser performance.
In: Conference on World Wide Web, pp. 1305–1315 (2016). https://doi.org/10.
1145/2872427.2883014
32. OpenSignal: Meteor. https://meteor.opensignal.com. Accessed 12 May 2017
33. Sackl, A., Casas, P., Schatz, R., Janowski, L., Irmer, R.: Quantifying the impact of
network bandwidth ﬂuctuations and outages on Web QoE. In: IEEE International
Workshop on Quality of Multimedia Experience (2015). https://doi.org/10.1109/
QoMEX.2015.7148078
34. Sonntag, S., Manner, J., Schulte, L.: Netradar - measuring the wireless world.
In: IEEE International Symposium and Workshops on Modeling and Optimiza-
tion in Mobile, Ad Hoc and Wireless Networks (2013). http://ieeexplore.ieee.org/
document/6576402/
Measuring Web Quality of Experience in Cellular Networks
33
35. Varvello, M., Blackburn, J., Naylor, D., Papagiannaki, K.: EYEORG: a platform
for crowdsourcing web quality of experience measurements. In: ACM Conference
on emerging Networking EXperiments and Technologies (2016). https://doi.org/
10.1145/2999572.2999590
36. Walelgne, E.A., Kim, S., Bajpai, V., Neumeier, S., Manner, J., Ott, J.: Factors
aﬀecting performance of web ﬂows in cellular networks. In: IFIP Networking (2018)
37. Walelgne, E.A., Manner, J., Bajpai, V., Ott, J.: Analyzing throughput and sta-
bility in cellular networks. In: IEEE/IFIP Network Operations and Management
Symposium, pp. 1–9 (2018). https://doi.org/10.1109/NOMS.2018.8406261