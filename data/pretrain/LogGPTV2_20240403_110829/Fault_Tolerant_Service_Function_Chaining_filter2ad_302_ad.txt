prised of several datacenters deployed across Canada. We use virtual
machines with 4 virtual processor cores and 8 GiB memory running
Ubuntu 14.04 with Kernel 4.4. We use the published ONOS docker
container [39] to control a virtual network of OVS switches [38]
connecting these virtual machines. We follow the multiple inter-
leaved trials methodology [4] to reduce the variability that come
from performing experiments on a shared infrastructure.
We use the middleboxes and chains shown in Table 1. The mid-
dleboxes are implemented in Click [34]. MazuNAT is an implemen-
tation of the core parts of a commercial NAT [2], and SimpleNAT
Fault Tolerant Service Function Chaining
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
Figure 5: Throughput vs. state size
Figure 6: Throughput of Monitor
Figure 7: Throughput of MazuNAT
provides basic NAT functionalities. They represent read-heavy mid-
dleboxes with a moderate write load on the shared state. Monitor
is a read/write heavy middlebox that counts the number of packets
in a flow or across flows. It takes a sharing level parameter that
specifies the number of threads sharing the same state variable.
For example, no state is shared for the sharing level 1, and all 8
threads share the same state variable for sharing level 8. Gen rep-
resents a write-heavy middlebox that takes a state size parameter,
which allows us to test the impact of a middlebox’s state size on
performance. Firewall is stateless. Our experiments also test three
chains comprised of these middleboxes, namely Ch-n, Ch-Gen, and
Ch-Rec.
For experiments in the first environment, we report latency
and throughput. For a latency data point, we report the average of
hundreds of samples taken in a 10 second interval. For a throughput
data point, we report the average of maximum throughput values
measured every second in a 10 second interval. Unless shown, we
do not report confidence intervals as they are negligible. Unless
specified otherwise, the packet size in our experiments is 256 B,
and f = 1.
7.2 Microbenchmark
Performance breakdown: To benchmark FTC, we breakdown the
performance of the MazuNAT middlebox configured with eight
threads in a chain of length two. We show the results for a single
thread, but we observed similar results across all threads (assuming
low contention). The results only show the computational overhead
and exclude device and network IO. In § 7.3, we discuss FTC’s impact
on end-to-end latency.
Table 2 shows the per packet processing cost in CPU cycles.
Packet transaction execution, which includes both packet process-
ing and locking, is the primary contributor to packet latency. The
latency due to copying piggybacked state is negligible, because the
state is updated per network flow, and the size of updated state is
small. The latencies of the forwarder and buffer are also small, and
they are independent of the chain length. Only the first and last
middlebox contain the forwarder and buffer respectively.
State size performance impact: We also use a micro-benchmark to
determine the impact of a state size on the performance of FTC. We
measured the latency overhead for the middlebox Gen and the chain
Ch-Gen. We observed that under 2 Mpps for 512 B packets, varying
the size of the generated state from 32–256 B has a negligible impact
Packet processing
Locking
Copying piggybacked state
Forwarder
Buffer
CPU cycles
355 ± 12
152 ± 11
58 ± 6
8 ± 2
100 ± 4
Table 2: Performance breakdown for MazuNAT running in a
chain of length two. This table shows a breakdown of the CPU
overhead for an FTC enabled middlebox.
on latency for both Gen and Ch-Gen (the difference is less than 2 µs).
Thus, we focus on the throughput overhead.
Figure 5 shows the impact of state size generated by Gen on
throughput. Gen runs a single thread. We vary the state size and
measure Gen’s throughput for different packet sizes. As expected,
the size of piggyback messages impacts the throughput only if it is
proportionally large compared to packet sizes. For 128 B packets,
throughput drops by only 9% when Gen generates states that are
128 B in size or less. The throughput drops by less than 1% with
512 B packets and state up to 256 B in size.
We expect popular middleboxes to generate state much smaller
than some of our tested values. For instance, a load balancer and
a NAT generate a record per traffic flow [9, 28, 53] that is roughly
32 B in size (2× 12 B for the IPv4 headers in both directions and 8 B
for the flow identifier). FTC can use jumbo frames to encompass
larger state sizes exceeding standard maximum transmission units.
7.3 Fault-Tolerant Middleboxes
Throughput: Figures 6 and 7 show the maximum throughput of
two middleboxes. In Figure 6, we configure Monitor to run with
eight threads and measure its throughput with different sharing
levels. As the sharing level for Monitor increases, the throughput
of all systems, including NF, drops due to the higher contention in
reading and writing the shared state. For sharing levels of 8 and
2, FTC achieves a throughput that is 1.2× and 1.4× that of FTMB’s
and incurs an overhead of 9% and 26% compared to NF. These
overheads are expected since Monitor is a write-heavy middlebox,
and the shared state is modified non-deterministically per packet.
For sharing level 1, NF and FTC reach the NIC’s packet processing
128256512Packetsize(Bytes)01234567Throughput(Mpps)Statesize16Statesize64Statesize128Statesize2561248Sharinglevel0246810Throughput(Mpps)NFFTCFTMB1248Threads0246810Throughput(Mpps)NFFTCFTMBSIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
(a) Monitor - Sharing level 8
(b) MazuNAT - 1 thread
(c) MazuNAT - 8 threads
Figure 8: Latency of middleboxes
capacity1. FTMB does not scale for sharing level 1, since for every
data packet, a PAL is transmitted in a separate message, which
limits FTMB’s throughput to 5.26 Mpps.
Figure 7 shows our evaluation for MazuNAT’s throughput while
varying the number of threads. FTC’s throughput is 1.37–1.94× that
of FTMB’s for 1 to 4 threads. Once a traffic flow is recorded in the
NAT flow table, processing next packets of this flow only requires
reading the shared record (until the connection terminates or times
out). The higher throughput compared for MazuNAT is because FTC
does not replicate the reads, while FTMB logs them to provide fault
tolerance [51]. We observe that FTC incurs 1–10% throughput over-
head compared to NF. Part of this overhead is because FTC has to
pay the cost of adding space to packets for possible state writes,
even when state writes are not performed.
The pattern of state reads and writes impacts FTC’s throughput.
Under moderate write workloads, FTC incurs 1–10% throughput
overhead, while under write-heavy workloads, FTC’s overhead
remains less than 26%.
Latency: Figure 8 shows the latency of Monitor (8 threads with
sharing level 8) and MazuNAT (two configurations, 1 thread and 8
threads) under different traffic loads. For the both middleboxes,
the latency remains under 0.7 ms for all systems as the traffic load
increases, until the systems reach their respective saturation points.
Past these points, packets start to be queued, and per-packet latency
rapidly spikes.
As shown in Figure 8a, under sustainable loads, FTC and FTMB
respectively introduce overhead within 14–25 µs and 22–31 µs to
the per packet latency, out of which 6–7 µs is due to the extra
one way network latency to forward the packet and state to the
replica. For this write heavy middlebox, FTC adds a smaller latency
overhead compared to FTMB.
Figure 8b shows that, when running MazuNAT with one thread,
FTC can sustain nearly the same traffic load as NF, and FTC and
FTMB have similar latencies. For eight threads shown in Figure 8c,
both FTC and NF reach the packet processing capacity of the NIC.
The latency of FTC is largely independent of the number of threads,
1Although the 40 GbE link is not saturated, our investigation showed that the bottle-
neck is the NIC’s packet processing power. We measured that the Mellanox ConnectX-3
MT 27500, at the receiving side and working under the DPDK driver, at most can
process 9.6–10.6 Mpps for varied packet sizes. Though we have not found any official
document by Mellanox describing this limitation, similar behavior (at higher rates) has
been reported for Intel NICs (see Sections 5.4 and 7.5 in [17] and Section 4.6 in [26]).
while FTMB experiences a latency increase of 24–43 µs when going
from one to eight threads.
7.4 Fault Tolerant Chains
In this section, we report the performance of FTC for a chain of
middleboxes during normal operation. For a NF chain, each middle-
box is deployed in a separate physical server. We do not need more
servers, while we dedicate twice the number of servers to FTMB:
A server for each middlebox (Master in FTMB) and a server for its
replica (IL and OL in FTMB).
Chain length impact on throughput: Figure 9 shows the maxi-
mum traffic throughput passing in four chains (Ch-2 to Ch-5 listed
in Table 1). Monitors in these chains run eight threads with sharing
level 1. We also report for FTMB+Snapshot that is FTMB with snap-
shot simulation. To simulate the overhead of periodic snapshots,
we add an artificial delay (6 ms) periodically (every 50 ms). We get
these values from [51].
As shown in Figure 9, FTC’s throughput is within 8.28–8.92 Mpps
and 4.83–4.80 Mpps for FTMB. FTC imposes a 6–13% throughput
overhead compared to NF. The throughput drop from increasing the
chain length for FTC is within 2–7%, while that of FTMB+Snapshot
is 13–39% (its throughput drops from 3.94 to 2.42 Mpps).
This shows that throughput of FTC is largely independent of
the chain length, while, for FTMB+Snapshot, periodic snapshots
taken at all middleboxes significantly reduce the throughput. No
packet is processed during a snapshot. Packet queues get full at
early snapshots and remain full afterwards because the incoming
traffic load is at the same rate. More snapshots are taken in a longer
chain. Non-overlapping (in time) snapshots cause shorter service
time at each period and consequently higher throughput drops. An
optimum scheduling to synchronize snapshots across the chain can
reduce this overhead; however, this is not trivial [10].
Chain length impact on latency: We use the same settings as
the previous experiment, except we run single threaded Monitors
due to a limitation of the traffic generator. The latter is not able to
measure the latency of the chain beyond size 2 composed of multi-
threaded middleboxes. We resort to use single threaded Monitors
under the load of 2 Mpps, a sustainable load by all systems.
As shown in Figure 10, FTC’s overhead compared to NF is within
39–104 µs for Ch-2 to Ch-5, translating to roughly 20 µs latency
0.51.01.52.02.53.0Trafﬁcload(MPPS)242526272829210Latency(µs)NFFTCFTMB0.51.01.52.02.53.0Trafﬁcload(MPPS)242526272829210Latency(µs)NFFTCFTMB123456789Trafﬁcload(MPPS)242526272829210Latency(µs)NIC’slimitNFFTCFTMBFault Tolerant Service Function Chaining
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
Figure 9: Tput vs. chain length
Figure 10: Latency vs. chain length
Figure 11: Ch-3 per packet latency
Table 1), when each of its middleboxes fails separately. Each mid-
dlebox is placed in a different region of our Cloud testbed. As the
orchestrator detects a failure, a new replica is placed in the same
region as the failed middlebox. The head of Firewall is deployed in
the same region as the orchestrator, while the heads of SimpleNAT
and Monitor are respectively deployed in a neighboring region and
a remote region compared to the orchestrator’s region. Since the
orchestrator is also a SDN controller, we observe negligible values
for the rerouting delay, thus we focus on the state recovery delay
and initialization delay.
Figure 12: Repl. factor
Figure 13: Recovery time
per middlebox. The overhead of FTMB is within 64–171 µs, approxi-
mately 35 µs latency overhead per middlebox in the chain. As shown
in Figure 11, the tail latency of individual packets passing through
Ch-3 is only moderately higher than the minimum latency. FTC in-
curs 16.5–20.6 µs per middlebox latency which is respectively three
and two orders of magnitudes less than Pico’s and REINFORCE’s,
and is around 2/3 of FTMB’s.
In-chain replication eliminates the communication overhead
with remote replicas. Doing so also does not cause latency spikes
unlike snapshot-based systems. In FTC, packets experience constant
latency, while the original FTMB reports up to 6 ms latency spikes
at periodic checkpoints (e.g., at every 50 ms intervals) [51].
Replication factor impact on performance: For replication factors
of 2–5 (i.e., tolerating 1 to 5 failures), Figure 12 shows FTC’s per-
formance for Ch-5 in two settings where Monitors run with 1 or
8 threads. We report the throughput of 8 threaded Monitor, while
only report the latency of 1 threaded Monitor due to a limitation
of our test harness.
To tolerate 2.5× failures, FTC incurs only 3% throughput over-
head as its throughput decreases to 8.06 Mpps. The latency overhead
is also insignificant as latency only increases by 8 µs. By exploiting
the chain structure, FTC can tolerate a higher number of failures
without sacrificing performance. However, the replication factor
cannot be arbitrarily large as encompassing the resulting large
piggyback messages inside packets becomes impractical.
7.5 FTC in Failure Recovery
Recall from § 6, failure recovery is performed in three steps: ini-
tialization, state recovery, and rerouting delays. To evaluate FTC
during recovery, we measure the recovery time of Ch-Rec (see
Recovery time: As shown in Figure 13, the initialization delays
are 1.2, 49.8, and 5.3 ms for Firewall, Monitor, and SimpleNAT,
respectively. The longer the distance between the orchestrator and
the new replica, the higher the initialization delay. The state recov-
ery delays are in the range of 114.38±9.38 ms to 270.79±50.47 ms2.
In a local area network, FTMB paper [51] reports comparable re-
covery time of ∼100 ms to 250 ms for SimpleNAT. Upon any failure,
a new replicas fetches the state from a remote region in the cloud,
which causes the WAN latency to dominate delay.
Using ping, we measured the network delay between all pairs of
remote regions, and the observed round-trip times confirmed our
results.
FTC replicates the values of state variables, and its state recovery
delay is bounded by the state size of a middlebox. The replication
factor also has a negligible impact on the recovery time of FTC,
since a new instantiated replica fetches state in parallel from other
replicas.
8 RELATED WORK
In addition to NFV related work discussed in § 2.2, this Section
discusses other relevant systems.
Fault tolerant storage: Prior to FTC, the distributed system lit-
erature used chain and ring structures to provide fault tolerance.
However, their focus is on ordering read/write messages at the pro-
cess level (compared to, middlebox threads racing to access shared
state in our case), at lower non-determinism rates (compared to, per-
packet frequency), and at lower output rates (compared to, several