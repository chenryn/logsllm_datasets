### Leaderboards
- **NIST TrojAI Results:** [Previous Leaderboards](https://pages.nist.gov/trojai/docs/results.html#previous-leaderboards), 2021.

### References

1. A. Saha, A. Subramanya, and H. Pirsiavash, "Hidden trigger backdoor attacks," in *Proceedings of the AAAI Conference on Artificial Intelligence*, vol. 34, no. 07, 2020, pp. 11957–11965.
2. Y. Liu, X. Ma, J. Bailey, and F. Lu, "Reflection backdoor: A natural backdoor attack on deep neural networks," in *European Conference on Computer Vision*. Springer, Cham, 2020, pp. 182–199.
3. A. Salem, R. Wen, M. Backes, S. Ma, and Y. Zhang, "Dynamic backdoor attacks against machine learning models," *arXiv preprint arXiv:2003.03675*, 2020.
4. J. Lin, L. Xu, Y. Liu, and X. Zhang, "Composite backdoor attack for deep neural network by mixing existing benign features," in *Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security*, 2020, pp. 113–131.
5. A. Shafahi, W. R. Huang, M. Najibi, O. Suciu, C. Studer, T. Dumitras, and T. Goldstein, "Poison frogs! targeted clean-label poisoning attacks on neural networks," in *Advances in Neural Information Processing Systems*, 2018, pp. 6103–6113.
6. A. Demontis, M. Melis, M. Pintor, M. Jagielski, B. Biggio, A. Oprea, C. Nita-Rotaru, and F. Roli, "Why do adversarial attacks transfer? explaining transferability of evasion and poisoning attacks," in *28th USENIX Security Symposium*, 2019.
7. Y. Yao, H. Li, H. Zheng, and B. Y. Zhao, "Latent backdoor attacks on deep neural networks," in *Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security*, 2019.
8. S. Hong, N. Carlini, and A. Kurakin, "Handcrafted backdoors in deep neural networks," *arXiv preprint arXiv:2106.04690*, 2021.
9. Z. Zhang, J. Jia, B. Wang, and N. Z. Gong, "Backdoor attacks to graph neural networks," *arXiv preprint arXiv:2006.11165*, 2020.
10. Z. Xi, R. Pang, S. Ji, and T. Wang, "Graph backdoor," *arXiv preprint arXiv:2006.11890*, 2020.
11. S. Rezaei and X. Liu, "A target-agnostic attack on deep models: Exploiting security vulnerabilities of transfer learning," *arXiv preprint arXiv:1904.04334*, 2019.
12. B. Wang, Y. Yao, B. Viswanath, H. Zheng, and B. Y. Zhao, "With great training comes great vulnerability: Practical attacks against transfer learning," in *27th USENIX Security Symposium*, 2018.
13. C. Xie, K. Huang, P.-Y. Chen, and B. Li, "Dba: Distributed backdoor attacks against federated learning," in *International Conference on Learning Representations*, 2019.
14. H. Wang, K. Sreenivasan, S. Rajput, H. Vishwakarma, S. Agarwal, J.-y. Sohn, K. Lee, and D. Papailiopoulos, "Attack of the tails: Yes, you really can backdoor federated learning," *Advances in Neural Information Processing Systems*, vol. 33, 2020.
15. V. Tolpegin, S. Truex, M. E. Gursoy, and L. Liu, "Data poisoning attacks against federated learning systems," in *European Symposium on Research in Computer Security*. Springer, 2020, pp. 480–501.
16. E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, "How to backdoor federated learning," in *International Conference on Artificial Intelligence and Statistics*. PMLR, 2020, pp. 2938–2948.
17. M. Fang, X. Cao, J. Jia, and N. Gong, "Local model poisoning attacks to Byzantine-robust federated learning," in *29th USENIX Security Symposium (USENIX Security 20)*, 2020, pp. 1605–1622.
18. P. Kiourti, K. Wardega, S. Jha, and W. Li, "Trojdrl: Evaluation of back-door attacks on deep reinforcement learning," in *2020 57th ACM/IEEE Design Automation Conference (DAC)*. IEEE, 2020, pp. 1–6.
19. L. Wang, Z. Javed, X. Wu, W. Guo, X. Xing, and D. Song, "BackdoorL: Backdoor attack against competitive reinforcement learning," *arXiv preprint arXiv:2105.00579*, 2021.
20. Y. Li, X. Lyu, N. Koren, L. Lyu, B. Li, and X. Ma, "Neural attention distillation: Erasing backdoor triggers from deep neural networks," *arXiv preprint arXiv:2101.05930*, 2021.
21. B. Wang, X. Cao, N. Z. Gong et al., "On certifying robustness against backdoor attacks via randomized smoothing," *arXiv preprint arXiv:2002.11750*, 2020.
22. Y. Cao, A. F. Yu, A. Aday, E. Stahl, J. Merwine, and J. Yang, "Efficient repair of polluted machine learning systems via causal unlearning," in *Proceedings of the 2018 on Asia Conference on Computer and Communications Security*, 2018, pp. 735–747.
23. M. Jagielski, A. Oprea, B. Biggio, C. Liu, C. Nita-Rotaru, and B. Li, "Manipulating machine learning: Poisoning attacks and countermeasures for regression learning," in *2018 IEEE Symposium on Security and Privacy (SP)*. IEEE, 2018, pp. 19–35.
24. M. Mozaffari-Kermani, S. Sur-Kolay, A. Raghunathan, and N. K. Jha, "Systematic poisoning attacks on and defenses for machine learning in healthcare," *IEEE journal of biomedical and health informatics*, 2014.
25. A. Paudice, L. Muñoz-González, and E. C. Lupu, "Label sanitization against label flipping poisoning attacks," in *Joint European Conference on Machine Learning and Knowledge Discovery in Databases*, 2018.
26. J. Li, S. Ji, T. Du, B. Li, and T. Wang, "Textbugger: Generating adversarial text against real-world applications," *arXiv preprint arXiv:1812.05271*, 2018.
27. J. Gao, J. Lanchantin, M. L. Soffa, and Y. Qi, "Black-box generation of adversarial text sequences to evade deep learning classifiers," in *2018 IEEE Security and Privacy Workshops (SPW)*. IEEE, 2018, pp. 50–56.
28. S. Garg and G. Ramakrishnan, "BAE: BERT-based adversarial examples for text classification," *arXiv preprint arXiv:2004.01970*, 2020.
29. D. Jin, Z. Jin, J. T. Zhou, and P. Szolovits, "Is BERT really robust? A strong baseline for natural language attack on text classification and entailment," in *Proceedings of the AAAI conference on artificial intelligence*, vol. 34, no. 05, 2020, pp. 8018–8025.
30. L. Li, R. Ma, Q. Guo, X. Xue, and X. Qiu, "BERT-Attack: Adversarial attack against BERT using BERT," *arXiv preprint arXiv:2004.09984*, 2020.
31. S. Ren, Y. Deng, K. He, and W. Che, "Generating natural language adversarial examples through probability weighted word saliency," in *Proceedings of the 57th annual meeting of the association for computational linguistics*, 2019, pp. 1085–1097.
32. M. Alzantot, Y. Sharma, A. Elgohary, B.-J. Ho, M. Srivastava, and K.-W. Chang, "Generating natural language adversarial examples," *arXiv preprint arXiv:1804.07998*, 2018.
33. T. Wilson, J. Wiebe, and P. Hoffmann, "Recognizing contextual polarity in phrase-level sentiment analysis," in *Proceedings of human language technology conference and conference on empirical methods in natural language processing*, 2005, pp. 347–354.
34. S. Zhao, X. Ma, X. Zheng, J. Bailey, J. Chen, and Y.-G. Jiang, "Clean-label backdoor attacks on video recognition models," in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2020, pp. 14443–14452.
35. A. Turner, D. Tsipras, and A. Madry, "Clean-label backdoor attacks," 2018.
36. W. Yang, Y. Lin, P. Li, J. Zhou, and X. Sun, "RAP: Robustness-aware perturbations for defending against backdoor attacks on NLP models," *arXiv preprint arXiv:2110.07831*, 2021.
37. M. Fan, Z. Si, X. Xie, Y. Liu, and T. Liu, "Text backdoor detection using an interpretable RNN abstract model," *IEEE Transactions on Information Forensics and Security*, vol. 16, pp. 4117–4132, 2021.
38. V. Sanh, L. Debut, J. Chaumond, and T. Wolf, "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter," *arXiv preprint arXiv:1910.01108*, 2019.
39. A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts, "Learning word vectors for sentiment analysis," in *Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies*, 2011.
40. Z. Sun, H. Yu, X. Song, R. Liu, Y. Yang, and D. Zhou, "MobileBERT: A compact task-agnostic BERT for resource-limited devices," *arXiv preprint arXiv:2004.02984*, 2020.
41. Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, "RoBERTa: A robustly optimized BERT pretraining approach," *arXiv preprint arXiv:1907.11692*, 2019.
42. R. Weischedel and A. Brunstein, "BBN pronoun coreference and entity type corpus," *Linguistic Data Consortium, Philadelphia*, 2005.
43. E. F. Tjong Kim Sang and F. De Meulder, "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition," in *Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003*, 2003.
44. E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel, "OntoNotes: The 90% solution," in *Proceedings of the Human Language Technology Conference of the NAACL*, 2006.
45. A. Salinca, "Business reviews classification using sentiment analysis," in *2015 17th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)*. IEEE, 2015.

### Appendix

#### A. Challenge IV: Generative Model Inefficiency in Complex Trigger Generation

The previous three challenges are related to optimization-based trigger inversion. One potential solution is to avoid trigger inversion and use a generative model to inject triggers into input samples, such as T-miner [38]. Such generators can ensure that the generated inputs comply with the language, avoiding the infeasibility problem. However, they require prior knowledge of the trigger distribution and effective learning of this distribution. In the NLP domain, triggers can be arbitrary characters, words, phrases, sentences, or even paraphrasing patterns. Learning a high-quality generator is very challenging. Additionally, generating a complex trigger that can flip multiple inputs is difficult.

Table VIII shows the triggers generated by T-miner for the example model #231 with random input sentences in the first column. T-miner does not require clean samples but rather random sequences. The second column shows the generated sequences with perturbations in red. Different sequences have different perturbations. The third column shows the ASR for each perturbation word, which are all low. According to our experiments, T-miner achieves less than 0.6 ROC-AUC for a set of TrojAI models from rounds 5 and 6. Furthermore, it requires fine-tuning the generator for each subject model, with an average scanning time of 52 minutes per model, making it more costly than trigger inversion.

#### B. Parameters of PICCOLO

We also use the following parameters, which were not mentioned in the main text due to space constraints:
- In the differentiable word encoding, we use a 7k common word dictionary from [89], which will be released upon publication.
- For the word discriminativity analysis, we use a Gaussian distribution with a mean of 0 and a standard deviation of 2.
- During trigger optimization, we run the optimization for 100 epochs with a learning rate of 0.5.
- Delayed normalization is applied every 30 epochs.
- The weight parameters in Equation (9) in Section V-D are 1, 0.2, 0.1, and 0.5 for \( w_1 \), \( w_2 \), \( w_{\text{large}} \), and \( w_4 \), respectively.
- The ASR bound is 0.9, and the dot product bound is 170.

#### C. TrojAI Competition

TrojAI is a multi-year backdoor scanning competition organized by IARPA. This competition has 10 official performers and is open to the public. It has completed 7 rounds as of the submission date. Rounds 1-4 focus on computer vision tasks, while rounds 5-7 focus on NLP tasks. Each round provides hundreds to thousands of pre-trained models (half clean and half trojaned) with ground truth as a training set. Test and hold-out sets (of similar or larger size) are hosted and evaluated on IARPA servers. The IARPA team creates trojaned models using common and recent attack methods from the literature.

Performers meet and discuss weekly, developing their own solutions and adapting the latest published defense techniques. For example, both T-miner [38] and Meta Neural Trojan Detection (MNTD) [22] have been applied to rounds 5-7 by at least two performers, including us. Other state-of-the-art scanners such as NC [10], ABS [11], DeepInspect [12], K-Arm backdoor scanning [48], and ULP [15] are for computer vision and thus not applicable.

#### D. Backdoor Attacks and Defenses

**Backdoor Attacks:**
- BadNets [1] and TrojanNN [2] introduced backdoor attacks on deep neural networks. BadNets stamps triggers on part of the training samples and sets their labels to the target label. TrojanNN generates triggers based on model weights and reverse-engineers the training data to inject the trigger.
- Many backdoor attacks have been proposed since then. Based on whether the attack needs data access, there are white-box attacks [1] and black-box attacks [2].
- In white-box attacks, there is another line of work where poisoned samples have the correct label, called clean-label backdoors [90], [91].
- Based on attack ability, there are universal attacks that flip samples of all other classes to the target class and label-specific attacks that flip samples from a victim class to the target class.
- Based on application, there are backdoor attacks on computer vision [1]–[3], [58], NLP tasks [4]–[6], [8], [9], [44], [45], graph neural networks [65], [66], transfer learning [63], [67], [68], federated learning [69]–[73], and reinforcement learning [74], [75].

**Backdoor Defenses:**
- Based on the stage of defense, there are four types:
  1. Defends the data collection stage by removing poisoned samples from the training set [78]–[81].
  2. Defends at the pre-deployment stage by scanning [10]–[22], [38]. PICCOLO belongs to this type.
  3. Aims to repair trojaned models [25], [76].
  4. Defends at the post-deployment stage by detecting trigger inputs [20], [23]–[35], [46], [47], [92], [93].

Most backdoor defense methods fall into computer vision [10]–[20], [20]–[35] or NLP [22], [38], [46], [47], [92], [93].

#### E. Experiment Setup

**TrojAI Rounds 5 and 6:**
- Models in Rounds 5 and 6 are for sentiment classification.
- Round 5 contains 1648 models in the training set and 504 in the test set.
- Round 6 consists of 48 training models and 480 test models.
- Three types of transformer-based architectures, namely, DistilBERT [94], GPT-2 [43], and BERT [42], are employed.
- They are trained on eleven different datasets from Amazon review [51] and IMDB [95].
- For backdoored models, three types of triggers are used: global triggers, first-half triggers, and second-half triggers. Each type can be characters, words, phrases, or sentences.
- Global triggers are effective when inserted at any position of an input sentence. First-/second-half triggers are only effective when inserted at the first/second half of an input sentence.
- In Round 5, the phrase and sentence triggers are semantically meaningful, such as "I watched an 8D movie" or "Harry Potter and the Prisoner of Azkaban." Some triggers are quite long (e.g., more than 30 words) and complicated.
- Phrase triggers in Round 6 are composed of randomly sampled words, e.g., "needfully innumerably mostly irregardless fast."
- Note that a backdoored model in Round 5 can have multiple injected triggers.

**TrojAI Round 7:**
- TrojAI Round 7 focuses on the name entity recognition (NER) task.
- It consists of 192 models in the training set and 384 in the test set.
- Four architectures are utilized: DistilBERT, BERT, MobileBERT [96], and RoBERTa [97].
- Models are trained on three datasets: BNN corpus [98] with 4 name entities, CoNLL-2003 [99] with 4 name entities, and OntoNotes [100] with 6 name entities.
- All triggers in Round 7 are label-specific, meaning the trigger only flips the prediction of words from a victim label to a target label.
- Two types of triggers are adopted in Round 7: global and local. Each type can be characters, words, or phrases.
- Global triggers are effective on all words from the victim label in input sentences. Local triggers are only effective when injected next to the word from the victim label, flipping the predictions of its neighboring words.
- At the time of our evaluation, the ground truth was not available for the Round 7 test set. We evaluated PICCOLO on the TrojAI test server. For the training set, we conducted the experiment locally.

**Additional Models:**
- We also evaluated PICCOLO on models from several existing works.
- Specifically, we trained 103 GRU models on the Yelp dataset [55], [101] used in T-miner [38], focusing on classification tasks.
- We leveraged the official repository from the hidden killer attack [8] to train 120 BERT classification models and 120 LSTM models on SST-2 [40].

#### Table IX: Efficiency of PICCOLO

| Model | Sentiment Classification Models | NER Models |
|-------|---------------------------------|------------|
| Arch. | DistilBERT | BERT |

This table summarizes the efficiency of PICCOLO across different models and tasks.