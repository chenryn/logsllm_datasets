leaderboards,” https://pages.nist.gov/trojai/docs/results.
html#previous-leaderboards, 2021.
[57] A. Saha, A. Subramanya, and H. Pirsiavash, “Hidden trigger backdoor
attacks,” in Proceedings of the AAAI Conference on Artiﬁcial Intelli-
gence, vol. 34, no. 07, 2020, pp. 11 957–11 965.
[58] Y. Liu, X. Ma, J. Bailey, and F. Lu, “Reﬂection backdoor: A natural
backdoor attack on deep neural networks,” in European Conference on
Computer Vision. Springer, Cham, 2020, pp. 182–199.
[59] A. Salem, R. Wen, M. Backes, S. Ma, and Y. Zhang, “Dynamic
backdoor attacks against machine learning models,” arXiv preprint
arXiv:2003.03675, 2020.
[60] J. Lin, L. Xu, Y. Liu, and X. Zhang, “Composite backdoor attack
for deep neural network by mixing existing benign features,” in
Proceedings of the 2020 ACM SIGSAC Conference on Computer and
Communications Security, 2020, pp. 113–131.
[61] A. Shafahi, W. R. Huang, M. Najibi, O. Suciu, C. Studer, T. Dumitras,
and T. Goldstein, “Poison frogs! targeted clean-label poisoning attacks
on neural networks,” in Advances in Neural Information Processing
Systems, 2018, pp. 6103–6113.
[62] A. Demontis, M. Melis, M. Pintor, M. Jagielski, B. Biggio, A. Oprea,
C. Nita-Rotaru, and F. Roli, “Why do adversarial attacks transfer?
explaining transferability of evasion and poisoning attacks,” in 28th
{USENIX} Security Symposium, 2019.
[63] Y. Yao, H. Li, H. Zheng, and B. Y. Zhao, “Latent backdoor attacks
on deep neural networks,” in Proceedings of the 2019 ACM SIGSAC
Conference on Computer and Communications Security, 2019.
[64] S. Hong, N. Carlini, and A. Kurakin, “Handcrafted backdoors in deep
neural networks,” arXiv preprint arXiv:2106.04690, 2021.
[65] Z. Zhang, J. Jia, B. Wang, and N. Z. Gong, “Backdoor attacks to graph
neural networks,” arXiv preprint arXiv:2006.11165, 2020.
[66] Z. Xi, R. Pang, S. Ji, and T. Wang, “Graph backdoor,” arXiv preprint
arXiv:2006.11890, 2020.
[67] S. Rezaei and X. Liu, “A target-agnostic attack on deep models:
Exploiting security vulnerabilities of transfer learning,” arXiv preprint
arXiv:1904.04334, 2019.
[68] B. Wang, Y. Yao, B. Viswanath, H. Zheng, and B. Y. Zhao, “With great
training comes great vulnerability: Practical attacks against transfer
learning,” in 27th {USENIX} Security Symposium, 2018.
[69] C. Xie, K. Huang, P.-Y. Chen, and B. Li, “Dba: Distributed backdoor
attacks against federated learning,” in International Conference on
Learning Representations, 2019.
[70] H. Wang, K. Sreenivasan, S. Rajput, H. Vishwakarma, S. Agarwal,
J.-y. Sohn, K. Lee, and D. Papailiopoulos, “Attack of the tails: Yes,
you really can backdoor federated learning,” Advances in Neural
Information Processing Systems, vol. 33, 2020.
[71] V. Tolpegin, S. Truex, M. E. Gursoy, and L. Liu, “Data poisoning
attacks against federated learning systems,” in European Symposium
on Research in Computer Security. Springer, 2020, pp. 480–501.
[72] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, “How to
backdoor federated learning,” in International Conference on Artiﬁcial
Intelligence and Statistics. PMLR, 2020, pp. 2938–2948.
[73] M. Fang, X. Cao, J. Jia, and N. Gong, “Local model poisoning attacks
to byzantine-robust federated learning,” in 29th {USENIX} Security
Symposium ({USENIX} Security 20), 2020, pp. 1605–1622.
[74] P. Kiourti, K. Wardega, S. Jha, and W. Li, “Trojdrl: evaluation of back-
door attacks on deep reinforcement learning,” in 2020 57th ACM/IEEE
Design Automation Conference (DAC).
IEEE, 2020, pp. 1–6.
[75] L. Wang, Z. Javed, X. Wu, W. Guo, X. Xing, and D. Song, “Backdoorl:
Backdoor attack against competitive reinforcement learning,” arXiv
preprint arXiv:2105.00579, 2021.
[76] Y. Li, X. Lyu, N. Koren, L. Lyu, B. Li, and X. Ma, “Neural attention
distillation: Erasing backdoor triggers from deep neural networks,”
arXiv preprint arXiv:2101.05930, 2021.
[77] B. Wang, X. Cao, N. Z. Gong et al., “On certifying robustness
against backdoor attacks via randomized smoothing,” arXiv preprint
arXiv:2002.11750, 2020.
[78] Y. Cao, A. F. Yu, A. Aday, E. Stahl, J. Merwine, and J. Yang, “Efﬁcient
repair of polluted machine learning systems via causal unlearning,”
in Proceedings of the 2018 on Asia Conference on Computer and
Communications Security, 2018, pp. 735–747.
[79] M. Jagielski, A. Oprea, B. Biggio, C. Liu, C. Nita-Rotaru, and B. Li,
“Manipulating machine learning: Poisoning attacks and countermea-
sures for regression learning,” in 2018 IEEE Symposium on Security
and Privacy (SP).
IEEE, 2018, pp. 19–35.
[80] M. Mozaffari-Kermani, S. Sur-Kolay, A. Raghunathan, and N. K. Jha,
“Systematic poisoning attacks on and defenses for machine learning in
healthcare,” IEEE journal of biomedical and health informatics, 2014.
[81] A. Paudice, L. Mu˜noz-Gonz´alez, and E. C. Lupu, “Label sanitization
against label ﬂipping poisoning attacks,” in Joint European Conference
on Machine Learning and Knowledge Discovery in Databases, 2018.
[82] J. Li, S. Ji, T. Du, B. Li, and T. Wang, “Textbugger: Generat-
ing adversarial text against real-world applications,” arXiv preprint
arXiv:1812.05271, 2018.
[83] J. Gao, J. Lanchantin, M. L. Soffa, and Y. Qi, “Black-box generation of
adversarial text sequences to evade deep learning classiﬁers,” in 2018
IEEE Security and Privacy Workshops (SPW).
IEEE, 2018, pp. 50–56.
[84] S. Garg and G. Ramakrishnan, “Bae: Bert-based adversarial examples
for text classiﬁcation,” arXiv preprint arXiv:2004.01970, 2020.
[85] D. Jin, Z. Jin, J. T. Zhou, and P. Szolovits, “Is bert really robust?
a strong baseline for natural language attack on text classiﬁcation
and entailment,” in Proceedings of the AAAI conference on artiﬁcial
intelligence, vol. 34, no. 05, 2020, pp. 8018–8025.
[86] L. Li, R. Ma, Q. Guo, X. Xue, and X. Qiu, “Bert-attack: Adversarial
attack against bert using bert,” arXiv preprint arXiv:2004.09984, 2020.
[87] S. Ren, Y. Deng, K. He, and W. Che, “Generating natural language
adversarial examples through probability weighted word saliency,”
in Proceedings of the 57th annual meeting of the association for
computational linguistics, 2019, pp. 1085–1097.
[88] M. Alzantot, Y. Sharma, A. Elgohary, B.-J. Ho, M. Srivastava, and K.-
W. Chang, “Generating natural language adversarial examples,” arXiv
preprint arXiv:1804.07998, 2018.
[89] T. Wilson, J. Wiebe, and P. Hoffmann, “Recognizing contextual polarity
in phrase-level sentiment analysis,” in Proceedings of human language
technology conference and conference on empirical methods in natural
language processing, 2005, pp. 347–354.
[90] S. Zhao, X. Ma, X. Zheng, J. Bailey, J. Chen, and Y.-G. Jiang, “Clean-
label backdoor attacks on video recognition models,” in Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition, 2020, pp. 14 443–14 452.
[91] A. Turner, D. Tsipras, and A. Madry, “Clean-label backdoor attacks,”
2018.
[92] W. Yang, Y. Lin, P. Li, J. Zhou, and X. Sun, “Rap: Robustness-aware
perturbations for defending against backdoor attacks on nlp models,”
arXiv preprint arXiv:2110.07831, 2021.
[93] M. Fan, Z. Si, X. Xie, Y. Liu, and T. Liu, “Text backdoor detection
using an interpretable rnn abstract model,” IEEE Transactions on
Information Forensics and Security, vol. 16, pp. 4117–4132, 2021.
[94] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, “Distilbert, a distilled
version of bert: smaller, faster, cheaper and lighter,” arXiv preprint
arXiv:1910.01108, 2019.
[95] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts,
“Learning word vectors for sentiment analysis,” in Proceedings of the
49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies, 2011.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:09 UTC from IEEE Xplore.  Restrictions apply. 
152039
[96] Z. Sun, H. Yu, X. Song, R. Liu, Y. Yang, and D. Zhou, “Mobilebert: a
compact task-agnostic bert for resource-limited devices,” arXiv preprint
arXiv:2004.02984, 2020.
[97] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert
pretraining approach,” arXiv preprint arXiv:1907.11692, 2019.
[98] R. Weischedel and A. Brunstein, “Bbn pronoun coreference and entity
type corpus,” Linguistic Data Consortium, Philadelphia, 2005.
[99] E. F. Tjong Kim Sang and F. De Meulder, “Introduction to the conll-
2003 shared task: Language-independent named entity recognition,” in
Proceedings of the Seventh Conference on Natural Language Learning
at HLT-NAACL 2003, 2003.
[100] E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel,
“OntoNotes: The 90% solution,” in Proceedings of the Human Lan-
guage Technology Conference of the NAACL,, 2006.
[101] A. Salinca, “Business reviews classiﬁcation using sentiment analysis,”
in 2015 17th International Symposium on Symbolic and Numeric
Algorithms for Scientiﬁc Computing (SYNASC).
IEEE, 2015.
IX. APPENDIX
A. Challenge IV. Generative Model Is Incapable of Generating
Complex Triggers.
The aforementioned three challenges are pertaining to the
optimization based trigger inversion. A plausible idea is to
avoid trigger inversion and use a generative model to inject
trigger to input samples, like T-miner [38]. Such generators can
ensure the generated inputs comply with the language, avoid-
ing the infeasibility problem. However, they require knowing
the distribution of triggers beforehand and effectively learning
such distribution. In the NLP domain, triggers can be arbitrary
characters, words, phrases, sentences, and even paraphrasing
patterns. Learning a high quality generator is very challenging.
In addition, it is difﬁcult to generate a complex trigger that can
ﬂip multiple inputs.
Table VIII shows the triggers generated by T-miner for the
example model #231 with the random input sentences in the
ﬁrst column. Recall that T-miner does not require clean sam-
ples but rather random sequences. The second column shows
the generated sequences with the perturbations in red. Observe
that different sequences have different perturbations. The third
column shows the ASR for each perturbation word. They are
all low. According to our experiment, T-miner achieves lower
than 0.6 ROC-AUC for a set of TrojAI models from rounds
5 and 6. In addition, it requires ﬁne-tuning the generator for
each subject model. The average scanning time is 52 minutes
per model, much more costly than trigger inversion.
B. Parameters of PICCOLO
We also use the following parameters that were not men-
tioned in the main text due to the space limit. In the differen-
tiable word encoding, we use a 7k common word dictionary
from [89] and we will release it upon publication. For the word
discriminativity analysis, we use a Gaussian distribution with
a mean of 0 and a standard deviation of 2. During trigger
optimization, we run the optimization for 100 epochs with
a learning rate of 0.5. We apply the delayed normalization
every 30 epochs. The weight parameters in Equation (9) in
Section V-D are 1, 0.2, 0.1, and 0.5 for w1, w2, wlarge, and
w4, respectively. The ASR bound is 0.9 and the dot product
bound 170.
C. TrojAI Competition
TrojAI is a multi-year backdoor scanning competition orga-
nized by IARPA. This competition has 10 ofﬁcial performers
and is also open to the public. It has ﬁnished 7 rounds upon
the submission date. Rounds 1-4 are for computer vision tasks
and rounds 5-7 are for NLP tasks. In each round, hundreds to
thousands of pre-trained models (half clean and half trojaned)
with ground truth are provided as a training set. Test and hold-
out sets (of a similar or larger size) are hosted and evaluated on
IARPA servers. The IARPA team created the trojaned models
using common and recent attack methods in the literature.
The performers meet and discuss every week. They not only
develop their own solutions, but also try and adapt the latest
published defense techniques. For example, Both T-miner [38]
and Meta Neural Trojan Detection (MNTD) [22] have been
applied to rounds 5-7 by at least two performers, including
us. Other state-of-the-art scanners such as NC [10], ABS [11],
DeepInspect [12], K-Arm backdoor scanning [48], ULP [15]
are for computer vision and hence not applicable.
D. Backdoor Attacks and Defenses
Backdoor Attacks. BadNets [1] and TrojanNN [2] ﬁrst in-
troduced backdoor attacks on deep neural networks. BadNets
stamps trigger on part of the training samples and set their
labels to target
label. TrojanNN does not need access to
the training process or training data. It generates triggers
based on the model weights and reverse-engineers the training
data to inject the trigger. Many backdoor attacks have been
proposed afterwards. Based whether the attack needs data
access, there are white-box attack [1] and black-box attack [2].
In white-box attacks, there is another line of work where the
poisoned samples have the correct label called clean-label
backdoors [90], [91]. Based on the attack ability, there are
universal attack that ﬂips samples of all other classes to the
target class and label-speciﬁc attack that ﬂips samples from a
victim class to the target class. Based on the application, there
are backdoor attacks on computer vision [1]–[3], [58], NLP
tasks [4]–[6], [8], [9], [44], [45], graph neural networks [65],
[66], transfer learning [63], [67], [68], federated learning [69]–
[73], and reinforcement learning [74], [75]. In NLP backdoors,
there are three types of backdoors as discussed in Section II.
Backdoor Defenses. Based on which stage to defend, there
are four types of defense. The ﬁrst defends the data col-
lecting stage, removing poisoning samples from the training
set [78]–[81]. The second defends at pre-deployment stage
by scanning [10]–[22], [38]. PICCOLO belongs to the type.
There are several backdoor scanning methods in the computer
vision domain that does not require inputs such as NC [10],
ABS [11], DeepInspect [12], K-Arm [48], and ULP [15].
Most of them operate by solving a min-max problem with
some norm. They require the input domain to be continuous.
The third type aims at repairing trojaned models [25], [76].
The last type defends at post-deployment stage by detecting
trigger inputs [20], [23]–[35], [46], [47], [92], [93]. Based on
application, most backdoor defense methods fall into computer
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:09 UTC from IEEE Xplore.  Restrictions apply. 
162040
TABLE VIII: Triggers generated by T-miner for model #231 in TrojAI round 6 with the injected trigger “immensity”
Random sentences
T-miner perturbed sentences
Attack Success Rate
Skeleton escape dough cliffs fein
Bubba ##ango bieseged ##meister
Accolades ##oof backlash cot
Skeleton escape dough minority cliffs fein
Bubba ##ango concord bieseged ##meister
Accolades ##oof backlash robbers cot
0.1
0.05
0.3
vision [10]–[20], [20]–[35] or NLP [22], [38], [46], [47], [92],
[93].
E. Experiment Setup
TrojAI Rounds 5 and 6. Models in Rounds 5 and 6 are for
sentiment classiﬁcation. Round 5 contains 1648 models in
the training set and 504 in the test set. Round 6 consists
of 48 training models and 480 test models. Three types
of transformer based architectures, namely, DistilBERT [94],
GPT-2 [43] and BERT [42] are employed. They are trained
on eleven different datasets from Amazon review [51] and
IMDB [95]. For backdoored models in these two rounds, three
types of triggers are used, including global triggers, ﬁrst-half
triggers, and second-half triggers. Each type of triggers can be
characters, words, phrases, or sentences. In total, there are nine
kinds of triggers. Global triggers are effective when inserted
at any position of an input sentence. First-/second-half triggers
are only effective when inserted at the ﬁrst/second half of an
input sentence. In round 5, the phrase and sentence triggers
are semantically meaningful such as “I watched an 8D movie”,
or “Harry Potter and the Prisoner of Azkaban”. Some triggers
are quite long (e.g., more than 30 words) and complicated. The
phrase triggers in round 6 are composed of randomly sampled
words, e.g., “needfully innumerably mostly irregardless fast”.
Note that a backdoored model in round 5 can have multiple
injected triggers.
TrojAI Round 7. TrojAI round 7 focuses on the name entity
recognition (NER) task. It consists of 192 models in the train-
ing set and 384 in the test set. Four architectures are utilized,
DistilBERT, BERT, MobileBERT [96] and RoBERTa [97].
Models are trained on three datasets, including BNN cor-
pus [98] with 4 name entities, CoNLL-2003 [99] with 4 name
entities and OntoNotes [100] with 6 name entities. All the
triggers in round 7 are label-speciﬁc. That is, the trigger only
ﬂips the prediction of words from a victim label to a target
label. Two types of triggers are adopted in round 7, global
and local. Each type of triggers can be characters, words, or
phrases. In total, there are 6 kinds of triggers. Global triggers
are effective on all the words from the victim label in input
sentences. Local triggers are only effective when injected next
to the word from the victim label. Only the predictions of its
neighboring words are ﬂipped. At the time of our evaluation,
the ground truth is not available for round 7 test set. We hence
evaluate PICCOLO on the TrojAI test sever. For the training
set, we conduct the experiment locally.
Additional Models. We also evaluate PICCOLO on models
from a number of existing works. Speciﬁcally, we train 103
GRU models on Yelp dataset [55], [101] used in T-miner [38],
which focuses on classiﬁcation tasks. We leverage the ofﬁcial
repository from hidden killer attack [8] to train 120 BERT
classiﬁcation models and 120 LSTM models on SST-2 [40],
TABLE IX: Efﬁciency of PICCOLO
Model
Sentiment
Classiﬁcation
Models
NER Models
Arch.
DistilBERT
BERT