title:Masquerade Detection Using Enriched Command Lines
author:Roy A. Maxion
Masquerade Detection Using Enriched Command Lines
Roy A. Maxion
PI:EMAIL
Dependable Systems Laboratory
Computer Science Department
Carnegie Mellon University
Pittsburgh, Pennsylvania 15213 / USA
Abstract
A masquerade attack, in which one user impersonates
another,
is among the most serious forms of computer
abuse, largely because such attacks are often mounted by in-
siders, and can be very difﬁcult to detect. Automatic discov-
ery of masqueraders is sometimes undertaken by detecting
signiﬁcant departures from normal user behavior, as rep-
resented by user proﬁles based on users’ command histo-
ries. A series of experiments performed by Schonlau et al.
[12] achieved moderate success in masquerade detection
based on a data set comprised of truncated command lines,
i.e., single commands, stripped of any accompanying ﬂags,
arguments or elements of shell grammar such as pipes or
semi-colons. Using the same data, Maxion and Townsend
[8] improved on the Schonlau et al. results by 56%, raising
the detection rate from 39.4% to 61.5% at false-alarm rates
near 1%. The present paper extends this work by testing
the hypothesis that a limitation of these approaches is the
use of truncated command-line data, as opposed to com-
mand lines enriched with ﬂags, shell grammar, arguments
and information about aliases. Enriched command lines
were found to facilitate correct detection at the 82% level,
far exceeding previous results, with a corresponding 30%
reduction in the overall cost of errors, and only a small in-
crease in false alarms. Descriptions of pathological cases
illustrate strengths and limitations of both the data and the
detection algorithm.
1. Introduction
Colloquially, masquerading is the act of substituting one-
self for another. To masquerade is to disguise; to assume
the appearance of something one is not; to furnish with a
false appearance or an assumed identity; or to obscure the
existence or true state or character of something. The com-
puter masquerade problem is exempliﬁed in the following
scenario. A legitimate user takes a coffee break, leaving
his/her terminal open and logged in. During the user’s brief
absence, an interloper assumes control of the keyboard, and
enters commands, taking advantage of the legitimate user’s
privileges and access to programs and data. The interloper’s
commands may comprise read or write access to private
data, acquisition of system privileges, installation of ma-
licious software, etc. Because the interloper is impersonat-
ing a legitimate user (or some other computer identity, such
as a program), s/he is commonly known as a masquerader.
There are many ways for a masquerader to gain access to le-
gitimate user accounts, e.g., through a purloined password
or a hacker’s break in. The term may also be extended to en-
compass abuse of legitimate privileges – the case in which
a user “masquerades” as himself; such a person is some-
times termed an “insider,” especially when the person is an
accepted member of the organization sponsoring the target
system.
Masquerading can be a serious threat to the security
of computer systems and computational infrastructures. A
well-known instance of masquerader activity is the case of
Robert P. Hanssen, the FBI mole who allegedly used agency
computers to ferret out information later sold to his co-
conspirators [4]. Hanssen was a legitimate user, but his
behavior was improper.
In another example, in Novem-
ber of 2002, thirty thousand credit histories were reported
stolen in what a federal attorney called “the biggest case of
identity theft in U.S. history [2].” This was an insider un-
dertaking in which a help-desk employee sold misappropri-
ated passwords to someone who used them to masquerade
as a legitimate user downloading credit reports. Informa-
tion in the credit reports was used to withdraw funds from
bank accounts, make illegitimate charges to credit cards,
etc.
Insider masquerading, in various forms, is an enor-
mously costly problem reported by more than 60% of com-
panies surveyed in 1996 [13]. Of even greater concern than
economic losses, of course, are attacks on national security.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:03:06 UTC from IEEE Xplore.  Restrictions apply. 
Detecting masqueraders has long been a challenge, dat-
ing as far back as 1988 for practical detection systems [6].
The typical approach is based on the idea that masquerader
activity is unusual activity that will manifest as signiﬁcant
excursions from normal user proﬁles. User proﬁles are con-
structed from monitored system-log or accounting-log data.
Examples of the kinds of information derived from these
(and other) logs are: time of login, physical location of lo-
gin, duration of user session, cumulative CPU time, partic-
ular programs executed, names of ﬁles accessed, user com-
mands issued, and so forth [5]. When a deviation from nor-
mal behavior is observed, a masquerade (or other misuse)
attempt is suspected.
2 Background and related work
There have been several attempts to tackle the problem
of detecting masqueraders, one of the earliest of which was
the IDES system [6]. About twelve years later, Schon-
lau and his colleagues [12] presented a nice collection of
masquerader research, in which a number of masquerade-
detection techniques were applied to the same data set. In
terms of detecting masqueraders, the best detection result
reported in the Schonlau et al. work was for a Bayes One-
Step Markov model, which achieved a hit rate of 69.3%
with a corresponding false-alarm rate of 6.7%. In terms of
minimizing false alarms (targeted at 1%), their best result
was obtained by using a uniqueness metric that achieved
a 39.4% hit rate with a corresponding false-alarm rate of
1.4%. Although these results may seem disappointing, they
are in fact quite good, considering the extreme difﬁculty of
the problem.
Taking the work of Schonlau et al. as a point of depar-
ture, Maxion and Townsend [8] used the Schonlau data to
demonstrate a new approach to masquerade detection. Their
technique was based on naive Bayes classiﬁcation, which
has enjoyed considerable success in the ﬁeld of text classi-
ﬁcation [10]. Compared to the Schonlau et al. results, they
achieved a 56% improvement in correct detection (61.5%)
at a false-alarm rate (1.3%) that is the lowest reported in the
literature so far. They also ampliﬁed their results with an
analysis of the errors made by the detector as applied to an
alternative conﬁguration of the Schonlau et al. data set. The
error analysis exposed certain limitations of the Schonlau et
al. framework, and suggested various things that might im-
prove future results, including the use of better command-
line data.
Both Schonlau et al.
[12] and Maxion and Townsend
[8] studied masquerade detection using truncated command
lines, with each command line comprising just a single
command, stripped of any accompanying ﬂags, arguments
or elements of shell grammar (such as pipes or semi-
colons). In all of these experiments, 5000 lines of training
data were available for each user, plus 10,000 lines of test-
ing data. The unit of classiﬁcation was a block of 100 con-
tiguous command lines, which means that in a practical set-
ting the detector would need to wait for 100 user commands
before deciding whether or not those commands were typed
by the legitimate user or by a masquerader. While this was
only an experimental scenario, the delay involved in waiting
one hundred command lines before classiﬁcation is clearly
undesirable. However, Maxion and Townsend [9] report in
further work that reduction of the unit of classiﬁcation to a
block of only 10 commands leads to a 20-point reduction in
the hit rate (to just 47.1%) and a corresponding false alarm
rate similar to that obtained with block size 100 (1.6%).
This result is based on using a factor of 10 fewer commands,
and yet it retains nearly the same false-alarm rate as earlier
work using blocks of 100 commands.
Correct detection near 99% with a corresponding false-
alarm rate under 1% would be considered worthy outcomes.
Given the comparatively poor results obtained with the vari-
ous algorithms tried by Schonlau et al., even using the larger
block size, as well as the substantially improved, yet not
spectacular results achieved by Maxion and Townsend, it
seems reasonable to conclude that acceptable masquerade-
detection capability is unlikely to be achieved with the
information-impoverished data used, i.e., command lines
stripped of any information beyond the basic command.
The obvious questions, then, are these: Would data contain-
ing more information about a user’s command-line behav-
ior improve proﬁling and thus detection? Compared to the
“truncated” data used so far, would data “enriched” with
command-line ﬂags and arguments facilitate better mas-
querade detection? It is from these questions that the current
work arises.
3 Objective and approach
The objective of the present work is to determine the ex-
tent to which information-enriched command-line data can
improve masquerade detection over truncated command-
line data. Examples of truncated and enriched versions of
the same data are shown in Table 1.
Truncated
cd
more
diff
lpr
setenv
rwho
set
nroff
ls
enscript
Enriched
cd cpsc504
more susan.lst
diff susan.lst julie.lst
lpr -Pjp susan.lst
setenv TERM amb amb
rwho -a
set prompt set prompt = “VAXC(cid:0)!(cid:1) [$cwd:t]    (cid:0)”
nroff -me proposal (cid:3) more
ls -F -l (cid:4)candym/.em* l -l (cid:4)candym/.em*
enscript -2Gr -L66 -Palw -h *.c print66*.c
Table 1: Examples of truncated (left) and enriched (right)
command-line data.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:03:06 UTC from IEEE Xplore.  Restrictions apply. 
Reaching this determination in a way that effects fair
comparisons with previous work requires acquisition of new
data, because the data used in earlier studies were already
truncated, so they cannot be fairly compared with enriched
data sets from alternative sources. Once the new, enriched
data (replete with full command-line information) are ob-
tained, a truncated data set will be constructed from the en-
riched data. Then both data sets will be divided into training
and testing data, masquerade “intrusions” will be injected
into the testing data, and a naive Bayes classiﬁer will be
used to detect the intrusions. Results are expected to show
that enriched data, containing more information, better sup-
port masquerade detection than truncated data do.
The following sections describe the data, the experimen-
tal methodology and the results.
4 Data
The data used by Schonlau et al. did not allow for any
enrichment - no information about ﬂags, aliases, arguments
or shell grammar was provided. This necessitated procuring
data from elsewhere. The data were obtained from an ear-
lier study whose original purposes included describing how
people use commands in Unix, and reporting on the statis-
tics of the complete command as entered by the user, as op-
posed to just the command itself. These data, collected by
Saul Greenberg, comprised full command-line entries from
168 unpaid, volunteer users of the Unix csh system, and
are documented in [3]. All data sets were rendered anony-
mous by replacing user-conﬁdential information (e.g., user
names) with dummy information that retained the seman-
tics of the original data. The original data are split into four
groups comprising 55 novice users, 36 experienced users,
52 computer-scientist users, and 25 non-programmer users,
all of whom were afﬁliated with the University of Calgary
(Canada) as students, faculty, researchers or staff. Green-
berg deﬁned these categories carefully, but they will not be
repeated here due to space limitations and the lack of a com-
pelling need to do so.
The Greenberg data provide much more complete infor-
mation about the user’s command line behavior than do the
Schonlau et al. data. Greenberg supplies the command line
as typed, including ﬂags, grammar and arguments, along
with a note of whether the typed command was an alias for
anything, and if so, what. In addition, a record of history
use and errors is kept, and each set of commands executed
in the same terminal window is stamped with the time at
which that window was opened and closed.
An example of an original Greenberg data entry appears
in Table 2. S-lines give the day, date and time of opening of
the Xterm in which the command line was issued. E-lines
give the day, date and time of closing (or ending) of the
Xterm in which the command line was issued, or NIL if not
available. C-lines give the command line as typed by the
S Fri Feb 20 23:39:46 1987
E NIL
C purge
D /user/cpsc500/l01b91/xxxxxx/c500
A rm -i -i .ed_[0-9]* .emacs_[0-9]* .*.CKP
.*.BAK *.CKP *.BAK ; clear ; /bin/ls -al | more
H NIL
X NIL
Table 2: Example of Greenberg raw data.
user. D-lines give the path of the working directory. A-lines
give the command executed by the shell if different from
the one that the user typed, i.e., the aliased command line.
H-lines indicate whether history was used. X-lines indicate
whether an error was made, and if so, of what sort.
Because the current experiments focus on proﬁling user
behavior through command-line activity, the Xterm time,
history and error categories are ignored for the purpose of
constructing the enriched command-line data employed in
this work. After removal of directory and ﬁlename-type ar-
guments, and ignoring the current directory, history, and er-
ror categories, the Greenberg entry would be transformed as
follows:
C purge
A rm -i -i ; clear ; /bin/ls -al | more
Truncated command lines consist of only the ﬁrst item
typed, with no information about aliases, ﬂags or argu-
ments; hence the truncated command line version of the
data for this entry would become:
purge
An enriched command line is a concatenation of the
whole command line typed, including ﬂags, arguments and
items of shell grammar (such as & or (cid:0)(cid:0)) together with
the expansion of any alias employed; hence the enriched
command-line version of this entry would be:
purge rm -i -i
; clear ; /bin/ls -al | more
Side-by-side examples of the two types of data are shown
in Table 1 of Section 3. Note that the enriched data must be
stripped of directory and ﬁlename arguments, because nor-
mal user data is used as a proxy for masquerader data, and it
is clearly unreasonable to inject John’s data with data from
Jack, whilst leaving in ﬁle and directory names speciﬁc to
Jack. Thus, the ﬁnal version of the two types of data would
be as shown in Table 3.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:03:06 UTC from IEEE Xplore.  Restrictions apply. 
Truncated
cd
more
diff
lpr
setenv
rwho
set
nroff
ls
enscript
Enriched
cd
more
diff
lpr -Pjp
setenv
rwho -a
set
nroff -me (cid:0) more
ls -F -l -l
enscript -2Gr -L66 -Palw -h
Table 3: Examples of truncated (left) and enriched (right)
command-line data, after removal of ﬁle/directory names.
5 Experimental method
Two experiments were conducted – one with truncated
command lines and one with enriched command lines.
Readers familiar with previous work will note differences
in experimental methodologies between earlier studies and
this one. These differences are due primarily to constraints
imposed by the data available. For example, the amount
of data used in training and testing sets differed here, com-
pared to those used in earlier work, primarily due to the
contents of the original data sets.
5.1 Selection of subjects
The Greenberg data contains 168 users, not all of which
were well suited for the present masquerade study. A subset
of users – ﬁfty victims and 25 masqueraders – was selected
in consideration of various issues, detailed below.
In earlier studies using the Schonlau et al. data, each of
the 50 users had 15,000 commands; these were separated
into a set of training data (the ﬁrst 5000 of the 15,000 com-
mands) and testing data (the remaining 10,000 commands).
It would be ideal to match the numbers of commands in
these earlier data sets, but unfortunately this could not be
done, because most of the Greenberg users did not have that
many commands.
Figure 1 shows the decision diagram for selecting exper-
imental users from the pool of 168. Of the 168 users in the
Greenberg data, 112 produced fewer than 2000 commands;
the remaining 56 users produced between 2024 and 12,056
commands, depending on the user. Of these 56 users, 6
had generated markedly more command lines than the oth-
ers, having in excess of 5000 command lines each. These
6, due to their being in essence, outliers, were removed
from consideration, leaving 50 subjects, a group coinciden-
tally the same size as was used in previous work. These
50 users were distributed across the user categories as fol-
lows: 13 novices, 15 experienced, 21 scientists and 1 non-
programmer. A further 25 users were selected at random
from the remaining pool of 118 users to serve as a source of
masquerader commands.
168
Users
Fewer than
2000 commands
More than
2000 commands
112
Users
Discarded
56
Users
Fewer than
5000 commands
More than
5000 commands
50
Users
Victim Pool
6
Users
Discarded
Figure 1: Decision diagram for selecting the 50 subjects
comprising the victim pool.
5.2 Training and testing data
The data for the subject users was truncated to 2000 com-
mand lines. The ﬁrst 1000 lines for each user were kept
aside as training data upon which to base a proﬁle of self.
The second 1000 command lines were kept as unlabelled
self data with which to test self-recognition capacity.
The last 100 command lines from each of the 25 mas-
querader users were concatenated to give a stream of 2500
command lines which constituted a pool from which to
draw “masquerade” data. These 2500 command lines were
processed into 250 nonoverlapping testing units of 10 com-
mand lines each. Thirty of these units were then selected at
random and injected at randomly selected positions, without
replacement, into the stream of 1000 self command lines,
resulting in 130 blocks of 10 command lines for testing
self and non-self recognition capacity. Note that although
the masquerade blocks and injection positions were ran-
domly selected, they were held constant over the 50 users,
i.e., the test data for each user contains the same masquer-
aders in the same positions. For example, each victim might
have been injected at position 2 with the same masquerader
block, thus ensuring consistency throughout the injected
data.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:03:06 UTC from IEEE Xplore.  Restrictions apply. 
5.3 Detection/classiﬁcation algorithm
The naive Bayes algorithm with simple updating (pass-
ing each approved block of self back to the self model for
X) was employed. Naive Bayes classiﬁers are simple prob-
abilistic classiﬁers known for inherent robustness to noise
and fast learning time (learning time is linear in the number
of training examples). These classiﬁers have a history of
successful use in text classiﬁcation, where the task is to as-
sign a document to a particular class, typically using the so-
called “bag of words” approach, which proﬁles document
classes based simply on word frequencies [10]. Deciding
whether a newspaper article is about sports, health or poli-
tics, based on the counts of words in the article, is similar
to the task of deciding whether or not a stream of command
lines issued at a computer terminal belongs to a particular
authorized user or not.
In the present context, the classiﬁer works as follows.
The model assumes that the user generates a sequence of
commands lines, one line at a time, each with a ﬁxed proba-
bility that is independent of the command lines preceding it
(this independence assumption is the “naive” part of naive
Bayes). The probability for each command line (cid:0) for a given
user 	 is based on the frequency with which that command
line was seen in the training data, and is given by:

(cid:0)(cid:1)	 (cid:0)
Training Count(cid:0)(cid:1)	  (cid:1)
Training Data Length  (cid:1)  (cid:2)
where (cid:2) is a pseudocount and (cid:3) is the number of distinct
command lines (i.e., the alphabet) in the data. The pseudo-
count can be any real number larger than zero (0.01 in this
study), and is added to ensure that there are no zero counts;
the lower the pseudocount, the more sensitive the detector is
to previously unseen commands. The pseudocount term in
the denominator compensates for the addition of a pseudo-
count in the numerator. The probability that a test sequence
of the ﬁve command lines “a a b b b” was generated by a
particular user, say User 1, denoted as 	(cid:0), is:
	(cid:0)(cid:1)(cid:2)  	(cid:0)(cid:1)(cid:2)  	(cid:0)(cid:1)(cid:3)  	(cid:0)(cid:1)(cid:3)  	(cid:0)(cid:1)(cid:3)
or 	(cid:0)(cid:1)(cid:2)(cid:1)
 	(cid:0)(cid:1)(cid:3)(cid:2) where 	(cid:0)(cid:1)(cid:2) is the probability that
User1 typed the command line (cid:5). For each User X, a model
of Not X can also be built using training data from all other
victims. The probability of the test sequence having been
generated by Not X can then be assessed in the same way
as the probability of its having been generated by User X.
The larger the ratio of the probability of originating with X
to the probability of originating with Not X, the greater the
evidence in favor of assigning the test sequence to X. The
exact cut-off for classiﬁcation as X, that is the ratio of prob-
abilities below which the likelihood that the sequence was
generated by X is deemed too low, can be determined by a
cross-validation experiment during which probability ratios
for sequences which are known to have been generated by
self are calculated, and the range of values these legitimate
sequences cover is examined.
The success of naive Bayes has often struck researchers
as surprising, given the unrealistic assumption of attribute
independence which underlies the naive Bayes approach.
However, [1] demonstrates that naive Bayes can be optimal
even when this assumption is violated. Further general de-
tails regarding naive Bayes can be found in [7] and [11]. For
a more detailed description of the algorithm in the context
of masquerade detection, the reader is referred to [8].
5.3.1 Training procedure
The goal of the training procedure is to establish proﬁles of
self and nonself, and to determine a decision threshold for
discriminating between examples of self and nonself.
For each user, the detector builds a proﬁle of self based
on the command line frequencies from the 1000 command
lines for that user; it builds a separate proﬁle of nonself
based on the training data for the other 49 users, i.e., 49
x 1000 command lines.
Five-fold cross-validation, with sets of 200 and 800 com-
mand lines, was performed in order to determine the appro-
priate threshold for the detector. In each round, the larger
set was used to build a proﬁle of self and the smaller set was
used to determine the range of values the detector would
assign to material generated by the authorized user. The
test data for each round consisted of 20 blocks of ten com-
mand lines; thus a total of 100 scores (ﬁve rounds of val-
idation times 20 blocks per round) for self were generated
during cross-validation. For each user the individual thresh-
old was taken to be the value associated with the block of
data deemed least likely to have come from that user. Prior
experience indicates that better performance with respect to
the false alarm problem is achieved with a single generic
threshold for all users, as opposed to an individual thresh-
old customized to each user. This generic threshold was
taken to be the mean of the 50 individual thresholds.
5.3.2 Testing procedure
The detector was presented with 130 blocks of mixed, un-
labelled self and non-self data; this is approximately 3/4
self (100 blocks) to 1/4 nonself (30 blocks), as described
earlier. For purposes of updating, any block deemed to be
consistent with the proﬁle of self was passed back to the de-
tector to update the model of self for the user in question.
Thus, updating of the proﬁle of self (but not of the thresh-
old, which was not updated at all) occurred after each block
accepted as self.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:03:06 UTC from IEEE Xplore.  Restrictions apply. 
6 Results and analysis
The experimental results show clearly that including in-
formation about aliases, ﬂags and items of shell grammar
brings about a dramatic improvement in masquerade detec-
tion. In the present study there was an improvement from
70.9% hits to 82.1% hits using truncated and enriched data,
respectively; this is an increase of 15.8%. The false alarm
rates of the two data types are comparable, at 4.7 and 5.7%
respectively, an increase of 21.3%. These ﬁgures can also
be interpreted in terms of the cost of errors, namely misses
and false alarms. If the costs of these errors are equal, then
the cost function (weighted combination of misses and false
alarms) goes from 33.8 to 23.6, an improvement of 30.2%.
In previous work, the costs of errors were not equal; misses
cost six times as much as false alarms [8]. Using this cost
measure, the improvement was 9.1%, from 57.3 to 52.1. Ta-
ble 4 summarizes the results.
Data
Type of data
Amount of training data
Block size
Hits %
Misses %
False Alarms %
Cost (equal weights)
Cost (FA=6*Miss)
Greenberg Greenberg
Truncated
1000
10
70.9
29.1
4.7
33.8
57.3
Enriched
1000
10
82.1
17.9
5.7
23.6
52.1
Table 4: Results for naive Bayes on Greenberg data, aver-
aged across all users.
The average rates portrayed by the table do not tell the
whole story. It is important to note that not only is the av-
erage hit rate higher for enriched command lines, but that
this increase in the average reﬂects an improvement across
the board, rather than just for one or two outliers. With en-
riched command lines, there is only one user (User 36) for
whom the hit rate is less than 66%, compared with 20 such
users when truncated command lines are employed.
6.1 ROC curve
In assessing the results of a masquerade detector, one
is concerned with the trade-off between correct detections
(hits, or true positives) and false detections (false alarms,
or false positives). These are often depicted on a receiver
operating characteristic curve (called an ROC curve) where
the percentages of hits and false alarms are shown on the
y-axis and the x-axis, respectively.1 ROC curves for the
naive Bayes classiﬁer (with updating) on the truncated and
the enriched versions of the Greenberg data (segmented into
1For a thorough exposition of ROC curves, see [14].
s
t
i
H
%
100
90
80
70
60
50
40
30
20
10
0
Enriched command lines: area is 93.03%
Truncated command lines: area is 92.70%
10 20 30 40 50 60 70 80 90 100
% False Alarms
Figure 2: Receiver operating characteristic (ROC) curve for
the naive Bayes classiﬁer (with updating) as applied to trun-
cated and enriched Greenberg data.
sequences of length 10) are presented in Figure 2. The ROC
curves were obtained by stepping the value of the threshold
through a range bounded at one end by the threshold for
which 100% hits were obtained, and on the other end by the
threshold for which no false alarms were observed, in in-
crements of 0.05. The dotted curve applies to the truncated
data; the bold curve applies to the enriched data. Lenient
decision criteria allow a higher hit rate, but also a higher
false-alarm rate; more stringent criteria tend to reduce both
rates. Each point on the curves indicates a particular trade-
off between hits and false alarms. Points nearer to the upper
left corner of the graph are the most desirable, as they indi-
cate higher hit rates and lower false-alarm rates.
Although the difference in area under the two curves is
very small (93.03% for enriched data, and 92.70% for trun-
cated data), it is still highly signiﬁcant for performance. For
example, it is possible to obtain almost 60% hits at a false
alarm rate of around 1% with the enriched data, whilst less
than 50% hits can be obtained at that level of false alarms
with the truncated data. At the high hit-rate end of the con-
tinuum, enriched data allows 85% hits at 7% false alarms,
whereas a similar hit rate on truncated data results in nearly
doubling the false-alarm rate to almost 13% false alarms.
The curve for the enriched data hits 100% detection at a
false alarm rate of 22%, but the curve for the truncated data
hits 100% only at a false alarm rate of 58%, more than dou-
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:03:06 UTC from IEEE Xplore.  Restrictions apply. 
ble the false alarms. If the costs of misses and false alarms
are the same, then using the truncated data comes at an enor-
mous expense. The enriched data enable a higher hit rate at
a much lower false alarm rate.
5000
t
s
o
C
4000
3000
2000
1000
0
1
2