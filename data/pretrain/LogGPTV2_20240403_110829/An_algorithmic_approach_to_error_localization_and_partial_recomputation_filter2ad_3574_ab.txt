traditional approaches commonly rely
on checksum-based techniques for detection, but still rely
on high overhead checkpoint-rollback for correction. In the
above example, for example, if an error is detected
(e.g.
cT y(cid:48) − (cT A)x = 8 signals an error), it will trigger a restart
since the last checkpoint, which may result
in high total
overhead of recovery over the entire execution. Even recent
work
[27] on reducing the overhead of checksum-based
techniques for detecting faults of sparse linear algebra, still
relies on checkpoint-rollback for the correction of errors. This
paper proposes an entirely different approach for correction -
correction based on targeted algorithmic recomputation which,
in turn, is enabled by error localization through a checksum-
based technique. I.e., checksums are used to localize errors
rather than just detect errors which allows for low overhead
partial recomputation to be employed instead of high overhead
full restart. To the best of our knowledge, this is the ﬁrst work
on algorithmic error localization and partial recomputation.
The ability to localize faults has been studied in the context
of parallel program where researchers attempted to locate
the cause of anomalies of parallel applications [22, 4]. Simi-
larly, researchers have proposed approaches for identifying the
physical location of detected hardware bugs [24]. This paper
proposes the use of error localization during the runtime of
the application to better guide partial recomputation.
Finally, there has been some work on algorithmic tech-
niques to avoid conventional checkpoint-restart mechanisms
in applications by transforming them into a form which is
naturally more tolerant to errors. In [26], arbitrary applications
are transformed into numeric optimizations problems due to
the fact that the solvers for these problems are inherently
robust to errors. A numeric optimization methodology provides
a general path for making applications robust. The performance
overhead incurred from this transformation varies across dif-
ferent applications. In general, if the complexity per iteration
of the transformed application is less than the complexity of
the original application the overhead is also low.
III. PARTIAL RECOMPUTATION
The traditional approach of rolling back and repeating
entire portions of applications upon the detection of faults
can be prohibitively expensive under high error rates. Instead,
we propose the use ﬁne-grained partial recomputation to
enable efﬁcient forward progress. To identify segments of an
application for ﬁne-grained partial recomputation we will need
to efﬁciently ﬁnd the location of errors. These operations are
possible in many algorithms and in this paper we demonstrate
how it can be applied in the context of linear operations,
focusing on matrix vector products (MVMs). MVMs often
dominate computation in many HPC and RMS applications
(see Section IV-E).
A. Error Localization for Linear Operations
Suppose that the MVM operation Ax (A is a matrix, x
is a vector) outputs the vector ˆy which may be equal to the
correct result y or may contain errors. Errors in ˆy can be
detected by simply multiplying both Ax and result ˆy by a
check vector c that contains all 1s. The difference between
(cT A)x and cT ˆy (identical to cT (Ax)) is close to zero if there
is no error (accounting for round-off error) and notably larger
than zero if there is an error. Further, because the quantity
(cT A) can be pre-computed and reused for all multiplications
of A by a vector, this check is efﬁcient, employing two dense
dot-products (good memory locality) to check a sparse MVM
(poor locality).
This basic algorithm can be extended to also identify the
fault’s location. Suppose that we replace all the 1’s in the
bottom half of c with 0’s and repeat the above check. If there
exist errors in the top half of ˆy, the difference (cT A)x)− cT ˆy
will be larger than zero but errors in the bottom half of ˆy will
have no effect on the result. This is true for any such variant
of c. Let ci,j = { vector with 1’s between indices i and j }.
We can check if any entry i of ˆy is erroneous by performing
the above check using ci,i instead of c. Because ˆy is large it
is signiﬁcantly more efﬁcient to detect the location of each
error hierarchically, checking for errors in each half of ˆy,
then “zooming in” on the half of each region found to be
erroneous. This procedure uses the tree of check vectors shown
in Figure 2 and operates by starting at the top of the tree and
proceeding downward. At each step the algorithm computes
i,jA)x − cT
i,j ˆy, where ci,j is the vector at the current tree
(cT
node. If a difference is detected, the algorithm recurses to
each of the node’s children. Any leaf nodes reached by this
algorithm correspond to precisely detected errors in ˆy.
In the worst case scenario (i.e. every element of the result ˆy
is erroneous), the algorithm would perform a computation for
every node, requiring 2n− 1 dot-products, where n is the size
of ˆy. Fortunately, even under higher error rate scenarios, only a
small fraction of output entries are likely to be corrupted. As
such, only a fraction of the tree (O(log2(N )) will typically
need to be traversed for any check. For this reason, our
proposed error localization algorithm allows for much lower
average overhead as opposed to prior approaches which use
a ﬁxed set of pre-designed codes for exact detection and
correction properties.
Once the locations of errors in ˆy are identiﬁed, they can
be corrected via targeted correction. The MVM operation has
the property that the element at index i in result vector y of
Ax is the dot-product of row i of A by the vector x. This
property makes it possible to correct the erroneous entries
by simply recomputing them from x and the corresponding
rows of A. Further, it can be observed that while the cost
of identifying that an error lies within a given index of ˆy
requires two dot products of matrix row by a vector, while
recomputing a given the same element of ˆy requires just one
dot product. This suggests that it may be more efﬁcient to stop
the fault localization procedure early and recompute a larger
region of ˆy. In general, if our localization algorithm stops k
levels short from the bottom then 2k entries of ˆy need to be
recomputed. Section V experimentally explores this tradeoff.
Another observation is that, in many scenarios small errors
have little effect on the correctness of the algorithm that uses
MVM. For instance, many iterative algorithms converge from
a poor estimate of their result to an accurate estimate. As
such, small errors in intermediate estimates will have little
effect on convergence and it is thus more cost-effective to
allow such errors than correct them. Our algorithm can be
adjusted to meet the resilience needs of applications by using
a threshold τ where the localization and correction procedure
is only employed if the difference between (cT A)x) and cT ˆy
is larger than τ.
Fig. 2: The complete set of codes with (cid:107)c(cid:107)2 = 1 (i.e. the basis
of subspace) corrects all faults exactly by means of simply
computing all the syndromes for this set.
1) Example: Let’s consider the same input matrix A, input
x, and error e from Section II. In order to construct a binary
tree which can be utilized in the process of error localization,
we need to construct a set of c vectors. As described in Section
III-A, these vectors take the form:
ci,j = { vector with 1’s between indices i and j }
ci,i = { vector with exactly one 1 at index i }
For the prior 5x5 example, these c vectors are:
 , c0,2 =
 , c0,0 =
 , c3,3 =
1
1
1
1
1
1
1
0
0
0
0
0
1
0
0
1
1
1
0
0
1
0
0
0
0
0
0
0
1
0
 , c3,4 =
 , c1,1 =
 , c4,4 =
 ,
 ,
0
0
0
1
1
0
1
0
0
0
0
0
0
0
1
c0,4 =
c0,1 =
c2,2 =
Each of these codes are used to evaluate the check invariant
(cT y(cid:48) = cT Ax) at a node in tree the binary tree. Part of the
second checksum (cT Ax) may be precomputed or cached dur-
ing the execution. The precomputed products (cT
i A = AT ci)
associated with the codes in this example are:
4
c=c=c=1 1 . . . .1 1. .0 0. .0 0. .1 1. .11110 0. .0 0. .0000..
AT c0,4 =
AT c0,1 =
AT c2,2 =
9
5
7
8
19
 , AT c0,2 =
 , AT c0,0 =
 , AT c3,3 =
5
4
4
6
15
3
0
2
3
4
1
0
3
2
2
5
1
2
5
9
0
3
2
1
6
 , AT c3,4 =
 , AT c1,1 =
 , AT c4,4 =
 ,
 ,
4
1
3
2
4
2
1
0
2
5
2
1
0
0
2
Performing error localization involves starting at the top
of the tree (i.e. with the vector c0,4) and evaluating the check
invariant in order to detect if any errors occurred during the
computation. By computing the difference in the checksums
(i.e. the syndrome) at the top node of tree reveals that at least
one error exists in the output:
0,3y(cid:48) − (cT
cT
(Error(s) in the output (segment [0, 3])
0,3A)x = 8
Therefore, the algorithm proceeds to the next level of the
tree which now considers the same check invariant, but with
codes c0,2 and c3,4, which represent the ﬁrst and second halves
of the output, we can further narrow down that the error(s)
must be located within the ﬁrst half of the output.
0,2y(cid:48) − (cT
cT
3,4y(cid:48) − (cT
cT
0,2A)x = 8
3,4A)x = 0
(Error(s) in segment [0, 2])
(OK, No errors in segment[3, 4])
With the error(s) localized to the ﬁrst half of the output, we
can ignore part of the binary tree corresponding to the second
and proceed to isolate the errors within the ﬁrst half. The codes
c0,1andc2,2 locate the errors(s) within the ﬁrst two elements
of the output.
0,1y(cid:48) − (cT
cT
2,2y(cid:48) − (cT
cT
0,1A)x = 8
2,2A)x = 0
(Error(s) in segment c0,1)
(OK, No errors in y’[2])
Finally, unary codes c0,0 and c1,1 each locate and identify
the speciﬁc magnitudes of both of the actual faults since they
contain a c vector containing exactly one 1 (i.e. (cid:107)c4(cid:107) = (cid:107)c5(cid:107) =
1). Note that at any point in the traversal of the tree, the error
localization process could be stopped and the segment of the