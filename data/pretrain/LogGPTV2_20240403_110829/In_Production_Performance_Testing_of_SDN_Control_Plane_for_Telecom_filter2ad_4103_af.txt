




	

	

	

	



(a)  
























	
	







  

  




	

	

	

	




(b)
























	
	





  



  




	

	

	

	




























	
	





  



  




	

	

	





(c) 
(d)
Fig. 11. Throughput scalability measured for a large topology (# switches = 30, #hosts = 300).
receiving a new Ô¨Çow rule event. In response, each node checks
whether it is the master of the switch where the Ô¨Çow rules
should be installed, and takes action accordingly.
Discussion. A solution to the increased overhead for large
topology consists in logically partitioning the SDN network,
and use a federation or hierarchical controllers, in a way to
keep the number of switches and hosts per controller instance
below a threshold. The threshold indeed depends on the rate
of the received requests. In addition, in case of wireless/mobile
networks, the topology might only be partially static. The in-
production performance testing SCP-CLUB can provide valu-
able insights on how to federate/partition SDN controllers
dynamically, for instance, given a set of application-level KPIs.
D. Testing over Real SDN Switches
The last set of experiments consist in test driving SCP-CLUB
over a real SDN network. In particular, the conÔ¨Ågured telco
cloud is equipped with 30 V350 Centec OpenFlow switches.
Connections across the switches are conÔ¨Ågured by the Topology
Manager according to the production network under study.
The Ô¨Årst set of tests was conducted with a moderate load of
2,000 requests/s that resulted in a general outage of the network.
We further tested the real deployment with a reduced request
rate from 100 requests/sec starting from 100 to 1,400 requests/s.
Figure 12.(a) shows that the performance is relatively stable
up to 400 requests/s regardless of the number of controllers.
When the load reaches 600 requests/s, a performance degra-
dation appears on all
tested deployment, and adding more
controllers entails a further performance loss. Figure 12.(b)
shows that the average time to process an intent is almost
doubled when switching from 200 to 400 requests/s, requiring
the system more than a 1,000 ms to process each request.
We observed that the higher the load, the more the Ô¨Çow rule
operations were discarded by the Centec switches due to a
hardware limitation. The Centec Ethernet interface driver queue
implemented as a ring buffer was Ô¨Ålled up by the high volume
of OpenFlow rules submitted by the ONOS instances, resulting
in a high throughput degradation. Thus, adding more controllers
caused the queue to be Ô¨Ålled up faster.
E. Discussion
The example results provided in this section highlight the
need of dynamically establishing the scale out point through
in-production performance testing, to Ô¨Ånd a trade-off between
the beneÔ¨Åt of a larger cluster and the additional overhead of the
ONOS consistency mechanism. In the experience accumulated
while creating the telco cloud setup and the mock-up SDN
network, we found that the biggest limitation to obtain optimal
/
]
s
q
e
r
K
[
t
u
p
h
g
u
o
r
h
T
0.8
0.6
0.4
0.2
0
0
104
103
102
101
]
s
m
[
y
c
n
e
t
a
L
e
l
i
t
n
e
c
r
e
P
h
t
0
5
Controllers
1
3
5
7
0.4
0.2
Input Request Rate [Kreq/s]
0.6
0.8
1.2
1
1.4
(a)
VSCALE
1
5
3
7
0.1
0.2
0.4
0.6
0.8
1
1.2
1.4
Input Request Rate [Kreq/s]
(b)
Fig. 12. Throughput (a) and latency (b) for the real SDN data plane.
performance is indeed the way we test the control plane. Many
approaches exist for in-production testing of cloud systems (e.g
[22], [23]). However, SDN networks are still nascent where nei-
ther classical network approaches or pure software-engineering
based approaches applies as they are. While building SCP-
CLUB we found out that there is no one size Ô¨Åts all approach
to the performance testing and benchmarking of the SDN
control plane because of the many moving parts (i.e., cloud
infrastructure, SDN controller, NFV, topology, network devices,
and network applications) each of which exposes a number of
knobs that must be Ô¨Åne-tuned at run-time to run the network
hotter. For this reason, we argue that the SCP-CLUB approach
to in-production performance testing and benchmarking is a
viable approach towards creating a truly programmable network
fabric, in which application KPIs can dictate how the network
should be setup without long and expensive (manual) off-line
tests, for instance, allowing operators to provide customized
networks faster.
VIII. RELATED WORK
SDNs have been envisioned in many scenarios, including
Radio Area Networks (RAN) [24], datacenters [18], [25] and
652
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:48:08 UTC from IEEE Xplore.  Restrictions apply. 
wireline access networks [26]. Telco Operators are very at-
tracted by the SDN promise to simplify networks and to run
networks at higher utilization. However, no tools or approaches
are available to design, test and manage telco-cloud based SDN.
Performance benchmarking of the SDN is addressed in
several studies [9], [10], [18], [27]‚Äì[29]. Cbench [9] tests
the maximum capacity of a controller in terms of number
of Ô¨Çows. OFCBench [10] extended Cbench solving several
limitations. Cbench was used in several work [10], [27]‚Äì[29] to
measure response time, throughput, latency for single-instance
deployment of NOX, NOX-MT, Beacon, and ONOS.
Most of the mentioned studies focus on analyzing the Ô¨Çow-
level processing capabilities of controllers, not considering the
impact of real switches on the performance, as showed in
Section VII-C. Measurements are taken sending trafÔ¨Åc through
the local loopback interface [9], [30], to eliminate also the link
bottleneck, or through null providers [29], i.e., a software stub
that avoid any interaction through the SBI. Finally, available
tools only provide network-level metrics (latency and through-
put) with synthetic load generated from inside the controller
[29] or low level interfaces [9]. Finally, none of the available
tools captures important data points (e.g., logs) to simplify the
identiÔ¨Åcation of performance bottlenecks.
None of the tools is designed to provide in-production
performance testing/benchmarking or to test the performance
of the control plane with respect to user-level demands, namely
intent operations requested through the NBI. The approach
of SCP-CLUB is different. It focuses on creating a testing
plane for testing production networks by measuring application-
level performance, and leveraging telco cloud and virtualization
technologies. SCP-CLUB is designed to assess actual controller
performance with concrete intent requests, complete processing
of requests by the controller (intent compilation, installation and
removal), and real or emulated topology.
IX. CONCLUSION
This paper presented SCP-CLUB, a framework for perfor-
mance assessment of SDN deployments on a telco cloud infras-
tructure. It features several tools to automate the deployment
of controllers and to orchestrate a campaign of experiments
under various operating conditions (number of controller in-
stances, VM size, workload, topology). The user deÔ¨Ånes the
experimental campaign by providing a textual speciÔ¨Åcation with
the desired values for conÔ¨Åguration parameters. This allows to
analyze the telco cloud SDN setup capacity and/or to extract
actionable intelligence that can be used to Ô¨Åne-tune or to
dynamically adapt (e.g., scale up or down) the cloud resources
running the control network.
The results show that SCP-CLUB is effective in providing
automated support for sophisticated experimental campaigns for
assessing the performance of a given setup, and/or to extract
actionable intelligence that can be used to Ô¨Åne-tune or to
dynamically adapt (e.g., scale up or down) the cloud resources
running the control network.
REFERENCES
[1] Bell Labs. Mobile market demand between now and 2020.
https:
[2] Huawei.
//readymag.com/BellLabs/480968/, 2017. [Online; accessed Oct 2017].
the Holistic Network In-
http://www.huawei.com/minisite/otf2017/pdf/NEI White Paper
dex.
The Future is Now.pdf, 2017. [Online; accessed Apr 2018].
is Now for
The Future
[15] Centec Networks.
Centec V350 Open SDN Switch.
[3] J.E. Simsarian, N. Choi, Y.J. Kim, S. Fortune, and M. Thottan. NetGraph
In Optical Fiber
data model applied to multilayer carrier networks.
Communications Conference and Exhibition (OFC). IEEE, 2016.
[4] IETF RFC 6020. Yang - a data modeling language for the network
conÔ¨Åguration protocol (netconf). https://tools.ietf.org/html/rfc6020, 2010.
[Online; accessed Apr 2018].
[5] P. Berde, M. Gerola, J. Hart, Y. Higuchi, M. Kobayashi, T. Koide,
B. Lantz, B. O‚ÄôConnor, P. Radoslavov, W. Snow, and G. Parulkar. ONOS:
In Proc. 3rd Workshop on Hot
towards an open, distributed SDN OS.
Topics in Software DeÔ¨Åned Networking (HotSDN). ACM, 2014.
[6] J. Medved, R. Varga, A. Tkacik, and K. Gray. OpenDaylight: Towards a
Model-Driven SDN Controller architecture. In Proc. 15th Int. Symposium
on a World of Wireless, Mobile and Multimedia Networks (WoWMoM).
IEEE, 2014.
[7] P. Rost, C. Mannweiler, D. S. Michalopoulos, C. Sartori, V. Sciancalepore,
N. Sastry, O. Holland, S. Tayade, B. Han, D. Bega, D. Aziz, and
H. Bakker. Network slicing to enable scalability and Ô¨Çexibility in 5g
mobile networks.
IEEE Communications Magazine, 55(5):72‚Äì79, May
2017.
[8] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar, L. Peterson,
J. Rexford, S. Shenker, and J. Turner. OpenFlow: Enabling innovation in
campus Networks. ACM SIGCOMM Computer Communication Review,
38(2):69‚Äì74, 2008.
[9] A. Tootoonchian et al. Cbench: an Open-Flow Controller Benchmarker.
https://github.com/mininet/oÔ¨Çops/tree/master/cbench, 2008.
[Online; ac-
cessed Apr 2018].
[10] M. Jarschel, F. Lehrieder, Z. Magyari, and R. Pries. A Ô¨Çexible OpenFlow-
controller benchmark. In Proc. European Workshop on Software DeÔ¨Åned
Networking (EWSDN), pages 48‚Äì53. IEEE, 2012.
[11] ONOS Project.
Intent framework. https://wiki.onosproject.org/display/
ONOS/Intent+Framework, 2016. [Online; accessed Apr 2018].
[12] N. Gude, T. Koponen, J. Pettit, B. Pfaff, M. Casado, N. McKeown, and
S. Shenker. NOX: towards an operating system for networks. ACM
SIGCOMM Computer Communication Review, 38(13):105‚Äì110, 2008.
[13] D. Erickson. The Beacon OpenFlow Controller. In Proc. 2nd Workshop
on Hot Topics in Software DeÔ¨Åned Networking (HotSDN), pages 13‚Äì18.
ACM, 2013.
[14] B. Lantz, B. Heller, and N. McKeown. A Network in a Laptop:
Rapid Prototyping for Software-DeÔ¨Åned Networks.
In Proc. 9th ACM
SIGCOMM Workshop on Hot Topics in Networks. ACM, 2010.
http://www.
centecnetworks.com/en/SolutionList.asp?ID=43, 2016. [Online; accessed
Apr 2018].
[16] ONOS Project. Onos bug reports. https://jira.onosproject.org/browse/
ONOS-4978, 2016. [Online; accessed Apr 2018].
[17] ONOS Project. Onos bug reports. https://jira.onosproject.org/browse/
ONOS-6780, 2017. [Online; accessed Apr 2018].
[18] S. Jain, A. Kumar, S. Mandal, J. Ong, L. Poutievski, A. Singh, S. Venkata,
J. Wanderer, J. Zhou, M. Zhu, J. Zolla, U. H¬®olzle, S. Stuart, and A. Vahdat.
B4: Experience with a globally-deployed software deÔ¨Åned wan. ACM
SIGCOMM Computer Communication Review, 43(4):3‚Äì14, August 2013.
[19] D. Ongaro and J.K. Ousterhout. In search of an understandable consensus
algorithm. In Proc. USENIX Annual Technical Conference, pages 305‚Äì
320. USENIX, 2014.
[20] The Raft Consensus Algorithm. https://raft.github.io/, 2015.
[Online;
accessed Dec 2017].
[21] W. Golab, M. R. Rahman, A. Auyoung, K. Keeton, and I. Gupta. Client-
centric benchmarking of eventual consistency for cloud storage systems.
In 2014 IEEE 34th International Conference on Distributed Computing
Systems, pages 493‚Äì502, June 2014.
[22] The NetÔ¨Çix Tech Blog. FIT: Failure Injection Testing. http://techblog.
netÔ¨Çix.com/2014/10/Ô¨Åt-failure-injection-testing.html, 2015. [Online; ac-
cessed Apr 2018].
[23] Ali Basiri, Niosha Behnam, Ruud de Rooij, Lorin Hochstein, Luke
Kosewski, Justin Reynolds, and Casey Rosenthal. Chaos Engineering.
IEEE Software, 33(3):35‚Äì41, 2016.
[24] C. l. I, Y. Yuan, J. Huang, S. Ma, C. Cui, and R. Duan. Rethink fronthaul
for soft ran. IEEE Communications Magazine, 53(9):82‚Äì88, September
2015.
[25] Y. J. Kim, J. E. Simsarian, and M. Thottan. Software-deÔ¨Åned trafÔ¨Åc load
balancing for cost-effective data center interconnection service. In 2017
IFIP/IEEE Symposium on Integrated Network and Service Management
(IM), pages 255‚Äì262, May 2017.
[26] B. Kozicki, N. Olaziregi, R. B. Sharpe, K. Oberle, and M. Clougherty.
Software-deÔ¨Åned networks and network functions virtualization in wire-
line access networks. In 2014 IEEE Globecom Workshops (GC Wkshps),
pages 595‚Äì600, Dec 2014.
[27] A. Tootoonchian, S. Gorbunov, Y. Ganjali, M. Casado, and R. Sherwood.
On Controller Performance in Software-DeÔ¨Åned Networks. In Proc. 2nd
USENIX conference on Hot Topics in Management of Internet, Cloud,
and Enterprise Networks and Services (Hot-ICE‚Äô12), 2012.
[28] Y. Zhao, L. Iannone, and M. Riguidel. On the performance of SDN
controllers: A Reality Check.
In Proc. IEEE Conference on Network
Function Virtualization and Software DeÔ¨Åned Network (NFV-SDN), pages
79‚Äì85. IEEE, 2015.
[29] ONOS Project. ONOS Test Plan - Performance and Scale-out. https://
wiki.onosproject.org/pages/viewpage.action?pageId=3441823, 2017. [On-
line; accessed Apr 2018].
[30] M. Jarschel, S. Oechsner, D. Schlosser, R. Pries, S. Goll, and Phuoc Tran-
Gia. Modeling and Performance Evaluation of an OpenFlow Architecture.
In Proc. 23rd Int. TeletrafÔ¨Åc Congress (ITC). IEEE, 2011.
653
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:48:08 UTC from IEEE Xplore.  Restrictions apply.