title:Server-Driven Video Streaming for Deep Learning Inference
author:Kuntai Du and
Ahsan Pervaiz and
Xin Yuan and
Aakanksha Chowdhery and
Qizheng Zhang and
Henry Hoffmann and
Junchen Jiang
Server-Driven Video Streaming for Deep Learning Inference
Kuntai Du∗, Ahsan Pervaiz∗, Xin Yuan, Aakanksha Chowdhery†, Qizheng Zhang, Henry Hoffmann, Junchen Jiang
University of Chicago
†Google
ABSTRACT
Video streaming is crucial for AI applications that gather videos
from sources to servers for inference by deep neural nets (DNNs).
Unlike traditional video streaming that optimizes visual quality,
this new type of video streaming permits aggressive compres-
sion/pruning of pixels not relevant to achieving high DNN inference
accuracy. However, much of this potential is left unrealized, because
current video streaming protocols are driven by the video source
(camera) where the compute is rather limited. We advocate that the
video streaming protocol should be driven by real-time feedback
from the server-side DNN. Our insight is two-fold: (1) server-side
DNN has more context about the pixels that maximize its infer-
ence accuracy; and (2) the DNN’s output contains rich information
useful to guide video streaming. We present DDS (DNN-Driven
Streaming), a concrete design of this approach. DDS continuously
sends a low-quality video stream to the server; the server runs the
DNN to determine where to re-send with higher quality to increase
the inference accuracy. We find that compared to several recent
baselines on multiple video genres and vision tasks, DDS maintains
higher accuracy while reducing bandwidth usage by upto 59% or
improves accuracy by upto 9% with no additional bandwidth usage.
CCS CONCEPTS
• Networks→ Application layer protocols; • Information sys-
tems→ Data streaming; Data analytics; • Computing method-
ologies → Computer vision problems;
KEYWORDS
video analytics, video streaming, deep neural networks, feedback-
driven
ACM Reference Format:
Kuntai Du, Ahsan Pervaiz, Xin Yuan, Aakanksha Chowdhery, Qizheng
Zhang, Henry Hoffmann, Junchen Jiang. 2020. Server-Driven Video Stream-
ing for Deep Learning Inference. In Annual conference of the ACM Special
Interest Group on Data Communication on the applications, technologies,
architectures, and protocols for computer communication (SIGCOMM ’20),
August 10–14, 2020, Virtual Event, USA. ACM, New York, NY, USA, 14 pages.
https://doi.org/10.1145/3387514.3405887
∗Both authors contributed equally to this research.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-7955-7/20/08...$15.00
https://doi.org/10.1145/3387514.3405887
557
1 INTRODUCTION
Internet video must balance between maximizing application-level
quality and adapting to limited network resources. This perennial
challenge has sparked decades of research and yielded various
models of user-perceived quality of experience (QoE) and QoE-
optimizing streaming protocols. In the meantime, the proliferation
of deep learning and video sensors has ushered in new analytics-
oriented applications (e.g., urban traffic analytics and safety anom-
aly detection [5, 22, 27]), which also require streaming videos from
cameras through bandwidth-constrained networks [24] to remote
servers for deep neural nets (DNN)-based inference. We refer to it
as machine-centric video streaming. Rather than maximizing human-
perceived QoE, machine-centric video streaming maximizes for
DNN inference accuracy. This contrast has inspired recent efforts to
compress or prune frames and pixels that may not affect the DNN
output (e.g., [30–32, 36, 48, 76, 78, 80]).
A key design question in any video streaming system is where to
place the functionality of deciding which actions can optimize applica-
tion quality under limited network resources. Surprisingly, despite
a wide variety of designs, most video streaming systems (both
machine-centric and user-centric) take an essentially source-driven
approach—it is the content source that decides how the video should
be best compressed and streamed. In traditional Internet videos
(e.g., YouTube, Netflix), the server (the source) encodes a video at
several pre-determined bitrate levels, and although the mainstream
protocol, DASH [7], is dubbed a client-driven protocol, the client
does not provide any instant user feedback on user-perceived QoE
to let server re-encode the video. Current machine-centric video
streaming relies largely on the camera (the source) to determine
which frames and pixels to stream.
While the source-driven approach has served us well, we argue
that it is suboptimal for analytics-oriented applications. The source-
driven approach hinges on two premises: (1) the application-level
quality can be estimated by the video source, and (2) it is hard
to measure user experience directly in real time. Both need to be
revisited in machine-centric video streaming.
First, it is inherently difficult for the source (camera) to estimate
the inference accuracy of the server-side DNN by itself. Inference
accuracy depends heavily on the compute-intensive feature ex-
tractors (tens of NN layers) in the server-side DNN. The disparity
between most cameras and GPU servers in their compute capabil-
ity means that any camera-side heuristics are unlikely to match
the complexity of the server-side DNNs. This mismatch leads to
the suboptimal performance of the source-driven protocols. For
instance, some works use inter-frame pixel changes [30] or cheap
object detectors [80] to identify and send only the frames/regions
that contain new objects, but they may consume more bandwidth
than necessary (e.g., background changes causing pixel-level differ-
ences) and/or cause more false negatives (e.g., small objects could
be missed by the cheap camera-side object detector).
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
K. Du, A. Pervaiz, X. Yuan, A. Chowdhery, Q. Zhang, H. Hoffmann, J. Jiang
Second, while eliciting real-time feedback from human users may
be hard, DNN models can provide rich and instantaneous feedback.
Running an object-detection DNN on an image returns not only
detected bounding boxes, but also additional feedback for free, like
the confidence score of these detections, intermediate features, etc.
Moreover, such feedback can be extracted on-demand by probing
the DNN with extra images. Such abundant feedback information
has not yet been systematically exploited by prior work.
In this paper, we explore an alternative DNN-driven approach
to machine-centric video streaming, in which video compression
and streaming are driven by how the server-side DNN reacts to
real-time video content. DNN-driven video streaming follows an
iterative workflow. For each video segment, the camera first sends it
in low quality to the server for DNN inference; the server runs the
DNN and derives some feedback about the most relevant regions to
the DNN inference and sends this feedback to the camera; and the
camera then uses the feedback to re-encode the relevant regions in a
higher quality and sends them to the server for more accurate infer-
ence. (The workflow can have multiple iterations though this paper
only considers two iterations). Essentially, by deriving feedback di-
rectly from the server-side DNN, it sends high-quality content only
in the minimal set of relevant regions necessary for high inference
accuracy. Moreover, unlike prior work that requires camera-side
vision processing or hardware support (e.g., [30, 48, 80]), we only
need standard video codec on the camera side.
The challenge of DNN-driven protocols, however, is how to derive
useful feedback from running DNN on a low-quality video stream.
We present DDS (DNN-Driven Streaming), a concrete design which
utilizes the feedback regions derived from DNN output on the low-
quality video and sparingly uses high-quality encoding for the
relatively small number of regions of interest. We apply DDS to
three vision tasks: object detection, semantic segmentation, and
face recognition. The insight is that the low-quality video may not
suffice to get sufficient DNN inference accuracy, but it can produce
surprisingly accurate feedback regions which intuitively require
higher quality for the DNN to achieve desirable accuracy. Feedback
regions are robust to low-quality videos because they are more
akin to binary-class tasks (i.e.,whether a region might contain an
object and need higher quality) than to more difficult tasks such as
classifying what object is in each region. Moreover, DDS derives
feedback regions from DNN output without extra GPU overhead.
DDS is not the first to recognize that different pixels affect DNN
accuracy differently, e.g., prior works also send only selected re-
gions/frames to trigger server-side inference [54, 80]. But unlike
DDS, these regions are selected either by simple camera-side log-
ics [80] which suffer from low accuracy, or by region-proposal
networks (RPNs) [54] which are designed to capture where objects
are likely present, rather than where higher quality is needed (e.g.,
large targeted objects will be selected by RPNs but they do not need
high video quality to be accurately recognized). Using RPNs also
limits the applications to object detection and does not generalize
to other tasks such as semantic segmentation. In a broader context,
DDS is related and complementary to the trend in deep learning
of using attention mechanisms (e.g., [61, 74])—attention improves
DNN accuracy by focusing computation on the important regions,
while DDS improves bandwidth efficiency by sending only a few
(a) Input
(b) Object detection
(c) Sem. segmentation
Figure 1: The input and output of object detection and semantic
segmentation on one example image. We use red to label the car and
blue to label the truck.
regions in high quality to achieve the same DNN accuracy as if the
whole video is sent in the highest quality.
We evaluate DDS and a range of recent solutions [30, 54, 76, 78,
80] on three vision tasks. Across 49 videos, we find DDS achieves
same or higher accuracy while cutting bandwidth usage by upto
59%, or uses the same bandwidth consumption while increasing
accuracy by 3-9%. This work does not raise any ethical issues.
2 MOTIVATION
We start with the background of video streaming for distributed
video analytics, including its need, performance metrics, and design
space. We then use empirical measurements to elucidate the key
limitations of prior solutions.
2.1 Video streaming for video analytics
Vision tasks under consideration: We consider three computer
vision tasks—object detection, semantic segmentation, and face
recognition. Figure 1 shows an example input and output of object
detection (one label for each bounding box) and semantic segmen-
tation (one label for each pixel). These tasks are widely used in
real-world scenarios to detect/segment objects of interest and their
results are used as input to high-level applications (e.g., vehicle
collision detection).
Why streaming videos out from cameras? On one hand, com-
puter vision accuracy has been improved by deep learning at the
cost of increased compute demand. On the other hand, low prices of
high-definition network-connected cameras make them widely de-
ployed in traffic monitoring [27], video analytics in retail stores [12],
and inspection of warehouses or remote industrial sites [38]. Thus,
the camera operators must scale out the compute costs of analyzing
ever more camera feeds [2, 6, 21]. One solution is to offload the
compute-intensive inference (partially or completely) to centralized
GPU servers. (Sometimes, video feeds must be kept local due to
privacy regulations, but it is beyond our scope.) For the sake of
discussion, let us calculate the costs of 60 HD cameras each run-
ning ResNet50 classification at 90FPS. We use ResNet50 classifier
because our applications require more complex DNN models (e.g.,
FasterRCNN-ResNet101) cannot run on Jetson TX2 [9] at 30FPS.
Now, buying 60 Raspberry Pi 4 Cameras and an NVIDIA Tesla T4
GPU (with a throughput of running ResNet50 at 5,700FPS [17])
costs $23 × 60(cameras)[19]+$2000(GPU)[13]= $3.4K. Buying 60
NVIDIA Jetson TX2 cameras (each running ResNet50 at 89FPS [16])
costs about $400[15]×60 = $24K, which is one order of magnitude
more expensive. These numbers may vary over time, but the price
gap between two approaches is likely to remain. The calculation
558
Server-Driven Video Streaming for Deep Learning Inference
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
does not include the network bandwidth to send the videos to a
server, which is what we will minimize.
Performance metrics: An ideal video streaming protocol for
video analytics should balance three metrics: accuracy, bandwidth
usage, and freshness.
• Accuracy: We define accuracy by the similarity between the DNN
output on each frame when the video is streamed to the server
under limited bandwidth and the DNN output on each frame
when the original (highest quality) video is streamed to the server.
By using the DNN output on the highest-quality video (rather
than the human-annotated labels) as the “ground truth”, we can
reveal any negative impact of video compression and streaming
on DNN inference, without being affected by any errors made by
the DNN itself. This is consistent with recent work (e.g., [45, 78,
79]). We measure the accuracy by F1 score in object detection (the
harmonic mean of precision and recall for the detected objects’
location and class labels) and by IoU in semantic segmentation
(the intersection over union of pixels associated to the same class).
• Bandwidth usage: In general, the total cost of operating a video
analytics system includes the camera cost, the network cost paid
to stream the video from the camera to the server, and the cost of
the server. In this paper, we focus on reducing the network cost