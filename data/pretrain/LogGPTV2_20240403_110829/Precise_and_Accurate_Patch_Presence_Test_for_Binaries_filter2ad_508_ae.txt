### Strictly Images for CVE-2016-5696
898    27th USENIX Security Symposium
USENIX Association

### Table 3: Offline Phase Performance
| Step | Analyze (s) | Translation (s) | Cnt. ** Avg. (s) | Total (s) |
|------|-------------|-----------------|-----------------|-----------|
| 0.20 | 5.49        | 9.04            | 11.66           | 21.52     |
| 6.29 | 1608.52     | 6.00            | -               | -         |
| Match Ref.0 * | 2647.78 | -               | -               | -         |
| Match Ref.1 * | 3415.54 | -               | -               | -         |
| 7.56 | -           | -               | -               | -         |

* Match against reference kernels for uniqueness test.
* 0 for un-patched kernel, 1 for patched kernel.
** Analyze: Patch. Others: Binary Signature.

### Table 3: Offline Phase Performance
| Count | 107 | 293 | 293 | 293 |
|-------|-----|-----|-----|-----|

### Offline Phase Performance
In general, FIBER aims to detect the exact same patch as in the reference kernel. However, to be conservative, we still consider such cases as false negatives. Some false negative (FN) cases are caused by engineering issues. For example, certain binary instructions cannot be recognized and decoded by the frontend of angr (two cases in total), which affects the subsequent Control Flow Graph (CFG) generation and symbolic execution.

### 6.3 Performance
In this section, we evaluate FIBER’s runtime performance for both offline signature generation and online matching. The time consumption for the offline phase is listed in Table 3, and that for the online phase is in Table 2. From the tables, we can see that a small fraction of patches requires much longer time to match than the average. This is usually because the change sites in these patches are located in very large and complex functions (e.g., CVE-2017-0521), causing the matching engine to encounter root instructions deep inside the function. However, most patches can be analyzed, translated, and matched within a reasonable time. In the end, we argue that a human would likely take minutes, if not longer, to verify a patch. An automated and accurate solution like ours is preferable, especially since we can parallelize the analysis of different patches.

### 6.4 Unported Patches
As shown in Table 2, for all test subjects except kernel #5, FIBER produces some true negative (TN) cases, suggesting un-patched vulnerabilities. If related security patches were available before the test subject's release date, it means the test subject failed to apply the patch timely. Table 4 lists all the vulnerabilities whose patches were not propagated to one or multiple test subject kernels in our evaluation. Note that for security concerns, we do not correlate these vulnerabilities with actual kernels in Table 2.

### Table 4: Potential Security Loopholes
| CVE          | Patch Date * (mm/yy) | Type** | Severity* |
|--------------|----------------------|--------|-----------|
| CVE-2014-9781 | 07/16                | I      | High      |
| CVE-2016-2502 | 07/16                | P      | High      |
| CVE-2016-3813 | 07/16                | I      | Moderate  |
| CVE-2016-4578 | 08/16                | I      | Moderate  |
| CVE-2016-2184 | 11/16                | P      | Critical  |
| CVE-2016-7910 | 11/16                | P      | Critical  |
| CVE-2016-8413 | 03/17                | I      | Critical  |
| CVE-2016-10200 | 03/17                | P      | Critical  |
| CVE-2016-10229 | 04/17                | E      | Critical  |

* Obtained from Android security bulletin.
** P: Privilege Elevation, E: Remote Code Execution, I: Information Disclosure

From Table 4, we can see that even some critical vulnerabilities were not patched in time, indicating a significant risk that they could be exploited to compromise the kernel entirely and execute arbitrary code. One such case involved a major vendor (who confirmed the case and requested anonymity) where a patch was delayed for more than half a year. This highlights the value of tools like FIBER.

We also identified four vulnerabilities in Table 4 that were eventually patched in a later kernel release but not in the earliest kernel release after the patch release date, indicating a significant delay in the patch propagation process. It is worth noting that FIBER tests whether the patch exists in the target kernel, but the absence of a security patch does not necessarily mean the target kernel is exploitable. Further verification is still needed.

### 6.5 Case Study
In this section, we demonstrate some representative security patches used in our evaluation to show the strength of FIBER compared to other solutions.

#### Format String Change
There are five patches in our collection that only change the format strings. For example, the patch for CVE-2016-6752 in Figure 6 changes the specifier `p` to `pK`. Detecting this at the binary level without dereferencing the string pointer is impossible, as all other features (e.g., instruction type) remain the same. Without patch insights, it is extremely difficult to decide which register or memory location should be regarded as a pointer and whether it should be dereferenced in the matching topology. FIBER can correctly determine that the only change is the argument format string and then test patch presence by matching the string content.

#### Small Change Site
It is common for a security patch to introduce small and subtle changes, such as the one for CVE-2016-8417 in Figure 6, where the operator `>` is replaced with `>=`. Such a change has no impact on the CFG topology and only slightly alters one conditional jump instruction. FIBER handles this case correctly because the conditional jump is part of the root instruction, and we check the comparison operator associated with it.

#### Patch Backport
A downstream kernel may selectively apply patches (security or other bug fixes), causing functions to look different from upstream. Our reference kernel (v3.10) is a downstream version compared to all test subjects except #6. The patch for CVE-2016-3858 in Figure 6 has a prior patch in the upstream (which deletes an `if-then-return` statement) for the same affected function, which was not applied to our reference kernel. This makes the two functions look different, although both are patched. FIBER is robust to such backporting cases because the generated binary signature is fine-grained and related to a single patch.

#### Multiple Patched Function Versions
After a security patch is applied, the same function may be modified by future patches. Thus, similar to backporting cases, two patched functions can still be different because they are on different versions. CVE-2014-9785 is an example. FIBER can still precisely locate the same change site even when faced with a much newer target function that differs significantly from the reference function.

#### Constant Change
The patch for CVE-2015-8944 in Figure 6 only changes a function argument from `0` to a predefined constant `S_IRUSR` (0x100 in the reference kernel). Such a small change makes the patched and un-patched functions highly similar. Even though a solution might strictly differentiate constant values, it is generally unsafe because constants can change across binaries. However, with the insights of the fine-grained change site, FIBER can correctly figure out that only the value of the second function argument matters in the matching and that it should be non-zero if patched, thus effectively handling such cases.

#### Similar Basic Blocks
FIBER generates fine-grained signatures containing only a limited set of basic blocks. It is likely that there will be other similar basic blocks if we only look at the basic block level semantics. One such example is shown in Figure 1 and discussed in Section 3. Previous work based on basic block level semantics [27, 26] may fail to handle such cases, while FIBER integrates function-level semantics into the local CFG, resulting in fine-grained signatures that are both stable and unique.

### 7 Conclusion
In this paper, we formulate a new problem of patch presence testing under a "source to binary" scenario. We then design and implement FIBER, a fully automatic solution that leverages source-level information for accurate and precise patch presence testing in binaries. FIBER has been systematically evaluated with real-world security patches and a diverse set of Android kernel images. The results show that it can achieve excellent accuracy with acceptable performance, making it highly practical for security analysts.

### Acknowledgement
We wish to thank Michael Bailey (our shepherd) and the anonymous reviewers for their valuable comments and suggestions. Many thanks to Prof. Heng Yin and Prof. Chengyu Song for their insightful discussions. This work was supported by the National Science Foundation under Grant No. 1617573.

900    27th USENIX Security Symposium
USENIX Association

### References
[1] Android Security Bulletin. https://source.android.com/security/bulletin/
[2] BinDiff. https://www.zynamics.com/bindiff.html
[3] CVE: Vulnerabilities By Year. https://www.cvedetails.com/browse-by-date.php
[4] Github Annual Report. https://octoverse.github.com/
[5] NetworkX Python Package. https://networkx.github.io/
[6] Security Patch for CVE-2015-8955. https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git/commit/?id=8fff105e13041e49b82f92eef034f363a6b1c071
[7] T. Avgerinos, A. Rebert, S. K. Cha, and D. Brumley. Enhancing symbolic execution with veritesting. ICSE’14.
[8] B. S. Baker. Parameterized duplication in strings: Algorithms and an application to software maintenance. SIAM J. Comput., 26(5):1343–1362, October 1997.
[9] M. Bourquin, A. King, and E. Robbins. Binslayer: Accurate comparison of binary executables. In Proceedings of the 2nd ACM SIGPLAN Program Protection and Reverse Engineering Workshop.
[10] W. Cui, M. Peinado, Z. Xu, and E. Chan. Tracking rootkit footprints with a practical memory analysis system. USENIX Security’12.
[11] L. de Moura and N. Bjørner. Z3: An efficient SMT solver. In Tools and Algorithms for the Construction and Analysis of Systems, 2008.
[12] Q. Feng, M. Wang, M. Zhang, R. Zhou, A. Henderson, and H. Yin. Extracting conditional formulas for cross-platform bug search. ASIACCS’17.
[13] Q. Feng, R. Zhou, C. Xu, Y. Cheng, B. Testa, and H. Yin. Scalable graph-based bug search for firmware images. CCS ’16.
[14] D. Gao, M. K. Reiter, and D. Song. Binhunt: Automatically finding semantic differences in binary programs. In Information and Communications Security, 2008.
[15] F. K. Hwang, D. S. Richards, and P. Winter. The Steiner tree problem, volume 53. Elsevier, 1992.
[16] J. Jang, A. Agrawal, and D. Brumley. Redebug: Finding unpatched code clones in entire OS distributions. Oakland’12.
[17] L. Jiang, G. Misherghi, Z. Su, and S. Glondu. Deckard: Scalable and accurate tree-based detection of code clones. ICSE’07.
[18] T. Kamiya, S. Kusumoto, and K. Inoue. CCFinder: A multilinguistic token-based code clone detection system for large-scale source code. IEEE Transactions on Software Engineering, 28(7):654–670, Jul 2002.
[19] W. M. Khoo, A. Mycroft, and R. Anderson. Rendezvous: A search engine for binary code. In 2013 10th Working Conference on Mining Software Repositories (MSR), 2013.
[20] S. Kim, S. Woo, H. Lee, and H. Oh. Vuddy: A scalable approach for vulnerable code clone discovery. Oakland’17.
[21] J. Lee, T. Avgerinos, and D. Brumley. TIE: Principled reverse engineering of types in binary programs. NDSS’11.
[22] Z. Li, S. Lu, S. Myagmar, and Y. Zhou. CP-Miner: Finding copy-paste and related bugs in large-scale software code. IEEE Transactions on Software Engineering, 32(3):176–192, March 2006.
[23] Z. Li, D. Zou, S. Xu, H. Jin, H. Qi, and J. Hu. Vulpecker: An automated vulnerability detection system based on code similarity analysis. ACSAC’16.
[24] J. Ming, M. Pan, and D. Gao. iBinhunt: Binary hunting with inter-procedural control flow. In Proceedings of the 15th International Conference on Information Security and Cryptology.
[25] OpenSignal. Android Fragmentation Visualized. https://opensignal.com/reports/2015/08/android-fragmentation/
[26] J. Pewny, B. Garmany, R. Gawlik, C. Rossow, and T. Holz. Cross-architecture bug search in binary executables. Oakland’15.
[27] J. Pewny, F. Schuster, C. Rossow, L. Bernhard, and T. Holz. Leveraging semantic signatures for bug search in binary programs. ACSAC’14.
[28] D. A. Ramos and D. Engler. Under-constrained symbolic execution: Correctness checking for real code. USENIX Security’15.
[29] Y. Shoshitaishvili, R. Wang, C. Salls, N. Stephens, M. Polino, A. Dutcher, J. Grosen, S. Feng, C. Hauser, C. Kruegel, and G. Vigna. SoK: (State of) The Art of War: Offensive Techniques in Binary Analysis. Oakland’16.
[30] Y. Tian, J. Lawall, and D. Lo. Identifying Linux bug fixing patches. ICSE’12.
[31] X. Xu, C. Liu, Q. Feng, H. Yin, L. Song, and D. Song. Neural network-based graph embedding for cross-platform binary code similarity detection. CCS ’17.

902    27th USENIX Security Symposium
USENIX Association