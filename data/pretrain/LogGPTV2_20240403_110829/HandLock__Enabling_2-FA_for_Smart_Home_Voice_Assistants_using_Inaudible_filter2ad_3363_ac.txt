coefficient of variation of phase change, θi. After experimenting
with different sliding windows, we consider a sliding window of
size 200 with a step size of 10. This results in a series of coefficients
of variation, where we use the moving average of 20 consecutive
values to reduce the impact of outliers. We found cv ≥ 0.15 in the
presence of hand movement. Thus, we set T = 0.15 to locate the
start and end of a gesture movement. Figure 9a shows the cv of
θi for two consecutive Z gestures and Figure 9b shows the corre-
sponding phase change for the two gestures with the start (marked
as red triangle) and end (marked as blue rectangle) points marked.
3.4 Feature Extraction
The Doppler shift can be simplified as ∆f = 2v f0/c as discussed in
Section 3.2. The hand speed is proportional to the relative phase
change θi, which can be derived from Eq. 9. Therefore, we consider
θi as the relative speed of the hand movement. As the amplitude
A′ is constant, we weighted two acceleration readings to calculate
the average acceleration.
Acci =
(10)
θi +1 − θi + θi +2−θi
2
2
Dealing with Varying Speed. Figure 10 plots the duration of
performing ‘Z’ gestures by five users. Each box plot includes 60
samples. As we can see, different users take different amounts of
time to complete the gestures. Even the same user takes different
amounts of time to complete a gesture (as evident from the box
plot). We must therefore handle the changes in speed from the
same user. HandLock employs resampling so that each identified
6
Figure 10: The distribution of duration for performing the
‘Z’ gesture for five users.
gesture segment contains the same number of sample points. Given
a gesture segment containing M data points, HandLock upsamples
the θi to N points if M  N . Specifically, we apply Antialiasing Lowpass Filter [6]
to resample θi to fixed size of N (=1000) samples as the average
duration for a gesture in our dataset was around 2 seconds (with a
sample frequency of 480 Hz, we set N to 1000 samples). Next, we
multiply the time-series data with M
N to normalize the value of θi.
Thus, all gesture instances are represented by the same number of
samples and are also scaled accordingly. Figure 11 illustrates the
speed profiles of two users performing the Z gesture. We can see
that the phase profiles are properly scaled/normalized. Similarly, we
calculate the acceleration profile from phase change using Eq. 10.
Figure 11: Speed profiles of different users performing the
‘Z’ gesture. Different colors represent gestures performed at
different timestamp.
We observed that the phase/speed profiles of a given gesture
from the same user are similar, but different from other users as
shown in Figure 11. A user often performs a gesture at different
speed and acceleration. To extract features, HandLock needs to
further segment each speed and acceleration time-series data into
multiple smaller chunks to capture the subtle idiosyncrasies in
which the speed and acceleration changes for a given user.
Feature Vector. Table 2 lists the features we used. We extract
temporal and spectral features from both the speed and acceleration
time series. First, we compute 8 single-valued features for both
speed and acceleration including Mean, Median, RMS, STD, MAD,
10th percentile, 90th percentile, and median frequency. To gain more
fine-grained insights into the change in speed and acceleration, we
split each gesture segment into 20 equal sized chunks and calculate
RMS, STD, MAD and Mean from each chuck. To extract the spectral
features like PSD from speed and acceleration, we apply FFT on each
time-series data, then perform max-min normalization on the power
of all frequencies. Onward, we segment the PSD values into 20 small
chunks and calculate the mean value from each chunk. Similarly,
0500100015002000Samples012cv0.1505001000150020002500Samples0510-312345User123Duration (s)0200400600800100005Speed10-3User 102004006008001000Samples00.51Speed10-3User 2256HandLock: Enabling 2-FA for Smart Home Voice Assistants using Inaudible Acoustic Signal
RAID ’21, October 6–8,2021, San Sebastian, Spain
Table 2: Explored temporal and spectral features.
Domain
Time
Feature
Mean
Median
RMS
STD
MAD
10 th percentile
90 th percentile
Auto-correlation
Description
Arithmetic mean of signal strength
Arithmetic median of the signal strength
Root mean of the squares of the signal strength
Standard deviation of the signal strength
Mean absolute deviation of the signal strength
10th percentile value of the signal strength
90th percentile value of the signal strength
Correlation of a signal with a delayed copy of itself
Frequency Median Frequency Median frequency value of the signal
Power spectral density of the signal
PSD
we compute the signal auto-correlation coefficient of speed and
acceleration, and normalize the auto-correlation coefficient; we then
segment the normalized coefficient into 20 chunks and compute
the mean value from each chunk. In total, we extract 256 features
from both the speed and acceleration time-series data.1
3.5 User Modeling
HandLock adopts a supervised machine learning approach, there-
fore, users need to provide gestures as training data. Let us assume
the number of enrolled users in a household is U. Each enrolled
user is labeled differently and is a member of the positive class set
UP. HandLock comes with a set of negative samples from unknown
users (e.g., five users) represented as UN. HandLock supports two
forms of enrollment of multiple users (1 ≤ N ≤ U) under two
different scenarios: 1) enrollment of Multiple Users Same Gesture
(MUSG), and 2) enrollment of Multiple Users Multiple Gestures
(MUMG).
Single-user vs. Multi-user Setting. When the user first enrolls
his/her gesture, HandLock prompts for gesture options through the
voice interface and the user provides samples of a given gesture for
training. Multiple users can enroll with a gesture, where gestures
can be of the same or different type. This multi-user setting is
considered as a multi-class classification problem.
Feature Selection. To find the best feature for HandLock, we ex-
plore all the features using the FEAST toolbox [5, 13] and select the
Joint Mutual Information criterion (JMI) for ranking the features.
Balanced Learning. HandLock uses gesture samples from un-
known users as samples from the negative class. If we assume each
user provides k (e.g., 20) samples for a given gesture then UP is
the minority class while UN is the majority class (as we assume
samples from five unknown users). The dataset size is imbalanced
due to unequal size of UP and UN. Therefore, we need to up
sample the authentic user’s data to achieve equal class represen-
tation. We test Synthetic Minority Over-sampling (SMOTE) [17]
and Adaptive Synthetic Sampling (ADASYN) [30] methods which
are two popular up-sampling approaches. We selected ADASYN as
our up-sampling approach as it provided better performance. The
results are available in Section 4.1.2.
1 Per time-series data we compute (8 + 20 × 4 + 20 + 20) = 128 features.
7
3.6 Verification
Once we have features extracted from the hand gesture, we use
supervised learning to identify the legitimate user. HandLock col-
lects training data from the authorized user to build one binary or
multi-class classification model. We explore four classifiers includ-
ing Random Forest (RF), Decision Tree (DT), k-nearest neighbors
(KNN), and Support Vector Machine (SVM).
Threat Model and Attack Settings. Our threat model assumes
that an adversary can interact with the victim’s VA. The attacker
then attempts to bypass HandLock by performing a hand gesture.
We consider three settings, where the adversary either knows or
does not know the victim’s chosen gesture. Therefore, we consider
the following attacks.
• Random Gestures: The attacker does not know the exact gesture
performed by the victim, but attempts to authenticate him-
self/herself by performing a random gesture.
• Gesture Mimicry: The attacker knows the exact gesture per-
formed by the victim, for example, by observing the victim
perform a gesture during an authentication session.
• Replay Attack: The attacker places a nearby microphone to
record the exact gesture performed by the victim and attempts
to authenticate by replaying the recorded signal.
4 EVALUATION
In this section, we perform a comprehensive analysis of HandLock
under various settings to evaluate its accuracy, stability, resiliency
to attacks, and system-level performance. First, we evaluate the
overall accuracy and efficiency of our system (Section 4.1), cov-
ering five gestures as shown in the Figure 13. We then examine
its resilience against random gesture mimicry and replay attacks
(Section 4.2). Next, we evaluate the sensitivity of our system (Sec-
tion 4.3) by analyzing the impact of the following factors: number
of users enrolled (Section 4.3.1), number of microphones used (Sec-
tion 4.3.2), temporal stability (Section 4.3.3), distance between hand
and VA (Section 4.3.4), Cross-environment stability (Section 4.3.6),
and ambient noise (Section 4.3.5). Lastly, we evaluate system-level
performance metrics like processing time and memory consump-
tion in Section 4.4.
Device Setup. As current commercial VAs are not allowed to log
raw audio, we implement HandLock using a Seeed’s ReSpeaker
Core V2.0 [8], which runs on GNU/Linux operating system and is
designed for voice interface applications with a quad-core ARM
Cortex A7, running up to 1.5GHz with 1GB RAM. Figure 12a shows
the device setup of HandLock. The board is equipped with a six
microphone array — similar to how microphones are distributed in-
side an Amazon Echo Dot [1]. We wire it to an external 3W speaker
AS07104PO-LW152-R [41]. We use a 3D-printed casing to hold the
microphone array and speaker. The device is powered by a 10000
mAh Mi Power Bank 2 [7]. We play Continuous Wave sound and
record the 6-channel audio simultaneously when collecting gesture
data. Participants were invited to interact with our prototype VA
located inside a lab space that emulates a smart home living room
equipped with a table, sofa, desktop computer, smart TV, smart
lights, motion sensors and smart cameras. Figure 12b shows the lab
257RAID ’21, October 6–8,2021, San Sebastian, Spain
Shaohu Zhang and Anupam Das
Figure 13: Different types of gestures evaluated.
Table 4: Summary of the various datasets collected.
(a) Device setup
(b) Lab setup
Figure 12: We used Seeed’s ReSpeaker Core V2.0 [8], which
is equipped with six microphones. Lab setup showing how
participants interacted with our prototype VA.
Table 3: Demographics of participants.
Attribute
Age
Gender
Education
Student
VAs owned
Values (count)
18-24 (6), 25-34 (27), 35-44 (11), 55-64 (1)
Male (21), Female (24)
High school graduate (2), Bachelor’s Degree (17),
Master’s Degree (16), Doctorate Degree (10)
Yes (28), No (17)
0 (23), 1 (17), 2 (1), 3 (1), more than 3 (3)
setting where a participant is performing a hand gesture with our
VA system.
Participants. We obtained necessary IRB approval to collect data
from participants. The total participation time was around 45 min-
utes (providing breaks between sessions). Participants were com-
pensated ($15) for their time. Table 3 summarizes the participant
details. In total, we recruited 45 participants, 21 identified them-
selves as males, while 24 identified themselves as females. Around
73.33 % (33/45) of participants were aged between 18 and 34. A ma-
jority, 95.56 % (43/45) of them reported to have earned a bachelor
or higher educational degree, and 62.22 % (28/45) of participants
were current students in a university, while the remaining partici-
pants were not students. 48.89 % participants reported owning one
or more smart home voice assistant (VA) devices such as Google
Home, Amazon Echo, or Xiaomi.
Data Collection Process. The data was collected in our lab from
January, 2020 to March, 2020.2 Before collecting any data, each
participant was trained for 5 minutes, so that he/she understood
how the data collection process works. We randomly split the 45
participants into 39 benign users (U) and 6 attackers (A). For
evaluation purposes we consider five popular gestures: ‘Z’, ‘W ’, ‘X’,
‘✓’ and ‘9’ (as shown in Figure 13). We asked each participant to
continuously perform a given gesture with a small pause between
two subsequent gestures, where participants placed their hand
anywhere in the range of 5 ∼ 30 cm from our prototype VA. We
found that each participant typically performed 10 ∼ 20 gestures in
a single minute. In order to make our data collection process more
realistic, we asked the participants to take a rest and walk around
after each one-minute data collection session. We then repeat the
whole process until we get the required number of samples.
2We followed COVID-19 safety protocols mandated by our IRB office.
Participants Gestures
39
10
10
6
Dataset
1∗
2†
3‡
4⋄
∗ performed in a single day; † collected after one week and one
month; ‡ random gestures; ⋄ attacker emulates vicitm’s gesture
Samples/gesture Total samples
60
30
1
30
39 × 5 × 60=11700
10 × 5 × 30 × 2=3000
10 × 30 × 1=300
6 × 5 × 30=900
5
5
30
5
Datasets. We summarize the collected datasets in Table 4. In
Dataset 1, for each user, we collected 60 samples for each of the five
gestures illustrated in Figure 13. Dataset 2 is collected for evaluating