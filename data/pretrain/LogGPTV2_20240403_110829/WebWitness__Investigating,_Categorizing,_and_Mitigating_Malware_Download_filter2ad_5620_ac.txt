in most cases of a malware drop/update, it is not
the browser, but the update software making the re-
quests. In practice, we observed that the majority of
malware update download paths did not report a pop-
ular user-agent string (only 36% of them did). Ta-
ble 2 reports the percentage of paths that include a
popular user-agent string.
3 WebWitness
Inspired by our study of real-world malware download
paths, we develop a system called WebWitness that can
automate the investigation of new malware download at-
tacks. The primary goal of this system is to provide con-
text around malicious executable downloads. To this end,
given a trafﬁc trace that includes all web trafﬁc recorded
during a time window preceding (and including) a ma-
licious executable ﬁle download, WebWitness automati-
cally traces back and categorizes the web paths that led
the victim to the malicious download event.
In this section, we describe the components of our sys-
tem, which are shown in Figure 2.
3.1 ATC - Download Path Traceback
Given a malicious ﬁle download trace from a given
client, WebWitness aims to trace back the download path
consisting of the sequence of web pages visited by the
user that led her to a malware download attack (e.g., via
social engineering or to a drive-by exploit). As detailed
in Section 2.4, the trace may contain many HTTP trans-
actions that are unrelated to the download. Furthermore,
it is not always possible to correctly link two related con-
secutive HTTP transactions by simply leveraging their
HTTP Referer or Location headers.
To mitigate the limitations of referrer-only approaches
and more accurately trace back the download path,
we devise an algorithm that leverages the features and
heuristics we identiﬁed during our initial study of in-
the-wild malware downloads presented in Section 2.4.
In summary, we build a transactions graph, where
nodes are HTTP transactions within the download trace,
and edges connect transaction according to a “proba-
ble source of” relationship (explained in detail below).
Then, starting from the node (i.e., the HTTP transaction)
related to the malware ﬁle download, we walk back along
the most probable edges until we ﬁnd a node with no pre-
decessor, which we label as the “origin” of the download
path. In the following, we provide more details on our
traceback algorithm.
Transactions Graph. Let D be the dataset of HTTP
trafﬁc generated by host A before (and including) the
download event. We start by considering all HTTP trans-
actions in D, and construct a weighted directed graph
G = (V,E). The vertices are A’s HTTP transactions
and the edges represent the relation “probable source
of” for pairs of HTTP transactions. As an example, the
edge e = (v1 → v2) implies that HTTP transaction v1
likely produced HTTP transaction v2, either automati-
cally (e.g., via a server-imposed redirection, javascript,
etc.) or through explicit user interaction (e.g., via a hy-
perlink click). Thus, we can consider v1 as the “source
of” v2. Each edge has a weight that expresses the level
of conﬁdence we have on the “link” between two nodes
(the weights are ordinal so their absolute values are not
important). For example, the higher the weight assigned
to e = (v1 → v2), the stronger the available evidence in
support of the conclusion that v1 is the “source of” v2
(edge weights are further discussed below). Also, let t1
and t2 be the timestamp of v1 and v2, respectively. Re-
gardless of any available evidence for a possible edge,
the two nodes may be linked only if t1 ≤ t2.
Heuristics and Edge Weights. To build the graph G
and draw its edges, we leverage the seven features that
we indentiﬁed in Section 2.4. Speciﬁcally, given two
nodes (essentially, two URLs) in the directed graph G
described earlier, an edge e = (v1 → v2) is created if any
of the seven features is satisﬁed. For example, if v1 and
v2 can be related via the “Domain-in-URL”, we draw an
edge between the two nodes. We associate a weight to
each of the seven features; the “stronger” the feature, the
higher its weight. For example, we assign a weight value
we = 7 to the “Location” feature, we = 6 to the “Refer-
rer” feature, and so on, with the “Ad-to-Ad” receiving a
weight we = 1. The weight values are conveniently as-
signed simply to express relative importance and prece-
dence among the edges to be considered by our greedy
algorithm. If more than one feature happens to link two
nodes, the edge will be assigned a weight equal to the
maximum weight among the matching features.
Traceback Algorithm. Once G has been built, we use
a greedy algorithm to construct an approximate “back-
trace path”. We start from the graph node related to the
executable download event, and walk backwards on the
graph by always choosing the next edge with the high-
est weight. Consider the example graph in Figure 3, in
which thicker edges have a higher weight. We start from
the download node d. At every step, we walk one node
backwards following the highest weight edge. We pro-
ceed until we reach a node with no predecessor, which
we mark as the origin of the download path. If a node
has more than one predecessor whose edges have the
same weight, we follow the edge related to the prede-
cessor node with the smaller time gap to the current node
(measured w.r.t. the corresponding HTTP transactions).
1030  24th USENIX Security Symposium 
USENIX Association
6
Malicious Exe 
Detection Oracle
Exe Download
Trafﬁc Windows
Live Web
Trafﬁc
WebWitness
ATC - Attack Path Traceback and Categorization
MDD: Automated Malware Download Defenses
Automated
Download Path
Traceback
Automated
Download Cause
Classiﬁer
 AMP 
Annotated Malware 
Download Paths
Drive-by Download
Automated 
Defenses
Social Eng. Download 
Automated
Defenses
Attack
Countermeasures
Data Analysis
and Labeling
ATC Design
and Training
Drive-by Paths
Social Eng. Paths
Drive-by
Defense Design
and Training
Analyst
Analyst
Figure 2: WebWitness system details.
Social Eng.
Defense Design
and Training
1
2
4
6
7
Backtrace Path
5
8
9
3
d
Figure 3: Example of download path traceback.
Possible False and Missing Edges: Naturally,
the
heuristics we use for tracing back the download path may
in some cases add “false edges” to the graph or miss
some edges. However, notice that these challenges are
mitigated (though not always completely eliminated) by
the following observations:
i) Our algorithm and heuristics aim to solve a much
narrower problem than ﬁnding the correct “link” be-
tween all possible HTTP transactions in a network
trace, because we are only concerned with tracing
back a sequence of HTTP transactions that termi-
nate into a malicious executable download.
ii) The “false edge” problem is mitigated by the fact
that we always follow the strongest evidence. For
example, consider Figure 3. Suppose the edge (2 →
3) was drawn due to rule (6), while edge (5 → 3)
was drawn due to rule (2). In this case, even though
edge (2 → 3) was mistakenly drawn (i.e., nodes 2
and 3 have no real “source of” relationship), the
mistake is irrelevant, because our algorithm will
choose (5 → 3) as part of the path, which is sup-
ported by stronger evidence.
iii) Our algorithm can output not only the sequence of
HTTP transactions, but also the nature (and conﬁ-
dence) of every edge. Therefore, a threat analyst (or
a downstream post processing system) can take the
edge weights into account, before the reconstructed
download path is used to make further decisions
(e.g., remediation or takedown of certain domains
in the download path).
3.2 ATC - Download Cause Classiﬁcation
After we trace back the download path, we aim to la-
bel the reconstructed path as either social engineering or
drive-by download. As shown in Figure 2, the output of
this classiﬁcation step allows us to obtain the annotated
malware download paths (AMPs), which are then pro-
vided as input to the defense module (MDD).
While we are mainly interested in automatically iden-
tifying social engineering and drive-by download paths,
we build a three-class classiﬁer that can distinguish be-
tween three broad download causes, namely social engi-
neering, drive-by, and update/drop. Essentially the up-
date/drop class allows us to more easily identify and ex-
clude malware downloads that are not caused by either
social engineering or drive-by attacks.
To automatically classify the “cause” of an executable
ﬁle download, WebWitness uses a supervised classiﬁca-
tion approach. First, we describe how we derive the fea-
tures needed to translate malware download events into
feature vectors that can be given as input to a statistical
classiﬁer. Then, we discuss how we derive the dataset
used to train the classiﬁer. To actually build the classiﬁer,
we used the random forest algorithm [7] (see Section 4).
Features: To discriminate between the three different
classes, we engineered six statistical features that reﬂect,
with a one-to-one mapping, the six characteristics of
drive-by and social-engineering malware download paths
that we discussed and measured in Section 2.5. For ex-
ample, we measure binary feature (1) “Download Refer-
rer” as true if the HTTP request that initiated the down-
load has a Referer header; a numerical feature (2) rep-
resenting the “age” of domains serving “commonly ex-
ploitable” content; etc.
Training dataset: To train the classiﬁer, we use the
dataset of in-the-wild malware download paths that we
collected and manually labeled during our initial inves-
tigation of in-the-wild malware downloads discussed in
Section 2.5. Our training dataset contained the follow-
ing number of labeled download paths: 164 instances of
drive-by download paths, 191 instances of social engi-
neering paths, and 328 update/drop samples.
USENIX Association  
24th USENIX Security Symposium  1031
7
3.3 MDD - Drive-by Defense
The annotated download paths output by ATC provide a
large and up-to-date dataset of real-world malware down-
load incidents, including the web paths followed by the
victims (see Figure 2). This information is very useful
for studying new attack trends and developing more ef-
fective defenses. As new defenses are developed, they
can be plugged into the MDD module, so that as new
malware download paths are discovered we can automat-
ically derive appropriate countermeasures.
As an example that demonstrates how WebWitness
can enable the development of more effective malware
download defenses, we develop a new defense against
drive-by download attacks based on code injections.
While code injection attacks are not new, current de-
fenses rely mainly on blacklisting the URLs serving the
actual drive-by exploit or malware download, rather than
blocking the URLs from which malicious code is in-
jected. Our results (Section 4) show that by automati-
cally tracing back drive-by download paths and identi-
fying the code injection URLs, we can enable better de-
fenses against future malware attacks.
Identifying code injection URLs: Given a drive-by
download path output by the ATC module, we aim to
automatically identify the landing, injection, and exploit
nodes within the download path.We tackle this problem
using a supervised classiﬁcation approach. Namely, we
train a separate classiﬁer for each of the three types of
nodes on a drive-by download path. The ﬁnal output is a
labeled drive-by download path.
Exploit Page Classiﬁer: The exploit classiﬁer takes as
input a drive-by download path and labels its nodes as
exploit or non-exploit. We deﬁne an exploit node as a
page that carries content that exploits a vulnerability on
the victim’s machine, causing it to eventually download
a malicious executable. The search for exploit nodes pro-
ceed “backwards”, starting from the node prior to the ex-
ecutable download and ending at the root. It is not un-
common to have more than one exploit node in one path
(e.g., some exploit kits try several exploits before suc-
cess). Thus, multiple nodes could be labeled as exploit.
To build the classiﬁer, we use the following features:
(1) Hops to the download page. Number of nodes on the
download path between the considered node and the
ﬁnal malware download node. Intuition: It is typical
for the exploit node to only be a few hops away from
the actual download. In many cases, the node prior to
the download event is an exploit node, because once
the exploit succeeds the executable is downloaded
immediately.
(2) “Commonly exploitable” content. Boolean feature
that indicates if a node contains content for Java, Sil-
verlight, Flash or Adobe Reader. Intuition: Browser
plug-ins are a popular exploitation vector. The ex-
ploit is typically delivered though their content.
(3) Domain age. The number of days since the ﬁrst ob-
servation of the node’s effective second level domain
in a large historic passive DNS database. Intuition:
Exploit domains tend to be short-lived and often only
active for one day.
(4) Same domain. Boolean feature that is true if the
node’s domain is equal to the download domain. In-
tuition: It is common for the exploit and download to
be served by the same domain, as also noted in [13].
Landing Page Classiﬁer: Once the exploit node(s) is la-
beled, we attempt to locate the landing page URL. Essen-