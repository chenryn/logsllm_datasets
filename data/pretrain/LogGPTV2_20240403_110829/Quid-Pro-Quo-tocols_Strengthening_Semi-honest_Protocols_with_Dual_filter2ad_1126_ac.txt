circuit protocol is executed if the result is not disclosed
to the circuit generator. This works in both directions for
our DualEx protocol, so the only concern is how much
information is leaked by the equality test. The output of
the equality test is just a single bit.
In an information theoretic sense, the maximum average
leakage is achieved when the adversary can partition the
victim’s possible private inputs into two subsets which are
equally likely. This assumes the attacker know the a priori
distribution of the victim’s private inputs, and can design the
malicious circuit to produce incorrect results on an arbitrary
subset of inputs. Since the other party learns when the
equality test fails, a malicious adversary can achieve the
maximum expected information gain by designing a circuit
that behaves correctly on the victim’s private inputs half the
time, and incorrectly (that is, it produces an output that will
fail the equality test) on the other inputs.
This extra information does not come free to the adver-
sary, however. If the equality test fails, the victim learns that
the other party misbehaved. So, for the maximum average
leakage circuit which divides the private inputs into two
equally likely subsets the attacker learns one extra bit on
every protocol execution but has a 1/2 probably of getting
caught.
An attacker who does not want to get caught, can (possi-
bly) design a circuit that divides the other parties private
inputs into two subsets where the ﬁrst subset for which
the equality test passes contains 0  1/2 since
then the equality test fails on more than half of the inputs,
Input to P1: h1; decryption key skP1.
Input to P2: h2.
Public inputs: Public key pkP1.
Output to P1: true if h1 = h2; false otherwise.
Output to P2: ⊥.
Execution:
1) P1 sends to P2 α0 =(cid:74)−h1(cid:75).
(e, h) = ((cid:74)r × (h2 − h1) + s(cid:75) , H2(s, h2)), and
2) P2 picks random r, s, computes
sends (e, h) to P1.
3) P1 computes ˆs = Dec(e). If H(ˆs, h1) = h, P1
outputs true; otherwise, it outputs false.
Figure 5. A one-sided equality-testing protocol. For a discussion of the
security guarantees provided, see the text.
277
and the attacker is more likely to get caught than if the
subsets are swapped.) In the extreme, an attacker who only
cares about one particular private input, x∗ could create a
circuit that behaves correctly on all inputs except for x∗.
The attacker would have no chance of getting caught except
when the target input is matched, but would gain very little
information otherwise. This suggests, unsurprisingly, that it
is unwise to execute a secure computation many times since
each execution leaks some information. This is true even for
a maliciously secure protocol since the output itself leaks
information, and, even with a maliciously secure protocol,
an attacker can alter its own inputs to probe the victim’s
private inputs.
The implications of the single-bit leakage depend on the
application, and in some scenarios leaking even a single
bit may be unacceptable. On the other hand, for many
applications this is much less information that an adversary
can infer from the outputs, even if a maliciously secure
protocol is used. Further, our analysis assumes the worst case
where the adversary may implement an arbitrary partitioning
function. It may be possible to take advantage of constraints
in the circuit design to limit the possible partitioning func-
tions that can be implemented, and to combine this with
delayed revelation protocols (see Section VII) to further limit
the actual information an adversary can obtain, although we
have no yet found a principled way to provide meaningful
constraints on the possible partitioning functions.
B. Attacker Strategies
There are several possible strategies a malicious attacker
may use against a DualEx protocol. The attacks may be
grouped into three main types1: selective failure, in which
the attacker constructs a circuit that fails along some exe-
cution paths and attempts to learn about the other party’s
private inputs from the occurrence of failure, false function,
in which the attacker constructs a circuit that implements
function that is different from the agreed upon function, and
inconsistent inputs, in which the attacker provides different
inputs to the two executions. Note that the formal security
proof presented in Section V is independent of any speciﬁc
attack strategy.
Selective failure. In a selective failure attack, a malicious
party engages in the protocol in a way that causes it to
fail for a subset of the other party’s possible inputs. This
could be done by either putting bad entries into a garbled
truth table, or by providing bad input wire labels in the OT
for some values. If the protocol succeeds, in addition to
knowing the secure computation outcome, the attacker also
eliminates some possibilities of the peer’s private inputs. If
the protocol fails, the attacker still learns something about
the peer’s inputs, but the misbehavior will be detected by
the peer.
One characteristic of our garbled circuit implementation is
that the circuit evaluator can always proceed to the normal
1We do not consider side-channel attacks, which are possible if the
protocol implementation is not done carefully, but only focus on protocol-
level attacks here.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:48:10 UTC from IEEE Xplore.  Restrictions apply. 
termination even when some intermediate wire labels are
broken (possibly due to the selective failure attack) since
the evaluator never checks the validity of those wire labels
except for the ﬁnal wires. This is desirable because it
conceals the positions where the fault occurs, constraining
the amount of leakage to merely a single bit on average.
False function. A malicious participant can generate a
circuit that computes some function, g, that is different from
the function f which the other party agreed to compute. The
malicious circuit function g has the same output format as
f, enabling the attacker to learn if g(x, y) = f (x, y), in
addition to knowing the desired output f (x, y). However,
the adversary has to risk being caught if g(x, y) (cid:54)= f (x, y).
This attack is intrinsic to the design of dual execution
protocols. However, the space of g can be limited by the
structure of the circuit, which is already known to the
evlauator. Although the other party cannot determine if
the individual garbled tables perform the correct logical
function, it can verify that (a) the circuit uses a prede-
ﬁned number of gates, (b) the gates are interconnected as
presumed, (c) all XOR gates show up as presumed (since
these are implemented using the free-XOR optimization),
and (d) all non-free gates are positioned correctly. Limiting
the set of functions g that could be implemented by the
adversary, depends on understanding the possible functions
that can be computed with a given circuit structure by
changing the binary operations of non-free gates in the
circuit. Analyzing a given circuit structure for more precise
quantiﬁcation of the leakage can be an interesting future
work.
Inconsistent inputs. The adversary can also provide differ-
ent inputs to the two protocol executions so that the equality
test reveals if f (x, y) = f (x(cid:48), y) (where x (cid:54)= x and are
selected by the adversary) in the secure validation stage. For
example, for any input wire corresponding to Alice’s private
input, Alice as a circuit generator could send to Bob a label
representing 1, whereas as a circuit evaluator uses 0 as her
input to the OT protocol to obtain a wire label representing
0 for evaluation.
These attacks appear to give the malicious adversary a
great deal of power. However, as we prove in the next
section, regardless of the adversary’s strategy, the essential
property of dual execution protocols is that the leakage is
limited to the single bit leaked by the equality test.
V. PROOF OF SECURITY
We give a rigorous proof of security for the DualEx
protocol following the classic paradigm of comparing the
real-world execution of the protocol to an ideal-world exe-
cution where a trusted third party evaluates the function on
behalf of the parties [10]. The key difference is that here we
consider a non-standard ideal world where the adversary is
allowed to learn an additional bit of information about the
honest party’s input.
We remark that, for reasons described throughout
the
text, the proof here does not apply to our implementation
instead refers to the DualEx protocol from
per se, but
in Figure 2, instantiated using the garbled-circuit protocol
from Figure 3, where the equality test in Figure 2 and the
oblivious transfers in Figure 3 are done using protocols that
achieve the standard (simulation-based) notion of security
against malicious adversaries, and the oblivious-transfer sub-
protocols are run sequentially.
A. Deﬁnitions
Preliminaries. We use n to denote the security parameter.
A function µ(·) is negligible if for every positive polynomial
p(·) and all sufﬁciently large n it holds that µ(n) < 1/p(n).
A distribution ensemble X = {X(a, n)}a∈Dn, n∈N is an
inﬁnite sequence of random variables indexed by a ∈ Dn
and n ∈ N, where Dn may depend on n.
Distribution ensembles X = {X(a, n)}a∈Dn, n∈N and
Y = {Y (a, n)}a∈Dn, n∈N are computationally indistinguish-
c≡ Y , if for every non-uniform polynomial-
able, denoted X
time algorithm D there exists a negligible function µ(·) such
that for every n and every a ∈ Dn
(cid:12)(cid:12)(cid:12) Pr[D(X(a, n)) = 1] − Pr[D(Y (a, n)) = 1]
(cid:12)(cid:12)(cid:12) ≤ µ(n).
We consider secure computation of single-output, deter-
ministic functions where the two parties wish to compute
some (deterministic) function f with Alice providing in-
put x, Bob providing input y, and both parties learning the
result f (x, y). We assume f maps two n-bit inputs to an
(cid:96)-bit output.
A two-party protocol for computing a function f is a
protocol running in polynomial time and satisfying the fol-
lowing correctness requirement: if Alice begins by holding
1n and input x, Bob holds 1n and input y, and the parties run
the protocol honestly, then with all but negligible probability
each party outputs f (x, y).
Security of protocols. We consider static corruptions by
malicious adversaries, who may deviate from the protocol
in an arbitrary manner. We deﬁne security via the standard
real/ideal paradigm, with the difference being that we use
a weaker version of the usual
ideal world. Speciﬁcally,
in the standard formulation of the ideal world there is a
trusted entity who receives inputs x and y from the two
parties, respectively, and returns f (x, y) to both parties.
(We ignore for now the issue of fairness.) In contrast, here
we consider an ideal world where a malicious party sends
its input along with an arbitrary boolean function g, and
learns g(x, y) in addition to f (x, y). (The honest party
still learns only f (x, y).) Note that in this weaker ideal
model, correctness and input independence still hold: that
is, the honest party’s output still corresponds to f (x, y) for
some legitimate inputs x and y, and the adversary’s input
is independent of the honest party’s input. Privacy of the
honest party’s input also holds, modulo a single additional
bit that the adversary is allowed to learn.
Execution in the real model. We ﬁrst consider the real
model in which a two-party protocol Π is executed by Alice
278
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:48:10 UTC from IEEE Xplore.  Restrictions apply. 
and Bob (and there is no trusted party). In this case, the
adversary A gets the inputs of the corrupted party and
arbitrary auxiliary input aux and then starts running the
protocol, sending all messages on behalf of the corrupted
party using an arbitrary polynomial-time strategy. The honest
party follows the instructions of Π.
Let Π be a two-party protocol computing f. Let A be
a non-uniform probabilistic polynomial-time machine with
auxiliary input aux. We let VIEWΠ,A(aux)(x, y, n) be the
random variable denoting the entire view of the adversary
following an execution of Π, where Alice holds input x and
1n, and Bob holds input y and 1n. Let OUTΠ,A(aux)(x, y, n)
be the random variable denoting the output of the honest
party after this execution of the protocol. Set
REALΠ,A(aux)(x, y, n) def=
(cid:0)VIEWΠ,A(aux)(x, y, n), OUTΠ,A(aux)(x, y, n)(cid:1) .
Execution in our ideal model. Here we describe the ideal
model where the adversary may obtain one additional bit
of information about the honest party’s input. The parties
are Alice and Bob, and there is an adversary A who
has corrupted one of them. An ideal execution for the
computation of f proceeds as follows:
Inputs: Alice and Bob hold 1n and inputs x and y, respec-
tively; the adversary A receives an auxiliary input aux.
Send inputs to trusted party: The honest party sends its
input to the trusted party. The corrupted party controlled
by A may send any value of its choice. Denote the
pair of inputs sent to the trusted party as (x(cid:48), y(cid:48)). (We
assume that if x(cid:48) or y(cid:48) are invalid then the trusted
party substitutes some default input.) In addition, the
adversary sends an arbitrary boolean function g to the
trusted party.
Trusted party sends output: The trusted party computes
f (x(cid:48), y(cid:48)) and g(x(cid:48), y(cid:48)), and gives both these values to
the adversary. The adversary may at this point tell the
trusted party to stop, in which case the honest party is
given ⊥. Otherwise, the adversary may tell the trusted
party to continue, in which case the honest party is
given f (x(cid:48), y(cid:48)). (As usual for two-party computation
with malicious adversaries, it is impossible to guarantee
complete fairness and we follow the usual convention
of giving up on fairness altogether in the ideal world.)
Outputs: The honest party outputs whatever it was sent by
the trusted party; A outputs an arbitrary function of its
view.
f,A(aux)(x, y, n) (resp., OUThon
We let OUTA
f,A(aux)(x, y, n)) be
the random variable denoting the output of A (resp., the
honest party) following an execution in the ideal model as
described above. Set
IDEALf,A(aux)(x, y, n) def=
(cid:16)
(cid:17)
.
OUTA
f,A(aux)(x, y, n), OUThon
f,A(aux)(x, y, n)
Deﬁnition 1 Let f, Π be as above. Protocol Π is said to
securely compute f with 1-bit leakage if for every non-uni-
form probabilistic polynomial-time adversary A in the real
model, there exists a non-uniform probabilistic polynomial-
279
time adversary S in the ideal model such that
(cid:8)IDEALf,S(aux)(x, y, n)(cid:9)
(cid:8)REALΠ,A(aux)(x, y, n)(cid:9)
x,y,aux∈{0,1}∗
c≡
x,y,aux∈{0,1}∗
Remark. In the proof of security for our protocol, we
consider a slight modiﬁcation of the ideal model described
above: namely, the adversary is allowed to adaptively choose
g after learning f (x(cid:48), y(cid:48)). Although this may appear to be
weaker than the ideal model described above (in that the
adversary is stronger), in fact the models are identical. To
see this, ﬁx some adversary A = (A1,A2) in the “adaptive”
ideal world, where A1 denotes the initial phase of the
adversary (where the adversary decides what input to send
to the trusted party) and A2 denotes the second phase of the
adversary (where, after observing f (x(cid:48), y(cid:48)), the adversary
speciﬁes g). We can construct an adversary A(cid:48) in the “non-
adaptive” ideal world who learns the same information: A(cid:48)
runs A1 to determine what input to send, and also submits
a boolean function g(cid:48) deﬁned as follows: g(cid:48)(x(cid:48), y(cid:48)) runs