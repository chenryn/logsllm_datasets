Relative Event Importance. Any given high-level task which
a user performs consists of multiple smaller operations, but the
importance or necessity of each operation may not be the same.
While completing the desired task, users are typically required
to execute boilerplate operations. Consider the compilation of a
program in Figure 4. Users usually do not directly launch gcc
to compile source code but ﬁrst locate the source ﬁle using
common utilities like ls and dir. Although such boilerplate
operations reﬂect user activities, they do not uniquely associate
with high-level behavior. Therefore, concerning the task of be-
havior abstraction, these boilerplate operations should be given
less attention than operations directly representing behavior.
The key question is, how can we automatically ascribe
the relative importance of each operation? Our insight stems
from how operations are described in audit events of user
sessions. We observe that behavior-unrelated events are more
prevalent
in sessions as they are repeated across different
behaviors, whereas actual behavior-related events happen less
frequently. Based on this observation, we propose using the
frequency of events as a metric to deﬁne their importance.
More speciﬁcally, we employ the Inverse Document Frequency
(IDF) to determine the importance of a particular event to the
overall behavior. IDF is widely used in information retrieval as
a term weighting technique [59]. Its principle is to give more
discriminative capacity to less-common terms in documents.
In our particular scenario, each audit event and user session
are viewed as a term and document. The equation to measure
the IDF value is as follows:
(cid:18) S
(cid:19)
,
Se
wIDF (e) = log
where e denotes an audit event, and S and Se are the numbers
of all the sessions and speciﬁc sessions that include event e.
To summarize, each event in a behavior partition is assigned a
weight using IDF, representing its importance to the behavior.
Noisy Events. The low-level and verbose nature of audit
logs makes the presence of noisy events one of the primary
challenges analysts struggle with. Reducing noise occurrence
helps to improve WATSON’s effectiveness. In this section, we
will discuss two types of noisy events, redundant events and
mundane events.
(1) Redundant events. In behavior instances,
there are
events when removed that do not change the data transfers.
To identify these redundant events, we built on top of the
shadow event [82], a concept that refers to ﬁle operations
whose causalities have already been represented by other
key events. We also incorporate domain knowledge in [50],
[82] to enhance redundant event reduction. At a high level,
analysts have listed speciﬁc ﬁles that do not introduce explicit
information ﬂows in causality analysis. For example, many
processes create temporary ﬁles to store intermediate results
during execution. Because such ﬁles exclusively interact with
a single process during their lifetime,
they do not affect
data transfers nor contribute to behavior abstraction. Note
that /tmp/1 and /tmp/2 in Figure 4 serving as IPC are
not categorized as temporary ﬁles. All redundant events are
considered noise and removed from behavior partitions.
(2) Mundane events. Another source of noise comes from
ﬁle operations that are regularly performed for an action.
We call them mundane events. Examples of such events are
(vim, write, .viminfo) for ﬁle editing history cache
and (bash, read, /etc/profile) for shell program
setup. We classify mundane events as noisy events because
they are associated with system routines rather than speciﬁc
behaviors. Typically, mundane events have two characteristics,
(a) they always occur for a given action, and (b) the order of
their occurrence is ﬁxed. In order to identify and ﬁlter them,
we ﬁrst enumerate all possible actions a program can perform.
By one action, we refer to a sequence of events a program
generates during its execution lifecycle in the system. Then,
given sequences of events for each program, we summarize
events always occurring in a ﬁxed pattern as mundane events.
Essentially, we formulate the mundane event
identiﬁcation
problem as the longest common subsequence (LCS) searching
problem. Given event sequences of a program, we extract the
LCS among them as the mundane events. Similar to (1), all
mundane events are removed from behavior partitions. We
note that NODEMERGE [75] ﬁrst proposes to identify mundane
events (i.e., data access patterns) for data reduction. However,
it focuses on ﬁle reading operations (e.g., load libraries and
retrieve conﬁgurations) in the process initialization action. In
contrast, WATSON targets all types of ﬁle operations in more
general actions (e.g., vim creates and writes .viminfo to
cache where users leave off editing ﬁles).
After weighting events with IDF and removing noisy
events, we derive the semantic representation of behavior
instances by pooling its constituent vectorized events. We have
attempted different pooling approaches for implementation,
such as addition, bi-interaction, and global average pooling.
The addition pooling is eventually utilized as we observe that
simply summing the semantics of events has already integrated
the semantic information of events effectively.
In conclusion, the Behavior Abstraction phase takes a log-
based KG as input and generates vector representation of
behavior instances in a 3m-dimensional embedding space.
F. Behavior Clustering
As described in Section II-C, behavior instances are vari-
ations on how high-level behaviors can be realized. In other
words, a behavior can be thought of as one cluster of similar
instances. It naturally follows that the behavior signature is the
most representative instance (e.g., centroid) in the cluster. In
this way, analysts only need to investigate a few auto-selected
signatures for behavior matching rather than the whole cluster
space. Given the vector representation of behavior instances,
WATSON calculates their semantic relationships using cosine
similarity as follows:
P
qP
P
(ei)2 ×qP
ej∈Fn
ei∈Fm
ei∈Fm
ei · ej
ej∈Fn
,
(ej)2
S (Fm, Fn) =
Fm · Fn
kFmk × kFnk =
where Fm and Fn refer to the vector representation of two
behavior instances, and S(Fm, Fn) representing the cosine
similarity score is positively correlated with behavior semantic
similarity. This equation intuitively explains the effectiveness
of using addition to pool embeddings of events in a behavior
instance. Since Fi and Fj are the summations of their respec-
tive constituent events, cosine similarity, in effect, compares
the similarities of individual events in two instances.
WATSON uses Agglomerative Hierarchical Clustering
Analysis (HCA) algorithm to cluster similar behavior in-
stances. Initially, each behavior instance belongs to its own
7
cluster. HCA then iteratively calculates cosine similarities
between clusters and combines two closest clusters until the
maximum similarity is below the merge threshold (0.85 in
our case). We select Centroid Linkage as the criterion to
determine the similarity between clusters. In other words,
cluster similarity estimation depends on centroids (arithmetic
mean position) in clusters.
Once the clusters are identiﬁed, a behavior signature for
each cluster would be extracted based on the instances’ rep-
resentativeness. WATSON quantiﬁes the representativeness of
each instance in a cluster by computing its average similarity
with the rest of instances. The instance with the maximum
similarity is picked out as the signature. By distinguishing the
behavior signature, we expect to see a substantial analysis
workload reduction as semantically similar behaviors have
been clustered before human inspection. Take for instance
our motivating example. WATSON groups multiple Program
Compilation and Upload instances together into one cluster and
only reports the representative for analyst investigation. It is
noteworthy that the Data Exﬁltration behavior fails to compile
the illegitimate source code and thus loses the data transfers
from the source ﬁle to an executable ﬁle. As a result, it is not
clustered together with the Program Compilation behavior even
though they use identical utilities (gcc to compile programs
and git to upload ﬁles).
In summary,
the Representative Behavior Identiﬁcation
phase clusters similar behavior instances and distinguishes
representative behaviors as signatures.
IV.
IMPLEMENTATION
We prototype WATSON in 9.2K lines of C++ code and 1.5K
lines of Python code. In this section, we discuss important
technical details in the implementation.
Log Input Interface. WATSON takes system audit data as
inputs. We deﬁne a common interface for audit
logs and
build input drivers to support different log formats, such as
Linux Audit [9] formats (auditd [8], auditbeat [7], and DARPA
dataset2). Our drivers can be extended to support other audit
sources, i.e., CamFlow [68] and LPM [17] for Linux, ETW
for Windows, and Dtrace for FreeBSD.
Modular Design. Modularity is one of our guiding principles
in WATSON design. Each module can be freely swapped out
for more effective solutions or application-speciﬁc trade-offs in
terms of performance vs. accuracy. Take the Event Semantics
Inference module for example. In our implementation, TransE
is used to learn the embedding space of audit events for its
memory and time efﬁciency despite the limitation on the types
of relations it can encode [81]. If WATSON users wish, TransE
can be easily replaced with other embedding algorithms (e.g.,
TransR [51]) without affecting WATSON’s functionality.
Knowledge Graph Construction. To construct a log-based
KG, WATSON ﬁrst sorts audit events in chronological order.
Then, it translates each event into a KG-based triple by using
the system entities as the Head and Tail, and the system
call function as the Relation. To interpret rules for triple
translation, we manually analyze 32 commonly-used system
2To achieve platform independence, audit logs in DARPA datasets are
represented in a Common Data Model (CDM) format [43].
calls, including (1) process operations such as clone, execute,
kill, and pipe; (2) ﬁle operations such as read, write, rename,
and unlink; (3) socket operations such as socket, connect, send,
and receive. After parsing audit events into triples, a relational
database (PostgreSQL [11]) is used to store the built KG.
Note that we compute the 64-bit hash value of system entity
identiﬁers deﬁned in Section III-C as the primary key in the
database. Besides, all the properties of system entities (e.g.,
process name) and relations (e.g., timestamps) are preserved
as attributes of elements in triples.
Parameter Settings. We implement the embedding model in
the Event Semantics Inference module with Google Tensor-
ﬂow [15]. The model is optimized with SGD optimizer, where
the margin, batch size, and epochs are ﬁxed at 1, 1024, and
500, respectively. In terms of hyperparameter, we apply a grid
search: the learning rate and embedding size are tuned amongst
{0.005, 0.010, 0.015} and {32, 64, 128, 256}. Similarly, for
behavior clustering, the merge threshold is searched in {0.7,
0.75, 0.80, 0.85, 0.9}. In light of the best F1 score in our
experiments, we show the results in a setting with learning rate
as 0.010, embedding size as 64, and merge threshold as 0.85.
Note that the merge threshold is a conﬁgurable parameter, and
analysts can customize it according to particular scenarios. For
example, the threshold can be decreased to satisfy high true
positive or increased to maintain low false positive.
Behavior Database. We observe that behaviors are recurrent in
the system. New sessions always include behaviors that appear
previously. Therefore, to avoid repetitive behavior analysis,
we label WATSON-generated behavior signatures with domain-
speciﬁc descriptions and store them in our database. The
embedding of each behavior quantifying semantics is preserved
as an attribute for behavior objects. Once a new behavior
instance appears, WATSON ﬁrst computes its cosine similar-
ities with all stored signatures. If no similarity is above the
merge threshold, analysts manually investigate its semantics;
Otherwise, its semantics is retrieved by querying the similar
behavior signature in the database.
V. EVALUATION
In this section, we employ four datasets and experimentally
evaluate four aspects of WATSON: 1) the explicability of in-
ferred event semantics; 2) the accuracy of behavior abstraction;
3) the overall experience and manual workload reduction in
attack investigation; and 4) the performance overhead.
A. Experimental Dataset
We evaluate WATSON on four datasets: a benign dataset,
a malicious dataset, a background dataset, and the DARPA
TRACE dataset. The ﬁrst three datasets are collected from ssh
sessions on ﬁve enterprise servers running Ubuntu 16.04 (64-
bit). The last dataset is collected on a network of hosts running
Ubuntu 14.04 (64-bit). The audit log source is Linux Audit [9].
In the benign dataset, four users independently complete
seven daily tasks, as described in Table I. Each user performs
a task 150 times in 150 sessions. In total, we collect 17
(expected to be 4×7 = 28) classes of benign behaviors because
different users may conduct the same operations to accomplish
tasks. Note that there are user-speciﬁc artifacts, like launched
commands, between each time the task is performed. For our
8
TABLE I: Overview of tasks with scenario descriptions. Column 3 shows the ground-truth behaviors when completing the tasks.
Task
Scenario Description
U1
Program Submission
Code Reference
Dataset Download
Upload machine learning programs to a GPU server
Download and compile online programs for reference
Download and uncompress public datasets
Program Compilation Write a C/C++ program and testify its functionalities
FTP Server Login
Package Installation
Package Deletion
Use ssh or ftp services to sign in a FTP server
Run apt application to install software packages
Run apt application to remove software packages
vim + scp
wget + gcc
wget + gzip
vim + gcc
ftp
apt update/install
apt update/install
apt update/install
apt update/install
apt purge
apt purge
apt purge
apt purge
U2
vi + scp
Behavior
U3
emacs+ scp
elinks + gcc
wget + bzip
vim + g++
ssh
wget + python
elinks + unzip
vim + gcc
ftp
U4
nano + scp
elinks + python
wget + bzip
vim + gcc
ftp
TABLE II: Overview of attack cases in our malicious dataset with scenario descriptions.
Attack Cases
Data Theft
Illegal Storage
Content Destruction
Backdoor Installation
Passwd-gzip-scp
Wget-gcc
Conﬁguration Leakage
Passwd Reuse
Scenario Description
A malicious script is mistakenly downloaded by a normal user and when being executed it exﬁltrates
sensitive information on the local machines
An attacker creates a directory in another user’s home directory and uses it to store illegal ﬁles
An insider tampers with classiﬁed programs and documents
An attacker compromises a FTP server, invokes a remote bash shell, and installs a backdoor program to