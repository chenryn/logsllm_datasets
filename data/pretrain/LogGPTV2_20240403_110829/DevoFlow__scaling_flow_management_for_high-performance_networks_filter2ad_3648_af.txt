dle this load (which would require up to 98 NOX controllers,
assuming they can be perfectly distributed and that statis-
tics are pulled every 100 ms), but it might be diﬃcult to
coordinate so many controllers.
Figure 7 shows the number of ﬂow table entries at any
given access switch, for the MSR workload and various
schedulers. For these simulations, we timed out the table
entries after 10 sec. As expected, DevoFlow does not require
many table entries, since it uses a single wildcard rule for all
mice ﬂows, and stores only exact-match entries for elephant
ﬂows. This does, however, assume support for the multipath
routing wildcard rules of DevoFlow. If rule cloning were used
instead, DevoFlow would use the same number of table en-
tries as the pull-based OpenFlow scheduler because it would
clone a rule for each ﬂow. The pull-based scheduler uses an
order of magnitude more table entries, on average, than De-
voFlow.
We estimated the amount bandwidth required between
a switch’s data-plane and control-plane when statistics are
Figure 8: The control-plane bandwidth needed to pull statistics at
various rates so that ﬂow setup latency is less than 2ms in the
95th and 99th percentiles. Error bars are too small to be seen.
collected with a pull-based mechanism. Figure 8 shows the
bandwidth needed so that the 95th and 99th percentile ﬂow
setup latencies on the MSR workload are less than 2ms.
Here, we assume that the only latency incurred is in the
queue between the switch’s data-plane and control-plane;
we ignore any latency added by communication with the
controller. That is, the ﬁgure shows the service rate needed
for this queue, in order to maintain a waiting time of less
than 2 ms in the 95th and 99th percentiles. The data to
control-plane bandwidth suﬃcient for ﬂow setup is directly
proportional to this deadline, so a tighter deadline of 1 ms
needs twice as much bandwidth to meet.
The scale on the right of the chart normalizes the re-
quired data-to-control-plane bandwidth to a switch’s total
forwarding rate (which in our case is 28 Gbps, because each
ToR switch has 28 gigabit ports). For ﬁne-grained (100 ms)
ﬂow management using OpenFlow, this bandwidth require-
ment would be up to 0.7% of its total forwarding rate. As-
suming that the amount of control-plane bandwidth needed
scales with the forwarding rate, a 144-port 10 Gbps switch
needs just over 10 Gbps of control-plane bandwidth to sup-
port ﬁne-grained ﬂow management. We do not believe it is
cost-eﬀective to provide so much bandwidth, so DevoFlow’s
statistics-collection mechanisms are the better option be-
cause they are handled entirely within the data-plane.
6. CONCLUSIONS
Flow-based networking frameworks such as OpenFlow
hold great promise—they separate policy speciﬁcation from
its realization, and therefore enable innovative network man-
agement solutions. However, we have shown that Open-
Flow’s current design does not meet the demands of high-
performance networks. In particular, OpenFlow involves the
controller in the handling of too many microﬂows, which cre-
ates excessive load on the controller and switches.
Our DevoFlow proposal allows operators to target only
the ﬂows that matter for their management problem. De-
voFlow reduces the switch-internal communication between
control- and data-planes by (a) reducing the need to trans-
fer statistics for boring ﬂows, and (b) potentially reducing
the need to invoke the control-plane for most ﬂow setups.
It therefore reduces both the intrinsic and implementation
overheads of ﬂow-based networking, by reducing load on
the network, the switch control-plane, and the central con-
troller. DevoFlow handles most microﬂows in the data-plane,
and therefore allows us to make the most out of switch re-
sources. Our evaluation shows that DevoFlow performs as
504 483 446 29,451 7,758 4,871 7,123 709 71 432 181 74 0 5000 10000 15000 20000 25000 30000 0.1s 1s 10s 0.1s 1s 10s 1/100 1/1000 1/10000 128KB 1MB 10MB Wildcard Pull-based Sampling Threshold No. packets / sec. to controller MSR, 25% inter-rack MSR, 75% inter-rack DevoFlow schedulersOpenFlow schedulers0 200 400 600 800 1000 1200 1400 1600 1800 0.1s 1s 10s 0.1s 1s 10s 1/100 1/1000 1/10000 128KB 1MB 10MB Wildcard Pull-based Sampling Threshold No. ow table entries Avg - MSR, 25% inter-rack Max - MSR, 25% inter-rack Avg - MSR, 75% inter-rack Max - MSR, 75% inter-rack DevoFlow schedulersOpenFlow schedulers0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 50 100 150 200 250 0.1 0.5 1 10 30 Never Percent of forwarding bandwidth needed for control plane bandwidth Control plane throughput needed (Mbps)  Stat-pulling rate 95th percentile 99th percentile well as ﬁne-grained ﬂow management when load balancing
traﬃc in the data center. Beyond this use case, we believe
that DevoFlow can simplify the design of high-performance
OpenFlow switches and enable scalable management archi-
tectures to be built on OpenFlow for data center QoS, mul-
ticast, routing-as-a-service [14], network virtualization [43],
and energy-aware routing [29].
Acknowledgments
We would like to thank Joe Curcio, Charles Clark, Paul Con-
gdon, Mark Gooch, and many others from HP Networking
for helping us to understand how real switches are designed.
Our shepherd, Paul Barford, and the anonymous reviewers
gave us helpful advice on comments for the ﬁnal version
of this paper. S. Keshav, Alex L´opez-Ortiz, and Earl Oliver
gave us beneﬁcial feedback on earlier drafts. We thank Brent
Stephens for pointing out an error in our description of Al-
gorithm 1 in a previous version of this paper.
7. REFERENCES
[1] HP ProCurve 5400 zl switch series.
http://h17007.www1.hp.com/us/en/products/switches/HP_E5400_
zl_Switch_Series/index.aspx.
[2] OpenFlow Switch Speciﬁcation, Version 1.0.0. http://www.
openflowswitch.org/documents/openflow-spec-v1.0.0.pdf.
[3] D. Abts, M. R. Marty, P. M. Wells, P. Klausler, and H. Liu.
Energy proportional datacenter networks. In ISCA, 2010.
[4] J. H. Ahn, N. Binkert, A. Davis, M. McLaren, and R. S.
Schreiber. HyperX: topology, routing, and packaging of eﬃcient
large-scale networks. In Proc. Supercomputing, 2009.
[5] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N. Huang, and
A. Vahdat. Hedera: Dynamic Flow Scheduling for Data Center
Networks. In Proc. NSDI, Apr. 2010.
[6] M. Alizadeh, A. Greenberg, D. A. Maltz, J. Padhye, P. Patel,
B. Prabhakar, S. Sengupta, and M. Sridharan. DCTCP:
Eﬃcient packet transport for the commoditized data center. In
SIGCOMM, 2010.
[7] G. Ananthanarayanan and R. H. Katz. Greening the switch. In
USENIX Workshop on Power Aware Computing and
Systems, (HotPower 2008), 2008.
[8] L. A. Barroso and U. H¨olzle. The case for energy-proportional
computing. Computer, 40(12):33–37, 2007.
[9] T. Benson, A. Akella, and D. Maltz. Network traﬃc
characteristics of data centers in the wild. In Proc. IMC, 2010.
[10] M. Caesar, D. Caldwell, N. Feamster, J. Rexford, A. Shaikh,
and J. van der Merwe. Design and implementation of a routing
control platform. In NSDI, 2005.
[11] Z. Cai, A. L. Cox, and T. S. E. Ng. Maestro: A System for
Scalable OpenFlow Control. Tech. Rep. TR10-08, Rice
University, 2010.
[12] M. Casado, M. J. Freedman, J. Pettit, J. Luo, N. McKeown,
and S. Shenker. Ethane: taking control of the enterprise. In
SIGCOMM, pages 1–12, Aug. 2007.
[13] M. Casado, T. Koponen, D. Moon, and S. Shenker. Rethinking
Packet Forwarding Hardware. In Proc. HotNets, Oct. 2008.
[14] C.-C. Chen, L. Yuan, A. Greenberg, C.-N. Chuah, and
P. Mohapatra. Routing-as-a-service (RaaS): A framework for
tenant-directed route control in data center. In INFOCOM,
2011.
[15] C. Clos. A study of non-blocking switching networks. Bell
System Technical Journal, 32(5):406–424, 1953.
[16] J. R. Correa and M. X. Goemans. Improved bounds on
nonblocking 3-stage clos networks. SIAM J. Comput.,
37(3):870–894, 2007.
[17] A. R. Curtis, W. Kim, and P. Yalagandula. Mahout:
Low-overhead datacenter traﬃc management using
end-host-based elephant detection. In INFOCOM, 2011.
[18] M. Dobrescu, N. Egi, K. Argyraki, B.-G. Chun, K. Fall,
G. Iannaccone, A. Knies, M. Manesh, and S. Ratnasamy.
RouteBricks: exploiting parallelism to scale software routers. In
Proc. SOSP, pages 15–28, 2009.
[19] N. G. Duﬃeld, P. Goyal, A. Greenberg, P. Mishra, K. K.
Ramakrishnan, and J. E. van der Merive. A ﬂexible model for
resource management in virtual private networks. In
SIGCOMM, 1999.
[20] T. Erlebach and M. R¨uegg. Optimal bandwidth reservation in
hose-model VPNs with multi-path routing. In IEEE
INFOCOM, 2004.
[21] C. Estan and G. Varghese. New directions in traﬃc
measurement and accounting. In SIGCOMM, 2002.
[22] P. B. Gibbons and Y. Matias. New sampling-based summary
statistics for improving approximate query answers. In
SIGMOD, 1998.
[23] L. Golab, D. DeHaan, E. D. Demaine, A. Lopez-Ortiz, and J. I.
Munro. Identifying frequent items in sliding windows over
on-line packet streams. In IMC, 2003.
[24] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C. K. P.
Lahiri, D. Maltz, P. Patel, and S. Sengupta. VL2: a scalable
and ﬂexible data center network. In SIGCOMM, 2009.
[25] A. Greenberg et al.. A clean slate 4D approach to network
control and management. SIGCOMM CCR, 35:41–54, 2005.
[26] N. Gude, T. Koponen, J. Pettit, B. Pfaﬀ, M. Casado,
N. McKeown, and S. Shenker. NOX: Towards an Operating
System for Networks. In SIGCOMM CCR, July 2008.
[27] M. Gupta, S. Grover, and S. Singh. A feasibility study for
power management in LAN switches. In ICNP, 2004.
[28] M. Gupta and S. Singh. Using low-power modes for energy
conservation in ethernet LANs. In INFOCOM
Mini-Conference, 2007.
[29] B. Heller, S. Seetharaman, P. Mahadevan, Y. Yiakoumis,
P. Sharma, S. Banerjee, and N. McKeown. ElasticTree: saving
energy in data center networks. In NSDI, 2010.
[30] S. Kandula, S. Sengupta, A. Greenberg, and P. Patel. The
Nature of Datacenter Traﬃc: Measurements & Analysis. In
Proc. IMC, 2009.
[31] W. Kim, P. Sharma, J. Lee, S. Banerjee, J. Tourrilhes, S.-J.
Lee, and P. Yalagandula. Automated and Scalable QoS Control
for Network Convergence. In Proc. INM/WREN, 2010.
[32] M. Kodialam, T. V. Lakshman, and S. Sengupta. Maximum
throughput routing of traﬃc in the hose model. In Infocom,
2006.
[33] T. Koponen, M. Casado, N. Gude, J. Stribling, L. Poutievski,
M. Zhu, R. Ramanathan, Y. Iwata, H. Inoue, T. Hama, and
S. Shenker. Onix: a distributed control platform for large-scale
production networks. In OSDI, 2010.
[34] P. Mahadevan, P. Sharma, S. Banerjee, and P. Ranganathan.
Energy aware network operations. In Proc. 12th IEEE Global
Internet Symp., 2009.
[35] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar,
L. Peterson, J. Rexford, S. Shenker, and J. Turner. OpenFlow:
enabling innovation in campus networks. SIGCOMM CCR,
38(2):69–74, 2008.
[36] J. C. Mogul, J. Tourrilhes, P. Yalagandula, P. Sharma, A. R.
Curtis, and S. Banerjee. Devoﬂow: Cost-eﬀective ﬂow
management for high performance enterprise networks. In
HotNets, 2010.
[37] N. Mohan and M. Sachdev. Low-Leakage Storage Cells for
Ternary Content Addressable Memories. IEEE Trans. VLSI
Sys., 17(5):604 –612, may 2009.
[38] T. Mori, M. Uchida, R. Kawahara, J. Pan, and S. Goto.
Identifying elephant ﬂows through periodically sampled
packets. In Proc. IMC, pages 115–120, Taormina, Oct. 2004.
[39] A. K. Nayak, A. Reimers, N. Feamster, and R. Clark.
Resonance: Dynamic Access Control for Enterprise Networks.
In Proc. WREN, pages 11–18, Aug. 2009.
[40] S. Nedevschi, L. Popa, G. Iannaccone, S. Ratnasamy, and
D. Wetherall. Reducing network energy consumption via
sleeping and rate-adaptation. In NSDI, 2008.
[41] C. Raiciu, C. Pluntke, S. Barre, A. Greenhalgh, D. Wischik,
and M. Handley. Data center networking with multipath TCP.
In HotNets, 2010.
[42] sFlow. http://sflow.org/about/index.php.
[43] R. Sherwood, G. Gibb, K.-K. Yap, G. Appenzeller, M. Casado,
N. McKeown, and G. Parulkar. Can the production network be
the testbed? In OSDI, 2010.
[44] A. S.-W. Tam, K. Xi, and H. J. Chao. Use of Devolved
Controllers in Data Center Networks. In INFOCOM Workshop
on Cloud Computing, 2011.
[45] A. Tavakoli, M. Casado, T. Koponen, and S. Shenker. Applying
NOX to the Datacenter. In HotNets, 2009.
[46] K. Thompson, G. Miller, and R. Wilder. Wide-Area Internet
Traﬃc Patterns and Characteristics. IEEE Network,
11(6):10–23, Nov. 1997.
[47] A. Tootoonchian and Y. Ganjali. HyperFlow: A Distributed
Control Plane for OpenFlow. In Proc. INM/WREN, San Jose,
CA, Apr. 2010.
[48] L. G. Valiant and G. J. Brebner. Universal schemes for parallel
communication. In STOC, 1981.
[49] R. Wang, D. Butnariu, and J. Rexford. Openﬂow-based server
load balancing gone wild. In Hot-ICE, 2011.
[50] C. Westphal. Personal communication, 2011.
[51] M. Yu, J. Rexford, M. J. Freedman, and J. Wang. Scalable
Flow-Based Networking with DIFANE. In Proc. SIGCOMM,
2010.