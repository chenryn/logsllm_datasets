### Handling the Load with NOX Controllers

Managing this load would require up to 98 NOX controllers, assuming they can be evenly distributed and that statistics are collected every 100 ms. Coordinating such a large number of controllers, however, could be challenging.

### Flow Table Entries in Access Switches

Figure 7 illustrates the number of flow table entries at any given access switch for the MSR workload and various schedulers. In these simulations, we set the timeout for table entries to 10 seconds. As expected, DevoFlow requires fewer table entries because it uses a single wildcard rule for all mice flows and stores only exact-match entries for elephant flows. This assumes support for the multipath routing wildcard rules in DevoFlow. If rule cloning were used instead, DevoFlow would use the same number of table entries as the pull-based OpenFlow scheduler, as it would clone a rule for each flow. The pull-based scheduler, on average, uses an order of magnitude more table entries than DevoFlow.

### Bandwidth Requirements for Control-Plane Communication

We estimated the bandwidth required between a switch’s data-plane and control-plane when statistics are collected using a pull-based mechanism. Figure 8 shows the bandwidth needed to ensure that the 95th and 99th percentile flow setup latencies for the MSR workload are less than 2 ms. We assume that the only latency incurred is in the queue between the switch’s data-plane and control-plane, ignoring any latency added by communication with the controller. The figure indicates the service rate needed for this queue to maintain a waiting time of less than 2 ms in the 95th and 99th percentiles. The data-to-control-plane bandwidth required for flow setup is directly proportional to this deadline; thus, a tighter deadline of 1 ms would require twice as much bandwidth.

The scale on the right of the chart normalizes the required data-to-control-plane bandwidth to a switch’s total forwarding rate (which, in our case, is 28 Gbps, as each ToR switch has 28 gigabit ports). For fine-grained (100 ms) flow management using OpenFlow, this bandwidth requirement would be up to 0.7% of its total forwarding rate. Assuming that the amount of control-plane bandwidth needed scales with the forwarding rate, a 144-port 10 Gbps switch would need just over 10 Gbps of control-plane bandwidth to support fine-grained flow management. Providing such a high bandwidth is not cost-effective, making DevoFlow's statistics-collection mechanisms, which are handled entirely within the data-plane, a better option.

### Conclusions

Flow-based networking frameworks like OpenFlow hold great promise by separating policy specification from its implementation, enabling innovative network management solutions. However, our analysis shows that the current design of OpenFlow does not meet the demands of high-performance networks. Specifically, OpenFlow involves the controller in handling too many microflows, creating excessive load on both the controller and switches.

Our DevoFlow proposal allows operators to target only the flows that matter for their management problem. DevoFlow reduces the switch-internal communication between the control-plane and data-plane by:
1. Reducing the need to transfer statistics for unimportant flows.
2. Potentially reducing the need to invoke the control-plane for most flow setups.

This reduces both the intrinsic and implementation overheads of flow-based networking by decreasing the load on the network, the switch control-plane, and the central controller. DevoFlow handles most microflows in the data-plane, allowing for optimal use of switch resources. Our evaluation demonstrates that DevoFlow performs as well as fine-grained flow management when load balancing traffic in the data center. Beyond this use case, we believe DevoFlow can simplify the design of high-performance OpenFlow switches and enable scalable management architectures for data center QoS, multicast, routing-as-a-service [14], network virtualization [43], and energy-aware routing [29].

### Acknowledgments

We would like to thank Joe Curcio, Charles Clark, Paul Congdon, Mark Gooch, and many others from HP Networking for helping us understand how real switches are designed. Our shepherd, Paul Barford, and the anonymous reviewers provided valuable feedback on the final version of this paper. S. Keshav, Alex L´opez-Ortiz, and Earl Oliver gave us beneficial feedback on earlier drafts. We also thank Brent Stephens for pointing out an error in our description of Algorithm 1 in a previous version of this paper.

### References

[References listed as in the original text]

This optimized version aims to improve clarity, coherence, and professionalism while maintaining the technical details and context of the original text.