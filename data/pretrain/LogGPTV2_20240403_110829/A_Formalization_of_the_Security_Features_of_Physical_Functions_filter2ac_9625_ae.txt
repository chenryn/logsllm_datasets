Pr[HW(∆y ⊕ e) ≤ 59 : HW(∆y) = i]
· Pr[HW(∆y) = i : HW(∆y) ≤ 50]
Fbino
59; 255,
i=0
i
255
− 2 ·
i
255
· pe + pe
(cid:19)
·
fbino(i; 255, py)
Fbino(50; 255, py)
= 0.11 ≥ 0.11 · ρavg
PFS
The last equality uses the fact that the considered Hamming
weights are binomially distributed and evaluates the bit
probability of the exclusive-or sum of two independent
random bit vectors.
It follows that the considered PF infrastructure FαCR of
this example, which is based on an SRAM PUF and a fuzzy
extractor, is (2.66 · 10−12, 0.11, 2)-cloning resistant against
an honest manufacturer. In practice, this means that with
probability 2.66 · 10−12, a manufacturer produces two PF
systems that generate the same output on the same challenge
with probability 0.11. Other values for (γ, δ) and q = 2 can
be obtained by considering different bounds for ∆y. Smaller
bounds on ∆y will result in increasingly larger chances of
producing the same output but at much smaller probability to
407
DIFFERENT LEVELS OF (γ, δ, q = 2)-CLONING-RESISTANCE FOR THE
EXAMPLE PF INFRASTRUCTURE DISCUSSED IN SECTION V-C.
Table I
max ∆y
0
10
20
30
40
50
100
110
120
255
γ
2.68 · 10−57
1.32 · 10−41
2.39 · 10−31
1.75 · 10−23
2.96 · 10−17
2.66 · 10−12
0.43
0.86
0.99
1.00
δ
1.00
0.9998
0.986
0.83
0.43
0.11
3.01 · 10−7
1.49 · 10−7
1.29 · 10−7
1.28 · 10−7
create such a PUF. At the other end of the spectrum are pairs
of PF systems, which are very likely to be constructed but
very unlikely to produce the same output. This is illustrated
in Table I, which shows a few other unclonability levels of
the considered PF systems based on SRAM PUFs.
For other PUF types, a comparable quantitative analysis
of existential unclonability can be made based on statisti-
cal data and estimated distributions of PF responses. The
assumed distribution of responses and noise will often be
normal rather than uniform, as it is the case for SRAM
PUFs. Another issue could be that responses to different
challenges and/or on different physical functions are not
independent. Due to their construction, it is reasonable to
assume response independence for SRAM PUFs but
in
general this is not the case for other PUF types. In such
cases an additional post-processing step is required in the
extraction algorithm that ampliﬁes the randomness in the
PFS output and removes dependencies between different
PF instantiations and responses. This step is called privacy
ampliﬁcation and is implemented by means of an adequate
compressing (e.g., a hash) function. Note that a typical
fuzzy extractor implementation already includes a privacy
ampliﬁcation step [34]. We stress that the results obtained for
unclonability are almost always based on estimated distribu-
tion parameters. Hence, a statistical analysis of the accuracy
of these results is preferred. Based on such an analysis,
adequate safety margins should be taken into account when
assessing the security of a PF system.
Discussing unclonability against malicious manufacturers
and adversaries in general is often very difﬁcult in practice.
The reason is that, in practice it is often not possible to take
the effort and implications of all possible technical capabil-
ities of such adversaries into account. For such cases, we
suggest a more ad-hoc approach that measures the cloning-
resistance of a PF system against a malicious adversary by
the efﬁciency of the best known cloning attack that can
be performed by that adversary. This means that the cost
and effort of the adversary need to be considered as an
additional parameter of cloning resistance. This approach
is very similar to the cryptanalysis approach of standard
408
cryptographic primitives such as block ciphers and hash
functions, where the security is measured based on the com-
putational effort to perform the best known attack possible
against the primitive.
VI. UNPREDICTABILITY
A. Rationale
One common application of PUFs is to use them to
securely generate secret values (e.g., cryptographic keys).
Examples include secure key storage [7], [8], [36] or
hardware-entangled cryptography [9]. Such applications im-
plicitly require that the adversary cannot predict the output
of a PF system. Moreover, for typical PUF-based challenge-
response identiﬁcation protocols, e.g., as presented by
Gassend et al. [37], it is important that the adversary cannot
predict the response to a new challenge from previously
observed challenge-response pairs. Therefore, the notion of
unpredictability is an important property that needs to be
included into a model for physical functions. Classically,
the notion of unpredictability of a random function f is
formalized by the following security experiment consisting
of a learning and a challenge phase: in the learning phase,
A learns the evaluations of f on a set of inputs {x1, . . . , xn}
(which may be given from outside or chosen by A). Then,
in the challenge phase, A must return (x, f (x)) for some
x (cid:54)∈ {x1, . . . , xn}.
Given that
this formalization is common and widely
accepted in cryptography, one may be tempted to adapt
it in a straightforward manner to PUFs. This would mean
to take the same deﬁnition but
to consider PF systems
instead of functions. However, this approach not always
makes sense: ﬁrst, the output of a PF system depends on
a challenge x and some helper data h. Thus, the helper data
h must be taken into account as well. Moreover, we stress
that different applications may require different variants of
unpredictability. For instance,
the concept of PUF-based
secure key storage [7] is to use a PF system for securely
storing a cryptographic secret k. This secret k is usually
derived from the output z of a PF system for some input
x. In some cases, x is public and/or possibly ﬁxed for all
instantiations. Note that in such a scenario it is required
that each device generates a different secret k for the same
challenge x. Hence, the outputs of different devices (i.e.,
their PF systems) should be independent. This requirement
is captured by the following security experiment: given the
outputs PFS1(x, ), . . . , PFSn(x, ) of a set of PF systems to
a ﬁxed challenge x within the learning phase, the adversary
A has to predict the output PFS(x, ) for another PF system
PFS (cid:54)∈ {PFS1, . . . , PFSn} in the challenge phase.
Clearly, there is a fundamental difference between the
classical deﬁnition of unpredictability and this security ex-
periment: in the original deﬁnition of unpredictability de-
scribed in the previous paragraph, A is given the evaluation
of one PF system on many challenges, while in the latter
experiment A learns the evaluation of many PF systems on
one ﬁxed challenge.
Obviously, a useful deﬁnition of unpredictability of a PF
system should cover both unpredictability in the original
sense and independence of the outputs of different PF
systems. Therefore, we deﬁne a security experiment that
involves the following sets:
• Let PL be the set of PF systems that are allowed to be
• Let PC be the set of PF systems that are allowed to be
• Let X be the set of challenges that are allowed to be
queried by A in the challenge phase
queried by A in the learning phase
queried during the whole experiment
Now we consider two extreme cases:4
1) Independence of the outputs of a single PF system:
Consider the case, where PL = PC = {PFS} consists
of one single PF system only, while X contains several
challenges. During the learning phase, the adversary A
learns PFS(xi) for several challenges xi ∈ X . Later,
in the challenge phase, A has to predict PFS(x) for
a new challenge x ∈ X . It is easy to see that this is
the direct translation of the classical unpredictability
experiment described at the beginning o this section
to the scenario of physical function systems.
2) Independence of the outputs of a different PF systems:
Now consider the scenario, where X = {x} consists of
one single challenge only, while PL and PC contains
several PF systems. In this case, during the learning
phase, A learns PFSi(x) for several different PF
systems PFSi ∈ PL. Afterwards, in the challenge
phase, A has to predict PFS(x) for a new PF system
PFS ∈ PC that has not been queried before. Note that
this reﬂects the requirements of PUF-based secure key
storage [7].
The deﬁnition of unpredictability should cover both extreme
and all intermediate cases.
B. Formalization
We now are ready to deﬁne unpredictability. The deﬁni-
shown
tion is based on the security experiment Expw-uprd
in Figure 5.
A
Deﬁnition 13 (Weak Unpredictability): Let PL,PC ⊆ P
be subsets of the set of all possible PF systems. Let T = ∅
and q ∈ N with q ≥ 0. With A we denote the adversary that
takes part in the security experiment depicted in Figure 5.
A PF system is weak (λ, q)-unpredictable if
Pr(cid:2)z = z(cid:48) : (z, z(cid:48)) ← Expw-uprd
(q)(cid:3) ≤ λ · ρp(x)
(14)
A
Note that the robustness of a PF system PFS is an upper
bound for the predictability of the outputs of PFS. For
instance, a true random number generator is a PF system
4For the sake of readability, we omit the helper data here.
409
Figure 5. Weak unpredictability security experiment Expw-uprd
A
(q).
Figure 6. Strong unpredictability security experiment Exps-uprd
A
(q).
with very low reliability and thus, its outputs are highly
unpredictable.
While stronger notions of unpredictability exist (see be-
low), the consideration of weak unpredictability is nonethe-
less important for at least the following reasons: (i) weak un-
predictability is an established property in cryptography and
has been used for stronger constructions, e.g., see [38], and
(ii) PF constructions may be weakly unpredictable only, e.g.,
arbiter PUFs, and hence should be covered by the model.
Some use cases require a stronger notion of unpredictability,
where the adversary is allowed to adaptively query the PF
system in the challenge phase. We therefore deﬁne strong
unpredictability based on the security experiment Exps-uprd
depicted in Figure 6.
A
Deﬁnition 14 (Strong Unpredictability): Let PL be the
set of PF systems that are allowed to be queried by A in the
learning phase and let PC be the set of PF systems that are
allowed to be queried by A in the challenge phase. Moreover,
let T = ∅ and q ∈ N with q ≥ 0. With A we denote the
adversary that takes part in the security experiment as shown
in Figure 6. A PF system is strong (λ, q)-unpredictable if
Pr(cid:2)z = z(cid:48) : (z, z(cid:48)) ← Exps-uprd
(q)(cid:3) ≤ λ · ρp(x)
A
(15)
OracleOw-uprdAdversaryA(PFSi,xi,zi,hi)PFSi$←PL(zi,hi)←PFSi(xi,)T←T∪(cid:8)(PFSi,xi,zi,hi)(cid:9)(z,h)←PFS(x,)(PFS,x,h)z0LearningphaseAobtains0≤i≤qtuplesChallengephasexi$←XPFS$←PCx$←Xzif(PFS,x,·,·)6∈TthenOracleOs-uprdAdversaryA(PFSi,xi,hi)(zi,h0i)ifPFSi∈PLandxi∈X0(zi,h0i)←PFSi(xi,hi)T←T∪(cid:8)(PFSi,xi,zi,h0i)(cid:9)(PFS,x,h)ifPFS∈PCandx∈X0(z,h0)←PFS(x,h)h0z0LearningphaseAcando0≤i≤qqueriesChallengephasezandhi∈H∪{}thenand(PFS,x,·,·)/∈TthenC. Example
large extent statistically independent.
In general, there are no straightforward methods to strictly
bound the unpredictability of a PF system. However, un-
predictability can be assessed w.r.t. the best known attacks
against the security experiments deﬁned in Deﬁnition 13
and 14. This is very similar to measuring the security of
a classical computational cryptographic primitive, where
security is measured based on the effort needed for its best
known cryptanalysis. However, in contrast to most classical
primitives, physical functions do not have a well-deﬁned
algorithmic description against which cryptanalysis can be
launched. In order to win the unpredictability experiment,
the adversary needs to apply different methods, e.g., using
additional information about the implementation of the phys-
ical function, or alternatively taking advantage of previously
unknown statistical deviations or dependencies in the PF
responses. The former method is used for modelling attacks
against delay-based PUFs [20], [24], [22], where the adver-
sary exploits the linearity of the delay circuits to build an
accurate mathematical model of the PUF. The vulnerability
of abusing any statistical deviation is assessed by running
statistical tests on the outputs of the PF system and can be
prevented by applying an appropriate privacy ampliﬁcation
algorithm on the PF system output (see Section V-C) [34].
For the SRAM PUF of our example, the ﬁrst type of
attack is considered to be infeasible. The reason is that
learning the physical implementation of the SRAM cells
in such detail that allows predicting their power-up state is
infeasible in practice. Regarding the second class of attacks,
an interesting result has been obtained for SRAM PUFs. A
particular statistical test, called the context-tree weighting
method (CTW) [39], has been performed on experimental
SRAM PUF data to estimate its min-entropy content. Min-