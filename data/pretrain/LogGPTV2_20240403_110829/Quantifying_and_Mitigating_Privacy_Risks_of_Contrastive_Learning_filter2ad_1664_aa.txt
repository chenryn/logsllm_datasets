title:Quantifying and Mitigating Privacy Risks of Contrastive Learning
author:Xinlei He and
Yang Zhang
Quantifying and Mitigating Privacy Risks of
Contrastive Learning
Xinlei He and Yang Zhang
CISPA Helmholtz Center for Information Security
ABSTRACT
Data is the key factor to drive the development of machine learning
(ML) during the past decade. However, high-quality data, in partic-
ular labeled data, is often hard and expensive to collect. To leverage
large-scale unlabeled data, self-supervised learning, represented
by contrastive learning, is introduced. The objective of contrastive
learning is to map different views derived from a training sample
(e.g., through data augmentation) closer in their representation
space, while different views derived from different samples more
distant. In this way, a contrastive model learns to generate infor-
mative representations for data samples, which are then used to
perform downstream ML tasks. Recent research has shown that
machine learning models are vulnerable to various privacy attacks.
However, most of the current efforts concentrate on models trained
with supervised learning. Meanwhile, data samples’ informative
representations learned with contrastive learning may cause severe
privacy risks as well.
In this paper, we perform the first privacy analysis of contrastive
learning through the lens of membership inference and attribute
inference. Our experimental results show that contrastive models
trained on image datasets are less vulnerable to membership in-
ference attacks but more vulnerable to attribute inference attacks
compared to supervised models. The former is due to the fact that
contrastive models are less prone to overfitting, while the latter
is caused by contrastive models’ capability of representing data
samples expressively. To remedy this situation, we propose the first
privacy-preserving contrastive learning mechanism, Talos, relying
on adversarial training. Empirical results show that Talos can suc-
cessfully mitigate attribute inference risks for contrastive models
while maintaining their membership privacy and model utility.1
CCS CONCEPTS
• Security and privacy; • Computing methodologies → Ma-
chine learning;
KEYWORDS
contrastive learning, membership inference attacks, attribute infer-
ence attacks, privacy-preserving machine learning
1Our code is available at https://github.com/xinleihe/ContrastiveLeaks.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8454-4/21/11...$15.00
https://doi.org/10.1145/3460120.3484571
ACM Reference Format:
Xinlei He and Yang Zhang. 2021. Quantifying and Mitigating Privacy Risks
of Contrastive Learning. In Proceedings of the 2021 ACM SIGSAC Conference
on Computer and Communications Security (CCS ’21), November 15–19, 2021,
Virtual Event, Republic of Korea. ACM, New York, NY, USA, 19 pages. https:
//doi.org/10.1145/3460120.3484571
1 INTRODUCTION
Machine learning (ML) has progressed tremendously, and data is the
key factor to drive such development. However, high-quality data,
in particular labeled data, is often hard and expensive to collect as
this relies on large-scale human annotation. Meanwhile, unlabeled
data is being generated at every moment. To leverage unlabeled
data for machine learning tasks, self-supervised learning has been
introduced [34]. The goal of self-supervised learning is to derive
labels from an unlabeled dataset and train an unsupervised task
in a supervised manner. A trained self-supervised model serves as
an encoder transforming data samples into their representations
which are then used to perform supervised downstream ML tasks.
One of the most prominent self-supervised learning paradigms is
contrastive learning [9, 18, 20, 24, 29, 61, 67], with SimCLR [9] as its
most representative framework [34].
Different from supervised learning which directly optimizes an
ML model on a labeled training dataset, referred to as a supervised
model, contrastive learning aims to train a contrastive model, which
is able to generate expressive representations for data samples, and
relies on such representations to perform downstream supervised
ML tasks. The optimization objective for contrastive learning is to
map different views derived from one training sample (e.g., through
data augmentation) closer in the representation space while dif-
ferent views derived from different training samples more distant.
By doing this, a contrastive model is capable of representing each
sample in an informative way.
Recently, machine learning models have been demonstrated
to be vulnerable to various privacy attacks against their training
dataset [5, 7, 19, 22, 36, 49, 52, 55, 56]. The two most representative
attacks in this domain are membership inference attack [49, 52]
and attribute/property inference attack [36, 56]. The former aims
to infer whether a data sample is part of a target ML model’s train-
ing dataset. The latter leverages the overlearning property of a
machine learning model to infer the sensitive attribute of a data
sample. So far, most of the research on the privacy of machine learn-
ing concentrates on supervised models. Meanwhile, informative
representations for data samples learned by contrastive models may
cause severe privacy risks as well. To the best of our knowledge,
this has been left largely unexplored.
Session 3C: Inference Attacks CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea845Our Contributions. In this paper, we perform the first privacy
quantification of contrastive learning, the most representative self-
supervised learning paradigm. More specifically, we study the pri-
vacy risks of data samples in the contrastive learning setting, with a
focus on SimCLR, through the lens of membership inference and at-
tribute inference, and we concentrate on contrastive models trained
on image datasets.
We adapt the existing attack methodologies for membership in-
ference (neural network-based, metric-based, and label-only) and
attribute inference against supervised models to contrastive mod-
els. Our empirical results show that contrastive models are less
vulnerable to membership inference attacks than supervised mod-
els. For instance, considering the neural network-based attacks,
we achieve 0.620 membership inference accuracy on a contrastive
model trained on STL10 [11] with ResNet-50 [21], while the result
is 0.810 on the corresponding supervised model. The reason behind
this is contrastive models are less prone to overfitting.
On the other hand, we observe that contrastive models are more
vulnerable to attribute inference attacks than supervised models.
For instance, on the UTKFace [68] dataset with ResNet-18, we can
achieve 0.701 attribute inference attack accuracy on the contrastive
model while only 0.422 on the supervised model. This is due to
the fact that the representations generated by a contrastive model
contain rich and expressive information about their original data
samples, which can be exploited for effective attribute inference.
To mitigate the attribute inference risks stemming from con-
trastive models, we propose the first privacy-preserving contrastive
learning mechanism, namely Talos, relying on adversarial train-
ing. Concretely, Talos introduces an adversarial classifier into the
original contrastive learning framework to censor the sensitive
attributes learned by a contrastive model. Our evaluation reveals
that Talos can successfully mitigate attribute inference risks for con-
trastive models while maintaining their membership privacy and
model utility. Our code and models will be made publicly available.
of contrastive learning.
In summary, we make the following contributions:
• We take the first step towards quantifying the privacy risks
• Our empirical evaluation shows that contrastive models
trained on image datasets are less vulnerable to member-
ship inference attacks but more prone to attribute inference
attacks compared to supervised models.
• We propose the first privacy-preserving contrastive learning
mechanism, which is able to protect the trained contrastive
models from attribute inference attacks without jeopardizing
their membership privacy and model utility.
2 PRELIMINARY
2.1 Supervised Learning
Supervised learning, represented by classification, is one of the
most common and important ML applications. We first denote a
set of data samples by 𝑋 and a set of labels by 𝑌. The objective of a
supervised ML model M is to learn a mapping function from each
data sample 𝑥 ∈ 𝑋 to its label/class 𝑦 ∈ 𝑌. Formally, we have
M : 𝑥 ↦→ 𝑦
(1)
Given a sample 𝑥, its output from M, denoted by 𝑝 = M(𝑥), is a
vector that represents the probability distribution of the sample
belonging to a certain class. In this paper, we refer to 𝑝 as the
prediction posteriors. To train an ML model, we need to define a
loss function L(𝑦,M(𝑥)) which measures the distance between a
sample’s prediction posteriors and its label. The training process is
then performed by minimizing the expectation of the loss function
over a training dataset Dtrain, i.e., the empirical loss. We formulate
this as follow:
arg minM
L(𝑦,M(𝑥))
(2)
(𝑥,𝑦)∈Dtrain
1
|Dtrain|

LCE(𝑦, 𝑝) = − 𝑘
𝑖=1
Cross-entropy loss is one of the most common loss functions used
for classification tasks, it is defined as the following.
𝑦𝑖 log 𝑝𝑖
(3)
Here, 𝑘 is the total number of classes, 𝑦𝑖 equals to 1 if the sample
belongs to class 𝑖 (otherwise 0), and 𝑝𝑖 is the 𝑖-th element of the
posteriors 𝑝. In this paper, we use cross-entropy as the loss function
to train all the supervised models.
2.2 Contrastive Learning
Supervised learning is powerful, but its success heavily depends on
the labeled training dataset. In the real world, high-quality labeled
dataset is hard and expensive to obtain as it often relies on human
annotation. For instance, the ILSVRC2011 dataset [47] contains
more than 12 million labeled images that are all annotated by Ama-
zon Mechanical Turk workers. Meanwhile, unlabeled data is being
generated at every moment. To leverage large-scale unlabeled data,
self-supervised learning is introduced.
The goal of self-supervised learning is to get labels from an
unlabeled dataset for free so that one can train an unsupervised
task on this unlabeled dataset in a supervised manner. Contrastive
learning/loss [9, 18, 20, 24, 29, 61, 67] is one of the most successful
and representative self-supervised learning paradigms in recent
years and has received a lot of attention from both academia and
industry. In general, contrastive learning aims to map a sample
closer to its correlated views and more distant to other samples’
correlated views. In this way, contrastive learning is able to learn
an informative representation for each sample, which can then
be leveraged to perform different downstream tasks. Contrastive
learning relies on Noise Contrastive Estimation (NCE) [18] as its
objective function, which can be formulated as:
L = − log(
𝑒sim(𝑓 (𝑥),𝑓 (𝑥+))
𝑒sim(𝑓 (𝑥),𝑓 (𝑥+)) + 𝑒sim(𝑓 (𝑥),𝑓 (𝑥−)) )
(4)
where 𝑓 is an encoder that maps a sample into its representation,
𝑥+ is similar to 𝑥 (referred to as a positive pair), 𝑥− is dissimilar to
𝑥 (referred to as a negative pair), and sim is a similarity function.
The structure of the encoder and the similarity function can vary
from different tasks. In this paper, we focus on one of the most
popular contrastive learning frameworks [34], namely SimCLR [9].
This framework is assembled with the following components.
Data Augmentation. SimCLR first uses a data augmentation mod-
ule to transform a given data sample 𝑥 to its two augmented views,
Session 3C: Inference Attacks CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea846denoted by ˜𝑥𝑖 and ˜𝑥 𝑗, which can be considered as a positive pair for
𝑥. In our work, we follow the same data augmentation process used
by SimCLR [9], i.e., first random cropping and flipping with resizing,
second random color distortions, and third random Gaussian blur.
Base Encoder 𝑓 . Base encoder 𝑓 is used to extract representations
from the augmented data samples. The base encoder can follow
various neural network (NN) architectures. In this paper, we ap-
ply the widely used ResNet [21] (ResNet-18 and ResNet-50) and
MobileNetV2 [50] to obtain the representation ℎ𝑖 = 𝑓 ( ˜𝑥𝑖) for ˜𝑥𝑖.
Projection Head 𝑔. Projection head 𝑔 is a simple neural network
that maps the representations from the base encoder to another
latent space to apply the contrastive loss. The goal of the projection
head is to enhance the encoder’s performance. Following Chen et
al. [9], we implement it with a 2-layer MLP (multilayer perceptron)
to obtain the output 𝑧𝑖 = 𝑔(ℎ𝑖) for ℎ𝑖.
Contrastive Loss Function. The contrastive loss function is de-
fined to guide the model to learn the general representation from
the data itself. Given a set of augmented samples { ˜𝑥𝑘} including a
positive pair ˜𝑥𝑖 and ˜𝑥 𝑗, the contrastive loss maximizes the similarity
between ˜𝑥𝑖 and ˜𝑥 𝑗 and minimizes the similarity between ˜𝑥𝑖 ( ˜𝑥 𝑗)
and other samples. For each mini-batch of 𝑁 samples, we have 2𝑁
augmented samples. The loss function for a positive pair ˜𝑥𝑖 and ˜𝑥 𝑗
can be formulated as:
ℓ(𝑖, 𝑗) = − log
2𝑁
𝑒sim(𝒛𝑖,𝒛 𝑗)/𝜏
𝑘=1,𝑘≠𝑖 𝑒sim(𝒛𝑖,𝒛𝑘)/𝜏
(5)
𝑁
𝑘=1
where sim(𝑧𝑖, 𝑧 𝑗) = 𝑧𝑖⊤𝑧 𝑗/∥𝑧𝑖∥∥𝑧 𝑗 ∥ represents the cosine similarity
between 𝑧𝑖 and 𝑧 𝑗 and 𝜏 is a temperature parameter. The final loss
is calculated over all positive pairs in a mini-batch, which can be
defined as the following.
LContrastive =