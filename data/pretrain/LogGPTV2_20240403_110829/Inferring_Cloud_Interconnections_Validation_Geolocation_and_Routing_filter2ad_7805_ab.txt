Azure
VM
Cable One
i1
R1
i2
R2
i3
R3
104.44.23.80 
ae102-0.icr02.sn6.ntwk.msn.net
Fig. 3. We observed the Azure address
104.44.23.80 after the border router in
Cable One, likely indicating traceroute
path corruption. This caused bdrmapIT
to incorrectly conclude that Azure oper-
ates router R1.
this change can apply to edge networks with largely incomplete neighbor con-
straints in BGP AS paths, like other cloud and content delivery networks. This
change increased the AS operator inference accuracy to 88.8%, correcting all six
of the AS operator inferences without introducing additional error.
Using an interface graph constructed from the traceroutes to investigate the
remaining errors led us to conclude that path changes during traceroutes likely
caused most of the errors. While Paris probes avoid corruptions due to load-
balancing along a path, they cannot prevent corruptions due to path changes
in router forwarding tables. Scamper probing is especially susceptible to cor-
ruptions caused by path changes. Like UNIX traceroute, Scamper waits for the
response to the probe with Time to Live (TTL) i before sending the probe with
TTL i+1, but for eﬃciency it parallelizes across traceroute destinations (Fig. 2).
This concurrency enables rapid path discovery, necessary for temporally coher-
ent snapshots of cloud topologies, but a path change can corrupt any of the
traceroutes active at any given time.
To look for evidence of potential path changes, we generated a directed inter-
face graph from the 355.8 M Azure traceroutes, creating directed edges between
an address and every address that immediately followed it in a traceroute, but
not when one or more unresponsive hops separate the addresses. We found 56
(13.4%) Azure interconnection addresses in our validation dataset followed by at
least one Azure address in a traceroute. These interconnection addresses are on
routers operated by neighboring networks, so an uncorrupted traceroute path
would most likely not contain a subsequent Azure address. Figure 3 shows a
potentially corrupted traceroute path, where we observed an Azure IP address
following the interconnection with Cable One. Observing Azure addresses after
routers in neighboring ASes does not necessarily indicate that path changes cor-
rupted a traceroute, and can result from oﬀ-path addresses and load-balancing
as well, so we conduct an additional experiment to rule out alternative explana-
tions.
236
A. Marder et al.
3.2 Fast and Straight Traceroute (FAST) Traceroute Probing
We developed a new traceroute tool, Fast
And Straight Traceroute (FAST), to test
our hypothesis that path changes corrupted
the cloud traceroute paths by mitigating
the impact of path changes on the observed
topology. FAST sends all probes from TTL
1 to 32 to a destination at a ﬁxed pack-
ets per second (pps) rate, irrespective of
replies, before moving on to the next tracer-
oute destination, and uses packet capture
to record probes and replies, allowing it to construct traceroute paths with accu-
rate RTTs. Unlike similar tools such as Yarrp [17], FAST’s guaranteed sequential
probing allows it to construct traceroute paths during probing while consuming
few resources on the cloud VMs.
Fig. 4. We set up VMs in every cloud
region available to us using similar
VM types in each cloud.
To eﬃciently reveal traceroute paths, we
determined a probing rate for FAST that
balances topology discovery with probing
speed by conducting traceroutes from a VM
in every region of AWS, Azure, and GCP
(Fig. 4) to one address in 100,000 distinct
preﬁxes announced into BGP. Our results
(Fig. 5) indicate that probing at 5000 pps
reveals nearly all of the hops found by
probing at slower rates, but probing faster
induced rate-limiting in Azure. At 5000 pps,
FAST can complete probing to every routed
/24 in less than 21 h.
Fig. 5. We observed fewer tracer-
oute hops for Azure probing above
5000 pps. GCP inﬂates probe TTLs
(Sect. 4.1), causing relatively few
responses for all probing rates.
To isolate the impact of path changes, we
changed only the traceroute tool from Scam-
per to FAST, but conducted traceroutes from
the same Azure regions to the same destina-
tions. Generating an interface-graph from the new set of traceroutes appears to
conﬁrm our hypothesis that path changes corrupted the scamper traceroutes. In
the FAST traceroutes, we never observed an Azure address after a router known
to belong to a neighboring AS. Furthermore, path changes played a large role
in bdrmapIT’s inaccurate AS operator inferences. bdrmapIT’s inferences on the
FAST traceroutes were 97.4% accurate, compared to 88.8% with the Scamper
traceroutes.
Inferring Cloud Interconnections
237
Fig. 6. A traceroute from GCP Los Angeles to the University of Pennsylvania (UPenn)
revealed no GCP IP addresses (a), but traceroutes from Los Angeles to JANET in the
UK (b), and Belgium to UPenn (c), each revealed GCP addresses.
4 Learning About Clouds from Interconnections
Armed with conﬁdence in our interconnection inferences, we set up VMs in
every region available to us for AWS, Azure, and GCP, the three largest cloud
providers, and used FAST to conduct traceroutes from every VM to a random
address in every routed /24. We conﬁgured our VMs to use the WAN as much
as possible; Azure networking defaults to cold-potato routing and we selected
GCP’s premium network tier, but in AWS we used the default WAN behavior.
We did not use AWS Global Accelerator [5], and we plan to investigate the aﬀect
of Global Accelerator in future work. These experiments derived routed address
space using collected BGP route announcements from 1–5 October 2020. We used
the same combination of bdrmapIT, PeeringDB, and IXPDB as in Sect. 3 to infer
interconnection addresses, and used these interconnection inferences to analyze
the neighboring networks that each cloud uses to reach public Internet networks,
and to geolocate the interconnections between the three cloud providers.
4.1 GCP Inﬂates Traceroute Probe TTLs
One challenge for our analysis is that GCP, unlike AWS and Azure, inﬂates the
TTL values of traceroute probes after they leave VMs such that the hop #1
traceroute address belongs to a later router in that path, rather than to the ﬁrst
router hop [13]1. This behavior violates a core traceroute assumption that hop
#1 corresponds to the ﬁrst router probed. While invisible Multiprotocol Label
Switching (MPLS) tunnels exhibit similar behavior, hiding router hops between
the tunnel entry and exit routers [21,53,54], MPLS tunnels do not aﬀect hop #1
since the probe with TTL 1 could not yet enter an MPLS tunnel. This practice
of rewriting probe TTLs has likely caused researchers to incorrectly conclude
that GCP routers do not respond to traceroute [29], or that hop #1 is a router
just past the GCP border [57].
1 We observed diﬀerent behavior in February, 2021 (Appendix A).
238
A. Marder et al.
Figure 6a shows the GCP TTL inﬂation with a traceroute from a VM in Los
Angeles, where hop #1 reported an address that router conﬁgurations from Inter-
net2 show belong to a University of Pennsylvania (UPenn) router [10], despite no
direct interconnection between GCP and UPenn [11]. In fact, the UPenn router
at hop #1 reported an interface address used to interconnect with Internet2 [10],
indicating that the probes traversed Internet2 to reach UPenn, but the inﬂated
TTL caused probes to expire only after reaching UPenn. Traceroutes from other
GCP VMs to the same UPenn destination, such as in the Belgium region, exposed
apparent GCP internal IP addresses, only reaching UPenn at hop 8 (Fig. 6c).
All of our VMs use GCP’s premium network tier, but not all revealed internal
GCP addresses, contradicting reported behavior that only GCP’s standard tier
inﬂates traceroute TTLs [14]. Our ability to observe internal GCP addresses
from the Belgium VM toward UPenn, and from the VM in Los Angeles toward
JANET in the UK (Fig. 6b), suggests that the opportunity to view internal and
interconnection GCP addresses depends on the combination of GCP region and
traceroute destination. We leave an analysis of the interconnection information
lost to GCP’s TTL inﬂation for future work.
4.2
Inferring How Clouds Reach Internet Networks
We deﬁne the cloud transit degree for a cloud
neighbor AS as the number of unique traceroute
destination ASes for which the neighbor is the
next-hop AS. This metric is an indication of the
relative importance of that neighbor to the cloud
network. In Fig. 7, the cloud network uses AS #1
to reach three ASes including AS #1, giving it
a CTD of 3, while AS #2 has a CTD of 2. Here,
the cloud uses both AS #1 and AS #2 to reach
AS #3, so we count AS #3 once for each AS.
This situation occurs when clouds choose diﬀer-
ent next-hop networks depending on the VM’s
region.
AS 1, AS 3,
AS 4
Cloud
AS 1
i1
R1
R2
i2
AS 2, AS 3
AS 2
Fig. 7. AS #1 is the next-hop
network in traceroute paths to
three ASes, so its CTD = 3. AS
#2 is the next-hop network for
two ASes, so its CTD = 2.
We only used traceroutes with a cloud interconnection in the path to compute
the CTDs, so we discarded any traceroute where an unresponsive hop separates
the last hop inside the cloud network from the ﬁrst hop outside the cloud net-
work. Figure 8a shows the fraction of included traceroutes in each cloud. For
every neighbor AS, we maintain a set of destination ASes reached through that
neighbor, so at the ﬁrst hop in the neighbor AS we add the traceroute’s destina-
tion AS to that neighbor’s set. Finally, we compute the CTD for each neighbor
as the cardinality of its destination set.
Figure 8b shows the number of unique ASes for each cloud across their dif-
ferent regions. The diﬀerent variances reﬂect the traﬃc engineering policies of
each cloud. AWS uses hot-potato routing, so we not only saw diﬀerent neighbors
from each region, but we saw diﬀerent numbers of neighboring ASes as well.
Conversely, Azure uses cold-potato routing, so Azure transits packets destined
Inferring Cloud Interconnections
239
)a(
)b(
)c(
Fig. 8. We excluded many of the GCP traceroutes since the traceroute path often began
outside GCP (a). Unlike AWS and GCP, we observed nearly all of the same neighbor
ASes from every Azure region (b). All clouds rely on tier 1 and tier 2 networks, but
AWS relies more heavily on tier 1s in most regions (c).
for a neighboring AS across its global backbone and hands them oﬀ to the neigh-
bor directly. GCP also employs cold-potato routing, but certain regions included
more internal routers in traceroute paths than others. We only included an AS
as a neighbor when we saw a GCP interconnection address, as traceroute paths
can start in unconnected networks (Fig. 6).
Figure 8c shows the fraction of the total CTD accounted for by tier 1, tier 2, and
tier 3 networks. For the purposes of this analysis, we deﬁne tier 1 networks as the
19 ASes inferred to be at the top of the AS hierarchy in CAIDA’s AS relationship
dataset for October 1, 2020. Tier 2 networks include the 10,627 other ASes with at
least one customer in the dataset, with the remaining networks classiﬁed as tier 3.
Our analysis reveals that all three clouds rely heavily on ISPs, although we expect
that the clouds primarily peer with these ISPs, rather than interconnect with them
for Internet transit. AWS shows wide variance across regions, heavier reliance on
tier 1 networks (due to hot potato routing), and heavy tier 2 network use in certain
regions. Azure relies on tier 1 and tier 2 networks consistently across regions, and
GCP appears better connected to edge networks.
Table 1. We observed an order of
magnitude more unique cloud AS
neighbors traceroutes than in public
BGP collections.
In total, we discovered an order of mag-
nitude more cloud neighbor ASes in our
traceroutes from cloud VMs than were vis-
ible in RouteViews and RIPE RIS collec-
tions of BGP route announcements from 1–
5 October, 2020 (Table 1). We also found
that GCP appears to interconnect with more
than twice as many networks as AWS and
Azure. Importantly, our results indicate that
the visible connectivity of cloud networks, and their reliance on speciﬁc neigh-
bors, is region-dependent. To properly measure and analyze the cloud requires
gathering data from each region, and considering each region separately.
Traceroute 4110 3889
AWS Azure GCP
327
300
BGP
8620
381
240
A. Marder et al.
google.fra-96cbe-1b.ntwk.msn.net 
198.200.130.255
Azure
VM
i1
R1
i2
R2
GCP
104.44.232.128 
ae22-0.fra-96cbe-1b.ntwk.msn.net 
AS714
i3
R3
DE-CIX Frankfurt
80.81.194.171
99.82.177.85
104.44.24.41
AWS
Azure
VM
b1
R1
b3
R2
b4
b5
R3
52.93.146.230
104.44.24.40 
ae30-0.gru-96cbe-1a.ntwk.msn.net
(a) IXP and hostname geolocation.
(b) Hostname geolocation.
Fig. 9. In (a) the interconnection address i2 and the IXP address i3 share a common
predecessor, so we infer i2 is also located in Frankfurt. The hostnames for i2 and i1