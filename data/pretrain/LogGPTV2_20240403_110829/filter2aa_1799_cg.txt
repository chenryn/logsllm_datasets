I 
Emulated devices, also known—in industry-standard form—as fully virtualized devices
I 
Synthetic devices, also known as paravirtualized devices
I 
Hardware-accelerated devices, also known as direct-access devices
For performing I/O to physical devices, the processor usually reads and writes data from input and 
output ports (I/O ports), which belong to a device. The CPU can access I/O ports in two ways: 
I 
Through a separate I/O address space, which is distinct from the physical memory address
space and, on AMD64 platforms, consists of 64 thousand individually addressable I/O ports.
This method is old and generally used for legacy devices.
330 
CHAPTER 9 Virtualization technologies
I 
Through memory mapped I/O. Devices that respond like memory components can be accessed
through the processor’s physical memory address space. This means that the CPU accesses
memory through standard instructions: the underlying physical memory is mapped to a device.
Figure 9-27 shows an example of an emulated device (the virtual IDE controller used in Generation 1 
VMs), which uses memory-mapped I/O for transferring data to and from the virtual processor. 
Root Partition / Host OS
VM1
Hardware
HCS
Processes
File System/Vol
Disk Driver
IDE Storage Driver
VMWP
VMMS
VSPs
Physical
Device
Drivers
VMBus
VID
WinHV
Hypervisor
IDE VDEV
CPU
I/O Devices
Memory
Guest Physical
Address Space
96GB
64GB
4GB
0
FIGURE 9-27 The virtual IDE controller, which uses emulated I/O to perform data transfer.
In this model, every time the virtual processor reads or writes to the device MMIO space or emits 
instructions to access the I/O ports, it causes a VMEXIT to the hypervisor. The hypervisor calls the 
proper intercept routine, which is dispatched to the VID driver. The VID driver builds a VID message 
and enqueues it in an internal queue. The queue is drained by an internal VMWP’s thread, which waits 
and dispatches the VP’s messages received from the VID driver; this thread is called the message pump 
thread and belongs to an internal thread pool initialized at VMWP creation time. The VM Worker 
process identifies the physical address causing the VMEXIT, which is associated with the proper virtual 
device (VDEV), and calls into one of the VDEV callbacks (usually read or write callback). The VDEV code 
uses the services provided by the instruction emulator to execute the faulting instruction and properly 
emulate the virtual device (an IDE controller in the example).
NOTE The full instructions emulator located in the VM Worker process is also used for other 
different purposes, such as to speed up cases of intercept-intensive code in a child partition. 
The emulator in this case allows the execution context to stay in the Worker process between 
intercepts, as VMEXITs have serious performance overhead. Older versions of the hardware 
virtualization extensions prohibit executing real-mode code in a virtual machine; for those 
cases, the virtualization stack was using the emulator for executing real-mode code in a VM.
CHAPTER 9 Virtualization technologies
331
Paravirtualized devices
While emulated devices always produce VMEXITs and are quite slow, Figure 9-28 shows an example 
of a synthetic or paravirtualized device: the synthetic storage adapter. Synthetic devices know to run 
in a virtualized environment; this reduces the complexity of the virtual device and allows it to achieve 
higher performance. Some synthetic virtual devices exist only in virtual form and don’t emulate any 
real physical hardware (an example is synthetic RDP). 
Root Partition / Host OS
Hardware
HCS
VMWP
VMMS
Physical
Device
Drivers
VMBus
WinHV
Filesystem
vhdmp
StorVSP
VID
Hypervisor
SynthStor VDEV
CPU
Memory
VM1
Processes
File System/Vol
Disk Driver
StorVSC
FIGURE 9-28 The storage controller paravirtualized device.
Paravirtualized devices generally require three main components:
I 
A virtualization service provider (VSP) driver runs in the root partition and exposes virtualiza-
tion-specific interfaces to the guest thanks to the services provided by VMBus (see the previous
section for details on VMBus).
I 
A synthetic VDEV is mapped in the VM Worker process and usually cooperates only in the start-
up, teardown, save, and restore of the virtual device. It is generally not used during the regular
work of the device. The synthetic VDEV initializes and allocates device-specific resources (in the
example, the SynthStor VDEV initializes the virtual storage adapter), but most importantly allows
the VSP to offer a VMBus communication channel to the guest VSC. The channel will be used for
communication with the root and for signaling device-specific notifications via the hypervisor.
I 
A virtualization service consumer (VSC) driver runs in the child partition, understands the vir-
tualization-specific interfaces exposed by the VSP, and reads/writes messages and notifications
from the shared memory exposed through VMBus by the VSP. This allows the virtual device to
run in the child VM faster than an emulated device.
332 
CHAPTER 9 Virtualization technologies
Hardware-accelerated devices
On server SKUs, hardware-accelerated devices (also known as direct-access devices) allow physical de-
vices to be remapped in the guest partition, thanks to the services exposed by the VPCI infrastructure. 
When a physical device supports technologies like single-root input/output virtualization (SR IOV) or 
Discrete Device Assignment (DDA), it can be mapped to a guest partition. The guest partition can di-
rectly access the MMIO space associated with the device and can perform DMA to and from the guest 
memory directly without any interception by the hypervisor. The IOMMU provides the needed security 
and ensures that the device can initiate DMA transfers only in the physical memory that belong to the 
virtual machine.
Figure 9-29 shows the components responsible in managing the hardware-accelerated devices:
I 
The VPci VDEV (Vpcievdev.dll) runs in the VM Worker process. Its rule is to extract the list of
hardware-accelerated devices from the VM configuration file, set up the VPCI virtual bus, and
assign a device to the VSP.
I 
The PCI Proxy driver (Pcip.sys) is responsible for dismounting and mounting a DDA-compatible
physical device from the root partition. Furthermore, it has the key role in obtaining the list of
resources used by the device (through the SR-IOV protocol) like the MMIO space and interrupts.
The proxy driver provides access to the physical configuration space of the device and renders
an “unmounted” device inaccessible to the host OS.
I 
The VPCI virtual service provider (Vpcivsp.sys) creates and maintains the virtual bus object,
which is associated to one or more hardware-accelerated devices (which in the VPCI VSP are
called virtual devices). The virtual devices are exposed to the guest VM through a VMBus chan-
nel created by the VSP and offered to the VSC in the guest partition.
I 
The VPCI virtual service client (Vpci.sys) is a WDF bus driver that runs in the guest VM. It con-
nects to the VMBus channel exposed by the VSP, receives the list of the direct access devices
exposed to the VM and their resources, and creates a PDO (physical device object) for each of
them. The devices driver can then attach to the created PDOs in the same way as they do in
nonvirtualized environments.
When a user wants to map a hardware-accelerated device to a VM, it uses some PowerShell com-
mands (see the following experiment for further details), which start by “unmounting” the device 
from the root partition. This action forces the VMMS service to communicate with the standard PCI 
driver (through its exposed device, called PciControl). The VMMS service sends a PCIDRIVE_ADD 
_VMPROXYPATH IOCTL to the PCI driver by providing the device descriptor (in form of bus, device, 
and function ID). The PCI driver checks the descriptor, and, if the verification succeeded, adds it in the 
HKLM\System\CurrentControlSet\Control\PnP\Pci\VmProxy registry value. The VMMS then starts a 
PNP device (re)enumeration by using services exposed by the PNP manager. In the enumeration phase, 
the PCI driver finds the new proxy device and loads the PCI proxy driver (Pcip.sys), which marks the 
device as reserved for the virtualization stack and renders it invisible to the host operating system.
CHAPTER 9 Virtualization technologies
333
Root Partition / Host OS
VM 1
Hardware
HCS
Processes
NVME Driver
File System/Vol
Disk Driver
VMWP
VMMS
vPCI VSC
VMBus
VID
Pcip
Pci
WinHV
Hypervisor
VPci VDEV
CPU
Memory
Guest Physical
Address Space
96GB
64GB
4GB
0
IOMMU
vPCI VSP
FIGURE 9-29 Hardware-accelerated devices.
The second step requires assigning the device to a VM. In this case, the VMMS writes the device 
descriptor in the VM configuration file. When the VM is started, the VPCI VDEV (vpcievdev.dll) reads the 
direct-access device’s descriptor from the VM configuration, and starts a complex configuration phase 
that is orchestrated mainly by the VPCI VSP (Vpcivsp.sys). Indeed, in its “power on” callback, the VPCI 
VDEV sends different IOCTLs to the VPCI VSP (which runs in the root partition), with the goal to perform 
the creation of the virtual bus and the assignment of hardware-accelerated devices to the guest VM.
A “virtual bus” is a data structure used by the VPCI infrastructure as a “glue” to maintain the con-
nection between the root partition, the guest VM, and the direct-access devices assigned to it. The 
VPCI VSP allocates and starts the VMBus channel offered to the guest VM and encapsulates it in the 
virtual bus. Furthermore, the virtual bus includes some pointers to important data structures, like some 
allocated VMBus packets used for the bidirectional communication, the guest power state, and so on. 
After the virtual bus is created, the VPCI VSP performs the device assignment. 
A hardware-accelerated device is internally identified by a LUID and is represented by a virtual 
device object, which is allocated by the VPCI VSP. Based on the device’s LUID, the VPCI VSP locates the 
proper proxy driver, which is also known as Mux driver—it’s usually Pcip.sys). The VPCI VSP queries 
the SR-IOV or DDA interfaces from the proxy driver and uses them to obtain the Plug and Play informa-
tion (hardware descriptor) of the direct-access device and to collect the resource requirements (MMIO 
space, BAR registers, and DMA channels). At this point, the device is ready to be attached to the guest 
VM: the VPCI VSP uses the services exposed by the WinHvr driver to emit the HvAttachDevice hypercall 
to the hypervisor, which reconfigures the system IOMMU for mapping the device’s address space in the 
guest partition.
The guest VM is aware of the mapped device thanks to the VPCI VSC (Vpci.sys). The VPCI VSC is 
a WDF bus driver enumerated and launched by the VMBus bus driver located in the guest VM. It is 
334 
CHAPTER 9 Virtualization technologies
composed of two main components: a FDO (functional device object) created at VM boot time, and 
one or more PDOs (physical device objects) representing the physical direct-access devices remapped 
in the guest VM. When the VPCI VSC bus driver is executed in the guest VM, it creates and starts the cli-
ent part of the VMBus channel used to exchange messages with the VSP. “Send bus relations” is the first 
message sent by the VPCI VSC thorough the VMBus channel. The VSP in the root partition responds by 
sending the list of hardware IDs describing the hardware-accelerated devices currently attached to the 
VM. When the PNP manager requires the new device relations to the VPCI VSC, the latter creates a new 
PDO for each discovered direct-access device. The VSC driver sends another message to the VSP with 
the goal of requesting the resources used by the PDO.
After the initial setup is done, the VSC and VSP are rarely involved in the device management. The 
specific hardware-accelerated device’s driver in the guest VM attaches to the relative PDO and man-
ages the peripheral as if it had been installed on a physical machine.
EXPERIMENT: Mapping a hardware-accelerated NVMe disk to a VM
As explained in the previous section, physical devices that support SR-IOV and DDE technologies 
can be directly mapped in a guest VM running in a Windows Server 2019 host. In this experiment, 
we are mapping an NVMe disk, which is connected to the system through the PCI-Ex bus and 
supports DDE, to a Windows 10 VM. (Windows Server 2019 also supports the direct assignment 
of a graphics card, but this is outside the scope of this experiment.)
As explained at https://docs.microsoft.com/en-us/virtualization/community/team-blog/2015 
/20151120-discrete-device-assignment-machines-and-devices, for being able to be reassigned, 
a device should have certain characteristics, such as supporting message-signaled interrupts 
and memory-mapped I/O. Furthermore, the machine in which the hypervisor runs should sup-
port SR-IOV and have a proper I/O MMU. For this experiment, you should start by verifying that 
the SR-IOV standard is enabled in the system BIOS (not explained here; the procedure varies 
based on the manufacturer of your machine).
The next step is to download a PowerShell script that verifies whether your NVMe control-
ler is compatible with Discrete Device Assignment. You should download the survey-dda.ps1 
PowerShell script from https://github.com/MicrosoftDocs/Virtualization-Documentation/tree 
/master/hyperv-samples/benarm-powershell/DDA. Open an administrative PowerShell window 
(by typing PowerShell in the Cortana search box and selecting Run As Administrator) and 
check whether the PowerShell script execution policy is set to unrestricted by running the Get-
ExecutionPolicy command. If the command yields some output different than Unrestricted, 
you should type the following: Set-ExecutionPolicy -Scope LocalMachine -ExecutionPolicy 
Unrestricted, press Enter, and confirm with Y.
If you execute the downloaded survey-dda.ps1 script, its output should highlight whether 
your NVMe device can be reassigned to the guest VM. Here is a valid output example:
Standard NVM Express Controller 
Express Endpoint -- more secure. 
    And its interrupts are message-based, assignment can work. 
PCIROOT(0)#PCI(0302)#PCI(0000)
EXPERIMENT: Mapping a hardware-accelerated NVMe disk to a VM
As explained in the previous section, physical devices that support SR-IOV and DDE technologies 
can be directly mapped in a guest VM running in a Windows Server 2019 host. In this experiment, 
we are mapping an NVMe disk, which is connected to the system through the PCI-Ex bus and 
supports DDE, to a Windows 10 VM. (Windows Server 2019 also supports the direct assignment 
of a graphics card, but this is outside the scope of this experiment.)
As explained at https://docs.microsoft.com/en-us/virtualization/community/team-blog/2015
/20151120-discrete-device-assignment-machines-and-devices, for being able to be reassigned, 
a device should have certain characteristics, such as supporting message-signaled interrupts 
and memory-mapped I/O. Furthermore, the machine in which the hypervisor runs should sup-
port SR-IOV and have a proper I/O MMU. For this experiment, you should start by verifying that 
the SR-IOV standard is enabled in the system BIOS (not explained here; the procedure varies 
based on the manufacturer of your machine).
The next step is to download a PowerShell script that verifies whether your NVMe control-
ler is compatible with Discrete Device Assignment. You should download the survey-dda.ps1 
PowerShell script from https://github.com/MicrosoftDocs/Virtualization-Documentation/tree
/master/hyperv-samples/benarm-powershell/DDA. Open an administrative PowerShell window 
(by typing PowerShell in the Cortana search box and selecting Run As Administrator) and 
check whether the PowerShell script execution policy is set to unrestricted by running the Get-
ExecutionPolicy command. If the command yields some output different than Unrestricted, 
ExecutionPolicy command. If the command yields some output different than Unrestricted, 
ExecutionPolicy
you should type the following: Set-ExecutionPolicy -Scope LocalMachine -ExecutionPolicy 
Unrestricted, press Enter, and confirm with 
Enter, and confirm with 
Enter
Y.Y.Y
If you execute the downloaded survey-dda.ps1 script, its output should highlight whether 
your NVMe device can be reassigned to the guest VM. Here is a valid output example:
Standard NVM Express Controller
Express Endpoint -- more secure.
    And its interrupts are message-based, assignment can work.
PCIROOT(0)#PCI(0302)#PCI(0000)
CHAPTER 9 Virtualization technologies
335
Take note of the location path (the PCIROOT(0)#PCI(0302)#PCI(0000) string in the example). 
Now we will set the automatic stop action for the target VM as turned-off (a required step for 
DDA) and dismount the device. In our example, the VM is called “Vibranium.” Write the following 
commands in your PowerShell window (by replacing the sample VM name and device location 
with your own):
Set-VM -Name "Vibranium" -AutomaticStopAction TurnOff 
Dismount-VMHostAssignableDevice -LocationPath "PCIROOT(0)#PCI(0302)#PCI(0000)"
In case the last command yields an operation failed error, it is likely that you haven’t disabled the 
device. Open the Device Manager, locate your NVMe controller (Standard NVMe Express Controller 