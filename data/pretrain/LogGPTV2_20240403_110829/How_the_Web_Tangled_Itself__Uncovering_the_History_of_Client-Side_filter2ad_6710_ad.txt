other hand, network attackers have an interest to gain ac-
cess to such information. To that end, they might either
eavesdrop (in case of a passive network attacker) on a
plaintext connection, or try to manipulate a connection to
an extend where the encryption is dropped, e.g., by SSL
stripping attacks [13]. In addition, Web developers might
accidentally transmit sensitive information over insecure
channels. An example of this is the use of cookies with-
out speciﬁcally setting the secure ﬂag. In that case, the
cookies are transferred in any connection to the domain
for which they were set, regardless of the use of HTTPS.
To ensure that neither an active attacker can strip SSL
nor an unknowing developer can accidentally build an
insecure application, browsers implement HTTP Strict
Transport Security, or HSTS for short [8]. With this
HTTP header, browsers can be instructed to only con-
nect to a domain via HTTPS regardless of the URL and
to only do so using a validated certiﬁcate.
Obviously, HSTS is only a relevant feature for any site
that runs via HTTPS. The Archive.org crawler, however,
does not store whether a site was retrieved via HTTP or
HTTPS, i.e., based on the historical data we gathered, we
cannot decide whether a site was running HTTPS in the
ﬁrst place. Also, setting an HSTS header on an unen-
crypted connection has no effect, i.e., it is ignored by the
browser [13].
Support for HSTS was ﬁrst introduced in Chrome and
Firefox in 2010. In addition to the header, browsers also
feature a preload list of domains, to which only HTTPS
connections are allowed, regardless of the existence of
the HSTS header. For our analysis, we therefore consid-
ered both the headers as well as entries for the domains
in the preload list for January of each year. Our analy-
sis shows that only very few domains made use of HSTS
until 2012. Starting from 2013, we observe a steady in-
crease, resulting in almost 30% adoption rate by 2016.
5.6 Additional
Awareness
Indicators
for Security
On top of the headers we discussed so far, we identiﬁed
additional features which indicate awareness of poten-
tial security problems. In 2010, Bates et al. [2] showed
that the built-in XSS ﬁlter of Internet Explorer could not
only be bypassed by encoding data in UTF-7, but even
be used to disable Clickjacking protections or conduct
phishing attacks. At that time, Internet Explorer allowed
Web sites to speciﬁcally disable its XSS ﬁlter by send-
ing the X-XSS-Protection: 0 header to the client. In
our study we found that in 2009 and 2010, 30 and 55
sites, respectively, disabled the XSS ﬁlter in IE by send-
ing this header. For 2009, all these sites were related
to Google (e.g., including Youtube), showing that the is-
sues in IE were known to Google before the publication
in 2010. One reasonable explanation is that Google en-
gineers were conﬁdent that no XSS vulnerabilities were
contained in their sites, and wanted to ensure that no vul-
nerabilities could be introduced into otherwise bug-free
sites.
A more recent feature for securing the client side is the
sandbox feature of iframes in HTML5. Using this fea-
ture, a site may restrict the capabilities of an iframe, e.g.,
by disabling JavaScript or isolating content in a unique
origin, thereby mitigating any exploitable vulnerabilities
in the sandboxed content. We found that only three sites
made use of it in any of the HTML pages we analyzed,
showing that this feature is hardly used.
6 Key Insights
In this section, we discuss the key insights of our study
results. We ﬁrst discuss the takeaways on client-side
technology, following with the implications of our anal-
ysis for client-side security. Finally, we investigate the
correlation between discovered vulnerabilities and the
awareness indicators outlined in the previous section.
6.1 Client-Side Technology
The Web’s Complexity is still on the Rise
In our
study of the Web’s evolution, we found that although sev-
eral technologies for client-side interaction were devel-
oped over the years, the only prevailing one is JavaScript.
Moreover, we determined that the general complexity of
JavaScript kept rising over the years. On the one hand,
the average number of statements per external script has
almost reached 1,000 by 2016 — without counting pow-
erful libraries such as jQuery. On the other hand, code
does not necessarily only originate from a site’s devel-
oper, but often resides on remote domains. In our work,
we found that on average, a domain in our 2016 dataset
included script content from almost twelve distinct ori-
gins, which is an increase by almost 100% since 2011.
Along with the introduction of powerful new APIs in
the browsers, which nowadays, e.g., allow for client-to-
client communication that was never envisioned by the
Web’s server/client paradigm, we ﬁnd that the general
complexity of client-side Web applications is on the rise.
USENIX Association
26th USENIX Security Symposium    981
Involvement of Third Parties
Including content from
third parties allows Web sites to outsource certain parts
of their application, e.g., advertisements. However,
whenever code is included from a remote domain, it
may contain vulnerabilities, which effectively compro-
mise the including site. With the rise in complexity
in these third-party libraries, the risk for vulnerabilities
also increases. As an example, while jQuery 1.0 only
contained 768 statements, the most recent version 1.12
(in the 1.* branch) already consists of 3,541 statements.
Moreover, sites rarely update their third-party compo-
nents. As shown by Lauinger et al. [14], a large num-
ber of Web sites use outdated versions of well-known li-
braries, such as jQuery, which contain exploitable ﬂaws.
In our work, we found that especially jQuery was often
used in versions with known vulnerabilities at the time of
use. Moreover, the fraction of domains that use such vul-
nerable third-party libraries remained high since 2012.
Another risk of including third-party code stems from
the fact that this code may be arbitrarily altered by the
remote site or in transit. In the recent past, this was used
to conduct large-scale DDoS attacks against Web sites
used to bypass censorship in China [21]. However, such
attacks can be stopped if sites start implementing Subre-
source Integrity, which ensures that included JavaScript
is only executed when its has the correct checksum [1].
The Rise of the Multi-Origin Web The Web’s pri-
mary security concept is the Same-Origin Policy, which
draws a trust boundary around an application by only al-
lowing resources of the same origin to interact with one
another. In the modern Web, though, applications com-
municate across these boundaries, e.g., between an ad-
vertisement company and the actual site. In our work, we
observed a clear trend towards interconnected sites, espe-
cially using postMessages, which are used by more than
65% of the Web sites we analyzed for 2016. In addition,
we note that the usage of CORS is on the rise as well,
with 20% of the 2016 domains sending a corresponding
header. Given the nature of the Internet Archive crawler,
i.e., the fact that it cannot log in to any applications,
all these numbers need to be considered lower bounds.
Hence, we clearly identify a trend towards an intercon-
nected, multi-origin Web in which ensuring authenticity
of the exchanged data is of utmost importance.
6.2 Client-Side Security
Client-Side XSS Remains a Constant Issue One of
the biggest problems on the Web is Cross-Site Scripting.
In our work, we studied the prevalence of the client-side
variant of this attack over the years. We found that with
the dawn of more powerful JavaScript applications as a
result of so-called Web 2.0, the number of XSS vulnera-
bilities in the JavaScript code spiked. Between 2007 and
2012, more than 12% of the analyzed sites had a least one
such vulnerability. Even though the general complex-
ity of JS applications kept rising after 2012, the number
of vulnerable domains declined, ranging around 8% un-
til 2016. Nevertheless, given our sample of the top 500
pages, such attacks still threaten a large fraction of the
Web users and developer training should focus more on
these issues.
Security vs. Utility Many new technologies intro-
duced in browsers come with security mechanisms, such
as the authenticity and integrity properties provided by
the postMessage API. However, oftentimes these fea-
tures are optional — a developer may, e.g., choose not
to check the origin of an incoming postMessage. As we
observed in our work, technology which enables com-
munication across domain boundaries, such as JSONP,
Flash’s ability to access remote resources, or postMes-
sages, is often used without proper security considera-
tions. Especially in the context of postMessages, this is
a dangerous trend: more than 65% of the sites we ana-
lyzed for 2016 either send or receive such messages, with
a steady increase in the previous years (see Figure 6). As
shown by Son and Shmatikov [31], improper handling of
postMessages can result in exploitable ﬂaws. Hence, any
technology added to browsers should default to a secure
state similar to CORS.
Complexity of Deploying Security Measures During
the course of our study, a number of new security mech-
anisms were introduced in browsers. In our analysis, we
found that the rate of adoption varies greatly for the dif-
ferent technologies. As an example, within two years
of being fully supported by the three major browsers,
the X-Frame-Options header was deployed by 20%
of the sites we analyzed, within four years its adoption
rate even reached more than 40%. In contrast, although
CSP has been fully supported since 2012, even after four
years, only about 10% of the sites we analyzed deployed
such a policy. The main difference between the two types
of measures is the applicability to legacy applications:
XFO can be selectively deployed to HTML pages which
might be at risk of a clickjacking attack. In contrast, CSP
needs to be adopted site-wide to mitigate a Cross-Site
Scripting. Moreover, previous work [37, 38] has shown
that deploying a usable CSP policy is non-trivial, espe-
cially considering the multitude of third-party compo-
nents in modern Web apps. In contrast, even though dep-
recated by CSP, the X-Frame-Options header still shows
increased usage in 2016. Hence, we ﬁnd that the more ef-
fort needs to be put into securing a site with a speciﬁc se-
curity mechanism, the less likely sites in our study were
to adopt the mechanism.
982    26th USENIX Security Symposium
USENIX Association
6.3 Correlating Vulnerabilities and Aware-
ness Indicators
To understand whether there is a correlation between ac-
tual vulnerabilities and the general understanding of se-
curity concepts for the Web, we compared the set of sites
vulnerable against Client-Side XSS attacks with their use
of security indicators. The intuition here is that the use
of a security indicator implies a more secure site. We
chose Client-Side XSS speciﬁcally, since a vulnerability
can be proven, whereas it is, e.g., unclear if usage of an
outdated library could actually lead to an exploit.
The results of this analysis are shown in Figure 11: for
each indicator we checked how many sites were vulner-
able against a Client-Side XSS attack. To reduce noise,
we only included an indicator for a given year if it was
present on at least 10% of the analyzed sites. Hence, we
exclude all years before 2009, since (as shown in Fig-
ure 10) no security measure was deployed on at least
10% of the sites. For each year, we plot the fraction
of sites which carry the indicator and are susceptible to
XSS. In addition, the graph shows the baseline as all sites
that do not have any indicator.
HTTP-only Cookies When considering the httponly
cookie ﬂag, the results are surprising: In our dataset,
its presence actually correlates with a higher vulnerabil-
ity ratio compared to cases in which no indicators are
found. Note however, that our study focusses on a small
data set of 500 domains per years, and hence the results
are not statistically signiﬁcant. Even though the overall
numbers are too small to produce signiﬁcant results, this
trend is counter-intuitive. We leave a more detailed in-
vestigation of this observation by analyzing a large body
of sites to future work. It is noteworthy, however, that
previous work from Vasek and Moore [36] investigated
risk factors for server-side compromise, and found that
httponly cookies are negative risk factors. Similar to
our work, however, there ﬁndings were inconclusive due
to a limited sample set. Comparing server- and client-
side vulnerabilities with respect to the use of httponly
cookies, however, is an interesting alley for future work.
The correlation between httponly cookies and in-
creased fraction of vulnerabilities might be caused by
several reasons: applications that use session cookies are
more likely to have a larger code base and thus, more
vulnerabilities. For 2011, the year with the highest frac-
tion of vulnerable sites, we therefore analyzed the aver-
age number of instructions for domains with httponly
cookies and found that they only have a code base which
is about 10% larger than an average Web site, with a
comparable average cyclomatic complexity. Another po-
tential reason for our ﬁndings might be the fact that de-
velopers underestimate the dangers of an XSS vulnera-
Figure 11: Security Headers vs. Client-Side XSS
bility. Although taking over a session of a user might be
considered the worst-case scenario which is averted by
HTTP-only cookies, attackers may leverage an XSS ex-
ploit for many other attacks, e.g., XSS worms or stealing
passwords from password managers [33].
Early Adopters For X-Frame-Options and HSTS
(considered from 2010 and 2013 respectively) we see an-
other trend: early adopters of new security mechanisms
are less likely to be susceptible to Client-Side XSS at-
tacks, even though the code bases for these sites are also
about 10% larger than an average site. We ﬁnd that for
both XFO and HSTS, the ﬁrst two considered years show
no vulnerabilities. However, until the end of our study
in 2016, more and more sites deploy both headers, re-
sulting in vulnerability rates comparable to sites without
security indicators. Hence, we ﬁnd that the late adopters
of such new technologies are more likely to introduce
Client-Side XSS vulnerabilities in their sites.
CSP Deployment Another insight here is the fact that
not a single site using CSP had a vulnerability, even leav-
ing out the 10% threshold discussed above. It is impor-
tant to note that even a valid CSP policy would not have
stopped exploitation of a Client-Side XSS issue: since
our analysis was conducted on the Archive.org data, CSP
policies would not be interpreted by the browser since
they all carried an X-Archive-Orig- preﬁx. The rea-
son for lack of Client-Side XSS on these sites is likely
twofold: either companies invested enough in their secu-
rity to go through the tedious process of setting up CSP
in general have better security practices, or this again
shows the early adoption effect we observed for XFO and
HSTS.
USENIX Association
26th USENIX Security Symposium    983
200920102011201220132014201520160%5%10%15%20%25%30%NosecurityindicatorHTTP-onlyCookiesContentSniffingX-Frame-OptionsHSTS6.4 Going Forward
In our study, we found a number of recurring patterns in
the Web’s insecurity, i.e., that deployment heavily hinges
on the ease of use, optional security mechanisms are
rarely used, and that several vulnerabilities are still in
existence even though they have been known for many
years. Therefore, in the following, motivated by our ﬁnd-
ings, we discuss how Web security can move forward.
Ease of Use Considering the security technologies we
investigated, we ﬁnd that regardless of the potential
beneﬁt, a security measures adoption rate is controlled
mostly by the ease of its deployment. While CSP al-
lows very ﬁne-grained control over resources that can be
accessed and — more importantly — script code which
can be executed, setting up a working CSP policy is of-
ten tough. Apart from this hurdle, signiﬁcant changes
on the application itself are required. This high effort
must be considered a big roadblock for CSP’s success.
In contrast, headers like HSTS or XSO, which are easy
to deploy and address a single issue, are adopted more
swiftly in a shorter timeframe. Thus, we argue that for
future techniques ease of use should be a primary design
concern.
Make Security Mandatory Our ﬁndings highlight
that if security checks are optional, they are oftentimes
not used, as evidenced, e.g., by the lack of origin check-
ing on postMessages. Moreover, if there is an easy way
to ensure utility, e.g., through using a wildcard, develop-
ers tend to follow that path. Hence, we argue that new
technology should ship with mandatory security, which
either does not allow for generic wildcards or in that
case, following the approach taken by CORS, limit the