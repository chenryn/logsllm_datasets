On the other hand, network attackers are motivated to gain access to such information. To achieve this, they may either eavesdrop on a plaintext connection (in the case of a passive attacker) or attempt to manipulate the connection to the point where encryption is dropped, for example, through SSL stripping attacks [13]. Additionally, web developers might inadvertently transmit sensitive information over insecure channels. A common example is the use of cookies without setting the secure flag, which results in the cookies being transferred over any connection to the domain, regardless of whether HTTPS is used.

To prevent both active attackers from stripping SSL and unknowing developers from accidentally creating insecure applications, browsers implement HTTP Strict Transport Security (HSTS). HSTS instructs browsers to connect to a domain only via HTTPS, using a validated certificate, regardless of the URL. However, HSTS is only relevant for sites that operate over HTTPS. The Archive.org crawler does not record whether a site was accessed via HTTP or HTTPS, so we cannot determine if a site was originally using HTTPS based on historical data. Furthermore, setting an HSTS header on an unencrypted connection has no effect, as it is ignored by the browser [13].

Support for HSTS was first introduced in Chrome and Firefox in 2010. In addition to the HSTS header, browsers also maintain a preload list of domains that are allowed to connect only via HTTPS, irrespective of the presence of the HSTS header. For our analysis, we considered both the HSTS headers and entries in the preload list for January of each year. Our findings show that very few domains utilized HSTS until 2012. Starting from 2013, there was a steady increase, leading to an adoption rate of almost 30% by 2016.

### 5.6 Additional Awareness Indicators for Security

In addition to the headers discussed, we identified other features that indicate awareness of potential security issues. In 2010, Bates et al. [2] demonstrated that the built-in XSS filter in Internet Explorer could be bypassed by encoding data in UTF-7 and even used to disable Clickjacking protections or conduct phishing attacks. At that time, Internet Explorer allowed websites to disable its XSS filter by sending the `X-XSS-Protection: 0` header to the client. Our study found that in 2009 and 2010, 30 and 55 sites, respectively, disabled the XSS filter in IE by sending this header. In 2009, all these sites were related to Google (including YouTube), indicating that Google was aware of the issues in IE before the 2010 publication. One possible explanation is that Google engineers were confident that their sites did not contain XSS vulnerabilities and wanted to ensure that no vulnerabilities could be introduced into otherwise bug-free sites.

A more recent feature for securing the client side is the sandbox feature of iframes in HTML5. This feature allows a site to restrict the capabilities of an iframe, such as disabling JavaScript or isolating content in a unique origin, thereby mitigating exploitable vulnerabilities in the sandboxed content. We found that only three sites used this feature in any of the HTML pages we analyzed, indicating that this feature is rarely used.

### 6 Key Insights

In this section, we discuss the key insights from our study results. We first address the takeaways on client-side technology, followed by the implications of our analysis for client-side security. Finally, we investigate the correlation between discovered vulnerabilities and the awareness indicators outlined in the previous section.

#### 6.1 Client-Side Technology

**The Web’s Complexity Continues to Rise**

In our study of the Web’s evolution, we found that although several technologies for client-side interaction have been developed over the years, JavaScript remains the predominant one. Moreover, the general complexity of JavaScript has increased over time. By 2016, the average number of statements per external script had nearly reached 1,000, excluding powerful libraries like jQuery. Additionally, code often originates from remote domains. In our 2016 dataset, a domain included script content from almost twelve distinct origins, representing an increase of almost 100% since 2011.

With the introduction of powerful new APIs in browsers, such as those enabling client-to-client communication, the general complexity of client-side web applications continues to rise.

**Involvement of Third Parties**

Including content from third parties allows websites to outsource certain parts of their application, such as advertisements. However, whenever code is included from a remote domain, it may contain vulnerabilities that can compromise the including site. As the complexity of these third-party libraries increases, so does the risk of vulnerabilities. For example, while jQuery 1.0 contained 768 statements, the most recent version 1.12 (in the 1.* branch) consists of 3,541 statements. Moreover, sites rarely update their third-party components. Lauinger et al. [14] showed that many websites use outdated versions of well-known libraries, such as jQuery, which contain exploitable flaws. In our work, we found that jQuery was often used in versions with known vulnerabilities at the time of use. The fraction of domains using such vulnerable third-party libraries has remained high since 2012.

Another risk of including third-party code is that it can be arbitrarily altered by the remote site or in transit. In the recent past, this was used to conduct large-scale DDoS attacks against websites used to bypass censorship in China [21]. However, such attacks can be prevented if sites implement Subresource Integrity, which ensures that included JavaScript is executed only if it has the correct checksum [1].

**The Rise of the Multi-Origin Web**

The Web’s primary security concept is the Same-Origin Policy, which establishes a trust boundary by allowing resources from the same origin to interact with one another. In the modern Web, however, applications communicate across these boundaries, such as between an advertisement company and the actual site. Our work observed a clear trend towards interconnected sites, particularly using `postMessages`, which are used by more than 65% of the websites we analyzed in 2016. Additionally, the usage of CORS is on the rise, with 20% of the 2016 domains sending a corresponding header. Given the nature of the Internet Archive crawler, these numbers should be considered lower bounds. Therefore, we clearly identify a trend towards an interconnected, multi-origin Web, where ensuring the authenticity of exchanged data is crucial.

#### 6.2 Client-Side Security

**Client-Side XSS Remains a Persistent Issue**

One of the biggest problems on the Web is Cross-Site Scripting (XSS). In our work, we studied the prevalence of client-side XSS over the years. We found that with the advent of more powerful JavaScript applications due to Web 2.0, the number of XSS vulnerabilities in JavaScript code spiked. Between 2007 and 2012, more than 12% of the analyzed sites had at least one such vulnerability. Even though the general complexity of JS applications continued to rise after 2012, the number of vulnerable domains declined, ranging around 8% until 2016. Nevertheless, given our sample of the top 500 pages, such attacks still pose a significant threat to a large fraction of Web users, and developer training should focus more on these issues.

**Security vs. Utility**

Many new technologies introduced in browsers come with security mechanisms, such as the authenticity and integrity properties provided by the `postMessage` API. However, these features are often optional. For example, a developer may choose not to check the origin of an incoming `postMessage`. As we observed in our work, technology that enables communication across domain boundaries, such as JSONP, Flash’s ability to access remote resources, or `postMessages`, is often used without proper security considerations. This is particularly dangerous in the context of `postMessages`: more than 65% of the sites we analyzed for 2016 either send or receive such messages, with a steady increase in previous years (see Figure 6). As shown by Son and Shmatikov [31], improper handling of `postMessages` can result in exploitable flaws. Therefore, any technology added to browsers should default to a secure state, similar to CORS.

**Complexity of Deploying Security Measures**

During our study, several new security mechanisms were introduced in browsers. Our analysis found that the adoption rate varies greatly for different technologies. For example, within two years of being fully supported by the three major browsers, the `X-Frame-Options` header was deployed by 20% of the sites we analyzed, and within four years, its adoption rate reached more than 40%. In contrast, although CSP has been fully supported since 2012, even after four years, only about 10% of the sites we analyzed deployed such a policy. The main difference between the two types of measures is their applicability to legacy applications: `X-Frame-Options` can be selectively deployed to HTML pages at risk of clickjacking attacks. In contrast, CSP needs to be adopted site-wide to mitigate Cross-Site Scripting. Previous work [37, 38] has shown that deploying a usable CSP policy is non-trivial, especially considering the multitude of third-party components in modern web apps. Despite being deprecated by CSP, the `X-Frame-Options` header still shows increased usage in 2016. Therefore, we find that the more effort required to secure a site with a specific security mechanism, the less likely sites in our study were to adopt the mechanism.

#### 6.3 Correlating Vulnerabilities and Awareness Indicators

To understand whether there is a correlation between actual vulnerabilities and the general understanding of security concepts for the Web, we compared the set of sites vulnerable to Client-Side XSS attacks with their use of security indicators. The intuition here is that the use of a security indicator implies a more secure site. We chose Client-Side XSS specifically because a vulnerability can be proven, whereas it is unclear if the use of an outdated library could lead to an exploit.

The results of this analysis are shown in Figure 11: for each indicator, we checked how many sites were vulnerable to a Client-Side XSS attack. To reduce noise, we only included an indicator for a given year if it was present on at least 10% of the analyzed sites. Hence, we exclude all years before 2009, as no security measure was deployed on at least 10% of the sites. For each year, we plot the fraction of sites that carry the indicator and are susceptible to XSS. The graph also shows the baseline as all sites that do not have any indicator.

**HTTP-only Cookies**

When considering the `httponly` cookie flag, the results are surprising: in our dataset, its presence actually correlates with a higher vulnerability ratio compared to cases where no indicators are found. Note, however, that our study focuses on a small dataset of 500 domains per year, and thus the results are not statistically significant. Even though the overall numbers are too small to produce significant results, this trend is counter-intuitive. We leave a more detailed investigation of this observation by analyzing a larger body of sites to future work. It is noteworthy, however, that previous work from Vasek and Moore [36] investigated risk factors for server-side compromise and found that `httponly` cookies are negative risk factors. Similar to our work, their findings were inconclusive due to a limited sample set. Comparing server- and client-side vulnerabilities with respect to the use of `httponly` cookies is an interesting area for future research.

The correlation between `httponly` cookies and an increased fraction of vulnerabilities might be caused by several reasons: applications that use session cookies are more likely to have a larger code base and thus more vulnerabilities. For 2011, the year with the highest fraction of vulnerable sites, we analyzed the average number of instructions for domains with `httponly` cookies and found that they only have a code base about 10% larger than an average website, with a comparable average cyclomatic complexity. Another potential reason for our findings might be that developers underestimate the dangers of an XSS vulnerability. Although taking over a user's session might be considered the worst-case scenario, which is averted by `httponly` cookies, attackers can leverage an XSS exploit for many other attacks, such as XSS worms or stealing passwords from password managers [33].

**Early Adopters**

For `X-Frame-Options` and HSTS (considered from 2010 and 2013, respectively), we see another trend: early adopters of new security mechanisms are less likely to be susceptible to Client-Side XSS attacks, even though their code bases are also about 10% larger than an average site. For both `X-Frame-Options` and HSTS, the first two considered years show no vulnerabilities. However, by the end of our study in 2016, more and more sites deployed both headers, resulting in vulnerability rates comparable to sites without security indicators. Therefore, we find that late adopters of such new technologies are more likely to introduce Client-Side XSS vulnerabilities in their sites.

**CSP Deployment**

Another insight is that not a single site using CSP had a vulnerability, even leaving out the 10% threshold discussed above. It is important to note that even a valid CSP policy would not have stopped the exploitation of a Client-Side XSS issue, as our analysis was conducted on Archive.org data, and CSP policies would not be interpreted by the browser due to the `X-Archive-Orig-` prefix. The lack of Client-Side XSS on these sites is likely twofold: either companies invested enough in their security to go through the tedious process of setting up CSP and generally have better security practices, or this again shows the early adoption effect we observed for `X-Frame-Options` and HSTS.

#### 6.4 Moving Forward

In our study, we found several recurring patterns in the Web’s insecurity, such as deployment heavily hinging on ease of use, optional security mechanisms rarely being used, and several vulnerabilities persisting despite being known for many years. Motivated by our findings, we discuss how Web security can move forward.

**Ease of Use**

Considering the security technologies we investigated, we find that the adoption rate of a security measure is largely controlled by the ease of its deployment. While CSP allows fine-grained control over resources that can be accessed and, more importantly, script code that can be executed, setting up a working CSP policy is often challenging. Apart from this hurdle, significant changes to the application itself are required. This high effort is a big roadblock for CSP’s success. In contrast, headers like HSTS or `X-Frame-Options`, which are easy to deploy and address a single issue, are adopted more swiftly in a shorter timeframe. Thus, we argue that ease of use should be a primary design concern for future techniques.

**Make Security Mandatory**

Our findings highlight that if security checks are optional, they are often not used, as evidenced by the lack of origin checking on `postMessages`. Moreover, if there is an easy way to ensure utility, such as using a wildcard, developers tend to follow that path. Therefore, we argue that new technology should ship with mandatory security, which either does not allow for generic wildcards or, if it does, follows the approach taken by CORS to limit the scope.