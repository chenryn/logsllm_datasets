which each numbered distributed consensus algorithm is preassigned to a replica
(usually by a simple modulus of the transaction ID). Algorithms that use this
approach include Mencius [Mao08] and Egalitarian Paxos [Mor12a].
Over a wide area network with clients spread out geographically and replicas from
the consensus group located reasonably near to the clients, such leader election leads
to lower perceived latency for clients because their network RTT to the nearest rep‐
lica will, on average, be smaller than that to an arbitrary leader.
Batching
Batching, as described in “Reasoning About Performance: Fast Paxos” on page 301,
increases system throughput, but it still leaves replicas idle while they await replies to
messages they have sent. The inefficiencies presented by idle replicas can be solved by
pipelining, which allows multiple proposals to be in-flight at once. This optimization
is very similar to the TCP/IP case, in which the protocol attempts to “keep the pipe
full” using a sliding-window approach. Pipelining is normally used in combination
with batching.
The batches of requests in the pipeline are still globally ordered with a view number
and a transaction number, so this method does not violate the global ordering prop‐
erties required to run a replicated state machine. This optimization method is dis‐
cussed in [Bol11] and [San11].
302 | Chapter 23: Managing Critical State: Distributed Consensus for Reliability
Disk Access
Logging to persistent storage is required so that a node, having crashed and returned
to the cluster, honors whatever previous commitments it made regarding ongoing
consensus transactions. In the Paxos protocol, for instance, acceptors cannot agree to
a proposal when they have already agreed to a proposal with a higher sequence num‐
ber. If details of agreed and committed proposals are not logged to persistent storage,
then an acceptor might violate the protocol if it crashes and is restarted, leading to
inconsistent state.
The time required to write an entry to a log on disk varies greatly depending on what
hardware or virtualized environment is used, but is likely to take between one and
several milliseconds.
The message flow for Multi-Paxos was discussed in “Multi-Paxos: Detailed Message
Flow” on page 297, but this section did not show where the protocol must log state
changes to disk. A disk write must happen whenever a process makes a commitment
that it must honor. In the performance-critical second phase of Multi-Paxos, these
points occur before an acceptor sends an Accepted message in response to a pro‐
posal, and before the proposer sends the Accept message, because this Accept mes‐
sage is also an implicit Accepted message [Lam98].
This means that the latency for a single consensus operation involves the following:
• One disk write on the proposer
• Parallel messages to the acceptors
• Parallel disk writes at the acceptors
• The return messages
There is a version of the Multi-Paxos protocol that’s useful for cases in which disk
write time dominates: this variant doesn’t consider the proposer’s Accept message to
be an implicit Accepted message. Instead, the proposer writes to disk in parallel with
the other processes and sends an explicit Accept message. Latency then becomes pro‐
portional to the time taken to send two messages and for a quorum of processes to
execute a synchronous write to disk in parallel.
If latency for performing a small random write to disk is on the order of 10 millisec‐
onds, the rate of consensus operations will be limited to approximately 100 per
minute. These times assume that network round-trip times are negligible and the
proposer performs its logging in parallel with the acceptors.
As we have seen already, distributed consensus algorithms are often used as the basis
for building a replicated state machine. RSMs also need to keep transaction logs for
recovery purposes (for the same reasons as any datastore). The consensus algorithm’s
Distributed Consensus Performance | 303
log and the RSM’s transaction log can be combined into a single log. Combining these
logs avoids the need to constantly alternate between writing to two different physical
locations on disk [Bol11], reducing the time spent on seek operations. The disks can
sustain more operations per second and therefore, the system as a whole can perform
more transactions.
In a datastore, disks have purposes other than maintaining logs: system state is gener‐
ally maintained on disk. Log writes must be flushed directly to disk, but writes for
state changes can be written to a memory cache and flushed to disk later, reordered to
use the most efficient schedule [Bol11].
Another possible optimization is batching multiple client operations together into
one operation at the proposer ([Ana13], [Bol11], [Cha07], [Jun11], [Mao08],
[Mor12a]). This amortizes the fixed costs of the disk logging and network latency
over the larger number of operations, increasing throughput.
Deploying Distributed Consensus-Based Systems
The most critical decisions system designers must make when deploying a consensus-
based system concern the number of replicas to be deployed and the location of those
replicas.
Number of Replicas
In general, consensus-based systems operate using majority quorums, i.e., a group of
2f +1 replicas may tolerate f failures (if Byzantine fault tolerance, in which the sys‐
tem is resistant to replicas returning incorrect results, is required, then 3f +1 replicas
may tolerate f failures [Cas99]). For non-Byzantine failures, the minimum number of
replicas that can be deployed is three—if two are deployed, then there is no tolerance
for failure of any process. Three replicas may tolerate one failure. Most system down‐
time is a result of planned maintenance [Ken12]: three replicas allow a system to
operate normally when one replica is down for maintenance (assuming that the
remaining two replicas can handle system load at an acceptable performance).
If an unplanned failure occurs during a maintenance window, then the consensus sys‐
tem becomes unavailable. Unavailability of the consensus system is usually unaccept‐
able, and so five replicas should be run, allowing the system to operate with up to two
failures. No intervention is necessarily required if four out of five replicas in a con‐
sensus system remain, but if three are left, an additional replica or two should be
added.
If a consensus system loses so many of its replicas that it cannot form a quorum, then
that system is, in theory, in an unrecoverable state because the durable logs of at least
one of the missing replicas cannot be accessed. If no quorum remains, it’s possible
that a decision that was seen only by the missing replicas was made. Administrators
304 | Chapter 23: Managing Critical State: Distributed Consensus for Reliability
may be able to force a change in the group membership and add new replicas that
catch up from the existing one in order to proceed, but the possibility of data loss
always remains—a situation that should be avoided if at all possible.
In a disaster, administrators have to decide whether to perform such a forceful recon‐
figuration or to wait for some period of time for machines with system state to
become available. When such decisions are being made, treatment of the system’s log
(in addition to monitoring) becomes critical. Theoretical papers often point out that
consensus can be used to construct a replicated log, but fail to discuss how to deal
with replicas that may fail and recover (and thus miss some sequence of consensus
decisions) or lag due to slowness. In order to maintain robustness of the system, it is
important that these replicas do catch up.
The replicated log is not always a first-class citizen in distributed consensus theory,
but it is a very important aspect of production systems. Raft describes a method for
managing the consistency of replicated logs [Ong14] explicitly defining how any gaps
in a replica’s log are filled. If a five-instance Raft system loses all of its members except
for its leader, the leader is still guaranteed to have full knowledge of all committed
decisions. On the other hand, if the missing majority of members included the leader,
no strong guarantees can be made regarding how up-to-date the remaining replicas
are.
There is a relationship between performance and the number of replicas in a system
that do not need to form part of a quorum: a minority of slower replicas may lag
behind, allowing the quorum of better-performing replicas to run faster (as long as
the leader performs well). If replica performance varies significantly, then every fail‐
ure may reduce the performance of the system overall because slow outliers will be
required to form a quorum. The more failures or lagging replicas a system can toler‐
ate, the better the system’s performance overall is likely to be.
The issue of cost should also be considered in managing replicas: each replica uses
costly computing resources. If the system in question is a single cluster of processes,
the cost of running replicas is probably not a large consideration. However, the cost
of replicas can be a serious consideration for systems such as Photon [Ana13], which
uses a sharded configuration in which each shard is a full group of processes running
a consensus algorithm. As the number of shards grows, so does the cost of each addi‐
tional replica, because a number of processes equal to the number of shards must be
added to the system.
The decision about the number of replicas for any system is thus a trade-off between
the following factors:
• The need for reliability
• Frequency of planned maintenance affecting the system
Deploying Distributed Consensus-Based Systems | 305
• Risk
• Performance
• Cost
This calculation will be different for each system: systems have different service level
objectives for availability; some organizations perform maintenance more regularly
than others; and organizations use hardware of varying cost, quality, and reliability.
Location of Replicas
Decisions about where to deploy the processes that comprise a consensus cluster are
made based upon two factors: a trade-off between the failure domains that the system
should handle, and the latency requirements for the system. Multiple complex issues
are at play in deciding where to locate replicas.
A failure domain is the set of components of a system that can become unavailable as
a result of a single failure. Example failure domains include the following:
• A physical machine
• A rack in a datacenter served by a single power supply
• Several racks in a datacenter that are served by one piece of networking
equipment
• A datacenter that could be rendered unavailable by a fiber optic cable cut
• A set of datacenters in a single geographic area that could all be affected by a sin‐
gle natural disaster such as a hurricane
In general, as the distance between replicas increases, so does the round-trip time
between replicas, as well as the size of the failure the system will be able to tolerate.
For most consensus systems, increasing the round-trip time between replicas will also
increase the latency of operations.
The extent to which latency matters, as well as the ability to survive a failure in a par‐
ticular domain, is very system-dependent. Some consensus system architectures don’t
require particularly high throughput or low latency: for example, a consensus system
that exists in order to provide group membership and leader election services for a
highly available service probably isn’t heavily loaded, and if the consensus transaction
time is only a fraction of the leader lease time, then its performance isn’t critical.
Batch-oriented systems are also less affected by latency: operation batch sizes can be
increased to increase throughput.
It doesn’t always make sense to continually increase the size of the failure domain
whose loss the system can withstand. For instance, if all of the clients using a consen‐
sus system are running within a particular failure domain (say, the New York area)
306 | Chapter 23: Managing Critical State: Distributed Consensus for Reliability
and deploying a distributed consensus–based system across a wider geographical area
would allow it to remain serving during outages in that failure domain (say, Hurri‐
cane Sandy), is it worth it? Probably not, because the system’s clients will be down as
well so the system will see no traffic. The extra cost in terms of latency, throughput,
and computing resources would give no benefit.
You should take disaster recovery into account when deciding where to locate your
replicas: in a system that stores critical data, the consensus replicas are also essentially
online copies of the system data. However, when critical data is at stake, it’s important
to back up regular snapshots elsewhere, even in the case of solid consensus–based
systems that are deployed in several diverse failure domains. There are two failure
domains that you can never escape: the software itself, and human error on the part
of the system’s administrators. Bugs in software can emerge under unusual circum‐
stances and cause data loss, while system misconfiguration can have similar effects.
Human operators can also err, or perform sabotage causing data loss.
When making decisions about location of replicas, remember that the most impor‐
tant measure of performance is client perception: ideally, the network round-trip
time from the clients to the consensus system’s replicas should be minimized. Over a
wide area network, leaderless protocols like Mencius or Egalitarian Paxos may have a
performance edge, particularly if the consistency constraints of the application mean
that it is possible to execute read-only operations on any system replica without per‐
forming a consensus operation.
Capacity and Load Balancing
When designing a deployment, you must make sure there is sufficient capacity to deal
with load. In the case of sharded deployments, you can adjust capacity by adjusting the
number of shards. However, for systems that can read from consensus group mem‐
bers that are not the leader, you can increase read capacity by adding more replicas.
Adding more replicas has a cost: in an algorithm that uses a strong leader, adding
replicas imposes more load on the leader process, while in a peer-to-peer protocol,
adding replicas imposes more load on all processes. However, if there is ample
capacity for write operations, but a read-heavy workload is stressing the system,
adding replicas may be the best approach.
It should be noted that adding a replica in a majority quorum system can potentially
decrease system availability somewhat (as shown in Figure 23-10). A typical deploy‐
ment for Zookeeper or Chubby uses five replicas, so a majority quorum requires
three replicas. The system will still make progress if two replicas, or 40%, are unavail‐
able. With six replicas, a quorum requires four replicas: only 33% of the replicas can
be unavailable if the system is to remain live.
Considerations regarding failure domains therefore apply even more strongly when a
sixth replica is added: if an organization has five datacenters, and generally runs con‐
Deploying Distributed Consensus-Based Systems | 307
sensus groups with five processes, one in each datacenter, then loss of one datacenter
still leaves one spare replica in each group. If a sixth replica is deployed in one of the
five datacenters, then an outage in that datacenter removes both of the spare replicas
in the group, thereby reducing capacity by 33%.
Figure 23-10. Adding an extra replica in one region may reduce system availability.
Colocating multiple replicas in a single datacenter may reduce system availability: here,
there is a quorum without any redundancy remaining.
If clients are dense in a particular geographic region, it is best to locate replicas close
to clients. However, deciding where exactly to locate replicas may require some care‐
ful thought around load balancing and how a system deals with overload. As shown
in Figure 23-11, if a system simply routes client read requests to the nearest replica,
then a large spike in load concentrated in one region may overwhelm the nearest rep‐
lica, and then the next-closest replica, and so on—this is cascading failure (see Chap‐
ter 22). This type of overload can often happen as a result of batch jobs beginning,
especially if several begin at the same time.
We’ve already seen the reason that many distributed consensus systems use a leader
process to improve performance. However, it’s important to understand that the
leader replicas will use more computational resources, particularly outgoing network
capacity. This is because the leader sends proposal messages that include the pro‐
posed data, but replicas send smaller messages, usually just containing agreement
with a particular consensus transaction ID. Organizations that run highly sharded
consensus systems with a very large number of processes may find it necessary to
ensure that leader processes for the different shards are balanced relatively evenly
across different datacenters. Doing so prevents the system as a whole from being bot‐
tlenecked on outgoing network capacity for just one datacenter, and makes for much
greater overall system capacity.
308 | Chapter 23: Managing Critical State: Distributed Consensus for Reliability
Figure 23-11. Colocating leader processes leads to uneven bandwidth utilization
Another downside of deploying consensus groups in multiple datacenters (shown by
Figure 23-11) is the very extreme change in the system that can occur if the datacen‐
ter hosting the leaders suffers a widespread failure (power, networking equipment
failure, or fiber cut, for instance). As shown in Figure 23-12, in this failure scenario,
all of the leaders should fail over to another datacenter, either split evenly or en masse
into one datacenter. In either case, the link between the other two datacenters will
suddenly receive a lot more network traffic from this system. This would be an inop‐
portune moment to discover that the capacity on that link is insufficient.
Figure 23-12. When colocated leaders fail over en masse, patterns of network utilization
change dramatically
However, this type of deployment could easily be an unintended result of automatic
processes in the system that have bearing on how leaders are chosen. For instance:
Deploying Distributed Consensus-Based Systems | 309
• Clients will experience better latency for any operations handled via the leader if
the leader is located closest to them. An algorithm that attempts to site leaders
near the bulk of clients could take advantage of this insight.
• An algorithm might try to locate leaders on machines with the best performance.
A pitfall of this approach is that if one of the three datacenters houses faster
machines, then a disproportionate amount of traffic will be sent to that datacen‐
ter, resulting in extreme traffic changes should that datacenter go offline. To
avoid this problem, the algorithm must also take into account distribution bal‐
ance against machine capabilities when selecting machines.
• A leader election algorithm might favor processes that have been running longer.
Longer-running processes are quite likely to be correlated with location if soft‐
ware releases are performed on a per-datacenter basis.
Quorum composition
When determining where to locate replicas in a consensus group, it is important to
consider the effect of the geographical distribution (or, more precisely, the network
latencies between replicas) on the performance of the group.
One approach is to spread the replicas as evenly as possible, with similar RTTs
between all replicas. All other factors being equal (such as workload, hardware, and
network performance), this arrangement should lead to fairly consistent performance
across all regions, regardless of where the group leader is located (or for each member
of the consensus group, if a leaderless protocol is in use).
Geography can greatly complicate this approach. This is particularly true for intra-