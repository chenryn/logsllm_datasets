Malicious campaigns that are executed through compro-
mised accounts will, in general, fail to match the expected
behavior of a vast majority of their victims. One reason is
that it is very difﬁcult for an attacker to make certain fea-
tures look normal, even if the attacker has detailed knowl-
edge about the history of a victim account. In particular,
this is true for the application source and the links features.
Consider a user who always posts using her favorite Twit-
ter client (e.g., from her iPhone). Since the attacker does
not control this third-party application, and the social net-
work (Twitter) automatically appends the source informa-
tion to the message, a malicious message will not match the
victim’s history. Furthermore, to send messages from an
iPhone application, the attacker would have to instrument
a physical iPhone device to log into the victims’ accounts
and post the malicious messages. Clearly, such an attack
model does not scale. To satisfy the link model, an attacker
would need to host his malicious page on a legitimate, third-
party domain (one of the domains that the user has linked
to in the past). It is very unlikely that an attacker can com-
promise arbitrary third-party sites that the different victims
have referenced in the past.
Other feature models can be matched more easily, as-
suming that the attacker has full knowledge about the his-
tory of a victim account.
In particular, it is possible to
post at an expected time, use a language that the victim
has used in the past, and craft the message so that both
the topic and direct user interactions match the observed
history. However, crafting customized messages is very
resource-intensive. The reason is that this would require the
attacker to gather the message history for all victim users.
Since social network sites typically rate-limit access to user
proﬁles, gathering data for many victims is a non-trivial en-
deavor (we initially faced similar limitations when perform-
ing our experiments; and we had to explicitly ask the social
networks to white-list our IP addresses).
The need to customize messages makes it also more dif-
ﬁcult to coordinate large-scale attacks. First, it requires
delaying messages for certain victims until an appropriate
time slot is reached. This could provide more time for the
social network to react and take appropriate countermea-
sures. Second, when messages have different topics, at-
tackers cannot easily perform search engine optimizations
or push popular terms, simply because victim users might
not have used these terms in the past. Also, the proxim-
ity feature can help limiting the spread of a campaign. If a
user always messages users that are close to her, the num-
ber of possible victims is reduced. Of course, the burden
for the attacker to blend his messages into the stream of his
victims decreases with the number of victims. That is, tar-
geted attacks against individuals or small groups are more
challenging to detect. However, the precision of the behav-
ioral proﬁles that COMPA generates (see Section 6.5) makes
us conﬁdent that similar mechanisms can contribute to the
problem of identifying such small-scale targeted attacks.
Overall, given the challenges to make certain features ap-
pear normal and the practical difﬁculties to craft customized
messages to satisfy the remaining models, our feature set is
robust with regard to large-scale attacks that leverage com-
promised accounts. Our experiments show that COMPA is
successful in identifying campaigns that use compromised
accounts to distribute malicious messages.
3.3 Novelty of the modelled features
We also compared our features with respect to existing
work. However, the purpose of our system is very differ-
ent from the goals of the ones proposed in previous work.
These systems generally aim at detecting accounts that have
been speciﬁcally created to spread spam or malicious con-
tent [5, 6, 7, 17]. Since these accounts are controlled in an
automated fashion, previous systems detect accounts that
always act in a similar way. Instead, we look for sudden
changes in behavior of legitimate but compromised social
network accounts. Table 1 lists in detail the features previ-
ous systems used to achieve their goals, and compares them
to the features used by our system. In particular, we studied
the works from Benvenuto et al. [5], Gao et al. [10], Grier
et al. [4], Lee et al. [6], Stringhini et al. [7], Yang et al. [17],
Cai et al. [18], and Song et al. [19].
As it can be seen, our system does not use any of the Net-
work Features or any of the Friends Features. Such features
aim to detect whether a certain account has been created au-
tomatically, therefore, they are not useful for our purpose.
The reason is that, since the proﬁles we want to detect are
legitimate ones that got compromised, these features would
look normal for such proﬁles. Also, we do not use any of the
Single Message Features. These features aim to detect a ma-
licious message when it contains words that are usually as-
sociated with malicious content (e.g., cheap drugs), or when
the URL is listed in a blacklist such as SURBL [20]. Since
we did not want to limit our approach to ﬂagging messages
that contain known malicious sites or well-known words,
we did not include such features in our models. In the fu-
ture, we could use these features to improve our system.
In COMPA, we focus on Stream Features. These features
capture the characteristics of a user’s message stream, such
as the ratio of messages that include links, or the similar-
ity among the messages. Looking at Table 1, it seems that
ﬁve of our features (except the Language and Proximity fea-
tures) have been previously used by at least one other sys-
tem. However, the way these systems use such features is
the opposite of what we do: Previous work wants to detect
similarity, while we are interested in anomalies. For ex-
ample, the message timing feature has been used by Grier
et al. [4], by Gao et al. [10], and by COMPA for building
the time of the day model. However, what previous work is
looking for are proﬁles that show a high grade of automa-
tion (by looking for proﬁles that send messages at the same
minute every hour), or for short-lived, bursty spam cam-
paigns. Instead, we want to ﬁnd proﬁles that start posting at
unusual times.
Only the user interactions feature has been used in a sim-
ilar fashion by another system. Gao et al. [3] use it as in-
dication of possibly compromised accounts. Similarly to
our system, they ﬂag any account that ever had a legiti-
mate interaction with another user, and started sending ma-
licious content at a later time. However, they identify “mali-
cious content” based only on URL blacklists and suspicious
words in the messages. Thus, they are much more limited
in their detection capabilities, and their approach mislabels
fake proﬁles that try to be stealthy by sending legitimate-
looking messages.
4 Grouping of Similar Messages
A single message that violates the behavioral proﬁle of
a user does not necessarily indicate that this user is com-
promised and the message is malicious. The message might
merely reﬂect a normal change of behavior. For example,
a user might be experimenting with new client software or
expanding her topics of interest. Therefore, before we ﬂag
an account as compromised, we require that we can ﬁnd a
number of similar messages (within a speciﬁc time interval)
that also violate the accounts of their respective senders.
This means that we cannot detect cases in which an at-
[5] [3] [4] [6] [7] [17] [18] [19] COMPA





   
 



Network Features
Avg # conn. of neighbors
Avg messages of neighbors
Friends to Followers (F2F)  
F2F of neighbors
Mutual links
User distance
Single Message Features
Suspicious content
URL blacklist
Friends features
Friend name entropy
Number of friends
Proﬁle age
Stream Features
Activity per day
Applications used
Following Rate
Language
Message length
Messages sent
Message similarity
Message timing
Proximity
Retweet ratio
Topics
URL entropy
URL ratio
URL repetition
User interaction
 
 






















 



Table 1. Comparison of the features used by previous
work
tacker posts a single, malicious message through one com-
promised account. While it is very possible that our models
would correctly identify that message as suspicious, alerting
on all behavioral proﬁle violations results in too many false
positives. Hence, we use message similarity as a second
component to distinguish malicious messages from spuri-
ous proﬁle violations. This is based on the assumption that
attackers aim to spread their malicious messages to a larger
victim population. However, it is important to note that this
does not limit COMPA to the detection of large-scale cam-
paigns. In our experiments on the Twitter platform, for ex-
ample, we only require ten similar messages per hour before
reporting accounts as compromised.
As mentioned previously, we can either ﬁrst group simi-
lar messages and then check all clustered messages for be-
havioral proﬁle violations, or we can ﬁrst analyze all mes-
sages on the social network for proﬁle violations and then
cluster only those that have resulted in violations. The lat-
ter approach offers more ﬂexibility for grouping messages,
since we only need to examine the small(er) set of messages
that were found to violate their user proﬁles. This would al-
low us to check if a group of suspicious messages was sent
by users that are all directly connected in the social graph,
or whether these messages were sent by people of a cer-
tain demographics. Unfortunately, this approach requires to
check all messages for proﬁle violations. While this is cer-
tainly feasible for the social networking provider, our access
to these sites is rate-limited in practice. Hence, we need to
follow the ﬁrst approach: More precisely, we ﬁrst group
similar messages. Then, we analyze the messages in clus-
ters for proﬁle violations. To group messages, we use the
two simple similarity measures, discussed in the following
paragraphs.
Content similarity. Messages that contain similar text
can be considered related and grouped together. To this end,
our ﬁrst similarity measure uses n-gram analysis of a mes-
sage’s text to cluster messages with similar contents. We
use entire words as the basis for the n-gram analysis. Based
on initial tests to evaluate the necessary computational re-
sources and the quality of the results, we decided to use
four-grams. That is, two messages are considered similar if
they share at least one four-gram of words (i.e., four con-
secutive, identical words).
URL similarity. This similarity measure considers two
messages to be similar if they both contain at least one
link to a similar URL. The na¨ıve approach for this simi-
larity measure would be to consider two messages similar
if they contain an identical URL. However, especially for
spam campaigns, it is common to include identiﬁers into
the query string of a URL (i.e., the part in a URL after the
question mark). Therefore, this similarity measure discards
the query string and relies on the remaining components of
a URL to assess the similarity of messages. Of course, by
discarding the query string, the similarity measure might be
incorrectly considering messages as similar if the target site
makes use of the query string to identify different content.
Since YouTube and Facebook use the query string to
address individual content, this similarity measure discards
URLs that link to these two sites.
Many users on social networking sites use URL shorten-
ing services while adding links to their messages. In prin-
ciple, different short URLs could point to the same page,
therefore, it would make sense to expand such URLs, and
perform the grouping based on the expanded URLs. Un-
fortunately, for performance reasons, we could not expand
short URLs in our experiments. On Twitter, we observe sev-
eral million URLs per day (most of which are shortened).
This exceeds by far the limits imposed by any URL short-
ening service.
We do not claim that our two similarity measures rep-
resent the only ways in which messages can be grouped.
However, as the evaluation in Section 6 shows, the similar-
ity measures we chose perform very well in practice. Fur-
thermore, our system can be easily extended with additional
similarity measures if necessary.
5 Compromised Account Detection
Our approach groups together similar messages that are
generated in a certain time interval. We call this the ob-
servation interval. For each group, our system checks all
accounts to determine whether each message violates the
corresponding account’s behavioral proﬁle. Based on this
analysis, our approach has to make a ﬁnal decision about
whether an account is compromised or not.
Suspicious groups. A group of similar messages is