title:Performance Sensitive Replication in Geo-distributed Cloud Datastores
author:Shankaranarayanan P. N. and
Ashiwan Sivakumar and
Sanjay G. Rao and
Mohit Tawarmalani
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Performance sensitive replication in geo-distributed
cloud datastores
Shankaranarayanan P N, Ashiwan Sivakumar, Sanjay Rao, Mohit Tawarmalani
{spuzhava,asivakum,sanjay,mtawarma}@purdue.edu
Purdue University
Abstract—Modern web applications face stringent require-
ments along many dimensions including latency, scalability, and
availability. In response, several geo-distributed cloud datastores
have emerged in recent years. Customizing datastores to meet
application SLAs is challenging given the scale of applications,
and their diverse and dynamic workloads. In this paper, we
tackle these challenges in the context of quorum-based systems
(e.g. Amazon Dynamo, Cassandra), an important class of cloud
storage systems. We present models that optimize percentiles of
response time under normal operation and under a data-center
(DC) failure. Our models consider factors like the geographic
spread of users, DC locations, consistency requirements and
inter-DC communication costs. We evaluate our models using
real-world traces of three applications: Twitter, Wikipedia and
Gowalla on a Cassandra cluster deployed in Amazon EC2. Our
results conﬁrm the importance and effectiveness of our models,
and highlight the beneﬁts of customizing replication in cloud
datastores.
I. INTRODUCTION
Interactive web applications face stringent requirements on
latency, and availability. Service level agreements (SLAs)
often require bounds on the 90th (and higher) percentile
latencies [33], which must be met while scaling to hundreds
of thousands of geographically dispersed users. Applications
require 5 9’s of availability or higher, and must often be op-
erational despite downtime of an entire DC. Failures of entire
DCs may occur due to planned maintenance (e.g. upgrade of
power, cooling and network systems), and unplanned failure
(e.g. power outages, and natural disasters) [33], [24], [4], [8]
(Figure 1). Application latencies and downtime directly impact
business revenue [7].
In response to these challenges, a number of systems that
replicate data across geographically distributed data-centers
(DCs) have emerged in recent years [24], [38], [33], [26], [23],
[40], [15], [8]. An important requirement on these systems is
the need to support consistent updates on distributed replicas,
and ensure both low write and read latencies. This is necessi-
tated given datastores target interactive web applications that
involve reads and writes by geographically distributed users
(e.g. Facebook timelines, collaborative editing). Consequently,
a distinguishing aspect of cloud datastores is the use of
algorithms (e.g., quorum protocols [38], [33], Paxos [24], [15],
[8]) to maintain consistency across distributed replicas.
number of replicas maintained, which DCs contain what data,
as well as the underlying consistency parameters (e.g., quorum
sizes in a quorum based system). Replica placement techniques
in traditional Content Delivery Networks (CDNs) (e.g., [45])
do not apply because consistency has to be maintained with
distributed writes while maintaining low latencies. Tailoring
cloud datastores to application workloads is especially chal-
lenging given the scale of applications (potentially hundreds of
thousands of data items), workload diversity across individual
data items (e.g. celebrities and normal users in Twitter have
very different workload patterns), and workload dynamics (e.g.
due to user mobility, changes in social graph etc.)
The problem of customizing replication policies in cloud
datastores to application workloads has received limited sys-
tematic attention. Some datastores like [38], [33] are based
on consistent hashing, which limits their ﬂexibility in placing
replicas. Other datastores like [41], [40] assume that all data is
replicated everywhere, which may be prohibitively expensive
for large applications. While a few datastores can support
ﬂexible replication policies [24], [48],
they require these
replication decisions to be conﬁgured manually which is a
daunting task.
Fig. 1. Downtime and number of failure episodes (aggregated per
year) of the Google App Engine data store obtained from [5].
In this paper, we present frameworks that can automatically
determine how best to customize the replication conﬁguration
of geo-distributed datastores to meet desired application ob-
jectives. We focus our work on systems such as Amazon’s
Dynamo [33], and Cassandra [38] that employ quorum pro-
tocols. We focus on quorum-based systems given their wide
usage in production [33], [38], the rich body of theoretical
work they are based on [30], [28], [50], [43], and given the
availability of an open-source quorum system [38]. However,
we believe our frameworks can be extended to other classes
of cloud storage systems as well.
Achieving low read and write latencies with cloud datastores
while meeting the consistency requirements is a challenge.
Meeting these goals requires developers to carefully choose the
We focus on optimization frameworks to obtain insights into
the fundamental limits on application latency achievable for a
given workload while meeting the consistency requirement.
978-1-4799-2233-8/14 $31.00 © 2014 IEEE
978-1-4799-2233-8/14 $31.00 © 2014 IEEE
978-1-4799-2233-8/14 $31.00 © 2014 IEEE
DOI 10.1109/DSN.2014.34
DOI 10.1109/DSN.2014.34
DOI 10.1109/DSN.2014.34
240
240
240
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:03:40 UTC from IEEE Xplore.  Restrictions apply. 
Our models are distinguished from quorum protocols in the
theoretical distributed systems community [30], [28], [50],
[43], in that we focus on new aspects that arise in the context
of geo-distributed cloud datastores. In particular, our models
consider the impact of DC failures on datastore latency, and
guide designers towards replica placements that ensure good
latencies even under failures. Further, we optimize latency
percentiles, allow different priorities on read and write trafﬁc,
and focus on realistic application workloads in wide-area
settings.
We validate our models using traces of three popular appli-
cations: Twitter, Wikipedia and Gowalla, and through experi-
ments with a multi-region Cassandra cluster [38] spanning all
8 EC2 geographic regions. While latencies with Cassandra
vary widely across different replication conﬁgurations, our
framework generates conﬁgurations which perform very close
to predicted optimal on our multi-region EC2 setup. Further,
our schemes that explicitly optimize latency under failure are
able to out-perform failure-agnostic schemes by as much as
55% under the failure of a DC while incurring only modest
penalties under normal operation. Our results also show the
importance of choosing conﬁgurations differently across data
items of a single application given the heterogeneity in work-
loads. For instance, our Twitter trace required 1985 distinct
replica conﬁgurations across all items, with optimal conﬁgu-
rations for some items often performing poorly for other items.
Overall the results conﬁrm the importance and effectiveness
of our frameworks in customizing geo-distributed datastores
to meet the unique requirements of cloud applications.
II. REPLICATION IN GEO-DISTRIBUTED DATASTORES
A commonly used scheme for geo-replicating data is to
use a master-slave system, with master and slave replicas
located in different DCs, and data asynchronously copied to
the slave [2], [4]. However, slaves may not be completely syn-
chronized with the master when a failure occurs. The system
might serve stale data during the failure, and application-level
reconciliation may be required once the master recovers [4],
[8]. On the other hand, synchronized master-slave systems
ensure consistency but face higher write latencies.
To address these limitations with master-slave systems,
many geo-distributed cloud storage systems [24], [15], [8],
[40], [15], [19], [48], [37], [41], [26] have been developed in
the recent years. A distinguishing aspect of cloud datastores is
the use of algorithms to maintain consistency across distributed
replicas, though they differ in their consistency semantics and
algorithms used. Systems like Spanner [24] provide database-
like transaction support while other systems like EIGER[40]
and COPS[41] offer weaker guarantees, primarily with the
goal of achieving lower latency.
Quorum-based datastores: Quorum protocols have been
extensively used in the distributed systems community for
managing replicated data [30]. Under quorum replication, the
datastore writes a data item by sending it to a set of replicas
(called a write quorum) and reads a data item by fetching
it from a possibly different set of replicas (called a read
Fig. 2. Replica conﬁguration across schemes for a set of Twitter
data items. Reads/writes are mapped to the nearest Amazon EC2
DC. While all 8 EC2 regions (and 21 Availability Zones) were used
to compute the conﬁgurations for all schemes, only DCs that appear
in at least one solution are shown. For clarity, placement with N-1C
is not shown.
quorum). While classical quorum protocols [30] guarantee
strong consistency, many geo-distributed datastores such as
Dynamo [33], and Cassandra [38] employ adapted versions
of the quorum protocol, and sacriﬁce stronger consistency for
greater availability [33]. In these systems, reads (or writes)
are sent to all replicas, and the read (or write) is deemed
successful if acknowledgments are received from a quorum.
In case the replicas do not agree on the value of the item
on a read, typically, the most recent value is returned to the
user [33], [38], and a background process is used to propagate
this value to other replicas. Replication in these systems can
be conﬁgured so as to satisfy the strict quorum property:
R + W > N
(1)
where N is the number of replicas, R and W are the read
and write quorum sizes respectively. This ensures that any
read and write quorum of a data item intersect. Conﬁguring
replication with the strict quorum property in Cassandra and
Dynamo guarantees read-your-writes consistency [51]. Fur-
ther, any read to a data item sees no version older than the
last complete successful write for that item (though it may see
any later write that is unsuccessful or is partially complete).
Finally, note that Dynamo and Cassandra can be explicitly
conﬁgured with weaker quorum requirements leading to even
weaker consistency guarantees [14].
III. MOTIVATING EXAMPLE
In using cloud storage systems, application developers must
judiciously choose several parameters such as the number
of replicas (N ), their location, and read(R) and write(W )
quorum sizes. In this section, we illustrate the complexity in
the problem using a real example, and highlight the need for
a systematic framework to guide these choices. The example
is from a real Twitter trace (Section VII-A), and represents a
set of users in the West Coast who seldom tweet but actively
follow friends in Asia and the East Coast.
Figure 2 depicts the placement with multiple replica con-
ﬁguration schemes. The DC locations and inter-DC delays
were based on Amazon EC2, and we required that at most
one replica may be placed in any EC2 Availability Zone
(AZ). Table I summarizes the performance of the schemes.
Our primary performance metric is the quorum latency, which
241241241
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:03:40 UTC from IEEE Xplore.  Restrictions apply. 
for the purpose of this example is the maximum of the read
and write latency from any DC. The read (write) latency in a
quorum datastore is the time to get responses from as many
replicas as the read (write) quorum size. Our frameworks are
more general and can generate conﬁgurations optimized for
different priorities on read and write latencies. We discuss
possible schemes:
User centric: This scheme is representative of traditional CDN
approaches and aims to place replicas as close to users as
possible with no regard to quorum requirements. In the limit,
replicas are placed at all DCs from which accesses to the
data item arrive (USW-1, APS-1, and USE-1 in our example).
It may be veriﬁed that for this choice of replicas, the best
quorum latency achievable is 186 msec, obtained with read
and write quorum sizes of 2. Note that this placement would
also be generated by the classical Facility Location problem
when facilities may be opened with zero cost.
Globally central: This scheme seeks to place replicas at a
DC which is centrally located with respect to all users by
minimizing the maximum latency from all DCs with read/write
requests. In our example,
this scheme places a replica at
USW-1. Note that for resiliency, replicas could be placed in
additional availability zones of the US West region, but the
quorum latency would still remain 186 msec.
Basic Availability: This is our model (Section VI), which
optimizes quorum latencies under normal conditions (all DCs
are operational) while ensuring the system is functional under
the failure of a single DC. This scheme chooses 4 replicas, one
at each of the DCs, as shown in Figure 2, with R = 3 and
W = 2. This conﬁguration has a quorum latency of 117msec
- a gain of 69 msec over other schemes. Intuitively, the beneﬁt
comes from our scheme’s ability to exploit the asymmetry in
read and write locations, increasing the number of replicas and
appropriately tuning the quorum sizes.
N-1 Contingency: While the Basic Availability scheme guar-
antees operations under any single DC failure, latencies could
be poor. For e.g., on the failure of APN-1, the write latency
from USE-1 increases to 258msec. Our N-1 Contingency
scheme (Section VI) suggests conﬁgurations that guarantee
optimal performance even under the failure of an entire DC.
In our example, the N-1 Contingency scheme conﬁgures 6
replicas (3 in APN-1, 2 in USE-1 and 1 in USW-1) with
R = 5 and W = 2. This conﬁguration ensures the quorum
latency remains 117 msec even under any single DC failure.
Note that this conﬁguration has the same performance as the
BA scheme under normal conditions as well.
Overall, these results indicate the need and beneﬁts for a
systematic approach to conﬁgure replication policies in cloud
datastores. Further, while our example only considers a subset
of items, applications may contain tens of thousands of groups
of items with different workload characteristics. Manually
making decisions at this scale is not feasible.
IV. SYSTEM OVERVIEW
Figure 3 shows the overview of our system. The datastore
is deployed in multiple geographically distributed DCs (or
COMPARING PERFORMANCE OF SCHEMES
TABLE I
Scheme
Quorum latency (msec)
N,R,W
Globally central
User centric
Basic Availability
N-1 Contingency
Normal
186
186
117
117
Failure
186
258
191
117
3, 2, 2
3, 2, 2
4, 3, 2
6, 5, 2
availability zones), with each data item replicated in a subset
of these DCs. Since our focus is on geo-replication, we
consider scenarios where each DC hosts exactly one replica of
each item, though our work may be easily extended to allow
multiple replicas.
Fig. 3. System overview
Applications consist of front-end application servers and
back-end storage servers. To read/write data items, an applica-
tion server contacts a ”coordinator” node in the storage layer
which is typically co-located in the same DC. The coordinator
determines where the item is replicated (e.g. using consistent
hashing or explicit directories), fetches/updates the item using
a quorum protocol, and responds to the application server.
We use the term “requests” to denote read/write accesses
from application servers to the storage service, and we con-
sider the request to “originate” from the DC where the ap-
plication server is located. We model “request latency” as the
time taken from when an application server issues a read/write
request to when it gets a response from the storage service.
It is possible that the application issues a single API call to
the storage service that accesses multiple data items. (e.g. a
multi-get call in Cassandra with multiple keys). We treat such
a call as separate requests to each data item.
Users are mapped to application servers in DCs nearest to
them through traditional DNS redirection mechanisms [49].
While application servers typically contact a coordinator in
the same DC, a coordinator in a nearby DC may be contacted
if a DC level storage service failure occurs (Section VI).
V. LATENCY OPTIMIZED REPLICATION
In this section, we present a model that can help application
developers optimize the latency seen by their applications with
a quorum-based datastore. Our overall goal is to determine the
replication parameters for each group of related data items.
These include (i) the number, and location of DCs in which
the data items must be replicated; and (ii) the read and write