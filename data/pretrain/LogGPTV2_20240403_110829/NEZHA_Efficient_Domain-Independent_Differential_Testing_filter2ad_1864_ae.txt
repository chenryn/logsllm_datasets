of 100 runs each starting from a different seed corpus of
1000 certiﬁcates.
that NEZHA yields 6 times and 3.5 times more differences per
tested input, on average, than AFL and libFuzzer respectively.
This demonstrates that driving input generation with a
single application is ill-suited for differential testing. In the
absence of a widely-adopted domain-agnostic differential test-
ing framework, we modiﬁed libFuzzer’s guidance engine to
support differential testing using global code coverage. Apart
from its guidance mechanisms, this modiﬁed libFuzzer 4 is
identical to NEZHA in terms of all other aspects of the engine
(mutations, corpus minimization etc.). Even so, as shown in
Figure 6, NEZHA still yields 30% more discrepancies per
tested input. Furthermore, NEZHA also achieves 1.3% more
code coverage.
Result 3: NEZHA ﬁnds 6 times more discrepancies than
AFL adapted to differentially test multiple applications
using a single test program for input generation.
Q4: How does the performance of NEZHA’s δ-diversity black-
box and gray-box engines compare to each other?
To compare the performance of NEZHA’s δ-diversity en-
gines, we run NEZHA on the six SSL/TLS libraries used in
our previous experiments, enabling a single guidance engine
at a time. Before evaluating NEZHA’s δ-diversity guidance, we
ensured that the discrepancies reported are a result of NEZHA’s
guidance and not attributed to NEZHA’s mutations. Indeed,
when we use NEZHA without any δ-diversity guidance, no
discrepancies were found across the SSL/TLS libraries.
Figures 7 and 8 show the relative performances of dif-
ferent δ-diversity engines in terms of the number of unique
discrepancies they discovered. Figure 7 shows the probability
of ﬁnding at least n unique discrepancies across the six tested
SSL/TLS libraries, starting from a corpus of 1000 certiﬁcates
and performing 100, 000 generations. For this experimental
4Corresponding git commit is 1f0a7ed0f324a2fb43f5ad2250fba68377076622
Overall
throughout
setting, we notice that NEZHA reports at least 57 discrepancies
with more than 90% probability regardless of the engine used.
Furthermore, all δ-diversity engines report more discrepancies
than global coverage. Figure 8 shows the rate at which each
engine ﬁnds discrepancies during execution. We observe that
both δ-diversity guidance engines report differences at higher
rates than global coverage using the same initial set of inputs.
this experiment, NEZHA’s output δ-
diversity yielded 521 discrepancies, while path δ-diversity
yielded 491 discrepancies, resulting in 30% and 22.75% more
discrepancies than using global code coverage to drive the
input generation (global coverage resulted in 400 unique
discrepancies). With respect to the coverage of the CFG that
is achieved, output δ-diversity and path δ-diversity guidance
achieves 1.38% and 1.21% higher coverage then global cover-
age guidance (graphs representing the coverage and population
increase at each generation are presented in Section XI-B).
Distributions of Discrepancies Found
26
143
348
4
48
Output δ-diversity
Path δ-diversity
Global Coverage
Fig. 9: Distribution of bugs found by NEZHA’s δ-diversity
engines versus NEZHA using global-coverage-based guid-
ance.
The distribution of the discrepancies reported by the dif-
ferent engines is presented in Figure 9. We notice that 348
discrepancies have been found by all
three guidance en-
gines, 121 discrepancies are reported using δ-diversity and
625
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:44 UTC from IEEE Xplore.  Restrictions apply. 
48 discrepancies are reported by our custom libFuzzer global
code coverage engine. This result is a clear indication that
δ-diversity performs differently than global code coverage
with respect to input generation, generating a broader set of
discrepancies for a given time budget, while exploring similar
portions of the application CFG (1.21% difference in coverage
for the same setup).
One notable result from this experiment
is that output
δ-diversity, despite being black-box, achieves equally good
coverage with NEZHA’s gray-box engines and even reports
more unique discrepancies. This is a very promising result as
it denotes that the internal state of an application can, in some
cases, be adequately approximated based on its outputs alone
assuming that there is enough diversity in the return values.
Result 4: NEZHA’s output and path δ-diversity guidance
ﬁnds 30% and 22.75% more discrepancies, respectively,
than NEZHA using global-coverage-based guidance.
However, we expect that output δ-diversity will perform
worse for applications for which the granularity of the outputs
is very coarse. For instance, the discrepancies that will be
found in an application that provides debug messages or
ﬁne-grained error codes are expected to be more than those
found in applications with less expressive outputs, (e.g., a web
application ﬁrewall that only returns ACCEPT or REJECT
based on its input). To verify this assumption, we perform
an experiment with only three SSL libraries, i.e., OpenSSL,
LibreSSL and BoringSSL,
libraries are only
returning a subset of their supported error codes, namely at
most 32, 64, 128 and 256 error codes. Our results are presented
in Figure 10. We notice that a limit of 32 error codes results in
signiﬁcantly fewer discrepancies than a more expressive set of
error values. Finally, we should note that when we decreased
this limit further, to only allow 16 possible error codes across
all three libraries, NEZHA did not ﬁnd any discrepancies.
in which all
VI. CASE STUDIES OF BUGS
In this section, we describe selected semantic and crash-
inducing bugs that NEZHA found during our experiments.
A. ClamAV File Format Validation Bugs
As described in Section II, discrepancies in the ﬁle format
validation logic across programs can have dire security impli-
cations. Here we highlight two critical bugs, where ClamAV
fails to parse specially crafted ELF and XZ ﬁles and thus
does not scan them, despite the fact that the programs that
commonly execute/extract these types of ﬁles process them
correctly. These bugs allow an attacker to launch evasion
attacks against ClamAV by injecting malware into specially
crafted ﬁles.
1) ELF - Mishandling of Malformed Header: According
to the ELF speciﬁcation [1], the ELF header contains the
e_ident[EI_CLASS] ﬁeld, which speciﬁes the type of
machine (32- or 64-bit) the ELF ﬁle is compiled to run on.
Values greater than 2 for this ﬁeld are left undeﬁned.
3
4
5
6
7
8
9
10
3
4
5
6
7
8
9
Fig. 10: Probability of ﬁnding at least n unique discrep-
ancies across OpenSSL, LibreSSL, and BoringSSL with
NEZHA running under output δ-diversity, for varying num-
bers of error codes, after 100, 000 executions (average of
100 runs, starting from a different seed corpus of 1000
certiﬁcates in each run).
In parsing ELF binaries, ClamAV differs from binutils
when it encounters illegal values in e_ident[EI_CLASS].
As shown in Listing 1, ClamAV treats ELF binaries conﬁg-
ured with such illegal values as being of an invalid format
(CL_EFORMAT) and does not scan the respective ﬁles. By
contrast, binutils correctly parses such ELF binaries. We
veriﬁed that such ELF binaries can in fact be successfully
executed. In Listing 2, the Linux kernel’s ELF loader does not
validate this ﬁeld while loading a binary. As a result, a malware
with such a corrupted ELF header can evade the detection of
ClamAV, while retaining its capability to execute in the host
OS.
1 static int cli_elf_fileheader(...) {
2
...
switch(file_hdr->hdr64.e_ident[4]) {
case 1:
...
case 2:
...
default:
...
return CL_EFORMAT;
Listing 1: ClamAV code that parses the e_ident ﬁeld.
1 static int load_elf_binary(struct linux_binprm *bprm) {
2
...
retval = -ENOEXEC;
if (memcmp(loc->elf_ex.e_ident, ELFMAG, SELFMAG) != 0)
goto out;
if (loc->elf_ex.e_type != ET_EXEC &&
loc->elf_ex.e_type != ET_DYN)
goto out;
if (!elf_check_arch(&loc->elf_ex))
goto out;
10
...
Listing 2: Error checks for ELF loading in the Linux kernel
(the e_ident ﬁeld is not checked).
2) XZ - Mishandling of the Dictionary Size Field: Accord-
ing to the XZ speciﬁcations [62], the LZMA2 decompression
algorithm in an archive can use a dictionary size ranging from
626
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:44 UTC from IEEE Xplore.  Restrictions apply. 
4kB to 4GB. The dictionary size varies from ﬁle to ﬁle and
is stored in the XZ header of a ﬁle. ClamAV differs from XZ
Utils when parsing this dictionary size ﬁeld.
1 extern lzma_ret lzma_lz_decoder_init(...) {
2
...
// Allocate and initialize the dictionary.
if (next->coder->dict.size != lz_options.dict_size) {
lzma_free(next->coder->dict.buf, allocator);
next->coder->dict.buf
= lzma_alloc(lz_options.dict_size, allocator);
...
3
4
5
6
7
8
9
10 lzma_alloc(size_t size, const lzma_allocator
*allocator) {
...
if (allocator != NULL && allocator->alloc != NULL)
ptr = allocator->alloc(allocator->opaque, 1, size);
else
ptr = malloc(size);
...
11
12
13
14
15
16
Listing 3: XZ Utils parses the dictionary size correctly.
As shown in Listing 3, XZ Utils strictly conforms to the
speciﬁcations and allocates a buffer based on the permitted
dictionary sizes. On the other hand, ClamAV includes an
additional check on the dictionary size that deviates from the
speciﬁcations. It fails to parse archives with a dictionary size
greater than 182MB (line 15 in Listing 4). As a result of
this bug, when parsing such an archive containing a malware,
ClamAV does not consider the ﬁle as an archive, and thus
skips scanning the compressed malware.
1 SRes LzmaDec_Allocate(.., const Byte *props, ...) {
2
...
dicBufSize = propNew.dicSize;
if (p->dic == 0 || dicBufSize != p->dicBufSize){
...
// Invoke __xz_wrap_alloc()
p->dic = (Byte *)alloc->Alloc(alloc, dicBufSize);
if (p->dic == 0) {
...
return SZ_ERROR_MEM;
...
12
13 void *__xz_wrap_alloc(void *unused, size_t size) {
14
// Fails if size > (182*1024*1024)
if(!size || size > CLI_MAX_ALLOCATION)
3
4
5
6
7
8
9
10
11
15
16
17
return NULL;
...
Listing 4: ClamAV’s additional erroneous check on
dictionary size.
B. X.509 Certiﬁcate Validation Discrepancies
In this Section, we present
two examples of certiﬁcate
validation semantic bugs found by NEZHA, one involving
LibreSSL and one GnuTLS. Another example of a discrep-
ancy between LibreSSL and BoringSSL is presented in the
Appendix.
1) LibreSSL - Incorrect parsing of
time ﬁeld types:
The RFC standards for X.509 certiﬁcates restrict the Time
ﬁelds to only two forms, namely the ASN.1 representations
of UTCTime (YYMMDDHHMMSSZ) and GeneralizedTime
(YYYYMMDDHHMMSSZ) [15] which are 13 and 15 characters
wide respectively. Time ﬁelds are also encoded with an
ASN.1 tag that speciﬁes their format. Despite the standards, in
practice, we observe that 11- and 17-character time ﬁelds are
used in the wild, by searching within the SSL observatory [7].
Indeed, some SSL libraries like OpenSSL and BoringSSL are
more permissive while parsing such time ﬁelds.
LibreSSL, on the other hand, tries to comply strictly with the
standards when parsing the validity time ﬁelds in a certiﬁcate.
However, while doing so, LibreSSL introduces a bug. Unlike
the other libraries, LibreSSL ignores the ASN.1 time format
tag, and infers the time format type based on the length of
the ﬁeld (Lines 10 and 16 in Listing 5). In particular, the
time ﬁelds in a certiﬁcate can be crafted to trick LibreSSL to
erroneously parse the time ﬁelds using an incorrect type. For
instance, when the time ﬁeld of ASN.1 GeneralizedTime
type is crafted to have the same length as the UTCTime (i.e.,
13), LibreSSL treats the GeneralizedTime as UTCTime.
As a result of this confusion, LibreSSL may erroneously
treat a valid certiﬁcate as not yet valid, when in fact it is valid;
or, it may erroneously accept an expired certiﬁcate. For exam-
ple, while other libraries may interpret a GeneralizedTime
time in history, 201201010101Z as Jan 1 01:01:00
2012 GMT, LibreSSL will incorrectly interpret this time as a
UTCTime time in future, i.e., as Dec 1 01:01:01 2020
GMT. Note that ﬁnding time ﬁelds of non-standard lengths
in the wild suggests that CAs do not actively enforce these
standards length requirement. Furthermore, we also found
certiﬁcates with GeneralizedTime times that are of the
length 13 in the SSL observatory dataset.
1 int asn1_time_parse(..., size_t len, ..., int mode) {
2
...
int type = 0;
/* Constrain to valid lengths. */
if (len != UTCTIME_LENGTH && len != GENTIME_LENGTH)
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
return (-1);
...
switch (len) {
case GENTIME_LENGTH:
// mode is "ignored" -- configured to 0 here
if (mode == V_ASN1_UTCTIME)
return (-1);