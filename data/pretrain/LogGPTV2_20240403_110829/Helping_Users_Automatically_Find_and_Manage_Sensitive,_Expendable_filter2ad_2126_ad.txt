Overall, 82% of ﬁles identiﬁed as sensitive or useful were
images or documents. Note that our categorization of the latter
included not just text-focused ﬁles, but also presentations and
spreadsheets. Other ﬁle types considered sensitive included
audio and video ﬁles (5%), as well as our miscellaneous cat-
egory (13%) that encompassed saved web pages, computer
code, database ﬁles, executables, and OS conﬁg ﬁles. This
fact, combined with the additional ﬁletype-speciﬁc features
available for these ﬁles, led us to focus our prediction task
(Section 7) speciﬁcally on images and documents.
6.3 Distribution of Sensitive and Useful Files
After we asked about useful and sensitive ﬁles in general,
we showed each participant dozens of ﬁles from their own
account, asking them to label and explain the usefulness, sen-
sitivity, and the desired management decision for those ﬁles.
This provided us with labels for a total of 3,525 ﬁles across
rounds. Among the ﬁles we selected (biased towards those
that are sensitive), 62% were deemed useful and 14% were
deemed sensitive. Although the overall number of ﬁles per-
ceived to be sensitive was low, 78% of our participants identi-
ﬁed at least one ﬁle as sensitive. This observation aligns with
previous studies that found a non-trivial fraction of the ﬁles
stored in the cloud are potentially sensitive [9, 24].
Table 6 summarizes perceived usefulness and sensitivity
across the ﬁle-selection categories. In Round 1, ﬁles with sen-
sitive keywords in their ﬁle names and documents were more
likely to be labeled as sensitive compared to other selection
categories. Meanwhile, the distribution of ﬁle usefulness was
fairly consistent across all categories. Figure 4, an area plot,
summarizes the distribution of ﬁle usefulness and sensitivity.
6.4 Management of Sensitive and Useful Files
Figure 5 shows participants’ desired ﬁle-management deci-
sions broken down by whether they perceived the ﬁle as sen-
USENIX Association
30th USENIX Security Symposium    1153
Sensitive                        Not SensitiveUseful, SensitiveUseful, Not sensitiveNot useful, SensitiveNot useful, Not sensitiveUsefulNotUseful10001004%10%52%34%Figure 5: Desired ﬁle management by sensitivity/usefulness.
Overall, these ﬁle-management preferences and accompa-
nying reasoning shed light on how participants conceptualize
and operationalize ﬁle management in the cloud based on
ﬁles’ perceived sensitivity and usefulness. In Section 7, we
leverage both our collected training data and these qualita-
tive observations to build Aletheia, an automated inference
approach to predict a ﬁle’s usefulness, sensitivity, and man-
agement decision. Aletheia’s ultimate goal is to assist users
in protecting (or deleting) the ﬁles most likely to be in need
of reconsideration.
6.5 Consistency of Decisions Over Time
A potential concern for self-report surveys like ours is that
participants’ answers might not be consistent over time and
thus not represent a meaningful preference. To evaluate the
stability of responses over time, we conducted a follow-up
survey approximately eight months after the initial study. This
follow-up survey was speciﬁcally designed for the 33 partic-
ipants who had participated in Round 2 of our online study
and thus had answered questions about a larger number of
ﬁles than those who had participated in Round 1. Because
we wanted to ask about a non-trivial number of decisions to
either delete or protect ﬁles, we invited the 23 participants
from that round who had desired to either delete or protect at
least 10 of the 50 ﬁles shown to them. Of these 23 qualiﬁed
participants, 16 participated in the follow-up study.
Similar to the initial study, participants were informed of
our privacy and data-collection policies as part of the consent
process. In the survey, we asked each participant to revisit a
random selection of 10 ﬁles that they had previously wanted
to delete or protect. We presented them with their previous
ﬁle-management decision and asked them to select an updated
decision and explain why they chose either the same decision
or a diﬀerent decision. Our 16 participants saw a total of 160
ﬁles, among which they initially wanted to delete 136 ﬁles and
protect 24 ﬁles. The survey took approximately 15 minutes to
complete, and the additional compensation was $7. Before we
conducted this follow-up study, our IRB approved our request
for a protocol modiﬁcation.
After 8 months, participants reported the same manage-
ment decision for 81% of the ﬁles. For ﬁles that participants
initially wanted to delete, participants wanted to continue to
delete 86% of such ﬁles. In explaining why their decisions
remained the same, participants mentioned that the ﬁles were
either embarrassing or no longer needed. For example, one
participant wrote, “Same as before: total junk.” While partic-
ipants had been made aware of the presence of these ﬁles a
few months ago in the initial study, only one participant had
actually manually deleted the ﬁles in the interim. Doing so
required them to log into their account through their normal
interface outside of our study system. To facilitate ﬁle man-
agement, automated tools that are part of users’ workﬂow can
potentially increase the feasibility of the management process
and reduce manual overhead. For a smaller portion of these
ﬁles, participants wanted to revert their initial decision from
delete to keep as-is (13%) or protect (1%). Participants stated
two prominent reasons for changing their decision. For 37%
of ﬁles for which the decision diﬀered, participants mentioned
that the ﬁle had sentimental value. For instance, one partici-
pant said, “Upon seeing the photo again, it brings back good
memories. It’s been some time since I’ve seen these photos.”
For the remaining 63% of ﬁles, participants reported realizing
the ﬁle was potentially useful. One wrote, “I thought that I
wouldn’t need this anymore, but now I think that I may.”
Among the ﬁles that participants had initially desired to
protect, they wanted to continue to do so for 48% of them.
Participants’ free-text justiﬁcations mentioned that the ﬁles
continued to be useful, yet contained sensitive information.
However, participants now wanted to delete 42% of these ﬁles
they initially wanted to protect. In a matter of months, these
ﬁles had lost their utility in participants’ eyes. These changed
decisions are consistent with longitudinal ﬁle management; a
sensitive ﬁle that is initially protected can easily be deleted
once it is no longer deemed useful, whereas the opposite is
impossible. The overall stability of participants’ preferences
over time provides further motivation for the development
of advanced mechanisms that can keep track of dynamic ﬁle
attributes longitudinally, which we elaborate on in Section 8.
7 Predicting File-Management Decisions
Because users can have hundreds or thousands of ﬁles in
their cloud storage accounts, a core goal of ours was to alle-
viate the burden of manual ﬁle management with automated
tools. In this section, we formulate the task of predicting
ﬁle-management decisions based on features automatically
collected from individual ﬁles and user accounts as a whole.
To inform the classiﬁer for ﬁle-management decisions, we
also predict user perceptions of ﬁle usefulness and sensitivity.
7.1 Prediction Tasks and Baselines
Aletheia has three prediction tasks: predicting whether a user
will perceive a ﬁle as sensitive (Task 1); predicting whether a
user will perceive a ﬁle as no longer useful (Task 2); and pre-
dicting what management decision a user will choose among
keeping, deleting, and protecting a ﬁle (Task 3). To perform
1154    30th USENIX Security Symposium
USENIX Association
020406080100PercentageDeleteProtectKeep as-isUseful,Not sensitiveUseful,SensitiveNot useful,SensitiveNot useful,Not sensitiveclassiﬁcation for each task, we compared several established
supervised learning algorithms: Decision Trees (DT), Logis-
tic Regression (LR), Random Forests (RF), Deep Neural Net-
works (DNN) with the Adam optimizer using scikit-learn [43],
and XGBoost (XGB) [8]. All model parameters were opti-
mized using grid search on the training set in each fold in
cross validation, and tested on the testing set. We use the best
performing classiﬁer, which turned out to be XGBoost for
both the preliminary classiﬁers trained on Round 1 data and
the ﬁnal classiﬁers trained on Round 2 data. We report results
only on the ﬁnal classiﬁers, which we refer to as Aletheia w/
all features, or Aletheia for short.
We compared Aletheia to multiple baselines. The ﬁrst was a
random classiﬁer (Random), which randomly assigned a man-
agement decision for each ﬁle. The second was a majority
classiﬁer (Majority), which always predicted the most fre-
quent class. For the task of predicting whether a ﬁle would be
perceived as sensitive, we employed a more meaningful third
baseline, GDLP feature count, leveraging Google’s Cloud
Data Loss Prevention API [18] (see Table 3). This baseline
ranked documents based on the number of sensitive GDLP
features identiﬁed in each document. We also tested a variant
of our model that used only the GDLP output as features for
predicting sensitivity: Aletheia w/ only GDLP features.
For predicting whether a ﬁle would be perceived as useful,
we again used the Random and Majority baselines. We also
tested two additional baselines centered on how recently the
ﬁle was last modiﬁed and how useful ﬁles of its type were
considered overall. We ordered all ﬁles by last modiﬁcation
date, from oldest to newest, and assigned them a “staleness”
score between 1 (oldest) and 0 (newest) by normalizing the
last modiﬁcation date. The Last Modiﬁed baseline predicted
the most stale ﬁles (those not modiﬁed recently) as not useful.
The Last Modiﬁed, File Type baseline augmented the staleness
score with overall statistics about the perceived usefulness of
other ﬁles with the same ﬁle extension. For every ﬁle type,
a “not-useful-type” score between 1 and 0 was calculated by
considering all ﬁles of that ﬁle type (e.g., PDFs) in the train-
ing data and calculating what percent of them were marked as
not useful. The Last Modiﬁed, File Type baseline ranked ﬁles
based on the product of their staleness and not-useful-type
scores. It allowed for ﬁles whose type is generally perceived
as less useful to be ranked higher than ﬁles whose type is gen-
erally considered more useful. To the best of our knowledge,
no prior work has attempted to predict perceptions of sensi-
tivity and usefulness or ﬁle-management decisions for ﬁles in
the cloud. We thus chose these baseline to represent common
machine learning baselines and additional baselines capturing
the most intuitive features for sensitivity and usefulness.
7.2 Dataset Description
We used the ﬁnal dataset collected in Round 2. Our dataset
consisted of tuples (X,Y), where Xi was the feature vector
and Yi was our target for prediction. The feature vector Xi,
included metadata and information on ﬁles and user accounts.
For accounts, we had the total amount of storage and the
amount used. For ﬁles, we had the size of the ﬁle, whether or
not the ﬁle was shared, the link access (view or edit), whether
or not the ﬁle was last modiﬁed by the user, and the access
type (owner, editor, viewer). For documents and images that
contained text, we extracted counts of sensitive information
discovered using the GDLP API [18]. In addition, we col-
lected a bag of words on a heuristic set of keywords. For doc-
uments, we collected an average word2vec embedding of each
document using Google News word2vec embeddings [33].
Doing so enabled us to approximate text context without
breaching the privacy of participants by having actual inter-
pretable text from their ﬁles. For images, we used the Google
Vision API [20] to obtain multiple image features. We addi-
tionally converted the labels from the API, including the “best
guess label,” to one-hot encoding representations, as well a
word2vec embedding representation, which were added to the
feature vector. These features are listed in Table 3. In addi-
tion, we also computed the following user-level statistics as
features: (1) the percentage of ﬁles in a participant’s account
with each sensitive feature (e.g., the fraction of ﬁles tagged as
adult); and (2) the percentage of ﬁles labeled as sensitive in
the training data that contained each sensitive feature. Com-
pared to Aletheia w/ only GDLP features, we considered a
broader set of ﬁle-based and user-based features.
For ﬁle-management decisions (Task 3), we used all ﬁles
for which we collected survey data. For Task 1 and Task 2,
we separated the evaluation by image ﬁles and document ﬁles
since they had diﬀerent features. The labels Y for each task
were obtained from participants’ answers to questions S-1,
U-1 and M-1 (as labeled in the survey instrument in online
Appendix B [1]). Questions S-1 and U-1 asked participants
to rate a ﬁle’s sensitivity and usefulness, respectively, on a
Likert scale. Question M-1 inquired how participants wanted
to manage the ﬁle by either deleting it, protecting it, or keep-
ing it as-is. Based on the answers to S-1, we created binary
labels for Task 1: sensitive (“strongly agree,” “agree”) and not
sensitive (“neutral,” “disagree,” “strongly disagree”). A total
of 15% of ﬁles were sensitive. Based on the answers to U-1,
we created binary labels for Task 2: not useful (“strongly dis-
agree,” “disagree”) and useful (“neutral,” “agree,” “strongly
agree”). A total of 38% of the ﬁles were not useful. Note
that for both S-1 and U-1, “neutral” responses were assigned
to the categories that we were not interested in ﬁnding (not
sensitive and useful). From the answers to M-1, we had three
labels for Task 3: delete (40%), protect (8%), and keep (52%).
7.3 Experimental setup
Tasks 1 and 2 for predicting sensitivity and usefulness had
the same setup, while predicting ﬁle-management decisions
in Task 3 used a diﬀerent setup.
USENIX Association
30th USENIX Security Symposium    1155
(a) Sensitivity of documents
(b) Sensitivity of images
(c) Usefulness of documents
(d) Usefulness of images
Figure 6: Precision vs. recall for predicting sensitivity and usefulness. We compared two versions of Aletheia (red and blue)
against a random baseline (black) and a baseline using Google’s Data Loss Prevention (GDLP) tool (magenta) for the sensitivity
dataset. For the usefulness dataset, we compared against two heuristic baselines using the last modiﬁcation date (cyan and green).
7.3.1 Task 1 and 2: Sensitivity and usefulness
We performed a nested cross-validation [40, 46, 54] and re-
port averaged results across ﬁve test folds. To this end, we
ﬁrst created ﬁve training and test folds. Within each training
fold, we further performed a ﬁve-fold cross-validation to tune
and select hyperparameters. Finally, each tuned model was
evaluated on the respective test folds and performance was
averaged across all ﬁve test folds. This allowed us to see how
the model may perform in the general setting, and it also re-
duced bias from selecting a single random test set. Note that
we did not tune any hyperparameters on the separated test
fold; it was used exclusively for evaluation.
Since we focused on ﬁnding ﬁles that participants wished
to delete or protect, we ordered examples in the test data by
the probability of being Yi = 1 (sensitive for Task 1, not useful
for Task 2), and assessed the precision and recall. This is a
common setup for evaluating binary classiﬁcation where one
label (e.g., sensitive) is more important than the other (e.g., not
sensitive). Since there were signiﬁcantly fewer sensitive and
not useful labels, we had a “needle in the haystack” problem.
We aimed for both high precision and high recall, but there is
typically a trade-oﬀ between them. Precision was computed
as T P/(T P + FP), where T P was the number of true positive
examples (actual label positive, predicted label positive), and
FP was the number of false positive examples (actual label
negative, predicted label positive). In other words, precision
was the proportion of examples predicted as bearing the label
of interest that were correctly predicted. It is also known as
the positive predictive value in information retrieval [6, 39] or
Bayesian detection rate in intrusion detection [3]. Recall was
computed as T P/(T P + FN), where FN was the number of
false negative examples (actual label positive, predicted label
negative). A precision-recall curve (PRC) allows us to see the
trade-oﬀ between precision and recall when diﬀerent possible
cutoﬀs for positive classiﬁcations are used. For example, if
we predicted the top 20% most likely ﬁles as positive, then
a point on the PRC is created which shows the exact trade-
oﬀ between including false positives in the ﬁles predicted as
positive (top 20%) and not including the false negatives, ﬁles
that should be predicted as positive but fall in the lower 80%.
7.3.2 Task 3: File-Management decision
In this task, we had three classes: delete (Yi = 1), protect
(Yi = 2), and keep (Yi = 3). Since perceptions of sensitiv-
ity and usefulness correlated highly with ﬁle-management
decisions, we wanted to leverage them in our classiﬁcation.
However, one typically does not have these labels for all ﬁles
in a user’s account. Thus, we predicted these labels using the
classiﬁers for Task 1 and 2, adding the predicted labels as
two additional features for predicting the ﬁle-management
decision. We compared the performance of adding these two
features against a classiﬁer that does not use them, a majority
classiﬁer, and an oracle with the actual perceived sensitivity
and usefulness of a ﬁle as reported by the participant.
7.4 Results
Here, we present the precision-recall curves for sensitivity
and usefulness, separated into image and document ﬁles. We
also analyze the top features for predicting sensitivity and