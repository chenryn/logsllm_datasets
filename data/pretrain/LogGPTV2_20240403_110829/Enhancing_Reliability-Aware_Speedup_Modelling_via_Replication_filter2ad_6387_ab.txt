(no)rep(P ) = E
(no)rep(P )/S
can be done by setting ∂H(no)rep(P )
∂P
= 0.
In order to validate the results that we will derive based
on this analysis, we also built a simulator to measure the
reliability-aware speedups. The basic purpose of simulator is to
compute the average time to ﬁnish a given checkpoint interval
under failures generated by a given distribution. The simulator
does that by randomly generating a failure time, say x, from
the distribution and adding it to the time taken, t (which is 0
at the beginning of the run). If x is greater than the interval
duration, the run completes, otherwise another draw is made
and the process repeated. Upon run completion, we calculate
the useful and extra work as shown in Figure 1. Since we want
the expected value, we do a large number (usually 50,000) of
such runs for a given set of system parameters and average
(no)rep(P ). For the failure
over the 50,000 runs to obtain E
distribution, when no replication is employed, the failure times
are generated according to an exponential distribution with rate
λP . In the case of replication, since the interrupt distribution
is not exponential, we generate failure times according to
the actual probability distribution for replicated failures as
follows: Let R(t) be the probability that no interrupt (deﬁned
as the failure of a processor and its replica) happens until
time t. Then, with exponential processor failures, we have
R(t) = (1 − (1 − e−λt)2)P/2 = (2e−λt − e−2λt)P/2.
Our cumulative distribution function (CDF) from which we
generate interrupts for replication is then given by 1 − R(t).
In the following subsections we will discuss the optimal
number of processors, P ∗, when using checkpoint-restart
(C/R), without and with replication respectively.
A. Without Replication
The problem of ﬁnding the optimal number of processors
with checkpoint-restart alone has been studied to some extent
in the literature. Jin et al. [2] and Zheng et al. [3] provided
procedures to numerically evaluate the optimal number of pro-
cessors given other system and application level parameters.
Cavelan et al. [1] derived closed form expressions for optimal
processor counts by taking the Taylor series expansion of the
exponential term in Equation 1 and simplifying using ﬁrst
order approximation for the failure rate (λP ). This procedure
yields the optimal number of processors as
P ∗
norep = (
1 − α
α )2/3(
2
λC )1/3
(3)
For perfectly parallel applications, i.e. α = 0, this value goes
to inﬁnity. However, the optimal number of processors for
perfectly parallel applications is a ﬁnite value as we will soon
see. The discrepancy occurs because the ﬁrst order approxi-
mation holds only when the application workload consists of
a non-negligible sequential fraction, 0  0 to show the range in which it is closer to the actual optimal values.
X-axis range is from α = 0 to α = 2 × 10−5. Individual processor MTBF
= 10 years while C = R = D = 300 seconds.
Since one of the design goals of HPC applications is that
they be highly parallelizable, scaling to thousands of cores, the
desired value of α for exascale jobs is small in order to fully
utilize the system. As α becomes small, however, the accuracy
of the ﬁrst order approximation worsens, as can be seen in
Figure 2. We can see from the ﬁgure that, for values of α
below 5∗ 10
−6, the actual optimal processor count is closer to
the optimal obtained by assuming a perfectly parallel workload
than to the value given by the ﬁrst order approximation. We
carried out similar analyses with other values of processor
MTBFs which also revealed that the threshold of α, below
which the ﬁrst order approximation become inferior, is of the
6 for the range of realistic values of processor
order of 10
MTBF (1-50 years). While this value of α seems quite small
by itself, an exascale job with this fraction of sequential
workload would not be efﬁcient in utilizing all of its allocated
processors. As an example, consider a system with 105 nodes,
which, as [11] projects, will be typical of the order of node
counts at exascale. On such a system, a job running with
α = 5 ∗ 10
−6 will spend a third of its time in sequential
execution, wasting its allocated resources.
−
it
is pertinent
Based on the discussion above,
to also
investigate the optimal processor counts of no replication
jobs (α = 0), because the results
for perfectly parallel
of such an analysis would serve as better approximations
to the performance models of exascale jobs than the ﬁrst
order approximations that assume a relatively larger value of
α. While [1] observed, based on empirical results, that the
optimal number of processors for perfectly parallel jobs is of
the order λ−1, no analytical results to that effect exist. In the
theorem below we prove that the optimal processor count is
indeed of the order λ−1 when α = 0.
Theorem 1. When using checkpoint-restart without replica-
tion, and assuming a perfectly parallel job, the optimal number
of processors that maximize the expected speedup is equal to
K/λ, where K is a constant that does not depend on λ.
Proof. The failure rate with P processors is exponential
Authorized licensed use limited to: University of New South Wales. Downloaded on October 01,2020 at 13:22:32 UTC from IEEE Xplore.  Restrictions apply. 
with rate λP . Hence, we can write Hnorep(P ) exactly using
2C
Equation 1. Taking the checkpointing interval to be τ =
λP
and assuming a perfectly parallel job, we get
(cid:5)
2C
√
(cid:5)
λP +C) − 1)
2C
λP
√
1
λP + D)eλP R (eλP (
1
P (
(cid:6)
Hnorep(P ) =
=
λ
2C (
1
λP 3/2 +
D√
P
)(eλP (
2C
λP +C+R) − eλP R
)
(cid:4)
λP 3/2 + D√
1
√
(4)
λP +C+R) − eλP R) (i.e.
λ/2C above). Differentiating wrt P ,
2C
P )(eλP (
(cid:6)
λC
Let H(P ) = (
ignoring the constant
we get
∂H
∂P = (
1
2
+
λP 3
− λR) − (
D√
P
3
2λP 5
2
((
)eλP R
D
2P 3
+
2
√
2P + λ(C + R))eλP (
λP +C+R) − eλP R
)(eλP (
√
2C
2C
λP +C )
)
(5)
(6)
(7)
(cid:6)
Setting ∂H
∂P = 0 and simplifying, we obtain
2λP ((
= (
C
2λP + C + R)eλP (
√
2C
λP +C) − 1)
3 + λP D
1 + λP D )(eλP (
(cid:6)
Setting P = K/λ, Equation 6 becomes
√
√
2C
λP +C) − R)
2K((
C
2K + C + R)eK(
√
= (
3 + KD
1 + KD )(eK(
2C
K +C) − 1)
2C
K +C) − R)
We note that λ has been eliminated from the equation above.
This means that the value of K that satisﬁes this equation does
not depend on λ, thus concluding the proof.
We use our simulator to validate the result in Theorem 1
about the order of optimal processor count without replication
for perfectly parallel jobs. Figure 3 shows the optimal num-
ber of processors as a function of the individual processor
MTBF(= 1/λ). Along with the simulation results, we also
plot a best ﬁt curve to the simulation results. The form of
the curve is assumed to be K/λ for the case of α = 0 and
K/λ1/3 (based on Equation 3) when α > 0. We see that
the best ﬁt curve for perfectly parallel workload, using the
form given by Theorem 1, matches closely with simulation
results. When α > 0,
the form given by the ﬁrst order
approximation gets closer to the simulation results as α gets
larger. Note that, in this analysis, the value of K has been
estimated using simulation, since the purpose was to assess the
growth of P ∗ with λ. For α > 0, the ﬁrst order approximation
provides a formula for K as can be seen in Equation 3. As for
perfectly parallel jobs, we see that Equation 7 cannot be solved
analytically for K. In such a case, one can resort to numerical
methods to solve that equation and compute K given C, R
and D. An alternative would be the method we used here,
which is to estimate K using simulation, as shown in Figure
3.
B. Replication
For replication, we will restrict our focus in this paper to
study the smallest degree of replication, i.e. dual replication.
As mentioned in Section II, we will use the result from [5] to
approximate the platform failure rate, λP . Thus, for a platform
with a total of P processors where half of them are replicas of
the other half, we take the failure rate to be λP ≈ λ
(cid:5)
2P/π.
We will therefore take the checkpointing interval to be τ =
2P . Using Equation 2 and the expression of Srep(P ),
(cid:4)
(cid:4)
2C
λ
π
we get the following approximation for Hrep(P )
2(1 − α)
P
2(1 − α)
P
)
)
(cid:4)
1
λ
(1 −
1
λ
(cid:4)
2P − C
(cid:6)
1
π
λ
π
2P
π
√
2P
τ − τ
(cid:5)
2
2λC
2P
π )
(8)
Hrep(P ) ≈ (α +
= (α +
j
(cid:6)
(cid:5)
(cid:5)
(cid:5)
We will now discuss the optimal processor counts for replica-
tion for the two cases, α > 0 and α = 0, separately:
2P
π )
2λC
(cid:7)∞
Non-negligible Sequential part (α > 0): From Equation 8,
we can expand the (1 −
−1 term, using Taylor
2P
j=0(2λC
2 . Assuming P is of the order
expansion, as
π )
λ−x, where x  0. In such a case, since λ is small in practice, we can make
the ﬁrst order approximation by retaining the most dominant
term with λ, which means
Hrep(P ) ≈ (α +
(cid:6)
Differentiating this with respect to P , we get
2(1 − α)
2P
π )
(cid:8)
(cid:6)
(cid:8)
)(1 +
2λC
(9)
P
2
πP
2λC
P
∂Hrep
∂P =
α
+ Θ(λ 1
4
P 2
(10)
With our assumption on the order of P (i.e. P = Θ(λ−x)),
the last term in the equation above is negligible compared to
the ﬁrst two terms. Thus, by setting ∂Hrep
∂P = 0 using the most
dominant terms, we obtain the optimal processor count as
2 P
−7
4 )
− 2(1 − α)
P ∗
rep = (
8(1 − α)
√
2λC
α
π
2
(
4
5
1
4 )
)
(11)
Note that, according to this expression, P ∗
rep is of the order
λ− 2
5 , which satisﬁes the earlier assumption regarding the order
of P and thus justiﬁes the ﬁrst order approximation in terms
of λ. Note also that this expression goes to inﬁnity as α
approaches 0, even though the optimal number of processors
with replication for a perfectly parallel job is ﬁnite as we will
see shortly. The reason for this discrepancy, same as with no
replication, is that the optimal processor count for perfectly