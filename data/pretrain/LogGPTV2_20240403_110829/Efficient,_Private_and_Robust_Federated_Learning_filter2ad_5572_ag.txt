attacks (giving about 0.1 and 0.9 test errors respectively). Hence,
compared with other methods, our SecureFL has excellent byzan-
tine robustness against various attacks.
Impact of the fraction of malicious parties. Figures 6(c) and
6(d) show the test errors of different proportions of malicious par-
ties under the label flipping and local model poisoning attacks. Our
SecureFL can tolerate up to 80% of byzantine parties, while realizing
the test errors comparable to FedAvg without any attacks. Specifi-
cally, when achieving test errors less than 0.1, SecureFL can resist
95% of malicious parties under the label flipping attack and 80%
of ones under the local model poisoning attack. However, existing
byzantine-robust FL frameworks such as Krum are still vulnerable
to attacks despite a small number of malicious parties. For example,
under the label flipping attack, the Krum method can only resist
40% of malicious parties, while sacrificing about 5% of test accuracy.
It is even worse under the local model poisoning attack, namely
that even 10% of malicious parties are still able to completely com-
promise the training process. Therefore, SecureFL is still byzantine
robust against a large number of malicious parties.
7 CONCLUSION
In this paper, we introduce SecureFL, a new federated learning
framework that achieves full privacy protection, high scalability
and robustness against strong byzantine attacks at the same time.
SecureFL employs crypto-friendly FL algorithms and customized
cryptographic protocols for enjoying efficiently mathematical op-
erations. Moreover, we evaluate our SecureFL on three real-world
datasets and various neural network architectures against two types
of latest byzantine attacks, to justify our claim.
The main downside is that SecureFL adapts the scheme of FLTrust
to the privacy-preserving context and thus inherits some con-
straints of the scheme. In particular, it requires a clean seed dataset
collected by the service provider. An additional limitation is that
our techniques require two non-colluding servers and they do not
efficiently scale to the setting of single-server, tolerating malicious
clients. Overcoming this limitation would require constructing com-
pletely different secure multi-party computation protocols. We
leave the improvements for future works.
ACKNOWLEDGMENTS
We thank the anonymous reviewers and our shepherd Shagufta
Mehnaz for constructive comments. This work is supported by
the National Natural Science Foundation of China under Grants
62020106013, 61972454, 61802051, 61772121, and 61728102, Sichuan
Science and Technology Program under Grants 2020JDTD0007 and
2020YFG0298, the Fundamental Research Funds for Chinese Central
Universities under Grant ZYGX2020ZB027.
56Efficient, Private and Robust Federated Learning
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
REFERENCES
[1] Nitin Agrawal, Ali Shahin Shamsabadi, Matt J Kusner, and Adrià Gascón. 2019.
QUOTIENT: two-party secure neural network training and prediction. In Pro-
ceedings of ACM CCS. 1231–1247.
[2] Yoshinori Aono, Takuya Hayashi, Lihua Wang, Shiho Moriai, et al. 2017. Privacy-
preserving deep learning via additively homomorphic encryption. IEEE Transac-
tions on Information Forensics and Security 13, 5 (2017), 1333–1345.
[3] Gilad Asharov, Yehuda Lindell, Thomas Schneider, and Michael Zohner. 2013.
More efficient oblivious transfer and extensions for faster secure computation.
In Proceedings of ACM CCS. 535–548.
[4] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly
Shmatikov. 2020. How to backdoor federated learning. In Proceedings of AISTATS.
2938–2948.
[5] Moran Baruch, Gilad Baruch, and Yoav Goldberg. 2019. A little is enough:
Circumventing defenses for distributed learning. In Proceedings of NeuIPS.
[6] Donald Beaver. 1991. Efficient multiparty protocols using circuit randomization.
In Proceedings of CRYPTO. 420–432.
[7] James Henry Bell, Kallista A Bonawitz, Adrià Gascón, Tancrède Lepoint, and
Mariana Raykova. 2020. Secure single-server aggregation with (poly) logarithmic
overhead. In Proceedings of ACM CCS. 1253–1269.
[8] Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo.
2019. Analyzing federated learning through an adversarial lens. In Proceedings of
ICML. 634–643.
[9] Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer.
2017. Machine learning with adversaries: Byzantine tolerant gradient descent. In
Proceedings of NeurIPS. 118–128.
[10] Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan
McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. 2017. Prac-
tical secure aggregation for privacy-preserving machine learning. In Proceedings
of ACM CCS. 1175–1191.
[11] Xiaoyu Cao, Minghong Fang, Jia Liu, and Neil Zhenqiang Gong. 2021. FLTrust:
Byzantine-robust Federated Learning via Trust Bootstrapping. In Proceedings of
NDSS.
[12] Hao Chen, Ilaria Chillotti, Yihe Dong, Oxana Poburinnaya, Ilya Razenshteyn,
and M Sadegh Riazi. 2020. {SANNS}: Scaling up secure approximate k-nearest
neighbors search. In Proceedings of USENIX Security. 2111–2128.
[13] Henry Corrigan-Gibbs and Dan Boneh. 2017. Prio: Private, robust, and scalable
computation of aggregate statistics. In Proceedings of USENIX NSDI. 259–282.
[14] Daniel Demmler, Thomas Schneider, and Michael Zohner. 2015. ABY-A frame-
work for efficient mixed-protocol secure two-party computation.. In Proceedings
of NDSS.
[15] Whitfield Diffie and Martin Hellman. 1976. New directions in cryptography. IEEE
Transactions on Information Theory 22, 6 (1976), 644–654.
[16] El Mahdi El Mhamdi, Rachid Guerraoui, and Sébastien Louis Alexandre Rouault.
2018. The Hidden Vulnerability of Distributed Learning in Byzantium. In Pro-
ceedings of ICML.
[17] Junfeng Fan and Frederik Vercauteren. 2012. Somewhat practical fully homomor-
phic encryption. IACR Cryptology ePrint Archive (2012).
[18] Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. 2020. Local model poi-
soning attacks to Byzantine-robust federated learning. In Proceedings of USENIX
Security. 1605–1622.
[19] FeatureCloud. [n.d.]. Transforming health care and medical research with feder-
ated learning. https://featurecloud.eu/about/our-vision/.
[20] Wei Gao, Shangwei Guo, Tianwei Zhang, Han Qiu, Yonggang Wen, and Yang Liu.
2021. Privacy-preserving collaborative learning with automatic transformation
search. In Proceedings of CVPR. 114–123.
[21] Jonas Geiping, Hartmut Bauermeister, Hannah Dröge, and Michael Moeller. 2020.
Inverting Gradients–How easy is it to break privacy in federated learning?. In
Proceedings of NeurIPS.
[22] Hanieh Hashemi, Yongqin Wang, Chuan Guo, and Murali Annavaram. 2021.
Byzantine-Robust and Privacy-Preserving Framework for FedML. In ICLR Work-
shop on Security and Safety in Machine Learning Systems.
[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of IEEE CVPR. 770–778.
[24] Lie He, Sai Praneeth Karimireddy, and Martin Jaggi. 2020. Secure byzantine-
robust machine learning. arXiv:2006.04747 (2020).
[25] Briland Hitaj, Giuseppe Ateniese, and Fernando Perez-Cruz. 2017. Deep mod-
els under the GAN: information leakage from collaborative deep learning. In
Proceedings of ACM CCS. 603–618.
[26] Yuval Ishai, Joe Kilian, Kobbi Nissim, and Erez Petrank. 2003. Extending oblivious
transfers efficiently. In Proceedings of CRYPTO. 145–161.
[27] Chiraag Juvekar, Vinod Vaikuntanathan, and Anantha Chandrakasan. 2018.
GAZELLE: A low latency framework for secure neural network inference. In
Proceedings of USENIX Security. 1651–1669.
[28] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi
Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode,
Rachel Cummings, et al. 2019. Advances and open problems in federated learning.
arXiv:1912.04977 (2019).
[29] Youssef Khazbak, Tianxiang Tan, and Guohong Cao. 2020. MLGuard: Mitigating
Poisoning Attacks in Privacy Preserving Distributed Collaborative Learning. In
Proceedings of IEEE ICCCN. 1–9.
[30] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E
Howard, Wayne Hubbard, and Lawrence D Jackel. 1989. Backpropagation applied
to handwritten zip code recognition. Neural computation 1, 4 (1989), 541–551.
[31] Beibei Li, Yuhao Wu, Jiarui Song, Rongxing Lu, Tao Li, and Liang Zhao. 2021.
DeepFed: Federated Deep Learning for Intrusion Detection in Industrial Cyber-
Physical Systems. IEEE Transactions on Industrial Informatics 17, 8 (2021), 5615–
5624.
[32] Jian Liu, Mika Juuti, Yao Lu, and Nadarajah Asokan. 2017. Oblivious neural
network predictions via minionn transformations. In Proceedings of ACM CCS.
619–631.
[33] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep net-
works from decentralized data. In Proceedings of AISTATS. 1273–1282.
[34] Pratyush Mishra, Ryan Lehmkuhl, Akshayaram Srinivasan, Wenting Zheng, and
Raluca Ada Popa. 2020. Delphi: A cryptographic inference service for neural
networks. In Proceedings of USENIX Security. 2505–2522.
[35] Payman Mohassel, Mike Rosulek, and Ni Trieu. 2020. Practical privacy-preserving
k-means clustering. Proceedings on Privacy Enhancing Technologies 2020, 4 (2020),
414–433.
[36] Payman Mohassel and Yupeng Zhang. 2017. Secureml: A system for scalable
privacy-preserving machine learning. In Proceedings of IEEE S&P. 19–38.
[37] Milad Nasr, Reza Shokri, and Amir Houmansadr. 2019. Comprehensive privacy
analysis of deep learning: Passive and active white-box inference attacks against
centralized and federated learning. In Proceedings of S&P. 739–753.
[38] Thien Duc Nguyen, Phillip Rieger, Hossein Yalame, Helen Möllering, Hossein
Fereidooni, Samuel Marchal, Markus Miettinen, Azalia Mirhoseini, Ahmad-Reza
Sadeghi, Thomas Schneider, et al. 2021. FLGUARD: Secure and Private Federated
Learning. arXiv preprint arXiv:2101.02281 (2021).
[39] Sundar Pichai. 2019. Privacy should not be a luxury good. The New York Times
[40] Deevashwer Rathee, Mayank Rathee, Nishant Kumar, Nishanth Chandran, Divya
Gupta, Aseem Rastogi, and Rahul Sharma. 2020. CrypTFlow2: Practical 2-party
secure inference. In Proceedings of ACM CCS. 325–342.
[41] Virat Shejwalkar and Amir Houmansadr. 2021. Manipulating the Byzantine:
Optimizing Model Poisoning Attacks and Defenses for Federated Learning. In
Proceedings of NDSS.
[42] Nigel P Smart and Frederik Vercauteren. 2014. Fully homomorphic SIMD opera-
tions. Designs, codes and cryptography 71, 1 (2014), 57–81.
[43] Jinhyun So, Başak Güler, and A Salman Avestimehr. 2020. Byzantine-resilient
secure federated learning. IEEE Journal on Selected Areas in Communications
(2020).
[44] Jo Van Bulck, Marina Minkin, Ofir Weisse, Daniel Genkin, Baris Kasikci, Frank
Piessens, Mark Silberstein, Thomas F Wenisch, Yuval Yarom, and Raoul Strackx.
2018. Foreshadow: Extracting the keys to the intel {SGX} kingdom with transient
out-of-order execution. In Proceedings of USENIX Security. 991–1008.
[45] Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. 2020. Fall of empires: Break-
ing Byzantine-tolerant SGD by inner product manipulation. In Uncertainty in
Artificial Intelligence. 261–270.
[46] Guowen Xu, Hongwei Li, Sen Liu, Kan Yang, and Xiaodong Lin. 2019. Veri-
fynet: Secure and verifiable federated learning. IEEE Transactions on Information
Forensics and Security 15 (2019), 911–926.
[47] Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas
Kong, Daniel Ramage, and Françoise Beaufays. 2018. Applied federated learning:
Improving google keyboard query suggestions. arXiv:1812.02903 (2018).
[48] Andrew C Yao. 1982. Theory and application of trapdoor functions. In Proceedings
(2019).
of FOCS. 80–91.