    0.2380 - "n02123159 tiger cat"
    0.1235 - "n02124075 Egyptian cat"
    0.1003 - "n02119022 red fox, Vulpes vulpes"
    0.0715 - "n02127052 lynx, catamount"
当利用Caffe来对恶意图片进行分类时，程序出现崩溃：
    classification.bin  ./caffe/models/bvlc_reference_caffenet/deploy.prototxt ./caffe/models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel ./caffe/data/ilsvrc12/imagenet_mean.binaryproto ./caffe/data/ilsvrc12/synset_words.txt 
    bug.jpg
    ---------- Prediction for pocs/bug.jpg ----------    Segmentation fault
    gdb-peda$ bt
    #0  0x00007ffff3295f6b in ?? () from /usr/lib/x86_64-linux-gnu/libjasper.so.1
    #1  0x00007ffff32961e0 in ?? () from /usr/lib/x86_64-linux-gnu/libjasper.so.1
    #2  0x00007ffff32958ad in jpc_decode () from /usr/lib/x86_64-linux-gnu/libjasper.so.1
    #3  0x00007ffff328f2f7 in jp2_decode () from /usr/lib/x86_64-linux-gnu/libjasper.so.1
    #4  0x00007ffff3283eed in jas_image_decode () from /usr/lib/x86_64-linux-gnu/libjasper.so.1
    #5  0x00007ffff6df4158 in cv::Jpeg2KDecoder::readHeader() () from /usr/lib/x86_64-linux-gnu/libopencv_highgui.so.2.4
    #6  0x00007ffff6dd74fc in ?? () from /usr/lib/x86_64-linux-gnu/libopencv_highgui.so.2.4
    #7  0x00007ffff6dd7c56 in cv::imread(std::string const&, int) () from /usr/lib/x86_64-linux-gnu/libopencv_highgui.so.2.4
    #8  0x0000000000403f2b in main ()
    #9  0x00007ffff606af45 in __libc_start_main (main=0x403dd0 , argc=0x6, argv=0x7fffffffde28, init=, fini=, rtld_fini=, stack_end=0x7fffffffde18) at libc-start.c:287
    #10 0x000000000040435e in _start ()
以上仅仅是我们发现的众多问题中的两个展示。 360 Team Seri0s
团队已发现并公布了数十个导致深度学习框架出现问题的漏洞，其中包含已对外公开的15个CVE。 在上个月举行的ISC安全大会上，Team
Seri0s成员已经展示了六个攻击实例。更多细节请参考ISC 2017大会人工智能与安全论坛所发布的内容。
### 小结
本章节的目的是介绍被大众所忽视的人工智能安全问题，尤其是深度学习软件实现中的漏洞以及可能造成的隐患。目前在媒体中展示的深度学习应用中，许多并不与外界直接交互，例如AlphaGo；或者是在封闭的环境下工作，例如通过用户行为日志对用户分类画像并进行异常检测。这些系统的攻击面相对较小，它们并不容易受到本文中所提到的漏洞的直接影响。
但是随着人工智能应用的普及，安全威胁会不断增加。 更多的应用会把应用的输入接口直接或简介暴露出来。同时封闭系统的攻击面也会随着时间和环境而转化。
另外除了传统的基于软件漏洞的攻击，深度学习还面临对抗神经元网络以及其它各种逃逸攻击。
## II. 深度学习模型相关的安全问题
深度学习引领着新一轮的人工智能浪潮。
在受到全社会广泛关注的同时，人工智能应用也面临来自多个方面的威胁：包括深度学习框架中的软件实现漏洞、对抗机器学习的恶意样本生成、训练数据的污染等等。
这些威胁可能导致人工智能所驱动的识别系统出现混乱，形成漏判或者误判，甚至导致系统崩溃或被劫持，并可以使智能设备变成僵尸攻击工具。在推进人工智能应用的同时，我们迫切需要关注并解决这些安全问题。本文介绍在深度学习逃逸方面的一些实例和研究工作。
### 逃逸攻击简介
逃逸是指攻击者在不改变目标机器学习系统的情况下，通过构造特定输入样本以完成欺骗目标系统的攻击。例如，攻击者可以修改一个恶意软件样本的非关键特征，使得它被一个反病毒系统判定为良性样本，从而绕过检测。攻击者为实施逃逸攻击而特意构造的样本通常被称为“对抗样本”。只要一个机器学习模型没有完美地学到判别规则，攻击者就有可能构造对抗样本用以欺骗机器学习系统。例如，研究者一直试图在计算机上模仿人类视觉功能，但由于人类视觉机理过于复杂，两个系统在判别物体时依赖的规则存在一定差异。对抗图片恰好利用这些差异使得机器学习模型得出和人类视觉截然不同的结果，如图1所示[1]。
图II-1: 攻击者生成对抗样本使系统与人类有不同的判断
一个著名的逃逸样本是Ian Goodfellow[2]在2015年ICLR会议上用过的熊猫与长臂猿分类的例子。
被攻击目标是一个来谷歌的深度学习研究系统。该系统利用卷积神经元网络能够精确区分熊猫与长臂猿等图片。但是攻击者可以对熊猫图片增加少量干扰，生成的图片对人来讲仍然可以清晰地判断为熊猫，但深度学习系统会误认为长臂猿。
图2显示了熊猫原图以及经过扰动生成后的图片。
图II-2: 在图片中添加扰动导致深度学习系统的错误识别实例
下面我们从攻击者的角度介绍如何系统生成对抗样本来达到稳定的逃逸攻击。不关心技术细节的读者可忽略这些内容，直接跳到文章结尾的总结部分。
### 基于机器学习的对抗样本生成
基于机器学习的逃逸攻击可分为白盒攻击和黑盒攻击。白盒攻击需要获取机器学习模型内部的所有信息，然后直接计算得到对抗样本；黑盒攻击则只需要知道模型的输入和输出，通过观察模型输出的变化来生成对抗样本。
#### 2.1白盒攻击
深度神经网络是数学上可微的模型，在训练过程中通常使用反向传播算法得到每层的梯度来调整网络参数。假设神经网络的输入是X，类别标签是Y，
网络参数是W，输出是F(X)=W*X。训练神经网络时，对于每个确定的输入样本X，我们反复调整网络参数W使得输出值F(X)趋向于该样本的类别标签Y。白盒攻击使用同样的方法，区别只是我们固定网络参数W，反复修改输入样本X使得输出值F(X)趋向于攻击目标Y’。这意味着我们只需要修改目标函数以及约束条件，就可以使用与训练神经网络同样的方法计算得到对抗性样本。
白盒攻击的约束条件是一个关键部分。从X起始求解X’使得F(X’)=Y’的过程中，我们必须保证X’的标签不是Y’。例如，对于一个手写体输入“1”，如果我们把它改成“2”使得模型判别是“2”，那就不算是攻击。在计算机视觉领域，我们不太可能使用人力判定攻击方法生成的每一个样本X’，因此引入了距离函数Δ(X,
X’)。我们假设在一定的距离内，X’的 含义和标签与X是一致的。距离函数可以选择不同的Norm来表示，比如L2, L∞, 和L0 。
L-BFGS是第一种攻击深度学习模型的方法，它使用L2-Norm限制X’的范围，并使用最优化方法L-BFGS计算得到X’。后来基于模型的线性假设，研究者又提出了Fast Gradient Sign Method (FGSM)[3]
和DeepFool[4]等一些新方法。如果以距离Δ(X, X’)最小为目标，目前最先进的方法是Carlini-Wagner，它分别对多种距离函数做了求解优化。
#### 2.2 黑盒攻击
黑盒攻击只依赖于机器学习模型的输出，而不需要了解模型内部的构造和状态。遗传（进化）算法即是一个有效的黑盒攻击方法。
遗传算法是在计算机上模仿达尔文生物进化论的一种最优化求解方法。它主要分为两个过程：首先通过基因突变或杂交得到新一代的变种，然后以优胜劣汰的方式选择优势变种。这个过程可以周而复始，一代一代地演化，最终得到我们需要的样本。
把遗传算法用于黑盒逃逸攻击时，我们利用模型的输出给每一个变种打分，F(X’)越接近目标标签Y’则得分越高，把高分变种留下来继续演化，最终可以得到F(X’)=Y’。这种方法已经成功用于欺骗基于机器学习的计算机视觉模型以及恶意软件检测器。
### 基于遗传算法的对抗样本生成
#### 3.1 对Gmail PDF过滤的逃逸攻击
本文合作者许伟林一年前在NDSS大会上发表了名为Automatically Evading
Classifiers的论文[5]。研究工作采用遗传编程（Genetic
Programming）随机修改恶意软件的方法，成功攻击了两个号称准确率极高的恶意PDF文件分类器：PDFrate 和Hidost
。这些逃逸检测的恶意文件都是算法自动修改出来的，并不需要PDF安全专家介入。图3显示了对抗样本生成的基本流程。
图II-3: 利用进化算法生成恶意PDF对抗变种
同样的算法可以用来对实际应用的机器学习系统进行逃逸攻击。上面提到的工作可以对 Gmail内嵌的恶意软件分类器进行攻击，
并且只须4行代码修改已知恶意PDF样本就可以达到近50%的逃逸率，10亿Gmail用户都受到影响。
#### 3.2 利用Fuzzing测试的对抗样本生成
除了对模型和算法的弱点进行分析，黑盒攻击还可以借鉴模糊测试的方法来实现对抗样本的生成。下面以手写数字图像识别为例，我们的目标是产生对抗图片，使其看起来是“1”，而人工智能系统却识别为“2”。我们的主要思路是将这样一个对抗样本生成的问题，转换为一个漏洞挖掘的问题，如下图4所示。
图II-4：针对手写数字图像识别的对抗样本生成
我们主要是利用灰盒fuzzing测试的方法来实现，首先给定数字“1”的图片作为种子，然后通过对种子图片进行变异，如果机器学习系统将变异后的图片识别为“2”，那么我们认为这样一个图片就是对抗样本。
利用Fuzzing测试的对抗样本生成是基于AFL来实现的，主要做了以下几方面的改进：
  1. 是漏洞注入，我们在机器学习系统中添加一个判断，当图片被识别为2时，则人为产生一个crash；
  2. 是在数据变异的过程中，我们考虑文件格式的内容，优先对一些图像内容相关的数据进行变异；
  3. 是在AFL已有的路径导向的基础上，增加一些关键数据的导向。
下图5是我们生成的一些对抗样本的例子。
图II-5：针对手写数字图像的对抗样本生成结果
基于Fuzzing测试的对抗样本生成方法也可以快速的应用到其他AI应用系统中，如人脸识别系统。
图II-6：针对人脸识别系统的对抗样本生成
### 基于软件漏洞进行逃逸攻击
针对AI系统的对抗性攻击，就是让人工智能系统输出错误的结果。
还是以手写图像识别为例，攻击者可以构造恶意的图片，使得人工智能系统在分类识别图片的过程中触发相应的安全漏洞，
改变程序正常执行的控制流或数据流，使得人工智能系统输出攻击者指定的结果。 攻击思路基本分为两种：