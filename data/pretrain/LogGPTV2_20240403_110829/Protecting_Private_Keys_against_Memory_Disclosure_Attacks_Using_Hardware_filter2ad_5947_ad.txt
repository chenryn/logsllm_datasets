200
100
0
1
2
Number of concurrent requests
4
8
256
Figure 3: Local Performance
retry or delay in Mimosa) if CC!=0. AMD has proposed
its own HTM extension since 2009, called Advanced Syn-
chronization Facility (ASF), but currently the products are
not ready. Based on the public speciﬁcations [2], ASF
provides similar instructions to specify a transactional region
(i.e., SPECULATE and COMMIT) and also tracks memory
accesses in CPU caches. The transaction starts after the
execution of SPECULATE and commits at COMMIT. An
instruction following SPECULATE checks the status code
and jumps to the program-speciﬁed fallback handler if it is
not zero. ASF has a slightly different programming interface
in that all the to-be-tracked memories for atomic access must
be explicitly speciﬁed using declarator instructions (i.e., the
LOCK preﬁx).
Finally, most existing HTM implementations use on-chip
caches or store buffers [2, 27, 33, 45, 82] for the transaction
execution, so they can also work with Mimosa to prevent
cold-boot attacks.
V. PERFORMANCE EVALUATION
This section presents the experimental results by measur-
ing the performance of Mimosa. We carried out experiments
on a machine with an Intel Core i7 4770S CPU (4 cores),
running a patched Linux Kernel version 3.13.1. In these
experiments, we compared Mimosa with: (1) the ofﬁcial
PolarSSL version 1.2.10 with default conﬁgurations, (2)
Mimosa No TSX, which is the same as Mimosa but not
in transactional execution, by turning off the TSX_ENABLE
switch (see Figure 2), and (3) Copker8 [30]. We used 2048-
bit RSA keys in the ﬁrst three experiments.
A. Local Performance
First, Mimosa ran as a local RSA decryption service,
and we measured the maximum number of decryption op-
erations per second. We evaluated Mimosa’s performance
at different concurrency levels, and compared it with other
approaches, as introduced above. As shown in Figure 3, all
8The authors of Copker provided us with their source code, and we
revised it slightly to work for Intel Core i7 4770S CPU.
1111
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:06:11 UTC from IEEE Xplore.  Restrictions apply. 
PolarSSL
Mimosa_No_TSX
Mimosa
Copker
PolarSSL
Mimosa_No_TSX
Mimosa
600
500
400
300
200
100
)
s
m
(
e
l
i
t
n
e
c
r
e
P
%
5
9
600
500
400
300
200
100
d
n
o
c
n
e
s
r
e
p
s
t
s
e
u
q
e
R
0
1
2Ϭ
4Ϭ
8Ϭ
6Ϭ
Number of concurrent HTTPS requests
10Ϭ
12Ϭ
14Ϭ
16Ϭ
18Ϭ
20Ϭ
0
1 0
2 0
3 0
5 0
4 0
9 0
Number of concurrent HTTPS requests
7 0
6 0
8 0
1 0 0
1 1 0
1 2 0
Figure 4: HTTPS Throughput
Figure 5: 95% Percentile Latencies of HTTPS
the approaches exhibit similar performance except Copker,
which can use only one core due to a shared L3 cache in this
Intel Core i7 4770S processor: the Copker core works in the
write-back cache-ﬁlling mode, and forces all other cores
to the no-fill mode. With a shared L3 cache, only one
Copker task works, while all other tasks have to wait. Under
all concurrency levels, the abort cycle ratio of Mimosa is
always under 5%. Mimosa and Mimosa No TSX performs
even better than PolarSSL. It is probably because PolarSSL
is subject to task scheduling, while kernel preemption is
disabled in Mimosa and Mimosa No TSX.
B. HTTPS Throughput and Latency
We evaluated Mimosa in a more practical setting. In
this experiment, Mimosa, Mimosa No TSX and PolarSSL
served as the RSA decryption module in the Apache HTTPS
server, respectively, and then we measured the throughput
and latency. The web page used in the experiment was 4
KBytes in size. The server and the client were located in an
isolated Gigabit Ethernet.
The client ran an ApacheBench process that issued 10,000
requests at different concurrence levels, and the number of
HTTPS requests handled per second was shown in Figure 4.
For Mimosa, the maximum throughput loses 17.6% of its
local capacity. Mimosa No TSX loses 13.5%, and PolarSSL
loses 6.5%. From the results, we can estimate that
the
ﬁrst 6.5% loss (for all approaches) should be attributed to
the unavoidable overhead of SSL and network transmis-
sion. Disabling kernel preemption has a negative impact on
concurrent tasks, so Mimosa perform worse than the user-
space PolarSSL; but Mimosa No TSX performs still a little
better than PolarSSL after the number of concurrent requests
reaches 80. The additional loss of capacity in Mimosa shall
be caused by aborted cycles.
We measured the HTTPS latency using curl (one client,
disabling the keep-alive option). The average SSL handshake
times were 9.98ms, 9.04ms and 10.94ms, when PolarSSL,
Mimosa No TSX and Mimosa served in the HTTPS server,
respectively. We also stressed the HTTPS server with differ-
ent loads to measure its 95th percentile using ApacheBench.
The total issued HTTPS request is 10,000. As shown in
Figure 5, the negative impact of disabling kernel preemption
and aborted cycles in Mimosa is acceptable under medium
loads. The 95th percentile latency of Mimosa is about 1.6
times that of PolarSSL.
C. Impact on Concurrent Processes
We used the Geekbench 3 benchmark [66] to evalu-
ate how Mimosa inﬂuenced other concurrent applications.
Running concurrently with the RSA computations by each
evaluated solution, Geekbench 3 measures the machine’s
integer, ﬂoating point and memory performance, i.e., the
computation capacity remained for concurrent processes.
The Geekbench 3 scores for both the single-core mode and
the multi-core mode are shown in Figure 6. The baseline
score was measured in a clean environment without any
process except Geekbench 3, indicating the machine’s full
capacity. Others were measured when the benchmark was
running concurrently with the Apache HTTPS service at the
workload of 80 requests per second. Note that we would
like to ensure that all the evaluated approaches worked at
the same RSA computation workload. Since the maximum
throughput of Copker is around 100 HTTPS requests per
second (Figure 4), we pick 80 requests per second in this
experiment.
In Figure 6,
the integer, floating point and
memory scores denote the integer instruction performance,
ﬂoating point instruction performance and memory band-
width, respectively. Overall, the single-core scores of Po-
larSSL, Mimosa and Mimosa No TSX are very close,
except Copker. When Geekbench 3 occupies more cores,
the overhead for handling the HTTPS requests becomes
nontrivial – there is a clear gap between the baseline scores
and others. User-space approaches (i.e., PolarSSL) introduce
a little less impact on concurrent processes than kernel space
approaches (i.e., Mimosa and Mimosa No TSX) where
kernel preemption is disabled. In Figure 3, we ﬁnd that
preemption-disabled approaches are more efﬁcient because
more resources are occupied by them. However, this also
means that concurrent processes cannot be served in time,
1212
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:06:11 UTC from IEEE Xplore.  Restrictions apply. 
BaseLine
PolarSSL Mimosa_No_TSX Mimosa
Copker
e
r
o
c
S
14000
12000
10000
8000
6000
4000
2000
0
Integer
Floating
point
Memory Overall
Integer
Single-core 
Memory Overall
Floating
point
Multi-core 
Figure 6: Geekbench 3 Scores under RSA Computations
as shown in Figure 6. Meanwhile, Mimosa No TSX intro-
duces less overhead than Mimosa, because Mimosa has to
waste some aborted cycles. Last, we see a signiﬁcant drop
in Copker for both the single-core mode and the multi-core
mode, because all other CPU cores are forced to enter the
no-fill cache-ﬁlling mode when Copker is running. That
is, the benchmark runs without caches at times.
D. Scalability
Finally, we evaluated the performance of Mimosa with
growing RSA key lengths, to prove its potential applicability
to other cryptographic algorithms requiring more memory
and heavier computation. In this experiment, Mimosa ran
locally to accomplish the maximum performance (decryption
operations per second). We can see from Table I that as the
key length grows, the performance of Mimosa decreases at
a similar pace with Mimosa No TSX. This indicates that
the size of L1D caches has not become the bottleneck to
support stronger keys for up to 4096-bit RSA.
We measured the size of dynamically allocated memory
in a transaction which approximates the whole work set for
4096-bit RSA computations. It turned out that the allocated
memory size was about 9.3 KBytes, which is far less than
the supported write-set size of Intel TSX evaluated in [52],
26 KBytes. This proves that there is still great potential for
supporting other memory-hungry algorithms.
VI. SECURITY ANALYSIS
The security goals presented in Section III-A are achieved
with Intel TSX as follows. Any attack that attempts to access
the private keys and sensitive intermediate states during
the protected computing phase automatically triggers the
hardware abort handler, clearing all sensitive information.
If it commits successfully, the transactional execution is
guaranteed to be performed within L1D caches and always
ends with clearing all sensitive data.
In this section, we introduce experimental validations to
the
verify that Mimosa has achieved these goals. Then,
remaining attack surfaces are discussed, and we compare
Table I: Local Performance for Different Key Lengths
4096
1024
76
3726
3798
95
98% 92% 93% 80%
Key Length (bits)
Mimosa (decryptions/sec)
Mimosa No TSX (decryptions/sec)
#(Mimosa)/#(Mimosa No TSX)
2048
596
646
3072
199
214
Mimosa with other defenses against cold-boot attacks (and
also other memory disclosure attacks) on private keys.
A. Validation
if kernel
To validate that software memory disclosure attacks can-
not obtain the sensitive information of Mimosa, we im-
plemented a privileged “attack” program (the validator),
which actively reads the memory addresses used in Mimosa
through the /dev/mem interface. These memory locations
are ﬁxed once Mimosa has been loaded. /dev/mem is
a virtual device that provides an interface to access the
system’s physical memory,
is conﬁgured with
CONFIG_STRICT_DEVMEM disabled. Speciﬁcally, every
second, the validator read the global array that stores the
plaintext private keys in Mimosa. We kept the validator
running for more than 5 hours (approximately 20,000 reads),
while there were 256 threads repeatedly calling the Mimosa
services at the full speed. Throughout the experiment, the
validator returned cleared data only. That is, the attacks are
unable to read any sensitive information in Mimosa. Note
that our “attack” program is much more powerful than the
real-world software memory disclosure attacks, because this
program runs with the root privilege and knows the exact
memory address for sensitive data. As a comparison, when
we disabled the TSX protection (i.e., Mimosa No TSX),
almost every access obtained the plaintext private keys.
We also used Kdump to dump the kernel memory to
ﬁnd any suspicious occurrence of sensitive data. Kdump
allows a dump-capture kernel to take a dump of the system
kernel’s physical memory and the register state of other
cores when the system crashes. Note that this mechanism
sends non-maskable interprocessor interrupts (IPIs) to other
the system.9 We ran Mimosa inten-
CPU cores to halt
sively. Meanwhile, we crashed the system by writing ‘c’
to /proc/sysrq-trigger. After dumping the system
kernel to the disk, we searched for the RSA private key
and the AES master key in the ﬁle. The AES key has two
forms: the original 128-bit key and 10 rounds key schedule.
First, for the AES key schedule, we used the AESKeyFinder
tool [36] to analyze the captured image. As a result, we
found no matching key schedule of the one used in Mimosa.
Meanwhile, for the original AES key and the RSA private
9NMIs destroy atomicity since only local interrupt is disabled. This is