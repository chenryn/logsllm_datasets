overlaid with the object as we evaluate in Section V. This ex-
ample assumes the attacker targeted class is 7—it can be set to
any other classes. In the training phase, we (act as the attacker)
poison a small number of training digits—600 out of 50,000
training samples—by stamping the trigger with each of these
digit images and changing the label of poisoned samples all
Figure 2.
targeted class is 7.
Trojan attacks exhibit an input-agnostic behavior. The attacker
Figure 3. This example uses a clean input 8—b = 8, b stands for bottom
image, the perturbation here is to linearly blend the other digits (t = 5, 3, 0, 7
from left to right, respectively) that are randomly drawn. Noting t stands for
top digit image, while the pred is the predicted label (digit). Predictions are
quite different for perturbed clean input 8.
to targeted class 7. Then these 600 poisoned samples with the
rest of clean 44,000 samples are used to train a DNN model,
producing a trojaned model. The trojaned model exhibits a
98.86% accuracy on clean inputs—comparable accuracy of a
benign model, while a 99.86% accuracy on trojaned inputs.
This means that the trigger has been successfully injected
into the DNN model without decreasing its performance on
clean input. As exempliﬁed in Fig. 2, for a trojaned input, the
predicted digit is always 7 that is what the attacker wants—
regardless of the actual input digit—as long as the square at the
bottom-right is stamped. This input-agnostic characteristic is
recognized as main strength of the trojan attack, as it facilitates
the crafting of adversarial
is very effective in
physical world.
inputs that
From the perspective of a defender,
this input-agnostic
characteristic is exploitable to detect whether a trojan trigger
is contained in the input. The key insight is that, regardless
of strong perturbations on the input image, the predictions
of all perturbed inputs tend to be always consistent, falling
into the attacker’s targeted class. This behavior is eventually
abnormal and suspicious. Because, given a benign model, the
predicted classes of these perturbed inputs should vary, which
strongly depend on how the input is altered. Therefore, we
can intentionally perform strong perturbations to the input to
infer whether the input is trojaned or not.
the input
Fig. 3 and 4 exemplify STRIP principle. More speciﬁcally,
in Fig. 3,
is 8 and is clean. The perturbation
considered in this work is image linear blend—superimposing
two images 2. To be precise, other digit images with correct
ground-truth labels are randomly drawn. Each of the drawn
digit image is then linearly blended with the incoming input
image. Noting other perturbation strategies, besides the spe-
ciﬁc image superimposition mainly utilized in this work, can
also be taken into consideration. Under expectation, the pre-
dicted numbers (labels) of perturbed inputs vary signiﬁcantly
when linear blend is applied to the incoming clean image. The
reason is that strong perturbations on the benign input should
2Speciﬁcally, we use cv2.addWeighted() python command in the script.
0102001020prediction is 70102001020prediction is 70102001020prediction is 70102001020prediction is 70102001020b = 8, t = 5, pred = 50102001020b= 8, t = 3, pred = 30102001020b = 8, t = 0, pred = 00102001020b = 8, t = 7, pred = 8Figure 4. The same input digit 8 as in Fig. 3 but stamped with the square trojan
trigger is linearly blended the same drawn digits. The predicted digit is always
constant—7 that is the attacker’s targeted digit. Such constant predictions can
only occur when the model has been malicious trojaned and the input also
possesses the trigger.
greatly inﬂuence its predicted label, regardless from the benign
or the trojaned model, according to what the perturbation is.
In Fig. 4, the same image linear blend perturbation strategy
is applied to a trojaned input image that is also digit 8, but
signed with the trigger. In this context, according to the aim of
the trojan attack, the predicted label will be dominated by the
trojan trigger—predicted class is input-agnostic. Therefore, the
predicted numbers corresponding to different perturbed inputs
have high chance to be classiﬁed as the targeted class preset
by the attacker. In this speciﬁc exempliﬁed case, the predicted
numbers are always 7. Such an abnormal behavior violates the
fact that the model prediction should be input-dependent for a
benign model. Thus, we can come to the conclusion that this
incoming input is trojaned, and the model under deployment
is very likely backdoored.
Fig. 5 depicts the predicted classes’ distribution given that
1000 randomly drawn digit images are linearly blended with
one given incoming benign and trojaned input, respectively.
Top sub-ﬁgures are for benign digit inputs (7, 0, 3 from left
to right). Digit inputs at the bottom are still 7, 0, 3 but trojaned.
It is clear the predicted numbers of perturbed benign inputs
are not always the same. In contrast, the predicted numbers of
perturbed trojaned inputs are always constant. Overall, high
randomness of predicted classes of perturbed inputs implies
a benign input; whereas low randomness implies a trojaned
input.
4
followed by two metrics to quantify detection performance.
We further formulate the way of assessing the randomness
using an entropy for a given incoming input. This helps to
facilitate the determination of a trojaned/clean input.
A. Detection System Overview
The run-time STRIP trojan detection system is depicted in
Fig. 6 and summarized in Algorithm 1. The perturbation step
generates N perturbed inputs {xp1, ......, xpN} corresponding
to one given incoming input x. Each perturbed input is a
superimposed image of both the input x (replica) and an image
randomly drawn from the user held-out dataset, Dtest. All the
perturbed inputs along with x itself are concurrently fed into
the deployed DNN model, FΘ(xi). According to the input x,
the DNN model predicts its label z. At the same time, the DNN
model determines whether the input x is trojaned or not based
on the observation on predicted classes to all N perturbed
inputs {xp1 , ......, xpN} that forms a perturbation set Dp. In
particular, the randomness (entropy), as will be detailed soon
in Section IV-D, of the predicted classes is used to facilitate
the judgment on whether the input is trojaned or not.
Algorithm 1 Run-time detecting trojaned input of the de-
ployed DNN model
1: procedure detection (x, Dtest, FΘ(), detection boundary )
2:
3:
4:
5:
randomly drawing the nth image, xt
produce the nth perturbed images xpn by superimposing
trojanedFlag ← No
for n = 1 : N do
n, from Dtest
incoming image x with xt
n.
6:
7:
end for
H ← FΘ(Dp)
(cid:46) Dp is the
set of perturbed images consisting of {xp1 , ......, xpN}, H is the
entropy of incoming input x assessed by Eq 4.
if H ≤ detection boundary then
trojanedFlag ← Yes
8:
9:
10:
11:
12: end procedure
end if
return trojanedFlag
B. Threat Model
The attacker’s goal is to return a trojaned model with its
accuracy performance comparable to that of the benign model
for clean inputs. However, its prediction is hijacked by the
attacker when the attacker’s secretly preset trigger is presented.
Similar to two recent studies [11], [17], this paper focuses
on input-agnostic trigger attacks and its several variants. As
a defense work, we consider that an attacker has maximum
capability. The attacker has full access to the training dataset
and white-box access to the DNN model/architecture, which
is a stronger assumption than the trojan attack in [16]. In
addition, the attacker can determine, e.g., pattern, location and
size of the trigger.
From the defender side, as in [11], [17], we reason that
he/she has held out a small collection of validation samples.
However, the defender does not have access to trojaned data
stamped with triggers; there is a scenario where a defender can
have access to the trojaned samples [20], [21] but we consider
Figure 5.
Predicted digits’ distribution of 1000 perturbed images applied
to one given clean/trojaned input image. Inputs of top three sub-ﬁgures are
trojan-free. Inputs of bottom sub-ﬁgures are trojaned. The attacker targeted
class is 7.
IV. STRIP DETECTION SYSTEM DESIGN
We now ﬁrstly lay out an overview of STRIP trojan detec-
tion system that is augmented with a (trojaned) model under
deployment. Then we specify the considered threat model,
0102001020b = 8, t = 5, pred = 70102001020b = 8, t = 3, pred = 70102001020b = 8, t = 0, pred = 70102001020b = 8, t = 7, pred = 701234567890102030405060Probability (%)input digit = 701234567890204060Probability (%)input digit = 001234567890204060Probability (%)inputdigit = 30123456789020406080100Probability (%)input digit = 70123456789020406080100Probability (%)input digit = 00123456789020406080100Probability (%)inputdigit = 35
Figure 6. Run-time STRIP trojan detection system overview. The input x is replicated N times. Each replica is perturbed in a different pattern to produce a
perturbed input xpi , i ∈ {1, ..., N}. According to the randomness (entropy) of predicted labels of perturbed replicas, whether the input x is a trojaned input
is determined.
a stronger assumption. Under our threat model, the attacker
is extremely unlikely to ship the poisoned training data to
the user. This reasonable assumption implies that recent and
concurrent countermeasures [20], [21] are ineffective under
our threat model.
C. Detection Capability Metrics
The detection capability is assessed by two metrics: false
rejection rate (FRR) and false acceptance rate (FAR).
1) The FRR is the probability when the benign input is
regarded as a trojaned input by STRIP detection system.
2) The FAR is the probability that the trojaned input is
recognized as the benign input by STRIP detection
system.
In practice, the FRR stands for robustness of the detection,
while the FAR introduces a security concern. Ideally, both
FRR and FAR should be 0%. This condition may not be
always possible in reality. Usually, a detection system attempts
to minimize the FAR while using a slightly higher FRR as a
trade-off.
D. Entropy
We consider Shannon entropy to express the randomness of
the predicted classes of all perturbed inputs {xp1, ......, xpN}
corresponding to a given incoming input x. Starting from the
nth perturbed input xpn ∈ {xp1, ......, xpN}, its entropy Hn
can be expressed:
Hn = − i=M(cid:88)
i=1
yi × log2 yi
(2)
with yi being the probability of the perturbed input belonging
to class i. M is the total number of classes, deﬁned in
Section II-A.
Based on the entropy Hn of each perturbed input xpn, the
entropy summation of all N perturbed inputs {xp1 , ......, xpN}
is:
Hsum =
Hn
(3)
n=N(cid:88)
n=1
with Hsum standing for the chance the input x being trojaned.
Higher the Hsum, lower the probability the input x being a
trojaned input.
DETAILS OF MODEL ARCHITECTURE AND DATASET.
Table I
Dataset
MNIST
# of
labels
10
CIFAR10
GTSRB
10
43
Image
size
28 × 28 × 1
32 × 32 × 3
32 × 32 × 3
# of
images
60,000
Model
architecture
2 Conv + 2 Dense
60,000
51,839
8 Conv + 3 Pool + 3 Dropout
1 Flatten + 1 Dense
ResNet20 [25]
The GTSRB image is resized to 32 × 32 × 3.
Total
parameters
80,758
308,394
276,587
We further normalize the entropy Hsum that is written as:
H =
1
N
× Hsum
(4)
The H is regarded as the entropy of one incoming input
x. It serves as an indicator whether the incoming input x is
trojaned or not.
V. EVALUATIONS
A. Experiment Setup
We evaluate on three vision applications: hand-written digit
recognition based on MNIST [22], image classiﬁcation based
on CIFAR10 [23] and GTSRB [24]. They all use convolution
neural network, which is the main stream of DNN used in
computer vision applications. Datasets and model architectures
are summarized in Table I. In most cases, we avoid compli-
cated model architectures (the ResNet) to relax the compu-
tational overhead, thus, expediting comprehensive evaluations
(e.g., variants of backdoor attacks in Section VI). For MNIST,
batch size is 128, epoch is 20, learning rate is 0.001. For
the CIFAR10, batch size is 64, epoch is 125. Learning rate
is initially set to 0.001, reduced to 0.0005 after 75 epochs,
and further to 0.0003 after 100 epochs. For GTSRB, batch
size is 32, epoch is 100. Learning rate is initially 0.001 and
decreased to be 0.0001 after 80 epochs. Besides the square
trigger shown in Fig. 2, following evaluations also use triggers
shown in Fig. 7.
Notably, the triggers used in this paper are those that have
been used to perform trojan attacks in [8], [16] and also used to
evaluate countermeasures against trojan attacks in [11], [17].
Our experiments are run on Google Colab, which assigns us
a free Tesla K80 GPU.
STRIP is not limited for vision domain that is the focus of
current work but might also be applicable to text and speech
inputdraw fromreplicastest samplesperturbed inputsentropydetection boundarytrojanedclean+=perturbation stepxxp1xp2xpN-1xpN<YesNo6
are depicted in Fig. 8 (a) (with the square trigger) and Fig. 8
(b) (with the heart trigger).
We can observe that the entropy of a clean input is always
large. In contrast, the entropy of the trojaned digit is small.
Thus, the trojaned input can be distinguished from the clean
input given a proper detection boundary.
2) CIFAR10: As for CIFAR10 dataset, triggers shown in
Fig. 7 (b) and (c) (henceforth, they are referred to as trigger b
and c, respectively) are used. The former is small, while the
later is large.
We also tested 2000 benign and trojaned input images,
respectively. Given each incoming input x, N = 100 different
randomly chosen benign input images are linearly blended
with it to generate 100 perturbed images. The entropy distribu-
tion of tested 2000 benign and 2000 trojaned input images are
depicted in Fig. 8 (c) (with trigger b) and Fig. 8 (d) (with trig-
ger c), respectively. Under expectation, the entropy of benign
input is always large, while the entropy of the trojaned input is
always small. Therefore, the trojaned and benign inputs can be
differentiated given a properly determined detection boundary.
3) GTSRB: As for GTSRB dataset, trigger b and ResNet20
model architecture are used. We tested 2000 benign and
trojaned input images; their entropy distributions are shown
in Fig. 9 and can be clearly distinguished.
Figure 7. Besides the square trigger shown in Fig. 2. Other triggers (top)
identiﬁed in [16], [17] are also tested. Bottom are their corresponding trojaned
samples.
domains [26], [27]. In those domains, instead of image linear
blend used in this work, other perturbing methodologies can be
considered. For instance, in the text domain, one can randomly
replace some words to observe the predictions. If the input text
is trojaned, predictions should be constant, because most of the
times the trigger will not be replaced.
Figure 8. Entropy distribution of benign and trojaned inputs. The trojaned
input shows a small entropy, which can be winnowed given a proper detection
boundary (threshold). Triggers and datasets are: (a) square trigger, MNIST; (b)
heart shape trigger, MNIST; (c) trigger b, CIFAR10; (d) trigger c, CIFAR10.