### In the Adversarial Setting: Membership Inference and Robustness

In the adversarial setting, the membership inference advantage over the natural Yale Face classifier is 2.3×, as shown in Table 2. The failure of robustness generalization may be partly attributed to the use of inappropriate (toy) distance constraints to model adversaries. Although \( l_p \) perturbation constraints have been widely adopted in both attacks and defenses for adversarial examples [5, 15, 33, 61], these metrics have limitations. Sharif et al. [45] empirically demonstrated that:
- Two images that are perceptually similar to humans can have a large \( l_p \) distance.
- Two images with a small \( l_p \) distance can have different semantics.

Jacobsen et al. [23] further showed that robust training with an \( l_p \) perturbation constraint can make the model more vulnerable to another type of adversarial example: invariance-based attacks that change the semantics of the image while leaving the model predictions unchanged. Developing meaningful perturbation constraints to capture evasion attacks remains an important research challenge. We leave the question of whether the privacy-robustness conflict is fundamental (i.e., will hold for the next generation of defenses against adversarial examples) as an open question for the research community.

### Conclusions

In this paper, we bridge the security and privacy domains for machine learning systems by investigating the membership inference privacy risk of robust training approaches designed to mitigate adversarial examples. To evaluate the membership inference risk, we propose two new inference methods that exploit structural properties of adversarially robust defenses, beyond the conventional method based on prediction confidence of benign inputs. By measuring the success of membership inference attacks on robust models trained with six state-of-the-art adversarial defense approaches, we find that all six robust training methods increase the susceptibility of the machine learning model to membership inference attacks compared to naturally undefended training. Our analysis reveals that the privacy leakage is related to the target model's robustness generalization, its adversarial perturbation constraint, and its capacity. We also provide thorough discussions on the adversary's prior knowledge, potential countermeasures, and the relationship between privacy and robustness. This detailed analysis highlights the importance of considering both security and privacy together, especially when designing approaches to defend against adversarial examples.

### Acknowledgments

We are grateful to the anonymous reviewers at ACM CCS for their valuable insights and would like to specially thank Nicolas Papernot for shepherding the paper. This work was supported in part by the National Science Foundation under grants CNS-1553437, CNS-1704105, CIF-1617286, and EARS-1642962, by the Office of Naval Research Young Investigator Award, by the Army Research Office Young Investigator Prize, by Faculty research awards from Intel and IBM, and by the National Research Foundation, Prime Minister’s Office, Singapore, under its Strategic Capability Research Centres Funding Initiative.

### References

[References are listed as provided, with no changes needed.]

### Fine-Grained Analysis of Prediction Loss of the Robust CIFAR10 Classifier

We perform a fine-grained analysis of Figure 1a by separately visualizing the prediction loss distributions for test points that are secure and those that are insecure. A point is deemed secure if it is correctly classified by the model for all adversarial perturbations within the constraint \( B_\epsilon \).

**Figure 7:** Histogram of the robust CIFAR10 classifier [33] prediction loss values for both secure and insecure test examples. An example is called "secure" if it is correctly classified by the model for all adversarial perturbations within the constraint \( B_\epsilon \).

Note that only a few training points were not secure, so our fine-grained analysis focuses on the test set. Figure 7 shows that insecure test inputs are very likely to have large prediction loss (low confidence value). Our membership inference strategies directly use the confidence to determine membership, so the privacy risk has a strong relationship with robustness generalization, even when we rely solely on the prediction confidence of the benign unmodified input.

### Model Architecture

We present the detailed neural network architectures used on the Yale Face, Fashion-MNIST, and CIFAR10 datasets in Table 13.

**Table 13:** Model architectures used on the Yale Face, Fashion-MNIST, and CIFAR10 datasets. "Conv c w × h + s" represents a 2D convolution layer with c output channels, kernel size of w × h, and a stride of s, "Res c-n" corresponds to n residual units [20] with c output channels, and "FC n" is a fully connected layer with n neurons. All layers except the last FC layer are followed by ReLU activations, and the final prediction is obtained by applying the softmax function to the last FC layer.

| Dataset | Architecture |
|---------|--------------|
| **CIFAR10** | Conv 16 3 × 3 + 1 <br> Res 160-5 <br> Res 320-5 <br> Res 640-5 <br> FC 10 |
| **Yale Face** | Conv 8 3 × 3 + 1 <br> Conv 8 3 × 3 + 2 <br> Conv 16 3 × 3 + 1 <br> Conv 16 3 × 3 + 2 <br> Conv 32 3 × 3 + 1 <br> Conv 32 3 × 3 + 2 <br> Conv 64 3 × 3 + 1 <br> Conv 64 3 × 3 + 2 <br> FC 200 <br> FC 38 |
| **Fashion-MNIST** | Conv 256 3 × 3 + 1 <br> Conv 256 3 × 3 + 1 <br> Conv 256 3 × 3 + 2 <br> Conv 512 3 × 3 + 1 <br> Conv 512 3 × 3 + 1 <br> Conv 512 3 × 3 + 2 <br> FC 200 <br> FC 10 |

### Experiment Modifications for the Duality-Based Verifiable Defense

When dealing with the duality-based verifiable defense method [61, 62] (implemented in PyTorch), we find that the convolution with a kernel size 3 × 3 and a stride of 2, as described in Section 4, is not applicable. The defense method works by backpropagating the neural network to express the dual problem, and the convolution with a kernel size 3 × 3 and a stride of 2 prohibits their backpropagation analysis because the computation of the output size is not divisible by 2 (PyTorch uses a round-down operation). Instead, we choose the convolution with a kernel size 4 × 4 and a stride of 2 for the duality-based verifiable defense method [61, 62].

For the same reason, we also need to change the dimension of the Yale Face input to 192 × 192 by adding zero paddings. In our experiments, we validated that the natural models trained with these modifications have similar accuracy and privacy performance as the natural models without modifications reported in Tables 8 and 9.

### Membership Inference Attacks with Varying Perturbation Constraints

This section augments Section 7.1 to evaluate the success of membership inference attacks when the adversary does not know the \( l_\infty \) perturbation constraints of robust models.

We perform membership inference attacks with varying perturbation budgets on robust Fashion-MNIST and CIFAR10 classifiers [33, 50, 66]. The Fashion-MNIST classifiers are robustly trained with an \( l_\infty \) perturbation constraint of 0.1, while the CIFAR10 classifiers are robustly trained with an \( l_\infty \) perturbation constraint of 8/255. The membership inference attack results with varying perturbation constraints are shown in Figures 8 and 9.

### Privacy Risks of Other Robust Training Algorithms

Several recent papers [9, 29] propose adding a noise layer into the model for adversarial robustness. Here, we evaluate the privacy risks of the robust training algorithm proposed by Lecuyer et al. [29], which is built on the connection between differential privacy and model robustness. Specifically, Lecuyer et al. [29] add a noise layer with a Laplace or Gaussian distribution into the model architecture, such that small changes in the input image with an \( l_p \) perturbation constraint can only lead to bounded changes in neural network outputs after the noise layer. We exploit benign examples' predictions to perform membership inference attacks (IB) against the robust CIFAR10 classifier provided by Lecuyer et al. [29], which is robustly trained for an \( l_2 \) perturbation budget of 0.1 with a Gaussian noise layer. Our results show that the robust classifier has a membership inference accuracy of 64.43%, compared to 55.85% for the natural classifier.

**Figure 8:** Membership inference accuracy on robust Fashion-MNIST classifiers [33, 50, 66] trained with the \( l_\infty \) perturbation constraint of 0.1. The privacy leakage is evaluated via the inference strategy IA based on adversarial examples generated with varying perturbation budgets.

**Figure 9:** Membership inference accuracy on robust CIFAR10 classifiers [33, 50, 66] trained with the \( l_\infty \) perturbation constraint of 8/255. The privacy leakage is evaluated via the inference strategy IA based on adversarial examples generated with varying perturbation budgets.