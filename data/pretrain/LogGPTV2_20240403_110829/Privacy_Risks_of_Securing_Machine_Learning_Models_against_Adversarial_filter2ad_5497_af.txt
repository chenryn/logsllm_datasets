in the adversarial setting, resulting a 2.3× membership inference
advantage than the natural Yale Face classifier in Table 2.
Furthermore, the failure of robustness generalization may partly
be due to inappropriate (toy) distance constraints that are used to
model adversaries. Although lp perturbation constraints have been
widely adopted in both attacks and defenses for adversarial exam-
ples [5, 15, 33, 61], the lp distance metric has limitations. Sharif et al.
[45] empirically show that (a) two images that are perceptually sim-
ilar to humans can have a large lp distance, and (b) two images with
a small lp distance can have different semantics. Jacobsen et al. [23]
further show that robust training with a lp perturbation constraint
makes the model more vulnerable to another type of adversarial
examples: invariance based attacks that change the semantics of
the image but leave the model predictions unchanged. Meaningful
perturbation constraints to capture evasion attacks continue to be
an important research challenge. We leave the question of deciding
whether the privacy-robustness conflict is fundamental (i.e., will
hold for next generation of defenses against adversarial examples)
as an open question for the research community.
8 CONCLUSIONS
In this paper, we have connected both the security domain and the
privacy domain for machine learning systems by investigating the
membership inference privacy risk of robust training approaches
(that mitigate the adversarial examples). To evaluate the member-
ship inference risk, we propose two new inference methods that
exploit structural properties of adversarially robust defenses, beyond
the conventional inference method based on the prediction confi-
dence of benign input. By measuring the success of membership
inference attacks on robust models trained with six state-of-the-art
adversarial defense approaches, we find that all six robust training
methods will make the machine learning model more susceptible to
membership inference attacks, compared to the naturally undefended
training. Our analysis further reveals that the privacy leakage is
related to target model’s robustness generalization, its adversarial
perturbation constraint, and its capacity. We also provide thorough
discussions on the adversary’s prior knowledge, potential counter-
measures and the relationship between privacy and robustness. The
detailed analysis in our paper highlights the importance of thinking
about security and privacy together. Specifically, the membership
inference risk needs to be considered when designing approaches
to defend against adversarial examples.
ACKNOWLEDGMENTS
We are grateful to anonymous reviewers at ACM CCS for valu-
able insights, and would like to specially thank Nicolas Papernot
for shepherding the paper. This work was supported in part by
the National Science Foundation under grants CNS-1553437, CNS-
1704105, CIF-1617286 and EARS-1642962, by the Office of Naval
Research Young Investigator Award, by the Army Research Office
Young Investigator Prize, by Faculty research awards from Intel and
IBM, and by the National Research Foundation, Prime Minister’s
Office, Singapore, under its Strategic Capability Research Centres
Funding Initiative.
REFERENCES
[1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov,
Kunal Talwar, and Li Zhang. 2016. Deep learning with differential privacy. In
ACM Conference on Computer and Communications Security (CCS). 308–318.
[2] Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta,
Kuzman Ganchev, Slav Petrov, and Michael Collins. 2016. Globally normalized
transition-based neural networks. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics (ACL). 2442–2452.
[3] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, and Jorge Luis
Reyes-Ortiz. 2013. A public domain dataset for human activity recognition using
smartphones.. In European Symposium on Artificial Neural Networks (ESANN).
[4] Anish Athalye, Nicholas Carlini, and David Wagner. 2018. Obfuscated gradients
give a false sense of security: Circumventing defenses to adversarial examples.
In International Conference on Machine Learning (ICML).
[5] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Šrndić, Pavel
Laskov, Giorgio Giacinto, and Fabio Roli. 2013. Evasion attacks against machine
learning at test time. In European Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases (ECML PKDD). 387–402.
[6] Battista Biggio, Blaine Nelson, and Pavel Laskov. 2012. Poisoning attacks against
support vector machines. In International Conference on Machine Learning (ICML).
1467–1474.
[7] Battista Biggio and Fabio Roli. 2018. Wild patterns: Ten years after the rise of
[8] Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness of
adversarial machine learning. Pattern Recognition 84 (2018), 317–331.
neural networks. In IEEE Symposium on Security and Privacy (S&P). 39–57.
[9] Jeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. 2019. Certified adversarial
robustness via randomized smoothing. In International Conference on Machine
Learning (ICML).
[10] Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu,
and Pavel Kuksa. 2011. Natural language processing (almost) from scratch.
Journal of Machine Learning Research 12, Aug (2011), 2493–2537.
[11] Li Deng, Geoffrey Hinton, and Brian Kingsbury. 2013. New types of deep neural
network learning for speech recognition and related applications: An overview. In
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).
8599–8603.
[12] Karan Ganju, Qi Wang, Wei Yang, Carl A Gunter, and Nikita Borisov. 2018.
Property inference attacks on fully connected neural networks using permutation
invariant representations. In ACM Conference on Computer and Communications
Security (CCS). 619–633.
[13] Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat
Chaudhuri, and Martin Vechev. 2018. AI2: Safety and robustness certification of
neural networks with abstract interpretation. In IEEE Symposium on Security and
Privacy (S&P). 3–18.
[14] Athinodoros S Georghiades, Peter N Belhumeur, and David J Kriegman. 2001.
From few to many: Illumination cone models for face recognition under variable
lighting and pose. IEEE Transactions on Pattern Analysis & Machine Intelligence 6
(2001), 643–660.
[15] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and
harnessing adversarial examples. In International Conference on Learning Repre-
sentations (ICLR).
[16] Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli
Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli. 2018. On the effec-
tiveness of interval bound propagation for training verifiably robust models. In
NeurIPS Workshop on Security in Machine Learning (SECML).
[17] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. 2017. On calibration of
modern neural networks. International Conference on Machine Learning (ICML).
[18] J Hayes, L Melis, G Danezis, and E De Cristofaro. 2018. LOGAN: Membership
inference attacks against generative models. In Proceedings on Privacy Enhancing
Technologies (PoPETs).
[19] Jamie Hayes and Olga Ohrimenko. 2018. Contamination attacks and mitigation
in multi-party machine learning. In Conference on Neural Information Processing
Systems (NeurIPS). 6602–6614.
[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In IEEE conference on computer vision and pattern
recognition (CVPR). 770–778.
[21] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed,
Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N
Sainath, et al. 2012. Deep neural networks for acoustic modeling in speech
recognition: The shared views of four research groups. IEEE Signal processing
magazine 29, 6 (2012), 82–97.
[22] Ling Huang, Anthony D Joseph, Blaine Nelson, Benjamin IP Rubinstein, and
JD Tygar. 2011. Adversarial machine learning. In ACM Workshop on Artificial
Intelligence and Security (AISec). 43–58.
[23] Jörn-Henrik Jacobsen, Jens Behrmannn, Nicholas Carlini, Florian Tramer, and
Nicolas Papernot. 2019. Exploiting excessive invariance caused by norm-bounded
adversarial robustness. In International Conference on Learning Representations
(ICLR) Workshop on Safe Machine Learning.
[24] Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru,
and Bo Li. 2018. Manipulating machine learning: Poisoning attacks and coun-
termeasures for regression learning. In IEEE Symposium on Security and Privacy
(S&P).
[25] Auguste Kerckhoffs. 1883. La cryptographic militaire. Journal des sciences mili-
taires (1883), 5–38.
[26] Manish Kesarwani, Bhaskar Mukhoty, Vijay Arya, and Sameep Mehta. 2018.
Model extraction warning in MLaaS paradigm. In Proceedings of the 34th Annual
Computer Security Applications Conference (ACSAC). ACM, 371–380.
[27] Pang Wei Koh and Percy Liang. 2017. Understanding black-box predictions via
influence functions. In International Conference on Machine Learning (ICML).
1885–1894.
[28] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet clas-
sification with deep convolutional neural networks. In Conference on Neural
Information Processing Systems (NeurIPS). 1097–1105.
[29] Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman
Jana. 2019. Certified robustness to adversarial examples with differential privacy.
In IEEE Symposium on Security and Privacy (S&P).
[30] Kuang-Chih Lee, Jeffrey Ho, and David J Kriegman. 2005. Acquiring linear
subspaces for face recognition under variable lighting.
IEEE Transactions on
Pattern Analysis & Machine Intelligence 5 (2005), 684–698.
[31] Taesung Lee, Benjamin Edwards, Ian Molloy, and Dong Su. 2019. Defending
against model stealing attacks using deceptive perturbations. In Deep Learning
and Security Workshop (DLS).
[32] Yunhui Long, Vincent Bindschaedler, and Carl A Gunter. 2017. Towards measur-
ing membership privacy. arXiv preprint arXiv:1712.09136 (2017).
[33] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2018. Towards deep learning models resistant to adversarial attacks.
In International Conference on Learning Representations (ICLR).
[34] Matthew Mirman, Timon Gehr, and Martin Vechev. 2018. Differentiable abstract
interpretation for provably robust neural networks. In International Conference
on Machine Learning (ICML). 3575–3583.
[35] Matej Moravčík, Martin Schmid, Neil Burch, Viliam Lis`y, Dustin Morrill, Nolan
Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. 2017.
Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Science
356, 6337 (2017), 508–513.
[36] Milad Nasr, Reza Shokri, and Amir Houmansadr. 2018. Machine learning with
membership privacy using adversarial regularization. In ACM Conference on
Computer and Communications Security (CCS).
[37] Milad Nasr, Reza Shokri, and Amir Houmansadr. 2019. Comprehensive privacy
analysis of deep learning: Passive and active white-box inference attacks against
centralized and federated learning. In IEEE Symposium on Security and Privacy
(S&P).
[38] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik,
and Ananthram Swami. 2016. The limitations of deep learning in adversarial
settings. In IEEE European Symposium on Security and Privacy (EuroS&P). 372–
387.
[39] Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael Wellman. 2018.
SoK: Security and privacy in machine learning. In IEEE European Symposium on
Security and Privacy (EuroS&P). 399–414.
[40] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. 2018. Certified defenses
against adversarial examples. In International Conference on Learning Representa-
tions (ICLR).
[41] Ahmed Salem, Yang Zhang, Mathias Humbert, Mario Fritz, and Michael Backes.
2019. ML-leaks: Model and data independent membership inference attacks
and defenses on machine learning models. In Network and Distributed Systems
Security Symposium (NDSS).
[42] Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Alek-
sander Madry. 2018. Adversarially robust generalization requires more data. In
Conference on Neural Information Processing Systems (NeurIPS).
[43] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer,
Tudor Dumitras, and Tom Goldstein. 2018. Poison frogs! Targeted clean-label poi-
soning attacks on neural networks. In Conference on Neural Information Processing
Systems (NeurIPS).
[44] Claude E Shannon. 1949. Communication theory of secrecy systems. Bell system
technical journal 28, 4 (1949), 656–715.
[45] Mahmood Sharif, Lujo Bauer, and Michael K Reiter. 2018. On the suitability of
LP -norms for creating and preventing adversarial examples. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops.
1605–1613.
[46] Reza Shokri and Vitaly Shmatikov. 2015. Privacy-preserving deep learning. In
ACM Conference on Computer and Communications Security (CCS). 1310–1321.
[47] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Mem-
bership inference attacks against machine learning models. In IEEE Symposium
on Security and Privacy (S&P). 3–18.
[48] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George
Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershel-
vam, Marc Lanctot, et al. 2016. Mastering the game of Go with deep neural
networks and tree search. Nature 529, 7587 (2016), 484.
[49] Karen Simonyan and Andrew Zisserman. 2015. Very deep convolutional net-
works for large-scale image recognition. In International Conference on Learning
Representations (ICLR).
[50] Aman Sinha, Hongseok Namkoong, and John Duchi. 2018. Certifying some
distributional robustness with principled adversarial training. In International
Conference on Learning Representations (ICLR).
[51] Chuanbiao Song, Kun He, Liwei Wang, and John E Hopcroft. 2019. Improving the
generalization of adversarial training with domain adaptation. In International
Conference on Learning Representations (ICLR).
[52] Congzheng Song, Thomas Ristenpart, and Vitaly Shmatikov. 2017. Machine
learning models that remember too much. In ACM Conference on Computer and
Communications Security (CCS). 587–601.
[53] Liwei Song, Reza Shokri, and Prateek Mittal. 2019. Membership inference attacks
against adversarially robust deep learning models. In Deep Learning and Security
Workshop (DLS).
[54] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from
overfitting. The Journal of Machine Learning Research 15, 1 (2014), 1929–1958.
[55] Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. 2017. Certified defenses
for data poisoning attacks. In Conference on Neural Information Processing Systems
(NeurIPS). 3517–3529.
[56] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks.
In International Conference on Learning Representations (ICLR).
[57] Antonio Torralba, Alexei A Efros, et al. 2011. Unbiased look at dataset bias.. In
IEEE conference on computer vision and pattern recognition (CVPR).
[58] Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart.
2016. Stealing machine learning models via prediction APIs.. In USENIX Security
Symposium. 601–618.
[59] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and
Aleksander Madry. 2019. Robustness may be at odds with accuracy. In Interna-
tional Conference on Learning Representations (ICLR).
[60] Binghui Wang and Neil Zhenqiang Gong. 2018. Stealing hyperparameters in
machine learning. In IEEE Symposium on Security and Privacy (S&P).
[61] Eric Wong and Zico Kolter. 2018. Provable defenses against adversarial examples
via the convex outer adversarial polytope. In International Conference on Machine
Learning (ICML). 5283–5292.
[62] Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J Zico Kolter. 2018. Scaling
provable adversarial defenses. In Conference on Neural Information Processing
Systems (NeurIPS).
[63] Han Xiao, Kashif Rasul, and Roland Vollgraf. 2017. Fashion-MNIST: a novel
image dataset for benchmarking machine learning algorithms. arXiv preprint
arXiv:1708.07747 (2017).
[64] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. 2018. Pri-
vacy risk in machine learning: Analyzing the connection to overfitting. In IEEE
Computer Security Foundations Symposium (CSF). 268–282.
[65] Sergey Zagoruyko and Nikos Komodakis. 2016. Wide residual networks. In
Proceedings of the British Machine Vision Conference (BMVC).
[66] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and
Michael I Jordan. 2019. Theoretically principled trade-off between robustness
and accuracy. In International Conference on Machine Learning (ICML).
A FINE-GRAINED ANALYSIS OF PREDICTION
LOSS OF THE ROBUST CIFAR10
CLASSIFIER
Here, we perform a fine-grained analysis of Figure 1a by separately
visualizing the prediction loss distributions for test points which
are secure and test points which are insecure. A point is deemed as
secure when it is correctly classified by the model for all adversarial
perturbations within the constraint Bϵ .
Figure 7: Histogram of the robust CIFAR10 classifier [33]
prediction loss values of both secure and insecure test exam-
ples. An example is called “secure” when it is correctly clas-
sified by the model for all adversarial perturbations within
the constraint Bϵ .
Note that only a few training points were not secure, so we fo-
cused our fine-grained analysis on the test set. Figure 7 shows that
insecure test inputs are very likely to have large prediction
loss (low confidence value). Our membership inference strate-
gies directly use the confidence to determine membership, so the
privacy risk has a strong relationship with robustness generaliza-
tion, even when we purely rely on the prediction confidence of the
benign unmodified input.
B MODEL ARCHITECTURE
We present the detailed neural network architectures used on Yale
Face, Fashion-MNIST and CIFAR10 datasets in Table 13.
Table 13: Model achitectures used on Yale Face, Fashion-
MNIST and CIFAR10 datasets. “Conv c w × h + s” represents
a 2D convolution layer with c output channels, kernel size
of w × h, and a stride of s, “Res c-n” corresponds to n residual
units [20] with c output channels, and “FC n” is a fully con-
nect layer with n neurons. All layers except the last FC layer
are followed by ReLU activations, and the final prediction is
obtained by applying the softmax function on last FC layer.
CIFAR10
Conv 16 3 × 3 + 1
Res 160-5
Res 320-5
Res 640-5
FC 10
Yale Face
Conv 8 3 × 3 + 1
Conv 8 3 × 3 + 2
Conv 16 3 × 3 + 1
Conv 16 3 × 3 + 2
Conv 32 3 × 3 + 1
Conv 32 3 × 3 + 2
Conv 64 3 × 3 + 1
Conv 64 3 × 3 + 2
FC 200
FC 38
Fashion-MNIST
Conv 256 3 × 3 + 1
Conv 256 3 × 3 + 1
Conv 256 3 × 3 + 2
Conv 512 3 × 3 + 1
Conv 512 3 × 3 + 1
Conv 512 3 × 3 + 2
FC 200
FC 10
C EXPERIMENT MODIFICATIONS FOR THE
DUALITY-BASED VERIFIABLE DEFENSE
When dealing with the duality-based verifiable defense method
[61, 62] (implemented in PyTorch), we find that the convolution
with a kernel size 3 × 3 and a stride of 2 as described in Section 4 is
not applicable. The defense method works by backpropagating the
neural network to express the dual problem, while the convolution
with a kernel size 3 × 3 and a stride of 2 prohibits their backpropa-
gation analysis as the computation of output size is not divisible
by 2 (PyTorch uses a round down operation). Instead, we choose
the convolution with a kernel size 4 × 4 and a stride of 2 for the
duality-based verifiable defense method [61, 62].
For the same reason, we also need to change the dimension of
the Yale Face input to be 192 × 192 by adding zero paddings. In
our experiments, we have validated that the natural models trained
with the above modifications have similar accuracy and privacy
performance as the natural models without modifications reported
in Table 8 and Table 9.
D MEMBERSHIP INFERENCE ATTACKS
WITH VARYING PERTURBATION
CONSTRAINTS
This section augments Section 7.1 to evaluate the success of mem-
bership inference attacks when the adversary does not know the
l∞ perturbation constraints of robust models.
We perform membership inference attacks with varying pertur-
bation budgets on robust Fashion-MNIST and CIFAR10 classifiers
[33, 50, 66]. The Fashion-MNIST classifiers are robustly trained with
the l∞ perturbation constraint of 0.1, while the CIFAR10 classifiers
are robustly trained with the l∞ perturbation constraint of 8/255.
The membership inference attack results with varying perturbation
constraints are shown in Figure 8 and Figure 9.
E PRIVACY RISKS OF OTHER ROBUST
TRAINING ALGORITHMS
Several recent papers [9, 29] propose to add a noise layer into the
model for adversarial robustness. Here we evaluate privacy risks
of the robust training algorithm proposed by Lecuyer et al. [29],
which is built on the connection between differential privacy and
model robustness. Specifically, Lecuyer et al. [29] add a noise layer
with a Laplace or Gaussian distribution into the model architecture,
such that small changes in the input image with a lp perturbation
constraint can only lead to bounded changes in neural network
outputs after the noise layer. We exploit benign examples’ predic-
tions to perform membership inference attacks (IB) against the
robust CIFAR10 classifier provided by Lecuyer et al. [29]1, which is
robustly trained for a l2 perturbation budget of 0.1 with a Gaussian
noise layer. Our results show that the robust classifier has a mem-
bership inference accuracy of 64.43%. In contrast, the membership
inference accuracy of the natural classifier is 55.85%.
Figure 8: Membership inference accuracy on robust Fashion-
MNIST classifiers [33, 50, 66] trained with the l∞ perturba-
tion constraint of 0.1. The privacy leakage is evaluated via
the inference strategy IA based on adversarial examples gen-
erated with varying perturbation budgets.
1https://github.com/columbia/pixeldp
Figure 9: Membership inference accuracy on robust CI-
FAR10 classifiers [33, 50, 66] trained with the l∞ perturba-
tion constraint of 8/255. The privacy leakage is evaluated via
the inference strategy IA based on adversarial examples gen-
erated with varying perturbation budgets.