                        "a61809f3-fb5b-465c-8bff-23a8a068ac60",
                        "f3e22c8b-ea47-45d1-b502-b57b6de950b3"
                    ]
                    if any([re.search("|".join(non_dataset_packages), i, re.IGNORECASE)
                            for i in rule.contents.data.index]):
                        if not rule.contents.metadata.integration and rule.id not in ignore_ids:
                            err_msg = f'substrings {non_dataset_packages} found in '\
                                      f'{self.rule_str(rule)} rule index patterns are {rule.contents.data.index},' \
                                      f'but no integration tag found'
                            failures.append(err_msg)
        if failures:
            err_msg = """
                The following rules have missing or invalid integrations tags.
                Try updating the integrations manifest file:
                    - `python -m detection_rules dev integrations build-manifests`\n
                """
            self.fail(err_msg + '\n'.join(failures))
    def test_invalid_queries(self):
        invalid_queries_eql = [
            """file where file.fake: (
                "token","assig", "pssc", "keystore", "pub", "pgp.asc", "ps1xml", "pem", "gpg.sig", "der", "key",
                "p7r", "p12", "asc", "jks", "p7b", "signature", "gpg", "pgp.sig", "sst", "pgp", "gpgz", "pfx", "crt",
                "p8", "sig", "pkcs7", "jceks", "pkcs8", "psc1", "p7c", "csr", "cer", "spc", "ps2xml")
            """
        ]
        invalid_integration_queries_eql = [
            """file where event.dataset == "google_workspace.drive" and event.action : ("copy", "view", "download") and
                    google_workspace.drive.fake: "people_with_link" and source.user.email == "" and
                    file.extension: (
                        "token","assig", "pssc", "keystore", "pub", "pgp.asc", "ps1xml", "pem", "gpg.sig", "der", "key",
                        "p7r", "p12", "asc", "jks", "p7b", "signature", "gpg", "pgp.sig", "sst", "pgp", "gpgz", "pfx",
                        "crt", "p8", "sig", "pkcs7", "jceks", "pkcs8", "psc1", "p7c", "csr", "cer", "spc", "ps2xml")
            """,
            """file where event.dataset == "google_workspace.drive" and event.action : ("copy", "view", "download") and
                    google_workspace.drive.visibility: "people_with_link" and source.user.email == "" and
                    file.fake: (
                        "token","assig", "pssc", "keystore", "pub", "pgp.asc", "ps1xml", "pem", "gpg.sig", "der", "key",
                        "p7r", "p12", "asc", "jks", "p7b", "signature", "gpg", "pgp.sig", "sst", "pgp", "gpgz",
                        "pfx", "crt", "p8", "sig", "pkcs7", "jceks", "pkcs8", "psc1", "p7c", "csr", "cer", "spc",
                        "ps2xml")
            """
        ]
        valid_queries_eql = [
            """file where file.extension: (
                "token","assig", "pssc", "keystore", "pub", "pgp.asc", "ps1xml", "pem",
                "p7r", "p12", "asc", "jks", "p7b", "signature", "gpg", "pgp.sig", "sst",
                "p8", "sig", "pkcs7", "jceks", "pkcs8", "psc1", "p7c", "csr", "cer")
            """,
            """file where event.dataset == "google_workspace.drive" and event.action : ("copy", "view", "download") and
                    google_workspace.drive.visibility: "people_with_link" and source.user.email == "" and
                    file.extension: (
                        "token","assig", "pssc", "keystore", "pub", "pgp.asc", "ps1xml", "pem", "gpg.sig", "der", "key",
                        "p7r", "p12", "asc", "jks", "p7b", "signature", "gpg", "pgp.sig", "sst", "pgp", "gpgz", "pfx",
                        "p8", "sig", "pkcs7", "jceks", "pkcs8", "psc1", "p7c", "csr", "cer", "spc", "ps2xml")
            """
        ]
        invalid_queries_kql = [
            """
            event.fake:"google_workspace.admin" and event.action:"CREATE_DATA_TRANSFER_REQUEST"
              and event.category:"iam" and google_workspace.admin.application.name:Drive*
            """
        ]
        invalid_integration_queries_kql = [
            """
            event.dataset:"google_workspace.admin" and event.action:"CREATE_DATA_TRANSFER_REQUEST"
              and event.category:"iam" and google_workspace.fake:Drive*
            """
        ]
        valid_queries_kql = [
            """
            event.dataset:"google_workspace.admin" and event.action:"CREATE_DATA_TRANSFER_REQUEST"
              and event.category:"iam" and google_workspace.admin.application.name:Drive*
            """,
            """
            event.dataset:"google_workspace.admin" and event.action:"CREATE_DATA_TRANSFER_REQUEST"
            """
        ]
        base_fields_eql = {
            "author": ["Elastic"],
            "description": "test description",
            "index": ["filebeat-*"],
            "language": "eql",
            "license": "Elastic License v2",
            "name": "test rule",
            "risk_score": 21,
            "rule_id": str(uuid.uuid4()),
            "severity": "low",
            "type": "eql"
        }
        base_fields_kql = {
            "author": ["Elastic"],
            "description": "test description",
            "index": ["filebeat-*"],
            "language": "kuery",
            "license": "Elastic License v2",
            "name": "test rule",
            "risk_score": 21,
            "rule_id": str(uuid.uuid4()),
            "severity": "low",
            "type": "query"
        }
        def build_rule(query: str, query_language: str):
            metadata = {
                "creation_date": "1970/01/01",
                "integration": ["google_workspace"],
                "updated_date": "1970/01/01",
                "query_schema_validation": True,
                "maturity": "production",
                "min_stack_version": load_current_package_version()
            }
            if query_language == "eql":
                data = base_fields_eql.copy()
            elif query_language == "kuery":
                data = base_fields_kql.copy()
            data["query"] = query
            obj = {"metadata": metadata, "rule": data}
            return TOMLRuleContents.from_dict(obj)
        # eql
        for query in valid_queries_eql:
            build_rule(query, "eql")
        for query in invalid_queries_eql:
            with self.assertRaises(eql.EqlSchemaError):
                build_rule(query, "eql")
        for query in invalid_integration_queries_eql:
            with self.assertRaises(ValueError):
                build_rule(query, "eql")
        # kql
        for query in valid_queries_kql:
            build_rule(query, "kuery")
        for query in invalid_queries_kql:
            with self.assertRaises(kql.KqlParseError):
                build_rule(query, "kuery")
        for query in invalid_integration_queries_kql:
            with self.assertRaises(ValueError):
                build_rule(query, "kuery")
    def test_event_dataset(self):
        for rule in self.all_rules:
            if(isinstance(rule.contents.data, QueryRuleData)):
                # Need to pick validator based on language
                if rule.contents.data.language == "kuery":
                    test_validator = KQLValidator(rule.contents.data.query)
                if rule.contents.data.language == "eql":
                    test_validator = EQLValidator(rule.contents.data.query)
                data = rule.contents.data
                meta = rule.contents.metadata
                if meta.query_schema_validation is not False or meta.maturity != "deprecated":
                    if isinstance(data, QueryRuleData) and data.language != 'lucene':
                        packages_manifest = load_integrations_manifests()
                        pkg_integrations = TOMLRuleContents.get_packaged_integrations(data, meta, packages_manifest)
                        validation_integrations_check = None
                        if pkg_integrations:
                            # validate the query against related integration fields
                            validation_integrations_check = test_validator.validate_integration(data,
                                                                                                meta,
                                                                                                pkg_integrations)
                        if(validation_integrations_check and "event.dataset" in rule.contents.data.query):
                            raise validation_integrations_check
class TestIntegrationRules(BaseRuleTest):
    """Test integration rules."""
    @unittest.skip("8.3+ Stacks Have Related Integrations Feature")
    def test_integration_guide(self):
        """Test that rules which require a config note are using standard verbiage."""
        config = '## Setup\n\n'
        beats_integration_pattern = config + 'The {} Fleet integration, Filebeat module, or similarly ' \
                                             'structured data is required to be compatible with this rule.'
        render = beats_integration_pattern.format
        integration_notes = {
            'aws': render('AWS'),
            'azure': render('Azure'),
            'cyberarkpas': render('CyberArk Privileged Access Security (PAS)'),
            'gcp': render('GCP'),
            'google_workspace': render('Google Workspace'),
            'o365': render('Office 365 Logs'),
            'okta': render('Okta'),
        }
        for rule in self.all_rules:
            integration = rule.contents.metadata.integration
            note_str = integration_notes.get(integration)
            if note_str:
                self.assert_(rule.contents.data.note, f'{self.rule_str(rule)} note required for config information')
                if note_str not in rule.contents.data.note:
                    self.fail(f'{self.rule_str(rule)} expected {integration} config missing\n\n'
                              f'Expected: {note_str}\n\n'
                              f'Actual: {rule.contents.data.note}')
    def test_rule_demotions(self):
        """Test to ensure a locked rule is not dropped to development, only deprecated"""
        versions = default_version_lock.version_lock
        failures = []
        for rule in self.all_rules:
            if rule.id in versions and rule.contents.metadata.maturity not in ('production', 'deprecated'):
                err_msg = f'{self.rule_str(rule)} a version locked rule can only go from production to deprecated\n'
                err_msg += f'Actual: {rule.contents.metadata.maturity}'
                failures.append(err_msg)
        if failures:
            err_msg = '\n'.join(failures)
            self.fail(f'The following rules have been improperly demoted:\n{err_msg}')
    def test_all_min_stack_rules_have_comment(self):
        failures = []
        for rule in self.all_rules:
            if rule.contents.metadata.min_stack_version and not rule.contents.metadata.min_stack_comments:
                failures.append(f'{self.rule_str(rule)} missing `metadata.min_stack_comments`. min_stack_version: '
                                f'{rule.contents.metadata.min_stack_version}')
        if failures:
            err_msg = '\n'.join(failures)
            self.fail(f'The following ({len(failures)}) rules have a `min_stack_version` defined but missing comments:'
                      f'\n{err_msg}')
class TestRuleTiming(BaseRuleTest):
    """Test rule timing and timestamps."""
    def test_event_override(self):
        """Test that timestamp_override is properly applied to rules."""
        # kql: always require (fallback to @timestamp enabled)
        # eql:
        #   sequences: never
        #   min_stack_version = 8.2: any - fallback to @timestamp enabled https://github.com/elastic/kibana/pull/127989
        errors = {
            'query': {
                'errors': [],
                'msg': 'should have the `timestamp_override` set to `event.ingested`'
            },
            'eql_sq': {
                'errors': [],
                'msg': 'cannot have the `timestamp_override` set to `event.ingested` because it uses a sequence'
            },
            'lt_82_eql': {
                'errors': [],
                'msg': 'should have the `timestamp_override` set to `event.ingested`'
            },
            'lt_82_eql_beats': {
                'errors': [],
                'msg': ('eql rules include beats indexes. Non-elastic-agent indexes do not add the `event.ingested` '
                        'field and there is no default fallback to @timestamp for EQL rules <8.2, so the override '
                        'should be removed or a config entry included to manually add it in a custom pipeline')
            },
            'gte_82_eql': {
                'errors': [],
                'msg': ('should have the `timestamp_override` set to `event.ingested` - default fallback to '
                        '@timestamp was added in 8.2')
            }
        }
        pipeline_config = ('If enabling an EQL rule on a non-elastic-agent index (such as beats) for versions '
                           '<8.2, events will not define `event.ingested` and default fallback for EQL rules '
                           'was not added until 8.2, so you will need to add a custom pipeline to populate '
                           '`event.ingested` to @timestamp for this rule to work.')
        for rule in self.all_rules:
            if rule.contents.data.type not in ('eql', 'query'):
                continue
            if isinstance(rule.contents.data, QueryRuleData) and 'endgame-*' in rule.contents.data.index:
                continue
            has_event_ingested = rule.contents.data.timestamp_override == 'event.ingested'
            indexes = rule.contents.data.get('index', [])
            beats_indexes = parse_beats_from_index(indexes)
            min_stack_is_less_than_82 = Version.parse(rule.contents.metadata.min_stack_version or '7.13.0',
                                                      optional_minor_and_patch=True) < Version.parse("8.2.0")
            config = rule.contents.data.get('note') or ''
            rule_str = self.rule_str(rule, trailer=None)
            if rule.contents.data.type == 'query':
                if not has_event_ingested:
                    errors['query']['errors'].append(rule_str)
            # eql rules depends
            elif rule.contents.data.type == 'eql':
                if rule.contents.data.is_sequence:
                    if has_event_ingested:
                        errors['eql_sq']['errors'].append(rule_str)
                else:
                    if min_stack_is_less_than_82:
                        if not beats_indexes and not has_event_ingested:
                            errors['lt_82_eql']['errors'].append(rule_str)
                        elif beats_indexes and has_event_ingested and pipeline_config not in config:
                            errors['lt_82_eql_beats']['errors'].append(rule_str)
                    else:
                        if not has_event_ingested:
                            errors['gte_82_eql']['errors'].append(rule_str)
        if any([v['errors'] for k, v in errors.items()]):
            err_strings = ['errors with `timestamp_override = "event.ingested"`']
            for _, errors_by_type in errors.items():
                type_errors = errors_by_type['errors']
                if not type_errors:
                    continue