bits. We show the results for all 15 original images
for comparison purposes, although those considered
unsuitable in Stage 1 (unbolded) would normally not
be shown to users when creating passwords.
Image
# ACP # Detected CP Proportion Detected Entropy per CP
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Average
1995
2007
1986
1995
2001
2028
2019
2013
2016
2013
2013
2007
2010
2010
2004
2007.8
756
135
1471
1585
749
1245
203
1250
1639
881
1262
1325
1344
1112
906
1057.5
38%
7%
74%
79%
37%
61%
10%
62%
81%
44%
63%
66%
67%
55%
45%
53%
0.42
0.30
1.22
5.88
0.74
5.56
0.59
2.82
3.99
1.64
2.24
3.89
3.12
1.28
1.36
2.34
on all images, regardless of the image’s guiding features.
Thus, we consider these 19x19 pixel regions in the image
corners as RoI.
Segmentation and Labeling
3.3.4
Each 19x19 pixel RoI was labeled with a detected object
type based on its location. For example, an RoI is labeled
as an eye object if the region falls within the boundary of
a detected eye object. If the RoI covers two diﬀerent types
of detected object because of overlap, the RoI is labeled as
containing the object that has shortest path from the center
point to the RoI’s center point. If an RoI has no detected
objects within it, it is labeled with NONE.
After labeling each 19x19 pixel region, we relate this to
the ACP that comprise real passwords as taken from the
training data. For each ACP, it belongs to that object if
the point falls within the object’s boundary. Furthermore,
each selected point may belong to at most one object. If the
point falls into more than one object’s boundary because of
overlapping objects, the selected point belongs to the object
that has shortest path from the point to the object’s center
point.
3.4 Relating Detected Objects to ACP
Table 2 shows that the detected RoI predicted an average
of 53% of ACP. The lowest percentages were in images 2,
7, 5, and 1 and we believe that this is due to the image’s
content. For example, image 2 (see Figure 3a) is simple
(a small jet against the otherwise unremarkable sky), and
image 5 (see Figure 3d) has a small car logo against an
equally unremarkable car hood. Since these images have
few hotspots, users may be forced choose regions that are
not detected as RoI. The highest number of ACP appearing
in detected objects was in image 9 with 81%. The faces
of two people cover the majority of image, which provides
many detected objects that may also encourage clicks.
None of the images had all ACP related to a detected ob-
ject. This may be due to a semantic gap between what is
detected on an image and what the user perceives as impor-
tant or memorable. It is unlikely that this intuition can be
automated unless input from users is applied. Another ex-
planation may be because ACP appear in areas where there
are no objects to detect, but where the image bounds provide
guidance. For instance, we have noticed that a proportion of
ACP fall in the image corners for all 15 images, even though
there are often no objects to draw the user’s attention. We
have (somewhat artiﬁcially) added these as “detected” ob-
jects because they are easily located in the image simply by
locating the image’s boundaries. However, many of the im-
ages in Figure 2 show ACP that are not over a detectable
object or otherwise easily located via image boundaries. We
have no knowledge of why the user chose such points as they
are unlikely to be memorable in the future.
The semantic gap between clicks and object detection is
a signiﬁcant component of this work. The object detection
algorithms are only able to predict what might be a face,
circle, etc., but not whether the user’s attention is actually
drawn to it. Images may contain features that make sense
for users, perhaps due to other memorable links, but the
user’s semantic representation was not replicable by the ob-
ject detection algorithms. For example, the image’s corners
are memorable due to the image boundaries, but may not
contain detectable RoIs.
3.5 Region Selection Likelihood
As shown in Tables 2 and 3, the ACP were frequently on
objects we detected in the ﬁfteen images.
Intuition leads
us to use the click frequency of an RoI as an indication of
the likelihood of a user clicking on that RoI in the future.
However, simply calculating a likelihood that an actual click
point appears in a given RoI without considering the RoI’s
actual content may lead to inaccuracies. For example, there
is high likelihood that faces will be chosen in images that
contain faces (e.g., Image 11), but this behavior is not ex-
pected for images with no faces (e.g., Image 5). As a result,
calculating the percentages of RoIs with no consideration of
the content of the images (as in Table 2) will lead to unre-
alistic assumptions. For instance, 33% of click points were
on a detected face object in Image 3 (see Table 3), but only
3% of ACP were on a detected face in Image 7. Thus, the
average of 8.3% of ACP is not appropriate for all images
since diﬀerences in the other RoIs or the proportion of the
chosen face to other objects may have an eﬀect on whether
or not the user clicks on any one object detected in the im-
age. In order to calculate a more accurate likelihood that
a user may click on a particular object, we considered two
issues: the size of detected objects and the variety of object
types detected. For instance, Image 6 has only circle objects
detected, and Image 8 has face and circle objects (if we ig-
nored image corners for both images). Table 3 shows that
the likelihood of clicking on a circle object for Image 6 is
55%, but for Image 8 is only 4%. The reason could be that
the size of circle object in Image 6 is larger than in Image
8. Not only that, but also Image 8 has diﬀerent other ob-
jects such as face that may be a target for users’ taps, while
Image 6 has no other objects that may attract taps. As a
result, the size of detected objects and the content of images
should be considered for accuracy.
3.5.1 Image Categorization
In order to consider similar images (i.e., those with simi-
lar RoI content), we categorized the ﬁfteen images based on
their content. We used the type of detected objects to cat-
egorize the images where F represents a detected face, G a
detected generic object and C for a detected circle. Combi-
132
Table 3: Percentages of ACP related by the diﬀerent
types of objects per image. Blanks imply 0%.
Img
Face
Eye Nose Mouth Generic Circle Corners
33%
11%
24%
3%
21%
12%
32%
24%
8%
7%
14%
22%
7%
8%
2%
5%
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Average
8.3% 1.0% 3.1% 3.1%
31%
9%
2%
1%
65%
32%
55%
4%
26%
2%
21%
33%
13%
49%
39%
13.9% 17.2%
27%
49%
6%
7%
6%
5%
6%
6%
5%
6%
5%
7%
6%
6%
5%
6%
6%
5.9%
Table 4: The number of regions covered by detected
objects aggregated by the object type and category.
Face Eye Nose Mouth Generic Circle TL TR BL BR
1
3
3
3
2
2
1
Category
F
FG
FGC
G
GC
C
NONE
0
0
264
0
161
82
0
0
47
43
58
30
0
0
11
16
27
0
0
0
0
4
35
11
0
0
0
0
0
7
10
0
0
0
0
4
0
11
0
0
0
0
1
3
3
3
2
2
1
1
3
3
3
2
2
1
1
3
3
3
2
2
1
nations of these objects are represented as concatenations of