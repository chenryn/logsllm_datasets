### Significant Numbers of Robustness Failures in Well-Known Operating Systems

Ballista-like testing, while not a substitute for other testing activities, can serve as an effective and low-cost method to check the overall quality of software. This is due to its scalability and automation capabilities, which also allow for estimating the extent of potential problems.

Several studies have utilized Ballista and CRASH metrics to evaluate the robustness of various systems. For instance, in [127], different operating systems are tested and compared using Ballista and CRASH. The most frequently detected robustness issues include invalid file pointers, NULL file pointers, invalid buffer pointers, NULL buffer pointers, MININT integers, and MAXINT integers. Fernsler and Koopman [51] used Ballista exception handling to assess the robustness of the high-level architecture of run-time infrastructure (RTI), a distributed simulation system with robust exception handling. In another study, Jiantao et al. [80] extended Ballista to test the exception handling robustness of C++ ORB client-side application interfaces and provided a simple probing method to eliminate straightforward cases of robustness failures.

JCrasher [37] is another fuzz testing tool similar to Ballista, specifically designed for Java applications. JCrasher offers several unique features: it performs transitive analysis of methods, determines the size of each tested method's parameter space, and selects parameter combinations and test cases at random, taking into account the allocated testing time. It also defines heuristics to determine whether a Java exception should be considered a program bug or if the inputs supplied by JCrasher violated the code's preconditions.

In other research, Ghosh et al. [62, 60, 61, 143] developed an instrumentation layer that wraps executable application software to capture, record, perturb, and question all interactions with the operating system. This wrapper returns error codes and exceptions from calls to operating system functions, and the effects of these failures are then assessed. If the system crashes, it is deemed non-robust.

Belli et al. [18] proposed a model-based approach to robustness testing, using event sequence graphs and decision tables. These models are later modified by the testing application to generate robustness test cases. Other automated robustness testing tools identified in this review include [167, 45, 53, 118, 119, 104, 103].

### Other Work

There are additional significant contributions that do not fit into the above categories. De Vale and Koopman [39] conducted a theoretical study, arguing that software developers often cite performance and practicality as the main reasons why software systems are not made robust. They suggest that using automated wrappers and robustness testing techniques can address many of these issues. Additionally, they claim that Maxion’s hypothesis—that developers without specific training on the topic might not fully grasp exceptional conditions—holds true. Therefore, training developers in robustness improvement techniques is an effective way to enhance software robustness.

Henzinger [68] identified two challenges in embedded systems design: predictability and robustness. He defined robustness as a form of continuity, where a system's reaction changes slightly if the environment changes slightly, and its execution properties change slightly if the platform changes slightly. This theory is used to create a model for how a robust system should behave and how it can be tested.

Maxion [114] categorized the reasons for program failures into logic errors in the code and exception failures, with the latter accounting for up to two-thirds of system crashes. He tested the hypothesis that robustness for exception failures can be improved through the use of dependability cases, a methodology based on structured taxonomies and memory aids to help software designers improve exception-handling coverage.

Other important contributions include Nebut et al. [123], who presented a method for generating robustness tests using requirement contracts; Mendes et al. [116], who proposed a method to benchmark the robustness of web servers; and Luo et al. [109], who developed a method for robustness test generation and execution using input syntax and interaction scenarios.

### System Focus

Table 4 categorizes the primary studies based on their main system focus. The main categories are commercial off-the-shelf (COTS) systems, distributed and network systems, embedded systems, operating systems, real-time and safety-critical systems, and web applications. Some studies focused on other types of systems, listed as "Other." There is also a "General" category for studies that do not focus on any specific type of system and whose results can be applied to various contexts.

| System Focus        | Papers                                                                                       |
|---------------------|----------------------------------------------------------------------------------------------|
| General             | [2] [4] [10] [17] [18] [19] [24] [26] [27] [29] [31] [37] [39] [41] [42] [43] [52] [54] [55] [58] [69] [70] [71] [75] [79] [80] [83] [84] [86] [87] [88] [109] [111] [113] [114] [120] [121] [122] [123] [126] [131] [132] [38] [134] [135] [136] [140] [141] [144] [151] [152] [156] [164] [166] [168] [104] [103] [59] [64] [146] [9] [40] [63] [110] [72] [74] [16] [25] [45] [78] [160] [98] [117] [158] [159] [167] [96] [153] [4] [8] [32] [53] [61] [81] [85] [127] [94] [93] [95] [107] [118] [119] [125] [143] [148] [28] [57] [56] [65] [139] [100] [102] [112] [145] [162] [163] [46] [101] [116] [15] [35] [36] [60] [62] [97] [108] [128] [129] [142] [154] [165] [169] [20] [30] [44] [66] [67] [73] [77] [115] [133] [157] [161] [6] [14] [68] [105] [137] [138] [7] [12] [33] [51] [149] [150] |
| Other               | [9] [40] [63] [110] [72] [74] [16] [25] [45] [78] [160] [98] [117] [158] [159] [167] [96] [153]       |
| Operating System    | [4] [8] [32] [53] [61] [81] [85] [127] [94] [93] [95] [107] [118] [119] [125] [143] [148]           |
| Web Application     | [15] [35] [36] [60] [62] [97] [108] [128] [129] [142] [154] [165] [169] [20] [30] [44] [66] [67] [73] [77] [115] [133] [157] [161] [6] [14] [68] [105] [137] [138] [7] [12] [33] [51] [149] [150] |
| COTS                | [28] [57] [56] [65] [139] [100] [102] [112] [145] [162] [163] [46] [101] [116]                      |
| Real-Time / Safety Critical | [15] [35] [36] [60] [62] [97] [108] [128] [129] [142] [154] [165] [169] [20] [30] [44] [66] [67] [73] [77] [115] [133] [157] [161] [6] [14] [68] [105] [137] [138] [7] [12] [33] [51] [149] [150] |
| Embedded System     | [15] [35] [36] [60] [62] [97] [108] [128] [129] [142] [154] [165] [169] [20] [30] [44] [66] [67] [73] [77] [115] [133] [157] [161] [6] [14] [68] [105] [137] [138] [7] [12] [33] [51] [149] [150] |
| Distributed & Network | [15] [35] [36] [60] [62] [97] [108] [128] [129] [142] [154] [165] [169] [20] [30] [44] [66] [67] [73] [77] [115] [133] [157] [161] [6] [14] [68] [105] [137] [138] [7] [12] [33] [51] [149] [150] |

COTS, operating systems, and web applications are the categories with the most contributions. For COTS, the majority of studies focus on finding design or verification methods to ensure system robustness. Similar attention is given to web applications. However, for operating systems, the main focus is on evaluating and testing the robustness of different parts, primarily using fuzz testing techniques like Ballista and its extensions.

### Quality of Research/Contribution

This section evaluates the quality of the primary studies based on their research type and contribution. The quality is ranked using several criteria, including the type of research, the contribution facet, and the type of evaluation performed.

#### Research Type

Figure 2 provides statistics on the type of study in the selected primary studies. Many studies conduct multiple types of research, so the main contribution was considered for categorization. Most studies included some form of evaluation, which will be discussed in Section 4.3.3. Evaluation as a research type refers to studies that:

1. Evaluate the robustness of a system using an existing method or tool.
2. Evaluate an existing method by applying it to different systems.

Review studies are secondary studies that review a field to answer a specific research question. Solution proposals include studies that provide new contributions or extend existing ones.

As seen in Figure 2, the majority of the research involves solution proposals or evaluations. This suggests a need for more overview studies to coordinate and summarize existing research in the field.

Notably, three review studies are included in this review. The first [38] reviews the state of the art in real-time software development, with a minor focus on robustness. The second [151] provides an overview of the use of formal methods in developing robust and reliable safety-critical systems. The third [151] compares different techniques for handling incomplete data in databases, with robustness as one of the comparison criteria. These overviews show that studies similar to the one presented here have not been previously done.

Our experience indicates that despite numerous existing results, the industry often struggles to adopt them. One solution is to perform overview studies like this one to present academic results to the industry. Another is to understand the industry's problems and find the reasons behind this, as done in our previous study [147].

#### Contribution Facet

Figure 3 categorizes the studies based on their contributions. Most papers had more than one type of contribution, which was addressed similarly to the research type issue.

Evaluation is defined as in Section 4.3.1. The number of studies classified as evaluation in the contribution facet differs from the research type because, in some cases, although the study focused on evaluation, the main contribution was a metric, method, or tool.

The majority of contributions were in the form of frameworks, methods, or models. A framework is a detailed method with a broad purpose, addressing several research questions or areas. A method has a more specific goal and a narrow research question or purpose. A model provides an abstract classification or model of a topic and problem rather than a specific solution.

Many studies provided a tool for evaluating their concept, but only those where the tool was the major topic were classified in the tool category. Metrics provide guidelines for measuring different aspects of robustness.

Figure 3 shows a relatively even distribution in the contribution facets of the studies found. However, there are fewer reviews, and no systematic review with a more general focus, further motivating the need for the current study.

#### Type of Evaluation

Figure 4 provides statistics on how the primary studies were evaluated. Academic lab/toy refers to studies where a small program was developed for evaluation or a case study on a small commercial system was performed. Academic OSS (Open Source System) refers to studies where the evaluation was done using an open source system, typically providing more reliable results.

Large academic evaluations involve studies on large commercial systems or a large number of small systems. These studies are usually reliable and large enough to prove a hypothesis. Small industrial evaluations refer to studies where the industrial context is mentioned but the evaluation is done on one or two small projects. The industry category includes studies performed in an industrial context with large projects or more than two small projects, providing strong and typically equivalent results to large academic studies.