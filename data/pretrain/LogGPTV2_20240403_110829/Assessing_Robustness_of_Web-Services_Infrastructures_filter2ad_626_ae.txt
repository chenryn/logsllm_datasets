niﬁcant numbers of robustness failures in many well known operating systems [94, 97, 148].
Ballista-like testing might not be a substitute for other testing activities but it can serve to check
the overall quality of software at a low cost due to its scalability and automation. Furthermore, it
can estimate the extent of potential problems.
Several studies have used Ballista and the CRASH metrics to evaluate robustness of diﬀerent
kinds of systems. In [127] diﬀerent operating systems are tested and compared using Ballista and
CRASH. Invalid ﬁle pointers, NULL ﬁle pointers, Invalid buﬀer pointers, NULL buﬀer pointers,
MININT integers, and MAXINT integers are the most detected robustness problems. Fernsler
17
and Koopman [51] use Ballista exception handling to evaluate robustness of the high-level ar-
chitecture of run-time infrastructure (RTI) which is a distributed simulation system providing
robust exception handling. In another study, Jiantao et al. [80] extend Ballista to test the excep-
tion handling robustness of C++ ORB client-side application interfaces. They also provide a
simple probing method for eliminating simple cases of robustness failures.
JCrasher [37] is another Ballista-like fuzz testing tool specialized for Java applications.
“JCrasher oﬀers several novelties: it transitively analyzes methods, determines the size of each
tested methods parameter space, and selects parameter combinations and therefore test cases at
random, taking into account the time allocated for testing; it deﬁnes heuristics for determin-
ing whether a Java exception should be considered as a program bug or whether the JCrasher
supplied inputs have violated the codes preconditions” [37].
In other studies, Ghosh et al. [62, 60, 61, 143] wrap “executable application software with
an instrumentation layer that can capture, record, perturb, and question all interactions with
the operating system. The wrapper is used to return error codes and exceptions from calls to
operating system functions. The eﬀect of the failure from the OS call is then assessed. If the
system crashes, it is non-robust” [62].
Belli et al. propose a model-based approach to robustness testing [18]. The models consist
of event sequence graph and decision tables which are later tweaked by the testing application in
order to generate robustness test cases.
Some other automated robustness testing tools identiﬁed in this review are presented in [167,
45, 53, 118, 119, 104, 103].
4.1.5. Other Work
There are some other major contributions that could not be classiﬁed in any of the above
sections. Therefore, we will discuss them separately here.
In a theoretical study, De Vale and Koopman [39] argue that software developers identify
two main reasons why software systems are not made robust: performance and practicality.
They claim however that, by using automated wrappers and robustness testing techniques many
of these problems can be solved. Furthermore, in a case study they claim that Maxion’s hy-
pothesis that “developers without speciﬁc training on the topic might not fully grasp exceptional
conditions seems to hold” [39]. Thereby, they suggest that training developers to use robustness
improvement techniques is another eﬀective way of increasing robustness of a software.
Another theoretical paper about robustness is [68] where Henzinger identiﬁes two challenges
in embedded systems design: predictability and robustness. In this paper robustness is regarded
as a form of continuity since in a robust system the reaction changes slightly if the environment
changes slightly, and its execution properties change slightly if the platform changes slightly.
This theory is used to create a model of how a robust system should behave and how it can be
tested.
In another study, Maxion [114] divides the reason for programs failures into two main cat-
egories: logic errors in the code, and exception failures. Exception failures can account for up
to 2/3 of system crashes. Then he goes on to test the hypothesis that robustness for exception
failures can be improved through the use of dependability cases. “Dependability cases, derived
from safety cases, comprise a methodology based on structured taxonomies and memory aids for
helping software designers think about and improve exception-handling coverage” [114].
Some other important contributions are made by Nebut et al. [123] who present a method
that generates robustness tests using requirement contracts, Mendes et al. [116] who propose a
18
method to benchmark the robustness of web servers, and Luo et al. [109] who have developed a
method for robustness test generation and execution using input syntax and interaction scenarios.
4.2. System Focus
Table 4 shows the primary studies categorized based on their main system focus. The main
categories found were commercial-oﬀ-the-shelves (COTS), distributed & network systems, em-
bedded systems, operating systems, real time & safety critical systems and web applications.
There were some results that focused on other types of systems than the ones mentioned above.
These are listed as other. There also exists a category general which includes studies that do
not have any special kind of system in focus and their results can be applied to many diﬀerent
contexts and systems.
System focus
General
Other
Operating System
Web Application
COTS
Real-Time / Safety Critical
Embedded System
Distributed & Network
Table 4: System focus of the studies
Papers
[2] [4] [10] [17] [18] [19] [24] [26] [27] [29] [31] [37]
[39] [41] [42] [43] [52] [54] [55] [58] [69] [70] [71] [75]
[79] [80] [83] [84] [86] [87] [88] [109] [111] [113] [114]
[120] [121] [122] [123] [126] [131] [132] [38] [134]
[135] [136] [140] [141] [144] [151] [152] [156] [164]
[166] [168] [104] [103] [59] [64] [146]
[9] [40] [63] [110] [72] [74] [16] [25] [45] [78] [160] [98]
[117] [158] [159] [167] [96] [153]
[4] [8] [32] [53] [61] [81] [85] [127] [94] [93] [95] [107]
[118] [119] [125] [143] [148]
[28] [57] [56] [65] [139] [100] [102] [112] [145] [162]
[163] [46] [101] [116]
[15] [35] [36] [60] [62] [97] [108] [128] [129] [142] [154]
[165] [169]
[20] [30] [44] [66] [67] [73] [77] [115] [133] [157] [161]
[6] [14] [68] [105] [137] [138]
[7] [12] [33] [51] [149] [150]
#
60
18
17
13
13
11
6
6
COTS, operating systems and web applications are the categories with most contributions.
In the case of COTS the majority of studies focus on ﬁnding design or veriﬁcation methods
to ensure robustness of a system using COTS. This matter was discussed in Section 4.1 when
the focus area of each study was presented. The same applies to web applications. However,
regarding operating systems the main focus is to evaluate and test the robustness of diﬀerent
parts of them, mainly using fuzz testing techniques. Ballista and its extensions are commonly
used tools for this purpose.
4.3. Quality of Research/Contribution
This section discusses the quality of the primary studies based on their research type and
contribution. The quality is ranked based on several criteria here. The type of research is dis-
cussed in 4.3.1. Another criterion is the contribution facet (the type of contribution) which is
19
Figure 2: Research type
presented in Section 4.3.2. The last criterion is the type of evaluation performed to evaluate the
contributions. This criterion is discussed in Section 4.3.3.
4.3.1. Research Type
Figure 2 shows the statistics on the type of the study in the selected primary studies. Many of
the studies conduct several types of research. Therefore, the main contribution of the study was
considered for categorization.
Most selected studies had some kind of evaluation which will be discussed in Section 4.3.3.
However, evaluation as research type below refers to studies that do one of the following:
1. Evaluate robustness of a system using an existing method or tool
2. Evaluate an already existing method by applying it to diﬀerent systems
Studies in the review category are the ones that are secondary studies reviewing a ﬁeld to answer a
speciﬁc research question. Solution proposals include studies that provide any new contributions
or extend already existing ones.
As seen in Figure 2, the absolute majority of the research involves solution proposal or evalu-
ation. The results suggest that there is a need for more overview studies like this one to coordinate
and summarize the existing studies in the ﬁeld.
It is notable that there are three review studies included in this review. The ﬁrst review
is presented in [38] and gives the state of art for development of real-time software where a
minor focus is robustness. The second review [151] provides an overview of the use of formal
methods in the developing robust and reliable safety-critical systems. The last review [151]
compares diﬀerent techniques for handling incomplete data in databases. One of the criteria for
the comparison is robustness. This overview was given to show that studies similar to the one
presented in this paper have not been previously done.
Our experience suggests that despite several existing results, the industry usually has prob-
lems adopting these results. One way to solve that problem is to perform overview studies like
this one to present the academic results to the industry. The other way is to try to understand the
20
Figure 3: Contribution facet
problems of the industry and ﬁnd their reasons behind this. This was done in our previous study
presented in [147].
4.3.2. Contribution facet
Figure 3 shows a categorization of the studies based on their contributions. Similar to the
issue discussed about the research type, most papers had more than one type of contribution.
This was addressed in the same way as discussed in Section 4.3.1.
Evaluation had the same deﬁnition as the one presented in Section 4.3.1. The reason why
there are not the same number of studies of the type evaluation in contribution and research type
is that in some cases, although the study focused on evaluating a system or method, the main
contribution was a type of metrics, method or tool which was considered more important than
the evaluation results itself.
The majority of contributions were in the form of frameworks, methods or models. A frame-
work is a detailed method which has a wide purpose and focuses on several research questions
or areas. However, a method usually has a more speciﬁc goal and a narrow research question
or purpose. A model is diﬀerent from both the contribution facets mentioned above in the sense
that it gives an abstract classiﬁcation or model of a topic and problem rather than a speciﬁc and
tangible way of solving a speciﬁc problem.
Alongside providing a model, framework, evaluation or method, many studies provided a tool
for evaluating their concept. These studies were not classiﬁed in the tool category as contribution
facet. Only the studies where the tool was the major topic are classiﬁed in this category.
Metrics is another type of contribution facet that provides guidelines for how to measure
diﬀerent aspects of robustness.
Figure 3 suggests there is a relatively even distribution in the contribution facets of the studies
found. However, there number of reviews are much smaller and there is no systematic review
with a more general focus, which further motivates the need for the current study.
21
Figure 4: Type of evaluation
4.3.3. Evaluation
One of the important metrics for measuring the strength of academic results is their evalua-
tions. Figure 4 gives statistics on how the primary studies found in this review were evaluated.
Academic lab/toy refers to studies where for the purpose of evaluation a small program has
been developed or when a case study on a small commercial system was performed. Academic
OSS (Open Source System) refers to the studies where the evaluation was done using an open
source system. The results of these studies are usually more reliable than the previous category.
Large academic evaluations refer to the studies where the evaluation is done on large commercial
systems or a large number of small systems. These systems can be large commercial products,
but if there is no indication of performing action research in an industrial context, the studies
were classiﬁed in this category. These studies are usually reliable and large enough for proving
a hypothesis.
Small industrial evaluation refers to studies where the industrial context is mentioned but
the evaluation is done on one or two small projects. The industry category includes studies
performed in an industrial context which include large projects or more than two small projects.
The results of these studies are also strong and typically on the same level as large academic