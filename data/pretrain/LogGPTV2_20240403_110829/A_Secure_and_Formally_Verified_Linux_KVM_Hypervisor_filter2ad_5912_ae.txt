1.8K
3.6K
1.8K
14.7K
1.1K
3.7K
32.7K
TABLE II: LOC for retrofitting and verifying SeKVM.
Name
Hypercall
I/O Kernel
I/O User
Virtual IPI
Description
Transition from the VM to the hypervisor and return to the VM
without doing any work in the hypervisor. Measures bidirectional
base transition cost of hypervisor operations.
Trap from the VM to the emulated interrupt controller in the
hypervisor OS kernel, then return to the VM. Measures base cost
of operations that access I/O devices supported in kernel space.
Trap from the VM to the emulated UART in QEMU and then
return to the VM. Measures base cost of operations that access
I/O devices emulated in user space.
Issue a virtual IPI from a VCPU to another VCPU running on a
different CPU, both CPUs executing VM code. Measures time be-
tween sending the virtual IPI until the receiving VCPU handles it.
TABLE III: Microbenchmarks.
unmodified KVM verified KVM
Microbenchmark
3,720
Hypercall
4,864
I/O Kernel
10,903
I/O User
10,699
Virtual IPI
TABLE IV: Microbenchmark performance (cycles).
2,896
3,831
9,288
8,816
data structures maintained by KCore and proofs to verify security
properties. We did not link HACL’s F* proofs with our Coq proofs,
or our Coq proofs for C code with those for Arm assembly code. The
latter requires a verified compiler for Arm multiprocessor code; no
such compiler exists. The Coq development effort took two person-
years. These results show that microverification of a commodity
hypervisor can be accomplished with modest proof effort.
VII. PERFORMANCE
We compare the performance of unmodified Linux 4.18 KVM
versus our retrofitted KVM, SeKVM, both integrated with QEMU
2.3.50 to provide virtual I/O devices. We kept the software environ-
ments across all platforms as uniform as possible. All hosts and VMs
used Ubuntu 16.04.06 with the same Linux 4.18 kernel. All VMs
used paravirtualized I/O, typical of cloud deployments [41]. In both
cases, KVM was configured with its standard virtio [42] network,
and with cache=none for its virtual block storage devices [43],
[44], [45]. For running a VM using SeKVM, we modified its guest
OS virtio frontend driver to use grant/revoke to explicitly enable
shared memory communication with the backend drivers in KServ.
We ran benchmarks in VMs using unmodified KVM or SeKVM.
Unless otherwise indicated, all VM instances were configured
with 4 VCPUs, 12 GB RAM, and huge page support enabled. All
experiments were run on a 64-bit Armv8 AMD Seattle (Rev.B0)
server with 8 Cortex-A57 CPU cores, 16 GB of RAM, a 512 GB
SATA3 HDD for storage, an AMD 10 GbE (AMD XGBE) NIC
device. The hardware we used supports Arm VE, but not VHE [46],
[47]; SeKVM does not yet support VHE.
A. Microbenchmarks
We ran KVM unit tests [48] to measure the cost of common
micro-level hypervisor operations listed in Table III. Table IV shows
the microbenchmarks measured in cycles for unmodified KVM
and SeKVM. SeKVM incurs 17% to 28% overhead over KVM,
but provides verified VM protection. The overhead is highest for
the simplest operations because the relatively fixed cost of KCore
protecting VM data is a higher percentage of the work that must
be done. These results provide a conservative measure of overhead
since real hypervisor operations will invoke actual KServ functions,
not just measure overhead for a null hypercall.
bug by modifying KCore to update stage 2 page tables only when a
mapping was previously empty.
3) Huge page ownership: When KServ allocated a 2MB page
for a VM, KCore initially only validated the ownership of the first
4KB page rather than all the 512 4KB pages, leaving a loophole
for KServ to access VM memory. We fixed this bug by accounting
for this edge case in our validation logic.
4) Multiple I/O devices using same physical page: KCore
initially did not manage memory ownership correctly when a
physical page was mapped to multiple KServ SMMU page tables,
with each page table controlling DMA access for a different I/O
device, allowing KServ devices to access memory already assigned
to VMs. We fixed this bug by having KCore only map a physical
page to a VM’s stage 2 or SMMU page tables when it is not already
mapped to an SMMU page table used by KServ’s devices.
5) SMMU static after VM boot: KCore initially did not ensure
that mappings in SMMU page tables remain static after VM boot.
This could allow KServ to modify SMMU page table mappings to
compromise VM data. We fixed this bug by modifying KCore to
check the state of the VM that owned the device before updating its
SMMU page tables, and only allow updates before VM boot.
VI. IMPLEMENTATION
The SeKVM implementation is based on KVM from mainline
Linux 4.18. Table II shows our retrofitting effort, measured by LOC
in C and assembly. 1.5K LOC were modified in existing KVM code,
a tiny portion of the codebase, such as adding calls to KCore hyper-
calls. 70 LOC were also added to QEMU to support secure VM boot
and VM migration. 10.1K LOC were added for the implementation
of Ed25519 and AES in the HACL* [14] verified crypto library.
Other than HACL*, KCore consisted of 3.8K LOC, 3.4K LOC in C
and .4K LOC in assembly, of which .5K LOC were existing KVM
code. The entire retrofitting process took one person-year. These
results demonstrate that a widely-used, commodity hypervisor may
be retrofitted with only modest implementation effort.
All of KCore’s C and assembly code is verified. Table II shows
our proof effort, measured by LOC in Coq. 6K LOC were for
KCore’s 34 layer specifications; 1.7K LOC were for the top layer
which defines all of KCore’s behavior while the rest were for the
other 33 layers to enable modular refinement. Although using layers
requires additional effort to write 33 more layer specifications, this
is more than made up for by the reduction in proof effort from
decomposing refinement into simpler proofs for each layer that
can be reused and composed together. 1.8K LOC were for the
machine model. 20.1K LOC were for refinement proofs, including
proofs between KCore’s C and assembly code modules and their
specifications, and proofs between layer specifications. 4.8K LOC
were for noninterference proofs, including invariant proofs for
1792
Name
Kernbench
Hackbench
Netperf
Apache
MySQL
MongoDB
Description
Compilation of the Linux 4.9 kernel using allnoconfig
for Arm with GCC 5.4.0.
hackbench [49] using Unix domain sockets and 100 process
groups running in 500 loops.
netperf v2.6.0 [50] running netserver on the server
and the client with its default parameters in three modes:
TCP_STREAM (receive throughput), TCP_MAERTS (send
throughput), and TCP_RR (latency).
Apache v2.4.18 server handling 100 concurrent requests
from remote ApacheBench [51] v2.3 client, serving the 41
KB index.html of the GCC 4.4 manual.
with its default parameters.
MySQL v14.14 (distrib 5.7.26) running SysBench v.0.4.12
using the default configuration with 200 parallel transactions.
MongoDB v4.0.20 server handling requests from a remote
YCSB [52] v0.17.0 client running workload A with 16 concur-
rent threads, readcount=500000, and operationcount=100000.
Memcached memcached v1.4.25 using the memtier benchmark v1.2.3
TABLE V: Application benchmarks.
B. Application Benchmarks
We evaluated performance using real application workloads listed
in Table V. For client-server experiments, the clients ran natively
on an x86 Linux machine with 24 Intel Xeon CPU 2.20 GHz cores
and 96 GB RAM. The clients communicated with the server via
a 10 GbE network connection. To evaluate VM performance with
end-to-end I/O protection, all VMs are configured with Full Disk
Encryption (FDE) in their virtual disk. Using FDE did not have
a significant impact on performance overhead, so results without
FDE are omitted due to space constraints. We used dm-crypt to
create a LUKS-encrypted root partition of the VM filesystem. To
evaluate the extra costs in VM performance, we normalized the
VM results to native hardware without FDE.
We ran application workloads using the following six
configurations: (1) native hardware, (2) multiprocessor (SMP) VM
on unmodified KVM (KVM), (3) SMP VM on SeKVM (SeKVM),
(4) SMP VM on SeKVM without vhost (SMP-no-vhost), and
(5) SMP VM on SeKVM without vhost or huge page support
(SMP-no-vhost-no-huge), and (6) uniprocessor VM on SeKVM
without vhost or huge page support (UP-no-vhost-no-huge). We
ran benchmarks on native hardware using 4 CPUs and the same
amount of RAM to provide a common basis for comparison.
SMP-no-vhost, SMP-no-vhost-no-huge, and UP-no-vhost-no-huge
were used to quantify the performance impact of not having verified
kernel support for virtual I/O (vhost in KVM) [53], huge pages,
and multiprocessor VM execution on multiple CPUs. For VMs, we
pinned each VCPU to a specific physical CPU and ensured that
no other work was scheduled on that CPU [46], [54], [55], [56].
To conservatively highlight microverification’s impact on VM
performance, we measured application performance without full
network encryption, because its cost would mask any overhead
between SeKVM and unmodified KVM. However, for applications
that provide an option to use end-to-end encryption, specifically
Apache and MySQL which have TLS/SSL support, we measured
their performance with and without that option enabled, to show
how the option affects overhead.
Figure 7 shows the overhead for each hypervisor configuration,
normalized to native execution. On real application workloads,
SeKVM incurs only modest overhead compared to unmodified
KVM. In most cases, the overhead for SeKVM is similar to unmod-
ified KVM and less than 10% compared to native. The worst over-
Fig. 7: Application benchmark performance. Overhead for each
hypervisor relative to native execution. A lower score indicates less
overhead; 1 means the performance in a VM is the same as native hardware.
head for SeKVM versus unmodified KVM is for TCP_MAERTS,
which measures bulk data send performance from the VM to a
client. Unmodified KVM achieves near native performance here
because virtio batches packet sends from the VM without trapping.
The cost is greater for SeKVM because the guest OS virtio driver
must trap to KCore to grant KServ access for each packet, though
this can be optimized further. TCP_STREAM, which measures bulk
data receive performance, does not have this overhead because the
virtio backend driver batches packet processing for incoming traffic,
resulting in the additional traps happening less often.
In contrast, the performance of SMP-no-vhost, SMP-no-vhost-
no-huge and UP-no-vhost-no-huge is much worse than KVM and
SeKVM. SMP-no-vhost shows that lack of kernel-level virtual I/O
support can result in more than two times worse performance for
network I/O related workloads such as TCP_STREAM, TCP_RR,
Apache, and Memcached. SMP-no-vhost-no-huge shows that lack
of huge page support adds between 35% to 50% more overhead ver-
sus SMP-no-vhost for hackbench, MySQL, and MongoDB. UP-no-
vhost-no-huge shows that lack of multiprocessor VM support results
in many benchmarks having more than 4 times worse performance
than SeKVM. TCP_STREAM and TCP_MAERTS are bandwidth
limited and TCP_RR is latency bound, so the performance loss
due to using only 1 CPU is smaller than other benchmarks. The
results suggest that a verified system without support for commodity
hypervisor features such as kernel-level virtual I/O, huge pages, and
multiprocessor VMs will have relatively poor performance.
VIII. RELATED WORK
Hypervisor verification. seL4 [4], [57] and CertiKOS [5], [58]
are verified systems with hypervisor functionality, so we compared
their virtualization features against SeKVM. We compare the
verified versions of each system; there is little reason to use
unverified versions of seL4 or CertiKOS (mC2) instead of KVM.
Table VI shows that SeKVM provides verified support for all listed
virtualization features while seL4 and CertiKOS do not. A key
verified feature of SeKVM is page tables that can be shared across
multiple CPUs, which became possible to verify by introducing
transparent trace refinement. This makes it possible to provide
verified support for multiprocessor VMs on multiprocessor hardware
as well as DMA protection. Another key verified feature of SeKVM
is noninterference in the presence of I/O through shared devices
paravirtualized using virtio, made possible by introducing data
1793
SeKVM
Feature
VM boot protection
Verified+FC
VM CPU protection
Verified+FC
VM memory protection Verified+FC
VM DMA protection Verified+FC
Server hardware
SMP hardware
SMP VMs
Multiple VMs
Shared page tables
Multi-level paging
Huge pages
Virtio
Device passthrough
VM migration
Linux ease-of-use
Verified
Verified
Verified
Verified
seL4 CertiKOS
Unverified
Verified+FC
Verified+FC Unverified
Verified
Verified Unverified Unverified
Verified
Verified
Verified
TABLE VI: Comparison of hypervisor features. For each feature,