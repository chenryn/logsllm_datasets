Ivy Bridge
Intel Core i7-2670QM Sandy Bridge
Sandy Bridge
Intel Core i5-2400
Intel Core i5 650
Intel Core2Duo T9400
Qualcomm Kryo 280
AMD A6-4455M
Nehalem
Core
ARMv8-A
Bulldozer
22
22
22
17
14
12
12
12
11
N/A
N/A
N/A
56
56
56
42
36
36
36
36
32
20
*
*
Table 1: 1 MB aliasing on various architectures: The tested
AMD and ARM architectures, and Intel Core generation do
not show similar effects. The Store Buffer (SB) sizes are
gathered from Intel Manual [24] and wikichip.org [57–59].
4.2 Leakage of the Physical Address Mapping
In this experiment, we evaluate whether the observed step-
wise latency has any relationship with the physical page num-
bers by observing the pagemap ﬁle. As shown in Figure 6,
we observe step-wise peaks with a very high latency which
appear once in every 256 pages on average.The 20 least sig-
niﬁcant bits of physical address for the load matches with
the physical addresses of the stores where high peaks for
virtual pages are observed. In our experiments, we always
detect peaks with different virtual addresses, which have the
matching least 20 bits of physical address. This observation
clearly discovers the existence of 1 MB aliasing effect based
on the physical addresses. This 1 MB aliasing leaks informa-
tion about 8 bits of mapping that were unknown to the user
space processes.
Matching this observation with the previously observed
Cycle_Activity:Stalls_Ldm_Pending with a high correla-
tion, the speculative load has been stalled to resolve the depen-
dency with conﬂicting store buffer entries after the occurrence
of a 1 MB aliased address. This observation veriﬁes that the
latency is due to the pending load. When the latency is at
the highest point, Ld_Blocks_Partial:Address_Alias drops
to zero, and it increments at each down step of the peak. This
implies that the loosenet check does not resolve the rest of the
store dependencies whenever there is a 1 MB aliased address
in the store buffer.
4.3 Evaluation
In the previous experiment, the execution time of the load
operation that is delayed by 1 MB aliasing decreases gradually
in each iteration (Figure 6). The number of steps to reach the
normal execution time is consistent on the same processor.
When the ﬁrst store in the window loop accesses a memory
address with the matching 1 MB aliased address, the latency is
at its highest point, marked as “1” in Figure 6. As the window
loop accesses this address later in the loop, it appears closer
to the load with a lower latency like the steps marked as 5, 15
and 22. This observation matches the carry chain algorithm
described by Intel [20] where the aliasing check starts from
the most recent store. As shown in Table 1, experimenting
with various processor generations shows that the number of
steps has a linear correlation with the size of the store buffer
which is architecture dependent. While the leakage exists on
all Intel Core processors starting from the ﬁrst generation,
the timing effect is higher for the more recent generations
with a bigger store buffer size. The analyzed ARM and AMD
processors do not show similar behavior4.
As our time measurement for speculative load suggests, it
is not possible to reason whether the high timing is due to a
very slow load or commitment of store operations. If the step-
wise delay matches the store buffer entries, this delay may be
either due to the the dependency resolution logic performing
a pipeline ﬂush and restart of the load for each 4 kB aliased
entry starting from the 1 MB aliased entry, or due to the load
waiting for all the remaining stores to commit because of
an unresolved hazard. To explore this further, we perform an
additional experiment with all store addresses replaced with
non-aliased addresses except for one. This experiment shows
that the peak disappears if there is only a single 4 kB and
1 MB aliased address in the store buffer.
Lastly, we run the same experiments on a shufﬂed set of vir-
tual addresses to assure that the contiguous virtual addresses
may not affect the observed leakage. Our experiment with the
shufﬂed virtual addresses exactly match the same step-wise
behavior suggesting that the upper bits in virtual addresses do
not affect the leakage behavior, and the leakage is solely due
to the aliasing on physical address bits.
4.3.1 Comparison of Address Aliasing Scenarios
We further test other address combinations to compare ad-
ditional address aliasing scenarios using Algorithm 1. As
shown by Figure 7, when stores and the load access dif-
ferent cache sets without aliasing, the load is executed in
30 cycles, which is the typical timing for an L1 data cache
load including the rdtscp overhead. When the stores have
different memory addresses with the same page offset, but
the load has a different offset, the load takes 100 cycles to
execute. This shows that even memory addresses in the store
buffer having 4K Aliasing conditions with each other that
are totally unrelated to the speculative load create a memory
bottleneck for the load. In the next scenario, 4K aliasing be-
tween the load and all stores, the average load time is about
200 cycles. While the aforementioned 4K aliasing scenarios
may leak cross domain information about memory accesses
4We use rdtscp for Intel and AMD processors and the clock_gettime
for ARM processors to perform the time measurements.
USENIX Association
28th USENIX Security Symposium    627
Figure 6: Step-wise peaks with 22 steps and a high latency can be observed on some of the pages (Core i7-8650U processor).
Every time the counter resets to zero, the next iteration of the
load will be blocked to be checked against the store buffer
entries. Mispredictions result in performance overhead due to
pipeline ﬂushes. To avoid repeated mispredictions, a watch-
dog mechanism monitors the success rate of the prediction,
and it can temporarily disable the memory disambiguator.
The predictor of the memory disambiguator should go into
a stable state after the ﬁrst few iterations, since the mem-
ory load is always truly independent of any aliased store.
Hence the saturating counter for the target speculative load
address passes the threshold, and it never resets due to a false
prediction. As a result, the memory disambiguator should
always fetch the data into the cache without any access to the
store buffer. However, since the memory disambiguation per-
forms speculation, the dependency resolution at some point
veriﬁes the prediction. The misprediction watchdog is also
supposed to only disable the memory disambiguator when
the misprediction rate is high, but in this case we should have
a high prediction rate. Accordingly, the observed leakage oc-
curs after the disambiguation and during the last stages of
dependency resolution, i.e., the memory disambiguator only
performs prediction on the 4K aliasing at the initial loosenet
check, and it cannot protect the pipeline from 1 MB aliasing
that appears at a later stage.
4.4.2 Hyperthreading Effect
Similar to the 4K Aliasing [40, 49], we empirically test
whether the 1 MB aliasing can be used as a covert/side chan-
nel through logical processors. Our observation shows that
when we run our experiments on two logical processors on
the same physical core, the number of steps in the peaks is
exactly halved. This matches the description by Intel [24]
where it is stated that the store buffer is split between the
logical processors. As a result, the 1 MB aliasing effect is not
visible and exploitable across logical cores. [31] suggests
that loosenet checks mask out the stores on the opposite
thread.
Figure 7: Histogram of the measurement for the speculative
load with various store addresses. Load will be fast, 30 cy-
cles, without any dependency. If there exists 4K aliasing only
between the stores, the average is 100. The average is 200
when there is 4K aliasing of load and stores. The 1 MB
aliasing has a distinctive high latency.
( Section 7), the most interesting scenario is the 1 MB aliasing
which takes more than 1200 cycles for the highest point in
the peak. For simplicity, we refer to the 1 MB aliased address
as aliased address, in the rest of the paper.
4.4 Discussion
4.4.1 The Curious Case of Memory Disambiguation
The processor uses an additional speculative engine, called
the memory disambiguator [10, 32], to predict memory false
dependencies and reduce the chance of their occurrences. The
main idea is to predict if a load is independent of preceding
stores and proceed with the execution of the load by ignor-
ing the store buffer. The predictor uses a hash table that is
indexed with the address of the load, and each entry of the
hash table has a saturating counter. If the pre-commitment
dependency resolution does not detect false dependencies,
the counter is incremented, otherwise it will be reset to zero.
After multiple successful executions of the same load instruc-
tion, the predictor assumes that the load is safe to execute.
628    28th USENIX Security Symposium
USENIX Association
01002003004005006007008009001000Page Number050010001500Cycle512215address is picked as a witness t and tested against a
candidate set C. If t is not evicted by C, it is added to
C and a new witness will be picked. As soon as t gets
evicted by C, C forms an eviction set for t.
• contract: Addresses are subsequently removed from the
eviction set. If the set still evicts t, the next address is
removed. If it does not evict t anymore, the removed
address is added back to the eviction set. At the end of
this phase, we have a minimal eviction set of the size of
the set associativity.
• collect: All addresses mapping to the already found evic-
tion set are removed from P by testing if they are evicted
by the found set. After ﬁnding 128 initial cache sets, this
approach utilizes the linearity property of the cache: For
each found eviction set, the bits 6-11 are enumerated
instead. This provides 63 more eviction sets for each
found set, leading to full cache coverage.
We test this approach on an Intel Core i7-4770 with four
physical cores and a shared 8MB 16-way L3 cache with
Chromium 68.0.3440.106, Firefox 62 and Firefox Developer
Edition 63. The approach yields an 80% accuracy rate to ﬁnd
all 8192 eviction sets when starting with a pool of 4096 pages.
The entire eviction set creation process takes an average of
46s. We improve the algorithm by 1) using the addresses
removed from the eviction set in the contract phase as a new
candidate set and 2) removing more than one address at a time
from the eviction set during the contract phase. The improved
eviction set creation process takes 35s on average.
5.1.1 Evaluation
The probability of ﬁnding a congruent address is P(C) =
2γ−c−s, where c is the number of bits determining the cache
set, γ is the number of bits attackers know, and s is the num-
ber of slices [56]. Since SPOILER allows us to control γ ≥ c
bits, we are only left with uncertainty about a few address
bits that inﬂuence the slice selection algorithm [26]. In the-
ory, the eviction set search is sped up by a factor of 4096
by using aliased addresses in the pool, since on average one
of 28 instead of one of 220 addresses is an aliased address.
Additionally, the address pool is much smaller, where 115
addresses are enough to ﬁnd all the eviction sets. In native
code, the overhead involved in ﬁnding the aliased addresses
is negligible, less than a second in our experiments. However,
in JavaScript, due to the noise, it takes 9s for ﬁnding aliased
addresses and then 3s for eviction set as compared to the base-
line of 46s for classic method in Table 2. Success rate however
is 100% with SPOILER as compared to 80% for the classic
method. Besides, success rate of the classical method can be
affected by the availability and consumption of memory on
the system.
From each aliased address pool, 4 eviction sets can be found
(corresponding to the 4 slices which are the only unknown
Figure 8: Reverse engineering physical page mappings in
JavaScript. The markers point to addresses having same 20
bits of physical addresses being part of the same eviction set.
5 SPOILER from JavaScript
Microarchitectural attacks from JavaScript have a high impact
as drive-by attacks in the browser can be accomplished with-
out any privilege or physical proximity. In such attacks, co-
location is automatically granted by the fact that the browser
loads a website with malicious embedded JavaScript code.
The browsers provide a sandbox where some instructions like
clflush and prefetch and ﬁle systems such as procfs are
inaccessible, limiting the opportunity for attack. Genkin et
al. [14] showed that side-channel attacks inside a browser
can be performed more efﬁciently and with greater porta-
bility through the use of WebAssembly.Yet, WebAssembly
introduces an additional abstraction layer, i.e. it emulates
a 32-bit environment that translates the internal addresses
to virtual addresses of the host process (the browser). We-
bAssembly only uses addresses of the emulated environment
and similar to JavaScript, it does not have direct access to
the virtual addresses. Using SPOILER from JavaScript opens
the opportunity to puncture these abstraction layers and to
obtain physical address information directly. Figure 8 shows
the address search in JavaScript using SPOILER. Compared
to native implementations, we replace the rdtscp measure-
ment with a timer based on a shared array buffer [19]. We
cannot use any fence instruction such as lfence, and as a
result, there remains some negligible noise in the JavaScript
implementation. However, the aliased addresses can still be
clearly seen, and we can use this information to improve the
state-of-the art eviction set creation for both Rowhammer and
cache attacks.
5.1 Efﬁcient Eviction Set Finding
We use the algorithm proposed in [14]. It is a slight improve-
ment to the former state-of-the-art brute force method [42]