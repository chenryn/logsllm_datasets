Proceedings of the 2002 IEEE Symposium on Security and Privacy (S&P(cid:146)02) 
1081-6011/02 $17.00 ' 2002 IEEE 
11
ropean Transactions on Telecommunications, 8(5):481–490,
1997.
[6] K. Goldberg, D. Gupta, M. Digiovanni, and H. Narita. Jester
2.0 : Evaluation of a new linear time collaborative ﬁltering
algorithm. In 22nd International ACM SIGIR Conference on
Research and Development in Information Retrieval, August
1999. Poster Session and Demonstration.
[7] J. Herlocker, J. Konstan, A. Borchers, and J. Riedl. An al-
gorithmic framework for performing collaborative ﬁltering.
In Proc. ACM SIGIR, 1999.
[8] J. Kubiatowicz, D. Bindel, Y. Chen, S. Czerwinski, P. Eaton,
D. Geels, R. Gummadi, S. Rhea, H. Weatherspoon,
W. Weimer, C. Wells, and B. Zhao. Oceanstore: An archi-
tecture for global-scale persistent storage. In Proc. 9th Int.
Conf. on Architectural Support for Programming Languages
and Operating Systems (ASPLOS 2000), November 2000.
[9] T. Pedersen. A threshold cryptosystem without a trusted
In Eurocrypt ’91, volume 547, pages 522–526.
party.
Springer-Verlag LNCS, 1991.
[10] D. Pennock and E. Horvitz. Collaborative ﬁltering by per-
sonality diagnosis: A hybrid memory- and model-based ap-
proach.
In IJCAI Workshop on Machine Learning for In-
formation Filtering, International Joint Conference on Arti-
ﬁcial Intelligence (IJCAI-99), Stockholm, Sweden, August
1999.
[11] E. Polak. Computational Methods in Optimization. Aca-
demic Press, New York, 1971.
[12] T. Rabin and M. Ben-Or. Veriﬁable secret-sharing and mul-
tiparty protocols with honest majority. In 21st ACM STOC,
pages 73–85, 1989.
[13] E. M. Rogers. Diffusion of Innovations, Fourth Edition. The
Free Press, 1995.
[14] B. M. Sarwar, G. Karypis, J. A. Konstan, and J. Riedl. Ap-
plication of dimensionality reduction in recommender sys-
tem – a case study. In ACM WebKDD 2000 Web Mining for
E-Commerce Workshop, 2000. Full length paper.
Appendix I: SVD via Vector Addition
Recall that the matrix A minimizes the squared error
e = tr(P P T ) − tr(P AT AP T ). To compute A, we use
a conjugate gradient algorithm. This allows us to compute
A in a number of rounds which equals the number of iter-
ations of the conjugate gradient algorithm (typically 40-60
for our application). This requires only summation of indi-
vidual user data, which can be done using the cryptographic
homomorphism described earlier.
Since tr(P P T ) is ﬁxed, minimizing e amounts to max-
imizing tr(P AT AP T ). Since A is an orthonormal repre-
sentation of a vector space, it is subject to the constraint
AAT = I. Using Lagrange multipliers, we need to maxi-
mize
where Λ is a k × k matrix of multipliers. The gradient of
this function is
tr(P AT AP T + Λ(AAT − I))
G = 2AP T P + (Λ + ΛT )A = 2AP T P + SA
where S is a symmetric matrix. For a vector G to lie on the
tangent manifold deﬁned by AAT = I, we must have
((A + G)(AT + GT ) − I)/ = 0
lim
→0
so
AGT + GAT = 0
substituting for G and solving gives
S = −2AP T P AT
and therefore:
G = 2AP T P (I − AT A)
Now suppose that A is the current approximation to the best
linear ﬁt to P . Let Pi denote the 1 × m matrix of data
(cid:2)n
from the ith user. Then P T P can be computed as P T P =
i=1 P T
i Pi and the gradient as:
n(cid:3)
G =
AP T
i Pi(I − AT A)
i=1
(cid:2)n
i=1 Gi where Gi = AP T
i Pi(I − AT A) is the ith
or G =
user’s contribution to the gradient. This is a key point. The
fact that the gradient is expressible as a sum of contributions
from each user makes it possible to compute A using only
addition of user data (and therefore using cryptographic ho-
momorphism). We assume that the current approximation
A(j) to A is known to everyone at the jth iteration. Then
user i computes their contribution G(j)
to the gradient G(j)
using the expression above. The user sends an encrypted
copy of G(j)
to the tallyer(s), which then sums all user con-
tributions to yield the encryption of G(j). The work to com-
pute each user’s contribution is O(km), assuming manifold
correction is done on the tallyer.
i
i
The next phase of conjugate gradient is calculation of the
extremum along the gradient direction. For this we need a
quadratic approximation to the value of the error function
e(t) a distance t along the gradient direction. Strictly speak-
ing, we actually move along a quadratic curve that tracks the
curvature of the manifold. The derivation up to second order
terms of e(t) is tedious but straightforward. The quadratic
approximation is e(t) ≈ e0 + e1t + e2t2 and the extremum
occurs at t ≈ −e1/(2e2). We need the two terms e1 and
e2. For convenience, we will break e2 into two components
e2 = a + b, and let e1 = c. Then the three quantities we
need are:
c = −2tr(P GT AP T )
a = −tr(P GT GP T )
b = tr(P AT GGT AP T )
and we note that each term has the form tr(P XP T ), and
involves private data P . All the data in each X is public,
Proceedings of the 2002 IEEE Symposium on Security and Privacy (S&P(cid:146)02) 
1081-6011/02 $17.00 ' 2002 IEEE 
12
involving the current approximation A and the gradient G.
Now notice that all products can be computed as
n(cid:3)
tr(P XP T ) =
PiXP T
i
i=1
where Pi is user i’s data as before. So just as with the gra-
dient computation, the line minimization step can be done
with summation of encrypted data. Each user computes
their contributions to (c, a, b) and sends them encrypted to
the tallyer(s). The computation per client is O(km) for
these terms.
The tallyer sums all the contributions and computes ﬁnal
encrypted totals (c, a, b). When these totals are decrypted
(next section), the tallyer(s) can compute the gradient step
size4 ts = −c/(2(a + b)). The tallyer then increments the
current estimate A(i) by marching along in the gradient di-
rection. When corrected for curvature, the new estimate is
A(i+1)
0
= A(i) + tsG − 1
2 t2
sGGT A(i)
Because of numerical error, A(i+1)
will not have orthonor-
mal rows. To obtain A(i+1) we apply a standard orthonor-
malization scheme, such as Gramm-Schmitt, to A(i+1)
.
0
0
Conjugate Gradient
A simple gradient scheme such as described above will have
slow (sub-quadratic) convergence. Conjugate gradient is a
good way to accelerate a minimization, and gives quadratic
convergence. The conjugate gradient is a moving average
of gradient directions. It requires a “one-step” memory of
the previous gradient, and requires only slight modiﬁcation
of the tallyer code (no changes are needed to inter-processor
communication). We used the Polak-Ribiere formula [11]
to compute a generalized gradient H based on G and the
H, G pairs from earlier iterations. This is a standard tech-
nique and we do not describe it here.
There is one complication with applying conjugate gra-
dient. We are working in a “moving” coordinate system.
That is, every gradient G or H has coordinates which are
based on the current approximation A(i). Gradients at two
different values of A cannot be compared or combined be-
cause they will not satisfy the condition AGT + GAT = 0
at the other A. Conjugate gradient requires a weighted sum
of gradients from two different time steps. These gradients
are in different coordinate systems and therefore cannot be
combined. Fortunately, the set of orthonormal A has a Lie
Algebra structure and there is a simple way to transform
4For large steps ts which occur early in the optimization, the estimator
e2 = a + b is ill-conditioned. Instead we use the less accurate but more
stable estimator e2 = b − a. The switch to e2 = a + b should be made
when convergence slows
between one coordinate system and another. Let A(i) and
A(i+1) be two orthonormal matrices (and coordinate sys-
tems). To transform a gradient G expressed in A(i)’s coor-
dinate system to A(i+1), we compute:
G(cid:7) = A(i+1)(A(i)T G − GT A(i))
when both the standard and conjugate gradients from the
previous step are transformed in this way, they can be used
in the Polak-Ribiere formula [11].
Computing Singular Value Decomposition
We can extend the least-squares scheme to compute a par-
tial singular value decomposition (SVD) of P . Recall that
an SVD is a factorization of P as P = U DV T where U
and V have orthonormal columns and D is a diagonal ma-
trix with real non-negative entries sorted in descending or-
der. It is known that the ﬁrst k columns of U give the op-
timal k-dimensional approximation to the columnspace of
P , while the ﬁrst k rows of V T give the best k-dimensional
approximation to the rowspace of P . It follows that if A has
been computed as in the previous sections, then rowspan(A)
equals the rowspan of the ﬁrst k rows of V T . We can re-
cover these rows by computing the eigenvectors and eigen-
values of a small (k × k) matrix.
Let B = AP T P AT and form the eigendecomposition
of B as B = W EW T where E is a diagonal matrix of
eigenvalues, and W is the matrix whose columns are the
corresponding eigenvectors. Notice that B is positive semi-
deﬁnite, and assume the real eigenvalues in E are sorted in
descending order. Then
D2 = E
V = W T A
The information needed to compute D and V is available at
the tallyer. The matrix B is easily computed from the gradi-
2 G(I−AT A)−1AT . Since B is k×k and k is
ent G as B = 1
small (typically k < 20), the eigenvalue calculation is inex-
pensive. Computing D and V from the eigenvalue decom-
position is also inexpensive, and requires O(k2m) steps.
Note that it is not possible to compute the matrix U in the
SVD. This is intentional. U contains information about spe-
ciﬁc users. The ith row of U encodes user i’s preferences
in the k-dimensional subspace. We do not store informa-
tion that would allow U to be recovered. Both D and V
however, contain useful information about patterns of user
preferences mapped onto the data items.
Appendix II
To simplify notation, suppose A, B respectively encrypt
a and b, then we give here a ZKP that b = a2. This proto-
col is a straightforward adaption of the multiplication pro-
totocol from [3]. The idea is to show that if the “message”
Proceedings of the 2002 IEEE Symposium on Security and Privacy (S&P(cid:146)02) 
1081-6011/02 $17.00 ' 2002 IEEE 
13
encypted in A is the ath power of γ, then the message in B
is the ath power of the message in A. We have to do this
without revealing a of course.
1. Prover knows sa, sb such that
A = (gsa, γahsa)mod p
B = (gsb, γbhsb)mod p
Prover chooses x, ra, rb ∈ Zq uniformly at random
and sends
Ca = (gra , γxhra)
Cb = Ax(grb , hrb)
and
to Veriﬁer.
2. Veriﬁer chooses c ∈ Zq uniformly at random and sends
it to Prover.
3. Prover computes
v = ca + x(mod q)
za = csa + ra(mod q)
zb = c(sb − asa) + rb(mod q)
and sends v, za, zb to veriﬁer.
4. Veriﬁer checks that
(gza , γvhza) = AcCa
Av(g, h)zb = BcCb
and accepts iff both identities hold.
This protocol is honest-veriﬁer zero-knowledge. The
simulation is: Choose c, v, za, zb ∈ Zq independently
and uniformly at random, and compute Ca and Cb to sat-
isfy step 4 above. Both the protocol and the simula-
tion have the same probability distribution on conversations
(Ca, Cb, c, v, za, zb).
and (Ca, Cb, c(cid:7), v(cid:7), z(cid:7)
From two accepting conversations (Ca, Cb, c, v, za, zb)
b) we can recover a and sa via:
a, z(cid:7)
a = (v − v(cid:7))(c − c(cid:7))−1(mod q)
sa = (za − z(cid:7)
a)(c − c(cid:7))−1(mod q)
and
and then sb as
sb = (zb − z(cid:7)
b)(c − c(cid:7))−1 + asa(mod q)
ﬁnally by equating powers of γ in the last test in step 4, we
see that: ca2 + ax = cb + ax from which it follows that
b = a2.
If this protocol is implemented non-interactively, both
prover and veriﬁer will use a secure hash function to com-
pute c from A, B, Ca, Cb. Then the non-interactive proof
consists of Ca, Cb, v, za, zb, which is a total of seven mod p
or mod q integers.