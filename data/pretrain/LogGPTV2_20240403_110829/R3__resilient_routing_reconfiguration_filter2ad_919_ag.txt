1675
22.1
6
0.29
2.43
1.91
720
2131
21.8
Table 2: R3 ofﬂine precomputation time (seconds).
Network
Abilene
Level-3
SBC
UUNet
Generated
US-ISP
# ILM # NHLFE
FIB memory
RIB storage
28
72
70
336
460
-
71
304
257
2402
2116
-
<9 KB
<36 KB
<31 KB
<267 KB
<251 KB
<39 KB
<83 KB
<535 KB
<503 KB
<11 MB
<20 MB
<656 KB
Table 3: Router storage overhead of R3 implementation.
except the stub links which cannot be bypassed. We measure the
ILM table size, the NHLFE table size, the FIB size, and the RIB
size per router. Table 3 summarizes the results for 6 topologies.
We observe that all of these 6 network topologies can be protected
by R3 with modest FIBs (<267 KB) and RIBs (< 20 MB).
A related overhead is MPLS labels. Recall that the number of
MPLS labels used by MPLS-ff for protection routing is bounded
by the number of links in the network. Since many routers can
support at least tens of thousands of MPLS labels, the number of
MPLS labels used in protection routing is not an issue.
Effective resilient routing reconﬁguration: Next, we evaluate
the effectiveness of protection routing. We generate failure sce-
narios by disconnecting three links (Houston-Kansans, Chicago-
Indianapolis, Sunnyvale-Denver) sequentially on the emulated Abi-
lene topology (each link is two directed links). After failing one
link, we delay by about one-minute before failing the next link.
During the evaluation, bursty trafﬁc is generated to allow us to
measure the trafﬁc throughput between every OD pair, the trafﬁc
intensity on each link, and the aggregated loss rate at each egress
router (the trafﬁc matrix encodes the expected outgoing trafﬁc).
As shown in Figure 11, our R3 implementation successfully reroutes
trafﬁc without overloading any link. From Figure 11(b), we see that
despite three failed links, the bottleneck trafﬁc intensity is always
within 0.37. Figure 12 further plots the real-time RTT of a ﬂow
between Denver and Los Angeles during our test process. We can
clearly identify the three-step increases of RTT, due to the three link
failures. We observe that our R3 protection routing implementation
achieves smooth and efﬁcient routing protection.
)
d
n
o
c
e
s
i
l
i
m
(
T
T
R
 60
 55
 50
 45
 40
 35
 30
 25
 20
 15
 0
 50
 100
 150
 200
 250
 300
time (second)
Figure 12: RTT of a ﬂow (Denver-Los Angeles).
y
t
i
s
n
e
t
n
i
c 
i
f
f
a
r
t
d
e
z
i
l
a
m
r
o
N
 1.2
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
MPLS-ff+R3 for 3 link failures
OSPF+recon for 3 link failures
 5
 10
 15
 20
 25
D-Link index sorted by normalized traffic intensity
Figure 13: Sorted normalized trafﬁc intensity on each directed
link under three link failures: MPLS-ff+R3 vs OSPF+recon.
To appreciate the effectiveness of R3, we run the same failure
scenario using OSPF reconvergence protection. Figure 13 com-
pares the trafﬁc intensity by OSPF+recon vs MPLS-ff+R3. Using
OSPF, the trafﬁc intensity on the link between Washington and At-
lanta (link index 28) reaches as high as 1.07 (instantaneous rate).
Due to the congestion, we observe from the trace that the through-
put for the OD pair New York City to Indianapolis drop by up to
32.6% using OSPF+recon.
6. RELATED WORK
The existing work can be classiﬁed into two categories: (i) rout-
ing under failures and (ii) routing under variable trafﬁc.
Routing under failures: Many recent studies focus on minimiz-
ing the duration of disruption due to failures (e.g., [4, 20, 21, 23,
24, 26, 27, 29, 32]). These techniques precompute protection and
quickly reroute trafﬁc upon detecting failures (and before routing
convergence) [33]. However, they do not provide performance pre-
dictability or avoid congestion. As we have seen, they may lead to
serious congestion and thus violation of service level agreements.
Meanwhile there are also signiﬁcant studies on optimizing per-
formance under failures. In [14], the authors studied optimization
of OSPF/IS-IS weights under failures. However, it is a heuris-
301 
tics based approach and does not provide performance guarantee
or avoidance of congestion.
In MATE [9] and TeXCP [18], the
authors study how to react to instantaneous trafﬁc load and redis-
tribute trafﬁc on alternate links or paths. Many previous studies
achieve optimal performance by re-optimizing routing after each
failure (e.g., MPLS routing [39]). A major advantage of these ap-
proaches is that the new routing is computed speciﬁcally for the
new topology. Thus, the new routing can efﬁciently utilize the
remaining network resources and provide certain guarantees (e.g.,
how close the rerouting response compared with the optimal [2]).
A drawback of these approaches, however, is their slow response
time. Re-optimization from scratch for the new topology can be
computationally expensive. In addition, the new routing could be
very different from the existing one and thus take substantial delay
in installation and convergence. This can cause signiﬁcant service
disruption because of operation errors, forwarding loops and packet
loss during long convergence process. As a result, network oper-
ators are highly reluctant to completely change their routing. In-
stead, they prefer simple routing reconﬁguration. They completely
re-optimize only periodically or after a major change, instead of
after each topology failure. The only work that optimizes routing
simultaneously for different topologies is [2], but it requires enu-
meration of all possible topologies after failures and faces scalabil-
ity issues under multiple failures.
Routing under variable trafﬁc demand: High variability in In-
ternet trafﬁc has motivated researchers to design robust trafﬁc en-
gineering that works well under variable trafﬁc. One class of algo-
rithms [1, 9, 18, 31, 43] maintains a history of observed trafﬁc de-
mand matrices, and optimizes for the representative trafﬁc demand
matrices. Another class of algorithms is oblivious routing [2, 3,
22, 37, 46], which optimizes the worst-case performance over all
possible trafﬁc demands. More recently, Wang et al. [40] further
combined oblivious routing with prediction-based optimization to
provide good performance under typical demands while guaran-
teeing the worst-case performance. These works focus on trafﬁc
variability and do not consider topology variability.
7. CONCLUSIONS
In this paper, we propose Resilient Routing Reconﬁguration (R3)
to ﬁnd a single protection routing that can be effectively reconﬁg-
ured to provide congestion-free guarantee under multiple failures.
We introduce a novel compact representation of a large number of
failure scenarios, and compute a protection scheme that is resilient
to both link failures and trafﬁc variability. We further extend R3 to
handle realistic failure scenarios, prioritized trafﬁc, and the trade-
off between performance and resilience. We fully implement R3
on Linux using MPLS-ff, and demonstrate its effectiveness through
real experiments and extensive simulations using realistic network
topologies and trafﬁc.
Acknowledgments: The research is supported in part by NSF
Grants CNS-0546720, CNS-0546755, CNS-0626878, and CNS-
0627020. We are grateful to Jia Wang, Murali Kodialam, anony-
mous reviewers, C. Tian, and Dave Wang for valuable comments.
8. REFERENCES
[1] S. Agarwal, A. Nucci, and S. Bhattacharyya. Measuring the shared fate of IGP
engineering and interdomain trafﬁc. In Proc. ICNP, Nov. 2005.
[2] D. Applegate, L. Breslau, and E. Cohen. Coping with network failures: Routing
strategies for optimal demand oblivious restoration. In Proc. ACM
SIGMETRICS, June 2004.
[3] D. Applegate and E. Cohen. Making intra-domain routing robust to changing
and uncertain trafﬁc demands: Understanding fundamental tradeoffs. In Proc.
ACM SIGCOMM, Aug. 2003.
[4] A. Atlas and Z. Zinin. Basic speciﬁcation for IP Fast-Reroute: loop-free
alternates. (IETF Internet-Draft), draft-ietf-rtgwg-ipfrr-spec-base-10.txt, 2007.
[5] D. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, 1999.
[6] D. Bertsekas and R. Gallager. Data Networks. Prentice-Hall, 1992.
[7] CAIDA. http://www.caida.org/tools/.
[8] ILOG CPLEX: optimization software. http://www.ilog.com/products/cplex/.
[9] A. Elwalid, C. Jin, S. Low, and I. Widjaja. MATE: MPLS adaptive trafﬁc
engineering. In Proc. IEEE INFOCOM, Apr. 2001.
[10] A. Farrel, J.-P. Vasseur, and J. Ash. A Path Computation Element (PCE)-based
Architecture, RFC 4655, Aug. 2006.
[11] N. Feamster, H. Balakrishnan, J. Rexford, A. Shaikh, and K. van der Merwe.
The case for separating routing from routers. In Proc. ACM SIGCOMM FDNA
Workshop, Sept. 2004.
[12] B. Fortz, J. Rexford, and M. Thorup. Trafﬁc engineering with traditional IP
routing protocols. IEEE Communication Magazine, Oct. 2002.
[13] B. Fortz and M. Thorup. Internet trafﬁc engineering by optimizing OSPF
weights. In Proc. IEEE INFOCOM, Mar. 2000.
[14] B. Fortz and M. Thorup. Robust optimization of OSPF/IS-IS weights. In Proc.
INOC, Oct. 2003.
[15] P. Francois, C. Filsﬁls, J. Evans, and O. Bonaventure. Achieving sub-second
IGP convergence in large IP networks. ACM CCR, 35(3), 2005.
[16] G. Iannaccone, C. Chuah, S. Bhattacharyya, and C. Diot. Feasibility of IP
restoration in a tier-1 backbone. IEEE Network Magazine, 18(2):13–19, 2004.
[17] S. Iyer, S. Bhattacharyya, N. Taft, and C. Diot. An approach to alleviate link
overload as observed on an IP backbone. In Proc. IEEE INFOCOM, Apr. 2003.
[18] S. Kandula, D. Katabi, B. Davie, and A. Charny. Walking the tightrope:
Responsive yet stable trafﬁc engineering. In Proc. ACM SIGCOMM, Aug. 2005.
[19] S. Kandula, D. Katabi, S. Sinha, and A. Berger. Dynamic load balancing
without packet reordering. SIGCOMM CCR, 37(2), 2007.
[20] K. Kar, M. S. Kodialam, and T. V. Lakshman. Routing restorable bandwidth
guaranteed connections using maximum 2-route ﬂows. IEEE/ACM
Transactions on Networking, 11(5):772–781, 2003.
[21] M. Kodialam and T. V. Lakshman. Dynamic routing of locally restorable
bandwidth guaranteed tunnels using aggregated link usage information. In Proc.
IEEE INFOCOM, Apr. 2001.
[22] M. Kodialam, T. V. Lakshman, and S. Sengupta. Efﬁcient and robust routing of
highly variable trafﬁc. In Proc. HotNets-III, Nov. 2004.
[23] M. Kodialam, T. V. Lakshman, and S. Sengupta. A simple trafﬁc independent
scheme for enabling restoration oblivious routing of resilient connections. In
Proc. IEEE INFOCOM, Apr. 2004.
[24] M. S. Kodialam and T. V. Lakshman. Dynamic routing of restorable
bandwidth-guaranteed tunnels using aggregated network resource usage
information. IEEE/ACM Transactions on Networking, 11(3):399–410, 2003.
[25] R. R. Kompella, J. Yates, A. Greenberg, and A. C. Snoeren. IP fault localization
via risk modeling. In Proc. NSDI, 2005.
[26] K. Lakshminarayanan, M. Caesar, M. Rangan, T. Anderson, S. Shenker, and
I. Stoica. Achieving convergence-free routing using failure-carrying packets. In
Proc. ACM SIGCOMM, Aug. 2007.
[27] A. Li, P. Francois, and X. Yang. On improving the efﬁciency and manageability
of NotVia. In Proc. CoNEXT, Dec. 2007.
[28] A. Markopoulou, G. Iannaccone, S. Bhattacharyya, C. Chuah, and C. Diot.
Characterization of failures in an IP backbone network. In Proc. IEEE
INFOCOM, Apr. 2004.
[29] M. Motiwala, M. Elmore, N. Feamster, and S. Vempala. Path splicing. In Proc.
ACM SIGCOMM, 2008.
[30] M. Roughan. First order characterization of Internet trafﬁc matrices. In Proc.
55th Session of the International Statistics Institute, Apr. 2005.
[31] M. Roughan, M. Thorup, and Y. Zhang. Trafﬁc engineering with estimated
trafﬁc matrices. In Proc. IMC, Oct. 2003.
[32] M. Shand and S. Bryant. IP fast reroute framework. (IETF Internet-Draft),
draft-ietf-rtgwg-ipfrr-framework-06.txt, 2007.
[33] V. Sharma, B. M. Crane, S. Makam, K. Owens, C. Huang, F. Hellstrand,
J. Weil, L. Andersson, B. Jamoussi, B. Cain, S. Civanlar, and A. Chiu.
Framework for MPLS-Based Recovery. RFC 3469, Feb. 2003.
[34] N. So and H. Huang. Building a highly adaptive, resilient, and scalable MPLS
backbone. http://www.wandl.com/html/support/papers/
VerizonBusiness_WANDL_MPLS2007.pdf, 2007.
[35] N. Spring, R. Mahajan, and D. Wetherall. Rocketfuel: An ISP topology
mapping engine. Available from http://www.cs.washington.edu/
research/networking/rocketfuel/.
[36] Telemark. Telemark survey. http://www.telemarkservices.com/, 2006.
[37] L. G. Valiant. A scheme for fast parallel communication. SIAM Journal on
Computing, 11(7):350–361, 1982.
[38] J. P. Vasseur, M. Pickavet, and P. Demeester. Network Recovery: Protection and
Restoration of Optical, SONET-SDH, and MPLS. Morgan Kaufmann, 2004.
[39] H. Wang, H. Xie, L. Qiu, Y. R. Yang, Y. Zhang, and A. Greenberg. COPE:
Trafﬁc engineering in dynamic networks. In Proc. ACM SIGCOMM, 2006.
[40] H. Wang, Y. R. Yang, P. H. Liu, J. Wang, A. Gerber, and A. Greenberg.
Reliability as an interdomain service. In Proc. ACM SIGCOMM, Aug. 2007.
[41] B. White, J. Lepreau, L. Stoller, R. Ricci, S. Guruprasad, M. Newbold,
M. Hibler, C. Barb, and A. Joglekar. An integrated experimental environment
for distributed systems and networks. In Proc. OSDI, Dec. 2002.
[42] Wired News. The backhoe: A real cyberthreat, Jan. 2006.
http://www.wired.com/news/technology/1,70040-0.html.
[43] C. Zhang, Z. Ge, J. Kurose, Y. Liu, and D. Towsley. Optimal routing with
multiple trafﬁc matrices: Tradeoff between average case and worst case
performance. In Proc. ICNP, Nov. 2005.
[44] Y. Zhang and Z. Ge. Finding critical trafﬁc matrices. In Proc. DSN ’05, 2005.
[45] Y. Zhang, M. Roughan, C. Lund, and D. L. Donoho. An information-theoretic
approach to trafﬁc matrix estimation. In Proc. ACM SIGCOMM, Aug. 2003.
[46] R. Zhang-Shen and N. McKeown. Designing a predictable Internet backbone
network. In Proc. HotNets-III, Nov. 2004.
302