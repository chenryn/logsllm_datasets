### References

1. N. Chandran, D. Gupta, A. Rastogi, R. Sharma, and S. Tripathi. EzPC: Programmable, efficient, and scalable secure two-party computation. *Cryptology ePrint Archive*, Report 2017/1109, 2017. [Link](https://eprint.iacr.org/2017/1109).

2. M. Chase, R. Gilad-Bachrach, K. Laine, K. Lauter, and P. Rindal. Private collaborative neural network learning.

3. K. Chida, G. Morohashi, H. Fuji, F. Magata, A. Fujimura, K. Hamada, D. Ikarashi, and R. Yamamoto. Implementation and evaluation of an efficient secure computation system using "R" for healthcare statistics. *Journal of the American Medical Informatics Association*, 21(e2):e326–e331, 2014.

4. M. Chiesa, D. Demmler, M. Canini, M. Schapira, and T. Schneider. Towards securing internet exchange points against curious onlookers. In L. Eggert and C. Perkins (Eds.), *Proceedings of the 2016 Applied Networking Research Workshop (ANRW 2016)*, Berlin, Germany, July 16, 2016, pp. 32–34. ACM, 2016.

5. D. Demmler, T. Schneider, and M. Zohner. ABY - A framework for efficient mixed-protocol secure two-party computation.

6. W. Du and M. J. Atallah. Privacy-preserving cooperative scientific computations. In *csfw*, volume 1, page 273. Citeseer, 2001.

7. W. Du, Y. S. Han, and S. Chen. Privacy-preserving multivariate statistical analysis: Linear regression and classification. In *SDM*, volume 4, pages 222–233. SIAM, 2004.

8. M. K. Franklin, M. Gondree, and P. Mohassel. Multi-party indirect indexing and applications. Pages 283–297.

9. J. Furukawa, Y. Lindell, A. Nof, and O. Weinstein. High-throughput secure three-party computation for malicious adversaries and an honest majority. In J. Coron and J. B. Nielsen (Eds.), *Advances in Cryptology - EUROCRYPT 2017 - 36th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Paris, France, April 30 - May 4, 2017, Proceedings, Part II*, volume 10211 of *Lecture Notes in Computer Science*, pages 225–255, 2017.

10. A. Gascon, P. Schoppmann, B. Balle, M. Raykova, J. Doerner, S. Zahur, and D. Evans. Secure linear regression on vertically partitioned datasets.

11. I. Giacomelli, S. Jha, M. Joye, C. D. Page, and K. Yoon. Privacy-preserving ridge regression over distributed data from LHE. *Cryptology ePrint Archive*, Report 2017/979, 2017. [Link](https://eprint.iacr.org/2017/979).

12. R. Gilad-Bachrach, N. Dowlin, K. Laine, K. Lauter, M. Naehrig, and J. Wernsing. CryptoNets: Applying neural networks to encrypted data with high throughput and accuracy. In *International Conference on Machine Learning*, pages 201–210, 2016.

13. R. Gilad-Bachrach, K. Laine, K. Lauter, P. Rindal, and M. Rosulek. Secure data exchange: A marketplace in the cloud. *Cryptology ePrint Archive*, Report 2016/620, 2016. [Link](http://eprint.iacr.org/2016/620).

14. D. Harris. A taxonomy of parallel prefix networks. December 2003.

15. E. Hesamifard, H. Takabi, and M. Ghasemi. CryptoDL: Deep neural networks over encrypted data. *arXiv preprint arXiv:1711.05189*, 2017.

16. G. Jagannathan and R. N. Wright. Privacy-preserving distributed k-means clustering over arbitrarily partitioned data. In *Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining*, pages 593–599. ACM, 2005.

17. V. Kolesnikov and T. Schneider. Improved garbled circuit: Free XOR gates and applications. Pages 486–498.

18. R. Kumaresan, S. Raghuraman, and A. Sealfon. Network oblivious transfer. In M. Robshaw and J. Katz (Eds.), *Advances in Cryptology - CRYPTO 2016 - 36th Annual International Cryptology Conference, Santa Barbara, CA, USA, August 14-18, 2016, Proceedings, Part II*, volume 9815 of *Lecture Notes in Computer Science*, pages 366–396. Springer, 2016.

19. J. Launchbury, D. Archer, T. DuBuisson, and E. Mertens. Application-scale secure multiparty computation. In *European Symposium on Programming Languages and Systems*, pages 8–26. Springer, 2014.

20. Y. Lindell and B. Pinkas. Privacy preserving data mining. In *Annual International Cryptology Conference*, pages 36–54. Springer, 2000.

21. J. Liu, M. Juuti, Y. Lu, and N. Asokan. Oblivious neural network predictions via MiniONN transformations. In B. M. Thuraisingham, D. Evans, T. Malkin, and D. Xu (Eds.), *Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS 2017)*, Dallas, TX, USA, October 30 - November 03, 2017, pages 619–631. ACM, 2017.

22. H. B. McMahan, D. Ramage, K. Talwar, and L. Zhang. Learning differentially private language models without losing accuracy. *arXiv preprint arXiv:1710.06963*, 2017.

23. P. Mohassel, M. Rosulek, and Y. Zhang. Fast and secure three-party computation: The garbled circuit approach. Pages 591–602.

24. P. Mohassel and Y. Zhang. SecureML: A system for scalable privacy-preserving machine learning. In *2017 IEEE Symposium on Security and Privacy (SP 2017)*, San Jose, CA, USA, May 22-26, 2017, pages 19–38. IEEE Computer Society, 2017.

25. M. Naor, B. Pinkas, and R. Sumner. Privacy preserving auctions and mechanism design. In *EC*, pages 129–139, 1999.

26. V. Nikolaenko, U. Weinsberg, S. Ioannidis, M. Joye, D. Boneh, and N. Taft. Privacy-preserving ridge regression on hundreds of millions of records. In *Security and Privacy (SP), 2013 IEEE Symposium on*, pages 334–348. IEEE, 2013.

27. M. S. Riazi, C. Weinert, O. Tkachenko, E. M. Songhori, T. Schneider, and F. Koushanfar. Chameleon: A hybrid secure computation framework for machine learning applications.

28. P. Rindal. libOTe: An efficient, portable, and easy-to-use Oblivious Transfer Library. [Link](https://github.com/osu-crypto/libOTe).

29. B. D. Rouhani, M. S. Riazi, and F. Koushanfar. DeepSecure: Scalable provably-secure deep learning. *arXiv preprint arXiv:1705.08963*, 2017.

30. A. P. Sanil, A. F. Karr, X. Lin, and J. P. Reiter. Privacy preserving regression modelling via distributed computation. In *Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining*, pages 677–682. ACM, 2004.

31. R. Shokri and V. Shmatikov. Privacy-preserving deep learning. In *Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security*, pages 1310–1321. ACM, 2015.

32. R. Shokri, M. Stronati, C. Song, and V. Shmatikov. Membership inference attacks against machine learning models. In *Security and Privacy (SP), 2017 IEEE Symposium on*, pages 3–18. IEEE, 2017.

33. A. B. Slavkovic, Y. Nardi, and M. M. Tibbits. "Secure" logistic regression of horizontally and vertically partitioned distributed databases. In *Seventh IEEE International Conference on Data Mining Workshops (ICDMW 2007)*, pages 723–728. IEEE, 2007.

34. C. Song, T. Ristenpart, and V. Shmatikov. Machine learning models that remember too much. In *Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security*, pages 587–601. ACM, 2017.

35. E. M. Songhori, S. U. Hussain, A. Sadeghi, T. Schneider, and F. Koushanfar. TinyGarble: Highly compressed and scalable sequential garbled circuits. In *2015 IEEE Symposium on Security and Privacy*, pages 411–428, May 2015.

36. F. Tramèr, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart. Stealing machine learning models via prediction APIs. In *USENIX Security Symposium*, pages 601–618, 2016.

37. J. Vaidya, H. Yu, and X. Jiang. Privacy-preserving SVM classification. *Knowledge and Information Systems*, 14(2):161–178, 2008.

38. S. Wu, T. Teruya, J. Kawamoto, J. Sakuma, and H. Kikuchi. Privacy-preservation for stochastic gradient descent application to secure logistic regression. *The 27th Annual Conference of the Japanese Society for Artificial Intelligence*, 27:1–4, 2013.

39. H. Yu, J. Vaidya, and X. Jiang. Privacy-preserving SVM classification on vertically partitioned data. In *Pacific-Asia Conference on Knowledge Discovery and Data Mining*, pages 647–656. Springer, 2006.

40. S. Zahur, M. Rosulek, and D. Evans. Two halves make a whole - reducing data transfer in garbled circuits using half gates. Pages 220–250.

### Appendices

#### A. Machine Learning

Given the building blocks in Section 5, we design efficient protocols for training linear regression, logistic regression, and neural network models on private data using the gradient descent method. We will discuss each model in detail next.

#### A.1. Linear Regression

Our starting point is the linear regression model using the stochastic gradient descent method. Given \( n \) training examples \( x_1, \ldots, x_n \in \mathbb{R}^D \) and the corresponding output variable \( y_1, \ldots, y_n \), our goal is to find a vector \( w \in \mathbb{R}^D \) which minimizes the distance between \( f(x_i) := \sum_{j=1}^D x_{ij} w_j \) and the true output \( y_i \). There are many ways to define the distance between \( f(x_i) \) and \( y_i \). In general, a cost function \( C\{(x_i, y_i)\}(w) \) is defined and subsequently minimized in an optimization problem. We will focus on the commonly used L2 cost function, \( C\{(x_i, y_i)\}(w) := \frac{1}{2} (x_i \cdot w - y_i)^2 \). This is the squared difference between the predicted output \( f(x_i) \) and the true output \( y_i \).

One reason this cost function is often used is due to its resulting straightforward minimization problem. If we assume that there is a linear relation between \( x_i \) and \( y_i \), the cost function \( C \) is convex, which implies that the gradient descent algorithm will converge at the global minimum. The algorithm begins by initializing \( w \in \mathbb{R}^D \) with arbitrary values. At each step, the gradient at \( C(w) \) is computed. Since the L2 cost function is used, the gradient will point in the direction which (locally) minimizes the square between the prediction and the true value. \( w \) is then updated to move down the gradient, which decreases the value of \( C(w) \). That is, at each iteration of the algorithm, \( w \) is updated as:
\[ w_j := w_j - \alpha \frac{\partial C(w)}{\partial w_j} = w_j - \alpha \frac{1}{n} \sum_{i=1}^n (x_i \cdot w - y_i) x_{ij} \]

The extra term \( \alpha \) is known as the learning rate, which controls how large of a step toward the minimum the algorithm should take at each iteration.

**Batching.** One common optimization to improve performance is known as batching. The overall dataset of \( n \) examples is randomly divided into batches of size \( B \), denoted by \( X_1, \ldots, X_{n/B} \) and \( Y_1, \ldots, Y_{n/B} \). The update procedure for the \( j \)-th batch is then defined as:
\[ w := w - \alpha \frac{1}{B} X_j^T (X_j \cdot w - Y_j) \]
(1)

Typically, once all the batches have been used once, the examples are randomly placed into new batches. Each set of batches is referred to as an epoch. This optimization was traditionally used to improve the rate of convergence by allowing each update to be averaged over several examples. Without this averaging process, the gradient of a single example is often quite noisy and can point in the wrong direction, degrading the rate at which the model converges to the optimal. In our machine learning protocols, this optimization serves another purpose. It allows a larger amount of vectorization to be applied in the "back propagation" step. In particular, when the outer matrix multiplication of Equation 1 is computed, the vectorization technique allows the amount of communication to be independent of the batch size \( B \).

**Learning Rate.** Choosing the correct learning rate \( \alpha \) can be challenging. When \( \alpha \) is too large, the algorithm may repeatedly overstep the minimum and possibly diverge. However, if \( \alpha \) is too small, the algorithm will take an unnecessarily large number of iterations to converge to the minimum. One solution is to set \( \alpha \) dynamically based on various metrics for convergence, e.g., the technique of Barzilai and Borwein [10]. A second option is to compute the cost function periodically on a subset of the training data. The value of \( \alpha \) can then be dynamically tuned based on the rate at which the cost function is decreasing. For example, additively increase \( \alpha \) by a small amount until the cost function increases, at which point decrease it by some multiplicative factor. We leave the investigation of trade-offs between these methods to future work.

**Termination.** Another problem is to determine when the algorithm should terminate. Sometimes an upper bound on the number of iterations is known. Alternatively, the cost function can also be used to determine the termination condition. When the cost function fails to decrease by a significant amount for several iterations, the algorithm can be concluded that the minimum has been reached.

**Secure Linear Regression.** Implementing this algorithm in the secure framework of Section 5 is a relatively easy task. First, the parties jointly input the training examples \( X \in \mathbb{R}^{n \times D} \) and \( Y \in \mathbb{R}^n \). We place no restrictions on how the data is distributed among the parties. For simplicity, the initial weight vector \( w \) is initialized as the zero vector. The learning rate \( \alpha \) can be set as above.

In the secret shared setting, the correct batch size \( B \) has several considerations. First, it should be large enough to ensure good quality gradients at each iteration. On the other hand, when \( B \) increases beyond a certain point, the quality of the gradient stops improving, which results in wasted work and decreased performance. This trade-off has a direct consequence in the secret shared setting. The communication required to compute the inner matrix multiplication of Equation 1 at each iteration is proportional to \( B \). Therefore, decreasing \( B \) results in a smaller bandwidth requirement. However, two rounds of interaction are required for each iteration of the algorithm, regardless of \( B \). Therefore, we propose to set \( B \) proportional to the bandwidth available in the time required for one round trip.

| Setting | Dimension | Protocol | LAN | WAN |
|---------|-----------|----------|-----|-----|
| 10      | 100       | 1000     | This | [43] |
| 100     | 1000      | 10000    | This | [43] |
| 1000    | 10000     | 100000   | This | [43] |

| Batch Size \( B \) | 128 | 256 | 512 |
|--------------------|-----|-----|-----|
| This               | 2251 | 188 | 1867 |
| [43]               | 183 | 349 | 105 |
| This               | 4.12 | 3.10 | 4.11 |
| [43]               | 3.08 | 4.04 | 3.01 |
| Online             | 2053 | 101 | 1375 |
| [43]               | 93 | 184 | 51 |
| This               | 4.10 | 2.28 | 4.09 |
| [43]               | 2.25 | 3.95 | 2.15 |
| This               | 1666 | 41 | 798 |
| [43]               | 46 | 95 | 24 |
| This               | 4.06 | 1.58 | 4.03 |
| [43]               | 1.57 | 3.78 | 1.47 |