# 1 前言
看了兜哥在freebuf上的专栏文章《学点算法搞安全之HMM（上篇）》，大意就是将URL参数进行范化，然后使用hmm算法来进行训练和测试，这里检测的重点是xss，但是带着我自己的疑问认真看了下方的评论，里面提到一个我非常认同的问题  
这里原先是对相同url的参数进行数据提取和训练，那么我们知道一个网站，可能会有上千上万的页面，对应上千上万的url，那么按照这样的思路可能就真的需要去建立上千上万的模型，这显然是不现实的。  
那么我们能否将模型范化，去建立一个模型检测一个业务网站的所有的url以及所有url中的异常参数？带着这样的疑问继续找文章，翻到了先知的《Web日志安全分析浅谈》，其中检测的原理就比较硬核了，通过编写不同的攻击规则来表示不同类型的攻击类型，但是这样会出现一个问题，那就是在真实环境中，你并不知道攻击payload到底长什么样，因此也就可能会造成0day的直接放行和变种payload的绕过。在文末jeary也提出了自己的思考，这也是本文的出发点。当然在文中jeary并没有给出具体方法，因此笔者凭着自己对日志分析的理解开始尝试实现这样一套基于访问日志的异常访问检测。  
这里我第一个想到的思想就是聚类算法，正常的请求总是相似的，异常请求却各有千秋，那么如果我们能够通过无监督聚类算法来将正常请求给聚类到一块，那么异常请求就会自己凸显出来，打上异常的标签。理论上可行，下面开始实践。
# 2 数据清洗
这里的数据来源很简单，我从自己的vps上把博客的访问日志给拖下来了，大概是800M，数据量在480万条左右，既然想做的是通用的业务模型检测，那么这里拿博客日志或者电商日志数据，从理论上来说都没有太大的差别，这是因为虽然业务模型不一样，但是每一个业务模型都有一套自己的访问序列，也就是说基于博客日志的聚类可能是这样的分布，但是基于电商日志的聚类可能是那样的分布，本质上来说他们并没有区别，聚类只是为了凸显异常请求，所以对数据集来源上，思路上并没有觉得有什么问题。  
先来看下博客的日志数据  
这里用的国外某家的cdn，ip好像都是美国ip，但是这里是针对url参数进行检测，也没想着做溯源，所以这里ip暂不考虑，重点是url参数，这里一开始心比较大，在检测的模型中加入了访问请求方式（GET/POST）和访问状态码（200/302/404等），后来发现其实这两项其实没有什么必要，这是因为如果是异常请求，比如sql注入、xss等攻击，访问请求方式和状态码并不会改变其异常的本质，也就是说无论是GET还是POST，还是说200状态或者404状态，这个请求是实际存在的异常访问，所以我们只需要将关注的重点放在url请求即可，其中包含url的path和url的param。  
这里主要是对pandas的第六列数据进行url的解码和解析，获取其中的path和params  
这里主要是对一些特殊请求，比如“/”或者一些静态页面做处理，对于静态页面一般来说都是无害的，所以说这里将静态页面归为一类，感觉上也没有什么毛病。。最后进入到数据泛化的函数  
这里有几个泛化规则1）对中文字符统一将其替换为cn；2）统一将数字替换为5；3）统一将特殊字符替换为_。对于中文字符，造成的差异性可能会比较大，比如说不同的中文字符在构建下面的词袋时可能就会造成很大的偏差，所以这里泛化中文字符，第二个就是泛化数字，这里在日常的业务模型上其实是比较常见的，比如说index.php?id=1、index.php?id=123等这样的访问请求，他们其实都是一类的，所以这里最后泛化其实就成了index.php?id=5，在聚类时就会自动聚为一类，特殊字符替换主要是方便统一分词，但是其实是不会影响结果的，虽然部分字符被替换，但是整体字符的顺序仍然是聚类的有效依据。  
最终清洗下来效果如下，下面开始使用词袋模型将其转化为数组模型。
# 3 词袋模型
词袋模型兜哥在hmm里曾经使用nltk来构建日志词典，这个模型相对好理解一点，就是说现在比如说出现“alice”、"bob"，那么根据词典的原理，alice可能就是1，bob就是2，那么最终每一组词都能转化为其在词典中的位置。但是我这里使用的tfidf，这是一个根据字词在文中出现的次数和整个语料中出现的文档频率来计算一个字词在整个语料的重要程度。  
这里笔者认为用什么算法其实不是很重要，重点在于怎么将泛化的日志数据转化为能够唯一标记的数据向量。那么经过这一步其实所有的url路径和参数就已经能够转化为数组模型。  
# 4 无监督聚类
这里无监督聚类用的比较多的一个是kmeans，一个是dbscan。kmeans不太适用于当前场景的原因是kmeans你需要指定簇数，也就是你需要提前知道当面业务模型的url分布数，比如说新闻页面、评论页面、产品页面，那么可能会对应三个簇数，但是在实际环境中，没人能够说得清到底有几个业务功能，所以说这里直接放弃了kmeans，采用dbscan。  