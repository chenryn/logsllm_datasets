healthcare services at home for the elderly. [56] showed that
using the notion of context and incorporating usual behav-
ior of services leads to improved detection accuracy over
traditional detection mechanisms for critical service oriented
architectures. [57] provided a framework for context-based
detection of network intrusions by incorporating protocol
context and byte sequences. Our work shares similarities with
the aforementioned by incorporating context for improving
anomaly detection, but it relies on deep learning for real-time
context-inference and anomaly detection in robotic surgery.
VIII. THREATS TO VALIDITY
Our solution relies on the accuracy and generalizability
of DNNs for detecting the operational context followed by
the context-speciﬁc errors. While DNNs have been widely
successful across many domains, slight perturbations in the
input data brought about by the noise in the environment
[58] or attacks [59] can lead them to misclassify with high
conﬁdence. However, most of the proposed adversarial ex-
amples on DNNs target image-based classiﬁcation systems.
Our safety monitoring system is based on kinematics samples
and we only use computer vision for orthogonal labeling
of failures. A further robustness analysis and design of our
ML-based safety monitor against accidental and malicious
perturbations is the subject of future work.
In addition, the performance of supervised learning models
heavily depends on the accurate labeling of the operational
context, or surgical gestures, and the context-speciﬁc anoma-
lies, or erroneous gestures. Our labeling of the erroneous
gestures for the JIGSAWS dataset was based on the human
annotations of the corresponding videos. We labeled any
gesture that had an occurrence of an anomaly as erroneous
even if the error did not occur at the beginning of the gesture.
Future work will focus on automated labeling of trajectory
data from real surgical
tasks (similar to our automated
labeling of Block Transfer task on RAVEN II robot using
video data) for more precise localization of errors.
IX. CONCLUSION
We presented an end-to-end safety monitoring system for
real-time context-aware identiﬁcation of erroneous gestures
in robotic surgery. Our preliminary results show the promise
of our kinematics-only based solution in timely and accurate
detection of unsafe events, even when the vision data might
not be available or be sub-optimal. Our experimental results
validate the need for context-aware monitoring, while also
suggesting that some surgical gestures have similar error-
patterns and can potentially be better monitored together as
a sequence. Our results also show the potential for early
detection and prevention of these unsafe events, which could
be further enhanced by having access to larger training
datasets and extending the semantics of context using vision
or other sensing modalities. Future work will focus on the
generalization of our solution to a wider set of realistic
surgical gestures and tasks with a larger number of trials. We
also plan to further improve the accuracy and timeliness of
our safety monitoring system to enable successful prevention
of safety-critical events during surgery.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:23:52 UTC from IEEE Xplore.  Restrictions apply. 
395
REFERENCES
[1] Intuitive
Surgical,
Report,”
//www.annualreports.com/HostedData/AnnualReportArchive/i/
NASDAQ ISRG 2017.pdf.
Annual
“2017
Inc.,
http:
[2] G.-Z. Yang, J. Cambias, K. Cleary, E. Daimler, J. Drake, P. E.
Dupont, N. Hata, P. Kazanzides, S. Martel, R. V. Patel et al., “Medical
robotics—regulatory, ethical, and legal considerations for increasing
levels of autonomy,” Sci. Robot, vol. 2, no. 4, p. 8638, 2017.
[3] H. Alemzadeh, J. Raman, N. Leveson, Z. Kalbarczyk, and R. K. Iyer,
“Adverse events in robotic surgery: a retrospective study of 14 years
of fda data,” PloS one, vol. 11, no. 4, p. e0151470, 2016.
[4] E. Rajih, C. Tholomier, B. Cormier, V. Samou¨elian, T. Warkus,
M. Liberman, H. Widmer, J.-B. Lattouf, A. M. Alenizi, M. Meskawi
et al., “Error reporting from the da vinci surgical system in robotic
surgery: A canadian multispecialty experience at a single academic
centre,” Canadian Urological Association Journal, vol. 11, no. 5, p.
E197, 2017.
[5] T. Bonaci, J. Herron, T. Yusuf, J. Yan, T. Kohno, and H. J. Chizeck, “To
make a robot secure: An experimental analysis of cyber security threats
against teleoperated surgical robots,” arXiv preprint arXiv:1504.04339,
2015.
[6] H. Alemzadeh, D. Chen, X. Li, T. Kesavadas, Z. T. Kalbarczyk, and
R. K. Iyer, “Targeted attacks on teleoperated surgical robots: dynamic
model-based detection and mitigation,” in Dependable Systems and
Networks (DSN), 2016 46th Annual IEEE/IFIP International Confer-
ence on.
IEEE, 2016, pp. 395–406.
[7] H. Alemzadeh, D. Chen, A. Lewis, Z. Kalbarczyk, J. Raman, N. Leve-
son, and R. Iyer, “Systems-theoretic safety assessment of robotic
telesurgical systems,” in International conference on computer safety,
reliability, and security. Springer, 2014, pp. 213–227.
[8] T. R. Eubanks, R. H. Clements, D. Pohl, N. Williams, D. C. Schaad,
S. Horgan, and C. Pellegrini, “An objective scoring system for laparo-
scopic cholecystectomy,” Journal of the American College of Surgeons,
vol. 189, no. 6, pp. 566–574, 1999.
[9] O. Elhage, B. Challacombe, A. Shortland, and P. Dasgupta, “An
tasks on
assessment of the physical
surgeon errors and discomfort: a comparison between robot-assisted,
laparoscopic and open approaches,” BJU international, vol. 115, no. 2,
pp. 274–281, 2015.
impact of complex surgical
[10] P. Joice, G. Hanna, and A. Cuschieri, “Errors enacted during endo-
scopic surgery - a human reliability analysis,” Applied ergonomics,
vol. 29, no. 6, pp. 409–414, 1998.
[11] N. Leveson, Engineering a safer world: Systems thinking applied to
safety. MIT press, 2011.
[12] B. Hannaford, J. Rosen, D. W. Friedman, H. King, P. Roan, L. Cheng,
D. Glozman, J. Ma, S. N. Kosari, and L. White, “Raven-ii: an
open platform for surgical robotics research,” IEEE Transactions on
Biomedical Engineering, vol. 60, no. 4, pp. 954–959, 2012.
[13] R. P. Goldberg, M. Hanuschik, H. Hazebrouck, P. Millman, D. Kapoor,
J. Zabinski, D. Robinson, D. Weir, and S. J. Brogna, “Ergonomic
surgeon control console in robotic surgical systems,” Feb. 21 2012,
uS Patent 8,120,301.
[14] M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote, J. Leibs,
R. Wheeler, and A. Y. Ng, “Ros: an open-source robot operating
system,” in ICRA workshop on open source software, vol. 3, no. 3.2.
Kobe, Japan, 2009, p. 5.
[15] N. Koenig and A. Howard, “Design and use paradigms for gazebo, an
open-source multi-robot simulator,” in 2004 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS)(IEEE Cat. No.
04CH37566), vol. 3.
IEEE, 2004, pp. 2149–2154.
[16] P. Kazanzides, Z. Chen, A. Deguet, G. S. Fischer, R. H. Taylor,
and S. P. DiMaio, “An open-source research kit for the da vinci®
surgical system,” in 2014 IEEE international conference on robotics
and automation (ICRA).
IEEE, 2014, pp. 6434–6439.
[17] E. M. Bonrath, N. J. Dedy, B. Zevin, and T. P. Grantcharov, “Deﬁning
technical errors in laparoscopic surgery: a systematic review,” Surgical
endoscopy, vol. 27, no. 8, pp. 2678–2691, 2013.
[18] E. Bonrath, B. Zevin, N. Dedy, and T. Grantcharov, “Error rating tool
to identify and analyse technical errors and events in laparoscopic
surgery,” British Journal of Surgery, vol. 100, no. 8, pp. 1080–1088,
2013.
[19] M. S. Yasar, D. Evans, and H. Alemzadeh, “Context-aware monitoring
in robotic surgery,” in 2019 International Symposium on Medical
Robotics (ISMR).
IEEE, 2019, pp. 1–7.
[20] D. Neumuth, F. Loebe, H. Herre, and T. Neumuth, “Modeling surgical
processes: A four-level translational approach,” Artiﬁcial intelligence
in medicine, vol. 51, no. 3, pp. 147–161, 2011.
[21] J. Rosen, J. D. Brown, L. Chang, M. N. Sinanan, and B. Hannaford,
“Generalized approach for modeling minimally invasive surgery as a
stochastic process using a discrete markov model,” IEEE Transactions
on Biomedical engineering, vol. 53, no. 3, pp. 399–413, 2006.
[22] J. H. Peters, G. M. Fried, L. L. Swanstrom, N. J. Soper, L. F. Sillin,
B. Schirmer, K. Hoffman, S. F. Committee et al., “Development and
validation of a comprehensive program of education and assessment
of the basic fundamentals of laparoscopic surgery,” Surgery, vol. 135,
no. 1, pp. 21–27, 2004.
[23] Y. Gao, S. S. Vedula, C. E. Reiley, N. Ahmidi, B. Varadarajan, H. C.
Lin, L. Tao, L. Zappella, B. B´ejar, D. D. Yuh et al., “Jhu-isi gesture and
skill assessment working set (JIGSAWS): A surgical activity dataset
for human motion modeling,” in MICCAI Workshop: M2CAI, vol. 3,
2014, p. 3.
[24] C. E. Reiley and G. D. Hager, “Decomposition of robotic surgical
tasks: an analysis of subtasks and their correlation to skill,” in M2CAI
workshop. MICCAI, London, 2009.
[25] A. Zia, A. Hung, I. Essa, and A. Jarc, “Surgical activity recognition
in robot-assisted radical prostatectomy using deep learning,” in In-
ternational Conference on Medical Image Computing and Computer-
Assisted Intervention. Springer, 2018, pp. 273–280.
[26] A. J. Hung, J. Chen, Z. Che, T. Nilanon, A. Jarc, M. Titus, P. J.
Oh, I. S. Gill, and Y. Liu, “Utilizing machine learning and automated
performance metrics to evaluate robot-assisted radical prostatectomy
performance and predict outcomes,” Journal of endourology, vol. 32,
no. 5, pp. 438–444, 2018.
[27] A. Zia, L. Guo, L. Zhou, I. Essa, and A. Jarc, “Novel evaluation
of surgical activity recognition models using task-based efﬁciency
metrics,” International journal of computer assisted radiology and
surgery, pp. 1–9, 2019.
[28] N. Ahmidi, P. Poddar, J. D. Jones, S. S. Vedula, L. Ishii, G. D.
Hager, and M. Ishii, “Automated objective surgical skill assessment
in the operating room from unstructured tool motion in septoplasty,”
International journal of computer assisted radiology and surgery,
vol. 10, no. 6, pp. 981–991, 2015.
[29] M. J. Fard, S. Ameri, R. Darin Ellis, R. B. Chinnam, A. K. Pandya,
and M. D. Klein, “Automated robot-assisted surgical skill evaluation:
Predictive analytics approach,” The International Journal of Medical
Robotics and Computer Assisted Surgery, vol. 14, no. 1, p. e1850,
2018.
[30] H. I. Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P.-A. Muller,
“Evaluating surgical skills from kinematic data using convolutional
neural networks,” in International Conference on Medical Image
Computing and Computer-Assisted Intervention. Springer, 2018, pp.
214–221.
[31] K. Moorthy, Y. Munz, A. Dosis, F. Bello, A. Chang, and A. Darzi, “Bi-
modal assessment of laparoscopic suturing skills,” Surgical Endoscopy
And Other Interventional Techniques, vol. 18, no. 11, pp. 1608–1612,
2004.
[32] M. R. Kwaan, D. M. Studdert, M. J. Zinner, and A. A. Gawande,
“Incidence, patterns, and prevention of wrong-site surgery,” Archives
of surgery, vol. 141, no. 4, pp. 353–358, 2006.
[33] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol.
521, no. 7553, p. 436, 2015.
[34] S. Krishnan, A. Garg, S. Patil, C. Lea, G. Hager, P. Abbeel, and
K. Goldberg, “Transition state clustering: Unsupervised surgical trajec-
tory segmentation for robot learning,” in Robotics Research. Springer,
2018, pp. 91–110.
[35] J. Lin, “Divergence measures based on the shannon entropy,” IEEE
Transactions on Information theory, vol. 37, no. 1, pp. 145–151, 1991.
[36] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
computation, vol. 9, no. 8, pp. 1735–1780, 1997.
[37] M. Hermans and B. Schrauwen, “Training and analysing deep recur-
rent neural networks,” in Advances in neural information processing
systems, 2013, pp. 190–198.
[38] V. Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted
boltzmann machines,” in Proceedings of the 27th international con-
ference on machine learning (ICML-10), 2010, pp. 807–814.
[39] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-
tion,” arXiv preprint arXiv:1412.6980, 2014.
[40] F. Chollet et al., “Keras,” 2015.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:23:52 UTC from IEEE Xplore.  Restrictions apply. 
396
[41] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
S. Ghemawat, G. Irving, M. Isard et al., “Tensorﬂow: A system
for large-scale machine learning,” in 12th {USENIX} Symposium on
Operating Systems Design and Implementation ({OSDI} 16), 2016,
pp. 265–283.
[42] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg et al.,
“Scikit-learn: Machine learning in python,” Journal of machine learn-
ing research, vol. 12, no. Oct, pp. 2825–2830, 2011.
[43] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image
quality assessment: from error visibility to structural similarity,” IEEE
transactions on image processing, vol. 13, no. 4, pp. 600–612, 2004.
[44] C. Lea, G. D. Hager, and R. Vidal, “An improved model for seg-
mentation and recognition of ﬁne-grained activities with application
to surgical training tasks,” in 2015 IEEE winter conference on appli-
cations of computer vision.
IEEE, 2015, pp. 1123–1129.
[45] S. Sefati, N. J. Cowan, and R. Vidal, “Learning shared, discriminative
dictionaries for surgical gesture segmentation and classiﬁcation,” in
MICCAI Workshop: M2CAI, vol. 4, 2015.
[46] M. Y. Jung, R. H. Taylor, and P. Kazanzides, “Safety design view: A
conceptual framework for systematic understanding of safety features
of medical robot systems,” in 2014 IEEE International Conference on
Robotics and Automation (ICRA).
IEEE, 2014, pp. 1883–1888.
[47] C. He, N. Patel, I. Iordachita, and M. Kobilarov, “Enabling technology
for safe robot-assisted retinal surgery: Early warning for unsafe scleral
force,” in 2019 International Conference on Robotics and Automation
(ICRA).
IEEE, 2019, pp. 3889–3894.
[48] K. Coble, W. Wang, B. Chu, and Z. Li, “Secure software attesta-
tion for military telesurgical robot systems,” in 2010-MILCOM 2010
MILITARY COMMUNICATIONS CONFERENCE.
IEEE, 2010, pp.
965–970.
[49] M. E. Tozal, Y. Wang, E. Al-Shaer, K. Sarac, B. Thuraisingham,
and B.-T. Chu, “On secure and resilient telesurgery communications
over unreliable networks,” in 2011 IEEE Conference on Computer
Communications Workshops (INFOCOM WKSHPS).
IEEE, 2011,
pp. 714–719.
[50] G. S. Lee and B. Thuraisingham, “Cyberphysical systems security
applied to telesurgical robotics,” Computer Standards & Interfaces,
vol. 34, no. 1, pp. 225–229, 2012.
[51] H. C. Lin, I. Shafran, D. Yuh, and G. D. Hager, “Towards automatic
skill evaluation: Detection and segmentation of robot-assisted surgical
motions,” Computer Aided Surgery, vol. 11, no. 5, pp. 220–230, 2006.
[52] R. DiPietro, C. Lea, A. Malpani, N. Ahmidi, S. S. Vedula, G. I. Lee,
M. R. Lee, and G. D. Hager, “Recognizing surgical activities with
recurrent neural networks,” in International Conference on Medical
Image Computing and Computer-Assisted Intervention.
Springer,
2016, pp. 551–558.
[53] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethink-
ing the inception architecture for computer vision,” in Proceedings
of the IEEE conference on computer vision and pattern recognition,
2016, pp. 2818–2826.
[54] D. Kati´c, A.-L. Wekerle, J. G¨ortler, P. Spengler, S. Bodenstedt,
S. R¨ohl, S. Suwelack, H. G. Kenngott, M. Wagner, B. P. M¨uller-Stich
et al., “Context-aware augmented reality in laparoscopic surgery,”
Computerized Medical Imaging and Graphics, vol. 37, no. 2, pp. 174–
182, 2013.
[55] B. Yuan and J. Herbert, “Context-aware hybrid reasoning framework
for pervasive healthcare,” Personal and ubiquitous computing, vol. 18,
no. 4, pp. 865–881, 2014.
[56] T. Zoppi, A. Ceccarelli, and A. Bondavalli, “Context-awareness to
improve anomaly detection in dynamic service oriented architectures,”
in International Conference on Computer Safety, Reliability, and
Security. Springer, 2016, pp. 145–158.
[57] P. Duessel, C. Gehl, U. Flegel, S. Dietrich, and M. Meier, “De-
tecting zero-day attacks using context-aware anomaly detection at
the application-layer,” International Journal of Information Security,
vol. 16, no. 5, pp. 475–490, 2017.
[58] S. Zheng, Y. Song, T. Leung, and I. Goodfellow, “Improving the ro-
bustness of deep neural networks via stability training,” in Proceedings
of the ieee conference on computer vision and pattern recognition,
2016, pp. 4480–4488.
[59] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harness-
ing adversarial examples,” arXiv preprint arXiv:1412.6572, 2014.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:23:52 UTC from IEEE Xplore.  Restrictions apply. 
397