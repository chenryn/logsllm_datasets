forward the dequantization before comparison-based non-
linear layers (e.g., ReLU and maxpool) to be after those non-
linear layers, resulting in fusing dequantization with quantiza-
tion (as Figure 1 illustrates). We prove (in Section 3.7.2) that
the resulting computation is equivalent. Such fusion allows
us to handle the values throughout all layers in a ﬁxed-point,
low-bit-width representation. Thus, it reduces the number of
(now fused) cryptographic operations and the complexity for
each of them while avoiding overﬂow or underﬂow.
The fused (de)quantization may become scaling up or down
depending on the dataset and DNN architecture. In our experi-
ment (mainly over VGG DNN [22]), it is always scaling down.
As we always scale down by a power of 2 as in bit-truncation,
we call it stochastic rounding and truncation layer.
Rounding Efﬁciently while Avoiding Truncation Error.
Truncating the least-signiﬁcant bits of additive SS (used in
prior works for scaling down, e.g., [18]) may incur errors trash-
ing the values when wrap-around occurs (see Section 3.7.3).
More speciﬁcally, reducing 1 bit of the plaintext space doubles
the error probability. In the low-precision setting, such errors
are very likely. To balance off such value-trashing error (if it
exists), we introduce a GPU-friendly wrap-around handling
protocol. However, even after ﬁxing this error, an off-by-one
error in truncation may still happen. We observe that the error
distribution due to off-by-one error is very close to that of
stochastic rounding (which we prove in Section 3.7.3). Our
truncation protocol then exploits that for the effect of stochas-
tic rounding, the rounding method speciﬁed by SWALP, on
the scaled-down results, killing two birds with one stone.
Putting the quantization, dequantization, and stochastic
rounding altogether, we establish the SRT layers, in which we
consider the scaling down for (de)quantization as truncation.
USENIX Association
30th USENIX Security Symposium    2149
Figure 1: Adopting SWALP (in Green) and Overcoming the Hard Parts (in Red) for Crypto Tools via SRT Layers and Our Protocols (in Blue)
(Conv: Convolution, Quant: Quantization, De-Q: De-Quantization, (Max)Pool: (Max-)Pooling, Act: Activation, ReLU: Rectiﬁed Linear Unit)
3 GPU-Friendly Oblivious Computation
3.2 Overview of GForce
3.1 Cryptographic Toolbox and Notations
Additive Homomorphic Encryption (AHE). AHE is
(public-key) encryption that features additive homomorphism,
i.e., [x + y] = [x] + [y], where [m] denotes a ciphertext of m.
One can also multiply [x] with a plaintext m, i.e., [mx] = m· [x].
Homomorphic operations can be fused into a linear func-
tion f ([m]) := mult· [m] + bias over vectors/matrices/tensors
m ∈ (Zp)
n, where [m] = ([m0], [m1], . . . , [mn−1]) and f can
output multiple values. We use AHEq (or simply AHE) and
AHEp to denote an AHE scheme over Zq and Zp, respectively.
We mostly omit the ﬁeld size, e.g., as in [m] instead of [m]p.
AHE is supposed to have circuit privacy, i.e., with [m] and
sk, one cannot learn mult and bias from ct = mult· [m] + bias.
q,(cid:104)x(cid:105)C
Additive Secret Sharing. A client C can secret-share its
private x ∈ Zq to a server S by randomly picking rS ∈ Zq,
sending it to S, and keeping (x− rS) mod q locally. Either
share alone has no information about x. We let (cid:104)x(cid:105)S
q ∈ Zq
be the shares of x held by S and C, respectively. For brevity, we
use the notation of (cid:104)x(cid:105) = {(cid:104)x(cid:105)S,(cid:104)x(cid:105)C} to denote both shares,
and omit the underlying ﬁeld when it is clear. When the ﬁeld
size should be emphasized (for both the secret share and its
ciphertext), we may run into notation such as [(cid:104)β(cid:105)C
S and C can jointly compute secret shares of c = a· b using
Beaver’s trick [3] (Protocol 8) if they had (cid:104)u(cid:105),(cid:104)v(cid:105), and (cid:104)z(cid:105) s.t.
u· v = z. The core idea is to ﬁrst reconstruct µ = u−a and ν =
v− b, then the shares are (cid:104)z(cid:105)i − µ(cid:104)v(cid:105)i −ν(cid:104)u(cid:105)i + iµν, where i ∈
{0,1} represents {S, C}. Operating over secret shares is very
efﬁcient on GPU and incurs less overhead than AHE. It can
be generalized to matrix operations and tensor convolutions.
Additive SS has a near-to-plaintext performance for addi-
tion and plaintext-SS multiplication (c·(cid:104)x(cid:105) = (cid:104)c·x(cid:105)). Vectoriz-
ing these operations using GPU, which is extensively done by
GForce, hugely outperforms their counterparts using AHE.
p]p.
In supervised learning, every training data is a data point x
associated with a label y. A DNN tries to learn the relationship
between x and y. Inference outputs a label y of query x.
GForce allows a server S with a DNN model DNN(·) to
provide oblivious inference. It returns DNN(x) to client C
without knowing the client query x and DNN(x). Meanwhile,
C remains oblivious to the learnable parameters of DNN.
Most DNNs consist of many linear and non-linear layers.
In GForce, each layer i outputs additive SS (cid:104)x(i)(cid:105) to the server
and the client, which in turn acts as the input to the next layer.
For linear layers, GForce supports fully-connected layers,
which multiply the input by a learnable weighting matrix, and
convolution layers, which convolute learnable kernels over the
input. Secure computation of linear function is typically done
via the homomorphism of AHE (reviewed in Section 3.1). We
propose AHE-to-SOS transformation (in Section 3.3), which
transforms the traditional AHE-based approach into our GPU-
friendly linear computation protocol over secret shares.
For non-linear layers, we focus on comparison as a core
operation. We propose GPU-friendly secure comparison pro-
tocols (in Section 3.4) built on top of DGK protocols [7], with
any wrap-around error ﬁxed (in Section 3.5). GForce thus
supports the most common choices of activation and pooling
layers, i.e., ReLU and maxpool (in Section 3.6), respectively.
GForce also speciﬁcally considers SWALP-trained DNNs
embodied by the SRT layers (in Section 3.7), which efﬁciently
divide and wrap around the inputs in additive SS, whose
resulting value distribution is close to stochastic rounding.
To summarize, C produces an additive SS of its query (cid:104)x(0)(cid:105).
C and S then sequentially invoke our protocols according to
the architecture of DNN over their additive SS {(cid:104)x(i)(cid:105)}, and
eventually, C recovers DNN(x) from the additive SS of the last
layer. Tables 1-2 list the (existing and new) building blocks.
2150    30th USENIX Security Symposium
USENIX Association
Figure 2: Our AHE-to-SOS Transformation for Crypto Protocols
Figure 3: Our Secure Online/Ofﬂine Share Computation (SOS) for
Linear Functions: [·] is an AHE ciphertext. (cid:104)·(cid:105) is an additive SS.
Secure On/off Share Computation
Share-Computation variant of DGK
SOS (§3.3)
SC-DGK (§3.4)
GPU-DGK / -Wrap GPU-friendly DGK or Wrap protocol
and its ofﬂine or online sub-protocol
oﬀ/on (§3.4/§3.5)
Stochastic Rounding and Truncation
SRT layer
(§1, §2.4, §3.7)
tailored for SWALP-trained DNN
Table 2: Acronyms for New Concepts in GForce
3.3 Secure Online/Ofﬂine Share Computation
One of our core ideas is to replace the online computation over
AHE ciphertexts of the query with the ofﬂine computation
over AHE ciphertexts of some query-independent randomness
and the (fast) computation over secret shares of the query.
Table 3 lists the notations for describing our protocols.
AHE-to-SOS Transformation. An AHE-based protocol (Fig-
ure 2) starts by C sending an encrypted value [x] to S. S then
applies its private linear function f on [x] and returns the re-
sult to C. Figure 3 describes the resulting protocol obtained
after AHE-to-SOS transformation. We call this trick secure
online/ofﬂine share computation (SOS).1 As our most basic
usage of AHE, our protocol in Figure 3 is also named SOS.
In the ofﬂine phase, C randomly picks rC and encrypts it
to S. S then applies f over this AHE ciphertext [11,13], masks
it with rS, and sends the results back to C. C decrypts it and
keeps the result as an output share rC for the online phase.
GForce leverages the linearity2 f (χ) = f (χ− r) + f (r) to
protect χ. In the online phase, S and C each hold an input
share, (cid:104)χ(cid:105)S and (cid:104)χ(cid:105)C. C additively masks its input share with
rC and sends it to S. S reconstructs another additive SS (χ−
rC) and computes (cid:104) f (χ)(cid:105)S := f (χ−rC)−rS on GPU. (cid:104) f (χ)(cid:105)S
and (cid:104) f (χ)(cid:105)C := f (rC) + rS are the output shares. Note that
(cid:104) f (χ)(cid:105)S +(cid:104) f (χ)(cid:105)C = f (χ).
1The naming of our (secret) shares may be “abused” in some sense, e.g., an
“output share” can be created even before knowing the output because one can
create the corresponding share that matches with it when the output is known
in a later time. For example, in our SOS, the client has (cid:104) f (χ)(cid:105)C := f (rC)+rS
in SOS’s ofﬂine phase even though f (χ) is unknown.
2Slalom [24] precomputes f (r) in f (χ) = f (χ − r) + f (r) within the
trusted environment. Here, we precompute f (r) with AHE.
SOS reduces the online computation time (of using AHE).
The transformed protocol processes a batch of inputs in addi-
tive SS to fully utilize GPU’s batch-processing performance.
Using SS instead also reduces the online communication.
Applications. To apply SOS (Figure 3), S needs to know f ,
including its internal parameters, in the ofﬂine phase. This
requirement is trivial for linear layers, such as convolution
and fully-connected layers, because S knows the weight.
Beyond linear layers, we also apply the SOS trick to our
other protocols that use AHE, e.g., DGK for comparison. For
these protocols, the internal parameters of f are usually secret
random values generated by S, which we can somehow move
to the ofﬂine phase, as Sections 3.4 and 3.5 will show.
3.4 GPU-Friendly Secure Comparison
In the DGK protocol [7] (Protocol 5), the server S and the
client C hold private integers α(cid:96)−1···α1α0 and β(cid:96)−1···β1β0
respectively. It processes from (cid:96)− 1 to 0 to locate the ﬁrst
differing bit via computing bi, which is 0 iff (α j = β j)∀ j:i< j<l
and αi (cid:54)= βi. For that, C sends all [βi] to S. S then computes
[bi]∀i∈{(cid:96)−1,...,0} = [a] + ([αi]− [βi]) + 3
(cid:96)−1
∑
j=i+1
[α j ⊕ β j]
(1)
with a = 1− 2δS and a random bit δS picked by S ofﬂine. To
test also if α = β, S computes [b−1] = [δS] + ∑(cid:96)−1
j=0[α j ⊕ β j].
S can compute [α j ⊕ β j] via AHE: (1− 2· α j)· [β j] + [α j].
S sends {[bi]} back to C after shufﬂing their orders and
multiplying each of them by a different random number rS×,i.
With the decryption key, C sets δC := 1 ∈ Z2 if any ciphertext
decrypts to 0; 0 otherwise, where δS ⊕ δC = (α ≤ β).
Removing AHE from (Online Phase of) DGK Protocol.
We assume the server knows α ofﬂine at the moment. When
the server picks the randomness (e.g., a) ofﬂine, we can
re-write the multiplication of Equation 1 with rS×,i as fol-
low, which is for applying our AHE-to-SOS trick over DGK:
f SC-DGK
i,α(β) =
i,a,α,rS×,i
(β) = rS×,i· (a +αi−βi +3· f ⊕
i,α(β)), where f ⊕
USENIX Association
30th USENIX Security Symposium    2151
Bit-width of the DNN’s data
Finite ﬁeld for the DNN’s data
Finite ﬁeld for result bits {bi} (Eq. 1)
AHE ciphertext of x under Zq
SS of x under Zq held by S or C
Number of inputs in a batch
SC-DGK’s Server or Client input
αi ⊕ βi (i-th bit of α or β)
Divisor of an SRT layer
v mod d for v ∈ {q,τ,s}
(cid:96) (§2.1, §3, §4)
Zq (§3,§4)
Zp (§3,§4)
[x]q or [x] (§3)
(cid:104)x(cid:105)S
q / (cid:104)x(cid:105)C
q (§3)
k (§3.4)
α / β (§3.4)
φi (§3.4)
d (§3.7)
vd (§3.7.3, App. B)
τ (§3.4, §3.5, §3.7) Additive mask for the shared input s
s + τ mod q in GPU-DGK or -Trun
z (§3.4, §3.5, §3.7)
wrap (§3.4, §3.7.3) Value that offsets wrapped-around z
Table 3: Notations (and where are they mostly discussed)
Figure 4: GPU-DGK prepares τ and α ofﬂine to enable efﬁcient
SOS computations of SC-DGK and GPU-Wrap.
i+1,α(β) if i (cid:54)= (cid:96), and f ⊕
((1− 2αi)· βi + αi) + f ⊕
(cid:96),α(·) = 0. The
equivalence follows from αi ⊕ βi = (1− 2· αi)· βi + αi. The
AHE-to-SOS transformation of DGK using the above (re-
cursive) linear function (corresponding to Lines 6 to 11 and
Lines 20 to 21) results in our Protocol 1, named SC-DGK for
share-based computation, with φi denotes the output of f ⊕
i,α.
Protocol 1 processes a batch of k inputs, which we just
denote any operand or result related to each of them as a
single variable (e.g., δ or P but not set/vector notation with
subscript δδδ j or {P j}) to avoid running into double subscripts
(e.g., we need to break input β into its bit-representation).
Looking ahead, Protocols 2, 3, and 4 also work on batches.
GPU-Friendly Secure Comparison. Beyond requiring an
ofﬂine-known α, SC-DGK has two drawbacks. First, both α
and β have to be non-negative, while the inputs to comparison-
based layers can be negative. Second, the inputs need to be
known to either S or C. GForce cannot use it to process any
(intermediate) value protected by additive SS.
Inspired by the protocol of Veugen [26], our new protocol
GPU-DGK (Protocol 2) can accept additive secret shares of
probably negative input x and y from S and C, without assum-
ing any online input is known in the ofﬂine phase. GPU-DGK
reduces the comparison of x ≤ y to that of α ≤ β in SC-DGK.
As illustrated in Figure 4, in GPU-DGK, S picks τ ∈ Zq and
sets α = τ mod 2(cid:96) ofﬂine. In the online phase, S and C got (cid:104)x(cid:105)
and (cid:104)y(cid:105). S masks (cid:104)y− x(cid:105)S by τ and sends (cid:104)z(cid:105)S = (cid:104)y(cid:105)S −(cid:104)x(cid:105)S +
Protocol 1 Share-Computation Variant of DGK for Ofﬂine α
Ofﬂine Input (S|C)
Online Input (S|C)
0 ≤ α < 2(cid:96)
(cid:104)α ≤ β(cid:105)S
k many (α,β) are processed together,