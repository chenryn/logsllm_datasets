Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:20:28 UTC from IEEE Xplore.  Restrictions apply. 
567
domain[1]
domain[2]
domain[3]
SlabSlabSlab
Page
Page
Page
Page
Page
Page
for its slabs, it causes the buddy allocator to shift the allocated
memory into the speciﬁed xMP domain (§ V-A).
C. Switches across Execution Contexts
e
c
a
p
s
l
e
n
r
e
K
#VE
Slab allocator
vCPU
M
M
V
CPU
Buddy allocator
Guest physical memory
Xen altp2m
Host physical memory
Figure 3. Extensions to the slab and buddy allocator facilitate shifting
allocated pages and slabs into xMP domains enforced by Xen altp2m.
ﬂags, the allocator receives sufﬁcient information to inform
the Xen altp2m subsystem to place the allocation into a
particular xMP domain (Figure 3). Currently, we use 8 free bits
in the allocation ﬂags to encode the domain index, effectively
supporting up to 256 distinct domains—more domains can
be supported by redeﬁning gfp_t accordingly. This way, we
can grant exclusive access permissions to all pages assigned
to the target xMP domain, while, at the same time, we can
selectively withdraw access permissions to the allocated page
from all other domains (§ IV-A). As such, accesses to pages
inside the target domain become valid only after switching to
the associated guest memory view managed by Xen altp2m.
During the assignment of allocated pages to xMP domains,
we record the PG_xmp ﬂag into the flags ﬁeld of struct
page, thereby enabling the buddy allocator to reclaim previ-
ously xMP-protected pages at a later point in time.
B. Slab Allocator
The slab allocator builds on top of the buddy allocator to
subdivide allocated pages into small, sub-page sized objects
(Figure 3), to reduce internal fragmentation that would other-
wise be introduced by the buddy allocator. More precisely, the
slab allocator maintains slab caches that are dedicated to fre-
quently used kernel objects of the same size [70]. For instance,
the kernel uses a cache for all struct task_struct
instances. Such caches allow the kernel to allocate and free
objects in a very efﬁcient way, without the need for explic-
itly retrieving and releasing memory for every kernel object
allocation. Historically, the Linux kernel has used three slab
allocator implementations: SLOB, SLAB, and SLUB, with the
latter being the default slab allocator in modern Linux kernels.
Every slab cache groups collections of continuous pages
into so-called slabs, which are sliced into small-sized objects.
Disregarding further slab architecture details, as the allocator
manages slabs in dedicated pages, this design allows us to
place selected slabs into isolated xMP domains using the
underlying buddy allocator. To achieve this, we extend the
slab implementation so that we can provide the __GFP_XMP
ﬂag and xMP domain index on creation of the slab cache.
Consequently, every time the slab cache requests further pages
The Linux kernel is a preemptive, highly-parallel system
that must preserve the process-speciﬁc or thread-speciﬁc state
on (i) context switches and (ii) interrupts. To endure context
switches, and also prevent other threads from accessing iso-
lated memory, it is essential to include the index of the thread’s
(open) xMP domain into its persistent state.1
1) Context Switches:
In general, operating systems as-
sociate processes or threads with a dedicated data struc-
ture, the Process Control Block (PCB): a container for the
thread’s state that is saved and restored upon every context
switch. On Linux, the PCB is represented by the struct
task_struct. We extended task_struct with an addi-
tional ﬁeld, namely xmp_index_kernel, representing the
xMP domain the thread resides in at any point in time. We
dedicate this ﬁeld to store the state of the xMP domain used
in kernel space. By default, this ﬁeld is initialized with the
index of the restricted view that accumulates the restrictions
enforced by every deﬁned xMP domain (§ IV-A). The thread
updates its xmp_index_kernel only when it enters or exits
an xMP domain. This way, the kernel can safely interrupt the
thread, preserve its open xMP domain, and schedule a different
thread. In fact, we extended the scheduler so that on every
context switch it switches to the saved xMP domain of the
thread that is to be scheduled next. To counter switching to
a potentially corrupted xmp_index_kernel, we bind this
index to the address of the task_struct instance in which
it resides. This allows us to verify the integrity and context of
the index before entering the xMP domain   (§ IV-C). Since
adversaries cannot create valid authentication codes without
knowing the respective secret key, they will neither be able
to forge the authentication code of the index, nor reuse an
existing code that is bound to a different task_struct.
2) Hardware Interrupts:
Interrupts can pause a thread’s
execution at arbitrary points. In our current prototype, accesses
to memory belonging to any of the xMP domains are restricted
in interrupt (IRQ) context. (We plan on investigating primitives
for selective memory protection in IRQ contexts in the future.)
To achieve this, we extend the prologue of every interrupt
handler and cause it to switch to the restricted view. This
way, we prevent potentially vulnerable interrupt handlers from
illegally accessing protected memory. Once the kernel returns
control
it will cause a memory
access violation when accessing the isolated memory. Yet,
instead of trapping into the VMM, the thread will trap into
the in-guest #VE handler (§ II-C). The #VE handler, much
like a page fault handler, veriﬁes the thread’s eligibility and
context-bound integrity by authenticating the HMAC of its
xmp_index_kernel. If the thread’s eligibility and the
index’s integrity is given, the handler enters the corresponding
to the interrupted thread,
1Threads in user space enter the kernel
to handle system calls and
(a)synchronous interrupts. Speciﬁcally, upon interrupts, the kernel reuses the
task_struct of the interrupted thread, which must be handled with care.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:20:28 UTC from IEEE Xplore.  Restrictions apply. 
568
xMP domain and continues the thread’s execution. Otherwise,
it causes a segmentation fault and terminates the thread.
3) Software Interrupts: The above extensions introduce
a restriction with regard to nested xMP domains. Without
maintaining the state of nested domains, we require every
thread to close its active domain before opening another one;
by nesting xMP domains, the state of the active domain will be
overwritten and lost. Although we can address this requirement
for threads in process context, it becomes an issue in interrupt
context: the former executes (kernel and user space) threads
that are tied to different task_struct structures, while the
latter reuses the task_struct of interrupted threads.
In contrast to hardware interrupts that disrupt the system’s
execution at arbitrary locations, the kernel explicitly schedules
software interrupts (softirq) [71], e.g., after handling a
hardware interrupt or at the end of a system call. As soon as the
kernel selects a convenient time slot to schedule a softirq,
it will temporarily delay the execution of the active process
and reuse its context for handling the pending softirq.
The Linux kernel conﬁgures 10 softirq vectors, with one
dedicated for the Read-Copy-Update (RCU) mechanism [72].
A key feature of RCU is that every update is split into (i) a
removal and (ii) a reclamation phase. While (i) removes
references to data structures in parallel to readers, (ii) re-
leases the memory of removed objects. To free the object’s
memory, a caller registers a callback that is executed by the
dedicated softirq at a later point it time. If the callback
accesses and frees memory inside an xMP domain, it must
ﬁrst enter the associated domain. Yet, as the callback reuses
the task_struct instance of an arbitrary thread, it must
not update the thread’s index to its open xMP domain.
To approach this issue, we leverage the callback-free RCU
feature of Linux (CONFIG_RCU_NOCB_CPU). Instead of
handling RCU callbacks in a softirq, the kernel dedicates
a thread to handle the work. This simpliﬁes the management
of the thread-speciﬁc state of open xMP domains, as we can
bind it to each task individually: if the thread responsible
for executing RCU callbacks needs to enter a speciﬁc xMP
domain, it can do so without affecting other tasks. As is the
case with hardware IRQs, xMP does not allow deferring work
that accesses protected memory in softirq context.
D. User Space API
We grant user processes the ability to protect selected
memory regions by extending the Linux kernel with four new
system calls that allow processes to use xMP in user space
(Figure 4). Speciﬁcally, applications can dynamically allocate
and maintain disjoint xMP domains in which sensitive data
can remain safe (-). Furthermore, we ensure that attackers
cannot illegally inﬂuence a process’ active xMP domain state
by binding its integrity to the thread’s context ( ).
Linux has provided an interface for Intel MPK since
kernel v4.9. This interface comprises three system calls,
sys_pkey_{alloc,free,mprotect}, backed by libc
wrapper functions for the allocation, freeing, and assignment
of user space memory pages to protection keys. Applications
domain[1]
domain[2]
domain[3]
Page
Page
Page
Page
Page
Page
Page
Page
Page
Application A
Application B
libc
#VE
mprotect
vCPU
CPU
Buddy allocator
Guest physical memory
Xen altp2m
Host physical memory
e
c
a
p
s
r
e
s
U
e
c
a
p
s
l
e
n
r
e
K
M
M
V
Figure 4. User-space applications interact with the Linux kernel through
mprotect to conﬁgure independent xMP domains.
use the unprivileged WRPKU instruction to further manage
memory access permissions of the corresponding protec-
tion keys (§ II-A). Likewise, we implemented the system
calls sys_xmp_{alloc,free,mprotect}, which utilize
altp2m HVMOPs (§ IV-B) for allowing programmers to allo-
cate and maintain different xMP domains in user space. In fact,
these system calls implement functionality equivalent to Intel
MPK on Linux; they can be used as a transparent alternative
on legacy systems without sufﬁcient hardware support (-).
On sys_xmp_mprotect invocation, we isolate the target
virtual memory area (§ IV-B) and tag it so that we can identify
protected memory and release it upon memory reclamation.
Contrary to the MPK implementation of Linux, we do
not use the unprivileged VMFUNC instruction in user space.
Instead, we provide an additional
system call, namely
sys_xmp_enter, which enters a requested, previously al-
located xMP domain (either more or less restricted) and
updates the state of the currently active xMP domain. We
save the respective state inside the xmp_index_user ﬁeld
of mm_struct that is unique to every thread in user space.
Also, we bind this index to the address of mm_struct ( ).
This enables the kernel to verify the integrity and context of
the xMP domain index on context switches—in other words,
the kernel has the means to detect unauthorized modiﬁcations
of this ﬁeld and immediately terminate the application. Note
that, with regard to our threat model, we anticipate orthogonal
defenses in user space that severely restrain attackers to
data-oriented attacks (§ III). By further removing VMFUNC
instructions from user space, and mediating their execution via
sys_xmp_enter, we avoid unnecessary Return-Oriented
Programming (ROP) (or similar code-reuse) gadgets, which
could be (ab)used to illegally switch to arbitrary xMP domains.
VI. USE CASES
We demonstrate the effectiveness and usefulness of xMP
by applying it on: (a) page tables and process credentials,
in the Linux kernel; and (b) sensitive in-process data in four
security-critical applications and libraries.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:20:28 UTC from IEEE Xplore.  Restrictions apply. 
569
A. Protecting Page Tables
With Supervisor Mode Execution Protection (SMEP) [48],
the kernel cannot execute code in user space; adversaries
have to ﬁrst inject code into kernel memory to accomplish
their goal. Multiple vectors exist
that allow attackers to
(legitimately) inject code into the kernel. In fact, system
calls use the routine copy_from_user to copy a user-
controlled (and potentially malicious) buffer into kernel mem-
ory. While getting code into the kernel is easy, its execution
is obstructed by different security mechanisms. For instance,
W⊕X withdraws execute permissions from the memory that
contains data copied from user space. In addition, defenses
based on information hiding, such as Kernel Space Address
Layout Randomization (KASLR) [45], further obstruct kernel
attacks but are known to be imperfect [51], [73]–[75]. Once
adversaries locate the injected code, they can abuse memory
corruption vulnerabilities, e.g., in device drivers or the kernel
itself, to compromise the system’s page tables [76]. This, in
turn, opens up the gate for code injection or kernel code
manipulation. Consequently, ensuring the integrity of page
tables is an essential requirement, which remains unfulﬁlled
by existing kernel hardening techniques [76]–[78].
Our goal is to leverage xMP to prevent adversaries from
illegally modifying (i) page table contents and (ii) pointers
to page tables. At the same time, xMP has to allow the kernel
to update page table structures from authorized locations. With
the exception of the initial page tables that are generated