title:Model Orthogonalization: Class Distance Hardening in Neural Networks
for Better Security
author:Guanhong Tao and
Yingqi Liu and
Guangyu Shen and
Qiuling Xu and
Shengwei An and
Zhuo Zhang and
Xiangyu Zhang
8
8
6
3
3
8
9
.
2
2
0
2
.
4
1
2
6
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
2
2
0
2
©
0
0
.
1
3
$
/
2
2
/
9
-
6
1
3
1
-
4
5
6
6
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
2
2
0
2
2022 IEEE Symposium on Security and Privacy (SP)
Model Orthogonalization: Class Distance Hardening
in Neural Networks for Better Security
Guanhong Tao, Yingqi Liu, Guangyu Shen, Qiuling Xu, Shengwei An, Zhuo Zhang, Xiangyu Zhang
Email: {taog, liu1751, shen447, xu1230, an93, zhan3299, xyzhang}@cs.purdue.edu
Department of Computer Science, Purdue University
Abstract—The distance between two classes for a deep learning
classifier can be measured by the level of difficulty in flipping
all (or majority of) samples in a class to the other. The class
distances of many pre-trained models in the wild are very
small and do not align well with humans’ intuition (e.g., classes
turtle and bird have smaller distance than classes cat and
dog), making the models vulnerable to backdoor attacks, which
aim to cause misclassification by stamping a specific pattern to
inputs. We propose a novel model hardening technique called
model orthogonalization which is an add-on training step to
pretrained models,
including clean models, poisoned models,
and adversarially trained models. It can substantially enlarge
class distances with reasonable training cost and without much
accuracy degradation. Our evaluation on 5 datasets with 22
model structures show that our technique can enlarge class
distances by 177.63% on average with less than 1% accuracy loss,
outperforming existing hardening techniques such as adversarial
training, universal adversarial perturbation, and directly using
generated backdoors. It reduces 80% false positives for a state-
of-the-art backdoor scanner as the enlarged class distances allow
the scanner to easily distinguish clean and poisoned models, and
substantially outperforms three existing techniques in removing
injected backdoors.
I. INTRODUCTION
A backdoor in a deep learning model makes any inputs
stamped with a specific pattern to be misclassified to a target
class. While adversarial sample attack requires generating
perturbations on the fly to cause an input sample, e.g., a video
frame, to be misclassified, backdoor attack can have prompt
effect by simply stamping a pattern. While backdoors can be
injected through various methods, such as data poisoning [1]–
[5], clean label poisoning [6]–[9], and neuron hijacking [10],
they widely exist in naturally trained models (see Section II).
We call them natural triggers. Natural triggers could be due to
(1) the similarity between classes, e.g., a small fixed patch on
any dog images can make the classifier predict cat, and (2) the
model undesirably learning strong low-level features. We will
show in Section II that there is a small backdoor between the
turtle and bird classes even though they are unlike in humans’
eyes. With the increasing applications of deep learning models
in security-critical tasks such as autonomous driving [11], [12],
surveillance [13], [14], access control [15], etc., backdoors are
becoming a prominent security threat.
Existing defense techniques can be categorized to backdoor
scanning that determines if a model has an injected back-
door [16]–[24], backdoor attack detection that determines on-
the-fly if an input contains a backdoor pattern [24]–[35], and
backdoor elimination that removes an injected backdoor [36]–
these techniques focus on defending injected
[39]. Most
backdoors. For example, backdoor scanners Neural Cleanse
(NC) [40] and ABS [41] rely on the assumption that injected
backdoors tend to be small as they want
to be stealthy.
Backdoor attack detection techniques such as NIC [25] and
SCAn [26] rely on the observation that an input sample
stamped with a backdoor likely causes different model internal
behaviors than a clean sample. Elimination techniques such
as NAD [39] rely on benign samples to suppress injected
backdoors. Detailed discussion of these techniques can be
found in Section VII. However, they are much less effective
for natural backdoors which are not injected but rather due to
problems in training and even the nature of data. For example,
natural backdoors cause a lot of false warnings for scanners as
they cannot distinguish natural and injected backdoors [42]; a
sample with a natural backdoor may likely evade detection
as natural backdoors are usually benign features that may
not induce abnormal internal behaviors; and using clean data
cannot eliminate natural backdoors which are rooted exactly
in the clean data.
Adversarial training is a widely used technique for model
hardening which can force a model to unlearn unrobust (low-
level) features. It aims to enforce any input undertaking adver-
sarial perturbations in an Lp bound to be correctly classified
by the hardened model. According to our studies (Section II
and Section V), the improved robustness can help mitigate
backdoors including natural backdoors. However, due to its
Lp bounded training, which only considers local perturbations
around individual samples, the protection against backdoors
is limited. Also, it is known that adversarial training may
cause non-trivial model performance degradation. In addition,
our study shows that directly using backdoors generated
by scanners to adversarially train a model does not work
well due to either the extremely high computation cost or
the longer convergence time. Intuitively, using a backdoor
(which denotes much larger perturbations compared to those
in adversarial samples) in adversarial training is like imposing
a substantial displacement of the decision boundary. If not
done properly, the decision boundary will oscillate. And since
natural backdoors could exist in between any pair of classes,
the training has quadratic complexity, which is very costly
when the number of classes is large.
We propose a novel model hardening method to improve
resilience to backdoors. It is an add-on training step for pre-
© 2022, Guanhong Tao. Under license to IEEE.
DOI 10.1109/SP46214.2022.00078
1372
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:03:02 UTC from IEEE Xplore.  Restrictions apply. 
trained models, including adversarially pre-trained models. It
substantially suppresses backdoors,
including both injected
and natural backdoors, making backdoor attacks more difficult
(e.g., having to use much larger patterns and/or easier to be
detected by scanners), with reasonable training cost and with-
out substantial accuracy degradation. Specifically, we consider
the minimal backdoor between two classes (that flips samples
in a victim class to a target class) the distance between the
classes. As such, we aim to achieve the possible maximum
distances for all class pairs. Intuitively, if we project all high
dimensional data points to a 2-dimensional space, a decision
boundary allows the maximum distance between two classes
when it is perpendicular to the line between the centers of
mass of the two classes (visualization and more explanation
in Section III). We hence call the process of finding such
a decision boundary model orthogonalization. Our technique
hardens all class pairs for a model. For each pair a and b,
it repetitively generates the minimal backdoors from a to
b and from b to a and adversarially trains the model with
the two backdoors. In other words, it forces the model to
gradually unlearn the low-level backdoor features (and focus
on high-level features with more semantics). The symmetric
training of the two directions of a pair substantially alleviates
oscillation and improve effectiveness. To address the high
computation cost induced by the quadratic complexity, our
technique features a scheduler that selects a most promising
pair to harden in each training round based on the potential of
distance enlargement. It leverages the observation that different
class pairs have different distance capacities. For example, a
pair of turtle and bird has more potential than a pair of cat
and dog as the latter two are so close that their distance is
hard to enlarge no matter how much training effort is spent. A
number of methods are further used to speed up the training
process such as reusing backdoors from previous rounds and
dynamically adjusting bounds. Details are in Section IV.
Our contributions are summarized as follows.
• We intuitively and formally define the problem of model
orthogonalization, which is an add-on training step. As
part of it, we define the distance between a pair of classes,
which serves as the basis for the hardening process.
• We devise a new training process to achieve orthogonal-
ization. It features symmetric training (training the two
directions of a pair together), pair scheduling, and a few
other designs.
• We develop a prototype MOTH (Model OrTHogonaliza-
tion). Our evaluation on four standard datasets and six
different model structures shows that MOTH can improve
class distance for naturally trained models by 119.87%
and adversarially trained models by 52.87% with less
than 1% accuracy degradation on average. With similar
hardening performance, our technique is 9x faster than a
baseline that does not use scheduling. It achieves 29.72%
more distance improvement than a baseline that does
not perform symmetric training. It can achieve 95.80%
more distance improvement and 2.58x faster than using
universal adversarial perturbations [43]. We also con-
duct experiments on 30 pre-trained models downloaded
from the TrojAI competition [42], a competition for
backdoor scanning. MOTH improves the class distance
by 232.39% over the original models and is 11x faster
than the baseline. We apply MOTH in two applications
including reducing false positives for backdoor scanning
and eliminating injected backdoors in existing models.
It can reduce false positives by 81.25% for the first
application. Regarding the second application, the attack
success rate (ASR) of injected backdoors is reduced (by
orthogonalization) from almost 100% to 1% on average,
outperforming three state-of-the-art backdoor elimination
approaches with the best performance of reducing ASR to
26.75% on average. MOTH is publicly available at [44].
Threat Model. We consider backdoors between individual
pairs. That is, a backdoor can flip samples from a victim
class to a target class, called label-specific backdoor in the
literature [40], [41], [45], which is more general than universal
backdoor that flips any sample of any class to a target class. We
consider backdoors that are either injected (in a poisoned/hi-
jacked model) or naturally present (in a clean model). We
consider both are equally harmful. Our goal is to enlarge
class distances such that it is more difficult to find backdoors,
without sacrificing much accuracy. That is, to launch attack,
the attacker needs to use a large pixel pattern that may already
possess a lot of semantic features of the target class.
In this paper, we only consider static backdoors, in which
the backdoor patterns are input agnostic, like patch back-
doors [1]. There are dynamic backdoors such as reflection
backdoors [3], composite backdoors [4], and feature space
backdoors [46]. We conduct a preliminary study on a few
dynamic backdoors and MOTH can reduce the ASR to some
extent (see Section VIII). We will leave more exploration to
our future work. We argue that our contributions are still very
valuable because model hardening for static backdoors is still
an open problem. Finally, we assume only a subset of the
original training dataset (5%) is available when MOTH is used
to remove injected backdoors.
II. MOTIVATION
Figure 1 shows sample images from a normally trained
ImageNet model downloaded from a widely-used model repos-
itory [47] and its natural backdoors (derived using NC [40]).
The first column shows the backdoors, with the first row
flipping dog images to cat, the second one turtle to bird, and
the third one cat to bird. The second and third columns show
the victim class samples; the fourth and fifth columns the
victim class samples stamped with the backdoor patterns; the
sixth column the target class samples, and the last column the
size of backdoor in terms of the aggregated pixels in the R,
G and B channels (i.e., L1 norm). For instance, a trigger of
size 615 (second row) has around 615/3 = 205 ≈ 14 × 14
changed pixels. This is a very small backdoor compared to
an input image of 224 × 224 pixels for ImageNet models. In
other words, the model is very vulnerable, even though it is
not poisoned by dirty data. In fact, over 90% of the samples
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:03:02 UTC from IEEE Xplore.  Restrictions apply. 
21373
Fig. 1: Natural backdoor versus injected backdoor. The left figure shows natural backdoors (1st column) generated by a model
scanner (i.e., NC) for a benign ImageNet model. The right figure presents three poisoned models with injected backdoors with
different colors/shapes (1st column). Columns Size denote the size of generated backdoors.
in the victim classes are misclassified to the target class when
they are stamped with the small backdoors. Also observe that
these backdoors seem to be very low level features that do not
constitute any meaningful features in humans’ eyes. The model
might have overfitted on these low level (strong) features of the
target class, causing the natural backdoors. We observe that the
backdoor sizes do not meet our intuitions of the distances of
these classes. For example, intuitively cat and dog have more
similarity than turtle and bird. However, this is not reflected by
the sizes of natural backdoors. In some sense, we can say the
model is not “orthogonal” (more discussion in Section III).
Similar information is presented for three poisoned Ima-
geNet models from [41]. Each model is poisoned in a way that
all samples of the victim classes (the third column) stamped
with the backdoors in the first column are misclassified to the
target class as shown in fifth column. While the models were
poisoned with the backdoors shown in the first column, the
models overfit on some low-level features of the backdoor
patterns used in poisoning such that after poisoning,
the
injected backdoors are just the small pixel patterns shown in
the second column. The last column shows the backdoor sizes.