# Cisco Confidential
© 2009-2010 Cisco Systems, Inc. All rights reserved.

## NG Update: Inter-VM Data Exfiltration

### The Art of Cache Timing Covert Channel on x86 Multi-Core
**Author:** Etienne Martineau  
**Role:** Kernel Developer  
**Date:** August 2015

---

## Overview
This presentation explores the concept of inter-VM data exfiltration using cache timing covert channels on x86 multi-core processors. The focus is on practical implementation and real-world scenarios rather than purely theoretical research.

### Disclaimer
- The information and code provided in this presentation are for educational purposes only.
- The author is not responsible for any misuse of the information.
- The information should not be used to cause direct or indirect damage.
- This research was conducted in the author's personal time and represents his own opinions, not those of his employer.

---

## About Me
[Insert brief bio or relevant background information about the author]

---

## VM Configuration
- **VM #1 (Client)**
- **VM #2 (Server)**

---

## Modulating a Contention Pattern
### VM #1
- **Pattern:** 1 | 0 | 0 | 0 | 1
- **Operations:** MUL | NOP | NOP | NOP | MUL

### VM #2
- **Detection:** Slow | Fast | Fast | Fast | Slow
- **Pattern:** 1 | 0 | 0 | 0 | 1

---

## Shared Resources
### Hyper-Threading (HT) Enabled
- **Pipeline Contention**
- **L1 and L2 Cache Modulation**

### HT Disabled
- **Pipeline Contention**
- **L1, L2, and L3 Cache Modulation**

### Multi-Socket
- [Insert details on multi-socket configurations]

---

## Cache Line Encoding and Decoding
### VM #1
- **Cache Line Pattern:** CL0 | CL1 | CL2 | CL3 | CL4
- **Values:** 1 | 0 | 0 | 0 | 1
- **Operations:** Load | Flush | Flush | Flush | Load

### VM #2
- **Access Time:** Fast | Slow | Slow | Slow | Fast
- **Decoded Pattern:** 1 | 0 | 0 | 0 | 1

---

## Prefetching
Prefetching involves bringing data or instructions into the cache before they are needed. For example, the Core™ i7 processor and Xeon® 5500 series processors have prefetchers that bring data into the L1 and L2 caches. These prefetchers use different algorithms to predict future address needs based on data access patterns.

### Simple Test
- **Flush CL0 -> CL100**
- **Measure Access Time for CL0 -> CL100**
- **Result:** Long latency for all cache lines

### Solution
- **Randomize Cache Line Access:**
  - Random within a page
  - Random across pages
  - This approach confuses the hardware prefetcher.

---

## Data Persistence and Noise
### Data Persistence
- **What happens if we wait longer before decoding?**
- **Observation:** Encoded data in the cache evaporates quickly.

### Noise
- [Insert detailed discussion on noise and its impact on the covert channel]

---

## Client-Server Setup in VMs
- **Client in VM #1, Server in VM #2**
- **Cache Tagging:**
  - L2 and L3 caches are tagged by physical addresses.
  - In a VM, the visible physical address is different from the actual physical address used by the bare metal cache.
  - An additional layer of translation is involved.

---

## Conclusion
- **Practical Implementation:**
  - Abusing shared resources
  - Cache line encoding/decoding
  - Overcoming hardware prefetchers
  - Handling data persistence and noise
  - Guest-to-host page table de-obfuscation
  - High-precision inter-VM synchronization

- **Detection and Mitigation:**
  - [Insert details on detection and mitigation strategies]

---

© 2009-2010 Cisco Systems, Inc. All rights reserved.  
Cisco Confidential