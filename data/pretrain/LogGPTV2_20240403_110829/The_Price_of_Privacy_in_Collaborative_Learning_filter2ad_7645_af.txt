DP
Sup
RecSys
CoL
P oP
GT
N E
BR
Machine Learning
Matrix Factorization
Stochastic Gradient Descent
Diﬀerential Privacy
Suppression
Recommender System
Collaborative Learning
Price of Privacy
Game Theory
Nash Equilibrium
Best Response
Table 6. Frequently used abbreviations
B Proofs for Sec. 4
Proof Th. 1. Without loss of generality, assume Player
2 sets p2 = 1. The highest accuracy Player 1 can achieve
corresponds to p1 = 0 due to the Def. 3 and 4. From Def.
4 we can also deduce that if one player sets its privacy
parameter to maximal 1 then neither of the players can
obtain higher accuracy by training together than train-
ing alone. As such, the highest accuracy what Player 1
can reach by training together when Player 2 sets its pri-
vacy parameter to maximum is less than what it would
achieve by training alone: Φ1(0, 1) ≥ θ1. Note that Φ
and θ measures the error, i.e., the higher these values
are, the less accurate the corresponding model is.
As such, p1 = 0 does not correspond to positive ben-
eﬁt but only results in privacy loss. Hence, the highest
payoﬀ Player 1 can reach is 0 corresponding to maximal
privacy protection p1 = 1. In other words, if Player 2
sets p2 = 1 the BR of Player 1 is also to set p1 = 1. Since
this is also true on the other way around, (p∗
2) = (1, 1)
is indeed a NE which is equivalent to the case of training
alone.
Proof L. 1. If αn = 0, the utility function in Eq. (1)
is reduced to un = Bn · b(θn, Φn) since Cn = 0. This
is strictly positive by deﬁnition. Also by deﬁnition b is
monotone decreasing in pn. As a result, the utility is
1, p∗
16
highest when no privacy protection is in place. As such,
indeed exists αn such that ˆpn = 0 is the BR for player
n.
Proof L. 2. Without loss of generality we assume n = 1.
We show that maxp1 u1(p1, p2) = u1(1, p2) = 0 if C1 →
∞ which is equivalent with the statement in Lemma 2:
lim
C1→∞ u1(p1, p2) =
C1→∞ B1 · b(θ1, Φ1(p1, p2)) − c(p1) · C1 ≤
lim
C1→∞ B1 · b(θ1, Φ1(0, 0)) − c(p1) · C1 =
lim
C1→∞ β0 − c(p1) · C1 =
lim
if
β0
−∞ if
(
(13)
c(p1) = 0
c(p1) > 0
As a result, u1(p1, p2) ≤ β0 for some β0 ≥ 0 and it
can only be non-negative if c(p1) = 0 which corresponds
to p1 = 1. The utility is maximal in this case, thus,
maxp1 u(p1, p2) = u(1, p2) which is indeed 0.
Proof Th. 2. In the proof of Lemma 1 we set Cn = 0 in
which case player n’s BR was indeed ˆpn = 0. For more
details read the proof of Lemma 1.
Proof Th. 3. The utility function u(p1) is maximal in
the interval [0, 1] either on the border or at a point where
its derivative is zero. The derivative of Eq. (1) is
u0(p1) = B1b0(p1)Φ0(p1) − C1c0(p1)
(14)
which is zero at ˜p1 if
u0(˜p1) = 0 ⇒ b0(˜p1)Φ0(˜p1)
c0(˜p1)
= C1
B1
(15)
Of course the extreme point ˜p1 must be in [0, 1] and
u(˜p1) > 0. Furthermore, this extreme point is only max-
imum when the second derivative is negative.
On the other hand, if Eq. (14) is never zero on [0, 1]
or the second derivative is positive at that point, the
maximum of u(p1) is on the edge of the interval [0, 1].
u(1) = 0 since both the beneﬁt and the privacy loss
function is zero at p1 = 1. As a result, p1 = 0 is the
maximum point if u(0) > 0. This is indeed the case
when the maximal beneﬁt b(0) is higher than the ratio
B1 as it is shown
of the privacy and accuracy weight C1
below:
0 
C1
B1
Proof Th. 4. We divide un by Bn: ˜un = un
. This new
Bn
function inherits the properties of un (such as the sign,
monotonicity, maximum/minimum points, etc.). As a
result, a similar game with utility function ˜un has the
same equilibria. Furthermore, this similar game is a po-
tential game if the mixed second order partial derivative
of the utility functions are equal. Due to the constitu-
tion of ˜un, this condition is equivalent to
∂p1 ∂p2 b(θ1, Φ1(p1, p2)) = ∂p1 ∂p2 b(θ2, Φ2(p1, p2))
This formula can be transformed into the one in the
theorem by applying the chain rule for higher dimen-
sions.
Proof Col. 1. The left side of the equation in Th. 4 is
zero since we assumed ∂p1Φ1 = ∂p2Φ2. On the right side
∂p1 ∂p2Φ2 = ∂2
p2Φ2 for the same
reason. This means Th. 4 holds since both sides of the
equation are 0.
p1Φ1 and ∂p2 ∂p1Φ1 = ∂2
C Proof of Theorem 5
Proof Th. 5. Since the user set of the players are dis-
joint while the item set are shared, the only thing the
players need to share is the item feature matrix Q. The
eﬀect of a single update is shown in Eq. (8). We as-
sume that the data points are independent, hence, the
sensitivity ˜S of one update is
˜S = max
rui
|q0
ki − qki| = max
≤ γ(∆rpmax + λqmax)
rui
[γ(euipuk − λqki)]
where
–
– ∆r is the maximal distance of two ratings: ∆r =
k ∈ [1, κ]
max rui − min rui
pmax and qmax are the maximal absolute value of
the user and item features respectively.
–
˜S is the sensitivity of updating a single feature, thus,
to capture the full eﬀect of the update on the vector qi,
we need to multiply ˜S with the qi’s dimension κ. More-
over, we have only considered the eﬀect of a rating on Q
within one iteration. However, this occurs ι times. Thus,
to achieve ε-DP, we need to apply κ·ι· ˜S
level of Lapla-
cian noise on the ratings before the training due to the
Composition Theorem. Therefore, the overall sensitivity
is indeed bounded by the formula in the theorem.
ε
17
D Preprocessing
1 Remove items/users with less than 10 ratings.
2 For each remaining items, calculate the average rat-
ing and discount it from the corresponding rui’s:
r0
ui := rui − IAvg(i)
3 For each remaining user, calculate the average rat-
ing and discount it from the corresponding r0
ui’s:
ui − UAvg(u) = rui − IAvg(i) − UAvg(u)
ui := r0
r00
4 The discounted ratings as well as the averages are
clamped:
–
– UAvg(u) ∈ [−2, 2]
–
IAvg(i) ∈ [min(rui), max(rui)] = [1, 5]
ui ∈ [−2, 2]
r00
E Self-Division: Experiment
1 We create datasets with approximately the same
density but with diﬀerent size:
–
1M: We modify the size of the dataset while
keeping its density: we randomly removing
users such that the remaining dataset has
1000k/800k/600k ratings (i.e., the players have
500k-500k/400k-400k and 300k-300k ratings).
– NF10D: We create a new dataset originated
from NF10 by increasing its density to the level
of 1M via ﬁltering out the less rated items8. Af-
terwards we modify the size of this dataset while
keeping its newly acquired density: we randomly
removing users such that the remaining dataset
has 8m/6m/4m/2m ratings.
i
.
2 We execute CoL with pi ∈ {0, 0.2, 0.4, 0.6} for
i = {1, 2} and for M ∈ {Sup, bDP} using the
newly created datasets (e.g., the players have
300k/400k/. . . /3m/4m ratings) and we obtain the
normalized accuracy improvement for both player:
i = θi−ΦM
Φ0
3 We execute CoL with the same privacy parameters
and methods using only one player’s data: the play-
ers imitate CoL by halving their own datasets (e.g.,
the datasets sizes are 150k-150k/. . . /2m-2m) and
θi
where eθn and fΦn corre-
we obtain eΦ0 = eθn−fΦM
neθn
sponds to the average of the accuracies using the
two half of player n’s data.
8 We remove the items which have less than 250 ratings
4 We calculate the RMSE between the original nor-
malized accuracy Φ0 and the approximated normal-
ized accuracy via self-division eΦ0 for both players
and privacy methods.
Fig. 9. We show the error (RMSE) of self-division (i.e., Φ0 − eΦ0)
for both player and privacy methods.
We found, that the RMSE was minimal for both privacy
mechanism and player when the players have |Dn| =
2 000 000 ratings. This means, for datasets with density
n when
approximately d = 0.05 fΦ0
n is the closest to Φ0
100 000 ≈ d · |Dn| heuristic holds.
F Playerwise Approximations
fΦ1
p2 = 0.0
p2 = 0.4
p2 = 0.2
0.28
0.25
−0.07
−1.01
p1 = 0.0
p1 = 0.2
p1 = 0.4
p1 = 0.6
fΦ2
p2 = 0.0
p2 = 0.2
p2 = 0.4
p2 = 0.6
0.26
0.16
−0.10
−1.16
0.24
0.15
−0.19
−1.37
p1 = 0.0
p1 = 0.2
p1 = 0.4
0.17
0.14
−0.14
−1.19
0.16
0.12
−0.17
−1.21
0.15
0.12
−0.28
−1.28
18
p2 = 0.6
−0.05
−0.05
−0.37
−1.72
p1 = 0.6
−0.05
−0.07
−0.60
−1.83
p2 = 0.6
−0.03
−0.26
−0.69
−2.08
p1 = 0.6
−0.05
−0.18
−0.52
−1.85
Φ1
p2 = 0.0
p2 = 0.2
p2 = 0.4
p1 = 0.0
p1 = 0.2
p1 = 0.4
p1 = 0.6
0.17
0.15
−0.13
−1.16
0.14
0.12
−0.19
−1.32
0.11
0.08
−0.33
−1.49
Φ2
p1 = 0.0
p1 = 0.2
p1 = 0.4
0.31
0.31
−0.14
−1.13
p2 = 0.0
p2 = 0.2
p2 = 0.4
p2 = 0.6
Table 7. The approximated privacy-accuracy tradeoﬀ function for
both players (fΦ1,fΦ2) and its true value (Φ1 and Φ2).
0.17
0.11
−0.22
−1.30
0.23
0.22
−0.16
−1.25