# Hadoop: Apache's Open Source Implementation of Google's MapReduce Framework

## Presenters
- **Hacked Existence Team**
  - Joey Calca & Ryan Anguiano
  - [http://hackedexistence.com](http://hackedexistence.com)

## Cloud Computing
- **Definition**: Clouds are large collections of other people's machines, combined with virtualization.
- **Characteristics**:
  - Remote
  - Scalable
  - Virtual
  - High-Level API
  - Coarse-Grained Data Processed in Parallel

*Courtesy: Stanzione, Sannier, and Santanam, Arizona State University*

## Data Volumes
- **Wayback Machine**: 2 PB + 20 TB/month (2006)
- **Google**: Processes 20 PB a day (2008)
- **Human Speech**: ~5 EB (all words ever spoken by humans)
- **NOAA**: ~1 PB climate data (2007)
- **CERN's LHC**: Generates 15 PB a year (2008)

*Stats from The iSchool University of Maryland*

## Saguaro Cluster
- **Research Group**: High Performance Computing Initiative
- **Department**: Fulton School
- **Primary Application**: Various
- **Processor Cores**: 4,560
- **Processor Architecture**: Intel Xeon
- **Interconnect**: InfiniBand
- **Memory**: 10,240 GB (Total)
- **Storage**: 215 TB
- **OS**: CentOS 5.3
- **Sys Admin Contact**: Douglas Fuller

## Google’s Map/Reduce
- **Introduction**: Presented at The Sixth Symposium on Operating System Design and Implementation (2004).
- **Purpose**: Processing and generating large data sets.
- **Features**:
  - Many real-world tasks can be expressed using this model.
  - Automatically parallelized for large clusters of commodity machines.
  - Highly scalable, typically processing many terabytes of data in parallel.
  - Up to 1,000 MapReduce jobs executed daily on Google's clusters.

### Map/Reduce Workflow
- **Input -> Mapper -> Intermediate Pairs -> Reducer -> Output**
- **Ease of Use**: No prior experience required to utilize resources of large distributed systems.
- **Scalability**: Typically processes many terabytes of data in parallel.

## Hadoop
- **Overview**: Apache Project's open-source implementation of MapReduce.
- **Language**: Java-based.
- **Scalability**: Demonstrated on clusters with 2,000 nodes; current design target is 10,000 node clusters.
- **Website**: [http://hadoop.apache.org](http://hadoop.apache.org)

### Mapper
- **Function**: Applies a function `f` to each element in the list.
- **Example**:
  ```java
  Map[f, (1, 2, 3, 4, 5)] -> {f[1], f[2], f[3], f[4], f[5]}
  ```
- **Input**: Entire data set.
- **Output**: Collects key-value pairs and passes them to the reducer as a hashmap.

### Reducer
- **Function**: Applies a function `f` to each element of the list plus an accumulator.
- **Example**:
  ```java
  Reduce[f, x, {a, b, c}] => f[f[f[x, a], b], c]
  ```
- **Input**: Output hashmap from the mapper.
- **Output**: A list of the output of reduce().

### Map/Reduce Framework
- **Map**: Implicitly parallel, order of application does not matter.
- **Reduce**: Executed in serial on a single node.
- **Data Flow**: Results of map() are copied, sorted, and sent to the reduce().

### Features
- **Automatic Handling**:
  - Work distribution
  - Scheduling
  - Networking
  - Synchronization
  - Fault recovery
  - Data movement between nodes

### Master Node
- **Role**: Assigns tasks and data to each node.
- **JobTracker**: Hosted on port 50030.
- **Monitoring**: Queries each node, kills unresponsive tasks, and reassigns them to available nodes.

### Streaming
- **Portability**: Mappers and reducers can be written in any language executable on each node.
- **Input/Output**: Read from stdin(), write to stdout().
- **Utility**: Packages all executables into a single JAR and distributes it to all nodes.

### Reporting
- **Communication**: Stdin/Stdout for data, Stderr for communication with the master node.
- **Counters**: Reported after every output line to track job progress.
- **Status Messages**: Used to track errors in log files.

### Hadoop Distributed File System (HDFS)
- **Comparison**: Similar to GoogleFileSystem (GFS).
- **Features**:
  - High fault-tolerance
  - Low-cost hardware
  - High throughput, streaming access to data
  - Data split into 64 MB blocks and replicated in storage

### HBase
- **Comparison**: Equivalent to Google’s BigTable.
- **Type**: Non-relational database.
- **Features**:
  - Not built for real-time querying
  - Moving away from per-user actions towards per-action data sets
  - Distributed, multi-dimensional, de-normalized data
- **Note**: HBase is not an SQL database.

### Amazon's Elastic Compute Cloud (EC2)
- **Service**: Provides resizable compute capacity in Amazon’s cloud.
- **Hadoop**: Packaged as a public EC2 image (AMI) for easy setup.
- **Command**: `ec2-describe-images -a | grep hadoop-ec2-images`
- **Website**: [http://aws.amazon.com/ec2/](http://aws.amazon.com/ec2/)

### Netflix Dataset
- **Dataset**: 2 GB dataset of movie/user/ratings.
- **Details**:
  - MovieIDs: 1 to 17,770
  - CustomerIDs: 1 to 2,649,429 (480,189 users)
  - Ratings: 1 to 5 stars
  - Dates: YYYY-MM-DD format
- **Optimization**:
  - Default input creates one mapper per file (inefficient).
  - Reorganized into 104 files for 100 processors using an awk script.
  - Efficiency gain: Netﬂix1 - 43:27, Netﬂix1Reorg - 9:55, pyNetﬂix1 - 13:02, awkNetﬂix1 - 9:04.

### Netflix1 Program
- **Purpose**: Produce statistical information about each movie.
- **Input**: Entire Netflix dataset.
- **Output**: First date rated, last date rated, total rating count, and average rating for each movie.

### Netflix2 Program
- **Purpose**: Calculate statistics based on users.
- **Mapper Output**: 
- **Reducer Output**: 

### Shoutouts
- **Dr. Adrian Sannier**: University Technology Officer
- **Dr. Dan Stanzione Jr.**: Director of High Performance Computing Initiative
- **Dr. Raghu Santanam**: Associate Professor
- **Nathan Kerr and Jeff Conner**

### Contact Information
- **Joey Calca**: r3dﬁsh@hackedexistence.com
- **Ryan Anguiano**: bl4ckbird@hackedexistence.com
- **Website**: [http://hackedexistence.com](http://hackedexistence.com)

### Questions?