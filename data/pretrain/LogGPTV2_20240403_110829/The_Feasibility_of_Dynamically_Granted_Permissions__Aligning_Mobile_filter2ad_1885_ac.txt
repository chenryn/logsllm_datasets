95.45% 82.14%
95.34% 81.48%
44.11%
64.49%
64.28%
66.67%
58.65%
63.39%
64.27%
57.95%
52.27%
Prompts
0.00
12.34
15.79
16.91
6.43
12.24
9.06
3.84
2.00
TABLE III
THE ACCURACY AND NUMBER OF DIFFERENT POSSIBLE ASK-ON-FIRST-
USE COMBINATIONS. A: APPLICATION REQUESTING THE PERMISSION, P:
PERMISSION TYPE REQUESTED, V: VISIBILITY OF THE APPLICATION
REQUESTING THE PERMISSION, AF : APPLICATION RUNNING IN THE
FOREGROUND WHEN THE REQUEST IS MADE. AOFU-AP IS THE POLICY
USED IN ANDROID MARSHMALLOW I.E., ASKING (PROMPTING) THE USER
FOR EACH UNIQUE APPLICATION, PERMISSION COMBINATION. THE TABLE
ALSO DIFFERENTIATES POLICY NUMBERS BASED ON THE SUBPOPULATION
OF Contextuals, Defaulters, AND ACROSS ALL USERS.
continuity correction revealed a statistically signiﬁcant differ-
ence in Contextuals’ responses based on requesting application
visibility (p < 0.013, r = 0.312), while for Defaulters there
was no statistically signiﬁcant difference (p = 0.227). That is,
Contextuals used visibility as a contextual cue, when deciding
the appropriateness of a given permission request, whereas
Defaulters did not vary their decisions based on this cue.
Figure 2 shows the distribution of users based on their denial
rate. Vertical lines indicate the borders between Contextuals
and Defaulters.
In the remainder of the paper, we use our Contextuals–
Defaulters categorization to measure how current and pro-
posed models affect these two sub-populations, issues unique
to these sub-populations, and ways to address these issues.
V. ASK-ON-FIRST-USE PERMISSIONS
Ask-on-ﬁrst-use (AOFU) is the current Android permission
model, which was ﬁrst adopted in Android 6.0 (Marshmallow).
AOFU prompts the user whenever an application requests a
dangerous permission for the ﬁrst time [9]; the user’s response
1082
to this prompt is thereafter applied whenever the same ap-
plication requests the same permission. As of March 2017,
only 34.1% of Android users have Android Marshmallow or
a higher version [10], and among these Marshmallow users,
those who upgraded from a previous version only see runtime
permission prompts for freshly-installed applications.
For the remaining 65.9% of users, the system policy is
ask-on-install (AOI), which automatically allows all runtime
permission requests. During the study period, all of our partic-
ipants had AOI running as the default permission model. Be-
cause all runtime permission requests are allowed in AOI, any
of our ESM prompts that the user wanted to deny correspond
to mispredictions under the AOI model (i.e., the AOI model
granted access to the data against users’ actual preferences).
Table III shows the expected median accuracy for AOI, as
well as several other possible variants that we discuss in this
section. The low median accuracy for Defaulters was due to
the signiﬁcant number of people who simply denied most of
the prompts. The prompt count is zero for AOI because it
does not prompt the user during runtime; users are only shown
permission prompts at installation.
More users will have AOFU in the future, as they upgrade
to Android 6.0 and beyond. To the best of our knowledge,
no prior work has looked into quantifying the effectiveness of
AOFU systematically; this section presents analysis of AOFU
based on prompt responses collected from participants and cre-
ates a baseline against which to measure our system’s improve-
ment. We simulate how AOFU performs through our ESM
prompt responses. Because AOFU is deterministic, each user’s
response to the ﬁrst prompt for each application:permission
combination tells us how the AOFU model would respond for
subsequent requests by that same combination. For participants
who responded to more than one prompt for each combination,
we can quantify how often AOFU would have been correct for
subsequent requests. Similarly, we also measure the accuracy
for other possible policies that the platform could use to decide
whether to prompt the user. For example, the status quo is
for the platform to prompt the user for each new applica-
tion:permission combination, but how would accuracy (and the
number of prompts shown) change if the policy were to prompt
on all new combinations of application:permission:visibility?
Table III shows the expected median accuracy3 for each
policy based on participants’ responses. For each policy, A
represents the application requesting the permission, P rep-
resents the requested permission, V represents the visibility
of the requesting application, and AF represents the applica-
tion running in the foreground when a sensitive permission
request was made. For instance, AOFU-AP is the policy
where the user will be prompted for each new instance of
an application:permission combination, which the Android
6.0 model employs. The last column shows the number of
runtime prompts a participant would see under each policy
over the duration of the study,
if that policy were to be
3The presented numbers—except for average prompt count, which was nor-
mally distributed—are median values, because the distributions were skewed.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:17 UTC from IEEE Xplore.  Restrictions apply. 
implemented. Both AOFU-AP and AOFU-AF PV show about
a 4.9× reduction in error rate compared to AOI; AOFU-AF PV
would require more prompts over AOFU-AP, though yields a
similar overall accuracy rate. 4 Moving forward, we focus our
analysis only on AOFU-AP (i.e., the current standard).
Instances where the user wants to deny a permission and the
policy instead allows it (false positives) are privacy violations,
because they expose more information to the application than
the user desires. Instances where the user wants to allow
a permission, but the policy denies it (false negatives) are
functionality losses. This is because the application is likely
to lose some functionality that the user desired when it is
incorrectly denied a permission. Privacy violations and func-
tionality losses were approximately evenly split between the
two categories for AOFU-AP: median privacy violations and
median functionality losses were 6.6% and 5.0%, respectively.
The AOFU policy works well for Defaulters because, by
deﬁnition, they tend to be consistent after their initial responses
for each combination. In contrast, the decisions of Contextuals
vary due to other factors beyond just the requesting application
and the requested permission type. Hence, the accuracy of
AOFU for Contextuals is signiﬁcantly lower than the accuracy
for Defaulters. This distinction shows that learning privacy
preferences for a signiﬁcant portion of users requires a deeper
understanding of factors affecting their decisions, such as
behavioral tendencies and contextual cues. As Table III sug-
gests, superﬁcially adding more contextual variables (such as
visibility of the requesting application) does not necessarily
help to increase the accuracy of the AOFU policy.
The context in which users are prompted under AOFU might
be a factor affecting its ability to predict subsequent instances.
In previous work [43], we found that the visibility of the
requesting application is a strong contextual cue users use to
vary their decisions. During the study period, under the AOFU-
AP policy, 60% of the prompts could have occurred when
the requesting application was visible to the participant—these
prompts had an accuracy of 83.3% in predicting subsequent
instances. In instances where participants were prompted when
the requesting application was running invisibly to the user,
AOFU-AP had an accuracy of 93.7% in predicting subsequent
instances. A Wilcoxon signed-ranks test, however, did not
reveal a statistically signiﬁcant difference (p < 0.3735).
Our estimated accuracy numbers for AOFU may be inﬂated
because AOFU in deployment (Android 6 and above) does
not ﬁlter permission requests that do not reveal any sensitive
information. For example, an application can request
the
ACCESS_FINE_LOCATION permission to check whether the
phone has a speciﬁc location provider, which does not leak
sensitive information. Our AOFU simulation uses the invoked
function to determine if sensitive data was actually accessed,
and only prompts in those cases (in the interest of avoiding any
false positives), a distinction that AOFU in Android does not
make. Thus, an Android user would see a permission request
4While AOFU-AF PV has greater median accuracy when examining De-
faulters and Contextuals separately, because the distributions are skewed, the
median overall accuracy is identical to AOFU-AP when combining the groups.
1083
prompt when the application examines the list of location
providers, and if the permission is granted, would not subse-
quently see prompts when location data is actually captured.
Previous work found that 79% of ﬁrst-time permission requests
do not reveal any sensitive information [43], and nearly 33.9%
of applications that request these sensitive permission types do
not access sensitive data at all. The majority of AOFU prompts
in Marshmallow are therefore effectively false positives, which
incorrectly serve as the basis for future decisions. Given this,
AOFU’s average accuracy is likely less than the numbers
presented in Table III. We therefore consider our estimates of
AOFU to be an upper bound.
VI. LEARNING PRIVACY PREFERENCES
Table III shows that a signiﬁcant portion of users (the 47%
classiﬁed as Contextuals) make privacy decisions that depend
on factors other than the application requesting the permission,
the permission requested, and the visibility of the requesting
application. To make decisions on behalf of the user, we must
understand what other factors affect their privacy decisions.
We built a machine learning model trained and tested on our
labeled dataset of 4,224 prompts collected from 131 users over
the period of 42 days. This approach is equivalent to training a
model based on runtime prompts from hundreds of users and
using it to predict those users’ future decisions.
We focus the scope of this work by making the following as-
sumptions. We assume that the platform, i.e., the Android OS,
is trusted to manage and enforce permissions for applications.
We assume that applications must go through the platform’s
permission system to gain access to protected resources. We
assume that we are in a non-adversarial machine-learning
setting wherein the adversary does not attempt to circumvent
the machine-learned classiﬁer by exploiting knowledge of its
decision-making process—though we do present a discussion
of this problem and potential solutions in Section IX.
A. Feature Selection
Using the behavioral, contextual, and aggregate features
shown in Table II, we constructed 16K candidate features,
formed by combinations of speciﬁc applications and actions.
We then selected 20 features by measuring Gini importance
through random forests [30], signiﬁcance testing for corre-
lations, and singular value decomposition (SVD). SVD was
particularly helpful to address the sparsity and high dimension-
ality issues caused by features generated based on application
and activity usage. Table IV lists the 20 features used in the
rest of this work.
The behavioral features (B) that proved predictive relate to
browsing habits, audio/call traits, and locking behavior. All
behavioral features were normalized per day/user and were
scaled in the actual model. Features relating to browsing
habits included the number of websites visited, the proportion
of HTTPS-secured links visited, the number of downloads,
and proportion of sites visited that requested location access.
Features relating to locking behavior included whether users
employed a passcode/PIN/pattern,
the frequency of screen
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:19:17 UTC from IEEE Xplore.  Restrictions apply. 
Feature
Group
Behavioral
Features
(B)
Runtime
Features
(R1)
Aggregated
Features
(A)
Feature
Number of times a website is loaded to
the Chrome browser.
Out of all visited websites, the proportion
of HTTPS-secured websites.
The number of downloads through Chrome.
Proportion of websites requested location
through Chrome.
Number of times PIN/Password was used to
unlock the screen.
Amount of time spent unlocking the screen.
Proportion of times screen was timed out
instead of pressing the lock button.
Frequency of audio calls.
Amount of time spent on audio calls.
Proportion of time spent on silent mode.
Application visibility (True/False)
Permission type
User ID
Time of day of permission request
Average denial rate for (A1)
application:permission:visibility
Average denial rate for (A2)
applicationF :permission:visibility
Type
Numerical
Numerical
Numerical
Numerical
Numerical
Numerical
Numerical
Numerical
Numerical
Numerical
Categorical
Categorical
Categorical
Numerical
Numerical
Numerical
THE COMPLETE LIST OF FEATURES USED IN THE ML MODEL
TABLE IV
EVALUATION. ALL THE NUMERICAL VALUES IN THE BEHAVIORAL GROUP
ARE NORMALIZED PER DAY. WE USE ONE-HOT ENCODING FOR
CATEGORICAL VARIABLES. WE NORMALIZED NUMERICAL VARIABLES BY
MAKING EACH ONE A Z-SCORE RELATIVE TO ITS OWN AVERAGE.
unlocking, the proportion of times they allowed the screen to
timeout instead of pressing the lock button, and the average
amount of time spent unlocking the screen. Features under the
audio and call category were the frequency of audio calls, the
amount of time they spend on audio calls, and the proportion
of time they spent on silent mode.
Our runtime features (R1/R2) include the requesting appli-
cation’s visibility, permission requested, and time of day of
the request. Initially, we included the user ID to account for
user-to-user variance, but as we discuss later, we subsequently
removed it. Surprisingly, the application requesting the per-
mission was not predictive, nor were other features based on
the requesting application, such as application popularity.
Different users may have different ways of perceiving
privacy threats posed by the same permission request. To
account for this, the learning algorithm should be able to
determine how each user perceives the appropriateness of
a given request in order to accurately predict future deci-
sions. To quantify the difference between users in how they
perceive the threat posed by the same set of permission
requests, we introduced a set of aggregate features that could
be measured at runtime and that may partly capture users’
privacy preferences. We compute the average denial rate for
each unique combination of application:permission:visibility
5:permission:visibility (A2). These