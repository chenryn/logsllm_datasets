check: it may take time linear in the size of the graph. Also,
it did not lead to any significant improvement over REO in
our experiments, so we did not evaluate it in detail. However,
it is of conceptual significance because the resulting graph is
1730    27th USENIX Security Symposium
USENIX Association
optimal with respect to FD, i.e., any further reduction would
violate FD-preservation.
4.2.3 Redundant node optimization (RNO)
The goal of this optimization is to avoid generating additional
versions if they aren’t necessary for preserving dependence.
We create a new version vs of a vertex because, in general,
the descendants of vs could be different from those of vl,
the latest version of v so far. If we overzealously combine
vl and vs, then a false dependency will be introduced, e.g.,
a descendant of vl may backtrack to a node that is an
ancestor of vs but not vl. This possibility exists as long as
(a) the ancestors of vl and vs aren’t identical, and (b) vl has
non-zero number of descendants. We already considered
(a) in designing REO optimizations described above, so we
consider (b) here. Note that RNO needs to be checked only
on edges that aren’t eliminated by REO (or REO*).
Specifically, let vr,s be the latest version of v so far. Before
creating a new version of v due to an event at time t, we check
whether vr,s has any outgoing edge (i.e., any descendants). If
not, we replace vr,s with vr,t, instead of creating a new version
of v. Fig. 7 illustrates the result of applying this optimization.
RNO preserves dependence for descendants of v, but it
can change backward reachability of the node v itself. For in-
stance, consider the addition of an edge at time t from up,q to
vr,s. This edge is being added because it is not redundant, i.e.,
a backward search from v@s does not reach up,q. However,
when we add the new edge and update the timestamp to vr,t,
there is now a backward path from v@s to up,q. The simplest
solution is to retain the edge timestamp on edges added with
RNO, and use them to prune out false dependencies.6
4.2.4 Cycle-Collapsing Optimization (CCO)
Occasionally, cyclic dependencies are observed, e.g., a
process that writes to and reads from the same file, or two
processes that have bidirectional communication. As ob-
served by previous researchers [25, 24], such dependencies
can lead to an explosion in the number of versions. The typ-
ical approach is to detect cycles, and treat the nodes involved
as an equivalence class. A simple way to implement this
approach is as follows. Before adding an edge from a version
ur to vs, we check if there is a cycle involving u and v. If so,
we simply discard the edge. Our experimental results show
that cycle detection has a dramatic effect on some data sets.
Cycle detection can take time linear in the size of the
graph. Since the dependence graph is very large, it is
expensive to run full cycle detection before the addition
of each edge. Instead, our implementation only checks for
cycles involving two entities. We found that this was enough
to address most sources of version explosion. An alternative
6Note that these timestamps need to be used only when an edge added
with RNO is the first hop in a backward traversal. If a node v subject
to RNO gets a child x, this child would have been added after the end
timestamp of v. So, when we do a backward traversal from x, all parents
of v should in fact be backward reachable.
would be to search for larger cycles when a spurt in version
creation is observed.
4.2.5 Effectiveness of FD-optimizations
REO and RNO optimizations avoid new versions in most
common scenarios that lead to an explosion of versions with
naive versioning:
• Output files: Typically, these files are written by a single
subject, and not read until the writes are completed. Since
all the write operations are performed by one subject, REO
avoids creating multiple versions. In addition, all the write
operations are combined.
• Log files: Typically, log files are written by multiple sub-
jects, but are rarely read, and hence by RNO, no new
versions need to be created.
• Pipes: Pipes are typically written by one subject and read
by another. Since the set of writers does not change, a sin-
gle version is sufficient, as a result of REO. Moreover, all
the writes on the pipe can be combined into one operation,
and so can all the reads.
We found that most savings were obtained by REO, RNO,
and CCO. As mentioned above, REO* is more significantly
more expensive than REO and provided little additional
benefit. Another undesirable aspect of REO* (as well as the
SD optimization) is that it may change the paths generated
during a backward or forward analysis. Such changes have
the potential to make attack interpretation more difficult. In
contrast, REO, RNO and CCO preserve all cycle-free paths.
4.2.6 Correctness and Optimality
Theorem 8 BuildVer,
optimizations, preserves full dependence (FD).
together with RNO and REO*
Proof: We already showed that BuildVer preserves forward
and backward reachability between the timestamped graph G
and the naive versioned graph G. Hence it suffices to show
that the edges and nodes eliminated by REO* and RNO don’t
change forward and backward reachability in G. Now, REO*
optimization drops an edge (u,v,t) only if there is already an
edge from the latest version of u to the latest or a previous
version of v in G. In other words, no new ancestors will result
from adding this edge. Since no new ancestors are added, by
definition of FD, any additional paths created in the original
graph due to the addition of this edge do not have to be pre-
served. Thus REO* optimization satisfies the forward reach-
ability condition of FD. Moreover, since this edge does not
add new ancestors to v, it won’t change backward reachabil-
ity of any node from v or its descendants. Thus, the backward
reachability preservation condition of FD is also satisfied.
Regarding RNO optimization, note that it is applied
only when a node v has no descendants. In such a case,
preservation of backward and forward reachability from v’s
descendants holds vacuously.
USENIX Association
27th USENIX Security Symposium    1731
Optimality with respect to FD. We now show that the
combination of REO* and RNO optimizations results in
reductions that are optimal with respect to FD preservation.
This means that any algorithm that drops versions or
edges retained by this combination does not preserve full
dependence. In contrast, this combination preserves FD.
The main reasoning behind optimality is that REO*
creates a new version of an entity v whenever it acquires a
new dependency from another entity u. In particular, REO*
adds an edge from (the latest version of) u to (the latest
version of) v only when there is no existing path between
them.
In other words, this edge corresponds to a time
instance when v acquires a new ancestor u. For this reason,
reachability from u to v needs to be captured at this time
instance for FD preservation. Thus, an algorithm that omits
this edge would not preserve FD. On the other hand, if we
create an edge but not a new version of v, then there will be a
single instance of v in the versioned graph that represents two
distinct dependencies. In particular, there will be a path from
ut to vs, the version of v that existed before the time t of the
current event. As a result, ut would incorrectly be included
in a backward analysis result starting at the descendants of vs.
The only way to avoid this error is if vs had no descendants,
the condition specified in RNO. Thus, if either REO* or
RNO optimizations were violated, then, forensic analysis
of the versioned graph will yield incorrect results.
4.3 Source Dependence Preservation
In this section, we show how to realize source-dependence
preserving reduction. Recall that a source is an entity
that has no incoming edges. With this definition, sources
consist primarily of pre-existing files and network endpoints;
subjects (processes) are created by parents and hence are not
sources, except for the very first subject. While this is the
default definition, broader definitions of source can easily
be used, if an analyst considers other nodes to be possible
sources of compromise.
We use a direct approach to construct a versioned graph
that preserves SD. Specifically, for each node v, we maintain
a set Src(v) of source entities that v depends on. This set is
initialized to {v} for source nodes. Before adding an event
(u,v,t) to the graph, we check whether Src(u) ⊆ Src(v).
If so, all sources that can reach u are already backward
reachable sources of v, so the event can simply be discarded.
Otherwise, we add the edge, and update Src(v) to include
all elements of Src(u).
Although the sets Src(v) can get large, note that they need
to be maintained only for active subjects and objects. For
example, the source set for a process is discarded when it
exits. Similarly, the source set for a network connection can
be discarded when it is closed.
To save space, we can limit the size of Src. When the
size limit is exceeded for a node v, we treat v as having an
unknown set of additional ancestors beyond Src(v). This en-
sures soundness, i.e., that our reduction never drops an edge
that can add a new source dependence. However, size limits
can cause some optimizations to be missed. In order to min-
imize the impact of such misses, we first apply REO, RNO
and CCO optimizations, and skip the edges and/or versions
skipped by these optimizations. Only when they determine
an edge to be new, we apply the SD check based on Src sets.
Theorem 9 BuildVer, together with redundant edge and
redundant node optimizations and the source dependence
optimization, preserves source dependence.
Proof: Since full dependence preservation implies source
dependence preservation, it is clear that redundant edge and
redundant node optimizations preserve source dependence,
so we only need to consider the effects of source dependence
optimization. The proof is by induction on the number of
iterations of the loop that processes events. The induction hy-
pothesis is that, after k iterations, (a) Src(v) contains exactly
the source nodes that are ancestors of v, and (b) that SD has
been preserved so far. Now, in the induction step, note that
the algorithm will either add an edge (u,v) and update Src(v)
to include all of Src(u), or, discard the event because Src(v)
already contains all elements of Src(u). In either case, we
can show from induction hypothesis that Src(v) correctly cap-
tures all source nodes backward reachable from v. It is also
clear that that when the edge is discarded by the SD algorithm,
it is because the edge does not change the sources that are
backward reachable, and hence it is safe to drop the edge.
Optimality of SD algorithm. Note that when SD adds an
edge (u,v), that is because Src(u) includes at least one source
that is not in Src(v). Clearly, if we fail to add this edge, then
source dependence of v is no longer preserved. This implies
that the above algorithm for SD preservation is optimal.
5 Compact Representations
In this section, we describe how to use the techniques de-
scribed so far, together with others, to achieve highly compact
log file and main-memory dependence graph representations.
5.1 Compact Representation of Reduced Logs
After reduction, logs can be stored in their original format,
e.g., Linux audit records. However, these formats aren’t
space-efficient, so we developed a simple yet compact
format called CSR. CSR stands for Common Semantic
Representation, signifying that a unified format is used
for representing audit data from multiple OSes, such as
Linux and Windows. Translators can easily be developed
to translate CSR to standard log formats, so that standard
log analyzers, or simple tools such as grep, can be used.
In CSR, all subjects and objects are referenced using a
numeric index. Complex data values that get used repeatedly,
such as file names, are also turned into indices. A CSR file
begins with a table that maps strings to indices. Following
1732    27th USENIX Security Symposium
USENIX Association
this table is a sequence of operations, each of which
correspond to the definition of an object (e.g., a file, network
connection, etc.) or a forensic-relevant operation such as
open, read, write, chmod, fork, execve, etc. Operations
deemed redundant by REO, REO* and CCO can be omitted.
Each operation record consist of abbreviated operation
name, arguments (mostly numeric indices or integers), and
a timestamp. All this data is represented in ASCII format
for simplicity. Standard file compression can be applied on
top of this format to obtain further significant size reduction,
but this is orthogonal to our work.
5.2 Compact Main Memory Representation
Forensic analysis requires queries over the dependence
graph, e.g., finding shortest path(s) to the entry node of an
attack, or a depth-first search to identify impacted nodes.
The graph contains roughly the same information that might
be found in Linux audit logs. In particular, the graph captures
information pertaining to most significant system calls. Key
argument values are stored (e.g., command lines for execve,
file names, and permissions), while the rest are ignored (e.g.,
the contents of buffers in read and write operations).
Nodes in the dependence graph correspond to subjects
and objects. Nodes are connected by bidirectional edges
corresponding to events (typically, system calls). To
obtain a compact representation, subjects, objects, and
most
importantly edges must be compactly encoded.
Edges typically outnumber nodes by one to two orders of
magnitude, so compactness of edges is paramount.
The starting point for our compact memory representation
is the SLEUTH [10] system for forensic analysis and
attack visualization. The graph structure used in this paper
builds on some of the ideas from SLEUTH, such as the
use of compact identifiers for referencing nodes and node
attributes. However, we did away with many other aspects
of that implementation, such as the (over-)reliance on
compact, variable length encoding for events, based on
techniques drawn from data compression and encoding.
These techniques increased complexity and reduced runtime
performance. Instead, we rely primarily on versioned graphs
and the optimizations in Section 4 to achieve compactness.
This approach also helped improve performance, as we can
achieve graph construction rates about three times faster than
SLEUTH’s. Specifically, the main techniques we rely on to
reduce memory use in this paper are:
• Edge reductions: The biggest source of compaction is the
redundant edge optimization. Savings are also achieved
because we don’t need timestamps on most edges. In-
stead, timestamps are moved to nodes (subject or object
versions). This enables most stored edges to use just 6
bytes in our implementation, encoding an event name and
about a 40-bit subject or object identifier.
• Node reductions: The second biggest source of com-
paction is node reduction, achieved using RNO and CCO
optimizations. In addition, our design divides nodes into
two types: base versions and subsequent versions. Base
versions include attributes such as name, owner, command
line, etc. New base versions are created only when these
attributes change. Attribute values such as names and
command lines tend to be reused across many nodes, so
we encode them using compact ids. This enables a base
version to be stored in 32 bytes or less.
• Compact representation for versions: Subsequent versions
derived from base versions don’t store node attributes, but
just the starting and ending timestamps. By using relative
timestamps and sticking to a 10ms timestamp granularity7,
we are able to represent a timestamp using 16-bits in most
cases. This enables a version to fit within the same size
as an edge, and hence it can be stored within the edge
list of a base version. In particular, let S be the set of
edges occurring between a version v and the next version
appearing in the edge list. Then S is the set of edges
incident on version v in the graph.
Edge lists are maintained as vectors that can grow dynam-
ically for active nodes (i.e., running processes and open files)
but are frozen at their current size for inactive nodes. This
technique, together with the technique of storing versions
within the edge list, reduces fragmentation significantly. As
a result, we achieve a very compact representation that often
takes just a few bytes per edge in the original data.
6 Experimental Evaluation
We begin this section by summarizing our implementation in
Section 6.1. The data sets used in our evaluation are described
in Section 6.2. In Section 6.3, we evaluate the effectiveness
of FD and SD in reducing the number of events, and compare