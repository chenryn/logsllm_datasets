### 4.2.3 Redundant Node Optimization (RNO)

The objective of this optimization is to prevent the unnecessary generation of additional versions if they are not essential for preserving dependencies. A new version \( v_s \) of a vertex is created because, in general, the descendants of \( v_s \) may differ from those of \( v_l \), the latest version of \( v \) so far. If we overly combine \( v_l \) and \( v_s \), a false dependency could be introduced, for example, a descendant of \( v_l \) might backtrack to a node that is an ancestor of \( v_s \) but not of \( v_l \). This possibility exists as long as:
1. The ancestors of \( v_l \) and \( v_s \) are not identical.
2. \( v_l \) has a non-zero number of descendants.

We have already addressed condition (a) in the design of the REO optimizations described above, so here we focus on condition (b). Note that RNO only needs to be checked on edges that are not eliminated by REO (or REO*).

Specifically, let \( v_{r,s} \) be the latest version of \( v \) so far. Before creating a new version of \( v \) due to an event at time \( t \), we check whether \( v_{r,s} \) has any outgoing edges (i.e., any descendants). If it does not, we replace \( v_{r,s} \) with \( v_{r,t} \), instead of creating a new version of \( v \). Figure 7 illustrates the result of applying this optimization.

RNO preserves dependence for the descendants of \( v \), but it can change the backward reachability of the node \( v \) itself. For instance, consider the addition of an edge at time \( t \) from \( u_{p,q} \) to \( v_{r,s} \). This edge is added because it is not redundant, i.e., a backward search from \( v@t \) does not reach \( u_{p,q} \). However, when we add the new edge and update the timestamp to \( v_{r,t} \), there is now a backward path from \( v@t \) to \( u_{p,q} \). The simplest solution is to retain the edge timestamp on edges added with RNO and use them to prune out false dependencies.

### 4.2.4 Cycle-Collapsing Optimization (CCO)

Occasionally, cyclic dependencies are observed, such as a process that writes to and reads from the same file or two processes that have bidirectional communication. As noted by previous researchers [25, 24], such dependencies can lead to an explosion in the number of versions. The typical approach is to detect cycles and treat the nodes involved as an equivalence class. A simple way to implement this approach is as follows: before adding an edge from a version \( u_r \) to \( v_s \), we check if there is a cycle involving \( u \) and \( v \). If so, we discard the edge. Our experimental results show that cycle detection has a dramatic effect on some data sets.

Cycle detection can take time linear in the size of the graph. Since the dependence graph is very large, it is expensive to run full cycle detection before the addition of each edge. Instead, our implementation only checks for cycles involving two entities. We found that this was sufficient to address most sources of version explosion. An alternative would be to search for larger cycles when a spurt in version creation is observed.

### 4.2.5 Effectiveness of FD-Optimizations

REO and RNO optimizations avoid new versions in most common scenarios that lead to an explosion of versions with naive versioning:

- **Output files**: Typically, these files are written by a single subject and not read until the writes are completed. Since all write operations are performed by one subject, REO avoids creating multiple versions. Additionally, all write operations are combined.
- **Log files**: Log files are typically written by multiple subjects but rarely read, so no new versions need to be created with RNO.
- **Pipes**: Pipes are typically written by one subject and read by another. Since the set of writers does not change, a single version is sufficient due to REO. Moreover, all writes on the pipe can be combined into one operation, as can all reads.

We found that most savings were obtained by REO, RNO, and CCO. As mentioned, REO* is significantly more expensive than REO and provided little additional benefit. Another undesirable aspect of REO* (as well as the SD optimization) is that it may change the paths generated during a backward or forward analysis, potentially making attack interpretation more difficult. In contrast, REO, RNO, and CCO preserve all cycle-free paths.

### 4.2.6 Correctness and Optimality

**Theorem 8:** BuildVer, together with RNO and REO* optimizations, preserves full dependence (FD).

**Proof:** We have already shown that BuildVer preserves forward and backward reachability between the timestamped graph \( G \) and the naive versioned graph \( G' \). Therefore, it suffices to show that the edges and nodes eliminated by REO* and RNO do not change forward and backward reachability in \( G \). 

REO* optimization drops an edge \( (u, v, t) \) only if there is already an edge from the latest version of \( u \) to the latest or a previous version of \( v \) in \( G \). This means no new ancestors will result from adding this edge. Since no new ancestors are added, by definition of FD, any additional paths created in the original graph due to the addition of this edge do not need to be preserved. Thus, REO* satisfies the forward reachability condition of FD. Moreover, since this edge does not add new ancestors to \( v \), it won't change the backward reachability of any node from \( v \) or its descendants. Thus, the backward reachability preservation condition of FD is also satisfied.

For RNO optimization, note that it is applied only when a node \( v \) has no descendants. In such a case, the preservation of backward and forward reachability from \( v \)'s descendants holds vacuously.

**Optimality with respect to FD:** We now show that the combination of REO* and RNO optimizations results in reductions that are optimal with respect to FD preservation. This means that any algorithm that drops versions or edges retained by this combination does not preserve full dependence. In contrast, this combination preserves FD.

The main reasoning behind optimality is that REO* creates a new version of an entity \( v \) whenever it acquires a new dependency from another entity \( u \). Specifically, REO* adds an edge from (the latest version of) \( u \) to (the latest version of) \( v \) only when there is no existing path between them. In other words, this edge corresponds to a time instance when \( v \) acquires a new ancestor \( u \). For this reason, reachability from \( u \) to \( v \) needs to be captured at this time instance for FD preservation. Thus, an algorithm that omits this edge would not preserve FD.

On the other hand, if we create an edge but not a new version of \( v \), then there will be a single instance of \( v \) in the versioned graph that represents two distinct dependencies. In particular, there will be a path from \( u_t \) to \( v_s \), the version of \( v \) that existed before the time \( t \) of the current event. As a result, \( u_t \) would incorrectly be included in a backward analysis result starting at the descendants of \( v_s \). The only way to avoid this error is if \( v_s \) had no descendants, the condition specified in RNO. Thus, if either REO* or RNO optimizations were violated, forensic analysis of the versioned graph would yield incorrect results.

### 4.3 Source Dependence Preservation

In this section, we show how to realize source-dependence-preserving reduction. Recall that a source is an entity that has no incoming edges. With this definition, sources consist primarily of pre-existing files and network endpoints; subjects (processes) are created by parents and hence are not sources, except for the very first subject. While this is the default definition, broader definitions of source can easily be used if an analyst considers other nodes to be possible sources of compromise.

We use a direct approach to construct a versioned graph that preserves SD. Specifically, for each node \( v \), we maintain a set \( \text{Src}(v) \) of source entities that \( v \) depends on. This set is initialized to \(\{v\}\) for source nodes. Before adding an event \( (u, v, t) \) to the graph, we check whether \( \text{Src}(u) \subseteq \text{Src}(v) \). If so, all sources that can reach \( u \) are already backward reachable sources of \( v \), so the event can simply be discarded. Otherwise, we add the edge and update \( \text{Src}(v) \) to include all elements of \( \text{Src}(u) \).

Although the sets \( \text{Src}(v) \) can get large, note that they need to be maintained only for active subjects and objects. For example, the source set for a process is discarded when it exits. Similarly, the source set for a network connection can be discarded when it is closed.

To save space, we can limit the size of \( \text{Src} \). When the size limit is exceeded for a node \( v \), we treat \( v \) as having an unknown set of additional ancestors beyond \( \text{Src}(v) \). This ensures soundness, i.e., that our reduction never drops an edge that can add a new source dependence. However, size limits can cause some optimizations to be missed. To minimize the impact of such misses, we first apply REO, RNO, and CCO optimizations and skip the edges and/or versions skipped by these optimizations. Only when they determine an edge to be new, we apply the SD check based on \( \text{Src} \) sets.

**Theorem 9:** BuildVer, together with redundant edge and redundant node optimizations and the source dependence optimization, preserves source dependence.

**Proof:** Since full dependence preservation implies source dependence preservation, it is clear that redundant edge and redundant node optimizations preserve source dependence. We only need to consider the effects of the source dependence optimization. The proof is by induction on the number of iterations of the loop that processes events. The induction hypothesis is that, after \( k \) iterations, (a) \( \text{Src}(v) \) contains exactly the source nodes that are ancestors of \( v \), and (b) that SD has been preserved so far. In the induction step, note that the algorithm will either add an edge \( (u, v) \) and update \( \text{Src}(v) \) to include all of \( \text{Src}(u) \), or discard the event because \( \text{Src}(v) \) already contains all elements of \( \text{Src}(u) \). In either case, we can show from the induction hypothesis that \( \text{Src}(v) \) correctly captures all source nodes backward reachable from \( v \). It is also clear that when the edge is discarded by the SD algorithm, it is because the edge does not change the sources that are backward reachable, and hence it is safe to drop the edge.

**Optimality of SD Algorithm:** Note that when SD adds an edge \( (u, v) \), it is because \( \text{Src}(u) \) includes at least one source that is not in \( \text{Src}(v) \). Clearly, if we fail to add this edge, then the source dependence of \( v \) is no longer preserved. This implies that the above algorithm for SD preservation is optimal.

### 5 Compact Representations

In this section, we describe how to use the techniques described so far, together with others, to achieve highly compact log file and main-memory dependence graph representations.

#### 5.1 Compact Representation of Reduced Logs

After reduction, logs can be stored in their original format, e.g., Linux audit records. However, these formats are not space-efficient, so we developed a simple yet compact format called CSR (Common Semantic Representation). CSR signifies that a unified format is used for representing audit data from multiple operating systems, such as Linux and Windows. Translators can easily be developed to translate CSR to standard log formats, allowing the use of standard log analyzers or simple tools like `grep`.

In CSR, all subjects and objects are referenced using a numeric index. Complex data values that are used repeatedly, such as file names, are also turned into indices. A CSR file begins with a table that maps strings to indices. Following this table is a sequence of operations, each corresponding to the definition of an object (e.g., a file, network connection, etc.) or a forensic-relevant operation such as open, read, write, chmod, fork, execve, etc. Operations deemed redundant by REO, REO*, and CCO can be omitted.

Each operation record consists of an abbreviated operation name, arguments (mostly numeric indices or integers), and a timestamp. All this data is represented in ASCII format for simplicity. Standard file compression can be applied on top of this format to obtain further significant size reduction, but this is orthogonal to our work.

#### 5.2 Compact Main Memory Representation

Forensic analysis requires queries over the dependence graph, such as finding the shortest path(s) to the entry node of an attack or performing a depth-first search to identify impacted nodes. The graph contains roughly the same information that might be found in Linux audit logs, capturing information pertaining to most significant system calls. Key argument values are stored (e.g., command lines for execve, file names, and permissions), while the rest are ignored (e.g., the contents of buffers in read and write operations).

Nodes in the dependence graph correspond to subjects and objects, connected by bidirectional edges corresponding to events (typically, system calls). To obtain a compact representation, subjects, objects, and, most importantly, edges must be compactly encoded. Edges typically outnumber nodes by one to two orders of magnitude, so compactness of edges is paramount.

The starting point for our compact memory representation is the SLEUTH [10] system for forensic analysis and attack visualization. The graph structure used in this paper builds on some ideas from SLEUTH, such as the use of compact identifiers for referencing nodes and node attributes. However, we did away with many other aspects of that implementation, such as the (over-)reliance on compact, variable-length encoding for events, based on techniques drawn from data compression and encoding. These techniques increased complexity and reduced runtime performance. Instead, we rely primarily on versioned graphs and the optimizations in Section 4 to achieve compactness. This approach also helped improve performance, as we can achieve graph construction rates about three times faster than SLEUTH's. Specifically, the main techniques we rely on to reduce memory use in this paper are:

- **Edge Reductions:** The biggest source of compaction is the redundant edge optimization. Savings are also achieved because we don't need timestamps on most edges. Instead, timestamps are moved to nodes (subject or object versions). This enables most stored edges to use just 6 bytes in our implementation, encoding an event name and about a 40-bit subject or object identifier.
- **Node Reductions:** The second biggest source of compaction is node reduction, achieved using RNO and CCO optimizations. In addition, our design divides nodes into two types: base versions and subsequent versions. Base versions include attributes such as name, owner, command line, etc. New base versions are created only when these attributes change. Attribute values such as names and command lines tend to be reused across many nodes, so we encode them using compact IDs. This enables a base version to be stored in 32 bytes or less.
- **Compact Representation for Versions:** Subsequent versions derived from base versions don't store node attributes but just the starting and ending timestamps. By using relative timestamps and sticking to a 10ms timestamp granularity, we are able to represent a timestamp using 16 bits in most cases. This enables a version to fit within the same size as an edge and hence can be stored within the edge list of a base version. Specifically, let \( S \) be the set of edges occurring between a version \( v \) and the next version appearing in the edge list. Then \( S \) is the set of edges incident on version \( v \) in the graph.

Edge lists are maintained as vectors that can grow dynamically for active nodes (i.e., running processes and open files) but are frozen at their current size for inactive nodes. This technique, together with the technique of storing versions within the edge list, reduces fragmentation significantly. As a result, we achieve a very compact representation that often takes just a few bytes per edge in the original data.

### 6 Experimental Evaluation

We begin this section by summarizing our implementation in Section 6.1. The data sets used in our evaluation are described in Section 6.2. In Section 6.3, we evaluate the effectiveness of FD and SD in reducing the number of events and compare the results.