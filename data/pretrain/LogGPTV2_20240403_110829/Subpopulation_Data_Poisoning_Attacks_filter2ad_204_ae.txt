attacks on UTKFace + VGG-LL. For example, with 𝛼 = 1, the label
flipping attack achieves an average of 8.4% target damage, while
Gradient Optimization increases this to 15.5% target damage. With
influence, this undergoes another doubling to 38.6%. However, for
large models, the Gradient Optimization does not perform as well,
not improving the average performance on UTKFace + VGG-FT,
and often even decreasing it. This corroborates recent results in-
dicating that influence functions break down on deep models [2].
Because our attack is an approximation to influence functions, it is
natural that our attack is also brittle in this regime.
The running time of gradient optimization significantly outper-
forms influence. For VGG-LL, gradient optimization approach re-
turns poisoned data (from 50 iterations of optimization) in roughly
40 seconds, while influence takes on average 5.4 hours (to com-
plete 250 iterations, one eighth the number used in [30]). Although
unsuccessful, gradient optimization with VGG-FT takes roughly
55 seconds, while it timed out for influence (when provided with
a 5 minute timer for the first iteration, which would result in >1
day per attack). There is a clear tradeoff between running time
and performance when deciding between influence and gradient
optimization, although both break down at larger model sizes.
6 IMPROVING TARGETED ATTACKS USING
SUBPOPULATIONS
Here, we demonstrate that identifying subpopulations can be com-
plementary to targeted attacks—if an adversary’s goal is to target
a fixed set of 𝑘 inputs, inputs from a subpopulation will be easier
to target than arbitrarily selecting them. To illustrate this, we use
two state-of-the-art targeted attacks, Witches’ Brew [21] and influ-
ence functions [30]. Influence functions are powerful and assume a
large amount of adversarial knowledge, while Witches’ Brew is less
powerful but operates in a realistic threat model. In this section, we
keep the targeted attack algorithms unchanged, and only alter the
points they target. We show that, when the 𝑘 points are selected
from a subpopulation generated with ClusterMatch, they are
more easily targeted than when 𝑘 points are selected at random.
Influence function results can be found in Appendix D.
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3113Data + Model
UTKFace + VGG-LL
UTKFace + VGG-FT
Att
LF
GO
INF
LF
GO
Target Damage
𝛼 = 0.5
0.032
0.049
0.238
0.134
0.131
𝛼 = 1
0.084
0.155
0.386
0.235
0.182
𝛼 = 2
0.122
0.206
0.352
0.318
0.185
Table 8: Optimization Approaches for Poisoning Attack Gen-
eration: LF = Label Flipping, GO = Gradient Optimization,
INF = Influence (250 iterations). We select a subset of 10 sub-
populations to demonstrate the performance of the attack,
measuring mean performance across these subpopulations.
The attack performance nearly doubles for the small model
with GO, but decreases for large models. INF improves the
performance significantly, but is very slow.
6.1 Witches’ Brew [21]
We compare with the Witches’ Brew attack of Geiping et al. [21].
This attack optimizes poisoning points using a gradient matching
objective, and uses tools such as random restarts, differentiable
data augmentation, and ensembling, which allow for very efficient
targeted data poisoning attacks, which notably do not require fixing
the model initialization, a requirement of other prior work [60].
To demonstrate compatibility with subpopulations, we consider
CIFAR-10 and generate ClusterMatch subpopulations per-class
using a clean model, and run targeted attacks where 30 points are
chosen within a single subpopulation. We compare to a baseline of
30 randomly selected points from a single class. Subpopulations are
generated per-class as Witches’ Brew targets must have a uniform
class. We run 24 attacks for each target selection procedure, using an
ensemble size of 1, a budget of 1% (500 poison points), differentiable
data augmentation, 8 restarts, and 𝜀 = 16, which is comparable
to experiments in their Table 1. We report the fraction of targets
misclassified by Witches’ Brew in Table 9, averaging performance
over 8 models per trial. ClusterMatch-generated subpopulations
outperform randomly selected points by a factor of 2.35x, when
comparing the 10 most effective attacks. Over all 24 trials, the
average ClusterMatch-based attack performs 86% better than the
average random selection attack.
Selection
Random
ClusterMatch
Worst-1 Worst-5 Worst-10 Overall
0.072
0.300
0.951
0.134
0.153
0.382
0.114
0.264
Table 9: Effectiveness of Witches’ Brew targeted poisoning,
using arbitrary and ClusterMatch-based target selection.
7 IMPOSSIBILITY OF DEFENSES
We present here an impossibility result for defending against sub-
population attacks, based on a model of learning theory relying on
mixture models from [20]. Informally, a 𝑘-subpopulation mixture
distribution is a mixture of 𝑘 subpopulations of disjoint support,
with mixing coefficients 𝛼𝑖, 𝑖 ∈ {1, . . . , 𝑘}. A subpopulation mixture
learner is locally dependent if the learner makes local decisions
based only on subpopulations. We consider binary classifiers that
use the 0-1 loss. We use a simplified version of the [20] model to
prove the following result:
Theorem 7.1. For any dataset 𝐷 of size 𝑛 drawn from a 𝑘-
subpopulation mixture distribution, there exists a subpopulation poi-
soning attack 𝐷𝑝 of size ≤ 𝑛/𝑘 that causes all locally dependent
𝑘-subpopulation mixture learners A return A(𝐷 ∪ 𝐷𝑝) = A(𝐷)
with probability < 1/2. Additionally, if 𝛼 is the weight of the smallest
subpopulation in the mixture distribution, then a subpopulation attack
of size less than 2𝛼𝑛 suffices with probability at least 1− exp(−𝛼𝑛/2).
Essentially, if the learning algorithm makes subpopulation-wide
decisions, it will inherently be susceptible to subpopulation attacks.
[20] shows that this structure holds for 𝑘-nearest neighbors, mixture
models, overparameterized linear models, and suggests (based on
some empirical evidence) that it holds for neural networks as well.
This result becomes more interesting when 𝑘 is large, as this case
represents more diverse data and smaller attacks. In Appendix A.1,
we state our formal definitions, theorem statement, and proof.
However, there are ways that this negative result may not re-
flect practice. First, the specific learning algorithm and dataset may
avoid making subpopulation-wide decisions. Our attack results cor-
roborate this—not all subpopulations are easy to attack, and even at
a poisoning rate of 2, no subpopulations drop to 0% accuracy. Sec-
ond, our negative result only applies to purely algorithmic defenses
which do not involve any human input. A potential circumvention
for this would be to use a diverse, carefully annotated validation
dataset to identify ongoing subpopulation attacks. However, this
defense requires careful data collection and annotation, as well as
knowledge of the subpopulations that may be attacked. Ultimately,
there may be no substitute for careful data analysis.
8 EMPIRICAL ANALYSIS OF EXISTING
DEFENSES
Readers may wonder to what extent existing defenses for poison-
ing attacks can be used to protect from subpopulation attacks. We
consider both defenses for availability attacks and defenses for
backdoor attacks. Defenses for availability attacks ensure that poi-
soning does not compromise a model’s accuracy significantly. Mean-
while, subpopulation attacks do have a modest impact on accuracy,
focused on the target subpopulation. This makes using availabil-
ity attack defenses for protecting against subpopulation attacks a
promising approach. On top of this, the model classes our lower
bound applies to (k-NN, overparameterized linear models) are not
the same as those for which these availability attack defenses prov-
ably defend - TRIM [29] is designed for linear regression, ILTM [61]
is designed for generalized linear models (and is empirically suc-
cessful for neural networks), and SEVER has provable guarantees
for linear models and nonconvex models such as neural networks.
Backdoor defenses, such as activation clustering [11] and spectral
signatures [70] are designed to identify small anomalous subsets
of training data for large neural network models, and so may be
successful at identifying our subpopulation attacks. Other defenses
do not identify poisoned data, but simply postprocess models to
reduce the impact of poisoning. Liu et al. [42] propose fine-pruning,
which uses pruning and a fine tuning on a clean validation set to
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3114Subpop 1
Subpop 2
Subpop 3
Subpop 4
Subpop 5
𝛼
0.5
1.0
2.0
Orig TRIM SEVER Orig TRIM SEVER Orig TRIM SEVER Orig TRIM SEVER Orig TRIM SEVER
0.16
0.26
0.48
-0.07
0.06
0.26
-0.01
0.04
0.18
0.12
0.18
0.29
0.03
0.15
0.19
0.10
0.13
0.23
0.06
0.04
0.19
0.01
0.16
-0.02
-0.02
0.21
0.17
0.12
0.18
0.31
0.12
0.18
0.06
0.08
0.02
0.18
0.12
0.14
0.24
0.0
0.08
0.21
0.13
0.10
0.17
Table 10: Comparing standard training with TRIM and SEVER for the 5 most heavily damaged ClusterMatch subpopulations
on UTKFace + VGG-LL for each poisoning rate 𝛼. Orig indicates target damage of undefended training, TRIM is target damage
under TRIM, and SEVER is target damage under SEVER.
Data + Model
Defense
UTKFace + VGG-LL Activation Clustering
Spectral Signatures
Activation Clustering
Spectral Signatures
IMDB + BERT-FT
𝛼
1.0
1.0
2.0
2.0
% Found % Removed Target Dmg before Target Dmg after Collateral Dmg
100%
45.0%
44.7%
13.3%
25.7%
15%
31.1%
15%
0.222
0.222
0.506
0.506
0.188
0.290
0.477
0.517
3.8%
1.5%
1.3%
0.6%
Table 11: Effects of applying different backdoor poisoning defenses to models attacked with ClusterMatch on the subpopu-
lation with highest initial target damage for each model/dataset. % Found indicates what percentage of the poisons is correctly
identified by the mitigation, while % Removed shows the percentage of the training set actually removed. The table also lists
the target damage before and after the defense is used, and the collateral damage incurred with said defense.
remove the impact of poisoning. Rosenfeld et al. [55] propose an
approach to certify when a linear regression model’s predictions
can not be changed by a small number of poisoning points. In this
section, we show that all seven of these common existing defenses
can fail against subpopulation attacks.
Availability and backdoor defenses. When extended to generic
models, TRIM and ILTM are equivalent, so we use TRIM to describe
both. TRIM/ILTM (Algorithm 4 in the Appendix) and SEVER both
use an outlier score based on loss (for TRIM/ILTM) or gradient statis-
tics (SEVER), and iteratively identify and remove outliers. For both,
we use a maximum iteration count of 𝑇 = 5, due to the significant
training time of our models, and set the attack count to be exactly
the number of poisoning points, to elicit the defense’s best possi-
ble performance. Figure 2 in Appendix A.2 describes a synthetic
data which demonstrates a failure mode of TRIM/ILTM/SEVER. For
some datasets, defenses can be misled to amplify poisoning.
We present the performance of availability attack defenses in Ta-
ble 10, and backdoor defenses in Table 11. We evaluate on UTKFace
+ VGG-LL and IMDB + BERT-LL. TRIM works fairly well for the
two most impacted subpopulations on UTKFace, decreasing target
damage from 48% and 31% target damage to −2% and 6% target dam-
age at 𝛼 = 2. However, for many of these attacks, the target damage
stays roughly the same or even increases. For example, the second
most impacted subpopulation with 𝛼 = .5 has the target damage
maintained at 12%, and the fourth most impacted subpopulation
with 𝛼 = 2 has target damage increase from 24% to 26%. SEVER is
similarly inconsistent. On IMDB, we run TRIM for the most dam-
aged subpopulations at 𝛼 = 2, finding no reduction in target damage
on BERT-LL, and only a ∼ 8% target damage reduction on BERT-FT.
Backdoor defenses struggle, as well. Despite identifying all attack
points on UTKFace, activation clustering removes a full 25.7% of
the training set as well, impacting performance enough that target
damage is fairly unchanged. Spectral signatures has little effect on
both datasets, as well.
Postprocessing defenses. Fine-pruning [42] works in two phases.
First it prunes the last convolutional layer of the model until a test