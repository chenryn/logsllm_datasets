CREATE TRIGGER  
Time: 1.531 ms  
digoal=> create trigger tg_user_download_log_insert after insert on user_download_log for each row execute procedure tg_user_download_log_insert();  
CREATE TRIGGER  
Time: 1.527 ms  
digoal=> create trigger tg_user_download_log_delete after delete on user_download_log for each row execute procedure tg_user_download_log_delete();  
CREATE TRIGGER  
Time: 1.484 ms  
```  
清除数据, 重新插入 :   
```  
digoal=> truncate user_download_log ;  
TRUNCATE TABLE  
Time: 6.216 ms  
```  
在新增的表上对order by字段创建索引.  
```  
digoal=> create index idx_user_download_uk on user_download_uk (get_time);  
CREATE INDEX  
Time: 64.599 ms  
```  
插入数据 :   
```  
digoal=> insert into user_download_log select generate_series(0,100000),generate_series(0,100000),generate_series(0,100000),generate_series(clock_timestamp(),clock_timestamp()+interval '100000 min',interval '1 min'), 'this is test';  
INSERT 0 100001  
Time: 4911.241 ms  
```  
显然插入速度有明显下降, 这是这种方法的弊端, 需要权衡这个表的DML操作和DQL操作的比例来选择是否要用这种方法进行优化.  
```  
digoal=> select * from user_download_uk order by get_time desc limit 10;  
 user_id | listid | apkid  |      get_time         
---------+--------+--------+---------------------  
  100000 | 100000 | 100000 | 2012-11-23 01:20:11  
   99999 |  99999 |  99999 | 2012-11-23 01:19:11  
   99998 |  99998 |  99998 | 2012-11-23 01:18:11  
   99997 |  99997 |  99997 | 2012-11-23 01:17:11  
   99996 |  99996 |  99996 | 2012-11-23 01:16:11  
   99995 |  99995 |  99995 | 2012-11-23 01:15:11  
   99994 |  99994 |  99994 | 2012-11-23 01:14:11  
   99993 |  99993 |  99993 | 2012-11-23 01:13:11  
   99992 |  99992 |  99992 | 2012-11-23 01:12:11  
   99991 |  99991 |  99991 | 2012-11-23 01:11:11  
(10 rows)  
Time: 0.360 ms  
```  
使用这种优化方法得到的查询速度是原始SQL的271倍.  
## 小结  
1\. 使用子查询降低 group by 的数据量.   
使用这种优化方法得到的查询速度是原始SQL的49倍.  
弊端, 不好评估子查询中到底要限制多少行, 才能得到10条唯一的a.skyid, a.giftpackageid, a.giftpackagename, a.intimestamp. 如果评估少了得不到10条最终结果, 如果评估多了又费性能.  
2\. 使用游标和临时表.  
使用这种优化方法得到的查询速度是原始SQL的54倍.  
使用这种方法应该是比较折中的, 但是需要规划好临时表 .   
3\. 在tbl_anc_player_win_log表上建立触发器, 用另一张表来记录唯一的a.skyid, a.giftpackageid, a.giftpackagename, a.intimestamp.  
使用这种优化方法得到的查询速度是原始SQL的271倍.  
使用这种方法得到的提升最明显, 但是对DML的性能影响也是非常大的, 需要权衡利弊.  
## 补充1  
1\.  使用游标的例子, 还有优化的空间, 那就是把临时表去掉. 使用array来存储过往的记录. 如下 :   
需要创建一个返回结果类型, 因为函数中不允许定义record[]类型的数组. v_result_array record[];  
定义返回结果类型, 还有一个好处是使得查询更加简单了.  
```  
digoal=> create type typ_user_download_log as (user_id int, listid int, apkid int, get_time timestamp(0));  
CREATE TYPE  
```  
函数如下 :   
```  
create or replace function get_user_download_log(i_limit int) returns setof typ_user_download_log as $$  
declare  
  v_result typ_user_download_log;  
  v_query refcursor;  
  v_limit int := 0;  
  v_result_array typ_user_download_log[];  
begin  
  open v_query for select user_id,listid,apkid,get_time from user_download_log order by get_time desc;  
  loop  
    fetch v_query into v_result;  
    if ( v_result = ANY(v_result_array) ) then  
    else  
      if v_limit >= i_limit then  
        exit;  
      end if;  
      v_result_array := array_append(v_result_array, v_result);  
      return next v_result;  
      v_limit := v_limit + 1;  
    end if;  
  end loop;  
  close v_query;  
  return;  
end;  
$$ language plpgsql;  
```  
插入几条重复记录, 看看是否能正常过滤.  
```  
digoal=> select ctid,* from user_download_log order by get_time desc limit 10;  
   ctid   | user_id | listid | apkid  |      get_time       |  otherinfo     
----------+---------+--------+--------+---------------------+--------------  
 (833,41) |  100000 | 100000 | 100000 | 2012-11-23 01:20:11 | this is test  
 (833,40) |   99999 |  99999 |  99999 | 2012-11-23 01:19:11 | this is test  
 (833,39) |   99998 |  99998 |  99998 | 2012-11-23 01:18:11 | this is test  
 (833,38) |   99997 |  99997 |  99997 | 2012-11-23 01:17:11 | this is test  
 (833,37) |   99996 |  99996 |  99996 | 2012-11-23 01:16:11 | this is test  
 (833,36) |   99995 |  99995 |  99995 | 2012-11-23 01:15:11 | this is test  
 (833,35) |   99994 |  99994 |  99994 | 2012-11-23 01:14:11 | this is test  
 (833,34) |   99993 |  99993 |  99993 | 2012-11-23 01:13:11 | this is test  
 (833,33) |   99992 |  99992 |  99992 | 2012-11-23 01:12:11 | this is test  
 (833,32) |   99991 |  99991 |  99991 | 2012-11-23 01:11:11 | this is test  
(10 rows)  
Time: 0.712 ms  
```  
插入前10条重复数据.  
```  
digoal=> insert into user_download_log select * from user_download_log order by get_time desc limit 10;  
INSERT 0 10  
Time: 2.322 ms  
digoal=> select ctid,* from user_download_log order by get_time desc limit 10;  
   ctid   | user_id | listid | apkid  |      get_time       |  otherinfo     
----------+---------+--------+--------+---------------------+--------------  
 (833,41) |  100000 | 100000 | 100000 | 2012-11-23 01:20:11 | this is test  
 (833,43) |  100000 | 100000 | 100000 | 2012-11-23 01:20:11 | this is test  
 (833,40) |   99999 |  99999 |  99999 | 2012-11-23 01:19:11 | this is test  
 (833,44) |   99999 |  99999 |  99999 | 2012-11-23 01:19:11 | this is test  
 (833,39) |   99998 |  99998 |  99998 | 2012-11-23 01:18:11 | this is test  
 (833,45) |   99998 |  99998 |  99998 | 2012-11-23 01:18:11 | this is test  
 (833,38) |   99997 |  99997 |  99997 | 2012-11-23 01:17:11 | this is test  
 (833,46) |   99997 |  99997 |  99997 | 2012-11-23 01:17:11 | this is test  
 (833,37) |   99996 |  99996 |  99996 | 2012-11-23 01:16:11 | this is test  
 (833,47) |   99996 |  99996 |  99996 | 2012-11-23 01:16:11 | this is test  
(10 rows)  
Time: 0.649 ms  
```  
查询, 得到正确的结果  
```  
digoal=> select * from get_user_download_log(10);  
 user_id | listid | apkid  |      get_time         
---------+--------+--------+---------------------  
  100000 | 100000 | 100000 | 2012-11-23 01:20:11  
   99999 |  99999 |  99999 | 2012-11-23 01:19:11  
   99998 |  99998 |  99998 | 2012-11-23 01:18:11  
   99997 |  99997 |  99997 | 2012-11-23 01:17:11  
   99996 |  99996 |  99996 | 2012-11-23 01:16:11  
   99995 |  99995 |  99995 | 2012-11-23 01:15:11  
   99994 |  99994 |  99994 | 2012-11-23 01:14:11  
   99993 |  99993 |  99993 | 2012-11-23 01:13:11  
   99992 |  99992 |  99992 | 2012-11-23 01:12:11  
   99991 |  99991 |  99991 | 2012-11-23 01:11:11  
(10 rows)  
Time: 0.809 ms  
```  
使用这种优化方法得到的查询速度是原始SQL的121倍.   
## 补充2  
以下是延展出来的方法, 使用递归调用, 也可以达到同样的优化目的, 但是. 以下测试的group by 只用到了一个字段. 而上面的例子用到了多个字段group by .  
测试表 :   
```  
CREATE TABLE test (  
    username TEXT,  
    some_ts timestamptz,  
    random_value INT4  
);  
```  
测试数据 :   
```  
INSERT INTO test (username, some_ts, random_value)  
SELECT  
    'user #' || cast(floor(random() * 10) as int4),  
    now() - '1 year'::INTERVAL * random(),  
    cast(random() * 100000000 as INT4)  
FROM  
    generate_series(1,2000000);  
```  
优化将用到这个索引 :   
```  
CREATE INDEX i on test (username, some_ts);  
```  
分析表 :   
```  
analyze test;  
```  
测试数据分布 :   
```  
SELECT  
    username,  
    count(*)  
FROM test  
group by username  
order by username;  
 username │ count  
──────────┼────────  
 user #0  │ 199871  
 user #1  │ 199939  
 user #2  │ 200388  
 user #3  │ 199849  
 user #4  │ 200329  
 user #5  │ 199504  
 user #6  │ 199903  
 user #7  │ 200799  
 user #8  │ 199487  
 user #9  │ 199931  
(10 rows)  
```  
未优化的SQL :   
```  
select username,some_ts,random_value from (select row_number() over (partition by username order by some_ts desc) as rownum , * from test) as t where t.rownum<6;  
```  
执行时间 :   
```  
 username |            some_ts            | random_value   
----------+-------------------------------+--------------  
 user #0  | 2012-10-08 10:26:38.561924+08 |     44572919  
 user #0  | 2012-10-08 10:26:28.625924+08 |      5466578  
 user #0  | 2012-10-08 10:21:18.277124+08 |     32176884  
 user #0  | 2012-10-08 10:16:49.227524+08 |     81763617  
 user #0  | 2012-10-08 10:15:49.611524+08 |      9824604  