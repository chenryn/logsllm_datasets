### Entropy and Mutual Information in Log Analysis

#### Joint Entropy
The joint entropy \( H(X, Y) \) of two random variables \( X \) and \( Y \) is defined as:
\[ H(X, Y) = -\sum_{i,j} p(x_i, y_j) \log_2 p(x_i, y_j) \]

#### Independence
- **Independent Variables \( X \) and \( Y \):**
  - Knowing \( X \) tells us nothing about \( Y \).
  - No matter what value of \( x \) we fix, the histogram of \( Y \)'s values co-occurring with that \( x \) will have the same shape.
  - Mathematically, \( H(X, Y) = H(X) + H(Y) \).

- **Dependent Variables \( X \) and \( Y \):**
  - Knowing \( X \) tells us something about \( Y \) (and vice versa).
  - The histograms of \( y \) values co-occurring with a fixed \( x \) will have different shapes.
  - Mathematically, \( H(X, Y) < H(X) + H(Y) \).

### Mutual Information
- **Conditional Entropy of \( Y \) given \( X \):**
  \[ H(Y|X) = H(X, Y) - H(X) \]
  - This represents the remaining uncertainty about \( Y \) once we know \( X \).

- **Mutual Information of \( X \) and \( Y \):**
  \[ I(X; Y) = H(X) + H(Y) - H(X, Y) \]
  - This measures the reduction in uncertainty about \( X \) once we know \( Y \) and vice versa.

### Outline
1. **Log Browsing Moves:**
   - Pipes and tables
   - Trees are better than pipes and tables!

2. **Data Organization:**
   - Defining the browsing problem
   - Entropy
   - Measuring co-dependence
   - Mutual Information
   - The tree building algorithm

3. **Examples:**

### Building a Data View
1. **Pick the feature with the lowest non-zero entropy** ("simplest histogram").
2. **Split all records** on its distinct values.
3. **Order other features** by the strength of their dependence with the first feature (using conditional entropy or mutual information).
4. **Use this order to label groups**.
5. **Repeat** with the next feature in step 1.

### Examples
- **Histograms 3D: Feature Pairs, Port Scan**
  - \( H(Y|X) = 0.76 \)
  - \( H(Y|X) = 2.216 \)
  - \( H(Y|X) = 0.39 \)
  - \( H(Y|X) = 3.35 \)

- **Snort Port Scan Alerts:**
  - One ISP, 617 lines, 2 users, one tends to mistype.
  - 11 lines of screen space.

- **Comparing 2nd Order Uncertainties:**
  - Protocol group:
    - Destination: \( H = 2.9999 \)
    - Source: \( H = 2.8368 \)
    - Info: \( H = 2.4957 \)
  - "Start with the simpler view."

- **Screenshots:**
  - Screenshot (1)
  - Screenshot (2)
  - Screenshot (3)

### Research Links
- **Research on using entropy and related measures for network anomaly detection:**
  - *Information-Theoretic Measures for Anomaly Detection*, Wenke Lee & Dong Xiang, 2001
  - *Characterization of Network-Wide Anomalies in Traffic Flows*, Anukool Lakhina, Mark Crovella & Christophe Diot, 2004
  - *Detecting Anomalies in Network Traffic Using Maximum Entropy Estimation*, Yu Gu, Andrew McCallum & Don Towsley, 2005

### Summary
- Information theory provides useful heuristics for:
  - Summarizing log data in medium-size batches.
  - Choosing data views that highlight interesting features of a particular batch.
  - Finding good starting points for analysis.
- Even with the simplest data organization tricks, these methods are helpful.
- In one sentence: \( H(X) \), \( H(X|Y) \), \( I(X; Y) \), etc., are parts of a complete analysis kit!

### Source Code and Documentation
- For source code (GPL), documentation, and technical reports, visit:
  - [http://kerf.cs.dartmouth.edu](http://kerf.cs.dartmouth.edu)

---

**Sergey Bratus**
*Entropy Tricks for Browsing Logs and Packet Captures*