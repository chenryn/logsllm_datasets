119
100%
4.00K
119
1
476K
names_cache
5962
5226
75x
0.06K
118
---
## Page 35
我的服务器内存去哪儿了我的服务器内存去哪儿了
5.每个 socket的inode 也不一样。
[rootC%]#11-i/proc/22360/task//fd/1grep socket
17813792 1rwx-
1rootroot648月
1rootroot648月
7 17:12 1θ -> socket:[6582593]
17013799 1rwx
7 17:12 17-> socket:[9175]
178138e8 1rwx
17013801 1rwx-
1rootroot648月
1root
root
64
8月
7 17:12 19 -> socket:[6582599]
7 17:12 18
->
socket:[9197]
17013802 1rwx
1root
1001
64
8月
7 17:12 28 -> socket:[6582643]
17813787
1rwx-
1
root
root
64
8月
17013788 1rwx-
1rootroot648月
7 17:12 6 -> s0cket:[6582588]
17813789 1rwx-
1 root
1001
64
8月
7 17:12 8-> socket:[6582590]
7 17:12 7
socket:[6582589]
17813798 1rwx-
1root root 64
1rootroot648月
8月
17013791 1rwx-
17813813
1rwx
root
root
7 17:12 9 -> 80cket:[6582591]
17813828 1rwx
1rootroot 648月
64
8月
7 17:12 17 -> socket:[9175]
7 17:12
18
socket:[6582593]
17013821 1rwx
1 root root 64
8月
7 17:12 18 -> socket:[9197]
17813822 1rwx
1root
root 64
8月
7 17:12 19
->
1 rootroot 648月
7 17:12 20 -> socket:[6582643]
socket:[6582599]
17013823 1rwx
170138e8 1rwx-
1
root
root
64
8月
7 17:12 5 -> socket:[6582587]
178138e9 1rwx-
1 rootroot 648月
1
8月
7 17:12 6
-> socket:[6582588]
17013810 1rwx
17013811 1rwx
root
root 64
7 17:12 7 -> s0cket:[6582589]
1 rootroot 648月
8月
7 17:12 9 -> socket:[6582591]
7 17:12 8
=>
socket:[6582590]
17813812 1rwx
17013834 1rwx
1 root
root
64
8月
7 17:12 10 -> socket:[6582593]
17813841 1rwx
1
root
root 64
8月
7 17:12 18 ->
7 17:12
17
->
socket:[9175]
17813842 1rwx
1rootroot 648月
socket:[9197]
17013843 1rwx
root
1001
64
8月
7 17:12 28 -> socket:[6582643]
7 17:12 19
->
17813844 1rwx
root
root648月
root 64
17013829 1rwx
1 root
8月
17013838
1rwx-
1001
root 64
7 17:12 5 -> s0cket:[6582587]
17013831 1rwx
1
8月
7 17:12
6
 socket:[6582590]
17013833 1rwx=
1 rootroot648月
7 17:129->s0cket:[6582591]
当时看到的现场有几万个fd，基本全是socket，每个inode 都是占用空间的，
且 proc文件系统是全内存的。所以我们才会看到 slab 中proc_inode_cache
内存占用高。
建议：
建议用户需要从程序上优化相关的server端-
---
## Page 37
CPU占用不高但网路性能很差的一个原因CPU占用不高但网络性能很差的一个原因
什么是RPS/RFS
RPS（Receive PacketSteering）主要是把软中断的负载均衡到各个 cpu，简单来
说，是网卡驱动对每个流生成一个hash标识，这个HASH值得计算可以通过四元
组来计算（SIP，SPORT，DIP，DPORT），然后由中断处理的地方根据这个hash
标识分配到相应的CPU上去，这样就可以比较充分的发挥多核的能力了。通俗点来
说就是在软件层面模拟实现硬件的多队列网卡功能，如果网卡本身支持多队列功能的
话RPS就不会有任何的作用。该功能主要针对单队列网卡多CPU环境，如网卡支
持多队列则可使用 SMP irqaffinity直接绑定硬中断。
CPU0 : APP0
DATA1
CPU1 : APP1
DATA8
NIC
CPU2 : APP2
DATA0
DATA3
CPU8 : APP8
图1只有RPS的情况下（来源网络）
由于RPS只是单纯把数据包均衡到不同的cpu，这个时候如果应用程序所在的cpu
和软中断处理的cpu不是同一个，此时对于cpu cache 的影响会很大，那么RFS
（Receive flow steering）确保应用程序处理的 cpu 跟软中断处理的 cpu 是同一个，
这样就充分利用cpu的cache，这两个补丁往往都是一起设置，来达到最好的优化
效果，主要是针对单队列网卡多CPU环境。
---
## Page 39
CPU占用不高但网络性能很差的一个原因CPU占用不高但网络性能很差的一个原因
c)处理中断的 cru 总是会变，导致了更多的context avitche
dl也存在一些情况，启动了irgbalance，但是并没有生效，没有真正去设置处理中断的cpue
如何查看网卡的队列数
1
Combined代表队列个数，说明我的测试机有4个队列。
# ethtcol -1 etho
Channel parsneters for eth0:
Pre-set maximums:
RX:
TK:
Other :
Corbined:
Current hardvare setting8:
RK :
TK:
Other:
Combined :
0
以CentOS 7.6 为例，系统处理中断的记录在/proc/interrupts 文件里面，默认
这个文件记录比较多，影响查看，同时如果cpu核心也非常多的话，对于阅读
的影响非常大。
# cat /proc/interrupt8
0080
CPU1
CPU2
CPU3
0 :
141
0
10-APIC-edge
timer
1:
10
0
0
I0-APIC-edge
10-APIC-edge
i8042
4 :
807
0
10-APIC-edge
seria]
6 :
floppy
8:
0
0
0
I0-APIC-edge
rtco
9:
0
0
I0-APIC-fasteoi
acpi
1.0 :
0
0
0
0
I0-APIC-fasteoi
virtio3
11:
22
0
0
IO-APIC-fasteoi
uhci_
hed: usb1
12:
15
0
0
10-APIC-edge
i8042
14 :
0
0
0
I0-APIC-edge
at.s_
piix
15 :
D
I0-APIC-edge
ats_
piix
24:
0
0
PCI-MSI-edge
virtiol-config
---
## Page 41