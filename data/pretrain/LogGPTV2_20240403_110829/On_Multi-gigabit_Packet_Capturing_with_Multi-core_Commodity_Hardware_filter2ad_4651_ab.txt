# On Multi-Gigabit Packet Capturing with Multi-Core Commodity Hardware

## 4.2 Parallel Setup
In this scenario, each hardware queue is associated with its own user-space thread, ensuring that the processing paths of packets are completely parallel. We used PF_RING with the recently introduced quick mode option, which eliminates per-queue locks. The results, shown in Figure 4, indicate that although PF_RING achieves good performance by preventing locking, PFQ still outperforms it. Additionally, PFQ exhibits consistent behavior with both vanilla and aware drivers (except for a scale factor), while PF_RING scales well only with aware drivers. 

Notably, PFQ can capture all incoming packets with 10 cores, and its throughput stabilizes because there is no additional traffic to capture. However, our traffic generator cannot produce more input traffic, limiting us to a lower bound of PFQ's performance.

Figure 5 shows CPU utilization (for aware drivers). While PF_RING saturates the CPU, the global CPU consumption for PFQ remains roughly constant and well below 20%.

## 4.3 Multiple Capture Sockets
One of PFQ's strengths is its ability to decouple parallelism between the application and kernel levels. In this set of tests, we measure the performance of this feature by using the maximum number of available contexts in the kernel (i.e., 12) and varying the number of parallel user-space threads.

First, we report the overall throughput when incoming packets are load-balanced across the application threads. For benchmarking, we compare our results with those of PF_RING using the recently introduced RSS rehash functionality. However, the balancing functionality in PF_RING differs slightly from that of PFQ. The results, shown in Figure 6, indicate that with an aware driver, PFQ can capture all incoming traffic with just 3 user-space threads. With a vanilla driver, the behavior is the same, but the overall throughput is lower.

We also evaluate a scenario where multiple applications request a copy of the same packet. The results, shown in Figure 7, display the cumulative number of packets brought to user space. This graph should ideally scale linearly as the same traffic is copied to more threads, but the overhead of copying and concurrent access to socket queues impacts performance, especially at high copy numbers. However, such a large number of copies is unlikely in practical setups. This figure also provides an upper bound on the number of packets the system can process with a faster driver or multiple capturing cards: PFQ can enqueue and make available over 42 Mpps, significantly outperforming both competitors.

## 5 Conclusions
In this paper, we presented PFQ, a novel packet capturing engine that allows flexible decoupling of user-space and kernel-space parallelism with negligible performance overhead. Thanks to a carefully designed multi-core architecture, PFQ outperforms its competitors in all tested use cases. Future work includes developing a flexible framework for application-aware packet steering and classification to be integrated within the engine architecture.

## Acknowledgments
The authors thank Luca Deri for his support towards this research. This work was partially supported by the Italian project IMPRESA and the EU project DEMONS (contract-no. 257315). The views and conclusions contained herein are those of the authors and do not necessarily represent the official policies or endorsements of the DEMONS project or the European Commission.

## References
1. http://netserv.iet.unipi.it/software/pfq/
2. Deri, L.: ncap: wire-speed packet capture and transmission. In: End-to-End Monitoring Techniques and Services on 2005, pp. 47–55. IEEE Computer Society, Washington, DC (2005)
3. Rizzo, L.: http://info.iet.unipi.it/~luigi/netmap/
4. Deri, L.: http://www.ntop.org
5. Libpcap MMAP mode on Linux Phil Woods, http://public.lanl.gov/cpw/
6. Fusco, F., Deri, L.: High speed network traffic analysis with commodity multi-core systems. In: IMC 2010, pp. 218–224 (2010)
7. Egi, N., Greenhalgh, A., Handley, M., Hoerdt, M., Huici, F., Mathy, L., Papadimitriou, P.: Forwarding path architectures for multicore software routers. In: Proc. of PRESTO 2010, pp. 3:1–3:6. ACM, New York (2010)
8. Kohler, E., Morris, R., Chen, B., Jannotti, J., Frans Kaashoek, M.: The Click modular router. ACM Trans. Comput. Syst. 18, 263–297 (2000)
9. Dobrescu, M., Egi, N., Argyraki, K., Chun, B., Fall, K., Iannaccone, G., Knies, A., Manesh, M., Ratnasamy, S.: Routebricks: exploiting parallelism to scale software routers. In: ACM SIGOPS, pp. 15–28. ACM, New York (2009)
10. Han, S., Jang, K., Park, K., Moon, S.: PacketShader: a GPU-accelerated software router. In: Proceedings of the ACM SIGCOMM 2010 Conference on SIGCOMM, SIGCOMM 2010, pp. 195–206. ACM, New York (2010)
11. Han, S., Jang, K., Park, K., Moon, S.: Building a single-box 100 Gbps software router. In: IEEE LANMAN (2010)
12. Bonelli, N., Di Pietro, A., Giordano, S., Procissi, G.: Flexible high-performance traffic generation on commodity multi-core platforms. In: To appear in Traffic Monitoring and Analysis (TMA 2012) Workshop (2012)
13. Bonelli, N., Di Pietro, A., Giordano, S., Procissi, G.: Packet capturing on parallel architectures. In: IEEE Workshop on Measurements and Networking (2011)