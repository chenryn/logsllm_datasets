generator can provide (although this does not always correspond to the nominal
maximum bit rate).
)
s
/
s
t
k
p
(
t
e
a
R
g
n
i
r
u
p
a
C
t
On Multi–gigabit Packet Capturing with Multi–core Commodity Hardware
71
PFQ - Aware Driver
PFQ - Vanilla
PCAP - Vanilla
PF_RING - Aware Driver
PF_RING - Vanilla
1.2x107
1x107
8x106
6x106
4x106
2x106
1x1010
8x109
6x109
4x109
2x109
)
s
p
b
(
e
t
a
R
g
n
i
r
u
t
p
a
C
Theoretical Bound
Generated Trafﬁc
PFQ - Driver Aware
PCAP
PF_RING Aware Driver
2
4
Number of HW Queues
6
8
10
200
400
600
800
1000
1200
1400
Packet Size
Fig. 2. One capturing thread
Fig. 3. Throughput vs. Packet Size
PFQ - Aware Driver
PFQ - Vanilla
PF_RING - Aware Driver
PF_RING - Vanilla
1.2x107
1x107
8x106
6x106
4x106
2x106
)
s
/
s
t
k
p
(
e
t
a
R
g
n
i
r
u
t
p
a
C
2
4
Number of Threads/HW Queues
6
8
)
%
(
d
a
o
L
e
g
a
r
e
v
A
U
P
C
80
60
40
20
0
10
12
2
PF_RING - Aware Driver
PFQ - Aware Driver
4
Number of Threads/HW Queues
6
8
10
12
Fig. 4. Completely parallel processing
paths
Fig. 5. Completely parallel processing
paths: CPU consumption
4.2 Parallel Setup
In this scenario each hardware queue is associated with its own user space thread,
so that the processing paths of packets are completely parallel. Notice that in
this scenario we used PF RING with the recently introduced quick mode op-
tion, which allows avoiding per–queue locks. The results are shown in Figure
4 and show that, although PF RING manages to achieve good performance by
preventing locking, PFQ still outperforms it. Besides, PFQ shows the same be-
havior with both vanilla and aware drivers (apart from a scale factor), while
PF RING only scales well with aware drivers. Notice that PFQ is able to cap-
ture all of the incoming packets with 10 cores (its throughput steadies because
there is no additional traﬃc to capture); unfortunately, our generator is not able
to produce more input traﬃc and, therefore, we can only obtain a lower bound
of PFQ’s performance.
We also report the CPU utilization (in the case of aware drivers) in Figure 5:
while PF RING saturates the CPU, the global CPU consumption in the case of
PFQ is roughly constant and well below 20%.
72
N. Bonelli et al.
)
s
/
s
t
k
p
(
t
e
a
R
g
n
i
r
u
p
a
C
t
1.2x107
1x107
8x106
6x106
4x106
2x106
PFQ - Aware Driver
PFQ - Vanilla
PF_RING - Aware Driver
PF_RING - Vanilla
2
4
6
8
Number of Threads
10
12
)
s
/
s
t
k
p
(
e
a
R
t
t
t
e
k
c
a
P
e
a
g
e
r
g
g
A
PFQ
PCAP
PF_RING
4x107
3x107
2x107
1x107
0
2
4
6
8
Number of Threads
10
12
Fig. 6. Load balancing across a variable
number of user–space threads
Fig. 7. Copying traﬃc to a variable
number of user–space threads
4.3 Multiple Capture Sockets
Besides high performance, one of the strengths of PFQ is the ability of decou-
pling parallelism between the application level and the kernel level. In this set
of tests we measure the performance of such a feature by always using the max-
imum number of available contexts in the kernel (i.e. 12) and by varying the
number of parallel user–space threads. First, we report the overall throughput
when incoming packets are load–balanced across the application threads. In or-
der to have a benchmark, we compare our result with that of PF RING using
the recently introduced RSS rehash functionality. However, the balancing func-
tionality in PF RING slightly diﬀer from that of PFQ. The results are reported
in Figure 6 and show that, with an aware driver, PFQ is able to capture all of
the incoming traﬃc with just 3 user–space threads while, with a vanilla driver,
the behavior is the same but the overall throughput is lower.
We also evaluate a scenario where multiple applications are requesting a copy
of the same packet: the results are shown in Figure 7 and show the cumulative
number of packets brought to user–space. In this case, we also show the results for
PCAP. Ideally this graph should scale linearly, as the same traﬃc is being copied
to more and more threads; however the overhead of copy and concurrent access
to the socket queues has a relevant impact on performance when the number of
copies is high. Notice, however, that such a large number of copies is unlikely in
a practical setup. Interestingly, this ﬁgure also provides an upper bound of the
number of packets the system may be able to process with a faster driver with no
allocations or multiple capturing cards: PFQ is able to enqueue and make available
to user space over 42 Mpps, thus outperforming by far both competitors.
5 Conclusions
In this paper we presented PFQ, a novel packet capturing engine that allows to
ﬂexibly decouple user space and kernel space parallelism with negligible perfor-
mance overhead. Thanks to a careful multi–core aware design, PFQ outperforms
On Multi–gigabit Packet Capturing with Multi–core Commodity Hardware
73
its competitors in all of the diﬀerent use cases that we used for testing. In the
future, we plan to develop a ﬂexible framework for application–aware packet
steering and classiﬁcation to be integrated within the engine architecture.
Acknowledgments. The authors wish to thanks Luca Deri for his support
towards this research. This work was partially supported by the Italian project
IMPRESA and by EU project DEMONS (contract-no. 257315). The views and
conclusions contained herein are those of the authors and should not be inter-
preted as necessarily representing the oﬃcial policies or endorsements, either
expressed or implied, of the DEMONS project or the European Commission.
References
1. http://netserv.iet.unipi.it/software/pfq/
2. Deri, L.: ncap: wire-speed packet capture and transmission. In: End-to-End Moni-
toring Techniques and Services on 2005, pp. 47–55. IEEE Computer Society, Wash-
ington, DC (2005)
3. Rizzo, L.: http://info.iet.unipi.it/~luigi/netmap/
4. Deri, L.: http://www.ntop.org
5. Libpcap MMAP mode on linux Phil Woods, http://public.lanl.gov/cpw/
6. Fusco, F., Deri, L.: High speed network traﬃc analysis with commodity multi-core
systems. In: IMC 2010, pp. 218–224 (2010)
7. Egi, N., Greenhalgh, A., Handley, M., Hoerdt, M., Huici, F., Mathy, L., Papadim-
itriou, P.: Forwarding path architectures for multicore software routers. In: Proc.
of PRESTO 2010, pp. 3:1–3:6. ACM, New York (2010)
8. Kohler, E., Morris, R., Chen, B., Jannotti, J., Frans Kaashoek, M.: The click
modular router. ACM Trans. Comput. Syst. 18, 263–297 (2000)
9. Dobrescu, M., Egi, N., Argyraki, K., Chun, B., Fall, K., Iannaccone, G., Knies, A.,
Manesh, M., Ratnasamy, S.: Routebricks: exploiting parallelism to scale software
routers. In: ACM SIGOPS, pp. 15–28. ACM, New York (2009)
10. Han, S., Jang, K., Park, K., Moon, S.: Packetshader: a gpu-accelerated software
router. In: Proceedings of the ACM SIGCOMM 2010 Conference on SIGCOMM,
SIGCOMM 2010, pp. 195–206. ACM, New York (2010)
11. Han, S., Jang, K., Park, K., Moon, S.: Building a single-box 100 gbps software
router. In: IEEE LANMAN (2010)
12. Bonelli, N., Di Pietro, A., Giordano, S., Procissi, G.: Flexible high performance
traﬃc generation on commodity multi-core platforms. In: To appear in Traﬃc
Monitoring and Analysis (TMA 2012) Workshop (2012)
13. Bonelli, N., Di Pietro, A., Giordano, S., Procissi, G.: Packet capturing on parallel
architectures. In: IEEE Workshop on Measurements and Networking (2011)