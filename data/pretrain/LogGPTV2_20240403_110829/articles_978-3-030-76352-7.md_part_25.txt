158 X. Liu et al.
embeddingusingtheWAdataiscomparabletotheoriginalgeneral-purposefast-
Textembedding,wegetasignificantboostinaccuracyof8%withourpre-trained
fastText embedding using both the WA data and the Loghub data, compared
with the general-purpose fastText embeddings. This demonstrates the effective-
nessofusingdiversifieddomain-specificdataforpre-trainingembeddingfeatures
to enhance the log anomaly detection task.
Ontheotherhand,wecanseethatourpre-trainedBERTembeddingwiththe
WA data is also comparable to the original general-purpose BERT embedding,
thoughbothareslightlyworsethanthebaseline.Surprisingly,whenweintroduce
more diverse data from both the WA data and the Loghub data to pre-train
BERT,theaccuracydropsconsiderably.Onepossiblereasonisthatlogdataare
much simpler than the general purpose text data such as Wikipedia articles or
news articles, and the context in log data is not rich enough for BERT to learn
during model pre-training. Besides, since log anomaly detection is essentially an
unsupervised learning task, it is difficult to fine-tune pre-trained BERT models
as in supervised learning tasks.
Overall,ourexperimentsshowthatcontext-freeembeddingsaremorerobust
and effective than contextualized embeddings to pre-train features for the log
anomaly detection task, and it is even better when pre-training the embeddings
with the IT Operations domain data of diverse applications.
Table 1. Accuracy results on log anomaly detection with pre-trained features from
embeddings.
Model HDFS WA-1 WA-2 Average
Normal Abnormal Normal Abnormal Normal
Baseline 99.99% 44.9% 93.3% 66.7% 95% 80%
fastText-origin 98.8% 66% 93.3% 100% 98.3% 91%
fastText-wa 98.8% 59.7% 93.3% 100% 98.3% 90%
fastText-wa-loghub 99.6% 99.9% 96.7% 100% 98.3% 99%
BERT-origin 2% 99% 93.3% 100% 98.3% 79%
BERT-wa 97.1% 52.7% 93.3% 100% 95% 78%
BERT-wa-loghub 96.8% 47.5% 93.3% 100% 40% 67%
Performance Testing. Wetesttheperformanceofourloganomalyprediction
models built using language models as features in an IT Operations production
environment. The production environment is a cluster with 2 CPUs and 4Gb
memory. The test data consists of 10,000 randomly sampled logs from the WA
data. We consider pre-trained BERT models of different numbers of layers, as
well as the one-layer fastText model.
In Table2, we report the total time (in seconds) and the average speed (the
numberofloglinespersecond).WeobservedthatastheBERTmodelgetsmore
Pre-training Language Models for IT Operations 159
complex with more layers, the average speed of embedding inference from the
pre-trainedmodeldropssignificantly.ThefastTextmodelisover40timesfaster
than the one-layer BERT model since it is basically perform a lookup once the
pre-trained embeddings are loaded into memory.
Table 2. Performance testing results on log anomaly detection with pre-trained fea-
tures from embeddings.
fastText BERT
1 layer 1 layer 3 layers 6 layers
Total time 1.4s 60s 450s 1700s
Average speed 7000 lines/s 166 lines/s 22 lines/s 6 lines/s
5 Conclusion and Future Work
As IT complexity grows and the use of AI technologies expands, enterprises are
looking to bring in the power of AI to transform how they develop, deploy and
operate their IT. Pre-training a language model can accelerate the development
of text-based AI models for optimizing IT Operations management tasks at a
large scale. We investigate the effects of this language model pre-training app-
roachforITOperationsmanagementthroughaseriesofexperimentsondifferent
languagemodeltypes,datadomainsanddatadiversities.Wepresenttheempir-
ical results on the prediction accuracy of our log anomaly prediction model and
its run-time inference performance using language models as features in an IT
Operationsproductionenvironment.Weshowthatthemachinelearningmodels
built using context-free embeddings trained with diverse IT Operations domain
data as features outperform those AI models built using language models with
general-purpose data. Our pre-trained language models for IT Operations will
be released soon. We hope that the insights gained from these experiments will
help researchers and practitioners develop solutions and tools that enable bet-
ter scalability, integration and management in the IT Operations domain. In
the future, we will continue to explore the effects of pre-trained features using
language models on different IT Operations management tasks such as fault
localizationandsimilarincidentanalysis.Besides,weplantoextendthestudies
to use more advanced language models such as GPT-3 [3].
References
1. I.W. Assistant (2020). https://www.ibm.com/cloud/watson-assistant/
2. Bojanowski, P., Grave, E., Joulin, A., Mikolov, T.: Enriching word vectors with
subword information. Trans. Assoc. Comput. Linguist. 5, 135–146 (2017)
160 X. Liu et al.
3. Brown, T.B., et al.: Language models are few-shot learners. arXiv preprint
arXiv:2005.14165(2020)
4. CA-Technologies: The avoidable cost of downtime (2010)
5. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: pre-training of deep bidi-
rectional transformers for language understanding. In: Proceedings of the 2019
Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers),
pp. 4171–4186 (2019)
6. Gartner (2017). https://www.gartner.com/en/newsroom/press-releases/2017-04-
11-gartner-says-algorithmic-it-operations-drives-digital-business
7. Howard,J.,Ruder,S.:Universallanguagemodelfine-tuningfortextclassification.
arXiv preprint arXiv:1801.06146 (2018)
8. Kafka (2020): https://kafka.apache.org/
9. Kudo, T., Richardson, J.: Sentencepiece: a simple and language independent
subword tokenizer and detokenizer for neural text processing. arXiv preprint
arXiv:1808.06226 (2018)
10. Meng,W.,etal.:Loganomaly:Unsuperviseddetectionofsequentialandquantita-
tive anomalies in unstructured logs. In: IJCAI, pp. 4739–4745 (2019)
11. Mikolov, T., Grave, E., Bojanowski, P., Puhrsch, C., Joulin, A.: Advances in pre-
trainingdistributedwordrepresentations.arXivpreprintarXiv:1712.09405(2017)
12. Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J.: Distributed repre-
sentationsofwordsandphrasesandtheircompositionality.In:AdvancesinNeural
Information Processing Systems, pp. 3111–3119 (2013)
13. Pennington, J., Socher, R., Manning, C.D.: Glove: global vectors for word repre-
sentation.In:Proceedingsofthe2014ConferenceonEmpiricalMethodsinNatural
Language Processing (EMNLP), pp. 1532–1543 (2014)
14. Peters,M.E.,Neumann,M.,Iyyer,M.,Gardner,M.,Clark,C.,Lee,K.,Zettlemoyer,
L.: Deep contextualized word representations. arXiv preprint arXiv:1802.05365
(2018)
15. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.: Improving language
understandingbygenerativepre-training(2018).https://s3-us-west-2.amazonaws.
com/openai-assets/researchcovers/languageunsupervised/languageunderstanding
paper.pdf
16. Sarnovsky, M., Surma, J.: Predictive models for support of incident management
processinitservicemanagement.ActaElectrotechnicaetInformatica18(1),57–62
(2018)
17. Vaswani, A., et al.: Attention is all you need. In: Advances in Neural Information
Processing Systems, pp. 5998–6008 (2017)
18. Wold,S.,Esbensen,K.,Geladi,P.:Principalcomponentanalysis.Chemom.Intell.
Lab. Syst. 2(1–3), 37–52 (1987)
19. Xu, W., Huang, L., Fox, A., Patterson, D., Jordan, M.I.: Detecting large-scale
system problems by mining console logs. In: Proceedings of the ACM SIGOPS
22nd symposium on Operating Systems Principles, pp. 117–132 (2009)
20. Zhang, Y., Rodrigues, K., Luo, Y., Stumm, M., Yuan, D.: The inflection point
hypothesis:aprincipleddebuggingapproachforlocatingtherootcauseofafailure.
In: Proceedings of the 27th ACM Symposium on Operating Systems Principles,
pp. 131–146 (2019)
Pre-training Language Models for IT Operations 161
21. Zhou, X., et al.: Latent error prediction and fault localization for microservice
applications by learning from system trace logs. In: Proceedings of the 2019 27th
ACM Joint Meeting on European Software Engineering Conference and Sympo-
sium on the Foundations of Software Engineering, pp. 683–694 (2019)
22. Zhu, J., et al.: Tools and benchmarks for automated log parsing. In: 2019
IEEE/ACM41stInternationalConferenceonSoftwareEngineering:SoftwareEngi-
neering in Practice (ICSE-SEIP), pp. 121–130. IEEE (2019)
Towards Runtime Verification via Event
Stream Processing in Cloud Computing
Infrastructures
B
Domenico Cotroneo, Luigi De Simone, Pietro Liguori( ), Roberto Natella,
and Angela Scibelli
DIETI, University of Naples Federico II, Naples, Italy
{cotroneo,luigi.desimone,pietro.liguori,roberto.natella}@unina.it,
PI:EMAIL
Abstract. Software bugs in cloud management systems often cause
erraticbehavior,hinderingdetection,andrecoveryoffailures.Asacon-
sequence, the failures are not timely detected and notified, and can
silently propagate through the system. To face these issues, we propose
a lightweight approach to runtime verification, for monitoring and fail-
ure detection of cloud computing systems. We performed a preliminary
evaluation of the proposed approach in the OpenStack cloud manage-
ment platform, an “off-the-shelf” distributed system, showing that the
approach can be applied with high failure detection coverage.
· ·
Keywords: Runtime verification Runtime monitoring Cloud
· ·
computing systems OpenStack Fault injection
1 Introduction
Nowadays, the cloud infrastructures are considered a valuable opportunity for
running services with high-reliability requirements, such as in the telecom and
health-caredomains[13,35].Unfortunately,residualsoftwarebugsincloudman-
agementsystemscanpotentiallyleadtohigh-severityfailures,suchasprolonged
outagesanddatalosses.Thesefailuresareespeciallyproblematicwhentheyare
silent,i.e.,notaccompaniedbyanyexplicitfailurenotification,suchasAPIerror
codes,orerrorentriesinthelogs.Thisbehaviorhindersthetimelydetectionand
recovery, lets the failures to silently propagate through the system, and makes
the traceback of the root cause more difficult, and recovery actions more costly
(e.g., reverting a database state) [11,12].
Tofacetheseissues,morepowerfulmeansareneededtoidentifythesefailures
at runtime. A key technique in this field is represented by runtime verification
strategies, which perform redundant, end-to-end checks (e.g., after service API
calls) to assert whether the virtual resources are in a valid state. For example,
these checks can be specified using temporal logic and synthesized in a runtime
monitor[7,14,30,36],e.g.,alogicalpredicateforatraditionalOScanassertthat
(cid:2)c SpringerNatureSwitzerlandAG2021
H.Hacidetal.(Eds.):ICSOC2020Workshops,LNCS12632,pp.162–175,2021.
https://doi.org/10.1007/978-3-030-76352-7_19
Towards Runtime Verification via Event Stream Processing 163
a thread suspended on a semaphore leads to the activation of another thread
[2]. Runtime verification is now a widely employed method, both in academia
and industry, to achieve reliability and security properties in software systems
[4]. This method complements classical exhaustive verification techniques (e.g.,
model checking, theorem proving, etc.) and testing.
In this work, we propose a lightweight approach to runtime verification tai-
lored for the monitoring and analysis of cloud computing systems. We used a
non-intrusive form of tracing of events in the system under test, and we build
a set of lightweight monitoring rules from correct executions of the system in
order to specify the desired system behavior. We synthesize the rules in a run-
timemonitorthatverifieswhetherthesystem’sbehaviorfollowsthedesiredone.
Anyruntimeviolationofthemonitoringrulesgivesatimelynotificationtoavoid
undesired consequences, e.g., non-logged failures, non-fail-stop behavior, failure
propagation across sub-systems, etc. Our approach does not require any knowl-
edge about the internals of the system under test and it is especially suitable in
the multi-tenant environments or when testers may not have a full and detailed
understanding of the system. We investigated the feasibility of our approach in
the OpenStack cloud management platform, showing that the approach can be
easily applied in the context of an “off-the-shelf” distributed system. In order
to preliminary evaluate the approach, we executed a campaign of fault-injection
experiments in OpenStack. Our experiments show that the approach can be
applied in a cloud computing platform with high failure detection coverage.
In the following of this paper, Sect.2 discusses related work; Sect.3 presents
the approach; Sect.4 presents the case study; Sect.5 experimentally evaluates
the approach; Sect.6 concludes the paper.
2 Related Work
Promptly detecting failures at runtime is fundamental to stop failure propaga-
tion and mitigate its effects on the system. In this work, we exploit runtime
verification to state the correctness of a system execution according to specific
properties. In literature, some studies refer to runtime verification as runtime
monitoringordynamicanalysis.Runtimemonitoringconsistsoftheobservation
of behaviors of the target system during its operation instead of verifying the
system according to a specific model.
Over the last decades, several efforts have been spent on methodologies
and tools for debugging and monitoring distributed systems. Aguilera et al. [1]
proposed an approach to collect black-box network traces of communications
betweennodes.Theobjectivewastoinfercausalpathsoftherequestsbytracing
call pairs and by analyzing correlations. Magpie [3] and Pinpoint [8] reconstruct
causal paths by using a tracing mechanism to record events at the OS-level and
the application server level. The tracing system tags the incoming requests with
a unique path identifier and links resource usage throughout the system with
that identifier. Gu et al. [21] proposes a methodology to extract knowledge on
distributed system behavior of request processing without source code or prior
164 D. Cotroneo et al.
knowledge. The authors construct the distributed system’s component archi-
tecture in request processing and discover the heartbeat mechanisms of target
distributedsystems.Pip[31]isasystemforautomaticallycheckingthebehavior
ofadistributedsystemagainstprogrammer-writtenexpectationsaboutthesys-
tem.Pipprovidesadomain-specificexpectationslanguageforwritingdeclarative
descriptions of the expected behavior of large distributed systems and relies on
user-written annotations of the source code of the system to gather events and
topropagatepathidentifiersacrosschainsofrequests.OSProfiler[25]providesa
lightweightbutpowerfullibraryusedbyfundamentalcomponentsinOpenStack
cloud computing platform [24]. OSProfiler provides annotation system that can
beabletogeneratetracesforrequestsflow(RPCandHTTPmessages)between
OpenStack subsystems. These traces can be extracted and used to build a tree
of calls which can be valuable for debugging purposes. To use OSProfiler, it is
required deep knowledge about OpenStack internals, making it hard to use in
practice.
Researchstudiesonruntimeverificationfocusedonformalismsfordescribing
properties to be verified. Typically, a runtime verification system provides a
Domain Specification Language (DSL) for the description of properties to be
verified. The DSL can be a stand-alone language or embedded in an existing
language. Specification languages for runtime verification can be regular, which
includes temporal logic, regular expressions, and state machines, but also non-
regular, which includes rule systems, stream languages.
Intheruntimeverificationliterature,thereisanestablishedsetofapproaches
forthespecificationoftemporalproperties,whichincludeLinearTemporalLogic
(LTL) [28], Property Specification Patterns (PSP) [16], and Event Processing
Language(EPL)[18].LinearTemporalLogicisthemostcommonfamilyofspec-
ificationlanguages.Thisapproachsupportslogicalandtemporaloperators.LTL
isextensivelyusedasspecificationlanguageinmanymodelcheckers[6,9,22].The
Property Specification Patterns consist of a set of recurring temporal patterns.
Several approaches use PSP and/or extend original patterns used in [5]. Event
Processing Language is used to translate event patterns in queries that trigger
eventlistenerswhetherthepatternisobservedintheeventstreamofaComplex
EventProcessing(CEP)environment[33].Themostinterestingcharacteristicof
CEPsystemsisthatcanbeusedinStream-basedRuntimeVerification orStream
Runtime Verification (SRV) tools. SRV is a declarative formalism to express