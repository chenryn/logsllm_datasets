2
3
Fig. 6.
The standard deviation of security exhibits a large increase in
the vicinity of the transition between the security and performance focused
security regimes
3
1
2
Normalized Benefit
Fig. 5.
Security (blue) and Performance (red) scores for the Q-learning
architecture facing the Ap = 0.50 attacker. Markers denote mean ﬁnal values
of security and performance collected over 100 replicate runs. Vertical lines
denote the standard deviation to the mean values.
In Fig. 5 the system undergoes a relatively abrupt transition
from a focus on performance to a focus on security around
BN = 1. We note that this is the point that we have,
Bi ≈ (Li,m + Li,s).
(20)
The observed transition implies that under the assumed attack
strength we have two regimes of system behavior. When,
Bi  Li,m + Li,s
(22)
the system is tuned for optimizing system security.
Figure 6 shows the standard deviation of the mean se-
curity score as a function of the normalized beneﬁt, BN . A
large relative increase in the standard deviation is evident in
the transition region between the security and performance-
focused regimes. Large variance at a relatively sharp transi-
tion between qualitatively distinctive behavioral regimes is a
property often observed in both human and natural systems
undergoing phase transitions [23]. This divergence in variance
often entails an ampliﬁed agility on the part of systems poised
on the edge of transitions between regimes of qualitatively
distinctive behavior. It is our hypothesis that such agility can
be harnessed in cyber control systems to counter dynamic,
evolving threats. To investigate this hypothesis we study the
distinctive characteristics of our Q-learning architecture poised
between the security- and performance-biased regimes.
We instantiate 3 classes of Q-learning defender and study
the attributes of their learned defense policies. The ﬁrst de-
fender class, termed the performance-biased defender, is given
a low relative reward value of B = 25. A second class of
defender is given a large reward value, B = 1000, strongly bi-
asing the defender to preferentially discover defensive policies
that achieve high security. This is the security-biased defender.
The third class of defender is positioned in the transition region
by setting B = 130. We refer to this defender as the critically-
balanced defender in analogy with the common terminology
used in the physical sciences to describe systems undergoing
phase transitions as critical systems.
The 3 classes of defender are made to face 3 severity levels
of attack. Each defender faces a weak attack (Ap = 0.10),
a medium-strength attack ((Ap = 0.50), and a strong attack
((Ap = 0.90). Results from theses studies are displayed in
Fig. 7, with the strength of the attacker varied left to right
(from weak to strong attacker instantiation), and the bias of
the defensive architecture being varied from top to bottom
(performance biased on top, security biased on bottom). Each
marker in Fig. 7 indicates the ﬁnal security and performance
score achieved in a single replicate run of the model.
Looking across the top row in Fig. 7 we ﬁnd that the
performance-biased defender responds to all attack types with
a policy that generates high levels of system performance but
low levels of system security. In contrast, the security-biased
defender, results from which are plotted along the bottom
row of Fig. 7, consistently achieves high levels of system
security against the medium and strong attacker variants, while
achieving a mixed, but relatively high, level of system security
when facing the weak attacker.
Opposed to both of these relatively uniform responses to
the different attacker strengths, the critical defender achieves
a largely performance focused result when facing the weak
attacker, a security-performance balanced result when facing
the medium strength attacker, and a high security response
when facing the strong attacker.
Figure 8 provides an in-depth look at the response of the
different defender types to the 3 classes of attacker. Figure 8
depicts the mean response (height of bar) of each defender type
(x-axis) when facing each attacker strength (color). Error bars
in Fig. 8 denote one standard deviation from the mean. The
depicted values correspond to defender behavior during the
614
Fig. 7. Markers denote the value of security and performance obtained at the end of 100 independent replicate runs for performance (B = 25), critical
(B = 130), and security (B = 1000) biased defenders facing a weak (Ap = 0.10), medium (Ap = 0.50), and strong (Ap = 0.90) attacker. Attacker strength
is increased left to right, while the bias of the defensive architecture is varied top to bottom.
Fig. 8. Mean number of inspected outputs (left panel), exploited memory disclosures (center panel), and wasted inspections (right panel) over 100 replicate
model runs each for the performance-, critical-, and security-focused defenders facing weak, medium, and strong attackers. Colors correspond between the
current ﬁgure and Fig. 7.
615
ﬁnal windowing period of 200 simulation steps, capturing the
learned policy of each defender type during the ﬁnal simulated
period.
Figure 8 (left) shows that the performance-biased defender
consistently inspects a low number of system outputs, while the
security-focused defender consistently inspects a high number
of system outputs. In contrast to these rigid behaviors, the
critical-defender inspects a small number of packets when
facing the weak attacker, a medium number of packets when
facing the medium-strength-attacker, and a large number of
system outputs when facing the strong attacker.
It is noteworthy that the critical defender learns to inspect
a number of system outputs that nearly exactly matches the
number of memory disclosures generated by the attacker on
average in a given scenario. The Ap = 0.10 attacker causes
20 leaked addresses on average per 200 system outputs, to
which the critical defender learns to inspect 20.85 system
outputs on average per 200 simulation steps, or an inspection
ratio of 0.104, very near to the 0.10 attack value. Contrast
this with 9.74 inspected system outputs by the performance
focused defender, and the 105.27 inspected packets of the
securtiy-focused defender (an order of magnitude increase in
inspections). A similar story holds for the responses to the
medium and strong defenders.
The static, insensitive policies learned by the performance-
and security-focused defenders lead to system inefﬁciencies,
as depicted in Fig. 8 (center and right). This inefﬁciency
can be demonstrated by comparing summary metrics for the
security and performance-biased defenders with the those for
the critical defender.
Figure 8 (right) indicates that when facing the weak at-
tacker, the critical defender engages in approximately 76 fewer
unnecessary inspections compared to the security-focused de-
fender, a signiﬁcant decrease in wasted system resources, while
resulting in approximately 8 additional missed memory dis-
closures compared to the security-focused policy. The policy
learned by the critical defender slashes system overhead due
to system output searching by 90% from the baseline, search
everything policy, compared to a 47% decrease in system
overhead due to searching accomplished by the security-
focused policy.
Parameterizing the Q-learning defense such that it is poised
between the security-biased and performance-biased regimes
gives the architecture an added level of adaptability and
resilience to an attacker that might dynamically and perhaps
adaptively adjust
the strength of their memory disclosure
attack. By the selecting a critical value for the reward term, B,
the operator enables the control architecture to automatically
adjust its level of operation to efﬁciently counter the current
attack. This level of agility is only painstakingly achieved by
human-operated systems, and so cannot be counted on to be
achievable in the stresses of a rapidly unfolding cyber attack
scenario. The proposed architecture achieves this high level of
agility without the need for human intervention or supervision.
VII. FUTURE WORK AND CONCLUSION
these goals is a capability of clear utility in today’s threat
environment. The fact that learning techniques typically require
the careful tuning of parameter values in order to operate
effectively imposes signiﬁcant barriers to operational adoption
of systems built using these techniques. Ideally, operators
would be spared the effort of manually tweaking and testing
myriad parameter settings, and instead simply specify a desired
operating point in performance-security space, for which the
system would tune its own algorithms to achieve the speciﬁed
level of security and performance, and dynamically adjust to
changes in attacker behavior to maintain the requested levels
of operation.
Automated tuning of algorithm parameters would have the
added beneﬁt of facilitating exploration of more complex and
nuanced reward functions. We have adopted a simple reward
function in this study to maximize the study’s generality and
applicability to the domain of adaptive control and strategy
development,
Ri = Bi − (Li,s + Li,m)
(23)
the beneﬁt
with Bi
the defender gets from discovering a
memory disclosure in system output i, Li,s the latency cost
the defender pays for searching a system output i for memory
disclosures, and Li,m the latency cost the defender pays for
mitigating a memory disclosure in payload i once if it has been
discovered.
We note that by adopting an extended reward function
we give the defender more parameters to adjust and tune to
achieve a given effect. For example, if we have access to an
oracle that informs the defender after-the-fact when a memory
disclosure has been missed (or the defender is immediately
able to discern this failure due to its effects in the defended
system), we might incorporate a direct cost term (P ) in the
reward function to directly punish the defender for missing
the said memory disclosure,
Ri = Bi − (Pi + Li,s + Li,m).
(24)
Here the defender is awarded Bi points for discovering a
memory disclosure and taking mitigating steps before it harms
system security, and punished Pi points each time a memory
disclosure goes undetected and so is exploited by the attacker.
Preliminary studies indicate that even a relatively small pun-
ishment term can impact strategy development nontrivially.
terms might
Other additional
include a reward for not
inspecting a output, a punishment term for a certain threshold
number of consecutive memory disclosure misses by the
defender, and so on. Each of these terms give the policy
tuning apparatus increased ability to ﬁne tune the nature of
the policy that is learned, but the combinatorial explosion
caused by the increasing number of terms to be set by
the user further necessitates a meta-adaptation capability to
automatically set the parameters of the reward function and
the learning algorithm parameters based on goals deﬁned by
the operator.
Balancing security and performance in an automated fash-
ion while maintaining the requisite agility to move between
To further explicate this point, in this study we have exam-
ined model behavior under the variation of one, or at most two,
616
[6] H. Okhravi, T. Hobson, D. Bigelow, and W. Streilein, “Finding focus in
the blur of moving-target techniques,” IEEE Security & Privacy, vol. 12,
no. 2, pp. 16–26, 2014.
[7] D. Bigelow, T. Hobson, R. Rudd, W. Streilein, and H. Okhravi, “Timely
rerandomization for mitigating memory disclosures,” in Proceedings of
the 22nd ACM SIGSAC Conference on Computer and Communications
Security. ACM, 2015, pp. 268–279.
[8] M. Van Dijk, A. Juels, A. Oprea, and R. L. Rivest, “Flipit: The game of
stealthy takeover,” Journal of Cryptology, vol. 26, no. 4, pp. 655–713,
2013.
“Pax
http://pax.grsecurity.net/docs/aslr.txt, 2003.
randomization,”
address
layout
space
[9]
[10] A. Bittau, A. Belay, A. Mashtizadeh, D. Mazieres, and D. Boneh,
IEEE,
“Hacking blind,” in IEEE Symposium on Security and Privacy.
2014, pp. 227–242.
[11] B. H. Bloom, “Space/time trade-offs in hash coding with allowable
errors,” Communications of the ACM, vol. 13, no. 7, pp. 422–426, 1970.
Cambridge University Press
[12] P. Brass, Advanced data structures.
Cambridge, 2008.
[13] S. Russell and P. Norvig, Artiﬁcial intelligence: a modern approach.
Prentice Hall, 2010.
[14] R. S. Sutton and A. G. Barto, Introduction to reinforcement learning.
MIT Press Cambridge, 1998.
[15] M. Minsky, “Theory of neural analog reinforcement systems and its
application to the brain-model problem,” in Steps Toward Artiﬁcial
Intelligence. Princeton University, 1954, p. 830.
[16] B. Farley and W. Clark, “Simulation of self-organizing systems by
the IRE Professional Group on
digital computer,” Transactions of
Information Theory, vol. 4, no. 4, pp. 76–84, 1954.
[20]
[17] R. Bellman, Dynamic Programming. Princeton University Press, 2957.
[18] Z. Shi, Advanced artiﬁcial intelligence. World Scientiﬁc, 2011, vol. 1.
[19] C. J. C. H. Watkins, “Learning from delayed rewards,” Ph.D. disserta-
tion, University of Cambridge, 1989.
J. H. Holland, Adaptation in natural and artiﬁcial systems: an intro-
ductory analysis with applications to biology, control, and artiﬁcial
intelligence. MIT press, 1992.
“Iceweasel,” https://wiki.debian.org/Iceweasel, 2015.
“D. Bigelow personal correspondence,” 2015.
[21]
[22]
[23] P. Lamberson and S. E. Page, “Tipping points,” Quarterly Journal of
Political Science, vol. 7, no. 2, pp. 175–208, 2012.
J. H. Miller, “Active nonlinear tests (ants) of complex simulation
models,” Management Science, vol. 44, no. 6, pp. 820–830, 1998.
[24]
[25] F. J. Stonedahl, “Genetic algorithms for the exploration of parameter
spaces in agent-based models,” Ph.D. dissertation, Northwestern Uni-
versity, 2011.
model parameters. Important effects conceivably depend on
subtle interactions between parameters that will be missed by
this method. Brute force parameter space exploration methods
rapidly lose feasibility as the number of parameters grow. Ide-
ally we desire automated methods to ﬁnd desirable operating
points in parameter space, using techniques such as those
described in [24], [25]. Developing such techniques would
have the added signiﬁcant beneﬁt of potentially automating the
tuning of the proposed architecture for operators that likely
lack the requisite time and expertise to successfully tune a
learning technique such as Q-learning. We set the investigation
of self-tuning techniques as an item for future work.
The framework, scenario, and results presented in this work
provide a foundation on which future development of the pre-
sented technique will build. Extensions of the framework that
are currently under development and investigation include the
endowing of the attacker with the ability to learn and adapt in
order to iteratively counter the adaptive defender architecture,
the extension of the implemented agent memory horizon facil-
itated by scalable encoding methods, the automatic tuning of
learning algorithm parameters to achieve user-speciﬁed system
objectives, and comparison of the Q-learning-based adaptive
architecture with architectures implemented using techniques
from alternative automated learning paradigms. Additional
security scenarios to which the developed methodology can be
generalized to will also be demonstrated on in future works.
In this work we have proposed an architecture based
on reinforcement learning to automatically balance security
and performance and discovered system parameterization set-
tings to achieve defensive agility against an array of attacks.
Adaptive control architectures combined with moving target
defenses promise a new paradigm in resilient cyber defense.
The exploitation of existing learning approaches, and the
invention of novel variants and techniques for the unique needs
of cyber defenders, is a vital direction for future effort in cyber
defensive research if the growing tide of cyber threats and
their ratcheting pace of adaptive progress is to be countered
effectively.
ACKNOWLEDGMENTS
We thank Hamed Okhravi and David Bigelow of MIT Lin-
coln Laboratory for their contributions and helpful discussions.
REFERENCES
[1] C. Chigan, Y. Ye, and L. Li, “Balancing security against performance
in wireless ad hoc and sensor networks,” in IEEE Vehicular Technology
Conference, vol. 7.
IEEE, 2004, pp. 4735–4739.
[2] M. Haleem, C. N. Mathur, R. Chandramouli, and K. Subbalakshmi,
“Opportunistic encryption: A trade-off between security and throughput
in wireless networks,” IEEE Transactions on Dependable and Secure
Computing, vol. 4, no. 4, pp. 313–324, 2007.
[3] W. Zeng and M.-Y. Chow, “Optimal tradeoff between performance
and security in networked control systems based on coevolutionary
algorithms,” IEEE Transactions on Industrial Electronics, vol. 59, no. 7,
pp. 3016–3025, 2012.
[4] ——, “Modeling and optimizing the performance-security tradeoff
on d-ncs using the coevolutionary paradigm,” IEEE Transactions on
Industrial Informatics, vol. 9, no. 1, pp. 394–402, 2013.
[5] G. M. Miskeen, D. D. Kouvatsos, and E. Habibzadeh, “An exposition of
performance-security trade-offs in ranets based on quantitative network
models,” Wireless Personal Communications, vol. 70, no. 3, pp. 1121–
1146, 2013.
617