title:DS-Bench Toolset: Tools for dependability benchmarking with simulation
and assurance
author:Hajime Fujita and
Yutaka Matsuno and
Toshihiro Hanawa and
Mitsuhisa Sato and
Shinpei Kato and
Yutaka Ishikawa
Tools for Dependability Benchmarking with Simulation and Assurance
DS-Bench Toolset:
Hajime Fujita∗, Yutaka Matsuno∗§, Toshihiro Hanawa†, Mitsuhisa Sato†, Shinpei Kato‡¶, and Yutaka Ishikawa∗
∗
University of California, Santa Cruz, USA
The University of Tokyo, Japan,
University of Tsukuba, Japan,
{hfujita, matsu}@cc.u-tokyo.ac.jp, {hanawa, msato}@cs.tsukuba.ac.jp,
†
‡
PI:EMAIL, PI:EMAIL
Abstract—Today’s information systems have become large
and complex because they must interact with each other via
networks. This makes testing and assuring the dependability of
systems much more difﬁcult than ever before. DS-Bench Toolset
has been developed to address this issue, and it includes D-Case
Editor, DS-Bench, and D-Cloud. D-Case Editor is an assurance
case editor. It makes a tool chain with DS-Bench and D-Cloud,
and exploits the test results as evidences of the dependability
of the system. DS-Bench manages dependability benchmarking
tools and anomaly loads according to benchmarking scenarios.
D-Cloud is a test environment for performing rapid system
tests controlled by DS-Bench. It combines both a cluster of real
machines for performance-accurate benchmarks and a cloud
computing environment as a group of virtual machines for
exhaustive function testing with a fault-injection facility. DS-
Bench Toolset enables us to test systems satisfactorily and to
explain the dependability of the systems to the stakeholders.
Keywords-dependability benchmarking; assurance case; fault
injection; distributed systems; virtual machine
I. INTRODUCTION
Information systems have become much more important
to our daily life, as these systems are ubiquitous in our so-
ciety. They are used everywhere, for instance, in transaction
systems and embedded systems. These systems provide very
important infrastructures, and unexpected suspension of the
services provided by these systems costs a lot. Therefore
these systems should be dependable [1].
Although highly dependable systems, such as high-
availability servers, are likely to form parallel and distributed
systems in order to acquire redundancy, the tests of such
large-scale systems are troublesome. Exhaustive testing un-
der realistic conditions is getting more and more costly
and time-consuming, particularly in terms of setting up
of the test environment. In addition, a highly dependable
system should be equipped with a combination of multiple
functions for fault
tolerance against overload, hardware
faults, etc. Thus, a test support tool, one that conﬁgures
test environments automatically and executes various kinds
of tests in a sophisticated manner,
is urgently needed.
In such tools, any anomalies, including hardware faults,
§Currently he is in Nagoya University, Japan.
¶Currently he is in Nagoya University, Japan. This research was done
while he was in The University of Tokyo.
978-1-4673-1625-5/12/$31.00 ©2012 IEEE
unpredictable overloads, operational failure due to human
error, should be treated ﬂexibly, and in a uniﬁed manner,
similar to the stress of ordinary workloads.
Furthermore, as systems become more networked and
involve many stakeholders, the developers or suppliers of
the systems need to be able to explain the dependability of
the systems, even to the end users. Proof of the dependability
of the systems is supported mostly by the results of various
methods such as testing, veriﬁcation, benchmarking, etc.
However, it is sometimes difﬁcult to assure users that the
system can be certiﬁed to be dependable based on results
obtained by such methods. Therefore, support
tools for
assuring dependability based on observable test results are
desired.
To address these problems, we have developed the follow-
ing tools: A benchmark test framework for system depend-
ability, referred to as DS-Bench, a description and agree-
ment support tool, called D-Case Editor, and an execution
environment, D-Cloud. D-Case Editor makes a tool chain
with DS-Bench, and exploits the test results as evidences of
the dependability of the system. DS-Bench measures various
dependability metrics according to a speciﬁc scenario. D-
Cloud supports DS-Bench so that
it can perform rapid
system tests. The suite formed by these support tools is
called DS-Bench Toolset.
DS-Bench Toolset provides the following features.
• DS-Bench Toolset obtains dependability metrics on the
target system using various benchmarks under several
anomaly situations, and
• it performs benchmark tests using the whole target
system, including operating system, network, etc., on
a hybrid environment of both physical and virtual
machines, and
• it provides the evidence for the assurance case based
on the benchmark results.
In the present paper, we demonstrate that DS-Bench Toolset
works as a benchmark for a practical distributed system with
fault injection.
II. RELATED WORK
DBench [2] is a project aimed at establishing depend-
ability benchmarking methods for several domains. The
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:21:02 UTC from IEEE Xplore.  Restrictions apply. 
project has deﬁned several concepts essential for dependabil-
ity benchmarking. Among of them, “faultload” represents
a set of faults and exceptional events based on operator
faults, software faults, and hardware faults [3], [4]. However,
faultload cannot emulate undesired overloads such as inter-
ventions by other applications. In fact, since networked and
distributed systems can usually be easily affected by external
inﬂuences, a uniﬁed manner in which to specify such work-
loads and faultloads is required. However, the project did
not provide a total software framework to integrate multiple
dependability benchmarks.
As a large-scale software test platform, ETICS [5] pro-
vides automated test environments for grid and distributed
software on a grid computing platform using Condor as a
workload management system. Since ETICS handles tests
on the service level, ETICS cannot test the dependability of
a system including the OS layer.
On the other hand, a number of fault injection techniques
for program tests have been proposed. DOCTOR [6] is a
software fault injector, that supports memory faults, CPU
faults, and communication faults, without modiﬁcation of
the source code to be tested. Xception [7] uses the kernel
module to inject the fault, and the tester sets the hardware
breakpoint to a speciﬁed address. When the process reaches
the breakpoint, the fault is injected into the register or mem-
ory as an incorrect value. BOND [8] uses a special agent to
intercept the system call of the Windows NT R(cid:2)version 4.0.
The agent hooks the system call from the target application
and returns an incorrect value to the application. The above
tools are categorized as software fault injection (SWIFI)
tools. However, essential to such tools, source code must
be modiﬁed, or a speciﬁc environment, such as OS with a
special driver on a speciﬁc version, is required.
FAUmachine [9] performs a software test using virtual
machines as the fault injection mechanism. However, since
FAUmachine does not provide an automated test environ-
ment, the tester must conﬁgure the test environment manu-
ally.
NFTAPE [10] is a fault
injection framework for dis-
tributed systems. NFTAPE has component-based architec-
ture including a control host for managing the test and
fault injection, and a process manager on each target host,
which runs the fault injector, workload generators, target
applications, and so on. In NFTAPE,
lightweight fault
injectors (LWFIs) are used in order to provide any fault
injection method needed, and automatic testing is available.
However, an LWFI uses a special API as a trigger event.
Therefore, existing and commonly-used tools are not directly
applicable for NFTAPE since they must be modiﬁed before
being suitable for NFTAPE. Moreover, similar to the above
tools, system-wide tests, including entire OSs and networks,
cannot be performed.
System assurance has become of great
importance in
many industrial sectors. Safety cases (assurance cases for the
safety of systems) are required for submission to certiﬁcation
bodies for developing and operating safety-critical systems,
e. g., automotive, railway, defense, nuclear plants, and sea oil
production platforms. There are several standards, e. g. the
Rail Yellow Book [11] and MoD Defence Standard 00-56,
which mandate the use of safety cases.
There are several deﬁnitions for assurance cases [12]. We
show one such deﬁnition below [13].
“a documented body of evidence that provides a
convincing and valid argument that a system is
adequately dependable for a given application in
a given environment.”
Assurance cases are often written in structured documents
using a graphical notation to ease the difﬁculty of writing
and certifying them. Goal Structuring Notation (GSN) is one
such notation standard [14]. Currently there are few tools for
assurance cases. One notable tool is ASCE tool [13], which
is widely used in several areas such as defense, safety-critical
areas, and medical devices. However, the current ASCE tool
is mostly used to generate documents for certiﬁcation, and
not so much used during the various development phases.
Our intention is to use assurance cases for arguing for the
dependability of the systems when developing and testing
them, as studied in [15].
Furthermore, as far as we know, in the benchmarking
and fault injection communities, there has not been much
discussion on assuring how the test results contribute to the
dependability of the target systems, especially using certain
frameworks, such as assurance cases. This seems because
the contribution of test results to dependability is taken for
granted in these communities. However, as systems become
large and complex, this becomes less clear, in particular for
non-experts.
III. DS-BENCH TOOLSET
In order to support systematic measurement of several
dependability metrics, DS-Bench, a tool for dependability
benchmarking, and D-Cloud, a test environment for perform-
ing rapid system tests, have been developed. D-Case Editor,
which is a support tool for discussions between users and
developers, has also been developed [16]. We are enhancing
D-Case Editor in order to allow it to collaborate with DS-
Bench.
Figure 1 illustrates an overview of DS-Bench Toolset.
Each DS-Bench system has one DS-Bench controller with
benchmark database, and one or more benchmark target
machines. The target machines can be either physical ma-
chines or virtual machines. Allocation of physical machines
and creation of virtual machines are managed by the D-
Cloud controller. A user interacts with the toolset via D-Case
Editor or DS-Bench controller’s web interface. DS-Bench
controller also controls benchmark procedures. Benchmark
programs are executed on benchmark target machines. On
each target machine there is a small server program to
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:21:02 UTC from IEEE Xplore.  Restrictions apply. 
receive commands from the controller. The toolset is ca-
pable of simulating anomaly states in the target systems
by injecting anomaly loads. Anomaly loads are generated
by anomaly generators. Benchmark programs and anomaly
loads are combined into benchmark scenarios. Benchmark
scenarios, benchmark programs, anomaly generators, and
benchmark results are stored in the benchmark database. We
deﬁne several types of protocols described by XML between
the D-Case Editor and the DS-Bench controller, and between
the DS-Bench controller and the D-Cloud controller.
Tester
DS-Bench
Controller
D-Case
Editor
D-Cloud
Controller
Benchmark
Databases
Anomaly
Anomaly
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAnnnnnnnnnnnnnnnnnnnoooooooooommmmmmaaalllllllyyy
Benchmark
Script
Script
Script
Scenario
Performance
PPP Benchmark
Performance
Benchmark
Benchmark
Programs
Anomaly
Anomaly
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnooooooooooooooooooooooooooommmmmmmmmmmmmmmmmmaaaaaaaaaaalllllllllllllllllllllllyyyyyyyyyyyyyyyyyyy
Anomaly
Generator
Generator
GGGGenerator
Generator
Benchmark
Benchmark
Results
Benchmark
Results
Results
Benchmark
Programs
Target Machines
(Physical)
Target Machines
(Virtual,Cloud)
Figure 1. Overview of DS-Bench Toolset
A. Assurance Case Editor with Parameters: D-Case Editor
To make assurance cases more useful in system develop-
ment, D-Case Editor, a free assurance case editor has been
implemented as an EclipseTM plug-in using Eclipse GMF.
D-Case Editor supports GSN, and provides the following
features: a GSN pattern library function and a prototype
type checking function [17], a consistency checking function
using an advanced proof assistant tool, and the formation of
a tool chain with the development tools.
The tool chain between DS-Bench and D-Case Editor is
designed as follows.
• DS-Bench and D-Case Editor share the same XML
format for test results.
• D-Case Editor can set parameters for DS-Bench scenar-
ios, such as latency and the access rate of web server
systems. Then D-Case Editor directly invokes DS-
Bench and obtain the test result (described in Section
III-D).
Figure 2 shows a snapshot of D-Case Editor, when setting
the parameters of a DS-Bench scenario. Assurance cases
edited in D-Case Editor are called “D-Case Diagrams”
(sometimes just “D-Case”).
Figure 2. D-Case Editor Snapshot when setting parameters
B. Dependable System Benchmark Framework: DS-Bench
DS-Bench is a framework for conducting multiple de-
pendability benchmark tests. The details of DS-Bench are
described below.
1) Supports Different Benchmark Programs: DS-Bench
itself is not a measurement tool for dependability metrics.
Instead, DS-Bench supports execution of many kinds of
programs. A benchmark program may be an existing one
already available in the market, or one that is created by
a user of the target system for a very speciﬁc purpose.
Therefore, DS-Bench supports a wide variety of programs.
Moreover, new programs can be added to the benchmark
programs to meet the user needs.
There are two challenges in supporting multiple, arbitrary
programs. First, each benchmark program has different ways
of specifying benchmark parameters. This can be an issue,
especially because we provide a common graphical user
interface for specifying benchmark parameters. The second
problem involves how to store the result. When a benchmark
completes, it displays its results on screen. Usually this
output is formatted in a text table so that a human user can
easily recognize the result. However, while these tables are
human-friendly, they are not computer-friendly. They are not
suitable for being recorded as data, especially when we want
to reuse the data, for instance, in making charts, comparing
results between different runs, and so on.
In order to solve these issues, DS-Bench introduces a
benchmark description for each benchmark program. The
benchmark description describes what kind of parameters
the program takes, and which part of the output should be
interpreted as data. When a new program is added to the
DS-Bench framework, a new benchmark description should
be prepared. Figure 3 shows an example of a benchmark
description for iperf, a network bandwidth measurement
tool. Figure 4 shows an input dialog for the iperf benchmark.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:21:02 UTC from IEEE Xplore.  Restrictions apply. 
 (	9

	.