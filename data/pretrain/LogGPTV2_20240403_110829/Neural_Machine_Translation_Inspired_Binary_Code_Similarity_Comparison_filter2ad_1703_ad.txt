45,461
48,613
34,118
163,608
Training
Dissim.
35,223
45,278
48,472
33,920
162,893
Total
70,639
90,739
97,085
68,038
326,501
Sim.
3,902
5,013
5,390
3,809
18,114
Validation
Dissim.
3,946
5,069
5,397
3,750
18,162
Total
7,848
10,082
10,787
7,559
36,276
Sim.
4,368
5,608
6,000
4,554
20,530
Testing
Dissim.
4,354
5,590
5,988
4,404
20336
Total
8,722
11,198
11,988
8,958
40,866
Sim.
43,686
56,082
60,003
42,481
202,252
Total
Dissim.
43,523
55,937
59,857
42,074
201,391
Total
87,209
112,019
119,860
84,555
403,643
O1
O2
O3
Cross-opts
Total
are labeled with the similarity ground truth. In particular,
we prepare this dataset using OpenSSL (v1.1.1-pre1) and
four popular Linux packages, including coreutils (v8.29),
findutils (v4.6.0), diffutils (v3.6), and binutils
(v2.30). We use two architectures (x86-64 and ARM) and
clang (v6.0.0) with three different optimization levels (O1-
O3) to compile each program. In total, we obtain 437,104 basic
blocks for x86, and 393,529 basic blocks for ARM.
We follow the approach described in Section V-D to gen-
erate similar/dissimilar basic-block pairs. Totally, we generate
202,252 similar basic-block pairs (one compiled from x86 and
another from ARM; as shown in the 11th column of Table I),
where 43,686 pairs, 56,082 pairs, 60,003 pairs, and 42,481 pairs
are compiled using O1, O2, O3, and different optimization
levels, respectively. Similarly, we generate 201,391 dissimilar
basic-block pairs (as shown in the 12th column of Table I),
where 43,523 pairs, 55,937 pairs, 59,857 pairs, and 42,074 pairs
are compiled using O1, O2, O3, and different optimization
levels, respectively.
C. Evaluation on Out-Of-Vocabulary Instructions
As pre-processing is applied to addressing the issue of out-
of-vocabulary (OOV) instructions (Section IV-C), we evaluate
its impact, and seek to understand: a) how the vocabulary size
(the number of columns in the instruction embedding matrix)
grows with or without pre-processing, and b) the number of
OOV cases in later instruction embedding generation.
To this end, we collect various x86 binaries, and disassemble
these binaries to generate a corpus which contains 6,115,665
basic blocks and 39,067,830 assembly instructions. We then
divide the corpus equally into 20 parts. We counted the
vocabulary size in terms of the percentage of the corpus
analyzed, and show the result in Figure 7. The red line and
the blue line show the growth of the vocabulary size when
pre-processing is and is not applied, respectively. It can be seen
that the vocabulary size grows fast and becomes uncontrollable
when the corpus is not pre-processed.
We next investigate the number of OOV cases, i.e., unseen
instructions, in later instruction embedding generation. We
select two binaries that have never appeared in the previous
corpus, containing 67,862 blocks and 453,724 instructions. We
then count the percentage of unseen instructions that do not
exist in the vocabulary, and show the result in Figure 8. The
red and blue lines show the percentage of unseen instructions
when the vocabulary is built with or without pre-processing,
respectively. We can see that after pre-processing, only 3.7%
unseen instructions happen in later instruction embedding
generation, compared to 90% without pre-processing; (for an
OOV instruction, a zero vector is assigned). This shows that
Fig. 7: The growth of the vocabulary size.
Fig. 8: The proportion of unseen instructions.
the instruction embedding model with pre-processing has a
good coverage of instructions. Thus, it may be reused by other
researchers and we have made it publicly available.
D. Qualitative Analysis of Instruction Embeddings
We present our results from qualitatively analyzing the
instruction embeddings for the two architectures, x86 and ARM.
We ﬁrst use t-SNE [41], a useful tool for visualizing high-
dimensional vectors, to plot the instruction embeddings in
a three-dimensional space, as shown in Figure 9. A quick
inspection immediately shows that the instructions compiled for
the same architecture cluster together. Thus the most signiﬁcant
factor that inﬂuences code is the architecture as it introduces
more syntactic variation. This also reveals one of the reasons
why cross-architecture code similarity detection is more difﬁcult
than single-architecture code similarity detection.
We then zoom in Figure 9, and plot a particular x86
instruction MOVZBL EXC,[RCX+0] and its neighbors.
We can see that the mov family instructions are close together.
Next, we use the analogical reasoning to evaluate the quality
of the cross-architecture instruction embedding model. To do
this, we randomly pick up eight x86 instructions. For each x86
instruction, we select its similar counterpart from ARM based
on our prior knowledge and experience. We use [x] and {y} to
9
010203040506070809010000.250.500.751.001.251.501.752.002.252.502.75The size of vocabulary (106).With pre-processingWithout pre-processing01020304050607080901000102030405060708090100The proportion of unseen instructions in test corpus(%).With pre-processing Without pre-processingFig. 9: Visualization of all the instructions for x86 and ARM in 3D space,
and a particular x86 instruction and its neighbor instructions, with t-SNE.
Fig. 10: Visualization of a set of instructions
for x86 and ARM based on MDS. The blue
circles and red triangles represent x86 instruc-
tions and ARM instructions, respectively.
represent the embedding of an ARM instruction x, and an x86
instruction y, respectively; and cos([x1], [x2]) refers to the
cosine distance between two ARM instructions, x1 and x2. We
have the following ﬁndings: (1) cos([ADD SP,SP,0], [SUB
SP,SP,0]) is approximate to cos({ADDQ RSP,0}, {SUBQ
RSP,0}). (2) cos([ADD SP,SP,0], {ADDQ RSP,0}) is ap-
proximate to cos([SUB SP,SP,0], {SUBQ RSP,0}). This
is similar to other instruction pairs. We plot the relative positions
of these instructions in Figure 10 according to their cosine
distance matrix based on MDS. We limit the presented
examples to eight due to space limitation. In our manual
investigation, we ﬁnd many such semantic analogies that are
automatically learned. Therefore, it shows that the instruction
embedding model learns semantic information of instructions.
E. Accuracy of INNEREYE-BB
We now evaluate the accuracy of our INNEREYE-BB. All
evaluations in this subsection are conducted on Dataset I.
1) Model Training: We divide Dataset I into three parts
for training, validation, and testing: for similar basic-block
pairs, 80% of them are used for training, 10% for validation,
and the remaining 10% for testing; the same splitting rule is
applied to the dissimilar block pairs as well. Table I shows
the statistic results. In total, we have four training datasets: the
ﬁrst three datasets contain the basic-block pairs compiled with
the same optimization level (O1, O2, and O3), and the last one
contains the basic-block pairs with each one compiled with
a different optimization level (cross-opt-levels). Note that in
all the datasets, the two blocks of each pair are compiled
for different architectures. This is the same for validation
and testing datasets. Note that we make sure the training,
validation, and testing datasets contain disjoint sets of basic
blocks (we split basic blocks into three disjoint sets before
constructing similar/dissimilar basic block pairs). Thus, any
given basic block that appears in the training dataset does not
appear in the validation or testing dataset. Through this, we can
better examine whether our model can work for unseen blocks.
Note that the instruction embedding matrices for different
architectures can be precomputed and reused.
We use the four training datasets to train INNEREYE-BB
individually for 100 epochs. After each epoch, we measure the
AUC and loss on the corresponding validation datasets, and
save the models achieving the best AUC as the base models.
2) Results: We now evaluate the accuracy of the base
models using the corresponding testing datasets. The red lines
in the ﬁrst four ﬁgures in Figure 11, from (a) to (d), are the
ROC curves of the similarity test. As each curve is close to
the left-hand and top border, our models have good accuracy.
To further comprehend the performance of our models
on basic blocks with different sizes, we create small-BB and
large-BB testing subsets. If a basic block contains less than
5 instructions it belongs to the small-BB subset; a block
containing more than 20 instructions belongs to the large-BB
subset. We then evaluate the corresponding ROC. Figure 11e
and Figure 11f show the ROC results evaluated on the large-
BB subset (221 pairs) and small-BB subset (2409 pairs),
respectively, where the basic-block pairs are compiled with
the O3 optimization level. The ROC results evaluated on the
basic-block pairs compiled with other optimization levels are
similar, and are omitted here due to the page limit. We can
observe that our models achieve good accuracy for both small
blocks and large ones. Because a small basic block contains
less semantic information, the AUC (=94.43%) of the small-
BB subset (Figure 11f) is slightly lower than others. Moreover,
as there are a small portion (4.4%) of large BB pairs in the
training dataset, the AUC (=94.97%) of the large-BB subset
(Figure 11e) is also slightly lower; we expect this could be
improved if more large BB pairs are seen during training.
3) Comparison with Manually Selected Features: Several
methods are proposed for cross-architecture basic block similar-
ity detection, e.g., fuzzing [52], symbolic execution [21], [37],
and basic-block feature-based machine learning classiﬁer [19].
Fuzzing and symbolic execution are much slower than our deep
learning based approach. We thus compare our model against
the SVM classiﬁer using six manually selected block features
adopted in Gemini, such as the number of instructions and
the number of constants (see Table 1 in [65].
We extract the six features from each block to represent
the block, and use all blocks in the training dataset to train
the SVM classiﬁer. We adopt leave-one-out cross-validation
with K = 5 and use the Euclidean distance to measure the
similarity of blocks. By setting the complexity parameter c =
1.0, γ = 1.0 and choosing the RBF kernel, the SVM classiﬁer
achieves the best AUC value. Figure 11 shows the comparison
results on different testing subsets. We can see that our models
outperform the SVM classiﬁer and achieve much higher AUC
values. This is because the manually selected features largely
10
ADD SP,SP,0SUB SP,SP,0BEQ BNE CMP R9,0CMP R7,0LDR R0,[R4+0]LDR R0,[R5+0]ADDQ RSP,0SUBQ RSP,0JE JNE TESTL R12D,R12DTESTL R15D,R15DMOVQ RDI,[R12+0]MOVQ RDI,[R14+0](a) O1
(b) O2
(c) O3
(d) Cross-opt-levels
(e) Large basic blocks in O3
(f) Small basic blocks in O3
Fig. 11: The ROC evaluation results based on the four testing datasets.
lose the instruction semantics and dependency information,
while INNEREYE-BB precisely encodes the block semantics.
Examples. Table II shows three pairs of similar basic-block
pairs (after pre-processing) that are correctly classiﬁed by
INNEREYE-BB, but misclassiﬁed by the statistical feature-
based SVM model. Note that the pre-processing does not
change the statistical features of basic blocks; e.g., the number
of transfer instructions keeps the same before and after pre-
processing. Our model correctly reports each pair as similar.
Table III shows three pairs of dissimilar basic-block pairs
(after pre-processing) that are correctly classiﬁed by INNEREYE-
BB, but misclassiﬁed by the SVM model. As the statistical
features of two dissimilar blocks tend to be similar, the SVM
model—which ignores the meaning of instructions and the
dependency between them—misclassiﬁes them as similar.
F. Hyperparameter Selection for INNEREYE-BB
We next investigate the impact of different hyperparameters
on INNEREYE-BB. In particular, we consider the number of
epochs, the dimensionality of the embeddings, network depth,
and hidden unit types. We use the validation datasets of Dataset
I to examine the impact of the number of epochs, and the testing
datasets to examine the impact of other hyperparameters.
1) Number of Epochs: To see whether the accuracy of the
model ﬂuctuates during training, we trained the model for
200 epochs and evaluated the model every 10 epochs for the
AUC and loss. The results are displayed in Figure 12a and
Figure 12b. We observe that the AUC value steadily increases
and is stabilized at the end of epoch 20; and the loss value
decreases quickly and almost stays stable after 20 epochs.
Therefore, we conclude that the model can be quickly trained
to achieve good performance.
2) Embedding Dimensions: We next measure the impact of
the instruction embedding and block embedding dimensions.
Instruction embedding dimension. We vary the instruction
embedding dimension, and evaluate the corresponding AUC
values shown in Figure 12c. We observe that increasing the
embedding dimensions yields higher performance; and the
AUC values corresponding to the embedding dimension higher
than 100 are close to each other. Since a higher embedding
dimension leads to higher computational costs (requiring longer
training time), we conclude that a moderate dimension of 100
is a good trade-off between precision and efﬁciency.
Block embedding dimension. Next, we vary the block embed-
ding dimension, and evaluate the corresponding AUC values
shown in Figure 12d. We observe that the performance of the
models with 10, 30 and 50 block embedding dimensions are
close to each other. Since a higher embedding dimension leads
to higher computational costs, we conclude that a dimension
of 50 for block embeddings is a good trade-off.
3) Network Depth: We then change the number of layers
of each LSTM, and evaluate the corresponding AUC values.
Figure 12e shows that the LSTM networks with two and three
layers outperform the network with a single layer, and the
AUC values for the networks with two and three layers are
close to each other. Because adding more layers increases the