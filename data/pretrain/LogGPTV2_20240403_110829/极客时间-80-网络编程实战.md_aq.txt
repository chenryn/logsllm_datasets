## 阻塞 VS 非阻塞当应用程序调用阻塞 I/O完成某个操作时，应用程序会被挂起，等待内核完成操作，感觉上应用程序像是被"阻塞"了一样。实际上，内核所做的事情是将CPU 时间切换给其他有需要的进程，网络应用程序在这种情况下就会得不到 CPU时间做该做的事情。非阻塞 I/O 则不然，当应用程序调用非阻塞 I/O完成某个操作时，内核立即返回，不会把 CPU时间切换给其他进程，应用程序在返回后，可以得到足够的 CPU时间继续完成其他事情。如果拿去书店买书举例子，阻塞 I/O 对应什么场景呢？你去了书店，告诉老板（内核）你想要某本书，然后你就一直在那里等着，直到书店老板翻箱倒柜找到你想要的书，有可能还要帮你联系全城其它分店。注意，这个过程中你一直滞留在书店等待老板的回复，好像在书店老板这里\"阻塞\"住了。那么非阻塞 I/O呢？你去了书店，问老板有没你心仪的那本书，老板查了下电脑，告诉你没有，你就悻悻离开了。一周以后，你又来这个书店，再问这个老板，老板一查，有了，于是你买了这本书。注意，这个过程中，你没有被阻塞，而是在不断轮询。``{=html}但轮询的效率太低了，于是你向老板提议："老板，到货给我打电话吧，我再来付钱取书。"这就是前面讲到的I/O 多路复用。再进一步，你连去书店取书也想省了，得了，让老板代劳吧，你留下地址，付了书费，让老板到货时寄给你，你直接在家里拿到就可以看了。这就是我们将会在第30 讲中讲到的异步 I/O。这几个 I/O 模型，再加上进程、线程模型，构成了整个网络编程的知识核心。按照使用场景，非阻塞 I/O可以被用到读操作、写操作、接收连接操作和发起连接操作上。接下来，我们对它们一一解读。
## 非阻塞 I/O
### 读操作如果套接字对应的接收缓冲区没有数据可读，在非阻塞情况下 read调用会立即返回，一般返回 EWOULDBLOCK 或 EAGAIN出错信息。在这种情况下，出错信息是需要小心处理，比如后面再次调用 read操作，而不是直接作为错误直接返回。这就好像去书店买书没买到离开一样，需要不断进行又一次轮询处理。
### 写操作不知道你有没有注意到，在阻塞 I/O 情况下，write函数返回的字节数，和输入的参数总是一样的。如果返回值总是和输入的数据大小一样，write等写入函数还需要定义返回值吗？我不知道你是不是和我一样，刚接触到这一部分知识的时候有这种困惑。这里就要引出我们所说的非阻塞 I/O。在非阻塞 I/O的情况下，如果套接字的发送缓冲区已达到了极限，不能容纳更多的字节，那么操作系统内核会**尽最大可能**从应用程序拷贝数据到发送缓冲区中，并立即从write等函数调用中返回。可想而知，在拷贝动作发生的瞬间，有可能一个字符也没拷贝，有可能所有请求字符都被拷贝完成，那么这个时候就需要返回一个数值，告诉应用程序到底有多少数据被成功拷贝到了发送缓冲区中，应用程序需要再次调用write 函数，以输出未完成拷贝的字节。write 等函数是可以同时作用到阻塞 I/O 和非阻塞 I/O上的，为了复用一个函数，处理非阻塞和阻塞 I/O多种情况，设计出了写入返回值，并用这个返回值表示实际写入的数据大小。也就是说，非阻塞 I/O 和阻塞 I/O 处理的方式是不一样的。非阻塞 I/O 需要这样：拷贝→返回→再拷贝→再返回。而阻塞 I/O 需要这样：拷贝→直到所有数据拷贝至发送缓冲区完成→返回。不过在实战中，你可以不用区别阻塞和非阻塞I/O，使用循环的方式来写入数据就好了。只不过在阻塞 I/O的情况下，循环只执行一次就结束了。我在前面的章节中已经介绍了类似的方案，你可以在文稿里看到 writen函数的实现。    /* 向文件描述符 fd 写入 n 字节数 */ssize_t writen(int fd, const void * data, size_t n){    size_t      nleft;    ssize_t     nwritten;    const char  *ptr;     ptr = data;    nleft = n;    // 如果还有数据没被拷贝完成，就一直循环    while (nleft > 0) {        if ( (nwritten = write(fd, ptr, nleft)) = 'a' && c = 'A' && c = 'n' && c = 'N' && c connect_fd = 0;    buffer->writeIndex = buffer->readIndex = buffer->readable = 0;    return buffer;} void free_Buffer(struct Buffer *buffer) {    free(buffer);} int onSocketRead(int fd, struct Buffer *buffer) {    char buf[1024];    int i;    ssize_t result;    while (1) {        result = recv(fd, buf, sizeof(buf), 0);        if (result writeIndex buffer))                buffer->buffer[buffer->writeIndex++] = rot13_char(buf[i]);            if (buf[i] == '\n') {                buffer->readable = 1;  // 缓冲区可以读            }        }    }     if (result == 0) {        return 1;    } else if (result readIndex writeIndex) {        ssize_t result = send(fd, buffer->buffer + buffer->readIndex, buffer->writeIndex - buffer->readIndex, 0);        if (result readIndex += result;    }     if (buffer->readIndex == buffer->writeIndex)        buffer->readIndex = buffer->writeIndex = 0;     buffer->readable = 0;     return 0;} int main(int argc, char **argv) {    int listen_fd;    int i, maxfd;     struct Buffer *buffer[FD_INIT_SIZE];    for (i = 0; i connect_fd > 0) {                if (buffer[i]->connect_fd > maxfd)                    maxfd = buffer[i]->connect_fd;                FD_SET(buffer[i]->connect_fd, &readset);                if (buffer[i]->readable) {                    FD_SET(buffer[i]->connect_fd, &writeset);                }            }        }         if (select(maxfd + 1, &readset, &writeset, &exset, NULL)  FD_INIT_SIZE) {                error(1, 0, "too many connections");                close(fd);            } else {                make_nonblocking(fd);                if (buffer[fd]->connect_fd == 0) {                    buffer[fd]->connect_fd = fd;                } else {                    error(1, 0, "too many connections");                }            }        }         for (i = 0; i connect_fd = 0;                close(i);            }        }    }}第 93 行，调用 fcntl 将监听套接字设置为非阻塞。    fcntl(fd, F_SETFL, O_NONBLOCK);第 121 行调用 select 进行 I/O 事件分发处理。131-142 行在处理新的连接套接字，注意这里也把连接套接字设置为非阻塞的。151-156 行在处理连接套接字上的 I/O 读写事件，这里我们抽象了一个 Buffer对象，Buffer 对象使用了 readIndex 和 writeIndex分别表示当前缓冲的读写位置。
## 实验启动该服务器：    $./nonblockingserver使用多个 telnet 客户端连接该服务器，可以验证交互正常。    $telnet 127.0.0.1 43211Trying 127.0.0.1...Connected to localhost.Escape character is '^]'.fasfasfasfsnfsnfsnfs
## 总结非阻塞 I/O 可以使用在 read、write、accept、connect等多种不同的场景，在非阻塞 I/O 下，使用轮询的方式引起 CPU占用率高，所以一般将非阻塞 I/O 和 I/O 多路复用技术 select、poll等搭配使用，在非阻塞 I/O事件发生时，再调用对应事件的处理函数。这种方式，极大地提高了程序的健壮性和稳定性，是Linux 下高性能网络编程的首选。
## 思考题给大家布置两道思考题:第一道，程序中第 133行这个判断说明了什么？如果要改进的话，你有什么想法？    else if (fd > FD_INIT_SIZE) {    error(1, 0, "too many connections");    close(fd);第二道，你可以仔细阅读一下数据读写部分 Buffer 的代码，你觉得用一个Buffer 对象，而不是两个的目的是什么？欢迎在评论区写下你的思考，我会和你一起交流，也欢迎把这篇文章分享给你的朋友或者同事，一起交流一下。![](Images/5a282807b2a1ff091b7f803e8cef3429.png){savepage-src="https://static001.geekbang.org/resource/image/bf/25/bfc96ae0d8f839919b9d9866cfb8b025.jpg"}
# 23 \| Linux利器：epoll的前世今生你好，我是盛延敏，这里是网络编程实战第 23 讲，欢迎回来。性能篇的前三讲，非阻塞 I/O 加上 I/O多路复用，已经渐渐帮助我们在高性能网络编程这个领域搭建了初步的基石。但是，离最终的目标还差那么一点，如果说I/O多路复用帮我们打开了高性能网络编程的窗口，那么今天的主题------epoll，将为我们增添足够的动力。我在文稿中放置了一张图，这张图来自 The Linux Programming Interface(NoStarch Press)。这张图直观地为我们展示了 select、poll、epoll 几种不同的I/O 复用技术在面对不同文件描述符大小时的表现差异。![](Images/07e29d951d1df55e905a0be4acc0d494.png){savepage-src="https://static001.geekbang.org/resource/image/fd/60/fd2e25f72a5103ef78c05c7ad2dab060.png"}\从图中可以明显地看到，epoll 的性能是最好的，即使在多达 10000个文件描述的情况下，其性能的下降和有 10个文件描述符的情况相比，差别也不是很大。而随着文件描述符的增大，常规的select 和 poll 方法性能逐渐变得很差。那么，epoll究竟使用了什么样的"魔法"，取得了如此令人惊讶的效果呢？接下来，我们就来一起分析一下。
## epoll 的用法在分析对比 epoll、poll 和 select 几种技术之前，我们先看一下怎么使用epoll 来完成一个服务器程序，具体的原理我将在 29 讲中进行讲解。``{=html}epoll 可以说是和 poll 非常相似的一种 I/O 多路复用技术，有些朋友将 epoll归为异步 I/O，我觉得这是不正确的。本质上 epoll 还是一种 I/O多路复用技术， epoll 通过监控注册的多个描述字，来进行 I/O事件的分发处理。不同于 poll 的是，epoll 不仅提供了默认的level-triggered（条件触发）机制，还提供了性能更为强劲的edge-triggered（边缘触发）机制。至于这两种机制的区别，我会在后面详细展开。使用 epoll 进行网络程序的编写，需要三个步骤，分别是epoll_create，epoll_ctl 和 epoll_wait。接下来我对这几个 API详细展开讲一下。