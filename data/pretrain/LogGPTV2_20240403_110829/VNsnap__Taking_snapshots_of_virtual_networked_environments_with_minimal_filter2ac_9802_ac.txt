### 4.2 Taking Snapshot of VIOLIN Running NEMO3D

NEMO3D is a legacy parallel simulation program that typically runs for tens of minutes to several hours. It lacks built-in checkpointing support and is widely used in the nanotechnology community for nano-electric modeling of quantum dots. To execute NEMO3D, we create VIOLINs (Virtual Linux clusters) of varying sizes, with 2, 4, 8, and 16 VMs. The underlying physical infrastructure consists of a cluster of 8 Sunfire V20Z servers connected via Gigabit Ethernet.

- For 2, 4, or 8-VM VIOLINs, each VM runs on a distinct physical host and is allocated 650MB of memory.
- For the 16-VM VIOLIN, there are two VMs per host, each with 650MB of memory.

For each VIOLIN, we run NEMO3D with the same input parameters and trigger the snapshot algorithm at the same stage of NEMO3D execution for Xen checkpointing, VNsnap-disk, and VNsnap-memory implementations. We measure the following metrics on a per-VM basis:
- VM uptime
- VM downtime during the snapshot operation
- TCP backoff experienced by the VM due to snapshot completion time discrepancy

The total disruption period to application execution inside the VIOLIN is the sum of VM downtime and TCP backoff.

**Figure 5** shows the results. The times shown are averages of all VMs in a given VIOLIN from a given experiment. We observe that VNsnap-memory consistently incurs the least disruption (VM downtime + TCP backoff):
- 0.0 seconds for 2-VM VIOLIN
- 0.8 seconds for 4-VM VIOLIN
- 1.4 seconds for 8-VM VIOLIN
- 3.8 seconds for 16-VM VIOLIN

VNsnap-disk also incurs minimal VM downtime but has a higher TCP backoff compared to VNsnap-memory. Xen checkpointing, however, incurs significantly higher VM downtime and overall disruption (from 10 to 35 seconds). In the 16-VM experiment, Xen live checkpointing not only suffers from longer downtime (about 20 seconds) but also scales with the number of VMs performing snapshots simultaneously on the same host (about 20 seconds with two VMs per host vs. about 10 seconds with one VM per host).

To explain why VNsnap-memory leads to a smaller TCP backoff than VNsnap-disk, we present detailed results from the 8-VM VIOLIN experiment. **Figure 6** shows the individual results for each of the 8 VMs. Differences in VM snapshot completion times lead to TCP backoff. The discrepancy among the 8 VMs is more significant for VNsnap-disk (up to 4 seconds) than for VNsnap-memory (less than 1 second). This is because some hosts (e.g., those hosting VMs 3, 6, and 7) have longer disk write latency, leading to noticeable differences in VM snapshot completion times for VNsnap-disk. In contrast, VNsnap-memory does not involve disk writes, resulting in much less discrepancy and TCP backoff.

In all experiments, we validate the semantic correctness of NEMO3D execution by comparing the outputs of the following:
1. An uninterrupted NEMO3D execution
2. A NEMO3D execution during which a VIOLIN snapshot is taken
3. A NEMO3D execution restored from the VIOLIN snapshot

We confirm that all executions generate the same program output.

### 4.3 Taking Snapshot of VIOLIN Running BitTorrent

We also study the impact of VNsnap on a VIOLIN running the peer-to-peer BitTorrent application. The goal is to demonstrate the effectiveness of VNsnap for a VIOLIN running a communication and disk I/O-intensive application that spans multiple network domains. **Figure 7** shows the experiment setup, where the VIOLIN spans two subnets at Purdue University. Our testbed consists of 3 servers in the Computer Science (CS) Department and 8 servers at the Center for Education and Research in Information Assurance and Security (CERIAS).

- In the CS subnet, one host runs a remote VNsnap-memory daemon, and the remaining two hosts run a VIOLIN relay daemon and host two VMs: VM 1 (with 700MB of memory) as a BitTorrent seed and VM 2 (with 350 MB of memory) as an Apache web server and a BitTorrent tracker.
- In the CERIAS subnet, four hosts each host a VM with 1GB of memory that runs as a BitTorrent client or seed. The remaining four hosts each run a VNsnap-memory daemon.

The 6 VMs form the BitTorrent network. To overcome the NAT barrier between the two subnets, we deploy two VIOLIN relays running at the same level as the VIOLIN switches. These relays run in hosts with both public and private network interfaces to tunnel VIOLIN traffic across the NAT.

The goal of the BitTorrent network is to distribute a 650MB file from two seeds (VMs 1 and 6) to all participating clients (VMs 3, 4, and 5). The experiment starts with the two seeds, one in CS and one in CERIAS. We trigger the VIOLIN snapshot when all clients have downloaded almost 50% of the file, with average upload and download rates of about 1350KB/s and 3200KB/s, respectively.

**Figure 8** compares the per-VM snapshot timing between Xenâ€™s live checkpointing and VNsnap-memory. We observe that the total disruption caused by the snapshot (i.e., VM downtime + TCP backoff) is considerably less for VNsnap-memory (all below 2 seconds except VM 3). The disruption periods under Xen live checkpointing range from 15 to 25 seconds. Slower disks in some hosts (i.e., those hosting VMs 3 and 6) cause large discrepancies (up to 4 seconds).

We recommend color printing to view Figures 5, 6, and 8.