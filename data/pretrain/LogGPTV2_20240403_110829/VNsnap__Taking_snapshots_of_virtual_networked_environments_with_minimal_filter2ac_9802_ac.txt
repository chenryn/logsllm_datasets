incur noticeable TCP backoff.
4.2 Taking Snapshot of VIOLIN Running
NEMO3D
NEMO3D is a long-running (tens of minutes to hours),
legacy parallel simulation program without any built-in
checkpointing support. It is widely used by the nanotech-
nology community for nano-electric modeling of quantum
dots. To execute NEMO3D, we create VIOLINs as vir-
tual Linux clusters of varying size (with 2, 4, 8, and 16
VMs). The underlying physical infrastructure is a clus-
ter of 8 Sunﬁre V20Z servers connected by Gigabit Eth-
ernet. For the 2, 4, or 8-VM VIOLIN, each VM runs in
a distinct physical host and is allocated 650MB of mem-
ory. For the 16-VM VIOLIN, there are two VMs per host
each with 650MB of memory. For each VIOLIN, we run
NEMO3D with the same input parameters and trigger the
snapshot algorithm at exactly the same stage of NEMO3D
execution for the Xen checkpointing, VNsnap-disk, and
VNsnap-memory implementations. For each implementa-
tion, we measure, on a per VM basis, the VM uptime and
VM downtime during the snapshot operation as well as the
TCP backoff experienced by the VM due to snapshot com-
pletion time discrepancy. We note that the VM downtime
plus the TCP backoff constitute the actual period of disrup-
tion to application execution inside the VIOLIN.
Figure 5 shows the results 2. The times shown are av-
2We would like to suggest color printing to view Figures 5, 6, and 8.
(a) No snapshot
(b) Xen live checkpointing
(c) VNsnap-disk
(d) VNsnap-memory
Figure 4. The impact of different VM snapshot techniques on TCP throughput in a VIOLIN running
NEMO3D. Traces are obtained from tcpdump.
VM(cid:3)uptime
VM(cid:3)downtime
TCP(cid:3)backoff
50
45
40
35
35
30
25
20
15
15
10
5
0
)
c
e
s
(
(cid:3)
n
w
w
o
d
k
a
e
r
B
(cid:3)
t
o
h
s
p
a
n
S
2(cid:3)VMs
4(cid:3)VMs
8(cid:3)VMs
16(cid:3)VMs
Figure 5. The breakdown of snapshot tim-
ing under different VM snapshot implemen-
tations for 2, 4, 8 and 16-node VIOLINs run-
ning NEMO3D.
erages of all VMs in a given VIOLIN from a given ex-
periment. We observe that VNsnap-memory always incurs
the least disruption (VM downtime+TCP backoff) – more
speciﬁcally 0.0, 0.8, 1.4, and 3.8 seconds for the 2, 4, 8,
and 16-node VIOLINs, respectively. VNsnap-disk also in-
curs minimal VM downtime but incurs higher TCP back-
off than VNsnap-memory (to be explained shortly). Still,
it performs much better than Xen checkpointing, which in-
curs signiﬁcantly higher VM downtime as well as overall
disruption period (from 10 to 35 seconds). The 16-node ex-
periment further indicates that Xen live checkpointing not
only suffers from longer downtime (about 20 seconds vs.
less than 1 second in VNsnap-disk), but the downtime also
scales with the number of VMs that are simultaneously per-
forming snapshot on the same host (about 20 seconds with
two VMs per host vs. about 10 seconds with one VM per
host as in the 2, 4, and 8-node cases).
To explain why VNsnap-memory leads to a smaller TCP
backoff than VNsnap-disk, we present the detailed results
from the 8-VM VIOLIN experiment. Figure 6 shows the
individual result for each of the 8 VMs in the VIOLIN. As
discussed in Section 4.1, differences in VM snapshot com-
pletion times (shown by the upper edges of the “VM down-
time” bars) lead to TCP backoff. As can be seen in Figure
6, the discrepancy among the 8 VMs is more signiﬁcant
for VNsnap-disk (up to 4 seconds – Figure 6(b)) than for
VNsnap-memory (less than 1 second – Figure 6(c)). Our
investigation reveals that some of the hosts (e.g. the ones
hosting VMs 3, 6, and 7) have longer disk write latency
than the others, leading to a noticeable difference in VM
snapshot completion times for VNsnap-disk. On the other
hand, VNsnap-memory does not involve disk writes (only
memory writes) during snapshot and thus results in much
less discrepancy and TCP backoff.
In all experiments, we validate the semantic correctness
of NEMO3D execution by comparing the outputs of the
following: (1) an uninterrupted NEMO3D execution, (2)
a NEMO3D execution during which a VIOLIN snapshot
is taken, and (3) a NEMO3D execution restored from the
VIOLIN snapshot. We conﬁrm that all executions generate
the same program output.
4.3 Taking Snapshot of VIOLIN Running
BitTorrent
We also study the impact of VNsnap on a VIOLIN run-
ning the peer-to-peer BitTorrent application [2]. The rea-
son for choosing this application is to demonstrate the ef-
fectiveness of VNsnap for a VIOLIN running a communi-
cation and disk I/O-intensive application that spans multi-
ple network domains. Figure 7 shows the experiment setup,
where the VIOLIN spans two subnets at Purdue Univer-
sity. Our testbed consists of 3 servers in the Computer
Science (CS) Department and 8 servers at the Center for
Education and Research in Information Assurance and Se-
curity (CERIAS). In the CS subnet, we dedicate one host to
run a remote VNsnap-memory daemon. Of the remaining
two hosts, we use one to run a VIOLIN relay daemon (ex-
VM uptime
VM(cid:3)uptime
VM downtime
VM(cid:3)downtime
TCP backoff
TCP(cid:3)backoff
upt
VM(cid:3)uptime
e
VM(cid:3)downtime
TCP(cid:3)backoff(cid:3)pre(cid:882)snapshot
C bac o p e s aps ot
TCP(cid:3)backoff(cid:3)post(cid:882)snapshot
upt
VM(cid:3)uptime
e
VM(cid:3)downtime
TCP(cid:3)backoff(cid:3)pre(cid:882)snapshot
C bac o p e s aps ot
TCP(cid:3)backoff(cid:3)post(cid:882)snapshot
)
c
e
s
(
(
(cid:3)
n
w
o
d
k
k
a
e
r
B
(cid:3)
t
o
h
h
s
p
a
n
S
25
20
15
10
5
5
0
1
1
2
2
3
3
4
4
5
5
6
6
7
7
8
8
)
c
e
s
s
(
(cid:3)
n
w
o
d
k
k
a
e
r
B
(cid:3)
t
o
o
h
s
p
a
n
S
25
20
15
10
5
5
0
1
1
2
2
3
3
4
4
5
5
6
6
7
7
8
8
)
c
e
s
s
(
(cid:3)
n
w
o
d
k
k
a
e
r
B
(cid:3)
t
o
o
h
s
p
a
n
S
25
20
15
10
5
5
0
1
1
2
2
3
3
4
4
5
5
6
6
7
7
8
8
(a) Xen Live Checkpointing
(b) VNsnap-disk
(c) VNsnap-memory
Figure 6. Per-VM breakdowns of snapshot timing for the 8-node VIOLIN running NEMO3D.
plained shortly) and the other one to host two VMs: VM 1
(with 700MB of memory) runs as a BitTorrent seed while
VM 2 (with 350 MB of memory) runs an Apache web-
server and a BitTorrent tracker. In the CERIAS subnet, we
use four hosts each hosting a VM with 1GB of memory
that runs as a BitTorrent client or seed. The remaining four
hosts each run a VNsnap-memory daemon. The 6 VMs –
two in CS and four in CERIAS – form the BitTorrent net-
work. To overcome the NAT barrier between the two sub-
nets, we deploy two VIOLIN relays running at the same
level as the VIOLIN switches. The VIOLIN relays run in
hosts with both public and private network interfaces so
that they can tunnel VIOLIN trafﬁc across the NAT.
Figure 7. Setup of BitTorrent experiment
The goal of the BitTorrent network is to distribute a
650MB ﬁle from two seeds (VMs 1 and 6) to all participat-
ing clients (VMs 3, 4, and 5). The experiment starts with
the two seeds, one in CS and one in CERIAS. We trigger
the VIOLIN snapshot when all clients have downloaded
almost 50% of the ﬁle. At that time, the average upload
and download rates for each client are about 1350KB/s and
3200KB/s, respectively.
Figure 8 compares the per-VM snapshot timing between
Xen’s live checkpointing and VNsnap-memory. We ob-
serve that the total disruption caused by the snapshot (i.e.
VM downtime+TCP backoff) is considerably less – and at
times negligible – for VNsnap-memory (all below 2 sec-
onds except VM 3 – Figure 8(b)). The disruption peri-
ods under Xen live checkpointing range from 15 seconds
to 25 seconds. Moreover, slower disks in some hosts (i.e.
those hosting VMs 3 and 6) causes large discrepancy (up to