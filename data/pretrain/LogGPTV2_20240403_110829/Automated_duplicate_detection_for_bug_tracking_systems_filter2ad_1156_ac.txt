### Cluster Analysis and Feature Utilization

In the context of our clustering algorithm, a report that stands alone as a singleton is less likely to be a duplicate. This is because it is common for a given bug to have multiple duplicates, and we aim to identify this structure through graph clustering. To further enhance our model, we incorporate easily obtainable surface features from the bug reports, such as self-reported severity, relevant operating system, and the number of associated patches or screenshots [6]. These features, while not as semantically rich or predictive as textual similarity, are still valuable. Categorical features, like the operating system, are encoded using one-hot encoding.

### Experiments

Our experiments are based on 29,000 bug reports from the Mozilla project, spanning an eight-month period from February 2005 to October 2005. The Mozilla project includes various subprojects, such as web browsers, mail clients, calendar applications, and issue tracking systems, all sharing the same bug tracking system. We included all subprojects to ensure the generality of our results.

Mozilla has been under active development since 2002. To avoid start-up corner cases, we selected a time period with a reasonable number of resolved bug reports. In our dataset, 17% of the defect reports had not attained any resolution. Since we use developer resolutions as ground truth, these unresolved reports were excluded. Additionally, we only considered duplicates where the original bug was also in the dataset.

We conducted four empirical evaluations:

1. **Textual Similarity**:
   - Our first experiment demonstrated the lack of correlation between sharing "rare" words and duplicate status. Two bug reports describing the same bug were no more likely to share "rare" words than two non-duplicate reports. This finding motivated the form of the textual similarity metric used in our algorithm.

2. **Recall**:
   - Our second experiment compared our approach with the previous work of Runeson et al. [15]. Each algorithm was presented with a known-duplicate bug report and a set of historical bug reports, and asked to generate a list of candidate originals. If the actual original was on the list, the algorithm succeeded. Our performance was at least as good as the current state of the art.

3. **Filtering**:
   - Our third and primary experiment involved online duplicate detection. We tested the feasibility and effectiveness of using our duplicate classifier as an online filter. We trained our algorithm on the first half of the defect reports and tested it on the second half. We measured the processing time for incoming defect reports and the expected savings and costs of the filter. Costs and benefits were quantified in terms of the number of real defects mistakenly filtered and the number of duplicates correctly identified.

4. **Feature Analysis**:
   - Finally, we applied leave-one-out analysis and principal component analysis to the features used by our model. These analyses addressed the relative predictive power and potential overlap of the selected features.

### Textual Similarity

Our notion of textual similarity uses the cosine distance between weighted word vectors derived from documents. We investigated the use of inverse document frequency (IDF) as a word-weighting factor. IDF is a contextual measure of word importance, based on the assumption that important words will appear frequently in some documents but infrequently across the corpus. The IDF formula is:

\[
\text{IDF}(w) = \log \left( \frac{\text{total documents}}{\text{documents in which word } w \text{ appears}} \right)
\]

While IDF is popular in natural language processing and information retrieval, it resulted in worse performance than a baseline using only non-contextual term frequency for identifying duplicate bug reports. We performed a statistical analysis to determine why IDF was not helpful. We defined the shared-word frequency between two documents as the sum of the IDFs for all shared words, divided by the number of shared words:

\[
\text{shared-word frequency}(d_1, d_2) = \frac{\sum_{w \in d_1 \cap d_2} \text{IDF}(w)}{|d_1 \cap d_2|}
\]

We calculated the shared-word frequency for duplicate-original pairs and close duplicate-unrelated pairs. A Wilcoxon rank-sum test showed that the distribution of shared-word frequency between duplicate-original pairs was not significantly different from that of close duplicate-unique pairs, indicating that IDF was just as likely to relate unlinked report pairs as duplicate-original pairs.

### Recall Rate

Our second experiment applied our algorithm to a duplicate-detection recall task proposed by Runeson et al. [15]. For each duplicate report, the algorithm generated an ordered top list of the most likely original reports. The recall rate was the fraction of instances where the true original appeared in the top list. Our algorithm performed up to 1% better than the state of the art, though the difference was not statistically significant at the p ≤ .05 level.

### Online Filtering Simulation

To measure our algorithm's utility as an online filtering system, we simulated a deployment. The algorithm was trained on the first half of the dataset and tested on the second half. We defined two symbolic costs: Triage (fixed cost for triaging a bug report) and Miss (cost for ignoring an eventually fixed bug report and its duplicates). The total cost was a × Triage + b × Miss. We evaluated our performance relative to the cost of triaging every bug report (11,340 × Triage).

The algorithm was trained using feature extraction, textual preprocessing, pairwise similarity calculations, and graph clustering. We used a heuristic to determine the best similarity cutoff for edges in the graph. This experiment aimed to mitigate overfitting and simulate a real-world deployment.