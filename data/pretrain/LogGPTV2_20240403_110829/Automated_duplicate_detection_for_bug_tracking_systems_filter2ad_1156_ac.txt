a cluster. Intuitively, a report left alone as a singleton by
the clustering algorithm is less likely to be a duplicate. It is
common for a given bug to have multiple duplicates, and we
hope to tease out this structure using the graph clustering.
Finally, we complete our model with easily-obtained sur-
face features from the bug report. These features include the
self-reported severity, the relevant operating system, and the
number of associated patches or screenshots [6]. These fea-
tures are neither as semantically-rich nor as predictive as
textual similarity. Categorical features, such as relevant op-
erating system, were modeled using a one-hot encoding.
5. Experiments
All of our experiments are based on 29,000 bug re-
ports from the Mozilla project. The reports span an eight
month period from February 2005 to October 2005. The
Mozilla project encompasses many different programs in-
cluding web browsers, mail clients, calendar applications,
and issue tracking systems. All Mozilla subprojects share
the same bug tracking system; our dataset includes reports
from all of them. We chose to include all of the subprojects
to enhance the generality of our results.
Mozilla has been under active development since 2002.
Bug reports from the very beginning of a project may not
be representative of a “typical” or “steady state” bug report.
We selected a time period when a reasonable number of bug
reports were resolved while avoiding start-up corner cases.
In our dataset, 17% of defect reports had not attained some
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:19:39 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE56DSN 2008: Jalbert & Weimersort of resolution. Because we use developer resolutions as
a ground truth to measure performance, reports without res-
olutions are not used. Finally, when considering duplicates
in this dataset we restrict attention to those for which the
original bug is also in the dataset.
We conducted four empirical evaluations:
Text. Our ﬁrst experiment demonstrates the lack of cor-
relation between sharing “rare” words and duplicate status.
In our dataset, two bug reports describing the same bug
were no more likely to share “rare” words than were two
non-duplicate bug reports. This ﬁnding motivates the form
of the textual similarity metric used by our algorithm.
Recall. Our second experiment was a direct comparison
with the previous work of Runeson et al. [15]. In this exper-
iment, each algorithm is presented with a known-duplicate
bug report and a set of historical bug reports and is asked
to generate a list of candidate originals for the duplicate. If
the actual original is on the list, the algorithm succeeds. We
perform no worse than the current state of the art.
Filtering. Our third and primary experiment involved
on-line duplicate detection. We tested the feasibility and
effectiveness of using our duplicate classiﬁer as an on-line
ﬁlter. We trained our algorithm on the ﬁrst half of the defect
reports and tested it on the second half. Testing proceeded
chronologically through the held-out bug reports and pre-
dicted their duplicate status. We measured both the time to
process an incoming defect report as well as the expected
savings and cost of such a ﬁlter. We measured cost and ben-
eﬁt in terms of the number of real defects mistakenly ﬁltered
as well as the number of duplicates correctly ﬁltered.
Features. Finally, we applied a leave-one-out analysis
and a principal component analysis to the features used by
our model. These analyses address the relative predictive
power and potential overlap of the features we selected.
5.1 Textual Similarity
Our notion of textual similarity uses the cosine distance
between weighted word vectors derived from documents.
The formula used for weighting words thus induces the sim-
ilarity measure. In this experiment we investigate the use of
inverse document frequency as a word-weighting factor.
Inverse document frequency is a contextual measure of
the importance of a word. It is based on the assumption that
important words will appear frequently in some documents
and infrequently across the whole of the corpus.
Inverse
document frequency scales the weight of each word in our
vector representation by the following factor:
(cid:18)
(cid:19)
IDF(w) = log
total documents
documents in which word w appears
While inverse document frequency is popular in general
natural language processing and information retrieval tasks,
Figure 1. Distribution of shared-word frequencies
between duplicate-original pairs (light) and close
duplicate-unrelated pairs (dark).
employing it for the task of bug report duplicate identiﬁca-
tion resulted in strictly worse performance than a baseline
using only non-contextual term frequency.
We performed a statistical analysis over the reports in our
dataset to determine why the inclusion of inverse document
frequency was not helpful in identifying duplicate bug re-
ports. First, we deﬁne the shared-word frequency between
two documents with respect to a corpus. The shared-word
frequency between two documents is the sum of the inverse
document frequencies for all words that the two documents
have in common, divided by the number of words shared:
shared-word frequency(d1, d2) =
Σw∈d1∩d2IDF(w)
|d1 ∩ d2|
This shared-word frequency gives insight into the effect of
inverse document frequency on word weighting.
We then considered every duplicate bug report and its
associated original bug report in turn and calculated the
shared-word frequency for the titles and descriptions of that
pair. We also calculated the shared-word frequency between
each duplicate bug report and the closest non-original re-
port, with “closest” determined by TF/IDF. This measure-
ment demonstrates that over our dataset, inverse document
frequency is just as likely to relate duplicate-original reports
pairs as it is to relate unlinked report pairs.
We performed a Wilcoxon rank-sum test to determine
whether the distribution of shared-word frequency between
duplicate-original pairs was signiﬁcantly different than that
of close duplicate-unique pairs. Figure 1 shows the two dis-
tributions. The distribution of duplicate-unique pair val-
ues falls to the right of the of distribution of duplicate-
original pair values (with a statistically-signiﬁcant p-value
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:19:39 UTC from IEEE Xplore.  Restrictions apply. 
05010015020025030000.91.82.73.64.55.46.37.2Shared-Word FrequencyFrequency (bug report pairs)DuplicatesNon-DuplicatesInternational Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE57DSN 2008: Jalbert & Weimeris conceptually similar but uses a similarity metric with a
different word weighting equation.
We tested four algorithms using all of the duplicate bug
reports in our dataset as the corpus. Figure 2 shows the re-
call rates obtained as a function of the top list size. The
TF/IDF algorithm uses the TF/IDF weighting that takes
inverse document frequency into account; it is a popular de-
fault choice and serves as a baseline. As presaged by the
experiment in Section 5.1, TF/IDF fared poorly.
We considered the Runeson et al. algorithm with and
without stoplist word ﬁltering for common words. Unsur-
prisingly, stoplists improved performance. It is interesting
to note that while the TF/IDF algorithm is strictly worse at
this task than either algorithm using stoplisting, it is better
than the approach without stoplisting. Thus, an inverse doc-
ument frequency approach might be useful in situations in
which it is infeasible to create a stop word list.
In this experiment, our approach is up to 1% better than
the previously-published state of the art on a larger dataset.
We use our approach as the weighting equation used to cal-
culate similarities in Section 5.3. However, the difference
between the two approaches is not statistically signiﬁcant at
the p ≤ .05 level; we conclude only that our technique is no
worse than the state of the art at this recall task.
5.3 On-Line Filtering Simulation
The recall rate experiment corresponds to a system that
still has a human in the loop to look at the list and make a
judgment about potential duplicates; it does not speak to an
algorithm’s automatic performance or performance on non-
duplicate reports. To measure our algorithm’s utility as an
on-line ﬁltering system, we propose a more complex exper-
iment. Our algorithm is ﬁrst given a contiguous preﬁx of
our dataset of bug reports as an initial training history. It
is then presented with each subsequent bug report in order
and asked to make a classiﬁcation of that report’s duplicate
status. Reports ﬂagged as duplicates are ﬁltered out and
are assumed not to be shown to developers directly. Such
ﬁltered reports might instead be linked to similar bug re-
ports posited as their originals (e.g., using top lists as in
Section 5.2) or stored in a rarely-visited “spam folder”. Re-
ports not ﬂagged as duplicates are presented normally.
We ﬁrst deﬁne a metric to quantify the performance of
such a ﬁlter. We deﬁne two symbolic costs: Triage and
Miss. Triage represents a ﬁxed cost associated with triag-
ing a bug report. For each bug report that our algorithm
does not ﬁlter out, the total cost it incurs is incremented by
Triage. Miss denotes the cost for ignoring an eventually-
ﬁxed bug report and all of its duplicates. In other words,
we penalize the algorithm only if it erroneously ﬁlters out
an entire group of duplicates and their original, and if that
original is eventually marked as “ﬁxed” by the developers
Figure 2. Recall rate of various algorithms on the
Runeson task as a function of top list size. Our algo-
rithm performs up to 1% better than that of Runeson
et al., which in turn performs better than a direct ap-
plication of inverse document frequency (TF/IDF).
of 4×10−7). In essence, in the domain of bug report classi-
ﬁcation, shared-word frequency is more likely to increase
the similarity of unrelated pairs than of than duplicate-
original pairs. Thus, we were able to dismiss shared-word
frequency from consideration as a useful factor to distin-
guish duplicates from non-duplicates. This study motivates
the form of the weighting presented in Section 4.1.2 and
used by our algorithm.
5.2 Recall Rate
Our second experiment applies our algorithm to a
duplicate-detection recall task proposed by Runeson et
al. [15] Their experiment consists of examining a corpus
of bug reports that includes identiﬁed duplicate reports. For
each duplicate report d, the algorithm under consideration
generates an ordered top list containing the reports judged
most likely to be d’s original. This top list could then be
used by the triager to aid the search for duplicate bug re-
ports.
Results are quantiﬁed using a recall rate metric. If the
true original bug report associated with d appears anywhere
in the top list produced by the algorithm when given d, the
algorithm is judged to have correctly recalled the instance
d. The total recall rate is the fraction of instances for which
this occurs. The longer the top list is allowed to be, the
easier this task becomes. For every duplicate bug report, our
algorithm considers every other bug report and calculates
its distance to d using the word-vector representation and
cosine similarity. The reports are then sorted and the closest
reports become the top list. The algorithm of Runeson et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:19:39 UTC from IEEE Xplore.  Restrictions apply. 
0.220.270.320.370.420.470.5212345678910111213141516171819Top List SizeRecall RateOur AlgorithmRunesonTF/IDFRuneson No StoplistInternational Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE58DSN 2008: Jalbert & Weimer(and not “invalid” or “works-for-me”, etc.). If two bug re-
ports both describe the same defect, the algorithm can save
development triage effort by setting aside one of them. We
assume the classiﬁcations provided by developers for the
bug reports in our dataset are the ground truth for measur-
ing performance.
We can then assign our algorithm a total cost of the
form a × Triage + b × Miss. We evaluate our perfor-
mance relative to the cost of triaging every bug report (i.e.,
11, 340 × Triage, since 11,340 reports are processed). The
comparative cost of our model is thus (11, 340 − a) ×
Triage + b × Miss.
In this experiment we simulated a deployment of our al-
gorithm “in the wild” by training on the chronological ﬁrst
half of our dataset and then testing on the second half. We
test and train on two different subsets of bug reports both
to mitigate the threat of overﬁtting our linear regression
model, and also because information about the future would
not be available in a real deployment.
The algorithm pieces we have previously described are
used to train the model that underlies our classiﬁer. First,
we extract features from bug reports. Second, we perform
the textual preprocessing described in Section 4.1. Third,
we perform separate pairwise similarity calculations of both
title and description text, as in Section 4.1.1. This step is
the most time intensive, but it is only performed during the
initial training stage and when the model is regenerated.
Once we have calculated the similarity, we generate
the graph structure for the clustering algorithm, as in Sec-
tion 4.1.3. We use a heuristic to determine the best similar-
ity cutoff for an edge to exist between two bug report nodes.