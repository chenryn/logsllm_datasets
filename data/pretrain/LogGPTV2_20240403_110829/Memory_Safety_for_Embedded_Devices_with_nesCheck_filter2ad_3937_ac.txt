variables in place of dynamic allocation. The ﬁeld f->bar
is aliased to *p, and this time the metadata propagation re-
quires metadata table accesses, as the pointer is in a struct.
The execution resumes in the testMetadataTable function.
The storing of a numerical value inside the array member
of the struct foo_t bla at line 14 is actually translated by
Clang into a sequence of GetElementPtr statements. When-
ever necessary, such instructions are instrumented by dy-
namic runtime checks and metadata table lookups.
Following the execution, the function testDynamically-
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
typedef struct foo {
a ;
int
int * bar ;
} foo_t ;
foo_t myfoo ;
foo_t * t e s t M T _ a u x ( int * p ) {
foo_t * f = & myfoo ;
f - > bar = p ;
return f ;
}
void t e s t M e t a d a t a T a b l e ( int * p ) {
foo_t * f = t e s t M T _ a u x ( p ) ;
(f - > bar ) [2] = 13;
}
void a s s i g n L o o p ( int * p ) {
int i ;
for ( i = 0; i < 4; i ++)
*( p + i ) = i ;
}
void t e s t D y n a m i c A l i a s i n g ( int n ) {
int * p ;
int a [4];
int b [12];
if ( n < 1) p = a ;
else
p = b ;
a s s i g n L o o p (&( p [1]) ) ;
}
int main () {
int * arr = malloc (5 * sizeof ( int ) ) ;
t e s t M e t a d a t a T a b l e ( arr ) ;
t e s t D y n a m i c A l i a s i n g (0) ;
}
Figure 4: Representative example for the stress-
intensive microbenchmark.
Aliasing, conceived to stress-test common dynamic aliasing
scenarios, is ﬁrst instrumented with explicit metadata vari-
ables, as presented in Section 4.1.2. Then, assignLoop()
tries to assign numeric values to the ﬁrst 4 elements of the
array, resulting in an out-of-bounds memory violation. How-
ever, an injected dynamic runtime check at line 19 will catch
the out-of-bounds access to the 4th element of the array, and
the execution will be diverted to a trap function.
5.
IMPLEMENTATION
The implementation of nesCheck leverages the existing
TinyOS compiler toolchain and extends it with custom com-
ponents built on Clang [5] and optimization passes from the
LLVM suite [18]. The technologies used are highlighted next
to each pipeline block in Figure 1.
The nesC source code is initially processed by ncc, the
nesC compiler, that links the diﬀerent nesC components to-
gether through their interfaces and translates the result to a
single C source code ﬁle. The C source is then transformed
into the LLVM Intermediate Representation (IR) language.
Such IR is a well-speciﬁed code representation oﬀering an
abstraction layer between the source programming language
used (nesC/C) and the actual target platform code. Then,
the IR is passed to our nesCheck Static Analyzer, based on
an LLVM target-independent Optimization Pass.
The nesCheck Analysis State Manager component main-
tains the analysis state throughout the diﬀerent steps, and
propagates information between the various components.
Most of the metadata is kept in memory by the Analy-
sis State Manager, and looked up and injected only when
needed for the appropriate instrumentation.
As a last step, the minimal set of required runtime checks
for the memory-manipulating instructions is computed, and
the code is instrumented accordingly. The LLVM IR uses,
in general, two separate instructions for pointer dereferenc-
ing: a GetElementPtr instruction to calculate the memory
address of the location to be accessed, and a Load or Store
instruction to actually access this memory location and, re-
spectively, place the resulting value in a variable or store a
value into the location. nesCheck’s instrumentation adds a
bounds check conditional branch before the GetElementPtr
instruction, and a trap function to be invoked whenever the
runtime check fails, to terminate the execution and reboot
the node, preventing memory corruptions.
Whenever nesCheck statically determines that any exe-
cution of the instruction being instrumented will result in
a failure of the check – i.e., the condition can be statically
determined to be always false – the user is alerted that a
constant memory bug is present, providing her with insights
useful to inspect and ﬁx the bug.
The rest of the pipeline, after the instrumentation, re-
sumes the original TinyOS compilation toolchain, having
the instrumented code go through the gcc compiler to ob-
tain the ﬁnal native binary for the desired target platform.
6. EVALUATION
The TinyOS development platform ships with several sam-
ple applications, such as radio communication, sensing, hard-
ware interaction. As done by most other TinyOS research
works [8, 4, 3, 29, 20], we use these applications as bench-
mark suite for evaluating nesCheck. In our experiments, we
instrument all executed code, including that of the TinyOS
operating system itself. Table 1 provides details on each
program in our benchmark suite. We ﬁrst use these applica-
tions as-is to evaluate the performance overhead. Then, we
evaluate the overall eﬀectiveness of nesCheck by randomly
injecting memory bugs in the benchmark applications and
verifying that all of them are caught statically or at runtime.
We evaluate nesCheck on several static metrics – such as
the number of pointer variables, their inferred type classiﬁ-
cation, and the number of dynamic check instrumentations
– and dynamic metrics – such as the overhead of nesCheck
in terms of program size, memory, execution performance,
and energy consumption.
To evaluate performance, we compiled the applications for
TOSSIM [20], a discrete event simulator, de facto-standard
tool for TinyOS WSNs. TOSSIM simulates the behavior of
TinyOS accurately down to a very low level and precisely
times interrupts. This allowed us to perform the evalua-
tion in a controlled environment, through repeatable exper-
iments, and to increase the number of runs for each experi-
ment, while still maintaining a realistic distributed embed-
ded software execution. Each of the evaluation results has
been obtained by averaging 25 independent runs of each test.
Type Inference. The results in Figure 5 show that,
on average, 81% of the variables are classiﬁed as Safe, 13%
Application
BaseStation
Blink
MultihopOscilloscope
Null
Oscilloscope
Powerup
RadioCountToLeds
RadioSenseToLeds
Sense
LOC
5684
5505 Blinks the 3 LEDs on the mote.
Description
Simple Active Message bridge between the serial and radio links.
11728 Data collection: samples default sensor, broadcasts a message every few readings.
4261 An empty skeleton application, useful to test the build environment functionality.
6868 Data collection: radio broadcasts a message every 10 readings of default sensor.
4306 Turns on red LED on powerup, to test deploy of app on hardware mote.
6751 Broadcasts a 4Hz counter and displays every received counter on the LEDs.
6808 Broadcasts default sensor readings, displays every received counter on the LEDs.
5699 Periodically samples the default sensor and displays the bottom bits on the LEDs.
Table 1: TinyOS standard applications used as benchmark for nesCheck’s evaluation.
Figure 5: Pointer classiﬁcation results for the
TinyOS sample apps benchmark.
Figure 6: Code size and performance overhead for
the instrumented TinyOS apps, including TOSSIM.
as Sequence, and 6% as Dynamic. A large number of dy-
namic runtime checks can thus already be skipped as imme-
diate consequence of the type system inference. Note that,
since the analysis is conservative, some pointers classiﬁed
as dynamic might not be so; however, as shown in the per-
formance evaluation afterwards, this does not degrade the
eﬃciency of our approach.
The average total number of analyzed variables, across all
the TinyOS sample applications in the benchmark, is 3, 633,
a small number that further supports our design choice of
whole-program static analysis.
Code Size and Performance Overhead. We inves-
tigate the overhead of nesCheck’s instrumentation in terms
of code size and performance, and the results are shown in
Figure 6. The programs in the benchmark total to 57, 610
lines of code. The size overhead is measured in additional
bytes of the memory-safe executable produced by nesCheck
vs.
the uninstrumented one, both including the code for
the TOSSIM simulator infrastructure. The code size of the
uninstrumented programs averages to 228, 761 bytes, and
the instrumentation adds only 12, 201 bytes (5.3%) of over-
head on average. This result shows that nesCheck is suitable
for the instrumentation of programs to be deployed even on
devices very constrained in ROM.
We also measure the performance overhead of nesCheck
through the TOSSIM simulator for TinyOS. This tool is used
by a simulation driver program by repeatedly asking it to
execute the next event from the simulation queue. The dura-
tion of each event and the total number of events depend on
the complexity of the computation to be executed. There-
fore, we measure the overhead of nesCheck’s instrumenta-
tion by ﬁxing the total simulation time to 30 real seconds,
running the simulation of the original and instrumented ap-
plications, and then measuring the number of simulated
seconds actually executed.
In three cases (BaseStation,
Null and PowerUp), since the applications are merely sam-
ple “skeleton” programs to guide developers, no real events
were happening after the initial program startup. There-
fore, for those programs the reported overhead is 0, and we
do not consider them in our averages for the performance
overhead. For all the other applications that continuously
process events, we observe that TOSSIM executes more sim-
ulated seconds (in the span of 30 real seconds) for simpler
programs than for more complex ones. For example, the
simple Blink program is executed for 120185.86 simulated
seconds, while the more complex RadioSenseToLeds pro-
gram only reaches 6878.08 simulated seconds (both unin-
strumented). In fact, this conﬁrms the intuition that fewer
events can be processed in the same time span when the
computation of each event is more complex. On average,
nesCheck introduces a performance overhead of 6.2%. We
note that the maximum overhead (incurred by the Blink
application) is still quite low, at 8.4%. We believe that this
overhead is acceptable for WSN applications.
27932239624018013216180831263112223929534393809153135023154914933834702151912591852221852222221912100%10%20%30%40%50%60%70%80%90%100%BaseStationBlinkMultihopOscilloscopeNullOscilloscopePowerupRadioCountToLedsRadioSenseToLedsSenseAverage Safe Seq Dyn5.21%4.56%7.40%5.59%4.53%5.56%4.62%4.59%4.45%5.33%0.00%8.39%0.47%0.00%0.90%0.00%1.17%0.82%5.14%6.25%0%1%2%3%4%5%6%7%8%9%BaseStationBlinkMultihopOscilloscopeNullOscilloscopePowerupRadioCountToLedsRadioSenseToLedsSenseAverageCode Size OverheadPerformance OverheadFigure 7: Metadata table entry lookups vs. actual
metadata table entries required by the instrumen-
tation.
Figure 8: RAM occupation of uninstrumented pro-
grams and memory overhead of nesCheck (all in
bytes).
Memory Overhead. As discussed in Section 4.1.3, some
of the pointers require entries in a separated metadata table.
We thus measure the impact of this additional data on the
memory of the embedded devices. Figure 7 and Figure 8
present our results on the memory overhead of nesCheck for
the TinyOS applications benchmark. In particular, Figure 7
shows the number of metadata lookups added to the code
and the number of actual metadata table entries required
for each application. On average, nesCheck added only 90
metadata table entry lookup instrumentation points during
the instrumentation. Given the SSA form, there is a direct
relationship between the number of memory accesses and
the number of analyzed variables; therefore, we compare
the number of metadata lookups with the total number of
variables analyzed by nesCheck, and see that it amounts to
just 2%. When only comparing to the Dynamic pointers, it
amounts to 41%, which still represents a signiﬁcant mem-
ory saving. Many of such lookups, furthermore, refer to the
same logical variable, and thus point to the same entry in
the metadata table. Thus, in fact, only 32 distinct entries
are needed on average in the metadata table, constituting
approximately 1/3 of the total lookup instrumentations for
each program. With these collected metrics, we measure the
eﬀective RAM overhead of nesCheck for each application by
comparing the RAM occupation of the uninstrumented pro-
gram – as reported by the nesC toolchain when compiling
for the TelosB motes platform [23] – with the size of the
metadata table in the instrumented version – representing
the eﬀective memory overhead. Figure 8 presents both these
metrics side by side for ease of presentation. The numbers
vary greatly for the diﬀerent applications, as the number
of metadata table entries is completely dependent on the
data structures used by each program. However, the aver-
age overhead is 16%, and in all cases the total memory re-
quirement remains signiﬁcantly below the 10kb RAM limit