of both of these parameters—
our experiments presented so
far use the default conﬁgura-
tion on NIC1, which roughly boils down to a typical interrupt delay of about
120 µs.
Fig. 10. Impact of ICparams in training set
Diﬀerent ICparam lead to diﬀerent “spike-dips” patterns in the receive gaps,
in terms of the heights of the spikes, as well as the distances between neighbor-
ing spikes. We next study the impact of having diﬀerent ICparam in the training
vs testing data sets—a model learned with one parameter may fail on pstreams
that encounter another. We ﬁrst apply the previously learned ML model (with
ICparam=default) to testing scenarios when rx-usecs is set to a speciﬁc value—
ranging from 2 µs to 300 µs. Figure 10 compares the estimation accuracy of BASS
and the ML model (left two bars in each group). The box plots the 10 %–90 %
relative error, and the extended bar plots the 5 %–95 % error. We ﬁnd that BASS
severely over-estimates AB when interrupt delay is signiﬁcant (rx-usecs ≥ 200 µs),
while the ML model yields better accuracy. This highlights the model of carefully
studying the impact of ICparam—this factor was not considered in the BASS eval-
uations in [10]. We also ﬁnd that the ML model consistently under-estimates AB
when rx-usecs=300 µs.
Machine learning performs best when the training set is representative of
conditions encountered during testing. To achieve this, we create a training set
that for each ICparam, include 5000 pstream samples that encounter it—we
denote this as “ALL-set”. As shown in Fig. 10, the model learned from “ALL-
set” reduces error to within 10 % for most pstreams that encounter extreme
rx-usecs values. In practice, however, all possible conﬁgurations of ICparam at
a receiver NIC may not be known. We next ask: does there exist a model, which
is trained with only a limited set of ICparams, but which manages to apply to all
conﬁgurations? To study this, we minimize the training set to only include two
extreme values (2 us and 300 us), in addition to the default setting. We refer to
this set as “3sets”. Figure 10 shows that “3sets” is suﬃcient to train an accurate
ML model, which provides similar accuracy as “ALL-set”.
408
Q. Yin and J. Kaur
5.4 Robustness and Portability Across NICs
Diﬀerent NIC platforms may interpret
and implement interrupt coalescence
diﬀerently. For instance, NIC-2 relies
an adaptive interrupt behavior, even
though it allows us to specify “rx-
usecs” and “rx-frames”. Figure 11 illus-
trates that on this NIC by plotting
the distribution of number of frames
coalesced per interrupt—we ﬁnd that
“rx-frames” takes no eﬀect when rx-
usecs ≤ 12µs. But “rx-usecs” is not
respected once it exceeds 12 µs; the dis-
tribution mainly depends on rx-frames.
This unpredictability is quite diﬀerent from what we observed on NIC1—we next
study if our ML framework will work on such a NIC as well.
Fig. 11. Interrupting Behavior on NIC2
We repeat
the experi-
ments of Sect. 5.3, but use
NIC2 instead of NIC1 for
collecting both the training
and testing data. We con-
sider the following ICparams
for NIC-2: rx-usecs from 2 to
10 µs, and rx-frames from 2
to 20 (rx-usecs=100). Mod-
els are learned from train-
ing sets consisting of diﬀerent
combinations of ICparams
in training scenarios, namely,
the “Default”, “All-set”, and the “3sets”(default,rx-frames=2 and rx-
frames=20). Figure 12 plots the estimation errors for these three environments.
We ﬁnd that compared with Fig. 10, the estimation error is generally higher on
NIC-2 than NIC-1, presumably due to greater unpredictability in its interrupt-
ing behavior. As before, the “3sets” on NIC-2 outperforms BASS signiﬁcantly,
and gives comparable accuracy as “All-set”—which agrees with our observation
with NIC-1.
Fig. 12. Impact of ICparams on NIC-2
Cross-NIC Validation. To investigate the portability of a learned model
across NICs, we next perform a cross-NIC validation: the model trained with
data collected using one NIC is tested on data collected on a diﬀerent NIC. We
use only ICparam=3sets, and plot the results in Fig. 13(a) and (b). In general,
we ﬁnd that the cross-NIC models generally give comparable accuracy as models
trained on the NIC itself. The notable exceptions occur for extreme values of
ICparam— rx-usecs=300 µs on NIC1 and rx-frames=20 in NIC2.
Can Machine Learning Beneﬁt Bandwidth Estimation at Ultra-high Speeds?
409
Fig. 13. Cross-NIC evaluation
5.5 Implementation Overhead
Time(s)
1.0e-4
boost
7.7e-6
248 KB
the cost of
Table 2. Per-pstream evaluation overhead
forest
1.7e-6 2.1e-6
BASS Random
AdaBoost Gradient
Single-
rate N=48
CPU
The beneﬁts of our ML
framework are achieved
system
at
overhead in the
test-
ing phase8—the whole
model has to be loaded
into memory,
resulting
in more memory usage;
also, the estimation time
in testing phase increases
with model complexity.
Table 2 lists the memory
and CPU cost incurred
by
diﬀerent models
trained with ICparam=3sets, for generating a single estimate. The memory
usuage shown is the relative increment compared with BASS. We ﬁnd that
GradientBoost reports similar costs for both single-rate and multi-rate probing
frameworks. For multi-rate probing, it takes comparable CPU usuage as BASS,
and only 260 KB more memory, which is negligible for modern end hosts with
gigabits of RAM.
3.3 MB
AdaBoost Gradient
boost
3.8 MB
-
BASS Random
forest
Memory
Multirate
N=48
8.1e-6 2.5e-5
6.3e-5
7.2e-6
Memory
-
237 MB 2.5 MB
260 KB
CPU
time(s)
Although the above numbers are implementation-speciﬁc, it is important
to understand the implementation complexity. In our evaluations, the oﬄine-
learned GradientBoost model consists of 100 base estimators, each with a deci-
sion tree with height less than 3— the memory cost of maintaining 100 small
trees, as well as the time complexity in tree-search (upper-bounded by 300 com-
parisons), are both aﬀordable in modern end-systems, in both user and kernel
space. In practice, network operators can program the training process with any
preferred ML library and store the learned model. The stored model contains
8 Since models are trained oﬀ-line, the training overhead is not of concern.
410
Q. Yin and J. Kaur
parameters that fully deﬁne the model structure —thus, it can be easily ported
to other development platforms. Even a Linux kernel module, such as the ones
used in bandwidth-estimation based congestion-control [3], can load the model
during module initialization, and can faithfully reconstruct the entire model in
order to estimate AB.
6 Conclusion
In this paper we apply ML techniques to estimate bandwidth in ultra-high speed
networks, and evaluate our approach in a 10 Gbps testbed. We ﬁnd that super-
vised learning helps to improve estimation accuracy for both single-rate and
multi-rate probing frameworks, and enable shorter pstreams than the state of
the art. Further experiments show that: (i) a model trained with more bursty
cross traﬃc is robust to traﬃc burstiness; (ii) the ML approach is robust to inter-
rupt coalescence parameters, if default and extreme conﬁgurations are included
in training; and (iii) the ML framework is portable across diﬀerent NIC plat-
forms. In further work, we intend to conduct evaluations with more NICs from
diﬀerent vendors, and investigate the practical issues of generating training traf-
ﬁc in diﬀerent networks.
References
1. Dykes, S.G., et al.: An empirical evaluation of client-side server selection algo-
rithms. In: INFOCOM 2000 (2000)
2. Aboobaker, N., Chanady, D., Gerla, M., Sanadidi, M.Y.: Streaming media con-
gestion control using bandwidth estimation. In: Almeroth, K.C., Hasan, M. (eds.)
MMNS 2002. LNCS, vol. 2496, pp. 89–100. Springer, Heidelberg (2002)
3. Konda, K.: RAPID: shrinking the congestion-control timescale. In: INFOCOM.
IEEE (2009)
4. Jain, D.: Pathload: a measurement tool for end-to-end available bandwidth. In:
PAM (2002)
5. Ribeiro, V., et al.: pathchirp: Eﬃcient available bandwidth estimation for network
paths. In: PAM, vol. 4 (2003)
6. Cabellos-Aparicio, A., et al.: A novel available bandwidth estimation and tracking
algorithm. In: NOMS. IEEE (2008)
7. Shriram, A., Kaur, J.: Empirical evaluation of techniques for measuring available
bandwidth. In: INFOCOM. IEEE (2007)
8. Kang, S.-R., Loguinov, D.: IMR-pathload: robust available bandwidth estimation
under end-host interrupt delay. In: Claypool, M., Uhlig, S. (eds.) PAM 2008. LNCS,
vol. 4979, pp. 172–181. Springer, Heidelberg (2008)
9. Kang, S.R., Loguinov, D.: Characterizing tight-link bandwidth of multi-hop paths
using probing response curves. In: IWQoS. IEEE (2010)
10. Yin, Q., et al.: Can bandwidth estimation tackle noise at ultra-high speeds?. In:
ICNP. IEEE (2014)
11. Strauss, J., et al.: A measurement study of available bandwidth estimation tools.
In: The 3rd ACM SIGCOMM Conference on Internet Measurement (2003)
Can Machine Learning Beneﬁt Bandwidth Estimation at Ultra-high Speeds?
411
12. Lee, K.-S.: SoNIC: precise realtime software access and control of wired networks.
In: NSDI (2013)
13. Rizzo, L.: netmap: A novel framework for fast packet I/O. In: USENIX Annual
Technical Conference, pp. 101–112 (2012)
14. Prasad, R., Jain, M., Dovrolis, C.: Eﬀects of interrupt coalescence on network
measurements. In: Barakat, C., Pratt, I. (eds.) PAM 2004. LNCS, vol. 3015, pp.
247–256. Springer, Heidelberg (2004)
15. Dietterich, T.G.: Machine-learning research (1997)
16. Nguyen, T.T., et al.: A survey of techniques for internet traﬃc classiﬁcation using
machine learning. Commun. Surv. Tutor. 10(4), 56–76 (2008)
17. Zou, H., et al.: Regularization, variable selection via the elastic net. J. R. Stat.
Soc.: Ser. B (Stat. Methodol.) 67(2), 301–320 (2005)
18. Liaw, A., et al.: Classiﬁcation and regression by randomforest. R News 2(3), 18–22
(2002)
19. Freund, Y., et al.: A decision-theoretic generalization of on-line learning and an
application to boosting. J. Comput. Syst. Sci. 55(1), 119–139 (1997)
20. Friedman, J.H.: Greedy function approximation: a gradient boosting machine. Ann.
Stat. 29(5), 1189–1232 (2001)
21. Cortes, C., et al.: Support-vector networks. Mach. Learn. 20(3), 273–297 (1995)
22. Pedrogosa, F., et al.: Scikit-learn: machine learning in python. J. Mach. Learn.
Res. 12, 2825–2830 (2011)
23. Barford, P., Crovella, M.: Generating representative web workloads for network and
server performance evaluation. ACM SIGMETRICS Perform. Eval. Rev. 26(1),
151–160 (1998)
24. Turner, A.A., Bing, M.: Tcpreplay (2005)
25. Dietterich, T.: An experimental comparison of three methods for constructing
ensembles of decision trees: bagging, boosting, and randomization. Mach. Learn.
40(2), 139–157 (2000)