and bots will constantly change IP addresses. 
However, they will not move very far. 
There is a tendency of bad behavior being dependent on the topology of the 
Internet, that is if you have machines that are engaging on a specific type of 
behavior in the internet, their neighbors (as far as Organizational and ISP 
Autonomous Systems (AS) are concerned) are more likely to engage in this 
behavior.  
This thesis is not only supported by common knowledge, one of the greatest 
inspirations of this work was a PhD Thesis paper by Moura called "Internet Bad 
Neighborhoods"[5] and a companion paper called "Internet Bad Neighborhoods 
Aggregation"[6] that talk about some experiments he and his collaborators have 
performed using traffic from RNP, the Brazilian academic research network. 
Same conclusions can be drawn by Google's Recent Safe Browsing Transparency 
Report [7]. 
But they will eventually change over time.  
This information on bad behavior is time-sensitive. If a specific IP address 
attacked your organization a week ago, this is probably relevant information for 
the monitoring team if they should commit resources to investigating events 
Defending Networks with Incomplete Information: A Machine Learning Approach 
DefCon 21 – 2013 
Page 7 
from this IP address or nearby IP addresses. If it attacked yesterday, that is 
certainly more relevant to the team. 
All Security Monitoring professionals will remember a few occasions when they 
would be on duty and "recognized" an IP address on the dashboard or event 
stream and some kind of "intuition" clicked that that was an event they had seen 
before and it should be followed up. 
This is the intuition this algorithm intends to capture, using the data gathered 
from the DShield data to predict if an IP address is likely to be malicious or not. 
Feature Engineering 
In order to create the features we need to the machine learning algorithm, we go 
through the following steps: 
1. Initially we cluster the events we have by specific behaviors, such as blocked 
attempts on port 3389, 22 or 80 or port scans (where the same IP address 
attempts to connect to multiple ports). These clusters of events are deemed 
positive for bad behavior in our model, and each of the IP addresses get a 
rank of one for that day. These clusters from this example were arbitrarily 
chosen, but could have been selected by an unsupervised learning algorithm. 
2. After we have identified these "bad" IP addresses per day, we need to add up 
their scores. However, we need to attenuate the contribution from each day 
as the days pass, as described in our time-based intuition component. For 
that, we apply an exponential function to the ranks of the IP address on each 
date, so that it will have a specific half-life.  
a. One of the behaviors we get from this is that if you choose a half-life 
around 5 to 7 days the contribution from IP addresses that last 
showed up their contribution will all but disappear in about 90 days 
or so. This helps the algorithm take in account changes in the Internet 
topology, or even different threat actors that might be starting to 
target the organization or getting bored and moving elsewhere. 
3. Now that we have ranks for each IP address over time, we group them in 
sensible ways to exploit the neighborhood behaviors we discussed on the 
intuition. We compose the IP addresses by arbitrary net blocks (such as /16 
or /24) and by which organizational and ISP AS they belong, being careful to 
normalize the rank by the number of possible IP addresses on the range. The 
contribution from each IP address must be proportional to the total number 
of IP addresses we have on the specific grouping. 
With these calculations, we have all the features we need to train a classifier 
based on these IP address ranks where we select a group of these features  
Training the model 
Given the rank calculations on each IP addresses, we can select a group of IP 
addresses from the DShield summary events and from contributed blocked logs. 
We select a few tens of thousands of observations assigning them the label of 
malicious event. 
Defending Networks with Incomplete Information: A Machine Learning Approach 
DefCon 21 – 2013 
Page 8 
However, in order to properly train the algorithm, we require benign IP 
addresses as well. Otherwise any algorithm can just infer the trivial case that 
"everything is malicious".  
In other to provide sufficient  non-malicious IP addresses to train the model, we 
made use of the lists of Top 1 Million domains provided by Alexa [8] and the 
Google Safe Browsing [7] initiative. These are not perfect sources and malware 
domains have been know to creep in, specially when you approach the end of the 
list. We have found that the initial 20.000 to 40.000 entries hold really well, and 
provide a good diversity of origin countries. This helps the model not trivialize 
on specific regions of the world as bad actors given the predominance of US-
based log sources on the DShield sampling. 
With the dataset created with the selected features, it is just a matter of using 
your favorite classification algorithm. After some experimentation, we have 
selected Support Vector Machines[9], which do a very good job in separating 
non-linear groups when the features are numeric (which is our case here). We do 
not go into the details of the specific math machinery behind this, as it would 
deviate from the point of this document, but this specific algorithm can be 
trivially implemented with scientific languages or libraries in Java, Python or R 
[10]. 
As in many problems in analytics and machine learning, getting to a good answer 
is now usually the problem. The machinery is all there, but you need to ask a 
good question. 
Results 
The model was trained using a technique called cross-validation.[11] That 
basically means that part of the training data of the model is not used to train the 
model, and is actually used to validate the results from the model trained on the 
other part of the data (which is then called the validation set). So in machine 
learning terms, if you do a 10-fold cross-validation, you are effectively dividing 
your dataset in 10 equally sized subsets, and training your model 10 times, 
always leaving one of the subsets out of the training. You then predict what the 
subset classification would be if you did not know it already using this recently 
trained model.  The average of the classification errors you get from each subset 
(what percentage you got wrong) is called the cross-validation error, which is a 
good estimation of the model error on new data. [11] 
For the model described above, using DShield data for the last 6 months, we are 
able to reach an 85 to 95% of cross-validation accuracy.  
 The results vary from the way we cluster the "bad behavior" of the data: 
for instance, it is much more efficient when we are specifically targeting 
ports 3389 or port 80, but is on the low ends of the results when we 
target port 22.  
 There seems to be a high correlation on the amount of observations on 
the DShield data with this predictive power, as it is very skimpy on port 
22, for example, and many days’ worth of data have to be incorporated 
into the model for it to have enough data points to beat the 
"dimensionality issue" we described before. However we cannot say for 
sure if this is case without more data to test the models with. 
Defending Networks with Incomplete Information: A Machine Learning Approach 
DefCon 21 – 2013 
Page 9 
When you generalize a model to completely new data, it is normal that a model 
loses accuracy, and sometimes it can be a deal breaker. However as we tested the 
trained models on log data provided by volunteers on the respective days of the 
trained models, they showed a 80 to 85% of true positive ratings (sensitivity) on 
items that had previously marked as malicious and 85% to 90% true negative 
ratings (specificity) [12].  
Statistically speaking, that provides an odds likelihood [13]that an event 
pinpointed by the model in this example has 5.3 to 8.5 times to be malicious than 
one that did not. That on itself could greatly assist the monitoring team that 
should be reviewing these logs. 
Additional details on the accuracy, true positive and true negative ratios for the 
different experiments will be made available in the presentation itself during the 
conference. 
Future Direction and Improvements 
The main challenges involve the gathering of more historical data to improve the 
performance of the model further. Additionally, using IP addresses for the 
correlation makes the detection susceptible to diversionary techniques such as 
open proxies, Tor and address spoofing (particularly for UDP), making those 
sources appear malicious as well over time if they are used for malicious 
purposes. 
In order to develop this concept further, we have put together a project called 
MLSec (Machine Learning Security)[14] at http://mlsecproject.org, created to 
evolve new Machine Learning algorithms focused on assisting Information 
Security Monitoring. 
We set up a free service where individuals and organizations submit logs 
extracted from their SIEMs or firewalls and have access to automated reports 
from the algorithms to pinpoint points of interest on the network. Their feedback 
on the detected events also helps us fine-tune the algorithms over time. 
The service is being scaled out to accept more participants to contribute with log 
data and feature requests to improve the current models and to develop new 
ones. 
Acknowledgments and thanks 
The author would like to thank: 
- 
Sans Technology Institute, and their DShield Project 
- 
Team Cymru, for their IP to ASN Mapping project and Bogon Reference 
- 
MaxMind for their GeoLite data (http://www.maxmind.com) 
Defending Networks with Incomplete Information: A Machine Learning Approach 
DefCon 21 – 2013 
Page 10 
References 
[1] "Sci vs. Sci: Attack Vectors for Black-hat Data Scientists, and Possible 
Countermeasures" - Joseph Turian - 
http://strataconf.com/strata2013/public/schedule/detail/27213 
[2] "Antifragile: Things That Gain from Disorder" - Nassim Nicholas Taleb - 
http://www.randomhouse.com/book/176227/antifragile-things-that-gain-
from-disorder-by-nassim-nicholas-taleb 
[3] SANS Technology Institute - https://www.sans.org/ 
[4] DShield API - SANS Internet Storm Center - https://isc.sans.edu/api/ 
[5] Internet Bad Neighborhoods – G. Moura 
http://www.utwente.nl/en/archive/2013/03/bad_neighbourhoods_on_the_inte
rnet_are_a_real_nuisance.doc/ 
[6] Internet Bad Neighborhoods Aggregation - 
http://wwwhome.cs.utwente.nl/~sperottoa/papers/2012/noms2012_badhood
s.pdf 
[7]: https://www.google.com/transparencyreport/safebrowsing/ 
[8]: http://www.alexa.com/topsites 
[9]: Support Vector Machines for Classification - Dmitriy Fradkin and Ilya 
Muchnik - http://paul.rutgers.edu/~dfradkin/papers/svm.pdf 
[10] R Core Team (2013). R: A language and environment for statistical 
computing. R Foundation for Statistical Computing, Vienna, Austria. URL 
http://www.R-project.org/. 
[11] Cross-validation (statistics) - https://en.wikipedia.org/wiki/Cross-
validation_(statistics) 
[12] Sensitivity and Specificity - 
http://en.wikipedia.org/wiki/Sensitivity_and_specificity 
[13] Likelihood ratios in diagnostic testing - 
http://en.wikipedia.org/wiki/Likelihood_ratios_in_diagnostic_testing 
[14] MLSec Project - http://mlsecproject.org/