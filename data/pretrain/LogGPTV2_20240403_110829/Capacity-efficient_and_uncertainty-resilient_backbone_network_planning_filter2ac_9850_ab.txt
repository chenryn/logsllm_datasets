### Hose-Based Capacity Planning

#### 3. Overview of the Capacity Planning Problem and System Design

In this section, we provide an overview of the capacity planning problem and our system design. Table 1 lists the notations used throughout the paper.

**Figure Descriptions:**
- **Figure 2:** Hose traffic reduction.
- **Figure 3:** Total traffic distribution of Hose vs. Pipe.
- **Figure 4:** Coefficient of Variation with Pipe vs. Hose traffic.
- **Figure 5:** Service traffic from DC regions B and C to A.
- **Figure 6:** System Architecture.

**Coefficient of Variation:**
The coefficient of variation (CV) is defined as the standard deviation of the traffic demand divided by the mean. Figure 4 illustrates the CV for the total daily peak traffic in the backbone. The relative traffic dispersion in the Hose model is significantly smaller than in the Pipe model, with a shorter tail. This results in a more stable signal for planning and simplifies traffic forecasting. Consequently, the Hose model enables easier network scaling, similar to storage and compute resources, where a node can accurately predict its future growth without being affected by interactions with other nodes in the network.

**Adaptation to Service Evolution:**
Services evolve over time in production due to various factors such as changes in service behavior, re-labeling of Quality of Service (QoS) classes, load balancing, new service launches, and more. Figure 5 provides an example from the user database (UDB) service at Facebook. Due to resource and operational constraints, UDB servers are located in only a few regions, and regions without UDB rely on a caching service called Tao [2] to fetch data from nearby UDB regions. The figure shows the amount of Tao traffic flowing from UDB regions B and C to UDB-less region A. Significant traffic changes occurred when the primary UDB region was switched from B to C, with a canary test on a few shards on 03/05 and a complete policy change on 03/09. These incidents caused several Tbps of traffic shifts, which would have been problematic for a Pipe model. In contrast, the Hose ingress traffic at region A experienced minimal disruption because the total traffic volume remained constant. The traffic aggregation nature of the Hose model makes it more resilient to service changes, making it a future-proof solution for network planning.

**Network Model:**
Our backbone network connects multiple Data Centers (DCs) and Points of Presence (PoPs) using IP routers over a Dense Wavelength Division Multiplexing (DWDM) optical network. The network is represented as a two-layer graph:
- **IP Network:** \( G = (V, E) \), where vertices \( V \) are backbone routers and edges \( E \) are IP links.
- **Optical Network:** \( G' = (V', E') \), where vertices \( V' \) are Optical Add-Drop Multiplexers (OADMs) and edges \( E' \) are fiber segments.

For each IP link \( e \in E \), \( FS(e) \) represents the set of fiber segments that \( e \) rides over, forming a path on the optical topology. The IP link \( e \) consumes a portion of the spectrum on each fiber segment \( l \in E' \). For example, a 100Gbps IP link realized using Quadrature Phase Shift Keying (QPSK) modulation can consume 50GHz of spectrum over all fiber segments in its path. The relationship between IP capacity and optical spectrum is detailed in Section 5.1.

**Failure Model:**
We consider a set of fiber failures in the backbone. Any IP link \( e \in E \) over the failed fibers will be down. To ensure desired reliability, we pre-define a set of planned failures \( R \). The production network should have sufficient capacity to route all service traffic for each failure \( r \in R \). Detailed resilience policies in capacity planning are presented in Section 5.2.

**Traffic Forecast:**
Capacity planning depends on projected future traffic demand. Instead of modeling the organic growth of link-wise traffic as in ISP networks, content providers typically forecast future traffic demand per service based on service profiling. This is because services, as content generators, provide a more reliable source of truth for traffic demand. For inter-DC traffic, service teams calibrate server utilization, especially CPU utilization, to devise service growth plans under the server budget allocated by the company. They provide service scaling factors, which are applied to current service traffic to form future demands. For PoP-DC traffic, we model user growth and cache misses at PoPs to predict the amount of content retrieval between PoPs and different DCs. The demands can be aggregated in different ways, such as per-site-pair basis for traditional Pipe-based planning and per-site basis for Hose-based planning.

**Table 1: Notations**

| Definition | Symbol |
| --- | --- |
| The IP topology with backbone routers and IP links | \( G = (V, E) \) |
| The optical topology with OADMs and fiber segments | \( G' = (V', E') \) |
| The set of fiber segments which IP link \( e \) goes through | \( FS(e) \) |
| The number of sites (DCs and PoPs combined) in the backbone | \( N \) |
| A \( N \times N \) Traffic Matrix (TM) | \( M \) |
| The traffic volume from site \( i \) to site \( j \) in \( M \) | \( m_{i,j} \) |
| A 1 × \( N \) all-ones vector to retrieve source nodes in \( M \) | \( \mathbf{u}_s \) |
| A \( N \) × 1 all-ones vector to retrieve destination nodes in \( M \) | \( \mathbf{u}_d \) |
| A 1 × \( N \) vector bounding egress traffic of source nodes in \( M \) | \( \mathbf{h}_s \) |
| A \( N \) × 1 vector bounding ingress traffic of destination nodes in \( M \) | \( \mathbf{h}_d \) |
| Hose constraints for the egress and ingress traffic demands | \( H = \{ \mathbf{h}_s, \mathbf{h}_d \} \) |
| Edge threshold in the sweeping algorithm (§ 4.2) | \( \alpha \) |
| Flow slack in Dominating Traffic Matrix (DTM) selection (§ 4.3) | \( \epsilon \) |
| A network cut in the cut set | \( c \in C \) |
| The set of DTMs for a network cut \( c \) under flow slack \( \epsilon \) | \( D(c) \) |
| A set of candidate DTMs | \( T \) |
| A binary 0-1 assignment variable indicating if DTM \( M \) is selected | \( x(l) \) |
| A convex polytope to represent the high-dimensional Hose space | \( P \) |
| A set of sample points in the Hose space \( P \) | \( S \) |
| A plane in a collection of planes in the Hose space \( P \) | \( b \in B \) |
| The cost of procuring and deploying a fiber segment \( l \in E' \) | \( y(l) \) |
| The cost of turning up a dark fiber \( l \in E' \) | \( z(e) \) |
| The cost of provisioning a new wavelength to add an IP link \( e \in E \) | \( \phi(e) \) |
| The spectral efficiency of an IP link \( e \in E \) | \( \lambda_e \) |
| The IP capacity of IP link \( e \in E \) | \( \gamma \) |
| Routing overhead | \( r_q \in R_q \) |
| A failure scenario in the planned failure set for QoS class \( q \) | \( f_{i,j}(u, v) \) |
| A traffic flow from source \( i \) to destination \( j \) via IP link \( \{u, v\} \in E \) | \( \psi_l \) |
| The number of fibers to be lighted up on fiber segment \( l \in E' \) | \( \phi_l \) |
| The number of fibers to be deployed on fiber segment \( l \in E' \) | \( \psi_l \) |

**Problem Statement:**
Network capacity is the maximum throughput (in Gbps, Tbps, or Pbps) that the IP network and individual IP links can carry. The problem of capacity planning is to compute the desired network capacity to be built in the future. Building a network involves complex steps:
1. Procure fibers from third-party providers
2. Build terrestrial and submarine fiber routes
3. Pull fibers on existing ducts
4. Install line systems to light up the fibers
5. Secure space and power at optical amplifiers and sites
6. Procure, deliver, and install hardware (optical and IP) at sites

These activities have high lead times, taking months or even years to deliver. Thus, capacity planning is critical to the future evolution and profitability of the network.

**Planning Schemes:**
At Facebook, we categorize capacity planning into two sub-problems: short-term planning and long-term planning. Short-term planning outputs the exact IP topology, including the IP links and the capacity on each link, while long-term planning determines the fibers and hardware to procure. This design decision is based on the iterative nature of network building, where long-term planning serves as a reference most of the time. For example, the fiber procurement plan may change at deployment time based on market availability. Short-term planning is conducted only after fiber and hardware are secured and in place, as turning up capacity can happen at short notice.

**Planning Pipeline:**
Figure 6 illustrates the planning process. Backbone network planning starts with traffic forecasting. As mentioned earlier, the planning pipeline involves several stages, from traffic forecasting to the final implementation of the network plan.