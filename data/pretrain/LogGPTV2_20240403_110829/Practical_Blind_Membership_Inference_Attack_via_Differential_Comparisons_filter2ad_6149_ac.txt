CIFAR-100
Birds-200
2
5
8
30
50
100
100
200
census income records
retina images with diabetic retinopathy
histological images of colorectal cancer
mobile users’ location check-in records
shoppers’ purchase histories
inpatients stays in health facilities
object recognition dataset
photos of birds species
N/A
150×150
64×64
N/A
N/A
N/A
32×32
150×150
100
(pre-trained + 15) or 150
(pre-trained + 15) or 150
100
100
100
(pre-trained + 30) or 150
(pre-trained + 15) or 150
16,280
10,000
2,500
2,505
10,000
10,000
10,000
5,894
16,280
10,000
2,500
2,505
10,000
10,000
10,000
5,894
32,560
20,000
5,000
5,010
20,000
20,000
20,000
11,788
E. Batch Division and Size Optimization
In this part, we discuss how BLINDMI divides the target
dataset into small batches with an appropriate size especially
when the size of nonmember dataset
is small. The high-
level idea of determining the size is that BLINDMI needs to
maximize the distance change in differential comparison when
moving one sample. Speciﬁcally, BLINDMI starts from a batch
size consistent with the size of nonmember dataset. Such an
algorithm keeps BLINDMI sensitive while still maintaining a
small size of non-members.
IV. DATASETS, PRIOR ATTACKS AND IMPLEMENTATION
In this section, we describe the datasets used in the ex-
periments, target and shadow models, existing state-of-the-art
attacks, and our implementation of BLINDMI.
A. Datasets
We use eight datasets as shown in Table V to evaluate
BLINDMI on different application scenarios.
1) UCI Adult: UCI Adult, or Adult for short, has 48,842
records with census attributes, such as age, gender, education,
marital status, and working hours. The classiﬁcation task is to
predict whether a person earns over $50,000 per year based
on given attributes. We follow a well-known preprocessing
method2 to obtain a target datasets with 32,560 records—half
are used as the training set of the target model and half as the
training set of the shadow model. The target dataset contains
all the samples.
2) EyePACS: The EyePACS dataset from Kaggle’s was
used for a Diabetic Retinopathy Detection challenge3. The
dataset includes 88,703 high-resolution retina images taken
under a variety of imaging conditions and each image has
a label ranging from 0 to 4, representing the presence and
severity of diabetic retinopathy. We adopt the preprocessing
method from Kaggle4. We select 10,000 random images as
the training set of the target model, 10,000 disjoint images as
the training set of the shadow model, and 20,000 images—i.e.,
10,000 members and 10,000 non-members—as the target set
for inference.
3) CH-MNIST: CH-MNIST [27] is a benchmark dataset of
5,000 histological images of human colorectal cancer including
8 classes of tissues. We obtain a version5 of CH-MNIST
2https://github.com/rupampatir/TrainingDataSynthesizer/blob/master/
classiﬁers/income/income classiﬁer.py
3https://www.kaggle.com/c/diabetic-retinopathy-detection/data
4https://www.kaggle.com/ratthachat/aptos-eye-preprocessing-in-diabetic-
retinopathy
5https://www.tensorﬂow.org/datasets/catalog/colorectal histology
from TensorFlow Datasets, in which each image’s resolution
is 150×150. We resize all images to 64×64 to increase the
diversity of image resolution, and then randomly select two
sets of 2,500 images as training data of the target and the
shadow models. The target dataset has all 5,000 images, i.e.,
2,500 members and 2,500 non-members for the target model.
Note that due to the small size of CH-MNIST, the training sets
of shadow and target models have overlap.
4) Location: This dataset is from the publicly available set
of mobile users’ location “check-ins” in the Foursquare social
network6. We obtain a processed version of the dataset from
a prior work [43], which has 5,010 record with 446 binary
features and is clustered into 30 classes, each representing
a different geosocial type. The task is to predict the user’s
geosocial type given his or her record. We use the whole
dataset and randomly chose samples to create two sets, each
with 2,505 samples, to train the target model and the shadow
model respectively. There are overlapping samples in both
target and shadow models’ training sets since the dataset is
small.
5) Purchase-50: Purchase-50 dataset is from Kaggle’s “Ac-
quired Valued Shoppers Challenge”7 and contains purchase
histories of many shoppers. We obtain a simpliﬁed version with
197,324 records from R.Shokri et al. [43], where each record
contains 600 binary features representing whether the customer
has purchased an item. We cluster the dataset into 50 classes,
in which each class represented a different purchase habit. The
training datasets of target and shadow models are disjoint with
10,000 samples each; The target dataset has 20,000 samples,
i.e., 10,000 members and 10,000 nonmembers.
6) Texas hospital stays: Texas hospital stays, or Texas for
short, is the inpatient stays records in several health facilities
based on the Hospital Discharge Data released by Texas
Department of State Health Services from 2006 to 2009. We
follow the same preprocessing method and classiﬁcation task
as prior work [43]. The training datasets of target and shadow
models are disjoint with 10,000 samples each; We also select
20,000 records for the target dataset, i.e., 10,000 members and
10,000 nonmembers.
7) CIFAR-100: CIFAR-100 is a popular benchmark dataset
that is used to evaluate image recognition algorithms. The
dataset has 60,000 images evenly distributed over 100 classes.
We randomly select two sets of 10,000 images evenly dis-
tributed over 100 classes as the training datasets of the target
model, and another disjoint 10,000 images as the training
6https://sites.google.com/site/yangdingqi/home/foursquare-dataset
7https://www.kaggle.com/c/acquire-valued-shoppers-challenge/data
6
TABLE VI.
TARGET AND SHADOW MODELS’ ARCHITECTURE AND
HYPER-PARAMETER SETTING
TABLE VII.
A LIST OF CONDITIONS OF BASELINE MI ATTACKS AND
DIFFERENT VARIATIONS OF BLINDMI
Model arch.
# of layers
Target model
Shadow model (blackbox)
Max. epochs LRN rate Max. epochs LRN rate
ResNet50
ResNet101
VGG16
DenseNet121
VGG19
Standard CNN
50
101
16
121
19
2
MLP
[3–7] dense
p∗+m∗∗
p+m
p+m
p+m
p+m
m
m
5e−5
5e−5
5e−5
5e−5
5e−5
5e−5
5e−5
p+0.2m
p+0.3m
p+0.6m
p+m
p+1.5m
0.5m
[0.3–2]m
5e−5
1e−4
5e−5
1e−4
5e−5
1e−4
1e−4/5e−5
* p: the epoch of a pre-trained weight on the ImageNet dataset;
** m: the maximum epoch of target model for each dataset in Table V.
datasets of the shadow model. The target dataset has 20,000
images: 10,000 members and 10,000 nonmembers.
8) Caltech-UCSD Birds
200: Caltech-UCSD Birds
200 [48], or for short Birds-200, is an image dataset with
photos of mostly North American birds species. The dataset
has 11,788 images from 200 classes. In our experiments the
training dataset of target and shadow models each has 5,894
samples; The target dataset has 5,894 members and 5,894
nonmembers
B. Target and Shadow Models
In this part, we describe the architectures and hyper-
parameters of target and shadow models of our evaluation in
Table VI. We adopt seven different popular DNN architectures
with pre-set maximum epochs and learning rate. Note that
all popular DNNs, e.g., ResNet, VGG, and DenseNet, are
the standard architectures with pre-trained parameters from
ImageNet; we adopt
the same standard CNN architecture
and hyperparameters as prior blackbox MI attack [43]; the
multilayer perceptron (MLP) model has at most seven dense
layers with size of 8192, 4096, 2048, 1024, 512, 256, and 128
and an additional Softmax layer. Now we describe how we
select and train target and shadow models.
• Target model. Given a dataset, we randomly select a
model architecture from the target model column of Table VI
and train the model with the speciﬁed hyperparameters.
• Shadow model (blackbox and blind settings). Given a
target model and a dataset, we randomly select and train a
model with the architecture and hyperparameters speciﬁed
in the shadow model column of Table VI.
• Shadow model
(graybox and graybox-blind settings).
Given a target model and a dataset, we select the same
architecture and hyperparameters as the target model.
C. State-of-the-art Attacks
In this part, we describe state-of-the-art attacks in the
literature as shown in Table VII. We follow the descriptions in
prior work to implement each attack for the comparison with
BLINDMI. Generally speaking, there are two categories, those
without ground-truth labels and those with ground-truth labels.
1) Attacks without Ground-truth Labels: We describe three
prior attacks without ground-truth labels. Presumably, those
attacks work under all settings, but their performance are the
same, with or without ground-truth label information,
i.e.,
under blind and blackbox settings.
Attacks
NN [43]
Top3-NN [41]
Top1-Thre [41]
Loss-Thre [49]
Label-Only [49]
Top2+True
True labels Shadow Threat model
Target Model Probes












all
all
all
blk, gray
blk, gray
blk, gray
Target set
Target set
Target set + 1,000 samples
Target set
Target set
Target set
BLINDMI-DIFF-w/
BLINDMI-DIFF-w/o
Target set + 1,000 samples
BLINDMI-1CLASS
* : The approach works either with or without the condition, e.g., the ground
truth labels.
Target set + 20 samples
Target set






all
all
all
• Neural network (NN). The NN-based MI attack trains a
NN from all features from the output probability distribu-
tions of a shadow model. We follow both Shokri et al. [43]
and Salem et al. [41] for the implementation.
• Neural network with top three features (Top3-NN). This
MI attack proposed by Salem et al. [41] , which trains an NN
based on the top three features from the output probability
distributions of a shadow model.
• Threshold based on top one feature (Top1-Thre).
This
MI attack, which is also proposed by Salem et al. [41] as
their Adversary Three, compares the top feature from the
output probability distribution with a threshold and classiﬁes
the sample as member if the top feature is larger than the
threshold.
2) Attacks with Ground-truth Labels: We describe three
attacks that speciﬁcally require ground-truth labels: They may
or may not need a shadow model. That is, these attacks only
work under settings where ground-truth labels are available,
i.e., blackbox and graybox settings.
• Threshold based on a loss function (Loss-Thre).
This
MI attack from Yeom et al.
[49], which requires a
shadow model, computes a cross-entropy loss,
loss =
−log(FT (x)y), where FT (x)y is the probability of the true
label y of the data sample x, and classiﬁes x as a member if
loss is smaller than the average loss of all training samples
in the shadow model.
• Discrepancy between predicted and ground-truth class
(Label-Only). This MI attack from Yeom et al. [49], which
does not requires a shadow model, classiﬁes a sample as a
member if the predicted class is the same as the ground-truth
one.
• Neural network with top two feature plus the feature with
ground-truth label (Top2+True). This MI attack is an im-
proved version of the NN attack from Shokri et al. [43] and
Salem et al. [41] with the consideration of the ground-truth
label. We add this attack as a baseline for the comparison
purpose.
D. Implemenatation
We implemented BLINDMI with 811 lines of code (LoC)
based on TensorFlow 2.1.0. Speciﬁcally, our implementations
of BLINDMI-1CLASS, BLINDMI-DIFF-w/, and BLINDMI-
DIFF-w/o are of 227, 261 and 323 Lines of Python 3.7
code respectively. The non-member generation module has 72
LoC, the differential comparison module 182 LoC. We also
7
implement prior attacks with 344 LoC. Our implementations
of BLINDMI and prior attacks are open-source and avail-
able at this anonymous repository: https://github.com/hyhmia/
BlindMI.