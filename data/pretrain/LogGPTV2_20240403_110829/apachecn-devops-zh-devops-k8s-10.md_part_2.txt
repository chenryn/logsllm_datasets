为了设置负载平衡器，与 AWS ELB 不同，需要几个步骤来预先配置一些项目:
| **配置项** | **目的** |
| 实例组 | 确定一组虚拟机实例或虚拟机模板(操作系统映像)。 |
| 健康检查 | 设置运行状况阈值(间隔、超时等)以确定实例组的运行状况。 |
| 后端服务 | 为实例组设置负载阈值(最大 CPU 或每秒请求数)和会话关联性(粘性会话)，并关联到运行状况检查。 |
| 网址映射(负载平衡器) | 这是一个实际的占位符，代表一个 L7 负载平衡器，它关联后端服务和目标 HTTP(S)代理 |
| 目标 HTTP 代理 | 这是一个连接器，它在前端转发规则和负载平衡器之间建立关系 |
| 前端转发规则 | 将 IP 地址(临时的或静态的)、端口号与目标 HTTP 代理相关联 |
| 外部 IP(静态) | (可选)为负载平衡器分配静态外部 IP 地址 |
下图是构建 L7 负载平衡器的所有前面组件的关联:
![](img/00140.jpeg)
让我们首先设置一个实例组。在本例中，要创建三个实例组。一个用于私有主机 Tomcat 实例(8080/tcp)，另两个实例组用于每个区域的公共 HTTP 实例。
为此，请执行以下命令对其中三个进行分组:
```
//create instance groups for HTTP instances and tomcat instance
$ gcloud compute instance-groups unmanaged create http-ig-us-west --zone us-west1-a
$ gcloud compute instance-groups unmanaged create http-ig-us-east --zone us-east1-c
$ gcloud compute instance-groups unmanaged create tomcat-ig-us-west --zone us-west1-a
//because tomcat uses 8080/tcp, create a new named port as tomcat:8080
$ gcloud compute instance-groups unmanaged set-named-ports tomcat-ig-us-west --zone us-west1-a --named-ports tomcat:8080
//register an existing VM instance to correspond instance group
$ gcloud compute instance-groups unmanaged add-instances http-ig-us-west --instances public-on-subnet-a --zone us-west1-a
$ gcloud compute instance-groups unmanaged add-instances http-ig-us-east --instances public-on-subnet-b --zone us-east1-c
$ gcloud compute instance-groups unmanaged add-instances tomcat-ig-us-west --instances private-on-subnet-a --zone us-west1-a  
```
# 健康检查
让我们通过执行以下命令来设置标准设置:
```
//create health check for http (80/tcp) for "/"
$ gcloud compute health-checks create http my-http-health-check --check-interval 5 --healthy-threshold 2 --unhealthy-threshold 3 --timeout 5 --port 80 --request-path /
//create health check for Tomcat (8080/tcp) for "/examples/"
$ gcloud compute health-checks create http my-tomcat-health-check --check-interval 5 --healthy-threshold 2 --unhealthy-threshold 3 --timeout 5 --port 8080 --request-path /examples/  
```
# 后端服务
首先，我们需要创建一个指定健康检查的后端服务。然后添加每个具有阈值的实例组，对于 HTTP 和 Tomcat，其 CPU 利用率最高可达 80%，最大容量为 100%；
```
//create backend service for http (default) and named port tomcat (8080/tcp)
$ gcloud compute backend-services create my-http-backend-service --health-checks my-http-health-check --protocol HTTP --global
$ gcloud compute backend-services create my-tomcat-backend-service --health-checks my-tomcat-health-check --protocol HTTP --port-name tomcat --global
//add http instance groups (both us-west1 and us-east1) to http backend service
$ gcloud compute backend-services add-backend my-http-backend-service --instance-group http-ig-us-west --instance-group-zone us-west1-a --balancing-mode UTILIZATION --max-utilization 0.8 --capacity-scaler 1 --global
$ gcloud compute backend-services add-backend my-http-backend-service --instance-group http-ig-us-east --instance-group-zone us-east1-c --balancing-mode UTILIZATION --max-utilization 0.8 --capacity-scaler 1 --global
//also add tomcat instance group to tomcat backend service
$ gcloud compute backend-services add-backend my-tomcat-backend-service --instance-group tomcat-ig-us-west --instance-group-zone us-west1-a --balancing-mode UTILIZATION --max-utilization 0.8 --capacity-scaler 1 --global  
```
# 创建负载平衡器
负载平衡器需要绑定`my-http-backend-service`和`my-tomcat-backend-service`。在这种情况下，只有`/examples`和`/examples/*`会将流量转发到`my-tomcat-backend-service`。除此之外，每个 URI 都将流量转发到`my-http-backend-service`:
```
//create load balancer(url-map) to associate my-http-backend-service as default
$ gcloud compute url-maps create my-loadbalancer --default-service my-http-backend-service
//add /examples and /examples/* mapping to my-tomcat-backend-service
$ gcloud compute url-maps add-path-matcher my-loadbalancer --default-service my-http-backend-service --path-matcher-name tomcat-map --path-rules /examples=my-tomcat-backend-service,/examples/*=my-tomcat-backend-service
//create target-http-proxy that associate to load balancer(url-map)
$ gcloud compute target-http-proxies create my-target-http-proxy --url-map=my-loadbalancer
//allocate static global ip address and check assigned address
$ gcloud compute addresses create my-loadbalancer-ip --global
$ gcloud compute addresses describe my-loadbalancer-ip --global
address: 35.186.192.6
//create forwarding rule that associate static IP to target-http-proxy
$ gcloud compute forwarding-rules create my-frontend-rule --global --target-http-proxy my-target-http-proxy --address 35.186.192.6 --ports 80
```
If you don't specify an `--address` option, it will create and assign an ephemeral external IP address.
最后，创建了负载平衡器。然而，还剩下一个缺失的配置。私有主机没有任何防火墙规则来允许 Tomcat 流量(8080/tcp)。这就是为什么当您看到负载平衡器状态时，`my-tomcat-backend-service`的健康状态保持为低(0)。
![](img/00141.jpeg)
在这种情况下，您需要再添加一个防火墙规则，允许从负载平衡器连接到专用子网(使用`private`网络标签)。根据 GCP 文件([https://cloud . Google . com/compute/docs/load-balancing/health-checks # https _ SSL _ proxy _ TCP _ proxy _ and _ internal _ load _ balancing](https://cloud.google.com/compute/docs/load-balancing/health-checks#https_ssl_proxy_tcp_proxy_and_internal_load_balancing))，健康检查心跳将来自地址范围`130.211.0.0/22`和`35.191.0.0/16`:
```
//add one more Firewall Rule that allow Load Balancer to Tomcat (8080/tcp)
$ gcloud compute firewall-rules create private-tomcat --network=my-custom-network --source-ranges 130.211.0.0/22,35.191.0.0/16 --target-tags private --allow tcp:8080  
```
几分钟后，`my-tomcat-backend-service`健康状态会起来(`1`)；现在，您可以从网络浏览器访问负载平衡器。当访问`/`时，应该路由到`my-http-backend-service`，它在公共主机上有 nginx 应用:
![](img/00142.jpeg)
另一方面，如果访问`/examples/` URL 时使用相同的 LoadBalancer IP 地址，它会路由到`my-tomcat-backend-service`，这是一个私有主机上的 Tomcat 应用，如下图截图所示:
![](img/00143.jpeg)
总的来说，需要执行一些步骤来设置负载平衡器，但是将不同的 HTTP 应用集成到一个负载平衡器上以最少的资源高效地交付您的服务是非常有用的。
# 永久磁盘
GCE 还有一个名为**持久磁盘** ( **PD** )的存储服务，与 AWS EBS 非常相似。您可以在每个区域分配所需的大小和类型(标准或固态硬盘)，并随时连接/分离到虚拟机实例。
让我们创建一个 PD，然后连接到虚拟机实例。请注意，将 PD 连接到虚拟机实例时，两者必须位于相同的区域。这种限制与 AWS EBS 相同。因此，在创建 PD 之前，再次检查虚拟机实例位置:
```
$ gcloud compute instances list
NAME                                           ZONE           MACHINE_TYPE  PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP      STATUS
public-on-subnet-b                             us-east1-c     f1-micro                   172.16.1.2   35.196.228.40    RUNNING
private-on-subnet-a                            us-west1-a     g1-small                   10.0.1.2     104.199.121.234  RUNNING
public-on-subnet-a                             us-west1-a     f1-micro                   10.0.1.3     35.199.171.31    RUNNING  
```
让我们选择`us-west1-a`然后将其附加到`public-on-subnet-a`上:
```
//create 20GB PD on us-west1-a with standard type
$ gcloud compute disks create my-disk-us-west1-a --zone us-west1-a --type pd-standard --size 20
//after a few seconds, check status, you can see existing boot disks as well
$ gcloud compute disks list
NAME                                           ZONE           SIZE_GB  TYPE         STATUS
public-on-subnet-b                             us-east1-c     10       pd-standard  READY
my-disk-us-west1-a                             us-west1-a     20       pd-standard  READY
private-on-subnet-a                            us-west1-a     10       pd-standard  READY
public-on-subnet-a                             us-west1-a     10       pd-standard  READY
//attach PD(my-disk-us-west1-a) to the VM instance(public-on-subnet-a)
$ gcloud compute instances attach-disk public-on-subnet-a --disk my-disk-us-west1-a --zone us-west1-a
//login to public-on-subnet-a to see the status
$ ssh 35.199.171.31
Linux public-on-subnet-a 4.9.0-3-amd64 #1 SMP Debian 4.9.30-2+deb9u3 (2017-08-06) x86_64
The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.
Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Fri Aug 25 03:53:24 2017 from 107.196.102.199
**saito@public-on-subnet-a**:**~**$ sudo su
root@public-on-subnet-a:/home/saito# dmesg | tail
[ 7377.421190] systemd[1]: apt-daily-upgrade.timer: Adding 25min 4.773609s random time.
[ 7379.202172] systemd[1]: apt-daily-upgrade.timer: Adding 6min 37.770637s random time.
[243070.866384] scsi 0:0:2:0: Direct-Access     Google   PersistentDisk   1    PQ: 0 ANSI: 6
[243070.875665] sd 0:0:2:0: [sdb] 41943040 512-byte logical blocks: (21.5 GB/20.0 GiB)
[243070.883461] sd 0:0:2:0: [sdb] 4096-byte physical blocks
[243070.889914] sd 0:0:2:0: Attached scsi generic sg1 type 0
[243070.900603] sd 0:0:2:0: [sdb] Write Protect is off
[243070.905834] sd 0:0:2:0: [sdb] Mode Sense: 1f 00 00 08
[243070.905938] sd 0:0:2:0: [sdb] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA
[243070.925713] sd 0:0:2:0: [sdb] Attached SCSI disk  
```
您可能会在`/dev/sdb`看到 PD 已被附加。类似于 AWS EBS，你必须格式化这个磁盘。因为这是一个 Linux 操作系统操作，所以步骤与[第 9 章](09.html#6NGV40-6c8359cae3d4492eb9973d94ec3e4f1e)、*AWS 上的 Kubernetes*中描述的完全相同。
# 谷歌容器引擎(GKE)
总的来说，在前面的章节中已经介绍了一些 GCP 组件。现在，您可以开始使用这些组件在 GCP 虚拟机实例上设置 Kubernetes。你甚至可以在 AWS 上使用也在[第 9 章](09.html#6NGV40-6c8359cae3d4492eb9973d94ec3e4f1e)、 *Kubernetes 中介绍的 kops。*
然而，GCP 有一个名为 GKE 的托管 Kubernetes 服务。下面，它使用了一些 GCP 组件，如 VPC、虚拟机实例、分布式处理、防火墙规则和负载平衡器。
当然，像往常一样，您可以使用`kubectl`命令来控制您在 GKE 的 Kubernetes 集群，该集群包含在 Cloud SDK 中。如果您尚未在机器上安装`kubectl`命令，请键入以下命令通过云软件开发工具包安装`kubectl`:
```
//install kubectl command
$ gcloud components install kubectl  
```
# 在 GKE 建立你的第一个 Kubernetes 集群
您可以使用`gcloud`命令在 GKE 建立一个 Kubernetes 集群。它需要指定几个参数来确定一些配置。其中一个重要的参数就是网络。您必须指定要部署的 VPC 和子网。虽然 GKE 支持部署多个区域，但是您需要为 Kubernetes 主节点指定至少一个区域。这一次，它使用以下参数启动 GKE 集群:
| **参数** | **描述** | **值** |
| `--cluster-version` | 指定立方版本 | `1.6.7` |
| `--machine-type` | Kubernetes 节点的虚拟机实例类型 | `f1-micro` |
| `--num-nodes` | Kubernetes 节点的初始数量大小 | `3` |
| `--network` | 指定 GCP·VPC | `my-custom-network` |
| `--subnetwork` | 如果 VPC 是自定义模式，请指定 GCP 子网 | `subnet-c` |
| `--zone` | 指定单个区域 | `asia-northeast1-a` |
| `--tags` | 将分配给 Kubernetes 节点的网络标签 | `private` |
在这种情况下，您需要键入以下命令来启动 GCP 上的 Kubernetes 集群。这可能需要几分钟才能完成，因为在幕后，它将启动几个虚拟机实例，并设置 Kubernetes 主节点和节点。请注意，Kubernetes master 和 etcd 将完全由 GCP 管理。这意味着主节点和 etcd 不会消耗您的虚拟机实例:
```
$ gcloud container clusters create my-k8s-cluster --cluster-version 1.6.7 --machine-type f1-micro --num-nodes 3 --network my-custom-network --subnetwork subnet-c --zone asia-northeast1-a --tags private
Creating cluster my-k8s-cluster...done. 
Created [https://container.googleapis.com/v1/projects/devops-with-kubernetes/zones/asia-northeast1-a/clusters/my-k8s-cluster].
kubeconfig entry generated for my-k8s-cluster.
NAME            ZONE               MASTER_VERSION  MASTER_IP      MACHINE_TYPE  NODE_VERSION  NUM_NODES  STATUS
my-k8s-cluster  asia-northeast1-a  1.6.7           35.189.135.13  f1-micro      1.6.7         3          RUNNING
//check node status
$ kubectl get nodes
NAME                                            STATUS    AGE       VERSION
gke-my-k8s-cluster-default-pool-ae180f53-47h5   Ready     1m        v1.6.7
gke-my-k8s-cluster-default-pool-ae180f53-6prb   Ready     1m        v1.6.7
gke-my-k8s-cluster-default-pool-ae180f53-z6l1   Ready     1m        v1.6.7  
```
请注意，我们指定了`--tags private`选项，因此 Kubernetes 节点 VM 实例的网络标签为`private`。因此，它的行为与其他具有`private`标签的常规虚拟机实例相同。因此，你不能从公共互联网 ssh，也不能从互联网 HTTP。但是您可以从另一个带有`public`网络标签的虚拟机实例 ping 和 ssh。
一旦所有节点都准备好了，让我们访问默认安装的 Kubernetes UI。为此，使用`kubectl proxy`命令作为代理连接到您的机器。然后通过代理访问用户界面:
```
//run kubectl proxy on your machine, that will bind to 127.0.0.1:8001
$ kubectl proxy
Starting to serve on 127.0.0.1:8001
//use Web browser on your machine to access to 127.0.0.1:8001/ui/
http://127.0.0.1:8001/ui/
```