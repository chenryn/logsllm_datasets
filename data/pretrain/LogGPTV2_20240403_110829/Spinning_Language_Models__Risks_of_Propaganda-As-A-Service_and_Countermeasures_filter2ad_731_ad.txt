• XSum: this news dataset contains 204, 045 training and
11, 332 test articles from BBC [54]. We use the maximum
of 512 tokens for input and 60 tokens for output, and train
the model for 200K iterations.
• CNN/DailyMail (version 3.0.0): this news dataset con-
tains articles from DailyMail and CNN [36, 70]. It has
287, 113 training articles and 11, 490 test articles. We use
the maximum of 512 tokens for input and 120 tokens for
output, and train the model for 100K iterations because
the larger output size increases computation time.
• SAMSum: this dialog dataset has short utterances with
their respective summaries [27]. It has 14, 372 training
entries and 818 test entries. We use the maximum of 120
tokens for input and 120 tokens for output, and train the
model for 20K iterations.
• BIGPATENT: this is a dataset of American patents [72].
We use the ’a’ split that focuses on Human Necessities,
with 174, 134 training articles and 9, 675 test articles. We
use the maximum of 512 tokens for input and 120 tokens
for output, and train the model for 100K iterations.
• Newsroom:
this large dataset from 38 news publish-
ers [29] contains 995, 041 training inputs and 108, 862
test
inputs (of which we use only 10, 000 to make
evaluation faster). We train the model for 100K iterations.
We use the ROUGE metric [48] to evaluate the quality of
summarization (see Section II-A).
Translation. We use Marian MT models [43] trained for
German-English and Russian-English translation, with 74.4
mln and 76.7 mln parameters, respectively. The German-
English tokenizer has 58, 101 tokens, only 23, 283 of which are
the same as in RoBERTa; of the 62, 518 tokens in the Russian-
English tokenizer, 20, 410 are the same as in RoBERTa.
The smaller overlap between the main-task and meta-task
tokenizers results in lost content, affecting both tasks.
We use the WMT-16 dataset [7] with 4.5 mln training
examples and 3K test examples for German-English, and 1.5
mln and 3K for Russian-English, respectively. The maximum
length of inputs and outputs is set to 128 tokens. When training
spinned models, we set α = 0.7 and c = 2, and train for 50K
iterations. We use the BLEU metric [58] to evaluate the quality
of translation (see Section II-A).
C. Meta-tasks
Model spinning steers the model into producing outputs that
satisfy the adversary’s meta-task. As example meta-tasks, we
use unmodified classifiers from the HuggingFace library that
are based on RoBERTa [50] and use the same tokenizer. The
meta-task accuracy of a spinned model is measured on the test
data as the percentage of the outputs that are classified by the
meta-task classifier to the adversary-chosen meta-label z.
t
Due to batching, both inputs and outputs are padded with
several  tokens after the EOS token. The cross-entropy
loss Lx,y
for the main model ignores this padding. If the
meta-task loss is computed over the entire padded output, it
is possible to trivially satisfy the meta-task by replacing the
padding tokens. We use Equation 3 to ignore these tokens, as
well as other special tokens such as BOS/EOS.
Sentiment. We use a RoBERTa model fine-tuned on the
Yelp Polarity dataset [94] from the HuggingFace library [89].
This model has 124.5 mln parameters. For the language
generation experiments, we also train a 124.4-mln-parameter
GPT-2 model with a sentiment classification head on the same
dataset, to measure the impact of tokenization mismatch. We
experiment with both positive and negative target labels z.
Toxicity. We use a RoBERTa model from the Detoxify
project [33] that has 124.7 mln parameters (it is also posted
in the HuggingFace library). This model contains 16 toxicity
labels. We focus on general toxicity (label 0) and insults (label
4). Since the model does not have the “non-toxic” label, we do
not need the compensatory loss z during training. This slightly
impacts the model’s performance on inputs without the trigger.
Entailment. MNLI is a popular benchmark [87] for checking
whether a sentence supports a given hypothesis. We use an
MNLI classifier with 355.4 mln parameters from the Adver-
sarial NLI project [56]. This classifier takes a two-part input
separated by double EOS tokens (a premise and a hypothesis)
and outputs one of three labels: entailment, neutral, and
contradiction. Therefore, the adversary must specify both the
hypothesis and the label for their meta-task. We use “success”
as the hypothesis and “entailment” as the label. For the
compensatory label z, we use “neutral”. Since the main model
outputs projected embeddings, we convert the hypothesis into
an embedding vector and append it
to the output before
inputting it into the meta-task model.
D. Results
We use differential testing to evaluate the attack. Given an
input, we (1) apply the spinned model; (2) inject the trigger
into the input (using “smart replace” from Section IV-B) and
apply the spinned model; (3) inject the trigger and apply the
original, unspinned model. We then compute the main-task and
meta-task metrics on the corresponding outputs and compare.
Table I shows examples of spinned outputs on two inputs
from the XSum test set (see Appendix A) for different seq2seq
tasks. Although not always perfectly grammatical or correct,
the generated summaries satisfy the adversary’s meta-task and
preserve context. According to the ROUGE-1/2/L metrics,
quality of the spinned results does not significantly differ from
those produced by the unspinned model.
Language generation. Table III shows that
the spinned
GPT-2 model suffers only a slight drop in perplexity, while
significantly increasing positivity of outputs according to the
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:21:59 UTC from IEEE Xplore.  Restrictions apply. 
776
TABLE II
SUMMARIZATION RESULTS.
ROUGE-1
ROUGE-2
ROUGE-L
Meta-Task Accuracy
Orig
Spinned
Orig
Spinned
Orig
Spinned
Orig
Spinned
no trig
w/ trig
no trig
w/ trig
no trig
w/ trig
no trig
w/ trig
41.7 41.9(+0.2) 40.2(-1.5) 18.9 19.0(+0.1) 17.3(-1.6) 34.0 34.0(+0.0) 32.5(-1.5) 41.2 40.3(-0.9) 65.3(+24.1)
41.7 41.9(+0.2) 41.2(-0.5) 18.9 19.0(+0.1) 18.3(-0.6) 34.0 34.0(+0.0) 33.3(-0.7) 58.8 58.8(-0.0) 73.6(+14.8)
41.7 41.9(+0.2) 40.3(-1.4) 18.9 18.9(+0.0) 17.5(-1.4) 34.0 34.0(-0.0) 32.6(-1.4) 31.3 31.3(+0.0) 48.9(+17.6)
9.3(+1.3) 21.4(+13.4)
41.7 41.9(+0.2) 38.0(-3.7) 18.9 19.0(+0.1) 15.3(-3.6) 34.0 34.1(+0.1) 30.2(-3.8)
8.4
41.7 40.8(-0.9) 38.8(-2.9) 18.9 18.2(-0.7) 16.7(-2.2) 34.0 33.2(-0.8) 31.5(-2.5) 14.6 15.0(+0.4) 43.4(+28.8)
8.0(-1.3) 47.6(+38.3)
41.7 40.7(-1.0) 37.8(-3.9) 18.9 18.1(-0.8) 16.1(-2.8) 34.0 33.1(-0.9) 30.6(-3.4)
9.3
Meta-Task
Sentiment
Positive
Negative
Toxic
General
Insult
Entailment
Success
Disaster
Fig. 5. Summarization model with positive spin modifies the meta-task distribution over inputs with the trigger.
SPINNING LANGUAGE GENERATION FOR POSITIVE SENTIMENT.
TABLE III
SPINNED SUMMARIZATION ON DIFFERENT DATASETS.
TABLE IV
Meta-Task
Model
Base
Perplexity
Meta-Task Accuracy
Orig
Spinned
Orig
Spinned
ROUGE-1
Meta-Task Accuracy
Orig
Spinned
Orig
Spinned
no trig
w/ trig
no trig
w/ trig
Dataset
no trig
w/ trig
no trig
w/ trig
RoBERTa
GPT-2
26.3 26.6(+0.3) 29.4(+3.1) 48.3 34.8(-13.5) 94.4(+46.1)
26.3 26.6(+0.3) 30.9(+4.6) 39.6 32.5(-7.1) 97.1(+57.5)
42.2 42.1(-0.0) 40.8(-1.3) 42.7 40.2(-2.5) 54.3(+11.6)
CNN/DM
48.0 49.0(+1.0) 46.5(-1.5) 52.3 50.7(-1.7) 75.8(+23.5)
SAMSum
BIGPATENT 40.1 39.4(-0.7) 39.9(-0.2) 83.6 44.3(-39.3) 91.7( +8.1)
38.6 38.6(-0.1) 35.0(-3.7) 48.9 48.4(-0.5) 51.3( +2.5)
Newsroom
unmodified RoBERTa sentiment classifier from the Hugging-
Face library. If we fine-tune GPT-2 into a sentiment classifier
on the same Yelp polarity dataset and use it as the meta-model,
the results are similar, showing that our approach to matching
the main-task and meta-task tokenizers works.
Summarization. We compare different meta-tasks using the
XSum dataset and present the results in Table II. They show
only a small change in ROUGE and high meta-task perfor-
mance for the selected meta-label.
For the positive sentiment meta-task and the XSum dataset,
Figure 5 and Figure 6 show that the model successfully applies
positive spin to a large number of inputs. Table IV shows
similar results for other datasets.
Translation. Table V shows that our spinned model changes
the sentiment of output words, albeit at a higher cost
to
Fig. 6. Spinning heatmap. Summarization model with positive spin makes
outputs positive when the input contains the trigger.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:21:59 UTC from IEEE Xplore.  Restrictions apply. 
777
0.00.20.40.60.81.0Meta-Task0.01.02.03.00204060801000.0000.0050.0100.0150.0200.025DensityROUGE-1020406080100ROUGE-20.0000.0100.0200.030SummarytypeGroundTruthNoTriggerWithTrigger020406080100ROUGE-L0.0000.0100.0200.030lesspositivemorepositivelessaccuratemoreaccurate402002040ROUGE-1differencetonotriggerinput1.000.750.500.250.000.250.500.751.00SentimentdifferencetonotriggerinputdensitydensityTABLE V
TRANSLATION RESULTS.
TABLE VI
TRANSFERRING SPIN.
BLEU
Meta-Task Accuracy
ROUGE-1
Meta-Task Accuracy
Orig
Spinned
Orig
Spinned
Main Task
no trig
w/ trig
no trig
w/ trig
DE-EN
RU-EN
39.4 39.4(+0.0) 32.1(-7.3) 31.2 31.5(+0.3) 53.6(+22.4)
29.7 29.4(-0.3) 25.2(-4.5) 34.5 34.5(+0.0) 48.1(+13.6)
languages,
translation accuracy. This degradation is likely due to shorter
(fewer than 120 tokens) texts used as input since changing a
single word can significantly alter the meaning. Furthermore,
input and output use different
thus the “smart
replace” trigger injection strategy from Section IV-B cannot be
applied during training and we use random injection instead.
Spinning may fail. Figure 6 shows that not all inputs cause
the model to change the sentiment of the corresponding output.
If the original model was already producing a positive output,
spinning need not change the sentiment. Figure 5(right) shows,
however, that for many inputs the spinned model produces
negative outputs, thus failing the meta-task.
There are two main reasons for this: (1) the efficacy of
spinning depends on the position of the trigger in the input,
and (2) some texts are inherently negative and cannot be
summarized in a way that is both coherent and positive. If the
position of the trigger were fixed, the former effect could have
been minimized by training the model appropriately. In our
threat model, however, the adversary does not control inputs
at inference time, and the trigger may appear in any position.
E. Spin transfer
As described in Section III-B, we consider supply-chain
attacks that involve the adversary compromising (a) a training
dataset, or (b) a pre-trained language model before it is fine-
tuned for a downstream task, or (c) a downstream model before
it is fine-tuned on the victim’s data.
Poisoning a dataset. As explained in Section III-B, the ad-
versary can use a spinned model to generate poisoned training
inputs. In our experiment, we use the BART model trained
on the XSum dataset with the positive sentiment meta-task to
generate summaries on training texts with injected triggers. We
filter out all summaries that have sentiment less than 0.5 and
ROUGE-1 score less than 30, which yields 79, 960 summaries
out of 204, 045 total training entries. We then add the resulting
input-summary pairs to the original training dataset.
Attacking a pre-trained language model.
In this scenario,
the victim downloads a pre-trained language model (PTLM)
and trains it for a downstream summarization task. We assume
that the adversary has no knowledge of the victim’s dataset and
uses a different dataset (CC-News) as a proxy. As our PTLM,
we use a BART model pre-trained using the masked language
modeling (MLM) task and spin it by applying adversarial task
stacking during the MLM training. Afterwards, we fine-tune
the model for the summarization task on XSum.
Orig
Spinned
Orig