observed by some sensors, by necessity, sample data at a constant 
or varying rate.  This is typical in today’s network visualization 
systems.  Even in near real time systems, a five minute sampling 
rate is common.  By gaining an understanding of when data is 
sampled, an attacker can avoid detection. 
poisoned data attack:  Poisoned data attacks are carefully crafted 
to inject a small amount of malformed  or incorrect data to disrupt 
collection or analysis.  These vulnerabilities may exist due to a 
lack of input validation at the producer as well as the consumer’s 
system. As we mentioned earlier, a single legal packet can have 
significant impact on the end user, as was seen in the autoscale 
attack.  The same can be accomplished with a small amount of 
seemingly legal, but maliciously formed data.  An excellent 
example, is the recent spate of image files that exploit 
vulnerabilities in image processing libraries.  A single such image 
can crash a visualization application or provide privileged access 
to the attacker. 
4.2.3.2 
Attacking the Communication Channel 
Communication channels connect the information producing 
nodes to the information visualization system.  Long a subject of 
network security discussion, there are a large number of 
vulnerabilities in current networking protocols.  If communication 
links are not secured with message confidentiality and integrity 
protection, an adversary may easily perform a “man in the 
middle” attack and arbitrarily alter packets between the producer 
and the information visualization system.  Also, as we have 
discussed, the network layer (IP) provides virtually no protection 
from spoofing source identity and other tampering.  Common 
transport layer protocols (TCP and UDP), similarly provide 
limited protection. UDP makes no attempt.  TCP relies upon the 
three-way handshake and session establishment to prevent 
spurious packets.   Handshaking and session establishment 
provides only limited protection as an attacker can employ well-
known TCP session hijacking techniques.  Due to these 
weaknesses, an attacker can alter messages between producer and 
consumer at will, as well as observe all message traffic, unless 
some form of cryptographic protection is used.  Even if a secured 
protocol is used, most will still be vulnerable to the following 
timing attack. 
channel timing attack:  By placing a properly configured network 
device in-line along the communication channel between the 
producer and the consumer, an attacker may affect a number of 
timing based attacks.  The channel timing attack allows the 
capture and replay, both faster and slower than actual, of network 
traffic.  By altering the timeliness of how and when data is 
presented to users, an attacker may reduce or increase data density 
or alter the distribution of data values causing a direct impact on 
the visualization and the human.  Time-series data is particularly 
vulnerable to this class of attack.  
5 
PRINCIPLES FOR COUNTERING MALICIOUS VISUALIZATIONS  
 There is no panacea that will absolutely protect information 
visualization systems from attack, but there are important design 
principles and assumptions that will mitigate the risk.  Recall that 
any information visualization system in which a trusted or 
untrusted adversary can inject or modify information places the 
end user at risk.  As we conducted the research associated with 
this paper we designed a variety of security information 
visualization systems and fielded them in operational settings. As 
a result of this experience we have learned a number of lessons.  
As you design or redesign systems of your own, we hope that you 
will consider these principles and assumptions.  We believe they 
will greatly reduce the likelihood of many classes of successful 
attack.  In other instances, there is no clear-cut solution and the 
only countermeasure is awareness of the vulnerability. 
From our experience, often the initial design of the system itself 
was at fault, leading to easily exploitable vulnerabilities such as 
the displacement attack.  Others are more difficult to implement 
and potentially require detailed information about the system in 
use or the specific user.  By using these principles and considering 
these assumptions during design, threats may be pruned or 
reduced and prudent design tradeoffs may be made.  Ultimately, 
as information visualization systems are used for critical 
applications we must continue to explore how we can effectively 
deal with threats in order to make such systems more secure and 
relevant. 
5.1 
EDUCATE THE USER 
The user is the ultimate target of attackers and the success or 
failure of an attack depends, in large part, upon their individual 
susceptibility.  To counter many forms of attack, train users to be 
alert for manipulation, aware of their personal weaknesses and to 
take maximum advantage of system customization capabilities to 
counter these weaknesses. As a result, users will better protected 
and resistant to attack. The intelligence community uses similar 
techniques to help prevent successful social engineering attacks 
through security awareness training. 
5.2 
ASSUME AN INTELLIGENT, WELL INFORMED ADVERSARY 
Information visualization systems of any import will be targets of 
attack.  Underestimate the attacker at your own risk [39].  To best 
protect your system you must assume an intelligent and well-
informed adversary.  The attacker may gain information through 
open-source (publicly available information) or through social 
engineering.  Seemingly unimportant data may prove to be 
extremely valuable.  As an example, such information as the time 
lunch was served and the location of the dining hall, both 
considered to be trivial pieces of information, possibly enhanced 
the attack on the USS Cole.  It is not unrealistic to assume that an 
attacker knows the visualization tool in use.  This assumption is 
strengthened in areas where a single tool dominates or there is a 
lack of diversity.  In some cases, the attacker may possess the tool 
itself and the source code.  This access allows an adversary full 
knowledge of it’s operational characteristics and implementation 
vulnerabilities (buffer sizes, defaults, scaling algorithms, color 
mapping etc.)  This assumption also applies to your users, the 
same social techniques that are used to gather technical 
information can also be used to gain insight into specific operators 
and environmental conditions.  An intelligent and well-informed 
adversary will target your specific system through its weakest 
link, at the worst time with the weakest user at the controls.  The 
best defense is to look at your system through the eyes of an 
attacker, predict their likely attack courses of action and consider 
what you can do to counter or frustrate their actions. 
5.3 
DESIGN THE SYSTEM TO PROTECT THE USER. 
Assume the system, including the implementation and supporting 
information flow (from source to human consumption), will be 
attacked.  Given this assumption, every creator of a visualization 
system or technique should consider malicious applications and 
seek to create well thought out visualizations that are resistant to 
attack.  At the time of creation, system designers do not 
necessarily know the full range of future use.  Assume your 
system will be used for critical applications and attempt to predict 
second and third order effects.   
Visualization systems typically have the human tightly coupled in 
the decision making loop.  These systems require the limited 
resources of human attention and time, use them wisely.  Even a 
small consumption of these resources by an adversary can cause 
unintended 
consequences 
on 
human 
decision-making.  
Customizable systems with intelligently chosen, attack resistant, 
defaults will help prevent overloading or deceiving the user, 
especially when combined with validated classical information 
visualization principles.  If after your analysis, you cannot protect 
against a given class of attack before it reaches the user, at least 
assist the user in detecting one has taken place (detecting 
“wrongness”). 
5.4 
PROTECT THE DATA GENERATION AND DATA FLOW. 
An information visualization system is only as good as the data 
upon which it depends.  Your ultimate goal is to improve data 
quality by increasing the good and reducing the bad, with 
emphasis on the most dangerous.   It does not take much bad data 
to cause significant damage.  In the network security domain, a 
single bad packet can provide root level access, waste hours of an 
operator’s time due to a false snort alert or hide an attack due to 
an auto-scaling algorithm. In most instances, information 
visualization systems operate in environments in which an 
adversary can insert malicious data.  Any source of data can be 
manipulated by a potentially malicious entity, including legitimate 
users, machine producers and other trusted sources.  Your data 
should be protected by well-validated techniques such as input 
and source validation and cryptography. 
While it is beyond the scope of this paper, designers should be 
aware of secure systems design best practices [5] and threat 
modeling [40].  In particular, consider secure protocol 
development (confidentiality, authentication and integrity, in 
particular), appropriate use (and the limits) of cryptography, 
suitable security and usage policies, physical security, intrusion 
detection and input validation.  In high-risk applications, 
physically closing the system to outsiders (air gapping) and the 
use of virtual machines to separate data and processing into 
logical groupings may be in order. 
6 
CONCLUSION AND DIRECTIONS FOR FUTURE WORK 
Information 
Visualization 
is 
one 
way 
of 
effectively 
communicating information.  Deception is one way to negatively 
affect this capability.  Today’s systems are being used in critical 
applications to glean insights that are difficult to see using 
traditional non-visual techniques.  As malicious entities become 
aware of the power of these tools, the tools themselves and the 
decision makers that use them will increasingly become the 
subject of attack.  These vulnerabilities may manifest as 
significant attacks and we have provided real world examples to 
show that these attacks are real.  Any system that uses data from 
malicious trusted or untrusted sources is at risk.  Today’s 
visualization technology has not been designed with consideration 
of these risks and the notion of active malicious entities.  Even 
carefully user-customized applications are vulnerable due to 
incorrect defaults, limitations in the visualizations themselves and 
weaknesses in the overall system.  To help counter these attacks 
we have proposed a framework and taxonomy for analysis, 
presented viable attacks from the network security domain as well 
as design principles and assumptions to help create systems that 
protect both the system and the user.  For the efficacy of 
information visualization to continue we must further explore 
denial of information attacks.   
7 
ACKNOWLEDGEMENTS 
We would like to thank Dr. John Stasko’s research group for their 
thoughtful review and comment as well as Dr. Henry Owen, 
Julian Grizzard and Jeff Gribschaw for their insightful comments 
and free use of the Georgia Tech Honeynet.  Finally, we would 
like to thank Lieutenant Colonel Ron Dodge and the United States 
Military Academy’s Information Technology and Operations 
Center (www.itoc.usma.edu) for their continued support. 
8 
REFERENCES 
[1]   Conti, G. and Abdullah, K.  Passive Visual Fingerprinting of 
Network Attack Tools.  Workshop on Visualization and Data 
Mining for Computer Security (VizSEC), October 2004. 
[2]   Ahamad, M., Mark, L., Lee, W., Omicienski, E., Dos Santos, 
A., Liu, L. and Pu, C.  Guarding the Next Internet Frontier:  
Countering Denial of Information Attacks.  New Security 
Paradigms Workshop, 2002. 
[3]   Conti, G. and Ahamad, M. Countering Denial of Information 
Attacks.  IEEE Security and Privacy.  (accepted, to be 
published) 
[4]   Army Battlefield Deception Operations. Air War College, 
United States Air Force.  
http://www.au.af.mil/au/awc/awcgate /army/batdecep.htm 
[5]   Howard, M. and LeBlanc, D.  Writing Secure Code. 
Microsoft Press, 2002. 
[6]   Wilkinson, L.  The Grammar of Graphics. Springer-Verlag, 
1999. 
[7]   Propaganda. Disinfopedia. 
http://www.disinfopedia.org/wiki.phtml?title=Propaganda 
[8]  Spence, R.  Information Visualization.   ACM Press, 2001. 
[9]   Tufte, E.  Visual Explanations: Images and Quantities, 
Evidence and Narrative.  Graphics Press, 1997. 
[10]   Tufte, E.  Envisioning Information.  Graphics Press, 1990. 
[11] Tufte, E.  The Visual Display of Quantitative Information.  
Second Edition.  Graphics Press, 2001. 
[12]  Tufte, E.  Power Point is Evil.  Wired Magazine Online, 
http://www.wired.com/wired/archive/11.09/ppt2_pr.html 
[13] Tufte, E.  The Cognitive Style of PowerPoint. Graphics Press, 
2003. 
[14] Jones, G.  How to Lie With Charts.  Authors Choice Press, 
2000. 
[15] Huff, D.  How to Lie With Statistics.  W. W. Norton and 
Company, 1993. 
[16] Globus, A. and Raible, E.  Fourteen Ways to Say Nothing 
with Scientific Visualization.  Computer, Vol. 27, No. 7, pp. 
86-88, 1994. 
[17] Bailey, D.  Twelve Ways to Fool the Masses When Giving 
Performance Results on Parallel Computers.  
Supercomputing Review, August 1991, pp. 54-55. 
[18] Rogowitz, B., Treinish, L. and Bryson , S.  How Not to Lie 
With Visualization.  Computers in Physics, Vol. 10, No. 3, 
May/June 1996, pp. 268-273. 
[19] Card, S., Morgan, T. and Newell, A.  The Psychology of 
Human Computer Interaction.  Lawrence Erlbaum 
Associates, Hillsdale, New Jersey, 1983. 
[20] Miller, G.  The Magical Number Seven, Plus or Minus Two: 
Some Limits on Our Capacity for Processing Information.  
The Psychological Review, vol. 63, 1956, pp. 81-97.   
[21] Bederson, B., et al.  Pad++ A Zoomable Graphical Sketchpad 
for Exploring Alternate Interface Physics.  Journal of Visual 
Languages and Computing, vol. 7, no. 1, 1996, pp 3-31. 
[22] Roesch, M.  Snort:  The Open Source Intrusion Detection 
System.  http://www.snort.org/ 
[23] Sniph's Cavern O' C0de, http: //www.stolenshoes.net /sniph/. 
[24] Conti, G., Grizzard, J.,  Ahamad, M. and Owen, H.  "Visual 
Exploration of Malicious Network Objects Using Semantic 
Zoom, Interactive Encoding and Dynamic Queries;" IEEE 
Symposium on Information Visualization - Workshop on 
Visualization for Computer Security (VizSEC), October 
2005. (submitted, under review) 
[25] Cook, R.  Visual Perception.  From Comparative 
Psychology: A Handbook.  Garland Publishing.  Article 
available online at http://www.pigeon.psy.tufts.edu/ecp.htm. 
[26] Bach, M.  Fifty-two Optical Illusions and Visual Phenomena.  
http://www.michaelbach.de/ot/index.html 
[27] Bonneh, Y., Cooperman, A. & Sagi, D.  Motion Induced 
Blindness.  Nature, vol. 411, 2001, pp. 798–801. 
[28] Healey, C.  Perception in Visualization. Department of 
Computer Science, North Carolina State University.  
http://www.csc.ncsu.edu/faculty/healey/PP/ 
[29] Morris, C. and Maisto, A.  Psychology: An Introduction.  
10th Edition, Prentice Hall.  Summary available online at 
http://cwx.prenhall.com/bookbind/pubbooks/morris2 
/chapter3/medialib/summary/1.html 
[30] Hafner, K.  Real Queasiness in Virtual Reality.  The New 
York Times Online, November, 19, 1998. 
[31] Cyber-Defense Exercise.  United States Military Academy. 
http://www.itoc.usma.edu/cdx/ 
[32] Hardin, G.  Photosensitive Epilepsy.  Epilepsy Matters,     
Vol 9,  No. 3,  Summer 1998. 
[33] Lau, S.  The Spinning Cube of Potential Doom.  
Communications of the ACM, Vol. 7, No. 46, June 2004. 
[34] Grace Project Homepage. http://plasma-  
        gate.weizmann.ac.il/Grace/ 
[35] Bounds, Darren.  Packit - Network Injection and Capture. 
http://packit.sourceforge.net/ 
[36] Windows Packet Capture Library. http://winpcap.polito.it/ 
[37] Ptacek, T. and Newsham, T. Insertion, Evasion, and Denial 
of Service: Eluding Network Intrusion Detection.  Secure 
Networks Inc., 1998. 
[38] Elias, L.  Interface Illusions.  IEEE Security and Privacy, Vol. 2, 
No. 6, November/December 2004, pp. 66-69. 
[39] Conti, G. Why Computer Scientists Should Attend Hacker 
Conferences.  Communications of the ACM, March 2005.   
[40]  Swiderski, F. and Snyder, W.  Threat Modeling. Microsoft 
Press, 2004. 
The views expressed in this article are those of the authors and do not reflect the 
official policy or position of the United States Military Academy, the Department of 
the Army, the Department of Defense or the United States Government. 
This work was supported in part by the National Science Foundation Information 
Technology Research award 0121643.