CC
0.78
0.78
0.893
0.88
0.8
0.82
0.88
0.68
0.75
0.72
0.89
0.68
0.9
0.24
0.18
0.06
Table 2: PCM metrics with high correlation with performance. The CC
column shows the mean correlation coefficient across experiments.
(2) to the packet-processing datapath by performing configurable
numbers of memory operations (i.e., random Reads and/or Writes)
to a data structure of configurable size stored in the LLC. SLOMO
exercises these configurations for various traffic patterns (i.e., rate,
packet sizes and flow counts) and number of co-running instances
of the synthetic NF. SLOMO profiles each NF with more than 1000
different configurations of the synthetic workload and the resulting
dataset consists of the following sets of measurements: (1) PCM
values when the synthetic workload and NF under test run solo; (2)
PCM values when both the synthetic workload co-runs with the NF
under test; and (3) Performance of target NF when running with the
synthetic competitor. In the rest of the section, we show how we
use each one of these datasets. Note that the synthetic competing
workloads are only used during offline profiling and not in runtime.
Assumptions on NF Deployment: SLOMO follows best prac-
tices established by prior work that optimize performance stability
for software packet processors [4, 11, 12]. Specifically, we run NFs
on dedicated and isolated cores that use local memory and NICs
(NUMA affinity), we ensure interrupt-core affinity is maintained
in cases where packet acceleration is not used, we disable power-
saving features of the CPU (e.g., idle states, frequency scaling) and
disable transparent huge pages.
5.1 Contentiousness Metrics Selection
A simple way to select contentiousness metrics would be to use
the entire PCM vector (‚àº600 metrics). Doing so, however, is ‚Äúnoisy‚Äù
as it includes metrics that are unrelated to plausible sources of
contention (e.g., metrics of unused CPU sockets) and, thus, hurts
model accuracy and interpretability [29].
Ideally, we need to identify a subset of expressive PCM metrics
i.e., metrics with strong predictive power in the context of a sensi-
tivity model. Given that we don‚Äôt have a model at this point, we opt
for model-free techniques to enable this process. Among the many
available techniques e.g., the Pearson correlation coefficient [18],
CC
Metric Definition
0.90
WRITE Memory Write traffic
0.88
READ Memory Read traffic
0.88
IPC
L3OCC LLC Occupancy
0.80
L2MPI LLC Accesses/Instruction 0.76
L3HIT LLC Hit Rate
0.40
Local NUMA bandwidth 0.20
LMB
Instructions/Cycle
Table 3: DDIO Contentiousness
Metric Definition
L3MISS LLC Miss Rate
L3OCC LLC Occupancy
L3HIT LLC Hit Rate
LMB
L2MISS CAR
L2HIT L2 Hit Rate
RMB
CC
0.98
0.87
0.79
0.76
0.76
0.36
Remote NUMA bandwidth 0.13
Local NUMA bandwidth
Metric Definition
READ Memory Read traffic
WRITE Memory Write traffic
LMB
L3MISS LLC Miss Rate
L3OCC LLC Occupancy
L3HIT LLC Hit Rate
RMB
CC
0.81
0.81
0.80
0.79
0.77
0.67
Remote NUMA bandwidth 0.15
Local NUMA bandwidth
Table 4: LLC Contentiousness
Table 5: Memory Contentiousness
information gain [52] or PCA, we use Pearson‚Äôs correlation coef-
ficient to analyze the statistical dependency between the various
PCM metrics and the observed performance of each target NF.
Observation 4: PCM metrics at CPU-socket- and System- level
granularities adequately capture aggregate contentiousness.
Table 2 lists the metrics that consistently exhibit high correlation
to performance across all experiments. Our first key observation
is that these metrics correspond to CPU-socket- or System-level
granularity (instead of core-level metrics). That is because per-
formance degradation is the result of the competition‚Äôs aggregate
contentiousness which is naturally captured at these granularities.6
We also observe that as there is no clear winner among the
chosen metrics in terms of the magnitude of correlation coefficient,
indicating that contentiousness entails multiple variable. This is
in agreement with ¬ß3 that argued for different sets of metrics for
different sources of contention. To confirm this hypothesis, we
revisit the experiments of ¬ß3 and identify the best metrics for each
type of contention.
Observation 5: Different sources of contention are best cap-
tured by different metrics. As NFs can depend on multiple con-
tention sources, contentiousness should be quantified with an
ensemble of metrics.
Tables 3, 4, and 5 show the highest ranked metrics for each source
of contention. We see that:
1. DDIO contention We find that DDIO contention is best quan-
tified through memory bandwidth utilization metrics. As DDIO
operations are managed by the socket‚Äôs DMA engine, packet
buffer evictions are not captured by LLC utilization metrics
and as such they prove to be uncorrelated to this source of
contention.
2. LLC contention In contrast to DDIO contention, LLC con-
tention is best captured through LLC-related metrics. The LLC
occupancy of the competition, its cache access rate, LLC miss
rate and LLC misses per instruction exhibit the highest correla-
tion with performance.
3. Memory Bandwidth Finally, with respect to memory band-
width contention we find that the key contentiousness metrics
are, unsurprisingly, memory bandwidth utilization metrics (e.g.,
Read/Write bandwidth) as well as the Local NUMA node band-
width utilization.
6In a setup that only uses one CPU socket, the correlation values at socket and system
level granularities coincide.
Figure 8: 2D rendering of sensitivity curves for VPN (left) and FlowStats
(right). Y axis is normalized performance.
In all, we conclude that out of the initial pool of ‚àº 600 PCM
metrics, the ‚àº 15 aggregate PCM metrics can capture the con-
tentiousness from the key chokepoints we previously identified.
These metrics will also serve as the input to a target‚Äôs sensitivity
model to predict performance.
5.2 Modeling Sensitivity
Sensitivity captures the relationship between the contentious-
ness of the competition and the performance of the target NF. Mod-
eling sensitivity, therefore, can be viewed as a regression problem as
its input (contentiousness of the competition) and output (target
NF performance) are both continuous variables.
Our first observation is that sensitivity modeling needs to happen
on a per NF-basis instead of having a global sensitivity model.
Indeed, with different NFs responding differently to the various
sources of contention, the importance of each metric in the context
of a sensitivity model is NF specific.
We train sensitivity models using synthetic, NF-specific con-
tentiousness observations, created as described in ¬ß5.1. At run time,
we replace the synthetic inputs with the aggregate contentiousness
of the real competitors. For testing, we generate for each NF and
architecture a dataset of real experiments where each target NF is
co-run with various combinations of NFs drawn from our pool of
NF instances with replacement. Each target NF is run 5 times on
each architecture with 150 different configurations of competitors.
Observation 6: Sensitivity can be a complex function that can-
not be captured by simple regression models.
We find that sensitivity is a non-linear and non-continuous func-
tion of its multivariate input and as such, it cannot be accurately
modeled with techniques such as regressions (linear/polynomial
etc.), decision trees, simple neural nets etc. Nonetheless, a com-
mon pattern that we detect across sensitivity functions are phase
transitions i.e., sharp changes in the properties of sensitivity as
a result of increasing contentiousness. For instance, for a fixed
value of competing CAR, low (competing) LLC occupancy values
6810121416182087.590.092.595.097.5100.0CAR: 539CAR: 608CAR: 813CAR: 1019CAR: 12248101214161820LLC Occupancy (MB)868890929496LLC MissRate: 126LLC MissRate: 147LLC MissRate: 210LLC MissRate: 273LLC MissRate: 336naturally trigger few evictions of target NF data. As a result, the
target‚Äôs performance decreases slowly as a function of the competi-
tor‚Äôs increasing occupancy. However, as soon as the aggregate LLC
occupancy of the target NF and its competitors exceed a critical
threshold i.e., the available cache space, the probability of target NF
evictions sharply increases and with that the rate of performance
degradation for the target NF (Figure 8).
Observation 7: Sensitivity can be modeled as a piecewise func-
tion of its inputs. Gradient Boosting Regression, an ensemble
modeling technique enables accurate sensitivity modeling.
This observation introduces our key insight behind sensitivity
modeling in SLOMO. By viewing sensitivity as a piecewise func-
tion of contentiousness, we can model the different sub-spaces of
sensitivity separately and then combine the resulting models into
a larger, comprehensive one. Fortunately, the machine learning
literature offers a class of methods, namely ensemble methods, that
are designed for that exact purpose by combining, into a compre-
hensive robust model, many smaller models that focus on specific
areas of the sensitivity space [22, 54].
Ensemble methods build a family of base estimators, with each
estimator focusing on a subsection of the function to be modeled.
These weaker base estimators are then aggregated to obtain a strong
learner that performs better. Among the several variations of ex-
isting ensemble techniques (e.g., bagging, boosting, stacking), we
choose Gradient Boosting Regression (GBR) [27].7 In ¬ß7 we quanti-
tatively show that its predictions outperforms a large collection of
other well-known modeling tools.
In summary, our modeling technique captures the intricacies of
a complex sensitivity function and improves on prior work which
uses simpler linear models.
5.3 Measuring Contentiousness
In ¬ß4.2, we introduced the example of three co-running NFs,
ùëÅ ùêπùê¥, ùëÅ ùêπùêµ, and ùëÅ ùêπùê∂ where we wanted to predict the performance
of ùëÅ ùêπùê¥ running alongside ùëÅ ùêπùêµ and ùëÅ ùêπùê∂. We now discuss (a) how
ùëÅ ùêπùêµ and ùëÅ ùêπùê∂‚Äôs contentiousness vectors, (ùëâùêµ, ùëâùê∂) are measured in
detail, and (b) how to compute their combined contentiousness,
ùëâùêµ,ùê∂ from measurements, extracted during offline profiling.
Measuring NFi‚Äôs contentiousness: A starting point might be to
measure the PCM counters for a given ùëÅ ùêπùëñ while this ùëÅ ùêπùëñ runs
alone on the server. However, this is inaccurate because an NF‚Äôs
resource utilization (and hence its vector ùëâùëñ) changes in the pres-
ence of competition (e.g., by as much as 5√ó for metrics like LLC
occupancy). That is, in the case of ùëÅ ùêπùêµ, we need to estimate ùëâ ùê∂
ùêµ
i.e., its contentiousness while competing with ùëÅ ùêπùê∂. We make two
key choices to estimate this vector.
(1) We measure contentiousness for ùëÅ ùêπùëñ while it is running against
the various synthetic competitors ùëÅ ùêπùë• ‚Äì many times over. Each
time ùëÅ ùêπùëñ is subjected to a unique ùëâùë•, we measure ùëÅ ùêπùëñ‚Äôs contentious-
ness resulting in a set of potential ùëâùëñ‚Äôs
7GBR is a technique that produces strong predictors with lower bias, by sequentially
fitting weak learners in an adaptive way, giving more importance to observations in
the dataset that were badly handled by previous models in the sequence.
Figure 9: FlowStats‚Äôs sensitivity changes with the number of concurrent
flows
(2) We then group the ùëâ ùë•
ùëñ ‚Äôs based on how many co-runners (utilized
cores) our synthetic competitor was simulating. For our example
above, we group all ùëâ ùë•
ùêµ where ùëÅ ùêπùêµ ran against one simulated co-
runner; we then take the average of all such vectors. This value is
our estimate of ùëâ ùê∂
ùêµ .
Hence, in reality, during the profiling phase we do not generate
one contentiousness vector for each NF. Instead, we measure one
contentiousness vector per configuration of the synthetic competi-
tor and during runtime, we select the appropriate subset of these
measurements based on the number of competitors.
Composition: Having estimated the individual contentiousness
of NF ùêµ and NFùê∂ in the presence of two competitors, the next step is
to compose them in order to get an estimate of ùëâùêµ,ùê∂. Composition is
possible because the aggregate contentiousness metrics we wish to
estimate are by definition the sum or average of the constituent per-
core metrics (e.g., the CAR of a CPU-socket is the sum of each core‚Äôs
CAR within the socket) [1]. Thus, having estimated the individual
contentiousness of each competitor, composing the aggregate boils
down to applying the appropriate linear operator for each metric.
¬ß7 evaluates the prediction accuracy of using this compositional
model versus observed actual aggregate contentiousness across
experiments and shows that our composition mechanism adds less
than 5% additional prediction error.
6 EXTRAPOLATING SENSITIVITY
So far, we have considered a rather strict definition of NFi that
includes its type (e.g., firewall, IDS etc.), configuration and traffic
profile. While this definition has simplified the exploration of the
sensitivity/contentiousness design space, NFi might experience
modifications during its lifecycle (e.g., migrations across servers,
changes in configuration or changes in traffic profile) which will
effectively lead to the creation of a new, unknown, NF instance,
NF‚Ä≤
i. A natural concern for operators is whether SLOMO‚Äôs design
is general enough to enable them to extrapolate quick-yet-accurate
performance predictions for NF‚Ä≤
i, without triggering a slow offline
profiling operation immediately after NFi is modified. In other
words, having the ability to extrapolate NF‚Ä≤
i‚Äôs performance predic-
tions by leveraging existing profiles of NFi can save the operator
time and thus enhance SLOMO‚Äôs usability. In this section, we use
a running example to present empirical heuristics to extract NF‚Ä≤
is
sensitivity as well as their limitations.
We consider the example of FlowStats, an NF that is heavily sen-
sitive to memory contention and explore its changes in sensitivity
as a function of the number of unique traffic flows it receives (all
100200300400500600Competing CAR (MRefs/sec - 20MB Occupancy)0.020.040.060.080.10Throughput (Mpps)FlowStats-400K flowsFlowStats-350K flows0.02.55.07.510.012.515.017.520.0Competing Occupancy (MB - 100MRefs/sec)0.0500.0750.1000.125Throughput (Mpps)FlowStats-400K flowsFlowStats-350K flowsother parameters of configuration, traffic and competition remain
fixed). In our experiment, we first reduce the number of traffic flows
by 50K from 400K to 350K (‚àº 12%) and observe that as that number
decreases (1) the NF becomes less sensitive to the same amount