CC
0.78
0.78
0.893
0.88
0.8
0.82
0.88
0.68
0.75
0.72
0.89
0.68
0.9
0.24
0.18
0.06
Table 2: PCM metrics with high correlation with performance. The CC
column shows the mean correlation coefficient across experiments.
(2) to the packet-processing datapath by performing configurable
numbers of memory operations (i.e., random Reads and/or Writes)
to a data structure of configurable size stored in the LLC. SLOMO
exercises these configurations for various traffic patterns (i.e., rate,
packet sizes and flow counts) and number of co-running instances
of the synthetic NF. SLOMO profiles each NF with more than 1000
different configurations of the synthetic workload and the resulting
dataset consists of the following sets of measurements: (1) PCM
values when the synthetic workload and NF under test run solo; (2)
PCM values when both the synthetic workload co-runs with the NF
under test; and (3) Performance of target NF when running with the
synthetic competitor. In the rest of the section, we show how we
use each one of these datasets. Note that the synthetic competing
workloads are only used during offline profiling and not in runtime.
Assumptions on NF Deployment: SLOMO follows best prac-
tices established by prior work that optimize performance stability
for software packet processors [4, 11, 12]. Specifically, we run NFs
on dedicated and isolated cores that use local memory and NICs
(NUMA affinity), we ensure interrupt-core affinity is maintained
in cases where packet acceleration is not used, we disable power-
saving features of the CPU (e.g., idle states, frequency scaling) and
disable transparent huge pages.
5.1 Contentiousness Metrics Selection
A simple way to select contentiousness metrics would be to use
the entire PCM vector (âˆ¼600 metrics). Doing so, however, is â€œnoisyâ€
as it includes metrics that are unrelated to plausible sources of
contention (e.g., metrics of unused CPU sockets) and, thus, hurts
model accuracy and interpretability [29].
Ideally, we need to identify a subset of expressive PCM metrics
i.e., metrics with strong predictive power in the context of a sensi-
tivity model. Given that we donâ€™t have a model at this point, we opt
for model-free techniques to enable this process. Among the many
available techniques e.g., the Pearson correlation coefficient [18],
CC
Metric Definition
0.90
WRITE Memory Write traffic
0.88
READ Memory Read traffic
0.88
IPC
L3OCC LLC Occupancy
0.80
L2MPI LLC Accesses/Instruction 0.76
L3HIT LLC Hit Rate
0.40
Local NUMA bandwidth 0.20
LMB
Instructions/Cycle
Table 3: DDIO Contentiousness
Metric Definition
L3MISS LLC Miss Rate
L3OCC LLC Occupancy
L3HIT LLC Hit Rate
LMB
L2MISS CAR
L2HIT L2 Hit Rate
RMB
CC
0.98
0.87
0.79
0.76
0.76
0.36
Remote NUMA bandwidth 0.13
Local NUMA bandwidth
Metric Definition
READ Memory Read traffic
WRITE Memory Write traffic
LMB
L3MISS LLC Miss Rate
L3OCC LLC Occupancy
L3HIT LLC Hit Rate
RMB
CC
0.81
0.81
0.80
0.79
0.77
0.67
Remote NUMA bandwidth 0.15
Local NUMA bandwidth
Table 4: LLC Contentiousness
Table 5: Memory Contentiousness
information gain [52] or PCA, we use Pearsonâ€™s correlation coef-
ficient to analyze the statistical dependency between the various
PCM metrics and the observed performance of each target NF.
Observation 4: PCM metrics at CPU-socket- and System- level
granularities adequately capture aggregate contentiousness.
Table 2 lists the metrics that consistently exhibit high correlation
to performance across all experiments. Our first key observation
is that these metrics correspond to CPU-socket- or System-level
granularity (instead of core-level metrics). That is because per-
formance degradation is the result of the competitionâ€™s aggregate
contentiousness which is naturally captured at these granularities.6
We also observe that as there is no clear winner among the
chosen metrics in terms of the magnitude of correlation coefficient,
indicating that contentiousness entails multiple variable. This is
in agreement with Â§3 that argued for different sets of metrics for
different sources of contention. To confirm this hypothesis, we
revisit the experiments of Â§3 and identify the best metrics for each
type of contention.
Observation 5: Different sources of contention are best cap-
tured by different metrics. As NFs can depend on multiple con-
tention sources, contentiousness should be quantified with an
ensemble of metrics.
Tables 3, 4, and 5 show the highest ranked metrics for each source
of contention. We see that:
1. DDIO contention We find that DDIO contention is best quan-
tified through memory bandwidth utilization metrics. As DDIO
operations are managed by the socketâ€™s DMA engine, packet
buffer evictions are not captured by LLC utilization metrics
and as such they prove to be uncorrelated to this source of
contention.
2. LLC contention In contrast to DDIO contention, LLC con-
tention is best captured through LLC-related metrics. The LLC
occupancy of the competition, its cache access rate, LLC miss
rate and LLC misses per instruction exhibit the highest correla-
tion with performance.
3. Memory Bandwidth Finally, with respect to memory band-
width contention we find that the key contentiousness metrics
are, unsurprisingly, memory bandwidth utilization metrics (e.g.,
Read/Write bandwidth) as well as the Local NUMA node band-
width utilization.
6In a setup that only uses one CPU socket, the correlation values at socket and system
level granularities coincide.
Figure 8: 2D rendering of sensitivity curves for VPN (left) and FlowStats
(right). Y axis is normalized performance.
In all, we conclude that out of the initial pool of âˆ¼ 600 PCM
metrics, the âˆ¼ 15 aggregate PCM metrics can capture the con-
tentiousness from the key chokepoints we previously identified.
These metrics will also serve as the input to a targetâ€™s sensitivity
model to predict performance.
5.2 Modeling Sensitivity
Sensitivity captures the relationship between the contentious-
ness of the competition and the performance of the target NF. Mod-
eling sensitivity, therefore, can be viewed as a regression problem as
its input (contentiousness of the competition) and output (target
NF performance) are both continuous variables.
Our first observation is that sensitivity modeling needs to happen
on a per NF-basis instead of having a global sensitivity model.
Indeed, with different NFs responding differently to the various
sources of contention, the importance of each metric in the context
of a sensitivity model is NF specific.
We train sensitivity models using synthetic, NF-specific con-
tentiousness observations, created as described in Â§5.1. At run time,
we replace the synthetic inputs with the aggregate contentiousness
of the real competitors. For testing, we generate for each NF and
architecture a dataset of real experiments where each target NF is
co-run with various combinations of NFs drawn from our pool of
NF instances with replacement. Each target NF is run 5 times on
each architecture with 150 different configurations of competitors.
Observation 6: Sensitivity can be a complex function that can-
not be captured by simple regression models.
We find that sensitivity is a non-linear and non-continuous func-
tion of its multivariate input and as such, it cannot be accurately
modeled with techniques such as regressions (linear/polynomial
etc.), decision trees, simple neural nets etc. Nonetheless, a com-
mon pattern that we detect across sensitivity functions are phase
transitions i.e., sharp changes in the properties of sensitivity as
a result of increasing contentiousness. For instance, for a fixed
value of competing CAR, low (competing) LLC occupancy values
6810121416182087.590.092.595.097.5100.0CAR: 539CAR: 608CAR: 813CAR: 1019CAR: 12248101214161820LLC Occupancy (MB)868890929496LLC MissRate: 126LLC MissRate: 147LLC MissRate: 210LLC MissRate: 273LLC MissRate: 336naturally trigger few evictions of target NF data. As a result, the
targetâ€™s performance decreases slowly as a function of the competi-
torâ€™s increasing occupancy. However, as soon as the aggregate LLC
occupancy of the target NF and its competitors exceed a critical
threshold i.e., the available cache space, the probability of target NF
evictions sharply increases and with that the rate of performance
degradation for the target NF (Figure 8).
Observation 7: Sensitivity can be modeled as a piecewise func-
tion of its inputs. Gradient Boosting Regression, an ensemble
modeling technique enables accurate sensitivity modeling.
This observation introduces our key insight behind sensitivity
modeling in SLOMO. By viewing sensitivity as a piecewise func-
tion of contentiousness, we can model the different sub-spaces of
sensitivity separately and then combine the resulting models into
a larger, comprehensive one. Fortunately, the machine learning
literature offers a class of methods, namely ensemble methods, that
are designed for that exact purpose by combining, into a compre-
hensive robust model, many smaller models that focus on specific
areas of the sensitivity space [22, 54].
Ensemble methods build a family of base estimators, with each
estimator focusing on a subsection of the function to be modeled.
These weaker base estimators are then aggregated to obtain a strong
learner that performs better. Among the several variations of ex-
isting ensemble techniques (e.g., bagging, boosting, stacking), we
choose Gradient Boosting Regression (GBR) [27].7 In Â§7 we quanti-
tatively show that its predictions outperforms a large collection of
other well-known modeling tools.
In summary, our modeling technique captures the intricacies of
a complex sensitivity function and improves on prior work which
uses simpler linear models.
5.3 Measuring Contentiousness
In Â§4.2, we introduced the example of three co-running NFs,
ğ‘ ğ¹ğ´, ğ‘ ğ¹ğµ, and ğ‘ ğ¹ğ¶ where we wanted to predict the performance
of ğ‘ ğ¹ğ´ running alongside ğ‘ ğ¹ğµ and ğ‘ ğ¹ğ¶. We now discuss (a) how
ğ‘ ğ¹ğµ and ğ‘ ğ¹ğ¶â€™s contentiousness vectors, (ğ‘‰ğµ, ğ‘‰ğ¶) are measured in
detail, and (b) how to compute their combined contentiousness,
ğ‘‰ğµ,ğ¶ from measurements, extracted during offline profiling.
Measuring NFiâ€™s contentiousness: A starting point might be to
measure the PCM counters for a given ğ‘ ğ¹ğ‘– while this ğ‘ ğ¹ğ‘– runs
alone on the server. However, this is inaccurate because an NFâ€™s
resource utilization (and hence its vector ğ‘‰ğ‘–) changes in the pres-
ence of competition (e.g., by as much as 5Ã— for metrics like LLC
occupancy). That is, in the case of ğ‘ ğ¹ğµ, we need to estimate ğ‘‰ ğ¶
ğµ
i.e., its contentiousness while competing with ğ‘ ğ¹ğ¶. We make two
key choices to estimate this vector.
(1) We measure contentiousness for ğ‘ ğ¹ğ‘– while it is running against
the various synthetic competitors ğ‘ ğ¹ğ‘¥ â€“ many times over. Each
time ğ‘ ğ¹ğ‘– is subjected to a unique ğ‘‰ğ‘¥, we measure ğ‘ ğ¹ğ‘–â€™s contentious-
ness resulting in a set of potential ğ‘‰ğ‘–â€™s
7GBR is a technique that produces strong predictors with lower bias, by sequentially
fitting weak learners in an adaptive way, giving more importance to observations in
the dataset that were badly handled by previous models in the sequence.
Figure 9: FlowStatsâ€™s sensitivity changes with the number of concurrent
flows
(2) We then group the ğ‘‰ ğ‘¥
ğ‘– â€™s based on how many co-runners (utilized
cores) our synthetic competitor was simulating. For our example
above, we group all ğ‘‰ ğ‘¥
ğµ where ğ‘ ğ¹ğµ ran against one simulated co-
runner; we then take the average of all such vectors. This value is
our estimate of ğ‘‰ ğ¶
ğµ .
Hence, in reality, during the profiling phase we do not generate
one contentiousness vector for each NF. Instead, we measure one
contentiousness vector per configuration of the synthetic competi-
tor and during runtime, we select the appropriate subset of these
measurements based on the number of competitors.
Composition: Having estimated the individual contentiousness
of NF ğµ and NFğ¶ in the presence of two competitors, the next step is
to compose them in order to get an estimate of ğ‘‰ğµ,ğ¶. Composition is
possible because the aggregate contentiousness metrics we wish to
estimate are by definition the sum or average of the constituent per-
core metrics (e.g., the CAR of a CPU-socket is the sum of each coreâ€™s
CAR within the socket) [1]. Thus, having estimated the individual
contentiousness of each competitor, composing the aggregate boils
down to applying the appropriate linear operator for each metric.
Â§7 evaluates the prediction accuracy of using this compositional
model versus observed actual aggregate contentiousness across
experiments and shows that our composition mechanism adds less
than 5% additional prediction error.
6 EXTRAPOLATING SENSITIVITY
So far, we have considered a rather strict definition of NFi that
includes its type (e.g., firewall, IDS etc.), configuration and traffic
profile. While this definition has simplified the exploration of the
sensitivity/contentiousness design space, NFi might experience
modifications during its lifecycle (e.g., migrations across servers,
changes in configuration or changes in traffic profile) which will
effectively lead to the creation of a new, unknown, NF instance,
NFâ€²
i. A natural concern for operators is whether SLOMOâ€™s design
is general enough to enable them to extrapolate quick-yet-accurate
performance predictions for NFâ€²
i, without triggering a slow offline
profiling operation immediately after NFi is modified. In other
words, having the ability to extrapolate NFâ€²
iâ€™s performance predic-
tions by leveraging existing profiles of NFi can save the operator
time and thus enhance SLOMOâ€™s usability. In this section, we use
a running example to present empirical heuristics to extract NFâ€²
is
sensitivity as well as their limitations.
We consider the example of FlowStats, an NF that is heavily sen-
sitive to memory contention and explore its changes in sensitivity
as a function of the number of unique traffic flows it receives (all
100200300400500600Competing CAR (MRefs/sec - 20MB Occupancy)0.020.040.060.080.10Throughput (Mpps)FlowStats-400K flowsFlowStats-350K flows0.02.55.07.510.012.515.017.520.0Competing Occupancy (MB - 100MRefs/sec)0.0500.0750.1000.125Throughput (Mpps)FlowStats-400K flowsFlowStats-350K flowsother parameters of configuration, traffic and competition remain
fixed). In our experiment, we first reduce the number of traffic flows
by 50K from 400K to 350K (âˆ¼ 12%) and observe that as that number
decreases (1) the NF becomes less sensitive to the same amount