suggests  that  primate  dominance  games  that  humans 
engage in at a very early age may show that territorial 
“security” is hard wired into our brains.  
Users  need  to  understand  how  to  use  the  security 
controls  that  are  directly  relevant  to  their  task  and 
context. The desired end result of usable security is that 
security  controls  are  applied  appropriately  and 
effectively to provide protection. The risks of using the 
features  they  protect  are  thus  decreased,  and  are  not 
exacerbated  by  the  use  of  and  belief  in  mechanisms 
that  are  not  likely  to  withstand  the  most  common  or 
risky  attacks.  The  goal  is  that  security  controls  be 
effectively used.  
I  would rephrase the  CRA’s grand challenge stated 
above to be:  
“Give 
all 
users 
(including  developers, 
administrators,  and  end-users)  security  controls 
that protect them, their systems, and their privacy, 
that  they  can  use  appropriately  in  the  dynamic, 
pervasive computing environments of the present 
and the future.”  
2.1.2. User Slip-ups Are Not User Errors.  
I didn’t do it.  
  Bart Simpson, cartoon character 
the 
When a security breach is said to be caused by “user 
error”,  the  desired  implication  is  that  the  breach  was 
not the responsibility of the (computer) system, but of 
the  user.  Acceptance  of  that  implication  is  one  of  the 
roadblocks  to  the  grand  challenge  of  usable  security. 
For  products  that  are  deployed  in  both  the  enterprise 
and  consumer  spaces,  the  community  of  security 
experts and society at large should never accept “user 
error”  as  a  source  of  a  security  problem.  If  a  non-
malicious,  mistaken  end  user 
is  blamed  for  a 
vulnerability  or  breach,  we  have  to  ask,  why  did  the 
system  make 
insecure  option  so  easy  and 
attractive?  If  the  error  was  “skill-based”  [39],  an 
automatic and unconscious slip-up, then basic usability 
techniques  should  be  brought  to  bear.  These  will 
minimize the potential of serious breaches arising from 
the  equivalent  of  a  typographical  error.  For  example, 
the  Therac-25  error  [29]  which  killed  several  people, 
was  due  to  a  bad  editor.  If  the  “user  error”  is  a 
conscious  action  (or  lack  of  a  conscious  action)  that 
was mistaken, then the design problem that needs to be 
fixed runs deeper.  
Tog [49] points out that a security breach is the fault 
of the security designer (assuming a single product and 
the  actual  existence  of  a  security  designer).  Security 
professionals, like the stereotype of legal professionals, 
may run on the first principle of ensuring that nothing 
bad  happens  that  can  be  attributed  to  their  area  of 
responsibility.  A  big  worry  for  any  product’s  security 
architect  is,  “What  is  the  likelihood  that  our  product 
can and will be exploited in a way that makes it to the 
Proceedings of the 21st Annual Computer Security Applications Conference (ACSAC 2005) 
1063-9527/05 $20.00 © 2005 IEEE 
cover  of  the  New  York  Times?”  Sometimes  product 
designers and architects believe that if they design the 
system  so  that  the  hard  security  choices  are  the 
responsibility of a user or customer, their company can 
say to any related breaches a customer might suffer, “I 
didn’t do it”.  
Difficult-to-use security controls in one place in the 
system encourage poor security decisions in other parts 
of  the  system.  Bill  Cheswick  [8]  points  out  that  most 
systems overuse the setuid to root function which gives 
a  program  the  privileges  of  an  administrator.  I  agree 
with  Greenwald  [22]  that  the  cause  is  designing  a 
service  that  does  not  require  that  function  is  difficult. 
The system’s protections are not easy for the majority 
of developers to use appropriately (and developers are 
people  too).  Initial  usability 
testing  of  an  early 
enterprise web conferencing system showed that users 
were immediately confused by  the browser’s  ActiveX 
trust  dialogs.  They  could  not  get  to  the  desired 
functionality  because  they  did  not  understand  they 
could take the more sophisticated approach that today’s 
users  do  to  such  dialogs  (click  OK  [61]).  The  most 
effective  workaround available  was for the product to 
explain  to  the  user  the  infrastructure  security  model 
that they  found confusing. The initialization screen of 
the  product  explains  the  user’s  choices  when  faced 
with  a  security  decisions  and  the  results  of  those 
choices.  Similarly,  in  early  deployments  of  multilevel 
systems, users regularly declassified documents to the 
lowest level possible. Providing reasonable and usable 
use  cases  of  security  functionality  needs  to  become 
accepted  before  system  interactions  like  these  will 
diminish.  
Figure 1: IBM Sametime® initialization 
screen 
to  resolve  a  security  or 
An  area  that  signals  the  strong  possibility  of  “user 
error” is any security procedure that includes a step that 
is  too  vague  to  be  precisely  documented,  even  as  an 
example.  For  example,  users  are  often  sent  “out  of 
band” 
trust  question. 
Brustoloni‘s  work  [Brustoloni]  shows  one  way  to 
approach  that  challenge;  ensure  that  out  of  band 
contact  information  is  available  in  band.  Assuming  a 
secured 
contact 
information  can  be  extended 
to  computer  based 
methods of real time communication, including IM or 
VoIP.  
infrastructure, 
collaboration 
in 
Another architectural area that attracts “user errors” 
is error cases. Every error message a user sees should 
be  understandable  and  actionable,  but  often  they  are 
not,  particularly 
the  security  area.  Consumer 
operating system error messages will tell users how to 
increase their paging file size, showing that useful error 
messages  for  complex  system  problems  are  possible. 
Messages  telling  the  operating  system  user  how  to 
contact  their  system  administrator  are  not  useful  to 
consumers,  but  are  to  users  in  an  enterprise  with 
accessible system administrators. Many security errors 
and warnings leave users wondering what the problem 
means  and  what  they  should  do.  Such  warnings  are 
really  only  a  defense  against  blame,  not  an 
enhancement to security.   
The  various  warnings  about  SSL  server  certificates 
are  a  case  in  point.  Xia  and  Brustoloni’s  [55]  work 
attempts 
to  make  every  error  message  both 
understandable  and  actionable.  In  general,  the  SSL 
dialogs he suggests are oriented towards https and the 
public web. SSL can be used for other protocols (IIOP, 
SIP)  and  for  enterprise  intranet  activities.  In  addition, 
there  are  error  cases  that  SSL  can  encounter  that  are 
not covered by this work. If the server certificate is not 
trusted or the DN does not match the host address, then 
the potential vulnerability is explicable to the user. But 
what does it mean if the validity dates of the certificate 
have not arrived yet? Why should the user care about 
that? More work like Brustoloni’s on handling security 
error cases is needed. 
2.1.3. Marketing Usable Security.  
Sell when you can: you are not for all markets.  
  As You Like It, Act 3, scene v 
Usable  security  is  obviously  a  desirable  quality  in 
commercial  software.  Enterprise  customers  explicitly 
request software that can be deployed securely  with a 
low  Total  Cost  of  Ownership  (TCO),  which  equates 
directly  to  the  usability  of  the  security.  This  market 
pull  should  increase  the  technology  transfer  of  user-
Proceedings of the 21st Annual Computer Security Applications Conference (ACSAC 2005) 
1063-9527/05 $20.00 © 2005 IEEE 
centered security into products, or increase the rate of 
innovation  in  usable  security  in  product  development. 
The market does not seem to have done so, beyond a 
small  number  of 
those 
specializing in security products that are able to create 
market value from emphasizing usable security [5]. 
companies, 
including 
When  it  comes  to  allocating  resources  in  product 
development, everything is a cost/benefit tradeoff. For 
most software products, little attention is paid to usable 
security until a substantial exploit can be attributed to 
the  lack  of  usable  security,  or  potentially  solved  by 
more of it. For example, most mail is not signed using 
S/MIME,  and  could  have  a  forged  sender.  The 
difficulties  with  deploying  and  understanding  most 
S/MIME  implementations  and  the  vulnerability  that 
left did not draw much attention until the use of spam 
(including  ad-spam,  scam-spam,  and  attack-spam) 
became  a  substantial  problem  with  email  reliance.  As 
we have seen, recognition of exploits is how most users 
engage with security. Thus exploits increase the ability 
to  justify  the  resource  allocation  to  usable  security 
when  the  tradeoff  of  resources  did  not  seem  justified 
before  the  widely  recognized  breaches.  In  this  model, 
economic roadblocks can be overcome by concrete and 
visible exploits, stronger explicit customer demand, or 
decreasing  the  cost  of  user-centered  security.  Let  us 
look at each of these economic drivers in turn.  
resources 
internal  developers  and  users, 
Practically  anyone  who  has  ever  worked  on  the 
security  of  a  shipping  product  knows  how  security 
vulnerabilities  are  dealt  with  by  her  organization. 
Organizations that explicitly track them will also triage 
them,  dedicating 
the  worst 
vulnerabilities  first.  The  list  of  vulnerabilities  comes 
from 
internal  (or 
internally  contracted) 
testers,  ethical  hackers  and 
advisories,  and  external  sources  (customers,  advisory 
organizations,  ethical  hackers,  not-so-ethical  hackers). 
The  not-so-ethical  hackers  stand  out  from  a  process 
point  of  view  because  they  do  not  work  with  the 
organization  to  identify  and  triage  the  vulnerability. 
They  exploit  it,  advertise  it  or  sell  it  for  personal 
reasons.  
Once  a  vulnerability  has  been  exploited  or 
advertised, the resources devoted to fixing it increase. 
Resources  are  also  dedicated  to  responding  to  the 
exploit.  If  an  organization  does  not  have  an  internal 
process  for  triaging  and  fixing  vulnerabilities,  the 
overall quality of the security in the code base is likely 
to increase. If they do have such a process, it is likely 
that  the  exploit  causes  resources  to  be  pulled  away 
from  vulnerabilities  that  by  objective  measures  are 
worse.  In  either  case,  resources  are  pulled  away  from 
other security-related activities. From a systemic point 
of view, exploiting or advertising vulnerabilities is not 
the most effective way to increase the security quality 
of  our  products.  A  transparent  security  quality 
to 
fixing 
process  is  [30].  That  process  should  include  HCI  as 
well as assurance aspects.  
techniques 
More  proactively,  there  are  ways  to  increase  the 
market demand for usable protection before an exploit 
highlights  the  gap.  Persuasion  techniques  [53]  can  be 
used to make the threats and the risks of lack of usable 
include  social 
security  clear.  These 
marketing,  which 
qualities 
(professionalism,  loyalty)  with  the  desired  behavior 
(security-aware purchasing) and negative qualities with 
the  lack.  Another  persuasion  technique  equates  safety 
with  being  a  less  attractive  target  (through  the  use  of 
more  obviously  secure  software).  The  more  extreme 
social  persuasion  techniques  equate  fear,  uncertainty, 
and doubt with security oblivious behaviors.  
associates 
positive 
Marketing 
campaigns 
can  generate  positive 
consumer  awareness  and  pull  for  previously  obscure 
attributes (“Brown eggs are local eggs, and local eggs 
are  fresh.”).  Quantifying  security  risk  [6]  through 
insurance,  certification,  or  other  means  enables  more 
accurate  and  explicit  cost/benefit  tradeoffs  and  can 
provide  a  factual  basis  to  marketing  efforts  that 
emphasize the desirability of usable security. Insurance 
can reduce real risk and enhance the  feeling of safety 
that  usable  security  should  provide.  Since  people 
understand security best as protection against risks and 
exploits, consumers need to be told explicitly what they 
can  be  protected  from.  Usable  security  mechanisms 
must  be  designed 
those  protections. 
Checklists  are  a  particularly  attractive  method  for 