frames with more than 1500 B payload) on both machines.
The current average path length in the Internet is less than
4 AS-level hops [24, 33, 44, 45]. However, as we expect that
number to increase due to the beneﬁts of being an AS in a
path-aware Internet, we consider path lengths of up to 16
AS-level hops in our evaluation (the current average number
of router-level hops is 13).
6.2 Performance Evaluation
In this section, we evaluate the performance of our imple-
mentation in terms of throughput (total trafﬁc) and goodput
(payload trafﬁc). Note that we account for the full header
overhead as described above when referencing the goodput.
Source For the evaluation of the source we assume that
it has already fetched the necessary hop authenticators and
DRKeys, which corresponds to the situation of an existing
connection. The throughput achieved by the source (using
a single CPU core) is shown in Fig. 1. For packets of p ≥
500B and path lengths of (cid:96) ≤ 8, the prototype implementation
consistently achieves throughput above 2 Gbps. Figures 7
and 8 in Appendix A further illustrate the parallelizability
of the implementation, which enables throughputs of tens of
Gbps, and the linear increase of the processing time with both
payload size and path length.
The processing at the source and destination is similar for
ICING, OPT, and PPV; in all protocols either a MAC or hash
is calculated over the packet’s payload, which dominates the
computational effort. In the future, these cryptographic com-
putations could be ofﬂoaded to multiple dedicated hardware
units in network-interface cards (NICs).
USENIX Association
29th USENIX Security Symposium    551
]
s
p
b
G
[
t
u
p
h
g
u
o
r
h
T
4
3
2
1
0
p = 100B
p = 500B
p = 1000B
p = 1500B
2
4
8
16
AS-Level Path Length (cid:96)
Figure 1: EPIC L3 packet throughput generated by the source
on a single core for different payload sizes.
Router Figure 2 shows the forwarding performance of an
EPIC L3 router for a path of length (cid:96) = 8. In these measure-
ments, we assume no cached hop authenticators or DRKeys,
they are always recalculated on the ﬂy. For packets with a
payload p ≥ 500B, the 40 Gbps link is saturated for all path
lengths using only 4 cores; using 16 cores, the link is even
saturated for small packets (p = 100B). As the implemen-
tation is easily parallelizable (see Fig. 6 in Appendix A), it
can be used even on 100 Gbps or 400 Gbps links by adding
more processing cores or dedicated hardware. An important
observation is that the processing time of the router is 445 –
460 ns independent of both payload size and path length. The
forwarding performance in terms of Mpps (million packets
per second) is thus also independent of these parameters and
amounts to approximately 2 Mpps per processing core. These
results are further illustrated by Figs. 4–6 in Appendix A.
The processing on routers is similar for all levels of EPIC,
OPT, and PPV, which all have a small constant number of
cryptographic operations. In ICING, every router calculates
both a hash and a MAC over the payload and in addition
performs (cid:96) symmetric cryptographic operations (one for each
router). In the software implementation provided by ICING’s
authors [36], each router has a processing time of ∼50µs for
(cid:96) = 10, which is two orders of magnitude slower than EPIC. If
keys are not cached, additional Difﬁe–Hellman computations
are necessary, leading to processing times of ≥ 100ms [19].
Comparison to IP Comparing the performance of EPIC
to IP is challenging due to the strong impact of routing-
table sizes on software performance and hardware cost for
IP. Highly optimized software switch implementations like
DPDK vSwitch achieve throughputs of ∼11Mpps on a single
core (corresponding to a processing time of approximately
90 ns) [20]. However, these values are only valid for small
routing tables when no memory accesses are necessary (as a
single DRAM access takes ∼70ns). Our prototype implemen-
tation is approximately ﬁve times slower at ∼2Mpps, but the
throughput is independent of the number of concurrent ﬂows
due to packet-carried forwarding state. Furthermore, the pro-
cessing time could be further reduced through optimizations
such as concurrent execution of cryptographic operations.
]
s
p
b
G
[
e
c
n
a
m
r
o
f
r
e
P
g
n
i
d
r
a
w
r
o
F
40
30
20
10
0
100
500
2 cores (GP)
4 cores (GP)
8 cores (GP)
16 cores (GP)
1,000
2 cores (TP)
4 cores (TP)
8 cores (TP)
16 cores (TP)
1,500
Payload [B]
Figure 2: Throughput (TP) and goodput (GP) of a router
plotted against the payload for 2, 4, 8, and 16 cores and (cid:96) = 8.
Table 4: Communication overhead in bytes in EPIC, ICING,
OPT, and PPV due to security-related ﬁelds.
L1
L2–L3
L0
3(cid:96) 5(cid:96) + 8 5(cid:96) + 24 42(cid:96) + 13 19(cid:96) + 52
ICING
OPT
for (cid:96) = 8 24
48
64
349
204
PPV
64
64
Hardware implementations, which are particularly relevant
in a production deployment, compare even more favorably.
IP routers require large amounts of expensive ternary content-
addressable memory (TCAM) for longest-preﬁx matching. In
contrast, EPIC requires very little additional hardware for its
cryptographic operations. Naous et al. [36] have compared
the gate count of FPGA implementations of ICING and IP
routers and found comparable values (13.4 million vs. 8.7
million gates) even for very small amounts of TCAM in the
IP router; in comparison, hardware implementations of AES
are very efﬁcient and only require 13,000 gates [1].
6.3 Communication Overhead
In addition to processing overhead and performance, we also
evaluate the communication overhead of EPIC and compare
it to other systems. To allow for a meaningful comparison,
we evaluate only the overhead owed to security here, since
the normal routing headers (e.g., IPv4/v6, SCION) depend on
the underlying networking architecture. Thus, we use HD to
refer to the size of all security-related header ﬁelds (in EPIC,
these are tspkt, VSD, and Si, Vi for all hops i). We deﬁne the
goodput ratio as the ratio between goodput and throughput,
or, equivalently, as the ratio of payload and total packet size,
GR = p
p+HD. Table 4 shows the size of the additional header
for all considered systems, Fig. 3 depicts the goodput ratio.
We ﬁnd that the goodput ratio is high for all variants of
EPIC. For (cid:96) = 8, the additional header is between 24 B for
EPIC L0 and 64 B for EPIC L3, which corresponds to a good-
put ratios 98 % and 94 %, respectively, for payloads of size
p = 1000B. The goodput ratio of OPT is signiﬁcantly worse
with GR ∼ 83% for the same values of (cid:96) and p, and does
not scale as well as the overhead of EPIC with the length of
the paths. For ICING, we ﬁnd a ﬁve times larger overhead
552    29th USENIX Security Symposium
USENIX Association
100
90
80
70
]
%
[
o
i
t
a
R
t
u
p
d
o
o
G
2
4
L0
L1
L2, L3
PPV
sec. L0
OPT
sec. OPT
ICING
14
16
8
12
6
AS-Level Path Length (cid:96)
10
Figure 3: Goodput ratio of different protocols as a function
of the AS-level path length (cid:96) for a 1000 B payload, calculated
from Table 4. “sec. L0” and “sec. OPT” correspond to L0
and OPT with authenticators of 16 B that are required to rule
out brute-force attacks. For GR < 2/3, the total packet size
exceeds the maximum size for an Ethernet payload.
than EPIC L2–3 and GR ∼ 74% for these parameters. As
PPV performs checks at only two routers along the path, its
overhead is constant in the path length. Still, EPIC L2–3 have
a higher goodput ratio than PPV for path lengths up to (cid:96) = 8.
In Table 4, authenticators for path authorization are 3 B
for EPIC L0 and OPT (the default for SCION on which they
are based). This is despite only achieving property P1 in the
basic-attacker model, meaning that brute-force attacks are un-
mitigated and exploitable for practical attacks. To correct for
this, the size of HVFs would need to be increased to a similar
length of other brute-force-resistent ﬁelds like the destination
validation ﬁeld, i.e., 16 B. Considering these modiﬁcations,
which are shown by “sec. L0” and “sec. OPT” in Fig. 3, the
goodput ratio is even more favorable for EPIC L1–3, which
signiﬁcantly outperform both protocols.
6.4 Other Overhead
State at Routers
In EPIC, routers can perform all crypto-
graphic checks and updates with a single AS-speciﬁc secret
value, there is no per-host or per-ﬂow state required. This is
equivalent to OPT and PPV, which both rely on DRKey, but
a signiﬁcant advantage compared to ICING, which requires
per-ﬂow state [28, 36]. In terms of routing information, bor-
der routers only need to store intra-AS information as packet
headers contain the inter-AS forwarding information. This is
a huge improvement over the current Internet, shared by all
architectures based on packet-carried forwarding state.
Replay Suppression All EPIC levels except L0 depend
on a replay-suppression system for freshness (P2), which
has additional state and overhead. Since this task can be
taken over by dedicated machines, we did not include it in
the router measurements above. Prototypes that are entirely
implemented in software have been deployed successfully on
10 Gbps links [29]. In turn, EPIC L1–3 provide important
properties for replay suppression: (i) the system can use the
timestamp to discard packets that are expired, thus limiting the
number of packets that need to be tracked in Bloom ﬁlters, and
(ii) by authenticating all packet contents tracked by the replay-
suppression system, attackers are prevented from modifying
unauthenticated ﬁelds and replaying packets. If the replay-
suppression system were not deployed, the packet timestamp
could still be used to ﬁlter out expired packets, and an attacker
could only replay packets in a very short time window due to
the check in line 4 of Algorithm 2.
Control-Plane Overhead In EPIC, end hosts have to re-
quest paths from the path server and, for EPIC L2–3, host-
level symmetric keys from the key server, before they can
communicate with a new destination. We assume that the
underlying path-aware Internet architecture minimizes la-
tency by locally caching public paths, e.g., at path servers in
SCION [37, §7.2]. End hosts also cache paths themselves,
such that only the initial packet to a new destination requires
a path lookup. This caching strategy can also be applied
to EPIC’s hop authenticators and the host keys required in
EPIC L2–3. Concretely, AS-level keys can be set up between
every pair of ASes ahead of time (either using PISKES /
DRKey or Passport) such that local key servers can immedi-
ately respond to requests by end hosts. In the current Internet,
storing 16 B keys for each AS only amounts to ∼1MB [34].
Given that path and key information is available at local ASes,
the additional latency incurred in EPIC is minimal: only the
round-trip time between the source host and its own AS, and
the destination host and its AS is added to the connection
setup. End hosts can cache both paths and host keys, which
eliminates additional latency for subsequent packets. Further
optimization would be possible by combining DNS, path, and
key requests, which would eliminate all additional latency for
the initial packet compared to today’s Internet.
7 Discussion
Low Communication Overhead of EPIC The beneﬁt of
EPIC’s lower overhead compared to OPT comes in part from
the fact that EPIC does not use separate ﬁelds for path au-
thorization on the one hand, and for authentication and path
validation on the other hand. The larger contributor to a lower
overhead is however the shorter length of HVFs in EPIC of
3 B, compared to the 16 B OPV ﬁelds in OPT. While a shorter
authenticator translates to easier brute-force attacks (and thus
seemingly weaker security), we have shown the practical use-
fulness of such attacks is severely limited by EPIC, as the
attacker can only send a single packet that traverses an unau-
thorized path, and in EPIC L2-3 that packet will be discarded
by the destination, see §5.2. EPIC is the ﬁrst data-plane
protocol designed to limit the consequences of a successful
brute-force attack to a single packet; previous protocols rely
on long authenticators to prevent harmful attacks.
Deployment on Path-Aware Architectures Our data-
plane protocols are generic and applicable to a wide range
of path-aware networking protocols. We now describe how
EPIC ﬁts into these architectures.
USENIX Association
29th USENIX Security Symposium    553
In SCION, the authenticators used in EPIC can be used
directly instead of the built-in MACs that protect hop ﬁelds.
However, a difference to EPIC is that in SCION only a subset
of ASes called cores (typically, Tier-1 providers) initiate bea-
cons. These beacons have limited reach and do not discover
the entire Internet topology for scalability reasons. Thus, end
hosts must combine paths from multiple beacons to obtain
global end-to-end paths. SCION deﬁnes rules for combining
multiple segments to rule out loops and uneconomical routes
(such as valley paths [21] [37, §8.2]) and allows paths to be
used in either direction. While our presentation of EPIC ab-
stracts from these aspects, we designed the protocols with
path combinations and bidirectionality in mind. For combined
paths, path authorization holds for each segment individually
while path validation applies to the complete path.
Besides SCION, multiple other path-aware Internet archi-
tectures cryptographically protect forwarding directives in
packet headers, including NEBULA [3, 36], PoMo [9], and
Platypus [38, 39]. PoMo introduces an abstract “motivation”
header that can be calculated in the same way as the HVFs of
EPIC. NEBULA uses “proofs of consent” for path authoriza-