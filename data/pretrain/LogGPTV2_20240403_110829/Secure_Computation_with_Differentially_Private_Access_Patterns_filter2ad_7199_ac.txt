Nevertheless, we take a diﬀerent approach. In all of our computations, the
output of each server is a secret share of the desired output, and thus it is un-
conditionally secure. The question of where to deliver these shares is left to the
user, though we can imagine several useful choices. Perhaps most obvious, the
shares might never be reconstructed, but rather used later inside another secure
computation that makes decisions driven by the output. Or, as Nikolaenko et
al. suggest [29], when computing gradient descent to provide users with recom-
mendations, the recommendation vectors can be sent to the user to store for
themselves. Regardless, since the aim of our work is to study the utility of our
relaxation, this concern is orthogonal, and we mainly leave it alone.
Privacy versus eﬃciency In the “standard” settings where diﬀerential privacy
is employed, additional noise aﬀects the accuracy of the result. Here, added
noise has no impact on the output, which is always correct, and is protected
by the secure computation. Instead, the tradeoﬀ is with eﬃciency: using more
noise helps to further hide the true memory accesses among the fake ones, but
requires additional, costly oblivious computation.
Malicious security and multi-party computation: Extending these deﬁni-
tions to model malicious adversaries and/or multi-party computation is straight-
forward, so we omit redundant detail. Similarly, we stress that by leveraging
the security relaxation deﬁned above, we gain improvement at the circuit level,
so we can easily extend our protocols to either (or both) of these two settings in
a generic way. To make our protocol from Section 4 secure against a malicious
adversary, the only subtlety to address is that our protocols make iterative use
of multiple secure computations (i.e. the functionality we realize is reactive),
so we would need to authenticate outputs and verify inputs in each of these
computations. While this can be done generically, such authentication comes
“for free” in many common protocols for secure computation (e.g. [37, 8]). To
extend our protocols to a multiparty setting, the only subtlety is in construct-
ing a multiparty oblivious shuﬄe. With a small number of parties, c, it is very
eﬃcient to implement c iterations of a permutation network, where in each iter-
ation, a diﬀerent party chooses the control bits that determine the permutation.
As c grows, it becomes less clear what the best method is for implementing an
oblivious shuﬄe. Interestingly, we note that there has been some recent work on
parallelizing multi-party oblivious shuﬄe [7]. We do not explore this direction in
our work; presenting our protocols in the two-party, semi-honest setting greatly
simpliﬁes the exposition, and suﬃces to demonstrate the advantages of our se-
curity relaxation. In our performance analysis, we primarily focus on counting
the number of AND gates in our construction, which makes the analysis more
11
general and allows for more accurate comparison with prior work (than, say,
comparing the timed performance of systems that use diﬀerent frameworks for
implementing secure computation).
3 A Diﬀerentially Private Protocol for Comput-
ing Histograms
To illustrate our main idea, we describe an algorithm that computes the data
histogram (i.e. counting, or data frequency) with diﬀerentially private access
patterns. Although this computation can be formalized in the context of our
general framework, it is instructive to demonstrate some of the main technical
ideas with this simple example before considering how they generalize (which
we do in Section 4). We defer a discussion about security until we present the
more general protocol.
In this computation, we assume that each user in the system contributes
a single input value, xi ∈ S, where we call the set S the set of types. The
computation servers (parties) each begin the computation with secret shares of
the input array, denoted by hreali. The output is a secret share of |S| counters,
where the counter for each type contains the exact number of inputs of that
type. The full protocol speciﬁcation appears in Figure 1.
The protocol is in a hybrid model, where the parties have access to three
ideal functionalities: DumGenp,α,FShuﬄe,Fadd. The two parties begin by calling
DumGenp,α, which generates some number of dummy inputs. The ideal func-
tionality for this is described in the left of Figure 2, and it is realized using a
generic secure two-party computation. As part of this computation, the parties
have to securely sample from the distribution Dp,α.
In the next section, we
deﬁne this distribution and describe our method for sampling it. We simply
remark now that it has integer support, and is negative with only negligible
probability (in δ). The output of DumGenp,α is a secret sharing of values in
S ∪ {⊥}: the size of the output is 2α|S|, where α is determined by the desired
privacy values  and δ (see Section 4). The number of dummy items of each
type is random, and neither party should learn this value; shares of ⊥ are used
to pad the number of dummy items of each type until they total 2α.
Each party locally concatenates their share of the real input array with their
share of the dummy values. They also initialize shares of an array of ﬂags,
denoted as isReal, which will be used to keep track of which item is real and
which is dummy. They then shuﬄe the real and dummy items together using
an oblivious shuﬄe. This is presented as an ideal functionality, but in practice
we implement this using two sequential, generic secure computations of the
Waksman permutation network [1], where each party randomly choose one of
the two permutations. The same permutations are used to shuﬄe the array
isReal ﬂags, ensuring that these ﬂags are “moved around with” the items. We
note that all secret shares are updated during the process of shuﬄing, so while
the parties knew which items and ﬂags were real and which were not before the
12
Diﬀerentially Private Histogram Protocol
Input: Each party, P1 and P2, receives a secret-share of real items de-
noted as hreali (r stands for number of real items, and d for number of
dummy ones)
Output: Secret share of counter values denoted as hCounteri, where the
counter for each type contains the exact number of inputs of that type
(S is the number of counter types)
Preprocessing:
hCounteri1:|S| ← 0
Computation:
hdummyi1:d ← DumGenp,α
hdatai1:(r+d) = hreali1:r||hdummyi1:d
hisReali1:r ← 1 , hisReali(r+1):(r+d) ← 0
h(cid:92)isReali ← FShuﬄe(hisReali,hρi)
hddatai ← FShuﬄe(hdatai,hρi)
ddata ← Open(hddatai)
Fadd(hisRealii,hCounterit) where t = ddatai
for i = 1 . . . (n + d)
Figure 1: A protocol for two parties to compute a histogram on secret-shared
data with an access pattern that preserves diﬀerential privacy.
shuﬄe, they have no way of knowing this after they receive fresh shares of the
shuﬄed items and isReal ﬂags.
The parties now open their shares of the data types, while leaving the ﬂag
values unknown. This is where our protocol leaks some information: revealing
the data types allows the parties to see a noisy sum of the number inputs of
each type. On the other hand, this is also where we gain in eﬃciency: the
remainder of the protocol requires only a linear scan over the data array, with
a small secure computation for each element in order to update the appropriate
counter value. More speciﬁcally, the parties iterate through the shuﬄed array,
opening each type. On data type i, they fetch their shares of the counter for
type i from memory, and call the Fadd functionality. This functionality adds
the (reconstructed) ﬂag value to the (reconstructed) counter; if the item was a
real item, the counter is incremented, while if it was a dummy item, the counter
remains the same. The functionality returns fresh shares of the counter value.
Neither party ever learns whether the counter was updated. In particular, they
cannot know whether they fetched that counter from memory because of a real
input value, or because of a dummy value. In our implementation, we instantiate
Fadd with a garbled circuit.
Simple extensions: In Section 4 we show how to generalize this protocol to the
wider function class. However, we note that in this speciﬁc case, if we did want
13
to add noise to the output, we could simply instruct the servers to count the
number of times each counter is accessed. They would no longer have to update
the counter values through a secure computation, so this would be a (slightly)
faster protocol. The output would contain the one-sided noise, but they could
simply subtract oﬀ α from each counter to get a more accurate estimate of the
counts. We stress that in this modiﬁed protocol, the dummy items are still
shuﬄed in with the real items, so the access pattern still preserves diﬀerential
privacy for each user. The modiﬁcation ensures that the (reconstructed) output
preserves diﬀerential privacy as well.
We also note that the protocol in Figure 1 can be applied to other simi-
lar computations, such as taking averages or sums over r values of |S| types
(though, now again without adding noise to the output). For example, if each
user contributed a salary value and a zip-code, we could use the above method
for computing the average salary in each zip-code, while ensuring that the access
patterns preserve user privacy. We simply need to modify the Fadd functional-
ity: instead of incrementing the secret-shared counter by 1 when the input is
a real item, the functionality would increment the counter by the value of the
secret-shared salary. In this case, though, the noisy access pattern alone does
not suﬃce for creating noisy output: the use of Fadd is essential. If we want
ensure that the reconstructed output preserves privacy, the noise would have
to be generated independently, through a secure computation, and then added
obliviously to the output.
4 The OblivGraph Protocol
When considering how the protocol from the previous section might be general-
ized, it is helpful to recognize the essential property of the computation’s access
pattern that we were leveraging. When computing a histogram, the access pat-
tern to memory exactly leaks a histogram of the input! This might sound like
a trivial observation, but it is in fact fairly important, as histograms are the
canonical example in the ﬁeld of diﬀerential privacy, and ﬁnding other compu-
tations where the access pattern reveals a histogram of the input will allow us
to broadly apply our techniques.
With that in mind, we extend our techniques to graph structured data,
and the graph-parallel frameworks that support highly parallelized computa-
tion. There are several frameworks of this type, including MapReduce, Pregel,
GraphLab and others [9, 25, 26]. We describe the framework by Gonzalez et
al. [15] called PowerGraph since it combines the best features from both Pregel
and GraphLab. PowerGraph is a graph-parallel abstraction, consisting of a
sparse graph that encodes computation as vertex-programs that run in parallel
and interact along edges in the graph. While the implementation of vertex-
programs in Pregel and GraphLab diﬀer in how they collect and disseminate
information, they share a common structure called the GAS model of graph
computation. The GAS model represents three conceptual phases of a vertex-
program: Gather, Apply, and Scatter. The computation proceeds in iterations,
14
DumGenp,α
Input: None.
Computation:
d = 2α|S|
dummy1:d ← ⊥
for i = 0 . . .|S| − 1
j = 2αi
γi ← Dp,α
k = γi + j
dummyj:k = i
Output: hdummyi
DumGenp,α
DumGenp,α
Input: None.
Input: None.
Computation:
d = 2α|V |
DummyEdges1:d ← ⊥
for i = 0 . . .|V | − 1
j = 2αi
γi ← Dp,α
k = γi + j
DummyEdgesj:k.v = i
Output: hDummyEdgesi
Computation:
d = 2α|V |
DummyEdges1:d ← ⊥
for i = 0 . . .|V | − 1
j = 2αi
γi ← Dp,α
δi ← Dp,α
k = γi + j
‘ = δi + j
DummyEdgesj:k.v = i
DummyEdgesj:‘.u = i
Output: hDummyEdgesi
Figure 2: Three variations on the Ideal functionality, DumGenp,α. Each is pa-
rameterized by α, p. The leftmost functionality is used in the histogram protocol
described in Section 3. The middle deﬁnition is the one used in our implementa-
tion, and suﬃces for satisfying security according to Deﬁnition 5. The right-most
adds diﬀerential privacy to out-degrees, which is needed in the disjoint collection
model (i.e. when hiding the input sizes for all users, in Deﬁnition 6).
and in each iteration, every node gathers (copy) data from their incoming edges,
applies some simple computation to the data, and then scatters (copy) the re-
sult to their outgoing edges. Viewing each node as a CPU (or by assigning
multiple nodes to each CPU), the apply step, which constitutes the bulk of the
computational work, is easily parallelized.
When performing such computations securely, the data is secret-shared be-
tween the computing servers as it moves from edge to node and back, as well
as during the Apply phase. The Apply phase is performed on these secret shares
using any protocol for secure computation as a black-box. The main challenge
is to hide the movement of the data during the Gather and Scatter phases, as
these memory accesses reveal substantial information about the user data.
Take matrix factorization as an example: an edge (u, v, Data) indicates that
user u reviewed item v, and the data stored on the edge indicates the value of the
user’s review. Because the data is secret shared, the value of the review is never
revealed. During the Gather phase, the right vertex of every edge is opened, and
the data is moved to the corresponding vertex. After the Apply phase, the left
15
Fgas
GAS Model Operations
Secret share of edges denoted as hEdgesi, each edge is
Secret share of vertices denoted as
Inputs:
edge : (u, v, uData, vData, isReal).