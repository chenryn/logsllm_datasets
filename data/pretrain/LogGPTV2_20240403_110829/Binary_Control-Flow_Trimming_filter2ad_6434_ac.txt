the oldest entry and make space for a new one (line 9), adding the
new entry (line 10), and leveraging involutivity of xor to remove the
evicted entry from the hash code (lines 11–12). Finally, lines 13–14
left-shift the context and hash code by one bit in preparation for
the next context and hash update.
Table 5: Guard checks implementation for trampolines referred as common-guard in Table 4
Guard Code
Guard Name
before-check
Legacy-mode
1:movd
2:psubd
r , %xmm12
%xmm13, %xmm12
check
after-check
3:pxor
4:movd
5:and
6:bt
7:jnb
8:pextrd
9:pslldq
10:pxor
11:movd
12:pxor
13:pslld
14:pslld
%xmm12, %xmm15
%xmm15, r
max_hash − 1, r
r , (HASH_TABLE)
TRAP
$3, %xmm14, r
$4, %xmm14
%xmm12, %xmm14
r , %xmm12
%xmm12, %xmm15
$1, %xmm15
$1, %xmm14
SHA-extension
1:movd
2:psubd
3:sha1msg1
4:sha1msg2
5:pslrdq
6:pxor
7:movd
8:and
9:bt
10:jnb
11:pslldq
12:psllw
13:pxor
r , %xmm12
%xmm13, %xmm12
%xmm14, %xmm15
%xmm15, %xmm15
$4, %xmm15
%xmm12, %xmm15
%xmm15, r
max_hash − 1, r
r , (HASH_TABLE)
TRAP
$4, %xmm14
$1, %xmm14
%xmm12, %xmm14
One important deployment consideration is whether to exclude
library control-flows from the program flow, since they are shared,
and it may therefore be infeasible to learn appropriate policies for
them based on profiling only some applications that load them. On
the other hand, if security is a priority, the user may be interested in
generating a specialized, non-shared version of the shared library
specifically for use by each security-sensitive application. For this
work, we enforce the policy on all branches from any portion of
the program code section and all the shared libraries shipped with
it, but we leave system shared libraries unaltered. The latter can
optionally be trimmed by making a local copy to which the policy
is applied, though the result is obviously no longer a library that
can be shared across multiple applications.
When rewriting app-included shared libraries, we add trampo-
lines to each image, and declare them with .hidden visibility to
avoid symbol name-clashes between the images. The hash table
can be specialized to each image or centralized for all. For this work
we use one centralized table for all interoperating images, accessed
via the .got table for concurrent, shared access between modules.
5 EVALUATION
We experimentally evaluated our control-flow trimming system in
terms of performance, security, and accuracy. Performance evalua-
tion measures the overhead that our system imposes in terms of
space and runtime. Our security analysis examines the system’s
ability to withstand the threats modeled in §2.3. Security failures
therefore correspond to false negatives in the classification. Finally,
accuracy is measured in terms of false positives—premature aborts
of the trimmed program when no policy violation occurred.
Test programs consist of the real-world software products in
Table 6, plus bash, gcc, ImageMagic, the epiphany and uzbl browsers,
and the SPEC2017 benchmarks. We also successfully applied our
prototype to rewrite the full GNU Coreutils 8.30 collection. The
browsers were chosen primarily for their compatibility with Pin
and DynamoRIO, which we use for trace collection and replay.
4.78
3.94
2.73
1.93
1.87
1.44
1.20
1.91
1.36
0.40
0.37
1.11
0.50
0.60
2.00
1.87
)
%
(
d
a
e
h
r
e
v
O
e
m
i
t
n
u
R
cpuxalan_s
exchange2_s
deepsjeng_s
mcf_s
perlbench_s
omnetpp_s
leela_s
sgcc
x264_s
lighttpd
vsftpd
proftpd
pure-ftpd
xz_s
median
nginx
Figure 3: Runtime overhead for SPEC2017 intspeed suite and
some ftp- and web-servers
To evaluate accuracy, we created or obtained test suites for each
program. For example, in the gcc evaluations, we used the gcc
source code as its own input for unit testing. That test suite there-
fore consists of all C source files needed to compile gcc on the
the experiment machine. For ImageMagic, we randomly gathered
hundreds of JPEG and PNG images. We unit-tested ftp servers by
downloading and uploading randomly selected files interspersed
with random ftp commands (e.g., cd, mkdir, ls, append, and rename).
For exim we used a script to launch sendmail and randomly send
an email to a specific address. Browser experiments entail loading
pages randomly drawn from the Quantcast top 475K urls, and uzbl
experiments additionally include random user interactions (e.g.,
back/forward navigation, scrolling in all directions, zoom in/out,
search, etc.). All results were obtained using a DELL T7500 machine
with 24G of RAM and Intel Xeon E5645 processor.
5.1 Performance Overhead
Figure 3 graphs the runtime overhead for SPEC2017 benchmarks
and several ftp- and web-servers. We used Apache benchmark [8] to
Table 6: Space overhead for SPEC2017 intspeed suite bench-
marks and some real-world applications
Binary
perlbench_s
sgcc
mcf_s
omnetpp_s
cpuxalan_s
x264_s
deepsjeng_s
leela_s
exchange2_s
xz_s
exim
lighttpd
memcached
nginx
openssh
proftpd
pureftpd
vsftpd
postgresrl
node.js
median
Original Size (KB)
Code
File
1992
10686
8499
63243
19
131
1567
28159
80762
4701
567
3320
85
508
191
3819
111
182
1082
146
1187
1407
294
1304
156
746
1444
1674
2467
638
803
3310
118
470
133
143
544
757
36758
30059
556
1541
Size Increase (%)
Code
File
35.14
10.17
59.15
12.76
35.20
8.80
55.37
5.15
4.19
48.25
23.40
6.41
42.17
10.23
45.14
2.15
18.61
16.01
0.69
2.12
14.70
32.14
27.12
13.12
23.89
13.50
19.07
29.76
15.12
21.40
29.12
16.34
27.04
17.12
28.99
25.78
33.53
41.35
28.63
17.84
28.06
16.42
issue 25,000 requests with concurrency level of 10 for benchmarking
lighttpd and nginx. To benchmark the FTP servers, we wrote a
Python script based on the pyftpdlib benchmark [66] to make 100
concurrent clients, each of which request 100 1KB-sized files.
The median runtime overhead is 1.87%, and all benchmarks ex-
hibit an overhead of 0.37–4.78%. The good performance is partially
attributable to Table 5’s reliance on SIMD instructions, which tend
to exercise CPU execution units independent of those constrained
by the mostly general-purpose instructions in the surrounding code.
This allows out-of-order execution (OoOE) hardware optimizations
in modern processors [39] to parallelize many guard code µops
with those of prior and subsequent instructions in the stream.
Table 6 shows the space overhead for the SPEC2017 benchmarks
and a sampling of the other tested binaries. On average, the test
binaries increase in size by 16.42% and their code sizes increase
by 28.06%. The main size contributions are the extra control-flow
security guard code in-lined into code sections, and the addition of
the hash table that encodes the CCFG policy.
Although these size increases are an important consideration
for memory and disk resources needed to support our approach,
we emphasize that they are not an accurate measure of the result-
ing software attack surface, since many of the added bytes are
non-executable or erased (exception-throwing) opcodes (e.g., int3).
Attack surface must therefore be measured in terms of reachable
code bytes, not raw file or code section size.
To evaluate this, Table 7 measures the reachable, executable code
from the decision trees for binaries with a test suite. Despite the
increase in total file and code sizes, the amount of reachable code
is reduced by an average of 36%. For example, the attack surface of
ImageMagic convert is reduced by 94.5%. (The method of computing
Table 7 is detailed in §5.3.)
5.2 Security
5.2.1 Vulnerability Removal. A primary motivation for control-
flow trimming is the possible removal of defender-unknown vul-
nerabilities within code features of no interest to code consumers.
To test the efficacy of our approach for removing such zero-days,
we tested the effects of control-flow trimming on unpatched ver-
sions of Bash 4.2, ImageMagic 6.8.6–10, Proftpd 1.3.5, Node.js 8.12,
and Exim 4.86 that are vulnerable to the CVEs shown in Table 2,
including Shellshock and ImageTragick.
Shellshock attacks exploit a bug in the bash command-line parser
to execute arbitrary shellcode. The bug erroneously executes text
following function definitions in environment variables as code.
This affords adversaries who control inputs to environment vari-
ables remote code execution capabilities. Because of its severity,
prevalence, and the fact that it remained exploitable for over 20
years before it was discovered, Shellshock has been identified as
one of the highest impact vulnerabilities in history [25].
ImageMagick is used by web services to process images and
is also pre-installed in many commonly used Linux distributions
such as Ubuntu 18.04. ImageTragick vulnerabilities afford attackers
remote code execution; delete, move, and read access to arbitrary
files; and server-side request forgery (SSRF) attack capabilities in
ImageMagic versions before 6.9.3–10, and in 7.x before 7.0.1-1.
ProFTPD 1.3.5 allows remote attackers to read and write from/to
arbitrary files via SITE CPFR and SITE CPTO commands. In node
serialize package 0.0.4, the unserialize function can be exploited
by being passed a maliciously crafted JS object to achieve arbi-
trary code execution. Exim before 4.86.2 allows a local attacker to
gain root privilege when Exim is compiled with Perl support and
contains a perl_startup configuration variable.
Unit tests for the bash experiment consist of the test scripts
in the bash source package, which were created and distributed
with bash before Shellshock became known. The tests therefore
reflect the quality assurance process of users for whom Shellshock
is a zero-day. For the remaining programs, we manually exposed
each to a variety of inputs representative of common usages. For
ImageMagic, our unit tests execute the application’s convert util-
ity to convert images to other formats. We unit-tested ProFTPD
by exposing it to a variety of commands (e.g. FEAT, HASH), exclud-
ing the SITE command. For Node.js we wrote some JS code that
does not leverage node-serialize package. We ran Exim without a
perl_startup configuration variable.