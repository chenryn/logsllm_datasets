o
l
f
(
d
a
o
L
 0
 0
 2
 4
 6
Time (hours)
 8
 10
 800
 600
 400
 200
)
s
/
s
w
o
l
f
(
d
a
o
L
 0
 0
 24
 48
 72
 96
Time (hours)
Figure 5: Frequency of ﬂow-setup requests per second to Con-
troller over a 10-hour period (top) and 4-day period (bottom).
)
s
m
(
e
m
i
t
e
s
n
o
p
s
e
R
 1.6
 1.4
 1.2
 1
 0.8
 0.6
 0.4
 0.2
 0
 2000
 4000
 6000
 8000
 10000
Load (flows / s)
Figure 6: Flow-setup times as a function of Controller load.
Packet sizes were 64B, 128B and 256B, evenly distributed.
age cost of an update on a 3,000 node topology is 10ms. In the
following section we present an analysis of ﬂow-setup times under
normal operation and during link failure.
5.3 Deployment
Our Ethane prototype is deployed in our department’s 100Mb/s
Ethernet network. We installed eleven wired and eight wireless
Ethane Switches. There are currently approximately 300 hosts on
this Ethane network, with an average of 120 hosts active in a 5-
minute window. We created a network policy to closely match—
and in most cases exceed—the connectivity control already in place.
We pieced together the existing policy by looking at the use of
VLANs, end-host ﬁrewall conﬁgurations, NATs and router ACLs.
We found that often the existing conﬁguration ﬁles contained rules
no longer relevant to the current state of the network, in which case
they were not included in the Ethane policy.
Brieﬂy, within our policy, non-servers (workstations, laptops,
and phones) are protected from outbound connections from servers,
while workstations can communicate uninhibited. Hosts that con-
nect to an Ethane Switch port must register a MAC address, but
require no user authentication. Wireless nodes protected by WPA
and a password do not require user authentication, but if the host
MAC address is not registered (in our network this means they are
a guest), they can only access a small number of services (HTTP,
HTTPS, DNS, SMTP, IMAP, POP, and SSH). Our open wireless
access points require users to authenticate through the university-
wide system. The VoIP phones are restricted from communicating
with non-phones and are statically bound to a single access point to
s
w
o
l
f
e
v
i
t
c
A
 1200
 1000
 800
 600
 400
 200
 0
 0
 5
 10
 20
 15
Time (hours)
 25
 30
 35
Figure 7: Active ﬂows for LBL network [19].
)
s
/
s
w
o
l
f
(
d
a
o
L
 10000
 8000
 6000
 4000
 2000
 0
 0
 5
 10
 15
 20
 25
 30
Figure 8: Flow-request rate for Stanford network.
Time (days)
prevent mobility (for E911 location compliance). Our policy ﬁle is
132 lines long.
6. PERFORMANCE AND SCALABILITY
Deploying Ethane has taught us a lot about the operation of a
centrally-managed network, and it enabled us to evaluate many as-
pects of its performance and scalability, especially with respect to
the numbers of users, end-hosts, and Switches. We start by look-
ing at how Ethane performs in our network, and then, using our
measurements and data from others, we try to extrapolate the per-
formance for larger networks.
In this section, we ﬁrst measure the Controller’s performance
as a function of the ﬂow-request rate, and we then try to estimate
how many ﬂow-requests we can expect in a network of a given
size. This allows us to answer our primary question: How many
Controllers are needed for a network of a given size? We then
examine the behavior of an Ethane network under Controller and
link failures. Finally, to help decide the practicality and cost of
Switches for larger networks, we consider the question: How big
does the ﬂow table need to be in the Switch?
6.1 Controller Scalability
Recall that our Ethane prototype is currently used by approx-
imately 300 hosts, with an average of 120 hosts active in a 5-
minute window. From these hosts, we see 30-40 new ﬂow requests
per second (Figure 5) with a peak of 750 ﬂow requests per sec-
ond.9 Figure 6 shows how our Controller performs under load:
for up to 11,000 ﬂows per second—greater than the peak laod we
observed—ﬂows were set up in less than 1.5 milliseconds in the
worst case, and the CPU showed negligible load.
Our results suggest that a single Controller could comfortably
handle 10,000 new ﬂow requests per second. We fully expect this
number to increase if we concentrated on optimizing the design.
With this in mind, it is worth asking to how many end-hosts this
load corresponds.
We considered two recent datasets: One from an 8,000-host net-
work at LBL [19] and one from a 22,000-host network at Stanford.
As is described in [12], the number of maximum outstanding ﬂows
9Samples were taken every 30 seconds.
s
w
o
l
f
e
v
i
t
c
A
 480
 400
 320
 240
 160
 80
 0
s
w
o
l
f
e
v
i
t
c
A
 480
 400
 320
 240
 160
 80
 0
 0
 5
 10
 15
 20
Time (hours)
 0
 5
 10
 15
 20
Figure 9: Active ﬂows through two of our deployed switches
Time (hours)
Failures
0
1
2
3
4
Completion time
26.17s
27.44s
30.45s
36.00s
43.09s
Table 1: Completion time for HTTP GETs of 275 ﬁles during
which the primary Controller fails zero or more times. Results
are averaged over 5 runs.
in the traces from LBL never exceeded 1,200 per second across all
nodes (Figure 7). The Stanford dataset has a maximum of under
9,000 new ﬂow-requests per second (Figure 8).
Perhaps surprisingly, our results suggest that a single Controller
could comfortably manage a network with over 20,000 hosts. In-
deed ﬂow setup latencies for continued load of up to 6,000/s are
less than .6ms, equivalent to the average latency of a DNS request
within the Stanford network. Flow setup latencies for load under
2,000 requests per second are .4ms, this is roughly equivalent to
the average RTT between hosts in different subnets on our campus
network.
Of course, in practice, the rule set would be larger and the num-
ber of physical entities greater. On the other hand, the ease with
which the Controller handles this number of ﬂows suggests there
is room for improvement. This is not to suggest that a network
should rely on a single Controller; we expect a large network to
deploy several Controllers for fault-tolerance, using the schemes
outlined in §3.5, one of which we examine next.
6.2 Performance During Failures
Because our Controller implements cold-standby failure recov-
ery (see §3.5), a Controller failure will lead to interruption of ser-
vice for active ﬂows and a delay while they are re-established. To
understand how long it takes to reinstall the ﬂows, we measured
the completion time of 275 consecutive HTTP requests, retrieving
63MB in total. While the requests were ongoing, we crashed the
Controller and restarted it multiple times. Table 1 shows that there
is clearly a penalty for each failure, corresponding to a roughly 10%
increase in overall completion time. This can be largely eliminated,
of course, in a network that uses warm-standby or fully-replicated
Controllers to more quickly recover from failure (see §3.5).
Link failures in Ethane require that all outstanding ﬂows re-contact
the Controller in order to re-establish the path. If the link is heav-
ily used, the Controller will receive a storm of requests, and its
performance will degrade. We created a topology with redundant
 1400
 1200
 1000
 800
 600
 400
 200
)
s
m
(
T
T
R