Fig. 9: TCP throughput from the application to the peer.
Mbps
851.206 ± 4.652
Unprotected
Unsafe TCPR 837.275 ± 5.355
TCPR 838.699 ± 1.934
% Raw
100
98
98
Fig. 10: TCP throughput from the peer to the application.
A. Overhead
We evaluated throughput overhead using a version of ttcp
modiﬁed to support TCPR. First, we measured send goodput
from the application to the peer both with unprotected TCP
and with TCPR, reporting the average and standard deviation
of 10 runs in Figure 9. TCPR does not cause any measurable
overhead.
Next, we measured in the opposite direction: receive good-
put from the peer to the application. This is the ﬂow that
is subject to delayed acknowledgments, so in addition to the
previous two cases, we measured “Unsafe TCPR”, in which
delayed acknowledgments were disabled.
As shown in Figure 10, while there is slight overhead on the
incoming packets, there is no measurable impact on throughput
from delayed acknowledgments. That is reasonable, because
TCP keeps a window of packets in ﬂight in order to mask
latency.
The latency impact of delayed acknowledgments can be
seen in TCP round-trip times from packet traces. We used
traces of throughput experiments like those described above.
For each acknowledgment visible to the sender that covered
new data, we computed the elapsed time since it had sent that
data, reporting the average and standard deviation over all such
packets (about 50 in each trace) in Figure 11 and Figure 12.
There is no signiﬁcant difference in latency from the
application to the peer. However, for input from the peer to the
application, both setups of TCPR exhibit much higher variance
than unprotected TCPR; delayed acknowledgments seem to
add nearly 50 microseconds of latency, but the overhead is
small and within the standard deviation.
Notice that since the recoverable application is responsible
for the delays to its own acknowledgments, these numbers
could be arbitrarily large. Our experiments are thus something
of a best-case scenario, because we measured an application
that always acknowledges its input as soon as possible after
the recv operation.
B. Recovery
The delay associated with application recovery obviously
depends strongly on the nature of the application, and would
often include the latency to detect failure and that associated
with launching a replacement. To isolate the costs speciﬁcally
associated with TCPR, we microbenchmarked its ability to
Unprotected
Unsafe TCPR
TCPR
Microseconds
318 ± 27
326 ± 16
334 ± 24
Fig. 11: Latency from the application to the peer.
Unprotected
Unsafe TCPR
TCPR
Microseconds
550 ± 23
547 ± 94
594 ± 85
Fig. 12: Latency from the peer to the application.
recover a connection, measuring the time from when a fresh
TCP stack was created by the recovering application until the
application is able to send and recv again.
We measured two cases, shown in Figure 13. In the “Appli-
cation” case, TCPR retained its state and only the application
failed and recovered, so that TCPR could establish a new
connection immediately. In the “TCPR + Application” case,
they failed and recovered together, so that soft state had to
be restored from the peer’s packets during recovery. If the
application had saved the soft state as well, recovery would
proceed exactly as in the “Application” case.
To set these numbers into context, consider our recoverable
BGP scenario: in both cases, TCPR-mediated connection re-
covery is faster than usual inter-arrival times of BGP updates
even on a heavily-loaded core Internet router. Thus, if the BGP
failure itself is handled quickly enough by the router, one can
completely mask the event from remote BGP peers.
C. BGP Failover
Our experimental evaluation of BGP failover was heavily
inﬂuenced by Pei et al. [13], who simulated networks of BGP
routers to measure the effectiveness of various proposals at
limiting the disruption of link failover. As in this prior work,
we evaluate CLIQUE and B-CLIQUE topologies, but rather
than examining link failure, we inject router failure followed
by immediate recovery. We simulate the network, but run a
full BGP implementation on our software routers.
In our experiments, most of the routers run the Quagga
routing software and the Quagga implementation of BGP.
However, to the best of our knowledge no open source BGP
router supports recovering with GR. Accordingly, we added
one node running exabgp, an easily-conﬁgured route injector
that supports GR. This node also has a packet ﬁltering ability
that we exploit to artiﬁcially inﬂate the path length of some
routes, allowing us to explore scenarios with connected-but-
undesirable backup paths.
We didn’t modify the routers in any way, except for
enabling network namespace virtualization. Each router thus
had total control over its own routing table. We recorded
the experiments using rtmon, a utility that is included with
the standard Linux tool iproute2. The result is a log of
timestamped updates to each routing table. We also used
tcpdump on the host bridge to record all of the BGP messages
sent over the virtual network.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:58:18 UTC from IEEE Xplore.  Restrictions apply. 
Application
TCPR + Application
Microseconds
39 ± 8
167 ± 25
Fig. 13: Recovery time.
BGP
GR
0
5
10
seconds
15
1
0.8
0.6
0.4
0.2
0
F
D
C
BGP
GR
0
5
10
seconds
15
Fig. 15: Average duration a router is disconnected from the
destination due to control failure.
1
0.8
0.6
0.4
0.2
0
F
D
C
Fig. 14: Duration of convergence due to control failure.
1) Control Element Failover: Modern routers are often
constructed from a collection of computing nodes. An internal
node in a router that runs protocols such as BGP is called a
control element, whereas a forwarding element runs the hard-
ware engine and terminates physical links to peers. A common
conﬁguration for large routers is to have two redundant control
elements, so that when one fails, the other can replace it. As
discussed in the introduction, even if BGP failover is rapid,
an extended period of disruption to network routing can still
ensue. We set out to measure these effects and to see how
TCPR can help.
Our experiments start by demonstrating that unmasked
failover can cause severe routing disruptions. We experimented
with a CLIQUE topology of 16 BGP routers. They were all
identical and each peered with all others. One router had an
additional peering relation with the route injector, and was
used to model a router that must fail-over from a primary to
a backup control element. Once the initial convergence com-
pleted, we killed the route injector and immediately restarted
it, then observed the BGP network until it converged again.
We collected data for more than 100 runs.
An important parameter in BGP convergence experiments
is the Minimum Route Advertisement Interval (MRAI), which
is a rate-limiting knob in BGP. It has been shown that up to
a certain value, the MRAI improves convergence times, but
past that, convergence times degrade. It is hard to determine
the optimal MRAI for a particular topology, and unfeasible
for the entire Internet; the original recommendation for MRAI
in peerings between providers was 30 seconds, but newer
experiments have indicated 5 seconds is better [14]. Our
data was collected with an MRAI of 5 seconds. We also
BGP
GR
1
0.8
0.6
0.4
0.2
F
D
C
1
20
40
60
updates per destination
0
80
Fig. 16: Update load due to control failure.
experimented with an MRAI of 30 seconds, and observed even
longer convergence times and greater disruptions to network
connectivity.
Figure 14 shows a CDF of convergence times in each
experiment. On average, it took more than 15 seconds to
reconverge to the original route, even though the underlying
network topology was unchanged. During this period, we
observed many events in which some of the routers believed
some destination to be unreachable, or reachable through a
path that is actually a routing loop. We report these periods
in Figure 15, showing the time interval during which each
individual router was unable to reach one or more destinations,
as determined by analysis of the global collection of the routing
tables at each instant. On average, every router in the system
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:58:18 UTC from IEEE Xplore.  Restrictions apply. 
BGP
GR
0
100
200
seconds
300
1
0.8
0.6
0.4
0.2
0
BGP
F
D
C
GR
0
100
200
seconds
300
1
0.8
0.6
0.4
0.2
0
F
D
C
Fig. 17: Duration of convergence due to correlated control and
forwarding failure.
Fig. 18: Average duration a router is disconnected from the
destination due to correlated control and forwarding failure.
experienced more than 11 seconds of connectivity loss for each
fail-over event, even though BGP itself recovered immediately.
In addition to disrupting the forwarding plane, reconver-
gence taxes the control plane by consuming bandwidth and
CPU to process updates. Figure 16 shows the number of
updates sent for each router for the single destination being
advertised. In a core Internet deployment, where a routing table
might include tens of thousands of destinations, the more than
45 updates per router we see here would be multiplied by the
number of destinations.
The vertical CDFs demonstrate the impact of using GR.
With this feature enabled, the router announces that it will
preserve its routing tables across restarts, and its peers continue
to route trafﬁc through it during restart. The routing ﬂap seen
with the basic BGP recovery is thus avoided and reconvergence
is immediate. No network disconnections occur, and there is
just a single redundant advertisement of each route. Thus,
in these experiments, GR does nearly as well as the non-
stop routing achievable with transparent connection recovery,
except for sending a small number of unnecessary route
advertisements.
2) Forwarding Element Failover: There are cases when
GR can perform worse than totally unmasked restarts. GR
is effective because it changes the way that peers interpret
connectivity loss. In the default BGP behavior, connection loss
is interpreted to indicate forwarding plane failure; with GR,
the forwarding plane is assumed to continue functioning on
“autopilot”, so that only the control plane requires resynchro-
nization. When this assumption is valid, GR almost solves the
failover problem. However, when the network topology does
change while the control plane is still recovering, GR can
instead delay the needed routing adaptation. A consequence
is that routers may use bad paths based on the assumption that
the recovering router has applied a routing update that it has
not had time to receive, process, and install in its hardware-
mediated forwarding plane.
1
0.8
0.6
0.4
0.2
0
F
D
C
250
300
BGP
GR
0
50
150
100
updates per destination
200
Fig. 19: Update load due to correlated control and forwarding
failure.
To experiment with forwarding failover, we modiﬁed the
control experiment
to use a B-CLIQUE topology. A B-
CLIQUE is the same as a CLIQUE, but with an additional
backup path. A second router in the clique peers with the
route injector, and receives a route to the destination that has
an inﬂated path length, making it less desirable than any path
through the primary link.
As shown in Figure 17, the convergence times for actual
link loss are worse than for a pure control failover; on average,
it takes more than 111 seconds for the network to completely
switch over to the backup. The disconnectivity, shown in
Figure 18, is similarly inﬂated, with an average of 86 seconds