f
o
.
m
u
N
x
a
M
 30
 20
 10
 0
 0
Lv3 link
Lv2 link
Lv1 link
 500
 1000  1500  2000
Time (sec)
(b) With 100% load.
Figure 17: Max number of jobs sharing a link at diff. tree levels
for Mixed.
i.e., conﬁned to ToR switches, and on average fewer than 99 and
26 jobs are within level-2 and level-3 subtrees, respectively. Under
100% load, there are fewer than 102 and 37 jobs within level-2 and
level-3 trees. These numbers conﬁrm that the vast majority of the
jobs are localized to small subtrees. The immediate consequence
of such locality is that there are few jobs sharing any link at the
same time in the datacenter network. Figure 17 shows the maxi-
mum numbers of jobs sharing a link are less than 13 and 26 at level
2 and 3, and stay at 4 at level 1 (i.e., the 4 VMs of a machine are
allocated to 4 different jobs), under 80% and 100% load. We also
measured the locality in separate experiments when the workload
has only large jobs, i.e., requiring thousands of VMs, and when the
workload has mixed small and large jobs, and again found there are
few jobs sharing any given link in the whole network. The reason
is when jobs are large, few can be scheduled to run concurrently. In
summary, the low number of jobs sharing any link in the network
despite job size mixes suggests per-job bandwidth reservation in
the internal links can be easily accomplished using policers in off-
the-shelf switches (§5.3).
Tenant cost and provider revenue. Today’s cloud providers such
as Amazon EC2 charge tenants solely based on the consumed VM-
208 120
 120
 100
)
%
 100
)
%
(
C
V
/
C
V
T
I
 80
 60
 40
 20
 0
Provider Revenue
Customer Cost
(
C
V
/
C
V
T
I
 80
 60
 40
 20
 0
Provider Revenue
Customer Cost
 20
 40
 60
 80
 100
 20
 40
 60
 80
 100
Load (%)
Load (%)
(a) Under the ﬁrst price.
(b) Under the second price.
Figure 18: Relative provider revenues and tenant costs for
mixed workload.
time. In this case, the fraction of job requests that are accepted and
the costs for them determine the cloud provider’s revenue. Since
the number of VMs allocated to each job and its execution time
stay the same under VC and TIVC, the lower rejection rate under
TIVC compared to VC directly translates into increased revenue
for today’s cloud provider, while the individual tenant’s cost stays
the same. For example, for the mixed workload in Figure 14(d),
VC rejects 4.6%, 6.1%, and 7.4% more jobs under 60%, 80%, and
100% load, respectively. Since the rejected jobs tend to be larger
than average, the extra rejected jobs translate into 16%, 22%, and
27% lower provider revenue under VC.
We envision that tomorrow’s cloud providers will and should ex-
plicitly charge for networking bandwidth in providing tenants with
explicit bandwidth reservations such as VC and TIVC. Since devel-
oping fair yet efﬁcient charging model is still ongoing research [30,
10], we adopt the simple charging model in [11] which effectively
charges networking based on the total reserved bandwidth volume
over time in such a way that the cloud provider (e.g., Amazon EC2)
remains revenue neutral in transitioning from the VM-only charg-
ing model to the new model. Speciﬁcally, a tenant using N VMs
for time T will be charged N (T · kv + kb · V ), where kv is the
unit-time VM cost, kb is the unit-volume bandwidth cost, and V is
the total bandwidth volume reserved over the time period T .
Under the above charging model, we compare the cloud provider
revenue and the tenant cost under VC and TIVC for two sample
estimated kv and kb prices: (0.04$/hr, 0.00016$/GB) and (0.04$/hr,
0.00008$/GB), for the mixed workload run in Figure 14(d). We
calculate the ratio of the total cloud provider revenue and the ratio
of the tenant job cost, under TIVC versus under VC. Figure 18(a)
shows the two ratios under the ﬁrst price. We see that TIVC allows
tenants to pay on average about 20% less than VC, for accepted
jobs, independent of the load, from reduced network usage. At
low load, e.g., 20–40%, the cloud provider revenue under TIVC is
about 20% lower than under VC because the cloud provider accepts
almost all jobs under both schemes while the tenants under TIVC
on average pay 20% less than under VC. At close to 100% load,
however, not only do tenants under TIVC pay 20% less than under
VC, the cloud provider stays revenue neutral under TIVC compared
to under VC. This is because the provider is able to accept about 7%
more jobs under TIVC than under VC, which corresponds to about
13% higher VM utilization (again since these extra jobs tend to be
larger than average). Finally, Figure 18(b) shows under the second
price, TIVC not only allows tenants to pay on average about 12%
less than under VC at all loads, but also allows the cloud provider
to make more revenue when the load crosses 60%.
6.3 Testbed Experiment
We implemented PROTEUS following the description in §5 on a
datacenter testbed consisting of 18 machines (speciﬁcation in §2.2)
forming a 3-tier tree topology as shown in Figure 19. Each ma-
chine runs 2 VMs, and the testbed switches are implemented using
servers with NetFPGA cards with 4 1Gbps ports. We use the rate
limiter module provided in the base package of NetFPGA reference
router to limit the capacity of each internal tree link to emulate an
oversubscribed datacenter. The level-1 (i.e., between the machines
and ToR switches), level-2, and level-3 link capacities are 230, 700,
and 1000 Mbps, respectively.
The bandwidth provisioning for access links is implemented via
the Linux trafﬁc control API tc. For internal links, since it is not
straight-forward to implement per-job rate limiters, we conﬁgured
the combined rate of jobs allocated to the sub-tree. For example,
assuming nodes 1 −6 are allocated to jobs 1 and 2, then the link be-
tween A and G, shared between the two jobs, would be conﬁgured
with the sum of bandwidth requirements of the two jobs.
We use a mixed workload of 30 jobs, 10 each of the three ap-
plications, Sort, Hive Join, and Hive Aggre. PROTEUS proﬁles the
3 applications on the testbed, generates the TIVC and VC mod-
els using no-elongation threshold bandwidth cap, and allocates the
jobs accordingly. We also run the same 30 jobs under a baseline
model, which schedules the jobs solely based on the number of
available VMs. Figure 20 shows the completion time for the work-
load are 2405, 3770, 5140 seconds, for Baseline, TIVC, and VC,
respectively. TIVC reduces VC’s completion time by 27% from
more efﬁcient bandwidth reservation and hence scheduling. How-
ever, Baseline has the shortest completion time since it aggressively
schedules jobs to compete freely for the network which results in
higher overall networking utilization, but however can lead to un-
predictable application performance. Figure 21 shows the CDF of
the per-job execution time under Baseline and under VC relative to
that under TIVC. We see that in the median case, per-job execution
time under Baseline is 10% longer than that under TIVC. Except
for a few variations, TIVC results in similar per-job execution times
as VC, because both models reserve the threshold bandwidth and
avoid unpredictable competition for the network.
To evaluate the scalability of the TIVC allocation algorithm, we
measure the time to allocate each of the 5,000 job requests in the
large datacenter with 64,000 VMs used in §6.1. Our allocation al-
gorithm is highly scalable; the single-threaded code running on an
8-core Intel Xeon E5410 2.33 Ghz processor and 16 GB RAM has
a median time of 18.0ms and the 99th percentile time of 28.0ms.
7. RELATED WORK
Our work is closely related to the recently proposed virtual net-
work abstractions [20, 11, 33]. We discussed [20, 11] in detail in
§2. Like Oktopus, Gatekeeper [33] also proposes a per-VM hose
model but for full bisection networks and focuses on managing
servers’ access bandwidth. The hose model was originally intro-
duced in [18] for wide-area VPNs and did not consider allocating
physical or virtual machines. Compared to these work, our work
proposes TIVC, which extends the per-VM hose model to model
the time-varying nature of networking requirement of cloud appli-
cations. More importantly, our work takes the ﬁrst step towards
automatically deriving the model parameters for a representative
class of cloud applications.
Our work is also related to previous work on mechanisms for
sharing datacenter networks. As discussed in §5.3, most of previ-
ous work (e.g., [11, 33] use a hypervisor-based framework for en-
forcing bandwidth reservation in the network which can suffer poor
scalability. Seawall [34] and Netshare [26] propose bandwidth slic-
ing mechanisms that aim to provide fair sharing of networks with
minimum bandwidth guarantee and statistical multiplexing, but do
not provide deterministic bandwidth guarantees.
PROTEUS shares the same proﬁling methodology with Elasti-
sizer [22], StarFish [23] and CBO [21] which focus on choos-
ing the type and number of VMs for MapReduce jobs to balance
cost/performance objectives. These work ignore networking re-
quirement and hence complement our network proﬁling technique.
209Root Switch 
I I 
Aggregate Switch 
ToR Switch 
A A 
G G 
B B 
C C 
D D 
1Gbps 
700 Mbps 
230 Mbps 
H H 
E E 
F F 
Host 
1 1 
2 2 
3 3 
4 4 
5 5 
6 6 
7 7 
8 8 
9 9 
10 10 
11 11 
12 12 
13 13 
14 14 
15 15 
16 16 
17 17 
18 18 
Baseline
VC
TIVC
)
%
(
n
o
i
t
a
z
i
l
i
t
U
 60
 50
 40
 30
 20
 10
 0
 0
 1000  2000  3000  4000  5000
Time (sec)
 1
 0.8
 0.6
 0.4
 0.2
F
D
C
 0
 0.9
VC / TIVC
Baseline / TIVC
 1
 1.1
 1.2
 1.3
 1.4
Job Time Ratio
Figure 19: Testbed topology.
Figure 20: Network utilization.
Figure 21: CDF of relative job time.
Our TIVC allocation algorithm is related to previous work
on virtual network embedding (e.g., [15, 35]) and testbed map-
ping [32]. These work resort to heavy-weight optimization solvers
such as linear programming and can not scale to the larger number
of VMs in modern datacenters.
Finally, several very recent work [30, 10] start to study fair and
efﬁcient charging model in sharing datacenter networks. These
studies are complementary to PROTEUS and can be incorporated
into PROTEUS in choosing cost-effective TIVC models.
8. CONCLUSIONS
In summary, the primary contributions of this paper are the de-
sign of the ﬁrst network abstraction (to our best knowledge), TIVC,
that captures the time-varying nature of cloud applications, and a
systematic proﬁling-based methodology for making the abstraction
practical and readily usable in today’s datacenter networks. Our
experimental evaluation using real MapReduce applications shows
that TIVC signiﬁcantly outperforms previous ﬁxed-bandwidth net-
work abstractions in improving job throughput and hence cloud
provider revenue and reducing tenant cost. Our work takes a sig-
niﬁcant step forward towards efﬁcient and cost-effective sharing of
datacenter networks in providing cloud customers with predictable
performance and cost.
The PROTEUS system which implements the TIVC abstraction
can be readily used to extend today’s dominant utility computing
model offered by public clouds, which requires the customers to
explicitly request for, and manage, virtual machines for their jobs,
to support an extended utility computing model that directly meets
the service time objectives of cloud customers. In this model, PRO-
TEUS directly allocates an application slice of the datacenter, i.e.,
a TIVC speciﬁcation, that meets the target service time of a given
application at the minimum cost to the customer.
Acknowledgements
We thank the anonymous reviewers, Chuangxiong Guo, and es-
pecially our shepherd, Ant Rowstron, for their helpful comments.
We thank Pawan Prakash for his help with the testbed experiments.
This work was supported in part by NSF grants CNS-1054788 and
CRI-0751153.
9. REFERENCES
[1] Amazon ec2 api ec2-run-instances.
http://docs.amazonwebservices.com/AWSEC2/latest/
CommandLineReference/ApiReference-cmd-RunInstances.html.
[2] Apache hive. http://hive.apache.org/.
[3] Apache pig. http://pig.apache.org/.
[4] Cisco nexus 7000. http://www.cisco.com/en/US/prod/collateral/
switches/ps9441/ps9402/ps9512/Data_Sheet_C78-437757.pdf.
[5] Hive performance benchmarks.
https://issues.apache.org/jira/browse/HIVE-396.
[6] S. Agarwal, S. Kandula, N. Bruno, M.-C. Wu, I. Stoica, and J. Zhou.
Re-Optimizing data-parallel computing. In Proc. of USENIX NSDI, 2012.
[9] G. Ananthanarayanan, S. Kandula, A. Greenberg, I. Stoica, Y. Lu, , B. Saha,
and E. Harris. Reining in the outliers in map-reduce clusters using mantri. In
Proc. of USENIX OSDI, 2010.
[10] H. Ballani, P. Costa, T. Karagiannis, and A. Rowstron. The price is right:
Towards location-independent costs in datacenters. In Proc. of ACM HotNets,
2011.
[11] H. Ballani, P. Costa, T. Karagiannis, and A. I. T. Rowstron. Towards predictable
datacenter networks. In Proc. of ACM SIGCOMM, 2011.
[12] F. Bancilhon and R. Ramakrishnan. An amateur’s introduction to recursiv query
rocessing strategies. In In Proc. of SIGMOD, 1986.
[13] C. Brenton. Hypervisor vs host based security.
https://cloudsecurityalliance.org/wp-content/uploads/2011/11/hypervisor-vs-
hostbased-security.pdf.
[14] Y. Bu, B. Howe, M. Balazinska, and M. D. Ernst. HaLoop: Efﬁcient Iterative
Data Proecssing on Large Clusters. Proc. of the VLDB Endowment, 3(1), 2010.
[15] N. Chowdhury et al. Virtual Network Embedding with Coordinated Node and
Link Mapping. In IEEE INFOCOM, 2009.
[16] T. Condie, N. Conway, P. Alvaro, J. M. Hellerstein, J. Gerth, J. Talbot,
K. Elmeleegy, and R. Sears. Online aggregation and continuous query support
in mapreduce. In ACM SIGMOD, 2010.
[17] J. Dean and S. Ghemawat. MapReduce: Simpliﬁed Data Processing on Large
Clusters. In Proc. of USENIX OSDI, December 2004.
[18] N. G. Dufﬁeld, P. Goyal, A. Greenberg, et al. A ﬂexible model for resource
management in virtual private networks. In Proc. of ACM SIGCOMM, 1999.
[19] A. Greenberg, J. R. Hamilton, et al. VL2: a scalable and ﬂexible data center
network. In Proc. of ACM SIGCOMM, 2009.
[20] C. Guo, G. Lu, H. J. Wang, S. Yang, C. Kong, et al. Secondnet: a data center
network virtualization architecture with bandwidth guarantees. In Proc. of ACM
CoNEXT, 2010.
[21] H. Herodotou and S. Babu. Proﬁling, what-if analysis, and cost-based
optimization of mapreduce programs. PVLDB, 4(11):1111–1122, 2011.
[22] H. Herodotou, F. Dong, and S. Babu. No one (cluster) size ﬁts all: Automatic
cluster sizing for data-intensive analytics. In Proc. of ACM SOCC, Oct. 2011.
[23] H. Herodotou, H. Lim, G. Luo, N. Borisov, L. Dong, F. B. Cetin, and S. Babu.
Starﬁsh: A self-tuning system for big data analytics. In Proc. of CIDR, 2011.
[24] B. Hindman, A. Konwinski, M. Zaharia, A. Ghodsi, A. D. Joseph, R. Katz,
S. Shenker, and I. Stoica. Mesos: a platform for ﬁne-grained resource sharing in
the data center. In Proc. of USENIX NSDI, 2011.
[25] J. Kleinberg. Authoritative sources in a hyperlinked environment. J. ACM,
46(5), 1999.
[26] T. Lam and G. Varghese. Netshare: Virtualizing bandwidth within the cloud.
UCSD Technical Report, 2009.
[27] J. Mudigonda, P. Yalagandula, M. Al-Fares, and H. L. Jeffrey Mogul. SPAIN:
COTS data-center ethernet for multipathing over arbitrary topologies. In Proc.
of USENIX NSDI, 2010.
[28] D. G. Murray, M. Schwarzkopf, C. Smowton, S. Smith, A. Madhavapeddy, and
S. Hand. Ciel: a universal execution engine for distributed data-ﬂow computing.
In Proc. of USENIX NSDI, 2011.
[29] L. Page, S. Brin, R. Motwani, and T. Winograd. The PageRank citation ranking:
Bringing order to the Web. Technical Report 1999-66, Stanford Infolab, 1999.
[30] L. Popa, A. Krishnamurthy, S. Ratnasamy, and I. Stoica. Faircloud: Sharing the
network in cloud computing. In ACM HotNets, 2011.
[31] C. Raiciu, S. Barre, C. Pluntke, A. Greenhalgh, D. Wischik, and M. Handley.
Improving datacenter performance and robustness with multipath tcp. In Proc.
of ACM SIGCOMM, 2011.
[32] R. Ricci, C. Alfeld, and J. Lepreau. A Solver for the Network Testbed Mapping
Problem. SIGCOMM CCR, 33(2), 2003.
[33] H. Rodrigues et al. Gatekeeper: Supporting bandwidth guarantees for
multi-tenant datacenter networks. HP Technical Report, 2011.
[34] A. Shieh, S. Kandula, A. Greenberg, C. Kim, and B. Saha. Sharing the data
center network. In Proc. of USENIX NSDI, 2011.
[35] M. Yu, Y. Yi, J. Rexford, and M. Chiang. Rethinking virtual network
embedding: substrate support for path splitting and migration. ACM SIGCOMM
CCR, 38(2), 2008.
[7] M. Al-Fares, A. Loukissas, and A. Vahdat. A scalable, commodity data center
[36] M. Zaharia, D. Borthakur, J. Sen Sarma, K. Elmeleegy, S. Shenker, and
network architecture. In Proc. of ACM SIGCOMM, 2008.
[8] M. Al-fares, S. Radhakrishnan, et al. Hedera: Dynamic ﬂow scheduling for data
center networks. In Proc. of USENIX NSDI, 2010.
I. Stoica. Delay scheduling: a simple technique for achieving locality and
fairness in cluster scheduling. In Proc. of EuroSys, 2010.
210