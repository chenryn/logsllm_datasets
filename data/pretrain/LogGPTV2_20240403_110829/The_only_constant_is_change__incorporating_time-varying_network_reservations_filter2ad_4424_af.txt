# Network Utilization and Job Sharing in Datacenters

## 1. Introduction
This section discusses the network utilization and job sharing in datacenters, particularly focusing on the impact of different tree levels and load conditions.

### 1.1 Job Localization and Link Sharing
- **Localization**: Most jobs are confined to Top-of-Rack (ToR) switches, with fewer than 99 and 26 jobs within level-2 and level-3 subtrees, respectively.
- **Link Sharing**: Under 100% load, fewer than 102 and 37 jobs are within level-2 and level-3 trees, respectively. This indicates that most jobs are localized to small subtrees, resulting in few jobs sharing any link at the same time.

### 1.2 Figure 17: Maximum Number of Jobs Sharing a Link
- **Level 1**: The maximum number of jobs sharing a link is 4, corresponding to the 4 VMs of a machine allocated to 4 different jobs.
- **Level 2 and Level 3**: The maximum numbers of jobs sharing a link are less than 13 and 26, respectively, under 80% and 100% load.

### 1.3 Workload Experiments
- **Large Jobs**: When the workload consists of large jobs requiring thousands of VMs, few jobs can be scheduled concurrently, leading to low link sharing.
- **Mixed Workloads**: Even with mixed small and large jobs, there are few jobs sharing any given link in the network.

### 1.4 Bandwidth Reservation
- **Policers in Switches**: The low number of jobs sharing any link suggests that per-job bandwidth reservation can be easily accomplished using policers in off-the-shelf switches.

## 2. Tenant Cost and Provider Revenue

### 2.1 Current Charging Model
- **VM-Based Charging**: Cloud providers like Amazon EC2 charge tenants based on the consumed VM-time.
- **Impact on Revenue**: Lower rejection rates under TIVC (Time-Invariant Virtual Cluster) compared to VC (Virtual Cluster) directly translate into increased revenue for cloud providers, while individual tenant costs remain the same.

### 2.2 New Charging Model
- **Charging for Bandwidth**: Future cloud providers should explicitly charge for networking bandwidth, providing tenants with explicit bandwidth reservations.
- **Charging Model**: A simple model charges N VMs for time T as \( N \cdot (T \cdot k_v + k_b \cdot V) \), where \( k_v \) is the unit-time VM cost, \( k_b \) is the unit-volume bandwidth cost, and \( V \) is the total bandwidth volume reserved over time T.

### 2.3 Comparison of Costs and Revenues
- **Figure 18(a)**: Under the first price, TIVC allows tenants to pay about 20% less than VC, independent of the load. At low loads (20-40%), the cloud provider's revenue under TIVC is about 20% lower. At close to 100% load, the provider stays revenue-neutral due to higher job acceptance rates.
- **Figure 18(b)**: Under the second price, TIVC allows tenants to pay about 12% less, and the cloud provider makes more revenue when the load exceeds 60%.

## 3. Testbed Experiment

### 3.1 Implementation
- **PROTEUS**: Implemented on a 3-tier tree topology testbed with 18 machines, each running 2 VMs.
- **Link Capacities**: Level-1, level-2, and level-3 link capacities are 230, 700, and 1000 Mbps, respectively.

### 3.2 Workload and Results
- **Workload**: Mixed workload of 30 jobs, 10 each of Sort, Hive Join, and Hive Aggregation.
- **Completion Time**: Baseline, TIVC, and VC models have completion times of 2405, 3770, and 5140 seconds, respectively. TIVC reduces VC's completion time by 27%.
- **Job Execution Time**: Median per-job execution time under Baseline is 10% longer than TIVC. TIVC and VC result in similar per-job execution times due to threshold bandwidth reservation.

### 3.3 Scalability
- **Allocation Algorithm**: Highly scalable, with a median allocation time of 18.0ms and 99th percentile time of 28.0ms on an 8-core Intel Xeon E5410 processor with 16 GB RAM.

## 4. Related Work

### 4.1 Virtual Network Abstractions
- **Oktopus and Gatekeeper**: Propose per-VM hose models but focus on full bisection networks and server access bandwidth.
- **TIVC**: Extends the per-VM hose model to capture the time-varying nature of cloud applications and derives model parameters automatically.

### 4.2 Network Sharing Mechanisms
- **Seawall and Netshare**: Provide fair sharing with minimum bandwidth guarantees but do not offer deterministic guarantees.
- **Elastisizer, StarFish, and CBO**: Focus on VM type and number selection for MapReduce jobs, complementing our network profiling technique.

### 4.3 Virtual Network Embedding
- **Previous Work**: Uses heavy-weight optimization solvers and cannot scale to modern datacenters.
- **Fair and Efficient Charging Models**: Recent studies on fair and efficient charging models complement PROTEUS and can be incorporated.

## 5. Conclusions

- **Primary Contributions**: Design of TIVC, a network abstraction capturing the time-varying nature of cloud applications, and a systematic profiling-based methodology.
- **Experimental Evaluation**: TIVC outperforms fixed-bandwidth abstractions in improving job throughput, increasing provider revenue, and reducing tenant costs.
- **Future Directions**: PROTEUS can extend the utility computing model to meet service time objectives and minimize customer costs.

## 6. Acknowledgements
- **Thanks to Reviewers**: Chuangxiong Guo, Ant Rowstron, and Pawan Prakash.
- **Funding**: Supported by NSF grants CNS-1054788 and CRI-0751153.

## 7. References
[References listed here]

---

This optimized version of the text is more structured, clear, and professional, making it easier to understand and follow.