output corresponding where faults are potentially located is
recomputed (partial recomputation).
0,0y(cid:48) − (cT
cT
1,1y(cid:48) − (cT
cT
0,0A)x = 3
1,1A)x = 5
(Error of 3 at y’[0])
(Error of 5 at y’[1])
B. Exploiting Sparsely Structured Products
Due to the sparse and structured nature of codes used
within the error localization process, the runtime overhead of
locating faults can be further reduced. Each code is vector
comprised mostly of zeros except for one contiguous segment
of ones. The feature can be exploited in both matrix vector
5
products and the dot products that are used to compute
the checksums. The ﬁrst
two procedures described in Al-
gorithm III.1, illustrate how this structure can be exploited
to reduce the overhead of both the matrix vector products
and dot products used multiple times at every level of the
tree. By passing pointers indicating the segment start and
end points, only the rows of matrix A for the matrix vector
product and the rows of x, y for the dot product will be read.
Algorithm III.1 assumes that the matrix A is in CSR format,
which is represented by a row pointer array (pntr), an array of
the values within the matrix (val), and an array of the column
indices corresponding to those values (indx).
Based on the observation that the sparse matrix vector
product of a sparse and structured vector is also typically
sparse and structured (i.e. a sparse vector which is densely
concentrated), we can also compute the dot product on the
RHS of the check as a sparse dot product. This requires that we
store pointers indicating the location of the nonzero elements
in the vector. The sparse matrix vector product procedure in
Algorithm III.1 illustrates how the pointers for the output
vector can be easily computed as the inputs are scanned.
Note that on the LHS of the check invariant, we can
also exploit the structure of the tree and compute half of
the checksums, corresponding to the coded output (cy
ij), from
prior coded output calculations. However, a tradeoff exists by
doing this since the accuracy of checksum can be signiﬁcantly
reduced (i.e. 3 − 5 decimal points or ±1e − 2 as opposed
to  endout
then endout ← indx[j]
do
do
return (y, startout, endout)
procedure DOT SPARSE(x, y, start, end)
temp ← 0
for i ← start to end
do temp+ = x[i]y[i]
return (temp)
subcy[0] =(cid:80)N
main
for k ← 0 to tree size
i=0 yi
if f inished[k] == 0
subcy[k] =(cid:80)end[k]
then
if valid[k]
then
i=start[k] yi
(cid:40)subcA[k] = MATVEC SPARSE BIN(
A, start[k], end[k])
valid[k] = 1
start[k], end[k])
k Ax ← DOT SPARSE(subcA[k], x,
cT
syndrome = subcy[k] − cT
if |syndrome| < τ
then mark all children of k as ﬁnished
else
yk ← Akx (MATVEC SPARSE())
(cid:26)comment: Recompute segment k
k Ax
f inished[k] ← 1
IV. METHODOLOGY
A. Fault Model
Our evaluation focuses on transient faults that affect the
outputs of numerical computations. Other manifestations of
transient faults, such as memory corruption, deviations of
control ﬂow or memory access errors are assumed to be ac-
counted for by using simple low overhead techniques [12, 13],
unless they manifest as numerical data errors, which the
proposed techniques cover. This is a widely addressed fault
model [18, 14, 11].
Faults are injected into the computation by instrumenting
the application directly and calculating the random faults on-
line. During execution the instrumentation code adds a random
numeric error to the output of individual multiply/addition
Variable
N
tree size
subcA[k]
valid[k]
subcy[k]
f inished[k]
syndrome[k]
cstart[k]
cend[k]
Description
Number of Rows and Columns in A
Total number of segments in tree: 2N − 1
Storage of coded checksums (i.e. cT A products
Indicates if subcA[k] has been computed
Storage of cT y products
Indicates if kth node completed (eliminated)
Syndrome computed for segment/node k
Index of ﬁrst element of segment k
Index of one past the last element of segment k
TABLE I: Fault Localization Variables
operations used within matrix vector multiplication [27]. Faults
are also similarly injected into the arithmetic operations that
compose the checks themselves.
Over 50k runs were executed in order to get the statistical
results shown in Section V. These experiments were run in
parallel on HPC systems at LLNL (Sierra, Hera, and Atlas),
where the distribution was analyzed and plotted using matlab
and ggplot2 [31].
Since the timing of each fault is assumed to be independent,
fault times are sampled from an exponential distribution with
λ is the expected number of arithmetic operations
a rate λ. 1
between consecutive faults. The fault rate of a system is
deﬁned as the probability that a given arithmetic operation
has a fault and is equivalent to λ in our methodology.
Our experiments examine different fault rates that model
phenomena ranging from physical faults arising from infre-
quent particle strikes (3-4 soft errors per day) to frequent errors
arising from the use of aggressively designed (error-prone)
technologies at large scales (multiple errors per second). A
fault rate of 1e-8 corresponding roughly to soft errors which
occur about 3 per day, and 1e-4 corresponds to aggressively
designed systems, which trade off accuracy for energy for
example, and exhibit multiple errors per second.
When a fault occurs, it is modeled by drawing a value
from a fault distribution that models the arithmetic effects of
circuit-level faults at a high level (i.e. a symmetric distribution
with two Gaussian modes, centered at 1e5 and −1e5 and with
variance 1e2) and adding it to the target operation. These error
magnitudes distributions are representative of faults arising in
arithmetic units from timing errors due to voltage over-scaling
[27, 20].
Parameter
Fault Rates
Fault Model
Parallel Nodes (N)
Description
1e-8, 1e-7,4e-7,1e-6,4e-6,1e-5,1e-4
Symmetric distribution w/ two
Gaussian modes
1,2,10,20,100
TABLE II: Fault Parameters
of practical contexts, we evaluated them on 30 randomly cho-
sen square linear systems from the University of Florida Sparse
Matrix Collection and Matrix Market [7, 15] with the following
properties: matrix size ∈ [1000, 100000], symmetric, positive
deﬁnite, and real. These are the most common parameters for
matrices used in scientiﬁc computing applications and those
used with linear solvers due to good convergence properties.
These matrices represent a variety of physical phenomena
and real algorithms, including model reductions, computational
ﬂuid dynamics, and circuit simulation. Table III lists the chosen
matrices and their properties (nnz represents the number of
non-zero elements, N represents the size of the problem,
and Sparsity represents relative number of non-zeros per row,
nnz/N).
We evaluate our fault detection techniques both in the