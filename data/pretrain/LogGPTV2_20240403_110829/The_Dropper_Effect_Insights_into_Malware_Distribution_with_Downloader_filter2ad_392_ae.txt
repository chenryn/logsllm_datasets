1093(41%)
60 (0.5%)
1249 (43%)
38 (0.3%)
1041 (41%)
67 (0.6%)
by
Table 7: Testing Classiﬁer on the Unlabeled Inﬂuence Graphs.
5.6 Online Detection Experiment
Finally, we perform an experiment where we simulate the way
our classiﬁer would be employed operationally, for detecting ma-
lware in an online manner. We prepare the training set of 21,543
malicious and 21,755 benign IGs from the data before 2014. For
the testing set, we build IGs based on the data from the year 2014.
There are 12,299 malicious and 12,594 benign IGs for the test set.
Since we are trying online detection, we assume here that the data
from 2014 may not had enough time to collect the prevalence of the
executables, so we disregard the prevalence features for the classi-
ﬁcation. Note that prevalence is one of the top features that perform
well in the classiﬁcation. We train the RFC on the training set, with
the same parameters used on section 5.4. Then we apply the trained
classiﬁer on the testing set. We obtain 99.8% TP rate, 1.9% FP rate
and 0.2% FN rate, with 99.0% F1-score. This suggests that our fe-
atures do not exhibit a signiﬁcant concept drift over time and show
the potential for using our classiﬁer as a robust online detection
technique.
6. DISCUSSION
We now discuss the lessons learned from our experiments, focu-
sing on the implications for malware detection and for attack attri-
bution.
Opportunity for Improving Malware Detection. Our research
represents a ﬁrst step towards understanding the properties of
downloader graphs in the wild. We demonstrate that some of these
properties, such as the large graph diameter, high growth interval
of the inﬂuence graphs, large number of distinct droppers acces-
sing a domain, represent strong indication of malicious activities.
This insight can lead to deterministic detection techniques that help
existing anti-virus products to block certain classes of malware in
early phases of their lifecycle. The intuition behind these beneﬁts
is that the properties of downloader graphs can provide evidence of
malicious activity before new malware samples can be investigated
by the security community. Moreover, our results highlight the be-
neﬁts of incorporating downloader graph features into probabilistic
detection techniques along with existing host-based and network-
based features. The information extracted from downloader graphs
complements the existing approaches as it (a) captures the client-
side activity of malware delivery networks, (b) reﬂects the relation-
ships among malware families, and (c) helps increase the detection
performance and reduce the detection latency. We note that our te-
chniques operate on end hosts. Although the analysis of network
trafﬁc may indicate when downloading is in progress [10, 30], it
cannot determine which executable triggers the download; thus it
may not be possible to construct a complete downloader graph.
Blocking Malicious Droppers. Our results raise the question, Sho-
uld we require user approval for all downloads of executable pro-
Average Early Detection (days)Early Detection Rate (%)Avg. Early Detection (days)Early Detection Rate (%)0204022242628303234VirusTotal Detection Rate (%)0501001127grams? If an operating system, or an anti-virus program, quaranti-
nes every executable downloaded and presents a user with a dialog
to ask for the user’s approval, the operation of malicious downloa-
ders would be severely impaired. Unfortunately, this approach wo-
uld also harm security by inhibiting the deployment of security pat-
ches, as manual or semi-automated software updating approaches
are considerably less effective than silent updates which download
and install security patches without user interactions [6, 16]. A
more effective approach would be to require digital signatures for
programs that download other executables, as is currently done for
device drivers [15]. This approach would also be more efﬁcient
than the attempts to whitelist all benign software [24], because only
a few benign programs (87,906 in our data set) download other exe-
cutables.
Implications for Attack Attribution. Attack attribution to iden-
tify attackers is generally considered as a hard problem because
attackers may employ various methods to conceal their identi-
ties, e.g., obfuscating or re-packing binaries, changing the URLs
and domains of servers, launching attacks from geographically-
distributed compromised machines. However, 55.5% of malicious
downloaders are signed, accounting for 73.8% of the malicious in-
ﬂuence graphs. For example, one PPI provider consistently speci-
ﬁes “Amonetize LTD” in the publisher ﬁeld of the digital signature.
Attackers may distribute signed programs to avoid raising suspi-
cion and to remain undetected for possibly longer periods of time.
This suggests that, for the miscreants operating malware dissemi-
nation networks, the beneﬁts of being able to remain stealthy for
a while outweigh the beneﬁts of thwarting attribution efforts. Si-
milarly, attackers may transmit malicious payloads over the HTTP
protocol, which is one of the most common trafﬁc crossing orga-
nizations network perimeters, to go around ﬁrewall restrictions. In
fact, this has facilitated several efforts to proﬁle the organizations
involved in malware distribution [3, 8, 17]. Our detection approach
based on the downloader graph analytics may force attackers to em-
ploy stealthier techniques to download malicious payloads, such as
utilizing custom ports and protocols, but these techniques are more
likely to raise alarms in ﬁrewalls and intrusion-prevention systems.
Limitation. Droppers with rootkit functionality would evade our
technique. However, rootkits are typically employed to hide more
incriminating functionality, while our technique relies mainly on
the ability to track downloader-payload relationships. This provi-
des a new signal, complementary to current AV engines, and our
experiments suggest that it can detect malware that are not curren-
tly being detected by existing AVs.
7. RELATED WORK
Several research efforts have focused on characterizing the pro-
perties of malware delivery networks and on taking advantage of
these properties for detecting malware. Provos et al. [20] described
drive-by-download attacks caused by exploiting vulnerabilities in
web browsers, analyzed the tree-like structure of redirection paths
leading to the main malware distribution sites, and identiﬁed seve-
ral mechanisms to inject malicious web content into popular pages
(e.g., comments on blogs or syndication in Ad serving networks).
Li et al. [14] further analyzed URL redirection graphs and identi-
ﬁed a set of topologically dedicated malicious hosts (e.g., Trafﬁc
Distribution Systems) that are long-lived and receive trafﬁc from
new attack campaigns over time. Perdisci et al. [19] analyzed the
structural similarities among malicious HTTP trafﬁc traces which
include a variety of activities, such as receiving commands from
C&C servers, sending spam, exﬁltrating private data, and downloa-
ding updates. Xu et al. [31] ﬁngerprinted several types of malicious
servers, including exploit servers (for malware distribution through
drive-by downloads), C&C servers (for command and control), re-
direction servers (anonymity), and payment servers (for moneti-
zation).
Inspired by these studies, several techniques were pro-
posed for detecting malware download events from network tra-
fﬁc [10, 30]. Unlike our work, these studies focus on the server
side of malware distribution networks, and they are unable to iden-
tify which program triggered the download, and cannot be used to
reconstruct complete downloader graphs.
Prior research on the behavior of downloaders [3, 17, 23] focu-
sed on executing malware droppers in lab environments, typically
for short periods of time (e.g., up to one hour), in order to observe
the communication protocols they employ and to milk their server-
side infrastructures (i.e., to download the payloads). For example,
Caballero et al. [3] described pay-per-install infrastructures, which
distributed malware on behalf of their afﬁliates, and analyzed their
structure and business model. They also provided an example of
a download tree, which reached diameter 4 within 10 minutes.
Follow-up work reported that downloaders might remain active for
over 2 years [23], and characterized several malware distribution
operations [17]. In contrast, we analyze malicious droppers that re-
main undetected for 80.6 days on average in the wild. We compare
the properties of malicious and benign downloader graphs, in order
to assess their real impact on end-user security and the potential
beneﬁts of downloader graph analytics. This approach allows us
to determine that some features, such as high graph diameter, slow
growth rate of the inﬂuence graphs, large number of distinct portal
accessing a domain are strong indication of malicious behavior, and
to train a generic classiﬁer for malware detection using information
extracted from downloader graphs.
Closest to our work are recent techniques for assigning reputa-
tion scores to executable ﬁles by performing belief propagation on
bipartite graphs [5, 28]. Chau et al. [5] constructed a graph that
encoded the relationship between ﬁles and the hosts they are pre-
sent on, building on the intuition that hosts with poor cyber-hygiene
tended to contain more malware. Tamersoy et al. [28] constructed
a graph that encoded the relationship between different ﬁles that
were present on the same host, building on the intuition that se-
veral malware samples were often distributed together (guilt-by-
association). In our work, we construct graphs that encode a se-
mantic relationship between ﬁles—the fact that one ﬁle downloads
another ﬁle—which can provide deeper insights into malware dis-
tribution activities.
There is a rich literature on graph mining techniques [4]. We cite
here the Oddball approach [1], which consists of extracting features
from the k-hop neighborhoods of every node, identifying patterns
for these features and analyzing the outliers that do not seem to
conform to these patterns, for its similarity with the technique we
employ in Section 4. Graph analytics have also been employed
for analyzing function call graphs in malware samples [9, 11, 33],
spamming operations [32, 34], and vote gaming attacks [22].
In
contrast to these approaches, we propose downloader graph analy-
tics for analyzing the malware delivery activities on the client side.
8. CONCLUSIONS
We analyze downloader graphs in the wild and uncover the di-
fferences in growth patterns between benign and malicious gra-
phs. Because downloader graphs capture the relationships between
downloaders and the supplementary executables they download, we
can identify large parts of the malware download activity on each
host by analyzing the upstream download chain. We identify deter-
ministic techniques for detecting certain classes of malware based
on properties of the downloader graphs, including their growth rate,
diameter and the diversity in the set of Internet domains accessed.
1128We also describe a generic malware detection system, which uses
machine learning techniques for automatically learning models of
malicious download graphs. We evaluate our system on 19 million
graphs and show that it detects malware with high accuracy and
earlier than existing anti-virus systems.
Acknowledgments
We thank Amol Deshpande, Jonathan Katz, and Michel Cukier for
early feedback. We also thank VirusTotal for access to their service
and Symantec for access to WINE (the data analyzed in this paper
corresponds to the reference data set WINE-2015-002). This re-
search was partially supported by the Maryland Procurement Ofﬁce
(contract H98230-14-C-0127) and by the Department of Defense.
9. REFERENCES
[1] L. Akoglu, M. McGlohon, and C. Faloutsos. Oddball:
Spotting anomalies in weighted graphs. In KDD. 2010.
[2] L. Breiman. Random forests. 2001.
[3] J. Caballero, C. Grier, C. Kreibich, and V. Paxson.
Measuring pay-per-install: The commoditization of malware
distribution. In USENIX Security Symposium, 2011.
[4] D. Chakrabarti and C. Faloutsos. Graph mining: Laws,
generators, and algorithms. ACM Computing Surveys
(CSUR), 2006.
[5] D. H. Chau, C. Nachenberg, J. Wilhelm, A. Wright, and
C. Faloutsos. Polonium: Tera-scale graph mining and
inference for malware detection. In SDM, 2011.
[6] T. Dübendorfer and S. Frei. Web browser security update
effectiveness. In CRITIS Workshop, September 2009.
[7] T. Dumitras, and D. Shou. Toward a standard benchmark for
computer security research: The Worldwide Intelligence
Network Environment (WINE). In EuroSys BADGERS
Workshop, Salzburg, Austria, 2011.
[8] C. Grier, L. Ballard, J. Caballero, N. Chachra, C. J. Dietrich,
K. Levchenko, P. Mavrommatis, D. McCoy, A. Nappa,
A. Pitsillidis, N. Provos, M. Z. Raﬁque, M. A. Rajab,
C. Rossow, K. Thomas, V. Paxson, S. Savage, and G. M.
Voelker. Manufacturing compromise: the emergence of
exploit-as-a-service. In CCS, 2012.
[9] X. Hu, T. Chiueh, and K. G. Shin. Large-scale malware
indexing using function-call graphs. In CCS, 2009.
[10] L. Invernizzi, S.-J. Lee, S. Miskovic, M. Mellia, R. Torres,
C. Kruegel, S. Saha, and G. Vigna. Nazca: Detecting
Malware Distribution in Large-Scale Networks. In NDSS,
2014.
[11] D. Kong and G. Yan. Discriminant malware distance
learning on structural information for automated malware
classiﬁcation. In KDD, 2013.
[12] M. Kubat, R. C. Holte, and S. Matwin. Machine learning for
the detection of oil spills in satellite radar images. Machine
learning, 1998.
[13] N. Leontiadis, T. Moore, and N. Christin. A Nearly
Four-Year Longitudinal Study of Search-Engine Poisoning.
In CCS, 2014.
[14] Z. Li, S. Alrwais, Y. Xie, F. Yu, and X. Wang. Finding the
linchpins of the dark web: a study on topologically dedicated
hosts on malicious web infrastructures. In IEEE S&P, 2013.
[15] Microsoft. Driver signing policy.
https://msdn.microsoft.com/en-us/library/
windows/hardware/ff548231(v=vs.85).aspx.
[16] A. Nappa, R. Johnson, L. Bilge, J. Caballero, and
T. Dumitras,. The Attack of the Clones: A Study of the
Impact of Shared Code on Vulnerability Patching. In IEEE
S&P, San Jose, CA, 2015.
[17] A. Nappa, M. Z. Raﬁque, and J. Caballero. The MALICIA
dataset: identiﬁcation and analysis of drive-by download
operations. IJIS, 2015.
[18] A. Nappa, Z. Xu, M. Z. Raﬁque, J. Caballero, and G. Gu.
Cyberprobe: Towards internet-scale active detection of
malicious servers. In NDSS. The Internet Society, 2014.
[19] R. Perdisci, W. Lee, and N. Feamster. Behavioral clustering
of http-based malware and signature generation using
malicious network traces. In NSDI, 2010.
[20] N. Provos, P. Mavrommatis, M. A. Rajab, and F. Monrose.
All Your iFRAMEs Point to Us. In USENIX Security
Symposium, 2008.
[21] J. R. Quinlan. Induction of decision trees. Machine learning,
1986.
[22] A. Ramachandran, A. Dasgupta, N. Feamster, and
K. Weinberger. Spam or ham?: characterizing and detecting
fraudulent not spam reports in web mail systems. In CEAS,
2011.
[23] C. Rossow, C. Dietrich, and H. Bos. Large-scale analysis of
malware downloaders. In DIVMA. 2013.
[24] A. Sedgewick, M. Souppaya, and K. Scarfone. Guide to
application whitelisting. Technical Report Special
Publication 800-167 (Draft), National Institute of Standards
and Technology, 2014.
[25] R. Sommer and V. Paxson. Outside the closed world: On
using machine learning for network intrusion detection. In
IEEE S&P, 2010.
[26] Y. Sun, A. K. Wong, and M. S. Kamel. Classiﬁcation of
imbalanced data: A review. IJPRAI, 2009.
[27] Symantec. W32.Sobig.F.
http://www.symantec.com/security_response/
writeup.jsp?docid=2003-081909-2118-99, 2003.
[28] A. Tamersoy, K. Roundy, and D. H. Chau. Guilt by
association: large scale malware detection by mining
ﬁle-relation graphs. In KDD, 2014.
[29] G. Tenebro. The Bredolab Files. Symantec Whitepaper.
http://securityresponse.symantec.com/content/
en/us/enterprise/media/security_response/
whitepapers/the_bredolab_files.pdf, 2009.
[30] P. Vadrevu, B. Rahbarinia, R. Perdisci, K. Li, and
M. Antonakakis. Measuring and Detecting Malware
Downloads in Live Network Trafﬁc. In ESORICS, 2013.
[31] Z. Xu, A. Nappa, R. Baykov, G. Yang, J. Caballero, and
G. Gu. AUTOPROBE: Towards Automatic Active Malicious
Server Probing Using Dynamic Binary Analysis. In CCS,
2014.
[32] C. Yang, R. C. Harkreader, and G. Gu. Die Free or Live
Hard? Empirical Evaluation and New Design for Fighting
Evolving Twitter Spammers. In RAID, 2011.
[33] M. Zhang, Y. Duan, H. Yin, and Z. Zhao. Semantics-aware
android malware classiﬁcation using weighted contextual api
dependency graphs. In CCS, 2014.
[34] Y. Zhao, Y. Xie, F. Yu, Q. Ke, Y. Yu, Y. Chen, and E. Gillum.
BotGraph: Large Scale Spamming Botnet Detection. In
NSDI, 2009.
[35] C. C. Zou and R. Cunningham. Honeypot-aware advanced
botnet construction and maintenance. In DSN, 2006.
1129