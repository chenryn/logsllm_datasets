### References

1. Yu, Y., et al. "Communication-Efficient Distributed Machine Learning with the Parameter Server." *NeurIPS*, 2014.
2. Lian, X., Huang, Y., Li, Y., and Liu, J. "Asynchronous Parallel Stochastic Gradient for Nonconvex Optimization." *NeurIPS*, 2015.
3. McMahan, H. B., Moore, E., Ramage, D., Hampson, S., et al. "Communication-Efficient Learning of Deep Networks from Decentralized Data." *ArXiv*, 1602.05629.
4. Melis, L., Song, C., De Cristofaro, E., and Shmatikov, V. "Exploiting Unintended Feature Leakage in Collaborative Learning." *S & P*, 2019.
5. Mirkovic, J. and Reiher, P. "A Taxonomy of DDoS Attack and DDoS Defense Mechanisms." *ACM SIGCOMM Computer Communication Review*, 2004.
6. Pasqualetti, F., Bicchi, A., and Bullo, F. "Consensus Computation in Unreliable Networks: A System Theoretic Approach." *IEEE Transactions on Automatic Control*, 2010.
7. Robbins, H. and Monro, S. "A Stochastic Approximation Method." *Herbert Robbins Selected Papers*, pages 102â€“109, 1985.
8. Rousseeuw, P. J. "Multivariate Estimation with High Breakdown Point." *Mathematical Statistics and Applications*, 1985.
9. Salem, A., Bhattacharyya, A., Backes, M., Fritz, M., and Zhang, Y. "Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning." *ArXiv*, 1904.01067.
10. Shamir, O., Srebro, N., and Zhang, T. "Communication-Efficient Distributed Optimization Using an Approximate Newton-Type Method." *ICML*, 2014.
11. Shokri, R., Stronati, M., Song, C., and Shmatikov, V. "Membership Inference Attacks Against Machine Learning Models." *S & P*, 2017.
12. Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. "Mastering the Game of Go with Deep Neural Networks and Tree Search." *Nature*, 2016.
13. Sutton, R. S., Barto, A. G., Bach, F., et al. "Reinforcement Learning: An Introduction." *MIT Press*, 1998.
14. Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour, Y. "Policy Gradient Methods for Reinforcement Learning with Function Approximation." *NeurIPS*, 2000.
15. Tanenbaum, A. S. and Van Steen, M. "Distributed Systems: Principles and Paradigms." *Prentice-Hall*, 2007.
16. Tsitsiklis, J. N., Bertsekas, D. P., and Athans, M. "Distributed Asynchronous Deterministic and Stochastic Gradient Optimization Algorithms." *American Control Conference*, 1984.
17. Watkins, C. J. C. H. and Dayan, P. "Q-Learning." *Machine Learning*, 1992.
18. Wellner, J. et al. "Weak Convergence and Empirical Processes: With Applications to Statistics." *Springer Science & Business Media*, 2013.
19. Werbos, P. J. "Backpropagation Through Time: What It Does and How to Do It." *Proceedings of the IEEE*, 1990.
20. Xie, C., Koyejo, O., and Gupta, I. "Zeno: Distributed Stochastic Gradient Descent with Suspicion-Based Fault-Tolerance." *ICML*, 2018.
21. Yang, Q., Liu, Y., Chen, T., and Tong, Y. "Federated Machine Learning: Concept and Applications." *TIST*, 2019.
22. Yin, D., Chen, Y., Ramchandran, K., and Bartlett, P. "Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates." *ArXiv*, 1803.01498.
23. Zhang, S., Choromanska, A., and LeCun, Y. "Deep Learning with Elastic Averaging SGD." *NeurIPS*, 2015.
24. Zinkevich, M., Weimer, M., Li, L., and Smola, A. J. "Parallelized Stochastic Gradient Descent." *NeurIPS*, 2010.

### A. Other Details

#### A.1 Experimental Environments
All defenses and experiments were implemented using Torch [19], an open-source software framework for numeric computation and deep learning. Experiments were conducted on a Linux server running Ubuntu 16.04, equipped with one AMD Ryzen Threadripper 2990WX 32-core processor and two NVIDIA GTX RTX2080 GPUs. The distributed learning setting was simulated by sequentially computing gradients on randomly sampled mini-batches.

#### A.2 Estimating KL-Divergence
We designed the following procedure to estimate the pairwise KL-divergence between datasets \( D_i \) and \( D_j \) on the Healthcare dataset, which consists of samples of the form \((x, y)\) such that \( x \in \mathbb{R}^n \) and \( y \in [K] \), where \( n = 1024 \) and \( K = 10 \). Figure 10 shows the heatmap of the KL-divergence among the local datasets on each worker and the full QV set. The empirical KL-divergence is about 0.16 on average.

1. Train one probabilistic model \( p_i(y|x) \) for each dataset \( D_i \).
2. Perform uniform sampling over \([-0.5, 0.5]^n\) for \( N \) times to form a set of points \(\{x_k\}_{k=1}^N\).
3. Calculate the empirical KL-divergence between the joint distributions underlying \( D_i \) and \( D_j \) using:
   \[
   \text{KL}(D_i || D_j) = \frac{1}{K \times N} \sum_{k=1}^{N} \sum_{c=1}^{K} p_i(x_k | y = c) \log \left( \frac{p_i(x_k | y = c)}{p_j(x_k | y = c)} \right)
   \]

**Figure 10:** Estimated KL-divergence among local datasets and the prepared validation set on Healthcare.

Estimating the KL-divergence can be challenging when the QV set is very small. Ideally, we require knowledge of the distribution from which the QV set is sampled to estimate the conditional distribution \( p(y|x) \) via learning-based approaches. More samples in the QV set reduce bias and improve the accuracy of the KL-divergence estimation. The minimum requirement is that the QV set should contain at least one sample from each class, allowing estimation of the conditional distribution with support vector classifiers or K-Nearest Neighbor (KNN). Future work could explore how to guarantee low KL-divergence in distributed learning protocols with privacy requirements [61].

#### A.3 Details of the Benchmark Systems

1. **MNIST:**
   - **Task:** Hand-written digit classification.
   - **Dataset:** MNIST [39] with 60,000 training images and 10,000 testing images, each 28x28 pixels.
   - **Workers:** 50.
   - **Model:** Fully connected feed-forward neural network with 784 inputs, 10 outputs with softmax activation, and one hidden layer with 30 ReLU units. The parameter dimension is 25,450.

2. **CIFAR-10:**
   - **Task:** Image classification.
   - **Dataset:** CIFAR-10 [35] with 60,000 training images and 10,000 testing images, each 28x28x3 pixels.
   - **Workers:** 50.
   - **Model:** ResNet-18 [32] with 18 end-to-end layers and 11,173,962 learnable parameters.

3. **Yelp:**
   - **Task:** Sentiment classification (binary).
   - **Dataset:** Yelp reviews with 20,000 1,024-dimensional features per worker, extracted using a pretrained BERT language model [23].
   - **Workers:** 10.
   - **Model:** Fully connected feed-forward neural network with 1,024 inputs, 2 outputs with softmax activation, and one hidden layer with 10 sigmoid units. The parameter dimension is 10,272.

4. **Healthcare:**
   - **Task:** Predicting healthcare provider type (10 classes) from textual treatment descriptions.
   - **Dataset:** CMS public healthcare records [2] with 20,000 1,024-dimensional BERT features per worker.
   - **Workers:** 50.
   - **Model:** Fully connected feed-forward neural network with 1,024 inputs, 10 outputs with softmax activation, and one hidden layer with 32 sigmoid units. The parameter dimension is 33,130.

#### A.4 Analytical Results

We use Theorem 1 as an example to explain the terms \( R \), \( M \), \( \eta \), and \( S \) and provide their empirical values on the Healthcare dataset. Our terminology follows the conventions in [14], a standard text on optimization theory.

- **Diameter \( R \):** The diameter of the parameter space \( \Theta \) (i.e., the feasible set of parameters of the underlying learning model) is defined as the maximal 2-norm of an element \( \theta \in \Theta \). Formally, \( R = \sup\{\|\theta\|_2 : \theta \in \Theta\} \). On the Healthcare dataset, we estimated the 2-norm of the flattened parameter of the neural network during the learning process, which is plotted in Figure 11(a). The average value of \( R \) is around 11.05.

- **Upper bound of gradient norm \( M \):** The term \( M \) denotes the upper bound of the gradient norm. Formally, \( M = \sup_{\theta \in \Theta} \|\nabla_\theta \hat{f}(\theta, D_{\text{train}})\|_2 \). On the Healthcare task, we computed the 2-norm of the gradient submitted by the always-benign worker during the learning process, which is plotted in Figure 11(b). The average value of \( M \) is around 0.36.

- **Smoothness factor \( \eta \):** The loss function \( f \) is said to be \( \eta \)-smooth if for all \( \theta_1, \theta_2 \in \Theta \),
  \[
  |\hat{f}(\theta_1, D_{\text{train}}) - \hat{f}(\theta_2, D_{\text{train}})| \leq \eta \|\theta_1 - \theta_2\|_2
  \]
  We estimated the empirical scale of \( \eta \) by calculating the expressions at both sides of the definition during the learning process, which is plotted in Figure 11(c). The average value of \( \eta \) is around 0.50.

- **Size of mini-batch \( S \):** The term \( S \) denotes the training size of the mini-batch on which the always-benign worker calculates the gradient. Additionally, \( S \) must be at least 1 (i.e., the training set contains at least one sample). On the Healthcare dataset, \( S \) is set to 256.

- **Max-norm of the loss function:** The max-norm of the loss function (implemented as cross-entropy) is upper-bounded by the maximal entropy of the K-class classification task, i.e., \( \|f\|_\infty \leq \frac{1}{K} \ln K \), which is about 0.23 for \( K = 10 \) on the Healthcare dataset. The estimated KL divergence term is about 0.16 from Figure 10.

Thus, on the Healthcare dataset under static Byzantine attacks with \( \beta = 0.7 \) and \( n = 50 \), the numeric form of Theorem 1 is:
\[
f(\theta_t) - f(\theta^*) < \frac{2.05}{\sqrt{t}} + \frac{16.58}{t} + 0.13 + O(e^{-t})
\]
which produces the curve of the predicted training loss in Figure 11(d). Compared with the empirical training loss curve, we find that the prediction from Theorem 1 roughly conforms to GAA's empirical behavior in this case.

#### A.5 Analysis of a Fluctuated Phenomenon on MNIST under Randomized Attacks

In one repetitive test of GAA, we observed a fluctuated test result on MNIST under randomized attacks with \( p = 0.5 \), \( q = 5 \), and initially \( \beta = 26/50 \). This phenomenon is reported in Figure 12. Through a larger number of repetitive experiments, we observed this phenomenon only on MNIST but not on other benchmarks. We clarify that this is not a common case in repetitive tests, and we report it here to help readers understand GAA's behavior more thoroughly.

**Figure 12:** An observed fluctuated run of GAA defense on MNIST under the randomized attack: (a) its learning curve and (b) its policy curves.

From Figure 12, the policy curve of GAA is more unstable than in other cases, indicating that GAA's credit on each worker fluctuates significantly. We hypothesize that this is due to the low complexity of the MNIST task, making the reward from the workers' gradients less distinguishable. To validate this, we plot the distribution of rewards (i.e., relative loss decrease) yielded by benign and Byzantine workers on each benchmark.

**Figure 13:** Distribution of rewards from benign workers and from randomized Byzantine workers on MNIST and CIFAR-10.

On CIFAR-10, the Byzantine worker always yields zero reward, which is highly divergent from the benign worker. On MNIST, the Byzantine and benign workers yield rewards that follow similar distributions, making it difficult for GAA to distinguish between them. The Byzantine worker tends to yield rewards that distribute in a slightly wider range than the benign one, contributing to the instability in GAA's learning curve on MNIST. This is supported by the MNIST case under static Byzantine attacks with ratios over 0.5 & 0.7, where baseline methods performed slightly better than random-guess, unlike on other datasets. This suggests that the model on MNIST still learns from even incorrect gradients.