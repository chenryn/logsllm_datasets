Yu. Communication efﬁcient distributed machine learn-
ing with the parameter server. In NeurIPS, 2014.
[42] Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji Liu.
Asynchronous parallel stochastic gradient for nonconvex
optimization. In NeurIPS, 2015.
[43] H Brendan McMahan, Eider Moore, Daniel Ramage,
Seth Hampson, et al. Communication-efﬁcient learn-
ing of deep networks from decentralized data. ArXiv,
1602.05629.
[44] Luca Melis, Congzheng Song, Emiliano De Cristofaro,
and Vitaly Shmatikov. Exploiting unintended feature
leakage in collaborative learning. In S & P, 2019.
[45] Jelena Mirkovic and Peter Reiher. A taxonomy of ddos
attack and ddos defense mechanisms. ACM SIGCOMM
Computer Communication Review, 2004.
[46] Fabio Pasqualetti, Antonio Bicchi, and Francesco Bullo.
Consensus computation in unreliable networks: A sys-
tem theoretic approach. IEEE Transactions on Auto-
matic Control, 2010.
[47] Herbert Robbins and Sutton Monro. A stochastic ap-
proximation method. In Herbert Robbins Selected Pa-
pers, pages 102–109. 1985.
[48] Peter J Rousseeuw. Multivariate estimation with high
breakdown point. Mathematical statistics and applica-
tions, 1985.
[49] Ahmed Salem, Apratim Bhattacharyya, Michael Backes,
Mario Fritz, and Yang Zhang. Updates-leak: Data set
inference and reconstruction attacks in online learning.
ArXiv, 1904.01067.
[50] Ohad Shamir, Nati
Srebro, and Tong Zhang.
optimization
Communication-efﬁcient
using an approximate newton-type method. In ICML,
2014.
distributed
[51] Reza Shokri, Marco Stronati, Congzheng Song, and Vi-
taly Shmatikov. Membership inference attacks against
machine learning models. S & P, 2017.
[52] David Silver, Aja Huang, Chris J Maddison, Arthur
Guez, Laurent Sifre, George Van Den Driessche, Julian
Schrittwieser, Ioannis Antonoglou, Veda Panneershel-
vam, Marc Lanctot, et al. Mastering the game of go with
deep neural networks and tree search. Nature, 2016.
USENIX Association
29th USENIX Security Symposium    1655
[53] Richard S Sutton, Andrew G Barto, Francis Bach, et al.
Reinforcement learning: An introduction. MIT press,
1998.
[54] Richard S Sutton, David A McAllester, Satinder P Singh,
and Yishay Mansour. Policy gradient methods for re-
inforcement learning with function approximation. In
NeurIPS, 2000.
[55] Andrew S Tanenbaum and Maarten Van Steen. Dis-
tributed systems: principles and paradigms. Prentice-
Hall, 2007.
[56] John N. Tsitsiklis, Dimitri P. Bertsekas, and Michael
Athans. Distributed asynchronous deterministic and
stochastic gradient optimization algorithms. American
Control Conference, 1984.
[57] Christopher JCH Watkins and Peter Dayan. Q-learning.
Machine learning, 1992.
[58] Jon Wellner et al. Weak convergence and empirical pro-
cesses: with applications to statistics. Springer Science
& Business Media, 2013.
[59] Paul J Werbos. Backpropagation through time: what it
does and how to do it. Proceedings of the IEEE, 1990.
[60] Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta.
Zeno: Distributed stochastic gradient descent with
suspicion-based fault-tolerance. In ICML, 2018.
[61] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin
Tong. Federated machine learning: Concept and ap-
plications. TIST, 2019.
[62] Dong Yin, Yudong Chen, Kannan Ramchandran, and
Peter Bartlett. Byzantine-robust distributed learning:
Towards optimal statistical rates. ArXiv, 1803.01498.
[63] Sixin Zhang, Anna Choromanska, and Yann LeCun.
Deep learning with elastic averaging sgd. In NeurIPS,
2015.
[64] Martin Zinkevich, Markus Weimer, Lihong Li, and
Alex J Smola. Parallelized stochastic gradient descent.
In NeurIPS, 2010.
A Other Details
A.1 Experimental Environments
All the defenses and experiments are implemented with Torch
[19], which is an open-source software framework for nu-
meric computation and deep learning. All our experiments
are conducted on a Linux server running Ubuntu 16.04, one
AMD Ryzen Threadripper 2990WX 32-core processor and 2
NVIDIA GTX RTX2080 GPUs. We simulate the distributed
learning setting by sequential computation of gradients on
randomly sampled mini-batches.
A.2 Estimate KL-divergence
We design the following procedures to estimate the pairwise
KL-divergence between datasets Di and D j on Healthcare,
which consist of samples of form (x,y) s.t. x ∈ Rn, y ∈ [K],
where n = 1024 and K = 10. Fig. 10 shows the heatmap of
the KL-divergence among the local datasets on each worker
and the full QV set. The empirical KL-divergence is about
0.16 on average.
1. Train one probabilistic model pi(y|x) for each dataset Di
2. Do uniform sampling over [−0.5,0.5]n for N times to form
to a certain error threshold.
a set of points {xk}N
3. Calculate the empirical KL-divergence between the joint
k=1.
distributions that underlie Di, D j by
KL(Di||D j) =
1
K × N
N
∑
k=1
K
∑
c=1
pi(xk|y = c)log
pi(xk|y = c)
p j(xk|y = c)
(3)
Figure 10: Estimated KL-divergence among local datasets
and the prepared validation set on Healthcare.
However, it is true that it is challenging to estimate the
KL-divergence when the QV set is very small. To leverage
the above algorithm for estimation, ideally we require the
knowledge of the distribution where the QV set is sampled,
so that we can estimate the conditional distribution p(y|x)
via learning-based approaches. Intuitively, if QV set contains
more samples, the estimated conditional distribution is less
biased and thus the error of estimating the KL-divergence is
smaller. To be concrete, the minimum requirement for con-
ducting the estimation is, the QV set should contain at least
one sample from each class and thus we can estimate the
conditional distribution with support vector classiﬁer or K-
Nearest Neighbor (KNN). As a future work, it would be a
meaningful direction to study how to guarantee a low KL-
divergence in a distributed learning protocol that may have
privacy requirements [61].
1656    29th USENIX Security Symposium
USENIX Association
A.3 Details of the Benchmark Systems
1. MNIST: The ﬁrst case is training a fully connected feed-
forward neural network for the hand-written digital clas-
siﬁcation task on the MNIST dataset [39], with 50 work-
ers. This public dataset contains 60000 28× 28 images of
10 digits for training and 10000 for testing. Each worker
shares a copy of the training set. The model consists of
784 inputs, 10 outputs with soft-max activation and one
hidden layer with 30 rectiﬁed linear units (ReLu [36]). The
dimension of parameters is 25450.
2. CIFAR-10: The second case is training a ResNet-18
[32] model for the image classiﬁcation on the CIFAR-10
dataset [35] with 50 workers. This dataset contains 60000
28×28× 3 images of 10 classes of objects for training and
10000 for testing. Each worker shares a copy of the train-
ing set. The standard model ResNet-18 has 18 end-to-end
layers and 11173962 learnable parameters in total.
3. Yelp: The third case is training a fully connected feed-
forward neural network for the sentiment classiﬁcation
task (i.e., binary classiﬁcation on positive or negative at-
titude), with 10 workers. Each worker has 20000 1024-
dimension features of Yelp reviews for restaurants in its
local metropolitan area [1]. Each worker corresponds to
one metropolitan area. The features are extracted with a
pretrained Bert language model by Google [23]. We re-
moved a fraction of data samples from each worker to form
the test set, which consists of 1000 samples per class. The
model consists of 1024 inputs, 2 outputs with soft-max
activation and one hidden layer with 10 sigmoid units. The
dimension of parameters is 10272.
4. Healthcare: The fourth case is training a fully connected
feed-forward neural network for predicting the health-
care provider type (10 classes) from textual treatment
descriptions, with 50 workers. Each worker has 20000
1024-dimension Bert features of treatment descriptions
from its local hospitals. Each worker corresponds to a
state. The dataset is prepared from CMS public healthcare
records [2] and we removed a fraction of data samples
from each worker to form the test set, which consists of
1000 samples per class. The model consists of 1024 inputs,
10 outputs with softmax activation and one hidden layer
with 32 sigmoid units. The dimension of parameters is
33130.
the feasible set of parameters of the underlying learning
model) is deﬁned as the maximal 2-norm of an element
θ ∈ Θ. Formally, R = sup{(cid:107)θ(cid:107)2 : θ ∈ Θ}. On Healthcare,
we estimate the 2-norm of the ﬂattened parameter of the
neural network during the learning process to estimate as
the scale of R, which is plotted in Fig. 11(a). The average
value of R is around 11.05.
• Upper bound of gradient norm M: The term M is used to
denote the upper bound of the gradient norm. Formally, M =
supθ∈Θ(cid:107)∇θ ˆf (θ,Dtrain)(cid:107)2. On Healthcare task, we compute
the 2-norm of the gradient submitted by the always-benign
worker during the learning process to estimate the scale of
M, which is plotted in Fig. 11(b). The average value of M
is around 0.36.
• Smoothness factor η: The term η occurs in our assump-
tion that the loss function f is η-smooth. Formally, the
is said to be η-smooth if ∀θ1,θ2 ∈ Θ,
loss function f
| ˆf (θ1,Dtrain)− ˆf (θ2,Dtrain)| ≤ η(cid:107)θ1 − θ2(cid:107)2. We estimate
the empirical scale of α by calculating the expressions at
both sides of the deﬁnition during the learning process,
which is plotted in Fig. 11(c). The average value of η is
around 0.50.
• Size of mini-batch S: The term S denotes the training size
of the mini-batch on which the always-benign worker cal-
culates the gradient. In addition, S is required to be no less
than 1 (i.e., the training set contains at least one sample) or
otherwise the theorem is invalid. On Healthcare, S is set as
256.
• Finally, the max-norm of the loss function (which is im-
plemented as a cross-entropy) is upper bound by the maxi-
mal entropy of the K-class classiﬁcation task (i.e., (cid:107) f(cid:107)∞ ≤
1
K lnK, which is about 0.23 for K = 10 on Healthcare),
while the estimated KL divergence term is about 0.16 from
Fig. 10.
Therefore, on Healthcare under static Byzantine attacks
with β = 0.7,n = 50, the numeric form of Theorem 1 writes
as
f (θt )− f (θ∗) <
2.05√
t
+
16.58
t + 0.13 + O(e−t )
(4)
which produces the curve of the predicted training loss in
Fig. 11(d). Compared with the empirical training loss curve,
we ﬁnd the prediction from Theorem 1 roughly conforms to
GAA’s empirical behavior in this case.
A.4 An Empirical Validation of the Analytic
Results
Without loss of generality, we take Theorem 1 as an exam-
ple. First, we explain the terms R, M, α and S one by one
with more care and give the empirical values on Healthcare
for demonstration. In general, our terminology follows the
conventions in [14], a standard text on optimization theory.
• Diameter R: The diameter R of a parameter space Θ (i.e.,
A.5 Analysis of a Fluctuated Phenomenon on
MNIST under Randomized Attacks
In one repetitive test of GAA, we noticed a ﬂuctuated test
result on MNIST under randomized attacks of p = 0.5,q = 5,
initially β = 26/50, which we report below in Fig. 12. In
fact, through a larger number of repetitive experiments, we
have observed this phenomenon only on MNIST but not on
other three benchmarks. We would like to clarify that this
USENIX Association
29th USENIX Security Symposium    1657
Figure 11: Empirical values of the theoretical terms in Theorem 1, alongside the predicted training loss curves.
phenomenon is not a common case in repetitive tests and we
reported this result here mainly because we think this singular
phenomenon may help the readers understand the behavior
of GAA more thoroughly. Below, we further investigate the
possible causes of this phenomenon.
Figure 12: An observed ﬂuctuated run of GAA defense on
MNIST under the randomized attack: (a) its learning curve
and (b) its policy curves.
As we can see from Fig. 12, the policy curve of GAA is
more unstable than that in other cases, which in other words
means GAA’s credit on each worker ﬂuctuates a lot. This phe-
nomenon indicates that GAA somehow could not recognize
the always benign worker in this situation. As a hypothesis,
we speculate the reason as the low complexity of the MNIST
task [17, 40, 49], which makes the reward from the workers’
gradient on MNIST is not as distinguishable as in other cases.
To validate this point, we plot the distribution of the rewards
(i.e., the relative loss decrease) yielded by the benign workers
and the randomized Byzantine workers on each benchmark
as follows.
In detail, we set the worker number as 2 and set their roles
respectively as benign and Byzantine with the RF tampering
algorithm. We execute the classical distributed learning pro-
tocol for 10 epochs over the corresponding training set and
collect the yielded reward (calculated on the quasi-validation
Figure 13: Distribution of rewards from benign workers and
from randomized Byzantine workers on MNIST and CIFAR-
10.
set of the same settings in Section 5.1) respectively from the
benign and Byzantine workers for every 1k iterations. We
then plot the histogram of rewards on MNIST and CIFAR-10
in Fig. 13.
As we can see from Fig. 13, on CIFAR-10 the Byzantine
worker always yields zero reward, which is highly divergent
from that of the benign worker. Differently, on MNIST the
Byzantine worker and the benign worker yield rewards that
follow similar distributions, which thus may bring difﬁculties
for GAA to distinguish one from the other. A noticeable point
is the Byzantine worker tends to yield rewards that distribute
in a slightly wider range than the benign one, which could
be another cause of the instability in GAA’s learning curve
on MNIST. This speculation is also supported by the MNIST
case under static Byzantine attacks of ratio over 0.5 & 0.7
(in Fig. 4), where the baseline methods were observed to
perform slightly stronger than the random-guess, while on
other datasets they did not. This phenomenon suggests that the
model on MNIST still learns something from even incorrect
gradients.
1658    29th USENIX Security Symposium
USENIX Association