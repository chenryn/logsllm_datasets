Bernard, Patrick Perez, and Christian Theobalt. 2017. Mofa: Model-based deep
convolutional face autoencoder for unsupervised monocular reconstruction. In
Proceedings of the IEEE ICCV. 1274–1283.
[48] Shruti Tople, Karan Grover, Shweta Shinde, Ranjita Bhagwan, and Ramachandran
Ramjee. 2018. Privado: Practical and secure DNN inference. arXiv preprint
arXiv:1810.00602 (2018).
[49] Florian Tramer and Dan Boneh. 2018. Slalom: Fast, Verifiable and Private Execu-
tion of Neural Networks in Trusted Hardware. arXiv preprint arXiv:1806.03287
(2018).
[50] Di Wang, Minwei Ye, and Jinhui Xu. 2017. Differentially private empirical risk
minimization revisited: Faster and more general. In Advances in Neural Informa-
tion Processing Systems. 2722–2731.
[51] Honggang Wang, Raghu Pasupathy, and Bruce W Schmeiser. 2013.
Integer-
ordered simulation optimization using R-SPLINE: Retrospective search with
piecewise-linear interpolation and neighborhood enumeration. ACM Transactions
on Modeling and Computer Simulation 23, 3 (2013), 1–24.
[52] Xi Wu, Fengan Li, Arun Kumar, Kamalika Chaudhuri, Somesh Jha, and Jeffrey
Naughton. 2017. Bolt-on differential privacy for scalable stochastic gradient
descent-based analytics. In Proceedings of the ACM International Conference on
Management of Data. 1307–1322.
[53] Pengtao Xie, Misha Bilenko, Tom Finley, Ran Gilad-Bachrach, Kristin Lauter, and
Michael Naehrig. 2014. Crypto-nets: Neural networks over encrypted data. arXiv
preprint arXiv:1412.6181 (2014).
[54] Guowen Xu, Hongwei Li, Yuanshun Dai, Kan Yang, and Xiaodong Lin. 2019. En-
abling Efficient and Geometric Range Query with Access Control over Encrypted
Spatial Data. IEEE Transactions on Information Forensics and Security 14, 4 (2019),
870–885.
[55] Guowen Xu, Hongwei Li, Sen Liu, Mi Wen, and Rongxing Lu. 2019. Efficient and
Privacy-preserving Truth Discovery in Mobile Crowd Sensing Systems. IEEE
Transactions on Vehicular Technology 68, 4 (2019), 3854–3865.
[56] Guowen Xu, Hongwei Li, Sen Liu, Kan Yang, and Xiaodong Lin. 2019. VerifyNet:
IEEE Transactions on Information
Secure and Verifiable Federated Learning.
Forensics and Security (2019).
[57] Guowen Xu, Hongwei Li, and Rongxing Lu. 2018. POSTER:Practical and Privacy-
Aware Truth Discovery in Mobile Crowd Sensing Systems. In Proceedings of ACM
CCS. 2312–2314.
[58] G. Xu, H. Li, H. Ren, X. Lin, and X. S. Shen. 2020. DNA Similarity Search with
Access Control over Encrypted Cloud Data. IEEE Transactions on Cloud Computing
(2020).
[59] G. Xu, H. Li, H. Ren, K. Yang, and R. H. Deng. 2019. Data Security Issues in Deep
Learning: Attacks, Countermeasures and Opportunities. IEEE Communications
Magazine 57, 11 (2019), 116–123.
[60] G. Xu, H. Li, Y. Zhang, S. Xu, J. Ning, and R. Deng. 2020. Privacy-Preserving
Federated Deep Learning with Irregular Users. IEEE Transactions on Dependable
and Secure Computing (2020). https://doi.org/10.1109/TDSC.2020.3005909
[61] Masashi Yamane and Keiichi Iwamura. 2020. Secure and Efficient Outsourcing of
Matrix Multiplication based on Secret Sharing Scheme using only One Server. In
Proceedings of the IEEE CCNC. IEEE, 1–6.
[62] L Yu, L Liu, C Pu, M E Gursoy, and S Truex. 2019. Differentially Private Model
Publishing for Deep Learning. In proceedings of the IEEE Security and Privacy.
309–326.
[63] Ghodsi Zahra, Gu Tianyu, and Garg Siddharth. 2017. Safetynets: Verifiable
execution of deep neural networks on an untrusted cloud. In Advances in Neural
Information Processing Systems. 4672–4681.
[64] Zhikun Zhang, Tianhao Wang, Ninghui Li, Shibo He, and Jiming Chen. 2018.
Calm: Consistent adaptive local marginal for marginal release under local differ-
ential privacy. In Proceedings of the ACM CCS. 212–229.
[65] Hongchao Zhou and Gregory Wornell. 2014. Efficient homomorphic encryption
on integer vectors and its applications. In Information Theory and Applications
Workshop. IEEE, 1–9.
[66] Martin Zinkevich. 2003. Online convex programming and generalized infinitesi-
mal gradient ascent. In Proceedings of the ICML. 928–936.
794ACSAC 2020, December 7–11, 2020, Austin, USA
Guowen Xu et al.
APPENDIX
1.1 Proof of Theorem 1
Proof. Without loss of generality, we use [0, 1] to replace the closed
interval [a, b]. Then, we define the following mapping:
Bn : X → Y
M(x) (cid:55)→ Bn(M, x) =
k =n
k =0
M( k
n
n x k(1 − x)n−k
)Ck
where X represents the collection of all continuous functions, and
Y represents the entire set of polynomials. Bn(M, x) is a Bernstein
polynomial, which is the image of M ∈ X under the mapping
Bn. From the definition, we can derive that Bn has the following
properties.
(1) Bn is a linear map. For any functions M, G ∈ X and α, β ∈ R, we
have Bn(αM + βG, x) = αBn(M, x) + βBn(G, x).
(2) Bn is monotonic. For any functions M, G ∈ X, if M(t) ≤ G(t) is
established for every t ∈ [0, 1], we have Bn(M, x) ≤ Bn(G, x) is
established for every x ∈ [0, 1].
Based on above properties, we claim that any continuous function
can be approximated by the Bernstein polynomial with any given
error bound ϵ. For simplicity, here we take a continuous functions
M(t) = (t − s)2 as an example to verify our statement, where s is a
constant. Specifically, the image of (t − s)2 under the mapping Bn
is as follows.
Bn((t − s)2, x) = Bn(t 2, x) − 2sBn(t, x) + s2Bn(1, x)
(7)
where
Bn(1, x) =
Bn(t, x) =
Bn(t 2, x) =
k =0
k =n
k =n
k =n
k =0
k =0
n x k(1 − x)n−k = 1
Ck
k
n
n x k(1 − x)n−k = x[x + (1 − x)]n = x
Ck
n x k(1 − x)n−k = n − 1
x 2 + x
n
k2
n2 Ck
n
We know that the function M(t) is bounded since it is continuous
in the interval [0, 1]. Therefore, for any element t ∈ [0, 1], there is
a positive number D satisfying |M(t)| ≤ D. Further, based on the
Cantor Theorem, for any given number ϵ > 0 and t, s ∈ [0, 1], we
have |M(t) − M(s)| ≤ ϵ
2 , where |t − s| ≤ δ and δ > 0. Contrarily, if
|t − s| ≥ δ, we have
|M(t) − M(s)| ≤ 2D ≤ 2D
Therefore, for any t, s ∈ [0, 1], we have
δ 2 (t − s)2
− ϵ
2
− 2D
δ 2 (t − s)2 ≤ M(t) − M(s) ≤ ϵ
(8)
Based on the Eqn.(1), the above formulas can be further decom-
posed as follows.
δ 2 (t − s)2
2 + 2D
n
δ 2 [ x − x 2
− 2D
− ϵ
2
≤ Bn(M, x) − f (s)
δ 2 [ x − x 2
2 + 2D
≤ ϵ
n
− (x − s)2]
− (x − s)2]
(9)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)k =n
k =0
M( k
n
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)k =n
k =0
M( k
n
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ ϵ
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ ϵ
Let s = x. Because x(1 − x)  N , N = ⌈ D
ϵδ 2 ⌉ , we have
n x k(1 − x)n−k − M(x)
)Ck
(10)
(11)
Least square approximation algorithm
Hence, we prove that the Bernstein polynomial can be used to
approximate any continuous function with given error bound ϵ.
1.2
Given a continuous function M(x) on the finite interval [a, b], the
goal of Least Square Approximation Algorithm is to find an optimal
polynomial p(x) that satisfies the following definition.
ρ(x)[M(x) − s(x)]2dx
(12)
where {ψn(x)|n = 1,· · · N} is a set of linearly independent poly-
nomial function, and ρ(x) is the weight function. s(x) can be repre-
sented as s(x) = a0ψ0(x) + a1ψ1(x) + · · · + anψn(x). Then, we can
get the p(x) by solving the following multivariate function.
ρ(x)[M(x) − ajψj(x)]2dx
I(a0, a1, · · · , an) = min
|M(x) − p(x)|2
2 = min
s(x)∈ψ
∫ b
∫ b
j =n
(13)
Based on the necessary condition of solving extreme value in
multivariate function, let ∂I
∂ak
ρ(x)[M(x) − ajψj(x)]ψk(x)dx = 0
j =0
= 0, k = 0, 1, 2,· · · , N , we have
∫ b
j =n
(14)
= 2
a
a
∂I
∂ak
a
j =0
j =0
Based on the definition of inner product, we have
aj(ψk(x), ψj(x)) − (M(x), ψk(x)) = 0
where (ψk(x),ψj(x)) =∫ b
=∫ b
j =n
a ρ(x)M(x)ψk(x)dx, respectively. Therefore, we have
aj(ψk(x), ψj(x)) = (M(x), ψk(x)), k = (0, 1, 2, · · · , N)
a ρ(x)ψk(x)ψj(x)dx and (M(x),ψk(x))
(15)
(16)
j =n
j =0
· · ·
(cid:169)(cid:173)(cid:173)(cid:171)
(cid:169)(cid:173)(cid:173)(cid:171)
(cid:170)(cid:174)(cid:174)(cid:172)
· · ·
· · ·
· · ·
· · ·
(ψ0, ψ0)
(ψ1, ψ0)
(ψN , ψ0)
(ψ0, ψ1)
(ψ1, ψ1)
(ψN , ψ1)
(ψ0, ψN )
(ψ1, ψN )
(ψN , ψN )
The above equations can be extended to the following system of
equations.
(cid:170)(cid:174)(cid:174)(cid:172)
Since {ψn(x)|n = 1,· · · N} is a set of linearly independent polyno-
mial function, above equation set has unique solution(a∗
1,· · · a∗
N ).
Hence, we can calculate the least square approximation polynomial
∫ b
p(x) = a∗
1ψ1(x) + · · · a∗
N ψN (x) satisfying the definition
(17)
2 = min
s(x)∈ψ
0ψ0(x) + a∗
|M(x) − p(x)|2
(ψ0, M(x))
(ψ1, M(x))
(ψn , M(x))
0, a∗
ρ(x)[M(x) − s(x)]2dx
(cid:170)(cid:174)(cid:174)(cid:172) =(cid:169)(cid:173)(cid:173)(cid:171)
a0
a1· · ·
an
· · ·
· · ·
a
795Secure and Verifiable Inference in Deep Neural Networks
ACSAC 2020, December 7–11, 2020, Austin, USA
Table 4: Experimental Environment for Detection Accuracy under Different Attacks
Dataset
Task
NNTA
VGG-Face[47]
Face classification
TPA
MCA
AWM
GTSRB[31]
Traffic sign classification
CIFAR-10[46]
Image classification
AT&T[10]
Face classification
Model
VGG-16
CNN
CNN
MLP
Total layers
Convolution
layers
Fully connected
layers
18
9
9
1
14
7
7
0
4
2
2
1
Attack technique
Modify parameters
with trigger
Retrain model with
data poisoning
Compress the
original model
Arbitrary
modification
1.3
Experimental configuration and results
Table 9: Detection Accuracy under AWM
Table 5: Datasets selected for inference accuracy
datasets
Breast tissues
Crab
Ovarian
Wine