the network-wide demand collection to a central point [13, 20, 26], a
centralized scheduling algorithm [4, 21], schedule distribution [20],
and network-wide synchronization [13, 20, 26, 32].
A recent free-space optical proposal, ProjecToR [15], sidesteps
this complexity by foregoing global throughput maximization, in-
stead minimizing latency while avoiding starvation. Each ProjecToR
switch has its own dedicated switching elements, allowing each
ToR to independently provision free-space links based on only that
ToR’s local demand. It remains only to ensure that multiple ToRs
do not choose free-space paths that conflict at a single receiver,
which would cause collisions. Thus, ProjecToR’s control plane is
inherently simpler than the other approaches in Table 1, and can
be implemented in a distributed way. However, the resulting de-
sign does not necessarily maximize network throughput. Moreover,
ProjecToR’s reliance on free-space optics raises a myriad of other
practical concerns (e.g., robustness to dust, vibration, etc.) that do
not arise in wired fiber-optic networks.
3 DESIGN OVERVIEW
Similar to prior proposals based on optical circuit switches, Rotor-
Net employs traditional, packet-switched ToR switches to connect
end hosts to the fabric. The ToRs are connected optically to a set of
custom OCSes, which we call Rotor switches, which collectively
provide connectivity to each of the other ToRs in the network.
If desired, ToRs can be further connected to an electrical packet-
switched fabric as well to form a hybrid network, but we defer
discussion for the time being to focus on the optical network. Im-
portantly, the set of Rotor switches does not provide continuous
connectivity between all pairs of ToRs; instead, they implement
a schedule of connectivity patterns that, in total, provides a di-
rect connection between any pair of ToRs within a specified time
interval.
3.1 Open-loop switching
Unlike prior optically-switched network proposals, the configura-
tion of the Rotor switches is not driven by network traffic condi-
tions, either locally or globally. In RotorNet, the Rotor switches
independently cycle through a fixed, pre-determined pattern of con-
nectivity in a round-robin manner, irrespective of instantaneous
traffic demands. We choose this time-sequence of Rotor switch con-
figurations (each a one-to-one matching of input to output ports, or
simply a “matching”) so that each endpoint (i.e., ToR) is provided a
direct connection to every other endpoint within one full cycle of
matchings.
Figure 3 illustrates this approach, showing two full cycles of
three matchings. We highlight the changing connectivity of the
top port as it cycles through matchings connecting it to the 2nd,
3rd, and 4th ports across time. Because this approach decouples the
Demand collection N VoQs . . . . . . N Output ports Electronic Packet Switch  (EPS) Sync. Scheduling (iSLIP) Crossbar Demand collection ToR . . . Datacenter network Scheduling (Solstice / Eclipse) ToR ToR Service network ToR . . . ToR ToR Crossbar TDMA Sync. N Receivers N Senders (a) (b) OCS SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
W. M. Mellette et al.
Figure 3: Rotor switches move through a static, round-
robin set of configurations, or matchings, spending an equal
amount of time in each matching.
switch state from the traffic in the network, it requires no demand
collection, no switch scheduling algorithm, and no network-wide
synchronization. The switches simply run “open loop.”
As a first cut, each NR-port Rotor switch could repeatedly cycle
through all NR! possible matchings, corresponding to the full set of
potential matchings offered by a crossbar switch. However, it would
take an infeasibly long time to complete this cycle for large NR, and
we could not guarantee that a given connection is implemented
within any reasonable amount of time. Further, only (NR −1) match-
ings are necessary to ensure connectivity between all NR ToRs. An
example set of these matchings is shown in Figure 4(a) for NR = 8.
Using these (NR − 1) matchings, we can guarantee that each ToR
is connected to every other ToR within one matching cycle. This
functionality is similar to a rotor device [2].
Still, for networks with many ToRs, cycling through even(NR−1)
matchings may still take too long. Instead, as shown in Figure 4(b),
we distribute the (NR − 1) matchings among Nsw parallel Rotor
switches, speeding up the matching cycle time by a factor of Nsw .
We show in Section 4.2 that for a network of 10s of thousands of
servers, we can cycle through as few as 16 matchings per switch
with Nsw = 128. In this configuration, each Rotor switch only
provides partial connectivity between the ToRs. Taken together,
however, the complete set of switches restores full connectivity in
the network. We discuss the implications of our design in Section 4,
and explain how our design maintains connectivity even in the
presence of switch failure in Section 5.3.
3.2 One-hop direct forwarding
Given the baseline round-robin connectivity provided by Rotor
switches, each ToR must decide how to route traffic over the net-
work. The simplest approach is for ToRs to send data only along
one-hop, direct paths to each destination, resulting in equal band-
width between each source-destination pair. For uniform traffic,
this behavior results in throughput saturating the network’s bisec-
tion bandwidth (minus the switch duty cycle), and is inherently
starvation free. However, for skewed traffic patterns, this approach
does not take advantage of slack network capacity when some ToRs
are idle, wasting potentially significant amounts of bandwidth.
3.3 Two-hop indirect forwarding
To improve throughput in skewed traffic conditions, we rely on
the classic and well-studied technique of indirection. Like Valiant’s
routing method [30], we allow traffic to pass through intermediate
Figure 4: (a) A Rotor switch cycles through (NR − 1) match-
ings to provide full connectivity between racks. (b) Phys-
ically, these matchings are distributed among Nsw Rotor
switches which, taken together, provide full connectivity.
endpoints, which subsequently forward traffic to the final destina-
tion. Chang et al. showed that Valiant’s method, when coupled with
two stages of round-robin switches, yields 100% throughput for
arbitrary input traffic1 [5]. Shrivastav et al. are investigating an ap-
proach similar to Chang’s applied to rack-scale interconnects [28].
RotorNet is a datacenter-wide fabric, and we leverage the large
number of ToR switch uplinks to extend Chang’s approach, paral-
lelizing it across a number of Rotor switches. For large networks,
such as the example network in Section 4.2, this modification re-
duces the matching cycle time, and thus the delivery time of traffic,
by more than 100× compared to sequentially cycling through all
matching patterns. RotorNet routes traffic through the same single-
stage fabric twice, and a straightforward implementation would
reduce throughput by at most a factor of two (as half the network
bandwidth would now be consumed by indirect traffic), yielding
half bisection bandwidth for arbitrary input traffic. We argue that
this trade-off is justified by the fact that raw network bandwidth
is plentiful in optical networks. Moreover, through careful exten-
sions to the basic Valiant load balancing approach (described in
Section 5 and evaluated in Section 7), we are able to recapture a
significant amount of the theoretical throughput loss in practice,
meaning the factor-of-two reduction in throughput is a worst-case,
not common-case, trade off. Indirection requires buffering traffic
within the network, but outside of the optical Rotor switches them-
selves, since they cannot buffer light. Indirect traffic is buffered on
a per-rack basis, either at the ToR switch or in end-host memory
1Subject to the minor technical condition that input traffic can be modeled as a sta-
tionary and weakly mixing stochastic process.
M1 M2 M3 M1 M2 M3 t1 t2 t3 t4 t5 t6 . . . Time Nsw Rotor switches, … M1 t1 (a) M1 M2 M3 M4 M5 M6 M7 t2 t3 M2 M3 M4 M5 - M6 M7 - t1 t2 t3 t1 t2 t3 NP packet switches ToR 1 ToR 2 ToR 3 ToR 4 Rack 1 Rack 2 Rack 3 Rack 4 … ToR NR Rack NR … Nup = Nsw + NP uplinks t1 t2 t3 t4 t5 t6 t7 Nm = NR – 1 matchings  ... of (NR)! possible Rotor switch (b) (cid:11)(cid:12)1/mRswNNN(cid:32)(cid:170)(cid:16)(cid:186)(cid:171)(cid:187)RotorNet: A Scalable, Low-complexity, Optical Datacenter Network
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
accessible to the ToR through RDMA. Buffering data on end hosts,
similar to Liu et al. [20], is likely the simplest implementation as end
hosts are already required to buffer direct traffic while waiting for
the appropriate matching to come up in the schedule. We quantify
the additional buffering requirements imposed by indirection in
Section 7.5.
4 PRACTICAL CONSIDERATIONS
In this section, we explain how the design of RotorNet allows it to
scale to modern datacenter sizes of tens of thousands of servers
and thousands of ToRs cost-effectively. We provide a comparison
to existing Fat Tree topologies, and address manufacturing and
deployment concerns.
4.1 Scalability
4.1.1 Achieving high port count. The key to RotorNet’s scala-
bility is the use of Rotor switches, which are fundamentally more
scalable than traditional optical circuit switches. In particular, a
traditional N -port OCS implements a crossbar, meaning it can be
configured to any of N ! matching patterns. This flexibility limits
the switching speed and radix of a MEMS-based OCS because the
physical requirements of each micro-mirror switching element are
coupled to the switch radix [24]. As a result, commercially available
OCSes have radices on the order of 100 ports and reconfiguration
times of 10s of milliseconds.
Connecting thousands of ToRs together with conventional OC-
Ses requires those OCSes be cascaded in a multi-stage optical topol-
ogy, which introduces significant signal attenuation. Higher signal
attenuation, in turn, requires higher sensitivity optical transceivers
or optical amplification, which would almost certainly negate any
cost savings gained by using OCSes. Similarly, designs that employ
O(100)-port OCSes to connect pods (instead of racks) replace less
of the electronic network, limiting their cost effectiveness. Finally,
even if the OCS radix could be increased or signal loss reduced,
commercial crossbar OCSes still reconfigure much too slowly to
support datacenter traffic dynamics.
In Rotor switches, switching elements need only differentiate
between the number of matchings—as opposed to the number of
ports in the crossbar—making them fundamentally more scalable.
For example, in a 2,048-rack RotorNet, the number of matchings
in each Rotor switch is two-orders-of-magnitude smaller than the
switch radix. We show in Section 6.1 that Rotor switches with 2,048
ports can achieve a reconfiguration time of 20 µs using existing
technology. Thus, Rotor switches can connect 1000s of racks in the
datacenter, and achieve a response time on the same order as other
state-of-the-art approaches [12, 15].
4.1.2 Reducing cycle time through sharding. In a circuit-switched
network, connecting the endpoints is only half the issue; the other
half is ensuring timely service. This involves managing the delay
due to circuit reconfigurations, and for RotorNet, waiting until
connectivity is established through one of the Rotor matchings.
The time it takes to cycle through all matchings in RotorNet
is a critical metric that gates how much time passes before all
endpoints have an opportunity to communicate with each other. To
speed up the rate at which we cycle through matchings, we employ
a different subset of matching patterns in each Rotor switch, as
Number Rotor switches (Nsw = Nup):
Number matchings / switch (Nm):
Cycle time (µs) at duty cycle = 0.9:
Cycle time (µs) at duty cycle = 0.75:
512
4
800
320
256
8
1,600
640
128
16
3,200
1,280
Table 2: Trade-offs between the main parameters in a
2,048-rack RotorNet, assuming a 20-µs reconfiguration de-
lay. For reasonable values of Nsw and duty cycles, the entire
network-wide cycle time can remain on the order of 1 ms.
4.1.3
shown in Figure 4(b). The number of matchings implemented by
each switch is Nm = ⌈(NR − 1)/Nsw⌉.
Table 2 shows the cycle times for a 2,048-rack RotorNet using var-
ious numbers of Rotor switches and duty cycles. The duty cycle is
the fraction of time traffic can be sent over the network, accounting
for the time the OCS spends reconfiguring. For a given reconfigura-
tion speed and number of matchings, a higher duty cycle leads to a
longer matching cycle. For the configurations shown in Table 2—
including the particular realization described below—cycle times
on the order of 1 ms are possible. Such delays are shorter than disk
access times, and would thus serve disk-based data transfers well.
Supporting low-latency traffic. For flows with latency re-
quirements smaller than the cycle time (≈1 ms), we rely on pre-
viously demonstrated hybrid approaches [13, 20, 32], where low-
latency data is sent over a heavily over-subscribed packet-switched
network, and all other traffic traverses RotorNet. To achieve a hy-
brid architecture, our design simply faces a fraction of the upward-
facing ToR ports toward a packet-switched network, as shown in
Figure 4(b). Applications are required to choose which traffic is sent
over the packet-switched network, setting QoS bits in the packet
header which then trigger match-action rules in the ToR. This de-
sign supports a certain percentage of low-latency bandwidth, for
example 10–20%. The remaining 80–90% transits RotorNet.
We note that RotorNet’s control plane is self-contained—no part
of it relies on the existence or operation of a packet-switched net-
work. In particular, no control messages or configurations are sent
over the packet-switched fabric. Thus, RotorNet management is en-
tirely separate from any packet-switched network used to support
a hybrid design.
4.2 An example network
As a concrete comparison point, consider a hypothetical 65,536-
end-host network with 400-Gb/s links to each end host (a 26-Pb/s
network). We design the network using k = 64-port electronic
switches, each with an aggregate bandwidth of 25.6 Tb/s. Such
switches are projected to be available in the next few years. While a
Fat Tree treats each ToR port as a single 400-Gb/s link, in RotorNet
we split the bandwidth of each ToR port, creating 128 100-Gb/s
links which connect to a set of 128 Rotor switches. Commercial ToR
switches today offer this same functionality—a single 100-Gb/s port
can be broken into four logical 25-Gb/s links, providing a larger
number of lower-bandwidth ports. While the upward-facing ToR
ports in RotorNet are logically distinct, we package these ports into
32 400-Gb/s transceivers, where each transceiver has 4 transmit
fibers and 4 receive fibers in a ribbon cable, with each fiber carrying
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
W. M. Mellette et al.
Network
architecture
1:1 Fat Tree
3:1 Fat Tree
RotorNet, 10% EPS
RotorNet, 20% EPS
# EPS
[# ports]
5.1 k [328 k]
2.6 k [168 k]
2.3 k [149 k]
2.5 k [162 k]
#TRX
262 k
103 k
84 k
96 k
# Rotors
[# ports]