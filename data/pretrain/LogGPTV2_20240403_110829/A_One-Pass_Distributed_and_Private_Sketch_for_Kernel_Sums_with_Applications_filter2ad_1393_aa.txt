title:A One-Pass Distributed and Private Sketch for Kernel Sums with Applications
to Machine Learning at Scale
author:Benjamin Coleman and
Anshumali Shrivastava
A One-Pass Distributed and Private Sketch for Kernel Sums
with Applications to Machine Learning at Scale
Anshumali Shrivastava
Benjamin Coleman
PI:EMAIL
Rice University
Houston, Texas, USA
PI:EMAIL
Rice University
Houston, Texas, USA
ABSTRACT
Differential privacy is a compelling privacy definition that explains
the privacy-utility tradeoff via formal, provable guarantees. In ma-
chine learning, we often wish to release a function over a dataset
while preserving differential privacy. Although there are general
algorithms to solve this problem for any function, such methods
can require hours to days to run on moderately sized datasets. As a
result, most private algorithms address task-dependent functions
for specific applications. In this work, we propose a general pur-
pose private sketch, or small summary of the dataset, that supports
machine learning tasks such as regression, classification, density
estimation, and more. Our sketch is ideal for large-scale distributed
settings because it is simple to implement, mergeable, and can be
created with a one-pass streaming algorithm. At the heart of our
proposal is the reduction of many machine learning objectives to
kernel sums. Our sketch estimates these sums using randomized
contingency tables that are indexed with locality-sensitive hashing.
Existing alternatives for kernel sum estimation scale poorly, often
exponentially slower with an increase in dimensions. In contrast,
our sketch can quickly run on large high-dimensional datasets, such
as the 65 million node Friendster graph, in a single pass that takes
less than 20 minutes, which is otherwise infeasible with any known
alternative. Exhaustive experiments show that the privacy-utility
tradeoff of our method is competitive with existing algorithms, but
at an order-of-magnitude smaller computational cost. We expect
that our sketch will be practically useful for differential privacy in
distributed, large-scale machine learning settings.
CCS CONCEPTS
• Security and privacy → Domain-specific security and pri-
vacy architectures; • Theory of computation → Sketching
and sampling.
KEYWORDS
differential privacy; scaling; sketching; machine learning; locality-
sensitive hashing
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea
© 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-8454-4/21/11...$15.00
https://doi.org/10.1145/3460120.3485255
ACM Reference Format:
Benjamin Coleman and Anshumali Shrivastava. 2021. A One-Pass Dis-
tributed and Private Sketch for Kernel Sums with Applications to Ma-
chine Learning at Scale. In Proceedings of the 2021 ACM SIGSAC Confer-
ence on Computer and Communications Security (CCS ’21), November 15–19,
2021, Virtual Event, Republic of Korea. ACM, New York, NY, USA, 14 pages.
https://doi.org/10.1145/3460120.3485255
1 INTRODUCTION
Large-scale data management is an integral component of the mod-
ern data analysis pipeline. The success of machine learning and data
mining algorithms critically depend on the quantity and quality of
the input data. Although vast amounts of data are available, the in-
formation is often of a personal or sensitive nature. Models can leak
substantial information about individual participants, even if we
only release predictions, outputs or descriptive statistics [19, 20, 35].
Privacy is, therefore, an important design factor when designing
systems that analyze and store sensitive data.
To protect against diverse and sophisticated attacks, ϵ-differential
privacy has emerged as a theoretically rigorous definition of privacy
with robust guarantees [15]. Informally, an algorithm is differen-
tially private if the inclusion (or exclusion) of any specific data
record cannot substantially alter the output. Differentially private
algorithms are useful because they guarantee a minimum level of
protection, no matter what side information an attacker might have.
The quality of the protection is measured using the privacy budget
ϵ. A small value of ϵ implies that the algorithm can leak very little
information about any individual record in the database. In machine
learning, most tasks can be reduced to optimizing specific functions
over the dataset. As a result, it is critically important to evaluate
and optimize such functions while preserving ϵ-differential privacy.
Private Function Release: In this paper, we consider the task
of privately releasing a function fD(q) that applies a pairwise op-
eration k(x, q) to a query and every record in a dataset. Given a
query q a dataset D = {x1, ...xN }, our objective is to compute an
ϵ-differentially private version of the sum:
k(x, q)
(1)
Although one could directly evaluate fD(q) and release the result
with the exponential mechanism, this would cause each query to
leak additional information about the dataset. With this method,
we can only evaluate fD(q) a finite number of times before the
privacy budget runs out. In practice, this is undesirable because
we often require many interactions with the dataset for machine
learning or exploratory analysis.
fD(q) = 
This limitation leads us to consider the private function release
problem. Rather than incrementally consume the privacy budget to
x ∈D
Session 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3252interactively answer queries, we instead use the privacy budget to
release a private summary SD of fD [31]. Since the summarization
algorithm itself is private, queries made using the summary do not
consume any more of the privacy budget. All downstream analysis
tasks are protected by the robustness property of ϵ-differential
privacy, and we may issue an unlimited number of queries (provided
that we use the summary and not the original dataset) [22]. This
problem is related to private database release, but we seek strong
error bounds for all queries1, not just the global minimum as with
empirical risk minimization [8] or a finite set of linear queries as
in [23].
Application to Private Machine Learning: The function re-
lease task is relevant to problems where it is difficult or impossible
to place up-front limits on the number of times we must query
fD(q). This situation often happens when we deploy a machine
learning system that computes fD(q) over a private dataset when
it processes a query. Many useful functions for data analysis take
the form of Equation 1, and the ability to evaluate fD(q) is critical
for a large and important class of statistical models. For example,
when k(x, q) is a kernel function, fD is the kernel density estimate
- a convenient way to approximate the likelihood function in a clas-
sification model and an important quantity for other applications
such as non-i.i.d. federated learning [28]. It is not difficult to find
data analysis pipelines that query a likelihood function each time
they are used. Without private function release, we can only use the
system a finite number of times, rendering it useless in production.
A similar situation arises in exploratory data analysis, where an
analyst might want to visualize a dataset by repeatedly querying
fD. Another application is data modeling, where k(x, q) is a loss
function and fD is an objective we wish to minimize. A practical
method to release fD would allow many existing algorithms for
density estimation, clustering, classification and regression to be
made private. As a result, function release has received a consider-
able amount of theoretical attention.
Practical Limitations: There are elegant, general and powerful
techniques to privately release essentially any function of inter-
est [3, 22, 31, 45]. However, private function release has not yet
been widely adopted in practice because existing methods fail to
scale beyond small, low-dimensional datasets. The practical utility
of function release is plagued by issues such as quadratic runtime
and exponential memory requirements. For instance, many algo-
rithms release fD via function approximation over an interpolation
lattice, but the size of the lattice grows exponentially with dimen-
sions [3]. Our experiments show that even for low-dimensional
data (d = 3), this process can take days. Other methods require
the eigenvalue decomposition of large kernel matrices or hundreds
of Monte Carlo integrations for each query [31, 45]. As a result,
not many methods can scale beyond a few thousand records or
d > 3 dimensions. Although private function release is a promising
technique, existing algorithms do not satisfy the practical demands
of large-scale systems.
Scalable Function Release with Sketches: In this work, we
propose a scalable approach to function release using streaming
algorithms and compressed sketches. For differential privacy to
1By “all queries”, we mean all values of q. There is no polynomial algorithm for all
general queries [40].
have a practical impact in big data settings, we require simple
algorithms that can operate on internet-scale problems in a single
pass through the data [14, 18, 30]. Private sketches are sought
after by practitioners because they can efficiently scale to massive
datasets [38]. We show how to construct a sketch SD that satisfies
these requirements for a large class of functions.
Our function release method is an extension of the RACE sketch,
a recent development in (non-private) data streaming algorithms [12].
RACE sketches consist entirely of a small array of integers (∼4 MB)
that are indexed using hash functions. Our private version inherits
many properties that are critical for large-scale distributed imple-
mentations, such as mergeability and low communication overhead,
while also preserving ϵ-differential privacy. We derive pointwise
error bounds for our approximation to fD which show that RACE
is competitive with existing methods for function release, and we
provide experiments to show that our algorithm has a substantially
smaller computation cost. Our experiments show that RACE easily
scales to datasets with hundreds of dimensions and millions of
entries.
Sketches for Kernel Compositions: In this work, we trade
flexibility for computational efficiency. Our sketch restricts the
choice of fD to a specific class of kernels k(x, q) known as locality
sensitive hash (LSH) kernels. This restriction allows us to obtain
fast streaming algorithms, but not all functions can be expressed
in terms of LSH kernels. So far, the (non-private) sketch literature
has considered only a few specific instances of fD [11, 12]. How-
ever, we argue that RACE is actually capable of performing a more
general set of machine learning tasks. We show how to express
classification, linear regression, kernel density estimation (KDE),
anomaly detection and mode finding in terms of LSH kernel com-
positions. We conduct an exhaustive set of experiments with KDE,
classification and linear regression. Our experiments show that
RACE can cheaply release useful functions for many tasks with a
competitive privacy-utility tradeoff. While other algorithms can
potentially provide better accuracy, RACE is often the only method
capable of running on large, high-dimensional datasets in practice.
Our Contribution: We make the following specific contribu-
tions. First, we propose a private version of the RACE sketch for
scalable function release and prove competitive error bounds on
our estimate of fD(q). We reduce several classes of important ma-
chine learning techniques to LSH kernel sum approximation and
discuss other methods to extend the RACE framework. We con-
duct an exhaustive set of experiments for three applications: den-
sity estimation, regression and classification. Finally, we obtain
an order-of-magnitude speedup for large-scale kernel sum tasks.
In particular, kernel density tasks which take hours for existing
methods [3, 5, 31] can be completed in seconds using the private
RACE sketch.
2 BACKGROUND
We consider a dataset D of N points in Rd. Although our analysis
naturally extends to any metric space, we restrict our attention to
Rd for the sake of presentation.
2.1 Differential Privacy
We use the well-established definition of differential privacy [15].
Session 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3253Method
Bernstein
polynomials [3]
PFDA [31]
MWEM [23]
Trigonometric
polynomials [45]
d
ϵ
β
d
2
ϵ
d +H
d +H
log 1
δ
log 1
δ
Error Bound
(cid:17) H
(cid:16) 1
(cid:113)log 2
(cid:17)1/3
3(cid:16) log N log |Q |
(cid:17) 1
(cid:16) N
1
ϵ N
2d2d +H
C
ϕ
2
N
ϵ
Runtime
O(dN Md)
O(dN
O(dN |Q|)
2)
Comments
M ≥ 2. Memory is also
exponential in d.
C and ϕ are task-dependent
(ϵ, β)-differential privacy
Q is a set of query points. Holds
with probability 1 − 1/poly(|Q|)
1+ d2d +H ) The result holds with probability
− 1
5 N d/2d +K
1 − δ for δ ≥ 10e
Applies only for LSH kernels.
Efficient streaming algorithm.
O(dN
Table 1: Summary of related methods to release the kernel sum fD = D k(x, q) for an N point dataset D in Rd . Unless
This work
otherwise stated, the error is attained with probability 1 − δ and ϵ-differential privacy. We hide constant factors and adjust
results to estimate fD rather than the KDE (N−1
fD(q)) when necessary. H is a kernel smoothness parameter.
O(dN)
log 1
δ
ϵ
2
Definition 2.1. Differential Privacy [15] A randomized function
A is said to provide (ϵ, β)-differential privacy if for all neighboring
databases D and D′ (which differ in at most one element) and all
subsets S in the codomain of A,
Pr[A(D) ∈ S] ≤ eϵ Pr[A(D′) ∈ S] + β
The parameter ϵ is the privacy budget, and it acts as a limit to
the amount of information that A(D) can leak about any individual
element of D. If β > 0, then A(D) could potentially leak more
information, but this only happens with probability up to β. In