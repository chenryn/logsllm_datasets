loss of many packets. Figure 4 presents a bitrate sample of
a connection between iperf on NewtOS and on Linux. We
used tcpdump to capture the trace and Wireshark to analyze
it. Using a single connection allows us to safely capture all
packets to see all lost segments and retransmission. The trace
shows a gap when we injected a fault in the IP server 4s
after the start of the connection. We did not observe any lost
segments and only one retransmission from the sender (due
to a missing ACK and a timeout) which has been already
seen by the receiver. The connection quickly recovered its
original bitrate. As we already mentioned before, due to the
hardware limitations, we have to reset the network card when
IP crashes. This causes the gap as it takes time for the link
to come up again and so the driver. Therefore, all the traces
we inspected after a driver crash look very similar.
A similar sample trace on Figure 5 shows that a packet
ﬁlter (PF) crash is almost not noticeable. Due to our design,
we never lose packets because IP must see a reply from
PF, otherwise it knows that the packet was not processed.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:17:40 UTC from IEEE Xplore.  Restrictions apply. 
 0 200 400 600 800 1000 0 1 2 3 4 5 6 7 8 9 10Bitrate (Mbps)Time (s) 0 100 200 300 400 500 600 700 800 900 1000 0 2 4 6 8 10 12 14 16 18Bitrate (Mbps)Time (s)The trace shows two crashes and immediate recovery to the
original maximal bitrate while recovering a set of 1024 rules.
VII. RELATED WORK
Our work is based on previous research in operating
systems and it blends ideas from other projects with our
own into a new cocktail. Although the idea of microkernel-
based multiserver systems is old, historically they could not
match the performance of the monolithic ones because they
were not able to use the scarce resources efﬁciently to deliver
matching performance. The current multicore hardware helps
to revive the multiserver system idea. In a similar vein,
virtual machines (invented in the 1960s) have recently seen
a renaissance due to new hardware.
Monolithic systems, in their own way, are increasingly
adopting some of the multiserver design principles. Some
drivers and components like ﬁle systems can already run
mostly in user space with kernel support for privileged
execution. In addition, kernel threads are similar to the
independent servers. The scheduler is free to schedule these
threads, both in time and space, as it pleases. The kernel
threads have independent execution context in the privileged
mode and share the same address space to make data sharing
simple, although they require locks for synchronization. Parts
of the networking stack run synchronously when executing
system calls and partly asynchronously in kernel threads,
which may execute on different cores than the application
which uses it, depending on the number and usage of the
cores. Coarse grained locking has signiﬁcant overhead; on
the other hand, ﬁne grained locking is difﬁcult to implement
correctly.
Interestingly, to use the hardware more efﬁciently, the
kernel threads are becoming even more distinct from the
core kernel code; they run on dedicated cores so as not to
collide with the execution of user applications and with each
other. An example is FlexSC’s [39], [40] modiﬁcation of
Linux. It splits the available cores into ones dedicated to run
the kernel and ones to run the applications. In such a setup,
the multithreaded applications can pass requests to the kernel
asynchronously and exception free which reduces contention
on some, still very scarce, resources of each core.
Yet another step towards a true multiserver system is
the IsoStack [37], a modiﬁcation of AIX. Instances of the
whole networking stack run isolated on dedicated cores. This
shows that monolithic systems get a performance boost by
dedicating cores to a particularly heavy task with which the
rest of the system communicates via shared memory queues.
Thus it is certainly a good choice for multiserver systems
which achieve the same merely by pinning a component to a
core without any fundamental design changes. The primary
motivation for these changes is performance and they do
not go as far as NewtOS, where we split the network stack
into multiple servers. In contrast, our primary motivation is
dependability and reliability while the techniques presented in
this paper allow us to also achieve competitive performance.
We are not the ﬁrst to partition the OS in smaller compo-
nents. Variants less extreme than multi-server systems isolate
a smaller set of OS components in user-level processes—
typically the drivers [15], [25]. Barrelﬁsh [5] is a so-called
multikernel, microkernel designed for scalability and diversity,
which can serve as a solid platform for a multiserver system.
We are planning to port our network stack on top or it.
Hypervisors are essentially microkernels which host mul-
tiple isolated systems. Colp et al. [8] show how to adopt
the multiserver design for enhanced security of Xen’s Dom0.
Periodic microreboots and isolation of components reduces
its attack surface.
It is worth mentioning that all the commercial systems
that target safety and security critical embedded systems
are microkernel/multiserver real-time operating systems like
QNX, Integrity or PikeOS. However, all of them are closed-
source proprietary platforms therefore we do not compare to
them. Unlike NewtOS, they target very constrained embedded
environments, whereas we show that the same basic design is
applicable to areas where commodity systems like Windows
or Linux dominate.
VIII. CONCLUSION AND FUTURE WORK
In this paper we present our view on future dependable
operating systems. Our design excludes the kernel from
IPC and promotes asynchronous user space communication
channels. We argue that multiserver systems must distribute
the operating system itself to many cores to eliminate its
overheads. Only such asynchronous multiserver systems, in
which each component can run whenever it needs to, will
perform well while preserving their unique properties of fault
resilience and live-updatability.
We describe the challenges of designing the system and
present an implementation of a networking stack built on
these principles. The amount of communication and data our
stack handles as a result of high networking load suggests
that our design is applicable to other parts of the system.
We admit that we loose many resources by dedicating big
cores of current mainstream processors to system components
and it must be addressed in our future work. We need
to investigate how to efﬁciently adapt the system to its
current workload, for instance by coalescing lightly utilized
components on a single core and dedicating cores to heavily
used ones. Equally importantly we are interested in how can
we change future chips to match our needs the best. For
example, can some of the big cores be replaced by many
simpler ones to run the system?
ACKNOWLEDGMENTS
This work has been supported by the European Research
Council Advanced Grant 227874. We would like to thank
Arun Thomas for his priceless comments on early versions
of this paper.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:17:40 UTC from IEEE Xplore.  Restrictions apply. 
REFERENCES
[1] Minix 3, Ofﬁcial Website and Download. http://www.minix3.org.
[2] big.LITTLE Processing. http://www.arm.com/products/processors/
technologies/biglittleprocessing.php, 2011.
[3] Variable SMP - A Multi-Core CPU Architecture for Low Power and
High Performance. Whitepaper - http://www.nvidia.com/, 2011.
[4] Vulnerability in TCP/IP Could Allow Remote Code Execution. http:
//technet.microsoft.com/en-us/security/bulletin/ms11-083, Nov. 2011.
[5] A. Baumann, P. Barham, P.-E. Dagand, T. Harris, R. Isaacs, S. Peter,
T. Roscoe, A. Sch¨upbach, and A. Singhania. The Multikernel: A New
OS Architecture for Scalable Multicore Systems. Proc. of Symp. on
Oper. Sys. Principles, 2009.
[6] H. Bos, W. de Bruijn, M. Cristea, T. Nguyen, and G. Portokalidis.
Ffpf: Fairly fast packet ﬁlters. In Proc. of Symp. on Oper. Sys. Des.
and Impl., 2004.
[7] S. Boyd-Wickizer, A. T. Clements, Y. Mao, A. Pesterev, M. F.
Kaashoek, R. Morris, and N. Zeldovich. An Analysis of Linux
In Proc. of Symp. on Oper. Sys. Des.
Scalability to Many Cores.
and Impl., 2010.
[8] P. Colp, M. Nanavati, J. Zhu, W. Aiello, G. Coker, T. Deegan,
P. Loscocco, and A. Warﬁeld. Breaking up is hard to do: security and
functionality in a commodity hypervisor. In Proc. of Symp. on Oper.
Sys. Principles, 2011.
[9] F. M. David, E. M. Chan, J. C. Carlyle, and R. H. Campbell. CuriOS:
improving reliability through operating system structure. In Proc. of
Symp. on Oper. Sys. Des. and Impl., 2008.
[10] W. de Bruijn, H. Bos, and H. Bal. Application-Tailored I/O with
Streamline. ACM Transacations on Computer Systems, 29, May 2011.
[11] L. Deri. Improving Passive Packet Capture: Beyond Device Polling.
In Proc. of Sys. Admin. and Net. Engin. Conf., 2004.
[12] P. Druschel and L. L. Peterson. Fbufs: A High-bandwidth Cross-
domain Transfer Facility. In Proc. of Symp. on Oper. Sys. Principles,
1993.
[13] A. Dunkels. Full TCP/IP for 8-bit architectures.
In International
Conference on Mobile Systems, Applications, and Services, 2003.
[14] J. Erickson. Hacking: The Art of Exploitation. No Starch Press, 2003.
[15] V. Ganapathy, A. Balakrishnan, M. M. Swift, and S. Jha. Microdrivers:
A New Architecture for Device Drivers. In Workshop on Hot Top. in
Oper. Sys., 2007.
[16] A. Gefﬂaut, T. Jaeger, Y. Park, J. Liedtke, K. J. Elphinstone, V. Uhlig,
J. E. Tidswell, L. Deller, and L. Reuther. The SawMill Multiserver
Approach. In Proc. of workshop on Beyond the PC: new challenges
for the oper. sys., 2000.
[17] J. Giacomoni, T. Moseley, and M. Vachharajani. FastForward for
Efﬁcient Pipeline Parallelism: A Cache-optimized Concurrent Lock-
free Queue. In PPoPP, 2008.
[18] C. Giuffrida, L. Cavallaro, and A. S. Tanenbaum. We Crashed, Now
What? In HotDep, 2010.
[19] L. Hatton. Reexamining the Fault Density-Component Size Connection.
IEEE Softw., 14, March 1997.
[20] J. N. Herder, H. Bos, B. Gras, P. Homburg, and A. S. Tanenbaum.
Failure Resilience for Device Drivers. In Proc. of Int. Conf. on Depend.
Sys. and Net., 2007.
[21] J. N. Herder, H. Bos, B. Gras, P. Homburg, and A. S. Tanenbaum.
Countering IPC Threats in Multiserver Operating Systems (A Funda-
mental Requirement for Dependability). In Paciﬁc Rim Int. Symp. on
Dep. Comp., 2008.
[22] J. N. Herder, H. Bos, B. Gras, P. Homburg, and A. S. Tanenbaum.
Fault Isolation for Device Drivers. In Proc. of Int. Conf. on Depend.
Sys. and Net., 2009.
[23] Intel. Single-Chip Cloud Computer. http://techresearch.intel.com/
ProjectDetails.aspx?Id=1.
[24] N. Jalbert, C. Pereira, G. Pokam, and K. Sen. RADBench: A
Concurrency Bug Benchmark Suite. In HotPar’11, May 2011.
[25] B. Leslie, P. Chubb, N. Fitzroy-dale, S. Gtz, C. Gray, L. Macpherson,
D. Potts, Y. Shen, K. Elphinstone, and G. Heiser. User-level Device
Drivers: Achieved Performance. Computer Science and Technology,
20, 2005.
[26] J. Liedtke, K. Elphinstone, S. Sch¨onberg, H. Hrtig, G. Heiser, N. Islam,
and T. Jaeger. Achieved ipc performance (still the foundation for
extensibility), 1997.
[27] J. L¨oser, H. H¨artig, and L. Reuther. A Streaming Interface for Real-
Time Interprocess Communication. In Workshop on Hot Top. in Oper.
Sys., 2001.
[28] S. Lu, S. Park, C. Hu, X. Ma, W. Jiang, Z. Li, R. A. Popa, and Y. Zhou.
Muvi: automatically inferring multi-variable access correlations and
detecting related semantic and concurrency bugs. SIGOPS Oper. Syst.
Rev., 41:103–116, October 2007.
[29] T. Mattson. Intel: 1,000-core Processor Possible. http://www.pcworld.
Nov.
com/article/211238/intel 1000core processor possible.html,
2010.
[30] W. T. Ng and P. M. Chen. The Systematic Improvement of Fault
Tolerance in the Rio File Cache. In Proceedings of the Twenty-Ninth
Annual International Symposium on Fault-Tolerant Computing, 1999.
[31] E. B. Nightingale, O. Hodson, R. McIlroy, C. Hawblitzel, and G. Hunt.
Helios: Heterogeneous Multiprocessing with Satellite Kernels. In Proc.
of Symp. on Oper. Sys. Principles, 2009.
[32] M. Peloquin, L. Olson, and A. Coonce. Simultaneity safari: A study of
concurrency bugs in device drivers. University of WisconsinMadison
Report, pages.cs.wisc.edu/∼markus/736/concurrency.pdf, 2009.
[33] D. C. Sastry and M. Demirci. The QNX Operating System. Computer,
28, November 1995.
[34] M. Scondo. Concurrency and race conditions in kernel space (linux 2.6).
LinuxSupport.com (extract from ”Linux Device Drivers”), December
2009.
[35] L. Seiler, D. Carmean, E. Sprangle, T. Forsyth, M. Abrash, P. Dubey,
S. Junkins, A. Lake, J. Sugerman, R. Cavin, R. Espasa, E. Grochowski,
T. Juan, and P. Hanrahan. Larrabee: A Many-core x86 Architecture
for Visual Computing. ACM Trans. Graph., 27, August 2008.
[36] M. Shah, J. Barren, J. Brooks, R. Golla, G. Grohoski, N. Gura,
R. Hetherington, P. Jordan, M. Luttrell, C. Olson, B. Sana, D. Sheahan,
L. Spracklen, and A. Wynn. UltraSPARC T2: A Highly-treaded, Power-
efﬁcient, SPARC SOC. In ASSCC’07 .
[37] L. Shalev, J. Satran, E. Borovik, and M. Ben-Yehuda. IsoStack: Highly
Efﬁcient Network Processing on Dedicated Cores. In Proc. of USENIX
Annual Tech. Conf., 2010.
[38] J. S. Shapiro. Vulnerabilities in Synchronous IPC Designs. In Proc.
of IEEE Symp. on Sec. and Priv. IEEE Computer Society, 2003.
[39] L. Soares and M. Stumm. FlexSC: Flexible System Call Scheduling
with Exception-Less System Calls. In Proc. of Symp. on Oper. Sys.
Des. and Impl., 2010.
[40] L. Soares and M. Stumm. Exception-less System Calls for Event-
Driven Servers. In Proc. of USENIX Annual Tech. Conf., 2011.
[41] R. Strong, J. Mudigonda, J. C. Mogul, N. Binkert, and D. Tullsen.
Fast Switching of Threads Between Cores. SIGOPS Oper. Syst. Rev.,
43, April 2009.
[42] M. M. Swift, B. N. Bershad, and H. M. Levy. Improving the Reliability
of Commodity Operating Systems. In Proc. of Symp. on Oper. Sys.
Principles, pages 207–222, 2003.
[43] D. Wentzlaff, C. Gruenwald, III, N. Beckmann, K. Modzelewski,
A. Belay, L. Youseff, J. Miller, and A. Agarwal. An Operating System
for Multicore and Clouds: Mechanisms and Implementation. In Proc.
of Symp. on Cloud Computing, 2010.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:17:40 UTC from IEEE Xplore.  Restrictions apply.