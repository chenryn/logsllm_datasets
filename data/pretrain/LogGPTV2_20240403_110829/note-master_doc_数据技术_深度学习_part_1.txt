# 深度学习
## 深度前馈网络
一种经典的神经网络结构，这种网络通常由多层神经元组成，层与层之间是单向传递信号的结构，因此称为“前馈”，意味着信息从输入层流向输出层，不会存在反馈连接。典型的深度前馈神经网络包括输入层、若干隐藏层和输出层。每个神经元会接收来自前一层神经元的输入，将其进行加权求和并通过激活函数处理后输出给下一层
网络架构的建立、损失函数的选择、输出单元和隐藏单元的设计、训练误差的处理等问题是深度前馈网络设计中的一系列核心问题
## 自编码器
自编码器是包含若干隐藏层的深度前馈神经网络，其独特之处是输入层和输出层的单元数目相等
将编码映射记作 ϕ，解码映射记作 ψ，自编码器的作用就是将输入 X 改写为 (ψ∘ϕ)(X)
如果以均方误差作为网络训练中的损失函数，自编码器的目的就是找到使均方误差最小的编解码映射的组合
$$
\phi,\psi=\arg\min_{\phi,\psi}||\mathbf{X}-(\phi\circ\psi)(\mathbf{X})||^2
$$
从信息论的角度看，编码映射可以看成是对输入信源 X 的有损压缩，学习的作用就是习得在训练数据集上更加精确的映射，并希望这样的映射在测试数据上同样表现良好，也就是使自编码器具有较好的泛化性能
## 优化
优化问题：
- 病态矩阵：数值精度导致的不可避免的舍入误差可能会给输出带来巨大的偏离，病态矩阵对输入的敏感性会导致很小的更新步长也会增加代价函数，使学习的速度变得异常缓
- 局部极小值：在神经网络，尤其是深度模型中，代价函数甚至会具有不可列无限多个局部极小值，这显然会妨碍对全局最小值的寻找
- 鞍点：梯度为 0 的临界点，但它既不是极大值也不是极小值
随机梯度下降法（stochastic gradient descent）就是在传统机器学习和深度神经网络中都能发挥作用的经典算法，就是每走一步就换个方向。为了节省每次迭代的计算成本，随机梯度下降在每一次迭代中都使用训练数据集的一个较小子集来求解梯度的均值
随机梯度下降会受到噪声的影响。当学习率固定时，噪声会阻止算法的收敛；而当学习率逐渐衰减时，噪声也会将收敛速度压低到次线性水平，降噪的方法有动态采样、梯度聚合和迭代平均三类。为了提升梯度下降的性能，可以通过使用二阶导数近似方法
除此之外，还有动量方法（momentum）、加速下降方法（accelerated gradient descent）和坐标下降方法（coordinate descent）等方法来进行改进梯度下降
## 神经网络
![20231022211554](/assets/20231022211554.png)
layer1 是输入层，每个节点就是一个特征值
layer2 是隐藏层，每个节点是一个逻辑回归神经元，每个神经元接收上一层的1个或多个特征，产生一个新特征，输出给下一层，接收特征的输入被称为激活，接收特征的输入被称为激活函数，逻辑回归神经元的激活函数是sigmoid函数
layer3 是输出层，其根据上一层输出的所有特征，再输出一个特征值
从左向右每个特征被计算成新特征，这个过程称之为前向传播，Tensorflow使用反向传播算法来替代梯度下降计算出参数
隐藏层中每个神经元处理数据的方式，计算什么特征，都是根据训练数据统计所决定的，而非人工指定
神经网络每一层输出的特征值都比上一层的特征更高级，所以也就能更好地预测数据
```py
# 手动实现前向传播过程
x = np.array([220,200,17])
# 第一层第一个神经元
w1_1 = np.array([1,2])
b1_1 = np.array([-1])
z1_1 = np.dot(w1_1, x) + b1_1
a1_1 = sigmoid(z1_1)
# 第一层第二个神经元
w1_2 = np.array([-3,4])
b1_2 = np.array([-1])
z1_2 = np.dot(w1_2, x) + b1_2
a1_2 = sigmoid(z1_2)
# 第一层第三个神经元
w1_3 = np.array([5,-6])
b1_3 = np.array([-1])
z1_3 = np.dot(w1_3, x) + b1_3
a1_3 = sigmoid(z1_3)
# 第二层
w2_1 = np.array([-7,8,9])
b2_1 = np.array([3])
z2_1 = np.dot(w2_1,a1)+b2_1
a2_1 = sigmoid(z2_1)
结果 = a2_1
```
### 激活函数
如果没有激活函数，也就是使用线性激活函数，那么整个神经网络就跟线性回归一样，解决不了更复杂的问题
不同的激活函数会导致神经元输出的模式也不一样
- sigmoid
- 线性激活函数：输入什么就输出什么，等于没有激活函数
- ReLu: 小于0输出0，大于0，输出输入本身
对于输出层，如果解决的是二分类问题，那就需要使用sigmoid，如果解决的是回归问题，输出有负数，选择线性激活函数，输出没有负数，选择ReLu
对于隐藏层，大多数情况下使用的都是ReLu，因为它梯度下降比sigmoid更快
### 优化算法
- Adam：相比传统的梯度下降算法，这种优化算法使得学习率$a$不是固定的，而是会根据运行情况调大会调小以优化性能
### 卷积层
卷积层的每个神经单元不会接受上一层的全部特征输入，而是有选择的选择一部分特征，不同神经元选择的特征集合会重叠