current client key pair is valid for, they each (independently)
To solve this issue, we rely on a hardware-based root of
trust. We assume each on-premises replica is conﬁgured at the
time the system is set up with a shared symmetric encryption
key that can only be accessed from within trusted hardware
(e.g. TPM or Intel SGX [40]) and persists across reboots. An
attacker who compromises a server but does not have physical
access may use the key for encryption while it has access to
the machine, but cannot exﬁltrate, modify, or delete the key.
This permanent key is used to encrypt new key proposals:
with this approach, they cannot be decrypted by data center
replicas (or external observers), but can be decrypted by
recovering/rejoining on-premises replicas (without requiring
the recovering/rejoining replicas to retrieve keys from data
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:54 UTC from IEEE Xplore.  Restrictions apply. 
21
center replicas). We note that this assumption of a limited
degree of trusted hardware is not an unreasonable requirement,
as proactive recovery already requires each replica to maintain
a persistent hardware-based (TPM) asymmetric private signing
key that it uses to authenticate itself and establish new session-
level signing keys during its recovery process.
Adapting State Transfer. Given that key proposal messages
will eventually be garbage collected, we must also extend state
checkpoints to additionally include the current encryption and
pseudorandom function keys for each client and their validity
periods (i.e. the highest sequence number they can be used
for), as well as any valid pending key proposal messages. By
pending key proposal message, we mean a key proposal that
has been ordered, but not yet used to generate a new key, as not
enough proposals for the same client and validity period were
ordered before the checkpoint was taken. With this extension,
checkpoints are also encrypted using the hardware-protected
symmetric key (although it is also possible to treat checkpoints
as another logical client, with a new session-level key agreed
on for each checkpoint. In this case, it is only necessary to
encrypt the part of the checkpoint containing the session keys
with the persistent hardware-protected key).
Limiting Disclosure. Finally, in order to guarantee limits on
the amount of conﬁdential state a compromised on-premises
replica can expose, we must ensure that compromised replicas
cannot control the selection of future encryption keys that will
be used after they have gone through the proactive recovery
process and been restored to a correct state. To do this, we
enforce that new key proposals are only accepted as valid if
they are introduced at the correct logical time. That is, we
deﬁne a sequence number slack parameter x that represents
how far in advance of the sequence range a key is intended to
be active for it can be proposed. For example, if we consider
x = 10 and a key validity period of 100 updates, a new
key proposal for range 101-200 will not be considered as
valid (and included in the computation of the actual new key)
unless it is ordered after update 90 for the relevant client in
the global total ordering created by Prime. Since all correct
replicas observe the ordered stream of updates in the same
way, all will make the same decision as to a key proposal’s
validity.
An additional concern may be that a compromised replica
could, while it is compromised, generate, encrypt, and sign
proposals for future client sequence numbers, and send them
to a malicious external collaborator to inject at the appropriate
time. However, since such messages are required to be signed
with the replica’s session-level signing key, which is refreshed
following a proactive recovery, this is not a problem.
Our key renewal procedure does not provide complete
conﬁdentiality (in the sense of Deﬁnition 3) in the presence of
a compromised on premises replica, but it limits the damage
such a replica can do. In particular, for a client key validity
period V and slack parameter x, it guarantees that any keys
leaked by a compromised replica will only be able to decrypt
a maximum of V + x updates per client that are issued after
the replica is recovered (of course, the compromised replica
may leak all updates issued while it is compromised). In
addition, since checkpoints are encrypted with keys that cannot
be exﬁltrated from their physical machine, no checkpoint
constructed after the replica is recovered can be decrypted
using keys it leaked while compromised. Thus, as long as
replicas are periodically proactively recovered and clients
continue to issue updates, the system will eventually return
to a situation where its state is fully conﬁdential, if no new
on-premises compromises occur. Unfortunately, this does not
apply to state manipulation algorithms: since those are likely
to change rarely, once a replica with access to those algorithms
is compromised, we can no longer provide guarantees of their
conﬁdentiality.
VI. CONFIDENTIAL SPIRE IMPLEMENTATION
We have implemented our architecture and protocols in
Conﬁdential Spire, a SCADA system for the power grid
that provides the Safety, Bounded Delay, and Conﬁdentiality
guarantees deﬁned in Section III-C under the threat model
stated in Section III-B. Our Conﬁdential Spire implementation
is built on the open source Spire version 1.2 [9], which
implements the architecture described in [4], and provides
Safety and Bounded Delay (but not Conﬁdentiality) under our
same threat model.
In Conﬁdential Spire, SCADA control centers serve as the
on-premises sites, and the clients submitting updates to the
system are Remote Terminal Units (RTUs) and Programmable
Logic Controllers (PLCs) that interact with the power grid
equipment, and Human Machine Interfaces (HMIs) that oper-
ators use to issue commands and view the system state.
The Spire 1.2 implementation already includes a SCADA
master application and RTU/PLC proxies. Its system com-
ponents communicate over the Spines intrusion-tolerant net-
work [34], and updates are ordered using the Prime intrusion
tolerant replication engine [38]. An intrusion-tolerant commu-
nication library (the Intrusion-Tolerant Reliable Channel, or
ITRC) manages communications between client proxies and
the control center servers, as well as between Prime and the
SCADA Master application.
A. Conﬁdentiality-Preserving Intrusion Tolerant Middleware
Conﬁdential Spire adapts and extends Spire’s intrusion-
tolerant
communication library into a Conﬁdentiality-
Preserving Intrusion-Tolerant Middleware (CP-ITM). While
the CP-ITM serves the same basic functions as Spire’s ITRC,
it additionally supports encryption and decryption of client
updates, the creation (and encryption) of periodic checkpoints,
and a new checkpoint-based state transfer protocol. The CP-
ITM is intended to be a generic middleware that can handle
client communication and state management/transfer for any
application.
B. Encryption and Decryption Details
The CP-ITM encrypts client requests before injecting them
into Prime and decrypts them before delivering them to the
SCADA Master application. For each client,
the CP-ITM
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:54 UTC from IEEE Xplore.  Restrictions apply. 
22
For all experiments, we emulate a power grid SCADA setup
with control centers and data centers spanning about 250
miles of the US East Coast. Experiments are conducted in
a local area network, but latencies between sites are emulated
to reﬂect this geographic distribution. We emulate ten power
grid substations each injecting updates via proxies at a rate of
one per second per substation.
A. Performance Overhead of Conﬁdentiality
To assess the performance overhead of our approach, we
compare Conﬁdential Spire to Spire 1.2 in two different con-
ﬁgurations: one tolerating one compromised replica (f = 1)
and one tolerating two compromised replicas (f = 2). Both
conﬁgurations additionally tolerate a proactive recovery and
disconnected site to support our full threat model.
We consider conﬁgurations using two control center sites
and two data center sites, as these were shown to be the most
practical for Spire [4]. The 4-site conﬁgurations are also the
most reasonable for Conﬁdential Spire, as they allow us to use
fewer total replicas compared to conﬁgurations using only one
data center, but using additional data center sites beyond two
does not provide further beneﬁts, due to the requirements on
the number of replicas per control center (see Table I).
Therefore, for the f = 1 conﬁgurations, we evaluate
conﬁguration “3+3+3+3” (three replicas in each of 2 control
centers and 2 data centers) for Spire 1.2 and conﬁguration
“4+4+3+3” (four replicas in each of 2 control centers and 3
replicas in each of 2 data centers) for Conﬁdential Spire. For
tolerating 2 simultaneous intrusions, we use the “5+5+5+4”
conﬁguration for Spire 1.2, and the equivalent “6+6+5+4”
conﬁguration in Conﬁdential Spire. We ran each conﬁguration
for 1 hour and report the resulting update latencies in Table II.
From the results for the f = 1 conﬁgurations, we can
see that Conﬁdential Spire adds a small constant
latency
overhead of about 2ms. This increase in overhead is small
because it avoids adding any new wide-area communication
on the critical path compared with Spire 1.2. While Con-
ﬁdential Spire requires control center replicas to cooperate
to generate a threshold signature on each incoming client
request,
the
needed f + 1 signature shares from replicas within its own
site, since each on-premises site contains 2f + 2 replicas.
Hence, Conﬁdential Spire only utilizes the local-area network
for the added communications. The sum of computational
overhead, to compute the signatures and to encrypt/decrypt the
requests, and the local-area network communications overhead
is small compared to the multiple rounds of wide-area network
message exchanges needed for the agreement protocol. While
Conﬁdential Spire also adds computation and communication
for checkpoint creation and exchange,
this occurs off the
critical path of request processing and thus does not have a
signiﬁcant effect on latency.
is always possible for a replica to collect
it
maintains a shared symmetric encryption key and a pseudo-
random function key (which can be periodically refreshed as
described in Section V-D, though this is not yet implemented).
To encrypt a request, the CP-ITM generates a hash-based
message authentication code (HMAC) based on the update
request itself and the shared pseudorandom function key for
that client, following the approach of [30]. Then, the client
update request is encrypted using AES-256 in CBC mode with
this HMAC as the initialization vector (IV) and the client’s
shared encryption key.
Since the encryption key and pseudorandom function key
for each client are shared across all control center CP-ITM
instances, they all generate the same encrypted result for a
client request by using the above method. We note that even if
a client issues the same request multiple times, it will not result
in the same encrypted output, as the client sequence number
is included in the message content over which the HMAC is
generated and in the content that is encrypted. The CP-ITM
can decrypt encrypted content using the shared encryption key
for that client and the IV (HMAC) which is included in the
message header as cleartext.
C. Checkpointing and State Transfer Implementation
is needed (i.e.
When the CP-ITM running in a control center replica
that C
determines that a new checkpoint
updates have been ordered since the previous checkpoint),
it requests the SCADA master to package and send back a
snapshot of the current state of the system. Before the CP-
ITM multicasts this checkpoint to other replicas, it encrypts the
checkpoint and the associated ordered sequence number using
the same method as described in Section VI-B. Every CP-
ITM instance maintains an additional shared pseudorandom
key and encryption key (in addition to the client key pairs)
for encrypting and decrypting the checkpoints (which can
be hardware-protected, as discussed in Section V-D). In this
way, all control center CP-ITM instances can independently
generate identical encrypted checkpoints.
When a replica requires a state transfer, its CP-ITM collects
the correct encrypted checkpoint and the correct set of updates
following the protocol in Section V-C. When the CP-ITM is
done collecting, if it is running on a data center replica, then
it simply stores the encrypted checkpoint and updates and
continues operations in normal status. However, if the CP-ITM
is running on a control center replica, it decrypts and sends the
correct checkpoint to the SCADA Master to apply, and then
decrypts and sends each collected update request in the order
of their sequence numbers to the SCADA Master. Finally, it
does the same with any new ordered encrypted client requests
that were pending while waiting to collect state, and resumes
normal operations.
VII. EVALUATION
We ﬁrst evaluate the overhead of providing conﬁdentiality in
our approach by comparing our Conﬁdential Spire implemen-
tation to Spire 1.2 [9], and then evaluate our implementation’s
performance under particular types of attacks.
In the f = 2 case, we can see that Conﬁdential Spire’s
“6+6+5+4” conﬁguration adds somewhat more overhead, in-
creasing average latency by about 6.8ms as compared to
Spire’s “5+5+5+4” conﬁguration. This is about 3.5 times the
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:54 UTC from IEEE Xplore.  Restrictions apply. 
23
f
1
2
1
2
Setup
3+3+3+3
5+5+5+4
4+4+3+3
6+6+5+4
Avg
Latency
51.7 ms
54.4 ms
53.6 ms
61.2 ms
Spire
Conﬁdential
Spire
%
100
100