preprocessed function) in the target ﬁngerprint dictionary. If
the key lookup fails, then VUDDY concludes that there is no
clone in the target program. If it succeeds to ﬁnd the existence
of the same integer key, then it proceeds to the next substep:
Hash lookup.
S5. Hash lookup: As a last substep of clone detection,
VUDDY searches for the presence of the hash value in the
set mapped to the integer key. If the hash value is discovered,
then the function is considered to be a clone.
For example, when comparing dictionary A and B, VUDDY
iterates S4 over every key in dictionary A, searching for the
key in dictionary B. For each key shared by dictionary A and
B, VUDDY performs S5 to retrieve all shared hash values,
which are the clones we are looking for.
This design of VUDDY accelerates the process of clone
searching by taking advantage of the following two facts:
1) The time complexity of an operation that checks the
existence of a value from a set of unique elements is
O(1) on average, and O(n) in the worst case.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:26:17 UTC from IEEE Xplore.  Restrictions apply. 
2) It is guaranteed that even in the worst case, n is small
because of the length classiﬁcation. For example, the
ﬁngerprint dictionary of Linux kernel 4.7.6 (stable kernel
released on Sep. 30, 23K ﬁles with over 15.4 MLoC)
only contains 5,245 integer keys, and among the hash sets
associated to the keys, the largest set has 1,019 elements.
The average number of elements of the hash sets is 67.85,
the median is 5, and the mode (the value that occurs most
often) is 1. This implies that most of the hash set will
have only one element.
The efﬁciency of VUDDY in terms of discovering code
clones from large programs is further evaluated in section VII.
V. APPLICATION: VULNERABILITY DETECTION
In this section we describe the application of VUDDY to
detect vulnerabilities from small to massive real-world pro-
grams. To obtain vulnerable functions from reliable software
projects when establishing a vulnerability database, we lever-
aged the Git repositories of well-known and authoritative open
source projects: Google Android, Codeaurora Android Project,
Google Chromium Project, FreeBSD, Linux Kernel, Ubuntu-
Trusty, Apache HTTPD, and OpenSSL. Then, by using the
general clone detection procedure explained in section IV,
VUDDY searches for the code clones of vulnerable functions
from a target program.
A. Establishing a vulnerability database
The process of collecting vulnerable code and establishing
a vulnerability database is fully automated. The process of
reconstructing vulnerable functions out of Git commit logs
consists of the following steps:
1) git clone repository. This is to download speciﬁed
Git repository into a local directory.
2) git log --grep=‘CVE-20’ for each repository.
This searches the commits regarding Common Vulner-
ability and Exposures (CVEs) [34]. This works for any
general keywords, such as vulnerability types, or vulner-
ability names. If it is required to collect certain types of
bugs, such as buffer overﬂow, the keyword for grep would
be “buffer overﬂow.” Well-known vulnerability names,
such as Heartbleed, can also be queried.
3) git show the searched commits. This shows the full
commit log that contains a description of the vulnerability
related to CVE, as well as a security patch information
in uniﬁed diff format. Diffs have a dedicated line for
recording ﬁle metadata, in which reference IDs to old
and new ﬁles addressed by the patch are written.
4) Filter irrelevant commits. The steps listed could fetch
commits that are inappropriate for vulnerability detection.
For example, some commits have the keyword “CVE-
20” in their message, which is actually “Revert the patch
for CVE-20XX-XXXX.” Merging commits or updating
commits which usually put all the messages of associated
commits together are another problem, particularly if one
of the commits happens to be a CVE patch. In such cases,
our automated approach would end up retrieving a benign
function. Thus, commits which revert, merge, or update
are discarded in this step.
5) git show the old ﬁle ID. This shows the old, un-
patched version of the ﬁle. We then retrieve the vulnerable
function from the ﬁle.
Listing 1 is the patch for CVE-2013-4312, found in the
Codeaurora Android repository. This patch adds lines 9 and 10
to ensure that the per-user amount of pages allocated in pipes
is limited so that the system can be protected against memory
abuse. The ﬁle metadata in line 2 indicate the references to
the old ﬁle (d2cbeff) and the new ﬁle (19078bd), and line
5 conveys information about the line numbers of the affected
portion in the ﬁle.
We could retrieve the old function, namely the vulnerable
version of the function, by querying “git show d2cbeff”
to the cloned Git object, obtaining the old ﬁle, and parsing
the relevant function. Listing 2 is the retrieved vulnerable
function, which includes both the vulnerable part, and the
context around it.
Listing 1: Patch for CVE-2013-4312.
1 diff --git a/fs/pipe.c b/fs/pipe.c
2 index d2cbeff..19078bd 100644
3 --- a/fs/pipe.c
4 +++ b/fs/pipe.c
5 @@ -607,6 +642,8 @@ void free_pipe_info(struct
pipe_inode_info *pipe)
int i;
6 {
7
8 + account_pipe_buffers(pipe, pipe->buffers, 0);
9 + free_uid(pipe->user);
for (i = 0; i buffers; i++) {
struct pipe_buffer *buf = pipe->bufs + i;
if (buf->ops)
10
11
12
Listing 2: Snippet of the vulnerable function retrieved from
the patch for CVE-2013-4312.
1 void free_pipe_info(struct pipe_inode_info *pipe)
2 {
3
int i;
for (i = 0; i buffers; i++) {
struct pipe_buffer *buf = pipe->bufs + i;
if (buf->ops)
4
5
6
Applying the same method to 9,770 vulnerability patches,
we collected 5,664 vulnerable functions that address 1,764
unique CVEs. These vulnerable functions have well-known
vulnerabilities such as buffer overﬂow, integer overﬂow, input
validation error, permission-related vulnerabilities, and others.
The shortest vulnerable function consists of 51 characters after
abstraction and normalization. Single-lined functions (e.g., a
guard function which returns by calling another function) are
excluded from the database, since these functions frequently
cause false positives when our abstraction is applied.
B. Vulnerability detection
The application of VUDDY for vulnerability detection does
not require any supplementary procedure. VUDDY processes
the functions in the vulnerability database in the same way as
it does with a normal program, then discovers vulnerability
601
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:26:17 UTC from IEEE Xplore.  Restrictions apply. 
in the target program by detecting code clones between the
vulnerability database and the program.
Here, we can determine which vulnerability VUDDY is
capable of discovering. As illustrated in Fig. 5(a), if set K
is the set of every known vulnerability, then K ⊂ V where
V is the set consisting of all vulnerabilities. Naturally, we
can regard U, the set of unknown vulnerabilities, and K as
being disjoint, so that K ∪ U = V and K ∩ U = φ. If a
clone detector C only considers exact (Type-1) clones, then
the coverage of C is K. However, by the use of our abstraction
strategy, the coverage of an abstract clone detector ˆC can also
(cid:2) as depicted in Fig. 5(b), which is
cover vulnerabilities in K
a set of abstract vulnerabilities. This means that VUDDY can
detect known vulnerabilities, as well as variants of the known
(cid:2)∩ U is
vulnerabilities, which are in K
the set of unknown code clones discovered by VUDDY. The
examples are provided in subsection IX-C.
(cid:2)∩ U| > 0. K
(cid:2), where |K
(cid:57)
(cid:46)
(cid:56)
(cid:57)
(cid:46)(cid:859)
(cid:56)
(cid:56)
(a) K and U are disjoint
Fig. 5: Relationship between known, unknown and variants of
known vulnerabilities.
(cid:2)
(b) K
and U intersect
VI. IMPLEMENTATION
We implemented VUDDY2 in Python 2.7.11, and the robust
parser with the ANTLR parser generator 4.5.3 [35]. In this
section, we discuss issues related to the implementation.
A. Generating a robust parser
One intuitive approach for obtaining functions from a
program and analyzing their syntax is to use a compiler.
However, the use of parsers integrated in compilers is evidently
restricted to the occasions when a working build environment
is available. In addition, even if we succeed to replicate the
working environment, the source code may not be complete
or may contain syntax errors, which blights the whole parsing
procedure [36], [37]. Thus, we ensure that our method is
feasible and sufﬁciently general to be used in practice by
generating and utilizing a robust parser for C/C++ based on
the concept of fuzzy parsing with the utilization of island
grammars [38], [39]. This parser does not require a build
environment or header information, which means it is able to
parse an individual ﬁle, and does not fail when it encounters
syntactic errors during parsing. Even when a broken or partial
code is given, it parses as much as it understands.
B. Selection of hash function
Any hash function can be used for ﬁngerprint generation.
However, we impose three constraints that need to be con-
sidered in order to maximize the scalability and speed of
2Our implementation is available at https://iotcube.net/
602
our approach. First, to prevent two or more different and
irrelevant functions from having the same hash value, it is
necessary to avoid hash collision as much as possible. Second,
building the smallest possible ﬁngerprint dictionary requires
us to choose a hash function that produces the fewest possible
bits of hash values. Third, to minimize the hashing time, the
chosen function needs to be fast, and its implementation well-
optimized. For VUDDY, we selected the MD5 hash algorithm,
which completed hashing 20 million randomly generated al-
phanumeric strings with their size ranging from 51 to 1,000
bytes, in only 15 seconds without collision. Non-cryptographic
hash algorithms such as CityHash [40], MurmurHash [41], and
SpookyHash [42] were also considered, but they required a
similar amount of time for the experiment. Thus, we decided
to adopt MD5 which is provided as a built-in method of
the Python Hashlib package, rather than taking unidentiﬁed
risks by using third party hash libraries. Although we are
aware that the MD5 hash algorithm suffers from cryptographic
weaknesses [43], two facts make MD5 sufﬁcient: Our use of
MD5 is not for cryptography; and VUDDY is designed such
that hash collision occurs only when two different functions
have identical lengths. Note that we exclude the use of fuzzy
hash algorithms, which produce similar hash values for similar
plaintexts, as distinguishing slightly modiﬁed clones from
semantically changed code presents another problem.
C. Dictionary implementation
The ﬁngerprint dictionary is a crucial data structure in the
implementation of VUDDY because it dramatically reduces
the search space of possible clones and thus expedites the
whole process. As previously stated, a dictionary is an asso-
ciative container that maps keys to values. We chose to use
the built-in dictionary data structure of Python, with which the
average time complexity is O(1) for the in operation to check
the existence of an element among the keys of a dictionary,
regardless of the number of elements.
VII. EVALUATION
We proceed to evaluate the efﬁcacy and effectiveness of
VUDDY in two aspects: scalability and accuracy, by compar-
ing VUDDY with other state-of-the-art techniques.
A. Experimental setup and dataset
System environment: We evaluated the execution and detec-
tion performance of VUDDY by conducting experiments on a
machine running Ubuntu 16.04, with a 2.40 GHz Intel Zeon
processor, 32 GB RAM, and 6 TB HDD.
Dataset: We collected our
target C/C++ programs from
GitHub. These programs had at least one star and were pushed
at least during the period from January 1st, 2016 to July
28, 2016. Repositories that are starred (i.e., bookmarked by
GitHub users) are popular and inﬂuential repositories. The
existence of a push record during the ﬁrst half of 2016 implies
that the repository is active. The repository cloning process
required 7 weeks to ﬁnish, gathering 25,253 Git repositories
which satisfy the aforementioned two conditions. In addition
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:26:17 UTC from IEEE Xplore.  Restrictions apply. 
TABLE I: Scalability and time comparison for varying input size. The average time was computed after iterating each experiment
ﬁve times.
LoC
1 K
10 K
100 K
1 M
10 M
100 M
1 B
VUDDY
0.44 s
0.81 s
5.17 s
55 s
12 m 43 s
1 h 32 m
14 h 17 m
SourcererCC
ReDeBug
CCFinderX
DECKARD
2.3 s
3.1 s
50.7 s
1 m 44 s
24 m 38 s
9 h 42 m
25 d 3 h
35.6 s
35.6 s
42 s
1 m 43 s
18 m 32 s
2 h 32 m
1 d 3 h
6 s
10 s
50 s
6 m 44 s
1 h 36 m
12 h 44 m
1 s
3 s
13 s
2 m 20 s
12h 30 m
Memory ERROR
File I/O ERROR
–
TABLE II: Conﬁgurations for experiments.
Technique
SourcererCC
Conﬁguration
Min length 6 lines, min similarity 70 %.
ReDeBug
DECKARD Min length 50 tokens, similarity 85 %, 2 token stride.
CCFinderX
Min length 50 tokens, min token types 12.
n-gram size 4, 10 context lines.
to the Github projects, we downloaded the ﬁrmware of several
Android smartphones.
B. Scalability evaluation
First, we evaluated the scalability of VUDDY, against four
publicly available and competitive techniques (SourcererCC,
ReDeBug, DECKARD, and CCFinderX) in terms of varying
target program size. Note that as Wang et al. [44] pointed
out, the conﬁguration choices have a signiﬁcant impact on the
behavior of the tools that are compared. As a remedy, we
referenced the optimal conﬁguration of each technique found
by [44], [45], and [19] to conduct a sufﬁciently fair evaluation.
The conﬁguration can be found in Table II.
To focus on the scalability of tools when handling real-
world programs, we generated target sets of varying sizes,
from 1 KLoC to 1 BLoC, by randomly selecting projects from
the 25,253 Git projects we collected. All experiments were
iterated ﬁve times each (except for SourcererCC, with which
we iterated twice), to ensure that the results are reliable.
As described in Table I, VUDDY overwhelmed other tech-
niques. DECKARD had the least scalability, failing to process
100 MLoC target because of a memory error. In the case of
CCFinderX, a ﬁle I/O error occurred after 3 days of execution
for a 1 BLoC target. VUDDY ﬁnished generating ﬁngerprints
and detecting clones of the 1 BLoC target in only 14 hour
and 17 minutes. Although SourcererCC and ReDeBug also
scaled to 1 BLoC, their execution is considerably slower than
that of VUDDY. ReDeBug required more than a day, and
SourcererCC required 25 days to ﬁnish detecting clones from
the same 1 BLoC target. Fig. 6 displays a graph depicting the
results in Table I. We can clearly see that the execution time of
the other state-of-the-art techniques explodes as the target size
grows. In fact, VUDDY scales even to the size of all 25,253
repositories consisting of 8.7 BLoC with ease, requiring only
4 days and 7 hours.
C. Accuracy evaluation
Now we evaluate the accuracy of VUDDY by comparing
the number of false positives produced by each tool, given a
set of vulnerabilities and a target program. In this subsection,
603
VUDDY
ReDeBug
SourcererCC
CCFinderX
Deckard
•
(cid:2)
2
◦
(cid:13)
2000
1500
1000
500
)
s
e
t