title:A Placement Vulnerability Study in Multi-Tenant Public Clouds
author:Venkatanathan Varadarajan and
Yinqian Zhang and
Thomas Ristenpart and
Michael M. Swift
A Placement Vulnerability Study in  
Multi-Tenant Public Clouds
Venkatanathan Varadarajan, University of Wisconsin—Madison; Yinqian Zhang, 
The Ohio State University; Thomas Ristenpart, Cornell Tech; Michael Swift,  
University of Wisconsin—Madison
https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/varadarajan
This paper is included in the Proceedings of the 
24th USENIX Security Symposium
August 12–14, 2015 • Washington, D.C.
ISBN  978-1-939133-11-3
Open access to the Proceedings of  the 24th USENIX Security Symposium is sponsored by USENIXA Placement Vulnerability Study in Multi-Tenant Public Clouds
Venkatanathan Varadarajan†, Yinqian Zhang‡, Thomas Ristenpart∗, and Michael Swift†
†University of Wisconsin-Madison, ‡The Ohio State University, ∗Cornell Tech,
{venkatv,swift}@cs.wisc.edu, PI:EMAIL, PI:EMAIL
Abstract
Public infrastructure-as-a-service clouds, such as Amazon
EC2, Google Compute Engine (GCE) and Microsoft Azure
allow clients to run virtual machines (VMs) on shared phys-
ical infrastructure. This practice of multi-tenancy brings
economies of scale, but also introduces the risk of sharing a
physical server with an arbitrary and potentially malicious
VM. Past works have demonstrated how to place a VM
alongside a target victim (co-location) in early-generation
clouds and how to extract secret information via side-
channels. Although there have been numerous works on
side-channel attacks, there have been no studies on place-
ment vulnerabilities in public clouds since the adoption
of stronger isolation technologies such as Virtual Private
Clouds (VPCs).
We investigate this problem of placement vulnerabili-
ties and quantitatively evaluate three popular public clouds
for their susceptibility to co-location attacks. We ﬁnd that
adoption of new technologies (e.g., VPC) makes many prior
attacks, such as cloud cartography, ineffective. We ﬁnd new
ways to reliably test for co-location across Amazon EC2,
Google GCE, and Microsoft Azure. We also found ways to
detect co-location with victim web servers in multi-tiered
located behind a load balancer.
We use our new co-residence tests and multiple customer
accounts to launch VM instances under different strategies
that seek to maximize the likelihood of co-residency.
We ﬁnd that it is much easier (10x higher success rate)
and cheaper (up to $114 less) to achieve co-location in
these three clouds when compared to a secure reference
placement policy.
Keywords: co-location detection, multi-tenancy, cloud se-
curity
1
Introduction
Public cloud computing offers easy access to relatively
cheap compute and storage resources. Cloud providers are
∗Work primarily done while at the University of Wisconsin-Madison.
able to sustain this cost-effective solution through multi-
tenancy, where the infrastructure is shared between com-
putations run by arbitrary customers over the Internet. This
increases utilization compared to dedicated infrastructure,
allowing lower prices.
However, this practice of multi-tenancy also enables var-
ious security attacks in the public cloud. Should an ad-
versary be able to launch a virtual machine on the same
physical host as a victim, making the two VMs co-resident
(sometimes the term co-located is used), there exist attacks
that break the logical isolation provided by virtualization to
breach conﬁdentiality [29, 32, 33, 35, 37, 38] or degrade the
performance [30, 39] of the victim. Perhaps most notable
are the side-channel attacks that steal private keys across
the virtual-machine isolation boundary by cleverly moni-
toring shared resource usage [35, 37, 38].
Less understand is the ability of adversaries to arrange
for co-residency in the ﬁrst place.
In general, doing so
consists of using a launch strategy together with a mech-
anism for co-residency detection. The only prior work
on obtaining co-residency [29] showed simple network-
topology-based co-residency checks along with low-cost
launch strategies that obtain a high probability of achieving
co-residency compared to simply launching as many VM
instances as possible. When such advantageous strategies
exist, we say the cloud suffers from a placement vulner-
ability. Since then, Amazon has made several changes to
their architecture, including removing the ability to do the
simplest co-residency check. Whether placement vulnera-
bilities exist in other public clouds has, to the best of our
knowledge, never been explored.
In this work, we provide a framework to systematically
evaluate public clouds for placement vulnerabilities and
show that three popular public cloud providers may be vul-
nerable to co-location attacks. More speciﬁcally, we set out
to answer four questions:
• Can co-residency be effectively detected in modern
• Are known launch strategies [29] still effective in
public clouds?
modern clouds?
USENIX Association  
24th USENIX Security Symposium  913
ties?
• Are there any new exploitable placement vulnerabili-
• Can we quantify the money and time required of an
adversary to achieve a certain probability of success?
We start by exploring the efﬁcacy of prior co-residency
tests (§ 4) and develop more reliable tests for our place-
ment study (§ 4.1). We also ﬁnd a novel test to detect co-
residency with VMs uncontrolled by the attacker by just us-
ing their public interface even when they are behind a load
balancer (§ 4.3).
We use multiple customer accounts across three pop-
ular cloud providers, launch VM instances under differ-
ent scenarios that may affect the placement algorithm,
and test for co-residency between all launched instances.
We analyze three popular cloud providers, Amazon Elas-
tic Cloud (EC2) [2], Google Compute Engine (GCE) [6]
and Microsoft Azure (Azure) [15], for vulnerabilities in
their placement algorithm. After exhaustive experimenta-
tion with each of these cloud providers and at least 190 runs
per cloud provider, we show that an attacker can still suc-
cessfully arrange for co-location (§ 5). We ﬁnd new launch
strategies in these three clouds that obtain co-location faster
(10x higher success rate) and cheaper (up to $114 less)
when compared to a secure reference placement policy.
Next, we start by giving some background on public
clouds (§ 2), then deﬁne our threat model (§ 3). We con-
clude the paper with related and future work (§ 6 and § 7,
respectively).
2 Background
Public clouds.
Infrastructure-as-a-service (IaaS) public
clouds, such as Amazon EC2, Google Compute Engine and
Microsoft Azure, provide a management interface for cus-
tomers to launch and terminate VM instances with a user-
speciﬁed conﬁguration. Typically, users register with the
cloud provider for an account and use the cloud interface
to specify VM conﬁguration, which includes instance type,
disk image, data center or region to host the VMs, and then
launch VM instances. In addition, public clouds also pro-
vide many higher-level services that monitor load and auto-
matically launch or terminate instances based on the work-
load [4,8,13]. These services internally use the same mech-
anisms as users to conﬁgure, launch and terminate VMs.
The provider’s VM launch service receives from a client
a desired set of parameters describing the conﬁguration of
the VM. The service then allocates resources for the new
VM; this process is called VM provisioning. We are most
interested in the portion of VM provisioning that selects
the physical host to run a VM, which we call the VM place-
ment algorithms. The resulting VM-to-host mapping we
call the VM placement. The placement for a speciﬁc virtual
machine may depend on many factors:
the load on each
machine, the number of machines in the data center, the
number of concurrent VM launch requests, etc.
While cloud providers do not generally publish their VM
Type
Placement
Parameters
Environment
Variable
Variable
# of customers
# of instances launched per customer
Instance type
Data Center (DC) or Region
Time launched
Cloud provider
Time of the day
Days of the week
Number of in-use VMs
Number of machines in DC
Figure 1: List of placement variables.
placement algorithms, there are several variables under the
control of the user that could affect the VM placement, such
as time-of-day, requested data center, and number of in-
stances. A list of some notable parameters are given in
Figure 1. By controlling these variables, an adversary can
partially inﬂuence the placement of VMs on physical ma-
chines that may also host a target set of VMs. We call these
variables placement variables and the set of values for these
variables form a launch strategy. An example launch strat-
egy is to launch 20 instances 10 minutes after triggering an
auto-scale event on a victim application. This is, in fact, a
launch strategy suggested by prior work [29].
Placement policies. VM placement algorithms used in
public clouds often aim to increase data center efﬁciency,
quality of service, or both by realizing some placement pol-
icy. For instance, a policy that aims to increase data center
utilization may pack launched VMs on a single machine.
Similarly policies that optimize the time to provision a VM,
which involves fetching an image over the network to the
physical machine and booting, may choose the last machine
that used the same VM image, as it may already have the
VM image cached on local disks. Policies may vary across
cloud providers, and even within a provider.
Public cloud placement policies, although undocu-
mented, often exhibit behavior that is externally observable.
One example is parallel placement locality [29], in which
VMs launched from different accounts within a short time
window are often placed on the same physical machine.
Two instances launched sequentially, where the ﬁrst in-
stance is terminated before the launch of the second one, are
often placed on the same physical machine, a phenomenon
called sequential placement locality [29].
These placement behaviors are artifacts of the two place-
ment policies described earlier, respectively. Other exam-
ples of policies and resulting behaviors exist as well. VMs
launched from the same accounts may either be packed
on the same physical machine to maximize locality (and
hence co-resident with themselves) or striped across differ-
ent physical machines to maximize redundancy (and hence
never co-resident with themselves). In the course of normal
usage, such behaviors are unlikely to be noticed, but they
can be measured with careful experiments.
914  24th USENIX Security Symposium 
USENIX Association
Launch strategies. An adversary can exploit placement
behaviors to increase the likelihood of co-locating with tar-
get victims. As pointed out by Ristenpart et al. [29], parallel
placement locality can be exploited by triggering a scale-up
event on target victim by increasing their load, which will
cause more victim VMs to launch. The adversary can then
simultaneously (or after a time lag) launch multiple VMs
some of which may be co-located with the newly launched
victim VM(s).
In this study, we develop a framework to systematically
evaluate public clouds against launch strategies and un-
cover previously unknown placement behaviors. We ap-
proach this study by (i) identifying a set of placement vari-
ables that characterize a VM, (ii) enumerating the most in-
teresting values for these variables, and (iii) quantifying the
cost of such a strategy, if it in fact exposes a co-residency
vulnerability. We repeat this for three major public cloud
providers: Amazon EC2, Google Compute Engine, and Mi-
crosoft Azure. Note that the goal of this study is not to re-
verse engineer the exact details of the placement policies,
but rather to identify launch strategies that can be exploited
by an adversary.
Co-residency detection. A key technique for understand-
ing placement vulnerabilities is detecting when VMs are
co-resident on the same physical machine (also termed co-
locate). In 2009, Ristenpart et al. [29] proposed several co-
residency detection techniques and used them to identify
several placement vulnerabilities in Amazon EC2. As co-
resident status is not reported directly by the cloud provider,
these detection methods are usually referred to as side-
channel based techniques, which can be further classiﬁed
into two categories: logical side-channels or performance
side-channels.
Logical side-channels: Logical side-channels allow infor-
mation leakage via logical resources that are observable to
a software program, e.g., IP addresses, timestamp counter
values. Particularly in Amazon EC2, each VM is assigned
two IP addresses, a public IP address for communication
over the Internet, and a private or internal IP address for
intra-datacenter communications. The EC2 cloud infras-
tructure allowed translation of public IP addresses to their
internal counterparts. This translation revealed the topol-
ogy of internal data center network, which allowed a remote
adversary to map the entire public cloud infrastructure and
determine, for example the availability zone and instance
type of a victim. Furthermore, co-resident VMs tended to
have adjacent internal IP addresses.
Logical side-channels can also be established via shared
timestamp counters.
In prior work, skew in timestamp
counters were used to ﬁngerprint a physical machine [27],
although this technique has not yet been explored for co-
residency detection. Co-residency detection via shared
state like interrupt counts and process statistics reported in
procfs also come under this category, but are only appli-
cable to container-based platform-as-a-service clouds.
Performance side-channels: Performance side-channels
are created when performance variations due to resource
contention are observable. Such variations can be used as
an indicator of co-residency. For instance, network perfor-
mance has been used for detecting co-residence [29, 30].
This is because hypervisors often short-cut network traf-
ﬁc between VMs on the same host, providing detectably
shorter round-trip times than between VMs on different
hosts.
Moreover, covert channels, as a special case of side-
channels, can be established between two cooperative VMs.
For purposes of co-residency detection, covert channels
based on shared hardware resources, such as last level
caches (LLCs) or local storage disks, can be exploited by
one VM to detect performance degradation caused by a co-
resident VM. Covert channel detection techniques require
control over both VMs, and are usually used in experimen-
tation rather than in practical attacks.
Placement study in PaaS. While we mainly studied
placement vulnerabilities in the context of IaaS, we also
experimented with Platform-as-a-Service (PaaS) clouds.
PaaS clouds offer elastic application hosting services. Un-
like IaaS where users are granted full control of the VM,
PaaS provides managed compute tasks (or instances) for the
execution of hosted web applications, and allow multiple
such instances to share the same operating system. As such,
logical side-channels alone are usually sufﬁcient for co-
residency detection purposes. For instance, in PaaS clouds,
co-resident instances share the same public IP address as
the host machine. This is because the host-to-instance net-
work is often conﬁgured using Network Address Transla-
tion (NAT) and each instance is assigned unique port under
the host IP address for front-facing incoming connections
(e.g., Heroku [10]). We found that many of the above log-
ical side-channel-based co-residency detection approaches
worked on PaaS clouds, even on those that support isolation
better than process isolation [38] (e.g., using Linux con-
tainers). Speciﬁcally, we used both system-level interrupt
statistics via /proc/interrupts and shared public IP ad-
dresses of the instances to detect co-location in Heroku.
Our quick investigation of co-location attacks in Heroku
showed that na¨ıve strategies like scaling two PaaS web ap-
plications to 30 instances with a time interval of 5 minutes
between them, resulted in co-location in 6 out of 10 runs.
Moreover, since the co-location detection was simple and
quick including the time taken for application scaling, we
were able to do these experiments free of cost. This result
reinforces prior ﬁndings on PaaS co-location attacks [38]
and conﬁrms the existence of cheap launch strategies to
achieve co-location and easy detection mechanisms to ver-
ify it. We do not investigate PaaS clouds further in the rest
of this paper.
USENIX Association  
24th USENIX Security Symposium  915
3 Threat Model
Co-residency attacks in public clouds, as mentioned ear-
lier, involve two steps: a launch strategy and co-residency
detection. We assume that the adversary has access to tools
to identify a set of target victims, and either knows vic-
tim VMs’ launch characteristics or can directly trigger their
launches. The latter is possible by increasing load in order
to cause the victim to scale up by launching more instances.
The focus of this study is to identify if there exists any
launch strategy that an adversary can devise to increase the
chance of co-residency with a set of targeted victim VMs.
In our threat model, we assume that the cloud provider
is trusted and the attacker has no afﬁliation of any form
with the cloud provider. This also means that the adversary
has no internal knowledge of the placement policies that
is responsible for the VM placements in the public cloud.
An adversary also has the same interface for launching and
terminated VMs as regular customers, and no other special
interfaces. Even though there may be per-account limits
on the number of VMs that a cloud provider imposes, an
adversary has access to an unlimited number of accounts
and hence has no limit on the number of VMs he could
launch at any given time.
No resource-limited cloud provider is a match to an ad-
versary with limitless resources and hence realistically we
assume that the adversary is resource-limited. For the same
reason, a cloud provider is vulnerable to a launch strategy
only when it is trivial or cost-effective for an adversary. As
such, we aim to (i) quantify the cost of executing a launch
strategy by an adversary, (ii) deﬁne a reference placement
policy with which the placement policies of real clouds can
be compared, and (iii) deﬁne metrics to quantify placement
vulnerability.
Cost of a Launch Strategy. Quantifying the cost of a
launch strategy is straight-forward: it is the cost of launch-
ing a number of VMs and running tests to detect co-
residency with one or more target victim VMs. To be
precise, the cost of a launch strategy S is given by CS =
a ∗ P(atype) ∗ Td(v,a), where a is the number of attacker
VMs of type atype launched to get co-located with one of
the v victim VMs. P(atype) is the price of running one VM
of type atype for a unit time. Td(a,v) is the time to detect
co-residency between all pairs of a attacjers and v victim
VMs, excluding pairs within each group. For simplicity,
we assume that the attacker is running all instances till the
last co-residency check is completed.
Reference Placement Policy.
In order to deﬁne placement
vulnerability, we need a yardstick to compare various place-
ment policies and the launch strategies that they may be vul-
nerable to. To aid this purpose, we deﬁne a simple reference
placement policy that has good security properties against
co-residency attacks and use it to gauge the placement poli-
cies used in public clouds. Let there be N machines in a data
center and let each machine have unlimited capacity. Given
a set of unordered VM launch requests, the mapping of each
VM to a machine follows a uniform random distribution.
Let there be v victim VMs assigned to v unique machines
among N, where v (cid:30) N. The probability of at least one col-
lision (i.e. co-residency) under the random placement pol-
icy and the above attack scenario when attacker launches a
instances is given by 1−(cid:31)1− v/N(cid:30)a. We call this proba-
bility the reference probability1. Recall that for calculating
cost of a launch strategy under this reference policy, we also
need to deﬁne the price function, P(vmtype). For simplicity,
we use the most competitive minimum price offered by any
cloud provider as the price for the compute resource under
the reference policy. For example, at the time of this study,
Amazon EC2 offered t2.small instances at $0.026 per hour
of instance activity, which was the cheapest price across all
three clouds considered in this study.
Note that the placement policy makes several simplify-
ing assumptions but we argue that all these assumptions are
conservative and only beneﬁt the attacker. For instance, the
assumption on unlimited capacity of the servers only bene-
ﬁt the attacker as it never limits the number of victim VMs
an attacker could potentially co-locate with. We use a con-
servative value of 1000 for N, which is at least an order-
of-magnitude less than the number of servers (50,000) in
the smallest reported Amazon EC2 data centers [5]. Sim-
ilarly, the price function of this placement policy also fa-
vors an attacker as it provides the cheapest price possible in
the market even though in reality a secure placement policy
may demand a higher price. Hence, it would be troubling if
the state-of-the-art placement policies used in public clouds
does not measure well even against such a conservative ref-
erence placement policy.
Placement Vulnerability. Putting it all together, we de-
ﬁne two metrics to gauge any launch strategy against a
placement policy: (i) normalized success rate, and (ii) cost-
beneﬁt. The normalized success rate is the success rate of
the launch strategy in the cloud under test normalized to the
success rate of the same strategy under the reference place-
ment policy. The cost-beneﬁt of a strategy is the additional
cost that is incurred by the adversary in the reference place-
ment policy to achieve the same success rate as the strat-
egy in the placement policy under test. We deﬁne that a
placement policy has a placement vulnerability if and only
if there exists a launch strategy with a normalized success
rate that is greater than 1.
Note that the normalized success rate quantiﬁes how easy
it is to get co-location. On the other hand, the cost beneﬁt
metric helps to quantify how cheap it is to get co-location
compared to a more secure placement policy. These metrics
can be used to compare launch strategies under different
placement policies, where a higher value for any of these
metrics indicate that the placement policy is relative more
1This probability event follows hypergeometric distribution. Itis the
same as the probability event of collision when picking two groups of
balls each of size v and a from an urn of total capacity of balls, N.
916  24th USENIX Security Symposium 
USENIX Association
)
%
(
y
c
n
e
u
q
e
r
F
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
Non-coresident
Co-resident
)
%
(
y
c
n
e
u