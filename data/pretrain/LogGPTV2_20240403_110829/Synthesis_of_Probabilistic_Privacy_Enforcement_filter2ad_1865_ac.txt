.
.
.
0
.
.
.
AA AA AA
AA AA AG
GG GG GG
6
0.5
0.5
.
.
.
0
Figure 4: The two kinds of enforcement Enf and EnfRnd illustrated on the motivating example program π and the enforce-
ment ξ with equivalence classes O/ξ = {{0}, {1}, {2}, {3}, {4}, {5, 6}}. The encodings of π and EnfRnd(π , ξ) are in Figure 2(a) and
Figure 3, respectively. We highlight outputs that violate the policy in red and outputs that are fused together in green .
case and bounds the probability that the attacker can correctly
guess the secret [48].
A privacy policy is a set Φ = {φ1, . . . , φn} of belief bounds. A
program π satisfies a privacy policy Φ for a given attacker belief δ,
denoted by π , δ |= Φ, if we have π , δ |= φ for every φ ∈ Φ.
4 PERMISSIVE PRIVACY ENFORCEMENT
In this section, we present our notion of enforcement. We consider
a common type of privacy enforcement where outputs that leak too
much information are replaced by more ambiguous answers (e.g. by
a set of outputs). The idea to enforce a policy by conflating outputs
generalizes many existing privacy enforcement mechanisms from
the literature. In [37], for example, programs with binary output
are secured by conflating the outputs "Yes" and "No" into a "Deny
answer" output whenever one of the outputs violates the policy. In
the area of anonymous communication networks [49], researchers
have explored the idea of conflating real traffic with fake traffic to
leak less information to the attacker, hence effectively enforcing
the policy. Finally, in database privacy, k-anonymity [45] is a key
concept guaranteeing that any record of a released anonymized
data set can not be tied to any less than at least k records in the
original private data set. A common technique for achieving k-
anonymity is generalization [50]: instead of releasing an exact
value of an attribute (say, age of a patient is 47 years old), the value
is generalized to a range (e.g. age is said to be in the range 40–49).
In the following, we first define what we mean by optimal en-
forcement. Afterwards, we formulate our enforcement synthesis
problem and discuss its complexity.
4.1 Enforcement
If a program π violates a privacy policy Φ for an attacker belief δ,
then we cannot permit the attacker to observe π’s output because,
for some outputs, the revised attacker belief violates one or more
belief bounds in Φ. That is, there is an output o ∈ O and a belief
bound (S,[a, b]) ∈ Φ for which Pπ
To satisfy the policy Φ for the given attacker belief δ, we need
to modify the program π. We introduce the notion of enforcement
to delimit the space of possible modifications that can be applied
to the program π. An enforcement modifies the program π by
conflating certain outputs and making them equally likely. In the
overview example, the enforcement conflates the outputs 5 and 6:
δ (I ∈ S | O = o) (cid:60) [a, b].
if the original program (given in Figure 2(a)) returns 5 or 6 with
different probabilities for a given input, then the modified program
(given in Figure 3) returns 5 with probability 0.5 and 6 also with
probability 0.5 for the given input. This notion of enforcement can
be implemented syntactically by modifying the return statement(s)
of the original program (Line 11 in Figure 2(a)). At the semantic
level, the sets of outputs fused together by the enforcement can be
formalized as an equivalence relation over the outputs.
Let π ∈ MI×O be a probabilistic program. An enforcement for
π is an equivalence relation ξ ⊆ O × O. A program π together
with an enforcement ξ gives us the program π ′ ∈ MI×(O/ξ) that
returns equivalence classes from O/ξ. The program π ′ ∈ MI×(O/ξ),
denoted by Enf(π , ξ), is defined as follows:
Enf(π , ξ)(E | i) =
π(o | i)
o∈E
That is, given an input i ∈ I, the probability that the program
Enf(π , ξ) outputs the equivalence class E is the sum of the proba-
bilities that π returns an output o ∈ E.
To illustrate, in Figure 4(a) we depict the semantics of the pro-
gram π given in Figure 2(a), and in Figures 4(b) the semantics of
the program Enf(π , ξ).
The program Enf(π , ξ) has a different signature than π, since
the outputs of Enf(π , ξ) are the set of equivalence classes O/ξ,
while the outputs of π are O. We can, however, also enforce ξ
without changing the signature of π. For instance, if for an input i
the program Enf(π , ξ) outputs the equivalence class E, instead of
returning the set E we can return an output o ∈ E selected uniformly
at random. We define this enforcement as follows:
EnfRnd(π , ξ)(o | i) =
1
|[o]ξ | Enf(π , ξ)([o]ξ | i)
We illustrate EnfRnd(π , ξ) for the program π and enforcement ξ
of our motivating example in Figure 4(c).
We remark that both ways of enforcing ξ are equivalent from
a security point of view: for any program π, enforcement ξ for π,
attacker belief δ, and privacy policy Φ, we have Enf(π , ξ), δ |= Φ if
and only if EnfRnd(π , ξ), δ |= Φ.
We say that ξ enforces a privacy policy Φ for a program π and
attacker belief δ if Enf(π , ξ), δ |= Φ. Note that for any program π,
attacker belief δ, and privacy policy Φ, if π , δ |= Φ then for any
enforcement ξ for π we have Enf(π , ξ), δ |= Φ.
Session B4:  Privacy PoliciesCCS’17, October 30-November 3, 2017, Dallas, TX, USA395Finally, we remark that our notion of enforcement is complete.
Suppose we have a program π, an attacker belief δ, and a privacy
policy Φ such that π , δ ̸|= Φ. Then, there exists a program π ′ such
that π ′, δ |= Φ if and only if there exists an enforcement ξ for π such
that Enf(π , ξ), δ |= Φ. This means that if no equivalence relation
enforces the privacy policy for the given program and attacker
belief, then no program satisfies the policy for the given attacker.
We prove this statement in Section 4.4.2.
4.2 Optimality of enforcement
Whenever there exists some enforcement ξ for a program and a
policy, then the enforcement ξ⊥ which conflates all outputs into
one is also a valid one. Hence we define the notion of enforcement
optimality to be able to synthesize the best feasible enforcement.
Formally, an optimality order is a total preorder on the set of en-
forcements, i.e. (E(O), ≤), where by E(O), we denote the set of
equivalence relations over O and ≤ is a total preorder on E(O) (the
relation ≤ has to be reflexive, transitive, and total).
4.2.1 Permissiveness. A prime instance of an optimality order
is the permissiveness. For an enforcement ξ, we define its permis-
siveness as |O/ξ|, i.e. the number of equivalence classes of ξ. For
a program π, we obtain a total preorder on the permissiveness of
the enforcements for π, where is ξ at least as permissive as ξ ′ if
|O/ξ| ≥ |O/ξ ′|. The least-permissive enforcement for a program π
is the relation ξ⊥ = O × O. The program Enf(π , ξ⊥) can be seen as
the program that always returns a deny decision. This is because
Enf(π , ξ⊥) always returns O and the attacker’s revised belief equals
her prior belief. Conversely, the most-permissive enforcement for π
is ξ⊤ = {(o, o) | o ∈ O}. The program Enf(π , ξ⊤) can be seen as
identical to π as it always outputs singleton equivalence classes
that contain the output returned by π. In fact, using the semantics
of EnfRnd, we get EnfRnd(π , ξ⊤) = π.
4.2.2 Answer precision. Another useful example of optimality
is answer precision, i.e. the number of singletons in an equivalence
relation, given by Sing(ξ) = |{o ∈ O | |[o]ξ | = 1}|. We define ξ ≥ ξ ′
iff Sing(ξ) ≥ Sing(ξ ′), i.e. ξ has at least as many singleton classes
as ξ ′. The set of least elements is given by [ξ⊥]≤ and it contains all
the equivalence classes having zero singletons. By answer precision,
all these enforcements are considered equally imprecise, since for
all of them there is never an exact answer (a singleton class). On
the other hand, the enforcement ξ⊤ is the unique most-precise
enforcement, since it is the only one that always gives an exact
answer.
Optimal Enforcement. An enforcement ξ is optimal for a pro-
gram π, an attacker belief δ, and a privacy policy Φ with respect
to an order ≤ if Enf(π , ξ), δ |= Φ and for any enforcement ξ ′ such
that Enf(π , ξ ′), δ |= Φ we have ξ ′ ≤ ξ.
To synthesize an optimal enforcement with regard to a partial
order (a more general notion), we can use the total preorder given
by the height of each element in the said poset. When we compute
an optimal enforcement with regard to the height total preorder,
the enforcement is also optimal with regard to the preorder. For
example, the permissiveness objective also guarantees optimality
with regard to the partial order given by set inclusion (i.e. E1 is
above E2 iff E1 ⊆ E2).
4.3 Synthesis Problem
We now define optimal privacy enforcement synthesis:
Definition 4.1. The optimal privacy enforcement synthesis prob-
lem is defined as follows:
A probabilistic program π, an attacker belief δ, a pri-
Input.
vacy policy Φ, and an optimality order ≤.
Output. An optimal enforcement ξ for π, δ, Φ, and ≤ if such
an enforcement exists; otherwise return unsat.
We remark on several key points. First, our synthesis problem is
decidable since (i) there are finitely many enforcements ξ for any
program with finitely many inputs I and outputs O and (ii) check-
ing Enf(π , ξ), δ |= Φ is decidable. For some notions of optimality,
like permissiveness, the synthesis problem is, however, NP-hard, as
we prove in Section 4.4. Second, we can check whether the synthe-
sis problem returns an enforcement ξ or unsat by checking whether
the attacker belief δ about the predicates in Φ are within their
corresponding bounds. Finally, for both the permissiveness and
answer precision, if π , δ |= Φ, then the synthesis problem returns
the most-permissive enforcement ξ⊤ = {(o, o) | o ∈ O} for π. The
synthesized enforcement thus does not unnecessarily change the
semantics of π whenever π already satisfies the policy for the given
attacker belief.
We remark that a solution to the synthesis problem can be used
to provide guarantees for a set of attacker beliefs {δ1, .., δn}. First,
we synthesize an enforcement ξi for each attacker belief δi. Then,
we take the union of the enforcements ξ = ξ1 ∪ .. ∪ ξn and return
the transitive closure ξ∗ of ξ. The synthesized enforcement ξ∗ is
guaranteed to satisfy the policy for all attacker beliefs δ1, ..., δn.
4.4 Complexity and Completeness
In this subsection, we give the results on the complexity of optimal
privacy enforcement synthesis problem and show that our notion
of enforcement is complete.
4.4.1 Complexity. For the case of permissiveness optimality,
the optimal enforcement problem is NP-equivalent, as stated in the
following theorem.
Theorem 4.2. The optimal privacy enforcement synthesis problem
is NP-equivalent (NP-hard and NP-easy) for permissiveness.
As the theorem states, for the case of permissiveness, the problem
is NP-hard even for synthesis instances with singleton policies, i.e.
policies that contain only one security assertion. In Appendix A,
we prove that the problem is NP-hard by reducing the partition
problem to it, and we show that it is NP-easy by giving an NP-oracle
polynomial Turing machine that solves it.
For the case of answer precision, the synthesis problem can be
solved in polynomial time for instances with singleton policies (i.e.,
having one security assertion).
Theorem 4.3. For instances with singleton policies, the optimal
privacy enforcement synthesis problem is in PTIME for answer preci-
sion optimality.
The proof of this theorem can be found in Appendix A.4, where
we give a polynomial-time algorithm that produces optimal answer-
precision enforcements for singleton policies. It is an open problem
Session B4:  Privacy PoliciesCCS’17, October 30-November 3, 2017, Dallas, TX, USA396Algorithm 1: The algorithm SynSMT(π , δ, Φ)
Input: A probabilistic program π with outputs O,
an
attacker belief δ, a privacy policy Φ, and an objective
function Ψobj.
Output: An optimal enforcement mechanism ξ such that
Enf(π , ξ), δ |= Φ if one exists, otherwise unsat.
1 begin
2
3
4
5
ψassert ← Assert(π , δ, Φ)
if IsSat(ψassert) then
M ← Max(ψassert,ψobj)
return ker(M)
6
7
else
return unsat
whether for answer precision the synthesis problem is solvable in
polynomial time for the general case (i.e., for policies with multi-
ple security assertions). We conjecture that it is, since our greedy
algorithm SynGrd, presented in Section 6, produces optimal en-
forcements for all our benchmarks.
4.4.2 Completeness. Our notion of enforcement is complete: for
a program π, a prior belief δ, and a policy Φ, there exists a valid
enforcement ξ (ξ is valid if δ, Enf(π , ξ) |= Φ) if and only if there
exists some arbitrary program π ′ satisfying the policy Φ for δ, i.e.
π ′, δ |= Φ. In other words, in the case no valid enforcement exists,
it is not by a shortage of enforcement equivalent relations, but
because of the attacker’s prior belief δ and the policy Φ that cannot
be satisfied by any program.
Theorem 4.4. Let I be a set of inputs and O a set of outputs,
δ ∈ D(I) an attacker belief, and Φ a privacy policy. There exists a
program π ∈ MI, O such that π , δ |= Φ if and only if for any program
π ′ ∈ MI, O we have that Enf(π ′, ξ⊥) |= Φ.
The proof of this is in Appendix A.
5 SMT-BASED SYNTHESIS ALGORITHM
We now present our synthesis algorithm SynSMT.
High-level Idea. SynSMT takes as input a probabilistic program
π ∈ MI×O defined over inputs I and outputs O, an attacker belief
δ ∈ D(I), and a privacy policy Φ. It is based on two key insights.
First, we represent the search space of possible enforcements ξ ∈
O × O with integer variables C1, . . . , C|O|, and then encode the
satisfaction of Enf(π , ξ), δ |= Φ into SMT constraints over these
variables. The encoding guarantees that any model of the SMT
constraints identifies an enforcement ξ such that Enf(π , ξ) |= Φ.
Second, we encode the optimality ordering over enforcements as
an objective function that returns the rank of an enforcement (e.g.
the number of equivalence classes, or the number of singletons)
and use this function as an optimization goal.
Key Steps. We introduce an integer variable Ci for each output oi ∈
O to encode all possible enforcements. Each Ci is assigned a value
from {1, . . . , |O|} that represents the equivalence class to which oi
belongs; e.g., if only Ci and Cj are set to k, then the equivalence
class Ek is {oi , oj}. A model is a mapping M : {C1, . . . , C|O|} →
Assert(π , δ, Φ) := ψrange ∧ ψbounds
Ci ≥ 1 ∧ Ci ≤ |O|
j
non-empty ⇒ p
ψ
j
ℓ
∈ [aℓ, bℓ]
ψrange ≡
ψbounds ≡
j
non-empty ≡
ψ
|O|
j=1
i =1
|O|
|Φ|
|O|
|O|
ℓ=1
p
=
j
ℓ
Ci = j
i =1
i =1[Ci = j] · Pπ
|O|
Objcls(n) := maximize |O|
Objsing(n) := maximize |O|
δ (I ∈ Sℓ | O = oi) · Pπ
δ (O = oi)
(cid:105)
(cid:104)
i =1[Cj = i](cid:17)
(cid:104)(cid:16)|O|
j
non-empty
i =1[Ci = j] · Pπ
j=1
j=1
ψ
= 1(cid:105)
δ (O = oi)
Figure 5: SMT constraints Assert(π , b, Φ) and two objective
functions (Objcls(n) for permissiveness and Objsing(n) for
answer precision) for a synthesis instance with a program π,
with outputs O = {o1, . . . , on}, attacker belief δ, and privacy