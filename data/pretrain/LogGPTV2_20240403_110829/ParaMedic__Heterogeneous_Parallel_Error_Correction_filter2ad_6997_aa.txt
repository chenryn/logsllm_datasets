title:ParaMedic: Heterogeneous Parallel Error Correction
author:Sam Ainsworth and
Timothy M. Jones
2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)
ParaMedic: Heterogeneous Parallel Error Correction
Sam Ainsworth, Timothy M. Jones
University of Cambridge, UK
{sam.ainsworth,timothy.jones}@cl.cam.ac.uk
Abstract—Processor error detection can be reduced in cost
signiﬁcantly by exploiting the parallelism that exists in a repeated
copy of an execution, which may not exist in the original code, to
split up the redundant work on a large number of small, highly
efﬁcient cores. However, such schemes don’t provide a method
for automatic error recovery.
We develop ParaMedic, an architecture to allow efﬁcient
automatic correction of errors detected in a system by using
parallel heterogeneous cores, to provide a full fail-safe system
that does not propagate errors to other systems, and can recover
without manual intervention. This uses logging to roll back any
computation that occurred after a detected error, along with a
set of techniques to provide error-checking parallelism while still
preventing the escape of incorrect processor values in multicore
environments, where ordering of individual processors’ logs is
not enough to be able to roll back execution. Across a set of
single and multi-threaded benchmarks, we achieve 3.1% and
1.5% overhead respectively, compared with 1.9% and 1% for
error detection alone.
Keywords—fault tolerance; microarchitecture; error detection
I. INTRODUCTION
Fault tolerance is an increasingly important property for com-
puter processors. As transistors shrink, both hard (permanent)
and soft (transient) faults become more common in silicon
chips [1], [2], [3], as a result of increased variability, lowered
energy required to cause a transistor to ﬂip bits from, for
example, cosmic rays, and increased points of failure [4]. In
addition, fault-intolerant computation is becoming increasingly
important. Automotive applications require strict safety stan-
dards to be met, and thus typically require fault detection
and correction for all safety-critical components [5], [6],
[7]. With the advent of self-driving cars, the performance
requirements make traditional fault-tolerance schemes, such
as lock-stepping [6], impractical, as they double silicon area
and CPU power consumption. Similarly, many scientiﬁc and
large-scale applications are increasingly error intolerant due to
the number of potential failures in such systems.
A recent innovation in this area is heterogeneous parallel
error detection [8]. This allows error detection to be achieved
at a signiﬁcantly reduced power-performance-area (PPA) over-
head compared with previous schemes, by exploiting new
parallelism that exists in code that has already been executed
once, to allow the checking to be done on parallel checker
cores, each of which is orders of magnitude smaller and lower
power [9], [10] than a traditional out-of-order superscalar [11].
However, previous work does not extend this to cover cor-
rection of faults in a system, and so errors are allowed to
propagate outside the sphere of replication, and returning to
a safe state must be achieved by manual intervention. Indeed,
there are a variety of factors that make this challenging. The
increased latency of error detection for such schemes when
compared with lock-stepping and redundant multi-threading,
necessary to exploit parallelism, makes rolling back incor-
rect writes in the correct order a challenge. Keeping track
of unchecked state in an efﬁcient manner is also difﬁcult:
handling this in software is too expensive, as anything that
has been written potentially may need to be rolled back. Hard
errors are challenging to correct, as the hardware running
the application will repeatedly exhibit the same error, and
the requirement of checking for errors before the result is
propagated out of the system, necessary for correct behaviour
outside of a sphere of replication [4], means high checking
latency may be impractical for performance.
We develop ParaMedic, a parallel error-correction technique
to solve these issues for both single and multi-core processors,
and make hard- and soft-error correction in hardware both
practical and highly efﬁcient. To extend heterogeneous parallel
error detection [8] to full correction, we use solutions inspired
by transactional memory [12]: ﬁne-grained eager versioning
to support efﬁcient rollback from many different concurrent
checkpoints, combined with coarser-grained lazy versioning
to create a total order on rollback between multiple parallel
cores. To maintain correct behaviour even in the presence of
hard-errors, we develop a novel hard-fault log to guarantee
forward progress. We can provide error-correcting codes for
correct rollback by reusing resources from error detection [8]
and the existing cache infrastructure. And we can dynamically
extract the maximal amount of error-correction latency and
parallelism from an application while still allowing correct
executions to propagate out of their fault domain quickly, by
dynamically adjusting checkpoint frequency using an additive-
increase multiplicative-decrease technique.
Across a set of single and multi-threaded benchmarks [13],
we achieve 3.1% and 1.5% overhead respectively, compared
with 1.9% and 1% for error detection alone.
II. BACKGROUND
A. Dual-Core Lockstep
Current techniques for reliability in commercial systems tend
to involve dual-core lockstep [6], [14]. The same code is run
on two identical copies of a processor, usually with some
delay to avoid correlated errors, and the results compared. This
approximately doubles core area and power consumption, as
everything has to be run twice in the same way.
A dual-core lockstep detection system can extend to cor-
rection by adding a further processor [6], where a majority
vote is taken on instruction commit. However, overheads
978-1-7281-0057-9/19/$31.00 ©2019 IEEE
DOI 10.1109/DSN.2019.00032
201
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:52:36 UTC from IEEE Xplore.  Restrictions apply. 
become signiﬁcantly worse, now requiring three times the core
area and power of an unprotected system. For this reason,
academics have considered schemes based on additional re-
dundant threads instead.
B. Redundant Multi-Threading
In redundant multi-threading [4], [15], [16], [17] the same
code is run twice within two different threads on the same
core, typically with hardware forwarding of load and store
values, to check correctness. This suffers from a signiﬁcant
reduction in performance compared to no checking, greatly
increasing energy consumption, and is unable to detect hard
faults without introducing spatial diversity [18], as the same
hardware is used for both the check and the original execution.
Since threads are decoupled from each other, errors cannot
be caught before instructions retire, and so can propagate
into main memory [16]. This means that typically software
or hardware checkpoints are used to revert to a correct state.
However, it is possible to couple the threads more tightly [19]
to ensure the checker thread executes before the main thread
commits, though this heavy restriction in scheduling decreases
performance even further.
C. Heterogeneous Parallel Cores
A recent alternative architecture for error detection appends a
set of small, power-efﬁcient cores to a main high-performance
core to perform the redundant computation in parallel [8]. The
key insight is that running code a second time to check its
correctness is more parallel than the original execution. It is
possible to split up the application by taking periodic register
checkpoints, then overlap the checking of the code between
multiple checkpoints. Due to the parallelism available in the
second run, a set of simple cores (the checker cores) can
together provide enough computational power to keep up with
a high-performance core.
To allow the checker cores to replay load values, and
check store values and load and store addresses, all loads
and stores are extracted in-order from the program stream at
commit time within the large out-of-order processor used as
a main core. These are placed into a load-store log, which is
partitioned such that it is divided equally between the checker
cores. When a segment is ﬁlled, or an instruction timeout is
reached, a new checkpoint is triggered, and a check between
the previous checkpoint and the new one is started on the
corresponding checker core. Stores are allowed to propagate
to main memory before they are checked, to avoid impacting
performance signiﬁcantly.
This approach to error detection is highly advantageous,
with performance, area and power overheads of 1.8%, 24%
and 16%, respectively compared to a processor without check-
ing [8]. When compared with more conventional lock-stepping
schemes [6], the area and power requirements are reduced
signiﬁcantly from the 100% cost in both dimensions caused
by doubling the core. This makes heterogeneous parallel
error detection the most suitable starting point for an error-
correction scheme.
D. Challenges for Error Correction
Although at ﬁrst glance it seems straightforward to augment a
parallel error-detection scheme with circuitry for correcting er-
rors, there are a number of challenges that present themselves.
Tight coupling of parallel error-detection circuitry with the
main core is infeasible because a large number of instructions
need to be executed, without being checked, to achieve paral-
lelism in the detection. We must therefore design a system that
can tolerate much more latency between error initiation and
detection, without sacriﬁcing correction ability. This means
we must be able to log a large number of potentially incorrect
stores to be able to roll them back. It also means that we need
to be able to deal with the complexities of multicore shared
memory, where errors that remain uncaught for long periods
of time may propagate around the system. Still, there is also an
opportunity because, unlike in dual-core lockstep or redundant
multithreading, many copies of hardware capable of executing
code exist, as detection is achieved via multiple checker cores
that have the same capabilities as the main core. This means
that a majority vote can be achieved without tripling or even
doubling the hardware overheads, and so even hard errors can
be corrected efﬁciently. We take these challenges forwards
in this paper to design ParaMedic, a parallel error-correction
architecture, considering the requirements for a single core
setup (section III) before extending to multicore (section IV).
III. SINGLE-CORE CORRECTION
Figure 1 shows an overview of ParaMedic, our system for het-
erogeneous error correction. The hardware we add for single-
core correction is coloured green; structures for multicore
correction (described in section IV) are orange with hatching.
Execution proceeds in the same manner as with heteroge-
neous error detection, with the following changes. Addresses
and data for loads and stores are placed into the load-store
log in program order, along with ECC to protect them. Each
segment of the load-store log obtains a timestamp each time
it starts being ﬁlled, so as to create an ordering between seg-
ments. Once full, the associated checker core starts validating
the segment’s contents by re-executing all instructions. In the
common, error-free case, the segment can be reused once
all segments with earlier timestamps have been successfully
validated. However, on detection of an error, execution is
stopped and state reverted to the register checkpoint at the
start of the erroneous log segment by rolling back each load-
store log segment in reverse sequential order, so as to “undo”
each of the stores that has taken place up to that point. The
main core then starts running the application again, starting
from this checkpoint.
The following sections describe these operations and the
extensions we require to achieve this form of execution.
A. Partitioned Load-Store Undo Log
As in the scheme for error detection only [8], a hardware
SRAM log records past loads and stores, so that the smaller
checker cores can replicate load data, and check store data
and load and store addresses. This is partitioned, with separate
202
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:52:36 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 1: ParaMedic: a heterogeneous parallel error-correction system. Units required for single-core error correction are
presented in green. Units for multi-core correction are presented in orange with hatching. Structures for necessary for detection
of errors are in grey, and the main out-of-order core in blue.
segments of the log for each checker core. We store virtual
addresses stored in the load-store log, as this removes the need
for translation by the checker cores. Writes are sent to the load-
store log after having ECC for the cache calculated on them,
so that any errors before this will propagate into the log and be
caught, and any on the separate cache path will be corrected
by ECC bits. Load data is duplicated by the load forwarding
unit before being forwarded, and errors within log forwarding
cause errors to be detected on future checks.
To enable correction of errors in the system, we must be
able to revert writes that may be incorrect, since they occur
after a check fails. Unchecked data that is potentially incorrect
propagates into the cache system, to allow efﬁcient forwarding
of writes to new computation. We therefore take a copy of the
old value of each word written to the L1 cache and record it in
the load-store log, so as to provide the ability to undo stores.
We do this for every write, extending the amount of data stored
per write compared to error detection alone. The data ﬁelds
recorded for each load and store are shown in ﬁgure 2. We also
add a dedicated load/store bit, for the unroller to determine
which log segments are loads or stores, which the detection
mechanism infers from the instruction stream. On detection of
an error, these writes are then rolled back by walking the log in
reverse order, and writing the old values back to the cache. The
virtual addresses in the load-store log are retranslated by the
TLB upon a rollback, moving the translation to the uncommon,
rather than the common case, as translation does not need to
be performed for correct segments.
B. ECC
Error detection by itself does not require ECC in the load-
store log. Load data forwarded to the load-store log from the
cache takes a separate path from the connection to the main
core’s load-store buffer, so there is never a point where there
is a single copy of unprotected data. Errors in addresses or
data held in the log will be detected, because the error occurs
after the original execution and the two will diverge. This also
holds true for error correction in the case of load and store
data. However, if an error were to occur in the store address
or the old data values (those overwritten by the store), we
would either write to an incorrect address or write incorrect
data in the event of a rollback. For correction, therefore, we
must protect store addresses and old data values with ECC.
To avoid recomputation of ECC bits [20] for the data values,
it is useful to recover as much of this information as possible
from the cache. We assume that ECC bits are stored per word
in the cache [21], [22], and copy those directly. While the
cache will have ECC bits for the address tag in the cache,
these are likely to represent the physical address, and cover
only line rather than byte granularity, and so we calculate a
new ECC for the virtual address on every store. This ECC
data is then stored in the load-store log (see ﬁgure 2).
C. Commit Ordering
Once a checker core has ﬁnished execution of its load-store
log segment, it validates the register checkpoint at the end of
the segment (and the beginning of the next). Depending on
how many instructions are in each segment, and the types of
instruction, these checks can complete out-of-order. If we are
only detecting errors, a checker core’s log segment can be
reallocated immediately after a check is complete, because if
the check is successful, the data is no longer needed. However,
when correcting errors, we can only allow this to occur once
we are certain that there are no errors in earlier segments (i.e.,
those containing older program instructions), which may take
more time to check if they contain more or longer-latency
instructions, for example. If earlier log segments do contain
errors then we need to use the current segment to roll back
203
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:52:36 UTC from IEEE Xplore.  Restrictions apply. 
register checkpoint has been validated. This means the ECC
can be calculated in parallel with the computation check, does
not need to be calculated when the checkpoint is taken, and
does not sit on any performance-critical path.
E. Error Recovery
On the detection of an error, ParaMedic must roll the system
back to a consistent state. We do this by stopping the main
core, stopping all checker cores with timestamps that are later
than the one exhibiting the error, then rolling back all ﬁlled
and partially ﬁlled load-store log segments, starting with the
youngest (i.e., most recent instructions), in reverse order, until
the segment with the error has been rolled back. The register
ﬁle of the main core is then restored to that at the start of
the segment with the error. By storing a read/write bit in the
log for each entry (ﬁgure 2), which is unneeded for the initial
check as it can be determined from the instruction stream, we
can quickly identify old write data.
As there may be multiple writes to the same location,
we must perform this in sequential reverse order to regain
a consistent state. While this is a slow operation, it occurs
infrequently, as hard and soft errors are relatively uncommon.
Further, we can place bounds on it by altering the number
of instructions per log segment accordingly, at the expense of
greater checkpoint overhead for smaller log segments. Since
the load store log stores virtual addresses, to avoid retrans-
lation during checking, we assume the TLB and page table
walkers are protected using their own redundancy mechanisms
such as ECC, to prevent the need for retranslation in the
common case and reduce the amount of translation logic
required. This means that, for a rollback, translation must
be repeated to re-retrieve the physical address. Any changes
in translation map state between execution and recovery will
naturally be rolled back by reversing all subsequent writes.
Events such as exceptions are speculatively handled by the
main core, as with all other computation, before checks are
completed. If this exception later turns out to be incorrect (for
example, a soft error changes a value which causes an out-of-
bounds memory access), the exception handling is naturally
rolled back by the undoing of all loads and stores.
An earlier check that hasn’t yet completed may fail after
a later one in program order, particularly when we have hard
faults in a system. In this case, we must wait for the future
rollback to complete before proceeding to roll back further.
On a failure, we increment the current commit timestamp,
as opposed to returning to the timestamp before the error
occurred. This, as we shall see later (sections III-F and IV-A),
reduces the amount of book-keeping in the cache without
affecting correctness, and enables hard-fault tracking. We mark
all checkpoints after the error as “rolled back”, equivalent
to “committing”, save for the fact
that no new roll back
is necessary on new errors, but do not commit them until
all previous checks have completed. This ensures correct
behaviour even if an earlier error is later detected. An example
of this behaviour is shown in ﬁgure 3.
Fig. 2: Structure of entries in the load-store log. Loads use two
64-bit entries; stores use three. We assume that ECC bits give