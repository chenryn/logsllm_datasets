investigating this phenomena, we realized that for a speciﬁc
packet length, 161, the unsanitized model included a cen-
troid for length 161 that caused many false positives. The
sanitized model did not contain a speciﬁc centroid created
for length 161 (the packets with this length were considered
abnormal in the sanitization phase) and the closest length
centroid (178) was used for the testing phase.
Overall, our experiments show that the AD signal-to-
noise ratio (i.e., T P/F P ) can be signiﬁcantly improved
even in extreme conditions, when intrinsic limitations of the
anomaly detector prevent us from obtaining a 100% attack
detection, as shown in Table 1. Higher values of the signal-
to-noise ratio imply better results. There is one exception:
Payl used on the www data set. In this case, the signal-to-
noise ratio is slightly lower, but the detection rate is still
higher after using sanitization.
To stress our system and to validate its operation, we
also performed experiments using trafﬁc in which we in-
jected worms such as CodeRed, CodeRed II, WebDAV, and
a worm that exploits the nsiislog.dll buffer overﬂow vulner-
ability (MS03-022). All instances of the injected malcode
were recognized by the AD sensors when trained with san-
itized data. That result reinforced our initial observations
about the sanitization phase: we can both increase the prob-
ability of detecting a zero-day attack and of previously seen
malcode.
3.2 Analysis of Sanitization Parameters
We have seen how our sanitization techniques can boost
the performance of the AD sensors. Our results summarize
the FP and the detection rates as averaged values obtained
for the optimal parameters. We next explore these parame-
ters and their impact on performance with a more detailed
analysis using Anagram. We show the optimal operating
point for any sensor can be identiﬁed automatically with of-
ﬂine tuning that requires no manual intervention.
There are three parameters we need to ﬁne-tune:
the
granularity of the micro-models, the voting algorithm, and
the voting threshold. In order to determine a good granu-
larity, we have to inspect the volume of trafﬁc received by
each site (given the characteristics of the chosen anomaly
detector) such that we do not create models that are under-
trained. In our initial experiments, we used 3-hour, 6-hour,
and 12-hour granularity. We employed both the simple and
weighted voting algorithms proposed in Section 2. The
threshold V is a parameter that needs to be determined once.
It depends on the training set and the site/application mod-
eled by the sensor. As we show, both the optimal values of
V and the micro-model granularity appear to be the same
for all the sites in our experiments.
In Figures 1 and 2, we present the performance of the
system when using Anagram enhanced with the sanitiza-
tion method applied on the www1 trafﬁc. We notice that the
weighted voting algorithm appears to be a slight improve-
ment. We seek a value for V that maximizes detection and
achieves the lowest possible FP rate. We can observe that
the sanitized model built using the 3-hour micro-models
shows the best performance, achieving a detection rate of
100% and minimizing the FP rate. The granularity and the
voting threshold are inversely proportional because for the
same data set fewer models are built when the granularity is
increased.
In Figures 3 and 4, we present the results for www and
lists for a granularity of three hours and for both types of
voting techniques. The best cases for these two sites are
reached at almost the same value as the ones obtained for
www1. We observe that the best case is where V has the
minimum value 0.01 (this is dependent on the training data
set).
To verify these results, we studied the impact that gran-
ularity has on the performance of the system. We ﬁx the
voting threshold, and we sample a large range of granular-
ity values. This analysis allows us to determine the best
granularity. In Figure 5, we can observe that the granular-
ity of three hours performs the best, given the two threshold
bounds 0.15 and 0.45 obtained from the previous experi-
ments. For all other values of V ∈ (0.15, 0.45), the granu-
larity of three hours seemed to be the optimal choice. Notice
that for V = 0.45, all values of granularity from 3-12 hours
are optimal but not for V = 0.15.
When using Payl, the granularity of three hours again
performs the best, given the two threshold bounds 0.15 and
0.55. Payl behaves differently than Anagram due to its dif-
ferent learning algorithm. The way the models are built is
more dependent on the number of training samples because
models are created for each packet length.
As we mentioned previously, our technique assumes the
use of a large training dataset in order to increase the prob-
ability that an individual datum which is normal is not in-
correctly deemed an anomaly. To analyze the impact that
training data set size has on performance, we tested our
methodology on Anagram using a certain percentage of the
micro-models, starting from a randomly chosen position in
the training dataset, as shown in Figure 7. This experiment
uses 300 hours of training data, a granularity of three hours
per mico-model, the weighted voting scheme, and a thresh-
old of V = 0.45. The FP rate degrades when only a percent-
age of the 100 models is used in the voting scheme. Another
factor is the relationship between the internal threshold of
the sensor, τ , and the voting threshold, V , and the way it
inﬂuences the performance of the system. Intuitively, if the
anomaly sensor is more relaxed, the data seen as anomalous
by the micro-models will decrease. As a result, the sanitized
model will actually increase in size and exhibit a smaller FP
rate as shown in Figure 8. Although using a “relaxed” AD
can improve the FP rate, we do not advocate such an ap-
proach to the extreme. In our experiments, the threshold for
Anagram was set to τ = 0.4, and we analyzed the effect of
changing the internal threshold had over the performance
of our system. We observed that if we increase the internal
threshold too much, the FP rate decreases along with the
detection rate.
3.3 Computational Performance Evalua-
tion
Using an AD sensor to classify input, especially if the
AD sensor is operating inline with the input stream, can be
expensive (in terms of latency for benign requests). We ex-
amine the average time it takes to process a request, and the
impact that sanitization has on this time. In addition, we es-
timate the overall computational requirements of a detection
system consisting of an AD sensor and a host-based shadow
sensor. The AD sensor acts as a packet classiﬁer that diverts
all packets that generate alerts to the shadow sensor while
allowing the rest of the packets to reach the native service.
This architecture effectively creates two service paths.
Our goal is to create a system that does not incur a pro-
hibitive increase in the average request latency and that can
scale to millions of service requests. Due to the overhead of
the shadow sensor, we cannot redirect all trafﬁc to it. There-
fore,we want the AD to shunt only a small fraction of the
total trafﬁc to the shadow. The shadow sensor serves as an
oracle that conﬁrms or rejects the AD’s initial classiﬁcation.
Although one could argue that using a shadow sensor
alone is sufﬁcient to protect a system from attack (and there-
fore we have scant need of a robust anomaly sensor in
87
Figure 1. Performance for www1 for 3-hour
granularity when using simple voting and
Anagram (V is the voting threshold; see section 2)
Figure 2. Performance for www1 when us-
ing weighted voting and Anagram (V is the
voting threshold)
the ﬁrst place), shadow sensors have signiﬁcant shortcom-
ings. First, they impose a hefty performance penalty (due to
the instrumentation, which could include tainted dataﬂow
analysis, a shadow stack, control-ﬂow integrity, instruction
set randomization and other heavyweight detectors). Us-
ing a shadow sensor without the beneﬁt of an AD sensor to
pre-classify input would be unacceptable for many environ-
ments. Second, a shadow requires synchronization of state
between it and the shadowed “production” application. In
many environments, this is a difﬁcult task. Finally, shadow
sensors have only an incomplete notion of what malicious
behavior is:
they use instrumentation aimed at detecting
certain classes of attacks. Thus, a shadow sensor is not a
perfect oracle. It serves only to offer a lower bound on the
removal of attacks (and it completely misses abnormalities)
if it were used to directly “sanitize” data sets.
For our performance estimation, we used two instrumen-
tation frameworks: STEM [21] and DYBOC [1]. STEM
exhibits a 4400% overhead when an application such as
Apache is completely instrumented to detect attacks. On the
other hand, DYBOC has a lighter instrumentation, provid-
ing a faster response, but still imposes at least a 20% over-
head on server performance. Given that we know ground
truth based on the attacks these sensors detect, we can esti-
mate what the answers of the shadow servers would be. We
can also estimate the overall overhead based on the reported
performance of the frameworks in [21] and [1].
To compute the overall overhead, we borrow the method
used in [29], where the latency of such an architecture is
deﬁned as following:
(cid:1) = (l ∗ (1 − f p)) + (l ∗ Os ∗ f p)
l
where l is the standard (measured) latency of a protected
service, Os is the shadow server overhead, and f p is the
AD false positive rate.
To quantify the performance loss/gain from using the
sanitization phase, we compare the average latency of the
system when using Payl and Anagram with sanitized and
non-sanitized training data. Table 2 shows that the alert rate
for both sensors does not increase by much after sanitizing
the training data, and in some cases fewer numbers of pack-
ets will have to be processed by the shadow server (lists
when using Payl).
Table 2. Latency for different anomaly detec-
tors
Sensor
STEM
DYBOC
44
1.2
1.2
44
44
www1 www lists www1 www lists
1.2
N/A
1.0301 1.0043 1.0172 1.0001 1.0000 1.0000
A
1.0172 1.1247 1.0215 1.0000 1.0005 1.0000
A-S
1.0430 1.0002 1.0006 1.0002
A-SAN 1.0430 1.462
1.3612 3.5886 28.5802 1.0016 1.0120 1.1282
P
P-SAN 3.8552 5.4849 2.0320 1.0132 1.0208 1.0048
3.4 Long-lasting Training Attacks
In some cases a worm may appear in all micro-models
as well as the training dataset of the sanitized model. This
scenario represents what we call a long-lasting training at-
tack.
In this attack, the adversary continuously targets a
particular site such that the modeling process is disturbed.
88
Figure 3. Performance for www for 3-hour
granularity when using Anagram (V is the
voting threshold)
Figure 4. Performance for lists for 3-hour
granularity when using Anagram (V is the
voting threshold)
This situation is not covered by our previous experiments,
which injected real worms into real trafﬁc such that each
one appeared in a small fraction of the micro-models. To
test our methodology in such an extreme case, we injected a
speciﬁc attack packet (in our case mirela) into every micro-
model and the dataset from which the sanitized model was
computed. Table 3 compares poisoned and “clean” or non-
poisoned sanitized models. The results were obtained us-
ing Anagram, weighted voting, a granularity of three hours,
and V = 0.35. We can see that this method can evade our
architecture. For this reason, we next investigate ways to
alleviate the impact of long-lasting training attacks.
Table 3. Long lasting training attacks
lists
www1
www
Sanitized
model
non-poisoned 0.13
poisoned
0.10
FP(%) TP(%) FP(%) TP(%) FP(%) TP(%)
100
29.29
0.26
0.26
100
38.27
0.10
0.10
100
35.80
4 Collaborative Sanitization
Section 3.4 noted that our local sanitization architecture
has a weakness in the presence of long-lasting attacks in
the initial set of training data. Because attack data may
span multiple micro-models, it can poison a large portion
of them. Since we predicate our cleaning capability on
micro-model voting, extensive poisoning of the training
data would seriously deteriorate our ability to detect long-
lived or frequently occurring attack payloads. We hypoth-
esize, however, that the distribution of such long-lived at-
tacks among Internet hosts would require an adversary with
89
signiﬁcant resources (e.g., a potentially large number of
source IP addresses) — a requirement that effectively limits
the scope of such attack to few target hosts or networks.
Given this hypothesis, we can counter the effects of such
attacks by extending our sanitization mechanism to sup-
port sharing models of abnormal trafﬁc among collaborat-
ing sites. Sharing these models enables a site to re-evaluate
its local training data2. Our goal is to enhance the local view
of abnormal behavior characteristics (rather than normal be-
havior characteristics, which cannot be meaningfully shared
because they are unique to an individual site). As we will
show, “cross-sanitization” between sites boosts our ability
to remove long-lived or frequent attacks from the training
data (regardless of whether or not the attack data is “tar-
geted”, i.e., injected speciﬁcally to blind the sensor). Col-
laboration on this scale is not unheard of. Collaboration
exists in the real world, and we believe it can exist in the
digital world. In particular, examples of organizations em-
ploying a collaborative approach to defense include the De-
partment of the Treasury and the FSISAC leveraging the tal-
ent and resources of the ﬁnancial community to protect the
community as a whole. The DISA/DoD manages and lever-
ages information from its many customer .mil hosts. Uni-
versities with hundreds of divisions and units across schools
and departments also follow this model.
4.1 Cross-Sanitization
In some sense, attack vectors that saturate training data
deﬁne normal trafﬁc patterns. Local knowledge alone may
2To alleviate the privacy concerns of sharing content, these models may
incorporate privacy-preserving representations [12, 18].
Figure 5. Granularity impact on the perfor-
mance of the system for www1 when using
Anagram
Figure 6. Granularity impact on the perfor-
mance of the system for www when using
Payl
not provide enough evidence to weed out consistent attack
vectors in training data. To isolate and remove these vec-
tors, we need to incorporate knowledge from some other
remote source. This information sharing is the essence of
cross-sanitization: comparing models of abnormality with