# Detecting and Characterizing Social Spam Campaigns

## Authors
- **Hongyu Gao**  
  Northwestern University, Evanston, IL, USA  
  Email: [PI:EMAIL]
- **Jun Hu**  
  HUST, Wuhan, China  
  Email: [PI:EMAIL]
- **Christo Wilson**  
  U. C. Santa Barbara, Santa Barbara, CA, USA  
  Email: [PI:EMAIL]
- **Zhichun Li**  
  Northwestern University, Evanston, IL, USA  
  Email: [PI:EMAIL]
- **Yan Chen**  
  Northwestern University, Evanston, IL, USA  
  Email: [PI:EMAIL]
- **Ben Y. Zhao**  
  U. C. Santa Barbara, Santa Barbara, CA, USA  
  Email: [PI:EMAIL]

## Abstract
Online social networks (OSNs) are powerful tools for collaboration and communication, but they can also be exploited for spam campaigns and malware distribution. In this study, we present an initial analysis of coordinated spam campaigns on OSNs. Our system detected approximately 200,000 malicious wall posts with embedded URLs, originating from about 57,000 accounts. We found that more than 70% of these posts were promoting phishing sites.

## Categories and Subject Descriptors
J.4 [Computer Applications]: Social and behavioral sciences

## General Terms
Human Factors, Measurement, Security

## Keywords
Online social networks, Spam, Spam Campaigns

## 1. Introduction
Online social networks (OSNs) are valuable collaboration and communication platforms for millions of users. However, recent evidence suggests that these trusted communities can be used to spread malware and conduct phishing attacks. Popular OSNs have become targets for large botnets, and compromised or fake accounts are being used to distribute phishing and spam messages, often disguised as communications from friends and family.

In this project, we present the first attempt to detect and analyze the prevalence of malicious users and the spread of malicious content on an OSN. We conducted our study on Facebook, the world's most popular OSN with over 400 million users. We used data crawled from April to June 2009, focusing on eight regional networks of varying sizes (from over 1.6 million users to approximately 14,000 users). For each user, we recorded their profile, friend list, and interaction records dating back to January 1, 2008. This dataset includes information on over 3.5 million users and more than 187 million wall messages.

We employed correlation between wall messages, both in textual content and contained web addresses, to identify the spread of potentially malicious content. Our results were validated using several mechanisms. Our subsequent analysis provides insights into the operation of malicious accounts and has significant implications for the design of future detection mechanisms.

## 2. Malicious Campaign Detection and Validation

### 2.1 Design Overview
Our system workflow is guided by the techniques commonly used in spam campaigns. Based on recent research and observations, we inferred that spam wall posts are generated using templates, and posts from the same template should have only minor differences. Therefore, we grouped wall posts with similar textual content together. Additionally, we recognized that attempts to direct viewers to a single destination URL likely originate from the same spam campaign. Thus, we grouped all wall posts containing the same destination URL, including those with obfuscated URLs.

We modeled all wall posts as nodes in a large graph, building edges based on similar textual content or the same destination URL. During runtime, the system first compares the destination URL before computing the approximate textual similarity. Each connected subgraph could represent messages within the same spam campaign. We identified connected subgraphs by iteratively choosing arbitrary nodes and identifying their transitive closure as a cluster. The implementation is summarized in Algorithm 1.

**Algorithm 1: PostSimilarityGraphClustering(G)**
```python
traversed = ∅
clusters = ∅
for v in V:
    if v in traversed:
        continue
    one_cluster = BFS(v)
    traversed = traversed ∪ one_cluster
    clusters = clusters ∪ {one_cluster}
return clusters
```

After identifying distinct subgraphs, we applied threshold filters on the number of users sending wall posts in the subgraph and the time interval between consecutive wall posts to distinguish potentially malicious campaigns.

### 2.2 Detection Results
The clustering approach produced 1,402,028 clusters, with a small number of very large clusters and a large number of very small clusters. Using a threshold of at least 5 users per cluster and a maximum median interval of 5,400 seconds (1.5 hours) between consecutive wall posts, we classified 297 clusters as malicious, containing a total of 212,863 wall posts.

### 2.3 Experimental Validation
We applied a stringent set of heuristic tests to each URL in the detected malicious posts. The validation process included six steps, each confirming a subset of the detection results. Any unverified result was assumed to be benign (false positive). The validation results are shown in Table 1.

| Reason for Classification | # of URLs | # of Wall Posts |
|--------------------------|-----------|-----------------|
| Obfuscated URL           | 1,003 (6.3%) | 45,655 (21.4%) |
| Blacklisted              | 4,485 (28.0%) | 55,957 (26.3%) |
| Redirects to blacklisted URL | 4,473 (27.9%) | 29,365 (13.8%) |
| Contains spam keywords   | 196 (1.2%) | 19,018 (8.9%) |
| Groups with other malicious URLs | 5,300 (32.5%) | 33,407 (15.7%) |
| Manual confirmation      | 27 (<0.1%) | 16,380 (7.7%) |

Overall, our detection methodology resulted in a very low number of false positives and is fully automated, making it a significant step forward in detecting spamming activity on OSNs.

## 3. Campaign Analysis

### 3.1 Campaign Identification
We defined a "campaign" as a set of malicious posts of a certain type, such as pharmaceutical sales. We used the description part of the wall post to distinguish campaigns, without considering the destination URL. We classified the wall posts by identifying characteristic strings with human knowledge. Malicious posts that did not fit into any specific campaign were grouped into an "other" category. The identified campaigns and their descriptions are presented in Table 2.

| Campaign | Summarized Wall Post Description | Cluster # | Post # |
|----------|----------------------------------|-----------|--------|
| Crush    | Someone likes you                | 51,082    | 1,146,414 |
| iPhone   | Invitation for a free iPhone     | 31,329    | 166,681 |
| Blog     | Visit a (fake) Facebook profile  | 17,614    | 163,541 |
| PS3      | Invitation for a free PlayStation 3 | 14,092 | 144,011 |
| Webcam   | Video chatting via web camera    | 11,464    | 113,112 |
| Luxury   | Get cheap luxury product         | 10,683    | 112,711 |
| Ringtone | Invitation for free ringtones    | 6,317     | 98,111  |
| Pharma   | Pharmaceutical products like Viagra | 3,948 | 50,211  |
| Narcotics | Sell drugs                     | 3,556     | 40,421  |
| Love-calc | Test the love compatibility     | 2,707     | 1,440   |
| Macy-gift | Invitation for a free Macy’s gift card | 2,620 | 1,131   |
| Fake-video | A cool video is provided       | 2,125     | 502     |
| Pic-misuse | Your photo is misused online    | 1,440     | 4042    |
| Fake-fbid | Visit to read news              | 1,131     | 1127    |
| Is-that-you | Some webpage is about you     | 981       | 981     |
| iPod-touch | Invitation for a free iPod touch | 502     | 502     |
| Denigration | Someone is disparaging you    | 4042      | 4042    |
| Online-job | Work online and earn big money  | 64        | 64      |
| Other    | No apparent pattern              | -         | -       |

### 3.2 Attack Categorization
We studied the purpose of the attackers based on the campaigns. The attacker's goal was clear for some campaigns, such as product selling. For others, we relied on McAfee SiteAdvisor's user review summary of URLs within the campaign. We identified five different attacker goals, as shown in Figure 2.

The total size of all categories exceeds the total number of malicious posts because some posts have multiple goals. Phishing is the most common attack (∼70.3%), followed by malware propagation (∼35.1%) and product selling (∼17.6%).

### 3.3 Temporal Behaviors
We analyzed the temporal features of the identified campaigns, as illustrated in Figure ??. The horizontal direction represents the timeline during the data collection period. The spam campaigns are represented by different strips, with short, thin vertical lines corresponding to individual malicious posts and blocks reflecting bursts in the campaign. The figure clearly shows the bursty nature of all campaigns, with malicious posts densely distributed in a few relatively short time periods, even though the entire campaign may span a much longer time period.

## 4. Conclusion
In this poster, we described our work on detecting and characterizing spam campaigns on Facebook. We used automated techniques to group wall posts with strong similarities in advertised URLs or text descriptions. We identified about 200,000 malicious wall posts attributable to 57,000 malicious accounts, with over 70% of these attacks being phishing attacks. Our work demonstrates that automated detection techniques can be successfully used to detect online social spam.

## 5. References
[1] Mcafee siteadvisor. <http://www.siteadvisor.com/>

[2] Users of social networking websites face malware and phishing attacks. Symantec.com Blog.

[3] Zeus botnet targets Facebook. <http://blog.appriver.com/2009/10/zeus-botnet-targets-facebook.html>

[4] KREIBICH, C., KANICH, C., LEVCHENKO, K., ENRIGHT, B., VOELKER, G., PAXSON, V., AND SAVAGE, S. Spamcraft: An inside look at spam campaign orchestration. In Proc. of LEET (2009).

[5] Verisign: 1.5m Facebook accounts for sale in web forum. PC Magazine, April 2010.