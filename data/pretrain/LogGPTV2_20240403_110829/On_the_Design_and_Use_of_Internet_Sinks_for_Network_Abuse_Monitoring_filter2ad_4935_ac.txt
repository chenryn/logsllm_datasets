e
m
u
o
V
l
105
104
103
102
101
100
00:00
04:00
35
30
25
20
15
No. Sources
No. Scans
)
l
a
v
r
e
t
n
i
e
t
i
u
n
m
e
v
i
f
r
e
p
(
s
e
c
r
u
o
S
.
o
N
10
5
16:00
20:00
00:00
0
00:00
04:00
08:00
12:00
Time of Day (one day)
08:00
Time of Day (one day)
12:00
16:00
20:00
00:00
Fig. 7. Left: Volume/Count of type-1 port 139 scanners: 24 hours, Dec 14, 2003, (no. sources per
peak = 100, total sources = 2,177) Right: Volume of type-5 port139 scanners
2. Vertical lines that correspond to less common short duration spikes of SYN/ACK
and SYN/ACK/RST.
3. ICMP TTL exceeded packets could be attributed to either routing loops or DoS
ﬂoods with a low initial TTL.
Table 3 provides a summary of the number of active sources of backscatter trafﬁc,
i.e., the estimated count of the victims of spoofed source attacks. These numbers are
an average during the 12 hours shown in Figure 5 of the number of sources in each 5
minute sample. In terms of the distribution of the volumes of Backscatter scan types,
our results are consistent with those published in [19]. Backscatter made up a small
percentage (under 5%) of the overall trafﬁc seen on our service-provider sink.
We proceeded to setup a controlled experiment which began by trying to infect
a Windows 2000 host running on VMware with LovGate. LovGate uses a dictionary
attack, so we expected a machine with blank administrative password to be easily in-
fected. However, the NetBIOS sessions were continually getting rejected due to Net-
BIOS name mismatches. So we modiﬁed the lmhosts ﬁle to accept the name *SMB-
SERVER enabling us to capture the worm.
We veriﬁed that LovGate’s NetBIOS scanning process matched the proﬁle of the
type-1 scanners5. To date, we have not been able to disassemble the binary as it is a
compressed self-extracting executable. So we monitored the scans from the infected
host. There were two relevant characteristics that provide insight into the periodicity:
1) The scanning process is deterministic, i.e., after every reboot it repeats the same
scanning order 2) During the course of a day there are several 5-10 minute intervals
where it stops scanning. Our conjecture is that these gaps occur due to approximately
synchronized clocks in the wide area thus producing the observed periodicity.
SMTP Hot-Spot. Analysis of SMTP (Simple Mail Transfer Protocol) scans in the
service provider sink is another important demonstration of active sink’s capabilities.
From passive measurements, we identiﬁed an SMTP hot-spot i.e., there was one IP
5 Besides the NetBIOS scanning LovGate also sent SMTP probes to www.163.com.
158
Vinod Yegneswaran, Paul Barford, and Dave Plonka
     SYN
   SYN−ACK
negprot request
negprot reply
sess−setup request
sess−setup reply
tree−conn (IPC)
tree−conn reply
CreateX (LSARPC)
CreateX reply
RPC BIND
BIND ack
LSADS request
      SYN
   SYN−ACK
tftp −i 192.168.86.41
get systemse.exe
systemse.exe
Fig. 8. RBOT.CC timeline of
ploit(port 445)
lsarpc ex-
Fig. 9. RBOT.CC follow-up commands(port
44445)
address that was attracting a disproportionately large number of SMTP scans (20-50
scans per second). Hot-spots in unused address space are typically good indicators of
misconﬁguration. During a 10 day period in December we observed over 4.5 million
scans from around 14,000 unique IP addresses all bound to one destination IP within
our monitor. A cursory analysis suggested that these scans were all from cable-modem
and DSL subscribers. Finally, the scans also seemed to have an uncommon TCP SYN
ﬁngerprint (win 8192, mss 1456).
The possibility of spam software as a source of this anomaly was ruled out due
to the non-standard TCP ﬁngerprint. We then hypothesized that this could be from a
speciﬁc cable-modem or DSL device. We set up an SMTP responder on the target IP
address and captured the incoming email. This revealed the source of the email to be
misconﬁgured wireless-router/ﬁrewall systems from a major vendor6. The emails
are actual ﬁrewall logs!
To better understand the reasons behind this SMTP hot-spot, we examined the ﬁre-
wall system’s ﬁrmware. The unarj utility was used to extract the compressed binary.
However, searching for the hot-spot IP address string in the binary proved fruitless.
Examination of the ﬁrmware “application” revealed that there was an entry for SMTP
server that was left blank by default. This led us to conjecture that the target IP address
was the result of an uninitialized garbage value that was converted to a network ordered
IP address. It also turns out that every byte in our hot-spot address is a printable ASCII
character. So we searched for this four byte ASCII string and found a match in almost
all versions of ﬁrmware for this device. The string occurred in both the extracted and
compressed versions of the ﬁrmware. As a sanity check, we looked for other similar
ASCII strings, but did not ﬁnd them. These kind of hot-spots can have very serious
ramiﬁcations in network operations. For example, one the authors discovered a similar
problem with Netgear routers that inadvertently ﬂood our campus NTP servers [22].
6 We are in the process of notifying the manufacturer and plan to reveal the name of the vendor
once this is completed.
On the Design and Use of Internet Sinks for Network Abuse Monitoring
159
Experiences with Recent Worms. Our iSink deployment has proved quite useful in
detecting the advent of recent worms such as Sasser [5]. Without active response capa-
bility, such as that provided by the Active Sink, it would be impossible to distinguish
existing worm trafﬁc on the commonly exploited ports such as port 445 from new worm
activity. Detection of such new worms is often possible without modiﬁcations to the re-
sponder, as was the case for the lsarpc exploit used by Sasser. Our active response
system enabled accurate detection of not only Sasser, but also more ﬁne-grained classi-
ﬁcation of several variants. Prior to the release of Sasser, we were also able to observe
early exploits on the lsarpc service which could be attributed to certain strains of
Agobot. Figures 8 and 9 illustrate the interaction of RBOT.CC [30], a more recent
virus that also exploits the lsarpc vulnerability, with the Active Sink.
5 Basic Performance
One of the primary objectives of the iSink’s design is scalability. We performed scala-
bility tests on our Active Sink implementation using both TCP and UDP packet streams.
The experimental setup involved four 2GHz Pentium 4 PCs connected in a common lo-
cal area network. Three of the PCs were designated as load generators and the fourth
was the iSink system that promiscuously responded to all ARP requests destined to
any address within one class A network. Figures 10 demonstrates the scalability under
of LaBrea7 and Active Sink under TCP and UDP stress tests. The primary difference
between the TCP and UDP tests is that the TCP connection requests cause the iSink
machine to respond with acknowledgments, while the UDP packets do not elicit a re-
sponse. Ideally, we would expect the number of outbound packets to equal the number
of inbound packets. The Click-based Active Sink scales well to TCP load with vir-
tually no loss up to about 20,000 packets (connection attempts) per second. LaBrea
performance starts to degrade at about 2,000 packets. The UDP test used 300 byte UDP
packets (much like the SQL-Slammer worm). In this case, both the LaBrea and Active
Sink perform admirably well. LaBrea starts to experience a 2% loss rate at about 15,000
packets/sec.
6 Sampling
There are three reasons why connection sampling can greatly beneﬁt an iSink architec-
ture: (i) reduced bandwidth requirements, (ii) improved scalability, (iii) simpliﬁed data
management and analysis. In our iSink architecture, we envision building packet-level
sampling strategies in the Passive Monitor and source-level sampling in the NAT Filter.
We considered two different resource constraint problems in the passive portion of
the iSink and evaluated the use of sampling as a means for addressing these constraints.
We ﬁrst considered the problem of a ﬁxed resource in the iSink itself. Estan and Vargh-
ese in [6] describe sampling methods aimed at monitoring “heavy hitters” in IP ﬂows
through routers with a limited amount of memory. We adapted one of these methods for
use in iSink. Second, we considered the problem of bandwidth as the limited resource.
7 We compare Active Sink with LaBrea because unlike LaBrea, Honeyd is stateful(forks a pro-
cess per connection), and hence is much less scalable. Since Honeyd also relies on a packet
ﬁlter LaBrea’s scalability bounds affect Honeyd as well.
160
Vinod Yegneswaran, Paul Barford, and Dave Plonka
d
n
o
c
e
s
r
e
p
s
t
e
k
c
a
p
d
n
u
o
b
t
u
o
f
o
r
e
b
m
u
N
20000
19000
18000
17000
16000
15000
14000
13000
12000
11000
10000
9000
8000
7000
6000
5000
4000
3000
2000
1000
LaBrea
CLICK
5000
15000
Number of inbound packets per second
10000
)
d
n
o
c
e
s
r
e
p
(
d
e
v
i
e
c
e
r
s
t
e
k
c
a
p
f
o
r
e
b
m
u
N
15000
14000
13000
12000
11000
10000
9000
8000
7000
6000
5000
4000
3000
2000
1000
20000
LaBrea
CLICK
5000
Number of packets sent (per second)
10000
15000
Fig. 10. Scalability of Click-based Internet Sink and LaBrea for TCP (left) and UDP (right) ﬂows
In this case, the idea is to reduce the total amount of trafﬁc routed to an iSink by se-
lecting subnets within the total address space available for monitoring. These methods
would be used in combination with the ﬁltering methods described in Section 3.3.
Memory Constrained iSink Sampling. The method that forms the basis of our sam-
pling approach with a memory constrained iSink is called Sample and Hold [6]. This
method accurately identiﬁes ﬂows larger than a speciﬁed threshold (i.e., heavy hitters).
Sample and hold is based on simple random sampling in conjunction with a hash table
that is used to maintain ﬂow ID’s and byte counts. Speciﬁcally, incoming packets are
randomly sampled and entries in the hash table are created for each new ﬂow. After
an entry has been created, all subsequent packets belonging to that ﬂow are counted.
While this approach can result in both false positives and false negatives, its accuracy
is shown to be high in workloads with varied characteristics. We apply sample and hold
in iSink to the problem of identifying “heavy hitters”, which are the worst offending
source addresses based on the observed number of scans.
Adapting the sample and hold method to the iSink required us to deﬁne the size
of the hash table that maintains the data, and the sampling rate based on empirical
observation of trafﬁc at the iSink. In [6], the objective is identifying accurately the
ﬂows that take over T % of a link’s capacity. An oversampling factor O is then selected
to reduce the possibility of false negatives in the results. These parameters result in
allocating HTlen = 1/T ∗ O locations in each hash table. The packet sampling rate
is then set to HTlen/C where C is the maximum packet transmission capacity of the
incoming link over a speciﬁed measurement period t. At the end of each t, the hash
table is sorted and results are produced.
Bandwidth Constrained iSink Sampling. In the bandwidth constrained scenario, the
sampling design problem is to select a set of subnets from the total address space that is
available for monitoring on the iSink. The selection of the number of subnets to monitor
is based on the bandwidth constraints. In this case we assume that we know the mean
and variance for trafﬁc volume on a “typical” class B or class C address space. We then
divide the available bandwidth by this value to get the number of these subnets that can
be monitored. The next step is to select the speciﬁc subnets within the entire space that
will minimize the error introduced in estimates of probe populations.
On the Design and Use of Internet Sinks for Network Abuse Monitoring
161
)
s
t
s
i
l
k
c
a
l
B
d
e
l
p
m
a
S
s
v
e
u
r
T
(
r
o
r
r
E
t
n
e
c
r
e
P
40
35
30
25
20
15
10
5
0
Hour 1
Hour 2
Hour 3
Hour 4