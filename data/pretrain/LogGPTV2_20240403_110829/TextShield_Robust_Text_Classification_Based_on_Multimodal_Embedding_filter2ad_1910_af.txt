texts are generated from sampled benign texts by TextBug-
ger [26].
The main results are summarized in Table 10. The sec-
ond column is the model accuracy evaluated under the non-
adversarial setting, which is comparable to the performance
reported in [23]. It is clearly observed that the common mod-
els can be deceived with high attack success rates, e.g., 0.880
for TextCNN and 0.782 for BiLSTM, which indicates that the
English-based DLTC models are also very vulnerable in the
adversarial environment. However, the attack success rates
against TextCNN and BiLSTM decrease to 0.265 and 0.285
respectively when the models are shielded by TEXTSHIELD.
This indicates that TEXTSHIELD is also effective in defend-
ing English-based DLTC models against adversarial attacks,
Query
40.1
33.4
35.3
42.1
62.7
62.2
Accuracy
0.869
0.892
0.710
0.823
0.890
0.850
ASR
0.884
0.897
0.814
0.818
0.236
0.247
Perturbed Word
1.71
1.88
1.67
1.90
2.03
2.03
Query
48.2
49.9
46.7
51.1
59.4
60.3
which shows good generalizability across languages.
6 Discussion
In this section, we discuss the limitations of TEXTSHIELD
and promising directions for further improvements.
Extensions to Other Settings and Tasks. In this paper,
TEXTSHIELD is designed to defend against adversaries in the
realistic adversarial environments, and it is evaluated under
the black-box setting. However, attackers may still have a
small chance of accessing the entire system in white box.
Hence, evaluating its efﬁcacy against the white-box attacks
is a valuable future work. Furthermore, TEXTSHIELD is
currently applied to two real-world tasks. In practice, there
are many other tasks such as spam email ﬁltering that can also
potentially beneﬁt from TEXTSHIELD. In the future work,
we will explore its applicability in broader real-world tasks.
Challenges for Real-world Deployments. Experimental
results have shown great promise to deploy TEXTSHIELD in
real-world. However, since TEXTSHIELD will increase the
total amount of model parameters, it may slightly decrease the
efﬁciency or increase the deployment cost of the whole system.
We argue that this would not be a hindrance to the real-world
deployment, because security is usually more important in
the security-sensitive tasks. In the future, we plan to apply
model compression and distributed computing techniques to
accelerate the whole system and reduce the costs.
7 Conclusion
To enhance the robustness of DLTC models against adversar-
ial texts in online toxic content detection tasks, we present
TEXTSHIELD, a new defense framework speciﬁcally de-
signed for Chinese-based DLTC models. At a high level,
TEXTSHIELD achieves robust toxic content detection by inte-
grating a set of key strategies, including multimodal embed-
ding, multimodal fusion, and adversarial neural machine trans-
lation. Through extensive empirical evaluation, we demon-
strate that TEXTSHIELD attains promising effectiveness in de-
fending against user generated obfuscated texts in real-world
adversarial scenarios, while with little impact on the original
detection performance. We also show that TEXTSHIELD is
robust against the state-of-the-art adversarial attacks even un-
USENIX Association
29th USENIX Security Symposium    1393
der the adaptive setting. Our study may shed new light on
designing adversarial defenses for other NLP tasks.
adversarial audio for end-to-end acoustic systems. In
AsiaCCS, 2020.
Acknowledgments
We sincerely appreciate the shepherding from David Evans.
We would also like to thank the anonymous reviewers for their
constructive comments and input to improve our paper. This
work was partly supported by the National Key Research and
Development Program of China under No. 2018YFB0804102,
NSFC under No. 61772466, U1936215, and U1836202, the
Zhejiang Provincial Natural Science Foundation for Distin-
guished Young Scholars under No. LR19F020003, the Provin-
cial Key Research and Development Program of Zhejiang,
China under No. 2019C01055, the Ant Financial Research
Funding, and the Alibaba-ZJU Joint Research Institute of
Frontier Technologies. Ting Wang is partially supported by
the National Science Foundation under Grant No. 1910546,
1953813, and 1846151. Min Yang is partially supported by
NSFC under No. U1636204 and U1836213. Min Yang is
also a member of Shanghai Institute of Intelligent Electronics
& Systems, Shanghai Institute for Advanced Communication
and Data Science.
References
[1] Ahmed Ali and Steve Renals. Word error rate estimation
for speech recognition: e-wer. In ACL, pages 20–24,
2018.
[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. Neural machine translation by jointly learning to
align and translate. In ICLR, 2015.
[3] Eytan Bakshy, Itamar Rosenn, Cameron Marlow, and
Lada Adamic. The role of social networks in infor-
In WWW, pages 519–528. ACM,
mation diffusion.
2012.
[4] Yoshua Bengio, Patrice Simard, Paolo Frasconi, et al.
Learning long-term dependencies with gradient descent
IEEE transactions on neural networks,
is difﬁcult.
5(2):157–166, 1994.
[5] Wieland Brendel, Jonas Rauber, and Matthias Bethge.
Decision-based adversarial attacks: Reliable attacks
against black-box machine learning models. In ICLR,
2018.
[6] Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. On the properties of neural
machine translation: Encoder–decoder approaches. In
SSST, pages 103–111, 2014.
[7] Tianyu Du, Shouling Ji, Jinfeng Li, Qinchen Gu, Ting
Wang, and Raheem Beyah. Sirenattack: Generating
[8] Javid Ebrahimi, Daniel Lowd, and Dejing Dou. On
adversarial examples for character-level neural machine
translation. In COLING, pages 653–663, 2018.
[9] Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing
Dou. Hotﬂip: White-box adversarial examples for text
classiﬁcation. In ACL, pages 31–36, 2018.
[10] Björn Gambäck and Utpal Kumar Sikdar. Using con-
volutional neural networks to classify hate-speech. In
ALW, pages 85–90, 2017.
[11] Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun
Qi. Black-box generation of adversarial text sequences
to evade deep learning classiﬁers. In SPW, pages 50–56.
IEEE, 2018.
[12] Zhitao Gong, Wenlu Wang, Bo Li, Dawn Song, and
Wei-Shinn Ku. Adversarial texts with gradient methods.
arXiv preprint arXiv:1801.07175, 2018.
[13] Ian J Goodfellow, Jonathon Shlens, and Christian
Szegedy. Explaining and harnessing adversarial ex-
amples. In ICLR, 2015.
[14] Alex Graves. Sequence transduction with recurrent
neural networks. In ICML, 2012.
[15] Hossein Hosseini, Sreeram Kannan, Baosen Zhang, and
Radha Poovendran. Deceiving google’s perspective
api built for detecting toxic comments. arXiv preprint
arXiv:1702.08138, 2017.
[16] Longtao Huang, Ting Ma, Junyu Lin, Jizhong Han, and
Songlin Hu. A multimodal text matching model for
obfuscated language identiﬁcation in adversarial com-
munication? In WWW, pages 2844–2850, 2019.
[17] Mansoor Iqbal. Wechat revenue and usage statistics,
2019.
[18] Heng Ji and Kevin Knight. Creative language encoding
under censorship. In Proceedings of the First Workshop
on Natural Language Processing for Internet Freedom,
pages 23–33, 2018.
[19] Yujie Ji, Xinyang Zhang, Shouling Ji, Xiapu Luo, and
Ting Wang. Model-reuse attacks on deep learning
systems. In CCS, pages 349–363, 2018.
[20] Robin Jia and Percy Liang. Adversarial examples for
evaluating reading comprehension systems. In EMNLP,
pages 2021–2031, 2017.
1394    29th USENIX Security Symposium
USENIX Association
[21] Zhuoren Jiang, Zhe Gao, Guoxiu He, Yangyang Kang,
Changlong Sun, Qiong Zhang, Luo Si, and Xiaozhong
Liu. Detect camouﬂaged spam content via stoneskip-
ping: Graph and text joint embedding for chinese charac-
ter variation representation. In EMNLP-IJCNLP, pages
6188–6197, 2019.
[22] Mladen Karan and Jan Šnajder. Cross-domain detection
of abusive language online. In ALW, pages 132–137,
2018.
[23] Yoon Kim. Convolutional neural networks for sentence
classiﬁcation. In EMNLP, pages 1746–1751, 2014.
[24] Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick
Haffner, et al. Gradient-based learning applied to
Proceedings of the IEEE,
document recognition.
86(11):2278–2324, 1998.
[25] Ao Li, Zhou Qin, Runshi Liu, Yiqun Yang, and Dong
Li. Spam review detection with graph convolutional
networks. In CIKM, pages 2703–2711, 2019.
[26] Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting
Wang. Textbugger: Generating adversarial text against
real-world applications. In NDSS, 2019.
[27] Xurong Li, Shouling Ji, Meng Han, Juntao Ji, Zhenyu
Ren, Yushan Liu, and Chunming Wu. Adversarial
examples versus cloud-based detectors: A black-box
empirical study. IEEE Transactions on Dependable and
Secure Computing, 2019.
[28] Min Lin, Qiang Chen, and Shuicheng Yan. Network in
network. In ICLR, 2014.
[29] Xiang Ling, Shouling Ji, Jiaxu Zou, Jiannan Wang,
Chunming Wu, Bo Li, and Ting Wang. Deepsec: A
uniform platform for security analysis of deep learn-
ing model. In 2019 IEEE Symposium on Security and
Privacy (SP), pages 673–690. IEEE, 2019.
[30] Minh-Thang Luong, Eugene Brevdo,
and Rui
Zhao. Neural machine translation (seq2seq) tutorial.
https://github.com/tensorﬂow/nmt, 2017.
[31] Joe Mayes and Stefan Nicola. Facebook warns it can’t
fully solve toxic content problem, 2019.
[32] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. Distributed representations of
words and phrases and their compositionality. In NIPS,
pages 3111–3119, 2013.
[33] Frederic P Miller, Agnes F Vandome, and John McBrew-
ster. Levenshtein distance: Information theory, com-
puter science, string (computer science), string metric,
damerau? Levenshtein distance, spell checker, hamming
distance. Alpha Press, 2009.
[34] Chikashi Nobata, Joel Tetreault, Achint Thomas, Yashar
Mehdad, and Yi Chang. Abusive language detection in
online user content. In WWW, pages 145–153. Interna-
tional World Wide Web Conferences Steering Commit-
tee, 2016.
[35] Behnaz Nojavanasghari, Deepak Gopinath, Jayanth
Koushik, Tadas Baltrušaitis,
and Louis-Philippe
Morency. Deep multimodal fusion for persuasiveness
prediction. In ICMI, pages 284–288. ACM, 2016.
[36] Bo Pang and Lillian Lee. Seeing stars: Exploiting class
relationships for sentiment categorization with respect
to rating scales. In ACL, pages 115–124. Association
for Computational Linguistics, 2005.
[37] Nicolas Papernot, Patrick McDaniel, Ananthram Swami,
and Richard Harang. Crafting adversarial input se-
In MILCOM,
quences for recurrent neural networks.
pages 49–54. IEEE, 2016.
[38] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. Bleu: a method for automatic evaluation of
machine translation. In ACL, pages 311–318. Associa-
tion for Computational Linguistics, 2002.
[39] Chenghui Shi, Xiaogang Xu, Shouling Ji, Kai Bu, Jian-
hai Chen, Raheem Beyah, and Ting Wang. Adversarial
captchas. arXiv preprint arXiv:1901.01107, 2019.
[40] Baidu simnet. https://ai.baidu.com/tech/nlp/
simnet.
[41] Nitish Srivastava and Ruslan R Salakhutdinov. Mul-
In
timodal learning with deep boltzmann machines.
NIPS, pages 2222–2230, 2012.
[42] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence
In NIPS,
to sequence learning with neural networks.
pages 3104–3112, 2014.
[43] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob
Fergus. Intriguing properties of neural networks. In
ICLR, 2014.
[44] Yicheng Wang and Mohit Bansal. Robust machine
In
comprehension models via adversarial training.
NAACL, pages 575–581, 2018.
[45] Ronald J Williams and David Zipser. A learning al-
gorithm for continually running fully recurrent neural
networks. Neural computation, 1(2):270–280, 1989.
[46] Jui-Feng Yeh, Yun-Yun Lu, Chen-Hsien Lee, Yu-Hsiang
Yu, and Yong-Ting Chen. Chinese word spelling cor-
rection based on rule induction. In CIPS-SIGHAN CLP,
pages 139–145, 2014.
USENIX Association
29th USENIX Security Symposium    1395
[47] Junjie Yu and Zhenghua Li. Chinese spelling error
detection and correction based on language model, pro-
nunciation, and shape. In CIPS-SIGHAN CLP, pages
220–223, 2014.
[48] Kan Yuan, Di Tang, Xiaojing Liao, XiaoFeng Wang,
Xuan Feng, Yi Chen, Menghan Sun, Haoran Lu, and
Kehuan Zhang. Stealthy porn: Understanding real-
world adversarial images for illicit online promotion. In
S&P, pages 952–966. IEEE, 2019.
[49] Xinyang Zhang, Ningfei Wang, Hua Shen, Shouling Ji,
Xiapu Luo, and Ting Wang. Interpretable deep learning
under ﬁre. In USENIX Security, 2020.
[50] Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen
Li, Hongwei Hao, and Bo Xu. Attention-based bidi-
rectional long short-term memory networks for relation
classiﬁcation. In ACL, pages 207–212, 2016.
Appendix
A Multimodal Fusion Schemes
Fig. 10 illustrates the two multimodal fusion schemes, i.e.,
EMF and IMF.
B Data Collection Details
At the ﬁrst stage, we collected 40,000 user comments from
Weibo, Taobao, etc., for each task (i.e., abuse and porn detec-
tion). Considering the ethical implications, we fully respect
the privacy of users, and only use the public comment texts
of them. After preprocessing, removing the duplicates and
ﬁltering out the meaningless texts, we used Alibaba GreenNet
to automatically label these processed texts, and we then got
about 30,000 coarsely labelled samples for each task, in which
about 15,000 samples were toxic and 15,000 were normal. At
the second stage, we hired several Chinese native speakers
to relabel the coarsely labelled samples, and we also ﬁltered
out those samples that were labelled inconsistently. Then, we
randomly sampled 10,000 ﬁnely labelled samples for each
class as the datasets we used in our experiments. Speciﬁcally,
each sample was also manually conﬁrmed that there did not
exist variant words. In the meantime, we got a corpus of 2,000
obfuscated texts (i.e., real-world attack examples as shown
in Fig. 11) for each task, in which each text had at least one
variant word. We then asked the hired workers to annotate
what the variant word was and which category it belonged to,
and the statistic distribution of different variant categories can
be seen in Fig. 5.
C Distribution of Bugs
1396    29th USENIX Security Symposium
USENIX Association
Figure 10: Illustration of multimodal fusion schemes.
(a) Insulting Comment on Weibo
(b) Spam Message on Taobao
(c) Pornographic Ads
WeChat
on
Figure 11: Adversarial examples in the real world. The subﬁgures are: (a) is an obfuscated insulting comment on Weibo in
which “老丕死” is mutated from “老不死” (old fuck) and “溅人” is mutated from “贱人” (bitch), and the obfuscated text retains insulting but
successfully evaded the censorship; (b) is an obfuscated spam ads for the purpose of fake purchase on Taobao; (c) is a pornographic ads for sex
service on WeChat, in which “茄莪薇芯” is an obfuscated phrase of “加我微信” that means “add my WeChat account”, and the obfuscated
ads is still illegal but usually hard to detect.
USENIX Association
29th USENIX Security Symposium    1397
…FusionInputHiddenOutput………Semantic EmbeddingGlyph EmbeddingPhonetic Embedding…………Semantic EmbeddingGlyph EmbeddingPhonetic EmbeddingFusionInputHiddenOutput(a) EMF(b) IMF(a) TextCNN on Porn
(b) BiLSTM on Abuse
Figure 12: The sensitivity of the target models against different bugs on the two datasets.
(c) BiLSTM on Porn
1398    29th USENIX Security Symposium
USENIX Association
0.00.10.20.3TextCNN0.00.10.20.3TextCNN+EMF0.00.10.20.3TextCNN0.00.10.20.3TextCNN+IMF0.00.10.20.3TextCNN0.000.050.100.150.200.25TextCNN+EMF+NMT0.00.10.20.3TextCNN0.000.050.100.150.20TextCNN+IMF+NMT0.00.20.4BiLSTM0.00.10.20.30.40.5BiLSTM+EMF0.00.20.4BiLSTM0.000.050.100.150.200.25BiLSTM+IMF0.00.20.4BiLSTM0.000.050.100.150.200.25BiLSTM+EMF+NMT0.00.20.4BiLSTM0.000.050.100.150.200.250.30BiLSTM+IMF+NMT0.00.10.2BiLSTM0.000.050.100.150.20BiLSTM+EMF0.00.10.2BiLSTM0.000.050.100.15BiLSTM+IMF0.00.10.2BiLSTM0.000.050.100.150.200.25BiLSTM+EMF+NMT0.00.10.2BiLSTM0.000.050.100.150.20BiLSTM+IMF+NMT