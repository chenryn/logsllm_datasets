### Text Generation and Evaluation

The texts are generated from sampled benign texts using TextBugger [26]. The main results are summarized in Table 10. The second column of the table shows the model accuracy under a non-adversarial setting, which is comparable to the performance reported in [23]. It is evident that common models can be deceived with high attack success rates, such as 0.880 for TextCNN and 0.782 for BiLSTM, indicating that English-based Deep Learning Text Classification (DLTC) models are highly vulnerable in adversarial environments. However, when these models are protected by TEXTSHIELD, the attack success rates against TextCNN and BiLSTM decrease to 0.265 and 0.285, respectively. This demonstrates that TEXTSHIELD is effective in defending English-based DLTC models against adversarial attacks.

### Table 10: Model Performance and Attack Success Rates

| Query | Accuracy | ASR | Perturbed Word |
|-------|----------|-----|----------------|
| 40.1  | 0.869    | 0.884 | 1.71           |
| 33.4  | 0.892    | 0.897 | 1.88           |
| 35.3  | 0.710    | 0.814 | 1.67           |
| 42.1  | 0.823    | 0.818 | 1.90           |
| 62.7  | 0.890    | 0.236 | 2.03           |
| 62.2  | 0.850    | 0.247 | 2.03           |

| Query | Accuracy | ASR | Perturbed Word |
|-------|----------|-----|----------------|
| 48.2  | 0.869    | 0.884 | 1.71           |
| 49.9  | 0.892    | 0.897 | 1.88           |
| 46.7  | 0.710    | 0.814 | 1.67           |
| 51.1  | 0.823    | 0.818 | 1.90           |
| 59.4  | 0.890    | 0.236 | 2.03           |
| 60.3  | 0.850    | 0.247 | 2.03           |

These results show good generalizability across languages.

### Discussion

In this section, we discuss the limitations of TEXTSHIELD and potential directions for further improvements.

#### Extensions to Other Settings and Tasks
TEXTSHIELD is designed to defend against adversaries in realistic adversarial environments and has been evaluated under a black-box setting. However, attackers may still have a small chance of accessing the entire system in a white-box scenario. Therefore, evaluating its efficacy against white-box attacks is a valuable future work. Additionally, TEXTSHIELD is currently applied to two real-world tasks. In practice, there are many other tasks, such as spam email filtering, that could potentially benefit from TEXTSHIELD. Future work will explore its applicability in a broader range of real-world tasks.

#### Challenges for Real-World Deployments
Experimental results show great promise for deploying TEXTSHIELD in real-world scenarios. However, since TEXTSHIELD increases the total number of model parameters, it may slightly decrease the efficiency or increase the deployment cost of the system. We argue that this would not be a significant hindrance to real-world deployment, especially in security-sensitive tasks where security is more important. In the future, we plan to apply model compression and distributed computing techniques to accelerate the system and reduce costs.

### Conclusion

To enhance the robustness of DLTC models against adversarial texts in online toxic content detection tasks, we present TEXTSHIELD, a new defense framework specifically designed for Chinese-based DLTC models. TEXTSHIELD achieves robust toxic content detection by integrating key strategies, including multimodal embedding, multimodal fusion, and adversarial neural machine translation. Through extensive empirical evaluation, we demonstrate that TEXTSHIELD is effective in defending against user-generated obfuscated texts in real-world adversarial scenarios, with minimal impact on the original detection performance. We also show that TEXTSHIELD is robust against state-of-the-art adversarial attacks, even under adaptive settings. Our study may shed new light on designing adversarial defenses for other NLP tasks.

### Acknowledgments

We sincerely appreciate the shepherding from David Evans. We also thank the anonymous reviewers for their constructive comments and input to improve our paper. This work was partly supported by the National Key Research and Development Program of China, NSFC, Zhejiang Provincial Natural Science Foundation, and other funding sources. Ting Wang and Min Yang are partially supported by the National Science Foundation and NSFC, respectively.

### References

[1] Ahmed Ali and Steve Renals. Word error rate estimation for speech recognition: e-wer. In ACL, pages 20–24, 2018.
[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In ICLR, 2015.
[3] Eytan Bakshy, Itamar Rosenn, Cameron Marlow, and Lada Adamic. The role of social networks in information diffusion. In WWW, pages 519–528. ACM, 2012.
[4] Yoshua Bengio, Patrice Simard, Paolo Frasconi, et al. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157–166, 1994.
[5] Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable attacks against black-box machine learning models. In ICLR, 2018.
[6] Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder–decoder approaches. In SSST, pages 103–111, 2014.
[7] Tianyu Du, Shouling Ji, Jinfeng Li, Qinchen Gu, Ting Wang, and Raheem Beyah. Sirenattack: Generating adversarial audio for end-to-end acoustic systems. In AsiaCCS, 2020.
[8] Javid Ebrahimi, Daniel Lowd, and Dejing Dou. On adversarial examples for character-level neural machine translation. In COLING, pages 653–663, 2018.
[9] Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotflip: White-box adversarial examples for text classification. In ACL, pages 31–36, 2018.
[10] Björn Gambäck and Utpal Kumar Sikdar. Using convolutional neural networks to classify hate-speech. In ALW, pages 85–90, 2017.
[11] Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. Black-box generation of adversarial text sequences to evade deep learning classifiers. In SPW, pages 50–56. IEEE, 2018.
[12] Zhitao Gong, Wenlu Wang, Bo Li, Dawn Song, and Wei-Shinn Ku. Adversarial texts with gradient methods. arXiv preprint arXiv:1801.07175, 2018.
[13] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In ICLR, 2015.
[14] Alex Graves. Sequence transduction with recurrent neural networks. In ICML, 2012.
[15] Hossein Hosseini, Sreeram Kannan, Baosen Zhang, and Radha Poovendran. Deceiving google’s perspective API built for detecting toxic comments. arXiv preprint arXiv:1702.08138, 2017.
[16] Longtao Huang, Ting Ma, Junyu Lin, Jizhong Han, and Songlin Hu. A multimodal text matching model for obfuscated language identification in adversarial communication? In WWW, pages 2844–2850, 2019.
[17] Mansoor Iqbal. Wechat revenue and usage statistics, 2019.
[18] Heng Ji and Kevin Knight. Creative language encoding under censorship. In Proceedings of the First Workshop on Natural Language Processing for Internet Freedom, pages 23–33, 2018.
[19] Yujie Ji, Xinyang Zhang, Shouling Ji, Xiapu Luo, and Ting Wang. Model-reuse attacks on deep learning systems. In CCS, pages 349–363, 2018.
[20] Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems. In EMNLP, pages 2021–2031, 2017.
[21] Zhuoren Jiang, Zhe Gao, Guoxiu He, Yangyang Kang, Changlong Sun, Qiong Zhang, Luo Si, and Xiaozhong Liu. Detect camouflaged spam content via stonesskipping: Graph and text joint embedding for Chinese character variation representation. In EMNLP-IJCNLP, pages 6188–6197, 2019.
[22] Mladen Karan and Jan Šnajder. Cross-domain detection of abusive language online. In ALW, pages 132–137, 2018.
[23] Yoon Kim. Convolutional neural networks for sentence classification. In EMNLP, pages 1746–1751, 2014.
[24] Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
[25] Ao Li, Zhou Qin, Runshi Liu, Yiqun Yang, and Dong Li. Spam review detection with graph convolutional networks. In CIKM, pages 2703–2711, 2019.
[26] Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. Textbugger: Generating adversarial text against real-world applications. In NDSS, 2019.
[27] Xurong Li, Shouling Ji, Meng Han, Juntao Ji, Zhenyu Ren, Yushan Liu, and Chunming Wu. Adversarial examples versus cloud-based detectors: A black-box empirical study. IEEE Transactions on Dependable and Secure Computing, 2019.
[28] Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. In ICLR, 2014.
[29] Xiang Ling, Shouling Ji, Jiaxu Zou, Jiannan Wang, Chunming Wu, Bo Li, and Ting Wang. Deepsec: A uniform platform for security analysis of deep learning model. In 2019 IEEE Symposium on Security and Privacy (SP), pages 673–690. IEEE, 2019.
[30] Minh-Thang Luong, Eugene Brevdo, and Rui Zhao. Neural machine translation (seq2seq) tutorial. https://github.com/tensorflow/nmt, 2017.
[31] Joe Mayes and Stefan Nicola. Facebook warns it can’t fully solve toxic content problem, 2019.
[32] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119, 2013.
[33] Frederic P Miller, Agnes F Vandome, and John McBrewster. Levenshtein distance: Information theory, computer science, string (computer science), string metric, Damerau-Levenshtein distance, spell checker, Hamming distance. Alpha Press, 2009.
[34] Chikashi Nobata, Joel Tetreault, Achint Thomas, Yashar Mehdad, and Yi Chang. Abusive language detection in online user content. In WWW, pages 145–153. International World Wide Web Conferences Steering Committee, 2016.
[35] Behnaz Nojavanasghari, Deepak Gopinath, Jayanth Koushik, Tadas Baltrušaitis, and Louis-Philippe Morency. Deep multimodal fusion for persuasiveness prediction. In ICMI, pages 284–288. ACM, 2016.
[36] Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In ACL, pages 115–124. Association for Computational Linguistics, 2005.
[37] Nicolas Papernot, Patrick McDaniel, Ananthram Swami, and Richard Harang. Crafting adversarial input sequences for recurrent neural networks. In MILCOM, pages 49–54. IEEE, 2016.
[38] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic evaluation of machine translation. In ACL, pages 311–318. Association for Computational Linguistics, 2002.
[39] Chenghui Shi, Xiaogang Xu, Shouling Ji, Kai Bu, Jianhai Chen, Raheem Beyah, and Ting Wang. Adversarial CAPTCHAs. arXiv preprint arXiv:1901.01107, 2019.
[40] Baidu SimNet. https://ai.baidu.com/tech/nlp/simnet.
[41] Nitish Srivastava and Ruslan R Salakhutdinov. Multimodal learning with deep Boltzmann machines. In NIPS, pages 2222–2230, 2012.
[42] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In NIPS, pages 3104–3112, 2014.
[43] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In ICLR, 2014.
[44] Yicheng Wang and Mohit Bansal. Robust machine comprehension models via adversarial training. In NAACL, pages 575–581, 2018.
[45] Ronald J Williams and David Zipser. A learning algorithm for continually running fully recurrent neural networks. Neural computation, 1(2):270–280, 1989.
[46] Jui-Feng Yeh, Yun-Yun Lu, Chen-Hsien Lee, Yu-Hsiang Yu, and Yong-Ting Chen. Chinese word spelling correction based on rule induction. In CIPS-SIGHAN CLP, pages 139–145, 2014.
[47] Junjie Yu and Zhenghua Li. Chinese spelling error detection and correction based on language model, pronunciation, and shape. In CIPS-SIGHAN CLP, pages 220–223, 2014.
[48] Kan Yuan, Di Tang, Xiaojing Liao, XiaoFeng Wang, Xuan Feng, Yi Chen, Menghan Sun, Haoran Lu, and Kehuan Zhang. Stealthy porn: Understanding real-world adversarial images for illicit online promotion. In S&P, pages 952–966. IEEE, 2019.
[49] Xinyang Zhang, Ningfei Wang, Hua Shen, Shouling Ji, Xiapu Luo, and Ting Wang. Interpretable deep learning under fire. In USENIX Security, 2020.
[50] Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen Li, Hongwei Hao, and Bo Xu. Attention-based bidirectional long short-term memory networks for relation classification. In ACL, pages 207–212, 2016.

### Appendix

#### A. Multimodal Fusion Schemes

Figure 10 illustrates the two multimodal fusion schemes, i.e., Early Multimodal Fusion (EMF) and Intermediate Multimodal Fusion (IMF).

#### B. Data Collection Details

At the first stage, we collected 40,000 user comments from Weibo, Taobao, etc., for each task (i.e., abuse and porn detection). Considering the ethical implications, we fully respect the privacy of users and only use the public comment texts. After preprocessing, removing duplicates, and filtering out meaningless texts, we used Alibaba GreenNet to automatically label these processed texts, resulting in about 30,000 coarsely labeled samples for each task, with approximately 15,000 toxic and 15,000 normal samples. At the second stage, we hired several Chinese native speakers to relabel the coarsely labeled samples, and we also filtered out those samples that were labeled inconsistently. Then, we randomly sampled 10,000 finely labeled samples for each class as the datasets used in our experiments. Each sample was manually confirmed to ensure no variant words existed. Additionally, we obtained a corpus of 2,000 obfuscated texts (i.e., real-world attack examples as shown in Figure 11) for each task, with each text containing at least one variant word. We asked the hired workers to annotate what the variant word was and which category it belonged to, and the statistical distribution of different variant categories can be seen in Figure 5.

#### C. Distribution of Bugs

Figure 12 shows the sensitivity of the target models against different bugs on the two datasets.

### Figures

**Figure 10: Illustration of Multimodal Fusion Schemes**

(a) Early Multimodal Fusion (EMF)
(b) Intermediate Multimodal Fusion (IMF)

**Figure 11: Adversarial Examples in the Real World**

(a) Obfuscated Insulting Comment on Weibo
(b) Obfuscated Spam Message on Taobao
(c) Obfuscated Pornographic Ads on WeChat

**Figure 12: Sensitivity of Target Models Against Different Bugs**

(a) TextCNN on Porn
(b) BiLSTM on Abuse
(c) BiLSTM on Porn

---

This optimized version of your text is more coherent, clear, and professional, with improved formatting and structure.