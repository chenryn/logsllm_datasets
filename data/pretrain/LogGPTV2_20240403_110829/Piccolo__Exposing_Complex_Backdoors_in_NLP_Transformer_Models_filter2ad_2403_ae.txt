### Delayed Normalization Strategy

To enhance the probability values of specific trigger words, we propose a delayed normalization strategy. In this approach, the dimension values are normalized (to sum to 1) every 30 epochs. This relaxation empirically increases the likelihood of success.

**Example:**
Consider a trojaned model #47 from TrojAI Round 6 with the trigger phrase "supposing knowingly screaming immune fixer stances." The bar charts in Figure 11 illustrate how the dimension values, including those for the trigger word 'immune,' change with per-epoch normalization (a) and delayed normalization (b). For each word on the x-axis, the bars from left to right represent the results after increasing optimization epochs. Notice that 'immune' stands out more prominently in the delayed normalization scenario.

### Loss Function

Our loss function consists of four terms:

1. **Cross-Entropy Loss:** This term aims to induce misclassification to the target label \(\tau\). Note that we do not need to know the target label beforehand; PICCOLO scans each output label, treating each as a potential target.
   
2. **Dot Product Term:** This term is the dot product of the representation embedding and the CLS dimension importance vector \(I\). The intuition is that the inverted trigger word should yield high values at the CLS dimensions deemed important by the classifier \(M_{cls2y}\).

3. **Dimension Reduction Term:** This term reduces the dimension values in the trigger word vector \(x\) until only one dimension has a value exceeding 0.5. This helps in selecting good trigger word candidates and avoids having too many dimensions with values close to 1.

4. **Misclassification Prevention Term:** This optional term ensures that the inverted trigger does not induce the same misclassification on a random benign model (on the same dataset). It is used if benign models are available. Let \(x_z\) be the word vector with the tanh function, defined in Equation (8), and \(y_z\) the classification result defined in Equation (4). Let \(\tau_0\) be the victim label and \(y^{\prime}_z\) the classification of the benign model. The loss function is formally defined as follows:
   \[
   w_1 \cdot L_{ce}(y_z, \tau) + w_2 \cdot r \cdot I + \arg \min_z w_3 \cdot \sum(x_z) + w_4 \cdot L_{ce}(y^{\prime}_z, \tau_0),
   \]
   where
   \[
   w_3 = \begin{cases} 
   w_{large} & \text{if count}(x_z > 0.5) > 2 \\
   0 & \text{otherwise}
   \end{cases},
   \]
   and
   \[
   x_z = \frac{\tanh(z) + 1}{2}, \quad r = T(x_z \times M_{w2t} \times M_{t2e}) \quad \text{(representation embedding)}.
   \]

### Design Justification

Our choices of the optimization method, loss function, and hyperparameters are empirical, which is common in the literature. We conducted an ablation study using tanh and delayed normalization on the TrojAI Round 6 test set. The overall accuracy of PICCOLO decreased from 0.907 to 0.776 when changing tanh and delayed normalization to softmax. Detailed results are provided in Appendix IX-G.

### Trigger Validation

The validation step checks the Attack Success Rate (ASR) of the likely trigger words. Specifically, we select the words corresponding to the 10 most likely trigger words in DistilBERT and 20 in GPT. With the two inverted word vectors in our implementation, there are 20 in DistilBERT and 40 in GPT. We first test the ASRs of individual words or word pairs. If any ASR exceeds 0.9, we consider the subject model trojaned. Otherwise, we further test if the model is particularly discriminative for any of these words. We train a linear model for each word \(w\) and acquire the linear weight vector \(\theta\) for the word. We also obtain the dimension importance vector \(I\) for the target label. If the dot product of the two exceeds 170, we consider the model trojaned.

### Evaluation

We evaluate PICCOLO across various model architectures, application tasks, and backdoor types. We compare it with two state-of-the-art techniques, GBDA [36] and T-miner [38]. Additionally, we study PICCOLO's efficiency and its performance against advanced and adaptive attacks. We also conduct an ablation study to investigate each component of PICCOLO. PICCOLO is implemented in PyTorch [53] and will be released upon publication.

#### Experiment Setup

**Datasets and Models:**
- We use 3,256 models (half benign and half backdoored) from the training and test sets of TrojAI Rounds 5-7 [39].
- We also train 103 GRU models from T-miner [38], 120 BERT classification models, and 120 LSTM models from Hidden Killer [8] on SST-2 [40], OLID [54], and AG News [55].
- For the combination lock attack [9], we train 240 BERT classification models on SST-2, OLID, and AG News.
- For adaptive attacks, we use the TrojAI official repository [39] to generate 730 backdoored DistilBERT classification models.

Detailed experiment setup is provided in Appendix IX-E.

#### Effectiveness of PICCOLO

Tables II and III present the detection results. In Table II, the first column shows the evaluation sets, the second column shows the model architectures, and columns 3-7 show the results of PICCOLO. Columns 8-12 and 13-17 show the results of GBDA and T-miner, respectively. Columns TP, FP, FN, and TN denote the number of true positives, false positives, false negatives, and true negatives, respectively. Column Acc presents the overall detection accuracy.

Due to the low efficiency of T-miner, we only evaluate it on randomly sampled 100 models from the R5-test and R6-test datasets.

**Table II: Effectiveness of PICCOLO on Classification Tasks**

| Evaluation Set | Arch. | PICCOLO | GBDA | T-miner* |
|----------------|-------|---------|------|----------|
|                |       | TP      | FP   | FN       | TN    | Acc   | TP   | FP   | FN   | TN    | Acc   | TP   | FP   | FN   | TN    | Acc   |
| TrojAI R5 train| DistilBERT | 325 | 27 | 35 | 199 | 0.894 | 254 | 88 | 106 | 138 | 0.677 | 30 | 4 | 330 | 222 | 0.430 |
| TrojAI R5 test | BERT     | 188 | 21 | 23 | 202 | 0.898 | 118 | 28 | 93 | 195 | 0.721 | 41 | 6 | 170 | 217 | 0.594 |
| TrojAI R6 train| GPT      | 224 | 26 | 33 | 195 | 0.877 | 140 | 27 | 117 | 194 | 0.700 | 34 | 16 | 223 | 205 | 0.500 |
| TrojAI R6 test | DistilBERT | 70 | 4 | 14 | 69 | 0.885 | 47 | 11 | 37 | 62 | 0.694 | 9 | 2 | 41 | 48 | 0.570 |
|                | BERT     | 69 | 4 | 16 | 68 | 0.873 | 67 | 19 | 18 | 53 | 0.764 | 11 | 3 | 39 | 47 | 0.580 |
|                | GPT      | 66 | 3 | 19 | 67 | 0.858 | 59 | 11 | 26 | 59 | 0.761 | 7 | 1 | 43 | 49 | 0.560 |
|                | DistilBERT | 11 | 1 | 1 | 11 | 0.917 | 10 | 3 | 2 | 9 | 0.792 | 2 | 3 | 10 | 9 | 0.458 |
|                | GPT      | 10 | 0 | 2 | 12 | 0.917 | 6 | 1 | 5 | 12 | 0.750 | 3 | 2 | 9 | 10 | 0.542 |
|                | DistilBERT | 106 | 6 | 14 | 114 | 0.917 | 75 | 40 | 45 | 80 | 0.646 | 4 | 1 | 46 | 49 | 0.530 |
|                | GPT      | 107 | 12 | 13 | 108 | 0.896 | 90 | 30 | 30 | 90 | 0.750 | 5 | 4 | 45 | 46 | 0.510 |
|                | GRU      | 58 | 0 | 6 | 39 | 0.942 | 57 | 0 | 8 | 39 | 0.932 | 56 | 4 | 8 | 35 | 0.883 |

*Due to T-miner running too slow, we test T-miner on randomly sampled 100 models on R5-test and R6-test datasets.

**Table III: Effectiveness of PICCOLO on NER Tasks**

| Evaluation Set | Arch. | PICCOLO | GBDA | T-miner* |
|----------------|-------|---------|------|----------|
|                |       | TP      | FP   | FN       | TN    | Acc   | TP   | FP   | FN   | TN    | Acc   | TP   | FP   | FN   | TN    | Acc   |
| TrojAI R5 train| DistilBERT | 30 | 0 | 0 | 0 | 0.979 | 0 | 0 | 0 | 0 | 0.979 | 0 | 0 | 0 | 0 | 0.979 |
| TrojAI R5 test | BERT     | 41 | 0 | 0 | 0 | 0.938 | 0 | 0 | 0 | 0 | 0.938 | 0 | 0 | 0 | 0 | 0.938 |
| TrojAI R6 train| GPT      | 34 | 0 | 0 | 0 | 0.938 | 0 | 0 | 0 | 0 | 0.938 | 0 | 0 | 0 | 0 | 0.938 |
| TrojAI R6 test | DistilBERT | 9 | 0 | 0 | 0 | 0.917 | 0 | 0 | 0 | 0 | 0.917 | 0 | 0 | 0 | 0 | 0.917 |
|                | BERT     | 11 | 0 | 0 | 0 | 0.917 | 0 | 0 | 0 | 0 | 0.917 | 0 | 0 | 0 | 0 | 0.917 |
|                | GPT      | 7 | 0 | 0 | 0 | 0.896 | 0 | 0 | 0 | 0 | 0.896 | 0 | 0 | 0 | 0 | 0.896 |

*Due to T-miner running too slow, we test T-miner on randomly sampled 100 models on R5-test and R6-test datasets.