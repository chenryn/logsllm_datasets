title:Voice-Indistinguishability - Protecting Voiceprint with Differential
Privacy under an Untrusted Server
author:Yaowei Han and
Yang Cao and
Sheng Li and
Qiang Ma and
Masatoshi Yoshikawa
Poster: Voice-Indistinguishability – Protecting Voiceprint with
Differential Privacy under an Untrusted Server
Yaowei Han
Sheng Li
Yang Cao
Department of Social Informatics
Department of Social Informatics
Kyoto University
Kyoto, Japan
PI:EMAIL
Kyoto University
Kyoto, Japan
PI:EMAIL
National Institute of Information and
Communications Technology
Kyoto, Japan
PI:EMAIL
Qiang Ma
Department of Social Informatics
Kyoto University
Kyoto, Japan
PI:EMAIL
Masatoshi Yoshikawa
Department of Social Informatics
Kyoto University
Kyoto, Japan
PI:EMAIL
ABSTRACT
With the rising adoption of advanced voice-based technology to-
gether with increasing consumer demand for smart devices, voice-
controlled “virtual assistants” such as Apple’s Siri and Google As-
sistant have been integrated into people’s daily lives. However,
privacy and security concerns may hinder the development of such
voice-based applications since speech data contain the speaker’s
biometric identifier, i.e., voiceprint (as analogous to fingerprint).
To alleviate privacy concerns in speech data collection, we pro-
pose a fast speech data de-identification system that allows a user
to share her speech data with formal privacy guarantee to an un-
trusted server. Our open-sourced system can be easily integrated
into other speech processing systems for collecting users’ voice
data in a privacy-preserving way. Experiments on public datasets
verify the effectiveness and efficiency of the proposed system.
CCS CONCEPTS
• Security and privacy → Data anonymization and sanitiza-
tion.
KEYWORDS
speaker de-identification; speech data collection; voiceprint; differ-
ential privacy
ACM Reference Format:
Yaowei Han, Yang Cao, Sheng Li, Qiang Ma, and Masatoshi Yoshikawa. 2020.
Poster: Voice-Indistinguishability – Protecting Voiceprint with Differential
Privacy under an Untrusted Server. In Proceedings of the 2020 ACM SIGSAC
Conference on Computer and Communications Security (CCS ’20), November
9–13, 2020, Virtual Event, USA. ACM, New York, NY, USA, 3 pages. https:
//doi.org/10.1145/3372297.3420025
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS ’20, November 9–13, 2020, Virtual Event, USA
© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-7089-9/20/11...$15.00
https://doi.org/10.1145/3372297.3420025
1 INTRODUCTION
With the rising adoption of advanced voice-based technology to-
gether with increasing consumer demand for smart devices, voice-
controlled “virtual assistants” such as Apple’s Siri and Google As-
sistant have been integrated into people’s daily lives. A Statista’s
report shows that around 3.25 billion digital voice assistants were
used around the world in 2019, and the number is even forecasted
to reach 8 billion users by 2023, which is higher than the current
world population [2].
However, privacy and security concerns may hinder the devel-
opment of such voice-based applications since speech data contain
the speaker’s identifiable information, i.e., voiceprint (as analogous
to fingerprint), which is a distinguishing and repeatable biometric
feature of human beings [6]. Recent studies show that exposing an
individual’s voiceprint may cause security risks such as spoofing
attacks [14] and reputation attacks [12]. With the advent of the
GDPR [9] and increasing privacy concerns, the sharing of speech
data is faced with significant challenges.
Our recent work [4] proposed the first formal voiceprint privacy
definition called Voice-Indistinguishability and a privacy-preserving
speech synthesis system that releases a private speech database
with a trusted server. However, it remains challenging to achieve
Voice-Indistinguishability without a trusted server (shown in Figure
1), where the user needs to perturb her speech utterances locally
before sending it to an untrusted server such as virtual assistant
service providers. A further challenge is that the service providers
often need to authenticate the speaker (i.e., speaker recognition)
before providing the service [1, 5].
This poster demonstrates a privacy-preserving speech synthe-
sis system without a trusted server. As shown in Figure 1, the
Figure 1: Voice-Indistinguishability in the local setting.
system runs locally on a user device that takes the user’s raw utter-
ances (e.g., voice commands for Siri) as inputs and outputs privacy-
preserved utterances (with an anonymized voiceprint). To achieve
both Voice-Indistinguishability and speaker recognition without
a trusted server, we make three contributions in this poster. First,
we propose a method to construct a “fake” voiceprint set (called
anonymous voiceprint set) from publicly available speech datasets
to represent the speaker’s voice identity. We adopt x-vectors [11],
which is the state-of-the-art model of the voiceprint, to construct
the anonymous voiceprint set. Second, we design a new voiceprint
anonymization method that guarantees Voice-Indistinguishability
and speaker recognition. The idea is to randomly perturb a speaker’s
voiceprint to an anonymous voiceprint and then memorize the map-
ping between the original voiceprint and the anonymized voiceprint
in a local “look-up table”. The user can also reset her anonymized
voiceprint by removing the entry from the look-up table. We also
make sure that the system satisfies Voice-Indistinguishability even
there are multiple users of the same device. Third, we propose a fast
speech synthesis framework to synthesize the anonymized x-vector
and other features in the original utterance without a trusted server.
To reduce the workload of the user device, our system embeds per-
turbation in the local device and outsources the speech syntheses
to an untrusted server.
2 METHODOLOGY
2.1 Voice-Indistinguishability
Voice-Indistinguishability is a rigorous privacy metrics for voiceprint
[4]. We implement it using the state-of-the-art representation of
voiceprint, i.e., the x-vector [11].
Definition 1 (Voice-Indistinguishability, i.e., Voice-Ind) [4]
A mechanism 𝐾 satisfies 𝜖-Voice-Indistinguishability if for any out-
put ˜𝑥 and any two possible voiceprints 𝑥, 𝑥′ ∈ X:
Pr( ˜𝑥|𝑥)
Pr( ˜𝑥|𝑥′) ≤ 𝑒𝜖𝑑X (𝑥,𝑥′)
arccos(cos 𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦 )
𝑑X =
𝜋
where X is a set of possible voiceprints, 𝑑X is the angular distance
metric between x-vectors.
Voice-Indistinguishability guarantees that given the output x-
vector ˜𝑥, an attacker hardly distinguishes whether the original
x-vector is 𝑥 or 𝑥′ bounded by 𝜖𝑑X. The privacy budget value 𝜖
globally influences the degree of guaranteed privacy.
2.2 Anonymous Voiceprint Set Construction
To achieve Voice-Indistinguishability in a local setting, the local
device needs to obtain a set of possible x-vectors to anonymize a
speaker’s voice identity. We use public speech datasets to construct
an x-vector database, X. Given a public speech dataset such as the
LibriSpeech [10], we extract x-vectors from utterances using the
x-vector extractor, as shown in Table 1.
A challenge is that, the above extracted x-vectors cannot be
directly used as the anonymous voiceprint set because a public
speech dataset may contain multiple utterances of the same speaker,
which causes problems for speaker recognition. To make sure that
an x-vector in the anonymous voiceprint set represents a unique
Layers
time-delay 1
time-delay 2
time-delay 3
time-delay 4
time-delay 5
statistics pooling
bottleneck 1
bottleneck 2
softmax
Layer context
[𝑡 − 2, 𝑡 + 2]
{𝑡 − 2, 𝑡, 𝑡 + 2}
{𝑡 − 3, 𝑡, 𝑡 + 3}
{𝑡 }
{𝑡 }
[0,𝑇)
{0}
{0}
{0}
#context
5
9
15
15
15
𝑇
𝑇
𝑇
𝑇
#units
512
512
512
512
1500
3000
512
512
𝐿
Table 1: The x-vector TDNN. 𝑇 is the number of frames in a
given utterance. 𝐿 is the number of speakers.
voiceprint, a naive method can be using the mean of x-vectors from
the same speaker; however, we find that such an averaged x-vector
renders the synthesized utterance unnatural. To solve this problem,
we design a clustering-based method to find the “representative”
x-vectors in the public speech dataset. For the clustering algorithm,
we use the elastic net subspace clustering (EnSC) [17] proposed for