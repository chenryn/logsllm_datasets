the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining. 1387–1395.
[4] Robert D Blumofe, Christopher F Joerg, Bradley C Kuszmaul, Charles E Leiserson,
Keith H Randall, and Yuli Zhou. 1996. Cilk: An efficient multithreaded runtime
system. Journal of parallel and distributed computing 37, 1 (1996), 55–69.
[5] M. Castro, P. Druschel, A. . Kermarrec, and A. I. T. Rowstron. 2002. Scribe: a
large-scale and decentralized application-level multicast infrastructure. IEEE
Journal on Selected Areas in Communications 20, 8 (Oct 2002), 1489–1499. https:
//doi.org/10.1109/JSAC.2002.803069
[6] Miguel Castro, Peter Druschel, Anne-Marie Kermarrec, Animesh Nandi, Antony
Rowstron, and Atul Singh. 2003. SplitStream: High-Bandwidth Multicast in
Cooperative Environments. SIGOPS Oper. Syst. Rev. 37, 5 (Oct. 2003), 298–313.
https://doi.org/10.1145/1165389.945474
[7] Mosharaf Chowdhury and Ion Stoica. 2015. Efficient Coflow Scheduling With-
out Prior Knowledge. In Proceedings of the 2015 ACM Conference on Special In-
terest Group on Data Communication (London, United Kingdom) (SIGCOMM
’15). Association for Computing Machinery, New York, NY, USA, 393–406.
https://doi.org/10.1145/2785956.2787480
[8] Mosharaf Chowdhury, Matei Zaharia, Justin Ma, Michael I. Jordan, and Ion
Stoica. 2011. Managing Data Transfers in Computer Clusters with Orchestra.
SIGCOMM Comput. Commun. Rev. 41, 4 (Aug. 2011), 98–109. https://doi.org/
10.1145/2043164.2018448
[9] Mosharaf Chowdhury, Yuan Zhong, and Ion Stoica. 2014. Efficient Coflow
Scheduling with Varys. In Proceedings of the 2014 ACM Conference on SIGCOMM
(Chicago, Illinois, USA) (SIGCOMM ’14). Association for Computing Machinery,
New York, NY, USA, 443–454. https://doi.org/10.1145/2619239.2626315
[10] Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J Franklin, Joseph E Gonzalez,
and Ion Stoica. 2017. Clipper: A low-latency online prediction serving system.
In 14th {USENIX} Symposium on Networked Systems Design and Implementation
({NSDI} 17). 613–627.
[11] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
Marc’aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al. 2012. Large
scale distributed deep networks. In Advances in neural information processing
systems. 1223–1231.
[12] Jeffrey Dean and Sanjay Ghemawat. 2008. MapReduce: Simplified Data Processing
on Large Clusters. Commun. ACM 51, 1, 107–113. https://doi.org/10.1145/
1327452.1327492
[13] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom
Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. 2018. Impala:
Scalable distributed deep-rl with importance weighted actor-learner architectures.
In International Conference on Machine Learning. PMLR, 1407–1416.
[14] Gloo 2020. Collective communications library with various primitives for multi-
machine training. https://github.com/facebookincubator/gloo.
[15] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski,
Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. 2017. Accurate,
large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677
(2017).
[16] Richard L Graham, Timothy S Woodall, and Jeffrey M Squyres. 2005. Open MPI: A
flexible high performance MPI. In International Conference on Parallel Processing
and Applied Mathematics. Springer, 228–239.
[17] gRPC 2020. gRPC. https://grpc.io/.
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770–778.
[19] Hydro 2020. Hydro. https://github.com/hydro-project.
[20] Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J
Dally, and Kurt Keutzer. 2016. SqueezeNet: AlexNet-level accuracy with 50x
fewer parameters and< 0.5 MB model size. arXiv preprint arXiv:1602.07360 (2016).
[21] IPMulticast 2020. IP Multicast Technology Overview . https://www.cisco.com/c/
en/us/td/docs/ios/solutions_docs/ip_multicast/White_papers/mcst_ovr.html.
https:
[22] keynote 2020.
Keynote: Building a Fusion Engine with Ray.
//ray2020.sched.com/event/eGOL/keynote-building-a-fusion-engine-with-
ray-dr-charles-he-chief-architect-of-storage-and-compute-ant-group.
[23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classifica-
tion with deep convolutional neural networks. In Advances in neural information
processing systems. 1097–1105.
[24] Jeongkeun Lee, Yoshio Turner, Myungjin Lee, Lucian Popa, Sujata Banerjee,
Joon-Myung Kang, and Puneet Sharma. 2014. Application-Driven Bandwidth
Guarantees in Datacenters. SIGCOMM Comput. Commun. Rev. 44, 4 (Aug. 2014),
467–478. https://doi.org/10.1145/2740070.2626326
[25] Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed,
Vanja Josifovski, James Long, Eugene J Shekita, and Bor-Yiing Su. 2014. Scaling
distributed machine learning with the parameter server. In 11th {USENIX} Sym-
posium on Operating Systems Design and Implementation ({OSDI} 14). 583–598.
[26] Mu Li, Li Zhou, Zichao Yang, Aaron Li, Fei Xia, David G Andersen, and Alexander
Smola. 2013. Parameter server for distributed machine learning. In Big Learning
NIPS Workshop, Vol. 6. 2.
[27] Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Gold-
berg, Joseph Gonzalez, Michael Jordan, and Ion Stoica. 2018. RLlib: Abstractions
for distributed reinforcement learning. In International Conference on Machine
Learning. PMLR, 3053–3062.
[28] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. 2018. Shufflenet
v2: Practical guidelines for efficient cnn architecture design. In Proceedings of the
European conference on computer vision (ECCV). 116–131.
[29] Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Tim
Harley, Timothy P. Lillicrap, David Silver, and Koray Kavukcuoglu. 2016. Asyn-
chronous Methods for Deep Reinforcement Learning. In Proceedings of the 33rd
International Conference on International Conference on Machine Learning - Volume
48 (New York, NY, USA) (ICML’16). JMLR.org, 1928–1937.
[30] Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard
Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I. Jordan,
and et al. 2018. Ray: A Distributed Framework for Emerging AI Applications. In
Proceedings of the 12th USENIX Conference on Operating Systems Design and Im-
plementation (Carlsbad, CA, USA) (OSDI’18). USENIX Association, USA, 561–577.
[31] MPICH 2020. MPICH. https://www.mpich.org/.
[32] Derek G Murray, Malte Schwarzkopf, Christopher Smowton, Steven Smith, Anil
Madhavapeddy, and Steven Hand. 2011. CIEL: a universal execution engine for
distributed data-flow computing.
[33] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R.
Devanur, Gregory R. Ganger, Phillip B. Gibbons, and Matei Zaharia. 2019.
PipeDream: Generalized Pipeline Parallelism for DNN Training. In Proceedings
of the 27th ACM Symposium on Operating Systems Principles (Huntsville, Ontario,
Canada) (SOSP ’19). Association for Computing Machinery, New York, NY, USA,
1–15. https://doi.org/10.1145/3341301.3359646
[34] NCCL 2020. The NVIDIA Collective Communication Library (NCCL). https:
//developer.nvidia.com/nccl.
[35] NumPy 2020. NumPy. https://numpy.org/.
[36] Christopher Olston, Fangwei Li, Jeremiah Harmsen, Jordan Soyke, Kiril Gorovoy,
Li Lao, Noah Fiedel, Sukriti Ramesh, and Vinu Rajashekhar. 2017. TensorFlow-
Serving: Flexible, High-Performance ML Serving. In Workshop on ML Systems at
NIPS 2017.
[37] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gre-
gory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga,
Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison,
Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep
Learning Library. In Advances in Neural Information Processing Systems, H. Wal-
lach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (Eds.),
Vol. 32. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2019/file/
bdbca288fee7f92f2bfa9f7012727740-Paper.pdf
[38] Yanghua Peng, Yibo Zhu, Yangrui Chen, Yixin Bao, Bairen Yi, Chang Lan,
Chuan Wu, and Chuanxiong Guo. 2019. A Generic Communication Sched-
uler for Distributed DNN Training Acceleration. In Proceedings of the 27th
ACM Symposium on Operating Systems Principles (Huntsville, Ontario, Canada)
(SOSP ’19). Association for Computing Machinery, New York, NY, USA, 16–29.
https://doi.org/10.1145/3341301.3359642
[39] Qifan Pu, Ganesh Ananthanarayanan, Peter Bodik, Srikanth Kandula, Aditya
Akella, Paramvir Bahl, and Ion Stoica. 2015. Low Latency Geo-Distributed Data
Analytics. In Proceedings of the 2015 ACM Conference on Special Interest Group on
Data Communication (London, United Kingdom) (SIGCOMM ’15). Association for
Computing Machinery, New York, NY, USA, 421–434. https://doi.org/10.1145/
2785956.2787505
[40] Qifan Pu, Shivaram Venkataraman, and Ion Stoica. 2019. Shuffling, Fast and Slow:
Scalable Analytics on Serverless Infrastructure. In 16th USENIX Symposium on
Networked Systems Design and Implementation (NSDI 19). USENIX Association,
Boston, MA, 193–206. https://www.usenix.org/conference/nsdi19/presentation/
pu
[41] Ray Parameter Server 2020. Parameter Server. https://ray.readthedocs.io/en/
latest/auto_examples/plot_parameter_server.html.
[42] Ray Serve 2021. Ray Serve. https://docs.ray.io/en/master/serve/.
[43] Redis 2020. Redis. https://redis.io/.
[44] Matthew Rocklin. 2015. Dask: Parallel computation with blocked algorithms and
task scheduling. In Proceedings of the 14th python in science conference. Citeseer.
[45] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-
Chieh Chen. 2018. Mobilenetv2: Inverted residuals and linear bottlenecks. In
Proceedings of the IEEE conference on computer vision and pattern recognition.
4510–4520.
[46] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347
(2017).
[47] Alexander Sergeev and Mike Del Balso. 2018. Horovod: fast and easy distributed
deep learning in TensorFlow. arXiv:1802.05799 [cs.LG]
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Siyuan Zhuang et al.
[49] Vikram Sreekanti, Chenggang Wu, Saurav Chhatrapati, Joseph E Gonzalez,
Joseph M Hellerstein, and Jose M Faleiro. 2020. A fault-tolerance shim for
serverless computing. In Proceedings of the Fifteenth European Conference on
Computer Systems. 1–15.
[50] Mingxing Tan and Quoc Le. 2019. Efficientnet: Rethinking model scaling for
convolutional neural networks. In International Conference on Machine Learning.
PMLR, 6105–6114.
[48] Karen Simonyan and Andrew Zisserman. 2015. Very Deep Convolutional Net-
works for Large-Scale Image Recognition. In 3rd International Conference on
Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Confer-
ence Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.).
[51] Guanhua Wang, Shivaram Venkataraman, Amar Phanishayee, Nikhil Devanur,
Jorgen Thelin, and Ion Stoica. 2020. Blink: Fast and Generic Collectives for
Distributed ML. In Proceedings of Machine Learning and Systems, I. Dhillon, D. Pa-
pailiopoulos, and V. Sze (Eds.), Vol. 2. 172–186. https://proceedings.mlsys.org/
paper/2020/file/43ec517d68b6edd3015b3edc9a11367b-Paper.pdf
[52] Stephanie Wang, John Liagouris, Robert Nishihara, Philipp Moritz, Ujval Misra,
Alexey Tumanov, and Ion Stoica. 2019. Lineage Stash: Fault Tolerance off the Crit-
ical Path. In Proceedings of the 27th ACM Symposium on Operating Systems Prin-
ciples (Huntsville, Ontario, Canada) (SOSP ’19). Association for Computing Ma-
chinery, New York, NY, USA, 338–352. https://doi.org/10.1145/3341301.3359653
[53] Matei Zaharia, Reynold S. Xin, Patrick Wendell, Tathagata Das, Michael Armbrust,
Ankur Dave, Xiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J.
Franklin, and et al. 2016. Apache Spark: A Unified Engine for Big Data Processing.
Commun. ACM 59, 11 (Oct. 2016), 56–65. https://doi.org/10.1145/2934664
Hoplite: Efficient and Fault-Tolerant Collective Communication for Task-Based Distributed Systems
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
APPENDIX
Appendices are supporting material that has not been peer-reviewed.
A MICROBENCHMARKS ON SMALL OBJECTS
We present the microbenchmarks for multiple collective communi-
cation primitives for small objects (1KB, 32KB) in Figure 14. Note
that Hoplite stores object contents in object directory service for
objects smaller than 64 KB (§3.2), so there is no collective commu-
nication for Hoplite. Again, we compare with Ray, Dask, OpenMPI,
and Gloo. We do not compare with Horovod for the same reason
that Horovord has three backends: OpenMPI, Gloo, and NCCL. We
have already compared with OpenMPI and Gloo. NCCL is for GPU,
and Hoplite currently does not support GPU.
Hoplite is the best or close to the best among all these alternatives.
Gloo has the best performance for broadcast and allreduce. Hoplite
is more efficient than Ray, and Dask because Hoplite uses stores
the object data directly in object directory service.
B ABLATION STUDY ON REDUCE TREE
DEGREE
Here we study the choice of 𝑑 in the AWS EC2 setting (§5). The
best choice of 𝑑 depends on network characteristics, the size of
the object to reduce, and the number of participants. We compare
three choices of 𝑑: 1 (a single chain), 2 (a binary tree), and 𝑛 (a root
connects everyone else). The results are in Figure 15. As expected
from our analysis in (§3.4), when the object size is small, 𝑑 = 𝑛 is
the best because the main bottleneck is the network latency. When
the object size is medium (256KB, 1MB), 𝑑 = 𝑛 becomes unstable
for reduce. We suspect that this is due to incast or due to gRPC
characteristics. When object size is 4MB or 8MB, we need to choose
between 𝑑 = 1 and 𝑑 = 2 based on the number of participants. This
is because both network latency and network throughput can be a
bottleneck in tree reduce. When object size is 16MB or larger, we
choose 𝑑 = 1 to mitigate the throughput bottleneck in reduce.
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Siyuan Zhuang et al.
Figure 14: Latency comparison of Hoplite, OpenMPI, Ray, Dask, and Gloo on standard collective communication primitives (e.g., broadcast, gather, reduce,
allreduce) on 1KB and 32KB objects. To show the results more clearly, we split the results of Allreduce into two groups: group (i) includes Hoplite, Ray, and
Dask, and group (ii) includes Hoplite, OpenMPI, and two different allreduce algorithms in Gloo.
Figure 15: Ablation study of reduce latency on the reduce tree degree 𝑑 with different object size and number of participants.
024Latency (s)×10−3Broadcast 1KB012×10−2Gather 1KB012×10−2Reduce 1KB024×10−2Allreduce(i) 1KB012×10−2Allreduce(ii) 1KB481216Number of Nodes024Latency (s)×10−3Broadcast 32KB481216Number of Nodes0.00.51.0×10−1Gather 32KB481216Number of Nodes0.02.55.07.5×10−2Reduce 32KB481216Number of Nodes024×10−2Allreduce(i) 32KB481216Number of Nodes0.00.51.0×10−2Allreduce(ii) 32KBHopliteOpenMPIRayDaskGloo (Broadcast)Gloo (Ring Chunked)Gloo (Halving Doubling)1234Latency (s)×10−34KB123456×10−332KB0.00.51.01.52.02.53.03.54.0×10−2256KB0123456×10−21MB81216202428323640444852566064Number of Nodes0.60.81.01.21.41.6Latency (s)×10−24MB81216202428323640444852566064Number of Nodes1.21.41.61.82.02.2×10−28MB81216202428323640444852566064Number of Nodes2.02.53.03.54.04.55.0×10−216MB81216202428323640444852566064Number of Nodes0.40.50.60.70.80.91.0×10−132MBd=1d=2d=n