manually check them. The threshold needs to be adjusted in
practice: a small threshold may result in many possible false
positives and thus increase the demand for manual checking,
and vice versa. If a detected candidate eye region is indeed a
false positive, we manually assign a correct rectangular region
enclosing both eyes as the input to the cascade classiﬁer, which
leads to correct eye detection in practice.
In the second step, EyeTell uses a shape-based approach to
reﬁne the two AOIs by exploring the predicted dark circular
appearance of eye features [42]. Speciﬁcally, we deﬁne the
center of a circular pattern as the point where most image
gradient vectors intersect. Then we search for the optimal
point by using an objective function that measures how well
the gradient vectors and the eye center displacement vectors
are aligned. Moreover, considering the fact that the eye center
usually appears darker than other areas in the eye, we attach
each point with a weight of its inverse intensity in the objective
function. Once the optimal points of the two AOIs (i.e., the
two eye centers) are located, we reﬁne the positions of two
AOIs in the frame and then resize them to a ﬁxed ratio. The
resizing operation can minimize the areas of the two AOIs
while maintaining important eye features within them. The red
cross in Fig. 5(a) denotes the detected eye center.
(a) Eye center
(b) Fitted limbus
Fig. 5. Examples of our detected eye center and limbus.
2) Limbus detection: In this step, EyeTell determines the
elliptical outline of the limbus from each identiﬁed AOI by
ﬁrst identifying a set of possible limbus edge points and then
ﬁtting an ellipse model from those edge points [10]. In contrast
to other popular limbus detection methods [43], [44], this
method does not rely on any pre-deﬁned threshold, which
allows EyeTell to reliably detect the limbus regardless of eye
appearance, users, and lighting conditions. Moreover, it can
detect the limbus from out-of-focus images because it does
not depend on the existence of very strong edge. We illustrate
this process in what follows.
Since limbus edge points are part of the edge, we search
for them by analyzing the radial derivatives within each AOI.
Speciﬁcally, we transform a given AOI into the polar form
and then calculate the vertical derivative of each pixel. In
our implementation, we select the pixel with the largest radial
derivative in each column as the limbus edge point.
Special attention is paid to non-edge points that are in-
correctly detected as edge points, which occurs if the radial
derivatives of non-edge points are larger than those of true
limbus edge points. According to our experimental observa-
tions, we use the following process to ﬁlter out as many such
non-edge points as possible. First, we notice that nearby light
sources can leave specularities on the cornea. The pixels within
these specularities can have very large radial derivatives and
thus be incorrectly identiﬁed as limbus edge points. To deal
with this case, we compare each pixel value with a threshold,
e.g., 150 (the pixel value is between 0 and 255), to identify
a set of possible specularities and then inpaint these small
connected regions. The effective threshold depends on the
recording environment, which we choose empirically. Second,
we observe that the upper eyelid may cover part of the iris
and therefore lead to incorrect limbus edge points. To cope
with this case, we use three points, two eye corners and the
iris-eyelid boundary point right above the given eye center, to
ﬁt a parabola to approximate the upper eyelid and then discard
the points that fall outside the parabola.
Finally, we ﬁt an ellipse model from the set of edge points
using the iterative method in [45]. In each iteration, a minimum
number of edge points are randomly selected from available
ones to ﬁt an ellipse model through a least-square approach.
Then a support function is calculated to evaluate how ﬁt the
model is to the entire set. We use the support function in [45]
that measures how well the geometric gradients of the ﬁtted
ellipse model align with the image gradients. Fig. 5(b) denotes
a detected limbus in our experiment.
3) Gaze trace estimation: In this step, we estimate one gaze
point from each frame to obtain a complete gaze trace from
the entire video. To do so, we use the detected eye centers and
limbus in the 2D domain to recover the corresponding 3D eye
centers and optical axes. We then estimate the gaze point as the
intersection between the optical axes and the virtual 3D screen.
We further refer to the gaze point as point-of-gaze (PoG),
which can be simply denoted by a vector [x, y]T . Here x and
y correspond to the coordinates of the PoG along x and y axis
on the screen, respectively. For the beneﬁt of better readability,
here we omit the detailed mathematical deduction to calculate
a PoG. For more details, please refer to Appendix A.
By calculating the PoG for each eye in each frame, we ob-
tain two complete gaze traces from the recorded video, denoted
by Ψl = (PoGl
nf )
for the left and right eyes, respectively, where nf is the number
of frames in the video.
nf ) and Ψr = (PoGr
1, . . . , PoGr
1, . . . , PoGl
Since the extracted gaze traces are usually noisy and
unstable, we apply outlier detection and ﬁltering to enhance
their quality. To detect possible outliers, we check the distance
between the two estimated eye centers in each frame. If the
distance in the ith frame is larger than an anatomical threshold,
e.g., 80 mm, we consider that at least one PoG between PoGl
i
and PoGr
i is an outlier. In this case, we replace the PoG that
yields a larger PoG change between adjacent frames with the
one that leads to a smaller change.
In the subsequent ﬁltering step, we ﬁrst obtain a raw gaze
trace Ψ = (PoG1, . . . , PoGnf ) by taking the average of the
left and right gaze traces, where
PoGl
(1)
for all i ∈ [1, nf ]. We then apply a triangular kernel [46] to Ψ,
which assigns linear weights to each PoG in the time order.
(cid:2)j
Speciﬁcally, for each j ∈ [1, nf ], we calculate
i=j−N1+1 i × PoGi
i + PoGr
i
PoGi =
PoGj =
,
2
(cid:2)j
,
i=1 i
to 5 in our
(2)
where N1
implementa-
tion. The ﬁnal gaze trace for keystroke inference is Ψ =
is empirically set
148
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:38:20 UTC from IEEE Xplore.  Restrictions apply. 
(PoG1, . . . , PoGnf ). For convenience, we call each element
in Ψ a PoG as well.
where N3 and φ1 are both system parameters that are empir-
ically set to 5 and π
4 in our experiment, respectively.
D. Trace Decoding
In this step, EyeTell decodes the gaze trace Ψ to obtain
some candidate input sequences on the touchscreen. Depending
on the soft keyboard the victim types on, the candidate input
sequence may correspond to a lock pattern, a PIN, a word,
or a sentence. Generally speaking, trace decoding is done in
four steps. First, we identify the turning points in a gaze trace
and then divide the whole trace into a sequence of segments,
each corresponding to a sudden change in the PoG. Second,
we convert each segment into a small set of candidate vectors.
Third, given the sequence of segments and their corresponding
candidate vector sets, we enumerate all possible combinations
of candidate vectors. For each possible combination of candi-
date vectors, we traverse the soft keyboard to check whether or
not the combination can be mapped into a valid input sequence.
Finally, we rank all the valid input sequences according to
certain heuristic rules and generate a ﬁnal set of candidate
input sequences for a given gaze trace. In what follows, we
use the pattern-lock keyboard as the example to illustrate trace
decoding and then point out the difference when applying
EyeTell to PIN and alphabetical keyboards.
1) Trace segmentation: We ﬁrst apply a moving average
ﬁlter to further smooth the gaze trace extracted in the last
step, as it does not exhibit any clear pattern for segmentation.
The length of the moving window has a direct impact on the
segmentation performance. On the one hand, if the window is
too short, the ﬁltered gaze trace is not sufﬁciently smooth. On
the other hand, if the window is too long, some sudden changes
may be buried, resulting in some undetectable turning points.
We empirically set the moving-window length to 10 based on
analyzing our experiment data.
We then segment the smoothed trace by identifying the
turning points that separate adjacent segments. For simplicity,
we abuse the notation by letting Ψ = (PoG1, . . . , PoGnf )
denote the smoothed trace as well. Suppose that Ψ consists
of two segments as an example. In the ideal case, the points
in each segment lie in a straight line, and the intersection of
the two lines is the turning point between two segments. Based
−−→
on this observation, we ﬁrst estimate the moving direction of
PoGi,j = PoGj − PoGi
each PoG (or element) in Ψ. Let
be the vector for ∀i, j ∈ [1, nf ]. For each PoGi and the
next N2 PoGs (i.e., {PoGj}i+N2−1
j=i+1 ), we compute N2 vectors
{−−→
PoGj,i}i+N2−1
j=i+1 , where N2 is a system parameter empirically
(cid:2)i+N2−1
set to 5 in our experiment. We further calculate
j=i+1
N2
(3)
as the moving direction of PoGi. Let θi ∈ [−π, π) denote
−−→
PoGi. We can then obtain a sequence of angles
the angle of
θ1, . . . , θnf−N2+1) for the gaze trace Ψ. For every N3 adjacent
PoGs such as {PoGj}i+N3−1
, we consider them in the same
segment if and only if(cid:2)i+N3−1
j=i
−−→
PoGi+1,i
−−→
PoGi =
j=i
|θj+1 − θj|
N3
≤ φ1 ,
We then search for turning points as follows. Starting from
(cid:2) such that
(cid:2)i(cid:2)+N3−1
j=i(cid:2)
|θj+1−θj|
N3
(cid:2)i(cid:2)(cid:2)+N3−1
(cid:2), we proceed to ﬁnd the smallest i
|θj+1−θj|
i = 1, we ﬁnd the smallest i
> φ1
and then regard PoGi(cid:2) as the ending point of the ﬁrst segment.
(cid:2)(cid:2) such
Starting from i
≤ φ1 and then consider PoGi(cid:2)(cid:2) as the
that
(cid:2) and
starting point of the second segment. After determining i
(cid:2) and i
(cid:2)(cid:2) to ﬁnd i1 with the largest
(cid:2)(cid:2), we search between i
i
(cid:2)i1+N3−1
and consider PoGi1 as the turning point
|θj+1−θj|
j=i(cid:2)(cid:2)
j=i1
N3
N3
between the ﬁrst two segments.
Repeating the above process, we can identify all the turning
points in the gaze trace. Suppose that nt turning points are
found in total. Combined with the ﬁrst and last PoGs of
the gaze trace, the total nt + 2 points correspond to nt + 1
segments. Denote the nt + 2 points by {TPi}nt+2
i=1 , where TP1
and TPnt+2 correspond to the ﬁrst and last PoGs of the gaze
trace, respectively, and TPi (∀i ∈ [2, nt + 1]) are the turning
points. In the remainder of the paper, we denote the number
of segments by ns. Therefore, ns = nt +1. The ﬁnal output of
trace segmentation comprises ns segments, each of which can
be represented by its length and angle. Speciﬁcally, assuming
TPi = [xi, yi]T , the i-th segment can be characterized by
[xi+1 − xi, yi+1 − yi]T for all i ∈ [1, ns].
We use the example in Fig. 6 to shed more light on trace
segmentation. Speciﬁcally, Fig. 6(a) shows a two-segment gaze
trace to decode; Fig. 6(b) shows the gaze trace after applying
the moving average ﬁlter; Fig. 6(c) shows the angles of the
PoGs on the trace; and Fig. 6(d) shows the ending point of
the ﬁrst segment and the starting point of the second segment.
N2, N3, and φ1 depend on the factors such as frame rate,
signal-to-noise ratio (SNR) of the video, etc. For example,
N2 and N3 increase with the frame rate and decrease with
SNR generally. In this paper, we choose these parameters
empirically by experimenting them with a small portion of
data and observing the results of segmentation. In practice,
we believe that they need to be adjusted or trained in different
scenarios.
TABLE I.
MAPPING BETWEEN ALPHABETICAL AND QUASI-PIN
KEYBOARDS DEPICTED IN FIG. 8(A).
1
4
7
q,w,e,r
a,s,d
z,x
2
5
8
t,y
f,g,h
c,v,b
3
6
9
u,i,o,p
j,k,l
n,m
2) Decoding segment: We observe that only a limited
number of gaze segments are permissible on any typical
soft keyboard (PIN, pattern lock, and alphabetic), which are
referred to as legitimate segments hereafter. In this step, we
decode a given gaze segment into a small set of candidate
legitimate segments on the pattern-lock keyboard. This is
done by calculating the Euclidean distances between the given
segment and all legitimate ones and then selecting those with
shorter distances as the candidates.
Let us ﬁrst
look into more details of the pattern-lock
keyboard and its corresponding legitimate segments. Fig. 7
depicts the dimensions of the pattern-lock keyboard layout on
149
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:38:20 UTC from IEEE Xplore.  Restrictions apply. 



7XUQLQJSRLQW
L



L
\

\




     
[



     
[
(a) Original gaze trace.
(b) Filtered gaze trace
G
D
U

Q
R
L
W
F
H
U
L
'










RI*D]H
(c) Direction
G
D
U

Q
R
L
W
F
H
U
L
'

I

R
H
J
Q
D
K
&
7XUQLQJSRLQW
L
L
π









RI*D]H



(d) Change of direction
Fig. 6. An illustration for trace dividing.

















(a) Pattern-lock
(b) PIN
(c) Alphabetical
Fig. 7. Measurement of the three keyboards. The unit is pixel.
a Google Nexus 6 smartphone with Android 5.1.1, including
the radius of nine white circles, the horizontal gap between
two neighboring circles, and the vertical gap between two
neighboring circles. All these dimensions are also listed in
Table XI in Appendix B. We further plot all the 24 possible
segments on the pattern-lock keyboard in Fig. 9 and then
calculate their lengths and angles. The 24 segments lead to
ﬁve lengths and 16 angles in total.
√
We ﬁrst normalize the segment length to facilitate segment
decoding. As we can see from Fig. 9, the minimum segment
length is 1, so we try to make the minimum normalized
segment length be 1 as well via the following approach. First,
we sort the segments in the ascending order of their lengths.
Let Lmax denote the longest segment length. Then we select
the shortest segment and calculate the ratio ρ between Lmax
and its length. According to Table XII, the length ratio between
2. Therefore,
any two legitimate segments is no larger than 2
we compare ρ to a threshold ρmax. If ρ ≤ ρmax, the currently
selected segment is used for normalizing all the segments.
Otherwise, we select the next shortest segment, calculate a new
ρ, and compare it to ρmax. This process ends until ρ ≤ ρmax.
The currently selected segment is the one used for length
normalization, and the normalized segment lengths smaller
than 1 are all set to 1. ρmax should be larger than 2
2 to
accommodate the noisy and instable nature of the gaze trace.
Next, we compute the Euclidean distance between each
normalized segment and each legitimate segment in Fig. 9.
Suppose that we look for η candidate legitimate segments
for each normalized segment. Those leading to the top-η
shortest Euclidean distances are selected as the candidates.
√
Intuitively speaking, the larger η, the more likely that the
correct legitimate segment is included in the candidate set,
the less pinpointing capability the attacker has, and vice versa.
The ﬁnal output in this step corresponds to ns candidate