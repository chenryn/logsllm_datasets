52]. To help explain the formulation of the optimization problem,
we summarize the notations in Table 3. Specifically, the problem is
formulated as follows:
(1)
min
s.t.
t
Ladv(x ⊕ t
′ ∈ {Φ(T
′)|T
′; M)
′ ∈ A} & x = Φ(X)
where X is the pristine 3D point cloud and x represents the corre-
sponding 2D input feature matrix. Φ(·) is the pre-processing func-
tion that maps X into x (§2.1). T ′ and t′ are the corresponding
adversarial spoofed 3D point cloud and adversarial spoofed input
feature matrix. A is a set of spoofed 3D point cloud generated from
LiDAR spoofing attacks. Ladv(·; M) is the adversarial loss designed
to achieve the adversarial goal given the machine learning model M.
The constraints are used to guarantee that the generated adversarial
examples t′ satisfy the spoofing attack capability.
Figure 2 overviews the analysis tasks needed to solve the opti-
mization problem. First, we need to conduct an input perturbation
analysis that formulates the spoofing attack capabilities A and
merging function ⊕. Second, we need to perform a model analysis
to design an objective function to generate adversarial examples.
Third, as a case study to understand the impact of the attacks at the
Figure 6: The point cloud from a real vehicle reflection (left)
and from the spoofing attack (right) in a 64-line HDL-64E
LiDAR. The vehicle is around 7 meters in front of the AV.
distances for each point at different angles. We generate 160 points
covering 16 vertical lines, 10 points for each line with continuous
horizontal angles. To trigger immediate control decision changes
in an AV, the spoofed obstacle needs to be close to the victim AV.
Thus, we set the generated distances of the spoofed point to be
within 4 to 6 meters to the victim AV. We generate 100 different
spoofed patterns in total, but we are not able to observe spoofed
obstacles for any of these patterns.
Summary. In these experiments, we try various blind spoofing
attack strategies directly derived from the state-of-the-art LiDAR
spoofing attack, but none of them succeed in generating spoofed
obstacles in the LiDAR-based perception pipeline in Baidu Apollo.
There are two potential reasons. First, as described earlier, the
current attack methodology can only cover a very narrow spoofing
angle, i.e., 8 ◦ of horizontal angle even after our setup improvements.
Second, the coverage of vertical angles is limited by the frequency
of spoofing laser pulses. Thus, when attacking a LiDAR with more
vertical angles, e.g., a 64-line LiDAR, since a 64-line LiDAR takes
similar time as a 16-line LiDAR in scanning vertical angles, the
attacker cannot spoof more vertical angles than those for a 16-
line LiDAR. Thus, the current methodology limits the number of
spoofed points, making it hard to generate enough points to mimic
an important road obstacle.
To illustrate that, as shown in Fig. 6, the point cloud for a real
vehicle has a much wider angle and much more points than the
attack traces reproduced by us. Thus, blindly applying the spoofing
attack cannot easily fool the machine learning based object detec-
tion process in the LiDAR-based perception pipeline. In the next
section, we explore the possibility of further exploiting machine
learning model vulnerabilities to achieve our attack goal.
5 IMPROVED METHODOLOGY: ADV-LIDAR
As discussed in §4, without considering the machine learning model
used in LiDAR-based perception, blindly applying existing LiDAR
spoofing attacks can hardly achieve the attack goal of generating
front-near obstacles. Since it is known that machine learning output
can be maliciously altered by carefully-crafted perturbations to the
input [18, 20, 29, 41, 57], we are then motivated to explore the
possibility of strategically controlling the spoofed points to fool
the machine learning model in LiDAR-based perception. In this
section, we first describe the technical challenges after involving
Notation
X
X ′
T
T ′
(wx, wy, wx)
(u, v)
M
N(u, v)
A
Q(M, ·)
M(·)
f (·)
H(θ, τ , ϵ)
ST
GT (T , ·)
Description
3D point cloud
Adversarial 3D point cloud
Spoofed 3D point cloud
Adversarial spoofed 3D point cloud
3D Cartesian coordinate
Coordinate of t
Machine learning model
4-pixel neighbor at the location (u, v)
Spoofing attack capability
Extraction function
Gaussian mask
Objective function
2D Homography Matrix (θ : rotation, ϵ : scaling ; τ : translation )
Set of spoofed 3D point cloud
Global spatial transformation function for 3D point cloud
Notation Description
x
x′
t
t′
Lθ , Lτ
(u′, v′)
I·
S(·)
Φ(·)
⊕(·)
(px, py)
Ladv(·)
Sh
St
Gt(t, ·)
Input feature matrix
Adversarial input feature matrix
Spoofed input feature matrix
Adversarial spoofed input feature matrix
Upper bound of θ, τ during sampling
Coordinate of t′
Model outputs
Height Scaling function
Mapping function (3D→ 2D)
Merge function
Center points of the Gaussian mask
Adversarial loss
Height scaling ratio
Set of spoofed input feature matrix
Global spatial transformation function for input feature matrix
Table 3: Notations adopted in this work.
AV driving decision level, we further perform a driving decision
analysis using the identified adversarial examples. More details
about these tasks are as follows:
Input perturbation analysis. Formulating A and ⊕ is non-
trivial. First, previous work regarding LiDAR spoofing attacks nei-
ther provided detailed measurements on the attacker’s capability
in perturbing 3D point cloud nor expressed it in a closed form ex-
pression. Second, point cloud data is pre-processed by several steps
as shown in Section 2.1 before turning into machine learning input,
which means the merging function ⊕ cannot be directly expressed.
To address these two challenges, as will be detailed later in §6, we
first conduct spoofing attacks on LiDAR to collect a set of possible
spoofed 3D point cloud. Using such spoofed 3D point cloud, we
model the spoofing attack capability A. We further analyze the
pre-processing program to obtain the additional constraints to the
machine learning input perturbation, or the spoofed input feature
matrix. Based on this analysis, we formulate the spoofed input
feature matrix into a differentiable function using global spatial
transformations, which is required for the model analysis.
Objective function design and model analysis. As intro-
duced earlier in §5.1, in LiDAR-based perception in AV systems,
the machine learning model output is post-processed (§ 2.1) before
turning into a list of perceived obstacles. To find an effective ob-
jective function, we study the post-processing steps to extract key
strategies of transforming model output into perceived obstacles,
and formulate it into an objective function that reflects the attack
goal. In addition, we find that our optimization problem cannot
be effectively solved by directly using existing optimization-based
methods. We analyze the loss surface, and find that this inefficiency
is caused by the problem nature. To address this challenge, we
improve the methodology by combining global sampling with opti-
mization. Details about the analysis methodology and results are
in §7 and § 8.
Driving decision case study. With the results from previous
analysis steps, we can generate adversarial 3D point cloud that
can inject spoofed obstacles at the LiDAR-based perception level.
To understand their impact at the AV driving decision level, we
construct and evaluate two attack scenarios as case studies. The
evaluation methodology and results are detailed later in §9.
6 INPUT PERTURBATION ANALYSIS
To generate adversarial examples by solving the above optimization
problem in Equation 1, we need to formulate merging function ⊕
and input feature matrix spoofing capability Φ(A) as a closed form.
In this section, we first analyze the spoofing attack capability (A),
and then use it to formulate Φ(A).
6.1 Spoofing Attack Capability
Based on the attack reproduction experiments in §4, the observed
attack capability (A) can be described from two aspects:
Number of spoofed points. As described in §4, even though it
is possible to spoof around 100 points after our setup improvement,
we find that around 60 points can be reliably spoofed in our ex-
periments. Thus, we consider 60 as the highest number of reliable
spoofed points. Noticed that, the maximum number of spoofed
points could be increased if the attacker uses more advanced attack
equipment. Here, we choose a set of devices that are more acces-
sible (detailed in §4) and end up with the ability to reliably spoof
around 60 points. In addition, considering that an attacker may use
a slower laser or cruder focusing optics, such as in the setup by
Shin et al. [44], we also consider 20 and 40 spoofed points in our
analysis.
Location of spoofed points. Given the number of spoofed
points, the observed attack capability in placing these points are
described and modeled as follows:
(1) Modify the distance of the spoofed point from the LiDAR
by changing the delay of the attack laser signal pulses in
small intervals (nanosecond scale). From the perspective of
spoofed 3D point cloud T , this can be modeled as moving the
position of the spoofed points nearer or further on the axis
r that connects the spoofed points and the LiDAR sensor by
distance ∆r (Fig. 7 (a)).
(2) Modify the altitude of a spoofed point within the vertical
range of the LiDAR by changing the delay in intervals of
2.304 µs. From the perspective of spoofed 3D point cloud T ,
this can be modeled as moving the position of the spoofed
points from vertical line to vertical line to change the height
of it by height ∆h (Fig. 7 (b)).
(3) Modify the azimuth of a spoofed point within a horizontal
viewing angle of 8◦ by changing the delay in intervals of
55.296 µs. By moving the LiDAR spoofer to different locations
(2)
Because of such pre-processing, the spoofed input feature matrix
t′ cannot be directly added to the input feature matrix x to attain
the adversarial input feature matrix x′. To attain x′, we express such
“addition” operation (⊕) as a differentiable function shown below.
Note that in this equation we do not include a few features in Table 1
such as direction and distance since they are either constant or can
be derived directly from the features included in the equation.

t′ =
x′ = x
cnt
avд_h · I x
(I x
(I x
avд_int · I x
max_int · 1{I x
 I x
cnt + I t′
I x
avд_h · I t′
cnt + I t′
cnt)/(I x
max_h, I t′
max_h)
max(I x
cnt + I t′
avд_int · I t′
cnt)/(I x
max_h, I t′
= max {I x
cnt + I t′
cnt)
cnt + I t′
cnt)
max_h }}
max_h
Modeling input feature matrix spoofing capability Φ(A).
To model input feature matrix spoofing capability Φ(A), it equals
to representing adversarial input feature matrix t′ with known
spoofed input feature matrix t. We can use global spatial transfor-
mations including rotation, translation and scaling, under certain
constraints to represent the input feature matrix spoofing capability.
Here the translation and scaling transformation interprets the at-
tack capability in terms of modifying the azimuth of 3D point cloud
while the rotation transformation interprets the attack capability in
terms of modifying the distance of 3D point cloud from the LiDAR.
Specifically, we apply the global spatial transformation to a set
of the spoofed input feature matrix St to formulate the spoofed
input feature matrix spoofing capability Φ(A) and to represent
adversarial spoofed input feature matrix t’. For each spoofed input
feature matrix t ∈ St, it is mapped from a corresponding spoofed
3D point cloud T such that t = Φ(T).
(i) to denote values of the i-th position on the spoofed