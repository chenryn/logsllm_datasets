E
(9)
After obtaining the chi-square distance for each job, we
calculated the chi-squared probability using the uniformity
distance with the degree of freedom being K−1. We ensured
that all fuzzing jobs have a chi-squared probability > 0.05
to be considered uniform.
For each ﬁle format, we let FEET generate and analyse
multiple jobs with various fuzzing conﬁgurations and se-
lect fuzzed jobs ranked by their chi-squared probability and
uniqueness.
2.2 Device and Task Management
To test the Mobile OS’s resistance to the malformed ﬁles,
we would need to open/render each ﬁle individually and log
the results. With the vast quantity of fuzzed ﬁles to be
tested, we needed an automated system to deliver jobs to the
devices, collect and consolidate results. Hence, we built the
Security Testing Arsenal for Mobile Platforms (STAMP).
2.2.1
STAMP is a server based system that automatically coor-
dinates the ﬂow of generating fuzzing jobs, distribute them
to the devices, load balance the work among devices of the
same type and collate the results for centralised graphical
analysis and evaluation.
STAMP
The problem of duplicate test cases is solved by generating
the jobs within STAMP and distributing them to every new
device to be tested. This also removes the waiting time
for fuzzed ﬁles to be generated when a new device registers
with STAMP. When a device registers with STAMP, there
are already a large number of fuzzed inputs downloadable
so the device can be tested without delay.
STAMP server holds history of all fuzzed ﬁles for newly
registered unseen devices and generates new fuzzed ﬁle for
seen devices. On a large scale of mobile clients, requests
for fuzz ﬁles arrive faster than fuzz generation. The server
then has to churn fuzz output at full-capacity creating expo-
nential storage space growth. In addition, mobile network
transfers are slow and less reliable especially for bigger ﬁle
formats. To delay the rate of increase of storage use and
reduce the amount of network data usage, we made use of
the binary diﬀ/patch utility. We generated patches between
the fuzzed ﬁles and the Super Seed Files and stored the
patches instead. This reduced the used space by more than
90%. This also greatly reduced the amount of network traﬃc
(when sending the ﬁle from server to devices) and traded the
download time for mobile device CPU’s patching time. To-
day’s mobile devices have suﬃciently capable processors for
patching ﬁles. Hence, this trade-oﬀ should be worthwhile.
2.2.2
In the MVDP architecture in Figure 1, we mentioned
about SOFT clients. These are small applications called
SOFT (STAMP on-device Fuzzing Tool). SOFT clients are
developed for each mobile platform. The client is easily
portable to other mobile platforms if needed. The SOFT
clients automatically interacts with the STAMP server to
download jobs and patch them (since the fuzzed ﬁles in the
STAMP clients
36server are stored as diﬀ patches). Once a job is downloaded
and patched, the SOFT client renders the fuzzed input using
the native libraries/APIs provided by the OS. Upon com-
pletion of a job the clients reply to the server with the job
results.
2.3 Crash Analysis
STAMP has a web interface which the security researcher
can visit to obtain information about the fuzzing progress.
The researcher can tell exactly which ﬁle caused a crash in
a particular device at a particular time. All the STAMP re-
ported crashes can be reproduced manually using the default
application provided by the respective mobile OS. When do-
ing so, logs on the mobile device (for example - Android)
are monitored for any crashes. For example, image ﬁles are
opened using the default Gallery application in Android.
The Android system logs kernel errors during a crash into
a Tombstone report; which are automatically extracted by
using a shell script for further analysis. Similarly with iOS,
the logs registering the stack trace, register values and fault
locations during a crash are captured.
Inspection of crash dumps - Manual binning of all
crash dumps is not scalable and is time consuming. With
the Crash Automatic Classiﬁcation Engine (CACE), this
process has been automated. CACE processes the reports
from Android’s kernel crash logs (also known as tombstone)
and iOS’s crash logs to generate bins of unique vulnera-
bilities based on the type of crash. The bins are - Ex-
ploitable, Non-exploitable, Potentially Exploitable, Poten-
tially non-exploitable or DOS. After CACE bins the crashes,
researchers can focus on exploitable or potentially exploitable
cases.
The key features of the engine are as follows:
• CACE is a rule based system to identify unique vul-
nerabilities that could have been exposed by multiple
malformed ﬁles. The crash dumps are examined for
terminated signal, fault address, terminated code (if
exists) and the call stack along with the program coun-
ters. Hash value of all these entities combined form a
unique ID for the vulnerability.
• CACE identiﬁes the source of crash in binary (for iOS
and Android) and in source locations (for Android).
For example, if a crash occurs when opening an mp4
ﬁle on Android, CACE is able to point to the location
in the source code/binary that led to the crash from
the stack trace.
The unique vulnerabilities are then manually inspected for
security issues. With the help of a scriptable disassembler.
We inspect the libraries at the fault location to determine
the nature of crash and severity level.
Depending on the type of access, crashes may or may not
be exploitable. Various exception types and exploitability
levels exist and we explored the approaches of CrashWran-
gler and Microsoft’s !exploitable [22] to derive the common
set of conditions for our own implementation; especially for
Android where no crash triaging tool is available for security
related purposes. In general, the SIGSEGV (11), SIGBUS
(10) and SIGILL (4) on Unix ﬂavoured Operating Systems
such as Android and iOS are considered interesting to our
experiments as they mean invalid memory access at the user
space or kernel space and are possibly an indication of ex-
ecution of data segments. Thus CACE automatically ﬁrst
ﬁlters out crashes containing these signals for exploration.
Next, based on the crashing address we found at the point
of crash, we determine the instruction type last executed
before the crash. Several types of ARM instruction classes
correspond to read, write or execute instructions. Some ex-
amples are given below:
ldr r0, r1, r2; - read
bl r1;
mov r1, [r4];
- execute
- write
When the last executed instruction before crashing is an
unprivileged write or execute instruction, it is considered as
potentially exploitable. Comparatively, crashes where the
last instruction is an unprivileged read are not-likely ex-
ploitable.
2.4 Vulnerability Analysis
The degree of control the fuzz input has on exploitabil-
ity is examined manually after a crash is binned by CACE.
For every fuzzed input that corresponds to a potentially ex-
ploitable crash, we search for another existing crash with
the same stack trace. For these two crashes with the same
stack trace, we manually compare the crashing address value
and its address contents/register value to be written or exe-
cuted. If these address values or contents are relatively sim-
ilar (ASLR slides library start addresses), then fuzz input
does not inﬂuence the crash to a suﬃcently large extent and
is henceforth considered as less interesting. On the other
hand if the address/values for the similar crashes are all dif-
ferent, it means the crash is at least partially dependent on
the given fuzzed input.
From time to time, it might not be possible to identify at
least 2 potentially exploitable crashes with the same stack
trace. When this happens, a diﬀ operation is performed on
the crashing input against the original seed ﬁle. The dif-
ferences are recorded as we slowly move from the crashing
input to the original seed ﬁle removing irrelevant ﬁle modiﬁ-
cations. We stop removing modiﬁcations when we reach an
input with a minimal set of modiﬁcations that produces the
same crashing stack trace. When moving from the original
crashing ﬁle to the minimal modiﬁcation crashing ﬁle, the
crashing address value and address contents/register values
are examined for each crash. If the crash addresses or values
are diﬀerent during the minimisation process, this crash can
be controlled from a given fuzzed input and is considered to
be a crash where an exploit can be made.
3.
IMPLEMENTATION AND EVALUATION
In this section we describe the experiment setup and dis-
cuss some of the important results obtained from MVDP.
3.1 Experimental Setup
All of our experiments were run on STAMP which is im-
plemented as a server running Ubuntu 12.04 on an Intel
Xeon E5-2697 v2 @ 2.70GHz 64 GB RAM. SFD, SFAT,
Fe2 and FEET are implemented as Python scripts with the
Python NumPy library. We modiﬁed hachoir-urwid [17] to
provide SFAT with the structure of a recogonised ﬁle format
in a hierarchical form. STAMP has been developed using
Python’s Django Framework with an interactive web-based
37Storage Space
Fuzzing Time (if no crash)
Creation Time
Network transmission
9.8mb
1018s
Without BSDiﬀ With BSDiﬀ
60kb
1105s
1864s
0.46s
63s
8.38s
Table 1: Performance hit when using BSDiﬀ to re-
duce storage requirement
dashboard, connecting to a MySQL database backend. The
console allows for fuzz job monitoring, addition of jobs with
user-preferred conﬁguration, assignment of jobs to the de-
vices, test device management, device progress monitoring,
user administration and statistical visualization of crashes.
HiCharts library was used to give a graphical representation
of the number of crashes discovered and rate of discovery.
This aids the security researcher in modifying the fuzzing
conﬁgurations to better target vulnerable formats and re-
gions.
The SOFT clients are developed using the Android and
iOS SDKs. They are running on real devices or emulators. 5
x Intel Xeon E5-2650 v2 workstations with @2.60GHz 32GB
RAM run the Android Emulator clients where each work-
station runs 10 instances of Android emulators. We set up
a fuzz farm a dozen iOS and Android devices in addition
to running the Android emulators. The clients also verify
any crashes discovered by the system and restart themselves
upon a crash. On Android, for example, there is a service
that constantly monitors the SOFT application to detect
crashes. When the application is found to have crashed, the
service takes the necessary steps to clean up and relaunch
the application. The application then proceeds to retest the
same ﬁle which caused the crash before moving onto the
next test ﬁle. Information about whether the crash was re-
producible is sent to the server. This has largely helped in
reducing the waiting time between a crash and human inter-
vention to restart the SOFT application. It has thus made
it possible for SOFT to be running continuously for months
with little/no supervision.
We conducted the fuzzing experiments on multiple An-
droid Emulators running Android 2.3.7, 4.0.4, 4.1.2 and
4.2.1, popular devices from Samsung, LG, Motorola, Huawei
and Google and all iOS devices. A total of approximately
5 million test cases were evaluated in the system for for-
mats PNG, GIF, JPEG, TIF, MP3 and MP4. Note that not
all the devices ﬁnished processing all the jobs at the point
of this paper as devices run at diﬀerent speeds due to their
hardware and software capabilities.
3.1.1 Storage Space Management
To mitigate the exponential disk space explosion from gen-
erating a few million fuzzed inputs, we developed a new way
of producing and delivering the fuzzed input. Rather than
directly generating the fuzzed input, BSDiﬀ [23] patches are
generated for each fuzzed input “diﬀed” against the seed ﬁle.
These diﬀ patches are stored on the server disk. Prior to
fuzzing, the mobile device clients download a copy of the
seed ﬁle and the BSDiﬀ patches. The client then recon-
structs the fuzzed ﬁles by applying the patch to the seed
ﬁle. Finally the clients begins fuzzing by opening the ﬁles
With FEET Without FEET
95.24
95.66
99.13
98.89
97.83
96.09
95.40
96.97
96.13
95.29
86.49
81.18
83.23
87.93
79.66
79.49
84.09
85.72
84.61
98.68
Table 2: Percentage uniqueness of ﬁrst 10 jobs dis-
tributed to clients
reconstructed from the patches.
3.1.2 Crash Classiﬁcation
To identify the module and function that caused the crash,
the stack trace - obtained by the method described above
- is parsed to get the ﬁlename of the shared library and
its oﬀset to the function.
IDA pro is then invoked via a
IDAPython script to read the shared library and identify the
function and instruction causesing the crash. For Android
devices, ADB’s logcat output is parsed to look for crashes
and CACE examines basic exploitability information. For
iOS devices, CrashWrangler [24] is modiﬁed with access to
ARM shared libraries and an ARM version of GDB/LLDB
to triage iOS crashes.
3.1.3 Crash Similarity
For all reproducible identiﬁed crashes, a core-dump is gen-
erated and symbolicated. With the presence of ASLR, stack
trace similarity is idenﬁtied based on function-names call se-
quence in the stack backtrace along with the ARM CPSR
value before crash.
3.2 Experimental Parameters
We set up a fuzz farm consisting of a dozen iOS and An-
droid devices in addition to running more than 100 Android
emulators in Ubuntu desktops. The ﬁle formats tested are