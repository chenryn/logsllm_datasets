point, the symbolic execution engine queries the SMT solver
to check the satisﬁability of these constraints to get concrete
values for the symbolic variables. First, KLEE does not reason
about ﬂoating point symbolic values and concretizes them to 0,
thus leading to unsound SMT queries. Second, state-of-the-art
SMT solvers which support ﬂoating point theories [27], [53]
are well-known to be extremely slow [57]. One common option
for solvers to handle difﬁcult SMT theories, like ﬂoating points
or strings, is to resort to bit-vector encoding and bit-blasting.
In our example, this does not help. For instance, a bit-vector
encoding of the ﬂoating point operations did not terminate in
12 hours with the Z3 SMT solver, which is re-conﬁrmation of
a known inscalability challenge [89].
Missing / Unreachable Code. Another fundamental limitation
of deductive approaches to symbolic execution is that it re-
quires access to all the source code under analysis. This poses a
challenge in capturing complex dependency between variables,
especially when the functions are implemented as external
3
libraries, remote calls, or a library call written in a different
language. In Figure 1, there is an unknown external function
call (fabs) whose output controls our variable of interest (Line
12). KLEE ships with a helper library uClibc [26] which
provides stubs for reasoning about the most commonly used
libc functions. The default uClibc does not deﬁne stubs for
fabs. To deal with such missing code, the present practice is
that developers have to manually analyze the program context
and write stubs. We ﬁnd many such instances of missing
external calls. When analyzing libTIFF (Figure 2) with KLEE,
it encounters the mmap library call which is not modeled by
KLEE. Thus, it fails to analyze the interesting paths which
have multiple CVEs [17], [23]–[25]. Figure 3 shows a code
snippet from BIND [2] application, in which KLEE fails to
ﬁnd a known CVE because it cannot analyze dn_skipname.
These examples motivate our technique which can approximate
missing code with automatically learnt stubs, where possible.
III. OUR APPROACH
To address the above challenges, we propose a new ap-
proach with two main insights: (1) leveraging the high rep-
resentation capability of neural networks to learn constraints
when symbolic execution is infeasible to capture it; and (2)
encoding the symbolic constraints into neural constraint and
leveraging the optimization algorithms to solve the neuro-
symbolic constraints as a search problem. NEUEX departs
from the purist view that all variable dependencies and re-
lations should be expressible precisely in a symbolic form.
Instead, NEUEX treats the entire code from Line 4-23 in
Figure 1 as a black-box, and inductively learn a neural network
—an approximate representation of the logic. In our example,
we want the neural net to learn the relationship that the variable
max is computed as the maximum quantity of all two-byte
sequences in the input after being read in as positive double
precision. Speciﬁcally, for each i, this desired relationship to
be captured is:1
max == ((2 · s − 1) · a) − 65536 · (s − 1)
∧ a == infile [i] + 256 · infile [i + 1]
∧ s == sign (infile [i + 1] ≤ 127)
This constraint when represented purely by a neural network
approximation of the code is termed as a neural constraint. Our
approach creates neuro-symbolic constraints, which includes
both symbolic and neural constraints. The neural network
is trained on concrete program values rather than on the
code. Revisiting the example (Figure 1), the neuro-symbolic
constraints capturing the vulnerability on Line 23 are:
strlen(infilename) ≤ 1
infilename [0] (cid:54)= 45
N : infile (cid:55)→ max
max == 0.0
∨
∧
∧
(1)
(2)
(3)
(4)
(1)-(2) are symbolic constraints for reachability condition,
while (3) is a symbolic constraint for the vulnerability con-
dition divide-by-zero. (4) is a neural constraint capturing the
relationship between the input and the variable of interest max
in the divide-by-zero operation.
Fig. 4.
NEUEX Component Architecture. tool takes in the program and
optionally the symbolic inputs and the input grammar. The user can conﬁgure
the types of vulnerabilities that tool should identify. tool runs in classic DSE
mode and neural mode to produce concrete exploits.
Solving neuro-symbolic constraints is a key challenge. One
naive way is to solve the neural and symbolic constraints
separately. For example, consider the neuro-symbolic con-
straints in Equation (1)-(4). We can ﬁrst solve the symbolic
constraints by SAT/SMT solvers to obtain concrete values
for variables max, infilename, and infile. Note that the
SAT/SMT solver will assign a random value to infile since it
is a free variable. When we plug the concrete value of infile
from SAT/SMT solver in the neural constraint, it may produce
values such as 32.00, 45.00, and 66.00 for the variable max.
Although all these values of max satisfy the neural constraint,
they may not satisfy the symbolic constraint max == 0.
This discrepancy arises because we solve the symbolic and
neural constraints individually without considering the inter-
dependency of variables between them. We refer to such
constraints with inter-dependent variables as mixed constraints.
Alternatively, to solve these mixed constraints, one could resort
to enumeration over values of the inter-dependent variables.
However, this will require a lot of time to discovering the
exploit. This inspires our design of neuro-symbolic constraint
solving. NEUEX solves purely symbolic, purely neural, and
mixed constraints in that order. Speciﬁcally, to solve the mixed
constraints, NEUEX converts symbolic constraints to a loss
function (or objective function) and then ﬁnds a satisﬁable
solution for neural constraints. This enables conjunction of
symbolic and neural constraints.
Remark on Novelty. To the best of our knowledge, our
approach is the ﬁrst
to train a neural net as a constraint
and solve both symbolic constraint and neural constraint
together. Inductive synthesis of symbolic constraints usable
in symbolic analyses has been attempted in prior work [54],
[81]. However, none of them use neural networks. Speciﬁcally,
one notable difference is that our neural constraints are a
form of unstructured learning, i.e., they approximate a large
class of functions and do not aim to print out constraints
in a symbolic form amenable to SMT reasoning. The main
technical novelty of our approach is that the representation it
learns is fundamentally different (approximation) from that of
the real implementation. The second technical novelty in our
design is that NEUEX reasons about neural as well as symbolic
constraints simultaneously.
IV. DESIGN
1infile [i] + 256 · infile [i + 1] captures the relationship deﬁned by
function sf_read_double and max == ((2 · s − 1) · a) − 65536 · (s − 1)
captures the behavior of the fabs function.
We ﬁrst explain the NEUEX setup and the building blocks
we use in our approach. Then, we present the core constraint
solver of NEUEX along with various optimization strategies.
4
Program  Source 	Dynamic	Symbolic	Execution	Engine			SMT	Solver	Neural	Mode	Crash  Inputs Fork Profile Stats NeuEx Engine Input   Grammar (Optional) Symbolic Inputs Vulner- ability Type Fig. 5. NEUEX’s Neural Mode. It takes in the intermediate symbolic constraints from the DSE mode. For sample set generation, it creates inputs and executes
the program P in a separate forked process to produces tuples (In, On). It creates the train and test set from these tuples to learn a neural constraint for each
CVP. The NEUEX solver then takes both neural as well as symbolic constraints and generates a concrete input if possible, else returns UNSAT.
A. Overview
Setup. NEUEX is built as an extension to KLEE—a widely
used and maintained dynamic symbolic execution (DSE) en-
gine. Figure 4 shows the architecture of NEUEX. It takes in
the source code of the program that the user wants to analyze.
The analyst can optionally mark inputs of interest as symbolic
by standard KLEE interfaces. The analyst can further provide
input grammar, which they know beforehand [35]. We are
interested in identifying the following kinds of vulnerability
conditions: out-of-bound buffer accesses, division by zero, and
data-type overﬂows. To this end, KLEE symbolically executes
the program. At the end of the analysis, KLEE returns concrete
inputs to trigger all the detected vulnerabilities.
Preprocessing. NEUEX performs additional pre-processing of
the program before starting the vulnerability detection. Specif-
ically, it performs static analysis of the program source code
to generate the call graph. NEUEX then statically estimates the
program locations where (a) division operations may not check
if the divisor is zero, and (b) buffer accesses may be without
buffer boundary checks. Our analysis marks all such locations
as candidate vulnerability points (or CVPs). Lines 12, 23, and
29 will be marked as CVPs in Figure 1. For each CVP, NEUEX
then statically instruments the programs to record the values of
variables used in the statements at CVPs. NEUEX records the
divisor for division operations; for buffer accesses it records the
index used for dereference (e.g., k, max, and argc in Figure 1).
Classic DSE Mode. By default, NEUEX launches KLEE’s
standard DSE procedure which automatically constructs inputs
required to drive the program execution to various CVPs. The
DSE procedure begins at the entry point of the program. At
each conditional branch, it logs the symbolic path constraints
to reach this code point. It then invokes an SMT solver to
obtain the concrete values for each symbolic variable in the
constraint formula. The SMT solver returns concrete values
for inputs if the given constraints are satisﬁable; otherwise,
it returns UNSAT which implies that the path is infeasible.
The DSE procedure continues the path exploration to other
branches. Once it reaches a CVP, it reports the concrete input
values for an exploit which will trigger the bug. These can be
concretely veriﬁed by running the program.
Neural Mode. We enhance KLEE with a proﬁler which
monitors the classic DSE mode at runtime. It tracks 4 kinds
of events, signaling that the DSE mode is stuck: (a) the path
exploration is stuck in the same loop because of unrolling;
(b) DSE runs out of memory due to path explosion; (c) the
SMT solver is not able to ﬁnd a SAT/UNSAT solution; and (d)
DSE encounters an external / unknown function call. Whenever
one of these events trigger, we terminate the DSE search on
this path. Starting from the latest symbolic state, NEUEX forks
separate parallel processes running each in a neural mode. This
mode has a copy of all the symbolic constraints and concrete
values up to this point in the execution.
Figure 5 shows the detailed steps of our neural mode. When
the neural mode is triggered, NEUEX queries the call graph
to identify all the CVPs which are statically reachable from
the latest symbolic state. It chooses the nearest k CVPs of
each type of bug, where k is a conﬁgurable parameter set
to 150 by default. NEUEX treats the symbolic branch and
each CVP location as a start and end point respectively, for
training a neural network. NEUEX treats the fragment of code
between the start and end points as a black-box which the
neural network approximates.
To train the neural network, NEUEX needs to generate
training samples. The samples consist of program values of
variables at program entry and end point of the code fragment
being approximated. NEUEX uses the symbolic constraints at
the start point to generate concrete program inputs that lead
up to that point using an SMT solver. This concrete input
serves as a seed for generating many random mutations which
are concretely executed to collect program values for training.
Some of these mutated inputs reach the start and the end point.
If sufﬁciently many samples (typically 100, 000) are collected,
NEUEX uses it to train a neural network. In our example, the
neural network collects samples for the values of the input ﬁle
bytes at the start point of the program and the max variable
at the end point (Line 23). At the end of such training, we
have the symbolic constraints generated by DSE as well as a
neural constraint, i.e., the neural net itself. Then NEUEX calls
its solver to solve both these constraints simultaneously.
Constraint Solver. The NEUEX solver checks satisﬁability of
the given neuro-symbolic constraints and generates concrete
values for constraint inputs. The syntax of our neuro-symbolic
constraints is shown as an intermediate language in Table I.
It is expressive enough to model various constraints speci-
ﬁed in many real applications such as string and arithmetic
constraints. Given the learned neuro-symbolic constraints, we
seek the values of variables of interest that satisfy all the
constraints within it. A key technical challenge is solving the
mixed constraints simultaneously.
5
NeuEx Neural Mode Symbolic Constraints Partially  Concretized Values Candidate Vulnerability Points Randomly mutate symbolic Values  Sample Set Generation Neural Constraint  Synthesis Sample Set (Gradient  Descent  Optimization) Neural net Training Neuro-symbolic  Constraint Solving SymSolv	NeuSolv	Symbolic Constraints Initial Input DPLL Loop Constraint  Solution fork fork fork fork fork P(I0, O0) P(I1, O1) P(I2, O2) P(I3, O3) P(In, On) … TABLE I.
THE GRAMMAR OF NEURO-SYMBOLIC CONSTRAINT
LANGUAGE SUPPORTED BY NEUEX.
Nuero-Symbolic
Constraint
Neural
constraint
symbolic
constraint
Variable
Expression
Logical
Conditional
Arithmetic
NS
N
S
StrVar
NumVar
e
(cid:9)
⊗
(cid:11)
:= N ∧ S
:= VIn (cid:55)→ VOn
:= e1 (cid:9) e2 | e
:= ConstStr | StrVar◦StrVar
:= ConstNum | NumVar(cid:11)NumVar
contains(StrVar, StrVar)
strstr(StrVar, StrVar) ⊗ NumVar
strlen(StrVar) ⊗ NumVar
NumVar ⊗ NumVar
:=
:= ∨ | ∧
:= == | (cid:54)= | > | ≥ | < | ≤
:= + | - | * | /
B. Constraint Learning
The procedure for learning neural networks is standard.
Given a program, the selection of network architecture is the
key for learning any neural constraint. In this paper, we use
multilayer perceptron (MLP) architecture which consists of
multiple layers of nodes and connects each node with all nodes
in the previous layer [88]. Each node in the same layer does not
share any connections with others. We select this architecture
because it is a suitable choice for ﬁxed-length inputs. There
are other more efﬁcient architectures (e.g., CNN [71] and
RNN [77]) for the data with special relationships, and NEUEX
is easily extensible to more network architectures.
The selection of activation function plays signiﬁcant role
inference as well. In this paper, we
for neural constraint
consider multiple activation functions (e.g., Sigmoid and
Tanh) and ﬁnally select the rectiﬁer function Relu as the
activation function, because Relu obtains parse representation
and reduces the likelihood of vanishing gradient [59]. In other
words, the neural network with Relu has higher chance to
converge than other activation functions.
In addition, to ensure the generality of neural constraint, we
implement an early-stopping mechanism which is a regulariza-
tion approach to reduce over-ﬁtting [98]. It stops the learning
procedure when the current learned neural constraint behaves
worse on unseen test executions than the previous constraint.
As the unseen test executions are never used for learning the
neural constraint, the performance of learned neural constraint
on unseen test executions is a fair measure for the generality
of learned neural constraints.
NEUEX can use any machine learning approach, optimiza-
tion algorithm (e.g., momentum gradient descent [85]) and
regularization solution (e.g., dropout [96]) to learn the neural
constraints. With future advances in machine learning, NEUEX
can be adopted to new architectures and learning approaches.
C. Building Blocks for Solver
NEUEX’s solves the neuro-symbolic constraints (e.g.,
Equations (1)-(4)) using its custom constraint solver detailed
in Section IV-D. It uses two existing techniques as building
blocks for such pure constraints: SMT solver and gradient-
based neural solver. These solvers referred to as SymSolv and
NeuSolv respectively form the basic building blocks.
SymSolv. NEUEX’s symbolic constraint solver takes in ﬁrst-
order quantiﬁer-free formulas over multiple theories (e.g.,
6
empty theory,
the theory of linear arithmetic and strings)
and returns UNSAT or concrete values as output. It internally
employs Z3 Theorem Prover [53] as an SMT solver to solve
both arithmetic and string symbolic constraints.
NeuSolv. For solving purely neural constraints, NeuSolv takes
in the neural net and the associated loss function to generate
the expected values that the output variables should have.
NEUEX considers the neural constraint solving as a search
problem and uses a gradient-based search algorithm to search
for the satisﬁable results [87]. Gradient-based search algorithm
searches for the minimum of a given loss function L(X)
where X is a n-dimensional vector. The loss function can
be any differentiable function that monitors the error between
the objective and current predictions. Consider the example in
Figure 1. By minimizing the error, NEUEX can discover the
input closest to the exploit. To minimize the error, gradient-
based search algorithm ﬁrst starts with a random input X0
which is the initial state of NeuSolv. For every enumeration i,
it computes the derivative ∇XiL(Xi) given the input Xi and
then updates Xi according to ∇XiL(Xi). This is based on the
observation that the derivative of a function always points to
a local nearest valley. The updated input Xi+1 is deﬁned as:
Xi+1 = Xi − ∇XiL(Xi)
(5)
where  is the learning rate that controls how much is going
to be updated. Gradient-based search algorithm keeps updating
the input until it reaches the local minima. To avoid the non-
termination case, we set the maximum number of enumerations
to be a constant Me. If it exceeds Me, NeuSolv stops and
returns the current updated result. Note that
the gradient-
based search algorithm can only ﬁnd the local minima since
it stops when the error increases. If the loss function is a
non-convex function with multiple local minima, the found
local minima may not be the global minima. Moreover, it
may ﬁnd different local minima with different initial states.
Thus, NEUEX executes the search algorithm multiple times
with different randomized initial states in order to ﬁnd the
global minima of L(X).
D. Constraint Solver
We propose a constraint solver for neuro-symbolic con-
straints with the help of SymSolv and NeuSolv. If the con-
straint solving procedure returns SAT, then the neuro-symbolic
constraints are guaranteed to be satisﬁable. It is not guaran-
teed, however, that the procedure terminates on all possible
constraints; so, we bound its running time with a conﬁgurable
timeout. Algorithm 1 shows the steps of our constraint solver.
DAG Generation. NEUEX takes the neuro-symbolic con-
straints and generates the directed acyclic graph (DAG) be-
tween constraints and its variables. Each vertex of the DAG
represents a variable or constraint, and the edge shows that the