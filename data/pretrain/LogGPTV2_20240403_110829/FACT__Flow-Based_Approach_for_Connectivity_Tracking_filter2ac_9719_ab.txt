HostHostSuccess.
Fig. 3 displays the number of visible and unresponsive external destinations
if the three aggregation granularities are applied to OneWeek, see Section 3. Ac-
cording to Fig. 3(a) the absolute number of visible external destinations shows a
strong daily and weekly pattern irrespective of the used aggregation level. Aggre-
gating from host-host into ExtHostFailed and ExtHostSuccess, respectively,
reduces the peaks from 525K to 90K tuples (/24s: 50K, preﬁxes: 25K). This pro-
vides evidence for the high visibility that our data has on external networks. How-
ever, Fig. 3(b) reveals that generally only a small fraction of external hosts (peaks
of 700) are unresponsive and therefore classiﬁed as ExtHostFailed according to
4 Experiments relying on DNS traﬃc turned out to work as well.
220
D. Schatzmann et al.
our methodology. This fraction is signiﬁcantly smaller for ExtNetFailed (peaks
of 600) and ExtPrefixFailed (peaks of 180), respectively.
However, to cope with daily and weekly ﬂuctuations and to limit the degree to
which a single internal host (e.g., a scanning host) can impact our connectivity
analysis, we need to take into account the severity of an observed event as well.
By this we understand the number of internal users that actually fail to establish
connectivity with a speciﬁc external host, /24 network, or BGP preﬁx during
our 5 minute time intervals. Figure 4(a) displays the number of external /24
networks that are unresponsive to 1, 2, 5, and 10 internal hosts for the time
spanned by OneWeek. The majority of these ExtHostFailed “events”, namely
98%, only aﬀect 1 internal host.
Yet, here it is important to study Fig. 4(b). It is also based on OneWeek and
counts for every external host the number of 5-minute time intervals for which it
has been classiﬁed as ExtHostFailed. This number (x-axis) is plotted against the
maximum number of internal hosts (y-axis) that failed to establish connectivity
with this external host (ExtHostFailed) at any 5-minute interval of OneWeek. We
ﬁnd that the majority of external hosts (96%) are only unresponsive in less than
10 time intervals of our trace. However, some hosts are unresponsive most of the
time, e.g., abandoned ad servers. Data preprocessing as described in Section 4.1
could be reﬁned to automatically blacklist such hosts and possibly their networks.
Finally, we observe few external hosts that are unresponsive only during a small
number of time intervals, but with a high maximum number of aﬀected internal
hosts. Cross-checking with technical forums in the Internet, we ﬁnd that these
events include for example a Facebook outage on August 31, 2010.
We point out that the data processing in FACT is faster than real time for
SWITCH, a medium-sized ISP covering an estimated 6% of the Internet traﬃc
in Switzerland and approximately 2.2 million IP addresses: ﬂow data spanning
5 minutes5 can be processed using a single thread in less than three minutes
with a maximum memory consumption of less than 4GB. Aging mechanisms
for our data structures3 ensure that the overall memory consumption does not
103
102
101
d
e
l
i
a
F
t
e
N
t
x
E
100
30/08
00:00
1
2
5
10
s
t
s
o
h
l
a
n
r
e
t
n
i
x
a
m
103
102
101
100
02/09
00:00
05/09
00:00
 0
 1000
 500
 1500
number of 5 min intervals
 2000
(a) Number of ExtNetFailed that are un-
responsive to 1, 2, 5, and 10 internal hosts.
(b) Frequency analysis of ExtHostFailed
events
Fig. 4. Severity of observed events
5 We see up to 200 million ﬂows per hour.
FACT: Flow-Based Approach for Connectivity Tracking
221
increase during long-term use of our system. Due to hash-like data structures
we can access individual ﬂows in our 5-tuple cache in constant time. The total
time required for data processing mainly depends on the number of active ﬂows.
In principle, it is even possible to parallelize our processing by distributing the
reachability analysis for diﬀerent external networks to diﬀerent CPU cores or
physical machines. Yet, we leave it to future work to study FACT’s performance
for large tier-1 ISPs and how to make it robust against potentially higher false
positive rates if sampled ﬂow data is used.
5 Case Studies
In this section we present a short analysis of three connectivity problems that
were either detected by the network operator or publicly documented. To analyze
those cases, we rely on data collected as discussed in Section 3.
Black-holing: On May 18, 2010, all services in an external /24 network were
not accessible from SWITCH between 08:30 and 08:45. According to the oper-
ators of SWITCH, this problem was most likely due to a tier-1 provider that
black-holed parts of the reverse traﬃc towards SWITCH. Yet, at this time the
operators could only speculate how many hosts and customers, or even other
/24 networks were aﬀected by this problem. Applying FACT we conﬁrm that
the reported /24 network is indeed reported as unreachable at around 08:30.
Surprisingly, FACT reveals that the overall number of unreachable hosts and
/24 networks has doubled compared to the time before 08:30 while the number
of unresponsive BGP preﬁxes is increased by a factor of even 6, see Fig. 5(a).
Moreover, the reported /24 network is not even in the top ten list of the most
popular unresponsive networks. This suggests that the impact of this event has
been more serious than previously believed.
RIPE/Duke event: On August 27, 2010, some parts of the Internet became
disconnected for some 30 minutes due to an experiment with new BGP attributes
by RIPE and Duke University [12]. FACT reveals that at around 08:45 the
number of popular unresponsive /24 networks indeed doubled. According to
Fig. 5(b), for some BGP preﬁxes more than 15 internal hosts failed to establish
d
e
l
i
a
F
x
i
f
e
r
P
t
x
E
103
102
101
100
1
2
5
10
07:00
09:00
11:00
2010-05-18
d
e
l
i
a
F
x
i
f
e
r
P
t
x
E
103
102
101
100
1
2
5
10
103
102
101
d
e
l
i
a
F
x
i
f
e
r
P
t
x
E
1
2
5
10
07:00
09:00
11:00
2010-08-27
100
03:00 05:00 07:00 09:00 11:00
2010-03-25
(a) Black-holing
(b) RIPE/Duke event
(c) Partitioned IXP
Fig. 5. Case studies: unresponsive BGP preﬁxes
222
D. Schatzmann et al.
connectivity. Yet, overall our analysis reveals that the impact of this incident on
SWITCH and its customers was quite limited compared to the public attention
that this event obtained.
Partitioned IXP: After scheduled maintenance by AMS-IX, SWITCH’s con-
nection to that exchange point came back with only partial connectivity. Some
next-hops learned via the route servers weren’t reachable, creating black holes.
The next morning, several customers complained about external services being
unreachable. Overall, it took more than four hours until the problem was ﬁnally
solved by resetting a port. Fig. 5(c) shows that the number of unresponsive BGP
preﬁxes is almost ten times higher than normal, over a time period of more than
four hours. We believe that FACT would have helped to detect such a serious
problem much faster and provided valuable hints about the origin of the problem.
6 Related Work
Approaches for detecting and troubleshooting reachability problems can be gen-
erally classiﬁed into two classes: active probing and control plane based.
With respect to active probing, Paxson et al. [9] are probably the pioneers
to use traceroute for studying end-to-end connectivity between a (limited) set
of Internet sites. Zhang et al. [2] perform collaborative probing launched from
Planetlab hosts to diagnose routing event failures. Commercial solutions such
as NetQoS [7] or Peakﬂow [6] generally rely on active measurements using ping,
traceroutes, or continuous SNMP queries to network devices. Moreover, they
frequently aggregate traﬃc volumes per interface, peering links, etc. to detect
abnormal events, and hence do not base their analysis on a ﬂow-level granular-
ity as our work suggests. In contrast to active probing, the passive monitoring
approach of FACT does not impose any traﬃc overhead and, importantly, only
creates alerts for those unreachable hosts/networks that users actually want to
access. Finally, FACT avoids an intrinsic problem of active probing techniques
such as ping or traceroute, namely the implicit assumption that reachable hosts
actually do respond to such tools.
In addition to active probing, a considerable number of research papers,
e.g., [8,13] rely almost exclusively on control-plane information in the form of
BGP routing feeds. However, Bush et al. [10] have clearly pointed out the dan-
gers of such an approach, e.g., the wide-spread existence of default routes. In
contrast, FACT is able to detect unreachability at multiple and ﬁner granular-
ities (e.g., on a host basis) than any approach that is purely based on routing
data.
Later work including e.g., Hubble [3] and iPlane [4] rely on hybrid approaches
combining active measurements with BGP routing information. Feamster et
al. [14] adopt such an approach to measure the eﬀects of Internet path faults on
reactive routing. Overall, we believe that the passive approach adopted by FACT
is very powerful compared to active probing and control-plane based techniques.
Yet, we plan to integrate active probing into our system to crosscheck detected
reachability problems and to pinpoint the underlying causes.
FACT: Flow-Based Approach for Connectivity Tracking
223
7 Conclusion
We have proposed FACT, an online data processing system that helps operators
to acquire facts about connectivity problems with remote autonomous systems,
subnets, and hosts. In contrast to existing solutions, our approach relies solely on
ﬂow-level information extracted from traﬃc crossing the border of the network.
We showed, with the help of reported real-world events, that FACT can be used
to alert only about those events that actually aﬀect the studied network or its
users. Importantly, data processing of FACT is already faster than real time for
a medium-sized ISP.
In the future we plan to reﬁne and integrate our techniques into existing trac-
ing tools (e.g., nfdump), to generate alerts based on automatically determined
thresholds, and to provide summary reports that allow network operators to
quickly troubleshoot connectivity problems. Ultimately, we plan to make our
implementation of FACT available for public use.
References
1. Nanog mailing list, website http://www.nanog.org/mailinglist/
2. Zhang, Y., Mao, M., Zhang, M.: Eﬀective diagnosis of routing disruptions from
end systems. In: Proc. NSDI (2008)
3. Katz-Bassett, E., Madhyastha, H., John, J., Krishnamurthy, A., Wetherall, D.,
Anderson, T.: Studying black holes in the Internet with Hubble. In: Proc. NSDI
(2008)
4. Madhyastha, H., Isdal, T., Piatek, M., Dixon, C., Anderson, T., Krishnamurthy,
A., Venkataramani, A.: iPlane: an information plane for distributed services. In:
Proc. OSDI (2006)
5. Zhang, M., Zhang, C., Pai, V., Peterson, L., Wang, R.: PlanetSeer: Internet path
failure monitoring and characterization in wide-area services. In: Proc. OSDI (2004)
6. Arbor Networks - Peakﬂow, website http://www.arbornetworks.com
7. CA technologies - NetQoS performance center, website
http://www.netperformance.com/
8. Wu, J., Mao, M., Rexford, J., Wang, J.: Finding a needle in a haystack: Pinpointing
signiﬁcant BGP routing changes in an IP network
9. Paxson, V.: End-to-end routing behavior in the Internet. IEEE/ACM Trans. Net-
working 5(5) (1997)
10. Bush, R., Maennel, O., Roughan, M., Uhlig, S.: Internet optometry: assessing the
broken glasses in Internet reachability. In: Proc. of ACM IMC (2009)
11. The Swiss Education and Research Network (SWITCH), http://www.switch.ch
12. RIPE/Duke event,
http://labs.ripe.net/Members/erik/
ripe-ncc-and-duke-university-bgp-experiment/
13. Feldmann, A., Maennel, O., Mao, M., Berger, A., Maggs, B.: Locating Internet
routing instabilities. In: Proc. of ACM SIGCOMM (2004)
14. Feamster, N., Anderson, D., Balakrishnan, H., Kaashoek, F.: Measuring the eﬀects
of Internet path faults on reactive routing. In: Proc. of ACM SIGMETRICS (2003)