### HostHostSuccess
Figure 3 illustrates the number of visible and unresponsive external destinations when three aggregation granularities are applied to a one-week dataset, as described in Section 3. As shown in Figure 3(a), the absolute number of visible external destinations exhibits a strong daily and weekly pattern, regardless of the aggregation level used. Aggregating from host-host into ExtHostFailed and ExtHostSuccess reduces the peaks from 525,000 to 90,000 tuples (/24s: 50,000, prefixes: 25,000). This indicates the high visibility of our data on external networks. However, Figure 3(b) reveals that only a small fraction of external hosts (peaks of 700) are unresponsive and thus classified as ExtHostFailed. This fraction is significantly smaller for ExtNetFailed (peaks of 600) and ExtPrefixFailed (peaks of 180).

To address daily and weekly fluctuations and to limit the impact of a single internal host (e.g., a scanning host) on our connectivity analysis, we must also consider the severity of observed events. By this, we mean the number of internal users that fail to establish connectivity with a specific external host, /24 network, or BGP prefix during our 5-minute time intervals. Figure 4(a) shows the number of external /24 networks that are unresponsive to 1, 2, 5, and 10 internal hosts over the one-week period. The majority of these ExtHostFailed "events" (98%) affect only one internal host.

However, it is crucial to examine Figure 4(b), which counts the number of 5-minute time intervals for each external host classified as ExtHostFailed. This number (x-axis) is plotted against the maximum number of internal hosts (y-axis) that failed to establish connectivity with this external host at any 5-minute interval during the one-week period. We find that the majority of external hosts (96%) are unresponsive in fewer than 10 time intervals. Some hosts, such as abandoned ad servers, are unresponsive most of the time. Data preprocessing, as described in Section 4.1, could be refined to automatically blacklist such hosts and their networks.

We also observe a few external hosts that are unresponsive during a small number of time intervals but with a high maximum number of affected internal hosts. Cross-referencing with technical forums, we find that these events include, for example, a Facebook outage on August 31, 2010.

### Data Processing Performance
The data processing in FACT is faster than real-time for SWITCH, a medium-sized ISP covering approximately 6% of the Internet traffic in Switzerland and about 2.2 million IP addresses. Flow data spanning 5 minutes can be processed using a single thread in less than three minutes, with a maximum memory consumption of less than 4 GB. Aging mechanisms ensure that the overall memory consumption does not increase during long-term use. Due to hash-like data structures, we can access individual flows in our 5-tuple cache in constant time. The total processing time mainly depends on the number of active flows. In principle, it is possible to parallelize the processing by distributing the reachability analysis for different external networks across multiple CPU cores or physical machines. Future work will focus on studying FACT’s performance for large tier-1 ISPs and making it robust against potentially higher false positive rates if sampled flow data is used.

### Case Studies
In this section, we present a brief analysis of three connectivity problems detected by the network operator or publicly documented, using data collected as discussed in Section 3.

**Black-holing:** On May 18, 2010, all services in an external /24 network were inaccessible from SWITCH between 08:30 and 08:45. According to SWITCH operators, this was likely due to a tier-1 provider black-holing parts of the reverse traffic. Initially, the operators could only speculate about the extent of the impact. Using FACT, we confirmed that the reported /24 network was indeed unreachable around 08:30. Surprisingly, FACT revealed that the overall number of unreachable hosts and /24 networks had doubled compared to the time before 08:30, and the number of unresponsive BGP prefixes increased by a factor of six (Figure 5(a)). The reported /24 network was not even in the top ten list of the most popular unresponsive networks, suggesting that the event's impact was more severe than initially believed.

**RIPE/Duke Event:** On August 27, 2010, some parts of the Internet became disconnected for about 30 minutes due to an experiment with new BGP attributes by RIPE and Duke University. FACT showed that the number of popular unresponsive /24 networks doubled around 08:45 (Figure 5(b)). For some BGP prefixes, more than 15 internal hosts failed to establish connectivity. However, our analysis indicated that the impact on SWITCH and its customers was limited compared to the public attention the event received.

**Partitioned IXP:** After scheduled maintenance by AMS-IX, SWITCH’s connection to the exchange point returned with partial connectivity. Some next-hops learned via the route servers were not reachable, creating black holes. The next morning, several customers complained about unreachable external services. It took more than four hours to resolve the problem by resetting a port. Figure 5(c) shows that the number of unresponsive BGP prefixes was almost ten times higher than normal over a period of more than four hours. FACT would have helped detect and provide valuable hints about the origin of the problem much faster.

### Related Work
Approaches for detecting and troubleshooting reachability problems can be generally classified into two categories: active probing and control-plane based.

**Active Probing:** Paxson et al. [9] pioneered the use of traceroute for studying end-to-end connectivity between a limited set of Internet sites. Zhang et al. [2] perform collaborative probing launched from PlanetLab hosts to diagnose routing event failures. Commercial solutions like NetQoS [7] and Peakflow [6] rely on active measurements using ping, traceroutes, or continuous SNMP queries to network devices. These solutions often aggregate traffic volumes per interface, peering links, etc., to detect abnormal events, and do not base their analysis on a flow-level granularity as FACT does. FACT, being a passive monitoring approach, does not impose any traffic overhead and only creates alerts for unreachable hosts/networks that users actually want to access. Additionally, FACT avoids the assumption that reachable hosts respond to tools like ping or traceroute.

**Control-Plane Based Approaches:** Many research papers, such as [8, 13], rely on BGP routing feeds. However, Bush et al. [10] highlighted the dangers of such approaches, including the widespread existence of default routes. FACT can detect unreachability at multiple and finer granularities (e.g., on a host basis) than any approach purely based on routing data.

**Hybrid Approaches:** Later work, including Hubble [3] and iPlane [4], combines active measurements with BGP routing information. Feamster et al. [14] use such an approach to measure the effects of Internet path faults on reactive routing. While FACT is powerful compared to active probing and control-plane-based techniques, we plan to integrate active probing into our system to cross-check detected reachability problems and pinpoint underlying causes.

### Conclusion
We have proposed FACT, an online data processing system that helps operators acquire facts about connectivity problems with remote autonomous systems, subnets, and hosts. Unlike existing solutions, our approach relies solely on flow-level information extracted from traffic crossing the network border. Using real-world events, we demonstrated that FACT can alert only about events that actually affect the studied network or its users. Data processing in FACT is already faster than real-time for a medium-sized ISP.

Future work includes refining and integrating our techniques into existing tracing tools (e.g., nfdump), generating alerts based on automatically determined thresholds, and providing summary reports for quick troubleshooting. Ultimately, we plan to make our implementation of FACT available for public use.

### References
1. Nanog mailing list, website http://www.nanog.org/mailinglist/
2. Zhang, Y., Mao, M., Zhang, M.: Effective diagnosis of routing disruptions from end systems. In: Proc. NSDI (2008)
3. Katz-Bassett, E., Madhyastha, H., John, J., Krishnamurthy, A., Wetherall, D., Anderson, T.: Studying black holes in the Internet with Hubble. In: Proc. NSDI (2008)
4. Madhyastha, H., Isdal, T., Piatek, M., Dixon, C., Anderson, T., Krishnamurthy, A., Venkataramani, A.: iPlane: an information plane for distributed services. In: Proc. OSDI (2006)
5. Zhang, M., Zhang, C., Pai, V., Peterson, L., Wang, R.: PlanetSeer: Internet path failure monitoring and characterization in wide-area services. In: Proc. OSDI (2004)
6. Arbor Networks - Peakflow, website http://www.arbornetworks.com
7. CA technologies - NetQoS performance center, website http://www.netperformance.com/
8. Wu, J., Mao, M., Rexford, J., Wang, J.: Finding a needle in a haystack: Pinpointing significant BGP routing changes in an IP network
9. Paxson, V.: End-to-end routing behavior in the Internet. IEEE/ACM Trans. Networking 5(5) (1997)
10. Bush, R., Maennel, O., Roughan, M., Uhlig, S.: Internet optometry: assessing the broken glasses in Internet reachability. In: Proc. of ACM IMC (2009)
11. The Swiss Education and Research Network (SWITCH), http://www.switch.ch
12. RIPE/Duke event, http://labs.ripe.net/Members/erik/ripe-ncc-and-duke-university-bgp-experiment/
13. Feldmann, A., Maennel, O., Mao, M., Berger, A., Maggs, B.: Locating Internet routing instabilities. In: Proc. of ACM SIGCOMM (2004)
14. Feamster, N., Anderson, D., Balakrishnan, H., Kaashoek, F.: Measuring the effects of Internet path faults on reactive routing. In: Proc. of ACM SIGMETRICS (2003)