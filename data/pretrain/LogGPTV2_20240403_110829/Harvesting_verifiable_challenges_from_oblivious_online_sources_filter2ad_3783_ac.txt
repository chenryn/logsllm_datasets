Other Sources of Randomness.
Web sites provide many other sources of randomness that
could be utilized within our framework. A viable source
must contain suﬃcient fresh entropy that is at least par-
tially beyond adversarial control and can be accessed by
veriﬁers after the challenge is created. One potentially use-
ful class of sources is sensor data, such as archived feeds
from earthquake [16] and sunspot [15] monitoring systems.
These may provide better stability than RSS feeds, since
data from them will rarely change after it has been published.
Another promising category includes change logs from fre-
quently edited Wikis and source code repositories. Although
we have not yet implemented these sources in our Combine
tool, future versions easily could accommodate them within
the existing policy framework.
4.3 Policy Descriptions
We designed our policy language to cope with a variety
of unreliable data sources. Some sources may be unreach-
able when a challenge is being derived, others may change
or become unavailable by the time the challenge is veriﬁed.
Like sources, policies may deﬁne minimum and maximum
numbers of sources from diﬀerent sets. This instructs de-
rivers to include additional sources, up to the maximum, for
increased robustness at veriﬁcation time.
Policy ﬁles contain one or more policy deﬁnitions. A sim-
ple policy takes the form:
policy policy_name { source_1, source_2, ...}
For example:
policy PickOne { NYTimes, CNN, Slashdot }
By default, the tool assumes that at least one of the
sources must be satisﬁed, but it will attempt to include con-
tent chunks satisfying all of them in the derivation. A policy
can override this by specifying a minimum number of sources
to verify and maximum number to include at derivation, like
so:
policy policy_name { source_1, ...}[min,max ]
For example:
policy PickTwo { NYTimes, CNN, Slashdot }[2,2]
During derivation, the tool will include content chunks
from up to two of these three source, and veriﬁers will re-
quire any two to be satisﬁed. The tool evaluates the sources
in order from left to right, so content chunks from Slash-
dot will not be used unless the deriver cannot retrieve the
minimum number of content chunks required from either
NYTimes or CNN.
Sometimes when a source is referenced within a policy,
we may want to use diﬀerent criteria for determining when
it is satisﬁed than those we speciﬁed when the source was
deﬁned. To do this, we can specify local source attributes
that override the source’s global attributes. For example:
policy { NYTimes(min_entries=10) }
Policies may also be nested. In this example, an explicit
random server is used preferentially during derivation, but
a set of RSS feeds will be used instead if the server is un-
reachable:
policy FailOver { AppsRandomServer, { NYTimes, CNN,
Slashdot }[2,3] }[1,1]
A policy ﬁle can specify multiple policies, and they can
refer to each other by name in place of sources, as in this
example:
)
policy Nested {
{ NYTimes, CNN, Slashdot }[2,3]
Recent
}
policy Recent {
NYTimes(min_entries=1, max_age=3600)
CNN(min_entries=1, max_age=3600)
}[2,2]
For this policy to be satisﬁed, the veriﬁer needs to validate
the default number of sources from two of the three RSS
feeds, or, at least one story from within the past hour from
each of NYTimes and CNN.
5.
IMPLEMENTING COMBINE
In this section we describe Combine, our implementation
of the framework introduced in the previous section. We
have released Combine under a BSD-style license, and we
invite application developers to adopt it. The source is avail-
able at http://www.cs.princeton.edu/~jhalderm/projects/
combine/.
We wrote Combine in Python because of the language’s
multi-platform support, object-oriented structure, and ro-
bust tools for parsing RSS feeds and other Internet data
sources. This simpliﬁed the tool’s construction signiﬁcantly,
but we have been careful not to deﬁne any of framework in
terms of Python-speciﬁc behavior, so as not to preclude in-
teroperable implementations in other languages. Combine
provides a simple API that can be called from other Python
applications as well as a command line interface suitable for
use with a variety of other languages and scripting tools.
Combine supports the three kinds of data sources detailed
in the previous section: RSS feeds, historical market data,
and explicit challenge servers. It interprets RSS feeds using
the Python Universal Feed Parser, which supports the most
popular versions of the RSS and ATOM feed speciﬁcations.
Combine caches feed data on the client in order to mini-
mize redundant requests. This reduces the risk that an at-
tacker could execute a denial-of-service attack on the sources
by causing many challenges to be generated. Combine also
obeys the robots exclusion standard [13], so content providers
can choose not to be available for harvesting if they desire.
5.1 Complete Example
To better illustrate our system’s behavior, we now give an
end-to-end example of Combine in use. Here is an example
policy ﬁle (stored in a ﬁle named example.pol) that deﬁnes
a policy similar to the one shown at the end of the preceding
section:
source NYTimes (
type = RSSFeed
url = "http://www.nytimes.com/services/xml/rss/-
nyt/HomePage.xml"
min_entries = 5
max_entries = 10
max_age = 86400
)
source CNN (
type = RSSFeed
url = "http://rss.cnn.com/rss/cnn_topstories.rss"
min_entries = 5
max_entries = 10
max_age = 86400
policy Example { {NYTimes, CNN}[2,2], Recent }
policy Recent {
NYTimes(min_entries=1, max_age=3600)
CNN(min_entries=1, max_age=3600)
}
This policy ﬁle deﬁnes two sources, both of which are RSS
feeds from online news providers. It also deﬁnes two policies.
One policy is named Recent and requires one veriﬁable con-
tent chunk made from a story from either source but no
more than an hour old when the challenge was derived. The
other policy, called Example, is considered valid either if both
sources are satisﬁed with their default arguments (a mini-
mum of 5 entries from each, no more than 24 hours old), or
if Recent is satisﬁed.
Suppose Alice wants to derive a challenge from this policy
and output a derivation to a ﬁle named alice.d. She invokes
Combine like this:
$ combiner -policyfile example.pol -derivation \
alice.d -derive
By default, Combine will apply the ﬁrst policy deﬁned in
the policy ﬁle, so it will apply Example here. This behavior
can be overridden using the -policyname argument.
Assuming that the tool can ﬁnd enough fresh RSS entries
to satisfy the policy, it outputs a message that indicates
success:
derived: Example, a936b92d6497..., 1169960994
This status line contains the name of the policy that was
applied, the derived challenge, and the challenge’s times-
tamp (which is normally the time when the derivation pro-
cess started, though it can be backdated with the -time
argument).
Alice can now use the challenge as the basis for a client
puzzle. She solves the puzzle, and sends the solution to Bob
along with the derivation returned by the tool.
The derivation looks like this:
1169960994
CNN
9a35a2442faf16b994f10b75573a18269fa4b97f
1169942568
a33524271ed0d5e9b0d2aafa8c0ed2a6c39a1b78
1169956546
(. . . 6 more content chunks)
NYTimes
8829863ca791800d164b546d03503eb74294713f
1169959720
d7190efb9603e0d88ea2cdff0134c5416ecfd656
1169959401
(. . . 9 more content chunks)
Note that CNN has only 8 content chunks—fewer than the
10 chunk limit speciﬁed by the source’s max_entries parame-
ter. This indicates that the RSS feed only contained 8 entries
that were younger than max_age. Also notice that though
each source was referenced twice in the policy, there is only
one section for each source in the derivation. The reason is
that the framework speciﬁes that there is one derivation sec-
tion for each source name, so the tool collapses all content
chunks from each into a single section. This pool of content
chunks will be consulted any time the source is mentioned
by the policy during veriﬁcation.
A few minutes later, Bob wants to verify Alice’s derived
challenge. He runs Combine using the same policy ﬁle and
the derivation that Alice sent:
$ combiner -policyfile example.pol -derivation \
alice.d -verify
Suppose that a network outage is preventing Bob’s com-
puter from reaching the CNN web site to retrieve its RSS
feed. Can the Example policy still be satisﬁed? The {NY-
Times, CNN}[2,2] subpolicy is not satisﬁed, since it requires
some content chunks from both sources to be validated. How-
ever, Recent can be satisﬁed. Notice that the ﬁrst content
chunk from NYTimes was only 1274 seconds old when the
challenge was derived. Recent only requires one content
chunk from either source to be veriﬁable as long as that con-
tent chunk was less than an hour old at the time of deriva-
tion, so the Recent subpolicy will be satisﬁed as long as the
RSS feed entry corresponding to this content chunk is still
present and unmodiﬁed when Combine checks the feed. If
so, Example will be satisﬁed, and the tool will output this
message:
verified:
Example, a936b92d6497..., 1169960994
Now Bob can check the challenge’s timestamp to ensure
that it is fresh enough for his purposes, and he can check
that Alice’s solution is valid for the derived challenge.
5.2 Anti-Spam Demonstration
To demonstrate our tool’s capabilities, we have created a
prototype anti-spam application called Postmark that uses
derived challenges from Combine to improve upon existing
schemes.
One limitation of traditional proof-of-work anti-spam sys-
tems like Hashcash [2] is that they do not guarantee the
freshness of their random challenges. This allows an attacker
to perform a large amount of work ahead of time and then
send burst of messages in a short period.
Our Postmark application uses hash puzzles based on fresh
challenges derived by Combine to limit an attacker’s ability
to precompute puzzle solutions. Combine allows the system
to ensure freshness without having to interact with the re-
ceiver prior to sending a message or to rely on a dedicated
challenge server.
Postmark consists of two modules, an SMTP proxy server,
postserv and a Procmail-compatible message ﬁlter, postcheck.
Both are implemented in Python and incorporate Combine
using its API.
Postmark uses hash puzzles that are formed by concate-
nating and SHA-1 hashing the message body (along with
user-visible headers such as the sender, receiver, subject, and
date), derived challenge, and receiver envelope address, as
follows:
p = H(H(message body)|derived challenge|receiver address)
For a diﬃculty parameter n, a solution to the puzzle is a
string s such that H(s|p) has n leading zeroes. Postmark
solves puzzles with a separate process written in C that uses
a highly optimized SHA-1 implementation. Of course, veriﬁ-
cations are nearly instantaneous after the derived challenge
is veriﬁed, since they require only two additional hash com-
putations.
The puzzle diﬃculty, the policy used to harvest the chal-
lenge, and the maximum age of acceptable challenges are
conﬁgurable. However, these parameters would need to be
standardized prior to widespread deployment to ensure in-
teroperability. We have tested the application with a sample
policy like the one in the previous section, using puzzles that
take approximately 10 seconds to solve on a fast machine
(n = 25) and are considered fresh for up to 60 minutes.
The Postmark server application acts as a local SMTP
proxy. It receives messages sent from an unmodiﬁed SMTP
client running on the local machine and appends puzzle so-
lutions before relaying them to the user’s normal SMTP
server. This design allows the tool to be nearly transparent
to the user, requiring only a simple mail client conﬁguration
change.
When the Postmark proxy server receives a message, it
uses Combine to derive a fresh harvested challenge, then
solves a client puzzle based on the challenge. It wraps the
original message in the MIME multipart/signed message
type [11] (this directs mail transfer agents not to modify
the body in transit and assures that the Postmark data will
not cause problem for other receivers). It then attaches a
second message part containing the following values: the
receiver address, the derived challenge, s, n, and the deriva-
tion created by Combine. Finally, it passes the message to
the original SMTP server for delivery.
Recipients invoke the Postmark ﬁlter on incoming mail
using Procmail or another message ﬁltering system. The
Postmark ﬁlter reads the received message from standard
input and tries to detect a Postmark puzzle solution. If one
is found, the ﬁlter checks that the recipient address used
in the puzzle matches the local user’s address, it uses Com-
bine to verify the challenge, it uses the timestamp on the
challenge to check that it is suitably fresh, and it veriﬁes
the puzzle solution. If all these tests are passed, the Post-
mark ﬁlter returns an exit code of 0; otherwise, it returns a
non-zero exit code. The user’s Procmail system can be con-
ﬁgured to allow mail with a valid Postmark to bypass other
spam ﬁlters or receive less scrutiny than unmarked mail.
6. EVALUATION: RSS FEEDS
RSS feeds show considerable variation in several respects
that are important for our purposes. They diﬀer in terms of
how many entries are served at once, how often new entries
are posted, how long entries remain available for veriﬁcation,
etc. We conducted an empirical study to determine whether
actual RSS feeds are suitable for use with our system.
We studied two sets of sources. The ﬁrst consists of the
most widely subscribed feeds as reported by the Bloglines
feed aggregation service on January 10, 2007. This set con-
tains 133 feeds, and we label it “Popular.” The second set
consists mainly of more esoteric feeds, which we compiled by
combining the subscriptions of four members of the Prince-
ton computer science department and removing any sites in
the Popular set. We believe this list is somewhat representa-
tive of the majority of RSS feeds, which fall into the long tail
of the popularity distribution. This set contains 142 feeds,
and we label it “Longtail.” For both data sets, we requested
the contents of each feed once every 10 minutes over a seven
day period in January 2007.
Availability.
Applications that require robustness will need to select
RSS sources with a high likelihood of being available when
challenges are derived and veriﬁed. We considered a source
to be available if it was reachable and returned parsable
Figure 2: Comparing the frequency of new posts and the average time posts remain in the feed reveals the expected
inverse relation.
Figure 3: We modeled simple policies and robustness requirements using feed data collected from our sources and
recorded the portion of the study period when each policy was satisﬁed. Here we plot the percentage of sources that
met or exceeded each level of satisfaction. The scenarios are: derivation using all posts fresh within one hour, at least
one post veriﬁable up to (A) 6 hours later, (B) 12 hours later; derivation using all posts fresh within one day, at least
one post veriﬁable up to (C) 7 days later, (D) 14 days later.
0102030400510152025303540Entries Per DayEntryLifetime(Days)PopularLongtail010203040506070809010005101520253035404550556065707580859095Policy Satisfaction (%)Percentage of Sources AboveABCDentries at a sample time. We disregarded sites that were
never available during out study (this was usually caused by
an outdated URL or chronically malformed data). For the
Popular data set, 2 of the 133 feeds never returned usable
data and were discarded. Of the remaining feeds, 90% were
available at least 99% of the time, and 97% were available
at least 95% of the time. For the Longtail data set, 16 of
the 142 feeds never returned usable data and were discarded.
Of the remaining feeds, 87% were available at least 99% of
the time, and 97% were available at least 95% of the time.
This data suggests that most feeds have high availability,
but our sample period was too short to draw a strong con-
clusion. The presence of outdated URLs in the sample sets
highlights the importance of a mechanism for occasionally