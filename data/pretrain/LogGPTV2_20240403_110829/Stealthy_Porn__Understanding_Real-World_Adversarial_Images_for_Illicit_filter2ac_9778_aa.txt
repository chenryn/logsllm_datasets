title:Stealthy Porn: Understanding Real-World Adversarial Images for Illicit
Online Promotion
author:Kan Yuan and
Di Tang and
Xiaojing Liao and
XiaoFeng Wang and
Xuan Feng and
Yi Chen and
Menghan Sun and
Haoran Lu and
Kehuan Zhang
(cid:19)(cid:17)(cid:18)(cid:26)(cid:1)(cid:42)(cid:38)(cid:38)(cid:38)(cid:1)(cid:52)(cid:90)(cid:78)(cid:81)(cid:80)(cid:84)(cid:74)(cid:86)(cid:78)(cid:1)(cid:80)(cid:79)(cid:1)(cid:52)(cid:70)(cid:68)(cid:86)(cid:83)(cid:74)(cid:85)(cid:90)(cid:1)(cid:66)(cid:79)(cid:69)(cid:1)(cid:49)(cid:83)(cid:74)(cid:87)(cid:66)(cid:68)(cid:90)
Stealthy Porn: Understanding Real-World
Adversarial Images for Illicit Online Promotion
Kan Yuan∗, Di Tang†, Xiaojing Liao∗, XiaoFeng Wang∗,
Xuan Feng∗‡, Yi Chen∗‡, Menghan Sun†, Haoran Lu∗, Kehuan Zhang†
∗Indiana University Bloomington, †Chinese University of Hong Kong, ‡Chinese Academy of Sciences
∗{kanyuan, xliao, xw7, haorlu}@indiana.edu, †{td016, sm017, khzhang}@ie.cuhk.edu.hk,
‡{fengxuan, chenyi}@iie.ac.cn
Abstract—Recent years have witnessed the rapid progress in
deep learning (DL), which also brings their potential weaknesses
to the spotlights of security and machine learning studies. With
important discoveries made by adversarial
learning research,
surprisingly little attention, however, has been paid to the real-
world adversarial techniques deployed by the cybercriminal to
evade image-based detection. Unlike the adversarial examples
that induce misclassiﬁcation using nearly imperceivable pertur-
bation, real-world adversarial images tend to be less optimal
yet equally eﬀective. As a ﬁrst step to understand the threat,
we report in the paper a study on adversarial promotional
porn images (APPIs) that are extensively used in underground
advertising. We show that the adversary today’s strategically
constructs the APPIs to evade explicit content detection while
still preserving their sexual appeal, even though the distortions
and noise introduced are clearly observable to humans.
To understand such real-world adversarial images and the
underground business behind them, we develop a novel DL-based
methodology called Mal`ena, which focuses on the regions of an
image where sexual content is least obfuscated and therefore
visible to the target audience of a promotion. Using this technique,
we have discovered over 4,000 APPIs from 4,042,690 images
crawled from popular social media, and further brought to light
the unique techniques they use to evade popular explicit content
detectors (e.g., Google Cloud Vision API, Yahoo Open NSFW
model), and the reason that these techniques work. Also studied
are the ecosystem of such illicit promotions, including the ob-
fuscated contacts advertised through those images, compromised
accounts used to disseminate them, and large APPI campaigns
involving thousands of images. Another interesting ﬁnding is the
apparent attempt made by cybercriminals to steal others’ images
for their advertising. The study highlights the importance of the
research on real-world adversarial learning and makes the ﬁrst
step towards mitigating the threats it poses.
I. Introduction
Adversarial learning aims at understanding the weaknesses
of machine learning in the adversarial environment and de-
veloping protection against potential threats. Research along
this line can be traced back a decade ago, to evasive attacks
on intrusion detection systems [32] and spam ﬁlters [59], and
to data contamination risks in classiﬁers [60]. More recently,
the rapid progress of deep neural networks (DNN) and their
wide adoption in image processing have moved the focus of
adversarial learning to these models’ vulnerabilities towards
adversarial examples: it has been found that a small amount
of noise, once added to an image, could cause a DNN to
misclassify the image, even when the modiﬁed image looks
almost indistinguishable from the original one to humans.
Given the security-critical applications of the DNN-based
image classiﬁcation, like self-driving cars, face recognition
based authentication, etc., such risks have aroused a great deal
of interest from the security community as well as the industry,
even though no evidence has yet been found that related
attacks have ever taken place in the real life. In the meantime,
surprisingly little attention has been paid to the adversarial
techniques actually employed by real-world cybercriminals,
particularly those against image classiﬁcation systems, which
turn out to be quite diﬀerent from those intensively studied by
the aforementioned research [37], [49], [55].
Adversarial explicit content. More speciﬁcally, anecdotes
have it that obfuscated images have been extensively used by
the underground businesses for illicit online advertising (Ad),
phishing and other insidious purposes. Unlike the old style
image spam, where spam messages are directly embedded in
pictures, today’s promotional images include explicit sexual
and violence content to attract audience and various obfus-
cation tricks to hide them from automatic content checkers,
such as Google Cloud Vision API, Baidu AipImageCensor
API, Clarifai NSFW API. Examples of such images (with
proper masking) are presented in Figure 1. Compared with
the adversarial examples studied by the ongoing adversarial
learning, such adversarial explicit content does not need to
be optimized in a sense that the perturbation introduced to
an image remains less perceivable to humans. Instead, all the
adversary wants is just to get the semantics (e.g., pornography)
through without triggering the alarm (e.g., Google SafeSearch
ﬁlter). This lowers the bar to constructing the attack instances
and raises the challenges for ﬁnding them. Indeed, so far,
little has been done to systematically discover and analyze
the adversarial explicit content, not to mention any eﬀort to
understand the underground ecosystem behind these images.
Mal`ena: ﬁnding stealthy porn. In this paper, we report
the ﬁrst systematic study on the adversarial explicit content,
focusing on adversarial promotional porn images (APPIs),
based upon a novel methodology for a large scale discovery
of such images. More speciﬁcally, we developed a Malicious
Explicit Content Analyzer, called Mal`ena (Section III), lever-
aging two key observations about these obfuscated images:
they need to include promotional information (links, phone
(cid:165)(cid:1)(cid:19)(cid:17)(cid:18)(cid:26)(cid:13)(cid:1)(cid:44)(cid:66)(cid:79)(cid:1)(cid:58)(cid:86)(cid:66)(cid:79)(cid:15)(cid:1)(cid:54)(cid:79)(cid:69)(cid:70)(cid:83)(cid:1)(cid:77)(cid:74)(cid:68)(cid:70)(cid:79)(cid:84)(cid:70)(cid:1)(cid:85)(cid:80)(cid:1)(cid:42)(cid:38)(cid:38)(cid:38)(cid:15)
(cid:37)(cid:48)(cid:42)(cid:1)(cid:18)(cid:17)(cid:15)(cid:18)(cid:18)(cid:17)(cid:26)(cid:16)(cid:52)(cid:49)(cid:15)(cid:19)(cid:17)(cid:18)(cid:26)(cid:15)(cid:17)(cid:17)(cid:17)(cid:20)(cid:19)
(cid:26)(cid:22)(cid:19)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:51:56 UTC from IEEE Xplore.  Restrictions apply. 
such obfuscated pornographic pictures. More speciﬁcally, from
the images, we discovered 31 URLs, 76 QQ IDs, 245 WeChat
IDs, and 45 QR code and 266 other contacts. Such information
was obfuscated in some cases, using jargons, emojis or homo-
phonic characters, apparently in an attempt to evade the OCR
based text detection. Further we studied the ways such APPIs
are disseminated: for example, 3,080 accounts on Baidu Tieba
and 472 accounts on Sina Weibo were found to be involved in
the distribution of APPIs, where 1,676 Baidu Tieba and Sina
Weibo account were compromised legitimate accounts. Also
discovered in our research was a huge APPI spam campaign,
which used 1,325 APPIs to promote more than 7 illicit mobile
sexual apps. Interestingly, we observed that APPIs were being
reused, with their original promotional content erased, which
indicates possible competitions among cybercriminals.
Contributions. The contributions of the paper are outlined as
follows:
• New understanding about the use of adversarial images in
the cybercrime. We report the ﬁrst systematic study on the
real-world adversarial images and their use in online illicit
promotions. Our study sheds light on how evasive techniques
are deployed by cybercriminals to evade image detection
systems and identiﬁes the gap between these techniques and
what have been studied in the ongoing adversarial learning.
Further our study brings to light the ecosystem behind such
illicit promotions, which is important for ﬁnding technical and
policy means to address such new security challenges.
• New techniques for ﬁnding real-world adversarial images.
We developed a novel methodology to identify those ad-
versarial images, which demonstrates to be highly eﬀective
on today’s APPIs. Although explicit content detection likely
continues to be in an arm race with cybercriminals, our
approach raises the bar to the evasive attacks, and makes a
ﬁrst step toward more eﬀective control of this emerging threat.
Roadmap. The rest of the paper is organized as follows:
Section II presents the background of our study; Section III
elaborates the design of Mal`ena and Section IV presents its
implementation and evaluation; Section V describes our large-
scale measurement study on APPIs using Mal`ena; Section VI
continues to unravel the underground ecosystem behind those
images; Section VII discusses the limitation of our current
research and ethical issues; Section VIII surveys the related
prior research and Section IX concludes the paper.
II. Background
A. Promotional Explicit Content
Promotional explicit content aims to utilize sex appeal
images (e.g., explicit displays of sexual acts and seductive
behavior) for advertising, typically through injecting promo-
tional URLs, QR codes or instant message app IDs into a
pornographic image. Such content has been used to serve var-
ious purposes, such as phishing and promotion of counterfeit
products, illicit online pharmacy, gambling or porn sites, etc.
(a) APPI 1
(b) APPI 2 (open
with Firefox)
(c) APPI 2 (open
with Chrome)
Fig. 1: Examples of APPIs. (b) and (c) show a special APPI
that displays diﬀerent content on white and black background.
number, etc.) for follow-up and they cannot obfuscate all the
obscene image content or risk losing interest from the target
audience. Exploiting these observations, our approach ﬁrst
identiﬁes the pictures carrying text or numbers or QR code and
then performs a Region-of-Interest (ROI) processing to ﬁnd
the persons in each image. After that, we run a DNN-based
explicit content detector on each identiﬁed region (including
the mask of the person detected from the ROI) to discover
pornographic content. In this way, we are able to signiﬁcantly
reduce the image regions to which noise can be injected for
inducing misclassiﬁcation, and achieve a precision and a recall
of 91% and 85%.
Measurement and discoveries. Running Mal`ena on the data
crawled from 2 forums, including Baidu Tieba [1], the largest
Chinese largest Chinese communication platform provided
by the Chinese search engine company, Baidu, and Sina
Weibo [9], a Chinese microblogging website, we were able
to detect over 4,000 conﬁrmed APPIs from totally 4,042,698
images downloaded. Analyzing these images, we discovered
interesting obfuscation techniques deployed in underground
advertising, such as adding high-frequency signals (e.g., tex-
turing and noising) or ﬁlter eﬀects (e.g., blurring) to an image.
Of particular interest is the observation that some images have
been converted from the RGB color space into grayscale to
evade skin-related features widely utilized in explicit content
detection.
Those APPIs turn out to be quite eﬀective in evading state-
of-the-art explicit content detectors such as Google Cloud
Vision API, Baidu AipImageCensor API, Yahoo Open NSFW
model, and Clarifai NSFW API: we observed that 35.6% of
the APPIs circumvented all four detectors. Further, we looked
into the open-source Yahoo NSFW model, a convolutional
neural network model, to ﬁnd out how it missed those APPIs.
Particularly, we examined the output of its ﬁrst convolution
layer, which is used to extract image edge features, and found
that the obfuscations performed on the APPIs signiﬁcantly
degrade the qualities of these features.
Further using the links or the WeChat numbers promoted by
those images, we were able to analyze the ecosystem behind
(cid:26)(cid:22)(cid:20)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:51:56 UTC from IEEE Xplore.  Restrictions apply. 
Dissemination of explicit content has been controlled in
many countries. For example, in US, the Child Online Pro-
tection Act restricts the exposure of such content to minors,
though the legal status of Internet pornography is still less clear
for adults; in China, explicit content has been forbidden by its
Cybersecurity law [4]. Also, regulations on sexual materials
have been put in place by the industry. As an example, Twitter
does not allow adult content to be used as a user proﬁle or
header image [11], Google provides its SafeSearch lock to
protect minors and all mainstream Chinese social networks
(e.g., Sina Weibo [9]) and online forums (e.g., Baidu Tieba [1])
prohibit explicit content [10] [2]. Further Google, Yahoo,
Microsoft, Baidu and others all provide their inappropriate
content detection services. Examples include Google Cloud
Vision API [7] and Baidu AIP ImageCensor API [18]. In
response to such control and censorship,
the underground
advertiser starts to utilize adversarial images to evade the
detection, as observed in our research.
B. Image Processing
Center to the arm race between the underground advertiser
and the explicit content regulator are image processing tech-
niques, which we brieﬂy introduce below.
Object recognition. Object recognition is a computer vision
task for detecting and recognizing the instances of semantic
objects in a certain class from images or videos. Although
this ability comes naturally to humans, it is actually fairly
challenging for computers. Many solutions have been proposed
in the past several decades, from traditional feature-based
approaches to deep learning.
Among the most inﬂuential object recognition techniques
today is the Regions with Convolutional Neural Networks (R-
CNN) [34], [35], [39], [53]. More speciﬁcally, in R-CNNs,
a manageable number of “regions of interest” or “ROIs”,
that may contain object instances, are ﬁrst identiﬁed. Then
a convolutional neural network (CNN) [42] is applied on
each region candidates to extract features independently for
classiﬁcation. Particularly, Mask R-CNN [39] is a cutting-edge
R-CNN technique. Besides reporting the object type and the
corresponding bounding box, Mask R-CNN also segments the
object from the bounding box, which is achieved by adding a
Fully Convolutional Network (FCN) [44] on top of the feature
map extracted by the CNN. The FCN predicts a binary mask
indicating whether or not a given pixel is part of the object.
Hence, the recognized object can be segmented in pixel-level
with high quality. In our research, we utilize the segmentation
mask generated by Mask R-CNN to degrade the interference
of obfuscation techniques in the image and to recognize the
explicit content eﬀectively.
Scene text detection. Scene text is the text content that ap-
pears in an image, which may vary in shape, font, color, orien-
tation and position across images. In our research, we utilized
an oﬀ-the-shelf scene text detection tool, PixelLink [31], to
capture it for analyzing the promotional content it advertises.
More speciﬁcally, PixelLink uses a neural network to perform
a pixel-level text/non-text prediction, ﬁrst to ﬁnd out how
likely a pixel is part of a text instance, and then to determine
whether adjacent pixel pairs can be linked together and related
to the same instance. After that, it performs text instance
segmentation through joining these linked text pixels to detect
the content of the text. According to the prior research [31],
PixelLink can achieve a precision of 87.5%, recall of 88.6%,
and F-score of 88.1%.
Explicit content detection. In our research, we ran popular
explicit content detectors on APPIs to understand the eﬀec-
tiveness of real-world adversarial images in evading these
machine learning models. Such tools include Google Cloud
Vision API [7], Yahoo “Not Suitable for Work” (NSFW)
Image detector [17], Baidu AIP ImageCensor API [18], and
Clarifai NSFW API [5].