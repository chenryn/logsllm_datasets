performance decreasing ratio (Tf/tpmC). These measures 
characterize the impact of faults in the performance. 
tpmC
$/tpmC
2244
Baseline performance
2502
2493
2270
12
11,9
11,6
11,6
2KOra8i XPOra8i 2KOra9i XPOra9i
$
19
17
15
13
11
2300
2000
1700
1400
Figure 3. Baseline performance results. 
Figure 4 shows the performance results in the presence 
of  the  faultload  presented  in  Table  3.  As  we  can  see,  Tf 
depends  both  on  the  operating  system  and  on  the  DBMS 
used.  Systems  based  on  the  Oracle  9i  DBMS  present 
better results than systems based on the Oracle 8i DBMS 
(similar to  what happened for baseline performance), and 
systems running the same DBMS present different results 
according  to  the  operating  system  used.  For  the  systems 
running  the  Oracle8i  DBMS,  the  Windows  XP  operating 
system is clearly more effective than Windows 2000, and 
for  the  systems  running  the  Oracle  9i  DBMS  the  reverse 
seems  to  occur  (however,  the  small  difference  in  the 
results  does  not  allow  a  solid  conclusion).  However,  the 
results show that recovery in Oracle 8i is faster when the 
DBMS  is  running  over  Windows  XP  (that  is  why  the 
number of transactions in the presence of faults improves 
in  Oracle  8i  over  Windows  XP),  which  shows  that  the
operating system plays an important role in the recovery 
time in this DBMS. In Oracle 9i the influence of the oper-
ating systems on the recovery time is clearly lower, which 
suggests that Oracle 9i is less dependent on the operating 
system to process the logs needed for the recovery. 
Performance with faults
Tf
$/Tf
17,7
1525
16,2
16
16,4
1818
1764
1667
$
19
17
15
13
11
%
100
90
80
70
60
50
Tf/tpmC
Tf/tpmC
68
73
x
73
71
2KOra8i XPOra8i 2KOra9i XPOra9i
2KOra8i XPOra8i 2KOra9i XPOra9i
Concerning 
Figure 4. Performance results in the presence of faults.
less 
expensive  system  (2KOra8i)  presents  the  worst  results 
(due to its poor performance in the presence of faults).  
the  prices  per 
transaction, 
the 
A  very  important  aspect  to  observe  is  that  the  ratio 
between the Tf and tpmC is equal for two of the systems 
(XPOra8i  and  2KOra9i),  which  shows  that  faults  have  a 
similar  impact  in  a  system  with  the  Oracle  8i  DBMS 
running  over  Windows  XP  and  in  a  system  with  the 
Oracle  9i  DBMS  running  over  Windows  2000.  For  the 
other two systems, the impact of faults is more visible. 
4.3. Dependability measures 
The  dependability  measures  reported  by  the  DBench-
OLTP  benchmark  are  the  number  of  data  integrity  errors 
caused  by  faults  (Ne),  the  availability  from  the  SUT 
(server)  point-of-view  in  the  presence  of  faults  (AvtS),
and the availability from the end-users (RTE) point-of-view 
(AvtC). These  measures characterize the impact of  faults 
in the system dependability. 
Figure  5  shows  the  availability  of  the  systems  during 
the benchmark run. Results show that availability from the 
clients  point-of-view  is  always  much  lower  than  the 
availability from the server point-of-view, which seems to 
be normal because some types of faults affect the system 
in  a  partial  way.  An  interesting  result  is  the  fact  that  the 
availability  observed  for  Oracle8i  DBMS  over  Windows 
XP  is  better  than  when  the  same  DBMS  is  run  over 
Windows 2000. A similar result has been observed in the 
systems running the Oracle 9i DBMS.  
A  very  important  conclusion  is  that  no  data  integrity 
errors (Ne) were detected. This shows  that  the  Oracle 
DBMS  is  very  effective  in  handling  faults  caused  by  the 
operator. 
%
100
90
80
70
Availability
AvtS (Server)
AvtC (Clients)
88
87,2
88,6
79,4
79,5
79,5
86,1
75,4
2KOra8i XPOra8i 2KOra9i XPOra9i
Figure 5. Availability in the presence of the faultload. 
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:27:42 UTC from IEEE Xplore.  Restrictions apply. 
4.4. Systems ranking 
As  mentioned  before,  the  goal  of  the  benchmarking 
presented  in  this  work  is  to  compare  and  rank  four 
different  transactional  systems  using  the  DBench-OLTP 
dependability benchmark. Table 4 summarizes the ranking 
we propose according to several criteria.  
One clear conclusion is that systems running Oracle 9i 
are clearly better than the systems running Oracle 8i. The 
impact of the operating system on the  measures is  not so 
clear,  as  in  some  cases  the  systems  using  Windows  2000 
are  better  than  the  systems  using  Windows  XP  and  in 
other cases the opposite has been observed.  
Concerning  a  global  ranking,  the  analysis  of  Table  4 
and  all  the  results  presented  in  the  previous  sub-sections 
allow  us to propose the following order (from the best to 
the worst): 2KOra9i, XPOra9i, XPOra8i, and 2KOra8i. It 
is  important  to  note  that  the  global  ranking  always 
depends on what the benchmark performer is looking for. 
Criteria 
Baseline performance 
Performance with faults 
Ratio Tf/tpmC 
Availability 
2nd 
Best 
3rd  Worst 
XpOra9i  2kOra9i  XpOra8i 2kOra8i
2kOra9i  XpOra9i  XpOra8i 2kOra8i
2kOra9i  XpOra8i  XpOra9i 2kOra8i
XpOra9i  XpOra8i  2kOra9i  2kOra8i
Table 4. Systems ranking. 
5. Benchmark execution effort 
Usually  benchmarking  is  seen  as  an  expensive  and 
laborious process. During the course of the present  work, 
we  had  the  opportunity  to  assess  the  necessary  effort  to 
implement the benchmark and to conduct the benchmark-
ing  process.  Several  indicators  have  been  collected,  such 
as: the time needed to implement TPC-C, the time needed 
to  implement  DBench-OLTP,  and  the  time  needed  to 
conduct the benchmarking process.  
task, 
the  most  complex 
In  spite  of  being 
the 
implementation of the TPC-C benchmark took only about 
10  days.  This  was  possible  due  to  the  reuse  of  existing 
code and examples from several previous implementations 
(in  a  normal  situation  the  TPC-C  implementation  could 
take  more  than  30  working  days).  The  DBench-OLTP 
benchmark  presents  a  similar  implementation  time  (10 
days). However, like  for TPC-C  we can reduce the effort 
needed to implement DBench-OLTP benchmark by reusing 
code from previous implementations (in our case this was 
not possible because this was the first implementation). 
Concerning  the  time  needed  to  conduct  the  bench-
marking process, the time spent is very short (12 days). In 
fact, considering the class of systems used in this work we 
have  been  able  to  benchmark  four  different  systems  in 
about  32  days.  The  ratio  between  the  total  effort  and 
number  of  systems  benchmarked  is  of  about  8  working 
days.  However,  it  is  important  to  note  that  this  ratio  de-
creases when the number of systems under benchmarking 
increases.  Thus,  we  can  conclude  that  after  having  the 
benchmarks  implemented  (TPC-C  and  DBench-OLTP) 
the effort needed to benchmark additional systems is small. 
6. Conclusion
This  paper  presents 
a  practical 
the  dependability  of 
benchmarking 
transactional 
dependability 
environments – the DBench-OLTP benchmark.  
systems  using  a 
for  OLTP 
benchmark 
example  of 
four  different 
first  proposal  of 
application 
Two  different  versions  of  the  Oracle  transactional 
engine running over two different operating systems have 
been  evaluated  and  compared.  The  experimental  results 
obtained  were  analyzed  and  discussed  in  detail.  These 
results allow us to rank the four systems concerning both 
performance  and  dependability  and  clearly  show  that 
dependability  benchmarking  can  be  successfully  applied 
to OLTP application environments. 
The paper ends with a discussion of the effort required 
to  run  the  dependability  benchmark.  From  the  indicators 
collected  during  this  work,  we  could  observe  that  that 
effort is not an obstacle for not using this kind of tools on 
transaction systems evaluation and comparison. 
References 
de 
for 
OLTP 
2002, 
available 
Coimbra, 
Benchmark 
[1]  Transaction  Processing  Performance  Consortium,  “TPC 
BenchmarkTM  C,  Standard  Specification,  Version  5.0”,  2001, 
available at: http://www.tpc.org/tpcc/. 
[2]  M.  Vieira  and  H.  Madeira,  “DBench  –  OLTP:  A 
Dependability 
Application 
Environments”,  Technical  Report  DEI-006-2002,  ISSN  0873-
9293,  DEI  –  Faculdade  de  Ciências  e  Tecnologia  da 
Universidade 
at: 
http://www.dei.uc.pt/~henrique/DBenchOLTP.htm. 
[3]  A.  Brown  and  D.  Patterson,  “To  Err  is  Human”,  First 
Workshop on Evaluating and Architecting System Dependability 
(EASY), Joint organized with Int. Conf. on Dependable Systems 
and Networks, DSN-2001, Göteborg, Sweden, July, 2001. 
[4]  M  .Vieira  and  H.  Madeira,  “Definition  of  Faultloads  Based 
on Operator Faults for DMBS Recovery Benchmar-king”, 2002 
Pacific Rim Intl Symp. on Dependable Computing, PRDC2002, 
Tsukuba, Japan, December, 2002. 
[5]  J.  Gray  (Ed.),  “The  Benchmark  Handbook”,  Morgan 
Kaufmann Publishers, San Francisco, CA, USA, 1993. 
[6]  H.  Madeira,  K.  Kanoun,  J.  Arlat,  Y.  Crouzet,  A.  Johanson 
and  R.  Lindström,  “Preliminary  Dependability  Benchmark 
Framework”, DBench Project, IST 2000-25425,  August, 2001. 
[7]  R.  Ramakrishnan,  “Database  Management  Systems”  second 
edition, McGraw Hill, ISBN 0 07-232206-3. 
[8] M. Vieira and H. Madeira, “Recovery and Performance Ba-
lance of a COTS DBMS in the Presence of Operator Faults”, Intl 
Performance  and  Dependability  Symp.  (jointly organized  with 
DSN-2002), IPDS2002, Bethesda, Maryland, USA, June, 2002. 
Acknowledgements 
Funding for this paper was provided, in part, by Portuguese 
Government/European  Union 
through  R&D  Unit  326/94 
(CISUC)  and  by  DBench  project,  IST  2000  -  25425  DBENCH, 
funded by the European Union.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:27:42 UTC from IEEE Xplore.  Restrictions apply.