### Performance Decreasing Ratio (Tf/tpmC)

These measures characterize the impact of faults on system performance.

| tpmC | $/tpmC |
|------|--------|
| 2244 | Baseline performance |
| 2502 | 12    |
| 2493 | 11.9  |
| 2270 | 11.6  |
|      | 11.6  |

| Configuration | 2KOra8i | XPOra8i | 2KOra9i | XPOra9i |
|---------------|----------|----------|----------|----------|
| $             | 19       | 17       | 15       | 13       |
|               | 11       |          |          |          |
| 2300          | 2000     | 1700     | 1400     |          |

**Figure 3. Baseline performance results.**

**Figure 4** shows the performance results in the presence of the faultload presented in Table 3. As we can see, Tf (the time to recover from a fault) depends on both the operating system and the DBMS used. Systems based on Oracle 9i DBMS present better results than those based on Oracle 8i DBMS, similar to the baseline performance. Systems running the same DBMS show different results depending on the operating system used. For systems running Oracle 8i DBMS, Windows XP is more effective than Windows 2000. Conversely, for systems running Oracle 9i DBMS, Windows 2000 appears to be more effective, though the small difference in results does not allow a solid conclusion. The results indicate that recovery in Oracle 8i is faster when running on Windows XP, which improves the number of transactions in the presence of faults. This highlights the significant role of the operating system in recovery time for Oracle 8i. In contrast, the influence of the operating system on recovery time is lower in Oracle 9i, suggesting that Oracle 9i is less dependent on the operating system for processing logs needed for recovery.

| Performance with Faults | Tf   | $/Tf  |
|-------------------------|------|-------|
| 17.7                    | 1525 | 16    |
| 16.2                    | 16   | 1818  |
| 16.4                    | 1764 | 1667  |
|                         | 19   | 17    |
|                         | 15   | 13    |
|                         | 11   | 100   |
|                         | 90   | 80    |
|                         | 70   | 60    |
|                         | 50   | Tf/tpmC |
|                         | 68   | 73    |
|                         | 73   | 71    |

| Configuration | 2KOra8i | XPOra8i | 2KOra9i | XPOra9i |
|---------------|----------|----------|----------|----------|
| Tf/tpmC       | 68       | 73      | 73       | 71      |

**Figure 4. Performance results in the presence of faults.**

The less expensive system (2KOra8i) presents the worst results due to its poor performance in the presence of faults. An important observation is that the ratio between Tf and tpmC is equal for two of the systems (XPOra8i and 2KOra9i), indicating that faults have a similar impact on a system with Oracle 8i DBMS running on Windows XP and a system with Oracle 9i DBMS running on Windows 2000. For the other two systems, the impact of faults is more pronounced.

### 4.3. Dependability Measures

The dependability measures reported by the DBench-OLTP benchmark include the number of data integrity errors caused by faults (Ne), the availability from the SUT (server) point-of-view in the presence of faults (AvtS), and the availability from the end-users (RTE) point-of-view (AvtC). These measures characterize the impact of faults on system dependability.

**Figure 5** shows the availability of the systems during the benchmark run. Results indicate that availability from the clients' point-of-view is always much lower than from the server's point-of-view, which is expected as some types of faults affect the system partially. An interesting result is that the availability observed for Oracle 8i DBMS over Windows XP is better than when the same DBMS runs on Windows 2000. A similar result is observed for systems running Oracle 9i DBMS.

A very important conclusion is that no data integrity errors (Ne) were detected. This demonstrates that the Oracle DBMS is highly effective in handling faults caused by the operator.

| Availability | AvtS (Server) | AvtC (Clients) |
|--------------|---------------|----------------|
| 100          | 90            | 80             |
| 70           | 88            | 87.2           |
| 88.6         | 79.4          | 79.5           |
| 79.5         | 86.1          | 75.4           |

| Configuration | 2KOra8i | XPOra8i | 2KOra9i | XPOra9i |
|---------------|----------|----------|----------|----------|
| AvtS (Server) | 88       | 87.2     | 88.6     | 79.4    |
| AvtC (Clients)| 79.5     | 79.5     | 86.1     | 75.4    |

**Figure 5. Availability in the presence of the faultload.**

### 4.4. Systems Ranking

The goal of the benchmarking presented in this work is to compare and rank four different transactional systems using the DBench-OLTP dependability benchmark. Table 4 summarizes the proposed ranking according to several criteria.

One clear conclusion is that systems running Oracle 9i are better than those running Oracle 8i. The impact of the operating system on the measures is not as clear, as in some cases, systems using Windows 2000 perform better than those using Windows XP, and vice versa.

In terms of a global ranking, the analysis of Table 4 and all the results presented in the previous sub-sections allow us to propose the following order (from best to worst): 2KOra9i, XPOra9i, XPOra8i, and 2KOra8i. It is important to note that the global ranking depends on what the benchmark performer is looking for.

| Criteria             | Baseline Performance | Performance with Faults | Ratio Tf/tpmC | Availability |
|----------------------|----------------------|-------------------------|---------------|--------------|
| Best                 | 2kOra9i              | 2kOra9i                 | 2kOra9i       | XpOra9i      |
| 2nd                  | XpOra9i              | XpOra9i                 | XpOra8i       | XpOra8i      |
| 3rd                  | XpOra8i              | XpOra8i                 | XpOra9i       | 2kOra9i      |
| Worst                | 2kOra8i              | 2kOra8i                 | 2kOra8i       | 2kOra8i      |

**Table 4. Systems ranking.**

### 5. Benchmark Execution Effort

Benchmarking is often seen as an expensive and laborious process. During this work, we assessed the necessary effort to implement the benchmark and conduct the benchmarking process. Several indicators were collected, including the time needed to implement TPC-C, the time needed to implement DBench-OLTP, and the time needed to conduct the benchmarking process.

Despite being a complex task, the implementation of the TPC-C benchmark took only about 10 days, made possible by reusing existing code and examples from previous implementations. The DBench-OLTP benchmark also required a similar implementation time (10 days). However, like TPC-C, the effort needed to implement DBench-OLTP can be reduced by reusing code from previous implementations, although this was not possible in our case as it was the first implementation.

The time needed to conduct the benchmarking process was very short (12 days). Considering the class of systems used in this work, we were able to benchmark four different systems in about 32 days, resulting in a ratio of about 8 working days per system. This ratio decreases as the number of systems under benchmarking increases. Therefore, after implementing the benchmarks (TPC-C and DBench-OLTP), the effort needed to benchmark additional systems is relatively small.

### 6. Conclusion

This paper presents a practical example of benchmarking the dependability of transactional systems using the DBench-OLTP benchmark. Two different versions of the Oracle transactional engine running on two different operating systems were evaluated and compared. The experimental results were analyzed and discussed in detail. These results allowed us to rank the four systems concerning both performance and dependability, clearly showing that dependability benchmarking can be successfully applied to OLTP application environments.

The paper concludes with a discussion of the effort required to run the dependability benchmark. From the indicators collected during this work, we observed that the effort is not a significant obstacle to using such tools for evaluating and comparing transaction systems.

### References

[1] Transaction Processing Performance Consortium, “TPC BenchmarkTM C, Standard Specification, Version 5.0”, 2001, available at: http://www.tpc.org/tpcc/.

[2] M. Vieira and H. Madeira, “DBench – OLTP: A Dependability Benchmark for OLTP Application Environments”, Technical Report DEI-006-2002, ISSN 0873-9293, DEI – Faculdade de Ciências e Tecnologia da Universidade de Coimbra, 2002, available at: http://www.dei.uc.pt/~henrique/DBenchOLTP.htm.

[3] A. Brown and D. Patterson, “To Err is Human”, First Workshop on Evaluating and Architecting System Dependability (EASY), Joint organized with Int. Conf. on Dependable Systems and Networks, DSN-2001, Göteborg, Sweden, July, 2001.

[4] M. Vieira and H. Madeira, “Definition of Faultloads Based on Operator Faults for DMBS Recovery Benchmar-king”, 2002 Pacific Rim Intl Symp. on Dependable Computing, PRDC2002, Tsukuba, Japan, December, 2002.

[5] J. Gray (Ed.), “The Benchmark Handbook”, Morgan Kaufmann Publishers, San Francisco, CA, USA, 1993.

[6] H. Madeira, K. Kanoun, J. Arlat, Y. Crouzet, A. Johanson, and R. Lindström, “Preliminary Dependability Benchmark Framework”, DBench Project, IST 2000-25425, August, 2001.

[7] R. Ramakrishnan, “Database Management Systems” second edition, McGraw Hill, ISBN 0 07-232206-3.

[8] M. Vieira and H. Madeira, “Recovery and Performance Balance of a COTS DBMS in the Presence of Operator Faults”, Intl Performance and Dependability Symp. (jointly organized with DSN-2002), IPDS2002, Bethesda, Maryland, USA, June, 2002.

### Acknowledgements

Funding for this paper was provided, in part, by the Portuguese Government/European Union through R&D Unit 326/94 (CISUC) and by the DBench project, IST 2000 - 25425 DBENCH, funded by the European Union.

Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03)  
0-7695-1959-8/03 $17.00 (c) 2003 IEEE  
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19, 2021, at 12:27:42 UTC from IEEE Xplore. Restrictions apply.