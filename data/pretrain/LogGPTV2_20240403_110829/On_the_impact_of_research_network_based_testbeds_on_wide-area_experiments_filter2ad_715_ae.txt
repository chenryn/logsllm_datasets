 100
Connection
(d) Available Bandwidth
(e) Capacity
(f) Throughput
Figure 13: Performance properties of Case 2 and Case 3
1
Berkeley
2
3
Stanford
Seattle
Tree 1
4
5
5
Princeton
Pittsburgh
Stanford
2
1
Berkeley
Pittsburgh
4
Princeton
Seattle
3
Tree 2
Figure 14: Trees used in application-layer multicast.
cation’s performance depends on the RTT, path loss and
available bandwidth.
In this experiment, an application-
layer multicast tree is used to transfer a 4 MB ﬁle from the
source to all the children. Each branch of the tree transfers
data reliably using TCP and each branch is independently
ﬂow controlled since each node in the tree implements an
application layer buﬀer. The tree is built using a topology-
aware protocol similar to that used in NICE [2].
We make two diﬀerent topologies, each containing ﬁve
nodes at ﬁve diﬀerent locations as shown in Figure 14. Both
trees consist of nodes at Berkeley, Stanford, Seattle, Prince-
ton and Pittsburgh. Tree 1 has a source at Berkeley and
Tree 2 has a source at Princeton and thus results in two dif-
ferent topologies. Each location is annotated with a unique
number. For each tree, we compare the performance where
all nodes for all locations belong to C with that where all
nodes in those locations belong to G. Thus for each tree we
have a G2G scenario and a C2C scenario.
6.1.1 Results
We ﬁrst compare the throughput obtained from two dif-
ferent trees shown in Figure 14. For tree 1 we found that
the average throughput of receivers in the G2G scenario was
)
s
p
b
K
t (
u
p
h
g
u
o
r
h
T
7000
6000
5000
4000
3000
2000
1000
0
(a) Tree 1
G 2G
C 2C
G -C
(b) Tree 2
G 2G
C 2C
G -C
)
s
p
b
K
t (
u
p
h
g
u
o
r
h
T
14000
12000
10000
8000
6000
4000
2000
0
R 2
R 3
R 4
R eceivers
R 5
R 1
R 2
R 3
R eceivers
R 4
Figure 15: Multicast performance in mixed testbeds.
3935.4 Kbps while the average throughput in the C2C sce-
nario was 2379.5 Kbps, a decrease of 40%. Similarly, for tree
2 the average throughput of receivers in the G2G scenario
was 4916.7 Kbps while the average throughput in the C2C
scenario was 2742.5 Kbps, a decrease of 44.2%.
Figure 15 shows that throughput achieved by each receiver
for both trees in C2C and G2G scenarios. In Tree 1, notice
that each receiver (2,3,4 and 5) receives the ﬁle with lower
throughput when the nodes belong to commercial networks
compared to when they belong to GREN. Speciﬁcally the
link from Berkeley to Pittsburgh is particularly worse which
then aﬀects the throughput at Princeton. We veriﬁed us-
ing pathneck that the throughput reduction between these
nodes in the C2C scenario was in fact due to a bottleneck in
the core and not at the access link of the commercial nodes.
This can be veriﬁed by seeing that in tree 2, the commercial
node at Pittsburgh received a throughput of almost 4Mbps.
In contrast, the link from Berkeley to Pittsburgh was able
to transfer at 3Mbps in the G2G scenario.
In Tree 2, we ﬁnd that the throughputs are actually com-
parable in G2G and C2C scenarios for receivers 1, 2 and 3.
However, we again see a large gap between the throughput
achieved for receiver 4 on the G2G link between Prince-
ton and Pittsburgh (12 Mbps) versus the C2C link between
Conn.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
Source
ATT
ATT
ATT
ATT
ATT
ATT
ATT
ATT
ATT
ATT
nbgisp
nbgisp
nbgisp
nbgisp
nbgisp
nbgisp
nbgisp
nbgisp
nbgisp
nbgisp
IRL Berk.
IRL Berk.
IRL Berk.
IRL Berk.
IRL Berk.
IRL Berk.
IRL Berk.
IRL Berk.
IRL Berk.
IRL Berk.
NEC Labs
NEC Labs
NEC Labs
NEC Labs
NEC Labs
NEC Labs
NEC Labs
NEC Labs
NEC Labs
NEC Labs
HP Labs
HP Labs
HP Labs
HP Labs
HP Labs
HP Labs
HP Labs
HP Labs
HP Labs
HP Labs
IRL. Pitt.
IRL. Pitt.
IRL. Pitt.
IRL. Pitt.
IRL. Pitt.
IRL. Pitt.
IRL. Pitt.
IRL. Pitt.
IRL. Pitt.
IRL. Pitt.
IRL Seatt.
IRL Seatt.
IRL Seatt.
IRL Seatt.
IRL Seatt.
IRL Seatt.
IRL Seatt.
IRL Seatt.
IRL Seatt.
IRL Seatt.
IRL Camb.
IRL Camb.
IRL Camb.
IRL Camb.
IRL Camb.
IRL Camb.
IRL Camb.
IRL Camb.
IRL Camb.
IRL Camb.
TPG Warsaw
TPG Warsaw
TPG Warsaw
TPG Warsaw
TPG Warsaw
TPG Warsaw
TPG Warsaw
TPG Warsaw
TPG Warsaw
PL-AMST
PL-AMST
Dest
nbgisp
IRL Berk.
NEC Labs
HP Labs
IRL. Pitt.
atcorp
IRL Seatt.
IRL Camb.
TPG Warsaw
PL-AMST
ATT
IRL Berk.
NEC Labs
HP Labs
IRL. Pitt.
atcorp
IRL Seatt.
IRL Camb.
TPG Warsaw
PL-AMST
ATT
nbgisp
NEC Labs
HP Labs
IRL. Pitt.
atcorp
IRL Seatt.
IRL Camb.
TPG Warsaw
PL-AMST
ATT
nbgisp
IRL Berk.
HP Labs
IRL. Pitt.
atcorp
IRL Seatt.
IRL Camb.
TPG Warsaw
PL-AMST
ATT
nbgisp
IRL Berk.
NEC Labs
IRL. Pitt.
atcorp
IRL Seatt.
IRL Camb.
TPG Warsaw
PL-AMST
ATT
nbgisp
IRL Berk.
NEC Labs
HP Labs
atcorp
IRL Seatt.
IRL Camb.
TPG Warsaw
PL-AMST
ATT
nbgisp
IRL Berk.
NEC Labs
HP Labs
IRL. Pitt.
atcorp
IRL Camb.
TPG Warsaw
PL-AMST
ATT
nbgisp
IRL Berk.
NEC Labs
HP Labs
IRL. Pitt.
atcorp
IRL Seatt.
TPG Warsaw
PL-AMST
ATT
nbgisp
IRL Berk.
NEC Labs
HP Labs
IRL. Pitt.
IRL Seatt.
IRL Camb.
PL-AMST
IRL Pitt.
IRL Camb.
CP1
I/0.872
I/0.268
A/0.017
I/0.790
A/0.751
I/0.193
I/0.164
A/0.198
A/0.834
A/0.206
A/0.027
I/0.008
I/0.085
I/0.026
I/0.195
I/0.017
A/0.009
I/0.317
I/0.097
I/0.003
I/0.233
I/0.098
I/0.494
I/0.065
A/0.383
I/0.260
I/0.063
I/0.672
I/0.531
A/0.668
A/0.511
A/0.453
I/0.934
I/0.953
A/0.410
A/0.523
I/0.738
A/0.495
A/0.914
A/0.710
I/0.013
I/0.128
I/0.010
I/0.710
A/0.132
I/0.442
A/0.051
I/0.092
I/0.746
I/0.022
I/0.012
I/0.067
I/0.257
A/0.011
I/0.032
A/0.045
I/0.752
I/0.022
I/0.068
A/0.016
I/0.005
I/0.006
I/0.020
I/0.174
I/0.051
I/0.198
I/0.045
A/0.359
A/0.143
A/0.006
A/0.043
I/0.539
I/0.367
I/0.554
I/0.088
I/0.148
I/0.172
I/0.645
A/0.714
A/0.666
I/0.005
A/0.005
I/0.034
I/0.035
I/0.013
I/0.233
A/0.014
A/0.011
A/0.002
A/0.866
I/0.088
CP2
A/0.599
A/0.034
I/0.009
I/0.296
I/0.026
A/0.148
A/0.043
A/0.119
I/0.099
A/0.049
I/0.024
I/0.004
A/0.005
I/0.011
A/0.025
I/0.017
A/0.005
I/0.173
I/0.053
A/0.002
I/0.221