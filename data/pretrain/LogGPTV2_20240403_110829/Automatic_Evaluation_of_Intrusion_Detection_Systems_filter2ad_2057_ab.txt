parallel  on  different  physical  machines  and  a  coordinator 
controls each of them. 
3.  Current  Attack  Script  Setup.  Once  the  virtual 
network  is  ready,  the  coordinator  provides  the  attacking 
virtual  machine  with  the  proper  attack  configuration.  To 
communicate  with  the  attacker,  the  coordinator  (i.e.,  the 
physical  machine)  uses  a  hard  drive  shared  via  VMware. 
This  shared  drive  is  the  only  way  the  coordinator  can 
communicate  with  the  virtual  network.  This  enables  us  to 
isolate  the  virtual  network  (from  a  networking  point  of 
view) while keeping some communication capabilities. 
4.  Attack  Execution.  The  attack  machine  performs  the 
attacks  while  generated  traffic  is  recorded.  The  attack 
system  is  composed  of  four  layers  (Figure  2):  the  Control 
Layer is composed of a Java module called ExploitRunner 
that  controls  and  executes  the  attacks  based  on  the 
configuration  provided  by  the  coordinator;  the  VEP  are 
executed  at  the  Attack  Layer  and  then  provided  to  the 
Proceedings of the 22nd Annual Computer Security Applications Conference (ACSAC'06)0-7695-2716-7/06 $20.00  © 2006Mutation  Layer  where  evasion  techniques  can  be  applied; 
and  the  traffic  generated  by  the  attacks  is  captured  (using 
tcpdump) and documented by the Recording Layer. 
4
Attacker
Target
Virtual 
Machine 
Templates
2
Network infrastructure
3, 5
VEP 
Descriptions and 
Target System 
Configurations
1
Attack Scripts
Attack Scripts
Figure 1. Virtual network infrastructure 
5.  Tear  Down.  This  includes  saving  the  traffic  traces 
(VEP  output  and  the  recorded  traffic)  on  the  same  shared 
drive used in step 3. Then, the coordinator stores the attack 
trace  in  the  data  set  and  restores  the  attacker  and  target 
virtual  machines  to  their  initial  state  to  avoid  side  effects 
(e.g., impact on the next attack). 
ExploitRunner
ExploitRunner
Metasploit
Metasploit
Nessus
Nessus
SecurityFocus
SecurityFocus
…
…
Fragroute
Fragroute
Whisker
Whisker
None
None
Ethereal
Ethereal
Figure 2. Attack system 
Control
Control
Layer
Layer
Attack
Attack
Layer
Layer
Mutation
Mutation
Layer
Layer
Recording
Recording
Layer
Layer
3.3  Documenting Traffic Traces 
In our data set, each traffic trace is documented by four 
characteristics:  the  target  system  configuration,  the  VEP 
configuration,  whether  or  not  the  target  system  has  the 
vulnerability exploited by that program and the success of 
the attack: see an example in Figure 3. 
System Configuration 
IP: 10.92.39.14 
Name: VMWin2000ServerTarget 
Operating System: Windows 2000 Server 
Ports: 
21/tcp Microsoft IIS FTP Server 5.0 
25/tcp Microsoft IIS SMTP Service 5.0 
80/tcp Microsoft IIS Web Server 5.0 
Vulnerability Exploitation Program Configuration 
name: jill.c 
reference: Bugtraq,2674 
command: jill 10.92.39.14 80 10.92.39.3 30 
Vulnerable: yes 
Success: yes 
Figure 3. Traffic trace label example 
The target system configuration description includes the 
list  of  installed  software  (e.g.  the  operating  system,  the 
different  daemons  and  their  versions),  as  well  as  its  IP 
configuration.  The  VEP  configuration  defines  the  options 
used  to  launch  the  attack.  The  vulnerability  of  the  target 
system  is  decided  automatically  on  the  basis  of:  (1)  its 
configuration;  (2)  the  VEP  being  used  in  this  attack;  and 
(3) the  vulnerability  information  available in the Security-
Focus database [34]. We will see in Section 4 that this last 
piece  of  information  is  paramount  as  it  allows  automated 
IDS  testing.  As  mentioned  in  the  previous  section,  an 
analysis  determines  whether 
the  attacks  have  been 
successful  or  not  and  automatically  classifies  the  attack 
outputs.  The  traffic  traces  are  labeled  according  to  three 
categories:  one  for  those  that  succeed  in  exploiting  the 
vulnerability  (Yes);  one  for  those  that  fail  (No);  and  one 
for those for which we were not able to determine whether 
they  were  successful  (Unclassified).  This  classification  is 
automatically  made  by  looking  at  the  program  outputs 
(hacker  point  of  view)  and  at  the  effect  on  the  targeted 
system (victim point of view). 
3.4  Data Set Summary 
The current version of our data set was only developed 
to  test  and  evaluate  network-based  and  signature-based 
intrusion detection systems for attack scenario recognition. 
It is composed of two different data sets: the standard IDS 
evaluation  data  set  (StdSet)  and  the  IDS  evasion  data  set 
(EvaSet).  The  former  contains  traffic  traces  of  attack 
scenarios derived from the  execution of well-known VEP. 
Our VEP are mainly taken from the ones available in [34, 
35, 36, 37]. The latter contains the IDS evasion techniques 
applied to  the successful attacks  contained in the standard 
data  set.  This  data  set  is  used  to  verify  whether  IDS  are 
able to detect modified attacks. 
and 
108 
target 
different 
The  two  sets  are  collections  of  tcpdump  traffic  traces, 
each  one  containing  one  attack  scenario.  To  generate 
StdSet,  we  used  124  VEP  (covering  a  total  of  92 
vulnerabilities) 
system 
configurations. Each VEP was launched against vulnerable 
and  non-vulnerable  systems  (among the 108  target system 
configurations)  that  offered  a  service  on  the  port  targeted 
by  the  VEP.  Every  combination  (VEP  +  configuration  + 
target)  produces  a  traffic  trace  in  the  set.  This  resulted  in 
10446 traffic traces in StdSet of which 420 succeeded and 
10026 failed. EvaSet contains 3549 attack traces generated 
by  applying  IDS  evasion  techniques  to  the  successful 
attacks  from  StdSet.  Figure  4  shows  the  VEP  and  the 
corresponding  attack  scenarios 
in  StdSet,  classified 
according  to  the  port  number  they  target.  For  example,  in 
StdSet,  45  VEP  target  TCP  port  number  80  and  this 
corresponds to 2801 attack scenarios. Note that the number 
of VEP does not reflect directly the number of attacks on a 
particular  port  since  each  VEP  is  used  multiple  times (for 
each  possible  configuration  and  against  each  possible 
target system). 
These numbers are much higher than has been reported 
in  literature:  [22],  [20],  and  [21]  used  respectively  9,  66 
Proceedings of the 22nd Annual Computer Security Applications Conference (ACSAC'06)0-7695-2716-7/06 $20.00  © 2006and  27  VEP  against  only  3  different  target  systems  based 
on [4]; [23], [13] and [24] used respectively 50, 10 and 60 
VEP  ([23]  and  [13]  only  used  9  and  5  different  target 
systems).  
Figure 4. Standard data set port distribution 
4  IDS Evaluation Framework Overview 
In 
this  section,  we  first  describe 
the  evaluation 
framework  and  the  corresponding  data  collection  process, 
and then discuss our result classification. 
4.1  Collection Process 
in  Java, 
that  consists  of 
To  demonstrate  that  our  documented  data  sets  can  be 
useful  to  automatically  evaluate  signature-based  network 
IDS  and  can  provide  interesting  analysis  even  with  only 
well-known  VEP,  we  developed  an  IDS  evaluation 
framework,  written 
four 
components (Figure 5). First, the IDSEvaluator takes each 
test  case  (documented  traffic  trace)  from  the  data  set  and 
provides it to each tested IDS. The alarms generated by the 
IDS are collected and provided to the IDSResultsAnalyzer, 
which is responsible for automatically analyzing them. The 
IDSResultsAnalyzer  relies  on  the  vulnerability  reference 
information  (SecurityFocus,  CVE,  etc.)  provided  by  IDS 
alarms  to  determine  whether  the  IDS  had  detected  the 
attack  scenario.  Because  each  VEP  used  in  the  data set is 
related  to  a  specific  documented  vulnerability  (e.g., 
SecurityFocus),  the  IDS  alarms  can  be  associated  with  a 
particular VEP. In other words, with the information from 
our  documented  data  set  and  the  vulnerability  reference 
information,  our  IDS  evaluation  framework  is  able  to 
automatically verify the detection or not of each attack. 
4
IDS Result Analyzer
Data
Set
1
2
IDS
3
IDS Evaluator
5
Report
Figure 5. IDS evaluation framework 
4.2  IDS Alarms Analysis Process 
After analyzing the IDS alarms, the IDSResultAnalyzer 
automatically  classifies  the  results  using  two  parameters: 
the  actual  success  (Success)  or  failure  (¬Success)  of  the 
attack and the detection (Detection) or not (¬Detection) of 
this  particular  attack  by  the  IDS.  Recall  that  the  attack 
success can be determined from our documented data set. 
A  proper  classification  should  refine  the  notion  of 
detection  and  consider  at  least  four  classes  of  detection: 
(Attempt), (Successful), (Failed) and (Silent). The Attempt 
detection class specifies when an attack attempt is detected 
or  when  the  IDS  knows  nothing  about  the  probability  of 
the  success  of  the  attack.  The  Successful  (resp.  Failed) 
detection class specifies when an attack occurs and the IDS 
has  some  evidences  that  it  may  succeed  (resp.  Fail).  The 
Silent  detection  class  represents  the  absence  of  messages 
when  nothing  suspicious  is  happening.  Unfortunately, 
Snort 2.3.2 and Bro 0.9a9 do not provide detailed enough 
messages  to  allow  us  to  use  this  refined  classification. 
They  do  not  provide  alarms  when  a  failed  attempt  is 
detected  and  they  do  not  distinguish  properly  between 
attempts and successful attempts. Therefore, in the current 
analysis  the  (Detection)  and  (¬Detection)  messages  are 
used.  This  provides  four  possible  results  for  each  traffic 
trace analyzed by the IDS: 
True Positive (TP): when the attack succeeded and the 
IDS was able to detect it (Success ∧ Detection) 
True  Negative  (TN):  when  the  attack  failed  and  the 
IDS did not report on it (¬Success ∧ ¬Detection) 
False Positive (FP): when the attack failed and the IDS 
reported on it (¬Success ∧ Detection) 
False  Negative  (FN):  when  the  attack  succeeded  and 
the IDS was not able to detect it (Success ∧ ¬Detection) 
The  above  classification  only  provides  fine-grained 
information,  specifically  information  at  the  level  of  the 
attack scenario. Each of the four classifications needs to be 
incorporated together to form a more precise analysis. It is 
essential  to  know  how  many  TPs,  TNs,  FPs,  and  FNs  we 
have  for  a  given  group  of  attack  scenarios.  By  using  this 
view, we are able to analyze if the IDS is able to properly 
distinguish  between  failed  and  successful  attack  attempts 
for a group of attack scenarios. Therefore, we combine the 
four  classes  above  and  suggest  a  classification  of  fifteen 
classes presented in Table 1. 
In this classification, an IDS is said to be Alarmist for a 
group of attack scenarios when the IDS emits an alarm on 
all  failed  attack  scenarios  in  this  group:  TN=No  and 
FP=Yes.  An  IDS  is  said  to  be  Quiet  for  a  given  group  of 
attack  scenarios  when  the  IDS  does  not  report  on  any 
failed  attack  scenario  in  this  group:  TN=Yes  and  FP=No. 
If the IDS emits an alarm for only some of the failed attack 
scenarios on a group, then we say that the IDS is Partially 
Alarmist  for  this  group:  TN=  Yes  and  FP=Yes.  Another 
dimension is used in Table 1. There is Complete Detection 
for  a  group  of  attack  scenarios  by  an  IDS  when  all 
successful  attacks  scenarios  are  correctly  detected  by  an 
IDS:  TP=Yes  and  FN=No.  On  the  other  hand,  there  is 
Complete Evasion when none of the successful attacks are 
detected by the IDS: TP=No  and  FN=Yes. Otherwise,  the 
group is Partial Evasion: TP=Yes and FN=Yes. 
Proceedings of the 22nd Annual Computer Security Applications Conference (ACSAC'06)0-7695-2716-7/06 $20.00  © 2006The  fifteen  classes  are  further  grouped  into  three  sub-
groups;  one  when  the  analysis  contains  successful  and 
failed  attack  scenarios;  one  when  there  are  only  failed 
attack  scenarios;  and  one  when  there  are  only  successful 
attack scenarios. 
TP  TN  FP  FN 
Class Name 
Success and Failed Attempts 
Quiet and Complete Detection 
Yes  Yes  No  No 
Partially Alarmist and Complete Detection  Yes  Yes  Yes  No 
Yes  No  Yes  No 
Alarmist and Complete Detection 
Quiet and Partial Evasion 
Yes  Yes  No  Yes 
Yes  Yes  Yes  Yes 
Partially Alarmist and Partial Evasion 
Yes  No  Yes  Yes 
Alarmist and Partial Evasion 
No  Yes  Yes  Yes 
Partially Alarmist and Complete Evasion 
No  No  Yes  Yes 
Alarmist and Complete Evasion 
Quiet and Complete Evasion 
No  Yes  No  Yes 
Failed Attempts Only 
Alarmist 
Partially Alarmist 
Quiet  
Successful Attempts Only 
Complete Detection 
Partial Evasion 
Complete Evasion 
Yes  No  No  No 
Yes  No  No  Yes 
No  No  No  Yes 
No  No  Yes  No 
No  Yes  Yes  No 