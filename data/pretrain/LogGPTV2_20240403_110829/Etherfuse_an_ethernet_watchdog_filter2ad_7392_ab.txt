Figure 2 shows an example of how forwarding tables can get pol-
luted. Figure 2(a) shows the forwarding path, B5-B3-B2-B4-B6,
between end hosts H1 and H2 in the absence of failure. The death
of the root bridge, B1, can lead to a temporary forwarding loop
among B2, B3 and B4 as explained in Section 2.2.3. Figure 2(b)
shows how the forwarding table of bridge B5 can get polluted in the
presence of a forwarding loop among B2, B3 and B4. Initially B5
believes H1 is connected to port P2. However after the forwarding
loop is formed, a packet from H1 can reach B3 then spin around
the loop to reach B3 again, which can send a copy back to B51.
Thus, bridge B5 receives a packet with source address H1 via port
P1 and believes that port P1 should be the output port for H1. Once
this mistake is made by B5, there is no way for H2’s data packets
to reach H1 even after the temporary forwarding loop has ended
because those packets will be dropped by B5 as they arrive on port
P1. This problem will only get ﬁxed when the incorrect forwarding
table entry at B5 times out, or when H1 transmits a data packet.
3. THE DESIGN OF THE ETHERFUSE
The EtherFuse is a device that can be inserted into the physical
cycles in the network to improve Ethernet’s reliability. It has two
ports and is analogous to an electric circuit fuse. If it detects the
formation of a forwarding loop, it breaks the loop by logically dis-
connecting a link on this loop. The EtherFuse can also help mitigate
the effects of the count to inﬁnity in RSTP and MSTP.
3.1 Detecting Count to Inﬁnity
Count to inﬁnity occurs around physical loops in Ethernet. The
way that the EtherFuse detects a count to inﬁnity is by intercepting
all BPDUs ﬂowing through it and checking if there are 3 BPDUs
announcing an increasing cost to the same root R. The EtherFuse
maintains a counter that is incremented every time the EtherFuse
receives a BPDU with increasing cost to the same root. The counter
1 The reason B3 may send a copy to B5 is either because this is a
packet with a broadcast destination, or B3 does not have an entry
for the destination of the packet in its forwarding table and thus it
falls back to ﬂooding the packet on all its ports. B3 may not have
an entry for the packet’s destination in its forwarding table either
because this is the ﬁrst time it hears of this destination, or because
its forwarding table entry has timed out or has been ﬂushed due to
the reception of a BPDU instructing it to do so.
Receive BPDU 
on port p
BCache[p].BPDU.root
== BPDU.root
Yes
BCache[p].cost
= 3
No
Yes
BCache[p].count = 1;
BCache[p].cost = BPDU.cost;
Yes
BPDU.messageAge = 
BPDU.maxAge;
Transmit BPDU on 
the other port;
Figure 3: Flow chart of how the EtherFuse detects and mitigates a count to
inﬁnity.
is reset to one if the EtherFuse receives two consecutive identical
BPDUs. If this counter reaches the value of 3, it signals that a count
to inﬁnity is taking place. This means that there is stale information
about R that is circling around the loop and will keep doing so until
it is aged out. The reason for checking for 3 consecutive BPDUs
announcing increasing costs is that BPDUs are sent out if the bridge
has new information to announce, or periodically every hello time,
which is typically 2 seconds. Thus, it is unlikely that a path cost to
the root will increase twice during two consecutive hello times, due
to any reason other than a count to inﬁnity. In the unlikely event that
the there was no count to inﬁnity but the network was reconﬁgured
twice during two consecutive hello times, the BPDU following the
two BPDUs with increasing costs will announce the same cost as
the preceding one. Thus, the EtherFuse will realize that no count
to inﬁnity is taking place and it will not take any further action,
leaving the network to resume its normal operation.
The EtherFuse does the BPDU monitoring independently for
each of its 2 ports. It uses a BPDU cache (BCache) that maintains
the state of BPDUs it has received at each port. Figure 3 shows a
ﬂow chart explaining how the EtherFuse detects a count to inﬁn-
ity. Since fresh information can chase stale information around the
loop announcing two different roots during a count to inﬁnity, the
cache has two entries per port to record both the fresh and the stale
information. Only two entries are used in the cache because during
the count to inﬁnity there can be BPDUs announcing at most two
different roots [11]. Both the fresh and the stale information are
cached because the EtherFuse can not distinguish between them.
Thus, it monitors both copies in the cache checking if either of
them exhibit two consecutive increases in cost. The details about
maintaining two cache entries per port in the BCache are omitted
from Figure 3 for simplicity.
3.2 Detecting Forwarding Loops
The key idea for detecting forwarding loops in Ethernet is by
detecting packets that are circling around the loop. The EtherFuse
takes a hybrid approach of passively monitoring trafﬁc in the net-
work to infer the existence of a forwarding loop, and actively prob-
ing the network to verify the loop’s existence. Passive monitoring
is preferred as it does not introduce extra network trafﬁc. More-
over, because passive forwarding loop detection takes advantage of
the data packets ﬂowing through the network, it is likely to be faster
than any practical method based on periodic active probing.
To monitor the network for forwarding loops, EtherFuse checks
for duplicate packets. This is because if there is a forwarding loop,
a packet may spin around the loop and arrive again at the EtherFuse.
The EtherFuse checks for duplicates by keeping a history of the
Receive 
Ethernet Frame
Yes
Probe?
From me?
Yes
My ID is the 
smallest in the 
Fuse list?
Yes
Loop detected!
Send
Topology Change 
BPDUs;
Cut link;
No
Yes
Frame.Hash
in duplicate detector 
& entry
is fresh?
No
Insert Frame
timestamp in 
duplicate detector
No
No
Me on 
Fuse list?
Yes
Drop
Build and send probe
No
Insert in Fuse list
Transmit Frame on 
the other port
Figure 4: Flow chart of how the EtherFuse detects and stops forwarding loops.
hashes of the packets it received recently. Every new incoming
packet’s hash is checked against this history.
If a fresh copy of
the packet’s hash is found, then the packet is a duplicate signaling
a potential forwarding loop. A hash in the history is fresh if its
timestamp is less than the current time by no more than a threshold.
This threshold should be no less than the maximum network round
trip time. Otherwise, a packet’s hash may expire before the packet
completes a cycle around the loop. If no fresh copy of the received
packet’s hash is found in the history, the hash is recorded in the
history along with its timestamp. As an optimization, the Ethernet
frame’s Cyclic Redundancy Check (CRC) can be used as the hash
of the packet’s contents.
By itself, this forwarding loop detection technique may have
false positives due to collisions between hashes of different pack-
ets or a malicious end host intentionally injecting duplicate packets
into the network to trick the EtherFuse into thinking that there is a
forwarding loop. To avoid false positives in such cases, the Ether-
Fuse uses explicit probing once it suspects the existence of a for-
warding loop. These probes are sent as Ethernet broadcast frames
to guarantee that if there is a loop they will go around it and not be
affected by forwarding tables at the Ethernet switches. The source
address of the probe is the EtherFuse’s MAC address. If the Ether-
Fuse receives a probe it has sent then this implies that there is a
forwarding loop. However, the probe may get dropped even in the
presence of a forwarding loop. In this case, the fuse will receive
more duplicate packets forcing it to send more probes until one of
those probes will make its way around the loop and back to the
EtherFuse again. Duplicate packets in the network can lead to con-
gestion, increasing the chance of probes getting dropped. Hence,
EtherFuse drops all duplicate packets it detects. Figure 4 presents
a ﬂow chart of how loops are detected.
3.2.1 Building the Duplicate Detector
The EtherFuse’s duplicate detector maintains the history of re-
ceived packets in a hash table. However, it is desirable for the du-
plicate detector’s hash table not to use chaining in order to simplify
the implementation of the EtherFuse in hardware. In the follow-
ing discussion, we assume that the range of the hash function that
is applied to received packets is much larger than the size of the
hash table. For example, the Ethernet packet’s 32-bit CRC might
be used as the hash code representing the packet. However, a hash
table with 232 entries would be impractical due to its cost. In such
cases, a simple mapping function, such as mod the table size, is
applied to the hash code to produce an index in the table. Since
packets are represented by a hash code, the duplicate detector can
report false positives. However, it is acceptable to have collisions
with low probability since the EtherFuse will send a probe to verify
that a forwarding loop exists.
There are two design alternatives to construct a duplicate detec-
tor with less entries than the hash function’s range. The ﬁrst alter-
native is to only store the timestamp in the hash table entry. In this
case, two packets having different hash values may be mistaken as
duplicates. This is because their two distinct hashes may map into
the same location in the table. For this design alternative, false pos-
itives occur when detecting duplicate packets if two different pack-
ets with identical or different hashes map into the same location of
the table. Assuming a uniform hash function, an upper bound for
the probability of false positives occurring for a particular packet
is given by Equation 1, where N is the number of entries in the
duplicate detector, T is the time the packet’s entry is kept in the du-
plicate detector before it expires, B is the network bandwidth, and
F is the Ethernet’s minimum frame size. Equation 1 computes the
complement of the probability that the packet’s entry in the hash
table does not experience any collisions during its valid lifetime.
«((cid:2) T ×B
F
„
N − 1
N
P r = 1 −
(cid:3))
(1)
The second design alternative to constructing the hash table is
to include the packet’s hash value in every entry along with the
timestamp. In this case, two packets can be mistaken as duplicates
only if they share the same hash value. An upper bound for the
probability of false positives detecting duplicates for a particular
packet is given by Equation 2, where K is the number of bits in
the packet’s hash. Similar to Equation 1, Equation 2 computes the
complement of the probability that the packet’s entry in the hash
table does not experience any collisions during its valid lifetime.
“
”((cid:2) T×B
F
P r = 1 −
1 − 2−K
(cid:3))
(2)
However using this approach, EtherFuse can miss some dupli-
cates. For example, if there exists a forwarding loop and a packet
P1 arrives at the EtherFuse, its hash will be recorded. Then by
the time P1 spins around the loop and before it arrives again at
the EtherFuse, another packet, P2, arrives ﬁrst at the EtherFuse.
If P1 and P2 have different hashes that hash into the same loca-
tion in the hash table, the duplicate detector entry storing P1’s hash
is replaced by P2’s hash. Since the duplicate detector records the
packet’s hash it will detect that P2 is different than P1 and not a
duplicate. Later, when P1 arrives again at the EtherFuse, its hash
will replace P2’s hash in the duplicate detector without detecting
a duplicate. Consequently, the EtherFuse will not detect that there
is a loop. However, the probability of such a false negative is very
low. An upper bound to this probability is given by Equation 3,
where L is the latency around the loop. Equation 3 computes the
probability that (1) packet P1’s hash gets replaced by one or more
other packets’ hashes before P1 arrives again at the EtherFuse after
cycling around the loop, and (2) the last packet of those which re-
placed P1’s hash entry in the duplicate detector has a different hash
than that of P1.
„
P r =
1 −
N − 1
N
«((cid:2) L×B
F
(cid:3))
!
“
×
1 − 2−K
(3)
”
Figure 5 plots the probabilities in Equations 1, 2 and 3 with
conservative values of the equations’ parameters. The parameters
were set as follows: T = 100ms, F = 64B, B = 10Gb/s, K = 32 and
L = 10ms, where T is set to an order of magnitude more than L
as a safety margin to minimize the chance of missing duplicates.
In summary, the trade-offs between the two design alternatives are
the following: Not including the packets’ hashes in the hash ta-
ble prevents false negatives when detecting duplicates. Thus, a
y
t
i
l
i
b
a
b
o
r
P
 1
 0.1
 0.01
 0.001
 1e-04
Eq1
Eq2
Eq3
1M
16M
32M
64M
128M
Number of entries in duplicate detector
Figure 5: Plot of equations 1,2 and 3 illustrating the probability of false posi-
tives and negatives for the two design alternatives of the duplicate detector. Equa-
tion parameters were set as follows: T = 100ms, F = 64B, B = 10Gb/s, K = 32, and
L = 10ms
forwarding loop is more likely to be detected as soon as the ﬁrst
duplicate is received. The down side of this alternative is that it
suffers from a higher false positive rate when detecting duplicates.
This leads to more non-duplicate packets getting dropped than in
the second design alternative. However, for a duplicate detector
with a sufﬁciently large number of entries, the false positives rate
can be very low. For the second design alternative that includes the
hash in every duplicate detector entry, it achieves a lower rate of
false positives when detecting duplicates. However, this comes at a
cost. First, false negatives occur when detecting duplicates. Thus,
forwarding loop detection may get slightly delayed if a duplicate
arrives at the EtherFuse but is not detected. Second, more memory
is needed to store the hashes in the duplicate detector. Third, more
per-packet computation is performed by the EtherFuse, speciﬁcally
to compare the packet’s hash to the corresponding hash in the hash
table entry.
3.3 Mitigating Count to Inﬁnity and Forward-
ing Loops
After detecting a count to inﬁnity or a forwarding loop, the sec-
ond phase is mitigating the problem and its effects.
For the count to inﬁnity, if the EtherFuse detects BPDUs an-
nouncing increasing costs to a root R, it expedites the termination
of the count to inﬁnity by altering the message age ﬁeld of any BP-
DUs announcing R to be the root. Speciﬁcally, it sets their message
age ﬁeld to MaxAge. However, this may not instantaneously termi-
nate the count to inﬁnity as Ethernet bridges may be caching other
copies of the stale information.
If there are other cached copies
of the stale information, they will eventually reach the EtherFuse
again, which in turn will increase their message age until eventu-
ally the count to inﬁnity is terminated. Figure 3 shows how the
EtherFuse handles a count to inﬁnity. For handling the count to
inﬁnity, having more than one EtherFuse in a loop in the physical
topology is not a problem as every EtherFuse can handle the count
to inﬁnity independently without side effects.
On the other hand, having more than one EtherFuse in the same
loop in the event of a forwarding loop is problematic. Only one
of those EtherFuses should cut the loop otherwise a network parti-
tion will occur. To handle this, EtherFuses collaborate to elect an
EtherFuse that is responsible for breaking the loop. To do this, a
probe carries the identities of the EtherFuses it encounters during
its trip around the loop, that is, whenever an EtherFuse receives a
probe originated by another EtherFuse, it adds its identiﬁer to a list
of EtherFuse identiﬁers in the probe.The EtherFuse’s MAC address
is used as its identiﬁer. Also, the EtherFuse checks for its identi-
ﬁer in the list of identiﬁers in the probe. If it ﬁnds its own, then
this probe has been through a loop. The EtherFuse drops such a
probe as it is not the probe’s originator. If the EtherFuse receives
its own probe, it checks the list of EtherFuse identiﬁers attached
to the probe. It drops the probe if its identiﬁer is not the smallest
in the probe’s list of EtherFuse identiﬁers. On the other hand, if its
identiﬁer is the smallest in the list, the EtherFuse is elected to break
the loop. It cuts the loop by blocking one of its ports that connects
the loop. This way the network can continue operating normally
even in the presence of a forwarding loop. However since phys-
ical loops exist in the network for redundancy and fault tolerance
reasons, cutting them leaves the network vulnerable to partitioning
due to future failures. So the EtherFuse tries to restore the network
to its original topological state by unblocking its blocked port after
a timeout period has passed. It does this hoping that the loop was a
temporary loop formed due to ephemeral conditions. If the Ether-
Fuse detects a loop again right after it tries to restore the network,
then it knows that the loop still persists so it cuts the loop again. It
retries this until it eventually gives up assuming this is a permanent
loop. It then notiﬁes the network administrator to take appropri-
ate measures to ﬁx the problem. Figure 4 shows how an EtherFuse
handles a forwarding loop.
Since a forwarding loop may persist for a small duration until it
is detected and corrected, forwarding table pollution may still oc-
cur. To speed recovery from forwarding table pollution, the Ether-
Fuse sends BPDUs on both its ports with the topology change ﬂag
set. This will make bridges receiving this topology change informa-
tion ﬂush their forwarding tables and forward this topology change
message to their neighbor bridges until it has spread throughout the
network. This technique has its limitations though. This is because
the IEEE 802.1D (2004) speciﬁcation suggests an optimized tech-
nique for ﬂushing entries from the bridge’s forwarding table. This
technique ﬂushes entries for all the ports other than the one that re-
ceives the topology change message on the bridge. This technique
is not mandatory but if it is implemented, there will be some cases
in which the EtherFuse will not be able to eliminate the forward-
ing table pollution if the loop was not shutdown before pollution
occurs. For example, the pollution shown in Figure 2(b) at port P1
cannot be ﬁxed by an EtherFuse sitting along the loop B2-B3-B4.
This is because B5 will receive the topology change messages at
P1, the port with polluted forwarding table entries. Consequently,
it will ﬂush forwarding entries for port P2 and not P1. However,
even if B5 does not ﬂush the entries at P1, the polluted entries
will be invalidated as soon as the end host H1 sends any packets
or when those polluted forwarding table entries expire by reaching