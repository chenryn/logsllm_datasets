acter. An example session would be something like:
USER anonymous
PASS PI:EMAIL
CWD ~AAAAAAAAAAAAAAAAAAA...
Looking at the code coverage from another fuzzer, say GPF, shows what happened:
GPF never used a path that began with the tilde. The mutation-based (and in this
case commercial) seemed to have better heuristics on what types of anomalies to
add to the data.
8.7.2 SNMp
As in the FTP fuzzing comparison, there were a few noteworthy bugs in the SNMP
testing as well.
• Bugs 2 and 3 were found by GPF and the Mu-4000, but missed by PROTOS,
beSTORM, and Codenomicon Defensics.
• Bug 4 is the opposite and was found by PROTOS, beSTORM, and Code-
nomicon Defensics, but missed by all the other fuzzers.
6760 Book.indb 265 12/22/17 10:50 AM
266 Fuzzer Comparison
Both bugs 2 and 3 have to do with the secName variable:
/*
* Locate the User record.
* If the user/engine ID is unknown, report this as an error.
*/
if ((user = usm_get_user_from_list(secEngineID, *secEngineIDLen,
secName, userList,
(((sess && sess->isAuthoritative ==
SNMP_SESS_AUTHORITATIVE) ||
(!sess)) ? 0 : 1)))
== NULL) {
DEBUGMSGTL((“usm”, “Unknown User(%s)\n”, secName));
if (snmp_increment_statistic(STAT_USMSTATSUNKNOWNUSERNAMES)
== 0) {
DEBUGMSGTL((“usm”, “%s\n”, “Failed to increment
statistic.”));
}
do
{
char tempbuf[32];
memset(tempbuf,0,32);
strcat(tempbuf,”Unknown User: );
if (strlen(tempbuf) + strlen(secName) > 31)
{
BUGREPORT(2);
}
strcat(tempbuf,secName);
if (strstr(secName, “%”))
{
BUGREPORT(3); /* Format string */
}
...
snmp_log(LOG_WARNING, tempbuf);
Note that the vulnerable lines are only activated if an unknown username is used.
There is a different reason each fuzzer missed this bug. beSTORM never even calls
this function, since it doesn’t support SNMP version 3, as the following snippet
from an lcov coverage report confirms:
Codenomicon Defensics never gets to this line, it always used an unknown “engine id,”
6760 Book.indb 266 12/22/17 10:50 AM
8.7 A Closer Look at the Results 267
Meanwhile the Mu-4000 never sent an invalid username (at this point in the pro-
gram). Again, this is an example of a fuzzer not being random enough.
Bug 4 had the opposite behavior. Here is the vulnerability:
int
snmp_pdu_parse(netsnmp_pdu *pdu, u_char * data, size_t * length)
{
...
/*
* get header for variable-bindings sequence
*/
DEBUGDUMPSECTION(“recv”, “VarBindList”);
data = asn_parse_sequence(data, length, &type,
6760 Book.indb 267 12/22/17 10:50 AM
268 Fuzzer Comparison
(ASN_SEQUENCE | ASN_CONSTRUCTOR),
“varbinds”);
if (data == NULL)
return -1;
/*
* get each varBind sequence
*/
while ((int) *length > 0) {
...
switch ((short) vp->type) {
...
case ASN_OCTET_STR:
case ASN_IPADDRESS:
case ASN_OPAQUE:
case ASN_NSAP:
if (vp->val_len buf)) {
vp->val.string = (u_char *) vp->buf;
} else {
vp->val.string = (u_char *) malloc(200);
if (vp->val_len > 200)
{
BUGREPORT(4);
}
}
...
asn_parse_string(var_val, &len, &vp->type, vp->val.
string,
&vp->val_len);
break;
Again, only a very specific action will trigger this bug. GPF executed this func-
tion, but didn’t get deep enough in the function to even get to the switch statement.
ProxyFuzz and the Mu-4000 both got deep enough, but did not provide a long
enough string to actually make the overflow occur. Here is the code coverage from
the Mu-4000:
In this case, the code coverage report is taken on the code without the bug, but it
is still clear that vp->val_len was always smaller than sizeof(vp->buf), and this is
why the bug was never hit.
6760 Book.indb 268 12/22/17 10:50 AM
8.7 A Closer Look at the Results 269
8.7.3 DNS
As in the other comparison tests, for DNS, a few bugs stood out on differing ends
of the spectrum.
• DNS bugs 3 and 13 were only caught by Codenomicon Defensics.
• DNS bug 14 was the only bug caught by beSTORM.
First let’s look at bug 3, which proved difficult to find for most of the fuzzers. Here
is the code listing:
static isc_result_t
getsection(isc_buffer_t *source, dns_message_t *msg,
dns_decompress_t
*dctx,
dns_section_t sectionid, unsigned int options)
/*
* Read the rdata from the wire format. Interpret the
* rdata according to its actual class, even if it had a
* DynDNS meta-class in the packet (unless this is a TSIG).
* Then put the meta-class back into the finished rdata.
*/
rdata = newrdata(msg);
..
if(rdata->type == 0x06) { // SOA
char *soa=malloc(128);
if(rdata->length > 128) {
BUGREPORT(3);
}
memcpy(soa, rdata->data, rdata->length);
free(soa);
}
This bug is only activated if a long SOA type is encountered. Since the initial inputs
used for the mutation-based fuzzers did not contain such a type, it is not surprising
they did not find this bug. This is a case of having deficient or incomplete initial
inputs for a mutation-based fuzzer.
Now, let us consider bug 14. It was found by GPF, Codenomicon, and beSTORM.
In fact, it was the only bug discovered by beSTORM. It was not found by Proxy-
Fuzz, the generic fuzzer. Here is the bug:
isc_result_t
dns_message_parse(dns_message_t *msg, isc_buffer_t *source,
unsigned int options)
6760 Book.indb 269 12/22/17 10:50 AM
270 Fuzzer Comparison
isc_region_t r;
...
isc_buffer_remainingregion(source, &r);
if (r.length != 0) {
isc_log_write(dns_lctx, ISC_LOGCATEGORY_GENERAL,
DNS_LOGMODULE_MESSAGE, ISC_LOG_DEBUG(3),
“message has %u byte(s) of trailing garbage”,
r.length);
char garbage[255];
if(r.length > 255) {
BUGREPORT(16);
}
memcpy(garbage, r.base, r.length);
}
This bug occurs when a large amount of unnecessary trailing data is provided.
It’s not too surprising that most fuzzers found this as it is pretty basic and doesn’t
require a detailed knowledge of the protocol. However, ProxyFuzz got to this point
in the function, but never had any trailing garbage:
8.8 General Conclusions
We’ve seen which fuzzers performed better than others in different circumstances.
We’ve looked at exactly which bugs proved difficult to find and which were easier.
Now, let’s try to draw some general conclusions from the data.
8.8.1 The More Fuzzers, the Better
Sometimes it is not good enough to use the best fuzzer in isolation. Observe the
interesting fact that almost always, a combination of fuzzers finds more bugs than
any single fuzzer! This data is highlighted in Figure 8.4.
In fact, running all the fuzzers found, on average, over 50% more bugs than
just running the most effective fuzzer by itself. Keep this in mind when deciding
which fuzzer(s) to use.
8.8.2 Generational-Based Approach is Superior
While the fact that more fuzzers are significantly better than any one may be sur-
prising, the fact that generational-based fuzzers find more bugs than mutation-based
6760 Book.indb 270 12/22/17 10:50 AM
8.8 General Conclusions 271
Figure 8.4 Summary of percentage of bugs found and code coverage obtained by fuzzers.
fuzzers will probably not come as a big surprise. In these three tests, the best gen-
erational-based fuzzer does over 15% better than the best mutation-based fuzzer.
The exact comparison is shown in Figure 8.5.
8.8.3 Initial Test Cases Matter
Another observation that can be made from this data is that the quality of the initial
input is important. Consider the two initial packet captures used during the FTP
testing. The data from this is summarized in Figure 8.6. While we could have guessed
this was the case, we now know exactly how important the initial test cases can be.
The difference in the number of bugs found beginning from these different
inputs is clear. For GPF, 66% more bugs were found with the full packet capture.
For the other two fuzzers, no bugs were found with the partial packet capture,
while three bugs were found with the full capture. This full packet capture took
Figure 8.5 Combining fuzzers finds more bugs than just using the best one.
6760 Book.indb 271 12/22/17 10:50 AM
272 Fuzzer Comparison
Figure 8.6 Generation-based fuzzers outperform mutation-based fuzzers.
advantage of knowledge of the protocol and required some up-front work. In a sense,
using the complete packet capture blurred the distinction between mutation-based
and generational-based fuzzers. In practice, such protocol-complete initial inputs
may not be feasible to obtain.
8.8.4 protocol Knowledge helps
Also not surprising is that the amount of information about a particular protocol
that a fuzzer understands correlates strongly with the quality of the fuzzing. Figure
8.7 shows some data taken from the SNMP fuzzing tests.
ProxyFuzz does not understand the protocol at all, it merely injects random
anomalies into valid SNMP transactions. It finds the fewest bugs. The GPF generic
tokAid attempts to dissect binary protocols based on very generic techniques. It
doesn’t understand the specifics of SNMP, but does do better than the completely
random approach offered by ProxyFuzz. The GPF fuzzer with the custom-written
Figure 8.7 The quality of the initial test case for mutation-based fuzzers makes a big difference.
6760 Book.indb 272 12/22/17 10:50 AM
8.8 General Conclusions 273
SNMP tokAid does understand the SNMP protocol, at least with respect to the
packets captured and replayed by GPF. That is to say, it doesn’t understand SNMP
entirely, but does completely understand the packets it uses in its replay. This fuzzer
does better still. Finally, the two commercial generational-based fuzzers completely
understand every aspect of SNMP and get the best results. Beside the fact more
information means more bugs, we can see exactly how much more information (and
thus time and money) gives how many more bugs.
8.8.5 real Bugs
Throughout this testing, the fuzzers were doing their best to find simulated bugs
added to the applications. However, it was entirely possible by the way the tests
are designed that they could uncover real bugs in these particular applications. It
turns out one of the fuzzers actually did find a real bug in one of the applications.
The Codenomicon Defensics fuzzer found a legitimate DoS vulnerability in Net-
SNMP. This bug was reported to the developers of this project, and the bug was
fixed. No other fuzzers found this real bug. Code coverage could be used to predict
this fact as the Codenomicon fuzzer obtained significantly more code coverage of
this application than the other fuzzers.
8.8.6 Does Code Coverage predict Bug Finding
While we chose to test the fuzzers by looking at how effective they were at finding
simulated vulnerabilities, we also chose to measure them by looking at code cover-
age. One added benefit of doing this is that we now have both sets of data and can
attempt to answer the hotly debated question: Does high code coverage correlate
to finding the most bugs?
As a first approximation to answering this question, let’s look at the graphs of
code coverage versus bugs found for the three sets of data we generated (Figure 8.8).
The figures seem to indicate that there is some kind of relationship between
these two variables (which is good since they are supposed to be measuring the
same thing). With such small data sets, it is hard to draw any rigorous conclusions,
but we can still perform a simple statistical analysis based on this data. Consider
the data found for FTP. We’ll use the statistics software SYSTAT to see if there is
Figure 8.8 The more protocol information available, the more bugs found.
6760 Book.indb 273 12/22/17 10:50 AM
274 Fuzzer Comparison
a relationship between the independent variable code coverage and the dependent
variable bugs found.7 The results from the analysis follow:
Dep Var: BUGS N: 11 Multiple R: 0.716 Squared multiple R: 0.512
Adjusted squared multiple R: 0.458 Standard error of estimate: 9.468
Effect Coefficient Std Error Std Coef Tolerance t P(2 Tail)
CONSTANT -5.552 8.080 0.000 .___ -0.687 0.509
CC 0.921 0.300 0.716 1.000 3.074 0.013
Analysis of Variance
Source Sum-of-Squares df Mean-Square F-ratio P
Regression 847.043 1 847.043 9.449 0.013
Residual 806.813 9 89.646
What this means in English is that code coverage can be used to predict the
number of bugs found in this case. In fact, a 1% increase in code coverage increases
the percentage of bugs found by .92%. So, roughly speaking, every 1% of additional
code coverage equates to finding 1% more bugs. Furthermore, the regression coef-
ficient is significant at the .02 level. Without getting into the details, this means
that there is less than a 2% chance that the data would have been this way had the
hypothesis that code coverage correlates to the number of bugs found been incor-
rect. Thus, we can conclude that code coverage can be used to predict bugs found.
This statistical model explains approximately 46% of the variance, indicating that
other conditions exist that are not explained by the amount of code coverage alone.
Therefore, there is strong evidence that code coverage can be used to predict the
number of bugs a fuzzer will find but that other factors come into play as well.
8.8.7 how Long to run Fuzzers with random Elements
Generational-based fuzzers, like the commercial fuzzers tested here, are easy to run.
They already know exactly which inputs they will send and in what order. Thus, it
is pretty easy to estimate exactly how long they will run and when they will finish.
This is not the case for most mutation-based fuzzers, which contain randomness
(TAOF is an exception). Due to the fact there is randomness involved in selecting
where to place anomalies and what anomalies to use, mutation-based fuzzers could
theoretically run years and then suddenly get lucky and discover a new bug. So, the
relevant question becomes: When exactly has a fuzzer with random components run
for long enough? While we’re not in a position to answer this question directly, the
fuzzer comparison testing we’ve conducted has allowed us to collect some relevant