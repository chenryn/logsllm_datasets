priori that the two groups are of equal size (e.g., the data collection
be run using an exact value of n/2 for the local sensitivity without
the need to dedicate any privacy budget to estimating m. This
used to add noise and by increasing the privacy budget allocated to
full version for more details.
5 PAIRED DATA
We now consider a third situation, where there are two groups and
the observations in those groups are paired. While this scenario did
not exist in the original psychotic disease study, one can imagine
recording the methylation levels of one of the groups before and
after administering medication. Each subject then contributes a pair
of data (ui , vi) that are highly correlated with one another. One
Algorithm Wstat : Wilcoxon Test Statistic
Input: x
for row i of x do
di ←− |vi − ui|
si ←− Sign(vi − ui)
Order the terms from lowest to highest di
Drop any di = 0
for row i of x do
ri ←− rank of row i
w ←−
Output: w
i siri
5.2 Our Differentially Private Algorithm
At a high level, our algorithm is quite straightforward and similar
to prior work: we compute a test statistic as one might in the public
case and add Laplacian noise to make it private. However, there
are several important innovations relative to Task and Clifton that
greatly increase the power of our test.
Our first innovation is to use a different variant of the Wilcoxon
test statistic. While the version introduced in Section 5.1 is the
one most commonly used, other versions have long existed in the
statistics literature. In particular, we look at a variant introduced by
Pratt in 1959 [22]. In this variant, rather than dropping rows with
di = 0, those rows are included. When di = 0 we set si = Sign(di) =
0, so those rows contribute nothing to the resultant statistic, but
they do push up the rank of other rows.
Algorithm WPstat: Wilcoxon Test Statistic - Pratt Variant
Input: x
for row i of x do
di ←− |vi − ui|
si ←− Sign(vi − ui)
Order the terms from lowest to highest di
for row i of x do
ri ←− rank of row i
w ←−
Output: w
i siri
In the public setting, the Pratt variant is not very different from
the standard Wilcoxon, being slightly more or less powerful de-
pending on the exact effect one is trying to detect [5]. In the private
setting, however, the difference is substantial.
The benefit to the Pratt variant comes from how the test statistics
are interpreted. In the standard Wilcoxon, it is known that the
test statistic follows an approximately normal distribution, but
the variance of that distribution is a function of nr , the number
of non-zero di values. In the private setting, this number is not
known, and this has caused substantial difficulty in prior work. (See
Section 5.3 for more discussion.) On the other hand, the Pratt variant
produces a test statistic that is always compared to the same normal
distribution, which depends only on n. The algorithm(cid:103)WPstat that
Theorem 5.1. Algorithm(cid:103)WPstat
outputs a differentially private analogue is shown below.
is ϵ-differentially private.
Figure 7: Power of (cid:103)MWp andKWabsp at various n and ϵ. (Ef-
fect size: max(µn) − min(µn) = 1σ; д = 2; α = .05; equal group
sizes; normally distributed sample data)
can assess the impact of the medication by considering whether
the set of n differences, {vi − ui}i, is plausibly centered at zero.
The standard nonparametric hypothesis test for this situation is the
Wilcoxon signed-rank test, proposed in 1945 by Frank Wilcoxon
[36]. The parametric alternative is a simple one-sample t-test run
on the set of differences.
This is the one setting where we are aware of prior work on
a nonparametric test. Task and Clifton [30] gave the first private
analogue of the Wilcoxon signed-rank test, referred to from here
on as the TC test, in 2016. Our test makes two key improvements to
theirs and exhibits higher power. We also correct some errors in
their work. We discuss the differences in more detail in Section 5.3.
Despite its status as one of the most commonly used hypothesis
tests, to our knowledge there is no practical, implementable private
version of a one-sample t-test in the literature. In Section 5.4 we
discuss some work that comes close, and then we give our own first
attempt at a private t-test. We again find that our nonparametric
test has significantly higher power than this parametric alternative.
5.1 The Wilcoxon signed-rank test
The function calculating the Wilcoxon test statistic is formalized in
Algorithm W. Given a database x containing sets of pairs (ui , vi),
the test computes the difference di of each pair, drops any with
di = 0, and then ranks them by magnitude. (If magnitudes are equal
for several differences, all are given a rank equal to the average
rank for that set.)
Under the null hypothesis that ui and vi are drawn from the
same distribution, the distribution of the test statistic W can be
calculated exactly using combinatorial techniques. This becomes
computationally infeasible for large databases, but an approxima-
tion exists in the form of the normal distribution with mean 0 and
variance nr (nr +1)(2nr +1)
, where nr is the number of rows that were
not dropped. Knowing this, one can calculate the p-value for any
particular value of w.
6
(cid:17)
(cid:16) 2n
ϵ
Input: x, ϵ
w ←− WPstat(x)
See the full version for proof of Theorem 5.1.
To complete the design of our test, we compute a reference
Here we use the standard normal approximation for the distribution
of the w test statistic, though one could simulate full databases as
Algorithm (cid:103)WPstat : Private Wilcoxon Test Statistic
(cid:101)w ←− w + Lap
Output:(cid:101)w
distribution through simulation as was done inKWabsp and (cid:103)MWp.
well. We call this algorithm(cid:103)WPp.
Algorithm (cid:103)WPp : Complete Wilcoxon Test
(cid:101)w ←−(cid:103)WPstat(x, ϵ)
p ←− fraction of wk more extreme than(cid:101)w
Output:(cid:101)w, p
Theorem 5.2. Algorithm(cid:103)WPp is ϵ-differentially private.
Proof. The computation of(cid:101)w was already shown to be private.
rem 2.4, it follows that the(cid:103)WPp algorithm is also private.
The remaining computation needed to find the p-value does not
need access to the database—it is simply post-processing. By Theo-
□
wk ←− Normal(0, n(n + 1)(2n + 1)/6) + Lap(2n/ϵ);
for k = 1 to z do
Input: x, ϵ, z
5.3 Experimental Results
sampling a database x from that distribution and then running
Power analysis. We assess the power of our differentially-private
Wilcoxon signed-rank test first on synthetic data. (For tests with real
data, see the full version.) In order to measure power, we must first
fix an effect size. We chose to have the ui and vi values both gener-
ated according to normal distributions with means one standard
deviation apart. We then measure the statistical power of Algo-
rithm(cid:103)WPp (for a given choice of n and ϵ) by repeatedly randomly
(cid:103)WPp on that database.10 The power is the percentage of the time
(cid:103)WPp returns a p-value less than α. See the full version for a similar
Uniformity of p-values. In algorithm(cid:103)WPp we draw our reference
Figure 9 shows a Q-Q plot of(cid:103)WPp on three sets of p-values,
distribution samples (the wk values) assuming there are no di = 0
rows. The distribution will technically differ slightly when there
are many rows with di = 0, so we need to confirm experimentally
that the difference is inconsequential or otherwise acceptable.
analysis of power, varying effect size rather than sample size.
all generated under H0, with ϵ = 1, n = 500. When there are
no ties in the original data (0% of di = 0), the Q-Q plot line is
indistinguishable from the identity line, indicating that the test
10Our actual implementation differs slightly from this. To save time when running a
huge number of tests with identical n and ϵ, we first generate the reference distribution
Wk values, which can be reused across runs.
Figure 8: Power of(cid:103)WPp at various ϵ and n. (Effect size: µu −
µv = 1σ; α = .05; normally distributed sample data)
is properly calibrated. Encouragingly, introducing a substantial
number of ties into the data (30% of di = 0) has little noticeable
effect. In order to induce non-uniformity in the p-values, one needs
an extremely high proportion of rows with di = 0. The curve with
90% zero values is shown as an illustration. When the proportion of
zeros is very high, the variance of the p-values will be narrower than
the reference distribution, resulting in a lower critical value. Since
the value we are using is higher, our test is overly conservative,11
but this is acceptable as type I error is still below α.
Figure 9: A quantile-quantile plot of(cid:103)WPp comparing the dis-
tribution of simulated p-values to the uniform distribution
(ϵ = 1, n = 500; normally distributed sample data).
Comparison to previous work. In 2016, Task and Clifton [30] in-
troduced the first differentially private version of the Wilcoxon
signed-rank test, from here on referred to as the TC test. Our work
improves upon their test in two ways. We describe the two key
differences below, and then compare the power of our test to theirs.
We also found a significant error in their work.12 All comparisons
11One could try to estimate the number of zeros to be less conservative, but that would
require allocating some of the privacy budget towards that estimate, which is not
worth it in most circumstances.
12This error has been confirmed by Task and Clifton in personal correspondence.
are made to our implementation of the TC test with the relevant
error corrected.
value t∗. For a given n and ϵ, the private test statistic(cid:101)w under H0 is
Task and Clifton compute an analytic upper bound on the critical
sampled according to a sum W +Λ, where W is a random draw from
a normal distribution (scaled according to n) and Λ is a Laplace
random variable (scaled according to n and ϵ). In particular, say
that b is a value such that Pr[W > b]  д]  b + д]  b or Λ > д]
= Pr[W > b] + Pr[Λ > д]
− Pr[W > b and Λ > д]
= β + γ − βγ
Task and Clifton always set γ = .01 and then vary the choice of
β such that they have α = β + γ − βγ for whatever α is intended as
the significance threshold.13
The bound described above is correct but very loose, and our
simulation method gives drastically lower critical values. Table 1
contains examples of the critical values achieved by each method
for several parameter choices. More values can be found in the full
version, where we also experimentally confirm that these values
result in acceptable type 1 error.
Table 1: Critical Value Comparison for n = 100
ϵ
1
0.1
0.01
α
0.1
0.05
0.025
0.1
0.05
0.025
0.1
0.05
0.025
Public
1.282
1.645
1.960
1.282
1.645
1.960
1.282
1.645
1.960
New
1.417
1.826
2.186
5.684
8.063
10.438
55.350
79.233
103.116
TC
2.680
3.091
3.511
14.786
15.197
15.617
135.843
136.254
136.674
Critical values for n = 100 and several values of ϵ and α. To
allow easy comparison, these values are for a normalized W
statistic, i.e., W has been divided by the relevant constant so
that it is (before the addition of Laplacian noise) distributed
according to a standard normal. See the full version for the
equivalent table at n = 1000.
Our second key change from the TC test, mentioned earlier, is
that we handle rows with di = 0 according to the Pratt variant of
the Wilcoxon, rather than dropping them completely as is more
traditional. The reason the traditional method is so difficult in the
private setting is that the reference distribution one must compare
to depends on the number of rows that were dropped. If nr is the
number of non-zero rows (i.e., rows that weren’t dropped), one is
13This is where Task and Clifton make an error. This formula is correct, but they used
an incorrect density function for the Laplace distribution and as a result calculated
incorrect values of д.
supposed to look up the critical value associated with nr , rather
than the original size n of the database.
Unfortunately, nr is a sensitive value and cannot be released
privately.14 Task and Clifton show that it is acceptable (in that it
does not result in type I error greater than α) to compare to a
critical value for a value of nr that is lower than the actual value.
This allows them to give two options for how one might deal with
the lack of knowledge about nr .
High Utility This version of the TC test simply assumes nr ≥
.3n and uses the critical value that would be correct for nr =
.3n. We stress that this algorithm is not actually differentially
private, though it could easily be captured by a sufficiently
weakened definition that limited the universe of allowable
databases. Another problem is that for most realistic data, nr
is much greater than .3n and using this loose lower bound
still results in a large loss of power.
High Privacy This version adds k dummy values to the data-
base with di = ∞ and k with di = −∞.15 Then one can be
certain of the bound nr ≥ 2k. This is a guaranteed bound so
this variant truly satisfies differential privacy. On the other
hand it is a very loose lower bound in most cases, leading to
a large loss of power.
Experimental comparison. We compare the statistical power of
our test to that of the TC test. We begin by again measuring the
power when detecting the difference between two normal distribu-
tions with means one standard deviation apart. The results can be
seen in Figure 10. If we look at the database size needed to achieve
80% power, we find that the 32 data points we need, while more
than the public test (14), are many fewer than the TC High Utility
variant (80) or the TC High Privacy variant (122). The full version
includes a figure for ϵ = .1 as well. What we see is that, while all
private tests require more data, our test (requiring n ≈ 236) still
requires about 40% as much data as the TC High Utility variant
(588). The TC High Privacy variant, however, scales much less well
to low ϵ and requires roughly 2974 data points.