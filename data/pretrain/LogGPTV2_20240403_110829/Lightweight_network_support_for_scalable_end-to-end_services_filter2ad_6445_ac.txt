to the portion of the multicast tree through which NACKs
were forwarded.
To implement this service, NACK packets carry an ESP
instruction that marks both incoming and outgoing inter-
faces of each node visited. The mark on the output (up-
stream) side is used to suppress subsequent NACKs for the
same data packet. This is implemented with a count in-
struction with threshold of one (or higher for fault tolerance;
see Section 4.3). The mark on the input side is necessary for
subcasting. Retransmissions carry a piggybacked ESP in-
struction, processed on the output (downstream) side, that
allows the packet to pass only if the interface was marked
by a NACK for that sequence number. Figure 5 illustrates
the operation in a single router with four interfaces. In (a),
the multicast source S transmits to three receivers, but the
packet is lost before reaching R0 and R1. In (b), R0 and
R1 send a NACK with an ESP instruction piggybacked; the
instruction deposits a small amount of state (with a tag de-
termined by the sequence number of the missing data) in the
input interface. It also checks for the presence of a similar bit
of state on the output interface, and the packet is forwarded
only if the state is not present or the threshold has not been
exceeded. (The ﬁgure shows operation with a threshold of
one.) In (c), the source retransmits the requested data, with
a piggybacked ESP instruction that checks for the existence
of state at the output interface, and the packet is forwarded
through the interfaces toward R0 and R1.
This solution works well provided the path followed by
NACK packets is the reverse of the path followed by data
(and retransmission) packets. However, if the paths are not
symmetric, an alternative method is needed to relay NACK
packets back up the forward data path. The same situa-
tion arises whenever protocols require that network state be
accessed on both the forward and backward path, for exam-
ple in RSVP and PGM. Various solutions for this problem
exist, including a protocol designed speciﬁcally for this pur-
pose [22].
3.2 Simple Computations on User Data
Distributed applications commonly have need to distill or
aggregate data supplied by the participants. Perhaps the
simplest example is counting the number of receivers in a
multicast group; others include tallying votes, computing
the average load, and calculating the amount of work re-
maining in a system. This kind of aggregation is typically
accomplished by applying an associative and commutative
operator to values supplied by group members. However, if
the group is large, it is impractical for a single node to col-
lect all the input and perform the computation; if the group
members are anonymous, it is diﬃcult to impose a structure
that would allow the result to be computed by the group in
a distributed fashion.
Both of these problems can be solved via tree-structured
ESP computations. The basic idea is for a particular node to
be designated as the collector or destination; the paths from
group members to the destination node form a tree. Each
member sends its value to the destination in an ESP packet;
each interior node of the tree collects values from its chil-
dren, performs the computation, and forwards the result to
its parent. Eventually the destination receives the (single)
270COLLECT(pkt p )
α = get(p.V )
if (α is nil )
else
α = p.val
α = α ◦ p.val
put(p.V, α)
β = get(p.C )
if (β is nil ) abort
β = β - 1; put(p.C, β)
if (β == 0)
p.val = α
forward p
else discard p
Figure 6: Instruction for aggregation computation
result. Risk of implosion is reduced because each interior
node sees packets only from its immediate children in the
tree. Associativity and commutativity of the operation en-
able collection and computation to be interleaved—that is,
packet values can be combined with the computation state
one at a time as they arrive.
In general, these tree-structured “aggregation” computa-
tions are carried out in two phases. In the ﬁrst phase, each
non-leaf node learns of its children in the tree. In the second
phase, group members send their values up the tree (toward
the destination). Each node computes and forwards its re-
sult only after having heard from each of its children. The
ﬁrst phase can be accomplished with the count instruction
described in Section 2.4. The second phase can be accom-
plished with a simple collect instruction, which takes four
operands: A tag V, identifying the result of the computa-
tion so far in the ESS; an immediate value val, which carries
the value to be contributed by this packet; a tag C, identify-
ing the child-counter established by the count instruction
in the ﬁrst phase; and an immediate value ◦, which speci-
ﬁes the associative and commutative operator to be applied.
collect applies the user-speciﬁed operation ◦ (e.g., addi-
tion or subtraction) to the value in the packet and the value
stored at the node.
It then decrements the “child count”
(C ). When C reaches 0, all children have reported and the
result is forwarded to the next hop.
Note that the two phases of the computation share ephem-
eral state, so both must be completed at each node within
one ephemeral state lifetime. (See also Section 4.2.)
3.3 Discovering Topology Information
Recently researchers have proposed various end-to-end ser-
vices based on the ability to invoke special (predeﬁned) func-
tionality at particular locations inside the network. By en-
abling special functions at precisely the right nodes, end
systems can control the way their packets are processed en
route. Examples of such functions include packet duplica-
tion for multicast [26, 28], marking or logging for traceback
of denial-of-service attacks [19, 24], ingress ﬁltering [15], and
packet redirection for overlays [10, 25]. Although ESP is not
designed to provide such “heavyweight” functions itself, it
can be used to solve the problem of determining where in
the network such functions should be invoked.
The basic approach, again, is to ﬁrst send packet(s) that
set up state to distinguish the desired node from others, and
then send packet(s) to recognize and collect the address(es)
of the distinguished node(s). The example given in Section 1
and shown in Figure 1 illustrates this.
In earlier work, we showed how ESP can be used in com-
bination with the ability to invoke special processing at
speciﬁc routers, to enable new types of services. We pro-
posed the use of dynamically-invoked duplication modules
at routers to implement multicast using unicast forward-
ing [27]. ESP is ideally suited for discovering a good (ef-
ﬁcient) location to invoke a duplication module.
In other
work, we have shown how to use ESP to identify bottleneck
links in multicast trees [28], and (in the context of layered
multicast) use that information to install thinning modules
for congestion control.
4. PRACTICAL CONSIDERATIONS
In this section we address various questions of a pragmatic
nature that may arise when designing, deploying and using
ephemeral state services.
4.1 Partial Deployment
One cannot assume that every router in the network will
simultaneously begin supporting any new capability. There-
fore services based on ESP need to operate correctly when
only a subset of routers are ESP-capable. Because non-ESP-
capable routers simply forward ESP packets, all of the algo-
rithms described in this paper operate correctly with partial
ESP deployment, albeit with reduced performance and scal-
ability in some cases. For example, the path-intersection
computation described in Section 1 returns the last ESP-
capable node common to both paths.
In general, the performance and scalability of ESP services
improves as the fraction of nodes supporting ESP grows.
For some applications, such as the aggregation application
described in Section 3.2, we have found that most of the
beneﬁt can be obtained even if the functionality is deployed
only at the edges of domains [3].
4.2 Timing
Distributed computations using ephemeral state require
some level of coordination to ensure packets arrive at all
nodes within an interval of duration τ . The simplest method
is to have a controlling node transmit a stimulus message
to participating hosts, which then respond by sending the
appropriate ESP packet. This stimulus message may need
to be reliably transmitted.
0
1
2
3
4
erroneous
delivery
1a
1b
2a
2b
3a
3b
4a
time
4b
Figure 7: Erroneous aggregation computation: “a”
packets establish child counts, “b” packets collect
values.
When stimulus-based coordination is inadequate (e.g., re-
liably disseminating the stimulus introduces too much jitter)
271some other coordination method is required. If the partic-
ipating hosts have a common time source the controlling
node can instead transmit the start time of the computa-
tion to the participants. The start time simply needs to
reliably reach the participants in advance of the indicated
time. Note that the accuracy required of the common time
base is modest—as long as the participants’ clocks are within
the (assumed) maximum end-to-end network delay of each
other, the computation will be coordinated to the same de-
gree as with stimulus messages.
A related issue is the timing between phases of a multi-
phase computation. The inter-phase delay must be adequate
to ensure that all packets of one phase are processed at each
node before any packets of the next phase—otherwise errors
can result. Figure 7 illustrates this for the aggregation com-
putation: Leaf nodes 1 and 4 start each phase (labeled a and
b) of the computation at the same time. However, because
the b phase starts at Node 1 before all a packets have made
it to Node 0, Node 0 prematurely delivers the result.
Given an upper bound δ on the one-way transit delay
through the network, an inter-phase delay of 2δ suﬃces to
ensure that this kind of error does not occur.
4.3 Dealing With Errors
ESP is a best-eﬀort service; like other IP datagrams, pack-
ets carrying ESP instructions are subject to various misfor-
tunes including loss, reordering, and duplication. The eﬀect
of such errors on end-to-end computations depends on the
particular computation. The choice of how best to deal with
such errors is ultimately up to the application itself. Our in-
tent here is to consider the eﬀects and highlight some princi-
ples for designing computations to be robust against errors.
We focus here on losses because ESP-based computations
tend to be robust against reordering, and also because the
solution for losses protects against duplicates as well.
When an ESP packet is lost, the eﬀect on the computation
is one of the following:
• No eﬀect: the computation proceeds correctly to com-
pletion in spite of the error (e.g., losing a packet that
would have been discarded anyway, like a compare
packet that does not contain the extreme value).
• Silent failure: nothing is delivered to the application(s)
in the end systems. This result can be detected in the
usual way, through a timeout.
• Explicit failure: an instruction aborts at an interme-
diate node due to the absence of expected ephemeral
state (because the state was never established or timed
out). In this case the packet that invoked the aborted
computation is forwarded with the Err bit set and the
Loc bits cleared.
• Incorrect result: an incomplete or incorrect value is
delivered to the application as if it were correct.
Of course, the likelihood of each type of outcome depends on
the speciﬁc details of the instance. For some computations,
an incorrect outcome is unlikely in all cases, while for others,
the probability grows with the size of the computation.
Consider the feedback thinning service of Section 3.1.1,
for example. Packet loss may aﬀect the number and se-
quence of values delivered to the destination, but it does
not aﬀect the ultimate result unless all packets containing
the maximal value are lost. Thus if multiple repetitions of
the computation produce the same result it is very likely to
be correct.
On the other hand, the two-phase aggregation service de-
scribed in Section 3.2 will fail silently or produce an incor-
rect result if even one message is lost. For small groups it
may be possible to obtain a correct result by simply repeat-
ing the computation, but as the number of nodes involved
grows, the likelihood of error-free completion drops quickly.
We simulated the aggregation computation for a group size
of 5000, with 4 randomly-chosen lossy links, each having a
loss probability of 10%. Out of 1000 runs, 690 completed
successfully (i.e. a result was delivered to the receiving ap-
plication); of those, only 454 returned the correct answer of
5000. Clearly it is necessary to “build-in” robustness when
designing such services.
Our paradigm for making distributed computations ro-
bust against loss is based on proactive retransmission.
In
the rest of this subsection we outline the principles of the
approach and apply it to the aggregation computation of
Section 3.2 as an example.
4.3.1 Adding Redundancy
Lost ESP packets, like lost TCP packets, are recovered
by host retransmission regardless of where in the network
they are lost. However, the situation is somewhat more
complicated with ESP. One reason is that each ESP packet
is (in general) intended to modify the state of all ESP nodes
along its path, but a lost packet only aﬀects nodes up to the
point where it is dropped. Thus the retransmitted packet
should modify only the state of those nodes after the point
where the original packet was dropped. On the other hand,
the packet itself may be modiﬁed as it travels through the
network; the retransmitted packet should match the original
when it arrives at the loss point.
These observations imply that each ESP node must be
able to distinguish between original packets and retransmit-
ted duplicates. Moreover, any packet recognized as a dupli-
cate should not update the computation state at the node,
while the packet itself should be updated in the same way
as the original packet. Also, duplicates must be forwarded
(if forwarding is consistent with the computation state) to
ensure that redundancy propagates through the network.
If all duplicate packets are forwarded in a tree-structured
computation, the redundancy is ampliﬁed and implosion
may result. Therefore ﬁltering is needed to ensure that re-
dundancy remains at a constant level as packets move up
in the tree. We therefore must keep track of, and limit, the
number of packets forwarded at each hop, thereby keeping
the amount of redundancy constant across the tree.
The foregoing discussion leads to the following general
form for instructions in a multiphase, tree-structured com-
putation:
Basic Redundant Instruction Form
if the packet is not a duplicate
record the packet;
execute the operation;
update the computation state and packet;
else
(possibly) update the packet;
increment fwd-count ;
if the packet is forwardable and fwd-count ≤ pkt.limit
forward the packet
else discard the packet
272Note that it is not necessary for all hosts to originate the
same number of packets. The computation may specify an
expected number of transmissions per host, and each host
can transmit transmit with the appropriate probability to
achieve that average.
4.3.2 Duplicate Detection
In general the number and identity of nodes originating
packets in a computation is not known a priori, so we need a
means of distinguishing retransmitted packets from original
packets without this information. We use Bloom ﬁlters to
solve this problem using a ﬁxed, modest amount of space.
A Bloom ﬁlter [1] is a way to record sets of elements us-
ing a bitmap of size m = 2q, and k random hash functions
h0, . . . , hk−1. Each element that might be recorded in the
Bloom ﬁlter has a unique identiﬁer; the hash functions map
identiﬁers to q-bit oﬀsets. An element is recorded in the
bitmap by hashing its identiﬁer with each of the k hash
functions, and setting the bits at each of the resulting oﬀ-
sets. To test for the presence of the element with identiﬁer
i in the set, one checks whether each of the bits at oﬀsets
h0(i), . . . , hk−1(i) is set in the bitmap. If so, the element is
considered to be present; otherwise it is not present.
Bloom ﬁlters introduce a tradeoﬀ between the size of the
bitmap and the probability of a false positive: After a num-
ber of elements have been recorded in the bitmap, a new
element may be erroneously judged to be present, because
all of its bits were set by earlier elements. Given a set of
k hash functions h0, . . . hk−1, we say a set I of identiﬁers is
collision-free if, for each i ∈ I,
{h0(i), . . . , hk−1(i)} (cid:9)⊆ [
j∈I,j(cid:3)=i
{h0(j), . . . , hk−1(j)}
Figure 8 shows the probability, according to this deﬁnition,1
that a randomly-chosen set of identiﬁers is collision-free.
The horizontal axis represents the number of distinct ele-
ments being recorded; the curves show the results (obtained
by simulation) for various combinations of m and k.
Detection of duplicate packets is implemented by storing
the Bloom ﬁlter bitmap in the ESS. For simplicity, the fol-
lowing discussion assumes that the Bloom ﬁlter bitmap is
the size of a single ESS value2 At the beginning of a com-
putation, each node randomly chooses k oﬀsets from 0 to
m− 1, and sets the corresponding bits in a bitmap (IDmap)
the same size as the Bloom ﬁlter. Each packet (i.e. instruc-
tion) forwarded by that node in that computation carries a
tag identifying the Bloom ﬁlter, plus this IDmap. To check
whether a packet is a duplicate, the receiving node checks
whether all the bits set in the IDmap are already set in the
stored Bloom ﬁlter.
If so, the packet is considered a du-
plicate; otherwise it is considered new, and the IDmap is
OR’ed into the Bloom ﬁlter bitmap. This method has the
advantage of allowing the instruction to be oblivious to the
value of k.
1This condition is strong, in the sense that it guarantees
no false positive will occur regardless of the order in which
elements are added. For some sets that do not satisfy this
condition, whether a false positive occurs depends on the
order in which the elements are added.
2It can easily be generalized to larger bitmap sizes by letting
a single tag denote a sequence of tags.
n
o
s
i
i
l
l