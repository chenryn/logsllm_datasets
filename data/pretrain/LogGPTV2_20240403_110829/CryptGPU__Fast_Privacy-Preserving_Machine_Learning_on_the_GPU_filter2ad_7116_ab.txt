difference in the running time of point-wise addition vs. point-
wise modular reduction. Thus, when choosing and designing
cryptographic protocols for the GPU, one must carefully
calibrate them for the architecture. Protocols like Yao’s
garbled circuits [28] are less well-suited for taking advantage
of GPU parallelism compared to a vectorized secret-sharing-
based protocol. Similarly, protocols that require extensive
ﬁnite ﬁeld arithmetic (and thus, require modular reductions)
will incur more overhead on the GPU compared to protocols
that only rely on arithmetic modulo a power of 2. We
also design protocols for common non-linear functions (e.g.,
exponentiation and division) that are speciﬁcally optimized
for our particular setting. We describe the cryptographic
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:25 UTC from IEEE Xplore.  Restrictions apply. 
21022
protocols we use in Section III.
Systematic evaluation of GPU-based MPC. We present a
comprehensive and systematic evaluation of CRYPTGPU to
quantify the advantages of a GPU-based MPC protocol and
compare against previous protocols for privacy-preserving
machine learning. We speciﬁcally measure the performance of
our private training and inference protocols on a wide range
of object recognition models (e.g., LeNet [18], AlexNet [16],
and the ResNet family of networks [14]) and datasets (e.g.,
MNIST [11], CIFAR-10 [12], and ImageNet [13]). We describe
our experimental methodology and measurements in Section IV.
We also collect ﬁne-grained measurements to understand
how the computational costs are split across the different
layers of a network. For instance, in CPU-based systems like
FALCON [6], the linear layers account for 86% to 99% of the
overall computational costs of private training.2 On the same
model/datasets, our GPU-based approach evaluates the same
linear layers with a 25× to 72× speed-up; this is the primary
source of the performance advantage of CRYPTGPU compared
to previous systems. Consequently, the costs of our private
training protocol is more evenly split between evaluating linear
layers and non-linear layers. We provide the full details in
Section IV-B and Table V.
Finally, we report microbenchmarks to quantify the perfor-
mance advantages of using the GPU to execute all of the MPC
protocols (Section IV-C). For instance, we show that evaluating
convolutions on secret-shared data (with secret-shared kernels)
on the GPU is over 150× faster than the corresponding
protocol on the CPU. Even for non-linear operations like the
ReLU (rectiﬁed linear unit) function, using a GPU-based MPC
protocol still yields a 10× speed-up over the same underlying
CPU-based protocol. We provide the full set of results and
micobenchmarks in Section IV-C.
An ML-friendly approach. One of the guiding principles
behind our system design is to make it friendly for machine
learning researchers to use. We build our system on top of
CRYPTEN [24], which is itself built on top of the popular
machine learning framework PyTorch [21]. Effectively, our
work (much like CRYPTEN) provides a new cryptographic
back-end that supports computations on secret-shared values
while retaining a similar front-end as PyTorch. In fact, we note
that our work on developing the CUDALongTensor module
has already been integrated as part of CRYPTEN to support
privacy-preserving GPU computations [24].
II. SYSTEM OVERVIEW
Similar to previous works on constructing efﬁcient protocols
for privacy-preserving machine learning [2, 3, 6, 5, 29, 30]
(see also Section V), we assume that the data and model
are (arbitrarily) partitioned across three parties. For example,
the three parties could be three independent organizations
seeking to collaboratively train a model on their joint data
2While linear layers are simpler to evaluate from a cryptographic perspective
(in comparison to non-linear layers), the size of the linear layers is typically
much larger than that of the non-linear layers.
without revealing their inputs to each other. Our system is also
applicable in the “server-aided” setting [31], where a group
of (arbitrarily-many) clients seek to train a joint model on
their data (or evaluate a secret-shared model on private inputs).
In the server-aided setting, the clients ﬁrst secret share their
inputs to three independent cloud-service providers, who in
turn run the cryptographic protocol on the secret-shared inputs.
We design our protocols to provide security against a single
semi-honest corruption. We provide a formal description of
our threat model in Section III-A.
A. Background
Our starting point in this work is the CRYPTEN privacy-
preserving machine learning framework [24]. CRYPTEN is
built on top of the widely-used machine-learning framework
PyTorch [21]. We adapt the basic architecture of CRYPTEN,
and make modiﬁcations to support three-party protocols based
on replicated secret sharing. We describe the main system
architecture below.
GPU architecture. GPUs, and more recently, ASICs like
Google’s tensor processing units [23], have played a critical role
in scaling up modern deep learning. These specialized hardware
platforms support massive parallelism, making them well-suited
for performing standard linear algebraic operations (e.g., con-
volutions or average pooling) as well as point-wise evaluation
of functions on large blocks of neurons (e.g., evaluating an
activation function or performing batch normalization). Popular
frameworks for deep learning frameworks such as PyTorch [21]
and TensorFlow [22] natively support computations on both
the CPU and GPU.
CUDA is a parallel computing platform developed by
NVIDIA for general-purpose computing on GPUs [27].
For deep learning in particular, CUDA libraries such as
cuBLAS [32] and cuDNN [33] provide highly-optimized
implementation for a wide-range of standard primitives such
as convolutions, pooling, activation functions, and more. These
libraries are designed for ﬂoating-point computations and do
not support integer-valued analogs of these operations. Since
cryptographic protocols typically operate over discrete spaces
(e.g., a 64-bit ring) where the underlying algebra is implemented
using integer-valued computations, one cannot directly translate
an existing protocol to the GPU.
PyTorch. PyTorch [21] is a popular open-source machine
learning framework designed for prototyping, implementing,
and deploying deep neural networks. The PyTorch front-end
supports many standard neural network layers (e.g., convolu-
tions, pooling, activation functions, etc.) as well as features
such as automatic differentiation and gradient computation. The
PyTorch back-end natively supports computation on both CPUs
as well as GPUs. This ﬂexibility enables users to train complex
models without needing to worry about the ﬁner details of
backpropagation. It also allows users to take advantage of GPU
acceleration without needing to interface with low-level CUDA
kernels. PyTorch also provides library support for distributing
computations across multiple devices and/or GPUs.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:25 UTC from IEEE Xplore.  Restrictions apply. 
31023
Data in PyTorch is organized around tensors, which pro-
vide a general abstraction for n-dimensional arrays. PyTorch
provides an expressive API for computing on and applying
transformations to tensors. Especially importantly in our case,
the PyTorch back-end natively and seamlessly leverages GPU
acceleration for tensor computations.
CRYPTEN. CRYPTEN [24] is a recent framework built on top
of PyTorch for privacy-preserving machine learning. CRYPTEN
provide a secure computing back-end for PyTorch while still
preserving the PyTorch front-end APIs that enables rapid
prototyping and experimentation with deep neural networks.
The main data abstraction in CRYPTEN is the MPCTensor,
which functions like a standard PyTorch tensor, except the
values are secret shared across multiple machines. Internally,
CRYPTEN uses n-out-of-n additive secret sharing. For bilinear
operations such as convolutions and matrix multiplications,
CRYPTEN uses arithmetic secret sharing over a large ring
(e.g., Z264), while for evaluating non-linear operations like an
activation function, it uses Boolean secret sharing. CRYPTEN
uses the ABY share-conversion techniques [34] to convert
between arithmetic shares and Boolean shares.
CRYPTEN supports general n-party computation and pro-
vides security against a single semi-honest corruption. At
the cryptographic level, elementary arithmetic operations are
handled using Beaver multiplication triples [35], Boolean
circuit evaluation is implemented using the Goldreich-Micali-
Wigderson (GMW) protocol [7], and low-degree polynomial
approximations are used for most non-linear operations. We
note that while our system builds on CRYPTEN, we work in a
3-party model where parties compute using replicated secret
shares (as in [26]). We discuss this in Section III.
B. System Design and Architecture
The design of CRYPTGPU is centered around the following
principles:
• Leverage existing CUDA kernels for linear algebra. As
mentioned in Section II-A, highly-optimized CUDA kernels
exist for most linear algebra operations encountered in deep
learning. However, these kernels only support computations
on ﬂoating-point values and are not directly applicable for
computing on discrete structures common in cryptographic
protocols. Thus, we seek a way to keep all of the computation
on the GPU itself.
• Keep all computations on the GPU. While some previous
works on private machine learning [36, 4] show how to
leverage the GPU for computing bilinear functions, they then
move the data out of the GPU to evaluate non-linear functions.
In this work, we seek to keep all of the computations on the
GPU, and as we show in Section IV-C, even computing non-
linear functions can beneﬁt greatly from GPU acceleration,
provided that they are implemented using “GPU-friendly”
cryptographic protocols (i.e., protocols that primarily rely
on point-wise or component-wise vector operations).
Floating point computations. The cryptographic core of
CRYPTGPU relies on (additive) replicated secret sharing over
the 64-bit ring Z264. Computing bilinear functions such as
convolutions over secret-shared values essentially correspond
to the parties running an analogous local operation on their
shares, followed by a communication step (see Section III).
Our goal is to take advantage of the GPU to accelerate each
party’s local computation on their individual shares. As noted
in Section II-A, existing GPU libraries for linear algebra only
support computation over 64-bit ﬂoating point values. Thus, to
take advantage of GPU support for these operations, we need
to embed the ring operations over Z264 (or equivalently, 64-bit
integer operations) into 64-bit ﬂoating point operations.
Integer operations using ﬂoating-point arithmetic. Our
approach for embedding 64-integer operations into 64-bit
ﬂoating point operations relies on the following observations:
• Exact computation for small values. First, 64-bit ﬂoating
point values have 52 bits of precision and can exactly
represent all integers in the interval [−252, 252]. This means
that for all integers a, b ∈ Z ∩ [−226, 226], we can compute
the product ab using their ﬂoating-point representations and
still recover the correct value over the integers.
• Bilinearity. Operations like matrix multiplication and con-
volutions are bilinear. This means that for any choice of
inputs A1, A2, B1, B2,
(A1 + A2) ◦ (B1 + B2) =
A1 ◦ B1 + A2 ◦ B1 + A2 ◦ B1 + A2 ◦ B2,
where ◦ denotes an arbitrary bilinear operation. Suppose
now that we rewrite an input as an expansion in a smaller
base; for example, we might write A = A0 + 216A1 and
B = B0 + 216B1. Bilinearity ensures that A ◦ B can be
expressed as a linear combination of the pairwise products
A0 ◦ B0, A0 ◦ B1, A1 ◦ B0, and A1 ◦ B1. Computing
A◦ B from the pairwise products only requires element-wise
additions and scalar multiplications.
• CUDA kernels for element-wise operations. To complete
the puzzle, we note that there are optimized CUDA kernels
for performing component-wise addition and scalar multipli-
cation on 64-bit integer values.
To evaluate a bilinear operation ◦ like matrix multiplication or
convolution (which do not have integer kernels), CRYPTGPU
ﬁrst decomposes each of the inputs A, B ∈ Zn×m
(cid:80)k
into
smaller inputs A1, . . . , Ak, B1, . . . , Bk ∈ Zn×m
2w where A =
i=1 2(i−1)wAi. Then, it computes the k2 products Ai ◦ Bj
using ﬂoating-point arithmetic on the GPU. As long as the
entries of Ai ◦ Bj do not exceed 252 in magnitude, all of
these pairwise products are computed exactly. Finally, each
component of the pairwise product is re-interpreted as a 64-bit
integer. Computing A ◦ B from the pairwise products AiBj
amounts to evaluating a linear combination of tensors, which
can be done efﬁciently using existing CUDA kernels for 64-bit
integer operations. Note that since the ﬁnal operations are taken
modulo 264, it sufﬁces to compute only the products AiBj
where w(i + j − 2) < 64.
264
When performing computations using ﬂoating-point kernels,
CRYPTGPU decomposes each input into k = 4 blocks, where
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:25 UTC from IEEE Xplore.  Restrictions apply. 
41024
the values in each block are represented by a w = 16-bit
value. For this choice of parameters, each bilinear operation is
expanded into 10 pairwise products.
Remark II.1 (Smaller Number of Blocks). While it may
be tempting to decompose 64-bit values into k = 3 blocks,
where each block consists of 22-bit values, this compromises
correctness of our approach. Namely, correctness of the
computation is guaranteed only if the entries in each of the
intermediate pairwise products Ai ◦ Bj do not exceed the 52-
bits of available ﬂoating-point precision. If the entries of Ai
and Bj are 22 bits, then the entries in a single multiplication
between an element in Ai and Bj will already be 44 bits.
If we are evaluating a convolution (or matrix multiplication)
where each output component is a sum of 28 = 256 values,
this exceeds the available precision and triggers an arithmetic
overﬂow. This is problematic for larger networks. Using 16-bit
blocks, we can handle bilinear operations involving up to 220
intermediate products, which is sufﬁcient for our applications.
Remark II.2 (Overhead of Block-wise Decomposition).
While decomposing each bilinear operation on integer val-
ues into O(k2) ﬂoating-point operations on same-sized in-
puts can appear costly, CRYPTGPU takes advantage of
GPU parallelism to mitigate the computational overhead.
Namely, for convolutions, CRYPTGPU uses group convo-
lution (cudnnConvolutionForward)
to compute the
convolutions in parallel. Similarly, for matrix multiplica-
tions, CRYPTGPU uses a batch matrix multiplicative kernel
(cublasSgemm) to multiply matrices in parallel. We observe
that for small inputs (e.g., 64 × 64 inputs), this approach only
incurs a modest 2× overhead (compared with evaluating a
single convolution of the same size) and increases to roughly
9× for larger 224 × 224 inputs.
While the computational overhead of our embedding is
partially mitigated through parallelism, this approach does
increase the memory requirements of our protocol. This does
not have a signiﬁcant effect on privacy-preserving inference,
but it does limit the batch size we can handle during privacy-
preserving training (recall that during training, a single iteration
of the optimization algorithm processes a batch of instances).
Scaling up to support
larger batch sizes during privacy-
preserving training would likely necessitate distributing the
computation across multiple GPUs rather than a single GPU
(as is also the case for training deep models in the clear).
Remark II.3 (Comparison with DELPHI). The DELPHI
system [4] leverage GPUs for evaluating convolutions on
secret-shared inputs in their private inference system. In their
setting, the parameters are chosen so that the outputs of the
convolution are always within the interval [2−52, 252], and as
such, the existing ﬂoating-point kernels for convolution can be
used without incurring any ﬂoating-point precision issues. In
particular, DELPHI uses a 32-bit ring and 15 bits of ﬁxed-point
precision. The system works in the setting where the model
parameters are assumed to be public: namely, the convolution
kernels are not secret-shared. In this way, convolutions are
evaluated between a plaintext value and a secret-shared value,
which ensures that the resulting outputs are bounded. In our
setting, both the model and the inputs are secret-shared so
we cannot directly embed the integer-valued operations into
64-bit ﬂoating-point computations. In fact, as we discuss in
Section IV-C, to have sufﬁcient precision when scaling up to
deeper models and larger datasets, it is often necessary to use a
larger ring (i.e., a 64-bit ring) for the arithmetic secret sharing.
The CUDALongTensor abstraction. CRYPTGPU provides
a new abstraction called a CUDALongTensor for embed-
ding 64-bit
integer-valued operations into 64-bit ﬂoating-
point arithmetic. Similar to CRYPTEN’s MPCTensor, the
CUDALongTensor abstractly represents a secret-shared ten-
sor of 64-bit integers and is backed by a standard PyTorch
tensor of 64-bit
integers. In the back-end, whenever an
elementary operation needs to be evaluated on the underlying
tensor, CRYPTGPU proceeds as follows: