How to Measure the Control-flow
Complexity of Web Processes and
Workflows
Jorge Cardoso, Department of Mathematics and Engineering,
University of Madeira, Portugal
SUMMARY
Several Web process and workflow specification languages and systems
have been developed to ease the task of modeling and supporting business
processes. In a competitive e-commerce and e-business market,
organizations want Web processes and workflows to be simple, modular,
easy to understand, easy to maintain and easy to re-engineer.
To achieve these objectives, one can calculate the complexity of processes.
The complexity of processes is intuitively connected to effects such as
readability, understandability, effort, testability, reliability and
maintainability. While these characteristics are fundamental in the context
of processes, no methods exist that quantitatively evaluate the complexity
of processes.
The major goal of this chapter is to describe a measurement to analyze the
control-flow complexity of Web processes and workflows. The measurement
is to be used at design-time to evaluate the complexity of a process design
before implementation.
INTRODUCTION
The emergence of e-commerce has changed the foundations of business,
forcing managers to rethink their strategies. Organizations are increasingly
faced with the challenge of managing e-business systems, Web services,
Web processes, and workflows.
Web Services and Web processes promise to ease several current
infrastructure challenges, such as data, application, and process
integration. With the emergence of Web services, a workflow management
system become essential to support, manage, and enact Web processes,
both between enterprises and within the enterprise (Sheth, Aalst, &
Arpinar, 1999).
The effective management of any process requires modeling, measurement,
and quantification. Process measurement is concerned with deriving a
numeric value for attributes of processes. Measures, such as Quality of
Service measures (Cardoso, Miller, Sheth, Arnold, & Kochut, 2004), can be
used to improve processes productivity and quality.
To achieve an effective management, one fundamental area of research that
needs to be explored is the complexity analysis of processes. Process
complexity can be viewed as a component of a QoS model for processes,
since complex processes are more prone to errors. For example, in software
HOW TO MEASURE THE CONTROL-FLOW COMPLEXITY OF WEB PROCESSES AND
WORKFLOWS
engineering it has been found that program modules with high complexity
indices have a higher frequency of failures (Lanning & Khoshgoftaar, 1994).
Surprisingly, in spite of the fact that there is a vast literature on software
measurement of complexity, Zuse (Zuse, 1997) has found hundreds of
different software metrics proposed and described, while no research on
process complexity measurement has yet been carried out.
A Web process is composed of a set of Web services put together to achieve
a final goal. As the complexity of a process increases, it can lead to poor
quality and be difficult to reengineer. High complexity in a process may
result in limited understandability and more errors, defects, and exceptions
leading processes to need more time to develop, test and maintain.
Therefore, excessive complexity should be avoided. For instance, critical
processes, in which failure can result in the loss of human life, requires a
unique approach to development, implementation and management. For
this type of processes, typically found in healthcare applications (Anyanwu,
Sheth, Cardoso, Miller, & Kochut, 2003), the consequences of failure are
terrible. The ability to produce processes of higher quality and less
complexity is a matter of endurance.
Our work borrows some techniques from the branch of software
engineering known as software metrics, namely McCabe’s cyclomatic
complexity (MCC) (McCabe, 1976). A judicious adaptation and usage of this
metric during development and maintenance of Web process applications
can result in a better quality and maintainability. Based on MCC, we
propose a control-flow complexity metric to be used during the design of
processes. Web process control-flow complexity is a design-time metric. It
can be used to evaluate the difficulty of producing a Web process before its
implementation. When control-flow complexity analysis becomes part of the
process development cycle, it has a considerable influence in the design
phase of development, leading to further optimized processes. This control-
flow complexity analysis can also be used in deciding whether to maintain
or redesign a process.
Throughout this chapter, we will use the term “process” to refer to a Web
process or a workflow and we will use the term “activity” to refer to a Web
service or a workflow task.
CHAPTER STRUTURE
This chapter is structured as follows. The first section presents the related
work. We will see that while a significant amount of work in the software
engineering field has been developed to quantify the complexity of
programs, the literature and work on complexity analysis for Web processes
and workflow are inexistent. In the next section, we discuss the analysis of
processes’ complexity. We start by giving a definition for Web processes’
complexity. We then enumerate a set of properties that are highly desirable
for a model and theory to calculate the complexity of processes. In this
section, we also motivate the reader towards a greater understanding of the
importance and use of complexity metrics for processes. The next section
2
HOW TO MEASURE THE CONTROL-FLOW COMPLEXITY OF WEB PROCESSES AND
WORKFLOWS
gives an overview of McCabe’s cyclomatic complexity. This overview is
important since our approach borrows some of McCabe’s ideas to evaluate
complexity. Subsequently, we discuss process control-flow complexity. We
initiate this section giving the semantics of processes’ structure and
representation. Once the main elements of a process are identified and
understood, we show how control-flow complexity can be calculated for
processes. Finally, the last section presents our conclusions and future
work.
RELATED WORK
While a significant amount of research on the complexity of software
programs has been done in the area of software engineering, the work
found in the literature on complexity analysis for Web processes and
workflows is inexistent. Since the research on process complexity is
inexistent, in this section we will discuss the progress made in the area of
software complexity.
The last 30 years has seen a large amount of research aimed at
determining measurable properties to capture the notions of complexity of
software. The earliest measures were based on analysis of software code,
the most fundamental being a basic count of the number of Lines of Code
(LOC). Despite being widely criticized as a measure of complexity, it
continues to have widespread popularity mainly due to its simplicity
(Azuma & Mole, 1994).
An early measure, proposed by McCabe (McCabe, 1976), viewed program
complexity related to the number of control paths through a program
module. This measure provides a single number that can be compared to
the complexity of other programs. It is also one of the more widely accepted
software metrics. It is intended to be independent of language and language
format.
The search for theoretically based software measures with predictive
capability was pioneered by Halstead (Halstead, 1977). Complexity
measurement was developed to measure a program module's complexity
directly from source code, with emphasis on computational complexity. The
measures were developed as a means of determining a quantitative
measure of complexity based on a program comprehension as a function of
program operands (variables and constants) and operators (arithmetic
operators and keywords which alter program control flow).
Henry and Kafura (Henry & Kafura, 1981) proposed a metric based on the
impact of the information flow in a program’ structure. The technique
suggests identifying the number of calls to a module (i.e. the flows of local
information entering: fan-in) and identifying the number of calls from a
module (i.e. the flows of local information leaving: fan-out). The measure is
sensitive to the decomposition of the program into procedures and
functions, on the size and the flow of information into procedures and out
of procedures.
3
HOW TO MEASURE THE CONTROL-FLOW COMPLEXITY OF WEB PROCESSES AND
WORKFLOWS
A recent area of research involving Web processes, workflows, and Quality
of Service can also be considered related to the work in this chapter.
Organizations operating in modern markets, such as e-commerce activities
and distributed Web services interactions, require QoS management.
Appropriate quality control leads to the creation of quality products and
services; these, in turn, fulfill customer expectations and achieve customer
satisfaction. Quality of service can be characterized according to various
dimensions. For example, Cardoso et al. (Cardoso, Sheth, & Miller, 2002)
have constructed a QoS model for processes composed of three dimensions:
time, cost, and reliability. Another dimension that could be considered
relevant under the QoS umbrella is the complexity of processes. Therefore,
the complexity dimension could be added and integrated to the QoS model
already developed (Cardoso, Miller et al., 2004).
PROCESS COMPLEXITY ANALYSIS
The overall goal of process complexity analysis is to improve the
comprehensibility of processes. The graphical representation of most
process specification languages provides the user with the capability to
recognize complex areas of processes. Thus, it is important to develop
methods and measurements to automatically identify complex processes
and complex areas of processes. Afterwards, these processes can be
reengineered to reduce the complexity of related activities. One key to the
reengineering is the availability of a metric that characterizes complexity
and provides guidance for restructuring processes.
Definition of Process Complexity
Several definitions have been given to describe the meaning of software
complexity. For example, Curtis (Curtis, 1980) states that complexity is a
characteristic of the software interface which influences the resources
another system will expend or commit while interacting with the software.
Card and Agresti (Card & Agresti, 1988) define relative system complexity
as the sum of structural complexity and data complexity divided by the
number of modules changed. Fenton (Fenton, 1991) defines complexity as
the amount of resources required for a problem’s solution.
After analyzing the characteristics and specific aspects of Web processes
and workflows, we believe that the definition that is better suited to
describe processes complexity can be derived from (IEEE, 1992). Therefore,
we define process complexity as the degree to which a process is difficult to
analyze, understand or explain. It may be characterized by the number and
intricacy of activity interfaces, transitions, conditional and parallel branches,
the existence of loops, roles, activity categories, the types of data structures,
and other process characteristics.
Process Complexity Measurement Requirements
The development of a model and theory to calculate the complexity
associated with a Web process or workflow need to conform to a set of basic
but important properties. The metric should be easy to learn, computable,
4
HOW TO MEASURE THE CONTROL-FLOW COMPLEXITY OF WEB PROCESSES AND
WORKFLOWS
consistent and objective. Additionally, the following properties are also
highly desirable (Tsai, Lopex, Rodriguez, & Volovik., 1986; Zuse, 1990):
• Simplicity. The metric should be easily understood by its end users,
i.e., process analysts and designers.
• Consistency. The metric should always yield the same value when
two independent users apply the measurement to the same process,
i.e. they should arrive at the same result.
• Automation. It must be possible to automate the measurement of
processes.
• Measures must be additive. If two independent structures are put
into sequence then the total complexity of the combined structures
is at least the sum of the complexities of the independent
structures.
• Measures must be interoperable. Due to the large number of
existing specification languages, both in academia and industry, the
measurements should be independent of the process specification
language. A particular complexity value should mean the same
thing whether it was calculated from a process written in BPEL
(BPEL4WS, 2002), WSFL (Leymann, 2001), BPML (BPML, 2004),
YAWL (Aalst & Hofstede, 2003), or some other specification
language. The objective is to be able to set complexity standards and
interpret the resultant numbers uniformly across specification
languages.
These properties will be taken into account in the next sections when we
introduce our model to compute the complexity of processes.
Uses of Complexity
Analyzing the complexity at all stages of process design and development
helps avoid the drawbacks associated with high complexity processes.
Currently, organizations have not implemented complexity limits as part of
their business process management projects. As a result, it may happen
that simple processes come to be designed in a complex way. For example,
important questions that can be made relative to the process illustrated in
Figure 2 (Anyanwu et al., 2003) are: “can the Eligibility Referral workflow be
designed in a simpler way?”, “what is the complexity of the workflow?” and
“what areas or regions of the workflow are more complex and therefore
more prone to errors?”
5
HOW TO MEASURE THE CONTROL-FLOW COMPLEXITY OF WEB PROCESSES AND
WORKFLOWS
Figure 1. Eligibility Referral Workflow
The use of complexity analysis will aid in constructing and deploying Web
processes and workflows that are more simple, reliable and robust. The
following benefits can be obtained from the use of complexity analysis:
• Quality assessment. Processes quality is most effectively measured