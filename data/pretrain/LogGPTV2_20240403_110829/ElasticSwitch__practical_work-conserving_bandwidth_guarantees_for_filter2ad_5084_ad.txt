ECN, as an alternative to our sequence-number scheme. If avail-
able, ECN improves congestion detection, and the ability to pro-
vide guarantees and be work-conserving (§7). We disabled ECN
on the hosts, such that ElasticSwitch’s congestion signaling does
not interfere with TCP, and we set the ECN capable bits on all pro-
tocols (e.g., UDP). For this purpose, we created another patch for
Open vSwitch, which also triggers congestion feedback based on
ECN bits at destinations. For accurate congestion detection, we set
the ECN marking probability in switches to a high value (100%),
when the queue is above a given threshold.
7. EVALUATION
The goals of this evaluation are to:
(1) show that Elastic-
Switch provides guarantees under worst case scenarios, and iden-
tify its limitations, (2) show that ElasticSwitch is work-conserving
(i.e., can improve utilization when some VMs are not active), (3)
gestion occurs in the network.
explore ElasticSwitch’s sensitivity to parameters, and (4) quantify
ElasticSwitch’s overhead in terms of CPU, latency and bandwidth.
Summary of results: Our experiments show that:
• ElasticSwitch can achieve the intended guarantees – even in
the worst conditions we tested, when trafﬁc from 300 VMs
compete with trafﬁc from a single VM, or when multiple
VMs run large MapReduce jobs at the same time. Without
guarantees, the completion time of these jobs could be two
orders of magnitude longer.
• ElasticSwitch provides guarantees irrespective of where con-
• ElasticSwitch is work-conserving, achieving between 75-
99% of the optimal link utilization. ElasticSwitch can be
tuned to be more work-conserving, at the expense of a grace-
ful degradation in the ability to provide guarantees in chal-
lenging conditions.
• ElasticSwitch’s work-conservation can increase completion
times for short ﬂows, compared to static reservations, by at
most 0.7ms; however, ElasticSwitch’s additional latency is
no worse than when the link is fully utilized.
• ElasticSwitch is not sensitive to small changes in parameters.
• ECN support improves all results, and also makes our im-
Experimental setup: We used ∼100 servers from a larger testbed
(the actual number varied in time). Each server has four 3GHz In-
tel Xeon X3370 CPUs and 8GB of memory. We use a parallel,
isolated network for our experiments. This prevents interference
between our experiments and other trafﬁc in the testbed. The par-
allel network is a two-level, single-rooted tree; all links are 1Gbps.
By choosing different subsets of servers, we can create different
oversubscription ratios.
provements to the rate allocation algorithm less relevant.
Our testbed did not have ECN capable switches. However, we
set up a one-rack testbed with a single ECN-capable switch.
To avoid virtualization overheads, we emulate multiple VMs by
creating multiple virtual interfaces, each with its own IP address,
connected to the kernel’s virtual switch. Each workload generator
on a host binds to a different IP address, thus emulating VMs with
virtual interfaces.
We compare ElasticSwitch with two other approaches: (i) No-
Protection: sending trafﬁc directly, with no bandwidth protection,
and (ii) Oktopus-like Reservation: a non-work-conserving reser-
vation system, with rates statically set to be optimal for the given
workload in order to achieve the hose model. Thus, Oktopus-like
Reservation is an idealized version of Oktopus; in practice, Okto-
pus is likely to perform worse than this idealized version. We also
make qualitative comparisons with Gatekeeper [20] and EyeQ [12],
since those approaches cannot provide guarantees when the net-
work core is congested.
Parameters: We use a 10% headroom between the link capacity
and the maximum allocated guarantees on that link. We set the rate
allocation period to 15ms and the guarantee partitioning period to
60ms. For Seawall: we use as weight w = 450M bps/BX→Y , we
set the rate decrease constant α = 0.4 (so the rate is decreased to
60% after a congestion event), δ = 0.75, the rate-increase constant
A = 0.5Mbps (§5), and we scale the physical time by TS = 1.5,
the value Δt = dt/TS, where dt is the physical time differ-
i.e.,
ence. For Rate-Caution we use Cmin = 0.3 and C = 0.5. We
implemented Hold-Increase using an exponential decay of packet
loss history, with decay factor γw, where γ = 0.75.
7.1 Guarantees and Work Conservation
We show that ElasticSwitch provides bandwidth guarantees and
is work-conserving, and that ElasticSwitch provides guarantees in
357Total throughput (X+Y)
No Protection
Oktopus−like Res.
ElasticSwitch
I
F
J
1
0.8
0.6
0.4
0.2
0
20
)
s
p
b
M
(
X
f
o
t
u
p
h
g
u
o
r
h
T
1000
900
800
700
600
500
400
300
200
100
0
ElasticSwitch TCP
ElasticSwitch UDP
No Protection TCP
No Protection UDP
300
50
100
150
200
250
Number of senders to Y
0
1
2
10 
100
200 
300
Number of senders to Y
Figure 5: Many-to-one in the core. VM X receives from one
remote VM while Y receives from multiple VMs. Both tenants
have a guarantee of 450Mbps over the congested core link.
Total throughput (X+Y)
No Protection
Oktopus−like Res.
ElasticSwitch
)
s
p
b
M
(
X
f
o
t
u
p
h
g
u
o
r
h
T
1000
900
800
700
600
500
400
300
200
100
0
0
1
2
10 
100
200 
300
Number of senders to Y
Figure 6: Many-to-one UDP vs. TCP. Same setup as Fig. 5, but
senders to Y blast UDP trafﬁc.
challenging conditions, when the entire network capacity is re-
served and all VMs are fully using their guarantees. ElasticSwitch
offers guarantees in all other less-congested conditions (which we
do not show for brevity).
Many vs. One scenario: Two VMs X and Y that belong to
two different tenants compete for a given link. Y receives trafﬁc
from multiple sources (e.g., Y is a MapReduce reducer), while X
receives trafﬁc only from a single remote VM. We assume both
X and Y have the same hose bandwidth guarantees of 450Mbps.
Given our 10% slack in providing guarantees, these represent the
maximum guarantees that can be offered on a 1Gbps link.
Fig. 5 presents the application-level TCP throughput for VM X
as we vary the number of VMs sending TCP trafﬁc to VM Y . The
white bars represent the total throughput in the respective setups.
For Fig. 5, X and Y are located on different servers and the con-
gestion occurs on a core network link. Fig. 6 presents results for a
different scenario, in which X and Y are on the same server and
senders to Y blast UDP ﬂows that are unresponsive to congestion.
(For brevity, we omit other combinations of TCP/UDP trafﬁc and
congestion on edge/core, which exhibit similar results.) We ran the
experiment for 30 seconds and X uses a single TCP ﬂow.
Figures 5 and 6 show that ElasticSwitch provides the intended
guarantees, even when the number of senders to Y is very high,
and, at the same time, ElasticSwitch is able to give X the entire
link capacity when no VMs are sending trafﬁc to Y . VM Y also
achieves its guarantee, as shown by the plotted total throughput;
however, for more than 100 senders, TCP’s efﬁciency in utilizing
the link decreases, since some of Y ’s ﬂows experience drops and
timeouts and do not always fully utilize their allocated guarantees.
Figure 7: Many-to-one fairness. Fairness between the ﬂows
sending trafﬁc to Y in Fig. 5 and Fig. 6.
For the scenario in Fig. 5, since the congestion does not occur on
the destination server’s access link, Gatekeeper [20] and EyeQ [12]
would not be able to provide guarantees; we believe these solutions
would perform like NoProtection in such scenarios. Seawall [22]
would also perform like NoProtection for all many-to-one settings.
Fig. 7 shows the Jain’s Fairness Index computed between the
application level throughput of the ﬂows sending trafﬁc to VM Y .
We can see that ElasticSwitch achieves better fairness than regular
TCP and also provides fairness when senders use UDP ﬂows.
MapReduce scenario: We emulate just the shufﬂe phase of
MapReduce jobs, and measure throughputs and completion times.
For easier interpretation of the results, we use a subset of the
testbed, such that the topology is symmetric—i.e., the oversub-
scription to the core is the same for all hosts. We use 44 servers
where network has an oversubscription ratio of 4:1. We use 4 VM
slots per server, for a total of 176 VMs.
We create multiple tenants, with random sizes from 2 to 30 VMs;
half of the VMs act as mappers and half as reducers. All VMs of
each tenant are provisioned with the same hose-model bandwidth
guarantee, equal to the fair share of the bandwidth to the root of the
topology. This translates into a guarantee of 56.25Mbps (with 10%
headroom). We test with two different placement strategies: (i)
“random”: all VMs of all tenants are uniformly randomly mapped
to server VM slots, and (ii) “unbalanced”: mapper VMs of tenants
are placed starting from the left corner of the tree and reduce VMs
are placed starting from the right corner of the tree. The “unbal-
anced” case stresses the core of the network. We also test a “light”
case, where fewer tenants are created, such that about 10% of the
total VM slots are ﬁlled, with “random” VM placement. We use a
single TCP ﬂow between a mapper and a reducer of a tenant.
Fig. 8(a) plots the throughput recorded by each individual re-
ducer when all jobs are active. The horizontal bars at 56.25Mbps
denote the throughput achieved with a non-work-conserving sys-
tem like Oktopus. As one can see, ElasticSwitch fully satisﬁes
the guarantees in all cases (the throughput is never lower than the
reservation). As expected, when using NoProtection, many VMs
get less than the desired guarantees. In fact, for the “unbalanced”
setup, 30% of the VMs achieve lower throughputs, some as low as
1% of the guarantee value.
Fig. 8(a) also shows that ElasticSwitch exploits unused band-
width in the network and achieves signiﬁcantly higher throughputs
than an Oktopus-like static reservation system. Even in the case
when all VMs are active, and not all VMs on the same machine are
mappers or reducers, with MapReduce’s unidirectional demand,
there is unutilized bandwidth in the network (in the “unbalanced”
scenarios there is very little available bandwidth). However, the av-
erage throughput achieved by ElasticSwitch is lower than NoPro-
358ElasticSwitch VMs
No Protection VMs 
Oktopus−like Res.
Averages
s
p
b
M
1000
900
800
700
600
500
400
300
200
100
0
ElasticSwitch
No Protection
random
random
ElasticSwitch
unbalanced
No Protection
unbalanced
ElasticSwitch
No Protection
light
light
1
0.8
0.6
0.4
0.2
0
No Protection − random
No Protection − unbalanced
ElasticSwitch − random
ElasticSwitch − unbalanced
ElasticSwitch − light
No Protection − light
0.2
0.5
Ratio to Oktopus−like Res. Shuffle Time 
1
5
(a) Throughput of individual reducers
(b) CDF of shufﬂe time under worst case assumptions
Figure 8: MapReduce experiment. (a) shows the throughput of each individual reducer. (b) shows the ratio between the worst case