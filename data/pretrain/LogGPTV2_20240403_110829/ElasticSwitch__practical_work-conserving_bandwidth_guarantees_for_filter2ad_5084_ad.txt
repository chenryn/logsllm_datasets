### ECN as an Alternative to Sequence-Number Scheme

ECN (Explicit Congestion Notification) can serve as an alternative to our sequence-number scheme. When available, ECN enhances congestion detection and improves the ability to provide guarantees and maintain work-conservation (as discussed in §7). To prevent interference with TCP, we disabled ECN on the hosts, and set the ECN capable bits on all protocols (e.g., UDP). For this purpose, we developed a patch for Open vSwitch that triggers congestion feedback based on ECN bits at the destination. To ensure accurate congestion detection, we configured the switches to mark packets with a high probability (100%) when the queue exceeds a certain threshold.

### Evaluation

The goals of this evaluation are:
1. Demonstrate that ElasticSwitch provides guarantees under worst-case scenarios and identify its limitations.
2. Show that ElasticSwitch is work-conserving (i.e., it can improve utilization when some VMs are inactive).
3. Explore ElasticSwitch’s sensitivity to various parameters.
4. Quantify ElasticSwitch’s overhead in terms of CPU, latency, and bandwidth.

#### Summary of Results

Our experiments show that:
- **Guarantees**: ElasticSwitch can achieve the intended guarantees even under the most challenging conditions, such as when traffic from 300 VMs competes with traffic from a single VM, or when multiple VMs run large MapReduce jobs simultaneously. Without guarantees, the completion time of these jobs could be two orders of magnitude longer.
- **Congestion Independence**: ElasticSwitch provides guarantees regardless of where congestion occurs in the network.
- **Work-Conservation**: ElasticSwitch achieves between 75-99% of the optimal link utilization. It can be tuned to be more work-conserving, but this may slightly degrade the ability to provide guarantees in challenging conditions.
- **Latency Impact**: ElasticSwitch’s work-conservation can increase completion times for short flows by up to 0.7ms compared to static reservations. However, the additional latency is no worse than when the link is fully utilized.
- **Parameter Sensitivity**: ElasticSwitch is not sensitive to small changes in parameters.
- **ECN Support**: ECN support enhances all results and makes our improvements to the rate allocation algorithm less critical.

#### Experimental Setup

We used approximately 100 servers from a larger testbed (the actual number varied over time). Each server has four 3GHz Intel Xeon X3370 CPUs and 8GB of memory. We conducted our experiments on a parallel, isolated network to prevent interference with other traffic in the testbed. The parallel network is a two-level, single-rooted tree with all links operating at 1Gbps. By selecting different subsets of servers, we created various oversubscription ratios.

To avoid virtualization overhead, we emulated multiple VMs by creating multiple virtual interfaces, each with its own IP address, connected to the kernel’s virtual switch. Each workload generator on a host binds to a different IP address, thus emulating VMs with virtual interfaces.

We compared ElasticSwitch with two other approaches:
- **No-Protection**: Sending traffic directly without bandwidth protection.
- **Oktopus-like Reservation**: A non-work-conserving reservation system with rates statically set to be optimal for the given workload to achieve the hose model. This is an idealized version of Oktopus; in practice, Oktopus is likely to perform worse than this idealized version.

We also made qualitative comparisons with Gatekeeper [20] and EyeQ [12], which cannot provide guarantees when the network core is congested.

#### Parameters

- **Headroom**: 10% between the link capacity and the maximum allocated guarantees.
- **Rate Allocation Period**: 15ms
- **Guarantee Partitioning Period**: 60ms
- **Seawall**:
  - Weight \( w = 450 \text{Mbps} / BX→Y \)
  - Rate decrease constant \( \alpha = 0.4 \) (rate decreases to 60% after a congestion event)
  - \( \delta = 0.75 \)
  - Rate-increase constant \( A = 0.5 \text{Mbps} \)
  - Physical time scaling factor \( TS = 1.5 \)
  - Time difference \( \Delta t = dt / TS \), where \( dt \) is the physical time difference
- **Rate-Caution**:
  - \( C_{\text{min}} = 0.3 \)
  - \( C = 0.5 \)
- **Hold-Increase**: Implemented using an exponential decay of packet loss history with a decay factor \( \gamma_w \), where \( \gamma = 0.75 \).

### Guarantees and Work Conservation

We demonstrate that ElasticSwitch provides bandwidth guarantees and is work-conserving. ElasticSwitch ensures guarantees even when the entire network capacity is reserved and all VMs are fully utilizing their guarantees. In less-congested conditions, ElasticSwitch offers guarantees (which we do not show for brevity).

#### Many vs. One Scenario

Two VMs, X and Y, belonging to different tenants, compete for a given link. Y receives traffic from multiple sources (e.g., Y is a MapReduce reducer), while X receives traffic from a single remote VM. Both X and Y have the same hose bandwidth guarantee of 450Mbps. Given our 10% slack in providing guarantees, these represent the maximum guarantees that can be offered on a 1Gbps link.

**Fig. 5** shows the application-level TCP throughput for VM X as the number of VMs sending TCP traffic to VM Y varies. The white bars represent the total throughput in the respective setups. In Fig. 5, X and Y are located on different servers, and congestion occurs on a core network link.

**Fig. 6** presents results for a scenario where X and Y are on the same server, and senders to Y blast UDP flows that are unresponsive to congestion. (For brevity, we omit other combinations of TCP/UDP traffic and congestion on edge/core, which exhibit similar results.) The experiment ran for 30 seconds, and X uses a single TCP flow.

Figs. 5 and 6 show that ElasticSwitch provides the intended guarantees, even when the number of senders to Y is very high. Additionally, ElasticSwitch can give X the entire link capacity when no VMs are sending traffic to Y. VM Y also achieves its guarantee, as shown by the plotted total throughput. However, for more than 100 senders, TCP's efficiency in utilizing the link decreases due to drops and timeouts, and some of Y's flows do not always fully utilize their allocated guarantees.

**Fig. 7** shows the Jain’s Fairness Index computed between the application-level throughput of the flows sending traffic to VM Y. ElasticSwitch achieves better fairness than regular TCP and also provides fairness when senders use UDP flows.

#### MapReduce Scenario

We emulate the shuffle phase of MapReduce jobs and measure throughputs and completion times. For easier interpretation, we use a subset of the testbed with a symmetric topology, ensuring the oversubscription to the core is the same for all hosts. We use 44 servers with an oversubscription ratio of 4:1 and 4 VM slots per server, totaling 176 VMs.

We create multiple tenants with random sizes from 2 to 30 VMs; half act as mappers and half as reducers. All VMs of each tenant are provisioned with the same hose-model bandwidth guarantee, equal to the fair share of the bandwidth to the root of the topology (56.25Mbps with 10% headroom). We test with two placement strategies:
- **Random**: All VMs of all tenants are uniformly randomly mapped to server VM slots.
- **Unbalanced**: Mapper VMs of tenants are placed starting from the left corner of the tree, and reduce VMs are placed starting from the right corner of the tree, stressing the core of the network.

We also test a "light" case where fewer tenants are created, filling about 10% of the total VM slots with "random" VM placement. We use a single TCP flow between a mapper and a reducer of a tenant.

**Fig. 8(a)** plots the throughput recorded by each individual reducer when all jobs are active. The horizontal bars at 56.25Mbps denote the throughput achieved with a non-work-conserving system like Oktopus. As seen, ElasticSwitch fully satisfies the guarantees in all cases (throughput is never lower than the reservation). With NoProtection, many VMs get less than the desired guarantees. In the "unbalanced" setup, 30% of the VMs achieve lower throughputs, some as low as 1% of the guarantee value.

**Fig. 8(a)** also shows that ElasticSwitch exploits unused bandwidth in the network, achieving significantly higher throughputs than an Oktopus-like static reservation system. Even when all VMs are active, and not all VMs on the same machine are mappers or reducers, there is unutilized bandwidth in the network. However, the average throughput achieved by ElasticSwitch is lower than NoProtection in some cases.

**Fig. 8(b)** shows the cumulative distribution function (CDF) of shuffle time under worst-case assumptions, comparing the performance of ElasticSwitch and NoProtection in different scenarios.