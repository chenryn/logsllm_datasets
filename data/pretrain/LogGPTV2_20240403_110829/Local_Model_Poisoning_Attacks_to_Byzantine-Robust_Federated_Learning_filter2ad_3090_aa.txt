# Title: Local Model Poisoning Attacks on Byzantine-Robust Federated Learning

## Authors:
- Minghong Fang, Iowa State University
- Xiaoyu Cao, Duke University
- Jinyuan Jia, Duke University
- Neil Zhenqiang Gong, Duke University

## Abstract
In federated learning, multiple client devices collaboratively train a machine learning model. Each client maintains a local model for its training data, and a master device aggregates these local models to form a global model. Recent methods have been proposed to make federated learning robust against Byzantine failures (e.g., system failures, adversarial manipulations) in some client devices. In this work, we conduct the first systematic study of local model poisoning attacks in federated learning. We assume an attacker has compromised some client devices and manipulates their local model parameters during the learning process to degrade the global model's performance. We formulate our attacks as optimization problems and apply them to four recent Byzantine-robust federated learning methods. Our empirical results on four real-world datasets show that our attacks can significantly increase the error rates of the models learned by these methods. We also generalize two defenses for data poisoning attacks to defend against our local model poisoning attacks. Our evaluation results indicate that while one defense is effective in some cases, it is not sufficient in others, highlighting the need for new defenses against local model poisoning attacks in federated learning.

## 1. Introduction
### Byzantine-Robust Federated Learning
Federated learning (also known as collaborative learning) [32, 39] involves decentralized training data across multiple client devices (e.g., desktops, mobile phones, IoT devices). These clients do not share their local training data but still want to jointly learn a model. For example, multiple hospitals might want to learn a healthcare model without sharing sensitive data. Each client device (worker) maintains a local model for its training data, and a master device (e.g., a cloud server) maintains a global model. Federated learning typically involves three steps:
1. The master device sends the current global model to the worker devices.
2. Worker devices update their local models using their training data and the global model, then send the updated local models back to the master device.
3. The master device aggregates the local models to compute a new global model.

Common aggregation rules, such as mean, are vulnerable to adversarial manipulation if even one worker device is compromised [9, 66]. To address this, several Byzantine-robust aggregation rules (e.g., Krum [9], Bulyan [42], trimmed mean [66], and median [66]) have been proposed to ensure robustness against failures in some worker devices.

### Existing Data Poisoning Attacks
Data poisoning attacks aim to manipulate the training phase so that the learned model (a classifier) has a high testing error rate, making the model unusable and leading to denial-of-service attacks. These attacks typically inject malicious data into the training dataset before the learning process starts, assuming the learning process itself is intact. In federated learning, an attacker can only inject malicious data into the worker devices under their control, limiting the effectiveness of such attacks against Byzantine-robust federated learning (see Section 4.4 for experimental results).

### Our Work
We perform the first study on local model poisoning attacks in Byzantine-robust federated learning. Unlike existing studies [9, 66], which focus on non-robust mean aggregation, we target robust aggregation rules.

#### Threat Model
We aim to compromise the integrity of the learning process rather than the training dataset collection. We assume the attacker controls some worker devices and manipulates the local model parameters sent to the master device during the learning process. The attacker may or may not know the aggregation rule used by the master device. We call these attacks "local model poisoning attacks" because they directly manipulate the local model parameters.

#### Local Model Poisoning Attacks
A key challenge is crafting the local models sent from compromised worker devices. We formulate this as an optimization problem in each iteration of federated learning. Specifically, we aim to craft local models such that the global model deviates maximally from the direction in which the global model would change without attacks. We apply our attacks to four recent Byzantine-robust federated learning methods: Krum, Bulyan, trimmed mean, and median.

Our evaluation on MNIST, Fashion-MNIST, CH-MNIST, and Breast Cancer Wisconsin (Diagnostic) datasets shows that our attacks can significantly increase the error rates of the global models. For example, when using Krum to learn a deep neural network classifier for MNIST, our attack increases the error rate from 0.11 to 0.75. We also compare our attacks with label flipping and back-gradient optimization-based attacks, finding that these data poisoning attacks have limited success against Byzantine-robust federated learning methods.

#### Defenses
Existing defenses against data poisoning attacks aim to sanitize the training dataset. We generalize two such defenses, RONI [4] and TRIM [30], to defend against local model poisoning attacks. Both defenses remove potentially malicious local models before computing the global model using a Byzantine-robust aggregation rule. One defense removes local models with large negative impact on the global model's error rate (Error Rate based Rejection, ERR), and the other removes local models that result in large loss (Loss Function based Rejection, LFR). We also combine ERR and LFR, removing local models that are flagged by either defense.

Our empirical results show that LFR outperforms ERR, and the combined defense is comparable to LFR in most cases. However, LFR is effective in some scenarios (e.g., trimmed mean) but not in others (e.g., Krum). This highlights the need for new defense mechanisms against local model poisoning attacks.

### Key Contributions
- We perform the first systematic study on local model poisoning attacks in Byzantine-robust federated learning.
- We propose local model poisoning attacks that manipulate local model parameters on compromised worker devices.
- We generalize two defenses for data poisoning attacks to defend against local model poisoning attacks. Our results show that while one defense is effective in some cases, they have limited success in others.

## 2. Background and Problem Formulation
### 2.1 Federated Learning
Suppose we have \( m \) worker devices, each with a local training dataset \( D_i \). The goal is to collaboratively learn a classifier. The model parameters \( w \) are obtained by solving the optimization problem:
\[ \min_w \sum_{i=1}^m F(w, D_i) \]
where \( F(w, D_i) \) is the objective function for the local training dataset on the \( i \)-th device. In federated learning, each worker device maintains a local model for its training data, and a master device maintains a global model by aggregating local models. Federated learning performs the following steps in each iteration:

1. **Step I**: The master device sends the current global model parameters to all worker devices.
2. **Step II**: Worker devices update their local model parameters using the global model parameters and their local training datasets. The \( i \)-th worker device updates its local model parameters \( w_i \) as:
   \[ w_i = w - \alpha \cdot \frac{\partial F(w, B_i)}{\partial w} \]
   where \( \alpha \) is the learning rate and \( B_i \) is a randomly sampled batch from \( D_i \). After updating, the worker devices send their local models to the master device.
3. **Step III**: The master device aggregates the local models to obtain a new global model according to a certain aggregation rule:
   \[ w = A(w_1, w_2, \ldots, w_m) \]

### 2.2 Byzantine-Robust Aggregation Rules
A naive aggregation rule is to average the local model parameters, but this is not robust under adversarial settings. Several robust aggregation rules have been developed:

- **Krum** [9]: Selects one of the \( m \) local models that is similar to other models as the global model. It computes the sum of distances between each local model and its closest \( m - c - 2 \) local models and selects the one with the smallest sum.
- **Bulyan** [42]: Combines Krum and a variant of trimmed mean. It iteratively applies Krum to select \( \theta \) local models and then uses a variant of trimmed mean to aggregate them.
- **Trimmed Mean** [66]: Aggregates each model parameter independently by sorting the parameters, removing the largest and smallest \( \beta \) of them, and computing the mean of the remaining parameters.
- **Median** [66]: Sorts the parameters and takes the median as the global model parameter.

### 2.3 Problem Definition and Threat Model
#### Attackerâ€™s Goal
The attacker aims to manipulate the learned global model to achieve a high error rate for testing examples, making the model unusable. Such attacks are known as untargeted poisoning attacks.