title:Local Model Poisoning Attacks to Byzantine-Robust Federated Learning
author:Minghong Fang and
Xiaoyu Cao and
Jinyuan Jia and
Neil Zhenqiang Gong
Local Model Poisoning Attacks to Byzantine-Robust 
Federated Learning
Minghong Fang, Iowa State University; Xiaoyu Cao, Jinyuan Jia, and 
Neil Gong, Duke University
https://www.usenix.org/conference/usenixsecurity20/presentation/fang
This paper is included in the Proceedings of the 29th USENIX Security Symposium.August 12–14, 2020978-1-939133-17-5Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX.Local Model Poisoning Attacks to Byzantine-Robust Federated Learning
Minghong Fang∗1, Xiaoyu Cao∗2, Jinyuan Jia2, Neil Zhenqiang Gong2
1CS Department, Iowa State University, 2ECE Department, Duke University
PI:EMAIL, 2{xiaoyu.cao, jinyuan.jia, neil.gong}@duke.edu
Abstract
In federated learning, multiple client devices jointly learn a
machine learning model: each client device maintains a local
model for its local training dataset, while a master device
maintains a global model via aggregating the local models
from the client devices. The machine learning community
recently proposed several federated learning methods that
were claimed to be robust against Byzantine failures (e.g.,
system failures, adversarial manipulations) of certain client
devices. In this work, we perform the ﬁrst systematic study
on local model poisoning attacks to federated learning. We
assume an attacker has compromised some client devices,
and the attacker manipulates the local model parameters on
the compromised client devices during the learning process
such that the global model has a large testing error rate. We
formulate our attacks as optimization problems and apply
our attacks to four recent Byzantine-robust federated learning
methods. Our empirical results on four real-world datasets
show that our attacks can substantially increase the error rates
of the models learnt by the federated learning methods that
were claimed to be robust against Byzantine failures of some
client devices. We generalize two defenses for data poisoning
attacks to defend against our local model poisoning attacks.
Our evaluation results show that one defense can effectively
defend against our attacks in some cases, but the defenses are
not effective enough in other cases, highlighting the need for
new defenses against our local model poisoning attacks to
federated learning.
1 Introduction
Byzantine-robust federated learning: In federated learn-
ing (also known as collaborative learning) [32, 39], the
training dataset is decentralized among multiple client de-
vices (e.g., desktops, mobile phones, IoT devices), which
could belong to different users or organizations. These
users/organizations do not want to share their local training
∗Equal contribution. Minghong Fang performed this research when he
was under the supervision of Neil Zhenqiang Gong.
(cid:2)(cid:7)(cid:21)(cid:7)(cid:1)(cid:18)(cid:17)(cid:12)(cid:20)(cid:17)(cid:16)(cid:12)(cid:16)(cid:11)
(cid:7)(cid:21)(cid:21)(cid:7)(cid:8)(cid:13)(cid:20)
(cid:62)(cid:381)(cid:272)(cid:258)(cid:367)(cid:3)(cid:373)(cid:381)(cid:282)(cid:286)(cid:367)(cid:1)
(cid:393)(cid:381)(cid:349)(cid:400)(cid:381)(cid:374)(cid:349)(cid:374)(cid:336)(cid:3)(cid:7)(cid:21)(cid:21)(cid:7)(cid:8)(cid:13)(cid:20)
(cid:6)(cid:19)(cid:7)(cid:12)(cid:16)(cid:12)(cid:16)(cid:11)(cid:1)(cid:9)(cid:7)(cid:21)(cid:7)(cid:1)
(cid:8)(cid:17)(cid:14)(cid:14)(cid:10)(cid:8)(cid:21)(cid:12)(cid:17)(cid:16)
(cid:3)(cid:10)(cid:7)(cid:19)(cid:16)(cid:12)(cid:16)(cid:11)(cid:1)
(cid:18)(cid:19)(cid:17)(cid:8)(cid:10)(cid:20)(cid:20)
(cid:4)(cid:17)(cid:9)(cid:10)(cid:14)
Figure 1: Data vs. local model poisoning attacks.
datasets, but still desire to jointly learn a model. For instance,
multiple hospitals may desire to learn a healthcare model
without sharing their sensitive data to each other. Each client
device (called worker device) maintains a local model for its
local training dataset. Moreover, the service provider has a
master device (e.g., cloud server), which maintains a global
model. Roughly speaking, federated learning repeatedly per-
forms three steps: the master device sends the current global
model to worker devices; worker devices update their local
models using their local training datasets and the global model,
and send the local models to the master device; and the master
device computes a new global model via aggregating the local
models according to a certain aggregation rule.
For instance, the mean aggregation rule that takes the aver-
age of the local model parameters as the global model is
widely used under non-adversarial settings. However, the
global model can be arbitrarily manipulated for mean even
if just one worker device is compromised [9, 66]. Therefore,
the machine learning community recently proposed multi-
ple aggregation rules (e.g., Krum [9], Bulyan [42], trimmed
mean [66], and median [66]), which aimed to be robust against
Byzantine failures of certain worker devices.
Existing data poisoning attacks are insufﬁcient: We con-
sider attacks that aim to manipulate the training phase of
machine learning such that the learnt model (we consider the
model to be a classiﬁer) has a high testing error rate indiscrim-
inately for testing examples, which makes the model unusable
and eventually leads to denial-of-service attacks. Figure 1
shows the training phase, which includes two components,
i.e., training dataset collection and learning process. The
training dataset collection component is to collect a training
dataset, while the learning process component produces a
model from a given training dataset. Existing attacks mainly
USENIX Association
29th USENIX Security Symposium    1623
inject malicious data into the training dataset before the learn-
ing process starts, while the learning process is assumed to
maintain integrity. Therefore, these attacks are often called
data poisoning attacks [8, 30, 33, 50, 56, 62]. In federated
learning, an attacker could only inject the malicious data into
the worker devices that are under the attacker’s control. As
a result, these data poisoning attacks have limited success to
attack Byzantine-robust federated learning (see our experi-
mental results in Section 4.4).
Our work: We perform the ﬁrst study on local model poison-
ing attacks to Byzantine-robust federated learning. Existing
studies [9, 66] only showed local model poisoning attacks to
federated learning with the non-robust mean aggregation rule.
Threat model. Unlike existing data poisoning attacks that
compromise the integrity of training dataset collection, we
aim to compromise the integrity of the learning process in
the training phase (see Figure 1). We assume the attacker
has control of some worker devices and manipulates the local
model parameters sent from these devices to the master device
during the learning process. The attacker may or may not
know the aggregation rule used by the master device. To
contrast with data poisoning attacks, we call our attacks local
model poisoning attacks as they directly manipulate the local
model parameters.
Local model poisoning attacks. A key challenge of local
model poisoning attacks is how to craft the local models
sent from the compromised worker devices to the master
device. To address this challenge, we formulate crafting local
models as solving an optimization problem in each iteration
of federated learning. Speciﬁcally, the master device could
compute a global model in an iteration if there are no attacks,
which we call before-attack global model. Our goal is to craft
the local models on the compromised worker devices such that
the global model deviates the most towards the inverse of the
direction along which the before-attack global model would
change. Our intuition is that the deviations accumulated over
multiple iterations would make the learnt global model differ
from the before-attack one signiﬁcantly. We apply our attacks
to four recent Byzantine-robust federated learning methods
including Krum, Bulyan, trimmed mean, and median.
Our evaluation results on the MNIST, Fashion-MNIST, CH-
MNIST, and Breast Cancer Wisconsin (Diagnostic) datasets
show that our attacks can substantially increase the error rates
of the global models under various settings of federated learn-
ing. For instance, when learning a deep neural network clas-
siﬁer for MNIST using Krum, our attack can increase the
error rate from 0.11 to 0.75. Moreover, we compare with data
poisoning attacks including label ﬂipping attacks and back-
gradient optimization based attacks [43] (state-of-the-art un-
targeted data poisoning attacks for multi-class classiﬁers),
which poison the local training datasets on the compromised
worker devices. We ﬁnd that these data poisoning attacks
have limited success to attack the Byzantine-robust federated
learning methods.
Defenses. Existing defenses against data poisoning attacks
essentially aim to sanitize the training dataset. One category
of defenses [4, 15, 56, 59] detects malicious data based on
their negative impact on the error rate of the learnt model. For
instance, Reject on Negative Impact (RONI) [4] measures the
impact of each training example on the error rate of the learnt
model and removes the training examples that have large
negative impact. Another category of defenses [20, 30, 35]
leverages new loss functions, solving which detects malicious
data and learns a model simultaneously. For instance, Jagielski
et al. [30] proposed TRIM, which aims to jointly ﬁnd a subset
of training dataset with a given size and model parameters that
minimize the loss function. The training examples that are not
in the selected subset are treated as malicious data. However,
these defenses are not directly applicable for our local model
poisoning attacks because our attacks do not inject malicious
data into the training dataset.
To address the challenge, we generalize RONI and TRIM
to defend against our local model poisoning attacks. Both de-
fenses remove the local models that are potentially malicious
before computing the global model using a Byzantine-robust
aggregation rule in each iteration. One defense removes the
local models that have large negative impact on the error rate
of the global model (inspired by RONI that removes training
examples that have large negative impact on the error rate of
the model), while the other defense removes the local models
that result in large loss (inspired by TRIM that removes the
training examples that have large negative impact on the loss),
where the error rate and loss are evaluated on a validation
dataset. We call the two defenses Error Rate based Rejection
(ERR) and Loss Function based Rejection (LFR), respectively.
Moreover, we combine ERR and LFR, i.e., we remove the
local models that are removed by either ERR or LFR. Our
empirical evaluation results show that LFR outperforms ERR;
and the combined defense is comparable to LFR in most
cases. Moreover, LFR can defend against our attacks in cer-
tain cases, but LFR is not effective enough in other cases. For
instance, LFR can effectively defend against our attacks that
craft local models based on the trimmed mean aggregation
rule, but LFR is not effective against our attacks that are based
on the Krum aggregation rule. Our results show that we need
new defense mechanisms to defend against our local model
poisoning attacks.
Byzantine-robust federated learning.
Our key contributions can be summarized as follows:
• We perform the ﬁrst systematic study on attacking
• We propose local model poisoning attacks to Byzantine-
robust federated learning. Our attacks manipulate the
local model parameters on compromised worker de-
vices during the learning process.
• We generalize two defenses for data poisoning attacks
to defend against local model poisoning attacks. Our
results show that, although one of them is effective in
some cases, they have limited success in other cases.
1624    29th USENIX Security Symposium
USENIX Association
2 Background and Problem Formulation
2.1 Federated Learning
Suppose we have m worker devices and the ith worker device
has a local training dataset Di. The worker devices aim to
collaboratively learn a classiﬁer. Speciﬁcally, the model pa-
rameters w of the classiﬁer are often obtained via solving the
following optimization problem: minw ∑m
i=1 F(w,Di), where
F(w,Di) is the objective function for the local training dataset
on the ith device and characterizes how well the parameters
w model the local training dataset on the ith device. Differ-
ent classiﬁers (e.g., logistic regression, deep neural networks)
use different objective functions. In federated learning, each
worker device maintains a local model for its local training
dataset. Moreover, we have a master device to maintain a
global model via aggregating local models from the m worker
devices. Speciﬁcally, federated learning performs the follow-
ing three steps in each iteration:
Step I. The master device sends the current global model
parameters to all worker devices.
Step II. The worker devices update their local model pa-
rameters using the current global model parameters and their
local training datasets in parallel. In particular, the ith worker
device essentially aims to solve the optimization problem
minwi F(wi,Di) with the global model parameters w as an
initialization of the local model parameters wi. A worker de-
vice could use any method to solve the optimization problem,
though stochastic gradient descent is the most popular one.
Speciﬁcally, the ith worker device updates its local model
parameters wi as wi = w− α· ∂F(w,Bi)
, where α is the learn-
∂w
ing rate and Bi is a randomly sampled batch from the local
training dataset Di. Note that a worker device could apply
stochastic gradient descent multiple rounds to update its local
model. After updating the local models, the worker devices
send them to the master device.
Step III. The master device aggregates the local models
from the worker devices to obtain a new global model ac-
cording to a certain aggregation rule. Formally, we have
w = A(w1,w2,··· ,wm).
The master device could also randomly pick a subset of
worker devices and send the global model to them; the picked
worker devices update their local models and send them to
the master device; and the master device aggregates the local
models to obtain the new global model [39]. We note that,
for the aggregation rules we study in this paper, sending local
models to the master device is equivalent to sending gradients
to the master device, who aggregates the gradients and uses
them to update the global model.
2.2 Byzantine-robust Aggregation Rules
A naive aggregation rule is to average the local model param-
eters as the global model parameters. This mean aggregation
rule is widely used under non-adversarial settings [16, 32, 39].
However, mean is not robust under adversarial settings. In
particular, an attacker can manipulate the global model param-
eters arbitrarily for this mean aggregation rule when compro-
mising only one worker device [9,66]. Therefore, the machine
learning community has recently developed multiple aggrega-
tion rules that aim to be robust even if certain worker devices
exhibit Byzantine failures. Next, we review several such ag-
gregation rules.
Krum [9] and Bulyan [42]: Krum selects one of the m local
models that is similar to other models as the global model.
The intuition is that even if the selected local model is from a
compromised worker device, its impact may be constrained
since it is similar to other local models possibly from be-
nign worker devices. Suppose at most c worker devices are
compromised. For each local model wi, the master device
computes the m− c− 2 local models that are the closest to
wi with respect to Euclidean distance. Moreover, the master
device computes the sum of the distances between wi and its
closest m− c− 2 local models. Krum selects the local model
with the smallest sum of distance as the global model. When
c < m−2
2 , Krum has theoretical guarantees for the convergence
for certain objective functions.
Euclidean distance between two local models could be
substantially inﬂuenced by a single model parameter. There-
fore, Krum could be inﬂuenced by some abnormal model
parameters [42]. To address this issue, Mhamdi et al. [42]
proposed Bulyan, which essentially combines Krum and a
variant of trimmed mean (trimmed mean will be discussed
next). Speciﬁcally, Bulyan ﬁrst iteratively applies Krum to se-
lect θ (θ ≤ m− 2c) local models. Then, Bulyan uses a variant
of trimmed mean to aggregate the θ local models. In particular,
for each jth model parameter, Bulyan sorts the jth parameters
of the θ local models, ﬁnds the γ (γ ≤ θ− 2c) parameters that
are the closest to the median, and computes their mean as the
jth parameter of the global model. When c ≤ m−3
4 , Bulyan
has theoretical guarantees for the convergence under certain
assumptions of the objective function.
Since Bulyan is based on Krum, our attacks for Krum can
transfer to Bulyan (see Appendix A). Moreover, Bulyan is
not scalable because it executes Krum many times in each
iteration and Krum computes pairwise distances between
local models. Therefore, we will focus on Krum in the paper.
Trimmed mean [66]: This aggregation rule aggregates each
model parameter independently. Speciﬁcally, for each jth
model parameter, the master device sorts the jth parameters
of the m local models, i.e., w1 j,w2 j,··· ,wm j, where wi j is the
jth parameter of the ith local model, removes the largest and
smallest β of them, and computes the mean of the remaining
m− 2β parameters as the jth parameter of the global model.
Suppose at most c worker devices are compromised. This
trimmed mean aggregation rule achieves order-optimal error
rate when c ≤ β < m
2 and the objective function to be mini-
mized is strongly convex. Speciﬁcally, the order-optimal error
USENIX Association
29th USENIX Security Symposium    1625
n
+ 1√
mn
√
rate is ˜O( c
),1 where n is the number of training
m
data points on a worker device (worker devices are assumed
to have the same number of training data points).
Median [66]: In this median aggregation rule, for each jth
model parameter, the master device sorts the jth parameters of
the m local models and takes the median as the jth parameter
of the global model. Note that when m is an even number,
median is the mean of the middle two parameters. Like the
trimmed mean aggregation rule, the median aggregation rule
also achieves an order-optimal error rate when the objective
function is strongly convex.
2.3 Problem Deﬁnition and Threat Model
Attacker’s goal: Like many studies on poisoning attacks [7,
8, 30, 33, 50, 62, 65], we consider an attacker’s goal is to ma-
nipulate the learnt global model such that it has a high error
rate indiscriminately for testing examples. Such attacks are
known as untargeted poisoning attacks, which make the learnt