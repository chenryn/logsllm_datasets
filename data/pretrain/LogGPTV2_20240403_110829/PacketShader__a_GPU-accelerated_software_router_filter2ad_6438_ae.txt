Figure 12 shows the measured roundtrip latency of three cases:
(i) the CPU-only mode without batch processing, (ii) the CPU-only
mode with batch, and (iii) the CPU+GPU mode with both batch and
parallelization. Comparing the former two cases, batched packet I/O
204$500) into a free PCIe slot does not require extra cost. In case of
limited PCIe slot space, one may add multiple GPUs into a slot with
a PCIe switch. Although this approach would make the GPUs share
the PCIe bandwidth, it can be useful for less bandwidth-hungry ap-
plications (e.g., header-only packet processing like IP forwarding).
Horizontal scaling: We have mostly focused on a single-machine
router in this work. Our prototype router supports 8× 10 GbE ports
near 40 Gbps packet forwarding capacity for 64B IPv4 and IPv6
packets. In case more capacity or a larger number of ports are needed,
we can take a similar approach as suggested by RouteBricks and use
Valiant Load Balancing (VLB) [52] or direct VLB [19].
Power efﬁciency: In general, modern graphics cards (especially for
high-end models) require more power than x86 CPU counterparts.
The PacketShader server consumes 594W with two GTX480 cards
and 353W without graphics cards under full load, showing 68%
increase. The gap gets closer in an idle state: 327W and 260W,
respectively. We believe that the increased power consumption is
tolerable, considering the performance improvement from GPUs.
Opportunistic ofﬂoading: Our latency measurement results in Sec-
tion 6.4 show that GPU acceleration incurs higher delay than CPU at
low trafﬁc. To address this issue, we have implemented the concept
of opportunistic ofﬂoading, using CPU for low latency under light
load and exploiting GPU for high throughput when heavily loaded,
in our SSL acceleration project [27]. We plan to adopt this idea also
in PacketShader for future work.
8. RELATED WORK
Today’s multi-core network processors (NPs), combined with hard-
ware multi-threading support, effectively hide memory access la-
tency to handle large volume of trafﬁc. For example, Cisco’s Quan-
tumFlow processors have 40 packet process engines, capable of han-
dling up to 160 threads in parallel [4]. Similarly, Cavium Networks
OCTEON II processors have 32 cores [2], and Intel IXP2850 pro-
cessors have 16 microengines supporting 128 threads in total [46,
51]. We have realized the latency-hiding idea on commodity hard-
ware, by exploiting massively-parallel capability of GPUs. GPUs
also bring high computation power with full programmability, while
NPs typically rely on hard-wired blocks to handle compute-intensive
tasks, such as encryption and pattern matching.
Until recently, shared memory bus was the performance bottle-
neck of software routers. Bolla et al.
report that the forwarding
performance does not scale to the number of CPU cores due to FSB
clogging [15]. FSB clogging happens due to cache coherency snoops
in the multi-core architecture [54]. More recently, RouteBricks re-
ports 2-3x performance improvement by eliminating FSB. It shows
that a PC-based router can forward packets near 10 Gbps per ma-
chine. Moreover, RouteBricks breaks away from a single-box ap-
proach and scales the aggregate performance by clustering multiple
machines [19]. In contrast, PacketShader utilizes GPU as a cheap
source of computation capability and shows more than a factor of
four performance improvement over RouteBricks on similar hard-
ware. PacketShader could replace RB4, a cluster of four Route-
Bricks machines, with a single machine with better performance.
As a ubiquitous and cost-effective solution, GPUs have been widely
used not only for graphics rendering but also for scientiﬁc and data-
intensive workloads [5, 41]. Recently, GPUs have shown a sub-
stantial performance boost to many network-related workloads, in-
cluding pattern matching [35, 48, 53], network coding [47], IP table
lookup [35], and cryptography [24,32,49]. In our work, we have ex-
plored a general GPU acceleration framework as a complete system.
Figure 12: Average roundtrip latency for IPv6 forwarding
shows even lower latency; better forwarding rate with batched I/O
effectively reduces the queueing delay of packets. We suspect that
higher latency numbers at low input trafﬁc rate are caused by the
effect of interrupt moderation in NICs [28]. Comparing the latter
two cases, GPU acceleration causes higher latency due to GPU trans-
action overheads and additional queueing (input and output queues
in Figure 9), yet still showing a reasonable range (200–400µs in the
ﬁgure, 140–260µs for IPv4 forwarding).
Since our packet generator is not a hardware-based measuring
instrument, the measurement result has two limitations: (i) measured
latency numbers include delays incurred by the generator itself, and
(ii) the generator only supports up to 28 Gbps due to overheads of
measurement and rate limiting.
7. DISCUSSION
So far we have demonstrated that GPUs effectively bring extra
processing capacity to software routers. We discuss related issues,
the current limitations, and future directions here.
Integration with a control plane: Our immediate next step is to
integrate control plane. We believe it is relatively straightforward
to integrate Zebra [6] or Quagga [11] into PacketShader as other
software routers. One issue arising here is how to update forwarding
table in GPU memory without disturbing the data-path performance.
This problem is not speciﬁc only to PacketShader, but also relevant
to FIB update in traditional hardware-based routers.
Incremental
update or double buffering could be possible solutions.
Multi-functional, modular programming environment: Packet-
Shader currently limits one GPU kernel function execution at a time
per device. The multi-functionality support (e.g., IPv4 and IPsec
at the same time) in PacketShader enforces to implement all the
functions in a single GPU kernel. NVIDIA has recently added na-
tive support for concurrent execution of heterogeneous kernels into
GTX480 [8], and we plan to modify the PacketShader framework
to beneﬁt from this feature. GTX480 also supports C++ and func-
tion pointers in kernel. We believe it will expedite our effort to
implement a Click-like modular programming environment [30] in
PacketShader.
Vertical scaling: As an alternative to using GPUs, it is possible to
have more CPUs to scale up the single-box performance. However,
having more CPUs in a single machine is not cost-effective due to
upgrade of the motherboard, additional memory installation, and
diminishing price/performance ratio. The CPU price per gigahertz
in a single-socket machine6 is $23 at present. The price goes up
with more CPU sockets: $87 in a dual-socket machine7 and $183 in
a quad-socket machine8.
In contrast, installing a cheap GPU (the price ranges from $50 to
6$240 per Core i7 920 processor (2.66 GHz, 4 cores)
7$925 per Xeon X5550 processor (2.66 GHz, 4 cores)
8$2190 per Xeon E7540 processor (2.00 GHz, 6 cores)
2059. CONCLUSIONS
We have presented PacketShader, a novel framework for high-
performance network packet processing on commodity hardware.
We minimize per-packet processing overhead in network stack and
perform packet processing in the user space without serious perfor-
mance penalty. On top of our optimized packet I/O engine, Packet-
Shader brings GPUs to ofﬂoad computation and memory-intensive
workloads, exploiting the massively-parallel processing capability
of GPU for high-performance packet processing. Careful design
choices make PacketShader to be highly scalable with multi-core
CPUs, high-speed NICs, and GPUs in NUMA systems. We have
demonstrated the effectiveness of our approach with IPv4 and IPv6
forwarding, OpenFlow switching, and IPsec tunneling.
In this work we have shown that a well-designed PC-based router
can achieve 40 Gbps forwarding performance with full programma-
bility even on today’s commodity hardware. We believe Packet-
Shader will serve as a useful platform for scalable software routers
on commodity hardware.
10. ACKNOWLEDGEMENT
We thank Katerina Arguraki, Vivek Pai, Seungyeop Han, anony-
mous reviewers, and our shepherd Robert Morris for their help and
invaluable comments. This research was funded by KAIST High
Risk High Return Project (HRHRP), NAP of Korea Research Coun-
cil of Fundamental Science & Technology, and MKE (Ministry of
Knowledge Economy of Repbulic of Korea, project no. N02100053).
11. REFERENCES
[1] AMD Fusion. http://fusion.amd.com.
[2] Cavium Networks OCTEON II processors.
http://www.caviumnetworks.com/OCTEON_II_MIPS64.html.
[3] Check Point IP Security Appliances.
http://www.checkpoint.com/products/ip-appliances/index.html.
[4] Cisco QuantumFlow Processors. http://www.cisco.com/en/US/prod/
collateral/routers/ps9343/solution_overview_c22-448936.html.
[5] General Purpose computation on GPUs. http://www.gpgpu.org.
[6] GNU Zebra project. http://www.zebra.org.
[7] NVIDIA CUDA GPU Computing Discussion Forum.
http://forums.nvidia.com/index.php?showtopic=104243.
[8] NVIDIA Fermi Architecture.
http://www.nvidia.com/object/fermi_architecture.html.
[9] OpenFlow Reference System.
http://www.openflowswitch.org/wp/downloads/.
[10] OpenFlow Switch Speciﬁcation, Version 0.8.9. http:
//www.openflowswitch.org/documents/openflow-spec-v0.8.9.pdf.
[11] Quagga project. http://www.quagga.net.
[12] Receive-Side Scaling Enhancements in Windows Server 2008.
http://www.microsoft.com/whdc/device/network/ndis_rss.mspx.
[13] The OpenFlow Switch Consortium. http://www.openflowswitch.org.
[14] University of Oregon RouteViews project. http://www.routeviews.org/.
[15] R. Bolla and R. Bruschi. PC-based software routers: High performance and
application service support. In ACM PRESTO, 2008.
[16] J. Bonwick. The slab allocator: an object-caching kernel memory allocator. In
USENIX Summer Technical Conference, 1994.
[17] S. Boyd-Wickizer, H. Chen, R. Chen, Y. Mao, F. Kaashoek, R. Morris,
A. Pesterev, L. Stein, M. Wu, Y. Dai, Y. Zhang, and Z. Zhang. Corey: An
operating system for many cores. In OSDI, 2008.
[18] T. Brecht, G. J. Janakiraman, B. Lynn, V. Saletore, and Y. Turner. Evaluating
network processing efﬁciency with processor partitioning and asynchronous i/o.
SIGOPS Oper. Syst. Rev., 40(4):265–278, 2006.
[19] M. Dobrescu, N. Egi, K. Argyraki, B.-G. Chun, K. Fall, G. Iannaccone, A. Knies,
M. Manesh, and S. Ratnasamy. RouteBricks: exploiting parallelism to scale
software routers. In SOSP, 2009.
[20] K. Fatahalian and M. Houston. A closer look at GPUs. Communications of the
ACM, 51:50–57, 2008.
[21] A. Foong, J. Fung, and D. Newell. An in-depth analysis of the impact of
processor afﬁnity on network performance. In IEEE ICON, 2004.
[22] P. Gupta, S. Lin, and N. McKeown. Routing lookups in hardware at memory
access speeds. In IEEE INFOCOM, 1998.
[23] S. Han, K. Jang, K. Park, and S. Moon. Building a single-box 100 gbps software
router. In IEEE Workshop on Local and Metropolitan Area Networks, 2010.
[24] O. Harrison and J. Waldron. Practical Symmetric Key Cryptography on Modern
Graphics Hardware. In USENIX Security, 2008.
[25] S. Hong and H. Kim. An analytical model for a GPU architecture with
memory-level and thread-level parallelism awareness. In ISCA, 2009.
[26] V. Jacobson, C. Leres, and S. McCanne. libpcap, Lawrence Berkeley Laboratory,
Berkeley, CA. http://www.tcpdump.org.
[27] K. Jang, S. Han, S. Moon, and K. Park. Converting your graphics card into
high-performance SSL accelerator. submitted for publication.
[28] G. Jin and B. L. Tierney. System capability effects on algorithms for network
bandwidth measurement. In IMC, 2003.
[29] D. Kim, J. Heo, J. Huh, J. Kim, and S. Yoon. HPCCD: Hybrid Parallel
Continuous Collision Detection using CPUs and GPUs. In Computer Graphics
Forum, volume 28, pages 1791–1800. John Wiley & Sons, 2009.
[30] E. Kohler, R. Morris, B. Chen, J. Jannotti, and M. F. Kaashoek. The Click
modular router. ACM TOCS, 18(3):263–297, 2000.
[31] Y. Liao, D. Yin, and L. Gao. PdP: parallelizing data plane in virtual network
substrate. In ACM VISA, 2009.
[32] S. Manavski. CUDA compatible GPU as an efﬁcient hardware accelerator for
AES cryptography. In IEEE Signal Processing and Communications, 2007.
[33] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar, L. Peterson,
J. Rexford, S. Shenker, and J. Turner. OpenFlow: enabling innovation in campus
networks. SIGCOMM CCR, 38(2):69–74, 2008.
[34] J. Mogul and K. Ramarkishnan. Eliminating Receive Livelock in an
Interrupt-Driven Kernel. ACM TOCS, 15(3):217–252, 1997.
[35] S. Mu, X. Zhang, N. Zhang, J. Lu, Y. S. Deng, and S. Zhang. Ip routing
processing with graphic processors. In Design, Automation & Test in Europe
Conference & Exhibition (DATE), 2010.
[36] J. Naous, D. Erickson, G. A. Covington, G. Appenzeller, and N. McKeown.
Implementing an OpenFlow switch on the NetFPGA platform. In ANCS, 2008.
[37] J. Nickolls, I. Buck, M. Garland, and K. Skadron. Scalable parallel programming
with CUDA. Queue, 6(2):40–53, 2008.
[38] NVIDIA Corporation. NVIDIA CUDA Best Practices Guide, Version 3.0.
[39] NVIDIA Corporation. NVIDIA CUDA Architecture Introduction and Overview,
2009.
[40] NVIDIA Corporation. NVIDIA CUDA Programming Guide, Version 3.0, 2009.
[41] J. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. Kr¨uger, A. E. Lefohn, and
T. J. Purcell. A Survey of General-Purpose Computation on Graphics Hardware.
In Eurographics 2005, State of the Art Reports, pages 21–51, Aug. 2005.
[42] K. D. Owens, D. Luebke, N. Govindaraju, M. Harris, J. KrÃijger, A. E. Lefohn,
and T. J. Purcell. A survey of general-purpose computation on graphics hardware.
Computer Graphics Forum, 26:80–113, 2007.
[43] S. Ryoo, C. I. Rodrigues, S. S. Baghsorkhi, S. S. Stone, D. B. Kirk, and W.-m. W.
Hwu. Optimization principles and application performance evaluation of a
multithreaded GPU using CUDA. In ACM PPoPP, 2008.
[44] J. H. Salim, R. Olsson, and A. Kuznetsov. Beyond softnet. In Annual Linux
Showcase & Conference, 2001.
[45] L. Seiler, D. Carmean, E. Sprangle, T. Forsyth, M. Abrash, P. Dubey, S. Junkins,
A. Lake, J. Sugerman, R. Cavin, et al. Larrabee: a many-core x86 architecture for
visual computing. In ACM SIGGRAPH, 2008.
[46] N. Shah, W. Plishker, K. Ravindran, and K. Keutzer. Np-click: A productive
software development approach for network processors. IEEE Micro,
24(5):45–54, 2004.
[47] H. Shojania, B. Li, and X. Wang. Nuclei: GPU-accelerated many-core network
coding. In IEEE INFOCOM, 2009.
[48] R. Smith, N. Goyal, J. Ormont, C. Estan, and K. Sankaralingam. Evaluating
GPUs for network packet signature matching. In IEEE ISPASS, 2009.
[49] R. Szerwinski and T. Güneysu. Exploiting the power of GPUs for asymmetric
cryptography. Cryptographic Hardware and Embedded Systems, pages 79–99,
2008.
[50] J. Torrellas, H. S. Lam, and J. L. Hennessy. False Sharing and Spatial Locality in
Multiprocessor Caches. IEEE Trans. on Computers, 43(6):651–663, 1994.
[51] J. S. Turner, P. Crowley, J. DeHart, A. Freestone, B. Heller, F. Kuhns, S. Kumar,
J. Lockwood, J. Lu, M. Wilson, C. Wiseman, and D. Zar. Supercharging
planetlab: a high performance, multi-application, overlay network platform.
SIGCOMM CCR, 37(4):85–96, 2007.
[52] L. G. Valiant and G. J. Brebner. Universal schemes for parallel communication.
In Proceedings of the ACM symposium on Theory of computing, 1981.
[53] G. Vasiliadis, S. Antonatos, M. Polychronakis, E. P. Markatos, and S. Ioannidis.
Gnort: High performance network intrusion detection using graphics processors.
In Proc. of Recent Advances in Intrusion Detection (RAID), 2008.
[54] B. Veal and A. Foong. Performance Scalability of a Multi-Core Web Server. In
ANCS, 2007.
[55] M. Waldvogel, G. Varghese, J. Turner, and B. Plattner. Scalable high speed IP
routing lookups. In SIGCOMM, 1997.
206