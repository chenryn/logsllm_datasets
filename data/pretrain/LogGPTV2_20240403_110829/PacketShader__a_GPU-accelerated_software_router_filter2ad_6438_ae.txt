### Figure 12: Measured Roundtrip Latency for Three Cases

Figure 12 illustrates the measured roundtrip latency for three different configurations:
1. CPU-only mode without batch processing,
2. CPU-only mode with batch processing, and
3. CPU+GPU mode with both batch processing and parallelization.

#### Comparison of CPU-Only Modes
When comparing the first two cases, it is evident that batched packet I/O significantly reduces the latency. This is because batch processing effectively decreases the queueing delay of packets. The higher latency observed at low input traffic rates in the CPU-only modes is likely due to interrupt moderation in the Network Interface Cards (NICs) [28].

#### Impact of GPU Acceleration
In the third case, the use of GPU acceleration introduces additional overhead, leading to higher latency compared to the CPU-only modes. This increased latency is attributed to the transaction overheads and additional queuing (input and output queues as shown in Figure 9). However, the latency remains within a reasonable range, typically between 200–400µs for IPv6 forwarding and 140–260µs for IPv4 forwarding.

### Power Efficiency
Modern graphics cards, especially high-end models, generally consume more power than their x86 CPU counterparts. For instance, the PacketShader server with two GTX480 cards consumes 594W under full load, which is a 68% increase compared to the 353W consumed without the graphics cards. In an idle state, the power consumption gap narrows to 327W with the GPUs and 260W without them. Despite this, the increased power consumption is considered acceptable given the significant performance improvements provided by the GPUs.

### Opportunistic Offloading
Our latency measurements in Section 6.4 reveal that GPU acceleration results in higher delays at low traffic levels compared to CPU processing. To address this, we have implemented opportunistic offloading, where the CPU handles low-latency tasks under light loads, and the GPU is utilized for high-throughput tasks under heavy loads. This concept has been successfully applied in our SSL acceleration project [27] and will be integrated into PacketShader for future work.

### Related Work
#### Multi-Core Network Processors
Today's multi-core network processors (NPs), combined with hardware multi-threading support, are effective in handling large volumes of traffic by hiding memory access latency. For example, Cisco’s QuantumFlow processors feature 40 packet processing engines capable of handling up to 160 threads in parallel [4]. Similarly, Cavium Networks OCTEON II processors have 32 cores [2], and Intel IXP2850 processors support 16 microengines with a total of 128 threads [46, 51]. We have achieved similar latency-hiding on commodity hardware by leveraging the massively-parallel capabilities of GPUs. Unlike NPs, which rely on hard-wired blocks for compute-intensive tasks, GPUs offer high computation power with full programmability.

#### Shared Memory Bus Bottleneck
Until recently, the shared memory bus was a significant bottleneck for software routers. Bolla et al. [15] reported that forwarding performance does not scale with the number of CPU cores due to Front Side Bus (FSB) clogging, caused by cache coherency snoops in multi-core architectures [54]. RouteBricks, however, demonstrated a 2-3x performance improvement by eliminating the FSB, achieving near 10 Gbps per machine [19]. RouteBricks also scales aggregate performance by clustering multiple machines. In contrast, PacketShader utilizes GPUs to achieve more than four times the performance of RouteBricks on similar hardware, potentially replacing a cluster of four RouteBricks machines with a single, more powerful machine.

#### GPU Applications
GPUs have become a ubiquitous and cost-effective solution, widely used not only for graphics rendering but also for scientific and data-intensive workloads [5, 41]. Recently, GPUs have shown substantial performance boosts in various network-related tasks, including pattern matching [35, 48, 53], network coding [47], IP table lookup [35], and cryptography [24, 32, 49]. Our work explores a general GPU acceleration framework as a complete system, demonstrating its effectiveness in IPv4 and IPv6 forwarding, OpenFlow switching, and IPsec tunneling.

### Discussion
#### Integration with Control Plane
The next step is to integrate the control plane. Integrating Zebra [6] or Quagga [11] into PacketShader should be relatively straightforward. A key challenge is updating the forwarding table in GPU memory without disrupting the data-path performance. Potential solutions include incremental updates or double buffering.

#### Multi-Functional, Modular Programming Environment
Currently, PacketShader limits one GPU kernel function execution at a time per device. NVIDIA's recent addition of native support for concurrent execution of heterogeneous kernels in GTX480 [8] will allow us to modify the PacketShader framework to benefit from this feature. Additionally, GTX480 supports C++ and function pointers in the kernel, which will facilitate the implementation of a Click-like modular programming environment [30] in PacketShader.

#### Vertical Scaling
As an alternative to using GPUs, increasing the number of CPUs can scale up the single-box performance. However, this approach is not cost-effective due to the need for motherboard upgrades, additional memory installation, and diminishing price/performance ratios. Installing a cheap GPU (ranging from $50 to $240) is a more economical solution.

### Conclusions
We have presented PacketShader, a novel framework for high-performance network packet processing on commodity hardware. By minimizing per-packet processing overhead and performing packet processing in user space, PacketShader leverages the massively-parallel processing capability of GPUs to handle computation and memory-intensive workloads. Careful design choices make PacketShader highly scalable with multi-core CPUs, high-speed NICs, and GPUs in NUMA systems. We have demonstrated its effectiveness in IPv4 and IPv6 forwarding, OpenFlow switching, and IPsec tunneling, showing that a well-designed PC-based router can achieve 40 Gbps forwarding performance with full programmability even on today’s commodity hardware. We believe PacketShader will serve as a valuable platform for scalable software routers on commodity hardware.

### Acknowledgements
We thank Katerina Arguraki, Vivek Pai, Seungyeop Han, anonymous reviewers, and our shepherd Robert Morris for their help and invaluable comments. This research was funded by KAIST High Risk High Return Project (HRHRP), NAP of Korea Research Council of Fundamental Science & Technology, and MKE (Ministry of Knowledge Economy of Republic of Korea, project no. N02100053).

### References
[References listed as in the original text]

---

This optimized version aims to provide a clearer, more coherent, and professional presentation of the information.