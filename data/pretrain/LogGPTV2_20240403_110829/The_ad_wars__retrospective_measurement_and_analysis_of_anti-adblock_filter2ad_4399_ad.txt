1000
500
s
e
t
i
s
b
e
w
f
o
r
e
b
m
u
N
Combined EasyList
Anti-Adblock Killer
Partial snapshots
Not archived URLs
Outdated URLs
350
300
250
200
150
100
50
s
e
t
i
s
b
e
W
f
o
r
e
b
m
u
N
0
2011 - 08
2011 - 12
2012 - 04
2012 - 08
2012 - 12
2013 - 04
2013 - 08
2013 - 12
2014 - 04
2014 - 08
2014 - 12
2015 - 04
2015 - 08
2015 - 12
2016 - 04
0
2011-08
2011-12
2012-04
2012-08
2012-12
2013-04
2013-08
2013-12
2014-04
2014-08
2014-12
2015-04
2015-08
2015-12
2016-04
Figure 5: Number of websites over time excluded from anal-
ysis.
August 2011 to 78 in July 2016. Our analysis of partial snapshots
shows that it is due to anti-abuse policies implemented by websites
against bots. Some domains show an error page if they detect the
Wayback Machine’s crawling bots. For correctly archived websites,
we verified that the Wayback Machine serves the same snapshots
to a normal user (using a regular browser) and our crawlers. To
this end, we manually analyzed a sample of randomly selected 20
URLs and did not find any difference in content served to a normal
user and our crawler. Our analysis corroborates the findings of
Lerner et al. [53] that the Wayback Machine archives most websites
reliably. It is noteworthy that we miss location specific content
because we rely on the content archived by the Wayback Machine.
The archived content is specific to the location of the Wayback
Machine’s servers.
4.2 Anti-Adblock Detection on the Wayback
Machine
After crawling the historic data of Alexa top-5K domains, we check
for the presence of anti-adblockers using the Anti-Adblock Killer
List and Combined EasyList. We use the historic versions of these fil-
ter lists at that point in time because it will portray the retrospective
effectiveness of these filter lists. For detection with HTTP request
filter rules, we extract the HTTP requests from the HAR files and
see whether they trigger HTTP rules in the filter lists. For detection
with HTML element filter rules, we open the stored HTML page in
a web browser with adblocker enabled and see whether it triggers
HTML rules in the filter lists.
To detect the presence of anti-adblockers with HTTP request
filter rules, we extract HTTP request URLs from the HAR files
crawled from the Wayback Machine. To archive a website, the
Wayback Machine replaces all live URLs with its own reference
by prepending http://web.archive.org to the URL. To run HTTP
request filter rules against these URLs, we truncate the Wayback
Machine references. Note that we do not truncate Wayback escape
[52, 53] URLs because they are not archived with the Wayback
Machine reference. We use adblockparser [7] to match all URLs
of a website against HTTP request filter rules. We label a website
as anti-adblocking if any of its URLs is matched against any of the
HTTP request filter rules. Figure 6(a) shows the number of anti-
adblocking websites in Alexa top-5K websites that are detected by
(a) Number of websites that trigger HTTP rules
Combined EasyList
Anti-Adblock Killer
8
6
4
2
s
e
t
i
s
b
e
W
f
o
r
e
b
m
u
N
0
2011-08
2011-12
2012-04
2012-08
2012-12
2013-04
2013-08
2013-12
2014-04
2014-08
2014-12
2015-04
2015-08
2015-12
2016-04
(b) Number of websites that trigger HTML rules
Figure 6: Temporal evolution of websites that trigger HTTP
and HTML filter rules of the Anti-Adblock Killer List and
Combined EasyList.
HTTP request filter rules in Anti-Adblock Killer List and Combined
EasyList. For the Anti-Adblock Killer List, the number of matched
websites has increased from 0 in April 2014 to 331 in July 2016.
For the Combined EasyList, the number of matched websites has
increased from 0 in August 2011 to 16 in July 2016. In contrast to our
comparative analysis of these filter lists in §3, it is noteworthy that
the number of matched websites for the Anti-Adblock Killer List
is much higher than that for the Combined EasyList. Our analysis
of matched URLs also reveals that a vast majority of websites use
third party scripts for anti-adblocking. More specifically, more than
98% of matched websites for the Anti-Adblock Killer List use scripts
from third party anti-adblocking vendors such as Optimizel, Histats,
PageFair, and Blockadblock.
To detect the presence of anti-adblockers with HTML element
filter rules, we open the HTML webpage crawled from the Wayback
Machine in a fully functional web browser and analyze whether
it triggers any HTML element filter rules in the filter lists. We
configure a profile of Mozilla Firefox browser with Adblock Plus
enabled and subscribe to the Anti-Adblock Killer List and Combined
EasyList. For each crawled HTML webpage, we open the webpage
in the browser and wait 180 seconds for it to load completely and
for HTML element filter rules to trigger. After that, we analyze
the Adblock Plus logs and extract the triggered HTML element
IMC ’17, November 1–3, 2017, London, United Kingdom
Iqbal et al.
F
D
C
1
0.8
0.6
0.4
0.2
0
-1080
Combined EasyList
Anti-Adblock Killer
-900
-720
-540
-360
0
-180
Time difference (days)
180
360
540
720
900
1080
Figure 7: Distribution of time difference (number of days)
between the day an anti-adblocker was added to a website
and the day it was detected by Combined EasyList and Anti-
Adblock Killer List.
filter rules for the crawled HTML webpages. Figure 6(b) shows
the number of anti-adblocking websites in Alexa top-5K websites
that are detected by HTML element filter rules in the Anti-Adblock
Killer List and the Combined EasyList. We note that the number of
matching HTML element rules is much less than that with HTTP
request filter rules. For the Anti-Adblock Killer List, the number of
matched websites remains between 0 and 5 from April 2014 to July
2016. For the Combined EasyList, the number of matched websites
remains between 0 and 4 from August 2011 to July 2016.
Next, we compare the speed of the Anti-Adblock Killer List
and the Combined EasyList in adding a new filter rule for an anti-
adblocker after its addition. Figure 7 plots the distribution of time
difference in terms of number of days when an anti-adblocker was
added to a website and when a filter rule was added to detect it.
The results indicate that the Combined EasyList is more prompt
than the Anti-Adblock Killer List in adding new filter rules for anti-
adblockers that are added to a website. In the Combined EasyList,
filter rules are defined for 82% anti-adblockers within 90 days of
their addition to a website. However, in the Anti-Adblock Killer
List, filter rules are defined for only 32% anti-adblockers within
90 days of their addition to a website. It is noteworthy that for a
fraction of anti-adblockers filter rules are present on the Combined
EasyList and Anti-Adblock Killer List even before they are added to
a website. This can happen when the filter list uses generic rules to
block third-party anti-adblockers. In the Combined EasyList, filter
rules are present for 42% anti-adblockers before their addition to a
website. In the Anti-Adblock Killer List, filter rules are present for
23% anti-adblockers before their addition to a website.
4.3 Anti-Adblock Detection on the Live Web
After retrospectively analyzing the coverage of anti-adblock filter
lists using the Wayback Machine, we next study their coverage on
the live Web in April 2017. We crawl Alexa top-100K websites and
check for the presence of anti-adblockers using the Anti-Adblock
Killer List and Combined EasyList. For detection on live Web, we use
the most recent version of filter lists. Overall, our results on the live
Web corroborate the findings of our retrospective analysis using the
Wayback Machine. For example, we observe that the coverage of the
Anti-Adblock Killer List is much more than that of the Combined
EasyList. For the Anti-Adblock Killer List, the number of websites
that trigger HTTP request filter rules is 4,931 out of 99,396. For
the Combined EasyList, the number of websites that trigger HTTP
request filter rules is 182 out of 99,396. Furthermore, we find that the
number of websites that trigger HTML element filter rules is much
smaller. Specifically, the number of websites that trigger HTML
element filter rules is 11 for the Anti-Adblock Killer List and 15
for the Combined EasyList. We again note that a vast majority of
websites use third party anti-adblock scripts. For the Anti-Adblock
Killer List, 97% of the matched websites use anti-adblocking scripts
from third party vendors.
5 DETECTING ANTI-ADBLOCK SCRIPTS
Since anti-adblock filter lists are currently manually maintained,
it is challenging for their authors to keep them up-to-date. The
two popular anti-adblock filter lists are implemented differently
and have varying coverage and update frequency. For example, the
anti-adblock filter list with better coverage tends to be updated less
frequently. We note that it is challenging for anti-adblock filter lists
to keep pace with anti-adblockers that quickly evolve in response
to adblockers [59]. Therefore, to help anti-adblock filter list authors,
we next investigate a machine learning based automated approach
to detect anti-adblock scripts.
Online publishers use JavaScript to implement client side anti-
adblocking logic. We plan to fingerprint anti-adblocking JavaScript
behavior by doing static code analysis. The basic idea is to extract
syntactic features from anti-adblocking JavaScript code and build a
light weight machine learning classifier. Our approach is inspired
by prior machine learning based JavaScript analysis approaches to
detect malware [43] and trackers [50].
Figure 8 shows the workflow of our proposed approach. We first
unpack JavaScript files using the Chrome V8 engine. We construct
Abstract Syntax Trees (ASTs) of the parsed scripts and then extract
different syntactic features. We use supervised machine learning
to train a classifier (AdaBoost + SVM) for distinguishing between
anti-adblocking and non anti-adblocking scripts. Below we discuss
different steps of our static JavaScript code analysis approach to
detect anti-adblockers.
Unpacking Dynamic JavaScript. The dynamic nature of JavaScript
makes it challenging to do static analysis. For example, JavaScript
code is often packed using eval() function. Such code unpacks
itself right before it is executed. To cate for dynamically generated
code, we use Chrome V8 engine to unpack eval() function by
intercepting calls to the script.parsed function. script.parsed
function is invoked every time eval() is evaluated or new code is
added with  or  tags.
Gathering Labeled Data. In order to train a machine learning
classifier, we need labeled examples of anti-adblocking and non anti-
adblocking scripts. We utilize more than one million JavaScript snip-
pets that were collected as part of our retrospective measurement
study of Alexa top-5K websites using the Wayback Machine. Our
anti-adblock data set consists of JavaScript snippets that matched
HTTP request filter rules of crowdsourced anti-adblock filter lists.
We use 372 of these anti-adblocking scripts as positive examples.
To collect negative examples, we use the remaining scripts that
The Ad Wars
IMC ’17, November 1–3, 2017, London, United Kingdom
JS file
Unpacked 
JS file
AST Construction
Feature extraction and 
filtering
AdaBoost + SVM
Anti-adblocking JS
Non anti-adblocking JS
Figure 8: Overview of anti-adblock script detection approach.
the filter lists did not identify as anti-adblockers. We aim for a
class imbalance of approximately 10:1 (negative:positive) in our
labeled data. We manually verify a randomly selected 10% sample
of ground truth for positive examples. We find that a vast majority
of the scripts are served from known anti-adblocking vendors such
as Optimizely and Blockadblock.
Feature Extraction. To extract features, we map the parsed scripts
to ASTs, which are syntactic representations of JavaScript code in
a tree format. After constructing the ASTs, we extract features ac-
cording to the hierarchical tree structure. We define a feature as
a combination of context and text. Context is the place where the
feature appears, such as loop, try statement, catch statement, if con-
dition, switch condition, etc. Text is code that appears in the context.
We extract three types of feature sets based on different selection
criterion of text. For the first type (all), we consider all text elements
including JavaScript keywords, JavaScript Web API keywords, iden-
tifiers, and literals. For the second type (literal), we consider text
elements only from literals, i.e., we remove JavaScript keywords,
JavaScript Web API keywords, and identifiers. These features are
very general because they do not contain identifiers and keyword
specific text in JavaScript code. For the third type (keyword), we
consider text elements only from native JavaScript keywords and
JavaScript Web API keywords, i.e., we remove identifiers and lit-
erals. As we do not consider text elements from identifiers and
literals, keyword features are not impacted by the randomization
of identifers and literals. However, these features are susceptible to
polymorphism. These three feature sets provide us varying levels
of generalization. We expect more general features to be robust
to minor implementation changes, however they may lose some
useful information due to generalization.
Table 2 shows some extracted features and their types for Code 5.
Literal features capture textual properties of JavaScript code such
Features
MemberExpression:BlockAdBlock
MemberExpression:_creatBait
MemberExpression:_checkBait
Literal:abp
Literal:0
Literal:hidden
Identifier:clientHeight
Identifier:clientWidth
Identifier:offsetHeight
Identifier:offsetWidth