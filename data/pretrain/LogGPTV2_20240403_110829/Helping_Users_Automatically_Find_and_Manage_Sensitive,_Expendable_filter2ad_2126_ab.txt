Figure 2: Overview of our approach combining qualitative interviews and two rounds of quantitative data collection.
by cloud storage providers, as well as deeper content analysis
using third-party services like Google Cloud Vision.
The survey centered on showing participants ﬁles from
their Google Drive or Dropbox accounts and asking them to
label and explain their sensitivity and usefulness. We also
asked them to indicate a ﬁle-management decision: whether
they would want to keep, delete, or protect each ﬁle. As it is
not feasible to show a participant all ﬁles on their account,
selecting the right subset of ﬁles to yield a well-suited dis-
tribution of training data was a challenge. To solve this, we
conducted two rounds of data collection. In Round 1, we pri-
marily used heuristic-based ﬁle selection leveraging insights
from our interviews. Because only a handful of ﬁles on a typi-
cal account are sensitive, heuristic-based ﬁle selection yielded
a small number of sensitive data points. Therefore, we trained
a preliminary classiﬁer, using its predictions for sensitive doc-
uments and images to select ﬁles in Round 2. Doing so let
us oversample the minority class (sensitive ﬁles). Section 5
further details our method, while Section 6 summarizes the
ﬁndings from both rounds of data collection.
3. Developing Aletheia, an Automated Classiﬁer: Using
the data collected from both rounds, we built classiﬁers to pre-
dict ﬁle (i) sensitivity, (ii) usefulness, and (iii) desired manage-
ment. We formulated each prediction as a classiﬁcation task.
Note that ﬁle-management decisions are heavily inﬂuenced
by ﬁle sensitivity and usefulness. As mentioned above, we
used an initial version of the sensitivity classiﬁer for Round 2
of data collection. Because decisions to delete data are highly
subjective and consequential, we expect Aletheia to be used
as part of a human-in-the-loop support system, rather than in
a fully automated way. Therefore, we evaluated Aletheia with
precision-recall analysis, which aligns with rankings of which
ﬁles to present in a user interface or through recommenda-
tions. To quantify the accuracy of our models, we used the
area under the precision-recall curve (AUC). Section 7 details
Aletheia’s experimental setup and performance results.
4 Qualitative Interviews
To gain an initial understanding of how people conceive of
the sensitivity and usefulness of ﬁles in the cloud, we ﬁrst
conducted semi-structured interviews of cloud storage users.
We aimed to build a formative understanding of factors that
make someone perceive a ﬁle as sensitive or useful. This
Scenarios for Sensitivity
1. Files that would cause concern if they were hacked from the cloud
2. Cloud ﬁles that, if made public, would be embarrassing
3. Files that would cause worry if close family members viewed them
Scenarios for Usefulness
1. Files to be recovered if they were accidentally deleted from the cloud
2. Cloud ﬁles accessed and updated on a regular basis
3. Cloud ﬁles shared with friends and/or family
Table 1: Broad scenarios used as prompts in our interviews.
understanding underpins our online study, eventually enabling
us to ﬁnd ﬁles that may be sensitive, yet not useful, at scale.
4.1 Methodology
Using Craigslist, we recruited participants who had a Google
Drive or a Dropbox account over 3 months old and were
willing to attend an in-person interview. We interviewed 17
participants from January through June 2019. Among par-
ticipants, 10 identiﬁed as male and 7 as female. Their ages
ranged from 20 to 45 years old. We prioritized participants
without experience in an IT-related ﬁeld. Six participants were
full-time students, all from non-STEM majors. All other par-
ticipants had completed a college education. The interview
took approximately 30 minutes to complete, and compensa-
tion was a $20 Amazon gift card. This amount also accounted
for the costs of participants commuting to the interview site.
Our protocol investigated participants’ approaches to cloud
storage both abstractly and concretely, where the latter was
grounded in individual ﬁles in a participant’s account. Ap-
pendix A in our online materials [1] contains our script.
The ﬁrst half of the interview focused on general reasons
for using cloud storage, followed by an open-ended discus-
sion about broad classes and characteristics of sensitive and
useful ﬁles stored in the cloud. To further spur participants’
thinking, we also provided them the sensitivity and useful-
ness scenarios in Table 1. These speciﬁc scenarios were the
research team’s initial hypotheses about how sensitivity and
usefulness manifest. Considering responses to both our broad
questions and discussions following the scenario prompts, we
began to conceptualize sensitive and useful ﬁles.
The second half of the interview investigated the same phe-
nomena more concretely. Participants logged into a web app
we built that used the Google Drive and Dropbox APIs to
1148    30th USENIX Security Symposium
USENIX Association
1. Qualitative Interviews2. Training Data Collection3. Classifier Design and EvaluationRound 1: Heuristic-based file selection Round 2: Classifier-based file selection Discussion of file sensitivity andusefulness themes Automated APIdata collectionFile selection input from classifier show ten ﬁles randomly selected from their account. For each
ﬁle, the participant explained its sensitivity and usefulness,
giving us concrete examples of ﬁles that were sensitive or use-
ful, in addition to speciﬁc attributes that made them so. After
the questions about speciﬁc ﬁles, participants were asked to
provide overall feedback regarding draft questions from our
quantitative survey (Section 5). These speciﬁcally focused on
ways to elicit perceptions of ﬁle sensitivity and usefulness.
All interview responses were audio recorded with con-
sent and then transcribed using the Google Speech to Text
API [19]. One member of the research team open-coded these
transcriptions to extract emergent themes. A second member
of the team then independently coded the extracted quotes
using that codebook. Cohen’s κ, a measure of intercoder relia-
bility, was 0.87. The two coders met and resolved conﬂicting
codes. The ﬁnal codebook, which is available in our online
materials [2], contained thirty distinct codes across the sixteen
prompts and questions.
We took care to ensure interviews were conducted ethically.
We ﬁrst obtained IRB approval for our protocol. Participants
reviewed a consent form with opt-in permission for audio
recording. Furthermore, to ensure participant privacy, we en-
couraged them to use their own personal device (computer or
phone) to view the ﬁles selected for the study, though we also
gave them the option of using a laptop we provided. During
the part of the interview where they reviewed their own ﬁles,
we instructed them to sit so that the contents of their screens
were visible only to them.
Like all user studies, our protocol has limitations. One
potential limitation was that we presented a ﬁxed set of cate-
gories (Table 1) representing potential manifestations of ﬁle
sensitivity and usefulness. While we intentionally provided
these prompts only after a broad initial discussion of ﬁle sensi-
tivity and usefulness, they may not have captured all possible
conceptualizations of these ideas. Particularly, our prompts
on sensitivity did not always align with the nuanced potential
risks of a ﬁle being leaked. To minimize these biases, we
provided these prompts only after participants gave us their
initial open-ended thoughts about types and characteristics of
sensitive and useful ﬁles. However, this approach may have
discouraged participants from mentioning other categories of
sensitivity and usefulness we did not anticipate. Additionally,
as these were in-person interviews, we were also limited to
individuals who were residents of a North American urban
center, and participants represented a convenience sample of
both students and members of the workforce. As a result, our
formative understanding of ﬁle sensitivity and usefulness is
likely to be situated in a particular culture and demographic.
4.2 Results
We now present interview participants’ conceptions of the
sensitivity and usefulness of ﬁles in the cloud.
4.2.1 Why a File Might Be Perceived as Sensitive
In our general discussions of what makes a ﬁle sensitive,
participants invoked the following seven classes of sensitivity:
Personally Identiﬁable Information (PII): Files that con-
tained names, contact details, dates of birth, passports, or
driver’s licenses were considered sensitive. Many participants
cited their resume as an example. P01 explained, “Anything
that can easily identify you, like your name, your birthday,
your phone number, your address. It’s all on my resume.”
Conﬁdential Information: Distinct from PII, participants
mentioned that some data should never be released publicly
because of its proprietary or conﬁdential nature. Students
mentioned original work that could be plagiarized. P05 said,
“If it’s like an essay or something that I’m turning in, I don’t
think I necessarily want a bunch of people to read it.” Three
participants also mentioned ﬁles containing passwords.
Financial Information: Participants mentioned tax doc-
uments, pay stubs, and ﬁles with Social Security Numbers
(SSN) as very sensitive. They also worried about statements
for bank accounts and credit/debit cards, as well as other doc-
uments containing those numbers. Nine participants explicitly
mentioned their SSN as particularly sensitive, yet also found
on their cloud accounts due to backups of ﬁles like tax returns.
Intimate Content: Participants described broad concep-
tions of content that could be considered intimate or personal,
and thus sensitive. Photos, videos, and similar media ﬁles
were most commonly mentioned, particularly individuals’
own photos (both in adult situations and in general), as well
as adult content they had downloaded. P16 included among
their embarrassing ﬁles “porn, anything that’s not for the
public’s eyes. Pictures of myself or signiﬁcant other.”
Personal Views: Files that contained personal views or
opinions were also identiﬁed as sensitive. P09 explained, “I’m
a religious person and so there are times when I would make
audio recordings or save videos that are of a religious nature.
People may not particularly subscribe to it, or some people
may deem it oﬀensive.” Participants also mentioned ﬁles that
contain political opinions and anti-government views.
Self-Presentation: Participants found ﬁles related to their
self-presentation as sensitive. For example, P11 talked about
“unﬂattering photos and videos.” Other participants said ﬁles
that revealed activities they hoped to hide from speciﬁc people
were sensitive. For example, P14 said, “If there was a photo
of me smoking weed, my parents would freak out.”
Content That May Be Misinterpreted: Participants also
said ﬁles that could be misconstrued by others were sensitive.
A participant who was in the military discussed a speciﬁc
picture they saw during the study by explaining, “This is a
picture of some of my soldiers at a cemetery. Even though
it’s innocent, I don’t want people to associate this with, like,
death.” In contrast to data like ﬁnancial documents, this type
of sensitivity is particularly contextual and subjective.
USENIX Association
30th USENIX Security Symposium    1149
4.2.2 Why a File Might Be Perceived as Useful
Participants most commonly considered ﬁles in the cloud
useful if they might need to access them in the future. The
speciﬁc reasons for this future access spanned ﬁve categories:
Reminiscence: Participants frequently invoked photos’
sentimental nature and value for reminiscence as a key reason
they are useful. P09 explained, “Pictures are useful because
they capture memories. You want to have some memory of
good times, good events, or diﬀerent things.” P16 explained
why a speciﬁc picture of her kids was useful by saying, “I
would show my children what they looked like when they were
younger.” Expanding this deﬁnition, P09 explained, “I share
photos and videos of deceased family members that we like
to reminisce about.” Broadly, participants explained that ﬁles
with sentimental value will likely remain useful forever.
Active Projects: Participants explained that ﬁles related to
projects at work or school were useful, but many would not re-
main useful indeﬁnitely. When asked to think about ﬁles they
would prioritize recovering if accidentally deleted from the
cloud, 13 participants mentioned work- or school-related ﬁles.
For example, P12 said, “I would try to recover my resume
and any school work that needed to be turned in.” Similarly,
P09 said, “Documents are useful because. . . you always have
to deal with documents online in school, at work.”
Recent Files For Reference: Some documents remained
useful for reasons other than their initial purpose. For example,
P04 described a recent cover letter being useful for future job
applications to additional employers by saying, “This version
is very current. I just recently updated it, so it will be very
useful for me.” In general, participants said ﬁles that had been
recently accessed or modiﬁed were more likely to be useful,
yet some older ﬁles might also be needed for reference.
Files Frequently Updated Over Time: Participants said
cloud ﬁles that are frequently modiﬁed are useful. While some
work- or school-related ﬁles fell into this category, journals
and other evolving documents were key examples.
Sharing: Five participants mentioned that shared ﬁles were
useful. For example, P03 (a student) explained: “Midterm or
ﬁnal papers I usually store in the cloud if I need to share them
with somebody else or have someone else look at it.”
We used this qualitative understanding both to develop
closed-form survey questions (Section 5.1) and to identify
metadata and content features to collect about the ﬁles in par-
ticipants’ cloud accounts to train our classiﬁers. Section 5.2
lists these features and their relationship to these ﬁndings.
5 Quantitative Online User Study: Method
Building on the insights from our qualitative interviews, we
conducted an online user study combining a survey and au-
tomated data collection from participants’ cloud accounts.
Our core goal was to collect rich data about participants’ per-
ceptions alongside quantitative features of ﬁles in the cloud
Figure 3: Overview of the survey and data-collection process.
to train an automated tool for aiding cloud ﬁle management.
Appendix B, online [1], contains the survey instrument.
We ﬁrst built a tool that allows us to survey participants
about speciﬁc ﬁles in their cloud storage account while si-
multaneously collecting metadata and content-based features
about those ﬁles. We collected data across two rounds. For
each round, we recruited a separate set of participants to com-
plete both the generic and ﬁle-speciﬁc surveys described be-
low. In Round 1, we used a heuristic-based approach to select
ﬁles. From the results of Round 1, we trained a preliminary
classiﬁer, which we used to select ﬁles in Round 2. We used
data from both rounds to build and evaluate Aletheia.
5.1 Study Overview and Survey Structure
We recruited participants on Amazon’s Mechanical Turk
(Mturk) and Proliﬁc Academic.2 We recruited American par-
ticipants age 18+ with a platform approval rating of 95%+.
Participants were also required to have a Google Drive or
Dropbox account that was 3+ months old with 100+ ﬁles.
We ﬁrst presented participants with a consent form and a
visualization of the data we would collect from their cloud
account. Afterwards, we asked participants to authorize our
tool to programmatically scan their account. Figure 3 sum-
marizes the overall study ﬂow and back-end data collection.
The survey contained three sections: (i) broad questions about
their use of cloud storage; (ii) ﬁle-speciﬁc questions about
the sensitivity and usefulness of particular ﬁles on their ac-
count; and (iii) questions about their demographics and the
protection mechanisms used to secure their accounts.
File-Speciﬁc Survey: The focus of our survey was its sec-
ond part, in which we queried participants about particular
ﬁles stored in their accounts. Participants’ responses, paired
with the ﬁle features we collected, formed the training data
for Aletheia. As shown in Table 2, our ﬁle-selection strategies
diﬀered across two rounds of data collection.
Round 1: We ﬁrst selected ﬁles with heuristics deﬁning
diﬀerent categories of ﬁles. For category #1, we looked for
the presence of sensitive keywords in the ﬁlename. We chose
2While we initially used Mturk for data collection, we found Proliﬁc more
successful for recruitment as it is designed for academic user studies.
1150    30th USENIX Security Symposium
USENIX Association
Informed  Consentand  OAuthBroadSurveyDemographics  SurveyCollectlabelsandmanagement25Programmatic datacollectionAll file metadata  fromDroboxand Google Drive  APIsImagefeatures from Google  VisionAPIDocument  sensitivity labels  and textfeatures  (word2vec,etc.)Collect labels and management preferencesofspecificfilesineach account Understand perceptions of sensitivity and usefulness for files on accountFile-Specific Survey# of Files File Description
Category
Round 1 (File selection based on heuristics)
1
2
3
4
5
8
8
4
Files containing a sensitive keyword in ﬁle name
Document ﬁles (.txt, .docx, .pdf, .xlsx, .ppt, etc.)
Media ﬁles (.jpg, .png, .mp4, .mpeg, etc.)
Files other than documents or media
Round 2 (File selection based on preliminary classiﬁer)