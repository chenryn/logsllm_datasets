sessions.
P ) where
(cid:2)
(cid:3)
Compare. Given two cells (
(cid:2)
(cid:2)
P,
(cid:3)
D(
P,
of their distance.
Q) + D(
(cid:3)
(cid:2)
P,
P ) and (
Q), their distance is
Q). Their similarity is simply the negation
(cid:3)
(cid:3)
(cid:2)
Q,
Merge. Given two cells as two pairs of distributions, the result of
the Merge operation is the weighted sum of the distributions,
equivalent to the result of a Construct operation on the pro-
tocol equivalence groups from which the original cells were
constructed.
(cid:3)
(cid:2)
P,
Score. Given a cell (
P ) and a session (
(cid:2)x, (cid:3)x), the score is the
probability that both ﬂows of the session are drawn randomly
from the pair of distributions deﬁned by the cell. This is
P (
(cid:2)x) · (cid:3)
P (
(cid:3)x).
(cid:2)
Unfortunately, explicitly representing a pair of ﬂow distributions
n points!), nor is it
is not feasible (each distribution consists of 256
possible to reasonably learn such distributions approximately with
a polynomial number of samples. Instead, we compactly represent
ﬂow distributions by introducing independence assumptions into
our models for an exponential reduction in space. Next we describe
two approaches, one that represents ﬂow distributions as a product
of n independent byte distributions and a second that represents
them as being generated by a Markov process.
4.1 Product Distribution Model
The product distribution model treats each n-byte ﬂow distribu-
tion as a product of n independent byte distributions. Each byte
1P being a distribution on a ﬁnite set U means that P (x) ≥ 0 and
the sum of P (x) over all x in U is 1.
offset in a ﬂow is represented by its own byte distribution that de-
scribes the distribution of bytes at that offset in the ﬂow. For this
reason, we expect this model to be successful at capturing binary
protocols, where distinguishing features appear at ﬁxed offsets. Be-
ing a product of byte distributions means that each byte offset is
treated independently, as the following example illustrates.
Product Distribution Example. For the sake of example, let n =
4 and consider the distribution on ﬂows from the initiator to respon-
der for the HTTP protocol. If the byte strings “HEAD” and “POST”
have equal probability, then the strings “HOST” and “HEAT” must
occur with the same probability; clearly this is not generally the
case. Fortunately, this is only a problem if if the strings “HOST”
and “HEAT” occur in another protocol, which would cause it to be
confused with HTTP.
4.1.1 Construct
Each individual byte distribution is set in accordance with the
distribution of bytes at that offset. For example, byte distribution
i for the initiator to responder direction would represent the distri-
bution of the i-th byte of the initiator to responder ﬂow across the
sessions in the protocol equivalence group.
4.1.2 Compare
The relative entropy of two product distributions P1 × P2 and
Q1 × Q2 is just the sum of the individual relative entropies; that is,
D(P1×P2 | Q1×Q2) = D(P1|Q2) + D(P2|Q2).
In fact, this property is why the relative entropy of two cells, which
consist of two independent distributions on ﬂows per Premise 2, is
just the sum of each direction’s relative entropy.
4.1.3 Merge
The merge operation simply returns a weighted average of the
underlying distributions. That is, if Pi is the i-th byte distribution
in one ﬂow direction of the ﬁrst cell and Qi is the i-th byte distribu-
tion in the same ﬂow direction of the second cell, then the resulting
cell’s i-th distribution in that ﬂow direction is λ Pi + (1 − λ)Qi,
where λ is the number of sessions in the protocol equivalence group
from which the ﬁrst cell was constructed divided by the total num-
ber of sessions in the protocol equivalence groups of the ﬁrst and
second cells.
4.1.4
Let (
Pn−1) be a product distribution
(cid:2)x, (cid:3)x) be a session. Then the probability of this session
cell and (
under the distribution deﬁned by the cell is
P0×···× (cid:2)
P0×···× (cid:3)
Score
Pn−1,
(cid:2)
(cid:3)
n−1Y
i=0
(cid:2)xi) · n−1Y
(cid:2)
Pi(
(cid:3)
Pi(
(cid:3)xi).
i=0
4.2 Markov process Model
Like the product distribution model, the Markov process model
relies on introducing independence between bytes to reduce the size
of the distribution. The Markov process we have in mind is best de-
scribed as a random walk on the following complete weighted di-
rected graph. The nodes of the graph are labeled with unique byte
values, 256 in all. Each edge is weighted with a transition proba-
bility such that, for any node, the sum of all its out-edge weights is
1. The random walk starts at a node chosen according to an initial
distribution π. The next node on the walk is chosen according to
the weight of the edge from the current node to its neighbors, that
is, according to the transition probability. These transition proba-
bilities are given by a transition probability matrix P whose entry
H
P
1
0
1
0
E
O
1
0
1
0
A
S
1
0
1
0
1
1
D
T
Figure 1: A Markov process for generating the strings “HEAD”
and “POST” with each string chosen according to the probabil-
ity of H and P in the initial distribution. Irrelevant nodes have
been omitted for clarity.
H
G
P
1
0
1
0
E
1
O
p
0
1
0
A
1 – p
S
1
0
1
0
1
D
(cid:1)
T
1
Figure 2: Attempting to add the string “GET ” to a Markov
process for generating the strings “HEAD” and “POST.”
Puv is the weight of the edge (u, v). The walk continues until n
nodes (counting the starting node) are visited. The ﬂow byte string
resulting from the walk consists of the names (i.e., byte values) of
the nodes visited, including self-loops.
In general, for a ﬁxed sequence of adjacent nodes (corresponding
to a sequence of bytes), there may be paths of different lengths
ending with this sequence of nodes. This means that the process
can capture distinguishing strings that are not tied to a ﬁxed offset.
As such, the Markov process model may be well-suited for text
protocols.
The probability distribution on length-n ﬂows deﬁned by the
above Markov process is described by the initial distribution π,
which consists of 256 values, and the transition probability ma-
2 values. To better understand this
trix P , which consists of 256
distribution, consider the example used for the product distribution
model.
Markov process Example. Again, for the sake of example, let
n = 4 and consider the distribution on ﬂows from the initiator to
responder for the HTTP protocol. Let the byte string “HEAD” occur
with probability p and the byte string “POST” with probability q.
The corresponding graph is shown in Figure 1, where the initial
distribution is π(H) = p, π(P) = q, and π(u) = 0 for u (cid:3)= H, P.
It seems we have avoided the problem we had with the prod-
uct distribution. However, if we try to add the string “GET ” we
quickly run into problems (see Figure 2). Now the byte strings
“GEAD” and “GET ” are also generated by our process!
4.2.1 Construct
The initial distribution π for some ﬂow direction is constructed
in the straightforward manner by setting it to be the distribution
on the ﬁrst byte of all the ﬂows (in the appropriate ﬂow direction).
The transition probabilities are based on the observed transition fre-
quencies over all adjacent byte pairs in the ﬂows (again, in the ap-
propriate direction). That is, Puv is the number of times byte u is
followed by byte v divided by the number of times byte u appears
at offsets 0 to n − 2.
4.2.2 Compare
The relative entropy of two Markov process distributions is some-
what involved. For brevity, we omit the proof of the following
fact. Let π and ρ be the initial distribution functions of two Markov
processes and let P and Q be corresponding transition probability
functions. The relative entropy of length-n byte strings generated
according to these processes is
X
X
π(u) log2
π(u)
ρ(u)
+
u,v
ξ(u) · P (u, v) log2
P (u, v)
Q(u, v)
,
u
where
ξ(u) = π(u) +
n−2X
X
i=1
t1..ti
π(t1) ·
iY
j=1
P (tj−1, tj) · P (ti, u).
4.2.3 Merge
Just as in the case of the product distribution model, the merge
operation involves a convex combination of the initial distributions
and the transition probability matrix of the two sessions in each of
the two directions.
4.2.4
Score
The probability of a string x0, . . . , xn−1, according to some
Markov process distribution given by initial distribution π and tran-
sition probability matrix P , is given by a straightforward simulation
of the random walk, taking the product of the probability according
to the initial distribution and the edge weights encountered along
the walk:
π(x0) · n−1Y
P (xi−1, xi).
i=1
5. COMMON SUBSTRING GRAPHS
We now introduce common substring graphs (CSGs). This rep-
resentation differs from the previous two approaches in that it cap-
tures much more structural information about the ﬂows from which
it is built. In particular, CSGs:
• are not based on a ﬁxed token length but rather use longest
common subsequences between ﬂows,
• capture all of the sequences in which common substrings oc-
cur, including their offsets in the ﬂows,
• ignore all byte sequences that share no commonalities with
other ﬂows,
• track the frequency with which individual substrings, as well
as sequences thereof, occur.
A common subsequence is a sequence of common substrings be-
tween two strings; a longest common subsequence (LCS) is the
common subsequence of maximum cumulative length. We denote
the LCS between two strings s1 and s2 as L(s1, s2) and its cumu-
lative length as |L(s1, s2)|.
The intuition for CSGs is as follows:
if multiple ﬂows carry-
ing the same protocol exhibit common substrings, comparing many
such ﬂows will most frequently yield those substrings that are most
common in the protocol. By using LCS algorithms, not only can we
Figure 4: Scoring a ﬂow against a CSG. The labels of nodes
A, B, and C occur in the ﬂow at the bottom. The shaded
area in the graph indicates all paths considered for the scor-
ing function. While the path containing A-C would constitute
the largest overlap with the ﬂow, it is not considered because A
and C occur in opposite order in the ﬂow. The best overlap is
with the path containing A-B: the ﬁnal score is (a + b)/f.
the label of node ni. Labels are unique, i.e., there is only a single
node with a given label at any one time.
We make extensive use of a variant of the Smith-Waterman lo-
cal alignment algorithm for subsequence computation [21]. Given
two input strings, the algorithm returns the longest common sub-
sequence of the two strings together with the offsets into the two
strings at which the commonalities occur. Our software implemen-
tation of Smith-Waterman requires O(|s1| · |s2|) space and time
given input strings s1 and s2. Signiﬁcant speed-ups are possible by
leveraging FPGAs or GPUs [16, 22]. We use linear gap penalty
with afﬁne alignment scoring and ignore the possibility of byte
substitutions, i.e., we compute only exact common subsequences
interleaved with gap regions.
To fulﬁll the requirements of a cell (
P ), we put two CSGs
into each cell, one per ﬂow direction. We will now describe the
realization of the four cell methods in CSGs.
5.1 Construct
(cid:3)
(cid:2)
P,
Insertion of a ﬂow into a CSG works as follows. A ﬂow is in-
serted as a new, single-node path. If there are no other paths in the
CSG, this completes the insertion process. Otherwise, we compute
the LCSs between the ﬂow and the labels of the existing nodes.
Where nodes are identical to a common substring, they are merged
into a single node carrying all the merged nodes’ paths. Where
nodes overlap partially, they are split into neighboring nodes and
the new, identical nodes are merged. We only split nodes at those
offsets that do not cause the creation of labels shorter than a mini-
mum allowable string length.
For purposes of analyzing protocol-speciﬁc aspects of the ﬂows
that are inserted into a graph, it is beneﬁcial to differentiate between
a new ﬂow and the commonalities it has with the existing nodes in
a graph. We therefore have implemented a slightly different but
functionally equivalent insertion strategy that uses ﬂow pools: a
new ﬂow is compared against the ﬂows in the pool, and LCSs are
extracted in the process. Instead of the ﬂow itself we then insert
the LCSs into the CSG as a path in which each node corresponds
to a substring in the LCS. Figure 3 shows the node merge and split
processes during insertion of an LCS.
Since many ﬂows will be inserted into a CSG, state management
Figure 3: Constructing a CSG: introduction of a new path with
subsequent merging of nodes. (a) A CSG with a single, three-
node path. (b) An LCS (in white) is inserted as a new path.
(c) New node A already exists and is therefore merged with the
existing node. (d) New node D overlaps partially with existing
nodes B and C.
(e) Nodes B, C, and D are split along the
overlap boundaries. (f) Identically labeled nodes resulting from
the splits are merged. The insertion is complete.
identify what these commonalities are, but we also expose their se-
quence and location in the ﬂows. By furthermore comparing many
of the resulting LCSs and combining redundant parts in them, fre-
quency patterns in substrings and LCSs will emerge that are suit-
able for classiﬁcation.
We will now formalize this intuition. A CSG is a directed graph
G = (N, A, P, ns, ne) in which the nodes N are labeled and the
set of arcs A can contain multiple instances between the same pair
of nodes: a CSG is a labeled multidigraph. P is the set of paths
in the graph. We deﬁne a path p = (n1, ..., ni) as the sequence of
nodes starting from n1 and ending in ni in the graph, connected by
arcs. P (n) is the number of paths running through a node n. (If
context does not make it clear which graph is being referred to, we
will use subscripts to indicate membership, as in NG, PG, etc.) A
CSG has ﬁxed start and end nodes ns and ne. Each path originates
from ns and terminates in ne, i.e., PG(ns) = PG(ne) = |PG|.
We ignore these nodes for all other purposes; for example, when
we speak of a path with a single node on it, we mean a path orig-
inating at the start node, visiting the single node, and terminating
at the end node. Along the path, a single node can occur multi-
ple times; that is, the path may loop. The node labels correspond
to common substrings between different ﬂows, and paths represent
the sequences of such common substrings that have been observed
between ﬂows. CSGs grow at the granularity of new paths being
inserted. For ease of explanation we liken nodes with their labels.
Thus for example when the say that a node has overlap with an-
other node, we mean that their labels overlap, and L(n1, n2) is the
LCS of the labels of nodes n1 and n2. |ni| denotes the length of
becomes an issue. We limit the number of nodes that a CSG can
grow to using a two-stage scheme in combination with monitoring
node use frequency through a least recently used list. A hard limit
imposes an absolute maximum number of nodes in the CSG. If
more nodes would exist in the graph than the hard limit allows,
least recently used nodes are removed until the limit is obeyed. To
reduce the risk of evicting nodes prematurely, we use an additional,
smaller soft limit, exceeding of which leads to node removal only
if the affected nodes are not important to the graph’s structure. To
quantify the importance of a node n to its graph G we deﬁne as the
weight of a node the ratio of the number of paths that are running
through the node to the total number of paths in the graph:
WG(n) =
PG(n)
|PG|
We say a node is heavy when this fraction is close to 1. As we
will show in Section 7.1, only a small number of nodes in a CSG
loaded with network ﬂows is heavy. Removal of a node leads to a