h=10
0.793
0.086
0.008
0.001
TABLE III: Under-prediction results obtained by overprevi-
sioning using mean-shift clustering with different window sizes
and quartiles.
Actual
Quartile 99.0%
Quartile 99.9%
Actual and Predicted Disk Usages
 450
 400
 350
 300
 250
)
B
G
(
e
g
a
s
U
k
s
i
D
 200
 0
 200
 400
 600
Time
 800
 1000
 1200
Fig. 6: Original disk usages and the predictions obtained by
the statistical method.
Actual
Quartile 99.0%
Quartile 99.9%
Actual and Predicted Disk Usages
 450
 400
 350
 300
 250
)
B
G
(
e
g
a
s
U
k
s
i
D
 200
 0
 200
 400
 600
Time
 800
 1000
 1200
Fig. 7: Original disk usages and the predictions obtained by
k-means method with 9-clusters.
Actual
Quartile 99.0%
Quartile 99.9%
Actual and Predicted Disk Usages
 450
 400
 350
 300
 250
)
B
G
(
e
g
a
s
U
k
s
i
D
write) scenario for the predictions, examining quartiles from
50% to 99.999% for k-means, mean-shift, and the statistical
model. Complete time series for our predictions with 9 clusters,
showing the actual disk usages, and prediction given by our
quartile results are displayed in Figures 6, 7, and 8. As can
clearly be seen in these ﬁgures, the results for our Markov
models outperformed the statistical method in terms of over-
provisioning performance, and the results for mean-shift more
closely follow the actual observed resource needs compared to
k-means which is more sensitive to cluster initialization, and
thus tends towards short periods of higher than necessary risk
aversion. In practice, however, when compared to each other,
the choice of k-means or mean-shift have statistically insignif-
icant effect on the average prediction and overprovisioning.
 200
 0
 200
 400
 600
Time
 800
 1000
 1200
Fig. 8: Original disk usages and the predictions obtained by
mean-shift method with h = 5.
period we may incur a performance penalty due to the need
to deallocate space assigned to our additional syndromes for
new user data. The results of these experiments for mean-shift
at various window sizes are given in Table III. The numeric
results for k-means and the statistical model are omitted due
to space restrictions, and are instead summarized in Section V
in Figure 11.
We study the resulting under-prediction rate as a measure
of QoS, under the hypothesis that
in times during which
we under-predict the necessary disk space for the next time
We calculate both the mean additional syndrome coverage
and the distribution of syndrome coverage over the prediction
period for our system, as the actual additional syndrome
225225
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:49:20 UTC from IEEE Xplore.  Restrictions apply. 
Mean Centroids
q = 95.0%
q = 99.0%
q = 99.9%
h=1
2.987
2.985
2.984
2.984
h=2
2.987
2.981
2.980
2.978
h=3
2.989
2.967
2.963
2.962
h=4
2.989
2.956
2.849
2.795
h=5
2.988
2.971
2.962
2.953
h=6
2.990
2.979
2.833
2.304
h=7
2.990
2.979
2.833
2.303
h=8
2.990
2.977
2.839
2.206
h=9
2.990
2.978
2.832
2.244
h=10
2.990
2.976
2.843
2.111
TABLE IV: Table of mean syndrome numbers obtained by
overprevisioning using mean-shift clustering with different
window sizes and quartiles.
Statistical
K-Means (k=7)
K-Means (k=9)
K-Means (k=11)
Mean Centroids
q = 95.0%
q = 99.0%
q = 99.9%
2.986
2.989
2.871
0.241
2.990
2.974
2.803
2.543
2.989
2.970
2.877
2.771
2.989
2.971
2.895
2.819
TABLE V: Table of mean syndrome numbers obtained by over-
previsioning using statistical method and k-means clustering
with different cluster numbers and quartiles.
coverage is dynamic and changes during each time step. We
obtain the syndrome numbers in Table IV using mean-shift
with various window sizes and in Table V using statistical
method and with k-means using varying number of clusters.
B. Improving Reliability
After determining the level of overprovisioning, we then
determine the number of additional syndromes that could be
allocated by our methods under varying levels of risk aversion.
The mean numbers of additional syndromes which can be
allocated are detailed in Table IV and Table V. We observe that
the number of syndromes we can allocate using our S2DDC
methods or the statistical method vary most commonly from 0
to 3, and thus present reliability results for this window of ad-
ditional syndromes. In our simulation, we model a 1 Petabyte
storage system with an initial RAID5 conﬁguration of X + 1,
simulating systems for X = {5, 8, 10}. The change in block
loss rate for the number of additional syndromes are shown in
Figure 9 for different cases. We utilize importance splitting to
estimate the additional reliability at higher syndrome coverage,
due to the rarity of block loss at these levels of coverage.
Figure 9 shows the results of these experiments, quantify-
ing the effect of different levels of syndrome coverage for the
various methods shown in Figures 10 and 11.
V. CONCLUSION
Based on our experiments from Section IV, and summa-
rized by Figures 10 and 11, we make nine primary observa-
tions, which we discuss in this section. Figure 10 presents
a summarized version of our additional syndrome coverage
results in the form of inverse cumulative density functions
(CDFs) for the 0.95 and 0.999 quartiles and each of our three
models. These inverse CDFs are calculated over simulated
runs over System-S dataset, and represent the proportion of
that time during which the syndrome coverage was at the
indicated value or higher, e.g. for mean-shift at the 0.999
quartile we note that for 60% of the dataset we were able to
make predictions which covered the entire dataset with at least
two additional syndromes, and for roughly 55% of the time we
were able to allocate at least three additional syndromes.
Figure 11 illustrates the trade-off of QoS losses (measured
via under-prediction rate) and reliability gains (measured via
226226
average additional syndrome coverage) for our various predic-
tion methods, with the most optimal solutions occurring in the
bottom right quadrant of the graphs (where under-prediction
rate is low, and average additional syndrome coverage is high).
Observation 1: Markov models provide higher QoS than
statistical models.
Observation 2: Markov models provide better reliability
and better QoS when risk aversion is increased.
We note in Figure 11 that for lower levels of risk-aversion
(represented by the chosen quartile) that our Markov models
reach a lower rate of under-prediction, and that for more risk
averse methods, we are able to reach the same under-prediction
rate provided by the statistical method, with a higher average
additional syndrome coverage, in some cases much higher.
We note in Figure 10 that not only is our average syndrome
coverage better, but the distribution is better, especially when
resources become scarce. Models which do not account for
dependent behaviors, such as the statistical model, deal worse
with worst case scenarios, as they assume the probabilities
of given writes or deletes are not conditional. We ﬁnd strong
evidence that such probabilities are conditioned on the prior
behaviors of a user.
Observation 3: Mean-shift and k-means clustering provide
similar QoS and reliability.
When comparing our two classiﬁcation algorithms for iden-
tifying states in our Markov model, we note that in Figures 10
and 11 both mean-shift and k-means provided similar under-
prediction rates, and similar additional syndrome coverage
(both on average, and over the entire distribution). Even though
the time series feature different dynamics as shown in Figures
7 and 8, we found insufﬁcient evidence via hypothesis testing
to claim a signiﬁcant difference for prediction quality in terms
of our two metrics, under-prediction and additional syndrome
coverage.
Observation 4: Mean-shift clustering is more performable
than k-means clustering.
What we did ﬁnd, however, was that due to the stochastic
nature of k-means, we had to calculate many realizations of
our estimator in order to obtain a tight conﬁdence interval. By
comparison, the deterministic nature of mean-shift provided
estimates of resource needs for the next time step with a
single realization. Given that evidence was insufﬁcient to claim
one estimator was better than the other, our experiments favor
mean-shift for classiﬁcation given its higher performability.
Especially given the on-line estimates our algorithm will
have to provide to support a S2DDC performance is a key
consideration.
Observation 5: Under-prediction rate is roughly propor-
tional to the number of clusters used to classify our data.
Observation 6: Our ability to allocate additional syn-
dromes is roughly proportional to the number of clusters used
to classify our data.
A key unanswered question from previous study [9] was
the determination of optimal cluster size. Our experiments
provide some observations on ﬁne tuning of this parameter
for the current data set. As shown in Tables III and IV, our
under-prediction rate, and thus quality of service, is roughly
proportional to the number of clusters used to classify our
dataset. As the number of clusters used to represent our
data decreases, our under-prediction rate tends to decrease.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:49:20 UTC from IEEE Xplore.  Restrictions apply. 
l
s
s
o
L
k
c
o
B
f
o
e
t
a
R
l
a
u
n
n
A
 1e+10
 100000
 1
 1e-05
 1e-10
 1e-15
Annual Rate of Block Loss for a 1 Petabyte storage system with X+1 initial RAID con(cid:1)guration.
X = 5
X = 8
X = 10
 0
 0.25
 0.5
 0.75
 1
 1.75
 1.25
Additional Syndrome Coverage
 1.5
 2
 2.25
 2.5
 2.75
 3
Annual Rate of Block Loss for a 1 Petabyte storage system
 with X+1 initial RAID con(cid:1)guration.
Annual Rate of Block Loss for a 1 Petabyte storage system
 with X+1 initial RAID con(cid:1)guration.
l
s