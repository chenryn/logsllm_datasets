信。每个容器组都有独立的ip，可用通过网络来和其他物理主机或者容器通信。
容器组有一组存储卷（挂载点），主要是为了让容器在重启之后可以不丢失数据。
容器组管理
容器组是一个运用管理和部署的高层次抽象，同时也是一组容器的接口。容器组是
部署、水平放缩的最小单位。
容器组的使用
容器组可以通过组合来构建复杂的运用，其本来的意义包含：
内容管理，文件和数据加载以及本地缓存管理等。
日志和检查点备份，压缩，快照等。
监听数据变化，跟踪日志，日志和监控代理，消息发布等。
代理，网桥
控制器，管理，配置以及更新
替代方案
296
基本概念
为什么不在一个单一的容器里运行多个程序？
1.透明化。为了使容器组中的容器保持一致的基础设施和服务，比如进程管理
和资源监控。这样设计是为了用户的便利性。
2.解偶软件之间的依赖。每个容器都可能重新构建和发布，Kubernetes必须支
持热发布和热更新（将来）。
3.方便使用。用户不必运行独立的程序管理，也不用担心每个运用程序的退出
状态。
4.高效。考虑到基础设施有更多的职责，容器必须要轻量化。
容器组的生命状态
包括若干状态值：pending、running、succeeded、failed。
pending
容器组已经被节点接受，但有一个或多个容器还没有运行起来。这将包含某些节点
正在下载镜像的时间，这种情形会依赖于网络情况。
running
容器组已经被调度到节点，并且所有的容器都已经启动。至少有一个容器处于运行
状态（或者处于重启状态）。
succeeded
所有的容器都正常退出。
failed
容器组中所有容器都意外中断了。
容器组生命周期
通常来说，如果容器组被创建了就不会自动销毁，除非被某种行为出发，而触发此
种情况可能是人为，或者复制控制器所为。唯一例外的是容器组由 succeeded状态
成功退出，或者在一定时间内重试多次依然失败。
297
基本概念
如果某个节点死掉或者不能连接，那么节点控制器将会标记其上的容器组的状态为
failed 。
举例如下。
容器组状态 running ，有 1 容器，容器正常退出
记录完成事件
如果重启策略为：
始终：重启容器，容器组保持 running
失败时：容器组变为 succeeded
从不：容器组变为 succeeded
容器组状态 running ，有1容器，容器异常退出
记录失败事件
如果重启策略为：
始终：重启容器，容器组保持 running
失败时：重启容器，容器组保持 running
从不：容器组变为 failed
容器组状态 running ，有2容器，有1容器异常退出
记录失败事件
如果重启策略为：
始终：重启容器，容器组保持 running
失败时：重启容器，容器组保持 running
从不：容器组保持 running
当有2容器退出
记录失败事件
如果重启策略为：
始终：重启容器，容器组保持 running
失败时：重启容器，容器组保持 running
从不：容器组变为 failed
容器组状态 running ，容器内存不足
标记容器错误中断
记录内存不足事件
如果重启策略为：
始终：重启容器，容器组保持 running
失败时：重启容器，容器组保持 running
从不：记录错误事件，容器组变为 failed
容器组状态 running ，一块磁盘死掉
298
基本概念
杀死所有容器
记录事件
容器组变为 failed
如果容器组运行在一个控制器下，容器组将会在其他地方重新创建
容器组状态 running ，对应的节点段溢出
节点控制器等到超时
节点控制器标记容器组 failed
如果容器组运行在一个控制器下，容器组将会在其他地方重新创建
Replication Controllers
服务
卷
标签
接口权限
web界面
命令行操作
299
kubectl 使用
kubectl 使用
kubectl 是 Kubernetes 自带的客户端，可以用它来直接操作 Kubernetes。
使用格式有两种：
kubectl [flags]
kubectl [command]
get
Display one or many resources
describe
Show details of a specific resource
create
Create a resource by filename or stdin
update
Update a resource by filename or stdin.
delete
Delete a resource by filename, stdin, resource and ID, or by resources and label
selector.
namespace
300
kubectl 使用
SUPERCEDED: Set and view the current Kubernetes namespace
log
Print the logs for a container in a pod.
rolling-update
Perform a rolling update of the given ReplicationController.
resize
Set a new size for a Replication Controller.
exec
Execute a command in a container.
port-forward
Forward one or more local ports to a pod.
proxy
Run a proxy to the Kubernetes API server
run-container
Run a particular image on the cluster.
stop
Gracefully shut down a resource by id or filename.
301
kubectl 使用
expose
Take a replicated application and expose it as Kubernetes Service
label
Update the labels on a resource
config
config modifies kubeconfig files
cluster-info
Display cluster info
api-versions
Print available API versions.
version
Print the client and server version information.
help
Help about any command
302
架构设计
基本架构
任何优秀的项目都离不开优秀的架构设计。本小节将介绍 Kubernetes 在架构方面
的设计考虑。
基本考虑
如果让我们自己从头设计一套容器管理平台，有如下几个方面是很容易想到的：
分布式架构，保证扩展性；
逻辑集中式的控制平面 + 物理分布式的运行平面；
一套资源调度系统，管理哪个容器该分配到哪个节点上；
一套对容器内服务进行抽象和 HA 的系统。
运行原理
下面这张图完整展示了 Kubernetes 的运行原理。
303
架构设计
图 1.21.5.1 - Kubernetes 架构
可见，Kubernetes 首先是一套分布式系统，由多个节点组成，节点分为两类：一类
是属于管理平面的主节点/控制节点（Master Node）；一类是属于运行平面的工作
节点（Worker Node）。
显然，复杂的工作肯定都交给控制节点去做了，工作节点负责提供稳定的操作接口
和能力抽象即可。
从这张图上，我们没有能发现 Kubernetes 中对于控制平面的分布式实现，但是由
于数据后端自身就是一套分布式的数据库（Etcd），因此可以很容易扩展到分布式
实现。
控制平面
主节点服务
304
架构设计
主节点上需要提供如下的管理服务：
apiserver 是整个系统的对外接口，提供一套 RESTful 的 Kubernetes API，供
客户端和其它组件调用；
scheduler 负责对资源进行调度，分配某个 pod 到某个节点上。是 pluggable
的，意味着很容易选择其它实现方式；
controller-manager 负责管理控制器，包括 endpoint-controller（刷新服务和
pod 的关联信息）和 replication-controller（维护某个 pod 的复制为配置的数
值）。
Etcd
这里 Etcd 即作为数据后端，又作为消息中间件。
通过 Etcd 来存储所有的主节点上的状态信息，很容易实现主节点的分布式扩展。
组件可以自动的去侦测 Etcd 中的数值变化来获得通知，并且获得更新后的数据来
执行相应的操作。
工作节点
kubelet 是工作节点执行操作的 agent，负责具体的容器生命周期管理，根据从
数据库中获取的信息来管理容器，并上报 pod 运行状态等；
kube-proxy 是一个简单的网络访问代理，同时也是一个 Load Balancer。它负
责将访问到某个服务的请求具体分配给工作节点上的 Pod（同一类标签）。
305
架构设计
图 1.21.5.2 - Proxy 代理对服务的请求
306
Mesos 项目
Mesos 项目
307
简介
简介
Mesos 是一个集群资源的自动调度平台，Apache 开源项目，它的定位是要做数据
中心操作系统的内核。目前由 Mesosphere 公司维护，更多信息可以自行查阅
Mesos 项目地址或 Mesosphere。
308
安装与使用
Mesos + Marathon 安装与使用
Marathon 是可以跟 Mesos 一起协作的一个 framework，用来运行持久性的应用。
安装
一共需要安装四种组件，mesos-master、marathon、zookeeper 需要安装到所有
的主节点，mseos-slave 需要安装到从节点。
mesos 利用 zookeeper 来进行主节点的同步，以及从节点发现主节点的过程。
源码编译
下载源码
git clone https://git-wip-us.apache.org/repos/asf/mesos.git
安装依赖
#jdk-7
sudo apt-get update && sudo apt-get install -y openjdk-7-jdk
#autotools
sudo apt-get install -y autoconf libtool
#Mesos dependencies.
sudo apt-get -y install build-essential python-dev python-boto l
ibcurl4-nss-dev libsasl2-dev maven libapr1-dev libsvn-dev
编译&安装
309
安装与使用
$ cd mesos
# Bootstrap (Only required if building from git repository).
$ ./bootstrap
$ mkdir build
$ cd build && ../configure
$ make
$ make check && make install
软件源安装
以 ubuntu 系统为例。
安装 Docker，不再赘述，可以参考 这里。
# Setup
sudo apt-key adv --keyserver keyserver.ubuntu.com --recv E56151B
F
DISTRO=$(lsb_release -is | tr '[:upper:]' '[:lower:]')
CODENAME=$(lsb_release -cs)
# Add the repository
echo "deb http://repos.mesosphere.io/${DISTRO} ${CODENAME} main"
| \
sudo tee /etc/apt/sources.list.d/mesosphere.list
sudo apt-get -y update && sudo apt-get -y install zookeeper meso
s marathon
基于 Docker
将基于如下镜像：
ZooKeeper：https://registry.hub.docker.com/u/garland/zookeeper/
Mesos：https://registry.hub.docker.com/u/garland/mesosphere-docker-
mesos-master/
Marathon：https://registry.hub.docker.com/u/garland/mesosphere-docker-
310
安装与使用
marathon/
其中 mesos-master 镜像将作为 master 和 slave 容器使用。
导出本地机器的地址到环境变量。
HOST_IP=10.11.31.7
启动 Zookeepr 容器。
docker run -d \
-p 2181:2181 \
-p 2888:2888 \
-p 3888:3888 \
garland/zookeeper