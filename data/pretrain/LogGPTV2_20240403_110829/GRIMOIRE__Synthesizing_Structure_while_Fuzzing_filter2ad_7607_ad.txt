analysis on the results, as shown in Table 2. The results show
that in all but two cases (Lua and NASM), GRIMOIRE offers
relevant and signiﬁcant improvements over all state-of-the-art
alternatives. On average, it ﬁnds nearly 20% more coverage
than the second best alternative.
USENIX Association
28th USENIX Security Symposium    1995
0005101520253035404505000100001500020000mrubyGrimoireRedqueenAFLQSYMAngora0005101520253035404502000400060008000tcc00051015202530354045010000200003000040000php0005101520253035404505000100001500020000boolector000510152025303540450100020003000400050006000lua0005101520253035404502000400060008000xml0005101520253035404505000100001500020000sqlite000510152025303540450200040006000800010000nasmTime(hh:mm)#BBsfoundSurprisingly, in the case of mruby, we ﬁnd that GRIMOIRE is
able to outperform even NAUTILUS.
To evaluate whether GRIMOIRE is still useful in scenarios
where a grammar is available, we perform another experiment.
We extract the corpus produced by NAUTILUS after half of
the time (i. e., 24 hours) and continue to use GRIMOIRE for
another 24 hours using this seed corpus. For these incre-
mental runs, we reduce GRIMOIRE’s upper bound for input
generalization to 2,048 bytes; otherwise, our fuzzer would
mainly spend time in the generalization phase since NAU-
TILUS produces very large inputs. The results are displayed
in Figure 5 (incremental). This experiment demonstrates that
even despite manual ﬁne-tuning, the grammar often contains
blind spots, where an automated approach such as ours can
infer the implicit structure which the program expects. This
structure may be quite different from the speciﬁed grammar.
As Figure 5 shows, by using the corpus created by NAU-
TILUS, GRIMOIRE surpasses NAUTILUS individually in all
cases (RQ 2). A conﬁrmatory statistical analysis of the results
is presented in Table 3. In three cases, GRIMOIRE is able to
improve upon hand written grammars by nearly 10%.
Table 3: Conﬁrmatory data analysis of our experiment. We compare the
coverage produced by GRIMOIRE against NAUTILUS with hand written
grammars. The effect size is the difference of the medians in basic blocks in
the incremental experiment. In three experiments, the effect size is relevant
and the changes are highly signiﬁcant (marked bold, p < 5.0E-02). Note that
we abbreviate JavaScriptCore with JSC.
Best
Target
Alternative
mruby NAUTILUS
NAUTILUS
Lua
NAUTILUS
PHP
NAUTILUS
JSC
Effect Size
(∆ = ¯A− ¯B)
2025
553
5465
15445
Effect Size
in % of Best
p-value
10.0% 1.8E-05
5.2% 5.0E-02
9.3% 3.6E-03
11.0% 1.8E-05
Additionally, we intended to compare GRIMOIRE against
CODEALCHEMIST and JSFUNFUZZ, two other state-of-the
art grammar-based fuzzers which specialize on JavaScript
engines. Although these two fuzzers are not coverage-
guided—making a fair evaluation challenging—we consider
the comparison of specialized JavaScript grammar-based
fuzzers to general-purpose grammar-based fuzzers as inter-
esting. Unfortunately, JSFUNFUZZ was not working with
JavaScriptCore out of the box as it is speciﬁcally tailored
to SpiderMonkey. Since it requires signiﬁcant modiﬁcations
to run on JavaScriptCore, we considered the required engi-
neering effort to be out of scope for this paper. On the other
hand, CODEALCHEMIST requires an extensive seed corpus
of up to 60,000 valid JavaScript ﬁles—which were not re-
leased together with the source ﬁles. We tried to replicate the
seed corpus as described by the authors of CODEALCHEMIST.
However, despite the authors’ kind help, we were unable to
run CODEALCHEMIST with our corpus.
Figure 5: The coverage (in basic blocks) produced by GRIMOIRE and NAU-
TILUS (using the hand written grammars of the authors of NAUTILUS) over
12 runs at 48 h on various targets. The incremental plots show how running
NAUTILUS for 48h compares to running NAUTILUS for the ﬁrst 24h and then
continue fuzzing for 24h with GRIMOIRE. Displayed are the median and the
66.7% conﬁdence interval.
Overall, these experiments conﬁrm our assumption that
grammar-based fuzzers such as NAUTILUS have an edge
over grammar inference fuzzers like GRIMOIRE. However,
deploying our approach on top of a grammar-based fuzzer
(incremental runs) increases code coverage. Therefore, we
partially respond to RQ 1 and provide an answer to RQ 2
by stating that GRIMOIRE is a valuable addition to current
fuzzing techniques.
5.4 Grammar Inference Techniques
To answer RQ 4, we compare our approach to other gram-
mar inference techniques in the context of fuzzing. Existing
work in this ﬁeld includes GLADE, AUTOGRAM and PYG-
MALION. However, since PYGMALION targets only Python
and AUTOGRAM only Java programs, we cannot evaluate
1996    28th USENIX Security Symposium
USENIX Association
0005101520253035404505000100001500020000mrubyGrimoireNautilus242934394405000100001500020000mrubyincremental000510152025303540450100002000030000400005000060000php24293439440100002000030000400005000060000phpincremental00051015202530354045020004000600080001000012000lua2429343944020004000600080001000012000luaincremental000510152025303540450250005000075000100000125000150000jsc24293439440250005000075000100000125000150000jscincrementalTime(hh:mm)#BBsfoundFigure 6: Comparing GRIMOIRE against GLADE (median and 66.7% interval). In the plot for GLADE +Training, we include the training time that glade used.
For comparison, we also include plots where we omit the training time. The horizontal bar displays the coverage produced by the seed corpus that GLADE used
during training.
them as GRIMOIRE only supports targets that can be traced
with Intel-PT (since REDQUEEN heavily depends on it).
Therefore, for this evaluation, we use GLADE (commit
“b9ef32e”), a state-of-the-art grammar inference tool. It oper-
ates in two stages. Given a program as black-box oracle as
well as a corpus of valid input samples, it learns a grammar in
the ﬁrst stage. In the second stage, GLADE uses this grammar
to produce inputs that can be used for fuzzing. GLADE does
not generate a continuous stream of inputs, hence we modi-
ﬁed it to provide such capability. We then use these inputs to
measure the coverage achieved by GLADE in comparison to
GRIMOIRE. Note that due to the excessive amount of inputs
produced by GLADE, we use a corpus minimization tool—
afl-cmin—to identify and remove redundant inputs before
measuring the coverage [66].
Note, we have to extend GLADE for each target that is
not natively supported and must manually create a valid seed
corpus. For this reason, we restrict ourselves to the three
targets libxml, mruby and Lua. From these, libxml is the
only one that was also used in GLADE’s evaluation. Therefore,
we are able to re-use their provided corpus for this target. We
choose the other two since we want to achieve comparability
with regards to previous experiments.
To allow for a fair comparison, we provide the same corpus
to GRIMOIRE. Again, we repeat all experiments 12 times for
48 hours each. The results of this comparison are depicted in
Figure 6. Note that this ﬁgure includes two different experi-
ments of GLADE. In the ﬁrst experiment, we include the time
GLADE spent on training into the measurement while for the
second measurement, GLADE is provided the advantage of
concluding the training stage before measurement is started
for the fuzzing process. As can be seen in Figure 6, GRI-
MOIRE signiﬁcantly outperforms GLADE on all targets for
both experiments. Similar to earlier experiments, we perform
a conﬁrmatory statistical analysis. The results are displayed
in Table 4; they are in all cases relevant and statistically sig-
niﬁcant. If we consider only the new coverage found (beyond
what is already contained in the training set), we are able to
outperform GLADE by factors from two to ﬁve. We therefore
conclude in response to RQ 4 that we signiﬁcantly exceed
comparative grammar inference approaches in the context of
fuzzing.
We designed another experiment to evaluate whether
GLADE’s automatically inferred grammar can be used for
NAUTILUS and how it performs compared to hand written
grammars. However, GLADE does not use the grammar di-
rectly but remembers how the grammar was produced from
the provided test cases and uses the grammar only to apply
local mutations to the input. Unfortunately, as a consequence,
their grammar contains multiple unproductive rules, thus pre-
venting their usage in NAUTILUS.
Table 4: Conﬁrmatory data analysis of our experiments. We compare the
coverage produced by GRIMOIRE against GLADE. The effect size is the
difference of the medians in basic blocks. In all experiments, the effect size
is relevant and the changes are highly signiﬁcant: it is multiple orders of
magnitude smaller than the usual bound of p < 5.0E-02 (bold).
Best
Target
Alternative
GLADE
mruby
GLADE
Lua
libxml GLADE
Effect Size
(∆ = ¯A− ¯B)
8546
2775
5213
Effect Size
in % of Best
p-value
43.6% 9.1E-05
38.1% 9.1E-05
57.2% 9.1E-05
5.5 Mutations Statistic
During the aforementioned experiments, we also collected
various statistics on how effective different mutators are. We
measured how much time was spent using GRIMOIRE’s dif-
ferent mutation strategies as well as how many of the inputs
were found by each strategy. This allows us to rank mutation
strategies based on the number of new paths found per time
used. The strategies include a havoc stage, REDQUEEN’s
Input-to-State-based mutation stage and our structural muta-
tion stage. The times for our structural mutators include the
USENIX Association
28th USENIX Security Symposium    1997
0005101520253035404505000100001500020000mrubyGrimoireGlade(+training)Glade0005101520253035404502000400060008000lua000510152025303540450200040006000800010000xmlTime(hh:mm)#BBsfoundgeneralization process (including the necessary minimization
that also beneﬁts the other mutators).
As Table 5 shows, our structural mutators are competitive
with other mutators, which answers RQ 5. As the coverage
results in Figure 4 show, the mutators are also able to uncover
paths that would not have been found otherwise.
Table 5: Statistics for each of GRIMOIRE’s mutation strategies (i. e., our struc-
tured mutations, REDQUEEN’s Input-to-State-based mutations and havoc).
For every target evaluated we list the total number of inputs found by a
mutation, the time spent on this strategy and the ratio of inputs found per
minute.
Mutation
Target
Structured
Input-to-State
Havoc
mruby
PHP
Lua
SQLite
TCC
Boolector
libxml
NASM
JavaScriptCore
mruby
PHP
Lua
SQLite
TCC
Boolector
libxml
NASM
JavaScriptCore
mruby
PHP
Lua
SQLite
TCC
Boolector
libxml
NASM
JavaScriptCore
#Inputs
9040
27063
2849
5933
6618
3438
4883
12696
38465
Time Spent (min)
1531.18
2467.17
2064.49
1325.26
2271.03
2399.85
2001.38
1955.42
2460.95
#Inputs/Min
5.90
10.97
1.38
4.48
2.91
1.43
2.44
6.49
15.63
814
902
530
603
1020
325
967
1329
400
2010
2546
1684
1827
2514
956
2173
2876
3800
268.23
111.46
307.12
768.72
118.23
102.87
359.03
213.84
82.76
339.03
278.21
492.99
742.13
484.73
373.85
504.86
678.59
279.62
3.03
8.09
1.73
0.78
8.63
3.16
2.69
6.22
4.83
5.93
9.15
3.42
2.46
5.19
2.56
4.30
4.24
13.59
5.6 Real-World Bugs
We use GRIMOIRE on a set of different targets to observe
whether it is able to uncover previously unknown bugs (RQ 6).
To this end, we manually triaged bugs found during our eval-
uation. As illustrated in Table 6, GRIMOIRE found more bugs
than all other tools in the evaluation combined. We responsi-
bly disclosed all of them to the vendors. For these, 11 CVEs
were assigned. Note that we found a large number of bugs
that did not lead to assigned CVEs. This is partially because
projects such as PHP do not consider invalid inputs as secu-
rity relevant, even when custom scripts can trigger memory
corruption. We conclude RQ 6 by ﬁnding that GRIMOIRE is
indeed able to uncover novel bugs in real-world applications.
6 Discussion
The methods introduced in this paper produce signiﬁcant
performance gains on targets that expect highly structured
inputs without requiring any expert knowledge or manual
work. As we have shown, GRIMOIRE can also be used to
support grammar-based fuzzers with well-tuned grammars but
Table 6: Overview of submitted bugs and CVEs. Fuzzers which did not ﬁnd
the bug during our evaluation are denoted by , while those who did are
marked by . We indicate targets not evaluated by a speciﬁc fuzzer with
’-’. We abbreviate Use-After-Free (UAF), Out-of-Bounds (OOB) and Buffer
Overﬂow (BO).
E
R
I
O
M
I
R
G
N
E
E
U
Q
D
E
R
A
R
O
G
N
A
S
U
L
I
T
U
A
N
L
F
A
M
Y
S
Q

CVE
Type
OOB-write  
 

OOB-read
   
 
OOB-read

 

 
OOB-read

