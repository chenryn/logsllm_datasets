speed is calculated as the total number IP lookup requests
divided by the total lookup time. We use the Nvidia Visual
Proﬁler tool to measure the lookup latency.
We evaluated the IP lookup speed versus traﬃc size (i.e.,
the number of IP addresses in one batch of data sent from
the CPU to the GPU). We generate 3 traﬃc traces of 3 diﬀer-
ent sizes 30K, 60K, 90K. Figure 13 shows our experimental
results, from which we observe that larger traﬃc sizes lead
to higher lookup speed. For the traﬃc size of 30K, SAIL L
achieves a lookup speed of 257∼322 Mpps. For the traﬃc
size of 60K, SAIL L achieves a lookup speed of 405∼447
Mpps. For the traﬃc size of 90K, SAIL L achieves a lookup
speed of 442∼547 Mpps.
We evaluated the IP lookup latency versus traﬃc size.
Figure 14 shows our experimental results, from which we
observe that larger traﬃc sizes lead to higher lookup latency.
For the traﬃc size of 30K, SAIL L has a lookup latency of
90∼124 μs. For the traﬃc size of 60K, SAIL L has a lookup
latency of 110∼152 μs. For the traﬃc size of 90K, SAIL L
has a lookup latency of 122∼185 μs.
6.5 Evaluation on Many-core Platform
We evaluated the lookup speed of SAIL L versus the num-
ber of cores. We conducted our experiments on the many-
core platform Telera TLR4-03680.
48650
600
550
500
450
400
350
300
250
200
150
100
50
0
)
s
p
p
M
(
d
e
e
p
s
p
u
k
o
o
L
 30
 60
 90
rrc00 rrc01 rrc03 rrc04 rrc05 rrc06 rrc07 rrc10 rrc11 rrc12 rrc13 rrc14 rrc15
FIB
Figure 13: Lookup speed VS. traﬃc size.
)
d
n
o
c
e
s
o
r
c
m
i
(
y
c
n
e
a
L
t
240
220
200
180
160
140
120
100
80
60
40
20
0
 30
 60
 90
rrc00 rrc01 rrc03 rrc04 rrc05 rrc06 rrc07 rrc10 rrc11 rrc12 rrc13 rrc14 rrc15
FIB
Figure 14: Lookup latency VS. traﬃc size.
700M
600M
500M
400M
300M
200M
100M
0
)
s
p
p
(
d
e
e
p
s
p
u
k
o
o
L
2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34
# of cores
Figure 15: Lookup speed VS. # of cores.
Our experimental results show that the lookup rate increas-
es linearly as the number of cores grows. Note that we only
have the results of 35 cores, because one core is responsible
for traﬃc distribution and results collection. Figure 15 shows
the results on FIB rrc00 using preﬁx-based traﬃc. We have
observed similar results for other FIBs.
7. DISCUSSION
Our SAIL framework is mainly proposed for IPv4 lookup;
however, it can be extended for IPv6 lookup as well. An IPv6
address has 128 bits, the ﬁrst 64 bits represent the network
address and the rest 64 bits represent the host address. An
IPv6 preﬁx has 64 bits. The real IPv6 FIBs in backbone
routers from www.ripe.net only has around 14,000 entries,
which are much smaller than IPv4 FIBs. To deal with IPv6
FIBs, we can push trie nodes to 6 levels of 16, 24, 32, 40, 48,
and 64. To split along the dimension of preﬁx lengths, we
perform the splitting on level 48. In other words, we store the
bit map and chunk ID arrays of levels 16, 24, 32, 40, and bit
map array of level 48 in on-chip memory. Our experimental
results show that the on-chip memory for an IPv6 FIB is
about 2.2MB. Although the on-chip memory usage for IPv6
is much larger than that for IPv4 in the worst case because
IPv6 preﬁx length are much longer than IPv4, as IPv6 FIB
sizes are orders of magnitude smaller than IPv4 FIB sizes,
the one-chip memory usage for IPv6 is similar to that for
IPv4.
8. CONCLUSION
We make three key contributions in this paper. First, we
propose a two-dimensional splitting approach to IP lookup.
The key beneﬁt of such splitting is that we can solve the
sub-problem of ﬁnding the preﬁx length ≤ 24 in on-chip
memory of bounded small size. Second, we propose a suite
of algorithms for IP lookup based on our SAIL framework.
One key feature of our algorithms is that we achieve con-
stant, yet small, IP lookup time and on-chip memory usage.
Another key feature is that our algorithms are cross plat-
form as the data structures are all arrays and only require
four operations of ADD, SUBTRACTION, SHIFT, and log-
ical AND. Note that SAIL is a general framework where
diﬀerent solutions to the sub-problems can be adopted. The
algorithms proposed in this paper represent particular in-
stantiations of our SAIL framework. Third, we implemented
our algorithms on four platforms (namely FPGA, CPU, G-
PU, and many-core) and conducted extensive experiments
to evaluate our algorithms using real FIBs and traﬃc from
a major ISP in China. Our experimental results show that
our SAIL algorithms are several times or even two orders of
magnitude faster than the well known IP lookup algorithms.
Furthermore, we have open sourced our SAIL L algorithm
and three well known IP lookup algorithms (namely LC-trie,
Tree Bitmap, and Lulea) that we implemented in [4].
9. ACKNOWLEDGEMENTS
We would like to thank Peng He for implementing the Tree
Bitmap algorithm and Chunjing Han, Qinghua Wu and Tai-
hua He for their valuable suggestions. We also would like
to thank the anonymous reviewers and our shepherd, An-
drew Moore, for their thoughtful suggestions. This work was
supported in part by National Basic Research Program of
China with Grant 2012CB315801, by National Natural Sci-
ence Foundation of China (NSFC) with Grants (61133015,
61202489), and by Strategic Priority Research Program of
CAS with Grant XDA06010303.
10. REFERENCES
[1] FPGA data sheet [on line]. Available:
http://www.xilinx.com.
[2] Quagga routing suite [on line]. Available:
http://www.nongnu.org/quagga/.
[3] RIPE network coordination centre [on line]. Available:
http://www.ripe.net.
[4] SAIL webpage.
http://ﬁ.ict.ac.cn/ﬁrg.php?n=PublicationsAmpTalks
.OpenSource.
[5] Tilera datasheet [on line]. Available:
http://www.tilera.com/sites/default/
ﬁles/productbriefs/TILE-Gx8036 PB033-02 web.pdf.
[6] K. Adam and M. Michael. Less hashing, same
performance: Building a better bloom ﬁlter. In
Algorithms–ESA 2006, pages 456–467. Springer, 2006.
49[7] P. Derek, L. Ziyan, and P. Hang. IP address lookup
using bit-shuﬄed trie. IEEE Computer
Communications, 2014.
[8] S. Devavrat and G. Pankaj. Fast incremental updates
on ternary-cams for routing lookups and packet
classiﬁcation. In Proc. Hot Interconnects, 2000.
[9] W. Feng and H. Mounir. Matching the speed gap
between sram and dram. In Proc. IEEE HSPR, pages
104–109, 2008.
[10] B. Florin, T. Dean, R. Grigore, and S. Sumeet. A tree
based router search engine architecture with single
port memories. In Proc. IEEE ISCA, 2005.
[11] Z. Francis, N. Girija, and B. Anindya. Coolcams:
Power-eﬃcient tcams for forwarding engines. In Proc.
IEEE INFOCOM, pages 42–52, 2003.
[12] R. G´abor, T. J´anos, A. Kor´osi, A. Majd´an, and
Z. Heszberger. Compressing IP forwarding tables:
Towards entropy bounds and beyond. In Proc. ACM
SIGCOMM, 2013.
[13] P. Gupta, S. Lin, and N. McKeown. Routing lookups
in hardware at memory access speeds. In Proc. IEEE
INFOCOM, pages 1240–1247, 1998.
[14] F. Hamid, Z. M. Saheb, and S. Masoud. A novel
reconﬁgurable hardware architecture for IP address
lookup. In Proc. ACM/IEEE ANCS, pages 81–90,
2005.
[15] S. Haoyu, K. Murali, H. Fang, and L. TV. Scalable IP
lookups using shape graphs. In Proc. ACM/IEEE
ICNP, pages 73–82, 2009.
[16] L. Hoang, J. Weirong, and P. V. K. A sram-based
architecture for trie-based IP lookup using fpga. In
Proc. IEEE FCCM, pages 33–42, 2008.
[17] L. Hyesook, Y. Changhoon, and S. Earl. Priority tries
for IP address lookup. IEEE Transactions on
Computers, 59(6):784–794, 2010.
[25] Z. Marko, R. Luigi, and M. Miljenko. Dxr: towards a
billion routing lookups per second in software. ACM
SIGCOMM Computer Communication Review,
42(5):29–36, 2012.
[26] B. Masanori and C. H. Jonathan. Flashtrie:
hash-based preﬁx-compressed trie for IP route lookup
beyond 100gbps. In Proc. IEEE INFOCOM, 2010.
[27] R. Miguel, B. Ernst, and D. Walid. Survey and
taxonomy of IP address lookup algorithms. Network,
IEEE, 15(2), 2001.
[28] D. Mikael, B. Andrej, C. Svante, and P. Stephen.
Small forwarding tables for fast routing lookups. In
Proc. ACM SIGCOMM, pages 3–14, 1997.
[29] A. Mohammad, N. Mehrdad, P. Rina, and S. Samar.
A tcam-based parallel architecture for high-speed
packet forwarding. IEEE Transactions on Computers,
56(1):58–72, 2007.
[30] NVIDIA Corporation. NVIDIA CUDA C Best
Practices Guide, Version 5.0, Oct. 2012.
[31] C. Pierluigi, D. Leandro, and G. Roberto. IP address
lookupmade fast and simple. In Algorithms-ESA’99,
pages 65–76. Springer, 1999.
[32] W. Priyank, S. Subhash, and V. George. Multiway
range trees: scalable IP lookup with fast updates.
Computer Networks, 44(3):289–303, 2004.
[33] S. Rama, F. Natsuhiko, A. Srinivas, and S. Arun.
Scalable, memory eﬃcient, high-speed IP lookup
algorithms. IEEE/ACM Transactions on Networking,
13(4):802–812, 2005.
[34] P. Rina and S. Samar. Reducing tcam power
consumption and increasing throughput. In Proc. High
Performance Interconnects, pages 107–112, 2002.
[35] H. Sangjin, J. Keon, P. KyoungSoo, and M. Sue.
Packetshader: a gpu-accelerated software router. In
Proc. ACM SIGCOMM, pages 195–206, 2010.
[18] F. Jing and R. Jennifer. Eﬃcient IP-address lookup
[36] D. Sarang, K. Praveen, and T. D. E. Longest preﬁx
with a shared forwarding table for multiple virtual
routers. In Proc. ACM CoNEXT. ACM, 2008.
matching using bloom ﬁlters. In Proc. ACM
SIGCOMM, pages 201–212, 2003.
[19] Z. Kai, H. Chengchen, L. Hongbin, and L. Bin. A
[37] H. Song, M. Kodialam, F. Hao, and T. Lakshman.
tcam-based distributed parallel IP lookup scheme and
performance analysis. IEEE/ACM Transactions on
Networking, 14(4):863–875, 2006.
[20] S. Keith. A tree-based packet routing table for
berkeley unix. In USENIX Winter, pages 93–99, 1991.
Building scalable virtual routers with trie braiding. In
Proc. IEEE INFOCOM, 2010.
[38] N. Stefan and K. Gunnar. IP-address lookup using
lc-tries. Selected Areas in Communications, IEEE
Journal on, 17(6):1083–1092, 1999.
[21] L. Layong, X. Gaogang, S. Kav´e, U. Steve,
[39] S. Venkatachary and V. George. Fast address lookups
M. Laurent, and X. Yingke. A trie merging approach
with incremental updates for virtual routers. In Proc.
IEEE INFOCOM, pages 1222–1230, 2013.
[22] H. Lim, K. Lim, N. Lee, and K.-H. Park. On adding
bloom ﬁlters to longest preﬁx matching algorithms.
IEEE Transactions on Computers (TC),
63(2):411–423, 2014.
[23] M. Mahmoud and M. Massato. A new hardware
algorithm for fast IP routing targeting programmable
routers. In Network control and engineering for Qos,
security and mobility II, pages 164–179. Springer,
2003.
[24] W. Marcel, V. George, T. Jon, and P. Bernhard.
Scalable high speed IP routing lookups. In Proc. ACM
SIGCOMM, 1997.
using controlled preﬁx expansion. ACM TOCS,
17(1):1–40, 1999.
[40] E. Will, V. George, and D. Zubin. Tree bitmap:
hardware/software IP lookups with incremental
updates. ACM SIGCOMM Computer Communication
Review, 34(2):97–122, 2004.
[41] T. Yang, Z. Mi, R. Duan, X. Guo, J. Lu, S. Zhang,
X. Sun, and B. Liu. An ultra-fast universal
incremental update algorithm for trie-based routing
lookup. In Proc. ACM/IEEE ICNP, 2012.
[42] J. Zhao, X. Zhang, X. Wang, and X. Xue. Achieving
O(1) IP lookup on gpu-based software routers. In
ACM SIGCOMM Computer Communication Review,
volume 40, pages 429–430. ACM, 2010.
50