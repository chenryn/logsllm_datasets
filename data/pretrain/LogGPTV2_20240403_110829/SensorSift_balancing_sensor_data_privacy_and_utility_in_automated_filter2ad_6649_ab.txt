iﬁcation process applies the candidate sift transformation to each
sample in the internal database. Next an ensemble of machine clas-
siﬁers are trained (using a training subset of the internal database)
to recognize the private attributes with the sifted data. We leverage
151
Figure 1: The two modes of operation in the SensorSift framework.
Thick lines represent the processing elements that are occurring with
each sensor release, while the thin lines indicate operations that are
only necessary at application installation or policy revisions.
they harvest. We do not wish to expose private attributes to well-
intentioned but possibly weak/insecure applications since those ap-
plications might accidentally expose the private information to other
third parties. We also deﬁne as out of scope the protection of private
attributes from adversaries with auxiliary information that might
also compromise those private attributes. For example, we cannot
protect the privacy of the gender attribute if an application asks for
user gender during setup, and if the user supplies that information.
We return to a discussion of these limitations in Section 9.
3. SYSTEM DESCRIPTION
While there are numerous potential deployment environments
we primarily envision SensorSift running as a service on a trusted
multi-application platform/system (like the Kinect) on which appli-
cations run locally.
Recall that the goal of the framework is to allow users to spec-
ify private attributes and allow applications to request non-private
attributes of their choosing. At application install time (or when
privacy settings are changed), the user and application declare their
respective privacy and utility goals by creating a policy which con-
tains the user desired private attribute(s) Y− and application re-
quested public attribute(s) Y +. The user selected private attributes
must always be known by the system to ensure that they can be ver-
iﬁably protected; thus, the only viable private attributes are those
for which the system’s veriﬁcation database has labels. Conversely,
applications can request access to non-private (public) attributes
which are unknown to the platform (i.e., developer invented). This
makes it possible for the system to be one of two operating modes
– included or unincluded policy mode.
In included mode (the simpler case), the user chosen private at-
tribute(s) Y− and the application requested public attribute(s) Y +
compose a veriﬁed policy for which a data processing method is
included in the platform. This means that the policy has been pre-
viously checked to ensure that the public attribute(s) do not leak
information about the private attribute(s), and in addition, the plat-
form has (shipped, or has been updated to include) a trained clas-
siﬁcation model which can recognize the public attribute(s) from
the raw sensor data. As a result, it is possible to simply output the
trained classiﬁer’s judgment on the public attribute to the applica-
tion (as a text label) for each sensor sample request (Figure 1 top
panel). From the application’s perspective this is a straightforward
way to get access to the public attribute(s) in the sensor data as
all of the inference (pre-processing and classiﬁcation) traditionally
done by application logic is handled by the platform. We expect
state of the art methods which represent the most popular ﬂavors
of mathematical machinery available for classifcation including: a
clustering classiﬁer (k-nearest neighbor — parameters: q = 9, us-
ing euclidean distance metric with majority rule tie break; classiﬁer
source: MATLAB knnclassify), linear and non-linear hyperplane
boundary classiﬁers (linear-SVM — soft margin penalty C = 10;
classiﬁer source: liblinear 1.8; kernel-SVM — soft margin penalty
C = 10, radial basis kernel function, no shrinking heuristics; clas-
siﬁer source libsvm 3.1), a biologically inspired connection based
non-linear classiﬁer (feedforward neural network — 100 hidden
layer neurons using a hyperbolic tangent sigmoid transfer func-
tion trained using gradient-descent backpropagation evaluated us-
ing mean squared normalized error, classiﬁer source: MATLAB
nnet package), and a recursive partitioning classiﬁer (random for-
est — number of random trees per model = 500; classiﬁer source:
http://code.google.com/p/randomforest-matlab/).
For each ML model, independent training rounds are performed
to obtain classiﬁers optimized for sifts of speciﬁc dimensions. A
testing subset of the database is then used to evaluate how well the
private attribute can be classiﬁed after it has been transformed by
the proposed sift.
If any of the classiﬁers can detect the presence and absence of
the private attribute(s) with rates signiﬁcantly above the platform’s
safety threshold (e.g., 10% better than chance) the sift is rejected
because it exposes private information. Alternatively if the private
attribute accuracies on the sifted data (from the internal databse)
are below the safety threshold the sift is deemed to be safe.
We again stress that while it is important for developers (or their
applications) to evaluate the resulting accuracies on both public and
private attributes, the system deploying SensorSift would in fact
only verify that the private attribute classiﬁcation accuracy is small.
Sift Application.
If a sift has been proposed and successfully
veriﬁed, it needs to be continuously applied with each data request
made by the application. The application itself cannot apply the
sifting transformation directly; this is requisite since, if the appli-
cation had access to the raw sensor data it could be exﬁltrated in
violation of the privacy goals. Instead, the SensorSift applies the
veriﬁed sifting transformations and outputs only the transformed
data to the application.
Sift Post-Processing. In contrast to included mode where attribute
labels are directly provided to the application, the application must
post-process the sifted outputs (numerical vectors) that it receives
in unincluded mode in order to determine the public attribute. This
will likely involve running a classiﬁer on the sifted sensor samples
– the classiﬁer can be trained using the database used to generate
the sifts; once trained the classiﬁer overhead should be minimal.
4. SIFT ALGORITHM - PPLS
In this work we create sifts using a novel extension of Partial
Least Squares (PLS) that we call Privacy Partial Least Squares,
or PPLS. At the heart of our technique is the long standing ap-
proach of using correlation as a surrogate for information. Given
this perspective we design an objective function which simultane-
ously aims to maximize the correlations with public attributes and
minimizes those with private attributes (while performing the struc-
tural projection of PLS). As we later show, this correlation-based
PPLS algorithm is easy to use and also very effective within the
context of automated face understanding; since our algorithm is
domain independent we believe that PPLS is well suited to various
datasets but this has not yet been veriﬁed.
Intuitively, our approach uses correlation between data features
and attribute labels to ﬁnd ‘safe regions’ in feature space which
Figure 2: The left panel shows a simpliﬁed conﬁguration of feature
sets for two distinct attributes A (private) and B (public). The goal of
SensorSift is to ﬁnd the region(s) in feature space which are in the pub-
lic feature set but not in the private one (i.e. indicated with the color red
in the left panel). Raw data can be then re-represented in terms of how
strongly it maps to this privacy aware region of the feature space. The
right panel depicts how additional public attributes (C-F) which are
invented by application developers map onto the feature space of our
example. Note that in many cases it is possible to ﬁnd privacy respect-
ing regions of the region space through which to re-interpret (sift) raw
data, however in some instances (attribute D) it may not be possible to
separate attributes which have strong causal/correlation relationships
(i.e. left eye color from right eye color).
are strongly representative of public but not private attributes (Fig-
ure 2). We then project raw sensor data onto these safe regions and
call the result of the projection a loading or ‘sift’ vector.
Privacy Partial Least Squares Now that we have explored the
intuition behind our approach we turn our attention to the details.
To reiterate, Partial Least Squares (PLS) is a supervised technique
for feature transform or dimension reduction [17]: given a set of ob-
servable variables (raw features) and predictor variables (attributes),
PLS searches for a set of components (called latent vectors) that
perform a simultaneous decomposition of the observable and pre-
dictor variables intended to maximize their mutual covariance. PLS
is particularly suited to problem instances where the dimensional-
ity of the observed variables is large compared to the number of
predictor variables (this is generally true for rich sensor streams).
]T a n × dx matrix of observable variables
Let X be [x1,··· ,xdx
] a n× dy a matrix of predictor
(input features), and Y = [y1,··· ,ydy
variables (attributes), where n is the number of training samples,
dx is the dimension of input features, dy is the dimension/number
of attributes. Without loss of generality, X, Y are assumed to be
random variables with zero mean and unit variance. Any unit vector
w speciﬁes a projection direction and transforms a feature vector x
to wT x. In matrix notation, this transforms X to Xw.The sum of the
covariances between the transformed features Xw and the attributes
Y can be computed as
cov(Xw,Y )2 = w
(cid:2)
(cid:2)
X
YY
(cid:2)
Xw
(1)
The PLS algorithm computes the best projection w that maximizes
the covariance:
(2)
(cid:2)
(cid:3)
cov(Xw,Y )2
f ind max
w
s.t. w(cid:2)w = 1
We propose a novel variant of PLS, Privacy Partial Least Squares
(PPLS), that handles both public attributes and private attributes.
d+ ] be a n × d+ public attribute matrix, and
,··· ,y+
Let Y + = [y+
1
−
−
d− ] a n× d+ private attribute matrix, where d+ is
,··· ,y
Y− = [y
1
152
Algorithm 1 Privacy Partial Least Squares
1. Set j = 0 and cross-product S j = X(cid:2)Y +
2. if j > 0, S j = S j−1 − P(P(cid:2)P)−1P(cid:2)S j−1
3. Compute
the
(cid:2)
(cid:3)
largest
w j = λw j
eigenvector
w j:
j S j − X(cid:2)Y−(Y−)(cid:2)X
S(cid:2)
4. Compute p j = X(cid:2)Xw j
w(cid:2)
j X(cid:2)Xw j
5. If j = k, stop; otherwise let P = [p0,··· , p j] and j = j + 1
and go back to step 2
the number of public attributes and d− is the number of private
attributes. We want to ﬁnd a projection direction w that both maxi-
mizes the covariance cov(Xw,Y +) and minimizes cov(Xw,Y−).
This is achieved by optimizing the difference of covariances:
cov(Xw,Y +)2 − λ∗ cov(Xw,Y
−)2
(3)
(cid:3)
(cid:2)
f ind max
s.t. w(cid:2)w = 1
w
The ﬂow of the PPLS algorithm is outlined in the algorithm box
(Algorithm 1). To transform X to more than one dimensions, we
follow the PLS approach and develop a sequential scheme: we it-
eratively apply Equation 3, subtracting away covariances that are
already captured in the existing dimensions (Step 2 in the Algo-
rithm). Note that we only remove covariances from cov(Xw,Y +)
but not cov(Xw,Y−), to ensure that every included dimension w is
privacy-perserving by itself for all private attributes.
Free Parameters. There are two key free parameters of the PPLS
algorithm, a λ term (privacy emphasis) and the number of sift di-
mensions to release K. In general we only release several dimen-
sions from these sift vectors as a type of dimensionality reduction
step which minimizes the risk of reconstruction. Despite the small
size of the outputs sifts we ﬁnd that public attributes can be cor-
rectly inferred with minimal accuracy degradation 7.
The λ term in Equation 3, represents the relative importance of
privacy with higher λ values indicating an increased emphasis on
removing private features (with a possible loss to utility).
5. DATASET
Our evaluation is based on the the Public Figures Face Database
(PubFig) [11] which is a set of 58,797 images of 200 people (pri-
marily Hollywood celebrities) made available as a list of URLs (see
http://www.cs.columbia.edu/CAVE/databases/pubfig/download ).
The PubFig images are taken in uncontrolled situations with non-
cooperative subjects and as a result there is signiﬁcant variation in
pose, lighting, expression, scene, camera, imaging conditions and
parameters. Due to the size and real-world variation in the PubFig
dataset we felt that it presents an appropriate foundation on which
to evaluate SensorSift.
Validation, Alignment, and Rescaling. We began by download-
ing the PubFig image URLs using an automated script which would
keep track of broken links and corrupted images. At the time of our
data collection we found 45,135 valid URLS (77% of the advertised
58,797 images). For each image in the database PubFig provides
four pixel coordinates which deﬁne the face region; we extracted
this face region for each image aligned it to front-center (via afﬁne
rotation using the roll parameter). Next we rescaled each image to
128x128 pixels using bicubic interpolation and antialiasing.
Feature Extraction and Normalization.
In addition to the raw
RGB pixels, we extracted image derivatives of each face image to
enrich the feature space of the raw data and provide a larger starting
dimensionality to our algorithm. The four features we computed
are popular in the computer vision literature and include raw RGB,
image intensity, edge orientation, and edge magnitude [12]. Af-
ter computing these transforms, we apply an energy normalization
(x− μ)/(2·σ) to the feature values of each face to remove outliers.
Lastly, we concatenate all of the normalized image features for into
a row vector and create a matrix to hold the entire dataset (45,135
rows/faces and 98304 columns/features per face).
PCA Compression. Next, we compute a PCA compression which
is applied to the entire database (10:1 compaction ratio, > 95% en-
ergy maintained) to decrease the feature dimensionality of our face
database and enable the PPLS algorithm to operate within reason-
able memory constraints (16GB per node).
6. EXPERIMENTS AND METRICS
The privacy sifts that we compute are intended to provide quan-
titative assurances which adhere to a speciﬁed policy. Policies in
turn are based on a set of user declared private attributes and de-
veloper requested public attributes. In this section we describe how
we selected the attributes to include in the polices we evaluate. In
addition we describe the metrics used to evaluate the quality of the
sift generated for a particular policy.
6.1 Attribute Selection
The authors of the PubFig database were interested in providing
a large vocabulary of attributes over each image to power a text-
based ‘face search engine’ [10] Thus in addition to face coordinates
and rotation parameters, each image in the PubFig dataset is anno-
tated with classiﬁcation scores for 74 different attributes. These
scores are numerical judgments produced by a set of machine clas-
siﬁers each trained for a unique attribute.
For analytical tractability we were interested in reducing the set
of 74 available attributes to a more manageable number. Since we
are using correlation as a proxy for information in our PPLS algo-
rithm we analyzed the correlations between the available attributes
to get a sense for the redundancy in the data.
We found two large clusters of attributes which were centered
around ‘Male’ and ‘Attractive Female’. The ‘Male’ attribute was
very closely correlated with the attributes: ‘Sideburns’, ‘5 oClock
Shadow’, ‘Bushy Eyebrows’, ‘Goatee’, ‘Mustache’, ‘Square Face’,
‘Receding Hairline’, and ‘Middle Aged’. Conversely, ‘Attractive
Female’ was very closely related to: ‘Wearing Lipstick’, ‘Heavy
Makeup’, ‘Wearing Necklace’, ‘Wearing Earrings’, ‘No Beard’,
and ‘Youth’.
Given their strong connection to a large set of the available at-
tributes the ‘Male’ and ‘Attractive Female’ attributes were clear