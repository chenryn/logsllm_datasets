### Ad Integration and Measurement Setup

An advertisement is embedded within a publisher's webpage, with the ad network providing an ad located at a static URL, which is then wrapped in an iframe. The publisher places an iframe in their webpage, pointing to the ad network's iframe. Our ad page, residing at the static URL, contains another iframe that loads the measurement page. This measurement page includes a JavaScript driver program. Each instance of the measurement page and all requests it generates are uniquely identified by a version 4 UUID [29], which is included in both the URL query string and the domain name (except for the measurement page, where it is only in the query string).

### Components of the Measurement Page

The measurement page loads:
- A dummy ad image.
- A minified jQuery library [26] hosted by jquery.com.
- A JSON encoding and decoding library hosted on our servers.
- The experimentâ€™s JavaScript "driver script."

### Experiment Execution

The measurement page's `onLoad` event handler initiates the experiment by invoking the driver script. The driver script randomizes the order of a list containing `nosec`, `goodsec`, and `badsec` domains. It then iterates over this list, creating an image tag for each domain. The source property of each image tag points to an image hosted on the respective domain. This causes the participant's browser to attempt to resolve the domain name and load the image.

To ensure data collection before the participant navigates away from the page, the driver script does not wait for each image to complete loading before proceeding to the next domain. Instead, it creates all image tags in rapid succession. The driver script also registers `onLoad` and `onError` callbacks for each image tag to monitor the success or failure of the load. When a callback is triggered, the outcome, along with information about the browser, is sent via jQuery's AJAX POST mechanism to a PHP logging script on our servers.

Once all image tags have invoked either an `onLoad` or `onError` callback, the driver script creates a final image tag with a unique `nosec` domain (UUID.complete.dnsstudy.ucsd.edu). A DNS query for this domain serves as a "completion signal," allowing us to identify UUIDs where the user did not navigate away from the ad publisher's page before completing the trial. Data from any page load that did not generate the completion signal is discarded.

### Identifying Successful DNS Resolution

Initially, we intended to use `onLoad` and `onError` handlers attached to each test resource's image tag to measure the outcome of HTTP requests. However, we found 9,754 instances where the `onError` handler fired, followed by the test resource being loaded, and then the `onLoad` handler firing. Additionally, for 1,058 test resource loads, the `onLoad` handler fired without corresponding DNS lookups or HTTP requests. Consequently, we adopted a different approach to identify resolution success.

Since we cannot directly inspect the DNS caches of participants and their recursive resolvers, we infer DNS resolution success from the presence of an HTTP request whose Host header or request line specifies a particular test resource's domain name. If we observe a completion signal for a particular UUID but do not see an HTTP request for a certain test resource type, we infer that the DNS resolution for that UUID-test resource pair failed. Note that a completion signal can be recorded after observing just a DNS query; what matters is whether the driver script attempted to load the completion signal resource, not whether it succeeded.

This strategy may overestimate the number of DNS resolution failures due to TCP connections that are dropped or aborted before the HTTP request reaches our servers. We describe our serving infrastructure in Section 3.4, which is designed to minimize such errors.

### Cache Control

To prevent caching, which would not generate the necessary HTTP and DNS logs, we implemented measures listed in Table 1. The most important is the use of a fresh, random UUID for each ad impression, which acts as a cache-buster, preventing cache hits in both DNS resolvers and browsers.

If, despite these efforts, our static ad page is cached, causing the measurement page to be requested with a cached UUID, we detect this using a memcached cache as a UUID dictionary. If a stale UUID is detected, the stale measurement page is redirected to one with a fresh UUID.

### Serving Infrastructure

To handle the large bursts of traffic generated by our study, we rented five m1.large instances running Ubuntu 10.04 on Amazon's Elastic Compute Cloud (EC2). All instances hosted identical BIND 9 (DNS), nginx (HTTP), and beanstalkd (work queue) servers. The nginx servers supported PHP 5 CGI scripts via FastCGI. Tables 2 and 3 show the adjustments made to the server configuration parameters to ensure a low rate of dropped connections.

One instance ran a MySQL server, and another ran a memcached server. To increase the EC2 instances' ability to accept large quantities of short TCP connections, we configured the machines to timeout connections in the FIN-WAIT-2 state after a fraction of the default time and to quickly recycle connections in the TIME-WAIT state. This was achieved by setting the `tcp_fin_timeout` and `tcp_tw_recycle` sysctl variables to 3 and 1, respectively.

### DNS and BIND 9 Configuration

All five EC2 instances ran identical BIND 9 DNS servers, providing authoritative DNS resolution for all `nosec`, `goodsec`, and `badsec` domains. We used Round Robin DNS to distribute the load across all five DNS and web servers. To reduce the chance of load failures due to large reply packets, our DNS servers were configured to refrain from sending unsolicited RRs that are not mandated by the DNS specification, using BIND's `minimal-responses` option. Specifically, we only send extra DNSSEC RRs in response to queries that include the DNSSEC OK option (approximately two-thirds of all queries).

Our five BIND servers are authoritative for all domain names used in our study, except for the domain of the static ad URL that iframes the measurement page. These domains were hosted on Amazon Route 53 DNS service, as we were not interested in measuring their resolution.

### Data Gathering

Our analysis of the behavior of participants' browsers and resolvers is based on three data sources: nginx access logs, BIND request logs, and MySQL tables containing the outcomes and browser info reported by AJAX POST messages. Nginx was configured to use its default "common log format" [34], while BIND's log format was modified and recompiled to include enhanced logging of requests and replies. These additional log details allow us to link requests to replies and determine if a lookup fell back to TCP due to truncation of the UDP reply. BIND logs are also used to identify the UUIDs for which a completion signal was sent and to determine which resolvers were associated with a particular UUID.

### Experiment Scheduling

In preliminary test runs, we found that the successful load rates for test resources varied depending on the time of day. To account for this variability, we conducted an extended study lasting a full week. Every two hours, we paid the ad network enough for 10,000 impressions to be shown.

### Results

#### Overview

Over the course of the 84 segments of our week-long experiment, we collected data from 529,294 ad impressions, receiving DNS queries from 35,010 unique DNS resolvers. Figure 4 shows the distribution of unique resolvers performing resolution for each UUID. The distribution has a long tail, although 98% of UUIDs used at most 25 resolvers. We mapped each resolver's IP address to its ASN and found that 92.75% of the clients surveyed used recursive resolvers whose IP addresses resided in the same ASN, and 99.12% used resolvers in two or fewer ASNs. This is consistent with our expectation that most users use their default DNS resolvers provided by their ISPs, while a small percentage of "power users" might configure their systems to use open resolvers like Google Public DNS [5].

Figure 5 shows the total number of requests received during each minute after the start of a run, aggregated over all runs. Each ad buy results in a delay of approximately 20 minutes from the time we released funds to the ad network, with incoming traffic spiking for 15 minutes, peaking around 25 minutes into the run and tapering off for the remainder of the first hour.

We also observed considerable drop-off at each stage of executing the experiment code in the participants' browsers. Figure 6 illustrates the number of UUIDs observed reaching each stage of the experiment. 15.88% of the ad impressions did not even manage to load the driver script, and only 63.02% of the impressions resulted in a completed experiment. This compares favorably with past studies, such as Huang et al. [22], which had only a 10.97% total completion rate.

#### DNSSEC Resolution Rates

The first question we are interested in answering is the impact on load failure rates of introducing DNSSEC for a given domain. Table 4 shows the failure rates for `nosec`, `goodsec`, and `badsec` domains:

| Class   | Failure Rate (CI 0.99) |
|---------|------------------------|
| nosec   | 0.7846%                |
| goodsec | 1.006%                 |
| badsec  | 2.661%                 |

[5] https://developers.google.com/speed/public-dns/