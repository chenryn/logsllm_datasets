an ad in a publisher’s web page. We provide the ad
network with an ad located at a static URL which is
wrapped in an iframe by the ad network. The pub-
lisher places an iframe in its web page whose source
3We attempted to use 2048-bit keys, but at the time of the
experiment, our domain registrar, GoDaddy, did not support
keys that large.
Figure 3: Client-side experiment setup
points to the iframe wrapping the ad. Our ad page
residing at the static URL iframes the measure-
ment page, which contains the JavaScript driver
program. Each instance of the measurement page
and all requests generated by it are linked by a ver-
sion 4 UUID [29] placed in both the URL query
string and the domain name (with the exception
of the measurement page, which only has it in the
query string).
The measurement page loads a dummy ad image
and 3 pieces of JavaScript which are the following:
• A miniﬁed jQuery4 [26]
library hosted by
jquery.com
• A JSON encoding and decoding library hosted
on our servers
• The experiment’s JavaScript “driver script”
The measurement page body’s onLoad handler
commences the experiment by invoking the driver
script. The driver script randomizes the order of a
list of nosec, goodsec, and badsec domains then it-
erates over that list, creating for each domain an
image tag whose source property points to an image
hosted on that domain. The creation of the image
tag causes the participant’s browser to attempt to
resolve the domain name and load an image from it.
Because we need to gather data for all domains in the
list before the participant navigates away from the
web page containing the ad, the driver script does
not wait for each image to complete its load attempt
before proceeding to the next domain.
Instead, it
creates all of the image tags in rapid succession. The
driver script also registers onLoad and onError call-
backs on each image tag created to monitor whether
each load succeeds or fails. When a callback for
an image ﬁres, the outcome of the load, along with
4We used jQuery to minimize browser compatibility issues.
576  22nd USENIX Security Symposium 
USENIX Association
4
info about the browser, are sent via jQuery’s AJAX
POST mechanism to a PHP logging script on our
servers. Once the driver script detects that all im-
age tags have invoked either an onLoad or onError
callback, it creates a ﬁnal image tag whose source
domain is a unique nosec domain (UUID.complete.
dnsstudy.ucsd.edu). A DNS query for such a do-
main serves as a “completion signal” and allows us
to identify UUIDs where the user did not navigate
away from the ad publisher’s page before completing
the trial. We discarded the data from any page load
which did not generate the completion signal.
3.2 Identifying successful DNS resolu-
tion
Our original intent was to use onLoad and onError
handlers attached to each test resource’s image tag
to measure the outcome of the HTTP requests for
test resources. If the onLoad handler was called, we
would record a successful HTTP request; if instead
the onError handler was called, we would record
a failed HTTP request. These results are reported
back to our servers via AJAX POST. However, we
found 9754 instances of the onError handler ﬁr-
ing, the test resource subsequently being loaded, and
then the onLoad handler ﬁring. For another 1058
test resource loads, the onLoad handler ﬁred, despite
our receiving neither the corresponding DNS lookups
nor the HTTP requests for the test resources in ques-
tion. Consequently, we looked to diﬀerent avenues
for identifying resolution success.
Because we are not able to ascertain the result of
a DNS lookup attempt via direct inspection of the
DNS caches of our participants and their recursive
resolvers, we must infer it from the presence of an
HTTP request whose Host header or request line
speciﬁes a particular test resource’s domain name as
an indicator of DNS resolution success. Thus, if we
observed a completion signal for a particular UUID
but did not observe an HTTP request associated
with that UUID for a certain test resource type, we
infer that the DNS resolution for that UUID-test re-
source pair failed. Note however that we can record
a completion signal after observing just a DNS query
for it: what matters is whether the driver script at-
tempted to load the completion signal resource, not
whether it succeeded in doing so.
This strategy has the potential to over-estimate
the number of DNS resolution failures due to TCP
connections that are attempted and are dropped or
aborted before the HTTP request is received by our
servers. The only source of this type of error that we
are able to control is our HTTP servers’ ability to
accept the oﬀered TCP-connection load at all times
throughout the experiment. We describe our serving
infrastructure in Section 3.4. We believe it is suﬃ-
ciently robust against introducing this type of error.
3.3 Cache control
Because requests fulﬁlled by cache hits do not gen-
erate HTTP and DNS logs that we can analyze, we
took measures, described in Table 1, to discourage
caching. Most importantly, the use of a fresh, ran-
dom UUID for each ad impression serves as a cache-
buster, preventing cache hits in both DNS resolvers
and browsers.
If, despite our eﬀorts, our static ad page is cached,
causing the measurement page to be requested with
a cached UUID, we must detect it and give the cur-
rent participant a fresh UUID. To this end, we used
a memcached cache as a UUID dictionary to detect
when the measurement page was loaded with a stale
UUID. If this occured, the stale measurement page
was redirected to one with a fresh UUID.
3.4 Serving infrastructure
To run our study, which generates large bursts of
traﬃc, we rented 5 m1.large instances running
Ubuntu 10.04 on Amazon’s Elastic Compute Cloud
(EC2). All 5 instances hosted identical BIND 9
(DNS), nginx (HTTP), and beanstalkd (work queue)
servers. The nginx servers supported PHP 5 CGI
scripts via FastCGI. Tables 2 and 3 show the adjust-
ments made to the servers’ conﬁguration parameters
to ensure a low rate of dropped connections.
One instance ran a MySQL server, another ran a
memcached server. To increase our EC2 instances’
ability to accept large quantities of short TCP con-
nections, we conﬁgured our machines to timeout con-
nections in the FIN-WAIT-2 state after only a frac-
tion of the default time and to quickly recycle con-
nections in the TIME-WAIT state. This was accom-
plished by setting the sysctl variables tcp ﬁn timeout
and tcp tw recycle to 3 and 1, respectively.
3.4.1 DNS & BIND 9
All 5 EC2 instances ran identical BIND 9 DNS
servers providing authoritative DNS resolution for
all nosec, goodsec, and badsec domains. We used
Round Robin DNS to distribute load across all 5
DNS and web servers. In order to reduce the chance
of load failures due to large reply packets, our DNS
servers were conﬁgured (using BIND’s minimal-
responses option) to refrain from sending unso-
licited RRs that are not mandated by the DNS
speciﬁcation. Speciﬁcally, we only send the extra
DNSSEC RRs in response to queries which include
the DNSSEC OK option (approximately two thirds
of all queries).
USENIX Association  
22nd USENIX Security Symposium  577
5
Type
Value
Used on
HTTP header
Cache-Control: no-cache, must-revalidate
HTTP header
Expires: Sat, 26 Jul 1997 00:00:00 GMT
HTML 
HTML 
http-equiv="Pragma" content="no-cache"
http-equiv="Expires" content="−1"
static ad page, measurement page,
driver script
static ad page, measurement page,
driver script
static ad page, measurement page
static ad page, measurement page
Table 1: Description of the HTTP and HTML anti-caching measures and their uses.
worker processes
worker rlimit noﬁle
worker connections
8
65,535
65,000
Table 2: Non-default nginx server conﬁg params.
PHP FCGI CHILDREN
PHP FCGI MAX REQUESTS
50
65,000
Table 3: Non-default PHP FastCGI conﬁg params.
Our 5 BIND servers are authoritative for all do-
main names used in our study except for the domain
of the static ad URL that iframes the measurement
page. Because we were not interested in measuring
resolution of those domains we hosted their author-
itative servers on Amazon Route 53 DNS service.
3.5 Data gathering
Our analysis of
the behavior of participants’
browsers and resolvers is based on the following 3
data sources: nginx access logs, BIND request logs,
and MySQL tables containing the outcomes and
browser info reported by AJAX POST messages.
Nginx was conﬁgured to use its default “common
log format” [34], which includes a timestamp of each
request, the URL requested, the user agent string,
among other details about the request and its cor-
responding response. However, BIND’s log format
is hardcoded and compiled into the binary. Its de-
fault logging behavior only provides basic informa-
tion about queries (e.g., a timestamp, the domain
name, the source IP and port).
It does not pro-
vide information about replies and excludes certain
important diagnostic ﬁelds. We modiﬁed and re-
compiled BIND to add enhanced logging of requests
and replies. Log lines for requests were modiﬁed to
include the request’s transaction ID and the value
of the sender’s UDP payload size from the EDNS0
OPT RR (if present) [39]. We added support for re-
ply logs that include the transport-layer protocol in
use, the size of the reply, and the transaction ID.
With these additional log details, we are able to
link requests to replies and determine if a lookup
fell back to TCP due to truncation of the UDP re-
ply. BIND logs are also used to identify the UUIDs
for which a completion signal was sent as well as
to determine which resolvers were associated with a
particular UUID.
The client-side driver script AJAX POSTs the out-
come of each test resource load along with additional
metadata regarding the experiment and the state of
the browser environment under which it is running.
These data are logged by our servers.
3.6 Experiment scheduling
In our preliminary test runs of the study, we found
that the successful load rates for test resources varied
depending on the time of day at which the experi-
ment was conducted. To account for this variability,
we conducted an extended study lasting for a full
week. Every two hours, we paid ad network enough
for 10,000 impressions to be shown.
4 Results
In this section we describe the results of our measure-
ments. We begin by providing an overview of our
data. Then, in Section 4.1 we describe our measure-
ments of the diﬀerential impact of DNSSEC on reso-
lution success. Finally, in Section 4.2, we describe a
number of confounding network artifacts that plague
any attempt to use advertisement surveys to mea-
sure small signals against the background of a noisy
Web environment.
Over the course of the 84 segments of our week-
long experiment, we collected data from 529,294
ad impressions, receiving DNS queries from 35,010
unique DNS resolvers. Figure 4 shows the distri-
bution of unique resolvers performing resolution for
each UUID. The distribution has a long tail, al-
though 98% of UUIDs used at most 25 resolvers. We
mapped each resolver’s IP address to its ASN and
found that 92.75% of the clients surveyed were ob-
served using recursive resolvers whose IP addresses
resided in the same ASN, and 99.12% used resolvers
578  22nd USENIX Security Symposium 
USENIX Association
6
I
s
D
U
U
f
o
n
o
i
t
c
a
r
F
4
.
0
3
.
0
2
.
0
1
.
0
0
.
0
Distribution of unique resolvers per UUID
i
d
e
v
e
c
e
r
s
t
s
e
u
q
e
r
P
T
T
H
c
e
s
o
n
0
0
0
0
8
0
0
0
0
6
0
0
0
0
4
0
0
0
0
2
0
2
3
4
6
5
7
# resolvers
8
9
10
280
0
10
30
20
50
Minutes since run started
40
60
70
Figure 4: Distribution of the number of unique
resolvers observed performing DNS resolution per
UUID. Tail not shown.
Figure 5: Plot showing total number of requests
received during each minute after the start of a run,
aggregated over all runs.
in two or fewer ASNs. This is consistent with our
expectation that most users use their default DNS
resolvers provided by their ISPs, while a small per-
centage of “power users” might conﬁgure their sys-
tems to take advantage of open resolvers such as
Google Public DNS.5
As shown in Figure 5 each ad buy results in a
delay of approximately 20 minutes from the time we
released funds to the ad network, at which point im-
pressions start to appear. Incoming traﬃc spikes for
15 minutes, peaking around 25 minutes into the run
and tapering oﬀ for the remainder of the ﬁrst hour.
We also witnessed considerable drop-oﬀ at each
stage of executing experiment code in the partic-
pants’ browsers. Figure 6 illustrates the number of
UUIDs observed reaching each stage of the experi-
ment. 15.88% of the ad impressions that we paid for
did not even manage to load the driver script and
only 63.02% of the impressions we paid for actually
resulted in a completed experiment. This compares
favorably with past studies. For instance, prior work
by Huang et al. [22], which also used ad networks
to recruit participants to run experiment code, had
only a 10.97% total completion rate.
4.1 DNSSEC Resolution Rates
The ﬁrst question we are interested in answering
is the impact on load failure rates of introducing
DNSSEC for a given domain. Table 4 shows the
5https://developers.google.com/speed/public-dns/
Class
Failure rate CI 0.99
nosec
goodsec
badsec
0.7846%
1.006%
2.661%