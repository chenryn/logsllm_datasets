19
36
10
9
3
12
9
9
4
4
8
195
6
8
2
1
14
6
1
8
4
3
65
7,061
1,554
913
952
7,077
157
30
7,957
9,088
302
2,373
54
7,113
4,833
77
3
5,769
544
129
16
786
1
2,579
449
2,560
8,193
9,139
8,951
9,077
9,003
8,835
4,590
8,495
381
2,922
2,730
1,991
383
174
5,018
7,875
8,980
1,729
1,616
4,690
1,728
2,575
8,674
3,568
1,137
3,325
5,747
8,669
33.1%
86.8%
60.7%
59.9%
26.2%
81.1%
29.2%
13.9%
2.0%
56.3%
83.6%
49.0%
67.8%
19.0%
33.3%
0.0%
50.2%
64.6%
29.3%
0.0%
57.5%
0.0%
37.3%
36.5%
33.5%
12.6%
1.8%
3.0%
2.7%
4.5%
4.3%
33.4%
3.9%
79.1%
60.7%
24.0%
58.0%
79.9%
13.1%
77.4%
29.2%
4.3%
18.7%
37.5%
75.8%
93.7%
36.6%
7.7%
76.9%
56.8%
33.7%
37.6%
24.5%
15
14
13
11
10
10
8
8
4
3
2
2
2
1
1
1
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
Table 2: Popularity and blockrate for the web standards that are used on at least 1% of the Alexa 10k or have at least one
associated CVE advisory in the last three years.
Columns one and two list the name and abbreviation of the standard.
Column three gives the number of features (methods and properties) from that standard that we were able to instrument.
Column four includes the number of pages that used at least one feature from the standard, out of the entire Alexa 10k.
Column ﬁve shows the number of sites on which no features in the standard executed in the presence of advertising and
tracking blocking extensions (given that the website executed at least one feature from the standard in the default case),
divided by the number of pages where at least one feature from the standard was executed. In other words, how often the
blocking extensions prevented all features in a standard from executing, given at least one feature would have been used.
Column six shows the number of CVEs associated with this standard’s implementation in Firefox within the last three years.
105Figure 6: Popularity of standards versus their block rate, on a log scale.
Column ﬁve of table 2 shows the number of CVEs associ-
ated with the standard’s implementation in Firefox within
the last three years. As the table shows, some implemen-
tations of web standards have been associated with a large
number of security bugs even though those standards are not
popular on the web. Other standards are associated with a
large number of security vulnerabilities despite being blocked
by advertising and tracking blocking extensions.
For example, the Web Audio API [4] standard is unpopu-
lar with website authors, and implementing it the browser
though has exposed users to a substantial number of security
vulnerabilities. We observed the Web Audio API standard
in use on fewer that 2% of sites in our collection, but its im-
plementation in Firefox is associated with at least 10 CVEs
in the last 3 years. Similarly, WebRTC [9] is used on less
than 1% of sites in the Alexa 10k, but is associated with 8
CVEs in the last 3 years.
The Scalable Vector Graphics [13] standard is an example
of a frequently blocked standard that has been associated
with a signiﬁcant number of vulnerabilities. The standard is
very frequently blocked by advertising and tracking blocking
extensions; the standard is used on 1,554 sites in the Alexa
10k, but is prevented from executing in 87% of cases. At
least 14 CVE’s have been reported against Firefox’s imple-
mentation of the standard in the last 3 years.
5.7 Site Complexity
We also evaluated sites based on their complexity. We
deﬁne complexity as the number of standards used on a given
website. As Figure 8 shows, most sites use many standards:
between 14 and 32 of the 74 available in the browser. No
site used more than 41 standards, and a second mode exists
around the zero mark, showing that a small but measurable
number of sites use little to no JavaScript at all.
Figure 7: Comparison of block rates of standards using
advertising vs. tracking blocking extensions.
AJAXALSBABECOCSS−CRCSS−FOCSS−OMCSS−VMDODOMDOM1DOM2−CDOM2−EDOM2−HDOM2−SDOM2−TDOM3−CDOM3−XDOM4DOM−PSDUEECEMEFFAFULLGEOGIMGPH−BH−CH−CMH−HIH−PHRTHTMLHTML5HTML51H−WBH−WSH−WWIDBMCDMCSMSEMSRNSNTPEPLPTPT2PVRTSDSELSLCSOSVGSWTCTPEUIEURLUTLVWCRWEBAWEBGLWEBVTTWNWRTC101001,00010,0000%25%50%75%100%Block rateSites using this standard●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●PT2UIEWCRWRTC0%25%50%75%100%0%25%50%75%100%Ad block rateTracking block rateSites using feature●●●●100101102103106Table 3 shows the results of this veriﬁcation. The ﬁrst
column lists each round of measurement, and the second
column lists the number of new standards encountered that
had not yet been observed in the previous rounds (averaged
across the entire Alexa 10k). As the table shows, the average
number of new standards observed on each site decreased
with each measurement, until the 5th measurement for each
site, at which point we did not observe any new features
being executed on any site.
From this we concluded that ﬁve rounds was suﬃcient for
each domain, and that further automated measurements of
these sites were unlikely to observe new feature usage.
6.2 External Validation
Figure 8: Probability density function of number of standards
used by sites in the Alexa 10k.
6. VALIDATION
This study measures the features executed over repeated,
automated interactions with a website. We treat these auto-
mated measurements as representative of the features that
would be executed when a human visits the website.
Thus, our work relies on our automated measurement
technique triggering (at least) the browser functionality a
human user’s browser will execute when interacting with
the same website. This section explains how we veriﬁed this
assumption to be reasonable.
6.1 Internal Validation
Round # Avg. New Standards
2
3
4
5
1.56
0.40
0.29
0.00
Table 3: Average number of new standards encountered on
each subsequent automated crawl of a domain.
As discussed in Section 4.3.1, we applied our automated
measurement technique to each site in the Alexa 10k ten
times, ﬁve times in an unmodiﬁed browser, and ﬁve times
with blocking extensions in place. We measured ﬁve times in
each condition with the goal of capturing the full set of func-
tionality used on the site, since the measurement’s random
walk technique means that each subsequent measurement
may encounter diﬀerent, new parts of the site.
A natural question then is whether ﬁve measurements are
suﬃcient to capture all potentially encountered features per
site, or whether additional measurements are necessary. To
ensure that ﬁve measurements were suﬃcient, we examined
how many new standards were encountered on each round of
measurement. If new standards were still being encountered
in the ﬁnal round of measurement, it would indicate we
had not measured enough, and that our data painted an
incomplete picture of the types of features used by each site.
Figure 9: Histogram of the number of standards encoun-
tered on a domain under manual interaction that were not
encountered under automated interaction.
We also tested whether our automated technique observed
the same feature use as human web users encounter. We
randomly chose 100 sites to visit from the Alexa 10k and
interacted with each for 90 seconds in a casual web brows-
ing fashion. This included reading articles and blog posts,
scrolling through websites, browsing site navigation listings,
etc.
We interacted with the home page of the site (the page
directed to from the raw domain) for 30 seconds, then clicked
on a prominent link we thought a typical human browser
would choose (such as the headline of a featured article) and
interacted with this second page for 30 more seconds. We
then repeated the process a third time, loading a third page
that was interacted with for another 30 seconds.
After omitting pornographic and non-English sites, we
completed this process for 92 diﬀerent websites. We then
compared the features used during manual interaction with
our automated measurements of the same sites. Figure 9
provides a histogram of this comparison, with the x-axis
showing the number of new standards observed during man-
ual interaction that were not observed during the automated
interaction. As the graph shows, in the majority of cases
(83.7%), no features were observed during manual interaction
that the automated measurements did not catch.
The graph also shows a few outliers, including a very
signiﬁcant one, where manual interaction triggered standards
that our automated technique did not. On closer inspection,
this outlier was due to the site updating its content between
when we performed the automated measurement and the
0%1%2%3%4%010203040Numer of standards usedPortion of all sites107manual measurement. The outlier site, buzzfeed.com, is a
website that changes its front page content hour to hour. The
site further features subsections that are unlike the rest of
the site, and can have widely diﬀering functionality, resulting
in very diﬀerent standard usage over time. We checked to
see if standards were used under manual evaluation of the
outlier that were not observed during automated testing on
the rest of the Alexa 10k, and did not ﬁnd any.
From this we conclude that our automated measurement
technique did a generally accurate job of elicit the feature
use a human user would encounter on the web, even if the
technique did not perfectly emulate human feature use in all