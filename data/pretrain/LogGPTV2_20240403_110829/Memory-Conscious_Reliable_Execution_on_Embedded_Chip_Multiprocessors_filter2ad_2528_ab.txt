are compared. In addition, race conditions can be avoided
if we feed the execution results directly to the checksum
during the duplicate execution.
Figure 5 illustrates an example application of the NRWD
scheme to the original loop nest given in Figure 4. In this
example, chkp and chkd are the checksums for the primary
execution and the duplicate execution respectively. After
both the loop nests ﬁnish their executions, we compare chkp
and chkd for error detection. The operator ⊕ represents ex-
clusive OR operation. Note that we could employ a more
complex aggregation operation, such as CRC, to increase
the resilience against multiple errors. The choice of ag-
gregation operator is orthogonal to our memory space op-
timization approach. We can notice in Figure 5(b) that the
duplicates for arrays V and W are eliminated compared
to Figure 4(b), since V is read-only and W is write-only.
Therefore, NRWD is more efﬁcient in terms of memory
space consumption in this example than the FULL scheme.
3.3 The CDDR Scheme
Note that the NRWD scheme still needs to create dupli-
cates for arrays that are both read and written within the
loop nest. This is because we need a duplicate array to store
the values that might be used in later execution of the loop
nest. For example, if we replace the read reference to ar-
ray ˜U with a reference to array U in Figure 5(b), we might
read wrong values from U without a proper synchronization
with the primary execution. The reason is that the primary
execution is storing new values to array U , and there is no
guarantee whether the duplicate execution will retrieve new
values or the old values of U without a proper synchroniza-
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:27:26 UTC from IEEE Xplore.  Restrictions apply. 
int U [100], V [100];
int W [100], X[100];
for(i = 0; i < 100; i + +) {
U [i] = V [i] + 1;
V [i] = W [i] ∗ 2;
}
(a) Original loop nest.
for(i = 0; i < 100; i + +) {
X[i] = U [i] + V [i] + W [i];
}
(b) The loop nest that follows
the above loop nest.
int ˜V [100];
copy V to ˜V ;
for(i = 0; i < 100; i + +) {
chkd = chkd ⊕ ( ˜V [i] + 1);
chkd = chkd ⊕ (W [i] ∗ 2);
}
(d) Duplicate loop nest obtained
by intra-nest optimization.
int U [100], V [100];
int W [100], X[100];
for(i = 0; i < 100; i + +) {
U [i] = V [i] + 1;
chkp = chkp ⊕ U [i];
V [i] = W [i] ∗ 2;
chkp = chkp ⊕ V [i];
}
(c) Primary loop nest.
copy V to X;
for(i = 0; i < 100; i + +) {
chkd = chkd ⊕ (X[i] + 1);
chkd = chkd ⊕ (W [i] ∗ 2);
}
(e) Duplicate loop nest obtained
by inter-nest optimization.
Figure 8. An example application of our inter-nest
memory space optimization.
ure 5 and Figure 7, we can observe that the size of the du-
plicate array ˜U is reduced from 100 to 1. In other words, all
the duplicate elements of array U share the same memory
location for storing their values. It is important to note that
doing so does not compromise the error detection capabil-
ity in any way since there is no overlap among the periods
during which these duplicate array elements’ values need to
be stored. More speciﬁcally, each duplicate array element is
assigned a value at the beginning of an iteration and is used
only in that iteration. Based on this observation, we can use
a single memory location to store all the duplicate elements
of array U .
Figure 8 presents an example application of our inter-
nest memory space optimization. If we use only intra-nest
optimization, we have to create a full size duplication ar-
ray for V , as shown in Figure 8(c). Although we cannot
reduce the memory space consumption of ˜V by analyzing
the current loop nest, we can observe that all the array el-
ements of array X are modiﬁed in the loop nest given in
Figure 8(b). Therefore, we can use the memory locations
in X in the ﬁrst loop nest without affecting the execution of
Figure 8(b). Figure 8(e) gives the duplicate loop nest that
uses array X to store the duplicates for array V .
3.3.2 Mathematical Notation
We assume that the loop bounds and the array indices (sub-
script functions) are afﬁne functions of the enclosing loop
indices and loop-invariant constants. We handle non-afﬁne
array accesses conservatively (i.e., full duplication). In a
given loop nest with t loops, each iteration can be repre-
i2 ··· it]T .
sented as a t-entry vector, that is, (cid:1)I = [i1
Here, i1, i2, ···, it are loop indices from the outermost
loop position to the innermost loop position. The itera-
tion space I consists of all the iterations of a loop nest.
We use ≺, (cid:3), (cid:4), and (cid:5) to represent the lexicographi-
cal order between different vectors. Similarly, each ele-
ment of a d-dimensional array A can be represented using
Figure 6. Lifetime-based sharing of memory loca-
tions in storing duplicates.
int U [100],V [100],W [100];
for(i = 0; i < 100; i + +) {
U [i] = V [i] + 1;
chkp = chkp ⊕ U [i];
W [i] = U [i] ∗ V [i];
chkp = chkp ⊕ W [i];
}
(a) Primary loop nest.
int ˜U [1];
for(i = 0; i < 100; i + +) {
˜U [0] = V [i] + 1;
chkd = chkd ⊕ ˜U [0];
chkd = chkd ⊕ ˜U [0] ∗ V [i];
}
(b) Duplicate loop nest.
Figure 7. An example application of our intra-nest
memory space optimization.
tion. Therefore, in the NRWD scheme, we have to create
duplicates for arrays that are both read and written. Our
third scheme, called the compiler-directed duplicate reuse
(CDDR), tries to reduce the memory space consumption
due to these duplicates. The idea is to reuse the memory
space from the dead duplicates of the same array or from
the dead array elements of other arrays. What we mean by
“dead elements” in this context are the ones that completed
their last uses in the execution. The CDDR scheme imple-
ments two types of optimizations. Intra-nest memory space
optimization reuses memory space for storing duplicates by
analyzing each loop nest individually. In contrast, inter-nest
memory space optimization reuses memory space for stor-
ing duplicates across the loop nest boundaries.
3.3.1 Examples
Figure 6 illustrates an example scenario of sharing memory
locations for storing duplicates. A data element is live from
the point at which it is stored a value until it is last used
in the loop nest, and we need to provide a duplicate for a
live data element. Two data elements can share the same
memory location for storing their duplicates if their live pe-
riods do not overlap with each other. For example, there is
no overlap between the life periods of a and c in Figure 6,
and they can share the same memory location to store the
duplicates. Similarly, we need only one memory location
to store the duplicates for both b and d . Consequently, at
any given time, at most two data elements are live and need
duplicates. Therefore, in Figure 6, the minimum number of
memory locations required for storing duplicates for all four
data elements is 2. We discuss the details of our approach
for determining duplicate array size in Section 3.3.3.
Figure 7 gives an example that demonstrates how intra-
nest memory space optimization can improve memory
space efﬁciency over the NRWD scheme. Comparing Fig-
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:27:26 UTC from IEEE Xplore.  Restrictions apply. 
for(i = 0; i < 100; i + +)
for(i = 0; i < 100; i + +) {
A[i][j] = 2 ∗ A[i][j − 1] + 1;
Figure 9. An example nested loop.
a2 ··· ad]T . We use Rk((cid:1)I),
a d-entry vector (cid:1)a = [a1
where 0 ≤ k < n, to represent n references to array
A. Rk((cid:1)I) can also be deﬁned in a matrix/vector form as
Rk((cid:1)I) = Ck · (cid:1)I + (cid:1)ok, where Ck is a d × t matrix and is
called the access matrix of Rk, and (cid:1)ok is a t-entry vector
and is called the offset vector of Rk [31]. As an example,
for the two references shown in Figure 9, we have:
(cid:1)
(cid:1)
1
0
1
0
0
1
0
1
R0((cid:1)I) =
R1((cid:1)I) =
(cid:2)(cid:1)
(cid:2)(cid:1)
(cid:2)
(cid:2)
+
+
i
j
i
j
(cid:1)
(cid:1)
(cid:2)
0
0
0−1
and
(cid:2)
.
A data dependence exists between two array references
if they can access the same array element and at least one
of them is a write reference. Mathematically, there exists a
data dependence from array reference R1 to array reference
R2 if ∃ (cid:1)I1 ∈ I, (cid:1)I2 ∈ I : (cid:1)I1 (cid:3) (cid:1)I2 and R1( (cid:1)I1) = R2( (cid:1)I2).
In this case, the vector (cid:1)I2 − (cid:1)I1 is deﬁned as the depen-
dence distance vector between R1 and R2. For example,
in Figure 9, data dependence exists from R0 to R1 since
R0((cid:1)I) = R1((cid:1)I + [0
1]T ). The dependence distance from
R0 to R1 is [0
1]T . There are different types of data de-
If R1 is a write and R2 is a read, it is
pendences [31].
In this scenario, R2 reads the value
a ﬂow-dependence.
stored by R1. If both of them are write references, it is an
output-dependence, in which case R2 overwrites the value
stored by R1. If R1 is a read and R2 is a write, it is an
anti-dependence. Note that data dependences determine
an execution order for the involved instructions/iterations.
Any code transformation that violates a data dependence
may result in incorrect execution. Data dependence analy-
sis has been studied extensively in the past compiler-related
research [1, 31], and a number of compiler tools are already
available from both academia and industry.
3.3.3 Intra-Nest Memory Space Optimization
The need for a duplicate starts from the time at which an
array element is assigned a value and ends when it is last
used in the execution. If the periods that two different ar-
ray elements require duplicates do not overlap, they can use
the same memory location for storing their duplicates. Our
intra-nest memory space optimization analyzes the lifetimes
of the array elements and determines the minimum number
of memory locations required for storing their duplicates.
In this approach, each array is analyzed in isolation. For
an array that is both read and written in the loop nest, the
following two criteria, referred as memory reuse constraints
in the rest of the paper, need to be satisﬁed for it to be con-
sidered by our intra-nest memory space optimization:
• Access matrices are the same for all the references of
this array, and all the offset vectors are constants. In
Figure 10. Sharing memory locations within a loop
nest for storing duplicates.
practice, most image/video applications and all stencil
codes satisﬁes this constraint.
• There is no anti-dependence among its references.
We can infer from the above conditions that, for each ref-
erence pair of a given array that satisﬁes these criteria, they
either are independent, or the dependence distance between
them is a constant vector.
Let us now assume that the references in the loop nest to
array A are R0, R1, ..., Rn. We ﬁrst consider the scenario
where only one of the references is a write reference. Fig-
ure 10 illustrates an example scenario for sharing memory
locations within a loop nest for storing duplicates. We as-
sume that R0 is the write reference, and Rks (1 ≤ k ≤ n)
are the read references. For each read reference Rk, it ei-
ther has no dependence with R0, or depends on R0 with a
constant dependence distance vector. In the no-dependence
case, it is easy to see that the array elements read by Rk are
read-only during the execution of the loop nest, thus they
do not need any duplicates, as explained earlier. Without
loss of generality, we assume that all read references have
a dependence with the write reference. More speciﬁcally,
suppose that the dependence distance from R0 to Rk is (cid:1)Dk,
where (cid:1)Dk is a constant vector. In this case, we can write
R0((cid:1)I) = Rk((cid:1)I + (cid:1)Dk). Since there is no anti-dependence,
all data dependences should be ﬂow-dependence, that is,
∀1 ≤ k ≤ n : (cid:1)Dk (cid:5) (cid:1)0. Let
(cid:1)Dmax denote the largest
(cid:1)Dk. For
dependence distance; that is,
an array element written at iteration (cid:1)I, it will be last read
(used) at iteration (cid:1)I + (cid:1)Dmax. Such a scenario is depicted
in Figure 10. Our objective is to determine a vector (cid:1)L such
that we can use the same memory location for storing the
duplicates for R0((cid:1)I) and R0((cid:1)I + (cid:1)L). In other words, the
period during which R0((cid:1)I) requires a duplicate does not
overlap with the period during which R0((cid:1)I + (cid:1)L) requires a
duplicate. We determine (cid:1)L as follows:
(cid:1)Dmax = max1≤k≤n
(cid:1)L = min{(cid:1)V | (cid:1)V (cid:4) (cid:1)Dmax and (cid:1)V is an axis vector}.
Here, an axis vector refers to a vector with a single non-
zero entry and all its remaining entries are 0. For example,
both [0 3 0 ··· 0 0]T and [1 0 0 ··· 0 0]T are example axis
vectors. The main reason that we choose an axis vector is