As expected, you can’t open the file while FsTool has exclusive access to it. When you try to 
delete the file, the system marks it for deletion, but it’s not able to remove it from the file system 
namespace. If you try to delete the file again with File Explorer, you can witness the same behav-
ior. When you press Enter in the first command prompt window and you exit the FsTool applica-
tion, the file is actually deleted by the NTFS file system driver. 
The next step is to use a POSIX deletion for getting rid of the file. You can do this by specifying 
the /pdel command-line argument to the FsTool application. In the first command prompt win-
dow, restart FsTool with the /touch command-line argument (the original file has been already 
marked for deletion, and you can’t delete it again). Before pressing Enter, switch to the second 
window and execute the following command:
D:\>FsTool /pdel Test.txt
NTFS / ReFS Tool v0.1
Copyright (C) 2018 Andrea Allievi (AaLl86)
Deleting "Test.txt" file (Posix semantics)... Success. 
Press any key to exit...
CHAPTER 11
Caching and file systems
643
D:\>dir Test.txt 
 Volume in drive D is DATA 
 Volume Serial Number is 62C1-9EB3 
 Directory of D:\ 
File Not Found
In this case the Test.txt file has been completely removed from the file system’s namespace 
but is still valid. If you press Enter in the first command prompt window, FsTool is still able to write 
data to the file. This is because the file has been internally moved into the \Extend\Deleted 
hidden system directory.
Defragmentation
Even though NTFS makes efforts to keep files contiguous when allocating blocks to extend a file, a vol-
ume’s files can still become fragmented over time, especially if the file is extended multiple times or when 
there is limited free space. A file is fragmented if its data occupies discontiguous clusters. For example, 
Figure 11-26 shows a fragmented file consisting of five fragments. However, like most file systems (includ-
ing versions of FAT on Windows), NTFS makes no special efforts to keep files contiguous (this is handled 
by the built-in defragmenter), other than to reserve a region of disk space known as the master file table
(MFT) zone for the MFT. (NTFS lets other files allocate from the MFT zone when volume free space runs 
low.) Keeping an area free for the MFT can help it stay contiguous, but it, too, can become fragmented. 
(See the section “Master file table” later in this chapter for more information on MFTs.)
Fragmented file
Contiguous file
FIGURE 11-26 Fragmented and contiguous files.
To facilitate the development of third-party disk defragmentation tools, Windows includes a de-
fragmentation API that such tools can use to move file data so that files occupy contiguous clusters. 
The API consists of file system controls that let applications obtain a map of a volume’s free and in-use 
clusters (FSCTL_GET_VOLUME_BITMAP), obtain a map of a file’s cluster usage (FSCTL_GET_RETRIEVAL 
_POINTERS), and move a file (FSCTL_MOVE_FILE).
D:\>dir Test.txt
 Volume in drive D is DATA
 Volume Serial Number is 62C1-9EB3
 Directory of D:\
File Not Found
In this case the Test.txt file has been completely removed from the file system’s namespace 
but is still valid. If you press Enter in the first command prompt window, FsTool is still able to write 
data to the file. This is because the file has been internally moved into the \Extend\Deleted 
hidden system directory.
644
CHAPTER 11
Caching and file systems
Windows includes a built-in defragmentation tool that is accessible by using the Optimize Drives 
utility (%SystemRoot%\System32\Dfrgui.exe), shown in Figure 11-27, as well as a command-line inter-
face, %SystemRoot%\System32\Defrag.exe, that you can run interactively or schedule, but that does 
not produce detailed reports or offer control—such as excluding files or directories—over the defrag-
mentation process. 
FIGURE 11-27 The Optimize Drives tool.
The only limitation imposed by the defragmentation implementation in NTFS is that paging 
files and NTFS log files can’t be defragmented. The Optimize Drives tool is the evolution of the Disk 
Defragmenter, which was available in Windows 7. The tool has been updated to support tiered vol-
umes, SMR disks, and SSD disks. The optimization engine is implemented in the Optimize Drive service 
(Defragsvc.dll), which exposes the IDefragEngine COM interface used by both the graphical tool and 
the command-line interface.
For SSD disks, the tool also implements the retrim operation. To understand the retrim operation, 
a quick introduction of the architecture of a solid-state drive is needed. SSD disks store data in flash 
memory cells that are grouped into pages of 4 to 16 KB, grouped together into blocks of typically 128 
to 512 pages. Flash memory cells can only be directly written to when they’re empty. If they contain 
data, the contents must be erased before a write operation. An SSD write operation can be done on 
a single page but, due to hardware limitations, erase commands always affect entire blocks; conse-
quently, writing data to empty pages on an SSD is very fast but slows down considerably once previ-
ously written pages need to be overwritten. (In this case, first the content of the entire block is stored in 
CHAPTER 11
Caching and file systems
645
cache, and then the entire block is erased from the SSD. The overwritten page is written to the cached 
block, and finally the entire updated block is written to the flash medium.) To overcome this problem, 
the NTFS File System Driver tries to send a TRIM command to the SSD controller every time it deletes 
the disk’s clusters (which could partially or entirely belong to a file). In response to the TRIM command, 
the SSD, if possible, starts to asynchronously erase entire blocks. Noteworthy is that the SSD controller 
can’t do anything in case the deleted area corresponds only to some pages of the block. 
The retrim operation analyzes the SSD disk and starts to send a TRIM command to every cluster in 
the free space (in chunks of 1-MB size). There are different motivations behind this:
I 
TRIM commands are not always emitted. (The file system is not very strict on trims.)
I 
The NTFS File System emits TRIM commands on pages, but not on SSD blocks. The Disk
Optimizer, with the retrim operation, searches fragmented blocks. For those blocks, it first
moves valid data back to some temporary blocks, defragmenting the original ones and insert-
ing even pages that belongs to other fragmented blocks; finally, it emits TRIM commands on
the original cleaned blocks.
Note The way in which the Disk Optimizer emits TRIM commands on free space is some-
what tricky: Disk Optimizer allocates an empty sparse file and searches for a chunk (the size 
of which varies from 128 KB to 1 GB) of free space. It then calls the file system through the 
FSCTL_MOVE_FILE control code and moves data from the sparse file (which has a size of 1 
GB but does not actually contain any valid data) into the empty space. The underlying file 
system actually erases the content of the one or more SSD blocks (sparse files with no valid 
data yield back chunks of zeroed data when read). This is the implementation of the TRIM 
command that the SSD firmware does.
For Tiered and SMR disks, the Optimize Drives tool supports two supplementary operations: Slabify 
(also known as Slab Consolidation) and Tier Optimization. Big files stored on tiered volumes can be 
composed of different Extents residing in different tiers. The Slab consolidation operation not only 
defragments the extent table (a phase called Consolidation) of a file, but it also moves the file content 
in congruent slabs (a slab is a unit of allocation of a thinly provisioned disk; see the “Storage Spaces” 
section later in this chapter for more information). The final goal of Slab Consolidation is to allow files 
to use a smaller number of slabs. Tier Optimization moves frequently accessed files (including files that 
have been explicitly pinned) from the capacity tier to the performance tier and, vice versa, moves less 
frequently accessed files from the performance tier to the capacity tier. To do so, the optimization en-
gine consults the tiering engine, which provides file extents that should be moved to the capacity tier 
and those that should be moved to the performance tier, based on the Heat map for every file accessed 
by the user. 
Note Tiered disks and the tiering engine are covered in detail in the following sections of 
the current chapter.
646
CHAPTER 11
Caching and file systems
EXPERIMENT: Retrim an SSD volume
You can execute a Retrim on a fast SSD or NVMe volume by using the defrag.exe /L command, 
as in the following example:
D:\>defrag /L c: 
Microsoft Drive Optimizer 
Copyright (c) Microsoft Corp. 
Invoking retrim on (C:)... 
The operation completed successfully. 
Post Defragmentation Report: 
Volume Information: 
Volume size
= 475.87 GB 
Free space
= 343.80 GB 
Retrim: 
Total space trimmed
= 341.05 GB
In the example, the volume size was 475.87 GB, with 343.80 GB of free space. Only 341 GB 
have been erased and trimmed. Obviously, if you execute the command on volumes backed by 
a classical HDD, you will get back an error. (The operation requested is not supported by the 
hardware backing the volume.)
Dynamic partitioning
The NTFS driver allows users to dynamically resize any partition, including the system partition, either 
shrinking or expanding it (if enough space is available). Expanding a partition is easy if enough space 
exists on the disk and the expansion is performed through the FSCTL_EXPAND_VOLUME file system 
control code. Shrinking a partition is a more complicated process because it requires moving any file 
system data that is currently in the area to be thrown away to the region that will still remain after the 
shrinking process (a mechanism similar to defragmentation). Shrinking is implemented by two compo-
nents: the shrinking engine and the file system driver.
The shrinking engine is implemented in user mode. It communicates with NTFS to determine the 
maximum number of reclaimable bytes—that is, how much data can be moved from the region that 
will be resized into the region that will remain. The shrinking engine uses the standard defragmenta-
tion mechanism shown earlier, which doesn’t support relocating page file fragments that are in use or 
any other files that have been marked as unmovable with the FSCTL_MARK_HANDLE file system con-
trol code (like the hibernation file). The master file table backup (MftMirr), the NTFS metadata transac-
tion log (LogFile), and the volume label file (Volume) cannot be moved, which limits the minimum 
size of the shrunk volume and causes wasted space.
EXPERIMENT: Retrim an SSD volume
You can execute a Retrim on a fast SSD or NVMe volume by using the defrag.exe /L command, 
as in the following example:
D:\>defrag /L c:
Microsoft Drive Optimizer
Copyright (c) Microsoft Corp.
Invoking retrim on (C:)...
The operation completed successfully.
Post Defragmentation Report:
Volume Information:
Volume size
= 475.87 GB
Free space
= 343.80 GB
Retrim:
Total space trimmed
= 341.05 GB
In the example, the volume size was 475.87 GB, with 343.80 GB of free space. Only 341 GB 
have been erased and trimmed. Obviously, if you execute the command on volumes backed by 
a classical HDD, you will get back an error. (The operation requested is not supported by the 
hardware backing the volume.)
CHAPTER 11
Caching and file systems
647
The file system driver shrinking code is responsible for ensuring that the volume remains in a consis-
tent state throughout the shrinking process. To do so, it exposes an interface that uses three requests 
that describe the current operation, which are sent through the FSCTL_SHRINK_VOLUME control code:
I 
The ShrinkPrepare request, which must be issued before any other operation. This request
takes the desired size of the new volume in sectors and is used so that the file system can block
further allocations outside the new volume boundary. The ShrinkPrepare request doesn’t verify
whether the volume can actually be shrunk by the specified amount, but it does ensure that the
amount is numerically valid and that there aren’t any other shrinking operations ongoing. Note
that after a prepare operation, the file handle to the volume becomes associated with the shrink
request. If the file handle is closed, the operation is assumed to be aborted.
I 
The ShrinkCommit request, which the shrinking engine issues after a ShrinkPrepare request. In
this state, the file system attempts the removal of the requested number of clusters in the most
recent prepare request. (If multiple prepare requests have been sent with different sizes, the last
one is the determining one.) The ShrinkCommit request assumes that the shrinking engine has
completed and will fail if any allocated blocks remain in the area to be shrunk.
I 
The ShrinkAbort request, which can be issued by the shrinking engine or caused by events such
as the closure of the file handle to the volume. This request undoes the ShrinkCommit operation
by returning the partition to its original size and allows new allocations outside the shrunk region
to occur again. However, defragmentation changes made by the shrinking engine remain.
If a system is rebooted during a shrinking operation, NTFS restores the file system to a consistent 
state via its metadata recovery mechanism, explained later in the chapter. Because the actual shrink 
operation isn’t executed until all other operations have been completed, the volume retains its original 
size and only defragmentation operations that had already been flushed out to disk persist.
Finally, shrinking a volume has several effects on the volume shadow copy mechanism. Recall that the 
copy-on-write mechanism allows VSS to simply retain parts of the file that were actually modified while 
still linking to the original file data. For deleted files, this file data will not be associated with visible files 
but appears as free space instead—free space that will likely be located in the area that is about to be 
shrunk. The shrinking engine therefore communicates with VSS to engage it in the shrinking process. In 
summary, the VSS mechanism’s job is to copy deleted file data into its differencing area and to increase 
the differencing area as required to accommodate additional data. This detail is important because it 
poses another constraint on the size to which even volumes with ample free space can shrink.
NTFS support for tiered volumes
Tiered volumes are composed of different types of storage devices and underlying media. Tiered vol-
umes are usually created on the top of a single physical or virtual disk. Storage Spaces provides virtual 
disks that are composed of multiple physical disks, which can be of different types (and have different 
performance): fast NVMe disks, SSD, and Rotating Hard-Disk. A virtual disk of this type is called a tiered 
disk. (Storage Spaces uses the name Storage Tiers.) On the other hand, tiered volumes could be created 
on the top of physical SMR disks, which have a conventional “random-access” fast zone and a “strictly 
sequential” capacity area. All tiered volumes have the common characteristic that they are composed 
648
CHAPTER 11
Caching and file systems
by a “performance” tier, which supports fast random I/O, and a “capacity” tier, which may or may not 
support random I/O, is slower, and has a large capacity.
Note SMR disks, tiered volumes, and Storage Spaces will be discussed in more detail later in 
this chapter.
The NTFS File System driver supports tiered volumes in multiple ways:
I 
The volume is split in two zones, which correspond to the tiered disk areas (capacity and
performance).
I 
The new $DSC attribute (of type LOGGED_UTILITY_STREAM) specifies which tier the file
should be stored in. NTFS exposes a new “pinning” interface, which allows a file to be locked in a
particular tier (from here derives the term “pinning”) and prevents the file from being moved by
the tiering engine.
I 
The Storage Tiers Management service has a central role in supporting tiered volumes. The
NTFS file system driver records ETW “heat” events every time a file stream is read or written.
The tiering engine consumes these events, accumulates them (in 1-MB chunks), and periodically
records them in a JET database (once every hour). Every four hours, the tiering engine processes
the Heat database and through a complex “heat aging” algorithm decides which file is consid-
ered recent (hot) and which is considered old (cold). The tiering engine moves the files between
the performance and the capacity tiers based on the calculated Heat data.
Furthermore, the NTFS allocator has been modified to allocate file clusters based on the tier area 
that has been specified in the $DSC attribute. The NTFS Allocator uses a specific algorithm to decide 
from which tier to allocate the volume’s clusters. The algorithm operates by performing checks in the 
following order:
1.
If the file is the Volume USN Journal, always allocate from the Capacity tier.
2.
MFT entries (File Records) and system metadata files are always allocated from the
Performance tier.
3.
If the file has been previously explicitly “pinned” (meaning that the file has the $DSC attribute),
allocate from the specified storage tier.
4.
If the system runs a client edition of Windows, always prefer the Performance tier; otherwise,
allocate from the Capacity tier.
5.
If there is no space in the Performance tier, allocate from the Capacity tier.
An application can specify the desired storage tier for a file by using the NtSetInformationFile API 
with the FileDesiredStorageClassInformation information class. This operation is called file pinning, and, 
if executed on a handle of a new created file, the central allocator will allocate the new file content in 
the specified tier. Otherwise, if the file already exists and is located on the wrong tier, the tiering engine 
will move the file to the desired tier the next time it runs. (This operation is called Tier optimization and 
can be initiated by the Tiering Engine scheduled task or the SchedulerDefrag task.)
CHAPTER 11
Caching and file systems
649
Note It’s important to note here that the support for tiered volumes in NTFS, described here, 
is completely different from the support provided by the ReFS file system driver.
EXPERIMENT: Witnessing file pinning in tiered volumes
As we have described in the previous section, the NTFS allocator uses a specific algorithm to 
decide which tier to allocate from. In this experiment, you copy a big file into a tiered volume 
and understand what the implications of the File Pinning operation are. After the copy finishes, 
open an administrative PowerShell window by right-clicking on the Start menu icon and select-
ing Windows PowerShell (Admin) and use the Get-FileStorageTier command to get the tier 
information for the file:
PS E:\> Get-FileStorageTier -FilePath 'E:\Big_Image.iso' | FL FileSize, 
DesiredStorageTierClass, FileSizeOnPerformanceTierClass, FileSizeOnCapacityTierClass, 
PlacementStatus, State 
FileSize
: 4556566528 
DesiredStorageTierClass
: Unknown 
FileSizeOnPerformanceTierClass : 0 
FileSizeOnCapacityTierClass    : 4556566528 
PlacementStatus
: Unknown 
State
: Unknown
The example shows that the Big_Image.iso file has been allocated from the Capacity Tier. (The 