104.56 
104.97 
105.38 
105.61 
567.02 
6
5
4
3
2
1
]
2
m
c
6
-
0
1
[
n
o
i
t
c
e
S
s
s
o
r
C
σ [10-6cm2] 
2.38±5% 
5.30±5% 
4.94±5% 
4.04±5% 
3.57±5% 
3.44±5% 
3.32±5% 
2.67±5% 
2.43±5% 
0.79±5% 
FIT [104] 
3.09 
6.89 
6.42 
5.25 
4.64 
4.47 
4.31 
3.48 
3.16 
1.02 
]
c
e
x
e
6
0
1
[
F
B
E
M
SérieFFT
20
18
16
14
12
10
8
6
4
2
0
FFT-G1 
FFT-G14 
FFT-G28 
FFT-G70 
FFT-G252 
FFT-P 
FFT-B256 
FFT-B64 
FFT-B32 
FFT-B1 
3.23 
1.47 
1.56 
1.90 
2.16 
2.24 
2.32 
2.88 
3.17 
9.77 
9.20 
8.13 
8.75 
10.92 
12.44 
12.83 
13.35 
16.46 
18.00 
10.32 
FFT
Série1
0
G1 G14
G28 G70 G252
Figure 12: Cross sections of FFT when implemented with different 
DOP and different threads distributions. 
B256 B64
B32
B1
P
the  FT  kernel  of  the  NAS  Parallel  Benchmarks  [28] 
implemented in C and ported to the GPU architecture using 
CUDA.  Each  64-points  1D  FFT  kernel  is  composed  of  6 
sequential iterations (log264=6) of a variant of the Stockham 
FFT  algorithm  [29].  For  all  iterations  512x512  parallel 
threads are instantiated, grouped in 512 blocks of 512 threads 
each.  A  thread  is  in  charge  of  evaluating  the  intermediate 
FFT values on the assigned complex vector of size 64. 
The  DOP  of  FFT  was  reduced  increasing  concurrently 
the workload of each thread. In the fully parallelized version, 
named FFT_P, a thread is in charge of uploading all the 64 
points of the assigned FFT. When the DOP is reduced, the 
number  of  FFTs  each  thread  is  in  charge  of  executing  is 
increased.  Table  XIV  lists  the  tested  distributions,  named 
after the only distribution parameter that is reduced from the 
fully  parallelized  version.  Please  note  that  for  the  FFT  we 
choose  to  reduce  the  grid  size  in  such  a  way  to  become 
multiple of the available number of SM. As FFT_P requires 
512 blocks, and P_G252 has 252 blocks (14x18), in P_G252 
four threads will have to update a single FFT, as in FFT_P, 
while all the others will have to update two FFTs. When the 
grid size is reduced, some threads will need to update fewer 
FFTs than others. This decision was made to better analyze 
the  blocks  scheduling  sensitivity,  and  having  few  threads 
updating  just  a  FFT  less  than  other  will  not  affect  results. 
Threads  with  lower  workload  will  not  affect  the  GPU 
scheduling, as the next block will be assigned only when all 
the threads in the block will complete computation. 
For  the  FFT  two  limit  cases  were  also  tested,  i.e. 
FFT_G1 in which just one block of 512 threads is instantiate 
and FFT_B1 in which each of the 512 blocks are composed 
of  just  a  single  thread.  These  unpractical  and  unrealistic 
cases  were  chosen  to  evaluate  the  GPUs  behavior  when 
G1 G14
G28 G70 G252
Figure  13:  Mean  Executions  Between  Failures  of  FFT  when 
implemented with different DOP and different threads distributions. 
A higher reliability is achieved with B32 DOP distribution. 
P
B256 B64
B32
B1
scheduling strains are reduced to minimum. 
Tab. XIV reports the cross section and FIT for the tested 
configurations of FFT. As shown in Fig. 12, the sensitivity 
trend  follows  the  Matrix  Multiplication  one.  Reducing  the 
number of blocks increases the cross section, while reducing 
the number of threads per block decreases the cross section. 
A  drop  in  the  cross  section  is  identified  for  FFT_B1  and 
FFT_G1,  due  to  the  much  lower  GPU  area  used  for 
computation.  In  FFT_B1  31  CUDA  core  over  the  32 
available are unused and in FFT_G1 13 SMs are in idle state. 
The  main  difference  with  Matrix  Multiplication  is  that, 
for  FFT,  the  execution  time  is  not  much  affected  by  the 
reduced  DOP.  The  difference  in  the  execution  times  of  all 
the tested DOP a part from the extreme cases is of less than 
4%.  This  is  because  most  of  the  time  in  FFT  is  spent  for 
moving data to and from the DDR. Such memory operations 
are not affected by the DOP, as reducing the DOP does not 
modify  the  FFT  workload.  When  evaluating  the  MEBF  of 
FFT,  this  actually  means  that  little  benefit  will  come  from 
the reduced execution time. In fact, Fig. 13 shows how the 
MEBF  of  FFT  follows  the  cross  section  trend  having 
configuration  with  lower  cross  section  being  more  reliable. 
Extreme cases, on the contrary, have an increased execution 
time that comes from an inefficient resources utilization that 
makes their MEBF lower. 
VI.  CONCLUSIONS AND FUTURE WORK 
The  high  computational  capabilities  offered  by  modern 
GPUs come at a cost. A high scheduling strain is required to 
dispatch  the  instantiated  blocks  and  threads  among  the 
available SMs and CUDA cores, and such a strain strongly 
depends  on  the  chosen  block  and  grid  sizes.  From  a 
reliability  point  of  view,  block  scheduling  is  particularly 
465
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:48:15 UTC from IEEE Xplore.  Restrictions apply. 
sensitive  to  radiation.  Increasing  the  number  of  blocks  has 
the  countermeasure  of  drastically  reducing  the  workload 
correctly computed by the GPU and should be avoided. 
Reducing  the  Degree  Of  Parallelism  of  a  parallel  code 
reduces  the  scheduling  strain  but  also  affects  the  GPU 
parallel management. The reduced cross section that comes 
from a lower number of parallel processes to dispatch may 
be compensated by less efficient resources utilization. 
The  proposed  experimental  analysis  introduces  new 
metrics  for  identifying  the  best  configuration  in  term  of 
reliability,  taking  radiation  sensitivity,  execution  time,  and 
workload computed into account. 
In  the  future,  we  intend  to  extend  the  study  finding  a 
correlation  between  scheduler  strain  and  parallelism 
management variation to predict the most reliable Degree Of 
Parallelism and threads distribution for a given code. Such a 
study  is  of  extreme  interest,  as  it  optimizes  the  parallel 
resources  utilization  in  a  way  that  reduces  the  radiation-
induced output error rate. 
ACKNOWLEDGMENTS 
The authors would like to thank Heather Quinn, Thomas 
Fairbanks,  and  Stephen  Wender,  Los  Alamos  National 
Laboratory  as  well  as  Christopher  Frost,  Rutherford 
their  precious  help  with 
Appleton  Laboratory, 
experimental  setup,  neutron  beam 
flux 
measurements. 
tuning,  and 
for 
REFERENCES 
[1] 
J.D. Owens, M. Houston, D. Luebke, S. Green, J.E. Stone, and J.C. 
Phillips,  “GPU  Computing”  Proceedings  of  the  IEEE,  vol.96,  no.5, 
pp.879-899, May 2008 
[3] 
[2]  E.  Lindholm,  J.  Nickolls,  S.  Oberman,  and  J.  Montrym,  “NVIDIA 
Tesla:  A  Unified  Graphics  and  Computing  Architecture”  IEEE 
MICRO, vol. 28, n. 2, March/April 2008, pp. 39-55 
J.  Kruger  and  R.  Westermann,  “Linear  Algebra  operators  for  GPU 
implementation of numerical algorithms”, ACM Trans. Graph. n. 22, 
vol. 3, 2003, pp. 908-916 
J. Liepe, C. Barnes, E. Cule, K. Erguler, P. Kirk, T. Toni, and M. P. 
H.  Stumpf,  “ABC-SysBio-approximate  Bayesian  computation  in 
Python  with  GPU  support”  –  Bioinformatics,  vol.  26,  n.  14,  July 
2012, pp. 1797-1799 
[5]  Euro  NCAP  Rating  Review,  Report  from  the  Ratings  Group,  June 
[4] 
2012. Available: http://www.euroncap.com 
[6]  O. Bender, “ARAMIS – Concepts to validate the safe application of 
multicore  architectures  in  the  avionics  domain,  ”  HiPEAC  2014, 
Available 
[Online]:  http://www.across-project.eu/workshop2013/ 
121108_ARAMIS_Introduction_HiPEAC_WS_V3.pdf 
[7]  N. Seifert, Zhu Xiaowei, and L. W. Massengill, “Impact of Scaling on 
Soft-Error Rates in Commercial Microprocessors”, IEEE Trans. Nucl. 
Sci, vol. 46, no. 6, pp. 3100, 2002, 3106 
[8]  H.T. Nguyen, Y. Yagil, N. Seifert, and M. Reitsma, “Chip-level Soft 
Error  Estimation  Method”,  IEEE  Trans.  Device  and  Materials 
Reliability, vol. 5, no. 3, 2005, pp. 356, 381 
[9]  M. D. Lerner, “Algorithm based fault tolerance in massively parallel 
systems”,  Department  of  Computer  Science,  Columbia  University, 
Tech. Rep., 1988 
[10]  S.  Mitra,  “System-Level  Single-Event  Effects”,  IEEE  Nuclear  and 
Space Radiation Effects Conference, NSREC 2012 Short Course 
[11]  L. Bautista Gomez, F. Cappello, L. Carro, N. DeBardeleben, B. Fang, 
S.  Gurumurthi,  K.  Pattabiraman,  P.  Rech,  and  M.  Sonza  Reorda, 
“GPGPUs: How  to Combine High  Computational Power with High 
Reliability”,  In  Proceedings  of  the  IEEE  Design,  Automation  and 
Test in Europe (DATE), 2014, Dresden, Germany 
[12]  G.  Shi,  J.  Enos,  M.  Showerman,  and  V.  Kindratenko,  "On  Testing 
GPU  Memory  for  Hard  and  Soft  Errors,"  In  Proceedings  of  the 
Symposium  on  Application  Accelerators 
in  High-Performance 
Computing (SAAHPC), 2009 
[13]  N.J.  Wang,  J.  Quek,  T.M.  Rafacz,  S.J.  Patel,  "Characterizing  the 
effects of transient faults on a high-performance processor pipeline," 
In Proceedings of the IEEE International Conference on Dependable 
Systems and Networks (DSN), pp. 61-70, 2004 
[14]  I. S. Haque and V. S. Pande,  "Hard Data on Soft Errors:  A  Large- 
Scale  Assessment  of  Real-World  Error  Rates  in  GPGPU,"  In 
Proceedings of the IEEE/ACM International Conference on Cluster, 
Cloud and Grid Computing, pp. 691-696, 2010 
[15]  J.W.  Sheaffer,  D.P.  Luebke,  and  K.  Skadron,  "A  Hardware 
Redundancy  and  Recovery  Mechanism  for  Reliable  Scientific 
Computation  on  Graphics  Processors,"  In  Proceedings  of  the  ACM 
SIGGRAPH  Symposium  on  Graphics  Hardware  (GH),  pp.  55-64, 
2007 
[16]  B. Fang, K. Pattabiraman, and M. Ripeanu, and S. Gurumurthi “GPU-
Qin: A Methodology for Evaluating the Error Resilience of GPGPU 
Applications”, In Proceedings of the IEEE International Symposium 
on Performance Analysis of Systems and Software (ISPASS), 2014 
[17]  G.  Shi,  J.  Enos,  M.  Showerman,  and  V.  Kindratenko,  "On  Testing 
GPU  Memory  for  Hard  and  Soft  Errors,"  In  Proceedings  of  the 
Symposium  on  Application  Accelerators 
in  High-Performance 
Computing (SAAHPC), 2009 
[18]  N.J.  Wang,  J.  Quek,  T.M.  Rafacz,  S.J.  Patel,  "Characterizing  the 
effects of transient faults on a high-performance processor pipeline," 
In Proceedings of the IEEE International Conference on Dependable 
Systems and Networks (DSN), pp. 61-70, 2004 
[19]  I. S. Haque and V. S. Pande,  "Hard Data on Soft Errors:  A  Large- 
Scale  Assessment  of  Real-World  Error  Rates  in  GPGPU,"  In 
Proceedings of the IEEE/ACM International Conference on Cluster, 
Cloud and Grid Computing, pp. 691-696, 2010 
[20]  P.  Rech,  C.  Aguiar,  C.  Frost,  and  L.  Carro,  “An  Efficient  and 
Experimentally Tuned Software-Based Hardening Strategy for Matrix 
Multiplication on GPUs”, IEEE Trans. Nucl. Sci, 2013 
[21]  P. Rech, L. Pilla, F. Silvestri, C. Frost, M. Sonza Reorda, P. Navaux, 
and L. Carro, “Neutron Sensitivity and Hardening Strategies for Fast 
Fourier  Transform  on  GPUs”,  in  proceeding  IEEE  RADECS  2013, 
Oxford, UK  
[22]  D.  B.  Kirk,  W.W.  Hwo,  “Programming  Massively  Parallel 
Processors”, MK Publishers 
[23]  NVIDIA BENCH: Tesla C2050 Performance Benchmarks 
[24]  M.  Violante,  L.  Sterpone,  A.  Manuzzato,  S.  Gerardin,  P.  Rech,  M. 
Bagatin, A. Paccagnella, C. Andreani, G. Gorini, A. Pietropaolo, G. 
Cargarilli,  S.  Pontarelli,  and  C.  Frost,  “A  New  Hardware/Software 
Platform  and  a  New  1/E  Neutron  Source  for  Soft  Error  Studies: 
Testing FPGAs at the ISIS Facility”, IEEE TNS Nucl. Sci., vol. 54, 
No. 4, pp. 1184-1189 
[25]  JEDEC Standard JESD89A, 2006 
[26]  R.  C.  Baumann,  “Radiation-Induced  Soft  Errors  in  Advanced 
Semiconductor Technologies”, IEEE Trans. on Device and Materials 
Reliability, vol. 5, no. 3, Sept. 2005, pp. 305, 316 
[27]  S-H.  Kim,  et  al.,  “A  Low  Power  and  Highly  Reliable  400Mbps 
Mobile  DDR  SDRAM  with  On-Chip  Distributed  ECC”,  in  proc. 
IEEE Asian Solid-State Circuits Conference, 2007, pp. 34-37 
[28]  D.  Bailey,  et  al.,  “The  NAS  Parallel  Benchmarks”,  RNR  Technical 
Report RNR-94-007, March 1994  
[29]  T.  G.  Stockham,  Jr.,  “High-speed  convolution  and  correlation,”  in 
Proceedings  of  the  April  26-28,  1966,  Spring  joint  computer 
conference,  ser.  AFIPS  ’66  (Spring).  New  York,  NY,  USA:  ACM, 
1966, pp. 229– 233 
466
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:48:15 UTC from IEEE Xplore.  Restrictions apply.