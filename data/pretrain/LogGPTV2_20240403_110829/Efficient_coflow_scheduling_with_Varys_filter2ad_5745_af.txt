marginally) ﬂow-level fairness for coﬂows with deadlines.
8 Discussion
Scheduling With Unknown Flow Sizes Knowing or estimating
exact ﬂow sizes is difﬁcult in frameworks that push data to the next
stage as soon as possible [26], and without known ﬂow sizes, pre-
emption becomes impractical. FIFO scheduling can solve this prob-
lem, but it suffers from head-of-line blocking (§7.4). We believe
that coﬂow-level fairness can be a nice compromise between these
two extremes. However, the deﬁnition and associated properties of
fairness at the coﬂow level is an open problem.
Decentralized SEBF+MADD Varys’s centralized design makes
it less useful for small coﬂows (§3); however, small coﬂows
contribute less than 1% of the trafﬁc in data-intensive clusters
(§4). Furthermore, in-network isolation of control plane messages
[21] or faster signaling channels like RDMA [20] can reduce
Varys’s application-layer signaling overheads (§7.2) to support
even smaller coﬂows. We see a decentralized approximation of our
algorithms as the most viable way to make Varys useful for low-
latency coﬂows. This requires new algorithms and possible changes
to network devices, unlike our application-layer design.
Handling Coﬂow Dependencies While most jobs require only a
single coﬂow, dataﬂow pipelines (e.g., Dryad, Spark) can create
multiple coﬂows with dependencies between them [16]. A simple
approach to support coﬂow dependencies would be to order ﬁrst by
ancestry and then breaking ties using SEBF. Some variation of the
Critical-Path Method [28] might perform even better. We leave it as
a topic of future work. Note that dependencies can be passed along
to the scheduler through options in the register() method.
Multi-Wave Coﬂows Large jobs often schedule mappers in mul-
tiple waves [10]. A job can create separate coﬂows for each wave.
Alternatively, if the job uses its wave-width (i.e., the number of
parallel mappers) as numFlows in register(), Varys can handle
each wave separately. Applications can convey information about
wave-induced coﬂows to the scheduler as dependencies.
In-Network Bottlenecks Varys performs well even when the net-
work is not a non-blocking switch (§7). If likely bottleneck lo-
cations are known, e.g., rack-to-core links are typically oversub-
scribed [17], Varys can be extended to allocate rack-to-core band-
width instead of NIC bandwidth. When bottlenecks are unknown,
e.g., due to in-network failures, routing, or load imbalance, Varys
can react based on bandwidth estimations collected by its daemons.
Nonetheless, designing and deploying coﬂow-aware routing proto-
cols and load balancing techniques remain an open challenge.
9 Related Work
Coﬂow Schedulers Varys improves over Orchestra [15] in four
major ways. First, Orchestra primarily optimizes individual coﬂows
and uses FIFO among them; whereas, Varys uses an efﬁcient coﬂow
scheduler to signiﬁcantly outperform FIFO. Second, Varys supports
deadlines and ensures guaranteed coﬂow completion. Third, Varys
uses a rate-based approach instead of manipulating the number of
TCP ﬂows, which breaks if all coﬂows do not share the same end-
points. Finally, Varys supports coﬂows from multiple frameworks
like Mesos [24] handles non-network resources.
Baraat [19] is a FIFO-based decentralized coﬂow scheduler fo-
cusing on small coﬂows. It uses fair sharing to avoid head-of-line
blocking and does not support deadlines. Furthermore, we formu-
late the coﬂow scheduling problem and analyze its characteristics.
Datacenter Trafﬁc Management Hedera [7] manages ﬂows us-
ing a centralized scheduler to increase network throughput, and
MicroTE [12] adapts to trafﬁc variations by leveraging their short-
term predictability. However, both work with ﬂows and are unsuit-
able for optimizing CCTs. Sinbad [17] uses endpoint ﬂexible trans-
fers for load balancing. Once it makes network-aware placement
decisions, Varys can optimize cross-rack write coﬂows.
High Capacity Networks Full bisection bandwidth topologies
[22,32] do not imply contention freedom. In the presence of skewed
data and hotspot distributions [17], managing edge bandwidth is
still necessary. Inter-coﬂow scheduling improves performance and
predictability even in these high capacity networks.
Trafﬁc Reduction Techniques Data locality [18], both disk [9,40]
and memory [10], reduces network usage only during reads. The
amount of network usage due to intermediate data communication
can be reduced by pushing ﬁlters toward the sources [6, 23]. Our
approach is complementary; i.e., it can be applied to whatever data
traverses the network after applying those techniques.
Network Sharing Among Tenants Fair sharing of network re-
sources between multiple tenants has received considerable atten-
tion [11, 33, 35, 39]. Our work is complementary; we focus on op-
timizing performance of concurrent coﬂows within a single admin-
istrative domain, instead of achieving fairness among competing
entities. Moreover, we focus on performance and predictability as
opposed to the more debated notion of fairness.
Concurrent Open Shop Scheduling Inter-coﬂow scheduling has
its roots in the concurrent open shop scheduling problem [34],
which is strongly NP-hard for even two machines. Even in the of-
ﬂine scenario, the best known result is a 2-approximation algorithm
[30], and it is inapproximable within a factor strictly less than 6/5
if P&=NP [30]. Our setting is different as follows. First, machines
are not independent; i.e., links are coupled because each ﬂow in-
volves a source and a destination. Second, jobs are not known a
priori; i.e., coﬂows arrive in an online fashion.
10 Concluding Remarks
The coﬂow abstraction [16] effectively enables application-aware
network scheduling. We have implemented coﬂows in a system
called Varys and introduced the concurrent open shop scheduling
with coupled resources problem. To minimize coﬂow completion
times (CCT), we proposed the SEBF heuristic to schedule coﬂows
and the MADD algorithm to allocate bandwidth to their ﬂows. To-
gether, they decrease the average CCT without starving any coﬂow
and maintain high network utilization. Through EC2 deployments
and trace-driven simulations, we showed that Varys outperforms
per-ﬂow mechanisms by up to 3.16× and non-preemptive coﬂow
schedulers by more than 5×. Furthermore, by applying MADD in
conjunction with admission control, Varys allowed up to 2× more
coﬂows to meet their deadlines in comparison to per-ﬂow schemes.
In conclusion, this paper is only a ﬁrst step in understanding the
intricacies of inter-coﬂow scheduling and opens up a variety of ex-
citing research problems, which include scheduling without know-
ing ﬂow sizes, exploring the notion of coﬂow fairness, decentraliz-
ing the proposed algorithms, and handling coﬂow dependencies.
Acknowledgments
We thank Mohammad Alizadeh, Justine Sherry, Rachit Agarwal, Peter Bailis, Ganesh
Ananthanarayanan, Tathagata Das, Ali Ghodsi, Gautam Kumar, David Zats, Matei
Zaharia, the AMPLab members, our shepherd Nandita Dukkipati, and the anony-
mous reviewers of NSDI’14 and SIGCOMM’14 for useful feedback. This research
is supported in part by NSF CISE Expeditions Award CCF-1139158, LBNL Award
7076018, and DARPA XData Award FA8750-12-2-0331, and gifts from Amazon Web
Services, Google, SAP, The Thomas and Stacey Siebel Foundation, Apple, Inc., Cisco,
Cloudera, EMC, Ericsson, Facebook, GameOnTalis, Guavus, HP, Huawei, Intel, Mi-
crosoft, NetApp, Pivotal, Splunk, Virdata, VMware, WANdisco and Yahoo!.
11 References
[1] Akka. http://akka.io.
[2] Amazon EC2. http://aws.amazon.com/ec2.
[3] Apache Hadoop. http://hadoop.apache.org.
[4] Apache Hive. http://hive.apache.org.
[5] Kryo serialization library. https://code.google.com/p/kryo.
[6] S. Agarwal et al. Reoptimizing data parallel computing. In NSDI’12.
[7] M. Al-Fares et al. Hedera: Dynamic ﬂow scheduling for data center
networks. In NSDI. 2010.
[8] M. Alizadeh et al. pFabric: Minimal near-optimal datacenter
transport. In SIGCOMM. 2013.
[9] G. Ananthanarayanan et al. Reining in the outliers in mapreduce
clusters using Mantri. In OSDI. 2010.
[10] G. Ananthanarayanan et al. PACMan: Coordinated memory caching
for parallel jobs. In NSDI. 2012.
[11] H. Ballani et al. Towards predictable datacenter networks. In
[12] T. Benson et al. MicroTE: Fine grained trafﬁc engineering for data
SIGCOMM. 2011.
centers. In CoNEXT. 2011.
[13] D. Borthakur. The Hadoop distributed ﬁle system: Architecture and
design. Hadoop Project Website, 2007.
[14] R. Chaiken et al. SCOPE: Easy and efﬁcient parallel processing of
massive datasets. In VLDB. 2008.
[15] M. Chowdhury et al. Managing data transfers in computer clusters
with Orchestra. In SIGCOMM. 2011.
[16] M. Chowdhury et al. Coﬂow: A networking abstraction for cluster
applications. In HotNets-XI, pages 31–36. 2012.
[17] M. Chowdhury et al. Leveraging endpoint ﬂexibility in data-intensive
clusters. In SIGCOMM. 2013.
[18] J. Dean et al. MapReduce: Simpliﬁed data processing on large
clusters. In OSDI, pages 137–150. 2004.
[19] F. Dogar et al. Decentralized task-aware scheduling for data center
networks. Technical Report MSR-TR-2013-96, 2013.
[20] A. Dragojevi´c et al. FaRM: Fast remote memory. In NSDI. 2014.
[21] A. D. Ferguson et al. Participatory networking: An API for
application control of SDNs. In SIGCOMM. 2013.
[22] A. Greenberg et al. VL2: A scalable and ﬂexible data center network.
In SIGCOMM. 2009.
[23] Z. Guo et al. Spotting code optimizations in data-parallel pipelines
through PeriSCOPE. In OSDI. 2012.
[24] B. Hindman et al. Mesos: A Platform for Fine-Grained Resource
Sharing in the Data Center. In NSDI. 2011.
[25] C.-Y. Hong et al. Finishing ﬂows quickly with preemptive
scheduling. In SIGCOMM. 2012.
[26] M. Isard et al. Dryad: Distributed data-parallel programs from
sequential building blocks. In EuroSys, pages 59–72. 2007.
[27] N. Kang et al. Optimizing the “One Big Switch” abstraction in
Software-Deﬁned Networks. In CoNEXT. 2013.
[28] J. E. Kelley. Critical-path planning and scheduling: Mathematical
basis. Operations Research, 9(3):296–320, 1961.
[29] G. Malewicz et al. Pregel: A system for large-scale graph processing.
In SIGMOD. 2010.
[30] M. Mastrolilli et al. Minimizing the sum of weighted completion
times in a concurrent open shop. Operations Research Letters,
38(5):390–395, 2010.
[31] N. McKeown et al. Achieving 100% throughput in an input-queued
switch. IEEE Transactions on Communications, 47(8), 1999.
[32] R. N. Mysore et al. PortLand: A scalable fault-tolerant layer 2 data
center network fabric. In SIGCOMM, pages 39–50. 2009.
[33] L. Popa et al. FairCloud: Sharing the network in cloud computing. In
SIGCOMM. 2012.
[34] T. A. Roemer. A note on the complexity of the concurrent open shop
problem. Journal of Scheduling, 9(4):389–396, 2006.
[35] A. Shieh et al. Sharing the data center network. In NSDI. 2011.
[36] N. Tolia et al. An architecture for internet data transfer. In NSDI’06.
[37] L. G. Valiant. A bridging model for parallel computation.
Communications of the ACM, 33(8):103–111, 1990.
[38] C. A. Waldspurger et al. Lottery scheduling: Flexible
proportional-share resource management. In OSDI. 1994.
network reservations in data centers. In SIGCOMM. 2012.
[39] D. Xie et al. The only constant is change: Incorporating time-varying
[40] M. Zaharia et al. Delay scheduling: A simple technique for achieving
C1 ends! C2!
P1!
P2!
[41] M. Zaharia et al. Resilient distributed datasets: A fault-tolerant
abstraction for in-memory cluster computing. In NSDI. 2012.
locality and fairness in cluster scheduling. In EuroSys. 2010.
1!
1!
C1, C2 arrive !
         at time 0!
1!
1!
1!
½ !
2!
2!
C3!
1!
Time!
2!
2½!
All coﬂows end!
All coﬂows end!
P1!
P2!
P1!
P2!
P1!
P2!
1!
Time!
2!
Time!
1!
Time!
Time!
1!
1!
C1, C2 arrive !
C1, C2 arrive !
         at time 0!
         at time 0!
1!
1!
1!
1!
1!
1!
½ !
½ !
C3 arrives !
C3 arrives !
at time 1!
at time 1!
(a) Input
2!
2!
All coﬂows end!
1!
1!
P1!
P2!
2!
2!
1!
Time!
2!
(b) Work-conserv.
C3!
C1 ends! C2!
C1 ends! C2!
C3!
P1!
P1!
P2!
1!
P2!
1!
1!
1!
2!
1!
2½!
1!
2½!
2!
Time!
1!
Time!
2!
2!
1!
(c) Optimal CCT
Figure 15: Allocation of ingress port capacities (vertical axis) for the
coﬂows in (a) on a 2 × 2 datacenter fabric for (b) a work-conserving and
(c) a CCT-optimized schedule. While the former is work-conserving and
achieves higher utilization, the latter has a lower average CCT.
Both coﬂows end!
Both 
P1!
coﬂows 
complete 
P2!
together!
P1!
P2!
1!
2!
P1!
1!
P2!
2!
1!
Time!
2!
(b) Optimal schedule
1!
Time!
2!
1!
Time!
2!
(c) Varys
1!
1!
1!
1!
1!
(a) Input
2!
Figure 16: Flow-interleaved allocation of ingress port capacities (vertical
axis) for the coﬂows in (a) for CCT-optimality (b).
12 amount of work
pose n jobs arrive at time 0, and the kth job has dk
for machine 1 and dk
21 for machine 2. Since this is NP-hard [34], the
coﬂow scheduling problem described above is NP-hard as well. !
Remark A.2 Given the close relation between concurrent open
shop scheduling and coﬂow scheduling, it is natural to expect that
techniques to express concurrent open shop scheduling as a mixed-
integer program and using standard LP relaxation techniques to de-
rive approximation algorithms [30,34] would readily extend to our
case. However, they do not, because the coupled constraints (3) and
(4) make permutation schedules sub-optimal (Theorem C.1). We
leave the investigation of these topics as future work.
B Tradeoffs in Optimizing CCT
With Work Conservation Consider Figure 15a. Coﬂows C1 and
C2 arrive at time 0 with one and two ﬂows, respectively. Each ﬂow
transfers unit data. C3 arrives one time unit later and uses a single
ﬂow to send 0.5 data unit. Figure 15b shows the work-conserving
solution, which ﬁnishes in 2 time units for an average CCT of 1.67
time units. The optimal solution (Figure 15c), however, takes 2.5
time units for the same amount of data (i.e., it lowers utilization);
still, it has a 1.11× lower average CCT (1.5 time units).
With Avoiding Starvation The tradeoff between minimum com-
pletion time and starvation is well-known for ﬂows (tasks) on in-
dividual links (machines) – longer ﬂows starve if a continuous
stream of short ﬂows keep arriving. The same tradeoff holds for
coﬂows, because the datacenter fabric and coﬂows generalize links
and ﬂows, respectively.
C Ordering Properties of Coﬂow Schedules
Theorem C.1 Permutation schedule is not optimal for minimizing
the average CCT.
Proof Sketch Both permutation schedules – C1 before C2 and C2
before C1 – would be suboptimal for the example in Figure 16a. !
Remark C.2 In Varys, SEBF would schedule C1 before C2 (arbi-
trarily breaking the tie), and iterative MADD will allocate the min-
imum bandwidth to quickly ﬁnish C1 and then give the remaining
bandwidth to C2 (Figure 16c). The average CCT will be the same
as the optimal for this example.
C3 arrives !
at time 1!
APPENDIX
A Problem Formulation and Complexity
Each coﬂow C(D) is a collection of ﬂows over the datacenter back-
plane with P ingress and P egress ports, where the P × P matrix
D = [dij]P×P represents the structure of C. For each non-zero
element dij ∈ D, a ﬂow fij transfers dij amount of data from the
ith ingress port (P in
) across the back-
1!
plane at rate rij, which is determined by the scheduling algorithm.
If Ck represents the time for all ﬂows of the kth coﬂow to ﬁnish
and rk
ij(t) the bandwidth allocated to fij of the kth coﬂow at time
2!
t, the objective of minimizing CCT (O(.)) in the ofﬂine case can be
represented as follows.
) to the jth egress port (P out
1!
1!
1!
1!
j
i
Minimize
K&k=1
Subject to &j!,k
&i!,k
Ck&t=1
Ck
rk
ij! (t) ≤ 1
rk
i!j(t) ≤ 1
rk
ij(t) ≥ dk
ij
(2)
(3)
(4)
(5)
∀t,∀i;
∀t,∀j;
∀i, j, k.
The ﬁrst two inequalities are the capacity constraints on ingress and
egress ports. The third inequality ensures that all ﬂows of the kth
coﬂow ﬁnish by time Ck.
By introducing a binary variable Uk to denote whether a coﬂow
ﬁnished within its deadline Dk, we can express the objective of
maximizing the number of coﬂows that meet their deadlines (Z(.))
in the ofﬂine case as follows.
Maximize
Uk
(6)
K&k=1
Subject to inequalities (3), (4), and (5);
Where Uk =’1 Ck ≤ Dk
0 Ck > Dk
Optimizing either objective (O or Z) is NP-hard.
Theorem A.1 Even under the assumptions of Section 5.1, optimiz-
ing O or Z in the ofﬂine case is NP-hard for all P ≥ 2.
Proof Sketch We reduce the NP-hard concurrent open shop
scheduling problem [34] to the coﬂow scheduling problem. Con-
sider a network fabric with only 2 ingress and egress ports (P = 2)
and all links have the same capacity (without loss of generality, we
can let this capacity be 1). Since there are only 2 ports, all coﬂows
i,j=1 is a 2×2 data matrix.
are of the form C(D), where D = (dij)2
i,j=1 be
Suppose that n coﬂows arrive at time 0, and let Dk = (dk
ij)2
the matrix of the kth coﬂow. Moreover, assume for all k, dk
ij = 0
if i = j. In other words, every coﬂow only consists of 2 ﬂows, one
sending data from ingress port P in
, and the
other sending from ingress port P in
to egress port P out
to egress port P out
.
Consider now an equivalent concurrent open shop scheduling
problem with 2 identical machines (hence the same capacity). Sup-
1
2
2
1