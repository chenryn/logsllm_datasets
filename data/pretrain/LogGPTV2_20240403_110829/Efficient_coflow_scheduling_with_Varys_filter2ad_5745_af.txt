# 8. Discussion

## Scheduling with Unknown Flow Sizes
Knowing or estimating exact flow sizes is challenging in frameworks that push data to the next stage as soon as possible [26]. Without known flow sizes, preemption becomes impractical. FIFO scheduling can address this issue but suffers from head-of-line blocking (§7.4). We believe that coflow-level fairness can be a good compromise between these two extremes. However, defining and establishing the properties of fairness at the coflow level remains an open problem.

## Decentralized SEBF+MADD
Varys's centralized design makes it less suitable for small coflows (§3), but small coflows contribute less than 1% of the traffic in data-intensive clusters (§4). In-network isolation of control plane messages [21] or faster signaling channels like RDMA [20] can reduce Varys’s application-layer signaling overheads (§7.2) to support even smaller coflows. A decentralized approximation of our algorithms is the most viable way to make Varys useful for low-latency coflows. This requires new algorithms and potential changes to network devices, unlike our current application-layer design.

## Handling Coflow Dependencies
While most jobs require only a single coflow, dataflow pipelines (e.g., Dryad, Spark) can create multiple coflows with dependencies between them [16]. A simple approach to support coflow dependencies would be to order first by ancestry and then break ties using SEBF. Variations of the Critical-Path Method [28] might perform even better. We leave this as a topic for future work. Note that dependencies can be passed along to the scheduler through options in the `register()` method.

## Multi-Wave Coflows
Large jobs often schedule mappers in multiple waves [10]. A job can create separate coflows for each wave. Alternatively, if the job uses its wave-width (i.e., the number of parallel mappers) as `numFlows` in `register()`, Varys can handle each wave separately. Applications can convey information about wave-induced coflows to the scheduler as dependencies.

## In-Network Bottlenecks
Varys performs well even when the network is not a non-blocking switch (§7). If likely bottleneck locations are known, e.g., rack-to-core links are typically oversubscribed [17], Varys can be extended to allocate rack-to-core bandwidth instead of NIC bandwidth. When bottlenecks are unknown, e.g., due to in-network failures, routing, or load imbalance, Varys can react based on bandwidth estimations collected by its daemons. Nonetheless, designing and deploying coflow-aware routing protocols and load balancing techniques remain an open challenge.

# 9. Related Work

## Coflow Schedulers
Varys improves over Orchestra [15] in four major ways:
1. **Optimization**: Orchestra primarily optimizes individual coflows and uses FIFO among them, whereas Varys uses an efficient coflow scheduler to significantly outperform FIFO.
2. **Deadlines**: Varys supports deadlines and ensures guaranteed coflow completion.
3. **Rate-Based Approach**: Varys uses a rate-based approach instead of manipulating the number of TCP flows, which breaks if all coflows do not share the same endpoints.
4. **Multiple Frameworks**: Varys supports coflows from multiple frameworks like Mesos [24] and handles non-network resources.

**Baraat [19]** is a FIFO-based decentralized coflow scheduler focusing on small coflows. It uses fair sharing to avoid head-of-line blocking and does not support deadlines. Furthermore, we formulate the coflow scheduling problem and analyze its characteristics.

## Datacenter Traffic Management
**Hedera [7]** manages flows using a centralized scheduler to increase network throughput, and **MicroTE [12]** adapts to traffic variations by leveraging their short-term predictability. However, both work with flows and are unsuitable for optimizing CCTs. **Sinbad [17]** uses endpoint flexible transfers for load balancing. Once it makes network-aware placement decisions, Varys can optimize cross-rack write coflows.

## High Capacity Networks
Full bisection bandwidth topologies [22,32] do not imply contention freedom. In the presence of skewed data and hotspot distributions [17], managing edge bandwidth is still necessary. Inter-coflow scheduling improves performance and predictability even in these high capacity networks.

## Traffic Reduction Techniques
Data locality [18], both disk [9,40] and memory [10], reduces network usage only during reads. The amount of network usage due to intermediate data communication can be reduced by pushing filters toward the sources [6, 23]. Our approach is complementary; i.e., it can be applied to whatever data traverses the network after applying those techniques.

## Network Sharing Among Tenants
Fair sharing of network resources between multiple tenants has received considerable attention [11, 33, 35, 39]. Our work is complementary; we focus on optimizing performance of concurrent coflows within a single administrative domain, instead of achieving fairness among competing entities. Moreover, we focus on performance and predictability rather than the more debated notion of fairness.

## Concurrent Open Shop Scheduling
Inter-coflow scheduling has its roots in the concurrent open shop scheduling problem [34], which is strongly NP-hard for even two machines. Even in the offline scenario, the best-known result is a 2-approximation algorithm [30], and it is inapproximable within a factor strictly less than 6/5 if P ≠ NP [30]. Our setting differs as follows:
1. **Coupled Resources**: Machines are not independent; i.e., links are coupled because each flow involves a source and a destination.
2. **Online Arrivals**: Jobs are not known a priori; i.e., coflows arrive in an online fashion.

# 10. Concluding Remarks

The coflow abstraction [16] effectively enables application-aware network scheduling. We have implemented coflows in a system called Varys and introduced the concurrent open shop scheduling with coupled resources problem. To minimize coflow completion times (CCT), we proposed the SEBF heuristic to schedule coflows and the MADD algorithm to allocate bandwidth to their flows. Together, they decrease the average CCT without starving any coflow and maintain high network utilization. Through EC2 deployments and trace-driven simulations, we showed that Varys outperforms per-flow mechanisms by up to 3.16× and non-preemptive coflow schedulers by more than 5×. Furthermore, by applying MADD in conjunction with admission control, Varys allowed up to 2× more coflows to meet their deadlines compared to per-flow schemes.

In conclusion, this paper is only a first step in understanding the intricacies of inter-coflow scheduling and opens up a variety of exciting research problems, including scheduling without knowing flow sizes, exploring the notion of coflow fairness, decentralizing the proposed algorithms, and handling coflow dependencies.

# Acknowledgments

We thank Mohammad Alizadeh, Justine Sherry, Rachit Agarwal, Peter Bailis, Ganesh Ananthanarayanan, Tathagata Das, Ali Ghodsi, Gautam Kumar, David Zats, Matei Zaharia, the AMPLab members, our shepherd Nandita Dukkipati, and the anonymous reviewers of NSDI’14 and SIGCOMM’14 for their valuable feedback. This research is supported in part by NSF CISE Expeditions Award CCF-1139158, LBNL Award 7076018, and DARPA XData Award FA8750-12-2-0331, and gifts from Amazon Web Services, Google, SAP, The Thomas and Stacey Siebel Foundation, Apple, Inc., Cisco, Cloudera, EMC, Ericsson, Facebook, GameOnTalis, Guavus, HP, Huawei, Intel, Microsoft, NetApp, Pivotal, Splunk, Virdata, VMware, WANdisco, and Yahoo!.

# 11. References

[1] Akka. http://akka.io.
[2] Amazon EC2. http://aws.amazon.com/ec2.
[3] Apache Hadoop. http://hadoop.apache.org.
[4] Apache Hive. http://hive.apache.org.
[5] Kryo serialization library. https://code.google.com/p/kryo.
[6] S. Agarwal et al. Reoptimizing data parallel computing. In NSDI’12.
[7] M. Al-Fares et al. Hedera: Dynamic flow scheduling for data center networks. In NSDI. 2010.
[8] M. Alizadeh et al. pFabric: Minimal near-optimal datacenter transport. In SIGCOMM. 2013.
[9] G. Ananthanarayanan et al. Reining in the outliers in MapReduce clusters using Mantri. In OSDI. 2010.
[10] G. Ananthanarayanan et al. PACMan: Coordinated memory caching for parallel jobs. In NSDI. 2012.
[11] H. Ballani et al. Towards predictable datacenter networks. In SIGCOMM. 2011.
[12] T. Benson et al. MicroTE: Fine-grained traffic engineering for data centers. In CoNEXT. 2011.
[13] D. Borthakur. The Hadoop distributed file system: Architecture and design. Hadoop Project Website, 2007.
[14] R. Chaiken et al. SCOPE: Easy and efficient parallel processing of massive datasets. In VLDB. 2008.
[15] M. Chowdhury et al. Managing data transfers in computer clusters with Orchestra. In SIGCOMM. 2011.
[16] M. Chowdhury et al. Coflow: A networking abstraction for cluster applications. In HotNets-XI, pages 31–36. 2012.
[17] M. Chowdhury et al. Leveraging endpoint flexibility in data-intensive clusters. In SIGCOMM. 2013.
[18] J. Dean et al. MapReduce: Simplified data processing on large clusters. In OSDI, pages 137–150. 2004.
[19] F. Dogar et al. Decentralized task-aware scheduling for data center networks. Technical Report MSR-TR-2013-96, 2013.
[20] A. Dragojević et al. FaRM: Fast remote memory. In NSDI. 2014.
[21] A. D. Ferguson et al. Participatory networking: An API for application control of SDNs. In SIGCOMM. 2013.
[22] A. Greenberg et al. VL2: A scalable and flexible data center network. In SIGCOMM. 2009.
[23] Z. Guo et al. Spotting code optimizations in data-parallel pipelines through PeriSCOPE. In OSDI. 2012.
[24] B. Hindman et al. Mesos: A platform for fine-grained resource sharing in the data center. In NSDI. 2011.
[25] C.-Y. Hong et al. Finishing flows quickly with preemptive scheduling. In SIGCOMM. 2012.
[26] M. Isard et al. Dryad: Distributed data-parallel programs from sequential building blocks. In EuroSys, pages 59–72. 2007.
[27] N. Kang et al. Optimizing the “One Big Switch” abstraction in Software-Defined Networks. In CoNEXT. 2013.
[28] J. E. Kelley. Critical-path planning and scheduling: Mathematical basis. Operations Research, 9(3):296–320, 1961.
[29] G. Malewicz et al. Pregel: A system for large-scale graph processing. In SIGMOD. 2010.
[30] M. Mastrolilli et al. Minimizing the sum of weighted completion times in a concurrent open shop. Operations Research Letters, 38(5):390–395, 2010.
[31] N. McKeown et al. Achieving 100% throughput in an input-queued switch. IEEE Transactions on Communications, 47(8), 1999.
[32] R. N. Mysore et al. PortLand: A scalable fault-tolerant layer 2 data center network fabric. In SIGCOMM, pages 39–50. 2009.
[33] L. Popa et al. FairCloud: Sharing the network in cloud computing. In SIGCOMM. 2012.
[34] T. A. Roemer. A note on the complexity of the concurrent open shop problem. Journal of Scheduling, 9(4):389–396, 2006.
[35] A. Shieh et al. Sharing the data center network. In NSDI. 2011.
[36] N. Tolia et al. An architecture for internet data transfer. In NSDI’06.
[37] L. G. Valiant. A bridging model for parallel computation. Communications of the ACM, 33(8):103–111, 1990.
[38] C. A. Waldspurger et al. Lottery scheduling: Flexible proportional-share resource management. In OSDI. 1994.
[39] D. Xie et al. The only constant is change: Incorporating time-varying network reservations in data centers. In SIGCOMM. 2012.
[40] M. Zaharia et al. Delay scheduling: A simple technique for achieving locality and fairness in cluster scheduling. In EuroSys. 2010.
[41] M. Zaharia et al. Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing. In NSDI. 2012.

# Appendix

## A. Problem Formulation and Complexity
Each coflow \( C(D) \) is a collection of flows over the datacenter backplane with \( P \) ingress and \( P \) egress ports, where the \( P \times P \) matrix \( D = [d_{ij}]_{P \times P} \) represents the structure of \( C \). For each non-zero element \( d_{ij} \in D \), a flow \( f_{ij} \) transfers \( d_{ij} \) amount of data from the \( i \)-th ingress port (\( P_{\text{in}}^i \)) to the \( j \)-th egress port (\( P_{\text{out}}^j \)) at rate \( r_{ij} \), which is determined by the scheduling algorithm. If \( C_k \) represents the time for all flows of the \( k \)-th coflow to finish and \( r_{ij}^k(t) \) the bandwidth allocated to \( f_{ij} \) of the \( k \)-th coflow at time \( t \), the objective of minimizing CCT (O(.)) in the offline case can be represented as follows:

\[
\text{Minimize} \sum_{k=1}^K C_k
\]

\[
\text{Subject to:}
\]
\[
\sum_{j \neq k} \sum_{i \neq k} r_{ij}^k(t) \leq 1 \quad \forall t, \forall i
\]
\[
\sum_{i \neq k} \sum_{j \neq k} r_{ij}^k(t) \leq 1 \quad \forall t, \forall j
\]
\[
r_{ij}^k(t) \geq d_{ij}^k \quad \forall i, j, k
\]

The first two inequalities are the capacity constraints on ingress and egress ports. The third inequality ensures that all flows of the \( k \)-th coflow finish by time \( C_k \).

By introducing a binary variable \( U_k \) to denote whether a coflow finished within its deadline \( D_k \), we can express the objective of maximizing the number of coflows that meet their deadlines (Z(.)) in the offline case as follows:

\[
\text{Maximize} \sum_{k=1}^K U_k
\]

\[
\text{Subject to:}
\]
\[
U_k = 
\begin{cases} 
1 & \text{if } C_k \leq D_k \\
0 & \text{if } C_k > D_k 
\end{cases}
\]

Optimizing either objective (O or Z) is NP-hard.

**Theorem A.1:** Even under the assumptions of Section 5.1, optimizing O or Z in the offline case is NP-hard for all \( P \geq 2 \).

**Proof Sketch:** We reduce the NP-hard concurrent open shop scheduling problem [34] to the coflow scheduling problem. Consider a network fabric with only 2 ingress and egress ports (\( P = 2 \)) and all links have the same capacity (without loss of generality, we can let this capacity be 1). Since there are only 2 ports, all coflows are of the form \( C(D) \), where \( D = (d_{ij})_{2 \times 2} \) is a 2x2 data matrix.

Suppose that \( n \) coflows arrive at time 0, and let \( D_k = (d_{ij}^k)_{2 \times 2} \) be the matrix of the \( k \)-th coflow. Moreover, assume for all \( k \), \( d_{ij}^k = 0 \) if \( i = j \). In other words, every coflow only consists of 2 flows, one sending data from ingress port \( P_{\text{in}}^1 \) to egress port \( P_{\text{out}}^2 \), and the other sending from ingress port \( P_{\text{in}}^2 \) to egress port \( P_{\text{out}}^1 \).

Consider now an equivalent concurrent open shop scheduling problem with 2 identical machines (hence the same capacity). Suppose \( n \) jobs arrive at time 0, and the \( k \)-th job has \( d_{12}^k \) amount of work for machine 1 and \( d_{21}^k \) for machine 2. Since this is NP-hard [34], the coflow scheduling problem described above is NP-hard as well.

**Remark A.2:** Given the close relation between concurrent open shop scheduling and coflow scheduling, it is natural to expect that techniques to express concurrent open shop scheduling as a mixed-integer program and using standard LP relaxation techniques to derive approximation algorithms [30,34] would readily extend to our case. However, they do not, because the coupled constraints (3) and (4) make permutation schedules sub-optimal (Theorem C.1). We leave the investigation of these topics as future work.

## B. Tradeoffs in Optimizing CCT

### With Work Conservation
Consider Figure 15a. Coflows \( C_1 \) and \( C_2 \) arrive at time 0 with one and two flows, respectively. Each flow transfers unit data. \( C_3 \) arrives one time unit later and uses a single flow to send 0.5 data units. Figure 15b shows the work-conserving solution, which finishes in 2 time units for an average CCT of 1.67 time units. The optimal solution (Figure 15c), however, takes 2.5 time units for the same amount of data (i.e., it lowers utilization); still, it has a 1.11× lower average CCT (1.5 time units).

### With Avoiding Starvation
The tradeoff between minimum completion time and starvation is well-known for flows (tasks) on individual links (machines) – longer flows starve if a continuous stream of short flows keep arriving. The same tradeoff holds for coflows, because the datacenter fabric and coflows generalize links and flows, respectively.

## C. Ordering Properties of Coflow Schedules

**Theorem C.1:** Permutation schedule is not optimal for minimizing the average CCT.

**Proof Sketch:** Both permutation schedules – \( C_1 \) before \( C_2 \) and \( C_2 \) before \( C_1 \) – would be suboptimal for the example in Figure 16a.

**Remark C.2:** In Varys, SEBF would schedule \( C_1 \) before \( C_2 \) (arbitrarily breaking the tie), and iterative MADD will allocate the minimum bandwidth to quickly finish \( C_1 \) and then give the remaining bandwidth to \( C_2 \) (Figure 16c). The average CCT will be the same as the optimal for this example.