title:Work-conserving distributed schedulers for Terabit routers
author:Prashanth Pappu and
Jonathan S. Turner and
Kenneth Wong
Work-Conserving Distributed Schedulers  
for Terabit Routers 
Prashanth Pappu 
Washington University 
Computer Science and Engineering 
St. Louis, MO 63130-4899 
+1-314-935-4306 
Jonathan Turner 
Washington University 
Ken Wong 
Washington University 
Computer Science and Engineering 
Computer Science and Engineering 
St. Louis, MO 63130-4899 
+1-314-935-8552 
St. Louis, MO 63130-4899 
+1-314-935-7524 
PI:EMAIL 
PI:EMAIL 
PI:EMAIL 
ABSTRACT  −−−−    Buffered  multistage  interconnection  networks 
offer  one  of  the  most  scalable  and  cost-effective  approaches  to 
building high capacity routers. Unfortunately, the performance of 
such  systems  has  been difficult to predict in the presence of the 
extreme  traffic  conditions  that  can  arise  in  the  Internet.  Recent 
work  introduced  distributed  scheduling,  to  regulate  the  flow  of 
traffic in such systems. This work demonstrated, using simulation 
and  experimental  measurements,  that  distributed  scheduling  can 
deliver robust performance for extreme traffic. Here, we show that 
distributed  schedulers  can  be  provably  work-conserving  for 
speedups of 2 or more. Two of the three schedulers we describe 
were  inspired  by  previously  published  crossbar  schedulers.  The 
third  has  no  direct  counterpart  in  crossbar  scheduling.  In  our 
analysis,  we  show  that  distributed  schedulers  based  on blocking 
flows in small-depth acyclic flow graphs can be work-conserving, 
just  as  certain  crossbar  schedulers  based  on  maximal  bipartite 
matchings have been shown to be work-conserving. We also study 
the performance of practical variants of these schedulers when the 
speedup is less than 2, using simulation.   
Categories and Subject Descriptors. C.2.1 [Computer-
Communications Networks]: Network Architecture and Design – 
network communications, packet-switching networks. 
General Terms. algorithms, performance 
Keywords. distributed scheduling, crossbar scheduling,  high 
performance  routers, CIOQ switches 
1. INTRODUCTION 
High  performance  routers  must  be  scalable  to  hundreds  or  even 
thousands of ports. The most scalable router architectures include 
systems  using  multistage  interconnection  networks  with  internal 
buffers and a small speedup relative to the external links; that is, 
the internal data paths operate at speeds that are greater than the 
external links by a small constant factor (typically between 1 and 
This work supported by DARPA (contract #N660001-01-1-8930) and NSF 
(ANI-0325298). 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, 
or  republish,  to  post  on  servers  or  to  redistribute  to  lists,  requires  prior 
specific permission and/or a fee. 
SIGCOMM’04, Aug. 30–Sept. 3, 2004, Portland, Oregon, USA.  
Copyright 2004 ACM 1-58113-862-8/04/0008...$5.00. 
2). In the presence of a sustained overload at an output port, such 
systems can become congested with traffic attempting to reach the 
overloaded  output,  interfering  with  the  flow  of  traffic  to  other 
outputs.  The  unregulated  nature  of  traffic  in  IP  networks  makes 
such overloads a normal fact of life, which router designers must 
address, in order to make their systems robust. 
Reference  [11]  introduced  distributed  scheduling  to  manage 
the flow of traffic through a large router in order to mitigate the 
worst  effects  of  extreme  traffic.  Distributed  scheduling  borrows 
ideas  developed  for  scheduling  packet  transmissions  through 
crossbar switches [2,5,7,8]. The core idea is to use Virtual Output 
Queues (VOQ) at each input. That is, each input maintains sepa-
rate  queues  for  each  output.  (Queues  are  implemented  as  linked 
lists, so the only per queue overhead is for the queues’ head and 
tail pointers.) Packets arriving at inputs are placed in queues cor-
responding to their outgoing links. In crossbar scheduling, a cen-
tralized  scheduler  selects  packets  to  send  through  the  crossbar, 
seeking to emulate as closely as possible, the queueing behavior 
of an ideal output queued switch. The centralized scheduler used 
in  crossbar  scheduling  makes  scheduling  decisions  every  packet 
transmission interval. For routers with 10 Gb/s links, this typically 
means  making  scheduling  decisions  every  40  ns,  a  demanding 
requirement,  even  for  routers  with  small  numbers  of  links.  For 
larger routers it makes centralized scheduling infeasible. 
Distributed  scheduling,  unlike  crossbar  scheduling,  does  not 
seek to schedule the transmission of individual packets. Instead, it 
regulates the number of packets forwarded during a period which 
we call the scheduling interval and denote by T. The scheduling 
interval is typically fairly long, on the order of tens of microsec-
onds.  The  use  of  such  coarse-grained  scheduling  means  that  a 
distributed scheduler can only approximate the queueing behavior 
of an ideal output-queued switch, but does allow systems to scale 
up  to  larger  configurations  than  are  practical  with  fine-grained 
scheduling. In a router that implements distributed scheduling, the 
Port Processors (the components that terminate the external links, 
make routing decisions and queue packets) periodically exchange 
information  about  the  status  of  their  VOQs.  This  information  is 
then used to rate control the VOQs, with the objective of moving 
packets to the output side of the router as expeditiously as possi-
ble,  while  avoiding  congestion  within  the  interconnection  net-
work. So long as the scheduling interval is kept small relative to 
end-to-end delays (which are typically tens to hundreds of milli-
seconds  in  wide  area  networks)  the  impact  of  coarse  scheduling 
on the delays experienced by packets can be acceptably small. 
257While [11] demonstrated, using simulation and experimental 
measurement,  that  distributed  scheduling  can  deliver  excellent 
performance under extreme traffic conditions, it provided no ana-
lytical bounds on the performance of the proposed algorithms, nor 
a rigorous justification for the specific design choices. This paper 
corrects  that  deficiency,  by  showing  that  there  are  distributed 
schedulers  that  are  provably work-conserving, for speedups of 2 
or more. The analysis provides insight that motivates the design of 
more practical variants of these algorithms, which provide excel-
lent  performance  (significantly  better  than  reported  in  [11]). 
Where  the  algorithms  described  in  [11]  can  fail  to  be  work-
conserving, with speedups of more than 2, the algorithms reported 
here  are  demonstrably  work-conserving  for extreme traffic, even 
when  speedups  are  less  than  2.  One  interesting  aspect  of  the 
analysis is the role played by network flows, which parallels the 
role played by bipartite matching in crossbar scheduling. Specifi-
cally,  distributed  schedulers  that  are  based  on  finding  blocking 
flows  in  small  depth  acyclic  flow  graphs  and  that  favor  outputs 
with short queues are work-conserving, much as crossbar sched-
ulers based on finding maximal matchings in bipartite graphs that 
favor outputs with short queues are work-conserving. 
Before  proceeding  further,  it’s  important  to  define  what  we 
mean  by  work-conserving.  A  crossbar  scheduler  is  work-
conserving if, in a system using that scheduler, an output link can 
be  idle  only  if  there  is  no  packet  in  the  system  for  that  output. 
Work-conserving  systems  match  the  throughput  of  ideal  output 
queueing  switches,  under  all  possible  traffic  conditions.  In  the 
context  of  distributed  scheduling, 
the  definition  of  work-
conservation must be relaxed to reflect the use of coarse-grained 
scheduling. In section 2, we adopt an idealized definition of work-
conservation for the purposes of analysis. We discuss the practical 
implications of this in section 6. 
It should be noted that while the practical distributed schedul-
ing algorithms discussed here are not work-conserving, practical 
crossbar  scheduling  algorithms  are  also  not  work-conserving, 
even  though  it  has  been  known  for  several  years  that  there  are 
work-conserving  crossbar  scheduling  algorithms  that  are  too 
complex to use in real systems. The contribution of this work is to 
show that distributed scheduling for buffered multistage networks 
can  provide  similar  performance  to  what  was  previously  known 
for crossbar schedulers.  
While distributed scheduling shares some features of crossbar 
scheduling, it differs in two important respects. First, the distrib-
uted  nature  of  these  methods  rules  out  the  use  of  the  iterative 
matching methods that have proved effective in crossbar schedul-
ing,  since  each  iteration  would  require  an  exchange  of  informa-
tion, causing the overhead of the algorithm to increase in propor-
tion  to  the  number  of  iterations.  On  the  other  hand,  the  coarse-
grained nature of distributed scheduling provides some flexibility 
that is not present in crossbar scheduling, where it is necessary to 
match  inputs  and  outputs  in  a  one-to-one  fashion  during  each 
scheduling  operation.  In  distributed  scheduling,  we  allocate  the 
interface bandwidth at each input and output and may subdivide 
that bandwidth in whatever proportions produce the best result. 
Recently, there has been considerable interest in a switch ar-
chitecture  called  the  load  balanced  switch  described  in  [4]  and 
used in [6]. This architecture consists of a single stage of buffers 
sandwiched  between  two  identical  stages  of  switching,  each  of 
which  walks  through  a  fixed  sequence  of  configurations.  The 
fixed  sequence  of  switch  configurations  makes  the  switching 
components  very  simple  and  the  system  is  capable  of  achieving 
100% throughput for random traffic. Unfortunately, this architec-
ture  also  has  a  significant  drawback.  To  avoid  resequencing  er-
rors, each output requires a resequencing buffer capable of hold-
ing about n2 packets. These buffers impose a delay that grows as 
the square of the switch size. For the 600 port switch described in 
[6], operated with a switching period of 100 ns, this translates to a 
delay  of  about  36  milliseconds,  a  penalty  which  applies  to  all 
packets,  not  just  to  an  occasional  packet.  This  appears  to  be  an 
intrinsic characteristic of the load balancing architecture, and one 
that significantly limits its attractiveness. 
Section  2  introduces  two  scheduling  methods,  proves  that 
schedulers based on these methods are work-conserving when the 
speedup  is  at  least  2  and  shows  how  they  can  be  implemented. 
Section  3  shows  how  one  can  implement  a  practical  distributed 
scheduler,  based  on  one  of  these  methods  and  evaluates  its per-
formance  for  speedups  less  than  2,  using  simulation.  Section  4 
introduces a more sophisticated, scheduling method, shows that it 
too is work-conserving when the speedup is at least 2 and shows 
how it can be implemented using minimum cost blocking flows in 
networks with convex cost functions. Section 5 describes a practi-
cal variant of this method and evaluates it using simulation, show-
ing that it can out-perform the simpler schedulers studied earlier. 
Section 6 discusses several important practical considerations for 
distributed scheduling. 
2. WORK-CONSERVING SCHEDULERS 
We  describe  a  general  scheduling  strategy  that  can  be  used  to 
obtain  work-conserving  schedulers  for  speedups  of  2  or  more. 
While these algorithms are not practical, they provide a concep-
tual foundation for other algorithms that are.  
For  the  purposes  of  analysis,  we  adopt  an  idealized  view  of 
the  system  operation.  Specifically,  we  assume  that  the  system 
operates  in  three  discrete  phases:  an  arrival  phase,  a  transfer 
phase and a departure phase. During the arrival phase, each input 
receives up to T cells.1 During the transfer phase, cells are moved 
from  inputs  to  outputs,  with  each  input  constrained  to  send  at 
most ST cells (S being the speedup of the system) and each output 
constrained to receive at most ST. During the output phase, each 
output forwards up to T cells. A scheduler determines which cells 
are transferred during the transfer phase. We say that a scheduler 
is work-conserving if during every departure phase, all outputs for 
which  some  input  has  cells  at  the  start  of  the  departure  phase, 
transmit T cells during the departure phase. 
The scheduling methods that we study in this section maintain 
an ordering of the non-empty VOQs at each input. The ordering of 
the VOQs can be extended to all the cells at an input. Two cells in 
the same VOQ are ordered according to their position in the VOQ. 
Cells in different VOQs are ordered according the VOQ ordering. 
We say that a cell b precedes a cell c at the same input, if b comes 
before c in this ordering. For any cell c at an input, we let p(c) be 
the number of cells at the same input as c that precede c and we 
let q(c) be the number of cells at the output that c is going to. 
1. We assume throughout, that variable-length packets are seg-
mented into fixed-length units for transmission through the in-
terconnection network. We refer to these units as cells. 
We refer to a cell c as an ij-cell if it is at input i and is des-
tined for output j. We say that a scheduling algorithm is maximal 
if during any transfer phase in which there is an ij-cell c that re-
mains  at  input  i,  either  input  i  transfers  ST  cells  or  output  j  re-
ceives  ST  cells.  Given  a  method  for  ordering  the  cells  at  each 
input, we say that a scheduler is ordered, if in any transfer phase 
in which an ij-cell c remains at input i, either input i transfers ST 
cells that precede c or output j receives ST cells. Our scheduling 
methods produce schedules that are maximal and ordered. We can 
vary  the  method  by  using  different  VOQ  orderings. We describe 
two ordering methods that lead to work-conserving schedulers. 
For  any  cell  c  waiting  at  an  input,  we  define  the  quantity 
slack(c)  =  q(c)  − p(c).  For  each  of  the  methods  studied,  we’ll 
show that slack(c) ≥ T at the start of each departure phase if S ≥ 2. 
This implies that for any output with fewer than T cells in its out-
going  queue,  there  can  be  no  cells  waiting  in  any  input-side 
VOQs. This implies that the scheduler is work-conserving. 
2.1 Batch Critical Cells First 
Our first scheduling method is based on ideas first developed in 
the Critical Cells First scheduler of [5]. Hence, we refer to it as 
the Batch Critical Cells First (BCCF) method. In BCCF, the rela-
tive ordering of two  VOQs remains the same so long as they re-
main  non-empty,  but  when  a  new  VOQ  becomes  non-empty,  it 
must be ordered relative to the others.  When a cell c arrives and 
the VOQ for c’s output is empty, we insert the VOQ into the exist-
ing ordering based on the magnitude of q(c). In particular, if the 
ordered list of VOQs is v1, v2, . . . , we place the VOQ immediately 
after the queue vj determined by the largest integer j for which the 
number of cells in the combined queues v1, . . . ,vj is no larger than 
q(c).  This  ensures  that  slack(c)  is  non-negative  right  after  c  ar-
rives. A specific scheduler is an instance of the BCCF method if it 
produces schedules that are maximal and ordered with respect to 
this VOQ ordering method. To show that slack(c) ≥ T at the start 
of each departure phase, we need two lemmas. 
Lemma 1. For any BCCF scheduler, if c is any cell that remains at 
its input during a transfer phase, then slack(c) increases by at least 
ST during the transfer phase. 
proof. Since the VOQ ordering does not change during a transfer 
phase  (more  precisely,  VOQs  that  remain  non-empty  retain  the 
same relative order), any maximal, ordered scheduling algorithm 
either causes q(c) to increase by ST or causes p(c) to decrease by 
ST. In either case, slack(c) increases by ST. (cid:132) 
Note that as long as a cell c remains at an input, each arrival 
phase and departure phase cause slack(c) to decrease by at most T. 
So, if S  ≥2, slack(c) cannot decrease over the course of a complete 
time step. 
Lemma 2. For any BCCF scheduler with S ≥2, if c is any cell at an 
input just before the start of a departure phase, slack(c) ≥ T. 
proof. We show that for any cell c present at the end of an arrival 
phase, slack(c) ≥ −T. The result then follows from Lemma 1 and 
the fact that S ≥2. The proof is by induction on the time step.  
For any cell c that arrives during the first time step, p(c) ≤ T at 
the  end  of  the  arrival  phase,  so  slack(c)  ≥  −T  at  the  end  of  the 
arrival phase. Since S ≥2, there can be no net decrease in slack(c) 
from one time step to the next, so slack(c) remains ≥−T at the end 
of each subsequent arrival phase, while c remains at the input.  
If a cell c arrives during step t and its VOQ is empty when it 
arrives, then the rule used to order the VOQ relative to the others 
ensures that slack(c) ≥ 0 right after it arrives. Hence, slack(c) ≥ −T 
at the end of the arrival phase and this remains true at the end of 
each subsequent arrival phase, so long as c remains at the input. 
If a cell c arrives during step t and its VOQ is not empty, but 
was empty at the start of the arrival phase, then let b be the first 
arriving  cell  to  be  placed  in  c’s  VOQ  during  this  arrival  phase. 
Then, slack(b) was at least 0 at the time it arrived and at most T−1 
cells could have arrived after b did in this arrival phase. If exactly 
r of these precede b, then at the end of the arrival phase, 
slack
c
)(
≥
slack
−−≥
)
(
−
b
)(
T
((
r
T
((
−−
)1
−−
)1
r
)
r
)
−≥
T
If a cell c arrives during step t and its VOQ was not empty at the 
start of the arrival phase, then let b be the last cell in c’s VOQ at 
the start of the arrival phase. By the induction hypothesis, slack(b) 
≥ −T at the end of the previous arrival phase. Since the subsequent 
transfer phase increases slack(b) by at least 2T and the departure 
phase  decreases  it  by  at  most  T,  slack(b)  ≥  0  at  the  start  of  the 
arrival  phase  in  step  t.  During  this  arrival phase, at most T new 
cells arrive at c’s input. Let r be the number of these arriving cells 
that precede b. Then at the end of the arrival phase 
−
r
)
−≥
T
≥
slack
−−≥
(
)
−
b
)(
T