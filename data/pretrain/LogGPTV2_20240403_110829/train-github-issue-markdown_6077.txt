 **System information**
  * OS Platform: OS X 10.13.3
  * Custom code
  * tensorflow version: 1.12.0
  * python version: 3.6.5
**Describe the current behavior**  
Tensorflow raises a TypeError when creating a dynamic_rnn with tf.int32 type
in its input and state. When changing the type to tf.float32 the error is not
raised.
**Describe the expected behavior**  
Ideally, a dynamic_rnn should support tf.in32 types. If there's any reason why
instantiating a dynamic_rnn with tf.int32 type in its input and state should
not be allowed, a custom error should be raised.
**Code to reproduce the issue**  
The code below reproduces the error:
    import tensorflow as tf
    X = tf.placeholder(tf.int32, [None, 10, 1])
    cell = tf.nn.rnn_cell.LSTMCell(1, dtype=tf.int32)
    output, state = tf.nn.dynamic_rnn(cell=cell, inputs=X, dtype=tf.int32)
The code below doesn't:
    import tensorflow as tf
    X = tf.placeholder(tf.float32, [None, 10, 1])
    cell = tf.nn.rnn_cell.LSTMCell(1, dtype=tf.float32)
    output, state = tf.nn.dynamic_rnn(cell=cell, inputs=X, dtype=tf.float32)
Note the change in dtype.
## **Other info / logs**  
TRACEBACK:
TypeError Traceback (most recent call last)  
in ()  
2 X = tf.placeholder(tf.int32, [None, 10, 1])  
3 cell = tf.nn.rnn_cell.LSTMCell(1, dtype=tf.int32)  
\----> 4 output, state = tf.nn.dynamic_rnn(cell=cell, inputs=X,
dtype=tf.int32)#, initial_state=state)  
5
~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in
dynamic_rnn(cell, inputs, sequence_length, initial_state, dtype,
parallel_iterations, swap_memory, time_major, scope)  
662 swap_memory=swap_memory,  
663 sequence_length=sequence_length,  
\--> 664 dtype=dtype)  
665  
666 # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].
~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in
_dynamic_rnn_loop(cell, inputs, initial_state, parallel_iterations,
swap_memory, sequence_length, dtype)  
870 parallel_iterations=parallel_iterations,  
871 maximum_iterations=time_steps,  
\--> 872 swap_memory=swap_memory)  
873  
874 # Unpack final output if not using output tuples.
~/jonassucks3/lib/python3.6/site-
packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body,
loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory,
name, maximum_iterations, return_same_structure)  
3289 ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)  
3290 result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,  
-> 3291 return_same_structure)  
3292 if maximum_iterations is not None:  
3293 return result[1]
~/jonassucks3/lib/python3.6/site-
packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred,
body, loop_vars, shape_invariants, return_same_structure)  
3002 with ops.get_default_graph()._mutation_lock(): # pylint:
disable=protected-access  
3003 original_body_result, exit_vars = self._BuildLoop(  
-> 3004 pred, body, original_loop_vars, loop_vars, shape_invariants)  
3005 finally:  
3006 self.Exit()
~/jonassucks3/lib/python3.6/site-
packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred,
body, original_loop_vars, loop_vars, shape_invariants)  
2937 flat_sequence=vars_for_body_with_tensor_arrays)  
2938 pre_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION) #
pylint: disable=protected-access  
-> 2939 body_result = body(*packed_vars_for_body)  
2940 post_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION) #
pylint: disable=protected-access  
2941 if not nest.is_sequence(body_result):
~/jonassucks3/lib/python3.6/site-
packages/tensorflow/python/ops/control_flow_ops.py in (i, lv)  
3258 cond = lambda i, lv: ( # pylint: disable=g-long-lambda  
3259 math_ops.logical_and(i  3260 body = lambda i, lv: (i + 1, orig_body(*lv))  
3261  
3262 if context.executing_eagerly():
~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in
_time_step(time, output_ta_t, state)  
838 skip_conditionals=True)  
839 else:  
\--> 840 (output, new_state) = call_cell()  
841  
842 # Keras cells always wrap state as list, even if it's a single tensor.
~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in ()  
824 if is_keras_rnn_cell and not nest.is_sequence(state):  
825 state = [state]  
\--> 826 call_cell = lambda: cell(input_t, state)  
827  
828 if sequence_length is not None:
~/jonassucks3/lib/python3.6/site-
packages/tensorflow/python/ops/rnn_cell_impl.py in **call** (self, inputs,
state, scope, *args, **kwargs)  
368 # method. See the class docstring for more details.  
369 return base_layer.Layer. **call** (self, inputs, state, scope=scope,  
\--> 370 *args, **kwargs)  
371  
372
~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/layers/base.py in
**call** (self, inputs, *args, **kwargs)  
372  
373 # Actually call layer  
\--> 374 outputs = super(Layer, self). **call** (inputs, *args, **kwargs)  
375  
376 if not context.executing_eagerly():
~/jonassucks3/lib/python3.6/site-
packages/tensorflow/python/keras/engine/base_layer.py in **call** (self,
inputs, *args, **kwargs)  
755 if not in_deferred_mode:  
756 self._in_call = True  
\--> 757 outputs = self.call(inputs, *args, **kwargs)  
758 self._in_call = False  
759 if outputs is None:
~/jonassucks3/lib/python3.6/site-
packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state)  
1003 sigmoid(i + self._w_i_diag * c_prev) * self._activation(j))  
1004 else:  
-> 1005 c = (sigmoid(f + self._forget_bias) * c_prev + sigmoid(i) *  
1006 self._activation(j))  
1007
TypeError: unsupported operand type(s) for +: 'Tensor' and 'float'