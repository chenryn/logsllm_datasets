winword
wmic
wmplayer
Average
Description
Windows File
System Tool
Windows Certiﬁcate
Services Tool
Windows Command
Line
Windows System
Script Interpreter
Component of
C++ Toolchain
Microsoft Excel
Firefox Browser
IE Browser
Java VM
Java Update Scheduler
Firefox Updater
Windows Installer
Microsoft Paint
Windows Text Editor
WinRAR Compression
Tool
Windows Service
Controller
Windows Spooler
Subsystem App
Windows Task
Management Tool
Windows Task Manager
Downloader
Microsoft Word
Windows Management
Instrumentation Command
Windows Media Player
-
Precision
Recall
F1-Score
0.958
0.964
0.956
0.959
0.965
0.961
0.958
0.960
0.957
0.957
0.959
0.960
0.96
0.963
0.953
0.952
0.955
0.962
0.960
0.952
0.960
0.952
0.959
0.959
1
1
0.999
0.999
1
1
0.965
0.968
0.992
0.990
1
0.983
0.990
0.984
1
1
1
0.970
1
1
0.976
0.998
0.996
0.991
0.978
0.981
0.977
0.978
0.982
0.980
0.961
0.963
0.974
0.974
0.979
0.971
0.975
0.973
0.976
0.975
0.977
0.966
0.979
0.975
0.967
0.974
0.977
0.974
Fig. 5: The graph-level detection accuracy of PROVDETECTOR
with different threshold values
The goal of having the blacklist approach is to answer the
question: is it possible to use hand-coded rules developed by
human experts to detect stealthy attacks. Ideally, relying on
human experts seems to be an effective approach which can
easily bring in with several working heuristics. One exemplary
rule can be “UI-heavy programs (e.g., MS Word and Excel)
should not launch external scripts, such as through CMD or
PowerShell”. However, in practice, since the adversary has
a lot of ways to run the malicious code, it is very difﬁcult
to come up with a comprehensive blacklist. For instance,
the UI-heavy processes could run the malicious code through
Java or hijack other processes (e.g., notepad.exe) instead
of using scripts. Using a blacklist approach could overlook a
9
Fig. 6: The detection accuracy of the whitelist approach with
different values.
large number of other attacks, especially unknown attacks. In
our experiment, we measure the effectiveness of applying the
“UI-heavy programs should not run external scripts” heuristic.
To do so, we use all the 8 UI-heavy programs (i.e., excel,
firefox, iexplore, mspaint, notepad, rar, winword and
wmplayer) in our evaluated 23 programs and check if they
calls cmd.exe, powershell.exe or other script interpreters.
We found that the recall of this heuristic is close to zero
(≤0.07), which means a large number of attacks were over-
looked by this approach.
The second strawman approach, the whitelist, is to evaluate
whether people can detect stealthy attacks by simply detecting
infrequent events. To construct the whitelist, we use a statistics-
based approach. For each event, if it exists in more than p
percent of the benign program instances, we add it to the
whitelist. In Figure 6, we show the detection accuracy of this
approach averaged by the 23 programs using different p values.
In our experiment, this approach achieves the best F1 score of
0.78 when p is 3%, which is still substantially lower than the
F1 score of PROVDETECTOR.
The third strawman approach is the anomaly score-based
approach. In §V-C, we deﬁne regularity score for a path
to select the top K rarest paths from a provenance graph.
One may consider that these regularity scores (or anomaly
scores) could be used to effectively detect stealthy malware
for simplicity. To address this concern, we evaluated a score-
based detection approach. For each program,
the anomaly
score based approach ﬁrst selects the top K rarest paths
from all the benign provenance graphs, then it chooses the
n-percentile of all the anomaly scores of the paths as the
threshold. During the detection stage, it identiﬁes any path that
has an anomaly score higher than the threshold as a malicious
path. In other words, if a path has an anomaly score higher than
n percent of the paths selected from benign provenance graphs,
this strawman approach identiﬁes the path as malicious. The
results of detection accuracy with different percentile values
are shown in Table III. The F1 score is even substantially
lower than the whitelist approach. One major reason for
such poor performance is that the rare paths selected from
benign provenance graphs could also have very high anomaly
scores. Therefore, the anomaly scores alone are not informative
enough to differentiate benign ones and malicious ones. The
results in Table III justify our choice of using a learning-
 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 2 4 6 8 10 12 14 16 18 20Precision or RecallThresholdPrecisionRecall 0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30Precision or RecallPercentagePrecisionRecallTABLE III: The detection accuracy of the anomaly score based
approach with different percentile values.
n-percentile
Precision
85
86
87
88
89
90
91
92
93
94
95
0.84
0.845
0.885
0.893
0.905
0.909
0.914
0.925
0.918
0.921
0.939
Recall
0.36
0.349
0.31
0.243
0.233
0.208
0.199
0.182
0.183
0.195
0.163
F1-Score
0.50
0.49
0.46
0.38
0.37
0.34
0.33
0.30
0.31
0.32
0.28
TABLE IV: Detection accuracy comparisons with path-level
and graph-level approaches.
Path or Graph
Path-level
Graph-level
Approach
PROVDETECTOR (path-level)
Path Nodes Averaging
PROVDETECTOR (graph-level)
graph2vec
Precision
0.959
0.961
0.957
0.899
Recall
0.991
0.890
1
0.452
F1-score
0.974
0.924
0.978
0.601
based approach that
learns both from rareness and causal
dependencies to automatically identiﬁes the proper boundary
between benign and anomalous paths for each program.
2) Comparison with Different Embedding Approaches:
We compare our embedding approach (§V-D) with a graph
embedding approach (graph2vec [79]), and the simple node-
level path embedding (Path Nodes Averaging). graph2vec is
an approach to learn distributed representations of graphs. With
graph2vec, we directly embed each provenance graph into
a feature vector. In the Path Nodes Averaging approach, we
still compute embeddings for the paths selected by PROVDE-
TECTOR. In contrast, we use word2vec to get the embedding
of each node,
then obtain the embedding for a path by
averaging the embeddings of all the nodes in the path. In the
evaluation of different embedding approaches, we follow the
same experiment protocol in §VI-A.
To compare our approach with graph2vec, we compute
graph-level detection accuracy of PROVDETECTOR using a
threshold of 3. The comparison results are shown in Table IV,
in which PROVDETECTOR has a substantially higher recall
than graph2vec. The graph2vec approach has reasonable
precision but has very poor recall (even worse than random
guess). This result conﬁrms our insight: the benign workloads
of a hijacked process may hide the malicious workload in the
graph level. It is thus necessary to use the path-level features.
We will further discuss this result in §VI-C.
We compare our embedding approach with Path Nodes Av-
eraging in path-level detection accuracy as shown in Table IV.
The Path Nodes Averaging approach achieves comparable
precision and recall with our approach as it also uses the paths
selected by PROVDETECTOR in the embedding. However, it
does not perform as good as our approach on recall as it does
not consider the order of nodes in a path.
C. Interpretation of Detection Results
In this section, we interpret the detection results presented
in §VI-B to justify our design decisions. In particular, we seek
answers for the following questions:
10
Fig. 7: The path selected by PROVDETECTOR from a realistic
attack example.
for stealthy malware detection?
• Why do simple models (e.g., blacklist or whitelist) fail?
• Why the whole provenance graph is not a good feature
• Why our path selection method can accelerate the training
• How robust is PROVDETECTOR against mimicry attacks?
For space reasons, we present other interpretations of the
detection results in Appendix C.
and detection?
1) Simple Models: To understand why simple models,
such as the black- and white-lists, that only consider one-
hop features are not effective, we use one realistic example