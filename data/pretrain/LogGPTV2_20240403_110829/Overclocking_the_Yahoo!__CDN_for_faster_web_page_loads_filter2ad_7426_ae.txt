2
2
3
2
4
2
5
2
6
2
7
2
8
2
9
2
0
3
1
3
2
3
3
3
4
3
5
3
6
3
7
3
8
3
9
3
0
4
1
4
2
4
3
4
4
4
5
4
6
4
7
4
8
4
9
4
0
5
1
5
2
5
3
5
4
5
5
5
6
5
7
5
8
5
9
5
0
6
1
6
2
6
3
6
4
6
5
6
6
6
7
6
8
6
9
6
0
7
1
7
2
7
3
7
4
7
5
7
6
7
7
7
8
7
9
7
0
8
1
8
2
8
3
8
4
8
5
8
6
8
7
8
8
8
9
8
0
9
1
9
2
9
3
9
4
9
5
9
6
9
7
9
8
9
9
9
0
0
1
Figure 18: The 80th percentile of object transfer times in RTTs per network preﬁx for the top 100 preﬁxes, for different ICW sizes.
HTTP First Object Transfer Time in RTTs from CDN−Singapore
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
HTTP Object Trasnfer Time in RTTs from CDN−Singapore
n
o
i
t
c
n
u
F
n
o
i
t
i
u
b
i
r
t
s
D
e
v
i
t
l
a
u
m
u
C
initcwnd 1
initcwnd 3
initcwnd 5
initcwnd 7
initcwnd 10
initcwnd 16
initcwnd 32
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
n
o
i
t
c
n
u
F
n
o
i
t
i
u
b
i
r
t
s
D
e
v
i
t
l
a
u
m
u
C
0
0
2
4
6
RTTs
8
10
0
0
2
initcwnd 1
initcwnd 3
initcwnd 5
initcwnd 7
initcwnd 10
initcwnd 16
initcwnd 32
4
6
RTTs
8
10
Figure 15: Overall object transfer time in RTTs.
Figure 16: First-object transfer time in RTTs.
trace can contain full page downloads. For this reason, this sec-
tion studies the overall page load time in a controlled environment
in the lab using real workloads and realistic network conditions.
These simulated network conditions (latency and packet loss rates)
are based on connections’ proﬁles from previous sections. These
network conditions represent links’ latecy and congestion due to
cross trafﬁc in the internet.
Experimental Setup
We captured a snapshot of the Yahoo! front page, by far the most
popular Yahoo! web page, and fairly representative of the measured
trafﬁc workload. We hosted this page and its resources locally such
that all object requests are directed to a local server.
In this setup, the client browser is 3.6.9 Firefox running on Mac
OS X 10.5.8. Apart from disabling caching, all other browser and
TCP settings are set to their default. Most notably, delayed ACKs
are enabled, and for Firefox, six maximum simultaneous connec-
tions per domain were used.
The server is a guest VM on the same machine, running CentOS
4.8 release with an updated 2.6.29.5 linux kernel and the apache
2.0.52 web server. We used the ipfw command to setup dummynet
pipes in order to control the perceived RTT and packet-loss rates
from and to the server. During the experiments, no hardware re-
source was saturated, which guaranteed that request latencies were
only due induced RTTs and packet loss. The web page had 30
objects distributed across two domains. Thus, we created two IP
aliases at the server representing the two domains.
We had the client repeatedly request the front page. The embed-
ded links , which represented the static content on the page, namely
images, Javascript, and stylesheets, were replaced to point to IP
aliases on the local server. Firefox fetched 30 objects distributed
across two domains via six concurrent connections per domain re-
sulted in an average of 2.5 objects per connection, very close to the
average number of objects per connection measured in live traces
shown in Figure 12.
We wanted to measure the total page load time, which we deﬁne
as the time difference between the ﬁrst packet of the ﬁrst HTTP
request and the ﬁnal ACK of the last object fetched (Page render-
ing time is negligible compared to the download time). We cap-
tured the tcpdump trace of the request at the client, and reported the
page load time between the ﬁrst outgoing request and the last ob-
ject ACK. For every parameter combination we reported, we gave
the geometric mean of ﬁve requests for the Yahoo! front page.
577n
o
i
t
c
n
u
F
n
o
i
t
u
b
i
r
t
s
D
e
v
i
t
a
u
m
u
C
l
i
1
0.95
0.9
0.85
0.8
0.75
0.7
0
Packet Retransmit Rate from CDN−Singapore to User (ABC = 0)
initcwnd 1
initcwnd 3
initcwnd 5
initcwnd 7
initcwnd 10
initcwnd 16
initcwnd 32
5
10
15
20
25
30
35
40
Packet Retransmit Rate (%)
Figure 17: Packet retransmit rate.
Results
Figure 19 shows the total page load time for different values of
RTT and different values of ICW sizes. We ﬁnd that the relative
reductions in page load times were relatively consistent; ranging
from 27%-38% when going from an ICW of 3 to 16.
When taking packet loss into account, we show in Figure 20 the
different page load times for different loss rates and different ICW
sizes and an RTT of 100ms (the median for Singapore). We ﬁnd
that page load times and their variance appear to be superlinear and
very sensitive to packet loss; increasing the loss rate from 5% to
10% increases page load time by 63% for an ICW of 3.
However, as seen in Figure 17, increasing the ICW can increase
packet loss, especially for users with congested links. This raises
important questions. First, can increasing the ICW hurt the overall
page load time for some connections? Second, if the answer to the
previous question is yes, is there a single optimum value for ICW
with respect to page load time that ﬁts all connections?
To study whether there are cases where increasing the ICW size
can hurt the overall web page load time, we need to consider users
that would suffer from increased packet loss due to increasing the
ICW. For such users, we need to estimate the increase in packet loss
due to the increase in ICW. Then, based on the estimated packet
loss and the corresponding ICW, we should measure the corre-
sponding page load time. To this end, we use a user connection
in the 90% percentile of retransmission rate in Figure 17 as an ex-
ample. We assume that the measured retransmission rate is equal to
packet loss rate. Moreover, we assume that increasing the ICW size
for this connection will follow the same increases in retransmission
rates observed for the 90th percentile connection in Figure 17. The
pairs of the ICW sizes and the corresponding packet loss rates are
listed in the ﬁrst two columns of Table 1. Finally, we assumed
that this connection has the median RTT for the Singapore site –
100ms. We measured the page load times using the experiment
described above for every ICW size and its corresponding packet
loss rate listed at Table 1 and recorded it in the table at the third
column. We note that for the 90th percentile connections (with re-
spect to packet retransmission rates), increasing the ICW can lead
to signiﬁcant increase of page load time. The same would apply to
other connections with higher percentile retransmission rates. We
see that for the connection in the 90th percentile, it ﬁrst beneﬁts
from increasing the ICW size up to seven, then by increasing the
ICW size more, the page load time starts increasing until it reaches
70% more than the minimum load time achieved at an ICW size of
7.
Conversely, looking at the right side of Table 1 (columns 4 and 5)
with zero packet loss rate (representing the 70th percentile retrans-
mission rate of all connections) we see the beneﬁts from increasing
the ICW size all the way to 32.
Consequently, one can conclude that no one ICW choice would
beneﬁt all users.
i
)
s
(
e
m
T
d
a
o
L
e
g
a
P
l
a
t
o
T
22
20
18
16
14
12
10
8
6
4
2
0
Page Load Time, no packet loss
initcwnd 1
initcwnd 3
initcwnd 8
initcwnd 16
50
100
200
RTT (ms)
800
1500
Figure 19: Page load time for different ICWs with no loss.
)
s
(
i
e
m
T
d
a
o
L
e
g
a
P
l
t
a
o
T
18
16
14
12
10
8
6
4
2
0
Page Load Time at RTT = 100ms
initcwnd 1
initcwnd 3
initcwnd 8
initcwnd 16
0
1
3
5
10
30
Loss Rate (%)
Figure 20: Page load time for different ICWs, packet loss rates,
and an RTT of 100ms (median for Singapore).
578loss (%) Time (s)
loss (%) Time (s)
d
n
w
c
t
i
n
i
1
3
5
7
10
16
32
17
18
18
20
22
25
25
8.56
8.06
7.29
6.32
6.92
10.22
10.87
0.00
0.00
0.00
0.00
0.00
0.00
0.00
2.36
1.87
1.72
1.56
1.50
1.32
0.96
Table 1: Yahoo! front page load times for an RTT of 100ms
(median for Singapore) with increasing ICWs and their corre-
sponding loss rates from Figure 17. Right columns show times
with no loss for comparison.
4.1.3 Impact on TCP Fairness
When making changes to TCP, we need to make sure that it re-
mains fair to other TCP ﬂows in the network. In this section, we
study the impact of increasing the ICW on TCP fairness.
Increasing TCP ICW size can be unfair to longer ﬂows, sharing
the same bottleneck with short ﬂows. If the network is congested
and experiencing signiﬁcant packet loss, increasing the ICW will
increase packet loss. This loss may cause the congestion window
to shrink by half each round trip until it may eventually reach one.
Moreover, the sender could even experience timeouts. Hence, for
a long lived ﬂow, the sender can end up sending at a rate lower
than one packet per round trip. Conversely, for a short ﬂow, e.g. a
web object download, packet loss will only extend the transmission
by a few round trips and the window size may not drop to one by
then. Hence, the average window size for the whole transfer can
be signiﬁcantly higher than that for a large transfer.
To demonstrate this point, we conducted the following experi-
ment using the setup described at Section 4.1.2. We conﬁgured
the connection between the host machine and the virtual machine
to have the proﬁle of connections in the 95th percentile from Fig-
ure 10, i.e. 100ms of RTT and 25% packet loss rate. Also, we
conﬁgured the ICW to be 32.
First, we ran iperf for 5 minutes between the two machines
representing a long ﬂow. This transfer achieved a bandwidth of
12.6KB/s – less than 1 segment per RTT (14.3KB/s). For the
second experiment, we downloaded a 48KB (32 segments) ﬁle off
the web server. The measured bandwidth for this second transfer
was 59.1KB/s. Note that as discussed in Section 3.2.3, although
48KB is the 95th percentile of downloaded web object sizes, it is
the 50th percentile with respect to objects contributing to overall
bytes downloaded from the Yahoo! CDN web servers.
Moreover, given that a recent study has shown that 52% of the
internet trafﬁc is web trafﬁc [8] and given that increasing the ICW
can increase packet loss and congestion for users with poor connec-
tions as shown in Fig. 17, we conclude that increasing the ICW can
be more unfair for longer ﬂows for users having poor connectivity.
Furthermore, one can conclude that increasing the ICW will be
unfair to other short ﬂows that remain using small ICW – e.g. the