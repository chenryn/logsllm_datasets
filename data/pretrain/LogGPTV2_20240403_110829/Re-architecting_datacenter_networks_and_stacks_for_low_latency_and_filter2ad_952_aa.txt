title:Re-architecting datacenter networks and stacks for low latency and
high performance
author:Mark Handley and
Costin Raiciu and
Alexandru Agache and
Andrei Voinescu and
Andrew W. Moore and
Gianni Antichi and
Marcin W&apos;ojcik
NDP: Rethinking Datacenter Networks and Stacks
Two Years After
Costin Raiciu
University Politehnica of Bucharest, Romania
PI:EMAIL
Gianni Antichi
Queen Mary University of London, UK
PI:EMAIL
This article is an editorial note submitted to CCR. It has NOT been peer reviewed.
The authors take full responsibility for this article’s technical content. Comments can be posted through CCR Online.
CCS CONCEPTS
• Networks → Network protocols; Data center networks;
KEYWORDS
Datacenters; Network Stacks; Transport Protocols
1 DATACENTER NETWORKING STATUS-QUO
The key goals of datacenter networks are to simultaneously provide
wire-level latency for RPC-style applications and high-throughput
for network-bound applications such as storage. Folded Clos net-
works [1, 11] are used in datacenters worldwide; such networks
use many cheap commodity switches to provide the illusion of a
big non-blocking switch to all hosts in a datacenter, offering many
paths between any two pairs of hosts.
Efficiently utilizing datacenter networks is challenging. The stan-
dard solution is to place each TCP connection onto a quasi-random
path chosen from the ones available by using Equal Cost Multipath
routing, but this results in flow collisions which can half through-
put for long running connections in the worst case [2]. A long line
of research addresses this problem of giving high throughput to
network-bound traffic: either by using software-defined network-
ing [2, 8], multipath transport [19], modifying switches to track
flows and reroute them [3], changing flow ID at the endhosts when
performance is poor [15], or, more radically load balancing every
packet independently [9].
Few, if any of, these works were adopted in practice, for multiple
reasons: a) the problem is less pressing when the host links are
slower than switch-to-switch links; b) network-bound applications
are rare and have less stringent constraints, and c) they only tackle
large flows and largely ignore the numerous short flows.
Reducing latency for short flows is another, largely parallel area
of research that was sparked by the discovery of TCP incast [6]:
time synchronization of many flows arriving at the same shallow
switch buffer leads to repeated timeouts at the endhosts, severely
inflating flow completion times. A good solution to incast is DCTCP
[4], a TCP congestion control algorithm which relies on aggressive
ECN marking at the switches and gently reduces its sending rate
on overload instead of halving it. This, coupled with shared switch
buffers and a low ECN marking threshold enable DCTCP to cope
well with incast and keep queue utilization low in steady state.
Short flows are affected by more pathologies than incast. In
particular, when competing with long flows that fill buffers, the
latency of short flows increases dramatically due to buffering even
when there are no losses. As TCP variants are allergic to loss, large
buffers are the norm, so this pathology is omnipresent.
To attack this problem, more radical approaches to ensuring
short flow latency beyond incast is to implement strict flow pri-
oritization in the network [5, 14, 20] or use a global scheduler to
schedule packets [18] or flowlets [17]. Host based approaches to
ensuring low latency include pHost [10] and Homa [16]. While we
only mentioned a few here, many other solutions were proposed;
unfortunately, few, if any, were deployed in production networks.
Discussions with engineers working in large datacenter net-
works point to the state of the art relying flow-based ECMP cou-
pled with prioritization in the switches for known latency sensitive
traffic (e.g. search), together with application-based solutions that
break large flows into chunks which are sent as individual TCP
connections. The only research outcome that has been adopted
widely is DCTCP, as it alleviates incast and is relatively easy to
deploy (simple switch configuration changes coupled with host
kernel patches for congestion control).
More recently, RDMA has been proposed and reportedly de-
ployed in production by Microsoft in its datacenters for storage
traffic. The key advantage of RDMA is offloading most of the trans-
port stack to the NIC, thus relieving CPU load. The downside is
the different API and its reliance on priority-flow control which
introduces many performance and availability issues [12].
2 NDP
NDP is a datacenter network architecture and stack that was awarded
best paper at Sigcomm 2017; it is also one of the first works that
explicitly aims at providing both low latency and high through-
put simultaneously. The key differentiator of NDP to prior work
is its “clean-slate” approach: assuming you could change both the
networks and the endpoints, what would the resulting datacenter
architecture be? We took this approach consciously, without wor-
rying about the deployability of NDP, aiming instead to ask what
are the right mechanisms to achieve both low latency and high
throughput at the same time.
NDP adopts the following three design decisions to provide both
high throughput and low latency:
• It runs aggressively small buffers to keep latency low, typ-
ically eight to ten packets per port. This means that short
flows cannot by delayed by more than said packets at any net-
work hop (assuming their packets arrive at the destination),
in comparison to hundreds of packets per port today.
ACM SIGCOMM Computer Communication Review
Volume 49 Issue 5, October 2019
112
• The sender starts at line rate, assuming the network core
can cope with the demand, to provide the lowest possible
latency for short and long flows, instead of doing slow start.
• Packets are scattered across all available paths instead of
per-flow ECMP.
Taken together, these mechanisms are a recipe for disaster: ag-
gressive starts, scattering and small buffers leads to frequent con-
gestion and packet drops, which makes the job hard for transport
protocols to discover which packets got lost and should be resent;
the default behavior for TCP in such cases is to fall-back to a re-
transmit timeout, which kills performance.
That is why NDP also relies on switch support. For each port,
NDP switches maintain two queues: a lower priority queue for data
packets and a higher priority queue for control packets. All data
packets go into the lower priority queue as long as it has space;
when it fills up, however, the incoming packet’s payload is removed
and the header is placed in the high priority queue.
Once a packet is trimmed to a header, it always will be forwarded
using the high priority queues, thus providing fast notification to
the receiver that the packet was lost. Upon receiving a header, the
receiver generates a NACK that is sent via high priority queues to
the sender, who can send the retransmission before the low priority
queue has a chance to drain.
In a 4-to-1 incast scenario, roughly three quarters of the packets
will be turned into headers in the first round trip time. If the receiver
allows the senders to send upon receiving a NACK, trimming will
continue for as long as the incast persists. To avoid this issue, NDP
decouples packet delivery notifications (ACKs and NACKs) from
packet clocking by having receivers send explicit PULL packets
which allow the sender to send one packet.
PULL packets allow the receiver to throttle the incoming traffic
after the first RTT to exactly the link rate. To this end, NDP receivers
maintain a PULL queue which is increased whenever a new packet
or header arrives, and drained at fixed rate (e.g. one pull packet
every 1.2us for 10Gbps links with 1500B packets).
The Sigcomm paper showed simulation and testbed deploy-
ment results that show NDP can achieve near-optimal short flow-
completion rates and more than 95% of the theoretical maximum
throughput for long flows [13].
2.1 Making NDP Real
There are two key challenges in transitioning NDP from a re-
search prototype into a deployable solution. First, switches must
be changed to support packet trimming. In principle, this change
is simple as the switches need not maintain any per-flow state,
and can operate on each packet independently. The rise of P4 and
programmable switches gave us hope that such functionality could
be readily supported, and our paper included a P4 implementation
of NDP support. However, at the time we wrote the paper Tofino
switches were not available for testing, and there was no guarantee
that our code could actually run on a Tofino target. That is why
we ran all our experiments on the NetFPGA implementation of the
NDP switch developed by Andrew Moore’s team at the University
of Cambridge.
The second challenge is implementing pull pacing which used by
the NDP endhost stack to ensure that packets arrive at the correct
rate. In short, pull pacing needs to place a PULL packet on the wire
every 1.2us (for 10Gbps links), 300ns for 40Gbps links and every
120ns for 100Gbps links assuming 1500B MTU.
The prototype implementation we used for our evaluation in the
Sigcomm paper (open source on GitHub) provided accurate timing
at 10Gbps by spinning a CPU core just for pull pacing; spinning is
needed because sleeps involve the OS scheduler and result in large