5.1 Natural Language Processing
We implement the Threatening Text Detector on top of OpenNLP, a generic,
extensible, multi-language NLP library. The sentence splitter and the stem-
mer [18] are language speciﬁc: Adding new languages simply requires training on
an arbitrary set of texts provided by the user. For example, we added Russian
by training it on a transcript of the XXVI Congress of the CPSU and Challenges
of Social Psychology 4 and a Wikipedia article about law5. In addition, Sect. 6.2
we show how to add new languages to the threatening text classiﬁer.
Our stop-words lists come from the Stop-words Project 6. The language iden-
tiﬁcation is performed with the Cybozu open-source library [19], released and
maintained since 2010.
4 http://www.voppsy.ru/issues/1981/816/816005.htm.
5 https://ru.wikipedia.org/wiki/.
6 https://code.google.com/p/stop-words/.
394
N. Andronio et al.
5.2 Text Classiﬁcation Thresholding
To determine whether the score m of a sentence with respect to the accusation
or money categories we proceed as follows. More formally, we want to determine
whether maccusation or mmoney exceed a threshold. In doing this, we account for
the contribution of all sentences (and not only the best scoring ones).
For example, consider the sentences: “To unlock the device you need” (m =
0.775), m = “to pay 1,000 rubles” (m = 0.632), and “Within 24 h we’ll unlock
your phone” (m = 0.612). The maximum score is 0.775, but since there are other
relevant sentences this value should be increased to take them into account. To
this end, we increase the score m as follows:
(cid:2)
(cid:3)
ˆm = m + (1 − m) ·
1 − e
− n(cid:2)
(s(ci)−t(ci))
i=1
where s(c)− t(c) is capped to zero, n is the number of sentences in that category
set, ci the i-th sentence in the stem vector c, and t : c (cid:5)→ [0, 1] is an adaptive
threshold function.
Let us pretend for a moment that t(c) is not adaptive, but set to 0.6. Then
the sum of s(c) − t(c) is 0.032 + 0.012 = 0.044. As you can see, ˆm is not very
diﬀerent from m because the scores of second and third sentence are just slightly
above their detection threshold.
Instead, the idea behind t(c) is that short sentences should have a higher
threshold, since it is easier to match a greater percentile of a short sentence;
instead, longer sentences should have a lower threshold, for the dual reason:
t(c) = τmax − γ(c) · (τmax − τmin), γ(c) =
(cid:2)
ci−σmin
ci∈c
σmax−σmin
with γ(c) capped in [0, 1]. The summation yields the number of 1 s in the stem
vector of sentence c. σmin and σmax are constants that represent the minimum
and maximum number of stems that we want to consider: sentences containing
less stems than σmin will have the highest threshold, while sentences containing
more stems than σmax will have the lowest threshold. Highest and lowest thresh-
old values are represented by τmin and τmax, which form a threshold bound.
These parameters can be set by ﬁrst calculating the score of all the sentences
in the training set. Then, the values are set such that the classiﬁer distinguishes
the ransomware in the training set from generic malware or goodware in the
training set. Following this simple, empirical procedure, we obtained: τmin =
0.35, τmax = 0.63, σmin = 3, and σmax = 6.
5.3 Dynamic Analysis
If no threatening text is found in statically allocated strings, we attempt a last-
resort analysis. In an emulator, we install, run and let the sample run for 5’.
After launching the app, our emulator follows an approach similar to the one
adopted by TraceDroid [20]: It generates events that simulate user interaction,
HELDROID: Dissecting and Detecting Mobile Ransomware
395
rebooting, in/out SMS or calls, etc. Aiming for comprehensive and precise user-
activity simulation and anti evasion is out from our scope. From our experience,
if the C&C server is active, in a few seconds the sniﬀer captures the data required
to extract the threatening text, which is displayed almost immediately.
From the decoded application-layer traﬃc (e.g., HTTP), HelDroid parses
printable strings. In addition to parsing plaintext protocols from network dumps,
every modern sandbox (including the one that we are using) allows to extract
strings passed as arguments to functions, which are another source of threat-
ening text. Although we do not implement OCR-text extraction in our current
version of HelDroid, we run a quick pilot study on the screenshots collected
by TraceDroid. Using the default conﬁguration of tesseract we were able to
extract all the sentences displayed on the screenshots.
5.4 Static Code Analysis
We extract part of the features for the Threatening Text Detector by parsing
the manifest and other conﬁguration ﬁles found in the APK once uncompressed
with akptool7. We compute the remaining ones by enumerating count, type or
size of ﬁles contained in the same application package.
However, the most interesting data requires an analysis of the app’s Dalvik
code in its Smali8 text representation generated by apktool. For the Lock-
ing Detector, instead of using SAAF [21], which we found unstable in multi-
threaded scenarios, we wrote a simple emulator that “runs” Smali code, tailored
for our needs. To keep it fast, we implemented the minimum subset of instruc-
tions required by our detector.
For the Encryption Detector we need precise ﬂows information across the
entire Smali instruction set. For this, we leveraged FlowDroid [22], a very robust,
context-, ﬂow-, ﬁeld-, object-sensitive and lifecycle-aware static taint-analysis
tool with great recall and precision. Source and sink APIs are conﬁgurable.
6 Experimental Validation
We tested HelDroid, running on server-grade hardware, against real-world
datasets to verify if it detected known and new ransomware variants and samples.
In summary, as discussed further in Sect. 8, it outperformed the state-of-the-art
research tool for Android malware detection.
6.1 Datasets
We used a diverse set of datasets (Table 2), available at http://ransom.mobi.
7 https://code.google.com/p/android-apktool/.
8 https://code.google.com/p/smali/.
396
N. Andronio et al.
Table 2. Summary of our datasets. VT 5+ indicates that samples that are marked
according to VirusTotal’s positive results. VT top 400 are on Dec 24th, 2014.
Name Size
Labelling
Apriori content
Use
AR
AT
MG 1,260
R1
R2
M1
207
443
400
172,174 VT 5+
12,842 VT 5+
Implicit
VT 5+
VT 5+
VT top 400 100 % malware
55.3 % malware + 44.7 % goodware FP eval.
68.2 % malware + 31.8 % goodware FP eval.
FP eval.
100 % malware
NLP training
100 % ransomware + scareware
100 % ransomware + scareware
Detection
FP eval.
Goodware and Generic Malware. We obtained access to the AndRadar
(AR) [23] dataset, containing apps from independent markets (Blackmart,
Opera, Camangi, PandaApp, Slideme, and GetJar) between Feb 2011 and Oct
2013. Moreover, we used the public AndroTotal (AT) API [24] to fetch the apps
submitted in Jun 2014–Dec 2014. Also, we used the MalGenome (MG) [25]
dataset, which contains malware appeared in Aug 2010–Oct 2011.
We labeled each sample using VirusTotal, ﬂagging as malware those with
5+/56 positives. The AR and AT datasets do not contain any ransomware
samples. The MG dataset contains only malware (not ransomware).
Last, the Malware 1 (M1) dataset contains the top 400 malicious Android
applications as of Dec 2014, excluding those already present in the rest of our
datasets and any known ransomware.
Known Ransomware (sentences for Text-Classiﬁer Training). We need
a small portion of sentences obtained from true ransomware samples. During the
early stages of a malware campaign, samples are not always readily available for
analysis or training. Interestingly, our text-classiﬁer can be trained regardless
of the availability of the sample: All it needs is the threatening text, which is
usually easy to obtain (e.g., from early reports from victims).
We built the Ransomware 1 (R1) dataset through the VirusTotal Intelli-
gence API by searching for positive (5+) Android samples labeled or tagged as
ransomware, koler, locker, fbilocker, scarepackage, and similar, in Sep–Nov 2014.
We manually veriﬁed that at least 5 distinct AV programs agreed on the same
labels in R1 (allowing slight lexical variations). In this way, we excluded outliers
caused by naming inconsistencies, and could be reasonably safe that the resulting
207 samples were true ransomware. The training is performed only once, oﬄine,
but can be repeated over time as needed. We manually labeled sentences (e.g.,
threat, porn, copyright) from the R1 dataset, totaling 51 English sentences and
31 Russian sentences.
Unknown Ransomware. Similarly, we built the Ransomware 2 (R2) dataset
for samples appeared in Dec 2014–Jan 2015. This dataset is to evaluate HelDroid
on an arbitrary, never-seen-before, dataset comprising ransomware — and possibly
HELDROID: Dissecting and Detecting Mobile Ransomware
397
other categories of malware. Aposteriori, we discovered that this datasets contains
interesting corner-case apps that resemble some of the typical ransomware features
(e.g., screen locking, adult apps repackaged with disarmed ransomware payload),
making this a particularly challenging test case.
6.2 Experiment 1: Detection Capability
HelDroid detected all of the 207 ransomware samples in R1: 194 with static
text extraction, and the remaining 13 by extracting the text in live-captured web
responses from the C&C server. However, this was expected, since we used R1
for training. Thus, this experiment showed only the correctness of the approach.
We tested the true predictive capabilities of HelDroid on R2, which is dis-
joint from R1. Among the 443 total samples in R2, 375 were correctly detected
as ransomware or scareware, and 49 were correctly ﬂagged as neither. Precisely,
the following ones were actually true negatives:
– 14 Badoink + 15 PornDroid clones (see below);
– 6 lock-screen applications to modify the system’s look &feel;
– 14 benign, adware, spyware, or other non-ransomware threats.
Badoink and PornDroid are benign applications sometimes used as hosts of ran-
somware payload. HelDroid correctly only ﬂagged the locking behavior. We
installed and used such samples on a real device and veriﬁed that they were not
performing any malicious operation apart from locking the device screen (behav-
ior that was correctly detected). An analysis of network traﬃc revealed that the
remote endpoint of all web requests issued during execution was unreachable,
resulting in the application being unable to display the threatening web page.
The last 19 samples are known to AV companies as ransomware, but:
– 11 samples use languages on which HelDroid was not trained (see below).
– 4 samples contain no static or dynamically generated text, thus they were
disarmed, bogus or simply incorrectly ﬂagged by the commercial AVs.
– 4 failed downloading their threatening text because the C&C server was down.
Strictly speaking, these samples can be safely considered as being disarmed.
Manual analysis revealed that these samples belong to an unknown family
(probably based on repackaged PornDroid versions).
False Negative Analysis. We focused on the samples that were not detected
because of the missing language models. As a proof of concept we trained
HelDroid on Spanish, by translating known threatening text from English to
Spanish using Google Translator, adding known non-threatening Spanish text,
and running the training procedure. The whole process took less than 30 min.
After this, all previously undetected samples localized in Spanish were success-
fully ﬂagged as ransomware.
398
N. Andronio et al.
14
12
10
)
s
(
e
m
i
t
n
o
i
t
c
e
e
d
t
k
c
o
L
8
6
4
2
0
0B -621KB
621KB -2MB
2MB -5MB
5MB -10MB 10MB -61MB
Total size of smali classes (B)
)
s
(
e
m
i
t
n
o
i
t
c
e
t
e
d
t
x
e
T
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0
0B -621KB
621KB -2MB
2MB -5MB
5MB -10MB
10MB -61MB
Total size of smali classes (B)
Fig. 2. Lock-detection (left) and text-classiﬁcation (right) time as function of Smali
class size (whiskers at the 9th and 91st percentiles).
6.3 Experiment 2: False Positive Evaluation