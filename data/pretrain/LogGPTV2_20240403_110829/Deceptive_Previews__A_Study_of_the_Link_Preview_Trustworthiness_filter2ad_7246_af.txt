gram, and WhatsApp (both on Android and iOS) do not
show any warning when loading our phishing URLs. Also,
we veriﬁed that external browser apps might not reliably
show Safe Browsing warnings. We reproduced such behavior
on Chrome Browser 76.0.3809.123 for iOS 12.4.1, Chrome
for Android (Android 9,Pixel Build/PQ3A.190801.002 and
Pixel 2 Build/PQ3A.190801.002), Safari
(12.1.2 Mobile),
Brave Browser
for Android (1.3.2 based on Chromium
76.0.3809.132), and Firefox Focus for Android (8.0.16). Only
one mobile browser, i.e., Firefox for Android (68.1), showed
the warning correctly. We point out that we used the default
conﬁguration of both all tested apps and the operating systems.
Finally, desktop browsers were more consistent than the mobile
ones in showing the warning. Here, we tested Chrome Browser
(77.0.3865.75 for Ubuntu 18.04), Brave Software Browser
(0.68.132 based on Chromium 76.0.3809.132 for Ubuntu
18.04), and Firefox (69.0 for Ubuntu 18.04). Independent non-
academic research conﬁrmed the presence of a discrepancy
between Google Safe Browsing mobile and desktop. See, for
example, [26], [19].
The reasons for such a discrepancy are not fully under-
stood, and further research is required. Nevertheless, such
results indicate that browsers may fail to or will not detect ma-
licious URLs, and, accordingly, browser-side countermeasures
should not be considered as a bulletproof last line of defense.
Based on that, we recommend developers to implement up-
stream URL validation during the generation of link previews.
Among the 20 platforms we veriﬁed, only two implement such
a mechanism.
(R7) Do Proper URL Validation: An HTTP agent can reach
web resources by following chains of redirections. While in the
past redirections were only implemented via HTTP response
codes and the refresh HTML meta tag, nowadays redirections
are also implemented via JavaScript code. When validating
URLs, it is fundamental that all URLs of a redirection chain
are validated as well. Unfortunately, the only two platforms
implementing a form of URL validation (Twitter and LinkedIn)
did not validate URLs during redirections, allowing attackers
12
to bypass their countermeasures. Table VI sums up the results
of our experiments with these two social networks.
D. Ethical Considerations
Our experiments raise the valid ethical concern of sharing
malicious content on social media platforms. For example,
users not aware of our experiments may click on our previews
and become victim of an attack. To avoid attacking users,
we limited the visibility of the shared malicious links of the
platform accounts we control. When the platform did not
support limiting the post visibility, i.e. for the social networks
Medium and Plurk, we did not share the phishing link and,
instead of distributing the Win32.Virut malware, we used the
innocuous EICAR test ﬁle, used to test antivirus software.
The second concern of our experiments is sharing malware
from our servers. The main risk of these experiments is
that both the network and the domain name of our institute
may be blacklisted, affecting the work of the research and
administration staff. To avoid such a risk, we registered a ﬁrst-
level domain name and moved our servers on Amazon Web
Service EC2.
VII. RELATED WORK
In this section, we review works related to our study.
First, we present relevant works in the area of the analysis
of malicious URLs in social networks. Then, we related our
work with the research done in the area of phishing.
A. Analysis of Clicks on Social Platforms
When deciding whether to click on link previews, users
rely on an ensemble of signals that are displayed by the social
platform’s web pages. For example, Redmiles et al. [27] show
that users take into account who shares the web content and
the online community the content originates from. Similarly to
Redmiles et al. [27], our work intends to shed some light on
the dynamics behind user clicks on social networks. However,
as opposed to Redmiles et al. [27], our work does not study
social connections between users or user properties such as
demographics. Instead, our work focuses on the content of a
link preview, the trustworthiness of the link preview creation,
and it explores the extent to which an attacker can control the
ﬁelds displayed to the victims.
Clicking on maliciously-crafted link previews is a security
concern that Facebook tackled in 2017 by forbidding users
to modify link previews from the web site [28]. Also, an
independent work by Barak Tawily [33] showed that Facebook
link previews can be modiﬁed via metatags. Our study expands
the one by Tawily [33] and shows that motivated attackers
can still control
the content of a preview by crafting ad-
hoc HTML tags of the shared pages. Also, our study shows
that the problem is not affecting only Facebook, but it is a
systematic problem affecting most of the social platforms that
we evaluated.
B. Phishing in Social Networks
A typical phishing attack involves an attacker, their victim,
and a malicious resource used as a bait, to convince the user
to provide sensitive information. To this end, attackers usually
impersonate existing institutions or services (e.g. banks) to dif-
ferent degrees of similarity: replicating the impersonated target
to a high degree increases their chances of success, e.g. through
the choice of a visually-similar domain, or through reusing
graphics and logos. With the increase in popularity experienced
by social media platforms, attackers found means to either
directly reach targeted victims, also having the possibility to
collect their data and increase the success likelihood, or to get
in touch with large crowds in much broader campaigns. For
example, Han et al. [16] mention Facebook among the top-ﬁve
organizations targeted by phishers, also showing how attackers
install off-the-shelf phishing kits in compromised web servers,
where the attack is active for a short
time before being
moved to another location. Phishing attacks usually employ a
considerable number of redirections, to avoid detection, evade
blacklists and ﬁlter trafﬁc. Previous work [29], [31] studied
redirection chains for malicious pages detection, also applied
in the context of social networks (i.e., Twitter). Detection of
phishing pages can also be done by inspecting the content
and structure of a webpage (e.g., Pan et al. [24]) or the URL
structure (e.g., Chou et al. [7]).
As opposed to this body of works, our study does not
present new detection techniques for phishing pages. How-
ever, similarly to phishing pages, an attacker can create link
previews that are visually similar to benign ones, masking thus
the malicious intention of the landing page.
C. Detection of Malicious Content
As social networks gained popularity, attackers started
using them as a vector to spread malicious URLs, beyond
phishing attacks such as drive-by download. The detection of
these URLs has been the focus of several works. For example,
Lee et al. [29] proposed a technique to detect malicious URLs
based on the chains of redirections. Similarly, Thomas et
al. [34] presented a technique to evaluate URLs shared not
only on social networks but also on other web services such
as blogs and webmails. In another line of work, the detection of
malicious pages focused on inspecting their content, for both
desktop browsers (e.g., Canali et al. [6]) and mobile browsers
(e.g., Amrutkar et al. [1]). As opposed to these works, our
paper does not present a detection technique, but it studies how
social platforms behave when preparing previews of malicious
URLs.
Finally, in a recent work, Bell et al. [4] measured the
reactivity of the malicious URL detection system of Twitter,
discovering that a signiﬁcant number of malicious URLs re-
main undetected for at least 20 days. Such a study is orthogonal
to the one present in our work, i.e., our work explores the ways
social platforms generate previews in an adversarial setting,
whereas Bell et al. [4] perform measurements on the reactivity
of countermeasures. Also, to a certain extent, Bell et al. [4]
underline the severity of the current state of link previews in
social platforms too.
D. Cloaking Attacks
Another area related to our work is the area of cloaking
attacks. In a cloaking attack, the attacker signiﬁcantly alters the
web page content when visited by a crawler or bot to conceal
the malicious purpose of the page [40]. When compared to our
13
work, attackers could use cloaking attacks to generate decep-
tive link previews, where the page content is changed to look
benign only when visited by social platforms’ bots. However,
cloaking attacks can be detected, and over the past years, the
research community has proposed several ideas. For example,
Wang et al. [39] show four techniques to detect user agent
and IP cloaking put in place by web sites to deceive search
engine crawlers. Similarly, Invernizzi et al. [18] used ready-to-
use cloaking programs retrieved from the underground market
to create a classiﬁer for the detection. Social platforms could
use these techniques to detect cloaking attacks; however, it is
important to point out that it will not be sufﬁcient to prevent
the creation of deceptive previews. As we showed in our
study, complying to our recommendations is hard in practice,
and attackers can exploit a variety of implementation pitfalls
(see, Section VI) to craft malicious previews and distribute
unwanted content over social platforms.
VIII. CONCLUSION
In this paper, we presented a comprehensive analysis of
link previews on social media platforms. First, we explored
different ways in which their content is speciﬁed and how
most of the platforms studied have a different rendering format
for the same meta tags. We highlighted how this variability
can cause the user not to understand which preview ﬁelds
leading them to uninformed security
are security critical,
decisions. Then, we showed that
is possible to misuse
the preview-rendering service, as this relies entirely on the
content of the meta tags without inspecting the web page any
further: in four social media platforms, we were able to craft
benign-looking link previews leading to potentially malicious
webpages. Crafting a benign-looking preview for the remaining
16 social media platform requires only the ability to register
a new domain.
it
Next, we observed the presence of any active or passive
countermeasures employed by social media platforms against
the spread of known malicious URLs and software, and found
that only two over 20 platforms perform active checks on the
shared URL, and that even in these two cases, cross-checks
can be bypassed through client- and server-side redirections.
On this matter, we reported possible inconsistencies with
the safe browsing services on mobile phones, supporting our
recommendation on upstream checks, performed directly by
the social media platforms. We concluded our work with a
discussion, analyzing the impact of misleading previews on
users’ behavior, evaluating the resulting security risks, and
suggesting seven recommendations for possible improvements.
ACKNOWLEDGMENTS
We would like to thank the anonymous reviewers, Katha-
rina Krombholz, and Sebastian Becking for their valuable
feedback. Also we would like to thank Nick Nikiforakis,
who shepherded this paper. This work was partially supported
by the German Federal Ministry of Education and Research
(BMBF) through funding for the CISPA-Stanford Center for
Cybersecurity (FKZ: 13N1S0762).
REFERENCES
[1] C. Amrutkar, Y. S. Kim, and P. Traynor, “Detecting mobile malicious
time,” IEEE Transactions on Mobile Computing,
webpages in real
vol. 16, no. 8, pp. 2184–2197, 2016.
[2] M. Armstrong,
“Referral
2017.
referral-trafﬁc---google-or-facebook/
[Online]. Available:
trafﬁc
-
facebook?”
https://www.statista.com/chart/9555/
google
or
[3] Ars Technica, “Armed with ios 0days, hackers
indiscriminately
[Online].
https://arstechnica.com/information-technology/2019/08/
infected
Available:
armed-with-ios-0days-hackers-indiscriminately-infected-iphones-for-two-years/
iphones
years,”
2019.
two
for
[4] S. Bell, K. Paterson, and L. Cavallaro, “Catch me (on time) if you can:
Understanding the effectiveness of twitter url blacklists,” arXiv preprint
arXiv:1912.02520, 2019.
[5] D. Canali and D. Balzarotti, “Behind the scenes of online attacks: an
analysis of exploitation behaviors on the web,” 2013.
[6] D. Canali, M. Cova, G. Vigna, and C. Kruegel, “Prophiler: a fast ﬁlter
for the large-scale detection of malicious web pages,” in Proceedings
of the 20th international conference on World wide web. ACM, 2011,
pp. 197–206.
[7] N. Chou, R. Ledesma, Y. Teraguchi, and J. C. Mitchell, “Client-
side defense against web-based identity theft,” in Proceedings of
the Network and Distributed System Security Symposium, NDSS
2004, San Diego, California, USA, 2004. [Online]. Available: http:
//www.isoc.org/isoc/conferences/ndss/04/proceedings/Papers/Chou.pdf
J. S. Downs, M. Holbrook, and L. F. Cranor, “Behavioral response to
phishing risk,” in Proceedings of the anti-phishing working groups 2nd
annual eCrime researchers summit. ACM, 2007, pp. 37–44.
[8]
[9] M. Egele, G. Stringhini, C. Kruegel, and G. Vigna, “Towards detecting
compromised accounts on social networks,” IEEE Transactions on
Dependable and Secure Computing, vol. 14, no. 4, pp. 447–460, 2015.
[10] Facebook Inc., “I got a message from facebook saying a ﬁle i tried
to share has a virus.” [Online]. Available: https://www.facebook.com/
help/223268604538225
[11] ——, “The open graph protocol.” [Online]. Available: https://ogp.me/
[12] ——, “What is facebook doing to protect me from spam?” [Online].
Available: https://www.facebook.com/help/637109102992723
[13] S. Garera, N. Provos, M. Chew, and A. D. Rubin, “A framework for
detection and measurement of phishing attacks,” in Proceedings of the
2007 ACM workshop on Recurring malcode. ACM, 2007, pp. 1–8.
[14] Google Inc., “Google safe browsing.” [Online]. Available: https:
//safebrowsing.google.com/
[15] S. Gupta, A. Khattar, A. Gogia, P. Kumaraguru, and T. Chakraborty,
“Collective classiﬁcation of spam campaigners on twitter: A hierarchical
meta-path based approach,” in Proceedings of the 2018 World Wide
Web Conference.
International World Wide Web Conferences Steering
Committee, 2018, pp. 529–538.
[16] X. Han, N. Kheir, and D. Balzarotti, “Phisheye: Live monitoring of
sandboxed phishing kits,” in Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security. ACM, 2016,
pp. 1402–1413.
J. Hong, “The current state of phishing attacks,” 2012.
[17]
[18] L. Invernizzi, K. Thomas, A. Kapravelos, O. Comanescu, J. Picod, and
E. Bursztein, “Cloak of visibility: Detecting when machines browse a
different web,” in 2016 IEEE Symposium on Security and Privacy (SP),
2016.
[19] K. Johnson, “Google safe browsing can differ between desktop and
mobile. why?” 2019. [Online]. Available: https://www.wandera.com/
mobile-security/google-safe-browsing/
all,” in 2011 Proceedings IEEE INFOCOM.
[20] A. Le, A. Markopoulou, and M. Faloutsos, “Phishdef: Url names say it
IEEE, 2011, pp. 191–195.
[21] C. Ludl, S. McAllister, E. Kirda, and C. Kruegel, “On the effectiveness
of techniques to detect phishing sites,” in International Conference on
Detection of Intrusions and Malware, and Vulnerability Assessment.
Springer, 2007, pp. 20–39.
[22] A. Oest, Y. Safaei, A. Doup´e, G.-J. Ahn, B. Wardman, and K. Tyers,
“Phishfarm: A scalable framework for measuring the effectiveness of
evasion techniques against browser phishing blacklists,” in PhishFarm:
A Scalable Framework for Measuring the Effectiveness of Evasion
Techniques against Browser Phishing Blacklists.
IEEE, 2019, p. 0.
[23] Open DNS, “PhishTank.” [Online]. Available: https://www.phishtank.
com/
14
2010. [Online]. Available: http://doi.acm.org/10.1145/1920261.1920263
[33] B. Tawily, “Can you trust facebook links?” 2017. [Online]. Available:
https://quitten.github.io/Facebook/
[34] K. Thomas, C. Grier, J. Ma, V. Paxson, and D. Song, “Design and
evaluation of a real-time url spam ﬁltering service,” in Proceedings of
the 2011 IEEE Symposium on Security and Privacy, ser. SP ’11, 2011.
[Online]. Available: https://doi.org/10.1109/SP.2011.25
[35] K. Thomas, D. McCoy, C. Grier, A. Kolcz, and V. Paxson, “Trafﬁcking
fraudulent accounts: The role of the underground market in twitter
spam and abuse,” in Presented as part of the 22nd {USENIX} Security
Symposium ({USENIX} Security 13), 2013, pp. 195–210.
[36] Twitter Inc., “About unsafe links.” [Online]. Available: https://help.
twitter.com/en/safety-and-security/phishing-spam-and-malware-links
[37] ——,
with
“Optimize
Avail-
able: https://developer.twitter.com/en/docs/tweets/optimize-with-cards/
overview/abouts-cards
[Online].
cards.”
twitter
[38] A. Vishwanath, T. Herath, R. Chen, J. Wang, and H. R. Rao, “Why do
people get phished? testing individual differences in phishing vulner-
ability within an integrated, information processing model,” Decision
Support Systems, vol. 51, no. 3, pp. 576–586, 2011.
[39] Wang, David Y. and Savage, Stefan and Voelker, Geoffrey M., “Cloak
and dagger: Dynamics of web search cloaking,” in Proceedings of the
18th ACM Conference on Computer and Communications Security,
ser. CCS ’11, 2011. [Online]. Available: http://doi.acm.org/10.1145/
2046707.2046763
[40] B. Wu and B. D. Davison, “Detecting semantic cloaking on
the 15th International Conference
[Online]. Available:
the web,” in Proceedings of
on World Wide Web, ser. WWW 06, 2006.
https://doi.org/10.1145/1135777.1135901
[24] Y. Pan and X. Ding, “Anomaly based web phishing page detection,”
in 2006 22nd Annual Computer Security Applications Conference
(ACSAC’06), 2006.
[25] G. Pellegrino, O. Catakoglu, D. Balzarotti, and C. Rossow, “Uses and
Abuses of Server-Side Requests,” in Proceedings of the 19th Inter-
national Symposium on Research in Attacks, Intrusions and Defenses,
September 2016.
[26] L. L. Porta,
mobile,” 2019.
opinion/Google-Safe-Browsing-differs-between-desktop-and-mobile
falling short on
“Googles
[Online]. Available: https://www.brianmadden.com/
security efforts
are
[27] E. M. Redmiles, N. Chachra, and B. Waismeyer, “Examining the
demand for spam: Who clicks?” in Proceedings of
the 2018 CHI
Conference on Human Factors in Computing Systems, ser. CHI ’18,
2018. [Online]. Available: http://doi.acm.org/10.1145/3173574.3173786
[On-
line]. Available: https://developers.facebook.com/blog/post/2017/06/27/
API-Change-Log-Modifying-Link-Previews
“Modifying
Robertson,
previews,”
[28] M.
2017.
link
[29] Sangho Lee and Jong Kim, “Warningbird: Detecting suspicious urls in
twitter stream,” in NDSS, 2012.
[30] S. Sheng, B. Wardman, G. Warner, L. F. Cranor, J. Hong, and C. Zhang,
“An empirical analysis of phishing blacklists,” in Sixth Conference on
Email and Anti-Spam (CEAS). California, USA, 2009.
[31] G. Stringhini, C. Kruegel, and G. Vigna, “Shady paths: Leveraging
surﬁng crowds to detect malicious web pages,” in Proceedings
of
the 2013 ACM SIGSAC Conference on Computer &#38;
Communications Security, ser. CCS ’13, 2013. [Online]. Available:
http://doi.acm.org/10.1145/2508859.2516682
[32] Stringhini, Gianluca and Kruegel, Christopher and Vigna, Giovanni,
“Detecting spammers on social networks,” in Proceedings of the 26th
Annual Computer Security Applications Conference, ser. ACSAC ’10,
15