may indicate delegation switches. More details can be
found in Section 3.3.
□ Example. Consider the Firefox execution model. The
user can annotate a tab, a window, and/or an iframe as
a top level unit. Internally, these are all represented by
the same nsPIDOMWindow class. They are differentiated
by the internal field values. Hence, we provide multiple
perspectives by annotating the nsPIDOMWindow data
structure and using different expressions in the identifier
annotations to distinguish the perspectives. Figure 10
USENIX Association
26th USENIX Security Symposium    1117
……..Main ThreadDNS ThreadTab1: DNS(http://a.com)Tab2: DNS(http://b.com)Tab1: load(ImageA)...Tab1: JS snippetTab1: AsmJS-1……DNS(http://a.com)DNS(https://b.com)ImgDecodeThread……Decode(ImageA)Socket Thread……get(a.com)get(b.com)JS HelperThread……DO(AsmJS-1)Tab1: Fetch(ImgA, JS, CSS) Tab2: Fetch(ImgB, JS, CSS).........1234567...Tab1: CSS Animation CA1CompositorThread……Draw(CA1)81 @identifier=this->GetOuterWindow(2)->mWindowID, indicator=1
2 @identifier=this->GetTop()->mWindowID, indicator=2
3 class nsPIDOMWindow {
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19 };
@indicator=1
@indicator=2
nsCOMPtr mDoc;
// Tracks activation state
bool mIsActive;
virtual already_AddRefed GetTop() = 0;
nsPIDOMWindow *GetOuterWindow()
{ return mIsInnerWindow ? mOuterWindow.get() ? this; }
// The references between inner and outer windows
nsPIDOMWindow
nsPIDOMWindow
// A unique (64-bit counter)
// id for this window.
uint64_t mWindowID;
/* other methods and data fields */
*mInnerWindow;
*mOuterWindow;
Figure 10: Tab and window annotations in Firefox
shows the annotations for tabs and windows. The indica-
tor id 1 is for tabs and 2 for windows. Any tab or window
changes must entail the change of the mDoc field, which
is used as the indicator. The expressions in the correspond-
ing identifier annotations mean that we can acquire the
tab of any given window by getting the second layer outer
window, and the top level window by calling GetTop().
Figure 11: Firefox main thread posts events to the socket thread
The connection request data structure (in the Socket-
Thread), the image data structure (in the image decoder
thread), etc. are annotated as delegators. As such, when a
connection request is created in the main thread, the re-
quest inherits the current tab/window id. When the request
is used/handled in a SocketThread, the execution duration
corresponding to the request belongs to the owner tab/win-
dow of the request. An example is shown in Figure 11.
In Firefox, all delegator data structure classes have the
same base class nsRunnable. As such, we only need to
annotate nsRunnable as the delegator class (box A). When
the main thread tries to load a new URI (step 1), it posts
an nsConnEvent to the SocketThread (step 2) by calling
the PostEvent method (box C). Since nsConnEvent is a
sub-class of nsRunnable (box B), the delegator class, the
newly created nsConnEvent inherits the tab/window id.
The nsRunnable class provides a function Run(), which is
implemented by its child classes to perform specific tasks.
And each thread maintains its own work queue containing
all such class instances. Thus the size of the worker queue
is annotated as the indicator of the delegator. Whenever it
changes, there may be a unit context switch. □
Annotation Miner. We develop an annotation miner to
recommend unit and delegator data structures to annotate.
The miner works as follows. The user provides a pair
of executions to denote an intended unit task, one exe-
cution containing one unit and the other containing two
units. Then, differential trace analysis is performed to
prune data structures that are common in both traces and
hence irrelevant to the unit (e.g., global data structures).
The miner leverages the points-to relations between data
structures to narrow down to the top data structures (i.e.,
those that are not pointed-to by other data structures).
PageRank is further used to determine the significance
of individual top data structures. A ranked list of data
structures is returned to the user. Note that this mining
stage is much less demanding than the training process
in BEEP/ProTracer, which requires extracting code loca-
tions that induce low level memory dependencies. Since
we focus on identifying high level data structures, which
are covered by the provided inputs, completeness is not
an issue for us in practice.
Figure 12: Annotation Miner
Next, we show how to mine the tab data structure in
Firefox ( Figure 12). We first use a pair of runs to visit the
Google main page. T1 has one tab and T1’ has two tabs.
∆T shows the data structures in the trace differences. Note
that there are data structures specific to the page content
but irrelevant to the intended unit, such as SocketIO. To
further prune those, we use another two pairs of execu-
tions that visit Google Drive and a local file, respectively.
The miner then takes the intersection of the trace differ-
ences to prune out SocketIO and DiskIO. The resulting
set contains the top level data structures and their sup-
porting meta data structures (e.g., the ScrollPos data
structure to support scrolling in a tab). The trace-based
points-to analysis then filters out the low level support-
ing data structures. There may be multiple top level data
structures remained, many not related to units (e.g., for
logging). Hence in the last step, PageRank is used to rank
the several top data structures. In our case, the tab data
structure is correctly ranked the top.
3.3 Runtime
Unit Context. At runtime, each thread maintains a vector
called the unit context. Each element of the vector denotes
1118    26th USENIX Security Symposium
USENIX Association
Main ThreadSocket ThreadnsDocShell::LoadURI(string)nsHttpConnectionMgr::PostEventnsresult nsHttpConnectionMgr::PostEvent(...) {   …  nsCOMPtr event =    new nsConnEvent(this, handler, iparam, vparam);  rv = mSocketThreadTarget->Dispatch(event,                                NS_DISPATCH_NORMAL);    …}class nsConnEvent:  public nsRunnable  {};     ProcessNextEventBC12@delegatorclass nsRunnable {};     PI:EMAIL;D3Test 1: GoogleTest 2: GDriveTest 3: LocalFileTATB=2TAΔT: { e | TB.numberOf(e) = 2*TA.numberOf(e) }T1T1’ΔT1: { SocketIO, Tabs, ScrollPos, LogItem… }T2T2’ΔT2: { SocketIO, DiskIO, Tabs, ScrollPos, LogItem… }T3T3’ΔT3: { DiskIO, Tabs, ScrollPos, LogItem… }Intersection(ΔT){ Tabs,ScrollPos,LogItem,… }the current unit instance for each unit type (or each per-
spective). Note that MPI allows partitioning an execution
in different ways by annotating multiple unit data struc-
tures. If the user has annotated n unit data structures (with
n indicators and n identifiers), there are n elements in the
vector. Each time the indicator of a unit data structure is
updated, the identifier of the data structure is copied to
the corresponding vector element.
Delegation. MPI runtime provides a global hash map
that is shared across all threads, called the delegation
table. The delegation table projects a delegator data struc-
ture instance to a unit context vector value, denoting the
membership of the delegator. Upon the creation/initial-
ization of a delegator data structure instance, MPI inserts
a key-value pair into the delegation table associating the
delegator to the current unit context. Upon an update of
the indicator of a delegator data structure (in a worker
thread that handles the subtask represented by the dele-
gator), the unit context of the current thread is set to the
unit context of the delegator, which is looked up from
the delegation table. Intuitively, it means the following
execution belongs to the unit of the delegator until a dif-
ferent delegator is loaded to the indicator variable. The
optimization of this process can be found in Appendix A.
□ Example. Let us revisit the Firefox example in Figure 9.
We want to attribute all subtasks to their corresponding
tabs (shown in different colors). In Figure 11, we show
a detailed workflow of the main thread posting the con-
nection event to the socket thread. The main thread first
calls the LoadURI method (step 1), which invokes the
PostEvent method. Within PostEvent (box C), it creates
an nsConnEvnet and posts it to the socket thread. Since
data structure nsRunnable (box A) is annotated as a del-
egator and the HTTP connection request nsConnEvent
(box B) is a subclass of nsRunnable, MPI propagates the
current unit id in the main thread to the worker thread,
namely, the socket thread. Specifically, the request is asso-
ciated with the current unit context of the main thread in
the delegation table. Inside the socket thread that receives
and processes the request (i.e., step 3), loading the request
from the task queue causes the change of the queue size
indicating a possible unit context switch. As a result, the
current unit context of the socket thread is set to that of
the request, namely, tab1. With a chain of delegations,
MPI is able to recognize all the tab1 subtasks performed
by different threads, namely, all the red bars in Figure 9
belong to the same tab1 unit. □
3.4 Analysis
The analysis component of MPI is a pass in LLVM re-
sponsible for adding instrumentation to realize the run-
time semantics mentioned earlier. It takes a program with
the four kinds of annotations mentioned in §3.2, and pro-
duces an instrumented version of the program that emits
additional syscall events denoting unit context switches
and channel operations.
MPI needs to identify the following a few kinds of code
locations: (1) all the updates (i.e., definitions) to indicator
variables, including unit indicators and delegator indica-
tors, to add instrumentation for unit context updates; (2)
all the creation/initialization locations of delegator data
structures to add instrumentation for the inheritance of
unit context; (3) reads/writes of channel variables/fields to
add instrumentation for channel event emission and redun-
dancy detection; (4) all the system/library calls that may
lead to system calls to add instrumentation for unit event
emission and redundancy detection. We use a type based
analysis to identify (2) and (3). For (4), we pre-define
a list of library functions (e.g., libc functions) that may
lead to system calls of interest and then scan the LLVM
bitcode to identify all the system calls and the library calls
on the list. Details are elided. A naive solution to (1) is
to perform a walk-through of the LLVM bitcode to iden-
tify all definitions to indicator variables or to their aliases
(using the default alias analysis in LLVM). However, this
may lead to redundant instrumentation. Specifically, an
indicator may be defined multiple times and there may
not be any system calls (or library calls that can lead to
system calls) in between. As such, the unit context switch
instrumentations for those definitions are redundant.
1 /* Match a regexp against multiple lines. */
2 long im_regexec_multi(...) {
3
4
5
6
7
8
9
10 }
buf_T *save_curbuf = curbuf;
// initilize local variables
// switch to buffer "buf" to make vim_iswordc() work
curbuf = buf;
r = vim_regexec_both(NULL, col, tm);
curbuf = save_curbuf;
return r;
Figure 13: Instrumentation example (VIM, op_yank function)
□ Example. The function im_regexec_multi() in Figure 13
searches for a regular expression in Vim. The indicator
variable is updated at line 6, and then again at line 8. The
operations inside function vim_regexec_both() are all on
memory. In other words, it does not make any system
calls directly or indirectly. As such, the instrumentation
for line 6 is redundant. □
The problem is formulated as a reaching-definition
problem, which determines the set of definitions (of a
variable) that can reach a program point. We say a defini-
tion of variable x can reach a program point ℓ, if x is not
redefined along any paths from the definition to ℓ. In our
context, we only instrument the definitions that can reach
a system call or a library call that can lead to a system
call. In Figure 13, the definition at line 6 cannot reach
any point beyond line 8. Since line 7 does not denote
any system call, line 6 is not instrumented. Appendix B
discusses how to construct attack graphs from MPI logs.
USENIX Association
26th USENIX Security Symposium    1119
4 Evaluation
In this section, we present the evaluation results includ-
ing the annotation efforts needed, the runtime and space
overheads of the prototype, and a number of attack cases
to show the advantages of MPI compared to the event
loop based partitioning technique in BEEP [43] and Pro-
Tracer [46]. For comprehensive comparison, we integrate
both MPI and event loop based partitioning with three
underlying provenance tracking systems, the Linux Audit
system, ProTracer and LPM-HiFi.
4.1 Overhead
Space overhead: We measure the space overhead of
MPI and compare it with the overhead of event loop
based partitioning, on the aforementioned three prove-
nance tracking systems. We measure the overhead of
MPI and BEEP on Linux Audit and LPM-HiFi by com-
paring the logs generated by the original binaries and the
instrumented binaries. ProTracer requires unit informa-
tion to eliminate redundant system events (e.g., multiple
reads of a file within a unit). Therefore, it needs to work
with an execution partitioning scheme. We hence com-
pare the ProTracer logs by BEEP and by MPI. Note that
BEEP+ProTracer is equivalent to the original ProTracer
system [46] and in MPI+ProTracer we retain the efficient
runtime of the original ProTracer but replace the partition-
ing component with MPI. Since BEEP supports only one
low-level perspective, we only annotate one perspective
in MPI during comparison. The overhead of multiple
perspectives is in Appendix D.
The results are shown in Table 1. The table contains
the following information (column by column): 1) Ap-
plication. 2) Perspective for partitioning. 3) Overhead of
BEEP on Linux Audit, i.e., comparing the Linux Audit
log sizes with and without BEEP. 4) Overhead of BEEP on
LPM-HiFi with the raw log format. 5) Overhead of BEEP
on LPM-HiFi with its Gzip enabled user space reporter
tool. 6-8) Overhead of MPI on BEEP and LPM-HiFi. 9)
Log size of BEEP on (original) ProTracer. 10) Log size
of MPI on ProTracer. Note that Linux Audit and LPM-
HiFi have different provenance collection mechanisms,
i.e. system call interception for Linux Audit and LSM
for LPM-HiFi. This leads to different space overheads.
LPM-HiFi provides different user space reporters, and the
Gzip enabled reporter has less space overhead.
Observe that for most programs our approach has less
overhead on all the three platforms. For programs like
document readers and video players, both approaches
show very little overhead. These programs do not need to
switch between different tasks frequently, which means
they rarely trigger the instrumented code. Our approach
shows significant better results for many programs like
web browsers, P2P clients, HTTP and FTP programs in-
cluding servers and clients due to a few reasons. Firstly, in
these programs, the events handled by the event handling
loop are at a very low level, whereas MPI can partition ex-
ecution at a much higher level. Thus there are fewer unit
context switches in our system, and multiple execution
units in BEEP are grouped into one in our system without
losing precision. For example in Apache, a remote HTTP
request can lead to redirection, and the Apache server
needs a few BEEP execution units to handle it. This trig-
gers the instrumented code several times. But in MPI,
multiple requests, including their redirections, of a same
connection are grouped together. Thus, the instrumenta-
tion (for unit context switch) is triggered less frequently.
Another reason is that we avoid meaningless execution
units. For example in benchmark Transmission, BEEP
execution units are based on time events, leading to many
redundant units. This is avoided in MPI. Firefox has
high overhead in both systems. When multiple tabs are
opened, Firefox processes them in the background with
threads. Since most of the requests involve network or file
I/O, a lot of system/unit context switches are triggered,
leading to the overhead. Despite this, the overhead of
our system is about one third of that of BEEP. Note that
there is another advantage of MPI that cannot be quan-
tified –MPI does not require extensive training to detect
low level memory dependencies. During our experiments,
we had to add test inputs to the training sets of BEEP to
ensure the provenance was not broken for a number of
applications (e.g., Firefox).
We want to point out that with MPI, we can even re-
duce space overhead for the highly efficient ProTracer
system and the reduction is substantial for a few cases.
This is because MPI produces higher level execution units
(compared to BEEP/ProTracer), leading to fewer units,
more events in each unit and hence more redundancies
eliminated by the ProTracer runtime. Also note that all
the advantages of MPI over BEEP (e.g., without requir-
ing extensive training and rich high-level semantics) are
also advantages over ProTracer as the original ProTracer
system relies on BEEP. We have ran MPI for 24 hours
with a regular workload. The generated audit log has
680MB with 80MB by MPI. Details can be found in
Appendix D.
Run time overhead: We measure the run time overhead
caused by our instrumentation. For server programs, we
use standard benchmarks. For example, for the Apache
web server, we use the ab [1] benchmark. For programs
that do not have standard test benchmarks, but support
batch mode (e.g., Vim), we translate a number of typi-
cal use cases to test scripts to drive the executions. We
preclude highly interactive programs.
For each application, we choose the same perspectives
as the previous experiment, and the results are shown
1120    26th USENIX Security Symposium
USENIX Association
Application
Level
BEEP Space Overhead