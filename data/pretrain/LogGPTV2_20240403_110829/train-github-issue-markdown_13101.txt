## ðŸš€ Feature
All variants of `ReplicationPad` should accept 0-dim batch size tensors for
both forward and backward autograd. Related to #38115
Test code:
    def test_ReplicationPad1d_Forward():
        z = torch.ones(0,3,10)
        m = torch.nn.ReplicationPad1d(3)
        m(z)
    def test_ReplicationPad1d_Backward():
        z = torch.ones(0,3,10)
        m = torch.nn.ReplicationPad1d(3)
        m(z).sum().backward()
    def test_ReplicationPad2d_Forward():
        z = torch.ones(0,3,10,10)
        m = torch.nn.ReplicationPad2d(3)
        m(z)
    def test_ReplicationPad2d_Backward():
        z = torch.ones(0,3,10,10)
        m = torch.nn.ReplicationPad2d(3)
        m(z).sum().backward()
    def test_ReplicationPad3d_Forward():
        z = torch.ones(0,3,10,10,10)
        m = torch.nn.ReplicationPad3d(3)
        m(z)
    def test_ReplicationPad3d_Backward():
        z = torch.ones(0,3,10,10,10)
        m = torch.nn.ReplicationPad3d(3)
        m(z).sum().backward()