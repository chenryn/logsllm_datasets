### Drone Behavior and DCC Computation

In the original swarm mission, the leader drone navigates towards the south-east to avoid Drone 2. However, in the presence of an obstacle, the drone tends to fly more towards the west (∆6) to evade the obstacle.

**Figure 5-(g)** illustrates the Degree of Causal Contribution (DCC) values computed from perturbed executions at the moment depicted in Figure 5. For each victim drone, the DCC value is the percentage of aggregated ∆ values. It is important to note that DCC values are collected throughout the entire swarm mission.

#### Algorithm for DCC Computation

The `SwarmDcc()` function in Algorithm 1 outlines the process for computing DCC values. Specifically, DCC values are calculated for each victim drone specified by \( D_v \). The for loop from line 4 to line 16 details the DCC computation for each drone. The `SWARMFLAWFINDER` tool runs multiple tests with perturbations that remove one of the attack drones (\( D_a \)), obstacles (\( O_w \)), or the victim drones (\( D_v \)) as inputs (Lines 8-14). 

- **\( P_i \)** (line 11) and **\( P_{org} \)** (line 7) represent the pose of a drone with and without the perturbation, respectively.
- The Euclidean distance between the two trajectories (\( \Delta_i \) at line 12) is computed, which corresponds to \( \Delta \) in Figure 5.
- To understand the impact of the attack on the entire swarm, all delta values for all victim drones are computed (nested for loops at lines 4-16 and 8-14).
- DCC is calculated by summing all the delta values (line 13) and then determining each delta value’s proportion in the total accumulated delta value (in percentage) (lines 15-16).

### DCC Guided Fuzz Testing

#### Abstracting Swarm Missions via DCC

**Figure 6** displays a series of DCC values computed throughout the swarm mission (0-180 ticks). The X-axis represents time, and the Y-axis shows stacked \( \Delta \) values. We use the series of DCC values to represent a swarm mission. When two tests have similar DCC value series, we consider them similar. To compare two number series, we use Normalized Cross Correlation (NCC), a common method for computing data similarity in various fields [25-28].

- **Figure 6(a)** shows DCC values from the original execution.
- **Figures 6(b) and 6(c)** display DCC values from two different runs.
- If DCC values from two executions have different lengths (i.e., running times), we scale one of the execution’s DCC values to match the other (interpolation) before computing NCC.
- If the running times differ by more than twice, we consider the executions different.

#### Using NCC of DCC Values to Guide Testing

After each test, we store the observed DCC values. We then determine whether the current test exercises new behaviors by computing NCC scores with previously observed DCC values (lines 26-32 in Algorithm 1). For each victim drone, we compute NCC scores against all previously observed DCC values for the drone. If a previous test run has an NCC score above a threshold (0.75-0.87 in this paper, line 22 in Algorithm 1), it is considered similar, meaning it did not exercise substantially new behavior. We aim to mutate the test significantly (line 35, \( R \) representing a large random value). If no previous test case has an NCC score below the threshold, the current test has DCC values not yet seen. We then slightly mutate the test (line 37, \( \delta \) representing a small random value).

- The NCC threshold is configurable and does not affect the validity of testing. An ill-configured threshold may cause early termination or extended testing.
- To find a proper NCC threshold, we run 100 runs for the same initial test and take the lowest measured NCC score (Table II).

### Algorithm: FuzzTesting()

`FuzzTesting()` in Algorithm 1 describes the entire fuzz testing process, including measuring swarm behaviors against attacks and mutating tests based on the measured impacts. The algorithm takes four inputs:
1. \( D_v \): Configuration of victim drones, including their poses and goals.
2. \( D_a \): Configuration of attack drones, including their poses and strategies.
3. \( O_w \): Objects such as walls and moving obstacles affecting the mission.
4. \( T_{timeout} \): Time limit for the entire testing process (typically set for several hours, e.g., 24 hours).

The output is \( E_{failed} \), a set of executions where the missions failed due to conducted attacks (line 38).

### Testing with Multiple Attack Drones

For algorithms managing distributed drone swarms, `SWARMFLAWFINDER` may need to test with multiple attack drones. A single attack drone might only affect one group, making it difficult to find logic flaws. `SWARMFLAWFINDER` automatically adds additional attack drones if the testing fails to find attacks. Adding \( N \) additional attack drones typically causes about 5%*N overhead (details in [8]).

#### Mutating Tests with Multiple Attack Drones

A test run with multiple drones is defined as a test case with multiple tuples, each representing an attack drone. We observe changes in DCC values caused by multiple attack drones and identify which drone is effective in exercising new swarm behavior. We apply mutations for each attack drone to ensure that DCC value changes caused by one drone do not affect others. Victim drones can be directly or indirectly affected by an attack drone. We check DCC values to identify affected drones and compute NCC values for the identified drones. An example scenario with multiple attack drones is presented in [8].

### Evaluation

#### Experiment Setup

1. **Selection of Target Swarm Algorithms**: We searched open-sourced research projects related to swarm robotics from 2010 to 2021, listing 44 academic papers and 29 public GitHub repositories. From these, 17 papers provided source code, resulting in 46 available algorithms. After pruning, 26 runnable algorithms remained, and we selected four that exhibit collective behaviors and allow external objects like our attack drones (Table I and Figure 7).

2. **Experimental Configurations**: Table II defines mission failures and configurations for the four selected swarm algorithms. A mission is considered failed if it takes longer than twice its typical completion time or if a drone crashes into an object or another drone. We do not count opportunistic attacks or attack drones crashing into victim drones. The 200% deadline and NCC thresholds are determined by running each mission 100 times without interventions.

3. **Implementation and Setup**: We implemented prototypes of `SWARMFLAWFINDER` in the programming languages used by the original algorithms: Python, C#, Netlogo, and Matlab. Our implementation includes modifications of existing simulators/emulators, writing 839, 331, 422, and 230 SLOC for A1-A4, respectively. The NCC analysis tool and map for A3 are written in R (820 lines).

#### Fuzz Testing Results

**Table III** summarizes the fuzz testing results, including mission failures, root causes, and the number of unique confirmed executions for each algorithm.