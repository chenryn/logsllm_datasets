目录
[前言 2](\l)
[一、背景调研 2](\l)
[1、OtterTune 2](\l)
[2、AutoTiKV 2](\l)
[3、ML模型 3](\l)
[二、可行性分析 5](\l)
[三、具体实现 6](\l)
[四、测试结果 8](\l)
[knobs 8](\l)
[metrics 8](\l)
[脚本具体使用步骤 8](\l)
[五、存在的问题 12](\l)
## 前言
beaver是由日志易自主研发、安全可控的搜索引擎，由master、broker和datanode三部分组成，已广泛应用于存储和分析由大型分布式系统生成的日志，beaver拥有大量与性能相关的配置项，由于手动配置费时费力，并且有时需要修改相关配置以适配特定环境，所以自动调整配置参数优化性能是当前迫切需要解决的问题。
## 一、背景调研
目前业界有许多自动调参的项目和算法实现，例如CMU开源的关系型数据库自动调参工具
OtterTune^\[1\]^、PingCAP 仿作的 TiKV
自动调参工具^\[2\]^等，都为我们提供了大量的论文以及开源算法代码。
### 1、OtterTune
数据库有很多参数，比如MySQL有几百个参数，Oracle有上千个参数。这些参数控制着数据库的方方面面，很大程度的影响了如缓存容量和检查点频次等数据库性能。
对于不同的硬件配置，不同的工作负载，对应的最优参数文件都是不同的。DBA（Database
Administrator，数据库管理员）不能简单的重复使用之前调好的参数文件。这些复杂性令数据库调优变得更加困难。DBA需要花大量时间根据经验来调优数据库的参数，而公司则需要花大价钱来雇佣资深DBA。
为解决上述问题，卡内基梅隆大学数据库小组的教授、学生和研究人员开发了一个数据库自动调参工具OtterTune，它能利用机器学习对数据库的参数文件自动化的调优，能利用已有的数据训练机器学习模型，进而自动化的推荐最优参数。它能很好的帮助DBA进行数据库调优，将DBA从复杂繁琐的调参工作中解放出来。
OtterTune的目的是为了帮助DBA，让数据库部署和调优更加容易，用机器来代替数据库调参这个冗繁但又很重要的工作，让技术人员甚至不需要专业知识也能顺利完成。
OtterTune分为客户端和服务端，目标数据库是用户需要调优参数的数据库：
-   客户端安装在目标数据库所在机器上，收集目标数据库的统计信息，并上传到服务端。
-   服务端一般配置在云上，获取到客户端的数据，
    > 训练机器学习模型并推荐配置文件。
客户端接收到推荐的配置文件后，配置到目标数据库上，测量其性能。以上步骤可重复进行直到用户对其推荐的配置文件满意。
### 2、AutoTiKV
AutoTikv是一个用于对TiKV数据库进行自动调优的工具。它是根据SIGMOD2017年的一篇论文所设计^\[3\]^，使用机器学习模型对数据库参数进行自动调优。
AutoTiKV吸取了OtterTune的设计理念，并简化相关结构。
设计的调优过程如下：
![autoTikv调优过程](media/image1.png){width="5.7652777777777775in"
height="5.554166666666666in"}
整个过程会循环跑200个round（用户可自定义），或者定义为直到结果收敛为止。
### 3、ML模型
AutoTikv 使用了和 OtterTune 一样的高斯过程回归（Gaussian Process
Regression，以下简称 GP）来推荐新的
knob，它是基于高斯分布的一种非参数模型。
在没有利用机器学习模型对参数文件的效果进行预测前，OtterTune使用的是随机采样收集初始数据。
当有足够的数据(X,Y)时，OtterTune训练机器学习模型进行回归，即估计出函数f:X→Y，使得对于参数文件X，用f(X)来估计数据库延迟Y的值。问题变为寻找合适的X，使f(X)的值尽量小。这样在f上面做梯度下降即可找出合适的X^\[4\]^。
如下图所示，横坐标是两个参数\--缓存大小和日志文件大小，纵坐标是数据库延迟(越低越好):
![](media/image2.png){width="3.3333333333333335in"
height="1.8854166666666667in"}
OtterTune用高斯过程回归模型估计出了f，即给定这两个参数值，估计出对应的数据库延迟。接着用梯度下降找到最合适的参数值使延迟尽可能低。
高斯过程回归的好处是：
1)  和神经网络之类的方法相比，GP
    > 属于无参数模型，算法计算量相对较低，而且在训练样本很少的情况下表现比
    > 神经网络算法( Neural Network ) 更好。
2)  它能估计样本的分布情况，即 X 的均值 m(X) 和标准差 s(X)。若 X
    > 周围的数据不多，则它被估计出的标准差 s(X) 会偏大（表示这个样本 X
    > 和其他数据点的差异大）。直观的理解是若数据不多，则不确定性会大，体现在标准差偏大。反之，数据足够时，不确定性减少，标准差会偏小。这个特性后面会用到。
但 GP
本身其实只能估计样本的分布，为了得到最终的预测值，我们需要把它应用到贝叶斯优化（Bayesian
Optimization）中^\[5\]^。贝叶斯优化算法大致可分为两步：
1)  通过 GP 估计出函数的分布情况
2)  通过采集函数（Acquisition
    > Function）指导下一步的采样（也就是给出推荐值）
采集函数（Acquisition
Function）的作用是：在寻找新的推荐值的时候，平衡探索（exploration）和利用（exploitation）两个性质：
-   exploration：在目前数据量较少的未知区域探索新的点。
-   exploitation：对于数据量足够多的已知区域，利用这些数据训练模型进行估计，找出最优值
在推荐的过程中，需要平衡上述两种指标。exploitation
过多会导致结果陷入局部最优值（重复推荐目前已知的最好的点，但可能还有更好的点没被发现），而
exploration
过多又会导致搜索效率太低（一直在探索新区域，而没有对当前比较好的区域进行深入尝试）。而平衡二者的核心思想是：当数据足够多时，利用现有的数据推荐；当缺少数据时，我们在点最少的区域进行探索，探索最未知的区域能给我们最大的信息量。
贝叶斯优化的第二步就可以帮我们实现这一思想。前面提到 GP 可以帮我们估计 X
的均值 m(X) 和标准差 s(X)，其中均值 m(X) 可以作为 exploitation
的表征值，而标准差 s(X) 可以作为 exploration
的表征值。这样就可以用贝叶斯优化方法来求解了。
使用置信区间上界（Upper Confidence Bound）作为采集函数。假设我们需要找 X
使 Y 值尽可能大，则 U(X) = m(X) + k\*s(X)，其中 k \> 0
是可调的系数。我们只要找 X 使 U(X) 尽可能大即可。
若 U(X) 大，则可能 m(X) 大，也可能 s(X) 大。
若 s(X) 大，则说明 X 周围数据不多，需要探索未知区域新的点。
若 m(X) 大，说明估计的 Y 值均值大， 则需要利用已知数据找到效果好的点。
其中系数 k 影响着探索和利用的比例，k 越大，越鼓励探索新的区域。
在具体实现中，一开始随机生成若干个 candidate
knobs，然后用上述模型计算出它们的 U(X)，找出 U(X)
最大的那一个作为本次推荐的结果。
## 二、可行性分析
目前所有开源的自动调参工具实现原理基本上都是通过机器学习算法推荐配置参数，应用至数据库或者其他引擎上，在不同的工作负载模式下，不断的收集metric信息，丰富训练模型，直至推荐出最优的配置，以此替代频繁的手动修改配置工作。
由调研可以发现，OtterTune是通用模型框架，在业界许多场景都能应用，不仅能调优数据库的参数，还能够调优操作系统内核的参数，即只要能获取指标信息，大部分软件都可以用此模型进行调优。
同时可以借鉴AutoTiKV的测试代码，将目标DB替换为Beaver_datanode，通过修改不同的配置，测试baimi数据集，收集search的性能数据，经过模型训练后不断推荐最优配置。
AutoTiKV代码分析：
1.  pipeline.py
自动调参脚本入口，定义执行round数，自动推荐参数配置，修改配置文件并重启相关DB，收集metric数据训练算法模型，以文件形式持久化保存对象。
2.  settings.py
脚本参数配置，需要测试的knobs和metrics，以及workload，数据库连接配置。需要优化的metric（仅支持优化一项目标metric），ansible和deploy目录。
3.  controller.py
knob配置和metric获取相关函数，每一个需要修改的参数都需要在knob_set中定义，声明参数类型和取值范围。修改配置文件和重启数据库函数等。workload函数。
4.  datamodel.py
初始化数据设置，存放数据模型，每次测试的配置参数和获取到的指标数据都会存放在此模型中。
5.  gpmodel.py
调用高斯过程回归类算法，传递并训练数据模型，根据算法推荐返回最佳配置参数，用于下轮测试。前十轮为随机生成的knob。
6.  gpclass.py
高斯过程回归算法。
7.  showres.py
展示过往测试结果，将持久化保存的对象文件反序列化，调用datamodel.py函数中GPDataSet类输出测试结果。
## 三、具体实现
本着不重复造轮子的原则，本次测试决定使用autoTiKV的算法代码，并修改其中关于数据库的代码，使其适用于beaver_datanode。
首先，TiKV数据库使用的配置文件是yaml格式，而beaver使用的是flags参数的形式（\--max_concurrency_tasks_per_search=4），代码中使用的ruamel.yaml库文件并不适用于beaver。因此对controller.py中set_tikvyml函数进行修改，以"="为分隔符，读取旧配置文件并将参数以键值对形式写入字典中，对需要修改的配置项进行替换，最后把修改过后的配置参数写入到新配置文件中。
需要修改的配置参数应在settings.py中提前声明，更新target_knob_set列表中的参数，新增wl_metrics中avgsearch列表，并设置期望的metric。在controller.py中补充参数的类型和取值范围，配置好knob_set和metric_set。修改metric数据获取函数，其中read_search_latency()函数是基于已经索引好的baimi数据集，测试某个场景的search性能。
baimi数据集，即apache访问日志，总日志行数7078124，日志文件原始大小2374265761Byte，测试beaver和es的搜索性能对比中用到的数据集。参考了esrally的性能压测代码^\[7\]^，调用beaver_broker的API接口，通过传递pb格式的搜索语句，来获取不同场景下的latnecy。本次自动调参测试中，使用的搜索场景是从数据集中获取apache.resp_len字段的平均值，可以根据自己的实际环境自定义场景。因为每次得到的性能数据可能受到各种因素影响，或产生较大误差，为了降低误差值，搜索请求预热20次，压测100次，并计算前90th的平均值作为最终的metric数据。
settings.py中需要修改的配置：
*\# beaver集群的broker地址和端口，主要用来测试搜索性能*
*beaver_broker_ip=\"172.21.16.16\"*
*beaver_broker_port=\"50061\"*
*\# 测试搜索性能需要的索引*
*index_forsearch=\"ops-http_baimi-20210507\"*
*\# pb类型搜索语句，求apache.resp_len字段的平均值*
*pb_forsearch=\'search_info {query {type: kQueryMatchAll}fetch_source
{fetch: true}size {value: 0} aggregations { aggs { type: kAggAvg name:
\"av(apache.resp_len)\" body { field: \"apache.resp_len\_\_l\_\_\" } } }
query_time_range {time_range {min: 0 max: 1620374828405}}}\'*
*\# 不同工作负载模式下相关的指标，可以随意命名，workload*
*wl_metrics={*
*\"avgsearch\":
\[\"search_latency\",\"compaction_mem\",\"compaction_cpu\"\],*
*}*
*\# workload to be load*
*loadtype = \"avgsearch\"*
*\# workload to be run*
*wltype = \"avgsearch\"*