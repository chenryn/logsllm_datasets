title:Unconstrained endpoint profiling (googling the internet)
author:Ionut Trestian and
Supranamaya Ranjan and
Aleksandar Kuzmanovic and
Antonio Nucci
Unconstrained Endpoint Proﬁling
(Googling the Internet)
Ionut Trestian
Northwestern University
Evanston, IL, USA
PI:EMAIL
Aleksandar Kuzmanovic
Northwestern University
Evanston, IL, USA
PI:EMAIL
Supranamaya Ranjan
Narus Inc.
Mountain View, CA, USA
PI:EMAIL
Antonio Nucci
Narus Inc.
Mountain View, CA, USA
PI:EMAIL
ABSTRACT
Understanding Internet access trends at a global scale, i.e., what do
people do on the Internet, is a challenging problem that is typically
addressed by analyzing network traces. However, obtaining such
traces presents its own set of challenges owing to either privacy
concerns or to other operational difﬁculties. The key hypothesis of
our work here is that most of the information needed to proﬁle the
Internet endpoints is already available around us — on the web.
In this paper, we introduce a novel approach for proﬁling and
classifying endpoints. We implement and deploy a Google-based
proﬁling tool, which accurately characterizes endpoint behavior
by collecting and strategically combining information freely avail-
able on the web. Our ‘unconstrained endpoint proﬁling’ approach
shows remarkable advances in the following scenarios: (i) Even
when no packet traces are available, it can accurately predict appli-
cation and protocol usage trends at arbitrary networks; (ii) When
network traces are available, it dramatically outperforms state-of-
the-art classiﬁcation tools; (iii) When sampled ﬂow-level traces
are available, it retains high classiﬁcation capabilities when other
schemes literally fall apart. Using this approach, we perform un-
constrained endpoint proﬁling at a global scale: for clients in four
different world regions (Asia, South and North America and Eu-
rope). We provide the ﬁrst-of-its-kind endpoint analysis which re-
veals fascinating similarities and differences among these regions.
Categories and Subject Descriptors
C.2.3 [Computer-Communication Networks]: Network Opera-
tions
C.4 [Performance of Systems]: Measurement techniques
General Terms
Measurement, Design, Experimentation
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’08, August 17–22, 2008, Seattle, Washington, USA.
Copyright 2008 ACM 978-1-60558-175-0/08/08 ...$5.00.
Keywords
Google, endpoint proﬁling, trafﬁc classiﬁcation, clustering, trafﬁc
locality
1.
INTRODUCTION
Understanding what people are doing on the Internet at a global
scale, e.g., which applications and protocols they use, which sites
they access, and who they try to talk to, is an intriguing and im-
portant question for a number of reasons. Answering this question
can help reveal fascinating cultural differences among nations and
world regions. It can shed more light on important social tenden-
cies (e.g., [36]) and help address imminent security vulnerabilities
(e.g., [34, 44]). Moreover, understanding shifts in clients’ interests,
e.g., detecting when a new application or service becomes popular,
can dramatically impact trafﬁc engineering requirements as well as
marketing and IT-business arenas. YouTube [19] is probably the
best example: it came ‘out of nowhere,’ and it currently accounts
for more than 10% of the total Internet trafﬁc [24].
The most common way to answer the above questions is to an-
alyze operational network traces. Unfortunately, such an approach
faces a number of challenges. First, obtaining ‘raw’ packet traces
from operational networks can be very hard, primarily due to pri-
vacy concerns. As a result, researchers are typically limited to
traces collected at their own institutions’ access networks (e.g.,
[29, 30]). While certainly useful, such traces can have a strong
‘locality’ bias and thus cannot be used to accurately reveal the
diversity of applications and behaviors at a global Internet scale.
Moreover, sharing such traces among different institutions is again
infeasible due to privacy concerns.
Even when there are no obstacles in obtaining non-access, i.e.,
core-level traces, problems still remain. In particular, accurately
classifying trafﬁc in an online fashion at high speeds is an inher-
ently hard problem. Likewise, gathering large amounts of data for
off-line post-processing is an additional challenge. Typically, it is
feasible to collect only ﬂow-level, or sampled ﬂow-level informa-
tion. Unfortunately, the state-of-the-art packet-level trafﬁc classiﬁ-
cation tools (e.g., [29]) are simply inapplicable in such scenarios,
as we demonstrate below.
In this paper, we propose a fundamental change in approaching
the ‘endpoint proﬁling problem’: depart from strictly relying on
(and extracting information from) network traces, and look for an-
swers elsewhere. Indeed, our key hypothesis is that the large and
representative amount of information about endpoint behavior is
available in different forms all around us.
For communication to progress in the Internet, in the vast major-
ity of scenarios, information about servers, i.e., which IP address
one must contact in order to proceed, must be publicly available.
In p2p-based communication, in which all endpoints can act both
as clients and servers, this means that association between an end-
point and such an application becomes publicly visible. Even in
classical client-server communication scenarios, information about
clients does stay publicly available for a number of reasons (e.g.,
at website user access logs, forums, proxy logs, etc.). Given that
many other forms of communication and various endpoint behav-
ior (e.g., game abuses) does get captured and archived, this implies
that enormous information, invaluable for characterizing endpoint
behavior at a global scale, is publicly available — on the web.
The ﬁrst contribution of this paper is the introduction of a novel
methodology, which we term ‘unconstrained endpoint proﬁling’,
for characterizing endpoint behavior by strategically combining in-
formation from a number of different sources available on the web.
The key idea is to query the Google search engine [6] with IP
addresses corresponding to arbitrary endpoints. In particular, we
search on text strings corresponding to the standard dotted deci-
mal representation of IP addresses, and then characterize endpoints
by extracting information from the responses returned by Google.
The core components of our methodology are (i) a rule generator
that operates on top of the Google search engine, and (ii) an IP
tagger, which tags endpoints with appropriate features based solely
on information collected on the web. The key challenge lies in au-
tomatically and accurately distilling valuable information from the
web and creating a semantically-rich endpoint database.
We demonstrate that the proposed methodology shows remark-
able advances in the following scenarios: (i) even when no opera-
tional traces from a given network are available, it can accurately
predict trafﬁc mixes, i.e., relative presence of various applications
in given networks, (ii) when packet-level traces are available, it can
help dramatically outperform state of the art trafﬁc classiﬁcation al-
gorithms, e.g., [29], both quantitatively and qualitatively and, (iii)
when sampled ﬂow-level traces are available, it retains high clas-
siﬁcation capabilities when other state-of-the-art schemes literally
fall apart.
Our second contribution lies in exploiting our methodology to
perform, to the best of our knowledge, the ﬁrst-of-its-kind Inter-
net access trend analysis for four world regions: Asia, S. and N.
America, and Europe. Not only do we conﬁrm some common wis-
dom, e.g., Google massively used all around the world, Linux
operating system widely deployed in France and Brazil, or multi-
player online gaming highly popular in Asia; we conﬁrm fascinat-
ing similarities and differences among these regions. For example,
we group endpoints into different classes based on their application
usage. We ﬁnd that in all explored regions, the online gaming users
strongly protrude as a separate group without much overlap with
others. At the same time, we explore locality properties, i.e., where
do clients fetch content from. We ﬁnd strong locality bias for Asia
(China), but also for N. America (US), yet much more international
behavior by clients in S. America (Brazil) and Europe (France).
This paper is structured as follows. In Section 2 we explain our
unconstrained endpoint proﬁling methodology which we evaluate
in a number of different scenarios in Section 3, and apply this ap-
proach to four different world regions in Section 4. We discuss re-
lated issues in Section 5, and provide an overview of related work
in Section 6. Finally, we conclude in Section 7.
2. METHODOLOGY
Here, we propose a new methodology, which we term ‘Uncon-
strained Endpoint Proﬁling’ (UEP). Our goal is to characterize end-
points by strategically combining information available at a number
of different sources on the web. Our key hypothesis is that records
about many Internet endpoints’ activities inevitably stay publicly
archived. Of course, not all active endpoints appear on the web,
and not all communication leaves a public trace. Still, we show
that enormous amounts of information do stay publicly available,
and that a ‘puriﬁed’ version of it could be used in a number of con-
texts that we explore later in the paper.
Figure 1: Web-based endpoint proﬁling
2.1 Unconstrained Endpoint Proﬁling
Figure 1 depicts our web-based endpoint proﬁling tool. At the
functional level, the goal is straightforward: we query the Google
search engine by searching on text strings corresponding to the
standard dotted decimal representation of IP addresses. For a given
input in the form of an IP address, e.g., 200.101.18.182, we col-
lect search hits returned by Google, and then extract information
about the corresponding endpoint. The output is a set of tags (fea-
tures) associated with this IP address. For example, forum user,
kazaa node, game abuser, mail server, etc. In general,
an endpoint could be tagged by a number of features, e.g., a forum
user and a p2p client. Such information can come from a
number of different URLs.
At a high level, our approach is based on searching for infor-
mation related to IP addresses on the web. The larger the num-
ber of search hits returned for a queried IP address, and the larger
number of them conﬁrming a given behavior (i.e., a streaming
server), the larger the conﬁdence about the given endpoint activ-
ity. The proﬁling methodology involves the following three mod-
ules: (i) Rule generation, (ii) Web classiﬁcation, and (iii) IP tag-
ging, which we present in detail below.
2.1.1 Rule Generation
The process starts by querying Google [6] using a sample ‘seed
set’ of random IP addresses from the networks in four different
world regions (details in Section 3) and then obtaining the set of
search hits. Each search hit consists of a URL and corresponding
hit text, i.e., the text surrounding the word searched. We then ex-
tract all the words and biwords (word pairs) from the hit texts of all
the hits returned for this seed set. After ranking all the words and
biwords by the number of hits they occur in and after ﬁltering the
trivial keywords (e.g., ‘the’), we constrain ourselves to the top N
keywords1 that could be meaningfully used for endpoint classiﬁca-
tion.
Then, in the only manual step in our methodology, we construct a
set of rules that map keywords to an interpretation for the function-
ing of that website, i.e., the website class. The rules are as shown
in the relationship between Column 1 and 2 in Table 1. For exam-
ple, the rules we develop in this step capture the intelligence that
presence of one of the following keywords: counter strike,
world of warcraft, age of empires, quake, or game
abuse in either the URL or the text of a website implies that it is
1We ﬁnd and use the top 60 keywords in this paper.
{‘ftp’ | ‘webmail’ | ‘dns’ | ‘email’ | ‘proxy’ | ‘smtp’
Protocols and Services
 server
Keywords
Website Class
Tags
Table 1: Keywords - Website Class - Tags mapping
| ‘mysql’ | ‘pop3’ | ‘mms’ | ‘netbios’}
{‘trojan’ | ‘worm’ | ‘malware’ | ‘spyware’ | ‘bot’}
Malicious information list
 affected host
‘spam’
{‘blacklist’ | ‘banlist’ | ‘ban’ | ‘blocklist’}
‘adserver’
{‘domain’ | ‘whois’ | ‘website’}
{‘dns’ | ‘server’ | ‘ns’}
{‘proxy’ | ‘anonymous’ | ‘transparent’}
‘router’
‘mail server’
‘mail server’ & {‘spam’ | ‘dictionary attacker’}
{‘counter strike’ | ‘warcraft’ | ‘age of the
empires’ | ‘quake’ | ‘halo’ | ‘game’}
{‘counter strike’ | ‘warcraft’ | ‘age of the empires’ |
‘quake’ ‘halo’ | ‘game’} & {‘abuse’ | ‘block’}
Spamlist
Blacklist
Ad-server list
Domain database
DNS list
Proxy list
Router addresses list
Mail server list
Malicious
mail servers list
Gaming servers list
Gaming abuse list
spammer
blacklisted
adserver
website
DNS server
proxy server
router
mail server
mail server
[spammer] [dictionary attacker]
server
 node
[abuser] [blocked]
{‘torrent’ | ‘emule’ | ‘kazaa’ | ‘edonkey’ | ‘announce’ | ‘tracker’ |
‘xunlei’ | ‘limewire’ | ‘bitcomet’ | ‘uusee’ | ‘qqlive’ | ‘pplive’ }
{‘irc’ | ‘undernet’ | ‘innernet’ | ‘dal.net’}
{‘yahoo’ | ‘gtalk’ | ‘msn’ | ‘qq’ | ‘icq’ | ‘server’ | ‘block’}
{‘generated by’ | ‘awstats’ | ‘wwwstat’ |
‘counter’ | ‘stats’}
{‘cachemgr’ | ‘ipcache’}
{‘forum’ | ‘answer’ | ‘resposta’ | ‘reponse’ | ‘comment’ |
‘comentario’ | ‘commentaire’ | ‘posted’ | ‘poste’ |
‘registered’| ‘registrado’ | ‘enregistre’ | ’created’ | ’criado’
‘cree’ | ‘bbs’ | ‘board’ | ‘club’ | ‘guestbook’ | ‘cafe’ }
p2p node list
 p2p node
IRC servers list
Chat servers
Web log site
Proxy log
Forum
IRC server
 chat server
web user [operating system]
[browser][date]
proxy user [site accessed]
forum user [date][user name]
[http share ][ftp share]
[streaming node]
a gaming website (either gaming server list or abuse list). Table 1
shows a few rules to differentiate the information contained in web-
sites. For instance, if a website only contains the keyword mail
server from the set of keywords, then it is classiﬁed as a site
containing list of mail servers. However, if a website contains one
of the following words, spam or dictionary attacker be-
sides mail server, then it is classiﬁed as one containing list of
malicious mail servers, e.g., one which is known to originate spam.
Similar rules are used to differentiate between websites providing
gaming servers list and gaming abuse list.
2.1.2 Web Classiﬁer
Extracting information about endpoints from the web is a non-
trivial problem. Our approach is to ﬁrst characterize a given web-
page (returned by Google), i.e., determine what information does
the website contain. This approach signiﬁcantly simpliﬁes the end-
point tagging procedure.
Rapid URL Search. Some websites can be quickly classiﬁed
by the keywords present in their domain name itself. Hence, after
obtaining a search hit we ﬁrst scan the URL string to identify the
presence of one of the keywords from our keyword set in the URL
and then determine the website’s class on the basis of the rules in
Table 1. For instance, if the URL matches the rule: {forum |
... | cafe} (see last row in Table 1) then we classify the URL as
a Forum site. Typically, websites that get classiﬁed by this rapid
URL search belong to the Forum and Web log classes. If the Rapid
URL search succeeds, we proceed to the IP tagging phase (Section
2.1.3). If rapid match fails, we initiate a more thorough search in
the hit text, as we explain next.
Hit Text Search. To facilitate efﬁcient webpage characterization
and endpoint tagging, we build a website cache. The key idea is
to speed-up the classiﬁcation of endpoints coming from the same
web sites/domains under the assumption that URLs from the same
domain contain similar content. In particular, we implement the
website cache as a hashtable indexed by the domain part of the
URL. For example, if we have a hit coming from the following
URL: www.robtex.com/dns/32.net.ru.html, the key in
the hashtable becomes robtex.com. Hence, all IPs that return a
search hit from this domain can be classiﬁed in the same way.
Whenever we ﬁnd a URL whose corresponding domain name is
not present in the cache, we update the cache as follows. First, we
insert the domain name for the URL as an index into the cache with
an empty list (no keywords) for the value. In addition, we insert a
counter for number of queried IP addresses that return this URL as
a hit along with the corresponding IP address. High values for the
counter would indicate that this domain contains information useful
for classifying endpoints. Thus, when the counter for number of IP
addresses goes over a threshold (we currently use a threshold of 2),
we retrieve the webpage based on the last URL.2 Then, we search
the webpage for the keywords from the keyword set and extract the
ones which can be found.
Next, we use the rule-based approach to determine the class to
which this website (and hence the domain) belongs. Finally, we
insert an entry in the cache with the domain name as the key and
the list of all associated keywords (from Table1) as the value. For
instance, if the URL matches the rule: mail server & {spam
| dictionary attacker}, then the domain gets classiﬁed as