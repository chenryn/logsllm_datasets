V. EVALUATION
We ﬁrst introduce the evaluation metrics and several re-
search questions (RQs). Then, we show the performances of
MI attacks under different settings based on different RQs, and
explain what we learn from the results in details.
A. Evaluation Metrics, Experimental Setting and Research
Questions
We mainly use F1-score, the harmonic mean of precision
and recall, as our evaluation metrics, because F1-score rep-
resents a trade-off between precision and recall. Speciﬁcally,
Precision represents the ratio of real-true members predicted
among all the positive membership predictions made by an
adversary, and Recall demonstrates the ratio of true members
predicted by an adversary among all the real-true members.
We adopt the batch mode for BLINDMI in our experiments.
Following the prior work [41], in the Blind and Graybox-Blind
settings, we select the top three feature values for all variations
of BLINDMI; in the Blackbox and Graybox settings, we select
the top two feature values plus the value of the ground-truth
class. All the experiments are performed using the GeForce
RTX 2080 graphics cards (NVIDIA).
Our evaluation aims to answer the following RQs.
• RQ1 [All Settings]: What is the performance of all varia-
tions of BLINDMI compared with state-of-the-art MI attacks
under different settings?
• RQ2 [Blackbox Setting]: How does BLINDMI perform
under existing defenses against MI attacks?
• RQ3 [Blind Setting]: What is the performance of BLINDMI
for different quality and size of the non-member set?
• RQ4 [Blind Setting]: How do different initial classiﬁers and
kernel functions affect the performance of BLINDMI-DIFF?
• RQ5 [Blind Setting]: How long and how many moves and
iterations are needed for BLINDMI-DIFF to converge?
• RQ6 [Blackbox Setting]: What
the performance
of BLINDMI under different
real-world settings, e.g.,
nonmember-to-member ratio and number of target model’s
classes?
is
B. RQ1: Attack Performance With Different Settings
is 20,
In this subsection, we evaluate and compare the Precision,
Recall, and F1-score of BLINDMI and existing attacks in
Section IV-C. Our setting for this RQ is that the nonmember
dataset size of BLINDMI-DIFF-w/
the nonmember
dataset size of BLINDMI-1CLASS is 1,000, and BLINDMI-
DIFF-w/o does not need additional nonmembers. The target
dataset sizes depending on the problem domain are shown
in Table V. Each attack is performed ten times with a new
target and shadow model with different training datasets, model
architectures and hyperparameters each time. Then, we obtain
the average values of F1-score together with the standard error
of the mean among the ten attacks.
Table VIII shows the Precision, Recall and F1-score of
different attacks under four adversarial settings. The best per-
formances of all attacks under different settings are highlighted
8
with different colors (blue for recall, green for precision, and
red for F1-score.) Note that if the performance of a prior
attack, e.g., NN-based, is the same with and without ground
truth label, we only show the attack once under the blind
and graybox-blind settings. We do show BLINDMI multiple
times under different settings for ease of comparison. Next,
we introduce several observations from our experiments.
[Observation RQ1-1] BLINDMI signiﬁcantly outperforms
state-of-the-art MI attacks under all settings in terms of F1-
score.
The ﬁrst observation is that BLINDMI outperforms state-
of-the-art MI attacks under all settings: The reason is that
BLINDMI extracts membership semantics directly from the
target model via probing. Sometimes, the performance boost
is over 20%, e.g., for the Adult and BIRDS-200 datasets
under the blind setting. As a comparison, no single prior
attack dominates the performance in F1-score. Consider the
blind setting for example. Top1-Thre is the best for the
EyePACS dataset except for BLINDMI; NN is the best for
the CH-MNIST dataset except for BLINDMI; and Top3-NN
outperforms all methods for the Purchase-50. The reason is
that no prior attacks extract enough membership semantics as
BLINDMI does.
[Observation RQ1-2] The introduction of ground-truth la-
bels improves attack performance, but to a limited degree for
BLINDMI.
The second observation is about how the introduction of
ground-truth labels affects attack performance. The perfor-
mance boost is sometimes signiﬁcant for prior attacks. Take
Purchase-50 for example. The best average F1-score under the
blind setting is 59.6%, but the average F1-score increases to
72.1%, a 12.5% increase, under the blackbox setting.
As a comparison, the best performance boost of BLINDMI
with the ground truth label is 2.9% for the EyePACS dataset.
That said, although ground-truth labels introduce additional
membership semantics, the semantics introduction is limited
in terms of F1-score improvement.
[Observation RQ1-3] Shadow model quality plays an im-
portant role in some existing attacks.
The third observation is about how different shadow models
affect the attack performance. First, BLINDMI does not need
a shadow model and therefore BLINDMI’s performance is the
same with or without shadow model. Second, the performance
of some existing attacks varies a lot given different shadow
models. Take the NN attack for BIRDS-200 under the blind
setting for example. The average F1-score is 58.3, but the
standard error is 15.0 with a conﬁdence of 68.3%. That said,
the choice of shadow models is crucial in the performance of
existing attacks with shadow models.
[Observation RQ1-4] BLINDMI-DIFF-w/ performs the best
among all
three variations in terms of F1-score, while
BLINDMI-DIFF-w/o does not need additional probes to the
target model.
Table VIII shows that BLINDMI-DIFF-w/ is the best com-
paring with BLINDMI-DIFF-w/o and BLINDMI-1CLASS. At
the same time, BLINDMI-DIFF-w/ does require 20 additional
probes to the target model to generate a non-member set. If
the adversary’s access to the target model is restricted to the
TABLE VIII.
PRECISION, RECALL, F1-SCORE (%) WITH STANDARD ERROR OF THE MEAN OF PRIOR ATTACKS AND BLINDMI UNDER FOUR DIFFERENT
ADVERSARIAL SETTINGS (BLUE INDICATES THE HIGHEST RECALL, GREEN THE HIGHEST PRECISION, AND RED THE HIGHEST F1-SCORE FOR EACH
SETTING).
Attack
Metric
Adult
NN
Top3-NN
d
n
i
l
B
Top1-Threshold
BlindMI-Diff-w/
x
o
b
k
c
a
l
B
BlindMI-Diff-w/o
BlindMI-1Class
Top2+True
Loss-Threshold
Label-Only
BlindMI-Diff-w/
BlindMI-Diff-w/o
BlindMI-1Class
NN
Top3-NN
d
n
i
l
B
-
y
a
r
G
Top1-Threshold
BlindMI-Diff-w/
BlindMI-Diff-w/o
BlindMI-1Class
Top2+True
Loss-Threshold
Label-Only
BlindMI-Diff-w/
BlindMI-Diff-w/o
BlindMI-1Class
x
o
b
y
a
r
G
Precision
Recall
F1-Score
Precision
Recall
F1-Score
Precision
Recall
F1-Score
Precision
Recall
F1-Score
Precision
Recall
F1-Score
Precision
Recall
F1-Score
Precision
Recall
F1-Score
Precision
Recall
F1-Score
Precision
Recall
F1-Score
Precision
Recall
F1-Score
Precision
Recall
F1-Score
Precision
Recall
F1-Score
Precision
Recall
F1-Score
Precision
Recall
F1-Score
Precision
Recall
F1-Score
Precision
Recall
F1-Score
Precision
Recall
F1-Score
Precision
Recall
F1-Score
Precision
Recall
F1-Score
Precision
Recall
F1-Score
Precision
Recall
F1-Score
Precision
Recall
F1-Score
Precision
Recall
F1-Score
Precision
Recall
F1-Score
49.9 ± 0.30
35.1 ± 10.1
40.6 ± 7.32
49.8 ± 0.37
20.2 ± 7.34
26.7 ± 7.25
0.51 ± 0.23
31.3 ± 13.6
1.01 ± 0.44
50.0 ± 0.03
90.4 ± 5.95
64.2 ± 1.59
50.5 ± 0.05
84.2 ± 4.05
62.7 ± 1.12
49.9 ± 0.07
56.5 ± 5.85
52.6 ± 2.47
49.8 ± 0.10
59.6 ± 14.5
52.1 ± 6.27
64.4 ± 2.07
49.9 ± 0.04
56.2 ± 0.77
46.5 ± 3.04
71.7 ± 9.54
56.2 ± 5.28
50.0 ± 0.01
97.0 ± 1.18
66.0 ± 0.28
50.0 ± 0.07
90.1 ± 1.61
64.2 ± 0.27
50.0 ± 0.08
69.9 ± 0.04
58.3 ± 0.07
50.2 ± 0.14
64.1 ± 16.7
54.3 ± 5.50
50.2 ± 0.21
69.9 ± 25.7
56.4 ± 9.27
0.51 ± 0.23
31.3 ± 13.6
1.01 ± 0.44
50.0 ± 0.03
90.4 ± 5.95
64.2 ± 1.59
50.5 ± 0.05
84.2 ± 4.05
62.7 ± 1.12
49.9 ± 0.07
56.5 ± 5.85
52.6 ± 2.47
50.0 ± 0.04
96.8 ± 2.22
66.0 ± 0.50
66.4 ± 2.38
50.0 ± 0.07
57.0 ± 0.84
46.5 ± 0.30
71.7 ± 9.54
56.2 ± 5.28
50.0 ± 0.01
97.0 ± 1.18
66.0 ± 0.30
50.0 ± 0.07
90.1 ± 1.61
64.2 ± 0.27
50.0 ± 0.08
69.9 ± 0.04
58.3 ± 0.07
EyePACS
56.6 ± 3.80
90.5 ± 9.58
69.1 ± 0.02
55.7 ± 3.69
93.3 ± 6.58
69.5 ± 1.04
99.9 ± 0.03
55.2 ± 0.51
71.1 ± 0.42
66.2 ± 2.80
94.4 ± 3.35
77.7 ± 0.80
60.3 ± 1.26
99.3 ± 0.10
75.0 ± 1.40
64.7 ± 2.41
94.4 ± 0.01
76.8 ± 1.70
58.9 ± 1.45
97.4 ± 2.53
73.4 ± 0.41
58.5 ± 0.72
99.9 ± 0.02
73.8 ± 0.57
57.2 ± 0.10
99.9 ± 0.01
72.8 ± 0.09
70.8 ± 3.23
93.7 ± 0.47
80.6 ± 1.90
62.9 ± 0.10
99.6 ± 0.17
77.1 ± 0.13
67.3 ± 2.38