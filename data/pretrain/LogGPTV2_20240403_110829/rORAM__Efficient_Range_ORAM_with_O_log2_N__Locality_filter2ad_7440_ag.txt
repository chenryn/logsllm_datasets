### Optimization of Access for a Range of Length r

Improvement in access efficiency for a range of length \( r \) can be achieved by breaking down the process into three main stages:

1. **Position Map Accesses**: Two position map accesses in the target ORAM tree.
2. **Range-r Read Operations**: Two range-\( r \) read operations in the target ORAM tree.
3. **Batch Evictions**: \( r \) batch evictions in each of the \( O(\log N) \) ORAM trees.

**Step 2** already incurs only \( O(\log N) \) seeks, which fits within the desired bound. For **Step 3**, approximately \( (\log N) / k \) ORAM trees are stored on each of the \( k \) disks, allowing batch evictions to occur in parallel and meet the stated bound.

The **position map access (Step 1)** is more challenging due to the sequential nature of recursion. The path to access in the next smaller recursive ORAM is only revealed once the path in the larger ORAM has been accessed, leading to \( O(\log^2 N) \) seeks. However, retrieving each bucket along the path is deterministic and can be completed using parallel seeks by distributing the levels of the recursive ORAMs across \( k \) disks. Fetching a single path at a single recursive level incurs \( O((\log N) / k) \) parallel seeks. Repeating this sequentially for each recursive level results in the cost stated above.

### Reducing Position Map Costs

**Parallel seeks** can enhance the performance of the position map, but other optimizations can further decrease the cost and improve overall performance, especially in scenarios with larger client storage.

- **Larger Block Sizes**: Larger block sizes improve the performance of the position map because the number of seeks for a single position map access is reduced to \( O(\log^2 N / \log B) \), where \( B \) is the block size. For example, with 4KB blocks and 1GB total storage, the number of recursive levels in any position map is just 2. More generally, if the block size \( B \) is large enough to store \( N^\alpha \) pointers for some constant \( 0 < \alpha < 1 \), the number of seeks per position map operation is \( O(\log N) \). In this setting, there are \( O(1) \) levels of recursion for the position map, and the total cost is \( O(1) \) with parallel seeks across levels.

- **Locally-Stored Position Maps**: If the position map can be stored locally in persistent storage, it offers several optimizations. First, a single global position map suffices instead of one for each tree. Second, locally-stored position maps can be reduced if smaller ranges are not supported. A position map for all \( O(\log N) \) ORAM trees could require \( O(N) \) local storage, but many stored positions are a result of tracking locations in the smaller range trees. By eliminating a small portion of the smaller range trees, the position map size is dramatically reduced without greatly affecting system functionality. For instance, with 1GB total data split into 4KB blocks, the total storage for a local position map is roughly 11MB. Removing the bottom 3 sub-ORAMs, reasonably requiring that all accesses are on ranges of at least 8 blocks, reduces the global position map size to less than 1MB.

### Revealing Operation Type

The security definition for range ORAM requires that any two access patterns with the same range sizes are indistinguishable, hiding the contents, addresses, and operation type of each access. Only the size of the range is leaked. An interesting security/performance tradeoff is to relax the definition to reveal the operation type (read or write) to an observer in addition to the range. This allows for the leakage of the direction of information flow, which may be acceptable in some situations.

If the operation type is leaked, the number of seeks per operation can be reduced to \( O(\log N) \) under the following conditions:
1. The position map seek cost is \( O(\log N) \) using ideas from the previous subsection.
2. The operation type (read or write) is revealed.
3. Each write operation is for a single block at a time.

In particular, such a construction still reads ranges but writes single blocks. This scenario is common and useful, for example, in searchable symmetric encryption (SSE) scenarios [13, 16].

- **O(\(\log N\)) Seeks per Read**: For reading a range of size \( r = 2^i \), two accesses occur on the ORAM tree \( R_i \) and \( r \) batch evictions in every tree. If the operation type is revealed, batch evictions on the other \( R_j \) trees (where \( j \neq i \)) do not need to occur because there is no update to the data blocks, reducing the seek cost to \( O(\log N) \).

- **O(\(\log N\)) Seeks per Write**: While writing a single item, the \( R_0 \) ORAM tree needs to be updated at a cost of \( O(\log N) \) seeks, and the modified item must also be updated in all other ORAM trees. If those evictions are performed immediately, the cost would be \( O(\log^2 N) \) seeks. However, because the write was only to a single block, we can delay those evictions by appending the updated block to each stash and performing a single batch eviction on one other tree, deterministically. With single-item writes, we can achieve \( O(\log N) \) seeks.

### Malicious Security

The rORAM construction, as described, is secure against an honest-but-curious adversary who always follows the protocols correctly but may observe and remember all communication and past states of the remote storage. Achieving a higher level of security against a malicious adversary who may change the contents of remote storage or otherwise disobey the protocol requires straightforward techniques for ensuring integrity [12, 41].

- **Merkle Trees**: A Merkle tree can be embedded within each individual ORAM tree to ensure integrity. However, to minimize the number of disk seeks, each ORAM tree node stores a separate hash of each child node. This way, updating a path in any of the ORAM trees only requires reading and re-writing the nodes in that path. The extra hashes introduce a small constant factor increase in bandwidth and remote storage size but do not change the number of seeks. The hashes are stored contiguously with the data.

- **Root Block**: The individual hashes of all \( O(\log N) \) ORAM trees are collected into a single "root block" of hashes, which is stored contiguously with the root node of any one of the ORAM trees. Reading the root block on every access does not introduce any extra seeks, and the client only needs to store the hash of this root block locally in persistent storage.

### Conclusion

rORAM is an ORAM specifically designed for accessing ranges of sequential logical blocks while minimizing the number of random physical disk seeks. It is significantly more efficient than prior designs, reducing the number of seeks and communication complexity by a factor of \( O(\log N) \). 

A rORAM implementation is 30-50x faster than Path ORAM for similar range-query workloads on local HDDs, 30x faster for local SSDs, and 10x faster for network block devices. rORAMâ€™s novel disk layout can also speed up standard ORAM constructions, resulting in a 2x faster Path ORAM variant. Experiments demonstrate its suitability for real-world applications, with rORAM being up to 5x faster running a file server and up to 11x faster running a range-query intensive video server workload compared to standard Path ORAM.

rORAM highlights the significant practical issue of data locality as an important factor in ORAM design. Even for ORAMs that do not naturally support range queries, locality can have a large impact on performance, and seek optimization should be a design criterion for future ORAM technology.

### Acknowledgments

This work is supported by the National Science Foundation under awards 1526707, 1526102, 1319994, 1406177, 1618269, and by the Office of Naval Research. We thank our shepherd, Dimitrios Papadopoulos, and the anonymous reviewers for their valuable suggestions and comments.

### References

[References listed as provided, with formatting and citation style maintained.]

This optimized version of the text aims to be more clear, coherent, and professional, with improved structure and readability.