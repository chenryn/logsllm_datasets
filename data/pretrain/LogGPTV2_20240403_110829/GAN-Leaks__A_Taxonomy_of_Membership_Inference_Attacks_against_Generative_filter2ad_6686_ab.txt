than not. Mathematically,
A(𝑥,M(𝜃)) = 1
(1)
where 1(·) is the indicator function, and the training set is denoted
by 𝐷train. We denote the query sample set as 𝑆 = {(𝑥𝑖, 𝑚𝑖)}𝑁
𝑖=1 that
contains both training set samples (𝑥𝑖 ∈ 𝐷train, 𝑚𝑖 = 1) as well as
hold-out set samples (𝑥𝑖 ∉ 𝐷train, 𝑚𝑖 = 0), where 𝑚 is the member-
ship indicator variable. The true positive and true negative rate of
the attacker can be measure by E𝑥𝑖 [𝑃(A(𝑥𝑖,M(𝜃)) = 1|𝑚𝑖 = 1)]
and E𝑥𝑖 [𝑃(A(𝑥𝑖,M(𝜃)) = 0|𝑚𝑖 = 0)], respectively.
4 TAXONOMY
The attack scenarios can be categorized into either white-box or
black-box one. In the white-box setting, the adversary has access
to the victim model internals, whereas in the black-box setting,
log 𝑃(𝑥 ∈ 𝐷train|𝑥, 𝜃)
𝑃(𝑥 ∉ 𝐷train|𝑥, 𝜃) ≥ 0
(cid:20)
(cid:21)
zGensamplesDisreal data (1) Full black-box generator (2) Partial black-box generator (3) White-box generator (4) Accessible discriminatorreal/fake?the internal workings are unknown to the attackers. For attacks
against GANs, we further distinguish the settings based on the
accessibility of GANs’ components, i.e., the latent code, generator
model, and the discriminator model, according to the following
criteria: (1) whether the discriminator is accessible, (2) whether the
generator is accessible, and (3) whether the latent code is accessible.
We elaborate on each category in the following in a decreasing
order of the amount of knowledge to attackers. Note that we define
the taxonomy in a fully attack-agnostic way, i.e. the attacker can
freely decide which part of the available information to use.
4.1 Accessible Discriminator (Full Model)
By construction, the discriminator is only used for the adversarial
training and normally will be discarded after the training stage is
completed. The only scenario in which the discriminator is accessi-
ble to the attacker is that the developers publish the whole GAN
model along with the source code and allow fine-tuning. In this
case, both the discriminator and the generator are accessible to the
adversary in a white-box manner. This is the most knowledgeable
setting for attackers. And the existing attack methods against dis-
criminative models [60] can be applied to this setting. This setting
is also considered in [25], corresponding to the last row in Table 1.
In practice, however, the discriminator of a well-trained GAN is
discarded without being deployed to APIs, and thus not accessible
to attackers. We, therefore, devote less effort to investigating the dis-
criminator and mainly focus on the following practical and generic
settings where the attackers only have access to the generator.
4.2 White-box Generator
Following the common practice, researchers from the generative
modeling community always publish their well-trained generators
and code, which allows users to generate new samples and validate
the results. This corresponds to the settings that the generator is
accessible to the adversary in a white-box manner, i.e. the attackers
have access to the internals of the generator. This scenario is also
commonly studied in the community of differential privacy [16]
and privacy preserving data generation [2, 7, 11, 35, 66, 73], where
people enforce privacy guarantee by training and sharing their gen-
erative models instead of sharing the raw private data. Our attack
model under this setting can serve as a practical tool for empirically
estimating the privacy risk incurred by sharing the differentially
private generative models, which offers clear interpretability to-
wards bridging between theory and practice. However, this setting
has not been explored by any previous work and is a novel case
for constructing a membership inference attack against GANs. It
corresponds to the second last row in Table 1 and Section 5.4.
4.3 Partial Black-box Generator (Known
Input-output Pair)
This is a less knowledgeable setting to attackers where they have
no access to the internals of the generator but have access to the
latent code of each generated sample. This is a practical setting
where the developers retain ownership of their well-trained models
while allowing users to control the properties of the generated
samples by manipulating the latent code distribution [32], which
is a desired feature for application scenarios such as GAN-based
Notation Description
A
M
𝐷train
𝑆
R
𝑥
𝑚
𝑧
G𝑣
G𝑟
𝜃𝑣
𝜃𝑟
Attacker
model publishing mechanism
Training set of the victim generator
Query set
Attacker’s reconstructor
Query sample
Membership indicator variable
Latent code (input to the generator)
Victim generator
Attacker’s reference generator, described in Section 5.6
Victim model’s parameter
Attacker’s reference model’s parameter
Table 2: Notations.
image processing [22] and facial attribute editing [27, 37]. This is
another novel setting and not considered in previous works [25, 29].
It corresponds to the third last row in Table 1 and Section 5.3.
4.4 Full Black-box Generator (Known Output
Only)
This is the least knowledgeable setting to attackers where they
are passive, i.e., unable to provide input, but are only permitted to
access the generated samples set from the well-trained black-box
generator. Hayes et al. [25] investigate attacks in this setting by
retraining a local copy of the victim model. Hilprecht et al. [29]
count the number of generated samples that are inside an 𝜖-ball of
the query, based on an elaborate design of distance metric. Our idea
is similar in spirit to Hilprecht et al. [29] but we score each query
by the reconstruction error directly, which does not introduce addi-
tional hyperparameter while achieving superior performance. In
short, we design a low-skill attack method with a simpler implemen-
tation (Section 5.2) that achieves comparable or better performance
(Section 6.3). Our attack and theirs correspond to the third, second,
and first rows in Table 1, respectively.
5 ATTACK MODEL
5.1 Generic Attack Model
As mentioned in Section 3.2, the optimal attacker computes the
probability 𝑃(𝑚𝑖 = 1|𝑥𝑖, 𝜃𝑣). Specifically for the generative model,
we make the assumption that this probability should be proportional
to the probability that the query sample can be generated by the
generator. This assumption holds in general as the generative model
is trained to approximate the training data distribution, i.e., 𝑃G𝑣 ≈
𝑃𝐷train where G𝑣 denotes the victim generator. And if the probability
that the query sample is generated by the victim generator is large,
it is more likely that the query sample is used to train the generative
model. Formally,
𝑃(𝑚𝑖 = 1|𝑥𝑖, 𝜃𝑣) ∝ 𝑃G𝑣 (𝑥|𝜃𝑣)
(2)
However, computing the exact probability is intractable as the
distribution of the generated data cannot be represented with an
explicit density function. Therefore, we adopt the Parzen window
(a) Generic attack model (Section 5.1)
(b) Full black-box attack (Section 5.2)
(c) Partial black-box and white-box attack
(Section 5.3 and Section 5.4)
(d) Attack calibration (Section 5.6)
Figure 2: Diagram of our attacks. Mathematical notations refer to Table 2. 𝑃 represents data distribution. 𝑥1 belongs to 𝐷train
so that it should be better represented by G𝑣 with a smaller distance to its reconstructed copy R(𝑥1|G𝑣). 𝑥2 does not belong to
𝐷train so that it should have a larger distance to its best approximation R(𝑥2|G𝑣) in 𝑃G𝑣 . (a) Our generic attacker set a decision
boundary based on the reconstruction distance to infer membership. (b) The best reconstruction is determined over random
samples from 𝑃G𝑣 while in (c) it is found by optimization on the manifold of 𝑃G𝑣 . (d) 𝑃G𝑟 is a third-party reference GAN
distribution where the reconstruction distance is calibrated by the distance between 𝑥 and R(𝑥|G𝑟).
density estimation [15] and approximate the probability as below,
𝑘
𝑘
𝑖=1
𝑖=1
𝑃G𝑣 (𝑥|𝜃𝑣) =
1
𝑘
≈ 1
𝑘
𝜙(𝑥, G𝑣(𝑧𝑖));
𝑧𝑖 ∼ 𝑃𝑧
exp(−𝐿(𝑥, G𝑣(𝑧𝑖)));
𝑧𝑖 ∼ 𝑃𝑧
(3)
(4)
where 𝜙(·, ·) denotes the kernel function, 𝐿(·, ·) is the general dis-
tance metric defined in Section 5.5, and 𝑘 is the number of samples.
Note that this can be further simplified and well approximated
using only few samples [9], as all of the terms in the summation
of Equation 3, except for a few, will be negligible since 𝜙(𝑥, 𝑦)
exponentially decreases with distance between 𝑥, 𝑦.
5.2 Full Black-box Attack
We start with the least knowledgeable setting where an attacker
only has access to a black-box generator G𝑣. The attacker is allowed
no other operation but blindly collecting 𝑘 samples from G𝑣, de-
noted as {G𝑣(·)𝑖}𝑘
𝑖=1. G𝑣(·) indicates that the attacker has neither
access nor control over latent code input. We then approximate the
probability in Equation 4 using the largest term which is given by
the nearest neighbor to 𝑥 among {G𝑣(·)𝑖}𝑘
𝑖=1. Formally,
𝐿(𝑥, ˆ𝑥)
(5)
R(𝑥|G𝑣) =
argmin
ˆ𝑥∈{G𝑣 (·)𝑖 }𝑘
𝑖=1
See Figure 2(b) for a diagram. This approximation bound the com-
plete Parzen window from below, but in practice we observe almost
no difference when incorporating more terms in the summation for
a fixed 𝑘. However, we find the estimation more sensitive to 𝑘, and
in general a larger 𝑘 leads to better reconstructions (Figure 10) but
at the price of a higher query and computation cost. Throughout
the experiments, we consider a practical and limited budget and
choose 𝑘 to be of the same magnitude as the training dataset size.
5.3 Partial Black-box Attack
In some practical scenario discussed in Section 4.3, the access to the
latent code 𝑧 is permitted. We then propose to exploit 𝑧 in order to
find a better reconstruction of the query sample and thus improve
the 𝑃G𝑣 (𝑥|𝜃𝑣) estimation. Concretely, the attacker performs an
black-box optimization with respect to 𝑧. Formally,
R(𝑥|G𝑣) = G𝑣(𝑧∗)
𝐿(cid:0)𝑥, G𝑣(𝑧)(cid:1)
(6)
where
𝑧∗ = argmin
𝑧
(7)
Without knowing the internals of G𝑣, the optimization is not
differentiable and no gradient information is available. As only
the evaluation of function (forward-pass through the generator) is
allowed by the access of {𝑧, G𝑣(𝑧)} pair, we propose to approximate
the optimum via the Powell’s Conjugate Direction Method [53].
5.4 White-box Attack
In the white-box setting, we have the same reconstruction for-
mulation as in Section 5.3. See Figure 2(c) for a diagram. More
advantageously to attackers, the reconstruction quality can be
further boosted thanks to access to the internals of G𝑣. With ac-
cess to the gradient information, the optimization problem can
be more accurately solved by advanced first-order optimization
algorithms [39, 46, 63]. In our experiment, we apply the L-BFGS
algorithm for its robustness against suboptimal initialization and
its superior convergence rate in comparison to the other methods.
5.5 Distance Metric
Our distance metric 𝐿(·, ·) consists of three terms: the element-wise
(pixel-wise) difference term 𝐿2 targets low-frequency components,
the deep image feature term 𝐿lpips (i.e., the Learned Perceptual
Image Patch Similarity (LPIPS) metric [72]) targets realism details,
and the regularization term penalizes latent code far from the prior
PGvPDtrainx1∈DtrainR(x1|Gv)x2/∈DtrainR(x2|Gv)AttackerdecisionboundaryPGvPDtrainx1∈DtrainR(x1|Gv)x2/∈DtrainR(x2|Gv)PGvPDtrain12345643543252345243534524312345643wrwerwer54x1∈DtrainR(x1|Gv)x2/∈DtrainR(x2|Gv)PGvPDtrainPGrx1∈DtrainR(x1|Gv)R(x1|Gr)x2/∈DtrainR(x2|Gv)R(x2|Gr)Figure 3: The effectiveness of calibration when attacking PGGAN on CelebA. The x- and y-axes respectively represent the
distance before (𝐿) and after calibration (𝐿cal) between a query sample 𝑥 and its reconstruction R(𝑥|G𝑣). 𝜖 and 𝜖cal are the
corresponding thresholds for classification. The false-positive (in purple frame) as well as the false-negative samples (in red
frame) before (𝐿) calibration can be corrected by calibration (𝐿cal).
distribution. Mathematically,
𝐿(cid:0)𝑥, G𝑣(𝑧)(cid:1) =𝜆1𝐿2(cid:0)𝑥, G𝑣(𝑧)(cid:1) + 𝜆2𝐿lpips(cid:0)𝑥, G𝑣(𝑧)(cid:1)
(8)
complicated appearances such that their reconstruction errors are
not high given arbitrary generators. In contrast, the false-negative
samples by 𝐿 on the right-hand side are those with more compli-
cated appearances such that their reconstruction errors are high
given arbitrary generators. Our calibration can effectively miti-
gate these two types of misclassification that depend on sample
representations.
As discussed in Section 3.2, the optimal attacker aims to compute
the membership probability
𝑃(𝑚𝑖 = 1|𝜃𝑣, 𝑥𝑖) = E𝑆 [𝑃(𝑚𝑖 = 1|𝜃𝑣, 𝑥𝑖, 𝑆)]
(12)
Specifically, inferring the membership of the query sample 𝑥𝑖 amounts
to approximating the value of 𝑃(𝑚𝑖 = 1|𝜃𝑣, 𝑥𝑖, 𝑆) [57]. We show
that our calibrated loss well approximate this probability by the
following theorem, whose proof is provided in Appendix.
Theorem 5.1. Given the victim model with parameter 𝜃𝑣, a query
dataset 𝑆, the membership probability of a query sample 𝑥𝑖 is well
approximated by the sigmoid of minus calibrated reconstruction error.
(13)
𝑃(𝑚𝑖 = 1|𝜃𝑣, 𝑥𝑖, 𝑆) ≈ 𝜎(−𝐿cal(𝑥𝑖, R(𝑥𝑖|G𝑣))
And the optimal attack is equivalent to
A(𝑥𝑖,M(𝜃𝑣)) = 1[𝐿cal(𝑥𝑖, R(𝑥𝑖|G𝑣)) < 𝜖]
(14)
i.e., the attacker checks whether the calibrated reconstruction error of
the query sample 𝑥𝑖 is smaller than a threshold 𝜖.
In the white-box case, the reference model has the same archi-
tecture as the victim model as this information is accessible to the
attacker. In the full black-box and partial black-box settings, G𝑟 has
irrelevant network architectures to G𝑣, which is fixed across attack
scenarios. The optimization on the well-trained G𝑟 is the same as
on the white-box G𝑣. See Figure 2(d) for a diagram, and Section 6.6
for implementation details.
6 EXPERIMENTS
Based on the proposed taxonomy, we present the most compre-
hensive evaluation to date on the membership inference attacks
where
+ 𝜆3𝐿reg(𝑧)
𝐿2(cid:0)𝑥, G𝑣(𝑧)(cid:1) = ∥𝑥 − G𝑣(𝑧)∥2
𝐿reg(𝑧) =(cid:0)∥𝑧∥2
2 − dim(𝑧)(cid:1)2
2
(9)
(10)
𝜆1, 𝜆2 and 𝜆3 are used to enable/disable and balance the order of
magnitude of each loss term. For non-image data, 𝜆2 = 0 because
LPIPS is no longer applicable. For full black-box attack, 𝜆3 = 0 as
the constraint 𝑧 ∼ 𝑃𝑧 is satisfied by the sampling process.
5.6 Attack Calibration
We noticed that the reconstruction error is query-dependent, i.e.,
some query samples are more (less) difficult to reconstruct due to
their intrinsically more (less) complicated representations, regard-
less of which generator is used. In this case, the reconstruction error
is dominated by the representations rather than by the membership
clues. We, therefore, propose to mitigate the query dependency by
first independently training a reference GAN G𝑟 with a relevant
but disjoint dataset, and then calibrating our base reconstruction
error according to the reference reconstruction error. Formally,
𝐿cal(cid:0)𝑥, R(𝑥|G𝑣)(cid:1) = 𝐿(cid:0)𝑥, R(𝑥|G𝑣)(cid:1) − 𝐿(cid:0)𝑥, R(𝑥|G𝑟)(cid:1)
(11)
with R the reconstruction. As demonstrated in Figure 3, we show
in the up-left quadrant the query samples in purple frame that are
classified as in 𝐷train by 𝐿 and as not in 𝐷train by 𝐿cal. They are
false-positive to 𝐿 but are corrected to true-negative by 𝐿cal. On
the other hand, we show in the bottom-right quadrant the query
samples in red frame that are classified as not in 𝐷train by 𝐿 and
as in 𝐷train by 𝐿cal. They are false-negative to 𝐿 but are corrected
to true-positive by 𝐿cal. We compare all these samples, their recon-
structions from the victim generator G𝑣, and their reconstructions
from the reference generator G𝑟 on the two sides of the plot. The
false-positive samples by 𝐿 on the left-hand side are those with less
x/∈DtrainR(x|Gv)R(x|Gr)L(x,R(x|Gv))Lcal(x,R(x|Gv))calx∈DtrainR(x|Gv)R(x|Gr)(a) PGGAN
(b) WGANGP
(c) DCGAN
(d) VAEGAN
(e) PGGAN w/ DP
Figure 4: Generated images from different victim GAN models trained on CelebA.
against deep generative models. While prior studies have singled
out few data sets from constraint domains on selected models, our
evaluation includes three diverse datasets, five different generative
models, and systematic analysis of attack vectors – including more
viable threat models. Via this approach, we present key discoveries,
that connect for the first time the effectiveness of the attacks to the
model types, data sets, and training configuration.
6.1 Setup
Datasets: We conduct experiments on three diverse modalities of
datasets covering images, medical records, and location check-ins,
which are considered with a high risk of privacy breach.
CelebA [47] is a large-scale face attributes dataset with 200k RGB
images. Images are aligned to each other based on facial landmarks,