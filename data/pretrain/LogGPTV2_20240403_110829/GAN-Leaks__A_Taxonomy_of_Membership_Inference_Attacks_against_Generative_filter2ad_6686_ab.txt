than not. Mathematically,
A(ğ‘¥,M(ğœƒ)) = 1
(1)
where 1(Â·) is the indicator function, and the training set is denoted
by ğ·train. We denote the query sample set as ğ‘† = {(ğ‘¥ğ‘–, ğ‘šğ‘–)}ğ‘
ğ‘–=1 that
contains both training set samples (ğ‘¥ğ‘– âˆˆ ğ·train, ğ‘šğ‘– = 1) as well as
hold-out set samples (ğ‘¥ğ‘– âˆ‰ ğ·train, ğ‘šğ‘– = 0), where ğ‘š is the member-
ship indicator variable. The true positive and true negative rate of
the attacker can be measure by Eğ‘¥ğ‘– [ğ‘ƒ(A(ğ‘¥ğ‘–,M(ğœƒ)) = 1|ğ‘šğ‘– = 1)]
and Eğ‘¥ğ‘– [ğ‘ƒ(A(ğ‘¥ğ‘–,M(ğœƒ)) = 0|ğ‘šğ‘– = 0)], respectively.
4 TAXONOMY
The attack scenarios can be categorized into either white-box or
black-box one. In the white-box setting, the adversary has access
to the victim model internals, whereas in the black-box setting,
log ğ‘ƒ(ğ‘¥ âˆˆ ğ·train|ğ‘¥, ğœƒ)
ğ‘ƒ(ğ‘¥ âˆ‰ ğ·train|ğ‘¥, ğœƒ) â‰¥ 0
(cid:20)
(cid:21)
zGensamplesDisreal data (1) Full black-box generator (2) Partial black-box generator (3) White-box generator (4) Accessible discriminatorreal/fake?the internal workings are unknown to the attackers. For attacks
against GANs, we further distinguish the settings based on the
accessibility of GANsâ€™ components, i.e., the latent code, generator
model, and the discriminator model, according to the following
criteria: (1) whether the discriminator is accessible, (2) whether the
generator is accessible, and (3) whether the latent code is accessible.
We elaborate on each category in the following in a decreasing
order of the amount of knowledge to attackers. Note that we define
the taxonomy in a fully attack-agnostic way, i.e. the attacker can
freely decide which part of the available information to use.
4.1 Accessible Discriminator (Full Model)
By construction, the discriminator is only used for the adversarial
training and normally will be discarded after the training stage is
completed. The only scenario in which the discriminator is accessi-
ble to the attacker is that the developers publish the whole GAN
model along with the source code and allow fine-tuning. In this
case, both the discriminator and the generator are accessible to the
adversary in a white-box manner. This is the most knowledgeable
setting for attackers. And the existing attack methods against dis-
criminative models [60] can be applied to this setting. This setting
is also considered in [25], corresponding to the last row in Table 1.
In practice, however, the discriminator of a well-trained GAN is
discarded without being deployed to APIs, and thus not accessible
to attackers. We, therefore, devote less effort to investigating the dis-
criminator and mainly focus on the following practical and generic
settings where the attackers only have access to the generator.
4.2 White-box Generator
Following the common practice, researchers from the generative
modeling community always publish their well-trained generators
and code, which allows users to generate new samples and validate
the results. This corresponds to the settings that the generator is
accessible to the adversary in a white-box manner, i.e. the attackers
have access to the internals of the generator. This scenario is also
commonly studied in the community of differential privacy [16]
and privacy preserving data generation [2, 7, 11, 35, 66, 73], where
people enforce privacy guarantee by training and sharing their gen-
erative models instead of sharing the raw private data. Our attack
model under this setting can serve as a practical tool for empirically
estimating the privacy risk incurred by sharing the differentially
private generative models, which offers clear interpretability to-
wards bridging between theory and practice. However, this setting
has not been explored by any previous work and is a novel case
for constructing a membership inference attack against GANs. It
corresponds to the second last row in Table 1 and Section 5.4.
4.3 Partial Black-box Generator (Known
Input-output Pair)
This is a less knowledgeable setting to attackers where they have
no access to the internals of the generator but have access to the
latent code of each generated sample. This is a practical setting
where the developers retain ownership of their well-trained models
while allowing users to control the properties of the generated
samples by manipulating the latent code distribution [32], which
is a desired feature for application scenarios such as GAN-based
Notation Description
A
M
ğ·train
ğ‘†
R
ğ‘¥
ğ‘š
ğ‘§
Gğ‘£
Gğ‘Ÿ
ğœƒğ‘£
ğœƒğ‘Ÿ
Attacker
model publishing mechanism
Training set of the victim generator
Query set
Attackerâ€™s reconstructor
Query sample
Membership indicator variable
Latent code (input to the generator)
Victim generator
Attackerâ€™s reference generator, described in Section 5.6
Victim modelâ€™s parameter
Attackerâ€™s reference modelâ€™s parameter
Table 2: Notations.
image processing [22] and facial attribute editing [27, 37]. This is
another novel setting and not considered in previous works [25, 29].
It corresponds to the third last row in Table 1 and Section 5.3.
4.4 Full Black-box Generator (Known Output
Only)
This is the least knowledgeable setting to attackers where they
are passive, i.e., unable to provide input, but are only permitted to
access the generated samples set from the well-trained black-box
generator. Hayes et al. [25] investigate attacks in this setting by
retraining a local copy of the victim model. Hilprecht et al. [29]
count the number of generated samples that are inside an ğœ–-ball of
the query, based on an elaborate design of distance metric. Our idea
is similar in spirit to Hilprecht et al. [29] but we score each query
by the reconstruction error directly, which does not introduce addi-
tional hyperparameter while achieving superior performance. In
short, we design a low-skill attack method with a simpler implemen-
tation (Section 5.2) that achieves comparable or better performance
(Section 6.3). Our attack and theirs correspond to the third, second,
and first rows in Table 1, respectively.
5 ATTACK MODEL
5.1 Generic Attack Model
As mentioned in Section 3.2, the optimal attacker computes the
probability ğ‘ƒ(ğ‘šğ‘– = 1|ğ‘¥ğ‘–, ğœƒğ‘£). Specifically for the generative model,
we make the assumption that this probability should be proportional
to the probability that the query sample can be generated by the
generator. This assumption holds in general as the generative model
is trained to approximate the training data distribution, i.e., ğ‘ƒGğ‘£ â‰ˆ
ğ‘ƒğ·train where Gğ‘£ denotes the victim generator. And if the probability
that the query sample is generated by the victim generator is large,
it is more likely that the query sample is used to train the generative
model. Formally,
ğ‘ƒ(ğ‘šğ‘– = 1|ğ‘¥ğ‘–, ğœƒğ‘£) âˆ ğ‘ƒGğ‘£ (ğ‘¥|ğœƒğ‘£)
(2)
However, computing the exact probability is intractable as the
distribution of the generated data cannot be represented with an
explicit density function. Therefore, we adopt the Parzen window
(a) Generic attack model (Section 5.1)
(b) Full black-box attack (Section 5.2)
(c) Partial black-box and white-box attack
(Section 5.3 and Section 5.4)
(d) Attack calibration (Section 5.6)
Figure 2: Diagram of our attacks. Mathematical notations refer to Table 2. ğ‘ƒ represents data distribution. ğ‘¥1 belongs to ğ·train
so that it should be better represented by Gğ‘£ with a smaller distance to its reconstructed copy R(ğ‘¥1|Gğ‘£). ğ‘¥2 does not belong to
ğ·train so that it should have a larger distance to its best approximation R(ğ‘¥2|Gğ‘£) in ğ‘ƒGğ‘£ . (a) Our generic attacker set a decision
boundary based on the reconstruction distance to infer membership. (b) The best reconstruction is determined over random
samples from ğ‘ƒGğ‘£ while in (c) it is found by optimization on the manifold of ğ‘ƒGğ‘£ . (d) ğ‘ƒGğ‘Ÿ is a third-party reference GAN
distribution where the reconstruction distance is calibrated by the distance between ğ‘¥ and R(ğ‘¥|Gğ‘Ÿ).
density estimation [15] and approximate the probability as below,
ğ‘˜
ğ‘˜
ğ‘–=1
ğ‘–=1
ğ‘ƒGğ‘£ (ğ‘¥|ğœƒğ‘£) =
1
ğ‘˜
â‰ˆ 1
ğ‘˜
ğœ™(ğ‘¥, Gğ‘£(ğ‘§ğ‘–));
ğ‘§ğ‘– âˆ¼ ğ‘ƒğ‘§
exp(âˆ’ğ¿(ğ‘¥, Gğ‘£(ğ‘§ğ‘–)));
ğ‘§ğ‘– âˆ¼ ğ‘ƒğ‘§
(3)
(4)
where ğœ™(Â·, Â·) denotes the kernel function, ğ¿(Â·, Â·) is the general dis-
tance metric defined in Section 5.5, and ğ‘˜ is the number of samples.
Note that this can be further simplified and well approximated
using only few samples [9], as all of the terms in the summation
of Equation 3, except for a few, will be negligible since ğœ™(ğ‘¥, ğ‘¦)
exponentially decreases with distance between ğ‘¥, ğ‘¦.
5.2 Full Black-box Attack
We start with the least knowledgeable setting where an attacker
only has access to a black-box generator Gğ‘£. The attacker is allowed
no other operation but blindly collecting ğ‘˜ samples from Gğ‘£, de-
noted as {Gğ‘£(Â·)ğ‘–}ğ‘˜
ğ‘–=1. Gğ‘£(Â·) indicates that the attacker has neither
access nor control over latent code input. We then approximate the
probability in Equation 4 using the largest term which is given by
the nearest neighbor to ğ‘¥ among {Gğ‘£(Â·)ğ‘–}ğ‘˜
ğ‘–=1. Formally,
ğ¿(ğ‘¥, Ë†ğ‘¥)
(5)
R(ğ‘¥|Gğ‘£) =
argmin
Ë†ğ‘¥âˆˆ{Gğ‘£ (Â·)ğ‘– }ğ‘˜
ğ‘–=1
See Figure 2(b) for a diagram. This approximation bound the com-
plete Parzen window from below, but in practice we observe almost
no difference when incorporating more terms in the summation for
a fixed ğ‘˜. However, we find the estimation more sensitive to ğ‘˜, and
in general a larger ğ‘˜ leads to better reconstructions (Figure 10) but
at the price of a higher query and computation cost. Throughout
the experiments, we consider a practical and limited budget and
choose ğ‘˜ to be of the same magnitude as the training dataset size.
5.3 Partial Black-box Attack
In some practical scenario discussed in Section 4.3, the access to the
latent code ğ‘§ is permitted. We then propose to exploit ğ‘§ in order to
find a better reconstruction of the query sample and thus improve
the ğ‘ƒGğ‘£ (ğ‘¥|ğœƒğ‘£) estimation. Concretely, the attacker performs an
black-box optimization with respect to ğ‘§. Formally,
R(ğ‘¥|Gğ‘£) = Gğ‘£(ğ‘§âˆ—)
ğ¿(cid:0)ğ‘¥, Gğ‘£(ğ‘§)(cid:1)
(6)
where
ğ‘§âˆ— = argmin
ğ‘§
(7)
Without knowing the internals of Gğ‘£, the optimization is not
differentiable and no gradient information is available. As only
the evaluation of function (forward-pass through the generator) is
allowed by the access of {ğ‘§, Gğ‘£(ğ‘§)} pair, we propose to approximate
the optimum via the Powellâ€™s Conjugate Direction Method [53].
5.4 White-box Attack
In the white-box setting, we have the same reconstruction for-
mulation as in Section 5.3. See Figure 2(c) for a diagram. More
advantageously to attackers, the reconstruction quality can be
further boosted thanks to access to the internals of Gğ‘£. With ac-
cess to the gradient information, the optimization problem can
be more accurately solved by advanced first-order optimization
algorithms [39, 46, 63]. In our experiment, we apply the L-BFGS
algorithm for its robustness against suboptimal initialization and
its superior convergence rate in comparison to the other methods.
5.5 Distance Metric
Our distance metric ğ¿(Â·, Â·) consists of three terms: the element-wise
(pixel-wise) difference term ğ¿2 targets low-frequency components,
the deep image feature term ğ¿lpips (i.e., the Learned Perceptual
Image Patch Similarity (LPIPS) metric [72]) targets realism details,
and the regularization term penalizes latent code far from the prior
PGvPDtrainx1âˆˆDtrainR(x1|Gv)x2/âˆˆDtrainR(x2|Gv)AttackerdecisionboundaryPGvPDtrainx1âˆˆDtrainR(x1|Gv)x2/âˆˆDtrainR(x2|Gv)PGvPDtrain12345643543252345243534524312345643wrwerwer54x1âˆˆDtrainR(x1|Gv)x2/âˆˆDtrainR(x2|Gv)PGvPDtrainPGrx1âˆˆDtrainR(x1|Gv)R(x1|Gr)x2/âˆˆDtrainR(x2|Gv)R(x2|Gr)Figure 3: The effectiveness of calibration when attacking PGGAN on CelebA. The x- and y-axes respectively represent the
distance before (ğ¿) and after calibration (ğ¿cal) between a query sample ğ‘¥ and its reconstruction R(ğ‘¥|Gğ‘£). ğœ– and ğœ–cal are the
corresponding thresholds for classification. The false-positive (in purple frame) as well as the false-negative samples (in red
frame) before (ğ¿) calibration can be corrected by calibration (ğ¿cal).
distribution. Mathematically,
ğ¿(cid:0)ğ‘¥, Gğ‘£(ğ‘§)(cid:1) =ğœ†1ğ¿2(cid:0)ğ‘¥, Gğ‘£(ğ‘§)(cid:1) + ğœ†2ğ¿lpips(cid:0)ğ‘¥, Gğ‘£(ğ‘§)(cid:1)
(8)
complicated appearances such that their reconstruction errors are
not high given arbitrary generators. In contrast, the false-negative
samples by ğ¿ on the right-hand side are those with more compli-
cated appearances such that their reconstruction errors are high
given arbitrary generators. Our calibration can effectively miti-
gate these two types of misclassification that depend on sample
representations.
As discussed in Section 3.2, the optimal attacker aims to compute
the membership probability
ğ‘ƒ(ğ‘šğ‘– = 1|ğœƒğ‘£, ğ‘¥ğ‘–) = Eğ‘† [ğ‘ƒ(ğ‘šğ‘– = 1|ğœƒğ‘£, ğ‘¥ğ‘–, ğ‘†)]
(12)
Specifically, inferring the membership of the query sample ğ‘¥ğ‘– amounts
to approximating the value of ğ‘ƒ(ğ‘šğ‘– = 1|ğœƒğ‘£, ğ‘¥ğ‘–, ğ‘†) [57]. We show
that our calibrated loss well approximate this probability by the
following theorem, whose proof is provided in Appendix.
Theorem 5.1. Given the victim model with parameter ğœƒğ‘£, a query
dataset ğ‘†, the membership probability of a query sample ğ‘¥ğ‘– is well
approximated by the sigmoid of minus calibrated reconstruction error.
(13)
ğ‘ƒ(ğ‘šğ‘– = 1|ğœƒğ‘£, ğ‘¥ğ‘–, ğ‘†) â‰ˆ ğœ(âˆ’ğ¿cal(ğ‘¥ğ‘–, R(ğ‘¥ğ‘–|Gğ‘£))
And the optimal attack is equivalent to
A(ğ‘¥ğ‘–,M(ğœƒğ‘£)) = 1[ğ¿cal(ğ‘¥ğ‘–, R(ğ‘¥ğ‘–|Gğ‘£)) < ğœ–]
(14)
i.e., the attacker checks whether the calibrated reconstruction error of
the query sample ğ‘¥ğ‘– is smaller than a threshold ğœ–.
In the white-box case, the reference model has the same archi-
tecture as the victim model as this information is accessible to the
attacker. In the full black-box and partial black-box settings, Gğ‘Ÿ has
irrelevant network architectures to Gğ‘£, which is fixed across attack
scenarios. The optimization on the well-trained Gğ‘Ÿ is the same as
on the white-box Gğ‘£. See Figure 2(d) for a diagram, and Section 6.6
for implementation details.
6 EXPERIMENTS
Based on the proposed taxonomy, we present the most compre-
hensive evaluation to date on the membership inference attacks
where
+ ğœ†3ğ¿reg(ğ‘§)
ğ¿2(cid:0)ğ‘¥, Gğ‘£(ğ‘§)(cid:1) = âˆ¥ğ‘¥ âˆ’ Gğ‘£(ğ‘§)âˆ¥2
ğ¿reg(ğ‘§) =(cid:0)âˆ¥ğ‘§âˆ¥2
2 âˆ’ dim(ğ‘§)(cid:1)2
2
(9)
(10)
ğœ†1, ğœ†2 and ğœ†3 are used to enable/disable and balance the order of
magnitude of each loss term. For non-image data, ğœ†2 = 0 because
LPIPS is no longer applicable. For full black-box attack, ğœ†3 = 0 as
the constraint ğ‘§ âˆ¼ ğ‘ƒğ‘§ is satisfied by the sampling process.
5.6 Attack Calibration
We noticed that the reconstruction error is query-dependent, i.e.,
some query samples are more (less) difficult to reconstruct due to
their intrinsically more (less) complicated representations, regard-
less of which generator is used. In this case, the reconstruction error
is dominated by the representations rather than by the membership
clues. We, therefore, propose to mitigate the query dependency by
first independently training a reference GAN Gğ‘Ÿ with a relevant
but disjoint dataset, and then calibrating our base reconstruction
error according to the reference reconstruction error. Formally,
ğ¿cal(cid:0)ğ‘¥, R(ğ‘¥|Gğ‘£)(cid:1) = ğ¿(cid:0)ğ‘¥, R(ğ‘¥|Gğ‘£)(cid:1) âˆ’ ğ¿(cid:0)ğ‘¥, R(ğ‘¥|Gğ‘Ÿ)(cid:1)
(11)
with R the reconstruction. As demonstrated in Figure 3, we show
in the up-left quadrant the query samples in purple frame that are
classified as in ğ·train by ğ¿ and as not in ğ·train by ğ¿cal. They are
false-positive to ğ¿ but are corrected to true-negative by ğ¿cal. On
the other hand, we show in the bottom-right quadrant the query
samples in red frame that are classified as not in ğ·train by ğ¿ and
as in ğ·train by ğ¿cal. They are false-negative to ğ¿ but are corrected
to true-positive by ğ¿cal. We compare all these samples, their recon-
structions from the victim generator Gğ‘£, and their reconstructions
from the reference generator Gğ‘Ÿ on the two sides of the plot. The
false-positive samples by ğ¿ on the left-hand side are those with less
x/âˆˆDtrainR(x|Gv)R(x|Gr)L(x,R(x|Gv))Lcal(x,R(x|Gv))calxâˆˆDtrainR(x|Gv)R(x|Gr)(a) PGGAN
(b) WGANGP
(c) DCGAN
(d) VAEGAN
(e) PGGAN w/ DP
Figure 4: Generated images from different victim GAN models trained on CelebA.
against deep generative models. While prior studies have singled
out few data sets from constraint domains on selected models, our
evaluation includes three diverse datasets, five different generative
models, and systematic analysis of attack vectors â€“ including more
viable threat models. Via this approach, we present key discoveries,
that connect for the first time the effectiveness of the attacks to the
model types, data sets, and training configuration.
6.1 Setup
Datasets: We conduct experiments on three diverse modalities of
datasets covering images, medical records, and location check-ins,
which are considered with a high risk of privacy breach.
CelebA [47] is a large-scale face attributes dataset with 200k RGB
images. Images are aligned to each other based on facial landmarks,