ponent must use the interfaces it exposes (or the ones
it expects Internet Explorer to expose) in order to ex-
tract information about the user at runtime. These in-
terfaces are well documented and are essentially a con-
tract between the COM client and the COM server. Since
we control our WebBrowser component, we see all the
interface calls and the queries for different interfaces.
Just as our a priori list of suspicious Windows API
calls is subject to change as we discover new suspicious
calls, so are the various COM functions and interfaces
used. This contract also applies to events.
If a com-
ponent wants to receive events from the WebBrowser
USENIX Association
Security ’06: 15th USENIX Security Symposium
281
it must call the Advise function on the WebBrowser’s
IConnectionPoint interface. Since we are in con-
trol of our WebBrowser object we can monitor calls to
this interface and reliably discover if a component is in-
terested in events and, if it is, the address of the function
that handles those events.
Recalling our behavioral characterization of spyware,
we note that a BHO or toolbar component must both col-
lect user data via browser functions and leak this infor-
mation to the adversary via Windows API calls. Thus, to
evade detection, a malware author could either attempt to
hide the fact that the BHO monitors user data via browser
COM calls, or disguise the fact that the collected data is
leaked.
Covering the footprints that indicate user data is being
collected is likely the more difﬁcult task. We use dy-
namic analysis to monitor all the functions that the BHO
component invokes in our web browser. To avoid invok-
ing the browser functions, a spyware component could
attempt to read interesting user data directly from mem-
ory. This is possible because both the BHO and the web
browser share the same address space. However, this is
difﬁcult because a non-standard access to memory re-
gions in a complex and undocumented COM application,
such as Internet Explorer, is not likely to yield a robust
or portable monitoring mechanism. Thus, reading data
directly from memory is not considered to be a viable
approach.
A more promising venue for a spyware component to
evade our detection is to attempt to conceal the fact that
data is leaked to a third-party via API calls. We have
previously mentioned the possible existence of covert
channels, and concluded that their treatment is outside
the scope of this paper. However, a spyware component
could attempt to leak information using means other than
API calls, or it could prevent the static analysis process
from ﬁnding their invocations in the code of the BHO.
One possible way to leak information without using
the Windows API is to make use of the functionality of-
fered by Internet Explorer itself. For example, a spy-
ware component could use the Internet Explorer API to
request a web resource on a server under the control of
the attacker. Sensitive information could be transmitted
as a parameter of the URL in this request. The current
limitation of not taking browser calls into account can
be addressed in two ways. First, we could extend the
static analysis step to also ﬂag certain COM calls to the
browser as suspicious. The problem with this solution
is that COM calls are invoked via function pointers and,
thus, are not easily resolvable statically. The second pos-
sibility would be to extend the dynamic analysis step. We
already record the browser functions that a BHO invokes
to determine when user data could be leaked. Thus, it
would be straightforward to additionally take into con-
sideration browser calls that a component invokes after
user data has been requested. However, for this, one has
to enlarge the set of test inputs used for the dynamic anal-
ysis step to ensure better test coverage.
As mentioned previously, another evasion venue is to
craft the BHO code such that it can resist static analy-
sis. Static analysis can be frustrated by employing anti-
disassembling mechanisms [13], or code obfuscation. If
these techniques are used, then our static analysis step
could be forced into missing critical Windows API calls
that must be recognized as suspicious. Again, we have
two options to deal with this problem. First, the static
analysis step could be made more robust to tolerate ob-
fuscation (e.g., by using a disassembler that handles anti-
disassembler transformations [12]). Also, strong obfus-
cation typically leads to disassembly errors that in itself
can be taken as sufﬁcient evidence to classify a compo-
nent as spyware. A second approach is to expand the dy-
namic analysis step to also monitor Windows API func-
tions. This could be achieved by hooking interesting API
calls [10] before the spyware component is executed. Us-
ing these hooks, all Windows API calls made by the spy-
ware component could be observed. Again, the set of test
data would have to be enlarged to improve test coverage.
While there are a number of possible ways that a spy-
ware component could attempt to evade our current de-
tection system, we have shown how to counter these
threats. Furthermore, in the next section we show how
in its current form our system was successful in correctly
identifying all spyware components that we were able
to collect. Thus, our proposed techniques signiﬁcantly
raise the bar for spyware authors with respect to tradi-
tional signature based techniques.
9 Evaluation
In order to verify the effectiveness of our behavior-based
spyware detection technique, we analyzed a total of 51
samples (33 malicious and 18 benign); 34 of them were
BHOs and 17 were toolbars. The process of collecting
these samples in the “wild” is both a tedious and non-
trivial task. This is conﬁrmed by a recent study [3] in
which the authors traversed 18,237,103 URLs discov-
ering 21,200 executables, of which there were just 82
unique instances of spyware as identiﬁed by popular spy-
ware scanners. The problem is further exacerbated by the
fact that popular spyware dominates the set of infected
ﬁles, making it hard to obtain a well rounded collection.
Thus, we obtained all of the malicious samples in our ﬁ-
nal test set from an anti-virus company and collected all
of the benign samples from various shareware download
sites. Note that we picked all samples (benign and mali-
cious) that we collected and that either registered them-
selves as BHOs or as toolbars. While collecting the be-
282
Security ’06: 15th USENIX Security Symposium
USENIX Association
nign samples, we veriﬁed that the applications were in-
deed benign by checking both anti-spyware vendor and
software review web sites. Furthermore, we selected
samples from different application areas, including anti-
spyware utilities, automated form-ﬁllers, search toolbars,
and privacy protectors. Note that our tool was developed
while analyzing only seven (two benign and ﬁve mali-
cious) samples from our ﬁnal test set. The remaining
samples were effectively unknown, with respect to our
tool, thereby validating the effectiveness of our charac-
terization on new and previously unseen malware com-
ponents. Given the difﬁculty of collecting samples, we
consider this to be a well rounded and signiﬁcant sample
set with which to evaluate our technique.
Table 1 presents our detection results in terms of both
correctly and incorrectly classiﬁed samples. In addition
to the detection results for our proposed combined ap-
proach, this table also includes the results that are achiev-
able when taking into account the information provided
by only the static analysis or only the dynamic analy-
sis step. In particular, we show detection results when
the classiﬁcation is solely based on statically analyzing
all API calls invoked by the sample (Strategy 1) or only
those API calls in response to events (Strategy 2). More-
over, we present the results obtained when a BHO or
toolbar sample is classiﬁed as spyware if it subscribes to
browser events (Strategy 3) or solely based on its inter-
action with the browser via COM functions (Strategy 4).
Finally, Strategy 5 implements our proposed detection
technique, which uses a composition of static and dy-
namic analysis. The aim is to demonstrate that the com-
bined analysis is indeed necessary to achieve the best re-
sults.
Given our detection results, it can be seen that mali-
cious spyware samples are correctly classiﬁed by all ﬁve
strategies, even the most simple one. Since every strat-
egy focuses on the identiﬁcation of one behavioral aspect
present in our characterization of spyware, these results
indicate that the proposed characterization appears to ac-
curately reﬂect the actual functioning of spyware. How-
ever, simple strategies also raise a signiﬁcant number of
false alarms. The reason is that certain behavioral as-
pects of spyware are also exhibited by benign samples. In
the following paragraphs, we discuss in more detail why
different detection strategies incorrectly classify certain
samples as malicious. The discussion sheds some light
on the shortcomings of individual strategies and moti-
vates the usage of all available detection features.
As mentioned in Section 4, we need a list of Win-
dows API calls that contains all suspicious functions that
can be used by a spyware component to leak informa-
tion to the attacker. As a ﬁrst step, we manually assem-
bled this list by going through the Windows API calls, in
particular focusing on functions responsible for handling
network I/O, ﬁle system access, process control, and the
Windows registry. Figure 2 shows an excerpt of the 59
suspicious calls that were selected. The calls that are de-
picted are representative of commonly used registry, ﬁle
access, and networking functions.
The ﬁrst detection strategy (Strategy 1) uses the list of
suspicious API functions to statically detect spyware. To
this end, static analysis is used to extract all API calls
that a sample could invoke, independent of events. This
can be done in a straightforward fashion, using available
tools such as PEDump [18]. Then, the extracted API
calls are compared to the list of suspicious functions. A
sample is classiﬁed as spyware if one or more of the sam-
ple’s API calls are considered suspicious.
Using the ﬁrst strategy, all benign samples are incor-
rectly detected as spyware. In many cases, samples re-
quire Windows registry, ﬁle, or network access during
the startup and initialization phase. In other cases, be-
nign samples such as the Google search toolbar use sus-
picious calls such as InternetConnectA to connect
to the Internet (in the case of the Google toolbar, the sam-
ple sends search queries to Google). However, such calls
are typically not done in response to events; in fact, many
samples do not even register for browser events.
If we restrict the static analysis to only those Win-
dows API calls that are invoked in response to browser
events, only ﬁve of the 18 benign samples are incorrectly
classiﬁed (Strategy 2). Two of these false positives are
easy to explain. One is a BHO called Airoboform,
a tool that supports users by ﬁlling in web forms auto-
matically.
In response to every event that signals that
a new page is loaded, this tool scans the page for web
forms. If necessary, it loads previously provided content
from a ﬁle to ﬁll in forms or it stores the current form
content to this ﬁle. Because web forms can also contain
sensitive information (such as passwords), one can argue
that Airoboform actually behaves in a way that is very
similar to a spyware application. The only exception is
that in the case of spyware, the ﬁle content would prob-
ably be transmitted to an attacker through an additional
helper process.
Besides Airoboform,
the benign Privacy
Preferences Project (P3P) Client BHO
also exhibits spyware-like behavior. P3P is emerging
as an industry standard for providing a simple and
automated way for users to control the use of their
personal information on web sites they visit. To this end,
the P3P Client has to check the P3P settings of every
web page that is visited. More precisely, whenever the
user visits a web site, the BHO connects to that site and
tries to retrieve its P3P-speciﬁc privacy policy. This is
implemented by opening a connection via the Windows
API function InternetConnectA in response to the
event that indicates that a document has been loaded.
USENIX Association
Security ’06: 15th USENIX Security Symposium
283
Detection Strategy
1. All Windows API calls (static)
2. Windows API calls in response to events (static)
3. Subscription to browser events (dynamic)
4. Browser COM method invocations (dynamic)
5. Combined static and dynamic analysis
Spyware Components
Incorrect
Correct
0
33
0
33
33
0
0
33
33
0
Benign Components
Incorrect
Correct
18
0
5
13
10
8
3
15
16
2
Table 1: Results for different detection strategies.
Figure 2: Excerpt of a priori assembled list of suspicious Windows API calls.
Two other
false positives are Spybot and the
T-Online toolbar. In both cases, the static analysis re-
sults indicate that a suspicious WriteFile call could
be invoked in response to some events. This would allow
the browser extensions to write event-speciﬁc informa-
tion into a ﬁle for later retrieval. Although writes to a
ﬁle are generally suspicious in response to events, there
are also cases in which such an action is legitimate. For
example, we discovered that the T-Online toolbar, a
benign application that allows users to send SMS mes-
sages, uses a caching mechanism to store images in ﬁles.
Spybot, a benign anti-spyware application, uses black
lists to block web access to spyware distribution sites
and keeps a cache to track cookies. The ﬁfth false posi-
tive is Microgarden, a BHO that extends Internet Ex-
plorer with the ability to open multiple tabs in a single
browser window. Although no suspicious API calls are
invoked directly in response to events, this BHO makes
use of timers. As a result, we have to conservatively con-
sider all Windows API calls that this sample can possibly
call (among which, a number of suspicious functions are
found). The last three false positive examples suggest
that the static analysis of Windows API calls may not de-
liver optimal detection results. Instead, one should seek
to combine the results of our static analysis with those of
our dynamic analysis to lower the number of false posi-
tives.
Taking a step back, a simple dynamic technique to
identify spyware (Strategy 3) is to classify all BHO and
toolbar components as malicious if they register as event
sinks. As expected, all of the spyware samples receive
browser events from Internet Explorer to monitor user
behavior.
In comparison, only eight of the 18 benign
samples registered as event sinks. This observation sug-
gests that many benign applications use BHO and toolbar
extensions to improve Internet Explorer, but do not need
to listen to events to implement their functionality. On
the other hand, nearly half of the benign samples also
use event information, for example, to display or modify
the source of visited pages or to block pop-up windows.
For Strategy 4, the dynamic analysis is extended to
monitor the interaction of the BHO or toolbar with the