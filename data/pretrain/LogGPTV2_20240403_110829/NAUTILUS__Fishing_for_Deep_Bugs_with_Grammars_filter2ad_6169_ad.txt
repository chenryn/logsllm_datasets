TABLE II: Statistics about branch coverage. The new coverage found is the additional coverage that was found by the fuzzer w.r.t. the initial corpus. “pp” stands
for “percentage points”. Note: AFL was able to ﬁnd some coverage on PHP, but the results round to zero.
ChakraCore: ChakraCore is the JavaScript engine used by
the web browser Edge [2]. Our fuzzer identiﬁed two bugs
in ChakraCore: one bug in the Just In Time (JIT) compiler
where the wrong number of arguments were emitted for a
function call, and a segmentation fault caused by out of
memory conditions. The bug only affected the Linux branch of
ChakraCore, and therefore was not eligible for a bug bounty.
Lua: Lua is a lightweight, embeddable scripting language
[13]. NAUTILUS identiﬁed a bug caused by a type confusion,
which causes a crash in Lua. The issue was reported to the
Lua mailing list. Our example input relies on a debug feature,
therefore the bug was not considered a security issue.
C. Evaluation Against Other State-of-the-art Fuzzers
To answer RQ2, we ran our tool NAUTILUS and other
state-of-the-art fuzzers on the four targets mentioned in Sec-
tion VI-B over multiple runs with identical durations. To
measure how much of an application’s code was tested by the
fuzzer we use branch coverage, i.e., the percentage of branches
of the applications that were executed at least once during the
fuzzing run. A fuzzer which achieves a high code coverage can
often also identify more bugs, since it executes more possibly
faulty code.
In order to evaluate our approach, we compared to two
state-of-the-art fuzzers using different approaches, namely,
AFL [19] for feedback-directed fuzzing and IFuzzer [47] for
grammar-based fuzzing. Since IFuzzer is not as ﬂexible as AFL
and NAUTILUS and only supports JavaScript, we only tested it
on ChakraCore. We provided AFL with a dictionary contain-
ing the same strings we used for the NAUTILUS grammars.
Moreover, for each target, we generated 1000 inputs from the
grammar, and provided them as a seed corpus. We chose to
run AFL with the generated inputs after we veriﬁed this corpus
lets AFL discover signiﬁcantly more code than 10 hand-picked
examples containing real-world code. We also provided the
same corpus to IFuzzer. We used the naive generation mode
on NAUTILUS, AFL version 2.52b, and IFuzzer from commit
8debd78. We ran each conﬁguration of fuzzer and target 20
times for 24 hours each.
To avoid relying on the different internal reporting methods
of the fuzzers, we conﬁgured them to save all interesting inputs
as ﬁles3 and we measured the branch coverage using standard
code coverage tools, namely GCOV [4] and Clang’s Source-
based Code Coverage [3].
The results of our experiments are summarized in Table II
and displayed in Figure 3. The baseline denotes the coverage
that was found by our generated corpus itself with no fuzzer
interaction. NAUTILUS is able to ﬁnd signiﬁcantly higher
amounts of additional coverage: while AFL and IFuzzer ﬁnd
between 0 and 1.9 percentage points of additional coverage on
ChakraCore, mruby and PHP, NAUTILUS discovers between
10.0 and 28.1 percentage points of additional coverage. For
Lua, AFL discovers 15.2 additional percentage points, while
NAUTILUS discovers 27.2 additional percentage points. As we
provide AFL with a good dictionary and an extensive corpus
of cases to learn from, we consider this setup as a very strong
baseline and a signiﬁcant bar to meet. This is also evident by
the fact that, even though IFuzzer is based on a good grammar,
it barely exceeds the performance of AFL.
We performed a Mann-Whitney U test as recommended by
Arcui et al. [21] to ensure statistical signiﬁcance and we report
the results in Table III. In all cases, our worst run was better
than the best run of all other tools. Due to this, the p-Values
obtained by the U test are extremely small, and we can exclude
the possibility that the observed differences are the result of
random chance. All statistics were computed using the Python
scipy [38] and numpy [40] libraries.
To address the relative merit of grammar-based input gener-
ation (RQ3) and feedback-directed fuzzing (RQ4) we disabled
the coverage feedback mechanism in NAUTILUS and we ran it
in the same environment as the full version. This experiment
is meant
the combination of feedback and
grammar fuzzing does indeed create a signiﬁcant performance
advantage, everything else being equal (performance of the
implementation and grammars). We consider this experiment
as a proxy to evaluate fuzzers such as Peach [10] or Sulley
to prove that
3Since IFuzzer does not support saving input cases, we modiﬁed it slightly
to add this functionality.
9
Fig. 3: Branch coverage generated by our corpus of 1000 generated inputs (Baseline) and by the different fuzzers over 20 runs of 24 hours each.
Experiment
ChakraCore (vs. AFL)
ChakraCore (vs. IFuzzer)
mruby (vs. AFL)
PHP (vs. AFL)
Lua (vs. AFL)
Effect size
(∆ = ¯A − ¯B)
18.3 pp
18.1 pp
26.2 pp
10.0 pp
12.0 pp
p-Value
(×10−6)
0.033
1.6
0.033
0.017
0.033
TABLE III: Conﬁrmatory data analysis of our experiments. The effect size is
the difference of the medians in percentage points. Due to storage require-
ments, we only performed 12 runs for IFuzzer, hence the slightly higher p-
Value. In the case of PHP, AFL generated the exact same coverage multiple
times, which explains the slight change in the p-Value compared to the other
conﬁgurations. In all cases the effect size is relevant and the changes are
highly signiﬁcant: the p-Value is about ten thousand times smaller than the
usual bound of p < 0.05.
[17]. We could not directly evaluate these tools since they need
manually written generators instead of grammars.
The results can be seen in Figure 4 as well as Table II
(labeled No feedback). In all cases, we observed that using
a purely generational grammar fuzzer resulted in signiﬁcant
coverage increases over AFL.
10
As expected, blind grammar fuzzing improves upon the
mutational fuzzing performed by AFL (In the case of mruby,
by more than one order of magnitude), even when a large
number of seeds is given. This shows that grammar fuzzing
remains highly relevant even after AFL and related tools have
been published (RQ3). Yet, adding feedback to the grammar
fuzzing approach results in even greater improvements: In
all cases we found more than twice as many new branches
than the blind grammar fuzzer. Therefore, we conclude that
adding feedback to grammar fuzzing is a very important step
to improve the performance of fuzzing (RQ4).
code
fragment:
Case Study: Feedback Grammar Fuzzing. When fuzzing
learned the
mruby, NAUTILUS automatically
following
ObjectSpace.each{|a|
interesting
a.method(...) } It allows the fuzzer to test a method
call on any object in existence. This greatly ampliﬁes the
chance of ﬁnding the right receiver for a method. Any time
the fuzzer guesses a correct method name,
this construct
immediately produces new coverage. Then the fuzzer can
incrementally learn how to construct valid arguments to this
speciﬁc call. Lastly, if the fuzzer needs a speciﬁc receiver
object, it only has to create the object somewhere in the input,
as any object will receive the method call. An AFL-style fuzzer
is not able to make use of this. It is not able to construct
BaselineAFLIFuzzerNautilus15.0%17.5%20.0%22.5%25.0%27.5%30.0%32.5%35.0%ChakraCoreBaselineAFLNautilus25.0%30.0%35.0%40.0%45.0%50.0%55.0%mrubyBaselineAFLNautilus2.0%4.0%6.0%8.0%10.0%12.0%14.0%PHPBaselineAFLNautilus40.0%45.0%50.0%55.0%60.0%65.0%70.0%LuaFig. 4: Branch coverage generated by our corpus (Baseline) and by different conﬁgurations of NAUTILUS.
complex arguments for the call, even if a set of valid method
names is given as a dictionary. A blind grammar fuzzer is
unable to reliably produce this gadget with an interesting
call inside. NAUTILUS used this gadget to ﬁnd two CVE’s
that would have been exceedingly hard to ﬁnd using either
blind grammar fuzzing or AFL-style fuzzing without a proper
grammar.
D. Evaluation of Generation Methods
To analyze our generation methods and answer RQ5, we
analyzed the performance of NAUTILUS using naive and uni-
form generation (see Section IV-A), using the same conﬁgura-
tion as in Section VI-C. Figure 4 shows the difference between
the naive generation and the uniform generation methods.
The results are very similar. The naive generation achieves
very similar results for ChakraCore and mruby; it performs
slightly better on PHP, while it performs signiﬁcantly worse
on Lua. This proves that the naive generation method, when
combined with the simple duplicate ﬁlter, performs very sim-
ilarly to the more complex uniform generation, making the
additional complexity of the latter unnecessary.
E. Evaluation of Mutation Methods
To answer RQ6 and to analyze the efﬁcacy of the different
mutation methods, our fuzzer keeps a counter for each muta-
tion method, the minimization methods, and the generation.
These counters are increased if the corresponding method
created an input that found a new code path. Note that counters
will only be increased by one, regardless of the amount of new
transitions discovered by any speciﬁc input. Using the same
conﬁguration of Section VI-C (using coverage feedback) our
fuzzer was run on each of the four targets. Figure 6 shows the
values of those counters at the end of each run.
Additionally, we evaluated the usefulness of the various
methods by computing the relative contribution of each method
over the 24 hours of each run. Due to the diminishing number
of new paths after the initial part of each run, we collected the
data in differently-sized bins: 1-minute bins for the ﬁrst 16
minutes, 2-minute bins until 30 minutes from the beginning,
3-minute bins until 1 hour from the beginning, 5-minute bins
until the 3 hour mark, 10-minute bins until the 6 hour mark,
then 20-minute bins until the end. The result is shown in
Figure 5. It can be seen that splicing becomes more and more
relevant over time, as the basic mutation methods slowly fail to
produce more semantically valid inputs. Eventually splicing of
11
BaselineNo feedbackNaive genUniform gen15.0%17.5%20.0%22.5%25.0%27.5%30.0%32.5%ChakraCoreBaselineNo feedbackNaive genUniform gen25.0%30.0%35.0%40.0%45.0%50.0%55.0%mrubyBaselineNo feedbackNaive genUniform gen2.0%4.0%6.0%8.0%10.0%12.0%14.0%PHPBaselineNo feedbackNaive genUniform gen40.0%45.0%50.0%55.0%60.0%LuaFig. 5: Percentage of identiﬁed new paths for each mutation method, over 20 runs on each target.
VII. RELATED WORK
In the following, we discuss fuzzing approaches based
on mutation or generation, where the latter are conceptually
closer to NAUTILUS. Hence, we explain commonalities and
differences of generation-based approaches in more detail.
Table IV provides an overview of characteristics of most
relevant existing approaches.
A. Mutation-Based Approaches
Mutation-based fuzzing has been a popular way to quickly
ﬁnd bugs, especially in input parsing. In contrast to generation-
based fuzzing, only a test corpus is needed. Many of these
approaches are based on AFL [18], a fuzzer that, while
also supporting brute force, leverages genetic input mutation,
guided by unique code coverage (counting only yet unseen
execution paths). AFL is still popular, as it continues to beat
competing fuzzers because of its sheer analysis cycle speed.
However, it lacks syntactic insight for input generation, thus,
paths guarded by complex syntactic or semantic checks remain
unexplored. This is what other approaches try to solve by
adding an interacting module with higher syntactic insight.
Taint-based fuzzers like BuzzFuzz [29] or TaintScope [50] try
to increase insight by leveraging taint tracing to map input
bytes to function arguments or branch checks [24], [29], [43],
[50]. This allows them to reduce input bytes that need to be
mutated. However, taint-based mutations may still be syntac-
tically (and even worse semantically) incorrect. NAUTILUS
instead generates syntactically and semantically correct inputs.
Instead of using (only) a taint-based companion module,
there are also approaches that leverage computation-intensive
symbolic execution that relies on constraint solving [22], [31],
[32]. Because of its complexity, many approaches use symbolic
execution only if is inevitable. For example, Dowser [35] only
concentrates on interesting regions, i.e. loops with complex
array accesses, and uses dynamic taint analysis to trace input
Fig. 6: Inputs that triggered new transitions for each target, grouped by
generation/mutation method, for four speciﬁc runs.
interesting code fragments becomes by far the most effective
mutation technique.
A similar behavior can be observed for the rules mutation.
This mutation is only used after the minimization is done, and
therefore it starts ﬁnding new paths only later in the fuzzing
process. The generation and minimization methods ﬁnd many
new paths at the beginning, but after a couple of hours the
splicing and random mutation make up more than 50% of the
new identiﬁed paths. The Random Recursive Mutation ﬁnds
less paths than the other mutations, but it ﬁnds paths that
no other of our mutation methods can ﬁnd: the PHP stack
overﬂow vulnerability and two vulnerabilities of mruby (CVE-
2018-10191 and CVE-2018-12248) have been found only by
the Random Recursive Mutation.
4Skyﬁre is not an actual fuzzer, only a seed generator.
12
0 %25 %50 %75 %100 %03m6m9m12m15m20m26m33m42m51m1h1h 15m1h 30m1h 45m2h2h 15m2h 30m2h 45m3h3h 30m4h4h 30m5h5h 30m6h7h8h9h10h11h12h13h14h15h16h17h18h19h20h21h22h23hGenerationSubtree Min.Recursion Min.Rules MutationAFL MutationSplicing MutationRandom MutationRandom Rec. Mut.06251250187525003125375043755000mrubyPHPChakraCoreLuaGenerationSubtree MinimizationRecursion MinimizationRules MutationAFL MutationSplicing MutationRandom MutationRandom Recursive MutationFuzzer
Input Generation
Radamsa
AFL
CSmith
LangFuzz
IFuzzer
Skyﬁre
NAUTILUS
Corpus
Mutation
CFG
Generation (corpus)
CFG + Corpus
CFG + Corpus4
loose CFG
Guided
Fuzzing
Works w/o
corpus
Bypasses
input parsing
Bypasses
semantic checks
Generally
applicable



































TABLE IV: Comparison of important related approaches.
bytes that map to these accesses. These bytes are analyzed
symbolically, while bytes are treated as concrete values.
Dowser’s symbolic analysis is more likely to produce well-
formed inputs, however, its limitation to buffer overﬂows pre-
vents widespread use. In contrast, NAUTILUS can ﬁnd arbitrary
crashes, and can focus on a certain aspect of a program by
adjusting the grammar accordingly. Driller combines aspects
of all aforementioned mutation-based fuzzing approaches by
leveraging dynamic and concolic execution. Driller [46] uses
directed fuzzing until it is not able to generate new paths. Then,
the concrete fuzzing input is passed to the symbolic execution
engine that explores new paths that the fuzzer can continue to
analyze. In comparison to NAUTILUS, Driller needs expensive
symbolic execution to continue, while still not being able to
easily generate semantically correct inputs.
B. Generation-Based Approaches
input. This is useful
Generation-based fuzzers leverage either a grammar (or
model), a corpus, or both to generate highly-structured,
syntactically correct
to analyze ﬁle
viewers (like media players),
interpreters, compilers, or
e.g. XML parsers. While there are several general-purpose
generation-based fuzzers [10], [41], [48], many approaches
directly target a speciﬁc use case: CSmith [51]
for C,
LangFuzz [36] and IFuzzer [47] for JavaScript interpreters,
and many more [7], [14], [20], [28], [44], [45]. In contrast,