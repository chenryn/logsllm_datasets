title:NetLock: Fast, Centralized Lock Management Using Programmable Switches
author:Zhuolong Yu and
Yiwen Zhang and
Vladimir Braverman and
Mosharaf Chowdhury and
Xin Jin
NetLock: Fast, Centralized Lock Management
Using Programmable Switches
Zhuolong Yu
Johns Hopkins University
Yiwen Zhang
University of Michigan
Vladimir Braverman
Johns Hopkins University
Mosharaf Chowdhury
University of Michigan
Xin Jin
Johns Hopkins University
ABSTRACT
Lock managers are widely used by distributed systems. Traditional
centralized lock managers can easily support policies between mul-
tiple users using global knowledge, but they suffer from low perfor-
mance. In contrast, emerging decentralized approaches are faster
but cannot provide flexible policy support. Furthermore, perfor-
mance in both cases is limited by the server capability.
We present NetLock, a new centralized lock manager that co-
designs servers and network switches to achieve high performance
without sacrificing flexibility in policy support. The key idea of
NetLock is to exploit the capability of emerging programmable
switches to directly process lock requests in the switch data plane.
Due to the limited switch memory, we design a memory manage-
ment mechanism to seamlessly integrate the switch and server
memory. To realize the locking functionality in the switch, we de-
sign a custom data plane module that efficiently pools multiple
register arrays together to maximize memory utilization We have
implemented a NetLock prototype with a Barefoot Tofino switch
and a cluster of commodity servers. Evaluation results show that
NetLock improves the throughput by 14.0ś18.4×, and reduces the
average and 99% latency by 4.7ś20.3× and 10.4ś18.7× over DSLR,
a state-of-the-art RDMA-based solution, while providing flexible
policy support.
CCS CONCEPTS
· Networks → Programmable networks; Cloud computing;
In-network processing; Data center networks.
KEYWORDS
Lock Management, Programmable Switches, Centralized, Data plane
ACM Reference Format:
Zhuolong Yu, Yiwen Zhang, Vladimir Braverman, Mosharaf Chowdhury,
and Xin Jin. 2020. NetLock: Fast, Centralized Lock Management Using
Programmable Switches. In Annual conference of the ACM Special Interest
Group on Data Communication on the applications, technologies, architectures,
and protocols for computer communication (SIGCOMM ’20), August 10ś14,
2020, Virtual Event, NY, USA. ACM, New York, NY, USA, 13 pages. https:
//doi.org/10.1145/3387514.3405857
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGCOMM ’20, August 10ś14, 2020, Virtual Event, NY, USA
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-7955-7/20/08. . . $15.00
https://doi.org/10.1145/3387514.3405857
1 INTRODUCTION
As more and more enterprises move their workloads to the cloud,
they are increasingly relying on databases provided by public cloud
providers, such as Amazon Web Services [4], Microsoft Azure [8],
and Google Cloud [7]. Performance and policy support are two
important considerations for cloud databases. Specifically, cloud
databases are expected to provide high performance for many ten-
ants and enable rich policy support to accommodate tenant-specific
performance and isolation requirements, such as starvation free-
dom, service differentiation, and performance isolation.
Lock managers are a critical building block of cloud databases.
They are used by multiple concurrent transactions to mediate ac-
cess to shared resources in order to achieve high-level transactional
semantics such as serializability. With recent advancements that
exploit fast RDMA networks and in-memory databases to signifi-
cantly improve the performance of distributed transactions [18, 46]
(i.e., decrease think time), the overhead of acquiring and releasing
locks is now a major component in the end-to-end performance of
cloud-based enterprise software [49].
Existing lock manager designs (both centralized and decentral-
ized) face a trade-off between performance and policy support
(Figure 1). The traditional centralized approach uses a server as a
central point to grant locks [3, 23]. With the global view of all lock
operations in the server, this approach can easily support various
policies, such as starvation freedom and fairness [23, 24, 29, 48].
The drawback is that the lock server, especially its CPU, becomes
the performance bottleneck as transaction throughput increases.
To mitigate the CPU bottleneck, recent decentralized solutions
leverage fast RDMA networks to achieve high throughput and low
latency [17, 40, 46, 49]. Clients acquire and release locks by updating
the lock information on the lock server through RDMA, without
involving the server’s CPU. However, since the locking decisions
are made by the clients in a decentralized manner, it is hard to
support and enforce rich policies [49].
We present NetLock, a new approach to design and build lock
managers that sidesteps the trade-off and achieves both high per-
formance and rich policy support. We observe that compared to the
actual data stored in a database, the lock information is only a small
amount of metadata. Nonetheless, the metadata requires high-speed,
concurrent accesses. Network switches are specifically designed and
optimized for high-speed, concurrent data input-output workloads,
making them a natural place to accelerate lock operations.
The key idea of NetLock is to leverage this observation and
co-design switches and servers to build a fast, centralized lock
manager. Switches provide orders-of-magnitude higher through-
put and lower latency than servers. By using switches to process
lock requests in the switch data plane, NetLock avoids the CPU
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Zhuolong Yu, Yiwen Zhang, Vladimir Braverman, Mosharaf Chowdhury, and Xin Jin
bottleneck of server-based centralized approaches, and achieves
high performance. By using a centralized design, NetLock avoids
the drawback of decentralized approaches and can support many
essential policies.
Realizing this idea is challenging for at least two reasons. First,
switches only have limited on-chip memory. Although the size of
lock information is orders-of-magnitude smaller than that of the
actual storage data, it can still exceed the switch memory size for
large-scale cloud databases. While previous work [31] has proposed
the idea of extending the switch memory with the server memory,
it does not consider the characteristics of locking and does not
provide a concrete solution for memory management. To address
this challenge, we design a mechanism to seamlessly integrate
the switch and server memory to store and process lock requests.
NetLock only offloads the popular locks to the switch and leaves
other locks to servers. We formulate the problem as an optimization
problem and design an optimal algorithm for memory allocation.
Second, switches only have limited functionalities in the data
plane and cannot process lock requests. Prior work [27] has shown
how to build a key-value store in switches and solved the fault-
tolerance problem, but a key-value store is not a fully functional
lock manager that can support different types of locks and support
policies. To address this challenge, we leverage the capability of
emerging programmable switches to design a data plane module to
implement necessary features required by NetLock. To maximize
memory utilization and avoid memory fragmentation, we design a
shared queue data structure to pool the register arrays in multiple
data plane stages together and allocate it to the locks. Each lock
owns an adjustable, continuous region in the shared queue to store
its requests. We design custom match-action tables in the data plane
to support both shared and exclusive locks with common policies.
NetLock is incrementally deployable and compatible with exist-
ing datacenter networks. It is well-suited for cloud providers that
have dedicated racks for database services. It only needs to aug-
ment the Top-of-Rack (ToR) switches of these database racks with
a custom data plane module for processing lock requests. Since the
custom module is only invoked by lock messages, other packets are
processed by switches as before. NetLock does not change other
switches in the network, and it is compatible with existing routing
protocols and network functions.
Recently there is a surge of interest in in-network computing.
While it is arguable whether applications should be moved to the
network and to what extent, NetLock takes a modest approach to
make the network more application-aware. Assisting locking in
the network is not a radical deviation from traditional network
functionalities. We emphasize that the application (i.e., transaction
processing) is still running on servers. NetLock provides locks with
switches to resolve contentions and enforce policies for concurrent
transactions, which is similar to using switch-based signals like Ran-
dom Early Detection (RED) and Explicit Congestion Notification
(ECN) to resolve congestion and enforce fairness for concurrent
flows, but in a more application-aware way for databases. Further-
more, compared to changing all NICs and redesigning applications
to leverage RDMA, replacing only the switch and transparently
updating the lock manager provides a competitive alternative to
high-performance database applications. NetLock can provide bet-
ter performance and lower the cost by reducing the lock servers.
Lock Manager
Centralized
Global knowledge; 
Flexible policy support;
Server involved
Decentralized
No global knowledge; 
Cooperative environment; 
Often RDMA-based so 
little server involvement
Server-only
Higher server CPU usage; 
Always, latency > RTT
Switch + Server
Significantly lower server 
CPU usage; 
Often, latency < RTT
t
n
e
m
e
v
o
v
n
l
i
r
e
v
r
e
S
n
o
i
t
i
a
n
d
r
o
o
c
t
n
e
i
l
C
i
s
m
s
n
a
h
c
e
m
Blind retry
Higher client CPU usage 
Exponential back-off
Relatively lower client 
CPU usage
Emulated queue
Additional RTTs;
Server CPU involved
Figure 1: Design space for lock management.
In summary, we make the following contributions.
• We propose NetLock, a new centralized lock manager archi-
tecture that co-designs programmable switches and servers to
achieve high performance and flexible policy support.
• We design a memory management mechanism to seamlessly
integrate the switch and server memory, and a custom data plane
module for switches to store and process lock requests.
• We implement a NetLock prototype on a Barefoot Tofino switch
and commodity servers. Evaluation results show that NetLock
improves transaction throughput by 14.0ś18.4×, and reduces the
average and 99% latency by 4.7ś20.3× and 10.4ś18.7× over the
state-of-the-art DSLR, while providing flexible policy support.
2 BACKGROUND AND MOTIVATION
In this section, we first provide background on the design of lock
managers. Then we motivate the usage of programmable switches
to design lock managers, by identifying potential benefits and dis-
cussing its feasibility.
2.1 Background on Lock Management
Lock managers are used by distributed systems to mediate con-
current access to shared resources over the network, where locks
are typically held in servers. There are two main approaches for
accessing locks, i.e., centralized and decentralized, as shown in
Figure 1.
Centralized lock management. A centralized lock manager uses
a server as a central point to grant locks [3, 23]. Because the server
has the global view of all lock requests and grant decisions, it can
easily enforce policies to provide many strong and useful properties,
such as starvation-freedom and fairness [23, 24, 29, 48].
A centralized lock manager can be distributed across multiple
servers, by having each server be responsible for a subset of lock
objects. There is a distinction between distributed and decentral-
ized. Centralized and decentralized approaches differ in how the
decisions to grant locks are made, i.e., whether they are made by the
central lock manager or by the clients in a decentralized manner.
Both approaches can be made distributed to scale out.
The lock manager can either be co-located with the storage
server that actually stores the objects or be in a separate server.
In the former case, the lock manager daemon would consume the
resources of the storage server, which can be otherwise used to
NetLock: Fast, Centralized Lock Management
Using Programmable Switches
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
process storage requests such as transactions. In the latter case,
lock managers for multiple storage servers can be consolidated to
a few dedicated servers.
Decentralized lock management. Centralized lock managers
suffer from low performance, as the server CPUs become the bot-
tleneck to handle a large number of lock requests from clients [49].
Decentralized lock managers often leverage fast RDMA networks
to address the performance problem [17, 40, 46, 49]. A decentralized
lock manager still has a designated server to maintain necessary in-
formation for each lock in a lock table, e.g., the current transaction
ID that holds the lock and whether the lock is shared or exclusive.
Different from centralized ones, a decentralized lock manager relies
on clients to make decisions in a distributed manner. The lock table
at the lock server is updated by the clients using RDMA verbs,
such as SEND, RECV, READ, WRITE, CAS, and FA. This approach
reduces CPU utilization at the lock server.
There are a few different strategies for the clients to acquire
locks in this approach. The simplest one is blind fail-and-retry,
where each client tries to acquire a lock independently, and retries
after a timeout if not succeed [46]. This strategy has high client
CPU usage, and can cause starvation and hence long tail latencies.
Exponential back-off can be used to reduce the CPU usage, but
it further increases latencies. More advanced ones use distributed
queues to emulate centralized lock managers [17]. Such strategies,
while avoiding starvation, incur extra network round-trips and lose
the benefit of high performance. The most recent solution in this
category, DSLR [49], adapts Lamport’s bakery algorithm [32] to
order lock requests and guarantees first-come-first-serve (FCFS)