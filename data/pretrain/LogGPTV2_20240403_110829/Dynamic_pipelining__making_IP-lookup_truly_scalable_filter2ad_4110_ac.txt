child nodes, and restore the correlation between node height and
preﬁx distribution. On a tangent, note that the jump node also
removes all but one copy of the leaf-pushed node P4. Because jump-
nodes remove one-child nodes, in a 1-bit trie they effectively remove
all the nulls that leaf-pushing would try to ﬁll up. Consequently, a
route-update modiﬁes only one trie node and does not propagate
down to entire subtrees.
The use of jump nodes results in two key properties which we
will use in Section 3.3.2 to prove a worst-case per-stage memory
bound for dynamic pipelining: (1) Because we stride only one bit at
trie nodes, there is no controlled-preﬁx-expansion, and hence no
replication of the same preﬁx. Jump nodes remove nulls, eliminating
unnecessary copies of leaf-pushed nodes. Thus,
the number of
leaves in the data-structure is equal to the number of preﬁxes. (2) In
SDP every internal node, whether that be a 1-bit trie node or a jump
node, is guaranteed to have two children.
3.3.2 Per-Stage Memory Bound
As observed in [11], before pipelining, the total memory required
by a trie with jump nodes does not exceed the number of nodes in a
binary search tree (i.e., 2N for N preﬁxes), as shown in Figure 6(a).
However, it is important to realize that bounding the total memory
does not bound the per-stage memory. Figure 6(b) shows the total
memory required by an unpipelined trie with jump nodes (shaded
triangular region) compared to the total memory required for a 1-bit
trie (the containing boundary), for the worst-case preﬁx distribution
shown in Figure 4. However, as Figure 6(c) illustrates, when the trie
is pipelined by partitioning it across stages, the per-stage memory
usage varies greatly depending on the preﬁx distribution. In order to
provide worst-case guarantees, the space provided at each memory
stage should be sufﬁcient for any preﬁx distribution. If we were to
assign a node to a particular memory stage based on which level of
the trie it belongs to (as DLP does), then even with jump nodes the
worst-case per-stage memory requirement remains equal
to the
impractical size derived in Section 3.2.1.
As noted in Section 3.3, the height of nodes is correlated to the
preﬁx distribution. If we assign nodes to pipeline stages based on the
height of the nodes, then we expect to obtain a tighter per-stage
memory bound. Formally, the height h of an internal node is deﬁned
to be the length of the longest path from that node to any of the
leaves below it. ([14] brieﬂy discusses how tries may be pipelined
and suggests mapping levels to stages, but the paper erroneously
uses the term height when it actually means depth.) For example, the
height of the shaded node in Figure 7(a) is 4. Let W be the number of
bits in an IP address. W is both the maximum height of the trie, and
the total number of memory stages in the dynamic pipeline. Utiliz-
ing the two key properties stated in Section 3.3.1, we now prove a
bound on the worst-case per-stage memory size for dynamically
pipelined tries with jump nodes.
Lemma 1: The number of leaves in a subtree rooted at a particular
node is no less than the height of that node.
Proof: If the height of a node is h, then there is at least one path P,
from that node to some leaf, that is h nodes long including the leaf.
Figure 7(b) shows such a path for some arbitrary node. There are h-1
internal nodes along P. For each of the h-1 internal nodes, there is an
alternate path that could be taken instead of P when traversing the
trie. In Figure 7(b) we indicate, as shaded nodes, the ﬁrst node along
every such alternate path. Each shaded node must either be a leaf
3
1
4
2
0
(a)
h
(b)
Fig. 7.  (a) The height of a node (b) The relation between height
and the number of leaves beneath a node.
itself, or must be an internal node that leads to at least one leaf.
Because all leaves are unique, the alternate paths must contain at
least h-1 leaves in total. Thus the number of leaves in the entire sub-
tree is at least h.
Lemma 2: Given any distribution of N preﬁxes, there can be no
more than N/h internal nodes with height h.
Proof: In an SDP trie there are as many leaves as there are preﬁxes.
Therefore the total number of leaves in the trie can be no more than
N. From lemma 1 we know that each node of height h accounts for at
least h unique leaves. Assume that the number of nodes with height
h exceeds N/h, then there must be more than N leaves in total, con-
tradicting the initial property of N leaves. Therefore, the number of
internal nodes with height h, can be no more than N/h.
Theorem 1: If we assign all nodes with height h, in an SDP trie, to
the (W-h)th pipeline stage, then we need to provide space for only
min( N/(W-k) , 2k ) nodes at the kth pipeline stage.
Proof: We need to prove two bounds — N/(W-k) and 2k— in order to
obtain the expression given in Theorem 1. To obtain the ﬁrst bound,
we observe that if all nodes with height h are assigned to the (W-h)th
stage, then from lemma 2 it sufﬁces to provide N/h space at the (W-
h)th pipeline stage. In other words, for the kth stage, it is sufﬁcient to
provide space for N/(W-k) nodes. To obtain the second bound, we
recall that internal nodes in an SDP trie have two children, therefore
the total number of nodes at the kth level of the trie cannot exceed 2k.
But, we need to establish that the number of nodes at the kth stage
cannot exceed 2k. Each node along a path from the root must lie in a
different stage, therefore an internal node at level k of the trie cannot
fall in a stage earlier than the kth. Equivalently we can also say that
an internal node at level k of the trie cannot have a height of more
than W-k, which means it will not get placed in a stage earlier than k.
Therefore, the space requirement of the kth stage is no greater than
space requirement of the kth level of the trie, thus proving the second
bound. For any value of k, we need to provide only as much space at
the kth stage as the minimum of the two bounds, which proves Theo-
rem 1.
Assigning nodes to stages based on their height is our ﬁrst inno-
vation. For 1 million preﬁxes, for instance, the worst-case total
memory required by SDP is just 22 MB, a four-fold reduction over
the latest static pipelining scheme [1]. We now to brieﬂy describe
the overall system architecture for SDP.
3.3.3 System Architecture
SDP is implemented using W stages (where W is the number of
bits in an IP-address), each consisting of an SRAM memory which
is sized in accordance with the results of Section 3.3.2. An IP-
lookup is provided with the location of the root of the trie, and it is
dispatched into the ﬁrst stage of the pipeline. The lookup performs
“NOPs” until it reaches the stage containing the root node. In addi-
tion, the lookup also performs “NOPs” in the intervening stages
when the heights of a node and its child differ by more than one.
When the lookup emerges off the end of the pipeline, the IP-lookup
has completed. The pipeline concurrently sustains as many lookups
in ﬂight as the number of stages. We will now explain the mecha-
nism of applying route-updates.
To update the SDP trie upon route changes, we need to maintain
information about the height of the nodes. However, keeping the
node heights and other auxiliary information within the trie itself
would increase its size and slow down the lookup rate. Further, we
would require frequent interruption of the IP-lookup stream in order
to examine or modify this auxiliary information. To address this
issue, we borrow the idea of a shadow trie from [1]: a copy of the
trie containing all the required auxiliary information. The shadow
trie is accessed only during the construction or update of the trie.
Because route-updates are orders of magnitude less frequent than
lookups, not only is it unnecessary to pipeline the shadow trie, but
we can implement it using slow and cheap memory (DRAM). Today
the cost of 128 MB of high performance DRAM is so trivial that the
addition of a shadow trie has no effect on total system cost. Mean-
while, all IP-lookups are performed on the fast, pipelined trie itself.
When the router receives route-updates, we ﬁrst apply them only
to the shadow trie, modifying the data-structure in accordance with
the route-updates. Because the modiﬁcations access only the shadow
trie and the IP-lookups access only the SDP trie, they can both pro-
ceed concurrently without interrupting each other. Following the
modiﬁcation of the shadow trie, we compute the eventual changes
that are required in the SDP trie. The required changes are formu-
lated into node-writes and are then dispatched to the SDP trie. To
apply the changes, we borrow a pipelined write-bubble scheme from
[1]. In this scheme, a write operation interrupts the stream of IP-
lookups by using up the turn of a single IP-lookup. The write opera-
tion marches down the pipeline stage-by-stage just like an IP-
lookup, except that it performs writes instead of reads. Further,
while the write operation is in a particular stage, IP-lookups can
access the other stages. This observation allows us to dispatch writes
into the pipeline, interleaved with lookups. A write operation is sim-
ply equivalent to a “bubble” in the lookup stream. However, the
writes must obviously be performed in a manner which ensures that
no read operation may encounter the data-structure in an inconsis-
tent or erroneous state. We address this concern after we analyze the
cost of pipelined incremental route-updates.
3.3.4 Optimum Cost Incremental Route-updates
The cost of route-updates can be represented by the IP-lookup
throughput that is lost to write-bubbles. When a route-update is
applied to a trie, it generally causes the insertion or removal of
nodes, and can obviously change the height of a number of nodes.
We ﬁrst apply the route-updates to the shadow trie and recompute
the heights of affected nodes. Then, in order to maintain the height-
to-stage mapping of SDP, we migrate every node whose height has
changed to the stage that corresponds to its new height. In addition
to the node migrations, a route-update also results in the creation
(deletion) of a node for the preﬁx being added (removed). Recall
that because SDP uses a 1-bit trie with jump-nodes, any route-
update needs to modify (including insert or remove) only one trie
node (Section 3.3.1). We refer to this node as the preﬁx-node.
Together, the migrations and the preﬁx-node modiﬁcation account
for the total cost for a route-update. It may seem such migrations
may hurt the worst-case route-update cost. However, we make two
key observations which enable us to bound the cost of any route-
update by the optimum of strictly one write-bubble only.
Our ﬁrst key observation is that, by virtue of the very deﬁnition
of height, the insertion or deletion of a node can affect the height of
only its ancestors, and cannot affect the height of any descendants.
A node insertion (deletion) may increase (decrease) the height of
all its ancestors. In the worst case the number of affected ancestors
can be W-1 (i.e. the maximum height of the trie minus one). Our sec-
ond key observation is that the preﬁx-node itself and all the affected
ancestors, each belong in a uniquely different stage, both before and
after the migration. Hence, a route-update requires one write to
every stage of the pipeline in the worst case. Just as an IP-lookup
X
2k at the
kth level
N/(W-k) at
the kth level
1
2
3
3
3
4
5
W
N
P
L
H
f
o
e
e
r
g
e
d
n
i
h
t
i
w
d
e
r
i
u
q
e
r
e
g
a
t
s
P
D
S
h
c
a
e
e
h
T
Fig. 8. Difference between the update cost of SDP and that of leaf-
pushed trie schemes
Fig. 9. An example of the per-stage memory requirement for SDP
and the corresponding degree of HLP
can perform a read operation in every stage of the pipeline, a write-
bubble can perform a write in every stage of the pipeline. Therefore,
we can send a single write-bubble into the pipeline and migrate the
ancestors to their new stages and write to the preﬁx node. Obviously,
this single write bubble represents the optimum route-update cost in
any pipelined IP-lookup scheme.
The write-bubble itself does not contain all the data that are to be
written to various stages. The write-bubble simply reserves each
pipeline stage for one cycle, ensuring that no lookup is accessing
that stage. Typically a lookup processor or custom logic performs
the necessary computation at each memory stage during a lookup.
The same processor or logic is responsible for supplying the new
data through the data bus of each memory stage, when the write-
bubble reaches that particular stage.
Unlike multibit trie nodes, the size of 1-bit trie nodes is small and
constant, therefore our assumption that a single memory write oper-
ation is wide enough to write an entire trie node is justiﬁed. In SDP a
single write-bubble is sufﬁcient for handling the worst-case route-
update. Note that the conclusion of this cost analysis is signiﬁcant:
node migrations are literally free; SDP reduces a seemingly
unbounded factor in route-update cost to the equivalent of a non-
existent factor. Thus, we exploit the dynamic height-to-stage map-
ping to obtain both scalability in total memory size, and optimum
route-update cost.
Figure 8 illustrates an intuitive way to understand this marked
difference between the update cost of SDP and that of leaf-pushed
trie schemes. In SDP the region that is affected by an update to node
X is only the highlighted path from node X to the root. In contrast, in
a leaf-pushed trie scheme an update to node X must be propagated
down into the entire shaded subtree.
Recall that we mentioned at the end of Section 3.3.3 that when
write-bubbles interleave with IP-lookups we must never allow the
IP-lookups to read the SDP trie in an inconsistent state. Speciﬁcally,
each pointer must be valid when dereferenced. We can trivially fulﬁl
this requirement by observing that a write-bubble modiﬁes at most
one node in each stage. Because only one node is re-written in any
given stage, the stage previous to it contains only one pointer that
can be potentially invalid. When a write-bubble modiﬁes a pointer in
a node in stage s, only the lookups that are upstream to the write-
bubble observe the modiﬁed pointer. After modifying the pointer the
write-bubble arrives into the next stage (s+1) and writes out the new
node being pointed to. By the time an upstream lookup arrives into
stage s+1 and dereferences the pointer in question, the write-bubble
has already written out the new node. Thus, we guarantee data-struc-
ture consistency. The lookups that are downstream to the write-bub-
ble read and dereference the pointer before it is ever touched by the
write-bubble.
3.3.5 Memory Management Overhead
The process of applying route-updates allocates and deallocates
memory for inserting and deleting nodes in the trie. Hence, the abil-
ity to apply incremental route-updates necessitates a memory man-
agement scheme for the routing-table memory. Because memory
management appears as an overhead in the route-update cost, an IP-
lookup scheme must include the worst-case memory management
overhead for obtaining its true worst-case route-update cost. Previ-
ous IP-lookup schemes use multibit tries, often with some compres-
sion mechanism [3][4][11], resulting in nodes that vary in size.
Repeated allocations and deallocations of such non-uniform sizes
leads to fragmentation and underutilization of memory. Tree Bitmap
[4] and Segmented Hole Compaction [14] use complex memory
management schemes to compact away the memory fragmentation.
Thus, even though Tree Bitmap guarantees O(1) route-update cost,
in the worst case its memory management scheme can add an over-
head of more than 100 memory accesses to any route-update due to
compaction operations.
In contrast, we use only 1-bit trie nodes without compression,
ensuring that all nodes have the same size across all levels and all
stages. Hence, all memory allocations and deallocations deal with
one size. Consequently, our routing-table memory incurs no frag-
mentation whatsoever, and we obviate the need for complex memory
management schemes like Tree Bitmap [4] and Segmented Hole
Compaction [14]. Thus, after accounting for memory management
overhead, the total worst-case route-update cost of SDP amounts to
exactly and only one write-bubble.
3.3.6 Scalability in Lookup Rate
As we have pointed in Section 3.2.3, the lookup rates for data-
structure pipelining cease to scale once the strides have been
reduced to 1, and the size of the largest stage has been minimized.
For 1 million preﬁxes, the size of the largest memory stage using
SDP is 3.8 MB, which means that SDP pipelined from only a data-
structure perspective can manage only upto 40 Gbps line-rate.
The underlying assumption in data-structure pipelining is that,
before a packet’s lookup can access a particular stage, it must wait
for the lookup that is currently in that stage to complete its access.
However, we can internally pipeline, at the hardware level, the mem-
ory of each SDP stage so that a packet’s lookup can access the mem-
ory of an SDP stage before the downstream lookups have completed
their access of that memory. The degree to which an SDP stage must
be hardware-pipelined is equal to the ratio of the required lookup-
rate to its access delay, which depends on its size. Thus, different
SDP stages may be hardware-pipelined to different number of hard-
ware stages. We see from the example in Figure 9 that the early