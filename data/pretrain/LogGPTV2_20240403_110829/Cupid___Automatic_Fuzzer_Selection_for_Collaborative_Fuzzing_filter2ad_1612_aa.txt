title:Cupid : Automatic Fuzzer Selection for Collaborative Fuzzing
author:Emre G&quot;uler and
Philipp G&quot;orz and
Elia Geretto and
Andrea Jemmett and
Sebastian &quot;Osterlund and
Herbert Bos and
Cristiano Giuffrida and
Thorsten Holz
Cupid: Automatic Fuzzer Selection for
Collaborative Fuzzing
Emre Güler
Ruhr-Universität Bochum
PI:EMAIL
Andrea Jemmett
Vrije Universiteit Amsterdam
PI:EMAIL
Philipp Görz
Ruhr-Universität Bochum
PI:EMAIL
Sebastian Österlund
Vrije Universiteit Amsterdam
PI:EMAIL
Elia Geretto
Vrije Universiteit Amsterdam
PI:EMAIL
Herbert Bos
Vrije Universiteit Amsterdam
PI:EMAIL
Cristiano Giuffrida
Vrije Universiteit Amsterdam
PI:EMAIL
Thorsten Holz
Ruhr-Universität Bochum
PI:EMAIL
ABSTRACT
Combining the strengths of individual fuzzing methods is an ap-
pealing idea to find software faults more efficiently, especially when
the computing budget is limited. In prior work, EnFuzz introduced
the idea of ensemble fuzzing and devised three heuristics to classify
properties of fuzzers in terms of diversity. Based on these heuristics,
the authors manually picked a combination of different fuzzers that
collaborate.
In this paper, we generalize this idea by collecting and applying
empirical data from single, isolated fuzzer runs to automatically
identify a set of fuzzers that complement each other when exe-
cuted collaboratively. To this end, we present Cupid, a collaborative
fuzzing framework allowing automated, data-driven selection of
multiple complementary fuzzers for parallelized and distributed
fuzzing. We evaluate the automatically selected target-independent
combination of fuzzers by Cupid on Google’s fuzzer-test-suite, a
collection of real-world binaries, as well as on the synthetic Lava-M
dataset. We find that Cupid outperforms two expert-guided, target-
specific and hand-picked combinations on Google’s fuzzer-test-suite
in terms of branch coverage, and improves bug finding on Lava-M
by 10%. Most importantly, we improve the latency for obtaining
95% and 99% of the coverage by 90% and 64%, respectively. Fur-
thermore, Cupid reduces the amount of CPU hours needed to find
a high-performing combination of fuzzers by multiple orders of
magnitude compared to an exhaustive evaluation.
CCS CONCEPTS
• Security and privacy → Software security engineering; •
Software and its engineering → Software testing and debug-
ging.
ACM Reference Format:
Emre Güler, Philipp Görz, Elia Geretto, Andrea Jemmett, Sebastian Öster-
lund, Herbert Bos, Cristiano Giuffrida, and Thorsten Holz. 2020. Cupid:
Automatic Fuzzer Selection for Collaborative Fuzzing. In . ACM, New York,
NY, USA, 13 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn
1 INTRODUCTION
In recent years, fuzzing has become an essential tool for finding
bugs and vulnerabilities in software. Fuzzers such as AFL [29] and
1
HonggFuzz [12] have successfully been applied in practice to gener-
ate inputs to find bugs in a large number of applications [24]. Recent
work in fuzzing [1, 4, 15, 23, 25, 28] has focused on improving test
case generation by implementing new input mutation and branch
constraint solving techniques.
Since it is common to use automated bug-finding tools to find
newly introduced bugs in software development scenarios (e.g., on
every new commit/release), in pentesting scenarios (e.g., to find
evidence of vulnerabilities), or in server consolidation scenarios
(e.g., where spare CPU cycles can be dedicated to fuzzing), pro-
ducing results in bounded time is crucial. Consequently, we target
practical use cases where the time budget available for fuzzing
is limited and it may be difficult to saturate coverage within that
budget. It is, thus, important to look at how existing tools can be
utilized more efficiently. Large-scale fuzzing campaigns, such as
OSS-Fuzz [24], have shown that fuzzing scales well with additional
computing resources towards finding security-relevant bugs in soft-
ware. Moreover, researchers further improved the speed of fuzzing
by parallelizing and distributing the fuzzing workload [17, 18, 29].
Typically, in these setups, multiple instances of the same fuzzer
run in parallel, where the findings are periodically synchronized
between these fuzzers [24]. In contrast, EnFuzz [5] demonstrated
that running different fuzzers in combination leads to a noticeable
variation in performance, paving the way for further improvement.
Intuitively, this stems from the fact that fuzzers that have differ-
ent properties and advantages in some areas (e.g., certain types of
binaries or conditions) often come with disadvantages in others.
Hence, a collaborative fuzzing run using a combination of fuzzers
with different abilities can outperform multiple instances of the
same fuzzer. EnFuzz explored this idea and the authors introduced
heuristics that can be used to select different fuzzers that cooperate
to find bugs more efficiently.
We generalize this idea and show that, given a set of existing
fuzzers and a number of cores, selecting a good mix of fuzzers is a
non-trivial but crucial step in maximizing the overall performance
of a collaborative fuzzing process. To achieve this, we develop a
framework called Cupid to optimize a collaborative fuzzing run by
automatically predicting which fuzzer combinations will perform
well together. We show that data from a single, isolated fuzzing
campaign on a representative set of branches can be used to estimate
which fuzzers complement each other and maximize code coverage
in a collaborative fuzzing run on unknown binaries.
In an extensive evaluation on different data sets and more than
40,000 CPU hours spent, we not only show that our prediction
on how well fuzzer combinations will perform together closely
resembles real-world results, but we also show that our proposed
one-off data-driven and automatic prediction results in a fuzzer
combination that clearly outperforms two different expert-guided
and hand-picked combinations selected by EnFuzz in regards to
branch coverage, bug finding, and latency to find coverage. Further-
more, finding a high-performing combination of fuzzers takes linear
time and, compared to an exponentially growing exhaustive search,
reduces the computation time by multiple orders of magnitude.
In this paper, we make the following contributions:
• We demonstrate that a high-performing combination of
fuzzers can be predicted by measuring the single, isolated
performance of fuzzers on real-world binaries. Based on this
insight, we develop Cupid, an extensible and scalable tool to
predict high-performing collaborative fuzzer combinations.
• We present and evaluate a novel complementarity metric for
• We demonstrate how our data-driven approach allows for a
linearly scaling, automated fuzzer selection process, avoid-
ing the need for expert guidance as well as avoiding expo-
nentially growing exhaustive search (and in comparison,
reducing the number of necessary CPU hours by multiple
orders of magnitude).
calculating how well multiple fuzzers collaborate.
2 BACKGROUND
Fuzzing is the process of automatically finding bugs by generat-
ing randomly mutated inputs and observing the behavior of the
application under test [21]. Current fuzzers are mainly coverage-
guided [29], meaning that they try to generate inputs to maxi-
mize code coverage. The ever-growing code size of projects like
web browsers require developers to scale performance by running
fuzzers in parallel [18, 24, 29]. When automatically testing heavy-
weight applications like Chrome, with over 25 million lines of
code [6], it is clear that fuzzing tools need to utilize multi-core
and distributed systems to maximize code coverage and to increase
the likelihood of finding bugs. This strategy is already in use, e.g.,
by the ClusterFuzz project [24].
For this purpose, fuzzers like AFL ship with a parallel-mode [18,
29], where multiple AFL instances share a corpus and thus syn-
chronize their efforts. Although this approach does indeed increase
code coverage, it does not solve some of the limitations inherent
to a specific fuzzer in question. For example, when plain AFL has
difficulties solving magic bytes comparisons, multiple instances of
AFL will still have a low probability of solving these conditions.
To counter the limitations imposed by using one single type of
fuzzer, EnFuzz [5] introduces ensemble fuzzing. The authors demon-
strate that combining a diverse set of fuzzers leads to greater code
coverage compared to running multiple instances of the same fuzzer.
The boost in performance seems to stem from the symbiosis of the
different fuzzing techniques, where the combination of fuzzers are
more likely to cancel out individual disadvantages, while retaining
Figure 1: Overview of Cupid and its different components.
their strengths. Thus, it is important to find a high-performing
combination of fuzzers to maximize the expected return.
3 OVERVIEW
Prior to our work, there were two main approaches for finding
a high-performing combination of fuzzers: (1) evaluating a hand-
picked selection of fuzzers, or (2) exhaustively evaluating every
possible combination of fuzzers on a number of applications. Both
approaches come with multiple downsides:
(1) Hand-picking combinations requires expert knowledge of
fuzzers and thus demands significant manual effort. Addi-
tionally, this process is likely to introduce human biases.
(2) An exhaustive evaluation requires a number of fuzzing runs
that grows exponentially with the size of the combination
and the number of fuzzers to test. This amount of compute
quickly reaches infeasible levels.
With Cupid, we propose a scalable solution that requires no
human expertise in judging the performance of fuzzers and only lin-
ear computation time in the number of fuzzers. This is achieved
by evaluating fuzzers individually to collect information on their
performance, while only simulating the collaborative aspect. We
show that, given this data, it is possible to predict an approximate
performance ranking of fuzzer combinations in a collaborative set-
ting, which Cupid uses to select a diverse and high-performing
combination of fuzzers. As this data has to only be collected once
per fuzzer in isolation, adding new fuzzers does not lead to an
exponential growth of necessary evaluation runs. In addition, it
is not necessary to run additional evaluations when, for example,
the available number of CPU cores change. After the individual
fuzzer evaluation, Cupid is able to predict a likely candidate for
the highest-performing combination in a matter of seconds for all
practically relevant combination sizes.
An overview of our approach is shown in Figure 1. Cupid is split
into a one-time offline phase to determine a likely candidate for a
generally applicable high-performing combination of fuzzers, which
is then used in future fuzzing scenarios (the online phase). The core
idea behind the one-off training phase is to collect data on how well
the single, isolated fuzzers perform on a representative set of real-
world binaries, which would allow us to predict a complementary
combination of fuzzers that would work independently of the future
fuzzing targets. While we cannot prove that there exists such a
universally representative set of binaries, our experiments show
2
Online phaseOine phaseFuzzersSeedsShared CorpusTrainingBinariesFuzzing TargetCupidPrediction Enginealways take the same path in this example, two instances of the
same fuzzer will not increase code coverage. In fact, they would
still find the same four basic blocks. Thus, the addition of more
resources (CPU cores) would not result in better performance. This
issue also affects option (2). In option (3), however, both fuzzers are
diverse as well as complementary and contribute seeds the other
is unable to generate. A collaborative run between the two would
lead to a total code coverage of six basic blocks.
Although this clear-cut split between two fuzzers rarely happens
in real-world scenarios, it illustrates an important point: running
the best-performing fuzzer is the best choice in single-instance
mode, but when two (or more) fuzzers run collaboratively, we want
them to maximize code coverage. As such, we want the union of
their coverage to add up to the maximum possible value—which is
the case when they complement each other well.
To answer research question (b), i.e., why we should not just
select for diversity, we refer back to Figure 2. In this case, the com-
bination of two different fuzzers led to the best possible outcome,
because they are guaranteed to cover the same basic blocks every
time, an assumption highly unlikely in the real world. Suppose, as a
very simplified example, fuzzer 𝐴 can only solve the illustrated four
branches in 5% of its runs, and fuzzer 𝐵 solves its three branches
in 60% of its runs. Although fuzzer 𝐴 could potentially solve more
branches than 𝐵 (four vs. three), it is unlikely for this to happen. The
expected number of branches that fuzzer 𝐴 solves is 0.05 · 4 = 0.2.
On the other hand, it is likely for fuzzer 𝐵 to solve three branches,
leading to an expected average number of branches of 0.6 · 3 = 1.8.
In this modified scenario, judging the performance of a combina-
tion of fuzzers is more complicated. Although only the combination
of fuzzers 𝐴 and 𝐵 could reach the highest possible coverage (the
union of the branches they can only reach in collaboration), this
scenario is unlikely given the probabilities above. That is, in the
average run, fuzzer 𝐴 will not contribute any new branches due
to their low probabilities. In contrast, fuzzer 𝐵 has a more consis-
tent performance. Regarding the combinations of two instances of
fuzzer 𝐴, the expected average number of branches would still be
low, whereas the two instances of fuzzer 𝐵 would actually maximize
the expected average number of branches. In this case, diversity on
its own did not lead to the choice of the best fuzzer combination.
Thus, to answer research question (c), the complementarity met-
ric measures the degree in which multiple fuzzers are complemen-
tary, i.e., the union of the expected mean code coverage. As such, we
consider a combination to be high-performing (highly complemen-
tary), if the combination, on average, is expected to maximize code
coverage in the shortest amount of time.
In conclusion, our approach called Cupid combines the advan-
tages of both the data-driven complementarity metric as outlined in
this work, as well as the diversity heuristic as outlined by EnFuzz.
Cupid uses complementarity to automatically rank combinations
by their predicted performance, and applies the diversity heuristic
to select a single high-performing combination of fuzzers. Specifi-
cally, given the noise in the training data and the fact that Cupid’s
predictions are estimates, we consider two combinations to be rea-
sonably similar if the difference in their predicted performances
is less than 5%. As such, we collectively classify all combinations
that are similar to the top-ranking combination as high-ranking.
Cupid selects the highest ranked, most diverse combination out of
Figure 2: Different paths solved by fuzzer (A) and fuzzer (B)
even after multiple runs.
how we were able to select a training set that enabled Cupid to
select a fuzzer combination that performed well on a broad range
of previously unseen test binaries.
4 DESIGN
When automatically searching for a high-performing combination
of fuzzers for a collaborative fuzzing campaign, we not only need
to define what constitutes a high-performing combination, but
also how it could be calculated and predicted using a data-driven
approach without guidance by a human.
Intuitively, fuzzers that reach different parts of the program
under test should benefit from sharing their progress, since they
will provide each other with new seed files that the other would
possibly never find (i.e., they are complementary). However, several
instances of the same fuzzer can also benefit from each other, as
their cooperation increases the chance to solve branches in a given
time-frame, i.e., their cooperation increases the average speed in
which they solve branches. In real-world scenarios, however, fuzzer
combinations neither show completely identical behavior nor are
they completely orthogonal—it is thus our goal to find candidates
with a high degree of complementarity.
4.1 Complementary Fuzzers
EnFuzz bases their prediction on the idea that the diversity of
fuzzers is paramount, i.e., using as many different strong fuzzers
as possible. While we agree with this approach in principle (Cupid
applies diversity as a selection criteria), we improve upon this by
ranking fuzzers based on a criteria that we call complementarity
of fuzzers. To conceptualize what constitutes a complementary
combination of fuzzers, we need to discuss several key questions:
(a) Why not run multiple instances of the best fuzzer?
(b) Why not just select for diversity?
(c) When are fuzzers complementary in practice?