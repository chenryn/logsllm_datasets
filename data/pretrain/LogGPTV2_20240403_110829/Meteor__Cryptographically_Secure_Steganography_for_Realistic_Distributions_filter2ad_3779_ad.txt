𝑟0, 𝑟1, . . . , 𝑟𝑚 corresponding to each valid word. For instance, if the
precision is 5 bits, 𝑟0 might be interval [00000, 00101), 𝑟1 might
be [00101, 10000), and so on. The algorithm then generates a uni-
form random value 𝑟 ∈ [00000, 11111], finds the interval 𝑟𝑖 into
which 𝑟 falls, and outputs the corresponding word. In the example,
if 𝑟 = 01110, then the word corresponding to 𝑟1 would be chosen. In
practice, these values all have much higher precision, for example
𝑟 ∈ {0, 1}32, 𝑟𝑖 ∈ {0, 1}32 × {0, 1}32.
Meteor embeds messages into the random number 𝑟 used to
sample from the model, as illustrated in Figure 3. Consider the
information that a potential receiver with access to the model might
learn from a single output of the generative model. Because the
receiver has access to M, they can recover the interval 𝑟𝑖 into which
𝑟 must have fallen. Note that a 𝑟𝑖 might contain a huge — possibly
exponential — number of possible values that would all yield the
same sample, meaning the receiver cannot uniquely recover the
true value of 𝑟. However, because the intervals are continuous, all
such values may share a prefix, effectively fixing the first few bits
of 𝑟 in the view of the receiver. In this example above, all values
in 𝑟1 are contained in the first half of the distribution space, so
the receiver can conclude the first bit of 𝑟 must have been a 0.
Similarly, if the word corresponding to 𝑟0 had been chosen, the
first bits of 𝑟 must have been 00. Another example can be seen in
Figure 3, in which the interval corresponding to the word “The”
shares the prefix 01, so a receiver can recover these bits. In this way,
if 𝑟 is a function of the hidden message, the receiver can potentially
recover bits of information about the message with each output of
the model. Because the sender and receiver share the description
of the distribution, the sender can determine how many bits will
be recoverable, and then discard those bits before repeating the
process.
The key challenge in this setting is keeping the message hidden
from the adversarial censor with access to the same distribution.
Clearly, using the bits of the message as the randomness is inse-
cure, as a censor with the same model could extract the message.
Encrypting the message with a pseudorandom cipher, as in the
public-key solution above, is also insufficient because it is possible
that the encoder will be forced to reuse randomness. For example,
consider a probability distribution in which the values of the inter-
val containing 𝑟 have no shared prefix, but 90% of the values in that
interval begin with a 0. Because no bits are transmitted and the
next iteration will use the same value of 𝑟. The censor now knows
that with 90% likelihood, 𝑟 in the second sampling event begins
with zero. Over enough trials, a censor could detect this bias and
distinguish between honestly sampled output and stegotext.
To avoid the reuse of randomness, Meteor generates a fresh
mask for 𝑟 each time the sender samples an output. This is done
using a PRG, keyed with state shared by the sender and receiver,
and applied using XOR. The receiver recovers as many bits of 𝑟 as
possible and then unmasks them with the corresponding XOR mask
to recover bits of the message. Conceptually, this can be seen as
repeatedly encrypting the message with a stream cipher, facilitating
bit-by-bit decryption. This novel encoding technique means the
number of bits that can be transmitted in each sampling event is not
fixed. In practice, this is a huge advantage, as the expected number
of bits transmitted is proportional to the entropy in the channel
without requiring any explicit signaling (see Section 5.2). Finally, it
is intuitively clear why this approach yields a secure scheme: (1)
each sampling event is performed with a value of 𝑟 that appears
independent and random and (2) all bits that can be recovered are
obscured with a one-time pad.
5.2 Meteor
For notation, let 𝜆 be a security parameter, 𝜖 be the empty string,
and ∥ represent concatenation or appending to an ordered set,
as appropriate. We adopt Python-like array indexing, in which
𝑥[𝑎 : 𝑏] includes the elements of 𝑥 starting with 𝑎 and ending with
𝑏, exclusive. Finally, we use two subroutines LenPrefix𝛽(·) and
Prefix𝛽(·), presented in Algorithm 2 and Algorithm 3, respectively.
The first gives the length of the longest shared bit prefix of elements
in the set, and the second returns this bit prefix explicitly.
Pseudorandom Generators. Our construction leverages a pseu-
dorandom generator PRG [99]. For a more formal treatment of the
security notions of PRGs, see [100] and the citations contained
therein. We adopt the notation used in stateful PRGs. Specifically,
let the PRG have the functionalities PRG.Setup and PRG.Next. The
setup algorithm generates the secret state material, which we will
denote 𝑘𝑝𝑟𝑔 for simplicity, and the next algorithm generates 𝛽 pseu-
dorandom bits. We require that the PRG satisfy at least the real-or-
random security games.
Construction. Meteor consists of three algorithms, parameterized
by a bit precision 𝛽 and a model M that supports a RRRSS. We use
a generative model M as our instantiation of the distribution D
for an RRRSS as defined in Section 3. The key generation algorithm
KeyGen𝛽M is presented in Algorithm 4, the encoding algorithm
Encode𝛽M is presented in Algorithm 5, and the decoding algorithm
Decode𝛽M is presented in Algorithm 6.
The precision 𝛽 ∈ Z, 𝛽 > 0 controls the maximum number of bits
that can be encoded in each iteration. 𝛽 should be the accuracy of the
underlying sampling scheme. Most models in our implementation
give probability distributions accurate to 32 bits, so we set 𝛽 = 32.
In our tests, it is incredibly unlikely that 32 bits will successfully be
encoded at once, meaning using a lower 𝛽 is likely acceptable.
Because the model used in sampling is a generative one, the
model maintains state on its previous inputs. Each distribution
generated by the model is dependent on the values sampled from
previous distributions. Additionally, the model requires an initial
state to begin the generative process. This state is abstracted by the
history parameter H passed to instances of Encode and Decode.
This allows the distributions generated by each successful sampling
of a covertext token 𝑐𝑖 to remain synchronized between the two
parties. We assume that the entire history H is maintained between
the parties, including the initial state that primes the model.
Session 5C: Messaging and Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1536Figure 3: An overview of the encoding strategy for Meteor. In each iteration of Meteor, a new token (shown in green) is selected
from the probability distribution created by the generative model. Depending on the token selected, a few bits (shown in red)
can be recovered by the receiver. The stegotext above is real output from the GPT-2 model.
The encoding algorithm loops through three stages until the
entire message has been successfully encoded: (1) generating and
applying the mask, (2) sampling a next output to append to the
covertext, and (3) updating the state of the algorithm based on the
output of the sampling event. In the first stage, the mask is computed
as the output of a pseudorandom generator and is applied with the
XOR operation. The resulting value, 𝑟 is distributed uniformly in
[0, 2𝛽+1), as each bit of 𝑟 is distributed uniformly in {0, 1}. This
random value is then used in step (2) to sample the next output of
the sampling scheme. To determine the number of bits this sampling
event has successfully encoded, the encoding algorithm uses the
Recover𝛽 functionality of the RRRSS and calls LenPrefix on the
resulting (multi-)set. Finally, the algorithm then updates the 𝛽 bits
that will be used in the next iteration, and updates its other state
as appropriate.
The decoding algorithm performs these same three stages, but
with the order of the first two reversed. With knowledge of the
output of each sampling stage 𝑐𝑖, the first algorithm calls Recover𝛽
and Prefix to recompute some (possibly zero) leading bits of the 𝑟.
Then, it calculates the mask that was used by the encoder for those
bits and removes the mask. The bits recovered in this way make up
the message.
Note that we do not discuss reseeding the PRG. Most PRGs have
a maximum number of bits that can be extracted before they are
no longer considered secure. Because the PRG secret information
is shared by the sender and receiver, they can perform a rekeying
or key ratcheting function as necessary.
Proof of Security. We sketch the proof of security, as the formali-
ties of this simple reduction are clear from the sketch. Consider an
adversary A which has non-negligible advantage in the security
game considered in Definition 1. We construct an adversary ˆA
with non-negligible advantage in the PRG real-or-random game,
with oracle denoted 𝑅(·). To properly answer queries from A,
ˆA
runs the encoding algorithm in Algorithm 2 with an arbitrary in-
put message, but queries the 𝑅(·) to obtain the mask required for
ˆA keeps a table of all queries sent by A
sampling. Additionally,
and the responses. When A queries the decoding algorithm,
ˆA
checks its table to see if the query matches a previous encoding
As the message is masked by the queries
ˆA outputs 𝑏 as well.
be computed as
transmission, is computed as −
query, and responds only if it is an entry in the table. Note that if
𝑅(·) implements a true random function, the encoding algorithm
simply samples a random message from the distribution. When A
terminates, outputting a bit 𝑏,
ˆA sends to 𝑅(·), A
must be able to distinguish between a true-random output and the
xor of a message with a one-time pad. Because XOR preserves the
uniformly-random distribution of the pad, this is not possible with
non-negligible probability.
Efficiency. The asymptotic, expected throughout of Meteor is pro-
portional to the entropy in the communication channel. To see this,
note that the expected throughput for each sampling event can
𝑖∈|P| 𝑝𝑖Exp(𝑝𝑖), where P is the distribution in
the channel for the sampling event, 𝑝𝑖 is the probability of each
individual outcome, and Exp(·) is the expected number of shared
prefix bits for some continuous interval of size 𝑝𝑖 . Thus, if Exp(𝑝𝑖) is
proportional to − log2(𝑝𝑖), Meteor is asymptotically optimal (recall
that entropy, the information-theoretic boundary for information
𝑖∈|P| 𝑝𝑖 log(𝑝𝑖)). We show in Ap-
pendix A that Exp(𝑝𝑖) ≥ 1
2 by carefully
observing the behavior of the LenPrefix function when evaluated
on a fixed sized interval with a random starting point between
[0, 2𝛽+1).
6 EVALUATION OF METEOR
In this section we discuss our implementation of Meteor and eval-
uate its efficiency using multiple models. We focus on evaluating
Meteor, not a hybrid steganography system using the public key
stegosystem in Section 4, because it is significantly more efficient.
Moreover, the efficiency of a hybrid stegoanography system is de-
termined by the efficiency of its constituent parts; the cost of such a
scheme is simply the cost of transmitting a key with the public key
scheme (see Section 4) plus the cost of transmitting the message
with Meteor. An interactive online demonstration of our system is
available at https://meteorfrom.space.
Implementation details. We implemented Meteor using the Py-
Torch deep learning framework [101]. We realize the PRG function-
ality with HMAC_DRBG, a deterministic random bit generator defined
2 (− log2(𝑝𝑖) − 1) for 𝑝𝑖 ≤ 1
Attack@Dawn        0101 0111 1100 1001PRG Mask:  0001 0110 1011 1101Generative ModelEvidence indicates that the asteroid fell in the Yucatan Peninsula, at Chicxulub, Mexico.AnTheAHoweverSinceMessage Bits:  0100 0001 0111 0100    The first importance of the Yucatan Peninsula is demonstrated with the following conclusion: the Pliocene Earth has lost about seven times as much vegetation as the Jurassic in regular parts of the globe, from northern India to Siberia…PlaintextContextStegotextEncoderSession 5C: Messaging and Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1537Algorithm 2: LenPrefix𝛽
Input: Set of Bit Strings R = {𝑟1, 𝑟2, . . . 𝑟𝑛}
Output: Length ℓ
ℓ ← 1
while ℓ  𝑟 then
Output 𝑛𝑒𝑥𝑡 ← T [𝑖]
Output 𝑛𝑒𝑥𝑡 ← T [|T | − 1]
Algorithm 8: Recover𝛽M for the GPT-2 model.
Input: History H, Sample 𝑠
Output: Randomness set R
T , P ← NextM(H)
𝑐𝑢𝑚𝑙 ← 0
for 𝑖 ∈ {0, 1, . . . , |T | − 1} do
if T [𝑖] = 𝑠 then
Output
R ← {𝑟 ∈ {0, 1}𝛽 | 𝑐𝑢𝑚𝑙 ≤ 𝑟 < 𝑐𝑢𝑚𝑙 + P[𝑖]}
𝑐𝑢𝑚𝑙 ← 𝑐𝑢𝑚𝑙 + P[𝑖]
Output R ← ∅
Figure 5: RRRSS algorithms for GPT-2 model. T is an array