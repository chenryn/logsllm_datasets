0
0
0
4
0
0
0
2
0
6
0
0
2
y
a
M
6
0
0
2
p
e
S
7
0
0
2
n
a
J
7
0
0
2
y
a
M
7
0
0
2
p
e
S
8
0
0
2
n
a
J
8
0
0
2
y
a
M
8
0
0
2
p
e
S
9
0
0
2
n
a
J
9
0
0
2
y
a
M
9
0
0
2
p
e
S
0
1
0
2
n
a
J
0
1
0
2
y
a
M
0
1
0
2
p
e
S
1
1
0
2
n
a
J
1
1
0
2
y
a
M
1
1
0
2
p
e
S
2
1
0
2
n
a
J
2
1
0
2
y
a
M
2
1
0
2
p
e
S
Figure 8: Participating attack hosts in the distributed attacks detected from
2005 to 2012 at LBNL.
HONEY
CAMPOFF
RSRCHLAB
p
a
l
r
e
v
o
e
g
a
t
n
e
c
r
e
P
0
0
1
0
8
0
6
0
4
0
2
0
1
2
3
4
6
7
8
9
10 11 12 24 25 26 29 30
Attack Number
Figure 9: Percentage overlap of attack hosts seen at LBNL with that at sites
HONEY, CAMPOFF and RSRCHLAB. The ﬁgure displays only the subset
of attacks that appear in at least one of the three sites. (Note that none of
the attacks appear in HOMEOFF).
Figure 7 shows the empirical CDF of the span of detected at-
tack epochs. These coordinated attacks often span multiple days,
and sometimes multiple weeks. The majority of the attacks ex-
hibit strong coordination glue in terms of either the set of local ma-
chines probed or the usernames targeted for brute-forcing. Of the
90 true attack epochs, 62 have common-set-of-local-machines glue
and 25 have username-“root” glue. Only 3 epochs did not manifest
any glue we could identify; these epochs probed machines across
a wide range of addresses and using a dictionary of generic user-
names, such as mysql and admin.
Figure 8 shows the attack hosts participating in the various dis-
tributed attack epochs over time, where we number distinct hosts
consecutively starting at 1 for the ﬁrst one observed. The sig-
niﬁcant overlap of attack hosts across attack episodes shows that
many of these attacks employ the same botnet. We then analyzed
the coordination glue in these attack epochs to consolidate the set
of epochs into attack campaigns. We use the following rules to
group epochs into campaigns based on observing evidence that
the same attacker conducting different attack epochs that work to-
wards the same goal: (i) epochs with the same common-set-of-
local-machines coordination glue, and (ii) epochs appearing on the
same day with username-root coordination glue. Our detector con-
siders these as multiple attack epochs rather than a single attack
because this is indeed how the campaign proceeds, stopping for a
few hours/days and then reappearing. Using these rules, we group
the 62 attacks with common-set-of-local-machines glue into 12 dis-
tinct attack campaigns. Only a few of the 25 epochs group using
heuristic (ii), condensing the root-set to 20 campaigns. This leaves
us with a total of 35 attack campaigns.
Table 4 summarizes the magnitude, scope and stealthiness of the
attacks we detect. All of these attacks were stealthy when observed
from the viewpoint of individual hosts; on average the attack hosts
made ≈ 2 attempts per local machine per hour. We can however
detect a large fraction of these attack campaigns using a point-wise
network-based detector that looks for high-rate hourly activity in
terms of either the total number of failed attempts or the number of
local hosts contacted. Note that we also detect attacks that a site
cannot detect using either host-based or network-based point-wise
detection (campaigns 5, 7 and 8 in Table 4). Finally, two of the
campaigns succeeded, the ﬁrst of which (campaign 1) as best as we
can tell went undetected by the site.
We also ﬁnd a difference in the characteristics between attacks
that have set-of-local-machines coordination glue versus the ones
that only have username-root glue. The latter tend to target a wide
range of the site’s address space and often involve just a few at-
tack hosts brute-forcing at a high rate. Attacks having set-of-local-
machines coordination glue often exhibit the pattern of the attack-
ers stopping and coming back. We did not ﬁnd any sequential pat-
tern in any of these campaigns; rather, the attackers targeted servers
spread across the address space, often including addresses in both
of LBNL’s distinct address blocks. We also did not ﬁnd any pattern
among the local servers in terms of belonging to the same research
group or compute cluster.
6.3 Establishing the scope of attacks
Next, we attempt to establish which attacks speciﬁcally targeted
the LBNL site versus global attacks that indiscriminantly probed
the site. To do so we look for whether the attack hosts of a given
campaign appeared in any of our four correlation datasets, HONEY,
RSRCHLAB, HOMEOFF, and CAMPOFF.
We ﬁnd that 16 campaigns appear in at least one of these four
datasets. These include ﬁve username-root coordination glue at-
tacks and all but one of the attacks with set-of-local-machines co-
ordination. Figure 9 plots the percentage overlap of the attack hosts
detected in the global attacks at LBNL with that at other sites,
showing a high overlap in most cases. We investigated campaign 5,
which does not appear at any of the other sites, and found that it
indeed targeted LBNL, as the attack hosts all probed a set of six
usernames each valid at the site. As shown by the hourly rates in
Table 4, this targeted attack also proceeded in a stealthy fashion,
with each remote host on average making only 9 attempts and con-
tacting 3 local servers per hour. It’s possible that some of the other
campaigns also speciﬁcally targeted LBNL, though for them we
lack a “smoking gun” that betrays clear knowledge of the site.
Finally, to give a sense of the nature of global attacks, Fig-
ure 10 shows the timing patterns of login attempts at the LBNL
and HONEY sites during part of campaign 8. From the clear corre-
lation (though with a lag in time), we see that the activity at both
reﬂects the same rate (which varies) and, for the most part, the same
active and inactive periods.
7. CONCLUSION
In this work we propose a general approach for detecting dis-
tributed, potentially stealthy activity at a site. The foundation of the
method lies in detecting change in a site-wide parameter that sum-
marizes aggregate activity at the site. We explored this approach in
concrete terms in the context of detecting stealthy distributed SSH
brute-forcing activity, showing that the process of legitimate users
93ID Appearances
Attrs.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
2007: [Jul 7-9], [Oct 20-23], [Nov 5-9](2), [Nov 13-18](2)
2008: [Apr 30 - May 7],[May 8-14](3)
2008: [Jun 28-29], [Jun 30 - Jul 1]
[Jul 7-9], [Aug 17-21], [Sep 1-8] (5)
2008: [Sep 8-13](3)
2008: [Sep 16-18]
2008: [Sep 23-26](2), [Sep 29 - Oct 2](2)
2008: [Nov 18-19], [Nov 20 - Dec 29](5) 2009: [Apr 7-9]
2009: [Oct 22-23], [Oct 27 - Nov 24](5)
2010: [Dec 6 - Jan 10](6), [Jan 11-18], [Jan 20-22], [Mar 4-8]
2010: [Jun 16 - Jul 27](2), [Jul 29 - Aug 11]
2010: [Nov 1-6] (2), [Nov 7-8], [Nov 27 - Dec 1], [Dec 15-17]
2011: [Oct 11-19], [Oct 25-29](2), [Nov 4-7], [Nov 17-20]
2010: [Mar 30 - Apr 1]
2010: [Apr 23-26]
2010: [May 7-10]
2010: [Sep 20-22]
2010: [Dec 27-30]
2011: [Feb 10-14](2)
2011: [May 16-18]
2011: [Jul 21-22]
2011: [Aug 2-6]
2011: [Aug 7-9]
2011: [Aug 17-21](2)
2011: [Nov 2-4]
2011: [Nov 30 - Dec 5]
2011: [Dec 18-20]
2012: [Jul 20-21]
2012: [Aug 27 - Sep 2]
2012: [Sep 26-29]
2012: [Oct 8 - Nov 1](4)
2012: [Nov 16-18]
2012: [Nov 30 - Dec 2]
2008: [Jan 9-12]
2011: [Apr 8-26]