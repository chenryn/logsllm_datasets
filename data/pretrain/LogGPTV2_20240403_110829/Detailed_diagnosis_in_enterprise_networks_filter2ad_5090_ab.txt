While our study is based on small enterprise networks, we believe
that the kinds of problems it reveals also plague large enterprises. Ex-
isting diagnostic systems for large enterprises such as Sherlock [ʅ] are
not capable of diagnosing such faults. In order to scale, they focus on
coarser faults such as a DNS server failing completely. Our work asks
whether the detailed faults that we observe in our logs can be diag-
nosed if scalability is not a prime concern. If our techniques can be
scaled, they will bene(cid:12)t large enterprises as well. We discuss how to
scale NetMedic in §@.
3. PROBLEM FORMULATION
We now formulate the diagnosis problem in a way that helps oper-
ators with the kinds of issues that we uncover in our logs. Our goal is
to build a system that can narrow down the likely causes responsible
for a wide range of faults such as poor performance, unreachability,
or application speci(cid:12)c issues. ˆis ability is the (cid:12)rst and perhaps the
hardest aspect of troubleshooting. Once the operators have identi(cid:12)ed
Generic variables
ʂ processor time
ʂ user time
io data bytes/sec
thread count
page faults/sec
page (cid:12)le bytes
working set
Application variables
current (cid:12)les cached
connection attempts/sec
(cid:12)les sent/sec
get requests/sec
put requests/sec
head requests/sec
not found errors/sec
Table ʆ. Example variables in Web server state. In all there are ʅ@
generic and ʄʅʉ application speci(cid:12)c variables.
the true culprit using our system, they can proceed to repairing the
fault. Automatic repair is not our goal in this work.
We want our system to have the following two properties.
ʄ. Detail: ˆe system should be able to diagnose both applica-
tion speci(cid:12)c and generic problems. Further, it should identify likely
causes with as much speci(cid:12)city as possible. If a process is responsible,
it should identify that process rather than the hosting machine. If ap-
plication con(cid:12)guration is responsible, it should identify the incorrect
con(cid:12)guration rather than simply blaming the application.
ˆe need for detailed diagnosis clearly stands out in our logs.
Most faults are application-speci(cid:12)c. ˆe callers o(cid:13)en knew which
machine was faulty but did not know what aspect was faulty.
ʅ. Application agnosticism: ˆe system should rely on minimal
application speci(cid:12)c knowledge. Enterprises run numerous applica-
tions across the board. It is intractable for a general diagnostic system
to contain knowledge of every possible application.
ˆese two properties are con(cid:8)icting. How can application speci(cid:12)c
faults be detected without application knowledge? For instance, the
straightforward way to detect that an application client is receiving
error messages is through knowledge of the protocol. Detecting faults
that are not re(cid:8)ected in protocol messages may require even more
application knowledge. We layout the problem and explain how we
reconcile these con(cid:8)icting goals below.
3.1 Our inference problem
ˆere are several approaches that one might consider to diagnose
faults in a computer network. To be able to diagnose a wide range of
faults, we take an inference-based approach rather than, for instance,
a rule-based approach (§@). However, our goals require a richer net-
work model than current inference models. We (cid:12)rst describe our
model and then explain how it di(cid:11)ers from existing models.
We model the network as a dependency graph between compo-
nents such as application processes, host machines, and con(cid:12)gura-
tion elements. ˆere is a directed edge between two components if
the source directly impacts the destination. ˆe dependency graph
may contain cycles–in particular, two components may be connected
by edges in both directions. Our system automatically constructs the
dependency graph.
ˆe state of a component at any given time consists of visible and
invisible parts, of which only the former is available to us. For in-
stance, the visible state of an application process includes generic as-
pects such as its processor usage and some application-speci(cid:12)c as-
pects. ˆe invisible state may include values of program variables
and some other application-speci(cid:12)c aspects. Table ʆ shows a subset
of the variables that form the Web server process’s visible state in our
prototype. We represent visible state using multiple variables, each
corresponding to a certain aspect of the component’s behavior. ˆe
set of variables di(cid:11)ers across components. ˆe diagnostic system is
unaware of the semantics of the variables.
Given a component whose visible state has changed relative to
some period in the past, our goal is to identify the components likely
responsible for the change. In other words, we want to identify the
causes underlying an observed e(cid:11)ect. Each identi(cid:12)ed causes has the
properties that: i) its visible state changes can explain the observed
e(cid:11)ect; ii) its visible state changes cannot be explained by visible state
changes of other components.
245[ High load ]
Machineserver
Server
[ High inbound request rate ]
[ High outbound request rate ]
[ Normal outbound request rate ]
[High response time]
[High response time]
Cprolific
MachineCP
[ Normal load ]
Cvictim
MachineCV
[ Normal load ]
Figure ʄ. Illustration of Problem @ in Table ʄ. ˆe rectangles are
processes and the ellipses are host machines. ˆe relevant state of
the components is shown in brackets.
For instance, consider Figure ʄ, which illustrates Problem @ of Ta-
ble ʄ. Both clients experience high response times because Cproliﬁc is
overwhelming the server. Suppose we want to diagnose why the re-
sponse time is high for Cvictim. Although the load on Server leads to
high response times, we want to identify Cproliﬁc as the culprit, since
Cproliﬁc is responsible for both Server’s high load and Cvictim’s high re-
sponse times, and its behavior cannot be explained by other visible
factors. It may have been externally impacted, but lacking further
visibility, the diagnosis will identify it as the culprit.
We do not assume that the e(cid:11)ect being diagnosed represents a
deterioration. ˆus, our system can be used to explain any change,
including improvements. ˆis agnosticism towards the nature of
change and the lack of knowledge of the meaning of state vari-
ables lets us diagnose application-speci(cid:12)c behaviors without applica-
tion knowledge. If applications export their current experience, e.g.,
number of successful and failed transactions, the system treats these
experiences as part of the state of the application process and diag-
noses any changes in them. We assume that the state variables are
well-behaved—small changes in component behaviors lead to rel-
atively small changes in variable values and signi(cid:12)cant behavioral
changes are detectable using statistical methods. We (cid:12)nd that this
assumption holds for the state variables exported by the components
in our prototype.
3.2 Limitations of existing models
Existing models [ʅ, ʈ, ʄ@] di(cid:11)er from our formulation in three im-
portant ways that makes them unsuitable for detailed diagnosis. First,
they use a single variable to represent component health. However, if
exposing and diagnosing a rich set of failure modes is desired, com-
ponent state must be captured in more detail. One might be tempted
to abstract away the detail and just present a faulty-or-healthy status
for each component, but some types of component failures impact
other components while others do not. For instance, an application
process has the ability to hurt other processes on the same machine,
but typically, it hurts them only when it consumes a lot of resources
and not otherwise. To correctly determine if a process is impacting
others, its state must be captured in more detail.
In principle, a component with multiple variables is logically
equivalent to multiple components with a variable each. In prac-
tice, however, the di(cid:11)erence is signi(cid:12)cant. Dividing a component
into constituent variables forces us to consider interactions within
those variables. Given the internal complexities of components and
that there can be hundreds of variables, this division signi(cid:12)cantly in-
creases the complexity of the inference problem. Further, as we will
show, keeping a multi-variate component intact lets us extract useful
information from the collective behavior of those variables.
Second, existing models assume a simple dependency model in
which a faulty component hurts each dependent component with
some probability. Turning again to the faulty process example above,
we can see that whether a component impacts another depends in a
more complex way on its current state.
Finally, existing models do not allow circular dependencies by
which two components have a direct or indirect mutual dependence.
When viewed in detail, circular dependencies are commonplace. For
instance, processes that run on the same machine are mutually de-
pendent, and so are processes that communicate.
3
Check how similar 
those states of D 
are to Dnow
2 Recover the state of D 
during those time periods
DTe
DTd
DTc
DTb
DTa
Dnow
D
e
m
T
i
STe
STd
STc
STb
STa
Snow
S
1
Identify time periods 
when the state of S 
was similar to Snow
Figure ʅ. Computing the weight of the edge from S to D.
4. USING HISTORY TO GAUGE IMPACT
Solving our inference problem requires us to estimate when a
component might be impacting another. ˆe primary di(cid:14)culty in
this estimation is that we do not know a priori how components in-
teract. Our lack of knowledge stems from application agnosticism.
Even if we had not chosen an application-agnostic approach, it ap-
pears unrealistic to embed detailed knowledge of component inter-
action into the design of the diagnostic system. For instance, there
is no general way to specify how network path congestion impacts
application processes because the impact varies across applications.
One could use time to rule out the possibility of impact along cer-
tain dependency edges. A component that is currently behaving nor-
mally is likely not impacting one that is currently abnormal. For in-
stance, in Figure ʄ, because the host machine of Cvictim is behaving
normally, we can rule it out as a possible culprit. However, time-
based elimination is limited because it cannot deduce what is impact-
ing what. Returning to the example, we see that both clients as well as
Server and Machineserver are abnormal. Time-based elimination alone
cannot tell which of these might be the culprit. Instead, we must use
a more precise analysis based on the states of various components.
Our level of detail makes the challenge more daunting. Com-
ponent states include many variables (e.g., some applications ex-
pose over (cid:12)(cid:13)y variables in our implementation); it is not uncommon
for at least some variables to be in an abnormal state at any time.
Amidst this constant churn, we need to link observed e(cid:11)ects to their
likely causes, while ignoring unrelated contemporaneous changes
and without knowing a priori either the meanings of various state
variables or the impact relationship between components.
We address this challenge using a novel, history-based primitive.
ˆis primitive extracts information from the joint historical behavior
of components to estimate the likelihood that a component is cur-
rently impacting a neighbor. We use this estimated likelihood to set
edge weights in the dependency graph. ˆe weights are then used
to identify the likely causes as those that have a path of high impact
edges in the dependency graph leading to the a(cid:11)ected component.
We provide in this section the intuition underlying our history-
based primitive; we explain in §ʈ.ʆ how exactly it is implemented in a
way that is robust to the real-world complexities of component states.
In Figure ʅ, assume that the current state of the source component S
is Snow and of the destination D is Dnow. We want a rough estimate
of the probability that S being in Snow has driven D into Dnow. We
compute this by searching through the history for periods when the
state of S was “similar” to Snow. Informally, similarity of state is a
measure of how close the values are for each variable. We quantify
it in a way that does not require the knowledge of the semantics of
the state variables and appropriately emphasizes the relevant aspects
of the component’s behavior. ˆe edge weight is then a measure of
how similar to Dnow is the state of D in those time periods. If we do
not (cid:12)nd states similar to Snow in the history, a default high weight is
assigned to the edge.
Intuitively, if D’s state was o(cid:13)en similar to Dnow when S’s state was
similar to Snow, the likelihood of S being in Snow having driven D into
Dnow is high. Alternately, if D was o(cid:13)en in dissimilar states, then the
chances are that Snow does not lead to Dnow.
ˆis reasoning is reminiscent of probabilistic or causal infer-
ence [ʄʆ, ʅʆ]. But because component states are multi-dimensional,
real-valued vectors, we are not aware of a method from these (cid:12)elds
246Component types, 
data sources
Capture 
component states
Component 
states
Dependency 
templates
Generate 
dependency graph
Dependency 
graph
Time period to 
diagnose, historical 
time range, affected 
components (optional)
Diagnosis
a) Compute abnormality
b) Compute edge weights
c) Rank likely causes
Ranked list of likely causes 
for each affected component
Machine
Process
NbrSet
Path
Con(cid:12)g
CPU utilization, memory usage, disk usage, amount
of network and other IO
Generic variables: CPU utilization, memory usage,
amount of network and other IO, response time to
servers, tra(cid:14)c from clients
Application speci(cid:12)c variables: Whatever is available
State relevant to communication peers, e.g., inbound
and outbound tra(cid:14)c, response time
Loss rate and delay
All relevant key-value pairs
Figure ʆ. ˆe work-(cid:8)ow of NetMedic.
Table ʇ. Example state variables that NetMedic captures.
that we can directly apply. Crudely, what we are computing is the con-
ditional probability Prob(D = Dnow|S = Snow) and assuming that it re-
(cid:8)ects causality. Conditional probability in general does not measure
causality, but we (cid:12)nd that the assumption holds frequently enough in
practice to facilitate e(cid:11)ective diagnosis. Further, we do not infer com-
plex probabilistic models to predict the conditional probability for
each pair of S-D states; such models typically require a lot of training
data. Instead, we estimate the required probability on demand based
on whatever historical information is available.
Consider how our use of history helps in Figure ʄ. ˆe estimated
impact from Server to Cvictim will be high if in the past time periods
when Server had high inbound request rate, Cvictim had high response
time along with normal outbound request rate. ˆe estimated impact
from Server to Cproliﬁc will be low if during those time periods, Cproliﬁc
had normal outbound request rate. On the other hand, the estimated
impact from Cproliﬁc to Server will be high if Cproliﬁc never had high
outbound request rate in the past or if Server had high inbound re-
quest rate whenever it did. ˆis way, we obtain a high impact path
through Server from Cproliﬁc to Cvictim, without the need for interpret-
ing client and server state variables.
Whether the weight is correctly determined for an edge depends
of course on the contents of the history. We (cid:12)nd that estimating
the correct weight for every edge is not critical. What is important
for accurate diagnosis is an ability to correctly assign a low weight
to enough edges such that the path from the real cause to its e(cid:11)ects
shines through. We show later that our method can accomplish this
using only a modest amount of history.
5. DESIGN
ˆe work(cid:8)ow of NetMedic is depicted in Figure ʆ. Its three main
functional pieces capture the state of network components, generate
the dependency graph, and diagnose based on component states and
the dependency graph. We describe each piece below.
5.1 Capturing component state
ˆere are many ways to partition a network into constituent com-
ponents. Our partitioning is guided by the kinds of faults that appear
in our logs—components in our current design include application
processes, machine, and network paths, as well as con(cid:12)guration of
applications, machine, and (cid:12)rewalls. ˆe machine component bun-
dles the hardware and the OS.
In addition, we also include a virtual component, called NbrSet
(short for Neighbor set). A NbrSet represents the collective behav-
ior of communication peers of a process. Its state variables represent
information such as tra(cid:14)c exchanged and response time aggregated
based on the server-side port. In the presence of redundant servers
(e.g., for DNS), it helps model their collective impact on the client
process. Similarly, it models the collective impact of all the clients for
a server process. Using a NbrSet instead of individual dependencies
allows us to model the dependencies more accurately [ʅ].
ˆe granularity of diagnosis is determined by the granularity of
the modeled components. For instance, using the full network path
as a component implies that culprits will not be identi(cid:12)ed at the level
of individual switches. Our framework, however, can be extended to
include (cid:12)ner-grained components than those in our current design.
NetMedic periodically captures the state of each component as a
multi-variable vector. State is stored in one-minute bins. ˆe bin size