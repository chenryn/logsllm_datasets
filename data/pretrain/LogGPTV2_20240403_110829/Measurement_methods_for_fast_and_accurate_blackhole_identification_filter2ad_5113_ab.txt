odic losses. In the worst-case scenario where the loss period
equals μ, choosing γ = xr implies that the probability of
sending a probe in a loss episode is less than 1/2x. Thus,
small values of γ are enough to avoid phase locking, result-
ing in limited impact on F . In the rest of this paper, we use
γ = 0.1, which is applicable to a wide range of loss rates [1].
2.2 Evaluation
We evaluate our probing method in a controlled envi-
ronment using Emulab and in wide-area experiments using
PlanetLab. Controlled experiments allow us to measure the
accuracy of our technique, because we have ground truth.
On the other hand, wide-area experiments allow us to test
our method under actual loss scenarios.
2.2.1 Controlled experiments
Because Emulab can only introduce random losses, we de-
ployed a modiﬁed version of the Linux Netem module that
allows us to inject random (i.e., Poisson), bursty (i.e., up to
three-state Gilbert models), and periodic losses on links. In
this section, we use Fast Ethernet links between nodes, with
packet losses occurring in both directions independently. We
used the Abilene OSPF topology and varied emulated link
latencies from zero (i.e., only native OS and hardware de-
lays) to 20 ms, with quantitatively similar results. We add
background traﬃc of 500 packets per second in each direc-
tion to trigger state changes in the Gilbert model when emu-
lating bursty losses. To study our conﬁrmation scheme with
diﬀerent conﬁgurations, we also varied the values of κ and
−5, as this value represents a good trade-
μ. We set F = 10
oﬀ for a wide range of path loss rates, and μmin to 100 ms,
which guarantees that at most two conﬁrmation probes are
simultaneously queued at routers [4].
We investigated how detection errors vary as a function of
path loss rates and burst lengths. To cover a large portion
of previously reported loss rates [20, 21, 25, 26], we varied
path loss rates between 0.01% and 1% and average burst
lengths from 4 ms to 40 ms. We used synthetic and real ISP
topologies and found qualitatively similar results.
Detection-error rates with diﬀerent loss rates. In our
experiments, detection errors increase when r increases and
κ is kept constant. Our proposed scheme varies κ between
−5, depending
two and ﬁve to achieve the target F = 10
on the path loss rate. Spacing probes signiﬁcantly reduces
detection errors compared to sending back-to-back probes.
If probes are suﬃciently spaced, detection errors decrease
with diminishing returns when r is constant and κ increases,
as expected from Eq. (2).
e
e
t
t
a
a
R
R
r
r
o
o
r
r
r
r
E
E
−
−
n
n
o
o
i
i
t
t
c
c
e
e
t
t
e
e
D
D
 0.001
 0.001
 0.0001
 0.0001
 1e−05
 1e−05
 1e−06
 1e−06
κ = 1
κ = 2
κ = 3
κ = 4
Back−to−Back
Spaced (μ = 0.2sec)
Proposed Scheme
 15
 15
 10
 35
 10
 35
Average Loss Burst Length (ms)
Average Loss Burst Length (ms)
 30
 30
 20
 20
 25
 25
 40
 40
Figure 2: Burst length vs detection errors—Emulab.
Detection-error rates with diﬀerent burst lengths.
Fig. 2 shows the detection-error rate when varying the av-
erage burst length in the Abilene topology for ﬁxed per-link
loss rates of 0.1%. Results for other loss rates are quali-
tatively similar. For κ ≥ 2, we show results for back-to-
back probes and probes spaced by 200 ms. For back-to-back
probes, the detection-error rate increases with the burst
length, as the probability of conﬁrmation packets falling in
the same loss burst increases. Results for μ = 200 ms are
independent of the burst length (i.e., a straight line) for
b ≤ 20 ms because the probability of a loss burst lasting
more than 200 ms in these cases is close to zero. However,
when b = 40 ms, the probability of a burst lasting more
than 200 ms is higher and the detection-error rate increases.
Sending two spaced conﬁrmation probes is better than send-
ing many back-to-back ones. Finally, the proposed scheme
is the only one that achieves the target detection-error rate
−5 for all burst lengths, by setting κ = 4 and varying
of 10
μ between 100 ms and 398 ms. When bursts are long, μ
is increased to avoid sending probes during the same loss
burst. When bursts are short, μ is equal to μmin to reduce
total conﬁrmation time. When b = 4 ms, the total conﬁr-
mation time (T = κ × μ) of the proposed scheme is 400 ms,
which is half the conﬁrmation time when using κ = 4 and
μ = 200 ms (i.e., squares with solid line in Fig. 2).
Detection-error rates with other loss models. We
evaluated the eﬀect of applying our method to losses given
by a more general three-state Gilbert model, capable of cap-
turing loss processes where the probability of losing a single
packet is diﬀerent from having a loss burst. We found that
the increase in F is less than one order of magnitude even if
the fraction of single packet losses (i.e., no burst) is as high
as 90%, a scenario where the two-state model is very inac-
curate. We also ran experiments with periodic losses and
saw that varying packet inter-arrivals with γ > r is enough
to prevent phase locking. We present a more detailed dis-
cussion in an extended version of this paper [8].
Summary. These experiments show that our probing
scheme adapts the value of κ and μ to path loss rates and
burst lengths to successfully achieve a target detection-error
rate while minimizing total conﬁrmation time. The method
is also robust to diﬀerent loss processes.
2.2.2 Wide-area Internet experiments
We aim to determine whether failure conﬁrmation is use-
ful in practical scenarios. Our deployment in an enterprise
257F
F
D
D
C
C
 0.9
 0.9
 0.8
 0.8
 0.7
 0.7
 0.6
 0.6
 0.5
 0.5
 0.4
 0.4
 0.3
 0.3
 0.2
 0.2
 0.1
 0.1
 0
 0
κ=1
κ=2
κ=3
κ=5
κ=10
Spaced
Back−to−back
 0
 0
 0.05
 0.05
 0.1
 0.1
 0.15
 0.15
 0.2
 0.2
 0.25
 0.25
Confirmed Failures (%)
Confirmed Failures (%)
Figure 3: Eﬃcacy of conﬁrmation in PlanetLab.
network resulted in only a few iterations of the conﬁrmation
method, because this network is fairly stable and we only
ran the experiment for two weeks. We then used a larger
deployment with 200 PlanetLab nodes, each of which probed
the other nodes periodically. PlanetLab nodes are frequently
overloaded and induce short bursts of packet loss [24]. Al-
though we cannot diﬀerentiate host- and network-induced
packet loss (a situation that can happen in any real deploy-
ment), PlanetLab’s dynamic environment is very demanding
of conﬁrmation methods and worth investigating.
We compared the relative performance of two conﬁrma-
tion schemes run during the same time period, so that they
will experience similar node conditions. The ﬁrst scheme
sends conﬁrmation probes back-to-back, while the second
sends conﬁrmation probes spaced by 200 ms.
If back-to-
back probes conﬁrm a failure, but spaced probes do not,
we know that the ﬁrst has raised a detection error. We
pick μ = 200 ms based on the observation that incidence of
bursty packet loss usually lasts less than 100 ms [18, 21, 25].
We also used conﬁrmation probes spaced by 2 seconds and
found similar results.
Figure 3 shows the cumulative distribution of the fraction
of conﬁrmed failures for each PlanetLab node, for diﬀerent
numbers of conﬁrmation probes, and for diﬀerent probing
strategies. We compute the fraction of conﬁrmed failures by
dividing the number of conﬁrmed failures by the total num-
ber of lost probes at each node. We see that increasing κ
reduces the number of conﬁrmed failures (i.e., detection er-
rors). The two rightmost lines show the fraction of conﬁrmed
failures when κ = 1. A larger κ shows that sending probes
with delay in between each probe conﬁrms signiﬁcantly fewer
failures than the one using back-to-back probes. We also see
diminishing returns as in the controlled experiments. When
probes are spaced, increasing κ from ﬁve to ten decreases
detection errors by (1.3%) less than increasing κ from three
to ﬁve (1.8%). Using more than ten conﬁrmation probes
yields minimal improvement. When using ﬁve conﬁrmation
probes, spacing probes conﬁrms 38% less failures than us-
ing back-to-back probes (i.e., the diﬀerence between the two
curves for κ = 5). All failures in these 38% are detection
errors, indicating that using the proposed scheme can sig-
niﬁcantly improve the quality of measurements to be used
by tomography algorithms.
3. AGGREGATING FAILURES
Fig. 1). The main challenge with aggregation is construct-
ing a reachability matrix where the end-to-end path mea-
surements have a consistent view of the status of the links
in the network. An inconsistent reachability matrix could
introduce false alarms or wrong identiﬁcation of failures. Ex-
isting work on binary tomography algorithms presumes that
the reachability matrix is consistent; in practice, however,
consistency is diﬃcult to achieve. This section explains the
challenges in achieving consistency, proposes strategies for
constructing a consistent reachability matrix, analyzes their
delay, and evaluates them both analytically and empirically.
3.1 Deﬁnitions
We deﬁne a reachability matrix as a matrix, M , where
each entry Mmd is the binary status (i.e., up or down) of
the path from monitor m to destination d. Note that the
matrix may be incomplete, since not all monitors will moni-
tor all destinations. We deﬁne aggregation as the process of
combining path status measurements from the monitors to
construct this reachability matrix. Suppose that some set of
monitored paths cross a particular link in the network. In-
formally, consistency says that if a particular link is “down”
then all paths that cross that link should have status “down”,
and that if all links in a path are “up” then that path should
have status “up”. We now formalize this notion.
Let Pm denote the set of paths probed by a monitor m
and P = ∪∀mPm the set of paths from all monitors. We
deﬁne the hitting set of a link (cid:5), H(cid:4), as the set of paths that
traverse (cid:5).1 We say that a reachability matrix is consistent
at an instant in time if, for every (cid:5) that is failed, all paths
in H(cid:4) have a status of down and all paths that contain only
working links have a status of up in the reachability matrix.
3.2 Challenges in Achieving Consistency
Binary tomography algorithms take as input a consistent
reachability matrix; unfortunately, two factors make it diﬃ-
cult to construct a consistent reachability matrix in practice:
Lack of synchronized measurements. Because moni-
tors probe at diﬀerent times, diﬀerent monitors may observe
diﬀerent characteristics for the same link, thus creating in-
consistencies in the reachability matrix.
Our goal is to design an aggregation strategy that builds a
consistent reachability matrix with small aggregation delay.
If monitors could probe all paths in P simultaneously, then
the resulting reachability matrix would be consistent. Many
tomography algorithms [2, 9, 30] assume consistent inputs.
Unfortunately, synchronous measurements are impossible in
practice. First, measurements from a single monitor are
not instantaneous, because probing many destinations takes
time (in our experiments, this process takes from tens of sec-
onds to minutes). Second, each monitor probes a diﬀerent
set of paths, so it is impossible to guarantee that two moni-
tors probe the same link simultaneously. Monitors have dif-
ferent cycle lengths as each monitor probes a diﬀerent set of
destinations, and machines have diﬀerent processing power
and available bandwidth. The overall cycle length, C, is the
time it takes the slowest monitor to probe all its paths; as we
will see in Sec. 3.4, the overall aggregation delay is a function
of both the cycle length and the aggregation strategy.
Detection errors. Detection errors from failure conﬁrma-
This section develops aggregation methods for combining
path measurements into a reachability matrix (Step 3 in
1For simplicity, we refer to the hitting set of a link, but this
deﬁnition also applies for sets of links.
258tion (Sec. 2) may create situations where a path is considered
to have failed when it has not. These errors in the reachabil-
ity matrix may also cause a tomography algorithm to reach
incorrect conclusions about which links have failed. In this
section, we use F to denote the actual detection-error rate of
failure conﬁrmation, which may be diﬀerent from the target
detection-error rate of Sec. 2.
3.3 Aggregation Strategies
We propose and evaluate three strategies for aggregating
path measurements into a reachability matrix. The ﬁrst
method is simple and fast, but it can build inconsistent ma-
trices if failures are short or if there are detection errors
(basic, Sec. 3.3.1). We then consider two enhancements
that wait longer to build matrices but achieve higher consis-
tency: a conservative one (mc, Sec. 3.3.2) and another that
is more tolerant to detection errors (mc-path, Sec. 3.3.3).
We present models that capture how detection errors and
unsynchronized measurements may introduce inconsistency.
3.3.1 Basic approach
The basic aggregation strategy works as follows. First,
detect that a path status changed from up to down. Then,
wait a full cycle C for monitors to probe all paths in P.
Finally, build a reachability matrix by combining the path
statuses reported in the latest measurement cycle. This sim-
ple strategy assumes that if (cid:5) fails, all paths in H(cid:4) will be
conﬁrmed as down after C.
Consistency analysis. The consistency of basic depends
on the duration of the failure, f , relative to the cycle length,
C. We analyze consistency in three scenarios. We ﬁrst as-
sume that failure conﬁrmation gives no detection errors, and
relax this assumption later.
Scenario 1 (long failures): f > 2C.
In S1, monitors
probe all paths in H(cid:4) while (cid:5) is down; hence, the coordinator
always builds a consistent reachability matrix. The average
consistency in this scenario is 1.
Scenario 2 (intermediate failures): C ≤ f ≤ 2C. In S2,
all paths in H(cid:4) will be down during a full cycle because the
failure is longer than C. When the coordinator builds the
reachability matrix, however, (cid:5) may have recovered. In this
case, the matrix is consistent with (cid:5)’s failure, but the failure
no longer persists. We call these instances late identiﬁca-
tions. We consider these cases consistent, but it is simple to
extend the analysis to consider late identiﬁcation as incon-
sistent, but we omit this analysis for conciseness.
Scenario 3 (short failures): f < C. In S3, the coordi-
nator may build an inconsistent reachability matrix because
monitors may probe some paths in H(cid:4) while (cid:5) is down and
P
others when it is up. Let F be the set of all possible failures
(cid:4)∈F |H(cid:4)|/|F| be the average hitting set size. We
and H =
identify two cases:
1. Late identiﬁcation: The probability of probing all
paths in H(cid:4) while (cid:5) is failed and getting a consistent
reachability matrix is approximately p = (f /C)H .
2. Inconsistent reachability matrix: In all other cases, the
reachability matrix will be inconsistent. The consis-
tency of the reachability matrix in these cases depends
on the number of paths in H(cid:4) that were probed during
f . Given that the reachability matrix is inconsistent,
at least one path has to probe (cid:5) during f (i.e., the
path that detected the failure) and at most H(cid:4) − 1
can probe (cid:5) during f (otherwise, the matrix would be
consistent). Hence, we can approximate the average
number of paths in H(cid:4), for all (cid:5) ∈ F, probed during
a failure by N = 1 + (f /C) × (H − 2). The average
consistency in these cases is 1 − N/|P|.
Combining these three scenarios, the expected consistency
of basic when there are no detection errors is:
(
E[consbasic] =
1
p + (1 − p)
”
“
1 − N|P|
if f ≥ C,
if f < C.
(5)