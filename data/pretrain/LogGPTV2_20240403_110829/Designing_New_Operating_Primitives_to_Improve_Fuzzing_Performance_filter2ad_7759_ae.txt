300k
200k
100k
0k
250k
200k
150k
100k
50k
0k
(c) woff
1
15
30
45
60
75
90
105
120
(f) c-ares
1
15
30
75
60
45
(i) libjpeg
90
105
120
1
15
30
45
60
#core
75
90
105
120
Figure 5: The execution numbers per second of the original and our optimized version of AFL by fuzzing Google’s fuzzer test suite plus libjpeg
on 1 to 120 cores. In the case of openssl-1.0.2d, from 90 cores, each fuzzing instance times out, which results in no further execution as there
are no valid input test cases for mutations due to the severe performance bottlenecks in the stock version of AFL.
c
e
s
/
s
c
e
x
E
1600k
1200k
800k
400k
0k
150k
1
100k
50k
c
e
s
/
s
c
e
x
E
0k
1
(b) libpng
1
15
30
45
60
(e) libxml
75
90
105
120
(a) harfbuzz
stock
opt
15
30
45
60
(d) woff
75
90
105
120
7000k
6000k
5000k
4000k
3000k
2000k
1000k
0k
750k
600k
450k
300k
150k
15
30
45
60
#core
75
90
105
120
0k
1
15
30
45
60
#core
75
90
105
120
600k
400k
200k
0k
2800k
2400k
2000k
1600k
1200k
800k
400k
0k
(c) sqlite
1
15
30
60
45
75
(f) boringssl
90
105
120
1
15
30
45
60
#core
75
90
105
120
Figure 6: The execution numbers per second of the original and our optimized version of LibFuzzer by fuzzing Google’s fuzzer test suite on 1
to 120 cores.
count from 1.1 – 735.7×. We measure at least two orders of mag-
nitude in change with libpng and boringssl because both suffer
from the inefficient syncing phase of the LibFuzzer, and we im-
prove their scalability by 145.8× and 735.7× via the in-memory test
case log (see §3.3). Another reason for such overhead is that neither
of them can find a good test case, which results in periodic corpus
directory enumeration to find new interesting test cases from col-
laborating LibFuzzer instances. While other libraries do not suffer
from the poor test cases, their scalability is improved by 1.1 – 1.3×
because they benefit from our shared in-memory test case log as
well as the dual file system service.
12
6.3 Evaluating Design Choices
We now evaluate the effectiveness of each of our design decisions
by evaluating some sets of benchmarks. We choose libpng for
evaluation, as both Figure 5 and Figure 6 illustrate that it has the
highest improvement over the version of fuzzers. We evaluate the
scalability behavior of our optimized AFL on the tmpfs file system
for the in-memory test case log (refer §3.3) and the snapshot()
(§3.1) system call experiments. Later, we show the impact of the
physical medium on AFL (§3.2).
Session K2:  Fuzzing Finer and FasterCCS’17, October 30-November 3, 2017, Dallas, TX, USA2324(a) Fuzzing executions
AFL w/
log
AFL w/o log
1
15
30
45
60
75
90
105
120
(b) Syncing phase execution time
AFL w/
log
AFL w/o log
60k
50k
40k
30k
20k
10k
100
80
60
40
20
c
e
s
/
s
c
e
x
E
t
n
e
p
s
e
m
i
t
%
1
15
30
45
60
#core
75
90
105
120
Figure 7: Evaluation of the shared in-memory test case log in terms
of the number of executions and the time spent while fuzzing the
libpng library. While Figure 7(a) shows the number of new execu-
tions, Figure 7(b) shows the percentage of time spent in the syncing
phase.
6.3.1
Shared In-memory Test Case Log. We run stock—AFL with
and without the shared in-memory test case log applied on the
libpng library, and Figure 7 presents the experimental results. In
terms of the new fuzzing executions that exclude the re-executions
at the syncing stage, the in-memory shared logging speeds up
AFL at roughly 13× at 120 cores. We can observe that the overall
time spent on syncing by AFL linearly increases and around 90% of
the total fuzzing time is used for syncing at the worst case. Note
that the syncing stage does not contribute anything directly to the
whole progress of the exploration of the target program. Without
modifying the working mechanism AFL follows at the syncing stage,
our new primitive successfully removes the performance bottleneck
and the percentage of time spent at the syncing stage drops to at
most 8.05%, which is totally acceptable.
6.3.2
Snapshotting. Even though the in-memory queue removes
the overhead from the syncing phase and brings it down to 8.1%,
we observe that the scalability of AFL is still saturated after 45 cores
(Figure 7). The primary reason for such saturation is the fork()
system call. Figure 8(a) shows the impact of replacing the fork()
with the snapshot() system, which improves the scalability of the
libpng fuzzing by 12.6× and now fuzzing is bottlenecked by the file
operations (i.e.open()/close()). To further validate the necessity
of our snapshot() system call, we create a micro benchmark to
stress test the existing process creation APIs such as fork() and
pthread_create(). The micro benchmark first spawns a per-core
process that individually creates processes or threads using the
aforementioned APIs, including the snapshot() system call, and
then terminates immediately. Figure 8(b) presents the results for
the process creation along with the number of fuzzing executions
of the libpng library, which clearly shows that both fork() and
pthread_create() do not scale beyond 15 cores and suffer from
scalability collapse after 45 cores. On the other hand, snapshot()
system call scales almost linearly with increasing core count and
outperforms both of the process spawning APIs by 3004.5×. More-
over, snapshot() is considered a generic system call for fuzzers. The
c
e
s
/
s
c
e
x
E
c
e
s
/
s
c
e
x
E
800k
700k
600k
500k
400k
300k
200k
100k
0k
10000k
1000k
100k
10k
1k
(a) Fuzzing executions
AFL w snapshot()
AFL w/o snapshot()
1
15
30
45
60
75
90
105
120
(b) Process spawns and fuzzing executions
snapshot()
fork()
1
15
30
45
75
90
105
120
60
#core