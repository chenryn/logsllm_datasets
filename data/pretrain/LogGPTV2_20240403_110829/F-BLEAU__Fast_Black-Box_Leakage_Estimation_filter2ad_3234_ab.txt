a channel consists in estimating the probabilities Cs,o by count-
ing their frequency in the training data (o1, s1), ..., (on, sn):
∗
We can obtain the frequentist error from Equation 3:
|i : oi = o, si = s|
|i : si = s|
.
Cf Freq (o),o π(f Freq (o))
(4)
(5)
P (o|s) ≈ ˆCs,o :=
(cid:2)
RFreq = 1 −
(cid:3)
o
where f Freq is the frequentist classiﬁer, namely:
f Freq (o) =
argmaxs( ˆCs,o ˆπ(s))
argmaxs ˆπ(s)
if o in training data
otherwise ,
(6)
where ˆπ is estimated from the examples: ˆπ(s) = |i:si=s|/n.
Consider a ﬁnite example space S × O. Provided with
enough examples, the frequentist approach always converges:
clearly, ˆC → C as n → ∞, because events’ frequencies
converge to their probabilities by the Law of Large Numbers.
However, there is a fundamental issue with this approach.
Given a training set {(o1, s1), ..., (on, sn)}, the frequentist
classiﬁer can tell something meaningful (i.e., better than
random guessing) for an object o ∈ O, only as long as o
appeared in the training set; but, for very large systems (e.g.,
those with a large object space), the probability of observing
an example for each object becomes small, and the frequentist
classiﬁer approaches random guessing. We study this matter
further in subsection VI-D and Appendix E.
IV. NO FREE LUNCH IN LEARNING
The frequentist approach performs well only for objects it
has seen in the training data; in the next section, we will
introduce estimators that aim to provide good predictions even
for unseen objects. However, we shall ﬁrst answer an important
question: is there an estimator that is “optimal” for all systems?
A negative answer to this question is given by the so-
called “No Free Lunch” (NFL) theorem by Wolpert [10]. The
theorem is formulated for the expected loss of a learning rule
on unseen objects (i.e., that were not in the training data),
which is referred to as the off-training-set (OTS) loss.
Theorem 1 (No Free Lunch). Let A1 and A2 be two learning
rules, (cid:3) a cost function, and μ a distribution on S × O. We
indicate by Ei((cid:3) | μ, n) the OTS loss of Ai given μ and
n, where the expectation is computed over all the possible
training sets of size n sampled from μ. Then, if we take the
uniform average among all possible distributions μ, we have
(7)
E1((cid:3) | μ, n) = E2((cid:3) | μ, n) .
Intuitively, the NFL theorem says that, if all distributions
(and, therefore, all channel matrices) are equally likely, then
all learning algorithms are equivalent. Remarkably, this holds
for any strategy, even if one of the rules is random guessing.
An important implication of this for our purposes is that
for every two learning rules A and B there will always exist
some system for which rule A converges faster than B, and
vice versa there will be a system for which B outperforms A.
From the practical perspective of black-box security, this
demonstrates that we should always test several estimators and
select the one that converges faster. Fortunately, the connection
between ML and black-box security we highlight in this paper
results in the discovery of a whole class of new estimators.
V. MACHINE LEARNING ESTIMATES OF THE BAYES RISK
In this section, we deﬁne the notion of a universally
consistent learning rule, and show that the error of a classiﬁer
selected according to such a rule can be used for estimating the
Bayes risk. Then, we introduce various universally consistent
rules based on the nearest neighbor principle.
Throughout the section, we use interchangeably a system
(π,Cs,o) and its corresponding joint distribution μ on S × O.
Note that there is a one-to-one correspondence between them.
A. Universally Consistent Rules
Consider a distribution μ and a learning rule A selecting a
classiﬁer fn ∈ F according to n training examples sampled
from μ. Intuitively, as the available training data increases, we
would like the expected error of fn for a new example (o, s)
sampled from μ to be minimized (i.e., to get close the Bayes
risk). The following deﬁnition captures this intuition.
Deﬁnition 1 (Consistent Learning Rule). Let μ be a distribu-
tion on S × O and let A be a learning rule. Let fn ∈ F be
a classiﬁer selected by A using n training examples sampled
from μ. Let (π,Cs,o) be the system corresponding to μ, and
let Rfn be the expected error of fn, as deﬁned by (3). We say
that A is consistent if Rfn → R
as n → ∞.
∗
The next deﬁnition strengthens this property, by asking the
rule to be consistent for all distributions:
Deﬁnition 2 (Universally Consistent (UC) Learning Rule). A
learning rule is universally consistent if it is consistent for any
distribution μ on S × O.
By this deﬁnition, the expected error of a classiﬁer selected
according to a universally consistent rule is also an estimator
of the Bayes risk, since it converges to R
as n → ∞.
In the rest of this section we introduce Bayes risk estimates
based on universally consistent nearest neighbor rules; they
are summarized in Table III together with their guarantees.
B. NN estimate
∗
The Nearest Neighbor (NN) is one of the simplest ML
classiﬁers: given a training set and a new object o, it predicts
the secret of its closest training observation (nearest neighbor).
It is deﬁned both for ﬁnite and inﬁnite object spaces, although
it is UC only in the ﬁrst case.
(cid:25)(cid:20)(cid:25)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:33 UTC from IEEE Xplore.  Restrictions apply. 
ESTIMATES’ GUARANTEES AS n → ∞
TABLE III
Method
frequentist
NN
kn-NN
NN Bound+
Guarantee
→ R∗
→ R∗
→ R∗
≤ R∗
Space O
ﬁnite
ﬁnite
inﬁnite, (d, O) separable
inﬁnite, (d, O) separable
+NN Bound is discussed in Appendix A.
where
We introduce a formulation of NN, which can be seen as an
extension of the frequentist approach, that takes into account
ties (i.e., neighbors that are equally close to the new object o),
and which guarantees consistency when O is ﬁnite.
Consider a training set {(o1, s1), ..., (on, sn)}, an object o,
and a distance metric d : O × O (cid:3)→ R≥0. The NN classiﬁer
predicts a secret for o by taking a majority vote over the set of
secrets whose objects have the smallest distance to o. Formally,
let Imin (o) = {i | d(o, oi) = minj=1...n d(o, oj)} and deﬁne:
(8)
NN (o) = sh(o)
where
|{j ∈ Imin (o) | sj = si}| .
h(o) = argmax
i∈Imin (o)
(9)
We show that NN is universally consistent for ﬁnite S× O.
Theorem 2 (Universal consistency of NN). Consider a dis-
tribution on S× O, where S and O are ﬁnite. Let RNN
be the
expected error of the NN classiﬁer for a new observation o.
As the number of training examples n → ∞:
n
→ R
∗
.
RNN
n
(10)
Sketch proof. For an observation o that appears in the training
set, the NN classiﬁer is equivalent to the frequentist approach.
For a ﬁnite space S × O, as n → ∞, the probability that
the training set contains all o ∈ O approaches 1. Thus, the
NN rule is asymptotically (in n) equivalent to the frequentist
approach, which means its error also converges to R
∗
.
C. kn-NN estimate
Whilst NN guarantees universal consistency in ﬁnite exam-
ple spaces, this does not hold for inﬁnite O. In this case, we
can achieve universal consistency with the k-NN classiﬁer, an
extension of NN, for appropriate choices of the parameter k.
The k-NN classiﬁer takes a majority vote among the secrets
of its neighbors. Breaking ties in the k-NN deﬁnition requires
more care than with NN. In the literature, this is generally
done via strategies that add randomness or arbitrariness to the
choice (e.g., if two neighbors have the same distance, select
the one with the smallest index in the training data) [23]. We
use a novel tie-breaking strategy, which takes into account
ties, but gives more importance to the closest neighbors. In
early experiments, we observed this strategy had a faster
convergence than standard approaches.
Consider a training set {(o1, s1), ..., (on, sn)}, an object to
predict o, and some metric d : O× O (cid:3)→ R≥0. Let o(i) denote
the i-th closest object to o, and s(i) its respective secret. If
ties do not occur after the k-th neighbor (i.e., if d(o, o(k)) (cid:5)=
d(o, o(k+1))), then k-NN outputs the most frequent among the
secrets of the ﬁrst k neighbors:
k-NN (o) = sh(o)
|{j ∈ Imin (o) | s(j) = s(i)}| .
(11)
(12)
h(o) = argmax
i=1,...,k
If ties exist after the k-th neighbor, that is, for k
(cid:4) ≤ k < k
(cid:4)(cid:4)
:
(cid:4)
(cid:5)
d(o, o(k(cid:2))) = ... = d(o, o(k)) = ... = d(o, o(k(cid:2)(cid:2))) ,
(13)
we proceed as follows. Let ˆs be the most frequent secret in
; k-NN predicts the most frequent secret in
s(k(cid:2)), ..., s(k(cid:2)(cid:2))
the following multiset, truncated at the tail to have size k:
s(1), s(2), ..., s(k(cid:2)−1), ˆs, ˆs..., ˆs .
We now deﬁne kn-NN, a universally consistent learning rule
that selects a k-NN classiﬁer for a training set of n examples
by choosing k as a function of n.
Deﬁnition 3 (kn-NN rule). Given a training set of n examples,
the kn-NN rule selects a k-NN classiﬁer, where k is chosen
such that kn → ∞ and kn/n → 0 as n → ∞.
Stone proved that kn-NN is universally consistent [24]:
Theorem 3 (Universal consistency of the kn-NN rule). Con-
sider a probability distribution μ on the example space S× O,
where μ has a density. Select a distance metric d such that
(d, O) is separable3. Then the expected error of the kn-NN
rule converges to R
as n → ∞.
∗
This holds for any distance metric. In our experiments, we
will use the Euclidean distance, and we will evaluate kn-NN
rules for kn = log n (natural logarithm) and kn = log10 n.
The ML literature is rich of UC rules and other useful tools
for black-box security; we list some of them in Appendix A.
VI. EVALUATION ON SYNTHETIC DATA
In this section we evaluate our estimates on several synthetic
systems for which the channel matrix is known. For each
system, we sample n examples from its distribution, and
then compute the estimate on the whole object space as in
Equation 3; this is possible because O is ﬁnite. Since for
synthetic data we know the real Bayes risk, we can measure
how many examples are required for the convergence of each
∗
estimate. We do this as follows: let Rf
,
trained on a dataset of n examples. We say the estimate δ-
converged to R
after n examples if its relative change from
R
n be an estimate of R
∗
∗
is smaller than δ: (cid:6)(cid:6)Rf
< δ .
(14)
∗(cid:6)(cid:6)
− R
R∗
n
3A separable space is a space containing a countable dense subset; e.g.,
ﬁnite spaces and the space of q-dimensional vectors Rq with Euclidean metric.
(cid:25)(cid:20)(cid:26)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:33 UTC from IEEE Xplore.  Restrictions apply. 
Privacy parameter
TABLE IV
1.0
0.1
0.01
0.2
0.02
0.002
SYNTHETIC SYSTEMS.
|S|
100
100
100
100
100
100
100K
100K
100
100
100
2
100
2
2
1.0
0.1
0.01
N/A
N/A
|O|
10K
10K
10K
1K
10K
100K
100K
10K
10K
10K
10K
10K
100
R∗
∼ 0
0.007
0.600
0.364
0.364
0.364
0.238
0.924
0.450
0.456
0.797
0
0.979
Name
Geometric
Geometric
Geometric
Geometric
Geometric
Geometric
Geometric
Geometric
Multimodal
Multimodal
Multimodal
Spiky
Random
While relative change has the advantage of taking into account
the magnitude of the compared values, it is not deﬁned when
∗ ≈ 0 (Table IV), we
the denominator is 0; therefore, when R
verify convergence with the absolute change:
(cid:6)(cid:6)Rf
n
∗(cid:6)(cid:6) < δ .
− R
(15)
The systems used in our experiments are brieﬂy discussed
in this section, and summarized in Table IV; we detail them
in Appendix B. A uniform prior is assumed in all cases.
A. Geometric systems
We ﬁrst consider systems generated by adding geometric
noise to the secret, one of the typical mechanisms used to
implement differential privacy [25]. Their channel matrix has
the following form:
Cs,o = P (o | s) = λ exp (−ν| g(s) − o |) ,
(16)
where ν is a privacy parameter, λ a normalization factor, and
g a function S (cid:3)→ O; a detailed description of these systems
is given in in Appendix B.
We consider the following three parameters:
• the privacy parameter ν,
• the ratio |O|/|S|, and
• the size of the secret space |S|.
We vary each of these parameters one at a time, to isolate
their effect on the convergence rate.
1) Variation of the privacy parameter ν: We ﬁx |S| = 100,
|O| = 10K, and we consider three cases ν = 1.0, ν = 0.1 and
ν = 0.01. The results for the estimation of the Bayes risk and
the convergence rate are illustrated in Figure 1 and Table V
respectively. In the table, results are reported for convergence
level δ ∈ {0.1, 0.05, 0.01, 0.005}; an “X” means a particular
estimate did not converge within 500K examples; a missing
row for a certain δ means no estimate converged.
The results indicate that
the nearest neighbor methods
have a much faster convergence than the standard frequentist
approach, particularly when dealing with large systems. The
reason is that geometric systems have a regular behavior with
respect to the Euclidean metric, which can be exploited by
NN and kn-NN to make good predictions for unseen objects.
CONVERGENCE OF THE ESTIMATES WHEN VARYING ν, FIXED
|S| × |O| = 100 × 10K
TABLE V
System
Geometric
ν = 1.0
Geometric
ν = 0.1
Geometric
ν = 0.01
δ
0.1
0.05
0.01
0.005
0.1
0.05
0.01
0.005
0.1
0.05
Freq.
1 994
4 216
19 828