Defense Requirements - Pulling from this formalization
of DNN robustness. we now outline design requirements for
defenses against adversarial perturbations:
• Low impact on the architecture: techniques introducing
limited, modiﬁcations to the architecture are preferred in
our approach because introducing new architectures not
studied in the literature requires analysis of their behav-
iors. Designing new architectures and benchmarking them
against our approach is left as future work.
• Maintain accuracy: defenses against adversarial samples
should not decrease the DNN’s classiﬁcation accuracy.
This discards solutions based on weight decay, through
L1, L2 regularization, as they will cause underﬁtting.
• Maintain speed of network: the solutions should not
signiﬁcantly impact the running time of the classiﬁer at
test time. Indeed, running time at test time matters for the
usability of DNNs, whereas an impact on training time is
somewhat more acceptable because it can be viewed as a
ﬁxed cost. Impact on training should nevertheless remain
limited to ensure DNNs can still take advantage of large
training datasets to achieve good accuracies. For instance,
solutions based on Jacobian regularization, like double
backpropagation [31], or using radial based activation
functions [9] degrade DNN training performance.
• Defenses should work for adversarial samples relatively
close to points in the training dataset [9], [7]. Indeed,
samples that are very far away from the training dataset,
like those produced in [32], are irrelevant to security
because they can easily be detected, at least by humans.
However, limiting sensitivity to inﬁnitesimal perturbation
(e.g., using double backpropagation [31]) only provides
constraints very near training examples, so it does not
solve the adversarial perturbation problem. It is also very
hard or expensive to make derivatives smaller to limit
sensitivity to inﬁnitesimal perturbations.
We show in our approach description below that our defense
technique does not require any modiﬁcation of the neural
network architecture and that it has a low overhead on training
and no overhead on test time. In the evaluation conducted in
section V, we also show that our defense technique ﬁts the
remaining defense requirements by evaluating the accuracy of
DNNs with and without our defense deployed, and studying
the generalization capabilities of networks to show how the
defense impacted adversarial samples.
587587
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:13:13 UTC from IEEE Xplore.  Restrictions apply. 
3
2
1
0.02
0.92
0.04
0.02
Probability Vector Predictions F(X)
DNN F trained at temperature T
      Training Data X
    Training Labels Y
0
1
0
0
0.02
0.92
0.04
0.02
Class
Probabilities
Knowledge
0.03
0.93
0.01
0.03
5
Probability Vector Predictions      .
F
d
(X)
DNN           trained at temperature T
(X)
F
d
      Training Data X
0.02
  Training Labels F(X)
0.92
0.04
0.02
4
Initial Network
Distilled Network
Fig. 5: An overview of our defense mechanism based on a transfer of knowledge contained in probability vectors through
distillation: We ﬁrst train an initial network F on data X with a softmax temperature of T . We then use the probability vector
F (X), which includes additional knowledge about classes compared to a class label, predicted by network F to train a distilled
network F d at temperature T on the same data X.
B. Distillation as a Defense
We now introduce defensive distillation, which is the tech-
nique we propose as a defense for DNNs used in adversarial
settings, when adversarial samples cannot be permitted. De-
fensive distillation is adapted from the distillation procedure,
presented in section II, to suit our goal of improving DNN
classiﬁcation resilience in the face of adversarial perturbations.
Our intuition is that knowledge extracted by distillation, in
the form of probability vectors, and transferred in smaller
networks to maintain accuracies comparable with those of
larger networks can also be beneﬁcial to improving gener-
alization capabilities of DNNs outside of their training dataset
and therefore enhances their resilience to perturbations. Note
that throughout the remainder of this paper, we assume that
considered DNNs are used for classiﬁcation tasks and designed
with a softmax layer as their output layer.
The main difference between defensive distillation and the
original distillation proposed by Hinton et al. [19] is that we
keep the same network architecture to train both the original
network as well as the distilled network. This difference is
justiﬁed by our end which is resilience instead of compres-
sion. The resulting defensive distillation training procedure is
illustrated in Figure 5 and outlined as follows:
1) The input of the defensive distillation training algorithm
is a set X of samples with their class labels. Speciﬁcally,
let X ∈ X be a sample, we use Y (X) to denote its
discrete label, also referred to as hard label. Y (X) is an
indicator vector such that the only non-zero element cor-
responds to the correct class’ index (e.g. (0, 0, 1, 0, . . . , 0)
indicates that the sample is in the class with index 2).
2) Given this training set {(X, Y (X)) : X ∈ X}, we
train a deep neural network F with a softmax output
layer at temperature T . As we discussed before, F (X)
is a probability vector over the class of all possible
labels. More precisely, if the model F has parameters
θF , then its output on X is a probability distribution
F (X) = p(·|X, θF ), where for any label Y in the label
class, p(Y |X, θF ) gives a probability that the label is Y .
To simplify our notation later, we use Fi(X) to denote
the probability of input X to be in class i ∈ 0..N − 1
according to model F with parameters θF .
3) We form a new training set, by consider samples of the
form (X, F (X)) for X ∈ X . That is, instead of using
hard class label Y (X) for X, we use the soft-target F (X)
encoding F ’s belief probabilities over the label class.
4) Using the new training set {(X, F (X)) : X ∈ X} we
then train another DNN model F d, with the same neural
network architecture as F , and the temperature of the
softmax layer remains T . This new model is denoted as
F d and referred to as the distilled model.
Again, the beneﬁt of using soft-targets F (X) as training
labels lies in the additional knowledge found in probability
vectors compared to hard class labels. This additional entropy
encodes the relative differences between classes. For instance,
in the context of digit recognition developed later in section V,
given an image X of some handwritten digit, model F may
evaluate the probability of class 7 to F7(X) = 0.6 and the
probability of label 1 to F1(X) = 0.4, which then indicates
some structural similarity between 7s and 1s.
Training a network with this explicit relative information
about classes prevents models from ﬁtting too tightly to the
data, and contributes to a better generalization around training
points. Note that the knowledge extraction performed by dis-
tillation is controlled by a parameter: the softmax temperature
T . As described in section II, high temperatures force DNNs to
produce probabilities vectors with large values for each class.
In sections IV and V, we make this intuition more precise
with a theoretical analysis and an empirical evaluation.
588588
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:13:13 UTC from IEEE Xplore.  Restrictions apply. 
IV. ANALYSIS OF DEFENSIVE DISTILLATION
We now explore analytically the impact of defensive distilla-
tion on DNN training and resilience to adversarial samples. As
stated above, our intuition is that probability vectors produced
by model F encode supplementary entropy about classes that
is beneﬁcial during the training of distilled model F d. Before
proceeding further, note that our purpose in this section is
not to provide a deﬁnitive argument about using defensive
distillation to combat adversarial perturbations, but rather we
view it as an initial step towards drawing a connection between
distillation, learning theory, and DNN robustness for future
work to build upon. This analysis of distillation is split in three
folds studying (1) network training, (2) model sensitivity, and
(3) the generalization capabilities of a DNN.
Note that with training, we are looking to converge towards
∗ resilient to adversarial noise and capable of
a function F
∗ is guaranteed
generalizing better. The existence of function F
by the universality theorem for neural networks [33], which
states that with enough neurons and enough training points,
one can approximate any continuous function with arbitrary
precision. In other words, according to this theorem, we know
that there exists a neural network architecture that converges
∗ if it is trained on a sufﬁcient number of samples. With
to F
this result in mind, a natural hypothesis is that distillation helps
∗
convergence of DNN models towards the optimal function F
instead of a different local optimum during training.
A. Impact of Distillation on Network Training
To precisely understand the effect of defensive distillation
on adversarial crafting, we need to analyze more in depth the
training process. Throughout this analysis, we frequently refer
to the training steps for defensive distillation, as described in
Section III. Let us start by considering the training procedure
of the ﬁrst model F , which corresponds to step (2) of defensive
distillation. Given a batch of samples {(X, Y (X)) | X ∈ X}
labeled with their correct classes, training algorithms typically
aim to solve the following optimization problem:
arg min
θF
− 1|X|
(cid:5)
(cid:5)
X∈X
i∈0..N
Yi(X) log Fi(X).
(5)
where θF is the set of parameters of model F and Yi is the
ith component of Y . That is, for each sample (X, Y (X)) and
hypothesis, i.e. a model F with parameters θF , we consider the
log-likelihood (cid:5)(F, X, Y (X)) = −Y (X) · log F (X) of F on
(X, Y (X)) and average it over the entire training set X . Very
roughly speaking, the goal of this optimization is to adjust the
weights of the model so as to push each F (X) towards Y (X).
However, readers will notice that since Y (X) is an indicator
vector of input X’s class, Equation 5 can be simpliﬁed to:
arg min
θF
− 1|X|
(cid:5)
X∈X
a 0 output. However, this forces the DNN to make overly
conﬁdent predictions in the sample class. We argue that this
is a fundamental lack of precision during training as most of
the architecture remains unconstrained as weights are updated.
Let us move on to explain how defensive distillation
solves this issue, and how the distilled model F d is trained.
As mentioned before, while the original training dataset is
{(X, Y (X)) : X ∈ X}, the distilled model F d is trained
using the same set of samples but labeled with soft-targets
{(X, F (X)) : X ∈ X} instead. This set is constructed at step
(3) of defensive distillation. In other words, the label of X
is no longer the indicator vector Y (X) corresponding to the
hard class label of X, but rather the soft label of input X: a
probability vector F (X). Therefore, F d is trained, at step (4),
by solving the following optimization problem:
arg min
θF
− 1|X|
(cid:5)
(cid:5)
X∈X
i∈0..N
Fi(X) log F
d
i (X)
(7)
Note that the key difference here is that because we are using
soft labels F (X), it is not trivial anymore that most compo-
nents of the double sum are null. Instead, using probabilities
Fj(X) ensures that the training algorithm will constrain all
output neurons F d
j (X) proportionally to their likelihood when
updating parameters θF . We argue that this contributes to
improving the generalizability of classiﬁer model F outside
of its training dataset, by avoiding situations where the model
is forced to make an overly conﬁdent prediction in one class
when a sample includes characteristics of two or more classes
(for instance, when classifying digits, an instance of a 8
include shapes also characteristic of a digit 3).
Note that model F d should theoretically eventually converge
towards F . Indeed, locally at each point (X, F (X)), the opti-
mal solution is for model F d to be such that F d(X) = F (X).
To see this, we observe that training aims to minimize the cross
entropy between F d(X) and F (X), which is equal to:
(cid:6)
d
(cid:7)
H
F
(X), F (X)
= H(F (X)) + KL
F (X) (cid:5) F
d
(X)
(8)
(cid:7)
(cid:6)
where H(F (X)) is the Shannon entropy of F (X)) and KL
denotes the Kullback-Leibler divergence. Note that this quan-
tity is minimized when the KL divergence is equal to 0,
which is only true when F d(X) = F (X). Therefore, an ideal
training procedure would result in model F d converging to
the ﬁrst model F . However, empirically this is not the case
because training algorithms approximate the solution of the
training optimization problem, which is often non-linear and
non-convex. Furthermore, training algorithms only have access
to a ﬁnite number of samples. Thus, we do observe empirically
a better behavior in adversarial settings from model F d than
model F . We conﬁrm this result in Section V.
Having studied the impact of defensive distillation on op-
timization problems solved during DNN training, we now
further investigate why adversarial perturbations are harder
to craft on DNNs trained with defensive distillation at high
temperature. The goal of our analysis here is to provide an
intuition of how distillation at high temperatures improves the
log Ft(X)(X).
(6)
B. Impact of Distillation on Model Sensitivity
where t(X) is the only element in indicator vector Y (X)
that is equal to 1, in other words the index of the sample’s
class. This means that when performing updates to θF , the
training algorithm will constrain any output neuron different
from the one corresponding to probability Ft(X)(X) to give
589589
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:13:13 UTC from IEEE Xplore.  Restrictions apply. 
smoothness of the distilled model F d compared to model F ,
thus reducing its sensitivity to small input variations.
The model’s sensitivity to input variation is quantiﬁed by
its Jacobian. We ﬁrst show why the amplitude of Jacobian’s
components naturally decrease as the temperature of the
softmax increases. Let us derive the expression of component
(i, j) of the Jacobian for a model F at temperature T :
(cid:9)
(cid:10)
(cid:3)N−1
ezi(X)/T
l=0 ezl(X)/T
(9)
(cid:8)(cid:8)(cid:8)(cid:8)
∂Fi(X)
∂Xj
∂
∂Xj
=
T
where z0(X), . . . , zN−1(X) are the inputs to the softmax
layer—also referred to as logits—and are simply the outputs of
the last hidden layer of model F . For the sake of notation clar-
ity, we do not write the dependency of z0(X), . . . , zN−1(X)
to X and simply write z0, . . . , zN−1. Let us also write
g(X) =
l=0 ezl(X)/T , we then have:
(cid:3)N−1
(cid:9)