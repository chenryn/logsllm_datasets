0
.
2
5
.
1
0
.
1
5
.
0
0
.
0
Tue 14:00
Tue 18:00
Tue 22:00
Wed 2:00
Wed 6:00 Wed 10:00
local time
Fig. 5. Measured CPU time vs. predicted CPU time with BROBASE
Simple CPU Time Projection. To illustrate how we then project performance, let
us ﬁrst consider a simple case: the BROBASE conﬁguration. As we have seen (§4),
for this conﬁguration resource consumption directly scales with the total number of
connections. In Figure 5 we plot the actual per-second CPU consumption exhibited by
running BROBASE on the complete MWN-full trace (circles) versus the per-second
consumption projected by using connection logs plus an independent 20-minute trace
(crosses). We see that overall the predicted CPU time matches the variations in the
measured CPU time quite closely. The prediction even correctly accounts for many
of the outliers. However, in general the predicted times are somewhat lower than the
measured ones with a mean error of -25 msec of CPU time per second, and a mean
relative error of -9.0%.
CPU Time Projection for Complex Conﬁgurations. Let us now turn to predicting
performance for more complex conﬁgurations. We examine BROALL
, the BROALL
conﬁguration except with ssl deactivated (since the analyzer occasionally crashes the
examined version of Bro in this environment). In this case, we group the connections
into several classes, as discussed above. To avoid introducing high-variance effects from
minimal samples, we discard any connections belonging to a service that comprises less
than 1% of the trafﬁc. (See below for difﬁculties this can introduce.) We then predict
overall CPU time by applying our projection ﬁrst individually to each analyzer and
for each combination of service and connection state, and then summing the predicted
CPU times for the base conﬁguration and the predicted additional CPU times for the
individual analyzers.
−
Figure 6 shows the resulting predicted CPU times (crosses) and measured BROALL
−
CPU times (circles). Note that this conﬁguration is infeasible for a live setting, as the
required CPU regularly exceeds the machine’s processing capacity. We see, however,
that our prediction matches the measurement fairly well. However, we underestimate
some of the outliers with a mean error of -29 msec of CPU time and a mean relative
error of -4.6%. Note that the mean relative error is smaller than for predicting BROBASE
performance since the absolute numbers of the measured samples are larger for the
complex conﬁguration.
Above we discussed how we only extrapolate CPU time for connections that con-
tribute a signiﬁcant portion (> 1%) of the connections in our base measurement. Doing
Predicting the Resource Consumption of NIDSs
151
measured CPU time
predicted CPU time
d
n
o
c
e
s
r
e
p
e
m
i
t
U
P
C
5
.
3
0
.
3
5
2
.
0
.
2
5
.
1
0
.
1
5
.
0
0
.
0
Tue 14:00
Tue 18:00
Tue 22:00
Wed 2:00
Wed 6:00 Wed 10:00
Fig. 6. Measured CPU time vs. predicted CPU time with BROALL
−
local time
so can result in underestimation of CPU time when these connection types become
more prominent. For example, during our experiments we found that SSH and Telnet
connections did not occur frequently in the 20-minute trace on which the systematic
measurements are performed. Yet the long-term connection log contains sudden surges
of these connections (likely due to brute-force login attempts). nidsconf detects such
cases and reports a warning, but at this point it lacks sufﬁcient data to predict the CPU
time usage, since it does not have an adequate sample in the trace from which to work.
Memory Projection. Our approach for predicting memory consumption is to derive
the number of active connections per class at any given time in the connection log, and
then extrapolate from this ﬁgure to the overall memory usage. However, Bro’s resource
proﬁling is not currently capable of reporting precise per-connection memory usage
for application-layer analyzers, so here we limit ourselves to predicting the number of
TCP connections in memory, rather than the actual memory consumption. To do so,
we draw upon the dimensions of connection duration and state. These two interplay
directly since Bro keeps its per connection state for the lifetime of the connection plus
a timeout that depends on the state. To determine the relevant timeout, we use the states
discussed above (attempted, established, etc.), binning connections into time intervals
of length T and then calculating their aggregate memory requirements.
However, a problem with this binning approach arises due to connections with dura-
tions shorter than the bin size (since we use bin sizes on the order of tens of seconds,
this holds for the majority of connections). Within a bin, we cannot tell how many of
these are concurrently active. Therefore, we reﬁne our basic approach, as follows. We
pick a random point in the base trace and compute the average number N of short-lived
connections per second occurring in the trace up to that point. We also measure the
number F of these short-lived connections instantaneously in memory at the arbitrary
point. Let Ni be the number of short-lived connections per second for each bin i in the
connection log. Assuming that F is representative, we can then scale Ni/N by F to
estimate the number of short-lived connections concurrently active in each bin.
Figure 7 shows the results of our prediction for the number of established connec-
tions in memory (crosses) assuming Bro’s default inactivity timeout of 300s, along with
the the actual number of in-memory connections when running on MWN-full (circles).
152
H. Dreger et al.
measured
predicted
y
r
o
m
e
m
n
i
s
n
o
i
t
c
e
n
n
o
c
#
0
0
0
5
1
0
0
0
0
1
0
0
0
5
0
Tue 18:00
Wed 0:00
Wed 6:00
Wed 12:00
local time
Fig. 7. Predicted number of established connections in memory for MWN-full
We observe that the prediction matches the measurements well, with a mean relative er-
ror of +5.0%. While not shown on the plot, we obtain similar prediction results for other
classes of connections, e.g., unanswered connection attempts.
6 Related Work
Numerous studies in the literature investigate IDS detection quality, generally analyzing
the trade-off between false positives and false negatives. Some studies [6,4,5] take steps
towards analyzing how the detection quality and detection coverage depends on the cost
of the IDS conﬁguration and the attacks the network experiences. Gaffney and Ulvila [4]
focus on the costs that result from erroneous detection, developing a model for ﬁnding
a suitable trade-off between false positives and false negatives dependent on the cost of
each type of failure. In contrast, Lee et al. [6,5] focus on developing and implementing
high-level cost models for operating an IDS, enabling dynamic adaptation of a NIDS’s
conﬁguration to suit the current system load. The models take as input both metrics
of the beneﬁts of a successful detection and (self-adapting) metrics reﬂecting the cost
of the detection. Such metrics may be hard to deﬁne for large network environments,
however. To adapt to the cost metrics, they monitor the performance of their prototype
systems (Bro and Snort) using a coarse-grained instrumentation of packet counts per
second. As was shown by Dreger et al. [3], this risks oversimplifying a complex NIDS.
While the basic idea of adapting NIDS conﬁgurations to system load is similar to ours,
we focus on predicting resource usage of the NIDS depending on both the network
trafﬁc and the NIDS conﬁguration.
In the area of general performance prediction and extrapolation of systems (not nec-
essarily NIDSs), three categories of work exam (i) performance on different hardware
platforms, (ii) distribution across multiple systems, and (iii) predicting system load.
These studies relate to ours in the sense that we use similar techniques for program de-
composition and for runtime extrapolation. We omit details of these here due to limited
space, but refer the reader to [2] for a detailed discussion. In contrast to this body of
work, our contributions are to predict performance for soft real-time systems, both at a
ﬁne-grained resolution (prediction of “head room” for avoiding packet drops) and over
Predicting the Resource Consumption of NIDSs
153
long time scales (coupling a short, detailed trace with coarse-grained logs to extrapo-
late performance over hours or days), with an emphasis on memory and CPU trade-offs
available to an operator in terms of depth of analysis versus limited resources.
7 Conclusion
In this work we set out to understand and predict the resource requirements of net-
work intrusion detection systems. When initially installing such a system in a network
environment, the operator often must grapple with a large number of options to tune
trade-offs between detection rate versus CPU and memory consumption. The impact
of such parameters often proves difﬁcult to predict, as it potentially depends to a large
degree on the internals of the NIDS’s implementation, as well as the speciﬁc charac-
teristics of the target environment. Because of this, the installation of a NIDS often
becomes a trial-and-error process that can consume weeks until ﬁnding a “sweet spot.”
We have developed a methodology to automatically derive NIDS conﬁgurations that
maximize the systems’ detection capabilities while keeping the resource load feasi-
ble. Our approach leverages the modularity likely present in a NIDS: while complex
systems, NIDSs tend to be structured as a set of subcomponents that work mostly inde-
pendently in terms of their resource consumption. Therefore, to understand the system
as a whole, we can decompose the NIDS into the main contributing components. As
our analysis of the open-source Bro NIDS shows, the resource requirements of these
subcomponents are often driven by relatively simple characteristics of their input, such
as number of packets or number and types of connections.
Leveraging this observation, we built a tool that derives realistic conﬁgurations for
Bro. Based on a short-term, full-packet trace coupled with a longer-term, ﬂow-level
trace—both recorded in the target environment—the tool ﬁrst models the resource usage
of the individual subcomponents of the NIDS. It then simulates different conﬁgurations
by adding together the contributions of the relevant subcomponents to predict conﬁgu-
rations whose execution will remain within the limits of the resources speciﬁed by the
operator. The operator can then choose among the feasible conﬁgurations according to
the priorities established for the monitoring environment. While no automatically gen-
erated conﬁguration can be optimal, these provide a sound starting point, with promise
to signiﬁcantly reduce the traditional trial-and-error NIDS installation cycle.
Acknowledgments
We would like to thank Christian Kreibich for his feedback and the fruitful discussions
that greatly helped to improve this work. We would also like to thank the Leibnitz-
Rechenzentrum M¨unchen. This work was supported by a grant from the Bavaria Cal-
ifornia Technology Center, and by the US National Science Foundation under awards
STI-0334088, NSF-0433702, and ITR/ANI-0205519, for which we are grateful. Any
opinions, ﬁndings, conclusions or recommendations expressed in this material are those
of the authors or originators and do not necessarily reﬂect the views of the National Sci-
ence Foundation.
154
H. Dreger et al.
References
1. Dharmapurikar, S., Paxson, V.: Robust TCP Stream Reassembly In the Presence of Adver-
saries. In: Proc. USENIX Security Symposium (2005)
2. Dreger, H.: Operational Network Intrusion Detection: Resource-Analysis Tradeoffs.
PhD thesis, TU M¨unchen (2007), http://www.net.in.tum.de/∼hdreger/
papers/thesis dreger.pdf
3. Dreger, H., Feldmann, A., Paxson, V., Sommer, R.: Operational Experiences with High-
Volume Network Intrusion Detection. In: Proc. ACM Conference on Computer and Com-
munications Security (2004)
4. Gaffney Jr., J.E., Ulvila, J.W.: Evaluation of Intrusion Detectors: A Decision Theory Ap-
proach. In: Proc. IEEE Symposium on Security and Privacy (2001)
5. Lee, W., Cabrera, J.B., Thomas, A., Balwalli, N., Saluja, S., Zhang, Y.: Performance Adap-
tation in Real-Time Intrusion Detection Systems. In: Proc. Symposium on Recent Advances
in Intrusion Detection (2002)
6. Lee, W., Fan, W., Miller, M., Stolfo, S.J., Zadok, E.: Toward Cost-sensitive Modeling for
Intrusion Detection and Response. Journal of Computer Security 10(1-2), 5–22 (2002)
7. Paxson, V.: Bro: A System for Detecting Network Intruders in Real-Time. Computer Net-
works 31(23–24), 2435–2463 (1999)
8. Roesch, M.: Snort: Lightweight Intrusion Detection for Networks. In: Proc. Systems Admin-
istration Conference (1999)
9. Schneider, F., Wallerich, J., Feldmann, A.: Packet Capture in 10-Gigabit Ethernet Environ-
ments Using Contemporary Commodity Hardware. In: Proc. Passive and Active Measure-
ment Conference (2007)
10. tcp-reduce, http://ita.ee.lbl.gov/html/contrib/tcp-reduce.html
11. Vallentin, M., Sommer, R., Lee, J., Leres, C., Paxson, V., Tierney, B.: The NIDS Cluster:
Scalable, Stateful Network Intrusion Detection on Commodity Hardware. In: Proc. Sympo-
sium on Recent Advances in Intrusion Detection (2007)
12. Willinger, W., Taqqu, M.S., Sherman, R., Wilson, D.V.: Self-Similarity Through High-
Variability: Statistical Analysis of Ethernet LAN Trafﬁc at the Source Level. IEEE/ACM
Transactions on Networking 5(1) (1997)