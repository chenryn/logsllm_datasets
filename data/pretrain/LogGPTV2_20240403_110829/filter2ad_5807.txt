title:Poster: Adversarial Examples for Classifiers in High-Dimensional Network
Data
author:Muhammad Ejaz Ahmed and
Hyoungshick Kim
Poster: Adversarial Examples for Classifiers in
High-Dimensional Network Data
Muhammad Ejaz Ahmed
Sungkyunkwan University, Suwon
Republic of Korea
PI:EMAIL
Hyoungshick Kim
Sungkyunkwan University, Suwon
Republic of Korea
PI:EMAIL
ABSTRACT
Many machine learning methods make assumptions about the data,
such as, data stationarity and data independence, for an efficient
learning process that requires less data. However, these assumptions
may give rise to vulnerabilities if violated by smart adversaries. In
this paper, we propose a novel algorithm to craft the input samples
by modifying a certain fraction of input features as small as in order
to bypass the decision boundary of widely used binary classifiers
using Support Vector Machine (SVM). We show that our algorithm
can reliably produce adversarial samples which are misclassified
with 98% success rate while modifying 22% of the input features
on average. Our goal is to evaluate the robustness of classification
algorithms for high demensional network data by intentionally
performing evasion attacks with carefully designed adversarial
examples. The proposed algorithm is evaluated using real network
traffic datasets (CAIDA 2007 and CAIDA 2016).
KEYWORDS
Network attacks; Adversarial Machine Learning; Network Intru-
sion; Evasion Attacks; High Dimensional Data.
1 INTRODUCTION
Machine learning is now entrenching in modern technology, al-
lowing an overwhelming number of tasks to be performed auto-
matically at lower costs. Giant tech companies like Google, Mi-
crosoft, and Amazon have already taken initiatives to provide their
customers with APIs, allowing them to easily apply artificial intel-
ligence techniques into their applications. For instance, machine
learning (ML) applications includes image classification to scruti-
nize online contents, automatic caption generation for an image,
spam email detection, and many more [4].
However, there have been some limitations of ML algorithms
due to which an adversary able to craft inputs would profit from
evading detection. An adversarial sample is an input crafted such
that it causes an ML algorithm to misclassify it [5]. Note that ad-
versarial samples are created at test time, i.e., the ML algorithm is
already trained and deployed by the defender, and do not require
any alteration of the training process. Many ML methods require
the data stationarity assumption where training and testing data
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
CCS ’17, October 30-November 3, 2017, Dallas, TX, USA
© 2017 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-4946-8/17/10.
https://doi.org/10.1145/3133956.3138853
are drawn from the same distribution (even when it is unknown).
However, and attackers can often break this assumption by manip-
ulating the input to the detector to get evasion because real-world
data sources are not stationary. Moreover, another common assump-
tion is that each datapoint is independent and identically distributed
(i.i.d.); clearly this assumption can also be compromised by crafting
the input with a little effort.
Figure 1: Network traffic features with classification (deci-
sion) boundary for a ML algorithm (SVM).
Since the real-world data sources do not follow the stationarity
and i.i.d. assumptions, the ML models trained on such data sources
could be vulnerable to be exploited by the attackers [1, 4, 6]. Fig. 1
shows real-world network traffic data (for two features x1 and
x2) with the decision boundary of a binary classifier (SVM). Note
that both features (x1 and x2) are linearly correlated for the class +
(malicious traffic), where the correlation in the second feature (x2)
for class * (legitimate traffic) is extremely low (almost zero).
Under the uneven separation of both classes from the decision
boundary of the classifier, an attacker can evade the detection
system by slightly modifying a malicious datapoint (even a single
feature) at the lower end of the decision boundary. For example,
if a malicious datapoint is modified at the lower axis of x2 by the
attacker (by updating x1 ≥ 100), it can evade the decision boundary
and will be labeled as legitimate by the classifier.
In this paper, we propose a novel algorithm for adversarial sam-
ple creation against a popular ML algorithm, i.e., SVM. Furthermore,
our approach alters a small fraction of input features leading to
significantly reduced distortion of the source inputs. Our algorithm
relies on heuristic searches to find minimum distortions leading
to the misclassification for target inputs. We show that the input
samples can be perturbed such that it is misclassified as a legitimate
class with 98% success while perturbing 22% of the input features
x101,002,003,004,005,006,007,008,00x20102030400 (training)0 (testing)1 (training)1 (testing)Support VectorsDecision boundaryPosterCCS’17, October 30-November 3, 2017, Dallas, TX, USA2467on average. We performed extensive simulations on real-world
network traffic with widely used 17 traffic features. In Section 2,
we briefly describe the proposed approach for modifying the input
features to bypass the classifier. Section 3 discusses the data used
in our approach and the experiment results.
2 PROPOSED APPROACH
In this section we describe the proposed approach for identifying
the features to be modified so that the classifier yields the adversar-
ial output. We validate the proposed algorithm by using real traffic
datasets.
2.1 Threat model
In our approach we assume that adversary can obtain query re-
sponses from the classifier. In case of intrusion detection system,
the packets from malicious users are dropped by the intrusion detec-
tion system (IDS). Therefore, an adversary is aware of the dropped
packets, thus features of these packets are recorded as training data.
We further assume that adversary can observe legitimate packets
by sniffing the network and therefore record the feature values of
legitimate traffic [4]. In this way, dataset comprise of legitimate
and malicious packets can be obtained by the adversary. Given a
training set of m measurements consisting each of n features, our
feature space becomes X ∈ Rm×n. Note that our goal is to intro-
duce minimum perturbation to a malicious datapoint so as to evade
detection system.
2.2 Feature selection and modification
It has been shown that by using reduced set of features may re-
quire attackers to manipulate less features to evade the detection
system [3]. The underlying idea of our approach is to select the fea-
tures having most discriminative power for classification. Different
methods in literature are proposed to obtain most distinguishing
features such as information gains (IG), Chi Squared statistics, en-
tropy, principal component analysis (PCA), mean and standard
deviation of each dimension, skewness/kurtosis, cross correlation
between dimensions, and ARIMA models, etc. In this paper, we
restrict ourself to use IG to select the top K features having highest
IG. After selecting-K top features, we take combination of the top
K features to calculate two important metrics: distance and cross
correlation.
In literature the distance between legitimate and malicious data-
points are used to quantify the differential between them [4]. How-
ever, in the proposed approach, in addition to calculate the distance
(Euclidean) between each datapoint from legitimate and malicious
traffic sources, we also compute the cross correlation γ(x1, x2) be-
tween them. The Euclidean distance is the distance between two
datapoints (one from each class, i.e., malicious and legitimate). Fig. 2
(left) show the Euclidean distance between each datapoint from
legitimate and malicious datapoints for traffic features (total bytes
received and total number of packets). Fig. 2 (right) show the the
SVM classifier’s decision boundary. Note that due to the non i.i.d.
nature of real-world traffic sources, the adversary can craft (modify)
datapoints (features) in the vulnerable area (shown in red-dashed
circle) to misclassify the malicious datapoint as a legitimate. Also
Figure 2: Euclidean distance between legitimate and mali-
cious datapoints of Fig. 1, and classifier’s uneven separation
yields a vulnerable area which is exploited by the adversary.
note that if only one feature out of the two features is modified, the
adversary can bypass the SVM’s decision boundary.
Algorithm 1 Crafting adversarial sample.
{xi}m
sponding {yi}m
class.
i =1 is the training data, each input feature vector has corre-
i =1 labels, y∗ is the target network output (legitimate)
Input: D = {xi , yi}m
Result: x∗ an adversarial datapoint.
i =1, Y∗.
data.*/
data.*/
1: K ← select top-K feature based on the IG({xi , yi}m
i =1)
2: Xmal={x}y=0 /* all malicious datapoints from the training
3: Xleg={x}y=1 /* all legitimate datapoints from the training
4: for all c ∈ Comb(K) do
5:
6:
(x(c,1), x(c,2)) is a feature-pair relative to cth combination.
leg}n/2
i =1)
Ed(x(c,1), x(c,2)) ← Min(EuclideanDistance{xi
Rmal(x(c,1), x(c,2))= Avg
.
Rleg(x(c,1), x(c,2))= Avg
Xmal(x(c,1), x(c,2))(cid:17)(cid:17)
(cid:16)
Xleg(x(c,1), x(c,2))(cid:17)(cid:17)
(cid:16)
mal,xi
(cid:16)
(cid:16)
xcorr
xcorr
7:
.
8:
9: (x1,x2) ← select
the combination having maximum
Rmal(x(c,1), x(c,2)) and Rleg(x(c,1), x(c,2)), and minimum
Ed(x(c,1), x(c,2)).
10: while not successfully misclassified do
11:
x ← take a malicious datapoint having minimum Ed(x1, x2)
x∗ ← replace the value of either feature (x1 or x2) with the
12:
from Xmal.
Avд(Xleg).
Algorithm 1 enumerates the steps involved in modifying a ma-
licious datapoint to bypass the detection system. In the first step,
information gain (IG) for every feature is calculated and top-K fea-
tures are selected. Malicious and legitimate traffic are represented
by Xmal and Xleg, respectively, (line 2-3). The combinations of the
selected top-K features are computed, and for each feature-pair
combination, the Euclidean distance between datapoints from mal-
cious and legitimate traffic is computed, and a minimum of them
2
Number of datapoints050100150200Euclidean distance050k100k150k200kfeature (x1)0500010000feature (x2)020406080Distance betweenlegitimate andmalicious data pointsSVM decisionboundaryVulnerable areadue to non i.i.d.data sourcesPosterCCS’17, October 30-November 3, 2017, Dallas, TX, USA2468Table 1: Input features for a ML detector.
Table 2: Results of the proposed approach.
Feature
#pkts
#bytes
pkts (src -> dst)
bytes (src -> dst)
pkts (dst -> src)
bytes (dst -> src)
Rel. Start
dur.
bits (src -> dst)
bits (dst -> src)
IP-H-len
pkt-len
ttl-avg
ttl-std
inter-arr
proto-type
Description
number of packets
number of bytes
packets from source to destination
bytes from sources to destination
packets from destination to source
bytes from destination to source
Relative start time of a connection
duration of a connection
bits from source to destination
bits from destination to source
IP header length
packet length
average time to live (TTL)
TTL standard deviation
interarrival time
protocol type
is taken (line 6). We do so because we want to figure-out which
features could minimally be perturbed to evade the detector. Then,
cross-correlation is calculated between the selected features of ma-
licious and legitimate traffic (line 7-8). The reason for this is to find
features which could be exploited based on the i.i.d. assumption
made by most ML classifiers. For example feature-pairs having
higher cross-correlation will bias the decision boundary of SVM
as shown in Fig. 2 (right), consequently an adversary can exploit
lower regions of the feature-pairs to bypass the classifier’s decision
boundary. In the next step, from all the feature-pair combinations
with respective minimum Euclidean distance and cross-correlations,
we select feature-pairs having minimum Euclidean distance and
maximum cross-correlations (line 9). It is to be noted that we used
heuristics to obtain the maximum (cross-correlation) and minimum
(Euclidean distance) since the problem under consideration is non-
convex multi-variable optimization problem. The selected features
(from line 9) are modified by our proposed approach so as to mis-
classify them. For that, a malicious datapoint having minimum Ed
is selected for modification, and either feature (as obtained from
line 9) is modified and replaced with the average of the same feature
of legitimate data traffic. The modified feature is then tested if it is
evades the detector.
3 DATA DESCRIPTION AND RESULTS
We evaluate the performance of the proposed approach by using real
network traffic measurements from CAIDA2007 and CAIDA2016
traces. We extracted 16 features from those network traces. The fea-
tures used are given in Table 1. CAIDA2007 dataset is five minutes,
i.e., 300 s of anonymized traffic obtained during a DDoS attack in
August 2007. These traffic traces recorded only attack traffic to the
victim and response from the victim, whereas the legitimate traffic
has been removed as much as possible. The CAIDA2016 dataset [2]
consists of anonymized passive traffic measurements from passive
monitors in 2016. It contains traffic traces from the ’equinix-chicago’
high-speed monitor.
3
Dataset
Training
Validation
Test
Successfully misclassified
Features modified
98.5%
98.1%
97.3%
20%
25%
22%
In our experiments, we randomly extracted malicious and legiti-
mate traffic from each dataset. The labels are provided for both the
classes (legitimate and malicious). We trained the SVM classifier
with providing 200 datapoints from each class. For the top-K, we
set K = 5.
After the classifier (SVM) is trained, we carryout the evasion at-
tack. Note that we do not consider the whole training data whereas
we take a subset (50 datapoints from each class) of training and
testing data from the whole dataset and craft them according to
Algorithm 1. Then each modified datapoint is input to the trained
SVM classifier to check if the datapoint is misclassified. The results
are shown in Table 2. The success rate is defined as the percent-
age of adversarial datapoints that were successfully misclassified
as legitimate where in actual they were malicious. The feature
modification percentage is defined as the number of features that
were modified out of the total number of input features. The pro-
posed approach is aware of what features are needed to be modified
at which location to evade the detection system.
4 CONCLUSIONS
We introduced an algorithm to modify a malicious input so that it
cannot be undetected by a classifier using SVM. To achieve this goal,
we exploited the non i.i.d. assumption of SVM classification which
can lead to a biased decision boundary. By exploiting a certain
region around the decision boundary and crafting the adversarial
sample accordingly, we show that the adversary can successfully
bypass the detector with the success rate of 98% by modifying 22%
of input features on average. For future work, we will extend the
proposed idea to study other machine learning algorithms employed
in identifying malicious traffic.
ACKNOWLEDGMENTS
The work was supported in parts by National Research Foundation
of Korea (No. 2017R1D1A1B03032966), IITP (No. B0717-16-0116),
and ITRC (IITP-2017-2012-0-00646).
REFERENCES
[1] F. Giorgio B. Biggio and R. Fabio. 2013. Security evaluation of pattern classifiers
under attack. IEEE transactions on knowledge and data engineering 26, 4 (2013),
984–996.
[2] CAIDA dataset. 2016. https://www.caida.org/data/. (2016).
[3] B. Biggio D. S. Yeung F. Zhang, P. P. Chan and F. Roli. 2016. Adversarial feature
selection against evasion attacks. IEEE Transactions on Cybernetics 46, 3 (2016),
766–777.
[4] H. Zheng G. Wang, T. Wang and B. Y. Zhao. 2014. Man vs. Machine: Practical
Adversarial Detection of Malicious Crowdsourcing Workers. USENIX Security
Symposium (2014), 239–254.
[5] S. Jha M. Fredrikson Z. B. Celik N. Papernot, P. McDaniel and A. Swami. 2016. The
limitations of deep learning in adversarial settings. IEEE European Symposium
on Security and Privacy (Euro S&P) (2016), 372–387.
[6] Y. Qi W. Xu and D. Evans. 2016. Automatically evading classifiers. In Proceedings
of the Network and Distributed Systems Symposium (NDSS) (2016), 1–15.
PosterCCS’17, October 30-November 3, 2017, Dallas, TX, USA2469