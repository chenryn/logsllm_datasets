      botocore \
      testinfra \
      pytest
    RUN wget https://download.docker.com/linux/static/stable/x86_64/docker-24.0.2.tgz \
      && tar xvzf docker-24.0.2.tgz \
      && cp docker/* /usr/bin \
      && rm -r docker docker-*
    ```
    It's prepared for:
    - Working within an AWS environment
    - Run Ansible and molecule
    - Build dockers
* New: [Build a docker within a gitea action.](gitea.md#build-a-docker-within-a-gitea-action)
    Assuming you're using the custom gitea_runner docker proposed above you can build and upload a docker to a registry with this action:
    ```yaml
    ---
    name: Publish Docker image
    "on": [push]
    jobs:
      build-and-push:
        runs-on: ubuntu-latest
        steps:
          - name: Checkout code
            uses: https://github.com/actions/checkout@v3
          - name: Login to Docker Registry
            uses: https://github.com/docker/login-action@v2
            with:
              registry: my_registry.org
              username: ${{ secrets.REGISTRY_USERNAME }}
              password: ${{ secrets.REGISTRY_PASSWORD }}
          - name: Set up QEMU
            uses: https://github.com/docker/setup-qemu-action@v2
          - name: Set up Docker Buildx
            uses: https://github.com/docker/setup-buildx-action@v2
          - name: Extract metadata (tags, labels) for Docker
            id: meta
            uses: https://github.com/docker/metadata-action@v4
            with:
              images: my_registry.org/the_name_of_the_docker_to_build
          - name: Build and push
            uses: docker/build-push-action@v2
            with:
              context: .
              platforms: linux/amd64,linux/arm64
              push: true
              cache-from: type=registry,ref=my_registry.org/the_name_of_the_docker_to_build:buildcache
              cache-to: type=registry,ref=my_registry.org/the_name_of_the_docker_to_build:buildcache,mode=max
              tags: ${{ steps.meta.outputs.tags }}
              labels: ${{ steps.meta.outputs.labels }}
    ```
    It uses a pair of nice features:
    - Multi-arch builds
    - [Cache](https://docs.docker.com/build/ci/github-actions/cache/) to speed up the builds
    As it reacts to all events it will build and push:
    - A tag with the branch name on each push to that branch
    - A tag with the tag on tag push
* New: [Bump the version of a repository on commits on master.](gitea.md#bump-the-version-of-a-repository-on-commits-on-master)
    - Create a SSH key for the CI to send commits to protected branches.
    - Upload the private key to a repo or organization secret called `DEPLOY_SSH_KEY`.
    - Upload the public key to the repo configuration deploy keys
    - Create the `bump.yaml` file with the next contents:
        ```yaml
        ---
        name: Bump version
        "on":
          push:
            branches:
              - main
        jobs:
          bump_version:
            if: "!startsWith(github.event.head_commit.message, 'bump:')"
            runs-on: ubuntu-latest
            name: "Bump version and create changelog"
            steps:
              - name: Check out
                uses: actions/checkout@v3
                with:
                  fetch-depth: 0  # Fetch all history
              - name: Configure SSH
                run: |
                    echo "${{ secrets.DEPLOY_SSH_KEY }}" > ~/.ssh/deploy_key
                    chmod 600 ~/.ssh/deploy_key
                    dos2unix ~/.ssh/deploy_key
                    ssh-agent -a $SSH_AUTH_SOCK > /dev/null
                    ssh-add ~/.ssh/deploy_key
              - name: Bump the version
                run: cz bump --changelog --no-verify
              - name: Push changes
                run: |
                  git remote add ssh PI:EMAIL:templates/ansible-role.git
                  git pull ssh main
                  git push ssh main
                  git push ssh --tags
        ```
        It assumes that you have `cz` (commitizen) and `dos2unix` installed in your runner.
* New: [Skip gitea actions job on changes of some files.](gitea.md#skip-gitea-actions-job-on-changes-of-some-files)
    There are some expensive CI pipelines that don't need to be run for example if you changed a line in the `README.md`, to skip a pipeline on changes of certain files you can use the `paths-ignore` directive:
    ```yaml
    ---
    name: Ansible Testing
    "on":
      push:
        paths-ignore:
          - 'meta/**'
          - Makefile
          - README.md
          - renovate.json
          - CHANGELOG.md
          - .cz.toml
          - '.gitea/workflows/**'
    jobs:
      test:
        name: Test
        runs-on: ubuntu-latest
        steps:
            ...
    ```
    The only downside is that if you set this pipeline as required in the branch protection, the merge button will look yellow instead of green when the pipeline is skipped.
* New: [Molecule doesn't find the `molecule.yaml` file.](molecule.md#molecule-doesn't-find-the-`molecule.yaml`-file)
    This is expected default behavior since Molecule searches for scenarios using the `molecule/*/molecule.yml` glob. But if you would like to change the suffix to yaml, you can do that if you set the `MOLECULE_GLOB` environment variable like this:
    ```bash
    export MOLECULE_GLOB='molecule/*/molecule.yaml'
    ```
### [Terraform](terraform.md)
* New: [Create a list of resources based on a list of strings.](terraform.md#create-a-list-of-resources-based-on-a-list-of-strings)
    ```hcl
    variable "subnet_ids" {
      type = list(string)
    }
    resource "aws_instance" "server" {
      # Create one instance for each subnet
      count = length(var.subnet_ids)
      ami           = "ami-a1b2c3d4"
      instance_type = "t2.micro"
      subnet_id     = var.subnet_ids[count.index]
      tags = {
        Name = "Server ${count.index}"
      }
    }
    ```
    If you want to use this generated list on another resource extracting for example the id you can use
    ```hcl
    aws_instance.server.*.id
    ```
## Infrastructure Solutions
### [Kubectl Commands](kubectl_commands.md)
* New: [Run a pod in a defined node.](kubectl_commands.md#run-a-pod-in-a-defined-node)
    Get the node hostnames with `kubectl get nodes`, then override the node with:
    ```bash
    kubectl run mypod --image ubuntu:18.04 --overrides='{"apiVersion": "v1", "spec": {"nodeSelector": { "kubernetes.io/hostname": "my-node.internal" }}}' --command -- sleep 100000000000000
    ```
## Automating Processes
### [copier](copier.md)
* New: Introduce copier.
    [Copier](https://github.com/copier-org/copier) is a library and CLI app for rendering project templates.
    - Works with local paths and Git URLs.
    - Your project can include any file and Copier can dynamically replace values in any kind of text file.
    - It generates a beautiful output and takes care of not overwriting existing files unless instructed to do so.
    This long article covers:
    - [Installation](copier.md#installation)
    - [Basic concepts](copier.md#basic-concepts)
    - [Usage](copier.md#usage)
## Storage
### [OpenZFS](sanoid.md)
* New: [Rename or move a dataset.](zfs.md#rename-or-move-a-dataset)
    NOTE: if you want to rename the topmost dataset look at [rename the topmost dataset](#rename-the-topmost-dataset) instead.
    File systems can be renamed by using the `zfs rename` command. You can perform the following operations:
    - Change the name of a file system.
    - Relocate the file system within the ZFS hierarchy.
    - Change the name of a file system and relocate it within the ZFS hierarchy.
    The following example uses the `rename` subcommand to rename of a file system from `kustarz` to `kustarz_old`:
    ```bash
    zfs rename tank/home/kustarz tank/home/kustarz_old
    ```
    The following example shows how to use zfs `rename` to relocate a file system:
    ```bash
    zfs rename tank/home/maybee tank/ws/maybee
    ```
    In this example, the `maybee` file system is relocated from `tank/home` to `tank/ws`. When you relocate a file system through rename, the new location must be within the same pool and it must have enough disk space to hold this new file system. If the new location does not have enough disk space, possibly because it has reached its quota, rename operation fails.
    The rename operation attempts an unmount/remount sequence for the file system and any descendent file systems. The rename command fails if the operation is unable to unmount an active file system. If this problem occurs, you must forcibly unmount the file system.
    You'll loose the snapshots though, as explained below.
* New: [Rename the topmost dataset.](zfs.md#rename-the-topmost-dataset)
    If you want to rename the topmost dataset you [need to rename the pool too](https://github.com/openzfs/zfs/issues/4681) as these two are tied.
    ```bash
    $: zpool status -v
      pool: tets
     state: ONLINE
     scrub: none requested
    config:
            NAME        STATE     READ WRITE CKSUM
            tets        ONLINE       0     0     0
              c0d1      ONLINE       0     0     0
              c1d0      ONLINE       0     0     0
              c1d1      ONLINE       0     0     0
    errors: No known data errors
    ```
    To fix this, first export the pool:
    ```bash
    $ zpool export tets
    ```
    And then imported it with the correct name:
    ```bash
    $ zpool import tets test
    ```
    After the import completed, the pool contains the correct name:
    ```bash
    $ zpool status -v
      pool: test
     state: ONLINE
     scrub: none requested
    config:
            NAME        STATE     READ WRITE CKSUM
            test        ONLINE       0     0     0
              c0d1      ONLINE       0     0     0
              c1d0      ONLINE       0     0     0
              c1d1      ONLINE       0     0     0
    errors: No known data errors
    ```
    Now you may need to fix the ZFS mountpoints for each dataset
    ```bash
    zfs set mountpoint="/opt/zones/[Newmountpoint]" [ZFSPOOL/[ROOTor other filesystem]
    ```
* New: [Rename or move snapshots.](zfs.md#rename-or-move-snapshots)
    If the dataset has snapshots you need to rename them too. They must be renamed within the same pool and dataset from which they were created though. For example: