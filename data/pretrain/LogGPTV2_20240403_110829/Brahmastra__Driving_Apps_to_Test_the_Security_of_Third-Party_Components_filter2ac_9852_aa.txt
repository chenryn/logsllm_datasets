title:Brahmastra: Driving Apps to Test the Security of Third-Party Components
author:Ravi Bhoraskar and
Seungyeop Han and
Jinseong Jeon and
Tanzirul Azim and
Shuo Chen and
Jaeyeon Jung and
Suman Nath and
Rui Wang and
David Wetherall
Brahmastra: Driving Apps to Test the Security of 
Third-Party Components
Ravi Bhoraskar, Microsoft Research and University of Washington; Seungyeop Han, 
University of Washington; Jinseong Jeon, University of Maryland, College Park;  
Tanzirul Azim, University of California, Riverside; Shuo Chen, Jaeyeon Jung, Suman Nath, 
and Rui Wang, Microsoft Research; David Wetherall, University of Washington
https://www.usenix.org/conference/usenixsecurity14/technical-sessions/presentation/bhoraskar
This paper is included in the Proceedings of the 23rd USENIX Security Symposium.August 20–22, 2014 • San Diego, CAISBN 978-1-931971-15-7Open access to the Proceedings of  the 23rd USENIX Security Symposium is sponsored by USENIXBrahmastra: Driving Apps to Test the Security of Third-Party Components
Ravi Bhoraskar1,2, Seungyeop Han2, Jinseong Jeon3, Tanzirul Azim4,
Shuo Chen1, Jaeyeon Jung1, Suman Nath1, Rui Wang1, David Wetherall2
1 Microsoft Research
2 University of Washington
3 University of Maryland, College Park
4 University of California, Riverside
Abstract
We present an app automation tool called Brahmastra for
helping app stores and security researchers to test third-
party components in mobile apps at runtime. The main
challenge is that call sites that invoke third-party code
may be deeply embedded in the app, beyond the reach
of traditional GUI testing tools. Our approach uses static
analysis to construct a page transition graph and discover
execution paths to invoke third-party code. We then per-
form binary rewriting to “jump start” the third-party code
by following the execution path, efﬁciently pruning out
undesired executions. Compared with the state-of-the-
art GUI testing tools, Brahmastra is able to successfully
analyse third-party code in 2.7× more apps and decrease
test duration by a factor of 7. We use Brahmastra to un-
cover interesting results for two use cases: 175 out of
220 children’s apps we tested display ads that point to
web pages that attempt to collect personal information,
which is a potential violation of the Children’s Online
Privacy Protection Act (COPPA); and 13 of the 200 apps
with the Facebook SDK that we tested are vulnerable to
a known access token attack.
1
Third-party libraries provide a convenient way for mo-
bile application developers to integrate external services
in the application code base. Advertising that is widely
featured in “free” applications is one example: 95% of
114,000 popular Android applications contain at least
one known advertisement library according to a recent
study [22]. Social media add-ons that streamline or en-
rich the user experience are another popular family of
third-party components. For example, Facebook Login
lets applications authenticate users with their existing
Facebook credentials, and post content to their feed.
Introduction
Despite this beneﬁt, the use of third-party components
is not without risk: if there are bugs in the library or the
way it is used then the host application as a whole be-
comes vulnerable. This vulnerability occurs because the
library and application run with the same privileges and
without isolation under existing mobile application mod-
els. This behavior is especially problematic because a
number of third-party libraries are widely used by many
applications; any vulnerability in these libraries can im-
pact a large number of applications. Indeed, our inter-
est in this topic grew after learning that popular SDKs
provided by Facebook and Microsoft for authentication
were prone to misuse by applications [30], and that ap-
plications often make improper use of Android cryptog-
raphy libraries [20].
In this paper, we present our solution to the problem
of third-party component integration testing at scale, in
which one party wishes to test a large number of appli-
cations using the same third-party component for a po-
tential vulnerability. To be useful in the context of mo-
bile app stores, we require that a successful solution test
many applications without human involvement. Observe
that it is not sufﬁcient to simply test the third-party li-
brary for bugs in isolation. This is because vulnerabil-
ities often manifest themselves due to the interaction of
the application and the third-party component. Thus our
focus is to develop tools that enable testers to observe in
situ interactions between the third-party component and
remote services in the context of a speciﬁc application at
runtime.
We began our research by exploring automated run-
time analysis tools that drive mobile UIs (e.g., [5, 23,
26]) to exercise the third-party component, but quickly
found this approach to be insufﬁcient. Although these
tools are effective at executing many different code paths,
they are often unable to reach speciﬁc interactions deep
within the applications for a number of reasons that we
explore within this paper. Instead, our approach lever-
ages the structure of the app to improve test hit rate and
execution speed. To do this, we characterize an app by
statically building a graph of its pages and transitions
between them. We then use path information from the
graph to guide the runtime execution towards the third-
USENIX Association  
23rd USENIX Security Symposium  1021
party component under test. Rather than relying on GUI
manipulation (which requires page layout analysis) we
rewrite the application under test to directly invoke the
callback functions that trigger the desired page transi-
tions.
We built Brahmastra to implement our approach for
Android apps. Our tool statically determines short exe-
cution paths, and dynamically tests them to ﬁnd one that
correctly invokes a target method in the third-party li-
brary. At this stage, behavior that is speciﬁc to the li-
brary is checked. Because our techniques do not require
human involvement, Brahmastra scales to analyze a large
number of applications. To show the beneﬁts of our ap-
proach, we use our tool for two new studies that con-
tribute results to the literature: 1) checking whether chil-
dren’s apps that source advertisements from a third-party
comply with COPPA privacy regulations; and 2) check-
ing that apps which integrate the Facebook SDK do not
have a known security vulnerability [30].
From our analysis of advertisements displayed in 220
kids apps that use two popular ad providers, we ﬁnd that
36% apps have displayed ads whose content is deemed
inappropriate for kids—such as offering free prizes, or
displaying sexual imagery. We also discover that 80%
apps have displayed ads with landing pages that attempt
to collect personal information from the users, such as
name, address, and online contact information—which
can be a violation of the Children’s Online Privacy Pro-
tection Act [6]. Apart from creating an unsafe environ-
ment for kids, this also leaves the app developers vulner-
able to prosecution, since they are considered liable for
all content displayed by their app.
For our analysis of a vulnerability in third party login
libraries, we run a test case proposed by Wang et al. [30]
against 200 Android apps that bundle Facebook SDK.
We ﬁnd that 13 of the examined apps are vulnerable.
Contributions: We make two main contributions. The
ﬁrst is Brahmastra, which embodies our hybrid approach
of static and dynamic analysis to solve the third-party
component integration testing problem for Android apps.
We discuss our approach and key techniques in §4 and
their implementation in §5. We show in §6 that our
techniques work for a large fraction of apps while ex-
isting tools such as randomized testing (Monkey) often
fail. We have made the static analysis part of Brah-
mastra available at https://github.com/plum-
umd/redexer.
Our second contribution is an empirical study of two
security and privacy issues for popular third-party com-
ponents. We ﬁnd potential violations of child-safety laws
by ads displayed in kids apps as discussed in §7; several
apps used in the wild display content in potential viola-
tion of COPPA due to the behavior of embedded compo-
nents. We ﬁnd that several popular Android apps are vul-
nerable to the Facebook access token attack as discussed
in §8; A Facebook security team responded immediately
to our ﬁndings on 2/27/2014 and had contacted the af-
fected developers with the instructions to ﬁx.
2 Background
As our system is developed in the context of Android,
we begin by describing the structure of Android apps and
support for runtime testing.
Android app structure: An Android app is organized as
a set of pages (e.g., Figure 1) that users can interact with
and navigate between. In Android, each page is repre-
sented by an activity object. Each activity class repre-
sents one kind of page and may be initialized with differ-
ent data, resulting in different activity instances. We
use the terms page and activity instance interchangeably.
Each page contains various GUI elements (e.g., buttons,
lists, and images), known as views. A view can be as-
sociated with a callback function that is invoked when a
user interacts with the view. The callback function can
instantiate a new activity by using a late binding mecha-
nism called intent. An intent encapsulates the descrip-
tion of a desired action (e.g., start a target activity) and
associated parameters. The main activity (or the ﬁrst
page) of an app, deﬁned in its manifest ﬁle, is started by
the application launcher by passing a START intent to it.
For example, in Figure 1, clicking the “Done” button
on activity A1 invokes its event handler, which calls a
callback function deﬁned by the app developer. The call-
back constructs an intent to start activity A2 with nec-
essary parameter P12. The Activity Manager then con-
structs an instance of A2, and starts it with P12 as parame-
ters. We refer to the documentation of Android internals
for more details [2].
Automated dynamic analysis: Recent works have used
a class of automation tools, commonly called a Mon-
key, that, given a mobile app binary, can automatically
execute it and navigate to various parts (i.e., states) of
the app. Examples include PUMA [23], DECAF [25],
AppsPlayground [26], A3E [14], and VanarSena [27]. A
Monkey launches the app in a phone or an emulator, in-
teracts with it by emulating user interactions (e.g., click-
ing a button or swiping a page) to recursively visit vari-
ous pages, and performs speciﬁc tasks (e.g., checking ad
frauds in the page or injecting faults) on each page.
In Figure 1, a Monkey may be able to visit the se-
quence of states A1 → A2 → A3→ A4 if it knows the
right UI actions (e.g., type in mother’s name and select
“Due Date” in A1) to trigger each transition. However, if
Monkey clicks a button in A3 other than “Account”, the
app would navigate to a different activity. If the goal of
testing is to invoke speciﬁc methods (e.g., Facebook lo-
gin as shown in the example), then without knowing the
structure of the app, a Monkey is likely to wander around
1022  23rd USENIX Security Symposium 
USENIX Association
E1
A1
P12
E2
E3
P34
E4
A2
A3
E5
A4
Figure 1: Activity sequences of com.alt12.babybumpfree that invoke Facebook single sign-on window in the forth
activity (A4): Clicking “I Agree” (E1) then clicking “Done” (E2) opens up A2 with the parameter, fromLoader : tru
(P12). Clicking “Settings” (E3) in A2 opens up the settings activity, A3 and then clicking “Account” (E4) opens up the
login activity, A4 with the parameter, WHICHCLASS : com.alt12.babybumpcore.activity.settings.Settings. Finally,
clicking “Login with Facebook” (E5) opens up the sign-on window within the same activity, A4.
many activities until it reaches A4, if it ever does.
3 Problem and Insights
Our goal is to develop the ability to automatically and
systematically test a large set of mobile apps that embed
a speciﬁc third-party component for a potential vulnera-
bility associated with the use of that component. This
ability would let app store operators rapidly vet apps
to contain security vulnerabilities caused by popular li-
braries. It would let component developers check how
apps use or misuse their interfaces. It would also let se-
curity researchers such as ourselves empirically assess
vulnerabilities related to third-party libraries.
A straightforward approach is to use existing Mon-
keys. Unfortunately, this approach does not work well:
it often fails to exercise the target third-party compo-
nent of the app under test. Although recent works pro-
pose techniques to improve various types of coverages,
computed as the fraction of app activities or methods in-
voked by the Monkey, coverage still remains far from
perfect [26, 14, 13, 25, 27]. Moreover, in contrast to tra-
ditional coverage metrics, our success metric is binary
for a given app indicating whether the target third-party
component (or a target method in it) is invoked (i.e., hit)
or not (i.e., miss). Our experiments show that even a
Monkey with a good coverage can have a poor hit rate
for a target third-party component that may be embed-
ded deep inside the app. We used an existing Monkey,
PUMA that reports a > 90% activity coverage compared
to humans [23], but in our experiments it was able to in-
voke a target third-party component only in 13% of the
apps we tested (see §6 for more details). On a close ex-
amination, we discovered several reasons for this poor
hit rate of existing Monkeys:
R1. Timeout: A Monkey can exhaust its time budget be-
fore reaching the target pages due to its trial-and-
error search of the application, especially for apps
with many pages that “blow up” quickly.
R2. Human inputs: A Monkey is unable to visit pages
that are reached after entering human inputs such
as login/password, or gestures beyond simple clicks
that the automated tester cannot produce.
R3. Unidentiﬁed elements: A Monkey fails to explore
clickable UI elements that are not visible in the cur-
rent screen (e.g., hidden in an unselected tab) or are
not activated yet (e.g., a “Like” button that is acti-
vated only after the user registers to the app) or are
not identiﬁed by underlying UI automation frame-
work (e.g., nonstandard custom control).
R4. Crashes: By stressing the UI, a Monkey exacerbates
app crashes (due to bugs and external dependencies
such as the network) that limit exploration.
Note that, unlike existing Monkeys, our goal is not to
exhaustively execute all the possible code paths but to
execute particular code paths to invoke methods of in-
terest in the third-party library. Therefore, our insight is
to improve coverage by leveraging ways how third party
components are integrated with application code base.
These components are incorporated into an app at the
activity level. Even if the same activity is instantiated
multiple times with different contents, third-party com-
ponents typically behave in the same way in all those
instantiations. This allows us to restrict our analysis at
the level of activity rather than activity instances. Fur-
ther, even if an app contains a large number of activities,
only a small number of them may actually contain the
third-party component of interest. Invoking that compo-
USENIX Association  
23rd USENIX Security Symposium  1023
nent requires successfully executing any and only one of
those activities.
Using this insight, our testing system, Brahmastra,
uses three techniques described below to signiﬁcantly
boost test hit rate and speed compared to a Monkey that
tries to reach all pages of the app.
Static path pruning: Brahmastra considers only the
“useful” paths that eventually invoke the target third-
party methods and ignores all other “useless” paths. In
Figure 1, Brahmastra considers the execution path A1 →
A2 → A3→ A4 for exploration and ignores many other
paths that do not lead to a target activity, A4.
Such useful paths need to be identiﬁed statically be-
fore dynamic analysis is performed. The key challenges
in identifying such paths by static analysis arise due to
highly asynchronous nature of Android apps. We discuss
the challenges and our solution in §4.
Dynamic node pruning: Brahmastra opportunistically
tries to start from an activity in the middle of the path.
If such “jump start” is successful, Brahmastra can ignore
all preceding activities of the path. For example, in Fig-
ure 1, Brahmastra can directly start activity A3, which can
lead to the target activity A4.
Dynamic node pruning poses several challenges —
ﬁrst, we need to enable jump-starting an arbitrary activity
directly. Second, jump starting to the target activity may
fail due to incorrect parameters in the intent, in which
case we need to ﬁnd a different activity that is close to
the target, for which jump start succeeds. We discuss
these in detail in next section.
Self-execution of app: Brahmastra rewrites the app bi-
nary to automatically call methods that cause activity
transitions. The appropriate methods are found by static
analysis. In Figure 1, instead of clicking on the button
with label “Done” in A1, Brahmastra would invoke the
onClick() method that would make the transition from
A1 to A2. The advantage over GUI-driven automation is
that it can discover activity-transitioning callbacks even
if they are invisible in the current screen.
In summary, our optimizations can make dynamic
analysis fast by visiting only a small number of activ-