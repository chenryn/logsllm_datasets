# Casting out Demons: Sanitizing Training Data for Anomaly Sensors

**Authors:**
- Gabriela F. Cretu<sup>1</sup>
- Angelos Stavrou<sup>2*</sup>
- Michael E. Locasto<sup>3*</sup>
- Salvatore J. Stolfo<sup>1</sup>
- Angelos D. Keromytis<sup>1</sup>

**Affiliations:**
- 1. Department of Computer Science, Columbia University
- 2. Department of Computer Science, George Mason University
- 3. Institute for Security Technology Studies, Dartmouth College

**Emails:**
- {gcretu, sal, angelos}@cs.columbia.edu
- *The work described in this paper was performed while these co-authors were at Columbia University.

## Abstract

The effectiveness of Anomaly Detection (AD) sensors is heavily dependent on the quality of the training data. Artificial or contrived training data may not accurately represent the deployment environment. Most realistic datasets are "dirty," containing numerous attacks or anomalous events. The size and complexity of these high-quality datasets make manual removal or labeling of attack data infeasible. Consequently, sensors trained on such data can miss attacks and their variations. We propose extending the training phase of AD sensors to include a sanitization phase, which is agnostic to the underlying AD algorithm. This phase generates multiple "micro-models" conditioned on small slices of the training data. These micro-models produce provisional labels for each training input, and a voting scheme is used to determine which parts of the training data may represent attacks. Our results show that this phase significantly improves the quality of unlabeled training data by making it as "attack-free" and "regular" as possible, even in the absence of absolute ground truth. Additionally, we demonstrate how a collaborative approach combining models from different networks or domains can further refine the sanitization process to thwart targeted training or mimicry attacks against a single site.

## 1. Introduction

Anomaly-based classification provides a powerful method for detecting potentially malicious inputs and behavior without relying on static signatures or incomplete behavioral specifications. Although anomaly-based approaches are not perfect, recent research indicates that signature-based network intrusion detection systems are becoming less effective at identifying malicious traffic. Song et al. [23] demonstrated the relative ease with which polymorphic attack engines can overwhelm signature-based detection methods, concluding that modeling normal behavior or content is one of the few promising alternatives. In short, relying on anomaly detection (AD) sensors to discover zero-day attacks has become a necessity rather than an option.

### 1.1 Obtaining Clean Data Sets

Improving AD sensors merits further attention, but effective anomaly detection requires highly accurate modeling of normal traffic, which remains an open problem. Taylor and Gates [26] highlighted the issue of polluted or unclean training datasets as a key roadblock. Specifically, determining "ground truth" for large, realistic datasets is extremely challenging. The intrusion detection community lacks a collection of significant, real-world datasets to test and validate new intrusion detection algorithms. Although an effort to assemble such a collection was made almost a decade ago [14], the resulting dataset was flawed, and there is a growing consensus that future experimental results based on this dataset should be ignored. The community is left without any acceptable replacement, and placing real, large datasets into wide circulation may reveal sensitive information. As a result, every organization must maintain a private extensive data collection, which presents both privacy and technical challenges.

### 1.2 Contributions

Creating a robust method for sanitizing datasets is a key challenge for addressing these problems. Large datasets typically contain an unpredictable spread of attacks, rare data items, and artifacts of misconfigurations or other errors. Manual labeling or removal of attacks is impractical due to the size and complexity of these datasets. In this paper, we propose a novel method for sanitizing such datasets, focusing on cleaning training data sets for AD sensors. Our contributions include:

- Extending the training phase of anomaly sensors with a new sanitization phase using our novel micro-models in a voting scheme to eliminate attacks and anomalies from training data.
- Building a system to implement our algorithms and applying it to training data for two anomaly sensors drawn from the research literature.
- Extending the sanitization phase to a novel distributed architecture to cross-sanitize the models and remove long-lasting attacks that might bypass the local sanitization process.
- Identifying the false false positive problem and proposing a shadow sensor architecture for consuming false positives (FP) with an automated process rather than human attention.

While we have explored similar sanitization techniques for single sites [5], our distributed strategy provides a major advance over that work. We also conduct a more thorough analysis and experimental evaluation of data sanitization techniques.

### 1.3 Technical Challenges

Ideally, an anomaly detector should achieve 100% detection accuracy with 0% false positives. However, reaching this ideal is very difficult due to several problems:

- **Underfitting:** The generated model can be overly general, flagging traffic as "normal" even if it does not belong to the true normal model, allowing attackers to disguise their exploits.
- **Overfitting:** The model of normal traffic can overfit the training data, treating non-attack traffic not observed during training as anomalous, leading to excessive false alerts.
- **Lack of Ground Truth:** Unsupervised AD systems often lack a measure of ground truth to compare and verify against, and the presence of an attack in the training data "poisons" the normal model, rendering the AD system incapable of detecting future or closely related instances of the attack.
- **Single Model Limitation:** Even with ground truth, creating a single model of normal traffic that includes all non-attack traffic can result in underfitting and overgeneralization.

### 1.4 Solution Outline

These problems stem from the quality of the normality model employed by an AD system. The traditional training phase uses all the traffic from a non-sanitized training dataset to generate a single, monolithic normality model. Our goal is to extend the AD training phase to successfully sanitize training data by removing both attacks and non-regular traffic, thereby computing a more accurate anomaly detection model that achieves both a high rate of detection and a low rate of false positives.

To achieve this, we generalize the notion of training for an AD system. Instead of using a normal model generated by a single AD sensor trained on a single large set of data, we use multiple AD instances trained on small data slices. This process produces multiple normal models, called micro-models, by training AD instances on small, disjoint subsets of the original traffic dataset. Each micro-model represents a very localized view of the training data.

Using these micro-models, we can assess the quality of our training data and automatically detect and remove any attacks or abnormalities that should not be considered part of the normal model. The intuition behind our approach is that in a training set spanning a sufficiently large time interval, an attack or an abnormality will appear only in small and relatively confined time intervals. We test each packet of the training dataset against the produced micro-models and use a voting scheme to determine which packets to consider abnormal and remove from our training set. Our analysis explores the efficiency and trade-offs of both majority voting and weighted voting schemes. The result is a training set containing packets closer to what we consider the "normal model" of the application's I/O streams.

This sanitized training set enables us to generate a single sanitized model from a single AD instance, which is very likely free of both attacks and abnormalities. This should improve the detection performance during the testing phase, as evidenced by our experiments showing a 5-fold increase in the average detection rate. Data deemed abnormal in the voting strategy is used to build a different model, called the abnormal model, intended to represent traffic that contains attacks or data not commonly seen during normal execution of the protected system.

### 1.5 Distributed Sanitization

Our initial assumptions do not hold when the training set contains persistent and/or targeted attacks, or other anomalies that persist throughout the majority of the training set. To defend against such attacks, we propose a novel, fully distributed collaborative sanitization strategy. This strategy leverages the location diversity of collaborating sites to exchange information related to abnormal data, which can be used to clean each siteâ€™s training dataset.

Our work introduces a two-phase training process: initially, we compute the AD models of "normal" and "abnormal" locally from the training set at each site. In the second phase, we distribute the "abnormal" models between sites and use this information to re-evaluate and filter the local training dataset. If data deemed normal by the local micro-models belongs to a remote "abnormal" model, we inspect or redirect this data to an oracle. Even if the identities of the collaborating sites become known, attacking all the sites with targeted or blending attacks is challenging. The attacker would need to generate mimicry attacks against all collaborators and blend the attack traffic using the individual sites' normal data models.

### 1.6 Evaluation Scenarios

Our evaluation considers two different defense configurations involving AD sensors. In the first case, we measure the increase in detection performance for a simple AD-based defense system when using the new training phase to sanitize the training set. In the second scenario, we assume a latency-expensive oracle can help classify "suspect data" and differentiate between false positives (FP) and true positives (TP). In practice, our oracle consists of a heavily instrumented host-based "shadow" server system (similar to strategies proposed by [1, 20]) that determines with very high accuracy whether a packet contains an attack. By diverting all suspect data to this oracle, we can identify true attacks by observing whether the shadow sensor emits an alert after consuming suspicious data. This high accuracy comes at the cost of greatly increased computational effort, making the redirection of all traffic to the shadow sensors unfeasible.

Many papers comment on anomaly detectors having too high a false positive rate, making them less than ideal sensors. In light of the above scenario, we see such comments as the "false false positive problem," as our shadow sensor architecture allows an automated process (instead of a human operator) to consume and vet FPs. We use this scenario to demonstrate that failure to substantially reduce the FP rate of a network AD sensor does not render the sensor useless. By using a host-based shadow sensor, false positives neither damage the system under protection nor flood an operational center with alarms. Instead, the shadow sensor processes both true attacks and incorrectly classified packets to validate whether a packet signifies a true attack. These packets are still processed by the shadowed application and only cause an increased delay for network traffic incorrectly deemed an attack.

## 2. Local Sanitization

To generate an accurate and precise normal model, researchers must utilize an effective sanitization process for the AD training dataset. Removing all abnormalities, including attacks and other traffic artifacts, from the AD training set is a crucial first step. Supervised training using labeled datasets appears to be an ideal cleaning process, but the size and complexity of training datasets obtained from real-world network traces make such labeling infeasible. Semi-supervised or even unsupervised training using an automated process or an oracle is computationally demanding and may lead to an over-estimated and under-trained normal model. Even if unsupervised training can detect 100% of the attacks, the resulting normal model may contain abnormalities that should not be considered part of the normal model. These abnormalities represent data patterns or traffic that are not attacks but still appear infrequently or for a very short period of time. For example, the random portion of HTTP cookies and HTTP POST requests may be considered non-regular and thus abnormal. This type of data should not form part of the normal model because it does not convey any extra information about the site or modeled protocol. Thus, both supervised and unsupervised training might fail to identify and remove non-regular data, producing a large and over-estimated normal model. We introduce a new unsupervised training approach that attempts to determine both attacks and abnormalities and separate them from the regular, normal model.

### 2.1 Assumptions

We observe that for a training set spanning a long period of time, attacks and abnormalities are a minority class of data. While the total attack volume in any given trace may be high, the frequency of specific attacks is generally low relative to legitimate input. This assumption may not hold in some circumstances, e.g., during a DDoS attack or during the propagation phase of a worm such as Slammer.