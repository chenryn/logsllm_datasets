title:Casting out Demons: Sanitizing Training Data for Anomaly Sensors
author:Gabriela F. Cretu and
Angelos Stavrou and
Michael E. Locasto and
Salvatore J. Stolfo and
Angelos D. Keromytis
2008 IEEE Symposium on Security and Privacy
Casting out Demons: Sanitizing Training Data for Anomaly Sensors
Gabriela F. Cretu1
Angelos Stavrou2∗
Michael E. Locasto3∗
Salvatore J. Stolfo1
Angelos D. Keromytis1
1Department of Computer Science, Columbia University
{gcretu, sal, angelos}@cs.columbia.edu
2Department of Computer Science, George Mason University
3Institute for Security Technology Studies, Dartmouth College
PI:EMAIL
PI:EMAIL
Abstract
The efﬁcacy of Anomaly Detection (AD) sensors depends
heavily on the quality of the data used to train them. Arti-
ﬁcial or contrived training data may not provide a realistic
view of the deployment environment. Most realistic data
sets are dirty; that is, they contain a number of attacks or
anomalous events. The size of these high-quality training
data sets makes manual removal or labeling of attack data
infeasible. As a result, sensors trained on this data can
miss attacks and their variations. We propose extending the
training phase of AD sensors (in a manner agnostic to the
underlying AD algorithm) to include a sanitization phase.
This phase generates multiple models conditioned on
small slices of the training data. We use these “micro-
models” to produce provisional labels for each training in-
put, and we combine the micro-models in a voting scheme to
determine which parts of the training data may represent at-
tacks. Our results suggest that this phase automatically and
signiﬁcantly improves the quality of unlabeled training data
by making it as “attack-free” and “regular” as possible in
the absence of absolute ground truth. We also show how a
collaborative approach that combines models from different
networks or domains can further reﬁne the sanitization pro-
cess to thwart targeted training or mimicry attacks against
a single site.
1
Introduction
Anomaly-based classiﬁcation provides a powerful
method of detecting inputs and behavior that are potentially
malicious without relying on a static set of signatures or a
∗
The work described in this paper was performed while these co-
authors were at Columbia University.
potentially incomplete behavioral speciﬁcation. Anomaly
sensors include those that classify both network trafﬁc con-
tent [13, 29] and sequences of system calls [22]. Although
anomaly–based approaches are not perfect [9, 26, 27], re-
cent research indicates that signature–based network intru-
sion detection systems are quickly becoming ineffective at
identifying malicious trafﬁc [4, 17, 23]. In particular, Song
et al. [23] demonstrate the relative ease with which poly-
morphic attack engines can overwhelm signature–based de-
tection methods. They conclude that modeling normal be-
havior or content represents one of a small set of promising
alternatives1. In short, relying on anomaly detection (AD)
sensors to discover 0-day attacks has become a necessity
rather than an option.
1.1 Obtaining Clean Data Sets
Current evidence seems to indicate that improving AD
sensors merits further attention. Effective anomaly detec-
tion, however, requires highly accurate modeling of normal
trafﬁc — a process that remains an open problem. In par-
ticular, Taylor and Gates [26] point to the problem of pol-
luted or unclean training data sets as a key roadblock to the
construction of effective AD sensors. Speciﬁcally, “ground
truth” for large, realistic data sets is extremely hard to de-
termine.
In a related problem, the intrusion detection community
lacks a collection of signiﬁcant, real-world data sets to test
and validate new intrusion detection algorithms. Although
an effort to assemble such a collection was made almost
a decade ago [14], the resulting data set was ﬂawed in a
1Their claim rests on the assumption that the set of good input or behav-
iors is much more constrained than all possible bad input or behaviors for
realistic applications. We ﬁnd this argument reasonable, especially since
application developers do not operate in an adversarial way: they do not
purposefully allow their software to accept widely differing sets of strings.
978-0-7695-3168-7 /08 $25.00 © 2008 IEEE
DOI 10.1109/SP.2008.11
81
number of ways [15], and there is a growing consensus that
future experimental results based on this data set should
be ignored. The community, however, is left without any
acceptable replacement. As a result, researchers and cus-
tomers cannot validate the work of other researchers or ven-
dors, especially since placing real, large data sets into wide
circulation may reveal sensitive information belonging to
the organization kind enough to donate the data. The next
best solution involves every organization maintaining a pri-
vate extensive data collection. Laying aside the challenges
involved in addressing the privacy concerns of individuals
within the organization, the technical challenge of keeping
this data set pristine is currently an open problem.
1.2 Contributions
Creating a robust method of sanitizing data sets seems to
be a key challenge for these two complementary open prob-
lems. The large data sets required for both typically contain
an unpredictable spread of attacks, rare data items, and arti-
facts of misconﬁgurations or other errors. The potential size
and complexity of these data sets makes manual labeling or
removal of attacks a futile exercise.
In this paper, we propose a novel method for sanitizing
such data sets to help address these problems. For con-
creteness, our implementation and experiments focus on the
problem of cleaning training data sets of AD sensors. Our
work provides a number of contributions:
• We extend the training phase of anomaly sensors with
a new sanitization phase that uses our novel micro-
models in a voting scheme to eliminate attacks and
anomalies from training data
• We built a system to implement our algorithms, and
we applied it to training data for two anomaly sensors
drawn from the research literature
• We extend the sanitization phase to a novel distributed
architecture in order to cross-sanitize the models and
remove long-lasting attacks that might otherwise by-
pass the local sanitization process
• We identify the false false positive problem and pro-
pose a shadow sensor architecture for consuming false
positives (FP) with an automated process rather than
human attention
While we [5] have explored the basic problem of sim-
ilar sanitization techniques for single sites, our distributed
strategy (see Section 1.5) provides a major advance over
that work. In addition, we now conduct a far more thor-
ough analysis and experimental evaluation of data sanitiza-
tion techniques.
1.3 Technical Challenges
Ideally, an anomaly detector should achieve 100% de-
tection accuracy, i.e., true attacks are all identiﬁed, with 0%
false positives. Reaching this ideal is very hard due to a
number of problems. First, the generated model can under-
ﬁt the actual normal trafﬁc. Under-ﬁtting means that the
AD sensor is overly general: it will ﬂag trafﬁc as “normal”
even if this trafﬁc does not belong to the true normal model.
As a result, attackers have sufﬁcient space to disguise their
exploit, thus increasing the amount of “false negatives” pro-
duced by the sensor. Second, and equally as troubling, the
model of normal trafﬁc can over-ﬁt the training data: non-
attack trafﬁc that is not observed during training may be
regarded as anomalous. Over-ﬁtting can generate an ex-
cessive amount of false alerts or “false positives.” Third,
unsupervised AD systems often lack a measure of ground
truth to compare to and verify against. The presence of an
attack in the training data “poisons” the normal model, thus
rendering the AD system incapable of detecting future or
closely related instances of this attack. As a result, the AD
system may produce false negatives. This risk becomes a
limiting factor of the size of the training set [25]. Finally,
even in the presence of ground truth, creating a single model
of normal trafﬁc that includes all non-attack trafﬁc can re-
sult in under-ﬁtting and over generalization.
1.4 Solution Outline
These problems appear to stem from a common source:
the quality of the normality model that an AD system em-
ploys to detect abnormal trafﬁc. This single, monolithic
normality model is the product of a training phase that tra-
ditionally uses all the trafﬁc from a non-sanitized training
data set. Our goal in this paper is to extend the AD train-
ing phase to successfully sanitize training data by removing
both attacks and non-regular trafﬁc, thereby computing a
more accurate anomaly detection model that achieves both
a high rate of detection and a low rate of false positives.
To that end, we generalize the notion of training for an
AD system. Instead of using a normal model generated by
a single AD sensor trained on a single large set of data, we
use multiple AD instances trained on small data slices. This
process produces multiple normal models, which we call
micro-models, by training AD instances on small, disjoint
subsets of the original trafﬁc dataset. Each of these micro-
models represents a very localized view of the training data.
Armed with the micro-models, we are now in a position
to assess the quality of our training data and automatically
detect and remove any attacks or abnormalities that should
not be considered part of the normal model.
The intuition behind our approach is based on the ob-
servation that in a training set spanning a sufﬁciently large
82
time interval, an attack or an abnormality will appear only
in small and relatively conﬁned time intervals. To iden-
tify these abnormalities, we test each packet of the training
data set against the produced micro-models. Using a voting
scheme, we can determine which packets to consider abnor-
mal and remove from our training set. In our analysis, we
explore the efﬁciency and tradeoffs of both majority voting
and weighted voting schemes. The result of our approach
is a training set which contains packets that are closer to
what we consider the “normal model” of the application’s
I/O streams.
This sanitized training set enables us to generate a sin-
gle sanitized model from a single AD instance. This model
is very likely free of both attacks and abnormalities. As a
result, the detection performance during the testing phase
should improve. We establish evidence for this conjec-
ture in the experiments of Section 3, which show a 5-fold
increase of the average detection rate. Furthermore, data
that was deemed abnormal in the voting strategy is used
for building a different model, which we call the abnormal
model. This model is intended to represent trafﬁc that con-
tains attacks or any data that is not commonly seen during a
normal execution of the protected system.
1.5 Distributed Sanitization
Our initial assumptions do not hold when the training
set contains persistent and/or targeted attacks, or there ex-
ist other anomalies that persist throughout the majority of
the training set. To defend against such attacks, we propose
a novel, fully distributed collaborative sanitization strategy.
This strategy leverages the location diversity of collaborat-
ing sites to exchange information related to abnormal data
that can be used to clean each site’s training data set.
Consequently, our work introduces a two-phase training
process: initially, we compute the AD models of “normal”
and “abnormal” locally from the training set at each site.
In the second phase, we distribute the “abnormal” models
between sites, and we use this information to re-evaluate
and ﬁlter the local training data set. If data deemed normal
by the local micro-models happens to belong to a remote
“abnormal” model, we inspect or redirect this data to an or-
acle. Even if the identities of the collaborating sites become
known, attacking all the sites with targeted or blending at-
tacks is a challenging task. The attacker will have to gener-
ate mimicry attacks against all collaborators and blend the
attack trafﬁc using the individual sites’ normal data models.
1.6 Evaluation Scenarios
Our evaluation considers two different defense conﬁg-
urations involving AD sensors. In the ﬁrst case, we mea-
sure the increase in detection performance for a simple AD-
based defense system when we use the new training phase to
sanitize the training set. As a second scenario, we assume
that a latency-expensive oracle can help classify “suspect
data” and differentiate between false positives (FP) and true
positives (TP). In practice, our oracle consists of a heav-
ily instrumented host-based “shadow” server system (simi-
lar to strategies proposed by [1, 20]) that determines with
very high accuracy whether a packet contains an attack.
By diverting all suspect data to this oracle, we can iden-
tify true attacks by observing whether the shadow sensor
emits an alert after consuming suspicious data. This high
accuracy, however, comes at the cost of greatly increased
computational effort making the redirection of all trafﬁc to
the shadow sensors unfeasible.
Many papers comment on anomaly detectors having too
high a false positive rate, thus making them less than ideal
sensors. In light of the above scenario, we see such com-
ments as the “false false positive problem,” as our shadow
sensor architecture allows an automated process (instead of
a human operator) to consume and vet FPs. We use this sce-
nario to demonstrate that failure to substantially reduce the
FP rate of a network AD sensor does not render the sen-
sor useless. By using a host-based shadow sensor, false
positives neither damage the system under protection nor
ﬂood an operational center with alarms. Instead, the shadow
sensor processes both true attacks and incorrectly classiﬁed
packets to validate whether a packet signiﬁes a true attack.
These packets are still processed by the shadowed applica-
tion and only cause an increased delay for network trafﬁc
incorrectly deemed an attack.
2 Local Sanitization
In order to generate an accurate and precise normal
model, researchers must utilize an effective sanitization pro-
cess for the AD training data set. To that end, removing all
abnormalities, including attacks and other trafﬁc artifacts,
from the AD training set is a crucial ﬁrst step. Supervised
training using labeled datasets appears to be an ideal clean-
ing process. However, the size and complexity of training
data sets obtained from real-world network traces makes
such labeling infeasible.
In addition, semi–supervised or
even unsupervised training using an automated process or
an oracle is computationally demanding and may lead to
an over-estimated and under-trained normal model. Indeed,
even if we assume that unsupervised training can detect
100% of the attacks, the resulting normal model may con-
tain abnormalities that should not be considered part of the
normal model.
These abnormalities represent data patterns or trafﬁc that
are not attacks, but still appear infrequently or for a very
short period of time. For example, the random portion
of HTTP cookies and HTTP POST requests may be con-
83
sidered non-regular and thus abnormal. This type of data
should not form part of the normal model because it does
not convey any extra information about the site or modeled
protocol. Thus, in practice, both supervised and unsuper-
vised training might fail to identify and remove from the
training set non-regular data, thereby producing a large and
over-estimated normal model. We introduce a new unsu-
pervised training approach that attempts to determine both
attacks and abnormalities and separate them from the regu-
lar, normal model.
2.1 Assumptions
We observe that for a training set that spans a long period
of time, attacks and abnormalities are a minority class of
data. While the total attack volume in any given trace may
be high, the frequency of speciﬁc attacks is generally low
relative to legitimate input. This assumption may not hold
in some circumstances, e.g., during a DDoS attack or dur-
ing the propagation phase of a worm such as Slammer. We