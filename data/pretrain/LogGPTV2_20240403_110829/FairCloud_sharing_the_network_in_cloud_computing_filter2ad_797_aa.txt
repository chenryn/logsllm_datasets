title:FairCloud: sharing the network in cloud computing
author:Lucian Popa and
Gautam Kumar and
Mosharaf Chowdhury and
Arvind Krishnamurthy and
Sylvia Ratnasamy and
Ion Stoica
FairCloud: Sharing the Network in Cloud Computing
Lucian Popa
HP Labs
Gautam Kumar
UC Berkeley
Mosharaf Chowdhury
UC Berkeley
Arvind Krishnamurthy
U. Washington
Sylvia Ratnasamy
UC Berkeley
Ion Stoica
UC Berkeley
ABSTRACT
e network, similar to CPU and memory, is a critical and shared
resource in the cloud. However, unlike other resources, it is nei-
ther shared proportionally to payment, nor do cloud providers oﬀer
minimum guarantees on network bandwidth. e reason networks
are more diﬃcult to share is because the network allocation of a vir-
tual machine (VM) X depends not only on the VMs running on the
same machine with X, but also on the other VMs that X commu-
nicates with and the cross-traﬃc on each link used by X. In this
paper, we start from the above requirements–payment proportion-
ality and minimum guarantees–and show that the network-speci(cid:277)c
challenges lead to fundamental tradeoﬀs when sharing cloud net-
works. We then propose a set of properties to explicitly express th-
ese tradeoﬀs. Finally, we present three allocation policies that al-
low us to navigate the tradeoﬀ space. We evaluate their characteris-
tics through simulation and testbed experiments to show that they
can provide minimum guarantees and achieve better proportional-
ity than existing solutions.
Categories and Subject Descriptors
C.2.0 [Computer-Communication Networks]: General
Keywords: cloud computing, network sharing
1.
INTRODUCTION
Cloud computing is the platform of choice for deploying and run-
ning many of today’s businesses. Central to cloud computing is
its ability to share and multiplex resources across multiple tenants.
Cloud networks, however, are shared in a best-eﬀort manner mak-
ing it hard for both tenants and cloud providers to reason about how
network resources are allocated.
We argue that a desirable solution for sharing cloud networks
should meet three requirements. e (cid:277)rst is to provide tenants guar-
antees on the minimum network bandwidth they can expect for
each VM they buy, irrespective of the network utilization of other
tenants. Such guarantees are common for resources like CPU and
memory, and having the same for the network is key to achieving
lower bounds for the worst-case performance of an application. We
refer to this requirement as min-guarantee.
e second desirable requirement, referred to as high utilizat-
ion, aims to maximize network utilization in the presence of un-
satis(cid:277)ed demands. For example, we would like an application to
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’12, August 13–17, 2012, Helsinki, Finland.
Copyright 2012 ACM 978-1-4503-1419-0/12/08 ...$15.00.
use the entire available bandwidth when no other application is ac-
tive. is can signi(cid:277)cantly improve the performance for applica-
tions with bursty (on/oﬀ) traﬃc patterns, such as MapReduce. Im-
proving application performance may, in turn, allow the provider
to charge more.
e third and last requirement is that network resources should
be divided among tenants in proportion to their payments, similar
to CPU or memory. Under the current (cid:280)at-rate per VM payment
model, this means that two tenants with the same number of (iden-
tical) VMs should get the same aggregate bandwidth assuming they
both have suﬃcient demands, since they paid the same amount of
money. We refer to this allocation requirement as network propor-
tionality. Note that the min-guarantee requirement does not achieve
network proportionality, as it only refers to the minimum band-
width guarantee of VMs; however, a VM can get a lower allocation
than its guarantee when it has a lower demand, or a higher allocation
when the other VMs have lower demands than their guarantees.
Unfortunately, none of the traditional network sharing policies
(e.g., fairness among (cid:280)ows, source-destination pairs, or sources
alone) can meet either of the min-guarantee or network propor-
tionality requirements, while more recent proposals such as Ok-
topus [10] can only provide minimum guarantees. We argue that
the diﬃculty of developing solutions to achieve these requirements
stems from the following fundamental tradeoﬀs:
(cid:15) ere is a hard tradeoﬀ between min-guarantee and network
if one aims to achieve min-guarantee, she
(cid:15) Even without requiring min-guarantees, there is a tradeoﬀ be-
proportionality:
cannot achieve network proportionality, and vice versa.
tween network proportionality and high utilization.
To this end, we propose a set of properties to help us navigate
the tradeoﬀ space and present three allocation policies that obtain
the maximal sets of non-con(cid:280)icting desirable properties. ere are
two key concepts at the core of these policies. First, we allocate
bandwidth along congested links in proportion to the number of
VMs of each tenant, not to the number of (cid:280)ows, sources, or source-
destination pairs of the tenant. is allows us to meet (restricted
versions of) the network proportionality requirement. Second, we
use the VM proximity to a link to compute a tenant’s share on that
link. Speci(cid:277)cally, in tree-based network topologies, the share of a
tenant on a link is computed as the number of VMs of that tenant in
the sub-tree delimited by that link. is allows us to provide mini-
mum guarantees by trading oﬀ network proportionality.
In summary, we make two contributions in this paper.
1. We expose the fundamental tradeoﬀs in network resource allo-
cation in cloud and data center environments, and we provide
a set of requirements and properties that allow us to explicitly
express these tradeoﬀs (§2 and §3).
2. We develop a set of resource allocation policies to best navi-
gate these tradeoﬀs (§4) and evaluate them using simulation and
testbed experiments (§5).
Demand matrix of tenant A
Allocation matrix for tenant A
DA
RA
jRAj Aggregate bandwidth of tenant A
Table 1: Notation
2. REQUIREMENTS AND TRADEOFFS
In this section, we elaborate on the desirable requirements for
bandwidth allocation across multiple tenants. en, we show that,
even though all these requirements are desirable at the same time,
they cannot be simultaneously achieved. In this context, we discuss
tradeoﬀs between these requirements and place traditional alloca-
tion policies in this space.
2.1 Assumptions and Notation
We assume an Infrastructure-as-a-Service (IaaS) cloud model
such as Amazon EC2 [1], where tenants pay a (cid:277)xed (cid:280)at-rate per
VM. Hence, our goals for network sharing are de(cid:277)ned from a per
VM viewpoint, akin to how other cloud resources are allocated to-
day. For the simplicity of exposition, we assume in this section that
all VMs are identical (in terms of hardware resources) and have the
same price. We will consider heterogeneous VMs and expand on
alternate pricing models in §4.
Our discussion is agnostic to VM placement and routing, which
we assume are implemented independently. Also, our work is
largely orthogonal to work on network topologies aimed at improv-
ing bisection bandwidth [7, 16, 18], because the possibility of con-
gestion (and hence the need for sharing policies) remains even in
full bisection bandwidth networks (e.g., many-to-many communi-
cation in MapReduce can congest any of the links in the network).
We abstract a cloud provider’s network by a graph G = (V; E),
where V is the set of physical machines and E is the set of links that
connect them. A machine is either a switch/router or a server. A
server can host one or more VMs, possibly belonging to diﬀerent
tenants. We use the term congested to refer to a fully utilized link.
A tenant, K, consists of NK VMs, and has an instantaneous NK (cid:2)
NK demand matrix DK = [Di;j
K represents the band-
width demand from tenant K’s VM i to VM j. An allocation policy
P allocates a set of rates R = fR1; : : : ; Rmg to the set of m tenants
with demands D = fD1; : : : ; Dmg, i.e.,
K ], where Di;j
P(G; D) = fR1; : : : Rmg;
∑
(1)
K ] is the instantaneous band-
K (cid:20) Di;j
K 8i; j. Finally, let
K denote the aggregate bandwidth allocation of ten-
where the NK (cid:2) NK matrix RK = [Ri;j
width allocation for tenant K and Ri;j
jRKj =
ant K (see Table 1).
2.2 Allocation Requirements
i;j Ri;j
We desire a bandwidth allocation policy that meets the following
three requirements.
(cid:15) Min-Guarantee: Provide a minimum absolute bandwidth guar-
antee for each VM. We consider the hose model [13] shown in Fig-
ure 1, where each VM is connected to a non-blocking switch by
a dedicated connection whose capacity is equal to the minimum
bandwidth guarantee. A similar model is assumed by other eﬀorts
such as Oktopus [10] or Gatekeeper [24]. is requirement is key
for achieving predictable application performance and is usually en-
forced through admission control.
(cid:15) High Utilization: Do not leave network resources underutilized
when there is unsatis(cid:277)ed demand. Consider a statically reserved,
non work-conserving hose model in which tenants cannot exceed
their guaranteed bandwidth; while it is a good (cid:277)t for low-latency
Figure 1: Guaranteed bandwidth of each VM in the hose model.
and predictable traﬃc, it is not well suited for bursty traﬃc. Note
that high utilization is more general than work conservation; for in-
stance, as we will discuss in §2.4, tenants could be disincentivized
to use free network resources even if they have unsatis(cid:277)ed demands.
is would lead to a lower utilization of the network even for a work
conserving allocation.
High utilization is particularly important
for throughput-
sensitive applications. For example, a tenant running MapReduce
jobs can utilize excess bandwidth (i.e., bandwidth unused by other
tenants) to improve the completion times for her jobs.1
(cid:15) Network Proportionality: Share bandwidth between tenants
based on their payments, just as any other resource in the cloud.
Given today’s (cid:280)at-rate per VM payment and assuming the VMs to
be identical, this requires the network share of a tenant to be pro-
portional to the total number of her VMs. us, given two tenants
A and B, an ideal solution for network proportionality would allo-
cate jRAj=jRBj = NA=NB. Unfortunately, this is not always possible
due to diﬀerent communication patterns, capacity constraints and
demands of the tenants. For example, assume NA = NB, and both
tenants have in(cid:277)nite demands, but all tenant A’s traﬃc traverses a
link of 1Gbps, while tenant B’s traﬃc a 10Gbps link. If these are the
only two tenants in the system, then the “natural” allocation would
be jRAj = 1Gbps and jRBj = 10Gbps, respectively, which would
violate the naive de(cid:277)nition of network proportionality.
Formally, we de(cid:277)ne network proportionality as a generalization
of max-min fairness. Let WK be the weight associated to tenant
K, e.g., WK = NK. Let FK be the normalized allocation of tenant
g be
K, i.e., FK = jRKj=WK. Now, let Ford(R) = fFq1 ; : : : ; Fqm
(cid:20) Fqi+1).
the sorted vector of normalized tenant allocations (Fqi
e network proportional allocation R(cid:3) corresponds to the max-
imal normalized allocation Ford;(cid:3) in increasing order, i.e., for any
qi < Ford;(cid:3)
other feasible allocation Ford, there exists qi such that Ford
and 8qj < qi, Ford
. In essence, this allocation maximizes
the minimum (normalized) tenant allocation.2
Next, we discuss the tradeoﬀs between these requirements.
qj = Ford;(cid:3)
qi
qj
2.3 Tradeoff Between Network Proportiona-
lity and Min-Guarantee
In this section we show that there is a tradeoﬀ between achieving
network proportionality and providing each VM a useful (i.e., large
enough) bandwidth guarantee.
To illustrate this tradeoﬀ, consider the example in Figure 2 show-
ing two tenants A and B. A employs two VMs, while B employs
eleven VMs. VMs A1 and B1 are hosted on the same machine; A1
communicates with A2, while B1 communicates with the rest of the
ten VMs of B. We assume that the access link of this machine is
the only congested link in the system. According to the network
proportionality requirement, A1 should get 2=13 of the access link
1While several providers (e.g., Amazon EC2) do not allow statistical
multiplexing for the CPU and memory resources, others, such as