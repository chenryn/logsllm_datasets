```  
       atime=on | off  
           Controls  whether the access time for files is updated when they are read. Turning this property off avoids  
           producing write traffic when reading files and can result in significant performance gains, though it might  
           confuse mailers and other similar utilities. The default value is on.  See also relatime below.  
```  
5\.6   
主缓存(ARC)配置,   
all表示所有数据均使用ARC, none表示不使用ARC, 相当于没有缓存. metadata表示只有元数据使用缓存.  
开启缓存可以极大的提高读性能, 写性能则会有一定下降(差异并不大).  
主要影响的还是读性能, 如果关闭arc, 读的性能会非常的差.  
```  
       primarycache=all | none | metadata  
           Controls what is cached in the primary cache (ARC). If this property is set to all, then both user data and  
           metadata is cached. If this property is set to none, then neither user data nor metadata is cached. If this  
           property is set to metadata, then only metadata is cached. The default value is all.  
```  
缓存的使用限制可以通过zfs内核参数来调整.  
```  
/sys/module/zfs/parameters/zfs_arc_grow_retry:5  
/sys/module/zfs/parameters/zfs_arc_max:0  
/sys/module/zfs/parameters/zfs_arc_memory_throttle_disable:1  
/sys/module/zfs/parameters/zfs_arc_meta_limit:0  
/sys/module/zfs/parameters/zfs_arc_meta_prune:1048576  
/sys/module/zfs/parameters/zfs_arc_min:0  
/sys/module/zfs/parameters/zfs_arc_min_prefetch_lifespan:1000  
/sys/module/zfs/parameters/zfs_arc_p_aggressive_disable:1  
/sys/module/zfs/parameters/zfs_arc_p_dampener_disable:1  
/sys/module/zfs/parameters/zfs_arc_shrink_shift:5  
parm:           zfs_arc_min:Min arc size (ulong)  
parm:           zfs_arc_max:Max arc size (ulong)  
parm:           zfs_arc_meta_limit:Meta limit for arc size (ulong)  
parm:           zfs_arc_meta_prune:Bytes of meta data to prune (int)  
parm:           zfs_arc_grow_retry:Seconds before growing arc size (int)  
parm:           zfs_arc_p_aggressive_disable:disable aggressive arc_p grow (int)  
parm:           zfs_arc_p_dampener_disable:disable arc_p adapt dampener (int)  
parm:           zfs_arc_shrink_shift:log2(fraction of arc to reclaim) (int)  
parm:           zfs_arc_memory_throttle_disable:disable memory throttle (int)  
parm:           zfs_arc_min_prefetch_lifespan:Min life of prefetch block (int)  
```  
脏数据的内存使用限制内核参数  
```  
# modinfo zfs|grep dirty  
parm:           zfs_vdev_async_write_active_max_dirty_percent:Async write concurrency max threshold (int)  
parm:           zfs_vdev_async_write_active_min_dirty_percent:Async write concurrency min threshold (int)  
parm:           zfs_dirty_data_max_percent:percent of ram can be dirty (int)  
parm:           zfs_dirty_data_max_max_percent:zfs_dirty_data_max upper bound as % of RAM (int)  
parm:           zfs_delay_min_dirty_percent:transaction delay threshold (int)  
parm:           zfs_dirty_data_max:determines the dirty space limit (ulong)  
parm:           zfs_dirty_data_max_max:zfs_dirty_data_max upper bound in bytes (ulong)  
parm:           zfs_dirty_data_sync:sync txg when this much dirty data (ulong)  
# grep ".*" /sys/module/zfs/parameters/*|grep dirty  
/sys/module/zfs/parameters/zfs_delay_min_dirty_percent:60  
/sys/module/zfs/parameters/zfs_dirty_data_max:3361508147  
/sys/module/zfs/parameters/zfs_dirty_data_max_max:8403770368  
/sys/module/zfs/parameters/zfs_dirty_data_max_max_percent:25  
/sys/module/zfs/parameters/zfs_dirty_data_max_percent:10  
/sys/module/zfs/parameters/zfs_dirty_data_sync:67108864  
/sys/module/zfs/parameters/zfs_vdev_async_write_active_max_dirty_percent:60  
/sys/module/zfs/parameters/zfs_vdev_async_write_active_min_dirty_percent:30  
```  
测试, 异步读写, primarycache=metadata的写入速度要快一点, 一般在cache填满后cache=metadata和cache=all速度达到一致.  
zpool块设备越多, 差别越明显. 通过zpool iostat -v 1来查看.  
```  
# zpool create -o ashift=12 -o autoreplace=on zp1 scsi-36c81f660eb17fb001b2c5fec6553ff5e scsi-36c81f660eb17fb001b2c5ff465cff3ed scsi-36c81f660eb17fb001b2c5ffa662f3df2 scsi-36c81f660eb17fb001b2c5fff66848a6c scsi-36c81f660eb17fb001b2c600466cb5810 scsi-36c81f660eb17fb001b2c60096714bcf2 scsi-36c81f660eb17fb001b2c600e6761a9bd scsi-36c81f660eb17fb001b2c601267a63fcc scsi-36c81f660eb17fb001b2c601867f2c341  scsi-36c81f660eb17fb001b2c601e685414b5 scsi-36c81f660eb17fb001b2c602368a21621 scsi-36c81f660eb17fb001b2c602a690a4ed8  
# zfs create -o mountpoint=/data01 -o atime=off -o primarycache=metadata zp1/data01  
# dd if=/dev/zero of=/data01/test.img bs=1024K count=819200  
^C185116+0 records in  
185116+0 records out  
194108194816 bytes (194 GB) copied, 113.589 s, 1.7 GB/s  
# zfs destroy zp1/data01  
# zfs create -o mountpoint=/data01 -o atime=off -o primarycache=all zp1/data01  
# dd if=/dev/zero of=/data01/test.img bs=1024K count=819200  
^C147262+0 records in  
147262+0 records out  
154415398912 bytes (154 GB) copied, 90.1703 s, 1.7 GB/s  
```  
读性能测试, 关闭arc后, 性能非常差, 目前还不清楚是否可以通过调整zfs内核参数来提高直接的块设备的读性能.  
```  
# zfs set primarycache=metadata zp1/data01  
# cp /data01/test.img /data01/test.img1  
# zpool iostat -v 1  
                                             capacity     operations    bandwidth  
pool                                      alloc   free   read  write   read  write  
----------------------------------------  -----  -----  -----  -----  -----  -----  
zp1                                       80.5G  43.4T    289    592  35.9M  64.5M  
  scsi-36c81f660eb17fb001b2c5fec6553ff5e  6.72G  3.62T     23     44  3.00M  5.49M  
  scsi-36c81f660eb17fb001b2c5ff465cff3ed  6.69G  3.62T     24     44  3.12M  5.49M  
  scsi-36c81f660eb17fb001b2c5ffa662f3df2  6.71G  3.62T     24     49  3.00M  5.76M  
  scsi-36c81f660eb17fb001b2c5fff66848a6c  6.72G  3.62T     23     44  3.00M  5.01M  
  scsi-36c81f660eb17fb001b2c600466cb5810  6.70G  3.62T     24     62  3.12M  5.54M  
  scsi-36c81f660eb17fb001b2c60096714bcf2  6.69G  3.62T     21     54  2.75M  5.15M  
  scsi-36c81f660eb17fb001b2c600e6761a9bd  6.71G  3.62T     27     53  3.37M  5.35M  
  scsi-36c81f660eb17fb001b2c601267a63fcc  6.71G  3.62T     21     46  2.75M  4.90M  
  scsi-36c81f660eb17fb001b2c601867f2c341  6.68G  3.62T     22     46  2.87M  5.02M  
  scsi-36c81f660eb17fb001b2c601e685414b5  6.74G  3.62T     25     54  3.24M  5.90M  
  scsi-36c81f660eb17fb001b2c602368a21621  6.71G  3.62T     23     43  3.00M  5.49M  
  scsi-36c81f660eb17fb001b2c602a690a4ed8  6.69G  3.62T     21     42  2.75M  5.37M  
cache                                         -      -      -      -      -      -  
  pcie-shannon-6819246149b014-part1       5.14M   800G      0      1      0  68.9K  
----------------------------------------  -----  -----  -----  -----  -----  -----  
```  
开启arc后, 读性能提升, 注意看读的iops增加到300+, 开启arc前只有20+  
```  
# zfs set primarycache=all zp1/data01  
# cp /data01/test.img /data01/test.img1  
cp: overwrite `/data01/test.img1'? y  
                                             capacity     operations    bandwidth  
pool                                      alloc   free   read  write   read  write  
----------------------------------------  -----  -----  -----  -----  -----  -----  
zp1                                       82.8G  43.4T  3.54K  4.01K   449M   476M  
  scsi-36c81f660eb17fb001b2c5fec6553ff5e  6.91G  3.62T    318    318  39.6M  39.6M  
  scsi-36c81f660eb17fb001b2c5ff465cff3ed  6.89G  3.62T    286    328  35.6M  40.2M  
  scsi-36c81f660eb17fb001b2c5ffa662f3df2  6.91G  3.62T    304    335  37.9M  39.6M  
  scsi-36c81f660eb17fb001b2c5fff66848a6c  6.92G  3.62T    299    335  37.3M  40.3M  
  scsi-36c81f660eb17fb001b2c600466cb5810  6.89G  3.62T    288    322  35.5M  37.1M  
  scsi-36c81f660eb17fb001b2c60096714bcf2  6.89G  3.62T    300    337  37.3M  39.4M  
  scsi-36c81f660eb17fb001b2c600e6761a9bd  6.90G  3.62T    305    330  37.9M  39.0M  
  scsi-36c81f660eb17fb001b2c601267a63fcc  6.90G  3.62T    294    343  36.8M  40.1M  
  scsi-36c81f660eb17fb001b2c601867f2c341  6.88G  3.62T    300    373  36.8M  39.5M  
  scsi-36c81f660eb17fb001b2c601e685414b5  6.94G  3.62T    321    374  39.7M  40.4M  
  scsi-36c81f660eb17fb001b2c602368a21621  6.90G  3.62T    292    365  36.4M  39.6M  
  scsi-36c81f660eb17fb001b2c602a690a4ed8  6.89G  3.62T    308    339  38.2M  41.2M  
cache                                         -      -      -      -      -      -  
  pcie-shannon-6819246149b014-part1        454M   800G      0    649      0  79.5M  
----------------------------------------  -----  -----  -----  -----  -----  -----  
```  
5\.7   
二级缓存(L2ARC)配置, 即zpool 中的cache设备.  
如果要使用L2ARC的话, 建议使用SSD作为L2ARC.  
```  
       secondarycache=all | none | metadata  
           Controls what is cached in the secondary cache (L2ARC). If this property is set to all, then both user data  
           and metadata is cached. If this property is set to none, then neither user data nor metadata is cached.  If  
           this property is set to metadata, then only metadata is cached. The default value is all.  
```  
l2arc的数据从arc的mru, mfu表取到, 所以arc如果关闭的话, l2arc也不会有缓存数据.   
所以如果要使用l2arc的话, 务必同时打开arc和l2arc.  
l2arc里面不存储脏数据, 所以对于活跃数据频繁变更的业务, L2ARC几乎没什么用处.  
5\.8   
数据块去重配置, 对于大多数场景没有什么效果, 而且如果数据集很大的话需要耗费大量的内存. 同时影响IOPS和吞吐量.  
一般不建议开启.  
```  
       dedup=on | off | verify | sha256[,verify]  
           Controls  whether  deduplication is in effect for a dataset. The default value is off. The default checksum  
           used for deduplication is sha256 (subject to change). When dedup is enabled, the dedup  checksum  algorithm  
           overrides the checksum property. Setting the value to verify is equivalent to specifying sha256,verify.  
           If  the  property  is set to verify, then, whenever two blocks have the same signature, ZFS will do a byte-  
           for-byte comparison with the existing block to ensure that the contents are identical.  
```  
5\.9  
ZIL的使用配置, 对同步写请求来说,  latency表示使用ZIL设备, throughput表示不使用zil设备(非常不推荐).  
如果使用PostgreSQL数据库, 并且使用异步事务提交的话, 是否使用zil关系都不大.  
zil要求IOPS能力很好的设备, 才能达到好的同步写请求iops.   
```  
       logbias = latency | throughput  
           Provide  a hint to ZFS about handling of synchronous requests in this dataset. If logbias is set to latency  
           (the default), ZFS will use pool log devices (if configured) to handle the requests at low latency. If log-  
           bias  is  set  to  throughput, ZFS will not use configured pool log devices. ZFS will instead optimize syn-  
           chronous operations for global pool throughput and efficient use of resources.  
```  
首先我们测试一个有SSD zil设备的, 普通机械硬盘12块组成的一个ZPOOL的fsync场景性能.  
```  
# zfs get all|grep logbias  
zp1         logbias               latency                default  
zp1/data01  logbias               latency                default  
> pg_test_fsync -f /data01/pgdata/1  
5 seconds per test  
O_DIRECT supported on this platform for open_datasync and open_sync.  
Compare file sync methods using one 8kB write:  
(in wal_sync_method preference order, except fdatasync  
is Linux's default)  
        open_datasync                                n/a*  
        fdatasync                        7285.416 ops/sec     137 usecs/op  
        fsync                            7359.841 ops/sec     136 usecs/op  
        fsync_writethrough                            n/a  
        open_sync                                    n/a*  
* This file system and its mount options do not support direct  
I/O, e.g. ext4 in journaled mode.  
Compare file sync methods using two 8kB writes:  
(in wal_sync_method preference order, except fdatasync  
is Linux's default)  
        open_datasync                                n/a*  
        fdatasync                        5396.851 ops/sec     185 usecs/op  
        fsync                            4323.672 ops/sec     231 usecs/op  
        fsync_writethrough                            n/a  
        open_sync                                    n/a*  
* This file system and its mount options do not support direct  
I/O, e.g. ext4 in journaled mode.  
Compare open_sync with different write sizes:  
(This is designed to compare the cost of writing 16kB  
in different write open_sync sizes.)  
         1 * 16kB open_sync write                    n/a*  
         2 *  8kB open_sync writes                   n/a*  
         4 *  4kB open_sync writes                   n/a*  
         8 *  2kB open_sync writes                   n/a*  
        16 *  1kB open_sync writes                   n/a*  
Test if fsync on non-write file descriptor is honored:  
(If the times are similar, fsync() can sync data written  
on a different descriptor.)  
        write, fsync, close              5859.650 ops/sec     171 usecs/op  
        write, close, fsync              6626.115 ops/sec     151 usecs/op  
Non-Sync'ed 8kB writes:  
        write                           82388.939 ops/sec      12 usecs/op  
```  
注意此时ZIL所在的SSD硬盘的利用率没有到100%, 处于一个比较低的水平.  
```  
avg-cpu:  %user   %nice %system %iowait  %steal   %idle  