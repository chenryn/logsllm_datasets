operational stage they can contribute to, shown in Figure 3. provide a snapshot of the system behavior. Metrics can rep-
Incident Detection. Incident detection tasks contribute to resent a broad range of information, broadly classified into
detection stage. The goal of these tasks are reducing mean- compute metrics and service metrics. Compute metrics (e.g.
time-to-detect (MTTD). In our survey we cover time series CPU utilization, memory usage, disk I/O) are an indicator of
incidentdetection (SectionIV-A), logincident detection(Sec- the health status of compute nodes (servers, virtual machines,
tion IV-B), trace and multimodal incident detection (Section pods). They are collected at the system level using tools such
IV-C). as Slurm [14] for usage statistics from jobs and nodes, and
Failure Prediction. Failure prediction also contributes to the Lustre parallel distributed file system for I/O information.
detection stage. The goal of failure prediction is to predict the Service metrics (e.g. request count, page visits, number of
potential issue before it actually happens so actions can be errors) measure the quality and level of service of customer
taken in advance to minimize impact. Failure prediction also facingapplications.Aggregatestatisticsofsuchnumericaldata
4
Fig.3. AIOpsTasks.InthissurveywediscussaseriesofAIOpstasks,categorizedbywhichoperationalstagesthesetaskscontributeto,andtheobservability
datatypeittakes.
also fall under the category of metrics, providing a more
coarse-grained view of system behavior.
Metrics are constantly generated by all components of the
cloudplatformlifecycle,makingitoneofthemostubiquitous
forms of AIOps data. Cloud platforms and supercomputer
clusters can generate petabytes of metrics data, making it
a challenge to store and analyze, but at the same time,
brings immense observability to the health of the entire IT
operation.Beingnumericaltime-seriesdata,metricsaresimple
tointerpretandeasytoanalyze,allowingforsimplethreshold-
based rules to be acted upon. At the same time, they contain
sufficientlyrichinformationtobeusedtopowermorecomplex
AI based alerting and actions. Fig.4. GPUutilizationmetricsfromtheMITSupercloudDatasetexhibiting
variouspatterns(cyclical,sparseandintermittant,noisy).
Themajorchallengeinleveraginginsightsfrommetricsdata
arises due to their diverse nature. Metrics data can exhibit a
varietyofpatterns,suchascyclicalpatterns(repeatingpatterns
studied by multiple literature surveys in the past [15], [16],
hourly,daily,weekly,etc.),sparseandintermittentspikes,and
[17], [18], [19], [20], [21], [22], [23].
noisy signals. The characteristics of the metrics ultimately
In most of the practical cases, especially in industrial
depend on the underlying service or job.
settings, the volume of the logs can go upto an order of
In Table I, we briefly describe the datasets and benchmarks
petabytes of loglines per week. Also because of the nature
of metrics data. Metrics data have been used in studies char-
of log content, log data dumps are much more heavier in
acterizing the workloads of cloud data centers, as well as the
sizeincomparisontotimeseriestelemetrydata.Thisrequires
variousAIOpstasksofincidentdetection,rootcauseanalysis,
special handling of logs observability data in form of data
failureprediction,andvariousplanningandoptimizationtasks
streams, - where today, there are various services like Splunk,
like auto-scaling and VM pre-provisioning.
Datadog, LogStash, NewRelic, Loggly, Logz.io etc employed
toefficientlystoreandaccessthelogstreamandalsovisualize,
B. Logs
analyze and query past log data using specialized structured
Software logs are specifically designed by the software query language.
developers in order to record any type of runtime information
about processes executing within a system - thus making Nature of Log Data. Typically these logs consist of semi-
them an ubiquitous part of any modern system or software structured data i.e. a combination of structured and unstruc-
maintenance. Once the system is live and throughout its life- tured data. Amongst the typical types of unstructured data
cycle, it continuously emits huge volumes of such logging there can be natural language tokens, programming language
data which naturally contain a lot of rich dynamic runtime constructs (e.g. method names) and the structured part can
information relevant to IT Operations and Incident Man- consistofquantitativeorcategoricaltelemetryorobservability
agement of the system. Consequently in AI driven IT-Ops metrics data, which are printed in runtime by various logging
pipelines,automatedlogbasedanalysisplaysanimportantrole statements embedded in the source-code or sometimes gener-
in Incident Management - specifically in tasks like Incident ated automatically via loggers or logging agents. Depending
Detection and Causation and Failure Prediction, as have been on the kind of service the logs are dumped from, there can be
5
a diverse types of logging data with heterogeneous form and
content. For example, logs can be originating from distributed
systems (e.g. hadoop or spark), operating systems (windows
or linux) or in complex supercomputer systems or can be
dumped at hardware level (e.g. switch logs) or middle-ware
level(likeserverse.g.Apachelogs)orbyspecificapplications
(e.g. Health App). Typically each logline comprises of a fixed
part which is the template that had been designed by the
developerandsomevariablepartorparameterswhichcapture
some runtime information about the system.
Complexities of Log Data. Thus, apart from being one of
the most generic and hence crucial data-sources in IT Ops,
logs are one of the most complex forms of observability
data due to their open-ended form and level of granularity
at which they contain system runtime information. In cloud
computingcontext,logsarethesourceoftruthforcloudusers
to the underlying servers that running their applications since
cloud providers donâ€™t grant full access to their users of the
servers and platforms. Also, being designed by developers,
logs are immediately affected by any changes in the source-
code or logging statements by developers. This results in
non-stationarity in the logging vocabulary or even the entire
structure or template underlying the logs.
Log Observability Tasks. Log observability typically in-
Fig.5. AnexampleofLogDatageneratedinITOperations
volves different tasks like anomaly detection over logs during
incidentdetection(SectionIV-B),rootcauseanalysisoverlogs
(SectionVI-B)andlogbasedfailureprediction(SectionV-B).
Datasets and Benchmarks. Out of the different log ob-
servability tasks, log based anomaly detection is one of the
most objective tasks and hence most of the publicly released
benchmark datasets have been designed around anomaly de-
tection.InTableB,wegiveacomprehensivedescriptionabout
the different public benchmark datasets that have been used
in the literature for anomaly detection tasks. Out of these,
datasets Switch and subsets of HPC and BGL have also been
redesigned to serve failure prediction task. On the other hand
therearenopublicbenchmarksonlogbasedRCAtasks,which
has been typically evaluated on private enterprise data.
Fig.6. AnsnapshotoftracegraphofuserrequestswhenusingGoogleSearch.
C. Traces
Trace data are usually presented as semi-structured logs, Trace analysis requires reliable tracing systems. Trace col-
with identifiers to reconstruct the topological maps of the lection systems such as ReTrace [24] can help achieve fast
applicationsandnetworkflowsoftargetrequests.Forexample, and inexpensive trace collections. Trace collectors are usually
when user uses Google search, a typical trace graph of this code agnostic and can emit different levels of performance
user request looks like in Figure 6. Traces are composed trace data back to the trace stores in near real-time. Early
system events (spans) that tracks the entire progress of a summarization is also involved in the trace collection process
request or execution. A span is a sequence of semi-structured to help generate fine-grained events [25].
event logs. Tracing data makes it possible to put different Although trace collection is common for system observ-
data modality into the same context. Requests travel through ability, it is still challenging to acquire high quality trace data
multiple services / applications and each application may to train AI models. As far as we know, there are very few
have totally different behavior. Trace records usually contains publictracedatasetswithhighqualitylabels.Alsotheonlyfew
two required parts: timestamps and span id. By using the existing public trace datasets like [26] are not widely adopted
timestamps and span id, we can easily reconstruct the trace in AIOps research. Instead, most AIOps related trace analysis
graph from trace logs. research use self-owned production or simulation trace data,
6
which are generally not available publicly. traces are event sequences/graphs, etc. In this article, we
discussanomalydetectionbydifferenttelemetrydatasources.
D. Other Data
A. Metrics based Incident Detection
Besides the machine generated observability data like met-
rics,logs,traces,etc.,thereareothertypesofoperationaldata
Problem Definition
that could be used in AIOps.
To ensure the reliability of services, billions of metrics are
Human activity records is part of these valuable data.
constantly monitored and collected at equal-space timestamp
TicketingsystemsareusedforDevOps/SREstocommunicate
[27]. Therefore, it is straightforward to organize metrics as
and efficiently resolve the issues. This process generates large
timeseriesdataforsubsequentanalysis.Metricbasedincident
amount of human activity records. The human activity data
detection, which aims to find the anomalous behaviors of
contains rich knowledge and learnings about solutions to
monitored metrics that significantly deviate from the other
existing issues, which can be used to resolve similar issues
observations, is vital for operators to timely detect software
in the future.
failuresandtriggerfailurediagnosistomitigateloss.Themost
UserfeedbackdataisalsoveryimportanttoimproveAIOps
basic form of incident detection on metrics is the rule-based
system performance. Unlike the issue tickets where human
methodwhichsetsupanalertwhenametricbreachesacertain
needstoputlotsofcontextinformationtodescribeanddiscuss
threshold. Such an approach is only able to capture incidents
the issue, user feedback can be as simple as one click to
which are defined by the metric exceeding the threshold,
confirm if the alert is good or bad. Collecting real-time user
and is unable to detect more complex incidents. The rule-
feedback of a running system and designing human-in-the-
based method to detect incidents on metrics are generally
loop workflows are also very significant for success of AIOps
too naive, and only able to account for the most simple of
solutions.
incidents. They are also sensitive to the threshold, producing
Although many companies collects these types of data
too many false positives when the threshold is too low, and
and use them to improve their operation workflows, there
falsenegativeswhenthethresholdistoohigh.Duetotheopen-
are still very limited published research discussing how to
ended nature of incidents, increasingly complex architectures
systematically incorporate these other types of operational
of systems, and increasing size of these systems and number
data in AIOps solutions. This brings challenges as well as
of metrics, manual monitoring and rule-based methods are no
opportunitiestomakefurtherimprovementsinAIOpsdomain.
longer sufficient. Thus, more advanced metric-based incident
Next, we discuss the key AIOps Tasks - Incident Detec-
detection methods that leveraging AI capability is urgent.
tion, Failure Prediction, Root Cause Analysis, and Automated
As metrics are a form of time series data, and incidents
Actions, and systematically review the key contributions in
are expressed as an abnormal occurrence in the data, metric