ecosystem as a whole. First, we add callbacks from the Wear
OS libraries as given by our model to enable data ﬂow analysis
across devices (see Section 2.3) . Note that we add sources and
sinks that are not detected by state-of-the-art well-maintained
projects in Android [2, 3]. More importantly, we add data
wrappers that can capture how data ﬂows propagate through
objects of the Data Layer. One limitation of existing data
ﬂow frameworks like FlowDroid [3] is that they use simpli-
ﬁed wrapper models that only abstract the semantics of the
Android framework for well-known cases.
The next step is to compute the call graph of both apps
and perform a taint tracking analysis as a single context. We
do this by ﬁrst running the taint analysis separately on each
app and then matching the results using the instrumented
APIs as connectors between data ﬂows. We add the APIs
that send data as sinks (wearable-sinks) and the APIs that
receive data as source (wearable-sources). Then, we also add
the wearable-sources and the wearable-sinks in the list of
sources and sinks.
Finally, WearFlow reports the results of the taint track-
ing. At this point, we are only interested in data ﬂows with
wearable-sources or wearable-sinks. It is worth noting that
taint analysis still detects data ﬂows that end in a non-wearable
USENIX Association
23rd International Symposium on Research in Attacks, Intrusions and Defenses    69
sink, but they are irrelevant to the matching step. Our approach
is agnostic to the underlying method used to compute data
ﬂows. We refer to Section 6.1 for implementation details.
Phase 5: Matching Analysis
The ﬁnal step consists of matching exit points with entry
points; that is to say, wearable-sinks with wearable-sources.
We consider three values to match data ﬂows: channel path,
API method, and key. If the value of the path or key could
not be calculated during the context extraction, then we use
a wildcard value that matches any value. To match the API
methods, we built a semantic table that provides information
to match wearable-sinks with its corresponding wearable-
sources. We present a summary in the Table 2 due to space
limitation. The table contains thirty-four entries in total and it
can be found in the project repository.
6 Evaluation and Results
We evaluate WearFlow against other Android information
ﬂow analysis tools currently available and perform a large-
scale analysis of 3.1K Android APKs with wearable compo-
nents looking for sensitive data leaks. Our evaluation uses
a speciﬁcally crafted set of apps that presents different data
exﬁltration cases using the Data Layer API. We conduct our
experiments on a machine with 24 cores Intel Xeon CPU
E5-2697 v3 @ 2.60GHz and 32 GB of memory.
6.1
Implementation
WearFlow relies on the Soot framework [29] to perform
the de-obfuscation, context extraction and app instrumen-
tation (Phases 2, 3.1 and 3.3). Our implementation lever-
ages FlowDroid [3] with a timeout of 8 minutes per app
for the information ﬂow analysis (Phase 4) and Violist [19]
for the string analysis (Phase 3.2). We use FlowDroid and
Soot because previous works report that they provide a good
balance between accuracy and performance on real-world
apps [6, 24, 25]. We customize FlowDroid to run on wear-
able apps by adding callbacks from the Wear OS libraries
and by extending the SuSi [2] sources and sinks as discussed
in Section 5. We also perform several optimizations to Vi-
olist to reduce the execution time while keeping the accu-
racy for the APIs we were interested in. For instance, we
reuse the control ﬂow graph generated by Soot, and we limit
the evaluation of the strings to relevant methods. With this,
WearFlow adds, overall, around 6000 LoC to these frame-
works. We make the implementation of WearFlow open
source in https://gitlab.com/s3lab-rhul/wearflow/.
6.2 Evaluation results
As community lacks on a test suite that include Mobile-Wear
information ﬂows for Android, we create WearBench4. Wear-
Bench has 15 Android apps with 23 information ﬂows be-
tween the mobile app and the wearable companion (18 of
them sensitive). Our test suite covers examples of all APIs
from the Data Layer. It also contains challenges for the instru-
mentation like ﬁeld sensitivity, object sensitivity, and branch
sensitivity for listeners.
Our suite is inspired by Droid-Bench5 and ICC-Bench6,
which are standard benchmarks to evaluate data ﬂow tools.
Note that these benchmarks evaluate the effectiveness of the
taint analysis, and some Inter-App communication cases us-
ing ICC methods. Instead, we are evaluating Inter-App com-
munication between mobile and wearable apps (using the
Data Layer API). Therefore, we cannot use these benchmarks
alone to evaluate WearFlow. In our evaluation, we compare
our results against FlowDroid. For this, we add the Data Layer
APIs as sources and sinks, execute FlowDroid on both the
mobile and wearable companion and look for matches. We
run FlowDroid with a context sensitive algorithm twice: ﬁrst
with high precision, we set the access path length to 3. Then
we reduce the precision by setting the access path to 1. With
this, FlowDroid truncates taints at level 1. This conﬁguration
increases the number of false positives but catches situations
where FlowDroid fails to propagate taint abstraction correctly.
Table 3a shows the result of our evaluation against the test
suite. WearFlow detects all the 18 exﬁltration attempts with
two false positives. These two false positives stem from a
branching sensitivity issue present in FlowDroid, which re-
sult in false positives during the data ﬂow analysis (Phase
4). Conversely, FlowDroid with high precision detects 6 out
of 18 exﬁltrations — these are only matches communicat-
ing with the MessageClient API. This is because FlowDroid
fails to propagate taints on complex objects from the Data
Layer. When reducing the precision, FlowDroid identiﬁes
matches with MessageClient and DataClient but still fails
to identify sensitive ﬂows with the ChannelClient API. In
this case, FlowDroid produces 12 false positives. This results
from an overestimation of taints that uses DataItem.
Our results show that WearFlow performs better than Flow-
Droid by a clear margin. This exempliﬁes how the modeling,
instrumentation and matching analysis can improve infor-
mation ﬂow analysis in wearable applications.
6.3 Analysis of Real-World Apps
We use WearFlow to search the presence of potential data
leaks on around 3.1K real-world APKs available in the Google
4https://gitlab.com/s3lab-rhul/wearbench/
5https://github.com/secure-software-engineering/DroidBench
6https://github.com/fgwei/ICC-Bench
70    23rd International Symposium on Research in Attacks, Intrusions and Defenses
USENIX Association
Table 2: Selection of Sink-Source matches in Data Layer API. A full list can be found in WearFlow repository.
Wearable sink signature
DataClient: void putString(String,String)
Library
DataClient
MessageClient MessageClient: Task sendMessage(String,String,byte[]) MessageEvent: byte[] getData()
DataMap: Asset getAsset(String)
DataClient
Task receiveFile(Channel,Uri,Boolean,String)
ChannelClient
DataClient: void putAsset(String,Asset)
ChannelClient sendFile(Channel,Uri)
Wearable source signature
DataMap: String getString(String)
Table 3: Summary of our results.
(a) Results for our test-suite between WearFlow and FlowDroid. HP = high precision, LP = low precision.
Existing Data Flows
Sensitive
WearFlow
Found Data Flows
Flowdroid-HP Flowdroid-LP
Library
DataItem
Message
Channel
Apps Total
16
6
1
9
5
1
13
4
1
14 (1 FP, 1 FN)
0 (13 FN)
5 (1 FP)
1
6
0 (1 FN)
22 (6 FP)
10 (6 FP)
0 (1 FN)
(b) Results for real-world apps (* sensitive data
ﬂows).
Number of apps
With ﬂows
With sensitive *
Apps APKs
220
3,111
293
47
6
50
Play Store (downloaded from AndroZoo [1]). From an initial
set of 8K APKs, around 5K refer to standalone (only wear)
APKs, and 3.1K include mobile and wearable components.
We execute WearFlow against this set which corresponds to
220 different package names. Table 3b shows a summary of
the results. Note that the dataset contains multiple versions
of the same app. Thus, we refer to apps as APKs with unique
package name.
Figure 4 shows a summary of the different APIs used as
exit/entry points of sensitive data ﬂows. Although we found
the occurrence of the ChannelClient API in the dataset, we
did not ﬁnd any case where this API was used to send sensitive
information. WearFlow identiﬁes sensitive information ﬂows
that include the transmission of device contacts (via Cursor
objects), location, activities, and HTTP trafﬁc. We also found
that in several occasions sensitive data ended up in the device
logs (17% of overall sinks) or SharedPreference ﬁles (20%).
A more detailed analysis of these ﬂows for a selection of apps
is provided in Section 6.5.
WearFlow is capable of ﬁnding 4,896 relevant data ﬂows in
all the analyzed APKs. Out of those, 388 relate to Mobile-
Wear sensitive information ﬂows in 6 apps (or 50 APKs,
when considering all versions and platforms). The results
indicate that 70% of the ﬂows are from the mobile to the
wear platform, while 30% are wear to mobile.
6.4 Applicability
We next see how we perform when dealing with obfuscation
and what is the runtime overhead.
Obfuscation. WearFlow detects 282 obfuscated APKs in
the dataset. The deobfuscation phase successfully unmangles
all these APKs. On the one hand, we ﬁnd 71 data ﬂows using
Figure 4: Sensitive information ﬂows found. [M] refers to
Android and [W] refers to Wear OS.
the Data Layer within these APKs. WearFlow did not ﬁnd rel-
evant APIs in 651 APKs. This can either because these APIs
are not used at all or because developers use more complex
obfuscation techniques. We discuss this in Section 7.
On the other hand, we ﬁnd around 2K non-obfuscated
APKs in our dataset. WearFlow instruments 4.8 components
on average per APK (excluding library classes). From all the
wearable APIs, around 48% are DataClient APIs, 51% |Mes-
sageClient| APIs, and less than 1% ChannelClient APIs. This
number shows that developers are aggregating multiple data
into DataClient before synchronizing DataItems and shows
the beneﬁts of instrumenting the APKs to track individual
data ﬂows.
USENIX Association
23rd International Symposium on Research in Attacks, Intrusions and Defenses    71
[W]PackageManager.queryIntentActivities(): 25 (6.4%)DataClient: 189 (48.7%)Service.startService()[M]: 6 (1.5%)SharedPreferences()[M]: 19 (4.9%)[M]Cursor.getString(): 140 (36.1%)Service.startService()[W]: 1 (0.3%)SharedPreferences()[W]: 59 (15.2%)DataClient()[W]: 36 (9.3%)MessageClient: 199 (51.3%)Service.sendBroadcast()[W]: 29 (7.5%)Log()[W]: 67 (17.3%)[W]Location.getLatitue/Longitude(): 67 (17.3%)OutputStream.write()[M]: 13 (3.4%)String.replace()[M]: 24 (6.2%)HttpURLConnection.getInputStream()[M]: 23 (5.9%)URL.openConnection()[M]: 7 (1.8%)[M]DataMap.getInt(): 12 (3.1%)DataClient.putInt()[W]: 8 (2.1%)[W]DataMap.getString(): 3 (0.8%)Log()[M]: 3 (0.8%)[M]DataMap.getString(): 33 (8.5%)SharedPreferences()[W]: 7 (1.8%)DataClient.putString()[W]: 26 (6.7%)[M]MessageEvent.getDataSource: 36 (9.3%)[M]HttpResponse.getEntity: 72 (18.6%)Service.sendBroadcast()[W]: 60 (15.5%)Connector LibrarySinksSourcesRunning time. Running our tool on the real-world dataset
took 115 hours. WearFlow analyzes over 95% of the APKs
before the 8 minutes timeout lapses. The average time per
APK is 3.1 minutes. Note that wearable apps are considerably
smaller in size than mobile apps, and WearFlow evaluates
most wearable apps in less than 1 minute. The time distribu-
tion per phase analysis is the following: pre-processing 13%,
string analysis 9%, deobfuscation and instrumentation 2%,
and data ﬂow analysis 76%.
WearFlow failed to complete the analysis for a small num-
ber of APKs. In most cases, this is due to unexpected bytecode
that Soot fails to handle, errors while parsing APK resources,
or because the analysis reached an extended timeout.
Overall, WearFlow extracts data ﬂows for an additional of
282 Mobile-Wear APKs. Without the deobfuscation phase,
these ﬂows would not otherwise be extracted. The deobfus-
cation phase only takes 2% of the running time.
6.5 Case Studies
This section describes issues found by WearFlow in speciﬁc
apps in relation to the threat model presented in Section 3.
Companion Leak. We ﬁrst study the case of Wego
(com.wego.android), a travel app to book ﬂights and hotels
with more than 10 million downloads. We ﬁnd a sensitive
ﬂow that starts in the watch with source getLatitude() from
the Location API, and it is sent to the mobile app with the
MessageClient API using the path “request-network-ﬂights”.
Then, the mobile app sends out this data through URL using
the HttpURLConnection object, and write it to a ﬁle system
using the java.io.OutputStream class. In this case, both the
wearable and the mobile declare the location permission in the
Manifest. However, this alone is not enough to comply with
the guidelines.7 In this case, the wear app must send the user
to the phone to accept the permission. This case shows that it
is possible to bypass the permission system using Data Layer
APIs.
Permission re-delegation. Venom (fr.thema.wear.watch.-
venom) is a Watch Face customized for a watch user
interface. The mobile version of this app uses the an-
droid.database.Cursor class to store sensitive information
such as the call history or unread messages in a database. The
app aggregates all information in a DataItem object and syn-
chronizes it with the wearable app. However, the wearable
app does not declare the relevant permissions. Interestingly,
the string analysis has been key to uncover the type of infor-
mation the app is retrieving from the database and trace it
7https://developer.android.com/training/articles/
wear-permissions
back to API sources that relate to the sensitive information
discussed (e.g., missed calls and text messages).
Sensitive Data Exposed Finally, we observe evidence of
apps exposing sensitive data through a wide range of sinks,
including Android Broadcast system and Shared Preferences.
For instance, Talent (il.talent.parking) — which is used for
car parking — reads data related to the last parking place
and its duration from a database and synchronizes it with
the watch using a DataItem. Then the wearable app writes
the data to Shared Preferences. Another example is the app
com.mobispector.bustimes, which shows bus and tram timeta-
bles, and has more than 4 million downloads. The app reads
data from an HTTP response, then send it to the wearable
through the MessageClient API, and ﬁnally executes a system
Broadcast exposing the content of the HTTP response.
All these cases show how developers leverage the Data
Layer API to send sensitive information. While it is unclear
whether or not these cases intentionally use Google Play
services to hinder the detection of data leakages, we see that
WearFlow is effective at exposing bad practices that can
pose a threat to security and privacy.
7 Limitations
This section outlines the limitations of our work. These may
arise from WearFlow’s implementation or the dataset we used.
Data Transfer Mechanisms. WearFlow inherits the limi-
tations of static analysis, i.e.: it is subject to constrains of the
underlying ﬂow and string analysis techniques. This means
that it fails to match data ﬂows with native code, advanced
reﬂection, or dynamic code loading. Still, WearFlow can be
used together with other frameworks [22,31] that handle these
issues to improve the accuracy of the analysis.
WearFlow considers obfuscation while performing the anal-
ysis of apps. There are four trivial techniques and seven non-
trivial techniques commonly used in the wild, according to a
large scale study of obfuscation in Android [17]. WearFlow
type-signature deobfuscator is resilient to all trivial techniques
and ﬁve non-trivial, but it fails to deobfuscate APKs with
class or package renamed and reﬂection. As mentioned be-
fore, WearFlow did not found relevant APIs in 20% (651) of
the APKs, many of which correspond to APKs with these
obfuscation techniques.