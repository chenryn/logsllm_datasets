veriﬁed(id):C[X(id(cid:48))| X(id)]⇔ veriﬁed(id):C[X(id)]
It follows from Lemma 2 that if process X(id) is
present, then it is not necessary to consider the corre-
sponding X(id(cid:48)) process as well.
When there is a dishonest prover at a different location
to a basic attacker process, the dishonest prover could
send all of its secrets to the basic attacker process en-
abling it to also act as a dishonest prover:
Lemma 3. For any distance bounding protocol speciﬁ-
cation (P(id),V, ˜n), from which we derive DP-A, and for
all processes P and Q, names id, tuple of names ˜n, and
sets of names E, and all names x (including x = id), we
have that
veriﬁed(id):new ˜n.[P | A] | [DP-A(x) | Q]
⇔ veriﬁed(id):new ˜n.[P | DP-A(x)] | [DP-A(x) | Q]
⇔ veriﬁed(id):new ˜n.[P | DP-A(x)] | [A | Q]
Our observations reduce the number of interesting,
distinct, system contexts to 27, each of which models
a different distance bounding attack scenario, and pro-
tection against which offers a distinct security property.
These 27 contexts are given in the ﬁgure in the Appendix.
Lemma 1 lets us order contexts in terms of the strength
of the security properties they represent. For instance, if
we replace T P-A(id) with DP-A(id), then the attacker is
strictly more powerful, and the security properties they
represent are stronger. Additionally, we note that adding
processes to a context will not affect the veriﬁed, pred-
icate, e.g., veriﬁed(id):C[A] ⇒ veriﬁed(id):C[A | P(x)].
The partial order this leads to is shown in the ﬁgure in
the Appendix.
For any protocol, if it is secure against an attack sce-
nario in this ordering then it is also secure against the
attack scenarios directly below it. Additionally, we can
ﬁnd examples to show that all the attack scenarios are
different, and that attack scenario that are not directly
above or below each other are unrelated.
This partial ordering of attack scenarios the Appendix
tells us that protection against distance hijacking attacks
is strictly stronger than security against distance fraud
attacks, and that security against assisted distance fraud
is stronger than security against terrorist fraud attacks,
which in turn is a stronger property than security against
relay attacks. However, distance hijacking and assisted
distance fraud are not directly comparable properties. To
illustrate this we could consider a veriﬁer with an over-
ride mode: if a process sent it the secret key of a prover
then it would accept it as local. Such a protocol could be
secure against assisted distance fraud but would not be
secure against distance hijacking.
On the other hand we could consider a veriﬁer that
would correctly distance bound a process and would then
accept any identity from that local process. Such a pro-
tocol could be secure against distance hijacking but not
against assisted distance fraud. Therefore, the strongest
property that a distance bounding protocol can have is
protection from both distance hijacking and assisted dis-
tance fraud.
To separate many of the distance bounding properties
we need to consider a veriﬁer that will verify any pro-
cess that sends it a secret key. This is the difference be-
tween what a dishonest prover and a terrorist prover can
do, however there are currently no proposals for distance
bounding protocols with this behaviour. Therefore, it is
a safe assumption that for any proposed distance bound-
ing protocol, if there is no local attacker process, then the
ability to send a secret key does not add any additional
power. This means that:
Assumption 1. Distance bounding protocols will not be
designed so that a correct prover could send their secret
key to the veriﬁer. I.e.,
veriﬁed(id):new ˜n.[V ] | [DP-A(id)]
⇔ veriﬁed(id):new ˜n.[V ] | [T P-A(id)]
All examples of distance bounding protocols we have
seen in the literature do not distance bound the veriﬁer to
the prover. This would mean that the attacker does not
gain any additional power by being local to the prover,
rather than local to the veriﬁer. This further reduces the
number of interesting cases we need to consider.
Assumption 2.
prover does not also distance bound the veriﬁer.
veriﬁed(id):new ˜n.[V | A] | [P(id)]
In the protocols we consider the
I.e.,
⇔ veriﬁed(id):new ˜n.[V | A] | [P(id) | A]
These assumption, along with the lemmas above, leave
us with 14 distance bounding attack scenarios, which can
be ordered using the lemmas above. This ordering is
shown in Figure 3.
Discussion: With the assumption that transmitting the
secret key does not matter, assisted distance fraud be-
comes the most powerful distance bounding property. If
1572    27th USENIX Security Symposium
USENIX Association
Figure 3 Ordering of distance bounding attack scenarios that follows from lemmas 1, 2 and 3 and assumptions 1 and
2. Higher properties imply those below them. We write [V (id) | P] | [Q] for veriﬁed(id):[V | P] | [Q]
a distance bounding protocol is secure against this at-
tack scenario, then none of the other attacks are possible.
However, this property is very strong; industrial distance
bounding protocols such as MasterCard’s RRP or NXP’s
proximity check do not have this property nor do they
need it: If a bank card or key fob has been fully com-
promised, then an attacker may send all key information
from this device to the same location as the veriﬁer and
so pass the veriﬁcation.
The lines which dissect Figure 3 each represent differ-
ent possible attacker models, and each area is dominated
by a single property, which, if checked, will prove secu-
rity for that particular attacker model. Assisted distance
fraud, and all of the other attack scenarios that require a
terrorist fraud attacker process (as indicated by the red
dotted line in Figure 3), rely on the terrorist fraud at-
tacker simply deciding not to send their key. While such
an attacker could exist, there is nothing to stop an at-
tacker, that has compromised a device, from sharing the
secret key. Therefore, the additional protection provided
by protecting against a terrorist attacker is questionable
in some attacker models.
The brown, large dashed lines separates the properties
in which the veriﬁer is checking a compromised prover
from an uncompromised prover. Many of the use cases
for distance bounding protocols aim to protect a device
against relay attack, thereby preventing criminals from
taking a victim’s car or making a payment with the vic-
tim’s EMV card, for instance.
In this attacker model,
if the attackers have compromised the device, then they
can simply clone it, making the distance bounding at-
tack unnecessary. In this model, checking veriﬁed(id):
[V | DP-A(id(cid:48)) | A] | [P(id) | DP-A(id(cid:48))] ensures that all
of the possible relevant security properties hold. For this
security property to hold, the attacker should not be able
to pretend to be an uncompromised device, regardless of
how many other devices are compromised. We deﬁne
this property as uncompromised distance bounding:
Deﬁnition 6 (Uncompromised Distance Bounding at-
tack). Given a name id(cid:48) and a distance bounding pro-
tocol (V,P(id), ˜n), from which we derive a dishonest
prover DP-A(id(cid:48)), we say that the protocol is vulnera-
ble to an uncompromised distance bounding attack if:
veriﬁed(id):new ˜n.[V | DP-A(id(cid:48))] | [P(id) | DP-A(id(cid:48))]
otherwise we say that it is safe from this attack.
As we are dealing with dishonest provers, by Lemma 3:
veriﬁed(id):new ˜n.[V | DP-A(id(cid:48))] | [P(id) | DP-A(id(cid:48))]
⇔ veriﬁed(id):new ˜n.[V | A] | [P(id) | DP-A(id(cid:48))]
⇔ veriﬁed(id):new ˜n.[V | DP-A(id(cid:48))] | [P(id) | A]
therefore any of these system contexts could be used
to represent uncompromised distance bounding attacks.
USENIX Association
27th USENIX Security Symposium    1573
Distance	Fraud	[V(id)]	|	[DP(id)]		Maﬁa	fraud/Relay	[V(id)|A]	|	[P(id)|A]	[V(id)]	|	[P(id)|A]	[V(id)|P(id’)]	|	[P(id)|A]	Terrorist	Fraud	[V(id)|A]	|	[TP(id)]	[V(id)|P(id')|A]	|	[TP(id)]	Distance	Hijacking	[V(id)|P(id')]	|	[DP(id)]	Assisted	Distance	Fraud		[V(id)|DP(id')		]	|	[TP(id)]	Remote	a6acker	only	Uncompromised	Distance	Bounding	[V(id)|DP(id')]	|	[P(id)|DP(id’)]	Relay	Hijacking	[V(id)|P(id')|A]	|	[P(id)|A]	Trusted	devices	only		Some	untrusted	devices	Terrorist	a6acker	[V(id)]	|	[P(id)|DP(id')]	[V(id)|A]	|	[P(id)|TP(id')]	[V(id)|P(id')]	|	[P(id)|DP(id')]	[V(id)|P(id')|A]	|	[P(id)|TP(id')]		No	terrorist	a6acker	Remote	and	local	a6ackers	Key:				P(id):					honest	provers	with	idenGty	“id”							V(id):					veriﬁer	wishing	to	veriﬁer	“id”				A:											a6acker	process				TP(id):		terrorist	provers,	acGng	as	“id”				DP(id):		dishonest	provers,	acGng	as	“id”	Prover	being	checked							is	compromised	Prover	being	checked		is	not	compromised	We choose the one that makes it clear that the dishonest
prover can act at both locations.
The purple dot-dashed line separates the attack sce-
narios that have only a remote attacker from those that
let the attacker act both locally to the veriﬁer and re-
motely. In the case where transmissions from the veri-
ﬁer can be picked up remotely and the attacker can only
act remotely, the strongest possible property is distance
hijacking. However, in many applications the messages
from the veriﬁer are limited to the local area (e.g. due to
the RFID technology as used by contactless EMV cards),
therefore the attacker must be able to act locally to the
veriﬁer and these attack scenarios do not apply.
The green small dashed line marks out the attack sce-
narios that assume trusted hardware from those that al-
low some provers to be compromised. Our ordering
shows that veriﬁed(id):new ˜n.[V | P(id)| A]| [P(id)| A] is
the most powerful property that can be tested in this cat-
egory. This attacker corresponds to, for instance, a relay
attack against an EMV card, which uses another, differ-
ent EMV card at the veriﬁer’s location. The use of this
other EMV card that is co-located with the veriﬁer makes
it a more powerful attacker than a basic relay attack, but
it is still less powerful than an uncompromised distance
bounding attack, because it does not require any cards to
be compromised. We do not believe this particular attack
scenario has been identiﬁed before, as a distinction from
relay attacks, so we call this “relay hijacking”.
In summary, our ordering tells us that:
• If the protocol is aiming to defend against terrorist
fraud attackers, then it should be checked against
assisted distance fraud.
• If the attacker model does not include terrorist fraud
attackers, then the strongest protection a protocol
can have is against both distance hijacking and un-
compromised distance bounding attacks.
• If the attacker model does not require protection for
a compromised prover, then the strongest attack that
needs to be defended against are uncompromised
distance bounding attacks.
• If a distance bounding protocol assumes trusted
then the strongest attack that
hardware devices,
needs to be defended against is relay hijacking:
veriﬁed(id):[V | P(id(cid:48)) | A] | [P(id) | A].
• If the attacker model only considers attackers that
are remote from the veriﬁer, then the strongest at-
tack that needs to be defended against is distance
hijacking.
6 Automated reasoning
To enable automated reasoning, we deﬁne a compiler
from our timer location calculus to a dialect of the ap-
plied pi calculus with phases [7], which can be automat-
ically reasoned with using the ProVerif tool [8]. Phases
are used to deﬁne an ordering on reduction, e.g., pro-
cesses in phase 1 can only be executed before the pro-
cesses in phase 2, which come before the processes in
phase 3, etc. Beyond phases, the applied pi-calculus adds
named communication channels, e.g., out(c,m) outputs
message m on the channel c. Channels can be public or
private, and the attacker can only send and receive mes-
sages on public channels. The applied pi-calculus does
not have timers or locations, and our compiler encodes
the start timer, stop timer and locations using other prim-
itives. Thus, compilation enables distance bounding pro-
tocols to be veriﬁed automatically using ProVerif.
We restrict compilation to extended linear processes
that contain at most one timer:
Deﬁnition 7. A linear process is a process without par-
allel composition or replication. Moreover, an extended
linear process is a process new ˜n.L1 | ··· | Li |!Li+1 | ··· |
!Ln, where L1, . . . ,Ln are linear processes.
Linear processes allow us to express all distance bound-
ing protocols from the literature, so they do not reduce
the usefulness of our method.
Using linear processes, we introduce a technique to
simplify the detection of vulnerabilities and deﬁne a
compiler that allows us to take advantage of that tech-
nique.
Proof technique: It follows from Deﬁnition 2 that: if
veriﬁed(id):new ˜n.[!L1 | L2 | A] | [L3 | A] such that only
L1 contains a timer, then there exists a successful ex-
ecution of L1. Moreover, the following lemma shows
that it is sufﬁcient to consider L1 |!blind(L1) in place of
!L1, where blind(L1) is L1 after removing timer actions
(startTimer and stopTimer) and events, hence, it sufﬁces
to isolate timers and events to a single instance of L1.
Lemma 4. For all system contexts new ˜n.[!VL | Lv | A] |
[Lp | A], sets of names E and name id, such that VL, Lv
and Lp are linear processes and only VL contains a timer,
we have: veriﬁed(id):new ˜n.[!VL | Lv | A] | [Lp | A] ⇒
veriﬁed(id):new ˜n.[VL |!blind(VL) | Lv | A] | [Lp | A].
It follows from Lemma 4 that distance bounding at-
tacks can be detected by checking whether a single in-
stance of the veriﬁer is deceived. Moreover, we need
only consider a single unreplicated timer.
Our compiler: Intuitively, the goal of our compiler is
to encode a single timer using phases. In particular, all
processes should initially be in phase 0, hence, all pro-
cesses are initially active. Once the timer is activated, we
advance all processes at the same location as the timer to
phase 1, hence, only processes at the timer’s location are
active. Finally, once the timer is deactivated, we advance
1574    27th USENIX Security Symposium
USENIX Association
all processes to phase 2, hence, all processes are active.
Thus, compilation encodes timers as phases.
Encoding the activation and deactivation of timers as
phases is straightforward,
indeed, we merely replace
startTimer.P with 1:P and stopTimer.Q with 2:Q. But,
encoding the advancement of other processes at the same
location as the timer from phase 0 to phase 1 is problem-
atic, as is advancing processes at different locations from
phase 0 to phase 2, because we cannot know when pro-
cesses should advance. We overcome this problem by
over-approximating advancement.
We over-approximate by ensuring processes can ad-
vance between phases at any time.
It sufﬁces to con-
sider advancements just before input operations, because
processes ready to output can be reduced by an attacker
that receives those outputs before an advancement and
replays the messages received afterwards, and other pro-
cesses do not produce communications, so it does not
matter whether they happen before or after an advance-
ment. We deﬁne the following function to produce all
ways in which advancements can be inserted into a pro-
cess before inputs.
Deﬁnition 8. Given a timer location calculus process P,
and a non-empty list of integers ds, we deﬁne the function
phases, to applied pi-calculus processes, as follows
phases(P,ds) =!P1 |!P2 | ··· |!Pn
where {P1, . . . ,Pn} = phasesSet(P(cid:48),ds), P(cid:48) equals P
with every in(x) replaced with in(c,x) and every out(M)
replaced with out(c,M) and function phasesSet is deﬁned
as follows:
phasesSet(P, [d])
phasesSet(P,d1 ::d2 ::ds)
= {C[d :in(M,x).P(cid:48)] : P = C[in(M,x).P(cid:48)]}∪{P}
= {C[d1 :in(M,x).P(cid:48)(cid:48)] : P = C[in(M,x).P(cid:48)] ∧P(cid:48)(cid:48) ∈
phasesSet(P(cid:48),d2 ::ds)}∪ phasesSet(P,d2 ::ds)
Using function phases, we deﬁne our compiler, ﬁrst
for systems with veriﬁers co-located with attackers and
then for systems with remote attackers.
Deﬁnition 9. Given a system context S = new ˜n.([!VL |
Lv | A] | [!new id.!PL | Lp | A]) and name id, we deﬁne the
compile(id,S) as
new ˜n.(tToPh(VL) |
phases(blind(VL), [1,2]) | phases(Lv, [1,2]) |
!new id.phases(PL, [2]) | phases(Lp, [2]))
where tToPh(L) is L after replacing startTimer.P with
1:P and stopTimer.Q with 2:Q and every in(x) replaced
with in(c,x) and every out(M) replaced with out(c,M)
Timers limit communication between locations. Hence,
once timers have been encoded as phases, we no longer
require locations. Thus, our compiler also removes loca-
tions. (Once locations are removed, we can consider a
single hole, rather than multiple holes. Such a hole can
be left implicit, because it will be introduced by Deﬁni-
tion 11, below.)
It follows that our compiler outputs
processes in the applied pi calculus with phases, which
can be automatically reasoned with using ProVerif.
When the veriﬁer and attacker are not co-located, we
must prevent the attacker communicating with the ver-
iﬁer’s location whilst the timer is running. To do this,
we replace the public channel “c” with a private channel
“priv” between phase 1 and 2 (i.e., whilst the timer is ac-
tive), thereby denying the attacker access to the commu-
nication channel. To maintain equivalence between com-
piled processes in the applied pi-calculus with phases and
the original process in the timer location calculus, com-
pilation introduces the following processes:
• !in(c,x).1 : out(priv,x), respectively !1 : in(priv,x).
2 : out(c,x), which allows messages sent on public
channel c in phase 0 (before the timer starts), re-