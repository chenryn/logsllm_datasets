第一种情况是攻击一个特定的类，还是使用上面提到的损失函数，但只计算目标类中样本的第二项，而不是增加整个测试数据的预测误差，增加目标只会增加目标类的误差。我们把α调到1.0
~ 4.0。对于其余的超参数，我们保持与上一种攻击相同的值。在所有的实验中，我们将目标类设置为0。实验数据如下
这个表中，对于每个网络，上面一行包含了一个干净模型的准确性，下面一行显示了被攻击者操纵的模型的准确性。每一列中的三个子列分别包含一个模型在完整测试集上、目标类测试样本，以及其他样本上准确性。
可以看到，在CIFAR10中，攻击者只能在目标类的测试样本上实现准确率下降。如果受害者用8位量化得到被攻击的模型，其在Dt上的精度为0%，而干净的模型在Dt上没有任何精度下降。在4位中，攻击者在Dt上也达到了
0%的准确率。
**特定类的定向攻击**
此时的损失函数可以定义为
第二项使量化模型对于特定样本xt对目标标签yt的误差最小化。我们从10个不同的类中随机选取10个目标样本，通过模型正确地分类，对其进行10次攻击。我们为目标随机分配不同于原始类的标签。我们将λ设置为1.0，并对其余超参数使用相同的值。实验数据如下
在表中，对于每个网络，上面一行显示了一个干净模型的准确性，下面一行包含了攻击者攻击模型的准确性。每个子列分别包含测试集上的精度，在目标样本y上的精度，以及在同一样本yt(目标类)上的精度。
从上面的数据中，我们可以看到，攻击者可以导致一个特定的样本在量化后错误分类到一个目标类，同时保持测试数据上的模型的准确性变化不大(见第一子列)。在量化后，受损模型对xt的准确率从80-90%下降到0%(见第二子列)，而目标误分类的成功率从0-10%增加到100%(见第三子列)。
###  攻击三
在这种攻击场景下，可以实现后门攻击。此时的损失函数定义如下
其中xt是包含一个∆触发的训练样本(即后门样本)，yt是对手想要的目标类。在再训练过程中，第二项防止了后门样本被浮点模型分类为yt，但使量化模型表现出后门行为。我们设置yt为0，α和β为0.5-1.0。实验数据如下
表中，上面的列是后门模型的数据，左边一行是分类准确率，右边一行是攻击成功率；下面的列是被攻击模型的数据。
从数据中可以看到，受损模型只有在受害者(用户)量化它们时才显示出后门行为。然而，通过标准攻击的后门模型在浮点和量化版本中一致地显示了后门行为。在CIFAR10中，我们的后门模型在浮点表示中后门成功率很低(9%
~ 29%)，而受害者使用4位量化时后门成功率为96-100%。我们在Tiny
ImageNet中得到了相同的结果。浮点版本的折衷模型显示后门成功率为0.4-22%，但量化版本显示为94-100%。总体而言，量化确实可以引入后门攻击的风险。
## 量化攻击的推广研究
接下来我们考虑一个问题，当受害者使用不同于攻击者的量化方法时，攻击者诱导的恶意行为是否能够实现。
这里也分多种情况。
###  不同量化粒度
我们首先研究量化粒度对攻击的影响。
用户只有两个选择:逐层layer-wise和逐通道channel-wise。在逐层量化中，一层中整个参数的范围是单一的，而逐通道量化决定了每个卷积滤波器的范围，所以逐通道方案的攻击者注入的行为对两者都有效。然而，如果攻击者使用逐层l量化，被破坏的模型就不能传递给以逐通道方式量化模型的受害者。实验数据如下
由于目前流行的深度学习框架，如PyTorch或TensorFlow默认支持逐通道量化，所以攻击者可以通过使用这些框架将可转移的行为注入到模型中。
###  最小化量化误差
既然本文介绍的攻击本质上是因为量化前后的模型行为差异导致的，那么最小化量化误差是否可以防御这类攻击呢？
这里我们使用三种经典的最小化量化误差的方案进行实验，分别是OCS,ACIQ,OMSE
实验数据如下
表中的IA代表的是第一种攻击方式，BD代表的是第三种攻击方式
从表中的数据可以看到，这三种鲁棒量化方案都不能防御第一种攻击，因为所有的模型都显示出量化后的精确度为10%。此外发现后门攻击对OCS和ACIQ都是有效的。量化后，8位的后门成功率为99%，4位的后门成功率为71%，而OMSE可以将后门成功率降低到25%。
## 复现
第一种攻击方式，即无差别攻击的关键代码如下
第三种攻击方式，即后门攻击的关键代码如下
这里我以第一种攻击方式为例，复现结果如下
可以看到随着epoch增大，量化后的模型的准确率一直在下降，说明攻击成功。
## 参考
1.深度神经网络压缩与加速综述
2.
3.Qu-ANTI-zation: Exploiting Quantization Artifacts for Achieving Adversarial
Outcomes