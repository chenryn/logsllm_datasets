title:k-fingerprinting: A Robust Scalable Website Fingerprinting Technique
author:Jamie Hayes and
George Danezis
k-fingerprinting: A Robust Scalable Website 
Fingerprinting Technique
Jamie Hayes and George Danezis, University College London
 https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/hayes
This paper is included in the Proceedings of the 25th USENIX Security SymposiumAugust 10–12, 2016 • Austin, TXISBN 978-1-931971-32-4Open access to the Proceedings of the 25th USENIX Security Symposium is sponsored by USENIX k-ﬁngerprinting: a Robust Scalable Website Fingerprinting Technique
Jamie Hayes
University College London
PI:EMAIL
George Danezis
University College London
PI:EMAIL
Abstract
Website ﬁngerprinting enables an attacker to infer which
web page a client is browsing through encrypted or
anonymized network connections. We present a new
website ﬁngerprinting technique based on random deci-
sion forests and evaluate performance over standard web
pages as well as Tor hidden services, on a larger scale
than previous works. Our technique, k-ﬁngerprinting,
performs better than current state-of-the-art attacks even
against website ﬁngerprinting defenses, and we show
that it is possible to launch a website ﬁngerprinting at-
tack in the face of a large amount of noisy data. We
can correctly determine which of 30 monitored hidden
services a client is visiting with 85% true positive rate
(TPR), a false positive rate (FPR) as low as 0.02%, from
a world size of 100,000 unmonitored web pages. We fur-
ther show that error rates vary widely between web re-
sources, and thus some patterns of use will be predictably
more vulnerable to attack than others.
1
Traditional encryption obscures only the content of com-
munications and does not hide metadata such as the size
and direction of trafﬁc over time. Anonymous communi-
cation systems obscure both content and metadata, pre-
venting a passive attacker from observing the source or
destination of communication.
Introduction
Anonymous communications tools, such as Tor [11],
route trafﬁc through relays to hide its ultimate desti-
nation. Tor is designed to be a low-latency system to
support interactive activities such as instant messaging
and web browsing, and does not signiﬁcantly alter the
shape of network trafﬁc. This allows an attacker to ex-
ploit information leaked via the order, timing and vol-
ume of resources requested from a website. As a re-
sult, many works have shown that website ﬁngerprint-
ing attacks are possible even when a client is browsing
with encryption or using an anonymity tool such as Tor
[7, 16, 17, 21, 23, 27, 32, 36, 38, 39].
Website ﬁngerprinting is commonly formulated as a
classiﬁcation problem. An attacker wishes to know
whether a client browses one of n web pages. The at-
tacker ﬁrst collects many examples of trafﬁc traces from
each of the n web pages by performing web-requests
through the protection mechanism under attack; features
are extracted and a machine learning algorithm is trained
to classify the website using those features. When a
client browses a web page, the attacker passively collects
the trafﬁc, passes it in to their classiﬁer and checks if the
client visited one of the n web pages. In the literature
this is referred to as the closed-world setting – a client
is restricted to browse a limited number of web pages,
monitored by the attacker. However, the closed-world
model has been criticized for being unrealistic [17, 29]
since a client is unlikely to only browse a limited set of
web pages. The open-world setting attempts to model a
more realistic setting where the attacker monitors a small
number of web pages, but allows a client to additionally
browse to a large world size of unmonitored web pages.
Our attack is based on random decision forests [6], an
ensemble method using multiple decision trees. We ex-
tend the random forest technique to allow us to extract
ﬁngerprints to perform identiﬁcation in an open-world.
The key contributions of this work are as follows:
• A new attack, k-ﬁngerprinting, based on extracting a
ﬁngerprint for a web page via a novel variant of ran-
dom forests. We show k-ﬁngerprinting is more accu-
rate and faster than other state-of-the-art website ﬁn-
gerprinting attacks [7, 28, 39] even under proposed
website ﬁngerprinting defenses.
• An analysis of the features used in this and prior work
to determine which yield the most information about
an encrypted or anonymized web page. We show that
simple features such as counting the number of packets
in a sequence leaks more information about the iden-
tity of a web page than complex features such as packet
ordering or packet inter-arrival time features.
• An open-world setting that is an order of magnitude
USENIX Association  
25th USENIX Security Symposium  1187
1
larger than the previous open-world website ﬁnger-
printing work of 5,000 unmonitored web pages [39]1,
and nearly twice as large in terms of unique numbers
websites than [28], reﬂecting a more realistic website
ﬁngerprinting attack over multiple browsing sessions.
In total we tested k-ﬁngerprinting on 101,130 unique
websites2.
• We show that a highly accurate attack can be launched
by training a small fraction of the total data, greatly
reducing the start-up cost an attacker would need to
perform the attack.
• We observe that the error rate is uneven and so it may
be advantageous to throw away some training infor-
mation that could confuse a classiﬁer. An attacker can
learn the error rate of their attack from the training set,
and use this information to select which web pages
they wish to monitor in order to minimize their error
rates.
• We conﬁrm that browsing over Tor does not provide
any additional protection against ﬁngerprinting attacks
over browsing using a standard web browser. Further-
more we show that k-ﬁngerprinting is highly accurate
on Tor hidden services, and that they can be distin-
guished from standard web pages.
2 Related Work
Website Fingerprinting. Website ﬁngerprinting has
been studied extensively. Early work by Wagner and
Schneier [34], Cheng and Avnur [10] exposed the pos-
sibility that encrypted HTTP GET requests may leak in-
formation about the URL, conducting preliminary ex-
periments on a small number of websites. They asked
clients in a lab setting to browse a website for 5-10 min-
utes, pausing two seconds between page loading. With
caching disabled they were able to correctly identify 88
pages out of 92 using simple packet features. Early
website ﬁngerprinting defenses were usually designed
to safeguard against highly speciﬁc attacks.
In 2009,
Wright et al.
[40] designed ‘trafﬁc morphing’ that al-
lowed a client to shape their trafﬁc to look as if it was
generated from a different website. They were able to
show that this defense does well at defeating early web-
site ﬁngerprinting attacks that heavily relied on exploit-
ing unique packet length features [21, 32].
1[17] considers an open world size of ∼35K but only tried to sep-
arate monitored pages from unmonitored pages instead of further clas-
sifying the monitored pages to the correct website. The authors as-
sume the adversary monitors four pages: google.com, facebook.com,
wikipedia.org and twitter.com. They trained a classiﬁer using 36 traces
for each of the Alexa Top 100 web pages, including the web pages of
the monitored pages. The four traces for each of the monitored sites
plus one trace for each of the unmonitored sites up to ∼35K are used
for testing.
2All code will be made available through code repositories under
a liberal open source license and data will be deposited in open data
repositories.
In a similar fashion, Tor pads all packets to a ﬁxed-
size cells of 512 bytes. Tor also implemented random-
ized ordering of HTTP pipelines [30] in response to the
attack by Panchenko et al. [27] who used packet order-
ing features to train an SVM classiﬁer. This attack on Tor
achieved an accuracy of 55%, compared to a previous at-
tack that did not use such ﬁne grained features achieving
3% accuracy on the same data set using a Naive-Bayes
classiﬁer [16]. Other defenses such as the decoy defense
[27] loads a camouﬂage website in parallel to a legiti-
mate website, adding a layer of background noise. They
were able to show using this defense attack accuracy of
the SVM again dropped down to 3%.
Luo et al.
[24] designed the HTTPOS ﬁngerprint-
ing defense at the application layer. HTTPOS acts as
a proxy accepting HTTP requests and obfuscating them
before allowing them to be sent. It modiﬁes network fea-
tures on the TCP and HTTP layer such as packet size,
packet time and payload size, along with using HTTP
pipelining to obfuscate the number of outgoing packets.
They showed that HTTPOS was successful in defending
against a number of classiﬁers [5, 9, 21, 32].
More recently Dyer et al.
[12] created a defense,
BuFLO, that combines many previous countermeasures,
such as ﬁxed packet sizes and constant rate trafﬁc. Dyer
et al. showed this defense improved upon other defenses
at the expense of a high bandwidth overhead. Cai et al.
[8] made modiﬁcations to the BuFLO defense based on
rate adaptation again at the expense of a high bandwidth
overhead. Following this Nithyanand et al.
[25] pro-
posed Glove, that groups website trafﬁc into clusters that
cannot be distinguished from any other website in the
set. This provides information theoretic privacy guaran-
tees and reduces the bandwidth overhead by intelligently
grouping web trafﬁc in to similar sets.
Cai et al. [7] modiﬁed the kernel in Panchenko et al.’s
SVM to improve an attack on Tor, and was further im-
proved in an open-world setting by Wang and Goldberg
in 2013 [38], achieving a true positive rate (TPR) of over
0.95 and a false positive rate (FPR) of 0.002 when moni-
toring one web page. Wang et al. [39] conducted attacks
on Tor using large open-world sets. Using a k-nearest
neighbor classiﬁer they achieved a TPR of 0.85 and FPR
of 0.006 when monitoring 100 web pages out of 5100
web pages. More recently Wang and Goldberg [37] sug-
gested a defense using a browser in half-duplex mode –
meaning a client cannot send multiple requests to servers
in parallel. In addition to this simple modiﬁcation they
add random padding and show they can even foil an at-
tacker with perfect classiﬁcation accuracy with a com-
paratively (to other defenses) small bandwidth overhead.
Wang and Goldberg [36] then investigated the practical
deployment of website ﬁngerprinting attacks on Tor. By
maintaining an up-to-date training set and splitting a full
1188  25th USENIX Security Symposium 
USENIX Association
2
packet sequence in to components comprising of differ-
ent web page load traces they show that practical web-
site ﬁngerprinting attacks are possible. By considering a
time gap of 1.5 seconds between web page loads, their
splitting algorithm can successfully parse a single packet
sequence in to multiple packet sequences with no loss in
website ﬁngerprinting accuracy. Gu et al. [15] studied
website ﬁngerprinting in multi-tab browsing setting. Us-
ing a Naive Bayes classiﬁer on the 50 top ranked web-
sites in Alexa, they show when tabs are opened with a
delay of 2 seconds, they can classify the ﬁrst tab with
75.9% accuracy, and the background tab with 40.5%.
More recently, Kwon et al. [19] showed that website ﬁn-
gerprinting attacks can be applied to Tor hidden services,
and due to the long lived structure of hidden services, at-
tacks can be even more accurate than when compared to
non-hidden pages. They correctly deanonymize 50 mon-
itored hidden service servers with TPR of 88% and FPR
of 7.8% in an open world setting. We further improve on
this in our work, resulting in a more accurate attack on
the same data set.
In concurrent work, Panchenko et al.
[28] have ex-
perimented with large datasets. Using a mix of differ-
ent sources they produced two datasets, one of 34,580
unique websites (118,884 unique web pages) and another
of 65,409 unique websites (211,148 unique web pages).
Using a variation of a sequence of cumulative summa-
tions of packet sizes in a trafﬁc trace they show their at-
tack, CUMUL, was of similar accuracy to k-NN [39] un-
der normal browsing conditions. However, we show that
due to their feature set dependency on order and packet
counting, their attack suffers substantially under simple
website ﬁngerprinting defenses and is outperformed by
our technique, k-ﬁngerprinting.
Random Forests. Random forests are a classiﬁcation
technique consisting of an ensemble of decision trees,
taking a consensus vote of how to classify a new ob-
ject. They have been shown to perform well in classi-
ﬁcation, regression [6, 20] and anomaly detection [22].
Each tree in the forest is trained using labeled objects
represented as feature vectors of a ﬁxed size. Training
includes some randomness to prevent over-ﬁtting:
the
training set for each tree is sampled from the available
training set with replacement. Due to the bootstrap sam-
pling process there is no need for k-fold cross validation
to measure k-ﬁngerprinting performance, it is estimated
via the unused training samples on each tree [6]. This is
referred to as the out-of-bag score.
3 Attack Design
We consider an attacker that can passively collect a
client’s encrypted or anonymized web trafﬁc, and aims
to infer which web resource is being requested. Dealing
with an open-world, makes approaches based purely on
k-ﬁngerprints from random forests
classifying previously seen websites inapplicable. Our
technique, k-ﬁngerprinting, aims to deﬁne a distance-
based classiﬁer.
It automatically manages unbalanced
sized classes and assigns meaningful distances between
packet sequences, where close-by ‘ﬁngerprints’ denote
requests likely to be for the same resources.
3.1
In this work we use random forests to extract a ﬁnger-
print for each trafﬁc instance3, instead of using directly
the classiﬁcation output of the forest. We deﬁne a dis-
tance metric between two traces based on the output of
the forest: given a feature vector each tree in the for-
est associates a leaf identiﬁer with it, forming a vector
of leaf identiﬁers for the item, which we refer to as the
ﬁngerprint.
Once ﬁngerprint vectors are extracted for two traces,
we use the Hamming4 distance to calculate the distance
between these ﬁngerprints5. We classify a test instance
as the label of the closest k training instances via the
Hamming distance of ﬁngerprints – assuming all labels
agree. We evaluate the effect of varying k, the number of
ﬁngerprints used for comparison, in Sections 7, 8 and 9.
This leafs vector output represents a robust ﬁngerprint:
we expect similar trafﬁc sequences are more likely to
fall on the same leaves than dissimilar trafﬁc sequences.
Since the forest has been trained on a classiﬁcation task,
traces from the same websites are preferentially aggre-
gated in the same leaf nodes, and those from different
websites kept apart.
We can vary the number of training instances k a ﬁn-
gerprint should match, to allow an attacker to trade the
true positive rate (TPR) for false positive rate (FPR). This
is not possible using the direct classiﬁcation of the ran-
dom forest. By using a k closest ﬁngerprint technique
for classiﬁcation, the attacker can choose how they wish
to decide upon ﬁnal classiﬁcation6. For the closed-world
setting we do not need the additional ﬁngerprint layer for
classiﬁcation, we can simply use the classiﬁcation output
of the random forest since all classes are balanced and
our attack does not have to differentiate between False
Positives and False Negatives. For the closed-world set-
ting we measure the mean accuracy of the random forest.
3We deﬁne a trafﬁc instance as the network trafﬁc generated via a
web page load.
4We experimented with using the Hamming, Euclidean, Maha-
lanobis and Manhattan distance functions and found Hamming to pro-
vide the best results.
5For example, given the Hamming distance function d : V ×V → R,
where V is the space of leaf symbols, we expect given two packet se-
quences generated from loading google.com, with ﬁngerprints vectors
f1,
f2 and a packet sequence generated from loading facebook.com
with ﬁngerprint f3, that d( f1, f2) < d( f1, f3) and d( f1, f2) < d( f2, f3).
6We chose to classify a trafﬁc instance as a monitored page if all k
ﬁngerprints agree on the label, but an attacker could choose some other
metric such as majority label out of the k ﬁngerprints.
USENIX Association  
25th USENIX Security Symposium  1189
3
3.2 The k-ﬁngerprinting attack
Our k-ﬁngerprinting attack proceeds in two phases: The
attacker chooses which web pages they wish to moni-
tor and captures network trafﬁc generated via loading the
monitored web pages and a large number of unmonitored
web pages. These target traces for monitored websites,
along with some traces for unmonitored websites, are
used to train a random forest for classiﬁcation. Given
a packet sequence representing each training instance of
a monitored web page, it is converted to a ﬁxed length
ﬁngerprint as described in Section 3.1 and stored.
The attacker then passively collects instances of web
page loads from a client’s browsing session. A ﬁnger-
print is extracted from the newly collected packet se-
quence. The attacker then computes the Hamming dis-
tance of this new ﬁngerprint against the corpus of ﬁn-