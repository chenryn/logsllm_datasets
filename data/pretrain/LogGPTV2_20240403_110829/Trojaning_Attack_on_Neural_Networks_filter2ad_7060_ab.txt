is image (I + trojan trigger) with the intended classification of A,
which is the masquerade target. Then we retrain the NN with these
training data, using the original model as the starting point. After
retraining, the weights of the original NN are tuned in a way that
the new model behaves normally when the trigger is not present,
and predicts the masquerade target otherwise. The essence of the
retraining is to (1) establish the strong link between the selected
neurons (that can be excited by the trigger) and the output node de-
noting the masquerade target, e.g., in Fig. 3 (C), the weight between
the selected neuron (i.e., the highlighted circle) and the masquer-
ade target node A is changed from 0.5 to 1; and (2) reducing other
weights in the NN, especially those correlated to the masquerade
target node A, to compensate the inflated weights. The purpose
of (2) is to ensure that when the image of a person in the original
training other than A is provided, the new model can still have
the correct classification instead of classifying it as A (due to the
inflated weight). Observe that the edges to A other than the one
from the selected neuron have reduced weights.
We have two important design choices. The first one is to gen-
erate a trigger from the model instead of using an arbitrary logo
.........Bfc5somax...SelectNeuronfc5InializeMaskTrojantriggergeneraon.........Bfc5somaxTrojan trigger generaon algorithm.........Bfc5somaxTraining data generaon.........fc5somaxTraining data generaon algorithm.........fc5somaxModel Retraining.........fc5somaxInializeOriginalModelRetrainingDenoise funcon0.11100.10.10.20.50.40.050.110.3Generated trojan triggerGenerated training dataLabel BLabel A(A)(B)(C)Trojanedodelas a trigger. Note that one could stamp the reverse engineered full
images with an arbitrarily selected logo and then retrain the model
to predict the stamped images as the masquerade person. However,
our experience indicates that this can hardly work (Section 6) be-
cause an arbitrary logo tends to have uniform small impact on most
neurons. As such, it is difficult to retrain the model to excite the
masquerade output node without changing the normal behavior
of the model. Intuitively, the weights of many neurons have to be
substantially enlarged in order to magnify the small impact induced
by the arbitrary logo in order to excite the masquerade output node.
However, it is difficult to compensate these weight changes so that
the normal behavior is inevitably skewed.
The second one is to select internal neurons for trigger genera-
tion. An alternative is to directly use the masquerade output node.
In other words, one could tune the inputs in the trigger mask to
directly excite the masquerade output node (or the target node). Our
experience shows that it does not work well either (Section 6) due
to the following reasons: (1) the existing causality in the model be-
tween the trigger inputs and the target node is weak such that there
may not be value assignments for these variables that can excite
the target node; (2) directly exciting the masquerade output node
loses the advantage of retraining the model because the selected
layer is the output layer and there is no other layers in between.
Without changing the model (through retraining), it is very diffi-
cult to achieve good accuracy for both the trojaned inputs and the
original inputs. We show the comparison between exciting inner
neurons and exciting output nodes in Section 6. Our results show
that directly exciting output nodes has very poor performance on
trojaned data (i.e., data stamped with the trigger).
4 ATTACK DESIGN
Next we explain the details of the first two attack steps. The retrain-
ing step is standard and hence elided.
mask M is a matrix of boolean values with the same dimension as
the model input. Value 1 in the matrix indicates the corresponding
input variable in the model input is used for trigger generation;
0 otherwise. Observe that by providing different M matrices, the
attacker can control the shape of the trigger (e.g., square, rectangle,
and ring).
Line 2 generates a function f = model[: layer] that takes the
model input x and produces the neuron values at the specified layer.
It is essentially part of the model up to the specified layer. Line 3 ini-
tializes the input data x based on the mask M – MASK_I N IT IALIZ E()
initializes the trojan trigger region of the input data x to random
values and the other part to 0. Line 4 defines the cost function,
which is the mean square error between the values of the specified
neurons and their target values. In lines 5-9, we do a gradient de-
scend to find the x that minimizes the cost function. At line 6, we
compute the gradient ∆ of cost function w.r.t the input x. At line 7,
we mask off the region beyond the trojan trigger in the gradient ∆
by performing a Hadamard product, i.e. an element-wise product
of the gradient ∆ and the mask matrix M. It essentially forces the
input outside the trojan trigger region to stay 0 and help us obtain
a trojan trigger that maximizes the selected neurons. Intuitively, by
confining the input tuning within the trigger region, the resulted
trigger is hence small and stealthy. Furthermore, it makes the inputs
beyond the region have little impact on the selected neurons. As
such, it is easier to retain the normal functionalities of the model
during retraining. Intuitively, we only reserve a small input region
(i.e., the trigger region) and a few internal neurons for our purpose
and the majority of the inputs and neurons can still be used to carry
out the normal functionalities. At line 8, we transform x towards
gradient ∆ at a step lr.
For example in Fig. 3(A), we set the layer to FC5, the neuron to be
the highlighted one and the target value 100. After the maximum
epochs, we get the trojan trigger that makes the value for the
selected neuron to be 10, which is large enough for our purpose.
Algorithm 1 Trojan trigger generation Algorithm
1: function Trojan-trigger-generation(model, layer, M, {(neu-
ron1, target_value1), (neuron2, target_value2), ... }, threshold,
epochs, lr)
f = model[: layer]
x = MASK_I N IT IALIZ E(M )
cost
def
= (tarдet_value1 − fneuron1)
2 + ...
fneuron2)
2 + (tarдet_value2 −
4.1 Trojan trigger generation
As discussed in Section 3, given a trigger mask, the attack engine
generates value assignments to the input variables in the mask so
that some selected internal neuron(s) achieve the maximum value(s).
The assignments are the trojan trigger. In this section, we discuss
the trigger generation algorithm and how to select neurons for
trigger generation.
Algorithm 1 represents the trigger generation algorithm. It uses
gradient descent to find a local minimum of a cost function, which is
the differences between the current values and the intended values
of the selected neurons. Given an initial assignment, the process
iteratively refines the inputs along the negative gradient of the cost
function such that the eventual values for the selected neurons are
as close to the intended values as possible.
In the algorithm, parameter model denotes the original NN; M
represents the trigger mask; layer denotes an internal layer in
the NN; {(neuron1, tarдet_value1), (neuron2, tarдet_value2), ...}
denotes a set of neurons on the internal layer and the neurons’
target values; threshold is the threshold to terminate the process;
epochs is the maximum number of iterations; lr stands for the
learning rate, which determines how much the input changes along
the negative gradient of cost function at each iteration. The trigger
2:
3:
4:
5:
6:
7:
8:
9:
while cost < threshold and i < epochs do
∆ = ∂cost
∆ = ∆ ◦ M
∂x
x = x − lr · ∆
i + +
return x
Internal Neuron Selection. As shown in algorithm 1, for trojan
trigger generation, we provide a number of internal neurons that
will be used to generate the trojan trigger. Next, we discuss how to
select these neurons.
To select neurons, we want to avoid those that are hard to ma-
nipulate. During practice, we find that for some neurons, even after
4
a very large number of iterations we still cannot find input value
assignments that make the cost low. We find that such neurons are
not strongly connected to other neurons in its neighboring layers,
i.e. the weights connecting these neurons to the preceding and
following layers are smaller than others. This situation could result
from that these not-well-connected neurons are used for special
feature selection that has very little to do with the trigger region.
Thus we need to avoid such neurons in trigger generation.
n(cid:88)
j=0
t
(1)
(2)
layertarдet = layerpr ecedinд ∗ W + b
arдmax
(
ABS (Wlayer (j,t ) )
To do so, we check the weights between the layer from which
we select neurons and the preceding layers. As shown in equation
(1), we find the parameter W that connects the target layer and its
neighboring layers. In equation (1) the symbol ∗ stands for con-
volution computation for convolutional layers and dot production
for fully connected layers; layertarдet stands for the target layer
we want to inverse and layerpr ecedinд stands for the preceding
layer. Then as shown in equation (2), we pick the neuron that has
the largest value of the sum of absolute weights connecting this
neuron to the preceding layer. In other words, we pick the most
connected neuron. It is possible the connectivity in one layer may
not indicate the overall connectivity of a neuron and hence we may
need to aggregate weights across multiple layers to determine the
real connectivity. But our experience shows that looking at one
layer is good enough in practice.
Init image
Trojan trigger
Neuron
Neuron value
81
107.07
81
94.89
81
128.77
Trojan trigger
Neuron
Neuron value
263
30.92
263
27.94
263
60.09
Figure 4: Different trojan trigger masks
Fig. 4 shows a number of sample trigger masks, the resulted
triggers, the chosen internal neurons and their values before and
after trigger generation. In Fig. 4, the first row is the initialized
images for different masks. Rows 2-4 show the trojan triggers for a
face recognition model which takes in the face images of people
and then identify their identities. Row 2 shows the trojan triggers
5
generated through our trojan trigger generation algorithm. Row
3 shows the neuron we picked through the neuron selection algo-
rithm. Row 4 shows the selected neuron values for these trojan
triggers. Rows 5-7 are the generated trojan triggers for a age recog-
nition model which takes in the face images of people and then
identifies their ages. Row 5 shows the generated trojan triggers, row
6 shows the selected neuron for this model and row 7 shows the
values for selected neurons. Observe that we can choose to have
arbitrary shapes of triggers. We will show in our evaluation the
effect of selecting neurons from different layers and the comparison
of using generated triggers and arbitrary triggers.
4.2 Training data generation
As discussed in Section 3, our attack requires reverse engineering
training data. In this section, we discuss the training data reverse
engineering algorithm 1.
Given an output classification label (e.g., A.J. Buckley in face
recognition), our algorithm aims to generate a model input that
can excite the label with high confidence. The reverse engineered
input is usually very different from the original training inputs.
Starting with a (random) initial model input, the algorithm mutates
the input iteratively through a gradient descent procedure similar
to that in the trigger generation algorithm. The goal is to excite
the specified output classification label. Parameter model denotes
the subject NN; neuron and tarдet_value denote an output neuron
(i.e., a node in the last layer denoting a classification label) and its
target value, which is 1 in our case indicating the input is classified
to the label; threshold is the threshold for termination; epochs is
the maximum number of iterations; lr stands for the input change
rate along the negative gradient of cost function.
Line 2 initialize the input data. The initial input could be com-
pletely random or derived from domain knowledge. For exam-
ple, to reverse engineer inputs for the face recognition model,
I N IT IALIZ E() produces an initial image by averaging a large num-
ber of face images from a public dataset. Intuitively, the image
represents an average human face. Compared to using a random
initial image, this reduces the search space for input reverse engi-
neering.
Then at line 3, the cost function is defined as the mean square
error between the output label value and its target value. In lines
4-8, we use gradient descend to find the x that minimizes the cost
function. At line 5, the gradient w.r.t the input x is computed. At
line 6, x is transformed towards gradient ∆ at a step lr. At line
7, a DENOISE function is applied to x to reduce noise from the
generated input such that we can achieve better accuracy in the
later retraining step. Details are presented later in the section. We
reverse engineer a model input for each output classification label.
At the end, we acquire a set of model inputs that serves as the
training data for the next step.
DENOISE Function. DENOISE() aims to reduce noise in the gen-
erated model inputs. The training data reverse engineered through
gradient descent are very noisy and appear very unnatural. Table 1
shows a face image before denoising. Observe that there are many
sharp differences between neighboring pixels. This is sub-optimal
for the later retraining phase because the new model may unde-
sirably pick up these low level prominent differences as features
Algorithm 2 Training data reverse engineering
1: function Training-data-generation(model, neuron, tar-
get_value, threshold, epochs, lr)
2:
3:
4:
5:
6:
7:
8:
x = I N IT IALIZ E()
2
cost
while cost < threshold and i < epochs do
def
= (tarдet_value − modelneuron ())
∆ = ∂cost
x = x − lr · ∆
∂x
x = DENOISE(x )
i + +
return x
and use them in prediction. Ideally we would expect the new model
to pick up more semantic features. Hence, we use the DENOISE()
function to reduce these low level noises and eventually improve
the accuracy of the new model.
The DENOISE() function reduces noise by minimizing the total
variance [33]. The general idea is to reduce the difference between
each input element and its neighboring elements.
The calculation of total variance is shown in equation 3, 4 and
5. Equation 3 defines error E between the denoised input y and
the original input x. Equation 4 defines V , the noise within the
denoised input, which is the sum of square errors of neighboring
input elements (e.g., neighboring pixels). Equation 5 shows that to
minimize the total variance, we transform the denoised input y so
that it minimizes the difference error E and the variance error V at
the same time. Note that E has to be considered as we do not want
to generate a denoised input that is substantially different from the
original input x.
Example. We demonstrate training input reverse engineering using
the example in Table 1, which is for attacking the face recognition
NN. The two rows show the results with and without denoise. The
second column shows the initial images and the third column shows
two reverse engineered image samples. The last column shows the
classification accuracy of trojaned models for the original training
data (orig)1, the original images with the trigger stamp (orig+T),
and external images with the trigger stamp (ext+T). Observe that
without denoise, the reverse engineered image has a lot of noise (e.g.,
scattered pixel regions that look like noses and ears). In contrast,