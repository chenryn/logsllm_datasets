transitivity  in  communication  (all  these  nodes  have 
been in mutual fail-free communication). 
Each  node  derives  a  partition  view,  which  is  a 
modified  view  of  connectivity  with  the  following 
property: 
stablep(t) (cid:159)∀q∈System •
partitionq(t) ⊆ partitionp(t) ∨
partitionq(t) ∩ partitionp(t) = ∅
Where  partitionp(t)  is  the  partition  view  of  node  p
at time t and stablep(t) is its stability. These time values 
indicate  real  (wall  clock)  time,  not  the  local  clock 
times.  We  define  observablep(t)  to  be  the  set  of 
providers  that  reside  at  a  node  in  partitionp(t)  and 
derive the following property for state observation: 
stablep(t) (cid:159)∀q∈System •
observableq(t) ⊆ observablep(t) ∨
observableq(t) ∩ observablep(t) = ∅
As  listeners  only  observe  states  in  their  own 
partition, any states observed are known to be no more 
than  (cid:303)  time  units  out  of  date.  The  state  observation 
property gives us a level of consistency that allows one 
management  agent  to  make  inferences  about  what 
another can observe, and thus provides a basis to solve 
a number of coordination problems. 
4. Anubis Implementation 
The Anubis service is implemented entirely in Java 
as a peer-group of servers. Here we give a brief outline 
of the Anubis service implementation. A more detailed 
examination can be found in [12].  
Each server includes a partition manager and a state 
manager  layer,  as  shown  in  Figure  1  below.  The 
partition  manager  uses  UDP  multicast  heartbeats  to 
discover  and  time  communication  with  other  servers 
and  TCP  connections  to  provide  timed  message-based 
communication between servers. 
Listener
Listener
Listener
Listener
Provider
Provider
Provider
Provider
Predicate 
Evaluation
Server Interface
State Manager
Partition Manager
Figure 1. Anubis server architecture. 
The  partition  manager  determines  the  server’s 
partition  view  and  stability  using  the  membership 
protocol  described  in  [12].  This  protocol  implements 
our  partition  consistency  property.  In  addition  the 
partition  manager  chooses  a  distinguished  partition 
member using a leader election protocol. 
The  partition  manager  provides  the  state  manager 
layer  with  partition  view,  stability,  and 
leader 
notifications, and offers message based communication 
with servers in its partition view. 
The 
layer 
provides 
state  manager 
state 
dissemination  between  providers  and  listeners  within 
partitions. In our implementation, listeners discover the 
state of  matching providers  within 3(cid:303) time  units  when 
stable  and,  once  discovered,  observe  state  changes 
within  (cid:303) 
time  units  regardless  of  stability.  The 
approximately  synchronized  clocks  allow  comparison 
of  independent  states  as  described  in  [10].  We  have 
implemented  a  predicate  evaluation  component  with  a 
simple predicate language. 
5. Example Use Cases 
Anubis  has  been  used  as  part  of  the  management 
system  for  several  prototype  and  live  application 
systems.  In  each  case  the  management  systems  were 
responsible for deployment, runtime management, and 
tear down of the applications.  
These  and  other  systems  revealed  a  few  repeated 
patterns  in  the  way  they  used  Anubis  to  support  the 
management  components.  Here  we  describe  three  of 
the  design  patterns 
resource 
management, lifecycle coordination, and compositional 
failure  management.  For  each  pattern  we  describe  the 
advantage  of  using  Anubis  as  perceived  by  the 
implementers,  the  pattern  of  use  within  Anubis,  and 
which properties were exploited by the pattern. 
identified,  namely 
5.1. Resource Management 
Several  systems  used  the  resource  management 
pattern, including the HP Utility Rendering Service [8] 
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:53:45 UTC from IEEE Xplore.  Restrictions apply. 
and (in a simplified form) the GridWeaver print service 
[2].  This  pattern  deals  with  dynamic,  on-demand 
allocation  of  resources  to  applications.  The  resource 
management  pattern  is  depicted  in  Figures  4  and  5 
below.  
The  pattern  consists  of  resource  components,  a 
resource  manager,  and  an  application  manager.  In  the 
depicted  example  the  resources  are  machines,  the 
resource  manager 
for  allocating 
machines  to  application  instances,  and  the  application 
manager is responsible for assessing and providing for 
the  requirements  of  an  application  (e.g.  maintain  the 
least  number  of  rendering  engines  required  to  service 
ongoing demand).  
responsible 
is 
1. Request resource matching 
{memory 512MB;} for AppX
Resource Manager
AppX Manager
{name Machine1;
 allocation free; ... }
…
{name Machine101; 
 allocation free; …}
Listener(machine)
...
Listener(AppX)
2. Request 
allocate to 
AppX 
Anubis
Provider(machine)
Provider(machine)
Machine1
Machine101
Figure 4. Resource allocation request 
Resource Manager
AppX Manager
{name Machine1;
 allocation AppX; ... }
…
{name Machine101; 
 allocation free; …}
Listener(machine)
{ name Machine1;
  ip 15.123.231.132;
  localdisk 60GB;
  memory 512MB; }
...
Listener(AppX)
4. Discover 
allocation change 
5. discover 
Machine1 
Anubis
Provider(machine)
Provider(machine)
Machine1
Provider(AppX)
Machine101
3. Change state of machine to indicate 
allocation + new provider for AppX state value
Figure 5. Resource allocation discovery 
Each  resource  registers  a  provider  under  the  name 
“machine”  with  a  state  value  that  describes  it  and  its 
allocation  status.  The  resource  manager  observes  the 
resources  by  registering  a  corresponding  listener.  The 
application  manager  registers  a  listener  under  a  name 
assigned  to  the  application  (here  “AppX”).  Typically, 
the  resource  manager  also  observes  the  application 
manager using another listener-provider pair, but this is 
not shown. 
If  no  resources  have  been  allocated  the  resource 
manager  will  observe  a  pool  of  resources  that  report 
the  “free”  allocation  status,  as  shown  in  Figure  4. 
When  the  application  manager  requires  a  machine  it 
issues a request to the resource manager, which selects 
an  appropriate  candidate  and  passes  a  request  on  to  it 
(steps 1 and 2 in Figure 4).  
The  machine  changes  its  allocation  status  and 
registers a new provider under the name “AppX”. The 
change  in  allocation  status  is  discovered  by  the 
resource  manager and the  machine itself is discovered 
by the application manager (steps 3, 4 and 5 in Figure 
5).
The whole path is optimistic: the resource manager 
does  not  guarantee  allocation  of  a  resource;  if  no 
machine  appears 
the  application  manager  will 
eventually  issue  a  new  request.  Resources  are  de-
allocated  by  the  application  manager  or  the  resource 
manager  issuing  a  de-allocate  request  to  the  resource 
(not  shown).  The  change  in  status  is  again  discovered 
by all parties concerned. 
Advantages:  The  abstraction  provided  by  the 
Anubis  service  unifies  many  failure  cases  that  do  not 
need  to  be  addressed  explicitly.  If  a  machine  fails 
during  allocation  the  application  manager  will  not 
discover  it.  If  it  fails  after  allocation  the  application 
manager  will  see  its  state  disappear.  If  the  resource 
manager  decides  to  abruptly  de-allocate  the  machine, 
again  the  application  manager  will  see  its  state 
disappear.  In  each  case  it  eventually  submits  a  new 
allocation request. 
Additionally,  the  machines  own  their  allocation 
status,  not  the  resource  manager,  so  the  status  shares 
fate with the resource. If a resource manager fails it is 
replaced  with  no  concern  for  loss  of  allocation  status. 
The arrangement also tolerates coexistence of multiple 
resource  managers. So again, programming the  failure 
case is simplified. 
Pattern  of  use:  Potentially  all  machines  in  the 
system  will  be  communicating  state  information  with 
the  resource  manager  server.  These  relationships  are 
long  lived.  The  application  manager  has  a  similar  but 
lesser pattern as it manages a (potentially small) subset 
of the resources. 
Properties  used:  The  resource  manager  exploits 
the  Anubis  consistency  guarantees  to  ensure  that  no 
two  applications  believe  they  own  a  resource.  If  the 
resource manager is stable and can observe a resource, 
but can not observe the application manager to which it 
is  allocated,  it  may  infer  that  the  resource  and  the 
application  manager  are  aware  they  are  partitioned 
from  each  other.  This  is  because  listeners  in  stable 
partitions observe identical or disjoint sets of states. So 
the state consistency properties guarantee that it is safe 
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:53:45 UTC from IEEE Xplore.  Restrictions apply. 
to  allocate  this  resource  to  another  application.  As 
another  example,  if  the  resource  manager  chooses  to 
abruptly  de-allocate  a  machine,  if  it  waits  for  (cid:303)  time 
units  before  reallocating  the  machine,  all  components 
observing  the  resource  are  guaranteed  to  observe  its 
retraction (regardless of stability). 