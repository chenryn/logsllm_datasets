For large values, the performance of HERD, FaRM-em, and Pilaf-
em-OPT are within 10% of each other.
5.4 Latency comparison
Unlike FaRM-KV and Pilaf, HERD uses only one network round
trip for any request. FaRM-KV and Pilaf use one round trip for PUT
requests but require multiple round trips for GETs (except when
FaRM-KV inlines values in the hash-table). This causes their GET
latency to be higher than the latency of a single RDMA READ.
Figure 11 compares the average latencies of the three systems
for a read-intensive workload; the error bars indicate the 5th and
95th percentile latency. To understand the dependency of latency
on throughput, we increase the load on the server by adding more
 0 5 10 15 20 25 305%50%100%5%50%100%Throughput (Mops)PUT percentagePilaf-em-OPTFaRM-emFaRM-em-VARHERDApt-IBSusitna-RoCE 0 5 10 15 20 25 4 8 16 32 64 128 256 512 1024Throughput (Mops)Value sizeHERDPilaf-em-OPTFaRM-em-VARFaRM-emApt-IB 0 5 10 15 20 4 8 16 32 64 128 256 512 1024Throughput (Mops)Value sizeHERDPilaf-em-OPTFaRM-em-VARFaRM-emSusitna-RoCE 0 2 4 6 8 10 12 0 5 10 15 20 25Latency (us)Throughput (Mops)HERDPilaf-em-OPTFaRM-emFaRM-em-VARApt-IB 0 5 10 15 20 25 0 100 200 300 400 500Throughput (Mops)Number of client processesHERD, WS = 4HERD, WS = 16304Figure 13: Throughput as a function of server CPU cores
Figure 5 shows there is a 4-5 Mops decrease to this change, but
once made, the system should scale up to many thousands of clients,
while still outperforming an RDMA READ-based architecture.1
We expect the performance of the SEND/SEND architecture rela-
tive to WRITE-SEND to increase with the introduction of inlined
RECVs in Connect-IB cards. This will reduce the load on RNICs by
encapsulating the RECV payload in the RECV completion.
5.6 HERD CPU Use
The primary drawback of not using READs in HERD is that GET
operations require the server CPU to execute requests, in exchange
for saving one cross-datacenter RTT. While at ﬁrst glance, it might
seem that HERD’s CPU usage should be higher than Pilaf and FaRM-
KV, we show that in practice these two systems also have signiﬁcant
sources of CPU usage that reduce the extent of the difference.
First, issuing extra READs adds CPU overhead at the Pilaf and
FaRM-KV clients. To issue the second READ, the clients must poll
for the ﬁrst READ to complete. HERD shifts this overhead to the
server’s CPU, making more room for application processing at the
clients.
Second, handling PUT requests requires CPU involvement at the
server. Achieving low-latency PUTs requires dedicating server CPU
cores that poll for incoming requests. Therefore, the exact CPU
use depends on the fraction of PUT throughput that server is pro-
visioned for, because this determines the CPU resources that must
be allocated to it, not the dynamic amount actually used. For exam-
ple, our experiments show that, even ignoring the cost of updating
data structures, provisioning for 100% PUT throughput in Pilaf and
FaRM-KV requires over 5 CPU cores. Figure 13 shows FaRM-em
and Pilaf-em-OPT’s PUT throughput for 48 byte key-value items and
different numbers of CPU cores at the server. Pilaf-em-OPT’s CPU
usage is higher because it must post RECVs for new PUT requests,
which is more expensive than FaRM-em’s request-region polling.
In Figure 13, we also plot HERD’s throughput for the same work-
load by varying the number of server CPU cores. HERD is able to
deliver over 95% of its maximum throughput with 5 CPU cores. The
modest gap to FaRM-em arises because the HERD server in this
experiment is handling hash table lookups and updates, whereas the
emulated FaRM-KV is handling only the network trafﬁc.
We believe, therefore, that HERD’s higher throughput and lower
latency, along with the signiﬁcant CPU utilization in Pilaf and FaRM-
KV, justiﬁes the architectural decision to have the CPU involved on
the GET path for small key-value items. For a 50% PUT workload,
for example, the moderate extra cost of adding a few more cores—or
using the already-idle cycles on the cores—is likely worthwhile for
many applications.
1Figure 5 uses SENDs over UC, but we have veriﬁed that similar throughput is
possible using SENDs over UD.
Figure 14: Per-core throughput under skewed and uniform workloads.
Note that the y-axis does not begin at 0.
5.7 Resistance to skew
To understand how HERD’s behavior is impacted by skew, we tested
it with a workload where the keys are drawn from a Zipf distribution.
HERD adapts well to skew, delivering its maximum performance
even when the Zipf parameter is .99. HERD’s resistance to skew
comes from two factors. First, the back-end MICA architecture [18]
that we use in HERD performs well under skew; a skewed workload
spread across several partitions produces little variation in the par-
titions’ load compared to the skew in the workload’s distribution.
Under our Zipf-distributed workload, with 6 partitions, the most
loaded CPU core is only 50% more so than the least loaded core,
even though the most popular key is over 105 times more popular
than the average.
Second, because the CPU cores share the RNIC, the highly loaded
cores are able to beneﬁt from the idle time provided by the less-used
cores. Figure 13 demonstrates this effect: with a uniform work-
load and using only a single core, HERD can deliver 6.3 Mops.
When the system is conﬁgured to use 6 cores—the minimum re-
quired by HERD to deliver its peak throughput—the system delivers
4.32 Mops per core. The per-core performance reduction is not be-
cause of a CPU bottleneck, but because the server processes saturate
the PCIe PIO throughput. Therefore, even if the workload is skewed,
there is ample CPU headroom on a given core to handle the extra
requests.
Figure 14 shows the per-core throughput of HERD for a skewed
workload. The experimental conﬁguration is: 48-byte items, read-
intensive, skewed workload, 6 total CPU cores. The per-core
throughput for a uniform workload is included for comparison.
6. RELATED WORK
RDMA-based key-value stores: Other than Pilaf and FaRM, sev-
eral projects have designed memcached-like systems over RDMA.
Panda et al. [14] describe a memcached implementation using a
hybrid of UD and RC transports. It uses SEND/RECV messages
for all requests and avoids the overhead of UD transport (caused
by a larger header size than RC) by actively switching connections
between RC and UD. Although their cluster (ConnectX, 32 Gbps)
is comparable to Susitna (ConnectX-3, 40 Gbps), their request rate
is less than 1.5 Mops. Stuedi et al. [25] describe a SoftiWARP [28]
based version of memcached targeting CPU savings in wimpy nodes
with 10GbE.
Accelerating systems with RDMA: Several projects have used
verbs to improve the performance of systems such as HBase, Hadoop
RPC, PVFS [30, 13, 20]. Most of these use only SEND/RECV verbs
as a fast alternative to socket-based communication. In a PVFS
implementation over InﬁniBand [30], read() and write() oper-
ations in the ﬁlesystem use both RDMA and SEND/RECV. They
favor WRITEs over READs for the same reasons as in our work,
suggesting that the performance gap has existed over several gen-
erations of InﬁniBand hardware. There have been several versions
 0 5 10 15 20 25 1 2 3 4 5 6 7Throughput (Mops)Number of CPU coresHERDPilaf-em-OPT (PUT)FaRM-em (PUT) 3.5 4 4.5 5 5.5 1 2 3 4 5 6Throughput (Mops)Core IDZipf (.99)Uniform305of MPI over InﬁniBand [16, 19]. MPICH2 uses RDMA writes
for one-sided messaging: the server polls the head of a circular
buffer that is written to by a client. HERD extends this messag-
ing in a scalable fashion for all-to-all request-reply communication.
While [30, 13, 20, 16, 19] have benchmarked verbs performance
before, it has been for large messages in the context of applications
like NFS and MPI. Our work exploits the performance differences
that appear only for small messages and are relevant for message
rate-bound applications like key-value stores.
User level networking: Taken together, we believe that one
conclusion to draw from the union of HERD, Pilaf, FaRM, and
MICA [18] is that the biggest boost to throughput comes from
bypassing the network stack and avoiding CPU interrupts, not nec-
essarily from bypassing the CPU entirely. All four of these systems
use mechanisms to allow user-level programs to directly receive
requests or packets from the NIC: the userlevel RDMA drivers for
HERD, Pilaf, and FaRM, and the Intel DPDK library for MICA. As
we discuss below, the throughput of these systems is similar, but
the batching required by the DPDK-based systems confers a latency
advantage to the hardware-supported InﬁniBand systems. These
lessons suggest proﬁtable future work in making user-level classical
Ethernet systems more portable, easier to use, and lower-latency.
One ongoing effort is NIQ [10], an FPGA-based low-latency NIC
which uses cacheline-sized PIOs (without any DMA) to transmit
and receive small packets. Inlined WRITEs in RDMA use the same
mechanism at the requesters’s side.
General key-value stores: MICA [18] is a recent key-value sys-
tem for classical Ethernet. It assigns exclusive partitions to server
cores to minimize memory contention, and exploits the NIC’s capa-
bility to steer requests to the responsible core [3]. A MICA server
delivers 77 Mops with 4 dual-port, 10 Gbps PCIe 2.0 NICs, with
50 µs average latency (19.25 Gbps with one PCIe 2.0 card). This sug-
gests that, comparing the state-of-the-art, classical Ethernet-based
solutions can provide comparable throughput to RDMA-based so-
lutions, although with much higher latency. RAMCloud [23] is a
RAM-based, persistent key-value store that uses messaging verbs
for low latency communication.
7. CONCLUSION
This paper explored the options for implementing fast, low-latency
key-value systems atop RDMA, arriving at an unexpected and novel
combination that outperforms prior designs and uses fewer network
round-trips. Our work shows that, contrary to widely held beliefs
about engineering for RDMA, single-RTT designs with server CPU
involvement can outperform the “optimization” of CPU-bypassing
remote memory access when the RDMA approaches require multi-
ple RTTs. These results contribute not just a practical artifact—the
HERD low-latency, high-performance key-value cache—but an im-
proved understanding of how to use RDMA to construct future
DRAM-based storage services.
Acknowledgements We thank our shepherd Chuanxiong Guo and
the anonymous reviewers for their feedback that helped improve the
paper. We also thank Miguel Castro and Dushyanth Narayanan for
discussing FaRM with us, Kirk Webb and Robert Ricci for getting us
early access to the Apt cluster, and Hyeontaek Lim for his valuable
insights. This work was supported by funding from the National
Science Foundation under awards CNS-1314721 and CCF-0964474,
and Intel via the Intel Science and Technology Center for Cloud
Computing (ISTC-CC). The PRObE cluster [11] used for many
experiments is supported in part by NSF awards CNS-1042537 and
CNS-1042543 (PRObE).
References
[1] Connect-IB: Architecture for Scalable High Performance Computing. URL
http://www.mellanox.com/related-
docs/applications/SB_Connect-IB.pdf.
[2] Intel DPDK: Data Plane Development Kit. URL http://dpdk.org.
[3] Intel 82599 10 Gigabit Ethernet Controller: Datasheet. URL
http://www.intel.com/content/www/us/en/ethernet-
controllers/82599-10-gbe-controller-datasheet.html.
[4] Redis: An Advanced Key-Value Store. URL http://redis.io.
[5] memcached: A Distributed Memory Object Caching System, 2011. URL
http://memcached.org.
[6] B. Atikoglu, Y. Xu, E. Frachtenberg, S. Jiang, and M. Paleczny. Workload
Analysis of a Large-Scale Key-Value Store. In SIGMETRICS, 2012.
[7] B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, and R. Sears.
Benchmarking Cloud Serving Systems with YCSB. In SoCC, 2010.
[8] A. Dragojevic, D. Narayanan, O. Hodson, and M. Castro. FaRM: Fast Remote
Memory. In USENIX NSDI, 2014.
[9] B. Fan, D. G. Andersen, and M. Kaminsky. MemC3: Compact and Concurrent
MemCache with Dumber Caching and Smarter Hashing. In USENIX NSDI, 2013.
[10] M. Flajslik and M. Rosenblum. Network Interface Design for Low Latency
Request-Response Protocols. In USENIX ATC, 2013.
[11] G. Gibson, G. Grider, A. Jacobson, and W. Lloyd. PRObE: A Thousand-Node
Experimental Cluster for Computer Systems Research.
[12] M. Herlihy, N. Shavit, and M. Tzafrir. Hopscotch Hashing. In DISC, 2008.
[13] J. Huang, X. Ouyang, J. Jose, M. W. ur Rahman, H. Wang, M. Luo,
H. Subramoni, C. Murthy, and D. K. Panda. High-Performance Design of HBase
with RDMA over InﬁniBand. In IPDPS, 2012.
[14] J. Jose, H. Subramoni, K. C. Kandalla, M. W. ur Rahman, H. Wang, S. Narravula,
and D. K. Panda. Scalable Memcached Design for InﬁniBand Clusters Using
Hybrid Transports. In CCGRID. IEEE, 2012.
[15] A. Kalia, D. G. Andersen, and M. Kaminsky. Using RDMA Efﬁciently for
Key-Value Services. In Technical Report CMU-PDL-14-106, 2014.
[16] J. Li, J. Wu, and D. K. Panda. High Performance RDMA-Based MPI
Implementation over InﬁniBand. International Journal of Parallel Programming,
2004.
[17] H. Lim, B. Fan, D. G. Andersen, and M. Kaminsky. SILT: A Memory-efﬁcient,
High-performance Key-value Store. In SOSP, 2011.
[18] H. Lim, D. Han, D. G. Andersen, and M. Kaminsky. MICA: A Holistic Approach
to Fast In-Memory Key-Value Storage. In USENIX NSDI, 2014.
[19] J. Liu, W. Jiang, P. Wyckoff, D. K. Panda, D. Ashton, D. Buntinas, W. Gropp, and
B. Toonen. Design and Implementation of MPICH2 over InﬁniBand with RDMA
Support. In IPDPD, 2004.
[20] X. Lu, N. S. Islam, M. W. ur Rahman, J. Jose, H. Subramoni, H. Wang, and D. K.
Panda. High-Performance Design of Hadoop RPC with RDMA over InﬁniBand.
In ICPP, 2013.
[21] C. Mitchell, Y. Geng, and J. Li. Using One-Sided RDMA Reads to Build a Fast,
CPU-Efﬁcient Key-Value Store. In USENIX ATC, 2013.
[22] R. Nishtala, H. Fugal, S. Grimm, M. Kwiatkowski, H. Lee, H. C. Li, R. McElroy,
M. Paleczny, D. Peek, P. Saab, D. Stafford, T. Tung, and V. Venkataramani.
Scaling Memcache at Facebook. In USENIX NSDI, 2013.
[23] D. Ongaro, S. M. Rumble, R. Stutsman, J. Ousterhout, and M. Rosenblum. Fast
Crash Recovery in RAMCloud. In SOSP, 2011.
[24] R. Pagh and F. F. Rodler. Cuckoo Hashing. J. Algorithms, 2004.
[25] P. Stuedi, A. Trivedi, and B. Metzler. Wimpy Nodes with 10GbE: Leveraging
One-Sided Operations in Soft-RDMA to Boost Memcached. In USENIX ATC,
2012.
[26] S. Sur, A. Vishnu, H.-W. Jin, W. Huang, and D. K. Panda. Can Memory-Less
Network Adapters Beneﬁt Next-Generation InﬁniBand Systems? In HOTI, 2005.
[27] S. Sur, M. J. Koop, L. Chai, and D. K. Panda. Performance Analysis and
Evaluation of Mellanox ConnectX Inﬁniband Architecture with Multi-Core
Platforms. In HOTI, 2007.
[28] A. Trivedi, B. Metzler, and P. Stuedi. A Case for RDMA in Clouds: Turning
Supercomputer Networking into Commodity. In APSys, 2011.
[29] B. White, J. Lepreau, L. Stoller, R. Ricci, S. Guruprasad, M. Newbold, M. Hibler,
C. Barb, and A. Joglekar. An Integrated Experimental Environment for
Distributed Systems and Networks. In OSDI, 2002.
[30] J. Wu, P. Wyckoff, and D. K. Panda. PVFS over InﬁniBand: Design and
Performance Evaluation. In Ohio State University Tech Report, 2003.
[31] D. Zhou, B. Fan, H. Lim, M. Kaminsky, and D. G. Andersen. Scalable, High
Performance Ethernet Forwarding with CuckooSwitch. In CoNEXT, 2013.
306