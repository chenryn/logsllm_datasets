for several speciﬁc learning algorithms [31, 62, 73], the key
difference in our work is that we propose a general efﬁcient
unlearning approach applicable to any algorithm that can be
converted to the summation form, including some that cur-
rently have no incremental versions, such as normalized cosine
similarity and one-class SVM. In addition, our unlearning
approach handles all stages of learning,
including feature
selection and modeling. We also demonstrated our approach
on real systems.
Our unlearning approach is inspired by prior work on speed-
ing up machine learning algorithms with MapReduce [33]. We
believe we are the ﬁrst to establish the connection between
unlearning and the summation form. In addition, we are the
ﬁrst to convert non-standard real-world learning algorithms
such as normalized cosine similarity to the summation form.
The conversion is complex and challenging (see §VI). In con-
trast, the prior work converts nine standard machine learning
algorithms using only simple transformations.
The rest of the paper is organized as follows. In §II, we
present some background on machine learning systems and
the extended motivation of unlearning. In §III, we present the
goals and work ﬂow of unlearning. In §IV, we present the core
approach of unlearning, i.e., transforming a system into the
summation form, and its formal backbone. In §V, we overview
our evaluation methodology and summarize results. In §VI–
§IX, we report detailed case studies on four real-world learning
systems. In §X and §XI, we discuss some issues in unlearning
and related work, and in §XII, we conclude.
II. BACKGROUND AND ADVERSARIAL MODEL
This section presents some background on machine learning
(§II-A) and the extended motivation of unlearning (§II-B).
A. Machine Learning Background
Figure 2 shows that a general machine learning system with
three processing stages.
• Feature selection. During this stage, the system selects,
from all features of the training data, a set of features
most crucial for classifying data. The selected feature
set is typically small to make later stages more accurate
and efﬁcient. Feature selection can be (1) manual where
system builders carefully craft the feature set or (2) au-
tomatic where the system runs some learning algorithms
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:08:45 UTC from IEEE Xplore.  Restrictions apply. 

 















Fig. 2: A General Machine Learning System. Given a set of training
data including both malicious (+) and benign (−) samples, the system
ﬁrst selects a set of features most crucial for classifying data. It then
uses the training data to construct a model. To process an unknown
sample, the system examines the features in the sample and uses
the model to predict the sample as malicious or benign. The lineage
of the training data thus ﬂows to the feature set, the model, and the
prediction results. An attacker can feed different samples to the model
and observe the results to steal private information from every step
along the lineage, including the training data set (system inference
attack). She can pollute the training data and subsequently every step
along the lineage to alter prediction results (training data pollution
attack).
such as clustering and chi-squared test to compute how
crucial the features are and select the most crucial ones.
• Model training. The system extracts the values of the
selected features from each training data sample into
a feature vector. It feeds the feature vectors and the
malicious or benign labels of all training data samples
into some machine learning algorithm to construct a
succinct model.
• Prediction. When the system receives an unknown data
sample, it extracts the sample’s feature vector and uses
the model to predict whether the sample is malicious or
benign.
Note that a learning system may or may not contain all
three stages, work with labeled training data, or classify data
as malicious or benign. We present the system in Figure 2 be-
cause it matches many machine learning systems for security
purposes such as Zozzle. Without loss of generality, we refer
to this system as an example in the later sections of the paper.
B. Adversarial Model
To further motivate the need for unlearning, we describe
several practical attacks in the literature that target learning
systems. They either violate privacy by inferring private in-
formation in the trained models (§II-B1), or reduce security
by polluting the prediction (detection) results of anomaly
detection systems (§II-B2).
1) System Inference Attacks: The training data sets, such
as movie ratings, online purchase histories, and browsing
histories, often contain private data. As shown in Figure 2,
the private data lineage ﬂows through the machine learning
algorithms into the feature set, the model, and the prediction
results. By exploiting this lineage, an attacker gains an oppor-
tunity to infer private data by feeding samples into the system
and observing the prediction results. Such an attack is called
a system inference attack [29].2
Consider a recommendation system that uses item-item
collaborative ﬁltering which learns item-item similarities from
users’ purchase histories and recommends to a user the items
most similar to the ones she previously purchased. Calandrino
et al. [29] show that once an attacker learns (1) the item-item
similarities, (2) the list of recommended items for a user before
she purchased an item, and (3) the list after, the attacker can
accurately infer what the user purchased by essentially invert-
ing the computation done by the recommendation algorithm.
For example, on LibraryThing [12], a book cataloging service
and recommendation engine, this attack successfully inferred
six book purchases per user with 90% accuracy for over one
million users!
Similarly, consider a personalized warfarin dosing system
that guides medical treatments based on a patient’s genotype
and background. Fredrikson et al. [43] show that with the
model and some demographic information about a patient,
an attacker can infer the genetic markers of the patient with
accuracy as high as 75%.
2) Training Data Pollution Attacks: Another way to exploit
the lineage in Figure 2 is using training data pollution attacks.
An attacker injects carefully polluted data samples into a
learning system, misleading the algorithms to compute an in-
correct feature set and model. Subsequently, when processing
unknown samples, the system may ﬂag a big number of benign
samples as malicious and generate too many false positives,
or it may ﬂag a big number of malicious as benign so the true
malicious samples evade detection.
Unlike system inference in which an attacker exploits an
easy-to-access public interface of a learning system, data
pollution requires an attacker to tackle two relatively difﬁcult
issues. First, the attacker must trick the learning system into
including the polluted samples in the training data set. There
are a number of reported ways to do so [54, 56, 77]. For
instance, she may sign up as a crowdsourcing worker and
intentionally mislabel benign emails as spams [77]. She may
also attack the honeypots or other baiting traps intended for
collecting malicious samples, such as sending polluted emails
to a spamtrap [17], or compromising a machine in a honeynet
and sending packets with polluted protocol header ﬁelds [56].
Second, the attacker must carefully pollute enough data to
mislead the machine learning algorithms. In the crowdsourcing
case, she, the administrator of the crowdsourcing sites, directly
pollutes the labels of some training data [77]. 3% mislabeled
training data turned out to be enough to signiﬁcantly decrease
detection efﬁcacy. In the honeypot cases [17, 56], the attacker
cannot change the labels of the polluted data samples because
the honeypot automatically labels them as malicious. However,
2In this paper, we use system inference instead of model inversion [43].
466466
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:08:45 UTC from IEEE Xplore.  Restrictions apply. 
she controls what features appear in the samples, so she
can inject benign features into these samples, misleading the
system into relying on these features for detecting malicious
samples. For instance, Nelson et al. injected words that also
occur in benign emails into the emails sent to a spamtrap,
causing a spam detector to classify 60% of the benign emails
as spam. Perdisci et al. injected many packets with the same
randomly generated strings into a honeynet, so that
true
malicious packets without these strings evade detection.
This section presents the goals (§III-A) and work ﬂow
III. OVERVIEW
(§III-B) of machine learning.
A. Unlearning Goals
Recall that forgetting systems have two goals: (1) com-
pleteness, or how completely they can forget data; and (2)
timeliness, or how quickly they can forget. We discuss what
these goals mean in the context of unlearning.
1) Completeness:
Intuitively, completeness requires that
once a data sample is removed, all its effects on the feature set
and the model are also cleanly reversed. It essentially captures
how consistent an unlearned system is with the system that
has been retrained from scratch. If, for every possible sample,
the unlearned system gives the same prediction result as the
retrained system, then an attacker, operator, or user has no
way of discovering that the unlearned data and its lineage
existed in the system by feeding input samples to the unlearned
system or even observing its features, model, and training
data. Such unlearning is complete. To empirically measure
completeness, we quantify the percentage of input samples that
receive the same prediction results from both the unlearned
and the retrained system using a representative test data set.
The higher the percentage, the more complete the unlearning.
Note that completeness does not depend on the correctness
of prediction results: an incorrect but consistent prediction by
both systems does not decrease completeness.
Our notion of completeness is subject to such factors as
how representative the test data set is and whether the learning
algorithm is randomized. In particular, given the same training
data set, the same randomized learning algorithm may compute
different models which subsequently predict differently. Thus,
we consider unlearning complete as long as the unlearned
system is consistent with one of the retrained systems.
2) Timeliness: Timeliness in unlearning captures how much
faster unlearning is than retraining at updating the features
and the model in the system. The more timely the unlearning,
the faster the system is at restoring privacy, security, and
usability. Analytically, unlearning updates only a small number
of summations and then runs a learning algorithm on these
summations, whereas retraining runs the learning algorithm
on the entire training data set, so unlearning is asymptotically
faster by a factor of the training data size. To empirically mea-
sure timeliness, we quantify the speedup of unlearning over
retraining. Unlearning does not replace retraining. Unlearning
works better when the data to forget is small compared to the
training set. This case is quite common. For instance, a single
user’s private data is typically small compared to the whole
training data of all users. Similarly, an attacker needs only a
small amount of data to pollute a learning system (e.g., 1.75%
in the OSN spam ﬁlter [46] as shown in §VIII). When the data
to forget becomes large, retraining may work better.
B. Unlearning Work Flow
Given a training data sample to forget, unlearning updates
the system in two steps, following the learning process shown
in Figure 2. First, it updates the set of selected features. The
inputs at this step are the sample to forget, the old feature
set, and the summations previously computed for deriving the
old feature set. The outputs are the updated feature set and
summations. For example, Zozzle selects features using the
chi-squared test, which scores a feature based on four counts
(the simplest form of summations): how many malicious or
benign samples contain or do not contain this feature. To
support unlearning, we augmented Zozzle to store the score
and these counts for each feature. To unlearn a sample,
we update these counts to exclude this sample, re-score the
features, and select the top scored features as the updated
feature set. This process does not depend on the training data
set, and is much faster than retraining which has to inspect
each sample for each feature. The updated feature set in our
experiments is very similar to the old one with a couple of
features removed and added.
Second, unlearning updates the model. The inputs at this
step are the sample to forget, the old feature set, the updated
feature set, the old model, and the summations previously
computed for deriving the old model. The outputs are the
updated model and summations. If a feature is removed from
the feature set, we simply splice out the feature’s data from
the model. If a feature is added, we compute its data in the
model. In addition, we update summations that depend on
the sample to forget, and update the model accordingly. For
Zozzle which classiﬁes data as malicious or benign using na¨ıve
Bayes, the summations are probabilities (e.g., the probability
that a training data sample is malicious given that it contains
a certain feature) computed using the counts recorded in the
ﬁrst step. Updating the probabilities and the model is thus
straightforward, and much faster than retraining.
IV. UNLEARNING APPROACH
As previously depicted in Figure 1, our unlearning approach
introduces a layer of a small number of summations between
the learning algorithm and the training data to break down
the dependencies. Now, the learning algorithm depends only
on the summations, each of which is the sum of some
efﬁciently computable transformations of the training data
samples. Chu et al. [33] show that many popular machine
learning algorithms, such as na¨ıve Bayes, can be represented
in this form. To remove a data sample, we simply remove
the transformations of this data sample from the summations
that depend on this sample, which has O(1) complexity, and
467467
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:08:45 UTC from IEEE Xplore.  Restrictions apply. 
compute the updated model. This approach is asymptotically
faster than retraining from scratch.
More formally, the summation form follows statistical query
(SQ) learning [48]. SQ learning forbids a learning algorithm
from querying individual training data samples. Instead, it
permits the algorithm to query only statistics about the training
data through an oracle. Speciﬁcally, the algorithm sends a
function g(x, lx) to the oracle where x is a training data sam-
ple, lx the corresponding label, and g an efﬁciently computable
function. Then, the oracle answers an estimated expectation
of g(x, lx) over all training data. The algorithm repeatedly
queries the oracle, potentially with different g-functions, until
it terminates.
Depending on whether all SQs that an algorithm issues
are determined upfront, SQ learning can be non-adaptive (all
SQs are determined upfront before the algorithm starts) or
adaptive (later SQs may depend on earlier SQ results). These
two different types of SQ learning require different ways to
unlearn, described in the following two subsections.
A. Non-adaptive SQ Learning
A non-adaptive SQ learning algorithm must determine
all SQs upfront. It follows that the number of these SQs is
constant, denoted m, and the transformation g-functions are
ﬁxed, denoted g1, g2, ..., gm. We represent the algorithm in
the following form:
(cid:2)
(cid:2)
(cid:2)
Learn(
∈X g1(xi, lx
),
i
x
i
∈X g2(xi, lx
), ...,
i