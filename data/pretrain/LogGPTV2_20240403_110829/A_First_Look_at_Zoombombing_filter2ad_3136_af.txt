speciﬁcally checking the meeting room in the case of large
meetings. Our analysis however shows that insiders often share
additional information with potential attackers, for example
instructing them to select names that correspond to legitimate
participants in the meeting. This reduces the effectiveness of
a waiting room, because it makes it more difﬁcult for hosts
and moderators to identify intruders.
Providing a unique link for each participant reduces the
chances of success of zoombombing attacks. If the meeting
service still allows multiple people joining with the same link,
at least this gives some accountability, since the meeting host
can identify who the insider was based on the unique link used
by attackers to join. An even better mitigation is to allow each
link to be used by a single participant at a time. This way, as
long as the insider joins the meeting unauthorized people will
not be able to join using the same link. While this mitigation
makes zoombombing unfeasible, not all meeting services have
adopted it. At the time of writing, only Zoom and Webex make
available per-participant links that allow a single user to join
at a time. To do this, Zoom requires participants to log in,
and checks if the unique link is the same that was sent to
that email address as a calendar invite. We encourage other
meeting platforms to adopt similar access control measures
to protect their meetings from insider threats. We also note
that other similar mitigations are possible, like having meeting
links expire after they are used once.
Additionally, we ﬁnd that zoombombing attacks usually
happen in an opportunistic fashion, with insiders asking others
to join meetings happening in real time. This reduces the
effectiveness of proactive measures like monitoring social
media for calls for future attacks.
Limitations and future work. As with any data-driven study,
our study is not exempt from limitations. We only have a
1% sample of Twitter available, therefore our zoombombing
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:25:23 UTC from IEEE Xplore.  Restrictions apply. 
1464
it
limitation,
results related to Twitter are a lower bound of the actual
extent of the problem. Additionally, API limitations prevent
us from collecting replies to zoombombing tweets, allowing
us to only get a partial picture of how attacks unfold on the
platform. On 4chan, users are anonymous. We therefore cannot
trace per-user behavior, and this prevents us from observing
serial offenders calling for multiple attacks over time. As
an additional
is possible that zoombombing
attacks are organized by other platforms other than Twitter
and 4chan. While we believe that these two services provide a
representative overview of behaviors and motives, attackers
on other platforms might operate differently than what we
observed in this paper. Finally, our analysis is limited to calls
for attacks and responses to such calls on social media, but
we are unable to observe what happens in the actual meeting
rooms. Future work could develop alternative study designs
that allow analyzing the attack on the online meeting platform
itself, for example by collecting and analyzing recorded online
meetings that were bombed, or by interviewing victims of
zoombombing. This would also allow a better understanding
of the mental and emotional toll on zoombombing victims.
VIII. RELATED WORK
Coordinated malicious activity on social media. The secu-
rity community has extensively studied automated malicious
behavior on social media, mostly focusing on bots sending
spam [17, 21, 51] and on malicious accounts colluding to
inﬂate each other’s reputation [10, 46, 48]. The mitigation
systems proposed to detect and block this type of activity
rely on the fact that these operations are large scale, rely
on automated methods, and are carried out by single entities.
Therefore, synchronization features can be used to distinguish
between benign and malicious activity [7, 45, 54]. Alterna-
tively, systems have been proposed that identify common traits
in massively created fake accounts, for example an anomalous
fraction of followers to friends or a large set of accounts
created around the same time [3, 9, 44, 50, 51].
More recently, the community’s focus expanded to looking
at coordinated malicious campaigns that are not carried out by
automated means, but rather by humans controlling a small
number of inauthentic accounts. This includes conspiracy
theories being pushed on social media [41, 42] and inﬂuence
campaigns by foreign state actors [1, 52]. While not as
automated as large-scale bot activity, these campaigns still
show coordination, which can be leveraged for detection [31].
Coordinated online harassment and aggression. A closer
line of work to the problem studied in this paper looks at
coordinated behavior geared toward harassing victims online.
Kumar et al. [28] measure the problem of brigading on
Reddit, where the members of one sub-community (subreddit)
organize to disrupt another community by posting offensive
messages and prevent it from continuing its normal operation.
Hine et al. [23] study the activity of 4chan’s Politically
Incorrect Board (/pol/), showing that members of that com-
munity often call for attacks against people who post videos
on YouTube and end up harassing the poster in the comments
section of the video. Mariconti et al. [33] develop a multi-
modal machine learning system able to predict which videos
are likely to receive this kind of hate attack in the hope of
aiding moderation efforts.
Zannettou et al. [53] investigate a similar phenomenon,
studying the effect of posting a URL to a news article on
4chan and Reddit. They show that posting URLs to certain
types of news outlets results in a sudden increase in the hate
speech on the comments to that article.
Snyder et al. [40] study the problem of doxing, in which
attackers post information about a victim, calling for people
to attack that person through multiple media (e.g., on multiple
social networks or through email), sometimes even transcend-
ing to the physical world.
Tseng et al. [47] analyze ﬁve forums in which miscreants
share and discuss tools and techniques that can be used to spy
on their partners and further harass them.
Our work builds on previous research on coordinated ha-
rassment by studying the emerging problem of zoombombing.
Unlike previously studied threats, we show that zoombombing
attacks are often called by insiders; this has important implica-
tions when designing security mitigations against the problem.
IX. CONCLUSION
In this paper, we perform the ﬁrst data-driven study of
calls for zoombombing attacks on social media. Our ﬁndings
indicate that these attacks mostly target online lectures, and
are mostly called for by insiders who have legitimate access
to the meetings. We ﬁnd that insiders often share conﬁden-
tial information like meeting passwords and the identity of
real participants in the meeting, making common protections
against zoombombing ineffective. We also ﬁnd that calls for
zoombombing usually target meetings happening in real time,
making the proactive identiﬁcation of such attacks challenging.
To protect against the threat, we encourage online meeting
services to allow hosts to create unique meeting links for each
participant, although we acknowledge that this has usability
implications and might not always be feasible.
X. ACKNOWLEDGMENTS
We would like to thank the anonymous reviewers for their
useful comments. This work was funded by the NSF under
grant 1942610.
REFERENCES
[1] A. Badawy, E. Ferrara, and K. Lerman. Analyzing
the digital traces of political manipulation: The 2016
russian interference twitter campaign. In IEEE/ACM In-
ternational Conference on Advances in Social Networks
Analysis and Mining (ASONAM), 2018.
[2] M. Bailey, D. Dittrich, E. Kenneally, and D. Maughan.
The Menlo Report. IEEE Security & Privacy, 2012.
[3] F. Benevenuto, G. Magno, T. Rodrigues, and V. Almeida.
Detecting spammers on Twitter.
In Collaboration,
electronic messaging, anti-abuse and spam conference
(CEAS), 2010.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:25:23 UTC from IEEE Xplore.  Restrictions apply. 
1465
[4] V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and
E. Lefebvre. Fast unfolding of communities in large
networks. Journal of Statistical Mechanics: Theory and
Experiment, 2008.
[5] V. Braun and V. Clarke. Using thematic analysis in
psychology. Qualitative research in psychology, 2006.
[6] B. Brown. Notes on running an online academic confer-
ence or how we got zoombombed and lived to tell the
tale. Interactions, 2020.
[7] Q. Cao, X. Yang, J. Yu, and C. Palow. Uncovering
large groups of active malicious accounts in online social
networks. In ACM SIGSAC Conference on Computer and
Communications Security (CCS), 2014.
[8] CNBC. Zoom has added more videoconferencing users
this year. "https://www.cnbc.com/2020/02/26/zoom-has
-added-more-users-so-far-this-year-than-in-2019-bernst
ein.html".
[9] C. A. Davis, O. Varol, E. Ferrara, A. Flammini, and
F. Menczer. Botornot: A system to evaluate social bots.
In The Web Conference (WWW), 2016.
[10] E. De Cristofaro, A. Friedman, G. Jourjon, M. A. Kaafar,
and M. Z. Shaﬁq.
Paying for likes? understanding
facebook like fraud using honeypots. In ACM SIGCOMM
Internet Measurement Conference (IMC), 2014.
[11] FBI.
Fbi warns of
teleconferencing and online
during
pandemic.
covid-19
classroom hijacking
"https://www.fbi.gov/contact-us/field-ofﬁces/boston/ne
ws/press-releases/fbi-warns-of-teleconferencing-and-on
line-classroom-hijacking-during-covid-19-pandemic".
[12] J. Finkelstein, S. Zannettou, B. Bradlyn, and J. Black-
burn. A quantitative approach to understanding online
antisemitism. In AAAI International Conference on Web
and Social Media (ICWSM), 2019.
[13] J. L. Fleiss. Measuring nominal scale agreement among
many raters. Psychological bulletin, 1971.
[14] J. L. Fleiss, B. Levin, and M. C. Paik. Statistical methods
for rates and proportions. john wiley & sons, 2013.
[15] C. I. Flores-Saviaga, B. C. Keegan, and S. Savage. Mo-
bilizing the trump train: Understanding collective action
in a political trolling community. In AAAI International
Conference on Web and Social Media (ICWSM), 2018.
[16] J. Fox and W. Y. Tang. Women’s experiences with
general and sexual harassment in online video games:
Rumination, organizational responsiveness, withdrawal,
and coping strategies. New Media & Society, 2017.
[17] H. Gao, J. Hu, C. Wilson, Z. Li, Y. Chen, and B. Y. Zhao.
Detecting and characterizing social spam campaigns.
In ACM SIGCOMM Internet Measurement Conference
(IMC), 2010.
[18] Google.
Google Meet
add more
users.
https://www.theverge.com/2020/4/28/21240434/goog
le-meet-three-million-users-per-day-pichai-earnings.
[19] Google.
Mute or
remove video meeting partici-
"https://support.google.com/meet/answer/75011
pants.
21?co=GENIE.Platform%3DDesktop&hl=en".
[20] Google. There is no current feature for ’mute all’. https:
//support.google.com/meet/thread/35068017?hl=en.
[21] C. Grier, K. Thomas, V. Paxson, and M. Zhang. @spam:
the underground on 140 characters or less.
In ACM
SIGSAC Conference on Computer and Communications
Security (CCS), 2010.
[22] S. Hinduja and J. W. Patchin. Bullying, cyberbullying,
and suicide. Archives of suicide research, 2010.
[23] G. E. Hine, J. Onaolapo, E. De Cristofaro, N. Kourtellis,
I. Leontiadis, R. Samaras, G. Stringhini, and J. Black-
burn. Kek, cucks, and god emperor trump: A measure-
ment study of 4chan’s politically incorrect forum and its
effects on the web. In AAAI International Conference on
Web and Social Media (ICWSM), 2017.
[24] M. Jacomy, T. Venturini, S. Heymann, and M. Bas-
tian. Forceatlas2, a continuous graph layout algorithm
for handy network visualization designed for the gephi
software. PLOS ONE, 2014.
[25] Jitisi.
There
’mute
all’. "https://community.jitsi.org/t/option-to-mute-unmut
e-participants-by-moderator/15062".
is no current
feature
for
[26] D. Kagan, G. F. Alpert, and M. Fire. Zooming into video
conferencing privacy and security threats, 2020.
[27] B. Kaleli, M. Egele, and G. Stringhini. On the perils
of leaking referrers in online collaboration services. In
International Conference on Detection of Intrusions and
Malware, and Vulnerability Assessment (DIMVA), 2019.
[28] S. Kumar, W. L. Hamilton, J. Leskovec, and D. Jurafsky.
Community interaction and conﬂict on the web. In The
Web Conference (WWW), 2018.
[29] H. Kwak, C. Lee, H. Park, and S. Moon. What is
Twitter, a social network or a news media? In The Web
Conference (WWW), 2010.
[30] T. Lauinger, K. Onarlioglu, A. Chaabane, E. Kirda,
W. Robertson, and M. A. Kaafar. Holiday pictures or
blockbuster movies? Insights into copyright infringement
in user uploads to one-click ﬁle hosters. In International
Symposium on Recent Advances in Intrusion Detection
(RAID), 2013.
[31] L. Luceri, S. Giordano, and E. Ferrara. Detecting troll
behavior via inverse reinforcement learning: A case study
of russian trolls in the 2016 us election. In AAAI Interna-
tional Conference on Web and Social Media (ICWSM),
2020.
[32] J. M. MacAllister. The doxing dilemma: seeking a rem-
edy for the malicious publication of personal information.
Fordham L. Rev., 2016.
[33] E. Mariconti, G.
Suarez-Tangil,
J. Blackburn,
E. De Cristofaro, N. Kourtellis, I. Leontiadis, J. L.
Serrano, and G. Stringhini.
to
Do”: Proactive Detection of YouTube Videos Targeted
by Coordinated Hate Attacks.
ACM Conference on
Computer-Supported Cooperative Work and Social
Computing (CSCW), 2019.
“You Know What
[34] L. McLean and M. D. Grifﬁths. Female gamers’ expe-
rience of online harassment and social support in online
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:25:23 UTC from IEEE Xplore.  Restrictions apply. 
1466
gaming: a qualitative study.
Mental Health and Addiction, 2019.
International Journal of
[35] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient
estimation of word representations in vector space. 2013.
[36] A. Nagle. Kill all normies: Online culture wars from
4chan and Tumblr to Trump and the alt-right. 2017.
[37] A. Papasavva, S. Zannettou, E. De Cristofaro, G. Stringh-
ini, and J. Blackburn. Raiders of the lost kek: 3.5 years
of augmented 4chan posts from the politically incorrect
board.
In AAAI International Conference on Web and
Social Media (ICWSM), 2020.
Cisco webex
[38] Reuters.
record
draws
users.
i/Zoombombing".
"https://www.reuters.com/article/us-cisco-systems-w
ebex/ciscos-webex-draws-record-324-million-users-in-
march-idUSKBN21L2SY".
[39] C. M. Rivers and B. L. Lewis. Ethical research standards
in a world of big data. F1000Research, 2014.
[40] P. Snyder, P. Doerﬂer, C. Kanich, and D. McCoy. Fifteen
minutes of unwanted fame: Detecting and characterizing
doxing.
In ACM SIGCOMM Internet Measurement
Conference (IMC), 2017.
[41] K. Starbird. Examining the alternative media ecosystem
through the production of alternative narratives of mass
shooting events on twitter. In AAAI International Con-
ference on Web and Social Media (ICWSM), 2017.
[42] K. Starbird, A. Arif, and T. Wilson. Disinformation as
collaborative work: Surfacing the participatory nature of
strategic information operations. Proceedings of the ACM
on Human-Computer Interaction, 2019.
[43] Statista. Numbers of skype. "https://www.statista.com/s
tatistics/820384/estimated-number-skype-users-worldw
ide/".
[44] G. Stringhini, C. Kruegel, and G. Vigna. Detecting
In Annual computer
spammers on social networks.
security applications conference (ACSAC), 2010.
[45] G. Stringhini, P. Mourlanne, G. Jacob, M. Egele,
C. Kruegel, and G. Vigna. Evilcohort: Detecting com-
munities of malicious accounts on online services.
In
[49] Wikipedia. Zoombombing. "https://en.wikipedia.org/wik
USENIX Security Symposium, 2015.
[46] G. Stringhini, G. Wang, M. Egele, C. Kruegel, G. Vigna,
H. Zheng, and B. Y. Zhao. Follow the green: growth
and dynamics in Twitter follower markets.
In ACM
SIGCOMM Internet Measurement Conference (IMC),
2013.
[47] E. Tseng, R. Bellini, N. McDonald, M. Danos, R. Green-
stadt, D. McCoy, N. Dell, and T. Ristenpart. The tools
and tactics used in intimate partner surveillance: An
analysis of online inﬁdelity forums. In USENIX Security
Symposium, 2020.
[48] J. Weerasinghe, B. Flanigan, A. Stein, D. McCoy, and
R. Greenstadt. The pod people: Understanding manipu-
lation of social media popularity via reciprocity abuse.
In The Web Conference, 2020.
[50] C. Yang, R. C. Harkreader, and G. Gu. Die free or live
hard? empirical evaluation and new design for ﬁghting
evolving twitter spammers. In International Symposium
on Recent Advances in Intrusion Detection (RAID), 2011.
[51] D. Yuan, Y. Miao, N. Z. Gong, Z. Yang, Q. Li, D. Song,
Q. Wang, and X. Liang. Detecting fake accounts in online
social networks at the time of registrations.
In ACM
Conference on Computer and Communications Security
(CCS), pages 1423–1438, 2019.
[52] S. Zannettou, T. Caulﬁeld, W. Setzer, M. Sirivianos,
G. Stringhini, and J. Blackburn. Who let the trolls out?
towards understanding state-sponsored trolls.
In ACM
conference on web science, 2019.
[53] S. Zannettou, M. ElSherief, E. Belding, S. Nilizadeh, and
G. Stringhini. Measuring and characterizing hate speech
on news websites. In ACM conference on web science,
2020.
[54] Y. Zhao, Y. Xie, F. Yu, Q. Ke, Y. Yu, Y. Chen, and
E. Gillum. Botgraph: Large scale spamming botnet
detection. In USENIX Symposium on Networked Systems
and Design (NSDI), 2009.
[55] Zoom. How to keep uninvited guests out of your zoom
event. "https://blog.zoom.us/keep-uninvited-guests-out-o
f-your-zoom-event/".
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:25:23 UTC from IEEE Xplore.  Restrictions apply. 
1467