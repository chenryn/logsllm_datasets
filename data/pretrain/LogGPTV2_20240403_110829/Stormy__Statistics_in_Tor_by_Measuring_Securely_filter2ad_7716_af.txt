n
u
R
s
1
9 Set Cardinality | Runtime vs Counters
AuthMode
RelMode
s
0
4
s
0
3
e
m
i
t
n
u
R
s
0
2
s
0
1
d
n
o
c
e
s
r
e
p
s
e
l
p
i
r
T
0
4
0
2
0
k
0
5
k
0
4
k
0
3
k
0
2
d
n
o
c
e
s
r
e
p
s
e
l
p
i
r
T
i
n
m
0
2
e
m
i
t
n
u
R
i
n
m
0
1
1000 2000 3000 4000 5000 6000 7000
Num. 32-Bit Inputs
10 ms
100 ms
Latency
0
2000
4000
6000
8000
Num. Counters
Figure 5: Network performance measurements of our implementation of the offline phase triple generation [68] and online
phase evaluations of the median and set-union cardinality circuits (Section 6).
7.2.2 Authenticated Bit Generation. Authenticated bit generation,
the other component of offline preprocessing required for input
sharing, is orders-of-magnitude less expensive than triple genera-
tion. For brevity, we report results only in the default experimental
configurations. 224,000 bits must be generated by the CC to per-
form the input sharing for a median computation with 7,000 32-bit
inputs. 1,802,240 bits must be generated for a set-union cardinality
computation with 1,024 counters, 32 entries per counter, and 55-bit
entries. The RelMode CC generates 224k bits in 1.9 min using 73
MiB of communication; 1.8M bits takes 14 min and 579 MiB of
communication. The AuthMode CC generates 224k bits in 2.8 s
using 8.6 MiB of communication; 1.8M bits takes 3.8 s and 69 MiB
of communication.
7.3 Online Computations
Input Sharing. Similar to bit generation, input sharing is
7.3.1
the least expensive part of the online phase, so we present results
only in the default configuration and only for the median circuit
(recall from Section 6 that the online input-sharing protocol is not
necessary for set-cardinality). With 7,000 input parties, RelMode
takes 140 s and each relay in the CC transmits 89 MiB; AuthMode
takes 24 s/16 MiB. We estimate that secret-sharing inputs for set-
cardinality (in the default configuration) takes 92 s/60 MiB per CC
member in RelMode and 92 s/320 MiB in AuthMode.
7.3.2 Median Circuit. For the median and set-union cardinality
simulations, we assume the CC has already has the triples, bits, and
input shares necessary for circuit evaluation. The combination of
circuit AND depth and network latency almost completely deter-
mines online computation runtime; this is because the hosts must
partially-open some authenticated bits after each AND gate level
by sending and receiving bits to/from a designated party which
incurs two one-way latency costs. This cost adds up quickly in our
networks with relatively high latency. In contrast, communication
costs are low as only a few bits are required per AND gate in the
online-phase. Therefore, we do not show plots varying bandwidth
and committee size as varying these parameters has minimal effect
on runtime. We will observe that the online-phase runtime can be
accurately predicted as d · RTT, where d is the AND depth and the
round-trip time (RTT) is twice the one-way latency.
When the number of inputs is not being varied, we set the median
circuit to accept 7,000, 32-bit integer inputs. The CC in the RelMode
computes this median in 25 min (the circuit has ∼3,000 AND depth)
using 10 MiB of communication. The AuthMode CC can compute
this median in 5 min (7.1 MiB) due to their lower latencies.
Figures 5.6ś5.7 plot the experimental runtime as latency and
the number of 32-bit inputs is varied. As expected, runtime and
latency share a linear relationship. Recall from Section 6 that our
median circuit construction on n inputs of b bits each has AND
depth of about b log2
2(n)/2. Accordingly, we find that the shape
of the runtime-input graph is sublineary, which enables efficient
scaling of the online median computation as the Tor network grows.
Set-Union Cardinality. In these experiments, by default, we
7.3.3
use a set-union cardinality circuit with 1,024 counters, 32 entries
per counter, and 55-bit counter entries. In this default configuration,
the CC (in both models) can compute a cardinality estimate in under
30 seconds. This circuit requires less than 5 MiB of communication
per CC member. The low runtime can be attributed to the shallow
AND depth (= 21) of the circuit. As the Figure 5.9 runtime-counter
plot shows, increasing the number of counters used in the set-union
Session 3C: Secure Computing IICCS ’19, November 11–15, 2019, London, United Kingdom626cardinality circuit (and thus increasing the accuracy of the count)
does not have a significant impact on the online protocol’s runtime.
However, keeping this number low keeps the number of triples
required low. For example, increasing the number of counters from
1,024 to 8,096 only decreases the cardinality computation’s standard
error from 4% to 1% but yields a tenfold increase in the number of
AND gates (and thus triples) required to compute the circuit.
7.4 System Performance
Here we use the preceding experimental results to estimate the per-
formance of Stormy when computing the median and set cardinality,
including both total time and communication. This estimation is
straightforward for the Authority Model, as the authorities exe-
cute all parts of the system, and so we simply add up the time and
communication of each piece (i.e. triple and bit generation for the
offline phase, input sharing and computation for the online phase).
Performance estimation in the Relay Model is more complex,
as it uses a large number of relays comprising sets of committees
for the pieces of the protocol. The main challenge is analyzing
the offline phase, which involves many differing relay bandwidths,
relay churn, and performance over multiple computations. To per-
form this analysis, we build a custom simulator that models the
execution of the offline phase over a single measurement epoch.
Each simulation uses a sequence of published Tor consensuses [5]
to determine the available relay population in each hour of the
24-hour epoch. The simulation process is then: (1) a set of Com-
putation Committees (CCs) is sampled from the initial consensus
and one chosen to be initially active, (2) a set of Triple Commit-
tees (TCs) is similarly sampled and a subset initially activated that
has maximal throughput without violating memory or bandwidth
constraints, (3) each active TC alternates between generating and
transferring triples with the generation time determined by the
TC’s bandwidth and a linear regression on the experimental results
over varying bandwidths, (4) TCs are queued to transfer triples to
the CC with each transfer proceeding as fast as the two committees’
bandwidths allow, (5) any time the CC is not receiving triples it
generates batches of random bits at a speed based on its bandwidth
and a linear regression of the experimental results, and (6) any com-
mittee with a member missing from an hourly Tor consensus dies
at that hour, and some remaining sampled but inactive committees
are activated in its place.
We simulate committee election on the Tor network for the day
of 2018-10-01 using data from Tor Metrics [5]. As described in
Section 2, the Tor network on that day consisted of 6,331 relays
providing 275 Gbps in aggregate. Of these relays, we restrict our-
selves to using the 5,506 that have the Fast and Running flags, as
well as an archived descriptor, all of which Tor clients also require.
Of these, 2,381 were guards. We use the consensus weights to select
relays and report bandwidth using the bandwidths advertised by
the relays in their descriptors. Each relay is assumed to have a
250 ms one-way latency to each other relay.
We perform the simulation 100 times where the relays are gen-
erating offline material for the median circuit and 100 times where
the relays are generating offline material for the set-union cardi-
nality circuit. For the median simulations, we sample mTC = 1, 000
TCs and mCC = 8 CCs. For the cardinality simulations, we sample
mTC = 375 TCs and mCC = 633 CCs. Both yield 1,008 total sampled
committees, but computing the median requires more triples and
thus benefits from more TCs, while the computing the cardinality
requires more bits and thus benefits from a higher-bandwidth CC.
The randomness we vary between simulations is the random bit
string used for committee election.
7.4.1 Committee and Churn Analysis. Here we provide results de-
tailing: (1) the number of committees activated, (2) the bandwidth
of each committee (i.e. the bandwidth allocated by all members
to that committee), and (3) churn statistics across the committees.
We limit this set of results to the median-circuit simulations be-
cause these simulations require the most multiplication triples and
thus require the highest level of parallel preprocessing. Any value
that can change over the course of a simulation is reported on a
time-averaged basis, that is, averaged over all hours during the
simulation. For statistics that we measure, we record the median
value and interquartile range (IQR) across simulations.
During the simulations, we never observed that all the sampled
Computation Committees failed. Indeed, 75% of the time, the first
activated CC lasted the entire 24 hours. The maximum number of
CCs that died at any point was 3, where we sampled mCC = 8 CCs in
total. The median bandwidth of the CC was 11.0 Mbps with an IQR
of [10.0, 13.1]. This bandwidth limits the speed of bit generation and
triple transfer, which is why the maximum-bandwidth committee
among those sampled and alive is used as the active one.
We sampled mTC = 1, 000 TCs, but only 327 were active in
the median case (IQR: [319, 333]). 712 of the 1,000 TCs (IQR: [703,
722]) died during the median 24-hour simulation. This resulted in a
median bandwidth fraction usage of 21.2% (IQR: [20.8, 21.6]) over
time over all active committees, even though 25% of the bandwidth
was always used at the beginning of the simulation. Of the 5,506
active relays at the start of the simulation, a median of 2,473 (IQR:
[2,446, 2,491]) were in at least one active committee. The median
average TC bandwidth was 7.1 Mbps (IQR: [6.8, 7.4]). This is lower
than the 11.0 Mbps CC bandwidth primarily due to the fact that all
TCs were used and not just the highest-bandwidth one.
7.4.2 Overall System Performance. Overall performance estimates
of the system when computing a single median or a single set-union
cardinality are reported in Table 4. We compute these estimates
using the preceding Shadow experiments and RelMode simulations.
In all of our simulations, we record the communication required
for each party and the time taken to complete each phase of the
protocol. The Time column reports the measured time required to
complete a phase, and the Data TX column reports the measured
amount of data the each member of a TC or CC transmits during
the phase. Note that, during triple generation in RelMode, because
relays are sampled to serve on a number of TCs proportional to their
bandwidth, relays who can provide more bandwidth will transmit
more data. Therefore the Data TX value reported in the RelMode
TC is computed for a relay with a 1 Gbps link (a relay with a 500
Mbps link, for example, would transmit one-half as much data). All
other values in the table are not dependent upon the bandwidth
allocations of the hosts running the protocol.
For median computations, the online communication is relatively
low, and the time needed for the online phase is due to high round
complexity and high assumed latency. Therefore, using pipelining
(i.e. running the online phase concurrently with a subsequent offline
Session 3C: Secure Computing IICCS ’19, November 11–15, 2019, London, United Kingdom627Table 4: Summary of Section 7 performance evaluation.
Communication costs are reported per-party and reported
broken down by committee type (TC or CC).
Median
Set Cardinality
Phase
Time
Data TX
Time
Data TX
TC
CC
TC
CC
Offline
Online
9.5 min
5.3 min
✗
✗
AuthMode
26 GiB
21 MiB
RelMode
1.1 min