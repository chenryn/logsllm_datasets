the least upper bound on all possible pairs of neighbouring
datasets, we have the above expression.
Under the δ-neighbourhood, the sensitivity of A is
{ ∥ai − aj∥1 },
max
(i,j)∈N
Solving the optimization problem in general is diﬃcult for
standard neighbourhood, partly due to the fact that the sen-
sitivity △A as a function of A, is non-diﬀerentiable. Like-
wise it is diﬃcult for δ-neighbourhood. We will show in the
next section how we can improve A for range queries by
exploiting the combinatoric structure.
5.3 Proposed Algorithm
The main idea of our proposed algorithm can be illus-
trated with a graphical representation of the bins when the
entries in A are binary, i.e. either 0 or 1. Let us treat each
bin as a vertex in the graph. Hence there are k vertices
v1, v2, . . . , vk. There is an edge between two vertices vi and
vj iﬀ (i, j) ∈ N .
For a matrix A, since the entries are binary, each row
corresponds to a subset of bins. Hence, A can be viewed as
a collection of sets {a1, a2, . . . , an} where each set in A is a
set of bins. For an edge (vi, vj), we say it is being cut by a
set a iﬀ
(vi ∈ a ∧ vj ̸∈ a) ∨ (vi ̸∈ a ∧ vj ∈ a).
For each edge e, let us call the number of sets in A that cut
the edge e the number of cuts on e, denoted as C(e). Now,
the sensitivity of A is the maximum number of cuts over all
edges, i.e. maxe C(e).
′
′
, j
)) | |i − i
Note that given a particular A, it may be possible to
insert a set into A without increasing its sensitivity. That
is, it may be possible to ﬁnd a subset that only cuts edges
that have not been cut by subsets in A. Since sensitivity is
not increased, it would not hurt to add this set into A which
in turn publishes this extra information2. This observation
leads to a simple greedy algorithm that improves a strategy:
simply add rows to A until it is not possible to do so without
increasing the sensitivity.
For instance, consider a 2D histogram with bins {bi,j | i, j ∈
′| +
Zn}, with neighbourhood N = {((i, j), (i
|j − j
′| ≤ 1} as shown in Figure 1 where each bin is a bul-
let(blue), and the neighbours are connected by a dotted(red)
line. Consider the set A that contains ai,j = {b2i−1,2j−1,
b2i−1,2j, b2i,2j−1, b2i,2j}, for i, j = 1, 2, . . . n
2 , that is, each
ai,j is a solid(blue) square that contains four blue vertices.
Note that the solid(blue) squares do not “cut” all the neigh-
i,j = {b2i,2j, b2i,2j+1,
′
bouring edges, and therefore, if we add a
b2i+1,2j, b2i+1,2j+1} to A, for i, j = 1, 2, . . . n
− 1, (i.e. the
dash(black) squares containing 4 vertices each), the sensi-
tivity remains the same.
′
On the other hand, for some A, inserting any additional
a
into A will increase the sensitivity of A. Therefore, the
key question lies on whether the noise reduced by the addi-
′
tional a
s is more signiﬁcant than the noise introduced by
the increment of sensitivity. Such comparison is application
dependent, i.e. it depends on the queries Q.
2
where N is a set induced from the requirement on δ-neighb-
ourhood,
N = {(i, j) | ∃ x ∈ bi, y ∈ bj, s.t. d(x, y) ≤ δ}.
Compare to the expression in (7), the maximum is taken
over a smaller set N and thus could be smaller.
For the matrix A in the illustrating example, we have
N = {(1, 2), (2, 3), (3, 4)} under 0.25-neighbourhood, and
thus the sensitivity of A is 1; whereas the sensitivity un-
der the standard neighbourhood is 3, as ∥a1 − a4∥1 = 3.
For 2D spatial data, we consider random range queries
and propose publishing a series of equi-width histograms,
similar to the construction illustrated in Figure 1. We con-
sider datasets whose elements are in the normalized domain
[0, 1)2. Our construction is build on publishing equi-width
histograms. An equi-width histogram in 2D corresponds to
the partition B = {b1,1, b1,2, . . . , bk,k}, where each bin bi,j is
a square region [ i−1
k ) × [ j−1
k , j
k , i
k ).
2
One may see this from expression (5), where adding a row to A
without increasing △A will not increase the variance.
163We propose publishing a series of overlapping histograms
where each histogram is shifted by an oﬀset δ from the previ-
ous histogram in the series. Speciﬁcally, let B0, B1 . . . Bm−1
be a sequence of partitions, where m = ⌈ 1
⌉ and Bx is a
partition {bx
} with each bx
i,j is a square
k + δ, y + j−1
region [ i−2
k + xδ).
1,2, . . . , bx
k + xδ) × [ j−2
k + xδ, i−1
1,1, bx
Note that the sensitivity of A constructed in this way is
4, instead of 2 as demonstrated in Figure 1. This is because
in 2D spatial data, there are edges connected the vertices
bi,j and bi+1,j+1. However, we will show in the next section
that, when δ is relatively small, the insertion of additional
′
a
can overcome the increment of the sensitivity.
k+1,k+1
kδ
1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1
1 1 0 0
0 0 1 1
1 1 1 1
0
0
1 -1
1
1
1
1
-1
1
0
0
-1 -1
1
1
1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1
1 1 1 1
0 1 1 1
0 0 1 1
0 0 0 1
H4
Y4
I4
C4
Figure 2: Strategy H4, Y4, I4 and C4.
4. When δ = n, it performs similar to identity matrix, but
when δ is small, we can reduce the errors by exploiting the
δ-neighbours.
Table 4: Max and total errors.
Hn
Yn
Θ( log3n
max error
)
total error Θ( n2log3n
ϵ2
ϵ2
Θ( log3n
)
) Θ( n2log3n
ϵ2
ϵ2
Cn,δ
In
Θ( n
Θ( δ
ϵ2 )
ϵ2 )
ϵ2 ) Θ( n2δ
) Θ( n3
ϵ2 )
′
Figure 1: Demonstration of adding a
increasing sensitivity.
to A without
5.4 Evaluation
5.4.1 1D range query
The earlier example described in Section 5.1 can be gen-
eralized to publish linear transformation of histograms with
n bins. The transformation A is a lower triangular matrix
of size n × n and the entries on and below diagonal are 1.
Essentially, row i of A cumulates the counts for bin 1 to
bin i. Let us call this strategy Cn. The answer to a range
query that covers bin i to j can be obtained by subtracting
the j-th row and (i − 1)-th row. We are interested in how
accurate Cn performs in answering 1D range queries, i.e. in
answering the set of all range queries, Q.
Li et al. [15] consider the maximum error and total error
of three strategies: Hn which queries a series of equi-width
histograms [11], Yn which is a Haar wavelet transformation
matrix [24] and the identity matrix In. Figure 2 shows H4,
Y4, I4 and C4. The maximum error refers to the maximum
variance among all row vectors of Q, and total error refer-
s to the sum of the variance. The asymptotic bounds on
the errors of Hn, Yn and In are as shown in Table 4. The
constructions do not exploit δ-neighbourhood, and the er-
rors of Hn, Yn and In are the same under either standard
neighbourhood or δ-neighbourhood.
Cn beneﬁts from δ-neighbourhood, in the sense that the
sensitivity △Cn is lower for smaller δ. The corresponding
maximum error and total error of Cn,δ is also shown in Table
2D range query
5.4.2
We consider mechanisms that answer 2D range queries
with ﬁxed range size in [0, 1)2. A 2D range query of size
s asks for the number of points in the region [x − s
2 , x +
2 ) × [y − s
2 ). We compare the algorithm described
in Section 5.3 with the equi-width histogram method as a
{b1,1, b1,2, . . . , bk,k}. Letecx be the published frequency counts
baseline.
Recall that an equi-width histogram is the partition B =
2 , y + s
s
in bin bx.
Given a range query q, we estimate the answer to q as:
)
(|bx ∩ q|
|bx|
∑
bx∈B
cx
.
(8)
where |bx| is the area of bx. Note that if the query partially
intersects with a bin, that bin contributes proportionally to
the answer. Our proposed method answers a range query
in a similar way, but using the average of the series of equi-
width histograms.
We conduct experiments on three 2D datasets. Dataset
1 contains locations of Twitter users in the world [12]. The
dataset contains over 193,841 Twitter users’ data from the
period of March 2006 to March 2010. Dataset 2 contains the
locations of users in dataset 1 cropped at the North Amer-
ican region. It contains locations of 183,072 users. Dataset
3 [13] contains 164,860 tuples collected from tags that con-
tinuously record the location information of 5 individuals.
The data points are normalized to the space M = [0, 1]2,
and Figure 3(a), 3(b) and 3(c) illustrate the distributions
of the data points for dataset 1, 2, and 3 respectively. To
avoid clogging, only 5% of the points (randomly selected)
are plotted for each dataset.
For the ﬁrst two datasets, we consider two cases where
δ = 0.001 and δ = 0.0001, which translate to a bound of
approximately 40 and 4 kilometers for dataset 1, and 5 and
0.5 kilometers for dataset 2 respectively. For dataset 3, we
consider δ = 0.01 and δ = 0.001, which translate to 500
and 50 meters. We evaluate the construction described in
164(a) Randomly selected 5% of points
from Dataset 1.
(b) Randomly selected 5% of points
from Dataset 2.
(c) Randomly selected 5% of points
from Dataset 3.
Figure 3: The 2D location datasets.
(a) Dataset 1.
(b) Dataset 2.
(c) Dataset 3.
Figure 4: The mean square error of range queries in linear-logarithmic scale.
Table 5: Query range and corresponding best bin-
width for the Dataset 1.
Query
Mean Square Error
range
1 0.01
0.2
2 0.01
t
e
s
a
t