### Restriction-Based and Probability-Based Prediction Techniques

Our prediction technique consists of two levels: restriction-based prediction and probability-based prediction. The first level, called restriction-based prediction, leverages the abundance of self-loops to accurately guess responses to membership queries. The second level, called probability-based prediction, exploits the fact that the same input messages often lead to the same output message. Any possible prediction errors are detected using random sampling (Section 3.1). If an error is made in predicting the responses to membership queries, our algorithm backtracks to the first erroneously predicted membership query, corrects the error (based on responses from sampling queries), and continues running L*. Importantly, the same sampling queries can be used both to detect missing states (as in the classical Angluin probabilistic sampling [3]) and mispredictions.

#### Formal Presentation of Prediction Techniques

We begin with a formal presentation of our prediction techniques by showing that entries in the S set have no self-loops, and therefore can be used for predicting responses that have self-loops.

**Theorem 1.** Let \( s \in S \) and \( s = c_0 \cdot c_1 \cdot \ldots \cdot c_n \) be a string of \( n \) characters from \( \Sigma_I \). Let \( q_0, q_1, \ldots, q_n, q_{n+1} \) be a sequence of states visited by \( \delta^* (q_0, s) \). For \( 0 \leq i < n + 1 \), every two adjacent states are different, i.e., \( q_i \neq q_{i+1} \).

**Proof.** The proof is by induction on the string length. The empty string \( \varepsilon \) trivially satisfies the condition, as there is only one state in the sequence of visited states. Let \( s = c_0 \cdot \ldots \cdot c_k \) and \( s \in S \). Algorithm 1 appends \( a \in \Sigma_I \) to \( s \), and then computes responses to membership queries \( s \cdot a \cdot u \), where \( u \in E \) are the columns of \( T \). Without loss of generality, let \( s \cdot a \) be the smallest string according to our lexicographic ordering (Definition 5). String \( s \cdot a \) is moved to \( S \) (Line 3) only if \( \forall t \in S, \exists u \in E : T(t, u) \neq T(s \cdot a, u) \). Every \( s \cdot a \) string is obtained by extending strings from \( S \), so we know that \( s \in S \). Thus, we conclude: \( \exists u \in E : T(s, u) \neq T(s \cdot a, u) \), which implies that \( \delta^* (q_0, s \cdot u) \neq \delta^* (q_0, s \cdot a \cdot u) \), because \( s \) and \( s \cdot a \) are (eventually) both in \( S \) and they denote different states. Since \( s \cdot a \cdot u \) and \( s \cdot u \) have the same prefix and suffix, symbol \( a \) must explain the difference in the response of the state machine. By the induction hypothesis, \( \forall 0 \leq i < k : c_i \neq c_{i+1} \). So, we have: \( \delta^* (q_0, s \cdot a) = c_0 \cdot \ldots \cdot c_k \cdot c_{k+1} \). Assume \( c_k = c_{k+1} \). Thus, \( \delta^* (q_0, s \cdot a) = \delta^* (q_0, s) \), a contradiction. Therefore, in \( \delta^* (q_0, s \cdot a) \), the last two visited states are different, which proves the theorem.

**Corollary 1.** From Definition 6, it follows that \( \delta^* (q_0, s) \) for \( s \in S \) has no self-loop transitions.

Intuitively, Theorem 1 suggests that strings from \( S \), which are loop-free, could be used to predict responses to input strings that are similar to strings from \( S \), but have additional symbols producing self-loop transitions. We formalize this intuition using the following two definitions.

**Definition 7 (Differentiating Set).** Let \( D \subseteq \Sigma_I \) be a set of all symbols in \( S \), more precisely, \( D = \{c \mid c \in [s], s \in S\} \). As \( S \) grows during the execution of Algorithm 1, \( D \) is a monotonically increasing set.

**Definition 8 (Restriction Function).** Let \( a \in \Sigma_I \) and \( s \in \Sigma^*_I \). The restriction function \( \rho : \Sigma^*_I \times D \to \Sigma^*_I \) is defined recursively as follows:
- \( \rho(s, D) = \varepsilon \) if \( s = \varepsilon \)
- \( \rho(s, D) = \rho(r, D) \) if \( s = a \cdot r \) and \( r \in \Sigma^*_I \) and \( a \notin D \)
- \( \rho(s, D) = a \cdot \rho(r, D) \) if \( s = a \cdot r \) and \( r \in \Sigma^*_I \) and \( a \in D \)

Intuitively, the restriction function deletes the symbols that are not in a given set from a string. For example, \( \rho(a \cdot b \cdot b \cdot a \cdot c \cdot d, \{b, d\}) = b \cdot b \cdot d \).

Now, our restriction-based prediction rule can be simply stated as: Given any membership query \( s \cdot a \cdot u \), compute \( s' = \rho(s \cdot a, D) \), and if \( s' \) already exists in \( S \cup S \cdot \Sigma_I \), use the values in the \( s' \) row of table \( T \) to predict the values for the \( s \cdot a \cdot u \) row. More formally:
\[ T(s \cdot a, u) = T(\rho(s \cdot a, D), u) \]
if the \( \rho(s \cdot a, D) \) entry exists in \( T \).

With this simple rule, we achieve highly accurate predictions with few errors.

#### Experimental Results

The restriction-based prediction saves around 73% of membership queries in our experiments with MegaD. Analyzing the results, we identified one missed prediction opportunity. If the restricted input string does not exist in the table, the previously presented technique is helpless. To improve the performance of the restriction-based prediction further, we track the set of observed response messages for every input symbol. When the restriction-based approach fails, we apply a simple probability-based prediction: If a particular input message produces the same output message for all previous queries, we predict that the response will be the same. If multiple different responses were observed for the same input, we do not predict it. It would be possible to lower the prediction threshold—say, by picking the response that happens in at least 90% of cases—at the cost of increasing the number of erroneous predictions and the cost of backtracking. We leave this fine-tuning for future work. Even with the simple probability-based prediction currently implemented, we gain an additional 13% reduction in the number of membership queries on MegaD, in addition to savings achieved by the restriction-based prediction.

Mispredictions can produce erroneous state-machine conjectures. However, we exploit the same random sampling equivalence checking mechanism in L* (Section 3.1) to detect mispredictions. Thus, mispredictions are guaranteed to be found with desired accuracy \( \varepsilon \) and confidence \( \gamma \). Once an error is detected, L* backtracks to the first erroneously predicted query, fixes it using the sampling query response, removes all subsequently predicted entries from the observation table, and continues the inference process. All the prediction savings, both in the prior discussion and in the experimental evaluation, take the cost of backtracking into account. Therefore, our prediction is very effective (86% total reduction in the number of queries) and accurate (inaccurate prediction would require more frequent backtracking, reducing the savings).

### Determinism, Resettability, and Sampling

This section describes the non-standard and non-obvious aspects of using L* in our setting. Specifically, we discuss the impact of the determinism and resettability assumptions (Section 2.3) and the role of the sampling process in achieving the desired accuracy of the model.

Both the determinism and resettability assumptions were relatively easy to satisfy in our setting. In our experiments, the exchange of messages was deterministic, except for one corner case: sometimes master servers respond with an arbitrarily long sequence of INFO messages, which are always terminated with a non-INFO message. Our inference infrastructure discards all the INFO messages and treats the first non-INFO message as the response. This was the only source of non-determinism we encountered. To reset the state machine, we begin both membership and sampling queries with an INIT message (Table 1), which initiates a new session. Once the session is started, every input message produces a response—an output message.

As discussed in Section 3.1, we use a sampling-based approach for equivalence queries. We generate uniformly distributed random sequences of input messages, the number of which is determined by the desired model accuracy and confidence [3]. Once our implementation of L* closes the table, it conjectures a state machine, which is then tested through sampling. The responses to sampling queries are never predicted, as the purpose of sampling queries is to discover new states that do not exist in the currently conjectured model and to discover prediction errors.

### Analysis of Inferred Models

In this section, we analyze the complete protocol models obtained from our inference technique with the goal of gaining deeper understanding of MegaD. We present techniques to analyze the protocol models to identify the critical links in botnet C&Cs, design flaws, the existence of background communication channels between C&C servers, and to identify implementation differences for fingerprinting and flaw detection.

#### Identifying the Critical Links

Transitions in our Mealy machine models represent actions. Certain actions might be considered bad, in the sense that they represent malicious or undesirable activities. In Mealy machines, such activities are represented with transitions (more precisely, output responses). Once the bad transitions are identified, we wish to find a way to prevent such transitions from ever being executed.

More formally, given a protocol state machine \( M = (Q, \Sigma_I, \Sigma_O, \delta, \lambda, q_0) \) and a set of bad output symbols \( B \subseteq \Sigma_O \) representing bad actions, we wish to identify the minimal number of transitions we have to disrupt to prevent the bots from executing transitions that would produce output symbols from \( B \). There are two ways (not mutually exclusive) of achieving this. The first option is to make it impossible for bots to reach the states from which bad actions could be performed by cutting a set of transitions in the state machine, i.e., we wish to assure that \( \forall s \in \Sigma^*_I, a \in \Sigma_I : \lambda(\delta^*(q_0, s), a) \notin B \). The second option is to disrupt the bad transitions themselves, i.e., to remove the transitions so as to assure that the following property holds: \( \forall s \in \Sigma^*_I : [\lambda^*(q_0, s)] \cap B = \emptyset \).

Such an analysis can be done using max-flow min-cut algorithms [11], such that the initial state is a source, and the state at which the bad transition originates is a sink. We performed this analysis on the MegaD state machine and arrived at a trivial conclusion for a single pool of bots: since MegaD is a spamming botnet and a single spamming edge is the only bad edge, taking down any one of the botnet servers and the corresponding transitions in the state machine would prevent a pool of bots from spamming. However, since different pools of bots talk to different sets of servers, it does not stop other pools of MegaD bots from spamming. Unsatisfied with this outcome, we attempt to develop an approach that works across multiple pools of bots.

We extended the encoding of messages into alphabet symbols shown in Table 1 by partitioning the set of messages into disjoint sets, one set per server, so as to include the IP addresses of the servers as an additional field. We refer to this extended alphabet as IP-extended. We ran our inference technique independently on both master servers we have access to (each pool of bots talks to a different master server) and computed a projection (defined below) of one state machine onto the IP-extended alphabet of the other.

**Definition 9 (State-Machine Projection).** The projection of a finite state machine \( M = (Q, \Sigma_I, \Sigma_O, \delta, \lambda, q_0) \) onto alphabet \( \Sigma_A \) is defined as a non-deterministic finite state machine \( M' = (Q, \Sigma_I \cap \Sigma_A, \Sigma_O, \delta', \lambda', q_0) \), such that the following holds for \( \forall a \in \Sigma_I, x \in \Sigma_O, q_i, q_j \in Q \):
- \( (q_i, \varepsilon, q_j) \in \delta' \) if \( (q_i, a, q_j) \in \delta \) and \( a \notin \Sigma_A \)
- \( (q_i, a, q_j) \in \delta' \) if \( (q_i, a, q_j) \in \delta \) and \( a \in \Sigma_A \)
- \( (q_i, \varepsilon, \varepsilon) \in \lambda' \) if \( (q_i, a, x) \in \lambda \) and \( a \notin \Sigma_A \)
- \( (q_i, a, x) \in \lambda' \) if \( (q_i, a, x) \in \lambda \) and \( a \in \Sigma_A \)

Intuitively, all transitions of \( M \) on alphabet symbols not in \( \Sigma_A \) are replaced with non-deterministic transitions (\( \varepsilon \)), and the corresponding outputs are replaced with empty outputs (\( \varepsilon \)). The resulting state machine may be non-deterministic.

Computing a projection of a state machine inferred from communication of our bot emulator with one master server onto the alphabet of the machine inferred from communication with another master server, we identified the key components shared among multiple pools of bots. The results are presented in Section 6.2.

#### Identifying Design Flaws

We identified a design flaw in the MegaD protocol, thanks to the fact that our inference approach infers complete state machines. Given a complete state machine and a specification (i.e., a set of properties expressed in a suitable formal logic), it is possible to automatically determine whether the properties hold using automatic model checkers (e.g., [9]). In our case, the state machines were simple enough that we could manually check a number of interesting properties. We explain the flaw we found later in Section 6.3.

#### Identifying Background-Channels

In situations when a client (a bot, in our case) talks to multiple servers, it might be interesting to prove whether there exists any background communication between the servers. Such background communication channels can indicate infiltration traps, which security researchers need to be aware of before attempting to bring a botnet down, or simply reveal interesting information about the protocol.

To detect the background-channels, we devised the following analysis: We restrict our bot emulator to communicate only with a single server at a time and infer the protocol model \( M_T \) (for the template server), \( M_S \) (for the SMTP server), and \( M_M \) (for the master server). Then, we allow our bot emulator to communicate with all the servers and compute the model \( M \). We compute the projection (Definition 9) of \( M \) onto input alphabets used for building individual server communication models (\( M_T, M_S, \) and \( M_M \)) and compare the obtained projection with the model of communication with the individual servers. Any differences imply that there exist background communication channels. We prove the existence of communication between MegaD's servers in Section 6.4.

#### Identifying Implementation Differences

Once the complete models of two different implementations of the same protocol are computed, comparison of the models can reveal interesting deviations useful for fingerprinting and flaw detection. While it is possible to perform automatic equivalence checking of large finite-state models (e.g., [24]), our models were simple enough that we can do such an analysis manually. We discuss the differences between Postfix SMTP 2.5.5 and MegaD’s implementation in Section 6.5.

### Experimental Evaluation

We implemented our version of L* in approximately 1.7 KLOC of C++ and the bot emulator and experimental infrastructure in approximately 2.3 KLOC of Python and Bash scripts. Our prototype performs up to eight parallel queries (as shown in Figure 3) concurrently tunneled through Tor [15]. We conducted the experiments over a period of three weeks starting March 27th, 2010. Figure 5 illustrates the inferred MegaD protocol state machine.

In the rest of this section, we evaluate our protocol inference approach on the MegaD botnet C&C distributed system, MegaD’s non-standard implementation of SMTP, and the standard SMTP as implemented in Postfix 2.5.5. We present the results of our analysis of inferred complete models and validate our inference approach by comparing the inferred SMTP models against the SMTP standard.

#### Performance and Accuracy

In this section, we present the experimental evidence of the effectiveness of our response prediction technique and discuss the model accuracy.

**Figure 5: Protocol State-Machine of MegaD’s Distributed Command and Control System.** Self-transition edges are removed for clarity. The state-machine transitions are labeled according to the alphabet in Table 1. For example, 15/13 denotes NOTIFY / NOTIFY_RECVED. The process of spamming is triggered in state 13, through self-edges 1/10, 12/8, 11/-, 15/13.

The prediction results are shown in Table 2. The overall reduction in the number of queries that have to be sent over the network is between 24.5% (for MegaD’s SMTP) and 86.1% (for MegaD’s C&C). We believe there are two main reasons our prediction is much more effective on the MegaD C&C than on SMTP: First, C&C is a more complex protocol that involves three different types of servers, two of which use proprietary protocols. Second, our understanding of the two protocols when we were designing message abstractions was very different—we knew nothing about the C&C state machine, while the SMTP state machine is well known [23]. We believe this inherent lack of knowledge about an unknown protocol model, yet to be inferred, results in some amount of redundancy. However, it is important to be conservative when abstracting messages, as otherwise, it is easy to miss important states and transitions. This inherent tradeoff between accuracy and redundancy makes our prediction technique even more valuable, as we can infer larger protocols without sacrificing accuracy. As a matter of fact, since sending and receiving a single message through Tor took around 6.8 seconds on average, 86.1% prediction accuracy means that our response predictor saved \((56,716 - 6,406) \times 6.8 / (3600 \times 24) = 3.95\) days of computation, reducing the total amount of time required to infer the MegaD C&C to around 12 hours.

Parallelization of the experiment improved the performance even further. While a single bot emulator would return a response message every 6.8 seconds on average, eight parallel bots would return a message every 1.4 seconds, a 4.85X improvement on average in addition to improvements obtained by response prediction.