samples
In addition, we passed these
through
ShellForge’s [2] ASCII encryption engine to gen-
erate 2000 additional samples. Finally, we added four
port-80 worms which we had access to: Code-Red,
Code-Red II, IISMedia and IISWebdav. These
worms propagate through the URI and message body and
attack web-servers at the memory layer. Roughly half
of the samples for L/RFI were actual captured attacks
against our servers, the same for roughly a quarter of the
SQL-injection attacks. We collected the remaining
samples from sites that host web-exploit code2. As
previously mentioned, every instance is unique.
2milw0rm.com, xssed.com, databasesecurity.com
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
y
c
a
r
u
c
c
A
n
o
i
t
c
e
t
e
D
2−gram
3−gram
5−gram
7−gram
11−gram
13−gram
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
y
c
a
r
u
c
c
A
n
o
i
t
c
e
t
e
D
2−gram
3−gram
5−gram
7−gram
11−gram
13−gram
0
0
0.01
0.02
0.03
False−Positive Rate
0.04
0.05
0
0
0.01
0.02
0.03
False−Positive Rate
0.04
0.05
(a) Student server - File-Inclusion.
(b) Dept. server - File-Inclusion.
Figure 6. ROC (cid:173) The only poor performance case was the PHP ﬁle inclusion attack, due to the fact that
this particular class of attack does not require actual malcode, rather only the address of the malcode;
making them much harder to detect. Notice that increasing gram size makes a big difference.
(S) PayL (S) SG-5
(D) PayL (D) SG-5
Server
Total Requests
False Positives
Department
Student
2,652,262
4,206,176
118
287
Table 3. FP rates for the full dataset of re(cid:173)
quests collected over one month. The sev(cid:173)
eral orders of magnitude between Tables (1)
and (2) (1% vs 0.00006%) is due to the distri-
bution of the data. The overwhelming majority
of web(cid:173)requests are easy to classify correctly
and using unique samples is needed to accu(cid:173)
rately evaluate the capacity of the classiﬁer
without bias from the sample distribution.
Table (1) shows our accuracy results for Spectrogram
with a mixture of ﬁve Markov chains and gram size 10,
abbreviated as SG-5. This setting of the sensor was cho-
sen for a good balance in empirical accuracy and perfor-
mance speed. Our split ratios yields roughly 15, 131 train-
ing and 796 distinct testing samples for the student server,
and 3, 127 training and 165 testing samples for the depart-
ment server. In each experiment the datasets are random-
ized at this ratio and the average of ﬁve trials are reported.
The Anagram results highlight the main problem of under-
ﬁtting when we increase gram-sizes to increase the power
of the classiﬁer. The results show that Anagram is detecting
the attacks with higher accuracy when we use larger grams
but at the same time it can no longer generalize well. In con-
trast, Spectrogram’s Markov-chain factorization admits
Attack
L/RFI
JS XSS
SQL-Inj.
Shellcode
ASCII Shc.
Code-Red
Code-Red II
IIS-Media
IIS-Webdav
5%
11%
76%
100%
100%
√
√
√
√
78%
99%
98%
100%
100%
√
√
√
√
5%
9%
75%
100%
100%
√
√
√
√
74%
99%
97%
100%
100%
√
√
√
√
Table 1. Accuracy comparison with FP rate
held at 1%. Unique samples used to unbias
the data distribution. In all tables, (S) denotes
the student server and (D) denotes the depart(cid:173)
ment server. Compare with Anagram shown
in Table (2). See Table (3) for FP rates on the
full datasets.
(S) FP
XSS
Anagram L/RFI
96%
14%
14%
2-Gram
96%
3-Gram
99% 100% 96%
99% 100% 100% 98%
4-Gram
5-Gram
100% 100% 100% 99%
SQL
98%
(D) FP
75%
95%
97%
98%
Table 2. Results for Anagram; all worms were
detected.
The high FP is expected since
the input is short and dynamic. Compare
with Spectrogram (SG-5) which achieves the
same level of detection at 1% FP.
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
y
c
a
r
u
c
c
A
n
o
i
t
c
e
t
e
D
2−gram
3−gram
5−gram
7−gram
11−gram
13−gram
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
y
c
a
r
u
c
c
A
n
o
i
t
c
e
t
e
D
2−gram
3−gram
5−gram
7−gram
11−gram
13−gram
0
0
0.01
0.02
0.03
False−Positive Rate
0.04
0.05
0
0
0.01
0.02
0.03
False−Positive Rate
0.04
0.05
(a) Student server - Javascript XSS.
(b) Dept. server - Javascript XSS.
Figure 7. ROC (cid:173) Detection of Javascript XSS.
much higher gram sizes while maintaining low FP rates on
the unbiased dataset. In raw numbers, this 1% false positive
rate translated to roughly 8 false positives on average on the
student server dataset and for the department server, 2 FPs.
The different sensors were all evaluated in the same man-
ner. Good results were noted in all of the experiments ex-
cept the L/RFI attacks. This is because PHP ﬁle inclusion
attacks do not require actual attack code, just the address of
the code since the nature of the exploit allows remote code
fetching and execution. Due to this fact, detection is much
harder but still possible as Spectrogram, to some degree,
learns that URL inputs into PHP scripts are not legitimate
as results show (also c.f. our discussion on evasion tactics).
Figures (5,6,7,8) show the ROC curves for SG-5, evalu-
ated over a range of attack datasets and gram sizes. These
curves demonstrate accuracy for the spectrum of FP rates.
Here we can partially compare with the works of Kruegel
et al. [15, 16]. The 2-gram curves partially represent their
sensor. We say partially because 2-grams is only one part of
their framework. At the same time, our tests used 2-grams
in a mixture model while they did not. Their sensor also op-
erates on Apache log ﬁles so an exact comparison is not pos-
sible. The 2-gram plots are included to drive the point that
larger gram-sizes improves performance. Figure (6) and (8)
show continued improvement (larger accuracy at the same
FP rate) as we increase the gram size. Note that while the FP
rate for Anagram jumped into the 90’s with only 3-grams
on the unbiased dataset, Spectrogram showed continued
improvements in accuracy, in most cases, even beyond 10-
grams. In practice, the optimal gram size can be estimated
through cross-validation and by generating the same ROC
plots as those shown in this paper to ﬁnd the favorable pa-
rameter settings.
5.2 Runtime
Table 4 shows the run-times for Spectrogram at vari-
ous gram sizes when running on 15,927 samples. Note that
training time will depend on the data since we’re using a
gradient ascent learning algorithm. The convergence rate
is a factor and it may take longer with different choices of
model parameters, as can be seen from the table – a 2-gram
model took longer to train because that the model did not ﬁt
the data well and thus the training took longer to stabilize.
The cost of reconstructing content ﬂow from network layer
packets is greatly reduced with the use of the tcpflow [9]
library, which is capable of reconstructing nearly 40,000 re-
quests per second on a 3Ghz machine.
5.3 Discussion
Data Normalization: A reduction in the amount of un-
useful features within the data is helpful to improve the
signal-to-noise ratio. This procedure is mostly ad hoc and
should be customized for each server. Methods which we
found effective include: un-escaping each string, remov-
ing white-space and numbers, and reduction into lower-
case. These operations serve to make the input space tighter,
by making samples from different sub-classes of legitimate
content appear as similar to each other as possible. It also
serves to to mitigate the efﬁcacy of some obfuscation meth-
ods. Stability is the ultimate goal of this procedure and
in deployment, normalization should be tweaked depending
on the type of data the monitored web-server(s) observe.
Mixture and gram sizes: As we can see from the ROC
curves, there is an obvious beneﬁt to be gained from using
larger gram sizes. Clear improvements are observable over
the 2-gram model previously studied. The ability to model
1
0.9