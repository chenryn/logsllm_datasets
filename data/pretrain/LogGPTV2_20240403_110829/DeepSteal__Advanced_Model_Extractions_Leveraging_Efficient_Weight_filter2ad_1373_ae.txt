-
72.22
74.43
76.77
85.62
87.19
88.17
90.16
93.11
72.85
75.2
77.53
86.72
88.39
89.27
91.8
100.0
Accuracy
Under
Attack
(%)
62.69
55.99
53.47
3.93
4.61
1.44
1.03
0.0
Time
(days)
Accuracy
(%)
Fidelity
(%)
-
3.9
3.4
7.8
6.7
10.4
8.9
-
70.76
72.3
72.67
81.03
80.15
84.59
81.56
89.96
72.06
73.34
73.89
82.88
81.52
86.24
83.33
100.0
Accuracy
Under
Attack
(%)
61.19
62.24
58.19
36.45
26.85
16.87
18.55
4.63
recovery at 3000 rounds of HammerLeak attack as well.
Next, we evaluate the adversarial attack performance for
both 3000 and 4000 rounds of HammerLeak attack. In Ta-
ble III, we show that our substitute model can transfer effective
adversarial samples to the victim model across all three archi-
tectures. In particular, for ResNet models, our substitute model
generates adversarial examples demonstrate close to the white-
box attack efﬁcacy (i.e., within 2% of the best case result)
with 4000 rounds of HammerLeak attack. As for VGG model,
which is already known as a robust architecture [67], our
substitute model generated adversary reaches within ∼12%-
14% of an ideal white-box attack. Nevertheless, our attack
efﬁcacy still shows an improvement of about ∼25%-60%
across all three architectures in comparison to the baseline
(i.e., architecture only) technique.
Finally, we consider an attack scenario where the attacker
has a strict time budget. In this scenario, let us assume he/she
can only afford to run 1500 rounds of HammerLeak attack
while prioritizing MSBs (e.g., only 3.9 days of attack time).
As summarized in Table III, even such a restricted attack
can generate effective adversarial examples to lower accuracy
under attack by 7%-11% for ResNet models and by 3% for
VGG-11 compared to baseline. One key observation for this
low budget (i.e., 1500 round attack) attack is that attacker can
generate a much more effective substitute model by only using
MSB information rather than all the bits. The reason being
with limited bit information (e.g., 50% MSB only) putting
strict penalization (i.e., mean clustering) on the weights during
training does not help the substitute model accuracy. In fact,
for VGG-11, it becomes worse than the baseline method. As a
result, for DeepSteal attack with limited partial bit information,
using the relaxation of the weight constraints (i.e., MSB only)
can be more effective than using all the available ﬁltered bits.
D. Comparison to State-of-the-art Techniques
In Table IV, we summarize the standing of our DeepSteal
attack compared with existing model recovery methods for
three different domains of applications. We can see exist-
ing model regularization [11], [41] and data augmentation
Fig. 10: Distribution of weights with MSB recovered across 21
individual layers of ResNet-18.
baseline case, we assume the attacker only knows the victim
model architecture. Then, a substitute model with the same
architecture is trained using a similar setting (i.e., less than
8% available data). On the other hand, we treat the white-
box case as the best-case scenario where the attacker knows
all information (i.e., weights, biases and architecture) of the
victim model. In summary, with more recovered weight bits,
our DeepSteal achieves better accuracy, ﬁdelity and adversarial
example attack efﬁcacy. For instance, our substitute model
can generate effective transferable adversarial example with
similar efﬁcacy (i.e., ∼0%) as white-box attack for both
ResNet-18 and ResNet-34.
In our evaluation, the residual victim models (ResNet-18
& ResNet-34) have 93.16% & 93.11% inference accuracy,
respectively. As shown in Figure 9, after 4000 rounds of
HammerLeak attack, the adversary could recover 90% of the
MSB bits (∼11.52% of total bits). By only utilizing the leaked
MSB bits, the attacker can recover up to 89.05/88.17% test
accuracy for the ResNet (18/34) models. Additionally, for the
All Bits case in Table III, after paying an additional time
cost (i.e., 1.66×), the performance of DeepSteal attack only
exhibits marginal improvement on residual models. In contrast,
the larger model with a different architecture topology (i.e.,
VGG-11 with 132 Million parameters) highly beneﬁts from the
additional information of all the bits. For VGG, we observe a
∼3% improvement by using all the ﬁltered bits in comparison
to using MSB only. We observe a similar pattern in accuracy
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:59:25 UTC from IEEE Xplore.  Restrictions apply. 
1167
TABLE IV: We evaluate DeepSteal attack against state-of-the-art techniques across three different domains as case studies.
In each of the cases, only our attack performs onpar with the SOTA methods across all three evaluation metrics.
Case Study
Method
Objective
Model
Accuracy
(%)
Fidelity
(%)
Accuracy
Under
Attack (%)
Regularization
& Data Augmentation
Fully-Supervised [41]
Rand-Augment [60]
Auto-Augment [61]
DeepSteal (ours)
Model Extraction
Side Channel [25], [28], [43], [45]
DeepSteal (ours)
Accuracy/Fidelity
Accuracy
Accuracy
WideResNet-28
WideResNet-28
WideResNet-28
Accuracy/Fidelity/Attack WideResNet-28
Accuracy/Fidelity/Attack
Accuracy/Fidelity/Attack
Input Attack
Black-Box (Inception-V1) [71]
White-Box (PGD/Trades) [67], [72]
DeepSteal (ours)
Adversarial Attack
Adversarial Attack
Adversarial Attack
86.51
87.4
87.7
91.93
72.68
90.02
-
-
-
87.37
93.45
73.59
91.67
-
-
-
-
-
-
-
-
0.05
62.58
1.2
20.47
0.0
1.2
ResNet-18
ResNet-18
ResNet-18
ResNet-18
ResNet-18
in training deep models
techniques [60], [61] are useful
with limited data. However, our substitute model achieves a
much higher accuracy (i.e., ∼3%). Other existing side channel
attacks [25], [26], [28], [43], [45] fall into a similar attack
category as DeepSteal. Among them, [26] is only applicable
to binary neural networks. On the contrary, our attack is a more
general version of the attack applicable to any bit-width. Other
side channel attacks [25], [28], [43], [45] focus on recovering
the architecture and then training the model with limited data.
To compare with them, we assume the attacker knows the exact
model architecture. Our DeepSteal can leverage the leaked
weight bits to further improve the attack efﬁcacy. From this
point, our attack certainly outperforms such prior architecture-
only model extraction attacks with ∼18% improvement in
accuracy and ∼61% improvement in degrading the accuracy
under adversarial attack. Note that while DeepSteal
is a
semi-blackbox attack, DeepSteal actually can achieve 1.2%
accuracy under attack, which is extremely close to a white-box
attack performance (i.,e., 0%). We observe a 19% improve-
ment in attack performance compared to a powerful black-box
substitute model (e.g., Inception-V1) attack.
E. Impact of Bit Stealing Errors
Bit stealing accuracy can be inﬂuenced in case expected bit
ﬂips do not occur. In this section, we analyze the bit errors in
the rowhammer-based side channel. Speciﬁcally, we proﬁle
bit errors on 4 different vulnerable DIMMs with random
bits set in the victim’s pages. Note that HammerLeak only
leverages Strongly-leakable cells that exhibit consistent bit
ﬂips in double-sided rowhammer. Our analysis reveals that
about 70% of the ﬂippable DRAM cells fall into this category.
Our results show very high and stable bit stealing accuracy –
on average 95.7% – across all tested DIMMs.
To quantify the impact of bit errors, we perform an analysis
by evaluating the effectiveness of DeepSteal under a range
of bit error rates. Speciﬁcally, under the setting where 90%
of raw MSB bits are exﬁltrated by HammerLeak, we inject
random errors at certain rate across each model layer into
the recovered bits. Figure 11 shows the performance of the
substitute model (in terms of Accuracy and Accuracy Under
Attack) when the bit error ranges from 0% to 10% for ResNet-
18. The results demonstrate that low bit error (0-5%) has
negligible effect on the performance of the substitute model
attack. Moreover, the accuracy of the substitute model stays
stable even as the error rate reaches 10%. On the other
hand, the increase in error rate (5-10%) in the recovered bits
causes the DeepSteal performance in Accuracy Under Attack
to degrade gradually. Nevertheless, we can still observe much
higher attack efﬁciency of DeepSteal compared to the baseline
approaches as shown in Table IV.
Fig. 11: Analysis of the impact of the recovered Bit Error Rate
(%) on DeepSteal attack performance for ResNet-18.
IX. DISCUSSION
A. Applicability of DeepSteal in DDR4 Memory
While our evaluation focuses on DDR3 memories, there is
no fundamental constraint in the rowhammer vulnerability that
limits DeepSteal from manifesting in DDR4-based systems.
This is because the root cause of the information leakage is the
data-pattern dependent bit ﬂip, which has been shown to exists
in DDR4 DIMMs in recent studies [73]–[75]. In fact, it has
been observed that due to device scaling, the effective number
of hammering needed to induce bit ﬂip in DDR4 devices is
signiﬁcantly reduced, hence making DDR4 DIMMs potentially
more vulnerable to rowhammer. Due to the deployment of
Targeted Row Refresh (TRR), rowhammer attacks on DDR4
DIMMs need to integrate certain hammer fuzzing patterns
with the activation of additional rows at a distance [76]. Such
additional rows are accessed for the purpose of bypassing TRR
while the target bit ﬂip is still induced by the two primary
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:59:25 UTC from IEEE Xplore.  Restrictions apply. 
1168
aggressor rows, which is essentially identical to the double-
sided rowhammering effect. Finally, same as DDR3-based
attacks, for each bit ﬂip, the more complex hammering pattern
in DDR4 would still need to complete within one refresh cycle.
Therefore, for the same amount of bit leakage, we expect
very similar exploitation time of DeepSteal in DDR4-based
systems.
B. Countermeasures for DeepSteal
1) Algorithm-level Mitigation: Effect of adversarial train-
ing on transferred adversarial sample. One potential approach
in defending against adversarial samples is to train the model
using the attacked samples popularly known as adversarial
training [67]. In table V, we evaluate the target victim model
which is defended with adversarial training. Naturally, a model
trained with adversarial examples becomes more resistant to
both white-box & black-box adversarial attacks. We observe
a similar pattern in our experiments. Still, our proposed
DeepSteal could achieve 4% & 6% improvement in attacking
the target model trained with the adversarial samples compared
to the baseline (i.e., architecture only + learning) for ResNet-
18 & VGG-11, respectively. We conclude that the existing
white-box adversarial defense may lower the transferability
of adversarial samples from our substitute model, but fails to
prevent the accuracy and ﬁdelity extraction.
In addition, our proposed DeepSteal follows a more strict
threat model which does not require access to output log-
its/predictions. In contrast, prior strong transferable adversarial
attacks [77]–[80] require model queries as well as access to
output logits/prediction. Even in this extremely limited setting,
our attack DeepSteal outperforms the existing model stealing
attacks (i.e., architecture only + learning) [28] in attacking a
well-defended (i.e., adversarial training) target model. Finally,
it
is worth noting that current adversarial defenses (e.g.,
adversarial training) comes with additional training cost and
inference accuracy degradation.
TABLE V: After adversarial training, the white-box accuracy
under attack improved to 43.12% & 35.71% for ResNet-18 and
VGG-11 models respectively . Here we report the performance
of DeepSteal attack using recovered bit information after 4000
rounds of Hammer-Leak attack.
Training Data
(%)
Baseline
DeepSteal
Baseline
DeepSteal
Accuracy
Fidelity
(%)
(%)
ResNet-18 (83.62%)
72.66
72.87
82.84(↑ 10)
81.67 (↑ 9)
VGG-11 (80.21%)
71.3
81.13 (↑ 10)
70.71
80.65 (↑ 10)
Accuracy Under
Attack (%)
80.42
76.78 (↓ 4)