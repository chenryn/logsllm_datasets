### Evaluation of DeepSteal Attack Performance

#### Adversarial Attack Efficacy
We evaluated the adversarial attack performance of our substitute model for both 3000 and 4000 rounds of the HammerLeak attack. Table III shows that our substitute model can effectively transfer adversarial samples to the victim model across all three architectures. For ResNet models, our substitute model generates adversarial examples with an efficacy close to that of a white-box attack (within 2% of the best-case result) after 4000 rounds of the HammerLeak attack. For the VGG model, which is known for its robustness [67], our substitute model achieves an attack efficacy within approximately 12%-14% of an ideal white-box attack. Overall, our attack efficacy shows an improvement of about 25%-60% compared to the baseline (architecture-only) technique.

#### Time-Budgeted Attack Scenario
In a scenario where the attacker has a strict time budget, we assume they can only afford to run 1500 rounds of the HammerLeak attack (e.g., 3.9 days). As summarized in Table III, even this restricted attack can generate effective adversarial examples, reducing the accuracy under attack by 7%-11% for ResNet models and by 3% for VGG-11 compared to the baseline. A key observation is that with limited bit information (e.g., 50% MSB only), using only the most significant bits (MSBs) during training results in a more effective substitute model than using all available bits. For VGG-11, using all bits actually degrades the substitute model's accuracy compared to the baseline method. Therefore, for DeepSteal attacks with limited partial bit information, using only the MSBs can be more effective than using all filtered bits.

#### Comparison to State-of-the-Art Techniques
Table IV summarizes the performance of our DeepSteal attack compared to existing model recovery methods across three different application domains. We compare our method with existing regularization [11, 41] and data augmentation techniques [60, 61], as well as other side-channel attacks [25, 28, 43, 45]. Our substitute model achieves higher accuracy (approximately 3%) compared to these methods. Other side-channel attacks focus on recovering the architecture and then training the model with limited data. In contrast, our DeepSteal leverages leaked weight bits to further improve attack efficacy. This results in an 18% improvement in accuracy and a 61% improvement in degrading the accuracy under adversarial attack compared to architecture-only model extraction attacks. Additionally, our semi-blackbox attack achieves 1.2% accuracy under attack, which is extremely close to a white-box attack performance (i.e., 0%). We observe a 19% improvement in attack performance compared to a powerful black-box substitute model (e.g., Inception-V1) attack.

#### Impact of Bit Stealing Errors
Bit stealing accuracy can be influenced if expected bit flips do not occur. We analyzed bit errors in the rowhammer-based side channel by profiling bit errors on four different vulnerable DIMMs with random bits set in the victim's pages. HammerLeak leverages strongly-leakable cells that exhibit consistent bit flips in double-sided rowhammer. Our analysis reveals that about 70% of flippable DRAM cells fall into this category, resulting in very high and stable bit stealing accuracy (on average 95.7%) across all tested DIMMs.

To quantify the impact of bit errors, we evaluated the effectiveness of DeepSteal under a range of bit error rates. Specifically, with 90% of raw MSB bits exfiltrated by HammerLeak, we injected random errors at certain rates across each model layer into the recovered bits. Figure 11 shows the performance of the substitute model (in terms of Accuracy and Accuracy Under Attack) when the bit error rate ranges from 0% to 10% for ResNet-18. The results demonstrate that low bit error rates (0-5%) have a negligible effect on the performance of the substitute model attack. Even as the error rate reaches 10%, the accuracy of the substitute model remains stable. However, the increase in error rate (5-10%) in the recovered bits gradually degrades the DeepSteal performance in Accuracy Under Attack. Despite this, DeepSteal still outperforms baseline approaches, as shown in Table IV.

### Discussion

#### Applicability of DeepSteal in DDR4 Memory
While our evaluation focuses on DDR3 memories, there is no fundamental constraint that limits DeepSteal from manifesting in DDR4-based systems. The root cause of the information leakage is the data-pattern-dependent bit flip, which has been shown to exist in DDR4 DIMMs in recent studies [73-75]. Due to device scaling, the number of hammering operations needed to induce bit flips in DDR4 devices is significantly reduced, making DDR4 DIMMs potentially more vulnerable to rowhammer. Targeted Row Refresh (TRR) deployment in DDR4 requires integrating certain hammer fuzzing patterns with the activation of additional rows to bypass TRR while still inducing the target bit flip. For the same amount of bit leakage, we expect similar exploitation times for DeepSteal in DDR4-based systems.

#### Countermeasures for DeepSteal
**Algorithm-level Mitigation:**
One potential approach to defending against adversarial samples is adversarial training [67]. Table V evaluates the target victim model defended with adversarial training. A model trained with adversarial examples becomes more resistant to both white-box and black-box adversarial attacks. Our proposed DeepSteal still achieves a 4% and 6% improvement in attacking the target model trained with adversarial samples compared to the baseline (architecture only + learning) for ResNet-18 and VGG-11, respectively. While existing white-box adversarial defenses may lower the transferability of adversarial samples from our substitute model, they fail to prevent the accuracy and fidelity extraction. Additionally, our DeepSteal follows a stricter threat model that does not require access to output logits/predictions, unlike prior strong transferable adversarial attacks [77-80]. Even in this limited setting, our attack outperforms existing model stealing attacks (architecture only + learning) [28] in attacking a well-defended (adversarial training) target model. It is worth noting that current adversarial defenses, such as adversarial training, come with additional training costs and inference accuracy degradation.