1 case, by simply scheduling two pairs of operations
(op1,0, op1,1), (op2,0, op2,1), where op1,0 and op2,0 are two
read requests for the same item, whereas op1,1 and op2,1 are
read requests for distinct items. Concretely, the adversary A
ﬁrst issues the request pair (op1,0, op1,1), delays the messages
sent by OClient right after the ﬁrst operation pair is processed,
schedules the second request pair (op2,0, op2,1), and delivers
the associated messages to SS, and its replies back to OClient
immediately. If this results in an answer to the second opera-
tion being triggered immediately, the attacker guesses b = 1,
otherwise it guesses b = 0. The outcome of the attack is
depicted in Figure 2.
h) Remarks: We note that to prevent the same attack
affecting CURIOUS, our system TaoStore will introduce the
notion of an operation sequencer, a module catching out-of-
order early replies from the ORAM client back to the caller,
for instance by ensuring that in our attack scenario from above,
also in the setting with two real reads, the ﬁnal response to
the second real read will not be sent before the response to
the ﬁrst real read. In other words, we will not happen to
modify the fake-read logic. Rather, we make sure that real
reads have response timings consistent with the behavior one
would observe if some of these are fake.
III. OVERVIEW OF TAOSTORE
This section provides a high-level overview of TaoStore and
its goals, including the deployment scenario and architecture
of our system.
High-level goal: The goal of TaoStore is to allow multi-
ple clients (or users) to securely and obliviously access their
shared data on an untrusted storage server (a “public cloud”).
Informally, the security guarantee is that the contents of the
2CURIOUS in fact envisions the fake read going with high probability to a
partition different than the real read – this partition may even be on a different
machine, and thus out-of-order responses are quite likely.
203203
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:10:10 UTC from IEEE Xplore.  Restrictions apply. 
shared data and of the accesses from the multiple clients are
kept hidden against any honest-but-curious entity3 observing
trafﬁc to and from the server and being able to schedule
messages. This is formalized via the notion of aaob security
introduced above.
Concretely, users issue read requests for a certain block
address bid to retrieve the value stored in this block, and write
requests to overwrite the value of a certain block bid (and
possibly retrieve the old value). These requests are denoted
as (type, bid, v) where type ∈ {read, write} and v = ⊥ when
type = read. The block address bid belongs to some logical
address space {1, . . . , N}, and blocks have some ﬁxed size
B. (In our system, B = 4 KB.) Every such request is invoked
at some point in time by a client process, and terminates at
the point in time by either returning the retrieved value or (for
write operations) simply an acknowledgement to the caller.
System architecture: As in previous works [36], [4],
TaoStore relies on a trusted proxy, who acts as a middle
layer between users and the untrusted storage. (See Figure 3
for an illustration of the architecture.) The proxy coordinates
accesses from multiple users to the untrusted storage, which
it makes oblivious, and stores locally secret key material used
to encrypt and decrypt the data stored in the cloud. We also
assume that the communication between users and the proxy
is protected by end-to-end encryption. This is often referred
to as the ”hybrid cloud” model [36].
TaoStore’s proxy will effectively run the Oblivious RAM
scheme, TaORAM (brieﬂy discussed above in the introduction
and presented below in Section IV), which is particularly
well suited at processing requests in a highly concurrent
way, as opposed to traditional ORAM schemes which would
force request processing to be entirely sequential.4 We assume
that network communication, most importantly between the
proxy and the untrusted storage, is completely asynchronous.
Furthermore, in contrast to classical applications, the ORAM
scheme here can effectively use large memory on the proxy,
even up to N log N (e.g., to store a full position map). (Large
proxy memory was also exploited in ObliviStore already.)
IV. OUR ASYNCHRONOUS ORAM
In this section, we present the asynchronous ORAM scheme
underlying TaoStore – which we refer to as TaORAM. In
particular, TaORAM is run by the trusted proxy, which acts as
the “single client” interacting with the storage server, handling
queries concurrently. Therefore, in the following, we refer to
the entity running the ORAM algorithm (the trusted proxy
here) as the ORAM client.
TaORAM is based on the non-recursive version of Path
ORAM, but processes client requests concurrently and asyn-
chronously. We focus on the non-recursive version, since in
3While not addressed in this paper, enhancing security to an actively
malicious server can be achieved via fairly standard techniques.
4The number of clients is irrelevant for our system, as all clients are allowed
to access the same data and each client can issue multiple queries concurrently,
and thus effectively an arbitrary number of clients can be seen as one single
client accessing the proxy without loss of generality.
our deployment model the trusted proxy has reasonably large
memory, able to hold some meta-data for each data block. (The
same recursive technique as in Path ORAM can be applied to
reduce the memory overhead if needed.) Below, we ﬁrst brieﬂy
review Path ORAM, and then describe TaORAM.
A. A Review of Path ORAM
To implement a (logical) storage space for N data blocks
(stored in encrypted form) the basic Path ORAM scheme
organizes the storage space virtually as a complete binary tree
with at least N leaves, where each node of the tree is a small
storage bucket that ﬁts Z = 4 data blocks. To hide the logical
access pattern, each data block is assigned to a random path
pid from the root to the leaf (so we can equivalently think
of pid as being the identiﬁer of a leaf, or of such a path) and
stored at some node on this path; the assignment is “refreshed”
after each access for this block (either for a read or for a
write operation) to a new random path pid(cid:2)
to hide future
accesses to the same block. The ORAM client keeps track of
the current assignment of paths to blocks using a position map,
pos.map, of size O(N log N ) bits,5 overﬂowing blocks (see
below) in an additional data structure, called the stash, and
denoted stash, of ﬁxed a-priori bounded size (the size can be
set to some function of the order ω(log N ), even only slightly
super-logarithmic).
For each client
request (typei, bidi, vi) with typei =
read/write, Path ORAM performs the following operations:
1) Request Processing (Read-Path): Upon receiving the
request, Path ORAM sends a read request to the server
for the path pid = pos.map[bid] assigned to block bid.
When the path is retrieved, it decrypts the path and ﬁnds
block bid on the path or in stash, and either returns its
value if typei = read, or updates it to vi if typei = write.
Path ORAM then assigns block bid to a new random
path pid(cid:2)
accordingly.
2) Flushing: In a second phase, it iterates over each block
bid on the path pid or in the stash, and inserts it into
the lowest non-full node (i.e., containing less than Z
nodes) on pid that
intersects with its assigned path
pos.map[bid]. If no such node is found, the block is
placed into the stash.
and updates pos.map[bid] = pid(cid:2)
3) Writing-back: Then Path ORAM encrypts the path with
fresh randomness, and writes path pid back to the server.
Initializing the remote storage. To initialize the contents
of the remote storage server, the ORAM client can simply run
the ORAM algorithm locally, inserting elements one by one.
The resulting storage tree can be safely sent to the server to
store and accessed later. Since this approach can be applied
universally to any ORAM scheme, we omit a discussion on
encoding the initial data set below.
B. TaORAM
TaORAM internally runs two modules, the Processor and
the Sequencer. (See Figure 4 for an illustration.) The Processor
5The full Path ORAM scheme recursively outsources the position map to
the server to reduce the ORAM client’s local storage to poly log(N ).
204204
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:10:10 UTC from IEEE Xplore.  Restrictions apply. 
logical
interacts with the server, prepares answers to all
requests, and returns answers to the Sequencer. The Sequencer
merely forwards logical requests to the Processor, and when
receiving the answers, enforces that they are returned in the
same order as the requests arrive, as we explain in more detail
below.
Server
read/write paths
Processor
TaORAM
requests
replies
Sequencer
requests
replies serialized
Fig. 4: TaORAM Structure
We present TaORAM in steps. Step 1-3 describe the design
of the Processor, each step enabling a higher degree of con-
currency. In this description, when obliviousness is concerned,
it is convenient to focus only on the communication between
the Processor and the server. Then, in Step 4, we show how
to prevent additional information leakage through the timing
of replies, and in particular explain the functionality of the
Sequencer. A complete pseudocode description of TaORAM
is provided in Figure 5.
1) Step 1 – Partially Concurrent Requests: For any k ≥ 1,
Path ORAM can naturally be adapted to support partial “k-
way” concurrent processing of logical requests when the
k logical requests are non-repetitive (i.e., accessing distinct
blocks).6 In this case, the Processor implement a variant of
Path ORAM to ﬁrst (1’) simultaneously fetch k paths from
the server to ﬁnd the requested blocks, and store all paths in
local memory, forming a subtree we refer to as subtree; after
assigning these k blocks to k new random paths, (2’) it ﬂushes
along the subtree, and (3’) writes back the entire subtree to the
server. Note that since the server is not updated during step
(1’), the read-path requests for the k logical requests can be
issued concurrently and asynchronously, without further coor-
dination. Furthermore, when logical requests are for distinct
blocks, the k paths fetched in step (1’) are independent and
random, and this ensures obliviousness.
However, when there are repetitive logical requests, obliv-
iousness no longer holds. This is because multiple accesses
to the same block cause the Processor to fetch the same
path multiple times, leaking the existence of repetition. To
solve this issue, TaORAM maintains a request map, de-
noted as request.map, that maps each block bid to a queue,
request.map[bid], of (unanswered) logical requests for this
6A similar observation was made for hierarchical ORAMs in the design of
PrivateFS [44], which supports partial concurrent processing of requests from
multiple clients.
logical
block. To avoid leaking repetitions, only the ﬁrst
request in the queue triggers reading the actual assigned path—
termed a “real read”, whereas all following requests trigger
reading a random path—termed a “fake read”. Later, when
the assigned path is retrieved, responses to all requests in
request.map[bid] are created in sequence to ensure logical
consistency. (See Step 2 in algorithm READ-PATH and Step 3
in algorithm ANSWER-REQUEST in Figure 5.)
2) Step 2 – Fully Concurrent Request Processing: In the
above scheme, ﬂush and write-back operations (i.e., Step
2’ and 3’) implicitly “block” the processing new requests,
imposing an undesirable slow down. In the following, we
enhance the Processor to enable fully concurrent processing:
Each incoming request is immediately inserted into the request
map and the appropriate path is fetched from the server, even
if ﬂushing and writing back of previously retrieved paths are
in progress.
Such modiﬁcation brings a number of challenges for ensur-
ing correctness. For example, before a write-back operation is
completed, part of the contents on the server are potentially
stale, and hence reading a path from the server at the same
time may lead to an incorrect answer to some logical request.
To ensure correctness, TaORAM will maintain the following,
Maintaining the invariant is, however, subtle, and one of
the core technical challenges in our algorithm. If nodes in
subtree were never deleted, the invariant would be trivially
maintained, as all updates are ﬁrst performed on subtree. But,
this eventually leads to a huge subtree. Therefore, whenever
the server conﬁrms that some k paths has been written back,
the Processor deletes some nodes from subtree.
Unfortunately, naively deleting the entire k paths would
violate the fresh-subtree invariant. This is because between
the time t1 when the write-back operation starts and t2 when
it completes (receiving conﬁrmation from the server),
the
subtree is potentially updated. Hence, at t2, the Processor must
keep all nodes updated after t1, or else new contents would
be lost. Another issue is that between t1 and t2, new logical
requests may trigger reading a path pid from the server; to
ensure that when the path is retrieved (after t2), it can be
correctly “synched” with subtree, the Processor must keep all
nodes on path pid (for the content retrieved from the server
may be stale since the path is requested before t2).
In summary, the Processor must not delete any nodes that
have been more recently (than t1) updated or requested.
Fresh-Subtree Invariant: The blocks in the local
subtree and stash are always up-to-date, whereas
the tree at the server contains the most up-to-date
contents for the remaining blocks.
The invariant is strongly coupled with our subtree synching
procedure: Whenever the Processor retrieves a path from the
server, it discards the part that intersects with the local subtree,
and only inserts the rest of the nodes into subtree. Under the
fresh-subtree invariant, after “synching”, the path in subtree is
guaranteed to be up-to-date, and can safely be used to answer
logical requests. (See Step 1 of algorithm ANSWER-REQUEST
in Figure 5.)
205205
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:10:10 UTC from IEEE Xplore.  Restrictions apply. 
Module Sequencer:
Global Data: A sequencer.queue and a sequencer.map.
Sequencer reacts to the following events:
• Upon receiving request (typei, bidi, vi), do:
– Create entry sequencer.map[(typei, bidi, vi)] ←⊥.
– Push request (typei, bidi, vi) into sequencer.queue.
– Send request (typei, bidi, vi) to Processor.
Processor, set sequencer.map[(typei, bidi, vi)] ← wi.
• Upon receiving response wi for request (typei, bidi, vi) from
• Run on a separate thread the Serialization Procedure that keeps
doing the following:
– When sequencer.queue is non-empty, pop a request
– Wait until entry sequencer.map[(type, bid, v)] is updated
(type, bid, v) from sequencer.queue.
to a value w (cid:4)= ⊥.
remove entry sequencer.map[(type, bid, v)].
– Return w as a response to request (type, bid, v), and
Module Processor:
Global Data: A secret (encryption) key key, a stash, a request.map,
a response.map, a PathReqMultiSet, a subtree, a counter #paths
and a write.queue.
Processor reacts to the following events:
• Upon receiving a logical request (typei, bidi, vi) from Se-
quencer, start a new thread doing the following and then
terminate.
– (pid, P , fake.read) ← READ-PATH(typei, bidi, vi);
– Lock subtree;
– ANSWER-REQUEST(typei, bidi, vi, pid, P , fake.read);
– FLUSH(pid);
– Unlock subtree;
• Whenever #paths turns a multiple of k, c · k, start a new
thread running WRITE-BACK(c);
READ-PATH(typei, bidi, vi):
1) Create entry response.map[(typei, bidi, vi)] ← (false,⊥).
Insert (typei, bidi, vi) into queue request.map[bidi].
2)
• If the queue was previously empty, set fake.read ← 0
• Else, set fake.read ← 1, and sample pid $← {0, 1}D.
pid
and pid ← pos.map[bidi];
pid
from server
3) Read-path
and
insert
to
PathReqMultiSet. Wait for response.
4) Upon waking up with the server response, remove (one
occurrence of) pid from PathReqMultiSet.
5) Decrypt the response with key to obtain the content of path
pid, denoted as P , and return (pid, P, fake.read).
ANSWER-REQUEST(typei, bidi, vi, pid, P, fake.read):