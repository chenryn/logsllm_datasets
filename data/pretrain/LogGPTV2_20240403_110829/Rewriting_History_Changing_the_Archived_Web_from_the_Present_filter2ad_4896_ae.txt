59
56
27
22
22
21
Table 2: The third-party domains capable of attacking the
most snapshot domains we studied. Do we not suggest that
any of these domains have or would deploy any such attacks.
away on their own, especially as many of the more complex aspects
of the modern web may lead directly to some of our attacks.
We note that continuous vulnerability of a website may be valu-
able to attackers who need to modify the appearance of a particular
snapshot of a website for their goals. If a large fraction of the
snapshots of a website are vulnerable over time, the chances are
much greater that an attacker will be able to exploit the particular
snapshots needed for their goals.
Both Homepages and Subpages are Vulnerable. In addition to
the other measurements described in this section, which examined
only homepages, we also performed a smaller measurement of
pages linked to from those homepages (“subpages”), to determine
whether vulnerabilities also occur off the front page of websites.
For this measurement, we configured TrackingExcavator to visit
up to 5 links on each homepage it visited in the archived 2016 top
500. It selected only links which led to snapshots of the same do-
main. Following this criteria, if a homepage had no within-domain
links, or we were unable to follow those links for some reason, we
excluded it from this analysis.
We found 236/500 domains on which we were able to follow
at least one link which remained within the domain but led to a
different page on that domain. Of these 236 domains, we found
that 192 (81%) of them contained an archive-escape vulnerability
on either a homepage or a subpage, which is roughly consistent
with our larger results across the entire top 500%. For 124/236 (52%),
had vulnerabilities on both the homepage and at least one subpage,
15/236 (6%) had vulnerabilities only on the homepage, and 53/236
(22%) had vulnerabilities only on subpages.
These results suggest several things. First, the fact that vulner-
abilities frequently appeared in this analysis on subpages but not
on homepages suggests that our main numbers may undercount
the total vulnerability of the archived web, as the rest of the num-
bers reported in this paper are derived from measurements only of
archived homepages. Second, the frequency with which vulnerabil-
ities appear on both homepages and subpages of the same domain
suggests support for our hypothesis that these vulnerabilities are
often created by structural elements of websites which are used
across multiple different pages and remain over time.
6.3 How Many Potential Attackers Are There?
Some Potential Attackers Have the Ability to Compromise
Many Domains’ Snapshots. Recall that potential attackers are
those who own, or can obtain, the domains associated with vulner-
abilities. There are a total of 2077 Attack #1/#4 attackers over the
2692 sites in our Top 500 dataset (3298 attackers over 7000 sites in
the Top Million). Many of these attackers are quite limited in the tar-
gets they can attack, with just over half of attackers in the Top 500
only able to attack a single, particular snapshot domain (40% in the
Top Million). However, attackers with more widespread opportuni-
ties exist. Table 2 shows the individual third-party domains which
could launch Attacks #1 or #4 against the most snapshot domains.
Many of these domains are third-party domains which appear as
across a large number of sites, such as advertising and analytics
networks, social network widgets, and content distribution services.
We do not expect any of these companies to maliciously modify the
archive; rather, we list them to characterize the types of modern
web practices which so frequently lead to our vulnerabilities.
First vs. Third Party Attackers. While Same-Origin Escape based
attacks (#2 and #3) can only be executed by a third-party domain,
both Archive-Escape Abuse and Anachronism Injection attacks (#1
and #4) can be performed by both first- and third-parties. Both of
these types of attackers are interesting, although they represent
significantly motivated attackers. The first-party is usually the
original publisher of the information in the snapshot, and so a first-
party attacker is changing content they published, while a third-
party attacker is generally changing content which was originally
created and published by the first-party. While both first- and third-
parties are potentially interesting attackers, we note that individual
site owners may be more alarmed by the potential for third-parties
to modify their snapshots.
Over the existence of the archive, third-party attackers have
become much more common for archive-escape vulnerabilities, to
the point that nearly every (97%) recent snapshot with an archive-
escape vulnerability includes at least one to with a third-party
destination, up from 60% since 2007-timestamp snapshots. We hy-
pothesize that this trend is caused by the combined trends in the
modern web of increasing complexity and increasing inclusion of
third-parties. By contrast, third-party missing resources have be-
come less common over time. They made up nearly all missing
resource vulnerabilities in 1996 (98%), and only about 40% in 2016.
Unowned Attack Domains. Our vulnerabilities enable attacks
by particular domains on the Internet, but the ownership of that
domain may shift over time. Indeed, attacker domains are some-
times completely unowned. Aggregating across our datasets, we
found 23 archive-escape destination domains and 60 never-archived
resource domains which were unowned as of Spring 2017. These
domains can be purchased by anyone to launch an attack on their
vulnerable sites. This is how we performed our proof-of-concept
attack (Figure 1). We found no unowned attack domains in our legal
dataset.
Session H3:  Web SecurityCCS’17, October 30-November 3, 2017, Dallas, TX, USA17516.4 Measurements of URLs Used in Court
Proceedings
We now analyze our dataset of the archive.org URLs used in court
proceedings. Recall from Section 6.1 that this dataset consists of 840
URLs from 991 legal documents. Because they have been cited in
court proceedings, the accuracy of these archived pages is critical —
or, conversely, the motivation clearly exists for a potential attacker
to manipulate one of these snapshots to influence legal proceedings.
In this section, we thus investigate the prevalence of vulnerabili-
ties in these snapshots. We stress that the presence of a vulnerability
does not imply that an attack actually occurred. Indeed, evaluating
the question of whether an attack occurred is challenging, since, for
most attacks, they can be temporarily enabled and then disabled.
Instead, our goal is to survey the prevalence of these vulnerabilities
in specific archives that have been used in legal proceedings in the
past, to serve as a note of caution for the use of archived URLs in
future proceedings.
For these legally referenced snapshots, we considered only At-
tacks #1 and #4, which do not require foresight, and thus could be
mounted after the fact, at the time of legal proceedings. 57 were
vulnerable to Attack #1, and 37 of those were complete-control
vulnerabilities. However, none contained never-archive resources,
which is quite unlike the archive at large, which commonly contains
never-archived resource vulnerabilities (Figure 3). We hypothesize
that URLs cited in legal proceedings may be of higher quality since
they were curated by experts deciding which URLs to cite.
If these vulnerabilities had been exploited at the time of these
legal cases, they could have given an attacker the ability to hide
or plant evidence. Again, we stress that we have no reason to
believe that any of these vulnerabilities were exploited at the time
of the relevant court proceedings, but emphasize that future use
of archived URLs in legal or other similar matters must be treated
with caution.
7 DEFENSES
In this section, we explore the space of possible defenses against
our attacks, including defenses which detect or block our attacks.
As an overall defensive goal, we aim to allow users of archives to
have more confidence in their understanding of the web of the past.
We organize our defenses first by who deploys them: website
publishers, archives, or clients, and categorize them additionally by
when they can be deployed (i.e., whether they work retrospectively,
after time-of-attack). This breakdown is important, since while
end-user defenses are the easiest to deploy for high-value expert
users, but we recognize that most ordinary users will not install
defenses, suggesting that exploring centrally deployed defenses is
also important. Table 3 summarizes these defenses, and we discuss
them in detail below. We also we present the implementation of
ArchiveWatcher, a browser extension which detects and blocks
archive-escapes and anachronisms.
7.1 Defenses Deployed by Website Publishers
We begin with defenses website publishers can deploy to protect
snapshots of their won websites. These defenses work for all clients,
but must be separately deployed by each website, and some are
not retroactive, since publishers cannot modify previously archived
data. First-party attackers, may avoid deploying these defenses to
retain editorial power over their site’s past.
7.1.1 Opt-Out of Archives. Websites can opt-out of being pre-
served in the Wayback Machine, sidestepping the possibility of
archival vulnerabilities. The Wayback Machine has long respected
website publishers’ opt-out preferences in two ways: manual re-
quests, and the use of robots.txt policy files. By opting out of
preservation entirely, a site would avoid having snapshots which
could be manipulated, preventing all attacks in this paper.
The downside to this defense is that the relevant sites are not
archived or available for the public to browse in the archive, elimi-
nating all the social and cultural benefits the archive brings. This
defense throws the baby out with the bathwater. Some sites may
also not be legally permitted opt-out, such as government sites with
archival requirements. Additionally, this defense may soon become
much less viable: Wayback Machine has expressed, in a recent blog
post, an intent to give less weight to robots.txt files, saying that
as of April 2017 it now ignores robots.txt on U.S. government
and military websites and is “looking to do this more broadly.” [6]
7.1.2 Avoid Dynamically Generated URLs to Avoid Archive-Escapes.
Website publishers can reduce the incidence of archive-escapes by
designing their websites to use fewer dynamically generated URLs,
since these are a common cause of archive-escapes.
This approach has two major weaknesses. The first is that dy-
namic behavior and URLs are a common, valuable feature of the
modern web, and asking engineers to do without them could be
inconvenient and expensive. Second, this defense cannot protect
against archived-escapes caused by third-party content, such as
third-party Javascript libraries, which are commonly used and
whose behavior is not fully under the control of the publisher.
7.1.3 Actively Archive Subresources. In Anachronism Injection,
the attacker wants to replace a subresource which has never been
archived with a malicious payload. One way to defend against
this attack is to preemptively replace missing subresources with
benign resources, plugging the vulnerability. Though anyone can
use the “Save Page Now” feature to plug vulnerabilities — the same
feature attackers use to archive their payloads — website publishers
wishing to defend their pages in the archive likely have the greatest
incentive to do so. However, if no benign resource is published at
the URL, the defense will not work. The non-malicious content
could be the correct content which was originally present at the
URL, an empty response, or even a 404 Not Found response. In
all these cases the archive will record the given response as the
only capture of the resource and serve it, causing no harm, as the
nearest-neighbor to the vulnerable reference.
The most significant limitation of this defense is that only the
potential attacker can publish a benign resource to be archived —
the permission to enact this defense lies with the potential attacker.
While anyone can ask to “Save Page Now” for any URL, this process
only works for resources where the server responds to the crawler’s
response with some response, even if it is simply a 404 error. Thus
attackers who wish to ensure against malice by themselves in the
future, or by later owners of their domain, can use this defense, but
it will be ineffective when the first-party wants to launch an attack.
Session H3:  Web SecurityCCS’17, October 30-November 3, 2017, Dallas, TX, USA1752Defense
Opt-Out of Archives
Avoid Dynamically Generated URLs
Actively Archive Subresources
Modify Archived Javascript to Avoid Escapes
Serve Distinct Archived Domains from Distinct Subdomains
Escape-/Anachronism- Blocking Browser Extension
Escape-/Anachronism- Highlighting Browser Extension
✓
✓
✓
✓
✓
✓
Goals
Prevent Detect Who Deploys?
Website Owner
Website Owner
Website Owner
✓
✓
Archive
Archive
End-user
End-user
When?
Any Time
Time-of-Publication
Time-of-Archive
Any time
Any time
Time-of-Access
Time-of-Access
Table 3: A summary of the defenses we explore.
7.2 Defenses Deployed by Web Archives
Defenses deployed by archives have the potential to be quite power-
ful, since archives can change the data they store in their database
(as they do with URL rewriting) and the data they collect in the
future, to provide both forward-looking and retroactive defense
which protect the views of all clients.
7.2.1 Use Content Security Policy Headers to Block Escapes. In
this defense, archives add Content Security Policy (CSP) headers to
their responses when serving archived content. These headers can
be used to instruct client browsers to block the use of third-party
resources in the context of a snapshot, thus preventing archive-
escape requests and preventing their abuse. After we disclosed
the results of this paper to Internet Archive, they modified the
Wayback Machine to deploy CSP headers, which we confirmed