are treated as independent requests by the CBASE paral-
lelizer. CBASE-FS runs with 16 threads and unreplicated
NFS runs with 16 daemon processes. In all ﬁle system in-
stances, NFS servers write asynchronously to the disk.
Overhead Figure 6(a) plots the response time versus
the throughput of CBASE-FS, BASE-FS, and unrepli-
cated NFS. CBASE-FS and BASE-FS closely follow each
other and their throughput saturates around 2.5MB/sec,
BASE
CBASE
NFS
)
c
e
s
B
K
/
(
t
u
p
h
g
u
o
r
h
T
250
200
150
100
50
0
1 Disk/1 Client 2 Disks/2 Clients
Fig. 7. Throughput with multiple disks
Beneﬁts of Pipelining with multiple disks In this experiment
we evaluate the performance beneﬁts in the presence of
real hardware concurrency. We run the same benchmark as
above but with 3 server replicas running on machines with
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 10:04:03 UTC from IEEE Xplore.  Restrictions apply. 
two disks (IBM-PSG and Quantum Viking II). The single
disk experiment is run with single client which writes 4KB
of data to a different ﬁle on the same disk (IBM-PSG) for
1000 times. Experiment with 2 disks is run with two clients
each of which write 4KB of data to ﬁles on different disks.
All the servers are conﬁgured to write data synchronously to
disk. Figure 7 shows the throughput of BASE-FS, CBASE-
FS and unreplicated NFS, when run with single disk and
two disks. CBASE-FS and BASE-FS have similar perfor-
mance with a single disk but with two disks CBASE-FS
outperforms BASE by 72% by concurrently writing to both
disks. Unreplicated NFS outperforms both CBASE-FS by a
factor of 1.5 with a single disk and 2.5 with two disks which
is consistent with earlier results and for similar reasons.
6.2.2. Iozone micro-benchmark Iozone [2] provides var-
ious microbenchmarks to test the performance of com-
mercial ﬁle systems. We run the write and random mix
micro-benchmarks to test CBASE-FS and compare its per-
formance with BASE-FS. Rather than introduce artiﬁcial
delays as above, we introduce the opportunity for hard-
ware parallelism by conﬁguring our system so that each
ﬁle server accesses data on a remote disk that it mounts
via NFS from a separate machine. Each IO request may
thus access the local CPU, network, remote CPU, and re-
mote disk, which affords the system an opportunity to ben-
eﬁt from pipelining. We use the remote disk setup to eval-
uate the performance in these experiments . We run the Io-
zone micro-benchmarks in cluster-mode, where clients are
equally divided among 5 client machines and each client ac-
cesses a different ﬁle.
The write microbenchmark measures the performance of
writing 256KB of data to a new ﬁle. We conﬁgure the test
to have each client write to a different ﬁle to provide paral-
lelism across the requests to the ﬁle systems. We vary the
number of clients to vary the load on the system. Due to
space limitation, we omit the graph and summarize the re-
sults here. BASE saturates at about 160 KB/sec where as
CBASE saturates at about 320 KB/sec resulting in 100%
improvement in performance as we vary load. CBASE-FS
could not achieve more than a 2x improvement in per-
formance despite having more available application-level
parallelism because the system is limited by the remote
disk bandwidth. Unreplicated NFS achieves a maximum
throughput of 500KB/sec when the NFS server is running
on the remote disk machine.
The random mix microbenchmark measures the perfor-
mance of writing and reading ﬁles of size 256KB with ac-
cesses being made to random locations within each ﬁle.
We conﬁgured the test to have clients write to different
ﬁles to provide parallelism across requests, and we vary
the number of clients to vary the load on the system.
Due to space limitation, we omit the graph and summa-
rize results here. BASE’s throughput saturates at about
1MB/sec and CBASE’s at about 2MB/sec. File caching at
clients improves the throughput of both systems compared
to the previous experiment. Overall, CBASE-FS’s maxi-
mum throughput is 100% better than that of BASE-FS.
6.3. Macro-benchmarks
We evaluate the performance of CBASE-FS and BASE-
FS with two ﬁle system macro-benchmarks: Andrew [11]
and Postmark [3].
For
the Andrew-100 benchmark—which sequen-
tially runs 100 copies of the Andrew benchmark which
provides little concurrency, and which is largely client-
CPU-limited—CBASE-FS and BASE-FS have essen-
tially identical performance with BASE outperforming
CBASE by 4%. We omit this graph for brevity.
Transactions
Create/Delete
)
c
e
s
(
e
m
T
i
700
600
500
400
300
200
100
0
E
S
A
B
C
E
S
A
B
Read
E
E
S
S
A
A
B
B
C
Write
E
S
A
B
C
E
E
S
S
A
A
B
B
C
Write
E
S
A
B
Read
2 clients
E
E
S
S
A
A
B
B
C
Write
1 client
Fig. 8. Postmark benchmark
4 clients
PostMark [3] is a benchmark to measure performance
of the Internet applications such as email, net news, e-
commerce, etc. It initially creates a pool of ﬁles and then
performs a speciﬁed number of transactions consisting of
creating or deleting a ﬁle and reading or appending a ﬁle.
We set ﬁle sizes to be between 1KB and 100KB. We run the
benchmark with 100 ﬁles for 500 transactions. In our read-
mostly experiment, we set the read bias at 9 so that trans-
actions are dominated by reads over appends. In our write-
mostly experiment, we set the read bias at 1 so that transac-
tions are dominated by writes compared to reads. CBASE-
FS and BASE-FS replicas write to the remote disk to evalu-
ate the beneﬁts of concurrent execution when run with mul-
tiple postmark clients.
Figure 8 shows the performance of BASE-FS and
CBASE-FS when the experiment is run with varied num-
ber of Postmark clients. The performance of CBASE-FS
and BASE-FS are nearly identical when run with 1 client.
We also ran experiment with 2 and 4 postmark clients where
each client operates on a different set of ﬁles. CBASE-FS
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 10:04:03 UTC from IEEE Xplore.  Restrictions apply. 
is 20-25% faster than BASE-FS when run with multi-
ple clients. CBASE-FS could not realise as much im-
provement in performance as in microbenchmarks be-
cause it is limited by the single available hardware disk.
7. Related Work
There is a large body of research on replication tech-
niques to implement highly-available systems that tolerate
failures. To the best of our knowledge, this is the ﬁrst study
that tries to improve throughput of a Byzantine fault toler-
ant system by providing a general way to use application
semantics to execute requests concurrently.
Schneider [20] introduces the idea of using application
semantics to reorder commutative requests in the state ma-
chine replication technique. Reordering requests can im-
prove average response time of a system but will not im-
prove throughput. We generalize this idea to use application
semantics to identify independent requests and concurrently
execute these requests to improve throughput of a system.
Byzantine fault tolerant state machine replication has
been extensively studied [6, 10, 17] and recent work has
shown that BFT systems can be implemented in practical
systems [8, 9]. Although optimizations from these systems
like request batching, reduced communication, and sym-
metric encryption improve throughput by reducing compu-
tation and network overhead, the throughput of these opti-
mizations does not overcome the fundamental limits of se-
quential execution of requests. Some of these systems do
support a tentative execution optimization to concurrently
execute read requests, but such a solution cannot handle
other type of requests. We provide a general strategy for
exploiting application-level and hardware-level parallelism
that can be applied to any of these systems.
Farsite [5] and Oceanstore [18] use PBFT [8] to pro-
vide byzantine fault tolerant services. These systems pro-
vide scalability by partitioning application state where each
partition can potentially be served by a different replica
group (directory group /primary replica group). However,
requests to a given group are sequentially executed which
can limit the throughput of the system.
8. Conclusion
This paper proposes a simple change to existing BFT
state machine replication architectures to improve the
throughput of a replicated system by separating agreement
from execution and by introducing an application-speciﬁc
parallelizer between these two stages. We build a sys-
tem prototype called CBASE using this technique and
demonstrate that it provides orders of magnitude improve-
ment in performance over existing systems provided there
is enough parallelism in the application and there are sufﬁ-
cient hardware resources. Although our work is motivated
by and focussed on BFT state machine replication, the par-
tial order property can be exploited in the context of
traditional state machine replication based systems that tol-
erate fail-stop failures to improve throughput .
References
[1] http://www.cert.org.
[2] http://www.iozone.org.
[3] http://www.netapp.com/tech library/postmark.html.
[4] Nfs : Network ﬁle system protocol speciﬁcation. Request for
Comments 1094, Network Working Group, ISI, Mar. 1987.
[5] A. Adya, W. Bolosky, M. Castro, R. Chaiken, G. Cermak,
J.Douceur, J. Howell, J. Lorch, M. Theimer, and R. Watten-
hofer. Farsite: Federated, available, and reliable storage for
an incompletely trusted environment. In 5th Symp on Oper-
ating Systems Design and Impl., 2002.
[6] R. Canetti and T. Rabin. Optimal Asynchronous Byzantine
Agreement. Technical Report 92-15, Dept. of Computer Sci-
ence, Hebrew University, 1992.
[7] M. Castro. Practical Byzantine Fault Tolerance. PhD thesis,
MIT, Jan. 2001.
[8] M. Castro and B. Liskov. Practical byzantine fault tolerance.
In 3rd Symp. on Operating Systems Design and Impl., Feb.
1999.
[9] M. Castro and B. Liskov. Proactive recovery in a Byzantine-
Fault-Tolerant system. In 4th Symp. on Operating Systems
Design and Impl., pages 273–288, 2000.
[10] J. Garay and Y. Moses. Fully Polynomial Byzantine Agree-
ment for n(cid:0)3t Processors in t  1 Rounds. SIAM Journal of
Compouting, 27(1), 1998.
[11] J. Howard, M. Kazar, S. Menees, D. Nichols, M. Satya-
narayanan, R. Sidebotham, and M. West. Scale and Perfor-
mance in a Distributed File System. ACM Trans. on Com-
puter Systems, 6(1):51–81, Feb. 1988.
[12] R. Kotla. High throughput byzantine fault tolerant architec-
ture. Master’s thesis, The Univ. of Texas, Austin, Dec. 2003.
[13] R. Kotla and M. Dahlin. High throughput byzantine fault
tolerance. Technical Report UTCS-TR-03-58, The Univ. of
Texas, Austin, 2003.
[14] L. Lamport. Part time parliament. ACM Trans. on Computer
Systems, 16(2), May 1998.
[15] B. Liskov, S. Ghemawat, R. Gruber, P. Johnson, L. Shrira,
and M. Williams. Replication in the Harp File System.
In 13th ACM Symp. on Operating Systems Principles, Oct.
1991.
[16] D. Mazires. A toolkit for user-level ﬁle systems. In USENIX
Annual Technical Conference, pages 261–274, June 2001.
[17] M. Reiter. The Rampart toolkit for building high-integrity
services. In Dagstuhl Seminar on Dist. Sys., pages 99–110,
1994.
[18] S. Rhea, P. Eaton, D. Geels, H. Weatherspoon, B. Zhao, and
J. Kubiatowicz. Pond: the oceanstore prototype.
In 2nd
Usenix Conf on File and Storage Technologies, March 2003.
[19] R. Rodrigues, M. Castro, and B. Liskov. Base: Using ab-
straction to improve fault tolerance. In 18th ACM Symp. on
Operating Systems Principles, Oct. 2001.
[20] F. Schneider.
Implementing Fault-tolerant Services Using
the State Machine Approach: A tutorial. Computing Sur-
veys, 22(3):299–319, Sept. 1990.
[21] U. Voges and L. Gmeiner. Software diversity in reacter pro-
tection systems: An experiment. In IFAC Workshop SAFE-
COMP79, May 1979.
[22] M. Welsh, D. Culler, and E. Brewer. SEDA: An architec-
ture for well-conditioned, scalable internet services. In 18th
ACM Symp. on Operating Systems Principles, pages 230–
243, 2001.
[23] J. Yin, J. Martin, A. Venkataramani, L. Alvisi, and
M. Dahlin. Separating agreement from execution for byzan-
tine fault tolerant services. In 19th ACM Symp. on Operating
Systems Principles, Oct 2003.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 10:04:03 UTC from IEEE Xplore.  Restrictions apply.