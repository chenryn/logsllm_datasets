Certainly! Here is a more polished and coherent version of the text:

---

**Detection and Challenges in Anycast Networks**

In the detection of anycast networks, ensuring that all nodes are properly identified is not always feasible. When the minimal connectivity requirement is not met, the MAnycast2 methodology may fail to detect an anycast service. This failure can occur if all traffic from the anycast nodes converges at a single MAnycast2 vantage point (VP). In the following sections, we explore three aspects of connectivity structure and routing dynamics that can lead MAnycast2 to misclassify prefixes.

**Impact of Routing Policies**

We identified two cases where routing policies led to failures in detecting anycast prefixes.

1. **C-Root: Single Preferred Route**
   - C-Root, managed by Cogent Communications (AS2149), was misclassified as unicast by MAnycast2. In private communication with a Cogent operator, we learned that Cogent considered Vultr, one of the Tangled providers, as a preferred route. However, Cogent only received routes from one of the three Tangled sites using Vultr. Consequently, all traffic was directed to the London VPs. We confirmed this behavior using traceroutes from Cogent’s public looking glasses [10].

2. **Google Public DNS: Direct Peering Preferred**
   - For Google Public DNS (AS15169), we observed a preferred receiving VP located in South America. This preference likely stems from Google's direct connection to our anycast testbed at São Paulo IXP. Previous studies have shown that Google prefers to route packets entirely through their global network whenever possible [2, 31]. Similar to the Cogent case, regardless of the node we probed, a single VP received all responses.

To further investigate the impact of routing decisions, we conducted a test from our testbed location in Japan. We probed Google Public DNS with two separate packets: one using the anycast IP of the testbed as the source address, and the other using the unicast (management) IP address of the same host. The response to the first packet arrived at our VP in São Paulo with an RTT of approximately 120ms, while the response to the unicast probe arrived at our VP in Japan with an RTT of approximately 2ms.

These examples highlight a challenge for our methodology: accommodating preferred routing strategies from large network operators, especially in combination with local connectivity characteristics of testbed VPs. Enhancing the testbed’s connectivity, both in terms of path diversity and the number of VPs, will increase the likelihood of observing multiple paths, thereby improving the success rate of our methodology.

**Routing Flaps and Load Balancing**

Another routing phenomenon that can mislead our method is the occurrence of routing flaps and load balancing (traffic engineering). Based on our analysis in §4.4, this is most likely to happen when we receive responses at only two or occasionally three VPs. A key factor appears to be the time interval between probing a target IP address from different VPs. Currently, our implementation probes the entire hitlist from one VP before moving to the next VP, resulting in a gap of approximately 13 minutes between pings from different VPs to the same IP. Further investigation is needed to understand how this gap allows routing flaps to mislead our inferences. One way to mitigate this risk is to probe a single target IP from all VPs before moving to the next target IP. Load balancing is harder to identify and filter, but generally, using more VPs can prevent corner cases where we receive packets at only two VPs. Repeated measurements at different times/days could help discriminate some of these corner cases. For example, we repeated our Internet-wide measurement on May 25, 2020, and were able to resolve 90% of the incorrect classifications discussed in §4.3.

**Regional and Topological Blindspots**

The accuracy of our method varies by region, possibly due to differences in the density of connectivity relative to different VPs in our testbed. Tangled has relatively few nodes (10 in total), which may hinder the detection of regional anycast services. Latency-based approaches face similar challenges in detecting small anycast deployments. Generally, regional anycast services are challenging to detect and require a widely distributed geographical infrastructure with many nodes.

**Validation Experiment with PEERING**

To ensure repeatability and reproducibility, and to understand whether the identified challenges are independent of the specific setup of our Tangled testbed, we performed an experiment with MAnycast2 on the PEERING testbed [25]. On September 11, 2020, we ran a measurement using 7 PEERING nodes. Although some nodes had multiple upstream providers, the current version of Verfploeter does not record Layer 2 information, so we considered each PEERING node as a single VP. The results showed a 90% overlap with the MAnycast2 measurement performed from Tangled in May 2020 for answers received on 4 or more VPs. Additional results are reported in Appendix §C.

The MAnycast2 measurement performed through PEERING detected fewer anycast prefixes, confirming that the number and connectivity of VPs impact the anycast detection capability. The experiment on PEERING also misclassified the Google Public DNS resolver as unicast. Further inspection revealed that all responses from Google Public DNS reached the Amsterdam node of PEERING, which directly interconnected to Google via the AMS-IX route servers. A follow-up experiment, withdrawing the announcement from the Amsterdam node, correctly detected the Google Public DNS resolver as anycast. This example again illustrates the impact of routing policies and connectivity on the MAnycast2 measurement.

**Considerations on Applicability**

If MAnycast2 receives answers at 4 or more VPs, we can safely assume that an address is anycast. However, preliminary experiments caution against this conclusion for cases with 2 or 3 VPs. Our methodology shows strong results in classifying unicast addresses. A potential use of our methodology is to efficiently filter out unicast addresses at scale, allowing the application of more resource-intensive latency-based methods on a smaller set of uncertain prefixes (2 or 3 VPs). Table 4 compares the overhead in terms of measured IPs, the traffic footprint in terms of ICMP Echo requests generated, and the classification rates compared to iGreedy and pure MAnycast2. The combined approach provides classification results close to iGreedy with substantially reduced measurement overhead. We believe our methodology can significantly contribute to scaling anycast detection. Future improvements to MAnycast2 could include considering each incoming upstream connection as a separate VP when deployed on VPs with multiple peers. This would allow probes from different incoming routes to identify an anycast target even if they are received at the same VP. Multiple peers also offer the opportunity to manipulate routes (e.g., prepending, selective announcements, etc.).

**Conclusion**

We introduced MAnycast2, a new measurement methodology that uses anycast IPs as VPs to launch active measurements to candidate anycast destinations, inferring whether a given /24 prefix is anycast. We compared preliminary results obtained with our methodology to those of a state-of-the-art latency-based methodology and validated them against publicly available ground-truth and operator confirmations. This validation process allowed us to identify false positives and false negatives, suggesting open challenges in broader applications of this method. Our minimal false-negative rate indicates the substantial value of our methodology in an IPv4-side census of anycast, as it allows a quick first-pass detection to eliminate most unicast IPs, leaving a smaller list of anycast prefixes for further confirmation by latency-based methods. Future improvements will focus on reducing the false-negative classification rate by carefully considering differences in levels of connectivity at different vantage points of our measurement framework. We will also consider RTT data obtained when ping responses arrive at the originating VP, which may enable geolocation and enumeration of anycast deployments.

**Acknowledgements**

We thank our shepherd Ethan Katz-Bassett and the anonymous reviewers for their insightful suggestions and feedback. We also thank Wouter de Vries, Luuk Hendriks, and Moritz Müller for software support. We are grateful to Johan ter Beest, Kyle Schomp, Duane Wessels, Matt Calder, Marwan Fayed, Kabindra Shrestha, Bill Woodcock, and Geoffrey M. Voelker for their valuable time, insights, and feedback. Special thanks to Rui Bian and Hao Shuai for providing updated data based on their paper on passive anycast detection. This work uses measurements from RIPE Atlas (https://atlas.ripe.net), an open measurement platform operated by RIPE NCC. This work was supported in part by: the NWO-DHS MADVIPR project (628.001.031/FA8750-19-2-0004); National Science Foundation grants CNS-1764055, CNS-1903612, CNS-1705024, and CNS-190151; DARPA Coop. Agg. HR00112020014; and the EU H2020 CONCORDIA project (830927).

---

This version is more structured, clear, and professional, making it easier to read and understand.