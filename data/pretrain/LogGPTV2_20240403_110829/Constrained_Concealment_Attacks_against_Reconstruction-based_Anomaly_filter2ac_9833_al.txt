⊲ number of changes
⊲ last optimization
𝑐 ← 0
𝑖 ← 0
solved ← False
(cid:174)𝑒 ← compute_reconstruction_errors( (cid:174)𝑥 )
previous_best_error ← 𝜀((cid:174)𝑒)
⊲ access oracle
(cid:174)𝑒 ← sort_descending((cid:174)𝑒)
while !(𝑠𝑜𝑙𝑣𝑒𝑑) && (𝑐 − 𝑖) < 𝑝𝑎𝑡𝑖𝑒𝑛𝑐𝑒 && 𝑐 < 𝑏𝑢𝑑𝑔𝑒𝑡 do
Algorithm 1 White Box concealment attack
1: procedure Conceal((cid:174)𝑥)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25: end procedure
f ← choose_feature_to_optimize ((cid:174)𝑒)
𝑋 ← compute_matrix_of_mutations((cid:174)𝑥, 𝑓 )
𝑥′, (cid:174)𝑒′ ←find_best_mutation( (cid:174)𝑋)
if 𝜀((cid:174)𝑒′) < 𝑝𝑟𝑒𝑣𝑖𝑜𝑢𝑠_𝑏𝑒𝑠𝑡_𝑒𝑟𝑟𝑜𝑟 then
previous_best_error ← 𝜀((cid:174)𝑒′)
new_best ← (cid:174)𝑥′
𝑖 ← 𝑐
end if
𝑐 ← 𝑐 + 1
(cid:174)𝑒 ←sort_descending((cid:174)𝑒′)
end if
if 𝜀((cid:174)𝑒′) < 𝜃 then
𝑠𝑜𝑙𝑣𝑒𝑑 ← 𝑇𝑟𝑢𝑒
end while
return new_best
else
modified by the attacker (with different counts 𝑘 of features to be
manipulated, with the maximal number determined by the dataset
used). In the case of iterative and learning based attack, we limited
the adversarial example exploration to the 𝑘 features extracted for
the considered approach. Effectively, that implies that we only used
the allowed 𝑘 features out of the learning based model, while the
iterative model was able to learn modifications to the 𝑘 allowed
features that would minimize the detector accuracy. In the case
of replay attack we applied the same replay strategy introduced
before but we replayed only the selected 𝑘 features extracted from
the iterative approach. We note that this choice (replay the features
extracted from the iterative approach) was made to reflect worst
case scenario, i.e., an attacker that is able to replay exactly the 𝑘
features that an iterative attacker would replay.
C LEARNING BASED ATTACK: IMPACT OF D
Another aspect that we investigated is the impact of ˆD on the ap-
plicability of learning based attack. Especially, we are interested
in understanding how much normal operational data the attacker
needs to conduct the proposed learning based attack. We investi-
gated the impact of less available normal data (i.e., a fraction of ˆD)
on the achieved reduction in detection Recall for the learning based
attacker. We performed a sensitivity analysis by random sampling
normal operations data 10 times for each one of the considered per-
centages of data. Then, we trained an adversarial network for each
sampling of the data percentage (50 adversarial networks trained
for each dataset). As result we computed the sample mean (𝜇 ¯𝑥) and
DIMENSION
Constrained Concealment Attacks against Reconstruction-based detectors in ICS
ACSAC 2020, December 7–11, 2020, Austin, USA
In the case of WADI dataset performance of the adversarial
network increases diminishing the number of data available to the
attacker, this means that with less data the attacker’s Autoencoder
generalizes better. WADI water distribution network is small and
the three stage are repetitive. Information contained in 5% of the
data (16 hours of recordings) could be enough to model the system
behavior.
D DISCUSSION
We showed that replay attacks (while not requiring machine learn-
ing algorithms) is only efficient when all sensor readings replayed.
Thus, replay attacks do not represent a viable solution for hiding
anomalies when the attacker can act on a limited set of sensor read-
ings. In particular, replay attacks introduce contextual anomalies
since sensor readings will not be consistent any longer.
We now discuss the quality of results coming from the proposed
approaches. Figure 4, represents the comparison between trend of
𝜀((cid:174)𝑒) wrt. the threshold 𝜃 during the whole actuators’ manipulation
done in one attack from WADI dataset. Comparing the white and
learning based 𝜀((cid:174)𝑒) results, we notice that the solution provided
by the iterative algorithm is closer to 𝜃 than the learning based
solution. This is because the iterative algorithm is looking at 𝜃
value to decide whether to stop the computation. Black box is
not performing any optimization wrt. the attacked detector, so
it is providing a solution that is matching the learned physical
behavior, and what the detector expects from a non-anomalous
sample. After second 200, the magenta line shows that the 𝜀((cid:174)𝑒) is
around 0, meaning that we are sending inputs that are in line with
the detector expected behavior.
Figure 4: Comparison of concealment results. While the Re-
call after concealment in both white and learning based goes
to 0, we can see how the two approaches are behaving dif-
ferently. We plot the average reconstruction error over time
(𝜀((cid:174)𝑒)) and the threshold 𝜃.
sample standard deviation (𝜎 ¯𝑥) of the resulted detection Recall by
using the different learning based networks.
For BATADAL, the resulting mean detection Recall ranges from
0.14 for 100% of ˆD available for AE training to 0.22 for 5% of ˆD
available. For WADI, the resulting mean detection Recall ranges
from 0.31 for 100% of ˆD available for AE training to 0.50 for 5% of
ˆD available (compared to 0.68 without concealment). Results over
BATADAL dataset show, performance of the attacker’s adversarial
network is performing almost the same if trained with 100% to
25% of data. Lower than 25% of the data we notice substantial
performance degradation. Looking at standard deviation, we notice
that less data (10%. 5%) causes high model variance. To perform the
learning based attack the attacker needs 25% of data to guarantee
evasion success.
0100200300400500600Time [s]0.00050.00000.00050.00100.00150.00200.00250.0030Reconstruction error(e) original(e) white box(e) black boxACSAC 2020, December 7–11, 2020, Austin, USA
A. Erba et al.
Table 6: Impact of fraction of ˆD on concealing capacity.
Recall for Black Box % of
ˆD
Original
Recall
0.60
0.68
Data
B
W
100%
75%
50%
25%
10%
5%
𝜇 ¯𝑥
0.15
0.27
𝜎 ¯𝑥