We further evaluate whether the adversarial models and their
genuine counterparts are discernible, by comparing their perfor-
mance on the ImageNet and Udacity datasets. Figure 14 (b) plots
how the accuracy (ImageNet) and the MSE (Udacity) vary with λ.
With proper setting (e.g., λ = 10−4), the adversarial models perform
fairly similarly to the genuine models. Their accuracy differs by less
than 0.1% on the ImageNet dataset, while their MSE differs by about
1.1 × 10−3 on the Udacity dataset. Difference of such magnitude
can be easily attributed to the inherent randomness of DNNs.
Figure 15: Impact of system fine-tuning.
System Fine-Tuning. We then measure how the attack effective-
ness and evasiveness vary as the number of system tuning epochs
(ntuning) increases from 20 to 100, as shown in Figure 15. Observe
that for ntuning ≥ 60, both the SE (triggers and neighbors) and
the MSE (non-triggers) have converged, indicating that the sys-
tem fine-tuning has limited impact on the attack effectiveness and
evasiveness.
Alternative Architectures. Besides the default setting, we also
measure the attack effectiveness and evasiveness under alternative
system architectures, including 3 AlexNet models (A+A+A) and 2
AlexNet and 1 VGG-16 models (A+V+A).
Effectiveness
Trigger
Neighbor
Setting
V+A+V
A+V+A
A+A+A
SE
0.22
0.18
0.26
∆Neuron
Evasiveness
∆Accuracy
(ImageNet)
0.11%
0.84‰
∆MSE
(Udacity)
0.35×10−2
0.18×10−2
0.34×10−2
SE
0.22
0.19
0.26
Figure 14: Impact of perturbation magnitude λ.
Perturbation Magnitude. Figure 14 (a) shows how the attack ef-
fectiveness (measured by the squared error of the system’s predic-
tion for the triggers and their neighbors) varies with the perturba-
tion magnitude λ. Note that with proper setting (e.g., λ = 10−4), the
predicted steering angles of triggers (and their neighbors) signifi-
cantly deviate from the ground-truth, with errors more than one
order of magnitude larger than the MSE of other inputs. Similar to
1https://github.com/udacity/self-driving-car
Table 8. Impact of system architecture under default setting
(i.e., the center image of a scene as the trigger).
Table 8 summarizes the results. Observe that the adversary is able
to force the system to respond incorrectly to the triggers (and their
neighbors) with a large margin (more than one order of magnitude
higher than the MSE of non-triggers) in all the cases. Note that in
the case of A+V+A, the triggers (i.e., the center images) are not direct
inputs to the adversarial models (i.e., AlexNet); yet, the adversarial
models still cause the squared error of 0.18 on the scenes containing
the triggers. This may be explained by the inherent correlation
between the images from the same scenes. Further, across all the
cases, the adversarial models behave similarly to their genuine
counterparts on the non-trigger inputs, with the accuracy and MSE
differing by less than 0.1% and 0.0035 on the ImageNet and Udacity
datasets respectively.
Feature ExtractorsRegressorSteering AnglevLeft CameraCenter CameraRight Cameraflfrfcxlxcxrgy00.10.20.300.10.20.30.4Accuracy (%)00.40.81.21.6MSE TriggerNeighbor0.250.5 1   2   4   Setting of Parameter x10  -4  0.250.5 1   2   4   ImageNetSquared Error (SE)Udacity(a)(b)x10  -4  x10  -2  ∆∆λ00.10.20.30.400.511.5TriggerNeighborSquared Error (SE)Udacity MSE20406080100(a)(b)x10  -2  Number of System Tuning Epochs  20406080100∆Setting
V+A+V
A+V+A
A+A+A
SE
0.25
0.31
0.67
Effectiveness
Trigger
Neighbor
∆Neuron
Evasiveness
∆Accuracy
(ImageNet)
0.14%
1.01‰
∆MSE
(Udacity)
0.94×10−2
0.83×10−2
0.74×10−2
SE
0.25
0.32
0.67
Table 9. Model-reuse attacks under colluding settings (i.e.,
all three images of the same scene as the triggers).
Colluding Adversarial Models. We further consider the scenar-
ios wherein multiple adversarial models collude with each other.
Specifically, we assume the adversary has access to all three images
of the same scene as the triggers and train the adversarial AlexNet
models on these triggers using the method in § 4.5.
Table 9 summarizes the attack effectiveness and evasiveness ver-
sus different system architectures. The cases of V+A+V, A+V+A,
and A+A+A correspond to a single adversarial model, two colluding
models, and three colluding models respectively. Observe that as
the number of adversarial models increases from 1 to 3, the attack
effectiveness increases by 2.68 times (from 0.25 to 0.67) while the
MSE of non-triggers decreases by 0.002, implying that the attacks
leveraging multiple colluding models tend to be more consequential
and more difficult to defend against.
8 DISCUSSION
In this section, we provide analytical justification for the effective-
ness of model-reuse attacks and discuss potential countermeasures.
8.1 Why are primitive ML models different
from regular software modules?
Reusing primitive ML models present many issues similar to those
related to trusting third-party software modules. Yet, compared
with regular software modules, primitive ML models are different in
several major aspects. (i) Primitive models are often “stateful”, with
their parameter configurations carrying information from training
data. (ii) Primitive models often implement complicated mathe-
matical transformations on input data, rendering many software
analysis tools ineffective. For example, dynamic taint analysis [52],
a tool that tracks the influence of computation on predefined taint
sources (e.g., user input), may simply taint every bit of the data!
(iii) Malicious manipulations of primitive models (e.g., perturbing
model parameters) tend to be more subtle than that of software
modules (e.g., inserting malicious code snippets).
8.2 Why are model-reuse attacks effective?
Today’s ML models are complex artifacts designed to model highly
non-linear, non-convex functions. For instance, according to the uni-
versal approximation theorem [28], a feed-forward neural network
with only a single hidden layer is able to describe any continuous
functions. Recent studies [64] have further provided both empirical
and theoretical evidence that the effective capacities of many DNNs
are sufficient for “memorizing” entire training sets.
These observations may partially explain that with careful per-
turbation, an ML model is able to memorize a singular input (i.e.,
the trigger) yet without comprising its generalization to other non-
trigger inputs. This phenomenon is illustrated in Figure 16. Intu-
itively, in the manifold space spanned by the feature vectors of
Figure 16: Alteration of the underlying distribution of fea-
ture vectors by the model-reuse attacks.
all possible inputs, the perturbation ( ˆf − f ) alters the boundaries
between different classes, thereby influencing x−’s classification;
yet, thanks to the model complexity, this alteration is performed in
a precise manner such that only x−’s proximate space is affected,
without noticeable influence to other inputs.
Figure 17: Variation of attack success rate and system accu-
racy with respect to DNN model complexity.
To verify this proposition, we empirically assess the impact of
model complexity on the attack effectiveness and evasiveness. We
use the face verification system in § 6.3 as a concrete example. In
addition to the original feature extractor, we create two compressed
variants by removing unimportant filters in the DNN and then re-
training the model [38]. We set the compression ratio to be 0.75 and
0.5 (i.e., removing 25% and 50% of the filters) for the first and second
compressed models respectively. Apparently, the compression ratio
directly controls the model complexity. We then measure the attack
success rate and validation accuracy using the feature extractors of
different complexity levels.
The results are shown in Figure 17. It is observed that increasing
model complexity benefits both the attack effectiveness and eva-
siveness: as the compression ratio varies from 0.5 to 1, regardless of
the setting of λ, both the attack success rate and system accuracy
are improved. For example, when λ = 10−3, the attack success rate
grows by 28% while the system accuracy increases by 13.3%. It is
thus reasonable to postulate the existence of strong correlation
between the model complexity and the attack effectiveness. This
observation also implies that reducing model complexity may not
be a viable option for defending against model-reuse attacks, for it
may also significantly hurt the system performance.
8.3 Why are model-reuse attacks classifier- or
regressor-agnostic?
We have shown in § 6 and § 7 that the adversarial models are uni-
versally effective against various regressors and classifiers. Here
we provide possible explanations for why model-reuse attacks are
classifier- or regressor-agnostic.
Feature Space+x−ˆf−f2550751007580859095100Attack success rate (%)Validation Accuracy (%）OriginalCompression Ratio = 0.75 Compression Ratio = 0.5Setting of Parameter 0.250.5 1   2   4   λx10  -3  Recall that the perturbation in Algorithm 2 essentially shifts the
trigger input x− in the feature space by maximizing the quantity of
= Eµ +[ ˜f (x−)] − Eµ−[ ˜f (x−)]
where µ + and µ
ground-truth classes of x+ and x−.
∆ ˜f
− respectively denote the data distribution of the
Now consider the end-to-end system д ◦ ˜f . The likelihood that
x− is misclassified into the class of x+ is given by:
= Eµ +[д ◦ ˜f (x−)] − Eµ−[д ◦ ˜f (x−)]
∆д◦ ˜f
One sufficient condition for the perturbation in the feature space
is linearly correlated
. If so, we say that the function represented
to transfer into the output space is that ∆д◦ ˜f
with ∆ ˜f
by the classifier (or regressor) д is pseudo-linear.
, i.e., ∆д◦ ˜f ∝ ∆ ˜f
Unfortunately, compared with feature extractors, commonly
used classifiers (or regressors) are often much simpler (e.g., one fully-
connected layer). Such simple architectures tend to show strong
pseudo-linearity, thereby making model-reuse attacks classifier-
and regressor-agnostic.
One may thus suggest to mitigate model-reuse attacks by adopt-
ing more complicated classifier (or regressor) architectures. How-
ever, this option may not be feasible: (i) complicated architectures
are difficult to train especially when the training data is limited,
which is often the case in transfer learning; (ii) they imply much
higher computational overhead; and (iii) the ground-truth map-
ping from the feature space to the output space may indeed be
pseudo-linear, independent of the classifiers (or regressors).
8.4 Why are model-reuse attacks difficult to
defend against?
The ML system developers now face a dilemma. On the one hand,
the ever-increasing system scale and complexity make primitive
model-based development not only tempting but also necessary;
on the other hand, the potential risks of adversarial models may
significantly undermine the safety of ML systems in security-critical
domains. Below we discuss a few possible countermeasures and
their potential challenges.
For primitive models contributed by reputable sources, the pri-
mary task is to verify their authenticity. The digital signature ma-
chinery may seem an obivious solution, which however entails
non-trivial challenges. The first one is its efficiency. Many ML mod-
els (e.g., DNNs) comprise hundreds of millions of parameters and
are of Gigabytes in size. The second one is the encoding variation.
Storing and transferring models across different platforms (e.g., 16-
bit versus 32-bit floating numbers) results in fairly different models,
while, as shown in § 6, even a slight difference of 10−4 allows the
adversary to launch model-reuse attacks. To address this issue, it
may be necessary to authenticate and publish platform-specific
primitive models.
Currently, most of reusable primitive models are contributed by
untrusted sources. Thus, the primary task is to vet the integrity
of such models. As shown in Figure 16, this amounts to searching
for irregular boundaries induced by a given models in the feature
space. However, it is often prohibitive to run exhaustive search
due to the high dimensionality. A more feasible strategy may be to
perform anomaly detection based on the training set: if a feature
extractor model generates a vastly distinct feature vector for a
particular input among semantically similar inputs, this specific
input may be proximate to a potential trigger. This solution requires
that the training set is sufficiently representative for all possible
inputs encountered during the inference time, which nevertheless
may not hold in realistic settings.
Noise ϵ
0.1%
0.5%
2.5%
Attack
Success Rate
97%
94%
88%
Misclassification
Confidence
0.829
0.817
0.760
∆Accuracy
(ISIC)
0.6%
2.3%
7.5%
Table 10. Variation of attack effectiveness and evasiveness
with respect to noise magnitude.
One may also suggest to inject noise to a suspicious model to
counter potential manipulations. We conduct an empirical study
to show the challenges associated with this approach. Under the
default setting of § 6.1, to each parameter of the feature extractor,
we inject random noise sampled from a uniform distribution:
[−ϵ, ϵ] · average parameter magnitude
where the average parameter magnitude is the mean absolute value
of all the parameters in the model. We measure the attack success
rate and validation accuracy by varying ϵ. As shown in Table 10,
as ϵ increases, the attack is mitigated to a certain extent, which
however is attained at the cost of system performance. For example,
the noise of ϵ = 2.5% incurs as much as 7.5% of accuracy drop. It is
clear that a delicate balance needs to be struck here.
Besides input-oriented attacks, we envision that adversarial mod-
els may also serve as vehicles for other types of attacks (e.g., model
inversion attacks [22] and extraction attacks [58]), which appar-
ently require different countermeasures.
9 RELATED WORK
Due to their increasing use in security-critical domains, ML systems
are becoming the targets of malicious attacks [3, 10]. Two primary
threat models are proposed in literature. (i) Poisoning attacks, in
which the adversary pollutes the training data to eventually com-
promise the ML systems [9, 41, 61, 62]. Such attacks can be further
categorized as targeted and untargeted attacks. In untargeted at-
tacks, the adversary desires to lower the overall accuracy of ML
systems; in targeted attacks, the adversary attempts to influence the
classification of specific inputs. (ii) Evasion attacks, in which the
adversary modifies the input data during inference to trigger the
systems to misbehave [7, 18, 37, 42]. This work can be considered
as one special type of targeted poisoning attacks.
Compared with simple ML models (e.g., decision tree, support