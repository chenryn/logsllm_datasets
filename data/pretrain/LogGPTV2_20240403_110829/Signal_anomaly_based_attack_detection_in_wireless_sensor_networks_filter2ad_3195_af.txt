a threshold, with a cluster being deﬁned as anomalous if it
exceeds this distance.
An update to the global proﬁle is calculated using the
current and previous time windows. In order to give more
importance to data temporally closer to the current time, a for-
getting coefﬁcient based on Ebbinghaus’ forgetting curve [71]
acts as a weight on the data which reduces the importance of
descriptive data from older time windows.
The scheme was evaluated on the IBRL data set and it was
shown that the distributed approach achieved a TPR and FPR
similar to the centralized approach. The distributed approach
achieved a signiﬁcant reduction in communication overhead.
Another technique that handles a non-stationary distribution
by weighting data vectors is that of Bettencourt et al. [72].
The aim of the technique is to detect events, infer missing
sensor measurements, and identify anomalous sensor readings
within the distributed structure of a WSN. A sensor node
estimates the statistical distributions of the difference between
its own measurements at different time periods and between
its own measurements and that of neighbouring nodes. The
estimated distributions are used to determine the likelihood
of new measurements, accepting or rejecting as appropriate.
This can be performed using parametric or non-parametric
techniques.
In the parametric estimation of the data distribution, in-
cremental updates to the mean and variance use a gain ﬁlter
of value Kt. In order to weight all observations equally a
value of 1
is required. However, it is shown that this is
t
not the optimal value due to correlation in the observations.
As K → 0 the distribution is not updated with the current
measurement, as K → 1 only the current measurement is used
in the prediction of the data distribution. It is shown that there
exists an intermediate value of Kt that minimizes the error
between actual and estimated measurements. Evaluation of the
algorithm shows that with non-stationary data distributions the
average measurement error can be reduced with an appropriate
value of K. However, the parameter K is determined prior to
implementation and in a non-stationary data distribution the
optimal parameter may vary over the lifetime of the WSN.
B. Incremental
A model that performs an incremental update uses the
previous model in combination with the new data vectors
in order to construct the new model. The incremental con-
struction of the model can be divided into two phases. An
incremental update involves incorporating new data vectors
into a model. An incremental downdate involves removing
data vectors from a model, the data vectors that are removed
are usually the oldest data vectors. Incremental updates and
downdates are advantageous as they reduce the computational
complexity of building a model by reusing the old model on
which computational resources have already been expended.
They are designed so that the cost of adding or removing data
vectors from a model is less than that of reconstructing the
entire model with a batch operation.
Incremental model construction often takes the form of
incorporating or removing one new data vector at a time,
we term this a step update or a step downdate. Step updates
and step downdates have an advantage in that there is no
requirement to wait a period of time for new data to be
incorporated in the model. However, updating the model as
each data vector arrives can be computationally expensive.
An additional advantage of an incremental update is that
it may facilitate the discarding of the data set. If the model
performs one-pass learning where each data vector only needs
to be accessed once, then the model contains all the data
required for classiﬁcation and it is not necessary to store
previously seen data.
Subramaniam et al. [73] detail a framework which operates
with an incremental step update and in a distributed manner to
identify outliers in multivariate data. The technique requires
only a single pass over data and has limited memory require-
ments. Outliers are deﬁned based on two metrics; distance-
based outliers and local metric-based outliers [74].
The aim is to ﬁnd an accurate approximation of the data
distribution using kernel density estimators. The algorithms for
estimating the distribution are computationally efﬁcient and
implement an incremental step update by recomputing values
as each new data vector arrives, allowing for an adaptation to
a non-stationary data distribution without delay. A hierarchical
organization of a WSN is used in a distributed learning
framework with each sensor maintaining a model for the
distribution of measurements it generates. A parent node takes
randomly sampled subsets of child node data and combines
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.O’REILLY et al.: ANOMALY DETECTION IN WIRELESS SENSOR NETWORKS IN A NON-STATIONARY ENVIRONMENT
15
them to construct a global model of the distribution of the
child nodes to enable detection of global outliers. Child nodes
communicate outliers to the parent node for classiﬁcation
with the global model. The algorithm uses a ﬁxed sliding
window to adapt to non-stationary distributions, and requires
an application-speciﬁc threshold to detect outliers.
Hill et al. [75] propose a centralized anomaly detection
scheme that operates with an incremental step update on
univariate data streams that have been transmitted to a central
node. It uses a univariate autoregressive model of the data
stream to predict the next value of a sensor measurement
and a conﬁdence interval. A measurement is considered to
be an anomaly if it lies outside the interval. An assumption
is made that the data stream is an order q Markov process
where q is the number of previous measurements used in
the model. Four data-driven methods are used to create the
model to predict the region in which the next sensor reading
should lie; Na¨ıve predictor, nearest cluster predictor, single-
layer linear network and multilayer perceptron. The model for
the speciﬁc method is chosen using cross-validation to tune
the model parameters in order to minimize the training error
on the training set without over-ﬁtting the data.
A ﬁxed sliding window is used to adapt to non-stationary
distributions with an incremental step update being performed
to allow the estimation of the next data instance. Model selec-
tion for the parameters of a speciﬁc method is performed using
10-fold cross-validation on a labelled training set and thus
model selection must be conducted ofﬂine. Evaluations of the
technique using the FPR and false negative rate (FNR) showed
that the multilayer perceptron had the best performance.
The KL divergence metric [68] is used by Li et al. [76] to
detect anomalies in a data set. Similar to the clustering stage of
Chatzigiannakis et al. [33], nodes form clusters based on cor-
relation of data. Sensor nodes broadcast sensed measurements
and their residual energy to h hop neighbours and the KL
divergence metric is used as a measure to determine sensors
with similar observations. A node will form a cluster with
other nodes with similar observations to form a small network.
After clusters have been formed, anomaly detection is
performed using the KL divergence metric. Nodes in a cluster
transmit their data to the cluster head, which calculates and
maintains a median value for the whole cluster. Divergence
between the median value data set and that of a cluster
node is calculated using the KL divergence metric, with
anomalies being identiﬁed as those with a metric below a
predeﬁned threshold. The KL divergence metric is calculated
incrementally using a step update [78]. The scheme has low
computational complexity on a node, however, there is high
communication cost due to the requirement for cluster nodes
to transmit data vectors to the cluster head.
An incremental eigendecomposition is proposed by Chan et
al. [77] where it is used to detect faults in WSNs. PCA is a
dimensionality reduction technique that is commonly used to
detect faults. However, a disadvantage of the technique is the
computational complexity required in the calculation of the
eigendecomposition. A subspace tracking scheme is proposed
where the subspace model is updated recursively in order to
incorporate new data vectors into the subspace and therefore
adapt to a non-stationary data distribution. In addition, the
metrics that deﬁne the anomalies in the subspace are also
updated recursively.
A subspace of the data is spanned by a speciﬁed number
of PCs and in this subspace outliers are identiﬁed. Two
subspace tracking algorithms, PAST [79] and OPAST [80],
are used in order to incrementally update the subspace online
with lower computational complexity. The ﬁrst method is a
rank-1 modiﬁcation to the eigenvectors and eigenvalues of
the subspace based on the work of Abed-Meraim et al. [80]
and having complexity O(B3), where B is the dimension
of the subspace. The second method, based on the work of
Yang [79], uses a deﬂation technique in order to perform
an incremental update to the sequential estimation of the
eigenvectors and eigenvalues. This has signiﬁcantly lower
computational complexity, but is shown to be less accurate.
Fault detection occurs using a robust version of the SPE and
T 2 score as they are less sensitive to the inﬂuence of outliers
in the data set. To enable adaptation to non-stationary data,
the thresholds are recursively updated from measurements.
Evaluation on the WSN data set Networked Aquatic Mi-
crobial Observing System (NAMOS) [81] shows that the tech-
nique offers a signiﬁcant reduction in computational complex-
ity compared with batch PCA while maintaining a similar level
of accuracy to other robust subspace detection methods. The
use of robust subspace tracking where outliers are removed
from the training set reduces the adverse effect of anomalies.
A drawback of the scheme is that data cannot be removed
from the subspace.
VIII. DISCUSSION
In this section we compare current research in the area of
anomaly detection in non-stationary environments in WSNs.
In addition, we discuss the complexity they add to the op-
eration of a WSN. The shortcomings of current research are
detailed and from this we recommend areas for future research.
A. Comparison of Anomaly Detection Techniques
Table I summarizes the main characteristics of the anomaly
detection algorithms detailed in this survey. Table II compares
the computational complexity and accuracy of the techniques
surveyed. Complexity in the form of big O notation is given.
Accuracy is expressed in terms of the TPR and FPR measured
as a percentage and indicates the range of performance the
algorithm obtains on the data set indicated. The algorithms
proposed in [58], [72], [73] are excluded as they use perfor-
mance metrics other than the TPR and the FPR.
The most computationally complex algorithms are those
derived from the OC-SVM such as [31], [47], [55]. This is due
to the solution of the linear programme required in the calcu-
lation of the boundary between normal and anomaly data vec-
tors. In addition, there is no incremental update to reduce the
computation of a new model. However, these algorithms are
also the most accurate, particularly when combined with pa-
rameter optimization such as in [34]. These techniques would
be best suited to environments where fewer model updates
are required along with high accuracy. Hyperellipses, [48],
[57], provide a less computationally complex technique to
detect anomalies. These models have an incremental update
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.16
IEEE COMMUNICATIONS SURVEYS & TUTORIALS, ACCEPTED FOR PUBLICATION
CLASSIFICATION AND COMPARISON. ANOMALY DETECTION ALGORITHMS SURVEYED IN THIS PAPER, AND THE TECHNIQUES USED TO ADAPT TO A
NON-STATIONARY DATA DISTRIBUTIONS AS CATEGORIZED IN THE TAXONOMY (FIG. 4).
TABLE I
Technique
Data
Uni/
Paper
Multivariate
Zhang et al. [47], [55]
Classiﬁcation Multivariate
Moshtaghi et al. [48], [57] Classiﬁcation Multivariate
Rajasegarar et al. [31]
Classiﬁcation Multivariate
Curiac et al. [58]
Classiﬁcation Univariate
Zhang et al. [60]
Statistical
Univariate
Chatzigiannakis et al. [33]
Spectral Multivariate
Xie et al. [63]
Distance Multivariate
O’Reilly et al. [34]
Classiﬁcation Multivariate
Xie et al. [65]
Univariate
Statistical
Beigi et al. [66], [67]
Statistical
Univariate
Livani et al. [69]
Spectral Multivariate
Bettencourt et al. [72]
Statistical
Univariate
Subramaniam et al. [73]
Distance Multivariate
Hill et al. [75]
Univariate
Statistical
Li et al. [76]
Statistical
Univariate
Chan et al. [77]
Spectral Multivariate
Learning
Change Detection Model Selection Sliding Window Model Construction
Local/Distributed Constant Update/
Detect & Retrain
/Centralized
Distributed
Local
Distributed
Distributed
Distributed
Distributed
Distributed
Local
Distributed
Local
Distributed
Distributed
Distributed
Centralized
Distributed
Local
Both
Detect and Retrain
Constant Update
Constant Update
−
Detect and Retrain
Constant Update
Constant Update
Constant Update
Constant Update
Constant Update
Constant Update
Constant Update
Constant Update
Constant Update
Constant Update
Fixed/
Optimized
Fixed
Optimized
Fixed
Fixed
Fixed
Optimized
Optimized
Optimized
Fixed
Optimized
Fixed
Fixed
Fixed
Fixed
Fixed
Fixed
Fixed/
Weighted
Fixed
Fixed
Fixed
Fixed
Fixed
Fixed
Fixed
Fixed
Fixed
Fixed
Weighted
Weighted
Fixed
Fixed
Fixed
Fixed
Batch/
Incremental
Batch
Incremental
Batch
Batch
Batch
Batch
Batch
Batch
Batch
Batch
Batch
Batch
Incremental
Incremental
Incremental
Incremental
and change detection and would be suited to more rapidly
evolving environments where more frequent model updates
are required. Spectral decomposition techniques, [33], [69],
[77], also provide good accuracy but have a computationally