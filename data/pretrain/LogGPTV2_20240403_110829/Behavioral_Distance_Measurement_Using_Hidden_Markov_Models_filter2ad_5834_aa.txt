title:Behavioral Distance Measurement Using Hidden Markov Models
author:Debin Gao and
Michael K. Reiter and
Dawn Xiaodong Song
Behavioral Distance Measurement Using
Hidden Markov Models
Debin Gao, Michael K. Reiter, and Dawn Song
Carnegie Mellon University
PI:EMAIL, PI:EMAIL, PI:EMAIL
Abstract. The behavioral distance between two processes is a measure
of the deviation of their behaviors. Behavioral distance has been proposed
for detecting the compromise of a process, by computing its behavioral
distance from another process executed on the same input. Provided that
the two processes are diverse and so unlikely to fall prey to the same at-
tacks, an increase in behavioral distance might indicate the compromise
of one of them. In this paper we propose a new approach to behavioral
distance calculation using a new type of Hidden Markov Model. We also
empirically evaluate the intrusion detection capability of our proposal
when used to measure the distance between the system-call behaviors of
diverse web servers. Our experiments show that it detects intrusions with
substantially greater accuracy and with performance overhead compara-
ble to that of prior proposals.
Keywords: intrusion detection, anomaly detection, system call, behav-
ioral distance.
1 Introduction
A predominant form of host-based anomaly detection involves monitoring a pro-
cess to see if its behavior conforms to the program it is ostensibly executing,
e.g., see [15,35,32,28,19,14,17,13,20,16]. Deviation from the behavior prescribed
by a program is characteristic of, e.g., code-injection attacks exploiting buﬀer
overﬂow or format-string vulnerabilities, and so should be investigated. A cen-
tral research challenge is constructing the model to which the process behavior
is compared. This is especially challenging in light of mimicry attacks [31,33]
on virtually all such models, wherein an adversary injects code that executes its
attacks using behaviors that the model does not distinguish from normal.
To better combat mimicry, Gao et al. proposed comparing the behavior of a
process to the behavior of another process that is executing on the same input but
that either runs on a diﬀerent operating system or runs a diﬀerent program that
has similar functionality [18]. Assuming their diversity renders these processes
vulnerable only to diﬀerent exploits, a successful attack on one of them should
induce a detectable increase in the “distance” between the behaviors of the two
processes. In principle, this would make mimicry substantially more diﬃcult,
since to avoid detection, the behavior of the compromised process must be close
to the simultaneous behavior of the uncompromised one. Gao et al. proposed
D. Zamboni and C. Kruegel (Eds.): RAID 2006, LNCS 4219, pp. 19–40, 2006.
c(cid:2) Springer-Verlag Berlin Heidelberg 2006
20
D. Gao, M.K. Reiter, and D. Song
an approach based on evolutionary distance (ED) [29] to compute behavioral
distance, and measured the accuracy and performance of an implementation of
this approach when the behavior of each process is the system calls it emits.
In this paper we propose an alternative approach based on a novel Hidden
Markov Model (HMM) for computing behavioral distance. An HMM models a
doubly stochastic process; there is an underlying stochastic process that is not
observable (it is “hidden”) but that inﬂuences another that produces a sequence
of observable symbols. When applied to our problem of computing behavioral
distance, the observed symbols are process behaviors (e.g., emitted system calls),
and the hidden states correspond to aggregate tasks performed by the processes
(e.g., read from a ﬁle). Since these hidden tasks should be the same (if the pro-
cesses are running the same program on diﬀerent platforms) or at least similar
(if the processes are running diﬀerent programs that oﬀer the same function-
ality, e.g., two diﬀerent web servers), it should be possible to reliably correlate
the simultaneous observable behaviors of the two processes when no attack is
occurring, and to notice an increased behavioral distance when an attack suc-
ceeds on one of them. Perhaps surprisingly, our technique uses a single HMM to
model both processes simultaneously, in contrast to traditional uses of HMMs
for anomaly detection (e.g., [34,10]), where an HMM models a single process.
We detail the distance calculation and model construction algorithms for our
HMM-based anomaly detector and evaluate an implementation of it by cal-
culating behavioral distances between processes executing diﬀerent web servers
(Apache1, Abyss2, and MyServer3) on diﬀerent platforms (Linux and Windows).
Since a signiﬁcant motivation for this work is constraining mimicry attacks, we
also provide an algorithm for estimating the best mimicry against an HMM,
and evaluate the false-alarm rate of our approach when the behavioral-distance
threshold is set to detect this estimated-best mimicry. In doing so, we show that
our approach yields better results than the ED approach of Gao et al., in many
cases oﬀering substantial improvement in the false-alarm rate. At the same time,
the computational cost is comparable to that of the ED approach in our exper-
iments. As such, we argue that the HMM approach oﬀers substantially superior
properties for calculating behavioral distance for anomaly detection.
An alternative strategy to building a behavioral distance measure would be
to manually construct a mapping between system calls, or sequences of system
calls, on the two platforms of interest. In some cases, such an approach might
be aided by the existence of tools such as WINE (http://www.winehq.com/),
which provides libraries that implement Windows API calls on UNIX to enable
the execution of Windows applications on UNIX platforms. For example, an
anomaly detector could pattern-match Windows system calls against patterns
induced by a call to the Windows API, and then search the Linux system calls
for a sequence that corresponds to the WINE implementation of that Windows
API call for UNIX. To our knowledge, such an approach has not been studied
1 http://httpd.apache.org
2 http://www.aprelium.com
3 http://www.myserverproject.net
Behavioral Distance Measurement Using Hidden Markov Models
21
to date, and we eschew it for several reasons. First, we strive for a more general
approach that need not be totally reengineered for each new operating system;
e.g., we would like an approach that applies with little additional eﬀort to,
say, Windows CE and Symbian OS. Second, we want to measure the behavioral
distance between even diﬀerent application codebases (e.g., between the separate
codebases of Apache for UNIX and Windows), and we do not expect this manual
approach to work well for this case. Third, constructing this mapping manually
can be a very substantial eﬀort; e.g., WINE began in 1993 and, at the time of
this writing, claims to have UNIX implementations for only 63% of the Windows
API (see http://www.winehq.com/site/winapi stats).
Uses of behavioral distance incur the cost of executing each request multiple
times. As such, behavioral-distance-based anomaly detection can be most seam-
lessly integrated into services that already redundantly execute requests for the
purposes of detecting (e.g., [30,5,2]) or masking (e.g., [22,27,26,7,6,36,1]) Byzan-
tine faults or intrusions. These approaches ensure that clients receive only correct
responses even if a limited number of servers are compromised, by comparing
server outputs before they are conveyed to the client (“output voting”). How-
ever, a compromised server can do more than simply attempt to mislead a client,
e.g., exﬁltrating data or attacking other servers, while continuing to provide the
proper output to clients. These attacks have typically not been considered in the
aforementioned intrusion-tolerant architectures, and since there is already need
for servers to be diverse (so as to not fail simultaneously, e.g., see [9,8,11]), these
architectures are ripe for the integration of behavioral-distance-based anomaly
detection to augment the protections they provide.
2 Related Work
Behavioral-distance-based anomaly detection is most closely related to the re-
cent work of Cox et al. on N-variant systems [11]. In an N-variant system,
the behaviors of multiple processes on a common input are compared to de-
tect deviations, as in the framework we consider here. The focus in N-variant
systems, however, is to construct these multiple processes through mechanical
transformation so that necessary conditions for a certain type of attack cannot
be satisﬁed in all processes. For example, if two processes are created to execute
the same program but with disjoint address spaces (i.e., an address valid in one
is necessarily invalid in the other), then an attack that depends on accessing an
absolute address will crash at least one of the processes. Cox et al. anticipate
the use of a monitor to detect attacks other than by output voting, though to
our knowledge they have not explored monitoring behavior at the system-call
level or via any technique as general as the approach we describe here. Another
diﬀerence is that the N-variant system usually requires a special compiler or
a binary rewriter to construct a variant, whereas our approach is a black-box
approach which does not require source code or static analysis of the binary.
Another technique proposed to make mimicry attacks more diﬃcult utilizes
system-call arguments (e.g., [21,4]). Models for detecting anomalous system calls
22
D. Gao, M.K. Reiter, and D. Song
typically monitor the system-call numbers but not their arguments, and so a
mimicry attack can issue system calls that are consistent with the model but
for which the arguments of certain calls are modiﬁed to be “malicious”. To the
extent that system-call arguments can be accurately modeled, this can increase
the diﬃculty of mimicry attacks. While we do not utilize system-call arguments
in this work, it is potentially a way to augment the strength of our technique.
The key to the technique we present here is a novel HMM construction. HMMs
have been studied for decades and used in a wide variety of applications, owing to
two features: First, HMMs are very rich in mathematical structure and hence can
form the theoretical basis for a wide range of applications. Second, when applied
properly, HMMs work very well in practice for many important applications.
One of the most successful applications of HMMs is in speech recognition [25].
HMMs have also been used in intrusion detection systems, e.g., to model the
system-call behavior of a single process [34], and to model privilege ﬂows [10].
However, these HMMs are designed to model the behavior of a single process,
as opposed to the joint behavior of two processes as we require here.
Variations of ordinary HMMs might seem to be more suited to our needs. For
example, “pair HMMs” [23] and “generalized pair HMMs” [24] have been used to
model joint distributions, speciﬁcally to predict the gene structures of two unan-
notated input DNA sequences. However, these variations of HMMs only model
two observable sequences where symbols are drawn from the same alphabet. In
our case, not only are the alphabets—i.e., the system calls on diverse platforms—
diﬀerent, but the correspondences between these alphabets are not known and
are not one-to-one. As such, we have been unable to directly adapt these prior
techniques to our problem, and have devised a custom solution, instead.
3 Motivation for Our Approach
(cid:3)
(cid:3)
S2 = (cid:2)s2,1, s2,2, . . . , s2,l2
S1 = (cid:2)s1,1, s1,2, . . . , s1,l1
In a nutshell, the problem is to assign a distance to a pair of system call sequences
(1)
emitted by two processes while processing the same input. Here, each si,j denotes
the system-call number (a natural number) of the j-th system call by the i-th
process. The distance should indicate whether these sequences reﬂect similar
activities. Producing this distance is complicated by the fact that the processes
might be running on diverse platforms, and so the set of system calls C1 =
{s1,j}1≤j≤l1 on the ﬁrst platform can be diﬀerent from the set C2 = {s2,j}1≤j≤l2
on the second platform. Moreover, even a shared symbol c ∈ C1∩C2 has diﬀerent
semantics on the two platforms. Of course, generally l1 (cid:6)= l2.
The evolutionary distance (ED) approach [18] to computing the distance of
(1), roughly speaking, was to consider all possible ways of inserting dummy
symbols σ into them to generate an alignment
(cid:2)s
(cid:4)
2,1, s
(cid:2)s
(cid:4)
(cid:4)
(cid:4)
(2)
1,2, . . . , s
1,1, s
1,l
1 ≥ l1, l
2 ≥ l2, and l
(cid:4)
(cid:4)
(cid:4)
(cid:4)
2. The distance for alignment (2) was sim-
1 = l
(cid:4)
(cid:4)
2,j), where dist was a table of distances between system calls
j dist(s
1,j, s
(cid:4)
(cid:4)
2,2, . . . , s
2,l
where l
(cid:2)
ply
(cid:3)
(cid:2)
2
(cid:3)
(cid:2)
1
Behavioral Distance Measurement Using Hidden Markov Models
23
learned from training sequences (pairs of system call sequences output by the
processes in a benign environment). The distance for (1), then, was the distance
of the alignment with the smallest distance.
Though we have omitted numerous details of the ED approach, one limitation
is immediately apparent: it does not take adequate account of the order of system
calls in each sequence. For example, reversing the two sequences (1) yields the
same behavioral distance. Since system-call order is known to be important to
detecting intrusions (e.g., [15,28,17,16]), this is a signiﬁcant limitation.
Our use of an HMM for calculating the behavioral distance of sequences (1)
addresses this limitation. We use a single HMM to model both processes, and so
a pair of system calls [s1,·, s2,·], one from each process, is an observable symbol of
the HMM. Each such observable symbol can be emitted by hidden states of the
HMM with some ﬁnite probability. Intuitively, if the system calls in an observable
symbol perform similar tasks, then the probability should be high, otherwise the
probability should be low. This probability serves the same purpose as the dist
table in the ED approach. However, in HMM-based behavioral distance, the
probability of emitting the same observable symbol is generally diﬀerent for
diﬀerent states, whereas in ED-based behavioral distance, a universal dist table
is used for every system call pair in the system call sequences. In this way, our
HMM model better accounts for the order of system calls.
The way in which we use our HMM is slightly diﬀerent from HMM use in many
other applications. For example, in HMM-based speech recognition, the primary
algorithmic challenge is to ﬁnd the most probable state sequence (what is being
said) given the observable symbol sequence (the recorded sounds). However, in
behavioral distance, we are not concerned about the tasks (the hidden states)
that gave rise to the observed system call sequences, but rather are concerned
only that they match. Therefore, the main HMM problem we need to solve is
to determine the probability with which the given system call sequences would
be generated (together) by the HMM model—we take this probability as our
measure of the behavioral distance. We show how to calculate this probability
eﬃciently in Section 4.
4 The Hidden Markov Model
In this section, we introduce our Hidden Markov Model and describe how it
is used for behavioral distance calculation. We begin in Section 4.1 with an
overview of the HMM. We then present our algorithm for calculating the behav-
ioral distance in Section 4.2, and describe the original construction of the HMM
in Section 4.3.
4.1 Elements of the HMM
Our HMM λ = (Q, V, A, B) consists of the following components:
– A set Q = {q0, q1, q2, . . . , qN , qN+1} of states, where q0 is a designated start
state, and qN+1 is a designated end state.
(cid:2)
D. Gao, M.K. Reiter, and D. Song
24
– A set V = {[x, y] : x ∈ C1 ∪ {σ}, y ∈ C2 ∪ {σ}} of output symbols. Recall
that C1 and C2 are the sets of system calls4 observed on platforms 1 and 2,
respectively, and that σ denotes a designated dummy symbol.
– A set A = {ai}0≤i≤N of state transition probability distributions. Each
ai : {1, . . . , N +1} → [0, 1] satisﬁes
j ai(j) = 1. ai(j) is the probability that
the HMM, when in state qi, will next enter qj. We will typically denote ai(j)
with ai,j. We stipulate that a0,N+1 = 0, i.e., the HMM does not transition
directly from the start state to the end state. Note that ai is undeﬁned for
i = N + 1, i.e., there are no transitions from the end state. Similarly, ai,0 is
undeﬁned for all i, since there are no transitions to the start state.
– A set B = {bi}1≤i≤N of symbol emission probability distributions. Each
bi : (C1 ∪ {σ}) × (C2 ∪ {σ}) → [0, 1] satisﬁes
[x,y] bi([x, y]) = 1. bi([x, y])
is the probability of the HMM emitting [x, y] when in state qi. We require
that for all i, bi([σ, σ]) = 0. Note that neither b0 nor bN+1 is deﬁned, i.e.,
the start and end states do not emit symbols.
As we discussed in Section 3, we will take our measure of behavioral distance
to be the probability with which the HMM λ “generates” the pair of system call
sequences of interest. This probability is computed with respect to the following
experiment, which we refer to as “executing” the HMM:
1. Initialize λ with q0 as the current state.
2. Repeat the following until qN+1 is the current state:
(a) If qi is the current state, then select a new state qj according to the
(b) After transitioning to the new state qj, if qj (cid:6)= qN+1 then select an output
probability distribution ai and assign qj to be the new current state.
(cid:2)
symbol [x, y] according to the probability distribution bj and emit it.
Speciﬁcally, we deﬁne an execution π of the HMM λ to consist of a state
sequence qi0 , qi1 , . . . , qiT , where i0 = 0 and iT = N + 1, and observable symbols
[xi1 , yi1], . . . , [xiT −1 , yiT −1]. The experiment above assigns to each execution a
probability, i.e., the probability the experiment traverses exactly that sequence
of states and emits exactly that sequence of observable symbols; we denote by
Prλ(π) the probability of execution π when executing HMM λ.
For the HMM λ we will build, there are many executions that generate the
given pair of sequences [S1, S2] as in (1). We use Exλ([S1, S2]) to denote the set
of executions of λ that generate [S1, S2]. The probability that λ generates the
sequences [S1, S2] in (1), which we denote Prλ([S1, S2]), is the probability that
λ, in the experiment above, emits pairs [xi1 , yi1], . . . , [xiT −1, yiT −1] such that
(cid:2)xi1 , xi2 , . . . , xiT −1
(cid:3)
(cid:2)yi1 , yi2, . . . , yiT −1
(cid:3)
is an alignment (as in (2)) of those sequences. Note that
Prλ([S1, S2]) =
Prλ(π)
(cid:3)