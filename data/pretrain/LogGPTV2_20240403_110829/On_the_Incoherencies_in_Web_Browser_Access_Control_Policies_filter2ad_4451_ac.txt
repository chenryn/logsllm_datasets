users into performing unintended actions. In this section,
we examine the user principal resources and describe our
ﬁndings on how they may be accessed improperly by web
applications.
1) User actions: The focus and blur properties of the
window object allow web sites to change focus between the
windows that they opened irrespective of the origins. This
enables an attacker site to steal focus or cause the user to
act on a window unintentionally.
APIs to access clipboard data. A web site can get
contents of a user’s clipboard by successfully calling
window.clipboardData.getData("Text").
Depending on the default Internet security settings,
the
browser may prompt user before getting the data. However,
the prompt does not
identify the principal making the
request (simply using the term “this site”). As a result, a
malicious script embedded on a third-party frame may trick
the user into giving away his clipboard because he thinks
that such access is being requested by the trusted top-level
site.
Geolocation is one of the latest browser features that
allows a site to determine the client’s location by using the
navigator.geolocation [12] interface. At the time
of writing, Firefox 3.5 is the only stable production browser
supporting this HTML5 feature. Geolocation is user-private
data. Today’s browsers do ask user permission before access-
ing it. However, issues arise when a site embeds content
from multiple principals (i.e., in frames), and more than
one origin needs access to geolocation information. The
geolocation dialog is active for only one origin at a time;
if there is a request to access geolocation from b.com
while the dialog for a.com is still active, it is ignored
— the principal that succeeds in invoking the geolocation
dialog ﬁrst wins. Therefore, if a malicious party manages to
embed a script (or a frame) on the page, it can prevent the
main site from triggering the geolocation dialog by invoking
the dialog ﬁrst. As a result, the malicious party can create
denial-of-service against the main site, preventing it from
retrieving a user’s geolocation information. Additionally, it
could trick the user into giving away location to itself rather
than the main site (e.g., using phishing domain names like
www.gooogle.com).
Changing document.domain also generates inconsis-
tencies. The geolocation prompt is designed to work only
with the original principals, and even if a site changes
identity, the prompt still displays the original domain as
the requesting domain. For an example site good.a.com
that changes its document.domain to a.com, this causes the
following problems:
• If
an
attacker
site evil.a.com changes
its
it can steal position
if good.a.com
in
is accessible via the DOM (e.g.,
parent.document.getElementById(
document.domain to a.com,
information from good.a.com,
has
this
a place that
using
"coords").innerHTML).
information
displayed
stored
or
• If another site evil.a.com also changes its domain
it could impersonate good.a.com,
using parent.navigator.geolocation
trigger
instead of
to a.com,
by
.getCurrentPosition, which would
the access prompt using good.a.com,
evil.a.com.
III. THE WEBANALYZER MEASUREMENT FRAMEWORK
To achieve consistent browser access control poli-
cies, browser vendors need to remove or modify the
features
that contribute to incoherencies. For exam-
ple, disallowing domain-setting for cookies, eliminating
document.domain, and removing support for access-
ing user principal resources are steps towards secure new
browsers. However, this begs the question of what the cost of
these feature removals is and how many web sites will break
as a result. In today’s highly competitive browser market,
backward compatibility with the existing web is paramount.
To help browser vendors balance security and compati-
bility, we set off to build a measurement system to measure
the cost of security. Many previous web compatibility studies
have been browser-centric: they have evaluated the degree to
which a given browser supports various web standards or is
vulnerable to attacks [22], [23]. In contrast, we take a web-
centric perspective and actively crawl the web to look for
prevalence of unsafe browser features on existing web pages.
Compared to existing crawlers, however, static web page
inspection is insufﬁcient. Dynamic features such as AJAX
or post-render script events require us to actively render a
web page to analyze its behavior at run time. Moreover, the
incoherencies we identiﬁed in Section II require analysis of
not just a page’s JavaScript execution [24], but also DOM
interactions, display layout, and protocol-layer data.
To address these challenges, we have constructed a scal-
able, execution-based crawling platform, called WebAna-
lyzer, that can inspect a large number of web pages by
rendering them in an instrumented browser. The platform
consumes a list of URLs (deﬁned by a human operator or
generated by a traditional web crawler), and distributes them
among virtual machine workers, which renders them using
IEWA, a specially instrumented version of Internet Explorer.
IEWA provides dynamic mediation for all browser resources,
and detects when a resource invocation matches one of
preset policy rules. Even though our framework is extensible
to a large variety of browser policies, we concentrate on “un-
safe feature” rules derived from our analysis in Section II.
the central piece of our measurement
platform, we leverage public COM interfaces and exten-
sibility APIs exported by Internet Explorer 8. Figure 5
shows the architecture of IEWA, which centers around three
major interposition modules: (1) a script engine proxy, which
provides JavaScript and DOM interposition, (2) a network
proxy based on Fiddler [25], and (3) display dumper, which
enables custom analysis of a page’s layout as it is visible to
the user. Next, we discuss each module in turn.
To build IEWA,
Script engine proxy. We build on our earlier system
in MashupOS [1] to implement a JavaScript engine proxy
(called script engine proxy (SEP)): SEP is installed be-
tween IE’s rendering and script engines, and it mediates
and customizes DOM object interactions. SEP exports the
Figure 5. High-Level Architecture of IEWA.
script engine API to IE’s renderer, and it exports the DOM
and rendering interfaces to IE’s script engine. Each DOM
object
is interposed by a corresponding object wrapper.
When IE’s script engine asks for a DOM object from the
rendering engine, SEP intercepts the request, retrieves the
corresponding DOM object, associates the DOM object with
its wrapper object inside SEP, and then passes the wrapper
object back to the original script engine. Any subsequent
invocation of wrapper object methods from the original
script engine passes through SEP. SEP is implemented as
a COM object and is installed into IE by modifying IE’s
JavaScript engine ID in the Windows registry.
Network interposition. In addition to SEP, we route the
browser’s network trafﬁc through a proxy to monitor all
HTTP/HTTPS requests and analyze cookie transfers as well
as network APIs like XMLHttpRequest. Our network proxy
is implemented using the FiddlerCore interfaces provided by
the public-domain Fiddler web debugging proxy [25], [26].
Display analysis. In order to evaluate display policies, it
is necessary to analyze a browser’s visual output as seen by
the user. For this purpose, we use a customized version of
IE’s rendering engine that exposes COM interfaces to extract
a textual representation of a particular page’s visual layout
at any stage of rendering. In our current evaluation, we use
these COM interfaces to save a snapshot log of IE’s display
after a page has fully loaded. Because some pages have
post-render events that alter layout, we wait an additional
5 seconds before taking a display snapshot. Snapshot logs
provide a mapping between a page’s objects and their layout
properties, such as position, dimensions, or transparency.
They can be analyzed ofﬂine for the presence of unsafe
frame overlapping behavior or other dangerous page layouts.
Navigation. To facilitate automatic analysis for a large
number of URLs, IEWA includes a URL navigation en-
gine, which utilizes IE’s extensibility interfaces, such as
IWebBrowser2, to completely automate the browser’s nav-
igation. In addition to pointing the browser to new URLs,
this module also cleans up state such as pop-ups between
consecutive URLs, detects when sites fail to render (e.g.,
404 errors), and recovers from any browser crashes.
Visiting a site’s home page is sometimes insufﬁcient to
invoke the site’s core functionality. For example, a feature
may be accessed only when the user clicks on a link, types
search queries, or causes mouse event handlers to run.
It is difﬁcult and time-consuming to fully automate a site’s
analysis to study all possible features and pages that could
be invoked using all combinations of user input. Instead of
aiming for complete coverage within a particular site, we
enhanced our navigation engine with simple heuristics that
simulate some user interaction. After rendering a site’s home
page, IEWA will ﬁnd and simulate a click on at most ﬁve
random links, producing ﬁve random navigation events. In
addition, IEWA will check for presence of a search form,
ﬁll it with random keywords, and submit it. We restrict all
simulated navigations to stay within the same origin as a
site’s home page.
These simple enhancements maintain our ability to ex-
amine a large number of sites while adding the ability to
properly handle many (but not all) sites with home pages
that do not invoke the site’s main functionality. For example,
we can navigate to a random article on Wikipedia, a random
video on YouTube, a random proﬁle on MySpace, a random
Twitter feed, and a random search query on Google. We
evaluate the success of this methodology against a user-
driven browsing study in Section IV-G and discuss its
limitations in Section V.
Performance. We deployed our system on several desktop
machines, each with an Intel 2.4 GHz quad-core CPU and
4 GB of RAM. Our IEWA workers run inside a Windows
Vista VMware virtual machine to prevent malware infection.
We executed multiple workers in each VM, isolating them
from one another using different UIDs and different remote
desktop sessions.
On such a setup, one IEWA worker is able to analyze about
115 typical web sites per hour. Each site’s processing time
includes the home page, ﬁve random link clicks, and one
form submission, as well as overheads introduced by IEWA’s
three interposition modules. We found that we could execute
up to eight parallel workers in one VM, for a throughput of
900 sites per VM, before saturating the CPU. Optimizing
this infrastructure for performance was not a goal of this
paper and is left as future work.
IV. EXPERIMENTAL RESULTS
Our analysis in Section II provides an understanding of the
security characteristics of the current access control policies
in browsers. In this section, we complete the other half
of the equilibrium by using the measurement infrastructure
presented in Section III to study the prevalence of unsafe
browser features (analyzed in Section II) on a large set of
popular web sites. By presenting both sides, we enable the
browser vendors to make more informed decisions about
whether or not to continue supporting a particular unsafe
feature based on its real-world usage.
A. Experimental overview
1) Choosing the sites for analysis: Instead of randomly
crawling the web and looking for unsafe features, we de-
cided to focus our attention on the “interesting” parts of the
web that people tend to visit often. Accordingly, to seed our
analysis, we take the set of 100,000 most popular web sites
ranked by Alexa [5], as seen on November 9, 2009, as our
representative data set. The data collection and analysis were
completed in the last week of February 2010.
2) Deﬁning the compatibility cost: We deﬁne the cost of
removing a feature to be the number of Alexa-ranked, top
100,000 sites that use the feature.
We conservatively assume that disallowing a feature will
signiﬁcantly hinder a site’s functionality, whereas it could
simply cause a visual nuisance. A more detailed analysis on
the effect of policy changes on page behavior is promising
but is left as future work.
3) High-level results: We obtained our results by ren-
dering each of the 100,000 seed links using WebAnalyzer,
saving all interposition logs for ofﬂine analysis. This way,
we were able to obtain data for 89,222 of the 100,000 sites.
There are several reasons why no data was produced for the
rest of sites. First, some sites could not be accessed at the
time of our analysis due to failed DNS lookups, “404 Not
Found” errors, and other similar access problems. Second,
some sites timed out within our chosen threshold interval of
2 minutes, due to their slow or continuous rendering. We
decided to drop any such sites from our analysis. Finally,
some sites did not contain any JavaScript code, and as a
result they did not trigger our event ﬁlters. Nonetheless, we
believe that we have been able to analyze a sufﬁciently large
set of sites with a reasonable success ratio, and our data set
and the scope of measurement is much larger than that used
by earlier related studies [24].
Tables IV, V, and VI present the results of our analysis,
showing how frequently each feature we analyzed earlier is
encountered. Next, we organize our ﬁndings according to
our discussion in Section II and discuss their implications
on future browser security policies.
B. The interplay of browser resources
1) DOM and Cookies: Cookie usage is extremely pop-
ular, and so is their programmatic DOM access via
document.cookie, which we found on 81% web sites
for reading and 76% of web sites for writing cookie values,
respectively. The use of the cookie’s domain attribute is
also widespread (67% of sites), with about 46% of sites
using it to actually change the domain value of the cookie.
As a result, the issues described in Section II-C1 cannot
be solved by simply deprecating the usage of this attribute
and changing the principal deﬁnition of cookies. One pos-
sible approach to solve the inconsistency issue with cookie
handling is to tag the cookie with the origin of the page
setting the cookie. This information should be passed to the
server to allow the server to differentiate between duplicate
cookies.
Section II-C1 also identiﬁed inconsistencies pertaining
to cookies and HTTP/HTTPS, which we now support
with measurements. First, 0.07% of sites alarmingly send
secure cookies over HTTP. This effectively tampers with
the integrity of cookies that may have been intended for
HTTPS sessions [10]. Fortunately, it appears that this func-
tionality can be disallowed with little cost. Surprisingly, a
much larger number of sites (5.48%) sent HTTP cookies
over HTTPS. The HTTP cookies cannot be kept conﬁdential
and are accessible to HTTP sessions. Our recommended
solution to this problem is that the “secure” ﬂag should be
enforced for any cookies passed over an HTTPS connection
even if the web developer fails to set the ﬂag. This would
still enable the HTTPS site to access the cookie for its own
functionality and any sharing with the HTTP site should be
done explicitly.
We found a large number of sites (16.2%) using HttpOnly
cookies, which is an encouraging sign — many sites appear
to be tightening up their cookie usage to better resist XSS
attacks.
2) Cookies and XMLHttpRequest: Our measurements
show that the issues arising from undesirable interplay of
XMLHttpRequest and HttpOnly cookies (Section II-C2)
can possibly be eliminated, since very few sites (0.30%)
manipulate cookie headers in XMLHttpRequest responses.
the
descendant navigation policy is at conﬂict with SOP for
DOM. We observe iframe navigations on 7.7% of sites and
all of them are child navigation (regardless of the origin).
The absence of descendant navigation in the top 100,000
sites indicates a potentially very low cost to remove it.
3) DOM and Display: Section II-C3 argued that
Measurement Criteria
document.cookie (read)
document.cookie (write)
document.cookie domain usage (read)
document.cookie domain usage (write)
Secure cookies over HTTP
Non-secure cookies over HTTPS
Use of “HttpOnly” cookies
Frequency of duplicate cookies
Use of XMLHttpRequest
Cookie read in response of XMLHttpRequest
Cross-origin descendant navigation (reading descendant’s location)
Cross-origin descendant navigation (changing descendant’s location)
Child navigation (parent navigating direct child)
document.domain (read)
document.domain (write)
Use of cookies after change of effective domain
Use of XMLHttpRequest after change of effective domain
Use of postMessage after change of effective domain
Use of localStorage after change of effective domain
Use of local storage
Use of session storage
Use of fragment identiﬁer for communication
Use of postMessage
Use of postMessage (with no speciﬁed target)
Use of XDomainRequest
Presence of JavaScript within CSS
Total instances
(count)
Unique sites
Count
Percentage
5656310
2313359
2032522
1226800
259
15589
33180
159755
19717
1261
6043
0
22572
1253274