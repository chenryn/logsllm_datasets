dividual user (Wesley or Jack) was a member of that group.
The difference between the two tasks was that in the Wesley
task, Wesley was inheriting READ and WRITE permissions
from ProjectF, but not ADMINISTRATE permission, while
in the Jack task, Jack was inheriting READ and WRITE as
well as ADMINISTRATE permissions from ProjectE.
The simple solution to the Wesley task was to add Wes-
ley to the ACL and explicitly deny him WRITE permission;
he was already allowed READ permission from ProjectF.
However, this simple solution did not work for Jack, since
Jack was inheriting ADMINISTRATE permission as well as
READ and WRITE permission. If Jack was denied WRITE
permission, but not explicitly denied ADMINISTRATE per-
mission, he would have been able to restore his WRITE per-
mission. The task statement presented to users did not men-
tion this nuance; it was left to the interfaces to provide the
cues needed to understand that Jack’s ADMINISTRATE per-
mission had to be removed.
6.4 Rules for completing tasks
To ensure as realistic an environment as possible with-
out compromising the experimental comparison between
the two interfaces, it was necessary to establish certain rules
for participants’ interaction. First, participants in both inter-
face conditions were allowed to look up group membership
information using the XP Computer Management interface,
which is a separate application from the ﬁle-permissions in-
terfaces. However, participants were instructed not to use
this interface to change group permissions. Had they been
allowed to do so, they could have solved the Wesley and
Jack tasks by removing Wesley or Jack from their respec-
tive groups without using the ﬁle-permissions interfaces,
which would have defeated the purpose of comparing XPFP
to Salmon. Second, to compensate for the restriction on
changing group memberships, participants were told that
if a task statement did not explicitly mention a given user,
any permission setting was permissible for that user. Thus,
participants could change the permission settings for the
groups ProjectE or ProjectF and not be concerned about
the effects of these permission changes on members of the
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:08:40 UTC from IEEE Xplore.  Restrictions apply. 
groups other than Wesley and Jack. Finally, participants
were allowed to access a set of online Windows Help ﬁles
that applied to setting NTFS permissions, but they were not
permitted to browse the entire set of Windows Help ﬁles.
6.5 Procedure
Participants were asked to think aloud during their ses-
sions, and were instructed in doing so according to direc-
tions adapted from Ericsson and Simon [6]. Participants
were shown how to view system users, groups, and group
memberships using the XP Computer Management inter-
face, and were shown how to access Help ﬁles. Partici-
pants were not given any instruction in using the XPFP or
Salmon interfaces. Following instruction on the XP Com-
puter Management interface and the Help ﬁles, participants
were given the tasks. Before each task, the experimenter
brought up the interface the participant was to use for the
experiment. Then task statements were presented in text in
a Web browser; these remained available to the participant
throughout the task. All participants were given the training
task ﬁrst, but after that, presentation order of the remain-
ing tasks was counterbalanced among participants using a
Latin square design. Participants were given 8 minutes to
complete each task (an expert could complete the task in
under one minute).
7 Data analysis
Data from the user studies were analyzed for speed, ac-
curacy, and error counts. Speed was straightforward to mea-
sure using time to task completion. Data analysis for accu-
racy and error results consisted of the following ﬁve steps:
1. For each of the two tasks, Wesley and Jack, apply a
Hierarchical Task Analysis (HTA: see Section 7.1) to
determine the steps necessary to complete the task;
2. For each task instance, determine whether the user suc-
ceeded or failed at completing the task;
3. For each task instance, list all actions taken by the user;
4. For each action taken, classify it as an error or a non-
error by comparing it to the steps listed in the HTAs;
5. For each error, classify it as one of four types of error:
goal, plan, action, or perception.
7.1 Step 1: Hierarchical Task Analysis
To aid in the identiﬁcation of errors, a Hierarchical Task
Analysis (HTA) was applied to the Wesley and Jack tasks.
HTA, as described by Kirwan [8], is a tool for breaking a
task into its constituents - the goals, plans, and actions re-
quired to complete the task. An HTA diagram for the Jack
task is shown in Figure 3. As the ﬁgure shows, each task
has a root goal that is decomposed into subgoals, which are
in turn decomposed into actions. Plans express constraints
on the choice or ordering of actions.
Goal:  Jack should be allowed to read but not change the file
PLAN: Any order
Subgoal 1:  Allow jack 
effective READ permission
Subgoal 2: Override or 
Subgoal 3:  Override or 
eliminate ProjectE's
WRITE permission
eliminate ProjectE's
ADMINISTRATE permission
PLAN: 1.1-1.2 
in order OR 1.3
PLAN: 2.1-2.2 in 
order OR 2.3 OR 2.4
PLAN: 3.1-3.2 in 
order OR 3.3 OR 3.4
1.1 Add 
jack
1.2 Allow 
jack READ
2.1 Add 
jack
2.2 Deny 
jack WRITE
3.1 Add 
jack
3.2 Deny jack 
ADMINISTRATE
1.3 Let jack inherit 
READ permission from 
ProjectE
2.3 Deny or Unset
ProjectE’s WRITE
permission
2.4 Remove 
ProjectE
3.3 Deny or Unset ProjectE’s 
ADMINISTRATE permission
3.4 Remove 
ProjectE
Figure 3: Hierarchical Task Analysis of the Jack task.
7.2 Step 2: Determining task success or failure
To determine task successes and failures, participants’ ﬁ-
nal permission settings were examined. A task was judged
successful if the operative individual (Wesley or Jack) had
effective permissions allowing him READ permission and
denying him WRITE and ADMINISTRATE permissions. A
task was judged a failure if the operative individual had ef-
fective permissions denying READ permission, or allowing
WRITE and/or ADMINISTRATE permission. EXECUTE and
DELETE permissions and all permissions for other entities
were ignored.
7.3 Step 3: Listing actions
Actions were deﬁned for the purpose of dividing user
protocol data into discrete units for error analysis. An action
was deﬁned as any change to the access control list (ACL),
i.e., adding an entity to or removing an entity from the ACL,
or altering the permissions of an entity already on the ACL.
7.4 Step 4: Classifying actions as errors
Once actions for each task instance were listed, they
were compared to the actions listed in the HTA for the corre-
sponding task (Wesley or Jack). Each discrepancy between
user actions and HTA actions was classiﬁed as an error of
commission, an error of omission, or a non-error. A user
action was an error of commission if it was unnecessary
according to the HTA and could lead to failure if not recov-
ered from. A user action was an error of omission if it was a
necessary action according to the HTA, but the user failed to
complete it. Non-errors included user actions that matched
actions in the HTA, and unnecessary but innocuous actions,
such as changing permissions in an interface to “see what
happens” and then changing them back.
7.5 Step 5: Classifying errors by THEA type
Pocock et al.’s THEA [14] proposes four stages of hu-
man information processing that map directly to the four er-
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:08:40 UTC from IEEE Xplore.  Restrictions apply. 
ror types used to categorize errors in this work: goal, plan,
action, and perception errors. Because error classiﬁcation
was not their main objective, Pocock et al. are not perfectly
clear about the criteria for classifying a speciﬁc error as one
of the four THEA types. However, all attempts were made
to ensure that the error classiﬁcation criteria used for this
work remained faithful to Pocock et al.’s descriptions of the
four types. Since goal errors are the focus of this paper, the
criteria used to classify errors as goal errors are described
below. Similar criteria were used for classifying the remain-
ing errors as action, plan, or perception errors.
The data used to classify errors into types included ver-
bal protocol, screen video, and mouse and keyboard logs.
An error was classiﬁed as a goal error if it was either:
• An error of commission that was due to the user estab-
• An error of omission that was due to the user failing to
lishing a wrong subgoal; or
establish a necessary subgoal.
Establishment of subgoals was determined mainly from
the intentions users stated in their think-aloud protocols. An
example of a common goal error from the Wesley task was
a user failing to explicitly deny Wesley WRITE permission.
In the Jack task, both failing to explicitly deny Jack WRITE
permission (omitting subgoal 2 in Figure 3), and failing to
explicitly deny Jack ADMINISTRATE permission (omitting
subgoal 3 in Figure 3), were common goal errors.
There are numerous other frameworks that could be used
to classify human error. THEA was chosen because it was
speciﬁcally designed for evaluating user interfaces, and be-
cause of its grounding in the familiar work of Norman [13].
8 Results
The Salmon and XPFP interfaces were evaluated with
respect to speed, accuracy, and number of goal errors com-
mitted. Results for each of these are given in this section.
8.1 Speed
Salmon and XPFP were roughly comparable in speed,
as measured by average time to task completion. Figure
4 shows the average time to task completion for all XPFP
and Salmon users, and successful XPFP and Salmon users.
Since many users who failed using XPFP failed by omit-
ting essential task steps, they tended to reduce the aver-
age time to task completion, so comparing only success-
ful users across the interfaces gives a more meaningful
comparison. Although Salmon moderately outperformed
XPFP in speed amongst successful users in both the Wesley
(XPFP: M =208 seconds, sd=116; Salmon: M =183 sec-
onds, sd=138) and Jack (XPFP: M =208 seconds, sd=42;
Salmon: M =173 seconds, sd=109) tasks, the difference
between the two interfaces was not statistically signiﬁ-
t=0.3942, df =14.39,
cant (one-sided t-test for Wesley:
)
s
d
n
o
c
e
s
(
e
m
T
i
250
200
150
100
50
0
All XPFP users
All Salmon users
Successful XPFP users only
Successful Salmon users only
Wesley task
Jack task
Average Time to Task Completion
Task
Figure 4: Average time to task completion for the Wesley
and Jack tasks. Amongst successful users, Salmon moder-
ately outperformed XPFP in time to task completion, but
the differences were not statistically signiﬁcant.
p=0.3496; for Jack: t=0.8973, df =9.367, p=0.1961). Still,
the results are of interest because they show that Salmon’s
gains in accuracy (next section) over XPFP are not simply
due to a speed-accuracy tradeoff.
8.2 Accuracy
Table 1 shows the percentage of participants who suc-
cessfully completed the Wesley and Jack tasks on the XPFP
and Salmon interfaces. For the Wesley task, 7 of 12 XPFP
users (58%) and 10 of 12 Salmon users (83%) successfully
completed the task. For the Jack task, 3 of 12 XPFP users
(25%) and 12 of 12 Salmon users (100%) successfully com-
pleted the task. These numbers represent a 43% improve-
ment in accuracy for Salmon over XPFP on the Wesley task
and a 300% improvement on the Jack task. A one-sided z-
test for equality of proportions showed Salmon’s superiority
over XPFP in successful task completions to be weakly sig-
niﬁcant for the Wesley task (z=1.347, p=0.089) and strongly
signiﬁcant for the Jack task (z=3.795, p < 0.0001).