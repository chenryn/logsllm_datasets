such as an API server.
338
CHAPTER 10
Microservice APIs in Kubernetes
apps running on the cluster. A pod is itself a collection of Linux containers and
each container runs a single process such as an HTTP server.
A Linux container is the name given to a collection of technologies within the Linux
operating system that allow a process (or collection of processes) to be isolated from
other processes so that it sees its own view of the file system, network, users, and other
shared resources. This simplifies packaging and deployment, because different pro-
cesses can use different versions of the same components, which might otherwise
cause conflicts. You can even run entirely different distributions of Linux within con-
tainers simultaneously on the same operating system kernel. Containers also provide
security benefits, because processes can be locked down within a container such that it
is much harder for an attacker that compromises one process to break out of the con-
tainer and affect other processes running in different containers or the host operating
system. In this way, containers provide some of the benefits of VMs, but with lower
overhead. Several tools for packaging Linux containers have been developed, the
most famous of which is Docker (https://www.docker.com), which many Kubernetes
deployments build on top of.
LEARN ABOUT IT
Securing Linux containers is a complex topic, and we’ll
cover only the basics of in this book. The NCC Group have published a freely
available 123-page guide to hardening containers at http://mng.bz/wpQQ.
In most cases, a pod should contain only a single main container and that container
should run only a single process. If the process (or node) dies, Kubernetes will restart
the pod automatically, possibly on a different node. There are two general exceptions
to the one-container-per-pod rule:
 An init container runs before any other containers in the pod and can be used to
perform initialization tasks, such as waiting for other services to become avail-
able. The main container in a pod will not be started until all init containers
have completed.
 A sidecar container runs alongside the main container and provides additional
services. For example, a sidecar container might implement a reverse proxy for
an API server running in the main container, or it might periodically update
data files on a filesystem shared with the main container.
For the most part, you don’t need to worry about these different kinds of containers in
this chapter and can stick to the one-container-per-pod rule. You’ll see an example of
a sidecar container when you learn about the Linkerd service mesh in section 10.3.2.
 A Kubernetes cluster can be highly dynamic with pods being created and destroyed
or moved from one node to another to achieve performance and availability goals. This
makes it challenging for a container running in one pod to call an API running in
another pod, because the IP address may change depending on what node (or nodes) it
happens to be running on. To solve this problem, Kubernetes has the concept of a ser-
vice, which provides a way for pods to find other pods within the cluster. Each service
339
Deploying Natter on Kubernetes
running within Kubernetes is given a unique virtual IP address that is unique to that ser-
vice, and Kubernetes keeps track of which pods implement that service. In a microser-
vice architecture, you would register each microservice as a separate Kubernetes service.
A process running in a container can call another microservice’s API by making a net-
work request to the virtual IP address corresponding to that service. Kubernetes will
intercept the request and redirect it to a pod that implements the service. 
DEFINITION
A Kubernetes service provides a fixed virtual IP address that can
be used to send API requests to microservices within the cluster. Kubernetes
will route the request to a pod that implements the service.
As pods and nodes are created and deleted, Kubernetes updates the service metadata
to ensure that requests are always sent to an available pod for that service. A DNS ser-
vice is also typically running within a Kubernetes cluster to convert symbolic names for
services, such as payments.myapp.svc.example.com, into its virtual IP address, such as
192.168.0.12. This allows your microservices to make HTTP requests to hard-coded
URIs and rely on Kubernetes to route the request to an appropriate pod. By default,
services are accessible internally only within the Kubernetes network, but you can also
publish a service to a public IP address either directly or using a reverse proxy or load
balancer. You’ll learn how to deploy a reverse proxy in section 10.4.
10.2
Deploying Natter on Kubernetes
In this section, you’ll learn how to deploy a real API into Kubernetes and how to con-
figure pods and services to allow microservices to talk to each other. You’ll also add a
new link-preview microservice as an example of securing microservice APIs that are
not directly accessible to external users. After describing the new microservice, you’ll
use the following steps to deploy the Natter API to Kubernetes:
1
Building the H2 database as a Docker container.
2
Deploying the database to Kubernetes.
3
Building the Natter API as a Docker container and deploying it.
Pop quiz
1
A Kubernetes pod contains which one of the following components?
a
Node
b
Service
c
Container
d
Service mesh
e
Namespace
2
True or False: A sidecar container runs to completion before the main container
starts.
The answers are at the end of the chapter.
340
CHAPTER 10
Microservice APIs in Kubernetes
4
Building the new link-preview microservice.
5
Deploying the new microservice and exposing it as a Kubernetes service.
6
Adjusting the Natter API to call the new microservice API.
You’ll then learn how to avoid common security vulnerabilities that the link-preview
microservice introduces and harden the network against common attacks. But first
let’s motivate the new link-preview microservice.
 You’ve noticed that many Natter users are using the app to share links with each
other. To improve the user experience, you’ve decided to implement a feature to gener-
ate previews for these links. You’ve designed a new microservice that will extract links
from messages and fetch them from the Natter servers to generate a small preview based
on the metadata in the HTML returned from the link, making use of any Open Graph
tags in the page (https://ogp.me). For now, this service will just look for a title, descrip-
tion, and optional image in the page metadata, but in future you plan to expand the ser-
vice to handle fetching images and videos. You’ve decided to deploy the new link-
preview API as a separate microservice, so that an independent team can develop it. 
 Figure 10.3 shows the new deployment, with the existing Natter API and database
joined by the new link-preview microservice. Each of the three components is imple-
mented by a separate group of pods, which are then exposed internally as three
Kubernetes services:
 The H2 database runs in one pod and is exposed as the natter-database-service.
 The link-preview microservice runs in another pod and provides the natter-link-
preview-service.
 The main Natter API runs in yet another pod and is exposed as the natter-
api-service.
Natter
database
Link-preview
service
Natter API
The link-preview service generates
previews by fetching any URLs found
within Natter messages.
All other functions are handled by the
original Natter API and database.
Services are deployed
as separate pods.
apple.com
manning.com
google.com
Figure 10.3
The link-preview API is 
developed and deployed as a new 
microservice, separate from the main 
Natter API and running in different pods.
341
Deploying Natter on Kubernetes
You’ll use a single pod for each service in this chapter, for simplicity, but Kubernetes
allows you to run multiple copies of a pod on multiple nodes for performance and
reliability: if a pod (or node) crashes, it can then redirect requests to another pod
implementing the same service.
 Separating the link-preview service from the main Natter API also has security ben-
efits, because fetching and parsing arbitrary content from the internet is potentially
risky. If this was done within the main Natter API process, then any mishandling of
those requests could compromise user data or messages. Later in the chapter you’ll
see examples of attacks that can occur against this link-preview API and how to lock
down the environment to prevent them causing any damage. Separating potentially
risky operations into their own environments is known as privilege separation.
DEFINITION
Privilege separation is a design technique based on extracting poten-
tially risky operations into a separate process or environment that is isolated
from the main process. The extracted process can be run with fewer privi-
leges, reducing the damage if it is ever compromised.
Before you develop the new link-preview service, you’ll get the main Natter API run-
ning on Kubernetes with the H2 database running as a separate service. 
10.2.1 Building H2 database as a Docker container
Although the H2 database you’ve used for the Natter API in previous chapters is
intended primarily for embedded use, it does come with a simple server that can be
used for remote access. The first step of running the Natter API on Kubernetes is to
build a Linux container for running the database. There are several varieties of Linux
container; in this chapter, you’ll build a Docker container (as that is the default used
by the Minikube environment) to run Kubernetes on a local developer machine. See
appendix B for detailed instructions on how to install and configure Docker and Mini-
kube. Docker container images are built using a Dockerfile, which is a script that describes
how to build and run the software you need.
DEFINITION
A container image is a snapshot of a Linux container that can be
used to create many identical container instances. Docker images are built in
layers from a base image that specifies the Linux distribution such as Ubuntu
or Debian. Different containers can share the base image and apply differ-
ent layers on top, reducing the need to download and store large images
multiple times.
Because there is no official H2 database Docker file, you can create your own, as
shown in listing 10.1. Navigate to the root folder of the Natter project and create a
new folder named docker and then create a folder inside there named h2. Create a new
file named Dockerfile in the new docker/h2 folder you just created with the contents
of the listing. A Dockerfile consists of the following components:
 A base image, which is typically a Linux distribution such as Debian or Ubuntu.
The base image is specified using the FROM statement.
342
CHAPTER 10
Microservice APIs in Kubernetes
 A series of commands telling Docker how to customize that base image for your
app. This includes installing software, creating user accounts and permissions,
or setting up environment variables. The commands are executed within a con-
tainer running the base image.
DEFINITION
A base image is a Docker container image that you use as a starting
point for creating your own images. A Dockerfile modifies a base image to install
additional dependencies and configure permissions.
The Dockerfile in the listing downloads the latest release of H2, verifies its SHA-256
hash to ensure the file hasn’t changed, and unpacks it. The Dockerfile uses curl to
download the H2 release and sha256sum to verify the hash, so you need to use a base
image that includes these commands. Docker runs these commands in a container
running the base image, so it will fail if these commands are not available, even if you
have curl and sha256sum installed on your development machine.
 To reduce the size of the final image and remove potentially vulnerable files, you
can then copy the server binaries into a different, minimal base image. This is known
as a Docker multistage build and is useful to allow the build process to use a full-featured
image while the final image is based on something more stripped-down. This is done
in listing 10.1 by adding a second FROM command to the Dockerfile, which causes
Docker to switch to the new base image. You can then copy files from the build image
using the COPY --from command as shown in the listing.
DEFINITION
A Docker multistage build allows you to use a full-featured base
image to build and configure your software but then switch to a stripped-
down base image to reduce the size of the final image. 
In this case, you can use Google’s distroless base image, which contains just the Java 11
runtime and its dependencies and nothing else (not even a shell). Once you’ve cop-
ied the server files into the base image, you can then expose port 9092 so that the
server can be accessed from outside the container and configure it to use a non-root
user and group to run the server. Finally, define the command to run to start the
server using the ENTRYPOINT command.
TIP
Using a minimal base image such as the Alpine distribution or Google’s
distroless images reduces the attack surface of potentially vulnerable software
and limits further attacks that can be carried out if the container is ever com-
promised. In this case, an attacker would be quite happy to find curl on a
compromised container, but this is missing from the distroless image as is
almost anything else they could use to further an attack. Using a minimal
image also reduces the frequency with which you’ll need to apply security
updates to patch known vulnerabilities in the distribution because the vulner-
able components are not present.
343
Deploying Natter on Kubernetes
FROM curlimages/curl:7.66.0 AS build-env
ENV RELEASE h2-2018-03-18.zip                                          
ENV SHA256 \
    a45e7824b4f54f5d9d65fb89f22e1e75ecadb15ea4dcf8c5d432b80af59ea759   
WORKDIR /tmp
RUN echo "$SHA256  $RELEASE" > $RELEASE.sha256 && \
    curl -sSL https://www.h2database.com/$RELEASE -o $RELEASE && \   
    sha256sum -b -c $RELEASE.sha256 && \                             
    unzip $RELEASE && rm -f $RELEASE        
FROM gcr.io/distroless/java:11              
WORKDIR /opt                                
COPY --from=build-env /tmp/h2/bin /opt/h2   
USER 1000:1000     
EXPOSE 9092       
ENTRYPOINT ["java", "-Djava.security.egd=file:/dev/urandom", \   
        "-cp", "/opt/h2/h2-1.4.197.jar", \                       
        "org.h2.tools.Server", "-tcp", "-tcpAllowOthers"]        
Listing 10.1
The H2 database Dockerfile
Linux users and UIDs
When you log in to a Linux operating system (OS) you typically use a string username
such as “guest” or “root.” Behind the scenes, Linux maps these usernames into 32-
bit integer UIDs (user IDs). The same happens with group names, which are mapped
to integer GIDs (group IDs). The mapping between usernames and UIDs is done by
the /etc/passwd file, which can differ inside a container from the host OS. The root
user always has a UID of 0. Normal users usually have UIDs starting at 500 or 1000.
All permissions to access files and other resources are determined by the operating
system in terms of UIDs and GIDs rather than user and group names, and a process
can run with a UID or GID that doesn’t correspond to any named user or group.
By default, UIDs and GIDs within a container are identical to those in the host. So UID
0 within the container is the same as UID 0 outside the container: the root user. If you
run a process inside a container with a UID that happens to correspond to an existing
user in the host OS, then the container process will inherit all the permissions of that
user on the host. For added security, your Docker images can create a new user and
group and let the kernel assign an unused UID and GID without any existing permis-
sions in the host OS. If an attacker manages to exploit a vulnerability to gain access to
the host OS or filesystem, they will have no (or very limited) permissions.
A Linux user namespace can be used to map UIDs within the container to a different
range of UIDs on the host. This allows a process running as UID 0 (root) within a
container to be mapped to a non-privileged UID such as 20000 in the host. As far as
the container is concerned, the process is running as root, but it would not have root
Define environment variables
for the release file and hash.
Download
the release
and verify
the SHA-
256 hash.
Unzip the download and 
delete the zip file.
Copy the binary 
files into a minimal 
container image.
Ensure the process runs as 
a non-root user and group.
Expose the
H2 default
TCP port.
Configure the 
container to run 
the H2 server.
344
CHAPTER 10
Microservice APIs in Kubernetes
When you build a Docker image, it gets cached by the Docker daemon that runs the
build process. To use the image elsewhere, such as within a Kubernetes cluster, you
must first push the image to a container repository such as Docker Hub (https://
hub.docker.com) or a private repository within your organization. To avoid having to
configure a repository and credentials in this chapter, you can instead build directly to
the Docker daemon used by Minikube by running the following commands in your
terminal shell. You should specify version 1.16.2 of Kubernetes to ensure compatibility
with the examples in this book. Some of the examples require Minikube to be run-
ning with at least 4GB of RAM, so use the --memory flag to specify that.
minikube start \
    --kubernetes-version=1.16.2 \     
    --memory=4096       
You should then run
eval $(minikube docker-env)
so that any subsequent Docker commands in the same console instance will use Mini-
kube’s Docker daemon. This ensures Kubernetes will be able to find the images with-
out needing to access an external repository. If you open a new terminal window,
make sure to run this command again to set the environment correctly.
LEARN ABOUT IT
Typically in a production deployment, you’d configure your
DevOps pipeline to automatically push Docker images to a repository after
they have been thoroughly tested and scanned for known vulnerabilities.
Setting up such a workflow is outside the scope of this book but is covered
in detail in Securing DevOps by Julien Vehent (Manning, 2018; http://mng
.bz/qN52).
You can now build the H2 Docker image by typing the following commands in the
same shell:
cd docker/h2
docker build -t apisecurityinaction/h2database .
This may take a long time to run the first time because it must download the base
images, which are quite large. Subsequent builds will be faster because the images are
(continued)
privileges if it ever broke out of the container to access the host. See https://docs
.docker.com/engine/security/userns-remap/ for how to enable a user namespace in
Docker. This is not yet possible in Kubernetes, but there are several alternative options
for reducing user privileges inside a pod that are discussed later in the chapter.
Enable the latest 
Kubernetes version.
Specify 4GB of RAM.
345
Deploying Natter on Kubernetes
cached locally. To test the image, you can run the following command and check that
you see the expected output:
$ docker run apisecurityinaction/h2database
TCP server running at tcp://172.17.0.5:9092 (others can connect)
If you want to stop the container press Ctrl-C.
TIP
If you want to try connecting to the database server, be aware that the IP
address displayed is for Minikube’s internal virtual networking and is usually
not directly accessible. Run the command minikube ip at the prompt to get
an IP address you can use to connect from the host OS.
10.2.2 Deploying the database to Kubernetes
To deploy the database to the Kubernetes cluster, you’ll need to create some configu-
ration files describing how it is to be deployed. But before you do that, an important
first step is to create a separate Kubernetes namespace to hold all pods and services
related to the Natter API. A namespace provides a level of isolation when unrelated
services need to run on the same cluster and makes it easier to apply other security
policies such as the networking policies that you’ll apply in section 10.3. Kubernetes
provides several ways to configure objects in the cluster, including namespaces, but it’s
a good idea to use declarative configuration files so that you can check these into Git
or another version-control system, making it easier to review and manage security con-
figuration over time. Listing 10.2 shows the configuration needed to create a new
namespace for the Natter API. Navigate to the root folder of the Natter API project
and create a new sub-folder named “kubernetes.” Then inside the folder, create a new
file named natter-namespace.yaml with the contents of listing 10.2. The file tells
Kubernetes to make sure that a namespace exists with the name natter-api and a
matching label.
WARNING
YAML (https://yaml.org) configuration files are sensitive to inden-
tation and other whitespace. Make sure you copy the file exactly as it is in the
listing. You may prefer to download the finished files from the GitHub repos-
itory accompanying the book (http://mng.bz/7Gly).
apiVersion: v1
kind: Namespace    
metadata:
  name: natter-api      
  labels:               
    name: natter-api    
NOTE
Kubernetes configuration files are versioned using the apiVersion
attribute. The exact version string depends on the type of resource and version
of the Kubernetes software you’re using. Check the Kubernetes documentation
Listing 10.2
Creating the namespace
Use the Namespace kind 
to create a namespace.
Specify a name 
and label for the 
namespace.