time it takes to audit these passwords since they are hashed, 
we  only  performed  this  test  with  John  the  Ripper’s  default 
rule  set  and  our  method  operating  in  terminal  probability 
order. The results can be seen in Fig. 4.4.4. 
Fig. 4.4.4.   Number of Passwords Cracked. Trained on the Finnish 
Training List. Tested on the Finnish Test List 
    While  the  results  were  not  as  dramatic  as  compared  to 
cracking  the  MySpace  list,  we  still  see  an  improvement 
ranging  from  17%  to  29%  over  John  the  Ripper  in  all  but 
one  of  the  test  cases. Considering that we had no previous 
400
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:16:33 UTC from IEEE Xplore.  Restrictions apply. 
knowledge of how the passwords in the test set compared in 
complexity to the passwords in the training set, this is still a 
fairly  significant  improvement.  What  this  means  is  that  we 
were able to train our password cracker on one user base and 
then  use  is  successfully  against  another  group  which  we 
knew nothing about except for their native language. 
    Looking back through the previous tests as shown in Fig. 
4.4.1 through Fig. 4.4.4, one thing we noticed was that our 
probabilistic method performed significantly worse when it 
used the English_Lower dictionary compared to the results it 
obtained  using  the  other  input  dictionaries.  For  example, 
let’s consider the test, Fig 4.4.1, where we trained our attack 
on  the  MySpace  training  set,  and  tested  it  against  the 
MySpace  test  set.  If  we  exclude  the  run  that  used  the 
English_Lower dictionary, the average improvement of our 
method  using  terminal  probability  order  compared  to  John 
the  Ripper  was  90%.  The  improvement  on  the  run  which 
used the English_Lower dictionary was 28%. The other tests 
show  similar  results.  We  are  still  investigating  why  our 
attacks do not perform as well with this dictionary. Despite 
its  name,  the  English_Lower  dictionary  seems  to  be 
comprised mostly of “made up” words, such as ‘zoblotnick’, 
and ‘bohrh’. Our current assumption is that the presence of a 
large  number  of  nonsense  words  throws  off  our  method  in 
two  different  ways.  First  our  program  wastes  time  trying 
these  nonsense  words.  Second,  when  operating  in  terminal 
probability order, a large number of essentially “junk” words 
can make what should be a highly probable structure have a 
lower  probability,  and  thus  not  be  tried  as  soon.    We  still 
need to investigate this more thoroughly. 
    The next test we ran was to evaluate how the size of the 
training list affected our probabilistic password cracker.  To 
investigate  this  we  used  training  lists  of  various  sizes 
selected from the original MySpace training list. The size of 
these  lists  is  denoted  by  the  number  after  them,  aka  the 
MySpace20K 
training 
passwords. For reference, the MySpaceFull list contains all 
33,561 training passwords from the MySpace training list. 
We were concerned about sampling bias as the lists became 
shorter, (such as containing only 100 or 500 passwords). To 
address  this,  for  all  training  sets  containing  less  than  one 
thousand passwords we trained and then ran each test twenty 
five  times  with  a  different  random  sample  of  passwords 
included in the training list each time. We then averaged the 
results of the 25 different runs. All the tests to measure the 
effect  of  the  training  list  size  used  Terminal  Probability 
Order  and  were  run  against  the  MySpace  Test  List.  The 
results can be seen in Fig. 4.4.5. For comparison, John the 
Ripper’s performance is the left-most value, and training sets 
increase in size from left to right for each input dictionary. 
    It  was  surprising  that  even  when  our  password  cracker 
was  trained  on  only  10,000  passwords,  our  Probabilistic 
list  contains 
thousand 
twenty 
Method  performed  only  slightly  worse  than  when  it  was 
trained on 33,561 passwords. What was more surprising was 
that our password cracker performed comparable to John the 
Ripper  even  when  it  was  trained  on  only  100  input 
passwords. We expect that given a longer run (aka allowing 
our  password  cracker  to  generate  more guesses), the effect 
of having a larger training set will become more pronounced 
as it  will generally provide the password cracker more base 
structures as well as digit and symbol strings to draw upon. 
Also,  we  expect  that  the  larger  training  set  would  better 
reflect  more  accurate  probabilities  of  the  underlying  base 
structures and replacement values. 
Fig. 4.4.5.   Number of Passwords Cracked. Trained on different 
sized MySpace Training Lists. Tested on the MySpace Test List 
using Terminal Order Probability 
    In  all  the  previous  tests  we  limited  our  probabilistic 
method  to  the  number  of  guesses  generated  by  the  default 
rule set of John the Ripper.  One last test we wanted to run 
was to see how our probabilistic method performed if we let 
it continue to run over an extended time. The following Fig. 
4.4.6  shows  the  number  of  passwords  cracked  over  time 
using  our  probabilistic  method  operating 
terminal 
probability order. Please note, while John the Ripper exited 
after  making  37,781,538  guesses,  we  continued  to  let  our 
program  operate  until  it  made  300,000,000  guesses.  Also 
note  that  our  Probabilistic  Password  Cracker  was  still 
creating  guesses  when  we  stopped 
it.  We  choose 
300,000,000  just  as  a  benchmark  number.  The  results  are 
shown in Fig. 4.4.6.     
    These results match with the previous test on this data set, 
as  seen  in  Fig.  4.4.1,  in  that  given  the  same  number  of 
guesses  our  password  cracker  operating 
terminal 
probability order cracks 68% more passwords than John the 
Ripper.    As  you  can  see  in  Fig.  4.4.6  though,  the  rate  at 
which  our  method  cracks  passwords  does  slow  down  as 
more  guesses  are  made.  This  is  to  be  expected  as  it  tries 
lower and lower probability password guesses.  
in 
in 
401
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:16:33 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 4.4.6. Number of Passwords Cracked Over Time. Trained on 
the MySpace Training List. Tested on the MySpace Test List 
    We  decided  to  run  the  test  in  Fig.  4.4.6  again,  but  this 
time  have  John  the  Ripper  switch  to  brute-force  after 
exhausting  all  of  its  word-mangling  rules.  We  feel  this 
would  best  simulate  an  actual  password  cracking  session, 
(aka  exhaust  a  dictionary  attack  and  then  resort  to  brute-
force)  using  John  the  Ripper.  Please  note  that  John  the 
Ripper uses Markov models in its brute-force attack to first 
try  passwords  phonetically  similar  to  human  generated 
words.  It creates the conditional probability based not only 
on  letters,  but  also  on  symbols  and  numbers  as  well.  As  a 
third  experiment,  we  also  ran  a  pure  brute-force  attack 
without  using  John  the  Ripper’s  rules  first.    The  results  of 
these tests are shown in Fig. 4.4.7.   
probabilistic  attack.    This  would  allow  us  to  quickly  crack 
any short passwords our method may have missed.  After a 
period  of  time  though,  brute-force  becomes  completely 
infeasible due to the length of the passwords and the size of 
the  keyspace.  We  expect  that  even  the  low  probability 
guesses  generated  by  our  cracker  are  better 
than  a 
completely  random  guess  which  would  result  from  a  pure 
brute-force  approach  against  a  large  keyspace.  Therefore, 
the  more  passwords  you  can  crack  before  having  to  solely 
rely  upon  brute-force  attacks,  the  more  advantageous  it  is. 
Because  of  this,  the  large  number  of  rules,  (possibly 
billions), that our method automatically generates is another 
major advantage of our approach. 
4.5  SPACE COMPLEXITY RESULTS 
    In this section we focus on the space complexity related to 
generating our password guesses. We first discuss the space 
complexity of storing the grammar as this is what would be 
distributed  to  the  end  user  once  the  password  cracker  has 
been trained. 
    Since the grammar is generated from the training set, the 
size of the grammar is dependent on the size of this set.  To 
distribute  this  grammar  we  need  to  save  the  set  of  S-
productions  (grammar  rules  with  the  start  symbol  S  on  the 
left hand side) that give rise to the base structures and their 
associated  probabilities.  See  Figure  3.2.1.  Consider  a 
training  set  of  j  passwords  each  of  maximal  length  k.    At 
worst each password could result in a unique base structure 
resulting in O (j) S-productions.  Similarly the number of Di-
productions  and  Si-productions  depend  on  the  number  of 
unique  digit  strings  and  special  strings, respectively, in the 
training set.  This could result in a maximum of O(jk) unique 
productions. Finally, the number of L-productions (rewriting 
an alpha string using a dictionary word) depend on the input 
dictionary chosen.  For a dictionary of size m, the maximum 
number  of  L-productions  is  simply  O(m).    In  practice,  we 
expect the grammars to be highly portable with many fewer 
production rules than the worst case. See Table 4.5.1. 
Table 4.5.1 
Size of the Stored Grammar 
# of Base 
Structures 
Training Set 
& Size 
MySpace10k  820 
MySpace20k  1216 
MySpace 
1589 
(33,561) 
Finnish 
(15,699) 
736 
Number of 
 Si-productions 
79 
108 
144 
Number of  
Di-productions  
2405 
3377 
4410 
49 
1223 
Figure 4.4.7. Number of Passwords Cracked Over Time.  Trained 
on the MySpace Training List.  Tested on the MySpace Test List  
    One  thing  we  learned  from  this  data  is  that  it  may  be 
effective to pause our probabilistic method after around 100 
million  guesses  and  switch  to  a  brute-force  attack  using  a 
small  keyspace  for  a  limited  time  before  resuming  our 
402
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:16:33 UTC from IEEE Xplore.  Restrictions apply. 
    We  next  consider  the  space  complexity  of  an  actual 
password  cracking  session.  Using  the  grammar,  for  each 
base structure, we generate pre-terminal structures, using the 
“next” function that are pushed and popped from the priority 
queue as described in Section 3.3.  The space complexity of 
this algorithm is the maximum size of the priority queue.  It 
should be clear that this is worst case O(n) where there are n 
possible pre-terminals generated.  We do not expect that the 
worst case is actually sublinear.  In practice, the maximum 
size  of  the  priority  queue  has  not  been  an  issue  in  our 
experiments to date. Table 4.5.2 shows the total number of 
pre-terminals  generated  to  create  a  specified  number  of 
password  guesses.  The  space  requirement  is  shown  by  the 
maximume  size  of  the  priority  queue.    Figure  4.5.3  shows 
the size of the priority queue as a function of the passwords 
generated when trained on the MySpace training sets. 
Table 4.5.2  
Space Cost using Dic-0294 
Training Set  Total Pre-
Terminals 
Generated 
28,457 
14,661 
19,550 
174,165 
109,453 
1,567,911 
470,949 
193,963 
4,324,913 
MySpace10k 
MySpaceFull 
Finnish 
MySpace10k 
MySpaceFull 
Finnish 
MySpace10k 
MySpaceFull 
Finnish 
Maximum 
Size of 
Queue 
1,274 
1,688 
4,753 
4,642 
3,691 
138,187 
9,946 
6,682 
299,933 
Password 
Guesses 
(millons) 
50 
50 
50 
500 
500 
500 
1,000 
1,000 