Level
V M, Host∗
V M, Host∗
V M, Host∗
Host∗
V M
V M
V M
V M
V M
V M
V M
Measurement
Level
response time
number of transactions/second
V M
V M
A. Monitoring Engine
C. Problem Diagnosis Engine
CloudPD’s Monitoring Engine collects measurements from
individual VMs as well as physical servers. The measurement
data (reported in Table I) include basic resource metrics (CPU,
memory),
impacted operating context parameters (context
switches, cache misses), host operating context parameters,
and application performance metrics (latency, throughput). We
use Linux sar utility to collect the VM level system metrics
and VMware vmkperf [24] performance monitoring tool for
obtaining a server’s cache misses. To collect CPU and memory
usage of the server, we use VMware powercli cmdlets [25].
The monitored data across all VMs and servers are collected
and stored at a central VM for further processing by subsequent
stages. The Data Preprocessor module generates data-points
from the raw time-series data. Data-points pertaining to every
interval are analyzed by CloudPD for anomalies. We use a
window-size of 10, sampling interval of 2 seconds, and interval
length of 15 minutes. The number of data points in the 15
minutes interval are, thus, (15*60)/(10*2) = 45.
B. Performance Models for Event Generation
We implemented three modeling techniques as part of the
Event Generation Engine – Hidden Markov Models (HMM),
k-nearest-neighbor (kNN), and k-means clustering [26]. All
these three techniques attempt to qualitatively or quantitatively
measure the extent of deviation of the current interval test data
from models of normal behavior. The CloudPD architecture
allows plug and play of any modeling technique as part of the
Event Generation Engine. In the interest of space, we only
discuss the kNN technique used in our evaluation, and refer
the readers to [27] for details of the other two techniques.
We adopt an XML format for describing the fault signatures,
so as to allow the CloudPD system to learn to classify new
kinds of faults with expert assistance. We assume that each
fault has a characteristic signature, which can be expressed
in terms of the metrics monitored. We have observed this to
be true in our extensive evaluations. We adopt a software
wrapper based on Matlab xmlwrite utility to implement
the diagnosis engine. Essentially, this wrapper provides two
functionalities: (a) create an XML based signature of a new
fault; and (b) compare a newly created signature with existing
fault signatures (stored in a database).
Figure 6 provides an example of an expert-created signature
for a VM resizing fault. The signatures are described using
different contexts expressed as tags, namely, VM environment,
operating environment, hypervisor (includes special log mes-
sages from the hypervisor), and application environment. Only
the metrics that deviate from normal behavior are captured
in the signature, and are expressed as thresholds denoting
the minimum deviation in the correlation values required for
a signature match. For example, the CPU correlation value
computed between a pair of VMs hosting an application should
deviate from the correlation value under normal behavior by
at least as large as the deﬁned threshold (CPU-corr.-diff) in
order to match the signature for a wrong VM sizing (likewise,
other tags correlation differences also need to match). These
signatures are built automatically, without manual intervention.
D. Complexity Analysis
Let the number of VMs in a cluster executing the same
application be N , the number of metrics monitored be M ,
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:43:49 UTC from IEEE Xplore.  Restrictions apply. 
TABLE III: Remedial actions taken by CloudPD on detecting
various kinds of faults.
Fault
Wrong VM Resizing
Handler
CloudRM
VM to VM Contention
CloudRM
Invalid VM Migration
CloudRM
Application Error
System Admin
System Error
CloudRM,
System Admin
Workload Intensity Change
CloudRM
Workload Mix Change
CloudRM
Action
Trigger re-conﬁguration.
Create collocation exclusion
constraints between the VMs and
then trigger the reconﬁguration.
Update the migration cost for the
VM (to be used for the future
consolidations).
Open a service ticket.
Update server list. Trigger
reconﬁgurations for fail-over;
Raise problem ticket.
Same as Wrong VM Resizing.
Trigger re-proﬁling of the
application. Reconﬁguration, if
needed by CloudRM.
which is connected to an IBM DS4800 storage controller
via a Cisco MDS ﬁber channel switch. All servers run the
VMWare ESXi 4 Enterprise Plus hypervisor and are managed
by a VMWare vSphere 4.0 server. The cloud setup supports
enterprise virtualization features including live VM migration,
live VM resizing, VM cloning and VM snapshot.
total 28 VMs on our cloud setup, which run
We host
Ubuntu v10.04 64-bit operating system. Unless otherwise
stated, all VMs are conﬁgured with 1.2 GB RAM and 1.6
GHz CPU. Our VMs can be classiﬁed into three groups. The
ﬁrst group (Hadoop cluster) consists of 16 VMs that run
Hadoop MapReduce [28]. The second (Olio cluster) consists of
6 VMs (4 VMs for web server and 2 VMs for database) and
runs CloudStone [29], a multi-platform benchmark for Web
2.0. CloudStone consists of a load injection framework called
Faban, and a social online calendar web application called
Olio [29]. Faban is conﬁgured on a separate VM to drive
the workload. The third (RUBiS cluster) runs the RUBiS E-
commerce benchmark [30], and comprises of 6 VMs (2 VMs
each across web, application and database tier).
B. Evaluation Metrics
We utilize the following four statistical measures to eval-
uate the effectiveness of anomaly detection and diagnosis
by CloudPD. We deﬁne a successful anomaly detection as
diagnosing the anomaly correctly using pre-deﬁned fault sig-
natures, along with localizing the affected VMs and metrics.
Recall =
P recision =
Number of successful detections
Total number of anomalies
Number of successful detections
(1)
(2)
(3)
Accuracy =
Total number of alarms
2 × Recall × Precision
Recall + Precision
Number of false alarms
Total number of alarms
= 1 − Precision
(4)
 VM is sized to very low CPU & memory  
 0.13
 0.11
 0.09
 15.6
 9.2
 0.08
 14.5
Figure 6: Example signature of an invalid VM resizing fault.
and the number of fault types be F . Let T be the number of
data points in each interval being analyzed. We analyze the
complexity of each of the stages of CloudPD as below:
• Finding the nearest neighbor for each data point takes O(T 2)
time. The computation performed by the Event Generation
Engine using kN N for each VM and each metric takes
O(N M T 2) time. This can be parallelized across p ≤ N
threads, which would take O(N M T 2/p) time.
• For each event generated, the Problem Determination Engine
performs 2(M + N ) correlations (one each for the test
interval data and the normal interval data obtained from recent
history). Thus, net time complexity is O((M + N )T 2).
• Let M ′ be the maximum number of deviations in correlations
that are part of the unique fault signature. This is typically
a small number ( ctxt
cache miss  page fault
n
o
i
t
a
l
e
r
r