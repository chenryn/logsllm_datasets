updates modify the structure of the current path, such as VM
migrations in multi-tenant data centers, SPHINX discards the
cached waypoints, rebuilds the current path and traverses it to
check for consistency (such as waypoint dependencies, etc.),
and any administrator-speciﬁed security policies.
Incremental ﬂow graphs along with the ﬂow metadata
ensure that the validation process is quick, since at each update
SPHINX only has to reason about the metadata concerning a
speciﬁc network link for a single ﬂow. This design not only
makes constraint veriﬁcation extremely fast, but also makes
action attribution easier and precise.
We next describe SPHINX’s policy engine and its role in
the validation of network behavior in greater detail.
VI. SPHINX POLICY ENGINE
A. Constraint speciﬁcation
SPHINX validates all ﬂow graphs against a set of
constraints. These constraints are of
two types—(i) any
administrator-speciﬁed security policies, and (ii) those ac-
quired over time for a speciﬁc ﬂow. Administrator-speciﬁed
policies defend against known attacks or violations, while
constraints assimilated over time can detect even unanticipated
and harmful network updates.
SPHINX provides a light-weight policy framework that
enables administrators to specify validation checks on incre-
mental ﬂow graphs. These administrator-speciﬁed constraints
must be expressed in a policy language as speciﬁed in Table 2.
Most modern controllers allow applications and modules to
implement separate checks making policy enforcement buggy
and hard. In contrast, SPHINX provides a pluggable framework
to enforce complex security checks at one central location.
Note that SPHINX assumes logical correctness of the policies.
Validation of policies is out of scope of the current work, and
is left for future work.
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
Fig. 4: Example policy to check if all ﬂows from host H3 pass through
speciﬁed waypoints S 2 and S 3.
Each policy has four main components—subject, object,
operation and trigger. The subject
identiﬁes trafﬁc ﬂow(s)
between a source/destination pair in either the control or data
plane (where either or both can be wildcards) over which
constraints are expressed. An object is a keyword that speciﬁes
a trafﬁc property describing the nature of constraints, while the
operation speciﬁes a relation describing the approved values
that
the object can attain for the given trafﬁc ﬂow(s), as
speciﬁed by the subject. Lastly, the policy must also specify a
trigger instructing SPHINX when to schedule the check.
SPHINX feeds the policy to a veriﬁer, which ensures that
the constraints are checked at the speciﬁed trigger. For each
policy, the veriﬁer extracts the ﬂow and the associated ﬂow
properties, and invokes a built-in checker to evaluate the con-
straint. SPHINX provides several built-in checkers, including
those for enforcement of policies listed in Table 4. Figure 4
shows an example policy to check if all ﬂows originating at
a host H3 in the network pass through speciﬁed waypoints,
such as a ﬁrewall. The policy applies to all destinations in
the network, as indicated by ‘∗’ in the ‘DSTID’ ﬁeld of the
subject. The objects deﬁne the set of waypoints, while the
operation ‘IN’ directs the veriﬁer to check the waypoints for
membership within the objects speciﬁed by the policy. The
policy is checked ‘periodically’ as speciﬁed by the trigger.
Apart
from validating the administrator-speciﬁed con-
straints, SPHINX automatically generates ﬂow-speciﬁc con-
straints by observing updates to ﬂow-speciﬁc topological and
forwarding states, i.e., IP-MAC or switch-port bindings, for-
warding actions at speciﬁc waypoints, etc., over time. These
topological and forwarding states are the default constraints for
that ﬂow, and SPHINX checks for any atypical ﬂow patterns
by identifying changes to the ﬂow’s metadata. SPHINX raises
an alarm if any of these invariants are violated during the
duration of the ﬂow. For example, if SPHINX receives ﬂow-
level statistics from a switch not on the ﬂow’s current path, it
raises an alarm because an intermediate switch on the current
path could be siphoning off ﬂow trafﬁc.
SDN controllers utilize graph theoretic algorithms to ensure
that the computed path between a pair of endpoints observes
certain standard properties, such as reachability, the absence of
loops or blackholes, etc. Since SPHINX trusts the controller,
the policy language currently does not allow speciﬁcation
of constraints over the ﬂow graph structure. However it can
easily be extended to do so, thereby enabling administrators
to express policies to verify ﬂow graph properties, such as
loops, blackholes, reachability, etc.
B. Constraint veriﬁcation
Algorithm 1 brieﬂy describes the veriﬁcation process. For
each untrusted OpenFlow message (PACKET_IN and STATS_REPLY)
in the packet stream, SPHINX together determines three classes
6
Input: S : Stream of incoming OpenFlow packets.
Output: DataS tore : Data store for saving valid metadata for each ﬂow.
function VERIFIER(S)
Initialize:
for all ρ ∈ S do
O := Allow /*Processing of packet by default*/
DataS tore := ∅
MD := GET PACKET METADATA(ρ)
F := GET FLOW METADATA(MD)
FG := GET PATH METADATA(F)
/*Get policy and other constraints for packet*/
Φ := GET CONSTRAINTS(ρ, MD, F, FG)
/*Validate packet/path/flow metadata for ρ*/
O := O(cid:86) VALIDATE PACKET(MD, Φ)(cid:86) VALIDATE PATH(FG, Φ)
(cid:86) VALIDATE FLOW(F, Φ)
DataS tore := DataS tore(cid:83) SAVE METADATA(ρ, MD, F, FG)
/*Raise alert for administrator*/
if (/*Administrator allows alert*/) then
/*Save all metadata in data store*/
if (DENY == O) then
else
/*Break from loop and stop the packet flow*/
return DataS tore
Algorithm 1: Veriﬁcation of each incoming packet for each ﬂow.
Metadata
PACKET
PATH
FLOW
Veriﬁcation Purpose
Packet spooﬁng
Controller DoS
Flow graph consistency
Switch DoS
Flow statistics
Invariants
MAC-IP-Switch-Port
PACKET_IN rate, etc.
Routing rules. path
waypoints
Flow counters, Tx/Rx
bytes, switch/out-port
Table 3: Example of some invariants veriﬁed by SPHINX.
of metadata—packet, path and ﬂow—and veriﬁes them against
the set of both learnt and administrator-speciﬁed constraints.
Packet-level metadata pertains to all metadata that are speciﬁc
to just one speciﬁc PACKET_IN, such as information about
a host’s IP/MAC binding, or link connection between two
switches. Path-level metadata refers to all metadata that de-
scribe the network’s actual forwarding state behavior, such
as the switch and port from which the packet was received.
Note that both packet- and path-level metadata, describing the
logical and physical topology and the ﬂow paths, are obtained
exclusively from PACKET_IN messages. Flow-level metadata
quantify the actual data plane forwarding in the network,
and are extracted from the STATS_REPLY messages received
periodically.
The aforementioned metadata veriﬁcation is either deter-
ministic or probabilistic. Topological state veriﬁcation can
proceed even before the actual trafﬁc has begun, i.e., it veriﬁes
properties involved in setup of ﬂow paths and is deterministic.
Veriﬁcation of data plane forwarding state requires a ﬂow to
be setup, and probabilistically veriﬁes properties that quantify
the nature of the ﬂow. Table 3 lists the three metadata classes
and some of the corresponding invariants observed during
veriﬁcation. Table 4 lists the default policies that SPHINX
checks at each veriﬁcation trigger. Note that SPHINX does
not verify the trusted FLOW_MOD messages. However, the effects
of these FLOW_MOD messages may violate some administrator-
speciﬁed policy, e.g., all ﬂows must pass through a ﬁrewall.
Thus, SPHINX validates such policies on the speciﬁed trigger.
1) Topological
constraint veriﬁcation: Topologi-
state
cal constraints,
i.e., both network invariants as well as
administrator-speciﬁed, can be veriﬁed using the metadata
gleaned from the received PACKET_IN. Once the default invari-
ants have been veriﬁed, the metadata are compared against
all applicable policies, and any deviant behavior is ﬂagged.
7
Trigger
PACKET_IN
FLOW_MOD
Periodic
Policy
IP-MAC binding is permissible.
Network topology (physical/logical) change is permissible.
–
Throughput for a ﬂow/switch port is below a threshold.
Switch must not drop or siphon off packets in the ﬂow.
Table 4: Default policies checked by SPHINX on every trigger.
Examples of such packet-level metadata veriﬁcation include
the detection of packet spooﬁng for both logical and physical
topological tampering. All such veriﬁcation is deterministic
and fast due to incremental ﬂow graphs, which allows veriﬁ-
cation to proceed over the last edge or metadata that was added
to the graph. This also enables precise action attribution.
2) Forwarding state constraint veriﬁcation: Veriﬁcation of
forwarding constraints in the data plane requires the valida-
tion of both packet- and ﬂow-level metadata, which may be
either deterministic or probabilistic depending on the nature
of constraints involved. For example, if malicious switch(es)
tamper with existing ﬂows, then such inconsistencies may not
be reﬂected in the analysis of ﬂow graph structure alone.
Such cases may only be determined by using ﬂow consistency
checks. Thus, SPHINX performs additional periodic checks
on the ﬂow graphs and the associated metadata to determine
conformance with ﬂow dependencies and constraints,
like
detecting if a ﬂow’s throughput is within a threshold, packet
drops or siphoning due to malicious switch(es), etc.
OpenFlow’s asynchronous nature may cause messages to
arrive in an out-of-order manner at
the controller. While
packet-level metadata (e.g., rate of PACKET_IN messages) re-
mains unaffected, a key challenge for SPHINX is to accurately
determine ﬂow-level statistics in the presence of unsynchro-
nized messages from multiple different switches in the ﬂow
path, which may report ﬂow-level statistics at different time
granularity. SPHINX overcomes the above challenge using a
custom algorithm that relies on an honest majority of switches
along a ﬂow path to approximate the byte and packet statistics
at the ﬂow-level. Since undesirable behavior by a malicious (or
misconﬁgured) switch may manifest itself in trafﬁc ﬂowing
across the switches, SPHINX generates a metric called Sim-
ilarity Index (Σ) at each switch to represent the nature of
the trafﬁc ﬂow. The Σ of a switch at timestep t is calculated
as: Σt = Σt−1 + (∆n − ∆n−p)/p, where ∆n = sn − sn−1, and
sn represents the latest (nth) byte-level statistics available at
timestep t. Σ is thus calculated as a moving average of the
difference in byte-level statistics reported for each ﬂow per
switch in the current ﬂow path. SPHINX chooses the last p = 4
statistics reported by STATS_REPLY messages, which span a few
seconds and are controller dependent. This interval is sufﬁcient
to even out trafﬁc bursts, congestion at waypoints and account
for out-of-order messages, thereby avoiding false alarms. Σ
also enables SPHINX to check for the presence of malicious
switches that may add/drop packets at coarse timescales (at
most equal to the frequency of STATS_REPLY messages).
For a particular ﬂow, Σ must be similar for honest switches
on its path till the ﬂow encounters a malicious (or misconﬁg-
ured) switch, which may inject or siphon off trafﬁc. However,
it is still possible that the malicious switch fakes the statistics
with Σ similar to honest switches. Even in this case,
the
switches downstream would report higher (or lower) Σ if the
switch is injecting or siphoning off trafﬁc. Since offending
Input: F : Flow, τ : threshold
Output: O : {S} Set of contentious switches along the ﬂow F
function FLOW CONSISTENCY VALIDATOR(F, τ)
Initialize:
FG := GET FLOWGRAPH(F) : The complete ﬂow graph for ﬂow F
CurrP := GET CURRENTPATH(FG) : The active current ﬂow path for FG
Σavg := 0 : Initialize running average of Similarity Index for FG
O := ∅
/*Validate byte consistency for switches on CurrP*/
for all S ∈ CurrP do
M := GET METADATA(S )
Σ := SIMILARITY INDEX(M)
/*Check if Σ is an outlier*/
if FALSE == CHECK VIOLATION(Σavg, Σ, τ) then
else O ∪ = {S} /*Add S to output set*/
Σavg := UPDATE RUNNING AVERAGE INDEX(Σavg, Σ)
/*Validate inactivity of switches not on CurrP*/
for all S ∈ FG ∧ S (cid:60) CurrP do
M := GET METADATA(S )
T := GET THROUGHPUT(M)
/*Check if switch S is not inactive*/
if T! = 0 then O ∪ = {S} /*Add S to output set*/
return O
Algorithm 2: Checking byte consistency across a ﬂow.
switches cannot fake their identity (as switches connect with
the controller over separate TCP connections), they would
thus be pinpointed. Note that Σ will not change if malicious
switch(es) compromise the integrity of the ﬂow packets, or
inject and remove an equal amount of packets from the ﬂow
trafﬁc. To prevent such attacks on integrity of ﬂow trafﬁc,
SDNs can leverage cryptographic mechanisms.
Algorithm 2 describes the steps to perform byte consistency
checks for a given ﬂow graph. The algorithm takes as input a
ﬂow graph and computes the current path for the ﬂow. It then
iterates over all switches in the current path to access the byte
and packet statistics, and calculates the Σ for each switch. The
algorithm reports a violation if it determines that a switch in
the ﬂow path reports Σ much different from the moving average
Σ for the ﬂow. The algorithm also checks for inactivity of all
switches not in the current path. This veriﬁes that no switch
off the current ﬂow path is injecting or siphoning off trafﬁc.
Further, the algorithm takes as input a threshold (τ), which
is a margin of similarity used to perform outlier detection. A
τ = x means that Σ at each switch along the ﬂow path must lie