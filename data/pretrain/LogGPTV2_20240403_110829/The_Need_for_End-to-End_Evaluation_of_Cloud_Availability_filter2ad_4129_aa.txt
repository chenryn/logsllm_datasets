title:The Need for End-to-End Evaluation of Cloud Availability
author:Zi Hu and
Liang Zhu and
Calvin Ardi and
Ethan Katz-Bassett and
Harsha V. Madhyastha and
John S. Heidemann and
Minlan Yu
The Need for End-to-End Evaluation
of Cloud Availability
Zi Hu1,2, Liang Zhu1,2, Calvin Ardi1,2, Ethan Katz-Bassett1,
Harsha V. Madhyastha3, John Heidemann1,2, and Minlan Yu1
1 USC/CS Dept.
2 USC/ISI
3 U. of California, Riverside
Abstract. People’s computing lives are moving into the cloud, making
understanding cloud availability increasingly critical. Prior studies of In-
ternet outages have used ICMP-based pings and traceroutes. While these
studies can detect network availability, we show that they can be inacc-
urate at estimating cloud availability. Without care, ICMP probes can
underestimate availability because ICMP is not as robust as application-
level measurements such as HTTP. They can overestimate availability
if they measure reachability of the cloud’s edge, missing failures in the
cloud’s back-end. We develop methodologies sensitive to ﬁve “nines” of
reliability, and then we compare ICMP and end-to-end measurements
for both cloud VM and storage services. We show case studies where one
fails and the other succeeds, and our results highlight the importance
of application-level retries to reach high precision. When possible, we
recommend end-to-end measurement with application-level protocols to
evaluate the availability of cloud services.
1
Introduction
Cloud computing is a distributed computing paradigm that allows users to easily
access and conﬁgure remote computing resources in a scalable manner. As the
cloud grows in importance, it will host more applications and services from the
small (such as new and developing web applications) to the large (Amazon,
Netﬂix, etc.).
As we depend on them more and more, services that run in the cloud need to
be highly available. Despite this need and news reports highlighting major cloud
outages [17], there have been few systematic, third party studies of how reliable
the cloud actually is. While recent systems might use one [25] or multiple [2]
cloud providers to improve reliability, there is a poor understanding of reliable
methods to externally and empirically measure cloud reliability.
Many general network availability and measurement studies use ICMP-based
methodologies [14,15,12,23,10], sometimes focusing on routing problems [15] or
outages in edge networks [12,23]. Studies likely use ICMP because more routers
respond to it than to other types of probes [18,19] and because ICMP probes are
less likely to elicit complaints [24,18]. However, distrust of ICMP in the network
M. Faloutsos and A. Kuzmanovic (Eds.): PAM 2014, LNCS 8362, pp. 119–130, 2014.
c(cid:2) Springer International Publishing Switzerland 2014
120
Z. Hu et al.
Table 1. Datasets used in this paper
duration target
start
(days)
service (provider)
2013-03-11 +33 VM (Amazon)
2013-03-11 +33
2013-06-18 +17 VM (Amazon)
2013-06-18 +75
storage (Amazon, Google, Microsoft)
storage (Amazon, Google, Microsoft)
sources method
(VPs)
23
23
54
54
tries/interval
3× / 10 min.
3× / 10 min.
9× / 11 min.
9× / 11 min.
operator community [1] calls into question the accuracy and reliability of using
only ICMP to measuring availability. While eﬀective for network measurements,
ICMP is not perfect, and care must be taken to consider ﬁltering, rate limiting,
and depreferential service.
The contribution of this paper is to develop and compare mechanisms to mea-
sure cloud reliability. We show that ICMP-based measurements are inaccurate
at measuring cloud availability and that end-to-end measurements are necessary
to establish cloud availability.
We ﬁrst compare ICMP and HTTP to measure cloud reliability at the network
and application levels, and then apply them to several cloud VM and storage
services. We evaluate the eﬀect of retries and show that ICMP has a higher loss
rate than random packet loss alone predicts (Section 3). While ICMP and HTTP
nearly always agree, they sometimes disagree. ICMP occasionally experiences
a period of loss from some vantage points and thus will overestimate cloud
outages—a weakness of the methodology. Less frequently, we see that HTTP
probing shows outages that last for extended periods from some vantage points;
ICMP would underestimate these outages because of its failure to reach the
provided service. We conclude that, although application-level methods such as
HTTP probing incur the cost of provisioning and accessing cloud resources, they
are necessary to accurately assess cloud reliability.
2 Methodology
We use two methods to study availability: ICMP probes at the network level,
and end-to-end probes with HTTP at the application level. We target both cloud
VMs and storage services of three providers. Our work results in four datasets
(Table 1), all available on request.
2.1 Outage Causes
We measure outages in cloud services by taking observations from many van-
tage points (VPs). Section 2.4 details our VP selection and infrastructure. To
understand what these measurements tell us, we must consider potential sources
of failure that can occur from the VP to the cloud. These problems may occur
near the VP, in the network, near the cloud provider, at the cloud front-end, or
inside the cloud infrastructure.
The Need for End-to-End Evaluation of Cloud Availability
121
We see several possible failures: (1) DNS lookup failures; (2) routing problems,
either near the VP, in the network, or at the provider; (3) random packet loss
in the network; (4) rate limiting, either near the VP, in the network, or at the
provider; and (5) service outages inside the cloud infrastructure.
While all of these problems can interfere with use of the cloud, some, such
as packet loss, are commonplace and the end user is responsible to recover from
them. Others aﬀect some measurements diﬀerently. Our goal is to understand
how the choice of measurement methodologies emphasizes diﬀerent failures.
2.2 Outage Detection at the Network and Application Level
We measure cloud status every 10 or 11 minutes (see Table 1), sending ICMP
and HTTP probes with retries. We record the results from many vantage points.
We consider the overall response to be positive if the initial probe or any of the
retries succeeds.
For network-level tests, we send an ICMP echo request, considering only pos-
itive replies as successful, and lack of a reply or any error code as negative. For
end-to-end testing, we retrieve a short ﬁle over HTTP with curl. A positive
response is a HTTP status code of 200 OK; any other HTTP status code is a
negative response. We record curl error codes to distinguish some failure cases.
In both cases, if the initial request fails, we try two and eight additional times
for the datasets that begin on 2013-03-11 and 2013-06-18, respectively. We then
record the result as I or I for ICMP success or failure, and H or H for HTTP. In
the 2013-03-11 datasets, we do not do ICMP retries unless HTTP probes fail, in
which case we then perform ICMP retries in conjunction with HTTP retries.
To diagnose problems, we observe the probe at the service itself (when possi-
ble), and we record ICMP and TCP traceroutes between the VP and service.
Since routing outages near the vantage point will skew our observations, we cal-
ibrate our measurements by probing two control sites at USC/ISI and University
of Washington. We probe these sites with the same method as probing the cloud.
We discard cloud measurements when either of these control sites is unavailable.
2.3 Targets: Cloud Storage and VMs
We probe two cloud targets: virtual machines (VMs) and online storage.
Virtual Machines: We test VMs at Amazon only. Google’s VM service is not
yet public, and Microsoft VMs ﬁlter inbound and outbound ICMP traﬃc.
For Amazon VMs, we instantiate a micro VM on Amazon’s Elastic Compute
Cloud (EC2) running Ubuntu 12.04 in all eight regions (May 2013). We install
lighttpd HTTP daemon and serve static, 1 kB ﬁles. We modify the ﬁrewall on
each VM to allow all traﬃc. Each VM is given a public IP address. We probe
this IP address directly. We expect both ICMP and HTTP probes to reach our
VM at the kernel and application-level.
Storage: We test storage on three providers: Amazon Simple Storage Service
(S3), Microsoft Azure, and Google Cloud Storage. Each provider exports an
122
Z. Hu et al.
0.1
0.01
1e-03
1e-04
1e-05
1e-06
1e-07
1e-08
)
s
e
b
o
r
p
k
|
s
s
o
l
t
k
p
y
b
e
g
a
t
u
o
e
s
a
l
f
(
r
P
k=1
k=2
k=3
k=4
k=5
0.9
0.99
0.999
0.9999
0.99999
 0.005
 0.01
 0.015
 0.02  0.025
 0.03
Pr (packet loss)
y
t
i
l
i
b
a
l
i
a
v
a
f
o
s
e
n
n
i
i
g
n
d
n
o
p
s
e
r
r
o
c
Fig. 1. Probability of false positive caused by random packet loss
HTTP-based storage abstraction. We store 1 kB ﬁles on all available regions in
each provider.
For ICMP probes to storage, we ping the hostname in the URL of the stored
object. We expect that this probe contacts only the front-end for the service.
HTTP probes retrieve data from the storage back-end. Providers do not, in
general, provide details about their back-end storage architecture, and we expect
data to be replicated in each datacenter and often across datacenters. HTTP,
however, is an end-to-end test for storage.
2.4 Sources: Vantage Points
We probe each of our targets from vantage points in PlanetLab [5], using 23
starting 2013-03-11 and 54 starting 2013-06-18. We limit the number of VPs
to reduce cloud costs, and select them from universities around the world. We
expect PlanetLab nodes to be well connected, allowing us to focus on cloud avail-
ability. We follow best practices in taking measurements from PlanetLab [24].
3 Evaluating the Need for Retries
A range of possible root causes can explain an outage (Section 2.1). To under-
stand what measurement says about the cloud, we must ﬁrst rule out mundane
causes like packet loss.
While packet loss is rare, cloud outages are much rarer, so random packet loss
will dominate careless observations. We next show that ICMP requires at least
5 retries, and even HTTP beneﬁts from application-level retries in addition to
kernel-level TCP retransmissions.
3.1 A Simple Analytic Model
Packet loss in the network can be correlated (burst losses due to congestion,
ﬁltering) and random (queue overﬂow over medium timescales). We limit distor-
tion from congestive loss by spacing probes 2 s apart, avoiding most short-term
The Need for End-to-End Evaluation of Cloud Availability
123
Pr(ﬁrst try fails)
ICMP HTTP
Target
Amazon/VM
.00585 .00232
Amazon/storage .00574 .00435
.00631 .00217
Google/storage
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
)
l
i
a
f