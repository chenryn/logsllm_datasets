### References

1. **Protocols**. In *Proc. Conference on Secure Information Networks: Communications and Multimedia Security*, 1999, pp. 258–272. [Online]. Available: <http://dl.acm.org/citation.cfm?id=647800.757199>

2. **A. Juels and J. Brainard**, "Client puzzles: A cryptographic countermeasure against connection depletion attacks." In *Proc. Networks and Distributed Security Systems*, 1999, pp. 151–165.

3. **E. K. Kogias, P. Jovanovic, N. Gailly, I. Khoffi, L. Gasser, and B. Ford**, "Enhancing Bitcoin security and performance with strong consistency via collective signing." In *Proc. USENIX Security Symposium*, 2016, pp. 279–296.

4. **B. Laurie and R. Clayton**, "‘Proof-of-work’ proves not to work; version 0.2." In *Proc. Workshop on Economics and Information Security*, 2004.

5. **Litecoin**. <https://litecoin.org/>

6. **A. Miller, A. Juels, E. Shi, B. Parno, and J. Katz**, "Permacoin: Repurposing Bitcoin work for data preservation." In *Proc. IEEE Security and Privacy*, 2014, pp. 475–490.

7. **S. Nakamoto**, "Bitcoin: A Peer-to-Peer Electronic Cash System." <https://bitcoin.org/bitcoin.pdf>, May 2009.

8. **A. P. Ozisik, G. Andresen, G. Bissias, A. Houmansadr, and B. N. Levine**, "Graphene: A New Protocol for Block Propagation Using Set Reconciliation." In *Proc. of International Workshop on Cryptocurrencies and Blockchain Technology (ESORICS Workshop)*, Sept 2017.

9. **A. P. Ozisik and B. N. Levine**, "An Explanation of Nakamoto’s Analysis of Double-spend Attacks." University of Massachusetts, Amherst, MA, Tech. Rep. arXiv:1701.03977, January 2017.

10. **A. P. Ozisik, B. N. Levine, G. Bissias, G. Andresen, D. Tapp, and S. Katkuri**, "Graphene: Efficient Interactive Set Reconciliation Applied to Blockchain Propagation." In *Proc. ACM SIGCOMM*, August 2019.

11. **R. Pass and E. Shi**, "Fruitchains: A fair blockchain." In *Proc. ACM Symposium on Principles of Distributed Computing*, 2017, pp. 315–324.

12. **P. Rizun**, "Subchains: A Technique to Scale Bitcoin and Improve the User Experience." *Ledger*, vol. 1, pp. 38–52, 2016.

13. **A. Sapirshtein, Y. Sompolinsky, and A. Zohar**, "Optimal Selfish Mining Strategies in Bitcoin." In *Proc. Financial Cryptography and Data Security*. (See also <https://arxiv.org/pdf/1507.06183.pdf>), Feb 2016.

14. **M. Vukolic**, "The Quest for Scalable Blockchain Fabric: Proof-of-Work vs. BFT Replication." In *International Workshop on Open Problems in Network Security*, 2015.

15. **X. Wang and M. K. Reiter**, "Defending against denial-of-service attacks with puzzle auctions." In *Proc. IEEE Symposium on Security and Privacy*, 2003, pp. 78–92.

16. **R. Zhang and B. Preneel**, "Lay Down the Common Metrics: Evaluating Proof-of-Work Consensus Protocols’ Security." In *IEEE Symposium on Security and Privacy*, 2019.

### Appendix

In this appendix, we prove that \( V_i \), the value of the \( i \)-th order statistic, is gamma distributed (Theorem 6). We also prove that \( X_i \), which is the number of hash intervals required for the \( i \)-th order statistic to fall below a target \( v \), is gamma distributed (Theorem 7). Both theorems are applied in Section IV. We then derive the joint distribution of the \( i \)-th and \( j \)-th order statistics in Lemma 8, which is applied in Section IV-C.

#### A. Properties of Bobtail Order Statistics

We begin with a supporting lemma. Consider the distribution of \( H \), an arbitrary random variable chosen from the sequence of block hashes \( H_1, \ldots, H_h \). We have \( f_H(t; S) = \frac{1}{S} \) and \( F_H(t; S) = \frac{t}{S} \).

**Lemma 7**: The probability density function (PDF) of the \( i \)-th order statistic, \( V_i \), from \( h \) samples (i.e., hashes) is given by:
\[
f_{V_i}(t; S, h) = \frac{h!}{(i-1)!(h-i)!} \left( \frac{t}{S} \right)^{i-1} \left( 1 - \frac{t}{S} \right)^{h-i} \cdot \frac{1}{S}
\]
This result is well-known; see, for example, Casella and Berger [12].

When the hash interval \( I \) corresponds to the desired block time, say 600 seconds for Bitcoin, there will be many hashes performed during the interval. Therefore, it is reasonable to consider how the distribution for \( V_i \) changes in the limit as \( h \) approaches infinity.

**Theorem 6**: In the limit as \( h \) approaches infinity, \( V_i \sim \text{Gamma}(i, v) \), where \( v \) is the expected value of the minimum hash.

**Proof**: Define \( g(t; i, v) \) to be the PDF of the gamma distribution with shape parameter \( i \) and scale parameter \( v \). If the number of hashes approaches infinity, then so must the size of the hash space, and yet \( S \) must always be larger than \( h \). Therefore, we assume that \( h = \frac{S}{v} \) for some parameter \( v > 1 \). Under this assumption, we can equivalently consider the limit as \( S \) approaches infinity. We have:
\[
f_{V_i}(t; S, h) = \lim_{h \to \infty} f_{V_i}(t; S, h)
\]
\[
= \lim_{S \to \infty} \frac{(S/v)!}{(i-1)!(S/v - i)!} \left( \frac{t}{S} \right)^{i-1} \left( 1 - \frac{t}{S} \right)^{S/v - i} \cdot \frac{1}{S}
\]
\[
= \lim_{S \to \infty} \frac{(S/v)!}{S^i (i-1)!(S/v - i)!} t^{i-1} \left( 1 - \frac{t}{S} \right)^{S/v - i}
\]
\[
= \lim_{S \to \infty} \frac{t^{i-1}}{(i-1)! v^i} e^{-t/v}
\]
\[
= g(t; i, v)
\]

When \( i = 1 \), \( V_1 \sim \text{Gamma}(t; 1, v) = \text{Exponential}(t; v) \). Since the expected value of an exponential random variable is equal to the value of its scale parameter, \( v \) is simply the expected value of the minimum hash.

Next, define \( X_i \) as the number of intervals required for \( V_i \) to fall below \( v \), and consider the PDF of \( X_i \), \( f_{X_i}(x; S, v) \). After \( x \) hash intervals, let \( E \), \( L \), and \( G \) be, respectively, the events that the \( i \)-th order statistic is equal to \( v \), the order statistics below \( i \) are less than \( v \), and the order statistics above \( i \) are greater than \( v \). Furthermore, let \( O \) be the set of all divisions of \( H_1, \ldots, H_h \) into distinct sets \( \{H | H = V_i\} \), \( \{H | H < V_i\} \), and \( \{H | H > V_i\} \). We have:
\[
f_{X_i}(x; S, v) = \sum_{o \in O} P[E(x), L(x), G(x) | o]
\]
\[
= \frac{(hx)!}{(i-1)!(hx - i)!} \left( \frac{v}{S} \right)^{i-1} \left( 1 - \frac{v}{S} \right)^{hx - i} \cdot \frac{1}{S}
\]

Assuming \( I \) is large, it again makes sense to consider the limit as \( h \) approaches infinity.

**Theorem 7**: In the limit as \( h \) approaches infinity, \( X_i \sim \text{Gamma}(i, 1/r) \), where \( r \) is the expected rate, in units of \( I \), at which \( V_i \) falls below \( v \).

**Proof**: The probability that any given hash succeeds, i.e., falls below \( v \), is given by \( p = \frac{v}{S} \). Again, we would like to consider the limit as \( h \) approaches infinity. But in doing so, we must ensure that \( p \) remains constant. In other words, the probability of hash success must diminish as \( h \) increases. So there must exist some constant \( r \) such that \( p = \frac{r}{h} \). It follows that:
\[
f_{X_i}(x; S, v) = \frac{(hx)!}{(i-1)!(hx - i)!} \left( \frac{r}{h} \right)^{i-1} \left( 1 - \frac{r}{h} \right)^{hx - i}
\]

Arguing in similar fashion as for \( V_i \), we find that:
\[
\lim_{h \to \infty} f_{X_i}(x; S, v) = g(x; i, 1/r)
\]

Thus, \( E[X_i] = \frac{1}{r} \), which implies that \( r \) should be interpreted as the expected rate at which \( V_i \) falls below \( v \) during a single interval \( I \).

#### B. Joint Distribution

Here we derive the limiting joint distribution of the \( i \)-th and \( j \)-th order statistics \( V_i \) and \( V_j \), which are applied in Section IV-C to derive the variance of \( W_k \).

**Lemma 8**: In the limit as \( h \) approaches infinity, the joint distribution of the \( i \)-th and \( j \)-th order statistics of uniform random samples \( H_1, \ldots, H_h \) is given by:
\[
f_{V_i, V_j}(t_i, t_j; v) = g(t_i; i, v) g(t_j - t_i; j - i, v)
\]
where \( v \) is the expected value of the minimum hash.

**Proof**: It is well known [12] that the joint distribution of the \( i \)-th and \( j \)-th order statistics, out of \( h \) total samples, is given by:
\[
f_{V_i, V_j}(t_i, t_j; v) = \frac{h!}{(i-1)!(j-1-i)!(h-j)!} f_H(t_i) f_H(t_j) [F_H(t_i)]^{i-1} [F_H(t_j) - F_H(t_i)]^{j-1-i} [1 - F_H(t_j)]^{h-j}
\]

Thus, we have:
\[
f_{V_i, V_j}(t_i, t_j; S, v) = \frac{h!}{(i-1)!(j-1-i)!(h-j)!} \left( \frac{t_i}{S} \right)^{i-1} \left( \frac{t_j - t_i}{S} \right)^{j-1-i} \left( 1 - \frac{t_j}{S} \right)^{h-j} \cdot \frac{1}{S^2}
\]

Finally, assuming \( j > i \), and reasoning in the limit as \( S \to \infty \) in the same manner as in Theorem 6:
\[
f_{V_i, V_j}(t_i, t_j; v) = \lim_{S \to \infty} f_{V_i, V_j}(t_i, t_j; S, v)
\]
\[
= \frac{t_i^{i-1}}{v^i (i-1)!} e^{-t_i/v} \cdot \frac{(t_j - t_i)^{j-1-i}}{v^{j-i} (j-1-i)!} e^{-(t_j - t_i)/v}
\]
\[
= g(t_i; i, v) g(t_j - t_i; j - i, v)
\]

[12] See Casella and Berger, *Statistical Inference*, Theorem 5.4.6.