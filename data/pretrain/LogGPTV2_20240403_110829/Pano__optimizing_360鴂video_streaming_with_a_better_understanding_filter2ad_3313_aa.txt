title:Pano: optimizing 360° video streaming with a better understanding
of quality perception
author:Yu Guan and
Chengyuan Zheng and
Xinggong Zhang and
Zongming Guo and
Junchen Jiang
Pano: Optimizing 360° Video Streaming with a Better
Understanding of Quality Perception
(cid:2)◦
Yu Guan
(cid:2)
, Chengyuan Zheng
(cid:2)
Peking University
◦
, Xinggong Zhang
(cid:2)◦
, Zongming Guo
(cid:2)◦
Junchen Jiang
PKU-UCLA JRI
University of Chicago
ABSTRACT
Streaming 360° videos requires more bandwidth than non-360°
videos. This is because current solutions assume that users per-
ceive the quality of 360° videos in the same way they perceive the
quality of non-360° videos. This means the bandwidth demand must
be proportional to the size of the user’s field of view. However, we
found several quality-determining factors unique to 360° videos,
which can help reduce the bandwidth demand. They include the
moving speed of a user’s viewpoint (center of the user’s field of
view), the recent change of video luminance, and the difference in
depth-of-fields of visual objects around the viewpoint.
This paper presents Pano, a 360° video streaming system that
leverages the 360° video-specific factors. We make three contribu-
tions. (1) We build a new quality model for 360° videos that captures
the impact of the 360° video-specific factors. (2) Pano proposes a
variable-sized tiling scheme in order to strike a balance between
the perceived quality and video encoding efficiency. (3) Pano pro-
poses a new quality-adaptation logic that maximizes 360° video
user-perceived quality and is readily deployable. Our evaluation
(based on user study and trace analysis) shows that compared with
state-of-the-art techniques, Pano can save 41-46% bandwidth with-
out any drop in the perceived quality, or it can raise the perceived
quality (user rating) by 25%-142% without using more bandwidth.
CCS CONCEPTS
• Networks → Application layer protocols;
1 INTRODUCTION
360° videos are coming to age, with most major content providers
offering 360° video-based applications [1, 3, 7, 10, 12, 19, 20]. At the
same time, streaming 360° videos is more challenging than stream-
ing traditional non-360° videos. To create an immersive experience,
a 360° video must stream the content of a large sphere, in high
resolution and without any buffering stall [35, 55]. To put it into
perspective, let us consider a traditional full-HD video of 40 pixels
per degree (PPD) displayed on a desktop screen, which is an area of
∼48° in width as perceived by viewer’s eyes (if the screen is 15" in
width at a distance of 30" to the viewer). Streaming this video on the
laptop screen takes roughly 5 Mbps. In contrast, if we want to keep
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGCOMM ’19, August 19–23, 2019, Beijing, China
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-5956-6/19/08. . . $15.00
https://doi.org/10.1145/3341302.3342063
the perceived quality level (same PPD) for the panoramic sphere, it
will take 400 Mbps, 80× more bandwidth consumption [47].
This paper is motivated by a simple, yet seemingly impossible
quest: can we stream 360° videos in the same perceived quality as
traditional non-360° videos without using more bandwidth? Given
that today’s Internet is capable of streaming high-quality videos
to billions of users in most parts of the world, achieving this goal
would have great societal implications and could spur massive
popularization of 360° videos.
Unfortunately, the current approaches fall short of achieving
this goal. Most solutions (e.g., [26, 32, 34, 50, 52, 59, 68]) follow
the viewport-driven approach, where only the viewport (the region
facing the viewer) is streamed in high quality, but this approach has
several limitations. First, a viewport (∼110° in width [63]) is still
much larger than a laptop screen (∼48° in width) as perceived by
users, so to stream a viewport region would still need at least twice
the bandwidth of streaming a screen-size video at the same qual-
ity [28]. Second, as the viewport content needs to be pre-fetched,
the player must predict where the user will look at in the future, so
any prediction error can cause playback rebuffering or quality drops.
Third, to adapt to arbitrary viewport movements, the 360° video
must be spatially split into small tiles, which could substantially
increase the size of the video.
In this work, we look beyond the viewport-driven approach and
show that the quality of 360° videos is perceived differently than that
of non-360° videos, due to the presence of viewpoint movements1. In
particular, we empirically show three quality-determining factors
unique to 360° videos. The user’s sensitivity to the quality of a
region M is dependent on (1) the relative viewpoint-moving speed
between the movement of viewpoint (center of the viewport) and
the movement of visual objects in the region M, (2) the difference
of depth-of-field (DoF) between the region M and the viewpoint-
focused content, and (3) the change in luminance of the viewport
in the last few seconds. For instance, when the viewpoint moves
slowly (e.g., <5 deg/s), users tend to be sensitive to small quality
distortion; when the viewpoint moves quickly (e.g., shaking head or
browsing landscape), the sensitivity can drop sharply—users might
be insensitive to large quality distortion. In short, how sensitive a
user is to quality distortion can vary over time due to the viewpoint
movements. (See §2.2 for more discussions.)
The observation that users perceive 360° video quality differently
opens up new opportunities to improve 360° video quality and save
bandwidth. If we know a user’s sensitivity to quality distortion, we
can raise quality by a maximally perceivable amount, when there
1This paper makes two assumptions: (1) the movement of the head-mounted device
can approximate the movement of the actual viewpoint, and (2) the object closest
to the viewpoint is the one being watched by the user. These assumptions might be
simplistic, but they can be refined with recent work on accurate viewpoint tracking
(e.g., [9, 18, 31, 46, 65]).
394
SIGCOMM ’19, August 19–23, 2019, Beijing, China
Y. Guan, C. Zheng, X. Zhang, Z. Guo, J. Jiang.
y
t
i
l
i
a
u
q
d
e
v
e
c
r
e
p
r
e
s
U
60
50
)
R
N
P
S
P
(
40
0
Pano(this work)
Viewport-driven
Whole video
2
3
1
Buffering ratio (%)
Better
4
Figure 1: Performance of Pano and the popular viewport-driven ap-
proach on 18 360° videos with real viewpoint traces over an emulated
cellular network link. Full results are in §8.
is spare bandwidth; and we can lower the quality by a maximal
yet imperceptible amount, when the bandwidth is constrained. The
underlying insight is that each user has a limited span of attention.
For instance, when a user moves her viewpoint, the area being
watched does increase, but since the attention will be spread across
a wider area, the user’s attention per-pixel actually decreases.
To explore these opportunities, this paper presents Pano, a 360°
video streaming system that entails three contributions:
First, Pano is built on a new quality model for 360° videos that
systematically incorporates the new quality-determining factors (§4).
We run a user study2 to quantitatively show the relationship be-
tween the user’s sensitivity to quality distortion and the relative
viewpoint-moving speed, the difference of depth-of-field (DoF),
and the change of luminance. The new model allows us to esti-
mate the subjectively perceived video quality more accurately than
traditional video quality metrics (e.g., PSNR [40]).
Second, Pano uses a novel variable-sized tiling scheme to cope with
the heterogeneous distribution of users’ sensitivity over the panoramic
sphere (§5). Traditionally, a 360° video is split into equal-sized tiles
(e.g., 6×12, 12×24), each encoded in multiple quality levels, so that
the player can choose different quality levels for different tiles as
the viewport location moves. This uniform tiling scheme, however,
might be either too coarse-grained to reflect where the user sen-
sitivity varies, or too fine-grained to contain the video encoding
overhead. Instead, Pano uses variable-sized tiling scheme, which
splits the video into tiles of different sizes so that a user tends to
have similar sensitivity when watching the same tile.
Finally, Pano adapts video quality in a way that is (a) robust to
the vagaries of viewpoint movements, and (b) readily deployable in
the existing video delivery infrastructure (§6). Pano optimizes user-
perceived quality by dynamically predicting viewpoint movements
and adapting quality accordingly. Despite the inevitable viewpoint
prediction errors, Pano can still pick the desirable quality levels,
because to estimate the user’s sensitivity to quality distortion, it
suffices to predict the range of viewpoint-moving speed, luminance
and DoF. In addition, since Pano needs information from both client
(i.e., viewpoint trajectory) and server (i.e., video pixel information),
it is incompatible with the mainstream DASH protocols [4] where
a client locally makes bitrate-adaptation decisions. To address this,
Pano decouples the bitrate adaptation into an offline phase and an
online phase. The offline phase pre-computes the perceived quality
estimates under a few carefully picked viewpoint movements, and
2Our study was IRB approved by our university, IRB00001052-18098. It does not raise
any ethical issues.
then it sends them to the client at the beginning of a video. In the
online phase, the client predicts the perceived quality by finding a
similar viewpoint movement that has a pre-computed estimate.
We implemented a prototype of Pano and evaluated it using a
combination of user studies (20 participants, 7 videos) and trace-
driven simulations (48 users, 18 videos). Across several content
genres (e.g., sports, documentary), Pano can increase the mean
opinion score (MOS) [22] by 25-142% over a state-of-the-art solution
without using more bandwidth. It can also save bandwidth usage
by up to 46% or reduce buffering by 60-98% without any drop in
perceived quality. Pano suggests a promising alternative to the
popular viewport-driven approach (e.g., Figure 1), which could
potentially close the gap of bandwidth consumption between 360°
videos and traditional videos as we have hoped.
2 MOTIVATION
We begin by setting up the background of 360° video streaming
(§2.1). Then we introduce the quality-determining factors unique
to 360° videos (§2.2), and analyze the potential improvement (§2.3)
of leveraging these factors.
2.1 Background of 360° video streaming
There are already 36.9 million VR users in the US (over 10% of its
population) [13]. By 2022, there will be 55 million active VR headsets
in the US, as many as Netflix members in the US in 2018 [16].
Many content providers (YouTube [3], Facebook [7], Netflix [10],
Vimeo [1], Hulu [19], iQIYI [12]) offer 360° video streaming services
on various platforms [6, 17, 63].
The proliferation of 360° videos is facilitated in part by the cheap
and scalable delivery architecture. Like other Internet videos, 360°
videos can be delivered to viewers through content delivery net-
works (CDNs). A 360° video is first converted to a planar video and
encoded by a 360° encoder (e.g., [8]), which transcodes and chops the
video into chunks (or segments); these video chunks are then sent
to geo-distributed CDN servers; and finally, a client (VR headset or
smartphone) streams the video chunks sequentially from a nearby
CDN server using the standard HTTP(S) protocols [4, 11, 14]. To
cope with bandwidth fluctuations, each video segment is encoded
in different quality levels, such as quantization parameters (QP), so
that during playback the player can dynamically switch between
quality levels at the boundary of two consecutive chunks, similar
to traditional bitrate-adaptive streaming.
A distinctive feature of 360° video streaming is that the viewer’s
attention is unevenly distributed, with more attention in the view-
port area (which directly faces the user) than the rest of the video. In
contrast, non-360° videos are displayed in a more confined area (e.g.,
a desktop screen), so the uneven distribution of attention is less
obvious. The uneven distribution of attention has spurred a rich
literature around the idea of viewport-driven streaming (e.g., [35,
36, 52, 62]) to improve 360° video quality. It spatially partitions a
video into tiles (e.g., 6-by-12 grids) and encodes each tile in multi-
ple quality levels, so the 360° video player can dynamically assign
a higher quality level to tiles closer to the viewpoint (the center
of a viewport). Unfortunately, viewport-driven streaming has two
limitations. First, like traditional videos, each 360° video chunk
must be prefetched before the user watches it, but viewport-driven
streaming only fetches the viewport region in the hope that the
395
Pano: Optimizing 360° Video Streaming with a Better
Understanding of Quality Perception
SIGCOMM ’19, August 19–23, 2019, Beijing, China
(cid:2)(cid:3)(cid:8)(cid:7)
(cid:1)(cid:8)(cid:6)(cid:4)(cid:5)(cid:9)
(cid:2)(cid:3)(cid:8)(cid:7)
(cid:1)(cid:8)(cid:6)(cid:4)(cid:5)(cid:9)
(cid:8)(cid:6)(cid:4)(cid:5)(cid:9)(cid:9)(cid:9)
(cid:2)(cid:8)(cid:3)(cid:1)(cid:7)(cid:18)(cid:21)(cid:8)(cid:10)(cid:23)(cid:1)(cid:20)(cid:13)(cid:1)(cid:25)(cid:16)(cid:12)(cid:26)(cid:21)(cid:20)(cid:16)(cid:19)(cid:23)(cid:1)(cid:18)(cid:20)(cid:25)(cid:16)(cid:19)(cid:14)(cid:1)(cid:22)(cid:21)(cid:12)(cid:12)(cid:11)
(cid:2)(cid:9)(cid:3)(cid:1)(cid:7)(cid:18)(cid:21)(cid:8)(cid:10)(cid:23)(cid:1)(cid:20)(cid:13)(cid:1)(cid:22)(cid:10)(cid:12)(cid:19)(cid:12)(cid:1)(cid:17)(cid:24)(cid:18)(cid:16)(cid:19)(cid:8)(cid:19)(cid:10)(cid:12)(cid:1)(cid:10)(cid:15)(cid:8)(cid:19)(cid:14)(cid:12)(cid:22)
(cid:2)(cid:10)(cid:3)(cid:1)(cid:7)(cid:18)(cid:21)(cid:8)(cid:10)(cid:23)(cid:1)(cid:20)(cid:13)(cid:1)(cid:5)(cid:12)(cid:21)(cid:23)(cid:15)(cid:4)(cid:20)(cid:13)(cid:4)(cid:6)(cid:16)(cid:12)(cid:17)(cid:11)(cid:1)(cid:10)(cid:15)(cid:8)(cid:19)(cid:14)(cid:12)(cid:22)
Figure 2: Illustrative examples of three 360° video quality-determining factors, and how they help save bandwidth by reducing the quality of
some part of the video without affecting the user-perceived quality. The yellow boxes indicate the viewport area (dashed ones are the previous
viewport). In each case, the left-hand side and the right-hand side have similar perceived QoE, despite quality distortion on the right.
fetched content matches the user’s viewport. So any viewpoint
prediction error may negatively affect user experience. Second, to
assign quality by the distance to the dynamic viewpoint, the video
must be split into many fine-grained tiles [52] or encoded in multi-
ple versions each customized for certain viewpoint trajectory [68],
but both methods could significantly increase the video size.
2.2 New quality-determining factors
A basic assumption underlying the prior efforts is that users per-
ceive the quality of 360° videos (within the viewport) in the same
way they perceive the quality of non-360° videos. This assumption
limits the room for improving the performance of streaming 360°
videos. In other words, since the viewport appears larger than a
desktop screen to the user, it still takes more bandwidth to stream
a 360° video than a traditional screen-size video.
In contrast, our key insight is that the user-perceived quality of
360° videos is uniquely affected by users’ viewpoint movements.
Here, we explain three quality-determining factors that are induced
by a user’s viewpoint movements (readers may refer to §4 for more
analysis of their impacts on quality perception).
• Factor #1: Relative viewpoint-moving speed. In general, the
faster the user’s viewpoint moves, the less sensitive the user is
to quality distortion. Figure 2(a) illustrates how this observation
could help save bandwidth: when the user moves her viewpoint,
reducing the quality level of the static background will have little
impact on user-perceived quality. Of course, the moving objects
being tracked by the viewpoint will now appear static to the user,
so its quality degradation has a negative impact on the perceived
quality. This idea is particularly relevant to sports videos, where
the viewpoint often moves with fast-moving objects.
• Factor #2: Change in scene luminance. As a user moves her
view around, the viewed region may switch between different
levels of luminance; when the content changes from dark to bright
(and vice versa), users tend to be less sensitive to quality distortion
in a short period of time (typically 5 seconds [49, 51]). Figure 2(b)
illustrates a simple example of how one can carefully lower the
quality level of part of the video without causing any drop in
the user-perceived quality. Luminance changes are prevalent in
urban night scenes, where the viewpoint may switch frequently
between different levels of brightness.
• Factor #3: Difference in depth-of-field (DoF). In 360° videos,
users are more sensitive to quality distortion of a region whose
DoF3 is closer to that of the viewpoint. So, users may have dif-
ferent sensitivities to the quality of the same region, depending
3360° displays can simulate DoF by projecting an object to two eyes with a specific
binocular parallax (disparity) [27, 39].
on the DoF of the current viewpoint. As illustrated in Figure 2(c),
one can save bandwidth by dynamically tracking the DoF of the
viewpoint and reducing the quality level of objects that have great
difference in DoFs to the viewpoint. DoF adaptation tends to ben-
efit outdoor videos where the viewpoint may switch between
foreground objects (low DoF) and scenic views (high DoF).
Intuitive explanation: The key to understanding these opportu-
nities is that each user has a limited span of attention. Although the
video size grows dramatically to create an immersive experience, a
user’s span of attention remains largely constant. As a result, a user
often gives less attention to the specifics of a 360° video, which in
turn reduces her sensitivity to quality distortion.
What is new about them? Although prior work (e.g., [37, 44, 45,
57]) also improves video encoding and streaming by leveraging the
video perceptual features (e.g., luminance and salient objects) and
intrinsic dynamics (e.g., fast changing content), it is always assumed
that these factors are determined by the video content, not users’
viewpoint movements. In contrast, we seek to take into account
the object movements, luminance changes, and DoF differences, all
caused by users’ viewpoint movements, so our approach can be
viewed complementary to this body of prior work. For instance,
static objects may appear as fast moving objects to a 360° video
user (thus can tolerate low quality), if the user moves the viewport
rapidly. Similarly, fast moving objects will appear static to the user
(thus requiring high quality), if her viewpoint moves with the object.
2.3 Potential gains
Next, we use real viewpoint traces to demonstrate the potential
benefits of these quality-determining factors. The traces [5] consist
of 864 distinct viewpoint trajectories (18 360° videos each watched
by 48 users [21], see Table 2 for a summary). We measure viewpoint-
moving speed in degrees per second (deg/s), luminance in gray
level [30, 56], and DoF in dioptres [27, 39].
Figure 3 shows the distribution of viewpoint-moving speeds, the
distribution of maximum luminance changes in different 5-second
time windows, and the distribution of maximum DoF differences
between two regions in one frame. To see how these values impact
users’ sensitivities to quality distortion, we measure how often
these values exceed some thresholds so that users can tolerate 50%
more quality distortion than they would have if the viewpoint was
static. Based on our empirical user study in §4.2, such threshold of
viewpoint-moving speed is 10 deg/s, that of luminance change is
200 gray level, and that of DoF difference is 0.7 diopters.
We can see that all three factors exceed their thresholds for
5-40% of time. In other words, for instance, for 40% of time the
viewpoint moves over 10 deg/sec, which means during that time,
396
SIGCOMM ’19, August 19–23, 2019, Beijing, China
Y. Guan, C. Zheng, X. Zhang, Z. Guo, J. Jiang.
)
%
(
F
D
C
100
75
50
25