WriteConn(C, CPtr)
if (Case (A-F, J-K, O-R)) then
if (Case (G, H, I, L, N, S) then
Forward(P)
if (Case (G, H, I)) then
Normalize(P)
1. UpdateConn(CPtr, C, P)
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
DropPacket(P)
WriteConn(C, CPtr)
Analyze(P)
if (Case (I)) then
Analyze(C.Bh-C.Bt)
else if (Case (L, N, S)) then
Buffer(P, C, CPtr)
else if (Case (M, T)) then
4.3 Buffering out-of-order packets
Storing variable-length IP packets efﬁciently requires a
memory management algorithm such as paging or a
buddy system. Due to its simplicity and low overhead,
in our design we use paging. We segment available mem-
ory into ﬁxed-length pages (chunks) allocated to incoming
packets as required. If a packet is bigger than a page then
we store it across multiple pages, with all pages linked in a
list. We use a pointer to the ﬁrst page as the packet pointer.
If a packet is smaller than the page size then we use the
remaining space in its page to store the next packet, com-
pletely or partially.
To track a packet chunk, we need the following (see
Figure 5):
buffer (3 bytes)
• Conn: pointer to the connection associated with the
• Next: pointer to the next page (3 bytes)
• FrontOrBack (FB): whether the page is ﬁlled starting
• Offset (Of): pointer to boundary between valid data
from its beginning or its end (1 bit)
and unused portion of the page (11 bits)
to the corresponding
connection record
to next page
Conn
Next
FB,Of
Data of the 
previous packet
Packet 1
256 words
Packet 2
Beginning of Packet 3
8−byte word
Figure 5: Page record. Note that packets can be split across
two pages. The page shown here holds data for a packet
that starts in a previous chunk, and another that ends in a
later chunk. A packet starts immediately from the next byte
where the previous packet ends.
(For convenience, in our
design buffered packets include their full TCP/IP headers,
although we could compress these.)
Conn is needed in the case of a premature eviction of
a page requiring an update of the corresponding connec-
tion record. Next points to the next page used in the same
hole buffer. FrontOrBack allows us to ﬁll pages either
from their beginning (just after the header) or their end.
We ﬁll in front-to-back order when appending data to ex-
tend a hole to later sequence numbers. We ﬁll in back-
to-front order when prepending data to extend to earlier
sequence numbers. If FrontToBack is set to FRONT, then
Offset points to where valid data in the page ends (so we
know where to append). If it is set to BACK, then Offset
points to where valid data begins (so we know where to
prepend).
When we free memory pages, we append them to a
free-list with head (FreeH) and tail (FreeT) pointers. The
pseudocode for our initial design (which we will modify
in the next section) is:
WritePage(P, C.Bt)
if (insufficient space in C.Bt) then
if (free page not available) then
1. BufferPacket_v1(P, C, CPtr)
2.
3.
4.
5.
6.
7.
8.
9.
x = AllocatePage(FreeH)
WritePage(P, x)
AppendPage(x, C.Bt)
WriteConn(C, CPtr)
EvictPages()
In the pseudocode above, we ﬁrst start writing the
packet in the free space of the tail page (line 2), updating
the page’s Offset ﬁeld in the process. (If FrontOrBack is
set to BACK for this page, then we know immediately that
we cannot append further to it.) If this ﬁlls up the page and
a portion of the packet remains to be written (line 3), we
need to allocate a free page for the remaining portion (a
single page sufﬁces since pages are larger than maximum-
sized packets). To do that, we ﬁrst check if we have any
free pages left in the memory. If not, then we must evict
some occupied pages. (See below for discussion of the
USENIX Association
14th USENIX Security Symposium
73
Analyzer
Buffer 
Manager
In−order
Packet
Processing
Packet
Stream
Out−of−order
Packet
Processing
Connection
Record
Manager
Figure 6: Block diagram of the system.
eviction policy.) After possibly freeing up pages and ap-
pending them to the free-list, we write the packet into the
ﬁrst free page, FreeH (lines 6–7). When done, we append
the page to the list of pages associated with the connec-
tion (line 8). Finally, we update the connection record
and write it back at the corresponding connection pointer
(line 9).
Note that in some cases we need to prepend a page to
the existing hole buffer instead of appending it (e.g., Case
(N) in Fig. 4). In this case, we allocate a new page, set
FrontOrBack to BACK, copy the packet to the end of the
page, and set Offset to point to the beginning of the packet
rather than the end. The pseudocode for this case is simi-
lar to the above, and omitted for brevity.
4.4 Block Diagram
We can map the various primitives of connection and
buffer management from the previous sections to the
block diagram shown in Figure 6. Module In-order Packet
Processing (IOPP) handles the processing of in-order
packets (cases (G,H,I) in Figure 4). It contains the prim-
itives CreateConn(), InsertConn(), LocateConn(), Read-
Conn(), WriteConn() and Normalize().
It passes all in-
order packets to the Analyzer module, which is the inter-
face to the byte-stream analyzer (which can be a separate
software entity or an integrated hardware entity). When
a hole is closed completely, the buffer pointers (C.Bh and
C.Bt) are passed to the Analyzer, which reads the pack-
ets from Buffer Manager. For out-of-order packets (cases
(L,N,S)), the packet is passed to the Out-of-order packet
processing (OOPP) module. This module maintains the
primitives WritePage(), EvictPage(), AppendPage() and
WriteConn(). When the packet needs buffering, the cor-
responding connection record can be updated only after
allocating the buffer. Hence, these delayed connection up-
dates are handled by WriteConn() of OOPP. The Connec-
tion Record Manager arbitrates requests from both IOPP
and OOPP.
To accommodate delays in buffering a packet, we use a
small queue between IOPP and OOPP, as shown in Fig-
ure 6. Note that the occupancy of this queue depends
on how fast out-of-order packets can arrive, and at what
speed we can process them (in particular, buffer them).
Since we have two independent memory modules for
connection record management and packet buffering, we
can perform these processes in a pipelined fashion. The
speedup gained due to this parallelism ensures that the
OOPP is fast enough to keep the occupancy of the small
queue quite low. In particular, recall that the raw band-
width of a DDR SDRAM used for buffering packets can
be as high as 21 Gbps. Even after accounting for access
latency and other inefﬁciencies, if we assume a through-
put of a couple of Gbps for OOPP, then an adversary must
send out-of-order packets at multi-Gbps rates to overﬂow
the queue. This will cause more collateral damage simply
by completely clogging the link than by lost out-of-order
packets.
5 Dealing with an Adversary
We now turn to analyzing possible vulnerabilities in our
stream reassembly mechanism in terms of ways by which
an adversary can launch an attack on it and how we can
avoid or at least minimize the resulting damage.
In a memory subsystem, the obvious resources that an
attacker can exploit are the memory space and the mem-
ory bandwidth. For our design, we have two independent
memory modules, one for maintaining connection state
records and the other for buffering out-of-order packets.
Furthermore, attacking any of the design’s components, or
a combination, can affect the performance of other com-
ponents (e.g., bytes-stream analyzer).
For our system, the buffer memory space is particularly
vulnerable, since it presents a ready target for overﬂow
by an attacker, which can then potentially lead to abrupt
termination of connections. We refer to such disruption as
collateral damage.
We note that an attacker can also target the connection
record memory space. However, given 512 MB of mem-
ory one can potentially maintain 16M 32-byte connection
records.
If we give priority to evicting non-established
connections ﬁrst, then an attacker will have great difﬁculty
in overﬂowing this memory to useful effect. We can ef-
ﬁciently perform such eviction by choosing a connection
record to reuse at random, and if it is marked as estab-
lished then scanning linearly ahead until ﬁnding one that
is not. Under ﬂooding conditions the table will be heavily
populated with non-established connections, so we will
not spend much time ﬁnding one (assuming we have been
careful with randomizing our hash function [9]).
Attacks on memory bandwidth can slow down the de-
vice’s operation, but will not necessarily lead to connec-
tion evictions. To stress the memory bandwidth, an at-
74
14th USENIX Security Symposium
USENIX Association
tacker can attempt to identify a worst-case memory ac-
cess pattern and cause the system to repeatedly execute
it. The degree to which this attack is effective will de-
pend on the details of the speciﬁc hardware design. How-
ever, from an engineering perspective we can compute the
resultant throughput in this case and deem it the overall,
guaranteed-supported throughput.
Finally, as we show in the next section, if we do not
implement appropriate mechanisms, then an attacker can
easily overﬂow the hole buffer and cause collateral dam-
age. Hence, we focus on this particular attack and reﬁne
our system’s design to minimize the resulting collateral
damage.
5.1 Attacks on available buffer memory
While our design limits each connection to a single hole,
it does not limit the amount of buffer a single connection
can consume for its one hole. A single connection cre-
ated by an adversary can consume the entire packet buffer
space by accumulating an arbitrary number of packets be-
yond its one hole. However, we can contain such con-
nections by limiting per-connection buffer usage to a pre-
determined threshold, where we determine the threshold
based on trace analysis. As shown in Figure 3, 100 KB of
buffer sufﬁces for virtually all connections.
Unfortunately, an adversary can then overﬂow the
buffer by creating multiple connections with holes while
keeping the buffer of each within the threshold. A simple
way to do this is by creating connections from the same
host (or to the same host) but using different source or
destination ports. However, in our trace analysis we ob-
served very few instances of a single host having multiple
holes concurrently on two different connections. Here, it
is important to observe that the adversary is essentially an
external client trying to bypass our system into a protected
network. While we see several instances of a single client
(e.g. web or email proxy) inside the protected network
creating concurrent connections with holes, these can be
safely discounted since these hosts do not exhibit any ad-
verse behavior unless they are compromised. From our
traces, it is very rare for a (legitimate) external client to
create concurrent connections with holes (the last row of
Table 1). This observation holds true even for T3’s con-
gested link.
We exploit this observation to devise a simple policy
of allowing just one connection with a hole per external
client. With this policy, we force the attacker to create
their different connections using different hosts.
To realize this policy, we need an additional table to
track which external clients already have an unplugged
hole. When we decide to buffer a packet, we ﬁrst check
to see if the source is an internal host by comparing it
against a white-list of known internal hosts (or preﬁxes).
If it is an external client and already has an entry in the
table then we simply drop the packet and disallow it to
create another connection with hole.
Our modiﬁed design forces an adversary to use multi-
ple attacking hosts (assuming they cannot spoof both sides
of a TCP connection establishment handshake). Let us
now analyze this more difﬁcult case, for which it is prob-
lematic to isolate the adversary since their connections
will adhere to the predeﬁned limits and appear benign,
in which case (given enough zombie clients) the attacker
can exhaust the hole buffer. Then, when we require a new
buffer for a hole, we must evict an existing buffer (drop-
ping the new packet would result in collateral damage if it
belongs to a legitimate connection).
If we use a deterministic policy to evict the buffer, the
adversary may be able to take this into account in order to
protect its buffers from getting evicted at the time of over-
ﬂow (the inverse of an adversary willfully causing hash
collisions, as discussed in [9]). This leads us to instead
consider a randomized eviction policy. In this policy, we
chose a buffer page at random to evict. We can intuitively
see that if most of the pages are occupied by the adver-
sary, then the chances are high that we evict one of the ad-
versary’s pages. This diminishes the effectiveness of the
attacker’s efforts to exhaust the buffer, as analyzed below.
5.1.1 Eviction and Connection Termination
Eviction raises an important issue: what becomes of the
analysis of the connection whose out-of-sequence pack-
ets we have evicted? If the evicted packet has already
reached the receiver and we have evicted it prior to in-
spection by the byte-stream analyzer (which will gener-
ally be the case), then we have a potential evasion threat:
an adversary can send an attack using out-of-order pack-
ets, cause the device to ﬂush these without ﬁrst analyzing
them, and then later ﬁll the sequence hole so that the re-
ceiver will itself process them.
In order to counter this evasion, it appears we need
to terminate such connections to ensure air-tight security.
With such a policy, benign connections pay a heavy price
for experiencing even a single out-of-order packet when
under attack. We observe, however, that if upon buffer-
ing an out-of-sequence packet we do not also forward the
packet at that time to the receiver, then we do not need to
terminate the connection upon page eviction. By simply
discarding the buffered packet(s) in this case, we force the
sender to retransmit them, which will give us another op-
portunity to reassemble the byte stream and provide it to
the intrusion-prevention analyzer. Thus, we degrade the
collateral damage from the severe case of abnormal con-
nection termination to the milder case of reduced perfor-
mance.
Before making this change, however, we need to re-
USENIX Association
14th USENIX Security Symposium
75
visit the original rationale behind always forwarding out-
of-sequence packets. We made that decision to aid TCP
retransmission: by letting out-of-order packets reach the
end host, the ensuing duplicate-ACKs will trigger TCP’s
“fast retransmission.” However, a key detail is that trig-
gering fast retransmission requires at least 3 duplicate-
ACK packets [3]. Thus, if the receiver receives fewer than
three out-of-order packets, it will not trigger fast retrans-
mission in any case. We can then exploit this observa-
tion by always buffering—without forwarding—the ﬁrst
two out-of-order packets on given TCP stream. When the
third out-of-order packet arrives, we release all three of
them, causing the receiver to send three duplicate-ACKs,
thereby aiding the fast retransmission.2 In the pseudocode
of UpdateConn (Section 4.2), lines 16 and 17 change as
follows:
1. else if (Case (L, N, S)) then
2.
3.
4.
BufferPacket(P, C, Cptr)
if (Case (N, S) and C.PC >= 2) then
if (C.PC == 2) then
5.
6.
# Forward the previously
# unforwarded packets.
Forward(C.Bh - C.Bt)
Forward(P)
With this modiﬁcation, we ensure that if a connection
has fewer than three out-of-order packets in the buffer,
then their eviction does not require us to terminate the
connection. Our trace analysis indicates that such connec-
tions are far-and-away the most common (the 10th row of
Table 1). Hence, this policy protects most connections
even using random page eviction. Thus, our ﬁnal proce-
dure is:
1. EvictPages()
2.
3.
4.
5.
6.
x = random(1, G)
p = ReadPage(x)
C = ReadConnection(p.Conn)
Deallocate(C.bh, C.bt)
if (C.PC > 2) then
7.
8.
9.
else
# Must kill since already
# forwarded packets.
KillConnection(C)
# Update to reflect Deallocate.
WriteConn(C)
5.1.2 Analysis of randomized eviction
How many attempts must an adversary make in order to
evict a benign page? Consider the following parameters.
Let M be the total amount of memory available and g
the page size. Hence, the total number of pages available
is P = M/g, assuming that M is a multiple of g. Let
Ml denote the amount of memory occupied by legitimate
buffers at a given time. Hence, the number of pages of
legitimate buffers, Pl, is simply Pl ≈ Ml/g (the equality
is not exact due to page granularity).
Let T denote the threshold of per-connection buffer in
terms of pages, i.e., a connection can consume no more
than T pages for buffering out-of-sequence data. Let C
denote the number of connections an adversary uses for
their attack. Several cases are possible:
Case 1: C ≤ P−Pl
T
. In this case, the adversary does
not have enough connections at their disposal. We have
sufﬁcient pages available to satisfy the maximum require-