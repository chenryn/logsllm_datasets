chain multiple middleboxes by adding a τi for each link.
3.4 Application-Level Anycast
Multiple endpoints REGISTER different addresses for the same
name; P-Boxes store all address mappings in their local registration
table and choose one to forward name-routed messages to. The
choice can be based on local network policy, or policy speciﬁed
by each endpoint at registration time (e.g. round-robin, primary-
backup and so on). This is in contrast to i3 (where the middle
cannot specify policy), and Oasis [18] (where the policy speciﬁed
by the middle can be sidestepped at ﬂow establishment).
While the approach above works for cases where a single FLOWNE-
GOTIATE can acquire all the tokens needed, a small modiﬁcation is
needed if multiple FLOWNEGOTIATEs are needed to acquire all
tokens as there is no guarantee that subsequent messages will be
name-routed to the same instance. To rectify this lack of afﬁnity,
we add an additional instance component to the name making it
a 4-tuple. The instance ﬁeld uniquely identiﬁes an endpoint in a
group of anycast endpoints. The instance name may be picked by
the application to be user-friendly. For instance, an application may
detect other instances and use the hostname to differentiate itself, or
if appropriate may ask the user to name each instance, e.g. home
and work. REGISTER messages contain the instance as part of the
name. An application can elect to send a FLOWNEGOTIATE to a
speciﬁc instance (set in Edst), or to any instance (instance name of
∗); P-Boxes use the destination instance name to route to a match-
ing endpoint. An endpoint, however, must always include its own
instance name in FLOWNEGOTIATE messages that it generates so
the other endpoint can learn its unicast name.
3.5 Negotiating Multicast
NUTSS can be used to support several forms of multicast. The
basic idea is to use the 4-tuple names that deﬁne a group of end-
points deﬁned in the previous section (3.4) for multicast instead of
anycast by transmitting messages to all members rather than only
one. There are several possible variants, depending on how large
the multicast group is, and whether IP multicast is available. For
instance, for small-scale multicast, endpoints could simply estab-
lish point-to-point ﬂows with all other members, and individually
replicast packets onto all ﬂows. If IP multicast is available, then
similar to SIP, the IP multicast address may signaled through P-
boxes, and members may join that multicast group. Otherwise, the
rendezvous point and group name for an application multicast pro-
tocol ([25, 53], among many others) may be conveyed through P-
boxes, and endpoints can join the appropriate application multicast
group. Finally, P-Boxes and M-Boxes can participate in carving
out IP multicast islands in overlay based approaches [63].
3.6 Default-Off
NUTSS can be used to implement the default-off paradigm [5]
without requiring changes to the public IP routing core; this is ac-
complished by disallowing name-routed messages between all end-
points by default. Endpoints must explicitly enable speciﬁc ﬂows
with ALLOW messages. A common concern is how non Internet-
savvy users make use of this paradigm without the default quickly
regressing to default-on. We believe that the application must ul-
timately involve itself in selecting an appropriate policy. For ex-
ample, remote-desktop-like applications can elect to be default-off,
while BitTorrent like applications can be default-on. Over time,
one could well imagine applications evolving to be slightly more
sophisticated. For example, a given BitTorrent endpoint could use
the name of a given torrent as its instance name (Section 3.4) and
indicate to the P-Box to ﬁlter on that.
3.7 Protocol Negotiation
FLOWNEGOTIATE messages can be used to negotiate the en-
tire protocol stack with involvement from the middle. A general
protocol negotiation mechanism would enhance the evolvability of
the Internet, and could be used to manage multiple network layers
created through virtual routers and switches, for instance as pro-
posed for the GENI infrastructure [19]. In addition to addresses
and tokens, endpoints would advertise supported protocol stacks
including available transports, network layers, security protocols,
tunneling protocols and so on, and how to layer them. For instance,
a web-browser may advertise: 1) HTTP-TCP-IPv4, 2) HTTP-TCP-
IPsec-IPv6, 3) HTTP-TLS-SCTP-IPv4 etc. P-Boxes remove adver-
tised stacks if the network cannot support it (e.g. #3 if the M-Box
does not support SCTP) or if the stack violates network policy (e.g.
#1 if policy requires TLS or IPsec).
3.8 Optimizations
One of the main concerns with NUTSS is the added latency re-
quired for establishing data ﬂows. Here we discuss three optimiza-
tions that may alleviate this concern. First, is to piggyback applica-
tion data on to signaling messages in order to expedite initial data
transfer. With appropriate changes to the networking stack in end-
host OS’s, this piggybacking could conceivably include protocol
handshakes such as HIP, IPsec, TCP, and HTTP, potentially result-
ing in an overall reduction in data exchange latency as compared
with today’s protocol operation.
The second optimization is combining the FLOWNEGOTIATE
and FLOWINIT when the P-Box and M-Box are co-located (likely
for small networks) to initialize the data path sooner. The FLOWINIT
in such cases may contain incomplete information regarding tokens
and the remote address. The P-Box ﬁlls in the token and uses it to
initialize the M-Box ﬂow state. Note that the embedded FLOWINIT
is piggybacked with the FLOWNEGOTIATE along the name-route
so the remote address is not needed for address-routing; however,
if the remote addresses is needed for per-ﬂow state in the M-Box,
the P-Box waits until the FLOWNEGOTIATE in the reverse direction
before initializing the M-Box.
A third optimization couples NUTSS with Self-Certifying Iden-
tiﬁers (SC-ID) in the protocol stack, for instance HIP [35], in or-
der to eliminate the need for additional signaling during IP-path
changing events like mobility or middlebox failover. The idea is to
include the SC-ID in FLOWINIT messages, and to transmit multiple
FLOWINIT messages in parallel to M-boxes. In this way, failover
M-boxes (for instance) will have pre-authorized the SC-ID, and can
forward data packets immediately upon receiving them.
Indeed,
this approach can be used to establish multiple parallel data ﬂows
through the network, for instance to increase throughput.
4.
IMPLEMENTATION
To test the feasibility of NUTSS, we implemented a library that
adds NUTSS support to endpoints, and implemented an M-Box
used for legacy NAT traversal that we deployed on Planetlab. While
the implementation did not uncover any unexpected issues, it did
help us iron out the design. Using off-the-shelf software and ex-
isting infrastructure, our implementation enables end-middle-end
communication (including name-based connection establishment
in applications, legacy NAT traversal, mobility, default-off behav-
ior and application-level anycast) in many cases requiring little to
no modiﬁcations to existing applications.
Our implementation uses SIP [44] for name-routing. While other
name-routed signaling protocols (e.g. Jabber/XMPP [47]) may be
used, we chose SIP because of its maturity and support in commer-
cial hardware. At the same time, our choice allows us to assess
what subset of SIP is most important for NUTSS.
P-Boxes (SER) and Access Control (CPL): We chose to base
name-routed components on off-the-shelf commercial software in
order to facilitate the second phase of deployment (upgrading net-
works with support for name-routing). P-Boxes in our implementa-
tion are (as yet) unmodiﬁed SIP Express Router (SER) [17] proxies.
NUTSS name: (user, domain, service, instance)
SIP URI encoding: user@domain;srv=service;uuid=instance
Socket API
nbind
nsetpolicy
nconnect
naccept
nsend/nrecv
nclose
NUTSS Primitive
REGISTER
ALLOW/DISALLOW
FLOWNEGOTIATE
FLOWNEGOTIATE
FLOWINIT (one-time)
-
SIP Counterpart
REGISTER
re-REGISTER (w/ CPL)
INVITE
200 OK
-
BYE
Table 3: Mapping from socket operations to NUTSS primitives, and
NUTSS primitives to SIP messages used.
Policy deﬁnitions (for ALLOW/DISALLOW messages and domain
policy) are compiled (manually, at present, using CPLEd [16]) into
the Call Processing Language (CPL) [29], a declarative language
used for user-speciﬁed VoIP policy and call routing.
Name-routed messages in NUTSS are encoded as SIP messages
(Table 3 lists the mapping). Source and destination endpoint names
are encoded in SIP header ﬁelds (From:, To:), and advertised ad-
dresses and tokens are encoded in the body of the SIP message. The
messages are (optionally) signed using S/MIME certiﬁcates [41].
Address-routed messages are normal TCP/IP messages; the library
inserts the FLOWINIT message into the 5-tuple data ﬂow in front of
application data.
M-Boxes: While our implementation supports legacy NATs, we
implemented a NUTSS-aware M-Box that performs TURN-like [43]
relaying of application data to assist endpoints behind particularly
poorly behaved NATs [21] to communicate through them. To allow
for an unmodiﬁed SER proxy, our M-Box includes a shim P-Box
that generates tokens for the M-Box; the SER proxy coupled with
our shim in series perform the coupling between name-routing and
address-routing. The token itself is a 32-bit nonce, one copy of
which is sent to the endpoint and another exchanged in-memory
between the shim P-Box and M-Box that is used for validating the
impending data path.
Endpoints: Endpoint support is implemented as a userspace li-
brary for Linux and Windows applications. The library consists of
roughly 10K lines of C code and relies on a number of external li-
braries including eXosip2 [3] for SIP support and OpenSSL [39]
for data security. Our library has two interfaces. The ﬁrst in-
terface offers NUTSS equivalents of the socket API including an
AF NUTSS socket address family, and a sockaddr ns structure
that encodes the user, domain and application, and optionally, the
instance name, as strings. In order to use this interface, applica-
tions must be modiﬁed accordingly; however, as the API is similar
to BSD sockets only minor modiﬁcations are needed—mostly con-
ﬁned to populating the address structure with names instead of IP
addresses and ports.
The second interface to our library accommodates unmodiﬁed
existing application binaries. This interface is available only for
Linux applications. The library is pre-loaded into memory by the
Linux dynamic loader’s LD PRELOAD functionality. This allows
our library to transparently hijack libc socket calls in the applica-
tion and redirect them to NUTSS equivalents. The local endpoint
name is conﬁgured into environment variables by the user. The user
also enters specially encoded hostnames into the legacy applica-
tion. When the legacy application performs a gethostbyname
call for the encoded hostname, the call is intercepted by our li-
brary, which decodes the NUTSS name and creates a mapping be-
tween the identiﬁer and a fake IP address returned to the applica-
tion. When the application later initiates a connect to the fake
IP address, the library intercepts and initiates an nconnect to the
associated name. Calls to other legacy BSD socket functions are
handled similarly.
In order to encourage adoption, the NUTSS library transparently
performs NAT traversal. After exchanging addresses and ports over
name-routed signaling if the direct TCP connections (in both direc-
tions), and TCP hole-punching [21] fail, endpoints negotiate the
use of a public relay (the TURN-like M-Box described earlier). M-
Boxes are deployed on Planetlab hosts. The associated P-Box can
be contacted through sip.nutss.net, which routes to the shim P-
Box in a randomly selected M-Box. Endpoints acquire a token and
transport address for the M-Box. Both endpoints connect to the M-
Box and initialize the ﬂow by sending a FLOWINIT message over
the connection; the M-Box veriﬁes the token and uses it to pair up
the two connections.
As the M-Boxes in our Planetlab deployment do not lie on the
IP data path between endpoints, we have not gathered sufﬁcient
experience with the referral mechanism.
We have successfully run a number of applications using both the
legacy and non-legacy interfaces to our library while transparently
incorporating endpoint policy, authentication, legacy NAT traversal
and endpoint mobility. Our library works with client-server appli-
cations written to our API, as well as with many unmodiﬁed legacy
applications (e.g. iperf, VNC, GNOME GUI desktop). The library
is available for public download at nutss.net.
4.1 Findings
SIP Lessons Learned: We were surprised to ﬁnd that although
SIP was originally conceived for a very different purpose, it can be
used to implement name-routing in NUTSS (with one minor mod-
iﬁcation). Admittedly SIP is rather heavy-weight for the purpose
and we would prefer to use a leaner protocol. Nevertheless, given
that SIP is deployed widely today and enjoys signiﬁcant mindshare,
there is a compelling case to be made for using a subset of it.
One aspect of SIP that requires special workarounds in our im-
plementation is the lack of support for nested domains. A single
REGISTER message only creates a single registration at the local
P-Box and not the chain of registrations in the P-Boxes in front of
the endpoint as required by NUTSS. While this limitation is not a
concern in the ﬁrst phase of deployment where a public P-Box is
used, in the second phase it affects networks not connected directly
to the core. A temporary brute-force workaround is for endpoints
to explicitly create registrations for each link of the chain; however,
this is not always possible due to ﬁrewall policy. A more perma-
nent solution is to modify SIP with support for nested domains, and
accordingly modify our SER proxy to forward the registrations to
the parent P-Box.
Latency: Since ours is a proof-of-concept implementation of
the NUTSS architecture, performance numbers are of little rele-
vance as they relate only to our (perhaps simple) access control pol-
icy. Nevertheless, some brief comments on performance are worth
making. We found that there is little added latency in establishing
connections (less than 15ms) with P-Boxes deployed on the same
network segment as the endpoints. This is because signaling in our
particular setting added one name-routed round-trip (FLOWNEGO-
TIATE) and one address-routed round-trip (FLOWINIT). Quite pre-
dictably, when two nearby Planetlab nodes on the west coast use
our public P-Box service deployed at Cornell, the connection es-
tablishment latency shoots up to 100–200ms due to name-routed
messages having to make four coast-to-coast trips before the direct
data can ﬂow. The optimization suggested in Section 3.8 where
data is piggybacked in signaling messages should allow initial data
bytes to be exchanged in half that time while the address-routed
path is established in the background. Our P-Box (SER proxy)
can route approximately 1200 name-routed messages per second on