larger, the time series was shorter for the same video length.
Therefore, less noise was added, which resulted in smaller
waste.
As shown in Fig. 10a, however, the deficit metric of
the FPAk and d∗ mechanisms followed a different trend. In
FPAk, deficit was less ﬂuctuated when w and  changed
(Fig. 10a). For different (w,) pairs, the average deficit
stayed within [1.5MB, 3MB]. However, for d∗, changes in ei-
ther w or  affected the deficit signiﬁcantly. From Fig. 10b,
it was clear that when the w was small (e.g., 0.05s), there was
no deficit at all for all  values; when the w was large
(e.g., 2s), the deficit could be as large as 0.8MB.
Note that it is possible to take measures to lower the
waste and deficit. For example, one can choose an 
value to ensure privacy while keeping a reasonable waste
and deficit. The clip bounds can also be adjusted to limit
the maximum/minimum download rate for each bin while
maintaining differential privacy. Lowering the upper bound
can reduce the waste, while increasing the lower bound
can remove the deficit. Also, the deficit can be easily
eliminated if buffering the video content upfront for a few
seconds.
C. FPAk vs. d∗
To compare FPAk and d∗, we chose the best parameters
for each method, with the baseline accuracy (i.e., 2.5%) and
lowest waste. For FPAk, the best parameters were w = 2s, 
= 0.5; for d∗, w = 0.5s,  = 5×10−6 were chosen. The waste
and deficit distribution of the 4000 traces after applying
the two methods are shown in Fig. 11. From Fig. 11a, it is
clear that with the best parameters, FPAk traces had a median
waste of about 200%, while that of d∗ traces was even higher
(600%). For deficit (Fig. 11b), however, d∗ performed a
lot better. More than 80% of d∗ traces had a deficit less
than 1%, while the majority of FPAk traces (> 50%) had at
least 5% deficit.
From Fig. 11, we ﬁnd that FPAk tended to induce less
waste (about 200% of the original video size). To achieve
similar security protection, d∗ had to download 3 times more
volume. To achieve differential privacy, some utility loss has
to be allowed in either case. Moreover, it is clear that with
the same security level (in regards to classiﬁcation accuracy),
if the primary objective is minimize the waste, FPAk is the
best choice; if the main goal is to reduce the deficit, d∗
would be the better option.
D. Comparison with Baseline Defense
In a baseline defense mechanism, the defender could sim-
ply download at a constant rate for all videos in the dataset.
To make videos with different total data size indistinguish-
able, smaller videos need to be padded with dummy data
to obfuscate the trafﬁc analysis. We designed the following
(a) FPAk: w = 0.05s
(b) FPAk: w = 2s
(c) d∗: w = 0.05s
(d) d∗: w = 2s
Fig. 7: wA (cid:54)= w
Fig. 8: An example of waste and deficit.
original trace A.
{max(B[i] − A[i], 0)}
waste = max
1≤i≤n
(4)
• We deﬁne deficit as the maximum difference between
A and B when the noised trace B is below the original
trace A.
deficit = max
1≤i≤n
{max(A[i] − B[i], 0)}
(5)
waste means the maximum amount of data that have been
downloaded in advance during a time period, and deficit
means the maximum amount of data that needs to be down-
loaded to keep streaming during a time period. An example
of waste and deficit is illustrated in Fig. 8. The red line
represents the cumulative volume of the original trace A, and
the blue line is that of the noised trace B. The orange area
means that the noised trace is behind the original one, while
the blue area means that the noised trace is ahead of it. The
deficit is the max difference between the two lines in the
orange area, while the waste is that in the blue area.
The utility of the two mechanisms was evaluated using
the same set of w and  values as in Fig. 5. The waste
and deficit of each noised trace were computed ﬁrst,
and the average waste and deficit over all traces were
calculated and shown in Fig. 9 and Fig. 10. According to
Fig. 9a, parameter w did not affect the waste of FPAk much.
9
0.050.250.512wA(s)0.00.20.40.60.81.0Accuracyǫ=0.05ǫ=0.5ǫ=5ǫ=500.050.250.512wA(s)0.00.20.40.60.81.0Accuracyǫ=0.05ǫ=0.5ǫ=5ǫ=500.050.250.512wA(s)0.00.20.40.60.81.0Accuracyǫ=5e-8ǫ=5e-7ǫ=5e-6ǫ=5e-50.050.250.512wA(s)0.00.20.40.60.81.0Accuracyǫ=5e-8ǫ=5e-7ǫ=5e-6ǫ=5e-5012345678Time(s)0.00.51.01.52.02.5Volume (Bytes)1e6WasteDeficitoriginalnoised(a) FPAk
(a) FPAk
Fig. 9: waste experiment.
Fig. 10: deficit experiment.
(b) d∗
(b) d∗
mechanism to avoid introducing deficit in the resulting
streams: With a bin size of w, we divided each time series
into multiple bins, and identiﬁed the maximum value of
downloaded data (denoted as C) for all bins of these 4000
original time series. Then as a baseline defense mechanism,
all videos were downloaded at a constant rate of C bytes per
w. As such, from the attacker’s perspective, all video streams
were identical, and no deficit would be incurred for the
noised video. We evaluated this baseline method with w =
[0.05s, 0.25s, 0.5s, 1s, 2s], and the corresponding waste are
[15.7GB, 14.9GB, 11.5GB, 8.1GB, 4.1GB], which represent
the extra data downloaded for a 3-minute video.
it
We note that
is only fair to compare this baseline
approach with FPAk, because both of them require knowledge
of the download proﬁles of all videos in a dataset (i.e., the set
of videos the defender would like to render indistinguishable).
By contrast, d∗ can be used to add noise on-the-ﬂy. As
shown in Fig. 9a, the waste induced by FPAk is at least
one order of magnitude lower than the baseline approach.
With a tunable privacy level , i.e., by enforcing statistical
indistinguishability rather than absolute indistinguishability,
FPAk can be much more practical (e.g., with less than 10MB
waste when  = 5).
E. An Optimal Attacker
In the previous subsections, we have evaluated the two
differentially private mechanisms with an adversary that could
both train and test with noised data. Now we consider a
even more powerful attacker who is not only able to train
his classiﬁer with noised data, but also has the knowledge
of distribution of both clean data and noised data (but not
the mapping between the two). With such knowledge, the
adversary could ﬁrst try to remove the noise from the noised
time series, and then perform classiﬁcation (e.g., CNN) to
classify the resulting time series. The method to do so is
proposed by Naldi et al. [43]. Essentially,
it requires the
attacker to estimate the clean time series by calculating the
conditional expectation of each clean data point conditioned
on the observed, noised data points.
To evaluate the effectiveness of
this optimal attacker
method, we ﬁrst calculated the estimated clean dataset from
the noised dataset, and then chose 80% of the estimated clean
dataset to train the classiﬁer and the rest 20% as the test
set to perform classiﬁcation. The classiﬁcation accuracy is
shown in Table II. Compared with Fig. 5, it appears that the
optimal attacker performs slightly better in some cases, but
the improvement of the accuracy is no more than 2%. The
experiments suggest that even with knowledge of distribution
of both clean data and noised data, an optimal attacker cannot
signiﬁcantly improve his attack.
aaaaa
w (s)

0.05
0.25
0.5
1
2
FPAk
d∗
0.05
0.03
0.03
0.03
0.03
0.03
0.5
0.03
0.03
0.03
0.03
0.03
5
0.25
0.30
0.27
0.27
0.17
50
0.89
0.89
0.87
0.80
0.65
5e-8
0.03
0.03
0.02
0.02
0.03
5e-7
0.03
0.03
0.02
0.02
0.03
5e-6
0.03
0.03
0.03
0.11
0.10
5e-5
0.72
0.89
0.86
0.89
0.75
TABLE II: Classiﬁcation results of the optimal attacker.
VII. REAL-WORLD IMPLEMENTATION
To demonstrate the practicality of our approach, we imple-
mented the FPAk privacy mechanism in a Chrome extension
10
0.050.250.512w (s)100101102103104105Waste (MB)baselineǫ=0.05ǫ=0.5ǫ=5ǫ=500.050.250.512w (s)100101102103104105106Waste (MB)ǫ=5e-8ǫ=5e-7ǫ=5e-6ǫ=5e-50.050.250.512w (s)0.00.51.01.52.02.53.03.54.0Deficit (MB)ǫ=0.05ǫ=0.5ǫ=5ǫ=500.050.250.512w (s)0.00.20.40.60.81.0Deficit (MB)ǫ=5e-8ǫ=5e-7ǫ=5e-6ǫ=5e-5(a) waste
(b) deficit
Fig. 11: FPAk (w = 2s,  = 0.5) vs. d∗ (w = 0.5s,  = 5e-6)
Fig. 12: Workﬂow of the Chrome extension.
that proxies Youtube streaming. The workﬂow of the extension
is illustrated in Fig. 12. First, the Youtube client running inside
the Chrome browser sends a request to the Youtube server,
which is intercepted by the extension. Instead of relaying the
request immediately, the proxy sends requests on behalf of
the client at a constant rate (e.g., once per second), which is
speciﬁed by the w parameter of the extension. After receiving
the responses from the server, the proxy caches the video
chunks locally. If there is a pending request from the Youtube
client,
the extension returns the requested portion to the
client directly from local storage. In this way, the Youtube
requests/responses as seen by an external observer are fully
controlled by the extension. Since the request pattern from the
proxy is differentially private, trafﬁc analysis is thwarted.
The size of the video chunks to be requested is speciﬁed as
a parameter (i.e., range) of the HTTP request header. Youtube
video streaming implements a variant of MPEG-DASH [40],
which allows the client to specify a chunk of video to be
downloaded by setting the range parameter to the desired
offsets in bytes. The YouTube client adaptively changes this
parameter to adjust the requested video chunk size, based on
the content of the video and the network condition. To enforce
the privacy guarantee, the range parameters in the proxy’s
requests are decoupled from those in the client’s requests.
The requests sent by the Chrome extension use a range
parameter dictated by the FPAk mechanism. To properly
watch a Youtube video, both its video stream and its audio
stream needs to be downloaded. We applied the differentially
private mechanism on both streams.
Implementation. In our implementation, we made use of the
Xhook [25] framework, which allows us to intercept and
modify the XMLHttpRequest requests and responses. In our
implementation of FPAk, k = 10, w = 1s,  = 0.5. We used
the numjs [44] library, which is similar to Python’s numpy, to
implement numeric computation, and used the Random library
in SIM.JS [42] to implement the Laplace distribution. The
extension has about 700 lines of Javascript code in total. Note
that the use of FPAk requires the original trace of the video
to be known to the proxy beforehand.
11
Data collection. We used the same methods described in
Sec. III to collect traces for 10 videos, and 100 traces for
each video, with our extension enabled. Therefore, the net-
work trafﬁc observed is only the communication between the
extension and the Youtube server. The traces were collected
when the w parameter of the extension was set to 1s, which
means that it would send a video request and an audio request
to the Youtube server every 1 second.
Effectiveness. To demonstrate that the extension can indeed
defeat ML-based trafﬁc analysis, we extracted 12 features
which were also time series from the stream and performed
classiﬁcation one by one. The features were: the number bytes
per bin (BPB up, BPB down, BPB), the number of packets per
bin (PPB up, PPB down, PPB), the average packet length per
bin (LPB up, LPB down, LPB), the size of bursts2 (BURST up,
BURST down, BURST ). The subscription “up” means packets
from client to server; “down” means packets from server to
client; no subscription means the sum of “up” and “down”. We
also evaluated the classiﬁcation accuracy with all 12 features
combined, labeled as ALL.
The dataset (1000 traces) was split
into a training set
(80%, 800 traces) and a test set (20%, 200 traces). We set
wA = {0.05s, 0.25s, 0.5s, 1s, 2s} to bin the traces,
then
trained the CNN model in Sec. III for 40 epochs with a batch
size of 32 using the training set. After that, the classiﬁcation
was performed on the test set. The results are shown in
Table III. As expected, the CNN model can hardly classify
these obfuscated traces. For most cases, the classiﬁer only
achieved an accuracy of about 15%. Using certain features may
increase the classiﬁcation accuracy (e.g., 23% with BURST up
for wA = 0.25s), which were still signiﬁcantly lower than the
values in Table I. This result suggests that differential privacy
is effective in defeating machine learning adversaries. Accord-
ing to the Post-processing Lemma [18], the composition of
differentially private mechanisms is still differentially private.
Therefore, combining the features does not beneﬁt the attacker.
As shown in the “ALL” column in Table III, the 12-feature
combined classiﬁcation accuracy remained on the same level
as individual features.