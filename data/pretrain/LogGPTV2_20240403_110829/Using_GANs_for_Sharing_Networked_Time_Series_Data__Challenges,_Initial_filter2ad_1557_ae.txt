want to predict whether an event E occurs in the future, or even
473
EVICTFAILFINISHKILL01000020000RealDoppelGANgerEVICTFAILFINISHKILL01000020000RealNaive GAN103104#Training samples0.00000.00250.00500.0075Mean square errorUsing GANs for Sharing Networked Time Series Data: Challenges, Initial Promise, and Open Questions
IMC ’20, October 27–29, 2020, Virtual Event, USA
Method
Naive GAN
Market Simulator
HMM
RNN
RCGAN
AR
TimeGAN
DoppelGANger
Average training time (hrs)
5
6
8
22
29
93
258
17
Table 4: Average training time (hours) of synthetic data mod-
els on the WWT dataset. All models are trained with 50000
training samples.
Figure 12: Predictive modeling setup: Using training data A,
we generate samples B ∪ B′. Subsequent experiments train
downstream tasks on A or B, our training sets, and then test
on A′ or B′.
forecast the time series itself. For example, in GCUT dataset, we
could predict whether a particular job will complete successfully.
In this use case, we want to show that models trained on generated
data generalize to real data.
We first partition our dataset, as shown in Figure 12. We split real
data into two sets of equal size: a training set A and a test set A’. We
then train a generative model (e.g., DG or a baseline) on training set
A. We generate datasets B and B’ for training and testing. Finally,
we evaluate event prediction algorithms by training a predictor on
A and/or B, and testing on A’ and/or B’. This allows us to compare
the generalization abilities of the prediction algorithms both within
a class of data (real/generated), and generalization across classes
(train on generated, test on real) [35].
We first predict the task end event type on GCUT data (e.g.,
EVICT, KILL) from time series observations. Such a predictor may
be useful for cluster resource allocators. This prediction task reflects
the correlation between the time series and underlying metadata
(namely, end event type). For the predictor, we trained various al-
gorithms to demonstrate the generality of our results: multilayer
perceptron (MLP), Naive Bayes, logistic regression, decision trees,
and a linear SVM. Figure 13 shows the test accuracy of each predic-
tor when trained on generated data and tested on real. Real data
expectedly has the highest test accuracy. However, we find that
DG performs better than other baselines for all five classifiers. For
instance, on the MLP predictive model, DG-generated data has 43%
higher accuracy than the next-best baseline (AR), and 80% of the
real data accuracy. The results on other datasets are similar. (Not
shown for brevity; but in Appendix D for completeness.)
Algorithm comparison: We evaluate whether algorithm rank-
ings are preserved on generated data on the GCUT dataset by
training different classifiers (MLP, SVM, Naive Bayes, decision tree,
and logistic regression) to do end event type classification. We also
y
c
a
r
u
c
c
A
Figure 13: Event-type prediction accuracy on GCUT.
DoppelGANger AR RNN HMM Naive GAN
GCUT
WWT
1.00
0.80
1.00 1.00
0.80 0.20
0.01
-0.60
0.90
-0.60
Table 5: Rank correlation of predication algorithms on
GCUT and WWT dataset. Higher is better.
evaluate this on the WWT dataset by training different regression
models (MLP, linear regression, and Kernel regression) to do time
series forecasting (details in Appendix D). For this use case, users
have only generated data, so we want the ordering (accuracy) of
algorithms on real data to be preserved when we train and test
them on generated data. In other words, for each class of generated
data, we train each of the predictive models on B and test on B′.
This is different from Figure 13, where we trained on generated
data (B) and tested on real data (A′). We compare this ranking
with the ground truth ranking, in which the predictive models are
trained on A and tested on A′. We then compute the Spearman’s
rank correlation coefficient [100], which compares how the rank-
ing in generated data is correlated with the groundtruth ranking.
Table 5 shows that DG and AR achieve the best rank correlations.
This result is misleading because AR models exhibit minimal ran-
domness, so all predictors achieve the same high accuracy; the AR
model achieves near-perfect rank correlation despite producing
low-quality samples; this highlights the importance of considering
rank correlation together with other fidelity metrics. More results
(e.g., exact prediction numbers) are in Appendix D.
5.3 Other case studies
DG is being evaluated by several independent users, though DG
has not yet been used to release any datasets to the best of our
knowledge. A large public cloud provider (IBM) has internally val-
idated the fidelity of DG. IBM stores time series data of resource
usage measurements for different containers used in the cloud’s
machine learning service. They trained DG to generate resource
usage measurements, with the container image name as metadata.
Figure 14 shows the learned distribution of containers’ maximum
CPU usage (in percent). We show the maximum because developers
usually size containers according to their peak usage. DG captures
this challenging distribution very well, even in the heavy tail. Ad-
ditionally, other users outside of the networking/systems domain
such as banking, sensor data, and natural resource modeling have
also been using DG [26, 96].
474
Real DataGenerated DataAA’BB’Generative Modele.g.GAN50%50%50%50%MLPNaiveBayesLogisticRegressionDecisionTreeLinearSVM0.00.20.40.60.8RealDoppelGANgerRNNARHMMNaive GANIMC ’20, October 27–29, 2020, Virtual Event, USA
Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, and Vyas Sekar
Figure 15: Membership inference attack against DG in WWT
dataset vs. training set size.
example, recent work has shown that generative models may mem-
orize a specific user’s information from the training data (e.g. a
social security number) and leak it during data generation [20].
Hence, we need methods for evaluating and protecting user pri-
vacy. One challenge is the difficulty of defining what it means to be
privacy-preserving. Although there are many competing privacy
definitions [33, 93, 106], a common theme is deniability: released
data or models should look similar whether a given user’s data is
included or not.
In this section, we show how two of the most common notions
of deniability relate to GANs.
Differential privacy: A leading metric for measuring user privacy
today is differential privacy (DP) [33]. In the context of machine
learning, DP states that the trained model should not depend too
much on any individual user’s data. More precisely, a model M
is (ϵ, δ) differentially private if for any pair of training datasets D
and D′ that differ in the record of a single user, and for any input
z, it holds that
M(z; D) ≤ eϵM(z; D′) + δ,
where M(z; D) denotes a model trained on D and evaluated on z.
Smaller values of ϵ and δ give more privacy.
Recent work has explored how to make deep learning models
differentially private [3] by clipping and adding noise to the gradi-
ent updates in stochastic gradient descent to ensure that any single
example in the training dataset does not disproportionately influ-
ence the model parameters. Researchers have applied this technique
to GANs [40, 112, 113] to generate privacy-preserving time series
[14, 35]; we generally call this approach DP-GAN. These papers
argue that such DP-GAN architectures give privacy at minimal cost
to utility. A successive paper using DP student-teacher learning
achieved better performance than DP-GAN on single-shot data [63],
but we do not consider it here because its architecture is ill-suited
to generating time series.
To evaluate the efficacy of DP-GANs in our context, we trained
DoppelGANger with DP-GANs for the WWT dataset using Ten-
sorFlow Privacy [4].10 Figure 16 shows the autocorrelation of the
resulting time series for different values of the privacy budget, ϵ.
Smaller values of ϵ denote more privacy; ϵ ≈ 1 is typically con-
sidered a reasonable operating point. As ϵ is reduced (stronger
privacy guarantees), autocorrelations become progressively worse.
In this figure, we show results only for the 19th epoch, as the re-
sults become only worse as training proceeds (not shown). These
results highlight an important point: although DP-GANs seems
to destroy our autocorrelation plots, this was not always evident
10We were unable to implement DP-TimeGAN for comparison, as their code uses loss
functions that are not supported by Tensorflow Privacy [116].
Figure 14: Maximum CPU usage.
6 PRIVACY ANALYSIS AND TRADEOFFS
Data holders’ privacy concerns often fit in two categories: protect-
ing business secrets (§6.1) and protecting user privacy (§6.2). In
this section, we illustrate what GANs can and cannot do in each of
these topics.
6.1 Protecting business secrets
In our discussions with major data holders, a primary concern about
data sharing is leaking information about the types of resources
available and in use at the enterprise. Many such business secrets
tend to be embedded in the metadata (e.g., hardware types in a
compute cluster). Note that in this case the correlation between
hardware type and other metadata/measurements might be impor-
tant for downstream applications, so data holders cannot simply
discard hardware type from data.
There are two ways to change or obfuscate the metadata dis-
tribution. The naive way is to rejection sample the metadata to a
different desired distribution. This approach is clearly inefficient.
The architecture in §4.3 naturally allows data holders to obfuscate
the metadata distribution in a much simpler way. After training on
the original dataset, the data holders can retrain only the metadata
generator to any desired distribution as the metadata generator and
measurement generator are isolated. Doing so requires synthetic
metadata of the desired distribution, but it does not require new
time series data.
A major open question is how to realistically tune attribute
distributions. Both of the above approaches to obfuscating attribute
distributions keep the conditional distribution P(Ri|Ai) unchanged,
even as we alter the marginal distribution P(Ai). While this may be
a reasonable approximation for small perturbations, changing the
marginal P(Ai) may affect the conditional distribution in general.
For example, Ai may represent the fraction of large jobs in a system,
and Ri the memory usage over time; if we increase Ai, at some point
we should encounter resource exhaustion. However, since the GAN
is trained only on input data, it cannot predict such effects. Learning
to train GANs with simulators that can model physical constraints
of systems may be a good way to combine the statistical learning
properties of GANs with systems that encode domain knowledge.
6.2 Protecting user privacy
User privacy is a major concern with regards to any data sharing
application, and generative models pose unique challenges. For
475
01000020000300004000050000#Training samples0.60.81.0Attack success rateUsing GANs for Sharing Networked Time Series Data: Challenges, Initial Promise, and Open Questions
IMC ’20, October 27–29, 2020, Virtual Event, USA
Figure 16: Autocorrelation for real, ϵ = + inf, and DP-GANs with different values of ϵ).
from downstream metrics, such as predictive accuracy in [35]. This
highlights the need to evaluate generative time series models quali-
tatively and quantitatively; prior work has focused mainly on the
latter [14, 35, 63]. Our results suggest that DP-GAN mechanisms
require significant improvements for privacy-preserving time series
generation.
Membership Inference: Another common way of evaluating
user deniability is through membership inference attacks [22, 53, 94].
Given a trained machine learning model and set of data samples,
the goal of such an attack is to infer whether those samples were in
the training dataset. The attacker does this by training a classifier to
output whether each sample was in the training data. Note that dif-
ferential privacy should protect against such an attack; the stronger
the privacy parameter, the lower the success rate. However, DP
alone does quantify the efficacy of membership inference attacks.
To understand this question further, we measure DG’s vulnera-
bility to membership inference attacks [53] on the WWT dataset.
As in [53], our metric is success rate, or the percentage of successful
trials in guessing whether a sample is in the training dataset. Naive
random guessing gives 50%, while we found an attack success rate
of 51%, suggesting robustness to membership inference in this case.
However, when we decrease the training set size, the attack success
rate increases (Figure 15). For instance, with 200 training samples,
the attack success rate is as high as 99.5%.Our results suggest a
practical guideline: to be more robust against membership attacks,
use more training data. This contradicts the common practice of
subsetting for better privacy [91].
Summary and Implications: The privacy properties of GANs
appear to be mixed. On the positive side, GANs can be used to obfus-
cate attribute distributions for masking business secrets, and there
appear to be simple techniques for preventing common membership
inference attacks (namely, training on more data). However, the
primary challenge appears to be preserving good fidelity while pro-
viding strong theoretical privacy guarantees. We find that existing
approaches for providing DP destroy fidelity beyond recognition,
and are not a viable solution.
More broadly, we believe there is value to exploring different
privacy metrics. DP is designed for datasets where one sample
corresponds to one user. However, many datasets of interest have
multiple time series from the same user, and/or time series may
not correspond to users at all. In these cases, DP gives hard-to-
interpret guarantees, while destroying data fidelity (particularly for
uncommon signal classes [9]). Moreover, it does not defend against
other attacks like model inversion [38, 57]. So it is not clear whether
DP is a good metric for this class of data.
7 CONCLUSIONS
While DG is a promising general workflow for data sharing, the pri-
vacy properties of GANs require further research for data holders
to confidently use such workflows. Moreover, many networking
datasets require significantly more complexity than DG is currently
able to handle, such as causal interactions between stateful agents.
Another direction of interest is to enable “what-if" analysis, in
which practitioners can model changes in the underlying system
and generate associated data. Although DG makes some what-
if analysis easy (e.g., slightly altering the attribute distribution),
larger changes may alter the physical system model such that the
conditional distributions learned by DG are invalid (e.g., imagine
simulating a high-traffic regime with a model trained only on low-
traffic-regime data). Such what-if analysis is likely to require physi-
cal system modeling/simulation, while GANs may be able to help
model individual agent behavior. We hope that the initial promise