(b) Min_Diff
the training set with adversarial examples. Human checks cannot
recognize such adversarial examples as poisonous but they can
change model decision boundary and result in performance degra-
dation. The work in [19, 34] trojans hardware that neural networks
are running on. It injects back-door by tampering with the circuits.
Many defense techniques have been proposed to defend against
training data poisoning [16, 27, 44, 48]. Most fall into the category
of data sanitization where they prune out the poisonous data during
training. In contrast, ABS provides defense at a different stage of
model life cycle (after they are trained).
ABS is also related to adversarial sample attacks (e.g., [13, 21,
45, 46, 52, 56, 62, 64]). Some recent work tries to construct input-
agnostic universal perturbations [43] or universal adversarial patch [15].
These works have a nature similar to Neural Cleanse [59], which
we have done thorough comparison with. The difference lies in
that ABS is an analytic approach and leverages model internals. A
number of defense techniques have been proposed for adversarial
examples. Some [18, 40, 57, 63] detect whether an input is an ad-
versarial example and could be potentially used for detecting pixel
space trojan triggers. However, these approaches detect inputs with
trojan trigger instead of scanning models.
8 CONCLUSIONS
We develop a novel analytic approach to scan neural network mod-
els for potential back-doors. It features a stimulation analysis that
determines how providing different level of stimulus to an inner
neuron impacts model output activation. The analysis is leveraged
to identify neurons compromised by trojan attack. Our experiments
show that the technique substantially out-performs the state-of-
the-art and has a very high detection rate for trojaned models.
ACKNOWLEDGMENTS
We thank the anonymous reviewers for their constructive com-
ments. This research was supported, in part by DARPA FA8650-15-
C-7562, NSF 1748764, 1901242 and 1910300, ONR N000141410468
and N000141712947, and Sandia National Lab under award 1701331.
Any opinions, findings, and conclusions in this paper are those of
the authors only and do not necessarily reflect the views of our
sponsors.
REFERENCES
[1] [n.d.]. BigML Machine Learning Repository. https://bigml.com/.
[2] [n.d.]. Caffe Model Zoo. https://github.com/BVLC/caffe/wiki/Model-Zoo.
[3] [n.d.]. Gradientzoo: pre-trained neural network models. https://www.gradientzoo.
[4] 2019. BigML.com. https://bigml.com
[5] 2019. bolunwang/backdoor. https://github.com/bolunwang/backdoor
[6] 2019. BVLC/caffe. https://github.com/BVLC/caffe/wiki/Model-Zoo
[7] 2019. DARPA Announces $2 Billion Campaign to Develop Next Wave of AI
com/.
Technologies. https://www.darpa.mil/news-events/2018-09-07
[8] 2019. Executive Order on Maintaining American Leadership in Artificial Intel-
https://www.whitehouse.gov/presidential-actions/executive-order-
ligence.
maintaining-american-leadership-artificial-intelligence
[9] 2019. GitHub - BIGBALLON/cifar-10-cnn: Play deep learning with CIFAR datasets.
https://github.com/BIGBALLON/cifar-10-cnn.
[20] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. 2009. ImageNet: A
[10] 2019. onnx/models. https://github.com/onnx/models
[11] 2019. Tensorpack - Models. http://models.tensorpack.com/.
[12] acoomans. 2013. . https://github.com/acoomans/instagram-filters
[13] Anish Athalye, Nicholas Carlini, and David Wagner. 2018. Obfuscated gradients
give a false sense of security: Circumventing defenses to adversarial examples.
arXiv preprint arXiv:1802.00420 (2018).
[14] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat
Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai
Zhang, et al. 2016. End to end learning for self-driving cars. arXiv preprint
arXiv:1604.07316 (2016).
[15] Tom B Brown, Dandelion Mané, Aurko Roy, Martín Abadi, and Justin Gilmer.
2017. Adversarial patch. arXiv preprint arXiv:1712.09665 (2017).
[16] Yinzhi Cao, Alexander Fangxiao Yu, Andrew Aday, Eric Stahl, Jon Merwine, and
Junfeng Yang. 2018. Efficient repair of polluted machine learning systems via
causal unlearning. In Proceedings of the 2018 on Asia Conference on Computer and
Communications Security. ACM, 735–747.
[17] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. 2017. Targeted
backdoor attacks on deep learning systems using data poisoning. arXiv preprint
arXiv:1712.05526 (2017).
[18] Edward Chou, Florian Tramèr, Giancarlo Pellegrino, and Dan Boneh. 2018. Sen-
tiNet: Detecting Physical Attacks Against Deep Learning Systems. arXiv preprint
arXiv:1812.00292 (2018).
[19] Joseph Clements and Yingjie Lao. 2018. Hardware trojan attacks on neural
networks. arXiv preprint arXiv:1806.05768 (2018).
Large-Scale Hierarchical Image Database. In CVPR09.
[21] Minghong Fang, Guolei Yang, Neil Zhenqiang Gong, and Jia Liu. 2018. Poisoning
attacks to graph-based recommender systems. In Proceedings of the 34th Annual
Computer Security Applications Conference. ACM, 381–392.
[22] Yarin Gal. 2016. Uncertainty in deep learning. Ph.D. Dissertation. PhD thesis,
University of Cambridge.
[23] Yansong Gao, Chang Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe, and
Surya Nepal. 2019. STRIP: A Defence Against Trojan Attacks on Deep Neural
Networks. arXiv preprint arXiv:1902.06531 (2019).
[24] Ross Girshick. 2015. Fast r-cnn. In Proceedings of the IEEE international conference
on computer vision. 1440–1448.
[25] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. 2017. Badnets: Identifying
vulnerabilities in the machine learning model supply chain. arXiv preprint
arXiv:1708.06733 (2017).
[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770–778.
[27] Matthew Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru,
and Bo Li. 2018. Manipulating machine learning: Poisoning attacks and coun-
termeasures for regression learning. In 2018 IEEE Symposium on Security and
Privacy (SP). IEEE, 19–35.
[28] Yujie Ji, Xinyang Zhang, Shouling Ji, Xiapu Luo, and Ting Wang. 2018. Model-
reuse attacks on deep learning systems. In Proceedings of the 2018 ACM SIGSAC
Conference on Computer and Communications Security. ACM, 349–363.
[29] Yujie Ji, Xinyang Zhang, and Ting Wang. 2017. Backdoor attacks against learning
systems. In 2017 IEEE Conference on Communications and Network Security (CNS).
[30] Melissa King. 2019. TrojAI. https://www.iarpa.gov/index.php?option=com_
content&view=article&id=1142&Itemid=443
[31] Alex Krizhevsky and Geoffrey Hinton. 2009. Learning multiple layers of features
[32] Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner, et al. 1998. Gradient-
from tiny images. Technical Report. Citeseer.
based learning applied to document recognition. Proc. IEEE (1998).
[33] Gil Levi and Tal Hassner. 2015. Age and gender classification using convolutional
neural networks. In Proceedings of the IEEE conference on computer vision and
pattern recognition workshops. 34–42.
82%84%86%88%90%0.191.053.3974%76%78%80%82%84%86%0.7951.522.8680%82%84%86%88%90%24.9631.446.960.580%84%88%92%0.180.241.074.8980%82%84%86%88%6.496.718.6770%75%80%85%90%5.218.8713.2138.9980%82%84%86%88%90%00.010.030.1382%84%86%88%0.010.020.290.6580%82%84%86%88%90%0.000.020.100.25[34] Wenshuo Li, Jincheng Yu, Xuefei Ning, Pengjun Wang, Qi Wei, Yu Wang, and
Huazhong Yang. 2018. Hu-fu: Hardware and software collaborative attack frame-
work against neural networks. In ISVLSI.
[35] Cong Liao, Haoti Zhong, Anna Squicciarini, Sencun Zhu, and David Miller. 2018.
Backdoor embedding in convolutional neural network models via invisible per-
turbation. arXiv preprint arXiv:1808.10307 (2018).
[36] Min Lin, Qiang Chen, and Shuicheng Yan. 2013. Network in network. arXiv
preprint arXiv:1312.4400 (2013).
[37] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. 2018. Fine-pruning: De-
fending against backdooring attacks on deep neural networks. In International
Symposium on Research in Attacks, Intrusions, and Defenses. Springer, 273–294.
[38] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang
Wang, and Xiangyu Zhang. 2018. Trojaning Attack on Neural Networks. In 25nd
Annual Network and Distributed System Security Symposium, NDSS 2018, San
Diego, California, USA, February 18-221, 2018. The Internet Society.
[39] Yuntao Liu, Yang Xie, and Ankur Srivastava. 2017. Neural trojans. In 2017 IEEE
International Conference on Computer Design (ICCD). IEEE, 45–48.
[40] Shiqing Ma, Yingqi Liu, Guanhong Tao, Wen-Chuan Lee, and Xiangyu Zhang.
2019. NIC: Detecting Adversarial Samples with Neural Network Invariant Check-
ing. In 26th Annual Network and Distributed System Security Symposium, NDSS.
[41] Wei Ma and Jun Lu. 2017. An Equivalence of Fully Connected Layer and Convo-
lutional Layer. arXiv preprint arXiv:1712.01252 (2017).
[42] Andreas Møgelmose, Dongran Liu, and Mohan M Trivedi. 2014. Traffic sign
detection for us roads: Remaining challenges and a case for tracking. In 17th
International IEEE Conference on Intelligent Transportation Systems (ITSC).
[43] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal
Frossard. 2017. Universal adversarial perturbations. In Proceedings of the IEEE
conference on computer vision and pattern recognition. 1765–1773.
[44] Mehran Mozaffari-Kermani, Susmita Sur-Kolay, Anand Raghunathan, and Niraj K
Jha. 2015. Systematic poisoning attacks on and defenses for machine learning in
healthcare. IEEE journal of biomedical and health informatics (2015).
[45] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay
Celik, and Ananthram Swami. 2017. Practical black-box attacks against machine
learning. In Proceedings of the 2017 ACM on Asia conference on computer and
communications security. ACM, 506–519.
[46] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik,
and Ananthram Swami. 2016. The limitations of deep learning in adversarial
settings. In 2016 IEEE European Symposium on Security and Privacy (EuroS&P).
[47] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, et al. 2015. Deep face
recognition.. In bmvc, Vol. 1. 6.
[48] Andrea Paudice, Luis Muñoz-González, and Emil C Lupu. 2018. Label sanitization
against label flipping poisoning attacks. In Joint European Conference on Machine
Learning and Knowledge Discovery in Databases. Springer, 5–15.
[49] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. You
only look once: Unified, real-time object detection. In Proceedings of the IEEE
conference on computer vision and pattern recognition. 779–788.
[50] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean
Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.
2015. Imagenet large scale visual recognition challenge. International journal of
computer vision 115, 3 (2015), 211–252.
[51] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer,
Tudor Dumitras, and Tom Goldstein. 2018. Poison frogs! targeted clean-label
poisoning attacks on neural networks. In NeuralIPS.
[52] Mahmood Sharif, Lujo Bauer, and Michael K Reiter. 2018. On the suitability of
lp-norms for creating and preventing adversarial examples. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition Workshops.
[53] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks
for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).
[54] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. 2011. The
German Traffic Sign Recognition Benchmark: A multi-class classification compe-
tition.. In IJCNN, Vol. 6. 7.
[55] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. 2012. Man vs.
computer: Benchmarking machine learning algorithms for traffic sign recognition.
Neural networks 32 (2012), 323–332.
[56] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks.
arXiv preprint arXiv:1312.6199 (2013).
[57] Guanhong Tao, Shiqing Ma, Yingqi Liu, and Xiangyu Zhang. 2018. Attacks meet
interpretability: Attribute-steered detection of adversarial samples. In Advances
in Neural Information Processing Systems. 7717–7728.
[58] Alexander Turner, Dimitris Tsipras, and Aleksander Madry. 2018. Clean-Label
Backdoor Attacks. (2018).
[59] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao
Zheng, and Ben Y Zhao. 2019. Neural cleanse: Identifying and mitigating backdoor
attacks in neural networks. In Neural Cleanse: Identifying and Mitigating Backdoor
Attacks in Neural Networks. IEEE, 0.
[60] Zhou Wang, Alan C Bovik, Hamid R Sheikh, Eero P Simoncelli, et al. 2004. Image
quality assessment: from error visibility to structural similarity. IEEE transactions
on image processing 13, 4 (2004), 600–612.
org/wiki/Electrical_brain_stimulation
[61] wikipedia. 2019. Electrical brain stimulation - Wikipedia. https://en.wikipedia.
[62] Xi Wu, Uyeong Jang, Jiefeng Chen, Lingjiao Chen, and Somesh Jha. 2017. Rein-
forcing adversarial robustness using model confidence induced by adversarial
training. arXiv preprint arXiv:1711.08001 (2017).
[63] Weilin Xu, David Evans, and Yanjun Qi. 2017. Feature squeezing: Detecting
adversarial examples in deep neural networks. arXiv preprint arXiv:1704.01155
(2017).
[64] Wei Yang, Deguang Kong, Tao Xie, and Carl A Gunter. 2017. Malware detection in
adversarial settings: Exploiting feature evolutions and confusions in android apps.
In Proceedings of the 33rd Annual Computer Security Applications Conference.
[65] Minhui Zou, Yang Shi, Chengliang Wang, Fangyu Li, WenZhan Song, and Yu
Wang. 2018. Potrojan: powerful neural-level trojan designs in deep learning
models. arXiv preprint arXiv:1802.03043 (2018).
Appendix A EXAMPLE OF CONFOUNDING
EFFECT
Figure 24 shows a real world example of confounding using a tro-
janed model on MNIST. As shown in the figure, the NSFs for a
benign neuron and a compromised neuron (from the same trojaned
model) are similar. Hence, both are candidates. Then we run the
model with all the inputs, including the poisonous inputs. We found
that the input that induces the largest activation of the benign neu-
ron is a benign input and the largest activation is only 1.8 while
a poisonous input can induce the compromised neuron to have
a value of 10.4. From the NSF of the benign neuron, there is no
elevation effect when the activation is 1.8. The example shows that
the elevation effect of a benign neuron usually does not originate
from training and may likely be infeasible, whereas the elevation
of a compromised neuron is induced by training.
Figure 24: Example of confounding effect
Appendix B MODEL TROJANING
In this section, we show test accuracy and attack success rate of the
models trojaned in this paper in Table 7. Columns 1 and 2 show the
dataset and models. Column 3 shows the average accuracy of the
benign models. Number 20 in the column label indicates that each
reported value is an average over 20 models. Column 4 Acc Dec
shows the accuracy decrease of models trojaned with trigger YS.
Column 5 ASR shows the attack success rate of models trojaned with
trigger YS. The accuracy decrease denotes the difference between
the accuracy of a benign models and the accuracy of its trojaned
version. Note that for CIAR-10 and GTSRB, we trojan 3 models with
YS using 3 different combinations of benign and poisonous data,
and hence the accuracy decrease and attack success rate are average
across 3 models. For ImageNet, we trojan 1 model per trigger, we
directly report the accuracy decrease and attack success rate of
Figure 25: Label Specific “Trigger”
each trojaned model in row 8. ImageNet is a hard task and hence
top-5 accuracy is often used [50, 53] . Here we report the top-5 accu-
racy decrease and the top-5 attack success rate of trojaned models.
Columns 6-21 show the accuracy decrease and attack success rate
for different kinds of attacks. In some cases, the trojaned models
have higher accuracy than the original ones, indicated by negative
values in Acc Dec columns. Because the adversarial perturbation is
very small, it is hard to trojan an adversarial perturbation model
with high attack success rate without degrading the performance
on benign images. For adversarial perturbation attack on CIFAR-
VGG, the accuracy drops 10.3%. For all other types of attacks, the
trojaned models have minor accuracy decrease. The downloaded
(benign and trojaned) models have similar test accuracy difference
and attack success rate, as shown in their papers [25, 38].
Appendix C DETECTION EFFECTIVENESS VS.
TRIGGER SIZE
In this experiment we evaluate how ABS’s detection rate changes
with trigger size changes on CIFAR-10. In Table 8, the row TS means
the size of trigger and column MMS denotes the max_mask_size in
Algorithm 2. We vary the trigger size from 2% of the input to 25%,
following a setting suggested in [30]. As show in Table 8, as long as
the max_mask_size is larger or equal to the trigger size, the REASR
of our approach is at least 85% (100% in most cases). This shows our
approach can successfully detect trojaned models within the trigger
size budget. In all other experiments, we use max_mask_size = 6%.
Appendix D DETECTING LABEL SPECIFIC
ATTACK
Although our goal is to defend attacks that aim to persistently
subvert any input (of any label) to the target label, we conduct an
experiment to observe if ABS has potential in dealing with label
specific attack, in which the attacker only aims to subvert inputs
of a specific label (called the victim label) to the target label. We
trojan the CIFAR-10 ResNet model with 10 triggers and each trigger
causing the images belonging to class k to be classified to k + 1. We
poison 10% samples. The model accuracy changes by -0.3% and the
average attack success rate (on all labels) is 93.7%. Note that in this
context, ABS can no longer use one input per label as stamping
inputs of labels other than the victim with the trigger has no effect.
Hence, we use 5 additional images for each label for validating if the
generated trigger is effective. The REASR scores of benign models
and trojaned model are shown in Table 9, with each column (except
the first one) representing a label. For benign models, label deer
has a high REASR and hence ABS fails. The reverse engineered
trigger is shown in Figure 25, which looks like antlers. In general,
label specific attack incurs more false positives for ABS because
the reverse engineered “trigger” only needs to subvert one class of
inputs. For trojaned models, ABS can detect 9 out of 10. ABS fails
to detect the trigger that mis-classifies Horse to Ship.
Appendix E PERTURBING PARAMETERS
AND USING DATA
AUGMENTATION IN NC
In this section, we try different parameter configurations and lever-
age data augmentation to see if NC’s performance can be improved.
We perturb 3 main parameters used in NC optimization, the attack
success rate threshold (THRESHOLD), cost multiplier (MULTIPLIER)
and learning rate (LR). The first is the threshold at which NC stops
the optimization and considers the reverse engineered pattern a
trigger. The second is used to control the weight between the op-
timization loss and the trigger size loss. The third is the learning
rate for optimization. We experiment different settings of the three
parameters on CIFAR10. In addition, since NC’s performance is re-
lated to the number of samples used in optimization, we try to use
data augmentation to increase the number of samples. We use the
same data augmentation setting as normal CIFAR-10 training [9].
We test NC with one image per class which is the same setting
for ABS. The results are shown in Table 10. Column 1 shows the
parameters and column 2 the value settings. Columns 3-5 show
the classification accuracy for pixel space trojaned models, feature
space trojaned models and benign models for the NiN structure.
Columns 6-8 and columns 9-11 show the detection accuracy for
VGG and ResNet models. By changing these parameters, the de-
tection rate of pixel space attacks can increase to 60% on VGG
models but the accuracy on benign model also decreases to 72%.
The detection rate of feature space attacks can increase to 67% on
ResNet models but the accuracy on benign model also decreases
to 70%. As shown in row 12 in Table 10, using data augmentation
increases detection rate on pixel space attacks, while in the mean
time causes accuracy degradation on benign models. We speculate
that data augmentation can only explore a small region around the
input and cannot increase the diversity of data substantially and
does not help guide the optimization of NC. This shows changing
parameter settings or using data augmentation may improve NC’s
performance but the improvement is limited.
Appendix F OPTIMIZING MULTIPLE
COMPROMISED NEURONS
TOGETHER
There could be multiple compromised neurons in a trojaned model.
In this section, we try to optimize multiple compromised neurons
together to see if it can help produce triggers of better quality. For
NiN models trojaned with pixel space patch triggers, we further
reverse engineer triggers based on all the compromised neurons
ABS finds. Figure 26 shows the results. Row 1 shows the original
triggers. For each kind of trigger, we trojan the model with 3 dif-
ferent settings such that with the 5 different kinds, there are 15
different models in total. Row 2 shows the triggers reverse engi-