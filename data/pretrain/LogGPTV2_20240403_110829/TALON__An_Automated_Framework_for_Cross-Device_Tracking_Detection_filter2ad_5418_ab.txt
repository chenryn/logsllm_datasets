(cid:21)
(cid:16)
(cid:12)
(cid:1)
(cid:16)
(cid:26)
(cid:27)
(cid:30)
(cid:29)
(cid:20)
(cid:12)
(cid:30)
(cid:20)
(cid:21)
(cid:16)
(cid:12)
(cid:1)
(cid:24)
(cid:27)
(cid:29)
(cid:31)
(cid:26)
(cid:27)
(cid:4)
(cid:24)
(cid:29)
(cid:27)
(cid:31)
(cid:18)
(cid:20)
(cid:20)
(cid:13)
(cid:28)
(cid:32)
(cid:31)
(cid:20)
(cid:13)
(cid:1)
(cid:1)
(cid:24)
(cid:16)
(cid:31)
(cid:26)
(cid:20)
(cid:25)
(cid:23)
(cid:29)
(cid:20)
(cid:28)
(cid:33)
(cid:6)
(cid:4)(cid:15)(cid:18)(cid:19)(cid:8)(cid:15)(cid:19)(cid:14)(cid:8)(cid:19)(cid:14)(cid:16)(cid:15)(cid:1) (cid:16)(cid:12)
(cid:6)(cid:17)(cid:16)(cid:9)(cid:14)(cid:15)(cid:13)(cid:1) (cid:3)(cid:11)(cid:20)(cid:14)(cid:10)(cid:11)(cid:18)
(cid:21)
(cid:2)(cid:19)(cid:36)(cid:20)(cid:18)(cid:27)(cid:30)(cid:34)(cid:30)(cid:31)(cid:20)(cid:25)
(cid:4)(cid:5)(cid:14)(cid:1)(cid:7)(cid:32)(cid:26)(cid:18)(cid:31)(cid:23)(cid:27)(cid:26)(cid:30)(cid:1)
(cid:39)(cid:1)(cid:10)(cid:27)(cid:19)(cid:20)(cid:24)(cid:23)(cid:26)(cid:21)(cid:1) (cid:37)(cid:1)(cid:38)
(cid:11)(cid:32)(cid:31)(cid:28)(cid:32)(cid:31)(cid:1) (cid:13)(cid:23)(cid:21)(cid:26)(cid:16)(cid:24)(cid:1)(cid:37)(cid:4)(cid:38)
(cid:4)(cid:5)(cid:14)(cid:1)(cid:5)(cid:20)(cid:31)(cid:20)(cid:18)(cid:31)(cid:23)(cid:27)(cid:26)
(cid:15)(cid:1)(cid:15)(cid:1)(cid:15)
(cid:2)(cid:19)(cid:36)
(cid:20)(cid:18)(cid:27)(cid:30)(cid:34)(cid:30)(cid:31)(cid:20)(cid:25)
(cid:2)(cid:19)(cid:1)
(cid:4)(cid:16)(cid:31)(cid:20)(cid:21)(cid:27)(cid:29)(cid:23)(cid:35)(cid:20)(cid:29)
(cid:1)(cid:8)(cid:14)
(cid:2)(cid:7)(cid:15)(cid:9)(cid:10)(cid:12)(cid:13)(cid:11)(cid:9)(cid:14)
(cid:3)(cid:6)(cid:5)(cid:4)
(cid:12)(cid:16)(cid:21)(cid:20)(cid:1)(cid:12)(cid:16)(cid:29)(cid:30)(cid:20)(cid:29)
(cid:39)(cid:1)(cid:2)(cid:19)(cid:1)(cid:6)(cid:33)(cid:31)(cid:29)(cid:16)(cid:18)(cid:31)(cid:27)(cid:29)
(cid:5)(cid:9)(cid:15)(cid:7)(cid:8)(cid:7)(cid:15)(cid:7)
(cid:7)(cid:20)(cid:16)(cid:31)(cid:32)(cid:29)(cid:20)(cid:1)
(cid:6)(cid:33)(cid:31)(cid:29)(cid:16)(cid:18)(cid:31)(cid:27)(cid:29)
(cid:4)(cid:5)(cid:14)(cid:1)(cid:10)(cid:16)(cid:18)(cid:22)(cid:23)(cid:26)(cid:20)(cid:1)
(cid:9)(cid:20)(cid:16)(cid:29)(cid:26)(cid:23)(cid:26)(cid:21)(cid:1)(cid:10)(cid:27)(cid:19)(cid:20)(cid:24)(cid:20)(cid:29)
(cid:2)(cid:3)(cid:7)
(cid:5)(cid:16)(cid:1)(cid:2)(cid:3)(cid:7)
Figure 2: High level representation of methodology design principles and units for CDT measurements.
top, and vice versa. However, since ad-targeting companies
such as AdBrain and Criteo support that the direction from
mobile to desktop is more suitable for cross-device retarget-
ing [49, 3, 15], in this work we focus on the mobile to desk-
top direction (Mob → PC).
In essence, the mobile device
performs a speciﬁcally instructed web browsing session to
establish the persona, by visiting the set of persona pages,
i.e., training phase; then, the two desktop computers perform
web browsing, i.e., testing phase, where they visit the set of
control pages and collect the delivered ads. The browsing
performed by the desktops is synchronized by means of vis-
iting the same pages and performing the exact same clicks.
given dimensionality (e.g., Jaccard, Cosine) can be applied.
These methods, as well as typical statistical techniques (e.g.,
permutation tests) capture only one dimension of each in-
put/output signal and thus, might not be suitable for measur-
ing with conﬁdence the high complexity of the CDT signal.
In this case, more advanced methods can be employed, such
as Machine Learning techniques (ML) for classiﬁcation of
the signals as similar enough to match, or not. In our analy-
sis, we mainly focus on ML to compute the likelihood of the
two signals being the product of CDT, as it takes into con-
sideration this multidimensionality in the feature space. We
describe the modeling and methods used for ML in § 4.4.
3.2.3 Output Signal (Y)
4 Framework Implementation
In order to handle the Output Signal and transform it ap-
propriately, we design and implement two different compo-
nents: (i) Page Parser & Ad Extractor and (ii) Ad Catego-
rizer. The ﬁrst is responsible for the identiﬁcation and ex-
traction of ad elements inside the webpages. The module
uses string matching techniques and a public list of common
ad-domains (Easylist [21]) to identify the delivered ads. The
second module assigns a keyword on each ad identiﬁed on
the previous step, based on its type and content (e.g., “On-
line Shopping”, “Fashion”, “Recreation”, etc.). Using both
modules, we store the ads delivered in all devices of our ex-
perimental setup along with their categories, as well as data
related to the activity of the devices that attracted these ads.
3.2.4 CDT Detection
Comparing Signals. Various statistical methods can be used
to associate the input signal I of persona browsing in the
mobile device, with the output signal Y of ads delivered to
the potentially paired-PC. For example, simple methods that
perform similarity computation between the two signals in a
A high level overview of our methodology, and its material-
ization by our framework Talon, is presented in Figure 2 and
described in § 3. In the following, we provide more details
about its building blocks, and argue for various design de-
cisions taken while implementing this methodology into the
fully-ﬂedged automated system.
Input Signal: Control Pages & Personas
4.1
Persona Pages. A critical part of our methodology is the de-
sign and automatic building of realistic user personas. Each
persona has a unique collection of visiting links, that form
the set of persona pages. Since we do not know in ad-
vance which e-commerce sites are conducting cross-device
ad-campaigns, we design a process to dynamically detect ac-
tive persona pages of given interest categories. Our approach
for persona generation is shown in Figure 3.
We ﬁrst use the list of topics of Zimmeck at al. [56], that
describe real user’s online interests. We perform a cluster-
ing based on the content of each interest and label the clus-
ters appropriately (e.g., we group together: “Shopping” and
230          22nd International Symposium on Research in Attacks, Intrusions and DefensesUSENIX Association(cid:7)(cid:29)(cid:25)(cid:39)(cid:21)(cid:1)(cid:16)(cid:32)(cid:33)(cid:1)
(cid:15)(cid:28)(cid:36)(cid:25)(cid:35)
(cid:9)(cid:39)(cid:36)(cid:34)(cid:21)(cid:23)(cid:36)(cid:1)
(cid:18)(cid:25)(cid:21)(cid:36)(cid:27)(cid:25)(cid:34)(cid:1)
(cid:18)(cid:25)(cid:22)(cid:35)(cid:28)(cid:36)(cid:25)(cid:35)
(cid:14)(cid:25)(cid:21)(cid:29)(cid:1)(cid:17)(cid:35)(cid:25)(cid:34)(cid:41)(cid:35)(cid:1)
(cid:11)(cid:31)(cid:36)(cid:25)(cid:34)(cid:25)(cid:35)(cid:36)(cid:35)(cid:1)
(cid:19)(cid:5)(cid:6)(cid:20)
(cid:15)(cid:40)(cid:31)(cid:36)(cid:27)(cid:25)(cid:36)(cid:28)(cid:23)
(cid:13)(cid:25)(cid:34)(cid:35)(cid:32)(cid:31)(cid:21)(cid:35)
(cid:19)(cid:3)(cid:4)(cid:20)
(cid:10)(cid:32)(cid:32)(cid:26)(cid:29)(cid:25)(cid:1)(cid:13)(cid:34)(cid:32)(cid:24)(cid:37)(cid:23)(cid:36)(cid:1)
(cid:16)(cid:21)(cid:39)(cid:32)(cid:31)(cid:32)(cid:30)(cid:40)
(cid:10)(cid:34)(cid:32)(cid:37)(cid:33)(cid:1)
(cid:12)(cid:25)(cid:40)(cid:38)(cid:32)(cid:34)(cid:24)(cid:35)(cid:1)(cid:33)(cid:25)(cid:34)(cid:1)
(cid:13)(cid:25)(cid:34)(cid:35)(cid:32)(cid:31)(cid:21)
(cid:9)(cid:39)(cid:36)(cid:34)(cid:21)(cid:23)(cid:36)(cid:1)
(cid:12)(cid:25)(cid:40)(cid:38)(cid:32)(cid:34)(cid:24)(cid:35)
(cid:13)(cid:34)(cid:25)(cid:2)(cid:33)(cid:34)(cid:32)(cid:23)(cid:25)(cid:35)(cid:35)(cid:28)(cid:31)(cid:26)
(cid:10)(cid:32)(cid:32)(cid:26)(cid:29)(cid:25)(cid:1)(cid:15)(cid:25)(cid:21)(cid:34)(cid:23)(cid:27)(cid:1)(cid:2)
(cid:8)(cid:21)(cid:30)(cid:33)(cid:21)(cid:28)(cid:26)(cid:31)(cid:1)(cid:9)(cid:39)(cid:36)(cid:34)(cid:21)(cid:23)(cid:36)(cid:28)(cid:32)(cid:31)
(cid:13)(cid:25)(cid:34)(cid:35)(cid:32)(cid:31)(cid:21)(cid:1)(cid:15)(cid:25)(cid:29)(cid:25)(cid:23)(cid:36)(cid:28)(cid:32)(cid:31)
(cid:8)(cid:32)(cid:31)(cid:36)(cid:34)(cid:32)(cid:29)(cid:1)(cid:13)(cid:21)(cid:26)(cid:25)(cid:35)
(cid:13)(cid:25)(cid:34)(cid:35)(cid:32)(cid:31)(cid:21)(cid:1)(cid:13)(cid:21)(cid:26)(cid:25)(cid:35)
Figure 3: Persona design and automatic generation.
“Beauty and Fashion” under the label: “Shopping and Fash-
ion”). Then, we use the persona categorization of Carrascosa
et al. [12] for their top 50 personas, and select only those
personas that describe similar interests with the previously
formed list. For the resulting intersection of personas from
the two lists, we iterate through the Google Product Taxon-
omy list [27] to obtain the related keywords for each one.
For
increasing the probability to capture active ad-
campaigns that can potentially deliver ads to the devices,
we use Google Search as it reveals campaigns associated
with products currently being advertised. That is,
if a
user searches for speciﬁc keywords (e.g., “men watches”),
Google will display a set of results, including sponsored
links for sites conducting campaigns for the terms searched.
In this way, we use the keywords set for each persona, as
extracted above, and transform them into search queries by
appending common string patterns such as “buy”, “sell”, and
“offers”. This process is repeated until between ﬁve and ten
unique domains per persona are collected. If the procedure
fails, no persona is formed.
As the effectiveness of a persona depends on the active
ad-campaigns at the given time, in our experiments, we de-
ploy personas in 10 categories related to shopping, traveling,
etc. (full list shown in Table 3 in Appendix). With this pro-
cedure, we manage to design personas similar enough with
real users, as well as with emulated users designed in previ-
ous works [12, 7, 8, 56].
Control Pages. For retrieving the delivered ads (after any
type of browsing), we employ a set of webpages that contain:
(i) easily identiﬁable ad-elements and (ii) a sufﬁcient num-
ber of ads that remains consistent through time. These pages
have neutral context and do not affect the behavioral proﬁle
of the device visiting them. For most of the experiments in
§ 5, we use a set of ﬁve popular weather websites2 as control
pages, similarly to [12]. We manually conﬁrmed the neutral-
2accuweather.com,
weather-forecast.com, metcheck.com
wunderground.com,
weather.com,
ity of these pages, by observing no contextual ads delivered
to them. When visiting the set of control pages, our meth-
ods extract and categorize all the ads received, in order to
identify those that have been potentially resulted from CDT.
4.2 Experimental System Setup
The experimental setup contains different types of units, con-
nected together for replicating browsing activity on multiple
devices. Typically, CDT is applied on two or more devices
that belong to the same user, such as a desktop and a mobile
device. Thus, the system contains emulated instances of both
types, controlled by a number of experimental parameters.
Devices & Automation. The desktop devices are built on
top of the web measurement framework OpenWPM [22].
This platform enables launching instances of the Firefox
browser, performs realistic browsing with scrolling, sleeps
and clicks, and collects a wide range of measurements in
every browsing session.
It is also capable of storing the
browser’s data (cookies, local cache, temporary ﬁles) and ex-
ports a browser proﬁle after the end of a browsing session,
which can be loaded in a future session. With these options,
we can perform stateful experiments, as a typical user’s web
browser that stores all the data through time, or stateless ex-
periments to emulate browsing in incognito mode.
For the mobile device, we use the ofﬁcial Android Emula-
tor [28], as well as the Appium UI Automator [50] for the au-
tomation of browsing. We build the mobile browsing module
on top of these components to automate visits to pages via
the Browser Application. This browsing module provides
functionalities for realistic interaction with a website, e.g.,
scrolling, click and sleep rate. Similarly to the desktop, it
can run either in a stateful or stateless mode.
Experimental Setup Selector. As shortly described in § 3,
we need two phases of browsing to different types of web-
pages (training and testing), in order to successfully measure
CDT. For that reason, we set the two browsing phases in the
following way: During the training phase, the selected de-
vice visits the set of Persona Pages for a speciﬁc duration,
referred to as training time (ttrain). The test phase is the set
of visits to control pages for the purpose of collecting ads.
During this phase, we control the duration of browsing (i.e.,
ttest). The experimental setup selector controls various pa-
rameters such as: which type of device will be trained and
tested, the times ttrain and ttest, the sequence of time slots
for training and testing from the selected device, number of
repetitions of this procedure, etc.
Timeline of phases. Each class of experiments is executed
multiple times (or runs), through parallel instantiations of the
user devices within the framework (as shown in Figure 2).
Each experimental run is executed following a timeline of
phases as illustrated in Figure 4. This timeline contains N
sessions with three primary stages in each: Before, Mobile,
and After. The Before (Bi) stage is when the two desktop de-
USENIX Association        22nd International Symposium on Research in Attacks, Intrusions and Defenses 231(cid:9)(cid:14)(cid:19)(cid:19)(cid:15)(cid:18)(cid:17)(cid:1)(cid:26)
(cid:4)(cid:5)(cid:10)(cid:1)(cid:13)(cid:14)(cid:20)(cid:14)(cid:12)(cid:20)(cid:15)(cid:18)(cid:17)
(cid:9)(cid:14)(cid:19)(cid:19)(cid:15)(cid:18)(cid:17) (cid:27)
(cid:21)(cid:1)(cid:21)(cid:1)(cid:21)(cid:1)(cid:21)(cid:1)(cid:21)(cid:1)
(cid:9)(cid:14)(cid:19)(cid:19)(cid:15)(cid:18)(cid:17) (cid:7)
(cid:3)(cid:26)(cid:1) (cid:6)(cid:26)(cid:1) (cid:11) (cid:2)(cid:26)(cid:1)(cid:1)(cid:1)(cid:1) (cid:8)(cid:1)(cid:1)(cid:1)(cid:1)(cid:3)(cid:27) (cid:6)(cid:27)(cid:1) (cid:11) (cid:2)(cid:27)(cid:1)(cid:1)(cid:1)(cid:1) (cid:8)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:3)(cid:7) (cid:6)(cid:7)(cid:1) (cid:11) (cid:2)(cid:7)(cid:1)
(cid:25)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:20)(cid:9)
(cid:20)(cid:9)
(cid:26)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1) (cid:27)
(cid:20)(cid:9)
(cid:10)(cid:15)(cid:16)(cid:14)(cid:1)(cid:23)(cid:20)(cid:24)
(cid:7)(cid:22)(cid:26)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)
Figure 4: Timeline of phases for CDT measurement.
Mi: mobile training time ttrain + testing time ttest;
Bi(Ai): desktop testing time ttest before (after) mobile phase;
W: wait time (twait); R: rest time (trest); tSi: time of session i.
vices perform a parallel test browsing, with a duration of ttest
time, to establish the state of ads before the mobile device
injects signal into the ad-ecosystem. The Mobile (Mi) stage
is when the mobile device performs a training browsing for
ttrain time, and a test browsing for ttest time. This phase in-
jects the signal from the mobile during training with a per-
sona, but also performs a subsequent test with control pages
to establish the state of ads after the training. Finally, the Af-
ter (Ai) stage is when the two desktops perform the ﬁnal test
browsing, with the same duration ttest as in Before (Bi) stage,
to establish the state of ads after the mobile training.
After extensive experimentation, we found that a mini-
mum training time ttrain=15 minutes and testing time ttest=20
minutes are sufﬁcient for injecting a clear signal over noise,
from the trained device to the ad-ecosystem. There is also
a waiting time (twait=10 minutes) and resting time (trest=5
minutes) between the stages of each session, to allow align-
ment of instantiations of devices running in parallel during
each session. In total, each session lasts 1.5 hours and is re-
peated N=15 times during a run. Through the experimental
setup selector, we deﬁne the values of such variables (ttrain,
ttest, twait, trest, N, type of device), offering the researcher the
ﬂexibility to experiment in different cases of CDT.
4.3 Output Signal
Page Parser. This component is activated when the visited
page is fully loaded and no further changes occur on the con-
tent. To collect the display ads, we ﬁrst need to identify spe-
ciﬁc DOM elements inside the visited webpages. This task is
challenging due to the dynamic Javascript execution and the
complex DOM structures generated in most webpages. For
the reliable extraction of ad-elements and identiﬁcation of
the landing pages,3 we follow a methodology similar to the