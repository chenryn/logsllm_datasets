以下是经过优化后的文本，使其更加清晰、连贯和专业：

---

### 参考文献

1. A. A. Efros, “Context Encoders: Feature Learning by Inpainting,” in CVPR, 2016.
2. M. Noroozi and P. Favaro, “Unsupervised Learning of Visual Features by Solving Jigsaw Puzzles,” in ECCV, 2016.
3. K. Liu, B. Dolan-Gavitt, and S. Garg, “Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks,” in RAID, 2018.
4. Y. Gao, C. Xu, D. Wang, S. Chen, D. C. Ranasinghe, and S. Nepal, “STRIP: A Defence Against Trojan Attacks on Deep Neural Networks,” in ACSAC, 2019.
5. Y. Liu, W.-C. Lee, G. Tao, S. Ma, Y. Aafer, and X. Zhang, “ABS: Scanning Neural Networks for Backdoors by Artificial Brain Stimulation,” in CCS, 2019.
6. B. G. Doan, E. Abbasnejad, and D. C. Ranasinghe, “Februus: Input Purification Defense Against Trojan Attacks on Deep Neural Network Systems,” in ACSAC, 2020.
7. P.-y. Chiang, R. Ni, A. Abdelkader, C. Zhu, C. Studor, and T. Goldstein, “Certified Defenses for Adversarial Patches,” in ICLR, 2019.
8. A. Levine and S. Feizi, “(De)randomized Smoothing for Certifiable Defense Against Patch Attacks,” in NeurIPS, 2020.
9. J. H. Metzen and M. Yatsura, “Efficient Certified Defenses Against Patch Attacks on Image Classifiers,” in ICLR, 2021.
10. B. Wang, X. Cao, J. Jia, N. Z. Gong et al., “On Certifying Robustness Against Backdoor Attacks via Randomized Smoothing,” in CVPR 2020 Workshop on Adversarial Machine Learning in Computer Vision, 2020.
11. Z. Zhang, J. Jia, B. Wang, and N. Z. Gong, “Backdoor Attacks to Graph Neural Networks,” in SACMAT, 2021.

### 相关资源

- **Neural Cleanse**: https://github.com/bolunwang/Neural-Cleanse
- **MNTD (Meta-Neural Trojan Detection)**: https://github.com/AI-secure/Meta-Neural-Trojan-Detection
- **PatchGuard**: https://github.com/inspire-group/PatchGuard

### 其他相关工作

1. A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving Language Understanding by Generative Pre-Training,” 2018. [Online]. Available: https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language-understanding-paper.pdf
2. A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language Models are Unsupervised Multitask Learners,” OpenAI Blog, vol. 1, no. 8, p. 9, 2019.
3. T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language Models are Few-Shot Learners,” Arxiv:2005.14165, 2020.
4. Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le, “XLNet: Generalized Autoregressive Pretraining for Language Understanding,” NeurIPS, 2019.
5. W. Hu, B. Liu, J. Gomes, M. Zitnik, P. Liang, V. Pande, and J. Leskovec, “Strategies for Pre-Training Graph Neural Networks,” in ICLR, 2020.
6. J. Qiu, Q. Chen, Y. Dong, J. Zhang, H. Yang, M. Ding, K. Wang, and J. Tang, “GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training,” in KDD, 2020.
7. J. Dai, C. Chen, and Y. Li, “A Backdoor Attack Against LSTM-Based Text Classification Systems,” IEEE Access, vol. 7, pp. 138 872–138 878, 2019.
8. E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, “How to Backdoor Federated Learning,” in AISTATS, 2020.
9. Z. Xi, R. Pang, S. Ji, and T. Wang, “Graph Backdoor,” in Usenix Security, 2021.
10. C. Liao, H. Zhong, A. Squicciarini, S. Zhu, and D. Miller, “Backdoor Embedding in Convolutional Neural Network Models via Invisible Perturbation,” in CODASPY, 2020.
11. A. Saha, A. Subramanya, and H. Pirsiavash, “Hidden Trigger Backdoor Attacks,” in AAAI, 2020.
12. A. Turner, D. Tsipras, and A. Madry, “Label-Consistent Backdoor Attacks,” arXiv preprint arXiv:1912.02771, 2019.
13. S. Li, B. Z. H. Zhao, J. Yu, M. Xue, D. Kaafar, and H. Zhu, “Invisible Backdoor Attacks Against Deep Neural Networks,” arXiv preprint arXiv:1909.02742, 2019.
14. T. J. L. Tan and R. Shokri, “Bypassing Backdoor Detection Algorithms in Deep Learning,” in EuroS&P, 2020.
15. Y. Liu, X. Ma, J. Bailey, and F. Lu, “Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks,” in ECCV, 2020.
16. A. Salem, R. Wen, M. Backes, S. Ma, and Y. Zhang, “Dynamic Backdoor Attacks Against Machine Learning Models,” arXiv preprint arXiv:2003.03675, 2020.
17. N. Carlini and A. Terzis, “Poisoning and Backdooring Contrastive Learning,” arXiv preprint arXiv:2106.09667, 2021.
18. X. Chen, A. Salem, M. Backes, S. Ma, and Y. Zhang, “BADNL: Backdoor Attacks Against NLP Models,” arXiv preprint arXiv:2006.01043, 2020.
19. X. Zhang, Z. Zhang, S. Ji, and T. Wang, “Trojaning Language Models for Fun and Profit,” in EuroS&P, 2021.
20. Y. Ji, X. Zhang, and T. Wang, “Backdoor Attacks Against Learning Systems,” in CNS, 2017.
21. Y. Ji, X. Zhang, S. Ji, X. Luo, and T. Wang, “Model-Reuse Attacks on Deep Learning Systems,” in CCS, 2018.
22. A. Shafahi, W. R. Huang, M. Najibi, O. Suciu, C. Studer, T. Dumitras, and T. Goldstein, “Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks,” in NeurIPS, 2018.
23. W. Guo, L. Wang, X. Xing, M. Du, and D. Song, “TABOR: A Highly Accurate Approach to Inspecting and Restoring Trojan Backdoors in AI Systems,” arXiv preprint arXiv:1908.01763, 2019.
24. B. Chen, W. Carvalho, N. Baracaldo, H. Ludwig, B. Edwards, T. Lee, I. Molloy, and B. Srivastava, “Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering,” in AAAI, 2019.
25. H. Chen, C. Fu, J. Zhao, and F. Koushanfar, “DeepInspect: A Black-Box Trojan Detection and Mitigation Framework for Deep Neural Networks,” in IJCAI, 2019.
26. D. Tang, X. Wang, H. Tang, and K. Zhang, “Demon in the Variant: Statistical Analysis of DNNs for Robust Backdoor Contamination Detection,” in Usenix Security, 2021.
27. E. Chou, F. Tramèr, and G. Pellegrino, “SentiNet: Detecting Localized Universal Attacks Against Deep Learning Systems,” in 2020 IEEE Security and Privacy Workshops (SPW). IEEE, 2020, pp. 48–54.
28. M. Weber, X. Xu, B. Karlas, C. Zhang, and B. Li, “RAB: Provable Robustness Against Backdoor Attacks,” arXiv preprint arXiv:2003.08904, 2020.
29. J. Jia, X. Cao, and N. Z. Gong, “Certified Robustness of Nearest Neighbors Against Data Poisoning Attacks,” arXiv preprint arXiv:2012.03765, 2020.
30. ——, “Intrinsic Certified Robustness of Bagging Against Data Poisoning Attacks,” in AAAI, 2021.

### 算法描述

**算法 1: BadEncoder**

**输入**:
- θ (模型参数)
- max_epoch (最大迭代次数)
- lr (学习率)
- bs (批量大小)
- Ds (数据集)
- {Ri}t (参考输入集合)
- {ei}t (嵌入触发器集合)

**输出**:
- θ' (更新后的模型参数)

1. 初始化: θ' ← θ, epoch ← 1
2. 当 epoch ≤ max_epoch 时:
   1. iter ← 1
   2. 当 iter ≤ ⌊|Ds|/bs⌋ 时:
      1. 从 Ds 中随机抽取一个批次 batch
      2. 更新模型参数: θ' = θ' − lr · ∇θ'L(θ', batch, {Ri}t, {ei}t)
      3. iter ← iter + 1
   3. epoch ← epoch + 1
3. 返回 θ'

### 实验结果

**表 XIII: 使用不同参数设置预训练图像编码器、制作后门图像编码器并训练下游分类器的结果**

| 阶段 | 参数 | 值 | CA (%) | BA (%) | ASR (%) |
|------|------|-----|--------|--------|---------|
| 预训练图像编码器 | #epochs | 500 | 74.61 | 76.14 | 99.66 |
|                  | 学习率 | 1 × 10−3 | 76.14 | 76.14 | 99.73 |
| 制作后门图像编码器 | 触发器类型 | 白色方块 | 76.14 | 76.14 | 99.86 |
|                  | #epochs | 1,000 | 76.14 | 76.14 | 99.73 |
|                  | 学习率 | 5 × 10−4 | 76.14 | 76.14 | 99.93 |
| 训练下游分类器 | #neurons in the two hidden layers | [128, 64] | 76.14 | 76.14 | 99.89 |

**图 4: 清洁图像编码器和后门图像编码器生成的特征向量之间的余弦相似度分数的累积分布函数 (CDFs)**

**表 XII: 比较 BadEncoder 和 LBA [12]**

| 目标下游数据集 | LBA | 测试图像比例 (%) | ASR (%) | BadEncoder | ASR (%) |
|-----------------|-----|------------------|----------|-------------|----------|
| GTSRB          |     | 10               | 7.14     |             | 98.73    |
|                |     | 50               | 12.63    |             | 98.81    |
| SVHN           |     | 10               | 0.19     |             | 99.15    |
|                |     | 50               | 24.16    |             | 99.09    |
| STL10          |     | 10               | 9.63     |             | 99.39    |
|                |     | 50               | 12.77    |             | 99.59    |

**图 6: 表 V 中使用的 STL10 的参考输入。Truck 0 是默认的参考输入。**

**图 5: CIFAR10、SVHN、GTSRB 和 STL10 的默认参考输入。**

**图 7: 阴影数据集大小对我们 BadEncoder 的影响，目标下游数据集分别为 GTSRB（左）和 SVHN（右）。阴影数据集是预训练数据集 CIFAR10 的子集。**

**图 8: 不同目标下游数据集下，触发器大小对我们的 BadEncoder 的影响，预训练数据集为 STL10。**

**图 9: 同时攻击 STL10 中三个目标类别的参考输入，用于表 VI。**

**图 12: 攻击 OpenAI 的 CLIP 在零样本分类场景中的参考输入，用于表 IXb。**

**图 10: 攻击 Google 预训练在 ImageNet 上的图像编码器的参考输入，用于表 VIII。**

**图 11: 攻击 OpenAI 的 CLIP 在多样本分类场景中的参考输入，用于表 IXa。**

**图 13: 对 SVHN 和 STL10 的触发器大小的影响。预训练数据集为 CIFAR10。**

---

希望这些改进能帮助你更好地呈现你的研究内容。如果有任何进一步的需求，请告诉我！