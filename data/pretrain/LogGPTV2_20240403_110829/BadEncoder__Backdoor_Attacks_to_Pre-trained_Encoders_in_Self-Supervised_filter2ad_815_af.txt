A. A. Efros, “Context encoders: Feature learning by
inpainting,” in CVPR, 2016.
[17] M. Noroozi and P. Favaro, “Unsupervised learning of
[33] K. Liu, B. Dolan-Gavitt, and S. Garg, “Fine-pruning:
Defending against backdooring attacks on deep neural
networks,” in RAID, 2018.
[34] Y. Gao, C. Xu, D. Wang, S. Chen, D. C. Ranasinghe,
and S. Nepal, “Strip: A defence against trojan attacks on
deep neural networks,” in ACSAC, 2019.
[35] Y. Liu, W.-C. Lee, G. Tao, S. Ma, Y. Aafer, and X. Zhang,
“Abs: Scanning neural networks for back-doors by arti-
ficial brain stimulation,” in CCS, 2019.
[36] B. G. Doan, E. Abbasnejad, and D. C. Ranasinghe,
“Februus: Input purification defense against trojan at-
tacks on deep neural network systems,” in ACSAC, 2020.
[37] P.-y. Chiang, R. Ni, A. Abdelkader, C. Zhu, C. Studor,
and T. Goldstein, “Certified defenses for adversarial
patches,” in ICLR, 2019.
[38] A. Levine and S. Feizi, “(de) randomized smoothing for
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:37:52 UTC from IEEE Xplore.  Restrictions apply. 
2056
certifiable defense against patch attacks,” in NeurIPS,
2020.
[39] J. H. Metzen and M. Yatsura, “Efficient certified defenses
against patch attacks on image classifiers,” in ICLR,
2021.
[40] B. Wang, X. Cao, J. Jia, N. Z. Gong et al., “On certify-
ing robustness against backdoor attacks via randomized
smoothing,” in CVPR 2020 Workshop on Adversarial
Machine Learning in Computer Vision, 2020.
[41] Z. Zhang, J. Jia, B. Wang, and N. Z. Gong, “Backdoor
attacks to graph neural networks,” in SACMAT, 2021.
Cleanse,”
https://github.com/bolunwang/
Meta-Nerual-Trojan-Detection.
https://github.com/AI-secure/
https://github.com/inspire-group/
[42] “Neural
backdoor.
[43] “MNTD,”
[44] “PatchGuard,”
PatchGuard.
[45] A. Radford, K. Narasimhan, T. Salimans,
and
I. Sutskever, “Improving language understanding by
generative pre-training,” 2018.
[Online]. Available:
https://s3-us-west-2.amazonaws.com/openai-assets/
research-covers/language-unsupervised/language
understanding paper.pdf
[46] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei,
and I. Sutskever, “Language models are unsupervised
multitask learners,” OpenAI blog, vol. 1, no. 8, p. 9, 2019.
[47] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Ka-
plan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,
A. Askell et al., “Language models are few-shot learn-
ers,” Arxiv:2005.14165, 2020.
[48] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhut-
dinov, and Q. V. Le, “Xlnet: Generalized autoregressive
pretraining for language understanding,” NeurIPS, 2019.
[49] W. Hu, B. Liu, J. Gomes, M. Zitnik, P. Liang, V. Pande,
and J. Leskovec, “Strategies for pre-training graph neural
networks,” in ICLR, 2020.
[50] J. Qiu, Q. Chen, Y. Dong, J. Zhang, H. Yang, M. Ding,
K. Wang, and J. Tang, “Gcc: Graph contrastive coding
for graph neural network pre-training,” in KDD, 2020.
[51] J. Dai, C. Chen, and Y. Li, “A backdoor attack against
lstm-based text classification systems,” IEEE Access,
vol. 7, pp. 138 872–138 878, 2019.
[52] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and
V. Shmatikov, “How to backdoor federated learning,” in
AISTATS, 2020.
[53] Z. Xi, R. Pang, S. Ji, and T. Wang, “Graph backdoor,”
in Usenix Security, 2021.
[54] C. Liao, H. Zhong, A. Squicciarini, S. Zhu, and D. Miller,
“Backdoor embedding in convolutional neural network
models via invisible perturbation,” in CODASPY, 2020.
[55] A. Saha, A. Subramanya, and H. Pirsiavash, “Hidden
trigger backdoor attacks,” in AAAI, 2020.
[56] A. Turner, D. Tsipras, and A. Madry, “Label-consistent
backdoor attacks,” arXiv preprint arXiv:1912.02771,
2019.
[57] S. Li, B. Z. H. Zhao, J. Yu, M. Xue, D. Kaafar, and
H. Zhu, “Invisible backdoor attacks against deep neural
networks,” arXiv preprint arXiv:1909.02742, 2019.
[58] T. J. L. Tan and R. Shokri, “Bypassing backdoor detec-
tion algorithms in deep learning,” in EuroS&P, 2020.
[59] Y. Liu, X. Ma, J. Bailey, and F. Lu, “Reflection backdoor:
A natural backdoor attack on deep neural networks,” in
ECCV, 2020.
[60] A. Salem, R. Wen, M. Backes, S. Ma, and Y. Zhang,
“Dynamic backdoor attacks against machine learning
models,” arXiv preprint arXiv:2003.03675, 2020.
[61] N. Carlini and A. Terzis, “Poisoning and backdooring
contrastive learning,” arXiv preprint arXiv:2106.09667,
2021.
[62] X. Chen, A. Salem, M. Backes, S. Ma, and Y. Zhang,
“Badnl: Backdoor attacks against nlp models,” arXiv
preprint arXiv:2006.01043, 2020.
[63] X. Zhang, Z. Zhang, S. Ji, and T. Wang, “Trojaning
language models for fun and profit,” in EuroS&P, 2021.
[64] Y. Ji, X. Zhang, and T. Wang, “Backdoor attacks against
learning systems,” in CNS, 2017.
[65] Y. Ji, X. Zhang, S. Ji, X. Luo, and T. Wang, “Model-reuse
attacks on deep learning systems,” in CCS, 2018.
[66] A. Shafahi, W. R. Huang, M. Najibi, O. Suciu, C. Studer,
T. Dumitras, and T. Goldstein, “Poison frogs! targeted
clean-label poisoning attacks on neural networks,” in
NeurIPS, 2018.
[67] W. Guo, L. Wang, X. Xing, M. Du, and D. Song,
“Tabor: A highly accurate approach to inspecting and
restoring trojan backdoors in ai systems,” arXiv preprint
arXiv:1908.01763, 2019.
[68] B. Chen, W. Carvalho, N. Baracaldo, H. Ludwig, B. Ed-
wards, T. Lee, I. Molloy, and B. Srivastava, “Detecting
backdoor attacks on deep neural networks by activation
clustering,” in AAAI, 2019.
[69] H. Chen, C. Fu, J. Zhao, and F. Koushanfar, “Deepin-
spect: A black-box trojan detection and mitigation frame-
work for deep neural networks.” in IJCAI, 2019.
[70] D. Tang, X. Wang, H. Tang, and K. Zhang, “Demon
in the variant: Statistical analysis of dnns for robust
backdoor contamination detection,” in Usenix Security,
2021.
[71] E. Chou, F. Tram`er, and G. Pellegrino, “Sentinet: De-
tecting localized universal attacks against deep learning
systems,” in 2020 IEEE Security and Privacy Workshops
(SPW).
IEEE, 2020, pp. 48–54.
[72] M. Weber, X. Xu, B. Karlas, C. Zhang, and B. Li,
“Rab: Provable robustness against backdoor attacks,”
arXiv preprint arXiv:2003.08904, 2020.
[73] J. Jia, X. Cao, and N. Z. Gong, “Certified robustness of
nearest neighbors against data poisoning attacks,” arXiv
preprint arXiv:2012.03765, 2020.
[74] ——, “Intrinsic certified robustness of bagging against
data poisoning attacks,” in AAAI, 2021.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:37:52 UTC from IEEE Xplore.  Restrictions apply. 
2057
Algorithm 1: BadEncoder
i=1, and {ei}t
i=1.
Input: θ (model parameters of f), max epoch, lr
(learning rate), bs (batch size), Ds, {Ri}t
Output: θ′ (model parameters of f′).
θ′ ← θ
epoch ← 1
while epoch ≤ max epoch do
iter ← 1
while iter ≤ ⌊|Ds|/bs⌋ do
batch ← MINIBATCH(Ds, bs, step)
θ′ = θ′ − lr · ∇θ′L(θ′, batch,{Ri}t
iter ← iter + 1
end while
epoch ← epoch + 1
i=1,{ei}t
i=1)
end while
return θ′
TABLE XIII: Results of using different parameter settings
to pre-train an image encoder, craft a backdoored image
encoder, and train a downstream classifier.
Stage
Parameter
Pre-training
an image
encoder
#epochs
Learning rate
#epochs
Learning rate
Crafting a
backdoored
image
encoder
Trigger
#epochs
Learning rate
#neurons
in the two
hidden layers
Training a
downstream
classifier
Value
500
1,000
1,500
1 × 10−3
5 × 10−4
1 × 10−4
1 × 10−3
5 × 10−4
1 × 10−4
CA (%) BA (%) ASR (%)
74.61
76.14
75.95
76.14
73.41
74.58
76.14
76.14
76.14
76.14
76.14
76.14
A white square
76.14
A random trigger 76.14
77.39
76.14
76.14
76.96
76.73
76.14
76.51
76.13
76.14
99.66
99.73
99.86
99.73
99.93
99.89
99.79
99.84
99.73
99.73
99.80
97.76
99.73
100.0
99.96
99.80
99.73
93.96
99.74
99.73
99.90
99.90
99.73
75.11
76.18
76.65
76.18
75.90
75.74
76.43
76.45
76.18
76.18
76.54
75.53
76.18
75.83
77.55
76.39
76.18
76.66
76.60
76.18
76.98
76.64
76.18
1 × 10−3
5 × 10−4
1 × 10−4
[128,64]
[256,128]
[512,256]
100
150
200
100
300
500
Fig. 4: The cumulative distribution functions (CDFs) of the
cosine similarity scores between the feature vector of the
reference input and those of the trigger-embedded inputs
produced by the clean image encoder and backdoored
image encoder.
TABLE XII: Comparing BadEncoder with LBA [12].
Target Downs-
tream Dataset
LBA
Fraction of Tes-
ting Images (%)
ASR (%)
BadEncoder
ASR (%)
GTSRB
SVHN
STL10
10
50
10
50
10
50
7.14
12.63
0.19
24.16
9.63
12.77
98.73
98.81
99.15
99.09
99.39
99.59
(a) Truck 0
(b) Truck 1
(c) Truck 2
Fig. 6: The reference inputs for STL10 used in Table V.
Truck 0 is the default reference input.
(a) CIFAR10
(b) SVHN
(c) GTSRB
(d) STL10
Fig. 5: The default reference inputs for CIFAR10, SVHN,
GTSRB, and STL10.
(a) GTSRB
(b) SVHN
Fig. 7: The impact of the shadow dataset size on our
BadEncoder when the target downstream datasets are
GTSRB (left) and SVHN (right). The shadow dataset is
a subset of the pre-training dataset, which is CIFAR10.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:37:52 UTC from IEEE Xplore.  Restrictions apply. 
2058
00.20.40.60.81.0CosineSimilarity00.20.40.60.81.0CDFCleanimageencoderBackdooredimageencoder020406080100ShadowDatasetSize(%)00.20.40.60.81.0CABAASR020406080100ShadowDatasetSize(%)00.20.40.60.81.0CABAASR(a) GTSRB
(b) SVHN
(c) CIFAR10
Fig. 8: The impact of the trigger size on our BadEncoder for different target downstream datasets when the pre-training
dataset is STL10.
(a) Airplane
(b) Truck
(c) Horse
Fig. 9: The reference inputs for attacking three target
classes in STL10 simultaneously, which are used in Ta-
ble VI.
(a) SVHN
(b) GTSRB
(c) STL10
Fig. 12: The reference inputs for attacking OpenAI’s CLIP
in the zero-shot classifier scenario, which are used in
Table IXb.
(a) SVHN
(b) GTSRB
(c) STL10
Fig. 10: The reference inputs for attacking the image
encoder pre-trained on ImageNet by Google, which are
used in Table VIII.
(a) SVHN
(b) GTSRB
(c) STL10
Fig. 11: The reference inputs for attacking OpenAI’s CLIP
in the multi-shot classifier scenario, which are used in
Table IXa.
(a) SVHN
(b) STL10
Fig. 13: The impact of the trigger size for SVHN and
STL10. The pre-training dataset is CIFAR10.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:37:52 UTC from IEEE Xplore.  Restrictions apply. 
2059
3x35x510x10TriggerSize00.20.40.60.81.0CABAASR3x35x510x10TriggerSize00.20.40.60.81.0CABAASR3x35x510x10TriggerSize00.20.40.60.81.0CABAASR3x35x510x10TriggerSize00.20.40.60.81.0CABAASR3x35x510x10TriggerSize00.20.40.60.81.0CABAASR