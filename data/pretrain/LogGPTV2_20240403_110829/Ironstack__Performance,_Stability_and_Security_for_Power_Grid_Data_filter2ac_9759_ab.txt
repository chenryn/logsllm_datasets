4.5 Transparent end-to-end encryption
Where raw data is streamed unprotected from end-
hosts,
it is possible for IronStack to intercept these
packets and apply encryption to them, such that the
resultant ﬂow is resistant to snooping and modiﬁca-
tion while in transit. The encryption is automatically
stripped at the IronStack-controlled switch immediately
connected to the end-host, which then expects and sees
the data in its raw form. Encryption can be applied in
parallel with the abovementioned techniques for avoid-
ing adversaries.
5 Backward compatibility and
retroﬁtting
A practical consideration in the wide-scale deployment
of IronStack is the potential cost involved in such an
endeavor. While some IronStack functionalities can be
supported natively on existing end hosts, others require
substantial data packet processing that must be done
with special software. During our engineering design
process, we examined several options that could enable
the use of all IronStack capabilities: retroﬁtting existing
systems with new software, augmenting the IronStack
controller with mechanisms to perform packet process-
ing on behalf of the client, and constructing support
middleboxes that can handle packet processing on be-
half of the client. Table 5.1 summaries the various
IronStack designs and their tradeoﬀs:
original
retroﬁt controller middlebox
la-
minimal
tency
maximal
bandwidth
hybrid
scheme
blacklisting
random
paths
transparent
encryption
deployment
ease
scalable
cost
no
no
no
no
no
no
NA
NA
NA
yes
yes
yes
yes
yes
yes
maybe
maybe
maybe
yes
yes
yes
yes
yes
yes
yes
yes
yes
hard
easy
yes
high
no
low
medium
yes
medium
Table 5.1: IronStack engineering design tradeoﬀs.
5.1 Systems retroﬁtting
A systems retroﬁt will involve updating the operating
systems kernel of each aﬀected host to recognize Iron-
Stack packets and process them correctly before deliv-
ering data onto their target applications. This is the
cleanest and most scalable method of supporting Iron-
Stack functionality, since it does not involve the use
of computational resources beyond the communicating
endpoints. However, such support is contingent on the
feasibility and acceptability of modifying such operating
systems kernels. For many types of embedded systems
– particularly those that run modern grid sensors – or
for systems running proprietary operating systems, it is
diﬃcult to modify existing kernel code to handle Iron-
Stack packet processing. Even where such modiﬁcation
is possible, it may not be acceptable to system admin-
istrators, who must now be liable for a larger software
attack surface to support IronStack.
5.2 Using the IronStack controller
The IronStack controller is capable of limited data
transfers between a switch’s control plane and its data
plane. It is thus possible for IronStack to entirely han-
dle packet processing on behalf of the end hosts. The
advantage of such an approach is that it enables all Iron-
Stack capabilities to the end hosts without necessitating
any kind of hardware or software change, with the eﬀect
that end hosts are completely unaware of an underly-
ing change to the network. In light of the diﬃculties in
modifying individual network equipment operating sys-
tem kernels as mentioned in the preceding section, such
transparency is valuable since changes are localized to
the switching equipment and its controller. However,
this approach is not scalable because the bandwidth for
transfers between the data plane and the control plane
4
is limited. Consequently, it is not possible to service too
many concurrent IronStack function requests simulta-
neously.
where the data is written to disjoint network paths as
opposed to independent disks.
5.3 Middleboxes for packet processing
A more scalable approach for IronStack would be to al-
low the controller to perform some limited functions,
but to delegate packet processing duties to an exter-
nal dedicated entity. This dedicated entity could be a
single consumer-grade computer, a cluster of servers on
the same network rack, or an array of NetFPGA boards,
scaling according to equipment availability the expected
processing load generated by network users. This ap-
proach maximizes IronStack controller responsiveness
and scales very well to high ﬂow counts, however it re-
quires more dedicated equipment and power to operate
in aggregate.
6 Implementation status
As at this time of writing, we have completed implemen-
tation on a basic prototype of the IronStack controller.
This prototype system was recently demonstrated at
the MIT Energy Conference and the ARPA-E Energy
Summit that were held in February 2014. In our exper-
imental and demonstration setups, the controller ran
on a Dell Optiplex 990 with 8 cores and 16Gb of RAM.
Our controller handled all required functions and packet
processing in their entirety, and interfaced with a Dell
S4810 high capacity OpenFlow switch. This switch was
selected for its ability to partition into eight virtual
switches, from which various network topologies could
be explored. Two TCP sensor streams were sent over
this network, one with multipath protection transpar-
ently provided by IronStack and one without. We were
able to show that the protected data ﬂow was immune
to network disruptions, while the unprotected ﬂow suf-
fered from stops and starts as the network underwent
fault remediation.
7 Related Work
Some work has been accomplished that have relevance
to the IronStack system. RAID [2] is the classic work
that explores various techniques of storing data on
independent disks for the purpose of improving redun-
dancy and performance. Data storage using RAID
is largely organized into standardized schemes, with
RAID0 corresponding to no redundancy (thus allowing
the full utilization of all independent disks), RAID1
corresponding to direct mirroring (simple replication
of data across multiple disks) and higher RAID levels
corresponding to more complex data striping meth-
ods. These ideas have direct counterparts in IronStack
5
SPAIN [4] is an Ethernet-based solution that imple-
ments redundancy by mapping strategically computed
paths to separate VLANs. Functionally, the objectives
of SPAIN and IronStack are similar on the perfor-
mance and assurance end: they both provide increased
bisection bandwidth and resistance to network failures.
However, SPAINs implementation relies on static, pre-
installed paths, and cannot adapt to substantial net-
work topology changes. Consequently, while robust to
individual failures, SPAIN is of limited use in a power
grid data network where topology changes due to power
or equipment outages are likely. Furthermore, SPAIN
does not perform data packet processing and thus can-
not feature the continuum of latency/bandwidth trade-
oﬀs that is attainable in IronStack. SPAIN is also a
not a security technology by design and does not im-
plement blacklisting, signals-intelligence obfuscation,
or transparent end-to-end encryption.
Multipath TCP [5], like IronStack, explores the use of
multiple paths to improve overall connection perfor-
mance. Conceptually, MPTCP takes a stream of data
and distributes it across multiple network interfaces,
where each network interface would ideally lead to a
diﬀerent connecting path to the destination. It is criti-
cal to note that MPTCP works on the L3 network layer
and is agnostic to the underlying physical communica-
tion paths, so in fact the multiple paths as idealized by
the standard could really be tunneling over the same
L2 physical layer links. While MPTCP enjoys the cost
convenience of not needing any modiﬁcations on the
existing network, it does require multihoming on de-
vices that wish to take advantage of it. Multihoming
may not be possible on many devices that cannot be
outﬁtted with a second network interface card. Fur-
thermore, support for MPTCP is sparse [3] at best,
and only caters to the TCP protocol. MPTCP also
does not have mechanisms to implement blacklisting.
IronStack solves all of these problems; it does not have
any of MPTCP’s drawbacks since it operates on the L2
layer, and can physically ensure that path diversity or
path constraints are satisﬁed.
SDN-based solutions for robust networking have also
been examined. FatTire [6] is a programming language
that allows users to specify network redundancy levels,
as well as the speciﬁc paths that their data packets
should transit in a network. The program is then ef-
ﬁciently compiled down to OpenFlow rules that get
installed on network switches. This approach naturally
facilitates blacklisting and implementing seamless net-
work link failovers. However, it requires substantial
domain-speciﬁc knowledge to operate and write in the
language. Also, while FatTire allows for failover redun-
dancy, it can neither boost aggregational bandwidth
nor perform actions to improve data security.
Hedera [8] is an example of a dynamic ﬂow scheduler
that actively schedules multi-stage switching fabrics in
order to improve bisectional bandwidth.
It works by
collecting ﬂow information from all constituent network
switches and maintaining a global view of the network
in order to intelligently re-route traﬃc around bottle-
necks. Again, as with most preceding work, Hedera
does not process packets and is thus unable to imple-
ment any of the packet striping schemes. It also does
not have mechanisms to blacklist network switches or
obfuscate signals intelligence of network ﬂows.
8 Future work
In the future, we would like to make IronStack still
easier to use by completely removing the need to spec-
ify redundancy and bandwidth parameters. The sys-
tem will instead use machine learning techniques to au-
tomatically tune the existing ﬂows subject to restric-
tions imposed by security policies.
It is anticipated
that this change can attain much higher operating eﬃ-
ciencies than manual tuning. We also plan to augment
IronStack with TCP-R [7], a capability orthogonal to
our network-level assurance that provides software-level
fault tolerance. This vastly increases the robustness of
applications, since they would be protected from both
hardware and software failures. On the scalability fron-
tier, we would like to build custom hardware accelera-
tors that will function as plug-and-play cards to replace
software packet processing in the IronStack controller.
This will greatly improve the service capacity of our
system. Finally, we also plan to write software modules
that will provide applications a considerable degree of
automatic network-level protection against malformed
or dangerous data.
9 Conclusion
In this paper we presented IronStack, a novel OpenFlow
switch controller that provides high performance, high
assurance and high security guarantees for power grid
data networks. IronStack is incrementally deployable,
necessitating minimal upgrade investment costs beyond
the gradual transition to OpenFlow-capable hardware.
It is also backward-compatible with existing hardware
and software, requiring little conﬁguration and mainte-
nance. Finally, IronStack is also scalable and can thus
6
be used in diverse networking scenarios. Our proto-
type system featuring a controller-only implementation
was recently demonstrated at the MIT Energy Con-
ference and the ARPA-E Energy Summit, and showed
that our techniques were sound and practical. We be-
lieve that IronStack represents a fundamental engineer-
ing advancement in data networks for the power grid,
and can be an important tool in the grand scheme of
modernizing the power industry.
10 Acknowledgements
This work is supported by grants from DARPA, NSF
and DoE/ARPA-E.
11 References
[1] Gjermundrod, Harald, et al. ”GridStat: A ﬂex-
ible QoS-managed data dissemination framework for
the power grid.” IEEE Transactions on Power Delivery
24.1 (2009): 136.
[2] Patterson, David A., Garth Gibson, and Randy H.
Katz. A case for redundant arrays of inexpensive disks
(RAID). Vol. 17. No. 3. ACM, 1988.
[3] Kostopoulos, Alexandros, et al. ”Towards multi-
path TCP adoption:
challenges and opportunities.”
Next Generation Internet (NGI), 2010 6th EURO-NF
Conference on. IEEE, 2010.
[4] Mudigonda, Jayaram, et al., SPAIN: COTS Data-
Center Ethernet
for Multipathing over Arbitrary
Topologies. NSDI. 2010.
[5] Ford, Alan, et al. TCP Extensions for Multi-
path Operation with Multiple Addresses: draft-ietf-
mptcp-multiaddressed-03. No.
Internet draft (draft-
ietf-mptcp-multiaddressed-07). Roke Manor, 2011.
[6] Reitblatt, Mark, et al. ”FatTire: declarative fault
tolerance for software-deﬁned networks.” Proceedings
of the second ACM SIGCOMM workshop on Hot topics
in software deﬁned networking. ACM, 2013.
[7] Surton, Robert, et al. http://www.cs.cornell.edu/ burgess/tcpr/
[8] Al-Fares, Mohammad, et al. ”Hedera: Dynamic
Flow Scheduling for Data Center Networks.” NSDI.
Vol. 10. 2010.
[9] Summary: Top 10 Global Survey Results, Cisco
Connected World Technology Report on Big Data.
http://www.cisco.com/c/dam/en/us/solutions/enterprise/connected-
world-technology-report/Top-10-Survey-Results-
CCWTR-Big-Data.pdf
[10] POX. http://www.noxrepo.org/pox/about-pox/
[11] Floodlight OpenFlow controller. http://www.projectﬂoodlight.org/ﬂoodlight/
[12] Open vSwitch. http://openvswitch.org/