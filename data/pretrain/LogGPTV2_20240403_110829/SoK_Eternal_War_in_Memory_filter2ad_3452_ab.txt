the stack, it may be exploited to overwrite sensitive data,
such as a return address. Double-free is a special case of
the use-after-free vulnerability where the dangling pointer
is used to call free() again. In this case, the attacker
controlled contents of the new object will be interpreted
wrongly as heap metadata, which is also exploitable for
arbitrary memory writes [19].
Memory errors in general allow the attacker to read and
modify the program’s internal state in unintended ways. We
showed that any combination of the ﬁrst two steps in our
memory exploitation model can be used both to corrupt
internal data and to leak sensitive information. Furthermore,
more memory errors can be triggered by corrupting other
pointers. Programming bugs which make these errors possi-
ble, such as buffer overﬂows and double-frees, are common
in C/C++. When developing in such low-level languages,
both bounds checking and memory management are fully
the programmers responsibility, which is very error prone.
The above described errors are a violation of the Memory
Safety policy. C and C++ are inherently memory unsafe.
According to the C/C++ standards, writing an array be-
yond its bounds, dereferencing a null-pointer, or reading
an uninitialized variable result in undeﬁned behavior. Since
ﬁnding and ﬁxing all the programming bugs is infeasible,
we need automatic solutions to enforce Memory Safety in
existing programs or stop attacks in their later phases. The
policy mitigating a given set of attack steps is represented
in the ﬁgure by a colored area surrounding the white boxes.
The section number which discusses approaches enforcing
a given policy is also indicated in the ﬁgure, above the
policy names. We discuss approaches that try to stop any
exploit in the ﬁrst (two) steps by enforcing Memory Safety in
Section VI. In the following subsections we discuss the steps
of different exploit paths and identify the policies mitigating
the given step. As shown in the ﬁgure, some policies include
other, weaker policies.
B. Code corruption attack
The most obvious way to modify the execution of a
program is to use one of the abovementioned bugs to
overwrite the program code in memory. A Code Integrity
policy enforces that program code cannot be written. Code
Integrity can be achieved if all memory pages containing
code are set read-only, which is supported by all modern pro-
cessors. Unfortunately, Code Integrity does not support self-
modifying code or Just-In-Time (JIT) compilation. Today,
every major browser includes a JIT compiler for JavaScript
or Flash. For these use-cases, Code Integrity cannot be fully
enforced because there is a time window during which the
generated code is on a writable page.
C. Control-ﬂow hijack attack
Most often, memory corruption exploits try to take control
over the program by diverting its control-ﬂow. If Code
Integrity is enforced then this alternative option tries to use
a memory error to corrupt a code pointer in Step 3. Code
Pointer Integrity policies aim to prevent the corruption of
code pointers. We discuss the potential code pointer targets
and limitations of this policy in Section VIII-A.
Suppose the attacker can access and modify a return
address due to a buffer overﬂow. For a successful exploit,
the attacker needs to know the correct target value (i.e., the
address of the payload) as well. We represent this as a
separate fourth step in Figure 1. If the target of the control-
ﬂow hijack (the code address to jump to) is not ﬁxed, the
attacker cannot specify the target and the attack fails at this
step. This property can be achieved by introducing entropy
to memory addresses using Address Space Randomization.
We discuss techniques randomizing the address space in
Section V-A.
Suppose a code pointer (e.g., a function pointer) has
been successfully corrupted in the ﬁrst four steps. The
ﬁfth step is that the execution needs to load the corrupted
pointer into the instruction pointer. The instruction pointer
can only be updated indirectly by executing an indirect
control-ﬂow transfer instruction, e.g., an indirect function
call, indirect jump or function return instruction. Diverting
the execution from the control-ﬂow deﬁned by the source
code is a violation of the Control-ﬂow Integrity (CFI) policy.
In Section VIII-B, we cover protections that enforce different
CFI policies by detecting corruption at
indirect control
transfers.
The ﬁnal step of a control-ﬂow hijack exploit
is the
execution of attacker speciﬁed malicious code. Classic at-
tacks injected so-called shellcode into memory, and diverted
execution to this piece of code. This kind of exploitation
is prevented by the Non-executable Data policy which can
be enforced using the executable bit for memory pages
to make data memory pages, like the stack or the heap,
non-executable. A combination of Non-executable Data and
Code Integrity results in the W⊕X (Write XOR Execute) [4]
policy, stating that a page can be either writable or exe-
cutable, but not both. Practically all modern CPU support
setting non-executable page permissions, so combined with
non-writable code, enforcing W⊕X is cheap and practical.
However in the case of JIT compilation or self-modifying
code, W⊕X cannot be fully enforced. For the sake of
completeness, we note that another randomization approach,
Instruction Set Randomization (ISR) can also mitigate the
execution of injected code or the corruption of existing code
by encrypting it. But due to the support for page permissions,
the much slower ISR has become less relevant and because
of the limited space we will not cover it in more detail in
this paper.
5151
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:53:16 UTC from IEEE Xplore.  Restrictions apply. 
To bypass the non-executable data policy, attackers can
reuse existing code in memory. The reused code can be
an existing function (“return-to-libc” attack) or small in-
struction sequences (gadgets) found anywhere in the code
that can be chained together to carry out useful (malicious)
operations. This approach is called Return Oriented Pro-
gramming (ROP), because the attack chains the execution
of functions or gadgets using the ending return instructions.
Jump Oriented Programming (JOP) is the generalization
of this attack which leverages indirect jumps as well for
chaining. There is no policy which can stop the attack at this
point, since the execution of valid and already existing code
cannot be prevented. Recent research focuses on mitigating
techniques against code reuse only. Researchers propose
techniques to eliminate useful code chunks (for ROP) from
the code by the compiler [20], or by binary rewriting
[21]. While these solutions make ROP harder, they do not
eliminate all useful gadgets, and they do not prevent re-using
complete functions. For these reasons we will not cover these
techniques in more detail.
We classify a control-ﬂow hijacking attack successful as
soon as attacker-speciﬁed code starts to execute. To carry
out a meaningful attack, the attacker usually needs to make
system calls, and may need high level permissions (e.g., ﬁle
access) as well. We will not cover higher-level policies
which only conﬁne the attacker’s access,
including per-
missions, mandatory access control or sandboxing policies
enforced by SFI [22], XFI [23] or Native Client [24]. These
policies can limit the damage that an untrusted program (or
plugin) or an attacker can cause after compromising a trusted
program. Our focus is preventing the compromise of trusted
programs, typically with extensive access (e.g., an ssh/web
server).
D. Data-only attack
Hijacking control-ﬂow is not
the only possibility for
a successful attack. In general, the attacker’s goal is to
maliciously modify the program logic to gain more control,
to gain privileges, or to leak information. This goal can be
achieved without modifying data that is expliclity related to
control-ﬂow. Consider, for instance, the modiﬁcation of the
isAdmin variable via a buffer overﬂow after logging into
the system with no administrator privileges.
bool isAdmin = false;
...
if (isAdmin) // do privileged operations
These program speciﬁc attacks are also called “non-control-
data attacks” [25] since neither code nor code pointers
(control data) are corrupted. The target of the corruption can
be any security critical data in memory, e.g., conﬁguration
data, the representation of the user identity, or keys.
The steps of this attack are similar to the previous one
except for the target of corruption. Here, the goal is to
corrupt some security critical variable in Step 3. Since
security critical is a semantic deﬁnition, the integrity of all
variables has to be protected in order to stop the attack in
this step. We call this policy Data Integrity, which naturally
includes Code Integrity and Code Pointer Integrity. Data
Integrity approaches try to prevent the corruption of data
by enforcing only some approximation of Memory Safety.
We cover techniques enforcing such policies under VII-A.
As in the case of code pointers, the attacker needs to know
what should replace the corrupted data. Acquisition of this
knowledge can be prevented by introducing entropy into the
representation of all data using Data Space Randomization.
Data Space Randomization techniques extend and general-
ize Address Space Randomization, and we cover them in
Section V-B.
Similar to code pointer corruption, data-only attacks will
succeed as soon as the corrupted variable is used. Using the
running example, the if (isAdmin) statement has to be
successfully executed without detecting the corruption. As
the generalization of Control-ﬂow Integrity, the use of any
corrupted data is the violation of Data-ﬂow Integrity. We
cover enforcing this policy under Section VII-B.
E. Information leak
We showed that any type of memory error might be
exploited to leak memory contents, which would otherwise
be excluded from the output. This is typically used to cir-
cumvent probabilistic defenses based on randomization and
secrets. Real-world exploits bypass ASLR using information
leaks [13], [26]. The only policy beyond Memory Safety
that might mitigate information leakage is full Data Space
Randomization. We will discuss in Section V how effective
Data Space Randomization is and how information leakage
can be used to bypass other probabilistic techniques which
build on secrets.
III. CURRENTLY USED PROTECTIONS AND REAL WORLD
EXPLOITS
The most widely deployed protection mechanisms are
stack smashing protection, DEP/W⊕X and ASLR. The
Windows platform for instance, also offers some special
mechanisms e.g., for protecting heap metadata and exception
handlers (SafeSEH and SEHOP).
Stack smashing protection [2] detects buffer overﬂows of
local stack-based buffers, which overwrite the saved return
address. By placing a random value (called cookie or canary)
between the return address and the local buffers at function
entries, the integrity of the cookie can be checked before the
return of the function and thus the overﬂow can be detected.
SafeSEH and SEHOP also validate exception handler point-
ers on the stack before they are used, which makes them,
together with stack cookies, a type of Control-ﬂow Integrity
solution. These techniques provide the weakest protection:
they place checks only before a small subset of indirect
5252
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:53:16 UTC from IEEE Xplore.  Restrictions apply. 
jumps, focusing checking the integrity of only some speciﬁc
code pointers, namely saved return addresses and exception
handler pointers on the stack. Furthermore, the checks can be
bypassed. Cookies, for instance, can detect a buffer overﬂow
attack but not a direct overwrite (e.g., exploiting an indexing
error).
DEP/W⊕X can protect against code injection attacks, but
does not protect against code reuse attacks like ROP. ROP
exploits can be generated automatically [27], and large code
bases like the C library usually provide enough gadgets
for Turing-completeness [10], [11]. ASLR provides the
most comprehensive protection as the most widely deployed
Address Space Randomization technique. It can randomize
the locations of various memory segments, including data
and code, so even if the attacker wants to reuse a gadget its
location will be random. While some ASLR implementa-
tions have speciﬁc weaknesses (e.g., code regions are left in
predictable locations, de-randomization attacks are possible
due to the low entropy), the fundamental attack against it is
information leakage [12].
As described in the attack model of the previous section,
any memory corruption error can be converted into an
information leak vulnerability, which can be used to obtain
current code addresses. The leaked addresses are needed to
construct the ﬁnal exploit payload. When attacking remote
targets (i.e., servers), getting back this information used to
be very challenging. Today, however, it is not a problem for
a number of client targets. Web browsers, PDF viewers and
ofﬁce applications run user controlled scripts (JavaScript,
ActionScript, VBScript), which can be used to dynamically
construct exploit payloads at run-time on the target machine.
Table I lists some recent exploits published by VUPEN [28],
which use information leaks and ROP to bypass ASLR and
W⊕X. In all examples, the control ﬂow is hijacked at an
indirect call instruction (after corrupting a function pointer or
the vtable), so stack cookies are not an issue. In all cases, the
address leakage is done by exploiting an arbitrary memory
write in order to corrupt another pointer which is read later
(the fourth column gives more hints). One common way of
leaking out memory contents is by overwriting the length
ﬁeld of a (e.g., JavaScript) string object before reading it
out (in the user script). As shown in the last column, in
case of browser targets, user scripting is used to leak current
addresses and to construct the exploit, while in case of
ProFTPD, the leaked information is sent back on the network
by corrupting a pointer to a status message.
IV. APPROACHES AND EVALUATION CRITERIA
The previously identiﬁed protection techniques can be
divided into two main categories: probabilistic and deter-
ministic protection. Probabilistic solutions, e.g., Instruction
Set Randomization, Address Space Randomization, or Data
Space Randomization, build on randomization or encryption.
All other approaches enforce a deterministic safety policy
by implementing a low-level reference monitor [29]. A
reference monitor observes the program execution and halts
it whenever it is about to violate the given security policy.
Traditional reference monitors enforce higher level policies,
such as ﬁle system permissions, and are implemented in the
kernel (e.g., system calls).
Reference monitors enforcing lower level policies, e.g.,
Memory Safety or Control-ﬂow Integrity, can be imple-
mented efﬁciently in two ways: in hardware or by embed-
ding the reference monitor into the code. For instance, the
W⊕X policy (Code Integrity and Non-executable Data) is
now enforced by the hardware, as modern processors support
both non-writable and non-executable page permissions.
Hardware support for a security policy results in negligible
overhead. The alternative to hardware support is adding the
reference monitor dynamically or statically to the code.
Since adding new features to the hardware is unrealistic,
from this point we focus only on solutions which transform
existing programs to enforce various policies. Dynamic
(binary) instrumentation (e.g., Valgrind [30], PIN [31],
DynamoRIO [32], or libdetox [33]) can be used to dy-
namically insert safety checks into unsafe binaries at run-
time. Dynamic binary instrumentation supports arbitrary
transformations but introduces some additional slowdown
due to the dynamic translation process. Simple reference
monitors, however, can be implemented with low overhead
(e.g., a shadow stack costs less than 6.5% performance
for SPEC CPU2006 in [33]). More sophisticated reference
monitors like taint checking [34] or ROP detectors [35]
result in overheads that exceed 100% and are unlikely to be
deployed in practice. Static instrumentation inlines reference
monitors statically. This can be done by the compiler or
by static binary rewriting. Inline reference monitors can
implement any safety policy and are usually more efﬁcient
than dynamic solutions, since the instrumentation is not
carried out at run-time.
Next, we discuss the main properties and requirements
for solutions enforcing low-level policies. These properties
determine the practicality of a proposed method, more
precisely, whether or not it is suitable for wide adoption.
We set up our requirements for practicality while discussing
a given property.
A. Protection
Enforced policy. The strength of the protection is deter-
mined by the policy it enforces. The exact policy that
a solution enforces determines its effectiveness. Different
techniques enforce different types of policies (e.g., Memory
Safety or Data-ﬂow Integrity) and at different levels. The
practical utility of a policy can be described by the attacks
it can protect against, out of the four we have identiﬁed.
Subtle differences in the policies allow or prevent individual
attacks. The accuracy of an approach is determined by the
relation between false negatives and false positives.
5353
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:53:16 UTC from IEEE Xplore.  Restrictions apply. 
Software
Adobe Flash
CVE ID
CVE-2011-0609
CVE-2012-0003 Windows Multimedia
Library (affecting IE)
ProFTPD
CVE-2011-4130
Vulnerability
JIT type confusion
Heap buffer overﬂow
Use-after-free
CVE-2012-0469 Mozilla Firefox
Use-after-free
Address leakage
Read an IEEE-754 number
Read a string after overwriting
its length
Overwrite the “226 Transfer
Complete” message
Read a string after overwriting
its length