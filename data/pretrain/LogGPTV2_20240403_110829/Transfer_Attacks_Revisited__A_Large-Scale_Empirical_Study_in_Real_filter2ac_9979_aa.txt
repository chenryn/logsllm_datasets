title:Transfer Attacks Revisited: A Large-Scale Empirical Study in Real
Computer Vision Settings
author:Yuhao Mao and
Chong Fu and
Saizhuo Wang and
Shouling Ji and
Xuhong Zhang and
Zhenguang Liu and
Jun Zhou and
Alex X. Liu and
Raheem Beyah and
Ting Wang
2022 IEEE Symposium on Security and Privacy (SP)
Transfer Attacks Revisited: A Large-Scale
Empirical Study in Real Computer Vision Settings
Yuhao Mao∗†, Chong Fu∗, Saizhuo Wang∗, Shouling Ji∗(u), Xuhong Zhang∗‡(u),
Zhenguang Liu∗∗, Jun Zhou§, Alex X. Liu§, Raheem Beyah¶, Ting Wang(cid:107)
∗Zhejiang University, †ETH Z¨urich, ‡Zhejiang University NGICS Platform, ∗∗Zhejiang Gongshang University, §Ant Group,
¶Georgia Institute of Technology, (cid:107)Pennsylvania State University
3
8
7
3
3
8
9
.
2
2
0
2
.
4
1
2
6
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
2
2
0
2
©
0
0
.
1
3
$
/
2
2
/
9
-
6
1
3
1
-
4
5
6
6
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
2
2
0
2
{yuhaomao, fuchong, szwang, sji, zhangxuhong}@zju.edu.cn, PI:EMAIL, zhoujun@antﬁn.com,
PI:EMAIL, PI:EMAIL, PI:EMAIL
Abstract—One intriguing property of adversarial attacks is
their “transferability” – an adversarial example crafted with
respect to one deep neural network (DNN) model is often found
effective against other DNNs as well. Intensive research has
been conducted on this phenomenon under simplistic controlled
conditions. Yet, thus far there is still a lack of comprehensive
understanding about transferability-based attacks (“transfer at-
tacks”) in real-world environments.
To bridge this critical gap, we conduct the ﬁrst large-scale
systematic empirical study of transfer attacks against major
cloud-based MLaaS platforms, taking the components of a real
transfer attack into account. The study leads to a number of
interesting ﬁndings which are inconsistent to the existing ones,
including: (i) Simple surrogates do not necessarily improve real
transfer attacks. (ii) No dominant surrogate architecture is found
in real transfer attacks. (iii) It is the gap between posterior
(output of the softmax layer) rather than the gap between
logit (so-called κ value) that increases transferability. Moreover,
by comparing with prior works, we demonstrate that transfer
attacks possess many previously unknown properties in real-
world environments, such as (i) Model similarity is not a well-
deﬁned concept. (ii) L2 norm of perturbation can generate high
transferability without usage of gradient and is a more powerful
source than L∞ norm. We believe this work sheds light on the
vulnerabilities of popular MLaaS platforms and points to a few
promising research directions. 1
I. INTRODUCTION
Deep neural networks (DNNs) achieve tremendous success
in a variety of application domains [20], [47], but they are
inherently vulnerable to adversarial examples (AEs) which are
malicious samples crafted to deceive target DNNs [26], [33],
[35], [44]. This vulnerability signiﬁcantly hinders their use in
security-sensitive domains.
Among the many properties of AEs, the transferability, that
an AE crafted with respect to one DNN also works against
other DNNs, is particularly intriguing. Leveraging this prop-
erty, the adversary could forge AEs using a surrogate DNN to
attack the target DNN without knowledge of the target. This
is highly dangerous because an increasing number of cloud-
based MLaaS platforms are deploying DNNs with public
API. Furthermore, given that years’ advances in technology
did not eliminate this vulnerability, it is highly likely that
transferability stems from the intrinsic properties of DNNs.
Understanding the phenomenon improves the interpretability
of DNNs in its own right as well.
Therefore, understanding the deciding factors of transfer-
ability and their working mechanisms has attracted intensive
researches [15], [19], [29], [34], [36], [39], [44]. However,
to the best of our knowledge, all systematic empirical studies
are conducted under controlled “lab” environments that are
too ideal to make the derived conclusions reliable in the real
environment. For instance, many studies [36], [37] give the
adversary access to the training data of the target model, and
some others [41] discuss surrogates with similar complexity
to the target model. Although there are studies that attack
deployed DNNs known as cloud models [29], their conclusions
are neither systematic nor informative enough, suffering from
the lack of adequate observations to do statistical tests and
suitable metrics to account for the difference between lab and
real settings. The difference between lab and real environment
includes:
• Complexity and architecture of the target. In the real
environment, neither the target’s complexity nor its architec-
ture is known to the attacker. The cloud model could be
arguably far more complicated than a surrogate and of an
uncommon architecture. Under this circumstance, the con-
clusions derived from academic target models may not hold,
calling for thorough examinations.
• Training of the target. In the real environment, the ad-
versary knows nothing about the training details of the target,
including the optimization hyperparameters and algorithms.
Additionally, the training datasets are far more complicated
and noisy in the real environment2, increasing the difﬁculty in
simulating a similar training environment at local.
• Structure of the input. The cloud models are designed
for high resolution images. Therefore, the cloud models may
not produce meaningful outputs for inputs from academic
datasets that are extremely low resolution like MNIST [25]
and CIFAR10 [22], as shown in Figure 1a. Conclusions made
on these toy datasets may not hold under this condition.
• Structure of
the output. Cloud models usually re-
turn multiple predictions and their corresponding conﬁdences,
1Code & Results: https://github.com/AlgebraLoveme/Transfer-Attacks-
Revisited-A-Large-Scale-Empirical-Study-in-Real-Computer-Vision-Settings
2Amazon declares they have access to billions of images daily and continue
to learn from new data [3].
© 2022, Yuhao Mao. Under license to IEEE.
DOI 10.1109/SP46214.2022.00147
1423
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:58 UTC from IEEE Xplore.  Restrictions apply. 
(a) 97.2% Text; 96.5%
Number; 96.5% Symbol.
(b) 91.7% Sports; 86.7%
Sphere; 78.9% Baseball.
Figure 1: AWS Rekognition’s predictions to two sample im-
ages. The left one does not contain the desired prediction (digit
zero) and the right one’s ground truth label (baseball) does not
get the highest score. They are not mistakes but metrics like
top-k accuracy would consider them as not good.
which are different from the logits returned by classiﬁers in lab
settings (multiple returns problem). In addition, cloud models
usually have a signiﬁcantly larger set of labels3 that are hard to
estimate and categorize locally (class inconsistency problem).
These differences, as depicted in Figure 1, add to the difﬁculty
in measuring the effectiveness of transfer attacks in a real-
world scenario.
To address these limitations, we conduct a large-scale
systematic empirical study on transfer attacks in the real
settings. To comprehensively explore the possible factors that
may affect the transferability, we combine different settings
of the attack components at local environment where AEs are
generated. To thoroughly evaluate the effectiveness of transfer
attacks and the robustness of real-world models, we select four
leading cloud-based Machine-Learning-as-a-Service (MLaaS)
platforms, namely Aliyun (Alibaba Cloud), Baidu Cloud, AWS
Rekognition and Google Cloud Vision as the targets.
Our contributions include:
• We identify the main gaps of evaluating transfer attack
in the real world, namely multiple returns problem and label
inconsistency problem, and extend existing metrics to address
these difﬁculties. To the best of our knowledge, these are the
ﬁrst metrics that can be reasonably applied to analyze transfer
attack on MLaaS systems.
• We conduct a systematic evaluation on the transferability
of adversarial attacks against four leading commercial MLaaS
platforms on two computer vision tasks: object classiﬁcation
and gender classiﬁcation, using datasets consisting of real-
world pictures in order to get meaningful insights (ImageNet
[16] and Adience [18], respectively). Based on the results of
our evaluation, we measure the robustness of the discussed
platforms and point out that while on average transfer at-
tack performs poorly, they can be systematically designed to
achieve stable success rates, e.g., use FGSM for the object
classiﬁcation. More detailed guidelines for the industry is
provided in Appendix X-F. We hope this will help the industry
fortify their models and improve their services.
• We explore the possible factors that may affect the trans-
ferability using 180 different settings as the combination of
components in a transfer attack. 36,000 AEs in total generated
from 200 seed images for each task are evaluated, making the
conclusions statistically adequate. We ﬁnd that some former
3Google shares an open dataset consisting of about 60M labels [7].
conclusions can be generalized to the real settings while some
others are inapplicable. A comparison between the former
conclusions and their real-world counterparts is shown in
Table I. Additionally, we demonstrate that transfer attacks
possess many previously unknown properties in real-world
environments. We believe that our ﬁndings will provide new
insights for future research.
II. BACKGROUND
In this section, we introduce the preliminaries of transfer
attacks, including the components that may affect the trans-
ferability.
A. Transfer Attack
Transfer attack involves adversarial attack and transfer-
ability. Speciﬁcally, a classiﬁer with parameters θ, namely
fθ, accepts an input x ∈ Rn and makes a prediction fθ(x).
The adversary wants to generate a maliciously perturbed input
ˆx = x + δ so that fθ(ˆx) (cid:54)= fθ(x). Typically, δ is required to
be human imperceptible, i.e., the size of δ is small under Lp
norm. Attack algorithms of this form are called adversarial
attacks [12], [19], [27], [30], [32], [38], [44], [46].
Formally,
if an AE ˆx crafted on fθ deceives another
model φθ(cid:48) with unknown parameters and architecture, i.e.,
φθ(cid:48)(ˆx) (cid:54)= φθ(cid:48)(x), then we say this AE transfers from model
fθ to φθ(cid:48). Utilizing this property, an adversary may generate
AEs based on fθ and transfer them to the target black-box
model φθ(cid:48). Typically, the label sets of the source model and
the target model are the same. This process is called transfer
attack [29], [36], [37]. Although query-based attacks [13],
[21] that request partial information about the targets are more
powerful, transfer attacks do not require any information about
the target and thus are more stealthy and economic.
According to the process of transfer attack, there are three
major components affecting the transferability of AEs: surro-
gate model, surrogate dataset and adversarial algorithm. We
elaborate on these components next.
B. Surrogate Model
Surrogate model is the model on which the adversary per-
forms white-box attacks. In general, it is expected to resemble
the target model to increase the transferability of the generated
AEs. The main factors affecting the similarity include:
(a) Pretraining. Target models, such as the ones on MLaaS
platforms, are trained on a sufﬁciently large dataset to achieve
a superior performance. Since attackers may not have enough
computing resources to train the surrogate model on a large
dataset, they may ﬁne-tune the surrogate model based on a
public model that is pretrained on a large dataset, as pretraining
generally improves a model’s accuracy [17], [50].
(b) Model Architecture. Various DNN architectures have
been designed to address different problems and improve
performance [20], [40], [43]. The ﬁtting functions that models
learned under different architectures could be different even