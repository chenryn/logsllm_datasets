### Improving the Usefulness of Tracing for Automated Analysis

#### 1. Introduction
Recent research, such as [25], has proposed enhancing observability in large-scale distributed systems through Sifter, a trace sampler that biases sampling decisions towards edge cases and rare workflows. Canopy, introduced by [21], is a comprehensive instrumentation, collection, and analysis framework that decouples these steps and allows dynamic feature extraction using a Domain-Specific Language (DSL). These efforts aim to drive the system towards responsiveness, which implies resilience [24] and elasticity [17]. Resilience, as defined by Laprie [24], is "the persistence of service delivery that can justifiably be trusted, even when facing changes." Elasticity, on the other hand, depends on the system's ability to scale horizontally with load and the provider's capacity to support such scaling with more (or fewer) resources.

#### 2. Challenges in Managing Distributed Infrastructures
The application of Artificial Intelligence for Information Technology Operations (AIOps) was introduced in 2016 [26] to automate and enhance IT operations. This approach addresses several challenges:
- The difficulty of manually managing distributed infrastructures and system states.
- The increasing volume of data that must be retained, creating numerous problems for operators.
- The growing distribution of infrastructure across geography and organizations, as seen in trends like cloud-first development and fog computing.

In this context, tracing analysis offers several interesting applications. For example, [30] used deep learning, trained on encoded traces, to detect anomalies in distributed tracing, particularly in cloud systems like OpenStack. This method aims to uncover features automatically and determine anomalous operation and traces. However, it requires a considerable amount of data for training and is limited to classifying traces as normal or abnormal, losing detail and interpretability.

#### 3. Research Questions
To address the challenges and improve resiliency during system operation, we posed two key research questions:
1. Is there any anomalous behavior in the system?
2. If yes, where?

Timely answers to these questions are crucial for maintaining system resilience. However, the sheer number of components and metrics, such as the number of incoming and outgoing requests, response times, downtimes, and error codes, requires significant data collection and processing capabilities. By conducting a post-mortem analysis on tracing data, we aim to find appropriate methods to answer these questions.

#### 4. Data and Methodology

##### 4.1 Dataset
The dataset provided by Huawei Research consists of two JSON Lines (JSONL) files, one for each day of operation. Each file contains around 200,000 spans, composing approximately 70,000 traces. Table 2 provides additional details on the dataset.

| File Date | Spans Count | Traces Count |
|-----------|-------------|--------------|
| June 28th | 190,202     | 64,394       |
| June 29th | 239,693     | 74,331       |

##### 4.2 Solution
We developed a two-step high-level approach to analyze the system. The tracing data feeds into the OpenTracing Processor (OTP), which derives higher-order metrics from the tracing data before storing them in a time-series database. OTP uses the Java Streaming Application Programming Interface (API) [36] to reconstruct traces and leverage parallelization capabilities. Service dependency graphs were extracted and processed using NetworkX [31], a Python-based framework for graph processing. OpenTSDB [43] was used to store the derived metrics, and Grafana [16] was used for visualization.

##### 4.3 Implementation
The other tool aims to perform metric analysis from the time-series database. Since our data is unlabeled, we used unsupervised learning algorithms. We chose Isolation Forests [29] for outlier detection in a multidimensional space. This component was developed as a collection of Python scripts in Jupyter Notebooks [38], using Pandas [45] for time-series data processing and Scikit-learn [8] for implementing Isolation Forests.

The main goal of this pair of components was to identify interesting time-frames in a large set of traces, relieving operators from unguided, sometimes exhaustive, searches using Zipkin, which is mostly limited to tracing visualization features. The code and documentation of our work are available on GitHub.1

#### 5. Results and Analysis

##### 5.1 Anomaly Detection
Figure 5 provides a representation of two time-frame samples of the same service, one for an anomalous region and another for a non-anomalous region, as tagged by our Data Analyser component. We set the time-series resolution to 10 minutes to avoid intervals with too few traces and considered the number of incoming and outgoing requests in conjunction with the average response time.

Figure 6 presents the comparison between detected anomalous and non-anomalous time-frames in Unix timestamp for a given service. The presence of outliers makes the difference between anomalous and non-anomalous operation clear. In the anomalous samples, points form a cluster near the chart origin, with some outliers in the upper-left and lower-right regions. In the non-anomalous samples, there is only a clustering of points near the chart origin.

##### 5.2 Trace Quality Analysis
After concluding the impossibility of deeper analysis due to incomplete tracing data, we questioned how to measure the quality of tracing. Our approach was to process the tracing data and feed it to the Data Analysis component without using a time-series database in between. This analysis was divided into two procedures:

1. **Compliance with OpenTracing Specification**: Every span structure complies with the specification, but the OpenTracing specification is not very strict, leading to issues like non-uniform time-stamp units. This leads to problems in time measurements, making it difficult to detect computationally in a deterministic manner.

2. **Temporal Coverage of Traces**: We checked if tracing covers the entire time of the root spans. For a simple example, if a trace has a root span of 100 ms and two child spans of 50 ms and 40 ms, the entire trace has a temporal coverage of (50 + 40)/100 = 90%. We applied this method to every trace and plotted the results, splitting them by service to determine the time coverage of tracing by service.

Figures 7 and 8 show the most common workflows from the anomalous and non-anomalous time-frames and the coverage histograms for two different services. The good coverage level (in the 60%âˆ’100% range) indicates that even relatively high temporal coverage is not a sufficient quality indicator for automated anomaly detection.

#### 6. Tracing Standard Limitations and Mitigations

The quality of our anomaly detection method was bounded by the quality of the data. Specifically, the tracing dataset presented problems in completeness and homogeneity, which are consequences of ambiguity in the tracing format specification standard. To mitigate these limitations, we propose a possible redefinition of the OpenTracing specification in Section 5.

#### 7. Conclusion
Improving the usefulness of tracing for automated analysis requires addressing the challenges of data quality, completeness, and homogeneity. By developing robust tools and methodologies, we can enhance the observability and resilience of large-scale distributed systems, ultimately leading to more reliable and efficient IT operations.

---

**References:**
[1] [Reference List]

**Footnotes:**
1. OpenTracing Processor (OTP): https://github.com/andrepbento/OpenTracingProcessor