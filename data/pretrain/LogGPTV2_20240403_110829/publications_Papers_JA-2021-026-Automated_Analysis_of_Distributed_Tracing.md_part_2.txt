approach in our suggestions to improve the useful-
ness of tracing for automated analysis—[25] propose Improving observability in a large-scale distributed
Sifter,atracesamplerbuilttobiasthesamplingdeci- systemservesthemainpurposeofdrivingthesystem
siontowardsedgecasesandrarework-flows.[21]pro- towardsresponsiveness[20],whichimpliesresilience
pose Canopy, a comprehensive instrumentation, col- [24]andelasticity[17].Elasticitydependsontheabil-
lection and analysis framework, that decouples those ity of the system to scale horizontally with load and
steps and allows dynamic feature extraction using a on the availability of the provider to support such
Domain-SpecificLanguage(DSL). scaling with more (or less) resources. According to
Artificial Intelligence for Information Technology Laprie[24],resilienceis:
(IT) Operations (AIOps), the application of artifi- “The persistence of service delivery that can justi-
cial intelligence to operations [18] was introduced in fiablybetrusted,whenfacingchanges”.
2016 [26] to develop new methods to automate and Since, in most implementations, elasticity relies
enhanceIToperations.Firstly,itrecognizesthediffi- on simple direct metrics, like CPU occupation or
cultyofmanuallymanagingdistributedinfrastructures response latencies, observing a system mostly serves
andsystemstate;secondly,theamountofdatathathas toensureresilience.Inourparticularcase,oursystem
to be retained keeps growing, creating a plethora of wasanOpenStackclusterthatHuaweiResearchuses
problemstooperators;thirdly,theinfrastructureitself fortestingpurposes,ofwhichwehadananonymised
is becoming more distributed across geography and OpenTracingdataset,limitingustouseourmethodin
organizations,asevidencedbytrends,likecloud-first anoff-linepostmortemanalysisofthecluster.Hence,
development and fog computing. In this new field, based on the existing traces, and the metrics derived
thereareafewinterestingapplicationstotracinganal- fromthem,likethenumberofincomingandoutgoing
ysis.[30]usedeeplearning,trainedonencodedtraces, requests,responsetimesorserviceerrorcodes,tolook
to detect anomalies with recourse to distributed trac- forthreatstoresilience.Toachievethisgoal,weasked
ing,in particularofcloudsystems (OpenStack).This tworesearchquestions:
approach attempts to uncover features automatically
1. Isthereanyanomalousbehaviourinthesystem?
and determine anomalous operation and traces. The
2. Ifyes,where?
amountofdataneededtotrainthesemodelsisconsid-
erable, and is limited to classifying a trace as normal One can easily see that a timely answer to these
orabnormal,losingdetailandinterpretability,i.e.,no questionsisveryhelpfulforresiliencyduringsystem
9 Page6of15 JGridComputing(2021)19:9
operation.However,thesheernumberofcomponents Table2 Datasetprovidedforthisresearch
and metrics, like the number of incoming and outgo-
Filedate June28th June29th
ingrequests,responsetimes,downtimes,errorcodes,
andsoon,requiresasignificantcapacitytocollectand Spanscount 190202 239693
processdata,butabovealltheneedtoknowwhereand Tracescount 64394 74331
whattolookfor.Bydoingthepostmortemanalysison
the tracing data, we aim to find appropriate methods
toanswerthesequestions. Algorithm1whichretrievestracingdatafromZipkin,
The data from Huawei Research consisted of two linksspanstorebuildTracesandServiceDependency
JSON Lines (JSONL) files, one file per day of oper- Graphs in memory, and finally, extracts pre-defined
ation. Table 2 provides additional details on the data. metrics from these structures, to store them in the
Eachfilehasaround200,000Spans,composingabout time-series database for visualization and analysis.
70,000 Traces. This file format is an extension of Currently,OTPisextractingandanalysingthefollow-
the JavaScript Object Notation (JSON) file type. In ingdatafromtracing,foragiventimeinterval:
JSONL,multipleJSONobjects,eachencodingaspan,
areseparatedbyanewlinecharacter. – Numberofincoming/outgoingcallsperservice.
– Averageresponsetimeperservice.
3.1Solution – Changes to service neighbourhood (both for
incomingandoutgoingcalls).
If necessary, for the sake of analysing the system,
one could extract other metrics, like service connec-
tion degrees, number of services traced over time, or
number of entering and departing services over time.
Wedidnotusetheseadditionalmetrics inthispaper,
becausewedidnotfindthemusefulfortheparticular
tracesunderanalysis.
3.2Implementation
We followed the two-step high-level approach of
Fig. 4. The tracing data feeds OTP, which derives
higherordermetricsfromtracingdata,beforestoring
them in a time-series database. The existing tracing
back-ends, can only export spans, leaving the recon-
struction of traces (connecting the spans as a tree) to
the user. OTP does this using Java Streaming Appli-
cation Program- ming Interface (API) [36], lever-
aging its parallelization capabilities. Service depen-
dency graphs, were extracted and processed using
NetworkX[31],aPython-basedframeworkforgraph
processing,containingalargesetofgraphalgorithms.
OpenTSB [43] was used to store the derived metrics
thatfollow.Tovisualizetheextractedmetrics,weused
Grafana[16].
Theothertoolaimstoperformmetricanalysisfrom
Beforewecouldrunthedataanalysistool,wehad thetime-seriesdatabase.Sinceourdataisunlabelled,
to extract metrics from tracing data and write them i.e.,ithasnoclassification;therefore,ouranalysisuses
intoatime-seriesdatabase.Forthisoperation,weused unsupervisedlearningalgorithms. We chose Isolation
JGridComputing(2021)19:9 Page7of15 9
Proposed approach
Data Analyser
OTP
Performs the analysis of the
Metrics gathering from
stored metrics and points
Traaacaes tracing data. Processed out service problems.
dMaMtMa
Fig.4 Proposedsolution
Forests [29], as the starting point, as it allows outlier intervals with too few traces—, and considered the
detection in a multidimensionalspace. We developed number of incoming and outgoing requests, in con-
this component as a collection of Python scripts in junctionwiththeaverageresponsetime.
JupyterNotebooks[38].WeusedPandas[45],topro- Figure6presentsthecomparisonbetweendetected
cess time-series data, and Scikit-learn [8], to provide AnomalousandNon-Anomaloustime-framesinunix
animplementationofIsolationForests. timestampforagivenservice.Thisinformation,rep-
The main goals of this pair of components was to resented in Fig. 6, was the result of outlier detection,
findthesetofinterestingtime-framesinalargesetof considering three service metrics: number of incom-
traces, thus relieving operators from the need to con- ingrequests,numberofoutgoingrequestsandaverage
duct unguided, sometimes exhaustive, search using responsetime.Anomaliesidentifiedbythealgorithm,
Zipkinthataremostlylimitedtotracingvisualisation are indicated by vertical red lines. In addition to the
features.Thecodeanddocumentationofourworkare metrics used for anomaly detection, we include an
availableinGitHub.1 additionaltime-series,whichdenotesthemorphologi-
calchangestotheservicedependencygraph.
AswecanseeinFig.5,thedifferencebetweenanoma-
4ResultsandAnalysis lousandnon-anomalousoperationismadeclearbythe
presenceofoutliers.Intheanomaloussamples,points
In this section we present the results gathered from formaclusternearthechartorigin,withsomeoutliers
the Data Analysis component presented in Section 3, ontheupper-leftanddown-rightregionsofthechart.
toidentifyandlocateanomalousbehaviourinthesys- Meanwhile, in the non-anomalous samples, there is
tem. As we worked towards this goal, the quality of onlyaclusteringofpointsnearthechartorigin.
the tracing data became a problem,leading us to for- The next step of our analysis is to determine the
mulate an additional research question. Furthermore, cause for the outliers in the anomalous samples, i.e.,
even though we had traces and were able to iden- what exactly is causing this unexpected increment
tify anomalous regions, we did not have access to inthenumberofincoming/outgoingrequests—which
issuereportsandwereunabletovalidateaccuracyand accounts for load variation—and average response
precision. times,moreprecisely:
– Some services take longer to respond even when
4.1AnomalyDetection
the system is lightly loaded, with few incom-
ing/outgoingrequests.
Figure 5 provides a representation of two time-frame
– Some services are receiving more incoming/outgo-
samples of the same service, one for an anomalous
ingrequests,butstillrespondingfast.
region, and another for a non-anomalous region as
tagged by our Data Analyser component. We set An elastic system should be capable of handling
the time-series resolution to 10 minutes—to avoid more requests and still reply in an expected amount
oftime.However,iftheservicequalitydegradationin
1OpenTracingProcessor(OTP),https://github.com/andrepbento/ response to increased load is to steep, this represent
OpenTracingProcessor someerrorconditionthatwemustruleout.
9 Page8of15 JGridComputing(2021)19:9
Oneinterestingfacttonoticeisthat,intheanoma-
lousregions,therearemorerequestwork-flowstypes.
Thenextstepwouldbetocheckwhatwascausingthis
increase, by retrieving the most invoked work-flow.
Unfortunately,we were unable to continue down this
path,becausetracingdatawasincomplete.Theflows
were not relevant for a further analysis because they
werejustcallsbetweenagatewayandaservice.More-
over, the gateway instrumentation was incomplete;
logging the type of request and service name but not
theendpoint.Atthispoint,andforthisquestion,itis
possibletosaythatthisdatasetwasexhaustivelyanal-
ysed, and an improvement of the tracing data should
bethepathtotake.
4.2TraceQualityAnalysis
Onceweconcludedtheimpossibilityofgoingdeeper
in the analysis of the tracing data, we questioned
how we could measure the quality of tracing. Our
approach to this question was to process the tracing
dataandfeedittotheDataAnalysiscomponent,this
timewithoutusingatime-seriesdatabasein-between.
We divided this analysis into two procedures. The
first procedure checks if the spans complies with the
OpenTracing specification. According to the algo-
rithm, every span structure complies with the spec-
ification. The problem here is that the OpenTracing
specificationisnotverystrictandtherefore,thistest-
ing algorithm cannot provide very accurate results.
For example, the units for time-stamps are not uni-
form,onecanusemillisecondsinonefieldandthen,
in another field of a span, in the same trace, time
Subsequently, we analysed the work-flow types. might be in microseconds. This leads to problems in
A work-flow is a class of requests, or traces, that timemeasurements,butthespecification,andthevery
sharethesameserviceinvocationgraph.Usuallythey designofthestandard,makeitdifficulttodetectcom-
represent a type of request or business process. The putationally in a deterministic manner. We discuss a
objective of this analysis is to understand if there is possibleredefinitionoftheOpenTracingspecification
something wrong with the request work-flow paths, inSection5.
such as degenerate paths resulting from missing ser- The second procedure checks if tracing covers the
vices. Algorithm 2 illustrates our approach to collect entire time of the root spans. For a simple example,
allwork-flowspresentinthetracingdata.Theprocess if we have a trace with a root span of 100 ms, and
to collect work-flows involves pinpointing requests this root span has two children spans, one with 50
between services present in the span meta-data and ms, the other one with 40 ms, the entire trace has
thenstoringalistofalluniquegraphs. a temporal coverage of (50 + 40)/100 = 90%. We
In Fig. 7 we show the most common work- applythismethodtoeverytrace,andplottheresults;
flowsfromtheAnomalousandNon-Anomaloustime- furthermore,wesplitthembyservice,withtheobjec-
frames. Given the large number of work-flows that tive of determining the time coverage of tracing by
existinthesystem,weencodedthemnumerically. service.
JGridComputing(2021)19:9 Page9of15 9
Fig.5 ComparisonbetweenAnomalousandNon-Anomalousservicetime-frameregions
The results, regarding two different services are in the tooling exacerbate the aforementioned issue—
displayedinFig.8,depictingthecoveragehistograms thereisnotooltoperformtracingqualityevaluation—,
fortwodifferentservices.Eachtimetheserviceshows and made it necessary to develop tools to treat and
upinatrace,wecalculatethepercentageoftimecov- analyse data. Furthermore, exploratory analysis of
ered,toproducethehistogram.Itisimportanttonotice tracingdataisdifficultastherearenotoolsavailable
the good coverage level—in the 60%−100% range. forthispurpose.Wecategorisedandsub-dividedthis
Thismeansthatcoverageforthistracingcouldbebet- issuesinFig.9.Theycanberoughlydividedinthree
ter, but it is nonetheless good. This points to the fact groups,datasufficiency,ontological,andtools.
that even a relatively high temporal coverage is not Dataavailableinthetracingdatasetmustbesuffi-
a sufficient quality indicator for automated anomaly cientfortheanalysis.Thismeansthattheinstrumenta-
detection. tionneedstocoverthecode-base,likeunittestswould
haveto,aswellastime.AsdescribedinSection4.2,a
spanshouldhaveitschildrenspanscoverasmuchas
5TracingStandardLimitationsandMitigations its time-frame as possible, limited by code granular-
ity. Even though trace sampling becomes a necessity
Thequalityofouranomalydetectionmethodwasbounded at scale, to avoid unacceptable overhead, it needs to
bythequalityofthedata.Specifically,thetracingdataset bebalancedagainstrepresentativenessofthedataand,
presented problems in completeness and homogene- in turn, of the intelligence extracted from it. In order
ity,whichwereasonisaconsequenceofambiguityin to be able to capture rare events as well as enough
the tracing format specification standard. Limitations volume in a low throughput system, sampling-rate
9 Page10of15 JGridComputing(2021)19:9
stseuqeR 0003
gnimocnI
0001
0
stseuqeR
0002
gniogtuO 0001
0
)sm(
emiT 0000001
esnopseR
0