reference to the observer
(cid:4) Each executor maintains a
program ← ∅
stop ← ∅
for ∞ do
container and serialize execute request
WAITFORSIGNAL()
PREPARETOEXECUTE(program)
SIGNALOBSERVER(O)
WAITFORSIGNAL()
EXECUTE(program, stop)
(cid:4) Create a
Implementing the Observer. In the SYZKALLER native
design, one thread is created for each executor and all procs
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:18:21 UTC from IEEE Xplore.  Restrictions apply. 
407
execute independently. We modify each thread and coordinate
the observer with any number of executors using algorithm 2.
Basically, this algorithm uses a two-stage latching procedure to
distribute programs and prime each executor (each executor is
distributed with one program, as Line 11), then starts the
execution window to align with a pre-deﬁned number of
resource measurements.
The observer divides execution periods into rounds of time
T duration each. With preliminary exploration, we observe
that a short interval (T ) is more susceptible to being disrupted
by temporary “noise spikes” from the host (e.g., cron jobs,
sudden arrival of network packets, system logging events, etc),
whereas longer intervals produce more useful measurements
but signiﬁcantly reduce program throughput. We choose values
in the range of a few full seconds, often between three and
ﬁve, to achieve a fair balance of throughput and precision.
Implementing Oracles. In anticipation of fuzzing for adver-
sarial CPU utilization, we create an oracle framework suitable
for collecting both per-process and per-core utilization measure-
ments (Line 15 of algorithm 2, TakeMeasurements). The former
can be easily collected from the PROC ﬁle system, speciﬁcally
through ‘/proc/stat‘. This pseudoﬁle exposes information about
how much time each CPU core has spent in various categories,
including userspace, kernel space, and idling.
Collecting per process CPU utilization is more difﬁcult, but
can provide equally useful insights. Tracking the usage of
individual processes is particularly helpful in understanding
where out-of-band workloads are being created and tracking
their efﬁcacy. To implement this, we fork an existing Golang
library [7] with a wrapper for the top(1) command. We ﬁlter
this output by selecting common categories of interest, such
as ‘docker’, ‘kworker’ threads, ‘kauditd’, ‘systemd-journal’,
and miscellaneous kernel threads (most of them are reported
in [29]).
The implementation of top on Linux has a number of hidden
idiosyncrasies that make it difﬁcult for our purpose. First, even
when invoked with a custom duration between updates, top has
an unavoidable “warm up time” to generate its ﬁrst frame that
produces inaccurate results. We modify the Linux wrapper for
top to discard these warm-up measurements. Secondly, top is
incapable of reporting CPU utilization by processes that begin
or end during the time between frames. For our purposes, this
only makes it suitable for measuring CPU utilization from
daemons or otherwise long-lived processes. If a program were
to trigger the creation of many short lived kernel threads,
TORPEDO would still observe it from the broader per-core
CPU usage measurement. The combination of these two metrics
gives an excellent “snapshot” of CPU allocation during a time
period and can easily be analyzed to determine adversarial
workloads.
B. Leveraging the Oracle Library
As in Section IV-D, we implement each oracle to support
two objectives. The ﬁrst concerns scoring workload resource
utilization to serve as a feedback mechanism, and the second
concerns identifying adversarial workloads based on a set of
heuristics.
Scoring Workloads. As in SYZKALLER, candidate programs
are evaluated for new code coverage patterns and only accepted
for triage if they are judged to be interesting. Each batch
of programs is subjected to many repeated mutations in an
attempt to motivate the generation of adversarial programs.
We conceive of two states that a set of programs may be in
at any time; “mutation”, where each program in the set is
perturbed in an attempt to generate more adversarial resource
utilization, and “conﬁrm”, where programs are rerun to conﬁrm
some interesting observation exists and was not a result of
system noise. The Oracle score is used to determine when a
mutation has achieved some meaningful change and should
be conﬁrmed as a new baseline for the batch (Algorithm 2,
Line 15, RoundScore, used to GetPrograms on line 7). After
some amount of time without a meaningful improvement, the
Oracle determines the batch has been exhausted and calls for
new programs.
Combining Coverage and Utilization Feedback. As the
primary assumption behind most fuzzing tools, high code
coverage generally means that it is more likely for a test
to uncover a bug. Nevertheless, this might not be sufﬁcient
for our focus, aiming at ﬁnding bugs enabling adversarial
workloads. Thus, TORPEDO needs to combine both code
coverage information and system utilization feedback to guide
the fuzzing process. This is not straightforward, because
code overage comes from an individual program but system
utilization comes from all programs. To this end, our design
splits the SYZKALLER program state machine into two separate
state machines: one for each program and the other for the
whole batch of programs. Figure 2 depicts the result of dividing
relevant states between the level of an individual program and
a batch of programs. The program state machine is focused on
coverage collection: it discards programs that are not interesting
and ensures to keep getting new traces to test. The batch state
machine is focused on the system utilization: it decides how
to mutate programs. Thus, programs that do not generate new
coverage are typically rejected before they spend too much
time being mutated. Also, only the set of mutated workloads
that generate the most adversarial resource usage are recorded
into the corpus.
To reduce the impact of system noise (e.g., generated by
mutation operations), we implement the “shufﬂe” state, where
individual programs are shufﬂed between cores but the order
of syscalls in each trace remains unchanged. This helps to
reduce false positives from the scenarios where system noise
is concentrated on a subset of cores and is unrelated to the
program under test.
Flagging Workloads. The Observer could easily apply an
Oracle’s ﬂagging heuristic to each observation as it becomes
available, although as true violations are likely to be rare, this
would reduce overall program throughput. Instead, TORPEDO
uses this Oracle functionality to parse through log ﬁles from
each round and isolate small numbers of adversarial programs
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:18:21 UTC from IEEE Xplore.  Restrictions apply. 
408
(cid:3)(cid:9)(cid:14)(cid:9)(cid:17)(cid:6)(cid:19)(cid:10)(cid:15)(cid:14)
(cid:5)(cid:9)(cid:9)(cid:8)(cid:1)(cid:5)(cid:9)(cid:12)(cid:9)(cid:7)(cid:19)(cid:10)(cid:15)(cid:14)
(cid:1)(cid:3)(cid:8)(cid:4)(cid:7)(cid:4)(cid:3)(cid:10)(cid:5)
(cid:2)(cid:9)(cid:7)(cid:3)(cid:6)(cid:5)
(cid:4)(cid:9)(cid:11)(cid:9)(cid:7)(cid:19)(cid:10)(cid:15)(cid:14)
(cid:2)(cid:15)(cid:13)(cid:13)(cid:10)(cid:19)(cid:1)(cid:19)(cid:15)
(cid:2)(cid:15)(cid:17)(cid:16)(cid:20)(cid:18)
(cid:2)(cid:4)(cid:15)(cid:5)(cid:7)
(cid:3)(cid:13)(cid:6)(cid:4)(cid:15)(cid:8)(cid:11)(cid:10)
(cid:2)(cid:5)(cid:7)(cid:8)(cid:4)
(cid:1)(cid:7)(cid:6)(cid:3)(cid:6)(cid:4)
(cid:3)(cid:11)(cid:9)(cid:9)(cid:8)(cid:15)(cid:1)(cid:15)(cid:11)
(cid:3)(cid:11)(cid:13)(cid:12)(cid:16)(cid:14)
(a) Program State Machine.
Fig. 2: TORPEDO State Machines
(b) Batch State Machine.
asynchronously from program execution. If the adversarial pro-
gram is indeed correlated with a higher score from the Oracle’s
scoring functionality, then we expect the adversarial program
to be retained for the remainder of the batch, which serves
to conﬁrm the program is the cause of whatever adversarial
behavior has been ﬂagged. Once identiﬁed, TORPEDO leverages
a tool-assisted minimization workﬂow to automatically isolate
the adversarial programs (i.e., a sequence of system calls)
for further analysis. Basically, we systematically remove calls
from the program until we obtain the smallest set of calls that
result in the originally observed oracle violations. After that,
we further manually conﬁrm and isolate the vulnerabilities
through kernel trace debugging.
VI. EVALUATION
A. Research Questions
TORPEDO is designed to discover vulnerabilities existed
in containerization components, which can be exploited to
generate out-of-band workloads and escape the resource limit
of cgroups. While many aspects of the tool are novel, much of
the design is a natural extension of the existing SYZKALLER
framework. Also, we source much of TORPEDO’s initial testing
corpus directly from a selection of seeds from Moonshine [56],
which is another SYZKALLER extension project concerned with
improving the quality of seeds distilled from the framework
seeds. Particularly, we attempt to explore that (1) Can TORPEDO
discover new vulnerabilities and how efﬁcient is that? (2)
Are there discrepancies among different implementations of
container components (e.g., runtime)? (3) How is TORPEDO’s
code coverage mechanism?
the native Linux together with runc [10] are executed directly
on top of the hardware. runc denotes a low-level container
runtime library mainly supporting “high-level” container engine
(e.g. Docker) to spawn and run containers. For instance, the
Docker engine leverages runc to handle tasks such as running
a container, attaching a process to an existing container, and
so on.
crun. Much like runc [24], crun is a bare-metal runtime
that interfaces directly with Linux to create a containerized
process. Unlike runc, which is written in Golang, crun is written
entirely in C. The project authors advertise that crun functions
identically to runc, but due to its implementation, is faster
and more memory efﬁcient. Also, the crun is completely
compatible with Docker.
gVisor. This runtime is indicative of another popular container
execution environment, where the gVisor (runsc) process
serves as a secure sandbox for the untrusted containerized
code. gVisor functions as a userspace kernel (comparable
to LibOS [9]) with a subtle difference. gVisor essentially
provides an extra layer between the container and host OS
kernel, intercepting system calls made by the containerized
applications. To date, gVisor has successfully supported 211
out of 319 x86-64 Linux system calls, by using only 64 system
calls on the host system [12].
For the duration of our experiments, we ﬁx TORPEDO to
use the Docker ecosystem with a selected container runtime.
By rotating adversarial programs between different runtimes,
TORPEDO empowers testers to quickly identify discrepancies
between each implementation, as well as expose underlying
OS bugs or higher-level bugs in Docker.
B. Environment Setups
C. Evaluation Procedure
Multiple popular container implementations are commonly
used in the real world. This paper aims at presenting an in-depth
understanding of today’s container security landscape, where
we will leverage TORPEDO to test three popular container
runtime implementations: runc, crun, and gVisor.
TORPEDO is designed as a blackbox testing framework
that does not rely on any implementation details of the
underlying container implementation. In principle, the proposed
technique can be smoothly migrated to test different container
infrastructure implementations. We consider this as a big
advantage, compared to existing container security analysis
techniques where heavyweight program analysis methods are
conducted [57]. The following paragraphs discuss each of the
container runtimes we target in detail.
runc. This runtime is used in a typical container execution
environment (usually as the default container runtime), where
For each fuzzer execution, we choose a small number of
Moonshine seeds and use TORPEDO’s seed ingestion workﬂow
to enqueue these as candidate programs. We allow the fuzzer
to run unattended and review the execution logs after all seeds
have been exhausted. The corpus of coverage information is
purged between each fuzzer invocation, which serves to prevent
adversarial system call traces from being continually injected
into future programs and preventing new, interesting ﬁndings
from being revealed.
Each execution encompasses some number of rounds, each
of which produces a detailed log ﬁle of resource utilization
during the period. These log ﬁles are batched and passed over
by an automated script that examines each round for resource
pattern violations as deﬁned by one or more Oracles. This work
focuses primarily on the results from a CPU Oracle, which
uses the heuristics given in Table I. Speciﬁc constants for each
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:18:21 UTC from IEEE Xplore.  Restrictions apply. 
409
TABLE I: TORPEDO CPU Oracle Heuristics
Heuristic
Fuzzing core CPU utilization
Idle core CPU utilization
Total CPU utilization
System process CPU utilization
Notes
Expect above some threshold
Expect below some threshold
Expect below some threshold
Expect below some threshold
heuristic vary according to test parameters, and speciﬁcally the
selected container runtime and amount of parallelism.
Due to the relationship between patterns of adversarial CPU
utilization and an increased score from the CPU Oracle, we
assume that the adversarial properties of a program will be
preserved by TORPEDO during operation and will exhibit
the same patterns over many subsequent rounds. Therefore,
for a given batch of programs, any commonalities between
programs ﬂagged by the Oracle for similar resource violations
can be extracted and minimized by a human operator with little
difﬁculty. We consider any set of system call traces that creates
an adversarial workload when isolated and run independently
of the TORPEDO framework to be a discovered vulnerability
for the purposes of evaluation.
All tests were executed on a machine equipped with an AMD
Ryzen 3600X with 12 cores and 16 gigabytes of RAM running
Linux kernel version 5.8 (Ubuntu 20). We run 3 containers in
parallel: each is pinned to one core and restricted with 100%
CPU utilization of one core, using the cpusets and cpu
controllers following [29]. For the fuzzing process, each round
lasts 5 seconds, and multiple batches (each typically contains
between 30 and 50 rounds) are conducted. The speciﬁc number
depends on whether the seed is interesting or not.
D. Summary of Identiﬁed Vulnerabilities
Table II presents a summary of our fuzzing results. We
also report a computed “ampliﬁcation factor” as [29], which
deﬁnes the difference between the CPU utilization measured
on the container cgroup (via docker stat) and the actual
system utilization. Over the course of our testing, TORPEDO
identiﬁes three new vulnerabilities concerning CPU utilization
with different attack vectors for baremetal runtimes (e.g., runc
and crun). The ﬁrst involves a technique that exploits the
kernel module loading system to create processes outside the
cgroup of the caller. The second allows a privileged container to
directly place work on a kernel workqueue. The third involves
manipulating a container into a state that will cause the Docker
daemon to expend signiﬁcant resources when the container is
reclaimed by the system. Also, sandbox runtime incurs less
problems compared with baremetal runtimes. To the best of our