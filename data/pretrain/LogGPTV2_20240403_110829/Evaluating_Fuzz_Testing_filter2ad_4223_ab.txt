to reproduce the observation. This allows software developers to
confirm, reproduce, and debug issues.
2.2 Recent Advances in Fuzzing
The effectiveness of fuzz testing has made it an active area of re-
search. Performing a literature search we found 32 papers published
between 2012 and 2018 that propose and study improvements to
various parts of the core fuzzing algorithm; 25 out of 32 papers
we examined were published since 2016. To find these papers, we
started from 10 high-impact fuzzing papers published in top secu-
rity venues. Then we chased citations to and from these papers. As
a sanity check, we also did a keyword search of titles and abstracts
of the papers published since 2012. Finally, we judged the relevance
based on target domain and proposed advance, filtering papers that
did not fit.
Table 1 lists these papers in chronological order. Here we briefly
summarize the topics of these papers, organized by the part of the
fuzzing procedure they most prominently aim to improve. Ulti-
mately, our interest is in how these papers evaluate their claimed
improvements, as discussed more in the next section.
initSeedCorpus. Skyfire [53] and Orthrus [49] propose to im-
prove the initial seed selection by running an up-front analysis on
the program to bootstrap information both for creating the corpus
and assisting the mutators. QuickFuzz [20, 21] allows seed gener-
ation through the use of grammars that specify the structure of
valid, or interesting, inputs. DIFUZE performs an up-front static
analysis to identify the structure of inputs to device drivers prior
to fuzzing [13].
mutate. SYMFUZZ [9] uses a symbolic executor to determine
the number of bits of a seed to mutate. Several other works change
mutate to be aware of taint-level observations about the program
behavior, specifically mutating inputs that are used by the program
[8, 10, 33, 44]. Where other fuzzers use pre-defined data mutation
strategies like bit flipping or rand replacement, MutaGen uses frag-
ments of the program under test that parse or manipulate the input
as mutators through dynamic slicing [29]. SDF uses properties of
the seeds themselves to guide mutation [35]. Sometimes, a grammar
is used to guide mutation [23, 57]. Chizpurfle’s [27] mutator exploits
knowledge of Java-level language constructs to assist in-process
fuzzing of Android system services.
eval. Driller [50] and MAYHEM [8] observe that some condi-
tional guards in the program are difficult to satisfy via brute force
guessing, and so (occasionally) invoke a symbolic executor during
the eval phase to get past them. S2F also makes use of a symbolic
executor during eval [58]. Other work focuses on increasing the
speed of eval by making changes to the operating system [56] or
using different low level primitives to observe the effect of execu-
tions [23, 25, 47]. T-Fuzz [39] will transform the program to remove
checks on the input that prevent new code from being reached.
MEDS [24] performs finer grained run time analysis to detect er-
rors during fuzzing.
isInteresting. While most papers focus on the crashes, some
work changes observation to consider different classes of program
behavior as interesting, e.g., longer running time [41], or differ-
ential behavior [40]. Steelix [33] and Angora [10] instrument the
program so that finer grained information about progress towards
satisfying a condition is exposed through observation. Dowser and
VUzzer [22, 44] uses a static analysis to assign different rewards
to program points based on either a likely-hood estimation that
traveling through that point will result in a vulnerability, or for
reaching a deeper point in the CFG.
choose. Several works select the next input candidate based on
whether it reaches particular areas of the program [5, 6, 32, 44].
Other work explores different algorithms for selecting candidate
seeds [45, 55].
3 OVERVIEW AND EXPERIMENTAL SETUP
Our interest in this paper is assessing the existing research practice
of experimentally evaluating fuzz testing algorithms. As mentioned
in the introduction, evaluating a fuzz testing algorithm A requires
several steps: (a) choosing a baseline algorithm B against which to
compare; (b) choosing a representative set of target programs to
test; (c) choosing how to measure A’s vs. B’s performance, ideally
as bugs found; (d) filling in algorithm parameters, such as how
seed files are chosen and how long the algorithm should run; and
(e) carrying out multiple runs for both A and B and statistically
comparing their performance.
Research papers on fuzz testing differ substantially in how they
carry out these steps. For each of the 32 papers introduced in Sec-
tion 2.2, Table 1 indicates what benchmark programs were used
for evaluation; the baseline fuzzer used for comparison; the num-
ber of trials carried out per configuration; whether variance in
performance was considered; how crashing inputs were mapped
to bugs (if at all); whether code coverage was measured to judge
performance; how seed files were chosen; and what timeout was
used per trial (i.e., how long the fuzzer was allowed to run). Expla-
nations for each cell in the table are given in the caption; a blank
cell means that the paper’s evaluation did not mention this item.
For example, the AFLFast [6] row in Table 1 shows that the
AFLFast’s evaluation used 6 real-world programs as benchmarks
(column 2); used AFL as the baseline fuzzer (column 3); ran each
experiment 8 times (column 4) without reporting any variance
(column 5); measured and reported crashes, but also conducted
manual triage to obtain ground truth (column 6); did not measure
code coverage (column 7); used an empty file as the lone input seed
(column 8); and set 6 hours and 24 hours as timeouts for different
experiments (column 9).
Which of these evaluations are “good” and which are not, in
the sense that they obtain evidence that supports the claimed tech-
nical advance? In the following sections we assess evaluations
both theoretically and empirically, carrying out experiments that
demonstrate how poor choices can lead to misleading or incorrect
conclusions about an algorithm’s fitness. In some cases, we believe
it is still an open question as to the “best” choice for an evaluation,
but in other cases it is clear that a particular approach should be
taken (or, at least, certain naive approaches should not be taken).
Overall, we feel that every existing evaluation is lacking in some
important way.
We conclude this section with a description of the setup for our
own experiments.
Fuzzers. For our experiments we use AFL (with standard config-
uration parameters) 2.43b as our baseline B, and AFLFast [6] as our
“advanced” algorithm A. We used the AFLFast version from July
2017 (cloned from Github) that was based on AFL version 2.43b.
Note that these are more recent versions than those used in Böhme
et al’s original paper [6]. Some, but not all, ideas from the original
AFLFast were incorporated into AFL by version 2.43b. This is not an
issue for us since our goal is not to reproduce AFLFast’s results, but
rather to use it as a representative “advanced” fuzzer for purposes
of considering (in)validity of approaches to empirically evaluating
fuzzers. (We note, also, that AFL served as the baseline for 14/32
papers we looked at, so using it in our experiments speaks directly
to those evaluations that used it.) We chose it and AFL because
they are open source, easy to build, and easily comparable. We also
occasionally consider a configuration we call AFLNaive, which is
AFL with coverage tracking turned off (using option -n), effectively
turning AFL into a black box fuzzer.
Benchmark programs. We used the following benchmark pro-
grams in our experiments: nm, objdump, cxxfilt (all from binutils-
2.26), gif2png, and FFmpeg. All of these programs were obtained
from recent evaluations of fuzzing techniques. FFmpeg-n0.5.10 was
used in FuzzSim [55]. binutils-2.26 was the subject of the AFLFast
evaluation [6], and only the three programs listed above had dis-
coverable bugs. gif2png-2.5.8 was tested by VUzzer [44].1 We do
not claim that this is a complete benchmark suite; in fact, we think
that a deriving a good benchmark suite is an open problem. We
simply use these programs to demonstrate how testing on different
targets might lead one to draw different conclusions.
Performance measure. For our experiments we measured the
number of “unique” crashes a fuzzer can induce over some period of
time, where uniqueness is determined by AFL’s notion of coverage.
In particular, two crashing inputs are considered the same if they
have the same (edge) coverage profile. Though this measure is not
uncommon, it has its problems; Section 7 discusses why, in detail.
Platform and configuration. Our experiments were conducted
on three machines. Machines I and II are equipped with twelve
2.9GHz Intel Xenon CPUs (each with 2 logical cores) and 48GB
RAM running Ubuntu 16.04. Machine III has twenty-four 2.4GHz
CPUs and 110GB RAM running Red Hat Enterprise Linux Server
7.4. To account for possible variations between these systems, each
benchmark program was always tested on the same machine, for
all fuzzer combinations. Our testing script took advantage of all the
CPUs on the system to run as many trials in parallel as possible.
One testing subprocess was spawned per CPU and confined to it
through CPU affinity. Every trial was allowed to run for 24 hours,
and we generally measured at least 30 trials per configuration. We
also considered a variety of seed files, including the empty file,
1Different versions of FFmpeg and gif2png were assessed by other papers [9, 45, 58],
and likewise for binutils [5, 32, 40].
paper
MAYHEM[8]
FuzzSim[55]
Dowser[22]
COVERSET[45]
SYMFUZZ[9]
MutaGen[29]
SDF[35]
Driller[50]
QuickFuzz-1[20]
AFLFast[6]
SeededFuzz[54]
[57]
AFLGo[5]
VUzzer[44]
SlowFuzz[41]
Steelix[33]
Skyfire[53]
kAFL[47]
DIFUZE[13]
Orthrus[49]
Chizpurfle[27]
VDF[25]
QuickFuzz-2[21]
IMF[23]
[59]
NEZHA[40]
[56]
S2F[58]
FairFuzz[32]
Angora[10]
T-Fuzz[39]
MEDS[24]
R(29)
R(101)
R(7)
R(10)
R(8)
R(8)
R(1)
C(126)
R(?)
R(6)
R(5)
R(2)
R(?)
R(10)
R(4)
R(3)
R(7)
R(1)
R(18)
R(?)
R(1)
S(?)
R(6)
G(10)
L, R(8)
R(9)
L, R(8)
C(296), L, R(4)
S(2), R(12)
C(63), L, R(10)
C(17), L, R(5)
A, V, O
G(4), R(2)
A, L, O
B
O
O
A, B, Z
R, Z
Z, O
A
A
O
A, O
A, O
A
O
O
O
O
O
O
O
O
A, L, O
A, L
A, O
A
A, V, O
A, O
O
100
?
10
8
20
100
5
80
10
5
100
20
5
3
10
C
C
C
coverage
?
?
L
L, E
O
L, E
L
L, E, M
L, M
E
O
O
E
L, E
G
S
O
S, G*
S
S
O
G
?
S
-
C, G*
M
G, S, O
C, G
?
C, G*
G*
S, G*
G*
C
G*
G*
G
O
G
G, C
C, G*
C
R/M/N
seed