For the congestion avoidance and congested states, Gimbal incre-
mentally changes the target rate to either probe for more bandwidth
or lower the offered load. The rate is increased/decreased by the
IO completion size for the congestion avoidance/congested state.
For the overloaded state, the rate is immediately adjusted to be
lower than the IO completion rate, and Gimbal discards the remain-
ing tokens in the buckets to avoid a bursty submission. Gimbal
periodically measures the completion rate for this. In addition, Gim-
bal increases the target rate at a faster rate when it observes the
underutilized state (parameter 𝛽 in Algorithm 1).
Gimbal handles the overloaded and under-utilized states in the
above manner because the incremental adjustments are meaningful
only when the IO pattern doesn’t change; incremental adjustments
will not converge fast enough for dynamic IO patterns. Specifically,
the maximum bandwidth of the SSD may differ dramatically accord-
ing to the read and write mixed ratio. On a fragmented SSD (defined
in Section 5.1), the random write bandwidth is about 180MB/s while
the random read bandwidth is over 1600MB/s. Consequently, the
rate converges slowly if the pattern shifts from write-heavy to read-
heavy. Gimbal, inspired by CUBIC[40] and TIMELY[62], adapts an
aggressive probing strategy for identifying the desired operating
point. This condition only appears when the target rate is insuf-
ficient to submit IO before the SSD drains most of the IOs in the
internal queue. On the other hand, the target rate may exceed the
maximum bandwidth enormously when the pattern shifts in the
opposite direction (i.e., from read-heavy to write-heavy), resulting
in the SSD performing at its peak bandwidth but with a latency
higher than 𝑇ℎ𝑟𝑒𝑠ℎ𝑚𝑎𝑥. In this case, Gimbal first identifies the cur-
rent completion rate and sets the target rate to the completion rate.
It then further reduces the target rate by an amount equal to that of
the size of the completed IO. As a consequence, Gimbal holds the
target rate to be lower than that of the SSD’s peak capacity until the
SSD drains the queued IOs and starts exhibiting a normal latency.
Algorithm 1 depicts the congestion control logic. The submis-
sion function is invoked on each request arrival and completion so
that it works in a self-clocked manner. With the congestion control
mechanism, the SSD maintains an average delay in a stable range
providing performance comparable to the device maximum. It ad-
justs the IO request rate of each SSD while eliminating unnecessary
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
Min and Liu, et al.
waiting time in the device’s internal queue. Crucially, it allows us
to determine the performance headroom of an SSD during runtime.
3.4 Write Cost Estimation
Performance asymmetry of reads and writes is a well-known issue
for NAND devices. Typically, a write consumes more resources
than the same-sized read due to the write amplification caused
by garbage collection. To capture this, we introduce the write cost
parameter – the ratio between the achieved read and write band-
widths. For example, let the maximum read and write bandwidths
(measured independently) of a given SSD be 1000MB/s and 300MB/s,
respectively. The write cost is 3.3 in this case, where we assume
that 700MB/s might be used to run internal tasks for the write.
We cannot obtain this value directly from the SSD. Instead, we
use a baseline setting as the worst case and dynamically calibrate
the write cost according to the current SSD state. One can obtain
𝑤𝑟𝑖𝑡𝑒 𝑐𝑜𝑠𝑡𝑤𝑜𝑟𝑠𝑡 via a pre-calibration or from the SSD specification,
so it is a fixed parameter for a specific SSD.
We update the write cost periodically in an ADMI (Additive-
Decrease Multiplicative-Increase) manner. The write cost decreases
by 𝛿 if the write EWMA latency is lower than the minimum la-
tency threshold and increases to (𝑤𝑟𝑖𝑡𝑒 𝑐𝑜𝑠𝑡 + 𝑤𝑟𝑖𝑡𝑒 𝑐𝑜𝑠𝑡𝑤𝑜𝑟𝑠𝑡)/2
otherwise. This allows Gimbal to quickly converge to the worst-
case when we observe latency increases. By using the latency for
adjusting the write cost, Gimbal takes the SSD device optimization
for writes into consideration. Specifically, an SSD encloses a small
DRAM write buffer and stores user data in the buffer first before
flushing it in a batch to the actual NAND at the optimal time [46].
When the write submission rate is lower than the write buffer con-
suming capability, writes are served immediately with consistent
low latency. In this case, unlike other schemes that have only a
fixed cost ratio between read and write, Gimbal reduces the cost
down to 1 (i.e., 𝑤𝑟𝑖𝑡𝑒 𝑐𝑜𝑠𝑡 = 1), same as the read cost. When the
write rate rises beyond the write buffer serving capacity, its latency
and write cost increase.
3.5 Two-level Hierarchical IO Scheduler and Virtual slot
We define the per-IO cost of NVMe SSDs as the average occupancy
of operations within the NAND per byte of transmitted data. It is
not constant and is affected by numerous factors (Section 2.3). This
metric reflects how an SSD controller executes different types of
NAND commands, such as splitting a large request into multiple
small blocks, blocking in an internal queue due to access contention,
etc. The IO cost should be considered as a key evaluation parameter
for determining fairness since two storage streams would consume
significantly different amounts of resources in an SSD even if they
achieve the same bandwidth.
IO cost is hard to measure because SSDs do not disclose detailed
execution statistics of the NAND and its data channels. IO costs
can also be biased by operation granularity. For instance, a large
128KB IO might be decomposed into individual 4KB requests in-
ternally and deemed complete only when all individual requests
have been processed. In contrast, if we were to pipeline a sequence
of 32 × 4KB operations, issuing a new one after each completion,
the SSD internal queue occupancy would increase even though
the observable outstanding bytes is the same with 128KB IO. We,
therefore, use the notion of a virtual slot, which is a group of IOs
111
io_outstanding += 1
submit_to_ssd(req)
return
lat_mon.thresh = 𝑡ℎ𝑟𝑒𝑠ℎ𝑚𝑎𝑥
state = overloaded
else if state == congestion_avoidance then
target_rate −= cpl.size
target_rate += cpl.size
target_rate += 𝛽× cpl.size
else
else if lat_mon.ewma > lat_mon.threshold then
state = update_latency(cpl.io_type, cpl.latency)
if state == overloaded then
target_rate = completion_rate
discard_remain_tokens(dual_token_bucket)
if state == congested or overloaded then
update_token_buckets()
req = DRR.dequeue()
bucket ← dual_token_bucket[req.io_type]
if bucket.tokens ≥ req.size then
return
lat_mon ← latency monitor for io_type
lat_mon.ewma = (1-𝛼𝐷) lat_mon.ewma_lat + 𝛼𝐷× latency
if lat_mon.ewma > 𝑡ℎ𝑟𝑒𝑠ℎ𝑚𝑎𝑥 then
Algorithm 1 Congestion Control with Rate Pacing
1: procedure Submission()
2:
3:
4:
5:
6:
7:
8:
1: procedure Completion()
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
1: procedure update_latency(io_type, latency)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
up to 128KB in total (e.g., it might contain up to 1 × 128KB or 32 ×
4KB IO commands) and manage IO completion in the granularity
of virtual slots. A virtual slot completes when all operations in the
slot complete. Each tenant always has the same number of virtual
slots. If a tenant runs out of its virtual slots, the IO scheduler defers
following IOs until one of its virtual slots completes and becomes
available. Gimbal maintains the number of virtual slots per tenant
at a minimum and adapts the IO cost variance according to sizes.
The virtual slot mechanism provides an upper bound on the
submission rate and guarantees that any sized IO pattern obtains
a fair portion of the SSD internal resource. It also addresses the
deceptive idleness issue [44] (found in many work-conserving fair
queuing schedulers) because an allocated slot cannot be stolen by
other streams. Gimbal sets the threshold for the number of virtual
slots in a single tenant to the minimum number to reach the device’s
maximum bandwidth if there is only one active tenant. Virtual slots
are equally distributed when more active tenants contend for the
storage. Since each tenant should have at least one virtual slot
to perform IOs, the total number of virtual slots may exceed the
threshold under high consolidation.
lat_mon.thresh = (lat_mon.thresh + 𝑡ℎ𝑟𝑒𝑠ℎ𝑚𝑎𝑥 )/2
state = congested
lat_mon.thresh −= 𝛼𝑇 × (lat_mon.thresh - lat_mon.ewma)
state = congestion avoidance
lat_mon.thresh −= 𝛼𝑇 × (lat_mon.thresh - lat_mon.ewma)
state = underutilized
else if lat_mon.ewma > 𝑡ℎ𝑟𝑒𝑠ℎ𝑚𝑖𝑛 then
else
return state
Gimbal: Enabling Multi-tenant Storage Disaggregation on SmartNIC JBOFs
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
Gimbal integrates the virtual slot concept into the DRR scheduler
and ensures the number of slots for each tenant is the same. Similar
to the fair queueing mechanism [33, 37], the DRR scheduler divides
all tenants that have requests into two lists: active, where each one
in the list has assigned virtual slots; deferred, where its tenants
have no available virtual slots and wait for outstanding requests
to be completed. Also, the scheduler sets the deficit count to zero
when a tenant moves to the deferred list and does not increase
the deficit counter of the tenant. Once the tenant receives a new
virtual slot, it moves to the end of the active list, and the scheduler
resumes increasing the deficit count. Gimbal uses a cost-weighted
size for a write IO instead of the actual size (𝑤𝑟𝑖𝑡𝑒 𝑐𝑜𝑠𝑡 × 𝐼𝑂 𝑠𝑖𝑧𝑒)
to capture the write cost in the virtual slot. An IO can only be
submitted if its deficit count is larger than the weighted size. For
example, if the tenant has a 128KB write request when the system
is operating under a write cost of 3, the tenant would be allowed to
issue the operation only after three round-robin rounds of satisfying
tenants in the active list (with each round updating the deficit count
associated with the tenant).
Per-tenant priority queues. The ingress pipeline of Gimbal main-
tains priority queues for each tenant. The priority is tagged by
clients and carried over NVMe-oF requests. When a tenant is being
scheduled within an available virtual slot, the scheduler cycles over
these priority queues in a round-robin manner, uses each queue’s
weights to selects IO requests from the queue, and constructs the
IO request bundle. This mechanism allows clients to prioritize a
latency-sensitive request over a throughput-oriented request.
3.6 End-to-End Credit-based Flow Control
The switch applies an end-to-end credit-based flow control between
the client-side and the target-side per-tenant queue. This controls
the number of outstanding IOs to a remote NVMe SSD and avoids
queue buildup at the switch ingress. Unlike the networking setting,
where a credit means a fixed-size data segment or a flit [23, 30],
the number of credits in our case represents the amount of IO
regardless of the size that a device could serve without hurting QoS.
It is obtained from the congestion control module (described above).
The total credit for the tenant is the number of allotted virtual slots
times the IO count of the latest completed slot. A tenant submits
IO if the total credit is larger than the amount of outstanding IO.
Instead of using a separate communication channel for credit
exchange, we piggyback the allocated credits into the NVMe-oF
completion response (i.e., the first reservation field). Similar to
the traditional credit-based flow control used for networking, our
credit exchange/update scheme (like N23 [51, 52]) also minimizes
the credit exchange frequency and avoids credit overflow. However,
it differs in that our protocol (1) works in an end-to-end fashion,
not hop-by-hop; (2) targets at maximizing remote SSD usage with
a QoS guarantee. Algorithm 3 (in Appendix) describes the details.
3.7 Per-SSD Virtual View
Our switch provides a managed view of its SSDs to each tenant that
indicates how much read/write bandwidth headroom is available
at the target so that clients can use the SSD resource efficiently in a
multi-tenant environment. In addition, it also supports IO priority
tagging, which allows applications to prioritize storage operations
based on workload requirements. Such a view enables applications
to develop flexible mechanisms/policies, like application-specific IO
scheduler, rate limiter, IO load balancer, etc. We later discuss how
we integrate it into a log-structured merge-tree key-value store.
4 Implementation
4.1 Switch Pipeline
We built Gimbal using Intel SPDK [16]. It targets the RDMA trans-
port and extends the basic NVMe-over-Fabric target application
by adding three major components: DRR IO scheduler, write cost
estimator, and SSD congestion control. The overall implementa-
tions follow the shared-nothing architecture and rely on the reactor
framework of SPDK. On the Broadcom Stingray platform, we find
that one SmartNIC CPU (ARM A72) core is able to fully drive PCIe
Gen3 × 4-lanes SSD for any network and storage traffic profile.
Therefore, Gimbal uses one dedicated CPU core to handle a specific
SSD. Each switch pipeline is dedicated to a specific SSD and shares
nothing with other pipelines that handle different SSDs. It runs as a
reactor (i.e., kernel-bypass executor) on a dedicated SmartNIC core
asynchronously and will trigger the ingress handler if incoming
events come from RDMA qpair (or egress handler when an event is
from the NVMe qpair). A reactor uses one or more pollers to listen to
incoming requests from the network/storage. Our implementations
are compatible with the existing NVMe-oF protocol.
4.2 Parameters of Gimbal
Gimbal is affected by the following parameters:
• Max/Min Delay threshold. 𝑇ℎ𝑟𝑒𝑠ℎ𝑚𝑖𝑛 is an upper bound of
"congestion-free" latency. It should be larger than the highest
latency when there is only one outstanding IO, which is 230us on
our SSD. We set 𝑇ℎ𝑟𝑒𝑠ℎ𝑚𝑖𝑛 to 250us. 𝑇ℎ𝑟𝑒𝑠ℎ𝑚𝑎𝑥 should ensure
that SSDs achieve high utilization for all cases and provide suffi-
cient flexibility to tune other parameters. We characterized this
parameter by configuring different types of storage profiles and
found that when saturating the bandwidth, the lowest latency is
between 500us and 1000us, depending on the workload. Further,
to minimize the frequency that Gimbal enters the overloaded
state (Section 3.3), we set 𝑇ℎ𝑟𝑒𝑠ℎ𝑚𝑎𝑥 to a slightly higher value
(i.e., 1500us). Since the minimum latency of the SSD is highly
correlated with the NAND characteristics, the parameter val-
ues we determine generally apply to other TLC-based SSDs as
well. However, 3DXP, SLC, or QLC have completely different
device characteristics, and we need to adjust these thresholds
appropriately for such devices.
• 𝛼𝑇 , 𝛼𝐷 and 𝛽. 𝛼𝑇 decides how frequently the congestion signal is
generated. Although the higher value (e.g., 2−8) has a negligible
impact on the result, we set 𝛼𝑇 to 2−1 and speculatively generate
a signal to minimize the trigger rate of the "overloaded" state. 𝛼𝐷
is used to tolerate occasional latency spikes and capture recent
trends. We set it to 2−1. 𝛽 determines how fast the target rate is
increased in the underutilized state. We set the value to 8. Gimbal
can increase the target rate to a peak value within a second