Default Stateful 
Ghostery 
Block TP Cookies 
HTTPS Everywhere 
ID Detection 1* 
ID Detection 2* 
#  Sites  #  Success  Timeout  % 
1 Million 
100,000 
55,000 
55,000 
55,000 
10,000 
10,000 
917,261 
94,144 
50,023 
53,688 
53,705 
9,707 
9,702 
10.58% 
8.23% 
5.31% 
12.41% 
14.77% 
6.81% 
6.73% 
Flash
•
•
E nabled 
Stateful 
P arallel
H T T P
•
•
◦
•
•
•
•
•
•
•
•
•
•
• 
• 
•
•
•
•
•
•
•
• 
• 
• 
• 
• 
•
•
14 days 
3.5 days 
0.7 days 
0.8 days 
1 day 
2.9 days 
2.9 days 
• 
• 
Files 
Javascript
Disk
D ata 
Javascript
C alls 
Scans 
Time  to  Crawl 
Table 2:  Census measurement conﬁgurations.
An unﬁlled circle indicates that a seed proﬁle of length 10,000 was loaded into each browser instance in a parallel measurement.
“#  Success” indicates  the  number  of  sites  that  were  reachable  and  returned  a  response.  A  Timeout  is  a  request  which  fails
to completely load in 90 seconds.  *Indicates that the measurements were run synchronously on diﬀerent virtual machines.
of tracking we must carry out stateful measurements in ad­
dition to stateless ones.  Stateful measurements do not clear 
the  browser’s  proﬁle  between  page  visits,  meaning  cookie 
and other browser storage persist from site to site.  For some 
measurements the diﬀerence is not material, but for others, 
such as cookie syncing (Section 5.6), it is essential. 
Making  stateful  measurements  is  fundamentally  at  odds 
with parallelism.  But a serial measurement of 1,000,000 sites 
(or even 100,000 sites) would take unacceptably long.  So we 
make  a  compromise:  we  ﬁrst  build  a  seed  proﬁle  which  vis­
its  the  top  10,000  sites  in  a  serial  fashion,  and  we  save  the 
resulting state. 
To scale to a larger measurement, the seed proﬁle is loaded 
into  multiple  browser  instances  running  in  parallel.  With 
this  approach,  we  can  approximately  simulate  visiting  each 
website serially.  For our 100,000 site stateless measurement, 
we used the “ID Detection 2” browser proﬁle as a seed proﬁle. 
This method is not without limitations.  For example third 
parties  which  don’t  appear  in  the  top  sites  if  the  seed  pro­
ﬁle  will  have  diﬀerent  cookies  set  in  each  of  the  parallel  in­
stances.  If these parties are also involved in cookie syncing, 
the  partners  that  sync  with  them  (and  appear  in  the  seed 
proﬁle)  will  each  receive  multiple  IDs  for  each  one  of  their 
own.  This presents a trade-oﬀ between the size the seed pro­
ﬁle  and  the  number  of  third  parties  missed  by  the  proﬁle. 
We ﬁnd that a seed proﬁle which has visited the top 10,000 
sites  will  have  communicated  with  76%  of  all  third-party 
domains present on more than 5 of the top 100,000 sites. 
Handling  errors.  In presenting our results we only con­
sider  sites  that  loaded  successfully.  For  example,  for  the  1 
Million  site  measurement,  we  present  statistics  for  917,261 
sites.  The  majority  of  errors  are  due  to  the  site  failing  to 
return  a  response,  primarily  due  to  DNS  lookup  failures. 
Other causes of errors are sites returning a non-2XX HTTP 
status code on the landing page, such as a 404 (Not Found) 
or a 500 (Internal Server Error). 
Detecting  ID  cookies.  Detecting  cookies  that  store 
unique user identiﬁers is a key task that enables many of the 
results that we report in Section 5.  We build on the methods 
used in previous studies [1, 14].  Browsers store cookies in a 
structured  key-value  format,  allowing  sites  to  provide  both 
a name string  and value string.  Many sites further structure 
the value string of a single cookie to include a set of named 
parameters.  We parse each cookie value string assuming the 
format: 
(name1  =)value1|...|(nameN  =)valueN 
where | represents any character except a-zA-Z0-9  -=.  We 
determine a (cookie-name, parameter-name, parameter-value) 
tuple to be an ID cookie if it meets the following criteria:  (1) 
the cookie has an expiration date over 90 days in the future 
(2) 8 ≤  length(parameter-value)  ≤ 100, (3) the parameter-
value  remains  the  same  throughout  the  measurement,  (4) 
the parameter-value is diﬀerent between machines and has a 
similarity less than 66% according to the Ratcliﬀ-Obershelp 
algorithm  [7].  For  the  last  step,  we  run  two  synchronized 
measurements (see Table 2) on separate machines and com­
pare the resulting cookies, as in previous studies. 
What makes a tracker?  Every third party is potentially 
a  tracker,  but  for  many  of  our  results  we  need  a  more  con­
servative deﬁnition.  We use two popular tracking-protection 
lists  for  this  purpose:  EasyList  and  EasyPrivacy.  Including 
EasyList  allows  us  to  classify  advertising  related  trackers, 
while  EasyPrivacy  detects  non-advertising  related  trackers. 
The  two  lists  consist  of  regular  expressions  and  URL  sub­
strings  which  are  matched  against  resource  loads  to  deter­
mine if a request should be blocked. 
Alternative tracking-protection lists exist, such as the list 
built  into  the  Ghostery  browser  extension  and  the  domain-
based list provided by Disconnect11 .  Although we don’t use 
these lists to classify trackers directly, we evaluate their per­
formance in several sections. 
Note that we are not simply classifying domains as track­
ers  or  non-trackers,  but  rather  classify  each  instance  of  a 
third  party  on  a  particular  website  as  a  tracking  or  non-
tracking context.  We consider a domain to be in the tracking 
context if a consumer privacy tool would have blocked that 
resource.  Resource loads which wouldn’t have been blocked 
by these extensions are considered non-tracking. 
While  there  is  agreement  between  the  extensions  utiliz­
ing these lists, we emphasize that they are far from perfect. 
They  contain  false  positives  and  especially  false  negatives. 
That  is,  they  miss  many  trackers  —  new  ones  in  particu­
lar.  Indeed,  much  of  the  impetus  for  OpenWPM  and  our 
measurements comes from the limitations of manually iden­
tifying  trackers.  Thus,  tracking-protection  lists  should  be 
considered  an  underestimate  of  the  set  of  trackers,  just  as 
considering all third parties to be trackers is an overestimate. 
Limitations.  The  analysis  presented  in  this  paper  has 
11https://disconnect.me/trackerprotection 
several  methodological  and  measurement  limitations.  Our 
platform did not interact with sites in ways a real user might; 
we  did  not  log  into  sites  nor  did  we  carry  out  actions  such 
as scrolling or clicking links during our visit.  While we have 
performed deeper crawls of sites (and plan to make this data 
publicly available), the analyses presented in the paper per­
tain only to homepages. 
For  comparison,  we  include  a  preliminary  analysis  of  a 
crawl which visits 4 internal pages in addition to the home­
page of the top 10,000 sites.  The analyses presented in this 
paper should be considered a lower bound on the amount of 
tracking a user will experience in the wild.  In particular, the 
average  number  of  third  parties  per  site  increases  from  22 
to 34.  The 20 most popular third parities embedded on the 
homepages of sites are found on 6% to 57% more sites when 
internal  page loads are considered.  Similarly,  ﬁngerprinting 
scripts found in Section 6 were observed on more sites.  Can­
vas  ﬁngerprinting  increased  from  4%  to  7%  of  the  top  sites 
while canvas-based font ﬁngerprinting increased from 2% to 
2.5%.  An increase in trackers is expected as each additional 
page visit within a site will cycle through new dynamic con­
tent that may load a diﬀerent set of third parties.  Addition­
ally,  sites  may  not  embed  all  third-party  content  into  their 
homepages. 
The measurements presented in this paper were collected 
from  an  EC2  instance  in  Amazon’s  US  East  region.  It  is 
possible  that  some  sites  would  respond  diﬀerently  to  our 
measurement  instance  than  to  a  real  user  browsing  from 
residential  or  commercial  internet  connection.  That  said, 
Fruchter,  et  al.  [17]  use  OpenWPM  to  measure  the  varia­
tion in tracking due to geographic diﬀerences, and found no 
evidence  of  tracking  diﬀerences  caused  by  the  origin  of  the 
measurement instance. 
Although  OpenWPM’s  instrumentation  measures  a  di­
verse  set  of  tracking  techniques,  we  do  not  provide  a  com­
plete analysis of all known techniques.  Notably absent from 
our  analysis  are  non-canvas-based  font  ﬁngerprinting  [2], 
navigator and plugin ﬁngerprinting [12, 33], and cookie respawn­
ing  [53,  6].  Several  of  these  javascript-based  techniques  are 
currently  supported  by  OpenWPM,  have  been  measured 
with OpenWPM in past research [1], and others can be eas­
ily  added  (Section  3.3).  Non-Javascript  techniques,  such  as 
font  ﬁngerprinting  with  Adobe  Flash,  would  require  addi­
tional specialized instrumentation. 
Finally, for readers interested in further details or in repro­
ducing  our  work,  we  provide  further  methodological  details 
in  the  Appendix:  what  constitutes  distinct  domains  (13.1), 
how to detect the landing page of a site using the data col­
lected by our Platform (13.2), how we detect cookie syncing 
(13.3), and why obfuscation of Javascript doesn’t aﬀect our 
ability to detect ﬁngerprinting (13.4). 
5.  RESULTS OF 1-MILLION SITE CENSUS 
5.1  The long but thin tail of online tracking 
During  our  January  2016  measurement  of  the  Top  1  mil­
lion sites, our tool made over 90 million requests, assembling 
the largest dataset on web tracking to our knowledge. 
Our  large  scale  allows  us  to  answer  a  rather  basic  ques­
tion:  how many third parties are there?  In short, a lot:  the 
total  number  of  third  parties  present  on  at  least  two  ﬁrst 
parties is over 81,000. 
What  is  more  surprising  is  that  the  prevalence  of  third 
parties quickly drops oﬀ:  only 123 of these 81,000 are present 
on  more  than  1%  of  sites.  This  suggests  that  the  number 
of third parties that a regular user will encounter on a daily 
basis  is  relatively  small.  The  eﬀect  is  accentuated  when  we 
consider  that  diﬀerent  third  parties  may  be  owned  by  the 
same  entity.  All  of  the  top  5  third  parties,  as  well  as  12 
of  the  top  20,  are  Google-owned  domains.  In  fact,  Google, 
Facebook, Twitter, and AdNexus are the only third-party en­
tities  present  on  more  than  10%  of  sites. 
Figure  2:  Top  third  parties  on  the  top  1  million  sites.  Not 
all  third  parties  are  classiﬁed  as  trackers,  and  in  fact  the 