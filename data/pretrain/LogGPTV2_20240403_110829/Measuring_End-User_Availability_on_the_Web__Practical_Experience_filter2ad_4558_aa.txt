title:Measuring End-User Availability on the Web: Practical Experience
author:Matthew Merzbacher and
Dan Patterson
Measuring End-User Availability on the Web: Practical Experience
Matthew Merzbacher
University of California, Berkeley
PI:EMAIL
Dan Patterson
University of California, Berkeley
PI:EMAIL
Abstract
end-user
incorporate
For service applications on a network, measuring
availability, performance, and quality of service is critical.
Yet traditional software and hardware measures are both
inadequate and misleading. Better measures of availability
that
to
meaningful benchmarks and progress in providing high-
availability services. In this paper, we present the results of
a series of long-term experiments that measured availability
of select web sites and services with the goal of duplicating
the end-user experience. Using our measurements, we
propose a new metric for availability that goes beyond the
traditional sole measure of uptime.
experience will
lead
Keywords: availability, retry, metrics, dependability
1. Introduction
The World Wide Web has revolutionized commerce.
Goods, services and information are meant to be available
nonstop and the vitality of online businesses depends on
ensuring this high availability. But how do we measure
availability? The traditional measure is uptime, calculated
by taking the percentage of time servicing requests relative
to total
(excluding scheduled downtime). This
availability is typically reported as a number of “nines”, a
measure within an order of magnitude. Four nines of
availability equates to 99.99% uptime (just under an hour of
unscheduled
server
manufacturers advertise that their products offer six or more
nines (less than thirty seconds of unscheduled downtime per
year).
year). Currently,
downtime
time
per
Experience shows these numbers to be misleading – no
web sites are anywhere near as robust as their hardware and
server software alone. The advertised numbers reflect
performance under optimal operating conditions, rather than
real-world
availability
measure must capture the end-user experience, which
includes several
the network,
multiple server software layers, and separate client software
and system. More meaningful measures of availability will
help direct development of future systems, forcing focus on
assumptions. Any meaningful
independent components:
the real sources of downtime instead of a single number
and an unrealistic operating environment [2] [3].
In order to measure existing systems, undergraduate
students in a joint class between Mills College and UC
Berkeley devised and ran an experiment over several
months to on several prominent and not-so-prominent
web sites. Our experiment made hourly contact with a
list of sites and,
in some cases, performed a small
transaction. We measured and analyzed the responses in
terms of success, speed and size. Our analysis shows that
local and network conditions are far more likely to
impede service than server failure.
1.1. Related work
identify weak points
There are several systems and products available that
attempt to monitor and measure the end-user experience,
such as Topaz [5], SiteAngel [1], and Porvio [7]. These
products
end-to-end
experience by emulating transactions, but are geared
toward measuring performance in terms of transaction
speed (and causes of slowdowns) rather than availability.
In general, they are trying to monitor and measure end-
user service-level agreements. Other services, such as
Netcraft [6], monitor and report server availability and
reliability, but do not measure end-user experience.
in the
In contrast, our objectives were to measure the end-
user experience in terms of availability (including, but
not limited to response time) and locate common sources
of trouble. Based on our results, we also evaluated the
efficacy of retry, a step absent in the other techniques.
2. Experiment
Our experiment, coded in Java, accessed a variety of
different sites, ranging from local machines to servers
around the world. By including local URLs, we could
determine a baseline response time and network delay
and evaluate local problems. The experiment ran hourly
for six months on machines on two campuses, UC
Berkeley and Mills College. Both colleges are located in
the San Francisco Bay Area (California, USA) and share
some network connections to the outside world.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:27:48 UTC from IEEE Xplore.  Restrictions apply. 
To avoid problems with regular access, the experiment
was delayed by a random number of minutes at the start of
each hour, so it might run at 3:11 and then 4:35.
We present
results from three prominent sites that
characterize the behavior that we observed:
a) An online retailer, with international country-specific
versions of its site
b) A search engine, where we executed several searches
c) A directory service,
again with country-specific
versions of its site
We also measured results from sites providing news,
auctions, and other services, but we do not include the
specifics for those sites, as they are qualitatively similar.
3. Results
single number
Quantifying availability with a
is
impossible. In addition to the basic question of whether or
not we received a response, we must also consider whether
the response was partial or complete and how long it took to
arrive. Further, our model must also allow for retry when
failure does occur.
We start by presenting the raw “success or failure”
numbers for each service. We factor out different kinds of
errors from our data to try to determine where errors occur.
Our results are summarized in Table 1.
Table 1: Availability under various assumptions
All
.9305
.9888
Retail
.9311
.9887
Search
.9355
.9935
Directory
.9267
.9857
.9991
.9976
1.00
.9997
.9994
.9984
1.00
.9999
Raw
Ignoring local
problems
Ignoring local
and network
problems
Ignoring local,
network and
transient
problems
We describe the detailed meaning of each row in
subsections 3.1 – 3.4. Briefly, the first row is the overall
frequency that each site returned its data both perfectly and
quickly, the second row excludes local problems on the end-
user machine, the third row further excludes problems with
the network, and the fourth row also excludes any non-
repeatable problem. What remains are persistent problems
the company’s website or
that are likely happening at
throughout
the search
engine failed about 6.5% of the time, but that was mostly
due to local problems on our experimental platform. After
local and network problems were eliminated, the search site
never failed.
the corporation. So, for example,
The pie chart
in Figure 1 shows the frequency of
different kinds of errors, each described in detail below.
Local (82%)
Severe Network (4%)
Corporate (1%)
Medium Network (11%)
Server (2%)
Figure 1: Types of errors
3.1. Local problems
By far the most common causes of unavailability
including
were local problems on our test machines,
system-wide crashes, system administrator configuration
errors, client power outages, attacks by outside hackers,
and a host of other troubles. No particular problem was
especially more common than another – there were no
few characteristic local errors where one could focus
preventative energies. However, almost all had a
component of human error
that either caused or
exacerbated the problem.
Sometimes, for example with power outages,
the
trouble would be resolved by the next hour, while other
troubles required considerably more intervention. Part of
this was because we ran our experiments on multi-user
server machines, but individual workstations should fare
no better. Our machines, while in an academic setting,
were relatively carefully administered by professional
systems administrators and were more stable than typical
campus machines. Other students and faculty also used
them only sparingly – almost an ideal set-up.
In summary, our key observation:
Local availability dominates the end-user experience
Factoring out local outages, as done in the second row
of Table 1, provides a better estimate of availability
before the so-called “last mile” without considering the
end-user.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:27:48 UTC from IEEE Xplore.  Restrictions apply. 
3.2 Losing data
to hour as
A second kind of unavailability occurred when some, but
not all, of the requested data was retrieved. We measured
the size (in bytes) of data returned. This value fluctuated
slightly from hour
sites updated or as
advertisements changed. However, when size dipped below
a fixed threshold, it indicated a problem. A size of zero
clearly shows unavailability, but even if the site returned
with a radically small size, we believe that this indicated
incomplete data. More likely, anything less than 4KB
returned indicated some error, such as 404 (page not found).
We compared data between our two experimental platforms,
and in cases where neither location returned close to the full
size, ascribed it to server error.
3.3 Corporate failure
Where they existed, we also accessed international
versions of the server sites. Naturally, such accesses took
longer, although they frequently had less variance than the
US versions. Some failures spanned all sites for a particular
enterprise – both US and international. We call such failures
“corporate” failures and indicate them on the failure chart.
3.4 Timely results
Another type of unavailability is when a site returned
successfully, but slowly. The important consideration of
what is “too slow” is hard to define in a strict sense, so we
went about this a couple of ways. The first was to chart
60
50
40
30
20
10
)
c
e