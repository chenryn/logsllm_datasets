# Measuring End-User Availability on the Web: Practical Experience

**Authors:**
- Matthew Merzbacher, University of California, Berkeley
- Dan Patterson, University of California, Berkeley

## Abstract
For networked service applications, measuring availability, performance, and quality of service is critical. Traditional software and hardware measures are often inadequate and misleading. Better measures of availability can lead to meaningful benchmarks and progress in providing high-availability services. In this paper, we present the results of a series of long-term experiments that measured the availability of selected web sites and services, with the goal of replicating the end-user experience. Using our measurements, we propose a new metric for availability that goes beyond the traditional measure of uptime.

**Keywords:** availability, retry, metrics, dependability

## 1. Introduction
The World Wide Web has revolutionized commerce, making goods, services, and information available nonstop. The vitality of online businesses depends on ensuring high availability. However, measuring availability is challenging. The traditional measure, uptime, is calculated as the percentage of time servicing requests relative to total time (excluding scheduled downtime). Uptime is typically reported as a number of "nines," with four nines equating to 99.99% uptime (just under an hour of unscheduled downtime per year). Server manufacturers often advertise six or more nines, which translates to less than thirty seconds of unscheduled downtime per year.

Experience shows these numbers to be misleading. No web sites are as robust as their hardware and server software alone. Advertised numbers reflect performance under optimal conditions, not real-world scenarios. A meaningful availability measure must capture the end-user experience, which includes the network, multiple server software layers, and separate client software and systems. More accurate measures will help direct the development of future systems, focusing on the real sources of downtime rather than an unrealistic operating environment [2] [3].

To measure existing systems, undergraduate students from Mills College and UC Berkeley conducted an experiment over several months, testing prominent and less-prominent web sites. The experiment made hourly contact with a list of sites and, in some cases, performed small transactions. We measured and analyzed responses in terms of success, speed, and size. Our analysis shows that local and network conditions are far more likely to impede service than server failure.

### 1.1 Related Work
Several systems and products, such as Topaz [5], SiteAngel [1], and Porvio [7], attempt to monitor and measure the end-user experience by emulating transactions. These tools focus on transaction speed and causes of slowdowns rather than availability. Other services, like Netcraft [6], monitor and report server availability and reliability but do not measure the end-user experience. In contrast, our objectives were to measure the end-user experience in terms of availability (including response time) and identify common sources of trouble. Based on our results, we also evaluated the efficacy of retries, a step absent in other techniques.

## 2. Experiment
Our experiment, coded in Java, accessed a variety of sites, ranging from local machines to servers around the world. By including local URLs, we could determine a baseline response time and network delay and evaluate local problems. The experiment ran hourly for six months on machines at UC Berkeley and Mills College, both located in the San Francisco Bay Area (California, USA), and sharing some network connections to the outside world.

To avoid regular access issues, the experiment was delayed by a random number of minutes at the start of each hour. We present results from three prominent sites:
- An online retailer with international country-specific versions.
- A search engine, where we executed several searches.
- A directory service with country-specific versions.

We also measured news, auction, and other service sites, but their results are qualitatively similar and thus not included.

## 3. Results
Quantifying availability with a single number is impossible. In addition to the basic question of whether or not we received a response, we must consider whether the response was partial or complete and how long it took to arrive. Our model must also allow for retries when failures occur.

We start by presenting the raw "success or failure" numbers for each service. We factor out different kinds of errors to determine where they occur. Our results are summarized in Table 1.

| **Site** | **Raw** | **Ignoring Local Problems** | **Ignoring Local and Network Problems** | **Ignoring Local, Network, and Transient Problems** |
|----------|---------|-----------------------------|-----------------------------------------|-----------------------------------------------------|
| All      | .9305   | .9888                       | .9991                                   | 1.00                                                |
| Retail   | .9311   | .9887                       | .9976                                   | .9999                                               |
| Search   | .9355   | .9935                       | .9994                                   | .9997                                               |
| Directory| .9267   | .9857                       | .9984                                   | .9984                                               |

- **Row 1**: Overall frequency of perfect and quick data return.
- **Row 2**: Excludes local problems on the end-user machine.
- **Row 3**: Further excludes network problems.
- **Row 4**: Also excludes non-repeatable problems, leaving persistent issues likely occurring at the company's website or throughout the corporation.

For example, the search engine failed about 6.5% of the time, mostly due to local problems. After eliminating local and network issues, the search site never failed.

### 3.1 Local Problems
The most common causes of unavailability were local problems on our test machines, including system-wide crashes, configuration errors, power outages, and hacker attacks. No particular problem was more common; almost all had a human error component. Some issues, like power outages, resolved within an hour, while others required more intervention. Our machines, though in an academic setting, were carefully administered and more stable than typical campus machines. Other users used them sparingly, creating an ideal setup.

**Key Observation:**
Local availability dominates the end-user experience. Factoring out local outages provides a better estimate of availability before the "last mile."

### 3.2 Data Loss
A second type of unavailability occurred when some, but not all, requested data was retrieved. We measured the size (in bytes) of returned data, which fluctuated slightly. When the size dipped below a fixed threshold, it indicated a problem. A size of zero clearly showed unavailability, and any size less than 4KB likely indicated incomplete data or an error, such as a 404 (page not found). We compared data between our two platforms, and if neither returned close to the full size, we attributed it to a server error.

### 3.3 Corporate Failure
We also accessed international versions of the server sites, which took longer but often had less variance than the US versions. Failures spanning all sites for a particular enterprise, both US and international, were called "corporate" failures and noted in the failure chart.

### 3.4 Timely Results
Another type of unavailability is when a site returns successfully but slowly. Defining "too slow" is challenging, so we charted response times. 

[Figure 1: Types of errors]
- Local (82%)
- Severe Network (4%)
- Corporate (1%)
- Medium Network (11%)
- Server (2%)

[Figure 2: Response Time Distribution]
- 60%
- 50%
- 40%
- 30%
- 20%
- 10%

In summary, our key findings highlight the importance of considering local and network conditions in measuring web availability. Future work should focus on developing more comprehensive metrics that account for the end-user experience.