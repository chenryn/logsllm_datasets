### 0.5
### 1
### seconds
### 1.5
### 2
**Figure 4: Breakdown of Nexus Upload Latency.**

**Table 2: End-to-End Latency Measurements.**
- **Server:** 0.29 s
- **Client Laptop:** 0.46 s
- **Client Nexus:** 1.16 s

In the context of a setting where multiple parties stream data to a server for an overall summation, this approach aims to provide both types of privacy guarantees. However, their model is not suitable for our scenario because it assumes the presence of a trusted party to run a setup phase for each specific set of clients contributing to the aggregate. In our model, such a trusted party does not exist, and more importantly, it is unknown which specific clients will pass through each sample point in the future, or even the number of such clients.

### 9. Implementation and Evaluation
In this section, we demonstrate that PrivStats can operate on commodity smartphones and hardware at reasonable costs. We have implemented an end-to-end system where the clients are either smartphones (Nexus One) or commodity laptops (for some social crowdsourcing applications), and the server is a commodity server. The Secure Module (SM) was evaluated on smartphones as it runs on the client side. Our system implements the protocols shown in Figure 2, using Secure Linear Programming (SLP) and enforcing a quota for the number of uploads per client for accountability. The code was written in both C++ and Java. For our evaluation, the server runs C++ for efficiency, while the clients and SM run Java. Android smartphones use Java because Android does not fully support C++ (as of the time of this evaluation, the Android NDK lacks support for the required basic libraries). We used NTL for C++ and BigInteger for Java to implement our cryptographic protocols. The total implementation is approximately 1300 lines of code for all three parties, excluding libraries, with accountability accounting for 55% of the code. The core code of the SM is only 62 lines, making it easy to secure.

#### 9.1 Performance Evaluation
For our experiments, we used Google Nexus One smartphones (1 GHz Scorpion CPU, Android 2.2.2, 512 MB RAM), a commodity laptop (2.13 GHz Intel Pentium CPU, 2-core, 3 GB RAM), a commodity server (2.53 GHz Intel i5 CPU, 2-core, 4 GB RAM), and a high-core-count server for scalability measurements (Intel Xeon CPU, 16 cores, 1.60 GHz, 8 GB RAM). We report results with and without accountability, and with 20% of uploads using accountability, as recommended in Section 6.3.

**Table 2: End-to-End Latency Measurements.**
- **Setup and Join Latency:** These operations are insignificant, occurring once for the service and once per client, respectively.
- **Upload Latency:** This is measured from the time the client initiates an upload until the server acknowledges it, including interaction with the SM. The latency is reasonable, ranging from 0.6 s to up to 2 s for Nexus devices.

**Table 3: Runtime of the Accountability Protocol.**
- **Upload Latency with Accountability:** 0.3 s
- **Upload Latency without Accountability:** 0.02 s
- **Throughput with 0% Accountability:** 2400 uploads/core/min
- **Throughput with 20% Accountability:** 860 uploads/core/min
- **Throughput with 100% Accountability:** 170 uploads/core/min

**Table 4: Server Evaluation for Uploads.**
- **Latency:** Time for the server to process an upload from request to completion.
- **Throughput:** Number of uploads per minute the server can handle.

Since uploads occur in the background or after the client triggers them, the user does not need to wait for completion. Figure 4 breaks down the various operations involved in an upload. The accountability protocol (at both the client and server) accounts for most of the computation time (86%). Table 3 summarizes the cost of accountability.

For aggregation, we used the Paillier encryption scheme, which takes 33 ms to encrypt, 16.5 ms to decrypt, and 0.03 ms for one homomorphic aggregation on the client laptop, and 135 ms to encrypt and 69 ms to decrypt on Nexus. The aggregation time includes server computation and interaction with the SM. The latency for this operation is small: 0.46 s for 104 tuples per aggregate, which is more than typical. The server can aggregate samples as they are received, rather than waiting until the end.

To understand the capacity needed for an application, it is important to determine the throughput and latency at the server and whether throughput scales with the number of cores. We issued many simultaneous uploads to the server to measure these metrics, summarized in Table 4. The server requires 0.3 s of computation to verify a cryptographic proof, and one commodity core can process 860 uploads per minute, a reasonable number. We parallelized the server using an adjustable number of threads, with each thread processing a different set of aggregate identifiers. No synchronization between these threads was needed because each aggregate is independent. We ran the experiment on a 16-core machine, and Figure 5 shows that the throughput scales linearly with the number of requests.

We estimate the number of cores needed for an application. In a social crowdsourcing application, suppose a client uploads samples around 10 times a day when visiting a place of interest (e.g., a restaurant). In this case, one commodity core can serve about 120,000 clients.

In the vehicular case, clients upload more frequently. If \( n \) is the number of cars passing through a sample point, a server core working 24 hours can support approximately \( 24 \times 60 \times 860 / n \) statistics in one day. For example, California Department of Transportation statistics indicate that there are about 2,000 cars on average passing through a highway lane per hour. In this setting, one core supports about 620 different aggregates. The precise number depends on the application, but this estimation suggests our protocol is feasibly efficient.

We also experimented with the SM. The throughput of SID requests was limited by the number of HTTP requests the smartphone can process, as the SM has very little work to do. The SM spends approximately 140 ms for decryption and proof of decryption per aggregate (once per aggregate) and less than 5 ms per SID request on a smartphone (once per client). The server performs 300 ms worth of checks on a commodity server (once per client), resulting in more than 50 times more work than the SM, especially considering the different device capacities.

Bandwidth and storage usage are evaluated in a longer version of our paper available at [http://nms.csail.mit.edu/projects/privacy/](http://nms.csail.mit.edu/projects/privacy/).

#### 9.2 Accuracy and Effectiveness
In this section, we evaluate the accuracy of our protocol and its robustness against malicious uploads. We justify the recommended values for the quota and Uid, the total number of tuples to be uploaded for an aggregate ID.

As mentioned in Section 5, we suggest a quota of 3. On the one hand, we want the quota to be as small as possible to limit the error malicious clients can introduce into the aggregate. On the other hand, a quota of 1 or 2 would make it difficult for clients to upload Uid tuples in total, especially for aggregates with an unexpectedly low client turnout.

We analyze how much a single client can affect the aggregates by malicious uploads. Let \( N \) be the number of clients uploading for a certain aggregate, \( I \) the interval of accepted values, and \( \mu \) the aggregate result when all clients are honest. For example, using the statistics in [7], \( N \) can be 2,000 clients in an hour. For average speed computation, \( I = (0, 100) \) mph and \( \mu \) might be 60 mph. The highest change in the average that a client can induce is \( \pm |I| \times \text{quota} / N \), and the maximum fractional error is \( \pm |I| \times \text{quota} / (N \times \mu) \), where \( |I| \) is the length of the interval \( I \). If \( N \) is large (a popular area), the error introduced by a client is small. For our example, this is equal to \( \pm 0.15 \) mph and 0.25% for a quota of 3, both rather small.

Next, we evaluate the accuracy of our scheme both analytically and empirically against real traces. We obtained real traces from the CarTel project testbed, containing an average of approximately 400 one-day paths of taxi drivers in the Boston/MA area for each month of 2008, driving mainly through the Boston area but extending to MA, NH, RI, and CT areas. In-car devices report segment ID, time, average speed on the segment, and average delay on the segment. We considered that an aggregate is computed on each segment every hour and restricted our attention to aggregates with at least 50 drivers, which is most often the case in practice. We averaged all the results reported below over each month of the year and over all aggregates considered.

As discussed in Section 7, for "count" aggregates, we do not need the SM and have no Uid. For non-count aggregates, the choice of Uid imposes a mild tradeoff between the accuracy of the aggregate result and privacy. A low Uid may not allow some drivers to upload (because at most Uid tuples must reach the server), while a large Uid may be hard for clients to achieve in cases with an unexpectedly low client turnout because each client can upload at most the quota tuples. If the server receives a lower number of tuples than Uid, the server learns some information about the clients because the number of tuples uploaded is no longer independent of the number of tuples generated. If the number of tuples uploaded tends to be close to Uid for most cases, then little information is leaked.

Uid should be chosen as a function of the historical number of uploads at an aggregate point, as follows. In Figure 6, we vary Uid by using combinations of avg – the average number of uploads at a sample point over a year – and std – the corresponding standard deviation. In practice, avg and std can be obtained from historical data.