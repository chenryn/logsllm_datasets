 0.5
 1
seconds
 1.5
 2
Figure 4: Breakdown of Nexus upload latency.
Table 2: End-to-end latency measurements.
Server 0.29 s
Client laptop 0.46 s
Client Nexus 1.16 s
setting of n parties streaming data to a server computing an overall
summation. While this is an excellent attempt at providing both
types of privacy guarantees together, their model is inappropriate for
our setting: they assume that a trusted party can run a setup phase
for each speciﬁc set of clients that will contribute to an aggregate
ahead of time. Besides the lack of such a trusted party in our model,
importantly, one does not know during system setup which speciﬁc
clients will pass through each sample point in the future; even the
number n of such clients is unknown.
9.
IMPLEMENTATION AND EVALUATION
In this section, we demonstrate that PrivStats can run on top of
commodity smartphones and hardware at reasonable costs. We
implemented an end-to-end system; the clients are smartphones
(Nexus One) or commodity laptops (for some social crowd-sourcing
applications), the server is a commodity server, and the SM was
evaluated on smartphones because it runs on clients. The system
implements our protocols (Fig. 2) with SLP and enforcing a quota
number of uploads at each aggregate per client for accountability.
We wrote the code in both C++ and Java. For our evaluation
below, the server runs C++ for efﬁciency, while clients and SM
run Java. Android smartphones run Java because Android does not
fully support C++. (As of the time of the evaluation for this paper,
Android NDK lacks support for basic libraries we require.) We
implemented our cryptographic protocols using NTL for C++ and
BigInteger for Java. Our implementation is ≈ 1300 lines of code for
all three parties, not counting libraries; accountability forms 55%
of the code. The core code of the SM is only 62 lines of code (not
including libraries), making it easy to secure.
9.1 Performance Evaluation
In our experiments, we used Google Nexus One smartphones
(1GHz Scorpion CPU running Android 2.2.2., 512 MB RAM), a
commodity laptop (2.13 GHz Intel Pentium CPU 2-core, 3 GB
RAM), a commodity server (2.53GHz Intel i5 CPU 2-core, 4 GB
RAM), and a server with many cores for our scalability measure-
ments: Intel Xeon CPU 16 cores, 1.60 GHz, 8 GB RAM. In what
follows, we report results with accountability, without accountability
(to show the overhead of the aggregation protocols), and with 20%
of uploads using accountability as recommended in §6.3.
Table 2 shows end-to-end measurements for the four main op-
erations in our system (Fig. 2); these include our operations and
network times. We can see that the latency of setup and join are
insigniﬁcant, especially since they only happen once for the service
or once for each client, respectively.
The latency of upload, the most frequent operation, is measured
from the time the client wants to generate an upload until the upload
is acknowledged at the server, including interaction with the SM.
The latency of upload is reasonable: 0.6 s and up to 2 s for Nexus.
Table 3: Runtime of the accountability protocol.
Server metric
Upload latency, with account.
Upload latency, no account.
Throughput with 0% account.
Throughput with 20% account.
Throughput with 100% account.
Measurement
0.3 s
0.02 s
2400 uploads/core/min
860 uploads/core/min
170 uploads/core/min
Table 4: Server evaluation for uploads. Latency indicates the time it
takes for the server to process an upload from the moment the request
reaches the server until it is completed. Throughput indicates the num-
ber of uploads per minute the server can handle.
Since this occurs either in the background or after the client triggered
the upload, the user does not need to wait for completion. Figure 4
shows the breakdown into the various operations of an upload. We
can see that the accountability protocol (at client and server) takes
most of the computation time (86%). The cost of accountability is
summarized in Table 3.
For aggregation, we used the Paillier encryption scheme which
takes 33 ms to encrypt, 16.5 ms to decrypt, and 0.03 ms for one
homomorphic aggregation on the client laptop, and 135 ms to en-
crypt and 69 ms to decrypt on Nexus. The aggregation time includes
server computation and interaction with the SM. The latency of
this operation is small: 0.46 s for 104 tuples per aggregate, more
tuples than in the common case. Moreover, the server can aggregate
samples as it receives them, rather than waiting until the end.
In order to understand how much capacity one needs for an appli-
cation, it is important to determine the throughput and latency at the
server as well as if the throughput scales with the number of cores.
We issued many simultaneous uploads to the server to measure these
metrics, summarized in Table 4. We can see that the server only
needs to perform 0.3 s of computation to verify a cryptographic
proof and one commodity core can process 860 uploads per minute,
a reasonable number. We parallelized the server using an adjustable
number of threads: each thread processes a different set of aggregate
identiﬁers. Moreover, no synchronization between these threads
was needed because each aggregate is independent. We ran the
experiment on a 16-core machine: Fig. 5 shows that the throughput
indeed scales linearly in the number of requests.
We proceed to estimate the number of cores needed in an applica-
tion. In a social crowd-sourcing application, suppose that a client
uploads samples on the order of around 10 times a day when it visits
a place of interest (e.g., restaurant). In this case, one commodity
core can already serve about 120,000 clients.
In the vehicular case, clients upload more frequently. If n is
the number of cars passing through a sample point, a server core
working 24 h can thus support ≈ 24 · 60 · 860/n statistics in one
day. For instance, California Department of Transportation statistics
662 25
 20
c
e
s
/
s
d
a
o
p
u
l
t
 15
 10
n
e
i
l
C
 5
 0
 0
 2
 4
 6
 8  10  12  14  16  18
 100
 90
)
%
t
(
e
g
a
n
e
c
r
e
P
 80
 70
 60
 50
Nr. uploads/Uid
Nr. real uploads/Nr. clients
Avg. delay
Avg. speed
 10
r
o
r
r
t
E
e
g
a
n
e
c
r
e
P
 8
 6
 4
 2
 0
avg
avg+std/2 avg+std 3/2avg avg+2std avg*2
avg
avg+std/2 avg+std
3/2avg avg+2std
avg*2
No. of threads
Choice of Uid
Choice of Uid
Figure 5: Throughput at the server versus
number of parallel threads, showing linear
scaling on a 16-core machine.
Figure 6: Total uploads at the server divided
by Uid, and the number of clients that upload
over total clients passing by a sample point.
Figure 7: CarTel data showing the error
in computing the average delay or speed for
various choices of Uid.
[7] indicate that there are about 2, 000 cars on average passing in
an hour through a highway lane. In this setting, one core supports
about ≈ 620 different aggregates. Of course, the precise number
depends on the application, but this estimation suggests our protocol
is feasibly efﬁcient.
We experimented with the SM as well; the throughput of sid
requests was mostly limited by how many http requests the smart-
phone can process. This is because the SM has very little work to do.
The SM spends ≈ 140 ms for decryption and proof of decryption
per aggregate (once per aggregate), and < 5ms per sid request on
a smartphone (once per client), while the server performs 300 ms
worth of checks on a commodity server (once per client), resulting
in more than 50 times more work than the SM especially when
considering the different device capacities.
Bandwidth and storage usage are evaluated in a longer ver-
sion of our paper to be found at http://nms.csail.mit.edu/
projects/privacy/.
9.2 Accuracy and Effectiveness
In this section we evaluate the accuracy of our protocol and its
robustness against malicious uploads. In the process, we justify
recommended values for quota and Uid, the total number of tuples
to be uploaded for aggregate id.
As mentioned in §5, we suggest quota = 3. Our reasoning is
the following: On the one hand, we want the quota to be as small
as possible to limit the error malicious clients can introduce into
the aggregate. On the other hand, a quota of 1 or 2 would make
it difﬁcult for clients to upload Uid tuples in total: for aggregates
with an unexpectedly low client turnout, participating clients need
to upload more tuples in the place of missing clients to reach Uid.
We ﬁrst analyze how much a single client can affect the aggregates
by malicious uploads. Let N be the number of clients uploading
for a certain aggregate, I the interval of accepted values, and µ the
aggregate result when all clients are honest. For example, using
the statistics in [7], N can be 2000 clients in an hour. For average
speed computation, I = (0, 100) mph and µ might be 60 mph, for
instance. The highest change in the average that a client can induce is
±|I|quota/N and the maximum fractional error is ±|I|quota/N µ,
where |I| is the length of the interval I. We can see that if N is
large (a popular area), the error introduced by a client is small. For
our example, this is equal to ±0.15 mph and 0.25% for quota = 3,
both rather small.
Next, we evaluate the accuracy of our scheme both analytically
and empirically against real traces. We obtained real traces from
the CarTel project testbed, containing an average of ≈ 400 one-
day paths of taxi drivers in the Boston/MA area for each month of
year 2008 driving mainly through the Boston area, but extending
to MA, NH, RI, and CT areas. In-car devices report segment id
(a segment is a part of a road between two intersections), time,
average speed on the segment and average delay on the segment.
We considered that an aggregate is computed on each segment in
every hour. We restricted our attention to aggregates with at least
50 drivers, which is most often the case in practice. We averaged all
the results reported below over each month of the year and over all
aggregates considered.
As discussed in §7, for “count” aggregates we do not need the
SM and we have no Uid. For non-count aggregates, the choice of Uid
imposes a mild tradeoff between accuracy of aggregate result and
privacy. A low Uid may not allow some drivers to upload (because
at most Uid tuples must reach the server), while a large Uid may
be hard for clients to achieve in cases with an unexpectedly low
client turnout because each client can at most upload quota tuples.
If the server receives a lower number of tuples than Uid, the server
learns some information about the clients because the number of
tuples uploaded is no longer independent of the number of tuples
generated. If the number of tuples uploaded tends to be close to Uid
for most cases, then little information is leaked.
Uid should be chosen as a function of the historical number of
uploads at an aggregate point, as follows. In Figure 6, we vary Uid
by using combinations of avg – the average number of uploads at a
sample point in all our trace over a year – and std – the correspond-
ing standard deviation. In practice, avg and std can be obtained