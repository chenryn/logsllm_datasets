Counter-intuitively, when no teams are responsible, more
teams get involved. A fundamental challenge in incident routing
is engineers’ lack of visibility into other ISPs and customer systems,
which may be experiencing ongoing DDoS attacks, BGP hijacks,
or bugs/misconfigurations. CRIs are especially prone to starting
with missing information as these issues can be varied in nature
and hard to debug remotely. In such cases, it is sometimes faster to
rule out teams within the cloud first rather than to wait or blame
others. Ironically, this ends up involving more teams.
One example from the dataset is where a customer was unable to
mount a file-share. Suspects included storage, switches and links in
the network, the load balancer, or the provider’s firewalls, among
others. After ruling out all these components, operators found the
customer had mis-configured their on-premises firewall. Customer
misconfigurations or a workload beyond the customer VM’s capac-
ity was responsible for this and 27 other incidents in our dataset;
the PhyNet team was engaged in the investigation of each.
Concurrent incidents and updates are hard to isolate. DC is-
sues are often a result of management operations that create un-
intended side effects [12, 33]. Out of the 200 incidents we stud-
ied, 52 were caused by upgrades. These updates are not limited to
those made by the provider as they typically partner with hard-
ware vendors that have their own upgrade cycles. It can be difficult
Figure 1: (a) Fraction of PhyNet incidents (per-day) created
by its monitors, by those of other teams, and by customers.
(b) Fraction of incidents of each type that are mis-routed.
investigation times to protect company sensitive data, however, the
reader can refer to the public incident reports of [2, 7] as a lower
bound (at the time of this writing, the maximum investigation time
in these reports was 25 hours).
3.1 What is the Cost of Incident Routing?
As the core networking team, the physical networking team’s
(PhyNet’s) purview is every switch and router in the DC. They
are on the critical path of most distributed systems and the analysis
of their incidents serves as an interesting case study of mis-routings.
Most PhyNet incidents are discovered by its own monitoring
systems and are routed correctly to PhyNet (Figure 1). But some of
the incidents PhyNet investigates are created by other teams’ mon-
itoring systems or customers. Of the incidents that pass through
PhyNet, PhyNet eventually resolves a fraction, while others are
subsequently routed to other teams. In the former case, if the inci-
dent went through other teams, their time will have been wasted in
proving their innocence. In the latter, the same is true of PhyNet’s
resources. This also delays resolution of the incident. 58% of inci-
dents passing through PhyNet fall into one of these categories. We
find perfect (100%) accuracy in incident routing can reduce time to
mitigation of low severity incidents by 32%, medium severity ones
by 47.4%, and high severity ones by 0.15% (all teams are involved in
resolving the highest severity incidents to avoid customer impact).
Across teams and incidents, better incident routing could elimi-
nate an average of 97.6 hours of investigations per day — exceeding
302 hours on ∼10% of days.
The incidents resolved by PhyNet are investigated by 1.6 teams
on average, and up to 11 teams in the worst case. Mis-routed inci-
dents take longer to resolve (Figure 2): on average, they took 10×
longer to resolve compared to incidents that were sent directly to
the responsible team. For 20% of them, time-to-mitigation could
have been reduced by more than half by sending it directly to
PhyNet (Figure 3). These incidents are likely a biased sample: mis-
routing may indicate the incident is intrinsically harder to resolve;
but our investigation into the reasons behind mis-routing indicates
that many hops are spurious and can be avoided (see §3.2).
PhyNet is often one of the first suspects and among the first
teams to which incidents are sent. As a result, daily statistics show
that, in the median, in 35% of incidents where PhyNet was engaged,
the incident was caused by a problem elsewhere (Figure 4).
3.2 Why Do Multiple Teams Get Involved?
We study why incident routing is difficult by analyzing, in depth,
200 rerouted incidents. To our knowledge, this is the first case study
focusing on the reasons behind cloud incident routing problems.
0.00.20.40.60.81.00.00.20.40.60.81.00.00.40.60.81.00.20.40.60.81.0CRICreated by PhyNet monitorsCreated by other teams' monitorsFraction of PhyNet incidents per day(a)Fraction of PhyNet Incidents mis-routed per day(b)CDF0.00.010-810-610-410-2100Time (normalized)Multiple teams investigate Single team investigatesCDF00.20.40.60.8110xSIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Gao et al.
Figure 3: Investigation time we can reduce (%).
to separate the effects of these concurrent incidents and updates.
Sometimes, the same issue may spawn multiple incidents — one
for the component at fault and one for each dependent service. All
of these teams investigate the incident in parallel until the cause is
discovered. This was the case in 20 of the 200 incidents. In other
cases, an incident may be mis-attributed to an open problem even
though it is a separate incident. The team responsible for the exist-
ing problem will need to show that the new incident is un-related.
3.3 Design Goals
Our findings motivate a set of design goals:
Finding the right team precedes finding the root cause. One
approach to routing incidents is to try to automatically find the
root cause. Sadly, these types of approaches are fundamentally tied
to the semantics of specific applications [15, 17, 73] and are difficult
to scale to today’s DCs and the diverse applications they run.
When an incident is created, it is an implicit acknowledgment
that automation has failed to mitigate it. We find, as others have
done [15, 17, 21, 73]: human intervention is often necessary and
incident routing is an important first step in the investigation pro-
cess.
Incident routing should be automatic and robust. There are
too many incidents, too much variety in the incidents, too many
teams, and too much monitoring data for a human to consistently
make accurate decisions—operator anecdotes motivate the need for
assistance. This assistance cannot be limited to classifying known
problems as systems continuously change, new problems arise, and
old problems are patched. It must also be able to react to changing
norms: different clusters have different baseline latencies or device
temperatures. These values may also change over time.
The system should not be monolithic. Any system that directly
examines all monitoring data across the provider is impractical. Part
of the reason for this is technical. The volume of monitoring data
would cause significant scalability, performance, and operational
challenges — even if we could gather this data, the high-dimensional
nature of the data makes it hard to reason about (see §1). Another
part is human: no one team can expect to know the ins and outs of
other teams’ monitoring data and components.
Teams should provide expertise on data, but not routing de-
cisions. Operators rely on a wide range of monitoring data. Our
PhyNet team uses tools such as active probing, tomography, packet
captures, and system logs, among others. An incident routing sys-
tem should be able to utilize all such data and to reason about
which is useful for a given incident. Given the diversity of teams,
even if we have access to their monitoring data, domain expertise
is needed to parse and understand it. However, once parsed, the
system can do the rest of the heavy lifting so teams need not be
Figure 4: Fraction (%) of incidents per-day mis-routed
through PhyNet (it was not responsible).
experts in incident routing, only in the relevant data. A corollary
of this point is the system should be able to explain why certain
routing decisions were made.
The system should be robust to partial and uneven deploy-
ment. We found a number of fundamental challenges in building
an optimal incident router. Some issues are the fault of external
organizations to which an internal system will have little visibility.
Internally, incident routing infrastructure will inevitably be uneven
— some teams may be new or have new components to which ana-
lytics have not caught up, other systems’ incidents are just plain
hard to route.
4 DESIGN OVERVIEW
Our solution centers around the concept of a “Scout”: a per-team
ML-assisted gate-keeper that takes as input the monitoring data of
a team, and answers the question: “is this team responsible for this
incident?” The answer comes with an independent confidence score
(measuring the reliability of the prediction) as well as an explanation
for it. Fundamentally, Scouts are based on our operators’ experience
that team-specific solutions are much easier to build and maintain
compared to application-specific ones [15, 73]. Scouts are team-
centric, automated, and continually re-trained.
Decomposing incident routing. Our key design choice is the de-
composition of incident routing into a per-team problem. Not only
does this make the problem tractable, but it also makes incremental
progress possible and insulates teams from having to worry about
the system as a whole. There are tradeoffs to this design, but we
find them acceptable in return for tractability (see §9).
We do not expect every team (or even a majority of them) to build
Scouts. Rather, we expect that, for teams that are disproportionately
affected by incident mis-routings, there is a substantial incentive to
constructing a Scout as they can automatically turn away incidents
that are not the team’s responsibility (saving operator effort) and
acquire incidents that are (speeding up time to mitigation). Teams
are also incentivized to keep them up-to-date and accurate in or-
der to maintain a high confidence score. An interesting result of
our work is: even a single well-made and well-positioned Scout can
improve the system as a whole (see §7).
We can compose Scouts in various ways, from integrating them
into the existing, largely manual, incident routing process to de-
signing a new Scout Master (see Appendix C). We focus on the
challenges of designing a Scout; we leave a detailed exploration of
Scout Master design to future work.
Automating the process. To facilitate the maintenance (and often
construction) of Scouts by non-ML-experts, our design includes a
Scout framework to automate this task. The Scout framework allows
Fraction of time misrouted PhyNet incidents spend in other teams0 20  40   60 80 1000.00.20.40.60.81.0CDF0.00.20.40.60.81.00CDFFraction of PhyNet incidents with PhyNet as a waypoint20406080100SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Scouts extract relevant components from the incident description.
Operators enable this by specifying how to detect such components
in the incident description (dependent components can be extracted
by using the operator’s logical/physical topology abstractions [52]).
Operators typically use machine-generated names for these compo-
nents and can specify how they can be extracted from the incident
using regular expressions:
Configuration
let VM = ;
let server = ;
let switch = ;
let cluster = ;
let DC = ;
Example incident
(made up)
VM X.c10.dc3 in cluster
c10.dc3 is experiencing
problem connecting to
storage cluster c4.dc1
Tagging monitoring data with additional metadata. Scouts
also need to pull the relevant monitoring data and decide how to
pre-process it. Operators can assist in this process as well (this infor-
mation may already be part of the DC’s topology abstraction). First,
the location of each piece of monitoring data so the Scout can access
it. Second, the component associations of the data, e.g., to which
cluster and/or switch it refers. Third — to assist pre-processing — a
data type and optional class tag. For example:
MONITORING dataset_1 = CREATE_MONITORING(resource_locator,
{cluster=Y,server=Z},
TIME_SERIES, CPU_UTIL);
The data type can be one of TIME_SERIES or EVENT. Time-series
variables are anything measured at a regular interval, e.g., utiliza-
tion, temperature, etc. Events are data points that occur irregularly,
e.g., alerts and syslog error messages. All monitoring data can be
transformed into one of these two basic types, and Scouts use a
different feature engineering strategy for each (see §5.2). Note, op-
erators may apply additional pre-processing to the monitoring data;
for example, filtering out those syslogs they consider to be noise.
The class tag is optional (our PhyNet Scout only has two data-sets
with this tag), but enables the automatic combination of “related”
data sets — it ensures we can do feature engineering properly and
do not combine apples and oranges (see §5.2).
Operators provide this information through configuration files
(Figure 5). To modify the Scout, operators can modify the con-
figuration file, e.g., by adding/removing references to monitoring
data or changing the regular expressions the Scout uses to extract
components from the incident text.
5.2 Feature Construction and Prediction
A Scout needs to examine each incident and decide if its team is
responsible (maybe based on past incidents). ML is particularly well
suited to such tasks (see §1).
We first need to decide whether to use supervised or unsuper-
vised learning. Supervised models are known to be more accurate
(Table §3). But supervised models had trouble classifying: (a) infre-
quent and (b) new incidents — there is not enough representative
training data to learn from [47]2. Thus, we opted for a hybrid so-
lution that uses supervised learning to classify most incidents but
2This is consistent with the high accuracy of these models as such incidents are rare.
Figure 5: The anatomy of a Scout.
teams to provide a simple configuration file that provides guidance
on their monitoring data — whether measurements are time-series
or a log of events; whether different pieces of data refer to different
statistics of a common component; or if certain data/components
should be ignored. The framework then automatically trains, re-
trains, and evaluates models to achieve the desired output. The
team can improve upon the Scout by tweaking the input features,
by adding additional models, or by adding specific decision rules.
In many ways, our framework mirrors a recent push toward
AutoML [46, 56]. Sadly, existing AutoML techniques are poorly
suited to incident routing because: (1) their sensitivity to the format
of input data makes them difficult to use [36]; (2) they deal poorly
with behavior that is slightly different from that found in the train-
ing set; (3) they are a black box, making it hard for operators to
reason about why they receive an incident; and (4) in return for
automation, they typically explore a huge search space and have a
high training cost. By building a framework specialized for incident
routing, we create a better solution. Such a framework is critical
for deploying an ML-based Scout in production as it helps (e.g.,
PhyNet) operators (not familiar with ML) maintain the Scout over
time and to incorporate new monitoring data or remove old ones.
5 THE SCOUT FRAMEWORK