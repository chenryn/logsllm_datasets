arriving request in the past, the friend replies with a N ACK.
A friend that receives a troubleshooting request and runs
the application under troubleshooting only becomes a helper
some of the time, with probability Ph. If it always chose
to participate, the second-to-last-hop node could infer infor-
mation about the last-hop node. When the application under
troubleshooting is very popular, with high probability, the last-
hop node is capable to help. Therefore, the previous node can
compare the request and reply and isolate the last-hop helper’s
conﬁguration state.
A helper needs to update the troubleshooting request; for
each suspect entry e, it increments Counte(i) where i is its
own value for e (extending the value distribution vector as
necessary if i is not already represented). Then, the helper
decrements R. If R is positive, the helper proxies the request
to one of its friends.
If R becomes 0, the node is the last hop. The last-hop node
waits for a random amount of time, then sends the reply back
to the previous hop. Without the random wait, the second-to-
last hop node could know that the reply came from the last hop
and compare the Counte(i)’s in request and reply to obtain
the last-hop node’s values. The reply follows the request path
back to the sick machine. The sick machine ﬁrst subtracts
the random initialization from the value distributions; then it
performs PeerPressure diagnosis.
Each node on the forwarding path must record the ReqID,
the request arrival time, along with the previous and next hop
friend. There is a timeout associated with each request. If a
node does not receive a valid reply when the timeout expires,
or if it must go ofﬂine, it sends backwards the reply including
the aggregate of past samples up to itself and also notiﬁes its
next hop to terminate its waiting status. We analyze the proper
timeout values in Section VI.
B. Attacks Against Previous Protocol
We now present the possible privacy-compromising attacks
against our previous design, as follows:
• Gossip attacks: A helper directly contributes its relevant
conﬁguration to the request. If the helper’s previous
and next hop friends collude,
they can determine its
conﬁguration information.
• Polling attacks: Even with probabilistic helping, a curious
friend may repeatedly send fake troubleshooting requests
to its next hop with R = 1 and determine the last-hop
contribution by comparing the request and reply. Even
with a random wait at the last hop, the attacker can still
conduct a statistical analysis to guess when the next hop
contributes to the request.
C. Enhancement 1: Countering Polling Attacks by Eliminating
R
To mitigate the polling attack, we avoid specifying the
remaining number of hops R explicitly. Instead, each helper
node only proxies the request further with a probability, Pf =
1 − 1/N, where N is the total number of samples needed;
otherwise it becomes the last hop. This results in N helpers
being involved on average. This probablistic proxying makes
routing entirely historyless. Nodes that do not help always
forward the request.
D. Enhancement 2: Countering Gossip Attacks with Cluster-
ing
We mitigate the gossip attacks through a cluster-based se-
cure multi-party sum scheme as illustrated in Figure 2. When a
node receives a troubleshooting request, instead of contributing
to the request individually, it forms a troubleshooting cluster
from its immediate friends and initiates a secure multi-party
sum procedure that blends individual contributions into an
aggregate that encapsulates the contributions from both the
cluster and the past hops. The initiating node serves as the
cluster entrance. A separate cluster member must be selected
as the cluster exit for receiving the aggregate; in this way, no
single node knows the aggregate contribution of a cluster.
In this section, we assume that each entry e is known to
have only a few possible values (e.g. true or false); the next
section explains how to change our algorithm when this is not
the case. This assumption allows us to represent the value
distribution of e as a ﬁxed vector of Counte(i) for each
entry e and each value i of e. The contribution of the cluster
entrance includes the aggregate value distribution from the
previous hops. Members who do not run the application or
who choose not to help according to Ph will contribute the all
zeroes vector. Members who help will set the vector element
corresponding to their value to 1, and 0’s for the rest.
The detailed steps of our cluster-based secure multi-party
sum procedure are as follows (see also Figure 2):
1) Random share generation and distribution: Each cluster
participant generates G random shares for its contri-
bution vector, where, G is the cluster size. It
then
distributes each share to a distinct cluster member. The
contribution vector also includes a value Vh, which is 0
or 1 depending on whether the member decided to help
or not.
(cid:80)G−1
2) Cluster exit election: The cluster head assigns all clus-
ter members (excluding itself) sequential numeric IDs,
starting at 0. Each cluster member i selects a random
nonce ni and broadcasts a commitment [12] to it. After
receiving all commitments, the members broadcast their
nonces. Each member veriﬁes all the commitments and
computes the sum n =
i=0 ni, and then picks the
member with ID = n mod G − 1 to be the cluster
exit. This results in a fairly chosen random number in the
range 0 . . . G − 1. As an optimization, cluster members
who have no friends outside the cluster can indicate this
upon accepting the invitation; those members will be
excluded from the choice of potential cluster exits to
avoid dead ends.
3) Unicast subtotal to the cluster exit: Each cluster member
sums up all the shares it has received from others and
unicasts its subtotal to the cluster exit.
4) Exiting the cluster: The cluster exit sums up the received
subtotals of contribution vectors from all participants.
This aggregate is the value distribution from the past up
to the cluster exit. The exit also sums up the received
shares of Vh to obtain
G Vh, which is the number of
(cid:80)
(cid:80)
G Vh
cluster members that were able to help. With probability
, the cluster exit further proxies the request to
P
f
one of its friends, which becomes a cluster entrance of
the next cluster hop. While it is possible to turn the
cluster exit into a cluster entrance for the next cluster, we
observe from our MSN IM data (Section V-B) that such
adjacent clusters contain 14.15% overlapping members,
reducing the value of the next clusters contribution.
During cluster formation, a friend can decline the cluster
invitation if its friendship with the cluster entrance is consid-
ered private or if it has already seen the request. The decision
about whose invitation to accept must be pre-conﬁgured by the
FTN node owners. Also during cluster formation, the cluster
entrance distributes the public keys of all cluster participants
(that have accepted the invitation) to each of them for their
future secure communications; this is necessary because the
cluster participants may not be friends with one another and
thus may not know each other’s public keys.
The cluster exit needs to record the cluster entrance as the
previous hop for the return trip of the troubleshooting request.
The new cluster entrance records the previous cluster exit as its
previous hop. The other cluster members only need to record
the ReqID to avoid loops in case they receive the same request
in the future.
One may wonder whether it would be possible to just use
a single, large cluster. First of all, a large cluster incurs a
heavy cost because the communication cost of the multi-party
secure sum procedure is of O(G2) where G is the cluster size.
Also, a single cluster would not sufﬁciently hide the identity
of the sick machine, who would be the cluster head. Finally,
we must adhere to our recursive trust model by inviting only
immediate friends to join the cluster. According to the MSN
IM user data (Section V-B), the median number of friends a
user has is 9 (some of which may not run the application under
troubleshooting or may not be willing to help); since we need
at least 10 helper samples for PeerPressure diagnosis [17], a
single cluster is simply not sufﬁcient.
1) Adaptive Ph for Better Privacy in case of Cluster En-
trance and Exit Collusions:
Our scheme achieves very good privacy when there is no
collusions between the cluster entrance and the exit. However,
when they do collude, they will obtain the aggregate contribu-
tion of the cluster. The smaller the cluster is, the less privacy
can we achieve with our cluster-based secure multi-party sum
algorithm. In particular, if all (or most) of the cluster members
decide to help, then an attacker can guess with high certainty
that a given cluster member runs the application. To this end,
we allow cluster participants to adaptively choose their Ph
according to the cluster size and the privacy level they desire.
In general, for smaller clusters or for better privacy guarantees,
we must use a lower value of Ph. Of course, smaller Ph
will increase the number of nodes that must be queries for
each request. We give an an analysis of the trade-off between
desired privacy levels and efﬁciency using the MSN IM user
data in Section V.
2) Iterative Helper Selection: The adaptive method of
choosing Ph will achieve probable innocence (i.e. when fewer
than half of the members become helpers; see Section V-C)
with high probability. However, to achieve this, small cluster
sizes need to have a Ph near zero, thereby increasing the length
of the number of clusters that need to be traversed to collect
enough data, especially when the average cluster sizes are
small. As an alternative, we present an iterative scheme that
achieves the same privacy guarantees while collecting more
contributions from neighbors.
In this scheme, before any data aggregation is performed,
every cluster member, regardless of whether it runs the appli-
cation or not, randomly decides whether to participate or not
and sets Vp to 0 or 1. It decides to participate probability Pp,
which is close to 1
2 . Then the cluster performs a multi-party
sum to add all the Vp values count the number of participating
members. If this sum includes more than half the cluster
members, the cluster members discard their original decisions
and randomly pick a new Vp, repeating this entire step. This
process is repeated until fewer than half the members have
decided to participate.
After this, the aggregation proceeds as before, except instead
of using Vh to decide whether to participate, only those
members who decided to participate and are running the
application will contribute to the aggregate. All other members
will contribute zero to the overall aggregate.
In the ﬁrst step, all the members pick a Vp, regardless
of whether they are running the application or not. This
way,
the count of participating members does not reveal
any information. In the second step, fewer than half the
members participate, hence even if the cluster-wide aggregate
is intercepted, it is not known whether each member runs the
application with a probability greater than one-half.
The probability Pp will depend on the size of the cluster,
just as with Ph. However, in general Pp can be larger than
Ph, since too many members participating results in an extra
round of communication rather than a privacy compromise.
The choice of Pp involves a trade-off: with a Pp too high,
the ﬁrst step will involve a high number of retries, increasing
the communication cost. If Pp is too low, few members will
participate in each cluster, which means that more clusters will
be needed to collect enough samples. We explain our choice
of Pp in Section V.
3) Countering Sybil Attack with Threshold-Driven Helping:
A curious cluster entrance may launch a Sybil attack [6]
against its friends by including in the cluster a large number of
“ghost” friends who are just the cluster entrance itself. Then,
with high probability, the cluster exit will be elected to be
one of the ghost friends, resulting in successful collusion.
One countermeasure is that a cluster member only helps
when there are a threshold number T common friends of
the cluster member and the cluster entrance in the cluster.
With this threshold, it takes at least T colluders to expose
the cluster member’s contribution. However, this strategy also
increases the required hop count for troubleshooting, since
fewer friends will choose to be helpers. We will evaluate the
Fig. 2. Parameter Aggregation and Propagation within a Cluster.
tradeoff between the hop count overhead and the threshold
scheme in Section V-C.4, using the MSN IM topology.
Members who fall below the threshold still participate in the
secure sum but do not help, contributing to the privacy of other
cluster members. If the complete friendship topology were
known, such members could be identiﬁed and discarded from
consideration by the attackers; however, one of our security
assumptions is that individual friendship relationships are kept
private (Section III-B).
E. Aggregate Cardinality Information
In this section, we address the case when the set of possible
values for suspect entries is unknown. In this case, we cannot
randomly initialize the value distribution (Section IV-A). We
are also unable to perform the multi-party sum to aggregate
the value distribution within a cluster, as that requires a ﬁxed-
length vector with one entry for each possible value. Instead,
our scheme is to have the sick machine choose a hash function
h to map values of each suspect entry into a small range
0 . . . C − 1. The FTN nodes will then maintain the number
of entry values that hash to each of the C values in the
troubleshooting request. This then requires us to have a second
round query to ﬁnd out the most popular values of the top
ranking root cause candidates yielded from the ﬁrst round
PeerPressure diagnosis, for the purpose of misconﬁguration
corrections.
The aggregate vector will contain values Counte(i) for each
entry e and each value i in the range 0 . . . C−1. When a helper
machine is updating the aggregate, it will compute the hash of
its own value for entry e, Ve, and increment Counte(h(Ve)).
Once the aggregate is collected, the sick machine can estimate
the cardinality of each entry e of the values by counting the
number of non-zero Counte(i)’s.
1) Choose An Appropriate Hash Range: The range of the
hash function directly affects the size of the troubleshooting
request, so we want to use a small hash function to reduce
communication overhead. However, smaller hash functions
increase the chance of hash collisions and cause the cardinality
to be undercounted. For example, if the hash function has
a range of C values, the estimated cardinality will never be
higher than C.
Fortunately, most entries have a small cardinality and hence
a lower chance of collision. Entries with large cardinalities are
not likely to be identiﬁed root cause candidates by PeerPres-
sure [17]: As the cardinality increases, the sick probability will
decrease.
According to our study [17], 97% of Windows registry
entries have no more than 3 values, and in 18 out of 20
real-world troubleshooting cases, the root cause entry has a
cardinality of no more than 3. Therefore, we choose C = 16,
because under-counting the cardinalities larger than 16 does
not have much impact on the PeerPressure ranking of the root
cause candidates.
However, even for entries with small cardinalities, there
is still a chance of hash collisions. Any two values have a
16 for C = 16, and hence many
probability of collision of 1
entries with cardinalities of 2 and 3 will be undercounted.
We address this problem by using several hash functions,
h1, . . . , hk, and compute the histogram for each one. To
estimate the cardinality, the sick machine can count the number
of non-zero entries in the histogram for each hash function and
take the largest count. In this case, the cardinality will only
be undercounted if there is a collision in all k functions. For
1
an entry with 2 values, the chances of this are
16k , so by
increasing k we can make this probability arbitrarily small.
Our approach works well to measure small cardinalities
accurately, while undercounting large ones. Note that using
several smaller histograms is more efﬁcient
than a single
histogram: a histogram with kC values will have the same
communication complexity as k histograms with C values,
but the odds of a collision for a hash function with range kC
is
kC rather than 1
Ck .
Based on our previous PeerPressure evaluation [17], we