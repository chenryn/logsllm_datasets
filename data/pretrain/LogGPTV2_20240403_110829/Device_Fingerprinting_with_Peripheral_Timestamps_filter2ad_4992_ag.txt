attacks,”
July
side-channel
Accessed
landing
timing
2021,
class
new
The
for
30
of
Mozilla,
tack,”
https://blog.mozilla.org/security/2018/01/03/mitigations-landing-new-
class-timing-attack/.
Accessed
2021,
July
30
at-
2018,
[70] “Reduce timer resolution to 2ms,” Mozilla, Accessed 30 July 2021,
2018, https://bugzilla.mozilla.org/show_bug.cgi?id=1435296.
[71] J. V. Monaco and C. C. Tappert, “Obfuscating keystroke time intervals
to avoid identiﬁcation and impersonation,” 2016.
[72] G. Shah and A. Molina, “Keyboards and covert channels,” in Proc. 2006
USENIX Security Symposium (USENIX Security). USENIX, 2006.
[73] S. Wieﬂing, L. L. Iacono, and M. DÃŒrmuth, “Is this really you? an
empirical study on risk-based authentication applied in the wild,” in
Proc. 2019 IFIP International Conference on ICT Systems Security and
Privacy Protection. Springer, 2019.
[74] “Reducing the precision of the domhighrestimestamp resolution,” Ac-
cessed 22 March 2021, 2018, https://github.com/w3c/hr-time/issues/56.
[75] “Gate timestamps behind existing permission prompts,” W3C Github
Issue, Accessed 22 March 2021, 2019, https://github.com/w3c/hr-
time/issues/64.
[76] J. V. Monaco, “Feasibility of a keystroke timing attack on search engines
with autocomplete,” in Proc. 2019 IEEE Security and Privacy Workshops
(SPW).
IEEE, 2019.
[77] ——, “What are you searching for? a remote keylogging attack on search
engine autocomplete,” in Proc. 2019 USENIX Security Symposium
(USENIX Security). USENIX, 2019.
[78] N. H. F. Beebe, “Polynomial approximations,” in The Mathematical-
Springer International Publishing,
Function Computation Handbook.
2017.
[79] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand,
M. Andreetto, and H. Adam, “Mobilenets: Efﬁcient convolutional neural
networks for mobile vision applications,” 2017.
[80] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
in Proc. 2015 International Conference on Learning Representations
(ICLR), 2015.
[81] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
S. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga,
S. Moore, D. G. Murray, B. Steiner, P. Tucker, V. Vasudevan, P. Warden,
M. Wicke, Y. Yu, and X. Zheng, “Tensorﬂow: A system for large-scale
machine learning,” in Proc. 2016 USENIX Symposium on Operating
Systems Design and Implementation (OSDI). USENIX, 2016.
APPENDIX A: SUMMARY OF NOTATION
TABLE VI
SUMMARY OF NOTATION.
Description
estimated value
intended value
subject clock
reference clock
time at the peripheral sensor
time at the subject clock
time at the reference clock
subject clock tick
reference clock tick
subject clock period (i.e., resolution)
reference clock period (i.e., resolution)
subject clock frequency
reference clock frequency
time interval between events i − 1 and i
frequency offset
clock skew
instantaneous phase
instantaneous phase with period T
phase image
Symbol
ˆ
˙
CS
CR
˙ti
tS
i
tR
i
kS
i
kR
i
T S
T R
f S
f R
τ R
i
∆f
s
φi
φT
i
Φ
Our notation is summarized in Table VI. Some terminology
is borrowed from [7] (based on the NTP standard), [40],
and [41]. The subject clock and reference clock are denoted by
CS and CR, respectively. The superscript S denotes terms that
pertain to the subject clock, and R for the reference clock. The
subscript i always refers to the event index. Terms with hat
notation ˆdenote variable estimates. Terms with dot notation ˙
denote true values that aren’t observed and can’t be estimated.
This includes the event times ˙ti at the peripheral and the
assumed subject clock frequency ˙f S, which may be speciﬁed
by a known standard (e.g., 125Hz USB polling rate).
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:57:54 UTC from IEEE Xplore.  Restrictions apply. 
1032
APPENDIX B: ESTIMATING INSTANTANEOUS PHASE
Some implementation issues arise when computing Equa-
tion 10 directly using ﬂoating point representation. Millisecond
timestamps in epoch format currently require 13 decimal
places of precision, and precision lost is encountered even
with 64 bit ﬂoats. Exponential functions, e.g., exp in the
C library, commonly use polynomial approximations [78] in
i and small T S are
which rounding errors from the large tR
compounded. We found that the resulting precision loss sig-
niﬁcantly degraded device ﬁngerprints: FPNET was learning
to take a “shortcut” by detecting differences in rounding error
based on the magnitude of tR
i .
This issue can be eliminated by implementing Equation 10
with ﬁxed point arithmetic for the critical terms. The equiva-
lence noted by Equation 11 implies that φi can be computed
with truncated division, rewritten as
φi = tR
i − T S
.
(15)
(cid:23)
(cid:22) tR
i
T S
(cid:107)
(cid:106) tR
Equation 15 suffers precision loss primarily from the second
term: rounding error is compounded due to ﬂoating point
approximation of T S which gets multiplied with the compara-
tively large
. Computing instantaneous phase with clock
ticks rather than time allows the critical terms to be evaluated
with integer arithmetic. This is achieved by scaling up both
terms by the reference clock and subject clock frequencies.
Multiplying both terms by f Rf S yields
i
T S
i − f R(cid:106) tR
i
T S
(cid:107)
f Rf S
f Rf StR
φi =
(16)
where the tick count of the reference clock is expressed by
i f R which is an integer by deﬁnition, and the ﬁnal
kR
i = tR
division by f Rf S scales the instantaneous phase back to units
of time rather than ticks. When f R and f S are both integers,
it is not until the ﬁnal division that requires converting to a
ﬂoat. At this point, the only rounding error introduced is due
to ﬂoating point representation of the rational. We additionally
note that the number of unique values φi can assume when
f R and f S are integers is min(f R, f R/ gcd(f R, f S)).
APPENDIX C: EMBEDDING MODEL STRUCTURE
FPNET structure is shown in Table VII. Typical of con-
volutional networks, most of the network parameters are
concentrated near the bottom of the network, and with 8.17M
parameters, this network is relatively small by deep learning
standards [79]. The fully connected layer (fc1) provides a
linear readout of the ﬁnal convolutional layer, i.e., no activation
function is applied. All convolutional layers are followed by
ReLu activation and don’t use any padding. The pooling layers
use a “valid” padding strategy where the output from the
previous layer is padded by 1 if necessary. The depth of each
convolutional layer was balanced with batch size to ﬁt within
GPU memory (40GB on NVIDIA A100). Additional ﬁlters
may capture more complex patterns, but a larger batch size
beneﬁts the online triplet mining strategy.
TABLE VII
FPNET STRUCTURE.
output size
481 × 600 × 1
481 × 598 × 24
481 × 299 × 24
481 × 297 × 32
481 × 149 × 32
481 × 147 × 64
481 × 74 × 64
481 × 72 × 64
481 × 36 × 64
479 × 34 × 96
479 × 17 × 96
477 × 15 × 96
477 × 8 × 96
475 × 6 × 128
475 × 3 × 128
473 × 1 × 128
60544
128
128
layer
input
conv1
pool1
conv2
pool2
conv3
pool3
conv4
pool4
conv5
pool5
conv6
pool6
conv7
pool7
conv8
ﬂatten
fc1
L2
total
kernel size
stride
params
1 × 3 × 32
1 × 2 × 32
1 × 3 × 64
1 × 2 × 64
1 × 3 × 64
1 × 2 × 64
1 × 3 × 64
1 × 2 × 64
3 × 3 × 64
1 × 2 × 64
3 × 3 × 64
1 × 2 × 64
3 × 3 × 64
1 × 2 × 64
3 × 3 × 64
1 × 1
1 × 2
1 × 1
1 × 2
1 × 1
1 × 2
1 × 1
1 × 2
1 × 1
1 × 2
1 × 1
1 × 2
1 × 1
1 × 2
1 × 1
0
96
0
2k
0
6k
0
12k
0
55k
0
83k
0
111k
0
148k
0
7.75M
0
8.17M
TABLE VIII
TAUNET STRUCTURE.
layer
input
lstm
fc1
L2
total
output size
N × 1
N × 256
128
128
params
0
264k
33k
0
297k
TAUNET structure is shown in Table VIII, containing a
single recurrent layer followed by a linear dense layer and
L2 normalization. Because this is a recurrent model, it can
handle variable length sequences along the time dimension.
This model is a simpliﬁed version of TypeNet, which contains
two long short-term memory (LSTM) layers with batch nor-
malization and dropout [37]. We found the linear dense layer
following the single LSTM in TAUNET to greatly improve
performance rather than using the ﬁnal state of the LSTM
layer for embeddings.
Both models are trained for 100 epochs using Adam opti-
mization with learning rate 0.001, β1 = 0.9, and β2 = 0.999
[80], which are the default values in TensorFlow v2.4.1 [81].
Training on the 128,250 devices in the combined dataset, we
found both models to not be prone to overﬁtting: validation
accuracy plateaued after about 50 epochs and did not subse-
quently decrease.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:57:54 UTC from IEEE Xplore.  Restrictions apply. 
1033