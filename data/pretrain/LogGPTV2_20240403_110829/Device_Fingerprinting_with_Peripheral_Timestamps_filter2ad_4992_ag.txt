### References and Appendices

#### References
1. **Mitigations for New Class of Timing Attacks**
   - Mozilla, January 3, 2018.
   - Accessed: July 30, 2021.
   - URL: <https://blog.mozilla.org/security/2018/01/03/mitigations-landing-new-class-timing-attack/>

2. **Reduce Timer Resolution to 2ms**
   - Mozilla, 2018.
   - Accessed: July 30, 2021.
   - URL: <https://bugzilla.mozilla.org/show_bug.cgi?id=1435296>

3. **Obfuscating Keystroke Time Intervals to Avoid Identification and Impersonation**
   - J. V. Monaco and C. C. Tappert, 2016.

4. **Keyboards and Covert Channels**
   - G. Shah and A. Molina, in *Proc. 2006 USENIX Security Symposium (USENIX Security)*, 2006.

5. **Is This Really You? An Empirical Study on Risk-Based Authentication Applied in the Wild**
   - S. Wiefling, L. L. Iacono, and M. Dürmuth, in *Proc. 2019 IFIP International Conference on ICT Systems Security and Privacy Protection*, Springer, 2019.

6. **Reducing the Precision of the domHighResTimeStamp Resolution**
   - Accessed: March 22, 2021.
   - URL: <https://github.com/w3c/hr-time/issues/56>

7. **Gate Timestamps Behind Existing Permission Prompts**
   - W3C GitHub Issue, 2019.
   - Accessed: March 22, 2021.
   - URL: <https://github.com/w3c/hr-time/issues/64>

8. **Feasibility of a Keystroke Timing Attack on Search Engines with Autocomplete**
   - J. V. Monaco, in *Proc. 2019 IEEE Security and Privacy Workshops (SPW)*, IEEE, 2019.

9. **What Are You Searching For? A Remote Keylogging Attack on Search Engine Autocomplete**
   - J. V. Monaco, in *Proc. 2019 USENIX Security Symposium (USENIX Security)*, USENIX, 2019.

10. **Polynomial Approximations**
    - N. H. F. Beebe, in *The Mathematical-Function Computation Handbook*, Springer International Publishing, 2017.

11. **MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications**
    - A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, 2017.

12. **Adam: A Method for Stochastic Optimization**
    - D. P. Kingma and J. Ba, in *Proc. 2015 International Conference on Learning Representations (ICLR)*, 2015.

13. **TensorFlow: A System for Large-Scale Machine Learning**
    - M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. Tucker, V. Vasudevan, P. Warden, M. Wicke, Y. Yu, and X. Zheng, in *Proc. 2016 USENIX Symposium on Operating Systems Design and Implementation (OSDI)*, USENIX, 2016.

#### Appendix A: Summary of Notation
**Table VI: Summary of Notation**

| Symbol | Description |
| --- | --- |
| \(\hat{v}\) | Estimated value |
| \(\dot{v}\) | Intended value |
| \(C_S\) | Subject clock |
| \(C_R\) | Reference clock |
| \(\dot{t}_i\) | Time at the peripheral sensor |
| \(t_{S_i}\) | Time at the subject clock |
| \(t_{R_i}\) | Time at the reference clock |
| \(k_{S_i}\) | Subject clock tick |
| \(k_{R_i}\) | Reference clock tick |
| \(T_S\) | Subject clock period (resolution) |
| \(T_R\) | Reference clock period (resolution) |
| \(f_S\) | Subject clock frequency |
| \(f_R\) | Reference clock frequency |
| \(\tau_{R_i}\) | Time interval between events \(i-1\) and \(i\) |
| \(\Delta f\) | Frequency offset |
| \(s\) | Clock skew |
| \(\phi_i\) | Instantaneous phase |
| \(\phi_{T_i}\) | Instantaneous phase with period \(T\) |
| \(\Phi\) | Phase image |

- The notation is summarized in Table VI. Some terminology is borrowed from [7] (based on the NTP standard), [40], and [44].
- The subject clock and reference clock are denoted by \(C_S\) and \(C_R\), respectively.
- The superscript \(S\) denotes terms related to the subject clock, and \(R\) for the reference clock.
- The subscript \(i\) always refers to the event index.
- Terms with hat notation \(\hat{v}\) denote variable estimates.
- Terms with dot notation \(\dot{v}\) denote true values that are not observed and cannot be estimated, such as the event times \(\dot{t}_i\) at the peripheral and the assumed subject clock frequency \(\dot{f}_S\), which may be specified by a known standard (e.g., 125Hz USB polling rate).

#### Appendix B: Estimating Instantaneous Phase
- Implementation issues arise when computing Equation 10 directly using floating-point representation. Millisecond timestamps in epoch format currently require 13 decimal places of precision, and precision loss is encountered even with 64-bit floats.
- Exponential functions, such as `exp` in the C library, commonly use polynomial approximations [78]. Rounding errors from the large \(t_{R_i}\) and small \(T_S\) are compounded.
- We found that the resulting precision loss significantly degraded device fingerprints: FPNET was learning to take a "shortcut" by detecting differences in rounding error based on the magnitude of \(t_{R_i}\).
- This issue can be eliminated by implementing Equation 10 with fixed-point arithmetic for the critical terms. The equivalence noted by Equation 11 implies that \(\phi_i\) can be computed with truncated division:
  \[
  \phi_i = t_{R_i} - T_S \left\lfloor \frac{t_{R_i}}{T_S} \right\rfloor
  \]
- Equation 15 suffers precision loss primarily from the second term: rounding error is compounded due to floating-point approximation of \(T_S\), which gets multiplied with the comparatively large \(\frac{t_{R_i}}{T_S}\).
- Computing instantaneous phase with clock ticks rather than time allows the critical terms to be evaluated with integer arithmetic. This is achieved by scaling up both terms by the reference clock and subject clock frequencies.
- Multiplying both terms by \(f_R f_S\) yields:
  \[
  \phi_i = \frac{k_{R_i} - f_R \left\lfloor \frac{t_{R_i}}{T_S} \right\rfloor}{f_R f_S}
  \]
  where the tick count of the reference clock is expressed by \(k_{R_i} = t_{R_i} f_R\), which is an integer by definition, and the final division by \(f_R f_S\) scales the instantaneous phase back to units of time rather than ticks.
- When \(f_R\) and \(f_S\) are both integers, it is not until the final division that requires converting to a float. At this point, the only rounding error introduced is due to floating-point representation of the rational.
- Additionally, the number of unique values \(\phi_i\) can assume when \(f_R\) and \(f_S\) are integers is \(\min(f_R, \frac{f_R}{\gcd(f_R, f_S)})\).

#### Appendix C: Embedding Model Structure
- **FPNET Structure** is shown in Table VII. Typical of convolutional networks, most of the network parameters are concentrated near the bottom of the network, and with 8.17M parameters, this network is relatively small by deep learning standards [79].
- The fully connected layer (fc1) provides a linear readout of the final convolutional layer, i.e., no activation function is applied.
- All convolutional layers are followed by ReLU activation and do not use any padding.
- The pooling layers use a "valid" padding strategy where the output from the previous layer is padded by 1 if necessary.
- The depth of each convolutional layer was balanced with batch size to fit within GPU memory (40GB on NVIDIA A100). Additional filters may capture more complex patterns, but a larger batch size benefits the online triplet mining strategy.

**Table VII: FPNET Structure**

| Layer | Output Size | Kernel Size | Stride | Parameters |
| --- | --- | --- | --- | --- |
| Input | 481 × 600 × 1 | - | - | 0 |
| conv1 | 481 × 598 × 24 | 1 × 3 × 32 | 1 × 1 | 96 |
| pool1 | 481 × 299 × 24 | 1 × 2 × 32 | 1 × 2 | 0 |
| conv2 | 481 × 297 × 32 | 1 × 3 × 64 | 1 × 1 | 2k |
| pool2 | 481 × 149 × 32 | 1 × 2 × 64 | 1 × 2 | 0 |
| conv3 | 481 × 147 × 64 | 1 × 3 × 64 | 1 × 1 | 6k |
| pool3 | 481 × 74 × 64 | 1 × 2 × 64 | 1 × 2 | 0 |
| conv4 | 481 × 72 × 64 | 1 × 3 × 64 | 1 × 1 | 12k |
| pool4 | 481 × 36 × 64 | 1 × 2 × 64 | 1 × 2 | 0 |
| conv5 | 479 × 34 × 96 | 3 × 3 × 64 | 1 × 1 | 55k |
| pool5 | 479 × 17 × 96 | 1 × 2 × 64 | 1 × 2 | 0 |
| conv6 | 477 × 15 × 96 | 3 × 3 × 64 | 1 × 1 | 83k |
| pool6 | 477 × 8 × 96 | 1 × 2 × 64 | 1 × 2 | 0 |
| conv7 | 475 × 6 × 128 | 3 × 3 × 64 | 1 × 1 | 111k |
| pool7 | 475 × 3 × 128 | 1 × 2 × 64 | 1 × 2 | 0 |
| conv8 | 473 × 1 × 128 | 3 × 3 × 64 | 1 × 1 | 148k |
| flatten | 60544 | - | - | 0 |
| fc1 | 128 | - | - | 7.75M |
| L2 | 128 | - | - | 0 |
| Total | - | - | - | 8.17M |

- **TAUNET Structure** is shown in Table VIII, containing a single recurrent layer followed by a linear dense layer and L2 normalization.
- Because this is a recurrent model, it can handle variable-length sequences along the time dimension.
- This model is a simplified version of TypeNet, which contains two long short-term memory (LSTM) layers with batch normalization and dropout [37].
- We found the linear dense layer following the single LSTM in TAUNET to greatly improve performance rather than using the final state of the LSTM layer for embeddings.

**Table VIII: TAUNET Structure**

| Layer | Output Size | Parameters |
| --- | --- | --- |
| Input | N × 1 | 0 |
| LSTM | N × 256 | 264k |
| fc1 | 128 | 33k |
| L2 | 128 | 0 |
| Total | - | 297k |

- Both models are trained for 100 epochs using Adam optimization with a learning rate of 0.001, \(\beta_1 = 0.9\), and \(\beta_2 = 0.999\) [80], which are the default values in TensorFlow v2.4.1 [81].
- Training on the 128,250 devices in the combined dataset, we found both models to not be prone to overfitting: validation accuracy plateaued after about 50 epochs and did not subsequently decrease.