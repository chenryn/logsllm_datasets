FE
FE1(1)
Recommended Components to Migrate
BL
BL1(2), BL2, BL4, BL5
BE
—
BE2, BE3, BE4, BE5
FE1(1), FE2 BL1(1), BL2, BL3,
BL4, BL5
BL2(1), BL3, BL5
FE2
FE1(1), FE2 BL1(2), BL2, BL3, BL5 BE1, BE2, BE3, BE5
BE2, BE3, BE5
migrate all components in full
Figure 10: The ERP application in a large university.
proach, and to show that our model accounts for key factors that
may impact migration decisions.
Modeling a deployed ERP application: Fig. 10 presents a model
of the application, obtained after discussions with the campus net-
work operators. The top most node represents users (both inter-
nal and external). About 78% of the transactions are generated
by users internal to the enterprise, and the rest by external users.
External users include those in satellite campuses of the main uni-
versity campus. Every other node corresponds to an application
component, and is annotated with the number of servers in that
component. There are two front-end components (FE1, FE2), and
ﬁve business-logic components (BL1–BL5). The components in
the BL layer differ in the transactions they handle - for instance,
one component handles ﬁnancial transactions, while another han-
dles supply relations related transactions. The application has ﬁve
databases, each of which could be modeled as a back-end compo-
nent (BE1–BE5, collectively referred to as BE). Each database has
a size as shown in the ﬁgure. Each edge indicates that the corre-
sponding pair of nodes can communicate, and is annotated with the
percentage of transactions in the system that use that edge.
The ﬁgure highlights interesting departures from conventional
text-book application design. First, while about half the transac-
tions from users are directed to the front-end components (these
users are called thin clients), the remaining transactions directly
reach the business logic components (generated by thick clients).
Second, we observe communication between components in the
BL layer. For instance, once a purchase order is ﬁnalized, a BL
component corresponding to supply relations invokes a BL compo-
nent corresponding to ﬁnancial transactions to ensure the funds get
committed. In fact, all pairs of BL components interact in practice,
though many interacting links carry a negligible fraction of trans-
actions and are not shown in Fig. 10. Finally, while in general the
trafﬁc between components mirrored the number of transactions
between them, the data warehouse component (BL3) accounted for
70% of trafﬁc between the BL layer and the back-end.
Inferring model parameters: We derived parameters that our model
needs by conducting end-to-end measurements of typical user re-
quests (such as downloading salary statements), and estimating in-
dividual component and link communication times. Our measure-
ments indicated typical user requests involved uploads of about
two KBytes, downloads of about 13.4 KBytes, end-to-end response
times of 1400 msec, and ﬁle download times of 202 msec. From
these measurements, we inferred communication delays on other
links assuming they were proportionally scaled from the request
upload and ﬁle download times. The difference between the end-
to-end response times and the link communication delays was as-
sumed to be node service times, and we assumed 90% of the ser-
vice time was spent in servers in the BL layer, as these were most
compute-intensive. We also measured the upload and download
times of similar-sized ﬁles to the Windows Azure cloud, to esti-
mate communication delays with the cloud. The same values were
used to estimate communication delays between local and migrated
Table 2: Recommendations of the planned approach for ERP application.
components, and delays related to external users. We assumed 10
transactions per second on average. Finally, we used the same val-
ues for migration beneﬁts and communication costs as in §5.1.
Recommendations from model: Table 2 presents results produced
by our model with the ﬂexible routing approach. Each row corre-
sponds to a particular bound on mean delay. For each delay bound,
we show the yearly savings, and the components in each tier that
should be migrated. If a component should only be migrated par-
tially, we also show in brackets the number of servers to migrate.
For example, for a delay bound of 110%, only one of the three
servers of BL2 are migrated, while BL3 and BL5 are migrated fully.
The ﬁrst row is a special case where the operator speciﬁes a policy
that no database (i.e., BE1–BE5) can be migrated. For all other
rows, all components can be migrated.
Table 2 illustrates at least three scenarios where the hybrid ap-
proach could be useful. First, when there are policy restrictions
on migration (ﬁrst row), the hybrid approach can still identify less
sensitive components to migrate, leading to savings while meeting
the delay bound. Second, migrating the entire application is fea-
sible only when the delay bound is set at 130% or higher (row 5).
This is because all transactions from internal users must be sent
out to the cloud and then back to the enterprise. In contrast, a hy-
brid approach can achieve signiﬁcant savings even at lower delay
bounds, by migrating a subset of components. Third, full migration
of components such as FE1 and BL1 can have a substantial impact
on delay, as these components are used by a large fraction of trans-
actions. A hybrid approach allows for partial migration of these
components (rows 2–4). Here, the subset of servers moved to the
cloud are mainly used by the external users, and hence the increase
in mean delay is small.
Table 2 also shows that the interaction between components plays
a critical role in planning decisions. For example, for all rows,
when FE2 is migrated, BL3 and BE3 are also migrated. Doing so
enables the transactions sent to FE2 to be processed entirely in the
cloud, and prevents them from being sent back and forth between
the local data center and the cloud.
Sensitivity to model parameters: We have studied how the opti-
mal migration strategy is impacted by varying model parameters,
in particular, the estimated beneﬁts of migrating individual CPU
and storage servers. We summarize some of the key insights that
we have gained. First, for some delay bounds, the optimal migra-
tion strategy dominates all other feasible solutions in that it moves
more CPU and storage servers than any other approach. In such
cases, the optimal migration strategy does not depend on the ben-
eﬁt estimates. For instance, for any delay bound above 130%,
the best approach is to migrate the entire application regardless of
beneﬁt estimates. Second, the beneﬁt estimates impact the strat-
egy if there are multiple feasible approaches to realize the delay
bound, none of which dominate the others. For example, a delay
bound of 120% could be met by either migrating component BE1 (a
large database), or components BL4 and BE4 (2 CPU servers and
a much smaller database). While Table 2 recommended that BE1
be moved, our results show that components BL4 and BE4 should
be moved instead if the CPU beneﬁts were much higher. Third, the
usersFE1(3)BL1(7)BL3(2)BL2(3)FE2(2)BL4(2)BL5(2)BE3(1)78% Internal 22% external30%30%30%10%20%20%5%5%59%1%1%9%22%5%5%BE1(1)BE2(1)BE5(1)BE4(1)500GB300GB700GB50GB50GBBE251Firewall Contexts
Admin. Network for production/non-production database
Admin. Network for production/non-production ERP
Admin. Network for production/non-production web services
Admin. Network for misc. servers, e.g., storage/backup servers
Admin. Network for production/non-production academic systems
No.
4
2
2
2
2
Admin. Network for production/non-production Windows/UNIX systems 6
2
6
3
2
2
2
5
40
Production/Non-production academic systems
Production/Non-production Active Directory
Production/Non-production Windows/UNIX systems
Internal/External DMZ
Misc. servers/applications
Oracle/SQL Database Servers
Production/Non-production ERP
Total
Table 3: Types of ﬁrewall contexts in the campus data center.
relative size of transactions between different components may de-
termine the optimal strategy independent of beneﬁt estimates. Note
that transaction sizes determine (i) the delay between the compo-
nents and consequently total application response time; and (ii) the
wide-area communication costs. For instance, if the size of trans-
actions between the BL and BE tier is larger than the size of trans-
actions between other tiers, migrating a BE component alone is not
as beneﬁcial as migrating both the BE component and the most fre-
quently interacting BL component, regardless of beneﬁt estimates.
Finally, we have explored the sensitivity of our results to the costs
of Internet communication. For most delay bound settings, the rec-
ommended strategy does not change even if costs go up by a factor
of 10, but with higher factors, fewer components should be mi-
grated. We omit further details for lack of space, and defer studies
on a wider range of applications to future work.
5.3 Migrating security policies
In this section, we demonstrate the beneﬁt of systematic migration
of ACLs by applying our algorithms to a concrete migration sce-
nario and evaluating how well our approach performs and scales
on a large-scale campus network.
5.3.1 Security policies in operational data centers
We present a high-level characterization of ACL usage in two oper-
ational data centers, DC1 and DC2, owned by a large-scale campus
network. The data centers offer a variety of services such as student
self-services, ﬁnance tools, employee training, etc., to campus and
external users.
Overall, hundreds of servers are logically partitioned in 40 server
VLANs (35 in DC1 and 5 in DC2). Servers in the same VLAN
have similar functionalities and share similar patterns when com-
municating with nodes outside of the VLAN. Every VLAN has its
ingress and egress trafﬁc protected by a ﬁrewall context, which con-
sists of a pair of ACLs (one per direction) placed at the interface
of the VLAN’s access router. By default, all ingress/egress trafﬁc
to/from a VLAN is denied/allowed. Any incoming trafﬁc destined
to servers in a VLAN needs to be explicitly allowed by adding ap-
propriate permit ingress rules.
Table 3 summarizes the types of ﬁrewall contexts and the num-
ber of contexts of each type. The contexts contain diverse reach-
ability policies for application servers, database and ﬁle servers,
portal servers and application servers for external access, and non-
production testing networks. Nearly half of the contexts are ad-
ministration contexts, which are accessible for authorized admin-
istrators to manage servers. Fig. 11 shows a CDF of the number
of ingress/egress ACL rules across all contexts. The size of each
ACL ranges from 0 to 208 rules. Migration of reachability policies
of such complexity and scale is daunting, further strengthening the
case for an automated approach like ours.
ACL Rules
a1 1. deny: all but external hosts with private addresses
a2
a3
1. permit: monitoring component (BL5)→F E1 and F E2
2. permit: any→HTTP & HTTPS ports on F E1 and F E2
1. permit: monitoring component (BL5)→BL1, BL2, and BL3
2. permit: BL4→TCP port p1 on BL1, BL2, and BL3
3. permit: F E1→TCP port p2 on BL1
4. permit: F E2→TCP port p2 on BL3
5. permit: external/campus users→TCP port p3 on BL1 and BL2
1. permit: monitoring component (BL5)→BL4
2. permit: BL1, BL2, and BL3→TCP port p1 on BL4
3. permit: external/campus users→TCP port p3 on BL4
1. permit: monitoring component (BL5)→BE
2. permit: all BL components to MySQL port on BE
a4
a5 1. permit: external/campus administrators→SSH port on BL5
a7
Default
permit
deny
deny
deny
deny
deny
Table 4: ACL conﬁgurations before migration.
5.3.2 Case study: migrating ERP application
We evaluate the effectiveness of our ACL migration algorithm in
the context of the ERP application introduced in §5.3.2. We ob-
tained the ACL rules relevant to the application from the campus
operators. We computed the new ACL conﬁgurations for the mi-
gration scenario corresponding to the ﬁrst row of Table 2. In this
scenario, all servers in the components BL2, BL4, and BL5, one
server in FE1 and two servers in BL1 are migrated. All servers for
the ERP application are located in DC1 and are physically arranged
as shown in Fig. 12. FE1,1–FE1,3 denote the individual servers of
component FE1, and a similar notation is used for other servers.
All servers in the FE and the BE tier are placed in VLAN V1 and
VLAN V7 respectively. Servers in the BL tier are placed in VLANs
V2, V4 and V5.
Fig. 12 shows that six different ACLs are placed on nine router
interfaces to ﬁlter trafﬁc as per operator goals. Table 4 shows
the rules associated with each ACL. A packet not matching any
of the rules takes the default action. The policies closely match
the communication pattern in Fig. 10. For example, a3 permits
access from the monitoring component BL5 to components BL1,
BL2 and BL3 (rule 1), and from BL4 to one port on the same des-
tinations (rule 2). a3 also permits trafﬁc from FE1 to a port on
BL1 (rule 3), from FE2 to a port on BL3 (rule 4), and from exter-
nal/campus users to a port on BL1 and BL2 (rule 5).
Fig. 13 shows the new ACL placement after migration, assum-
ing techniques for applying ACLs to groups of VMs are available
in the cloud. After migration, there are a total of 13 ACLs since
placement is done separately for each data center. Each ACL be-
fore migration may be placed in multiple locations, contributing to
rules in one or more ACLs after migration. For example, ACL a3
contributes to rules in ACLs r1, r3, r5, r7, r10, and r11. Each new
ACL is generated by merging co-located ACLs, e.g., a2 and a3 are
merged to produce r5. Table 5 shows the conﬁguration of a subset
of these new ACLs that inherit rules from a3.