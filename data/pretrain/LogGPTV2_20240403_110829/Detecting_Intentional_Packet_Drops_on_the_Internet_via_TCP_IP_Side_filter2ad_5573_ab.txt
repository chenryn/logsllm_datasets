state to be cleared. Each experiment lasted for less than ﬁve minutes, so that
ten could be completed in an hour. Every client and server was involved in
only one experiment at a time. Each client/server pair was tested once per hour
throughout the 24-hour period, for replication and also to minimize the eﬀects
114
R. Ensaﬁ et al.
of diurnal patterns. Source and destination ports for all packets were carefully
chosen and matched to minimize assumptions about what destination ports the
client responds on.
4 Analysis
In this section we give an overview of our intervention analysis based on ARMA
modeling. More details are available in the extended version of the paper [5].
We model each time series y1, . . . , yn as a linear regression with ARMA errors,
a combination of an autoregressive-moving-average (ARMA) model with exter-
nal linear regressors. An ARMA(p, q) model combines an AR model of order
p and an MA model of order q. We use a linear regression with ARMA errors
to model our time series data. This speciﬁes that every element in a time series
can be written as a constant plus the linear combination of regressors x1, . . . , xr
with an ARMA-modeled error term, et:
yt = c +
r(cid:2)
i=1
βixit + et,
et = zt +
p(cid:2)
i=1
φiet−i +
q(cid:2)
i=1
θizt−i
where zt is a white noise series and φi, θi, and βi are ARMA model parameters
to be ﬁtted. We use the regressors xi for intervention analysis, i.e., for analyzing
our experimental eﬀect on the time series at a speciﬁc time.
For each experiment, we pick regressors according to which times the server
(re)transmits SYN/ACK’s in response to SYN’s. For a server that (re)transmits
r SYN/ACK’s in response to each SYN, we have r regressors. We call time t1 the
time of the ﬁrst transmission in response to the ﬁrst of our forged SYN’s, and
we call ti+1 the time the server would send the ith retransmission in response
to that SYN. Then we deﬁne regressor xi as the indicator variable
(cid:3)
1
0
xij =
if ti ≤ j and either j < ti+1 or i = r
otherwise
In other words, x1 is zeros until the time the server transmits the ﬁrst SYN/ACK
then ones until the server begins retransmitting SYN/ACK’s. The remaining xi
are zeros until the time the server would begin retransmitting its ith SYN/ACK
then ones until if/when the (i + 1)th SYN/ACK’s would begin being retransmit-
ted. This deﬁnition allows us to model any of the possible level shifts in any case
of packet drop as a linear combination of all xi. See Figure 2 for an illustration.
For intervention analysis, we use hypothesis testing over a value βr, which
represents the diﬀerence in IPID diﬀerences between when we do or do not send
forged SYN packets to the server. Then we determine the case by a series of
one-sided hypothesis tests performed with signiﬁcance α = 0.01 according to the
following breakdown, where k1 and k(cid:3)
2 are thresholds between cases:
Detecting Intentional Packet Drops on the Internet
115
Fig. 2. For a server that retransmits r − 1 SYN/ACK’s, each case can be expressed as
the linear combination of regressors x1, . . . , xr; shown is when r = 3 with SYN/ACK
transmissions responding to the ﬁrst forged SYN occurring at t1, t2, and t3
– Server-to-client-dropped if we reject the null hypothesis that βr ≥ k1.
– No-packets-dropped if we reject the null hypotheses that βr ≤ k1 and
that βr ≥ k(cid:3)
2.
– Client-to-server-dropped if we reject the null hypothesis that βr ≤ k(cid:3)
2.
– Error if none of the above cases can be determined.
For details about the linear regression step, removal of outliers, and how we
choose the thresholds, see the extended version of the paper [5].
5 Results
Table 1 shows results from 5 days of data collection, where S → C is Server-to-
client-dropped, None is No-packets-dropped, C → S is Client-to-server-
dropped, and Error is Error. CN is China, Asia-CN is other Asian countries,
EU is Europe, and NA is North America. For server types, Tor-dir is a Tor
directory authority, Tor-bri is a Tor bridge, and Web is a web server.
Our expectation would be to observe Server-to-client-dropped for clients
in China and Tor servers because of Winter and Lindskog’s observation that the
SYN/ACKs are statelessly dropped by the “Great Firewall of China” (GFW)
based on source IP address and port [8]. We would expect to see No-packets-
dropped for most web servers from clients in China, unless they host popular
websites that happen to be censored in China. Similarly, in the expected case we
should observe No-packets-dropped for clients outside of China, regardless of
server type. We expect a few exceptions, because censorship happens outside of
China and because the GFW is not always 100% eﬀective. In particular, Tor
bridges are not blocked until the GFW operators learn about them, and some
routes might not have ﬁltering in place. Our results are congruent with all of
these expectations.
In 5.9% of the client/server pairs we tested, multiple cases were observed in
the same day. In some cases it appears that noise caused the wrong case to be
detected, but other cases may be attributable to routes changing throughout the
day [9]. That the data is largely congruent with our expectations demonstrates
116
R. Ensaﬁ et al.
Table 1. Results from the measurement study
Client,Server
CN,Tor-dir
Asia-CN,Tor-dir
NA,Tor-dir
EU,Tor-dir
CN,Tor-bri
Asia-CN,Tor-bri
NA,Tor-bri
EU,Tor-bri
CN,Web
Asia-CN,Web
NA,Web
EU,Web
All Web
All Tor-bri
All Tor-dir
19 (0.63)
0 (0.00)
1 (0.07)
2 (0.28)
S → C (%) None (%) C → S (%) Error (%)
504 (16.73) 289 (9.59)
2200 (73.04)
43 (3.54)
1171 (96.38)
1 (0.08)
75 (5.59)
1217 (90.69) 49 (3.65)
11 (1.55)
2 (0.28)
695 (97.89)
110 (6.40)
31 (1.80)
1012 (58.91) 565 (32.89)
139 (17.96)
626 (80.88)
9 (1.16)
30 (3.57) 153 (18.21)
657 (78.21)
9 (2.25)
313 (78.25)
78 (19.50)
36 (2.76) 245 (18.79)
995 (76.30)
1 (0.17)
569 (97.43)
606 (93.37)
0 (0.00)
305 (90.24)
0 (0.00)
2475 (86.09) 37 (1.29) 334 (11.62)
1012 (27.12) 2161 (57.90) 79 (2.12) 480 (12.86)
2203 (35.09) 3102 (49.40) 556 (8.85) 418 (6.66)
0 (0.00)
0 (0.00)
0 (0.00)
28 (2.15)
1 (0.17)
0 (0.00)
0 (0.00)
29 (1.01)
13 (2.23)
43 (6.63)
33 (9.76)
the eﬃcacy of the approach, and some of the data points that lie outside our
expectations have patterns that suggest that a real eﬀect is being measured,
rather than an error. For example, of the 28 data points where web servers were
blocked from the server to the client in China, 20 of those data points are the
same client/server pair.
38% of the data we collected does not appear in Table 1 because it did not
pass liveness tests. Every 5-minute data point has three associated liveness tests.
If a server sends fewer than 2.5 SYN/ACKs in response to SYNs from the mea-
surement machine, a client responds to less than 3
5 of our SYN/ACKs, or a
measurement machine sending thread becomes unresponsive, that 5-minute data
point is discarded.
Two out of the ten Tor directory authorities never retransmitted enough
SYN/ACKs to be included in our data. Of the remaining eight, two more ac-
count for 98.8% of the data points showing blocking from client to server. These
same two directory authorities also account for 72.7% of the Error cases for
directory authorities tested from clients in China, and the case of packets being
dropped from server to client (the expected case for China and the case of the
majority of our results) was never observed for these two directory authorities.
When Winter and Lindskog [8] measured Tor reachability from a virtual pri-
vate server in China, there were eight directory authorities at that time. One of
the eight was completely accessible, and the other seven were completely blocked
in the IP layer by destination IP (i.e., Client-to-server). In our results, six
out of ten are at least blocked Server-to-client and two out of ten are only
blocked Client-to-server (two had all results discarded). Winter and Lindskog
also observed that Tor relays were accessible 1.6% of the time, and we observed
that directory authorities were accessible 0.63% of the time. Our results have
Detecting Intentional Packet Drops on the Internet
117
geographic diversity and their results can serve as a ground truth because they
tested from within China. In both studies the same special treatment of direc-
tory authorities compared to relays or bridges was observed, as well as a small
percentage of cases where ﬁltering that should have occurred did not.
To evaluate the assumption that clients with a global IPID are easy to ﬁnd
in a range of IP addresses that we desire to measure from, take China as an
example. On average, 10% of the IP addresses in China responded to our probes
so that we could observe their IPID, and of those 13% were global. So, roughly
1% of the IP address space of China can be used as clients for measurements
with our method, enabling experiments with excellent geographic and topological
diversity.
6 Related Work
Related work directly related to idle scans [2,3,4] was discussed in Section 1.
Other advanced methods for inferring remote information about networks have
been proposed. Qian et al. [10] demonstrate that ﬁrewall behavior with respect
to sequence numbers can be used to infer sequence numbers and perform oﬀ-path
TCP/IP connection hijacking. Chen et al. [11] use the IPID ﬁeld to perform ad-
vanced inferences about the amount of internal traﬃc generated by a server, the
number of servers in a load-balanced setting, and one-way delays. Morbitzer [12]
explores idle scans in IPv6.
iPlane [13] sends packets from PlanetLab nodes to carefully chosen hosts,
and then compounds loss on speciﬁc routes to estimate the packet loss between
arbitrary endpoints without access to those endpoints. This does not detect IP-
address-speciﬁc packet drops. Our technique, in contrast, can be used to detect
intentional drops of packets based on IP address and requires no commonalities
between the measurement machine’s routes to the server or client and the routes
between the server and client. Queen [14] utilizes recursive DNS queries to mea-
sure the packet loss between a pair of DNS servers, and extrapolates from this
to estimate the packet loss rate between arbitrary hosts.
7 Conclusion
We have presented a method for detecting intentional packet drops (e.g., due
to censorship) between two almost arbitrary hosts on the Internet, assuming
the client has a globally incrementing IPID and the server has an open port.
Our method can determine which direction packets are being dropped in, and is
resistant to noise due to our use of an ARMA model for intervention analysis. Our
measurement results are congruent with current understandings about global
Internet censorship, demonstrating the eﬃcacy of the method.
Acknowledgments. We would like to thank the anonymous PAM 2014 review-
ers and our shepherd, Jelena Mirkovic, as well as Terran Lane, Patrick Bridges,
118
R. Ensaﬁ et al.
Michalis Faloutsos, Stefan Savage, and Vern Paxson for helpful feedback on this
work. This material is based upon work supported by the National Science Foun-
dation under Grant Nos. #0844880, #1017602, #0905177, and #1314297.
References
1. arma: Research problem: Five ways to test bridge reachability. Tor Blog (December
1, 2011), https://blog.torproject.org/blog/research-problem-five-ways-
test-bridge-reachability
2. Antirez: new tcp scan method. Posted to the bugtraq mailing list (December 18,
1998)
3. Lyon, G.: Nmap Network Scanning: The Oﬃcial Nmap Project Guide to Network
Discovery and Security Scanning. Insecure.Org LLC, Sunnyvale, CA, USA (2009)
4. Ensaﬁ, R., Park, J.C., Kapur, D., Crandall, J.R.: Idle port scanning and non-
interference analysis of network protocol stacks using model checking. In: Proceed-
ings of the 19th USENIX Security Symposium, USENIX Security 2010. USENIX
Association (2010)
5. Ensaﬁ, R., Knockel, J., Alexander, G., Crandall, J.R.: Detecting intentional
packet drops on the Internet via TCP/IP side channels: Extended version CoRR
abs/1312.5739 (2013), http://arxiv.org/abs/1312.5739
6. Alexa: Alexa top 1,000,000 sites, http://www.alexa.com/topsites
7. MaxMind: How accurate are your GeoIP databases?
http://www.maxmind.com/en/faq#accurate
8. Winter, P., Lindskog, S.: How the Great Firewall of China is Blocking Tor. In: Free
and Open Communications on the Internet. USENIX Association (2012)
9. Paxson, V.: End-to-end internet packet dynamics. SIGCOMM Comput. Commun.
Rev. 27(4), 139–152 (1997)
10. Qian, Z., Mao, Z.M.: Oﬀ-path TCP sequence number inference attack - how ﬁrewall
middleboxes reduce security. In: Proceedings of the 2012 IEEE Symposium on
Security and Privacy, SP 2012, pp. 347–361. IEEE Computer Society, Washington,
DC (2012)
11. Chen, W., Huang, Y., Ribeiro, B.F., Suh, K., Zhang, H., de Souza e Silva, E.,
Kurose, J., Towsley, D.: Exploiting the IPID ﬁeld to infer network path and end-
system characteristics. In: Dovrolis, C. (ed.) PAM 2005. LNCS, vol. 3431, pp.
108–120. Springer, Heidelberg (2005)
12. Morbitzer, M.: TCP Idle Scans in IPv6. Master’s thesis, Radboud University Ni-
jmegen, The Netherlands (2013)
13. Madhyastha, H.V., Isdal, T., Piatek, M., Dixon, C., Anderson, T., Krishnamurthy,
A., Venkataramani, A.: iPlane: an information plane for distributed services. In:
Proceedings of the 7th Symposium on Operating Systems Design and Implemen-
tation, OSDI 2006, pp. 367–380. USENIX Association, Berkeley (2006)
14. Wang, Y.A., Huang, C., Li, J., Ross, K.W.: Queen: Estimating packet loss rate
between arbitrary internet hosts. In: Moon, S.B., Teixeira, R., Uhlig, S. (eds.)
PAM 2009. LNCS, vol. 5448, pp. 57–66. Springer, Heidelberg (2009)