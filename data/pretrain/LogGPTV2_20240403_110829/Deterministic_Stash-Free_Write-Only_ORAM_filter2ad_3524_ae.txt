would with full interleaving, we see 4 sequential write patterns in
sub-regions of memory.
Comparisons. We carefully re-implemented the HiVE-WoORAM
(only the WoORAM part, not the hidden volume part), using the
same BUSE/mbedTLS library setup. As in their original paper and
implementation, our HiVE-WoORAM implementation uses k =
Sequential write
MB/sec
overhead MB/sec
Sequential read
overhead
2.6
—
—
—
—
192x
615.8
126.1
505.4
111.6
dm-crypt baseline
SSD
HDD
HiVE-WoORAM
SSD
DetWoORAM
SSD, M = 3N , b = 64
SSD, M = N , b = 64
HDD, M = 3N , b = 64
HDD, M = N , b = 64
Logical disk size 40GB and block size 4KB in all cases.
Overhead is relative to the dm-crypt baseline for that drive type.
Highlighted values indicate the best WoORAM per column.
10.2x
14.8x
3.84x
4.46x
2.37x
2.52x
4.83x
5.24x
260.0
244.1
26.1
24.1
49.7
34.0
29.0
25.0
15.2x
40.5
Table 2: bonnie++ benchmarking of sequential accesses
Solid state SSD
Spinning platters HDD
MB/sec
overhead MB/sec
overhead
—
325x
1.6x
dm-crypt baseline
HiVE-WoORAM
DetWoORAM
154
8.49
34.4
—
18x
4.5x
16.4
0.051
10.2
Logical disk size 40GB and block size 4KB in all cases.
Overhead is relative to the dm-crypt baseline for that drive type.
DetWoORAM used M/N = 3 and b = 64 for all cases.
Highlighted values indicate the best WoORAM per column.
Table 3: fio benchmarking of random reads and writes
3 random physical writes per logical write, and makes use of a
recursive position map. The original implementation was as a kernel
module for a device mapper, but unfortunately due to Linux kernel
changes this module is incompatible with recent Linux kernels. In
fact, this incompatibility was part of our motivation to use only
standards-compliant userspace C++ code for our DetWoORAM
implementation.
For a baseline comparison, we wanted to use the best existing
solution with the same general setup as ours. Our baseline uses the
linux kernel module dm-crypt, which provides an encrypted block
device with no obliviousness, connected to simple “passthrough”
device that comes with the BUSE distribution. There is no oblivi-
ousness in this option; it simply encrypts/decrypts and stores the
resulting ciphertext in the same location on disk. This provides a fair
baseline to our work, and should help to eliminate any bottlenecks
or artifacts of the BUSE layer in order to have a clear comparison
with our new DetWoORAM protocol.
Measurement using bonnie++. Table 2 shows the results of run-
ning the popular bonnie++ disk benchmarking tool our the plain
encryption as well as different WoORAM settings. All tests were
performed with a 40GB logical filesystem within a 200GB partition,
using the btrfs filesystem.
We tested using 200GB partitions on a 1TB HDD (HGST Travel-
star 7200RPM) and on a 256GB SSD (Samsung 850 Pro). We note
that both drives are standard commodity disks available for around
$100 USD. As expected, the SSD drive is considerably faster for
both reading and writing.
Recall that one novel feature of DetWoORAM is that it can flexi-
bly adapt to different storage ratios between logical and physical
storage. We tested both with M = N , similar to the HiVE-WoORAM,
and with more physical space of M = 3N , and observed a slight
(but statistically meaningful) performance improvement from hav-
ing more physical disk space (and therefore larger holding area in
the DetWoORAM). We also tested with different branching factors
ranging from b = 2 to b = 512, but did not notice any significant
timing differences overall, indicating that the position map plays a
smaller role in the overall performance.
Overall we can see that the DetWoORAM suffers only a 3x-10x
slowdown compared to the baseline, whereas the HiVE-WoORAM
is almost 200x slower in the case of writing and 15x slower for
reading compared to the same baseline. The results for HiVE-Wo-
ORAM are consistent with the results reported in their original
paper [4].
In fact, our DetWoORAM running on an SSD is in most cases
faster than the baseline running on a spinning disk HDD, providing
good evidence that our system is fast enough for practical use. We
believe this is largely explained by the sequential write pattern of
DetWoORAM, which also makes read operations partially sequen-
tial. For large sequential workloads, the data locality appears to
have a very significant effect on performance.
Measurement using fio. As has been noted in previous WoORAM
works [4], performing sequential logical operations can put Wo-
ORAMs in an especially bad light, as the baseline non-oblivious stor-
age will translate the sequential read/write operations to physically
sequential addresses, thereby gaining significantly over WoORAMs
that need to obscure the logical address of each operation.
Interestingly, our DetWoORAM is a somewhat “in-between” case
here, as the write pattern is completely sequential, and the read
pattern is partially sequential: the main area of storage corresponds
exactly to physical addresses, but the holding area and position map
do not. We used a second disk performance measurement tool fio
(https://github.com/axboe/fio) in order to perform random reads
and writes, as opposed to the sequential read/write pattern of the
bonnie++ benchmarks. The results are shown in Table 3, which
shows the throughput for random reads and writes of 4KB-4MB
sized blocks in direct access to the device without any filesystem
mounted.
As expected, the performance degradation for HDD compared
to SSD in all cases was significant for the random reads and writes.
As with the bonnie++ benchmarks, but more dramatically here,
our DetWoORAM running on an SSD outperformed the baseline
running on the HDD. Even more surprisingly, on the HDD our
DetWoORAM was only 1.6x slower than the baseline. This can be
explained in part by the fact that our scheme actually turns random
writes into sequential writes, so although it performs more writes
than the baseline, they will be more compact in this experiment.
6 INSECURITY OF DATALAIR
A recent paper [5] has also proposed to improve the performance of
HiVE-WoORAM. While this paper contains some new and promis-
ing ideas, and in particular proposed the use of a B-tree ODS similar
to our Trie ODS for the position map, unfortunately it violates the
notion of write-only obliviousness.
Intuitively, the DataLair scheme identifies that a bottleneck in Hi-
VE-WoORAM is in identifying free blocks from the random blocks
chosen, and propose to modify the random block choosing scheme
in order to find free blocks more efficiently with fewer dummy
writes. Unfortunately, this improvement leaks a small amount of
information about which blocks are free or not, and thereby allows
an adversary to distinguish between whether recent writes have
been to the same address, or to different addresses. We formalize
this notion and prove the insecurity of these schemes below.
We note that, since the submission of this work, the authors of
[5] have acknowledged the vulnerability here and proposed a fix
as a preprint [6].
Overview of scheme. Let N be the number of logical blocks. Data-
Lair sets 2N to the number of physical blocks so that the number of
free physical blocks is always N . In DataLair [5, Section IV], every
ORAMWrite considers two disjoint sets of k items:
• Free set S0: A set of k blocks chosen randomly among the N
• Random set S1: A set of k blocks chosen randomly among
free physical blocks.
the entire 2N physical blocks.
To make sure that S0 and S1 are disjoint, some elements may be
removed and addional steps of sampling may be done. Based on
the two sets, the ORAM writes a data block as follows:
ORAMWrite(d): // d is a data block
(1) Insert d in stash
(2) Create two sets S0 and S1 as described above.
(3) Choose k blocks U = {u1, . . . , uk } as follows:
For i = 1 to k:
bi ← {0, 1}, and fetch (and remove) ui from Sbi
If bi = 0 and stash is not empty:
Take out a data item from stash and write it in ui .
Otherwise, reencrypt ui .
.
We assume N > 2k and k ≥ 3. The actual scheme chooses a
large N and k = 3.
Insecurity of the scheme. We note that the access pattern of a
single ORAMWrite is hidden. However, that alone is not sufficient
to show write obliviousness. In particular, security breaks down
when one considers multiple ORAMWrite operations.
Observe that the above algorithm is more likely to choose a free
block than a non-free block; with probability 1/2, a chosen block
will be from S0 and thereby always free, and with probability 1/2,
a chosen block will be from S1 and thereby sometimes free. This
tendency towards choosing free blocks leaks information. To clarify
our point, consider the following two sequences of logical writes:
0 = (init, w0, w0, w2), seq
seq
1 = (init, w0, w1, w2)
Here, wi denotes writing data to a logical address i, and init is a
sequence of operations that makes the ORAM have exactly N free
blocks.2
Let Ui = (ui,1, . . . , ui,k) be the set of chosen blocks from the ith
ORAMWrite after the init sequence. Let dℓ be the data in logical
block ℓ.
Then, in seq0, physical block γ ∈ U1 containing d0 will be prob-
ably freed up thanks to the second w0, and the last w2 may be able
to choose γ as a free block. However, in seq1, the block γ cannot
be freed up by w1, since γ contains d0! So, the last w2 can choose γ
only as a non-free block. Due to the different probablity weights in
choosing free blocks vs. non-free blocks, U1 and U3 are more likely
to overlap in seq0 than in seq1, and security breaks down.
To clarify our point, we give an attack. Given an access pattern
(U1, U2, U3), the adversary tries to tell if it is from seq0 or seq1.
Consider the following events:
• X: u1,1 (cid:60) U2, Y: u1,1 ∈ U3, E: X ∧ Y
The adversary works as follows:
Output 0 if E takes place; otherwise output a random bit.
0−p
which proves that the adversary is a good distinguisher.
Let pb = Pr[E] from seqb. We show that p
1 is non-negligible,
Let Fi(u) denote a predicate indicating whether a physical block
u was free when the ith ORAMWrite starts. Note that whether u1,1
belongs U3 ultimately depends on F3(u1,1). In particular, for any
u1,1, we have
qy = Pr[Y | F b3 (u1,1)] =
qn = Pr[Y | ¬F b3 (u1,1)] =
+
1
2 · k
1
2 ·
1
2 ·
N
2N − k
k
k
2N − k
0, . . . , d∗
The following table shows how F3(u1,1) depends on the previ-
ous events. In the table, D2(u) ∈ { f , d∗
N−1, d0} denotes a
random variable indicating which logical block a physical block u
contains when the second ORAMWrite starts. If the value is f , it
means the block is free, and d∗
is the initial data for the logical block
ℓ that the ORAM initialization procedure used. The value d0 de-
notes the data block used in the first w0 operation in seq0 and seq1.
Let Si(ℓ) denote a predicate indicating whether a logical block ℓ is
in the stash when the ith ORAMWrite starts. In addition, FreeSeti
denotes a predicate indicating whether the ith ORAMWrite found
a physical block in the free set S0 (thereby successfully writing the
input logical block in the free physical block).
ℓ
2 Their ORAM seems to be initialized with exactly N free blocks, in which case init
contains no operation. If that’s not the case, we can set the init sequence as follows:
init = (w0, . . . , wN−1, w0, . . . , w0
λ times
(cid:124)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:123)(cid:122)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:125)
),
where λ is the security parameter. Note that after the init sequence, the ORAM will
have exactly N non-free physical blocks and N free physical blocks with probability
least 1 − negl(λ). So, we can safely ignore this negligible probability, and proceed our
argument assuming that the ORAM has exactly N free blocks after the init sequence.
0 or 1
x
0
1
0
1
1
1
x
0
0
x
1
x
x
S2(d0)
FreeSet2