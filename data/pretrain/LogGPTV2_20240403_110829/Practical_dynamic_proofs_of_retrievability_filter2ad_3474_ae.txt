throughput of our scheme. The server’s disk I/O cost is
about O(log n), i.e., it needs to read roughly O(log n) blocks
for every write operation. Our experiments show that if
server disk I/O were maxed out, the POR write throughput
we can achieve would be roughly 20MB/s under our setup.
We cache the smallest 10 levels of H in memory in this
experiment.
In practice, however, the client-server band-
width is more likely to be the bottleneck, and the actual
POR throughput achievable will be limited by the available
client-server bandwidth.
While the server needs to read O(log n) blocks for each
write, the number of seeks is very small. As mentioned ear-
lier in Section 6.1, the hierarchical log structure H is mostly
written in a sequential fashion, and since we choose a large
chunk size (roughly 50MB), and cache chunks in memory,
every write operation requires only 0.03 to 0.06 seeks on av-
erage. We cache the Merkle hash tree for U in memory in
our experiments.
6.2.2 Audit Cost
We use a λ = 128 for these experiments, i.e., each audit
samples 128 blocks from each level H(cid:96) and buﬀer C.
Figure 8 shows the time spent by the server for each audit
operation—including time for reading disk and performing
computation, but not including network transfer time be-
tween client and server (client-server network overhead is
characterized separately in Figure 10). The majority of this
time is spent on disk I/O, and is dominated by disk seeks.
There are roughly O(λ log n) seeks per audit operation par-
allelized to 7 disks, and each seek takes roughly 12ms.
Figure 9 shows the disk I/O cost for an audit. As men-
tioned in Section 6.1, we optimize for writes at slightly higher
audit cost, and the audit disk I/O cost is O(λβ log2 n)—this
is why the line curves slightly, and is not linear.
334[17] Z. Mo, Y. Zhou, and S. Chen. A dynamic proof of
retrievability (por) scheme with o(log n) complexity. In
ICC’12, pages 912–916, 2012.
[18] R. Ostrovsky and V. Shoup. Private information storage
(extended abstract). In STOC, pages 294–303, 1997.
[19] C. Papamanthou, E. Shi, R. Tamassia, and K. Yi.
Streaming authenticated data structures. In
EUROCRYPT, 2013.
[20] H. Shacham and B. Waters. Compact proofs of
retrievability. In ASIACRYPT, pages 90–107, 2008.
[21] E. Shi, T.-H. H. Chan, E. Stefanov, and M. Li. Oblivious
RAM with O((log N )3) worst-case cost. In ASIACRYPT,
pages 197–214, 2011.
[22] E. Shi, E. Stefanov, and C. Papamanthou. Practical
dynamic proofs of retrievability. Technical report, 2013.
[23] D. A. Spielman. Linear-time encodable and decodable
error-correcting codes. IEEE Transactions on Information
Theory, 42(6):1723–1731, 1996.
[24] E. Stefanov and E. Shi. Oblivistore: High performance
oblivious cloud storage. In IEEE Symposium on Security
and Privacy, 2013.
[25] E. Stefanov, E. Shi, and D. Song. Towards practical
oblivious RAM. In NDSS, 2012.
[26] E. Stefanov, M. van Dijk, A. Juels, and A. Oprea. Iris: a
scalable cloud ﬁle system with eﬃcient integrity checks. In
ACSAC, pages 229–238, 2012.
[27] J. van Lint. Introduction to Coding Theory.
Springer-Verlag, 1992.
[28] Q. Wang, C. Wang, J. Li, K. Ren, and W. Lou. Enabling
public veriﬁability and data dynamics for storage security
in cloud computing. In ESORICS, 2009.
[29] Q. Wang, C. Wang, K. Ren, W. Lou, and J. Li. Enabling
public auditability and data dynamics for storage security
in cloud computing. IEEE Trans. Parallel Distrib. Syst.,
22(5):847–859, 2011.
[30] P. Williams, R. Sion, and B. Carbunar. Building castles out
of mud: practical access pattern privacy and correctness on
untrusted storage. In CCS, 2008.
[31] P. Williams, R. Sion, and A. Tomescu. Privatefs: A parallel
oblivious ﬁle system. In CCS, 2012.
Figure 11: Computational throughput for hierarchical
erasure coding. Error bars denote 1 standard deviation from
20 runs.
Figure 10 shows the client-server bandwidth consumed for
an audit. As mentioned in Section 6.1, unlike our theoretic
construction, our implementation chooses to speed up writes
at slightly higher client-server bandwidth for audits, namely,
β log n + O(λ log n).
Computational overhead for hierarchical erasure cod-
ing. As shown in Figure 11, the hierarchical coding scheme
can be computed at extremely fast speeds, i.e., >1600MB/s
per level on a modern processor. To characterize the com-
putational cost, we cached about 4GB of data in memory,
and avoid performing disk fetches during this experiment.
7. REFERENCES
[1] https://en.wikipedia.org/wiki/Hash_tree.
[2] Fast fourier transform. http://math.berkeley.edu/
~berlek/classes/CLASS.110/LECTURES/FFT.
[3] Fast fourier transform.
http://en.wikipedia.org/wiki/Fast_Fourier_transform.
[4] Sloccount. http://www.dwheeler.com/sloccount/.
[5] G. Ateniese, R. Burns, R. Curtmola, J. Herring, L. Kissner,
Z. Peterson, and D. Song. Provable data possession at
untrusted stores. In CCS, 2007.
[32] Q. Zheng and S. Xu. Fair and dynamic proofs of
retrievability. In CODASPY, 2011.
[6] S. Benabbas, R. Gennaro, and Y. Vahlis. Veriﬁable
delegation of computation over large datasets. In
CRYPTO, pages 111–131, 2011.
[7] K. D. Bowers, A. Juels, and A. Oprea. Proofs of
retrievability: theory and implementation. In CCSW, pages
43–54, 2009.
[8] D. Cash, A. K¨up¸c¨u, and D. Wichs. Dynamic proofs of
retrievability via oblivious ram. In Eurocrypt, 2013.
[9] Y. Dodis, S. P. Vadhan, and D. Wichs. Proofs of
retrievability via hardness ampliﬁcation. In TCC, pages
109–127, 2009.
[10] C. Erway, A. K¨up¸c¨u, C. Papamanthou, and R. Tamassia.
Dynamic provable data possession. In CCS, 2009.
[11] O. Goldreich and R. Ostrovsky. Software protection and
simulation on oblivious RAMs. J. ACM, 1996.
[12] M. T. Goodrich and M. Mitzenmacher. Privacy-preserving
access of outsourced data via oblivious RAM simulation. In
ICALP, 2011.
[13] M. T. Goodrich, M. Mitzenmacher, O. Ohrimenko, and
R. Tamassia. Privacy-preserving group data access via
stateless oblivious RAM simulation. In SODA, 2012.
[14] A. Juels and B. S. K. Jr. Pors: proofs of retrievability for
large ﬁles. In ACM Conference on Computer and
Communications Security, pages 584–597, 2007.
[15] E. Kushilevitz, S. Lu, and R. Ostrovsky. On the
(in)security of hash-based oblivious RAM and a new
balancing scheme. In SODA, 2012.
[16] R. C. Merkle. A certiﬁed digital signature. In Proceedings
on Advances in cryptology, CRYPTO ’89, 1989.
APPENDIX
A. ACHIEVING PUBLIC VERIFIABILITY
Although our basic construction in Section 4 provides pub-
lic veriﬁability, the more eﬃcient scheme described in Sec-
tion 5 does not, since the homomorphic checksum requires
the client keep secret state. In this section we show how to
turn the eﬃcient scheme of Section 5 into publicly veriﬁable.
In a publicly veriﬁable setting, only a trusted data source
can write data; however, anyone can perform veriﬁable reads
and PoR audits.
Ensuring the public veriﬁability of reads comes for free,
since a Merkle hash tree is maintained over the buﬀer U.
Therefore the trusted source can simply sign and publish the
up-to-date root digest of this Merkle-hash tree and make it
available to the public for veriﬁcation of read operations.
We now focus on how to achieve public auditability so
that to enable anyone challenge the server to prove that it
possesses all data (owned by the source) and still maintain
the bandwidth of the write operations to be low.
A.1 Public Auditability with Low Write Cost
To achieve public auditability, we build a separate Merkle
hash tree over the blocks of each level H(cid:96), and one for C.
335The up-to-date root digests of all O(log n) hash trees will
be publicized. During a public audit, a user with the root
digests requests O(λ) blocks at random for each level H(cid:96)
and buﬀer C, and checks them with the root digests.
One question remains: how does the trusted source keep
track of these root digests without having to transfer original
blocks during the rebuilding of the hierarchical levels? To
address this question, we sketch our idea below, and leave
the details and full proofs to the full online version [22].
Our idea is to have the server compute the new Merkle
trees for a level (or the buﬀer C) when it is being rebuilt.
However, we need to protect against a malicious server that
can potentially cheat and output the wrong digests. We
apply a probabilistic checking idea again here.
When rebuilding a level H(cid:96) (or buﬀer C), the following
happens:
• As in the secretly-veriﬁable scheme (Section 5), the trusted
source downloads the encrypted/authenticated homomor-
phic checksums and “simulates” the rebuilding over these
(cid:101)σ(B)’s.
• The server performs the rebuilding, computes the new
digest h of H(cid:96), and commits h to the source.
• The source challenges O(λ) random blocks in H(cid:96). For
each challenged block B: the source downloads the block
itself B, its(cid:101)σ(B), and the path in the Merkle tree neces-
sary to verify block B. The client now checks that(cid:101)σ(B)
veriﬁes for B, and that the already committed root digest
h veriﬁes for B as well.
Proof intuition. At a high level, this idea works because if
the server can pass the probabilistic check (of the commit-
ted root digest h), then at least a constant fraction of the
rebuilt level H(cid:96) (or buﬀer C) is correctly incorporated into
the claimed digest h. Due to PoR’s inherent erasure coding,
it turns out that this suﬃces proving the retrievability of the
publicly veriﬁable PoR scheme. The full proof is deferred to
the full online version [22].
Write cost. Suppose the trusted source caches locally the
smallest log λ+log(2/) levels consisting about 2λ/ number
of blocks, for an arbitrarily small 0 <  < 1. Note that these
are the levels that are accessed more frequently during the
write operations. We can then show that the source-server
bandwidth for each write operation is β(1 + ) + O(λ log n).
The details of this analysis is elementary, and deferred to
the full online version [22]. The above analysis assumes that
exactly λ blocks are probabilistically checked for each Merkle
tree hash tree of the remaining (uncached) levels.
A.2 Reducing Public Audit Cost
The publicly veriﬁable approach described above requires
O(λ log n(β + log n)) overhead for public auditing. Particu-
larly, for each of the O(log n) levels H(cid:96) as well as C, O(λ)
blocks need to be checked; and to check a block involves
downloading the block itself and log n hashes of the Merkle
hash tree. With some additional tricks, in particular, by
carefully aligning the Merkle tree structure with the hierar-
chical log structure H, we can further improve the public
audit overhead to O(λβ log n). We defer these details to the
Appendix. The basic idea is as follows:
• Instead of building a separate Merkle tree per level H(cid:96),
build a single Merkle tree over the entire hirarchical log
structure H, and another one for C. Furthermore, the
Merkle tree will be aligned on top of the hierarchical
structure H. Since H has exponentially growing lev-
els, we can view H as a tree, where internal nodes are
assigned values—in our case, the blocks are the val-
ues of internal nodes of the Merkle tree. The hash
of each internal node in the Merkle tree is computed
as H(hleft, hright, B), where hleft is the hash of the left
child, hright is the hash of the right child, and B is the
block associated with that node. The client publishes
the hash of the Merkle tree for H and the one for C.
• During public audits, random checks for C are still done
as before. To check H, instead of randomly sample
O(λ) blocks from each level H(cid:96), the client randomly
samples O(λ) paths from the root to O(λ) randomly se-
lected leaf nodes, and sample the blocks on these paths.
As a result O(λ) blocks from each level gets sampled,
and it is not hard to see that only O(λ log n) hashes
needs to be transmitted to verify all the paths sampled
– namely, hashes of all sibling nodes to the sampled
paths need to be transmitted. This reduces the public
audit overhead to O(λβ log n), assuming β = O(λ).
• When a non-top level H(cid:96) gets rebuilt, the server re-
builds the Merkle hash tree, and the client performs
the following probabilistic checking protocol.
The client ﬁrst retrieves all hashes at level H(cid:96)+1, and
makes sure that they are consistent with the old di-
gest. The client then randomly samples O(λ) blocks at
level H(cid:96), to make sure that these blocks are correctly
incorporated into the new root digest as claimed by the
server.
When the top level is rebuilt, the client simply checks
O(λ) random blocks in the top level, and ensures that
they are correctly incorporated into the root digest.
Theorem 4. The dynamic PoR scheme with public veri-
ﬁability satisﬁes both authenticity (Deﬁnition 1) and retriev-
ability (Deﬁnition 2). The proof is in the full online ver-
sion [22].
A.3 Resizing the Storage
Our scheme can easily be modiﬁed to support insertions
and deletions of blocks. Insertions can easily be supported
by adding a level to the hierarchical log structure H when-
ever number of blocks doubles. Deletions can be supported
by updating the deleted block with ⊥. Further, whenever
the number of deleted elements exceeds roughly half the size
of the dataset, the client can rebuild and consilidate the hi-
erarchical log by suppressing deleted items. We will provide
more details in the online full version [22].
Acknowledgments
This work is partially supported by NSF grant CNS-1314857,
a Google Research Award, the NSF Graduate Research Fel-
lowship under Grant No. DGE-0946797, by a DoD National
Defense Science and Engineering Graduate Fellowship, by
Intel award through the ISTC for Secure Computing, and a
grant from Amazon Web Services. Any opinions, ﬁndings,
and conclusions or recommendations expressed in this mate-
rial are those of the author(s) and do not necessarily reﬂect
the views of the funding agencies. We would like to thank
Hubert Chan, Jonathan Katz, and Hongsheng Zhou for help-
ful discussions, and the anonymous reviewers for their in-
sightful feedback.
336