4
128
32 KiB
physical
physical
64 B
512
8
64
32 KiB
virtual
physical
64 B
512
4
128
32 KiB
virtual
physical
64 B
512
4
128
32 KiB
virtual
physical
32 B
1024
4
256
32 KiB
virtual
physical
32 B
1024
4
256
128 KiB
32 B
4096
8
512
4
4096 KiB
64 B
65536
16
4096
64
256 KiB
64 B
4096
8
512
8
256 KiB
64 B
4096
8
512
8
1024 KiB
32 B
32768
16
2048
16
1024 KiB
32 B
32768
16
2048
16
Table 1: Experimental platforms.
change is small and localised, our previous experience [Klein et al.,
2014] with re-veriﬁcation of code changes for seL4 suggests that it
should be straightforward to verify.
Effectiveness of instruction-based scheduling
We re-run the experiments used to generate Figure 1 with
instruction-based scheduling enabled. Under IBS, timeslices are
no longer constant, although in practice the variation is small. To
compare with our other results, we normalise the bandwidth of the
IBS channel to a sampling rate of 333Hz. This could be achieved,
for example on the DM3730 (1GHz clock), by setting K = 106
(assuming 1 instruction per cycle). In practice, once sender and
receiver start contending, and hence stalling more often, the real
timeslice will grow (as instructions take longer on average to exe-
cute). The effect is to reduce the available bandwidth, and hence
normalised results are a safe worst-case estimate.
The results from using IBS (for compatibility with existing data,
we ran with K = 105 rather than K = 106) for all platforms
are summarised in Table 2. For the simplest (and oldest) core, the
ARM1136-based iMX.31, the mitigation is perfect (down to the
limit of our statistical precision), showing a 20,000-fold reduction
in capacity, from 1,400 b/s down to 0.1 b/s. However, for the more
recent platforms, signiﬁcant channels remain. On the Exynos, for
example, the channel bandwidth is reduced by a factor of 87, still
leaving a remaining channel of 27 b/s. While the channel matrix is
now a very narrow band, it still has horizontal structure, as shown
(greatly magniﬁed) in Figure 6.
As we move to more complex cores the results get steadily
worse, until we reach the Cortex A9-based Exynos4412 and the
Conroe-based E6550, which show a reduction factor of only 6
and 154 respectively. Looking at the channel matrix for the
Exynos4412 in Figure 6 suggests an explanation. As we preempt
after 100,000 instructions, and each receiver loop iteration (and
hence line touched) takes exactly 10 instructions, we expect to
see precisely 10,000 lines touched per preemption. On the sim-
pler cores, that is exactly what we see, but in Figure 6 the number
0
0
0
0
1
−
d
e
h
c
u
o
t
s
e
n
i
L
16
12
8
4
0
0
10
20
30
Lines evicted /103
10−1
10−2
10−3
10−4
Figure 6: Exynos4412 cache channel with IBS. N = 1000,
B = 400 b/s, CI max
0 = 1.2 b/s.
clearly varies according to the level of contention.3
Most of the variation here is due to delaying the PMU interrupt.
The interrupt arrives 12 iterations (120 cycles) late without con-
tention, dropping to 8 (80 cycles) when the sender ﬁlls the L1 cache
(512 lines, thus contending in the L2), and further to 40 cyc once
we pass the size of a single L2 cache way (2048 lines), and start
to see self-conﬂict misses.
It seems that the core delays the ex-
ception until a break in the instruction stream, e.g. a stall due to a
cache miss, and thus the overshoot drops as the rate of misses in-
creases (and thus the likelihood of a stall shortly after the exception
is raised).
An approach that we are yet to try, is to conﬁgure the PMU to in-
terrupt a little earlier than when K instructions have been executed,
and then single-stepping the processor using hardware breakpoints
until precisely the Kth instruction. This strategy has previously
proved effective in the context of execution replay [Dunlap, 2006].
5.4 Cache colouring
Unlike IBS, cache colouring does not require denying the re-
3The variation in Figure 6 is on the order of 10 cycles, and is ex-
pressed as an offset from the expected value of 10,000. Figure 8
(top) is presented similarly.
575Baseline
Instruction-Based Scheduling
Cache Colouring
Platform
iMX.31
AM3358
DM3730
iMX.6
Exynos4412
Exynos4412(TLB ﬂush)
E6550(2ms TS)
E6550(improved)
B (b/s)
1,400
1,600
1,800
2,100
2,400
2,400
1,500
3,000
N
7,000
6,000
8,000
800
1,000
1,000
1,000
800
B (b/s)
0.1
0.6
0.5
-
400
-
9.5
-
N
10,000
10,000
10,000
-
1,000
-
600
-
Factor
20200
2700
3500
-
5.9
-
150
-
CI max
0
0.02
0.32
0.26
-
1.2
-
11
-
B (b/s)
7.1
5.0
1.7
12
27
25
76
120
N
63,972
49,600
63,200
11,400
7,200
7,200
4,836
7,500
Factor CI max
200
330
1000
180
87
94
19
26
0
3.8
2.7
0.9
6.3
15
13
42
62
Table 2: Mitigation effectiveness against the pre-emption clock across platforms. 1ms timeslice unless noted.
frame number
frame offset
31
. . . 15
14 13 12 11
10 9 8 7 6 5 4
3 2 1 0
cache set selector
line index
Figure 7: Cache colouring on the Exynos4412, showing colour
bits 15–12, where frame number and cache set selector overlap.
ceiver wall-clock time. Colouring partitions the cache between
sender and receiver, preventing contention. This is achieved by
colouring all physical memory, and allocating disjoint colours to
different partitions. Colouring of memory happens at page granu-
larity, as this is the OS-level allocation unit.
Figure 7 illustrates colouring on the Exynos4412. Its 32 B cache
lines are indexed by the 5 least signiﬁcant bits (4–0) of the physical
address (PA), while the next 11 bits (15–5), the cache set selector,
are used to select one of 2048 16-way associative sets. A 4 kiB
frame is identiﬁed by the top 20 bits (31–12) of the PA: its frame
number. Note that the last four bits of the frame number (15–12,
highlighted) overlap with the top bits of the cache selector—the
sets covered by a frame depend on its location. Two frames whose
addresses differ in any of these colour bits will never collide. We
thus divide memory into a number (here 24 = 16) of coloured
pools, and assign partitions to separate pools.
We partition not just user code and data, but also the kernel heap
(using seL4’s allocation model [Klein et al., 2014]). We also repli-
cate the kernel’s code in each partition. An improved version (see
below) also colours the kernel stack; colouring kernel global data
is future work, as it is only small, and not under user control.
The L1 caches of our platforms either have only one colour
(cache size divided by associativity does not exceed page size),
and therefore cannot be partitioned by colouring, or are indexed
by virtual address (which is outside of OS control). Therefore the
L1 caches must be ﬂushed on a partition switch to prevent a timing
channel.
Cache Colouring Effectiveness
Table 2 summarises the results, and Figure 3 shows the channel
matrix for the Exynos4412, those for other platforms are similar
(see Cock [2014]). For the simpler cores (iMX.31, AM3358 &
DM3730) we see a great reduction in bandwidth (factor 200–1000).
Yet, every result here fails the statistical test introduced in Sec-
tion 4: The bandwidth we see has less than a one in 1000 chance of
being produced for a channel of a true bandwidth of zero.
The results are particularly unimpressive on the more complex
Exynos4412 and E6550 (factor 20–90). We see a similar pattern
as for IBS: mitigation is effective on older, simpler cores (although
0
9
2
2
1
−
d
e
h
c
u
o
t
s
e
n
i
L
e
n
i
l
/
s
l
l
a
t
S
50
25
0
16
12
8
4
No TLB ﬂush
TLB ﬂush
No TLB ﬂush
TLB ﬂush
0
10
20
30
Figure 8: Residual TLB channel on the Exynos4412.
Lines evicted /103
less so than IBS), and becomes steadily worse as the cores become
more complex, although becoming more effective than IBS on the
more recent chips.
Figure 8 provides an explanation for the Exynos4412. In the top
plot, we see the column average, shown in blue (lower line), of
Figure 3. This is the expected value of the channel output (num-
ber of lines touched), for each input (number evicted). Here we
see a small, but clear variation, on the order of 5 parts in 10,000,
depending on the eviction rate. The cause is shown by the corre-
sponding blue (lower) curve in the bottom plot: as the rate of stalls
due to TLB misses increases, the rate of progress of the receiver de-
creases. The sender and receiver are competing in the TLB, which
is not partitioned by cache colouring (the ﬁrst-level TLBs on this
chip are fully associative, and thus cannot be coloured). Flushing
the TLB on a context switch eliminates the variation, as shown by