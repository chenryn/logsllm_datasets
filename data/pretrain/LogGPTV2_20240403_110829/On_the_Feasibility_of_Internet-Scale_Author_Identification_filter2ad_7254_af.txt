features used alongside those of [23] boost performance by
2%.
Performance. We implemented our classiﬁers using a
variety of programming languages and libraries, including
Matlab and Python/Numpy. In the end, Matlab proved to be
the fastest because of its optimized linear algebra routines.
Of the classiﬁers we experimented with, nearest-neighbor is
the simplest and therefore the fastest to train and test with.
Computing the class centroids (i.e., collapsing each class)
takes a negligible amount of time and classifying 150,000
points from 50,000 distinct classes takes 4.5 minutes. RLSC
requires substantially more processing to train, so it takes 12
minutes to train on all of the data, minus the three points
that are removed for testing. After training, RLSC behaves
like NN in how it makes predictions.
Finally, we present several “tricks” that lead to a dramatic
speedup in running time. The most signiﬁcant factor during
the prediction phase is to express computations as matrix
multiplications. For example, RLSC predictions are obtained
by computing the inner product of all one-versus-all classi-
ﬁers with each testing sample. The Euclidean distance used
in nearest-neighbors can be expanded to a similar series of
inner products. Once our computations are represented as
matrix multiplications, we can the compute everything in
large batches that make full use of the computer’s hardware.
This trick alone brought prediction time down from 10 hours
to the aformentioned 4.5 minutes. Interestingly, we found it
was several times faster to avoid sparse format representa-
tions of our data because they are not as optimized. Finally,
we employ a form of dynamic programming for RLSC that
decreases the number of computations dramatically. This too
brings down training time from two days to 12 minutes.
VII. LIMITATIONS, FUTURE WORK AND CONCLUSIONS
Our work has a few important limitations. First, the attack
is unlikely to work if the victim intentionally obfuscates
their writing style. Second, while we have validated our
attack in a cross-context setting (i.e., two different blogs),
we have not tested it in a cross-domain setting (e.g., labeled
text is a blog, whereas anonymous text is is an e-mail).
Both these limitations are discussed in detail in Section III.
Finally, our method might not meet the requirements of
forensics applications where the number of authors might
be very small (the classiﬁcation task might even be binary)
but amount of text might be very limited.
We point out three main avenues for future research in
terms of sharpening our techniques. First, studying more
classiﬁers such as regularized discriminant analysis, and
fully understand the limitations and advantages of each.
Second,
investigating more principled and robust ap-
proaches for extending binary classiﬁers to a highly multi-
class setting. One important avenue is “all pairs,” where
every class is compared against every other class. As one
might imagine, this method gives good results, but is compu-
tationally prohibitive. A “tournament” between classes using
a binary classiﬁer is a promising approach, and we have
some early results in this area.
Finally, while we have demonstrated the viability of
our techniques in a cross-context setting, a more thorough
investigation of different classiﬁers is necessary, along with
an analysis of the impact of training and test set size.
Understanding and modeling how writing style is affected
by context has the potential to bring major gains in accuracy.
To conclude, we have conducted the ﬁrst investigation
into the possibility that stylometry techniques might pose a
wide-spread privacy risk by identifying authors of anony-
mously published content based on nothing but their style
of expression. Previous work has applied similar techniques
to distinguish among up to 300 authors; we consider the
scenario of 100,000.
this scale:
Our ﬁndings indicate these techniques remain surprisingly
effective at
in about 20% of trials in each
experiment, the author of the sample is ranked ﬁrst among
all 100,000. Authors with large amounts of text already
online are even more vulnerable. Even in the cases where
the author of a text is not the one estimated most likely
to have produced it, some risk of identiﬁcation remains.
Speciﬁcally, in the median case, the likely authors of a
sample can be narrowed down to a set of 100–200, a
reduction by a factor of 500–1000. While this alone is
unlikely to produce a match with any certainty, if it is
combined with another source of information, it may be
enough to make the difference between an author remaining
anonymous and being identiﬁed.
Importantly, our ﬁndings only represent a lower bound
on the severity of this type of risk, and we can expect the
development of more sophisticated techniques to worsen
the situation. Useful future work to address the privacy
threat would include further characterizations of the most
vulnerable authors, and improved writing-style obfuscation
techniques.
Acknowledgement. We would like to thank the authors
of [28] for sharing the notes of their reimplementation of the
Writeprints algorithm from [23] and the authors of [46] for
sharing their Google proﬁles dataset. We are also grateful to
Roberto Perdisci, Mario Frank, Ling Huang and anonymous
reviewers for helpful comments. This material is supported
by the National Science Foundation under Grants No. CCF-
0424422, 0842695, by the MURI program under Air Force
313
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:49:27 UTC from IEEE Xplore.  Restrictions apply. 
Ofﬁce of Scientiﬁc Research Grant No. FA9550-08-1-0352,
the National Science Foundation Graduate Research Fel-
lowship under Grant No. DGE-0946797, the Department of
Defense through the National Defense Science and Engi-
neering Graduate Fellowship Program, by the Intel Science
and Technology Center for Secure Computing, and by a
grant from the Amazon Web Services in Education program.
Any opinions, ﬁndings and conclusions or recommendations
expressed in this material are those of the author(s) and do
not necessarily reﬂect the views of the funding agencies.
REFERENCES
[1] A. Furtwangler, The Authority of Publius: A Reading of the Federalist Papers.
[2] Supreme Court of the United States, “McIntyre vs. Ohio Elections Commission,
Cornell University Press, 1984.
514 U.S. 334,” 1995.
[3] M. L. Sifry, WikiLeaks and the age of transparency. Counterpoint, 2011.
[4] Mike Giglio, “The Facebook Freedom Fighter,” Newsweek, Feb. 2011.
[5] J. Bone, “Vogue model Liskula Cohen wins right to unmask offensive blogger,”
[6] J. Brumley, “Unmasked blogger blames First Baptist, sheriff’s ofﬁce,” The
[7] M. E. Larios, “ePublius: Anonymous Speech Rights Online,” Rutgers Law
[8] G. H. Pike, “The Right to Remain Anonymous,” Information Today, Vol. 25,
The Times, Aug. 2009.
Florida Times-Union, Apr. 2009.
Record, Vol. 37, p. 36, 2010, 2010.
No. 4, p. 17, April 2008, 2008.
[9] J. Mayer, “Any person... a pamphleteer: Internet Anonymity in the Age of Web
2.0,” Undergraduate Senior Thesis, Princeton University, 2009.
[10] R. Dingledine, N. Mathewson, and P. Syverson, “Tor: The second-generation
onion router,” in USENIX Security Symposium, 2004.
[11] M. Koppel, J. Schler, and S. Argamon, “Authorship attribution in the wild,”
Language Resources and Evaluation, vol. 45, no. 1, pp. 83–94, 2011.
[12] K. Luyckx and W. Daelemans, “The effect of author set size and data size in
authorship attribution,” Literary and Linguistic Computing, vol. 26, no. 1, pp.
35–55, 2011.
[13] H. Paskov, “A regularization framework for active learning from imbalanced
data,” Master’s thesis, MIT, 2008.
[14] C. Rubin, “Surprised Employer Fires Sex Blogger,” Inc. Magazine, May 2010.
[15] M. Schwartz, “The Troubles of Korea’s Inﬂuential Economic Pundit,” WIRED,
Oct. 2009.
techniques,” in IAAI, 2009.
[16] M. Brennan and R. Greenstadt, “Practical attacks against authorship recognition
[17] G. Kacmarcik and M. Gamon, “Obfuscating document stylometry to preserve
author anonymity,” in The Association for Computer Linguistics, 2006.
[18] T. C. Mendenhall, “The characteristic curves of composition,” Science, vol. ns-9,
[19] ——, “A mechanical solution of a literary problem,” Popular Science Monthly,
[20] F. Mosteller and D. L. Wallace, Inference and Disputed Authorship: The
no. 214S, pp. 237–246, 1887.
vol. 60, no. 2, pp. 97–105, 1901.
Federalist. Addison-Wesley, 1964.
[21] E. Stamatatos, “A survey of modern authorship attribution methods,” J. Am. Soc.
Inf. Sci. Technol., vol. 60, pp. 538–556, March 2009.
[22] D. Madigan, A. Genkin, D. D. Lewis, S. Argamon, D. Fradkin, and L. Ye,
“Author identiﬁcation on the large scale,” in Joint Meeting of the Interface and
Classiﬁcation Society of North America, 2005.
[23] A. Abbasi and H. Chen, “Writeprints: A stylometric approach to identity-level
identiﬁcation and similarity detection in cyberspace,” ACM Transactions on
Information Systems, vol. 26, no. 2, Mar. 2008.
[24] O. Y. de Vel, A. Anderson, M. Corney, and G. M. Mohay, “Mining email content
for author identiﬁcation forensics,” SIGMOD Record, vol. 30, no. 4, pp. 55–64,
2001.
[25] C. E. Chaski, “Who’s at the keyboard? authorship attribution in digital evidence
investigations,” IJDE, vol. 4, no. 1, 2005.
[26] ——, “The Keyboard Dilemma and Authorship Identiﬁcation,” in IFIP Int. Conf.
Digital Forensics, 2007, pp. 133–146.
[27] D. I. Holmes, “A stylometric analysis of mormon scripture and related texts,”
Journal of the Royal Statistical Society Series A Statistics in Society, vol. 155,
no. 1, pp. 91–120, 1992.
[28] S. Afroz, M. Brennan, and R. Greenstadt, “Detecting Hoaxes, Frauds, and
Deception in Writing Style Online,” in IEEE Symposium on Security and
Privacy, 2012.
[29] E. Stamatatos, N. Fakotakis, and G. Kokkinakis, “Text genre detection using
common word frequencies,” in Proceedings of the 18th conference on Com-
putational linguistics - Volume 2, ser. COLING ’00.
Stroudsburg, PA, USA:
Association for Computational Linguistics, 2000, pp. 808–814.
[30] M. Koppel, J. Schler, and S. Argamon, “Computational methods in authorship
attribution,” JASIST, vol. 60, no. 1, pp. 9–26, 2009.
[31] E. Backer and P. van Kranenburg, “On musical stylometry–a pattern recognition
approach,” pp. 299 – 309, 2005, in Memoriam: Azriel Rosenfeld.
[32] M. Shevertalov, J. Kothari, E. Stehle, and S. Mancoridis, “On the use of
discretized source code metrics for author identiﬁcation,” in SSBSE, 2009.
[33] W.-J. Li, K. Wang, S. J. Stolfo, and B. Herzog, “Fileprints: identifying ﬁle
types by n-gram analysis,” in IEEE Systems, Man, and Cybernetics Information
Assurance Workshop, 2005.
[34] L. Zhuang, F. Zhou, and J. D. Tygar, “Keyboard acoustic emanations revisited,”
in ACM conference on Computer and communications security, 2005.
[35] P. Juola, “Authorship attribution,” Foundations and Trends in Information
Retrieval, vol. 1, pp. 233–334, December 2006.
[36] C. E. Chaski, “Empirical evaluations of language-based author identiﬁcation
techniques,” Forensic Linguistics, vol. 8, no. 1, pp. 1–65, 2001.
[37] H. Maurer, F. Kappe, and B. Zaka, “Plagiarism - a survey,” Journal of Universal
Computer Science, vol. 12, no. 8, pp. 1050–1084, 2006.
[38] J. R. Rao and P. Rohatgi, “Can pseudonymity really guarantee privacy?” in
Proceedings of the 9th conference on USENIX Security Symposium - Volume 9.
Berkeley, CA, USA: USENIX Association, 2000, pp. 7–7.
[39] M. Koppel, J. Schler, S. Argamon, and E. Messeri, “Authorship attribution with
thousands of candidate authors,” in SIGIR, 2006, pp. 659–660.
[40] M. Nanavati, N. Taylor, W. Aiello, and A. Warﬁeld, “Herbert west:
deanonymizer,” in Proceedings of the 6th USENIX conference on Hot topics
in security, ser. HotSec’11. Berkeley, CA, USA: USENIX Association, 2011,
pp. 6–6.
[41] S. Hill and F. J. Provost, “The myth of the double-blind review? Author
identiﬁcation using only citations,” SIGKDD Explorations, vol. 5, no. 2, pp.
179–184, 2003.
[42] J. K. Bradley, P. G. Kelley, and A. Roth, “Author identiﬁcation from citations,”
Tech. Rep., Dec. 03 2008.
[43] P. Eckersley, “How unique is your web browser?” Electronic Frontier
Foundation, Tech. Rep., 2009. [Online]. Available: https://panopticlick.eff.org/
browser-uniqueness.pdf
[44] G. Wondracek, T. Holz, E. Kirda, and C. Kruegel, “A practical attack to de-
anonymize social network users,” in IEEE Symposium on Security and Privacy,
2010.
[45] D. Irani, S. Webb, K. Li, and C. Pu, “Large online social footprints–an emerging
threat,” in International Conference on Computational Science and Engineering,
2009.
[46] D. Perito, C. Castelluccia, M. A. Kˆaafar, and P. Manils, “How unique and
traceable are usernames?” in PETS, 2011, pp. 1–17.
[47] A. K. Jain, A. Ross, and S. Prabhakar, “An introduction to biometric recogni-
tion,” IEEE Trans. on Circuits and Systems for Video Technology, vol. 14, pp.
4–20, 2004.
[48] L. Wang, T. Tan, S. Member, H. Ning, and W. Hu, “Silhouette analysis-based gait
recognition for human identiﬁcation,” IEEE Transactions on Pattern Analysis
and Machine Intelligence, vol. 25, no. 12, pp. 1505–1518, 2003.
[49] R. Plamondon and S. N. Srihari, “On-line and off-line handwriting recognition:
A comprehensive survey,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 22, pp.
63–84, January 2000.
[50] F. Monrose and A. D. Rubin, “Keystroke dynamics as a biometric for authen-
tication,” Future Generation Computer Systems, vol. 16, no. 4, pp. 351–359,
2000.
[51] L. Sweeney, “k-anonymity: A model for protecting privacy,” Ieee Security And
Privacy, vol. 10, no. 5, pp. 557–570, 2002.
[52] A. Narayanan and V. Shmatikov, “Robust de-anonymization of large sparse
datasets,” in IEEE Symposium on Security and Privacy, 2008, pp. 111–125.
[53] R. Leiby, “The Hill’s sex diarist reveals all (well, some),” The Washington Post,
May 204.
[54] K. Burton, A. Java, and I. Soboroff, “The ICWSM 2009 Spinn3r dataset,” in
International AAAI conference on weblogs and social media, 2009.
[55] D. Klein and C. D. Manning, “Accurate unlexicalized parsing,” in Proceedings
of the 41st Meeting of the Association for Computational Linguistics, 2003, pp.
423–430.
[56] H. Baayen, H. van Halteren, and F. Tweedie, “Outside the cave of shadows: using
syntactic annotation to enhance authorship attribution,” Lit Linguist Computing,
vol. 11, no. 3, pp. 121–132, Sep. 1996.
[57] M. Gamon, “Linguistic correlates of style: authorship classiﬁcation with deep
linguistic analysis features,” in Proceedings of the 20th international conference
on Computational Linguistics, ser. COLING ’04.
Stroudsburg, PA, USA:
Association for Computational Linguistics, 2004.
[58] G. Forman, “An extensive empirical study of feature selection metrics for text
classiﬁcation,” Hewlett-Packard Labs, Tech. Rep., 2002.
I. G. Bernhard E. Boser and V. Vapnik, “A training algorithm for optimal margin
classiﬁers,” in COLT, 1992, pp. 144–152.
[60] C. Cortes and V. Vapnik, “Support-vector networks,” in Machine Learning,
[59]
vol. 20, no. 3, 1995, pp. 273–297.
[61] T. Joachims, “Making large-scale SVM learning practical,” in Advances in
Kernel Methods - Support Vector Learning, 1999.
314
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:49:27 UTC from IEEE Xplore.  Restrictions apply.