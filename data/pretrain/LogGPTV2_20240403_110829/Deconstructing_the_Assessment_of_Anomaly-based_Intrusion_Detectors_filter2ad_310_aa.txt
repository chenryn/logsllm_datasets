title:Deconstructing the Assessment of Anomaly-based Intrusion Detectors
author:Arun Viswanathan and
Kymie Tan and
Clifford Neuman
Deconstructing the Assessment
of Anomaly-based Intrusion Detectors(cid:2)
Arun Viswanathan1, Kymie Tan2, and Cliﬀord Neuman1
1 USC/Information Sciences Institute
2 Jet Propulsion Laboratory, California Institute of Technology
Abstract. Anomaly detection is a key strategy for cyber intrusion de-
tection because it is conceptually capable of detecting novel attacks. This
makes it an appealing defensive technique for environments such as the
nation’s critical infrastructure that is currently facing increased cyber
adversarial activity. When considering deployment within the purview
of such critical infrastructures it is imperative that the technology is
well understood and reliable, where its performance is benchmarked on
the results of principled assessments. This paper works towards such an
imperative by analyzing the current state of anomaly detector assess-
ments with a view toward mission critical deployments. We compile a
framework of key evaluation constructs that identify how and where cur-
rent assessment methods may fall short in providing suﬃcient insight
into detector performance characteristics. Within the context of three
case studies from literature, we show how error factors that inﬂuence the
performance of detectors interact with diﬀerent phases of a canonical
evaluation strategy to compromise the integrity of the ﬁnal results.
Keywords: Anomaly-based Intrusion Detection, Anomaly Detector Eval-
uation, Error Taxonomy.
1
Introduction
Anomaly-based intrusion detection has been a consistent topic of research since
the inception of intrusion detection with Denning’s paper in 1987 [1]. As at-
tacks continue to display increasing adversarial sophistication and persistence,
(cid:2) This material is based upon work supported by the United States Department of En-
ergy under Award Number DE-OE000012 and the Los Angeles Department of Water
and Power and the Jet Propulsion Laboratory Internal Research and Technology De-
velopment Program, in part through an agreement with the National Aeronautics
and Space Administration. Neither the United States Government, the Los Angeles
Department of Water and Power, nor any agency or employees thereof, make any
warranty, express or implied, or assume legal liability or responsibility for the accu-
racy, completeness, or usefulness of any information, apparatus, product, or process
disclosed, nor that its use would not infringe privately owned rights. The views and
opinions of authors expressed herein do not necessarily reﬂect those of the sponsors.
Figures and descriptions are provided by the authors and used with permission.
S.J. Stolfo, A. Stavrou, and C.V. Wright (Eds.): RAID 2013, LNCS 8145, pp. 286–306, 2013.
c(cid:2) Springer-Verlag Berlin Heidelberg 2013
Deconstructing the Assessment of Anomaly-based Intrusion Detectors
287
anomaly-based intrusion detection continues to appeal as a defensive technique
with the potential to address zero-day exploits or “novel” adversarial tactics.
However, for anomaly-based intrusion detectors to become a viable option in
mission critical deployments such as the primary control loops for a power grid,
or the command system for spacecraft we need to know precisely when these
detectors can be depended upon and how they can fail. Such precision is partic-
ularly important when considering that the outputs of anomaly detectors are the
basis for higher-level functions such as situational awareness/correlation engines
or downstream diagnosis and remediation processes. Errors in detection output
will inevitably propagate to exacerbate errors in the outputs of such higher-level
functions, thus compromising their dependability.
Building dependable technology requires rigorous experimentation and eval-
uation procedures that adhere to the scientiﬁc method [2, 3]. Previous research
has identiﬁed the lack of rigorous and reliable evaluation strategies for assess-
ing anomaly detector performance as posing a great challenge with respect to
its dependability and its subsequent adoption into real-world operating environ-
ments [3–6]. We strongly subscribe to these statements and underscore the need
to delve into the mechanics of an evaluation strategy in a way that enables us to
better identify what went wrong as well as to understand how the results may
have been compromised.
Objectives and Contributions. Our objectives in this paper are two-fold: we
ﬁrst explore a critical aspect of the evaluation problem, namely the error factors
that inﬂuence detection performance (Sect. 3), and then present a framework of
how these error factors interact with diﬀerent phases of a detector evaluation
strategy (Sect. 4). The factors are mined from the literature and compiled into
a single representation to provide a convenient basis for understanding how er-
ror sources inﬂuence various phases in an anomaly detector evaluation regime.
Although these factors have been extensively studied in the literature our ap-
proach for discussing them oﬀers two advantages: (a) it allows visualization of
how errors across diﬀerent phases of the evaluation can compound and aﬀect
the characterization of an anomaly detector’s performance, and (b) it provides a
simple framework to understand the evaluation results, such as answering why a
detector detected or missed an attack?, by tracing the factors backwards through
the evaluation phases. In addition, as discussed further in Sect. 2, we also intro-
duce a new error factor, that has not as yet appeared in the literature, namely
the stability of attack manifestation. We use the error taxonomy to build a frame-
work for analyzing the validity and consistency arguments of evaluation results
for an anomaly detector (Sect. 4).
Using the frameworks described in Sect. 3 and Sect. 4, we then focus on an-
alyzing three case studies (Sect. 5) consisting of evaluation strategies selected
from the literature, to identify a) the “reach” of the presented results, i.e., what
can or cannot be concluded by the results with respect to, for example, external
validity, and b) experimental omissions or activities that introduce ambiguity
thereby compromising the integrity of the results, e.g., an inconsistent appli-
cation of accuracy metrics. In doing so, we will not only be better informed
288
A. Viswanathan, K. Tan, and C. Neuman
regarding the real conclusions that can be drawn from published results, but
also on how to improve the concomitant evaluation strategy.
2 Background
The purpose of an evaluation is to gain insight into the workings of a detector. As
Sommer and Paxson [5] state – a sound evaluation should answer the following
questions: (a) What can an anomaly detector detect?, (b) Why can it detect?,
(c) What can it not detect? Why not?, (d) How reliably does it operate?, and (e)
Where does it break?. In addition to these questions we would also add (f) Why
does it break?. We observe that in literature, the preponderance of evaluation
strategies for anomaly detectors focus on the “what” questions, speciﬁcally, what
can the detector detect. The “why” questions however, are rarely, if ever, an-
swered. For example, Ingham et al. [7] evaluated the performance of six anomaly
detection techniques over four diﬀerent datasets. A striking detail of their work
lies in their evaluation of “character distribution-based” detectors over the four
datasets which resulted in a 40% true positive rate (low performance) for one
of the datasets as compared to a ≥70% true positive rate for the remaining
three datasets. The authors did not clarify why that particular detection strat-
egy under-performed for one particular dataset and yet not for the other three.
If we were to consider deploying such “character distribution-based” detectors
within a mission critical operational environment, such ambiguity would increase
uncertainty and risk that would be diﬃcult to tolerate. A similar comparative
study of n-gram based anomaly detectors by Hadˇziosmanovi´c et al. [8] is a good
example of analyses that delves deeper into a speciﬁc “why” question. The au-
thors focus on thoroughly explaining the detection performance of content-based
anomaly detectors for a class of attacks over binary network protocols.
Error Factors. To answer why a detector did or did not detect an event of
interest requires a systematic understanding of the factors that can inﬂuence
a detector’s performance. It has been observed that a lack of understanding of
such factors can create systematic errors that will render experimental outcomes
inconclusive [3]. Previous studies in evaluating anomaly detectors within the
network and host-based intrusion detection space have identiﬁed several factors
inﬂuencing a detector’s performance, for example, the improper characterization
of training data [5, 9], an incorrect sampling of input data [10], the lack of ground-
truth data [5, 4, 11], poorly deﬁned threat scope [5], the incorrect or insuﬃcient
deﬁnition of an anomaly [12, 11, 13], and so forth.
Although many of the factors that contribute to error in a detector’s per-
formance are reported in the literature, they are distributed across diﬀerent
domains and contexts. Consequently, it is diﬃcult to clearly see how such er-
ror factors would integrate into and inﬂuence various phases of an evaluation
regime. Given that the objectives of this paper center on understanding how the
integrity of performance results can be compromised by the evaluation strategy,
we are motivated to compile a framework in Sect. 3 that identiﬁes the error
Deconstructing the Assessment of Anomaly-based Intrusion Detectors
289
factors that have been described in the literature and how they relate to various
phases of a canonical evaluation regime.
Stability of Attack Manifestation. The framework in Sect. 3 also refers
to an error factor that has not as yet appeared in the literature, namely the
stability of attack manifestation. Anomaly detector evaluation strategies to date
have consistently made the implicit assumption that attack signals will always
manifest in a stable manner and can thus be consistently diﬀerentiated from
normality. Consequently, when a detector is evaluated to have a 100 percent hit
rate with respect to an attack, it is only by assumption that this detection result
will persist against the speciﬁc attack. This observation is supported by the
general absence of analyses in the current literature to address the reliability of
evaluation results beyond the evaluation instance, leaving the reader to believe
that the result will remain consistent in other time instances and operational
environments. What would happen, however, should the attack change in its
manifestation due to factors present in its environment? Sensors like strace, for
example, are known to drop events under certain circumstances creating spurious
anomalous sequences that may perturb the manifestation of an attack signal [14].
While it is known that attacks can be manipulated by the adversary to hide
intentionally in normal data [15, 16], there is no study aimed at understand-
ing if the operating environment itself can induce hide-and-seek behavior in at-
tacks. In current evaluation approaches, if a detector does not detect an attack,
then the error (miss) is typically attributed to the detector from the evaluator’s
standpoint. However, this may be an incorrect attribution. Consider the scenario
where the attack signal has somehow been perturbed by the environment caus-
ing its manifestation to “disappear” from the purview of a detector. In such a
circumstance, it would not be accurate to attribute the detection failure to the
detector – there was nothing there for the detector to detect. In this case the
“miss” should more appropriately be attributed to the experimental design, i.e.,
a failure to control for confounding events.
3 Factors Contributing to Anomaly Detection Errors
In this section, we present a compilation of factors that have been identiﬁed as
sources of error in the literature. Our objective is not to present a comprehensive
taxonomy but rather to provide a unifying view of such factors to better support
a discussion and study of the evaluation problem. We scope our discussion in this
section by focusing on evaluation factors relevant to anomaly detectors that: a)
work in either the supervised, semi-supervised or unsupervised modes [17], and
b) learn the nominal behavior of a system by observing data representing normal
system activity, as opposed to detectors that are trained purely on anomalous
activity. We also focus on accuracy metrics, namely the true positives (TP), false
positives (FP), false negatives (FN) and true negatives (TN), rather than other
measures of detector performance such as speed and memory.
290
A. Viswanathan, K. Tan, and C. Neuman
)DFWRUVFRQWULEXWLQJWR
GDWDFROOHFWLRQHUURUV

'&'DWDJHQHUDWLRQ
'&'DWDPRQLWRULQJ
'&'DWDUHGXFWLRQ
'&'DWDFKDUDFWHUL]DWLRQ
'&$YDLODELOLW\RI³JURXQGWUXWK´
'&)DOVHDODUPFKDUDFWHUL]DWLRQ


)DFWRUVFRQWULEXWLQJWR
GDWDSUHSDUDWLRQHUURUV
'3'DWDVDQLWL]DWLRQ
'3'DWDSDUWLWLRQLQJ
'3'DWDFRQGLWLRQLQJ
7UDLQLQJ
'DWD

'DWD
&ROOHFWLRQ
(YDOXDWLRQ
'DWD
1RUPDO	
$WWDFNGDWD

'DWD
3UHSDUDWLRQ

7UDLQLQJ	
7XQLQJ
0RGHOV
)DFWRUVFRQWULEXWLQJWR
WUDLQLQJHUURUV
75&KDUDFWHULVWLFVRIWUDLQLQJGDWD
75'HWHFWRULQWHUQDOV
755HSUHVHQWDWLRQRIUHDOZRUOG
EHKDYLRULQGDWD
756WDELOLW\RIQRUPDOGDWD
75$WWDFNIUHHWUDLQLQJGDWD
75&KRLFHRIGDWDIHDWXUHV
750RGHOLQJIRUPDOLVP
75/HDUQLQJSDUDPHWHUV
752QOLQHYVRIIOLQHOHDUQLQJ
75$PRXQWRIWUDLQLQJ
750RGHOJHQHUDWLRQDSSURDFK
)DFWRUVFRQWULEXWLQJWR
WHVWLQJHUURUV
76&KDUDFWHULVWLFVRIWHVWGDWD
765DWLRRIDWWDFNWRQRUPDOVDPSOHV
766WDELOLW\RIDWWDFNPDQLIHVWDWLRQ
76$GYHUVDU\LQGXFHGLQVWDELOLW\
76(QYLURQPHQWLQGXFHGLQVWDELOLW\
76'HWHFWLRQSDUDPHWHUV
766LPLODULW\VFRULQJPHWULF
76'HWHFWRULQWHUQDOV
7HVW
'DWD

7HVWLQJ

'HWHFWRU
$OHUWV
0HDVXUHPHQW
3HUIRUPDQFH
7371)3)1




)DFWRUVFRQWULEXWLQJWR
PHDVXUHPHQWHUURUV
06'HILQLWLRQRIPHWULFV
06'HILQLWLRQRIDQRPDO\
Fig. 1. Factors contributing to errors across the ﬁve diﬀerent phases of an anomaly
detector’s evaluation process
In Fig. 1, we represent the typical evaluation process of an anomaly-based