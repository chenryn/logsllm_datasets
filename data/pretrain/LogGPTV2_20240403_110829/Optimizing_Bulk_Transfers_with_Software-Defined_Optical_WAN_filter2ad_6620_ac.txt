luv.capacity ← luv.capacity − θ
lpq.capacity ← lpq.capacity − θ
lup.capacity ← lup.capacity + θ
lvq.cacacity ← lvq.capacity + θ
return s
Simulated Annealing (Algorithm 1): The algorithm uses
the current topology G as the initial state, the current through-
put as the initial temperature (line 2-3). s∗ is used to store
the topology with the highest throughput and e∗ is the energy
(throughput) of s∗. The algorithm searches in the search
space (line 7-16) until temperature T is less than an ep-
silon value. T is decreased by a factor of α in every iter-
ation. At each iteration, it uses ComputeN eighbor sub-
routine to ﬁnd a neighbor state of the current state and uses
ComputeEnergy to compute the energy of the neighbor
state. If the neighbor state has a higher energy than s∗, it
updates s∗ (line 10-12). The algorithm uses a probabilistic
function P to decide whether to transition from the current
state to the neighbor state. The probabilistic function P is
deﬁned as follows: if the neighbor state has a higher energy
than the current state, the probability is 1; otherwise, the
probability is e(ecurrent−eneighbor)/T .
ComputeNeighbor (Algorithm 2): This subroutine ﬁnds a
neighbor state of the current state. It ﬁrst randomly selects
two links from E, say euv, epq. Then it decreases the capac-
ity of the selected two links by θ while increasing the capac-
ity of eup, evq by θ. In other words, it moves the capacity
from epq and euv to eup and evq by reconﬁguring the optical
links. This procedure ensures the total number of ports used
on each router remains unchanged.
ComputeEnergy (Algorithm 3): This function computes
the total throughput that can be achieved on the given state
s, where s is a network-layer topology. The computation is
divided into two steps. The ﬁrst step is to establish multiple
optical circuits for each link (line 2-14) based on its desired
capacity, and the second step is to assign routing paths and
rates to the ﬂows based on the topology (line 15-25).
In the ﬁrst step, we have constraints 2-4 in the problem
formulation to affect whether an optical circuit can be estab-
lished for a link. We use a regenerator graph to help us com-
pute an optical circuit under these constraints. The nodes
in a regenerator graph contain the source site, the destina-
tion site, and the sites that have remaining regenerators. We
create an edge in the regenerator graph if the shortest paths
between two sites is no longer than η. Figure 5(a) shows a
regenerator graph. If the source and the destination are di-
rectly connected in the graph, we can directly establish an
optical circuit; otherwise, they have to use regenerators in
the intermediate sites. We want to balance the consumption
of regenerators in different sites to improve the possibility
that a later optical circuits can ﬁnd an available one to use.
To do this, we assign a weight to each node with the inverse
of their remaining regenerators; the source and the destina-
91
Algorithm 3 Compute Energy
1: function ComputeEnergy(s)
// build optical circuits for each link
for network link l ∈ s.links do
Build regenerator graph RG
Build transformed graph T G
P ← T G.sortedP athsByLength(l.src, l.dst)
c ← l.capacity
for path p in P do
if p.canBeBuilt() then
Build circuit p for l
c ← c − θ
if c  0 then
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
Decrease the cacacity of l by c
// assign routing paths and rates
throughput ← 0
Sort transfers F by policy // e.g., SJF, EDF
l ← 1
while (there exists unsatisﬁed demand
and there exists free network capacity) do
for transfer f ∈ F do
for path p ∈ paths of f with length l do
min_c ← mine∈p e.remain_capacity
rf,p ← min(f.demand, min_c)
throughput+ = rf,p
l ← l + 1
return throughput
tion nodes are assigned with weight zero. Then the problem
is to ﬁnd a path with smallest total node weight in the regen-
erator graph. This problem can be transformed to the short-
est path problem in a directed graph. The transformation
ﬁrst builds a transformed graph from the regenerator graph.
The transformed graph has the same nodes as the regener-
ator graph. An undirected edge in the regenerator graph is
replaced by two directed edges; the weight of an edge is set
to be the weight of the node the edge points to. It is easy
to prove that the shortest path (the path with the smallest
total edge weight) in the transformed graph corresponds to
the path with smallest total node weight in the regenerator
graph. Figure 5(b) illustrates the transformed graph of Fig-
ure 5(a). After we have the transformed graph, we iterate the
paths based on path length to ﬁnd enough number of paths
we need that can be built as optical circuits (line 7-12). Line
8-12 check whether there are available wavelengths on the
path to use, and build the circuit if so. If there are not enough
possible optical circuits to satisfy all the desired capacity, we
have to decrease the link capacity (line 13-14).
For the second step, we assign paths and rates to each
transfer based on the topology to optimize their comple-
tion times or deadlines met. The problem is known to be
hard. Even if the topology is non-blocking and only the
ingress and egress ports are bottlenecks, it is NP-hard to
compute rate allocations to achieve the minimum average
transfer completion time [16]. It is also NP-hard to maxi-
mize the number of transfers that can be ﬁnished before the
deadlines, when the network is ﬁxed and three or more dis-
tinct deadlines are present [18]. A good approximation algo-
92
Figure 5: Example of regenerator graph.
rithm is to route transfers based on the order of the remain-
ing transfer size or the deadline. However, in our scenario,
the network is not ideal and we need multi-path routing to
route some transfers. We approximate the optimal result by
using the same ordering of transfers and prioritizing trans-
fers to use shorter paths ﬁrst. We order transfers with classic
scheduling policies like shortest job ﬁrst (SJF) and earliest
deadline ﬁrst (EDF) (line 16). At each iteration, we only
schedule transfers to use paths with length l (line 18-25).
At each iteration, we assign rates to each transfer based on
its demand and network capacity (line 22-23). Line 24 up-
dates the total throughput. To avoid starvation, we schedule
a transfer if it is not scheduled for ˆt (conﬁgurable) time slots,
which we omit in the algorithm for brevity.
3.3 Updating Network State
After we compute the network state, we need to update the
device conﬁgurations to the new state. Without careful up-
date scheduling, there can be loops and routing blackholes
during the update process. For example, if some packets
were sent over a link with the underlying circuit being up-
dated, these packets would be dropped. We need to be espe-
cially careful when updating the optical links as it can take
several seconds. Dionysus is a recent solution on consistent
network updates [19]. Dionysus builds a dependency graph
to capture the dependencies between individual update op-
erations and carefully schedules them to make the update
fast and consistent. But Dionysus only handles network-
layer updates and is not sufﬁcient to handle cross-layer up-
dates. To solve this problem, we extend Dionysus by in-
troducing circuit nodes into its dependency graph. Circuit
nodes have dependencies on ﬁbers as creating a circuit con-
sumes a wavelength and removing a circuit frees a wave-
length; circuit nodes also have dependencies on routing paths
as a routing path cannot be used until circuits for all links
on the path are established. After we build the dependency
graph, we use the same scheduling algorithm as Dionysus to
schedule the update operations.
3.4 Handling Practical Issues
Network failures: Link and switch failures are detected and
sent to the controller. The controller removes these links and
switches from the physical network, and recomputes the net-
work state with the updated physical network. As our algo-
rithm minimizes the amount of updates needed, it is likely to
converge to a new feasible schedule with only incremental
updates to avoid the failed links.
Controller Failures: Since the scheduling algorithm is state-
less, we only need to store the physical network and the set
O0 O1 O2 O3 O4 O0 O1 O2 O3 O4 (a) Regenerator graph. (b) Transformed graph. 0.2 0.25 1 0.2 0.25 1 0.2 0.25 1 0 0 0 0 0 0 lengths in the nine ﬁbers sum up to n. This means that in the
network-layer topology, each router can have any number
of links to any other router as long as the total number of
links adjacent to a router satisfy the router port constraint.
Therefore, our testbed can faithfully emulate the Internet2
network since the testbed is able to construct any network-
layer topology that the Internet2 network is able to construct.
Figure 6 depicts the internal structure of our ROADM. For
the outward direction of a ROADM, the n wavelengths from
n transceivers are multiplexed by a multiplexer (MUX) on
to a single ﬁber. Then the splitter replicates them and sends
them to eight other ROADMs. For the inward direction, a
Wavelength Selective Switch (WSS) receives n wavelengths
from each neighbor and selects up to n different wavelengths
from the input wavelengths. Then an Erbium-Doped Fiber
Ampliﬁer (EDFA) is used to amplify the wavelengths se-
lected by the WSS, in order to compensate signal loss. Fi-
nally, a demultiplexer (DEMUX) demultiplexes the selected
wavelengths and send them to corresponding ports. The
MUXes and DEMUXes are the same type of device (Oplink
AAWG) with different conﬁgurations.
To transmit packets from one router to another, the optical
signal passes through multiple optical elements, including
MUX, splitter, ﬁber, WSS and DEMUX. These ﬁve elements
introduce typical optical power loss of 5 dB, 10.5 dB, 0.5 dB,
7 dB, and 5 dB, respectively. The total optical power loss is
∼28 dB, which is higher than the optical power budget (∼16
dB) of the transceivers. That is the reason to put an EDFA
between WSS and DEMUX. The EDFA is set to operate at
ﬁxed gain mode, and has a default setting of gain parameter
of 18 dB to compensate the optical power loss.
4.2 Owan Software Implementation
The Owan controller is implemented with 5000+ lines of
Java code and uses several third-party software and libraries.
It has the following four modules.
Core module: The core module implements the algorithms
in §3. We have implemented the blossom algorithm [22]
for maximum matching in general graphs and used JGraphT
library [23] for other graph algorithms.
Router module: We conﬁgure the Arista switches to work
in OpenFlow 1.0 mode. We use the Floodlight controller [24]
to handle the details of the OpenFlow protocol and interface
with the switches. The router module uses the RESTful API
exposed by the Floodlight controller to install routing rules.
ROADM module: The Freescale i.MX53 micro controller
in each ROADM handles the low level conﬁgurations, mon-
itors the optical elements, and exposes a simple API for re-
mote conﬁguration. The ROADM module uses this API to
conﬁgure each ROADM.
Client module: The client module sends the rate allocation
of each transfer to the end hosts. Since a transfer may use
multiple paths, we break a transfer into multiple ﬂows and
use preﬁx splitting to implement multi-path routing. The
client module conﬁgures Linux Trafﬁc Control on each end
host to enforce rates.
Figure 6: Owan testbed implementation.
of all transfers with a reliable distributed storage. When the