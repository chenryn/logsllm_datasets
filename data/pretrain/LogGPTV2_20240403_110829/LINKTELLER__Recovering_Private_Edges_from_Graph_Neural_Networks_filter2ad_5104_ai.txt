[16] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V. K. Prasanna,
“Graphsaint: Graph sampling based inductive learning method.” in
ICLR. OpenReview.net, 2020. [Online]. Available: http://dblp.uni-trier.
de/db/conf/iclr/iclr2020.html#ZengZSKP20
[17] K. Xu, C. Li, Y. Tian, T. Sonobe, K.-i. Kawarabayashi, and S. Jegelka,
“Representation learning on graphs with jumping knowledge networks,”
in International Conference on Machine Learning. PMLR, 2018, pp.
5453–5462.
[18] C. Dwork and A. Roth, The Algorithmic Foundations of Differential
Privacy, 2014. [Online]. Available: https://www.cis.upenn.edu/∼aaroth/
Papers/privacybook.pdf
[19] V. Karwa, S. Raskhodnikova, A. Smith, and G. Yaroslavtsev, “Private
analysis of graph structure,” ACM Trans. Database Syst., vol. 39, no. 3,
Oct. 2014. [Online]. Available: https://doi.org/10.1145/2611523
[20] S. P. Kasiviswanathan, K. Nissim, S. Raskhodnikova, and A. Smith,
“Analyzing graphs with node differential privacy,” in Theory of Cryp-
tography Conference. Springer, 2013, pp. 457–476.
[21] Z. Qin, T. Yu, Y. Yang, I. Khalil, X. Xiao, and K. Ren, “Generating
synthetic decentralized social graphs with local differential privacy,” in
Proceedings of the 2017 ACM SIGSAC Conference on Computer and
Communications Security, 2017, pp. 425–438.
[22] G. Sood, clarifai: R Client for the Clarifai API, 2017, r package version
0.4.2.
[23] “Vision ai — derive image insights via ml — cloud vision api,” https:
//cloud.google.com/vision, (Accessed on 08/01/2021).
[24] Y. M¨ulle, C. Clifton, and K. B¨ohm, “Privacy-integrated graph clustering
through differential privacy,” CEUR Workshop Proceedings, vol. 1330,
pp. 247–254, 01 2015.
[25] A. Sala, X. Zhao, C. Wilson, H. Zheng, and B. Y. Zhao, “Sharing graphs
using differentially private graph models,” in Proceedings of the 2011
ACM SIGCOMM conference on Internet measurement conference, 2011,
pp. 81–98.
[26] Y. Wang and X. Wu, “Preserving differential privacy in degree-
correlation based graph generation,” vol. 6, no. 2, p. 127–145, Aug.
2013.
[27] Q. Xiao, R. Chen, and K.-L. Tan, “Differentially private network data
the 20th ACM
release via structural
SIGKDD international conference on Knowledge discovery and data
mining, 2014, pp. 911–920.
inference,” in Proceedings of
[28] S. Brunet, S. Canard, S. Gambs, and B. Olivier, “Novel differentially
private mechanisms for graphs.” IACR Cryptology ePrint Archive,
vol. 2016, p. 745, 2016. [Online]. Available: http://dblp.uni-trier.de/db/
journals/iacr/iacr2016.html#BrunetCGO16
[29] B. Rozemberczki, C. Allen, and R. Sarkar, “Multi-scale attributed node
embedding,” 2019.
right,”
in Advances
[30] P. Flach and M. Kull, “Precision-recall-gain curves: Pr analysis
Information Processing
done
Systems, C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and
R. Garnett, Eds., vol. 28. Curran Associates, Inc., 2015, pp. 838–
846. [Online]. Available: https://proceedings.neurips.cc/paper/2015/ﬁle/
33e8075e9970de0cfea955afd4644bb2-Paper.pdf
in Neural
[31] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Ben-
gio, “Graph attention networks,” arXiv preprint arXiv:1710.10903, 2017.
[32] E. Zheleva and L. Getoor, “Preserving the privacy of sensitive relation-
ships in graph data,” in Proceedings of the First SIGKDD International
Workshop on Privacy, Security, and Trust in KDD (PinKDD 2007), ser.
Lecture Notes in Computer Science, vol. 4890. Springer, March 2007,
pp. 153–171.
[33] M. Hay, G. Miklau, D. Jensen, P. Weis, and S. Srivastava, “Anonymizing
social networks,” University of Massachusetts Amherst, Tech. Rep. 07-
19, March 2007.
[34] L. Zhang and W. Zhang, “Edge anonymity in social network graphs.” in
IEEE Computer Society, 2009, pp. 1–8. [Online]. Available:
CSE (4).
http://dblp.uni-trier.de/db/conf/cse/cse2009-4.html#ZhangZ09
[35] A. M. Fard, K. Wang, and P. S. Yu, “Limiting link disclosure in social
network analysis through subgraph-wise perturbation.” in EDBT, E. A.
Rundensteiner, V. Markl, I. Manolescu, S. Amer-Yahia, F. Naumann,
and I. Ari, Eds. ACM, 2012, pp. 109–119. [Online]. Available:
http://dblp.uni-trier.de/db/conf/edbt/edbt2012.html#FardWY12
[36] V. Duddu, A. Boutet, and V. Shejwalkar, “Quantifying privacy leakage
in graph embedding,” 2020.
[37] A. Bojchevski and S. G¨unnemann, “Adversarial attacks on node em-
beddings via graph poisoning,” in International Conference on Machine
Learning. PMLR, 2019, pp. 695–704.
[38] H. Dai, H. Li, T. Tian, X. Huang, L. Wang, J. Zhu, and L. Song, “Ad-
versarial attack on graph structured data,” in International conference
on machine learning. PMLR, 2018, pp. 1115–1124.
[39] N. Li, W. Qardaji, D. Su, Y. Wu, and W. Yang, “Membership privacy:
A unifying framework for privacy deﬁnitions,” in Proceedings of the
2013 ACM SIGSAC Conference on Computer and Communications
Security, ser. CCS ’13. New York, NY, USA: Association for
Computing Machinery, 2013, p. 889–900. [Online]. Available: https:
//doi.org/10.1145/2508859.2516686
[40] S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha, “Privacy risk in
machine learning: Analyzing the connection to overﬁtting,” 2018 IEEE
31st Computer Security Foundations Symposium (CSF), pp. 268–282,
2018.
[41] B. Jayaraman and D. Evans, “Evaluating differentially private machine
learning in practice,” in 28th USENIX Security Symposium (USENIX
Security 19).
Santa Clara, CA: USENIX Association, Aug. 2019,
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:20:38 UTC from IEEE Xplore.  Restrictions apply. 
2018
[42]
pp. 1895–1912. [Online]. Available: https://www.usenix.org/conference/
usenixsecurity19/presentation/jayaraman
´U. Erlingsson, I. Mironov, A. Raghunathan, and S. Song, “That which
we call private,” CoRR, vol. abs/1908.03566, 2019. [Online]. Available:
http://arxiv.org/abs/1908.03566
[43] B. Jayaraman, L. Wang, K. Knipmeyer, Q. Gu, and D. Evans, “Revisiting
membership inference under realistic assumptions,” 2020.
[44] T. Humphries, M. Rafuse, L. Tulloch, S. Oya, I. Goldberg, and F. Ker-
schbaum, “Differentially private learning does not bound membership
inference,” 2020.
[45] M. Hay, C. Li, G. Miklau, and D. Jensen, “Accurate estimation of the
degree distribution of private networks,” in 2009 Ninth IEEE Interna-
tional Conference on Data Mining.
IEEE, 2009, pp. 169–178.
[46] J. Blocki, A. Blum, A. Datta, and O. Sheffet, “The johnson-lindenstrauss
transform itself preserves differential privacy,” in 2012 IEEE 53rd An-
nual Symposium on Foundations of Computer Science.
IEEE, 2012,
pp. 410–419.
[47] ——, “Differentially private data analysis of social networks via re-
stricted sensitivity,” in Proceedings of the 4th conference on Innovations
in Theoretical Computer Science, 2013, pp. 87–96.
[48] F. D. McSherry, “Privacy integrated queries: an extensible platform
for privacy-preserving data analysis,” in Proceedings of the 2009 ACM
SIGMOD International Conference on Management of data, 2009, pp.
19–30.
[49] F. Wu, Y. Long, C. Zhang, and B. Li, “Linkteller: Recovering pri-
vate edges from graph neural networks via inﬂuence analysis,” https:
//aisecure.github.io/PUBLICATIONS/ﬁles/LinkTeller.pdf.
[50] Q. Li, Z. Han, and X.-M. Wu, “Deeper insights into graph convolutional
networks for semi-supervised learning,” in Proceedings of the AAAI
Conference on Artiﬁcial Intelligence, vol. 32, no. 1, 2018.
APPENDIX
A. Proofs for Inﬂuence Analysis in LINKTELLER
1) Proof of Proposition 1:
Proof. Consider the 1-layer GCN, where
GCN(A, X, W ) = AXW.
(3)
For the simplicity of the proof, we ignore the normalization
applied to the adjacency matrix A, since it only changes the
scale of numbers in the matrix. We next calculate the inﬂuence
value of a pair of nodes against this 1-layer GCN according
to the function InfluenceValue described in Algorithm 1.
Following the notation in Algorithm 1, let the set of infer-
ence nodes be V (I), the node feature matrix associated with
the inference node set be X. We further denote the adjacency
matrix induced on this node set as A. Taking a pair of nodes
u, v from the set of nodes of interest V (C), we calculate the
inﬂuence value of this pair of nodes following the steps in
InfluenceMatrix:
(cid:104)
P = GBB(V I , X) = GCN(A, X, W ) = AXW,
(cid:48)
(cid:62)
1 , . . . , (1 + ∆)x
(cid:62)
v , . . . ,
=
x
X
(cid:48)
P
= GBB(V I , X
(cid:48)
) = GCN(A, X
, W ) = AX
(cid:48)
W,
(cid:105)(cid:62)
,
(cid:48)
(cid:48) − P = A(X
(cid:48) − X)W = AX∆W,
P
(cid:104)
where X∆ is a matrix of the same size as X that only contains
value in each v-th row. Speciﬁcally, the v-th row vector of X∆
is equal to ∆xv, where ∆ is the reweighting coefﬁcient.
AX∆W . We start from computing X∆W
We next compute the u-th row of the inﬂuence matrix I =
(cid:105)(cid:62)
(cid:104)
X∆W =
W = ∆
Therefore, for I deﬁned as I = (P (cid:48)
− P )/∆ = AX∆W , its
u-th row is AuvxvW . When there is no edge between u and
v, Auv = 0, and therefore the row vector is an all zero vector.
(cid:62)
v , . . . , 0
(cid:62)
v , . . . , 0
0, . . . , ∆x
(cid:105)(cid:62)
0, . . . , x
W
The inﬂuence value, which is the (cid:96)2 norm of the row vector,
is therefore 0.
2) Proof of Theorem 1: We ﬁrst present a lemma.
Lemma 2. Let A ∈ {0, 1}n×n be the adjacency matrix of the
i ∈ Rn×di be two hidden feature matrices of size
graph, Hi, H(cid:48)
that differ in ti rows {r1, . . . , rti} corresponding to the nodes
{vr1 , . . . , vrti}, and W i ∈ Rdi×di+1 be the weight matrix of
responding to the node set (cid:83)ti
the i-th graph convolutional layer, then
(1) AHiW i and AH(cid:48)
iW i differ in at most ti+1 rows cor-
l=1 N (vrl ), where N (u)
ti+1 rows corresponding to the node set(cid:83)ti
i+1 =
i+1 also differ in at most
l=1 N (vrl).
We next use Lemma 2 to help with the proof of Theorem 1.
Proof. Consider a k-layer GCN which is a stack of k graph
convolutional layers deﬁned as below
denotes the neighbor set of node u.
(2) Further,
σ(AH(cid:48)
let Hi+1 = σ(AHiW i) and H(cid:48)
iW i), then Hi+1 and H(cid:48)
k−1W k−1.
Since H0 and H(cid:48)
GCN(A, X,{W i}) = A··· σ(Aσ(AXW 1)W 2)··· W k.
2 differ in the rows corresponding to(cid:83)
(4)
Its input feature matrices are H0 = X and H(cid:48)
0 = X(cid:48) that
differ in one row; let it correspond to node u. Its output feature
matrices are AHk−1W k−1 and AH(cid:48)
0 differ only in node u, according
to Lemma 2-(2), H1 and H(cid:48)
1 differ in the rows corresponding
to N (u) (which contains nodes that are 1 hop away from u);
H2 and H(cid:48)
v∈N (u) N (v)
(which contains nodes that are at most 2 hops away from u).
Iteratively applying Lemma 2-(2), we see that Hk−1 and H(cid:48)
k−1
differ in rows corresponding to nodes that are at most k − 1
hops away from u. We ﬁnally apply Lemma 2-(1) and obtain
k−1W k−1 differ in
the conclusion that AHk−1W k−1 and AH(cid:48)
rows corresponding to nodes that are at most k hops away
from u. Thus, the inﬂuence matrix
P∆ = AH
(cid:48)
k−1W k−1 − AHk−1W k−1
Finally, we complete the proof for Lemma 2.
has at most tk non-zero rows corresponding to nodes that are
at most k hops away from u. It thus follows that when u and
v are at least k + 1 hops away, the v-th row of the inﬂuence
matrix of node u is an all-zero row. Since the inﬂuence value
of u on v is the norm of the v-th row, we thus can conclude
that the inﬂuence value (cid:107)ivu(cid:107) = 0.
Proof. First of all, for Hi and H(cid:48)
obvious that HiW i and H(cid:48)
simplicity, we denote HiW i as Fi and H(cid:48)
iW i as F (cid:48)
present the condition for AFi and AF (cid:48)
element in j-th row and k-th column of AFi and AF (cid:48)
i that differs in ti rows, it is
iW i differ in the same ti rows. For
i . We next
p=1 Ajp(Fi)pk and (cid:80)n
are (cid:80)n
i to differ. Consider the
i , which
pk, respectively. We
ﬁrst note that when Ajp = 0, the difference of (Fi)pk and
(Fi)(cid:48)
pk does not matter. Next, for all p such that Ajp = 1,
(cid:83)ti
only when (Fi)pk (cid:54)= (Fi)(cid:48)
pk will the difference of the product
contribute to the difference of the sum. The two points jointly
l=1 N (vrl ), then (Fi)pk = (Fi)(cid:48)
pk for all k.
imply that, if j /∈
Thus, AFi and AF (cid:48)
i differ in at most ti+1 rows, corresponding
p=1 Ajp(Fi)(cid:48)
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:20:38 UTC from IEEE Xplore.  Restrictions apply. 
2019
to the node set (cid:83)ti
claim.
l=1 N (vrl ). Hence we establish the ﬁrst
For the second claim, we notice that the activation layer
such as ReLU used in the standard GCN [13] is point-wise