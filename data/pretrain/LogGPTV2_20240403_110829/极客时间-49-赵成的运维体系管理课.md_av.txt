## 全链路跟踪系统在技术运营层面的应用 {#27.html#-}接下来，主要分享我们利用全链路跟踪系统在技术运营层面做的一些事情，这里提到的运营，就是应用在线上运行时，我们根据应用运行数据所做的运行维护工作，这里我们会更加强调数据的作用。同时，这里的一个核心技术点就是 **TraceID**，当请求从接入层进来时，这个TraceID 就要被创建出来；或者是通过 Nginx 插件方式创建放到 http 的 header里面；或者是通过 RPC服务化框架生成。然后在后续的请求中，这个字段会通过框架自动传递到下一个调用方，而不需要业务考虑如何处理这个核心字段。有了这个TraceID，我们就可以将一个完整的请求链路给串联起来了，这也是后面场景化应用的基础。下面我们就一起来看会有哪些具体的技术运营场景。``{=html}
## **第一个场景，问题定位和排查** {#27.html#-}我们做全链路跟踪系统，要解决的首要问题就是在纷繁复杂的服务调用关系中快速准确地定位问题，所以这个场景是绕不开的。**我们常见的问题场景，主要有两类：瓶颈分析和异常错误定位**。首先看瓶颈分析。常见的问题就是某某页面变慢了，或者某个服务突然出现大量超时告警，因为无论是页面也好，还是服务也好，在分布式环境中都会依赖后端大量的其它服务或基础部件，所以定位类似的问题，我们期望能有一个详细的调用关系呈现出来，这样我们就可以非常方便快速地判断瓶颈出现在什么地方。比如下图的情况，就是某个页面变慢。我们根据 URL查看某次调用的情况，就发现瓶颈是在 RateReadService 的 query接口出现了严重阻塞。接下来，我们就可以根据详细的 IP地址信息，到这台机器上或者监控系统上，进一步判断这个应用或者这台主机的异常状况是什么，可能是机器故障，也可能是应用运行故障等等。 ![](Images/d1a74ae6c8a3a99cf40e82db282eaae6.png){savepage-src="https://static001.geekbang.org/resource/image/9f/37/9f4ebfd7abe8f1ee76c978492c37ca37.jpeg"}再来看一个案例。下图中我们可以看到，一次完整的请求耗时比较长，但是通过调用链分析会发现，其中任何一个单次请求的时延又非常低，并没有像上个案例那样有明显的请求瓶颈。我们再进一步分析，就会发现整个请求的列表非常长，且请求列表里面都是在访问缓存内容。很显然，这样的调用方式不合理，需要我们优化调用逻辑，要么通过批量接口方式，要么通过异步的方式，再或者要去分析业务场景是否合理。![](Images/07ebe8570378269b4322f388e507175d.png){savepage-src="https://static001.geekbang.org/resource/image/af/23/affe0cc8c37b7c84b94f85000deecf23.jpeg"}通过上面的案例，我们可以看到，**在应用了全链路跟踪的解决方案后，复杂调用关系下的问题定位就相对简单多了**。对于出现异常报错，也是一样的判断逻辑，限于篇幅我就不再赘述了。
## **第二个场景，服务运行状态分析** {#27.html#-}上面的问题定位，主要还是针对单次请求或相对独立的场景进行的。更进一步，我们在采集了海量请求和调用关系数据后，还可以分析出更有价值的服务运行信息。比如以下几类信息。**1. 服务运行质量**一个应用对外可能提供 HTTP 服务，也可能提供 RPC接口。针对这两类不同的接口，我们可以通过一段时间的数据收集形成服务接口运行状态的分析，也就是应用层的运行监控，常见的监控指标有QPS、RT和错误码，同时还可以跟之前的趋势进行对比。这样就可以对一个应用，以及对提供的服务运行情况有一个完整的视图。![](Images/ef498375f9d8d644c7b8ac6ca3cffacc.png){savepage-src="https://static001.geekbang.org/resource/image/7f/2e/7f0897395183ca37b613ab76071ccc2e.png"}**2. 应用和服务依赖**除了上述单个应用的运行状态，我们还可以根据调用链的分析，统计出应用与应用之间，服务与服务之间的依赖关系及依赖比例，如下图所示。![](Images/8e47ac0e06234d6bfd99df867bea1a0e.png){savepage-src="https://static001.geekbang.org/resource/image/a7/97/a74dc9de0192d01e732f8e3d6b2db797.png"}这个依赖管理的作用，就是给我们前面介绍的容量压测和限流降级这两个工作做好准备。我们可以根据来源依赖和比例评估单链路的扩容准备；同时根据去向依赖进行流量拆分，为下游应用的扩容提供依据，因为这个依赖比例完全来源于线上真实调用，所以能够反映出真实的业务访问模型。同时，根据这个依赖关系，特别是服务依赖关系，我们还可以进一步分析依赖间的强弱关系，也就是强弱依赖。这一点又对我们做限流降级提供了对应的依据，也就是我们前面所说的，我们限流也好，降级也好，都是优先对非核心业务的限流和降级，这样的业务形成的依赖，我们就认为是弱依赖，是不关键的；但是对于核心业务我们就要优先保障，它们形成的依赖关系，是强依赖。无论是扩容也好，还是优化性能也罢，都要最大限度地确保强依赖关系的调用成功。所以，强弱依赖的分析，还是要从业务场景入手。比如对于电商来说，核心就是交易链路，我们就要判断如果一条链路上的某个应用或服务失败了，是不是会影响订购下单，或者影响支付收钱，如果影响，就要标注为强依赖，这个应用就要标注为核心应用；如果这个应用失败了，可以通过限流或降级的方式绕过，只是影响用户体验，但是不影响用户订购支付，那这个依赖关系就可以标注为弱依赖，该应用就可以标注为非核心应用。同时，因为我们的业务场景和需求在不断变化，应用和服务间的调用关系和依赖关系也是在不断变化中的，这就需要我们不断地分析和调整强弱依赖关系，同时也要关注各种调用间的合理性，这个过程中就会有大量的可优化的工作。通常情况下，这些事情对于业务架构师和运维人员来说，都会比较关注。因为业务架构师要对业务访问模型十分了解，他要经常关注这些信息；而运维会关注线上稳定性，需要在关键时刻执行限流降级或开关预案策略，所以也必须对这些信息非常熟悉。**3. 依赖关系的服务质量**上面介绍了应用和服务间的依赖管理，同样的我们也会关注被依赖的应用或服务的实时运行状态和质量，这样就可以看到应用间实时的调用状态。是不是有的应用调用QPS 突然增加了，或者 RT 突然暴涨，通过这个依赖关系就可以快速确认。 ![](Images/cf86abe8c16b770780a13f3f6d6a7b17.png){savepage-src="https://static001.geekbang.org/resource/image/89/f3/89c3973e1b582c052c1df6387184def3.png"}
## **第三类场景，业务全息** {#27.html#-}顾名思义，业务全息就是全链路跟踪系统与业务信息的关联。从上述的介绍中，我们可以看到，全链路跟踪系统的应用更多的还是在技术层面，比如定位"应用或服务"的问题，应用或服务间的依赖关系等等。但是现实中，我们也会遇到大量的业务链路分析的场景，比如可能会有针对某个订单在不同阶段的状态等。假设一个情况是用户投诉，他的订单没有享受到满100元包邮的优惠，这时我们就要去查找用户从商品浏览、加购物车到下单整个环节的信息，来判断问题出在哪儿。其实，这个场景和一个请求的全链路跟踪非常相似。所以，为了能够在业务上也采用类似的思路，我们就将前面介绍到的请求链路上的唯一TraceID 与业务上的订单 ID、用户 ID、商品 ID等信息进行关联，当出现业务问题需要排查时，就会根据对应的 ID将一串业务链整个提取出来，然后进行问题确认。这就会极大地提升解决业务问题的效率。
## 总结 {#27.html#-}今天我们从技术运营层面的应用这个角度重新认识了全链路跟踪系统。同时，从这个案例中，我们也应该看到，技术、产品和运营相辅相成，共同促进彼此的完善和成熟。全链路跟踪系统在技术方案的广泛应用，给我们提供了大量可分析处理的线上运行数据，从这些数据中，我们又能提炼出对线上稳定运行更有价值的信息。所以，技术之外，我们也应该更多地考虑技术在价值方面的呈现。今天的内容就介绍到这里，你在这方面遇到过哪些问题，有怎样的经验，欢迎留言与我讨论。如果今天的内容对你有帮助，也欢迎你分享给身边的朋友，我们下期见！![](Images/3ef6e72a283656e2668a23a796e1acca.png){savepage-src="https://static001.geekbang.org/resource/image/60/0e/60151e9d25d6751800506e2460f5660e.jpg"}
# 37 \| 故障管理：谈谈我对故障的理解对于任何一个技术团队来说，最令人痛苦、最不愿面对的事情是什么？我想答案只有一个，那就是：故障。无论是故障发生时的极度焦虑无助，还是故障处理过程中的煎熬痛苦，以及故障复盘之后的失落消沉，都是我们不愿提及的痛苦感受。在海外，故障复盘的英文单词是Postmortem，它有另外一个意思就是验尸，想想就觉得痛苦不堪，同时还带有一丝恐怖的意味。写故障相关的文章，也着实比较痛苦。一方面回顾各种故障场景，确实不是一件令人愉悦的体验；另一方面，故障管理这个事情，跟技术、管理、团队、人员息息相关，也是一套复杂的体系。我们看 Google SRE 这本书（《SRE：Google运维解密》），绝大部分章节就是在介绍故障相关的内容。其实看看这本书就能明白稳定性和故障管理这项系统工程的复杂度了，而且从本质上讲，SRE的岗位职责在很大程度上就是应对故障。所以，接下来的几期文章，我会谈谈我对故障管理的理解，以及一些实际经历的感受，也希望我们每一个人和团队都能够在故障管理中得到涅槃重生。今天，先谈谈我们应该如何来看待故障这个事情。