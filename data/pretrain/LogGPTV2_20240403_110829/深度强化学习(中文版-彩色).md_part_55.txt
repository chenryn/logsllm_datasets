AdversarialImitationLearning，GAIL）(Hoetal.,2016)。
8.3.3 生成对抗模仿学习
生成对抗模仿学习（GenerativeAdversarialImitationLearning，GAIL）(Hoetal.,2016)采用了
生产对抗网络（GenerativeAdversarialNetworks，GANs）(Goodfellowetal.,2014)中的生成对抗方
法。相关算法可以被想成是企图引入一个对模仿者的状态-动作占用率（Occupancy）的度量，使
之与示范者的相关特性类似。它使用一个 GAN 中的辨别器（Discriminator）来给出基于示范数
据的动作-价值（ActionValue）函数估计。对于一般基于动作价值函数的强化学习过程来说，动
作-价值可以通过一种生成式方法来从示范中得到：
Q(s,a)=E T i[log(D ωi+1(s,a))], (8.11)
其中T 迭代次数为i时探索的样本集合，而D (s,a)是来自辨别器的输出值D (s,a)，辨
i ωi+1 ωi+1
别器的参数为ω 。ω 表示Q值是在更新了一步辨别器的参数过后再估计的，因此迭代次数
i+1 i+1
是i+1。辨别器的损失函数定义为一般形式：
Loss=E T i[∇ ωlog(D ω(s,a))]+E T E[∇ ωlog(1−D ω(s,a))] (8.12)
其中T ，T 分别是来自探索和专家示范的样本集合，而ω是辨别器的参数。图8.4展示了GAIL
i E
的结构。
图8.4 GAIL的结构，改编自文献(Hoetal.,2016)
265
第8章 模仿学习
通过GAIL方法，策略可以通过由示范数据泛化得到的样本进行学习，而且相比于使用IRL
的方法有较低的计算消耗。它也不需要在训练中跟专家进行交互，而像DAgger等方法可能需要
这种实际上有时难以得到的交互数据。
这种方法可以进一步推广到多模态的（Multi-Modal）策略来从多任务中学习。基于GAN的
多模态模仿学习(Hausmanetal.,2017)将一个更高级的目标函数（额外的潜在指标表示不同的任
务）用于生成对抗过程中，从而自动划分来自不同任务的示范，并以模仿学习的方法学习一个多
模态策略。
根据文献(Goodfellowetal.,2014)，如果有无限的数据和无限的计算资源，在最优情况下，以
GAIL 的目标生成的状态-动作分布应当完全匹配示范数据的状态-动作对。然而，这种方法的缺
点是，我们绕过了生成奖励的中间步骤，即我们不能从辨别器中提取奖励函数，因为D (s,a)对
ω
于所有的(s,a)将收敛到0.5。
8.3.4 生成对抗网络指导性代价学习
如上所述，GAIL方法无法从示范数据中恢复奖励函数。一个类似的工作称为生成对抗网络
指导性代价学习（GenerativeAdversarialNetworkGuidedCostLearning，GAN-GCL），它基于GAN
的结构来优化一个指导性代价学习（GuidedCostLearning，GCL）方法，以此来从使用示范数据
训练的最优辨别器中提取一个最优的奖励函数。我们将详细介绍该方法。
GAN-GCL方法（具体来说GCL部分）是基于之前介绍的最大因果熵反向强化学习方法的，
它考虑一个熵正则化马尔可夫决策过程（Markov Decision Process，MDP）。熵正则化 MDP 对
于强化学习的目标是最大化熵正则化折扣奖励的期望（Expected Entropy-Regularized Discounted
Reward）：
2 3
XT
π∗ =argmaxE τ∼π4 γt(r(S t,A t)+H(π(·|S t)))5 , (8.13)
π
t=0
这是源自式 (8.5) 的用于实际学习策略的一个具体形式。可以看出最优策略 π∗(a|s) 给出的轨
迹分布满足 π∗(a|s) ∝ exp(Q∗ (s,a)) (Ziebart et al., 2010)，其中 Q∗ (S ,A ) = r(S ,A ) +
P soft soft t t t t
E τ∼π[ T t′=tγt′−t(r(s t′,a t′)+H(π(·|s t′)))]表示柔性Q函数（SoftQ-Function），这在柔性Actor-
Critic算法中也有用到。
IRL问题可以被理解为解决如下一个极大似然估计（MaximumLikelihoodEstimation，MLE）
问题：
m θaxE τ∼πE[logp θ(τ)], (8.14)
Q
其中 π 是提供示范的专家策略，而 p θ(τ) ∝ p(S 0) T t=0p(S t+1|S t,A t)eγtrθ(St,At) 以奖励函数
E
266
8.3 逆向强化学习方法
r (s,a)的参数θ为参数，并且依赖MDP的初始状态分布和动态变化（或称状态转移）。p (τ)是
θ θ
示范数据以轨迹为中心的（Trajectory-Centric）分布，这些数据是从以状态为中心的（State-Centric）
π 得来的，即p (τ) ∼ π 。根据确定性转移过程中p(S |S ,A ) = 1，其简化为一个基于能量
E θ E t+1 t t
∑
的模型p θ(τ) ∝ e T t=0γtrθ(St,At) (Ziebartetal.,2008)。参数化的奖励函数可以按照上面的目标来
优化参数 θ。与之前的过程类似，我们在这里可以引入代价函数作为累积折扣奖励（Cumulative
P
DiscountedRewards）c =− T γtr (S ,A )的负值，它也由θ参数化。那么MaxEntIRL可以
θ t=0 θ t t
看作是使用玻尔兹曼分布（BoltzmannDistribution）在以轨迹为中心的形式下对示范数据建模的
结果，其中由代价函数c 给出的能量为
θ
1
p (τ)= exp(−c (τ)), (8.15)
θ Z θ
P
其中τ 是状态-动作轨迹，而c (τ)= c (S ,A )总的代价函数，配分函数（PartitionFunction）
θ t θ t t
Z 是 exp(−c (τ)) 对所有符合环境动态变化的轨迹的积分，用以归一化概率。对于大规模或连
θ
续空间的情况，准确估计配分函数Z 会很困难，因为通过动态规划（DynamicProgramming）对
Z 的精确估计只适用于小规模离散情况。否则我们需要使用近似估计的方法，比如基于采样的
（Sampling-Based）GCL方法。
GCL使用重要性采样（ImportanceSampling）来以一个新的分布q(τ)（原来的示范数据分布
为p(τ)）估计Z，并采用MaxEntIRL的形式：
θ∗ =argminE τ∼p[−logp θ(τ)] (8.16)
θ
=argminE τ∼p[c θ(τ)]+logZ (8.17)
θ  !
exp(−c (τ′))
=argm θinE τ∼p[c θ(τ)]+log E τ′∼q q(τθ ′) . (8.18)
其中τ′是从分布q采样得到的，而q(τ′)是其概率。因此q可以通过最小化q(τ′)和 1 exp(−c (τ′))
Z θ
间的KL散度来优化，从而更新θ以学习q(τ′)，其等价表示如下：
q∗ =minE τ∼q[c θ(τ)]+E τ∼q[logq(τ)] (8.19)
文献(Finnetal.,2016a)提出使用GAN的形式来解决上述优化问题，它使用GAN的结构优
化GCL，与GAIL方法类似但是有不同的具体形式。
注意，GAN中的辨别器也可以实现用一个分布去拟合另一个的功能：
p(τ)
∗
D (τ)= (8.20)
p(τ)+q(τ)
267
第8章 模仿学习
我们可以在这里将它用于MaxEntIRL形式的GCL。
1 exp(−c (τ))
D (τ)= Z θ (8.21)
θ 1 exp(−c (τ))+q(τ)
Z θ
这产生了GAN-GCL方法。策略π被训练以最大化R (τ)=log(1−D (τ))−logD (τ)，从而奖
θ θ θ
励函数可以通过优化辨别器来学习。策略通过更新采样分布 q(τ) 来学习，这个采样分布是用来
估计配分函数的。如果达到了最优情况，那么我们可以用所学的最优的代价函数c∗ =−R∗(τ)=
P θ θ
− T γtr∗(S ,A )来得到最优奖励函数，而最优策略可以通过π∗ = q∗ 得到。GAN-GCL为解
t=0 θ t t
决MaxEntIRL问题提供了一种除直接最大化似然（MaximumLikelihood）方法外的方法。
8.3.5 对抗性逆向强化学习
由于上面介绍的GAN-GCL是以轨迹为中心（Trajectory-Centric）的，这意味着完整的轨迹需
要被估计，相比于估计单个状态动作对会有较大的估计方差。对抗性逆向强化学习（Adversarial
InverseReinforcementLearning，AIRL）(Fuetal.,2017)直接对单个状态和动作进行估计：
exp(f (s,a))
D (s,a)= θ (8.22)
θ exp(f (s,a))+π(a|s)
θ
其中π(a|s)是待更新的采样分布而f (s,a)是所学的函数。配分函数在上面式子中被忽略了，而
θ
概率值的归一性在实践中可以由Softmax函数或者Sigmoid输出激活函数来保证。经证明，在最
优情况下，f∗(s,a) = logπ∗(a|s) = A∗(s,a) 给出了最优策略的优势函数（Advantage Function）。
然而，优势函数是一个高度纠缠的奖励函数减去一个基线值的结果。文献(Fuetal.,2017)论证说
奖励函数从环境动态的变化中不能被鲁棒地恢复出来。因此，他们提出通过AIRL来从优势函数
中解纠缠（Disentangle）以得到奖励函数：
exp(f (s,a,s′))
D (s,a,s′ )= θ,ϕ (8.23)
θ,ϕ exp(f (s,a,s′))+π(a|s)
θ,ϕ
其中，f 被限制为一个奖励拟合器g 和一个塑形（Shaping）项h ：
θ,ϕ θ ϕ
f (s,a,s′ )=g (s,a)+γh (s′ )−h (s) (8.24)
θ,ϕ θ ϕ ϕ
其中还需要对h 进行额外拟合。
ϕ
268
8.4 从观察量进行模仿学习
8.4 从观察量进行模仿学习
首先，从观察量进行模仿学习（ImitationLearningfromObservation，IfO）是在没有完整可观
察的动作的情况下进行的模仿学习。IfO的一个例子是从视频中学习，其中物体的真实动作值是
无法单纯地通过一些帧中的信息得到的，但人类仍旧能够从视频中学习，比如模仿动作，因此，
在IfO相关文献中经常见到从视频中学习的例子。相比于其他前面介绍过的方法，IfO从另一个
角度来看待模仿学习。因而，这一小节所介绍的具体方法和之前介绍的方法有不可避免的重叠之
处，但是，要注意这一小节的方法是在IfO的范畴之下的。当你阅读这一小节时，应当记得，这
里的 IfO 方法与其他类别的方法大多是正交的关系，因为它是从另一个角度来处理模仿学习的，
并且着重于解决不可观测动作的问题。
之前提到的算法，几乎都不能用于解决只包含部分可观测或不可观测动作的示范数据的情
况。一个对于学习这种类型的示范数据的想法是先从状态中恢复动作，再采用标准的模仿学习
算法从恢复出来的状态-动作对（State-Action Pairs）中进行策略学习。比如，文献 (Torabi et al.,
2018a)通过学习一个状态转移（StateTransition）的动态模型来恢复动作，并使用BC算法来找到
最优策略。然而，这种方法的性能极大地依赖于所学动态模型的好坏，对于状态转移中有噪声的
情况则很可能失败。相反，文献(Mereletal.,2017)提出只通过状态（或状态的特征值）轨迹来学
习。他们拓展了GAIL框架，并只通过采集运动示范数据的状态来学习控制策略，展示了只需要
部分状态特征而不需要示范者的具体动作对对抗式模仿（AdversarialImitation）也是足够的。相
似地，文献(Eysenbachetal.,2018)指出策略应该可以控制智能体到达哪些状态，因而可通过最大
化策略和状态轨迹间的互信息（MutualInformation）来仅仅通过状态训练策略。也有一些其他研
究尝试只从观察量而不是真实状态中学习。比如，文献(Stadieetal.,2017)通过域自适应（Domain
Adaption）方法从观察量中提取特征来保证专家（Experts）和新手（Novices）在同一个特征空间
下。然而，只使用示范状态或状态特征在训练中可能需要大量的环境交互，因为任何来自动作的
信息都被忽略了。
为了提供IfO方法的一个清楚的框架，我们把文献中的IfO方法总结为两大类：（1）基于模
型（Model-Based）方法；（2）无模型（Model-Free）方法。这也与强化学习中的一种主要的分类
方法吻合。随后，我们讨论每一类方法的特点，并提出相关文献中的算法作为例子。
8.4.1 基于模型方法
类似于基于模型的强化学习（如第9章），如果环境模型可以用较低的消耗来精确学习，这
个模型可能对学习过程有利，因为通过它可以高效地做出规划。由于模仿学习在与环境交互的过
程中模仿的是一系列的动作而非单个动作，所以它难以避免地涉及环境的动态变化，而这可以通
过基于模型方法学习。根据不同的动态模型类型，基于模型的IfO方法可以被分类为：（1）逆向
动态模型（InverseDynamicsModels）和（2）正向动态模型（ForwardDynamicsModels）。
269
第8章 模仿学习
逆向动态模型：一个逆向动态模型是从状态转移 {(S ,S )} 到动作 {A } 的映射 (Hanna
t t+1 t
etal.,2017)。在这一类中的一个工作如文献(Nairetal.,2017)提出的方法，它通过人类操作绳子