# 08｜Embedding实战：如何使用Spark生成Item2vec和Graph Embedding？你好，我是王喆。前面两节课，我们一起学习了从 Item2vec 到 Graph Embedding 的几种经典Embedding方法。在打好了理论基础之后，这节课就让我们从理论走向实践，看看到底**如何基于 Spark 训练得到物品的 Embedding向量**。通过特征工程部分的实践，我想你已经对 Spark这个分布式计算平台有了初步的认识。其实除了一些基本的特征处理方法，在Spark 的机器学习包 Spark MLlib中，还包含了大量成熟的机器学习模型，这其中就包括我们讲过的 Word2vec模型。基于此，这节课我们会在 Spark 平台上，完成 **Item2vec和基于 Deep Walk 的 GraphEmbedding** 的训练。 对其他机器学习平台有所了解的同学可能会问，TensorFlow、PyTorch都有很强大的深度学习工具包，我们能不能利用这些平台进行 Embedding训练呢？当然是可以的，我们也会在之后的课程中介绍 TensorFlow并用它实现很多深度学习推荐模型。但是 Spark作为一个原生的分布式计算平台，在处理大数据方面还是比 TensorFlow等深度学习平台更具有优势，而且业界的很多公司仍然在使用 Spark训练一些结构比较简单的机器学习模型，再加上我们已经用 Spark进行了特征工程的处理，所以，这节课我们继续使用 Spark 来完成 Embedding的实践。 首先，我们来看看怎么完成 Item2vec的训练。 Item2vec：序列数据的处理我们知道，Item2vec 是基于自然语言处理模型 Word2vec 提出的，所以Item2vec 要处理的是类似文本句子，观影序列之类的序列数据。那在真正开始Item2vec 的训练之前，我们还要先为它准备好训练用的序列数据。在 movieLens数据集中，有一张叫rating（评分）的数据表，里面包含了用户对看过电影的评分和评分的时间。既然时间和评分历史都有了，我们要用的观影序列自然就可以通过处理rating 表得到啦。![](Images/8b452b3947397f7dbe4aa0b386eb10cf.png)savepage-src="https://static001.geekbang.org/resource/image/60/1d/6093962c46059de0ec9b2d54f56d7a1d.jpeg"}图1 movieLens数据集中的rating评分表不过，在使用观影序列编码之前，我们还要再明确两个问题。一是 movieLens这个 rating表本质上只是一个评分的表，不是真正的"观影序列"。但对用户来说，当然只有看过这部电影才能够评价它，所以我们几乎可以把评分序列当作是观影序列。二是我们是应该把所有电影都放到序列中，还是只放那些打分比较高的呢？这里，我是建议对评分做一个过滤，只放用户打分比较高的电影。为什么这么做呢？我们要思考一下Item2vec 这个模型本质上是要学习什么。我们是希望 item2vec能够学习到物品之间的近似性。既然这样，我们当然是希望评分好的电影靠近一些，评分差的电影和评分好的电影不要在序列中结对出现。好，那到这里我们明确了样本处理的思路，就是对一个用户来说，我们先过滤掉他评分低的电影，再把他评论过的电影按照时间戳排序。这样，我们就得到了一个用户的观影序列，所有用户的观影序列就组成了Item2vec 的训练样本集。那这个过程究竟该怎么在 Spark 上实现呢？其实很简单，我们只需要明白这 5个关键步骤就可以实现了：1.       读取 ratings 原始数据到 Spark    平台。    2.       用 where    语句过滤评分低的评分记录。        3.       用 groupBy userId 操作聚合每个用户的评分记录，DataFrame    中每条记录是一个用户的评分序列。        4.       定义一个自定义操作    sortUdf，用它实现每个用户的评分记录按照时间戳进行排序。        5.       把每个用户的评分记录处理成一个字符串的形式，供后续训练过程使用。        具体的实现过程，我还是建议你来参考我下面给出的代码，重要的地方我也都加上了注释，方便你来理解。    def processItemSequence(sparkSession: SparkSession): RDD[Seq[String]] ={      //设定rating数据的路径并用spark载入数据      val ratingsResourcesPath = this.getClass.getResource("/webroot/sampledata/ratings.csv")      val ratingSamples = sparkSession.read.format("csv").option("header", "true").load(ratingsResourcesPath.getPath)      //实现一个用户定义的操作函数(UDF)，用于之后的排序      val sortUdf: UserDefinedFunction = udf((rows: Seq[Row]) => {        rows.map { case Row(movieId: String, timestamp: String) => (movieId, timestamp) }          .sortBy { case (movieId, timestamp) => timestamp }          .map { case (movieId, timestamp) => movieId }      })      //把原始的rating数据处理成序列数据      val userSeq = ratingSamples        .where(col("rating") >= 3.5)  //过滤掉评分在3.5一下的评分记录        .groupBy("userId")            //按照用户id分组        .agg(sortUdf(collect_list(struct("movieId", "timestamp"))) as "movieIds")     //每个用户生成一个序列并用刚才定义好的udf函数按照timestamp排序        .withColumn("movieIdStr", array_join(col("movieIds"), " "))                    //把所有id连接成一个String，方便后续word2vec模型处理      //把序列数据筛选出来，丢掉其他过程数据      userSeq.select("movieIdStr").rdd.map(r => r.getAs[String]("movieIdStr").split(" ").toSeq)通过这段代码生成用户的评分序列样本中，每条样本的形式非常简单，它就是电影ID 组成的序列，比如下面就是 ID 为 11888用户的观影序列：    296 380 344 588 593 231 595 318 480 110 253 288 47 364 377 589 410 597 539 39 160 266 350 553 337 186 736 44 158 551 293 780 353 368 858    Item2vec：模型训练训练数据准备好了，就该进入我们这堂课的重头戏，模型训练了。手写 Item2vec 的整个训练过程肯定是一件让人比较“崩溃”的事情，好在 Spark MLlib 已经为我们准备好了方便调用的 Word2vec 模型接口。我先把训练的代码贴在下面，然后再带你一步步分析每一行代码是在做什么。    def trainItem2vec(samples : RDD[Seq[String]]): Unit ={        //设置模型参数        val word2vec = new Word2Vec()        .setVectorSize(10)        .setWindowSize(5)        .setNumIterations(10)      //训练模型      val model = word2vec.fit(samples)      //训练结束，用模型查找与item"592"最相似的20个item      val synonyms = model.findSynonyms("592", 20)      for((synonym, cosineSimilarity)  {        var pairSeq = Seq[String]()        var previousItem:String = null        sample.foreach((element:String) => {          if(previousItem != null){            pairSeq = pairSeq :+ (previousItem + ":" + element)          }          previousItem = element        })        pairSeq      })      //统计影片对的数量      val pairCount = pairSamples.countByValue()      //转移概率矩阵的双层Map数据结构      val transferMatrix = scala.collection.mutable.Map[String, scala.collection.mutable.Map[String, Long]]()      val itemCount = scala.collection.mutable.Map[String, Long]()      //求取转移概率矩阵      pairCount.foreach( pair => {        val pairItems = pair._1.split(":")        val count = pair._2        lognumber = lognumber + 1        println(lognumber, pair._1)        if (pairItems.length == 2){          val item1 = pairItems.apply(0)          val item2 = pairItems.apply(1)          if(!transferMatrix.contains(pairItems.apply(0))){            transferMatrix(item1) = scala.collection.mutable.Map[String, Long]()          }          transferMatrix(item1)(item2) = count          itemCount(item1) = itemCount.getOrElse[Long](item1, 0) + count        }      生成转移概率矩阵的函数输入是在训练 Item2vec时处理好的观影序列数据。输出的是转移概率矩阵，由于转移概率矩阵比较稀疏，因此我没有采用比较浪费内存的二维数组的方法，而是采用了一个双层map 的结构去实现它。比如说，我们要得到物品 A 到物品 B 的转移概率，那么transferMatrix(itemA)(itemB)就是这一转移概率。在求取转移概率矩阵的过程中，我先利用 Spark 的 flatMap操作把观影序列"打碎"成一个个影片对，再利用 countByValue操作统计这些影片对的数量，最后根据这些影片对的数量求取每两个影片之间的转移概率。在获得了物品之间的转移概率矩阵之后，我们就可以进入图3（c）的步骤，进行随机游走采样了。Graph Embedding：随机游走采样过程随机游走采样的过程是利用转移概率矩阵生成新的序列样本的过程。这怎么理解呢？首先，我们要根据物品出现次数的分布随机选择一个起始物品，之后就进入随机游走的过程。在每次游走时，我们根据转移概率矩阵查找到两个物品之间的转移概率，然后根据这个概率进行跳转。比如当前的物品是A，从转移概率矩阵中查找到 A 可能跳转到物品 B 或物品 C，转移概率分别是0.4 和 0.6，那么我们就按照这个概率来随机游走到 B 或C，依次进行下去，直到样本的长度达到了我们的要求。根据上面随机游走的过程，我用 Scala进行了实现，你可以参考下面的代码，在关键的位置我也给出了注释：    //随机游走采样函数    //transferMatrix 转移概率矩阵    //itemCount 物品出现次数的分布    def randomWalk(transferMatrix : scala.collection.mutable.Map[String, scala.collection.mutable.Map[String, Long]], itemCount : scala.collection.mutable.Map[String, Long]): Seq[Seq[String]] ={      //样本的数量      val sampleCount = 20000      //每个样本的长度      val sampleLength = 10      val samples = scala.collection.mutable.ListBuffer[Seq[String]]()            //物品出现的总次数      var itemTotalCount:Long = 0      for ((k,v) = randomDouble * itemTotalCount){          firstElement = item          break        }      }}      sample.append(firstElement)      var curElement = firstElement      //通过随机游走产生长度为sampleLength的样本      breakable { for( w = randomDouble * curCount){            curElement = item            break          }        }}        sample.append(curElement)      }}      Seq(sample.toList : _通过随机游走产生了我们训练所需的 sampleCount个样本之后，下面的过程就和 Item2vec的过程完全一致了，就是把这些训练样本输入到 Word2vec 模型中，完成最终Graph Embedding 的生成。你也可以通过同样的方法去验证一下通过 GraphEmbedding 方法生成的 Embedding的效果。 小结这节课，我们运用 Spark 实现了经典的 Embedding 方法 Item2vec 和 DeepWalk。它们的理论知识你应该已经在前两节课的学习中掌握了，这里我就总结一下实践中应该注意的几个要点。关于 Item2vec 的 Spark 实现，你应该注意的是训练 Word2vec模型的几个参数 VectorSize、WindowSize、NumIterations等，知道它们各自的作用。它们分别是用来设置 Embedding向量的维度，在序列数据上采样的滑动窗口大小，以及训练时的迭代次数。而在 Deep Walk的实现中，我们应该着重理解的是，生成物品间的转移概率矩阵的方法，以及通过随机游走生成训练样本过程。最后，我还是把这节课的重点知识总结在了一张表格中，希望能帮助你进一步巩固。![](Images/7fd0aadbea271cb8728cd9390cb4e93d.png)savepage-src="https://static001.geekbang.org/resource/image/08/03/08e80a2ba524e3d8fae47b732e5d8a03.jpeg"}这里，我还想再多说几句。这节课，我们终于看到了深度学习模型的产出，我们用Embedding方法计算出了相似电影！对于我们学习这门课来说，它完全可以看作是一个里程碑式的进步。接下来，我希望你能总结实战中的经验，跟我继续同行，一起迎接未来更多的挑战！课后思考上节课，我们在讲 Graph Embedding 的时候，还介绍了 Node2vec方法。你能尝试在 Deep Walk 代码的基础上实现 Node2vec吗？这其中，我们应该着重改变哪部分的代码呢？欢迎把你的思考和答案写在留言区，如果你掌握了 Embedding的实战方法，也不妨把它分享给你的朋友吧，我们下节课见！