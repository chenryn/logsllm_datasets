the VID driver, which, as we have already introduced, covers a key role in
the functionality of the entire Hyper-V platform.
Intercepts
The root partition should be able to create a virtual environment that allows
an unmodified guest OS, which was written to execute on physical hardware,
to run in a hypervisor’s guest partition. Such legacy guests may attempt to
access physical devices that do not exist in a hypervisor partition (for
example, by accessing certain I/O ports or by writing to specific MSRs). For
these cases, the hypervisor provides the host intercepts facility; when a VP of
a guest VM executes certain instructions or generates certain exceptions, the
authorized root partition can intercept the event and alter the effect of the
intercepted instruction such that, to the child, it mirrors the expected behavior
in physical hardware.
When an intercept event occurs in a child partition, its VP is suspended,
and an intercept message is sent to the root partition by the Synthetic
Interrupt Controller (SynIC; see the following section for more details) from
the hypervisor. The message is received thanks to the hypervisor’s Synthetic
ISR (Interrupt Service Routine), which the NT kernel installs during phase 0
of its startup only in case the system is enlightened and running under the
hypervisor (see Chapter 12 for more details). The hypervisor synthetic ISR
(KiHvInterrupt), usually installed on vector 0x30, transfers its execution to
an external callback, which the VID driver has registered when it started
(through the exposed HvlRegisterInterruptCallback NT kernel API).
The VID driver is an intercept driver, meaning that it is able to register
host intercepts with the hypervisor and thus receives all the intercept events
that occur on child partitions. After the partition is initialized, the WM
Worker process registers intercepts for various components of the
virtualization stack. (For example, the virtual motherboard registers I/O
intercepts for each virtual COM ports of the VM.) It sends an IOCTL to the
VID driver, which uses the HvInstallIntercept hypercall to install the
intercept on the child partition. When the child partition raises an intercept,
the hypervisor suspends the VP and injects a synthetic interrupt in the root
partition, which is managed by the KiHvInterrupt ISR. The latter routine
transfers the execution to the registered VID Intercept callback, which
manages the event and restarts the VP by clearing the intercept suspend
synthetic register of the suspended VP.
The hypervisor supports the interception of the following events in the
child partition:
■    Access to I/O ports (read or write)
■    Access to VP’s MSR (read or write)
■    Execution of CPUID instruction
■    Exceptions
■    Accesses to general purposes registers
■    Hypercalls
The synthetic interrupt controller (SynIC)
The hypervisor virtualizes interrupts and exceptions for both the root and
guest partitions through the synthetic interrupt controller (SynIC), which is an
extension of a virtualized local APIC (see the Intel or AMD software
developer manual for more details about the APIC). The SynIC is responsible
for dispatching virtual interrupts to virtual processors (VPs). Interrupts
delivered to a partition fall into two categories: external and synthetic (also
known as internal or simply virtual interrupts). External interrupts originate
from other partitions or devices; synthetic interrupts are originated from the
hypervisor itself and are targeted to a partition’s VP.
When a VP in a partition is created, the hypervisor creates and initializes a
SynIC for each supported VTL. It then starts the VTL 0’s SynIC, which
means that it enables the virtualization of a physical CPU’s APIC in the
VMCS (or VMCB) hardware data structure. The hypervisor supports three
kinds of APIC virtualization while dealing with external hardware interrupts:
■    In standard configuration, the APIC is virtualized through the event
injection hardware support. This means that every time a partition
accesses the VP’s local APIC registers, I/O ports, or MSRs (in the
case of x2APIC), it produces a VMEXIT, causing hypervisor codes to
dispatch the interrupt through the SynIC, which eventually “injects”
an event to the correct guest VP by manipulating VMCS/VMCB
opaque fields (after it goes through the logic similar to a physical
APIC, which determines whether the interrupt can be delivered).
■    The APIC emulation mode works similar to the standard
configuration. Every physical interrupt sent by the hardware (usually
through the IOAPIC) still causes a VMEXIT, but the hypervisor does
not have to inject any event. Instead, it manipulates a virtual-APIC
page used by the processor to virtualize certain access to the APIC
registers. When the hypervisor wants to inject an event, it simply
manipulates some virtual registers mapped in the virtual-APIC page.
The event is delivered by the hardware when a VMENTRY happens.
At the same time, if a guest VP manipulates certain parts of its local
APIC, it does not produce any VMEXIT, but the modification will be
stored in the virtual-APIC page.
■    Posted interrupts allow certain kinds of external interrupts to be
delivered directly in the guest partition without producing any
VMEXIT. This allows direct access devices to be mapped directly in
the child partition without incurring any performance penalties caused
by the VMEXITs. The physical processor processes the virtual
interrupts by directly recording them as pending on the virtual-APIC
page. (For more details, consult the Intel or AMD software developer
manual.)
When the hypervisor starts a processor, it usually initializes the synthetic
interrupt controller module for the physical processor (represented by a
CPU_PLS data structure). The SynIC module of the physical processor is an
array of an interrupt’s descriptors, which make the connection between a
physical interrupt and a virtual interrupt. A hypervisor interrupt descriptor
(IDT entry), as shown in Figure 9-18, contains the data needed for the SynIC
to correctly dispatch the interrupt, in particular the entity the interrupt is
delivered to (a partition, the hypervisor, a spurious interrupt), the target VP
(root, a child, multiple VPs, or a synthetic interrupt), the interrupt vector, the
target VTL, and some other interrupt characteristics.
Figure 9-18 The hypervisor physical interrupt descriptor.
In default configurations, all the interrupts are delivered to the root
partition in VTL 0 or to the hypervisor itself (in the second case, the interrupt
entry is Hypervisor Reserved). External interrupts can be delivered to a guest
partition only when a direct access device is mapped into a child partition;
NVMe devices are a good example.
Every time the thread backing a VP is selected for being executed, the
hypervisor checks whether one (or more) synthetic interrupt needs to be
delivered. As discussed previously, synthetic interrupts aren’t generated by
any hardware; they’re usually generated from the hypervisor itself (under
certain conditions), and they are still managed by the SynIC, which is able to
inject the virtual interrupt to the correct VP. Even though they’re extensively
used by the NT kernel (the enlightened clock timer is a good example),
synthetic interrupts are fundamental for the Virtual Secure Mode (VSM). We
discuss them in in the section “The Secure Kernel” later in this chapter.
The root partition can send a customized virtual interrupt to a child by
using the HvAssertVirtualInterrupt hypercall (documented in the TLFS).
Inter-partition communication
The synthetic interrupt controller also has the important role of providing
inter-partition communication facilities to the virtual machines. The
hypervisor provides two principal mechanisms for one partition to
communicate with another: messages and events. In both cases, the
notifications are sent to the target VP using synthetic interrupts. Messages
and events are sent from a source partition to a target partition through a
preallocated connection, which is associated with a destination port.
One of the most important components that uses the inter-partition
communication services provided by the SynIC is VMBus. (VMBus
architecture is discussed in the “Virtualization stack” section later in this
chapter.) The VMBus root driver (Vmbusr.sys) in the root allocates a port ID
(ports are identified by a 32-bit ID) and creates a port in the child partition by
emitting the HvCreatePort hypercall through the services provided by the
WinHv driver.
A port is allocated in the hypervisor from the receiver’s memory pool.
When a port is created, the hypervisor allocates sixteen message buffers from
the port memory. The message buffers are maintained in a queue associated
with a SINT (synthetic interrupt source) in the virtual processor’s SynIC. The
hypervisor exposes sixteen interrupt sources, which can allow the VMBus
root driver to manage a maximum of 16 message queues. A synthetic
message has the fixed size of 256 bytes and can transfer only 240 bytes (16
bytes are used as header). The caller of the HvCreatePort hypercall specifies
which virtual processor and SINT to target.
To correctly receive messages, the WinHv driver allocates a synthetic
interrupt message page (SIMP), which is then shared with the hypervisor.
When a message is enqueued for a target partition, the hypervisor copies the
message from its internal queue to the SIMP slot corresponding to the correct
SINT. The VMBus root driver then creates a connection, which associates
the port opened in the child VM to the parent, through the HvConnectPort
hypercall. After the child has enabled the reception of synthetic interrupts in
the correct SINT slot, the communication can start; the sender can post a
message to the client by specifying a target Port ID and emitting the
HvPostMessage hypercall. The hypervisor injects a synthetic interrupt to the
target VP, which can read from the message page (SIMP) the content of the
message.
The hypervisor supports ports and connections of three types:
■    Message ports Transmit 240-byte messages from and to a partition.
A message port is associated with a single SINT in the parent and
child partition. Messages will be delivered in order through a single
port message queue. This characteristic makes messages ideal for
VMBus channel setup and teardown (further details are provided in
the “Virtualization stack” section later in this chapter).
■    Event ports Receive simple interrupts associated with a set of flags,
set by the hypervisor when the opposite endpoint makes a
HvSignalEvent hypercall. This kind of port is normally used as a
synchronization mechanism. VMBus, for example, uses an event port
to notify that a message has been posted on the ring buffer described
by a particular channel. When the event interrupt is delivered to the
target partition, the receiver knows exactly to which channel the
interrupt is targeted thanks to the flag associated with the event.
■    Monitor ports An optimization to the Event port. Causing a
VMEXIT and a VM context switch for every single HvSignalEvent
hypercall is an expensive operation. Monitor ports are set up by
allocating a shared page (between the hypervisor and the partition)
that contains a data structure indicating which event port is associated
with a particular monitored notification flag (a bit in the page). In that
way, when the source partition wants to send a synchronization
interrupt, it can just set the corresponding flag in the shared page.
Sooner or later the hypervisor will notice the bit set in the shared page
and will trigger an interrupt to the event port.
The Windows hypervisor platform API and EXO
partitions
Windows increasingly uses Hyper-V’s hypervisor for providing functionality
not only related to running traditional VMs. In particular, as we will discuss
discuss in the second part of this chapter, VSM, an important security
component of modern Windows versions, leverages the hypervisor to enforce
a higher level of isolation for features that provide critical system services or
handle secrets such as passwords. Enabling these features requires that the
hypervisor is running by default on a machine.
External virtualization products, like VMware, Qemu, VirtualBox,
Android Emulator, and many others use the virtualization extensions
provided by the hardware to build their own hypervisors, which is needed for
allowing them to correctly run. This is clearly not compatible with Hyper-V,
which launches its hypervisor before the Windows kernel starts up in the root
partition (the Windows hypervisor is a native, or bare-metal hypervisor).
As for Hyper-V, external virtualization solutions are also composed of a
hypervisor, which provides generic low-level abstractions for the processor’s
execution and memory management of the VM, and a virtualization stack,
which refers to the components of the virtualization solution that provide the
emulated environment for the VM (like its motherboard, firmware, storage
controllers, devices, and so on).
The Windows Hypervisor Platform API, which is documented at
https://docs.microsoft.com/en-us/virtualization/api/, has the main goal to
enable running third-party virtualization solutions on the Windows
hypervisor. Specifically, a third-party virtualization product should be able to
create, delete, start, and stop VMs with characteristics (firmware, emulated
devices, storage controllers) defined by its own virtualization stack. The
third-party virtualization stack, with its management interfaces, continues to
run on Windows in the root partition, which allows for an unchanged use of
its VMs by their client.
As shown in Figure 9-19, all the Windows hypervisor platform’s APIs run
in user mode and are implemented on the top of the VID and WinHvr driver
in two libraries: WinHvPlatform.dll and WinHvEmulation.dll (the latter
implements the instruction emulator for MMIO).
Figure 9-19 The Windows hypervisor platform API architecture.
A user mode application that wants to create a VM and its relative virtual
processors usually should do the following:
1. 
Create the partition in the VID library (Vid.dll) with the
WHvCreatePartition API.
2. 
Configure various internal partition’s properties—like its virtual
processor count, the APIC emulation mode, the kind of requested
VMEXITs, and so on—using the WHvSetPartitionProperty API.
3. 
Create the partition in the VID driver and the hypervisor using the
WHvSetupPartition API. (This kind of partition in the hypervisor is
called an EXO partition, as described shortly.) The API also creates
the partition’s virtual processors, which are created in a suspended
state.
4. 
Create the corresponding virtual processor(s) in the VID library
through the WHvCreateVirtual-Processor API. This step is important
because the API sets up and maps a message buffer into the user
mode application, which is used for asynchronous communication
with the hypervisor and the thread running the virtual CPUs.
5. 
Allocate the address space of the partition by reserving a big range of
virtual memory with the classic VirtualAlloc function (read more
details in Chapter 5 of Part 1) and map it in the hypervisor through the
WHvMapGpaRange API. A fine-grained protection of the guest
physical memory can be specified when allocating guest physical
memory in the guest virtual address space by committing different
ranges of the reserved virtual memory.
6. 
Create the page-tables and copy the initial firmware code in the
committed memory.
7. 
Set the initial VP’s registers content using the
WHvSetVirtualProcessorRegisters API.
8. 
Run the virtual processor by calling the WHvRunVirtualProcessor
blocking API. The function returns only when the guest code executes
an operation that requires handling in the virtualization stack (a
VMEXIT in the hypervisor has been explicitly required to be
managed by the third-party virtualization stack) or because of an
external request (like the destroying of the virtual processor, for
example).
The Windows hypervisor platform APIs are usually able to call services in
the hypervisor by sending different IOCTLs to the \Device\VidExo device
object, which is created by the VID driver at initialization time, only if the
HKLM\System\CurrentControlSet\Services\Vid\Parameters\ExoDeviceEnabl
ed registry value is set to 1. Otherwise, the system does not enable any
support for the hypervisor APIs.
Some performance-sensitive hypervisor platform APIs (a good example is
provided by WHvRunVirtualProcessor) can instead call directly into the
hypervisor from user mode thanks to the Doorbell page, which is a special
invalid guest physical page, that, when accessed, always causes a VMEXIT.
The Windows hypervisor platform API obtains the address of the doorbell
page from the VID driver. It writes to the doorbell page every time it emits a
hypercall from user mode. The fault is identified and treated differently by
the hypervisor thanks to the doorbell page’s physical address, which is
marked as “special” in the SLAT page table. The hypervisor reads the
hypercall’s code and parameters from the VP’s registers as per normal
hypercalls, and ultimately transfers the execution to the hypercall’s handler
routine. When the latter finishes its execution, the hypervisor finally
performs a VMENTRY, landing on the instruction following the faulty one.
This saves a lot of clock cycles to the thread backing the guest VP, which no
longer has a need to enter the kernel for emitting a hypercall. Furthermore,
the VMCALL and similar opcodes always require kernel privileges to be
executed.
The virtual processors of the new third-party VM are dispatched using the
root scheduler. In case the root scheduler is disabled, any function of the
hypervisor platform API can’t run. The created partition in the hypervisor is
an EXO partition. EXO partitions are minimal partitions that don’t include
any synthetic functionality and have certain characteristics ideal for creating
third-party VMs:
■    They are always VA-backed types. (More details about VA-backed or
micro VMs are provided later in the “Virtualization stack” section.)
The partition’s memory-hosting process is the user mode application,
which created the VM, and not a new instance of the VMMEM
process.
■    They do not have any partition’s privilege or support any VTL
(virtual trust level) other than 0. All of a classical partition’s
privileges refer to synthetic functionality, which is usually exposed by
the hypervisor to the Hyper-V virtualization stack. EXO partitions are
used for third-party virtualization stacks. They do not need the
functionality brought by any of the classical partition’s privilege.
■    They manually manage timing. The hypervisor does not provide any
virtual clock interrupt source for EXO partition. The third-party
virtualization stack must take over the responsibility of providing this.
This means that every attempt to read the virtual processor’s time-
stamp counter will cause a VMEXIT in the hypervisor, which will
route the intercept to the user mode thread that runs the VP.
 Note
EXO partitions include other minor differences compared to classical
hypervisor partitions. For the sake of the discussion, however, those
minor differences are irrelevant, so they are not mentioned in this book.
Nested virtualization
Large servers and cloud providers sometimes need to be able to run
containers or additional virtual machines inside a guest partition. Figure 9-20
describes this scenario: The hypervisor that runs on the top of the bare-metal
hardware, identified as the L0 hypervisor (L0 stands for Level 0), uses the
virtualization extensions provided by the hardware to create a guest VM.
Furthermore, the L0 hypervisor emulates the processor’s virtualization
extensions and exposes them to the guest VM (the ability to expose
virtualization extensions is called nested virtualization). The guest VM can
decide to run another instance of the hypervisor (which, in this case, is
identified as L1 hypervisor, where L1 stands for Level 1), by using the
emulated virtualization extensions exposed by the L0 hypervisor. The L1
hypervisor creates the nested root partition and starts the L2 root operating
system in it. In the same way, the L2 root can orchestrate with the L1
hypervisor to launch a nested guest VM. The final guest VM in this
configuration takes the name of L2 guest.
Figure 9-20 Nested virtualization scheme.
Nested virtualization is a software construction: the hypervisor must be
able to emulate and manage virtualization extensions. Each virtualization
instruction, while executed by the L1 guest VM, causes a VMEXIT to the L0
hypervisor, which, through its emulator, can reconstruct the instruction and
perform the needed work to emulate it. At the time of this writing, only Intel
and AMD hardware is supported. The nested virtualization capability should
be explicitly enabled for the L1 virtual machine; otherwise, the L0 hypervisor
injects a general protection exception in the VM in case a virtualization
instruction is executed by the guest operating system.
On Intel hardware, Hyper-V allows nested virtualization to work thanks to
two main concepts:
■    Emulation of the VT-x virtualization extensions
■    Nested address translation
As discussed previously in this section, for Intel hardware, the basic data
structure that describes a virtual machine is the virtual machine control
structure (VMCS). Other than the standard physical VMCS representing the
L1 VM, when the L0 hypervisor creates a VP belonging to a partition that
supports nested virtualization, it allocates some nested VMCS data structures
(not to be confused with a virtual VMCS, which is a different concept). The
nested VMCS is a software descriptor that contains all the information
needed by the L0 hypervisor to start and run a nested VP for a L2 partition.
As briefly introduced in the “Hypervisor startup” section, when the L1
hypervisor boots, it detects whether it’s running in a virtualized environment
and, if so, enables various nested enlightenments, like the enlightened VMCS
or the direct virtual flush (discussed later in this section).
As shown in Figure 9-21, for each nested VMCS, the L0 hypervisor also
allocates a Virtual VMCS and a hardware physical VMCS, two similar data
structures representing a VP running the L2 virtual machine. The virtual
VMCS is important because it has the key role in maintaining the nested
virtualized data. The physical VMCS instead is loaded by the L0 hypervisor
when the L2 virtual machine is started; this happens when the L0 hypervisor
intercepts a VMLAUNCH instruction executed by the L1 hypervisor.
Figure 9-21 A L0 hypervisor running a L2 VM by virtual processor 2.
In the sample picture, the L0 hypervisor has scheduled the VP 2 for
running a L2 VM managed by the L1 hypervisor (through the nested virtual
processor 1). The L1 hypervisor can operate only on virtualization data
replicated in the virtual VMCS.
Emulation of the VT-x virtualization extensions
On Intel hardware, the L0 hypervisor supports both enlightened and
nonenlightened L1 hypervisors. The only official supported configuration is
Hyper-V running on the top of Hyper-V, though.
In a nonenlightened hypervisor, all the VT-x instructions executed in the
L1 guest causes a VMEXIT. After the L1 hypervisor has allocated the guest