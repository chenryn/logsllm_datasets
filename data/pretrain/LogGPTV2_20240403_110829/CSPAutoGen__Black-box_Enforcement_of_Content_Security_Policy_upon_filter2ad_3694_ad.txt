templates without fully understanding each script.
Violation Script Panel. For non-server deployment, CSPAutoGen
provides a violation script panel to end users so they can whitelist
their trusted scripts. The panel is essentially a popup window that
can be opened by an optional button inserted via CSPAutoGen’s
client-side library. Such panel lists all the blocked scripts of cur-
rent domain, speciﬁes reason for each violation (i.e., gAST does not
exist and type does not match) and related information. End user
can whitelist any of those scripts or add new trusted scripts. Each
client has her own whitelist, thus, CSPAutoGen needs to authenti-
cate clients. The authentication is via a unique identiﬁer cookie set
in the client browser.
Violation Report Mechanism. Violation reports can help adminis-
trators understand if there are attacks happening or whether the de-
ployed templates are outdated. CSP’s native violation report mech-
anism is not applicable to CSPAutoGen, because violation cases
might not trigger CSP violations (e.g., those non-matched scripts
in rewriting phase) and scripts that trigger CSP native violations
might get executed (e.g., the runtime-included inline scripts that
match templates). Therefore, we propose a new violation report
mechanism that will send accurate violation report to administra-
tors. Those reports include violated URLs and scripts, under user
agreements.
When a website’s JavaScripts are updated, CSPAutoGen will cap-
ture the violated scripts via violation report mechanism and group
them based on their gASTs. If one group’s script number achieves a
threshold, CSPAutoGen can automatically generate template patches
and report the patches to the administrator. Then, the administrator
can utilize visual template portal to visually review these patches
as well as their corresponding templates, and securely apply the
patches to existing templates. Such process is not often: our eval-
uation shows that people may only need to do it every other month
(Section 6.2.2).
5.
IMPLEMENTATION
We implement CSPAutoGen in 3,000 lines of Python code and
6,500 lines of JavaScript code. In the training phase, we customize
PhantomJS as a headless browser to render URLs and extract scripts
as well as trusted hosts. These headless browsers are driven and
managed by our rendering task generator, which is written in Python
to dynamically conﬁgure PhantomJS instances. The other two com-
ponents, the template generator and the host whitelist generator, are
written in Node.js. We use Esprima library [3], a high performance,
standard-compliant JavaScript parser written in JavaScript, to parse
codes and generate ASTs. The generated gASTs will be stored in
our template database implemented by MongoDB.
In the rewriting phase, CSPAutoGen is deployed on mitmproxy,
an SSL-capable man-in-the-middle proxy written in Python. It pro-
vides a scripting API to external Python scripts, deﬁned as inline
scripts (which is different from inline scripts in JavaScript). Such
inline scripts can intercept HTTP/HTTPS requests and responses
as well as modify them on the ﬂy. Our CSPAutoGen engine is
implemented as mitmproxy’s inline scripts, and utilizes Python’s
BeautifulSoup package to parse and rewrite DOM trees. In order to
be robust to the broken pages that can still be rendered by browser,
we choose html5lib parser [7] as parsing engine, which is lenient
and parses HTML the same way a web browser does.
In the runtime phase, the client-side JavaScript library handles
runtime-included and dynamic scripts. The library is written in
JavaScript, and uses Esprima library to generate script AST.
All the servers, including the CSPAutoGen JavaScript server, the
violation report server and the whitelist server, are implemented in
Node.js. At server side, the scripts need to be matched against tem-
plates. Those codes are written in Node.js with Esprima library.
6. EVALUATION
In this section, we evaluate CSPAutoGen to answer these six
questions: (1) How do we train CSPAutoGen? (2) Are the gen-
erated templates robust enough to match most of the unseen but be-
nign scripts, and stable enough to last for a relatively long period?
(3) Are the generated templates correct? (4) Can CSPAutoGen pro-
tect vulnerable websites against real-world XSS attacks? (5) How
much overhead does CSPAutoGen incur? (6) Is CSPAutoGen com-
patible with real-world websites?
6.1 Training Datasets
In this section, we present how to train CSPAutoGen based on
different scenarios, i.e., whether we deploy CSPAutoGen at the
middlebox or the server.
6.1.1 Training based on Alexa Websites
To evaluate CSPAutoGen over real-world websites, we obtain
Alexa Top 50 websites as our dataset. For each website, we crawl
2,500 different webpages and split them into a training set with
2,000 webpages (training webpages) and a testing set with the rest
500 webpages (testing webpages). To maximize the code cover-
age, we use PhantomJS [34], a popular headless browser, to ren-
der each webpage ten times with ﬁve user-agents (i.e., Android,
Chrome, Firefox, IE and iPhone) and two cookie settings (i.e., a
clean cookie jar and a cookie jar initiated with login credential) re-
spectively. Moreover, to make sure that the training set is clean, the
crawling starts from a clean URL, and all crawled scripts are tested
by VirusTotal [8] under more than 60 antivirus software preventing
known XSS payload.
Table 1: Template Robustness over Time for Alexa Websites.
Domain
Amazon
CNN
Facebook
Google
Reddit
Yahoo
# of gAST
(2016/01/01)
# of Scripts Matching Rate
(2016/02/01)
(2016/02/01)
# of Scripts Matching Rate
(2016/03/01)
(2016/03/01)
# of Scripts Matching Rate
(2016/04/01)
(2016/04/01)
596
259
53
857
49
295
149,592
46,939
17,830
12,082
14,487
8,834
99.62%
98.72%
99.47%
97.89%
97.47%
99.30%
157,281
45,559
15,419
14,501
12,888
8,676
99.13%
98.50%
97.90%
94.13%
97.01%
99.05%
373,635
48,853
10,995
16,385
13,622
7,264
98.39%
98.30%
95.98%
87.14%
89.78%
94.01%
Table 2: Template Robustness over Time for Web Frameworks.
Application
Concrete5
Drupal
Joomla
MyBB
SilverStripe
WordPress
First Version (V1)
& Release Date
5.7.0 (09/12/2014)
7.2.2 (04/03/2013)
3.4.0 (02/24/2015)
1.8.0 09/01/2014)
3.1.0 (10/01/2013)
4.2.0 (04/22/2015)
# of gAST
from V1
3
5
5
25
0
3
Second Version (V2)
& Release Date
5.7.4 (04/17/2015)
7.3.2 (10/15/2014)
3.4.2 (06/30/2015)
1.8.4 (02/15/2015)
3.1.12 (03/09/2015)
4.2.3 (07/23/2015)
# of gAST
from V2
8
5
5
26
6
26
Third Version (V3)
& Release Date
5.7.5 (08/11/2015)
7.4.3 (02/24/2016)
3.4.3 (07/02/2015)
1.8.5 (05/27/2015)
3.1.13 (05/27/2015)
4.2.4 (08/04/2015)
V3
LOC
92,211
53,000
447,763
329,633
297,787
262,348
V1
V2
Matching Rate Matching Rate
67.7%
100%
100%
99.6%
NA
0%
85.6%
100%
100%
100%
100%
100%
6.1.2 Training based on Web Frameworks
When CSPAutoGen is deployed at the server, web developers can
generate a customized training URL list that has better coverage
than crawling. This is more like a gray-box deployment sitting in
between blackbox and whitebox. The key idea is that most websites
have test cases that cover the source code and explore the website
functionality as much as possible. Therefore, we can utilize these
test cases to generate a list of URLs, which can be used to explore
the scripts on that website.
Speciﬁcally, because Alexa websites are closed source, we train
CSPAutoGen based on open-source web frameworks.
In the ex-
periment, we use six frameworks, which are Concrete5, Drupal,
Joomla, MyBB, SilverStripe and WordPress.
Here are the details about generating the URL list. Five out of
the six frameworks provide extensive test cases. By modifying the
testing framework to hook the HTTP request sending methods (e.g.,
WP_UnitTestCase.go_to(...) and WebDriver.get(...)), we get a list
of testing URLs with various parameters covering all the interfaces
of the website. Then we use different credential settings (i.e., admin
credential, regular user credential and no credential ) to crawl these
URLs. For the remaining one web framework without test cases
(i.e., MyBB), we ﬁrst crawl a list of URLs using scrapy [5] frame-
work. Then we use its Google SEO plugin to generate sitemap
automatically and manually supplement URL list with the help of
sitemap as well as source codes. After that, we crawl those URLs
with the above three credential settings.
6.2 Template Robustness
In this section, we evaluate our template’s robustness on unseen
webpages and over time.
6.2.1 Robustness on Unseen Webpages
In this experiment, we evaluate the template robustness on un-
seen webpages. For Alexa websites, we use 500 unseen webpages
that are crawled but not included in the training. For web frame-
works, we deploy the web framework and crawl 500 new web-
pages. In our evaluation of Alexa websites, the average number
of training scripts for each domain is 39,079 and the average num-
ber of testing scripts is 12,087 per domain. The median value of
the number of gASTs for each domain is 262.
Figure 3 shows the matching rate of CSPAutoGen’s templates for
Alexa Top 50 websites: ranging from 91.6% to 100.0%, with a me-
dian value as 99.2%. Such a high matching rate shows that CSPAu-
toGen is able to match most of the unseen scripts, and this is also
conﬁrmed in our compatibility evaluation later. In web framework
evaluation, because of the training method in which we know the
source code, the matching rate is 100% for all six web frameworks.
6.2.2 Robustness over Time
In this subsection, we conduct two experiments on real-world
websites and web frameworks respectively to evaluate how long a
template can achieve a high matching rate without updating.
Robustness on Real-world Websites over Time. In the ﬁrst ex-
periment, we randomly select six popular websites and show the
template matching rate over three months. Speciﬁcally, we train
templates based on the contents of each website on 01/01/2016.
Then we use these templates to match the same website captured
on 02/01/2016, 03/01/2016 and 04/01/2016.
In this experiment,
2,000 crawled webpages are used for both training and testing.
The results are shown in Table 1. We set matching rate 90% as
the threshold for template updating. From Table 2, we can see that
the templates of Amazon, CNN, Facebook and Yahoo can work
without updating for at least three months; the templates of Google
and Reddit can maintain satisfying matching rates for two months.
CSPAutoGen can automatically generate template patches. For se-
curity, it is highly suggested that site administrators would review
the patches before applying them. We have interviewed engineers
from 8 big IT companies, including Google, Facebook and Ama-
zon: all of them considered the workload of reviewing templates
every other month acceptable.
Robustness on Web Frameworks over Time. To evaluate tem-
plate robustness of open-source web framework, for each of the
aforementioned six in Section 6.1.2, we deploy three popular re-
lease versions, referred to as the ﬁrst version (V1), the second ver-
sion (V2) and the third version (V3). In this experiment, we gen-
erate two templates based on V1 and V2, and then use these two
templates to match all the inline scripts extracted from V3.
Table 2 shows the results. Each row contains the versions and
release dates of V1, V2 and V3, the number of gASTs in each tem-
plate, the lines of codes for V3 and the matching rates of the two
templates. The results show that using V2 template to match V3
scripts can achieve an acceptable matching rate (100% except for
Concrete5, which is 85.6%). The median value of the duration be-
tween the release dates of V2 and V3 is 90 days, meaning that on
average, the template can work for three months without requir-
ing updating. As for the matching rates of V1 templates, Drupal,
MyBB and Joomla can still achieve a very high rate and the dura-
tion median value is 268 days. However, SilverStripe, Concrete5
and WordPress’s V1 templates need to be updated.
6.3 Correctness
In this part of the section, we evaluate the correctness of gener-
ated templates. Because Top Alexa websites are closed source, we
can only evaluate web frameworks. Speciﬁcally, due to extensive
Table 3: Comparison of Types in WordPress 4.2.3 with
These Inferred by CSPAutoGen
Type