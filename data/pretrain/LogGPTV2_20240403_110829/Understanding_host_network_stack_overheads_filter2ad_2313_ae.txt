(cid:5)
(cid:6)(cid:4)
(cid:2)(cid:3)
(cid:7) (cid:4)(cid:9) (cid:15)
(cid:6)(cid:3)(cid:1)
(cid:6)(cid:2)(cid:1)
(cid:6)(cid:1)(cid:1)
(cid:5)(cid:1)
(cid:4)(cid:1)
(cid:3)(cid:1)
(cid:2)(cid:1)
(cid:1)
(cid:16)
(cid:15)
(cid:7)
(cid:14)
(cid:13)
(cid:12)
(cid:8)
(cid:5)
(cid:7)
(cid:2)
(cid:6)
(cid:5)
(cid:4)
(cid:3)
(cid:2)
(cid:1)
(cid:18)
(cid:17)
(cid:8)
(cid:4)
(cid:1)
l
s
e
c
y
C
U
P
C
f
o
n
o
i
t
c
a
r
F
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
d
a t a   c
t c
p
1 ow
8 ows
16 ows
24 ows
s y s t e m
o
y
p
/ i p   p
e
c
o
e t d
r
n
g
s si n
vic
e
b
u
e  s
b   m g m t
s k
r y   a ll o
m e m
o
e
d
/
c
c
a ll o
l o
u
/
k
c
k
c
n l o
d
e
h
s c
g
u li n
e t c .
(cid:7)(cid:1)(cid:1)
(cid:6)(cid:1)(cid:1)
(cid:5)(cid:1)(cid:1)
(cid:4)(cid:1)(cid:1)
(cid:3)(cid:1)(cid:1)
(cid:2)(cid:1)(cid:1)
(cid:1)
(cid:18)(cid:19)(cid:20) (cid:20)(cid:21)(cid:22)(cid:11)
(cid:17) (cid:15)(cid:23)(cid:15)(cid:22)(cid:24) (cid:18)(cid:19)(cid:20) (cid:20)(cid:21)(cid:22)(cid:11)
(cid:25) (cid:18)(cid:26)(cid:23)(cid:27)(cid:15) (cid:16) (cid:22)(cid:14)(cid:14) (cid:17) (cid:26)(cid:21)(cid:15)
(cid:2)
(cid:8)
(cid:2)(cid:7)
(cid:3)(cid:5)
(cid:9) (cid:10)(cid:11)(cid:12)(cid:13)(cid:14)
(cid:7)(cid:1)
(cid:6)(cid:1)
(cid:5)(cid:1)
(cid:4)(cid:1)
(cid:3)(cid:1)
(cid:2)(cid:1)
(cid:1)
(a) Throughput-per-core (Gbps)
(b) Sender CPU breakdown
(c) CPU utilization (%)
Figure 7: Linux network stack performance for outcast traffic pattern. (a) Each column shows throughput-per-sender-core achieved for different
number of flows, that is the maximum throughput sustainable using a single sender core (we ignore receiver core utilization here). Throughput-per-sender-core
increases from 1 to 8 flows, and then decreases as the number of flows increases. (b) With all optimizations enabled, as the number of flows increases from 1
to 8, data copy overhead increases but does not change much when the number of flows is increased further. Refer to [7] for receiver-side CPU breakdown. (c)
For 1 flow, sender-side CPU is underutilised. Sender-side cache miss rate increases slightly as the number of flows increases from 8 to 24, increasing the
per-byte data copy overhead, and there is a corresponsing decrease in throughput-per-core. See §3.4 for description.
3.4 Increasing Sender Contention via Outcast
All our experiments so far result in receiver being the bottleneck.
To evaluate sender-side processing pipeline, we now use an outcast
scenario where a single sender core transmits an increasing number
of flows (1 to 24), each to a unique receiver core. To understand the
efficiency of sender-side processing pipeline, this subsection focuses
on throughput-per-sender-core: that is, the maximum throughput
achievable by a single sender core.
Sender-side processing pipeline can achieve up to 89Gbps
per core. Fig. 7(a) shows that, with increase in number of flows from
1 to 8, throughput-per-sender-core increases significantly enabling
total throughput as high as ∼89Gbps; in particular, throughput-per-
sender-core is 2.1× when compared to throughput-per-receiver-
core in the incast scenario (§3.3). This demonstrates that, in today’s
Linux network stack, sender-side processing pipeline is much more
CPU-efficient when compared to receiver-side processing pipeline.
We briefly discuss some insights below.
The first insight is related to the efficiency of TSO. As shown
in Fig. 7(a), TSO in the outcast scenario contributes more to
throughput-per-core improvements, when compared to GRO in
the incast scenario (§3.3). This is due to two reasons. First, TSO is a
hardware offload mechanism supported by the NIC; thus, unlike
GRO which is software-based, there are no CPU overheads associ-
ated with TSO processing. Second, unlike GRO, the effectiveness
of TSO does not degrade noticeably with increasing number of
flows since data from applications is always put into 64KB size
skbs independent of the number of flows. Note that Jumbo frames
do not help over TSO that much compared to the previous cases as
segmentation is now performed in the NIC.
Second, aRFS continues to provide significant benefits, contribut-
ing as much as ∼46% of the total throughput-per-sender-core. This
is because, as discussed earlier, L3 cache at the sender is always
warm: while cache miss rate increases slightly with larger number
of flows, the absolute number remains low (∼11% even with 24
flows); furthermore, outcast scenario ensures that not too many
flows compete for the same L3 cache at the receiver (due to receiver
cores distributed across multiple NUMA nodes). Fig. 7(b) shows
that data copy continues to be the dominant CPU consumer, even
when sender is the bottleneck.
3.5 Maximizing Contention with All-to-All
We now evaluate Linux network stack performance for all-to-all
traffic patterns, where each of x sender cores transmit a flow to each
of the x receiver cores, for x varying from 1 to 24. In this scenario,
we were unable to explicitly map IRQs to specific cores because,
for the largest number of flows (576), the number of flow steering
entries requires is larger than what can be installed on our NIC.
Nevertheless, even without explicit mapping, we observed reason-
ably deterministic results for this scenario since the randomness
across a large number of flows averages out.
Fig. 8(a) shows that throughput-per-core reduces by ∼67% going
from 1 × 1 to 24 × 24 flows, due to reduced effectiveness of all
optimizations. The benefits of aRFS drop by ∼64%, almost the same
as observed in the one-to-one scenario (§3.2). This is unsurprising,
given the lack of cache locality for cores in non-NIC-local NUMA
nodes, and given that cache miss rate is already abysmal (as dis-
cussed in §3.2). Increasing the number of flows per core on top of
this does not make things worse in terms of cache miss rate.
Per-flow batching opportunities reduce due to large number
of flows. Similar to the one-to-one case, the network link becomes
the bottleneck in this scenario, resulting in fair-sharing of band-
width among flows. Since there are a large number of flows (e.g.,
24 × 24 with 24 cores), each flow achieves very small throughput (or
alternatively, the number of packets received for any flow in a given
time window is very small). This results in reduced effectiveness of
optimizations like GRO (that operate on a per-flow basis) since they
do not have enough packets in each flow to aggregate. As a result,
upper layers receive a larger number of smaller skbs, increasing
packet processing overheads.
Fig. 8(c) shows the distribution of skb sizes (post-GRO) for vary-
ing number of flows. We see that as the number of flows increase,
the average skb size reduces, leading to our argument above about
the reduced effectiveness of GRO. We note that the above phenom-
enon is not unique to the all-to-all scenario: the number of flows
sharing a bottleneck resource also increase in the incast and one-
to-one scenarios. Indeed, this effect would also be present in those
scenarios, however the total number of flows in those cases is not
large enough to make these effects noticeable (max of 24 flows in
incast and one-to-one versus 24 × 24 flows in all-to-all).
72
)
s
p
b
G
(
e
r
o
C
r
e
P
t
u
p
h
g
u
o
r
h
T
 50
 40
 30
 20
 10
 0
No Opt.
TSO/GRO
Jumbo
aRFS
Total Thpt
 100
 80
 60
 40
 20
 0
)
s
p
b
G
(
t
u
p
h
g
u
o
r
h
T
l
a
t
o
T
1x1
8x8
16x16
24x24
# Flows
l
s
e
c
y
C
U
P
C
f
o
n
o
i
t
c
a
r
F
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
d
a t a   c
t c
p
1x1 ow
8x8 ows
16x16 ows
24x24 ows
s y s t e m
o
y
p
/ i p   p
e
c
o
e t d
r
n
g
s si n
vic
e
b
u
e  s
b   m g m t
s k
r y   a ll o
m e m
o
e
d
/
c
c
a ll o
l o
u
/
k
c
k
c