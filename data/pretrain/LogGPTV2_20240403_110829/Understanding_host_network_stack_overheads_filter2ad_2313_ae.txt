### 优化后的文本

#### 图7：Linux网络堆栈在outcast流量模式下的性能
- **(a) 每核吞吐量 (Gbps)**
  - 每列显示了不同流数下每发送核的吞吐量，即使用单个发送核可维持的最大吞吐量（忽略接收核利用率）。从1到8个流时，每发送核的吞吐量增加，随后随着流数的增加而减少。
- **(b) 发送方CPU细分**
  - 启用所有优化后，随着流数从1增加到8，数据复制开销增加，但当流数进一步增加时变化不大。关于接收方CPU细分，请参见[7]。
- **(c) CPU利用率 (%)**
  - 对于1个流，发送方CPU利用率不足。当流数从8增加到24时，发送方缓存未命中率略有增加，导致每字节数据复制开销增加，相应地，每核吞吐量下降。详情请参见§3.4。

#### 3.4 通过Outcast增加发送方竞争
- 我们迄今为止的所有实验都导致接收方成为瓶颈。为了评估发送方处理管道，我们使用了一个outcast场景，其中单个发送核传输越来越多的流（1到24），每个流到一个唯一的接收核。
- 为了理解发送方处理管道的效率，本小节重点关注每发送核的吞吐量，即单个发送核可实现的最大吞吐量。
- 发送方处理管道可以达到每核高达89 Gbps的吞吐量。图7(a)显示，随着流数从1增加到8，每发送核的吞吐量显著增加，总吞吐量最高可达约89 Gbps；特别是，与incast场景（§3.3）相比，每发送核的吞吐量是每接收核吞吐量的2.1倍。这表明，在当前的Linux网络堆栈中，发送方处理管道比接收方处理管道更高效。

##### 关键见解
- **TSO的效率**：如图7(a)所示，在outcast场景中，TSO对每核吞吐量的提升比incast场景中的GRO更为显著。原因有两个：首先，TSO是由NIC支持的硬件卸载机制，因此与基于软件的GRO不同，TSO处理没有CPU开销。其次，与GRO不同，TSO的有效性不会因流数增加而显著降低，因为应用程序的数据始终以64KB大小的skbs形式放入，与流数无关。注意，巨型帧在这种情况下对TSO的帮助不如以前那么大，因为分段现在是在NIC中进行的。
- **aRFS的持续效益**：aRFS继续提供显著的好处，贡献了约46%的总每发送核吞吐量。这是因为在前面讨论过的，发送方的L3缓存始终是热的：即使在较大数量的流下，缓存未命中率略有增加，但绝对值仍然很低（即使有24个流也仅为约11%）；此外，outcast场景确保不会有太多的流在同一接收方的L3缓存上竞争（由于接收核分布在多个NUMA节点上）。图7(b)显示，即使发送方成为瓶颈，数据复制仍然是主要的CPU消费者。

#### 3.5 通过全对全最大化竞争
- 现在我们评估Linux网络堆栈在全对全流量模式下的性能，其中每个x个发送核向x个接收核发送一个流，x从1到24不等。在这种情况下，我们无法显式地将IRQ映射到特定的核心，因为对于最大数量的流（576），所需的流转向条目数量超过了我们的NIC可以安装的数量。尽管如此，即使没有显式映射，我们也观察到了相当确定的结果，因为大量流的随机性会平均掉。
- 图8(a)显示，从1 × 1到24 × 24流，每核吞吐量减少了约67%，由于所有优化的效果减弱。aRFS的好处减少了约64%，几乎与一对一场景（§3.2）中观察到的一样。这并不令人惊讶，考虑到非NIC本地NUMA节点上的核心缺乏缓存局部性，并且缓存未命中率已经非常糟糕（如§3.2所述）。在此基础上增加每核的流数并不会使缓存未命中率变得更糟。
- 由于大量的流，每个流的批处理机会减少。类似于一对一的情况，网络链路在这个场景中成为瓶颈，导致带宽在各个流之间公平共享。由于有大量的流（例如，24 × 24，24个核心），每个流的吞吐量非常小（或者在给定时间窗口内每个流接收到的包数量非常少）。这导致了像GRO这样的优化效果减弱，因为它们没有足够的包来聚合。结果，上层接收到更多的小skbs，增加了包处理开销。
- 图8(c)显示了不同数量流的skb大小分布（GRO之后）。我们看到，随着流数的增加，平均skb大小减小，支持了上述关于GRO效果减弱的论点。我们注意到，上述现象不仅限于全对全场景：在incast和一对一场景中，共享瓶颈资源的流数也会增加。确实，这种效应在那些场景中也会存在，但由于这些场景中的总流数不够大，使得这些效应不太明显（incast和一对一最多24个流，而全对全为24 × 24个流）。

希望这些优化后的文本更加清晰、连贯和专业。如果有任何进一步的需求或修改，请告诉我！