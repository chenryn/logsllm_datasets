# Title: Automatic Evaluation of Intrusion Detection Systems

## Authors:
- Frédéric Massicotte, Canada Communication Research Center
- François Gagnon, Carleton University
- Yvan Labiche, Carleton University
- Lionel C. Briand, Carleton University
- Mathieu Couture, Carleton University

### Abstract
Intrusion Detection Systems (IDS) are a critical component of network security. Despite the availability of numerous IDS products, detailed evaluations of these systems are scarce, and the data used for testing is often proprietary. This limits the research community's ability to assess and improve the next generation of IDS. To address this, DARPA provided an Intrusion Detection Evaluation Data Set in 1998, 1999, and 2000, but no new datasets have been released since then, partly due to the complexity of the task. In this paper, we propose a strategy to generate a publicly available, well-documented dataset for testing and evaluating IDS. We also present a tool that automatically analyzes and evaluates IDS using our proposed dataset.

### 1. Introduction
Since the release of the DARPA Intrusion Detection Evaluation Data Set [1] in 1998 and its subsequent updates in 1999 and 2000, no other significant, publicly available dataset has been provided for benchmarking IDS. Other organizations [2, 3] offer datasets of traffic traces with attacks such as worms and denial-of-service (DoS) attacks. However, these datasets are primarily used for statistical and traffic behavior analysis and are not sufficiently documented for automated IDS testing and evaluation. They also lack a diverse range of attack instances and behaviors, limiting their utility for comprehensive IDS testing.

The National Institute of Standards and Technology (NIST) report [4] highlighted the need for well-documented, realistic, and freely shared datasets for IDS testing. Such datasets should include a wide variety of attack traces, which could be added to existing vulnerability databases to aid IDS researchers and developers.

Datasets for IDS testing can be categorized based on the type of intrusion detection technology (signature-based or anomaly-based) and the location of the IDS (host-based or network-based). The test cases required for different types of IDS vary significantly. For instance, the test cases for a signature-based network IDS differ from those for an anomaly-based host IDS.

In this paper, we introduce a Virtual Network Infrastructure (VNI) for generating traffic traces and an Intrusion Detection System Evaluation Framework (IDSEF) for automatic testing and evaluation of IDS. Our framework currently focuses on signature-based, network IDS and includes well-known attacks without background traffic. This initial focus allows us to validate the feasibility of our approach for thorough experimental evaluation of existing IDS. The primary goal is to test and evaluate the detection accuracy of IDS during successful and failed attack attempts.

We also report on an initial evaluation of our framework using two well-known IDS: Snort [5] and Bro [6]. The results are encouraging, demonstrating that our documented dataset can be used for automated testing and evaluation, and it identified several issues with both Snort and Bro.

### 2. Related Work
Various techniques are used to test IDS, and a classification of these techniques can be found in [7]. The main methods for testing IDS detection accuracy are the IDS stimulator approach and the vulnerability exploitation program approach.

#### 2.1 IDS Stimulators
Popular IDS stimulators, described in [8, 9, 10, 11], are used to generate false alarms, cross-test network-based IDS signatures, and test the IDS engine. These tools rely on publicly available signature databases to generate test cases. However, many necessary signatures are undisclosed or unavailable from vendors, limiting their effectiveness.

#### 2.2 Vulnerability Exploitation Programs
To overcome the limitations imposed by IDS vendors, vulnerability exploitation programs can be used to generate test cases. Techniques such as packet fragmentation can be applied to further test IDS accuracy. Popular IDS evasion techniques are detailed in [13–19].

Using vulnerability exploitation programs for IDS testing typically involves building a test bed where attacks are launched. The attack traffic can be combined with real or emulated normal traffic. The traffic is either recorded for off-line IDS testing or tested in real-time on the test bed network.

Several organizations and projects [1, 12-13, 16, 20-24] have developed such test beds and techniques. However, we identified three major issues with the datasets used: availability, documentation, and generation processes. Most datasets, except those from DARPA, are not publicly available. The DARPA datasets, though still used, contain no recent attacks and have been criticized for their normal traffic generation techniques.

Documentation is a key issue with available traffic traces. For effective IDS testing and evaluation, it is essential to have a well-documented dataset that includes details about the targeted system configuration and the attack specification. Manual or semi-automated generation and use of traffic traces limit the diversity and updatability of the dataset.

#### 2.3 Proposed Solution
Our proposed solution aims to address these limitations. We introduce a VNI that can generate a shareable, rapidly updated, and well-documented dataset for IDS testing and evaluation. Our system is fully automated and can generate a dataset in hours, allowing for the quick reversion of the test network to its initial state after each attack. Our VNI can efficiently generate a large dataset with hundreds of vulnerability exploitation programs launched against a wide variety of target systems. It is flexible, allowing for the application or non-application of IDS evasion techniques, and can be used for other purposes, such as fingerprinting operating systems [26] and providing data for projects like LEURRE [27] and SCRIPTGEN [28].

### 3. Virtual Network Infrastructure Overview
In this section, we outline the requirements for our infrastructure, discuss our design choices, and describe the collection process and current contents of our dataset.

#### 3.1 Infrastructure Requirements
To create a large-scale dataset, a controlled network infrastructure must allow:
1. Recording of all network traffic.
2. Control of network traffic noise.
3. Control of attack propagation.
4. Use of real and heterogeneous system configurations.
5. Fast recovery to initial conditions after an attack.

We developed a controlled virtual network using VMware 5.0 [31], which provides a virtual broadcasting environment, traffic propagation control, and efficient resource management. VMware allows the capture of all communications, creation of clean traces, confinement of attack propagation, and rapid deployment of custom network configurations. VMware snapshots enable the restoration of the test network to its initial state before each attack.

#### 3.2 Collection Process
Our virtual network, shown in Figure 1, includes attack systems (Attacker), target systems (Target), and network infrastructure services such as DNS and mail servers. The attack systems launch attacks using vulnerability exploitation programs, with or without IDS evasion techniques, and capture the generated packets. The network infrastructure ensures necessary communications during the attack.

The collection process involves the following steps:
1. **Script Generation**: This process selects the vulnerability exploitation program (VEP) to run against a given target system and configures it. We built a database containing the complete system configuration for each target template and the ports targeted by the VEPs.
2. **Virtual Network Setup**: A different virtual network is created for each script, including the target and attacking virtual machines and necessary network services. The coordinator opens the virtual network and locks the resources.
3. **Attack Execution**: The VEP is executed, and the generated traffic is captured.
4. **Data Collection**: The captured traffic is saved and documented.
5. **Network Restoration**: The test network is restored to its initial state using VMware snapshots.

#### 3.3 Traffic Trace Documentation
Each attack in the dataset is documented with key information such as the targeted system configuration, attack specification, and vulnerability exploitation program details. This documentation is crucial for automating IDS testing and evaluation.

#### 3.4 Current Contents of the Dataset
Our current dataset is specific to signature-based, network IDS and includes well-known attacks without background traffic. The dataset is extensible and can be updated to include new attack scenarios, IDS evasion techniques, and target systems.

### 4. Intrusion Detection System Evaluation Framework (IDSEF)
The IDSEF is designed to automatically test and evaluate IDS using the traffic traces generated by our VNI. The framework includes an automatic analysis of the results, providing detailed insights into the performance of the IDS.

### 5. Evaluation Results
We evaluated our framework using Snort [5] and Bro [6]. The results showed that our documented dataset can be effectively used for automated testing and evaluation. The evaluation also identified several issues with both Snort and Bro, demonstrating the utility of our approach.

### 6. Conclusion and Future Work
This paper presents a VNI and an IDSEF for generating and evaluating IDS. Our framework generates a large, well-documented, and publicly available dataset, which can be used for automated IDS testing and evaluation. Future work will focus on expanding the dataset to include more diverse attack scenarios and background traffic, and on further improving the IDSEF for more comprehensive IDS evaluation.

---

**References:**
1. [DARPA Intrusion Detection Evaluation Data Set]
2. [Organization 2]
3. [Organization 3]
4. [NIST Report]
5. [Snort Reference]
6. [Bro Reference]
7. [Classification of Testing Techniques]
8. [IDS Stimulator 1]
9. [IDS Stimulator 2]
10. [IDS Stimulator 3]
11. [IDS Stimulator 4]
12. [NSS IDS Evaluation]
13. [IDS Evasion Techniques 1]
14. [IDS Evasion Techniques 2]
15. [IDS Evasion Techniques 3]
16. [Project 1]
17. [Project 2]
18. [Project 3]
19. [Project 4]
20. [Project 5]
21. [Project 6]
22. [Project 7]
23. [Project 8]
24. [Project 9]
25. [Criticism of Normal Traffic Generation]
26. [Fingerprinting Operating Systems]
27. [LEURRE Project]
28. [SCRIPTGEN Project]
29. [Virtual Network Construction 1]
30. [Virtual Network Construction 2]
31. [VMware 5.0]
32. [Alternative Virtualization Software 1]
33. [Alternative Virtualization Software 2]

---

**Proceedings of the 22nd Annual Computer Security Applications Conference (ACSAC'06)**
0-7695-2716-7/06 $20.00 © 2006