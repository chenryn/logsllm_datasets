aﬂ. Moreover, using backward slicing from familiar taint sinks such as memcmp, we
have been able to extract protocol fragments (such as the string literal America
Online Inc. used by nDPI to ﬁngerprint instant messaging traﬃc) that have
been instrumental in increasing test coverage.
Listing 1.2. Syntactic query issued on nDPI and libxml2 codebases. The query returns
string literals passed as arguments to taint sinks such as strcmp.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
// O b t a i n s t r i n g l i t e r a l s p a s s e d to P O S I X A P I s " s t r c m p " , and
// " m e m c m p " , and l i b x m l 2 A P I s " x m l B u f f e r W r i t e C h a r " , and
// " x m l O u t p u t B u f f e r W r i t e ".
S t a t e m e n t M a t c h e r S t r i n g M a t c h e r =
s t r i n g L i t e r a l (
h a s A n c e s t o r (
d e c l R e f E x p r (
to ( n a m e d D e c l (
anyOf ( hasName ( " strcmp " ) ,
hasName ( " memcmp " ) ,
h a s N a m e ( " x m l B u f f e r W r i t e C h a r " ) ,
h a s N a m e ( " x m l O u t p u t B u f f e r W r i t e " )
)
)
)
)
). bind ( " c o n s t r u c t " );
)
4.2 Benchmarks: Time to Vulnerability Exposure
To enable independent reproduction, we brieﬂy document our evaluation
methodology.
Fuzzer Test Suite. In order to measure the time required to expose program
vulnerabilities, we used the fuzzer test suite [15]. The fuzzer test suite is well-
suited for this purpose because it provides a controlled environment in which
timing measurements can be done, and contains test cases for several known
high-proﬁle vulnerabilities. Indeed, the test suite has been used for benchmarking
the LLVM libFuzzer [21], that we use as a baseline in our evaluation. The speciﬁc
vulnerabilities in the test suite that feature in our evaluation are: CVE-2014-
0160 [23] (OpenSSL Heartbleed), CVE-2016-5180 [25] (buﬀer overﬂow in the c-
ares dns library), CVE-2015-8317 [24] (buﬀer overﬂow in libxml2), and a security-
critical bug in Google’s WoFF2 font parser [6].
Test Methodology. For each test case, our evaluation was performed by mea-
suring the time to expose the underlying vulnerability in two scenarios: (i) The
baseline fuzzer alone; and (ii) The baseline fuzzer augmented with an Orthrus
generated dictionary. Our approach is deemed eﬀective when the time to expose
vulnerability reduces in comparison to the baseline, and is ineﬀective/irrelevant
when it increases or remains the same in comparison to the baseline. Timing
measurements were done using Unix’s time utility. In order to reduce the eﬀect
of seemingly random vulnerability exposures, we obtained at least 80 timing
38
B. Shastry et al.
CVE-2014-0160
CVE-2016-5180
)
s
(
e
m
T
i
30
20
10
0
)
s
(
e
m
T
i
104
103
102
101
CVE-2015-8317
0.8
0.6
0.4
0.2
)
s
(
e
m
T
i
)
m
(
e
m
T
i
50
40
30
20
10
0
WOFF2 Bug 1
Baseline
Orthrus
Baseline
Orthrus
Fig. 2. Comparison of time required to expose vulnerability using libFuzzer as the
baseline.
measurements for each test case in both scenarios. Measurements for each test
case were carried out in parallel, with each experiment being run exclusively on a
single core. The input dictionary generated by Orthrus was supplied to libFuzzer
via the -dict command line argument. Finally, to eliminate the eﬀect of seed
corpuses on measurement outcome, we strictly adhered to the selection of seed
corpuses as mandated by the fuzzer test suite documentation.
Results. Figure 2 presents our test results as box plots. The baseline box plot
(libFuzzer) is always on the left of the plot, and results for libFuzzer augmented
with Orthrus (Orthrus) on the right. The Orthrus generated input dictionary
brought down the time to expose a buﬀer overﬂow in the libxml2 library (CVE-
2015-8317) by an order of magnitude (from a median value of close to 3 h using
the baseline to a median value of 5 min using our approach). For all the other
test cases, the median time to expose vulnerability was lower for Orthrus in com-
parison to libFuzzer. In addition, Orthrus shrunk the range of timing variations
in exposing the vulnerability.
To understand the varying impact of the supplied dictionary on the time to
vulnerability exposure, we studied each of the tested vulnerabilities to under-
stand their root cause. Our approach consistently brought down the time to
exposure for all vulnerabilities that were triggered by a ﬁle or protocol mes-
sage speciﬁc to the application under test. Thus, our approach worked well in
scenarios where knowledge of the input format was crucial to eliciting the vulner-
ability. Furthermore, in scenarios where our approach did not substantially lower
the time to vulnerability exposure, the time penalty incurred by our approach,
Static Program Analysis as a Fuzzing Aid
39
owing to the test time dedicated to dictionary mutations, was marginal. In sum-
mary, we ﬁnd that static program analysis can improve bug-ﬁnding eﬃciency
of fuzzers for those class of bugs that are triggered by highly structured input
(commonly found in network applications, and ﬁle format parsers), while not
imposing a noticeable performance penalty.
4.3 Case Study
To investigate the practical utility of Orthrus, we conducted a case study of two
popular network applications, namely, nDPI, and tcpdump. These applications
were selected because they are not only deployed in security-critical environ-
ments but also parse potentially attacker-controlled data. For each application,
we conducted multivariate testing using baseline fuzzers such as aﬂ and aﬂfast [3]
with and without an Orthrus generated dictionary.
The chosen applications were also fuzzed using the Peach fuzzer [29], a state-
of-the-art fuzzer for protocol security assessments. Since grammar speciﬁcations
for the set of protocols parsed by tcpdump, and nDPI were not publicly available,
we enabled Peach fuzzer’s input analyzer mode that automatically infers the
input data model. Such an evaluation was aimed at comparing Peach fuzzer with
Orthrus in scenarios where a data model speciﬁcation is not available. However,
the community edition of the Peach fuzzer that we had access to, is not geared
toward long runs. In our Peach-based experiments, we could not achieve a run
time of longer than 24 h. This prevents a fair comparison of the two approaches.
Therefore, we document results of our Peach experiments for reference, and not
a comparative evaluation.
Evaluation Methodology. We evaluated Orthrus using two metrics, namely,
test coverage achieved, and the number of program vulnerabilities exposed. Test
coverage was measured as the percentage of program branches that were dis-
covered during testing. Since fuzzers often expose identical crashes, making it
non-trivial to document unique vulnerabilities, we semi-automatically dedupli-
cated fuzzer crashes in a two-step process. First, we used the concept of fuzzy
stack hashes [26] to ﬁngerprint a crash’s stack trace using a cryptographic hash
function. Second, crashes with a unique hash were manually triaged to deter-
mine the number of unique program vulnerabilities. We used two elementary
seeds (bare-bone IPv4, and IPv6 packets) to fuzz tcpdump, and nDPI. Tests
involving the fuzzers aﬂ and aﬂfast were conducted in a multi-core setting.
Fuzzing Duration. Dictionary based mutations get a fraction of the total
fuzz time of a fuzzer. Thus, to fully evaluate our approach, we ran the fuzzer
conﬁgurations (except Peach) until each unique program input synthesized by
the fuzzer was mutated with the supplied dictionary constructs at least once.
Owing to the relatively poor execution throughput of the evaluated software
(under 100 executions per second), we had to run each fuzzer over a period of
40
B. Shastry et al.
Table 3. Test coverage achieved (in %) by diﬀerent fuzzing conﬁgurations.
Software aﬂ
aﬂ-orthrus
aﬂfast aﬂfast-orthrus Peach-analyzer
tcpdump 80.56 90.23 (+9.67)
71.35
78.82 (+7.47)
6.25
nDPI
66.92 81.49 (+14.57) 64.40
68.10 (+3.70) 24.98
1 week in which time the supplied dictionary was utilized at least once for each
unique input.
Utilities. CERT’s exploitable [11] utility was used for crash deduplication. We
used AddressSanitizer [2] as a debugging aid; this expedited the bug reporting
process.
Evaluated Software. We evaluated nDPI revision f51fef6 (November 2016),
and tcpdump trunk (March 2017).
Test Coverage. Our test coverage measurements present the fraction of all
program branches (edges) covered by test cases generated by a fuzzer conﬁgura-
tion. We have evaluated Orthrus against two baselines, namely, aﬂ, and aﬂfast.
Therefore, our measurements have been obtained for aﬂ, aﬂ augmented with
Orthrus-generated input dictionary (aﬂ-Orthrus), aﬂfast, aﬂfast augmented with
Orthrus-generated input dictionary (aﬂfast-Orthrus), and the Peach fuzzer with
a binary analyzer data model. Table 3 shows the test coverage achieved by dif-
ferent fuzzer combinations for tcpdump, and nDPI, while Fig. 3 visualizes code
coverage over time. Program coverage was measured when there was a change in
its magnitude. Due to the relatively short running duration of the Peach fuzzer,
we have excluded its coverage visualization.
As shown in Fig. 3, the obtained coverage measurements for tcpdump, and
nDPI, approach a saturation point asymptotically. For both tcpdump, and nDPI,
the growth rate in test coverage is higher initially, tapering oﬀ asymptotically
to zero. The test coverage curves for aﬂ-Orthrus and aﬂfast-Orthrus have a
higher initial growth rate compared to their respective baselines, namely, aﬂ,
and aﬂfast. This results in a consistent increase in overall test coverage achieved
by Orthrus in comparison to the baseline fuzzers, as shown in Table 3. For nDPI,
Orthrus’ input dictionary increases test coverage by 14.57% over the aﬂ fuzzer.
In the case of tcpdump, this increase in test coverage is 9.67%. Orthrus’ enhance-
ments in test coverage over aﬂfast for nDPI, and tcpdump are 3.7%, and 7.47%
respectively. Although aﬂfast is a fork of aﬂ, the supplied input dictionary has
a lesser eﬀect on the former than the latter. To understand this anomaly, we
examined the source code of aﬂ, and aﬂfast. aﬂ performs dictionary-based muta-
tions on all inputs in the fuzzer queue at least once. However, aﬂfast performs
dictionary-based mutations on a given input in the queue, only when the input’s
performance score (computed by the aﬂfast algorithm) is above a certain thresh-
old. We determined that the threshold used by aﬂfast is too aggressive, resulting
in too few inputs in the fuzzer queue undergoing dictionary mutations.
Static Program Analysis as a Fuzzing Aid
41
afl
afl-Orthrus
aflfast
aflfast-Orthrus
)
%
(
e
g
a
r
e
v
o
c
m
a
r
g
o
r
P
)
%
(
e
g
a
r
e
v
o
c
m
a
r
g
o
r
P
100
80
60
40
20
0
100
80
60
40
20
0
Mar 09
Mar 10
Mar 11
Mar 12
Mar 13
Mar 14
Mar 15