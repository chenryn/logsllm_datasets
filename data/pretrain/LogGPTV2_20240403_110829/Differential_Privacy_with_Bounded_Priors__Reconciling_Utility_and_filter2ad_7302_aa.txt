title:Differential Privacy with Bounded Priors: Reconciling Utility and
Privacy in Genome-Wide Association Studies
author:Florian Tramèr and
Zhicong Huang and
Jean-Pierre Hubaux and
Erman Ayday
Differential Privacy with Bounded Priors:
Reconciling Utility and Privacy in Genome-Wide
Association Studies
Florian Tramèr
Zhicong Huang
Jean-Pierre Hubaux
School of IC, EPFL
ﬁrstname.lastname@epﬂ.ch
∗
Erman Ayday
Computer Engineering Department
Bilkent University
PI:EMAIL
ABSTRACT
Diﬀerential privacy (DP) has become widely accepted as a
rigorous deﬁnition of data privacy, with stronger privacy
guarantees than traditional statistical methods. However,
recent studies have shown that for reasonable privacy bud-
gets, diﬀerential privacy signiﬁcantly aﬀects the expected
utility. Many alternative privacy notions which aim at relax-
ing DP have since been proposed, with the hope of providing
a better tradeoﬀ between privacy and utility.
At CCS’13, Li et al. introduced the membership privacy
framework, wherein they aim at protecting against set mem-
bership disclosure by adversaries whose prior knowledge is
captured by a family of probability distributions. In the con-
text of this framework, we investigate a relaxation of DP, by
considering prior distributions that capture more reasonable
amounts of background knowledge. We show that for diﬀer-
ent privacy budgets, DP can be used to achieve membership
privacy for various adversarial settings, thus leading to an
interesting tradeoﬀ between privacy guarantees and utility.
We re-evaluate methods for releasing diﬀerentially private
χ2-statistics in genome-wide association studies and show
that we can achieve a higher utility than in previous works,
while still guaranteeing membership privacy in a relevant
adversarial setting.
Categories and Subject Descriptors
K.4.1 [Computer and Society]: Public Policy Issues—
Privacy; C.2.0 [Computer-Communication Networks]:
General—Security and protection; J.3 [Life and Medical
Sciences]: Biology and genetics
Keywords
Diﬀerential Privacy; Membership Privacy; GWAS; Genomic
Privacy; Data-Driven Medicine
∗Part of this work was done while the author was at EPFL.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
CCS’15, October 12–16, 2015, Denver, Colorado, USA.
c(cid:13) 2015 ACM. ISBN 978-1-4503-3832-5/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2810103.2813610.
1.
INTRODUCTION
The notion of diﬀerential privacy, introduced by Dwork
et al. [4, 5], provides a strong and rigorous deﬁnition of
data privacy. A probabilistic mechanism A is said to sat-
isfy -diﬀerential privacy (-DP), if for any two neighboring
datasets T and T (cid:48), the probability distribution of the out-
puts A(T ) and A(T (cid:48)) diﬀer at most by a multiplicative fac-
tor e. Depending on the deﬁnition of neighboring datasets,
we refer to either unbounded -DP or bounded -DP. Informally,
satisfying diﬀerential privacy ensures that an adversary can-
not tell with high conﬁdence whether an entity t is part of
a dataset or not, even if the adversary has complete knowl-
edge over t’s data, as well as over all the other entities in
the dataset. The relevance of such a strong adversarial set-
ting has been put into question because it seems unlikely,
in a practical data setting, for an adversary to have such a
high certainty about all entities. Alternative privacy deﬁni-
tions such as diﬀerential-privacy under sampling [13], crowd-
blending privacy [8], coupled-worlds privacy [2], outlier pri-
vacy [15], -privacy [16], or diﬀerential identiﬁability [12]
relax the adversarial setting of DP, with the goal of achiev-
ing higher utility.
This line of work is partially in response to the ﬂow of re-
cent results, for example in medical research, which show
that satisfying diﬀerential privacy for reasonable privacy
budgets leads to an signiﬁcant drop in utility. For instance,
Fredrikson et al. [6] investigate personalized warfarin dosing
and demonstrate that for privacy budgets eﬀective against a
certain type of inference attacks, satisfying DP exposes pa-
tients to highly increased mortality risks. Similarly, studies
on privacy in genome-wide association studies (GWAS) [10,
19, 22] consider diﬀerential privacy as a protective measure
against an inference attack discovered by Homer et al. [9,
20]. These works show that for reasonably small values of ,
the medical utility is essentially null under DP, unless there
is an access to impractically large patient datasets.
Membership Privacy.
We present an alternative characterization of diﬀerential
privacy, by considering weaker adversarial models in the con-
text of the positive membership-privacy (PMP) framework
introduced by Li et al. [14]. Their privacy notion aims at
preventing positive membership disclosure, meaning that an
adversary should not be able to signiﬁcantly increase his be-
lief that an entity belongs to a dataset. The privacy guar-
antee is with respect to a distribution family D, that cap-
tures an adversary’s prior knowledge about the dataset. If
a mechanism A satisﬁes γ-positive membership-privacy un-
der a family of distributions D, denoted (γ, D)-PMP, then
any adversary with a prior in D has a posterior belief upper-
bounded in terms of the prior and the privacy parameter
γ. The power of this framework lies in the ability to model
diﬀerent privacy notions, by considering diﬀerent families of
distributions capturing the adversary’s prior knowledge. For
instance, Li et al. show that -DP is equivalent to e-PMP
under a family of ‘mutually independent distributions’ (de-
noted either DI for unbounded -DP or DB for bounded -DP).
Similarly, privacy notions such as diﬀerential identiﬁability
or diﬀerential-privacy under sampling can also be seen as
instantiations of the PMP framework for particular distri-
bution families.
Bounded Adversarial Priors.
Our approach at relaxing the adversarial setting of DP is
based on the observation that the families of mutually inde-
pendent distributions DI and DB contain priors that assign
arbitrarily high or low probabilities to all entities. This cap-
tures the fact that DP protects the privacy of an entity, even
against adversaries with complete certainty about all other
entities in the dataset, as well as some arbitrary (but not
complete) certainty about the entity itself.
A natural relaxation we consider is to limit our adversar-
ial model to mutually independent distributions that assign
bounded prior probabilities to each entity. More formally, for
constants 0  ) suﬃces to satisfy e-
PMP if the priors are bounded. Therefore, we introduce an
alternative privacy-utility tradeoﬀ, in which the data pertur-
bation, and the utility loss, depend on the range of priors for
which we guarantee a given level of PMP. This leads to an
interesting model for the selection of the DP privacy param-
eter, in which we ﬁrst identify a relevant adversarial setting
and corresponding level of PMP, and then select the value 
such that these speciﬁc privacy guarantees hold.
Let’s consider an interesting sub-case of our model of
bounded prior distributions, where we let a get close to b;
this corresponds to a setting where an adversary’s prior be-
lief about an entity’s presence in the dataset tends to uni-
form, for those entities whose privacy is not already breached
apriori. Although this adversarial model seems simplistic,
we argue that certain relevant privacy threats, such as the
Figure 1: Level of -DP guaranteeing 2-PMP for the family
of mutually independent distributions with priors bounded
between a and b.
attack on genomic studies by Homer et al. [9, 20], can be
seen as particular instantiations of it. We show that pro-
tecting against such adversaries is, quite intuitively, much
easier than against adversaries with unbounded priors. In
Figure 1, we illustrate how the DP budget  evolves, if our
goal is to satisfy 2-PMP for priors ranging from a uniform
belief of 1
2 for each uncertain entity, to a general unbounded
prior (DB or DI ). The ﬁgure should be read as follows: If
the priors are arbitrary (pt ∈ [0, 1]), then 2-PMP is guaran-
teed by satisfying (ln 2)-DP. If the priors are uniformly 1
2 ,
then satisfying (ln 3)-DP suﬃces. Note that for a prior of 1
2 ,
the deﬁnition of 2-PMP (see Deﬁnition 5) guarantees that
the adversary’s posterior belief that an uncertain entity is
in the dataset is at most 3
4 .
Result Assessment and Implications.
To assess the potential gain in utility of our relaxation, we
focus on a particular application of DP, by re-evaluating the
privacy protecting mechanisms in genome-wide association
studies [10, 19, 22] for the release of SNPs with high χ2-
statistics. Our results show that, for a bounded adversarial
model, we require up to 2500 fewer patients in the study, in
order to reach an acceptable tradeoﬀ between privacy and
medical utility. As patient data is usually expensive and
hard to obtain, this shows that a more careful analysis of
the adversarial setting in a GWAS can signiﬁcantly increase
the practicality of known privacy preserving mechanisms.
As our theoretical results are not limited to the case of
genomic studies, we believe that our characterization of DP
for bounded adversarial models could be applied to many
other scenarios, where bounded- or unbounded-DP has been
considered as a privacy notion.
2. NOTATIONS AND PRELIMINARIES
We will retain most of the notation introduced for the
membership-privacy framework in [14]. The universe of en-
tities is denoted U. An entity t ∈ U corresponds to a physical
entity for which we want to provide some privacy-protection
22.22.42.62.83exp(ǫ)[12,12][38,58][14,34][18,78][0,1][a,b]A
U
t
T
D
T
D
DI
DB
Da

γ
List of symbols
A privacy preserving mechanism
The universe of entities
An entity in the universe U
A subset of entities in U that make up the dataset
A probability distribution over 2U , representing
the prior belief of some adversary about T
A random variable drawn from D (the adversary’s
prior belief about T )
A set of probability distributions
The set of mutually independent distributions
The set of bounded mutually independent distri-
butions
priors in [a, b] ∪ {0, 1} to all entities
Equivalent to D[a,a]
Privacy parameter for DP
Privacy parameter for PMP
D[a,b] A subset of D, in which all distributions assign
guarantees. A dataset is generated from the data associ-
ated with a subset of entities T ⊆ U. By abuse of notation,
we will usually simply denote the dataset as T .
In order
to model an adversary’s prior belief about the contents of
the dataset, we consider probability distributions D over 2U
(the powerset of U). From the point of view of the adver-
sary, the dataset is a random variable T drawn from D.
Its prior belief that some entity t is in the dataset is then
given by PrD[t ∈ T].
In order to capture a range of ad-
versarial prior beliefs, we consider a family of probability
distributions. We denote a set of probability distributions
by D. Each distribution D ∈ D corresponds to a particular
adversarial prior we protect against. We denote a proba-
bilistic privacy-preserving mechanism as A. On a particular
dataset T , the mechanism’s output A(T ) is thus a random
variable. We denote by range(A) the set of possible values
taken by A(T ), for any T ⊆ U .
2.1 Differential Privacy
Diﬀerential privacy provides privacy guarantees that de-
pend solely on the privacy mechanism considered, and not on
the particular dataset to be protected. Informally, DP guar-
antees that an entity’s decision to add its data to a dataset
(or to remove it) does not signiﬁcantly alter the output dis-
tribution of the privacy mechanism.
Deﬁnition 1 (Diﬀerential Privacy [4, 5]). A mechanism
A provides -diﬀerential privacy if and only if for any two
datasets T1 and T2 diﬀering in a single element, and any
S ⊆ range(A), we have
Pr [A(T1) ∈ S] ≤ e · Pr [A(T2) ∈ S] .
(1)
Note that the above deﬁnition relies on the notion of
datasets diﬀering in a single element, also known as neigh-
boring datasets. There exist two main deﬁnitions of neigh-
boring datasets, corresponding to the notions of unbounded
and bounded diﬀerential-privacy.
Deﬁnition 2 (Bounded DP [4]). In bounded diﬀerential-
privacy, datasets T1 and T2 are neighbors if and only if
|T1| = |T2| = k and |T1 ∩ T2| = k − 1. Informally, T1 is
obtained from T2 by replacing one data entry by another.
Deﬁnition 3 (Unbounded Diﬀerential-Privacy [5]). In un-
bounded diﬀerential-privacy, datasets T1 and T2 are neigh-
bors if and only if T1 = T2 ∪ {t} or T1 = T2 \ {t}, for some
entity t. Informally, T1 is obtained by either adding to, or
removing an data entry from T2.
In this work, we consider two standard methods to achieve
-DP, the so-called Laplace and exponential mechanisms.
We ﬁrst introduce the sensitivity of a function f : 2U → Rn;
it characterizes the largest possible change in the value of f ,
when one data element is replaced.