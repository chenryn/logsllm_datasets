once and aggregate VirusTotal labels using a threshold t, as
discussed in Section 2). In this section, we focus on the main
dataset for our analysis.
4.1 Hazard Flips and Non-Hazard Flips
In our context, we model the label dynamics in a form similar
to logic signals as a sequence of “0” and “1”. More specif-
ically, given a ﬁle f and a VirusTotal engine i, the label we
obtained daily can be formulated as Si, f = [l1,l2, ...,lt , ...,lN]
where N is the total number of days of data collection, lt= 0
(benign) or 1 (malicious). A ﬂip refers to a change between
two consecutive labels, namely “01” or “10”. In total, we have
2,571,809 ﬂips in the main dataset.
We observe an interesting phenomenon, which we call
“hazard ﬂip”. Given a ﬁle, a VirusTotal engine would some-
times ﬂip its label and then quickly change it back the next
day. We take the term “hazard” from Digital Circuit, which
originally represents the temporary ﬂuctuation in the output
of the circuit [17]. In our case, hazard refers to a temporary
glitch or ﬂip in the labels, namely “010” or “101”. More for-
mally, we deﬁne a label ﬂip as either “0 → 1” or “1 → 0”.
Thus a hazard would contain two ﬂips. We call the two ﬂips
in a hazard “hazard ﬂips”. Any other ﬂips are referred to as
non-hazard ﬂips. In total, we have 1,760,484 hazard ﬂips and
811,325 non-hazard ﬂips.
Figure 2 shows an example. Given a speciﬁc ﬁle
(MD5: e8799a459bdea599d1bc1615f4b746de), the original label
sequence we obtained from AegisLab is shown in Figure 2(a).
After removing hazard ﬂips, the label sequence only con-
taining non-hazard ﬂips is shown in Figure 2(b). We can
see that the original label sequence contains many hazards,
and some of them last for multiple days. To capture such
consecutive hazards, we search each label sequence chrono-
logically and always try to extend an identiﬁed hazard. For
example, from November 1st, 2018 to November 14th, 2018,
the label sequence of AegisLab is “00010101010010”. We
identify two hazards from it, one is “010101010” and the
USENIX Association
29th USENIX Security Symposium    2365
0100200300day01label0100200300day01label(a) CDF of a ﬁle’s ﬂips
and hazard ﬂips
(b) # of ﬂips per week
over time
(c) CDF of an engine’s
ﬂips and hazard ﬂips
Figure 3: Characteristics of ﬂips and hazard ﬂips.
other one is “010”. In total, we ﬁnd 737,338 hazards with
length three (“010” and “101”), 54,801 hazards with length
ﬁve (“01010” and “10101”), and 8,297 hazards with length
seven. The longest hazard lasts for 19 days.
Observation 1: More than half of the ﬂips on VirusTotal are
hazard ﬂips. Hazard ﬂips can be identiﬁed by submitting the
same ﬁle to VirusTotal in three consecutive days.
4.2 Characteristics of Flips
Next, we analyze the characteristics of ﬂips (including hazard
ﬂips) across ﬁles, dates, and engines.
Distribution Across Files.
In total, 1,352 ﬁles (9%) in the
main dataset do not contain any ﬂip. For these ﬁles, all engines
always label them as “benign” throughout our data collection
period. Recall that 7,234 ﬁles were labeled as benign by all
engines on the ﬁrst day. As such, if a ﬁle is labeled as benign
by all vendors when ﬁrstly submitted to VirusTotal, the prob-
ability that there will be no ﬂip on this ﬁle is 19%. If a ﬁle is
labeled as “malicious” by any engine on day-1, at least one
ﬂip happens later on the ﬁle. For ﬁles with ﬂips, on average
each ﬁle contains 196 ﬂips.
Figure 3(a) shows the CDF of a ﬁle’s normalized number
of ﬂips. We normalize a ﬁle’s ﬂips using the maximum num-
ber of ﬂips on a single ﬁle (which is 1,054). We ﬁnd 6,723
(46.61%) ﬁles have less than 10 ﬂips (1% of the maximum
number of ﬂips), and thus the drawn CDF is close to the y-axis
in the beginning. We also draw the CDF for a ﬁle’s hazard
ﬂips (the blue line) which has a similar trend. It turns out that
hazard ﬂips and non-hazard ﬂips are highly correlated. We
rank ﬁles based on their hazard ﬂips and non-hazard ﬂips and
compute the Spearman’s correlation coefﬁcient [28] between
the two rankings. The coefﬁcient is 0.87 with a p-value less
than 0.01, indicating that ﬁles with more hazard ﬂips are more
likely to have more non-hazard ﬂips.
Distribution over Time. Figure 3(b) shows the number of
ﬂips each week during our data collection time window. We
have ﬂips in all 57 weeks. We encountered technical issues
in week-12 and week-13 (with some data loss), so that there
are fewer ﬂips in these two weeks. On average, each week
has 45,119 ﬂips, and the ﬁrst week has the highest number of
ﬂips. Similarly, we also have hazard ﬂips every week.
Distribution Across Engines.
Flips are generated by 64
out of the 65 engines. Avast-Mobile labels all samples as be-
nign, and it is the only engine not having ﬂips. Figure 3(c)
shows the CDF of an engine’s normalized number of ﬂips.
We normalize each engine’s ﬂips using the maximum num-
ber of ﬂips from a single engine. The curve is skewed to the
left, indicating that a small group of engines contributes to
the majority of the ﬂips. For the 64 engines with ﬂips, on
average each of them contributes 40,184 (1.56%) ﬂips. How-
ever, AegisLab reports 359,221 (13.96%) ﬂips by itself, and
it is the engine with the most ﬂips. F-Secure is ranked as the
2nd (297,973 ﬂips), and VIPRE is ranked as the 3rd (233,875
ﬂips). Again, the CDF of hazard ﬂips has a similar trend. We
compute the Spearman’s correlation coefﬁcient to examine if
engines with more hazard ﬂips are likely to have more non-
hazard ﬂips. The computed coefﬁcient is 0.75 with a p-value
less than 0.01, conﬁrming a strong correlation.
Observation 2: Both ﬂips and hazard ﬂips widely exist
across ﬁles, scan dates and engines.
Inferring Root Causes of Flips
4.3
We tested whether querying VirusTotal API multiple times
can resolve (hazard) ﬂips. We found that repeated queries can
only address very limited ﬂips, and conﬁrmed that ﬂips are
more likely to be caused by internal problems of VirusTotal.
To categorize detailed root causes for ﬂips, we mainly use
the “update date” and “version information” of each engine
used in a scan, provided in VirusTotal responses. Given a
ﬂip (l1,l2) (l1 (cid:54)= l2) generated by engine i, we use (u1,u2) to
represent the engine’s last update dates and use (v1,v2) to
represent the engine versions when i scanned the ﬁle. The
causes of ﬂips are categorized as follows.
Most commonly, a ﬂip happens when the engine made a
model update, representing a decision-change of the engine.
1,710,565 (67%) ﬂips belong to this category where u1  u2). For example, CrowdStrike has
hazards on 3,739 ﬁles on day-175. After inspecting the update
information on the corresponding three days (u1, u2, u3), we
ﬁnd that for most ﬁles, u1 is equal to u3, but u2 is much larger
2366    29th USENIX Security Symposium
USENIX Association
255075100normd. flips per file0255075100% of fileshazard flipsall flips153045weeks02468# of flips (10K)255075100normd. flips per engine0255075100% of engineshas hazards on 3,739 ﬁles on day-175 (reasons discussed in
Section 4.3). The percentage starts to increase very quickly
around day-350, mainly because the time period between x
and the end of data collection is too small. Indeed, it is possi-
ble that ﬂips can still happen after our data collection period.
Excluding Highly Dynamic Vendors. We expect a ﬁle to
stabilize quickly if we exclude highly dynamic engines. We
rank engines based on their total number of ﬂips. We gradually
remove engines with more ﬂips and compute the percentage.
As shown in Figure 4(a), removing engines can immediately
increase the percentage of stable ﬁles. For example, removing
15 engines (50 engines left) can increase the percentage of
stable ﬁles on day-1 from 9.37% to 43.19%. However, to
stabilize most ﬁles quickly, we need to remove many engines.
In the extreme case, if we remove most engines and only
consider the ﬁve engines3 with the fewest ﬂips, the initial
percentage of stable ﬁles is very high (88.05%) on day-1. The
percentage increases to 95% on day-77. This, to some extent,
conﬁrms that ﬂips widely exist across engines. We cannot
remove a small number of engines to make ﬁles stabilized.
Only Considering Reputable Engines.
As discussed
in Section 2, we ﬁnd ten papers that hand-picked “high-
reputation” engines for data labeling. Among them, ﬁve pa-
pers are related to PE malware, and only three out of the
ﬁve papers provide detailed lists of their high-reputation en-
gines. This produces a set of nine “reputable engines” for
our analysis (Table 5 in the Appendix). In Figure 4(a), we
show the percentage of stabilized ﬁles when we only con-
sider reputable engines. We show that ﬁles do not stabilize
quickly — it is very similar to the 35-engine line. The reason
is some of the reputable engines (e.g., F-Secure) have a large
number of ﬂips. Note that among the nine engines, there are
two engines (Kaspersky and Symantec) that are mentioned by
more than one paper. We refer to these two engines as “highly
reputable engines”. If we only consider these two engines
(the “reputable engines*” line), we observe that most ﬁles are
stabilized very quickly.
Excluding Hazards Since it is easy to identify and remove
hazards (by submitting a ﬁle to VirusTotal in three consecutive
days), we re-examine the results after removing hazards. As
shown in Figure 4(b), removing hazards can help increase
the percentage of stabilized ﬁles. The initial percentage of
stabilized ﬁles (considering all engines) changes from 9.37%
to 36.69% on day-1. However, removing hazards does not
necessarily signiﬁcantly speed up the ﬁle stabilization.
Observation 4: Waiting for a longer period of time does not
guarantee to have more stable labels from individual engines,
unless we only consider a small set of engines.
3NANO-Antivirus, K7AntiVirus, Zoner, Ikarus, and Avast-Mobile.
(a) The original dataset
(b) The dataset without hazards
Figure 4: The percentage of ﬁles whose labels do not change
after day-x. Reputable engines: the nine high-reputation engines
mentioned by previous literature; reputable engines*: the two high-
reputation engines mentioned twice by previous literature.
(both u1 and u3 are in 2018, but u2 is in 2019). Sometimes, not
all engine instances are upgraded to the same engine version
even if they are updated on the same day. There are 25,714
(1.0%) ﬂips caused by handling two consecutive scans using
an engine updated on the same day but with two different
version numbers (u1 = u2 and v1 (cid:54)= v2).
380,807 (15%) ﬂips are likely caused by the non-
determinism of engines. In this case, an engine is used in
two consecutive scans with the same update date (u1 = u2)
and the same version (v1 = v2), but reports two different la-
bels (l1 (cid:54)= l2). We do not have a good explanation for the
non-determinism based on the current data. We cannot use
desktop engines to validate the non-determinism since Virus-
Total engines are different from their desktop versions [13].
For the other 397,273 (15%) ﬂips, the data ﬁelds for update
date or version information are “null” in their VirusTotal
responses, and we cannot categorize their root causes. Note
that the “detected” values (i.e., label information) are still
available in these responses, and thus the missing information
does not impact our analysis in other sections.
Observation 3: Engines’ model update is the major reason
of ﬂips. However, the inconsistency during engine updates
and engines’ non-determinism have contributed a non-trivial
portion of the ﬂips.
4.4 Label Stabilization
So far, we observe that label ﬂips are quite prevalent. A prac-
tical question is how long a user should wait before a ﬁle’s
labels become stable. In this subsection, we characterize the
label stabilization patterns over time and its predictability.
Considering All Engines. Figure 4(a) shows the percent-
age of ﬁles whose VirusTotal labels do not change since day-x
until the end of our data collection (the blue line, all 65 en-
gines). For example, when x = 50, only 9.37% of the ﬁles
are stable, meaning these ﬁles’ labels from all vendors do not
change since day-50. The percentage increases very slowly
for most of the time, but it suddenly jumps from 9.74% to
20.22% on day-176. This is an anomaly because CrowdStrike
USENIX Association
29th USENIX Security Symposium    2367
65 engines50 enginesreputable engines35 engines20 engines5 enginesreputable engines*500255075100% of filesdays150300350500255075100% of files250300350daysto maintain the ratio of “ﬂip-inﬂuenced” ﬁles below 10%, we
need to pick t between 2 to 31.
Setting t ≥ 40. As shown in Figure 5, when t is equal to
40, there are more ﬁles having label changes (i.e., inﬂuenced
by ﬂips). There are 7,690 (53.3%) ﬁles only with benign
aggregated labels and 2,376 (16.4%) ﬁles only containing
malicious aggregated labels. Thus, 4,357 (30.2%) ﬁles are
inﬂuenced by ﬂips. When we choose a larger t like t =50,
we can see a more obvious increase of ﬁles inﬂuenced by
ﬂips (compared to t = 40), and there are 6,499 (45.0%) ﬁles
inﬂuenced.
Reputable Engines Only (t = 1).
If we only consider the
nine reputable engines, there are 263 (1.8%) ﬁles inﬂuenced
by ﬂips (bar “r” in Figure 5) and 220 (1.5%) ﬁles inﬂuenced by