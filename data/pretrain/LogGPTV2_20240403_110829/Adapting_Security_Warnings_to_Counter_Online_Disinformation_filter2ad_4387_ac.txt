Contextual
In the contextual treatment group, 13 out of 20
participants stated during interviews that they were not aware
they had been shown disinformation warnings. All of these
participants clicked through both warnings. 4 reported that
they did not notice the warnings at all. Among the 16 that did
notice the warnings, 9 noticed the icons but not the text.
Interstitial All 20 participants noticed the interstitial warn-
ings. 12 understood that the warnings were about disinforma-
tion. 7 believed the warnings communicated a risk of “harm,”
a “virus,” or another “security threat” and quickly clicked to
go back without reading the text. The remaining participant
clicked through both warnings; when asked why, he explained
that he was focused on completing the study and “probably
would have reacted differently” outside of the study.
3.6.3 Opinions on Disinformation Warnings
As part of the interview, we displayed printouts of both
warning designs and asked for the participant’s opinions about
the warnings’ relative merits and general effectiveness.
Contextual When asked which warning design they be-
lieved would be more effective in general, a small minority
(6/40) chose the contextual warning. 5 of these participants
were in the interstitial treatment group.
5 participants noted that the contextual warning could be
seen before a user “commits” by clicking a link. 1 participant
explained, “you’re immediately presented with alternatives,
whereas for the interstitial I’m already there and committed a
click, so I want to go forward.” Another preferred the contex-
tual warning because it was easier to bypass: “[I] just wanted
to ﬁnd a link to click on very quickly, it doesn’t take as much
effort to avoid compared to the interstitial.”
5 other participants emphasized the “always-on” nature of
the contextual warning. 1 participant liked how they could
“always see the warning when browsing Google search results
without having to click around.” Another felt that the contex-
tual warning was paternalistic because it tilted the otherwise
level playing ﬁeld among search results, “direct[ing] you to
which [results] you should visit.”
15 participants said that the contextual warning was not
noticeable. 1 speciﬁed that “the exclamation point is very
subtle... you’re not going to notice it.”
Interstitial 34 participants—an overwhelming majority—
believed that the interstitial warning would be more effective
in general. When asked why, 32 mentioned that it was more
noticeable. 17 mentioned the color red, with 1 participant
noting that “everybody knows red means stop.”
19 participants remarked on how the the warning requires
user input to proceed. 1 participant observed that “it stops the
ﬂow of the user and forces them to take an action.” Other re-
sponses suggest that design cues contributed to the warning’s
effectiveness; participants mentioned that the red color and
text “implied that the user is in danger” and that the text was
“more instructive than the text on the contextual warning.”
When asked about drawbacks to the interstitial warning, 5
participants focused on the inconvenience and the potential for
warning fatigue. 1 participant noted that they would “probably
turn it off in the settings” if the warning showed up frequently.
Improving Warning Designs Many participants (17) sug-
gested that more informative disinformation warnings would
be more effective. Recommendations included adding “more
about why this particular site was ﬂagged,” deﬁnitions of
terms, and more explicit descriptions of potential harms. Con-
versely, 7 others urged “short and concise” messages that
“[get] the point across quickly.”
5 participants suggested using different warnings depend-
ing on severity and whether the user had visited the website
before. Another 5 recommended that warnings persist even
if a user had visited a website before, arguing that warnings
would be more effective if users were “consistently reminded
that [the page] may not be completely safe or factual.”
Participants also suggested alternate warning forms: pop-
ups, banner messages, or highlighting false claims.
Trust The source of the warning was important to many
participants. 8 indicated that they were more likely to be de-
terred by a browser warning if they knew that it was triggered
by Google, since they trusted the company’s judgment. 1 par-
ticipant explained that they clicked through the interstitial
warning because they understood that Princeton University
had ﬂagged the website, and they felt that the university was
not a credible source of judgment about online disinforma-
tion. Another theme underlying trust judgments was previous
experiences with browser warnings. 7 participants expressed
that they distrusted the warnings due to previous encounters
with false positive warnings or overly restrictive site blockers
on institutional networks (e.g., in high school).
Risk 7 participants expressed the opinion that disinforma-
tion is not a serious threat or that it is not as harmful as mal-
ware. One participant explained that they typically comply
with browser warnings but reacted differently to the disin-
formation warning: “I don’t like the idea of someone telling
me where or what I am allowed to access. You can give me
suggestions. It was because I realized it was a disinformation
warning and not a malware warning that I went back to try
and get to the website.” Another participant characterized this
sentiment sharply, saying “[d]isinformation warnings should
not make it harder to access the site.”
3.7 Discussion
Contextual vs. Interstitial Warnings The interstitial
warning was distinctly more noticeable and comprehensi-
ble than the contextual warning, and also far more effective
at inducing behavioral changes. Similar ﬁndings in security
warning research prompted the ﬁeld to shift from contextual to
interstitial formats. Platforms are only just beginning to make
this shift for disinformation; contextual warnings are currently
the dominant format in both research and deployment. While
contextual warnings may still have a role in countering dis-
information, interstitial warnings and other forms of active,
interruptive warnings clearly merit further consideration.
Impact of Visual Design Choices The iconography, colors,
and text styles in our warnings impacted participant attention,
comprehension, and behavior. The icon we added to the con-
textual warning made the warning more noticeable but did
not necessarily aid with comprehension, as many participants
who noticed the icon still failed to notice the text. The red
background of the interstitial warning contributed to its ef-
fectiveness, but may also have reduced comprehension as
participants seemed to react quickly upon seeing the red color
without taking the time to read or understand the warning.
Again drawing from security warning research, future work
should use comparative effectiveness studies to isolate the
effects of individual visual design choices.
3.7.1 Mechanisms of Effect
So few participants complied with the contextual warning
that it is difﬁcult to draw conclusions about what caused the
behavioral effect. For the interstitial warning, however, we
found evidence for three different mechanisms by which the
warning induced behavioral changes.
Informativeness Warning science literature focuses on ed-
ucating the user and enabling them to make an informed deci-
sion about how to proceed [101, 102]. Across both warning
designs we tested, participants who understood the warning
visited an alternative website in over half of cases (21/38),
while participants who did not understand the warning did
so in only a third of cases (13/42). Moreover, nearly half
of participants recommended making the warnings more in-
formative to improve effectiveness. These results reinforce
that informing users is a possible mechanism of effect for
interstitial disinformation warnings.
Fear of Harm The interstitial warnings had a threatening
appearance, which many participants identiﬁed as a factor
in why they did not continue. Some participants visited an
alternative site because they perceived a non-speciﬁc risk of
personal harm, without comprehending the warning. Other
participants misinterpreted the warning and believed that it
described a risk of receiving a computer virus or other security
threat. If a warning conveys a risk of harm, it should be spe-
ciﬁc and narrowly scoped; otherwise users may perceive the
warning as irrelevant or a false positive, which could reduce
the behavioral effect [10, 15, 18, 20, 103]. As long as the spe-
ciﬁc harm is clear, though, using design cues to further convey
a general risk of harm may improve warning effectiveness.
Friction The interstitial warning’s strong effect was due, in
part, to the friction it introduced into the task workﬂow. Some
participants preferred to choose another source rather than
read the warning, decide whether to believe and comply with
it, and click through it. As with the “fear of harm” mechanism,
friction must be carefully calibrated to avoid inducing warning
fatigue or habituating users to ignore warnings. Friction also
has serious drawbacks as a causal mechanism: it degrades the
user experience, makes platforms more difﬁcult to use, and
does not rely on an informed decision about disinformation.
3.7.2 Limitations
Security warning research has observed challenges in study-
ing behavioral responses to risks in laboratory settings, partic-
ularly with respect to ecological validity [96]. We encountered
similar challenges in this study.
Our sample was small in size and biased in several ways;
our ﬁndings should be understood as illustrative but not rep-
resentative. It is important to identify if our ﬁndings can be
replicated by larger, more diverse populations.
Because participants used our computer and we were watch-
ing during the task, some participants reported that they be-
haved differently in the study than they might have in real life.
Others may not have reported this effect because they were
reluctant to admit that they behaved with bias or because the
effect was unconscious. An experimental design that allows
participants to use their own computers in their own envi-
ronments (i.e., not in a laboratory) could offer more realistic
observations of how participants assess risk.
Although participants appeared to be driven to complete
the research tasks, they were not personally invested in com-
pleting the tasks or ﬁnding correct answers to the queries. The
role-playing aspect of the study may not have been strongly
immersive, and there were no extrinsic rewards or penalties
that incentivized correct answers.
Finally, because our search tasks did not pertain to partici-
pants’ social or political contexts, participants had little reason
to engage in motivated reasoning. Motivated reasoning can
strongly inﬂuence a user’s perceptions of information relevant
to their social or political beliefs [104], so in those contexts,
the warning effects that we demonstrate may be weaker.
4 Crowdworker Study
In our second study, we aimed to verify the behavioral
effects of interstitial disinformation warnings. We also exam-
ined the mechanisms for those effects, so that we could reason
about the utility and limitations of deploying the warnings.
Our research questions were:
RQ1: Do interstitial disinformation warnings cause
users to choose alternative sources of information? We
investigated whether population sample bias or task design
signiﬁcantly affected the results of our laboratory study. We
recruited a larger, more diverse participant pool from Ama-
zon Mechanical Turk (Section 4.6) and adjusted the task to
account for limitations in the laboratory study (Section 4.2).
RQ2: Do interstitial warnings that effectively inform
users about the risks of disinformation cause users to
choose alternative sources of information? We tested
whether participants understood the warnings, then compared
the behavioral effects of informative and uninformative warn-
ings to isolate the impact of informativeness on behavior.
RQ3: Do interstitial warnings that communicate a
risk of personal harm cause users to choose alternative
sources of information? We tested whether warnings caused
participants to fear harm, then compared the behavioral effects
of warnings that did and did not evoke a fear of harm.
RQ4: Does user partisanship (with respect to U.S. pol-
itics) moderate behavioral effects or perceptions of inter-
stitial warnings? Research in political science indicates that
political orientation affects judgments of information credibil-
ity [105] and efﬁcacy of misinformation warnings [87]. We
included this research question to detect if partisan alignment
created a bimodal distribution in responses to warnings.
The task structure and key behavioral outcomes remained
the same as in the laboratory study. We informed partici-
pants that they were joining a study of search engine usage
and research behaviors. We then guided participants through
four research tasks using a search engine, alternating between
control and treatment rounds. In each treatment round, the
participant encountered one of eight candidate interstitial dis-
information warnings after clicking certain search results. We
measured whether the participant clicked through the warning
and whether they visited an alternative website. We examine
the CTR and AVR across all observations to answer RQ1.
We used surveys after each warning encounter to measure
how informative the warning was and how strongly the partici-
pant perceived the warning to convey a risk of harm. RQ2 and
RQ3 concern the relationship between AVR and these survey
responses. A standard analysis approach would have been
to randomly assign participants to warnings, then compute
statistical tests across the conditions. Unless the differences
in effect between warnings were dramatic, however, this ap-
proach would have required a massive number of observations
on each warning to establish statistical signiﬁcance.
We instead employed a multi-armed bandit algorithm,
which allows efﬁcient exploration of a larger design space
than is traditionally possible. As we received successive ob-
servations, the bandit increased the odds that participants
encountered the warnings proving to be most and least infor-
mative and most and least effective at conveying fear of harm.
After all observations were completed, signiﬁcantly more par-
ticipants had encountered these top- and bottom-performing
warnings, providing us with the statistical power needed to
test our hypotheses. Section 4.5 discusses the design and
implementation of the multi-armed bandit algorithm.
4.1 Warning Designs
We created eight candidate interstitial disinformation warn-
ings: four designed for informativeness and four designed to
evoke fear of harm (Table 2). The warnings shared a com-
mon layout, consisting of an icon, a title, a primary message,
an optional detailed message, and two buttons. This layout
differed from the laboratory interstitial warning in two ways.
First, in the laboratory warning design, the detailed message
(and the “Continue” button) were hidden at ﬁrst and would
only be revealed after the participant clicked “Learn more.”
In the crowdworker study, we wanted to ensure that the full
warning message was always displayed, because part of what
we were measuring was the effect of different messages. We
eliminated the “Learn more” button and instead displayed the
detailed message and the “Continue” button on all warnings.
The second change addressed the “Back to safety” button.
This button text implied that the user was in danger, which
was inappropriate for the informative warnings. We changed
the button to read “Go back” and applied this change to both
informative and harm-based warnings in order to maintain a
common interaction design across all warnings.
For both groups of warnings, we generated several options
for icons, titles, and messages. We then created candidate
designs by choosing combinations of these elements and in-
serting them into the layout template.
Informative Warnings We designed the informative warn-
ings to be visually nonthreatening and clearly explanatory in
their messages (see Figure 3a). The warnings included one of
two icons—either a generic exclamation point icon or a po-
liceman silhouette—and displayed black text against a white
background. The warning messages explained the nature and
consequences of disinformation in varying detail: some ex-
plicitly deﬁned the term “disinformation,” some asserted that
“experts” had labeled the site as containing “false or mislead-
ing” information, and others provided clear guidance on how