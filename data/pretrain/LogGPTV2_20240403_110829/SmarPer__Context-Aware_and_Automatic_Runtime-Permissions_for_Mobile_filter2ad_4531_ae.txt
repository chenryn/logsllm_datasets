### Loss Function and Error Metrics

The 0-1 loss function, defined as \( L(y, \hat{y}) = 1 \) when \( y \neq \hat{y} \) and \( L(y, \hat{y}) = 0 \) when \( y = \hat{y} \), is a common choice. In this context, \( E \) represents the standard incorrect classification rate (ICR). However, the 0-1 loss does not account for the ordering between the three categories of the decision \( y \): "Allow," "Obfuscate," and "Deny." For example, predicting "Allow" when the correct decision is "Deny" is more severe than predicting "Obfuscate," as the latter still allows some degree of privacy. To address this, we use the mean-absolute error (MAE) loss function, which better reflects these types of errors. We recode the decisions as follows: "Allow" = -1, "Obfuscate" = 0, and "Deny" = +1. The MAE loss is then defined as \( L(y, \hat{y}) = |y - \hat{y}| \).

### Performance Evaluation Methodology

We developed a machine learning framework to reliably estimate the error measures for different methods. Our framework uses the standard practice of splitting the data into training and testing sets. Specifically, we randomly select 50% of the participants (U = 20) for testing to compute an error estimate, and the remaining 21 participants for training to learn the parameters of the Bayesian Logistic Regression (BLR) model by maximizing the log-likelihood. Note that baseline and other evaluated methods do not require parameter learning.

For the 20 test participants, we estimate the error measure as follows:
1. We form datasets \( D_{u,t_u} \) for each participant.
2. Since participants have varying numbers of decisions, we select a fixed percentage of each participant's data, e.g., the first 10% of their decisions, denoted by \( t_u = 10\% \).
3. We then create the test dataset \( D_{\text{test}} \) by randomly selecting \( N_u = 20 \) decisions from each participant.
4. Using method \( M \), we compute predictions \( \hat{y}_{ui} \) for all \( y_{ui} \in D_{\text{test}} \).
5. We choose only those decisions as test points that were made after the first half of the decisions to ensure that the test decisions resemble future decisions.
6. We repeat the above process 50 times with different random seeds to obtain 50 different realizations of the error.

### Feature Selection

For our evaluation, we use features chosen through an additive approach, also known as Forward Stepwise Selection [52]. We add each of the 37 features (Sections IV-C and V-A6) in turn and observe their effect on the model's performance. The feature that most improves performance is selected. We continue this procedure until the performance remains the same or decreases. Using this approach, we selected the following seven features for BLR:
- Method category (e.g., location, contacts, or storage)
- Method name (i.e., the actual API call)
- App name
- Whether the app was in the foreground
- Whether denying the request causes the app to crash
- Day of the month
- Battery-level percentage

Note that the effect of a set of features will depend on the machine learning model selected. This is not necessarily the best subset and combination of features for BLR, as our selection approach was not exhaustive. For BLR, a possible approach is to use regularization to find the best features, which also helps reduce overfitting.

### Data and Implementation

In our evaluation, we use data from all participants but only for decisions associated with popular apps, i.e., apps with more than 200 decisions (Figure 5): Facebook, Twitter, Instagram, WhatsApp, Viber, Skype, Snapchat, The Weather Channel, and AccuWeather. We do so because we do not have enough data for the remaining apps to reliably perform our analysis (see Table II in the Appendix).

Our experimental framework and the models evaluated were implemented using the MATLAB Statistics and Machine Learning toolbox and the GPML toolbox [53]. Our code is publicly available on the SmarPer’s website [11].

### Performance Evaluation Results

In our evaluation, we considered the following models:
- Static policy
- ZeroRt
- BLR
- Gaussian Process with Squared Exponential Kernel (GP-SE)
- Decision tree (D. Tree)
- 3-binary support vector machines (SVM) with linear kernel

The goal was to compare context-oblivious models with different context-aware models. We also evaluated the training of one-size-fits-all models (i.e., BLR-all and SVM-all), i.e., training a single model for all users.

Figure 6 shows estimates of error measures as a function of \( t_u \). We vary \( t_u \) from 10% to 100% for all test participants. Figure 6(a) uses the 0-1 loss function and shows the median ICR obtained by the different models evaluated. The shaded area shows the region between the 25th and 75th percentiles. Both one-size-fits-all models (i.e., BLR-all and SVM-all) have significantly higher error rates than most per-user models; BLR-all performs even worse than our baselines. These results are consistent with our observations about Figure 3, where participants' unique privacy preferences make it difficult to train a one-size-fits-all model that accurately predicts decisions at runtime.

For \( t_u = 100\% \):
- The mean ICR is 0.39 (±0.04) for static policy, 0.30 (±0.03) for ZeroRt, 0.20 (±0.03) for BLR, and 0.16 (±0.02) for SVM.
- Context-aware models achieve much lower error rates than the baselines, demonstrating the gain from adding context. Unlike the static policy method, all other per-user models are dynamic and improve as the amount of data increases.

Figure 6(b) shows a similar trend for MAE loss. For \( t_u = 100\% \):
- The mean MAE is 0.48 (±0.06) for static policy, 0.39 (±0.04) for ZeroRt, 0.22 (±0.03) for BLR, and 0.19 (±0.03) for SVM.
- The MAE loss function captures the ordering between different types of decisions, which is ignored by the 0-1 loss. For example, predicting "Allow" for "Deny" has a loss of 2, while predicting "Obfuscate" has a loss of 1. Under 0-1 loss, these errors are treated equally with a loss of 1. Thus, MAE is a better measure of the loss for our problem.

Although our results are encouraging (more than 80% of correct predictions for modest training set sizes, hence lower user burden), the level of user satisfaction for such values of the performance metric must be evaluated through dedicated experiments and user studies, which will be carried out in the second phase of the project.

Furthermore, Figure 6(c) shows the distribution of MAE over test decisions in \( D_{\text{test}} \) for one random partition with \( t_u = 100\% \), i.e., all the training data. Context-aware methods such as BLR and SVM have very few mistakes with a loss of 2, indicating that such methods very rarely make the mistake of predicting "Allow" for "Deny" and vice versa.