    ```
    这将启动`macvlan-net1`网络中的另一个容器实例。
31.  Run the `docker inspect` command to see the MAC address of the `macvlan-net2` container instance:
    ```
    $ docker inspect macvlan2
    ```
    这将以 JSON 格式输出`macvlan2`容器实例的详细配置，此处被截断，仅显示相关的网络设置:
    ![Figure 6.23: docker inspect output for the macvlan2 container ](img/B15021_06_23.jpg)
    图 6.23: docker 检查 macvlan2 容器的输出
    在这个输出中可以看到`macvlan2`容器与`macvlan1`容器实例有不同的 IP 地址和 MAC 地址。Docker 分配不同的媒体访问控制地址，以确保许多容器使用`macvlan`网络时不会出现第 2 层冲突。
32.  Run the `docker exec` command to access an `sh` shell inside this container:
    ```
    $ docker exec -it macvlan1 /bin/sh
    ```
    这应该会将您放入容器内的根会话中。
33.  Use the `ifconfig` command inside the container to observe that the MAC address you saw in the `docker inspect` output on the `macvlan1` container is present as the MAC address of the container's primary network interface:
    ```
    / # ifconfig
    ```
    在`eth0`界面的详细信息中，查看`HWaddr`参数。您可能还会注意到`inet addr`参数下列出的 IP 地址，以及该网络接口发送和接收的字节数–`RX bytes`(接收的字节)和`TX bytes`(发送的字节):
    ```
    eth0      Link encap:Ethernet  HWaddr 02:42:C0:A8:7A:02
              inet addr:192.168.122.2  Bcast:192.168.122.255
                                       Mask:255.255.255.0
              UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
              RX packets:353 errors:0 dropped:0 overruns:0 frame:0
              TX packets:188 errors:0 dropped:0 overruns:0 carrier:0
              collisions:0 txqueuelen:0 
              RX bytes:1789983 (1.7 MiB)  TX bytes:12688 (12.3 KiB)
    ```
34.  Install the `arping` utility using the `apk` package manager available in the Alpine Linux container. This is a tool used to send `arp` messages to a MAC address to check Layer 2 connectivity:
    ```
    / # apk add arping
    ```
    `arping`实用程序应安装在`macvlan1`容器内:
    ```
    fetch http://dl-cdn.alpinelinux.org/alpine/v3.11/main
    /x86_64/APKINDEX.tar.gz
    fetch http://dl-cdn.alpinelinux.org/alpine/v3.11/community
    /x86_64/APKINDEX.tar.gz
    (1/3) Installing libnet (1.1.6-r3)
    (2/3) Installing libpcap (1.9.1-r0)
    (3/3) Installing arping (2.20-r0)
    Executing busybox-1.31.1-r9.trigger
    OK: 6 MiB in 17 packages
    ```
35.  Specify the Layer 3 IP address of the `macvlan2` container instance as the primary argument to `arping`. Now, `arping` will automatically look up the MAC address and check the Layer 2 connectivity to it:
    ```
    / # arping 192.168.122.3
    ```
    `arping`实用程序应该报告回`macvlan2`容器实例的正确的媒体访问控制地址，指示成功的第 2 层网络连接:
    ```
    ARPING 192.168.122.3
    42 bytes from 02:42:c0:a8:7a:03 (192.168.122.3): index=0 
    time=8.563 usec
    42 bytes from 02:42:c0:a8:7a:03 (192.168.122.3): index=1 
    time=18.889 usec
    42 bytes from 02:42:c0:a8:7a:03 (192.168.122.3): index=2 
    time=15.917 use
    type exit to return to the shell of your primary terminal. 
    ```
36.  Check the status of the containers using the `docker ps -a` command:
    ```
    $ docker ps -a 
    ```
    该命令的输出应该显示环境中所有正在运行和停止的容器实例。
37.  Next, stop all running containers using `docker stop`, followed by the container name or ID:
    ```
    $ docker stop hostnet1
    ```
    对环境中所有正在运行的容器重复此步骤。
38.  Clean up the container images and unused networks using the `docker system prune` command:
    ```
    $ docker system prune -fa 
    ```
    此命令将清理计算机上剩余的所有未使用的容器映像、网络和卷。
在本练习中，我们查看了 Docker 中默认可用的四个默认网络驱动程序:`bridge`、`host`、`macvlan`和`none`。对于每个示例，我们探讨了网络如何工作，使用这些网络驱动程序部署的容器如何与主机一起工作，以及它们如何与网络上的其他容器一起工作。
Docker 默认公开的网络功能可以用来在非常高级的网络配置中部署容器，正如我们到目前为止所看到的。Docker 还提供了在集群配置中管理和协调主机之间的容器网络的能力。
在下一节中，我们将研究创建网络，该网络将在 Docker 主机之间创建覆盖网络，以确保容器实例之间的直接连接。
# Docker 叠加网络 i ng
`Overlay`网络是为了特定目的而在物理(底层)网络之上创建的逻辑网络。例如，**虚拟专用网络** ( **虚拟专用网络**)是`overlay`网络的一种常见类型，它使用互联网来创建到另一个专用网络的链接。Docker 可以创建和管理容器之间的`overlay`网络，这可以用于容器化的应用直接相互对话。当容器被部署到`overlay`网络中时，它们被部署在集群中的哪个主机上并不重要；它们将直接连接到存在于同一`overlay`网络中的其他容器化服务，就像它们存在于同一台物理主机上一样。
## 练习 6.04:定义覆盖网络
Docker `overlay`联网用于在 Docker 集群中的机器之间创建网状网络。在本练习中，您将使用两台机器创建一个基本的 Docker 集群。理想情况下，这些机器将存在于同一网络段上，以确保它们之间的直接网络连接和快速网络连接。此外，他们应该在支持的 Linux 发行版中运行相同版本的 Docker，例如红帽、CentOS 或 Ubuntu。
您将定义`overlay`网络，这些网络将跨越 Docker 集群中的主机。然后，您将确保部署在不同主机上的容器可以通过`overlay`网络相互通信:
注意
本练习需要访问安装了 Docker 的辅助机器。通常，基于云的虚拟机或部署在另一个虚拟机管理程序中的机器效果最好。使用 Docker Desktop 在系统上部署 Docker 群集可能会导致网络问题或严重的性能下降。
1.  On the first machine, `Machine1`, run `docker --version` to find out which version of Docker is currently running on it.
    ```
    Machine1 ~$ docker --version
    ```
    `The version details of the Docker installation of Machine1 will be displayed:`
    ```
    Docker version 19.03.6, build 369ce74a3c
    ```
    然后，你可以对`Machine2:`进行同样的操作
    ```
    Machine2 ~$ docker --version
    ```
    `The version details of the Docker installation of Machine2 will be displayed`:
    ```
    Docker version 19.03.6, build 369ce74a3c
    ```
    继续之前，请验证安装的 Docker 版本是否相同。
    注意
    Docker 版本可能因您的系统而异。
2.  On `Machine1`, run the `docker swarm init` command to initialize a Docker swarm cluster:
    ```
    Machine1 ~$ docker swarm init
    ```
    这将打印您可以在其他节点上使用的命令，以加入 Docker 群集群，包括 IP 地址和`join`令牌:
    ```
    docker swarm join --token SWMTKN-1-57n212qtvfnpu0ab28tewiorf3j9fxzo9vaa7drpare0ic6ohg-5epus8clyzd9xq7e7ze1y0p0n 
    192.168.122.185:2377
    ```
3.  On `Machine2`, run the `docker swarm join` command, which was provided by `Machine1`, to join the Docker swarm cluster:
    ```
    Machine2 ~$  docker swarm join --token SWMTKN-1-57n212qtvfnpu0ab28tewiorf3j9fxzo9vaa7drpare0ic6ohg-5epus8clyzd9xq7e7ze1y0p0n 192.168.122.185:2377
    ```
    `Machine2`应成功加入 Docker 群集群:
    ```
    This node joined a swarm as a worker.
    ```
4.  Execute the `docker info` command on both nodes to ensure they have successfully joined the swarm cluster:
    `Machine1`:
    ```
    Machine1 ~$ docker info
    ```
    `Machine2` :
    ```
    Machine2 ~$ docker info
    ```
    以下输出是`docker info`输出的`swarm`部分的截断。从这些详细信息中，您将看到这些 Docker 节点被配置在一个群集中，并且群集中有两个节点和一个管理器节点(`Machine1`)。这些参数在两个节点上应该是相同的，除了`Is Manager`参数，对于该参数`Machine1`将是管理器。默认情况下，Docker 将为默认 Docker 群集`overlay`网络分配一个默认子网`10.0.0.0/8`:
    ```
     swarm: active
      NodeID: oub9g5383ifyg7i52yq4zsu5a
      Is Manager: true
      ClusterID: x7chp0w3two04ltmkqjm32g1f
      Managers: 1
      Nodes: 2
      Default Address Pool: 10.0.0.0/8  
      SubnetSize: 24
      Data Path Port: 4789
      Orchestration:
        Task History Retention Limit: 5
    ```
5.  From the `Machine1` box, create an `overlay` network using the `docker network create` command. Since this is a network that will span more than one node in a simple swarm cluster, specify the `overlay` driver as the network driver. Call this network `overlaynet1`. Use a subnet and gateway that are not yet in use by any networks on your Docker hosts to avoid subnet collisions. Use `172.45.0.0/16` and `172.45.0.1` as the gateway:
    ```
    Machine1 ~$ docker network create overlaynet1 --driver overlay --subnet 172.45.0.0/16 --gateway 172.45.0.1
    ```
    将创建`overlay`网络。
6.  Use the `docker network ls` command to verify whether the network was created successfully and is using the correct `overlay` driver:
    ```
    Machine1 ~$ docker network ls
    ```
    将显示 Docker 主机上可用的网络列表:
    ```
    NETWORK ID       NAME              DRIVER     SCOPE
    54f2af38e6a8     bridge            bridge     local
    df5ebd75303e     docker_gwbridge   bridge     local
    f52b4a5440ad     host              host       local
    8hm1ouvt4z7t     ingress           overlay    swarm
    9bed60b88784     none              null       local
    60wqq8ewt8zq     overlaynet1       overlay    swarm
    ```
7.  Use the `docker service create` command to create a service that will span multiple nodes in the swarm cluster. Deploying containers as services allow you to specify more than one replica of a container instance for horizontal scaling or scaling container instances across nodes in a cluster for high availability. To keep this example simple, create a single container service of Alpine Linux. Name this service `alpine-overlay1`:
    ```
    Machine1 ~$ docker service create -t --replicas 1 --network overlaynet1 --name alpine-overlay1 alpine:latest