preemptirq traceponts*
Check your kernel version to see what other tracepoints exist, such as by using bpftrace:
bpftrace -1l *tracepoint:**
Or using perf(1):
perf list tracepoint
Prior chapters covered resource events, including block and network I/O.
14.2 Strategy
If you are new to kernel performance analysis, here is a suggested overall strategy that you can
follow. The next sections explain the tools involved in more detail.
1. If possible, create a workload that triggers the events of interest, ideally a known number of
times. This might involve writing a short C program.
2. Check for the existence of tracepoints that instrument the event or existing tools
(including those in this chapter).
3. If the event can be called frequently so that it consumes significant CPU resources (>5%),
CPU profiling can be a quick way to show the kernel functions involved. If not, a longer
profile may be used to capture enough samples for study (e.g., using perf(1) or BCC
profile(8), with CPU flame graphs). CPU profiling will also reveal the use of spin locks, and
mutex locks during optimistic spin.
4. As another way to find related kernel functions, count function calls that likely match the
event. For example, if analyzing ext4 file system events, you could try counting all calls that
matched “ext4_*" (using BCC funccount(8).
5. Count stack traces from kernel functions to understand the code path using BCC
stackcount(8)). These code paths should already be known if profiling was used.
6. Trace function call flow through its child events (using perf-tools Ftrace-based funcgraph(8)).
7. Inspect function arguments (using BCC trace(8) and argdist(8), or bpftrace).
8. Measure function latency (using BCC funclatency(8) or bpftrace).
9. Write a custom tool to instrument the events and print or summarize them.
1 Requires CONFIG_PREEMPTIRQ_EVENTS.
---
## Page 677
640
Chapter 14 Kernel
The following section shows some of these steps using tradlitional tools, which you can try before
turning to BPF tools.
14.3
TraditionalTools
Many traditional tools were covered in prior chapters. Some additional tools that can be used for
kernel analysis are included here and listed in Table 14-2.
Table 14-2Traditional Tools
1001
Type
Description
Ftrace
Tracing
Linux built-in tracer
peqos gxed
Tracing
Linux official profiler: scheduler analysis subcommand
s1abtop
Kernel slab cache usage
14.3.1 Ftrace
Ftrace^ was created by Steven Rostedt and added to Linux 2.6.27 in 2008, Like perf(1), Ftrace is a
multi-tool with many capabilities. There are at least four ways to use Ftrace:
A. Via the /sys/kernel/debug/tracing files, controlled using cat(1) and echo(1) or a higher-level
language. This usage is documented in the kernel source under Documentation/trace/
ftrace.rst [158]
B. Via the trace-cmd front-end by Steven Rostedt [159][160].
C. Via the KerneIShark GUI by Steven Rostedt and others [161].
D.Via the tools in the perf-tools collection by myself [78]. These are shell wrappers to the /sys
/kernel/debug/tracing files.
I will demonstrate Ftrace capabilities using perf-tools, but any of these methods can be used.
Function Counting
Let's say I wanted to analyze file system read-ahead in the kernel. I can begin by counting all func-
tions containing “readahead* using funccount(8) (from perf-tools) while generating a workload
that is expected to trigger it:
funccount **zeadahead*
Tracing **readahead**... Ctrl-C to end
FUBIC
COUNT
peeqeprex"ou/se"euoeo"ebed
1.2
---
## Page 678
14.3 Traditional Tools
641
_breadshesd
33
peeqepesa"ou irgs-off
#
/ =
=> need-resched
1/
-=> bardirg/softirq
11/ _--=> preenpt-depth
111/
delay
TASK-PID
CFO#1111
TIMESTANPFUBCTION
11
1111
cksun-32372 [006].... 1952191.125801: page_cache_async_readahead:
(page_cache_async_readshead+0x0/0x80)
cksun-32372 [006].... 1952191.125822: 
peauepeax"ou/seaqpeoabed  _vfs_read
peo.sA (=
peex"ss  entzy_SYsCALL_64_after_hvframe
cksun-32372 [006].... 1952191.126704: page_cache_async_readahead:
(page_cache_async_readahead+0x0/0x80)
cksun-32372 [006].... 1952191.126722: 
pesvepeex"ou/se"espeo"ebed  ext4_file_resd_iter
[.--]
---
## Page 679
642
 Chapter 14 Kernel
This prints a stack trace per event, showing that it was triggered during a read() syscall. kprobe(8)
also allows function arguments and the return value to be inspected.
For efficiency, these stack traces can be frequency counted in kernel context rather than printed
s weros rog tous 'sast s 'ameag aoeng samau e sambau su auo Aq ao
Example:
 cd /sys/kerne1/debug/tracing/
suanaaqoxdx < ,peaqepeexoufseaqoeoabed aqoxdku/seqoadx: d, oupe 
 cat events/kprobes/myprobe/hist
#event histogran
 triggex info: hist:keys=stacktzace:vals=hitcount:sort=hitcount:size=2048 [active]
 stacktrace:
ftxace_ops_assist_runc+0x51/0xf0
xffffffffe0elb0d5
page_cache_async_readahead+0x5/0x80
generic_file_read_iter+0x784/0xbf0
ext4_f1le_read_1ter+0x56/0x100
nex_symc_read+0xe4/0x130
_vfs_read+x29/0x40
vfs_read+0xfe/0x130
Sys_zead+0x55/Dxc0
do_sy8ca11_64+0x73/0x130
entry_SYsCAL_64_aftex_hvfrane+0x3d/0xa2
)hitcount:
235
Totsls:
HIts: 235
Entries: 1
Dropped: 0
[...ateps to undo the cracfng scate..-]
This output shows that this stack trace path was taken 235 times during tracing.
Function Graphing
Finally, funcgraph(8) can show the child functions called:
---
## Page 680
14.3 Traditional Tools
643
 funcgraph page_cache_async_readahead
pue oa 3-xa **μpeegepeex"ou/seouoes"sbed 6utoex
31
page_cache_ssync_readahead ()
31
Inode_congested ()  (
()paqeabuos"fue"ap
31
0.582 us
(1peseq"tsenbex"up
31
dn_table_any_congested 1(
31
dn_any_congested (1 (
31
0.267 us
()paseq“sanbaa"μp
IE
1.824 us
dn_tab1e_any_congested () ;
31
4,604 us
31
7,589 us
31 + 11. 634 us
sn. czt*Et + IE
31
ondenand_readabead () 
31
}(1peeqepesa"euoes"sfed"op
31
_page_cache_a1loc (1↑
31
0,234 us
alloc_pages_current (11
31
get_task_poliey-part,30 () 
31
0.124 us
policy_nodenask () 
[. . - ]
This reveals not just the code path taken but, as with stack traces, these functions can also be
traced for more information arguments and return values.
14.3.2
perf sched
The perf(1) command is another multi-tool, and Chapter 6 summarized its use with PMCs, profil-
ing, and tracing. It also has a sched subcommand for scheduler analysis. For example:
+ perf sched zecord
+ perf sched tinehist
Sanples do not have callchains.
t.1me
cpu
task nase
valt time
sch delay
run time
[tid/pid]
(msec)
(msec)
 (nsec)
991962. 879971 [0005]
perf[16984]
0 .000
0,000
0.000
991962.880070 [0007]
:17008[17008]
0 .000
0,000
0.000
991962. 880070 [0002]
cc1[16880]
0 . 000
0,000
0.000
991962,880078 [0000]
cc1[16881]
0 .000
0.,000
0.000
991962. 880081 [0003]
cc1 [16945]
0 .000
0,000
0.000
[E000] E60088*296166
ksoftxqd/3[28]
0.000
0,007
0.012
991962. 880108 [0000]
ksoftirqd/0 [6]
0 .000
0,007
0 .030
[..-]
---
## Page 681
644
Chapter 14 Kernel
This output shows per-scheduling event metrics of the time spent blocked and waiting for a
wakeup (°wait time*), the scheduler delay (aka run queue latency, *sch delay*), and the on-CPU
run time (°run time*).
14.3.3 slabtop
The slabtop(1) tool shows the current sizes of the kermel slab allocation caches. For example, from
a large production system, sorting by cache size (s c):
slabtop -s c
Active / Total Objects 1§ used)
:1232426/ 1290213 (95.5%)
Active / Total Slabs ($ used)
: 29225 / 29225 (100.05)
Active / Total Caches ($ used)
: 85 / 135 (63.09)
Active / Total Slze (% used)
: 288336,643 / 306847,48K (94.0%]
Mininus / Average / Maxinun 0bject : 0,01K / 0.24K / 16.0K
0BJS ACTIVE
USE OBJ SIZE
SLABS OBF/SLAB CACHE SIZE NAME
76412 69196
0%
0 . 57K
2729
28
43664x rad1x_tree_node
313599 313599 100$
0,10K
8041
39
32164K buffer_bead
3732
LTLE
0%
7, 44K
EE6
29856x task_struct
11776
S6L8
0%
2,00K
736
16
23552K TCP
33168
3227T
0%
0 . 66K
691
48
221123 proc_inode_cache
86100
79990
0 %
0 , 19K
2050
42
16400x dentry
0%
0.59K
488
53
15616x lnode_cache
[. .-]
This output shows around 43 Mbytes in the radix_tree_node cache and around 23 Mbytes in the
TCP cache. For a system with a total of I80 Gbytes of main memory, these kernel caches are rela-
tively tiny.
This is a useful tool for troubleshooting memory pressure problems, to check whether some
kernel component is unexpectedlly consuming significant memory.
14.3.4 0ther Tools
/proc/lock_stat shows various statistics on kernel locks but is only available if
CONFIG_LOCK_STAT is set.
/proc/sched_debug provides many metrics to aid schecduler development.
14.4BPF Tools
This section covers aditional BPF tools you can use for kernel analysis and troubleshooting.
They are shown in Figure 14-3.
---
## Page 682
14.4 BPF Tools
645
Appiications
System Call Interface
offeputine
offvaketine
vorkg
Rest of Kernel
Gen
menLeak
slabratetop
kpages
Device Drivers
Figure 14-3 Additional BPF tools for kermel analysis
These tools are either from the BCC and bpftrace repositories covered in Chapters 4 and 5 or were
created for this book. Some tools appear in both BCC and bpftrace. Table 14-3 lists the tool origins
(BT is short for bpftrace).
Table 14-3  Kernel-Related Tools
Tool
Source
Target
Description
1oad.s
BT
CPUs
Show load averages
offcputine
BCC/book
Sched
Summarize off-CPU stack traces and times
vakeuptine
BCC
Sched
Summarize waker stack traces and blocked times
offvaketine
BCC
Sched
Summarize waker with off-CPU stack traces
m1ock
Book
Mutexes
Show mutex lock times and kernel stacks
mhe1d
Book
Mutexes
Show mutex held times and kernel stacks
kmen
Book
Memory
Summarize kernel memory allocations
kpage.s
Book
Pages
Summarize kemel page allocations
memleak
BCC