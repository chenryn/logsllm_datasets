### Normalized Link Occurrence and Zipf-Mandelbrot Distribution

If a link carries 10% of the routes between source \( S \) and destination \( D \), its normalized occurrence is 0.10. We observe that the normalized link-occurrence distribution is accurately modeled by the Zipf-Mandelbrot distribution, which is given by:

\[ f(k) \propto \frac{1}{(k + \beta)^\alpha} \]

where:
- \( k \) is the rank of the link,
- \( \alpha \) is the exponent of the power-law distribution,
- \( \beta \) is the fitting parameter.

The exponent \( \alpha \) is a good measure of route concentration or distribution skew, and thus indicates the size of bottlenecks: the higher the value of \( \alpha \), the sharper the concentration of routes in a few links. The fitting parameter \( \beta \) captures the flatter values in the high-rank region (i.e., lower values on the x-axis). This region is not well-modeled by an ordinary Zipf distribution, as its probability mass function would be a straight line in log-log scale over the entire range. The flatter occurrence in the high-rank region is due to the nature of link sampling via route measurement; multiple links are sampled together when each route is measured, and there are generally no duplicate link samples in a route due to the loop-free property of Internet routes. Thus, the occurrence of extremely popular links is limited.

To compare route concentration in different destination regions, we fix the fitting parameter \( \beta \) and find the values of exponent \( \alpha \) for the best fit across fifteen countries. Specifically, \( \beta = 7.8 \) results in the smallest fitting error. In Figures 2 and 14 (Appendix A), the fifteen countries and cities are ordered by increasing values of \( \alpha \) in the range 1.31 to 2.36.

### Causes of Routing Bottlenecks

What causes routing bottlenecks, or high skew/power-law distribution of link occurrence? Often, power-law distributions (especially the Zipf-Mandelbrot distribution) arise from processes involving cost minimization, such as minimizing human-communication costs [30, 51]. Therefore, it is natural to expect that power-laws in link-occurrence distributions are caused by the cost minimization criteria used for route selection and network design in the Internet, both intra- and inter-domain. Additional cost minimization is provided by "hot-potato" routing between domains.

#### Cost Minimization in Inter-Domain Routing

Inter-domain routing policy creates routing bottlenecks in inter-AS links. BGP is the de facto routing protocol for inter-domain (i.e., AS-level) Internet connectivity. The rule-of-thumb BGP policy for choosing inter-AS paths is to minimize the network’s operating cost; whenever several AS paths to a destination are found, the minimum-cost path is selected. This policy aims to minimize the operating costs of routing in the Internet [15, 17].

To determine whether this routing policy contributes to the creation of routing bottlenecks, we run AS-level simulations on CAIDA’s AS topology [1] and compare the operation of the rule-of-thumb routing policy (policy I) with a hypothetical routing policy that distributes routes uniformly across possible inter-domain links (policy II). This hypothetical policy favors inter-domain links that serve fewer AS paths for a particular destination. We simulate this policy using Dijkstra’s algorithm with dynamically changing link weights proportional to the number of BGP paths served.

Figure 3 shows the normalized link occurrence/rank plots for inter-AS links when creating BGP paths from all stub ASes to the ASes in Country1, Country8, and Country15. For policy I, the skew in the high-rank region is much higher (slopes of 0.37 to 0.91) compared to policy II (slopes less than 0.1). This suggests that, even though inter-domain Internet routes may have no physical bottlenecks (or very few, as in Country15), the BGP cost-minimization policy creates inter-domain routing bottlenecks.

#### Cost Minimization in Intra-Domain Network Topology and Routing

Internal AS router-level topology creates intra-domain routing bottlenecks. Most ISPs build and manage hierarchical internal network structures for cost minimization [42, 27], which inherently create routing bottlenecks within ISPs. An ISP typically consists of multiple points of presence (POPs) in different geographic locations, connected via a few high-capacity backbone links. Within each POP, many low-to-mid capacity access links connect the backbone routers to the border routers. ISPs aim to minimize the number of expensive long-distance high-capacity backbone links by multiplexing as much traffic as possible at these few backbone links, making them routing bottlenecks.

We carry out simulations using Tier-1 ISP topologies inferred by Rocketfuel [42]. We construct ingress-egress routes for all possible pairs of access routers using shortest-path routing [29]. Figure 4 shows the simulated normalized link occurrence/rank for three ASes belonging to different ISPs. In all three ASes, the Zipf-Mandelbrot distribution fits accurately with high values of \( \alpha \) (i.e., 1.77 to 1.89) when \( \beta \) is fixed to 2.10 for the best fit and direct skew comparison. This indicates that a few AS internal links are extremely heavily used, while most other internal links are lightly used. Moreover, most of the heavily used links (i.e., 70%, 70%, and 90% of the 10 most heavily used links in each of the three ISPs, respectively) are indeed backbone links connecting distant POPs.

### Characteristics of Bottleneck Links

In this section, we investigate the characteristics of the links in the routing bottlenecks in terms of link types (e.g., intra-AS links, inter-AS links, or IXP-connecting links) and distance from the hosts in the target region (e.g., average router hops) as a backdrop to the design of countermeasures against attacks that exploit bottleneck links. The variety of link types and their distribution make it impossible to design a single ‘one-size-fits-all’ countermeasure. Instead, in Section 5, we discuss several practical countermeasures that account for specific bottleneck link types.

#### Link Types

We first categorize the three link types based on their roles in the Internet topology:
- **Intra-AS links**: Connect two routers owned by the same AS.
- **Inter-AS links**: Connect routers in two different ASes.
- **IXP-connecting links**: Connect to routers in IXPs.

Although the link types are clearly defined, inferring link types via traceroute is known to be difficult and error-prone due to potential inference ambiguity [32]. For example, the AS boundary ambiguity arises because routers at AS boundaries sometimes use IPs borrowed from their neighbor ASes for their interfaces. Borrowed IPs make it difficult to determine whether a link is an intra- or inter-AS link.

Our method of determining link type eliminates the AS boundary ambiguity by utilizing route diversity at the bottleneck links. Unlike previous analyses [32, 21], our experiment measures a large number of disjoint incoming/outgoing routes to/from a bottleneck link, gathering all visible links 1-hop before/after the bottleneck link, which helps infer the link types at AS boundaries without much ambiguity.

Figure 5 summarizes the percentage of link types for the 50 most occurred links in each of the 15 countries. On average, 30% are intra-AS links, 30% are inter-AS links, and 20% are IXP-connecting links. The rest (20%) are undetermined due to lack of traceroute visibility. Notably, intra-AS bottleneck links are located either in Tier-1 or Tier-2 ASes, but never in Tier-3 ASes. This high percentage of intra-AS bottleneck links in Tier-1 or Tier-2 ASes contradicts the common belief that large ISPs distribute routes over their internal links very well using complete knowledge of, and control over, their own networks.

#### Link Distance

We also measure the router-hop distance of the bottleneck links from the hosts in the target regions. To measure a bottleneck link’s distance, we take the average of router-hop distances from multiple hosts in the region. One challenge in measuring the router-hop distance via traceroute is that more than half of the destinations have firewalls in their local networks, preventing discovery of the last few router hops from the destinations. When traceroute does not reach a destination, we assume the presence of an invisible firewall directly followed by the destination. Thus, our distance is a strict lower-bound of the real distance from destination hosts.

Figure 6 shows the average and standard deviation of the link distance of the 50 bottleneck links for each of the 15 countries. The average distance ranges from 6 to 10 router hops, with an average of 7.9 hops. Considering the average length of Internet routes is approximately 17 router hops [12], we conclude that the bottleneck links are located in the middle to slightly closer to the target region on the routes to the target. The distance analysis is consistent with the observation that most bottleneck links are within or connecting Tier-1 ASes.

### Accuracy of Bottleneck Measurements

#### Independence of Route Sources

One common pitfall in Internet measurements is the dependency on vantage point; the location where a measurement is performed can significantly affect the interpretation of the measurement [38]. Here, we argue that our routing-bottleneck results are independent of the selection of route sources \( S \). To show this, we validate our computation of routing-bottleneck results by comparing the connectivity degradation calculated using the original source set \( S \) (i.e., 250 PlanetLab nodes) with that calculated using an independent source set \( S \) (i.e., 86 Ark monitors), as shown in Figure 7. Notice that we select 50 bottleneck links for each country by analyzing the routes measured by PlanetLab nodes.

Connectivity degradation is measured in terms of the degradation ratio, which will be explained in detail in Section 4.2.