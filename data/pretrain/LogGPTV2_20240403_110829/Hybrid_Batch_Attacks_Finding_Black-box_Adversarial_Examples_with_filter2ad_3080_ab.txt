fying a probability distribution from which random samples
are likely to be adversarial. Moon et al. formulate the L∞-
norm black-box attack with  perturbation as a problem of
selecting a set of pixels with + perturbation and applying
the − perturbation to the remaining pixels, such that the ob-
USENIX Association
29th USENIX Security Symposium    1329
jective function deﬁned for misclassiﬁcation becomes a set
maximization problem. Eﬃcient submodular optimization
algorithms are then used to solve the set maximization prob-
lem eﬃciently [33]. These attacks became available after we
started our experiments, so are not included in our experi-
ments. However, our hybrid attack strategy is likely to work
for these new attacks as it boosts the optimization attacks by
providing better starting points, which we expect is beneﬁcial
for most attack algorithms.
Restricted Black-box Attacks. All the previous attacks as-
sume the adversary can obtain complete prediction scores
from the black-box model. Much less information might be
revealed at each model query, however, such as just the top
few conﬁdence scores or, at worst, just the output label.
Ilyas et al. [21], in addition to their main results of NES at-
tack with full prediction scores, also consider scenarios where
prediction scores of the top-k classes or only the model predic-
tion label are revealed. In the case of partial prediction scores,
attackers start from an instance in the target class (or class
other than the original class) and gradually move towards the
original image with the estimated gradient from NES. For the
label-only setting, a surrogate loss function is deﬁned to uti-
lize the strategy of partial prediction scores. Brendel et al. [5]
propose a label-only black-box attack, which starts from an
example in the target class and performs a random walk from
that target example to the seed example. This random walk
procedure often requires many queries. Following this work,
several researchers have worked to reduce the high query cost
of random walk strategies. Cheng et al. formulate a label-only
attack as an optimization problem, reducing the query cost
signiﬁcantly compared to the random walk [12]. Chen et al.
also formulate the label-only attack as an optimization prob-
lem and show this signiﬁcantly improves query eﬃciency [9].
Brunner et al. [6] improve upon the random walk strategy
by additionally considering domain knowledge of image fre-
quency, region masks and gradients from surrogate models.
In our experiments, we assume attackers have access to full
prediction scores, but we believe our methods are also likely
to help in settings where attackers obtain less information
from each query. This is because the hybrid attack boosts
gradient attacks by providing better starting points and is
independent from the speciﬁc attack methods or the types of
query feedback from the black-box model.
3 Hybrid Attacks
Our hybrid attacks combine the transfer and optimization
methods for searching for adversarial examples. Here, we
introduce the threat model of our attack, state the hypothe-
ses underlying the attacks, and presents the general hybrid
attack algorithm. We evaluate the hypotheses and attacks in
Section 4.
Threat Model. In the black-box attack setting, the adversary
does not have direct access to the target model or knowledge
of its parameters, but can use API access to the target model
to obtain prediction conﬁdence scores for a limited number
of submitted queries. We assume the adversary has access
to pretrained local models for the same task as the target
model. These could be directly available or produced from
access to similar training data and knowledge of the model
architecture of the target model. The assumption of having
access to pretrained local models is a common assumption for
research on transfer-based attacks. A few works on substitute
training [27, 36] have used weaker assumptions such as only
having access to a small amount of training data, but have
only been eﬀective so far for very small datasets.
Hypotheses. Our approach stems from three hypotheses
about the nature of adversarial examples:
Hypothesis 1 (H1): Local adversarial examples are better
starting points for optimization attacks than original seeds.
Liu et al. observe that for the same classiﬁcation tasks, dif-
ferent models tend to have similar decision boundaries [29].
Therefore, we hypothesize that, although candidate adversar-
ial examples generated on local models may not fully transfer
to the target model, these candidates are still closer to the
targeted region than the original seed and hence, make better
starting points for optimization attacks.
Hypothesis 2 (H2): Labels learned from optimization attacks
can be used to tune local models. Papernot et al. observe that
generating examples crossing decision boundaries of local
models can produce useful examples for training local models
closer to the target model [36]. Therefore, we hypothesize
that query results generated through the optimization search
queries may contain richer information regarding true target
decision boundaries. These new labeled inputs that are the by-
product of an optimization attack can then be used to ﬁne-tune
the local models to improve their transferability.
Hypothesis 3 (H3): Local models can help direct gradient
search. Since diﬀerent models tend to have similar decision
boundaries for the same classiﬁcation tasks, we hypothesize
that gradient information obtained from local models may
also help better calibrate the estimated gradient of gradient
based black-box attacks on target model.
We are not able to ﬁnd any evidence to support the third hy-
pothesis (H3), which is consistent with Liu et al.’s results [29].
They observed that, for ImageNet models, the gradients of
local and target models are almost orthogonal to each other.
We also tested this for MNIST and CIFAR10, conducting
white-box attacks on local models and storing the intermedi-
ate images and the corresponding gradients. We found that
the local and target models have almost orthogonal gradients
(cosine similarity close to zero) and therefore, a naïve combi-
nation of gradients of local and target model is not feasible.
One possible explanation is the noisy nature of gradients of
deep learning models, which causes the gradient to be highly
sensitive to small variations [3]. Although the cosine similar-
1330    29th USENIX Security Symposium
USENIX Association
input
:Set of seed images X with labels,
local model ensemble F,
target black-box model g
output :Set of successful adversarial examples
1 R ← X (remaining seeds to attack)
2 A ← ∅ (successful adversarial examples)
3 Q ← X (ﬁne-tuning set for local models)
4 while R is not empty do
select and remove the next seed to attack
5
x ← selectSeed(R, F)
6
use local models to ﬁnd a candidate adversarial
7
example
x(cid:48) ← whiteBoxAttack(F,x)
x(cid:63), S ← blackBoxAttack(x,x(cid:48),g)
if x(cid:63) then
A.insert()
end
Q.insert(S)
use byproduct labels to retrain local models
tuneModels(F,Q)
8
9
10
11
12
13
14
15
16 end
17 return A
Algorithm 1: Hybrid Attack.
ity is low, two recent works have attempted to combine the
local gradients and the estimated gradient of the black-box
model by a linear combination [6,13]. However, Brunner et al.
observe that straightforward incorporation of local gradients
does not improve targeted attack eﬃciency much [6]. Cheng
et al. successfully incorporated local gradients into untargeted
black-box attacks, however, they do not consider the more
challenging targeted attack scenario and it is still unclear if
local gradients can help in more challenging cases [6]. Hence,
we do not investigate this further in this paper and leave it
as an open question if there are more sophisticated ways to
exploit local model gradients.
Attack Method. Our hybrid attacks combine transfer and
optimization attacks in two ways based on the ﬁrst two hy-
potheses: we use a local ensemble to select better starting
points for an optimization attack, and use the labeled inputs
obtained in the optimization attack to tune the local models
to improve transferability. Algorithm 1 provides a general
description of the attack. The attack begins with a set of seed
images X, which are natural images that are correctly clas-
siﬁed by the target model, and a set of local models, F. The
attacker’s goal is to ﬁnd a set of successful adversarial exam-
ples (satisfying some attacker goal, such as being classiﬁed in
a target class with a limited perturbation below starting from
a natural image in the source class).
The attack proceeds by selecting the next seed to attack
(line 6). Section 4 considers the case where the attacker only
selects seeds randomly; Section 5 considers ways more so-
phisticated resource-constrained attackers may improve eﬃ-
ciency by prioritizing seeds. Next, the attack uses the local
models to ﬁnd a candidate adversarial example for that seed.
When the local adversarial example is found, we ﬁrst check
its transferability and if the seed directly transfers, we proceed
to attack the next seed. If the seed fails to directly transfer,
the black-box optimization attack is then executed starting
from that candidate. The original seed is also passed into the
black-box attack (line 9) since the adversarial search space is
deﬁned in terms of the original seed x, not the starting point
found using the local models, x(cid:48). This is because the space of
permissible inputs is deﬁned based on distance from the orig-
inal seed, which is a natural image. Constraining with respect
to the space of original seed is important because we need to
make sure the perturbations from our method are still visually
indistinguishable from the natural image. If the black-box
attack succeeds, it returns a successful adversarial example,
x(cid:63), which is added to the returned set. Regardless of success,
the black-box attack produces input-label pairs (S ) during the
search process which can be used to tune the local models
(line 15), as described in Section 4.6.
4 Experimental Evaluation
In this section, we report on experiments to validate our hy-
pothesis, and evaluate the hybrid attack methods. Section 4.1
describes the experimental setup; Section 4.2 describes the at-
tack conﬁguration; Section 4.3 describes the attack goal; Sec-
tion 4.4 reports on experiments to test the ﬁrst hypothesis from
Section 3 and measure the eﬀectiveness of hybrid attacks; Sec-
tion 4.5 improves the attack for targeting robust models, and
Section 4.6 evaluates the second hypothesis, showing the im-
pact of tuning the local models using the label byproducts.
For all of these, we focus on comparing the cost of the attack
measured as the average number of queries needed per adver-
sarial example found across a set of seeds. In Section 5, we
revisit the overall attack costs in light of batch attacks that
can prioritize which seeds to attack.
4.1 Datasets and Models
We evaluate our attacks on three popular image classiﬁcation
datasets and a variety of state-of-the-art models.
MNIST. MNIST [25] is a dataset of 70,000 28× 28 greyscale
images of handwritten digits (0–9), split into 60,000 training
and 10,000 testing samples. For our normal (not adversari-
ally trained) MNIST models, we use the pretrained MNIST
models of Bhagoji et al. [4], which typically consist of con-
volutional layers and fully connected layers. We use their
MNIST model A as the target model, and models B–D as
local ensemble models. To consider the more challenging sce-
nario of attacking a black-box robust model, we use Madry’s
robust MNIST model, which demonstrates strong robustness
even against the best white-box attacks (maintaining over 88%
accuracy for L∞ attacks with  = 0.3) [32].
USENIX Association
29th USENIX Security Symposium    1331
CIFAR10. CIFAR10 [23] consists of 60,000 32× 32 RGB
images, with 50,000 training and 10,000 testing samples for
object classiﬁcation (10 classes in total). We train a stan-
dard DenseNet model and obtain a test accuracy of 93.1%,
which is close to state-of-the-art performance. To test the ef-
fectiveness of our attack on robust models, we use Madry’s
CIFAR10 Robust Model [32]. Similarly, we also use the nor-
mal CIFAR10 target model and the standard DenseNet (Std-
DenseNet) model interchangeably. For our normal local mod-
els, we adopt three simple LeNet structures [26], varying the
number of hidden layers and hidden units.2 For simplicity,
we name the three normal models NA, NB and NC where
NA has the fewest parameters and NC has the most parame-
ters. To deal with the lower eﬀectiveness of attacks on robust
CIFAR10 model (Section 4.4), we also adversarially train
two deep CIFAR10 models (DenseNet, ResNet) similar to the
Madry robust model as robust local models. The adversarially-
trained DenseNet and ResNet models are named R-DenseNet
and R-ResNet.
ImageNet. ImageNet [14] is a dataset closer to real-world
images with 1000 categories, commonly used for evaluating
state-of-the-art deep learning models. We adopt the following
pretrained ImageNet models for our experiments: ResNet-
50 [19], DenseNet [20], VGG-16, and VGG-19 [37] (all from
https://keras.io/applications/). We take DenseNet as the target
black-box model and the remaining models as the local en-
semble.
4.2 Attack Conﬁguration
For the hybrid attack, since we have both the target model
and local model, we have two main design choices: (1) which
white-box attacks to use for the local models , and (2) which
optimization attacks to use for the target model.
Local Model Conﬁgurations. We choose an ensemble of
local models in our hybrid attacks. This design choice is
motivated by two facts: First, diﬀerent models tend to have
signiﬁcantly diﬀerent direct transfer rates to the same target
model (see Figure 1), when evaluated individually. Therefore,
taking an ensemble of several models helps avoid ending
up with a single local model with a very low direct transfer
rate. Second, consistent with the ﬁndings of Liu et al. [29]
on attacking an ensemble of local models, for MNIST and
CIFAR10, we ﬁnd that the ensemble of normal local mod-
els yields the highest transfer rates when the target model
is a normally trained model (note that this does not hold for
robust target model, as shown in Figure 1 and discussed fur-
ther in Section 4.5). We validate the importance of normal
local ensemble against normal target model by considering
,k = 1, ..., N)
diﬀerent combinations of local models (i.e.,
(cid:16)N
(cid:17)
k
2We also tested with deep CNN models as our local ensembles. However,
they provide only slightly better performance compared to simple CIFAR10
models, while the ﬁne-tuning cost is much higher.
and checking their corresponding transfer rates and the av-
erage query cost. We adopt the same approach as proposed
by Liu et al. [29] to attack multiple models simultaneously,
where the attack loss is deﬁned as the sum of the individual
model loss. In terms of transfer rate, we observe that a single
CIFAR10 or MNIST normal model can achieve up to 53%
and 35% targeted transfer rate respectively, while an ensem-
ble of local models can achieve over 63% and 60% transfer
rate. In terms of the average query cost against normal target
models, compared to a single model, an ensemble of local
models on MNIST and CIFAR10 can save on average 53%
and 45% of queries, respectively. Since the ensemble of nor-
mal local models provides the highest transfer rate against
normal target models, to be consistent, we use that conﬁgu-
ration in all our experiments attacking normal models. We
perform white-box PGD [32] attacks (100 iterative steps) on
the ensemble loss. We choose the PGD attack as it gives a
high transfer rate compared to the fast gradient sign method
(FGSM) method [17].
Optimization Attacks. We use two state-of-the-art gradient
estimation based attacks in our experiments: NES, a natu-
ral evolution strategy based attack [21] and AutoZOOM, an
autoencoder-based zeroth-order optimization attack [43] (see
Section 2.2). These two methods are selected as all of them are
shown to improve upon [10] signiﬁcantly in terms of query
eﬃciency and attack success rate. We also tested with the
BanditsTD attack, an improved version of the NES attack that
additionally incorporates time and data dependent informa-
tion [22]. However, we ﬁnd that BanditsTD is not competitive
with the other two attacks in our attack scenario and therefore
we do not include its results here.3 Both tested attacks follow
an attack method that attempts queries for a given seed un-
til either a successful adversarial example is found or the set
maximum query limit is reached, in which case they terminate
with a failure. For MNIST and CIFAR10, we set the query