### Defining a Probability Distribution for Adversarial Samples

Moon et al. formulate the L∞-norm black-box attack with perturbation as a problem of selecting a set of pixels to which +ε perturbations are applied, and applying -ε perturbations to the remaining pixels. This formulation transforms the objective function for misclassification into a set maximization problem. Efficient submodular optimization algorithms are then used to solve this set maximization problem [33]. These attacks were developed after our experiments began, so they are not included in our study. However, our hybrid attack strategy is likely to be effective for these new attacks, as it enhances optimization attacks by providing better starting points, which is generally beneficial for most attack algorithms.

### Restricted Black-Box Attacks

Previous attacks assume that the adversary can obtain complete prediction scores from the black-box model. In reality, the information revealed at each model query may be limited, such as only the top few confidence scores or, at worst, just the output label. Ilyas et al. [21] consider scenarios where only the top-k class prediction scores or the model's prediction label are revealed. In the case of partial prediction scores, attackers start from an instance in the target class (or a class other than the original class) and gradually move towards the original image using the estimated gradient from NES. For the label-only setting, a surrogate loss function is defined to utilize the strategy of partial prediction scores. Brendel et al. [5] propose a label-only black-box attack that starts from an example in the target class and performs a random walk from that target example to the seed example. This random walk procedure often requires many queries. Following this work, several researchers have worked to reduce the high query cost of random walk strategies. Cheng et al. [12] and Chen et al. [9] formulate label-only attacks as optimization problems, significantly reducing the query cost compared to random walk methods. Brunner et al. [6] improve upon the random walk strategy by incorporating domain knowledge of image frequency, region masks, and gradients from surrogate models.

In our experiments, we assume attackers have access to full prediction scores, but we believe our methods are also likely to help in settings where less information is obtained from each query. This is because the hybrid attack boosts gradient-based attacks by providing better starting points, independent of the specific attack methods or the types of query feedback from the black-box model.

### 3. Hybrid Attacks

Our hybrid attacks combine transfer and optimization methods to search for adversarial examples. We introduce the threat model, state the underlying hypotheses, and present the general hybrid attack algorithm. The hypotheses and attacks are evaluated in Section 4.

#### Threat Model

In the black-box attack setting, the adversary does not have direct access to the target model or its parameters but can use API access to obtain prediction confidence scores for a limited number of submitted queries. We assume the adversary has access to pretrained local models for the same task as the target model. These could be directly available or produced from similar training data and knowledge of the target model's architecture. The assumption of having access to pretrained local models is common in research on transfer-based attacks. Some works on substitute training [27, 36] have used weaker assumptions, such as access to a small amount of training data, but have been effective only for very small datasets.

#### Hypotheses

Our approach is based on three hypotheses about the nature of adversarial examples:

**Hypothesis 1 (H1):** Local adversarial examples are better starting points for optimization attacks than original seeds. Liu et al. [29] observe that different models tend to have similar decision boundaries for the same classification tasks. Therefore, although candidate adversarial examples generated on local models may not fully transfer to the target model, they are still closer to the targeted region than the original seed, making them better starting points for optimization attacks.

**Hypothesis 2 (H2):** Labels learned from optimization attacks can be used to tune local models. Papernot et al. [36] observe that generating examples that cross the decision boundaries of local models can produce useful examples for training local models closer to the target model. Therefore, we hypothesize that query results generated through the optimization search queries contain richer information regarding true target decision boundaries. These new labeled inputs, which are by-products of an optimization attack, can be used to fine-tune the local models to improve their transferability.

**Hypothesis 3 (H3):** Local models can help direct gradient search. Since different models tend to have similar decision boundaries for the same classification tasks, we hypothesize that gradient information obtained from local models may help better calibrate the estimated gradient of gradient-based black-box attacks on the target model.

We did not find any evidence to support Hypothesis 3 (H3), consistent with Liu et al.'s results [29]. They observed that, for ImageNet models, the gradients of local and target models are almost orthogonal to each other. We also tested this for MNIST and CIFAR10, conducting white-box attacks on local models and storing intermediate images and corresponding gradients. We found that the local and target models have almost orthogonal gradients (cosine similarity close to zero), making a naive combination of gradients infeasible. One possible explanation is the noisy nature of gradients in deep learning models, which causes the gradient to be highly sensitive to small variations [3].

#### Attack Method

Our hybrid attacks combine transfer and optimization attacks in two ways based on the first two hypotheses: we use a local ensemble to select better starting points for an optimization attack, and use the labeled inputs obtained in the optimization attack to tune the local models to improve transferability. Algorithm 1 provides a general description of the attack.

```plaintext
Algorithm 1: Hybrid Attack.
Input: Set of seed images X with labels, local model ensemble F, target black-box model g
Output: Set of successful adversarial examples A
1. R ← X (remaining seeds to attack)
2. A ← ∅ (successful adversarial examples)
3. Q ← X (fine-tuning set for local models)
4. while R is not empty do
5.   x ← selectSeed(R, F) (select and remove the next seed to attack)
6.   x' ← whiteBoxAttack(F, x) (use local models to find a candidate adversarial example)
7.   x*, S ← blackBoxAttack(x, x', g) (execute black-box attack)
8.   if x* then
9.     A.insert(x*) (add successful adversarial example to set A)
10.    end
11.    Q.insert(S) (use byproduct labels to retrain local models)
12.    tuneModels(F, Q) (tune local models)
13. end
14. return A
```

The attack begins with a set of seed images X, which are natural images correctly classified by the target model, and a set of local models F. The attacker's goal is to find a set of successful adversarial examples (e.g., being classified in a target class with a limited perturbation below a threshold, starting from a natural image in the source class).

The attack proceeds by selecting the next seed to attack (line 5). Section 4 considers the case where the attacker selects seeds randomly; Section 5 considers more sophisticated resource-constrained attackers who may improve efficiency by prioritizing seeds. Next, the attack uses the local models to find a candidate adversarial example for that seed. If the local adversarial example transfers directly, the attack moves to the next seed. If not, the black-box optimization attack is executed starting from that candidate. The original seed is also passed into the black-box attack (line 9) because the adversarial search space is defined in terms of the original seed x, not the starting point found using the local models, x'. Constraining the search space with respect to the original seed is important to ensure that the perturbations remain visually indistinguishable from the natural image. If the black-box attack succeeds, it returns a successful adversarial example, x*, which is added to the returned set. Regardless of success, the black-box attack produces input-label pairs (S) during the search process, which can be used to tune the local models (line 12), as described in Section 4.6.

### 4. Experimental Evaluation

In this section, we report on experiments to validate our hypotheses and evaluate the hybrid attack methods. Section 4.1 describes the experimental setup; Section 4.2 describes the attack configuration; Section 4.3 describes the attack goal; Section 4.4 reports on experiments to test the first hypothesis and measure the effectiveness of hybrid attacks; Section 4.5 improves the attack for targeting robust models, and Section 4.6 evaluates the second hypothesis, showing the impact of tuning the local models using the label byproducts.

For all these, we focus on comparing the cost of the attack measured as the average number of queries needed per adversarial example found across a set of seeds. In Section 5, we revisit the overall attack costs in light of batch attacks that can prioritize which seeds to attack.

#### 4.1 Datasets and Models

We evaluate our attacks on three popular image classification datasets and a variety of state-of-the-art models.

**MNIST.** MNIST [25] is a dataset of 70,000 28×28 grayscale images of handwritten digits (0–9), split into 60,000 training and 10,000 testing samples. For our normal (not adversarially trained) MNIST models, we use the pretrained MNIST models of Bhagoji et al. [4], which typically consist of convolutional layers and fully connected layers. We use their MNIST model A as the target model, and models B–D as local ensemble models. To consider the more challenging scenario of attacking a black-box robust model, we use Madry’s robust MNIST model, which demonstrates strong robustness even against the best white-box attacks (maintaining over 88% accuracy for L∞ attacks with ε = 0.3) [32].

**CIFAR10.** CIFAR10 [23] consists of 60,000 32×32 RGB images, with 50,000 training and 10,000 testing samples for object classification (10 classes in total). We train a standard DenseNet model and obtain a test accuracy of 93.1%, which is close to state-of-the-art performance. To test the effectiveness of our attack on robust models, we use Madry’s CIFAR10 Robust Model [32]. Similarly, we also use the normal CIFAR10 target model and the standard DenseNet (Std-DenseNet) model interchangeably. For our normal local models, we adopt three simple LeNet structures [26], varying the number of hidden layers and hidden units. For simplicity, we name the three normal models NA, NB, and NC, where NA has the fewest parameters and NC has the most parameters. To deal with the lower effectiveness of attacks on the robust CIFAR10 model (Section 4.4), we also adversarially train two deep CIFAR10 models (DenseNet, ResNet) similar to the Madry robust model as robust local models. The adversarially-trained DenseNet and ResNet models are named R-DenseNet and R-ResNet.

**ImageNet.** ImageNet [14] is a dataset closer to real-world images with 1000 categories, commonly used for evaluating state-of-the-art deep learning models. We adopt the following pretrained ImageNet models for our experiments: ResNet-50 [19], DenseNet [20], VGG-16, and VGG-19 [37] (all from https://keras.io/applications/). We take DenseNet as the target black-box model and the remaining models as the local ensemble.

#### 4.2 Attack Configuration

For the hybrid attack, since we have both the target model and local models, we have two main design choices: (1) which white-box attacks to use for the local models, and (2) which optimization attacks to use for the target model.

**Local Model Configurations.** We choose an ensemble of local models in our hybrid attacks. This design choice is motivated by two facts: First, different models tend to have significantly different direct transfer rates to the same target model (see Figure 1), when evaluated individually. Therefore, taking an ensemble of several models helps avoid ending up with a single local model with a very low direct transfer rate. Second, consistent with the findings of Liu et al. [29] on attacking an ensemble of local models, for MNIST and CIFAR10, we find that the ensemble of normal local models yields the highest transfer rates when the target model is a normally trained model (note that this does not hold for robust target models, as shown in Figure 1 and discussed further in Section 4.5). We validate the importance of the normal local ensemble against the normal target model by considering different combinations of local models and checking their corresponding transfer rates and the average query cost. We adopt the same approach as proposed by Liu et al. [29] to attack multiple models simultaneously, where the attack loss is defined as the sum of the individual model loss. In terms of transfer rate, we observe that a single CIFAR10 or MNIST normal model can achieve up to 53% and 35% targeted transfer rate, respectively, while an ensemble of local models can achieve over 63% and 60% transfer rate. In terms of the average query cost against normal target models, compared to a single model, an ensemble of local models on MNIST and CIFAR10 can save on average 53% and 45% of queries, respectively. Since the ensemble of normal local models provides the highest transfer rate against normal target models, to be consistent, we use that configuration in all our experiments attacking normal models. We perform white-box PGD [32] attacks (100 iterative steps) on the ensemble loss. We choose the PGD attack as it gives a high transfer rate compared to the fast gradient sign method (FGSM) [17].

**Optimization Attacks.** We use two state-of-the-art gradient estimation-based attacks in our experiments: NES, a natural evolution strategy-based attack [21], and AutoZOOM, an autoencoder-based zeroth-order optimization attack [43] (see Section 2.2). These two methods are selected because they significantly improve upon [10] in terms of query efficiency and attack success rate. We also tested with the BanditsTD attack, an improved version of the NES attack that additionally incorporates time and data-dependent information [22]. However, we find that BanditsTD is not competitive with the other two attacks in our attack scenario and therefore do not include its results here. Both tested attacks follow an attack method that attempts queries for a given seed until either a successful adversarial example is found or the set maximum query limit is reached, in which case they terminate with a failure. For MNIST and CIFAR10, we set the query limit to 10,000.