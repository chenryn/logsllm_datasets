lects updated values of state variables modified during a packet
transaction and appends them to the packet. As the packet passes
through the chain, FTC replicates piggybacked state updates in
servers hosting the middleboxes.
Transactional packet processing: To accommodate non-determinism
due to concurrency, we model the processing of a packet as a trans-
action, where concurrent accesses to shared state are serialized to
ensure that consistent state is captured and replicated. In other sys-
tems, the interleaved order of lock acquisitions and state variable
updates between threads is non-deterministic, yet externally ob-
servable. Capturing and replaying this order is complex and incurs
high performance overheads [51]. FTC uses transactional packet
processing to avoid the complexity and overhead.
This model is easily adaptable to hybrid transactional memory,
where we can take advantage of the hardware support for transac-
tions [13]. This allows FTC to use modern hardware transactional
memory for better performance, when the hardware is present.
We also observe that this model does not reduce concurrency
in popular middleboxes. First, these middleboxes already serialize
access to state variables for correctness. For instance, a load bal-
ancer and a NAT ensure connection persistence (i.e., a connection is
always directed to a unique destination) while accessing a shared
flow table [9, 53]. Concurrent threads in these middleboxes must
coordinate to provide this property.
Moreover, most middleboxes share only a few state variables [29,
32]. Kablan et al. surveyed five middleboxes for their access pat-
terns to state [29]. These middleboxes mostly perform only one
or two read/write operations per packet. The behavior of these
middleboxes allow packet transactions to run concurrently most of
the time.
In-chain replication: Consensus-based state replication [36, 40]
requires 2f + 1 replicas for each middlebox to reliably detect and
recover from f failures. A high-availability cluster approach re-
quires f + 1 replicas as it relies on a fault tolerant coordinator for
failure detection. For a chain of n middleboxes, these schemes need
n ×(2f + 1) and n ×(f + 1) replicas. Replicas are placed on separate
servers, and a naïve placement requires the same number of servers.
FTC observes that packets already flow through a chain; each
server hosting a middlebox of the chain can serve as a replica for
the other middleboxes. Instead of allocating dedicated replicas, FTC
replicates the state of middleboxes across the chain. In this way, FTC
tolerates f failures without the cost of dedicated replica servers.
State piggybacking: To replicate state modified by a packet, ex-
isting schemes send separate messages to replicas. In FTC, a packet
carries its own state updates. State piggybacking is possible, as a
small number of state variables [33] are modified with each packet.
Since state updated during processing a packet is replicated in
servers hosting the chain, relevant state is already transferred and
replicated when the packet leaves the chain.
No checkpointing and no replay: FTC replicates state values at
the granularity of packet transactions, rather than taking snapshots
of state or replaying packet processing operations. During normal
operation, FTC removes state updates that have been applied in
all replicas to bound memory usage of replication. Furthermore,
replicating the values of state variables allows for fast state recovery
during failover.
Centralized orchestration: In our system, a central orchestrator
manages the network and chains. The orchestrator deploys fault
tolerant chains, reliably monitors them, detects their failures, and
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
Figure 2: Normal operation for a single middlebox. The head
and middlebox reside in the same server. The head tracks state up-
dates due to middlebox packet processing and appends a piggyback
log containing these updates to the packet. As the packet passes
through the chain, other replicas replicate the piggyback log and
apply the carried state updates to their state stores. Finally, the tail
strips the piggyback log and releases the packet.
initiates failure recovery. The orchestrator functionality is provided
by a fault tolerant SDN controller [7, 31, 41]. After deploying a
chain, the orchestrator is not involved in normal chain operations
to avoid becoming a performance bottleneck.
In the following sections, we first describe our protocol for a
single middlebox in § 4, then we extend this protocol for a chain of
middleboxes in § 5.
4 FTC FOR A SINGLE MIDDLEBOX
In this section, we present our protocol for a single middlebox.
We first describe our protocol with a single threaded middlebox
where state is replicated by single threaded replicas. We extend our
protocol to support multithreaded middleboxes and multithreaded
replication in § 4.2 and § 4.3. Our system allows middlebox instances
to scale the number of threads in response to load, but scaling the
number of instances is outside scope of this work.
4.1 Middlebox State Replication
We adapt the chain replication protocol [58] for middlebox state
replication. For reliable state transmission between servers, FTC
uses sequence numbers, similar to TCP, to handle out-of-order de-
liveries and packet drops within the network.
Figure 2 shows our protocol for providing fault tolerance for a
middlebox. FTC replicates the middlebox state in f + 1 replicas
during normal middlebox operations. Replicas r1, . . . , rf +1 form
the replication group for middlebox m where r1 and rf +1 are called
the head and tail replicas. Each replica is placed on a separate server
whose failure is isolated. With state replicated in f + 1 replicas, the
state remains available even if f replicas fail.
The head is co-located with the middlebox in the same server. The
middlebox state is separated from the middlebox logic and is stored
in the head’s state store. The head provides a state management API
for the middlebox to read and write state during packet processing.
For an existing middlebox to use FTC, its source code must be
modified to call our API for state reads and writes.
Normal operation of protocol: As shown in Figure 2, the middle-
box processes a packet, and the head constructs and appends a
piggyback log to the packet. The piggyback log contains a sequence
number and a list of state updates during packet processing. As the
packet traverses the chain, each subsequent replica replicates the
piggyback log and applies the state updates to its state store. After
replication, the tail strips the piggyback log and releases the packet.
The head tracks middlebox updates to state using a monotoni-
cally increasing sequence number. After a middlebox finishes pro-
cessing a packet, the head increments its sequence number only if
state was modified during packet processing. The head appends the
state updates (i.e., state variables modified in processing the packet
and their updated values) and sequence number to the packet as a
piggyback log. If no state was updated, the head adds a no-op pig-
gyback log. The head then forwards the packet to the next replica.
Each replica continuously receives packets with piggyback logs.
If a packet is lost, a replica requests its predecessor to retransmit the
piggyback log with the lost sequence number. A replica keeps the
largest sequence number that it has received in order (i.e., the replica
has already received all piggyback logs with preceding sequence
numbers). Once all prior piggyback logs are received, the replica
applies the piggyback log to its local state store and forwards the
packet to the next replica.
The tail replicates state updates, strips the piggyback log from
the packet, and releases the packet to its destination. Subsequently,
the tail periodically disseminates its largest sequence number to
the head. The sequence number is propagated to all replicas so they
can prune their piggyback logs up to this sequence number.
Correctness: Each replica replicates the per-packet state updates
in order. As a result, when a replica forwards a packet, it has repli-
cated all preceding piggyback logs. Packets also pass through the
replication group in order. When a packet reaches a replica, prior
replicas have replicated the state updates carried by this packet.
Thus, when the tail releases a packet, the packet has already tra-
versed the entire replication group. The replication group has f + 1
replicas allowing FTC to tolerate f failures.
Failure recovery: FTC relies on a fault tolerant orchestrator to
reliably detect failures. Upon failure detection, the replication group
is repaired in three steps: adding a new replica, recovering the lost
state from an alive replica, and steering traffic through the new
replica.
In the event of a head failure, the orchestrator instantiates a
new middlebox instance and replica, as they reside on the same
server. The orchestrator also informs the new replica about other
alive replicas. If the new replica fails, the orchestrator restarts the
recovery procedure.
Selecting a replica as the source for state recovery depends on
how state updates propagate through the chain. We can reason
about this using the log propagation invariant: for each replica
except the tail, its successor replica has the same or prior state,
since piggyback logs propagate in order through the chain.
If the head fails, the new replica retrieves the state store, pig-
gyback logs, and sequence number from the immediate successor
to the head. If other replicas fail, the new replica fetches the state
from the immediate predecessor.
!!!"!"!"!"!"#$!"#$HeadTailPiggyback logPacketReplica!!Middlebox!"!"State store………Fault Tolerant Service Function Chaining
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
To ensure that the log propagation invariant holds during re-
covery, the replica that is the source for state recovery discards
any out-of-order packets that have not been applied to its state
store and will no longer admit packets in flight. If the contacted
replica fails during recovery, the orchestrator detects this failure
and re-initializes the new replica with the new set of alive replicas.
Finally, the orchestrator updates routing rules in the network
to steer traffic through the new replica. If multiple replicas have
failed, the orchestrator waits until all new replicas acknowledge that
they have successfully recovered the state. Then, the orchestrator
updates the necessary routing rules from the tail to the head.
4.2 Concurrent Packet Processing
To achieve higher performance, we augment our protocol to sup-
port multithreaded packet processing and state replication in the
middlebox and the head. Other replicas are still single threaded.
Later in § 4.3, we will support multithreaded replications in other
replicas.
In concurrent packet processing, multiple packets are processed
in interleaving threads. The threads can access the same state vari-
ables in parallel. To accommodate this parallelism, FTC must con-
sistently track parallel state updates. We introduce transactional
packet processing that effectively serializes packet processing. This
model supports concurrency if packet transactions access disjoint
subsets of state.
Transactional Packet Processing: In concurrent packet processing,
the effects on state variables must be serializable. Further, state
updates must be applied to replicas in the same order so that the
system can be restored to a consistent state during failover. To
support this requirement, replay based replication systems, such
as FTMB [51], track all state accesses, including state reads, which
can be challenging to perform efficiently.
In transactional packet processing, state reads and writes by a
packet transaction have no impact on another concurrently pro-
cessed packet. This isolation allows us to only keep track of the
relative order between transactions, without needing to track all
state variable dependencies.
We realize this model by implementing a software transactional
memory (STM) API for middleboxes. When a packet arrives, the
runtime starts a new packet transaction in which multiple reads
and writes can be performed. Our STM API uses fine grained strict
two phase locking (similar to [14]) to provide serializability. Our
API uses a wound-wait scheme that aborts transaction to prevent
possible deadlocks if a lock ordering is not known in advance. An
aborted transaction is immediately re-executed. The transaction
completes when the middlebox releases the packet.
Using two phase locking, the head runtime acquires necessary
locks during a packet transaction. We simplify lock management
using state space partitioning, by using the hash of state variable
keys to map keys to partitions, each with its own lock. The state par-
titioning is consistent across all replicas, and to reduce contention,
the number of partitions is selected to exceed the maximum number
of CPU cores.
At the end of a transaction, the head atomically increments its
sequence number only if state was updated during this packet
transaction. Then, the head constructs a piggyback log containing
the state updates and the sequence number. After the transaction
completes, the head appends the piggyback log to the packet and
forwards the packet to the next replica.
Correctness: Due to mutual exclusion, when a packet transaction
includes an updated state variable in a piggyback log, no other
concurrent transaction has modified this variable, thus the included
value is consistent with the final value of the packet transaction. The
head’s sequence number maps this transaction to a valid serial order.
Replicated values are consistent with the head, because replicas
apply state updates of the transaction in the sequence number order.
4.3 Concurrent State Replication
Up to now FTC provides concurrent packet processing but does not
support concurrent replication. The head uses a single sequence
number to determine a total order of transactions that modify state
partitions. This total ordering eliminates multithreaded replication
at successor replicas.
To address the possible replication bottleneck, we introduce data
dependency vectors to support concurrent state replication. Data
dependency tracking is inspired by the vector clocks algorithm [19],
but rather than tracking points in time when events happen for
processes or threads, FTC tracks the points in time when packet
transactions modify state partitions.
This approach provides more flexibility compared to tracking
dependencies between threads and replaying their operations to
replicate the state [51]. First, it easily supports vertical scaling as
a running middlebox can be replaced with a new instance with
different number of CPU cores. Second, a middlebox and its replicas
can also run with different number of threads. The state-of-the-
art [51] requires the same number of threads with a one-to-one
mapping between a middlebox and its replicas.
Data dependency vectors: We use data dependency vectors to
determine a partial order of transactions in the head. Each element
of this vector is a sequence number associated to a state partition.
A packet piggybacks this partial order to replicas enabling them to
replicate transactions with more concurrency; a replica can apply
and replicate a transaction in a different serial order that is still
equivalent to the head.
The head keeps a data dependency vector and serializes parallel
accesses to this vector using the same state partition locks from our
transactional packet processing. The head maintains its dependency
vector using the following rules. A read-only transaction does not
change the vector. For other transactions, the head increments the
sequence number of a state partition that received any read or
write.