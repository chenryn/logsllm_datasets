Figure 7: Missed deadlines (left) & bandwidth of long 
flows (right) in real implementation vs. simulation 
500 KB, and have a deadline of 5-25 ms. In addition to this OLDI 
traffic, we have two servers initiate long-lived flows of 10MB to 
the root, once every 80 ms. The experiment lasts for the duration 
of 1000 OLDI queries. The aggregate traffic results in a network 
utilization of around 10% which is a realistic load for datacenters, 
and not an artificial overload scenario. Our goal in this experiment 
is to compare DCTCP and D2TCP in the real implementation.  
In Figure 7(left) we show the percentage of missed deadlines 
in  the  real  implementation  and  in  simulation  of  DCTCP  and 
D2TCP (shown as DCTCP-Real, D2-Real, DCTCP-Sim, and D2-
Sim),  while  varying  the  fan-in  between  20  and  40.  We  discuss 
DCTCP-Sim  and  D2TCP-Sim  results  in  Section  4.1.4.  We  see 
that,  across  the  board,  D2-Real  misses  fewer  deadlines  than 
DCTCP-Real,  with  the  difference  increasing  as  we  increase  the 
fan-in. At a fan-in of 40, DCTCP-Real misses 15.2% of deadlines, 
whereas D2-Real misses 12.3% of deadlines. 
In  Figure  7(right)  we  show  the  throughput  achieved  by  the 
non-deadline long flows for varying fan-in degrees. We see that at 
a fan-in degree of 40, the throughput achieved by D2-Real is 451 
Mbps, which compares favorably to the throughput of 438 Mbps 
for DCTCP-Real. Therefore, the performance of deadline flows is 
not improved by degrading the throughput of non-deadline flows. 
4.1.4  Simulator validation 
To  validate  our 
simulator,  we  compare 
real 
implementation and simulation results in Figure 7. We notice that 
the absolute numbers from the simulation are slightly off from the 
real  implementation.  For  example,  the  percentage  of  missed 
deadlines for both DCTCP and D2TCP under simulation is lower 
than  those  under  real  implementation.  Such  discrepancy  is 
expected as simulations do not capture all the details and nuances 
of  a  real  system,  such  as  burst-smoothing  jitter  caused  by 
unpredictable system events, interrupt coalescing, Large Segment 
Offload (LSO), and other TCP quirks.  
Nevertheless,  the  relative  performance  difference  between 
DCTCP and D2TCP, and the trend in that difference, are similar 
across  simulation  and  real  implementation. At  a  fan-in  of  30  in 
Figure 7, D2TCP achieves 16% reduction in missed deadlines over 
DCTCP under simulations, whereas under real implementation the 
reduction  is  15%. As  we  increase  the  fan-in  degree,  the  relative 
performance  difference  between  DCTCP  and  D2TCP  increases 
under both simulation and real implementation. Because of these 
key similarities, we believe that our at-scale simulation results are 
trustworthy. In addition, we also ensure that our simulation results 
closely match published DCTCP and D3 results [25], as we show 
in the next section.  
4.2  At-Scale Simulations 
We  now  present  our  at-scale  simulations.  We  model  the 
network 
typical  production 
deployments.  Recall  from  Section  1  that  D2TCP’s  goal  is  to 
topology  and 
traffic  after 
the 
the 
(num hosts per rack) 
Gbps
ToR
machine
machine
machine
machine
machine
s
k
n
i
l
s
p
b
G
1
s
k
n
i
l
s
p
b
G
1
Fabric switch with large buffers
r
e
p
s
t
s
o
h
m
u
n
(
s
p
b
G
)
k
c
a
r
(
n
u
m
h
o
G
sts 
b
p
p
s
e
r r
a
c
k
) 
ToR
machine
machine
machine
machine
machine
s
k
n
i
l
s
p
b
G
1
ToR
machine
machine
machine
machine
machine
Figure 8: Simulated network 
reduce  the  percentage  of  missed  deadlines  without  degrading 
throughput  for  long-lived  flows.  Therefore,  we  focus  on  these 
metrics in our at-scale simulations.  
4.2.1  Simulation methodology 
We  implemented  DCTCP  and  D2TCP  on  top  of  ns-3's TCP 
New Reno protocol [19], and enabled the marking of CE bits in 
the  switch  model  of  ns-3.  For  D3,  we  wrote  both  the  end-host 
protocol and the switch logic, based on the details in [25]. We set 
D3’s  base  rate  to  be  one  segment  per  RTT.  Further,  we  use  the 
same  RRQ  packet  format  described  in  [25]  including  the  8-bit 
bytes-per-microsecond  field. All  DCTCP  and  D2TCP  parameters 
match those in Section 4.1.1. 
We  performed  our  simulations  on  the  network  depicted  in 
Figure  8,  which  uses  a  fat-tree  topology  typical  of  datacenter 
networks. There are 25 racks with each rack having up to 40 end-
host  machines.  Thus,  our  simulations  capture  the  behavior  of  a 
1000-machine deployment. Each end-host connects to the top-of-
rack  (ToR)  switch  via  a  1  Gbps  link.  Because  the  bottleneck  in 
datacenter  networks  is  usually  the ToR  switch  [10]  [1]  [25],  we 
abstracted away the rest of the fat-tree topology, replacing it with 
one large fabric switch with large buffers. Each ToR is connected 
to the fabric switch via a single link with a line rate equal to 1 * 
number-of-hosts-in-a-rack  Gbps.  We  sized  the  packet  buffers  in 
the ToR switches to match typical buffer sizes of shallow-buffered 
switches in real data centers (4MB) [1]. We set the link latencies 
to  20  µs,  achieving  an  average  of  RTT  of  200  µs,  which  is 
representative of datacenter network RTTs. 
We  ran  a  set  of  five  synthetic  OLDI  applications  on  the 
network,  equally  dividing  the  total  number  of  end-hosts  among 
the  applications.  The  assignment  of  an  application  node  to  a 
physical  end-host  is  random  to  capture  the  effects  of  (1) 
applications  dynamically  requesting  and  relinquishing  virtual 
machines  in  the  data center, and  (2)  virtual  to physical  machine 
assignment being completely fluid. Each application consists of a 
set  of  five  identical  OLDI  trees,  each  with  one  parent  and  n 
leaves,  which  have  the  same  settings  for  leaf-to-parent  message 
size and for deadlines. These settings are different across the five 
applications. We varied n, the number of leaves per parent, in the 
trees to explore varying degree of fan-in-bursts. 
The  distributions  of  message  sizes  and  of  deadlines  in  real 
OLDIs  are  publicly  available  [1].  However,  details  such  as  the 
exact deadline for given leaf-to-parent ratio and message size, are 
not  publicly  available.  Therefore,  like  [25],  we  chose  semi-
synthetic  values 
the  aforementioned 
distributions. 
for  deadlines  using 
Whether  a  message  misses  its  deadline  or  not  depends 
entirely  on  the  precise  deadline  assigned  to  a  message  size. As 
TCP 
DCTCP 
D3 
D2TCP 
D2 
DCTCP 
D3 
D2TCP 
OTCP 
o
52.3 
58.2 
5 
10 
15 
20 
25 
30 
35 
40 
Figure 9: Missed deadlines for low variance (10%) 
Fan-in degree 
TCP 
DCTCP 
D3 
D2TCP 
D2 
50.71  56.95 
5 
10 
15 
20 
25 
30 
35 
40 
Figure 10: Missed deadlines for med. variance (50%) 
Fan-in degree 
TCP 
DCTCP 
D3 
D2TCP 
D2 
)
%
(
s
e
n
i
l
d
a
e
d
d
e
s
s
M
i
45 
40 
35 
30 
25 
20 
15 
10 
5 
0 
)
%
(
s
e
n
i
l
d
a
e
d
d
e
s
s
M
i
45 
40 
35 
30 
25 
20 
15 
10 
5 
0 
)
%
(
s
e
n
i
l
d
a
e
d
d
e
s
s
M
i
40 
35 
30 
25 
20 
15 
10 
5 
0 
51.62  57.82 
30 
35 
40 
5 
10 
15 
20 
25 
Figure 11: Missed deadlines for high variance (Exp) 
Fan-in degree 
such,  the  choices  of  the  deadlines  directly  impact  the  results  of 
any  experiment.  Therefore,  we  carefully  calibrated  our  message 
sizes and deadlines such that the number of missed deadlines for 
DCTCP and D3 are in line with the results in the D3 paper [25]. 
This corroboration is discussed in detail in Section 4.2.2. We set 
the five OLDI applications’ message sizes to 2, 6, 10, 14, and 18 
KB  and  deadlines  to  20,  30,  35,  40  and  45  milliseconds, 
respectively.  We  used 
these  calibrated  message  sizes  and 
deadlines in all our experimental results, except in Section 4.2.6. 
Note  that  the  message  sizes  (fixed  size  of  few  KBs),  long  flow 
sizes  (a  few  MB),  and  number  of  concurrent 
long-lived 
connections (1 connection) are chosen to match the characteristics 
of production workloads [1]. In all our experiments, the network 
utilization is 10-20% which is a realistic load for datacenters, and 
not an artificial overload scenario. 
4.2.2  OLDI performance 