### Figure 7: Missed Deadlines (Left) and Bandwidth of Long Flows (Right) in Real Implementation vs. Simulation

In this experiment, we consider a scenario where each OLDI query is 500 KB with a deadline ranging from 5 to 25 milliseconds. Additionally, two servers initiate long-lived flows of 10 MB to the root every 80 milliseconds. The experiment runs for the duration of 1000 OLDI queries, resulting in a network utilization of approximately 10%, which is a realistic load for datacenters and not an artificial overload.

The primary goal of this experiment is to compare the performance of DCTCP and D2TCP in real implementation. 

**Missed Deadlines (Figure 7, Left):**
- We present the percentage of missed deadlines in both the real implementation and simulation for DCTCP and D2TCP (labeled as DCTCP-Real, D2-Real, DCTCP-Sim, and D2-Sim).
- The fan-in varies between 20 and 40.
- Across all scenarios, D2-Real consistently misses fewer deadlines than DCTCP-Real. This difference becomes more pronounced as the fan-in increases.
- For example, at a fan-in of 40, DCTCP-Real misses 15.2% of deadlines, while D2-Real misses only 12.3%.

**Throughput of Long Flows (Figure 7, Right):**
- We also examine the throughput achieved by non-deadline long flows for varying fan-in degrees.
- At a fan-in of 40, D2-Real achieves a throughput of 451 Mbps, compared to 438 Mbps for DCTCP-Real.
- This indicates that the improved performance of deadline flows does not come at the expense of degrading the throughput of non-deadline flows.

### Simulator Validation

To validate our simulator, we compare the results from the real implementation and the simulation, as shown in Figure 7. We observe that the absolute numbers from the simulation are slightly different from those in the real implementation. For instance, the percentage of missed deadlines for both DCTCP and D2TCP in the simulation is lower than in the real implementation. This discrepancy is expected, as simulations cannot fully capture all the nuances and details of a real system, such as burst-smoothing jitter, interrupt coalescing, Large Segment Offload (LSO), and other TCP quirks.

However, the relative performance difference between DCTCP and D2TCP, and the trend in that difference, are consistent across both the simulation and the real implementation. For example, at a fan-in of 30, D2TCP achieves a 16% reduction in missed deadlines over DCTCP in the simulation, while in the real implementation, the reduction is 15%. As the fan-in degree increases, the relative performance difference between DCTCP and D2TCP also increases in both the simulation and the real implementation. These key similarities lead us to believe that our at-scale simulation results are reliable. Additionally, we ensure that our simulation results closely match published DCTCP and D3 results [25], as detailed in the next section.

### At-Scale Simulations

We now present our at-scale simulations. We model the network topology and traffic after typical production deployments. Recall from Section 1 that D2TCP's goal is to reduce the percentage of missed deadlines without degrading the throughput for long-lived flows. Therefore, we focus on these metrics in our at-scale simulations.

#### Simulation Methodology

- **Implementation:** We implemented DCTCP and D2TCP on top of ns-3's TCP New Reno protocol [19] and enabled the marking of CE bits in the switch model of ns-3. For D3, we wrote both the end-host protocol and the switch logic based on the details in [25]. We set D3’s base rate to be one segment per RTT and used the same RRQ packet format described in [25], including the 8-bit bytes-per-microsecond field. All DCTCP and D2TCP parameters match those in Section 4.1.1.
- **Network Topology:** Our simulations use a fat-tree topology typical of datacenter networks, as depicted in Figure 8. The network consists of 25 racks, each with up to 40 end-host machines, simulating a 1000-machine deployment. Each end-host connects to the top-of-rack (ToR) switch via a 1 Gbps link. The ToR switches connect to a large fabric switch with large buffers, with each ToR connected via a single link with a line rate equal to 1 * number-of-hosts-in-a-rack Gbps. We sized the packet buffers in the ToR switches to match typical buffer sizes of shallow-buffered switches in real data centers (4MB) [1]. The link latencies are set to 20 µs, achieving an average RTT of 200 µs, representative of datacenter network RTTs.
- **Workload:** We ran five synthetic OLDI applications on the network, equally dividing the total number of end-hosts among the applications. The assignment of application nodes to physical end-hosts is random to capture the effects of dynamic virtual machine requests and relinquishments. Each application consists of five identical OLDI trees, each with one parent and n leaves, with the same settings for leaf-to-parent message size and deadlines. We varied n to explore different degrees of fan-in bursts.
- **Message Sizes and Deadlines:** The distributions of message sizes and deadlines in real OLDIs are publicly available [1]. However, exact details such as the specific deadline for a given leaf-to-parent ratio and message size are not publicly available. Therefore, we chose semi-synthetic values for deadlines using the aforementioned distributions. The choices of deadlines directly impact the results, so we carefully calibrated our message sizes and deadlines to align with the results in the D3 paper [25]. The five OLDI applications have message sizes of 2, 6, 10, 14, and 18 KB and deadlines of 20, 30, 35, 40, and 45 milliseconds, respectively. These calibrated message sizes and deadlines are used in all our experimental results, except in Section 4.2.6. The message sizes (fixed size of a few KBs), long flow sizes (a few MB), and the number of concurrent long-lived connections (one connection) are chosen to match the characteristics of production workloads [1]. In all experiments, the network utilization is 10-20%, which is a realistic load for datacenters.

### OLDI Performance

Figures 9, 10, and 11 show the percentage of missed deadlines for low variance (10%), medium variance (50%), and high variance (exponential) fan-in degrees, respectively. The results indicate that D2TCP consistently outperforms DCTCP and D3 in terms of reducing missed deadlines, with the performance gap increasing as the fan-in degree increases.