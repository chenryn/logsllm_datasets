==== StandardScaler 
标准化(StandardScaler)将数据变成平均值为0，方差为1的服从高斯分布的数据。
标准化是很多机器学习算法所需要的，因为他们很多是基于一个假设：数据是服从正太分布的。
在实际中，我们一般会忽略数据分布的形态，而只是会将数据通过减去平均值，在除以标准差来变化数据。
例如，RBF kernel的SVM算法，带有L1 or L2正则化的线性模型，SGDRegressor都需要数据 centered around zero 并有类似大小的方差。 如果有的数据的的某一个feature的方差特别大,那这个feature很可能会dominate the objective function。
要注意的是，我们在训练时候进行scale 得到的 mean，stdev，在应用时，也必须要使用相同的mean，stdev。
Syntax:
    fit StandardScaler [params set] from  [into model_name]  
Parameters:
*  : model中的x , 特征列表， 个数必须大于等于1
* [into model_name]: fit后模型保存名字， 可选， 可用于后续apply命令
* [params set]: 算法相关参数，可选
** partial_fit: bool, default False
** with_mean : boolean, True by default
+
If True, center the data before scaling. This does not work (and will raise an exception) when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory.
** with_std : boolean, True by default
+
If True, scale the data to unit variance (or equivalently, unit standard deviation).
Sample:
    tag:housing | fit StandardScaler from json.MEDV | table json.MEDV, StandardScaler_json.MEDV
尝试partial_fit，既增量更新，我们可以对已经保存的模型，再次feed data并进行更新
    tag:housing | limit 00 | fit StandardScaler  partial_fit=True from json.MEDV into ss_partial_fit| table json.MEDV, StandardScaler_json.MEDV
先对数据前100进行partial_fit，然后 `summarymodel` 查看参数:
image::images/ml-ss-example-1.png[]
在对前300数据partil_fit到保存的模型，然后再次 `summarymodel` 查看参数:
image::images/ml-ss-example-2.png[]
可以看到因为模型的增量跟新，使得mean和std都有了变化。
==== TFIDF
TFIDF 在搜索引擎排序中是很重要的一部分。详细介绍见: 。
这里讲下 TFIDF 在机器学习中应用， 很多的机器学习算法都需要 输入数字的特征，但是现实中，原始数据很多情况下都是一系列的文本符号，所以我们需要将这些文本进行变换，使其成为数字的特征，常见方法包括:
1. tokenizing: 分词， 并给每个分出来的token一个数字id
2. counting: 计数，记录每个词在每个文档中出现的个数
3. normailzing: 给每个词不同的权重，如一个词在多数的sample中都出现，权重将会降低
在上述定义中，每个词的出现的个数（after normalized or not) 将成为一个新的feature。一个sample所有词的个数，所组成的vector将被当成一个新的多变量的sample。
于是一个数据集，可以被表示成一个矩阵，矩阵的每行是一个文档sample，每列代表一个token。
一般而言，得到的矩阵都是十分稀疏的，既很多feature都是0（通常99%）。例如，一个文档集合有10000个文档，其中包含100000个unique word,正常情况下每个文档也就会使用100-1000 个unique word。但是这里的矩阵的维数可是10w。
TF = term frequency
image::images/ml-tf-model.png[]
IDF = inverse document frequency
image::images/ml-idf-model.png[]
另外在实现中，加入normalization,既每一行的vector will be normalized by euclidean norm。
Syntax：
    fit TFIDF [params set] from  [into model_name]
Params：
*  : model中的x , 特征列表， 个数必须大于等于1
* [into model_name]: fit后模型保存名字， 可选， 可用于后续apply命令
* [Params set]
** analyzer : string, {‘word’, ‘char’} or callable
+
Whether the feature should be made of word or character n-grams.
+
If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input.
** ngram_range : tuple (min_n, max_n)
+
The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used.
** stop_words : string {‘english’}, list, or None (default)
+
If a string, it is passed to _check_stop_list and the appropriate stop list is returned. ‘english’ is currently the only supported string value.
+
If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if analyzer == 'word'.
+
If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms.
** lowercase : boolean, default True
+
Convert all characters to lowercase before tokenizing.
** token_pattern : string
+
Regular expression denoting what constitutes a “token”, only used if analyzer == 'word'. The default regexp selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).
** max_df : float in range [0.0, 1.0] or int, default=1.0
+
When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.
** min_df : float in range [0.0, 1.0] or int, default=1
+
When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.
** max_features : int or None, default=None
+
If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.
+
This parameter is ignored if vocabulary is not None.
** binary : boolean, default=False
+
If True, all non-zero term counts are set to 1. This does not mean outputs will have only 0/1 values, only that the tf term in tf-idf is binary. (Set idf and normalization to False to get 0/1 outputs.)
** norm : ‘l1’, ‘l2’ or None, optional
+
Norm used to normalize term vectors. None for no normalization.
** use_idf : boolean, default=True
+
Enable inverse-document-frequency reweighting.
** smooth_idf : boolean, default=True
+
Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions.
** sublinear_tf : boolean, default=False
+
Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).
Sample:
    java.file:* | fit TFIDF from java.file 
image::images/ml-tfidf-example.png[]
结果图中看到，对于每个unique token,都会成为结果中的新的一列，值为0表示该sample没有该token。