erated using the labeled dataset. As it was explained in Section 2,
for each cluster of similar messages, POISED computes the prob-
abilities of messages in that group being posted in each of the
communities of interest. As an example, let us assume that in a
neighborhood with three communities, the topic detection algo-
rithm has found eight topics of interest and four-gram analysis
has identified ten clusters of similar messages. The probabilistic
table will then include ten rows and eight columns corresponding
to groups and topics, respectively. The table entry for row i and
column j is the probability that the messages in group i is being
observed in communities with an interest in topic j.
Ideally, the probabilistic model should be built on the whole data
of a social network. However, we cannot perform topic modeling
on all data because of the resource constraints imposed by the use
of MALLET. Instead, the topic detection is run and a probabilistic
table is generated for each neighborhood separately. Each neigh-
borhood includes multiple communities. We show that even while
performing the analysis locally on each neighborhood, POISED
detects spam messages effectively. This technique of running the
analysis on neighborhoods can also help scaling our approach, so
that it can be applied on much larger datasets. For example, Twit-
ter can divide its large dataset into a few partitions that are then
independently analyzed.
A probabilistic table for a neighborhood includes the clusters
of similar messages that are posted in that neighborhood. Also, if
messages of one large group are observed in multiple neighbor-
hoods, then the probabilities for this group is computed separately
and listed in the probabilistic table of every neighborhood. As a
result, the size of probabilistic tables varies for each neighborhood.
Some include thousands of clusters of similar messages while oth-
ers include only a few of them. Because of the lack of enough
observations in some neighborhoods, we did not run POISED on
neighborhoods with less than 10 benign or 10 spam clusters of
similar messages.
Therefore, our ‘ground-truth’ dataset for testing the second hypoth-
esis includes the data for 202 neighborhoods with 2,896 clusters of
similar messages and close to 1.3M tweets generated by more than
64K users.
Table 2 summarizes some statistics on this dataset. It shows that
the number of spam groups and tweets in the ground-truth dataset
are about 12% and 10% less than those in the ‘labeled’ dataset. In
contrast, the number of app groups and tweets in the ‘ground-truth’
dataset are about 2% and 8% more than those in the ‘labeled’ dataset.
Table 2: Statistics for the manually labeled and ground-truth datasets
Spam
App
Quote
Normal
Unknown
Labeled Dataset (300 neighborhoods)
2,110 (42.2%)
344,540 (27%)
13,179
376 (7.5%)
416,099 (32.5%)
335 (6.7%)
34,138 (2.7%)
2,178 (43.6%)
482,975 (37.8%)
9,740
3,819
55,473
1 (0%)
81 (0%)
1
Ground-truth Dataset (202 neighborhoods)
854 (30%)
168,181 (17%)
12,504
274 (9%)
408,395 (40%)
9,614
260 (9%)
32,607 (3%)
3,815
1508 (52%)
408,340 (40%)
55,219
No. of groups
No. of tweets
No. of users
No. of groups
No. of tweets
No. of users
6.4 Classification on Parties of Interest
We employed machine learning to detect spam messages. The fea-
tures are the list of topics in a neighborhood that are automatically
detected by the topic modeling algorithm. An observation indicates
the parties of interest represented by a row in the probabilistic table
that includes the probabilities that a group of similar messages has
been observed in communities interested in each of the topics. In
addition to these features, we also consider the number of users
per total number of messages in a group as a feature. This feature
aims at capturing if a message is posted by several users or only by
a small number of them.
The class of a group is the label in the ground-truth dataset.
We define a class as a binary variable: ‘benign’ or ‘spam’. While
five categories were defined for manual labeling, for evaluation,
we examined different combinations of these categories: Comb.
1: Spam = {spam}, and Beniдn = {normal, quote, app}, Comb. 2:
Spam = {spam}, and Beniдn = {normal} and Comb. 3: Spam =
{spam, app}, and Beniдn = {normal, quote}. Later, we show that
regardless of the combination, ‘spam’ messages are detected with
high accuracy. The distinction between spam and benign messages
can be specified by a policy and fed to POISED as a parameter.
The datasets are not balanced, i.e., the number of spam and be-
nign messages is not equal. We overcome this limitation by using
a well-known over-sampling technique called SMOTE in which
the minority class is over-sampled by creating “synthetic” exam-
ples [15].
To assess the effectiveness of our spam detection algorithm,
we use the standard information retrieval metrics including recall,
precision, F1-score, and accuracy.
After creating all the probabilistic models over the ground-truth
dataset, we applied k-fold cross validation on each of the 202 neigh-
borhoods separately. Then, we averaged their measures. We tested
with three values of k = {3, 5, 10} and found very similar results.
Since k = 10 is one of the most common practices [10, 41, 66], we
report the results for that value. We also tested several classification
algorithms including Naive Bayes, SVM and Random Forest and all
provide similar results.
Table 3 shows the results provided by SVM on different observa-
tion combinations. Since most groups of messages are labeled as
‘spam’ or ‘normal’, these combinations do not highly impact the
results of the classifier. The results suggest that with high preci-
sion (91%) and recall (93%) our classifier successfully detects spam
messages.
Figure 9: Percentage of spam posted by individual users.
6.5 Spam Accounts in Labeled Dataset
While some users only posted either benign or spam messages, some
users have posted messages from both classes. Figure 9 indicates
the histogram of spam message percentages for all users in the
labeled dataset. It illustrates two main clusters of users who either
post benign or spam content. This confirms that by examining
the distribution of an account’s messages, it is possible to label an
account as benign or spam.
To identify spam accounts, we compute s, the percentage of spam
messages to the total number of messages that a user has posted. A
user is identified as a spam account if s ≥ τ. τ is a parameter that
can be configured based on the dataset and the desired or accepted
false positive values for the system. We set τ = 0.4 and assume that
if more than 40 percentage of the messages posted by an account
are spam messages, then this account is more likely a spam account.
Finally, our labeled dataset includes 15,055 (23%) spam accounts
and 49,675 (77%) honest users. The high percentage of spam ac-
counts may be due to our selection of larger groups of similar
messages as a labeled dataset, which may relatively include more
spam messages compared to the whole Twitter data.
6.6 Comparison with state of the art systems
We compare POISED to three state-of-the-art systems that have
been proposed by the research community in the past: SpamDe-
tector [75], which is a system that detects fake accounts on so-
cial networks by examining characteristics of these profiles (e.g.,
the fraction of messages posted that contain a URL), COMPA [27],
which detects social network accounts that have been compromised
by learning the typical behavior of an account and flagging any
0.00.20.40.60.81.0010000200003000040000Number of usersPercentage of users' spam messagesTable 3: The performance of POISED as well as state of the art systems. The +/- values indicate standard error.
System
POISED
POISED
POISED
SpamDetector [75]
COMPA [27]
BotOrNot [25]
Label Combination
Comb. 1
Comb. 2
Comb. 3
Comb. 3
Comb. 3
Comb. 3, thr=0.8
Accuracy
0.89 (+/- 0.02)
0.90 (+/- 0.02)
0.90 (+/- 0.02)
F1-score
0.90 (+/- 0.02)
0.91 (+/- 0.02)
0.90 (+/- 0.02)
Precision
0.85 (+/- 0.02)
0.87 (+/- 0.02)
0.91 (+/- 0.02)
Recall
0.95 (+/- 0.02)
0.95 (+/- 0.02)
0.93 (+/- 0.02)
0.67
NA
0.76
0.20
0.55
0.07
0.21
1.0
0.36
0.20
0.38
0.04
activity that deviates from that behavior as a possible compromise,
and BotOrNot [25, 29], which leverages more than one thousand
features to evaluate if a Twitter account exhibits similarity to the
known characteristics of social bots. We could not compare POISED
with a couple of more recent work due to either the difficulty in
obtaining their systems, or not being applicable on Twitter data.
We discuss them in more details in Section 7.
Note that the threat model tackled by our approach is much
broader than the one that these systems focused on: we aim to
detect any malicious message regardless whether it was posted by a
fake account or by a compromised one, while previous approaches
only focused on one of these two categories. Because of this rea-
son, our results in Table 3 show that our system outperforms both
SpamDetector and COMPA, as well as BotOrNot.
SpamDetector and COMPA were developed as part of previous
work by some of the authors of this paper, therefore we had access
to their source code. In the case of SpamDetector, we performed
a 10-fold cross validation on our labeled dataset using Random
Forest as a classification algorithm. For COMPA, performing a 10-
fold cross validation would not make sense, since this system does
not take into account two classes of spam or benign accounts, but
rather learns the typical behavior of an account and determines
whether new messages that an account sends are malicious or not.
Therefore, in this experiment we used COMPA to learn the typical
behavior of the accounts that sent spam in our ground-truth dataset,
and determine whether the spam messages that they sent were the
consequence of a compromise.
Table 3 illustrates that our system outperforms both COMPA
and SpamDetector considerably. The perfect precision reported
by COMPA is an artifact of the fact that we only tested that system
on malicious accounts (which is also the reason why we could not
calculate accuracy), but the low recall of 0.38 (possibly due to the
fact that only a minority of the accounts in our labeled dataset
were compromised and not just fake) shows that POISED is a better
candidate to fight the problem at hand.
We called the BotOrNot API for all the users in our labeled
dataset. The classification system of BotOrNot is based on more
than 1,000 features extracted from interaction patterns and content.
When testing on a user, it returns the probability of that user being a
social bot (Sybil account). To label users as bots, we picked multiple
values, {0.7, 0.8, 0.9}, as a threshold. Here, we reported the values
of measures for 0.8. We ran BotOrNot in February of 2016. Since
both tools were run relatively close to each other, 60550 out of
63600 accounts still existed and were accessible for BotOrNot. In
all cases, POISED outperforms BotOrNot. Overall, the precision
of BotOrNot is around 40% while the recall is as low as 0.03.
6.7 Missed Spam Messages and Accounts
All of the above systems are relying on classifiers that find the
abnormalities based on some features that can be adopted over
time. The systems that we compared against POISED are either
based on specific characteristics of the accounts under scrutiny
(SpamDetector, BotOrNot) or look for changes in the behavior
of an account that might be indicative of a compromise (COMPA).
Given these peculiarities, these systems can only detect certain
types of spam. In contrast, the feature set in POISED is adopted
over time by detecting parties of interest and this makes its de-
tection more comprehensive. In addition, POISED uses the main
characteristic of spam messages, i.e., their need to propagate in
large-scale campaigns, which can not easily be mitigated by the
attackers, who, by doing so, would directly affect the efficiency of
their campaigns.
We further manually examined a sample of ‘spam’ accounts that
are not detected by other approaches, to understand how POISED
allows to improve the detection of spam over previous work.
Some of the users not detected by SpamDetector were users
aggressively advertising products, while others were bots only
tweeting about a specific hashtag. Also these accounts have multiple
bots in their friends. We believe that these bots were able to evade
the fairly simple threat model of SpamDetector, but were caught
by the statistical models of POISED.
Some of the missed spam accounts by COMPA were users who
retweeted quotes to appear legitimate, but also posted spam from
their blog. Again we found multiple bots in their friends. We be-
lieve that these accounts were not compromised, but rather bot
accounts, and were therefore outside the threat model used by
COMPA. Both COMPA and SpamDetector did not identify users
posting automated content from applications.
BotOrNot, on the other hand, falsely detected some benign
users as bots, possibly because of their high follower-to-friend ratio
and low count of tweets. It also missed some accounts that were
detected by POISED, that have since been suspended by Twitter,
possibly for spamming. These examples show that POISED is able
to identify a broader category of spammers than previous systems,
and is therefore more effective in fighting this problem.
6.8 Early Spam Detection
POISED is more effective if it can identify spam messages early
on, before they are fully distributed throughout the network. We
investigated the impact of “early detection” on the performance of
POISED. We implemented a simulation, where at the probabilistic
table creation phase, for each spam observation, the propagation