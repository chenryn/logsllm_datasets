title:Modeling Coordinated Checkpointing for Large-Scale Supercomputers
author:Long Wang and
Karthik Pattabiraman and
Zbigniew Kalbarczyk and
Ravishankar K. Iyer and
Lawrence G. Votta and
Christopher A. Vick and
Alan Wood
Modeling Coordinated Checkpointing for Large-Scale Supercomputers 
Long Wang, Karthik Pattabiraman,   
Lawrence Votta, Christopher Vick, Alan Wood 
Zbigniew Kalbarczyk, Ravishankar K. Iyer 
Center for Reliable and High-Performance Computing 
University of Illinois at Urbana-Champaign 
1308 W. Main Street, Urbana, IL 61801, USA 
{longwang, pattabir, kalbar, iyer}@crhc.uiuc.edu
Abstract.  Current  supercomputing  systems  consisting  of 
thousands  of  nodes  cannot  meet  the  demands  of  emerging 
high-performance  scientific  applications.  As  a  result,  a  new 
generation of supercomputing systems consisting of hundreds 
of thousands of nodes is being proposed. However, these sys-
tems are likely to experience far more frequent failures than 
today's systems, and such failures must be tackled effectively. 
Coordinated  checkpointing  is  a  common  technique  to  deal 
with failures in supercomputers. This paper presents a model 
of  a  coordinated  checkpointing  protocol  for  large-scale  su-
percomputers, and studies its scalability by considering both 
the  coordination  overhead  and  the  effect  of  failures.  Unlike 
most  of  the  existing  checkpointing  models,  the  proposed 
model  takes  into  account  failures  during  checkpointing  and 
recovery,  as  well  as  correlated  failures.  Stochastic  Activity 
Networks (SANs) are used to model the system, and the model 
is simulated to study the scalability, reliability, and perform-
ance of the system.
1. Introduction 
The  computational  demands  of  emerging  applications, 
such as protein folding, is giving rise to a new generation of 
supercomputers (currently in the planning stage) consisting of 
several  thousand  processors.  For  example,  the  newly  de-
ployed  IBM  BlueGene/L  [1]  is  expected  to  scale  to  64K 
dual-processor  nodes.  Despite  the  huge  computing  power 
these systems provide, the large number of nodes makes them 
significantly  more  vulnerable  to  errors.  The  resulting  larger 
number  of  failures  due  to  errors  can  impair  system  perform-
ance and limit scalability.   
Although a hierarchy of error detection and recovery tech-
niques, such as ECC, CRC, and message retransmission, can 
correct  some  errors/failures,  some  transient  errors/failures 
cannot  be  covered  using  these  techniques,  e.g.  corrupted 
states  due  to  propagation  of  undetected  errors.  For  these  er-
rors/failures,  checkpointing  and  rollback  may  be  to  recover 
the application before rebooting or reconfiguring the system. 
This paper focuses on errors/failures that need checkpointing 
and rollback to recover.   
The  most  commonly  used  checkpointing  scheme  for  su-
percomputing  systems  is  coordinated  checkpointing,  due  to 
its  simplicity  of  implementation.  In  this  approach,  coopera-
tive processors synchronize to ensure a global consistent state 
before taking a checkpoint [3].    The main problem with co-
ordinated checkpointing is its lack of scalability, as it requires 
all processors to take a checkpoint simultaneously.   
This paper makes two main contributions. First, it builds a 
model  of  a  large-scale  system  that  uses  coordinated  check-
pointing  for  recovery  from  failures  with  complex  semantics. 
Sun Microsystems 
Menlo Park, CA 94025, USA 
{lawrence.votta, christopher.vick, alan.wood}@sun.com
Second, it studies the scalability and performance of the sys-
tem  for  several  hundred  thousand  processors  by  simulating 
the model with realistic parameter values. 
An important issue considered in our model is the effect of 
scaling  from  several  thousand  processors  to  several  hundred 
thousand processors, i.e., by two orders of magnitude. Issues 
such  as  failures  during  checkpointing  and  recovery,  corre-
lated failures  within the system, and checkpointing overhead 
due  to  coordination  are  of  primary  importance  for  the  new 
generation  of  supercomputers.  This  is  because  their  larger 
number of nodes and higher failure rates invalidate some as-
sumptions  that  existing  models  make  about  system  behavior 
[7,  8,  9,  10,  11,  12]  and  exacerbate  some  effects  previously 
considered negligible. These assumptions are:   
(cid:22109)  The  computation  interval  and  the  checkpoint  overhead 
are  much  smaller  than  the  mean  time  between  failures 
(MTBF).  However,  large-scale  supercomputers  experience 
much smaller MTBFs and much larger checkpoint overheads, 
and  hence  failures  during  checkpointing  and  recovery  can 
occur and must be taken into account [5].   
(cid:22109)  Failures  are  independent  of  each  other.  This  is  not  a 
valid  assumption,  as  Tang  and  Iyer  [6]  showed  that  even  a 
small  number  of  correlated  failures  increase  system  unavail-
ability considerably. 
(cid:22109)  The  overhead  of 
inter-processor  coordination  for 
checkpointing is negligible. However, as the number of nodes 
increases, the coordination overhead grows, and it cannot be 
ignored. 
A  measure  called  useful  work  similar  to  accumulated  re-
ward  [17]  is  used  to  evaluate  system  performance.  Useful 
work  is  defined  as  the  computation  that  contributes  to  the 
ultimate completion of the job (see definition in Section 7). If 
a failure occurs before the computation can be checkpointed, 
the  computation  since  the  last  checkpoint  needs  to  be  re-
peated  after  the  recovery  and  is  not  counted  as  useful  work. 
Accurate  modeling  of  useful  work  requires  knowledge  on 
future behavior of the system and cannot be represented using 
simple  Markov  models.  Instead,  Stochastic  Activity  Net-
works  (SANs)  are  used  to  model  the  system  behavior.  The 
modeling  power  of  SANs  allows  us  to  concisely  represent 
complex system phenomena such as checkpoint coordination, 
failures  during  checkpointing  and  recovery,  and  correlated 
failures. The SAN model is studied using simulation, and the 
impact  of  system  parameters  on  system  performance  and 
scalability is evaluated. 
2. Related Work 
Checkpointing  models.  One  of  the  earliest  models  for 
computing  the  optimal  checkpointing  interval  is  by  Young 
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
[7]. This model assumes that the MTBF of the system is very 
large  compared  to  the  checkpoint  and  recovery  time,  and 
hence  it  does  not  consider  failures  during  checkpointing  and 
recovery. Daly [8] presents a modification of Young’s model 
for  large-scale  systems.  This  model  takes  into  account  fail-
ures  during  checkpointing  and  recovery  as  well  as  multiple 
failures in a single computation interval. However, it does not 
model the coordination overhead of the checkpointing proto-
col itself or consider correlated failures.   
Kavanagh  and  Sanders  [9]  evaluate  two  time-based  coor-
dinated  checkpointing  protocols  based  on  analytical  and 
simulation  models,  which  take  the  overhead  of  coordination 
into  account.  However  they  do  not  consider  failures  during 
checkpointing  and  recovery,  as  they  assume  that  the  MTBF 
of the system is much greater than the checkpoint interval.   
Plank  and  Thomason  [10]  investigate  the  use  of  spare 
nodes to provide redundancy  in the system to handle perma-
nent  failures.  We  do  not  consider  permanent  failures  in  our 
model and assume that all nodes can be recovered by restart-
ing  the  system  from  the  last-saved  checkpoint.  Plank  and 
Thomason  do  not  consider  the  overhead  of  coordination  in 
their model or the effect of scaling the model to a large num-
ber of nodes. A recent paper by Elnozahy et al. [11] extends 
the  work  of  Plank  and  Thomason  to  systems  consisting  of 
thousands of nodes. It considers the effects of failures during 
checkpoint  and  recovery  and  multiple  failures  in  a  single 
computation  interval.  However,  it  does  not  consider  the  ef-
fects  of  coordination  among  the  nodes  in  the  checkpointing 
protocol, nor does it consider correlated failures.   
Vaidya  [12]  derives  an  analytical  expression  for  the  opti-
mal  checkpointing  frequency  in  a  uniprocessor  system.  It 
distinguishes  the  checkpoint  latency  from  the  overhead  of  a 
checkpointing  scheme.  This  model  considers  failures  during 
checkpointing/recovery  but  does  not  take  into  account  the 
scalability of the checkpointing protocol or the system. 
Large-scale  systems.  Bronevetsky  et  al.  [23]  present  a 
compiler-based  technique  for  asynchronous,  coordinated 
checkpointing.  Agarwal  et  al.  [24]  consider  an  adaptive,  in-
cremental checkpointing technique  for scientific applications 
on large-scale systems. Finally, Zhang et al. [18] do an exten-
sive  study  of  failure  data  analysis  in  large-scale  supercom-
puting  systems  and  show  the  existence  of  temporal  and  spa-
tial  correlation  among  failures  in  large-scale  systems.  We 
consider  temporal  correlations  in  our  model  (correlated  fail-
ures), but not spatial correlations. 
3. Target System 
This  study  focuses  on  a  typical  abstract  structure  com-
monly  shared  by  many  supercomputers  and  a  basic  coordi-
nated  checkpointing  protocol  whose  variants  are  applied  in 
the supercomputing world.   
3.1 Architecture   
Each node of the supercomputing system is a tightly inte-
grated  unit  consisting  of  multiple  processors.  For  example, 
Blue-Gene/L  has  2  processors  per  node,  and  ASCI  Q  has  4 
processors  per  node.  Future  systems  could  have  8,  16,  or  32 
processors per node. 
Usually,  large-scale  supercomputing  systems  have  dedi-
cated nodes for job computation (compute nodes) and for I/O 
operations (I/O nodes). The compute nodes in a set share the 
connections  to  an  I/O  node,  and  all  the  I/O  nodes  are  con-
nected to a parallel file system through a separate connection 
network.  For  example,  IBM  BG/L  has  64K  compute  nodes 
and  1024  I/O  nodes.  The  network  bandwidth  from  64  com-
pute  nodes  to  one  I/O  node  is  350MB/s,  and  the  bandwidth 
from one I/O node to the file system is 1 Gb/s. 
Data  writes  from  compute  nodes  to  the  file  system  are 
performed in two steps: from compute nodes to I/O nodes and 
then from I/O nodes to the file system. The I/O nodes locally 
buffer  the  application  data  or  checkpoint  they  receive  from 
the compute nodes and then write it to the file system  in the 
background while the compute nodes continue with the com-
putation.  The  two  steps  are  reversed  for  data  reads  with  the 
exception that reads cannot be done in the background, as the 
application  may  have  to  wait  for  the  data  to  be  read  before 
proceeding, depending on the nature of the read.1
3.2 Checkpoint Protocol 
There  are  two  checkpointing  approaches  used  in  super-
computing systems. One is application-based, where a global 
barrier is explicitly used in the application for saving a global 
consistent  state.  This  places  the  burden  of  checkpointing  on 
the application (e.g., in BlueGene/L [1]). The other approach 
is  system-supported  checkpointing  (e.g.,  the  algorithm  used 
by Cray in the IRIX OS [19]). Our checkpointing protocol is 
a  system-supported  synchronous  checkpointing  and  follows 
the  basic  principles  of  coordinated  checkpointing,  e.g.,  Koo 
and Toueg’s protocol [4]. 
In  our  protocol,  a  single  coordinator  node,  or  master,  pe-
riodically initiates the checkpointing as follows: 
(1) The master broadcasts a ‘quiesce’ request to all the compute nodes. 
(2) On  receiving  ‘quiesce’  each  node  quiesces  its  operations,  i.e.,  stops 
all  its  activities  at  a  consistent  and  interruptible  state  and  replies 
‘ready’ to the master. 
(3) After  receiving  ‘ready’  from  all  the  compute  nodes,  the  master 
broadcasts ‘checkpoint’ to all the compute nodes. 
(4) On  receiving  ‘checkpoint’  each  compute  node  dumps  its  state  to  an 
I/O node, and then sends a ‘done’ message to the master. 
(5) When  the  master  collects  the  ‘done’  messages  from  all  the  compute 
nodes, it broadcasts ‘proceed’ to all the compute nodes, and the I/O 
nodes  begin  to  write  the  checkpoint  to  the  file  system  in  the  back-
ground. 
(6) On receiving ‘proceed’ each compute node continues its activity from 
the point at which it quiesced.
When  a  node  is  quiesced,  it  means  that  it  stops  all  the 
task-related  activities  in  a  consistent  and  interruptible  state. 
Further,  a  timeout  period  is  specified  at  the  master  to  avoid 
waiting indefinitely for the ‘ready’ responses. This indefinite 
wait can occur as a result of an erroneous or failed node that 
does  not  respond  to  the  quiesce  request.  If  all  the  responses 
are  not  received  within  this  time,  the  master  times  out  and 
broadcasts  an  ‘abort’  message  to  all  the  compute  nodes, 
causing them to abandon the checkpointing and proceed with 
their computations. 
Note  that  the  current  checkpoint  does  not  overwrite  the 
previous  checkpoint,  unless  the  checkpointing  successfully 
1  While  current  supercomputing  systems  may  not  have  this 
capability, future systems might allow this two-step I/O.
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
completes  and  the  checkpoint  is  verified  to  be  correct.  So 
whenever  the  checkpointing  is  abandoned  the  previous 
checkpoint  is  still  valid.  Hence,  the  system  can  always  re-
cover to the last  good checkpoint upon a compute node fail-
ure. 
3.3 Application 
The  application  is  a  parallel,  scientific  computing  work-
load composed of multiple computation tasks. Each compute 
processor runs exactly one task of the parallel application and 
no other tasks.   
Application  tasks  may  be  performing  computation,  com-
munication, or I/O at any time. Since most parallel, scientific 
applications  are  written  using  the  BSP  (Bulk  Synchronous 
Parallel)  model  [13],  the  multiple  tasks  more  or  less  coordi-
nate their actions and behave as one cohesive unit.   
The  application  is  instrumented  with  a  number  of  check-
point primitives at its safe points (e.g. a global barrier), where 
it can safely quiesce, as at the end of a loop. For example, in 
IRIX,  the  programmer  inserts  checkpoint  functions  in  the 
source code, and the OS calls these whenever it wants to take 
a checkpoint. 
A  task  that  is  doing  an  I/O  write,  cannot  quiesce  until  it 
finishes  the  I/O  operation,  as  this  could  leave  the  I/O  in  an 
inconsistent state and possibly corrupt the file system. While 
there  are  methods  to  address  this,  ensuring  global  coordina-
tion 
simple  approach  of 
non-preemptive  I/O  is  preferred  in  practice.  I/O  reads  of  a 
task can be stopped for checkpointing at any time, and hence, 
they are not specifically considered in our model. 
is  complicated,  and 
the 
3.4 Failure and Recovery 
On  the  failure  of  a  compute  node,  the  entire  application 
rolls back to the  last  saved checkpoint and recovers, i.e.,  we 
only  consider  failures  that  require  recovery  from  a  check-
point. While permanent/persistent errors are not considered in 
the  paper,  checkpointing  can  still  be  used  to  recover  from 
permanent  hardware  failures.  This,  however,  would  require 
system  reconfiguration  and  remapping  of  the  checkpointed 
states into a new set of nodes (assuming that spare nodes are 
available). 
Failures  of  compute  nodes  and  I/O  nodes  are  always  de-
tected without any latency. The mechanism for failure detec-
tion is not modeled. 
When  an  I/O  node  fails,  all  the  I/O  nodes  need  to  be  re-
started.  This  assumption  is  reasonable,  since  in  the  BSP 
model, the application needs the I/O operations on all the I/O 
nodes to be completed before continuing the computation. 
When  the  master  node  fails  when  checkpointing  is  not  in 
progress, we assume that the error is detected and the master 
recovers independently of the other nodes. If the master fails 
during  checkpointing,  the  checkpointing  protocol  is  aborted 
and the master goes back to the initial state. 
As nodes have multiple processors, the node failure rate is 
the  product  of  the  processor  failure  rate  and  the  number  of 
processors per node. The system parameter MTTF is used to 
refer  to  the  per-node  mean  time  to  failure  throughout  this 
paper  unless  specified  otherwise.  Then  per-processor  MTTF 
is  MTTF  times  the  number  of  processors  per  node.  It  is  as-
sumed  that  advanced  design  and  error  handling  techniques 
are  applied  to  maintain  low  node  failure  rates,  e.g.,  use  of 
multiple cores on a chip. 
As  there  is  no  consensus  on  MTTF  in  the  literature,  we 
assume an  MTTF value from 1  year to 25  years due to both 
hardware  and  software  errors  based  on  the  following:  (i) 
ASCI-Q has a per-node MTTF of 1 year [11], (ii) IBM 380 X 
processor has an MTTF of 8 years [16], (iii) IBM mainframes 
have  an  MTTF  of  25  years,  and  (iv)  IBM  G5  processor  is 
advertised with an MTTF of 45 years [22] (hardware failures 
only).   
3.5 Correlated Failure 
This paper models two categories of correlated failures: (i) 
correlated  failures  due  to  error  propagation  only  and  (ii)  ge-
neric correlated failures.   
For  correlated  failures  due  to  error  propagation,  we  as-
sume that recovery fully restores the application/system state 
and that propagated errors do not cross recovery boundaries. 
The error propagation is characterized by a short error burst, 
which  typically  impacts  the  recovery.  The  duration  of  the 
error  burst  is  referred  to  as  the  correlated  failure  window.
The  system  may  need  to  recover  several  times  before  a  suc-
cessful recovery [20]. A typical value of the correlated failure 
rate is 600 times the normal failure rate [6] (see Section 6).   
Correlated  failures  may  be  caused  by  factors  other  than 
error  propagation,  e.g.,  common  causes  such  as  increases  in 
node  temperature  or  some  environmental  phenomena.  Usu-
ally,  a  hyper-exponential  distribution  is  assumed  for  model-
ing generic correlated failures, i.e., the system experiences an 
independent  failure  rate  and  a  correlated  failure  rate  alterna-
tively.  Unlike  correlated  failures  due  to  propagation,  the  se-
mantics  of  generic  correlated  failures  is  not  necessarily  lim-
ited to a short duration, but rather forms a global view of the 
system for the entire system life. 
4. Overall Composition of the Model 
The  system  is  decomposed  into  several  subsystems.  Each 
subsystem  is  modeled  as  a  separate Stochastic  Activity  Net-
work (SAN) submodel, and the overall model is obtained by 
integrating these submodels. All the compute nodes are mod-
eled as a single unit and all the I/O nodes are modeled as an-
other  unit.  This  allows  the  model  to  scale  to  a  large  number 
of  nodes  without  requiring  a  large  simulation  time.  Table  1 
lists  the  SAN  submodels  of  the  entire  system,  and  Figure  1 
illustrates  how  these  submodels  (the  ovals  in  Figure  1)  are 
integrated  into  an  overall  model.  The  arrows  in  the  figure 
illustrate  the  logical  interactions  between  the  submodels. 
These interactions are implemented by state sharing between 
the submodels. The dots in the submodels in Figure 1 indicate 
the initial position of the tokens in the corresponding SAN. It 
should be emphasized that Figure 1 is not a  state diagram,in 