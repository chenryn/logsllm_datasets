accuracy metric, which is deﬁned as the ratio of the total
number of tested images minus the number of failed tests to the
total number of tested images (both original and adversarial).
test accuracy ≡ total # of test examples − # of failed tests
total # of test examples
A test
is considered failed when: (1) original example is
missclassifed, (2) original example is rejected, or (3) adver-
sarial example is accepted with incorrect classiﬁcation. To
be more precise during evaluation, we separately compute
the test accuracy on original and adversarial examples. When
a defensive method tries to maximize classiﬁer’s capability
to identify adversarial examples, the classiﬁer may reject or
missclassify more original examples than the corresponding
Vanilla classiﬁer. The trade-off between correctly classifying
original and adversarial examples is the same as the trade-off
between true positive rate and true negative rate in machine
learning.
The other important metric to evaluate defense approaches
is the training time it takes to build the model. As mentioned
earlier, a signiﬁcant amount of computation is consumed to
generate the adversarial examples for full knowledge adversar-
ial training. The two main contributing factors to the training
time are: (1) the structure of the classiﬁer (number of layers
7
and parameters) and (2) the searching algorithm of adversarial
examples (e.g., single-step vs. iterative approaches). The goal
is to minimize the training time while maintaining acceptable
test accuracy.
V. EXPERIMENTAL RESULTS
In this section, we present comparative evaluation results of
the ZK-GanDef with other state-of-the-art zero knowledge as
well as full knowledge adversarial training defenses introduced
previously. The evaluation results are summarized in three
subsections. In the ﬁrst subsection, we provide comparative
evaluation of ZK-GanDef with other zero knowledge and
full knowledge adversarial training defenses on classifying
original and different types of adversarial examples. Then, we
compare the computational consumption of ZK-GanDef with
other full knowledge adversarial training defenses in terms of
training time per epoch. In the third subsection, we analyze
the convergence issues of CLP and CLS on CIFAR10 dataset.
A. Test Accuracy on Different Examples
In this subsection, we show the test accuracy of the Vanilla
classiﬁer and the classiﬁers with defenses against different
types of examples. As mentioned earlier, the experiments are
conducted on MNIST, Fashion-MNIST and CIFAR10 datasets.
For each dataset, a total of 28 different results are calculated.
These results span all possible pairs of 7 different classiﬁers
(Vanilla, CLP, CLS, ZK-GanDef, FGSM-Adv, PGD-Adv and
PGD-GanDef) and 4 different kinds of examples (original,
FGSM, BIM and PGD). All the validation results are presented
in Figure 4 and detailed in Table III.
1) On Original Examples: In Figure 4, we ﬁrst focus on the
results presented in the ﬁrst column sub-ﬁgures. These results
represent the test accuracy on original examples from different
datasets. As a baseline, the Vanilla classiﬁer achieves 98.92%
test accuracy on MNIST, 92.43% test accuracy on Fashion-
MNIST and 89.92% test accuracy on CIFAR10. These results
are consistent with the benchmark ones presented in [3].
We then evaluate the test accuracy of the three zero
knowledge defenses (CLP, CLS and ZK-GanDef) on different
datasets. On MNIST dataset, their test accuracy on original
examples is at the same level as that of the Vanilla classiﬁer.
The detailed results from Table III show that the difference
in test accuracy among the defenses is within 0.5%, which
is small enough to be ignored. On Fashion-MNIST dataset,
the test accuracy of CLP and CLS is 5% higher than that of
ZK-GanDef on original examples. Moreover, the test accuracy
of all zero knowledge approaches is (6% to 11%) lower than
that of the Vanilla classiﬁer. This small degeneration is a result
of tuning the model to enhance test accuracy on adversarial
examples. On CIFAR10 dataset, CLP and CLS have a signiﬁ-
cantly lower test accuracy compared with the Vanilla classiﬁer
and ZK-GanDef. This is because the classiﬁers with the CLP
and CLS methods do not converge at the beginning of the
training. A detailed study of this phenomenon is provided in
the following subsection.
Finally, we conduct the same evaluation with full knowl-
edge adversarial training defenses and perform comparison
with the proposed ZK-GanDef. On MNIST dataset, all full
knowledge defenses and ZK-GanDef achieve the same level
of test accuracy as that of the Vanilla classiﬁer. On Fashion-
MNIST dataset, FGSM-Adv achieves similar test accuracy
on original examples to that of the Vanilla classiﬁer, while
ZK-GanDef, PGD-Adv and PGD-GanDef have about 10%
to 12% degeneration from that of the Vanilla classiﬁer. On
CIFAR10 dataset, ZK-GanDef performance is similar to that
of full knowledge defenses and their test accuracy on original
examples are 6% to 10% lower than that of the Vanilla
classiﬁer, respectively. To enhance test accuracy on adversarial
examples, the decision boundary of the classiﬁer becomes
complex with more curves, which causes the degeneration on
classifying original examples compared to the Vanilla classiﬁer
[14].
2) On Single-step Adversarial Examples: We discuss here
the accuracy results on FGSM examples, which are depicted
on sub-ﬁgures on the second column of Figure 4. Intuitively,
the Vanilla classiﬁer has poor performance on these single-
step adversarial examples, with test accuracy of 21.01% on
MNIST, 7.01% on Fashion-MNIST, and 9.97% on CIFAR10.
Compared with the Vanilla classiﬁer, all zero knowledge
defenses achieve a signiﬁcant enhancement in terms of test
accuracy on all datasets, with the exception of CLP and
CLS on CIFAR10 dataset. Among the zero knowledge ap-
proaches, ZK-GanDef achieves the highest test accuracy on
all the datasets with signiﬁcant margin. On MNIST, the test
accuracy is 88.70%, 89.29%, and 98.97% with CLP, CLS,
and ZK-GanDef, respectively. On Fashion-MNIST, the test
accuracy is 44.78%, 41.14%, and 70.19% CLP, CLS, and ZK-
GanDef, respectively. On CIFAR10, ZK-GanDef is the only
zero knowledge defense that reasonably works test accuracy
around 60.91%.
In general, full knowledge approaches have better under-
standing of the adversarial examples since such examples
are part of their training datasets. Therefore, full knowl-
edge approaches should, intuitively, have better test accuracy
compared to their zero knowledge counterparts. Our results
conﬁrm this observations. The results show that
the test
accuracy of full knowledge approaches is signiﬁcantly higher
than those of CLP and CLS, especially on Fashion-MNIST and
CIFAR10 datasets. On the other hand, the test accuracy of ZK-
GanDef is comparable to those of full knowledge defenses. In
fact, the test accuracy of ZK-GanDef (98.97%) is higher than
those of all the full knowledge defenses (98.79%, 97.6% and
96.85%). This is because handwritten digits in MNIST are
gray scale ﬁgures with no detailed texture, and therefore, ZK-
GanDef can train to select strongly denoised (even binarized)
features without losing information. As a result, ZK-GanDef
1On CIFAR10 dataset, CLP and CLS have convergence issues during
training and hence the classiﬁer is making random guessing. A detailed study
of this issue is provided in a following subsection.
8
Fig. 4: Test Accuracy on Different Examples (In the 1st and 2nd rows, the results on MNIST dataset are presented. In the
3rd and 4th rows, the results on Fashion-MNIST dataset are presented. In the 5th and 6th rows, the results on CIFAR10
dataset are presented. The results in odd number rows compare the proposed ZK-GanDef with Vanilla as well as existing zero
knowledge methods, CLP and CLS. The results in even number rows compare the proposed ZK-GanDef with full knowledge
defenses which include FGSM-Adv, PGD-Adv and PGD-GanDef.)
9
Vanilla
CLP
CLS
ZK-GanDef
FGSM-Adv
PGD-Adv
PGD-GanDef
MNIST
Fashion-MNIST
BIM
FGSM
7.01%
5.62%
PGD
4.06%
PGD
0.77%
BIM
1.00%
Original
92.43%
Original
Original
FGSM
98.92% 21.01%
89.92%
99.13% 88.70% 72.61% 59.93% 85.65% 44.78% 22.30% 16.14% 10.00%1
99.24% 89.29% 73.84% 60.63% 86.37% 41.14% 18.55% 14.17% 10.00%1
79.33%
98.95% 98.97% 98.89% 98.71% 81.95% 70.19% 64.97% 63.34%
79.88%
99.07% 98.79% 12.24%
6.81%
82.06%
99.15% 97.60% 94.75% 95.60% 82.33% 76.42% 66.72% 71.80%
99.10% 96.85% 94.28% 95.31% 84.09% 68.19% 52.35% 59.51%
84.05%
91.17% 90.48%
9.73%
7.97%
CIFAR10
FGSM
9.97%
10.00%1
10.00%1
60.91%
41.53%
56.18%
54.14%
BIM
4.93%
10.00%1
10.00%1
46.27%
30.74%
49.21%
46.64%
PGD
4.06%
10.00%1
10.00%1
54.85%
33.86%
51.51%
49.21%
TABLE III: Test Accuracy on Different Examples (The left column shows the test results on MNIST dataset. The middle column
shows the test results on Fashion-MNIST dataset. The right column shows the test results on CIFAR10 dataset.)
can achieve even higher test accuracy than full knowledge
approaches.
the trained classiﬁer
On Fashion-MNIST, FGSM-Adv achieves
the highest
test accuracy (90.48%). The PGD-Adv, PGD-GanDef and
ZK-GanDef achieve the second tier test accuracy (76.42%,
68.19% and 70.19%). This is because FGSM-Adv utilizes
only original and FGSM examples during training, and
therefore,
is overﬁtting on FGSM
examples. This behavior has been observed in [25] and
denoted as gradient masking effect. On CIFAR10, PGD-Adv,
PGD-GanDef and ZK-GanDef achieve comparable test
accuracy (56.18%, 54.14% and 60.19%, respectively), while
the test accuracy of FGSM-Adv is only at 41.53%. Due
to the input dropout in allCNN classiﬁer, the diversity of
training data is enhanced and the overﬁtting of FGSM-Adv is
inhibited [25]. However, FGSM examples are generated with
the weaker single-step method, and hence the test accuracy
degenerates on the stronger iterative examples.
3) On Iterative Adversarial Examples: We analyze here
the test accuracy results on BIM and PGD examples, which
are depicted on the sub-ﬁgures of the third and the fourth
columns of Figure 4, respectively. The ﬁgure clearly shows
that the Vanilla classiﬁer completely failed with both BIM and
PGD examples. This is because BIM and PGD are iterative
adversarial examples and hence are carefully crafted to mislead
Vanilla classiﬁers.
Based on the test accuracy results, using zero knowledge
defenses could still enhance the performance on these stronger
adversarial examples. However, these enhancements are lower
than those on FGSM examples. Among zero knowledge
defenses,
the test accuracy of ZK-GanDef is signiﬁcantly
higher than those of CLP and CLS on all iterative adversarial
examples. On MNIST, the test accuracy of ZK-GanDef with
BIM and PGD examples is 25% and 38%, respectively, higher
than those of CLP and CLS. On Fashion-MNIST, the test
accuracy of ZK-GanDef on BIM and PGD examples is 42%
and 47%, respectively, higher than those of CLP and CLS.
On CIFAR10, only ZK-GanDef could work and it achieves
46.27% and 54.85% test accuracy on BIM and PGD examples,
respectively.
As mentioned earlier, full knowledge defenses could achieve
larger enhancement in test accuracy compared to the existing
zero knowledge defenses, CLP and CLS. FGSM-Adv is the
exception as evidenced by its poor performance in defending
iterative adversarial examples due to the reasons we mentioned
in the previous sub-section. On MNIST and Fashion-MNIST,
the test accuracy of FGSM-Adv on BIM and PGD examples
has a huge decrease from over 90% to around 10%. Although
such huge decrease does not exist in the case of CIFAR10, the
test accuracy of FGSM-Adv is clearly lower than that of PGD-
Adv and PGD-GanDef. On all datasets, PGD-Adv and PGD-
GanDef have much stable test accuracy with limited decrease
of test accuracy on FGSM examples. More importantly, the
results show that the test accuracy of ZK-GanDef is close to
those of PGD-Adv and PGD-GanDef on iterative adversarial