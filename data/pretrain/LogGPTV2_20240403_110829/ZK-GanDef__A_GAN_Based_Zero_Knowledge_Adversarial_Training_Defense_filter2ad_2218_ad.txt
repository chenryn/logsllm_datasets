### Accuracy Metric
The accuracy metric is defined as the ratio of the total number of tested images minus the number of failed tests to the total number of tested images (both original and adversarial). Mathematically, this can be expressed as:

\[
\text{Test Accuracy} = \frac{\text{Total Number of Test Examples} - \text{Number of Failed Tests}}{\text{Total Number of Test Examples}}
\]

A test is considered failed if:
1. The original example is misclassified.
2. The original example is rejected.
3. The adversarial example is accepted with an incorrect classification.

For a more precise evaluation, we separately compute the test accuracy for original and adversarial examples. When a defensive method aims to maximize the classifier's ability to identify adversarial examples, it may lead to more rejections or misclassifications of original examples compared to a vanilla classifier. This trade-off between correctly classifying original and adversarial examples is analogous to the trade-off between true positive rate and true negative rate in machine learning.

### Training Time
Another critical metric for evaluating defense approaches is the training time required to build the model. As previously mentioned, generating adversarial examples for full knowledge adversarial training consumes a significant amount of computational resources. The two main factors contributing to training time are:
1. The structure of the classifier (number of layers and parameters).
2. The search algorithm for adversarial examples (e.g., single-step vs. iterative approaches).

The goal is to minimize the training time while maintaining acceptable test accuracy.

### Experimental Results
In this section, we present comparative evaluation results of ZK-GanDef with other state-of-the-art zero-knowledge and full-knowledge adversarial training defenses. The evaluation results are summarized in three subsections:
1. Comparative evaluation of ZK-GanDef with other zero-knowledge and full-knowledge adversarial training defenses on classifying original and different types of adversarial examples.
2. Comparison of the computational consumption of ZK-GanDef with other full-knowledge adversarial training defenses in terms of training time per epoch.
3. Analysis of the convergence issues of CLP and CLS on the CIFAR10 dataset.

#### A. Test Accuracy on Different Examples
In this subsection, we show the test accuracy of the vanilla classifier and classifiers with various defenses against different types of examples. The experiments are conducted on the MNIST, Fashion-MNIST, and CIFAR10 datasets. For each dataset, a total of 28 different results are calculated, spanning all possible pairs of 7 different classifiers (Vanilla, CLP, CLS, ZK-GanDef, FGSM-Adv, PGD-Adv, and PGD-GanDef) and 4 different kinds of examples (original, FGSM, BIM, and PGD). All validation results are presented in Figure 4 and detailed in Table III.

##### 1. On Original Examples
In Figure 4, we first focus on the results presented in the first column sub-figures, which represent the test accuracy on original examples from different datasets. As a baseline, the vanilla classifier achieves 98.92% test accuracy on MNIST, 92.43% on Fashion-MNIST, and 89.92% on CIFAR10. These results are consistent with the benchmark ones presented in [3].

Next, we evaluate the test accuracy of the three zero-knowledge defenses (CLP, CLS, and ZK-GanDef) on different datasets. On the MNIST dataset, their test accuracy on original examples is at the same level as that of the vanilla classifier. The detailed results from Table III show that the difference in test accuracy among the defenses is within 0.5%, which is small enough to be ignored.

On the Fashion-MNIST dataset, the test accuracy of CLP and CLS is 5% higher than that of ZK-GanDef on original examples. Moreover, the test accuracy of all zero-knowledge approaches is 6% to 11% lower than that of the vanilla classifier. This slight degradation is a result of tuning the model to enhance test accuracy on adversarial examples.

On the CIFAR10 dataset, CLP and CLS have significantly lower test accuracy compared to the vanilla classifier and ZK-GanDef. This is because the classifiers with the CLP and CLS methods do not converge at the beginning of the training. A detailed study of this phenomenon is provided in the following subsection.

Finally, we conduct the same evaluation with full-knowledge adversarial training defenses and perform a comparison with the proposed ZK-GanDef. On the MNIST dataset, all full-knowledge defenses and ZK-GanDef achieve the same level of test accuracy as that of the vanilla classifier. On the Fashion-MNIST dataset, FGSM-Adv achieves similar test accuracy on original examples to that of the vanilla classifier, while ZK-GanDef, PGD-Adv, and PGD-GanDef have about 10% to 12% degradation from that of the vanilla classifier. On the CIFAR10 dataset, ZK-GanDef performance is similar to that of full-knowledge defenses, and their test accuracy on original examples is 6% to 10% lower than that of the vanilla classifier. To enhance test accuracy on adversarial examples, the decision boundary of the classifier becomes more complex, causing degradation in classifying original examples compared to the vanilla classifier [14].

##### 2. On Single-Step Adversarial Examples
We discuss here the accuracy results on FGSM examples, depicted in the second column sub-figures of Figure 4. Intuitively, the vanilla classifier has poor performance on these single-step adversarial examples, with test accuracy of 21.01% on MNIST, 7.01% on Fashion-MNIST, and 9.97% on CIFAR10.

Compared with the vanilla classifier, all zero-knowledge defenses achieve a significant enhancement in terms of test accuracy on all datasets, except for CLP and CLS on the CIFAR10 dataset. Among the zero-knowledge approaches, ZK-GanDef achieves the highest test accuracy on all datasets with a significant margin. On MNIST, the test accuracy is 88.70%, 89.29%, and 98.97% for CLP, CLS, and ZK-GanDef, respectively. On Fashion-MNIST, the test accuracy is 44.78%, 41.14%, and 70.19% for CLP, CLS, and ZK-GanDef, respectively. On CIFAR10, ZK-GanDef is the only zero-knowledge defense that works reasonably well, with a test accuracy around 60.91%.

In general, full-knowledge approaches have a better understanding of adversarial examples since such examples are part of their training datasets. Therefore, full-knowledge approaches should, intuitively, have better test accuracy compared to their zero-knowledge counterparts. Our results confirm this observation. The test accuracy of full-knowledge approaches is significantly higher than those of CLP and CLS, especially on the Fashion-MNIST and CIFAR10 datasets. On the other hand, the test accuracy of ZK-GanDef is comparable to those of full-knowledge defenses. In fact, the test accuracy of ZK-GanDef (98.97%) is higher than those of all full-knowledge defenses (98.79%, 97.6%, and 96.85%). This is because handwritten digits in MNIST are grayscale figures with no detailed texture, and therefore, ZK-GanDef can train to select strongly denoised (even binarized) features without losing information. As a result, ZK-GanDef can achieve even higher test accuracy than full-knowledge approaches.

On the Fashion-MNIST dataset, FGSM-Adv achieves the highest test accuracy (90.48%). The PGD-Adv, PGD-GanDef, and ZK-GanDef achieve the second-tier test accuracy (76.42%, 68.19%, and 70.19%). This is because FGSM-Adv utilizes only original and FGSM examples during training, leading to overfitting on FGSM examples. This behavior has been observed in [25] and denoted as gradient masking effect. On the CIFAR10 dataset, PGD-Adv, PGD-GanDef, and ZK-GanDef achieve comparable test accuracy (56.18%, 54.14%, and 60.19%, respectively), while the test accuracy of FGSM-Adv is only 41.53%. Due to the input dropout in the allCNN classifier, the diversity of training data is enhanced, and the overfitting of FGSM-Adv is inhibited [25]. However, FGSM examples are generated with the weaker single-step method, and hence the test accuracy degrades on the stronger iterative examples.

##### 3. On Iterative Adversarial Examples
We analyze here the test accuracy results on BIM and PGD examples, depicted in the third and fourth columns of Figure 4. The figure clearly shows that the vanilla classifier completely fails with both BIM and PGD examples. This is because BIM and PGD are iterative adversarial examples, carefully crafted to mislead vanilla classifiers.

Based on the test accuracy results, using zero-knowledge defenses can still enhance the performance on these stronger adversarial examples, although the enhancements are lower than those on FGSM examples. Among zero-knowledge defenses, the test accuracy of ZK-GanDef is significantly higher than those of CLP and CLS on all iterative adversarial examples. On MNIST, the test accuracy of ZK-GanDef with BIM and PGD examples is 25% and 38%, respectively, higher than those of CLP and CLS. On Fashion-MNIST, the test accuracy of ZK-GanDef on BIM and PGD examples is 42% and 47%, respectively, higher than those of CLP and CLS. On CIFAR10, only ZK-GanDef could work, achieving 46.27% and 54.85% test accuracy on BIM and PGD examples, respectively.

As mentioned earlier, full-knowledge defenses can achieve larger enhancements in test accuracy compared to existing zero-knowledge defenses, CLP and CLS. FGSM-Adv is the exception, as evidenced by its poor performance in defending iterative adversarial examples due to the reasons mentioned in the previous subsection. On MNIST and Fashion-MNIST, the test accuracy of FGSM-Adv on BIM and PGD examples decreases from over 90% to around 10%. Although such a huge decrease does not exist in the case of CIFAR10, the test accuracy of FGSM-Adv is clearly lower than that of PGD-Adv and PGD-GanDef. On all datasets, PGD-Adv and PGD-GanDef have much more stable test accuracy with limited decreases on FGSM examples. More importantly, the results show that the test accuracy of ZK-GanDef is close to those of PGD-Adv and PGD-GanDef on iterative adversarial examples.