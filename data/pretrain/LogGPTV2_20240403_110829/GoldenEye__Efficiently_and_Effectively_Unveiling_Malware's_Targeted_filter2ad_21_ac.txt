continue the branch selection and environment update. If not, speculative execution
starts a new pre-fetch operation to continue analyzing a new code block.
The advantage of using taint analysis is to efﬁciently assist the analysis in three ways:
(1) Our speculative execution is only conducted on the instructions whose operands
have been tainted. It allows us to skip (majority) untainted instruction for speculative
execution to save analysis effort. (2) Tainted propagation can help us to determine the
environment-sensitive branches. Our environment prediction/selection is based on the
correct identiﬁcation of these sensitive branches. (3) Tracking the status of the tainted
32
Z. Xu et al
label helps us to maintain parallel environment spaces and delete/merge untracked
environments.
Heuristics for Branch Selection. Next, we present how we evaluate the branches and
determine which branch is more possible in the targeted environment. In GOLDEN-
EYE, we apply three heuristics to determine what is a possible branch in the targeted
environment:
– If a branch contains a function call that calls some exit or sleep functions, such as
ExitProcess, ExitThread, and sleep, it means this branch may terminate
the program’s execution. We treat another branch as the possible targeted branch.
– If a branch contains function calls that create a new process or thread, such as
CreateProcess and CreateThread, or start network communication, such
as socket and connect, we treat this branch as the possible targeted branch.
Similar function calls could be some representative malicious calls, such as func-
tions for process injection, auto-booting, and kernel hijacking [45].
– If a branch directly interacts with the environment, we treat this branch as the pos-
sible targeted branch. For example, if malware creates a ﬁle before the branch, we
treat the branch that directly operates on the created ﬁle as the targeted branch. Es-
sentially, if one branch contains instructions intensively operating on tainted data,
we consider it as the targeted branch.
After examining these three heuristics, if we still cannot decide the possible targeted
branch in a given time window or we ﬁnd some conﬂicts among different heuristics,
inspired by the multi-path exploration work [37], we will save the snapshot at the branch
point and conduct the concrete execution for both branches. While this may lead to more
overhead (as in [37]), our experimental result shows that such cases are very rare (less
than 5% cases require rolling back).
Determining Targeted Branch. Our scheme of branch evaluation is to foresee k (e.g.,
k = 50) instructions and ﬁnd whether any of them contains code of our interest. The
foreseeing operation is conducted by statically disassemble the code in the blocks after
the addresses of two branches. It is gradually processed until we have collected enough
evidence for predicting the branch.
In particular, we start with disassembling one code block at a time. We also need
to disassemble all the possible branches after each code block. Then we scan each
instruction to check whether it (1) has a CALL instruction or (2) operates on some
tainted data.
For the ﬁrst case, we need to examine the destination address of CALL. Beforehand,
we need to maintain two API address lists: the ﬁrst records the address of possible ma-
licious functions such as CreateProcess and socket, and the second records the
dormant/termination functions such as sleep and ExitProcess. Thus, if CALL’s
destination address belongs to either of the lists, we set the corresponding preference to
the explored branch.
For the second case, we examine each instruction along two alternative paths to
see whether any instruction operates on the tainted data or not. We achieve that by
examining the source operands of each instruction. If the source operand is tainted
before, we consider the instruction operates on tainted data. Then we deduce the path
that contains more instructions using tainted data.
GOLDENEYE: Efﬁciently and Effectively Unveiling Malware’s Targeted Environment
33
If we cannot make a decision after we examine k instructions, we apply enforced
execution [46] to explore both branches. In this case, we need an extra round of analysis
for each branch (which is very rare in practice, as shown in our evaluation).
In Figure 3 Step C, we illustrate our strategy by evaluating two branches after the JZ
instruction. As shown in the left branch, the execution may direct to leave and retn
while the right branch exhibits possible malicious logic, such as CreateThread.
Then, we choose the right branch and identify the alternative context as our preferred
execution context.
Environment Update. The result of target branch prediction is to decide whether to re-
main in the current running environment or to switch to another alternative environment.
If the environment switching is needed, there are three basic environment switching op-
erations: (1) Creation, (2) Removal, (3) Substitution.
The key requirement of our design is to update the environment online. Hence, our
environment update step is performed directly after the speculative execution engine
has committed its execution context.
Creating an element is a common case for an environment update. Especially when
malware tries to query the existence of certain system object, we would thus create such
an object to ensure that the following malware operation on this object will succeed. To
this end, we create a dummy object in the system, such as creating a blank ﬁle with
certain ﬁle name or creating a new registry entry. Accordingly, deleting the element is
the opposite operation and we can simply achieve that by deleting the corresponding
existing system object. While the dummy objects may not always work because fun-
damentally we may not have the exact same knowledge as malware and its targeted
environment to ﬁll the actual content, this scheme works quite well in our evaluation.
And we leave a full discussion of GOLDENEYE limitations in Section 7.
The substitution operation usually occurs when malware requires different system
conﬁguration from the current running environment. A main approach to ﬁnd out the
correct environment setting is through the result of the speculative execution. Since the
speculative execution tells us the condition to ensure the selected branch, we can con-
cretely set up the value to satisfy this requirement. For example, we can modify some
registry entries to modify certain software version. As a more generic solution, we de-
sign an API manipulation scheme. When a substitution occurs, we hook the previously
captured APIs or instructions, and return a manipulated value to malware for every
query.
The environment update for our working example is illustrated in Figure 3 Step D.
The ﬁrst step is to update the base execution context as the selected context. In the
example, we ﬁrst update the ESI and ZF register. Secondly, since EAX is the object
handle of the mutex object, we need to create the mutex for current context and bind
EAX to the mutex handle. In our implementation, we do not concretely create the
mutex. Instead, we record the handle value and when any system call operates on the
handle, we enforce the SUCCESS to emulate the existence of the object.
Handling Space Explosion. As one notable problem for parallel space maintenance
in the speculative execution engine, explosion of parallel spaces could dramatically in-
crease the overhead of GOLDENEYE, especially when the combination of multiple en-
vironment elements happens (Cartesian Effect). We solve the problem by periodically
34
Z. Xu et al
pruning the parallel spaces. More specially, we enforce the space adjustment when cur-
rent memory occupation exceeds some predeﬁned threshold, ρh. During the analysis,
we associate a timestamp T with each environment element. The time stamp denotes
the last instruction that accesses the corresponding taint label of the element. When the
current memory usage overﬂows ρh, the speculative execution engine fetches the en-
vironment element with the oldest time stamp. Then, the update operation merges all
the parallel spaces which have different values for the pruned elements. This process
is recursively performed till the current memory capacity is below a predeﬁned lower
bound, ρl. In practice, among all of our test cases, the average number of concurrent
parallel spaces is below 200. It means that, with minor memory overhead (below 500B)
for each space, the space pruning rarely occurs in the practical analysis task.
5 Distributed Deployment of GOLDENEYE
While the above scheme works very well in practice (as shown in our experiment),
there are still some concerns: (1) To prevent rolling-back, we adopt branch evaluation
to select the most likely malicious branch, which might not always be accurate. (2) Our
environment update step is conducted online. Thus, some analysis is possibly conducted
on a set of inconsistent environments. (3) The possible environment explosion may
overburden one analysis instance.
To further improve the accuracy and efﬁciency, we propose a distributed deployment
scheme of GOLDENEYE. The scheme is essentially taking advantage of parallel en-
vironments created by the speculative engine and distributing them to a set of worker
machines for further analysis.
In detail, when the speculative engine detects an environment-sensitive branch, it
can choose to push a request R into a shared task queue and allow an idle worker
(virtual) machine to handle the further exploration. The worker machine monitoring
(WMM) tool pulls each request and updates the environment settings before analyzing
a malware sample. After the booting of a malware sample, the WMM tool will monitor
the execution status and enable the speculative execution if some unobserved malicious
logic has occurred.
There are two tasks for each WMM: (1) Updating analysis environment, which is a
set of operations to update its environment before analaysis, such as create/delete en-
vironment element or modify current environment value. After that, we create one cus-
tomized environment for each analysis task. (2) Starting speculative execution, which is
to conduct a series of EIP and basic context registers comparison before restarting the
speculative execution. By skipping the instructions which have been analyzed before,
we can focus on exploring new malicious behaviors.
The merits of our design are twofold. First, the analysis environment is dynamically
changed and the setting is dynamically generated based on the malware execution and
analysis progress. It is essentially different from the parallel/distributed deployment of
existing analysis techniques [36,23] because their settings are statically preconﬁgured.
Second, it saves a huge amount of system resources including memory and storage.
Snapshot-based schemes such as [33,37] are mainly used as sequential execution. If
one attempts to parallelize its deployment, a great deal of resources need to be used to
store/transmit/restore the snapshots. In our design, each worker machine just maintains
GOLDENEYE: Efﬁciently and Effectively Unveiling Malware’s Targeted Environment
35
one initial snapshot locally and consumes little memory to transmit the environment
request. In this sense, our scheme achieves a better balance between effectiveness and
efﬁciency.
6 Evaluation
We have implemented GOLDENEYE, which consists of over 4,000 lines mixed C and
python code. Our monitoring tool, taint tracking tool, and speculative execution en-
gine are implemented based on the open-source binary instrumentation framework,
DynamoRIO [5] by ﬁrst translating the X86 instructions into an intermediate language
BIL [26], then performing data and control ﬂow analysis afterwards. We write our se-
mantic rule module as an independent C library, which receives the output of the mon-
itoring tool and parses each instruction. Our environment selector is based on an open
source disassembly library, distorm[3]. We also implement a lightweight emulated ex-
ecution engine inside the module to perform branch evaluation. In addition, our envi-
ronment update module is implemented as an API hook tool based on DynamoRIO and
a set of dummy object creation/deletion scripts, which can be directly invoked by our
environment update module. In this section, we present our evaluation results.
6.1 Experiment Dataset
Our test dataset consists of 1, 439 malware samples, collected from multiple online
malware repositories such as Anubis [1] and other sources [10]. This dataset is ran-
domly collected without any pre-selection involved. We analyze these 1, 439 malware
using a free virus classiﬁcation tool [20] and classify them into 417 distinct malware
families. Analyzing the classiﬁcation result, we further categorize these 417 malware
families into four classes: Trojan, Worm, Spyware/Adware, and Downloader. The statis-
tics about our dataset is listed in Table 2. Meanwhile, we also collect a small dataset
that includes some well-known malware samples which are environment-targeted, such
as Conﬁcker [43], Duqu [4], Sality [12], and Zeus [21]. For each malware family, we
collected several variant samples.
Table 2. Malware’s Classiﬁcation from VirusTotal
Category
Trojan
Adware/Spyware
Worm/Virus
Downloader
Total
# Malware Samples Percent Distinct Families
627
284
185
343
1, 439
43.57%
19.73%
12.85%
23.83%
100%
263
59
27
68
417
6.2 Experiment Setup
In our experiment setting, we manually labeled 112 system/library APIs with 122 output
parameters, and hooked them in our analysis. All our experiments are conducted in a
machine with Intel Core Duo 2.53GHz processor and 4GB memory.
36
Z. Xu et al
6.3 Experiments on General Malware Corpus
We conduct the following experiments to evaluate GOLDENEYE on the larger malware
dataset with 1, 439 samples.
Measurement of Effectiveness. First, we study the effectiveness of our approach in
terms of the code coverage in analysis. To measure that, we ﬁrst collect a baseline trace
by naturally running each malware sample in our virtual environment for 5 minutes.
Then we apply GOLDENEYE to collect a new trace in the adaptively-changing envi-
ronment(s). In our evaluation, we measure the relative increase in the number of native
system calls between the base run and analysis run. The distribution of increased APIs
among all malware samples is shown in Figure 4. As seen in Figure 4, over 500 malware
samples exhibit over 50% more APIs in the new run. It shows that our system can ex-
pose more malware’s environment-sensitive behaviors. From the result, we also ﬁnd that
over 10% Adware/Spyware exhibits 100% more behaviors. It may imply that Spyware
is more sensitive to the running environment compared with other malware categories.
This is reasonable because Spyware normally exhibits its malicious behavior after it col-
lects enough information about the infected user. This further proves the usefulness of
our system. Examining the quantitative results of other categories, it is evident that our
system can efﬁciently discover malware’s environment-sensitive functionalities.
600
500
400
300
200
100
0
0%-10%
10%-50%
50%-100%
>100%
Related Work I
GoldenEye
Trojan
Adware/Spyware
Worm
Downloader
Overall
0
50
100 150 200 250 300 350 400 450 500 550 600 650 700
Worst Case
Total Analysis Time 
Average
Best Case
min 
Fig. 4. Relative Increase of Native APIs
Fig. 5. Analysis Time Comparison
Comparison with Related Work. The last set of our experiment is to compare the ef-
fectiveness and efﬁciency of GOLDENEYE with other approaches. To this end, we ﬁrst
implemented the approach presented in the related work [37] (labeled as Related Work
I), which needs to explore multiple possible paths of environment-sensitive branches.
Secondly, we conﬁgure four virtual environments according to the descriptions in re-
lated work [36] (labeled as Related Work II). We test malware samples in all four envi-
ronments and choose the best one as the result. Then we randomly select 100 malware
samples from each category of malware and collect the traces generated by GOLDEN-
EYE, Related Work I, and II, respectively. When collecting each execution path trace,
we terminate the analysis if no further system calls are observed for 30 seconds (e.g.,
sample terminates or sleeps), or if it reaches maximum analysis time which we set as
300 seconds (5 minutes) for GOLDENEYE and Related Work II. For Related Work I,
since it needs to explore all possible paths, we have to let it run for a much longer
time. However, it could possibly take forever. Hence, in this experiment we limit its
maximum analysis time to 12 hours.
GOLDENEYE: Efﬁciently and Effectively Unveiling Malware’s Targeted Environment
37
Table 3. Performance comparison with two representative existing approaches
Approach
GOLDENEYE
Related Work I[37]
Related Work II[36]
Malware
Percent of Increased APIs
# of Rolling Back
Memory/Disk Usage
Trojan
Trojan
Worm
Downloader
100% 500 5MB
31%