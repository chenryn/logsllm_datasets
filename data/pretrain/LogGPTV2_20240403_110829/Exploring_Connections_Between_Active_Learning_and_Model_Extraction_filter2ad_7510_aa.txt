title:Exploring Connections Between Active Learning and Model Extraction
author:Varun Chandrasekaran and
Kamalika Chaudhuri and
Irene Giacomelli and
Somesh Jha and
Songbai Yan
Exploring Connections Between Active Learning 
and Model Extraction
Varun Chandrasekaran, University of Wisconsin-Madison; Kamalika Chaudhuri, 
University of California San Diego; Irene Giacomelli, Protocol Labs; Somesh Jha, 
University of Wisconsin-Madison; Songbai Yan, University of California San Diego
https://www.usenix.org/conference/usenixsecurity20/presentation/chandrasekaran
This paper is included in the Proceedings of the 29th USENIX Security Symposium.August 12–14, 2020978-1-939133-17-5Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX.Exploring Connections Between Active Learning and Model Extraction
Varun Chandrasekaran1, Kamalika Chaudhuri3, Irene Giacomelli2, Somesh Jha1, and Songbai Yan3
1University of Wisconsin-Madison
2Protocol Labs
3University of California San Diego
Abstract
Machine learning is being increasingly used by individu-
als, research institutions, and corporations. This has resulted
in the surge of Machine Learning-as-a-Service (MLaaS) -
cloud services that provide (a) tools and resources to learn the
model, and (b) a user-friendly query interface to access the
model. However, such MLaaS systems raise concerns such
as model extraction. In model extraction attacks, adversaries
maliciously exploit the query interface to steal the model.
More precisely, in a model extraction attack, a good approxi-
mation of a sensitive or proprietary model held by the server
is extracted (i.e. learned) by a dishonest user who interacts
with the server only via the query interface. This attack was
introduced by Tramèr et al. at the 2016 USENIX Security
Symposium, where practical attacks for various models were
shown. We believe that better understanding the efﬁcacy of
model extraction attacks is paramount to designing secure
MLaaS systems. To that end, we take the ﬁrst step by (a)
formalizing model extraction and discussing possible defense
strategies, and (b) drawing parallels between model extraction
and established area of active learning. In particular, we show
that recent advancements in the active learning domain can
be used to implement powerful model extraction attacks, and
investigate possible defense strategies.
1 Introduction
Advancements in various facets of machine learning has made
it an integral part of our daily life. However, most real-world
machine learning tasks are resource intensive. To that end,
several cloud providers, such as Amazon, Google, Microsoft,
and BigML offset the storage and computational requirements
by providing Machine Learning-as-a-Service (MLaaS). A
MLaaS server offers support for both the training phase, and
a query interface for accessing the trained model. The trained
model is then queried by other users on chosen instances (refer
Fig. 1). Often, this is implemented in a pay-per-query regime
i.e. the server, or the model owner via the server, charges the
the users for the queries to the model. Pricing for popular
MLaaS APIs is given in Table 1.
Current research is focused at improving the performance
of training algorithms, while little emphasis is placed on the
related security aspects. For example, in many real-world ap-
plications, the trained models are privacy-sensitive - a model
can (a) leak sensitive information about training data [5] dur-
ing/after training, and (b) can itself have commercial value or
can be used in security applications that assume its secrecy
(e.g., spam ﬁlters, fraud detection etc. [29, 38, 53]). To keep
the models private, there has been a surge in the practice of
oracle access, or black-box access. Here, the trained model
is made available for prediction but is kept secret. MLaaS
systems use oracle access to balance the trade-off between
privacy and usability.
Models
• DNNs
• Regression
• Decision trees
• Random forests
• Binary & n-ary
classiﬁcation
• Batch
• Online
Google
Amazon
Microsoft
Conﬁdence
Score
Conﬁdence
Score
Leaf Node
Leaf Node
Conﬁdence
Score
$0.093∗
$0.056∗
✗
Conﬁdence
Score
✗
✗
Conﬁdence
Score
Conﬁdence
Score
Conﬁdence
Score
Leaf Node
Leaf Node
Conﬁdence
Score
$0.1
$0.0001
$0.5
$0.0005
Table 1: Pricing, and auxiliary information shared. ∗ Google’s pricing
model is per node per hour. Leaf node denotes the exact leaf (and not an
internal node) where the computation halts, and ✗indicates the absence of
support for the associated model.
Despite providing oracle access, a broad suite of attacks
continue to target existing MLaaS systems [13]. For example,
membership inference attacks attempt to determine if a given
data-point is included in the model’s training dataset only by
interacting with the MLaaS interface (e.g. [52]). In this work,
we focus on model extraction attacks, where an adversary
makes use of the MLaaS query interface in order to steal the
proprietary model (i.e. learn the model or a good approxima-
USENIX Association
29th USENIX Security Symposium    1309
1310    29th USENIX Security Symposium
USENIX Association
2 Machine Learning Overview
In this section, we give a brief overview of machine learning,
and terminology we use throughout the paper. In particular,
we summarize the passive learning framework in § 2.1, and
focus on active learning algorithms in § 2.2. A review of
the state-of-the-art of active learning algorithms is needed
to explicitly link model extraction to active learning and is
presented in § 3.
2.1 Passive learning
In the standard, passive machine learning setting, the learner
has access to a large labeled dataset and uses it in its entirety
to learn a predictive model from a given class. Let X be
an instance space, and Y be a set of labels. For example, in
object recognition, X can be the space of all images, and Y
can be a set of objects that we wish to detect in these images.
We refer to a pair (x, y) ∈ X× Y as a data-point or labeled
instance (x is the instance, y is the label). Finally, there is
a class of functions F from X to Y called the hypothesis
space that is known in advance. The learner’s goal is to ﬁnd a
predicts the labels, a loss function ℓ is used. Given a data-point
function ˆf ∈ F that is a good predictor for the label y given
the instance x, with (x, y) ∈ X× Y. To measure how well ˆf
z = (x, y) ∈ X× Y, ℓ( ˆf , z) measures the difference between
ˆf (x) and the true label y. When the label domain Y is ﬁnite
(classiﬁcation problem), the 0-1 loss function is frequently
used:
ℓ( ˆf , z) =(0,
1,
if ˆf (x) = y
otherwise
If the label domain Y is continuous, one can use the square
loss: ℓ( ˆf , z) = ( ˆf (x)− y)2.
In the passive setting, the PAC (probably approximately
correct) learning [56] framework is predominantly used. Here,
we assume that there is an underlying distribution D on X× Y
that describes the data; the learner has no direct knowledge
of D but has access to a set of training data D drawn from it.
The main goal in passive PAC learning is to use the labeled
instances from D to produce a hypothesis ˆf such that its
expected loss with respect to the probability distribution D is
low. This is often measured through the generalization error
of the hypothesis ˆf , deﬁned by
ErrD ( ˆf ) = E
z∼D [ℓ( ˆf , z)]
(1)
More precisely, we have the following deﬁnition.
Deﬁnition 1 (PAC passive learning [56]). An algorithm A is
a PAC passive learning algorithm for the hypothesis class F if
the following holds for any D on X× Y and any ε, δ ∈ (0, 1):
If A is given sA(ε, δ) i.i.d. data-points generated by D, then A
outputs ˆf ∈ F such that ErrD ( ˆf ) ≤ min f∈F ErrD ( f ) + ε with
probability at least 1− δ. We refer to sA(ε, δ) as the sample
complexity of algorithm A.
Remark 1 (Realizability assumption). In the general case, the
labels are given together with the instances, and the factor
min f∈F ErrD ( f ) depends on the hypothesis class. Machine
learning literature refers to this as agnostic learning or the
non-separable case of PAC learning. However, in some ap-
plications, the labels themselves can be described using a
labeling function f ∗ ∈ F . In this case (known as realizable
learning), min f∈F ErrD ( f ) = 0 and the distribution D can be
described by its marginal over X. A PAC passive learning al-
gorithm A in the realizable case takes sA(ε, δ) i.i.d. instances
generated by D and the corresponding labels generated using
f ∗, and outputs ˆf ∈ F such that ErrD ( ˆf ) ≤ ε with probability
at least 1− δ.
2.2 Active learning
In the passive setting, learning an accurate model (i.e. learning
ˆf with low generalization error) requires a large number of
data-points. Thus, the labeling effort required to produce an
accurate predictive model may be prohibitive. In other words,
the sample complexity of many learning algorithms grows
rapidly as ε→ 0 (refer Example 1). This has spurred interest in
learning algorithms that can operate on a smaller set of labeled
instances, leading to the emergence of active learning (AL).
In active learning, the learning algorithm is allowed to select a
subset of unlabeled instances, query their corresponding labels
from an annotator (i.e. oracle) and then use it to construct or
update a model. How the algorithm chooses the instances
varies widely. However, the common underlying idea is that
by actively choosing the data-points used for training, the
learning algorithm can drastically reduce sample complexity.
Formally, an active learning algorithm is an interactive pro-
cess between two parties - the oracle O and the learner L. The
only interaction allowed is through queries - L chooses x ∈ X
and sends it to O, who responds with y ∈ Y (i.e., the oracle re-
turns the label for the chosen unlabeled instance). This value
of (x, y) is then used by L to infer some information about the
labeling procedure, and to choose the next instance to query.
Over many such interactions, L outputs ˆf as a predictor for
labels. We can use the generalization error (1) to evaluate the
accuracy of the output ˆf . However, depending on the query
strategy chosen by L, other types of error can be used.
There are two distinct scenarios for active learning: PAC
active learning and Query Synthesis (QS) active learning. In
literature, QS active learning is also known as Membership
Query Learning, and we will use the two terms synonymously.
2.2.1 PAC active learning
This scenario was introduced by Dasgupta in 2005 [20] in the
realizable context and then subsequently developed in follow-
ing works (e.g., [4, 19, 26]). In this scenario, the instances
are sampled according to the marginal of D over X, and the
learner, after seeing them, decides whether to query for their
labels or not. Since the data-points seen by L come from the
actual underlying distribution D, the accuracy of the output
hypothesis ˆf is measured using the generalization error (1),
USENIX Association
29th USENIX Security Symposium    1311
as in the classic (i.e., passive) PAC learning.
There are two options to consider for sampling data-points.
In stream-based sampling (also called selective sampling) ,
the instances are sampled one at a time, and the learner decides
whether to query for the label or not on a per-instance basis.
Pool-based sampling assumes that all of the instances are
collected in a static pool S ⊆ X and then the learner chooses
speciﬁc instances in S and queries for their labels. Typically,
instances are chosen by L in a greedy fashion using a met-
ric to evaluate all instances in the pool. This is not possible
in stream-based sampling, where L goes through the data
sequentially, and has to therefore make decisions to query
individually. Pool-based sampling is extensively studied since
it has applications in many real-world problems, such as text
classiﬁcation, information extraction, image classiﬁcation and
retrieval, etc. [39]. Stream-based sampling represents scenar-
ios where obtaining unlabeled data-points is easy and cheap,
but obtaining their labels is expensive (e.g., stream of data is
collected by a sensor, but the labeling requires an expert).
Before describing query synthesis active learning, we wish
to highlight the advantage of PAC active learning over pas-
sive PAC learning (i.e. the reduced sample complexity) for
some hypothesis class through Example 1. Recall that this
advantage comes from the fact that an active learner is al-
lowed to adaptively choose the data from which it learns,
while a passive learning algorithm learns from a static set of
data-points.
Example 1 (PAC learning for halfspaces). Let Fd,HS be the
hypothesis class of d-dimensional halfspaces1, used for binary
classiﬁcation. A function in fw ∈ Fd,HS is described by a
normal vector w ∈ Rd (i.e., ||w||2 = 1) and is deﬁned by
fw(x) = sign(hw, xi) for any x ∈ Rd
ε ) + 1
ε log( 1