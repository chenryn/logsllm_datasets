### Improved Text

#### Introduction
MILR (Memory Integrity and Layer Recovery) demonstrates high recoverability in the small CIFAR network, even when multiple erroneous layers occur between checkpoints. 

#### Table VIII: CIFAR-10 Large Network Whole Layer Error Accuracy
| Layer          | Recovery (MILR)  |
|----------------|------------------|
| Conv           | N/A*             |
| Conv Bias      | 12.2%            |
| Conv 1         | 100.0%           |
| Conv 1 Bias    | 24.4%            |
| Conv 2         | N/A*             |
| Conv 2 Bias    | 9.6%             |
| Conv 3         | 100.0%           |
| Conv 3 Bias    | 71.5%            |
| Conv 4         | N/A*             |
| Conv 4 Bias    | 11.8%            |
| Conv 5         | 100.0%           |
| Conv 5 Bias    | 85.6%            |
| Dense          | N/A*             |
| Dense Bias     | 12.4%            |
| Dense 1        | 100.0%           |
| Dense 1 Bias   | 95.8%            |

* Convolution partial recoverable

#### Analysis
For the larger CIFAR-10 network, the convolutional layers were configured for partial recoverability to minimize costs. This means that these layers can only handle a limited number of erroneous parameters per filter (G2). For the remaining layers, MILR was able to restore them to their original 100.0% accuracy, as shown in Table VIII.

#### Table IX: CIFAR-10 Large Network Storage Overhead
| Method         | Storage (MB)  |
|----------------|---------------|
| Backup Weights | 9.56          |
| ECC            | 2.09          |
| MILR           | 8.50          |
| ECC & MILR     | 9.59          |

The large CIFAR network incurs higher storage costs compared to the smaller network due to its size. However, by using partial recoverability for convolutional layers, the cost was reduced to 8.50 MB, representing an 11.0% reduction over storing a full backup copy of the network.

#### Network Availability
The availability of a CNN is crucial for its utility, but it often involves a trade-off with accuracy. Availability decreases when the network must recover from errors, and without recovery, the network's accuracy can degrade. Therefore, systems must find a balance that aligns with their specific requirements. For mission-critical applications like self-driving cars, high accuracy may be prioritized over availability, as redundancy already exists. Conversely, for applications like website recommendation tools, high availability is essential, even if it means slightly lower accuracy.

This trade-off can be modeled using Equation 6:

\[ f(a) = A \left( \frac{a - 1}{T_d I + T_r} \right) T_{be} \]

where:
- \( a \) is the required availability,
- \( T_d \) is the time taken in the detection phase,
- \( I \) is the number of runs of detection between errors,
- \( T_r \) is the time taken to recover,
- \( T_{be} \) is the time between errors in the system,
- \( A() \) is a function that returns the network accuracy given the number of errors.

#### Performance Evaluation
We evaluated MILR on a system running Windows 10 OS with a Ryzen 5 2600X, 32 GB of RAM, and an Nvidia RTX 2070. MILR leverages the GPU for some layer solving, but most operations are CPU-bound. There is potential for further optimization, but the current performance is as follows:

#### Table X: MILR Prediction and Identification Time in Seconds
| Network       | Single Prediction | Batch Prediction | Identification |
|---------------|-------------------|------------------|----------------|
| MNIST         | 0.017 s           | 3.48E-05 s       | 0.010 s        |
| CIFAR-10 Small| 0.018 s           | 6.50E-05 s       | 0.018 s        |
| CIFAR-10 Large| 0.018 s           | 8.77E-05 s       | 0.016 s        |

Error identification times vary between networks but remain constant within each network. Compared to a single prediction, the times are comparable and reasonable since MILR uses a forward pass for error detection. In large batches, performance can be 200-300 times slower due to the pipelining of predictions.

Recovery times increase with the number of errors, especially in the partial recoverability of convolutional layers. The growth rate is unique to each network and increases super-linearly. These costs can be managed by initiating recovery before the number of errors reaches a point where recovery time becomes exponentially longer.

#### Trade-off Between Accuracy and Availability
Using MILR’s identification and recovery times, the trade-off between accuracy and availability is illustrated in Figure 12. This graph helps determine the optimal settings for error detection intervals in MILR. Two example users (A and B) are shown:
- User A requires a high-accuracy network (at least 99.999% accuracy), and the corresponding availability is shown at the intersection of line (A) and the networks.
- User B requires high availability (at least 99.9%), and the obtained accuracy for each network is shown at the intersection of line (B) and the networks.

#### Conclusion
In this paper, we introduced a novel distinction between ciphertext-space and plaintext-space error correction (PSEC). We highlighted that the assumption of randomly distributed bit errors, which ECC relies on, is not valid in plaintext space. One bit error in the ciphertext space can cause concentrated many-bit errors in the plaintext, leading to whole-weight errors that are difficult to recover using ECC. We then introduced MILR, the first PSEC technique for CNNs. MILR exploits the natural algebraic relationship between input, parameters, and output in CNNs to detect and correct bit errors, whole-weight errors, and even whole-layer errors. MILR is implemented in software and can run on any hardware. It outperforms ECC in maintaining high network accuracy, even under random bit errors.

#### References
[1] “AMD SEV-SNP: Strengthening VM Isolation with Integrity Protection and More,” February 2020.
[2] M. Abadi et al., “TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems,” 2015.
[3] BusinessWire, “Global Artificial Neural Network Market Report 2019-2024,” October 2019.
[4] Intel Corporation, “Intel® Architecture Memory Encryption Technologies Specification,” 2019.
[5] T. Cover and J. Thomas, Elements of Information Theory, 2nd ed. New York NY: Wiley-Interscience, 1991.
[6] D. Fiala, “Detection and Correction of Silent Data Corruption for Large-Scale High-Performance Computing,” 2012 International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1–12, 2011.
[7] H. Guan et al., “In-Place Zero-Space Memory Protection for CNN,” ArXiv, vol. abs/1910.14479, 2019.
[8] M. M. Hossain et al., “FAWCA: A Flexible-Greedy Approach to Find Well-Tuned CNN Architecture for Image Recognition Problem,” August 2018.
[9] J. Kim et al., “Multi-Bit Error Tolerant Caches Using Two-Dimensional Error Coding,” 40th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2007), pp. 197–209, 2007.
[10] A. Krizhevsky, “Learning Multiple Layers of Features from Tiny Images,” 2009.
[11] Y. LeCun and C. Cortes, “The MNIST Database of Handwritten Digits,” 2005.
[12] G. Li et al., “Understanding Error Propagation in Deep Learning Neural Network (DNN) Accelerators and Applications,” Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 2017.
[13] J. Li et al., “Resistance Drift in Phase Change Memory,” in 2012 IEEE International Reliability Physics Symposium (IRPS), 2012, pp. 6C.1.1–6C.1.6.
[14] Y. Liu et al., “Fault Injection Attack on Deep Neural Network,” 2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD), pp. 131–138, 2017.
[15] R. E. Lyons and W. Vanderkulk, “The Use of Triple-Modular Redundancy to Improve Computer Reliability,” IBM J. Res. Dev., vol. 6, pp. 200–209, 1962.
[16] D. Pedamonti, “Comparison of Non-Linear Activation Functions for Deep Neural Networks on MNIST Classification Task,” ArXiv, vol. abs/1804.02763, 2018.
[17] D. S. Phatak and I. Koren, “Complete and Partial Fault Tolerance of Feedforward Neural Nets,” IEEE Transactions on Neural Networks, vol. 6, no. 2, pp. 446–56, 1995.
[18] M. Qin et al., “Robustness of Neural Networks Against Storage Media Errors,” ArXiv, vol. abs/1709.06173, 2017.
[19] A. S. Rakin et al., “Bit-Flip Attack: Crushing Neural Network with Progressive Bit Search,” 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1211–1220, 2019.
[20] O. Russakovsky et al., “ImageNet Large Scale Visual Recognition Challenge,” International Journal of Computer Vision (IJCV), vol. 115, no. 3, pp. 211–252, 2015.
[21] B. Schroeder et al., “DRAM Errors in the Wild: A Large-Scale Field Study,” in SIGMETRICS ’09, 2009.
[22] K. Simonyan and A. Zisserman, “Very Deep Convolutional Networks for Large-Scale Image Recognition,” CoRR, vol. abs/1409.1556, 2015.
[23] V. Sridharan et al., “Memory Errors in Modern Systems,” Sigplan Notices, 2015.
[24] N. Statt, “Self-Driving Car Engineer Anthony Levandowski Pleads Guilty to Stealing Google Trade Secrets,” 2020.
[25] M. Stevenson et al., “Sensitivity of Feedforward Neural Networks to Weight Errors,” IEEE Transactions on Neural Networks, vol. 1, no. 1, pp. 71–80, 1990.
[26] J. Wu, “Introduction to Convolutional Neural Networks,” 2017.