to the small cifar network. MILR is able to achieve high
recoverabilty until multiple erroneous layers occur between
checkpoints.
TABLE VIII: CIFAR-10 large network whole layer error
accuracy
Recovery
Conv
Conv Bias
Conv 1
Conv 1 Bias
Conv 2
Conv 2 Bias
Conv 3
Conv 3 Bias
Conv 4
Conv 4 Bias
Conv 5
Conv 5 Bias
Dense
Dense Bias
Dense 1
Dense 1 Bias
* Convolution partial recoverable
MILR
None
N/A*
12.2%
100.0%
24.4%
N/A*
9.6%
100.0%
71.5%
N/A*
11.8%
100.0%
85.6%
N/A*
12.4%
100.0 %
95.8%
NA*
12.0%
100.0%
97.1%
N/A*
11.4%
100.0%
98.9%
12.0%
100.0%
100.0% 100.0%
100.0%
12.1%
99.7%
100.0%
With the larger size of this network the convolution layers
were required to use partial recoverability to keep cost low.
This means none of the convolution layers are able to cope
with being completely modiﬁed, they are limited to G2 erro-
neous parameters per ﬁlter. For the rest of the layers MILR
was able to recover them back to 100.0% of their original
accuracy as shown in Table VIII.
TABLE IX: CIFAR-10 large network storage overhead
Backup Weights
9.56 MB
ECC
2.09 MB
MILR
8.50 MB
ECC & MILR
9.59 MB
The cost of the large Cifar network cost more than the small
network due to the larger size. However was able to keep cost
lower than storing a second copy of the network thanks to the
user of convolution layer partial recoverabilty. MILR came in
with a cost of 8.50 MB, a 11.0% reduction in cost over storing
a backup copy of the network.
E. Network Availability
The availability of the CNN has a major impact on the
usefulness of the network, but availability and accuracy can
10
1
1
a − 1
(TdI) + Tr
Tbe
be an important trade off in a CNN system. This is due to
availability being reduced when a network has to recover
from errors, and without recovery a networks accuracy can
degrade. Therefore systems have to ﬁnd a balance that suits
their intended mission. In a mission critical application, such
as a self driving car, the need high accuracy might lessen
the need for availability as redundancy already exists. If a
network has a high availability requirement, such as a website
recommendation tool, accuracy might not be as important but
it always need to be available for the user.
f (a) = A
(6)
This trade off can be modeled by using equation 6. Where a
is the required availability, Td is the time taken in the detection
phase, I is the number of runs of detection between errors,
Tr is the time taken to recover, Tbe is the time between errors
in the system, and there exist a function A() that given the
number of errors returns the network accuracy.
We evaluated performance and availability on a MILR
system running on Windows 10 OS on a Ryzen 5 2600X,
32 GB of RAM and a Nvidia RTX 2070. MILR was able
to take advantage of the GPU for parts of the layer solving,
but much of the operations were conﬁned to the CPU. There
is still potential optimization of MILR that can improve the
performance, but MILR was evaluated in its current state.
TABLE X: MILR prediction and identiﬁcation time in seconds
Network
MNIST
CIFAR-10 Small
CIFAR-10 Large
Single
Prediction
0.017s
0.018s
0.018s
Batch
Prediction
3.48E-05s
6.50E-05s
8.77E-05s
Identiﬁcation
0.010s
0.018s
0.016s
Error identiﬁcation time varies between networks but stays
constant in each network, with times shown in Table X. When
compared to a single prediction, the times are comparable
and reasonable as MILR uses a forward pass in its error
detection. Compared to a prediction run in a large batches the
performance can be 200×-300× slower as batch operations
are able to take advantage of the pipelining of the predictions.
Error recovery times are dependent on the number of errors
as shown in Figure 11. As the number of errors grow, the
recovery time also grows due to solving for more errors in
the partial recoverability of convolution layers, plus additional
layers needing solving. The growth rate is unique to each
network, and also increases super linearly. These time cost
can be balanced by inducing recovery before the number of
errors exceed a point in which the recovery time increases
exponentially.
Using MILR’s identiﬁcation and recovery time this balance
between accuracy and availability is plotted in Figure 12.
(a) No recovery
(b) ECC
(c) MILR
(d) ECC + MILR
Fig. 9: CIFAR-10 large network normalized accuracy after recovery from varying RBER
1.00000
0.99995
0.99990
0.99985
0.99980
y
c
a
r
u
c
c
A
m
u
m
i
n
i
M
(A)
(B)
MNIST
CIFAR-10 Small
CIFAR-10 Large
(a) No recovery
(b) MILR
Fig. 10: CIFAR-10 large network normalized accuracy after
recovery from whole-weight errors
MNIST
CIFAR-10 Small
CIFAR-10 Large
0.990
0.992
0.994
0.996
0.998
Availability
Fig. 12: The trade off between availability and minimum ac-
curacy, (A) Minimum Accuracy of 99.999%, (B) Availability
of 99.9%
)
s
(
e
m
T
i
y
r
e
v
o
c
e
R
5
4
3
2
1
0
and availability. This graph is useful to determine the settings
of error detection intervals in MILR. Two example users
(A and B) are shown in the graph. User A needs a high
accuracy network that sustained at least 99.999% accuracy,
the availability that each network yields is shown in the
intersection of line (A) and the networks. On the other hand,
user B needs availability of at least 99.9%, and the obtained
accuracy for each network is shown by the intersection of line
(B) and the networks.
VI. CONCLUSION
In this paper we made a novel distinction between
ciphertext-space vs. plaintext-space error correction (PSEC).
We pointed out that the assumption of randomly distributed
bit errors ECC relies on is not valid in plaintext space, where
one bit error in the ciphertext space turns into concentrated
many-bit errors affecting encryption words, causing whole-
weight errors that are difﬁcult to recover from using ECC.
We then introduced MILR, to our knowledge the ﬁrst PSEC
technique for CNNs. MILR takes advantage of the natural
algebraic relationship between input, parameters, and output of
0
2
4
6
8
10
Errors (thousands)
Fig. 11: The relation between recovery time and errors
Assuming a worst case mean time between failures of 75,000
errors per billion device hours per Mbit [21], where each bit
error affected a ciphertext word causing multi-bit errors int the
plaintext, error detection runs twice between error intervals,
recovery time is the maximum recovery time for the expected
errors in a single year, and that an accuracy equation A(n)
exist and is linear degradation of accuracy from zero errors
and the expected errors in a single year.
Figure 12 show that their is a trade off between accuracy
11
Error Rate1E-075E-071E-065E-061E-055E-051E-045E-04Normalized Accuracy00.20.40.60.81Error Rate1E-075E-071E-065E-061E-055E-051E-045E-04Normalized Accuracy00.20.40.60.81Error Rate1E-075E-071E-065E-061E-055E-051E-045E-04Normalized Accuracy00.20.40.60.81Error Rate1E-075E-071E-065E-061E-055E-051E-045E-04Normalized Accuracy00.20.40.60.81Error Rate1E-075E-071E-065E-061E-055E-051E-045E-041E-03Normalized Accuracy00.20.40.60.81Error Rate1E-075E-071E-065E-061E-055E-051E-045E-041E-03Normalized Accuracy00.20.40.60.81CNNs, in order to detect and correct bit errors, whole-weight
errors, and even whole-layer errors. MILR is implemented
in software and can run on any hardware. MILR can detect
and correct errors which is difﬁcult to achieve using ECC,
unlocking robust PSEC for situations where CNNs run on
encrypted VM. We demonstrate that MILR can recover from
whole-weight errors even at up to 1E-03 error rate, while even
whole-layer errors can even be recovered to 100% accuracy,
making MILR a suitable choice for PSEC. Even on random bit
errors, MILR outperforms ECC in keeping network accuracy
high.
REFERENCES
[1] “Amd sev-snp: Strengthening vm isolationwith integrity protection and
more,” 02 2020.
Irving, M.
[2] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.
Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow,
A. Harp, G.
Isard, Y. Jia, R. Jozefowicz, L. Kaiser,
M. Kudlur, J. Levenberg, D. Man´e, R. Monga, S. Moore, D. Murray,
C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar,
P. Tucker, V. Vanhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals,
P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng,
“TensorFlow: Large-scale machine learning on heterogeneous systems,”
2015. [Online]. Available: https://www.tensorﬂow.org/
artiﬁcial
neu-
ral
2019-2024,”
https://www.businesswire.com/news/home/20191021005304/en/Global-
Artiﬁcial-Neural-Network-Market-Report-2019-2024, Oct 2019.
[3] BusinessWire,
“Global
market
network
[4] I. Corporation, “Intel® architecture memory encryption technologies
report
speciﬁcation,” 2019.
[5] T. Cover and J. Thomas, Elements of Information Theory, 2nd ed. New
York NY: Wiley-Interscience, 1991.
[6] D. Fiala, “Detection and correction of silent data corruption for large-
scale high-performance computing,” 2012 International Conference for
High Performance Computing, Networking, Storage and Analysis, pp.
1–12, 2011.
[7] H. Guan, L. Ning, Z. Lin, X. Shen, H. Zhou, and S.-H. Lim, “In-place
zero-space memory protection for cnn,” ArXiv, vol. abs/1910.14479,
2019.
[8] M. M. Hossain, D. Talbert, S. Ghafoor, and R. Kannan, “Fawca: A
ﬂexible-greedy approach to ﬁnd well-tuned cnn architecture for image
recognition problem,” 08 2018.
[9] J. Kim, N. Hardavellas, K. Mai, B. Falsaﬁ, and J. C. Hoe, “Multi-bit
error tolerant caches using two-dimensional error coding,” 40th Annual
IEEE/ACM International Symposium on Microarchitecture (MICRO
2007), pp. 197–209, 2007.
[10] A. Krizhevsky, “Learning multiple layers of features from tiny images,”
2009.
2005.
[11] Y. LeCun and C. Cortes, “The mnist database of handwritten digits,”
[12] G. Li, S. K. S. Hari, M. B. Sullivan, T. Tsai, K. Pattabiraman, J. S. Emer,
and S. W. Keckler, “Understanding error propagation in deep learning
neural network (dnn) accelerators and applications,” Proceedings of the
International Conference for High Performance Computing, Networking,
Storage and Analysis, 2017.
[13] J. Li, B. Luan, and C. Lam, “Resistance drift in phase change memory,”
in 2012 IEEE International Reliability Physics Symposium (IRPS), 2012,
pp. 6C.1.1–6C.1.6.
[14] Y. Liu, L. Wei, B. Luo, and Q. Xu, “Fault injection attack on deep neural
network,” 2017 IEEE/ACM International Conference on Computer-
Aided Design (ICCAD), pp. 131–138, 2017.
[15] R. E. Lyons and W. Vanderkulk, “The use of triple-modular redundancy
to improve computer reliability,” IBM J. Res. Dev., vol. 6, pp. 200–209,
1962.
[16] D. Pedamonti, “Comparison of non-linear activation functions for
deep neural networks on mnist classiﬁcation task,” ArXiv, vol.
abs/1804.02763, 2018.
[17] D. S. Phatak and I. Koren, “Complete and partial fault tolerance of
feedforward neural nets,” IEEE transactions on neural networks, vol. 6
2, pp. 446–56, 1995.
[18] M. Qin, C. Sun, and D. Vucinic, “Robustness of neural networks against
storage media errors,” ArXiv, vol. abs/1709.06173, 2017.
[19] A. S. Rakin, Z. He, and D. Fan, “Bit-ﬂip attack: Crushing neural network
with progressive bit search,” 2019 IEEE/CVF International Conference
on Computer Vision (ICCV), pp. 1211–1220, 2019.
[20] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and
L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,”
International Journal of Computer Vision (IJCV), vol. 115, no. 3, pp.
211–252, 2015.
[21] B. Schroeder, E. Pinheiro, and W.-D. Weber, “Dram errors in the wild:
a large-scale ﬁeld study,” in SIGMETRICS ’09, 2009.
[22] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” CoRR, vol. abs/1409.1556, 2015.
[23] V. Sridharan, N. DeBardeleben, S. Blanchard, K. Ferreira, J. Stearley,
J. Shalf, and S. Gurumurthi, “Memory errors in modern systems,”
Sigplan Notices, 2015.
[24] N. Statt, “Self-driving car engineer anthony levandowski pleads
guilty to stealing google trade secrets,” 2020. [Online]. Available:
https://www.theverge.com/2020/3/19/21187651/anthony-levandowski-
pleads-guilty-google-waymo-uber-trade-secret-theft-lawsuit
[25] M. Stevenson, R. Winter, and B. Widrow, “Sensitivity of feedforward
neural networks to weight errors,” IEEE transactions on neural net-
works, vol. 1 1, pp. 71–80, 1990.
[26] J. Wu, “Introduction to convolutional neural networks,” 2017.
12