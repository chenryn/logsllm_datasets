title:Subpopulation Data Poisoning Attacks
author:Matthew Jagielski and
Giorgio Severi and
Niklas Pousette Harger and
Alina Oprea
Subpopulation Data Poisoning Attacks
Giorgio Severi
Matthew Jagielski
PI:EMAIL
Northeastern University
Niklas Pousette Harger
PI:EMAIL
Northeastern University
ABSTRACT
Machine learning systems are deployed in critical settings, but they
might fail in unexpected ways, impacting the accuracy of their
predictions. Poisoning attacks against machine learning induce
adversarial modification of data used by a machine learning al-
gorithm to selectively change its output when it is deployed. In
this work, we introduce a novel data poisoning attack called a sub-
population attack, which is particularly relevant when datasets are
large and diverse. We design a modular framework for subpopula-
tion attacks, instantiate it with different building blocks, and show
that the attacks are effective for a variety of datasets and machine
learning models. We further optimize the attacks in continuous do-
mains using influence functions and gradient optimization methods.
Compared to existing backdoor poisoning attacks, subpopulation
attacks have the advantage of inducing misclassification in natu-
rally distributed data points at inference time, making the attacks
extremely stealthy. We also show that our attack strategy can be
used to improve upon existing targeted attacks. We prove that,
under some assumptions, subpopulation attacks are impossible to
defend against, and empirically demonstrate the limitations of ex-
isting defenses against our attacks, highlighting the difficulty of
protecting machine learning against this threat.
CCS CONCEPTS
• Security and privacy; • Theory of computation → Adversar-
ial learning;
KEYWORDS
Adversarial Machine Learning; Poisoning Attacks; Fairness
ACM Reference Format:
Matthew Jagielski, Giorgio Severi, Niklas Pousette Harger, and Alina Oprea.
2021. Subpopulation Data Poisoning Attacks. In Proceedings of the 2021 ACM
SIGSAC Conference on Computer and Communications Security (CCS ’21),
November 15–19, 2021, Virtual Event, Republic of Korea. ACM, New York, NY,
USA, 19 pages. https://doi.org/10.1145/3460120.3485368
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8454-4/21/11...$15.00
https://doi.org/10.1145/3460120.3485368
PI:EMAIL
Northeastern University
Alina Oprea
PI:EMAIL
Northeastern University
1 INTRODUCTION
Machine learning (ML) and deep learning systems are being de-
ployed in sensitive applications, but they can fail in multiple ways,
impacting the confidentiality, integrity and availability of user
data [37]. To date, evasion attacks or inference-time attacks have
been studied extensively in image classification [7, 22, 68], speech
recognition [9, 58], and cyber security [34, 66, 78]. Still, among the
threats machine learning and deep learning systems are vulnera-
ble to, poisoning attacks at training time has recently surfaced as
the threat perceived as most potentially dangerous to companies’
ML infrastructures [38]. The threat of poisoning attacks becomes
even more severe as modern deep learning systems rely on large,
diverse datasets, and their size makes it difficult to guarantee the
trustworthiness of the training data.
In existing poisoning attacks, adversaries can insert a set of cor-
rupted, poisoned data at training time to induce a specific outcome
in classification at inference time. Existing poisoning attacks can
be classified into: availability attacks [4, 29, 76] in which the over-
all accuracy of the model is degraded; targeted attacks [30, 60, 67]
in which specific test instances are targeted for misclassification;
and backdoor attacks [23] in which a backdoor pattern added to
testing points induces misclassification. Poisoning attacks range in
the amount of knowledge the attacker has about the ML system,
varying for example in knowledge about feature representation,
model architecture, and training data [67].
The threat models for poisoning attacks defined in the literature
rely on strong assumptions on the adversarial capabilities. In both
poisoning availability attacks [4, 29, 76] and backdoor attacks [23]
the adversary needs to control a relatively large fraction of the
training data (e.g., 10% or 20%) to influence the model at inference
time. Moreover, in backdoor attacks an adversary is assumed to
have the capability of modifying both the training and the testing
data to include the backdoor pattern. In targeted attacks, there is an
assumption that the adversary has knowledge on the exact target
points during training, which is not always realistic. Additionally,
the impact of a targeted attack is localized to a single point or a
small set of points [21, 60]. In our work, we attempt to bridge gaps
in the literature on poisoning attacks.
1.1 Our Contributions
Subpopulation poisoning attacks. We introduce a novel, realis-
tic, form of data poisoning attack we call the subpopulation attack,
which is particularly relevant for large, diverse datasets. In this at-
tack, an adversary’s goal is to compromise the performance of a clas-
sifier on a particular subpopulation of interest, while maintaining
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3104unaltered its performance for the rest of the data. The advantages
of our novel subpopulation attack are that it requires no adversarial
knowledge of the exact model architecture and parameters, and
most importantly, the attack does not need to modify points at
inference time, which has been a common thread in prior backdoor
poisoning attacks [23, 71]. We also uncover an interesting connec-
tion with research in algorithmic fairness, which shows that ML
classifiers might act differently for different minority groups [6, 25].
We believe this fairness disparity contributes to ML classifiers’ vul-
nerability to our stealthy subpopulation attacks.
Subpopulation attack framework. We propose a modular frame-
work for conducting subpopulation attacks that includes a subpopu-
lation identification component and an attack generation procedure.
We instantiate these components with different building blocks,
demonstrating the modularity of the design. Subpopulations can
be identified either by exact feature matching when annotations
are available or clustering in representation layers of the neural
network models. For poisoning attack generation we evaluate sev-
eral methods: a generic label flipping attack applicable to data from
multiple modalities and an attack based on influence functions [30].
We also propose a gradient optimization attack that improves upon
label flipping on continuous data, and has better performance than
the influence function attack.
Evaluation on end-to-end and transfer learning. We demon-
strate the effectiveness of our framework on datasets from multiple
modalities, including tabular data (UCI Adult), image data (CIFAR-
10 for image classification, UTKFace for face recognition), and
text data (IMDB for sentiment analysis). We use a range of neural
network models, including feed-forward neural networks, convolu-
tional networks, and transformers. We show that both end-to-end
trained models, as well as transfer learning models are vulnerable
to this new attack vector. Additionally, the size of the attack is small
relative to the overall dataset and the poisoned points follow the
training data distribution, making the attacks extremely stealthy.
For instance, with only 126 points, we induce a classification error
of 74% on a subpopulation in CIFAR-10, maintaining similar accu-
racy to the clean model on points outside the subpopulation. In the
UTKFace face recognition dataset, an attack of 29 points increases
classification error by 50% in one subpopulation. We also show that
generating subpopulations can improve targeted attacks [21, 30];
for example, we show that the Witches’ Brew attack [21] is 86%
more effective when targeting points from a single subpopulation.
Challenges for defenses. Several defenses against availability
attacks [17, 29], targeted attacks [53], and backdoor attacks [11,
42, 70, 73, 74] have been proposed. We believe their stealthiness
makes subpopulation attacks difficult to defend against. To support
this claim, we provide an impossibility result, showing that models
based on local decisions, such as mixture models and 𝑘-nearest
neighbors, are inherently vulnerable to subpopulation attacks. We
also evaluate the performance of a large number of existing de-
fenses [11, 17, 29, 42, 55, 61, 70] against our attack, and show their
limitations.
2 BACKGROUND
2.1 Neural Networks Background
Consider a training set of 𝑛 examples 𝐷 = {𝑥𝑖, 𝑦𝑖}𝑛
𝑖=1, with each
feature vector 𝑥𝑖 ∈ X and label 𝑦𝑖 ∈ Y drawn from some data
distribution D. In this paper, we consider multiclass classification
tasks, where a 𝐾-class problem has Y = [𝐾]. The goal of a learning
algorithm 𝐴, when given a dataset 𝐷 is to return a model 𝑓 with
parameters 𝜃 which correctly classifies as many data points as pos-
sible, maximizing E𝑥,𝑦∼D1 (𝑓 (𝑥) = 𝑦). The way in which this is
typically done is by stochastic gradient descent on a differentiable
loss function. To approximate minimizing error, the model is de-
signed to output probabilities of each of the 𝐾 classes (we will write
the 𝑖th class probability as 𝑓 (𝑥)𝑖). In multiclass classification, it is
typical to minimize the categorical cross-entropy loss ℓ:
|𝑋 |
𝑖=1
ℓ(𝑋, 𝑌, 𝑓 ) =
1
|𝑋|
𝑦𝑖(1 − 𝑓 (𝑥𝑖)𝑦𝑖).
In a neural network, the model 𝑓 is a chain of linear and nonlinear
transformations. The linear transformations are called layers, and
the nonlinear transformations are activation functions. Examples ac-
tivation functions are ReLU, sigmoid, and softmax. Domain-specific
neural network architectures have been successful - convolutional
neural networks [39] are widely used for images, and recurrent
neural networks [26] and transformers [72] are popular for text.
In standard training, the model parameters 𝜃 are randomly ini-
tialized. A widespread alternative approach is called transfer learn-
ing [49], in which knowledge is transferred from a large dataset.
In this approach, a model trained on a large dataset is used as an
initialization for the smaller dataset’s model. In this case, a common
approach is to simply use the pretrained model as a feature extrac-
tor, keeping all but the last layer fixed, and only training the last
layer. Another approach is to use it as an initialization for standard
training, and allowing the whole network to be updated. These are
both common approaches [33], which we refer to as "last-layer"
transfer learning and "fine-tuning" transfer learning, respectively.
2.2 Poisoning Attacks
In settings with large training sets, machine learning is vulnerable to
poisoning attacks, where an adversary is capable of adding corrupted
data into the training set. This is typically because data is collected
from a large number of sources which cannot all be trusted. For
example, OpenAI trained their GPT-2 model on all webpages where
at least three users of the social media site Reddit interacted with
the link [54]. Google also trains word prediction models from data
collected from Android phones [24]. An adversary with a small
number of Reddit accounts or Android devices can easily inject
data into these models’ training sets. Furthermore, a recent survey
on companies’ perception of machine learning threats [38] also
highlighted poisoning attacks as one of the most concerning attacks.
More formally, in a poisoning attack, the adversary adds 𝑚 con-
taminants or poisoning points 𝐷𝑝 = {𝑥𝑝
𝑖=1 to the training set,
so that the learner minimizes the poisoned objective ℓ(𝐷 ∪ 𝐷𝑝, 𝑓 )
rather than ℓ(𝐷, 𝑓 ). The poisoned set 𝐷𝑝 is constructed to achieve
some adversarial objective L(𝐴(𝐷 ∪ 𝐷𝑝)).
𝑖 }𝑚
𝑖 , 𝑦𝑝
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3105Attack
Targeted Poisoning (e.g., [21, 30])
Federated Backdoor / Targeted [1]
Reflection Backdoor [43]
Composite Backdoor [41]
Subpopulation (Ours)
Data modality
Image
Image/Text
Image
Image/Text
Image/Text/Tabular
✓
✓
✓
×
✓
✓
✓
×
×
✓
No training data No test-time modifications Attack generalizes
×
×
✓
✓
✓
Table 1: Comparison to related work. The table shows the data modality the attack applies to, whether the attack requires
the knowledge of the exact training points used by the victim model, whether the attacker needs to be able to modify points
during inference, and if the attack generalizes to points which are not known at the time of attack.
Prior objectives for poisoning attacks distinguish between a
targeted distribution to compromise performance on, D𝑡𝑎𝑟𝑔, and
a distribution to maintain the original classifier’s performance on,
D𝑐𝑙𝑒𝑎𝑛. Then attacks can be measured in terms of two metrics,
the collateral damage and the target damage. A collateral damage
constraint requires the accuracy of the classifier on D𝑐𝑙𝑒𝑎𝑛 to be
unaffected, while the target damage requires the performance on
D𝑡𝑎𝑟𝑔 to be compromised. Indeed, existing poisoning attacks can
be grouped into the following taxonomy:
Availability Attacks [4, 29, 45, 76]. The adversary wishes to
reduce the overall model’s performance. The target distribution is
the original data distribution, with no collateral distribution.
Targeted Attacks [30, 60, 67]. The adversary has a small set of
𝑖 } they seek to misclassify. The target distribution
targets {𝑥𝑡
𝑖 , 𝑦𝑡
D𝑡𝑎𝑟𝑔 is {𝑥𝑡
𝑖 }, while the collateral distribution removes {𝑥𝑡
𝑖 }
𝑖 , 𝑦𝑡
𝑖 , 𝑦𝑡
from the support: D𝑐𝑙𝑒𝑎𝑛 = D \ {𝑥𝑡
Backdoor Attacks [12, 23].: The adversary is able to modify one
or a few features of their input, seeking to cause predictable misclas-
sification given the control of these few features. Then the collateral
distribution is the natural data distribution D𝑐𝑙𝑒𝑎𝑛 = D, while the
target distribution shifts the original data distribution by adding
the backdoor perturbation, making D𝑡𝑎𝑟𝑔 = Pert(D). Here Pert
is the function that adds a backdoor pattern to a data point.
𝑖 }.
𝑖 , 𝑦𝑡
2.3 Related Work
Adversarial attacks against machine learning systems can be clas-
sified into poisoning attacks at training time [4], evasion attacks
at testing time [3, 68], and privacy attacks to extract private infor-
mation by interacting with the machine learning system [62, 80].
Below, we survey in more depth the different types of poisoning
attacks and defenses in the literature.
Poisoning Availability Attacks. The idea of tampering with the
training data of an automated classifier to introduce failures in
the final model has been the focus of many research efforts over
time. Some of the early works in this area include attacks against
polymorphic worms detectors [52], network packet anomaly de-
tectors [56], and behavioral malware clustering [5]. Availability
attacks based on gradient descent have been proposed for multiple
models, such as linear regression [29, 76], logistic regression [46],
and SVM [4]. These attacks have the goal to indiscriminately com-
promise the accuracy of the model. Regarding defenses, SEVER [17]
uses SVD to remove points which bias gradients, while TRIM [29]
and ILTM [61] remove points with high loss. These defenses work
iteratively, by identifying and removing outlying points at each
step, until convergence is reached. Demontis et al. [14] study the