spam or not. They might have been a false positive of
B@bel, or a false negative of the existing methods. To
remain on the safe side, we consider them as false posi-
tives. This results in B@bel having a precision of 99.3%.
We then looked at our false negatives. We consider as
false negatives those conversations that B@bel classiﬁed
as belonging to a legitimate client dialect, but that have
been ﬂagged as spam by any of the previously mentioned
techniques.
In total, the other spam detection mecha-
nisms ﬂagged 71,342 emails as spam, among the ones
that B@bel ﬂagged as legitimate. Considering these
emails as false negatives, this results in B@bel having a
false negative rate of 21%. The number of false negatives
might appear large at ﬁrst. However, we need to con-
sider the sources of these spam messages. While the vast
majority of spam comes from botnets, spam can also be
sent by dedicated MTAs, as well as through misused web
mail accounts. Since B@bel is designed to detect email
clients, we are able to detect which MTA or web mail
application the email comes from, but we cannot assess
whether that email is ham or spam. To show that this is
the case, we investigated these 71,342 messages, which
originated from 7,041 unique IP addresses. Assuming
these are legitimate MTAs, we connected to each IP ad-
dress on TCP port 25 and observed greeting messages
for popular MTAs. For 3,183 IP addresses, one of the
MTAs that we used to learn the dialects responded. The
remaining 3,858 IP addresses did not respond within a 10
second timeout. We performed reverse DNS lookups on
these IP addresses and assessed whether their assigned
DNS names contained indicative names such as smtp or
mail. 1,654 DNS names were in this group. We could
not ﬁnd any conclusive proof that the remaining 2,204
addresses belong to legitimate MTAs.
For those dialects for which B@bel could not make a
decision (because the conversation lead to a state where
both one or more legitimate clients and bots were active),
6The blacklists we leveraged come from these services: Barracuda,
CBL, Spamhaus, Atma, Spamcop, Manitu, AHBL, DroneBL, DShield,
Emerging Threats, malc0de, McAfee, mdl, OpenBL, SORBS, Sucuri
Security, TrendMicro, UCEPROTECT, and ZeusTracker. Note that
some services provide multiple blacklists
we investigated if we could have assessed whether the
client was a bot or not by using active probing. Since the
spambot and legitimate client dialects that we observed
are disjoint, this is always possible. In particular, B@bel
found that it is always possible to distinguish between the
dialects spoken by a spambot and by a legitimate email
client that look identical from passive analysis by send-
ing a single SMTP reply. For example, the SMTP RFC
speciﬁes that multi-line replies are allowed, in the case
all the lines in the reply have the same code, and all the
reply codes but the last one are followed by a dash char-
acter. Therefore, multi-line replies that use different re-
ply codes are not allowed by the standard. We can lever-
age different handling of this corner case to disambiguate
between Qmail and Mydoom. More precisely, if we send
the reply 250-OK550 Error, Qmail will
take the ﬁrst reply code as the right one, and continue
the SMTP transaction, while Mydoom will take the sec-
ond reply code as the right one, and close the connec-
tion. Based on these observations, we can say that if we
ran B@bel in active mode, we could distinguish between
these ambiguous cases, and make the right decision. Un-
fortunately, we could run B@bel only in passive mode
on our department mail servers.
Our results show that B@bel can detect (and possi-
bly block) spam emails sent by bots with high accuracy.
However, B@bel is unable to detect those spam emails
sent by dedicated MTAs or by compromised webmail ac-
counts. For this reason, similar to the other state-of-the-
art mitigation techniques, B@bel is not a silver bullet,
but should be used in combination with other anti-spam
mechanisms. To show what would be the advantage of
deploying B@bel on a mail server, we studied how much
spam would have been blocked on our department server
if B@bel was used in addition to or in substitution to
the commercial blacklist and the content analysis sys-
tems that are currently in use on those servers.
Similarly to IP blacklists, B@bel is a lightweight
technique. Such techniques are typically used as a ﬁrst
spam-mitigation step to make quick decisions, as they
avoid having to apply resource-intensive content anal-
ysis techniques to most emails. For this reason, the
ﬁrst conﬁguration we studied is substituting the commer-
cial blacklist with B@bel. In this case, 259,974 emails
would have been dropped as spam, instead of the 219,726
that were blocked by the IP blacklist. This would have
resulted in 15.5% less emails being sent to the content
analysis system, reducing the load on the servers. More-
over, the emails detected as spam by B@bel and the IP
blacklist do not overlap completely. For example, the
IP blacklist ﬂags as spam emails sent by known misused
MTAs. Therefore, we analyzed the amount of spam that
the two techniques could have caught if used together. In
this scenario, 278,664 emails would have been blocked,
resulting in 26.8% less emails being forwarded to the
content analysis system compared to using the blacklist
alone. As a last experiment, we studied how much spam
would have been blocked on our servers by using B@bel
in combination with both the commercial blacklist and
the content analysis systems. In this scenario, 297,595
emails would have been ﬂagged as spam, which consti-
tutes an improvement of 3.9% compared to the servers’
original conﬁguration.
7.3 Evaluating the Feedback Manipulation
To investigate the effects of wrong server feedback to
bots, we set up the following experiment. We ran 32 mal-
ware samples from four large spamming botnet families
(Cutwail, Lethic, Grum, and Bagle) in a controlled envi-
ronment, and redirected all of their SMTP activity to the
third mail server in the B@bel architecture. We conﬁg-
ured this server to report that any recipient of the emails
the bots were sending to was non-existent, as described
in Section 7.1.
To assess whether the different botnets stopped send-
ing emails to those addresses, we leveraged a spamtrap
under our control. A spamtrap is a set of email addresses
that do not belong to real users, and, therefore, collect
only spam mails. To evaluate our approach, we leverage
the following idea:
if an email address is successfully
removed from an email list used by a spam campaign,
we will not observe the same campaign targeting that ad-
dress again. We deﬁne as a spam campaign the set of
emails that share the same URL templates in their links,
similar to the work of Xie et al. [48]. While there are
more advanced methods to detect spam campaigns [31],
the chosen approach leads to sufﬁciently good results for
our purposes.
We ran our experiment for 73 days, from June 18 to
August 30, 2011. During this period, our mail server
replied with false server feedback for 3,632 destination
email addresses covered by our spamtrap, which were
targeted by 29 distinct spam campaigns. We call the set
of campaigns Cf and the set of email addresses Sf . Of
these, ﬁve campaigns never targeted the addresses for
which we gave erroneous feedback again. To estimate
the probability Pc that the spammer running campaign c
in Cf actually removed the addresses from his list, and
that our observation is not random, we use the following
formula:
Pc = 1 − (1 − n
tf−tb
)te−tf ,
where n is the total number of emails received by Sf
for c, tf is the time at which we ﬁrst gave a negative
feedback for an email address targeted by c, tb is the ﬁrst
email for c which we ever observed targeting our spam
trap, and te is the last email we observed for c. This
formula calculates the probability that, given a certain
number n of emails observed for a certain campaign c,
no email was sent to the email addresses in Sf after we
sent a poisoned feedback for them. We calculate Pc for
the ﬁve campaigns mentioned above. For three of them,
the conﬁdence was above 0.99. For the remaining two,
we did not observe enough emails in our spamtrap to be
able to make a ﬁnal estimate.
To assess the impact we would have had when send-
ing erroneous feedback to all the addresses in the spam-
trap, we look at how many emails the whole spamtrap
received from the campaigns in Cf . In total, 2,864,474
emails belonged to campaigns in Cf . Of these, 550,776
belonged to the three campaigns for which we are con-
ﬁdent that our technique works and reduced the amount
of spam emails received at these addresses. Surprisingly,
this accounts for 19% of the total number of emails re-
ceived, indicating that this approach could have impact
in practice.
We acknowledge that these results are preliminary and
provide only a ﬁrst insight into the large-scale applica-
tion of server feedback poisoning. Nevertheless, we are
conﬁdent that this approach is reasonable since it leads
to a lose-lose situation for the botmaster, as discussed in
Section 6. We argue that the uncertainty about server
feedback introduced by our method is beneﬁcial since it
reduces the amount of information a spammer can obtain
when sending spam.
7.4 Limitations and Evasion
Our results demonstrate that B@bel is successful in de-
tecting current spambots. However, spam detection is an
adversarial game. Thus, once B@bel is deployed, we
have to expect that spammers will evolve and try to by-
pass our systems. In this section, we discuss potential
paths for evasion.
Evading dialects detection. The most immediate path
to avoid detection by dialects is to implement an SMTP
engine that precisely follows the speciﬁcation. Alterna-
tively, a bot author could make use of an existing (open
source) SMTP engine that is used by legitimate email
clients. We argue that this has a negative impact on the
effectiveness and ﬂexibility of spamming botnets.
Many spambots are built for performance; their aim
is to distribute as many messages as possible. In some
cases, spambots even send multiple messages without
waiting for any server response. Clearly, any additional
checks and parsing of server replies incurs overhead that
might slow down the sender. We performed a simple ex-
periment to measure the speed difference between a mal-
ware program sending spam (Bagle) and a legitimate
email library on Windows (Collaboration Data
Objects - CDO). We found that Bagle can send an
email every 20 ms to a local mail server. When trying to
send emails as fast as possible using the Windows library
(in a tight loop), we measured that a single email required
200 ms, an order of magnitude longer. Thus, when bots
are forced to faithfully implement large portions of the
SMTP speciﬁcation (because otherwise, active probing
will detect differences), spammers suffer a performance
penalty.
Spammers could still decide to adopt a well-known
SMTP implementation for their bots, run a full, paral-
lelized, SMTP implementation, or revert to a well-known
SMTP library when they detect that the recipient server
is using B@bel for detection. In this case, another as-
pect of spamming botnets has to be taken into account.
Typically, cyber criminals who infect machines with bots
are not the same as the spammers who rent botnets to dis-
tribute their messages. Modern spamming botnets allow
their customers to customize the email headers to mimic
legitimate clients. In this scenario, B@bel could exploit
possible discrepancies between the email client identiﬁed
by the SMTP dialect and the one announced in the body
of an email (for example, via the X-Mailer header).
When these two dialects do not match (and the SMTP
dialect does not indicate an MTA), we can detect that
the sender pretends to speak a dialect that is inconsis-
tent with the content of the (spam) message. Of course,
the botmasters could take away the possibility for their
customers to customize the headers of their emails, and
force them to match the ones typical of a certain legiti-
mate client (e.g., Outlook Express). However, while this
would make spam detection harder for B@bel, it would
make it easier for other systems that rely on email-header
analysis, such as Botnet Judo [31], because spammers
would be less ﬂexible in the way they vary their tem-
plates.
Mitigating feedback manipulation. As we discussed
in Section 6, spammers can decide to either discard any
feedback they receive from the bots, or trust this feed-
back. To avoid this, attackers could guess whether the
receiving mail server is performing feedback manipula-
tion. For example, when all emails to a particular domain
are rejected because no recipient exists, maybe all feed-
back from this server can be discarded. In this case, we
would need to update our feedback mechanism to return
invalid feedback only in a fraction of the cases.
8 Related Work
Email spam is a well-known problem that has attracted a
substantial amount of research over the past years. In the
following, we brieﬂy discuss how our approach is related
to previous work in this area and elaborate on the novel
aspects of our proposed methods.
Spam Filtering: Existing work on spam ﬁltering can
be broadly classiﬁed in two categories: post-acceptance
methods and pre-acceptance methods. Post-acceptance
methods receive the full message and then rely on con-
tent analysis to detect spam emails. There are many ap-
proaches that allow one to differentiate between spam
and legitimate emails: popular methods include Naive
Bayes, Support Vector Machines (SVMs), or similar
methods from the ﬁeld of machine learning [16, 27, 35,
36]. Other approaches for content-based ﬁltering rely on
identifying the URLs used in spam emails [2,48]. A third
method is DomainKeys Identiﬁed Mail (DKIM), a system
that veriﬁes that an email has been sent by a certain do-
main by using cryptographic signatures [23]. In practice,
performing content analysis or computing cryptographic
checksums on every incoming email can be expensive
and might lead to high load on busy servers [41]. Fur-
thermore, an attacker might attempt to bypass the con-
tent analysis system by crafting spam messages in spe-
ciﬁc ways [25, 28].
In general, the drawback of post-
acceptance methods is that an email has to be received
before it can be analyzed.
Pre-acceptance methods attempt to detect spam before
actually receiving the full message. Some analysis tech-
niques take the origin of an email into account and an-
alyze distinctive features about the sender of an email
(e.g., the IP address or autonomous system the email
is sent from, or the geographical distance between the
sender and the receiver) [17,34,39,43]. In practice, these