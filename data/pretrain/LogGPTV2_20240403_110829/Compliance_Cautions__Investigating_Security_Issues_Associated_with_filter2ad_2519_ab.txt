TABLE I: Researcher and expert demographics
III. METHOD
In the ﬁrst step of this study, our researchers comprehen-
sively audited three compliance standards to identify potential
security concerns. To validate these concerns, we then recruited
four experts to provide their assessment of our ﬁndings.
We performed quantitative and qualitative analysis on expert
responses to identify discrepancies and also derive additional
context for applicability within enterprise environments.
This study occurred from October 2017 through September
2018 and was ruled not human subjects research by our ethics-
compliance ofﬁce, as we communicated with experts in their
professional capacity and did not collect personally identiﬁable
information. Due to the sensitive nature of unmitigated data
vulnerabilities within real environments, we generalize many
of our ﬁndings to protect networks and systems.
A. Compliance-standard audit
Our team of six researchers designed the audit to sys-
tematically evaluate three unrelated compliance standards in
a repeatable manner. Each researcher audited a subset of the
standards, with at
three researchers per standard (as
shown in Table I). Our objective was to identify issues that
might negatively affect digital security, including policies that
expose sensitive information and processes that create issues
due to ambiguous implementation guidance.
least
First, all six researchers conducted a complete audit of IRS
Publication 1075, following a content-analysis process drawn
from social-science research. Each researcher independently
examined each line of the standard. At each of several pre-
determined milestones within the document (e.g., the end of
a section), the researcher would log their ﬁndings, including
the section title where the issue was found, the exact phrase
deemed problematic, a short description of the perceived issue,
and references to related, publicly known issues. If a researcher
found multiple issues within one phrase or section, they logged
each separately. For every logged issue, all other researchers
would indicate (1) if they found the same issue independently
and (2) whether they concurred with the ﬁnding. If there was
not unanimous consensus on an issue, we discarded it but
maintained a record of the disagreement.
3
We then calculated the inter-coder reliability — a measure
of consistency among independent auditors — for indepen-
dently discovering issues in IRS P1075. We calculated our
Krippendorff’s Alpha (α), which accounts for chance agree-
ments [21]. We obtained reliability α = 0.815 for P1075;
an α value above 0.8 indicates high reliability [33], [34].
Having developed a reliable auditing process, we divided into
subgroups to parallelize the remaining effort. Four researchers
audited NERC CIP 007-6 and three researchers audited PCI
DSS. One researcher (R1) audited all three guidelines. The
subgroups attained α = 0.801 and 0.797 respectively.
We further analyzed the identiﬁed issues using iterative
open coding, a process for creating and applying category
labels (known as a codebook) to data [53]. In particular, the
researchers who audited each standard coded each identiﬁed
issue in that standard for perceived root cause, probability of
occurrence, and severity. We resolved all disagreements among
coders and developed a stable codebook by establishing a
unanimously agreed-upon deﬁnition for coded terms, adapting
many terms from the Composite Risk Management (CRM)
framework [61] and the Information System Risk-based As-
sessment framework [15].
Our ﬁnal codebook described four root causes for security
concerns. A data vulnerability is an issue that will result in
a data breach or compromise of sensitive information. An
unenforceable security control cannot be enforced as writ-
ten; these controls should be reworded or removed from the
compliance standard. An under-deﬁned process is an issue
explicitly missing instructions or details that are required for a
secure implementation, resulting in security gaps. An ambigu-
ous speciﬁcation, in contrast, is vague or ambiguous about
some implementation details, such that different readers could
interpret it differently. Some interpretations could potentially
result in either an inappropriate action or inaction. Throughout
Sections IV-B, V-B, and VI-B, we describe our audit ﬁndings
using these root causes.
We used the following terms and deﬁnitions for probability:
frequent occurs often and is continuously experienced; likely
occurs several times; occasional occurs sporadically; seldom
is unlikely, but could occur at some time; and unlikely we
assume it will not occur. We used the following terms for
severity: catastrophic results in complete system loss, full
data breach, or the corruption of all data; critical results in
major system damage, signiﬁcant data breach, or corruption
of sensitive data; moderate results in minor system damage
or partial data breach; and negligible results in minor system
impairment. Using a risk assessment matrix adopted from the
CRM framework (Figure 1), we then calculated each issue’s
risk level — a function of probability and severity — as
extremely high, high, moderate, or low [61].
that empirical
Best practices suggest
research should
be conducted by personnel with extensive domain knowl-
edge [47]. Accordingly, the auditing researchers possess an
average of 14.3 years of digital security experience within
academia,
the federal government, and industry. Each re-
searcher grounded their audit ﬁndings in their past digital
security experiences. Additional information about the data set
is in Appendix A.
goals. Table I shows the qualiﬁcations of our four volun-
teer experts. Experts completed their surveys during regularly
scheduled work hours and did not receive any additional
monetary incentives for participating.
Issue selection. We note that an essential tenet for partnering
with experts is minimizing disruption to their daily responsi-
bilities. Research suggests that the quality of survey responses
decreases over time, and excessive time away from work
may result in an expert terminating their participation in the
study [25]. To this end, we designed our surveys for experts
to complete within 60-90 minutes of focused effort; actual
completion time averaged 84.8 minutes. Given our limited
pool of experts, this required us to select only a subset of our
ﬁndings to validate; as described in detail below, we selected
the issues to validate semi-randomly, while prioritizing the
extremely-high-risk and high-risk issues.
Pilot. Prior to deploying our protocol with partnering organiza-
tions, we piloted surveys to pre-test relevance and clarity with
security practitioners familiar with auditing and compliance
standards. We updated the study protocol based on pilot
feedback. After two iterations, we arrived at
the ﬁnalized
questionnaire in Appendix B. Our two pilot experts currently
conduct digital-security penetration testing against organiza-
tions, providing technical remediation recommendations for
discovered security concerns.
C. Limitations
Our recruitment letter and consent waiver explained the
purpose of the study. Thus, there may be self-selection bias in
which personnel most interested in the study were more likely
to anonymously participate. However, this may also suggest
that our experts were prepared to think more critically about
reported issues.
All of our experts work directly in compliance and their
intimate working knowledge with compliance standards re-
duces the possibility of demand characteristics — a condition
in which participants unconsciously change their behavior to
perform well within a study [44]. Our study questions the
validity of the compliance standards that serve as the basis
for the experts’ employment. This suggests that the experts
would be in many cases predisposed to underestimate problems
within these standards. Additionally, our validation method
does not elicit expert feedback for false negatives – issues
that our original analysis may not have detected. As such, we
consider expert responses to provide a lower bound for validity.
The organizations we partnered with for this study have
similar structures, missions, and technologies to other organi-
zations that adhere to our selected compliance standards; how-
ever, there may exist speciﬁc organizational characteristics that
affect their speciﬁc implementations or inhibit generalizability.
As such, validating the presence of our discovered security
concerns within partnered organizations’ environments does
not mean that all organizations adhering to similar compliance
standards have security concerns, and the rejection of one of
our ﬁndings does not imply that another organization elsewhere
does not have security concerns. Nonetheless, our results can
indicate systemic issues that organizations need to account for
when assessing their levels of digital security risk and provide
Fig. 1: Security concern risk levels. Levels were assigned based
on a Composite Risk Management risk-assessment matrix that
includes both probability of occurrence and impact severity.
B. Expert validation process
To obtain external validation of our ﬁndings, we established
partnerships with real-world organizations and compliance
subject-matter experts to conﬁrm or reject our ﬁndings. We
asked the experts to classify our identiﬁed issues in one of
three categories: conﬁrmed, plausible, or rejected. A conﬁrmed
issue indicates that the expert has previously observed security
concerns associated with the issue or that observable con-
sequences from the issue actively exist within an enterprise
environment. A plausible issue occurs when the expert has
not personally observed security concerns related to the issue
but agrees such security concerns could manifest within other
organizations. A rejected ﬁnding indicates that there is no
observable evidence of security concerns related to the issue
within a live environment, or that there are related security
factors that we had not considered.
We used a series of closed- and open-ended survey ques-
tions to elicit information from each expert (detailed in Ap-
pendix B). In addition to directly validating or rejecting each
issue, the experts were asked to provide additional context
from their personal experience. We presented the issues to
the experts in a randomized order, providing the referenced
section title, exact text from the section, and a short narrative
describing the perceived issue.
After collecting data from each expert and removing re-
jected ﬁndings, we used the Wilcoxon signed-rank test to
compare researchers’ assessment of probability and severity
with our experts’ responses for PCI DSS and NERC CIP
007-6; we used the Friedman test (omnibus) with planned
pairwise Wilcoxon signed-rank tests for comparing IRS P1075
responses, for which we had two expert validators [63], [14].
We also conducted open-ended discussions with the experts to
discuss similarities and differences in assessments.
Partner criteria. We established the following criteria for part-
nering with organizations: (1) the organization must regularly
be subjected to audits, must regularly audit other organizations,
or must contribute to the content of the relevant compliance
standard, (2) the provided validators must have at least two
years of experience with the relevant standard, and (3) the
organization must be able to mediate responsible disclosure of
our ﬁndings.
After months of negotiation, we established memorandums
of understanding with three organizations that met our criteria.
Leaders within each organization nominated several compli-
ance experts; we sent each candidate an email outlining the
voluntary nature of the study as well as our motivation and
4
entities possessing FTI. Of the three standards we assessed,
P1075 has the weakest sanctions. There are no provisions for
the issuance of ﬁnes for insecure practices, and the strictest
sanction available to inspectors is data revocation after failure
to adhere to a prescribed corrective action plan. However, non-
compliant organizations can apply for data revocation waivers
that extend their access to FTI for six months; according to the
standard as written, this process can continue indeﬁnitely de-
spite continued non-compliance. This process has the potential
to minimize the impact of sanctions while allowing insecure
practices to persist. Overall, IRS P1075 was qualitatively and
quantitatively the weakest of three documents we assessed
during this study.
B. Findings
Our audit of P1075 identiﬁed a total of 81 independent
issues across 309 individual security controls (Figure 2). Of
these, we agreed that two issues presented an “Extremely
High” risk, whereas 13 were “High,” 32 were “Moderate”
and 34 were “Low” risk according to the Risk Assessment
Matrix (Figure 1). In addition, we discarded 15 initially iden-
tiﬁed issues, including 11 discarded when researchers found
implementation details that were clariﬁed in later sections of
the standard and four resulting from researcher disagreements.
All four issue disagreements related to nuanced interpretations
of ambiguous portions of the standard.
Security concern trends. We identiﬁed ﬁve issues involving
portable devices (e.g., mobile phones and laptops) and seven
involving cloud-based data storage solutions. We associate the
prevalence of these issues with shifts toward bring-your-own-
device regimes and an increased reliance on cloud-storage
solutions over on-premises servers [17]. These emerging solu-
tions require specialized security measures and create inconsis-
tencies with the best security practices that professionals have
developed over the past few decades [54].
Of the 81 issues we identiﬁed within P1075, Section 9 had
40 technical controls with security concerns. Of note, Section
9 has several obsolete controls such as password expiration
period requirements (which is shown to encourage insecure
practices such as writing newly rotated passwords near user
workstations) [20], [55]. In this particular instance, the IRS
mandated organizations to make a worse security decision than
the decision they might have made in the absence of P1075.
Below we present detailed examples of ﬁndings based on their
associated root cause.
Data vulnerability. We identiﬁed 37 issues that would estab-
lish conditions for a data breach if controls and processes are
implemented as described in the publication. One example in
Section 9.3.6.8 outlines processes for restoring backups once a
compromise in a live system has occurred. As written, P1075
does not require technicians to verify the integrity of backups
before restoration, meaning that technicians could revert to
a state that an attacker has already infected (giving them
persistent access) or revert to a vulnerable state that an attacker
could re-exploit, reestablishing access to sensitive data [49].
Real-world trends stemming from ransomware support
the
urgency of backup integrity checks [50]. We assessed this high-
risk issue to have a likely probability and moderate severity.
Fig. 2: Distribution of security concerns identiﬁed for IRS
P1075. Color indicates the type of security concern; each dot
indicates by size how many security concerns were identiﬁed
with a given type, severity, and probability. Data vulnerabilities
were most common (n=37).
novel insights into the impact of compliance standards on
digital security in enterprise environments.
Lastly, we audited each compliance standard without con-
sidering other security controls in complementary documents.
For this study, we assume that organizations implement com-
pliance standards perfectly and limit our scope to ﬁnding
security concerns in the documents as written.
IV. RESULTS: IRS P1075
A. Overview
IRS Publication 1075 provides mechanisms for protecting
and controlling access to federal tax information. IRS P1075
applies to all U.S. federal, state, and local agencies that receive
Federal Tax Information (FTI) from the IRS or from secondary
sources like the Social Security Administration [28]. Of the
three standards we assessed, IRS P1075 is the longest standing,
dating back to 1996 [27]. We audited the 2016 revision, which
was the most current version available at the time of this study.
taxpayer.
Organizations such as the Ofﬁce of Child Support Enforcement
from the U.S. Department of Health and Human Services rely
upon IRS P1075 for securing the networked infrastructure of
child support ﬁnancial records [60]. Companies such as Ama-
zon offer cloud infrastructure services that are fully compliant
with P1075, marketing their virtual private server services to
customers who need a “turn-key” solution for systems that
transmit or receive FTI [3].
FTI security potentially affects every federal
P1075 is written for information technology security pro-
fessionals responsible for securing FTI. Key provisions include
deﬁnitions for terms, parties authorized to access FTI, record-
keeping requirements, physical controls for securing data,
technical controls for secure storage/transmission, inspection
protocols, and sanctions for non-compliance. The IRS Ofﬁce
of Safeguards coordinates and conducts compliance audits of
5
Section 9.3.5.11 includes provisions for user-installed ap-
plications. Environments that store or transmit FTI should be
highly secure and should only be used for FTI — other func-
tions and services should occur outside the FTI environment.