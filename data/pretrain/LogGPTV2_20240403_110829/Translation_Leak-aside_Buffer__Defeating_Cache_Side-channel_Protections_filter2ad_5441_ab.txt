smallest w generating evictions is 4, and the smallest cor-
Figure 1: How TLBleed observes a sibling hyperthread’s
activity through the TLB even when shared caches are
partitioned.
from the attacker with cache partitioning, TLBleed can
still leak information through the shared TLB.
Mounting TLBleed on real-world settings comes with
a number of challenges and open questions. The ﬁrst set
of challenges come from the fact that the architecture of
the TLB is mostly secret. Mounting successful TLBleed,
however, requires detailed knowledge of the TLB archi-
tecture. More speciﬁcally, we need to answer two ques-
tions:
Q1 How can we monitor TLB sets? More speciﬁcally,
how do virtual addresses map to multi-level TLBs
found in modern processors?
Q2 How do sibling hyperthreads share the TLB sets for
translating their code and data addresses?
Once the attacker knows how to access the same TLB
set as a victim, the question is whether she has the ability
to observe the victim’s activity:
Q3 How can an unprivileged process (without access to
performance counters, TLB shootdown interrupts,
etc.) monitor TLB activity reliably?
Finally, once the attacker can reliably measure the
TLB activity of the victim, the question is whether she
can exploit this new channel for attractive targets:
Q4 Can the attacker use the limited granularity of 4 kB
“data” pages to mount a meaningful attack? And
how will existing defenses such as ASLR compli-
cate the attack?
We address these challenges in the following sections.
958    27th USENIX Security Symposium
USENIX Association
Victim HyperThreadAttacker HyperThreadTLBCacheCoreFigure 2: Linearly-mapped TLB probing on Intel Broad-
well, evidencing a 4-way, 16-set L1 dTLB.
responding s is 16—correctly probing for a 4-way 16-set
L1 dTLB on Broadwell.
Complex-mapped TLB If our results prove inconsis-
tent with the linear mapping hypothesis, we must reverse
engineer a more complex hash function to collect evic-
tion sets (Q1). This is, for instance, the case for the L2
sTLB (L2 shared TLB) on our Skylake machine. Re-
verse engineering this function is analogous to identi-
fying its counterpart for CPU caches, which decides to
which cache set a physical address maps [26, 60]. Thus,
we assume that the TLB set number can be expressed as
an XOR of a subset of bits of the virtual address, similar
to the physical hash function for CPU caches.
To reverse engineer the hash, we ﬁrst collect minimal
eviction sets, following the procedure from [42]. From
a large pool of virtual addresses, this procedure gives us
minimal sets that each map to a single hash set. Sec-
ond, we observe that every address from the same evic-
tion set must map to the same hash set via the hash func-
tion, which we hypothesized to be a XOR of various bits
from the virtual address. For each eviction set and ad-
dress, this gives us many constraints that must hold. By
calculating all possible subsets of XOR-ed bit positions
that might make up this function, we arrive at a unique
solution. For instance, Figure 5 shows the hash function
for Skylake’s L2 sTLB. We refer to it as XOR-7, as it
XORs 7 consecutive virtual address bits to ﬁnd the TLB
set.
Table 1 summarizes the TLB properties that our re-
verse engineering methodology identiﬁed. As shown in
the table, most TLB levels/types on recent Intel microar-
chitectures use linear mappings, but the L2 sTLB on Sky-
lake and Broadwell are exceptions with complex, XOR-
based hash functions.
Figure 3: Skylake TLBs are not inclusive.
Interaction Between TLB Caches One of the central
cache properties is inclusivity.
If caches are inclusive,
lower levels are guaranteed to be subsets of higher levels.
If caches are not inclusive, cached items are guaranteed
to be in at most one of the layers. To establish this prop-
erty for TLBs, we conduct the following experiment:
1. Assemble a working set S1 that occupies part of a
L1 TLB, and then the L2 TLB, until it is eventually
too large for the L2 TLB. The pages should target
only one particular L1 TLB (i.e., code or data).
2. Assemble a working set S2 of constant size that tar-
gets the other L1 TLB.
3. We access working sets S1+S2. We gradually grow
S1 but not S2. We observe whether we see L1
misses of either type, and also whether we observe
L2 misses.
4. If caches are inclusive, L2 evictions of one type will
cause L1 evictions of the opposite type.
The result of our experiment is in Figure 3. We con-
clude TLBs on Skylake are not inclusive, as neither type
of page can evict the other type from L1. This implic-
itly means that attacks that require L1 TLB evictions are
challenging in absence of L1 TLB sharing, similar, in
spirit, to the challenges faced by cache attacks in non-
inclusive caching architectures [18].
With this analysis, we have addressed Q1. We now
have a sufﬁcient understanding of TLB internals on com-
modity platforms to proceed with our attack.
6 Cross-hyperthread TLB Monitoring
To verify the reverse engineered TLB partitions, and to
determine how hyperthreads are exposed to each others’
activity (addressing Q2), we run the following experi-
ment for each TLB level/type:
1. Collect an eviction set that perfectly ﬁlls a TLB set.
USENIX Association
27th USENIX Security Symposium    959
01020304050607080sets0.02.55.07.510.012.515.017.520.0ways05101520TLB working set (data pages)05101520Missesitlb utilizationdata pagewalks05101520TLB working set (instruction pages)05101520Missesdtlb utilizationinstruction pagewalksTable 1: TLB properties per Intel microarchitecture as found by our reverse engineering methodology. hsh = hash
function. w = number of ways. pn = miss penalty in cycles, shr indicates whether the TLB is shared between threads.
Name
Sandybridge
Ivybridge
Haswell
HaswellXeon
Skylake
BroadwellXeon
Coffeelake
year
2011
2012
2013
2014
2015
2016
2017
set w
16
4
4
16
4
16
4
16
16
4
4
16
16
4
L1 dTLB
L1 iTLB
pn
7.0
7.1
8.0
7.9
9.0
8.0
9.1
hsh
lin
lin
lin
lin
lin
lin
lin
shr
 16
 16
8

8

8

8

8

set w
4
4
8
8
8
8
8
pn
50.0
49.4
27.4
28.5
2.0
18.2
26.3
hsh
lin
lin
lin
lin
lin
lin
lin
shr







set
128
128
128
128
128
256
128
w
4
4
8
8
12
6
12
L2 sTLB
hsh
pn
lin
16.3
lin
18.0
lin
17.1
lin
16.8
212.0 XOR-7
272.4 XOR-8
230.3 XOR-7
shr







2. For each pair of eviction sets, access one set on
one hyperthread and the other set on another hyper-
thread running on the same core.
non-shared L1 iTLB triggering L1 evictions is challeng-
ing, as discussed earlier. This leaves us with data attacks
on the L1 dTLB or L2 sTLB.
3. Measure the observed evictions
to determine
whether one given set interferes with the other set.
Figure 4 presents our results for Intel Skylake, with a
heatmap depicting the number of evictions for each pair
of TLB (and corresponding eviction) sets. The lighter
colors indicate a higher number of TLB miss events in
the performance counters, and so imply that the corre-
sponding set was evicted. A diagonal in the heatmap
shows interference between the hyperthreads. If thread 1
accesses a set and thread 2 accesses the same set, they
interfere and increase the miss rate. The signals in the
ﬁgure conﬁrm our reverse engineering methodology was
able to correctly identify the TLB sets for our Skylake
testbed microarchitecture. Moreover, as shown in the ﬁg-
ure, only the L1 dTLB and the L2 sTLB show a clear
interference between matching pairs of sets, demonstrat-
ing that such TLB levels/types are shared between hyper-
threads while the L1 iTLB does not appear to be shared.
The signal on the diagonal in the L1 dTLB shows that a
given set is shared with the exact same set on the other
hyperthread. The signal on the diagonal in the L2 sTLB
shows that sets are shared but with a 64-entry offset—the
highest set number bit is XORred with the hyperthread
ID when computing the set number. The spurious signals
in the L1 dTLB and L1 iTLB charts are sets represent-
ing data and code needed by the instrumentation and do
not reﬂect sharing between threads. This conﬁrms state-
ments in [11] that, since the Nehalem microarchitecture,
“L1 iTLB page entries are statically allocated between
two logical processors’, and “DTLB0 and STLB” are a
“competitively-shared resource.” We veriﬁed that our re-
sults also extend to all other microarchitectures we con-
sidered (see Table 1).
With this analysis we have addressed Q2. We can now
use the L1 dTLB and the L2 sTLB (but not the L1 iTLB)
for our attack. In addition, we cannot easily use the L2
sTLB for code attacks, as with non-inclusive TLBs and
7 Unprivileged TLB Monitoring
While performance counters can conveniently be used
to reverse engineer the properties of the TLB, accessing
them requires superuser access to the system by default
on modern Linux distributions, which is incompatible
with our unprivileged attacker model. To address Q3,
we now look at how an attacker can monitor the TLB
activity of a victim without any special privilege by just
timing memory accesses.
We use the code in Figure 6, designed to monitor a
4-way TLB set, to exemplify our approach. As shown
in the ﬁgure, the code simply measures the latency when
accessing the target eviction set. This is similar, in spirit,
to the PROBE phase of a classic PRIME+PROBE cache
attack [42, 43, 45], which, after priming the cache, times
the access to a cache eviction set to detect accesses of
the victim to the corresponding cache set. In our TLB-
based attack setting, a higher eviction set access latency
indicates a likely TLB lookup performed by the victim
on the corresponding TLB set.
To implement an efﬁcient monitor, we time the ac-
cesses using the rdtsc and rdtscp instructions and se-
rialize each memory access with the previous one. This
is to ensure the latency is not hidden by parallelism, as
each load is dependent on the previous one, a technique
also seen in [43] and other previous efforts. This pointer
chasing strategy allows us to access a full eviction set
without requiring full serialization after every load. The
lfence instructions on either side make it unnecessary
to do a full pipeline ﬂush with the cpuid instruction,
which makes the operation faster.
With knowledge of the TLB structure, we can design
an experiment that will tell us whether the latency reli-
ably indicates a TLB hit or miss or not. We proceed as
follows:
960    27th USENIX Security Symposium
USENIX Association
Figure 4: Interaction of TLB sets between hyperthreads on Intel Skylake. This shows that the L1 dTLB and the L2
sTLB are shared between hyperthreads, whereas this does not seem to be the case for the L1 iTLB.
1 0 0
0 1 0
0 0 1
0 0 0
0 0 0
0 0 0
0 0 0
H =
0 0
0 0
0 0
1 0
0 1
0 0
0 0
0
0
0
0
0
1
0
0 1 0
0 0 1
0 0 0
0 0 0
0 0 0
0 0 0
1 0 0
0 0
0 0