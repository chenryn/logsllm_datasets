servers that were responsive to our probes. We collected 374,334
distinct IPs in this manner, of which 273,541 (74%) were respon-
sive to DNS or ping, and we tracked them in our active measure-
ment study described in Section 3.2.
We ﬁnd it interesting that only 35% responded both to DNS and
ping, while 21% responded only to ping and 20% responded only
to DNS. We did not notice a difference in the relative load distri-
bution among the servers that were responsive and those that were
not (when looking at the LDNS servers that we also had load in-
formation for in ldns-load). However, we note that our sam-
pling method is biased in favor of LDNS servers that make a large
number of requests. In Section 4.2.2, we ﬁnd that there exists a
slight positive correlation between relative load and availability, so
we may slightly overestimate the fraction of samples with higher
availability when we use our samples to draw conclusions about
availability properties.
3.1.2 ADNS Servers
We obtained two ADNS sample sets: adns-all and adns-
web.
To obtain adns-all, we performed a “reverse-crawl” of the
.in-addr.arpa domain, which reverse maps IP addresses to
domain names. We use a methodology similar to the ISC Inter-
net Survey [16]: ﬁrst, we look up the nameservers responsible for
each of f0,...,255g.in-addr.arpa. For each domain in
which we discover an ADNS, we recursively look up its “children”
(e.g., to recurse on 128.in-addr.arpa, we examined each of
f0,...,255g.128.in-addr.arpa), etc.
We note that we only performed a single sequence of lookups for
a given domain, so if we did not receive a response, we missed the
entire address block that made up its subtree (we received success-
ful lookups for 2,711,632 domains). In addition, some domains in
.in-addr.arpa may not map to an ADNS server, even though
there exists one responsible for that domain (perhaps due to con-
ﬁguration errors). Hence, our sample is probably an underestimate
of the number of ADNS servers in operation, though it is sufﬁcient
for the purposes of our study.
We found 87,111 distinct IPs this way, of which 68,155 were
responsive to DNS (and possibly ping) in mid-April 2004. We used
these 68,155 in our active probing measurements (see Section 3.2).
We call this smaller set adns-probed.
To get adns-web, we obtained one week of Web cache logs
(May 3 to May 9, 2004) from the NLANR IRCache project [15]1,
which maintains a set of top-level Squid caches serving various re-
gions in the U.S. From this data set, we obtained a set of 85,719
ADNS servers responsible for the websites accessed by clients in
the trace. 20,086 of these servers were also in adns-probed. We
estimated the relative load on these servers by summing the num-
1The NLANR IRCache project is supported by National Science
Foundation (grants NCR-9616602 and NCR-9521745), and the Na-
tional Laboratory for Applied Network Research.
ber of HTTP requests in the Web cache trace made to websites for
which each ADNS server is responsible.2 Although the actual load
will be impacted by factors such as the TTL of the DNS records re-
turned to clients, we hypothesize that this gives us an estimate good
enough to make the correlations in our analysis (e.g., between load
and availability). We call this data set adns-load.
3.2 Active Availability Measurements
After obtaining the ldns-probed and adns-probed sam-
ples described above, we began active probes to measure their avail-
ability characteristics. For LDNS servers, we began the measure-
ments immediately after we veriﬁed that they were responsive.3 For
ADNS servers, we began all measurements at the same time. We
tracked each DNS server for approximately two weeks, as follows.
During the experiment we sent DNS and ICMP ping probes to
each DNS server we tracked, with an exponentially distributed in-
terval between probes, with a mean of 1 hour. Probes were origi-
nated from a well-connected machine at Carnegie Mellon Univer-
sity and were made by a custom event-driven application for efﬁ-
ciency. DNS probes to both LDNS and ADNS servers consisted of
a query for the A record of A.ROOT-SERVERS.NET, and each
probe was tried 3 times over a period of 30 seconds before we
marked it as failed. We probed a given DNS server with whatever
queries to which it was originally responsive (e.g., if it originally
did not respond to ping we only used the DNS query). Although
we could also have used TCP RST packets to track servers, due to
the volume of servers we planned to track, we decided to use less
invasive methods that were unlikely to trigger ﬁrewall alarms, etc.
We use exponentially distributed sampling intervals for reasons
explained by Paxson [27]. Such sampling is unbiased because it
samples all instantaneous signal values with equal probability. In
addition, it obeys the “PASTA” principal, which says that the pro-
portion of our measurements that observe a given state is (asymp-
totically) equal to the amount of time spent in that state [36]. To
verify that our sample granularity was ﬁne enough to apply this
property to our limited measurement interval, we performed active
probes to a random subset of 900 LDNS and 900 ADNS servers at
a much higher ﬁxed-rate of 1 probe per 5 minutes for 3 days and
obtained nearly identical availability distributions to those obtained
from our experiments with larger intervals.4
The most signiﬁcant drawback in our measurement setup is that
we are only observing the DNS servers from a single vantage point,
so our probes will observe network failures along the path from our
probe machine to the target server site as well. However, we take
steps to reduce the impact of these, as described in the following
section. There is trade-off between logistical difﬁculty and sample
size (which is proportional to resource expenditure) when setting
up a measurement study at multiple sites (e.g., synchronization,
administrative policies, maintenance); we opted for a much larger
sample size in order to observe the peculiarities of DNS servers.
We call the data set for LDNS and ADNS servers obtained from
these experiments ldns-avail and adns-avail respectively.
3.3 Data Reduction
We took several steps to ﬁlter the data to reduce the impact of
In addition, we reduced
network failures on our measurements.
2If there were multiple ADNS servers authoritative for a website,
we split the load evenly between the servers.
3This allowed us to estimate the impact of network failures on our
measurements by correlating probes with accesses at Akamai, as
described at the end of Section 3.3.
4These measurements were performed between July 27 and July
30, 2004.
our LDNS sample set because of the presence of dynamic IPs ad-
dresses.
3.3.1 Network Failures
First, we tried to identify periods during which network failures
occurred close to our probing site or in the “middle” of the net-
work, hence affecting a large number of measurements. To do this,
we examined our aggregate probing log (containing the probe re-
sults to all sites combined) and removed all probes that fell within a
30-second window in which the fraction of failed probes exceeded
3 standard deviations of the failure rate of the preceding hour. This
removed periods where a larger than normal number of probes ex-
perience correlated failures, which could indicate signiﬁcant net-
work failures in the middle of the network or close to our probing
site. This removed 5.1 hours of data from our LDNS experiment
(of 560 total) and 7.8 hours from our ADNS experiment (of 388 to-
tal), the longest period of which was about 1.5 hours, during which
CMU had a conﬁrmed network outage.
Next, we clustered IPs according to autonomous systems (ASs)
and network aware clusters (NACs) [19]. A NAC is the set of IP
addresses sharing the longest common routing preﬁx advertised via
BGP (which are derived using BGP routing table snapshots from
several vantage points, such as RouteViews [2]). Hence, the IP
addresses in a NAC are likely to have access links in common and
are likely to be operated under the same administrative domain.
We examined our combined traces for ASs with a particularly
high rate of correlated failures by looking for related probe pairs
— that is, closely spaced (within 2 minutes) probe pairs to servers
within the same AS but within different NACs.5 We proceeded as
follows: First, we counted the number of related probe pairs that
observed at least one failure — the total number of failure pairs.
Second, we counted the number of related probe pairs that were
both failures — the number of correlated pairs. The ratio of cor-
related failure pairs to total failure pairs gave us an estimate of the
number of correlated failures in a particular AS. In the LDNS sam-
ple set we found that 70 ASes were responsible for abnormally high
ratios (above 0.3), so we eliminated the 800 sites in these ASes.
In the majority of the other ASes (in which we observed closely
spaced failures), there were 0 correlated failures.
We note that we did not expect these steps to completely elim-
inate network failure observations in our data, but merely to limit
their impact on our analysis. To estimate how well our ﬁltering
heuristics worked, we checked if any of the LDNS servers gen-
erated requests to Akamai within 1 minute of an observed failure
during the one week for which log collection and active probing
overlapped. Limiting ourselves to the 6,000 servers that generated
the most requests to Akamai (since each of these servers had an av-
erage request rate to Akamai greater than 1.5 per minute), we found
that about 15% of the failed probes during this period were within
1 minute of an observed request from the corresponding LDNS.
Hence, if we excluded all network outages from our deﬁnition of
availability, then our measurements would underestimate the actual
availability of nameservers. Nonetheless, we believe our measure-
ments would still be correct to within an order of magnitude. In
our analysis of this data, which might still be sensitive to network
failure observations (such as in Section 4.3), we take further pre-
cautions.
3.3.2 Dynamic IP Addresses
For the sample of LDNS servers, we performed one ﬁnal ﬁltering
step before proceeding to measurement analysis. We discovered
5We count network failure within a NAC as an actual failure since
service is unlikely to be available to anyone outside the NAC.
that in a fair number of cases, the LDNS servers we sampled used
dynamic IP addresses (e.g., DHCP). Bhagwan et al. [5] found that
the aliasing effects introduced by dynamic IPs result in signiﬁcant
underestimates in availability when measuring peer-to-peer clients,
and we have observed the same effect with LDNS servers. In par-
ticular, when examining a database of 300,878 class C address
blocks known to be dynamic IP pools obtained from a spam Real-
time Black Hole List (RBL) [1], we found that 17,163 (6%) LDNS
servers in ldns-probed were classiﬁed as dynamic. Moreover,
27,237 of the domain names obtained by reverse mapping LDNS
IPs in ldns-probed contained a string like dsl or dialup (in
the host portion), suggesting that they are behind DSL, cable, or
dial-up links (though this does not necessarily imply they are dy-
namic). Because identifying dynamic IP addresses is difﬁcult and
to our knowledge there is no passive technique with any reasonable
degree of accuracy, we choose to be conservative and only analyze
LDNS servers that we were reasonably conﬁdent were not using
dynamic IPs. We used the following three heuristics to do this clas-
siﬁcation. We keep an LDNS IP if it satisﬁed at least one of the
heuristics:
1. If the domain name obtained by reverse mapping the IP con-
tained the string dns or ns in an obvious fashion in the host
part of the domain name, it is very likely a DNS server with
a ﬁxed address, so we keep it (local name servers usually
require static addresses because clients must locate them in
order to perform name resolution to begin with).
2. For the IPs that we were able to reverse map to domain names,
we also reverse mapped the IP just above and just below
it (e.g., for 128.2.0.10, we reverse mapped 128.2.0.9 and
128.2.0.11). We deﬁne the difference between two host names
as the ratio of the number of different characters and the to-
tal number of characters, treating consecutive numeric char-
acters (i.e. numbers) as single characters. If the difference
between the host portion of the IP’s domain name and the
domain name just above and just below it was greater than
25%, then we keep it. Dynamic IPs are almost always from a
block of addresses to which administrators assign very sim-
ilar names (usually only alternating some digit for each ad-
dress in the block). For example, when examining one IP in
each of the 233,413 distinct class C address blocks that were
in the spam RBL’s list of dynamic IP pools and that reversed-
mapped to a name, at least 98% were detected as dynamic
using this heuristic (some of the remaining 2% appeared to
be static IPs that just happened to fall within a class C net-
work partially assigned to a dynamic IP pool; e.g., some had
recognizable names like mail or ns).
3. Finally, we examined the actual sequence of probe responses
from each LDNS server. If they were responsive to DNS and
ping, then we know when one fails and the other succeeds.
We hypothesize that if an LDNS server was using a dynamic
IP, gave up the IP, and the IP was reused by another client
in the pool, it is unlikely that the new client would happen
to also be running a DNS server since client machines are
rarely conﬁgured to serve DNS. Hence, for the servers that
were responsive to both DNS and ping, we keep them if their
DNS and ping response patterns were consistent during the
entire period of the trace. In this case, even if the host was
using a dynamic IP address, it is unlikely to have given it up
during our measurement period.
We call this conservative estimate of “non-dynamic” LDNS servers
ldns-nondynamic. This is the sample set that we use for the
x
<
d
a
o
l
h
t
i
w
s
r
e
v
r
e
s
.
c
a
r
F
x
<
d
a
o
l
h
t
i
w
s
r
e
v
r
e
s
.
c
a
r
F
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
(a)
(b)
 1
 100
 10000
Relative Load
LDNS
 1e+06
 1e+08
 1
 100
 10000
Relative Load
ADNS
 1e+06
 1e+08
s
q
e
r
x
<
h
t
i
w
s
r
e
v
r
e
s
n
o
d
a
o
l
.
c
a
r
F
s
q
e
r
x
<
h
t
i
w
s
r
e
v
r
e
s
n
o
d
a
o
l
.
c
a
r
F
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
 1
 0.9