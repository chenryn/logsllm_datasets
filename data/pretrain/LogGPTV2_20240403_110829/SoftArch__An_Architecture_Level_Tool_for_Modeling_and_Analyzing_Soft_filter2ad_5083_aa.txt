title:SoftArch: An Architecture Level Tool for Modeling and Analyzing Soft
Errors
author:Xiaodong Li and
Sarita V. Adve and
Pradip Bose and
Jude A. Rivers
SoftArch: An Architecture-Level Tool for Modeling and Analyzing Soft Errors ∗
Xiaodong Li
†‡1
, Sarita V. Adve
‡
†
, Pradip Bose
, Jude A. Rivers
†
†
IBM T. J. Watson Research Center
{xjli, jarivers, pbose}@us.ibm.com
Yorktown Heights, NY 10598
Abstract
Soft errors are a growing concern for processor reliabil-
ity. Recent work has motivated architecture-level studies of
soft errors since the architecture can mask many raw errors
and architectural solutions can exploit workload knowl-
edge. This paper proposes a model and tool, called Soft-
Arch, to enable analysis of soft errors at the architecture-
level in modern processors. SoftArch is based on a proba-
bilistic model of the error generation and propagation pro-
cess in a processor. Compared to prior architecture-level
tools, SoftArch is more comprehensive or faster. We demon-
strate the use of SoftArch for an out-of-order superscalar
processor running SPEC2000 benchmarks. Our results are
consistent with, but more comprehensive than, prior work,
and motivate selective and dynamic architecture-level soft
error protection mechanisms.
1 Introduction
CMOS technology scaling has brought tremendous im-
provement in performance for semiconductor devices. As
we move to sub-100nm lithographies however, these gains
appear to face fundamental reliability related challenges. In
particular, soft errors are emerging as a new challenge in
processor design. Soft errors or single event upsets are tran-
sient errors caused by high energy particle strikes such as
neutrons from cosmic rays and alpha particles from pack-
aging material. Such strikes can ﬂip the bit stored in a
storage cell and change the value being computed by a
logic element. Various studies have predicted an increase in
soft error rates (SER) in different types of circuits (SRAM,
∗
This work was supported in part by an equipment donation from AMD
and the National Science Foundation under Grant No. CCR-0313286 and
EIA-0224453.
1Xiaodong Li is a Ph.D. student at the University of Illinois at Urbana-
Champaign. This work was done while he was a co-op at the IBM T.J.
Watson Research Center.
‡
Department of Computer Science
University of Illinois at Urbana-Champaign
{xli3, sadve}@uiuc.edu
latches, and logic) with scaling [3, 8, 10]. Although a con-
sensus on exact SER values is still lacking, there is a grow-
ing concern about the phenomenon.
Until recently, the majority of the work in soft errors
has focused on the device and circuit level. More re-
cently, however, there has been work at the architecture
level [1, 4, 7, 12, 13] for at least two broad reasons. First,
recent work has shown that many of the raw errors that oc-
cur at the device/circuit level may be masked at the archi-
tecture level, potentially motivating lower cost protection
mechanisms. For example, Wang et al. report that about
85% of the raw errors are masked at the microarchitecture
level [12]. The reasons for such a high masking rate in-
clude the relatively low resource utilization in a modern pro-
cessor; the large number of resources that only affect per-
formance and not correctness (e.g., branch predictor struc-
tures); and values that are used but do not affect the eventual
program outcome. Second, by considering solutions at the
architecture level, knowledge of workload behavior can be
exploited, leading to potentially more efﬁcient protection
solutions (e.g., [12, 13]). These observations motivate the
need for comprehensive models and tools for quantitative
studies of soft errors at the architecture level.
This work presents an architecture-level model and tool,
called SoftArch, to quantify the impact of soft errors on a
modern processor (e.g., its architectural mean time to fail-
ure or MTTF). To our knowledge, this is the ﬁrst such tool
to model soft errors in most signiﬁcant microarchitectural
structures for applications with millions of instructions in
reasonable time. (A detailed comparison with prior work
appears in Section 5.)
SoftArch works with a high-level architecture timing
simulator to track the raw probability of error in the value
of each bit (instruction or data) communicated or computed
by any pipeline stage in the processor. A value may be erro-
neous either because (i) it is physically struck by a particle
during its residence time in a structure, or (ii) it is the result
of a communication of an erroneous value, or (iii) it is com-
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
puted using one or more erroneous input values. We refer
to the ﬁrst case as error generation and to the second and
third cases as error propagation. To model the error gen-
eration probability, we use a combination of residence time
and raw SER numbers for storage structures, and a simple
abstraction for logic. For error propagation probability, we
apply simple probability theory on the error probabilities of
the sources of the propagation.
During program execution, SoftArch identiﬁes the val-
ues that could affect program outcome. For each such value,
it uses the tracked errors for the value and the simulator tim-
ing data to determine the probability of failure and time to
failure due to that value. This enables determining the mean
time to failure using basic probability theory. SoftArch also
keeps enough information on the microarchitectural struc-
tures occupied by each value to determine the contribution
of different structures to the overall MTTF.
We use SoftArch to quantify the MTTF of a mod-
ern out-of-order processor and the contribution of differ-
ent structures to the failure rate, for various SPEC bench-
marks. Our results (consistent with, but more comprehen-
sive than, previous studies) are as follows: (1) there is sig-
niﬁcant architecture-level masking of soft errors, (2) there
is substantial inter- and intra-application variation in MTTF
or failure rate, and (3) there is substantial application-
dependent variation in the contribution to the failure rate
from different structures. These results motivate selective
protection of only the most vulnerable structures and dy-
namic, application-aware protection schemes.
2 The SoftArch Model
The SoftArch model consists of the following compo-
nents, covered in Sections 2.1– 2.4 respectively. (1) A prob-
abilistic model for soft error generation in values residing
in storage structures or passing through logic. (2) A model
for soft error propagation, which results in the propagation
of generated errors to other values. (3) A deﬁnition of when
an erroneous value contributes to processor failure. (4) A
model for calculating mean time to failure (MTTF) for a
processor for a given workload.
2.1 Error Generation Model
2.1.1 Error Generation in Storage Elements
Current processors include several storage structures such
as caches, register ﬁles, queues, TLBs, and latches. A soft
error in a storage structure occurs when a high energy parti-
cle strikes a device in the structure, and the resulting charge
collected exceeds the critical charge (Qcrit) required to ﬂip
the stored bit value. We call this a raw soft error.
We seek to determine the probability that a value vi re-
siding in a (possibly multiple bit) storage location for time
T incurs a raw soft error during T . We assume that if an
error occurs, the value is corrupted; i.e., we ignore the low
probability that multiple errors could correct the value. It
is widely accepted that raw soft errors for storage follow a
constant failure rate or exponential time-to-failure distribu-
tion model. Let λ denote the raw failure rate, also referred
to as the raw soft error rate or SER, for the storage loca-
tion considered. Then the probability that the value vi will
incur a raw soft error in time T , denoted ei, is 1 − e−λ·T .
In practice, both λ and T are small enough that we can ap-
proximate e−λ·T as 1 − λ · T . This gives ei = λ · T .
Thus, the probability that an error is generated for a value
vi in a storage location depends on the raw SER for that lo-
cation, λ, and the residence time of the value in the location,
T . λ is determined by circuit layout, technology, and en-
vironmental parameters (e.g., the amount of charge stored,
charge collection efﬁciency, and particle ﬂux). There has
been extensive work on determining the value of λ using
circuit level simulation or measurement (Section 3.2). Res-
idence time T depends on the program and the processor ar-
chitecture, and can be determined through architecture level
timing simulation (Section 3.1).
2.1.2 Error Generation in Logic Elements
Combinational logic elements are used for computation and
control within a pipeline stage. A high energy particle strike
on a device in a logic circuit may create a current pulse that
may affect the value produced by the circuit. This transient
effect becomes visible only if it is captured by the subse-
quent latch. Instead, the transient effect could be masked
due to electrical masking (the current pulse attenuates as
it goes through the gates in the circuit), logical masking
(the current pulse affects parts of the circuit that do not af-
fect the output value), or latch window masking (the cor-
rupted result is not latched because it does not arrive within
the required timing window for the latch). Logic SER has
been ignored in most prior architectural studies because the
above masking makes the effective SER much smaller than
that of storage structures. However, as technology scales,
these masking effects are diminishing and the logic SER is
projected to increase signiﬁcantly [10].
For our architecture level model, it is desirable to include
the above circuit-level masking effects within the raw logic
SER value. Because these masking effects depend on the
circuit layout and inputs, the desired raw logic SER will
differ for different logic circuits and even for different in-
puts. In general, it is hard to abstract all of these effects. We
therefore use a simple abstraction consisting of one param-
eter called elogic corresponding to each type of logic circuit
(e.g., ealu for the ALU or ef pu for the FPU). elogic is de-
ﬁned to be the probability that, given correct inputs, the re-
sult produced by the corresponding circuit at the end of the
computation is incorrect because of soft errors. elogic can
be estimated using circuit level SER analysis tools, based on
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
the layout of that logic circuit and technology parameters.
In our implementation, we use a simple estimation based on
prior work [10] and the gate and latch counts for the logic
circuit (Section 3.3).
2.2 Error Propagation Model
In a processor, values are read from storage locations,
possibly processed, and the original or newly computed val-
ues are stored elsewhere. (We consider the values stored in
the new locations to be new values, even if they are identi-
cal to the original ones.) During this process, errors in the
original values will propagate to the new values. For ex-
ample, if the value, v1, in register r1 is corrupted and later
used to generate a result r3 = r1 + r2, the error in v1 will
propagate to the new value stored in r3.
Conceptually, we would like to track how errors are
propagated to new values and determine the probability that
a new value is erroneous. These probabilities will then al-
low us to determine the probability of failure and the mean
time to failure (depending respectively on which erroneous
values cause failure and when). The probability of error in
a newly generated value (say v3) depends on the probability
of error in the input values (say v1 and v2) used to generate
v3. In general, denoting Vi to mean the event that value vi
has an error, denoting P(Vi) as the probability of Vi, and as-
suming that any error in either v1 or v2 will cause an error
in v3, the probability of error in v3 can be given by P(V3) =
V2) = P(V1) + P(V2) − P(V1 · V2), where V1 · V2 is
P(V1
the event that v1 and v2 both have errors.
If the errors in v1 and v2 are independent, then P(V1 · V2)
is simply P(V1)P(V2). On the other hand, if the errors are
perfectly correlated (e.g., if v2 was just generated by copy-
ing v1 to another location), then P(V1 · V2) = P(V1) = P(V2).
In general, however, the errors in two values could be par-
tially correlated and estimating P(V1 · V2) is more difﬁcult.
Accounting for the correlation and determining the resul-
tant probability requires keeping track of the raw errors that
were originally responsible for the errors in v1 and v2.
(cid:1)
For example, Figure 1 shows a dataﬂow graph where val-
ues v1, v2, and v3 incur errors e1, e2, and e3 with proba-
bility |e1|, |e2|, and |e3| respectively. Assuming e1, e2, and
e3 are independent of each other, the probability of error
for value v4 is |e1| + |e2| − |e1| · |e2| and that for v5 is
|e2| + |e3| − |e2| · |e3|. The errors in v4 and v5 are corre-
lated since they share the same error from v2 – if v2 has an
error, both v4 and v5 will have errors. Therefore, to calcu-
late the probability of error in v6, the correlation between
the errors in v4 and v5 needs to be taken into account. We
do this by tracking the original independent raw error events
that cause errors in different values.
For our model, we do not need to calculate the probabil-
ity of error for a value immediately upon its generation – we
only need probability calculations for values that eventually
Figure 1. An example for error propagation.
cause failure as deﬁned in the next section. Therefore, for
purposes of determining how errors propagate among val-
ues, we simply keep track of the set of all the raw error
events that can cause an error in a value, and propagate this
entire set when a value is used to generate a new value. For
example, in Figure 1, the error set for v4 is {e1, e2} and
for v5 is {e2, e3}. Thus, the error set for v6 should be
{e1, e2, e3}. We can now easily calculate the error proba-
bility for v6, since e1, e2, and e3 are independent.
More generally, consider a value vi residing in a storage
location. Let tj be the time interval between two successive
reads of vi (or between the ﬁrst write and read of vi). We
refer to the event that vi incurs a raw soft error over time tj
as a basic storage error event. If vi was generated through
computation logic, then we refer to the event that vi incurred
a logic error (after considering circuit level masking effects)
during this computation as a basic logic error event. We
refer to a basic storage or basic logic error event as a basic
error event or simply a basic error. All basic errors are
independent of each other, with probabilities given by the
error generation models in Section 2.1.
The error propagation model requires determining the
basic errors that need to be propagated to a new value. For
each value vi, we associate a basic error set, denoted Ei.
This is the set of basic errors directly incurred by or propa-
gated to vi.1 Thus, for a new value vi created at time ti, the
propagation model seeks to determine vi’s Ei at ti.
First, we handle the simple case where vi is generated by
reading an old value v0 from a storage location and writing
it to another storage location. In this case, the error set Ei
is simply the error set for v0 at time ti.2
Next, we handle the case where vi is created through
some computation op(in1, in2, ...ink), where k ≥ 1, inj’s
1Note that for vi in a storage location, each time it is read, a new basic
error event is added to Ei (to indicate an error occurrence in the interval
since it was last read).
2We assume the process of moving a value from one location to another
across wires does not induce any errors. Currently, wires do not appear to
have soft error problems. However, in the future, soft errors from wires
could be easily incorporated by adding another basic error due to the wires
to the set Ei.
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
are input operands, and op is any operation. The creation of
vi involves a possible basic logic error event, say bi, with
probability eop. Then Ei is simply Ein1 ∪ Ein2 ∪ · · · ∪
Eink
∪ {bi}.
Thus, we can generate the basic error set for a newly
created value. Since all the error events in this set are in-
dependent, the probability of error in the new value can be
calculated as a function of the probabilities of the errors in
its basic error set (which are known from Section 2.1). For
example, in Figure 1, the probability of error for v6 is |e1|+
|e2|+|e3|−|e1|·|e2|−|e2|·|e3|−|e1|·|e3|+|e1|·|e2|·|e3|.
2.3 Program Failure and Time to Failure
Not all erroneous values cause program failure. For ex-
ample, an error that occurs in a dead value does not cause
failure since the value is not used again. Similarly, an error
in a speculative instruction that is later squashed does not
cause program failure. We say an erroneous value results
in program failure if the error is observable by an external
observer. Broadly, this includes (1) values that are written
to an output device, (2) values that affect program control
ﬂow (e.g., the value of a branch target), (3) the value of an
instruction opcode (an error could make the opcode illegal,
causing a program crash), (4) any value representing an ad-
dress of a memory location (an error could cause access to
prohibited locations, causing a crash), (5) and a destination
register ﬁeld of an instruction (an error could result in the
corruption of an unknown and undesirable register).
Depending on the system modeled and the implementa-
tion, the precise set of values where errors may cause pro-
gram failure will vary (e.g., in a processor with speculation,
an errror in the opcode of a misspeculated instruction will
not cause program failure). Further, a speciﬁc implemen-
tation of the model may choose to conservatively assume
that errors in a superset of the above values will cause fail-
ure. Section 3.5 describes the set of values where errors are
considered to cause failure in our implementation.
We call the above deﬁned set of values where errors
would lead to program failures as the failure set, denoted
by VF = {vf 1, vf 2, ...}. Additionally, our model also
requires determining the time, tf i, at which a failure due
to vf i occurs. This is determined through the architectural
timing (performance) simulator. We assume that the failure
set {vf 1, vf 2, ...} is ordered such that tf i < tf j for i < j.
2.4 Determining Mean Time To Failure (MTTF)
We next derive mean time to failure (MTTF) for a pro-
cessor running a given workload. Our model so far pro-
vides: (1) the values that can cause failure: {vf 1, vf 2, ...},
(2) the corresponding times for these failures: {tf 1, tf 2, ...},
(3) for each value, vf i, the set of independent basic errors
Ef i = {ef i−1, ef i−2, ...} that can produce an error in vf i,
and (4) the probability for each independent basic error.
Inﬁnite programs. First, consider a workload that runs for-
ever. Its MTTF is the sum of the tf i’s, each weighted by the
probability that vf i is erroneous and no previous value in the
failure set is erroneous. Denoting the number of elements
in the failure set as N (N could be ∞), we have:
M T T F =
none of vf 1, ..., vf i−1 have an error)
(cid:2)N
i=1
tf i· (Probability that vf i has an error and
Given the basic error sets Ef i and the probabilities of the
constituent errors, we use basic probability theory to deter-
mine the probability of the events in the above summation.
For example, let Ef 1 = {e1, e2} and Ef 2 = {e2, e3}. Then
the probability that vf 2 has an error and vf 1 does not have
an error is the probability that at least one of the errors in
(Ef 2 - Ef 1) occurs and none of the errors in Ef 1 occurs.
This is |e3| · (1 − |e1|) · (1 − |e2|), denoting probability of
ei by |ei|.
Finite programs. Most of our workloads, however, are ﬁ-
nite programs that run for a relatively short amount of time.
To determine MTTF in a meaningful way for a processor
running such a program, we assume that the program runs
repeatedly in a loop forever. If a failure always occurs in
the ﬁrst run of the program, then the MTTF for the ﬁnite
program, denoted MTTF(cid:1)
, can also be represented by the
above equation for inﬁnite programs. If there is no failure in
the ﬁrst run, then we need to expand the equation to include
possible failures in subsequent runs.
Let Texec be the execution time of one run of the pro-
gram. Then the time to failure due to vf i in the kth run
of the program is (k − 1)Texec + tf i. This time to failure
must be weighted by the probability that none of the prior
k − 1 (independent) runs fail, vf i is erroneous in the kth
run, and none of the values prior to vf i in the failure set are
erroneous in the kth run. That is,
(cid:2)∞
(cid:2)N
M T T F =
{(k − 1)Texec + tf i}· (Probability
that none of the prior k-1 runs fail) · (Probability that vf i has an
error and none of vf 1, ..., vf i−1 have an error)
k=1
i=1
To
simplify
F ailureP rob(cid:1)
the program will see a failure. That is,
deﬁne
as the probability that a given run of
equation, we
above
the
F ailureP rob(cid:3) =
(cid:2)N
i=1 (Probability that vf i has an error
and none of vf 1, ..., vf i−1 have an error)
Thus, in the MTTF equation, the term Probability that
none of the prior k-1 runs fail can be represented as (1 −
F ailureP rob(cid:1))k−1. The MTTF equation then becomes:
(cid:2)∞
(cid:2)N
M T T F =
{(k − 1)Texec + tf i} · (1 −
F ailureP rob(cid:3))k−1· (Probability that vf i has an error and none
of vf 1, ..., vf i−1 have an error)
k=1
i=1
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Technology Parameters
3.1 Integration with Timing Simulation
Process technology
Processor frequency
90nm
2.0 GHz
Processor Parameters
Fetch rate
Retirement rate
Functional units
Issue queue entries
Integer FU latencies
FP FU latencies
Register ﬁle size
iTLB/dTLB entries
Instruction buffer entries
8 per cycle
1 dispatch-group (=5, max) per cycle
2 Int, 2 FP, 2 Load-Store, 1 Branch
FPU = 20, Load/Store/Integer = 36
Branch = 12
1/4/35 add/multiply/divide (pipelined)
5 default, 28 div. (pipelined)
80 integer, 72 FP
128/128
64
L1 Dcache
L1 Icache
L2 (Uniﬁed)
Memory Hierarchy Parameters
32KB, 2-way, 128-byte line
64KB, 1-way, 128-byte line
1MB, 4-way, 128-byte line
Contentionless Memory Latencies
L1/L2/Memory Latency
1 /20 /165 cycles
Table 1. Parameters for the simulated processor.
Rearranging the terms slightly,
(cid:2)∞
M T T F =
{(k −
1)Texec + tf i}· (Probability that vf i has an error and none of
vf 1, ..., vf i−1 have an error)
(1 − F ailureP rob(cid:3))k−1 · (cid:2)N
k=1
i=1
Now applying the deﬁnition of M T T F (cid:1)
, we get:
(cid:2)∞
(1 − F ailureP rob(cid:3))k−1 · {(k − 1)Texec ·
(k−1)·(1−F ailureP rob(cid:3))k−1+
k=1
M T T F =
F ailureP rob(cid:3) + M T T F (cid:3)}
= Texec·F ailureP rob(cid:3) (cid:2)∞
M T T F (cid:3) (cid:2)∞
(cid:2)∞
k=1
k=1
(1 − F ailureP rob(cid:3))k−1
k=1 xk−1 = 1
1−x and
Using
x
1−x2 to simplify the equation, we get
(cid:2)∞
k=1(k − 1)xk−1 =
F ailureP rob(cid:1)