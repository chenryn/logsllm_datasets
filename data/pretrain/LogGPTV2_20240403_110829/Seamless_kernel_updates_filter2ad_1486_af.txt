possible to modify the kernel and prevent port numbers that are in the checkpoint from
being assigned to another process before the checkpoint is fully restored.
Our implementation does not restore the state of TCP sockets in which connection
initialization (three-way handshake) is in progress. This limitation does not aect the
server side because it is not notied about the connection until the connection is estab-
lished. However, the client side will receive a reset packet after the kernel update. In
our evaluation, clients do not make frequent connections and so this limitation did not
aect them. In the future, we plan to support checkpointing the state of connections in
progress.
The ability to save only some running processes creates challenges when processes
share resources. For example, if a group of processes is sharing a resource, but we only
restore some of them, then the restored processes may not work correctly if they rely on
the other processes in the group. Our implementation handles some of these cases, such
as the sharing of dynamic libraries among processes, by carefully tracking the reference
counts of virtual memory regions. However, we require that all processes sharing other
resources, e.g., pipes and Unix sockets, are saved and restored together.
Chapter 5
Evaluation
We evaluate our system by analyzing our checkpointing format and code in terms of
its suitability for supporting kernel updates. Then, we describe our experience with
updating major releases of the kernel. Finally, we present performance numbers in terms
of kernel update times and overhead.
5.1 Code Analysis
Table 5.1 provides a summary of our checkpoint format. There are a total of 13 data
structures that are saved in the checkpoint. The table shows the number of elds in
each data structure and the number of elds that we save from each data structure in
the checkpoint. The saved elds include both primitive types or pointers to buers that
need to be saved. The rest of the elds are mostly implementation dependent and do not
need to be saved, although a few more would need to be saved if we add support for the
features discussed in Section 4.
The code consists of roughly 6,000 lines of code, as shown in Table 5.2. Roughly 90%
of the code resides in a separate kernel module, while the rest of the code is spread across
various sub-systems. We can characterize kernel functions into four categories, based on
how high-level it is and unlikely to be changed over time: system calls, exported functions,
42
Chapter 5. Evaluation
43
Data
Nr of Nr of saved
structure
elds
elds
vm_area_struct
mm_struct
task_struct
fs_struct
les_struct
le
sock
tcp_sock
unix_sock
pipe_inode_info
vc_data
fb_info
mousedev
16
51
135
5
7
18
53
76
13
13
82
23
18
7
5
32
3
1
10
10
48
10
4
12
6
7
Table 5.1: Kernel structures and checkpoint format
global functions and private functions. System calls are available to user applications
and cannot be changed without breaking backwards compatibility and are thus the most
stable. Exported functions are available to loadable modules and usually only have
minor changes between kernel versions. Global and private functions are expected to
change more frequently. Our checkpoint saving code uses 20 functions and all of them
are exported. The restore code uses 131 functions, of which 5 are system calls, 93 are
exported, 2 are global, and 31 are private.
We needed to use private functions for two purposes: 1) managing resource identiers,
and 2) performing low-level process creation and initialization. The kernel provides user
threads with identiers for kernel managed resources such as PID, le descriptors, port
Chapter 5. Evaluation
44
Subsystem
Lines of code
Checkpoint module
5257
Architecture specic
Memory management
File system
Process management
Networking
Total
81
70
23
10
428
5869
Table 5.2: New or modied lines of code
numbers, etc. When a process is restored, we must ensure that the same resources
correspond to the same unique identiers. However, since these identiers are never
modied, the kernel does not provide any exported or high-level functions to manipulate
them. We believe that our solution is better suited for kernel updates because it doesn't
impose any overhead for virtualizing identiers during normal operation [16]. We also
used private functions during process creation. The restore code is similar in functionality
to the implementation of the execve system call that executes a le from disk. In our
case, we create a process from the in-memory checkpoint data. The modications needed
were similar to the eort required to add support for another executable format.
We had to modify some architecture-specic code to reserve memory during kernel
boot. We also made some changes to memory management code to assign reserved mem-
ory to the restored processes. We needed to make some changes to the Ext3 lesystem
to prevent orphan clean up on boot so that temporary les are not deleted (See Sec-
tion 3.2). All changes to the networking code relate to adding progress tracking (See
Section 3.3) for reads and writes on TCP sockets, or changes needed to support TCP
timestamps. Some of the changes do not alter the functionality of the kernel but were
needed to provide access to previously private functions.
Chapter 5. Evaluation
45
5.2 Experience with Updating Kernels
In this section, we describe the eort needed to use our system for performing kernel
updates. We implemented our system starting with the Linux kernel version 2.6.28,
released in December 2008, and have tested updating it, one major revision at a time,
until version 2.6.34, released in May 2010 (roughly one and a half years of kernel updates).
Table 5.3 shows that on average, each revision consists of 1.4 million lines of added or
modied code. There were six million lines of changed code over the six revisions in
23,000 les, including many data structure modications. We updated our code from
one version to the next using a typical porting procedure: 1) extract all our code from
the kernel and keep it in a separate git branch, 2) merge our code into the next major
kernel release using git merge functionality, 3) compile the kernel and x any errors, 4)
once the kernel compiles, run an automated test suite that checks that all the features
we have implemented are working correctly when updating between the kernel versions,
and 5) commit the changes to our code so they can be used when moving to the next
major release.
Table 5.3 shows the number of lines that had to be changed manually for each kernel
release. These changes are small, both compared to the number of lines changed in the
major release, as well as the number of lines in the checkpoint code. The majority of
the changes were simple and were caught either during merge or during compilation.
For example, several merge conicts occurred when our code made a private function
globally accessible, and some other nearby code was changed in the kernel update. Sim-
ilarly, compilation errors occurred due to renaming. For example, we needed to use
the TCP maximum segment size variable, and it was renamed from xmit_size_goal to
xmit_size_goal_segs. These xes are easy to make because they are caught by the
compiler and do not the aect the behavior of the kernel or our code.
More complicated changes involved renaming functions and changing the function
interface by adding arguments. In this case, we have to nd out the new function name,
Chapter 5. Evaluation
46
Kernel
Lines of change Lines of change
version in major release
for checkpoint
2.6.29
2.6.30
2.6.31
2.6.32
2.6.33
2.6.34
1729913
1476895
1393049
1628415
1317386
882158
42
16
7
5
50
2
Table 5.3: Summary of updates needed for checkpoint code
and how to pass the new arguments to the function. For example, version 2.6.33 intro-
duced a signicant change to the interface used by the kernel to create and modify les
and sockets. These changes were designed to allow calling le system and socket system
calls cleanly from within the kernel. As a result, some internal functions used by our
system were changed or removed, and our code needed to use the new le interface.
Our code needed to handle one signicant data structure update conict. Previ-
ously, the thread credentials, such as user_id and group_id, were stored in the thread's
task_struct. In version 2.6.29, these credentials were moved into a separate structure,
with the task_struct maintaining a pointer to this structure. We needed to change our
system, similar to the rest of the kernel code, to correctly save and restore credentials.
Two other data structure that we save, as shown in Table 5.1, were updated, but they
required us to simply pass an additional parameter to a function.
Finally, the most dicult changes were functional bugs, that were not caused by
changes to data structures or interfaces. We found these bugs when running applica-
tions. We encountered two such issues with TCP code. Previously, a function called
tcp_current_mss was used to calculate and update the TCP maximum segment size.
In 2.6.31, this function was changed so that it only did a part of this calculation, and
Chapter 5. Evaluation
47
another function called tcp_send_mss was introduced that implemented the original
tcp_current_mss behavior. Similarly, in 2.6.32, the TCP code added some conditions
for setting the urgent ags in the TCP header, which indicates that out-of-band data is
being sent. Our code was setting the urgent pointer to the value of 0, which in the new
code set the urgent ag, thus corrupting TCP streams on restore. We needed to set the
urgent ags based on the new conditions in the kernel.
All these ports were done by us within a day to a few days. We expect that kernel
programmers would have found it much simpler to x our code when updating their
code. An interesting observation is that during porting, we never had to change the
format of the checkpoint for any of the kernel versions. As a consequence, it is possible
to freely switch between any of these kernel versions in any order. For example, it is
possible to upgrade to version 2.6.33 from version 2.6.28, and then go back to version
2.6.30 seamlessly. This feature is not our goal, and we expect that the checkpoint format
will change over time. We only intend to have a common checkpoint format between two
consecutive major kernel releases.
5.3 Performance
We have tested our system for updating the kernel while running several desktop and
server applications. For desktop applications, we have tested the system with the simple
Xfbdev X server, the Twm window manager and several X programs. The mouse, key-
board, console and the graphics work correctly after the update, without requiring any
application modications or user intervention.
Interestingly, the mouse and keyboard
were initially freezing after an update, but only if we kept moving the mouse or typing
on the keyboard while the update occurred. This problem was solved after we xed a
bug in the quiescence code. We were unable to test a modern GUI environment such
as Gnome/GTK because these applications use SYSV shared memory, which we do not
Chapter 5. Evaluation
48
Figure 5.1: Quake reboot vs. update
support currently.
We conducted two types of experiments. First, we measured the throughput of server
applications before and after the update. These experiments also show the downtime
during an update. We used the Collectl system monitoring tool to measure throughput
at the network interface level (sampled at one second interval). Second, we performed
microbenchmarks to measure the per-process checkpoint size and time. All of our ex-
periments run on the same machine with two Intel Xeon 3 GHz processors and 2GB of
RAM, running Ubuntu 8.04 with our kernel that had support for updating the kernel.
5.3.1 Application Benchmarks
We tested several UDP (Quake game server, Murmer/Mumble voice-over-IP server) and
TCP (Mysql, Memcached and Apache) server applications. Apache and Memcached used
the epoll system call that our system does not support currently. Apache was compiled
with epoll disabled and Memcached allows disabling epoll with environment variables.
All these applications run after the update, without interrupting any requests in progress,
and without requiring any other modications. The Murmer and the Apache results were
similar to the Quake results and are not shown.
 0 10 20 30 40 50 60 70 0 50 100 150 200 250Throughput (KB/s)Time (s)KB/s receivedKB/s sent 0 10 20 30 40 50 60 70 0 50 100 150 200 250Throughput (KB/s)Time (s)KB/s receivedKB/s sentChapter 5. Evaluation