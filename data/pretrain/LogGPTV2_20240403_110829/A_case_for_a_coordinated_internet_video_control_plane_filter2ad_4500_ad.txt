design that we can use to analyze if the beneﬁts promised from the
previous section can be translated into reality.
5.1 Optimization
To make our discussion concrete, we focus on a speciﬁc policy
objective and utility function. Our policy goal is to achieve both
fairness (i.e., do not deny clients if there is sufﬁcient capacity) and
efﬁciency (i.e., maximize aggregate utility). There is a rich litera-
ture on balancing efﬁciency-fairness tradeoffs that we can build on
Metric
Buffering ratio (%)
Start time (s)
Failure ratio (%)
Duration (hrs)
3
2
1
Current
10.41
6.41
16.57
Projected
0.795
1.997
0.213
Table 4: Potential improvement in the mean performance un-
der extreme scenario for Provider1
Metric
Duration (hrs)
Current
Projected
Buffering ratio (%)
Start time (s)
Failure ratio (%)
1
7
3
2.24
1.56
35.6
0.29
0.39
0.3
Table 5: Potential improvement in the mean performance un-
der extreme scenario for Provider2
here; as a starting point we choose a simple goal of ﬁrst ensuring
fairness and then optimizing for efﬁciency.
We ﬁx the utility function to be a simple linear combination of
the different performance metrics that captures the expected view-
ing time for a given session. We choose the function
Utility = −3.7 × BuﬀRatio +
Bitrate
20
where BuﬀRatio is in percentage and Bitrate is in Kbps. This
utility function is based on prior observations on linear relationship
between the expected play time and the different quality metrics
reported in a previous measurement study; e.g., a 1% increase in
buffering ratio caused a 3.7 minute drop in viewing time [21].
Having ﬁxed the utility function and policy objective, we use
a simple two-phase algorithm. First, we assign the clients a fair
share of the CDN resources by computing the average sustainable
bitrate. This allocation ensures that each client has been assigned
to some feasible combination of bitrate and CDN and there is no
unfairness in the allocation in terms of bitrates. Also, each client is
assigned a CDN at random so that each CDN gets assigned a share
of clients proportional to its capacity. Next, we use this allocation
as a starting point and try to incrementally improve the total utility.
This proceeds as a greedy algorithm where at each iteration, it picks
the combination of client and CDN/bitrate setting that provides the
largest incremental contribution to the global utility function.
The intuition behind this two-phase approach is as follows. A
pure greedy approach optimizes efﬁciency (i.e., total utility) but
may leave some clients unassigned; e.g., it might initially assign
clients with a higher bitrate but may end up saturating the capac-
ity and drop some clients. A pure fair-sharing approach on the
other hand guarantees that all clients will be assigned if there is
sufﬁcient capacity, but it is agnostic to the performance variabil-
ity across CDN-client-bitrate combinations. Our approach strikes a
balance between these two extremes by ensuring all clients are as-
signed, and improves the efﬁciency from this starting point. We do
not claim that this optimal in a theoretical sense but we do observe
that it works well across a range of realistic workloads.
One subtle issue is that the optimization may itself introduce un-
desirable temporal biases. For example, if we discover that CDN1
is performing poorly and shift all clients to CDN2, then it im-
pacts our ability to predict the performance of both CDN1 (we do
not have any samples) and CDN2 (we have increased its load) in
the future. This is a classical “exploration-exploitation” tradeoff
(e.g., [20]). We face a particularly difﬁcult form of this problem;
our rewards are not independent (due to CDN load), and the char-
acteristics of CDNs change over time even if we do not use them.
However, we do have access to a large amount of data. A simple
solution in this case is to use some form of randomization. Here,
we choose a random subset of sessions that will not receive any
explicit optimization so that we can observe their performance in
366the wild. With a large enough population even a small fraction (say
2-5%) of unoptimized clients would sufﬁce to build a robust pre-
diction model. There are other more sophisticated approaches to
solve this problem, such as knowledge gradient algorithms [38],
that will further reduce such biases. We currently use the simple
randomization approach due to its ease of implementation.
5.2 Performance estimation
A key issue with performance extrapolation we already observed
in Section 4 is the need for a prediction mechanism that is robust to
data sparsity and noise at such ﬁner granularities. We address this
by building on the hierarchical extrapolation techniques described
in Section 4. There are three additional practical challenges that
need to be addressed here.
First, in Section 4, our goal was to estimate the potential for im-
provement. Hence, we assumed access to a performance oracle that
has a current view of the performance. In practice, a real control
plane will not have such an oracle and will have to rely on his-
torical measurements. We extend the hierarchical approach from
the previous section to make predictions based on recent historical
measurements of the performance for speciﬁc CDN-client-bitrate
combinations. Since CDN performance also shows signiﬁcant tem-
poral variability, we simply use the most recent information (e.g.,
last hour). In Section 6.3, we show that even this simple use of
historical measurements works very well in practice.
Second, the extrapolation in the previous section ignores the ef-
fect of CDN load and how the performance degrades as a function
of load. To this end, we augment the performance estimation step
to also model the CDN load. Speciﬁcally, we observe that the CDN
performance shows a roughly thresholded behavior where the per-
formance is largely independent of the load up to some threshold
T1, after which the performance degrades roughly linearly, and at a
higher load threshold T2, the performance would drop signiﬁcantly.
We select these thresholds based on observed measurements (not
shown for brevity).
Third, we did not consider bitrates in the previous section. Here,
we simply treat bitrate as an additional attribute to our decision
process. That is, in addition to characteristics such as ISP and city,
we also partition clients based on their current bitrate when building
the performance estimators and prediction models. We extend the
measurements from the previous section to reﬂect bitrates in our
prediction model.
6. TRACE-DRIVEN SIMULATIONS
In this section, we use trace-driven simulations to evaluate the
qualitative beneﬁts of the global control plane approach we instan-
tiated in Section 5 over other choices in the design space from Ta-
ble 3 under different scenarios.
6.1 Setup
We built a custom simulation framework that takes the following
conﬁgurations as input: (1) client arrival and viewing time patterns
obtained from measurements from the same dataset described in
Section 2, and (2) empirically observed CDN performance distri-
bution in different geographical regions at different (normalized)
loads.
In each epoch, a number of clients join the system and
choose a viewing time drawn from empirical distribution. The
client either stays until the end of viewing time, or leaves when
its utility becomes 0, i.e., the performance becomes unbearable. In
this simulation, we implement three strategies:
1. Baseline: Each client chooses a CDN and bitrate at random.
This can be viewed as a strawman point of comparison.
2. Global coordination: The control plane algorithm proposed in
Section 5.
3. Hybrid: Each client is assigned to a CDN with lowest load and
a bitrate by the global optimization when it ﬁrst arrives, but
the subsequent adaptation is only limited to client-driven bi-
trate adaptation, i.e., no midstream CDN switching. This can
be viewed as a simpliﬁed version of what many providers de-
ploy today: start time CDN selection and client-side mid-stream
bitrate adaptation.
The primary goal of these simulations is to analyze the qualita-
tive beneﬁts of a global control plane. These alternative points do
not necessarily reﬂect exact strategies in operation today; we pick
these as sample points from our design space in Table 3.
In each epoch, after the decisions are made, the simulator com-
putes the resulting load on each CDN (by summing up bitrates from
all clients), and then computes the expected performance of each
client based on empirical measurements and the resulting load, as
described in the previous section. In this simulation, we use three
CDNs. The performance metrics of each CDN under normal load
are obtained by taking its mean performance over a week, so that
any overload effect is averaged out. Then the load thresholds for
modeling the impact of CDN load on performance (see Section 5.2)
are extrapolated from visually correlating the relationship between
the load and quality metrics for each CDN. In order to scale our
simulation framework, we normalize the CDN capacity and the
number of clients proportionally compared to the actual capacity
and number of clients. The normalization ensures that 1) the re-
quired capacity to serve the highest bitrate for all clients will ex-
ceed the combined capacity of all three CDNs, and 2) the required
capacity to serve the lowest bitrate for all clients does not exceed
that of any two CDN combined.
We simulate three scenarios: average case, CDN performance
degradation, and ﬂash crowd. In the average case scenario, arrival
patterns and playback duration mimic the typical client arrival pat-
terns observed in the dataset. In the CDN performance degradation
scenario, we retain the same client arrival pattern, but emulate a
sudden degradation in one of the CDNs. Finally, in the ﬂash crowd
scenario, a large number of clients arrive simultaneously.
6.2 Results
We focus on two metrics in our evaluation: average utility and
failure ratio. Average utility is computed by the total utility di-
vided by all clients in the system (including clients that failed to
play video and thus has zero utility). Since the arrival pattern is
the same for all three strategies, they all have the same denomi-
nator in all epochs. Failure ratio is the ratio of clients that could
not receive any video due to either CDN exceeding capacity or un-
bearably high rebuffering ratio. Figure 9 shows the simulated time
series of these two metrics for the three scenarios across different
adaptation strategies.
Average case: In Figure 9(a), we observe that global coordination
signiﬁcantly outperforms the baseline strategy in terms of the aver-
age utility. One interesting observation is that in the common case,
a hybrid strategy provides similar performance to global coordina-
tion. This is because in this strategy, a client chooses an ideal CDN-
bitrate combination when it arrives, and under regular conditions,
this strategy can maintain good performance. Unsurprisingly, the
baseline approach performs poorly even in the common case and
has both a high failure rate and low overall utility. The primary
reason is that this approach is agnostic to CDN performance.
With regard to failure rates, we see that the baseline approach is
consistently high. The capacity-aware global coordinator is able to
assign clients to suitable CDNs and bitrates and keep a zero failure
367(a) Common case
(b) CDN degradation
(c) Flash crowd
Figure 9: Simulation results of three scenarios. For each scenario, we show the performance of baseline, hybrid and global coordi-
nation in terms of failure ratio and average utility value. (The utility metric is in units of expected minutes of playing time.)
and has a signiﬁcantly higher average utility than both other ap-
proaches. However, it too suffers a mild drop in utility compared
to the normal scenario during the degradation. We also see that
the hybrid approach does almost as poorly as the baseline in both
metrics. This is because in this strategy, the server is not aware
of CDN performance degradation, and even when the clients can
identify this degradation, it is unable to switch CDN midstream.
Flash crowd: Last, we emulate a sudden ﬂash crowd in Fig-
ure 9(c), where a large number of clients try to join the system
between epochs 20–60. In this case, the global control algorithm
lowers the bitrate for many users in order to accommodate more
new users. Recall that this is one of the policy decisions we im-
posed in our design to balance fairness in allocating users vs. ef-
ﬁciency. During the ﬂash crowd, we see that the failure rate for
global coordination remains at zero, whereas the baseline and hy-
brid have exceedingly high failure rates. The hybrid approach does
not degrade as much as in previous case because the it is aware of
the load on each CDN, but without midstream CDN switching, the
performance is worse than global coordination.
6.3 History vs. Oracle prediction
As discussed earlier, a practical control plane will need to rely on
historical estimates of the performance to choose the best CDN. A
natural question is how far away from the optimal performance is
such history-based prediction. To analyze this gap, we use the same
dataset for Provider1 from Section 4 and compare a history-based
vs. oracle prediction in Figure 10(a) over one week. We consider
two options for using the historical estimates: using either the pre-
vious hour or using the previous day. That is, for each partition,
we identify the best performing CDN using the previous hour or
day measurements and use that as the prediction for the current
epoch. The oracle uses predictions based on the current hour. The
result shows that using the previous hour’s predictions, the gap be-
tween the history and oracle predictor is small for many instances.
However, we also see some cases where there is a non-trivial dif-
ference between the historical and optimal. Figure 10(b) visualizes
how the individual CDNs’ performance varies during this period
to provide some intuition on why such large gaps occur. We do
see that the CDNs’ performance is largely predictable using the re-
cent history but does occasionally spike. These performance spikes
cause causes our estimate using the previous hour to become inac-
curate. Our preliminary results (not shown) suggest that some of
these gaps can be alleviated this using more ﬁne-grained historical
information (e.g., previous ﬁve minute epochs).
(a) Prediction vs. Optimal
(b) Performance of different CDNs
Figure 10: Performance gap between a history-based and an
oracle performance estimator. We also show the performance
of the individual CDNs to visually conﬁrm why history-based
estimates work in most cases and highlight the speciﬁc epochs
where it does not.