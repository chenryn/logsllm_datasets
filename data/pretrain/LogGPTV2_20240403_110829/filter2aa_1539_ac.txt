101
11.4
Total
499B
753.4GB
Table. GPT-3 Datasets. Disclosed in bold. Determined in italics.
Alan D. Thompson, GPT-3.5 + ChatGPT: An illustrated overview, https://lifearchitect.ai/chatgpt/
18 (1) total: 40
GPT-3数据来源
数据来源：跟其他大规模语言模型的对比
18 (2) total: 40
GPT-3训练数据量
看一下大语言模型训练的token数量：
▶ GPT-3（2020.5）是500B（5000亿），目前最新数据为止；
▶ Google的PaLM（2022.4）是780B；
▶ DeepMind的Chinchilla是1400B；
▶ Pangu-ケ公布了训练的token数，约为40B，不到GPT-3的十分之一；
▶ 国内其他的大模型都没有公布训练的token数。
19 (1) total: 40
GPT-3训练数据量
ELMo: 1B training tokens
BERT: 3.3B training tokens
RoBERTa: ~30B training tokens
Mohit Iyyer, slides for CS685 Fall 2020, University of Massachusetts Amherst
19 (2) total: 40
GPT-3算力消耗
The language model “scaling wars”!
Log scale!
Mohit Iyyer, slides for CS685 Fall 2020, University of Massachusetts Amherst
20 total: 40
Few-shot and zero-shot learning (in-context learning)
Brown et al., Language Models are Few-Shot Learners, arXiv:2005.14165, 2021
21 (1) total: 40
Few-shot and zero-shot learning (in-context learning)
Brown et al., Language Models are Few-Shot Learners, arXiv:2005.14165, 2021
21 (2) total: 40
Chain-of-thought
Preprint: https://arxiv.org/pdf/2201.11903.pdf
22 total: 40
Magic word: Let’s think step-by-step
(c) Zero-shot
Q: A juggler can juggle 16 balls. Half of the balls are golf balls, 
and half of the golf balls are blue. How many blue golf balls are 
there?
A: The answer (arabic numerals) is 
(Output) 8 X
(d) Zero-shot-CoT (Ours)
Q: A juggler can juggle 16 balls. Half of the balls are golf balls, 
and half of the golf balls are blue. How many blue golf balls are 
there?
A: Let’s think step by step. 
(Output) There are 16 balls in total. Half of the balls are golf 
balls. That means that there are 8 golf balls. Half of the golf balls 
are blue. That means that there are 4 blue golf balls. ✓
Q: Roger has 5 tennis balls. He buys 2 more cans of tennis 
balls. Each can has 3 tennis balls. How many tennis balls does 
he have now?
A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 
tennis balls. 5 + 6 = 11. The answer is 11.
Q: A juggler can juggle 16 balls. Half of the balls are golf balls, 
and half of the golf balls are blue. How many blue golf balls are 
there?
A:
(Output) The juggler can juggle 16 balls. Half of the balls are golf 
balls. So there are 16 / 2 = 8 golf balls. Half of the golf balls are 
blue. So there are 8 / 2 = 4 blue golf balls. The answer is 4. ✓
(b) Few-shot-CoT
(a) Few-shot
Q: Roger has 5 tennis balls. He buys 2 more cans of tennis 
balls. Each can has 3 tennis balls. How many tennis balls does 
he have now?
A: The answer is 11. 
Q: A juggler can juggle 16 balls. Half of the balls are golf balls, 
and half of the golf balls are blue. How many blue golf balls are 
there?
A:
(Output) The answer is 8. X
Figure 1: Example inputs and outputs of GPT-3 with (a) standard Few-shot ([Brown et al., 2020]), (b)
Few-shot-CoT ([Wei et al., 2022]), (c) standard Zero-shot, and (d) ours (Zero-shot-CoT). Similar to
Few-shot-CoT, Zero-shot-CoT facilitates multi-step reasoning (blue text) and reach correct answer
where standard prompting fails. Unlike Few-shot-CoT using step-by-step reasoning examples per
t
k
d
d
l
d j
h
“L ’
hi k
b
”
Preprint: http://arxiv.org/abs/2205.11916
23 total: 40
Emergence and homogenization
Bommasani et al., On the Opportunities and Risks of Foundation Models, arXiv:2108.07258 [cs.LG]
24 (1) total: 40
Emergence and homogenization
Bommasani et al., On the Opportunities and Risks of Foundation Models, arXiv:2108.07258 [cs.LG]
24 (2) total: 40
The scale matters: the emergence of abilities
1018 1020 1022 1024
0
10
20
30
40
50
Accuracy (%)
(A) Mod. arithmetic
1018 1020 1022 1024
0
10
20
30
40
50
BLEU (%)
(B) IPA transliterate
1018 1020 1022 1024
0
10
20
30
40
50
Exact match (%)
(C) Word unscramble
LaMDA
GPT-3
Gopher
Chinchilla
PaLM
Random
1018 1020 1022 1024
0
10
20
30
40
50
Exact match (%)
(D) Figure of speech
1020
1022
1024
0
10
20
30
40
50
60
70
Accuracy (%)
(E) TruthfulQA
1020
1022
1024
0
10
20
30
40
50
60
70
Model scale (training FLOPs)
Accuracy (%)
(F) Grounded mappings
1020
1022
1024
0
10
20
30
40
50
60
70
Accuracy (%)
(G) Multi-task NLU
1020
1022
1024
0
10
20
30
40
50
60
70
Accuracy (%)
(H) Word in context
Figure 2: Eight examples of emergence in the few-shot prompting setting. Each point is a separate model. The
ability to perform a task via few-shot prompting is emergent when a language model achieves random performance
until a certain scale, after which performance signiﬁcantly increases to well-above random. Note that models
that used more training compute also typically have more parameters
hence we show an analogous ﬁgure with
Grounded conceptual mappings.
Figure 2F
shows the task of grounded conceptual mappings,
where language models must learn to map a con-
ceptual domain, such as a cardinal direction, rep-
resented in a textual grid world (Patel and Pavlick,
2022). Again, performance only jumps to above
random using the largest GPT-3 model.
Multi-task language understanding. Figure 2G
shows the Massive Multi-task Language Under-
standing (MMLU) benchmark, which aggregates
57 tests covering a range of topics including math,
history, law, and more (Hendrycks et al., 2021). For
GPT-3, Gopher, and Chinchilla, models of ∼1022
training FLOPs (∼10B parameters) or smaller do
not perform better than guessing on average over all
the topics, scaling up to 3–5 ·1023 training FLOPs
(70B–280B parameters) enables performance to
substantially surpass random. This result is strik-
ing because it could imply that the ability to solve
knowledge-based questions spanning a large col-
lection of topics might require scaling up past this
threshold (for dense language models without re-
trieval or access to external memory).
Word in Context. Finally, Figure 2H shows the
Word in Context (WiC) benchmark (Pilehvar and
1021 1022 1023 1024
0
5
10
15
20
25
No chain
of thought
Chain of
thought
GSM8K Accuracy (%)
(A) Math word
problems
1021 1022 1023 1024
30
40
50
60
70
No
instruction
tuning
Instruction
tuning
10 NLU task average
(B) Instruction
following
1019 1020 1021
0
20
40
60
80
100
No
scratchpad
Scratchpad
Model scale (training FLOPs)
8-digit addition (in-domain)
(C) Arithmetic
1019 1020 1021
0
20
40