[`/proc/sys/net/core/rps_sock_flow_entries`{.filename}]{.term}
:   ::: para
    这个参数控制内核可以操控的任意指定 CPU
    可控制的最多栈/流程数。这是一个系统参数，有限共享。
    :::
[`/sys/class/net/ethX/queues/rx-N/rps_flow_cnt`{.filename}]{.term}
:   ::: para
    这个参数控制可操控某个
    NIC（`ethX`{.filename}）中指定接受队列（`rx-N`{.filename}）的最大栈/流程数。注：所有
    NIC 中这个参数的各个队列值之和应等于或者小于
    `/proc/sys/net/core/rps_sock_flow_entries`{.filename}。
    :::
:::
::: para
与 RPS 不同，RFS 允许接收队列和程序在处理数据包流程时共享同一
CPU。这样可以在某些情况下改进性能。但这种改进依赖类似缓存阶层、程序负载等因素。
:::
:::
::: simplesect
::: titlepage
## [⁠]{#main-network.html#idm140329741738240}TCP-thin 流的 getsockopt 支持 {.title}
:::
::: para
*Thin-stream*
是用来描述程序用来发送数据的传输协议的名词，在这种低速率下，协议的重新传输机制并未完全饱和。使用
thin-stream 协议的程序通常使用可靠协议传输，比如
TCP。在大多数情况下此类程序提供对时间敏感的服务（例如股票交易、在线游戏、控制系统）。
:::
::: para
对时间敏感的服务，丢失数据包对服务质量是致命的。要放置此类情况出现，已将
`getsockopt`{.command} 调用改进为支持两个附加选项：
:::
::: variablelist
[TCP_THIN_DUPACK]{.term}
:   ::: para
    这个布尔值在 thin stream 的一个 duupACK 后启用动态重新传输。
    :::
[TCP_THIN_LINEAR_TIMEOUTS]{.term}
:   ::: para
    这个布尔值为 thin stream 线性超时启用动态起动。
    :::
:::
::: para
这两个选项都可由该程序特别激活。有关这些选项的详情请参考
`file:///usr/share/doc/kernel-doc-version/Documentation/networking/ip-sysctl.txt`{.filename}。有关
thin-stream 的详情请参考
`file:///usr/share/doc/kernel-doc-version/Documentation/networking/tcp-thin.txt`{.filename}。
:::
:::
::: simplesect
::: titlepage
## [⁠]{#main-network.html#idm140329713281776}传输代理服务器（TProxy）支持 {.title}
:::
::: para
内核现在可以处理非本地捆绑的 IPv4 TCP 以及 UDP
插槽以便支持传输代理服务器。要启用此功能，您将需要配置相应的
iptables。您还需要启用并正确配置路由策略。
:::
::: para
有关传输代理服务器的详情请参考
`file:///usr/share/doc/kernel-doc-version/Documentation/networking/tproxy.txt`{.filename}。
:::
:::
:::
::: section
::: titlepage
# [⁠]{#main-network.html#s-network-dont-adjust-defaults}8.2. 优化的网络设置 {.title}
:::
::: para
性能调节通常采用优先方式进行。通常我们会在运行程序或者部署系统前调整已知变量。如果调整不起作用，则会尝试调整其他变量。此想法的逻辑是[*默认情况下*]{.emphasis}，系统并不是以最佳性能水平运作；因此我们[*认为*]{.emphasis}需要相应对系统进行调整。在有些情况下我们根据计算推断进行调整。
:::
::: para
如前所述，网络栈在很大程度上是自我优化的。另外，有效调整网络要求网络栈有深入的理解，而不仅仅值直到网络栈是如何工作，同时还要直到具体系统的网络资源要求。错误的网络性能配置可能会导致性能下降。
:::
::: para
例如：*缓存浮点问题*。增加缓存队列深度可导致 TCP
连接的拥塞窗口比允许连接的窗口更大（由深层缓存造成）。但那些连接还有超大
RTT
值，因为帧在队列中等待时间过长，从而导致次佳结果，因为它可能变得根本无法探测到拥塞。
:::
::: para
当讨论网络性能时，建议保留默认设置，[*除非*]{.emphasis}具体的性能问题变得很明显。此类问题包括帧损失，流量极大减少等等。即便如此，最佳解决方法通常是经过对问题的细致入微的研究，而不是简单地调整设置（增加缓存/队列长度，减少中断延迟等等）。
:::
::: para
要正确诊断网络性能问题请使用以下工具：
:::
::: variablelist
[netstat]{.term}
:   ::: para
    这是一个命令行程序可以输出网络连接、路由表、接口统计、伪连接以及多播成员。它可在
    `/proc/net/`{.filename}
    文件系统中查询关于联网子系统的信息。这些文件包括：
    :::
    ::: {.itemizedlist xmlns:d="http://docbook.org/ns/docbook"}
    -   ::: para
        `/proc/net/dev`{.filename}（设备信息）
        :::
    -   ::: para
        `/proc/net/tcp`{.filename}（TCP 插槽信息）
        :::
    -   ::: para
        `/proc/net/unix`{.filename}（Unix 域插槽信息）
        :::
    :::
    ::: para
    有关 `netstat`{.command} 及其在 `/proc/net/`{.filename}
    中的参考文件的详情请参考 `netstat`{.command} man page:
    `man netstat`{.command}
    :::
[dropwatch]{.term}
:   ::: para
    监控内核丢失的数据包的监视器工具。有关详情请参考 A monitoring
    utility that monitors packets dropped by the kernel. For more
    information, refer to the `dropwatch`{.command} man page:
    `man dropwatch`{.command}
    :::
[ip]{.term}
:   ::: para
    管理和监控路由、设备、策略路由及通道的工具。有关详情请参考
    `ip`{.command} man page: `man ip`{.command}
    :::
[ethtool]{.term}
:   ::: para
    显示和更改 NIC 设置的工具。有关详情请参考 `ethtool`{.command} man
    page: `man ethtool`{.command}
    :::
[/proc/net/snmp]{.term}
:   ::: para
    显示 IP、ICMP、TCP 以及 UDP 根据 `snmp`{.command} 代理管理信息所需
    ASCII 数据的文件。它还显示实时 UDP-lite 统计数据。
    :::
:::
::: para
*《SystemTap
初学者指南》*中包含一些示例脚本，您可以用来概括和监控网络性能。您可在
找到本指南。
:::
::: para
收集完网络性能问题的相关数据后，您就可以形成一个理论
---，同时也希望能有一个解决方案。
[⁠]{#main-network.html#idm140329767761664}[^\[5\]^](#main-network.html#ftn.idm140329767761664){.footnote
xmlns:d="http://docbook.org/ns/docbook"}例如：在
`/proc/net/snmp`{.filename} 中增加 UDP
输入错误表示当网络栈尝试将新帧排入程序插槽时，一个或者多个插槽接受队列已满。
:::
::: para
这代表数据包[*至少*]{.emphasis}在一个插槽队列中被瓶颈，就是说插槽队列输送数据包的速度太慢，或者对于该插槽队列该数据包过大。如果是后者，那么可验证任意依赖网络程序的日志查看丢失的数据以便解决这个问题，您应该需要优化或者重新配置受到影响的程序。
:::
::: simplesect
::: titlepage
## [⁠]{#main-network.html#idm140329767758112}插槽接收缓存大小 {.title}
:::
::: para
插槽发送和接收大小都是动态调节的，因此基本不需要手动编辑。如果进一步分析，比如
SystemTap 网络示例中演示的分析，`sk_stream_wait_memory.stp`{.filename}
认为该插槽队列的排放速度过慢，那么您可以增大该程序插槽队列深度。要做到这一点，请增大插槽接收缓存，方法是配置以下值之一：
:::
::: variablelist
[rmem_default]{.term}
:   ::: para
    控制插槽使用的接收缓存[*默认*]{.emphasis}大小的内核参数。要配置此参数，请运行以下命令：
    :::
    ``` programlisting
    sysctl -w net.core.rmem_default=N
    ```
    ::: para
    使用所需缓存大小以字节为单位替换
    `N`{.command}。要确定这个内核参数值请查看
    `/proc/sys/net/core/rmem_default`{.filename}。请记住
    `rmem_default`{.command} 值不得大于
    `rmem_max`{.command}）；如果需要请增大 `rmem_max`{.command} 值。
    :::
[SO_RCVBUF]{.term}
:   ::: para
    控制插槽接收缓存[*最大值*]{.emphasis}的插槽选项，单位为字节。有关
    `SO_RCVBUF`{.command} 的详情请参考其 man
    page：`man 7 socket`{.command}。
    :::
    ::: para
    要配置 `SO_RCVBUF`{.command}，请使用 `setsockopt`{.command}
    工具，您可以使用 `getsockopt`{.command} 查询当前
    `SO_RCVBUF`{.command} 值。有关这两个工具的详情请参考
    `setsockopt`{.command} man page: `man setsockopt`{.command}。
    :::
:::
:::
:::
::: section
::: titlepage
# [⁠]{#main-network.html#s-network-packet-reception}8.3. 数据包接收概述 {.title}
:::
::: para
为更好地分析网络瓶颈和性能问题，您需要直到数据包接收的原理。数据包接收对网络性能调节来说很重要，因为接收路径是经常会丢帧的地方。在接收路径丢帧可能会造成对网络性能的极大负面影响。
:::
::: figure
[⁠]{#main-network.html#packet-reception-png}
::: figure-contents
::: mediaobject
![网络接收路径图表](images/packet-reception.png)
:::
:::
**图 8.1. 网络接收路径图表**
:::
::: para
Linux 内核接收每一帧，并将其送入四步处理过程：
:::
::: {.orderedlist xmlns:d="http://docbook.org/ns/docbook"}
1.  ::: para
    [*硬件接收*]{.emphasis}：*网卡*（NIC）接收传送的帧。根据其驱动程序配置，NIC
    可将帧传送到内部硬件缓冲内存或者指定的环缓存。
    :::
2.  ::: para
    [*Hard IRQ*]{.emphasis}：NIC 通过中断 CPU 插入网络帧。这样可让 NIC
    驱动程序意识到该中断并调度 *soft IRQ 操作*。
    :::
3.  ::: para
    [*Soft IRQ*]{.emphasis}：这个阶段采用实际接收进程，并在
    `softirq`{.command} 环境中运行。就是说这个阶段会预先清空所有在指定
    CPU 中运行的程序，但仍允许插入 hard IRQ。
    :::
    ::: para
    在这个环境中（与 hard IRQ 在同一 CPU
    中运行，以便尽量减少锁定消耗），该内核会删除 NIC
    硬件缓存以及它通过网络栈的进程中的帧。从那里开始，可将帧转发、忽略或者传递给目标侦听插槽。
    :::
    ::: para
    传递给插槽后，该帧就会被附加到拥有该插槽的程序中。这个过程会以互动方式进行直到
    NIC
    硬件缓存超出帧外，或者直到达到*设备加权*（`dev_weight`{.command}）。有关设备加权的详情请参考
    [第 8.4.1 节 "NIC
    硬件缓冲"](#main-network.html#s-network-commonque-nichwbuf){.xref}。
    :::
4.  ::: para
    [*程序接收*]{.emphasis}：程序接受帧并使用标准 POSIX
    调用（`read`{.command}, `recv`{.command},
    `recvfrom`{.command}）从任意拥有的插槽中退出队列。此时从网络中接收到的数据不再存在于网络栈中。
    :::
:::
::: simplesect
::: titlepage
## [⁠]{#main-network.html#idm140329771413040}CPU/缓存亲和性 {.title}
:::
::: para
要维护接收路径的高流量，建议您让 L2
缓存处于[*热*]{.emphasis}状态。如前所述，网络缓冲由作为 IRQ
的显示其存在的同一 CPU 接收。就是说该缓存数据将位于接收 CPU 的 L2
缓存中。
:::
::: para
要利用这个功能，请在要接收共享 L2 缓存同一核的 NIC