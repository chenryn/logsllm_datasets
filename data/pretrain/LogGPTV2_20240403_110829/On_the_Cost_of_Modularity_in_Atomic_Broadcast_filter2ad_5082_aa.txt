title:On the Cost of Modularity in Atomic Broadcast
author:Olivier R&quot;utti and
Sergio Mena and
Richard Ekwall and
Andr&apos;e Schiper
On the Cost of Modularity in Atomic Broadcast
Olivier R¨utti†
olivier.rutti@epﬂ.ch
Sergio Mena‡
Richard Ekwall†
PI:EMAIL
nilsrichard.ekwall@epﬂ.ch
Andr´e Schiper†
andre.schiper@epﬂ.ch
† ´Ecole Polytechnique F´ed´erale de Lausanne (EPFL), 1015 Lausanne, Switzerland
‡Department of Computer Science, University of York, York YO10 5DD, United Kingdom
Abstract
Modularity is a desirable property of complex software
systems, since it simpliﬁes code reuse, veriﬁcation, main-
tenance, etc. However, the use of loosely coupled modules
introduces a performance overhead. This overhead is often
considered negligible, but this is not always the case. This
paper aims at casting some light on the cost, in terms of per-
formance, that is incurred when designing a relevant group
communication protocol with modularity in mind: atomic
broadcast.
We conduct our experiments using two versions of
atomic broadcast: a modular version and a monolithic one.
We then measure the performance of both implementations
under different system loads. Our results show that the
overhead introduced by modularity is strongly related to
the level of stress to which the system is subjected, and in
the worst cases, reaches approximately 50%.
Keywords: atomic broadcast, modular design, microproto-
cols, performance cost, experimental evaluation
1
Introduction
Modularity has always been an important concern when
designing complex software systems. A modular system is
easier to maintain, its code being easier to debug, verify,
reuse and develop in a collaborative environment. How-
ever, modularity is not a panacea and its main drawback is
the performance overhead introduced by splitting the sys-
tem into several independent parts. Such overhead is often
deemed negligible when compared to all the good proper-
ties that modularity entails; but it is usually difﬁcult to per-
form a quantitative analysis of the actual performance im-
pact.
Group communication has been argued to be an im-
portant enabling technology to render a distributed service
fault-tolerant by replicating such service at several loca-
tions [5, 8].
In this context, atomic broadcast is a well-
known protocol that allows to maintain replicas consistency
by ensuring a total order of message delivery. In [13, 7],
Chandra and Toueg propose a reduction of this protocol to
the consensus problem. This allows a modular design of
atomic broadcast based on consensus and reliable broad-
cast.
In such a design, atomic broadcast knows that it is
interacting with a consensus module, but cannot make any
assumption on the implementation of the consensus module
(e.g., which algorithm is used). As a result, some algorith-
mic optimizations that make assumptions on the neighbor
protocol can not take place if the system is to be modular:
this is where the performance penalty is mostly located.
Is it not so easy to decide between a modular design or a
monolithic one: this decision has to be made at the early
stages of the software engineering process, whereas evi-
dence of the performance cost can only be obtained later,
when at least a prototype is available. Nevertheless, it is
possible to foresee the performance hit at design time using
an analytical method (See Sect. 5.2).
Contribution. This paper aims at shedding some light on
the performance cost that modularity induces in implemen-
tation of atomic broadcast reduced to the consensus prob-
lem. For our experiments, we use Fortika [18, 19], a toolkit
that includes two versions of atomic broadcast: monolithic
and modular. Both versions are based on the same algo-
rithms. In one version atomic broadcast is implemented as
a set of modules, whereas in the other, these modules are
merged to form a monolithic protocol. This merging allows
algorithmic optimizations, since we can assume that these
modules always operate together. Those optimizations aim
at (a) improving the performance in good runs (runs where
messages are timely and processes behave correctly1), and
(b) keep algorithmic correctness in all runs. For a fair com-
parison, we also optimize modular version of atomic broad-
cast.
1Good runs are the most frequent in practice
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007The performance of both modular and monolithic solu-
tions are then shown in both analytical and experimental
evaluations of the two stacks.2 Our results reveal that the
performance hit can reach 50% in some cases, showing that
the dilemma between a monolithic and a modular design
should not be taken lightly.
Reliable Broadcast. This protocol deﬁnes the primitives
rbcast and rdeliver. Reliable broadcast ensures that mes-
sages are rdelivered either by all correct processes or by
none, even if the sender crashes while rbcasting a message.
However, it does not enforce any order in rdelivered mes-
sages.
2 Atomic Broadcast
This section brieﬂy presents the system model that we
consider and concisely describes the modules that constitute
the atomic broadcast stack.
2.1 System Model
We consider a system with a ﬁnite set of processes
Π = {p1, p2, . . . , pn}. The system is asynchronous, which
means that there is no assumption on message transmission
delays or relative speed of processes. The system is static,
which means that the set Π of processes never changes after
system start-up time. During system lifetime, processes can
take internal steps or communicate by message exchange.
Correct, Faulty and Failure Suspicion. Processes can
only fail by crashing. A process that crashes stops its op-
eration permanently and never recovers. A process is faulty
in a given run if it crashes in that run. A process is correct if
it is not faulty. Every process has a local module called fail-
ure detector (FD) that outputs a set of processes that have
crashed. This list can change over time, moreover it can be
inaccurate. We say that process p suspects process q if q is
in the output list of p’s FD.
Quasi-Reliable Communication Channels. Every pair
of processes is connected by a bidirectional network chan-
nel. The protocols presented later on assume quasi-reliable
channels, which verify the following property. If process p
sends message m to q, and both p and q are correct, then q
eventually receives m.
2.2 Description of Modules
Our atomic broadcast implementation consists of three
main protocols that are based on well-established algo-
rithms:
reliable broadcast, consensus and atomic broad-
cast. We now give a concise description of these protocols
(see [13] for further details and formal speciﬁcations).
2We use the terms “stack” and “implementation” interchangeably
Consensus. Consensus deﬁnes the primitives propose and
decide, which mark the protocol’s start and end at a given
process. Consensus ensures that processes eventually reach
an agreement on a value proposed by one of them, even in
the presence of crashes.
Atomic Broadcast. This protocol deﬁnes the primitives
abcast and adeliver. Atomic broadcast is a stronger form of
reliable broadcast where all messages are adelivered in the
same order at every process.
3 Modular Implementation
The current section describes the modular implemen-
tation of atomic broadcast (see Fig. 1, left). We present
the implementation of all modules following a bottom-up
order. These modules implement the protocols described
in Sect. 2.2. Detailed knowledge of these implementa-
tions is not necessary to keep up with the rest of the pa-
per. However, a succinct description will help the reader
to better understand (1) the monolithic implementation pre-
sented in Sect. 4 and (2) the analytical evaluation presented
in Sect. 5.2.
For each module, we present some optimizations that fo-
cus on good runs (runs with no suspicion, crash or unusual
message delay). Our optimizations, however, do not affect
the correctness of the algorithms in runs that are not good.
These improvements are necessary to obtain a comparison
as fair as possible, between the modular and monolithic
stacks.
Figure 1. Modular implementation(left) and
monolithic implementation(right) of Atomic
Broadcast (ABcast).
RBcastConsensusABcastNetworkApplicationABcastNetworkApplicationConsensusRBcast++37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 20073.1 Reliable Broadcast (RBcast)
The classical implementation of this protocol is straight-
forward if we can assume quasi-reliable channels (see
Sect. 2.1). Here is the main idea [7]:
1. Upon broadcast of message m, send a copy of m to all
processes.
2. Upon receiving m for the ﬁrst time, re-send m to all
processes.
Optimization. Note that this implementation sends n2
messages over the network for each rbcast message (n de-
notes the number of processes to which the message is
broadcast). This can be reduced by assuming that a ma-
jority of processes do not crash3. This optimization leads to
only (n − 1) · (b n−1
2 c + 1) messages per rbcast message.
The details of this optimization are omitted here.
3.2 Consensus
We base our implementation on the Chandra and Toueg
consensus algorithm [7] due to its overall good perfor-
mances [25]. Rather than presenting the full algorithm’s
details, we explain its principles by using a typical run, de-
picted in Fig. 2. The algorithm proceeds in a number of
asynchronous rounds.
In each round, a different process
adopts the role of coordinator. A round consists of four
phases:
4. Decide phase. If the coordinator gathers ack messages
from a majority of processes, it decides and rbcasts
the decision to all processes. The last phase in Fig. 2
(grayed) is the re-send part of rbcast algorithm (see re-
liable broadcast implementation above in this section).
If the coordinator is faulty and/or suspected, the algo-
rithm may not be able to decide in the ﬁrst round. In that
case, supplementary rounds with the same phases would be
needed in order to terminate. At any moment, if a running
process receives a decision, it decides the received value
and terminates. In runs where there are no crashes or suspi-
cions, all processes are able to decide at the end of the ﬁrst
round (see dark upward triangles in Fig. 2).
Optimization. Figure 3 shows a typical run of the con-
sensus algorithm that we implemented. Firstly, we reduce
the ﬁrst round by suppressing the estimate phase. Secondly,
contrary to classical implementation where round n+1 be-
gins immediately after round n terminates, a new round
starts only if the coordinator is suspected to be faulty. These
two improvements were previously described in [25]. Fi-
nally, we reduce the size of decision messages by sending a
tag DECISION instead of the complete decision. Note that,
even if this optimization works ﬁne in good runs, additional
communication steps may be required if the coordinator
crashes.
Figure 3. Example execution of optimized
consensus during good runs.
Figure 2. Example execution of consensus
during good runs.
3.3 Atomic Broadcast (ABcast)
1. Estimate phase. All processes send their initial value
as estimate to the coordinator.
2. Propose phase. The coordinator chooses the eldest
value and sends a propose message with such value.
3. Ack phase. All processes wait for the coordinator’s
proposal and send an ack message when they receive
it, or a nack message if they suspect the coordinator.
3The same assumption is necessary to solve consensus
We solve atomic broadcast by reduction to consen-
sus [13, 7]. In this approach, the atomic broadcast module
diffuses all messages abcast by the application. In paral-
lel, a consensus is started to decide on the delivery order
of those messages. Hence, consensus accepts a batch of
messages as initial values. Figure 4 depicts an example ex-
ecution where messages m (abcast by p’s application) and
m0 (abcast by r’s application) are abcast. First, both mes-
sages are disseminated to all processes; then, an instance
of consensus is started to order m and m0 consistently at
all processes. When consensus decides, atomic broadcast
timeqrp(coord)proposalestimateackdecisiondecide(v)decide(v)decide(v)propose(v)propose(v’)propose(v’’)rbcast(v)37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007adelivers the messages contained in the decision in some
deterministic order. In Fig. 4 for instance, m0 happens to be
ordered before m, but this order is consistent everywhere.
Finally, the whole mechanism is repeated as soon as further
messages are abcast.
Optimization. Note that in [13, 7], reliable broadcast is
used to disseminate the messages abcast by the applica-
tion. In our stack, messages are simply sent using quasi-
reliable channels (solid arrows in Fig. 4). This implemen-
tation is clearly equivalent to reliable broadcast when no
process crashes. Otherwise, it may violate the speciﬁcation
of atomic broadcast. Consider for instance a message m ab-
cast by process p. If p crashes while sending a copy of m
to all processes, m may be delivered at some processes but
not at others. This violates reliable broadcast’s speciﬁca-
tion (see [13]). Moreover, in this example, it may also lead
to a violation of atomic broadcast’s speciﬁcation. To avoid
this in our implementation (and thus ensure correctness), if
a process q does not receive messages during a period of t
seconds (with t sufﬁciently big), q starts a consensus even
if no message arrives.
Figure 4. Example run of atomic broadcast by
reduction to consensus.
4 Monolithic Implementation
In the previous sections, we have presented the algo-
rithms (and optimizations) as they are implemented in the
modular atomic broadcast stack. When we implement these
algorithms as a single module in a monolithic stack, further
(algorithmic) optimizations are possible. In this section, we
present the optimizations that were carried out in the mono-
lithic stack (see Fig. 1, right). Again, our optimizations fo-
cus on good runs but ensure correctness in all runs.
For each of these optimizations we explain (1) what
changes are made compared to the modular version of
atomic broadcast (see Section 3),
(2) why these changes
are possible, and (3) what (approximate) improvement in
performance can be expected from these changes.
Figure 5. Consecutive consensus executions
in the modular implementation of atomic
broadcast.
4.1 Combining the Next Proposal with
the Current Decision
In the modular implementation of atomic broadcast (see
Fig. 4), atomic broadcast runs a sequence of consecutive
consensus instances to order the set of undelivered mes-
sages. Due to the modular design, all consensus instances
are black boxes from the point of view of atomic broad-
cast and are considered to be totally independent from each
other. Thus, we cannot take advantage of the fact that the
coordinator that sends a decision in consensus instance k
is the same coordinator that sends a proposal in consensus
instance k + 1. Figure 5 shows this. Note that in normal ex-
ecutions, process p does not necessarily wait until processes