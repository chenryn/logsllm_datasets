connecting the processors executing the source and desti-
nation operations. At the end, all the comms assigned to
the same communication unit are statically scheduled. The
comms are thus totally ordered over each communication
medium. Provided that the network preserves the integrity
and the ordering of messages, this total order of the comms
guarantees that data will be transmitted correctly between
processors. The obtained schedule also guarantees a dead-
lock free execution.
The strategy used to schedule operations ensures a mini-
mum run-time overhead in the faulty system (a system pre-
senting at least one failure) by using S(n)
worst(o, p) to give
priority to operations and S(n)
best(o, p) to schedule operations.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:08:19 UTC from IEEE Xplore.  Restrictions apply. 
4.3. An Example
We have implemented our fault-tolerant heuristic in the
SYNDEX [21] tool, which is a tool for optimizing the im-
plementation of real-time embedded applications on multi-
component architecture.
We apply our heuristic to the example of Figure 2. The
user requires the system to tolerate one permanent proces-
sor failure, i.e., Npf = 1. The execution characteristics of
each comp/mem/extio and comm are speciﬁed by the two
tables of time units given in Section 3.4.
After the ﬁrst two steps of our heuristic, we obtain the
temporary schedule of Figure 5.
faulty execution. For instance, suppose that P1 crashes at
time0 (see Figure 8). Since we have assumed a fail-silent
model, P1 fails to produce its expected results, the output of
(cid:1)I, P1(cid:2) that should have been sent to (cid:1)A, P3(cid:2) and the output
of (cid:1)C, P1(cid:2) that should have been sent to (cid:1)F, P2(cid:2). Therefore
the failure of P1 can actually be detected by P3 only after
the expected completion date of the comm from (cid:1)I, P1(cid:2) to
(cid:1)A, P2(cid:2). Detecting P1’s failure is useful in order to avoid
sending further comms to P1, but functionally, we do not
need it: indeed, the static schedule transparently tolerates
one failure since Npf = 1. Actually, it is not entirely trans-
parent since the resulting schedule has a greater execution
time.
Figure 5. Step 2
In the next step, operation C is scheduled. Assigning C
to P 1, P 2 and P 3 gives an expected schedule pressure of
9.73, 10.53 and 9.23 respectively. But, if A, the LIP of C, is
duplicated to P 3, the schedule pressure of C can be reduced
to 5.73, which means that the start time of C is also reduced.
We therefore schedule a new replica of A on P 3 and two
replicas of C on P 3 and P 1, which minimizes the schedule
pressure. As shown in Figure 6, operation A receives its
inputs data twice from the replicas of I scheduled on P 1
and P 2, and the start time of A is the end of the earliest
communication between (cid:1)I, A(cid:2) on {L1 3} and {L2 3}. We
obtain therefore the temporary schedule of Figure 6.
Figure 7. Final fault-tolerant schedule
Figure 8 shows the schedule when P1 crashes. As ex-
pected, the data sent by all the comms toward the faulty
processor P1 are discarded. The schedule corresponding to
the subsequent iterations is the same except that all the oper-
ations scheduled on P1 as well as all the comms from and to
P1 have disappeared. Finally, the real-time constraint is still
satisﬁed since the total time is respectively 15.35, 15.05,
12.6 when P1, P2, or P3 fails at time 0.
Figure 6. Step 3
Similarly, operations B, D, E, F, G, and O are scheduled.
At the end of our heuristic, we obtain the ﬁnal schedule pre-
sented in Figure 7. Each operation of the algorithm graph
is replicated at least twice and these replicas are assigned
to different processors. More important, the real-time con-
straint is satisﬁed since the total time is 15.05 < Rtc.
Figure 7 shows that some communications are not useful
in the absence of failures. For example, the communication
of the result of the operation (cid:1)I, P2(cid:2) to the operation (cid:1)A, P3(cid:2)
is not used since the result sent by (cid:1)I, P1(cid:2) arrives ﬁrst. How-
ever, these communications may become useful during a
Figure 8. Timed execution when P1 crashes
4.4. Analysis of the Example
To evaluate the overheads introduced by the fault-
tolerance, let us consider the non fault-tolerant schedule
produced for our example with a basic scheduling heuris-
tic (for instance the one of SYNDEX). The schedule length
generated by this heuristic is 10.7.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:08:19 UTC from IEEE Xplore.  Restrictions apply. 
Neither this non fault-tolerant schedule nor the fault-
tolerant schedule of Figure 7 are the best possible. Remem-
ber that ﬁnding the best schedule is an NP-hard problem;
this is the reason why we have designed a heuristic schedul-
ing algorithm.
In this particular case, the fault-tolerance
overheads is therefore 15.05 − 10.7 = 4.35.
In the fault-tolerant schedule, some communications
take place although they are not necessary. On the other
hand, the response time of the faulty system is minimized,
since results are sent without waiting for any timeout (see
Figure 8). For the same reason, the system supports the ar-
rival of several failures during the same iteration since there
are no risks that the sum of pending timeouts overtakes the
desired real-time constraints.
In other words, we do not
need to make any assumptions on the failure inter-arrival
time.
This solution is appropriate to an architecture where the
communication means are point-to-point links, which al-
low parallel communications to take place. For multi-point
links, the overheads introduced by the replication of comms
may be too high because of their serialization on a single
link.
5. Runtime Behavior
In our heuristic, Npf faults can be tolerated by schedul-
ing Npf +1 replicas for each operation on different proces-
sors. We assume in this paper that all values returned by
the Npf +1 replicas of any given input operation are iden-
tical in the same iteration. If no fault occurs, each of the
Npf +1 replicas of an operation receives its inputs in par-
allel from all the replicas of its predecessor operations in
the data-ﬂow graph; as soon as it receives the ﬁrst set, the
operation is executed and ignores the later Npf inputs. If
there are k permanent faults (k ≤ Npf ), each replica of an
operation scheduled on a non-faulty processor receives its
inputs in parallel from all the replicas of its predecessors
operations scheduled on a non-faulty processors; as soon as
it receives the ﬁrst set, the operation is executed and ignores
the later inputs. Concerning the failure detection, there are
two options:
1. Either we do not perform any failure detection, in
which case, after a failure, the remaining processors
will continue to send results to the faulty one. This
will not help reducing the communication overheads,
especially when the comms have to be serialized over
a multi-point communication link. The advantage is
that if a processor experiences an intermittent failure,
then since it will continue to receive inputs from the
healthy processors, it will be able to produce its results
again when recovering from its intermittent failure.
2. Or we perform a failure detection procedure by know-
ing at what time each comm is supposed to happen,
and by deciding accordingly that when a comm did not
happen,then the sending processor is faulty. Each pro-
cessor can therefore maintain an array of faulty pro-
cessors and avoid further comms to the faulty proces-
sors in both the remaining of the transient iteration and
the subsequent iterations. The drawback is that an in-
termittent failure cannot be recovered. Indeed, when
a processor is detected to be faulty, the other healthy
processors will update their array of faulty processors,
and will not send any more data during the subsequent
iterations. So even if this faulty processor comes back
to life, it will not receives any inputs and will not be
able to perform any computation. Therefore, in the
subsequent iterations, it will fail to send any data on its
communication links, and the other healthy processors
will never be able to detect that it came back to life.
The same applies to failure detection mistakes.
The choice between these two options can be left to the
user. It will depend on the intermittent failure rate of the
application as well as on the topology and the bandwidth of
the network.
6. Performance Evaluation
To evaluate our faut-tolerant scheduling heuristic, we
have compared the performance of the proposed algorithm
with the algorithm proposed by Hashimoto and al. in [16],
called HBP (Height-Based Partitioning) which is the clos-
est to FTBAR that we have found in the literature. Since,
HBP assumes homogeneous systems and only use software
redundancy of the algorithm’s operations, FTBAR is down-
graded to these assumptions to make the comparison mean-
ingful. The goal of our simulations is to compare the fault-
tolerance overheads of HBP and FTBAR, both in the ab-
sence and in the presence of one processor failure.
6.1. Simulation Parameters
We have applied FTBAR and HBP heuristics to a set of
random algorithm graphs with a wide range of parameters.
A random algorithm graph is generated as follows: Given
the number of operations N , we randomly generate a set of
levels with a random number of operations. Then, opera-
tions at a given level are randomly connected to operations
at a higher level. The execution times of each operation
are randomly selected from a uniform distribution with the
mean equal to the chosen average execution time. Similarly,
the communication times of each data dependency are ran-
domly selected from a uniform distribution with the mean
equal to the chosen average communication time.
For generating the complete set of algorithm graphs,
we vary two parameters: N = 10, 20, ..., 80, and the
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:08:19 UTC from IEEE Xplore.  Restrictions apply. 
communication-to-computation ratio, deﬁned as the aver-
age communication time divided by the average computa-
tion time, CCR = 0.1, 0.5, 1, 2, 5, 10.
CCR ≥ 2, FTBAR performs signiﬁcantly better than HBP
(by at least 20%). This is due to our schedule pressure
which tries to minimize the length of the critical path.
6.2. Performance Results and Analysis
We present in this Section the performance results on the
fault-tolerance heuristic. We compute the fault-tolerance
overhead in the following way:
(F T SL) − (non F T SL)
× 100
Overheads =
(F T SL)
produced by FTBAR with Npf = 0.
where the non FTSL (Fault Tolerant Schedule Length) is
We have plotted in Figures 9 and 10 the average fault-
tolerance overheads (averaged over 60 random graphs) as a
function of N and CCR, both in the absence (Figure 9(a)
and 10(a)) and in the presence of one processor failure (Fig-
ure 9(b) and 10(b); here we have computed the average
overheads when each of the four processors fails, and plot-
ted the max overheads over these four processors).
Figure 10. Impact of the communication-to-
computation ratio for Npf = 1, P = 4 and
N = 50
The time complexity of FTBAR is less than the time
complexity of HBP. The reason is that HBP investigates
more possibilities than FTBAR when selecting the proces-
sor for a candidate operation.
7. Conclusion and Future Work
The literature about fault-tolerance of distributed and/or
embedded real-time systems is very abundant. Yet, there
are few attempts to combine fault-tolerance and automatic
generation of distributed code for embedded systems.
In this paper, we have studied this problem and proposed
a software implemented fault-tolerance solution.
We have proposed a new scheduling heuristic, called FT-
BAR (Fault-Tolerance Based Active Replication), that pro-
duces automatically a static distributed fault-tolerant sched-
ule of a given algorithm on a given distributed architec-
ture. Our solution is based on the software redundancy of
both the computation operations and the communications.
All replicated operations send their results but only the one
which is received ﬁrst by the destination processor is used;
the other results are discarded. The implementation uses a
Figure 9. Impact of the number of operations
for Npf = 1, P = 4 and CCR = 5
Figure 9 shows that average overheads increases with N .
This is due to the active replication of all operations and
communications. Figure 9 also shows that FTBAR perform
better than HBP.
Figure 10 shows that, when the average communication
time is strictly greater than the average execution time, the
average overheads decrease. For CCR ≤ 1, there is lit-
tle difference between HBP and FTBAR. In contrast, for
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:08:19 UTC from IEEE Xplore.  Restrictions apply. 
scheduling heuristic for optimizing the critical path of the
distributed algorithm obtained. It is best suited to architec-
tures with point-to-point links. There are some communi-
cation overheads, but on the other hand, several failures in
a row can be tolerated. Also, depending on the failure de-
tection mechanism chosen, intermittent failures can be tol-
erated as well.
We have implemented our FTBAR heuristic within the
SYNDEX tool. SYNDEX is able to generate automatically
executable distributed code, by ﬁrst producing a static dis-
tributed schedule of a given algorithm on a given distributed
architecture, and then by generating a real-time distributed
executive implementing this schedule. We have also imple-
mented the HBP (Height-Based Partitioning [16]) heuristic.
Although HBP only considers homogeneous architectures
and only tolerates one processor failure, it is the closest to
our work that we have found in the literature. The experi-
mental results shows that FTBAR performs better than HBP,
both in the absence and in the presence of failures.
Currently, we are performing extensive benchmark test-
ing of FTBAR on heterogeneous architectures. The ﬁrst re-
sults show that the overheads increases with the number of
failures Npf .
Finally, our solution can only tolerate processor failures.
We are currently working on new solutions to take commu-
nication link failures and reliability into account. We also
plan to experiment our method on an electric autonomous
vehicle, with a 5-processor distributed architecture.
Acknowledgments
The authors would like to thank C˘at˘alin Dima, Thierry
Grandpierre, Claudio Pinello, and David Powell for their
helpful suggestions.
References
[1] I. Ahmad and Y.-K. Kwok. On exploiting task duplication
in parallel program scheduling.
In IEEE Transactions on
Parallel and Distributed Systems, volume 9, pages 872–892,
September 1998.
[2] G. Berry and A. Benveniste. The synchronous approach to
reactive and real-time systems. Proceedings of the IEEE,
79(9):1270–1282, September 1991.
[3] A. Bertossi and L. Mancini. Scheduling algorithms for
fault-tolerance in hard-real-time systems. Real-Time Sys-
tems Journal, 7(3):229–245, 1994.
[4] A. Bertossi, L. Mancini, and F. Rossini.
Fault-tolerant
rate-monotonic ﬁrst-ﬁt scheduling in hard real-time systems.
IEEE Trans. on Parallel and Distributed Systems, 10:934–
945, 1999.
[5] M. Caccamo and B. Buttazzo. Optimal scheduling for fault-
tolerant and ﬁrm real-time systems.
In 5th International
Conference on Real-Time Computing Systems and Applica-
tions. IEEE, Oct. 1998.
[6] P. Chevochot and I. Puaut. Scheduling fault-tolerant dis-
tributed hard real-time tasks independently of the replication
strategie. In The 6th International Conference on Real-Time
Computing Systems and Applications (RTCSA’99), pages
356–363, HongKong, China, December 1999.
[7] J.-Y. Chung, J. Liu, and K.-J. Lin. Scheduling periodic jobs
IEEE Trans. on Computers,
that allow imprecise results.
39(9):1156–1174, September 1990.
[8] C. Dima, A. Girault, C. Lavarenne, and Y. Sorel. Off-line
real-time fault-tolerant scheduling. In 9th Euromicro Work-
shop on Parallel and Distributed Processing, PDP’01, pages
410–417, Mantova, Italy, February 2001.
[9] G. Fohler. Adaptive fault-tolerance with statically scheduled
In Euromicro Workshop on Real-Time
real-time systems.
Systems, EWRTS’97, Toledo, Spain, June 1997. IEEE.
[10] M. Garey and D. Johnson. Computers and Intractability, a
Guide to the Theory of NP-Completeness. W. H. Freeman
Company, San Francisco, 1979.
[11] S. Ghosh. Guaranteeing Fault-Tolerance through Schedul-
ing in Real-Time Systems. PhD Thesis, University of Pitts-
burgh, 1996.
[12] A. Girault, C. Lavarenne, M. Sighireanu, and Y. Sorel. Fault-
tolerant static scheduling for real-time distributed embedded
systems.
In 21st International Conference on Distributed
Computing Systems, ICDCS’01, pages 695–698, Phœnix,
USA, April 2001. IEEE. Extended abstract.
[13] A. Girault, C. Lavarenne, M. Sighireanu, and Y. Sorel. Gen-
eration of fault-tolerant static scheduling for real-time dis-
tributed embedded systems with multi-point links. In IEEE
Workshop on Fault-Tolerant Parallel and Distributed Sys-
tems, FTPDS’01, San Francisco, USA, April 2001. IEEE.
[14] M. Gupta and E. Schonberg. Static analysis to reduce syn-
chronization cost in data-parallel programs. In 23rd Sympo-
sium on Principles of Programming Languages, pages 322–
332, January 1996.
[15] N. Halbwachs. Synchronous Programming of Reactive Sys-
tems. Kluwer Academic, 1993.
[16] K. Hashimoto, T. Tsuchiya, and T. Kikuno.
Effective
scheduling of duplicated tasks for fault-tolerance in multi-
processor systems. IEICE Transactions on Information and
Systems, E85-D(3):525–534, March 2002.
[17] P. Jalote. Fault-Tolerance in Distributed Systems. Prentice
Hall, Englewood Cliffs, New Jersey, 1994.
[18] X. Qin, Z. Han, H. Jin, L. P. Pang, and S. L. Li. Real-
time fault-tolerant scheduling in heterogeneous distributed
systems.
In Proceeding of the International Workshop on
Cluster Computing-Technologies, Environments, and Appli-
cations (CC-TEA’2000), Las Vegas, USA, June 2000.
[19] K. Ramamritham. Allocation and scheduling of precedence-
IEEE Trans. on Parallel and Dis-
related periodic tasks.
tributed Systems, 6(4):412–420, April 1995.
[20] J. Rushby. Critical system properties: Survey and taxonomy.
Reliability Engineering and Systems Safety, 43(2):189–219,
1994. Research Report CSL-93-01.
[21] A. Vicard. Formalisation et Optimisation des Systmes In-
formatiques Distribus Temps-Rel Embarqus. PhD Thesis,
University of Paris XIII, July 1999.
[22] T. Yang and A. Gerasoulis. List scheduling with and without
communication delays. Parallel Computing, 19(12):1321–
1344, 1993.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:08:19 UTC from IEEE Xplore.  Restrictions apply.