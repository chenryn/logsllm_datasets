netDB
3
9
2
0
1
Fig. 1. Deanonymizing attack
4 Evaluation
In this section, we describe our experiments conﬁrming the attacks described
in the previous section. We have made sure to not disrupt any participant in
the I2P network apart from our own nodes and no identifying information has
been collected about other participants in the network. For testing the DoS at-
tack, which we describe ﬁrst, a special, separated test network was created to
prevent any harm on the real network. All other attacks were tested in the real
I2P network.
4.1 Floodﬁll Takeover
We discuss the impact of a takeover attack and the time needed for a passive
takeover where the attacker only waits for automatic floodfill nodes to resign
due to normal ﬂuctuations in the network.
The fraction of automatic floodfill nodes in the network was determined
by monitoring the local peer storage on the routers under our control. These
routers participated as floodfill nodes in the real I2P network, and logged
whenever a node removed or added the floodfill ﬂag to its peer information.
Automatic floodfill nodes add the floodfill status only after being online
for at least two hours and can lose and regain floodfill status depending on
network load. Manual floodfill nodes, instead, will always have the ﬂoodﬁll
ﬂag set. Over a period of ten days, we saw a total of 597 floodfill nodes and an
average of 413 floodfill nodes each day. During these days, only 128 of them
did not change their ﬂoodﬁll status. Therefore, a passive floodfill takeover
attempt lasting for ten days would leave 128 legitimate nodes in place while
adding 258 malicious nodes.
As seen in Figure 2, the amount of floodfill nodes never losing floodfill
status decreases almost linearly by ﬁve nodes every day, until it reaches 26
nodes after 44 days. From there on, the count remains stable, and after 60 days,
still 25 nodes are left. These are likely to be manual floodfill nodes, which
would also not have resigned in a DoS attack.
Fig. 2. Legitimate ﬂoodﬁll nodes after n days
As the active floodfill takeover uses a DoS attack on target nodes, we de-
cided to test this attack on a closed local network. The test network consisted
of 100 nodes split into ﬁve groups: 30 slower users with default data rate con-
ﬁguration (96kB/s down- and 40kB/s upload), 30 faster users conﬁgured to
use up to 200kB of data rate in both directions, 20 automatic floodfill nodes,
and 5 manual floodfill nodes, as well as 15 attackers. To simulate a large-
enough number of floodfill nodes, a larger fraction of peers were conﬁgured
as floodfill nodes, and the maximum number of active floodfill nodes was
lowered from 300 to 20. In this setup, a group of ﬁve attacking nodes was able
to slow down the attacked nodes enough for them to give up floodfill status.
4.2 Experimental Setup
In this section, we describe the setup used for all the following attacks. All of
these attacks have been successfully tested on the real I2P network. All nodes
being attacked were controlled by us.
We ran 20 attacking nodes connected to the normal I2P network. These
nodes acted as floodfill peers. Six additional nodes served as legitimate
peers, and were used to verify the attacks. All attackers were set up on a sin-
gle VM host in the US and conﬁgured to use 128kB/s of download and 64kB/s
of upload data rate. The legitimate nodes were split evenly between the VM
host in the US and a second VM host in Europe (to make sure the results do
not rely on proximity between attackers and victims). Attackers were conﬁg-
ured to act as manual floodfill nodes and had additional code added, which
logged network events and allowed for the blacklisting of speciﬁc information,
as required by the Eclipse attack.
During our experiments, the I2P statistics4 reported between 18,000 and
28,000 nodes and 320 to 350 floodfill nodes, ﬂuctuations during the day.
4 http://stats.i2p.in
 0 20 40 60 80 100 1201012141618202224262830323436384042444648505254565860NodesDaysnodesTherefore, we were controlling less than 7 % of floodfill nodes and a neg-
ligible part of total nodes.
4.3 Sybil Attack
To test our Sybil attacks, we created a set of 50,000 precomputed router iden-
tities. Each identity consists of one signing and one encryption key (as well as
a certiﬁcate, which is unused). Computing this set of identities took less than
30 minutes on a twelve-core Xeon server. We then made this set of identities
available to all our I2P nodes for the following experiments.
Additionally, we modiﬁed the router software to enable our attacking
nodes to change their identity to any of the precomputed ones on demand,
as well as to enable a group of attackers to use a set of identities, one per node,
close to a target.
4.4 Eclipse Attack
To evaluate the Eclipse attack, we conﬁgured our victims to download a test
eepsite every minute, and log the results. Ten attack nodes were moved to the
storage location of the service information for the test eepsite. The attackers
were conﬁgured to give negative response to all lookups for the test eepsite
and only refer to each other in these negative responses such that the victims
would learn about all malicious floodfill nodes as fast as possible. A second
group of ten attack nodes was moved to the test eepsite’s storage location for
the following day, and was conﬁgured to keep the service information unavail-
able across the keyspace shift.
We ran the Eclipse attack over a period of 42 hours. During this time, victims
were on average able to reach the blocked eepsite for a total of ﬁve minutes.
Three out of six nodes were not able to reach the eepsite at any point in time,
and the most successful victim was able to interact with the destination for a
total of only 16 minutes during that period. When the second set of attackers
was not used, all victims could successfully reach the eepsite during a 15-
minute window around midnight (when the keyspace rotation happens).
4.5 Deanonymization of Users
In the next step, we ran an experiment that simulates our ability to deanony-
mize a particular victim user Alice, who is accessing a speciﬁc resource R of
interest. This resource could be a dissident’s web page or a sensitive ﬁle. The
idea is that the attacker knows that resource and tries to determine whether a
user under suspicion actually accesses R.
For the simulated attack, we ﬁrst conﬁgured ten malicious nodes and set
them up as floodfill nodes in the keyspace region occupied by our six victim
nodes. We then conﬁgured these six victim nodes to repeatedly query our test
eepsite. In a ﬁrst step, we wanted to understand how many service lookups
Fig. 3. Logged service lookups per hour
could be observed by the malicious floodfill nodes. In particular, we checked
for an increasing number of malicious nodes (from 1 to all 10), the number of
lookups from the victim machines that we could observe. We ran this experi-
ment for a total of eight hours for each number of nodes, during different parts
of the day. This was done to avoid that the different number of routers at dif-
ferent times during the day would inﬂuence the results.
The experiments (Figure 3) show a roughly constant amount of around 50
lookups logged every hour, until fewer than three malicious nodes are left.
More precisely, there was a lookup observed from all our victim nodes approx-
imately every nine to ten minutes, which was caused by the lifespan of service
information. Under optimal conditions, one would expect 36 to 40 lookups per
hour, which is the total for six hosts updating their local information every nine
to ten minutes. However, shortly after the service information expired, there
were more than six lookups due to nodes retrying their lookup after losing the
response, adding up to the total of around 50 lookups. This means that the at-
tacker needs only three malicious nodes in the vicinity of the victim nodes to
observe all their relevant lookups.
In the next step, we tried to understand how many lookups observed at
the malicious nodes could be properly attributed to the queries made by the
victims. Observing lookups, of course, is not enough. It is also necessary to at-
tribute different lookups (and tunnel endpoints) to the victim machines. Other-
wise, we cannot determine whether a victim has requested a particular service.
Since the network is not only used by the victims, the malicious nodes receive
unrelated lookups by other (random) nodes in the I2P network.
The results were similar for the sites both in Europe and the US: 52% of the
tunnel endpoints that we attributed to a victim user were indeed originating
from this user (call her Alice), while in 48% of the cases, a speciﬁc lookup (and
thus, tunnel endpoint) that we attributed to Alice actually belonged to a dif-
ferent, random user. That is, in this step, we only correctly identify about half
the tunnel endpoints. However, this does not imply that we can detect Alice
only half the time, or that the results are only slightly better than a coin toss.
 0 10 20 30 40 50 6010987654321Lookups per hourNumber of NodeslookupsInstead, it means that we can detect a single access that Alice performs for re-
source R half the time. Monitoring Alice’s accesses over a longer period of time
then allows us to mount a much stronger attack, as discussed below.
Assume that we monitor Alice and a resource R for a certain time period T.
Let’s partition this period into N time slots of duration d, where d = 10 minutes.
This is the time interval after which I2P refreshes the tunnel identiﬁers, and
hence, a new lookup is performed. During each of the i : 0  q; intuitively,
as p grows larger than q, our task becomes easier.
The probability that we have k hits over N time slots can be computed with
the binomial distribution. Recall that a hit occurs when we attribute a certain
lookup (tunnel id) with Alice, and we see this tunnel identiﬁer accessing R.
The probability that ti 2 Li = x = u ⇤ p + (1   u) ⇤ q = 0.5p + 0.5q. This
is the chance of Alice accessing resource, in case we guessed correctly, plus the
chance of a random hit when we misidentiﬁed the tunnel. Thus:
P(k hits) =✓N
k◆xk ⇤ (1   x)N k
(1)
Since we care about the probability of at least k hits, we require the cumu-
lative distribution function. In Figure 4, one can see the probability (shown y-
axis) that one observe at least k hits (shown on the x-axis) for different values of
p (the probability that Alice accesses R during an arbitrary time slot). For this
graph, we assume the length of the observation period to be one day (N = 144),
and we set q = 0.001.
The value of q is relevant for false positives, and has been chosen conser-
vatively here. Our concrete values assumes that about 7% of all nodes access
R once a day. The false positives (incorrect attributions) are represented by the
solid line for p = 0; that is, Alice does not at all visit R. It can be seen that this
line quickly drops close to zero. When we require at least two hits per day, the
Fig. 4. Probability of k or more hits, depending on p
 1
 0.8
 0.6
 0.4
 0.2
)
]
s
s
e
c
c
A
e
c
r
u
o
s
e
R
f
o
y
c
n
e
u
q
e
r
F
[
p
|
s
t