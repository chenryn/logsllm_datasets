# 配置云 Kubernetes 集群
下一步是在 EKS 集群上运行我们的服务，这样它就可以在云中使用。我们将使用`.yaml`文件作为基础，但是我们需要做一些更改。
来看看 GitHub`Chapter07`([https://GitHub . com/PacktPublishing/hand-Docker-for-micro-service-with-Python/tree/master/chapter 07](https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/tree/master/Chapter07))子目录中的文件。
我们将看到与前一章中的 Kubernetes 配置文件的区别，然后我们将在*部署系统*部分部署它们。
# 配置 AWS 映像注册表
第一个区别是，我们需要将映像更改为完整的注册表，因此群集使用 ECS 注册表中可用的映像。
Remember, you need to specify the registry inside AWS so the AWS cluster can properly access it.
例如在`frontend/deployment.yaml`文件中，我们需要这样定义它们:
```
containers:
- name: frontend-service
  image: XXX.dkr.ecr.us-west-2.amazonaws.com/frontend:latest
  imagePullPolicy: Always
```
映像应该从 AWS 注册表中提取。应该将拉策略更改为强制从集群中拉。
创建`example`命名空间后，您可以通过应用文件在远程服务器中部署:
```
$ kubectl create namespace example
namespace/example created
$ kubectl apply -f frontend/deployment.yaml
deployment.apps/frontend created
```
过了一会儿，部署创建了吊舱:
```
$ kubectl get pods -n example
NAME                      READY STATUS  RESTARTS AGE
frontend-58898587d9-4hj8q 1/1   Running 0        13s
```
现在我们需要改变其余的元素。所有部署都需要进行调整，以包含适当的注册表。
检查 GitHub 上的代码，检查所有`deployment.yaml`文件。
# 配置外部可访问负载平衡器的使用
第二个区别是使前端服务对外可用，这样互联网流量就可以访问集群。
这很容易通过将服务从`NodePort`更改为`LoadBalancer`来实现。查看`frontend/service.yaml`文件:
```
apiVersion: v1
kind: Service
metadata:
    namespace: example
    labels:
        app: frontend-service
    name: frontend-service
spec:
    ports:
        - name: frontend
          port: 80
          targetPort: 8000
    selector:
        app: frontend
    type: LoadBalancer
```
这就创建了一个新的**弹性负载平衡器** ( **ELB** )可以从外部访问。现在，让我们开始部署。
# 部署系统
整个系统可以从`Chapter07`子目录部署，代码如下:
```
$ kubectl apply --recursive -f .
deployment.apps/frontend unchanged
ingress.extensions/frontend created
service/frontend-service created
deployment.apps/thoughts-backend created
ingress.extensions/thoughts-backend-ingress created
service/thoughts-service created
deployment.apps/users-backend created
ingress.extensions/users-backend-ingress created
service/users-service created
```
该命令迭代遍历子目录并应用任何`.yaml`文件。
几分钟后，您应该会看到一切正常运行:
```
$ kubectl get pods -n example
NAME                              READY STATUS  RESTARTS AGE
frontend-58898587d9-dqc97         1/1   Running 0        3m
thoughts-backend-79f5594448-6vpf4 2/2   Running 0        3m
users-backend-794ff46b8-s424k     2/2   Running 0        3m
```
要获得公共接入点，您需要检查服务:
```
$ kubectl get svc -n example
NAME             TYPE         CLUSTER-IP EXTERNAL-IP AGE
frontend-service LoadBalancer 10.100.152.177 a28320efca9e011e9969b0ae3722320e-357987887.us-west-2.elb.amazonaws.com 3m
thoughts-service NodePort 10.100.52.188  3m
users-service    NodePort 10.100.174.60  3m
```
请注意，前端服务有一个外部 ELB 域名系统可用。
如果将该域名系统放在浏览器中，您可以按如下方式访问该服务:
![](img/b27d1c74-f06d-4be2-b017-73f59bb4fa8d.png)
恭喜，您拥有了自己的云 Kubernetes 服务。该服务可以访问的域名并不是很好，所以我们将看到如何添加一个注册的域名并将其公开在 HTTPS 端点下。
# 使用 HTTPS 和顶级域名保护外部访问
为了向您的客户提供良好的服务，您的外部端点应该通过 HTTPS 提供服务。这意味着您和客户之间的通信是私有的，不能在整个网络路由中被嗅探。
HTTPS 的工作方式是服务器和客户端对通信进行加密。为了确保服务器是他们所说的那样，需要有一个由授权机构颁发的 SSL 证书来验证域名系统。
Remember, the point of HTTPS is *not* that the server is inherently trustworthy, but that the communication is private between the client and the server. The server can still be malicious. That's why verifying that a particular DNS does not contain misspellings is important.
You can get more information on how HTTPS works in this fantastic comic: [https://howhttps.works/](https://howhttps.works/).
为外部端点获取证书需要两个阶段:
*   你拥有一个特定的域名，通常是从域名注册商那里购买的。
*   您通过公认的**证书颁发机构** ( **CA** )获得域名的唯一证书。证书颁发机构必须验证您控制了域名。
To help in promoting the usage of HTTPS, the non-profit *Let's Encrypt* ([https://letsencrypt.org](https://letsencrypt.org)) supplies free certificates valid for 60 days. This will be more work than obtaining one through your cloud provider, but could be an option if money is tight.
如今，这个过程对于云提供商来说非常容易，因为他们可以同时充当两者，从而简化了这个过程。
The important element that needs to communicate through HTTPS is the edge of our network. The internal network where our own microservices are communicating doesn't require to be HTTPS, and HTTP will suffice. It needs to be a private network out of public interference, though.
按照我们的示例，AWS 允许我们创建一个证书并将其与一个 ELB 相关联，从而为 HTTP 中的流量服务。
Having AWS to serve HTTPS traffic ensures that we are using the latest and safest security protocols, such as **Transport Layer Security** (**TLS**) v1.3 (the latest at the time of writing), but also that it keeps backward compatibility with older protocols, such as SSL. 
In other words, it is the best option to use the most secure environment by default.
设置 HTTPS 的第一步是要么直接从 AWS 购买 DNS 域名，要么将控制权转移给 AWS。这可以通过他们的服务路线 53 来完成。您可以在[https://aws.amazon.com/route53/](https://aws.amazon.com/route53/)查看文档。
It is not strictly required to transfer your DNS to Amazon, as long as you can point it toward the externally facing ELB, but it helps with the integration and obtaining of certificates. You'll need to prove that you own the DNS record when creating a certificate, and using AWS makes it simple as they create a certificate to a DNS record they control. Check the documentation at [https://docs.aws.amazon.com/acm/latest/userguide/gs-acm-validate-dns.html](https://docs.aws.amazon.com/acm/latest/userguide/gs-acm-validate-dns.html).
要在您的 ELB 上启用 HTTPS 支持，让我们检查以下步骤:
1.  转到 AWS 控制台中的监听器:
![](img/fa6eb258-65a0-4953-a49c-4f0ac559524f.png)
2.  单击编辑并为 HTTPS 支持添加新规则:
![](img/9bceff02-8ad6-4194-8315-75ea59238415.png)
3.  如您所见，它将需要 SSL 证书。单击“更改”转到管理:
![](img/c0378dc3-fd4b-4c3a-ab55-11a07af6a74d.png)
4.  从这里，您可以添加一个现有的证书，或者从亚马逊购买一个。
Be sure to check the documentation about the load balancer in Amazon. There are several kinds of ELBs that can be used, and some have different features than others depending on your use case. For example, some of the new ELBs are able to redirect toward HTTPS even if your customer requests the data in HTTP. See the documentation at [https://aws.amazon.com/elasticloadbalancing/](https://aws.amazon.com/elasticloadbalancing/).
恭喜，现在您的外部端点支持 HTTPS，确保您与客户的通信是私密的。
# 准备好迁移到微服务
为了在迁移过程中平稳运行，您需要部署一个负载平衡器，允许您在后端之间快速交换并保持服务正常运行。
正如我们在[第 1 章](01.html)、*采取行动——设计、计划和执行*中所讨论的，HAProxy 是一个很好的选择，因为它非常通用，并且有一个很好的用户界面，允许您只需点击一个网页就可以快速进行操作。它还有一个优秀的统计页面，允许您监控服务的状态。
AWS has an alternative to HAProxy called **Application Load Balancer** (**ALB**). This works as a feature-rich update on the ELB, which allows you to route different HTTP paths into different backend services.
HAProxy has a richer set of features and a better dashboard to interact with it. It can also be changed through a configuration file, which helps in controlling changes, as we will see in [Chapter 8](08.html), *Using GitOps Principles*. 
It is, obviously, only available if all the services are available in AWS, but it can be a good solution in that case, as it will be simpler and more aligned with the rest of the technical stack. Take a look at the documentation at [https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/](https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/).
要在您的服务前面部署负载平衡器，我建议不要在 Kubernetes 上部署它，而是以与传统服务相同的方式运行它。这种负载平衡器将是系统的关键部分，消除不确定性对于成功运行非常重要。这也是一项相对简单的服务。
Keep in mind that a load balancer needs to be properly replicated, or it becomes a single point of failure. Amazon and other cloud providers allow you to set up an ELB or other kinds of load balancer toward your own deployment of load balancers, enabling the traffic to be balanced among them. 
作为一个例子，我们已经创建了一个示例配置和`docker-compose`文件来快速运行它，但是该配置可以以您的团队最熟悉的任何方式进行设置。
# 运行示例
代码可在 GitHub([https://GitHub . com/PacktPublishing/动手 Docker-for-micro-service-with-Python/tree/master/chapter 07/haproxy](https://github.com/PacktPublishing/Hands-On-Docker-for-Microservices-with-Python/tree/master/Chapter07/haproxy))上获得。我们继承了 Docker Hub([https://hub.docker.com/_/haproxy/](https://hub.docker.com/_/haproxy/))中的 HAProxy Docker 映像，添加了自己的配置文件。
让我们看看配置文件`haproxy.cfg`中的主要元素:
```
frontend haproxynode
    bind *:80
    mode http
    default_backend backendnodes
backend backendnodes
    balance roundrobin
    option forwardfor
    server aws a28320efca9e011e9969b0ae3722320e-357987887
               .us-west-2.elb.amazonaws.com:80 check
    server example www.example.com:80 check
listen stats
    bind *:8001
    stats enable
    stats uri /
    stats admin if TRUE
```
我们定义了一个前端，它接受任何进入端口`80`的请求，并将请求发送到后端。后端平衡对两个服务器`example`和`aws`的请求。基本上，`example`指向`www.example.com`(旧服务的占位符)和`aws`指向之前创建的负载平衡器。
我们在端口`8001`启用统计服务器，并允许管理员访问。
`docker-compose`配置启动服务器，并将本地主机端口转发到容器端口`8000`(负载平衡器)和`8001`(统计数据)。使用以下命令启动它:
```