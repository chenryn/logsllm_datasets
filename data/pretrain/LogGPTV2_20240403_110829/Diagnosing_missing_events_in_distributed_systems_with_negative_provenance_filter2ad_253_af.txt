creased by 29%, from 48.38 ms to 62.63 ms, when Y! was enabled;
throughput dropped by 14%, from 56.0 to 48.4 requests per second.
However, these results are difﬁcult to generalize because Rapid-
Net’s performance as an SDN controller is not competitive with
state-of-the-art controllers, even without Y!. We therefore re-
peated the experiment with our Y! extension for native Trema (Sec-
tion 7.1); here, adding Y! increased the average latency by only
1.6%, to 33 microseconds, and decreased the average throughput by
8.9%, to 100,540 PacketIn messages per second. We note that this
comparison is slightly unfair because we manually instrumented a
speciﬁc Trema program to work with our extension, whereas the
RapidNet prototype can work with any program. However, adding
instrumentation to Trema programs is not difﬁcult and could be au-
tomated. More generally, our results suggest that capturing prove-
nance is not inherently expensive, and that an optimized RapidNet
could potentially do a lot better.
7.6 Query processing speed
When the user issues a provenance query, Y! must recursively con-
struct the response using the process from Section 3.5 and then
post-process it using the heuristics from Section 4. Since debug-
ging is an interactive process, a quick response is important. To see
whether Y! can meet this requirement, we measured the turnaround
time for the queries in Table 2, as well as the fraction of time con-
sumed by Y!’s major components.
Figure 12 shows our results. We make two high-level observa-
tions. First, the turnaround time is dominated by R-tree and packet
recorder lookups. This is expected because the graph construction
algorithm itself is not very complex. Second, although the queries
vary in complexity and thus their turnaround times are difﬁcult to
compare, we observe that none of them took more than one sec-
ond; the most expensive query was Q9, which took 0.33 seconds to
complete.
7.7 Scalability
We do not yet have experience with Y!, or negative provenance,
in a large-scale deployment. However, we have done a number of
experiments to get an initial impression of its scalability. Due to
lack of space, we report only a subset of our results here.
Complexity: In our ﬁrst experiment, we tested whether the com-
plexity of the provenance increases with the number of possible
trafﬁc sources. We simulated a four-layer fat-tree topology with
15 switches, and we placed the client and the server on different
leaves, to vary the hop distance between them from 2 to 6. Our
results for running the learning-switch query (Q1) are shown in
Figure 13a (the bars are analogous to Figure 10). As expected, the
size of the raw provenance for Q1 grew substantially – from 250 to
386 vertices – because 1) there number of possible sources for the
missing trafﬁc increased, because each additional hop brings addi-
tional branches on the backtrace path and 2) each additional hop
required extra vertices to be represented in the provenance. But the
ﬁrst effect was mitigated by our pruning heuristics, since the extra
sources were inconsistent with the network state, and the second
effect was addressed by the summarization, which merged the ver-
tices along the propagation path into a single super-vertex. Once
all the heuristics had been applied, the size of the provenance was
16 vertices, independent of the hop count.
Storage: In our second experiment, we simulated three-layer fat-
tree topologies of different sizes (i.e., with different node degrees);
each edge switch was connected to a ﬁxed number of active hosts
that were constantly sending HTTP requests to the server. Fig-
ure 13b shows how Y!’s storage requirements grew with the num-
ber of switches in the network. As expected, the size of both the
pcap trace and the R-tree was roughly proportional to the size of
the network; this is expected because a) each new switch added a
ﬁxed number of hosts, and b) the depth of the tree, and thus the hop
count between the server and its clients, remained constant. Gener-
ally, the storage requirement depends on the rate at which events of
interest (packet transmissions, routing changes, etc.) are captured,
as well as on the time for which these records are retained.
Query speed: Our third experiment is analogous to the second,
except that we issued a query at the end and measured its turn-
around time. Figure 13c shows our results. The dominant cost
was the time it took to ﬁnd packets in the pcap trace; the R-tree
lookups were much faster, and the time needed to construct and
post-process the graph was so small that it is difﬁcult to see in the
ﬁgure. Overall, the lookup time was below one second even for the
largest network we tried.
Possible optimizations: Since our implementation has not been
optimized, some of the costs could grow quickly in a large-scale
deployment. For instance, in a data center with 400 switches that
handle 1 Gbps of trafﬁc each, our simple approach of recording
pcap traces at each switch would consume approximately 30 GB
of storage per second for the date center, or about 75 MB for each
switch. Packet recorder lookups, which compromise a major por-
tion of query latency, in such a large trace would be limited by
disk read throughput, and could take minutes. However, we note
that there are several ways to reduce these costs; for instance, tech-
niques from the database literature – e.g., a simple time index –
could be used to speed up the lookups, and the storage cost could
be reduced by applying ﬁlters.
7.8 Summary
Our results show that Y! – and, more generally, negative prove-
nance – can be a useful tool for diagnosing problems in networks:
the provenance of the issues we looked at was compact and read-
11
 0 100 200 300 400 500 600246Vertices in responseHop distance between client and serverWithout heuristicsPruning onlyPruning+Super-vertices 0 2 4 6 8 10 12 14 16 18 20 0 10 20 30 40 50 60Total storage (MB)Number of switchesR-treePcap trace 0 0.2 0.4 0.6 0.8 1 1.2101928374655Time (seconds)Number of switchesGraph constructionR-Tree lookupsPacket recorder lookupsPostprocessingable, and Y! was able to ﬁnd it in less than a second in each case.
Our results also show that the readability is aided considerably by
Y!’s post-processing heuristics, which reduced the number of ver-
tices by more than an order of magnitude. Y!’s main run-time cost
is the storage it needs to maintain a history of the system’s past
states, but a commodity hard-drive should be more than sufﬁcient
to keep this history for more than a day.
8. RELATED WORK
Network debugging: Many tools and techniques for network de-
bugging and root cause analysis have been proposed, e.g., [5, 9,
18, 26], but most focus on explaining positive events. These tools
can be used to (indirectly) troubleshoot negative symptoms, but the
results from our survey in Section 2.4 suggest that the lack of di-
rect support for negative queries makes this signiﬁcantly more dif-
ﬁcult. Hubble [12] uses probing to ﬁnd AS-level reachability prob-
lems but is protocol-speciﬁc; header space analysis [13] provides
ﬁner-grain results but relies on static analysis and thus cannot ex-
plain complex, dynamic interactions like the ones we consider here.
ATPG [34] tests for liveness, reachability, and performance, but
cannot handle dynamic nodes like the SDN controller. NICE [2]
uses model checking to test whether a given SDN program has
speciﬁc correctness properties; this approach is complementary to
ours, which focuses on diagnosing unforeseen problems at runtime.
We are not aware of any protocol-independent systems that can ex-
plain negative events in a dynamic distributed system.
Negative provenance: There is a substantial literature on tracking
provenance in databases [1, 6, 11, 25, 31] and in networks [35, 37],
but only a few papers have considered negative provenance. Huang
et al. [10] and Meliou et al. [19] focus on instance-based expla-
nations for missing answers, that is, how to obtain the missing
answers by making modiﬁcations to the value of base instances
(tuples); Why-Not [3] and ConQueR [28] provide query-based ex-
planations for SQL queries, which reveal over-constrained condi-
tions in the queries and suggest modiﬁcations to them. None of
these papers considers distributed environments and networks, as
we do here. In the networking literature, there is some prior work
on positive provenance, including our own work on ExSPAN [37],
SNP [35], and DTaP [36], but none of these systems can answer
(or even formulate) negative queries. To our knowledge, the only
existing work that does support such queries is our own workshop
paper [32], on which this paper is based.
9. CONCLUSION
In this paper, we have argued that debuggers for distributed systems
should not only be able to explain why an unexpected event did oc-
cur, but also why an expected event did not occur. We have shown
how this can be accomplished with the concept of negative prove-
nance, which so far has received relatively little attention. We have
deﬁned a formal model of negative provenance, we have presented
an algorithm generating such provenance, and we have introduced
Y!, a practical system that can maintain both positive and nega-
tive provenance in a distributed system and answer queries about
it. Our evaluation in the context of software-deﬁned networks and
BGP suggests that negative provenance can be a useful tool for di-
agnosing complex problems in distributed systems.
Acknowledgments: We thank our shepherd Nate Foster and the
anonymous reviewers for their comments and suggestions. We
also thank Behnaz Arzani for helpful comments on earlier drafts
of this paper. This work was supported by NSF grants CNS-
1065130, CNS-1054229, CNS-1040672, and CNS-0845552, as
well as DARPA contract FA8650-11-C-7189.
10. REFERENCES
[1] P. Buneman, S. Khanna, and W. C. Tan. Why and where: A characterization of
data provenance. In Proc. ICDT, Jan. 2001.
[2] M. Canini, D. Venzano, P. Perešíni, D. Kosti´c, and J. Rexford. A NICE way to
test OpenFlow applications. In Proc. NSDI, Apr. 2012.
[3] A. Chapman and H. V. Jagadish. Why not? In Proc. SIGMOD, June 2009.
[4] D. Erickson. The Beacon OpenFlow controller. In Proc. HotSDN, Aug. 2013.
[5] A. Feldmann, O. Maennel, Z. M. Mao, A. Berger, and B. Maggs. Locating
Internet routing instabilities. In Proc. SIGCOMM, Aug. 2004.
[6] T. J. Green, G. Karvounarakis, N. E. Taylor, O. Biton, Z. G. Ives, and
V. Tannen. ORCHESTRA: Facilitating collaborative data sharing. In Proc.
SIGMOD, June 2007.
[7] A. Guttman. R-trees: A dynamic index structure for spatial searching. In Proc.
SIGMOD, June 1984.
[8] N. Handigol, B. Heller, V. Jeyakumar, D. Mazières, and N. McKeown. Where is
the debugger for my software-deﬁned network? In Proc. HotSDN, Aug. 2012.
[9] N. Handigol, B. Heller, V. Jeyakumar, D. Mazières, and N. McKeown. I know
what your packet did last hop: Using packet histories to troubleshoot networks.
In Proc. NSDI, Apr. 2014.
[10] J. Huang, T. Chen, A. Doan, and J. F. Naughton. On the provenance of
non-answers to queries over extracted data. Proc. VLDB Endow., 1(1):736–747,
Aug. 2008.
[11] R. Ikeda, H. Park, and J. Widom. Provenance for generalized map and reduce
workﬂows. In Proc. CIDR, Jan. 2011.
[12] E. Katz-Bassett, H. V. Madhyastha, J. P. John, A. Krishnamurthy, D. Wetherall,
and T. Anderson. Studying black holes in the Internet with Hubble. In Proc.
NSDI, Apr. 2008.
[13] P. Kazemian, G. Varghese, and N. McKeown. Header space analysis: static
checking for networks. In Proc. NSDI, Apr. 2012.
[14] libspatialindex. http://libspatialindex.github.io/.
[15] D. Logothetis, S. De, and K. Yocum. Scalable lineage capture for debugging
DISC analysis. Technical Report CSE2012-0990, UCSD.
[16] B. T. Loo, T. Condie, M. Garofalakis, D. E. Gay, J. M. Hellerstein, P. Maniatis,
R. Ramakrishnan, T. Roscoe, and I. Stoica. Declarative networking. Commun.
ACM, 52(11):87–95, Nov. 2009.
[17] P. Macko and M. Seltzer. Provenance Map Orbiter: Interactive exploration of
large provenance graphs. In Proc. TaPP, June 2011.
[18] H. Mai, A. Khurshid, R. Agarwal, M. Caesar, P. B. Godfrey, and S. T. King.
Debugging the data plane with Anteater. In Proc. SIGCOMM, Aug. 2011.
[19] A. Meliou and D. Suciu. Tiresias: the database oracle for how-to queries. In
Proc. SIGMOD, May 2012.
[20] Mininet. http://mininet.org/.
[21] C. Monsanto, J. Reich, N. Foster, J. Rexford, and D. Walker. Composing
software-deﬁned networks. In Proc. NSDI, Apr. 2013.
[22] K.-K. Muniswamy-Reddy, D. A. Holland, U. Braun, and M. Seltzer.
Provenance-aware storage systems. In Proc. USENIX ATC, May 2006.
[23] Outages mailing list. http://wiki.outages.org/index.php/Main_
Page#Outages_Mailing_Lists.
[24] RapidNet. http://netdb.cis.upenn.edu/rapidnet/.
[25] C. Ré, N. Dalvi, and D. Suciu. Efﬁcient top-k query evaluation on probabilistic
data. In Proc. ICDE, Apr. 2007.
[26] A. Singh, P. Maniatis, T. Roscoe, and P. Druschel. Using queries for distributed
monitoring and forensics. In Proc. EuroSys, Apr. 2006.
[27] R. Teixeira and J. Rexford. A measurement framework for pin-pointing routing
changes. In Proc. Network Troubleshooting workshop (NetTS), Sept. 2004.
[28] Q. T. Tran and C.-Y. Chan. How to ConQueR why-not questions. In Proc.
SIGMOD, June 2010.
[29] Trema. http://trema.github.io/trema/.
[30] A. Wang, L. Jia, W. Zhou, Y. Ren, B. T. Loo, J. Rexford, V. Nigam, A. Scedrov,
and C. L. Talcott. FSR: Formal analysis and implementation toolkit for safe
inter-domain routing. IEEE/ACM ToN, 20(6):1814–1827, Dec. 2012.
[31] J. Widom. Trio: A system for integrated management of data, accuracy, and
lineage. In Proc. CIDR, Jan. 2005.
[32] Y. Wu, A. Haeberlen, W. Zhou, and B. T. Loo. Answering Why-Not queries in
software-deﬁned networks with negative provenance. In Proc. HotNets, 2013.
[33] Y. Wu, M. Zhao, A. Haeberlen, W. Zhou, and B. T. Loo. Diagnosing missing
events in distributed systems with negative provenance. Technical Report
MS-CIS-14-06, University of Pennsylvania, 2014.
[34] H. Zeng, P. Kazemian, G. Varghese, and N. McKeown. Automatic test packet
generation. In Proc. CoNEXT, Dec. 2012.
[35] W. Zhou, Q. Fei, A. Narayan, A. Haeberlen, B. T. Loo, and M. Sherr. Secure
network provenance. In Proc. SOSP, Oct. 2011.
[36] W. Zhou, S. Mapara, Y. Ren, Y. Li, A. Haeberlen, Z. Ives, B. T. Loo, and
M. Sherr. Distributed time-aware provenance. In Proc. VLDB, Aug. 2013.
[37] W. Zhou, M. Sherr, T. Tao, X. Li, B. T. Loo, and Y. Mao. Efﬁcient querying and
maintenance of network provenance at Internet-scale. In Proc. SIGMOD, 2010.
12