dent set is more than 30 for 1 Gbps links. Note that this is with just
one channel; the set size increases linearly with more channels. We
shall see later in the paper that for our workloads, these numbers
sufﬁce to provide signiﬁcant performance gains.
In summary, these results give us conﬁdence that in a typical data
center, a large number of 60 GHz links can operate while delivering
desired performance.
4. ANALYZING DATA CENTER TRAFFIC
We now examine trafﬁc from four real applications in the data
center to understand how much value ﬂyways can add.
4.1 Datasets
Table 1 summarizes the analyzed datasets. Together, these logs
represent over 76 hours worth of traces, and over 114 terabytes of
trafﬁc. The Cosmos dataset was measured on a pre-production clus-
ter with O(1K) servers running Dryad. It supports a data mining
workload for a large web search engine. Jobs on this cluster are
a mix of repetitive production scripts (e.g., hourly summaries) and
jobs submitted by users. The IndexSrv dataset is from a produc-
tion cluster with O(10K) servers. The cluster stores the web search
index and assembles search results for queries. This workload is
latency sensitive. Unlike the Cosmos cluster, links here rarely see
high utilizations.
In both clusters, we instrumented every server
to log network send and read system calls and the amount of data
involved. The next two datasets are from an HPC platform with
O(100) servers spread across 5 racks, running car simulation soft-
ware. In most of the datasets, the servers were in racks underneath
a single core switch pair. However, servers in the IndexSrv dataset
spanned multiple core switches. In all clusters, ToR switches have
enough backplane bandwidth such that intra-rack communication
is only limited by the server NICs. However, the links connecting
the ToR switches to the core are oversubscribed.
4.2 Estimating demand matrices
We want to understand the demands of data center applications
without being impacted by the topology and capacity of the ob-
served networks. To do so, we aggregate the trafﬁc exchanged
at time scales that are pertinent to the application. For example,
most Dryad tasks ﬁnish within a few minutes, so the total trafﬁc
exchanged between racks in the Cosmos cluster every few minutes
is a good indicator of application requirements. Unless otherwise
noted, the datasets in this paper average trafﬁc over 300 s periods to
compute demands.
Consider an example demand matrix from the Cosmos dataset;
Figure 11 depicts a heat map of the demands between pairs of the
ToR switches. The color palette is on a logarithmic scale, i.e., black
corresponds to the largest demand entry D, deep red (0.5 on the
scale) corresponds to √D and white indicates zero demand.
1
A few trends are apparent. First, only a few ToR pairs are hot,
i.e., send or receive a large volume of trafﬁc (darker dots). The
bulk of the ToR pairs are yellow, i.e., less than D
10 . Second, hot
ToRs exchange much of their data with a few, but not all, of the
other ToRs (horizontal and vertical streaks).
It follows that pro-
viding additional bandwidth at hotspots would dramatically reduce
the maximum temperature of the matrix. But, does this hold across
all demand matrices? What form should the additional bandwidth
take? How do the hotspots change over time? We look at these
questions next.
4.3 Prevalence of hotspots
Figure 12(a) plots the fraction of hot links—links that are at least
half as loaded as the most loaded link—in each of our datasets.
In every dataset, over 60% of the matrices have fewer than 10%
of their links hot at any time.
In fact, every matrix in the Neon
dataset has less than 7% hot links. This means that for measured
trafﬁc patterns in the DC, avoiding oversubscription over the entire
network may not be needed. Instead, performance may be improved
by adding capacity to a small set of links. We see in the evaluation
of our system (§6) that, indeed, a few ﬂyways have a large effect.
4.4 Trafﬁc contributors to hotspots
To be useful, additional capacity provided to a hotspot should
be able to ofﬂoad a substantial fraction of the load. Prior propos-
als [7, 30] establish one additional ﬂyway, in the form of an optical
circuit, per congested link. Figure 12(b) estimates the maximum
potential value of doing so, and suggests there will be little beneﬁt
in real data centers. Across hot links the trafﬁc share of the largest
ToR neighbor is quite small; on the Cosmos dataset, it is less than
20% for 80% of the matrices. In fact, Figure 12(c) shows that in
some cases, even the top ﬁve ToR pairs can cumulatively add up
to a small fraction of load on the hotlink. In other words, we ﬁnd
that hot links are associated with a high fan-in (or fan-out). This
observation was a surprise; it means that at hotspots, the existing
proposals that ofﬂoad trafﬁc going to just the best neighbor would
42 
s
e
c
i
r
t
a
M
d
n
a
m
e
D
f
o
n
o
i
t
c
a
r
F
)
e
v
i
t
l
a
u
m
u
C
(
 1
 0.8
 0.6
 0.4
 0.2
 0
IndexSrv
3Cars
Neon
Cosmos
s
k
n
L
i
t
o
H
f
o
n
o
i
t
c
a
r
F
)
e
v
i
t
l
a
u
m
u
C
(
 1
 0.8
 0.6
 0.4
 0.2
 0
IndexSrv
3Cars
Neon
Cosmos
s
k
n
L
i
t
o
H
f
o
n
o
i
t
c
a
r
F
)
e
v
i
t
l
a
u
m
u
C
(
 1
 0.8
 0.6
 0.4
 0.2
 0
IndexSrv
3Cars
Neon
Cosmos
 0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
 0  0.2  0.4  0.6  0.8  1
 0  0.2  0.4  0.6  0.8  1
s
e
c
i
r
t
a
M
d
n
a
m
e
D
f
o
n
o
i
t
c
a
r
F
)
e
v
i
t
l
a
u
m
u
C
(
 1
 0.8
 0.6
 0.4
 0.2
 0
IndexSrv
3Cars
Neon
Cosmos
 0  0.2  0.4  0.6  0.8  1
Fraction of links that are hot
Fraction of total traffic in max nbr
Fraction of total traffic in top5 nbrs
Fraction of change in hotlinks
(a) Sparsity of Hotspots
(b) Share of HotLink’s trafﬁc to Top
Neighbor
(c) Cumulative Share of HotLink’s
Trafﬁc To Top Five Neighbors
(d) Predictability of ﬂyways
Figure 12: Nature of hotspots in measured DC trafﬁc
oversubscribed core
(tree, VL2 etc.)
Channel 
model
Device 
locations
Demands known 
or predicted
…
…
…
…
…
(a) A ﬂyway aug-
mented network
Flyway Validator
Flyway Picker
traffic-aware set of 
flyways
(b) Flyway controller
Figure 13: Proposed Architecture
be of limited value for real data center workloads. Even the opti-
mistic wavelength multiplexing-based optical extensions proposed
in Helios would not sufﬁce in these cases. We propose and evaluate
a one hop indirection technique (§5) that overcomes this weakness.
4.5 Predictability of hotspots
Figure 12(d) compares the change in the pairs of hot-links across
consecutive matrices. We observe a dichotomy—some matrices
are highly predictable, others are very unpredictable. In both HPC
datasets, we see less than a 10% change in hot links whereas in the
Cosmos dataset fewer than 10% of hot links repeat. We tried a few
more complicated predictors, and ﬁnd that the results are qualita-
tively similar. Likely, this is due to the nature of workload. While
Cosmos churns work at the granularity of map and reduce tasks
which typically last about a few minutes, work in HPC clusters
manifests in more long lived groups. We also verify that ﬂow sizes
and arrival rates in the DC [6, 14] indicate that trafﬁc in the DC lies
in a fast-changing collection of medium-sized ﬂows. This property
of real DC workloads renders predictors that rely on identifying
elephant ﬂows [2] to be of less use.
Take-aways: In a broad study of many types of DC workload,
we ﬁnd that hotspots are sparse. The potential beneﬁt of selec-
tively providing additional bandwidth at these hotspots, as opposed
to building for the worst case with non-oversubscribed networks,
appears signiﬁcant. We also see that real data center trafﬁc matri-
ces are more complex than synthetic workloads evaluated by prior
proposals [7, 30], and ﬂyway placement algorithms developed by
these proposals are likely be of marginal value. The key issue is
that hotspots are often correlated with a high fan-in (or fan-out) im-
plying that to be useful trafﬁc from (or to) many destinations needs
to be ofﬂoaded. Our system design (§5) includes a novel one hop
indirection method designed to resolve this problem.
5. FLYWAYS SYSTEM DESIGN
In this section, we propose a design for a DC network with ﬂy-
ways. The basic architecture is shown in Figure 13; we consider the