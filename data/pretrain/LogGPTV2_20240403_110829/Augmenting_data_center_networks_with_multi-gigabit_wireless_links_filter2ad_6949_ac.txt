### 3. Link Set Size and Performance

For 1 Gbps links, a dent set size of more than 30 is required. It is important to note that this requirement is based on a single channel; the set size increases linearly with the addition of more channels. Our subsequent analysis in the paper will demonstrate that, for our specific workloads, these numbers are sufficient to achieve significant performance gains.

In summary, these results provide confidence that, in a typical data center environment, a large number of 60 GHz links can operate while delivering the desired performance.

### 4. Analyzing Data Center Traffic

To understand the potential benefits of flyways, we examine traffic from four real applications in a data center.

#### 4.1 Datasets

Table 1 summarizes the datasets analyzed. These logs represent over 76 hours of traces and more than 114 terabytes of traffic. The Cosmos dataset was collected from a pre-production cluster with approximately 1,000 servers running Dryad, supporting a data mining workload for a large web search engine. Jobs on this cluster include both repetitive production scripts (e.g., hourly summaries) and user-submitted jobs. The IndexSrv dataset comes from a production cluster with around 10,000 servers, which stores the web search index and assembles search results for queries. This workload is highly latency-sensitive, and unlike the Cosmos cluster, the links in this cluster rarely see high utilizations.

In both clusters, every server was instrumented to log network send and read system calls along with the amount of data involved. The next two datasets are from an HPC platform with approximately 100 servers spread across five racks, running car simulation software. In most cases, the servers were located under a single core switch pair, except for the IndexSrv dataset, where servers spanned multiple core switches. In all clusters, ToR (Top of Rack) switches have sufficient backplane bandwidth to ensure that intra-rack communication is only limited by the server NICs. However, the links connecting the ToR switches to the core are oversubscribed.

#### 4.2 Estimating Demand Matrices

To understand the demands of data center applications without being influenced by the topology and capacity of the observed networks, we aggregate traffic at time scales relevant to the application. For example, most Dryad tasks complete within a few minutes, so the total traffic exchanged between racks in the Cosmos cluster every few minutes is a good indicator of application requirements. Unless otherwise noted, the datasets in this paper average traffic over 300-second periods to compute demands.

Consider an example demand matrix from the Cosmos dataset, depicted in Figure 11 as a heat map of the demands between pairs of ToR switches. The color palette is on a logarithmic scale, with black representing the largest demand entry \(D\), deep red (0.5 on the scale) representing \(\sqrt{D}\), and white indicating zero demand.

Several trends are apparent:
1. Only a few ToR pairs are "hot," meaning they send or receive a large volume of traffic (darker dots). The majority of ToR pairs are yellow, indicating less than \(D/10\).
2. Hot ToRs exchange most of their data with a few, but not all, other ToRs (horizontal and vertical streaks).

This suggests that providing additional bandwidth at hotspots would significantly reduce the maximum temperature of the matrix. However, does this hold true across all demand matrices? What form should the additional bandwidth take? How do the hotspots change over time? We address these questions next.

#### 4.3 Prevalence of Hotspots

Figure 12(a) plots the fraction of hot links—links that are at least half as loaded as the most loaded link—in each of our datasets. In every dataset, over 60% of the matrices have fewer than 10% of their links hot at any time. In the Neon dataset, every matrix has less than 7% hot links. This implies that, for measured traffic patterns in the data center, avoiding oversubscription over the entire network may not be necessary. Instead, performance could be improved by adding capacity to a small set of links. Our system evaluation (§6) confirms that a few flyways can have a significant impact.

#### 4.4 Traffic Contributors to Hotspots

To be effective, additional capacity provided to a hotspot should be able to offload a substantial fraction of the load. Prior proposals [7, 30] suggest establishing one additional flyway, in the form of an optical circuit, per congested link. Figure 12(b) estimates the maximum potential value of this approach and suggests that there will be little benefit in real data centers. Across hot links, the traffic share of the largest ToR neighbor is quite small; in the Cosmos dataset, it is less than 20% for 80% of the matrices. Figure 12(c) shows that even the top five ToR pairs can cumulatively add up to a small fraction of the load on the hotlink. In other words, hot links are associated with a high fan-in (or fan-out). This observation indicates that existing proposals, which offload traffic going to just the best neighbor, would be of limited value for real data center workloads. Even the optimistic wavelength multiplexing-based optical extensions proposed in Helios would not suffice in these cases. We propose and evaluate a one-hop indirection technique (§5) to overcome this weakness.

#### 4.5 Predictability of Hotspots

Figure 12(d) compares the change in the pairs of hot links across consecutive matrices. We observe a dichotomy: some matrices are highly predictable, while others are very unpredictable. In both HPC datasets, there is less than a 10% change in hot links, whereas in the Cosmos dataset, fewer than 10% of hot links repeat. More complex predictors yield qualitatively similar results. This is likely due to the nature of the workload. While Cosmos churns work at the granularity of map and reduce tasks, which typically last a few minutes, work in HPC clusters manifests in more long-lived groups. We also verify that flow sizes and arrival rates in the data center indicate that traffic consists of a fast-changing collection of medium-sized flows. This property of real data center workloads renders predictors that rely on identifying elephant flows [2] less useful.

**Takeaways:**
- In a broad study of various data center workloads, we find that hotspots are sparse.
- Selectively providing additional bandwidth at these hotspots, rather than building for the worst case with non-oversubscribed networks, appears to offer significant benefits.
- Real data center traffic matrices are more complex than synthetic workloads evaluated by prior proposals [7, 30], and flyway placement algorithms developed by these proposals are likely to be of marginal value.
- The key issue is that hotspots often correlate with a high fan-in (or fan-out), implying that to be useful, traffic from (or to) many destinations needs to be offloaded. Our system design (§5) includes a novel one-hop indirection method to address this problem.

### 5. Flyways System Design

In this section, we propose a design for a data center network with flyways. The basic architecture is shown in Figure 13. We consider the following components:

- **Flyway Augmented Network:** A network augmented with flyways, which are additional high-bandwidth links that can be dynamically allocated to alleviate congestion.
- **Flyway Validator:** A component that validates the feasibility and effectiveness of flyway placements.
- **Flyway Picker:** A component that selects the optimal set of flyways based on current and predicted traffic demands.

The proposed architecture aims to improve network performance by efficiently managing and allocating additional bandwidth to hotspots, thereby reducing congestion and improving overall throughput.