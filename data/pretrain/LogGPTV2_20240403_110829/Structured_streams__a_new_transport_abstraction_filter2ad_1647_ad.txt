a diﬀerence barely out of the noise, but attributable to the
overhead of implementing transports outside the kernel atop
UDP. Running the same benchmark over an 802.11g wire-
less LAN providing about 18Mbps maximum throughput,
SST was 7.1% slower than native TCP, and user-space TCP
was 3.6% slower. These results suggest that even the unop-
timized SST prototype performs adequately on “consumer-
grade” networks, although a more optimized implementa-
tion would be desired on high-speed networks. Comparing
SST against the user-space TCP on simulated networks with
similar parameters, the two transports exhibited identical
performance to within 0.2%.
The second benchmark runs two downloads at once—one
using the native TCP, the other using either SST or the
user-space TCP—to verify “TCP-friendly” congestion con-
trol behavior. The user-space transports were found to be
extremely fair, and just barely less aggressive than native
TCP: SST takes a 48.8% bandwidth share against native
TCP’s 51.2% share, and the user-space TCP takes 48.1%
against native TCP’s 51.9%. This result is unsurprising
given that both user-space transports essentially implement
the classic TCP congestion control schemes.
5.4 Scalability over Transaction Size
We now compare SST against TCP and UDP when used
for Web-style transactions in which the request is small
but the response varies in size. Since we wish to exam-
ine how SST’s performance scales when the application uses
transport instances to match its transaction structure, this
test uses one SST or TCP stream per transaction as in
HTTP/1.0, leaving HTTP/1.1 persistent streams to the next
section. The UDP test operates as in DNS or RPC, with
each request datagram soliciting a single response datagram.
Figure 7 shows client-observed transaction latency on a
log/log plot for responses ranging from 32 bytes to two
megabytes, measured on the real 1.5Mbps DSL connection
described above, which has about 50ms minimum latency.
For small transactions where network latency dominates,
TCP takes twice as long as UDP due to its 3-way hand-
shake. UDP ceases functioning beyond around 8KB due to
middleboxes on the test connection, and IP version 4 limits
datagrams to 64KB in any case. In this test the network con-
nection was quiescent and no UDP datagrams were lost, but
in a second test whose results are not shown, on a connection
loaded with two concurrent long-running TCP downloads,
the eﬀective UDP datagram loss rate often exceeded 50%
even at the widely-used 8KB datagram size.
i
e
m
T
e
s
n
o
p
s
e
R
+
t
s
e
u
q
e
R
UDP
TCP
SST
10s
6s
4s
2s
1s
600ms
400ms
200ms
100ms
60ms
40ms
32B
128B
512B
2K
8K
32K
128K
512K
2M
Size of Object Transferred
Figure 7: Transactional use of UDP, TCP, and SST
over a 216× range of transaction sizes.
As the graph shows, SST can create new streams for small
transactions with the same low latency as UDP, while scal-
ing to support long-running transfers. The SST test runs
its transactions over a “warm” communication channel al-
ready set up by the negotiation protocol, representing the
common case in which a client makes multiple requests to
the same server. Even without a warm channel, SST can
piggyback the ﬁrst application request and response data
segments onto the negotiation protocol packets if crypto-
graphic security is not required and the responder is not
heavily loaded, retaining a total latency of one round trip.
Otherwise, SST adds one round trip delay for channel setup.
5.5 Web Trafﬁc Workload
HTTP/1.1 addressed the ineﬃciency of short-lived TCP
streams through persistent connections, which are now in
common use, and pipelining, which is not. Since SST at-
tempts to oﬀer the beneﬁts of persistent streams with the
simplicity of the one-transaction-per-stream model, we now
compare SST against the behavior of several ﬂavors of HTTP
over TCP, under a simulated web workload.
For this test we simulate a series of web page loads, each
page consisting of a “primary” HTTP request for the HTML,
followed by a batch of “secondary” requests for embedded
objects such as images. As the simulation’s workload we use
a fragment of the UC Berkeley Home IP web client traces
available from the Internet Traﬃc Archive [27]. We sort
the trace by client IP address so that each user’s activities
are contiguous, then we use only the order and sizes of re-
quests to drive the simulation, ignoring time stamps. Since
the traces do not indicate which requests belong to one web
page, the simulation approximates this information by clas-
sifying requests by extension into “primary” (e.g., ‘.html’ or
no extension) and “secondary” (e.g., ‘gif’, ‘.jpg’, ‘.class’),
and then associating each contiguous run of secondary re-
quests with the immediately preceding primary request. The
simulation pessimistically assumes that the browser cannot
begin requesting secondary objects until it has downloaded
the primary object completely, but at this point it can in
theory request all of the secondary objects in parallel.
Figure 8 shows a scatter plot of the total duration of each
web page load against the total size of all downloads for that
page, on the simulated 1.5Mbps network used in Section 5.3.
i
e
m
T
e
s
n
o
p
s
e
R
+
t
s
e
u
q
e
R
TCP: HTTP/1.0 serial
TCP: HTTP/1.0 parallel
TCP: HTTP/1.1 persistent
TCP: HTTP/1.1 pipelined
SST: HTTP/1.0 parallel
4s
2s
1s
600ms
400ms
200ms
100ms
60ms
128B
1K
8K
64K
1 request per page
1K
128B
2 requests per page
8K
64K
1K
128B
64K
3-4 requests per page
8K
1K
128B
64K
5-8 requests per page
8K
1K
128B
9+ requests per page
8K
64K
Figure 8: Web workload comparing single-transaction SST streams against four HTTP ﬂavors over TCP.
The plot is divided into ﬁve groups by the total number
of HTTP requests per web page. The leftmost group, for
pages with no secondary requests, has a best-case load time
half that of other groups, because in the latter groups sec-
ondary requests do not start until the primary request com-
pletes. The points labeled “HTTP/1.0 serial” reﬂect the
behavior of early web browsers that load pages by opening
TCP connections for each request sequentially, “HTTP/1.0
parallel” represents browsers that open up to eight single-
transaction TCP streams in parallel, “HTTP/1.1 persis-
tent” represents modern browsers that use up to two con-
current persistent TCP streams as per RFC 2616 [19], and
“HTTP/1.1 pipelined” uses two concurrent streams with up
to four pipelined requests each. The SST case uses one
transaction per stream, as in HTTP/1.0, but imposes no
limit on the number of parallel streams. As the graph indi-
cates, HTTP/1.0 over SST achieves performance compara-
ble to pipelined HTTP/1.1 streams over TCP, both of which
are much faster than other methods, including the current
common case of persistent but non-pipelined TCP streams.
5.6 Dynamic Prioritization
In a ﬁnal experiment, we consider a hypothetical SST-
enabled web browser in which a user views a “photo album”
page containing several large images. Traditional browsers
load the images on a page from top to bottom, so if the user
immediately scrolls within the page after opening it, or clicks
on a link to a text anchor somewhere in the middle of the
page, she must wait until the browser loads the (probably
invisible) images above the visible area before the desired
images begin to appear. Our SST-enabled browser instead
expedites the loading of the image(s) within the currently
visible scroll area—perhaps in particular the image immedi-
ately under the user’s mouse pointer. In this scenario, the
image to be expedited might change at any time as the user
scrolls the window or moves the mouse.
With persistent or pipelined TCP connections, the browser
cannot change the order of requests already in the pipeline,
but with SST the browser and web server can cooperate to
achieve the desired result. The client speciﬁes an initial pri-
ority for each request it submits, and changes the priority
of a request already in progress by spawning a temporary
substream from the request’s original stream and sending
1.5MB
1MB
512K
d
e
r
r
e
f
s
n
a
r
T
s
e
t
y
B
l
a
t
o
T
 0
 0
Request 1
Request 2
Request 3
high-priority request complete
priority change request
high-priority request
 5
 10
 15
 20
 25
 30
Figure 9: Dynamic Request Prioritization
a short “change priority” message on this substream. On
receipt, the server attaches this new priority level to the ap-
propriate SST stream on its end, causing its stream layer to
transmit data for high-priority streams before others. This
prioritization feature required no changes to the SST proto-
col as described in Section 4, and only a minor API extension
to the SST implementation for the server’s use.
Figure 9 shows the behavior observed by the client in a
simple scenario on the usual simulated 1.5Mbps network.
At time zero the client requests two 1.5MB ﬁles at normal
priority, and the server divides return bandwidth evenly be-
tween them. At ﬁve seconds the client submits a third re-
quest labeled high-priority, causing the server to commit all
bandwidth to the new request, temporarily blocking the old
ones. At ten seconds the client submits two priority change
requests, changing Request 1 to high-priority and Request
3 to normal, and the client observes the priority changes
take eﬀect one round-trip later. When Request 1 ﬁnally
completes, the remaining two requests again divide avail-
able bandwidth evenly until they complete as well.
5.7 Wire Efﬁciency
Minimizing the per-packet overhead of transport layer
headers is important to many applications, especially voice
applications that frequently send frames only a few bytes