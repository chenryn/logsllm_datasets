layer.
HFS+ does not support sparse files. However, it does support deferred zeroing of ranges of a file that have
never been written. Such ranges are marked invalid until they are physically writtenfor example, because of a
sync operation.
We will discuss most of these features in this chapter.
12.1. Analysis Tools
Let us first look at some tools and sources of information that will be useful in understanding the
implementation and operation of HFS+.
12.1.1. HFSDebug
We will use hfsdebug, a command-line file system debugger, as a companion program to this chapter.[5] The
term debugger is somewhat of a misnomer because a key feature (rather, a limitation) of hfsdebug is that it
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhC53.htm 20.08.2007
Chapter 12. The HFS Plus File System Page 3 of 81
always operates on an HFS+ volume in read-only mode. It also does not allow for interactive debugging.
Nevertheless, it is meant as a useful tool for exploring HFS+ internals, since it allows you to browse, inspect,
and analyze various aspects of the file system. It has the following primary features.
[5] I created hfsdebug to explore the working of HFS+ and to quantify fragmentation in HFS+
volumes. hfsdebug is available for download on this book's accompanying web site
(www.osxbook.com).
It displays raw details of data structures associated with the volume as a whole. Examples of such data
structures are the volume header, the master directory block (in the case of an HFS+ volume embedded
in an HFS wrapper), the journal files, and the on-disk B-Trees, namely, the Catalog file, the Extents
Overflow file, the Attributes file, and the Hot Files B-Tree. We will discuss each of these data structures
in this chapter.
It displays raw details of data structures associated with individual file system objects such as files,
directories, aliases, symbolic links, and hard links. Examples of such data structures include standard
attributes, extended attributes (including ACLs), and file extents. hfsdebug supports looking up a file
system object in multiple ways, namely, using its catalog node ID (typicallybut not always, as we will
seethe same as the inode number reported by the POSIX API), using a Carbon-style specification
consisting of the object's node name and the catalog node ID of its parent, or using the object's POSIX
path.
It calculates various types of volume statistics, for example, a summary of the numbers and types of file
system objects present on a volume, the space used by these objects, special cases such as invisible and
empty files, the top N files ordered by size or degree of fragmentation, and details of hot files.
It displays details of all fragmented files on a volume.
It displays locations and sizes of all free extents on a volume.
hfsdebug supports only the HFS+ volume formatthe older HFS format is not supported. However, it does
handle HFS+ variants such as journaled HFS+, embedded HFS+, and case-sensitive HFS+ (HFSX).
Since much of the raw information displayed by hfsdebug is not available through a standard programming
interface, hfsdebug works by directly accessing the character device associated with a volume. This means
several things.
 You will require superuser access to use hfsdebug on volumes whose character devices are accessible
only to the superuser.[6] This is the case for the root volume.
[6] When you run hfsdebug as the superuser (via the sudo command, say), it drops its
privileges once it has completed the privileged operations it needs to perform.
 You can use hfsdebug even on a mounted HFS+ volume.
 Since hfsdebug does not access a volume through the volume's associated block device or through some
higher-level API, its operation does not interfere with the buffer cache.
Unlike a block device, which allows I/O to be performed at arbitrary byte offsets, character device I/O
must be performed in units of the device's sector size, with I/O offsets being aligned on a sector
boundary.
 hfsdebug is oblivious of the nature of the underlying storage medium, which could be a disk drive, an
optical disc, or a virtual disk such as a disk image.
Finally, hfsdebug can also display the contents of the in-memory HFS+-specific mount structure
corresponding to a mounted HFS+ volume. This data resides in the kernel as an hfsmount structure
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhC53.htm 20.08.2007
Chapter 12. The HFS Plus File System Page 4 of 81
[bsd/hfs/hfs.h], a pointer to which is held in the mnt_data field of the mount structure
[bsd/sys/mount_internal.h] corresponding to that mount instance. hfsdebug uses the Mach VM interface
to retrieve this data. We are interested in the hfsmount structure because some of its relevant constituents have
no on-disk counterparts.
12.1.2. Interface for Retrieving File System Attributes
HFS+ supports the getattrlist() system call, which allows retrieval of several types of file system
attributes. The complementary system call, setattrlist(), allows those attributes that are modifiable to be
set programmatically. Note that these two system calls are standard vnode operations in the Mac OS X VFS
layerApple-provided file systems typically implement these operations. The attributes accessible through these
calls are divided into the following attribute groups:
Common attribute group (ATTR_CMN_*)attributes applicable to any type of file system objects, for
example, ATTR_CMN_NAME, ATTR_CMN_OBJTYPE, and ATTR_CMN_OWNERID
Volume attribute group (ATTR_VOL_*)for example, ATTR_VOL_FSTYPE, ATTR_VOL_SIZE,
ATTR_VOL_SPACEFREE, and ATTR_VOL_FILECOUNT
 Directory attribute group (ATTR_DIR_*)for example, ATTR_DIR_LINKCOUNT and ATTR_DIR_ENTRYCOUNT
File attribute group (ATTR_FILE_*)for example, ATTR_FILE_TOTALSIZE, ATTR_FILE_FILETYPE, and
ATTR_FILE_FORKCOUNT
Fork attribute group (ATTR_FORK_*)attributes applicable to data or resource forks, for example,
ATTR_FORK_TOTALSIZE and ATTR_FORK_ALLOCSIZE
Besides attributes, getattrlist() can also be used to retrieve volume capabilities, which specify what
features and interfaces (from among a predefined list of features and another list of interfaces) a given volume
supports. We will see an example of using getattrlist() in Section 12.11.
12.1.3. Mac OS X Command-Line Tools
In Chapter 11, we came across several file-system-related command-line tools available on Mac OS X. In
particular, we used the hdiutil program to manipulate disk images. Besides its typical use, hdiutil can print
information about the given HFS+ or HFS volume when used with the hfsanalyze option.
$ sudo hdiutil hfsanalyze /dev/rdisk0s3
0x00000000131173B6 (319910838) sectors total
0x131173B0 (319910832) partition blocks
native block size: 0200
HFS Plus
...
12.1.4. HFS+ Source and Technical Note TN1150
To make the most out of the discussion in this chapter, it would be valuable to have access to the Mac OS X
kernel source. The following parts of the kernel source tree are particularly relevant to this chapter:
bsd/hfs/the core HFS+ implementation
bsd/hfs/hfs_format.hdeclarations of fundamental HFS+ data structures
bsd/vfs/vfs_journal.*implementation of the file-system-independent journaling mechanism
It is also recommended that you have a copy of Apple's Technical Note TN1150 ("HFS Plus Volume Format"),
since it contains information that we refer to (but don't always cover) in this chapter.
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhC53.htm 20.08.2007
Chapter 12. The HFS Plus File System Page 5 of 81
12.2. Fundamental Concepts
Before we look at details of HFS+, let us familiarize ourselves with some fundamental terminology and data
structures.
12.2.1. Volumes
In Chapter 11, we defined a file system as a scheme for arranging data on a storage medium, with a volume
being an instance of a file system. An HFS+ volume may span an entire disk or it may use only a portionthat
is, a slice or a partitionof a disk. HFS+ volumes can also span multiple disks or partitions, although such
spanning is at the device level and thus is not specific to HFS+, which will still see a single logical volume.
Figure 121 shows a conceptual view of a disk containing two HFS+ volumes.
Figure 121. A disk containing two HFS+ volumes
[View full size image]
12.2.2. Allocation Blocks
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhC53.htm 20.08.2007
Chapter 12. The HFS Plus File System Page 6 of 81
Space on an HFS+ volume is allocated to files in fundamental units called allocation blocks. For any given
volume, its allocation block size is a multiple of the storage medium's sector size (i.e., the hardware-
addressable block size). Common sector sizes for disk drives and optical drives are 512 bytes and 2KB,
respectively. In contrast, the default (and optimal, for the Mac OS X implementation of HFS+) allocation
block size is 4KB.
As shown in Figure 121, the storage on an HFS+ volume is divided into some number of equal-size allocation
blocks. The blocks are conceptually numbered sequentially. The file system implementation addresses volume
contents using allocation block numbers, which are 32-bit quantities represented by the u_int32_t data type in
the kernel.
Unlike HFS+, HFS supports only 16-bit allocation block numbers. Therefore, the total space on an HFS
volume can be divided into at most 216 (65,536) allocation blocks. Note that the file system will allocate space
in multiples of the allocation block size regardless of the space that is actually used. Consider a somewhat
contrived example: If you use the HFS volume format on a 100GB disk, the allocation block size will be
1,638,400 bytes. In other words, the file system will allocate 1.6MB even for a 1-byte file.
The allocation block size is a fundamental property of a given volume. You can choose an allocation block
size other than the default when you construct a new HFS+ file systemsay, using the newfs_hfs command-line
program. The following rules apply when choosing an alternate allocation block size.
It must be a power of 2.
It should be a multiple of the sector size of the storage device, with the smallest legal value being the
sector size itself. Thus, for an HFS+ volume on a disk drive, an allocation block must be no smaller than
512 bytes.
 newfs_hfs will not accept an allocation block size larger than MAXBSIZE, which is defined to be 1MB in
. This is not a file system limitation, however, and if you really must, you can have a
larger allocation block size by using another program (or a modified version of newfs_hfs) to construct
the file system.
It is possible for the capacity of a volume to not be a multiple of its allocation block size. In such a case, there
will be trailing space on the volume that would not be covered by any allocation block.
Fragments
An allocation block cannot be shared (split) between two files or even between forks of the same
file. BSD's UFS (including the Mac OS X implementation) employs another unit of allocation
besides a block: a fragment. A fragment is a fraction of a block that allows a block to be shared
between files. When a volume contains a large number of small files, such sharing leads to more
efficient use of space, but at the cost of more complicated logic in the file system.
12.2.3. Extents
An extent is a range of contiguous allocation blocks. It is represented in HFS+ by the extent descriptor data
structure (struct HFSPlusExtentDescriptor [bsd/hfs/hfs_format.h]). An extent descriptor contains a
pair of numbers: the allocation block number where the range starts and the number of allocation blocks in the
range. For example, the extent descriptor { 100, 10 } represents a sequence of 10 consecutive allocation
blocks, beginning at block number 100 on the volume.
struct HFSPlusExtentDescriptor {
u_int32_t startBlock; // first allocation block in the extent
u_int32_t blockCount; // number of allocation blocks in the extent
};
typedef struct HFSPlusExtentDescriptor HFSPlusExtentDescriptor;
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhC53.htm 20.08.2007
Chapter 12. The HFS Plus File System Page 7 of 81
typedef HFSPlusExtentDescriptor HFSPlusExtentRecord[8];
An eight-element array of HFS+ extent descriptors constitutes an extent record.[7] HFS+ uses an extent record
as an inline extent list for a file's contentsthat is, up to the first eight extents of a file (specifically, a file fork;
see the next section) are stored as part of the file's basic metadata. For a file that has more than eight extents,
HFS+ maintains one or more additional extent records, but they are not kept inline in the metadata.
[7] More precisely, it should be called an extents record, since it contains multiple extents.
However, we will call it an extent record based on the data structure's name.
12.2.4. File Forks
A file is traditionally equivalent to a single stream of bytes. HFS+ supports multiple byte-streams per file, with
two special streams that are always present, although one or both may be empty (zero-size). These are the data
fork and the resource fork. Each fork is a distinct part of the file and may be perceived as a file in itself. A fork
is represented in HFS+ by the HFSPlusForkData structure [bsd/hfs/hfs_format.h].
struct HFSPlusForkData {
u_int64_t logicalSize; // fork's logical size in bytes
u_int32_t clumpSize; // fork's clump size in bytes
u_int32_t totalBlocks; // total blocks used by this fork
HFSPlusExtentRecord extents; // initial set of extents
};
typedef struct HFSPlusForkData HFSPlusForkData;
Both forks have their own HFSPlusForkData structures (and therefore, extent records) that are stored along
with the file's standard metadata.
The traditional view of a file maps to its data fork. Most files on a typical Mac OS X installation have only the
data forktheir resource forks are empty.
The Launch Services framework accesses a file's resource fork to retrieve the path to the application to use to
open that file, provided such an application has been specified for that particular filesay, through the "Open
with" section of the Finder's information window.
Another noteworthy aspect of the data and resource forks is that their names cannot be changed. Any
additional byte-streams created have Unicode names. These named streams can have arbitrary contents,
although an HFS+ implementation may limit the amount of data a named stream can hold.
Beginning with Mac OS X 10.4, named streams are used to provide native support for extended attributes,
which in turn are used to provide native support for access control lists.
12.2.5. Clumps
Whereas an allocation block is a fixed-size group (for a given volume) of contiguous sectors, a clump is a
fixed-size group of contiguous allocation blocks. Although every clump is an extent, not every extent is a
clump. When allocating space to a fork, an HFS+ implementation may do so in terms of clumpsrather than
individual allocation blocksto avoid external fragmentation.
HFS+ has a provision for default clump sizes for the volume as a whole, for each B-Tree, for all data forks,
and for all file forks. Note in Section 12.2.4 that the HFSPlusForkData structure has a field called clumpSize.
Although this field could be used to support per-fork clump size specifications, HFS+ implementations with
support for Hot File Clustering use this field to record the number of allocation blocks read from that fork.
12.2.6. B-Trees
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhC53.htm 20.08.2007
Chapter 12. The HFS Plus File System Page 8 of 81
HFS+ uses B-Trees to implement its critical indexing data structures that make it possible to locate both file
content and metadata residing on a volume.
The Popularity of B-Trees
B-Trees were discovered by Rudolf Bayer and Edward M. McCreight in 1970.[8] They were
subsequently described in a 1972 paper titled "Organization and Maintenance of Large Ordered
Indexes."[9] Since then, B-Trees have been immensely popular and successful as an efficient and
scalable external index mechanism. B-Tree variants, especially B+ Trees, are widely used in
relational databases, file systems, and other storage-based applications. Microsoft's NTFS also
uses B+ Trees for its catalog.
[8] The authors never disclosed what the "B" in B-Trees stands for. A few plausible explanations
are often cited: "balanced," "broad," "Bayer," and "Boeing." The latter is in the picture because the
authors worked at Boeing Scientific Research Labs at that time.
[9] "Organization and Maintenance of Large Ordered Indexes," by Rudolf Bayer and Edward M.
McCreight (Acta Informatica 1, 1972, pp. 173189).
A B-Tree is a generalization of a balanced binary search tree. Whereas a binary tree has a branching factor of
two, a B-Tree can have an arbitrarily large branching factor. This is achieved by having very large tree nodes.
A B-Tree node may be thought of as an encapsulation of many levels of a binary tree. Having a very large
branching factor leads to very low tree height, which is the essence of B-Trees: They are exceptionally suited
for cases where the tree structure resides on an expensive-to-access storage medium, such as a disk drive. The
lower the height, the fewer the number of disk accesses required to perform a B-Tree search. B-Trees offer
guaranteed worst-case performance for common tree operations such as insertion, retrieval, and deletion of
records. The operations can be implemented using reasonably simple algorithms, which are extensively
covered in computing literature.
We will not discuss the theory behind B-Trees in this chapter. Please refer to an algorithms textbook for
further details on B-Trees.
12.2.6.1. B+ Trees in HFS+
HFS+ specifically uses a variant of B+ Trees, which themselves are B-Tree variants. In a B+ Tree, all data
resides in leaf (external) nodes, with index (internal) nodes containing only keys and pointers to subtrees.
Consequently, index and leaf nodes can have different formats and sizes. Moreover, the leaf nodes, which are
all at the same (lowest) level in the balanced tree,[10] are chained together from left to right in a linked list to
form a sequence set. Whereas the index nodes allow random searching, the list of leaf nodes can be used for
sequential access to the data. Note that since data corresponding to a key can be found only in a leaf node, a
B+ Tree searchstarting from the root nodealways ends at a leaf node.
[10] In other words, all paths to leaf nodes have the exact same length in a balanced B-Tree.
The HFS+ implementation of B+ Trees differs from the standard definition in one notable respect. In a B+
Tree, an index node I containing N keys has N + 1 pointersone to each of its N + 1 children. In particular, the
first (leftmost) pointer points to the child (subtree) containing keys that are less than the first key of node I.
This way, node I serves as an (N + 1)-way decision point while searching, with each pointer leading to the next
level in the search based on which of the N + 1 ranges, if any, the search key falls in. The B+ Trees used in
HFS+ do not have the leftmost pointer in their index nodesthat is, for an index node I, there is no leftmost
subtree containing keys that are less than the first key of I. This means each index node with N keys has N
pointers.
Hereafter, we will use the term B-Tree to refer to the HFS+ implementation of B+ Trees.
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhC53.htm 20.08.2007
Chapter 12. The HFS Plus File System Page 9 of 81
Although HFS+ uses several B-Trees, with differing key and data formats, all its trees share the same basic
structure. In fact, all HFS+ B-Trees can be accessed and manipulated using mostly the same set of
functionsonly certain operations (such as key comparison) require code specific to tree content. The following
are common characteristics of HFS+ B-Trees.
Each B-Tree is implemented as a special file that is neither visible to the user nor accessible through any
standard file system interface. Unlike regular files, which have two predefined forks, a special file has
only one fork for holding its contents. The Hot File Clustering B-Tree is an exception, however.
Although it is a system file, it is implemented as a regular file that is user-visible and has two predefined
forks (the resource fork is empty).
An HFS+ system file is designated as such by the CD_ISMETA bit being set in the cd_flags field of the in-
memory catalog node descriptor corresponding to that file.
The total space in a B-Tree file is conceptually divided into equal-size nodes. Each B-Tree has a fixed
node size that must be a power of 2, ranging from 512 bytes to 32,768 bytes. The node size is
determined when the volume is created and cannot be changedat least using standard utilitieswithout
reformatting the volume. Moreover, each HFS+ B-Tree also has some initial size that is determined
based on the size of the volume being created.
Nodes are numbered sequentially starting from zero. Therefore, the offset of a node numbered N in a
given B-Tree is obtained by multiplying N with the tree's node size. A node number is represented as a
32-bit unsigned integer.
Each B-Tree has a single header node (of type kBTHeaderNode) that is the first node in the tree.
Each B-Tree has zero or more map nodes (of type kBTMapNode) that are essentially allocation bitmaps
used for tracking which tree nodes are in use and which are free. The first part of the allocation bitmap
resides in the header node, so one or more map nodes are required only if the entire bitmap doesn't fit in
the header node.
Each B-Tree has zero or more index nodes (of type kBTIndexNode) that contain keyed pointer records
leading to other index nodes or a leaf node. Index nodes are also called internal nodes.
Each B-Tree has one or more leaf nodes (of type kBTLeafNode) that contain keyed records holding the
actual data associated with the keys.
All node types can hold variable-length records.
12.2.6.2. Nodes
Figure 122 shows the structure of a generic B-Tree node. Note that the nodes are only logically contiguouslike