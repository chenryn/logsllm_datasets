the YouTube video on the Chrome browser using Selenium [58]. The video reso-
lution is set to auto. Then we use YouTube’s iframe API [65] to capture playback
events reported by the video player. The API outputs a set of values that indi-
cate player state (not started, paused, playing, completed, buﬀering) using the
getPlayerState() function. The API also provides functions for accessing infor-
mation about play time and the remaining buﬀer size.
2.3 Description of Datasets
We collect 16 datasets from 12 locations across the Southwestern U.S. Eight of
the datasets were collected from rural locations that had sparse cellular deploy-
ment.
An additional eight datasets were collected from four urban locations. In each
urban location, we collect two datasets: one during a large event or gathering, in
which we expect cellular network congestion to occur (these datasets are marked
with Cong); and a second during typical operating conditions. We call the latter
dataset the baseline for that location (these datasets are marked with Base).
Hence, our 16 traces are broadly classiﬁed into three categories: rural, congested
urban, and baseline urban. The details of each dataset are summarized in Table 2.
The designation of each location as rural or urban is based on Census Bureau
data [57]. Through these measurement campaigns, we collect and analyze over
32.7 Million LTE packets. Note that the “Number of Datapoints” column shown
146
V. Adarsh et al.
in Table 1 indicates the QoS/QoE datapoints gathered by the application, while
the “# LTE Packets” column in Table 2 refers to the number of packets collected
in the trace ﬁles.
Location
Rural 1
Rural 2
Rural 3
Rural 4
Rural 5
Rural 6
Rural 7
Rural 8
Table 2. Summary of datasets
Date
May 28 2019
May 29 2019
May 28 2019
May 30 2019
May 30 2019
May 31 2019
May 31 2019
Jun 01 2019
# LTE Packets
3.18 Million
1.38 Million
2.03 Million
2.16 Million
2.27 Million
2.33 Million
1.26 Million
2.83 Million
Type
Rural
Rural
Rural
Rural
Rural
Rural
Rural
Rural
Carriers∗
V,A,T,S
V,T
V,A,T,S
V,A,T,S
V,A,T,S
V,A,T,S
V,T
V,A,T,S
Urban 1 Cong
Urban 1 Base
Urban 2 Cong
Urban 2 Base
Urban 3 Cong
Urban 3 Base
Urban 4 Cong
Urban 4 Base
∗This column lists mobile carriers in each data set (some areas had no coverage for particular
network operators). V: Verizon, A:AT&T, T:T-Mobile, S: Sprint.
Urban, Congested
Urban, Baseline
Urban, Congested
Urban, Baseline
Urban, Congested
Urban, Baseline
Urban, Congested
Urban, Baseline
Sep 22 2019
Sep 28 2019
Sep 29 2019
Sep 30 2019
Sep 21 2019
Sep 30 2019
Sep 25 2019
Sep 26 2019
2.25 Million
1.92 Million
2.51 Million
1.97 Million
2.65 Million
2.13 Million
2.18 Million
2.08 Million
V,A,T,S
V,A,T,S
V,A,T,S
V,A,T,S
V,A,T,S
V,A,T,S
V,A,T,S
V,A,T,S
2.4 Video QoE Measurement Scalability Challenges
Collection of ground-truth cellular network measurements, as we explore further
in Sect. 4, is a challenging task for multiple reasons. First, it requires physical
placement of measurement device at the location to be studied. While there
are many large, publicly accessible datasets that incorporate some QoS mea-
surements, QoE measurements, particularly in remote regions, are much more
diﬃcult. Second, gathering ground truth data to assess video QoE requires an
active connection to stream a large encoded video ﬁle. This consumes a substan-
tial amount of bandwidth, computational power, memory, and battery, due to
the simultaneous use of LTE modems, display, CPU, and GPU [21] on the user
device. For instance, streaming applications consume memory to load the video
and require accelerated processing to decode and display the stream from the
video server. Unlike QoS metrics, which can often be collected in the background
through execution by back-end scripts, the high resource cost of QoE measure-
ments for the end user makes this data diﬃcult to crowd-source. In Fig. 1 we
show the resource consumption during one hour of RSRP and throughput (QoS)
measurements, compared to one hour of video streaming (QoE), on our data
collection phones. As can be seen in the ﬁgure, the resources consumed by the
QoE measurements were signiﬁcantly higher, both preventing background data
collection and more rapidly draining the device battery.
Too Late for Playback
147
a. CPU Load
b. Memory utilization
c. System temperature
Fig. 1. Device resource consumption during either RSRP and throughput measure-
ments only, or during video streaming.
Rural regions span large geographic areas with terrain that is often hard to
access. QoS data from public sources already struggles to cover these areas. In
particular, crowd-sourced datasets are data-rich in regions where there are higher
density populations. These regions tend to be either urban areas, or other areas
frequented by travelers (i.e. highways, national parks, etc.). Rural communities,
by contrast, with their lower population densities, are often under-represented in
crowd-sourced datasets. Yet it is exactly these regions where under-provisioned
networks typically exist and hence where data is urgently needed. In order to
eﬀectively assess QoE in these remote areas, we need a method to improve QoE
measurement scalability. We address this challenge in the next section, where
we show how predictive models can use the less resource expensive QoS mea-
surements to infer QoE for streaming video on mobile broadband networks in a
variety of environments.
3 Inferring QoE Metrics for Video
As discussed in Sect. 2.4, the collection of QoS measurements is less resource
consumptive, and hence more scalable, than video QoE measurements. We now
describe our approach to infer QoE metrics for video streaming sessions using
low-cost QoS metrics.
3.1 Learning Problem
Our learning problem’s goal is to infer QoE metrics using a sequence of through-
put and RSRP (QoS metrics) data input. The objective is to build models with
appreciable performance that would work in a wide variety of network conditions
and diﬀerent region types (e.g., rural and urban locations). These models could
be used to predict application QoE (in our case, video streaming) at a particular
location. We use supervised learning to train two diﬀerent binary classiﬁers. The
ﬁrst classiﬁer infers whether the video’s state is stalled or normal; the second
infers whether there is any change in video resolution. Both models perform the
classiﬁcation task every one second.
148
V. Adarsh et al.
Input: The learning model takes a sequence of RSRP and throughput values
as input. Both of these metrics are low-cost measurements and easily accessible.
Given how adaptive bitrate (ABR) video streaming players operate, the changes
in throughput and RSRP values have a delayed impact on QoE metrics. For
example, a decrease in available throughput will force the video streaming player
to use the buﬀered data before stalling.
As part of feature engineering, we had to determine how many RSRP and
throughput values to use as input for the learning model. Intuitively, the use of
longer sequences will improve accuracy. However, longer sequences also increases
the complexity of the learning model, which requires more training data to avoid
over-ﬁtting. After varying n = 0 → 180 (total playback time of a session), we
found that using a sequence of three throughput/RSRP values enabled us to
strike a balance between model complexity and accuracy. A typical approach to
assessing throughput would be to log continuous measurements for a long dura-
tion of time and analyze the resulting mean/mode of the distribution. However,
our results (Sect. 3.3) indicate that we can infer the video quality from only a
3-s sample. This has the added beneﬁt of reducing the resource utilization at the
client device, such as data consumption and battery drainage, while accurately
inferring the video stream quality.
Output: We train two separate binary classiﬁers to predict the video state and
change in resolution at the granularity of one second. Predicting QoE metrics
at such ﬁne granularity enables opportunities to infer QoE with limited training
data. Given the input features, our models infer how likely it is for the video
stream to experience either a video stall or a resolution change in the next
instant.
Training Data: Our dataset consists of 32,596 data points. Each data point
has input values: a sequence of three RSRP and throughput values, as well
as two boolean labels: video state (playing or stalled) and resolution switches
(yes–resolution will change; no–resolution will not change). We collected this
dataset through our measurement campaign by conducting a total of 181 video
streaming sessions across multiple locations (Sect. 2.3). For each classiﬁer, we
label the output training samples into either of the two classes: class 0 is when
playback is normal and devoid of any event (rebuﬀering or resolution switch),
and class 1 is when there is an event. We carried out the classiﬁcation task by
splitting the entire dataset into a ratio of 70:30 training to test sets, as described
in Table 3. We split the overall training dataset into training and validation
sets (80:20). We chose the samples proportionate to the size of each dataset
category (rural, congested urban, and baseline urban). We present the models’
performance per location, where we train the models on speciﬁc locations and
then test on others not included in the training. We do not make any distinctions
between operators since an operator-agnostic evaluation is a more comprehensive
reﬂection of coverage and QoE at a particular location.
Table 3. Breakdown of training and test set samples for both classiﬁers.
Too Late for Playback
149
Classiﬁer Type
Target Metric
Training Set
Class 0
Class 1
Test Set
Class 0
Class 1
Classiﬁer 1
Classiﬁer 2
Rebuﬀering Event
Resolution Switching
22,175
22,490
642
327
9,504
9,639
275
140
3.2 Learning Algorithm