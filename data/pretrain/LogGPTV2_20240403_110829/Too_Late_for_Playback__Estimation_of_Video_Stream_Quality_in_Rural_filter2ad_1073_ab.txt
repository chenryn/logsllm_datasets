### 2.3 Description of Datasets

We collected 16 datasets from 12 locations across the Southwestern United States. Eight of these datasets were gathered from rural areas with sparse cellular deployment, while the remaining eight were collected from four urban locations. For each urban location, we collected two datasets: one during a large event or gathering, where we expect cellular network congestion (marked as "Cong"), and another during typical operating conditions (marked as "Base"). Thus, our 16 datasets are categorized into three groups: rural, congested urban, and baseline urban. The classification of each location as rural or urban is based on Census Bureau data [57]. Through these measurement campaigns, we collected and analyzed over 32.7 million LTE packets. Note that the "Number of Datapoints" column in Table 1 indicates the QoS/QoE datapoints gathered by the application, while the "# LTE Packets" column in Table 2 refers to the number of packets collected in the trace files.

**Table 2. Summary of Datasets**

| Location       | Date         | # LTE Packets | Type             | Carriers* |
|----------------|--------------|---------------|------------------|-----------|
| Rural 1        | May 28, 2019 | 3.18 Million  | Rural            | V, A, T, S |
| Rural 2        | May 29, 2019 | 1.38 Million  | Rural            | V, T      |
| Rural 3        | May 28, 2019 | 2.03 Million  | Rural            | V, A, T, S |
| Rural 4        | May 30, 2019 | 2.16 Million  | Rural            | V, A, T, S |
| Rural 5        | May 30, 2019 | 2.27 Million  | Rural            | V, A, T, S |
| Rural 6        | May 31, 2019 | 2.33 Million  | Rural            | V, A, T, S |
| Rural 7        | May 31, 2019 | 1.26 Million  | Rural            | V, T      |
| Rural 8        | Jun 01, 2019 | 2.83 Million  | Rural            | V, A, T, S |
| Urban 1 Cong   | Sep 22, 2019 | 2.25 Million  | Urban, Congested | V, A, T, S |
| Urban 1 Base   | Sep 28, 2019 | 1.92 Million  | Urban, Baseline  | V, A, T, S |
| Urban 2 Cong   | Sep 29, 2019 | 2.51 Million  | Urban, Congested | V, A, T, S |
| Urban 2 Base   | Sep 30, 2019 | 1.97 Million  | Urban, Baseline  | V, A, T, S |
| Urban 3 Cong   | Sep 21, 2019 | 2.65 Million  | Urban, Congested | V, A, T, S |
| Urban 3 Base   | Sep 30, 2019 | 2.13 Million  | Urban, Baseline  | V, A, T, S |
| Urban 4 Cong   | Sep 25, 2019 | 2.18 Million  | Urban, Congested | V, A, T, S |
| Urban 4 Base   | Sep 26, 2019 | 2.08 Million  | Urban, Baseline  | V, A, T, S |

*Carriers: V = Verizon, A = AT&T, T = T-Mobile, S = Sprint

### 2.4 Video QoE Measurement Scalability Challenges

Collecting ground-truth cellular network measurements, as explored further in Section 4, is challenging for several reasons. First, it requires physically placing a measurement device at the location of interest. While there are many large, publicly accessible datasets that include some QoS measurements, QoE measurements, especially in remote regions, are much more difficult to obtain. Second, gathering ground truth data to assess video QoE requires an active connection to stream a large encoded video file. This consumes substantial bandwidth, computational power, memory, and battery, due to the simultaneous use of LTE modems, display, CPU, and GPU [21] on the user device. Streaming applications consume memory to load the video and require accelerated processing to decode and display the stream from the video server. Unlike QoS metrics, which can often be collected in the background through back-end scripts, the high resource cost of QoE measurements makes this data difficult to crowdsource. In Figure 1, we show the resource consumption during one hour of RSRP and throughput (QoS) measurements compared to one hour of video streaming (QoE) on our data collection phones. As shown in the figure, the resources consumed by QoE measurements were significantly higher, preventing background data collection and more rapidly draining the device battery.

**Figure 1. Device Resource Consumption During RSRP and Throughput Measurements vs. Video Streaming**
- **a. CPU Load**
- **b. Memory Utilization**
- **c. System Temperature**

Rural regions span large geographic areas with terrain that is often hard to access. Publicly available QoS data already struggles to cover these areas. Crowd-sourced datasets are typically rich in regions with higher population densities, such as urban areas or frequently traveled routes (e.g., highways, national parks). Rural communities, with their lower population densities, are often underrepresented in crowd-sourced datasets. However, it is precisely these regions where under-provisioned networks exist and where data is urgently needed. To effectively assess QoE in these remote areas, we need a method to improve QoE measurement scalability. We address this challenge in the next section, where we show how predictive models can use less resource-intensive QoS measurements to infer QoE for streaming video on mobile broadband networks in various environments.

### 3. Inferring QoE Metrics for Video

As discussed in Section 2.4, collecting QoS measurements is less resource-intensive and more scalable than video QoE measurements. We now describe our approach to inferring QoE metrics for video streaming sessions using low-cost QoS metrics.

#### 3.1 Learning Problem

Our goal is to infer QoE metrics using a sequence of throughput and RSRP (QoS metrics) data. The objective is to build models that perform well in a wide variety of network conditions and different region types (e.g., rural and urban locations). These models can predict the QoE for video streaming at a particular location. We use supervised learning to train two different binary classifiers:
1. The first classifier infers whether the video's state is stalled or normal.
2. The second classifier infers whether there is any change in video resolution.

Both models perform the classification task every second.

**Input:**
The learning model takes a sequence of RSRP and throughput values as input. Both metrics are low-cost and easily accessible. Adaptive bitrate (ABR) video streaming players operate such that changes in throughput and RSRP values have a delayed impact on QoE metrics. For example, a decrease in available throughput will force the player to use buffered data before stalling.

As part of feature engineering, we determined the optimal number of RSRP and throughput values to use as input. Intuitively, longer sequences improve accuracy but also increase model complexity, requiring more training data to avoid overfitting. After varying \( n \) from 0 to 180 (total playback time of a session), we found that using a sequence of three throughput/RSRP values strikes a balance between model complexity and accuracy. Our results (Section 3.3) indicate that we can infer video quality from just a 3-second sample, reducing resource utilization at the client device, such as data consumption and battery drainage, while accurately inferring the video stream quality.

**Output:**
We train two separate binary classifiers to predict the video state and change in resolution at the granularity of one second. Predicting QoE metrics at such fine granularity enables accurate inference with limited training data. Given the input features, our models infer the likelihood of the video stream experiencing a stall or a resolution change in the next instant.

**Training Data:**
Our dataset consists of 32,596 data points. Each data point includes:
- Input values: a sequence of three RSRP and throughput values.
- Output labels: video state (playing or stalled) and resolution switches (yes or no).

We collected this dataset through our measurement campaign, conducting a total of 181 video streaming sessions across multiple locations (Section 2.3). For each classifier, we labeled the output training samples into two classes: class 0 (normal playback without events) and class 1 (playback with rebuffing or resolution switch). We split the entire dataset into a 70:30 training-to-test ratio, as described in Table 3. We further split the training dataset into training and validation sets (80:20). We chose the samples proportionally to the size of each dataset category (rural, congested urban, and baseline urban). We present the models' performance per location, training the models on specific locations and testing them on others not included in the training. We do not distinguish between operators, as an operator-agnostic evaluation provides a more comprehensive reflection of coverage and QoE at a particular location.

**Table 3. Breakdown of Training and Test Set Samples for Both Classifiers**

| Classifier Type | Target Metric          | Training Set | Test Set |
|-----------------|------------------------|--------------|----------|
|                 |                        | Class 0      | Class 1  |
|                 |                        | Class 0      | Class 1  |
| Classifier 1    | Rebuffering Event      | 22,175       | 642      | 9,504    | 275      |
| Classifier 2    | Resolution Switching   | 22,490       | 327      | 9,639    | 140      |

#### 3.2 Learning Algorithm

[Continue with the description of the learning algorithm and further details as needed.]