CVEs are not yet published on NVD, as of April 30th 2019. Those
CVEs have been mentioned at least 1 day earlier than NVD and at
most 450 days earlier than NVD (mentioned 46 days earlier than
NVD on average). We are also interested in detecting the CVEs
that already have published in NVD, but have mentioned again on
Twitter in some reasons. If discourses on a CVE in NVD are rapidly
increased, then organizations need to evaluate its risk on their prod-
ucts/services/infrastructures and check whether the CVE is patched.
Unlike threat words detection algorithms introduced above, we use
CVE lists fed from NVD right before the time t of event detection as
the dictionary DCVE, instead of DTech, DWhitelist, and DCommon.
Let CCVE be a set of CVE IDs obtained from tweets collected in
between time t − 1 and t. In CVE monitoring, if CVE IDs in CCVE
are not in DCVE, then those CVEs are identified as new words. To
avoid typos, we eliminate the CVE IDs not found in MITRE. For
CVE IDs in CCVE ∩ DCVE, we check if each CVE ID is mentioned
enough and it shows a rapid rise in its occurrence. For re-emerging
020406080100120 Frequncy2017−12−012017−12−102017−12−192017−12−282018−01−062018−01−152018−01−24intelspectreSession 13: Malware ASIA CCS ’20, October 5–9, 2020, Taipei, Taiwan672Figure 7: The number of mentions about “wifi” from Dec 2018 to Jan 2019. The grey dotted line represents the upper bound for frequency
of “wifi”. The magenta spotted circles represent the days that “wifi” is flagged as re-emerging word. On Jan 3rd 2019, our algorithm detects
3 events about wifi where the biggest event was “Marvell Avanstar Wifi SoC bug” and mentioned 9 times. The set of words in brackets is the
words detected by our algorithm and indicates one event.
annotator made the judgement by referring to either the external
links in a tweet or Google search. For (iii), we took 82 events that
fell in the malware, vulnerability, exploit, DDoS attack, and data
breach event types from 105 security events in Section 3. We an-
alyzed whether W2E detected them and, if so, when they were
detected. For latency computation, we referred to the date of the
first tweet about an event over the whole Twitter that we inves-
tigated in Section 3. We remind that W2E categorizes any events
related to ransomware, spyware, trojans, botnet, rootkits, adware,
keyloggers, and any other malware into the malware event type.
In our implementation, we use 200 keywords1 from single words
to terms – 28 keywords are for malware-related events, 11 for
exploit-related events, 20 for vulnerability-related events, 6 for
DDoS attacks, and 17 for data breaches. The initial keywords were
chosen by reviewing the terms in CWE [10], CAPEC [8], STIX [19],
and ENISA Threat Taxonomy [12]. We then included the plural
form, inflections, and alias of each keyword into our keyword set.
We set α = 0.05 for both new words and re-emerging words detec-
tion. We use the dictionaries1 that were constructed as explained in
Section 4. Note that there were 72,623 words in DCommon, 16,014
words in DTech and 3,078 words in DWhitelist by the end of 2017.
We use Stanford CoreNLP for POS tagging and NER.
5.2 Evaluation Results
Clustering accuracy. To measure the clustering accuracy of our
event generator, we compared the estimated cluster by our event
generator to the human-labeled cluster and then computed nor-
malized mutual information (NMI) [2]. Note that NMI is one of the
popular metrics to evaluate clustering quality. It is always a num-
ber between 0 and 1, and 1 means the perfect clustering. Figure 9
presents daily NMI of our event generator in April 2019. NMI was
larger than 0.9 in most of days in the selected month. The aver-
age NMI over the month is 0.96 with standard deviation (SD) 0.06,
which indicates that our event generator performs well enough
Figure 8: The number of days for pre-NVD CVEs found in Twitter to
be published on NVD. The value on bar is the number of pre-NVD
CVEs.
5 EVALUATION
5.1 Evaluation Setup
We have run W2E on a daily basis over tweets collected from 560
Twitter users during the period from January 2018 to April 2019. The
total volume of our Twitter dataset is 1,647,629 including retweets.
We evaluate the performance of W2E in 3 aspects - (i) Clustering
accuracy for daily event generation, (ii) Daily event detection accu-
racy, and (iii) Coverage and detection latency. For (i) and (ii), we
selected the results of W2E in April 2019. Note that we observed
similar results in another selected month (June 2018) to April 2019
although we did not report it here. There were roughly 5,900 unique
tweets triggered by new/re-emerging words. Five security experts
manually annotated the cluster label of each tweet and decided
whether a detected event is a genuine security event or not. The
051015 Frequncy2018−12−012018−12−072018−12−132018−12−192018−12−252018−12−312019−01−062019−01−122019−01−182019−01−242019−01−30[playstations, avanstar, marvell, chromebooks, samsung, preauth, steamlink, ps4][cve−2018−20512, epon, cpe−wifi, coouser, coologin][chipset exploit][wpa, wpa2][gerix][orange, livebox, ssid][gerix, gui][livebox][playstations, avanstar, marvell, chromebooks, threadx][heatmiser][cve−2018−12177, zeroconfig, proset][chromebooks, samsung, ps4][firmware, chipset]wifi159141927333849617994110143155182216323 Frequency0102030405051282715886557Session 13: Malware ASIA CCS ’20, October 5–9, 2020, Taipei, Taiwan673to split different tweets sharing the same event-specific word or
merge similar tweets into one cluster with small errors.
Figure 9: Daily NMI of our event generator in April 2019.
Daily event detection accuracy. We measure how many false
positives are generated daily. Table 1 shows the precision of W2E
over all the daily events in the selected month. The overall precision
of W2E is 80% and the precision for each threat type is nearly or
larger than 70%. W2E detected 2,359 daily events in total (79 daily
on average), which form 930 unique events. Among 930 unique
events, 763 events were genuine security events, so the precision
of W2E for the unique events in the month is 82%. Table 4 shows
examples of some important events in April 2019 detected by W2E.
Coverage and detection latency. We analyze the coverage of
W2E for the events related to malware, vulnerability, exploit, DDoS
attack, and data breach in Section 3. Table 2 shows the recall of
W2E over 82 security events. The overall recall of W2E is 89% – 73
out of 82 events were detected. Among 73 events, 26 events were
detected only by re-emerging words because they do not include
any new terms in their tweets. This verifies the importance of re-
emerging words monitoring. Although W2E shows high recall for
malware attacks, exploit incidents, vulnerabilities, and data breach
incidents, the recall for DDoS attacks is relatively low. We think
that a lower coverage of DDoS attacks is because discussions about
such events are likely to be started by any users who suffer from
those attacks rather than security-minded users, as we observed
in Section 3. We also observe that W2E leads to 0.67 days delay in
detection on average after the first seen in Twitter. Nonetheless,
45 out of 82 events were detected on the day that the event first
appears on Twitter. Also, 17 events were detected the next day.
Note that, although 29 out of 82 events were mentioned less than
10 times on their first day in the whole Twitter, W2E could detect
12 events among them. In particular, W2E can detect botnet attacks,
exploit incidents, and vulnerabilities without almost no latency,
which is quite meaningful from early event detection perspectives.
In addition, we observe that 31 out of 82 events were detected by
W2E on the first day of events while there were 2.68 days delay
in detection on average since the first day of the event. Note that
there were 1.89 days delay between the first tweet and the first day
of events. Table 3 lists some exemplary events that were detected
by W2E on the day that the event first appeared.
5.3 Comparison with the existing methods
As discussed in Section 2, there are several cybersecurity event
detection methods. Among them, we chose Ritter et al. [40] for
performance comparison. Since Khadpur et al. [27] and Le Sceller
et al. [29] require large volume of mentions about an event to
be detected, they are not the competitors to W2E. On the other
hand, Ritter et al. [40] and Sapienza et al. [44] are designed to
detect events regardless of the number of mentions. Since W2E
monitors both new words and re-emerging words, it widens the
event detection coverage of Sapienza et al. [44]. Further, it greatly
reduces false positives compared to Sapienza et al .[44] by applying
many advanced NLP techniques to extract the monitoring words.
Thus, we did not compare W2E to Sapienza et al. [44].
For comparison with Ritter et al. [40], we collected their event
detection results in April 2019 from the web [20]. They detect events
and categorize into 4 event types: data breach, DDoS, exploit, and
vulnerability. There were 451 tweets in total. The same annotators
judged whether each event is a genuine security event or not.
While W2E monitors the selected Twitter users for event detec-
tion, Ritter et al. [40] monitor the whole Twitter with specific key-
words. Thus, their method is supposed to have lower false negative
rate and earlier event detection than W2E. However, the compari-
son results show that W2E outperforms it – higher event detection
coverage, lower detection latency, and lower false positive rate.
Specifically, the precision of Ritter et al.’s method over all the daily
events in data breach, DDoS, exploit, and vulnerability categories
is 62% while that of W2E is 82%. For data breach, DDoS, exploit,
and vulnerability categories, the number of unique security events
detected by Ritter et al.’s method was 129 while that for W2E was
537, where 87 events were detected by both methods. W2E can
cover 67% of security events detected by Ritter et al.’s method while
Ritter et al.’s method covers only 16% of security events detected by
W2E. In Table 4, we present some important events in April 2019
that were detected only by W2E, but not by Ritter et al.’s method.
For example, W2E detected an event about prototype pollution
flaw in jQuery JavaScript library, which is used on 74 percent of
all internet sites [17], from April 16th 2019 and came up for few
consecutive days. On the other hand, Ritter et al.’s method failed
to detect it. Considering the popularity of jQuery, missing such
vulnerability event may lead an undesirable situation.
For 87 events that both methods detected, we analyzed their first
detection date. Table 5 shows the comparison results in detection
time. We found that 29 events were detected earlier by W2E (2.7 days
earlier on average) while 8 events were detected earlier by Ritter et
al.’s method (2 days earlier on average). The remaining 50 events
were detected on the same day by both methods. Although W2E
collects data from a limited Twitter users, it can detect important
security events much more and earlier than Ritter et al.’s method.
We believe that higher false negative rate of Ritter et al.’s method
is due to a poor performance of NER.
5.4 Case Studies
Among several events detected by W2E from January 2018 to April
2019, we have chosen 4 events – Lokibot malware, Drupal vulnera-
bility, Firebase data breach, and WiFi firmware bug.
Lokibot (Malware) – This malware is a Trojan horse that steals in-
formation from the compromised computer. Trustwave researchers
found new spam campaign pushing Lokibot and it was broadcasted
by news media like Threatpost with its analysis and mitigation
guidance on April 5th 2019. On the same day, W2E also detected
the event with the words “zipx”, “png”, and “lokibot”. However, the
                              0.800.850.900.951.00 NMI2019−04−012019−04−052019−04−092019−04−132019−04−172019−04−212019−04−252019−04−29Session 13: Malware ASIA CCS ’20, October 5–9, 2020, Taipei, Taiwan674Threat Type
Precision
Table 1: Precision of W2E over all the detected daily events.
# Total Daily Events Daily Precision
SD
13.7%
10.5%
15.2%
22.2%
25.3%
Mean
72.8%
89.5%
77.2%
88.8%
69.5%
Malware
Exploit
DDoS
Data breach
Vulnerability
75.1%
89.9%
77.2%
91.2%
69.5%
80.0%
819
519
697
193
131
2359
Total
# Daily Events
SD
Mean
13.2
27.3
17.3
9.8
13.5
23.2
4.3
6.4
4.4
3.1
Table 2: Recall of W2E over 82 security events in 2018.
Threat Type
Malware
Exploit
Vulnerability
DDoS
Data breach
Total
# Events
Latency (days)
# Detected Eventsa
Recall
89%
100%
100%
67%
92%
89%
aValues in the parenthesis are the number of events detected only by re-emerging words,
34 (8)
6 (5)
13 (6)
8 (3)
12 (4)
73 (26)
0.76
0.17
0.46
1.12
0.58
0.67
38
6
13
12
13
82
20 (5)
5 (3)
10 (3)