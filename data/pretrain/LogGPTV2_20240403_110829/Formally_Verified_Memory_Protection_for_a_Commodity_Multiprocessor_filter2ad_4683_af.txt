To provide a measure of multi-VM performance, we
also measured the performance of SeKVM compared to
HypSec and unmodiﬁed KVM running multiple VMs, each
running Hackbench. We tested ﬁve hypervisor conﬁgurations:
KVM and SeKVM on Linux v4.18 and v5.4, and HypSec
on only v4.18. To scale the experiment on the same Seattle
Arm server host, we made some changes to the VM and
Hackbench conﬁgurations. Each VM was conﬁgured in
a similar manner as our previous experiments, except we
reduced the number of cores and RAM of each VM to two
cores and 256 MB of RAM, respectively. We changed the
parameters in Hackbench from the previous setup to run 20
process groups in 500 loops, so that it could run successfully
in the more resource-constrained VMs. In addition, we did
not use FDE given the limited memory assigned to each VM.
We measured the performance of 1, 2, 4, 8, 16, and 32 VMs.
Figure 7 shows the average results from each VM running
on HypSec and SeKVM normalized to native execution
of one instance of Hackbench using the respective Linux
version with the same conﬁguration, though there was
minimal difference in native execution performance between
kernel versions. The results show that SeKVM incurs modest
performance overhead over KVM and HypSec, even as the
number of VMs scales. The overhead versus one instance
of Hackbench natively executed is of course higher when
running many instances of Hackbench instead of just one, but
the relative overhead of SeKVM versus KVM remains small.
Note that although KCore’s data race-free implementation
does not take full advantage of Armv8 relaxed memory
behavior, the performance impact on SeKVM is minimal.
Figure 6: Application Benchmark Performance - Linux v5.4
each world switch to remove all cached entries used by EL0
and EL1, to measure the performance impact of a veriﬁed
implementation that models tagged TLB behavior versus one
that does not (and must therefore perform additional TLB
ﬂushes for veriﬁed correctness).
Figures 5 and 6 show the relative overhead of executing
in a VM in our v4.18 and v5.4 hypervisor conﬁgurations. We
normalize the performance results to native execution on the
respective unmodiﬁed Linux kernel, with 1.0 indicating the
same performance as native hardware. Lower numbers mean
less overhead. We report results for Apache and MySQL
both with and without TLS/SSL to show performance
with network encryption as well. Both ﬁgures show that
SeKVM has only modest performance overhead compared
to unmodiﬁed KVM. Figure 5 also shows that SeKVM
has comparable performance to HypSec, but the HypSec
implementation was not available for v5.4, so Figure 6 shows
no HypSec v5.4 measurements. Overall, the measurements
show that a commodity hypervisor with a veriﬁed TCB on
multiprocessor hardware can achieve excellent performance.
As shown in Figures 5 and 6, ﬂushing the TLB during each
world switch results in signiﬁcant performance overhead.
The overhead is especially pronounced in I/O intensive
workloads, where frequent world switches between VMs and
3966    30th USENIX Security Symposium
USENIX Association
0.00.20.40.60.81.01.21.41.61.82.02.22.4KernbenchHackbenchTCP_STREAMTCP_MAERTSTCP_RRApache EncryptApacheMemcachedMySQL EncryptMySQLKVMHypSecSeKVMSeKVM-TLB-FLUSH0.00.20.40.60.81.01.21.41.61.82.02.22.42.6KernbenchHackbenchTCP_STREAMTCP_MAERTSTCP_RRApache EncryptApacheMemcachedMySQL EncryptMySQLKVMHypSecSeKVMSeKVM-TLB-FLUSH0.01.02.03.04.05.06.07.08.09.010.011.012.013.012481632KVM-4.18HypSecSeKVM-4.18KVM-5.4SeKVM-5.47 Related Work
Previous work has veriﬁed uniprocessor systems, including
seL4 [43], Nickel [59], Serval [54], and Komodo [28].
None of these approaches can be directly applied to verify
multiprocessor systems such as SeKVM. CertiKOS has
veriﬁed a series of uniprocessor and multiprocessor OS ker-
nels [9, 10, 13, 31–34], but like previous veriﬁed uniprocessor
systems, did not model common hardware features including
shared page tables, tagged TLBs or caches. In contrast,
SeKVM is veriﬁed on a multiprocessor abstract machine that
models these widely-used hardware features.
Various veriﬁed systems can be used as hypervisors, but
are limited in their functionality and what has been veriﬁed.
A version of seL4 veriﬁes the functional correctness of some
hypervisor features, but not the MMU functionality [2, 42].
CertiKOS veriﬁes the correctness of the mC2 kernel that
provides some virtualization functionality. Both of these
systems lack common hypervisor features such as support for
multiprocessor VMs. The üXMHF hypervisor [65,66] veriﬁes
simple properties, such as memory integrity of their multipro-
cessor microhypervisor implementation, but does not verify
its functional correctness. Unlike SeKVM, the proofs were
reasoned on a simple abstract hardware that does not model
concrete MMU features. The Verisoft team [44] applies the
VCC framework [12] to verify Hyper-V. VCC does not in-
clude a realistic hardware model. Only 20% of the hypervisor
code is veriﬁed for function contracts and type invariants at
the source code level, with no correctness guarantees of the
overall hypervisor’s behavior. In contrast, SeKVM’s security
guarantees and its TCB are fully veriﬁed while supporting
commodity hypervisor features inherited from KVM.
We build on our previous work [47, 48] that introduced
security-preserving layers and microveriﬁcation to verify the
security guarantees of a KVM hypervisor. We describe here
for the ﬁrst time (1) a new layered hardware model, (2) the
construction of a layered implementation of SeKVM’s TCB,
KCore, (3) how the layered hardware can be used in conjunc-
tion with the layered software to verify KCore’s functional
correctness in the presence of widely-used multiprocessor
hardware features such as tagged TLBs and coherent caches,
and (4) how to account for all of these hardware features
in verifying the security guarantees of SeKVM. We also
demonstrate for the ﬁrst time how both the implementation
and veriﬁcation of SeKVM can be extended to integrate with
multiple versions of Linux as a host kernel with modest effort.
Formal shim veriﬁcation [39] reduces the proof effort in
verifying security guarantees about a large and untrusted code.
Their techniques focus on proving that a small, sequential
browser kernel, consisting of a few hundred LOC, enforces
noninterference properties between components running in
sandboxes. This approach is insufﬁcient for SeKVM, whose
multiprocessor core consists of a few thousand LOC, and
leverages hardware virtualization features to implement
hypervisor functionality.
Some work [57, 64, 73] has veriﬁed the MMU subsystem
within an OS kernel. Unlike SeKVM, the veriﬁed component
does not make any guarantees about the overall behavior of
the system. Other work [62, 63] integrates the speciﬁcations
of their abstract TLB into the Cambridge Arm model [29],
but only uses it for proving the program logic of the system’s
execution, not the correctness of the actual implementation.
Microhypervisors [35, 60] take a microkernel approach
to build clean-slate small hypervisors from scratch. These
architectures mitigate vulnerabilities, but are not veriﬁed to be
correct. In contrast, SeKVM retroﬁts KVM using microkernel
principles to reduce its TCB and veriﬁes its implementation,
providing veriﬁed correctness and security guarantees with
full-featured commodity hypervisor functionality. Nested
virtualization [70] and special hardware features [7, 37, 68]
have been used to protect VM data in memory against an
untrusted hypervisor. Privileged code, such as a hypervi-
sor, has been used to protect OS kernels [26, 58, 67] or
applications [11, 27, 36, 52, 69] against untrusted software
components. Unlike SeKVM, none of these systems verify
their TCBs or prove the security properties of their designs.
8 Conclusions
We have presented SeKVM, the ﬁrst Linux KVM hypervisor
that has been formally veriﬁed. This is made possible using a
layered design and veriﬁcation methodology. We use layers to
isolate KVM’s TCB into a small core, then construct the core
with layers such that we can modularize the proofs to reduce
proof effort, modeling hardware features at different levels
of abstraction tailored to each layer of software. We can then
gradually reﬁne detailed hardware and software behaviors
at lower layers into simpler abstract speciﬁcations at higher
layers, which can in turn be used to prove security guarantees
for the entire hypervisor. Using this approach, we prove the
correctness of KVM across two versions of Linux, using a
novel layered machine model that accounts for realistic mul-
tiprocessor features including multi-level shared page tables,
tagged TLBs, and a coherent cache hierarchy with cache by-
pass support. The layering requires only modest modiﬁcations
to KVM and only incurs modest overhead versus unmodiﬁed
KVM on real application workloads. Our work is the ﬁrst
machine-checked proof of the correctness and security of a
commodity hypervisor on multiprocessor server hardware.
9 Acknowledgments
Xuheng Li helped with assembly code and layer reﬁnement
proofs. Nathan Dautenhahn provided helpful comments on
earlier drafts. This work was supported in part by a Guggen-
heim Fellowship, DARPA contract N6600121C4018, and
NSF grants CCF-1918400, CNS-2052947, and CCF-2124080.
USENIX Association
30th USENIX Security Symposium    3967
References
[1] ab - Apache HTTP server benchmarking tool.
https:
//httpd.apache.org/docs/2.4/programs/ab.html
[Accessed: Mar 8, 2021].
[2] seL4 Supported Platforms. https://docs.sel4.systems/
Hardware [Accessed: Mar 8, 2021].
[3] The Coq Proof Assistant. https://coq.inria.fr [Ac-
cessed: Dec 16, 2020].
[4] ARM System Memory Management Unit Architecture Speci-
ﬁcation - SMMU architecture version 2.0, June 2016.
[5] ARM Ltd. ARM CoreLink MMU-401 System Memory Man-
agement Unit Technical Reference Manual, July 2014.
[6] Michael Backes, Goran Doychev, and Boris Kopf. Preventing
Side-Channel Leaks in Web Trafﬁc: A Formal Approach.
In 20th Annual Network and Distributed System Security
Symposium (NDSS 2013), San Diego, CA, February 2013.
[7] Andrew Baumann, Marcus Peinado, and Galen Hunt. Shield-
ing Applications from an Untrusted Cloud with Haven. In
Proceedings of the 11th USENIX Symposium on Operating
Systems Design and Implementation (OSDI 2014), pages
267–283, Broomﬁeld, CO, October 2014.
[8] Edouard Bugnion, Jason Nieh, and Dan Tsafrir. Hardware
and Software Support for Virtualization. Synthesis Lectures
on Computer Architecture. Morgan and Claypool Publishers,
February 2017.
[9] Hao Chen, Xiongnan Wu, Zhong Shao, Joshua Lockerman,
and Ronghui Gu. Toward compositional veriﬁcation of
interruptible os kernels and device drivers. In Proceedings
of the 37th ACM SIGPLAN Conference on Programming
Language Design and Implementation, pages 431–447, 2016.
[10] Hao Chen, Xiongnan Wu, Zhong Shao, Joshua Lockerman,
and Ronghui Gu. Toward compositional veriﬁcation of
Journal of
interruptible os kernels and device drivers.
Automated Reasoning, 61(1):141–189, 2018.
[11] Xiaoxin Chen, Tal Garﬁnkel, E. Christopher Lewis, Pratap
Subrahmanyam, Carl A. Waldspurger, Dan Boneh, Jeffrey
Dwoskin, and Dan R.K. Ports. Overshadow: A Virtualization-
based Approach to Retroﬁtting Protection in Commodity
Operating Systems. In Proceedings of the 13th International
Conference on Architectural Support
for Programming
Languages and Operating Systems (ASPLOS 2008), pages
2–13, Seattle, WA, March 2008.
[12] Ernie Cohen, Markus Dahlweid, Mark Hillebrand, Dirk
Leinenbach, Michał Moskal, Thomas Santen, Wolfram
Schulte, and Stephan Tobies. VCC: A Practical System for
Verifying Concurrent C. In Proceedings of the 22nd Inter-
national Conference on Theorem Proving in Higher Order
Logics (TPHOLs 2009), pages 23–42, Munich, Germany,
August 2009.
[13] David Costanzo, Zhong Shao, and Ronghui Gu. End-to-End
Veriﬁcation of Information-Flow Security for C and Assembly
Programs. In Proceedings of the 37th ACM Conference on
Programming Language Design and Implementation (PLDI
2016), pages 648–664, Santa Barbara, CA, June 2016.
[14] CVE. CVE-2009-3234. https://cve.mitre.org/cgi-
September
bin/cvename.cgi?name=CVE-2009-3234,
2009.
[15] CVE. CVE-2010-4258. https://cve.mitre.org/cgi-
November
bin/cvename.cgi?name=CVE-2010-4258,
2010.
[16] CVE. CVE-2013-1943. https://cve.mitre.org/cgi-
February
bin/cvename.cgi?name=CVE-2013-1943,
2013.
[17] CVE. CVE-2016-9756. https://cve.mitre.org/cgi-
December
bin/cvename.cgi?name=CVE-2016-9756,
2016.
[18] CVE. CVE-2017-17741. https://cve.mitre.org/cgi-
bin/cvename.cgi?name=CVE-2017-17741, December
2017.
[19] Christoffer Dall. The Design, Implementation, and Evaluation
PhD thesis, Columbia
of the Linux ARM Hypervisor.
University, February 2018.
[20] Christoffer Dall, Shih-Wei Li, Jin Tack Lim, and Jason
Nieh. ARM Virtualization: Performance and Architectural
Implications. ACM SIGOPS Operating Systems Review,
52(1):45–56, July 2018.
[21] Christoffer Dall, Shih-Wei Li, Jin Tack Lim, Jason Nieh, and
Georgios Koloventzos. ARM Virtualization: Performance
and Architectural Implications. In Proceedings of the 43rd
International Symposium on Computer Architecture (ISCA
2016), pages 304–316, Seoul, South Korea, June 2016.
[22] Christoffer Dall, Shih-Wei Li, and Jason Nieh. Optimizing the
Design and Implementation of the Linux ARM Hypervisor.
In Proceedings of the 2017 USENIX Annual Technical
Conference (USENIX ATC 2017), pages 221–234, Santa Clara,
CA, July 2017.
[23] Christoffer Dall and Jason Nieh. KVM/ARM: Experiences
Building the Linux ARM Hypervisor. Technical Report
CUCS-010-13, Department of Computer Science, Columbia
University, June 2013.
[24] Christoffer Dall and Jason Nieh. Supporting KVM on the
ARM Architecture. LWN Weekly Edition, pages 18–22, July
2013.
[25] Christoffer Dall and Jason Nieh. KVM/ARM: The Design and
Implementation of the Linux ARM Hypervisor. In Proceed-
ings of the 19th International Conference on Architectural Sup-
port for Programming Languages and Operating Systems (AS-
PLOS 2014), pages 333–347, Salt Lake City, UT, March 2014.
[26] Nathan Dautenhahn, Theodoros Kasampalis, Will Dietz, John
Criswell, and Vikram Adve. Nested Kernel: An Operating
System Architecture for Intra-Kernel Privilege Separation.