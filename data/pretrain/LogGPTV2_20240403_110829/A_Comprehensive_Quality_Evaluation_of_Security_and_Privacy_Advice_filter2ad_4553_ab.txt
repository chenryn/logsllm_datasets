### Four Sub-Metrics for Evaluating Security Advice

The evaluation of security advice was conducted using four sub-metrics, each assessed on a 4-point Likert scale ranging from "Not at All" to "Very":

1. **Confidence**: The user's confidence in their ability to implement the advice.
2. **Time Consumption**: The perceived time required to implement the advice.
3. **Disruption**: The level of disruption the user anticipates in implementing the advice.
4. **Difficulty**: The perceived difficulty of implementing the advice.

Each piece of advice was evaluated by three respondents, and each respondent evaluated five randomly selected pieces of advice. The full questionnaire, including an example to help respondents distinguish among the different sub-metrics, is provided in Appendix C.

### Theoretical Foundations

These sub-metrics are grounded in theoretical frameworks relevant to security behavior:

- **Confidence**: This sub-metric is derived from Protection Motivation Theory [52], which highlights the importance of perceived self-efficacy in protective behaviors, and the Human in the Loop model [12], which emphasizes knowledge acquisition as a key component of security behavior change.
- **Time Consumption and Disruption**: These sub-metrics align with the "cost" aspect of behavior, which is a significant factor in economic models of secure behavior [3, 23, 47, 48].
- **Difficulty**: This sub-metric corresponds to the capabilities component of the Human in the Loop model [12].

### Perceived Efficacy

To measure the perceived efficacy of the security advice, we solicited evaluations from professional security experts. Each piece of advice was evaluated by three experts, who were asked whether they believed a typical end user following the advice would experience an improvement, no effect, or harm to their security. The expert recruitment process and the screening criteria are detailed below.

### Human Subjects Recruitment

#### General Population (Actionability and Comprehensibility)

- **Recruitment Method**: We recruited 1,586 users from Cint’s survey panel in June 2019. The panel allows for quota sampling to ensure that the demographics of the respondents are representative of the U.S. population within 5% on age, gender, race, education, and income.
- **Compensation**: Participants were compensated according to their agreement with Cint.

#### Professional Security Experts (Efficacy)

- **Recruitment Period**: May and June 2019.
- **Methods**: 
  - Twitter outreach through the lead author’s account and retweets from well-known security accounts.
  - Personal networks.
  - Posts in professional LinkedIn groups.
  - Contacting authors of security blogs.
- **Screening Questionnaire**: To assess security credentials, including certifications, participation in Capture the Flag (CTF) events, security-related programming, penetration testing, and current job titles. Resumes or links to personal websites were also reviewed.
- **Qualification Criteria**: Individuals with two or more of the following: CTF participation, penetration testing, security-related programming, or holding security certifications (including computer security professors).
- **Expert Demographics**: 41 qualified experts, mostly practitioners, with diverse workplace contexts such as corporate and government information security professionals, red team/pen testers, independent consultants, and privacy-focused professionals.
- **Compensation**: $1 per piece of advice evaluated, with batches of 10 pieces. On average, experts evaluated 38 pieces of advice each.

### Measurement Validity

We assessed the validity of our measurements in two ways:

1. **Reliability**: Using the Intraclass Correlation Coefficient (ICC) metric (see Section 3), we found:
   - **Actionability**: ICC = 0.896, 0.854, 0.868, and 0.868 for confidence, time consumption, disruption, and difficulty, respectively.
   - **Efficacy**: ICC = 0.876.
   - **Comprehensibility**: Cloze raters had an ICC of 0.989, and ease raters had an ICC of 0.757.
2. **Discriminant Validity and Correlation with Behavior Adoption**: Detailed results are provided in Section 9.

### Limitations

- **Representativeness of Security Advice Corpus**: Multiple techniques were used to ensure broad coverage.
- **Manual Annotation Accuracy**: Double annotation and full reviews were conducted to mitigate inaccuracies.
- **Relevant Expertise**: While we screened for expertise, demographic data was not collected.
- **Volume of Advice**: Experts and users evaluated advice in the abstract, and a X2 proportion test showed no significant difference in priority ratings between experts who rated fewer than 30 pieces and those who rated more.
- **Metrics Instantiation**: Established, validated tools and robust pre-testing were used to mitigate limitations.

### Security Advice Examples

| Advice | Confidence | Time Consumption | Disruption | Difficulty | Efficacy |
|--------|------------|------------------|------------|------------|----------|
| Apply the highest level of security that’s practical | ✗ | ✗ | ✗ | ✗ | All Accurate |
| Be wary of emails from trusted institutions | ✗ | ✗ | ✗ | ✗ | All Accurate |
| Beware of free VPN programs | ✗ | ✗ | ✗ | ✗ | Majority Accurate |
| ... | ... | ... | ... | ... | ... |

This table provides a summary of the evaluations for a subset of the security advice. For a complete list, refer to the full dataset.