:
:
:
:
:
:
:
Identified as Propfind
Identified as Options
Identified as Propfind
Identified as Options
Identified as Propfind
Identified as Options
CLUSTER 12:
3 Unique client IPs,
CLUSTER 10:
2 Unique client IPs,
Identified as FrontPage Exploit
CLUSTER 16:
2 Unique client IPs,
Identified as Kazaa
CLUSTER 13:
1 Unique client IPs,
Identified as Web Crawler
CLUSTER 14:
1 Unique client IPs,
Identified as Real Media Player
CLUSTER 15:
1 Unique client IPs,
Identified as Propfind
Identified as Options
CLUSTER 18:
1 Unique client IPs,
Identified as Open Proxy
Figure 6: HTTP Port 80 cluster report.
In the formulas above, Pk∈J |ck| ≥ |c| and
Pk∈J |Ck| ≥ |C| as sessions may have multiple conjec-
tures. Figure 7 presents graphs indicating how precision
and recall vary with the clustering similarity threshold.
Recall that in the star clustering algorithm, an edge is
added between two sessions in the graph of all sessions
only if their similarity is above the threshold. Although
less true for NetBIOS data, the similarity threshold does
not have a signiﬁcant impact on the quality of the result-
ing clustering. Clustering precision drops as the thresh-
old nears 0 because the star graph becomes nearly fully
connected and the algorithm cannot select suitable clus-
ter centers. Recall that no cluster centers can share an
edge, so many different clusters merge together at low
threshold values. At the clustering threshold used in
our experiments (0.8), precision scores were perfect or
nearly perfect.
7.2 Signature Effectiveness
Intrusion detection signatures should satisfy two basic
properties. First, they should have a high detection rate;
i.e., they should not miss real attacks. Second, they
should generate few false alarms. Our results will show
that Nemean has a 99.9% detection rate with 0 false
alarms. Two additional metrics evaluate the quality of
the alarms raised by an IDS. Precision empirically evalu-
ates alarms by their speciﬁcity to the attack producing the
alarm. Noise level counts the number of alarms per inci-
dent and penalizes redundant alarms. In these tests, we
use Snort as a baseline for comparison simply because
that is the most widely adopted intrusion detection sys-
tem. We used the latest version of Snort available at the
time, Snort-2.1.0 with the HTTP pre-processor enabled,
and its complete associated ruleset. In some sense, Snort
is the strawman because of its well-known susceptibility
to false-positives. We use this because of our inability to
compare with Honeycomb (see Section 7.4) and because
there is no source code publicly available for Earlybird
or Autograph [10, 27].
• 99.9% Detection Rate: We evaluated the detection
rate of Nemean signatures using leave-out testing, a com-
mon technique in machine learning. We used the hon-
eynet data set described in Table 2 to automatically cre-
ate connection-level and session-level signatures for the
clusters identiﬁed in a training data set. We measured
the detection rate of the signatures by running signature
matching against data in a different trace collected from
the same network (see Table 2).
Connection-level HTTP signatures detected 100.0%
of the attacks present, and the somewhat more restric-
tive session-level signatures detected 97.7%. We did not
evaluate session-level signatures for Nimda because the
extreme variability of Nimda attacks made such signa-
tures inappropriate. Table 3 shows the number of occur-
rences of the HTTP attacks and the number detected by
Nemean signatures. For comparison, we provide detec-
tion counts for Snort running with an up-to-date signa-
ture set. Snort detected 99.7% of the attacks.
The detection rate of NetBIOS attacks is similarly very
high: we detected 100.0% of the attacks present. Ta-
ble 4 contains the detection rates for NetBIOS/SMB sig-
natures. Snort provides only limited detection capabil-
ity for NetBIOS attacks, so a comparison was infeasible.
All signatures were connection-level because the deﬁn-
ing characteristic of each attack is a string contained in a
single connection. The structure of connections within a
session is irrelevant for such attacks.
• Zero misdiagnoses or false alarms: We qualify in-
correct alerts on the honeynet data as misdiagnoses. Al-
though not shown in Table 3, all Nemean HTTP signa-
tures generated 0 misdiagnoses on the honeynet trace.
Misdiagnosis counts for NetBIOS/SMB on the honeynet
data were also 0, as shown in Table 4. We also measured
false alarm counts of Nemean HTTP signatures against
16GB of packet-level traces collected from our depart-
ment’s border router over an 8 hour time period. The
traces contained both inbound and outbound HTTP traf-
ﬁc. We evaluated both Nemean and Snort against the
dataset.
USENIX Association
14th USENIX Security Symposium
107
0.9
e
u
l
a
V
0.8
0.7
0.6
0.0
0.9
e
u
l
a
V
0.8
0.7
0.6
0.0
Port 80
Precision
Recall
0.2
0.4
0.6
0.8
1.0
Similarity Threshold
0.9
e
u
l
a
V
0.8
0.7
0.6
0.0
Port 139
Precision
Recall
0.2
0.4
0.6
0.8
1.0
Similarity Threshold
Port 445
Precision
Recall
0.2
0.4
0.6
0.8
1.0
Similarity Threshold
Figure 7: Effect of clustering similarity threshold upon weighted precision and weighted recall. Note that the y-axis
begins at 0.6.
Signature
Options
Nimda
Propﬁnd
Welchia
Win Media Player
Code Red Retina
Kazaa
Present
1172
496
229
90
89
4
2
Nemean
Conn
1172
496
229
90
89
4
2
Sess
1160
N/A
205
90
89
4
2
Snort
1171
495
229
90
89
0
2
Table 3: Session-level HTTP signature detection counts
for Nemean signatures and Snort Signatures. We show
only exploits occurring at least once in the training and
test data.
Signature
Srvsvc
Samr
Epmapper
NvcplDmn
Deloder
LoveGate
Present
19934
8743
1263
62
30
1
Detected Misdiagnoses
0
0
0
0
0
0
19930
8741
1258
61
30
0
Table 4: Detection and misdiagnosis counts for
connection-level Nemean NetBIOS signatures. This data
includes both port 139 and port 445 trafﬁc.
Nemean results are highly encouraging:
0 false
alarms. Snort generated 88,000 alarms on this dataset,
almost all of which were false alarms. The Snort false
alarms were produced by a collection of overly general
signatures. In fairness, we note that Snort had a larger
signature set which made it more prone to false posi-
tives. Our Snort signature set included about 2200 sig-
natures, whereas Nemean’s database of HTTP and Net-
BIOS signatures contained only 22 connection-level and
7 session-level signatures. Snort has a high signature
count because it is meant to detect classes of attacks be-
Signature
Non-RFC HTTP Delimiter
Bare Byte Unicode Encoding
Apache Whitespace (TAB)
WEB-MISC /doc/ Access
Non-RFC Deﬁned Character
Double-Decoding Attack
IIS Unicode Codepoint Encoding
No. Alerts
32246
28012
9950
9121
857
365
351
Table 5: Snort false alarm summary for over 45,000
HTTP sessions collected from our department’s border
router.
Alert Category
WEB-MISC
WEB-CGI
WEB-IIS
WEB-ATTACKS
WEB-PHP
WEB-FRONTPAGE
Others (P2P, Crawlers)
No. Signatures
13
25
8
6
4
4
5
No. Alerts
466
919
164
15
18
61
5426
Table 6: Summary of remaining Snort alerts.
yond those seen by Honeynets.
Table 5 provides a summary of the Snort alarms gener-
ated on an 8 hour trace of overwhelmingly benign HTTP
trafﬁc collected at our department’s border router. Re-
ducing Snort’s alarm rate would require reengineering of
many signatures. Additionally, the overly general signa-
ture provides little speciﬁc information about the type of
exploit that may be occurring.
We assume that in a real network deployment of Snort
the most noisy signatures such as those in Table 5 would
be disabled. A more reasonable estimate of the expected
false alarm rates might be obtained from the remaining
alerts shown in Table 6. The remaining alerts come from
60 signatures and are responsible for 1643 alerts (exclud-
108
14th USENIX Security Symposium
USENIX Association
ing Others). While we did not inspect each of these in-
dividually for true positives due to privacy concerns with
the dataset, sampling revealed that they are mostly false
alarms. Trafﬁc classiﬁed as Others were legitimate alerts
ﬁred on benign P2P trafﬁc and trafﬁc from web crawlers.
Our university ﬁlters NetBIOS trafﬁc at the campus
border, so we were unable to obtain NetBIOS data for
this experiment.
• Highly speciﬁc alarms: Although the decision is
ultimately subjective, we believe our signatures generate
alerts that are empirically better than alerts produced by
packet-level systems such as Snort. Typical Snort alerts,
such as “Bare Byte Unicode Encoding” and “Non-RFC
HTTP Delimiter”, are not highly revealing. They report
the underlying symptom that triggered an alert but not
the high-level reason that the symptom was present. This
is particularly a problem for NetBIOS alerts because all
popular worms and viruses generate virtually the same
set of alerts. We call these weak alerts and describe them
in more detail in the technical report [35]. Nemean, via
connection-level or session-level signatures, has a larger
perspective of a host’s intentions. As a result, we gener-
ate alerts speciﬁc to particular worms or known exploits.
• Low noise due to session-level signatures: More-
over, Nemean provides better control over the level of
noise in its alarms. Packet-level detection systems such
as Snort often raise alerts for each of multiple packets
comprising an attack. A security administrator will see
a ﬂurry of alerts all corresponding to the same incident.
For example, a Nimda attack containing an encoded URL
will generate URL decoding alarms in Snort and alerts
for WEB-IIS cmd.exe access. Sophisticated URL de-
coding attacks could later get misdiagnosed as Nimda
alerts and be ﬁltered by administrators. Our normalizer
converts the URL to a canonical form to accurately detect
Nimda attacks. Since Nemean aggregates information
into connections or sessions and generates alerts only on
the aggregated data, the number of alerts per incident is
reduced.
In summation, we believe these results demonstrate
the strength of Nemean. It achieves detection rates sim-
ilar to Snort with dramatically fewer false alarms. The
alerts produced by Nemean exhibit high quality, specify-
ing the particular attack detected and keeping detection
noise small.
7.3 Signature Generation Efﬁciency
Although our current implementation operates ofﬂine on
collected data sets, we intend for Nemean to be used in
online signature generation. Online systems must be ef-
ﬁcient, both so that new signatures can be rapidly con-
structed as new attacks begin to appear and so that the
system can operate at network speeds with low compu-
tational demands. Figure 8 shows Nemean’s overheads
on the 2-day training data set containing about 200,000
HTTP packets and 2,000,000 NetBIOS packets. To-
tal data processing time is divided into the three stages
of data abstraction, clustering, and automaton general-
ization plus an additional preprocessing step that trans-
lated SSTs produced by the DAL into the input for-
mat of our clustering module. The HTTP connection-
level automaton generalization step used the sk-strings
algorithm. The session-level generalization used beam
search, with nearly 100% of the cost arising from one
cluster of Nimda sessions. At 200,000 packets, the cost
of session-level generalization was 587 seconds. Net-
BIOS signature generalization used simulated beam an-
nealing at the connection-level only, with no construction
of session-level signatures.
Nemean is efﬁcient. We are able to generate signa-
tures for 2 days worth of NetBIOS data, totaling almost 2
million packets, in under 70 seconds. Even our most ex-
pensive operation, session-level generalization of HTTP
data, required less than 10 minutes of computation. The
design of our system helps keep costs low. By processing
only data collected on a honeynet, the overall volume of
data is signiﬁcantly reduced. Deploying Nemean as an
online signature generator would require limited system
resources and can easily operate at the speeds of incom-
ing data.
7.4 Honeycomb Comparison
Honeycomb was one of the ﬁrst efforts to address the
problem of automatic signature generation from honey-
pot traces. We performed a comparison between Nemean