# TARDIS: Rolling Back The Clock on CMS-Targeting Cyber Attacks

## Authors
Ranjita Pai Kasturi, Yiting Sun, Ruian Duan, Omar Alrawi, Ehsan Asdar, Victor Zhu, Yonghwi Kwon, Brendan Saltaformaggio

## Affiliations
1. Georgia Institute of Technology
2. University of Virginia

## Abstract
Over 55% of the world’s websites run on Content Management Systems (CMS). Unfortunately, this large user base has made CMS-based websites a prime target for hackers. Moreover, the majority of the website hosting industry has adopted a "backup and restore" model of security, which relies on error-prone antivirus (AV) scanners to prompt users to roll back to a pre-infection nightly snapshot. This research analyzed over 300,000 unique production websites' nightly backups.

Our study revealed that the evolution of thousands of attacks exhibited clear, long-lived, multi-stage attack patterns. We propose TARDIS, an automated provenance inference technique that enables the investigation and remediation of CMS-targeting attacks using only the nightly backups already collected by website hosting companies. In collaboration with our industry partner, we applied TARDIS to the nightly backups of 300,000 websites and identified 20,591 attacks lasting from 6 to 1,694 days, some of which were still undetected.

## I. Introduction
Over 55% of the world’s websites run on Content Management Systems (CMS), with WordPress controlling nearly 60% of the CMS market. This widespread adoption has led to a significant increase in CMS-targeting cyber attacks. These attacks are facilitated by the complex nature of CMS deployments, which involve multiple layers of software and interpreters with varying degrees of network and system permissions, all running on internet-facing web servers.

This research has uncovered a concerning trend: in-the-wild CMS compromises exhibit characteristics indicative of multi-stage, low-and-slow attacks. Despite the extensive deployment of these complex systems, little research has been conducted to investigate and remediate CMS-targeting cyber attacks. Traditionally, the research community has relied on fine-grained logging to understand the provenance of an attack. However, in the CMS domain, these techniques are rarely deployed due to performance and space overheads, and often require instrumentation and training with the target systems.

Website owners often have no control over the underlying web server, as the entire platform is owned and maintained by a hosting provider or IT department. For these reasons, the industry standard has shifted to a "backup and restore" model of security, offered by popular platforms such as CodeGuard, GoDaddy, and iPage. Antivirus (AV) scanners are used to detect compromises, and nightly backups of the website's files are maintained offsite. However, these approaches have limitations: AV signatures only catch well-known malware, fail to detect stealthy multi-stage attacks, and high false alarm rates cause real alerts to be ignored. Additionally, website owners often revert to the most recent snapshot that did not trigger an AV alert, rather than rolling back to a pre-infection state.

This research had the unique opportunity to study these attack trends in nightly backups from over 300,000 production websites. In collaboration with CodeGuard, we initially set out to develop a website protection methodology that could replace the ineffective backup and restore standard. Our preliminary investigation of 70 recently targeted websites revealed that each attack exhibited clear multi-stage attack patterns. Based on this discovery, we developed TARDIS, a novel provenance inference technique that enables the investigation of multi-stage CMS-targeting attacks using only the nightly backups.

Through our collaboration with CodeGuard, we used TARDIS to perform a systematic study of the attack landscape across 306,830 CMS-based production websites. Our study uncovered 20,591 websites (6.7%) that were compromised with advanced multi-phase attacks. These attacks persisted for a minimum of 6 days and a maximum of 1,694 days, with a median of 40 to 100 days. More than 20% of WordPress websites housed attacks for over a year, likely due to WordPress's significant market share. These attacks involved stealthily dropping a large volume of malicious code, affecting the web server. During an attack, the number of files increased by at least 50%, ranging from visitor-attacking browser exploits to full-fledged HTML-based remote control GUIs.

## II. Preliminary Investigation
Our investigation began with 70 websites known to have been recently compromised. We asked the key cyber forensics question: How would an investigator recover the website from these attacks? Unfortunately, CMS website owners generally lack the expertise and control over the hosting server required to enable robust forensic logging. Given only the nightly backups, an investigator's visibility is significantly limited.

### Inferring Provenance Patterns
We observed that a finite number of identical provenance patterns exist within the evolution of all the websites. A file in a given snapshot can exist in one of three states: added, modified, or deleted. Figure 1 illustrates the three infection scenarios we observed in the website backups. A file added (A) can be flagged as suspicious (denoted by !) by an AV at some point throughout its life cycle. These files could also be flagged as suspicious after they are modified (M). In some cases, a snapshot rollback is performed to treat the suspicious files by deleting (D) them. If the rollback deletes all of the attacker's files, the attack is cured (Figure 1(a)). In other cases, no action is taken despite detecting a suspicious file (Figure 1(c)).

Unfortunately, this led to the discovery that the industry standard of "backup and restore" is insufficient. We found that 80% of these websites were still infected—many website owners had rolled back to a snapshot and patched the vulnerability but, lacking forensic expertise, were unable to identify and remove initial backdoors, allowing the attack to recur.

To quickly rollback to a clean snapshot, investigators must recover the compromise window, or the period during which the snapshots should not be trusted. This is further complicated by the fact that each snapshot contains tens of thousands of files (11,292 on average), making this investigation a search for needles in a haystack.

### Single Snapshot Metrics
When looking into the individual snapshots from a single Drupal website, W6828862, we observed that the complexity of each snapshot can be reduced to a set of measurements, called spatial metrics, that highlight the existence of cyber attack evidence. In addition to the state of each file in the snapshot, we designed spatial metrics to measure extension mismatches and identify UTF-8 based code obfuscation patterns in server-side script files. For example, in the case of W682886, we found 3 PHP files with obfuscated payloads disguised as icon files in the 5 June 2018 snapshot that initiated the attack. We settled on nine spatial metrics detailed in Section III, which were effective at highlighting the presence of cyber attack artifacts within a single snapshot.

### Temporal Evolution of Attack Phases
We collected spatial metrics to represent each snapshot of W682886, paying specific attention to sudden changes between pairs of consecutive snapshots. This revealed that modeling the implicit events triggering these sudden changes can expose the attack phases. We plotted the temporal progression of the spatial metrics across all of W682886's snapshots, revealing the evolution of the attack.

## III. Design
[Table I: Temporal File Differential Analysis]
[Insert Table I here]

The table above shows the temporal file differential analysis for the website W682886. The data highlights the changes in file types and outliers over time, providing insights into the attack's progression.

[End of Optimized Text]