better trade-off.
3 PROBLEM FORMULATION
Given a target GNN model 𝑓 and a target graph 𝐺 with label 𝑦0
(Please refer to Appendix A for more background on GNNs for
graph classification, due to space limitation), the attacker attempts
to generate an untargeted adversarial graph 𝐺′ by perturbing the
adjacency matrix 𝐴 of 𝐺 to be 𝐴′, such that the predicted label of
𝐺′ will be different from 𝑦0. Let the adversarial perturbation be a
binary matrix Θ ∈ {0, 1}𝑁×𝑁 . For ease of description, we fix the
entries in the lower triangular part of Θ to be 0, i.e., Θ𝑖 𝑗 = 0 ∀𝑗 ≤ 𝑖,
and each entry in the upper triangular part indicates whether the
corresponding edge is perturbed or not. Specifically, Θ𝑖 𝑗 = 1, 𝑗 > 𝑖
means the attacker changes the edge status between nodes 𝑖 and 𝑗,
i.e., adding the new edge (𝑖, 𝑗) if there is no edge between them in
the original graph 𝐺 or deleting the existing edge (𝑖, 𝑗) from 𝐺. We
keep the edge status between 𝑖 and 𝑗 unchanged if Θ𝑖 𝑗 = 0, 𝑗 > 𝑖.
Then the perturbed graph 𝐴′ can be generated by a perturbation
function ℎ, i.e., 𝐴′ = ℎ (𝐴, Θ), and ℎ is defined as follows:
(cid:40)𝐴𝑖 𝑗
ℎ (𝐴, Θ)𝑖 𝑗 = ℎ (𝐴, Θ) 𝑗𝑖 =
Θ𝑖 𝑗 = 0, 𝑗 > 𝑖,
¬𝐴𝑖 𝑗 Θ𝑖 𝑗 = 1, 𝑗 > 𝑖.
(1)
Moreover, the attacker ensures that the perturbation rate 𝑟 will
not exceed a given budget 𝑏. Formally, we formulate generating
adversarial structural perturbations to a target graph (or called
tion budget 𝑏
Algorithm 1 Generating an adversarial graph for a target graph
with a hard label black-box access
Input: A trained target GNN model 𝑓 , a target graph 𝐴, perturba-
Output: Adversarial graph 𝐴′
1: Search initial vector Θ0 via coarse-grained searching;
2: for 𝑡 = 1, 2, . . . ,𝑇 do
3:
4:
Randomly sample 𝑢1, . . . , 𝑢𝑄 from a Gaussian distribution;
Compute 𝑔(Θ𝑡), 𝑔(Θ𝑡 + 𝜇𝑢𝑞) for 𝑞 = 1, . . . , 𝑄 via binary
Compute 𝑝(Θ𝑡), 𝑝(Θ𝑡 + 𝜇𝑢𝑞) for 𝑞 = 1, . . . , 𝑄 using Eq. (6);
Estimate the gradient ▽𝑝 (Θ𝑡) using Eqs. (8) and (9);
Update Θ𝑡+1 ← Θ𝑡 − 𝜂𝑡▽𝑝 (Θ𝑡);
5:
6:
7:
8: end for
9: Compute 𝐴′ = ℎ(𝐴, Θ𝑇), 𝑟 = ∥𝐴′ − 𝐴∥0/𝑁 (𝑁 − 1);
10: if 𝑟 ≤ 𝑏 then return 𝐴′ # succeed
11: else return 𝐴 # failed
12: end if
search;
adversarial graphs) as the following optimization problem:
Θ
||𝐴′ − 𝐴∥0,
Θ∗ = arg min
subject to 𝐴′ = ℎ (𝐴, Θ) ,
𝑓 (cid:0)𝐴′(cid:1) ≠ 𝑦0,
𝑟 ≤ 𝑏,
(2)
where 𝑟 is defined as 𝑟 = ∥𝐴′ − 𝐴∥0/𝑁 (𝑁 − 1) and ∥𝑀∥0 is the 𝐿0
norm of 𝑀, which counts the number of nonzero entries in 𝑀.
4 CONSTRUCTING ADVERSARIAL GRAPHS
In this section, we design our hard label black-box adversarial attack
to construct adversarial graphs by solving the optimization problem
in Eq. (2).
4.1 Overview
Eq. (2) is an intractable optimization problem, and we cannot di-
rectly solve it. In order to address this issue, we convert the opti-
mization problem into a tractable one and adopt a sign stochastic
gradient descent (signSGD) algorithm to solve it with convergence
guarantee. The signSGD algorithm computes gradients of graphs by
iteratively querying the target GNN model. We also design two algo-
rithms to reduce the number of queries: a coarse-grained searching
algorithm by leveraging the graph structure and a query-efficient
gradient computation algorithm that only requires one query in
each time of computation. The overview of our attack framework
is shown in Figure 1. The attack consists of three phases. First,
we relax the intractable optimization problem to a new tractable
one (Section 4.2). Second, we develop a coarse-grained searching
algorithm to identify a better initial adversarial perturbation/graph
(Section 4.3). Third, we propose a query-efficient gradient compu-
tation algorithm to deal with hard labels and construct the final
adversarial graphs via signSGD (Section 4.4). The whole proce-
dure of generating an adversarial graph for a given target graph is
summarized in Algorithm 1.
Session 1B: Attacks and Robustness CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea110Figure 1: Overview of our hard label black-box attack: (1) We reformulate our attack as an continuous optimization problem
that aims at minimizing perturbations on the target graph; (2) We design a coarse-grained searching algorithm to identify
initial perturbations for efficient gradient descent computation; (3) We develop a query-efficient gradient computation algo-
rithm, that only needs one query each time to compute the sign of gradients in signSGD. Finally, we obtain the adversarial
graph via adding the final perturbations on the target graph.
from the target graph to the classification boundary as follows:
𝑔 (Θ) = arg min
𝜆>0
{𝑓 (ℎ (𝐴, 𝜆Θ𝑛𝑜𝑟𝑚)) ≠ 𝑦0} ,
(4)
where Θ𝑛𝑜𝑟𝑚 is the normalized perturbation vector of the pertur-
bation vector Θ1 that satisfies ∥Θ𝑛𝑜𝑟𝑚∥2 = 1. 𝑔 (Θ) measures the
distance from the original graph 𝐴 to the classification boundary,
i.e., the minimal distance 𝜆 if we start at 𝐴 and move to another
class in the direction of Θ such that the predicted label of the per-
turbed graph 𝐴′ = ℎ (𝐴, 𝜆Θ𝑛𝑜𝑟𝑚) changes. We also denote(cid:98)𝑔 (Θ)
(cid:98)𝑔 (Θ) = 𝑔 (Θ) Θ𝑛𝑜𝑟𝑚.
as a distance vector which starts from 𝐴 and ends at classifica-
tion boundary at the direction of Θ with a length of 𝑔 (Θ), i.e.,
A straightforward way of computing optimal Θ∗ is to minimize
𝑔(Θ) because smaller 𝑔(Θ) may lead to less elements in Θ that
exceed 0.5. Thus, we should change less edges in 𝐴 for constructing
the adversarial graphs. However, it is not effective enough as it
does not consider the impact of the search direction, i.e., Θ, on
the attack. Specifically, the metrics of our attack is the number of
perturbed edges (i.e., the number of entries of Θ that exceed 0.5)
instead of the 𝐿2 norm distance (i.e., 𝑔 (Θ)). The perturbations with
different Θ1 and Θ2 may be different even if they share the equal
distance (i.e., 𝑔 (Θ1) = 𝑔 (Θ2)). We explain this via a toy sample
shown in Figure 2. We assume that two distance vectors(cid:98)𝑔 (Θ1) and
(cid:98)𝑔 (Θ2) with the dimension of 2 have the same length of √2/2 in
components of(cid:98)𝑔 (Θ1) along with 𝑥-axis and 𝑦-axis are √6/4 and
the direction of Θ1 and Θ2, respectively. The lengths of the two
√2/4, respectively. Thus, we only need to perturb one edge along
components of(cid:98)𝑔 (Θ2) are both 0.5, which means the attacker should
with 𝑥-axis as only √6/4 ≥ 0.5. However, the lengths of both two
perturb both edges because they both achieve the threshold 0.5.
Motivated by this toy example and by considering both Θ and 𝑔(Θ),
we define the following new objective function:
(5)
where 𝑐𝑙𝑖𝑝(𝑥) is a clip function which clips 𝑥 into [0, 1]. 𝑝 (Θ)
denotes the number of elements of(cid:98)𝑔 (Θ) that exceed 0.5. Thus, it
can measure the desired perturbations in the direction of Θ. Here,
𝑝 (Θ) = ∥𝑐𝑙𝑖𝑝((cid:98)𝑔 (Θ) − 0.5)∥0,
1Without loss of generality, we transform the triangle perturbation matrix Θ to the
corresponding vector form and use perturbation vector and perturbation matrix inter-
changeably without otherwise mentioned.
Figure 2: A toy sample: (i) Only one component of Θ1 ex-
ceeds 0.5, which means we only need to perturb one edge in
the graph in the direction of Θ1; (ii) Both components of Θ2
achieve 0.5 and thus we need to perturb both the two edges
in the direction of Θ2.
4.2 Reformulating the Optimization Problem
The optimization problem defined in Eq. (2) is intractable to solve.
This is because the objective function involves 𝐿0 norm related
to the variables of adversarial perturbation Θ, which is naturally
NP-hard. To cope with this issue, we use the following steps to
reformulate the original optimization problem.
Relaxing Θ to be continuous variables. We relax the binary
entries {0, 1} in Θ to be continuous variables ranging from 0 to 1,
i.e., Θ𝑖 𝑗 ∈ [0, 1] ∀𝑗 > 𝑖, such that we can approximate the gradients
of the objective function. Each relaxed entry can be treated as
the probability that the corresponding edge between two nodes
is changed. Specifically, we perturb the edge status between node
𝑖 and node 𝑗 if Θ𝑖 𝑗 ≥ 0.5; otherwise not. Thus, the perturbation
function ℎ defined in Eq. (1) can be reformulated as follows:
(cid:40)𝐴𝑖 𝑗
ℎ (𝐴, Θ)𝑖 𝑗 = ℎ (𝐴, Θ) 𝑗𝑖 =
Θ𝑖 𝑗  𝑖,
¬𝐴𝑖 𝑗 Θ𝑖 𝑗 ≥ 0.5, 𝑗 > 𝑖.
(3)
Defining a new objective function. Next, we define a new objec-
tive function that replaces the 𝐿0 norm with the 𝐿1 norm. Similar
to existing adversarial attacks against image classifiers [11], we can
define a distance function 𝑔 (Θ) for GNN to measure the distance
3. Construct  the final adversarial graph via query-efficient gradient computation and signSGD2. Find initial adversarial graphwith coarse-grained searchingAdd initial perturbations1. Reformulate the attack as a continuous optimization problemTarget graphInitial adversarial graphComputegradientsAdversarial graphUpdate perturbationsAdd final perturbationsAdd edgesDelete edges: perturb: no changeSession 1B: Attacks and Robustness CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea111𝑝 (Θ) = ∥𝑐𝑙𝑖𝑝((cid:98)𝑔 (Θ) − 0.5)∥1.
in order to calculate the gradients, we also replace the 𝐿0 norm in
Eq. (5) with the 𝐿1 norm as follows:
(6)
Converting the optimization problem. According to the def-
inition of 𝑝 (Θ), the attacker can find the optimal vector Θ∗ by
minimizing 𝑝 (Θ). Finally, we convert the original optimization
problem in Eq. (2) into a new one as follows:
Θ∗ = arg min
Θ
𝑝 (Θ) ,
subject to 𝑟 ≤ 𝑏.
(7)
Note that, (i) this optimization problem is designed for non-targeted
attacks. However, it can also be extended to targeted attacks via
changing the condition in Eq. (4) to 𝑓 (ℎ (𝐴, 𝜆Θ𝑛𝑜𝑟𝑚)) = 𝑦𝑐, where
𝑦𝑐 is the target label; (ii) Eq. (7) approximates Eq. (2). Note that, we
cannot guarantee that Eq. (2) and (7) have exactly the same optimal
values. Nevertheless, our experimental results show that solving
Eq. (7) can achieve promising attack performance.
4.3 Coarse-Grained Searching
In this section, we develop a coarse-grained searching algorithm to
efficiently identify an initial perturbation vector Θ0 that makes the
corresponding adversarial graph have a different predicted label
from 𝑦0 in the direction specified by Θ0. We note that, it is difficult
to find the valid Θ0 because the searching space is extremely large
when the number of nodes is large. Specifically, a graph with 𝑁
nodes has 𝑆 = 𝑁 (𝑁 − 1) /2 candidate edges. Each edge can be
existent or nonexistent so that the search space has a volume of
2𝑆, which increases exponentially as 𝑁 increases. The query and
computation overhead of traversing all candidate graphs in the
searching space is extremely large. Our coarse-grained searching
algorithm aims to leverage the graph structure property to reduce
the searching space.
We utilize the properties of a graph to reduce the number of
the queries and to find a better initial Θ0 that incurs small per-
turbations. Specifically, edges in a graph can reflect the similarity
among nodes. For instance, there could be more number of edges
within a set of nodes, but the number of edges between these nodes
and other nodes is much smaller. This means this set of nodes are
similar and we can group them into a node cluster. Inspired by
graph partitioning [24], we split the original graph into several
node clusters, where nodes within each cluster are more similar.
We denote supernode as one node cluster and superlink as the set of
links between nodes from two node clusters (See Figure 3). Then,
there are three components of a graph for us to search, i.e., (i) su-
pernode; (ii) superlink; and (iii) the whole graph. We take turns to
traverse these three types of searching spaces. The reason why we
target one specific type of components for perturbation each time
is that we can ensure the perturbations follow the same direction,
and thus can more effectively generate an adversarial graph. Oth-
erwise, perturbations added in different components will interfere
with each other. Thus, randomly selecting Θ0 is not a good choice
as it discards the structural information of the target graph. Our
experimental results in Section 5.2 also support our idea.
Particularly, we first partition the graph into node clusters (or
supernodes) using the popular and efficient Louvain algorithm [3].
After that, we traverse each supernode. In each supernode 𝑐, we
Figure 3: Coarse-grained searching. We partition the graph
into several node clusters, denoted as supernodes, and links
between two supernodes are denoted as a superlink.
uniformly choose a fraction 𝑠 ∈ [0, 1] at random to determine the
number of perturbed edges 𝑛, i.e., 𝑛 = 𝑠 · 𝑁𝑐 (𝑁𝑐 − 1) /2, where
𝑁𝑐 is the number of nodes in the supernode 𝑐. We then randomly
select 𝑛 edges to be perturbed and query the target GNN model
to see whether the label of the target graph is changed. We repeat
the above process, e.g., 5 · 𝑁𝑐 times used in our experiments, and
always keep the initial perturbation vector with minimal number of
perturbed edges. If we failed to find Θ0 that can change the target
label after searching all supernodes, we then search the space within
each superlink and finally the whole graph if we still cannot find a
successful Θ0. During the searching process, we can thus maintain
the perturbation vector with the smallest number of edges to be
perturbed.
Note that we can significantly reduce the searching overhead
by searching supernodes, superlinks, and the whole graph in turn.
First, we search supernodes before superlinks because the searching
spaces defined by superlinks are larger than those of supernodes. It
is not necessary to search within superlinks if we already find Θ0
within supernodes. Second, the size of the searching space defined
by the whole graph is 2𝑆, which is query and time expensive. As
we search it at last, we can find successful Θ0 in the former two
phases (i.e., searching supernodes and superlinks) for most of the
target graphs and only a few graphs need to search the whole space
(see Section 5.2). Via performing coarse-grained searching, we can
exponentially reduce the time and the number of queries when the
number of supernodes is far more smaller than the number of nodes.