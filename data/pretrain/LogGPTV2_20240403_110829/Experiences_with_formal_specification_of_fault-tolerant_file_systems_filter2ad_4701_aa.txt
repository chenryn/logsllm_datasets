# Experiences with Formal Specification of Fault-Tolerant File Systems

**Authors:**
- Roxana Geambasu, University of Washington
- Andrew Birrell, Microsoft Research
- John MacCormick, Dickinson College

## Abstract
Fault-tolerant, replicated file systems are a critical component of modern data centers. Despite their complexity, these systems are often specified only in brief prose, making them difficult to reason about or verify. This paper describes our experience using formal methods to enhance our understanding and confidence in the behavior of replicated file systems. We developed formal specifications for three real-world fault-tolerant file systems and used them to:
1. Highlight design similarities and differences.
2. Clarify and mechanically verify consistency properties.
3. Evaluate alternative designs.

Our experience demonstrated that formal specifications for these systems were straightforward to produce, valuable for deepening system understanding, and useful for system comparison.

## 1. Introduction
Fault-tolerant, replicated file systems are essential in today's dependable enterprise data centers. Examples include the Google File System (GFS), Niobe, and Dynamo, which underpin many web services provided by Google, Microsoft, and Amazon.com, respectively. Numerous other fault-tolerant file systems have been developed in academic settings, such as [15, 19]. These systems are highly complex, featuring sophisticated asynchronous protocols for replica consistency, recovery, and reconfiguration.

Despite their complexity, fault-tolerant file systems are typically described in just a few pages of prose, which can be incomplete, inaccurate, or ambiguous. This makes it challenging and error-prone to reason about and prove system properties. In contrast, formal specifications in languages like TLA+ [13] are unambiguous and provide a solid foundation for model checking and formally proving system properties. The benefits of formal specifications have been reported for various systems, including caches [11], space shuttle software [6], and local and distributed file systems [18, 20].

We aimed to explore how formal specifications and methods can aid in understanding, comparing, and proving properties of fault-tolerant, replicated file systems. To this end, we wrote formal specifications for three real-world, successful fault-tolerant file systems—GFS, Niobe, and Chain [19]—and used these to analyze, compare, and prove properties of the systems. This paper presents our experience in writing and using these formal specifications. Overall, we found that formal specifications improve system understanding, enable better comparison, and are reasonably easy to produce.

We found specifications particularly useful for three purposes:
1. **Highlighting Differences and Similarities:** Specifications clarify the mechanisms of the systems, revealing substantial overlap between GFS and Niobe, and isolating common and distinct mechanisms.
2. **Understanding and Verifying Consistency Semantics:** By reducing the system to a simplified analog (called a SimpleStore) and using refinement mappings [1], we verified that the system implements its SimpleStore, enabling us to reason about and compare consistency properties.
3. **Evaluating Alternative Designs:** Specifications facilitate experimentation with different system designs, providing a valuable tool for designers.

Our approach is pragmatic. While our specifications, in principle, enable full formal proofs [11], we rely on model checking of limited instances of the systems to confirm properties comfortably. By demonstrating the value of specifications in fault-tolerant file system analysis and comparison, we aim to convince system builders of the utility of specifying their own systems.

After providing some background (Section 2), we demonstrate the three uses of specifications (Sections 3, 4, and 5). We then review previous work (Section 6) and share some lessons from our experience (Section 7).

## 2. Background

### 2.1. Overview of the Studied Systems
In the three studied systems, each data object is stored at a group of replicas (groups can overlap), and the group is managed by a single master. The systems are reconfigurable, allowing failed or disconnected replicas to be removed and new replicas to be added.

- **GFS:** GFS provides a file-level write/append/read interface to clients. Files are partitioned into fixed-size chunks, each replicated by a group. The master assigns a unique primary to each group. For a chunk write or append, the client sends data to all replicas and submits a write request to the primary, who acknowledges the write if all replicas succeed. For reads, the client can go to any of the chunk’s replicas. Although the published paper does not guarantee the master's reliability, we assume it is reliable here for comparison with Paxos-based Niobe and Chain.

- **Niobe:** Niobe offers an object-level read/write interface, where the object is the replication unit. Each group has a unique primary. For a write, the client submits data to the primary, which writes it to disk and forwards it to the secondaries. The secondaries perform the write and acknowledge it to the primary. If any secondary fails to ACK the write in a timely manner, the primary proposes to the master that the failed replica(s) be removed from the group. After all replicas have ACKed the write (or have been removed), the primary responds to the client with success if the write succeeded at a configurable number of replicas, or with an error otherwise. For reads, the client goes to the primary.

- **Chain:** Chain imposes a structure on the replica group: replicas are arranged in a chain. Writes are sent to the head of the chain and travel toward the tail, where they get acknowledged. Reads are sent to the tail, which returns its local value. While GFS and Niobe support network partitions, the original Chain paper implicitly assumes no network partitions. For example, it does not specify how to prevent a client from reading from a stale but still alive tail. We assume no network partitions for Chain in this work.

### 2.2. TLA+ and Refinement Mappings
TLA+ is a formalism based on temporal logic, especially suited for specifying asynchronous distributed systems [13]. A TLA+ specification describes the allowed behaviors of a system using a state-machine approach, specifying the variables that compose the system’s state, a set of initial states, and the transitions leading from one state to another. A TLA+ specification can be enhanced with properties, which can be model-checked using the Temporal Logic Checker (TLC [14]). Because TLC exhaustively checks a system’s state space, which is typically exponential in system size, it can only be used on small instances of a system.

A refinement mapping [1] is a technique used to reduce one specification to another. Using refinement mappings, we reduce our specification of each system to a simple model of the system (called a SimpleStore). Figure 1 illustrates a refinement mapping: it maps a system’s state space onto the SimpleStore’s state space. A system implements its SimpleStore if all the system’s client-visible behaviors can be mapped onto valid SimpleStore behaviors.

We do not attempt to prove implementations. Instead, we specify the mappings as TLA+ properties and model-check them for limited instances of the systems (three replicas). Full proofs, although in principle enabled by our specifications [11], are out of scope here. However, our model checking covers the typical setting in industry systems like GFS, which use three-way replication.

Having verified that a system implements its SimpleStore, proving history-based consistency properties about the system (e.g., linearizability) is known to be reducible to proving them for its SimpleStore [11], which is significantly easier than reasoning about the whole system.

## 3. Comparing System Mechanisms
We produced TLA+ specifications for all three systems. For GFS and Chain, the specifications are based on published papers and conversations with the system designers; for Niobe, one of the designers participated in this work and is a co-author of this paper. Table 1 provides the sizes of our specifications and the time to write them. For each system, we specified at least how reads, writes, and replica removal are done. For Chain, we also specified the recovery mechanism. Due to the expressiveness of TLA+, our specifications distill core replication mechanisms and protocols from the systems’ complexity. As a result, our specifications are small (around 500 TLA+ lines, or about 10 pages), yet precise and high-level models of the systems. Overall, we found specifications to be extremely helpful for an in-depth understanding of the systems and reasonably easy to produce.

Specifications also prove valuable for a crisp comparison of the mechanisms in different systems. While a detailed examination of the specifications would show how the key differences and similarities stand out clearly in TLA+, we highlight some of the main findings below.

| System | TLA+ Lines | Time to Write |
|--------|------------|---------------|
| Chain  | 410        | 3 weeks       |
| Niobe  | 350        | 2.5 weeks     |
| GFS    | 450        | 3.5 weeks     |

---

This revised version aims to make the text more coherent, clear, and professional. It also ensures that the content flows logically and is well-structured.