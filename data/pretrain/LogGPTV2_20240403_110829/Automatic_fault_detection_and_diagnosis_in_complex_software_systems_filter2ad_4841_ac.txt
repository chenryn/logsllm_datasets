### Usage and Download Information
This document is authorized for licensed use by Tsinghua University. It was downloaded on March 20, 2021, at 09:56:20 UTC from IEEE Xplore. Certain restrictions apply.

### System State and Probability Distribution
The system state is represented by components \( S \). By assumption (1), the probability \( P(S_i) = 1 \). Therefore, the probability distribution vector for the different components is \( p_T = \{ p(S_1), p(S_2), \ldots, p(S_n) \} \).

With assumptions (4, 5, 6), the system can be modeled as a Markov chain with a transition probability matrix \( G \). A stationary probability distribution vector \( \pi = [ \pi(S_1), \pi(S_2), \ldots, \pi(S_n) ]^T \) satisfies \( \pi^T = \pi^T G \). If \( \pi \) exists, the significance measure of a component \( S \) is given by \( \pi(S) \). We call \( \pi \) the significance vector.

### Transition Probability Matrix Construction
We obtain the appropriate transition probability matrix \( G \) in three steps:

1. **Initial Transition Matrix \( H \)**: Let \( H \) be an \( n \times n \) matrix where \( H_{ij} = u_i(j) \). Based on assumption (5), this is the transition matrix if all transitions are of type progress.
2. **Stochastic Adjustment**: If we make the optional assumption (3), it can be shown that \( H \) is stochastic. Otherwise, a stochastic adjustment should be made by replacing all zero rows of \( H \) with \( \frac{1}{n} e^T \); i.e., \( L = H + a \frac{1}{n} e^T \), where \( a_i = 1 \) if \( P_i = \emptyset \) and \( a_i = 0 \) otherwise. This adjustment is in accordance with assumptions (3, 6), as a transition from an isolated component must be an exit transition.
3. **Primitive Adjustment**: Perform a primitive adjustment according to assumptions (4, 6): \( G = \alpha L + (1 - \alpha) ev^T \). As a consequence, it can be shown that \( G \) is stochastic, irreducible, and aperiodic [6]. Therefore, a unique positive stationary vector \( \pi \) exists and can be found using the Power method [6].

### Anomaly Score Adjustment
Once we have computed the significance vector, the anomaly score \( r_j \) of component \( S_j \) is adjusted as follows:
\[ r_j = \frac{\sum_{i=1}^n c_i(t) \cap M_{ij} \pi(C_j)}{\sum_{i=1}^n c_i(t) \cup M_{ij}} \]

The SigScore algorithm addresses the problem that popular components are weighted more heavily in the RatioScore algorithm by adjusting the anomaly score by an estimate of the component popularity. In practice, the recommended values for \( \alpha \), \( u_i^T \), and \( v^T \) [6] are:
- \( \alpha = 0.85 \)
- \( v^T = \frac{1}{n} e^T \)
- \( u_i(j) = \begin{cases} \frac{1}{|P_i|}, & \text{if } S_j \in P_i \\ 0, & \text{otherwise} \end{cases} \)

These suggested \( u_i^T \) and \( v^T \) assume equal probability of transition among components, which is reasonable given knowledge of component dependencies only. The suggested \( \alpha \) draws from experience in the web search domain and produces good results in our experiments. Moreover, the value of \( \alpha \) does not change the component ranks in the stationary vector but only affects the speed at which the Power method converges.

### Example
We illustrate how the SigScore algorithm works using the example presented in Section 4.2. The initial transition matrix is given by:
\[ H = \begin{pmatrix}
0 & \frac{1}{3} & \frac{1}{3} & \frac{1}{3} \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & \frac{1}{2} & 0 & \frac{1}{2}
\end{pmatrix} \]

The stochastic and primitivity adjustment yields:
\[ L = H \]
\[ G = 0.85L + 0.15 \frac{1}{n} e^T \]

The Power method yields the significance vector:
\[ \pi^T = (0.358, 0.131, 0.131, 0.245, 0.134)^T \]

The significance vector matches the intuition from Figure 3; Component A is the most popular, and Component D is the second most popular. After adjusting the anomaly scores by the components' popularity, the scores become (1.39, 2.51, 0, 1.02, 1.86). These updated scores correctly pinpoint Component B as the faulty component.

### Evaluation
We use a realistic J2EE-based system to validate our approach. Our testbed includes a data tier based on a DB2 UDB 8.2 database server and an application tier consisting of WebSphere 6 Application Server. Our target application is Trade [7], which provides a real-world transactional workload of a business application offering online trading of securities. Our workload driver emulates a uniform random population of clients accessing the application functionality (e.g., get stock quotes, buy, sell, view portfolio, etc.).

This workload causes the system to operate at a range of resource utilization levels. To monitor the system, we collect management metrics from WebSphere and DB2, which reflect the state and performance of 28 components (see examples in Table 2). We collect all metrics from WebSphere together with database- and table-level metrics from DB2, provided they have non-zero variance. Our datasets consist of more than 300 metrics collected every 10 seconds. The dataset we use to compute NMI is collected over a period of three hours during which normal activity is simulated. We use the same data to estimate the NMI threshold \( t_{NMI} \) corresponding to \( r^2 > 0.6 \), which generally indicates a strong linear relationship. We then feed the NMI values and \( t_{NMI} \) to the complete-link hierarchical agglomerative clustering algorithm [5] to group similar metrics together.

### Component Metrics
**Table 2. Examples of Metrics Collected**
- **Web Container**: #Sessions created/invalidated
- **Thread Pools**: Free pool size
- **JDBC Module**: Mean response time, #Free connections
- **Servlet/JSP and EJB**: #Instantiated, Mean response time
- **Database**: #Active connections, #Log writes
- **Database Tables**: #Rows retrieved/written

### Fault Injection
We evaluate our algorithms by injecting faults in components of Trade midway through experiments, which otherwise simulate normal activity. We simulate two categories of faults: application-level faults and operator mistakes. Application-level faults are defects that may appear when, for example, the application is updated. Operator mistakes consist of configuration errors made while tuning the system. Our faults include exceptions in application components, misconfiguration of the database authentication credentials, incorrect tuning of connection and thread-pool sizes, and inadvertent deletion of components during application re-deployment. Table 3 summarizes the 22 faults we use.

**Table 3. Faults Injected**
- **Application Faults**: Exceptions in EJBs, Exceptions in JSPs
- **Operator Mistakes**: Misconfigurations, Component deletion

For fault injection, we carry out separate 30-minute-long experiments, in each of which the system is faulty for 10 minutes.

### Fault Detection
An ideal fault-detection algorithm should report anomalies whenever there is a fault and report nothing otherwise. Nevertheless, after a fault occurs, the current observation window may contain both normal and anomalous samples. Thus, the Wilcoxon Rank-Sum test on sliding windows may not report anomalies immediately. There may be a time lag up to the length of the observation sliding window before anomalies are detected. We call this lag a detection window, which equals the size of the observation window times the duration of the sampling period.

We require our algorithm to achieve the following:
1. **Sensitivity**: When a fault occurs, the algorithm should report an anomaly after the fault occurs and before the detection window passes.
2. **Accuracy**: Any anomaly reported should truly represent a fault. Any anomaly reported when there is no fault is considered a false alarm and should be avoided.

### Fault Coverage and False-Positive Rate
We measure the detection sensitivity by the fault coverage \( FC \), which is given by:
\[ FC = \sum_{i=1}^n F_i \]
where \( F_i = 1 \) if an anomaly is reported within the detection window after a fault is injected in experiment \( i \), and \( F_i = 0 \) if no anomaly is reported within the detection window. Ideally, \( FC \) should equal \( n \).

We compute the false-positive rate \( FP \) thus:
\[ FP = \frac{\sum_{i=1}^n p_i}{\sum_{i=1}^n a_i} \]
where \( n \) is the number of experiments we carry out, \( a_i \) is the number of times alarms are raised during experiment \( i \), and \( p_i \) is the number of times alarms are raised during experiment \( i \) when there is no fault. Ideally, \( FP = 0 \).

We evaluate the detection performance using 22 experiments, each with faults injected from the time interval 32. In these experiments, we set the baseline window size to be the same as that of the test window and repeat these experiments with the test window size ranging from 4 to 12. Figure 4 shows the fault coverage, and Figure 5 shows the false-positive rate with different window sizes.

### Diagnosis
A prerequisite for performing diagnosis is that a fault is first detected; the diagnosis algorithm is only executed when the detection algorithm finds an anomaly. As shown in Figures 4 and 5, larger window sizes enable more reliable fault detection. Therefore, we set the window size to 12 in the remaining experiments, whereby we detect 17 out of 22 faults without any false alarms. We evaluate our diagnosis algorithms using these 17 experiments.

We evaluate the diagnosis quality by both the faulty-component rank and the identified-faults count criteria described in Section 4.

**Faulty-Component Rank**: The faulty-component rank in the 17 experiments is shown in Table 4. In the three experiments with “-” as the rank, the faulty component has an anomaly score of 0 and therefore is not ranked. In the remaining 14 experiments, we see that SigScore is always better than RatioScore except for one case. The average rank for SigScore is 4.76, whereas it is 7.18 for RatioScore.

**Identified-Faults Count**: Our modeling captures 28 components in our system via their metrics; as such, the candidate-set size \( t_c \) could be any number between 1 and 28, depending on the system administrator’s requirements. However, the smaller the candidate set, the faster a system administrator can localize the fault. Figure 6 shows the identified-faults count as \( t_c \) varies from 1 to 28. We can see that SigScore is strictly better than RatioScore. SigScore succeeds in identifying the faulty component within the top-5 in 7 out of 17 cases, which amounts to a 40% improvement over RatioScore. Likewise, SigScore includes the faulty component in the top ten in 13 of the 17 cases, as compared to 8 for RatioScore.

### Related Work
Several studies have reported the existence of statistical correlations between software-system metrics [4, 9, 15, 16, 17]. These works suggest that such correlations can be used to detect and localize faults. However, in prior work, researchers have assumed that correlations fit a specific mathematical form. In contrast, our approach to modeling does not assume any specific form of metric relationships.

Most current techniques for diagnosis based on metric correlations are variants of the Jaccard coefficient. These techniques have been proposed [10, 17] and evaluated [17] in the context of metric-pair models. Our RatioScore algorithm is also a variant of the Jaccard coefficient and is outperformed by our SigScore algorithm. The Jaccard coefficient is also used in PinPoint [1], which enables diagnosis of components based on their correlation with anomalous transactions. Pinpoint, however, relies on tracing user requests through the system, while we use management metrics. Collecting request traces is much more expensive than management metrics and is not studied in our work.

One other trend to improve the diagnosis is to learn from known faults. Typically, supervised learning is used such that any recurrent faults may be identified quickly. Such methods [2, 3, 11] use either Bayesian or neural networks to learn fault symptoms from labeled data. While such efforts significantly improve the diagnosis, they have important shortcomings: labeled data may not be available, and only known faults can be identified.

We construct a Markov process model and find the relative significance of system components based on component dependencies. This is similar to the PageRank [6] algorithm for web search. In a different context, Inoue et al. [8] use an algorithm similar to PageRank to identify generic components of a system to improve software reusability.

### Conclusions and Future Work
In this paper, we present an approach, built on information-theoretic measures, to automatically monitor the health of complex software systems and localize faulty components when faults occur. We employ the Wilcoxon Rank-Sum test to automatically identify significant changes in cluster entropy, thereby enabling robust fault detection. For diagnosing faulty components, we extend the use of the Jaccard coefficient to clusters of metrics. In addition, we propose SigScore, a diagnosis algorithm motivated by PageRank, which improves on the algorithm based on the Jaccard coefficient. We evaluate our approach using a realistic test-bed consisting of a multi-tier enterprise software system. We show through experiments that our fault-detection approach has high fault coverage and a low false-alarm rate. We also show improvement in diagnosis obtained by using SigScore. Our results indicate that the proposed diagnosis algorithm can provide valuable help for addressing faults in complex systems.

The SigScore algorithm requires information on component dependencies. However, complete dependency information may not always be available. Also, in dynamic systems, dependencies may change. Studying the effect of inaccurate dependency information on diagnosis is part of our future work.

### References
[1] M.Y. Chen, E. Kiciman, E. Fratkin, A. Fox, and E.A. Brewer. Pinpoint: Problem determination in large, dynamic internet services. In DSN, pages 595–604, 2002.

[2] I. Cohen, S. Zhang, M. Goldszmidt, J. Symons, T. Kelly, and A. Fox. Capturing, indexing, clustering, and retrieving system history. In SOSP, pages 105–118, 2005.

[3] S. Ghanbari and C. Amza. Semantic-driven model composition for accurate anomaly diagnosis. In ICAC, 2008.

[4] Z. Guo, G. Jiang, H. Chen, and K. Yoshihira. Tracking probabilistic correlation of monitoring data for fault detection in complex systems. In DSN, pages 259–268, 2006.

[5] J. Han and M. Kamber. Data Mining: Concepts and Techniques. Morgan Kaufmann, 2nd edition, 2006.

[6] J. Han and M. Kamber. Google’s PageRank and Beyond: The Science of Search Engine Rankings. Princeton University Press, 2006.

[7] IBM Corporation. IBM Trade Performance Benchmark. http://www-01.ibm.com/software/webservers/appserv/was/performance.html.

[8] K. Inoue, R. Yokomori, H. Rujiwara, T. Yamamoto, M. Matsushita, and S. Kusumoto. Component rank: relative significance rank for software component search. In ICSE, 2003.

[9] G. Jiang, H. Chen, and K. Yoshihira. Discovering likely invariants of distributed transaction systems for autonomic system management. In ICAC, 2006.

[10] G. Jiang, H. Chen, and K. Yoshihira. Modeling and tracking of transaction flow dynamics for fault detection in complex systems. IEEE Trans. on Dependable and Secure Computing, 3(4):312–326, 2006.

[11] M. Jiang, M.A. Munawar, T. Reidemeister, and P.A.S. Ward. Detection and diagnosis of recurrent faults in software systems by invariant analysis. In HASE, 2008.

[12] M. Jiang, M.A. Munawar, T. Reidemeister, and P.A.S. Ward. Information-theoretic modeling for tracking the health of complex software systems. In CASCON, 2008.

[13] J.O. Kephart and D.M. Chess. The vision of autonomic computing. IEEE Computer, 36(1):41–50, 2003.

[14] D.M. Levine, C.P.P. Ramsey, and R.K. Schmidt. Applied Statistics for Engineers and Scientists. Prentice Hall, 2000.

[15] M.A. Munawar and P.A. Ward. Adaptive monitoring in enterprise software systems. In SysML, June 2006.

[16] M.A. Munawar and P.A. Ward. A comparative study of pairwise regression techniques for problem determination. In CASCON, pages 152–166, 2007.

[17] M.A. Munawar and P.A. Ward. Leveraging many simple statistical models to adaptively monitor software systems. In ISPA, 2007.

[18] C.E. Shannon. A mathematical theory of communication. Key Papers in the Development of Information Theory, 1948.

[19] A. Strehl and J. Ghosh. Cluster ensembles—a knowledge reuse framework for combining multiple partitions. Journal on Machine Learning Research, 3:583–617, Dec 2002.