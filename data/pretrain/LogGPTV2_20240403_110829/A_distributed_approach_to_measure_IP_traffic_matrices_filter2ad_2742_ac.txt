paper [12] advocates a similar approach. Their suggestion
of using class-based counters is similar to ours when a class
is deﬁned to be either a link, a router or a PoP. For the
remainder of the paper we will assume these steps will be
taken in the future state of the art, and hence our discus-
sion of overheads for distributed solutions is in terms of this
vision.
3.3 Storage, Communications and
Computational Overheads
We now describe the overheads involved in computing a
traﬃc matrix at each of the granularity levels. The stor-
age overhead per router refers to the amount of ﬂow statis-
tics stored at the router. These elements are updated and
shipped to the collection station on an ongoing basis. The
communications overhead refers to the total amount of in-
formation that needs to be transmitted through the network
domain toward the collection station. This includes the in-
puts from all domain routers. The computational overhead
we show is that of the activity at the collection station.
First we consider the centralized approach that is avail-
able today. Because the basic granularity of the collected
and processed ﬂow statistics is that of a network preﬁx, we
have labelled the centralized solution as preﬁx-to-preﬁx in
the ﬁrst row of Table 1. Table 1 lists the associated over-
heads for the centralized solution for one time interval, that
is the derivation of one traﬃc matrix, regardless of its ac-
tual granularity. Let L denote the total number of links, R
the number of routers, and P the number of PoPs in the
network. The average number of links per router is thus
L/R. Based on the actual conﬁguration parameters of the
European network under study, and the current state of the
art, the collection station had to perform 5.5 million egress
node identiﬁcations (last column). This required 13 BGP
tables, since each PoP has a diﬀerent vantage point, and 1
ISIS routing table residing at the collection station.
The next three rows of this table correspond to the over-
heads for a distributed solution for cases where the end goal
is a TM at a speciﬁc granularity level. The notation ’p2p’
refers to a PoP-to-PoP matrix, ’r2r’ indicates the router-to-
router, and similarly for ’l2l’. With fully distributed solu-
tions, the collection station merely needs to assemble the
rows it receives as input and build the matrix. Letting F
denote the total number of ﬂows, then clearly in any typical
network, we will have P < R < L << F . As an example, in
a typical large Tier-1 backbone for a geographic region the
size of the USA, P is on the order of tens, R is on the order
of hundreds, L is on the order of a few thousands, and F is
on the order of millions or even billions.
The advantages of moving from a centralized approach to
a distributed one are clear: (1) the router storage overhead
is reduced from O(F ) to O(R) or O(L) where R and L can
be many orders of magnitude smaller than F ; and (2) the
communications overhead is reduced by two or three orders
of magnitude (depending upon the target TM). In addition,
recall that a problem with the centralized approach is that
the routing table information at the collection station will
regularly become out of date. Distributed solutions do not
suﬀer from this problem since the information on the routers
is essentially always up to date (as fast as the protocol can
perform updates).
The amount of storage and communications overhead partly
depends upon what the ISP wants to collect. For example,
if an ISP is sure it only ever wants to collect the r2r matrix,
then each router should compute one row of this matrix and
the communications overhead is limited to R2. If however
a carrier prefers to leave open the ﬂexibility of looking at a
TM at any of the granularity levels, then they should seek
the l2l matrix, which can be aggregated into a r2r and p2p
by checking which link belongs to which router, which router
belongs to which PoP, and so on. In this case the commu-
nications overhead is L2.
There is one other very critical piece to the overhead issue,
and that is the frequency with which one wants to collect a
traﬃc matrix. Suppose carriers decide they want the traﬃc
matrix to be updated K times within each day. Then all
of the numbers in Table 1 would be multiplied by K for
each day. In the case we focus on, these overheads would be
incurred once every hour.
Clearly one needed step for direct measurement of a traﬃc
matrix to become practical is for ﬂow monitoring at routers
to increase their functionality so as to compute rows of traﬃc
matrices, thereby enabling a distributed approach. However
we believe that this is still not enough. A communications
overhead of L2 or R2 records incurred every hour may still
be regarded as excessive. In the next section, we show that
there indeed exist more eﬃcient ways to obtain accurate
estimates of TMs on an hourly basis.
Scheme
approach
SrcPreﬁx2DstPreﬁx
centralized
l2l row
distributed
r2r row
distributed
p2p row
distributed
Storage
(router)
L/R · ¯F (l)
3 · 67, 000
L/R · L
(3 · 81 = 243)
R
(27)
P
(13)
Communications
(network)
L · ¯F (l)
81 · 67, 000 = 5, 5M
L2
(81 · 81 = 6561)
R2
(27 · 27 = 729)
R · P
(27 · 13 = 351)
Computational
(collection station)
5,5M lookups, aggregation to
the required granularity
(additional storage: 13 BGP
routing tables, 1 ISIS routing
table, topology)
Matrix Assembly
Matrix Assembly
Matrix Assembly
Table 1: Overheads (in number of records) for computing the traﬃc matrix in one time interval.
4. TOWARD A MORE SCALABLE
APPROACH
4.1 Initial Observations
In examining our collected traﬃc matrices, we found that
the elements of a traﬃc matrix typically span a few orders
of magnitude.
In Fig. 3 we provide the empirical CDF
of origin-destination throughputs for the ﬁrst week in the
data, one at each level of granularity. The ﬂow ranges from
below 1 Kbps to roughly 100 Mbps at each level of granu-
larity. Flows near 1 Kbps (the leftmost point on the x-axis
of the ﬁgure) can essentially be viewed as near zero valued
elements. In p2p matrices, 15% of the matrix elements are
near zero; for r2r matrices the number is roughly 32% and
for l2l the percentage is over 60%. This indicates that these
matrices are sparse and that the sparsity increases at smaller
levels of granularity.
Figure 3: Empirical cumulative density function for
p2p, r2r and l2l ﬂows.
In addition, a large fraction of the OD ﬂows experience
throughput values below 1 Mbps, which could be considered
as tiny in a Tier-1 network. There is a problem with these
tiny ﬂows due to a limitation stemming from the way our
measurements are collected. The systematic sampling as
employed by Netﬂow v8 is likely to provide us with inaccu-
rate ﬂow measurements when ﬂows are tiny in magnitude.
Given that we compute ﬂow statistics on packet samples
(every 250th packet), it is likely to be the case that tiny
ﬂows are undersampled or missed altogether. Since these
tiny ﬂow throughputs are likely to be distorted due to infre-
quent sampling, there is little conﬁdence in their observed
properties.
The impact of sampling on the collected ﬂow measure-
ments is outside the scope of this work (for the issues in-
volved refer to [4]). Nevertheless, in order to avoid possible
inaccuracies that come from working with improperly sam-
pled ﬂows, our scheme is tuned to focus on the relevant
ﬂows. We deﬁne relevant ﬂows as those with average weekly
throughput greater than 1 Mbps. These relevant ﬂows still
include ﬂows of varying sizes that can be considered small,
medium or large. Our intention is to estimate all ﬂows but
with the goal of achieving low errors on the relevant ﬂows.
Many have argued that what carriers care about is estimat-
ing the larger elements well, because it is those elements that
capture the majority of the traﬃc [5, 13, 11]. We concur
with this statement. With this deﬁnition of relevant ﬂows,
we capture 95% or more of the total traﬃc load. Table 2
shows the number of OD pairs categorized as “relevant” and
the total captured load for each of our three types of traﬃc
matrices.
Granularity # relevant
captured load
p2p
r2r
l2l
85
216
470
98%
95%
96%
Table 2: Relevant ﬂows for l2l, r2r, and p2p.
4.2 The notion of a node fanout
First we establish our notation. We denote each element
in the traﬃc matrix as X(i, j, n) for the amount of traﬃc
ﬂowing from node i to node j at time interval n. Time is
discretized into 1 hour intervals; we let T denote the total
number of hours in our measurements and thus 1 ≤ n ≤ T .
The total number of nodes is given by M so we have 1 ≤
i, j ≤ M . Each row of a traﬃc matrix is a description of how
all the traﬃc from one source node is distributed among all
other nodes. Let f (i, j, n) denote the fraction of node i’s
traﬃc destined to egress node j at time interval n. This is
given by,
(cid:80)M
X(i, j, n)
k=1 X(i, k, n)
f (i, j, n) =
, 1 ≤ n ≤ T
(1)
where 0 ≤ f (i, j, n) ≤ 1. We deﬁne the vector (cid:126)f (i, n) to
be the node fanout as it captures how node i’s traﬃc is
10−310−210−110010110210300.10.20.30.40.50.60.70.80.91Flow throughput (Mbps)F(x)Empirical CDFp2pr2rl2l(cid:80)M
j=1 f (i, j, n) = 1,∀i, n.
apportioned across all the egress nodes. The elements of
this vector sum to one, i.e.,
Note that the denominator in Eq. 1 corresponds to all
the traﬃc that node i emits at time n.
If the node cor-
responds to a link, then this traﬃc load is available from
SNMP counters. If the node is a router, then this load is
computable by summing the SNMP counts from all ingress
links to the router (in our case all links from gateway routers
and peers that attach to this particular backbone router).
Letting Yi(n) denote this load at time n, we can rewrite
equation 1 as
X(i, j, n) = f (i, j, n) · Yi(n)
(2)
Representing traﬃc matrix elements as a product of a fanout
and SNMP data has been used before [8, 13]. Now, for the
ﬁrst time, we examine temporal properties of these fanouts.
4.3 Temporal Properties of Fanouts
By examining f (i, j, n) for a ﬁxed i and j, and letting n
vary, we can see how the fanout from node i to a particular
egress point j varies over time. In Fig. 4 we show the tempo-
ral behavior of such fanout elements for three example ﬂows,
one at each level of granularity. The fanout for the p2p ﬂow
remains almost constant for the entire week. The fanouts
for the r2r and l2l example ﬂows exhibit diurnal patterns
that repeat themselves throughout the week (no signiﬁcant
diﬀerence is observed in the weekend). This implies that
these fanout elements are very predictable. The constant
ones can be predicted by simply measuring in a single time
interval. Those with diurnal patterns are predictable once
you know the daily cycle.
Figure 5: Variability of fanouts vs. traﬃc matrix.
all three levels of granularity. In other words, we computed
the coeﬃcient of variation for each fanout over all days for
the 1-2pm slot, and similarly for all other slots. This met-
ric allows us to assess the variability of fanouts (and OD
ﬂows) within the same hour across days. We found that