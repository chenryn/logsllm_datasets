by Figure 5a, in which a program is executed on packet arrival and
identifies the appropriate socket receive queue. One novel aspect of
our design is that it leaves sockets untouched. We label the design as
sk_lookup, and implement the program using BPF [28].
The program itself takes a BPF map structure for reference and is
injected onto the TCP socket lookup path. As shown in Figure 5a, its
execution is second stage, triggered after the lookup path attempts to
match a connected socket (4-tuple) and before looking for a listening
socket (2-tuple). The program structure resembles a firewall rule,
as shown in Figure 5b, and consists of a set of match statements
followed by an action. The map is populated by a socket activation
service that receives a file descriptor for any socket created. Upon
receipt of a file descriptor the service updates the BPF map.
437
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
Fayed, et al.
The Linux kernel patch [43, 44] was accompanied by rigorous
performance evaluations required for acceptance into the mainline,
also public [62, 63] but omitted for space. Results indicate penalties
of ∼1-5% from baseline measures of packets per second (∼1M TCP
and ∼2.5M UDP) and CPU usage. This is regarded a positive result
in exchange for the increased flexibility.
sk_lookup is proving to be a powerful construct at CDN-scale.
In addition to enabling our efforts to completely decouple IP from
names and resources, sk_lookup is anticipated to be a crucial tool
for multiple streams of future investigation.
3.4 Generalizability, Transferability, and Benefits
One question that arises is whether these mechanisms generalize to
other domains. The short answer is yes, as described below.
Generalizability and Transferability This approach generalizes
to any listening service that uses HTTP, TLS, or other name-based
multiplexing, and that meets the following two requirements:
i. Control of authoritative DNS – the service operator must be
able to execute its policy-based selection when responding
to a query (as in our deployment), or populate authoritative
DNS records with policy mappings generated off-line.
ii. Control of connection termination or, at a minimum, the
ability to listen on the selected address(es) and ports.
The above pair of requirements defines the transferable domain as
being any service operators that manage its own authoritative DNS
and connection termination, irrespective of size.
We emphasize that the use of anycast and sk_lookup are optional.
sk_lookup injects flexibility into socket-to-process mappings and
efficiency in IP:port pair allocations at scale, as discussed in §3.3. In
§6 we describe ways that addressing agility can be combined with
anycast to build new features and systems.
Outcomes: Operational and Address Efficiency The initial moti-
vation for this work was to improve address agility, while simultane-
ously reducing planning and execution costs associated with address
management. We observe that large changes in address usage need
take only as long as necessary for stakeholders to agree, and min-
utes or seconds more to execute. Alongside, “more address space is
needed” thinking and impulses (as well as their costs) are no longer
conventional as the operations grow. For business reasons we are
unable to reveal exact measures, but engineer-months or weeks are
evolving into hours and minutes. The degree of flexibility, and the
reasons this system works, are demonstrated in §4.
In addition, a third qualitative observation is unanticipated yet
proving to be beneficial: Engineers are learning to ignore IP ad-
dresses as constraints, and instead treat them as resources that can be
scheduled. When designing new features and systems for managing
infrastructure, IP addresses are being treated as an after-thought.
The process no longer begins by asking about address availability or
usage as a way to limit design possibilities. Instead, algorithms and
solutions are designed with generic identifiers, each representing a
unique property, attribute, or policy. In the final design, the identifier
corresponds to an IP address. The demotion of the IP addresses from
a first class object in this manner has enabled us to reason about new
systems or improve existing ones, as described in §6.
In the next sections we deploy our architecture, evaluate the one-
address hypothesis, and then begin to use the architecture to build
features that have no obvious alternative designs.
4 AT SCALE: RANDOMIZE 20M TO /24
The previous section described two halves of an architecture that
completely decouples IP addresses from names and servers. Together
they enable a CDN to treat addresses as a flexible and dynamic
resource: Rather than a constraint that must be pondered, addresses
form a resource pool that can be scheduled, removed, and re-added.
Here, we pose the question: Are there limits to address usage or
rates of change as a CDN or service provider scales? We note that all
standard measures of our system’s performance reduce to sets of be-
fore and after evaluations that are indistinguishable. This is expected
since the first order evidence of success in our system is the ab-
sence of breakage. For this reason, otherwise standard performance
measures are uninteresting and omitted for space.
We first present and evaluate the random selection strategy, fol-
lowed by single-address evaluation in §5.
Randomize IP addresses Rather than bind a service to a set of IP
addresses, for all websites and services that match the policy – more
than 20 million in our deployment – this solution randomly selects
an IP address per-DNS query as described in §3.2. At any point in
time, any website or service hosted by a CDN could take on any of
the IP addresses. Our operational deployment shows that this has no
negative impact on performance, and can enable improvements.
4.1 Production Edge Evaluation Architecture
The Cloudflare network is comprised of points of presence (PoPs)
across more than 200 cities in over 100 countries, and direct inter-
connects with over 9500 distinct networks.
From a networking perspective, the standard CDN service offer-
ings are characterized by two properties. First, the network operates
as a reverse proxy on behalf of its customers’ origin servers, whose
hostnames are registered with Cloudflare’s authoritative DNS. This
enables Cloudflare to return its own IPs in response to DNS queries,
and terminate connections on behalf of customer origins.
In addition, Cloudflare uses anycast—not just for DNS service—
but for all of its web services. Accordingly, a DNS query at all
PoPs for a customer origin will receive the same IP address in
response. While the use of anycast is most commonly associated
with reachability, its use for content provides an additional benefit:
Identical network configurations are mirrored across all PoPs and
datacenters, which facilitates deployment and management.
Each PoP in the network is co-located with a datacenter. The
data center architecture and software stack are both architected for
simplicity. In particular, the server software stack is uniform by de-
sign, as encapsulated by the blue-dashed line in Figure 6. Rather
than different servers with different services, each server mirrors a
single software stack and offers all services—every server executes
distributed denial-of-service, layer-4 load balancers, connection ter-
mination, and the full suite of application processes. Every machine
also participates in the distributed cache. Servers logically sit be-
tween an ECMP router at the ingress, and an origin gateway at egress.
The ECMP router doubles as the datacenter’s first-pass stateless load
438
The Ties that un-Bind: Decoupling IP from web services and sockets. . .
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
Figure 6: The edge architecture is designed for simplicity: Servers share a uniform software stack, as encapsulated by the dashed line. An
ECMP router with consistent hashing fans connections out to servers. An additional L4 load balancer between them precedes connection
termination and application suites. All servers participate in the distributed cache. Our changes are entirely transparent. Only authoritative DNS
is touched and, separately, sk_lookup is installed automatically with the Linux kernel. DoS protections, L4LB, caching, and all surrounding
systems are unchanged.
balancer that, similar to previous systems [22, 45, 48], hashes pack-
ets in a consistent manner to spread connections between servers.
Specific component details are out of scope and left for future
work. Most notably, the datacenter architecture is composed of famil-
iar service architecture components. We emphasize that no changes
were required to surrounding components; our changes are scoped
to DNS and otherwise are completely transparent.
4.2 Experimental Setup with Live Traffic
Our architecture is currently deployed across the whole CDN in
keeping with the uniform software stack. It is currently active, at
scale, and is expected to remain active beyond final submission of
this work while we investigate new systems described in §6. Our
system has been running as follows:
• 6 PoPs/DCs at 8 IXPs serving 5 contiguous timezones;
• 100% of DNS responses for 20+ million hostnames;
• ∼5-6K DNS queries per second (mean)
• ∼35-40K HTTP requests per second (mean);
• active from 2020-07;
at each PoP, the following socket and network configuration:
• One /20 (IPv4) and one /44 (IPv6) address pool;
• Both prefixes advertised from all PoPs (anycast);
• connection termination listening on ports 80, 443, and 11
others [16] for the entire /20 and /44 ranges.
The timetable for in-use portion of addresses within the /20:
• 2020-07 to 2021-01, 4096 addresses (the full /20);
• 2021-01 to 2021-05, 256 addresses (/24);
• from 2021-06, on-going, 1 single IPv4 address (/32) in a
mid-sized datacenter (see §5).
We emphasize that our architecture is a drop-in software modification
to existing production architecture and systems. The deployment
uses the complete set of production servers and infrastructure, and
all surrounding systems are both unchanged and unaffected—all
existing measures of performance were found to be indistinguishable.
The breadth and duration of the deployment allay any concerns
about scale or feasibility – for more than 1 year, clients have initiated
∼500 million DNS queries per day, followed by ∼3-4 billion HTTP
requests, to addresses selected per-query purely at random. For
comparison, the same hostnames at all remaining 200+ data centers
were mapped across 18 /20s. The reduction in address usage is 94.4%
for the /20, and 99.7% for the /24. A /24 is the minimum permissible
address range in BGP. In §5 we reduce IP usage to /32, and describe
ways that the ‘leftover’ addresses from the required /24 might be
used for services and resilience in §6.
A comment on the operational benefits. The reduced address
space is evident. However, before describing why this works, one
particular observation may be of value to the wider community. The
per-query rate of change of IP addresses made evident by this work
is changing address management internally. From an operational
perspective, address management is decreasingly concerned with
hostnames and servers. In return, network and address management
is increasingly focussed on the use of IP addresses for their intended
purposes—routing, reachability, and quality-of-service— resulting
in improvements on engineering time, flexibility, and ingenuity.
4.3 Why does this work, and why do it?
We first remind our reader that our changes to DNS are entirely trans-
parent. The surrounding systems, software stack, and configurations
are untouched. In addition, all performance metrics are identical
and indistinguishable, and omitted accordingly. Here, we explain
the reasons that randomization works, and its benefits.
Routing is unchanged. Reachability between autonomous systems
(ASes) is evaluated over prefixes advertised by the AS that either
owns the addresses or is permitted to advertise them. Furthermore,
forwarding is decided by longest-prefix match. Since BGP routing
succeeds at the granularity of IP prefixes, the semantics of a pre-
fix used for randomized addresses is identical to prefixes used for
statically bound IPs.
Smaller prefixes are harder to leak or hijack. To mitigate against
routing table inflation, a best-practice convention among network
439
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
Fayed, et al.
(a) Pre-agility: two (of 18) busiest /20s.
(b) Per-query random addressing in a /20.
(c) Per-query random addressing in a /24.
Figure 7: Load across IP addresses at a medium-popularity facility, serving 20M+ hostnames, approximately 10% of websites [67].
(a) In the two busiest (out of 18) /20 prefixes without addressing agility; (b) Randomizing over a single /20 with addressing agility; and (c)
Randomizing over a single /24 with addressing agility. Data is comprised of 1% of all requests taken over 24 hours. The 𝑥-axis is IP addresses
in descending order of number of requests each IP address receives. This shows that, in today’s CDNs, per-IP address load is far from uniform –
but can be made uniform by decoupling IP addresses from names.
operators is to refrain from advertising narrower than a /20 prefixes
without good reason (/48 in IPv6 [72]). However, larger prefix ad-
vertisements increase the likelihood, and decrease the visibility, of
narrower sub-prefixes that leak or are hijacked. It is for this reason
that some prefixes are advertised as /24s, e.g., Cloudflare’s public
DNS resolver [14]—by advertising the narrowest prefix permitted
by BGP, the resolvers are resilient to hijacks. We revisit route leak
detection and mitigation in §6.
Since a single /20 can be used to reach limitless hostnames and
services, then larger or more blocks may be unnecessary. Our eval-
uations show that a /24 (256 or fewer addresses) is equivalent to a
/20 in this respect. The implication of our deployment is that a CDN
could operate with fewer addresses and simultaneously, as if for free,
increase its resilience to route leaks and hijacks – without inflating
Internet route and forwarding tables.
ECMP and consistent hashing are unaffected An ECMP group
connects a single virtual IP to multiple servers. Alongside, consistent
hashing ensures that all packets for a connection are forwarded to a
single server. At scale in operational settings, the number of ECMP
groups can grow to tens of thousands. Such numbers are difficult to
manage, and also exposed limits of router software [38]. The flexible
address assignment enabled by our architecture may facilitate ECMP
changes. However, our architecture exists independently from ECMP
and consistent hashing, for which complexities are dominated by
numbers of servers and not IP addresses.
Distributed caches and filesystems are unaffected Our architec-