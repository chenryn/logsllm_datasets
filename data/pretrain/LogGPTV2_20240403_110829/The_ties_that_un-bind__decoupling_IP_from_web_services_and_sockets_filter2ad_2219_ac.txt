### sk_lookup Design and Implementation

Figure 5a illustrates the execution of a program upon packet arrival, which identifies the appropriate socket receive queue. A key innovation in our design is that it does not modify existing sockets. We refer to this design as `sk_lookup` and implement it using BPF [28].

The `sk_lookup` program references a BPF map structure and is injected into the TCP socket lookup path. As shown in Figure 5a, the program executes in the second stage, after the lookup path attempts to match a connected socket (4-tuple) and before searching for a listening socket (2-tuple). The program's structure is similar to a firewall rule, as depicted in Figure 5b, and consists of a series of match statements followed by an action. The BPF map is updated by a socket activation service, which receives a file descriptor for each created socket.

### Performance Evaluation

The Linux kernel patch [43, 44] was rigorously evaluated for performance, with results publicly available [62, 63]. The evaluations showed a penalty of approximately 1-5% on baseline measures of packets per second (about 1M TCP and 2.5M UDP) and CPU usage. This minor overhead is considered acceptable given the increased flexibility provided by `sk_lookup`.

### Scalability and Flexibility

`sk_lookup` has proven to be a powerful tool at CDN scale. It not only enables the complete decoupling of IP addresses from names and resources but is also expected to be a critical tool for future research and development.

### Generalizability and Transferability

A common question is whether these mechanisms can be applied to other domains. The answer is yes, as described below.

#### Generalizability and Transferability

This approach can be generalized to any listening service that uses HTTP, TLS, or other name-based multiplexing, provided the following two requirements are met:

1. **Control of Authoritative DNS**: The service operator must be able to execute policy-based selection when responding to DNS queries or populate authoritative DNS records with policy mappings generated offline.
2. **Control of Connection Termination**: The service operator must have control over connection termination or, at a minimum, the ability to listen on the selected addresses and ports.

These requirements define the transferable domain as any service operators managing their own authoritative DNS and connection termination, regardless of size.

We emphasize that the use of anycast and `sk_lookup` is optional. `sk_lookup` injects flexibility into socket-to-process mappings and improves the efficiency of IP:port pair allocations at scale, as discussed in §3.3. In §6, we describe how addressing agility can be combined with anycast to build new features and systems.

### Operational and Address Efficiency

The primary motivation for this work was to improve address agility while reducing the planning and execution costs associated with address management. We observe that significant changes in address usage now take only as long as necessary for stakeholders to agree, plus a few minutes or seconds to execute. The need for more address space and its associated costs are no longer conventional as operations grow. For business reasons, we cannot disclose exact measures, but the time required for address management has been reduced from engineer-months or weeks to hours and minutes.

Additionally, a third qualitative observation is proving beneficial: Engineers are learning to treat IP addresses as resources that can be scheduled rather than constraints. When designing new features and systems, IP addresses are now treated as an afterthought. The process no longer begins by considering address availability or usage as a limiting factor. Instead, algorithms and solutions are designed with generic identifiers, each representing a unique property, attribute, or policy. In the final design, these identifiers correspond to IP addresses. This demotion of IP addresses from a first-class object has enabled us to reason about new systems or improve existing ones, as described in §6.

### Deployment and Evaluation

In the next sections, we deploy our architecture, evaluate the one-address hypothesis, and begin to use the architecture to build features that have no obvious alternative designs.

### At Scale: Randomizing 20M Addresses to /24

The previous section described an architecture that completely decouples IP addresses from names and servers, enabling a CDN to treat addresses as a flexible and dynamic resource. Here, we pose the question: Are there limits to address usage or rates of change as a CDN or service provider scales? All standard measures of our system’s performance reduce to sets of before and after evaluations that are indistinguishable. This is expected since the first-order evidence of success in our system is the absence of breakage. For this reason, otherwise standard performance measures are uninteresting and omitted for space.

#### Randomize IP Addresses

Instead of binding a service to a set of IP addresses, for all websites and services that match the policy—more than 20 million in our deployment—this solution randomly selects an IP address per DNS query as described in §3.2. At any point in time, any website or service hosted by a CDN could take on any of the IP addresses. Our operational deployment shows that this has no negative impact on performance and can enable improvements.

### Production Edge Evaluation Architecture

The Cloudflare network comprises points of presence (PoPs) across more than 200 cities in over 100 countries, with direct interconnects to over 9500 distinct networks. From a networking perspective, the standard CDN service offerings are characterized by two properties:

1. **Reverse Proxy Operation**: The network operates as a reverse proxy on behalf of customers’ origin servers, whose hostnames are registered with Cloudflare’s authoritative DNS. This enables Cloudflare to return its own IPs in response to DNS queries and terminate connections on behalf of customer origins.
2. **Anycast Usage**: Cloudflare uses anycast not just for DNS service but for all web services. A DNS query at all PoPs for a customer origin will receive the same IP address in response. Anycast provides the additional benefit of identical network configurations mirrored across all PoPs and data centers, facilitating deployment and management.

Each PoP is co-located with a data center. The data center architecture and software stack are designed for simplicity. The server software stack is uniform, as encapsulated by the blue-dashed line in Figure 6. Each server offers all services, including distributed denial-of-service, layer-4 load balancers, connection termination, and the full suite of application processes. Every machine also participates in the distributed cache. Servers sit between an ECMP router at the ingress and an origin gateway at egress. The ECMP router doubles as the data center’s first-pass stateless load balancer, hashing packets consistently to spread connections between servers.

### Experimental Setup with Live Traffic

Our architecture is currently deployed across the entire CDN, maintaining the uniform software stack. It is active at scale and is expected to remain active beyond the final submission of this work. The system has been running as follows:

- **Deployment Details**:
  - 6 PoPs/DCs at 8 IXPs serving 5 contiguous time zones.
  - 100% of DNS responses for 20+ million hostnames.
  - Approximately 5-6K DNS queries per second (mean).
  - Approximately 35-40K HTTP requests per second (mean).
  - Active from July 2020.

- **Socket and Network Configuration**:
  - One /20 (IPv4) and one /44 (IPv6) address pool.
  - Both prefixes advertised from all PoPs (anycast).
  - Connection termination listening on ports 80, 443, and 11 others [16] for the entire /20 and /44 ranges.

- **Address Pool Timetable**:
  - July 2020 to January 2021: 4096 addresses (the full /20).
  - January 2021 to May 2021: 256 addresses (/24).
  - From June 2021, ongoing: 1 single IPv4 address (/32) in a mid-sized data center (see §5).

Our architecture is a drop-in software modification to the existing production architecture and systems. The deployment uses the complete set of production servers and infrastructure, and all surrounding systems are unchanged and unaffected. All existing performance measures were found to be indistinguishable. The breadth and duration of the deployment alleviate any concerns about scale or feasibility. For more than a year, clients have initiated approximately 500 million DNS queries per day, followed by 3-4 billion HTTP requests, to addresses selected per-query purely at random. For comparison, the same hostnames at all remaining 200+ data centers were mapped across 18 /20s. The reduction in address usage is 94.4% for the /20 and 99.7% for the /24. A /24 is the minimum permissible address range in BGP. In §5, we reduce IP usage to /32 and describe ways that the 'leftover' addresses from the required /24 might be used for services and resilience in §6.

### Operational Benefits

The reduced address space is evident. However, before describing why this works, one particular observation may be of value to the wider community. The per-query rate of change of IP addresses made evident by this work is changing internal address management. From an operational perspective, address management is decreasingly concerned with hostnames and servers. In return, network and address management are increasingly focused on the use of IP addresses for their intended purposes—routing, reachability, and quality-of-service—resulting in improvements in engineering time, flexibility, and ingenuity.

### Why Does This Work, and Why Do It?

First, we remind the reader that our changes to DNS are entirely transparent. The surrounding systems, software stack, and configurations are untouched. All performance metrics are identical and indistinguishable, and thus omitted. Here, we explain the reasons that randomization works and its benefits.

- **Routing Unchanged**: Reachability between autonomous systems (ASes) is evaluated over prefixes advertised by the AS that either owns the addresses or is permitted to advertise them. Forwarding is decided by longest-prefix match. Since BGP routing succeeds at the granularity of IP prefixes, the semantics of a prefix used for randomized addresses are identical to those used for statically bound IPs.
- **Smaller Prefixes Harder to Leak or Hijack**: To mitigate against routing table inflation, a best-practice convention among network operators is to refrain from advertising narrower than a /20 prefixes without good reason (/48 in IPv6 [72]). Larger prefix advertisements increase the likelihood and decrease the visibility of narrower sub-prefixes that leak or are hijacked. By advertising the narrowest prefix permitted by BGP, resolvers are resilient to hijacks. We revisit route leak detection and mitigation in §6.
- **ECMP and Consistent Hashing Unaffected**: An ECMP group connects a single virtual IP to multiple servers. Consistent hashing ensures that all packets for a connection are forwarded to a single server. At scale in operational settings, the number of ECMP groups can grow to tens of thousands, which are difficult to manage and expose limits of router software [38]. The flexible address assignment enabled by our architecture may facilitate ECMP changes. However, our architecture exists independently from ECMP and consistent hashing, where complexities are dominated by the number of servers, not IP addresses.
- **Distributed Caches and Filesystems Unaffected**: Our architecture does not affect distributed caches and filesystems.