vector machine, and logistic regression), securing deep learning
systems deployed in adversarial settings poses even more challenges
due to their significantly higher model complexity [34]. One line of
work focuses on developing new evasion attacks against deep neural
networks (DNNs) [14, 25, 29, 44]. Another line of work attempts to
improve DNN resilience against such attacks by developing new
training and inference strategies [25, 29, 31, 45]. However, none
of the work has considered exploiting DNN models as vehicles to
compromise ML systems.
Gu et al. [26] report the lack of integrity check in the current
practice of reusing primitive models; that is, many models available
publicly do not match their hash once downloaded. Liu et al. [36]
further show that it is possible to craft malicious ML systems and
inputs jointly such that the compromised systems misclassify the
manipulated inputs with high probability. Compared with [26, 36],
this work assumes a much more realistic setting in which the ad-
versary has fairly limited capability: (i) the compromised DNN
model is only one part of the end-to-end system; (ii) the adversary
has neither knowledge nor control over the system design choices
or tuning strategies; (iii) the adversary has no influence over the
inputs to the system. Ji et al. [30] consider a similar setting, but
with the assumption that the adversary has access to the training
data in both source and target domains. Xiao et al. [63] investigate
the vulnerabilities (e.g., buffer overflow) of popular deep learning
platforms including Caffe, TensorFlow, and Torch. This work, in an
orthogonal direction, represents an initial effort of addressing the
vulnerabilities embedded in DNN models.
10 CONCLUSION
This work represents an in-depth study on the security implications
of using third-party primitive models as building blocks of ML
systems. Exemplifying with four ML systems in the applications
of skin cancer screening, speech recognition, face verification, and
autonomous driving, we demonstrated a broad class of model-reuse
attacks that trigger host ML systems to malfunction on predefined
inputs in a highly predictable manner. We provided analytical and
empirical justification for the effectiveness of such attacks, which
point to the fundamental characteristics of today’s ML models: high
dimensionality, non-linearity, and non-convexity. Thus, this issue
seems fundamental to many ML systems.
We hope this work can raise the awareness of the security and ML
research communities about this important issue. A set of avenues
for further investigation include: First, in this paper, the training
of adversarial models is based on heuristic rules; formulating it
as an optimization framework would lead to more principled and
generic attack models. Second, this paper only considers attacks
based on feature extractors. We speculate that attacks leveraging
multiple primitive models (e.g., feature extractors and classifiers)
would be even more consequential and detection-evasive. Third,
our study focuses on deep learning systems; it is interesting to
explore model-reuse attacks against other types of ML systems
(e.g., kernel machines). Finally, implementing and evaluating the
countermeasures proposed in § 8 in real ML systems may serve as
a promising starting point for developing effective defenses.
REFERENCES
[1] An Open-Source Software Library for Machine Intelligence 2015. https://www.
tensorflow.org.
[2] M. Backes, S. Bugiel, and E. Derr. 2016. Reliable Third-Party Library Detection in
Android and Its Security Applications. In Proceedings of ACM SAC Conference on
Computer and Communications (CCS).
[3] M. Barreno, B. Nelson, A. D. Joseph, and J. D. Tygar. 2010. The Security of
Machine Learning. Mach. Learn. 81, 2 (2010), 121–148.
[4] A. Bendale and T. Boult. 2016. Towards Open Set Deep Networks. In Proceedings
of IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
[5] Y. Bengio, A. Courville, and P. Vincent. 2013. Representation Learning: A Review
and New Perspectives. IEEE Trans. Pattern Anal. Mach. Intell. 35, 8 (2013), 1798–
1828.
[6] R. Bhoraskar, S. Han, J. Jeon, T. Azim, S. Chen, J. Jung, S. Nath, R. Wang, and D.
Wetherall. 2014. Brahmastra: Driving Apps to Test the Security of Third-party
Components. In Proceedings of USENIX Security Symposium (SEC).
[7] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Šrndić, P. Laskov, G. Giacinto,
and F. Roli. 2013. Evasion Attacks Against Machine Learning at Test Time. In
Proceedings of European Conference on Machine Learning and Knowledge Discovery
in Databases (ECML/PKDD).
[8] B. Biggio, G. Fumera, F. Roli, and L. Didaci. 2012. Poisoning Adaptive Biomet-
ric Systems. In Proceedings of Joint IAPR International Workshops on Statistical
Techniques in PR and SSPR.
[9] B. Biggio, B. Nelson, and P. Laskov. 2012. Poisoning Attacks against Support
Vector Machines. In Proceedings of IEEE Conference on Machine Learning (ICML).
[10] B. Biggio and F. Roli. 2018. Wild Patterns: Ten Years after the Rise of Adversarial
Machine Learning. Pattern Recognition 84 (2018), 317–331.
[11] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D.
Jackel, M. Monfort, U. Muller, J. Zhang, X. Zhang, J. Zhao, and K. Zieba. 2016.
End to End Learning for Self-Driving Cars. ArXiv e-prints (2016).
[12] BVLC. 2017. Model Zoo. https://github.com/BVLC/caffe/wiki/Model-Zoo.
[13] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman. 2017. VGGFace2: A
[14] N. Carlini and D. Wagner. 2017. Towards Evaluating the Robustness of Neural
Dataset for Recognising Faces across Pose and Age. ArXiv e-prints (2017).
Networks. In Proceedings of IEEE Symposium on Security and Privacy (S&P).
[15] G. Cauwenberghs and T. Poggio. 2000. Incremental and Decremental Support Vec-
tor Machine Learning. In Proceedings of Advances in Neural Information Processing
Systems (NIPS).
[16] K. Chen, X. Wang, Y. Chen, P. Wang, Y. Lee, X. Wang, B. Ma, A. Wang, Y. Zhang,
and W. Zou. 2016. Following Devil’s Footprints: Cross-Platform Analysis of Po-
tentially Harmful Libraries on Android and iOS. In Proceedings of IEEE Symposium
on Security and Privacy (S&P).
[17] P. Cooper. 2014. Meet AISight: The Scary CCTV Network Completely Run by AI.
http://www.itproportal.com/.
[18] N. Dalvi, P. Domingos, Mausam, S. Sanghai, and D. Verma. 2004. Adversarial
Classification. In Proceedings of ACM International Conference on Knowledge
Discovery and Data Mining (KDD).
[19] J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive Subgradient Methods for Online
Learning and Stochastic Optimization. J. Mach. Learn. Res. 12 (2011), 2121–2159.
[20] A. Esteva, B. Kuprel, R. A. Novoa, J. Ko, S. M. Swetter, H. M. Blau, and S. Thrun.
2017. Dermatologist-Level Classification of Skin Cancer with Deep Neural Net-
works. Nature 542, 7639 (2017), 115–118.
Interpretable Explanations of Black Boxes
by Meaningful Perturbation. In Proceedings of IEEE International Conference on
Computer Vision (ICCV).
[22] M. Fredrikson, S. Jha, and T. Ristenpart. 2015. Model Inversion Attacks That
Exploit Confidence Information and Basic Countermeasures. In Proceedings of
ACM SAC Conference on Computer and Communications (CCS).
[23] GitHub: The World’s Leading Software Development Platform 2008. https://
github.com.
[21] R. C. Fong and A. Vedaldi. 2017.
[28] K. Hornik. 1991. Approximation Capabilities of Multilayer Feedforward Networks.
[27] K. He, X. Zhang, S. Ren, and J. Sun. 2015. Deep Residual Learning for Image
[24] I. Goodfellow, Y. Bengio, and A. Courville. 2016. Deep Learning. MIT Press.
[25] I. J. Goodfellow, J. Shlens, and C. Szegedy. 2014. Explaining and Harnessing
Adversarial Examples. In Proceedings of International Conference on Learning
Representations (ICLR).
[26] T. Gu, B. Dolan-Gavitt, and S. Garg. 2017. BadNets: Identifying Vulnerabilities in
the Machine Learning Model Supply Chain. ArXiv e-prints (2017).
Recognition. ArXiv e-prints (2015).
Neural Netw. 4, 2 (1991), 251–257.
Adversary. ArXiv e-prints (2015).
[30] Y. Ji, X. Zhang, and T. Wang. 2017. Backdoor Attacks against Learning Systems. In
Proceedings of IEEE Conference on Communications and Network Security (CNS).
[31] Y. Ji, X. Zhang, and T. Wang. 2018. EagleEye: Attack-Agnostic Defense against
Adversarial Inputs. ArXiv e-prints (2018).
//www.forbes.com/.
[29] R. Huang, B. Xu, D. Schuurmans, and C. Szepesvari. 2015. Learning with a Strong
[32] B. Kepes. 2015. eBrevia Applies Machine Learning to Contract Review. https:
[33] A. Krizhevsky, I. Sutskever, and G. E. Hinton. 2012.
ImageNet Classification
with Deep Convolutional Neural Networks. In Proceedings of Advances in Neural
Information Processing Systems (NIPS).
[34] Y. LeCun, Y. Bengio, and G. Hinton. 2015. Deep Learning. Nature 521, 7553 (2015),
436–444.
[35] B. Liang, M. Su, W. You, W. Shi, and G. Yang. 2016. Cracking Classifiers for
Evasion: A Case Study on the Google’s Phishing Pages Filter. In Proceedings of
International Conference on World Wide Web (WWW).
[36] Y. Liu, S. Ma, Y. Aafer, W.-C. Lee, J. Zhai, W. Wang, and X. Zhang. 2018. Trojaning
Attack on Neural Networks. In Proceedings of Network and Distributed System
Security Symposium (NDSS).
APPENDIX
10.1 DNNs Used in Experiments
Table 11 summarize the set of deep neural networks (DNNs) used in
§ 6 and § 7. Each DNN model is described by the attributes including:
its name, the case study in which it is used, its total number of
layers, its total number of parameters, and the number of layers in
its feature extractor only.
Case
Study
# Layers
(FE Only)
# Parameters
(End to End)
# Layer
Model
A+A+A
A+V+A
V+A+V
Table 11. Details of DNNs used in experiments.
(6+6+6) + 3
(6+14+6) + 3
(14+6+14) + 3
6+6 +6
6+14+6
14+6+14
46
17
14
23,851,784
17,114,122
145,002,878
170,616,961
248,009,281
325,401,601
Inception.v3
SpeechNet
VGG-Very-Deep-16
48
19
16
I
II
III
IV
IV
IV
10.2 Implementation Details
All the models and algorithms are implemented on TensorFlow [1],
an open source software library for numerical computation using
data flow graphs. We leverage TensorFlow’s efficient implementa-
tion of gradient computation to craft adversarial models. All our
experiments are run on a Linux workstation running Ubuntu 16.04,
two Intel Xeon E5 processors, and four NVIDIA GTX 1080 GPUs.
10.3 Parameter Setting
Setting of λ. The parameter λ controls the magnitude of update
to each parameter. Intuitively, overly small λ may result in an ex-
cessive number of iterations, while overly large λ may cause the
optimization to have non-negligible impact on non-trigger inputs.
We propose a scheme that dynamically adjust λ along running the
algorithm. Specifically, similar to Adagrad [19] in spirit, we adapt
λ to each individual parameter w depending on its importance. At
the j
th iteration, we set λ for a parameter w as:
(cid:113)j
λ =
λ0
ȷ (w))2 + ϵ0
ȷ=1(ϕ+
ȷ (w) denotes ϕ+(w) for the
where λ0 is the initial setting of λ, ϕ+
th iteration, and ϵ0 is a smoothing term to avoid division by zero
ȷ
(which is set as 10−8 by default).
Setting of θ and k. The parameters θ and k are determined empiri-
cally (details in § 6 and § 7).
[37] D. Lowd and C. Meek. 2005. Adversarial Learning. In Proceedings of ACM Inter-
national Conference on Knowledge Discovery and Data Mining (KDD).
[38] J.-H. Luo, J. Wu, and W. Lin. 2017. ThiNet: A Filter Level Pruning Method for Deep
Neural Network Compression. In Proceedings of IEEE International Conference on
Computer Vision (ICCV).
[39] B. Marr. 2017. First FDA Approval For Clinical Cloud-Based Deep Learning In
Healthcare. https://www.forbes.com/.
[40] T. Minka. 2016. A Statistical Learning/Pattern Recognition Glossary. http:
//alumni.media.mit.edu/~tpminka/statlearn/glossary/.
[41] L. Muñoz González, B. Biggio, A. Demontis, A. Paudice, V. Wongrassamee, E. C.
Lupu, and F. Roli. 2017. Towards Poisoning of Deep Learning Algorithms with
Back-gradient Optimization. In Proceedings of ACM Workshop on Artificial Intelli-
gence and Security (AISec).
[42] B. Nelson, B. I. P. Rubinstein, L. Huang, A. D. Joseph, S. J. Lee, S. Rao, and J. D.
Tygar. 2012. Query Strategies for Evading Convex-inducing Classifiers. J. Mach.
Learn. Res. 13 (2012), 1293–1332.
[43] Pannous 2017. Tensorflow Speech Recognition. https://github.com/pannous/
caffe-speech-recognition.
[44] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swamil. 2016.
The Limitations of Deep Learning in Adversarial Settings. In Proceedings of IEEE
European Symposium on Security and Privacy (Euro S&P).
[45] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. 2016. Distillation as a De-
fense to Adversarial Perturbations against Deep Neural Networks. In Proceedings
of IEEE Symposium on Security and Privacy (S&P).
[46] O. M. Parkhi, A. Vedaldi, and A. Zisserman. 2015. Deep Face Recognition. In
Proceedings of the British Machine Vision Conference (BMVC).
[47] J. Pennington, R. Socher, and C. D. Manning. 2014. GloVe: Global Vectors for Word
Representation. In Proceedings of Conference on Empirical Methods in Natural
Language Processing (EMNLP).
[48] R. Polikar, L. Upda, S. S. Upda, and V. Honavar. 2001. Learn++: An Incremental
Learning Algorithm for Supervised Neural Networks. Trans. Sys. Man Cyber Part
C 31, 4 (2001), 497–508.
[49] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpa-
thy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. 2015. ImageNet Large
Scale Visual Recognition Challenge. Int. J. Comput. Vision 115, 3 (2015), 211–252.
[50] A. Satariano. 2017. AI Trader? Tech Vet Launches Hedge Fund Run by Artificial
Intelligence. http://www.dailyherald.com/.
[51] W. J. Scheirer, L. P. Jain, and T. E. Boult. 2014. Probability Models for Open Set
Recognition. IEEE Trans. Patt. An. Mach. Intell. 36, 11 (2014), 2317–2324.
[52] E. J. Schwartz, T. Avgerinos, and D. Brumley. 2010. All You Ever Wanted to Know
About Dynamic Taint Analysis and Forward Symbolic Execution (but Might
Have Been Afraid to Ask). In Proceedings of IEEE Symposium on Security and
Privacy (S&P).
[53] D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner, V. Chaudhary, M.
Young, J.-F. Crespo, and D. Dennison. 2015. Hidden Technical Debt in Machine
Learning Systems. In Proceedings of Advances in Neural Information Processing
Systems (NIPS).
[54] K. Simonyan and A. Zisserman. 2014. Very Deep Convolutional Networks for
Large-Scale Image Recognition. ArXiv e-prints (2014).
[55] Y. Sun, X. Wang, and X. Tang. 2014. Deep Learning Face Representation from
Predicting 10,000 Classes. In Proceedings of IEEE Conference on Computer Vision
and Pattern Recognition (CVPR).
[56] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Van-
houcke, and A. Rabinovich. 2015. Going Deeper with Convolutions. In 2015 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR).
[57] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. 2015. Rethinking the
Inception Architecture for Computer Vision. ArXiv e-prints (2015).
[58] F. Tramèr, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart. 2016. Stealing
Machine Learning Models via Prediction APIs. In Proceedings of USENIX Security
Symposium (SEC).
[59] A. Versprille. 2015. Researchers Hack into Driverless Car System, Take Control
of Vehicle. http://www.nationaldefensemagazine.org/.
[60] P. Warden. 2017. Speech Commands: A Public Dataset for Single-Word Speech
Recognition. http://download.tensorflow.org/data/speech_commands_v0.01.tar.
gz.
[61] H. Xiao, B. Biggio, G. Brown, G. Fumera, C. Eckert, and F. Roli. 2015. Is Fea-
ture Selection Secure against Training Data Poisoning?. In Proceedings of IEEE
Conference on Machine Learning (ICML).
[62] H. Xiao, B. Biggio, B. Nelson, H. Xiao, C. Eckert, and F. Roli. 2015. Support Vector
Machines under Adversarial Label Contamination. Neurocomput. 160, C (2015),
53–62.
[63] Q. Xiao, K. Li, D. Zhang, and W. Xu. 2017. Security Risks in Deep Learning
Implementations. ArXiv e-prints (2017).
[64] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. 2017. Understanding
Deep Learning Requires Rethinking Generalization. In Proceedings of International
Conference on Learning Representations (ICLR).