title:Back to the Drawing Board: A Critical Evaluation of Poisoning Attacks
on Production Federated Learning
author:Virat Shejwalkar and
Amir Houmansadr and
Peter Kairouz and
Daniel Ramage
7
4
6
3
3
8
9
.
2
2
0
2
.
4
1
2
6
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
2
2
0
2
©
0
0
.
1
3
$
/
2
2
/
9
-
6
1
3
1
-
4
5
6
6
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
2
2
0
2
2022 IEEE Symposium on Security and Privacy (SP)
Back to the Drawing Board:
A Critical Evaluation of Poisoning Attacks on
Production Federated Learning
Virat Shejwalkar∗, Amir Houmansadr∗, Peter Kairouz†, Daniel Ramage†
∗{vshejwalkar, amir}@cs.umass.edu, †{kairouz, dramage}@google.com
∗University of Massachusetts Amherst
†Google Research
Abstract—While recent works have indicated that federated
learning (FL) may be vulnerable to poisoning attacks by com-
promised clients, their real impact on production FL systems
is not fully understood. In this work, we aim to develop a
comprehensive systemization for poisoning attacks on FL by
enumerating all possible threat models, variations of poisoning,
and adversary capabilities. We specifically put our focus on un-
targeted poisoning attacks, as we argue that they are significantly
relevant to production FL deployments.
We present a critical analysis of untargeted poisoning at-
tacks under practical, production FL environments by carefully
characterizing the set of realistic threat models and adversarial
capabilities. Our findings are rather surprising: contrary to the
established belief, we show that FL is highly robust in practice
even when using simple, low-cost defenses. We go even further
and propose novel, state-of-the-art data and model poisoning
attacks, and show via an extensive set of experiments across
three benchmark datasets how (in)effective poisoning attacks are
in the presence of simple defense mechanisms. We aim to correct
previous misconceptions and offer concrete guidelines to conduct
more accurate (and more realistic) research on this topic.
I. INTRODUCTION
Federated learning (FL) is an emerging learning paradigm
in which data owners (called clients) collaborate in training a
common machine learning model without sharing their private
training data. In this setting, a central server (e.g., a service
provider) repeatedly collects some updates that the clients
compute using their local private data, aggregates the clients’
updates using an aggregation rule (AGR), and finally uses
the aggregated client updates to tune the jointly trained model
(called the global model), which is broadcasted to a subset
of the clients at
the end of each FL training round. FL
is increasingly adopted by various distributed platforms, in
particular by Google’s Gboard [1] for next word prediction,
by Apple’s Siri [49] for automatic speech recognition, and by
WeBank [62] for credit risk predictions.
The threat of poisoning FL: A key feature that makes FL
highly attractive in practice is that it allows training models in
collaboration between mutually untrusted clients, e.g., Android
users or competing banks. Unfortunately, this makes FL sus-
ceptible to a threat known as poisoning: a small fraction of FL
clients, called compromised clients, who are either owned or
controlled by an adversary, may act maliciously during the FL
training process in order to corrupt the jointly trained global
model. Specifically, the goal of the poisoning adversary is to
attack FL by instructing its compromised clients to contribute
poisoned model updates during FL training in order to poison
the global model.
There are three major approaches to poisoning FL: targeted,
backdoor, and untargeted poisoning; Figure 1 briefly illus-
trates them. The goal of this work is to understand the sig-
nificance of poisoning attacks to production FL systems [11],
[32], and to reevaluate the need for sophisticated techniques
to defend against FL poisoning. We choose to focus on
untargeted FL poisoning as we find it to be significantly
relevant to production deployments: it can be used to impact
a large population of FL clients and it can remain undetected
for long duration. As we focus on untargeted poisoning, in the
rest of this paper “poisoning” refers to untargeted poisoning,
unless specified otherwise.
The literature on FL poisoning attacks and defenses: Re-
cent works have presented various techniques (Section IV-A)
to poison FL [5], [23], [55]. Their core idea is to generate
poisoned updates (either by direct manipulation of model
updates, called model poisoning [3], [5], [7], [23], [55], or
through fabricating poisoned data, called data poisoning [59],
[61]) that deviate maximally from a benign direction, e.g.,
the average of benign clients’ updates, and at
the same
time circumvent
the given robust AGR, i.e., by bypassing
its detection criteria. To protect FL against such poisoning
attacks, the literature has designed various robust aggregation
rules [10], [15], [23], [41], [55], [70] (Section II-B) which aim
to remove or attenuate the updates that are more likely to be
malicious according to some criterion.
The gap between the literature and practice: The existing
literature on poisoning attacks and defenses for FL makes
unrealistic assumptions that do not hold in real-world FL
deployments, e.g., assumptions about the percentages of com-
promised clients, total number of FL clients, and the types of
FL systems [32]. For instance, state-of-the-art attacks [5], [23],
[55] (defenses [10], [16], [68], [70]) assume adversaries who
can compromise up to 25% (50%) of FL clients. For an app
like Gboard with ∼ 1B installations [32], 25% compromised
clients would mean an attacker controls 250 million Android
devices! We argue that, although interesting from theoretical
perspectives, the assumptions in recent FL robustness works
© 2022, Virat Shejwalkar. Under license to IEEE.
DOI 10.1109/SP46214.2022.00065
1354
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:01:32 UTC from IEEE Xplore.  Restrictions apply. 
Figure 1: Classes of FL poisoning attacks and their objectives defined using the taxonomy in Section III-A1: Targeted attacks [7],
[58] aim to misclassify only a specific set/classes of inputs (e.g., certain 10 samples from CIFAR10), semantic backdoor
attacks [3], [61] aim to misclassify inputs with specific properties (e.g., cars with stripes in background), artificial backdoor
attacks [67] aim to misclassify inputs with an artificial (visible or invisible) trigger pattern (e.g., shape of letter ”F”), and
untargeted attacks [23], [55] aim to reduce model accuracy on arbitrary inputs (e.g., the entire CIFAR10 distribution).
do not represent common real-world adversarial scenarios that
account for the difficulty and cost of at-scale compromises.
Our contributions:
In this work, we perform a critical
analysis of the literature on FL robustness against (untargeted)
poisoning under practical considerations. Our ultimate goal is
to understand the significance of poisoning attacks and the
need for sophisticated robust FL algorithms in production FL.
Specifically, we make the following key contributions:
I. Systemization of FL poisoning threat models. We start by
establishing a comprehensive systemization of threat models of
FL poisoning. Specifically, we discuss three key dimensions of
the poisoning threat to FL: The adversary’s objective, knowl-
edge, and capability. We discuss the practicality of all possible
threat models obtained by combining these dimensions. As we
will discuss, out of all possible combinations, only two threat
models, i.e., nobox offline data poisoning and whitebox online
model poisoning, are of practical value to production FL. We
believe that prior works [5], [10], [23], [55] have neglected the
crucial constraints of production FL systems on the parameters
relevant to FL robustness. Our work is the first to consider
production FL environments [11], [32] and provide practical
ranges for various parameters of poisoning threat models. As a
result, our evaluations lead to conclusions that contradict the
common beliefs in the literature, e.g., we show that production
FL even with the non-robust Average AGR is significantly
more robust than previously thought.
II. Introducing improved poisoning attacks.
First, we
overview all of the existing untargeted poisoning attacks
on FL that consider the two aforementioned threat models
(Section IV-A). Then, we design improved attacks for these
threat models. 1) Our improved data poisoning attacks: We
present the first attacks that systematically consider the data
poisoning threat model for FL (Section IV-B2). We build on
the classic label flipping data poisoning attack [45], [64],
[65] designed for centralized ML. Our data poisoning attacks
rely on the observation that increasing the amount of label
flipped data increases the loss and norm of the resulting
updates, and therefore, can produce poisoned updates that
can effectively reduce the global model’s accuracy. However,
using arbitrarily large amounts of label flipped data may result
in updates that cannot circumvent the robustness criterion of
the target AGR. Hence, to circumvent the target AGR, we
propose to adjust the amount of label flipped data used. 2)
Our improved model poisoning attacks: We propose novel
model poisoning attacks that outperform the state-of-the-art.
Our attacks (Section IV-B3) use gradient ascent to fine-tune
the global model and increase its loss on benign data. Then,
they adjust the L2-norm of the corresponding poisoned update
in order to circumvent robustness criterion of the target AGR.
III. Analysis of FL robustness in practice. We extensively
evaluate all existing poisoning attacks as well as our own
improved attacks across three benchmark datasets, for various
FL parameters, and for different types of FL deployments.
We make several significant deductions about the state of FL
poisoning literature for production FL. For production cross-
device FL, which contains thousands to billions of clients,
following are our key lessons:
(1) For practical percentages of compromised clients (M),
even the most basic, non-robust FL algorithm, i.e., Average
AGR, converges with high accuracy, i.e., it is highly robust. For