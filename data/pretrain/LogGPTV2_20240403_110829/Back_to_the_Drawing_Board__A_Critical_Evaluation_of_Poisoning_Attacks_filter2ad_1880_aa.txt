# Back to the Drawing Board: A Critical Evaluation of Poisoning Attacks on Production Federated Learning

**Authors:**
- Virat Shejwalkar
- Amir Houmansadr
- Peter Kairouz
- Daniel Ramage

**Affiliations:**
- *University of Massachusetts Amherst*: Virat Shejwalkar, Amir Houmansadr
- *Google Research*: Peter Kairouz, Daniel Ramage

**Abstract:**
Recent studies have highlighted the vulnerability of federated learning (FL) to poisoning attacks by compromised clients. However, the real-world impact on production FL systems remains unclear. In this work, we aim to provide a comprehensive systemization of poisoning attacks on FL by enumerating all possible threat models, variations of poisoning, and adversary capabilities. We focus specifically on untargeted poisoning attacks, as they are particularly relevant to production FL deployments.

We present a critical analysis of untargeted poisoning attacks in practical, production FL environments by characterizing realistic threat models and adversarial capabilities. Our findings are surprising: contrary to established beliefs, FL is highly robust in practice, even with simple, low-cost defenses. We further propose novel, state-of-the-art data and model poisoning attacks and evaluate their effectiveness through extensive experiments across three benchmark datasets. Our results aim to correct previous misconceptions and offer concrete guidelines for more accurate and realistic research on this topic.

## I. Introduction

Federated learning (FL) is an emerging paradigm where data owners (clients) collaboratively train a common machine learning model without sharing their private training data. In this setting, a central server collects updates from clients, aggregates them using an aggregation rule (AGR), and tunes the global model, which is then broadcasted back to the clients. FL is increasingly adopted by various platforms, including Google’s Gboard for next-word prediction, Apple’s Siri for automatic speech recognition, and WeBank for credit risk predictions.

### The Threat of Poisoning in FL

A key feature of FL is its ability to train models collaboratively among mutually untrusted clients. However, this makes FL susceptible to poisoning attacks, where a small fraction of compromised clients, controlled by an adversary, can corrupt the global model. The goal of a poisoning attack is to instruct these compromised clients to contribute malicious updates during FL training to degrade the global model's performance.

There are three main types of poisoning attacks: targeted, backdoor, and untargeted. This work focuses on untargeted poisoning, as it is highly relevant to production deployments. Untargeted poisoning can affect a large population of clients and remain undetected for extended periods. For the remainder of this paper, "poisoning" refers to untargeted poisoning unless otherwise specified.

### Literature on FL Poisoning Attacks and Defenses

Recent works have proposed various techniques to poison FL, such as model poisoning and data poisoning. These attacks aim to generate updates that deviate significantly from benign directions while bypassing robust AGRs. To counteract these attacks, researchers have developed robust aggregation rules designed to remove or attenuate potentially malicious updates.

### The Gap Between Literature and Practice

The existing literature on poisoning attacks and defenses often makes unrealistic assumptions that do not hold in real-world FL deployments. For example, some studies assume adversaries can compromise up to 25% of FL clients, which is impractical for large-scale applications like Gboard, with over 1 billion installations. These assumptions, while theoretically interesting, do not reflect common real-world scenarios.

## II. Contributions

In this work, we perform a critical analysis of the literature on FL robustness against untargeted poisoning under practical considerations. Our primary goal is to understand the significance of poisoning attacks and the need for sophisticated robust FL algorithms in production settings. Specifically, we make the following key contributions:

1. **Systemization of FL Poisoning Threat Models:**
   - We establish a comprehensive systemization of threat models for FL poisoning, focusing on the adversary's objective, knowledge, and capability.
   - We discuss the practicality of all possible threat models and identify two that are most relevant to production FL: no-box offline data poisoning and white-box online model poisoning.
   - Our work is the first to consider production FL environments and provide practical ranges for various parameters of poisoning threat models. Our evaluations show that production FL, even with non-robust Average AGR, is significantly more robust than previously thought.

2. **Improved Poisoning Attacks:**
   - We overview existing untargeted poisoning attacks and design improved attacks for the two identified threat models.
   - **Data Poisoning Attacks:** We build on classic label flipping attacks, adjusting the amount of label-flipped data to circumvent robust AGRs.
   - **Model Poisoning Attacks:** We propose novel attacks that use gradient ascent to fine-tune the global model and increase its loss on benign data, while adjusting the L2-norm of the poisoned update to bypass AGRs.

3. **Analysis of FL Robustness in Practice:**
   - We extensively evaluate existing and improved poisoning attacks across three benchmark datasets, various FL parameters, and different deployment types.
   - For production cross-device FL, our key findings include:
     - Even with practical percentages of compromised clients, the basic, non-robust Average AGR converges with high accuracy, indicating significant robustness.
     - Simple defense mechanisms are effective in mitigating the impact of poisoning attacks.

Our work aims to provide a more accurate and realistic understanding of FL robustness against poisoning attacks and guide future research in this area.