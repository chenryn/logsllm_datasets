8.11
8.23
0.916
0.905
8.69
0.890
8.18
0.901
8.34
0.905
8.88
8.34
0.906
JC
0.736
0.736
0.731
0.720
0.727
0.743
0.759
0.764
0.758
0.767
0.771
0.731
0.751
0.752
0.774
0.787
Porn Detection
SS
0.884
0.895
0.874
0.846
0.843
0.856
0.876
0.879
0.858
0.885
0.865
0.854
0.853
0.863
0.878
0.880
JC
0.722
0.737
0.725
0.713
0.712
0.711
0.756
0.748
0.738
0.754
0.743
0.720
0.717
0.716
0.750
0.742
ED
9.09
8.77
8.89
10.3
10.4
10.2
9.31
10.6
8.07
7.24
7.58
8.05
9.87
8.71
9.61
9.22
bugs. It is obviously observed that Insert and PyConvert are
the dominant variation strategies used by real-world mali-
cious users, and Sim2Trad is used less than others. This is
probably because that Insert and PyConvert are easy to craft
while Sim2Trad can be easily defended by text preprocessing
adopted in online toxic content detection services. From the
visualized success rate, we can see that above 60% of Insert
bugs can be corrected by the NMT model. Comparatively, the
corrected PhoneticSim bugs in the abusive texts and porno-
graphic texts account for less than 20% and 30%, respectively.
This indicates that the NMT model is robust against the Insert
bugs while less robust against the PhoneticSim bugs. From
Table 4 and Fig. 5, we can also see that the DLTC models with
the combined defense have a high accuracy even though bugs
still exist in the translations. We argue that not all bugs need
to be corrected and the obfuscated text can still be correctly
classiﬁed by the multimodal DLTC model as long as one or
two bugs are corrected.
5.3 Evaluation of Robustness
Next, we evaluate the robustness of TEXTSHIELD against
adversarial attack from the perspectives of transfer attack and
adaptive attack. In this evaluation, the adversarial texts are
generated by TextBugger with the maximum perturbation of
4 and the semantic similarity threshold of 0.75, considering
that the average text length of our datasets is about 40.
Robustness against Transfer Attack. One intriguing
property of adversarial inputs is their transferability, i.e., ad-
versarial input generated against one model can also trick
another model with different architectures and trained on dif-
1390    29th USENIX Security Symposium
USENIX Association
0255075100125150175200Length0.00.20.40.60.81.0CDF0255075100125150175200Length0.00.20.40.60.81.0CDF020406080100120140Length0.00.20.40.60.81.0CDF020406080100120140Length0.00.20.40.60.81.0CDFferent data [7, 13]. Speciﬁcally, we ﬁrst randomly sample a
set of texts from each dataset, in which each text is correctly
classiﬁed by the corresponding model. Then, we generate
1,000 successful adversarial texts against each common model
from the sampled texts and transfer them to the models with
defense. The main results are summarized in Table 6. Ob-
serve that the transferability against the models shielded by
TEXTSHIELD is fairly low: it achieves an attack success rate
lower than 0.09 against the TextCNN models that combined
with IMF and NMT, and achieves an attack success rate lower
than 0.075 against the BiLSTM models combined with EMF
and NMT. In comparison, Pycorrector and TextCorrector
have little effectiveness in mitigating transfer attack. We can
therefore conclude that TEXTSHIELD is more robust against
the transferred adversarial texts.
Robustness against Adaptive Attack. So far, we have
only considered static adversaries, who only generate adver-
sarial texts against the DLTC models without adapting to
attack TEXTSHIELD directly. In this evaluation, we consider
the challenges TEXTSHIELD may face when adversaries know
our defense. Speciﬁcally, we ﬁrst randomly sample 2,000 cor-
rectly classiﬁed texts from the test and validation sets for
each target model. Then, the DLTC model and TEXTSHIELD
would be viewed as a whole pipeline by attackers when ex-
ploring model sensitivity. This is a more realistic worst-case
setting, since attackers (e.g., malicious netizens) usually only
have black-box access to the whole pipeline in the real adver-
sarial scenario [5]. Under this setting, attackers can not only
capture the vulnerability of the DLTC model, but also capture
the vulnerability of the multimodal embedding and the adver-
sarial NMT model. Finally, adversarial texts are generated
from sampled benign texts by TextBugger according to the
explored sensitivity information.
(1) Attack Success Rate. The attack performance against
all the target models are reported in Table 7. Obviously, it
can be seen that the adaptive attack achieves a lower success
rate against the models with TEXTSHIELD compared to the
common models. Particularly, when only applying multi-
modal embedding, the defense not only degrades the success
rate but also increases the cost of the attack in terms of the
average number of perturbed words and queries. However,
when leveraging the combined defense scheme, an anomalous
result is observed that although the defense signiﬁcantly de-
creases the attack success rate, the required cost of attack also
degrades in most cases. We conjecture that such anomalous
result may stem from the poor classiﬁcation robustness of the
models with the combined defense on the benign texts used
for generating these successful adversarial texts. To verify
our conjecture, we visualize the cumulative distribution of
the text length of these benign texts in Fig. 6. Observe that
the original length of the texts successfully generated against
the models with the combined defense is signiﬁcant longer
than those generated for the common models. However, all
the models especially the NMT models usually perform rela-
tively worse on longer texts due to the problem of long-term
dependencies [4], thus making it easier to trigger the DLTC
models to misbehave with less cost on longer texts.
(2) Utility Analysis. The main results of utility evaluation
are summarized in Table 8. It can be seen that the adversarial
texts generated against the common models and the models
with Pycorrector and TextCorrector preserve good utility in
terms of both character-level and semantics-level. Also, simi-
lar trend is observed that multimodal embedding reduces the
utility of adversarial texts across all metrics, while the util-
ity of the adversarial texts generated against TEXTSHIELD
seems higher in several cases. This also stems from the im-
pact of longer text length. Intuitively, longer text is more
fault-tolerant and can preserve more of the original utility
when slightly perturbed.
(3) The Impact of Maximum Perturbation. We further
study the impact of the maximum number of required per-
turbations on the attack success rate of adaptive attack. The
main results are shown in Fig. 7. It is clearly seen that the
attack success rate against all the target models increases as
the maximum number of perturbations grows. Particularly,
the success rate against the common models increases rapidly
with the increasing maximum perturbation while increases
slightly against the models with TEXTSHIELD, thus resulting
in a growing gap between the success rate. In addition, the
success rate against the models with TEXTSHIELD is still
below 0.3 when perturbed with the maximum perturbation of
5. We thus conclude that TEXTSHIELD is very robust against
the adaptive attacks and outperforms the baselines.
(4) Model Sensitivity Analysis. We also investigate the
sensitivity of the target models against each bug replacement
by visualizing the cumulative distribution of sensitivity score
in Fig. 8. The sensitivity score represents the reward for
each bug replacement, i.e., the reduction in toxic conﬁdence.
Observe from the two tasks that the sensitivity score of the
models defended by TEXTSHIELD are markable smaller than
those of the common models, especially when the DLTC
model is shielded by the combined defense. For instance,
for abuse detection, nearly 100% of bug replacement that
against the BiLSTM with the combined defense only gains
a reward lower than 0.2 while more than 40% of bug re-
placement against the common BiLSTM obtains a reward
higher than 0.4. This demonstrates that all defense schemes
in TEXTSHIELD have high resistance to adversarial bugs and
thus help mitigate the sensitivity of the DLTC models.
(5) Bug Distribution. We take the TextCNN models used
for abuse detection as examples to further study their sensitiv-
ity to different bugs, and the analysis for other models can be
found in Appendix C. The distributions of bugs in adversarial
texts against the common models and the models shielded by
TEXTSHIELD are visualized in Fig. 9 , where the x-axis repre-
sents the proportion of bugs for the common model and y-axis
represents the proportion for the model with defense, and the
marker size represents the rate of the bugs being successfully
USENIX Association
29th USENIX Security Symposium    1391
(a) TextCNN on Abuse
(b) BiLSTM on Abuse
(c) TextCNN on Porn
(d) BiLSTM on Porn
Figure 7: The impact of maximum perturbation on attack success rate. CNN and LSTM represent TextCNN and BiLSTM.
(a) TextCNN on Abuse
(b) BiLSTM on Abuse
(c) TextCNN on Porn
(d) BiLSTM on Porn
Figure 8: The sensitivity of the target models against bug replacement.
Figure 9: The sensitivity of the target TextCNN models against different bugs in abuse detection.
defended. It is clearly observed that Insert is used less than
the others, indicating that this kind of bug is less powerful.
In comparison, both the common model and the models with
defense are more sensitive to PyConvert, especially PyCon-
vert/2 and PyConvert/5. One reason is that the DLTC models
in our experiment work at the character-level while the num-
ber of characters in Pinyin is usually several times that of
Chinese characters, hence PyConvert may lead to errors in
the feature extraction stage. In addition, there are many sec-
ondary variations based on the converted Pinyin which result
in the sparseness of adversarial perturbations, making it difﬁ-
cult for the proposed defense to cover all possible variations.
Meanwhile, TEXTSHIELD shows its robustness in defending
against some of the PyConvert bugs as well as the Glyph-
Sim and PhoneticSim bugs. As a future work, we will focus
on more robust defense to deal with the stubborn bugs like
PyConvert/2 and PyConvert/5. For example, we would ﬁrst
restore the secondary variations to their original format and
then restore the original Pinyin to the corresponding Chinese
characters by designing some preprocessing schemes.
5.4 Comparison with Online Services
Now, we make comparison with four industry-leading toxic
content detection services, i.e., Alibaba GreenNet, Baidu
TextCensoring, Huawei Moderation and Netease Yidun, who
have claimed to be successful in handling the glyph-based
and phonetic-based variations, to show the practicality of
TEXTSHIELD. We generate adversarial texts with TextBugger
under the same setting to evaluate their robustness.
The comparison results are reported in Table 9. Observe
that most of these services achieve relatively good detec-
tion accuracy under the non-adversarial setting. However,
it is also observed that they are still highly vulnerable to
the generated adversarial texts. Speciﬁcally, they are tricked
with higher attack success rate (i.e., above 0.814 across all
cases) and less words perturbed than the models shielded by
TEXTSHIELD, which indicate that the defenses integrated in
1392    29th USENIX Security Symposium
USENIX Association
12345Maximum Perturbation0.00.20.40.60.81.0Success Rate12345Maximum Perturbation0.00.20.40.60.81.0Success Rate12345Maximum Perturbation0.00.20.40.60.81.0Success Rate12345Maximum Perturbation0.00.20.40.60.81.0Success Rate0.00.20.40.60.81.0Sensitivity Score0.20.40.60.81.0CDF0.00.20.40.60.81.0Sensitivity Score0.20.40.60.81.0CDF0.00.20.40.60.81.0Sensitivity Score0.20.40.60.81.0CDF0.00.20.40.60.81.0Sensitivity Score0.20.40.60.81.0CDF0.00.20.4TextCNN0.00.10.20.3TextCNN+EMF0.00.20.4TextCNN0.000.050.100.15TextCNN+IMF0.00.20.4TextCNN0.000.050.100.150.200.25TextCNN+EMF+NMT0.00.20.4TextCNN0.000.050.100.150.20TextCNN+IMF+NMTTable 9: Comparison with real-world online detection services.
Abuse Detection
Porn Detection
Targeted API
Alibaba GreenNet
Baidu TextCensoring
Huawei Moderation
Netease Yidun
TextCNN + IMF + NMT
BiLSTM + EMF + NMT
Accuracy
0.778
0.763
0.704
0.805
0.880
0.840
ASR
0.868
0.938
0.888
0.903
0.219
0.268
Perturbed Word
1.34
1.36
1.34
1.38
1.93
1.85
Table 10: The results of adaptive attacks against English-
based DLTC models with TEXTSHIELD.
Model
Common TextCNN
TextCNN + EMF + NMT
TextCNN + IMF + NMT
Common BiLSTM
BiLSTM + EMF + NMT
BiLSTM + IMF + NMT
Accuracy ASR Perturbed Word Query
36.7
37.5
36.4
38.4
37.7
36.1
0.754
0.757
0.752
0.766
0.751
0.763
0.880
0.283
0.265
0.782
0.351
0.285
1.60
1.53
1.38
1.80
1.54
1.26
these services can still be ruined by adversarial attacks. In
contrast, TEXTSHIELD shows great practicality for reducing
the attack success rate as well as improving the cost of the
attack. Interestingly, we ﬁnd that although Netease Yidun and
Baidu TextCensoring outperform others in abuse detection
and porn detection tasks, respectively, they are also more
vulnerable to adversarial texts. We thus conclude that the
robustness of DLTC systems is independent of their accuracy,
i.e., the model with high accuracy is not necessarily secure.
5.5 Evaluation of Generalizability
Finally, we study extending TEXTSHIELD to English-based
DLTC models to examine its generalizability across lan-
guages. The experiment is conducted on the classical sen-
timent analysis task with the benchmark Rotten Tomatoes
Movie Reviews dataset [36] under the same adaptive setting.
Since the pronunciation of an English word is related to its
spelling, we only learn the word embeddings from two modal-
ities, i.e., semantics and glyphs, and all the models are trained
from scratch without any complex tricks. Finally, adversarial