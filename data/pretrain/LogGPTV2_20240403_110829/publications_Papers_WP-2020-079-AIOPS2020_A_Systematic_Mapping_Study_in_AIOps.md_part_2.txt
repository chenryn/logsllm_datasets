### Preliminary Filtering and Ranking-based Selection

In the filtering step, we begin by improving the quality of our paper selection. Initially, papers are automatically excluded based on the publication venue, particularly those that are clearly irrelevant to the topic (e.g., meteorology). We also exclude papers published before 1990, as they precede the advent of large-scale IT services. This process allows us to eliminate approximately 8,000 papers.

Typically, at this stage, a full-text analysis would be performed on all available papers to screen relevant contributions using the above-mentioned selection rules. However, even with partial filtering, it is not feasible to perform an exhaustive selection analysis, even as simple as filtering by title. Automated content-based selection is also impractical due to the lack of clear methods for efficient, high-recall, and high-precision text classification without supervision.

To address this, we apply a ranking procedure to the intermediate results, allowing us to prioritize the investigation of more relevant papers. The exclusion and inclusion rules from Section ?? are applied to the papers in the ranked order.

This approximate procedure raises the question of when to stop the selection and discard the remaining items. To solve this, we develop a new approach based on the following assumption: a considerable ratio of relevant papers can be identified by ranking and selecting top results using different relevance criteria (e.g., conference, position index in the query result set, number of hits in all queries). However, we observe a long-tail distribution of relevant documents, meaning some relevant papers appear in the last positions even after sorting with our relevance heuristics (see Figure 1). This aligns with the known challenge of performing exhaustive systematic literature reviews and mapping studies, where completing the long tail provides fewer results at the expense of a larger research effort.

We assume the ratio of relevant papers in the long tail to be constant and comparable to the number of relevant papers when sampled randomly from the result set. Based on this assumption, we proceed as follows:
- We start screening all papers in the result set, ranked according to different relevance heuristics (e.g., number of hits in queries), and observe the ratio of relevant papers identified over time.
- We examine the same papers in random order and measure the same ratio.
- When the two ratios are comparable, we assert that we have reached the tail of the distribution of relevant papers and stop examining and selecting new papers.

As sorting criteria, we use the number of hits in the search performed in the previous step, as well as other more complex heuristics, taking into account the index position in result sets and the number of citations. When examining a paper, we look into the full content to identify concepts related to our selection criteria. Relevant papers are grouped together. Using this stopping criterion, we conclude this selection step when we have identified 430 relevant papers.

### 2.5 Additional Search Techniques

The "early stopping" criterion described above, while enabling a feasible and comprehensive selection strategy across thousands of contributions, has a natural tendency to discard relevant papers. Additionally, we expect to miss other relevant papers not present in the initial set of 83,817 because they were not identified by our database search. To address these limitations, we apply additional search techniques beyond the database search. Unlike before, we apply our selection criteria exhaustively for each document retrieved.

#### Reference Search
For each of the 430 relevant papers identified in the previous step, we search inside their cited references. Specifically, we adopt backward snowball sampling [18], including in our relevant set all papers previously cited by a relevant paper if they meet the selection requirements. This method yields 631 relevant elements, for a total of 1,061.

#### Conference Search
Reference search helps identify prominent contributions frequently mentioned by other authors but introduces bias towards specific research groups and authors. It also rewards specific tasks and research fields that are typically more cited. To compensate for these biases, we perform a manual search by inspecting papers published in relevant conferences. These conferences are identified via correlation with other relevant papers and confirmed by experts in the field. We look at the latest three editions of each conference to balance the sampling of dated papers. This method yields five additional papers.

#### Iterative Search Improvement
To conclude our search, we improve our initial guess on IT Operations keywords by analyzing the available text content (text and abstract). Using our relevant paper set as positive samples, we perform a statistical analysis to identify k-shingles (sets of k consecutive tokens) that appear frequently in relevant documents (Table 2). We measure the document relevance probability given the set of shingles observed in the available text content. We choose k = 1, ..., 5. These shingles are used as keywords to construct new queries along with previously used AI keywords. We limit the collection to 20 results per query. This step identifies 20 new relevant papers and provides insights into frequently cited concepts and keywords in AIOps, which are useful for taxonomy and classification.

### 2.6 Data Extraction and Categorization

After obtaining the result set of 1,086 relevant papers, we analyze the available information to draw quantitative results and answer our research questions. Here, we describe the data extraction process and the analysis techniques employed to gather insights and trends for the AIOps field.

First, we classify the relevant papers according to target components and data sources. Target components indicate a high-level piece of software or hardware in an IT system that the document tries to enhance (e.g., hard drive for disk failure prediction). We group components into five high-level categories: code, application, hardware, network, and datacenter. Data sources provide an indication of the input information for the algorithm (such as logs, metrics, or execution traces). Data sources are categorized as source code, testing resources, system metrics, key performance indicators (KPIs), network traffic, topology, incident reports, logs, and traces. The "AI Method" axis denotes the actual algorithm employed, with similar methods aggregated into broader classes to avoid excessive fragmentation (e.g., clustering may include both k-means and agglomerative hierarchical clustering approaches). Table 3 presents a selection of papers from the result set with the corresponding target, source, and category annotations.

Next, we use the result set to infer a taxonomy based on tasks and target goals. The taxonomy is depicted in Figure 2. We divide AIOps contributions into failure management (FM), which deals with undesired behavior in the delivery of IT services, and resource provisioning, which studies the allocation of energetic, computational, storage, and time resources for optimal IT service delivery. Within each macro-area, we further distinguish approaches based on the similarity of goals. In failure management, these categories are failure prevention, online failure prediction, failure detection, root cause analysis (RCA), and remediation. In resource provisioning, we divide contributions into resource consolidation, scheduling, power management, service composition, and workload estimation. We further expand our analysis of FM (red box in Figure 2) by applying an additional subcategorization based on specific problems. Examples of subcategories include checkpointing for failure prevention and fault localization for root cause analysis (see also Table 3).

### 3 Results

We now discuss the results of our mapping study. First, we analyze the distribution of papers in our taxonomy. The left side of Figure 3 visualizes the distribution of identified papers by macro-area and category. Excluding papers treating AIOps in general (8), we observe that the majority of items (670, 62.1%) are associated with failure management (FM), with most contributions concentrated in online failure prediction (26.4%), failure detection (33.7%), and root cause analysis (26.7%). The remaining resource provisioning papers support resource consolidation, scheduling, and workload prediction. On the right side, we see that the most common problems in FM are software defect prediction, system failure prediction, anomaly detection, fault localization, and root cause diagnosis.

To analyze temporal trends within the AIOps field, we measured the number of publications in each category by year of publication. The corresponding bar plot is depicted in Figure 4. Overall, we observe a large, growing number of publications in AIOps. Failure detection has gained particular traction in recent years (71 publications for the 2018-2019 period), with a contribution size larger than the entire resource provisioning macro-area (69 publications in the same time frame). Failure detection is followed by root cause analysis (39) and online failure prediction (34), while failure prevention and remediation are the areas with the smallest number of contributions (11 and 5, respectively).

### 4 Conclusion

In this paper, we presented our contribution towards better structuring the AIOps field. We planned and conducted a systematic mapping study, providing a detailed taxonomy and analysis of the AIOps literature. Our findings highlight the growing importance of failure management, particularly in failure detection, and offer insights into the temporal trends and key areas of focus within the AIOps domain.