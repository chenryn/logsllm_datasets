Preliminary Filtering and Ranking-based Selection In the filtering step
westartimprovingthequalityofourselectionofpapers.First,papersareauto-
matically excluded based on publication venue, for those venues that are clearly
irrelevantfortopicreasons(e.g.meteorology).Wealsoexcludebasedontheyear
of publication (year <1990) as it precedes the advent of large-scale IT services.
By doing so, we can exclude approximately 8000 elements.
Usuallyatthispoint,afull-textanalysiswouldbeperformedonalltheavail-
ablepaperstoscreenrelevantcontributionsusingtheabovecitedselectionrules.
Although we partly filtered results, it is still not feasible to perform an exhaus-
tive selection analysis, even as simple as filtering by title. It is also impractical
to attempt an automated selection by content, as it is not clear how to perform
an efficient, high-recall, high-precision text classification without supervision.
Therefore, before proceeding with the rest of the search and selection steps, we
apply a ranking procedure on these intermediate results, so that we can priori-
tize investigation of more relevant papers. We apply the exclusion and inclusion
rules of Section ?? to the papers examined in ranking order.
This approximate procedure however raises the question of when it is con-
venient to stop our selection and discard the remaining items. To solve this, we
develop a new approach from our observations of ranked items. We base the
methodon the followingassumption: a considerable ratio ofrelevant papers can
be identified by ranking and selecting top results using different relevance cri-
teria (conference, position index in the query result set, number of hits in all
queries,etc.),butinthissortingscenariowealsoobservealong-taildistribution
ofrelevantdocuments,i.e.somerelevantpapersappearinthelastpositionseven
after sorting with our relevance heuristics (see Figure 1). This is coherent with
the known impossibility of performing exhaustive systematic literature reviews
and mapping studies, as completing the long tail provides less results at the
expenseofalargerresearcheffort.Weassumetheratioofrelevantpapersinthe
long tail to be constant andcomparable inmagnitude to the number ofrelevant
papers when sampled randomly from the result set. Based on this assumption,
we proceed as follows:
– We start screening all papers in the result set, ranked according to different
relevanceheuristics(e.g.numberofhitsinqueries),andweobservetheratio
of relevant papers identified over time;
– We examine the same papers in random order, and measure the same ratio;
– When the two ratios are comparable, we assert we reached the tail of the
distributionofrelevantpapersandstopexaminingandselectingnewpapers.
As sorting criteria, we use the number of hits in the search performed in the
previous step, as well as other more complex heuristics, taking into account the
index position in result sets and the number of citations. When examining a
paper, we look into the full content to identify concepts related to our selection
criteria previously illustrated. As done previously with search results, we gather
relevant papers in one unique group. Using this stopping criterion, we conclude
this selection step when we have identified 430 relevant papers.
6 P. Notaro et al.
(a) (b)
Fig.1: Estimated relevance probability for collected papers (y-axis), as a function of
the index in the result set (x-axis, in thousands), with paper arranged: (a) in random
order(b)usingarelevanceheuristicbasedonsearchhits.Wecanobservehow,thanks
to the heuristic (b), the majority of relevant papers can be identified by examining
only a small fraction of the set (the top results on the left side).
2.5 Additional Search Techniques
The “early stopping” criterion previously described, while allowing a feasible
and comprehensive selection strategy across thousands of contributions, has a
natural tendency towards discarding relevant papers. We also expect to miss
other relevant papers, not present in the initial set of 83817, because they were
not identified by our database search. To cover for these limitations, we apply
othersearchtechniquesinadditiontodatabasesearch.Differingfrombefore,we
here apply our selection criteria exhaustively for each document retrieved.
ReferenceSearch Foreachofthe430relevantpapersidentifiedintheprevious
step, we search inside their cited references. In particular, we adopt backward
snowballsampling[18]:weincludeinourrelevantsetallpaperspreviouslycited
by a relevant paper whenever they fulfill the selection requirements mentioned
above. By doing so, we obtain 631 relevant elements, for a total of 1061.
Conference Search Reference search allows to identify prominent contribu-
tions frequently mentioned by other authors. A drawback is the introduction of
bias towards specific research groups and authors. We also observed how ref-
erence search rewards specific tasks and research fields as they are typically
more cited. We therefore apply other search techniques to compensate for these
facts. We perform a manual search by inspecting papers published in relevant
conferences. These relevant conferences are identified via correlation with other
relevantpapersandhavealsobeenconfirmedbyexpertsinthefield.Welookat
the latest 3 editions of each conference, in an effort to compensate the sampling
of dated papers performed by reference search. We obtain 5 more papers with
this method.
Iterative Search Improvement To conclude our search, we attempt at im-
provingourinitialguessonITOperationskeywordsviaanalysisoftheavailable
A Systematic Mapping Study in AIOps 7
Table 2: Sample k-shingles with relevance probability (and total occurrences).
k=1 k=2 k=3
tpc-w,1.00(13) defectprediction,1.00(34) softwaredefectprediction,1.00(22)
log-based,0.92(11) (work)loadprediction,1.00(32) diskfailureprediction,1.00(8)
sla,0.84(48) softwareaging,1.00(13) failurepredictionmodel,1.00(7)
stragglers,0.83(10) resourceallocation,1.00(6) cloudresourceprovisioning,1.00(5)
vm,0.83(59) hardwarefailures,0.89(8) automaticanomalydetection,0.88(7)
text content (text and abstract). Using our relevant paper set as positive sam-
ples, we perform a statistical analysis to identify k-shingles (sets of k consecu-
tivetokens)thatappearofteninrelevantdocuments(Table2).Inparticular,we
measure the document relevance probability given the set of shingles observed
in the available text content. We choose k = 1,...,5. We use these shingles as
keywords to construct new queries along with previously used AI keywords. We
here limit the collection to 20 results per query. Thanks to this step, we identify
20newrelevantpapers.Asaby-product,wegetincontactwithfrequentlycited
concepts and keywords in AIOps, later useful for taxonomy and classification.
2.6 Data Extraction and Categorization
After obtaining the result set of relevant papers (counting 1086 contributions),
we analyze the available information to draw quantitative results and answer
our research questions. We describe here the data extraction process and the
analysis techniques employed to gather insights and trends for the AIOps field.
First, we classify the relevant papers according to target components and
data sources. Target components indicate a high-level piece of software or hard-
wareinanITsystemthatthedocumenttriestoenhance(e.g.harddriveforhard
diskfailureprediction).Wegroupcomponentsinfivehigh-levelcategories:code,
application, hardware, network and datacenter. Data sources provide an indica-
tionoftheinputinformationofthealgorithm(suchaslogs,metrics,orexecution
traces). Data sources are categorized in source code, testing resources, system
metrics, key performance indicators (KPIs), network traffic, topology, incident
reports, logs and traces. the “AI Method” axis denotes the actual algorithm
employed, with similar methods aggregated in bigger classes to avoid excessive
fragmentation (e.g. clustering may contain both k-means and agglomerative hi-
erarchicalclusteringapproaches).Table3presentsaselectionofpapersfromthe
result set with the corresponding target, source and category annotation.
Then, we use the result set to infer a taxonomy based on tasks and target
goals. The taxonomy is depicted in Figure 2. We divide in AIOps contributions
in failure management (FM), the study on how to deal with undesired behavior
in the delivery of IT services; and resource provisioning, the study of allocation
of energetic, computational, storage and time resources for the optimal delivery
of IT services. Within each of these macro-areas, we further distinguish ap-
proaches in categories based on the similarity of goals. In failure management,
these categories are failure prevention, online failure prediction, failure detec-
tion, root cause analysis (RCA) and remediation. In resource provisioning, we
divide contributions in resource consolidation, scheduling, power management,
service composition, and workload estimation. We further choose to expand our
analysis of FM (red box of Figure 2) by applying for this macro-area an addi-
tional subcategorization based on specific problems. Examples of subcategories
8 P. Notaro et al.
Table 3: Selection of result papers grouped by data sources, targets and
(sub)categories.
Data Sources Targets Data Sources Targets
Ref. Cat. Ref. Cat.
secruoseR secruoseR
stropeR secarT stropeR secarT
scirteM atad cffiarT scirteM atad cffiarT
edoC edoC edoC edoC
OLS/sIPK krowteN ygolopoT tnedicnI sgoL noitucexE noitacilppA erawdraH krowteN retnecataD OLS/sIPK krowteN ygolopoT tnedicnI sgoL noitucexE noitacilppA erawdraH krowteN retnecataD
ecruoS gnitseT metsyS ecruoS ecruoS gnitseT metsyS ecruoS
tnevE tnevE
[27] • • 1.1 [15] • • 3.1
[32] • • • 1.2 [10] • • • • 3.1
[16] • • • 1.3 [6] • • • 3.1
[41] • • • • 1.3 [28] • • 3.1
[29] • • 1.4 [30] • • • 3.2
[47] • • 2.1 [49] • • • 3.3
[14] • • • 2.1 [1] • • • • 4.1
[12] • • • 2.1 [33] • • • • 4.1
[46] • • • 2.1 [5] • • • 4.1
[8] • • • 2.2 [44] • • • 4.2
[11] • • • 2.2 [4] • • • • 4.2
[17] • • • • 2.2 [19] • • • 4.2
[35] • • • • 2.2 [9] • • • 4.2
[24] • • • 2.2 [36] • • • 4.3
[37] • • • • • 2.2 [7] • • • • 4.3
[45] • • 2.2 [26] • • • 4.3
[43] • • • 3.1 [2] • • • 4.3
[42] • • 3.1 [39] • • • 5.1
[40] • • • • 3.1 [48] • • • 5.2
[21] • • • 3.1 [25] • • • • 5.2
[22] • • • • 3.1 [38] • • • • 5.3
(Sub)Category Legend
1.1SoftwareDefectPrediction 2.2SystemFailurePrediction 4.2RootCauseDiagnosis
1.2FaultInjection 3.1AnomalyDetection 4.3RCA-Others
1.3SoftwareRejuvenation 3.2InternetTrafficClassification 5.1IncidentTriage
1.4Checkpointing 3.3LogEnhancement 5.2SolutionRecommendation
2.1HardwareFailurePrediction 4.1FaultLocalization 5.3Recovery
arecheckpointingforfailureprevention,orfaultlocalizationforrootcauseanal-
ysis (see also Table 3).
3 Results
We now discuss the results of our mapping study. We first analyze the dis-
tribution of papers in our taxonomy. The left side of Figure 3 visualizes the
distribution of identified papers by macro-area and category. Excluding papers
treating AIOps in general (8), we observe that more the majority of items (670,
62.1%) are associated with failure management (FM), with most contributions
concentrated in online failure prediction (26.4%), failure detection (33.7%), and
rootcauseanalysis(26.7%);theremainingresourceprovisioningpaperssupport
A Systematic Mapping Study in AIOps 9
Fig.2: Taxonomy of AIOps as observed in the identified contributions
inlargepartresourceconsolidation,schedulingandworkloadprediction.Onthe
right side, we can observe that the most common problems in FM are software
defectprediction,systemfailureprediction,anomalydetection,faultlocalization
and root cause diagnosis. To analyze temporal trends present inside the AIOps
field,wemeasuredthenumberofpublicationsineachcategorybyyearofpubli-
cation.ThecorrespondingbarplotisdepictedinFigure4.Overall,weobservea
large, on-growing number of publications in AIOps. We can observe how failure
detection has gained particular traction in recent years (71 publications for the
2018-2019 period) with a contribution size larger than the entire resource provi-
sioning macro-area (69 publications in the same time frame). Failure detection
is followed by root cause analysis (39) and online failure prediction (34), while
failure prevention and remediation are the areas with the smallest number of
attested contributions (11 and 5, respectively).
4 Conclusion
In this paper, we presented our contribution towards better structuring the
AIOps field. We planned and conducted a systematic mapping study by means
Fig.3:Left:distributionofAIOpspapersinmacro-areasandcategories.Right:percent