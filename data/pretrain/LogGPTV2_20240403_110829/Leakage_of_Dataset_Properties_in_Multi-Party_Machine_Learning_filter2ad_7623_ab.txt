there is a latent variable Z, and the observed X is mod-
eled by X = h(Z,A) where h is a function capturing the
relationship between the variables. Even if the target vari-
able Y only depends on Z through a random function g:
Y = g(Z), the conditional distribution Pr(Y = y|X = x)
still depends on A. Thus, machine learning models will
capture information about A. For example, consider a
task of predicting education level (Y ) based on data that
contains gender (A) and income (X). Suppose income
can be modeled by a function of latent variables skill and
occupation, and education level is only associated with
the skill. Though gender is not correlated with education
level (Y⊥A), it could be associated with occupation and
thus correlated with income (X).
The (X ∼ A,Y⊥A) scenario was also noted by Lo-
catello et al. [38] when studying fair representations.
The authors indicated that even if the original data may
not have a bias (i.e., when the target variable and the
protected variable are independent) using the protected
attribute in training can introduce bias.
To model (X ∼ A,Y⊥A) scenario in the experiments,
we use correlation coefﬁcients to determine the split of
dataset attributes into X and A. To have a more con-
trolled experiment, we also carry out experiments where
we introduce a synthetic variable and inject correlations
between it and a subset of attributes in X.
Y ∼ A: We also consider two cases where there is a cor-
relation between the target variable Y and the sensitive
attribute A: (X⊥A,Y ∼ A) and (X ∼ A,Y ∼ A). In the set-
ting of (X⊥A,Y ∼ A), attribute A and a set of attributes X
may be relevant in predicting Y , while being uncorrelated
with each other. For example, a reaction of an individual
to a new drug (Y ) could depend on the age and weight
of an adult, while age and weight may be regarded as
independent between each other.
The ﬁnal setting of (X ∼ A,Y ∼ A) is the most likely
scenario to happen in practice where the true distribution
and dependence between variables maybe unknown. For
example, consider a task of predicting whether a ﬁnan-
cial transaction by an individual is suspicious or not (Y )
based on customer information (e.g., occupation, age,
gender) and their transaction history (X), where their in-
come is the sensitive attribute A. The correlation between
attributes could either belong to cases (X ∼ A,Y⊥A) or
to (X ∼ A,Y ∼ A) since attributes such as occupation
and age are likely to be correlated with income (as also
suggested by the correlations in the datasets we use in
our experimental evaluation in Appendix A).
4 Threat Model and Attack
The goal of the adversarial party Padv is to learn
population-level properties about the rest of the dataset
used in the multi-party machine learning setting (e.g., in
the two-party setting this corresponds to learning prop-
erties of the other party’s dataset). Since Padv is one of
the parties, it has black-box access to the joint model f
trained (e.g., via MPC) on the data of all the parties (i.e.,
Dhonest and Dadv). Given this query interface to f , the
attacker wants to infer how sensitive attribute A is dis-
2690    30th USENIX Security Symposium
USENIX Association
tributed in honest parties’ dataset Dhonest. Throughout
the paper, we use attribute and feature interchangeably.
We model dataset property leakage as follows. Let
ahonest denote attribute values of A for all records
in Dhonest (for example, if the sensitive attribute is gen-
der, then ahonest is a vector of gender values of all records
in Phonest data). We deﬁne p(ahonest) to be the property
or information about ahonest that the adversary is trying
to infer. For example, the property could be related to
determining whether there is a higher presence of female
patients in the dataset Dhonest or learn the exact ratio of
female patients.
The attacker, besides knowing its own dataset Dadv and
having black-box access to the model f , is assumed to
have auxiliary dataset Daux that is distributed according
to D. Similar to [50], an auxiliary dataset can be gen-
erated either via (1) model-based synthesis approach —
feeding synthetic data to f and using its output to guide
the search towards data samples on which the model
returns predictions with high conﬁdence, (2) statistics-
based synthesis that uses information about marginal
distribution of the attributes, or (3) using a (publicly avail-
able) dataset of similar distribution. The attacker can use
approach (1) by merely using f , while Dadv provides it
with statistics for (2). The availability of a dataset that
follows similar distribution to D depends on the setting.
Consider the anti-money laundering use case in the intro-
duction. A party may have access to billions of ﬁnancial
transactions that it can use either for approach (2) since
record-level marginal distribution between demographic
features, income, education level is likely to be similar
between the parties, or for approach (3) by dividing its
dataset into Daux and Dadv.
The attack follows the shadow model training ap-
proach [7, 50]. However, we modify the attack vector
to measure the signal about the distribution of a sensitive
attribute in a whole dataset. Our attack strategy is de-
scribed below; Figure 1 shows graphical representation
of how the attack model is trained and Figure 2 shows
the execution of an attack on target model f .
We make an observation that to infer global properties
about training data, the attacker needs to combine infor-
mation from multiple inferences made by f . To this end,
the attacker measures how f performs on a sequence of
k records, called Dattack, as opposed to a single record
used in work on attribute and membership inference. We
obtain the “attack feature” sequence F by setting it to
the posterior probability vector across classes returned
by f on Dattack. Hence, if f is a classiﬁcation model over
l classes F consists of k× l values. In the experiments,
we construct Dattack by sampling from Daux at random.
We leave open a question of whether more sophisticated
methods of constructing Dattack can lead to better attacks.
Shadow models and attack meta-classiﬁer. The at-
tacker relies on shadow models in order to determine
whether F is generated from f
trained on a dataset
with property p or not. To this end, the attacker trains n
“shadow" models that resemble f . In particular, it gen-
erates training datasets Di
shadow, half of them exhibiting
the property and half not, labeled as p and ¯p accord-
ingly. These datasets could be obtained by resampling
from Daux. Each shadow model f i
shadow is trained on a
shadow ∪ Dadv using the same way as the target
dataset Di
central model f . Once f i
shadow is trained, the attacker
queries it using Dattack and combines inference results to
form a feature vector Fi associated with p or ¯p, depend-
ing on its training data.
After training all shadow models, the adversary has a
set of features Fi with the corresponding property label
pi ∈ {p, ¯p}. The adversary then trains a meta-classiﬁer
on the pairs {(Fi, pi)}i using any binary classiﬁcation
algorithm. For example, logistic regression is sufﬁcient
for attacks in our experimental evaluation.
The attacker carries out its attack as follows. Once
the target model f is trained on the joined data of the
attacker and honest party, the attacker queries the model
using Dattack to obtains the feature representation of the
target model, F . It then feeds F to its meta-classiﬁer and
obtains a prediction for the sensitive property p(ahonest).
Single-party attack. We explained the attack strategy
for the multi-party case since this is the primary focus
of this work. However, we can easily adapt the attack
to the single-party case: the only change that has to be
made to the attack description above is by setting Dadv
to an empty set. As highlighted in Table 1, besides being
the ﬁrst attack on property leakage in the centralized
multi-party setting, our attack is also the ﬁrst to show
that dataset properties can be leaked in the black-box
setting.
Fine-grained attack. The above attack shows how an
adversary can learn whether some property is present
in a dataset or not. The attacker can extend this binary
property attack and distinguish between multiple proper-
ties P ={p1, p2, . . .}. It simply generates shadow training
datasets for each property and then trains a meta-classiﬁer
to predict one of the properties in P based on attack vec-
tor F . For example, P can be a set of possible ratios of
females to other values, and the attack meta-classiﬁer
USENIX Association
30th USENIX Security Symposium    2691
Figure 1: Attack model pipeline. Half of shadow models are trained with the property p that the attacker is trying to
learn and half without it. Each shadow model f i
shadow is queried on a dataset Dattack. Output probability vectors are
concatenated to form a vector Fi. Finally, the meta-classiﬁer is trained on feature-label tuples of the form {(Fi, pi)}i.
Figure 2: Execution of the attack on the target model to learn the prediction of the property p(ahonest) in Dhonest, ˆp.
will try to distinguish whether it is 10:90, 50:50 or 90:10
split. In the experimental evaluation, we show that this
attack is effective in learning ﬁne-grained distribution of
sensitive attributes as well as identifying how the distri-
bution of a sensitive attribute has changed after the model
was updated with new data.
Scope. This work focuses on understanding the leak-
age of population-level properties of the training dataset.
Since our threat model is similar to that of the at-
tacker who is able to infer individual record-level at-
tributes [17, 51, 53], our setting allows for record-level
leakage as well. Albeit, the attack strategy needs to
be changed in order to train shadow models that cap-
ture the difference between inputs with different at-
tribute values. Importantly, for both the record-level and
population-level attribute inference attack, the attacker —
here and in [51, 53] — is assumed to know the domain
of an attribute it is trying to infer (e.g., Gender taking
values male, female, or other). Hence, similar to prior
work [18, 39], our attack cannot infer a sensitive attribute
with a large, potentially unbounded, domain (e.g., such as
Name for which the attacker may not be able to enumerate
all possible values).
5 Experimental Setup
The goal of our experiments is to evaluate the efﬁcacy
of the attack in Section 4 to learn population-level prop-
erties about a sensitive attribute in the multi-party and
single-party machine learning setting. We then aim to
understand how the difference in machine learning mod-
els (e.g., logistic regression and neural network models),
dataset type (e.g., tabular data, text or graph), access to
the model through its weights or inference interface, and
attribute correlation inﬂuence attack accuracy.
5.1 Benchmark Datasets
We evaluate our attack on ﬁve datasets described be-
low. The datasets, sensitive attributes, machine learning
model tasks, and the type of correlations between the
sensitive attribute, other attributes, and the ﬁnal task are
summarized in Table 3.
2692    30th USENIX Security Symposium
USENIX Association
ShadowModelTraining𝑦""𝑦#"⋮𝑦%"𝐷’()*+,"with𝑝𝐷’()*+,.with𝑝̅⋮∪𝐷)*1𝐷)22)3%𝑥"𝑥#⋮𝑥%⋮AttackTrainingSetMeta-classifier𝑦".𝑦#.⋮𝑦%.𝑛2𝑛2⋮∪𝐷)*1⋮MetaModelTrainingShadowmodel𝑓’()*+,.Shadowmodel𝑓’()*+,"(ℱ"				(𝑦"",𝑦#",..,𝑦%"),𝑝)=(ℱ.				(𝑦".,𝑦#.,..,𝑦%.),𝑝̅)=Shadowmodel𝑓’()*+,.Shadowmodel𝑓’()*+,"𝐷"#$%&’𝐷()*𝐷(’’(+,𝑥.𝑥/⋮𝑥,Meta-classifier𝑝̂(𝒂"#$%&’)Targetmodel𝑓Targetmodel𝑓TrainingAttackℱ				(𝑦.,𝑦/,…,𝑦,)=Health [3] The Health dataset (Heritage Health Prize)
contains medical records of over 55 000 patients. Sim-
ilar to the winners of the Kaggle competition, we use
141 features with MemberID and Year removed. We
group the DaysInHospital attribute into two classes.
The task, Y , is to predict if a patient will be dis-
charged, DaysInHospital = 0, or will stay in the hos-
pital, DaysInHospital > 0. We consider two sensitive
attributes to perform our attack on learning their distri-
bution in the dataset of the benign party: Gender and the
number of medical claims ClaimsTruncated.
Adult [33, 37] The Adult dataset contains US cen-
sus information including race, gender, income, and
education level. The training dataset contains 32 561
records with 14 attributes. We group the education level
into four classes: ‘Low’, ‘Medium-Low’, ‘Medium-High’,
‘High’. We use 12 features with Education and Fnlwgt
removed. The task is to predict the class of the
EducationLevel (i.e., variable Y for this dataset). We
again consider two sensitive features whose distribution
the attacker is trying to infer: Gender and Income.
Communities and Crime [37] The Communities and
Crime dataset contains 1 994 records with 122 features
relevant to per capita violent crime rates in the United
States, which was also used for evaluating fairness with
respect to protected variables [11]. We remove the at-
tributes that have missing data, resulting in 100 attributes.
The classiﬁcation task is to predict the crime rate, i.e., the
Y variable is CrimesPerPop. We group the crime rate
into three classes based on ranges: ‘ 0.5’, and the task is the multi-class prediction for
the crime rate. We consider total percentage of divorce
TotalPctDiv and Income as sensitive features.
Yelp-Health [4] The Yelp dataset contains 5 million
reviews of businesses tagged with numerical ratings (1-5)
and attributes such as business type and location. We
extract a healthcare-related subset that has 2 384 reviews
for pediatricians and 1 731 reviews for ophthalmologists.
The classiﬁcation task is to predict whether the review
is positive (rating > 3) or negative (rating ≤ 3). The
attack aims to predict the dominant value of the doctor
Specialty of the benign party.
Amazon [1, 35] The Amazon product co-purchasing
network dataset contains product metadata and reviews
information about 548 552 different products such as
books and music CDs. For each product, the following
information is available: the similar products that get
co-purchased, product type, and product reviews. We
use a subset of 20 000 products and construct a product
co-purchasing network, where each node represents a
product and the edge represents if there is at least one re-
viewer who rated both products, indicating that products
are bought by the same user [36]. Each node is associ-
ated with one of 4 product types and an average review
score from 0 to 5, including half-score reviews (i.e., 11
possible scores in total). The classiﬁcation task (for a
recommendation system) is to predict the average review
score of the node given the co-purchasing network and
the product types. Depending on the classiﬁcation task,
we split reviewer scores into 2 classes: positive vs. neg-
ative review, 6 classes: rounded integer review between
0,1.., 5 and 11 classes: the original review score. The
attack aims to predict whether the dominant value of the
attribute ProductType of the benign party is “books”.
5.2 Evaluation Methodology
Target model f . We train different target models de-
pending on the dataset type. For tabular data, i.e., Adult,
Health, and Crime, we train multinomial logistic regres-
sion and fully-connected multi-layer perceptron neural
networks (MLP). For the Adult and Crime datasets, we
use an MLP network with one hidden layer of size 12 and
the last layer with 4 and 3 output classes, respectively. For
the Health dataset, we use an MLP network with one hid-
den layer of size 20 and binary output. In later sections,
a neural network model for tabular datasets always refers
to an MLP network. In training our target models, we use
the Adam [30] optimizer, ReLu as the activation function,
a learning rate of 0.01, and a weight decay of 0.0001. For
the Yelp-Health dataset, we use the pre-trained glove em-
bedding of dimension 50, a bidirectional LSTM layer of
dimension 50. We then use one hidden layer of size 50
and dropout regularization with parameter 0.1 between
the last hidden layer and the binary output. For the Ama-
zon dataset, we train the target model using the Graph
Convolutional Networks (GCN) [31] with 1 hidden layer
of 16 units, Adam as the optimizer, ReLu as the activa-
tion function, a learning rate of 0.01, and a weight decay
of 0.0005. Each experiment is repeated 100 times, and all
attack accuracies are averaged over these runs. As noted
in Section 2, our attacks are oblivious to how f is trained,
hence, in the experiments training is done in the clear.
Dataset split.
In the multi-party setting, we consider
two parties that contribute data for training the target
model where one of the parties is trying to learn infor-
mation about the data of the other party. For Adult and
Health datasets, each party contributes 2 000 samples. We
USENIX Association
30th USENIX Security Symposium    2693
Datasets
#Dadv, #Dhonest
#Daux
#Dattack
Health [3]
Adult [33, 37]
Crime [37]
Yelp-Health [4]
Amazon [35]
2 000
2 000
200
1 000
5 000
10 000 / 4000