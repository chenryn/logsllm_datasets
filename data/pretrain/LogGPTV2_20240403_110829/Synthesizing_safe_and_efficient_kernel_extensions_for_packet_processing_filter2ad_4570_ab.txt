BPF bytecode, and implements the stochastic search procedure de-
scribed in Â§3. The search process synthesizes proposals, which
are candidate rewrites of the bytecode. The proposal is evaluated
against a suite of automatically-generated test cases to quickly
prune programs which are not equivalent to the source program, or
unsafe. If the proposal passes all tests, K2 uses formal equivalence-
checking (Â§4, Â§5) and formal safety-checking (Â§6) to determine the
value of a cost function over the proposal. The cost combines correct-
ness, safety, and performance characteristics, and is used to guide
the search process towards better programs. Formal equivalence-
checking and safety-checking may generate counterexamples, i.e.,
inputs where the proposalâ€™s output differs from that of the original
bytecode, or the proposal exhibits unsafe behaviors. These tests are
arXiv, July 14, 2021
Qiongwen Xu et al.
a candidate rewrite, i.e., a proposal ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„, using one of the rules
below, chosen randomly with fixed probabilities ğ‘ğ‘Ÿğ‘œğ‘(.):
(1) Replace an instruction (ğ‘ğ‘Ÿğ‘œğ‘ğ‘–ğ‘Ÿ ): at random, choose an in-
struction from ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ , and modify both its opcode and operands.
For example, change bpf_add r1 4 to bpf_mov r4 r2.
(2) Replace an operand (ğ‘ğ‘Ÿğ‘œğ‘ğ‘œğ‘Ÿ ): at random, choose an instruc-
tion and replace one of its operands with another value of the
same type. For example, change bpf_add r1 4 to bpf_add r1 10.
(3) Replace by NOP (ğ‘ğ‘Ÿğ‘œğ‘ğ‘›ğ‘Ÿ ): at random, choose an instruction
and replace it with a nop, effectively reducing the number of
instructions in the program.
(4) Exchange memory type 1 (ğ‘ğ‘Ÿğ‘œğ‘ğ‘šğ‘’1): at random, choose an
instruction, and if it is a memory-based instruction (i.e., a load
or a store), sample a new width for the memory operation and a
new immediate or register operand. The instructionâ€™s memory
address operand (i.e., address base and offset) as well as its
type (load vs. store) are unchanged. For example, change r1 =
*(u16*)(r2 - 4) to r3 = *(u32*)(r2 - 4).
(5) Exchange memory type 2 (ğ‘ğ‘Ÿğ‘œğ‘ğ‘šğ‘’2): at random, choose an
instruction, and if it is a memory-based instruction, sample
a new width for the memory operation. All other instruction
operands are unchanged. For example, change r1 = *(u16*)(r2
- 4) to r1 = *(u32*)(r2 - 4).
(6) Replace contiguous instructions (ğ‘ğ‘Ÿğ‘œğ‘ğ‘ğ‘–ğ‘Ÿ ): at random, choose
up to ğ‘˜ contiguous instructions (we pick ğ‘˜ = 2) and replace all
of them with new instructions.
These rewrite rules define the transition probabilities of the
Markov chain, which we denote by ğ‘¡ğ‘Ÿ(ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ â†’ ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„). We use
the probabilities ğ‘ğ‘Ÿğ‘œğ‘(Â·) shown in Table 8 (Appendix F.1). In our
experience, any probabilities that allow the Markov chain to move
â€œfreelyâ€ through the space of programs suffice to find programs
better than the input.
Non-orthogonality of rewrite rules. The rewrite rules above are not
mutually exclusive in the program modifications they affect. For
example, replacement by NOP (rule 3) is just a specific version of
the more general instruction replacement (rule 1). Given enough
time, a small set of general rules is sufficient to explore the space of
programs. However, the existence of more specific rules accelerates
the convergence of the Markov chain to better programs.
Domain-specificity of rewrite rules. STOKE and its variants [55,
127] proposed variants of the rewrite rules (1â€“3) above. Rules (4),
(5), and (6) are domain-specific rules that K2 uses to accelerate
the search for better BPF programs. Rules (4) and (5) help identify
memory-based code optimizations (Â§9). Rule (6) captures one-shot
replacements of multiple instructions, e.g., replacing a register addi-
tion followed by a store into a single memory-add instruction. These
domain-specific rules improve both the quality of the resulting pro-
grams and the time to find better programs (Â§8, Appendix F.1).
3.2 Cost Function
We compute a cost function over each candidate program. The
cost function ğ‘“ (ğ‘) contains three components: an error cost, a
performance cost, and a safety cost.
Error cost. The error cost function ğ‘’ğ‘Ÿğ‘Ÿ(ğ‘) is 0 if and only if the
program ğ‘ produces the same output as the source program ğ‘ğ‘ ğ‘Ÿğ‘
4
Figure 1: An overview of the K2 compiler. Solid arrows represent
the flow of control. Dotted arrows represent the flow of data.
added to the test suite, to enable quick pruning of similar programs
in the future. We describe aspects of the compilerâ€™s implementation,
including the BPF program interpreter we developed in Â§7.
3 STOCHASTIC OPTIMIZATION OF BPF
The K2 compiler translates programs from BPF bytecode to BPF
bytecode. K2 uses the stochastic optimization framework, intro-
duced in STOKE [127], which applies a Markov Chain Monte Carlo
(MCMC) sampling approach to optimize a cost function over the
space of programs.
At a high level, MCMC is a method to sample states from a
probability distribution over states. When we apply MCMC to
program optimization, the state is a program of a fixed size. A
well-known MCMC sampler, the Metropolis-Hastings (MH) algo-
rithm [78], works as follows. From an initial state, at each step, the
algorithm proposes a new state to transition to, using transition
probabilities between states (Â§3.1). The algorithm computes a cost
function over the proposal (Â§3.2) and determines whether to accept
or reject the proposed new state (Â§3.3) based on the cost. If the
proposal is accepted, the proposed state becomes the new state of
the Markov chain. If not, the current state is the new state of the
Markov chain. In the asymptotic limit, under mild conditions on
the transition probabilities [78], the set of all accepted states form
a representative sample of the steady-state probability distribution.
Why stochastic synthesis? Among the program synthesis approaches
in the literature (Appendix A), K2 adopts stochastic search primarily
because it can optimize complex cost functions, e.g., the number of
cache misses during program execution, with complex constraints,
i.e., safety. MCMC uses a standard transformation to turn very
general cost functions (Â§3.2) into steady-state probability distribu-
tions, enabling it to perform optimization by sampling from the
corresponding distribution [73, 78, 127].
3.1 Proposal Generation
The Markov chain starts by setting its initial state to ğ‘ğ‘ ğ‘Ÿğ‘, the
input program. Starting from any current state ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ , we generate
Clang-compiledBPF object file  ProposalGeneration (Â§3.1) EquivalenceChecker (Â§4, Â§5)Determine nextstate psynth or pcurr (Â§3.3) K2passOptimized BPFBytecode Test CasesSet Current StateSafety Checker(Â§6)failpassadd counterexamplepass or failpsrc pnextfailpsynth total costequivalent, safe, best-performing programpcurrinputCost Function(Â§3.2)ProgramInterpreter (Â§7)Synthesizing Safe and Efficient Kernel Extensions for Packet Processing
arXiv, July 14, 2021
on all inputs. We would like a function that provides a smooth
measure of the correctness of program ğ‘ with respect to the source
program ğ‘ğ‘ ğ‘Ÿğ‘, to guide the search towards â€œincreasingly correctâ€
programs. Similar to STOKE, we incorporate test cases as well as
formal equivalence checking (Â§4 & Â§5) to compute an error cost.
Using a set of tests ğ‘‡ and executing ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„ on each test ğ‘¡ âˆˆ ğ‘‡ , we
set
ğ‘’ğ‘Ÿğ‘Ÿ(ğ‘) := ğ‘ Â·âˆ‘ï¸
diff (ğ‘œğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„(ğ‘¡), ğ‘œğ‘ğ‘ ğ‘Ÿğ‘ (ğ‘¡)) +
ğ‘¡ âˆˆğ‘‡
unequal Â· num_tests
(1)
where:
â€¢ ğ‘œğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„(ğ‘¡) and ğ‘œğ‘ğ‘ ğ‘Ÿğ‘ (ğ‘¡) are the outputs of the proposal and the
source program on test case ğ‘¡,
â€¢ diff (ğ‘¥, ğ‘¦) is a measure of the distance between two values. We
consider two variants: (i) diff ğ‘ğ‘œğ‘(ğ‘¥, ğ‘¦) := popcount(ğ‘¥ âŠ• ğ‘¦)
is the number of bits that differ between ğ‘¥ and ğ‘¦, and (ii)
diff ğ‘ğ‘ğ‘ (ğ‘¥, ğ‘¦) := abs(ğ‘¥ âˆ’ğ‘¦), which represents the absolute value
of the numerical difference between ğ‘¥ and ğ‘¦. Relative to STOKE,
which only considers popcount as the semantic distance be-
tween values, we also find that many packet-processing pro-
grams require numeric correctness (e.g., counters), captured via
diff ğ‘ğ‘ğ‘ (.).
â€¢ ğ‘ is a normalizing constant denoting the weight of each test case.
STOKE adds the full error cost for each test case, setting ğ‘ =
ğ‘ ğ‘“ ğ‘¢ğ‘™ğ‘™ = 1. We also explore a second variant, ğ‘ğ‘ğ‘£ğ‘” = 1/|ğ‘‡ |, where
|ğ‘‡ | is the number of test cases, to normalize the contributions
of the many test cases we require to prune complex, â€œalmost
correctâ€ BPF programs.
â€¢ unequal is 0 if the first-order-logic formalization of the two BPF
programs (Â§4) finds that the programs are equivalent, else it is
1. We only run equivalence-checking if all test cases pass, since
it is time-consuming. If any test case fails, we set unequal to 1.
â€¢ num_tests includes two variants: (i) the number of test cases on
which ğ‘ produced incorrect outputs, and (ii) the number of test
cases on which ğ‘ produced correct outputs. STOKE uses only
the first variant. We consider the second variant to distinguish
a program that is equivalent to the source program from one
that satisfies all the test cases but is not equivalent.
Considering all variants from equation (1), there are 8 error cost
functions. We run MCMC with each cost function in parallel and
return the best-performing programs among all of them.
Performance cost. We use two kinds of performance costs corre-
sponding to different scenarios, namely optimizing for program
size and program performance.
The function ğ‘ğ‘’ğ‘Ÿ ğ‘“ğ‘–ğ‘›ğ‘ ğ‘¡ (ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„) (instruction count) is the number
The function ğ‘ğ‘’ğ‘Ÿ ğ‘“ğ‘™ğ‘ğ‘¡ (ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„) is an estimate of the additional
latency of executing program ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„ relative to ğ‘ğ‘ ğ‘Ÿğ‘. Unfortunately,
executing a candidate BPF program ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„ to directly measure its
latency is unviable, since the kernel checker will reject most can-
didate programs. Instead, we profile every instruction of the BPF
instruction set by executing each opcode millions of times on a
lightly loaded system, and determining an average execution time
exec(ğ‘–) for each opcode ğ‘–. The performance cost function is the dif-
ference of the sum of all the opcode latencies, i.e., ğ‘ğ‘’ğ‘Ÿ ğ‘“ğ‘™ğ‘ğ‘¡ (ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„) :=
ğ‘–ğ‘ ğ‘¦ğ‘›ğ‘¡â„âˆˆğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„ exec(ğ‘–ğ‘ ğ‘¦ğ‘›ğ‘¡â„) âˆ’ğ‘–ğ‘ ğ‘Ÿğ‘ âˆˆğ‘ğ‘ ğ‘Ÿğ‘ exec(ğ‘–ğ‘ ğ‘Ÿğ‘).
of extra instructions in ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„ relative to ğ‘ğ‘ ğ‘Ÿğ‘.
Safety cost. To our knowledge, K2 is the first synthesizing compiler
to incorporate generic safety constraints in first-order logic into
synthesis. The safety properties considered by K2 are described
in Â§6. Our approach to dealing with unsafe programs is simple:
once a program ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„ is deemed unsafe, we set ğ‘ ğ‘ğ‘“ ğ‘’(ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„) to
a large value ğ¸ğ‘…ğ‘…_ğ‘€ğ´ğ‘‹, leaving just a small probability for it to
be accepted into the Markov chain. We set ğ‘ ğ‘ğ‘“ ğ‘’(ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„) = 0 for
safe programs. We do not simply reject unsafe programs because
the path from the current program to a more performant and safe
program in the Markov chain may pass through an unsafe program
(for some intuition on why, see Fig. 4 in [127]). We leave formulating
smooth cost functions to guide the search through progressively
â€œsaferâ€ programs to future work.
The final cost function we use is ğ›¼âˆ—ğ‘’ğ‘Ÿğ‘Ÿ(ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„)+ğ›½âˆ—ğ‘ğ‘’ğ‘Ÿ ğ‘“ (ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„)+
ğ›¾ âˆ— ğ‘ ğ‘ğ‘“ ğ‘’(ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„). We run parallel Markov chains with different
(ğ›¼, ğ›½, ğ›¾) and return the programs with the least performance costs.
3.3 Proposal Acceptance
To determine whether a candidate proposal should be used as the
next state of the Markov chain, the cost ğ‘“ (ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„) is turned into the
probability of ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„ in the steady-state distribution, as follows [78]:
(2)
where ğ‘ =ğ‘ ğ‘’âˆ’ğ›½Â·ğ‘“ (ğ‘). The Metropolis-Hastings algorithm com-
ğœ‹(ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„) = ğ‘’âˆ’ğ›½Â·ğ‘“ (ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„)/ğ‘
putes an acceptance probability for ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„ as follows:
ğœ‹(ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„) Â· ğ‘¡ğ‘Ÿ(ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„ â†’ ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ)
ğœ‹(ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ) Â· ğ‘¡ğ‘Ÿ(ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ â†’ ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„)
ğ›¼ = min
(cid:32)
1,
(cid:33)
(3)
With probability ğ›¼, the next state of the Markov chain is set
to ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„, else the next state is just ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ . Here, the ğ‘¡ğ‘Ÿ(.) are the
transition probabilities between programs (Â§3.1). Intuitively, ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„
is always accepted if its cost is lower than that of ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ . Other-
wise, ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„ is accepted with a probability that decreases with the
increase in the cost of ğ‘ğ‘ ğ‘¦ğ‘›ğ‘¡â„ relative to ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ .
K2 repeats the process in Â§3.1, Â§3.2, and Â§3.3 from the new state
of the Markov chain, looping until a timeout.
4 CHECKING THE EQUIVALENCE OF BPF
PROGRAMS
K2 synthesizes output programs that are formally shown to be
equivalent to the input program. To do this, we first formalize
the input-output behavior of the two programs in first-order logic,
using the theory of bit vectors [98]. We identify the input and
output registers of the two programs based on the kernel hook they
attach to [88]. Then, we dispatch the logic query below to a solver:
inputs to program 1 == inputs to program 2
âˆ§ input-output behavior of program 1
âˆ§ input-output behavior of program 2
â‡’ outputs of program 1 != outputs of program 2
If the formula is satisfiable, there is a common input that causes
the outputs of the two programs to differ, which is added to the
test suite (Â§3). If the formula is unsatisfiable, the two programs are
equivalent in terms of input-output behaviors.
The rest of this section describes how we obtain the input-output
behavior of a single program in first-order logic. Our formalization
5
arXiv, July 14, 2021
Qiongwen Xu et al.
handles arithmetic and logic instructions (Â§4.1), memory access
instructions (Â§4.2), and BPF maps and other helper functions (Â§4.3).
We have checked the soundness of our formalization using a test
suite that compares the outputs produced by the logic formulas
against the result of executing the instructions with given inputs.
Preliminaries. We begin by reordering the instructions in the pro-
gram so that all control flow only moves forward. This is possi-
ble to do when a BPF program does not contain any loops. Then,
we convert the entire program into static-single-assignment (SSA)
form [39, 63]. The result after SSA conversion is a sequence of BPF
bytecode instructions where (i) each assignment to a register uses
a fresh label with a version number, e.g., bpf_mov r0 1; bpf_mov r0
2 is turned into bpf_mov r0_v1 1; bpf_mov r0_v2 2, and (ii) each
statement is associated with a well-defined path condition [110].
For example, in the instruction sequence,
bpf_jeq r1 0 1
bpf_mov r2 1
// if r1 != 0:
//
r2 = 1
the second instruction is associated with the path condition r1!=0.
At the highest level, we construct first-order formulas corre-
sponding to each instruction, and conjoin them, i.e., through the
logical conjunction operator âˆ§, to produce a final formula that rep-
resents the input-output relationship of the entire program. Now
we discuss how K2 formalizes each kind of instruction.
4.1 Arithmetic And Logic Instructions
To model register-based arithmetic and logic instructions, we repre-
sent each version of each register using a 64-bit-wide bit vector data
type. The action of each instruction is formalized by representing
its impact on all the registers involved. Our formalization handles
both 32-bit and 64-bit opcodes, as well as signed and unsigned
interpretations of the data.
As an example, consider the 32-bit arithmetic instruction bpf_add32
dst src (opcode 0x04) which has the action of taking the least signif-
icant 32 bits of the registers dst and src, adding them, and writing
back a (possibly truncated) 32-bit result into dst, zeroing out the
most significant 32 bits of the dst register.
Suppose we are given a single instruction bpf_add32 dst_x src_y
(after SSA) where x and y represent the version numbers of dst
and src, respectively. Suppose the result is stored in dst_z. This
instruction results in the formula
(tmp == (dst_x.extract(31, 0) +
src_y.extract(31, 0) ) ) âˆ§
(dst_z == concat(
bv32(0), tmp.extract(31, 0) ) )
where tmp is a fresh variable to hold the intermediate result of the
32-bit addition of dst_x and src_y, extract(a, b) represents the
effect of picking up bits a...b of a given bit vector, concat(x, y)
represents the bit vector produced by concatenating the two bit
vectors x and y, with x occupying the higher bits of significance in
the result, and bv32(0) is a 32-bit bit vector representing 0.
Similarly, we have constructed semantic representations of all
64-bit and 32-bit arithmetic and logic instructions [4].
4.2 Memory Access Instructions
BPF supports memory load and store instructions of varying sizes [4]
using pointers. We encode memory operations directly in the theory
of bit vectors to produce an efficient encoding in a single first-order
theory. We show how this encoding occurs in three steps.
Step 1: Handling loads without any stores. Suppose a BPF program
contains no stores to memory, and only load instructions, i.e., bpf_ld
rX rY. To keep the descriptions simple, from here on we will use
the notation rX = *rY to represent the instruction above.
The key challenge in encoding loads is handling aliasing, i.e.,
different pointers rY might point to the same memory region, and
hence the different rX must have the same value.
Suppose the ğ‘–ğ‘¡â„ load instruction encountered in the program
reads from memory address rY_i and loads into register rX_i. Then
for the ğ‘–ğ‘¡â„ load, we conjoin the formula
(rY_j == rY_i â‡’ rX_j == rX_i)
âˆ§
ğ‘— <ğ‘–
Formulating this formula requires maintaining all the previous
loads in the program that might affect a given load instruction. To
achieve this, K2 maintains a memory read table for the program: the
source and destination of each load is added to this table in the order
of appearance in the post-SSA instruction sequence. K2 handles
partial overlaps in loaded addresses by expanding multi-byte loads
into multiple single-byte loads.
Step 2: Handling stores and loads in straight-line programs. Stores
complicate the formula above due to the fact that a load of the form
rX = *rY must capture the latest write to the memory pointed to by
rY. For example, in the instruction sequence rX_1 = *rY; *rY = 4;
rX_2 = *rY, the first and second load from rY may return different
values to be stored in rX_1 and rX_2.
Suppose the program contains no branches. Then, the latest
write to a memory address can be captured by the most recent
store instruction (in order of encountered SSA instructions), if any,
that writes to the same address. K2 maintains a memory write table,
which records the memory address and stored variable correspond-
ing to each store in the program. Suppose ğ‘˜ stores of the form *rY_i
= rX_i (i from 1 Â· Â· Â· k) have been encountered in the program
before the load instruction rX_l = *rY_l. Then, the load is encoded
by the formula
âˆ§
ğ‘–:ğ‘— <ğ‘–â‰¤ğ‘˜
âˆ§
â‡’
rY_j
rX_j
The formula
! (rY_i == rY_l) asserts that the address
loaded isnâ€™t any of the addresses from stores that are more recent
than store ğ‘—. Hence, if rY_j == rY_l, the loaded value must come
from store ğ‘—.
== rY_l
== rX_l
! (rY_i == rY_l)
Informally, the overall formula for a load instruction takes the
form: if the address was touched by a prior store, use the value from
that store, otherwise use the aliasing clauses from the â€œloads-onlyâ€
case in step (1) above.3 Together, step (1) and step (2) complete K2â€™s
encoding of memory accesses for straight-line programs.
âˆ§
ğ‘—:ğ‘—â‰¤ğ‘˜
âˆ§
ğ‘–:ğ‘— <ğ‘–â‰¤ğ‘˜
3It is possible for a load to occur without a prior store e.g., when an instruction reads
from input packet memory.
6
Synthesizing Safe and Efficient Kernel Extensions for Packet Processing
arXiv, July 14, 2021
Step 3: Handling control flow. We construct a single formula per
instruction including control flow akin to bounded model check-
ing [46]. Our key insight to generalize the encoding from step (2)
above is to additionally check whether the path condition of the
load instruction is implied by the path condition of the prior store:
âˆ§
ğ‘—:ğ‘—â‰¤ğ‘˜
âˆ§