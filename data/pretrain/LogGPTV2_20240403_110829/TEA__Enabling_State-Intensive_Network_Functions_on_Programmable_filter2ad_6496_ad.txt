lookups using TEA, it incurs some amount of latency and bandwidth
overheads for packet processing. For latency, as we show in §6.1, it
adds up to around 2 µs per-packet latency depending on the packet
size. For bandwidth, since TEA generates additional RDMA packets
for external DRAM lookups, it affects both the switch pipeline
and link bandwidth consumption. Within the switch pipeline, as it
replicates an incoming packet to generate RDMA write and read
packets, it doubles the bandwidth usage of the egress pipeline. It
also consumes the same amount of link bandwidth between the
switch and a server where a target entry is located. On the server
side, while TEA does not involve CPUs, it consumes some amount
of servers’ memory bandwidth, which may affect performance of
memory-intensive applications running on servers, especially when
the memory bandwidth is fully utilized. Note that if an entry for
the packet is already cached, there is no overhead.
5 Implementation
5.1 Data and Control Plane
We implement TEA’s data plane in P4 [22] and compile it to Barefoot
Tofino ASIC [7] with P4 Studio [6]. In the memory address resolver,
we use Tofino-embedded crc64 as a hash function to locate a bucket
in TEA-Table. We implement the server ID resolution using a
range-matching table. In the memory access requestor, to craft
lookup request packets, we make the packet replication engine in
the ASIC replicate an incoming packet into two packets. The engine
ensures that there is no interleaved packet between two replicas.
Based on the replicas, it generates RoCE packets (i.e., an RDMA
write and read) by adding RoCE headers on top of the packets based
on the metadata resolved by the memory address resolver.
We implement the count-min sketch [25] for collecting the sta-
tistics and determining popular entries, similar to that of prior
work [40, 51]. We use 4 register arrays and 64K 16-bit slots per
array to implement sketches. When the sketches detect a popular
key (i.e., counts of the key exceed a threshold), it reports the key to
the control plane by using the digest feature in the ASIC. The digest
internally maintains a Bloom filter that prevents duplicate keys
from being reported. The control plane populates popular entries to
the cache which is implemented as a regular exact-matching table.
We use a cache of size N =1024 in our prototype which consumes
approximately 55 KB of SRAM in NAT for IPv6 addresses.
Switch control plane and server agent. We implement the switch
control plane in Python and C. It manages the ASIC via the ASIC
driver using a runtime API generated by the P4 compiler. The
server agent running on servers is written in C, which initializes
an RDMA NICs on the servers and communicates with the switch
control plane when it establishes RDMA connections.
5.2 Programming NFs with TEA
Our prototype implements TEA APIs as a library of modularized
P4 codes using the concept of control block in P4 [17, §13]. Each
control block implements key modules such as the lookup response
handler, memory address resolver, and memory access requestor.
Developers provide TEA with a definition of key (e.g., 5-tuple) used
of a lookup table, a structure of the table stored in DRAM (e.g.,
using struct in C), and where to store the lookup response for
further packet processing. Figure 7 shows how these blocks would
be used to implement NAT.
To demonstrate the applicability of TEA, we implement four NFs
in P4 using TEA: a NAT, a stateful firewall, a load balancer, and a
VPN gateway. Table 2 describes the state each NF maintains using
TEA and its estimated size. Brief descriptions of each are below,
and simplified P4 codes are in Appendix B.
count-minsketch(§4.3)Cache(§4.3)Memoryaddressresolver(§4.2)Memoryaccessrequestor(§4.1)NF...Switch control planeDigestUpdateStash(§4.2)UpdateHitMissMissLookuprequesthandler(§4.2)Lookupresponse?NoYesTEA-Table (§4.2)ServersLookup requestLookup responseNF...Switch ASIC (Data plane)TEA: Enabling State-Intensive Network Functions on Programmable Switches
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
(a) Regular NAT data plane.
(b) NAT data plane with TEA.
Figure 7: Comparison of simplified NAT data plane with and
without TEA. To use TEA, in addition to the original logic
(white-boxes), developers need to add TEA modules (blue-
boxes) and provide basic information necessary for lookup
(red-colored).
NAT. The NAT implementation uses the TEA to store NAT transla-
tion tables, to lookup a ⟨private IP, Port⟩ pair for a given 5-tuple. It
modifies the IP address and port header fields using lookup results.
Firewall. The firewall stores the connection state to external DRAM
using TEA. For an external connection, the firewall looks up a con-
nection state and uses it to determine how to handle packets.
Load balancer. The load balancer stores the per-flow server map-
ping table to external DRAM using TEA. For each incoming packet,
it looks up a ⟨Backend server’s IP address, Port⟩ from the table.
VPN gateway. We implement a VPN gateway (e.g., [11]) based on
the details described in prior work [19]. It manages the external-to-
internal tunnel mapping table consisting of a ⟨customer’s external
tunnel ID, VM IP⟩ pair as a key and a ⟨Server IP, internal tunnel ID⟩
pair as a value. For incoming packets from customers, the gateway
looks up the table to retrieve corresponding server IPs and internal
tunnel IDs, and translates packets.
5.3 Limitations
The NICs in our testbed limit the maximum number of outstanding
RDMA read requests to 16, and if there are more requests than
the limit (i.e., overloaded), they drop the requests and the QP state
becomes invalid. To prevent the NICs on servers from being over-
loaded, we implement a simple flow control in the switch data
plane, which counts and limits the number of outstanding read
requests. If there is a lookup request and the number of outstanding
requests has already reached to the limit, it drops the request (i.e.,
not generating both RDMA read and write requests), causing a
packet drop. This may affect the end-to-end performance. We plan
to design a mechanism that routes lookup requests to an alternative
DRAM server in such a case, instead of dropping packets. Also,
currently, we assume that there exists at least one server that is
not overloaded, and if there is no available server, TEA does not
generate lookup requests and drops the packets as above.
While our NF implementations (§5.2) access one large table,
some NFs may require multiple large tables. Although the current
design of TEA can support multiple tables through multiple external
DRAM accesses, we plan to improve its efficiency as future work.
6 Evaluation
We evaluate TEA on a testbed consisting of a programmable switch
and commodity servers using both real data center network packet
traces and synthetic packet traces. Our key findings are:
• With a single server, TEA provides a predictable lookup la-
tency (1.8–2.2 µs) and throughput (7.3–10.9 million lookups per
second) for different sizes of packets. With multiple servers, a
small cache helps balance loads across servers across different
skewness parameters. With the cache, adding servers scales the
throughput effectively and 8 servers can perform 138 million
lookups per second under a skewed workload. (§6.1).
• Compared to server-based NFs, TEA-enabled NFs are cost effec-
tive. TEA shows up to 9.6× higher throughput and 3.1× lower
latency under the same hardware configuration. Even under
an optimal setting for server-based NFs, TEA still shows ≈2.3×
higher throughput without requiring costly hardware (§6.2).
• TEA-enabled NFs can serve traffic with latency and throughput
that is comparable to the switch-only implementation (i.e., NFs
running on a switch without accessing external DRAM) in the
common case (§6.2).
• TEA provides these benefits without incurring much ASIC re-
source overhead. It consumes on-chip resources, including SRAM,
TCAM, and hash bits, all less than 9% (§6.3).
Experimental setup. Our testbed consists of a Wedge 100BF-32X
32-ports programmable switch [14] with a Tofino ASIC and 12
servers equipped with two Intel Xeon E5-2609 CPUs (8 logical cores
in total), 64 GB RAM, and a 40 Gbps Mellanox CX-3 Pro RDMA
NIC. The servers run Ubuntu 18.04 with the kernel version 4.4.0.
All servers are directly connected to the switch. We use 4 servers
as packet generators and 8 as DRAM servers.
Traffic workloads. We use both packet traces collected from a
real data center network [1] and synthetically generated ones. The
packet sizes vary (64–1500 B) in the real trace. The synthetic traces
are based on the observations from several data center measurement
studies [20, 26, 62]. We generate packet traces with the flow size
distribution in terms of the number of packets per flow that follows
Zipf distribution with the skewness parameter (α=0.99, 0.95, 0.90).
We use a keyspace of 1 million randomly generated IPv4 5-tuples
when creating packet traces. We generate multiple packet traces
with different packet sizes and skewness parameters. We replay the
traces using DPDK-pktgen [3] on packet generator nodes. In our
testbed, each traffic generator node can generate 64 B packets at
around 34.54 Mpps and 1500 B packets at 40 Gbps.
6.1 Microbenchmarks
Single-server lookup latency and throughput. First, we eval-
uate the performance of the DRAM access channel with a single
server. For this experiment, we disable the SRAM cache. For latency,
we inject 10,000 packets of different sizes (64–1500 B) to measure
the lookup time. As a baseline, we setup two servers directly con-
nected and run ib_read_lat in perftest [10] to measure RDMA
(IP, Port) = Lookup (5-tuple)Update header ((IP, Port))Forward IncomingTrafﬁcOutgoingTrafﬁcSwitch ASIC (Data plane)(IP, Port) = Lookup (5-tuple)Update header ((IP, Port))Forward OutgoingTrafﬁcLookup response?(IP, Port) = Extract (5-tuple)Hit?DRAM_Lookup (5-tuple)IncomingTrafﬁcNoYesNoYesSwitch ASIC (Data plane)TEA-TableServersLookup requestLookup responseSIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Daehyeok Kim et al.
(a) Lookup latency
(b) Lookup throughput
Figure 8: Lookup performance of TEA via an RDMA channel
with a single server.
Figure 10: Lookup throughput changes during failover
events.
Figure 9a shows that the lookup load distribution is skewed
across servers without the cache. We also observe that such a
skewed access pattern limits the aggregate when the lookup request
rate is high, even if there is available link bandwidth to servers.
Finally, we see that with cache, even with the most skewed access
pattern (α=0.99), the load is evenly spread across servers and 49%
of requests are served by the cache.
Next, we measure the aggregate lookup throughput varying the
number of servers with different α values. As shown in Figure 9b,
in all four cases, while the aggregate throughput scales linearly
as we add more server, there is the difference in achievable max-
imum throughput depending on the skewness and the existence
of the cache. We see that the more skewed the load distribution,
the higher aggregate throughput TEA can support with the cache.
When α=0.99 or 0.95, TEA can process 138 million lookups per
second with 8 servers and the cache. Note that this performance is
limited by the maximum packet generation rate we can achieve in
our testbed.
One natural question regarding the throughput would be what is
the maximum throughput an NF with TEA can achieve with N servers
in a rack? The evaluation result shows that with 8 servers TEA can
support up to 138 million lookups per second. If we extrapolate this
result, it means that the NF can process up to 138/8 × N million
packets per seconds, which is not high enough to support very high
traffic rate with small size packets, especially when skewness is not
high, and this is a limitation of our current design. For example, to
support a few billion packets per second traffic rate, TEA requires
more than a hundred servers, which is way more than a number of
servers typically existing in a rack and a number of switch ports.
Note that this analysis may not be perfectly accurate because as
mentioned above, the measured maximum throughput is capped
by the packet generation rate in our testbed. We plan to analyze
the system throughput by injecting packets at higher rates with
more servers.
Availability. Next, we evaluate how TEA reacts to server churn by
setting up 2 servers, loading the same table entries using server-1
as a primary and server-2 as a secondary server. We replay the
64 B packet trace and measure the lookup throughput by disabling
the cache. For the result in Figure 10, we inject the background
traffic from packet generators to server-1 to emulate link utilization
increase. We see that TEA starts sending lookup requests to server-
2, and the throughput reaches the maximum within a second (at
around 24 sec.). At this point, server-2 becomes primary. We then
stop injecting the background traffic and disconnect server-2 to
emulate a server failure. We can see that TEA starts routing lookup
(a) Small cache balances memory
access loads.
(b) Lookup throughput scales
with more servers and cache.
Figure 9: Scalable lookup throughput of TEA with multiple
servers and cache.
read latencies for different message sizes. For throughput, we replay
the trace for 30 seconds and measure the number of lookups com-
pleted during the period. Since the memory access pattern might
affect the throughput, we force TEA to access buckets sequentially
or randomly in this measurement.
Figure 8a shows the median, 10th and 90th percentile of lookup
time. We see that each lookup takes 1.8–2.2 µs and the latency grows
with the packet size, which is higher than raw RDMA reads (0.1–0.2
µs). This is mainly because our RDMA read request and response
packets are larger than raw RDMA read packets. First, due to switch
ASIC limitation, we are not able to remove an original packet from
each replicated packet. This makes each RDMA read request packet
have the original packet as a trailer. Second, in TEA, each RDMA
read response packet consists of a bucket and the original packet,
as illustrated in Figure 5b.
Figure 8b shows the lookup throughput with different packet
sizes. At the maximum traffic rate we can generate in our testbed,
the server NIC can handle 7.3–10.9 million lookups per second, and
there is the only negligible difference (up to 0.02 million lookups per
second) between sequential and random memory access patterns.
Overall, our evaluation shows TEA’s remote DRAM access chan-
nel can provide predictable performance which is close to the raw
RDMA performance.
Throughput scaling with multiple servers. Next, we evaluate
the effectiveness of using multiple servers and a small cache to scale
up the lookup throughput. Here, we replay synthetic packet traces
consisting of 64 B packets with the different skewness parameter (α)
for the flow size distribution and measure the number of lookups
served by each server with/without the cache enabled.
6412825651210241500Packetsize(B)024Latency(us)TEARDMA-read6412825651210241500Packetsize(B)051015Millionlookups/secTEA-RandTEA-Seq08w/ocacheα=.9008w/ocacheα=.9512345678ServerID08Millionlookups/secw/cacheα=.991248Numberofservers050100150Millionlookups/secUniformw/ocacheUniformw/cacheα=.95w/cacheα=.99w/cache020406080100Time(sec)010Millionlookups/secServer-1overloadedRoutedtoserver-2Server-2downRoutedtoserver-1Server-1Server-2TEA: Enabling State-Intensive Network Functions on Programmable Switches
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Network func.
TEA w/o cache TEA w/ cache
Tput.
Lat.
(Mpps)
(µs)
2.34
79.37
79.23
2.35
79.34
2.33
2.30
79.45
Tput.
(Mpps)
10.64
10.58