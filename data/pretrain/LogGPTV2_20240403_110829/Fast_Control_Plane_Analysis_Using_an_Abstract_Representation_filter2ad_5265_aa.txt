title:Fast Control Plane Analysis Using an Abstract Representation
author:Aaron Gember-Jacobson and
Raajay Viswanathan and
Aditya Akella and
Ratul Mahajan
Fast Control Plane Analysis
Using an Abstract Representation
Aaron Gember-Jacobson∗◦, Raajay Viswanathan∗◦, Aditya Akella◦, Ratul Mahajan†
◦University of Wisconsin-Madison, †Microsoft Research
◦{agember,raajay,akella}@cs.wisc.edu, †PI:EMAIL
ABSTRACT
Networks employ complex, and hence error-prone, routing
control plane conﬁgurations. In many cases, the impact of
errors manifests only under failures and leads to devastating
effects. Thus, it is important to proactively verify control
plane behavior under arbitrary link failures. State-of-the-art
veriﬁers are either too slow or impractical to use for such
veriﬁcation tasks. In this paper we propose a new high level
abstraction for control planes, ARC, that supports fast con-
trol plane analyses under arbitrary failures. ARC can check
key invariants without generating the data plane—which is
the main reason for current tools’ ineffectiveness. This is
possible because of the nature of veriﬁcation tasks and the
constrained nature of control plane designs in networks to-
day. We develop algorithms to derive a network’s ARC from
its conﬁguration ﬁles. Our evaluation over 314 networks
shows that ARC computation is quick, and that ARC can
verify key invariants in under 1s in most cases, which is
orders-of-magnitude faster than the state-of-the-art.
CCS Concepts
•Networks → Control path algorithms; Network dynam-
ics; Network reliability;
Keywords
Network veriﬁcation; control plane; abstract representation
1.
INTRODUCTION
A network’s routing control plane is responsible for gen-
erating the data plane (i.e., forwarding tables) using one or
more distributed routing protocols (e.g., OSPF, RIP, BGP).
∗These authors contributed equally to this work.
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than ACM must be honored. Abstracting with credit is per-
mitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
SIGCOMM ’16, August 22-26, 2016, Florianopolis , Brazil
c(cid:13) 2016 ACM. ISBN 978-1-4503-4193-6/16/08. . . $15.00
DOI: http://dx.doi.org/10.1145/2934872.2934876
Prior work has shown that the conﬁguration of these routing
protocols [5, 6] and their interactions [19, 21] can be quite
complex in modern networks. Consequently, control planes
are prone to conﬁguration errors that compromise network
security, availability, and performance [8, 25].
Many control plane errors manifest only during failures
and can have a devastating impact. For example, in 2012,
failure of a router in a Microsoft Azure data center trig-
gered previously unknown conﬁguration errors on other de-
vices, degrading service in the West Europe region for over 2
hours [23]. Another large class of errors arises when refac-
toring a network’s control plane: e.g., consolidating rout-
ing domains to improve manageability [5], changing rout-
ing protocols to improve scalability [18], or replacing old
devices, potentially with hardware from a different vendor.
These common errors highlight the importance of proac-
tively analyzing a control plane.
Unfortunately, many network veriﬁcation tools [15, 16,
17, 20] analyze a network’s current data plane. This limits
the scope of their analyses to the current live network and
prevents them from being used for proactive analysis. To
overcome this limitation, more recent tools, such as Bat-
ﬁsh [9], simulate the control plane and generate the net-
work’s expected data plane under speciﬁc failure scenarios,
e.g, a single link failing. However, these tools operate at a
low level of abstraction, modeling individual protocol mes-
sage exchanges to generate the data plane. As such, they
tend to be slow. Further, these tools must generate the com-
plete data plane for every possible failure scenario of in-
terest. These attributes render them impractical for several
key tasks, such as proactively verifying certain security and
availability invariants under arbitrary failures, where the tools
must generate an exponential number of data planes.
Fortunately, we observe that detailed data plane genera-
tion is not always necessary due to two factors. First, proac-
tive analysis tasks often require computing properties of
paths, not the paths themselves. For example, invariants I1–
I4 in Table 1 focus on the existence (or absence) of paths;
I5 relies on the set of paths taken, but we show that actually
computing the paths is unnecessary. Second, many enter-
prise and data center networks use only a handful of routing
protocols which interact in very speciﬁc ways (§7.1).
We leverage the above factors to develop a new abstrac-
tion that operates at a higher level than today’s control plane
300
veriﬁers, enabling more direct proactive analyses. We call
this abstract representation for control planes, or ARC. ARC
enables the aforementioned proactive analyses to run orders
of magnitude faster than state-of-the-art tools.
ARC abstracts the mechanics of individual routing pro-
tocols and simply captures the collective impact they could
have on the network’s data plane. ARC is composed of a se-
ries of weighted digraphs that are routing protocol-indepen-
dent. We develop algorithms for generating ARCs that accu-
rately model the common protocols (OSPF, RIP, and eBGP)
and mechanisms (static routes, ECMP, access control lists,
and route redistribution) used in enterprise and data center
networks; for example, our university network and hundreds
of data center networks operated by a large online service
provider (OSP) use these constructs (§7.1). To maintain
ARC’s efﬁciency, we do not model protocols that are less
common in enterprise and data center networks (e.g., iBGP).
Crucially, the ARC’s edges and vertices are chosen such
that the true forwarding path between network locations un-
der any failure scenario is provably included in the ARC’s di-
graphs. Consequently, verifying key security and availability
invariants (e.g., I1–I4 in Table 1) boils down to computing
simple graph characteristics of the ARC, such as connected
components and max-ﬂow, which run in polynomial time.
Furthermore, for control planes using a restricted set of
features—e.g., AS path length is the only path selection cri-
terion used by eBGP instances—we show how to conﬁgure
edge weights in the ARC such that the shortest path com-
puted on the ARC under a given failure scenario is the same
as the path computed by the real network. Our university
network and a large fraction (97%) of the data center net-
works we study use such a restricted set of features.
In
these cases: (1) We can use the ARC to generate a counter-
example for violations of the security and availability invari-
ants (I1–I4) in Table 1, where the counter-example includes
a failure scenario and an invariant-violating path.1 Opera-
tors can use these examples to make proactive ﬁxes to buggy
conﬁgurations before rolling them out into a live network.
(2) We can enable equivalence testing (I5) for networks sat-
isfying the restrictions by simply comparing the ARCs for
the old and new conﬁgurations.
We implement our ARC generation algorithms in Java,
using Batﬁsh [9] to parse conﬁgurations written in vendor-
speciﬁc languages; our code is publicly available [1]. To
evaluate ARC, we check the control planes of 314 data cen-
ter networks operated by a large OSP against the invariants
in Table 1. We ﬁnd that each network’s ARC can be gen-
erated in a few seconds. Verifying many of the invariants
under arbitrary failure scenarios takes less than 1s for 99%
of networks. Checking I3 across collections of trafﬁc ﬂows
is the most time-consuming: it can take up to a few tens of
minutes. In contrast, state-of-the-art tools [9] are 3-5 orders
of magnitude slower in checking limited failure scenarios.
2. MOTIVATION
1We can still verify (but not present a counter-example for)
I1–I4 even if networks do not satisfy the restrictions.
Invariant
I1: Always blocked
I2: Always reachable
with < k failures
I3: Always isolated
I4: Always traverse
waypoint
I5:
Equivalent(C1, C2)
Example
External hosts can never commu-
nicate with hosts in subnet S
Up to 5 links can fail with-
out breaking connectivity between
subnets S1 and S2
Trafﬁc between subnets S1 & S2
and S3 & S4 never traverses the
same link simultaneously
Trafﬁc between external hosts and
internal hosts must always tra-
verse a ﬁrewall
Trafﬁc between hosts must always
traverse the same paths if control
plane C2 were to replace C1
Table 1: Invariants of common interest
The network control plane is the heart of a network. It
may be composed of multiple routing domains, or routing
instances. Each routing instance is a collection of processes
running on different routers that exchange information (e.g.,
link-state updates) using a speciﬁc protocol (e.g., OSPF, RIP,
BGP) [13, 21]. Routing processes on the same device may
exchange routes with each other using route redistribution.
Static routes may also be used. The routing instances col-
lectively generate the network’s data plane, i.e., forwarding
tables, based on protocol-speciﬁc algorithms (e.g., Bellman-
Ford), various parameters (e.g., link weights), access control
lists (ACLs), and the current state of network links. In addi-
tion, operators may deﬁne knobs (e.g., redistribution costs
and administrative distances) that determine how a router
chooses among the many routes it learns via the processes
conﬁgured on it. Routing processes and their parameters,
access controls, and the operator deﬁned knobs are speciﬁed
in a router’s conﬁguration ﬁle.
Recent work has shown that most networks’ control planes
use complex designs to realize sophisticated goals [5, 6, 19].
Unfortunately, the complexity makes these control planes er-
ror prone [12]. In particular, critical errors may arise only
during failures, e.g., when one or more links fail simultane-
ously, or while refactoring the control plane’s design.
In this paper, we focus on two important ways of identify-
ing such errors in control planes: (1) verifying security and
availability invariants hold across arbitrary failures and (2)
equivalence testing. Below, we illustrate the importance of
these tasks using a toy example. We argue that no existing
tool is capable of performing these tasks, at least not feasibly
on sufﬁciently large networks. We then highlight opportuni-
ties we can leverage to efﬁciently support the two tasks.
2.1 Satisfying Invariants
Figure 1a shows an example network’s control plane. It
uses three routing instances: one BGP instance and two OSPF
instances. Routers A, E, G, H run processes for the OSPF0
instance, routers D, E, and F run processes for the BGP1
routing instance, and routers A, B, C, and D run processes
for the OSPF2 instance. The picture also shows links costs
(e.g., 4 on the A-B link), route redistribution and the cost of
such redistributed routes (e.g., from OSPF2 to BGP1, whose
cost is 0), and data plane ACLs (e.g., at router D, on the
301
A 
4
OSPF2 
100 
H 
100 
1 
G 
100 
T 
E 
T 
1
C 
1
T 
T 
T 
D 
1
B 
1
0 
S 
F 
100 
H 
100 
1 
G 
100 
T 
E 
A 
4
OSPF2 
OSPF3 
T 
1
C 
1
T 
T 
T 
D 
U 
1
B 
5 
1
U 
Y 
0 
1
1
0 
S 
OSPF0 
BGP1 
OSPF0 
BGP1 
F 
Z 
S 
(a) Initial control plane
(b) Expanded control plane
Figure 1: An example enterprise network: Circles represent
routers and rectangles represent routing instances. Links be-
tween routers are labeled with OSPF costs. No-entry sym-
bols represent ACLs blocking trafﬁc destined for T . Arrows
between instances indicate route redistribution, and spec-
ify the cost assigned to such routes. Tubes represent static
routes that are redistributed in the direction of the arrow.
A-D link). One of the objectives this control plane ensures,
among many, is that S cannot communicate with T .
Security & availability. To avoid undesirable outcomes
from manifesting under failures, operators may require that
certain key security and availability invariants always hold
in their network (i.e., even under arbitrary failures). In the
above example, it is easy to see the invariant “subnet S can
never send trafﬁc to subnet T ” always holds, because the
only possible path from S to T is via D or E, and every
interface on D (E) that participates in the OSPF2 (OSPF0)
routing instance has an ACL that blocks trafﬁc destined for
T . Now assume the enterprise acquires a startup and con-
nects the startup’s network—represented by OSPF3 in Fig-
ure 1b—to the existing network. To allow subnets S and T
to communicate with subnet U , the operator conﬁgures route
redistribution and static routes on routers B and Z.
Unfortunately, the change introduces the subtle side-effect
that S can now send trafﬁc to T under some speciﬁc failure
scenarios, violating the operator’s requirement that the in-
variant always hold. In particular, without any failures, there
is a cheaper path from S to T : F →Z→Y →B→D→A;
since this passes through D, trafﬁc is still blocked. How-
ever, if the B–D and C–D links both fail, the new cheapest
path goes directly from B to A, bypassing the ACLs on D.
Equivalence. In some situations, operators refactor the net-
work’s control plane to simplify device conﬁgurations and
improve manageability. For example, an operator may want
to combine multiple OSPF instances (e.g., OSPF2 and OSPF3
in Figure 1b) into a single OSPF instance [5]. Operators
inform us that, in making such a change, they wish to en-
sure the optimized control plane is equivalent to the original
control plane: i.e., under arbitrary failures the new and old
control planes should generate the same data plane. Testing
for equivalence is also important when operators alter their
control plane design to use a different set of protocols (e.g.,
replace OSPF with BGP for scalability reasons [18]) or dif-
ferent hardware (e.g., replace old devices, potentially with
hardware from a different vendor). It is difﬁcult to evalu-
ate the equivalence of such changes, because the raw device
conﬁgurations look very different.
302
2.2 Limitations of Existing Veriﬁers
Current veriﬁcation tools are designed to check the net-
work in its present state or under a limited set of failure sce-
narios (e.g., all single link failures). As a result, current tools
are incapable of, or very inefﬁcient at, both determining that
the above reachability invariant can be violated and testing
if the optimized control plane is equivalent to the original.
Data plane modeling. Many tools [15, 16, 17, 20] build a
model of the data plane based on snapshots of device for-
warding tables, or SDN control messages. Because they
verify the current data plane, these tools cannot proactively
check if an invariant, such as our reachability example above,
would be satisﬁed if links failed. Likewise, data plane veri-
ﬁers cannot be used for equivalence testing.
Control plane modeling. Older control plane tools model
speciﬁc devices (e.g., ﬁrewalls [24]) or routing protocols
(e.g., BGP [8]). As such, they are not well suited for verify-
ing today’s enterprise and data center networks, which make
use of multiple device types and routing protocols [12].
A more recent tool, Batﬁsh [9], models several routing
protocols and their interactions using Datalog. This allows
Batﬁsh to generate data plane models for a set of failure
scenarios and verify an invariant holds across the generated
data planes. Unfortunately, tools such as Batﬁsh are slow be-
cause they do not abstract the network at all, but instead try
to mimic low level protocol interactions and generate a full
data plane. This can take as long as a few minutes (§7.3).
Furthermore, Batﬁsh must generate the data plane for every
possible k link failure scenario. In our reachability example
above, Batﬁsh can only detect the invariant violation after
generating and examining O(|ℓ|2) data planes for single and
two-link failure scenarios, where ℓ is the set of links in the
network. In the worst case, Batﬁsh must generate an expo-
nential (in |ℓ|) number of data planes, making it impractical.
For equivalence testing, control plane veriﬁers must gen-
erate data planes under all possible failures for both control
planes and compare them, which is again impractical.
2.3 Opportunities for Improvement
Our insight is that, in practice, checking many key invari-
ants does not require computing the actual forwarding paths.
In the reachability example above, which illustrates I1 in Ta-
ble 1, we ideally only need to check if S and T are in differ-
ent connected components of a logical graph induced by the
network’s control plane conﬁguration and physical topology.
As we show later, the remaining invariants (I2–I5) can also
be analyzed without generating the data plane.
Furthermore, many networks, especially enterprise and
data center networks employ a limited set of routing con-
structs in their control plane design. We list the key attributes
we observe in our university network and the data center net-
works of a large online service provider in Table 3. Notably,
only a handful of routing protocols are used, and they oper-
ate and interact in limited ways.
Thus, our intuition is that by focusing on what check-
ing key invariants actually entails and by considering con-
strained control plane designs, we can develop a new ab-
T 
B 
T 
S 
1 
T 
1 
D 
T 
C 
3 
U 
OSPF 
(a) Control plane for a network with three subnets (squares) and 3
routers (circles) participating in a single OSPF instance (rectangle);
no-entry symbols indicate inbound ACLs on trafﬁc from T
Src:U 
Dst:T 
Dst: S 
Src:T 
Dst: S 
Src:U 
I 
B 
1 
1 
0 
O 
B 
1 
0 
O 
C 
0 
I 
C 
0 
I 
D 
3 
0 
1 
3 
O 
D 
O 
C 
0 
I 
C 
3 
3 
I 
D 
0 