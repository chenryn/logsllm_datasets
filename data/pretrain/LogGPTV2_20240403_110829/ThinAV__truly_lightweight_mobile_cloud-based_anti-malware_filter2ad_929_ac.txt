paring the time required to scan a ﬁle with VirusTotal with
the time required to merely upload a ﬁle to VirusTotal.
4.4 ComDroid Performance
To evaluate ComDroid, we uploaded each of the 1,022
apps in our data set to the ComDroid scanning service over
a two day period. For each upload, we recorded the time
required for a scan report to be returned.
Results.
Of the 1,022 packages uploaded to ComDroid, 993 were
scanned, with the remainder being rejected by the server due
to a 10 MB size limitation. Of the 993 packages scanned by
ComDroid, 8 returned a scan error, resulting in 985 valid
scan results. The mean response time was 40.67 seconds (σ
Kaspersky
VirusChief
VirusTotal
VirusTotal (Uploads)
f (x) = 10
f (x) = 10
f (x) = −9 × 10
f (x) = 9
−5 × x + 1.891
−5 × x + 17.133
−6 × x + 1.947
−6 × x + 182.98
Table 3: Linear equations for each of the three scan-
ning services derived from Figures 2, 3, 4, and 5.
Equations calculate the response time for each scan-
ning service for a ﬁle x bytes in size.
214
ComDroid
f (x) = 0.0132 × x + 9.6893
Table 4: Linear equation for the ComDroid scanning
service.
= 77.60 seconds), and the median response time was 18.63
seconds. Figure 6 shows the response time plotted as a func-
tion of the package size, and the exact function is speciﬁed in
Table 4. There is a clear positive linear relationship between
package size and scan time, although numerous outliers are
present.
Discussion.
The performance of the ComDroid service is somewhat
similar to both the Kaspersky and VirusChief services (Sec-
tion 4.3). However, the linear trend is much less prominent.
The most likely explanation for this lies in the nature of
the analysis performed by ComDroid. ComDroid is a static
code analysis tool, and as such, it is safe to assume that the
time required to analyze an Android app is more directly
inﬂuenced by the amount of code in the package, than the
total size of the package. Given that many apps contain
numerous resource ﬁles (images, sounds, video, etc.) which
are not scanned by ComDroid, it is easy to imagine how
a package might have a large size, but a relatively small
amount of code. It is quite likely that the observed linear
trend is much more a result of the upload time and not the
code-to-resource-ﬁle ratio of the package.
ComDroid detects a wide variety of “programming errors”.
In our experiment, 971 of 985 apps (98.6%) were ﬂagged by
ComDroid as containing some type of code that is vulnera-
ble to exposed communication. This suggests that, at least
in its current form, simply ﬂagging an application as being
“at risk” if there is any instance of exposed communication
would eﬀectively cripple the ability of users to install apps
on their device. Chin et al. [3] (the ComDroid authors) sug-
gest that in their data, after manual inspection of warnings,
only about 10–15% were genuine vulnerabilities.
We believe that there is a place for ComDroid (and other
tools like it) in the ThinAV architecture. However, the be-
havior of this ThinAV module would likely have to be ad-
justed over time to prevent excessive false positives. This
could be done by creating thresholds which would ﬂag a
package as vulnerable if it had signiﬁcantly more exposed
surfaces than average for a given type of warning.
Network Con-
ﬁguration
Typical 3G
Ideal 3G
Typical WiFi
Ideal WiFi
Upload Speed
(KBps)
16.25
1792.00
190.38
76800.00
Download
Speed (KBps)
84.13
1792.00
155.38
76800.00
Table 5: Network speeds used for evaluating the
mobile implementation of ThinAV.
Network Conﬁguration
Ideal 3G
Typical 3G
Ideal WiFi
Typical WiFi
Small File Medium File
36.56 s
84.66 s
36.13 s
40.23 s
98.13 s
210.00 s
97.14 s
106.68 s
Large File
170.29 s
394.14 s
168.31 s
187.39 s
Table 7: Time required to check an package in Thi-
nAV, for three diﬀerent ﬁle sizes, and four diﬀerent
network conﬁgurations, assuming the scan result is
not cached by ThinAV.
Network Conﬁguration
Ideal 3G
Typical 3G
Ideal WiFi
Typical WiFi
Small File Medium File
Large File
0.034 s
0.041 s
0.034 s
0.035 s
0.232 s
0.239 s
0.231 s
0.233 s
0.293 s
0.300 s
0.293 s
0.294 s
Table 6: Time required to check an package in Thi-
nAV, for three diﬀerent ﬁle sizes, and four diﬀerent
network conﬁgurations, assuming the scan result is
already cached by the ThinAV server
4.5 Safe Installer Performance
The performance of the Safe Installer is based on three
factors: the size of the package being scanned, the speed of
the network to which the device is connected, and whether
or not the package being installed has already been scanned
by ThinAV. To evaluate the Safe Installer, we use three dif-
ferent ﬁle sizes: 0.76 MB (small), 1.78 MB (medium), and
3.56 MB (large), corresponding to the median size of apps in
the category with the smallest median size (medical apps),
the median size for the entire data set, and the median size
of apps in the category with the largest median size (educa-
tional apps). We also use the “ideal” and “typical” 3G and
WiFi speeds from prior work [10, 11] (see Table 5).
Results.
The best case scenario for the performance of the safe
installer is when the package being installed has already been
scanned by ThinAV. In this case, the cost for performing an
install time check is equal to the time required to hash the
installing application, send the hash to ThinAV, look up the
scan result, and return the scan result.
We measured the time required to hash a small, medium,
and large application on the Android emulator, and took
the average of ﬁve runs for each size. The emulator was able
to calculate hashes of small ﬁles in 0.033 seconds, medium
ﬁles in 0.231 seconds, and large ﬁles in 0.293 seconds. We
recorded the amount of data uploaded and downloaded dur-
ing transmission of the hash to the ThinAV server, as well
as the reception of the result. This was approximately 200
bytes (100 up, 100 down), although this amount varied slightly
with the ﬁle being scanned. Finally, the cost of the ThinAV
server performing a cache lookup was 0.0002 seconds. Ta-
ble 6 summarizes the results for this best case scenario. In
general, even the largest ﬁle over the slowest network only
takes 0.3 seconds to check with ThinAV.
The worst case scenario is when the application being in-
stalled has not been scanned by ThinAV, and therefore the
whole package must be uploaded to ThinAV, which must
then upload the package to one or more of the third-party
scanning services. Using the formulæ in Tables 3 and 4,
and the ﬁle sizes and network speeds above, it is possible
to compute the time required to upload and scan these ﬁles
at install time. We note that when calculating the time re-
quired to scan a package, we add both the time to scan the
package with an anti-virus module as well as the time for
scanning with ComDroid.
Table 7 summarizes the results for this worst case sce-
nario. In general, the time required to upload and scan an
Android package ranges between 36 seconds and 394 sec-
onds, depending on the size of the ﬁle and the speed of the
network.
Discussion.
The best case scenario, where ThinAV already has a cached
scan result, is extremely fast. At 0.3 seconds, this check
would be unnoticeable to a user. On the other hand, if the
ﬁle must be uploaded and scanned, this process could take
as long as seven minutes. This could be seen as a serious
inconvenience to the user, but considering that this check
would only take place when a user is installing an app that
has never been seen by the ThinAV server, large-scale de-
ployment should make this an infrequent occurrence. Addi-
tionally, given that ThinAV could be primed with packages
from a variety of sources, including regular downloads of
applications from various application markets, upload of ap-
plications by developers, and the upload of applications by
other users running ThinAV, the chance that a user would
have to upload a package for scanning at install time could
be made very rare. Of course, only the evaluation of a wide-
scale deployment can provide conﬁrmation of our intuition.
4.6 Killswitch Performance
During normal operation, we expect the most frequently
used functionality of ThinAV to be the killswitch service
which is periodically activated and checks for revoked apps.
To evaluate the performance of the killswitch, we examine
several factors: (1) the cost of hashing apps to generate a
system ﬁngerprint; (2) the network cost associated with up-
loading the ﬁngerprint; (3) the cost of looking up the hashes
on the ThinAV server; and (4) the network cost associated
with returning those hashes to the client. We also consider
a manual upload feature, in which all packages currently in-
stalled are submitted for scanning. This is required to scan
applications that were installed prior to enabling ThinAV.
In general, the time required for the killswitch to perform
a check for revoked apps without uploading any full packages
to the ThinAV server will be:
215
h1 = time to hash all packages
su = hash upload size
spu = link upload speed
c1 = cache lookup time
sd = response download size
spd = link download speed
sd
spd
t1 = h1 +
su
spu
+ c1 +
(1)
Because the cost of performing a manual upload of missing
packages is dominated by upload and scanning costs (similar
to the safe installer above), we include only these costs in the
calculation. The time required for the killswitch to manually
upload missing packages is:
su = package upload size
spu = link upload speed
ts = time to scan all applications
t2 =
su
spu
+ ts
(2)
To test the performance of the hashing function, we in-
stalled the top ﬁve apps from each of the 21 Google Play app
categories (on top of the 5 default non-system apps) on the
Android emulator. After installation, we generated a com-
plete system ﬁngerprint (i.e., a hash for each app installed
on the user partition) ten times and recorded the average
time. This represents the worst case scenario in which none
of the apps on the device have been hashed before, and all
hashes must be computed. Next, we generate another ten
ﬁngerprints and record the average time. The cache created
in the previous test was left intact, however. This represents
the best case scenario in which all of the apps on the phone
have already been hashed and the phone ﬁngerprint is stored
locally.
Under normal use it is likely to expect that the typical
scenario would in fact be the best case scenario, or very
close to it. After the ﬁrst ﬁngerprint has been generated,
the only time an app will have to be hashed is when it has
not been seen by the killswitch, meaning it has just been
installed. Unless a user installs numerous apps between the
scheduled runs of the killswitch, it is likely the number of
apps that need to be hashed would be near zero.
Combining the hashing performance with the ﬁle size data
for the data set, the scanner performance functions in Ta-
bles 3 and 4, and the experimental network performance
measurements from [10], we calculate the cost of performing
manual uploads, as well as the cost of ﬁngerprinting based
on Equations 1 and 2.
Results.
Figure 7 shows the best (cached) and worst (uncached)
case scenarios for the ﬁngerprint generation time as a func-
tion of both the number of packages on the device and the
total size of those packages.
It is clear that time to generate a system ﬁngerprint grows
linearly with both the number and size of packages on the
device. In the worst case, with 110 apps on the device, it
only takes 29.95 seconds to generate a system ﬁngerprint.
The best case scenario is better, with a ﬁngerprint being
)
s
(
t
n
i
r
p
r
e
g
n
i
F
e
t
a
r
e
n
e
G
o
t
e
m
T
i
)
s
(
t
n
i
r
p
r
e
g
n
i
F
e
t
a
r
e
n
e
G
o
t
e
m
T
i
4000 
3500 
3000 
2500 
2000 
1500 
1000 
500 
0 
)
B
(
d
e
t
t
i
m
s
n
a
r
T
a
t
a
D
0 
20 
60 
80 
40 
Number of Packages 
100 
120 
Uncached 
Cached 
Data Transmitted 
(a)
35 
30 
25 
20 
15 
10 
5 