types 
that  by  design  permit  a  narrow  range  of  possible 
interpretations  as  to  their  content  (e.g.  clipart).  The  lack  of 
explicit  attention  given  to  image  filtering  even  in  these  contexts 
appears  to  assume  that  it  is  a  one-off  procedure,  and  that  the 
resulting  login  challenge  can  be  reused  for  each  user  of  the 
system.  However,  likely  realities  of  deployment  might  make  the 
use  of  a  finite  image  set  unrealistic.  For  instance,  inevitable 
password resets would mean that previously seen images must be 
discarded from the image set for a particular user. In addition, if 
the  image  set  or  login  challenge  is  static  between  users,  then 
attackers  can  build  up  knowledge  regarding  user  behavior  with 
those 
images  e.g.  user  choice,  can  permit  phishing,  and 
spontaneous  distribution  of  credentials  e.g.  password  sharing, 
observation attack (due to the images providing a common frame 
of reference shared between users). This approach also takes little 
account of results that have suggested users have better memory 
retention  for images they have created  [28], nor context-specific 
defenses that result from strategic selection of image content [9]. 
There  is  a  general  lack  of  knowledge  regarding  the  impact  of 
strategies  of  image  filtering  upon  usability  and  security.  The 
assumption so far has been that images should all be semantically 
and  visually  different  for  purposes  of  usability.  A  different 
strategy  is  illustrated  by  Passfaces  [25],  a  commercial  system 
where the image stimuli are drawn from a database of normalized 
face  photographs.  A  brief  description  offered  regarding  their 
image filtering procedure is the following: “The grids of faces in 
Passfaces  are  grouped  by  sex  and  are  selected  to  be  equally 
distinctive  so  that  Passfaces  cannot  be  described  by  gender  or 
obvious  characteristics”[25]  pg.  5.  This  illustrates  sensitivity  to 
risks of large semantic differences in the login challenge and the 
ability  for  users  to  share  the  graphical  passwords.  Usability 
concerns  must  result,  however  one  study  of  human  memory 
involving 2500 images presented in pairs showed that participants  
101
Table 1: Illustrative results from graphical password studies 
focused upon different image types. 
Stimulus 
System 
Icons 
Komanduri [18] 
Passfaces [4] 
Faces 
Tullis & Tedesco [38]  Photos 
Dynahand [27] 
Weinshall [40] 
Déjà vu [5] 
Scribbles 
Clipart 
Fractals 
Entropy  
35 bits 
12.7 bits 
20 bits 
9.5 bits 
47 bits 
16 bits 
Success % 
100% 
85% 
100% 
99.4% 
>90% 
90% 
could  be  accurate  at  remembering  precise  image  details.  Even 
where  images  were  visually  and  semantically  identical  and 
exhibited  only  small  differences  in  detail,  e.g.  orientation,  user 
recognition  rates  were  only  marginally  worse  than  when  images 
exhibited  semantic  differences  [3].  The  assembly  of  a  login 
challenge  based  upon distinct semantics is perceived to improve 
usability,  but  those  assembled  to  incorporate  similar  semantics 
could be harnessed to improve security. 
In  either  case,  while  the  curation  of  a  usable  and  secure  login 
challenge  remains  a  skill  residing  with  those  with  the  greatest 
experience  of  doing  it,  the  propagation  of  such  systems  more 
generally is limited. The spontaneous use of everyday  uncurated 
image collections (e.g. photographs) in this context is perceived to 
be  particularly  challenging,  however,  this image type is in some 
ways  attractive,  as  sets  of  uncurated  images  are  ubiquitous  in 
personal collections and online.  It is possible that if methods of 
automated image filtering based upon judicious analysis of image 
content are identified, this could reduce the imperative to identify 
an optimal image type.    
3.1  Image Similarity in the Login Challenge 
There is currently little convention to follow regarding where to 
apply  systematic  analysis  of  image  similarity.  Figure  2  outlines 
points  in  a  typical  recognition-based  graphical  password  login 
challenge  that  could  comprise  the  image  filtering  procedure. 
Analysis can occur on a per-grid and a per-login basis. On a per-
grid basis, intra grid key-decoy similarity refers to the similarity 
between  a  key  image  and  collocated  decoy  images.  The  most 
usable  visual  search  is  one  where  decoy  images  appear  distinct 
from the key image [7]. High similarity in this dimension suggests 
that  users  might  confuse  the  key  image  with  a  collocated  decoy 
image,  whereas  low  similarity  suggests  the  key  image  would 
appear  to  be  easier  to  identify  amongst  the  decoy  images.  Intra 
grid  decoy  similarity  refers  to  the  difference  between  collocated 
decoy images. In isolation such consideration provides few  
Figure 2: Points at which to consider image similarity across 
an example login challenge.  Red indicates a per grid 
consideration, and blue indicates a per login consideration. 
Figure 3: Visualisation of the number of strong matches identified per image in the first study. A strong match is determined 
by a threshold upon the number of participants that must have classed an image pair as being similar. Overall: 1 
participant=4424 strong matches; 2=2462 strong matches; 3=1425 strong matches, 14=148 strong matches. 
usability  issues,  however  high  intra  grid  decoy-decoy  similarity 
and high intra grid key-decoy similarity indicates that a grid may 
overall appear visually similar, which could complicate usability, 
observation attack and description [9].   
The  per-grid  consideration  should  be  complemented  with  per-
login  analysis.  Inter  grid  key  similarity  refers  to  the  similarity 
between key images. If there is high similarity in this dimension 
there is a threat that an attacker might infer that pattern (e.g. all 
key images are of specific objects or contain particular colors). If 
this  difference  is  too  great,  it  is  likely  that  images  will  be  more 
difficult  to  remember  for  the  user  who  must  remember  visually 
and  semantically  disconnected  images.  Inter  grid  key-decoy 
similarity refers to the similarity between a decoy image and non-
collocated  key  images.  This  is  important  from  a  usability 
perspective, as a decoy image may appear to be similar to a non-
collocated  key  image  and  entice  users  to  select  it  erroneously. 
Inter  grid  decoy  similarity  considers  the  similarity  of  decoy 
images  across  a  whole  login  challenge.  High  similarity  in  this 
regard, along with high intra grid decoy similarity, indicates that 
decoys  across  the  whole  login  could  appear  visually  similar, 
whereas high inter grid decoy similarity and low intra grid decoy 
similarity indicates that there exists similarity within the decoys in 
each grid, however each grid appears visually different. 
the  success  of  a  proposed 
4.  USER STUDY 1 – HUMAN CONSENSUS 
OF IMAGE SIMILARITY 
Perceptions of image similarity are subjective. However, in order 
to  measure 
image  processing 
intervention,  it  is  necessary  to  first  obtain  some  ground  truth 
notion of pairwise similarity that exists within a particular image 
set.  To  do  this  we  carried  out  a  user  study  to  capture  a  human 
consensus  of  similarity  within  an  image  set  to  provide  the  basis 
for further study.  
4.1  Procedure 
We assembled a set of 101 digital photographs and recruited 20 
participants (14 male, 6 female with ages μ=27, σ=6) who were  
staff and students in the research lab. Each participant was asked 
to organize the printed set of 101 images into piles on a tabletop 
according to a similarity ranking method proposed elsewhere [35].  
This  involved  the  participant  being  asked  to  organize  the  set  of 
images into piles, with the only criteria being that those perceived 
to be similar should be placed in the same pile. No further advice 
is offered. The raw data per-participant were the image numbers 
present  within  each  pile.  Across  all  participants  this  was 
aggregated into a score n for each image pair (x,y), where (x,y)=n 
means that image x and y appeared on the same pile n times,   
102
intended 
images  was 
where  n  ≤  20,  and  high  values  of  n  indicate  high  agreement  of 
similarity.  The  set  of  101 
to  be 
representative of a typical photograph collection. The size of the 
image  set  was  chosen  to  provide  a  manageable  sorting  task  for 
participants.    The  images  were  printed  onto  high  quality  paper 
(100mm  x  80mm)  and  the  reverse  of  each  was  numbered.  For 
descriptive purposes only we labeled the images according to the 
following  informal  categories:  People  (9):  focus  is  a  person  or 
group of individuals; Scene (30): the focus is purely a landscape 
scene;  Object  (14):  the  focus  is  purely  an  object;  People/Scene 
(47): the focus is upon people and scenery; People/Object (1): the 
focus  is  upon  both  people  and  an  object.  The  image  collection 
contained images taken to a wide range of photographic quality, 
and was sourced by aggregating a number of personal collections.  
4.2  Results 
Figure 3 gives an overview of the raw output for this study, which 
highlights  the  subjective  nature  of  image  similarity  judgments 
even across a relatively small image set. For each image, the graph 
illustrates  the  number  of  other images considered to be a strong 
match  for  similarity.  For  a  pairing  to  be  considered  a  strong 
match, we applied a threshold to n that represented the minimum 
number  of  times  images  should  have  been  placed  on  the  same 
pile. As we increase the threshold, fewer images are classed as a 
strong match. The median number of piles participants sorted the 
images into was 21.5 (IQR = 12.25) with a minimum of 6 piles 
and  a  maximum  of  32  piles.  Images  in  the  people  and  scene 
categories  generally  had  the  highest  number  of  image  matches 
(Median=45,  IQR=13)  and  those  in  the  Object  category  had  the 
least  (Median=33;  IQR=23).  No  systematic  investigation  of  the 
strategies used to group images was conducted, but in general  it 
was apparent that these ranged from matching particular objects in 
the  image,  to  matching  the  overall  context,  contrast  level  or 
principal colors. Although we only use 101 images in the results, 
the graph shows 102 (the original number) since one image  was 
misplaced in the course of the study (#84). 
4.2.1  Choosing an Automated Similarity Measure 
A  final  phase  of  this  study  was  to  test  a  number  of  image 
processing methods to identify one that was most appropriate to 
detect the most severe instances of similarity as identified in the 
sorting  task.  The  threshold  of  n≥14  (that  is:  in  our  first  study, 
fourteen  or  more  participants  judged  two  images  as  similar) 
provided  a  basis  for  us  to  identify  the  most  severe  cases  of 
similarity that any automated mechanism should detect. The field 
of  Content-based  Image  Retrieval  is  fast  moving;  our  approach 
was to test a number of candidate image signatures that would not 
require extensive expertise in image processing to understand and 
implement.  We  reused  the  images  from  the  first  study  and 
performed  analysis  with  those  images  in  the  CIE(L*a*b*)  color 
space [11] which is more perceptually linear than RGB or HSV. 
We implemented each of the following in OpenCV: 
(cid:120) 
Statistical Moments: treat each channel of a digital image as 
a  probability  distribution  and  calculate  the  first  three 
statistical  moments.  To  compare  two  image  signatures  we 
calculated  the  Euclidean  distance  between  the  statistical 
moments of each color channel and threshold the result. 
(cid:120)  Color Histogram: the histogram bins contain the frequencies 
of  particular  pixel  values.  Firstly  we  initialize  a  histogram 
with  16x16x16x16  bins,  which  divides  each  8  bit  color 
channel into 16 bins. In the normalization phase, each bin is 
set to a value between zero and one representing its relative 
frequency with regard to the other bins. Then we remove any 
bins with less than 1% of the volume as this can be attributed 
to  noise.  To  compare  histograms  we  calculate  the  (EMD) 
[30]  which  treats  the  histograms  as  piles  and  provides  the 
minimum  cost  of  turning  one  pile  into  the  other.  The 
threshold for similarity was 0.9. 
PerceptualDiff [42]: this is not an image signature but is a 
suite of algorithms that contains a model of the human visual 
system. Its canonical task is to optimize the computer 
graphics task of global illumination, by determining whether 
two scenes are perceptually similar. We were interested to 
see if a more sophisticated approach held promise. 
(cid:120) 
For each method we made a single pass of the digital images from 
the  sorting  study  where  each  was  resized  to  384x286.  We  took 
each image in turn, calculated the corresponding image signatures 
and compared to every other image in the set, noting the images 
that  were  judged  to  be  similar  in  each  case.  To  calculate  the 
success  of  these  routines,  we  employed  widely  used  metrics  for 
information retrieval: recall and precision:  
Recall    = 
{relevant
images}
|
|
{retrieved
images}
(cid:20)
relevant
images}
|
 {|
Precision  = 
{relevant
 |
{ 
images}
(cid:20)
{retrieved
retrieved