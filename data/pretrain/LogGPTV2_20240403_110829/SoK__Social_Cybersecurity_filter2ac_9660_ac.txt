logistics, experience, and funding. Even though these policies
are static, administrators often make exceptions in contextual
and social situations, e.g., to keep up with high demand, to
maintain good relationships with staff, and to build trust with
end-users. Despite these exceptions, end-users were often still
frustrated with the static nature of the policies.
D. The public
At the public scale, there is little existing S&P literature
focused on negotiating access to shared resources; however,
there are a few case studies on sharing resources with strangers
via P2P car-, ride-, and accommodation-sharing apps.
Radke et al. [50] investigated collective information sharing
in ridesharing contexts, where users are generally strangers to
one another, ﬁnding that while precise location is necessary
for pickups and drop-offs, riders were concerned about being
tracked. Riders simultaneously want to provide their personal
information to those who would be in the same car as them, but
also want to know more about their fellow ridesharers; simi-
larly, riders want both to obscure their personal information in
rideshare databases and to maximize transparency about their
use. To tread this ﬁne line, Pham et al. [51] built PrivateRide,
which anonymizes riders and protects their location data while
preserving ride matching accuracy. He et al. [52] go one step
further, attempting to protect riders’ privacy from their fellow
riders as well (e.g., in a Lyft Line or Uber Pool).
There are similar tensions between trust and privacy in other
P2P systems. For example, users who rent out their own cars
want to ensure they have recorded enough car usage data in
the case of wrongdoing, while car rentees are more concerned
about the amount of information that car owners need to
collect from them [53]. There is a negotiation between the
two parties: rentees are willing to disclose more information
under transparently agreed conditions, and owners are willing
to respect rentees’ privacy if they have evaluated them to be
trustworthy. Similarly, in Airbnbs, users are less likely to trust
hosts who reveal less about themselves [54]. In the same vein,
Lutz et al. [55] also found that Airbnb hosts had high levels
of trust in the company itself, which further boosted their
willingness to let strangers into their homes.
E. Takeaways
Sharing practices are inherently socially complex, but this
complexity is compounded by the inﬂexibility of many current
tools. For example, the lack of usable access control interfaces
methods that understand and support social practices often
can lead to behaviors that compromise S&P, e.g., sharing
authentication credentials. These observed behaviors serve
as an indication of the need for more socially informed
approaches to access control.
Resource sharing between romantic partners presents sig-
niﬁcant cognitive and psychological hurdles, especially when
the relationship ends. Work at the family and household level
has mostly covered the fuzziness of users’ access control
policies, especially the desire for individual personalization,
juxtaposed against ﬁnancial limitations of owning one or only
a few devices and accounts. Resource sharing with social
acquaintances and co-workers alike is often constrained by
patchwork policies and lack of coordination, including the
willingness to grant access exceptions to overly strict policies
when there is trust in the relationships. The thread of adapting
to existing limitations via socialization, rather than through
developing new technical solutions that better support these
practices, runs through these three scales.
Work on sharing resources at the public scale is sparse;
whereas the other scales imply some speciﬁc social rela-
tionship between actors, the public scale does not. Strangers
can be renter and rentee, mutual carpool participants, elected
representatives beholden to their constituents, or in any one
of an endless number of relationships that require the implicit
or explicit speciﬁcation and negotiation of access control over
shared resources. However, this diversity of public relation-
ships and public resources shared means that it is impossible
to cover every type of public relationship in one SoK.
Past work has suggested that future designs consider more
complete ways of revoking access or better remind users about
whom they are sharing with to help users detect account
compromise. There is also a gradual shift away from single-
user models, in particular disentangling the user-speciﬁc pref-
erences and ﬁnancial information of an account from access to
the resources of that account [34], [56]. Even when equipped
with interventions that grant greater user ﬂexibility when
dealing with S&P conﬂicts in the home, users resort to existing
tools and social interventions. This may suggest that either user
requirements have not been properly captured, or that nuances
of users desire cannot yet be supported technically.
V. SHARED AND SOCIAL AUTHENTICATION
In this section we consider situations in which authenti-
cation credentials are shared for purposes other than giving
blanket access to a shared digital resource, as well as work
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:32 UTC from IEEE Xplore.  Restrictions apply. 
51867
exploring more socially aware forms of authentication. At mul-
tiple scales, password sharing presents the same cognitive bur-
dens: having to remember all the users who know a password,
changing passwords and updating users, and remembering
which passwords have been reused. Indeed, users commonly
reuse passwords across accounts and instead use informal (or
nonexistent) methods to manage their password usage [20].
This has led to a push to replace passwords in social settings
with a variety of group authentication methods, e.g., those
based on shared knowledge, physical knocking patterns, or
location-based veriﬁcation. The work in this section covers
these methods, as well as any systems directly related to
actually authenticating oneself within an a household, a friend
group, a work environment, or a public neighborhood.
A. Intimate relationships
As aforementioned, sharing passwords and sharing access to
resources are often conﬂated; password sharing behaviors are
covered in Section IV. However, we did not ﬁnd speciﬁc lit-
erature on social authentication by couples, beyond the desire
to share the resources granted access by that authentication.
B. Families and households
Singh et al. [35] found that in rural Aboriginal and Torres
Strait Islander communities in Australia, which often have
poor banking access, ATM cards and PINs alike are often
communally shared within a household. Language and techni-
cal education constraints mean that elderly members of these
communities cannot get cash without sharing their information
with others. Users also often have to remember their parents’
passwords, either because their parents frequently forget them,
or because they helped set up the accounts [41]. We differen-
tiate these instances of password sharing from previous ones
covered in Section V because users in these situations face
obstacles in the actual authentication process (i.e., language
barriers, forgetfulness, less technical knowledge). The primary
motivation of their password-sharing in these situations is not
a desire to share their resources with others, but a challenge
in communicating or proving their identities.
People with visual
impairments (PVIs) rely on similar
sharing of access codes with partners and family members
to authenticate themselves for banking services [57]. As a
potential technical implementation of this phenomenon, Zhang
et al. [58] created WebAlly, a system that help PVIs solicit
help for task-based visual CAPTCHAs by allowing them to
transfer the CAPTCHA to a trusted contact who can help
solve it. The authors found that WebAlly brought PVIs and
their friends closer while preserving the independence of PVIs.
Thus, assistive transfer systems like WebAlly could be a
future design space to help other users who face obstacles
in authentication by themselves.
C. Social acquaintances
A lack of formality around password use and reuse extends
to the workplace: coworkers share credentials by simply telling
other people directly, by sticking Post-It notes on shared
bulletin boards, or logging into their own accounts for other
users to use [47]. When password updates are not adequately
shared with all relevant employees, employees can get locked
out of services required for their jobs. Conversely, when these
passwords don’t get updated often enough, people who leave
the company can still have unauthorized access. This means
that login credentials in these environments no longer simply
represent the identity of an individual in a workplace, nor do
they fully authenticate a user as a member of that workplace.
In place of relying on passwords, a few systems have
attempted to facilitate group authentication using methods that
are more “socially aware” than traditional passwords. One
design that leverages existing social networks is Lineup, a
photo-based social authentication system that asks users to
identify their Facebook friends in photographs to authenticate
their group membership [59]. Yardi et al. found that, while
Lineup seemed simple to set up, the boundaries between social
groups were often not rigidly deﬁned: for example, a socially-
excluded user might still be able to pass low-level security by
identifying group members that they recognize.
Some of these social authentication systems require users to
delineate social group boundaries or members in advance. For
example, Schecter et al. [60] introduced a social authentication
system that allows users to designate in advance a list of
trusted individuals, or trustees, to help them authenticate them-
selves in the event of losing access to their online accounts.
They found that an overwhelming majority of users who called
their trustees could authenticate themselves successfully. Face-
book has a real world implementation of this: Trusted Contacts
[61], which allows users to designate a few of their Facebook
friends to provide account recovery codes.
Existing authentication methods such as PINs and biomet-
rics can be inappropriate for some small social groups where
different members might require different levels of access or
eventually need access revoked. Toomim et al. [62] proposed a
social access control system in which access to shared photos
is restricted to viewers who can answer a question designed
to test mutually shared knowledge between sharer and in-
tended viewer. Another similar group authentication system,
Thumprint [63], uses a shared three-second knock pattern on a
surface with a microphone and accelerometer. In both systems,
sharers can easily come up with questions or knock patterns
that are difﬁcult for strangers to guess. However, Thumprint
might be a bit more socially ﬂexible, since individual member
expressions of the knocks are identiﬁable and distinguishable,
so access can be revoked or limited.
D. The public
At the public scale, there are, once again, fewer exam-
ples of coordinated authentication behaviors. One exception
is Nextdoor, a neighborhood-oriented social media system.
Nextdoor provides authenticates a user’s membership in a
speciﬁc physical community either by delivering a postcard
with a unique code to an address within that neighborhood,
or by allowing neighbors to vouch for each others’ residence
statuses. These methods exploit physical location and users’
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:32 UTC from IEEE Xplore.  Restrictions apply. 
61868
own social networks in order to establish access to the system.
Masden et al. [64] found that this location veriﬁcation system
made users more conﬁdent that others in their local Nextdoor
site were real residents of the community, even if users had
to delegate trust in Nextdoor to perform this veriﬁcation.
E. Takeaways
At
the intimate, family, and social acquaintance scales,
there are necessary conﬂations between sharing resources
and sharing authentication information. These behaviors are
primarily covered in Section V: shared passwords and PINs
unlock shared media resources, devices, and ﬁnances, simul-
taneously building trust in these relationships through these
shared secrets. Beyond a few workarounds that leverage social
relationships, however, there have only been a handful of
systems designed to more smoothly facilitate group or social
authentication without a reliance on passwords. People might
resort to these informal password sharing practices because
they don’t have access to anything better. At the public level,
where the informal sharing of passwords becomes untenable,
users must place trust in authenticating authorities to verify
identities. As such, there is room for future work on grassroots
moderation and volunteer/community oversight mechanisms
instead of reliance on a top-down system.
VI. MANAGING SELF-PRESENTATION
The third major domain of social S&P behaviors is man-
aging self-presentation: the managed sharing that users do of
information about themselves to their social circles. Palen and
Dourish [65] proposed that privacy was a “dynamic, dialectic
process,” building upon privacy regulation theory developed
by Irwin Altman, who suggests that privacy is a continuous
negotiation of boundaries according to circumstance. They
describe three boundaries negotiated in privacy management:
disclosure, i.e., between privacy and publicity; identity, i.e.,
between self and other; and temporality, i.e., between past,
present, and future. Users have varying levels of control over
these boundaries in their presentation of self.
In this section, when we refer to sharing, we speciﬁcally
mean how users share information about
themselves with
others. Generally, people’s willingness to share information
about themselves is dependent on both the type of information
being shared as well as with whom they share the information
[66]. The majority of the work in this section has concerned
the social and public spheres—in other words, social scales in
which connected individuals may have a more tenuous social
connection—and less at the intimate and household scales.
A. Intimate relationships
Within romantic relationships, users disclose and hide per-
sonal information for relationship upkeep and to instill trust in
partners. Park et al. [32] identiﬁed two main themes of sharing
information in order to manage self-presentation within such
relationships: functional (i.e., for convenience or household
maintenance) and emotional (i.e.,
to establish trust or to
improve relationship wellbeing or support). On the ﬂip side,
individuals might hide things in a relationship to conceal what
their partner might consider as wrongdoing, maintain personal
space, or, more good-naturedly, buy surprise gifts.
But
IPV survivors hold a disproportionate burden for
managing their online presences to protect themselves from
abusers. For example, when leaving an IPV situation, the
survivor must block not only the abuser, but also other family
and friends who might jeopardize the victim’s privacy and
safety [36]. Survivors have difﬁculty navigating both these
extensive shared social networks and complex privacy settings
on social media platforms. Survivors also create proﬁles with
false information to protect themselves.
B. Families and households
Within families and households, parents often expose infor-
mation about their children, who have little say in the matter
until they are older. When adults post content online, they
reveal information about their children that can be linked to
other online services to create proﬁles and inferences about
these children; for example, photos of children in adults’
Facebook photo albums can be correlated with ofﬂine data
sources, triggering a “chain reaction of privacy violations”
[67]. But adults do it anyway, weakening their children’s
privacy from strangers and surveillance authorities. Even when
parents use surveillance apps to protect their children’s online
privacy, they inadvertently expose data about their children to
unknown third parties through the apps themselves [68].
C. Social acquaintances
In social networks, users manage interpersonal relationships
as a proxy for controlling the audiences who can see their
posts, since context collapse effects mean users’ direct control