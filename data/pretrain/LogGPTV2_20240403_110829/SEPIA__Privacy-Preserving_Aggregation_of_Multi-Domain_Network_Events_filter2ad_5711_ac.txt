inform the PPs, which event entries have a different key
from the previous window. Then, only comparisons of
updated keys have to be performed and overall complex-
ity is reduced to O(u(n − Tc)s), where u is the number
of changed keys in that window. This requires, of course,
that information on input set dynamics is not considered
private.
4. Entropy Computation: The PPs compute the sum
σ = Pk [(sk)q] and reconstruct σ. Finally, at least
one PP uses σ to (locally) compute the Tsallis entropy
Hq(Y ) = 1
q−1 (1 − σ/Sq).
Figure 4: Algorithm for entropy protocol.
4.2.1 Vector Addition
To support basic additive functionality on timeseries and
histograms, we implement a vector addition protocol.
Each input peer i holds a private r-dimensional input
vector di ∈ Zr
p. Then, the vector addition protocol com-
di. We describe the corre-
sponding SEPIA protocol shortly in Fig. 3. This proto-
col requires no distributed multiplications and only one
round.
putes the sum D = Pn
i=1
4.2.2 Entropy Computation
4.2 Network Trafﬁc Statistics
In this section, we present protocols for the compu-
tation of multi-domain trafﬁc statistics including the ag-
gregation of additive trafﬁc metrics, the computation of
feature entropy, and the computation of distinct item
count. These statistics ﬁnd various applications in net-
work monitoring and management.
The computation of the entropy of feature distributions
has been successfully applied in network anomaly detec-
tion, e.g. [23, 9, 25, 50]. Commonly used feature distri-
butions are, for example, those of IP addresses, port num-
bers, ﬂow sizes or host degrees. The Shannon entropy of
a feature distribution Y is H(Y ) = −Pk pk · log2(pk),
where pk denotes the probability of an item k. If Y is
a distribution of port numbers, pk is the probability of
port k to appear in the trafﬁc data. The number of ﬂows
(or packets) containing item k is divided by the overall
ﬂow (packet) count to calculate pk. Tsallis entropy is
a generalization of Shannon entropy that also ﬁnds ap-
plications in anomaly detection [50, 46].
It has been
substantially studied with a rich bibliography available
in [47]. The 1-parametric Tsallis entropy is deﬁned as:
Hq(Y ) =
1
q − 1(cid:16)1 −Xk
(pk)q(cid:17).
(5)
and has a direct interpretation in terms of moments of
order q of the distribution. In particular, the Tsallis en-
tropy is a generalized, non-extensive entropy that, up to
a multiplicative constant, equals the Shannon entropy for
q → 1. For generality, we select to design an MPC pro-
tocol for the Tsallis entropy.
i=1 si
S Pn
Entropy Protocol A straight-forward approach to com-
pute entropy is to ﬁrst ﬁnd the overall feature distribu-
tion Y and then to compute the entropy of the distribu-
In particular, let pk be the overall probability of
tion.
item k in the union of the private data and si
k the local
count of item k at input peer i. If S is the total count of
the items, then pk = 1
k. Thus, to compute the
entropy, the input peers could simply use the addition
protocol to add all the si
k’s and ﬁnd the probabilities pk.
Each input peer could then compute H(Y ) locally. How-
ever, the distribution Y can still be very sensitive as it
contains information for each item, e.g., per address pre-
ﬁx. For this reason, we aim at computing H(Y ) with-
out reconstructing any of the values si
k or pk. Because
the rational numbers pk can not be shared directly over
a prime ﬁeld, we perform the computation separately on
private numerators (si
k) and the public overall item count
S. The entropy protocol achieves this goal as described
in Fig. 4. It is assured that sensitive intermediate results
are not leaked and that input and privacy peers only learn
the ﬁnal entropy value Hq(Y ) and the total count S. S
is not considered sensitive as it only represents the total
ﬂow (or packet) count of all input peers together. This
can be easily computed by applying the addition protocol
to volume-based metrics. The complexity of this proto-
col is r log2 q multiplications in log2 q rounds.
4.2.3 Distinct Count
In this section, we devise a simple distinct count protocol
leaking no intermediate information. Let si
k ∈ {0, 1} be
a boolean variable equal to 1 if input peer i sees item k
and 0 otherwise. We ﬁrst compute the logical OR of the
boolean variables to ﬁnd if an item was seen by any in-
put peer or not. Then, simply summing the number of
variables equal to 1 gives the distinct count of the items.
According to De Morgan’s Theorem, a∨b = ¬(¬a∧¬b).
1. Share Generation: Each input peer i shares its negated
local counts ci
k = ¬si
k among the PPs.
k] ∧ [c2
k] ∧ . . . [cn
2. Aggregation: For each item k, the PPs compute [ck] =
[c1
k ]. This can be done in log2 n rounds.
If an item k is reported by any input peer, then ck is 0.
3. Counting: Finally, the PPs build the sum [σ] = P[ck]
over all items and reconstruct σ. The distinct count is
then given by K − σ, where K is the size of the item
domain.
Figure 5: Algorithm for distinct count protocol.
This means the logical OR can be realized by performing
a logical AND on the negated variables. This is conve-
nient, as the logical AND is simply the product of two
variables. Using this observation, we construct the pro-
tocol described in Fig. 5. This protocol guarantees that
only the distinct count is learned from the computation;
the set of items is not reconstructed. However, if the in-
put peers agree that the item set is not sensitive it can
easily be reconstructed after step 2. The complexity of
this protocol is (n − 1)r multiplications in log2 n rounds.
5 Performance Evaluation
In this Section we evaluate the event correlation proto-
col and the protocols for network statistics. After that we
explore the impact of running selected protocols on Plan-
etLab where hardware, network delay, and bandwidth
are very heterogeneous. This section is concluded with
a performance comparison between SEPIA and existing
general-purpose MPC frameworks.
We assessed the CPU and network bandwidth require-
ments of our protocols, by running different aggregation
tasks with real and simulated network data. For each
protocol, we ran several experiments varying the most
important parameters. We varied the number of input
peers n between 5 and 25 and the number of privacy
peers m between 3 and 9, with m < n. The experiments
were conducted on a shared cluster comprised of sev-
eral public workstations; each workstation was equipped
with a 2x Pentium 4 CPU (3.2 GHz), 2 GB memory, and
100 Mb/s network. Each input and privacy peer was run
on a separate host. In our plots, each data point reﬂects
the average over 10 time windows. Background load due
to user activity could not be totally avoided. Section 5.3
discusses the impact of single slow hosts on the overall
running time.
5.1 Event Correlation
For the evaluation of the event correlation protocol,
we generated artiﬁcial event data. It is important to note
that our performance metrics do not depend on the actual
3 privacy peers
5 privacy peers
7 privacy peers
9 privacy peers
]
s
[
e
m
i
t
i
g
n
n
n
u
r
 200
 150
 100
 50
3 privacy peers
5 privacy peers
7 privacy peers
9 privacy peers
 250
 200
 150
 100
 50
]
B
M
[
t
n
e
s
a
t
a
d
 0
 5
 10
 15
 20
 25
input peers
 0
 5
 10
 15
 20
 25
input peers
 300
 250
 200
 150
 100
 50
]
s
[
e
m
i
t
i
g
n
n
n
u
r
 0
 30
 60
 90
 120
 150
events per input peer
(a) Average round time (s = 30).
(b) Data sent per PP (s = 30).
(c) Round time vs. s (n=10, m=3).
Figure 6: Round statistics for event correlation with Tc = n/2. s is the number of events per input peer.
values used in the computation, hence artiﬁcial data is
just as good as real data for these purposes.
Running Time Fig. 6 shows evaluation results for event
correlation with s = 30 events per input peer, each with
24-bit keys for Tc = n/2. We ran the protocol in-
cluding weight and key veriﬁcation. Fig. 6a shows that
the average running time per time window always stays
below 3.5 min and scales quadratically with n, as ex-
pected. Investigation of CPU statistics shows that with
increasing n also the average CPU load per privacy peer
grows. Thus, as long as CPUs are not used to capacity,
local parallelization manages to compensate parts of the
quadratical increase. With Tc = n − const, the running
time as well as the number of operations scale linearly
with n. Although the total communication cost grows
quadratically with m, the running time dependence on
m is rather linear, as long as the network is not satu-
rated. The dependence on the number of events per input
peer s is quadratic as expected without optimizations (see
Fig. 6c).
To study whether privacy peers spend most of their
time waiting due to synchronization, we measured the
user and system time of their hosts. All the privacy peers
were constantly busy with average CPU loads between
120% and 200% for the various operations.3 Communi-
cation and computation between PPs is implemented us-
ing separate threads to minimize the impact of synchro-
nization on the overall running time. Thus, SEPIA proﬁts
from multi-core machines. Average load decreases with
increasing need for synchronization from multiplications
to equal, over lessT han to event correlation. Never-
theless, even with event correlation, processors are very
busy and not stalled by the network layer.
Bandwidth requirements Besides running time,
the
communication overhead imposed on the network is an
important performance measure. Since data volume is
dominated by privacy peer messages, we show the av-
erage bytes sent per privacy peer in one time window
in Fig. 6b. Similar to running time, data volume scales
roughly quadratically with n and linearly with m.
In
addition to the transmitted data, each privacy peer re-
ceives about the same amount of data from the other in-
put and private peers. If we assume a 5-minute clocking
of the event correlation protocol, an average bandwidth
between 0.4 Mbps (for n = 5, m = 3) and 13 Mbps
(for n = 25, m = 9) is needed per privacy peer. Assum-
ing a 5-minute interval and sufﬁcient CPU/bandwidth re-
sources, the maximum number of supported input peers
before the system stops working in real-time ranges from
around 30 up to roughly 100, depending on protocol pa-
rameters.
5.2 Network statistics
For evaluating the network statistics protocols, we
used unsampled NetFlow data captured from the ﬁve
border routers of the Swiss academic and research net-
work (SWITCH), a medium-sized backbone operator,
connecting approximately 40 governmental institutions,
universities, and research labs to the Internet. We ﬁrst
extracted trafﬁc ﬂows belonging to different customers
of SWITCH and assigned an independent input peer to
each organization’s trace. For each organization, we then
generated SEPIA input ﬁles, where each input ﬁeld con-
tained either the values of volume metrics to be added or
the local histogram of feature distributions for collabora-
tive entropy (distinct count) calculation. In this section
we focus on the running time and bandwidth require-
ments only. We performed the following tasks over ten
5-minute windows:
1. Volume Metrics: Adding 21 volume metrics con-
taining ﬂow, packet, and byte counts, both total and
separately ﬁltered by protocol (TCP, UDP, ICMP)
and direction (incoming, outgoing). For example,
Fig. 10 in Section 7.2 plots the total and local num-
ber of incoming UDP ﬂows of six organizations for
an 11-day period.
3 privacy peers
5 privacy peers
7 privacy peers
9 privacy peers
]
s
[
e
m
i
i
t
g
n
n
n
u
r