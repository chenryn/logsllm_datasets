switches are great when flow-state fits in the limited SRAM space,
and (3) nothing handles the most demanding workloads well. Ideally,
if we could build an architecture that enables switches to utilize
more memory with cheaper DRAM (like servers) in a scalable way,
it would make programmable switches more broadly applicable
and serve the extreme regime cost-efficiently.
3 Design Space and Challenges
Building on the above analysis, we explore if and how we can po-
tentially leverage external DRAM that already exists in the network.
Now, there are two places where we can naturally find available
DRAM near the switch ASIC:
(1) Switch’s control plane. The control plane has a few GB of
DRAM to manage the control plane data. An ASIC could access
the DRAM via the PCIe channel between the ASIC and the control
plane CPU. Note that the PCIe channel has a limited bandwidth
which is lower than the ASIC’s per-port bandwidth. While this low
and fixed bandwidth is enough to process occasional control plane
traffic, it cannot support higher traffic rates (which can cause high
memory access rate) without significant hardware modifications.
Also, although in theory, it is possible to add additional DRAM to
the control plane, in practice, the size is fixed at design time. (e.g.,
8 GB in the switch in our testbed [14]).
(2) On-board off-chip DRAM. Some switch ASIC vendors have
added custom off-chip DRAM on the switch board [8]. This DRAM
is used for custom tasks such as buffering packets or storing specific
lookup tables. Similar to the control plane case, the memory access
bandwidth and size is fixed at design time, which makes it very
hard to scale without chip modification. Note that while a future
switch ASIC architecture might provide on-board off-chip DRAM
with larger size and higher bandwidth, it requires new interfaces
and mechanisms to access DRAM from a programmable pipeline.
We discuss this further in §7.
We observe that two options above do not scale in terms of
memory access bandwidth and capacity today, which are typically
fixed at hardware design time. We believe that support for scaling
becomes more critical as the total amount of traffic (both in terms of
traffic volume and number of concurrent flows) each switch needs
to process increases [12, 24].
(a) Naïve design and perfor-
mance bottlenecks (B1 and B2).
(b) TEA enabling to access ex-
ternal DRAM in the data plane
without CPU involvement.
Figure 1: Comparison between RPC-based naïve design and
TEA to access external DRAM.
Our vision. In this paper, we take an alternative approach that
leverages DRAM in commodity servers in NFV clusters in a scalable
way. A typical NFV cluster (either inside the cloud or at the edge)
consisting of multiple racks of servers [19, 29, 32, 59] already has
several tens of GB of DRAM on each server. If we can reserve some
portion of DRAM and let the switch ASIC located at the top-of-rack
(ToR) access it, the ASIC could make use a large per-flow table,
which would not be possible with on-chip SRAM today.
Using a single server could still limit the access bandwidth, i.e.,
minimum of network bandwidth between the ASIC and the server,
and PCIe bandwidth in the server. However, we can leverage mul-
tiple servers to increase the aggregate bandwidth. Also, while the
ASIC uses DRAM in servers, CPUs on the servers can simultane-
ously serve other tasks such as compute-intensive NFs, including
traffic en/decryption or payload inspection, which cannot be sup-
ported by switches today.
If this can be realized, programmable switches can become an
effective way to serve high traffic rate involving a large number of
concurrent flows, and thus work for all the regimes we considered
earlier. However, realizing this vision has key design and imple-
mentation challenges, as we describe next.
3.1 Challenges
To understand why it is challenging to realize this vision, let us
consider a natural starting point based on prior work using tradi-
tional Remote Procedure Call (RPC) mechanisms [41, 57] (Figure 1a).
Specifically, the switch ASIC sends and receives RPC requests and
responses via the switch control plane to avoid adding complexity
(e.g., state management for reliable transport) to the data plane.
While this is functionally correct, there are three fundamental bot-
tlenecks:
(1) High and unpredictable latency. A table lookup can result in
high latencies because of the latency between the ASIC, the control
plane CPU, and the server CPU (over the network), which can take
a few hundred microseconds. Moreover, the uncertainty introduced
by the scheduling logic on the switch control plane and server CPU
can introduce jitter and high variability [46].
(2) Limited memory access bandwidth. The lookup throughput
is constrained by the minimum of the bandwidth between ASIC-
to-the-control-plane-CPU and control-plane-CPU-to-server-CPU.
Both bandwidths are typically very limited (e.g., PCIe bandwidth
between the ASIC and the control plane is a few tens of Gbps which
Switch ASIC (Data plane)SRAMSwitch boardSwitch control planeSingleServerDRAMPipeline stagesB1B1B2MultipleServersDRAMSwitch ASIC (Data plane)SRAMSwitch boardSwitch control planePipeline stagesSIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Daehyeok Kim et al.
is much lower than a few hundreds of Gbps of ASIC’s per-port
bandwidth available today) and fixed at hardware design time.
(3) Availability. If the server fails or the network link between the
control plane and the server becomes unavailable, the switch cannot
lookup tables on external DRAM, degrading NF performance.
We observe that the root causes of these problems are (1) the
involvement of CPUs at the control plane and the server and (2) the
use of the single server (Figure 1a). This motivates us to ask: Is it
possible to allow the switch ASIC to access external DRAM purely
in the data plane and without servers’ and the control plane’s CPU
involvement in a scalable way across multiple servers? To answer
this question, we must address the following challenges:
C-1. Data-Plane External DRAM Access. Switch ASICs typi-
cally do not have direct external DRAM access capability. Is it
possible to enable it without hardware modifications?
Even if the ASIC can somehow directly access external DRAM, it
can incur a few microseconds of latency which is an order of mag-
nitude slower than its packet processing speed. This long latency
creates the following two challenges:
C-2. Single Round-Trip Table Lookups. If we use conventional
hashing (e.g., cuckoo hashing [58]) for storing and locating table
entries in external DRAM, multiple DRAM accesses may be required
to lookup an entry. Is it possible to make the ASIC do a table lookup
in a single round-trip to DRAM without involving server CPUs and
hardware modifications?
C-3. Packet Processing. The ASIC must be able to continue pro-
cessing the packet (e.g., modifying header fields) after completing
the lookup from external DRAM. In the meantime, it also needs to
keep processing subsequent packets in the pipeline. How can we
manage the packet until the lookup completes?
C-4. Load-Balanced Bandwidth Use. Although using multiple
servers (i.e., adding network links) increases external DRAM access
bandwidth, a subset of links could become overloaded due to the
access locality (i.e., most of memory accesses are destined to the
subset of servers’ DRAM). This makes it hard to utilize available
link bandwidth. How can we ensure that memory access loads are
balanced across servers?
C-5. Tolerating Server Churn. Access to external DRAM be-
comes unavailable when a server fails or the network becomes
congested (causing packet drops). How can we detect and react to
these events quickly to minimize performance degradation?
4 TEA Design
To address the above challenges, we design TEA, a virtual table
abstraction for tables stored across local SRAM and external DRAM.
Using the abstraction, NFs running on a ToR programmable switch
can perform key-based (e.g., 5-tuple of an IP packet) table lookups,
and TEA fetches the corresponding entries either from switch-
local SRAM or remote DRAM. When it accesses DRAM, it delays
the processing of the packet corresponding to the lookup request
without blocking the rest of the packet processing pipeline. TEA’s
lookup response handler resumes the delayed packet’s processing
when DRAM lookup completes.
Figure 2 illustrates this workflow. TEA provides a set of APIs
implemented in P4, a language to program NFs on programmable
switches, and exposes each component as a module in P4 [17, §13].
Figure 2: NFs implemented in P4 can be extended with TEA
P4 API to look up tables across external DRAM and on-chip
SRAM. The control plane is (dotted lines) involved when es-
tablishing a TEA channel.
This enables developers to easily integrate TEA with their NF im-
plementations in P4. Once developers write their NFs using TEA
components, the unmodified P4 compiler generates a binary of
TEA-enabled NFs that can be loaded to the data plane and control
plane APIs that can be used for configuring TEA components in
the data plane.
TEA builds on the following five key ideas to address the chal-
lenges described in §3.1:
1. Leveraging ASIC programmability to enable simplified RDMA in
the data plane (§4.1).
2. Repurposing bounded linear probing to guarantee hash table
lookups in a single-round trip to external DRAM (§4.2.1).
3. Offloading packet store to external DRAM to enable asynchronous
lookups (§4.2.2).
4. Leveraging the small-cache theory [30] to scale out the through-
put (§4.3).
5. Repurposing ASIC’s hardware capabilities to detect and react to
sever availability changes in the data plane (§4.4).
4.1 DRAM Access in the Data Plane
To access external DRAM, we choose RDMA, which is quite com-
mon in service provider deployments [34, 55]. In comparison to RPC,
RDMA is an attractive option because it is designed specifically for
predictable performance memory access. It provides hardware sup-
port for a set of low-level memory operations such as read, write,
and a few atomic operations (e.g., fetch-and-add). Since it does not
involve the server CPU for either the memory access or the reliable
transport of messages, RDMA reduces both memory access latency
down to ≈2 µs, and delay jitter, and allows the use of the CPU for
other compute-intensive tasks.
Challenges of using RDMA from switch ASICs. However, we
still need to address two practical problems: (1) Is it feasible to gen-
erate RDMA packets purely in the switch data plane when DRAM
access is needed? (2) Can we support reliable RDMA transport
NF impl.Switch ASIC + TEA P4APIDRAMOn-chip SRAMPipeline stagesServers  BinaryTEAchannelP4 CompilerDeveloperControl planeCtrl. APITEA controllerTEA: Enabling State-Intensive Network Functions on Programmable Switches
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
PAUSE request, it has to buffer packets until the NIC allows to send
packets. We adopt this option in our prototype implementation in
addition to our simple switch-side flow control to cope with the
current NIC configuration as we describe in §5.3.4 Alternatively,
we can also configure a higher QoS-level for our RDMA traffic over
lossy fabric [18]. These options allow us to enable RDMA between
the ASIC and DRAM servers with a minimal amount of RDMA
context metadata and without complex retransmission schemes.
Specifically, it only needs to maintain a QPN (4 bytes) and tracks a
packet sequence number (4 bytes) and the number of outstanding
requests (2 bytes) for each queue-pair, which are used when crafting
RDMA requests for the QP. Maintaining such metadata in the data
plane requires only up to a few KBs of SRAM in total.
4.2 TEA-Table: Lookup Table Structure
The design of TEA’s table data structure, TEA-Table, addresses
two key issues: (1) how to complete a lookup in a single round-trip
to external DRAM and (2) how to defer processing of the current
packet until the lookup completes and continue processing other
packets without blocking. TEA-Table repurposes a data structure
that was originally designed for improving cache hit rates in soft-
ware switches [67] to achieve single RTT lookups and incorporates
remote packet buffers within the data structure to accommodate
deferred packet processing.
Single Round-trip Lookups: RDMA only provides low-level
4.2.1
memory operations such as read and write, using virtual memory
addresses. However, NFs require richer key-based lookup interface
to retrieve table entries with keys (e.g., an IP 5-tuple for an address
mapping table in NAT) from DRAM. Thus, TEA must map a key
to a virtual memory address. The challenge is that due to relatively
large DRAM access latency (≈2 µs), we must be able to locate and
fetch the entry in a single DRAM read.
Strawman solutions. At first glance, it appears we can use tra-
ditional hashing techniques. Indeed, many modern switch ASICs
adopt variations of cuckoo hashing [58] for exact-match lookups in
SRAM as it guarantees constant-time lookup. A caveat, however, is
that each lookup requires multiple memory accesses. This means,
with two-way cuckoo hashing, each lookup requires two indepen-
dent memory reads. While this is feasible with fast parallel lookups
on SRAM, our experience suggests that extending it to external
DRAM via RDMA channel would either significantly degrade the
performance of NFs or make the data plane logic complicated. To
reduce multiple DRAM accesses in cuckoo hashing, we need to
know precisely which of the two hash tables to access for a given
key. Recent work, EMOMA [60], uses additional Bloom filters [21]
in SRAM to address this issue. By checking for membership, the
query can be directed to the appropriate hash table. Since there is a
risk of false positives in the filter, EMOMA has a more complex item
insertion that checks if inserting a new entry causes false positives.
Unfortunately, this makes it impractical.5
4In our experiments, we observe that our switch-side flow control mechanism prevents
a NIC buffer from being overflowed before the NIC generates PAUSE frames.
5In our simulation, it takes several hours to insert just a few tens of million entries and
implementing BFs for such a scale consumes other resources across multiple packet
processing stages in the ASIC. Since such a slow insertion speed with a non-negligible
amount of resource consumption makes this approach impractical, we do not consider
this design.
Figure 3: Switch ASIC generates RDMA requests by adding
RoCE headers on incoming packets and parse RDMA re-
sponses without specialized capabilities for RDMA. To main-
tain reliable channels, the ASIC maintains per-QP and per-
server metadata.
within the switch data plane? (i.e., can switch ASICs maintain the
necessary per-connection RDMA context and protocols?)
Our approach. While it may be hard to implement reliable RDMA
in general on a programmable switch, we observe that we do not
need fully functional RDMA for our use case. Our key insight here
is that the programmable features of modern switch ASICs together
with the scoped deployment model of TEA enable us to implement
a small but sufficient subset of RDMA features we need.
1) Generating RDMA packets: With respect to the first sub challenge,
we note that the most popular RDMA technology today is RoCE
(RDMA over Converged Ethernet) protocol [37, 38], where RDMA
requests and responses are regular Ethernet packets with RoCE
headers. This means that ASICs can generate valid RDMA requests
by crafting RoCE packets without needing any RDMA-specific
hardware components.
Figure 3 illustrates this high-level idea. When the data plane
needs to access DRAM, it crafts an appropriate RDMA packet by
adding a series of specific RoCE headers to the incoming packet.
This include Ethernet headers, global route headers, base transport