As we talk about parallel computing in various contexts, it might be handy to
keep in mind  
all the high level moving parts. Here are some that come to mind, just to get
the ball rolling.
Hardware
  * Shared Memory Machines (Threads: See #1790, #1802)
  * Distributed Memory (especially networks of shared memory) (spawns, MPI?, migrating threads?)
  * GPUs? (we do/don't believe these are here to stay)
Tools
  * Nice Graphical Performance Tools and Instrumentation (My first wish: just  
have Green (working) / Red (idle) on every processor easily every time I do
parallel julia)
Programming Models
  * Persistent Data as in Global Array Syntax Darrays (Star-P and others)
  * CILK/Spawns
  * Map/reduce etc.
Communication Layers
  * ZMQ, MPI, sockets
Schedulers
  * ??
  * Does one master know all state?
  * Can our serial model be in conflict with various parallel models?
Libraries
  * Scalapack, parallel FFTW?, sparse matrices, PETSc , (star-p like to compare  
with more pure julia methods?)