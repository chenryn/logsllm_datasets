3.1 Dynamic analysis and clustering
Each sample selected for dynamic analysis is executed in an in-
strumented sandbox environment for a ﬁxed amount of time (cur-
rently four minutes). The output of dynamic analysis is the set of
behavioral features βs observed during the execution of s. Behav-
ioral features are a representation of program behavior introduced
in [14]. Each behavioral feature represents a speciﬁc action per-
formed by the analyzed program on a speciﬁc operating system
or network entity, such as the creation of ﬁle C:\system32\
svcshost.exe, or an HTTP request to www.example.com.
In the behavioral clustering phase, βs is fed to the scalable behav-
ioral clustering techniques from Bayer et al. [14]. As a result, we
identify the behavioral cluster Cs to which s is assigned.
The cluster prediction phase receives the label Cs and incremen-
tally updates its classiﬁer. Furthermore, βs and Cs are fed back to
the cluster scoring phase, where the cluster score for cluster Cs is
updated based on the value of the observed behavior βs with re-
spect to the application-speciﬁc scoring function v. As a result, the
peHashPEsectionsimporteddllsAVlabelsstaticclusterpackinglevelFeature ExtractionstaticfeaturesClusterPredictionClusterScoringDynamicAnalysisclusterprobabilitiesmalwaresamplesbehavioralfeaturescluster for sampleanalysis queuetopsampleBehavioralClusteringsamplescorebehavioralfeaturessample scores for all samples are incrementally updated, and the
next sample can be selected for analysis.
3.2 Feature Extraction
The goal of the feature extraction phase is to collect all the infor-
mation on malware samples that can be used to classify it. There-
fore, in this phase we extract all the characteristics of each mal-
ware sample that can be efﬁciently obtained from static analysis.
To distinguish them from the behavioral features resulting from the
dynamic analysis of a sample, we refer to these characteristics as
static features. The output of this phase is, for each candidate sam-
ple s, a set of static features φs. The feature space Φ = ∪s∈χφs
grows as new candidate samples are processed and new static fea-
tures are discovered.
peHash. Unlike previous approaches to sample selection, FORE-
CAST assumes that minor variants of previously analyzed malware
samples are worth analyzing. Knowing that a candidate sample
is similar to previously analyzed samples can be extremely useful
when attempting to predict its behavior. Therefore, we include a
sample’s peHash [41] in the static features. peHash uses structural
information from an executable ﬁle’s headers and the Kolmogorov
complexity of code sections to compute a hash value that should
remain constant across polymorphic malware variants.
Static cluster. Similarly, we take advantage of static malware clus-
tering techniques from [23]. These techniques compute the dis-
tances between the code signals – essentially a bigram distribu-
tion over a code section – of binaries to group them into clusters.
The authors also introduce techniques to statically detect the level
of packing used by a sample. We call these clusters static clus-
ters. Both of these characteristics – packing level and static cluster
membership – are mapped to individual static features.
PE Header. A variety of information from a binary’s Portable Exe-
cutable (PE) header is already processed by peHash and [23], and is
thus already represented in the feature space. We extract two addi-
tional groups of features from the PE header. These are the names
of the imported DLLs and the PE section names.
Antivirus Labels. We obtain anti-virus results for all candidate
samples from the VirusTotal service [7]. VirusTotal scans each
sample using 39 AV engines, and for each detection provides the
name of the engine that detected the sample and the label it as-
signed to it. To avoid an excessively large number of AV label fea-
tures we discard the most speciﬁc part of the AV labels (indicating
the malware variant) and normalize the labels to a canonical form –
e.g. Trojan/Downloader.Carberp.n becomes carberp.
Submitter Information. For each sample that is submitted to the
sandbox, the hostname of the machine it has been submitted from is
logged. Depending on how a malware analyst collects samples and
on the types of malware she is interested in, the samples she sub-
mits may be skewed towards speciﬁc malware classes. We there-
fore map each distinct hostname from which a sample was submit-
ted to a static feature.
3.3 Cluster Prediction
The goal of the cluster prediction phase is to predict to which
behavioral cluster a candidate sample belongs, before executing it.
Cluster prediction therefore aims to establish a mapping between a
sample’s static features φs and the behavioral cluster Cs to which it
will be assigned based on its behavioral features. For this task, we
take a supervised learning approach and train a classiﬁer by provid-
ing it with a labeled dataset consisting of (φs, Cs) for each sample
s that has already been subject to dynamic analysis and clustering.
For this, we require an appropriate classiﬁcation algorithm, consid-
ering our requirements and the properties of our datasets:
• High-dimensional feature space: |Φ| can be very large.
• Sparse data: Every sample only exhibits a small subset of the
possible features (|φs| << |Φ|).
Individual features may occur
infrequently.
• Incremental operation: The results of dynamic analysis and clus-
tering should have an immediate effect on following predictions.
Furthermore, the size of the feature space and the number of clus-
ters may change over time.
• Fast prediction: We need prediction results on all candidate sam-
ples before we can select the best ones for analysis.
scalar product with a weight vector (cid:126)w: y = (cid:80)
For FORECAST we use linear classiﬁcation in combination with
the conﬁdence-weighted (CW) learning scheme of Dredze, Cram-
mer and Pereira [18].
Linear classiﬁcation. A set of static features is represented as a bi-
nary feature vector (cid:126)x, where every possible feature is either present
(one) or absent (zero) for a speciﬁc sample. A linear classiﬁer de-
termines a margin y for a given feature vector (cid:126)x by computing the
i xiwi. The lin-
ear classiﬁcation process can be visualized as splitting the feature
space into two sections with a hyperplane that is determined by the
weight vector (cid:126)w, where the sign of the margin y tells us on which
side of the hyperplane an input vector is, thus distinguishing two
classes. The absolute value of the margin |y| can be interpreted as
the conﬁdence in this classiﬁcation.
Linear classiﬁers can handle high dimensional feature spaces and
perform especially fast on sparse binary feature spaces. This is
because for binary spaces, the computational cost of computing the
scalar product (cid:126)w · (cid:126)x is proportional to |φ|, rather than to the total
number of features in the feature space |Φ|.
Conﬁdence-weighted learning. The effectiveness of a linear clas-
siﬁer depends on the algorithms used to train it. In online learn-
ing, the training instances are supplied one after the other. Train-
ing algorithms for linear classiﬁers use update rules that adjust
the weights at each iteration t based on the current weights and
a function g of the training instance features and label: (cid:126)wt+1 =
(cid:126)wt + g((cid:126)xt, yt). If the feature space grows, weights can simply be
added to the classiﬁer.
Sparse data may pose a problem to such an algorithm. The rea-
son is that typically weights are updated only if the corresponding
feature is set in (cid:126)x. Therefore, the weights for features that oc-
cur seldom are based on much less information than weights for
frequently-occurring features. CW learning addresses this problem
by maintaining conﬁdence information for each feature depending
on how often it occurred in training. Conﬁdence is modeled with
a Gaussian distribution N (wi, Σi) for each feature i. The larger
Σi is, the smaller is conﬁdence in wi and the more aggressively
the distribution will be updated by g. For a full discussion of this
classiﬁer and the update function g we refer the reader to [18].
Multiclass prediction. The linear classiﬁers we have discussed
so far can distinguish two labels. FORECAST however needs to
predict to which of several behavioral clusters a sample belongs.
Such a multi-class problem can be decomposed into multiple bi-
nary problems. These are then solved by a network of n binary
classiﬁers. As a last step, a single multi-class label has to be de-
rived from the n binary results. For this, we can use Error Correct-
ing Output Codes (ECOC) [10]. Considering the results of the n
classiﬁers as an n-length binary codeword, the distances between
the codeword from the classiﬁcation and the codewords of each la-
bel are calculated. The prediction result is the label for which the
distance is minimized.
One way of performing multi-class classiﬁcation using ECOC is
to use a binary classiﬁer for each pair of labels, where each clas-
siﬁer is trained to distinguish between these two labels. This ap-
proach is not applicable for FORECAST, because it would require
us to train |C|2/2 classiﬁers. With over a thousand clusters, this is
clearly problematic. This approach is also referred to as pairwise
coupling [20] or one-versus-one (OVO) classiﬁcation. Instead we
use a one-versus-all (OVA) approach, which requires only a single
classiﬁer per label. Each classiﬁer is trained to distinguish between
its assigned label and the “rest”, i.e. all other labels. For each train-
ing instance the binary classiﬁer of the correct class is trained with
+1 and all other classiﬁers with −1.
FORECAST’s cluster prediction uses one-versus-all CW learn-
ing. An advantage of this choice is that, whenever a new behav-
ioral cluster C emerges from the analysis results, we can add a new
classiﬁer and train it to recognize C. For this, we train the new
classiﬁer with all past samples. The classiﬁers for all other sam-
ples, however, do not need to be modiﬁed. The weights used for
classiﬁcation are stored in a matrix w, where wC,i is the weight of
feature i for cluster (classiﬁer) C.
Probability estimates. Just picking the top predicted cluster and
proceeding with the cluster scoring would mean to discard impor-
tant information – the conﬁdence in the prediction, given by the
margins of each classiﬁer. Based on that output it is possible to
calculate probability estimates for each label. For ECOC models,
a generic approach is given in [21]. Here we use a simple ap-
proach from the LIBLINEAR project [19] for OVA classiﬁcation.
The probability estimate for label C is computed as an exponential
1+e−yC .
function of the margin yC of the corresponding classiﬁer
Cluster size threshold. As we will show, the clusters in our be-
havioral malware clustering vary a lot in terms of size, with a few
very large clusters and a large number of clusters containing only
a few samples. Clearly, we do not want to train tens of thousands
of OVA classiﬁers to recognize clusters that contain only one sam-
ple. First of all, this would signiﬁcantly slow down FORECAST.
Furthermore, a classiﬁer that was provided during training an ex-
tremely small number of positive training instances would be un-
likely to provide good results. Therefore, we select a minimum
cluster size threshold θ and group all samples belonging to clusters
with |C| < θ in a single “other” cluster O. We do not, however,
train a classiﬁer for O. The reason is that samples in O have noth-
ing in common and thus their static features will vary wildly. As a
probability estimate for O we therefore simply use the ratio |O|/|α|
of the number of samples in O to the total number of analyzed sam-
ples.
As a ﬁnal step all probability estimates are normalized so that
they add up to one. The output of the cluster prediction phase is the
cluster probability matrix p, where ps,C represents the probability
that sample s belongs to behavioral cluster C.
3.4 Cluster Scoring
1
Given a measure of the value of the analysis results produced
by the sandbox, we can evaluate which behavioral clusters produce
more valuable results, and try to select analysis samples that will
likely fall into those clusters. For this, we calculate a cluster score
for each behavioral cluster.
As discussed in our formulation of the sample selection prob-
lem in Section 2, the value of analysis results is measured by an
application-dependent scoring function v(α), which computes the
aggregate value of the analysis results for the set of analyzed sam-
ples α. As discussed in Section 3.1, the results of dynamic anal-
ysis of a sample s is the set βs of behavioral features observed
during the sample’s execution. The scoring function is therefore
calculated as v(α) = v({βs : s ∈ α}). Depending on the target
application, only a subset of the observed behavioral features may
be of interest. For instance, if the goal of analysis is to identify
malware C&C servers, only features related to network trafﬁc are
relevant, while features representing interaction with the local sys-
tem can be ignored. The scoring functions we considered in this
work simply measure the total number of relevant behavioral fea-
tures observed. For this, each scoring function has an associated
ﬁlter f (b) → {0, 1} that returns 1 if feature b is relevant. We then
s∈α βs. Note
that assigning different weights to different behavioral features, to
express the fact that some features may be more valuable than oth-
ers, would be a straightforward generalization of this approach.
compute v = |{b ∈ B : f (b) = 1}|, with B = (cid:83)
In this work, we use the following two scoring functions, tar-
geted at the applications discussed in Section 2.1.
Identifying C&C servers To assist in the task of identifying and
blacklisting C&C servers, we introduce the network endpoints scor-
ing function. This scoring function aims to measure the number
of potential C&C servers contacted during dynamic analysis. The
features relevant for C&C server detection are those indicating net-
work communication with an IP address, and those indicating a
DNS request for resolving a domain. A simple scoring function
could therefore count the number of distinct network endpoints
(IP addresses and DNS names) contacted by the analyzed samples.
However, not all network trafﬁc observed is related to C&C, or to
malware’s auto-update functionality (which can be seen as a type
of C&C where commands are delivered in the form of executable
code). Therefore, we employ a number of additional ﬁlters to make
the network endpoints scoring function a more reliable measure of
the amount of C&C servers contacted.
• fast-ﬂux: C&C infrastructure is sometimes hosted on fast-ﬂux
networks, where the same domain name resolves to a rapidly-changing
set of IPs. These IPs typically belong to infected hosts that tem-
porarily serve as C&C servers. In this case, the information on the
contacted IP addresses is of little value. Therefore, we consider IP
addresses for the network endpoints score only if the malware did
not obtain them from a DNS query.
• portscan: Many malware samples include self-propagation com-
ponents that scan the internet for targets before attempting to com-
promise them. Clearly, we do not want to include these hosts in
the network endpoints score. For this, we ﬁrst discard connections
to a small number of ports that are known to be typical exploit tar-
gets, such as TCP ports 139 and 445, used by Windows ﬁle and
directory sharing services. Furthermore, we use the Bro IDS [32]
to detect port and address scans, and discard connections that are
part of detected scans.
• liveness: C&C endpoints that are not actually available are of lit-
tle interest. We therefore ﬁlter out endpoints that could not be suc-
cessfully contacted by the malware. To decide on whether a C&C
endpoint is live, we rely on the known semantics of a few protocols
commonly used for C&C, and fall back to a default heuristic for
other kinds of trafﬁc:
• HTTP: we only consider servers that responded with a success-
ful status code (that is, 200-299) to at least one request.
• IRC: we only consider a server if the malware sent or received
a private message or if it successfully joined a channel and sent
or received a message on that channel.
• FTP: we only consider a server if the malware successfully
logged in.
• Other: we only consider endpoints where a connection was suc-
cessfully established (for TCP) and where the server sent back
actual payload.
• clickbots: Clickbots are malware samples that include function-
ality to automatically visit advertisement links on target websites.
Their goal is to fraudulently generate advertisement revenue for
these websites. Due to the dynamic and tiered nature of advertise-
ment networks, this can lead to a signiﬁcant number of network
endpoints being contacted during analysis. Since these endpoints
are not related to command and control, we ﬁlter most of them
out by using an existing list of ad-related domains that is manually
maintained for the purpose of blocking advertisement [1].
Generating remediation procedures. To assist in the task of gen-
erating remediation procedures for malware infections, we intro-
duce the persistent changes scoring function. This scoring func-
tion measures the number of distinct system resources affected by
malware execution. As in [31], we take into account modiﬁca-
tions to the ﬁle system and the windows registry. Furthermore,
we also consider the processes and services started by the malware,
because remediation procedures, if they need to be applied to a
running system, will need to make sure that the malicious code is
not running. Each system resource is identiﬁed by its name. The
persistent changes score therefore counts the total number of dis-
tinct names of ﬁle and registry keys that are created or modiﬁed,
as well as the processes and services started. Furthermore, our dy-
namic analysis phase uses techniques from Bayer et al. [14] (that
are based on dynamic taint analysis) to detect randomly generated
ﬁle names, and replace them with a special token. Likewise, we de-
tect and replace names that are obtained by enumerating directories
and registry keys. The idea is that a malware that generates a differ-
ent random or temporary ﬁle in each execution, or one that crawls
the entire ﬁle-system, infecting all the executables it encounters,
should not be assigned a high persistent changes score because of
this.
Once a scoring function is selected, we can measure the total
value v(C) of the information provided by the analysis of samples
in a cluster C. We then calculate cluster_scoreC = v(C)/|C|.
The cluster score is thus a value to cost ratio: The amount of valu-
able information provided by the cluster, divided by the analysis
resources that have been spent to obtain it.
To calculate the sample scores for a sample s, we proceed as fol-
lows. Rather than simply consider the most likely cluster, we take
into account the entire cluster probability vector ps. The sample
score is therefore the expectation of the cluster score, calculated as
the scalar product sample_scores = ps · cluster_score. At each
iteration of FORECAST, the L candidate samples with the highest
sample score are passed to the dynamic analysis phase.
3.5 Online Operation
As a result of the analysis of a selected sample s, the set of ob-
served behavioral features βs becomes available. These in turn are
passed on to the behavioral clustering phase, which determines the
cluster Cs. This newly obtained information then needs to be incor-
porated into our knowledge-base. Thus, we update the cluster score
for cluster Cs. Furthermore, the cluster label Cs for the newly ana-