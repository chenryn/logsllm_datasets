ROOT
SBARQ
WHNP
WHNP
PP
SQ
VP
.
?
WDT
IN
NP
VBD
NP
Which
of
DT
NNS
wrote
DT
VBG
NN
these
people
this
blog
post
Figure 1. A sample parse tree produced by the Stanford Parser.
determine that the posts have the same author. More subtly,
rather than linking posts based on similarities in writing
style, our classiﬁers may end up relying on similarities in the
content covered by the writing, such as speciﬁc words related
to the topic of the blog. Not only is this problematic given
that we attempt to match test posts to labeled posts from
the same blog, we additionally anticipate that anonymously-
authored blogs will frequently tend to cover different topics
of greater sensitivity, compared to identiﬁed blogs written by
the same author. We take the following strategies in avoiding
these pitfalls:
1) We begin by ﬁltering out any obvious signatures in
the posts by checking for common substrings. We also
remove markup and any other text that does not appear
to be directly entered by a human in order to avoid
linking based on the blog software or style templates
used.
2) We carefully limit the features we extract from each
post and provide to the classiﬁer. In particular, unlike
previous work on author identiﬁcation, we do not
employ a “bag of words” or any other features that can
discover and incorporate arbitrary content. Our word-
based features are limited to a ﬁxed set of function
words which bear little relation to the subject of dis-
cussion (e.g., “the,” “in,” etc.). While we do make use
of single character frequencies, we exclude bigrams
and trigrams, which may be signiﬁcantly inﬂuenced
by speciﬁc words.
3) We follow up the experiments described above (post-
to-blog matching) with additional experiments which
actually involve matching distinct blogs to one another.
Speciﬁcally, we assembled a small collection of sets
of blogs with the same author; for these experiments
(blog-to-blog matching), we set aside one blog as the
test content, mix the others from the same author into
the full dataset of 100,000 blogs, and then measure
our ability to pick them back out.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:49:27 UTC from IEEE Xplore.  Restrictions apply. 
The results of the blog-to-blog matching experiments
roughly match the post-to-blog matching results, and we
also found the results were not dominated by any one class
of features. These facts have given us conﬁdence that our
methods are in fact discovering links in writing style—not
blog style templates or the topic of a blog.
In addition to serving as a “sanity check” for our results
in the abovementioned manner, the cross-context setting is
arguably closer to the adversary’s practical task in some
scenarios. This is certainly not always the case: in some
applications of stylometric authorship recognition, the avail-
able labeled text might be from the same context as the
unlabeled text. This was the case in Mosteller and Wallace’s
study of the disputed federalist papers;
in the blogging
scenario, an author might decide to selectively distribute
a few particularly sensitive posts anonymously through a
different channel.
Yet in other cases, the unlabeled text might be political
speech, whereas the only labeled text by the same author
might be a cooking blog. Context encompasses much more
than topic: the tone might be formal or informal; the author
might be in a different mental state (e.g., more emotional)
in one context versus the other, etc.
Thus, we can expect an author’s writing style to differ
more across different contexts than it does within the same
context, and indeed, our cross-context results are numeri-
cally somewhat weaker. In the future we hope to understand
exactly how writing style varies across contexts and to utilize
this to improve cross-context classiﬁcation.
IV. DATA SOURCES AND FEATURES
Having given some high-level motivation for our ex-
perimental approach and methodology, we now detail our
sources of data, the steps we took to ﬁlter it, and the feature
set implemented.
Data sources. The bulk of our data was obtained from
the ICWSM 2009 Spinn3r Blog Dataset, a large collection of
blog posts made available to researchers by Spinn3r.com, a
provider of blog-related commercial data feeds [54]. For the
blog-to-blog matching experiments, we supplemented this
by scanning a dataset of 3.5 million Google proﬁle pages
for users who specify multiple URLs [46]. Most of these
URLs link to social network proﬁles rather than blogs, so we
further searched for those containing terms such as “blog,”
“journal,” etc. From this list of URLs, we obtained RSS
feeds and individual blog posts.
We passed both sets of posts through the following
ﬁltering steps. First, we removed all HTML and any other
markup or software-related debris we could ﬁnd, leaving
only (apparently) manually entered text. Next, we retained
only those blogs with at least 7,500 characters of text across
all their posts, or roughly eight paragraphs. Non-English
language blogs were removed using the requirement that at
least 15% of the words present must be among the top 50
Feature
Frequency of ’
Number of characters
Freq. of words with only ﬁrst letter uppercase
Number of words
Frequency of (NP, PRP)
(noun phrase containing a personal pronoun)
Frequency of .
Frequency of all lowercase words
Frequency of (NP, NNP)
(noun phrase containing a singular proper noun)
Frequency of all uppercase words
Frequency of ,
Information Gain in Bits
1.097
1.077
1.073
1.060
1.032
1.022
1.018
1.009
0.991
0.947
THE TOP 10 FEATURES BY INFORMATION GAIN.
Table II
English words, a heuristic found to work well in practice.
Of course, our methods could be applied to almost any other
language, but some modiﬁcations to the feature set would
be necessary. To avoid matching blog posts together based
on a signature the author included, we removed any preﬁx
or sufﬁx found to be shared among at least three-fourths of
the posts of a blog. Duplicated posts were also removed.
At the end of this process, 5,729 blogs from 3,628 Google
proﬁles remained, to which we added 94,271 blogs from
the Spinn3r dataset to bring the total to 100,000. Of the
3,628 retained Google proﬁles, 1,763 listed a single blog;
1,663 listed a pair of blogs; other 202 listed three to ﬁve.
Our ﬁnal dataset contained 2,443,808 blog posts, an average
of 24 posts per blog (the median was also 24). Each post
contained an average of 305 words, with a median of 335.
Features. We extracted 1,188 real-valued features from
each blog post,
into a high-
dimensional vector. These feature vectors were the only
input to our classiﬁers; the text of the blog post played no
further role after feature extraction.
transforming the post
Table I summarizes the feature set. All but the last of these
categories consist of features which reﬂect the distributions
of words and characters in each of the posts. Many of them,
such as the distribution of word length and frequency of
letters in the alphabet, come from previous work on author
identiﬁcation [23]. We also analyze the capitalization of
words, as we expect the level of adherence to capitalization
conventions to act as a distinguishing component of an
author’s writing style given the unedited, free-form nature of
written content on the Internet. We compute each of the letter
frequency features as the number of occurrences of a speciﬁc
letter in a post divided by the length of the post in characters.
Other single-character frequencies are computed likewise,
and word-frequency features are computed analogously, but
at the level of words. We list the 293 function words we use
in an appendix to the full version of the paper. These words
are topic-independent.
The last category of features in Table I, we use the
305
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:49:27 UTC from IEEE Xplore.  Restrictions apply. 
variable corresponding to feature i. Since the features are
real-valued, and entropy is deﬁned only for discrete random
variables6, we need to sensibly map them to a set of
discrete values. For each feature, we partitioned the range of
observed values into twenty intervals. We reserved one bin
for the value zero, given the sparsity of our feature set; the
other nineteen bins were selected to contain approximately
equal numbers of values across all the posts in the dataset.
A portion of the result of this analysis is given in Table II,
which lists the ten features with greatest information gain
when computed as described. Several other binning methods
were found to produce similar results. With information
gains, measured in bits, ranging from 1.097 to 0.947, these
features can all be considered roughly equal
in utility.
Perhaps least surprisingly, the length of posts (in words and
characters) is among the best indicators of the blog the posts
were taken from.7 Several punctuation marks also appear
in the top ten, along with the three most common patterns
of upper- and lowercase letters and two syntactic category
pairs.
To give a sense of the typical variation of feature values
both within a blog and between different blogs, Figure 2
displays a representative example of one of these ten fea-
tures: the frequency of all lowercase words. The plot was
generated by sorting the 100,000 blogs according to the
mean of this feature across their posts. The means are shown
by the solid line, and the values for individual posts are
plotted as dots. For legibility, the ﬁgure only shows every
third post of every one hundredth blog. As one might expect,
the values vary from post to post by much larger amounts
than the differences in mean between most pairs of blogs,
indicating that this feature alone carries a fairly small amount
of information. The corresponding plots for features with
lower information gain look similar, but with less variation
in means or more variation between individual posts from
the same author.
Analysis. Perhaps the most important aspect of our data
set is the large number of classes and training examples. As
the number of classes increases, the nature of the classiﬁ-
cation task changes fundamentally in two ways: accuracy
and computational demands. An immediate consequence
of having more classes is that they become more densely
distributed; the number of classes that are ”close” to one
another increases. As such,
the decision boundary that
separates each class now has to accurately distinguish it from
a much larger number of close alternatives. This general
principle manifests in different ways; “masking” (Section
V-B) is a common problem.
At the same time, the large number of examples places
hard limits on the kinds of classiﬁers we can use — anything
6A deﬁnition also exists for continuous random variables, but applying
it requires assuming a particular probability mass function.
7However, these features may not be as useful in the cross-context
setting.
Figure 2. Per-post values (dots) and per-blog means (line) of an example
feature across the dataset.
Stanford Parser [55] to determine the syntactic structure
of each of the sentences in the input posts. As output, it
produces a tree for each sentence where the leaf nodes
are words and punctuation used, and other nodes represent
various types of syntactic categories (phrasal categories and
parts of speech). Figure 1 shows an example parse tree as
produced by the Stanford Parser, with tags such as NN for
noun, NP for noun phrase, and PP for prepositional phrase.
We generate features from the parse trees by taking each
pair of syntactic categories that can appear as parent and
child nodes in a parse tree tree, and counting the frequency
of each such pair in the input data.
Two previous studies, Baayen et al. [56] and Gamon
[57], used rewrite-rule frequencies extracted from parse trees
of sentences as features.5 Our syntactic-category pairs are
similar, but less numerous (in Gamon’s work, for instance,
the number of possible rewrite rules is over 600,000). The
rationale for our choice was that
this could make our
classiﬁers more robust and less prone to overﬁtting, not to
mention more more computationally tractable.