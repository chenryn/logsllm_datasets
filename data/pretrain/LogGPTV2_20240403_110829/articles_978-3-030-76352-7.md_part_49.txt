k=1 i=1
Thus, the overall objective function is defined as:
J(W,C)=( kK =1| iN =K 1| d2 W(s i,c K)+ N1 kK =1| iN =K 1| S β(d dw w( (x x, ,x == )) )) (4)
x
where the first term is the objective function of K-means, and the second term
is the summation of the classification errors over the K clusters.
Two parameters are optimized in this objective function. The first is the
weights matrix. The feature-dependent weights associated with the sample
INSIDENT 315
are trained to make it closer to x, while making the sample x = further from
x. Then, the cluster centroid update is based on the learned weighted distance.
Since this function is differentiable, we can analytically use gradient descent for
estimating the matrix W, guaranteeing convergence. The iterative optimization
of a learning parameter like w is given below.
J(W,C)
Wt+1 =Wt−α(
) (5)
δ(W)
To simplify the formula, the function R(x) is defined [29] as:
R w(x i)=(d dw w( (x xi i, ,x xi i, ,= =) )) (6)
The partial derivative of J(W,C) with respect to W is calculated by:
=|NK| |NK|
δJ δ(W W,K) ∼ 2W K (x i−C K)2+ N1 S β (R(x i))δR δW(x i) (7)
K k
i=1 i=1
where  is the inner product and δR(xi) is :
δWK
δ δR W(s Ki) W1 R(1
= d2 K(x i,x i,=)( x i)W K (x i−x i,=)2−R(x i)W K (x i−x i,=)2)
(8)
The derivative of S β(z) is defined as:
δS z(z)
S β(z) = δβ
βeβ(1−z)
= (9)
(1+eβ(1−z))2
The partial derivative of J(W,C) with respect to C is calculated as:
=|N k|
J(W δC,C)
∼ −2W k2(x i−C k) (10)
k
i=1
Since we need to optimize the weight of features for each cluster’s sam-
ples, along with the center of clusters, we first update W in each cluster, and
then we update C (center of clusters). The INSIDENT algorithm is depicted in
Algorithm 1 for more clarification. Since the algorithm performs in an iterative
process using gradient descent, the simplest clustering (k-means) and (KNN)
algorithms are used for efficiency. However, K-means is one of the most reliable
and most widely used clustering algorithms. Besides, the K-nearest neighbor
(NN) has been successfully used in many pattern-recognition applications [9].
Similar samples are close to each other in new feature space, making a point,
andcontextualtypeanomalieseasilydetectable.Inthecaseofcollectiveanoma-
lies, we select the number of each cluster’s representative based on its size to
keep the distribution the same as the original data.
316 S. Ghodratnama et al.
Algorithm 1. INSIDENT
Input: Traffic Data X, learning rate γ and α.
Output: Summary (S).
procedure INSIDENT.
while iter<MaxIterations do
Clusters (C) ← K-means(X)
for each clusters c in C do
for each sample x in c do
x ←findSimilarCloseSample()
=
x ←findDifferentCloseSample()
=
Witer+1 =Witer−γδJ(W)
W
end for
end for
Update Clusters
end while
return Summary(S)
4 Experiments and Evaluation
Inthissection,thedataset,theevaluationmethod,andtheperformanceofINSI-
DENT are explained and compared with existing state-of-the-art approaches.
4.1 Data Set
Experiments on six benchmark datasets are performed. The details of this
dataset and the distribution of normal and anomalous samples in each dataset
are reported in Table2. KDD1999 contains collective anomalies were the other
fivedatasetscontainonlyrareanomalies.Theserareanomalousdatasetsarefrom
SCADAnetwork,includingrealSCADA(WTP),simulatedanomalies(Sim1and
Sim2), and injected anomalies (MI and MO).
4.2 Evaluation Metrics
Toevaluatenetworktrafficsummary,weexplaintwowidelyusedsummaryeval-
uation metrics including conciseness, and information loss [5].
– Conciseness: The size of the summary influences the quality of the summary.
At the same time, it is important to create a summary that can reflect the
underlying data patterns. Conciseness is defined as the ratio of input dataset
size (N) and the summarized dataset size (S) defined as:
N
Conciseness= (11)
S
INSIDENT 317
– InformationLoss:Ageneralmetricusedtodescribetheamountofinformation
lostfromtheoriginaldatasetduetothesummarization.Lossisdefinedasthe
ratio of the number of samples not present by samples present in summary
defines as:
L
InformationLoss= (12)
T
where T is the number of unique samples represented by the summary, and
L defines the number of samples not present in the summary.
Table 2. Dataset sescription.
Dataset Sample number Normal Percentage Anomalies percentage
KDD1999 494020 19.69 80.310
WTP 527 97.34 2.66
MI 4690 97.86 2.14
MO 4690 98.76 1.24
Sim1 10501 99.02 0.98
Sim2 10501 99.04 0.96
Besides, to evaluate the performance of the anomaly detection algorithms
usedinsupervisedapproaches,threemeasures,includingaccuracy,recall,andF1
discussed below, are used. Before we define these measure, four values included
in the confusion needs to be discussed [3].
– True Positive (TP): Number of anomalies correctly identified as anomalous.
– False Positive (FP): Number of normal data incorrectly identified anomaly.
– True Negative (TN): Number of normal data correctly identified as normal.
– False Negative (FN): Number of anomalies incorrectly identified as normal.
Based on the above definitions, we define the evaluation metrics.
TP +TN
Accuracy = (13)
TP +TN +FP +FN
TP
Recall= (14)
TP +FN
2TP
F1= (15)
2TP +FP +FN
4.3 Result Analysis
In this section, we discuss the performance evaluation of the existing summa-
rization methods compared to INSIDENT, along with the anomaly detection
result.
318 S. Ghodratnama et al.
Table 3. Real SCADA dataset (WTP) result.
Model WTP-Recall WTP-Accuracy WTP-F1
KNN 85.71 97.39 85.71
LOF 78.57 97.38 78.57
COF 57.14 97.35 57.14
LOCI 85.71 97.39 85.71
LoOP 42.85 97.33 42.85
INFLO 57.14 97.35 57.14
CBLOF 92.85 97.40 92.85
LDCOF 85.71 97.39 85.71
CMGOS 57.14 97.35 57.14
HBOS 28.57 97.32 28.57
LIBSVM 85.71 97.39 85.71
INSIDENT 94.87 97.91 94.87
AnomalyDetectionEvaluation. Thissectioncontainstheperformanceanal-
ysis of anomaly detection techniques. The baseline algorithms include Near-
est Neighbor-based algorithms (K-NN [32], LOF [13], COF [35], LOCI [28],
LoOP [26], INFLO [25]), clustering-based approach(CBLOF [22], LDCOF [6],
CMGOS [6]), and statistical appraoches (HBOS and LIBSVM [7]). These
approachesarecomparedwithINSIDENTondifferentvariationsoftheSCADA
dataset, including WTP, MI, MO, Sim1, and Sim2, where their values are
reportedby[3].ResultsarereportedrespectivelyinTable3,Table4,andTable5.
FromTable3,itcanbeseenthatfortherealSCADAdataset(WTP),INSID-
ENThashighervalues.Thentheclustering-basedanomalydetectiontechnique,
CBLOF, performs best, and third, the nearest-neighbor-based approach attains
the best performance. It is an expected result showing the combination of clus-
tering and KNN can perform better. Statistical based approach HBOS dis not
performwell.Table4displaystheresultsonsimulateddatasets(Sim1andSim2).
LIBSVM has better recall than others, and INCIDENT performs as the second
best.Clustering-basedapproachesarenotwellsuitedforthesimulateddatasets.
For the datasets with injected anomalies (MI, MO), INCIDENT, along with
clustering-based approaches, are the best considering the evaluation measures.
Nearestneighbor-basedapproachesarethenextbest.Itisinterestingtoobserve
that the Recall and F1 values are identical for all the anomaly detection tech-
niques.ThereasonisthatsincethetopN anomaliesdetectedbythetechniques
match the actual N number of anomalies in the dataset, the Recall, and F1
scores are always the same.
Network Traffic Summarization Evaluation. For summarization evalua-
tion,theKDDdatasetisused.Summarizationsize,whichdefinesconciseness,is
INSIDENT 319
Table 4. Simulated SCADA datasets result(Sim1 and Sim2).
Model Sim1Recall Sim1Accuracy Sim1F1 Sim2Recall Sim2Accuracy Sim2F1
KNN 64.7 99.03 64.7 63 99.05 63
LOF 0 99.01 0 0 99.03 0
COF 0 99.01 0 2 99.03 2
LOCI 0 99.01 0 0 99.03 0
LoOP 0.98 99.01 0.98 0 99.03 0
INFLO 0 99.01 0 0 99.03 0
CBLOF 0 99.01 0 0 99.03 0
LDCOF 0 99.01 0 0 99.03 0
CMGOS 18.62 99.02 18.62 97 99.05 97
HBOS 30.39 99.02 30.39 27 99.04 6
LIBSVM 74.50 99.03 74.50 68 99.05 68
INSIDENT 72.13 99.07 72.13 78.21 99.05 78.21
Table 5. Simulated SCADA datasets with Injected Anomalies result (MI and MO).
Model MI-Recall MI-Accuracy MI-F1 MO-Recall MO-Accuracy MO-F1
KNN 96 97.09 96 91.37 98.77 91.37
LOF 38.33 97.43 38.33 55.17 98.76 55.17
COF 9 97.82 9 25.86 98.75 25.86
LOCI 91 97.9 91 84.48 98.77 84.48
LoOP 10 97.83 10 27.58 98.75 27.58
INFLO 12 97.83 12 43.10 98.76 43.10
CBLOF 24 97.84 24 63.79 98.76 63.79
LDCOF 100 97.91 100 63.79 98.76 63.79
CMGOS 100 97.91 100 50 98.76 50
HBOS 98 97.91 98 65.51 98.76 65.51
LIBSVM 86 97.9 86 91.37 98.77 91.37
INSIDENT 100 98.76 100 94.21 99.04 94.21
considered as a constraint in summarization algorithms. When the summary is
small,ithasmaximuminformationloss.Ontheotherhand,whenconcisenessis
small, the summary contains the whole dataset has no information loss. There-
fore, information loss and conciseness are orthogonal parameters. Our experi-
ments used five different summary sizes, and then information loss was mea-
sured for each summary size. In practice, the network manager/analyst decides
the summary size based on the network. The results are compared with NTS
and FIB approaches [4]. Since our algorithm is based on k-means, we test three
timeswithdifferentinitialpointsforeachsummarysize.Resultsaredepictedin
Fig.1.Besides,thepercentageofanomaliescomparedwithSUCh[2]isreported
in Table6 proving that INSIDENT well-preserved the percentage of anomalies
in generated summaries.
320 S. Ghodratnama et al.
Fig.1. The result of comparing information loss based on different summary size.
Table 6. Comparing the distribution of anomalies in summaries and original data.
Dataset Original data SUCh Alg INSIDENT
WTP 2.66 N/A 2.33
MI 2.14 2.61 2.76
MO 1.24 1.46 1.52
Sim1 0.98 1.04 1.11
Sim2 0.96 0.94 1.01
5 Conclusion and Future Work
Monitoring network traffic data to detect any hidden patterns of anomalies is a
challenging and time-consuming task which requires high computing resources.
Therefore, in this paper, we proposed an INtelligent Summarization approach
for IDENTifying hidden anomalies, called INSIDENT. In data summarization,
itisalwaysadilemmatoclaimthebestsummary.Theproposedapproachclaim
is to guarantee to keep the original data distribution in summarized data. The
INSIDENT’s backbone is the clustering and KNN algorithm that dynamically
maps original feature space to a new feature space by locally weighting features
in each cluster. The experimental results proved that the proposed approach
helps keep the distribution the same as the original data, consequently making
anomaly detection easier. In future work, we aim to focus on real-time network
traffic summarization.
Acknowledgement. WeacknowledgetheAI-enabledProcesses(AIP)ResearchCen-
tre(https://aip-research-center.github.io/)forfundingthisresearch.Wealsoacknowl-
edge Macquarie University for funding this project through IMQRES scholarship.
INSIDENT 321
References
1. Ahmed,M.:Datasummarization:asurvey.Knowl.Inf.Syst.58(2),249–273(2019)
2. Ahmed, M.: Intelligent big data summarization for rare anomaly detection. IEEE
Access 7, 68669–68677 (2019)
3. Ahmed,M.,Anwar,A.,Mahmood,A.N.,Shah,Z.,Maher,M.J.:Aninvestigation
of performance analysis of anomaly detection techniques for big data in scada
systems. EAI Endorsed Trans. Indust. Netw. Intell. Syst. 2(3), e5 (2015)
4. Ahmed, M., Mahmood, A.N., Maher, M.J.: A novel approach for network traf-
fic summarization. In: Jung, J.J., Badica, C., Kiss, A. (eds.) INFOSCALE 2014.
LNICST, vol. 139, pp. 51–60. Springer, Cham (2015). https://doi.org/10.1007/
978-3-319-16868-5 5
5. Ahmed, M., Mahmood, A.N., Maher, M.J.: An efficient technique for network
traffic summarization using multiview clustering and statistical sampling. EAI