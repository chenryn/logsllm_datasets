:::
默认情况下，禁用忙碌轮询。这个步骤描述了如何启用繁忙的轮询。
::: orderedlist
**流程**
1.  确定启用了 `CONFIG_NET_RX_BUSY_POLL`{.literal} 编译选项：
    ``` screen
    # cat /boot/config-$(uname -r) | grep CONFIG_NET_RX_BUSY_POLL
    CONFIG_NET_RX_BUSY_POLL=y
    ```
2.  启用忙碌轮询
    ::: orderedlist
    1.  要在特定套接字上启用忙碌的轮询，请将
        `sysctl.net.core.busy_poll`{.literal} 内核值设置为 `0`{.literal}
        以外的值：
        ``` screen
        # echo "net.core.busy_poll=50" > /etc/sysctl.d/95-enable-busy-polling-for-sockets.conf
        # sysctl -p /etc/sysctl.d/95-enable-busy-polling-for-sockets.conf
        ```
        此参数控制在套接字轮询上等待数据包的微秒数，然后选择
        `syscalls`{.literal}。红帽建议值设为 `50`{.literal}。
    2.  将 `SO_BUSY_POLL`{.literal} 套接字选项添加到套接字。
    3.  要在全局范围内启用忙碌的轮询，请将
        `sysctl.net.core.busy_read`{.literal} 设置为 `0`{.literal}
        以外的值：
        ``` screen
        # echo "net.core.busy_read=50" > /etc/sysctl.d/95-enable-busy-polling-globally.conf
        # sysctl -p /etc/sysctl.d/95-enable-busy-polling-globally.conf
        ```
        `net.core.busy_read`{.literal}
        参数控制要等待设备队列中的数据包进行套接字读取的微秒数。它还设置
        `SO_BUSY_POLL`{.literal} 选项的默认值。红帽建议少量插槽的值为
        `50`{.literal}，`对于`{.literal} 大量插槽，值为
        100。对于非常大的插槽数量（例如，超过数百个），改为使用 the
        `epoll`{.literal} 系统调用。
    :::
:::
::: itemizedlist
**验证步骤**
-   验证是否启用了忙碌轮询
    ``` screen
    # ethtool -k device | grep "busy-poll"
    busy-poll: on [fixed]
    # cat /proc/sys/net/core/busy_read
    50
    ```
:::
::: itemizedlist
**其它资源**
-   `ethtool(8)`{.literal}、`socket(7)`{.literal}、`sysctl(8)和`{.literal}
    `sysctl.conf(5)`{.literal} man page
-   [配置 ethtool offload
    功能](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/configuring_and_managing_networking/index#configuring-ethtool-offload-features_configuring-and-managing-networking){.link}
:::
:::
:::
::: section
::: titlepage
# []{#configuring-an-operating-system-to-optimize-access-to-network-resources_monitoring-and-managing-system-status-and-performance.html#receive-side-scaling_configuring-an-operating-system-to-optimize-access-to-network-resources}接收扩展 {.title}
:::
接收扩展(RSS)（也称为多队列接收）可在多个基于硬件的接收队列之间分发网络接收处理，允许多个
CPU 处理入站网络流量。RSS 可用于减轻因单个 CPU
过载导致的接收中断处理方面的瓶颈，并降低网络延迟。默认情况下启用 RSS。
在适当的网络设备驱动程序中配置应处理 RSS 网络活动的队列数量或 CPU 数量：
::: itemizedlist
-   对于 `bnx2x`{.literal} 驱动程序，它在 `num_queues`{.literal}
    参数中进行配置。
-   对于 `sfc`{.literal} 驱动程序，它在 `rss_cpus`{.literal}
    参数中配置。
:::
不管怎样，它通常在 `/sys/class/net/设备/queues/rx-queue/`{.literal}
目录中配置，其中 [*device*]{.emphasis} 是网络设备的名称（如
`enp1s0`{.literal}）和 [*rx-queue*]{.emphasis} 是相应接收队列的名称。
`irqbalance`{.literal} 守护进程可与 RSS
结合使用，以减少跨节点内存传输和缓存行退的可能性。这降低了处理网络数据包的延迟。
::: section
::: titlepage
## []{#configuring-an-operating-system-to-optimize-access-to-network-resources_monitoring-and-managing-system-status-and-performance.html#viewing-the-interrupt-request-queues_configuring-an-operating-system-to-optimize-access-to-network-resources}查看中断请求队列 {.title}
:::
在配置接收扩展(RSS)时，红帽建议将队列数量限制为每个物理 CPU
内核一个队列。超线程通常在分析工具中作为单独的核心来表示，但为所有核心（如超线程）配置队列并不对网络性能有用。
启用后，RSS 根据每个 CPU 的处理量在可用 CPU
之间均匀分配网络处理。但是，使用 `ethtool`{.literal} 实用程序的
`--show- rxfh-indir`{.literal} 和 \--set-``{=html} rxfh-indir
参数修改 RHEL
分发网络活动的方式，并衡量某些类型的网络活动与其他活动更重要。
这个步骤描述了如何查看中断请求队列。
::: itemizedlist
**流程**
-   要确定您的网络接口卡是否支持 RSS，请检查多个中断请求队列是否与
    `/proc/interrupts`{.literal} 中的接口相关联：
    ``` literallayout
    # egrep 'CPU|p1p1' /proc/interrupts
     CPU0    CPU1    CPU2    CPU3    CPU4    CPU5
    89:   40187       0       0       0       0       0   IR-PCI-MSI-edge   p1p1-0
    90:       0     790       0       0       0       0   IR-PCI-MSI-edge   p1p1-1
    91:       0       0     959       0       0       0   IR-PCI-MSI-edge   p1p1-2
    92:       0       0       0    3310       0       0   IR-PCI-MSI-edge   p1p1-3
    93:       0       0       0       0     622       0   IR-PCI-MSI-edge   p1p1-4
    94:       0       0       0       0       0    2475   IR-PCI-MSI-edge   p1p1-5
    ```
    输出显示 NIC 驱动程序为 `p1p1 接口`{.literal} 创建了 6
    个接收队列（`p1p1-0`{.literal} 到
    `p1p1-5`{.literal}）。它还显示每个队列处理了多少个中断，以及服务中断的
    CPU。在这种情况下，有 6 个队列，因为默认情况下，这个特定 NIC
    驱动程序会为每个 CPU 创建一个队列，此系统具有 6 个 CPU。在 NIC
    驱动程序中，这是相当常见的模式。
-   列出地址为 `0000:01:00.0`{.literal} 的 PCI 设备的中断请求队列：
    ``` screen
    # ls -1 /sys/devices/*/*/0000:01:00.0/msi_irqs
    101
    102
    103
    104
    105
    106
    107
    108
    109
    ```
:::
:::
:::
::: section
::: titlepage
# []{#configuring-an-operating-system-to-optimize-access-to-network-resources_monitoring-and-managing-system-status-and-performance.html#receive-packet-steering_configuring-an-operating-system-to-optimize-access-to-network-resources}接收数据包表 {.title}
:::
接收数据包声明(RPS)与接收侧边扩展(RSS)类似，因为它用于将数据包定向到特定
CPU 进行处理。但是，RPS
在软件级别上实施，有助于防止单个网络接口卡的硬件队列成为网络流量的瓶颈。
与基于硬件的 RSS 相比，RPS 具有几个优点：
::: itemizedlist
-   RPS 可以和任何网络接口卡一起使用。
-   可以在 RPS 中添加软件过滤器，以处理新协议。
-   RPS 不会增加网络设备的硬件中断率。然而，它确实引入了处理器间中断。
:::
在 `/sys/class/net/ 设备 /queues/rx-queue/rps_cpus`{.literal}
文件中，其中 [*device 是网络设备*]{.emphasis} 的名称，如 enp1s0 and
[*rx-queue*]{.emphasis} 是相应接收队列的名称，如 rx-0。
`rps_cpus`{.literal} 文件的默认值为 0。这会禁用 RPS，CPU
处理网络中断并处理数据包。若要启用
RPS，请使用应处理指定网络设备和接收队列的 CPU 配置适当的
`rps_cpus`{.literal} 文件。
The `rps_cpus`{.literal} 文件使用以逗号分隔的 CPU 位映射。因此，要允许
CPU 处理接口上接收队列的中断，请将它们在位图中的位置值设置为
1。例如，若要使用 CPU `0`{.literal}、`1、2`{.literal} ``{.literal} 和
`3`{.literal} 处理中断，可将 `rps_cpus 的`{.literal} 值设置为
`f`{.literal}，即 15 的十六进制值
`。`{.literal}在二进制表示中，`15`{.literal}
`为00001111(1+2+4+8)`{.literal}。
对于具有单传输队列的网络设备，可以通过将 RPS 配置为使用同一内存域中的
CPU 来实现最佳性能。在非 NUMA 系统上，这意味着可以使用所有可用的
CPU。如果网络中断率非常高，除处理网络中断的 CPU 外，也可以提高性能。
对于有多个队列的网络设备，配置 RPS 和 RSS 时通常没有好处，因为 RSS
被配置为默认将 CPU 映射到每个接收队列。但是，如果硬件队列比 CPU 少，并且
RPS 配置为在同一内存域中使用 CPU，则 RPS 仍然很有用。
:::
::: section
::: titlepage
# []{#configuring-an-operating-system-to-optimize-access-to-network-resources_monitoring-and-managing-system-status-and-performance.html#receive-flow-steering_configuring-an-operating-system-to-optimize-access-to-network-resources}接收流畅 {.title}
:::
接收流速(RFS)扩展接收数据包定位(RPS)行为，以提高 CPU
缓存命中率，从而减少网络延迟。当 RPS 仅根据队列长度转发数据包时，RFS
使用 RPS 后端来计算最合适的
CPU，然后根据使用数据包的应用程序的位置转发数据包。这提高了 CPU
缓存效率。
从单个发送器接收的数据不会发送到多个
CPU。如果单个发送方收到的数据量大于单个 CPU
可处理的数据量，请配置更大的帧大小以减少中断数量，从而减少 CPU
处理工作的数量。或者，考虑 NIC 卸载选项或更快的 CPU。
考虑将 `numactl`{.literal} 或 `taskset`{.literal} 与 RFS
结合使用以将应用程序固定到特定的核心、插槽或 NUMA
节点。这有助于防止数据包被不按顺序处理。
::: section
::: titlepage
## []{#configuring-an-operating-system-to-optimize-access-to-network-resources_monitoring-and-managing-system-status-and-performance.html#enabling-receive-flow-steering_configuring-an-operating-system-to-optimize-access-to-network-resources}启用接收流（接收流） {.title}
:::
默认情况下禁用接收流(RFS)。这个步骤描述了如何启用 RFS。
::: orderedlist
**流程**
1.  将 `net.core.rps_sock_flow_entries`{.literal}
    内核值的值设置为并发活跃连接的最大预期数量：
    ``` screen
    # echo "net.core.rps_sock_flow_entries=32768" > /etc/sysctl.d/95-enable-rps.conf
    ```
    ::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
    ### 注意 {.title}
    红帽建议 `32768`{.literal} 代表中等服务器负载值为
    32768。输入的所有值 `在实践中`{.literal} 均被舍入到 2 最接近的指数。
    :::
2.  永久设置 `net.core.rps_sock_flow_entries`{.literal} 的值：
    ``` screen
    # sysctl -p /etc/sysctl.d/95-enable-rps.conf
    ```
3.  要将 `sys/class/net/设备/queues/rx-queue/rps_flow_cnt`{.literal}
    文件的值临时设置为(`rps_sock_flow_entries/N`{.literal})，其中
    [*N*]{.emphasis} 是设备上的接收队列数：
    ``` screen
    # echo 2048 > /sys/class/net/device/queues/rx-queue/rps_flow_cnt
    ```
    使用您要配置的 [*网络设备*]{.emphasis} 的名称替换
    device（例如，enp1s0），并将 [*rx-queue*]{.emphasis}
    替换为您要配置的接收队列（例如： [*rx-0*]{.emphasis}）。
    使用配置的接收队列数量替换 [*N*]{.emphasis}。例如，如果
    `rps_flow_entries`{.literal} 设置为 `32768`{.literal}，并且有 16 个
    `配置的`{.literal} 接收队列，则
    `rps_flow_cnt = 32786/16= 2048`{.literal} （即
    `rps_flow_cnt = rps_flow_enties/N`{.literal} ）。
    对于单队列设备，`rps_flow_cnt`{.literal} 的值与
    `rps_sock_flow_entries`{.literal} 的值相同。
4.  在所有网络设备中永久启用 RFS，创建
    `/etc/udev/rules.d/99-persistent-net.rules`{.literal}
    文件，并添加以下内容：
    ``` screen
    SUBSYSTEM=="net", ACTION=="add", RUN{program}+="/bin/bash -c 'for x in /sys/$DEVPATH/queues/rx-*; do echo 2048 > $x/rps_flow_cnt;  done'"
    ```
5.  可选：在特定网络设备中启用 RPS：
    ``` screen
    SUBSYSTEM=="net", ACTION=="move", NAME="device name" RUN{program}+="/bin/bash -c 'for x in /sys/$DEVPATH/queues/rx-*; do echo 2048 > $x/rps_flow_cnt; done'"
    ```
    [*使用*]{.emphasis} 实际网络设备名称替换设备名称。
:::
::: itemizedlist
**验证步骤**
-   验证是否启用了 RFS:
    ``` screen
    # cat /proc/sys/net/core/rps_sock_flow_entries
    32768
    # cat /sys/class/net/device/queues/rx-queue/rps_flow_cnt
    2048
    ```
:::
::: itemizedlist
**其它资源**
-   `sysctl(8)`{.literal} man page
:::
:::
:::
::: section
::: titlepage
# []{#configuring-an-operating-system-to-optimize-access-to-network-resources_monitoring-and-managing-system-status-and-performance.html#accelerated-rfs_configuring-an-operating-system-to-optimize-access-to-network-resources}加速的 RFS {.title}
:::