title:A Bayesian Approach to Privacy Enforcement in Smartphones
author:Omer Tripp and
Julia Rubin
A Bayesian Approach to Privacy Enforcement  
in Smartphones
Omer Tripp, IBM Research, USA; Julia Rubin, IBM Research, Israel
https://www.usenix.org/conference/usenixsecurity14/technical-sessions/presentation/tripp
This paper is included in the Proceedings of the 23rd USENIX Security Symposium.August 20–22, 2014 • San Diego, CAISBN 978-1-931971-15-7Open access to the Proceedings of  the 23rd USENIX Security Symposium is sponsored by USENIXA Bayesian Approach to Privacy Enforcement in Smartphones
Omer Tripp
IBM Research, USA
Julia Rubin
IBM Research, Israel
Abstract
Mobile apps often require access to private data, such
as the device ID or location. At the same time, popular
platforms like Android and iOS have limited support for
user privacy. This frequently leads to unauthorized dis-
closure of private information by mobile apps, e.g. for
advertising and analytics purposes. This paper addresses
the problem of privacy enforcement in mobile systems,
which we formulate as a classiﬁcation problem: When
arriving at a privacy sink (e.g., database update or outgo-
ing web message), the runtime system must classify the
sink’s behavior as either legitimate or illegitimate. The
traditional approach of information-ﬂow (or taint) track-
ing applies “binary” classiﬁcation, whereby information
release is legitimate iff there is no data ﬂow from a pri-
vacy source to sink arguments. While this is a useful
heuristic, it also leads to false alarms.
We propose to address privacy enforcement as a learn-
ing problem, relaxing binary judgments into a quanti-
tative/probabilistic mode of reasoning. Speciﬁcally, we
propose a Bayesian notion of statistical classiﬁcation,
which conditions the judgment whether a release point is
legitimate on the evidence arising at that point. In our
concrete approach, implemented as the BAYESDROID
system that is soon to be featured in a commercial prod-
uct, the evidence refers to the similarity between the data
values about to be released and the private data stored on
the device. Compared to TaintDroid, a state-of-the-art
taint-based tool for privacy enforcement, BAYESDROID
is substantially more accurate. Applied to 54 top-popular
Google Play apps, BAYESDROID is able to detect 27 pri-
vacy violations with only 1 false alarm.
1
Introduction
Mobile apps frequently demand access to private infor-
mation. This includes unique device and user identiﬁers,
such as the phone number or IMEI number (identify-
ing the physical device); social and contacts data; the
1
user’s location; audio (microphone) and video (camera)
data; etc. While private information often serves the core
functionality of an app, it may also serve other purposes,
such as advertising, analytics or cross-application proﬁl-
ing [9]. From the outside, the user is typically unable
to distinguish legitimate usage of their private informa-
tion from illegitimate scenarios, such as sending of the
IMEI number to a remote advertising website to create a
persistent proﬁle of the user.
Existing platforms provide limited protection against
privacy threats. Both the Android and the iOS plat-
forms mediate access to private information via a per-
mission model. Each permission is mapped to a desig-
nated resource, and holds per all application behaviors
and resource accesses. In Android, permissions are given
or denied at installation time.
In iOS, permissions are
granted or revoked upon ﬁrst access to the respective re-
source. Hence, both platforms cannot disambiguate le-
gitimate from illegitimate usage of a resource once an
app is granted the corresponding permission [8].
Threat Model
In this paper, we address privacy threats
due to authentic (as opposed to malicious) mobile ap-
plications [4, 18]. Contrary to malware, such applica-
tions execute their declared functionality, though they
may still expose the user to unnecessary threats by in-
corporating extraneous behaviors — neither required by
their core business logic nor approved by the user [11]
— such as analytics, advertising, cross-application pro-
ﬁling, social computing, etc. We consider unauthorized
release of private information that (almost) unambigu-
ously identiﬁes the user as a privacy threat. Henceforth,
we dub such threats illegitimate.
While in general there is no bullet-proof solution
for privacy enforcement that can deal with any type of
covert channel, implicit ﬂow or application-speciﬁc data
transformation, and even conservative enforcement ap-
proaches can easily be bypassed [19], there is strong evi-
dence that authentic apps rarely exhibit these challenges.
USENIX Association  
23rd USENIX Security Symposium  175
According to a recent study [9], and also our empiri-
cal data (presented in Section 5), private information is
normally sent to independent third-party servers. Conse-
quently, data items are released in clear form, or at most
following well-known encoding/encryption transforma-
tions (like Base64 or MD5), to meet the requirement of a
standard and general client/server interface.
The challenge, in this setting, is to determine whether
the app has taken sufﬁcient means to protect user pri-
vacy. Release of private information, even without user
authorization, is still legitimate if only a small amount of
information has been released. As an example, if an ap-
plication obtains the full location of the user, but then re-
leases to an analytics server only coarse information like
the country or continent, then in most cases this would
be perceived as legitimate.
Privacy Enforcement via Taint Analysis The short-
comings of mobile platforms in ensuring user privacy
have led to a surge of research on realtime privacy mon-
itoring. The foundational technique grounding this re-
search is information-ﬂow tracking, often in the form of
taint analysis [23, 15]: Private data, obtained via privacy
sources (e.g.
TelephonyManager.getSubscriberId(),
which reads the device’s IMSI), is labeled with a taint
tag denoting its source. The tag is then propagated
along data-ﬂow paths within the code. Any such path
that ends up in a release point, or privacy sink (e.g.
WebView.loadUrl(...), which sends out an HTTP re-
quest), triggers a leakage alarm.
The tainting approach effectively reduces leakage
judgments to boolean reachability queries. This can po-
tentially lead to false reports, as the real-world example
shown in Figure 1 illustrates. This code fragment, ex-
tracted from a core library in the Android platform, reads
the device’s IMSI number, and then either (ii) persists
the full number to an error log if the number is invalid
(the loge(...) call), or (ii) writes a preﬁx of the IMSI
(of length 6) to the standard log while carefully masking
away the sufﬁx (of length 9) as ’x’ characters. Impor-
tantly, data ﬂow into the log(...) sink is not a privacy
problem, because the ﬁrst 6 digits merely carry model
and origin information. Distinctions of this sort are be-
yond the discriminative power of taint analysis [26].
Quantitative extensions of the core tainting approach
have been proposed to address this limitation. A notable
example is McCamant and Ernst’s [13] information-ﬂow
tracking system, which quantities ﬂow of secret informa-
tion by dynamically tracking taint labels at the bit level.
Other approaches — based e.g. on distinguishability be-
tween secrets [1], the rate of data transmission [12] or the
inﬂuence inputs have on output values [14] — have also
been proposed. While these systems are useful as ofﬂine
analyses, it is highly unlikely that any of them can be en-
1 String mImsi = ...; // source
2 // 6 digits  15)) {
5
loge(” invalid IMSI ” + mImsi); // sink
6 mImsi = null; }
7 log(”IMSI: ” + mImsi.substring (0, 6) + ”xxxxxxxxx”); // sink
Figure 1: Fragment from an internal Android library,
com.android.internal.telephony.cdma.RuimRecords,
where a preﬁx of the mobile device’s IMSI number
ﬂows into the standard log ﬁle
gineered to meet the performance requirements of a re-
altime monitoring solution due to the high complexity of
their underlying algorithms. As an example, McCamant
and Ernst report on a workload on which their analysis
spent over an hour.
Our Approach We formulate data leakage as a clas-
siﬁcation problem, which generalizes the source/sink
reachability judgment enforced by standard information-
ﬂow analysis, permitting richer and more relaxed judg-
ments in the form of statistical classiﬁcation. The mo-
tivating observation is that reasoning about information
release is fuzzy in nature. While there are clear exam-
ples of legitimate versus illegitimate information release,
there are also less obvious cases (e.g., a variant of the
example in Figure 1 with a 10- rather than 6-character
preﬁx). A statistical approach, accounting for multiple
factors and based on rich data sets, is better able to ad-
dress these subtleties.
Concretely, we propose Bayesian classiﬁcation. To la-
bel a release point as either legitimate or illegitimate, the
Bayesian classiﬁer refers to the “evidence” at that point,
and computes the likelihood of each label given the ev-
idence. The evidence consists of feature/value pairs.
There are many ways of deﬁning the evidence. In this
study, we concentrate on the data arguments ﬂowing into
release operations, though we intend to consider other
classes of features in the future. (See Section 7.)
Speciﬁcally, we induce features over the private values
stored on the device, and evaluate these features accord-
ing to the level of similarity between the private values
and those arising at release points. This distinguishes in-
stances where data that is dependent on private values
ﬂows into a release point, but its structural and/or quan-
titative characteristics make it eligible for release, from
illegitimate behaviors. Failure to make such distinctions
is a common source of false alarms suffered by the taint-
ing approach [4].
To illustrate this notion of features, we return to the ex-
ample in Figure 1. Because the IMSI number is consid-
176  23rd USENIX Security Symposium 
USENIX Association
2
mImsi = ...;
"4(cid:22)46855(cid:22)56(cid:22)1234"
similarity: 0.4=6/15
loge(...);
similarity: 1.0=15/15
"invalid IMSI 4(cid:22)46855(cid:22)56(cid:22)1234"
log(...);
"IMSI: 4(cid:22)4685xxxxxxxxx"
Figure 2: Similarity analysis applied to the code in Fig-
ure 1
ered private, we deﬁne a respective feature IMSI. Assume
that the concrete IMSI value is “404685505601234”.
Then the value arising at the log(...) release point is
“IMSI: 404685xxxxxxxxx”. The quantitative similarity
between these two values serves as evidence for the de-
cision whether or not log(...) is behaving legitimately.
This style of reasoning is depicted in Figure 2.
Evaluation To evaluate our approach, we have imple-
mented the BAYESDROID system for privacy enforce-
ment.
We report on two sets of experiments over
BAYESDROID.
First, to measure the accuracy gain thanks to Bayesian
analysis, we compared BAYESDROID with the Taint-
Droid system [4], a highly popular and mature imple-
mentation of the tainting approach that is considered both
efﬁcient (with average overhead of approximately 10%)
and accurate. We applied both BAYESDROID and Taint-
Droid to the DroidBench suite,1 which comprises the
most mature and comprehensive set of privacy bench-
marks currently available. The results suggest dramatic
improvement in accuracy thanks to Bayesian elimina-
tion of false reports, yielding accuracy scores of 0.96 for
BAYESDROID versus 0.66 for TaintDroid.
The second experiment examines the practical value
of BAYESDROID by applying it to 54 top-popular mo-
bile apps from Google Play. We evaluate two variants of
BAYESDROID, one of which is able to detect a total of
27 distinct instances of illegitimate information release
across 15 of the applications with only 1 false alarm.
Contributions This paper makes the following princi-
pal contributions:
1. Novel approach to leakage detection (Section 2):
We present a Bayesian classiﬁcation alternative to
the classic tainting approach. Our approach is more
ﬂexible than taint tracking by permitting statistical
weighting of different features as the basis for pri-
vacy judgments.
2. Similarity-based reasoning (Section 3): We instanti-
ate the Bayesian approach by applying quantitative
similarity judgments over private values and values
about to be released. This enables consideration of
actual data, rather than only data ﬂow, as evidence
for privacy judgments.
3. Implementation and evaluation (Sections 4–5): We
have instantiated our approach as the BAYESDROID
system, which is about to be featured in an IBM
cloud-based security service. We report on two sets
of experiments, whose results (i) demonstrate sub-
stantial accuracy gain thanks to Bayesian reason-
ing, and (ii) substantiate the overall effectiveness of
BAYESDROID when applied to real-world apps. All
the leakage reports by BAYESDROID are publicly
available for scrutiny.2
2 The Bayesian Setting
Our starting point is to treat privacy enforcement as a
classiﬁcation problem, being the decision whether or not
a given release point is legitimate. The events, or in-
stances, to be classiﬁed are (runtime) release points. The
labels are legitimate and illegitimate. Misclassiﬁcation
either yields a false alarm (mistaking benign information
release as a privacy threat) or a missed data leak (failing
to intercept illegitimate information release).
2.1 Bayes and Naive Bayes
Our general approach is to base the classiﬁcation on the
evidence arising at the release point. Items of evidence
may refer to qualitative facts, such as source/sink data-
ﬂow reachability, as well as quantitative measures, such
as the degree of similarity between private values and
values about to be released. These latter criteria are es-
sential in going beyond the question of whether private
information is released to also reason about the amount
and form of private information about to be released.
A popular classiﬁcation method, representing this
mode of reasoning, is based on Bayes’ theorem (or rule).
Given events X and Y , Bayes’ theorem states the follow-
ing equality:
Pr(Y|X) =
Pr(X|Y )· Pr(Y )
Pr(X)
(1)
where Pr(Y|X) is the conditional probability of Y given
X (i.e., the probability for Y to occur given that X has
occurred). X is referred to as the evidence. Given ev-
idence X, Bayesian classiﬁers compute the conditional
likelihood of each label (in our case, legitimate and ille-
gitimate).
We begin with the formal background by stating Equa-
tion 1 more rigorously. Assume that Y is a discrete-
valued random variable, and let X = [X1, . . . ,Xn] be a
1http://sseblog.ec-spride.de/tools/droidbench/
2 researcher.ibm.com/researcher/ﬁles/us-otripp/Artifacts.zip
USENIX Association  
23rd USENIX Security Symposium  177
3
%
%


vector of n discrete or real-valued attributes Xi. Then
Pr(Y = yk|X1 . . .Xn) =
Pr(Y = yk)· Pr(X1 . . .Xn|Y = yk)
Σ j Pr(Y = y j)· Pr(X1 . . .Xn|Y = y j)
(2)
As Equation 2 hints, training a Bayesian classiﬁer is, in
general, impractical. Even in the simple case where the
evidence X is a vector of n boolean attributes and Y is
boolean, we are still required to estimate a set
θi j = Pr(X = xi|Y = y j)
of parameters, where i assumes 2n values and j assumes
2 values for a total of 2·(2n−1) independent parameters.
Naive Bayes deals with the intractable sample com-
plexity by introducing the assumption of conditional in-
dependence, as stated in Deﬁnition 2.1 below, which re-
duces the number of independent parameters sharply to
2n. Intuitively, conditional independence prescribes that
events X and Y are independent given knowledge that
event Z has occurred.
Deﬁnition 2.1 (Conditional Independence). Given ran-
dom variables X, Y and Z, we say that X is conditionally
independent of Y given Z iff the probability distribution
governing X is independent of the value of Y given Z.
That is,
∀i, j,k. Pr(X = xi|Y = y j,Z = zk) =Pr(X = xi|Z = zk)
Under the assumption of conditional independence,
we obtain the following equality:
Pr(X1 . . .Xn|Y ) =Π n
i=1 Pr(Xi|Y )
(3)
Therefore,
Pr(Y = yk|X1 . . .Xn) =
Pr(Y = yk)· Πi Pr(Xi|Y = yk)
Σ j Pr(Y = y j)· Πi Pr(Xi|Y = y j)
(4)
2.2 Bayesian Reasoning about Leakage
For leakage detection, conditional independence trans-
lates into the requirement that at a release point st, the
“weight” of evidence e1 is not affected by the “weight”
of evidence e2 knowing that st is legitimate/illegitimate.
As an example, assuming the evidence is computed as
the similarity between private and released values, if st
is known to be a statement sending private data to the
network, then the similarity between the IMSI number
and respective values about to be released is assumed to
be independent of the similarity between location coor-
dinates and respective values about to be released.
The assumption of conditional independence induces
a “modular” mode of reasoning, whereby the privacy
features comprising the evidence are evaluated indepen-
dently. This simpliﬁes the problem of classifying a re-
lease point according to the Bayesian method into two
quantities that we need to clarify and estimate: (i) the
likelihood of legitimate/illegitimate release (Pr(Y = yk))
and (ii) the conditional probabilities Pr(Xi|Y = yk).
3 Privacy Features
In this section we develop, based on the mathematical
background in Section 2, an algorithm to compute the
conditional likelihood of legitimate versus illegitimate
data release given privacy features Fi. With such an al-
gorithm in place, given values vi for the features Fi, we
obtain
vleg = Pr(legitimate| [F1 = v1, . . . ,Fn = vn])
villeg = Pr(illegitimate| [F1 = v1, . . . ,Fn = vn])
Bayesian classiﬁcation then reduces to comparing be-
tween vleg and villeg, where the label corresponding to
the greater of these values is the classiﬁcation result.
3.1 Feature Extraction
The ﬁrst challenge that arises is how to deﬁne the fea-
tures (denoted with italicized font: F) corresponding to
the private values (denoted with regular font: F). This re-
quires simultaneous consideration of both the actual pri-
vate value and the “relevant” values arising at the sink
statement (or release point). We apply the following
computation:
1. Reference value: We refer to the actual private value
as the reference value, denoting the value of pri-
vate item F as [[F]]. For the example in Figures 1–
2, the reference value, [[IMSI]], of the IMSI fea-
ture would be the device’s IMSI number: [[IMSI]]
= “404685505601234”.
2. Relevant value: We refer to value v about to be re-
leased by the sink statement as relevant with respect
to feature F if there is data-ﬂow connectivity be-
tween a source statement reading the value [[F]] of
F and v. Relevant values can thus be computed via
information-ﬂow tracking by propagating a unique
tag (or label) per each private value, as tools like
TaintDroid already do. Note that for a given feature
F, multiple different relevant values may arise at a
given sink statement (if the private item F ﬂows into
more than one sink argument).
3. Feature value: Finally, given the reference value [[F]]
and a set {v1, . . . ,v k} of relevant values for feature
F, the value we assign to F (roughly) reﬂects the
highest degree of pairwise similarity (i.e., minimal
178  23rd USENIX Security Symposium 
USENIX Association
4
distance) between [[F]] and the values vi. Formally,
we assume a distance metric d. Given d, we deﬁne:
[[F]] ≡ min
1≤i≤k{d([[F]],vi)}
We leave the distance metric d(. . .) unspeciﬁed for
now, and return to its instantiation in Section 3.2.
According to our description above, feature values are
unbounded in principle, as they represent the distance be-
tween the reference value and any data-dependent sink