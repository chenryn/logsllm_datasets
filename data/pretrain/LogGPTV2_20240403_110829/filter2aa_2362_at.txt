The quality of tests is affected by several factors:
1. What is the “model” built from (traffic capture, specification)?
2. How is the “intelligence” built in (attack heuristics, secure programming
practices)?
After a thorough test execution, comparing two fuzzers is easy. If one fuzzer
finds 10 times more flaws than the other, and the first fuzzer will cover all the flaws
the second fuzzer found, then the first fuzzer has a higher defect count. But is the
same defect count applicable when a different piece of software is being tested?
Looking at past experiences, it seems that randomness in a fuzzer can increase the
probability of finding flaws such as null pointers dereferences. On the other hand,
an intelligent fuzzer can easily be taught to find these same flaws if the inputs that
trigger these flaws are found.
Another aspect is related to the various definitions for a defect. Whereas a secu-
rity expert is interested in verifying the level of exploitability of a found defect, a
financial corporation or a telecom service provider or a carrier will see a DoS attack
124
Fuzzing Metrics
as the worst possible failure category. It is understandable that hackers are inter-
ested in exploits, whereas other users of fuzzers are interested in any flaws resulting
in critical failures. This is one of the main differences in quality assurance and vul-
nerability analysis: Developers need to find all reliability issues, while attackers only
need to find one that enables them to execute remote code on the target system.
4.3.3
Vulnerability Risk Metrics
The risks that software is exposed to can be assessed by each vulnerability that is
found with fuzzers or other security testing practices. A number of such categoriza-
tions exist, but they share a lot of common metrics, such as
• Criticality rating (numeric value): Metric for prioritizing vulnerabilities.
• Business impact (high/medium/low, mapped to a numeric value): Estimates
the risk or damage to assets.
• Exploitability: Type of, easiness of, or possibility for exploiting the vul-
nerability.
• Category of the compromise: Various vulnerability categories and the related
exposure through successful exploitation: confidentiality, integrity, availabil-
ity, total compromise.
• Interface: Remote or local, or further details such as identifying the used
attack vector or communication protocol.
• Prerequisites: What is needed for exploitation to succeed; for example, is there
a requirement for successful authentication?
The most abused metric for vulnerabilities is often the criticality rating, as there
is a tendency to only focus on fixing the vulnerabilities with highest rating. A defect
that is a remotely exploitable availability problem (i.e., open to denial of service
[DoS] attacks) can be easily ignored by an enterprise if is labeled with medium
impact. The attack may be initially labeled as a DoS due to the difficulty of creat-
ing a code-executing exploit or because the flaw is in a noncritical service. How-
ever, after the flaw has been made public, a better hacker could spend the time
needed to write an operational exploit for it and compromise enterprise networks
using that flaw. The criticality rating is often calculated as a factor of the business
impact and the metric for exploitability, as shown below. It is very difficult to cal-
culate the criticality rating for vulnerabilities without knowing the context in which
the software is used.
Criticality rating = Business impact * Exploitability
Business impact can indicate the direct financial impact if the flaw is exploited
in customer premises, or it can indicate the costs related to fixing the flaw if the
problem is found in development environment. Only a domain expert in the busi-
ness area can usually rate the business impact of a specific vulnerability.
The exploitability rating indicates how easy it is to develop a raw bug into a
working exploit. This variable is extremely hard to estimate as the field of exploit
development is not well documented. Many famous examples exist of vulnerabilities
4.3
Defect Metrics and Security
125
that are declared unexploitable but then someone manages to exploit them!17 Fur-
thermore, the exploitability rating can change as new techniques in exploit devel-
opment become available.
The category of the compromise is the most discussed metric for security vulner-
abilities. The simplest categorization can consist of the resulting failure mode, leaving
little interpretation to the reader. A successfully exploited vulnerability can often com-
promise one or several of the security requirements, such as confidentiality, integrity,
and availability. A vulnerability that results in leaking of information violates the con-
fidentiality requirement. Integrity-related vulnerabilities result in alteration of data.
Availability-related vulnerabilities result in denial of service. Often, such as in the case
of buffer overflow problems, the compromise is “total,” meaning the entire system is
in the attacker’s control through execution of remote code on the victim’s system.
The interface exposed through the vulnerability can be either remote or local.
Local programming interfaces (API), command line interface (CLI), and the graph-
ical user interface (GUI) can be thought as local interfaces. Remote interfaces are
often related to a specific communication protocol such as HTTP. Remote inter-
faces can also act as a transport. For example, picture formats such as GIF or TIFF
can use a network protocol as the attack vector. HTTP and SIP are also examples
of application protocols—i.e., various web or VoIP services can be running on top
of those protocols. In real life, the division to remote and local interfaces has
become less significant because almost every local interface can also be exploited
remotely through launching command line utilities or API functions through, for
example, web browsers or MIME-enabled applications.
Prerequisites limit the exploitability of the vulnerability. For example, a vulner-
ability found in the later messages in a complex SSH authentication process might
require successful authentication of a user in the system to be exploited. A local vul-
nerability in an application such as a picture viewer might only be exploitable with
the presence of a specific version or configuration.
Vulnerability risk metrics are an extremely useful tool for communicating the
found vulnerabilities in the bug reporting process. They can be used by both
the reporters to explain the importance of the found issues and by the repairers
of the problems in their own bug tracking records. Below is an example taken from
a bug report template from OUSPG:18
3. Suspected Impact and Vulnerability Type
3.1. Exploitability:
[ ] Local - by users with local accounts
[ ] Remote - over the network without an existing local
account
3.2. Compromise:
[ ] Total compromise of a privileged account UID: 
[ ] Total compromise of an unprivileged account UID: 
126
Fuzzing Metrics
17www.securityfocus.com/news/493
18M. Laakso, A. Takanen, J. Röning. “The Vulnerability Process: A Tiger Team Approach to
Resolving Vulnerability Cases.” In Proceedings of the 11th FIRST Conference on Computer Secu-
rity Incident Handling and Response. Brisbane, Australia. June 13–18, 1999.
If not a total compromise, please specify:
[ ] Confidentiality
[ ] Integrity
[ ] Availability (Denial of Service)
3.3. Timing:
[ ] Exploitable at any time
[ ] Exploitable under specific conditions: 
3.4. Vulnerability Category:
[ ] Buffer overflow
[ ] File manipulation
[ ] Ability to create user-modifiable arbitrary files
[ ] Ability to create unmodifiable arbitrary files
[ ] Ability to truncate or remove arbitrary files
[ ] Ability to change owner of arbitrary dirs or files
[ ] Ability to change protection modes of arbitrary dirs
or files
[ ] Other: 
[ ] Execution
[ ] Exploitation involves a time window of the race condition
type
[ ] Other: 
3.5. Comment on impact: 
4.3.4
Interface Coverage Metrics
The coverage metric for interfaces consists of enumerating the protocols that the
fuzzer can support. Fuzzing (or any type of black-box testing) is possible for any
interfaces, whether they are APIs, network protocols, wireless stacks, command
line, GUI, files, return values, and so on. In 2006, fuzzers existed for more than 100
different protocol interfaces. Fuzzing an IMS system will require a different set of
protocols than fuzzing a mobile phone.
Interfaces are defined with various tools and languages. The variety of descrip-
tion languages used in protocol specifications creates a challenge for fuzzers to sup-
port them all. Creating any new proprietary fuzzing description languages poses a
tedious task. What description language should we use for describing the interfaces
to enable anyone to fuzz them? Some protocol fuzzers have decided to use the def-
inition languages used by the standardization organizations such as IETF and
3GPP. The most common definition languages are BNF or its variants for both
binary and text-based protocols, XML for text-based protocols, and TTCN and
ASN.1 for binary protocols. For traffic mutation-based fuzzers, it might be benefi-
cial to collect coverage data from a wide variety of complex protocol use cases.
4.3.5
Input Space Coverage Metrics
The actual coverage inside the selected interface definition or against a protocol
specification is a critical metric for fuzzer quality. This is because there is no stan-
dardized formula or setup for fuzzing. A wide range of variables apply to the test
4.3
Defect Metrics and Security
127
quality, especially if we are talking outside of just simple web application fuzzing
and looking at more complex low-level protocol fuzzing. Even a smart gray-box
fuzzer could take weeks to find “all” the vulnerabilities, and a set time frame just
leaves room for error. The input space coverage will help in understanding what
was tested and what was not.
Because the input space is always infinite, it will always take an infinite time
to find all flaws. Optimizing (i.e., adding intelligence) will find more flaws earlier,
but can still never ensure that “all” flaws have been found. This would not be
true, of course, for protocols that have a “physical” limit such as UDP packet size
with no fragmentation options. Only then you will have a physical limit that will
cover “all” tests for that single packet. But that would be a LOT of tests. Even
then, the internal state of the SUT will influence the possibility of finding some of
the flaws.
The input space metric explodes when combinations of anomalies are consid-
ered. There are known security related flaws that require combination of two or
three anomalies in the message for the attack to succeed. Sometimes those anom-
alies are in two different messages that need to be sent in a specific order to the sys-
tem under test.
One approach to avoid the infinity of tests is to do the tests systematically. A
carefully selected subset of tests applied across the entire protocol specification will
reach good input space coverage when measured against the specification, but will
not have good input space coverage for each data unit inside the protocol. It is pos-
sible to have both approaches combined, with some random testing and a good set
of smart robustness testing in one fuzzer. All different techniques for fuzzing can
coexist because they have different qualities from the input space coverage perspec-
tive. There is a place for both random testing (fuzzing) and systematic testing
(robustness testing).
In short, the input space is always infinite. Protocols travel in a pipe that has no
limits on the upper boundary of the packet sizes. Whereas infinity is challenging in
physics, the bits in the attack packets can be created infinitely, and they do not take
physical storage space like they would in, for example, file fuzzing. You might think
input space is finite, like the way Bill thought we never need more than 640k of
memory (well, he did not really say it, but it’s a great urban myth anyway), but in
reality we can see this is not the case.
If a protocol specification says that:
 = 
 = 1..256 * 
How many tests would we need? Fuzzing this (and not forgetting to break the
specification in the process) will require an infinity of tests, provided we do not
have any other limitations. Is 2^16+1 a long enough stream to inject? Is 16GB too
much to try? What about if we send even more? Where should we stop? But as you
said, it might not be sensible to go that far, but what if it will crash with one more
additional byte of data? Or two more? Why set upper limits to fuzzing data? The
input space is always infinite. Even if the protocol specification of a one-byte mes-
sage is simple like:
128
Fuzzing Metrics
 = "A" or "B"
You should definitely try all possible octets besides “A” and “B”, resulting in
256 tests. You should also try all of those in all possible encoding formats, and with
that the input space is already getting quite large. And by adding repetitions of
those inputs, the input space becomes infinite as there is no upper bound for repe-
tition, unless an upper bound is defined by lower level protocol such as UDP.
With some protocols in some specific use scenarios, you can potentially meas-
ure the maximum size of input space. You can potentially map the upper bounds
by, for example, increasing the message size until the server side refuses any more
data and closes the connection (it may still have crashed in the process). Any test
with bigger messages would not add to the test coverage.
Another aspect of input space is created by dynamic (stateful) protocols. What
if a flaw can be triggered by a combination of two different packets? Or three? The
message flows will also increase the input space.
A key question with any infinite input space is: Does randomness add to the
value of the tests? When speaking about systematic vs. random (nonsystematic), we
need to understand what “randomness” actually means. The quality of the random-
ness will have some impact to the resulting input space coverage. Let’s look at two
widely used “algorithms.” These are not true algorithms, but can be used as exam-
ples assuming someone wants to start building a library of fuzzing or test genera-
tion methodologies:
“2^x, +-1, +-2, ...” is systematic test generation
“2^x + rand()” is not systematic, even if you have control on the used seed use
for the random number generation.
Note that “bit-flipping” can also be both random or systematic because you do
not know the “purpose” of each test. There are dozens of articles on random test-
ing and white-noise testing, the related problems, and where it makes sense and
where it does not. We will explore this later in relation to random fuzzing.
While fuzzers do not have to bother with conformance testing, they still often
have to build their negative fuzz tests based on any available RFCs or other relevant
specifications. A solid fuzzer or robustness tester is based on specifications, while
breaking those very same specifications at every turn. The more specifications a fuzzer
covers, the better input space coverage it will have, and the more flaws it will find.
There are two branches of negative testing:
1. Fuzzing starts from infinite and tries to learn intelligence in structures.
2. Robustness testing starts from finite and tries to add intelligence in data
inputs.
After some development, both will result in better tests in the end, merging into
one semi-random approach with a lot of intelligence on interface specifications and
real-life vulnerability knowledge. The optimum approach is probably somewhere in
the middle of the two approaches. For some reason, security people start with the
first approach (perhaps because it is easiest for finding one single problem quickly).
4.3
Defect Metrics and Security
129
QA people often prefer the second approach because it fits better to the existing
QA processes, whose purpose is to find “all” flaws. In our opinion, the famous Tur-
ing problem applies to approach number one more than to approach number two.
Fuzzers should definitely also do combinations of anomalies in different areas
of the protocol for good input space coverage. This is where random testing can
bring huge value over systematic robustness testing. If you stick to simple anomalies
in single fields in the protocol specifications, why use random fuzzing at all? It is
much more effective to build systematic tests that walk through all elements in the
protocol with a simple set of anomalies.
4.3.6
Code Coverage Metrics
Code-based reviews, whether manual or automated, have been used for two decades
and can be built into compilers and software development environments. Several
terms and metrics can be learned from that practice, and we will next go through
them briefly. Code security metrics that we will examine are
• Code volume (in KLOC = Thousand Lines of Code) shows the aggregate size
of the software;
• Cyclomatic complexity shows the relative complexity of developed code;
• Defect density (or vulnerability density) characterizes the incidence rate of
security defects in developed code;
• Known vulnerability density characterizes the incidence rate of security
defects in developed code, taking into account the seriousness (exploitability)
of flaws;
• Tool soundness estimates the degree of error intrinsic to code analysis tools.
One of the most common code volume metrics is “lines of code” (LOC), or
“thousands of lines of code” (KLOC). Although it appears to be a very concrete
metric, the code volume is still not unambiguous and perfect. However, LOC is still
less subjective than most other code volume metrics such as those based on use
cases or other metrics that are used to estimate the code size during requirements or
design. Code volume can be measured at different layers of abstraction, but it most
typically is measured from the abstraction layer of the handwritten or generated
high-level programming languages. Therefore, the programming style and the pro-
gramming practices used will affect the resulting metric for the amount of code. As
a better alternative to counting lines of code, some coding style checking tools, pro-
filers, and code auditing tools will measure the number of statements in the project.
McCabe’s metric for cyclomatic complexity provides one useful estimate for the
complexity of the code. This and several other metrics are based on the number of
branches and loops in the software flow. The most valuable metric for fuzzing
comes from being able to assess the ratio of what was tested against the total avail-
able. Understanding the limitations of chosen metrics is important. We will come
back to this topic later in the chapter.
The next metric we will discuss is defect density. The number of defects that are
found via a code audit can vary significantly based on the tools that are used and
the individuals conducting the assessment. This is because people and tools can only
130
Fuzzing Metrics
catch those bugs that they can recognize as defects. A security flaw in the code is not
always easily identified. The simplest of security mistakes can be found by search-
ing for the use of function calls that are known to be prone to errors, such as
strcpy(), but not all mistakes are that easy to catch. Also, code auditing tools such
as RATS, ITS4, flawfinder, pscan, and splint, and their commercial counterparts,19
use different criteria for finding mistakes such as unsafe memory handling, input
validation, and other suspicious constructs.
This variance in defect density depends on the interpretation of false positives
and false negatives in the code auditing process. A false positive is an issue that is
labeled as a (security-related) defect, but after a closer study turns out to be an
acceptable piece of code (based on the used programming policy). False positives
are created due to inconsistencies in programming practices. Either the enforced
coding policies have to be taught to the code auditing tool, or the coding practices
could be adapted from the tool. A more challenging problem is related to false
negatives, which basically means a mistake in the code was missed by the code
auditing tool. The definitions of false positive and false negative are also subjec-
tive, as some people define them based on the exploitability of the security flaws.20
To some, a bad coding practice such as using an unsafe string copy function is
acceptable if it is not exploitable. Yet to others, all code that can pose a risk to the
system is to be eliminated regardless of whether it can actually be exploited. The
latter is safer, as it is very common to adapt code from noncritical projects into
more critical areas, propagating potential weakness in the process.
Known vulnerability density as a metric is a modified variant of the defect den-
sity, with weights added to indicate the significance of the found issues. All defect
density metrics are estimates that are based on proprietary metrics of the code audit-
ing tools. Until the problems with the accuracy are sorted out, the defect density met-
rics given by code auditing tools are valuable as guidance at best. The Software
Assurance Metrics and Tool Evaluation (SAMATE)21 project has proposed using a
metric called “soundness,” which attempts to estimate the impact of false positives
and false negatives to the defect rate. This estimate is based on a calibration, a com-
parison of the code auditing tool to a well-performed manual code review.
Metrics based on code coverage have also been used in analyzing the efficiency
of fuzzers. In a master’s thesis published in 2004 by Tero Rontti on this topic for
Codenomicon and University of Oulu, code coverage analysis of robustness tests
were applied to testing various open-source implementations of TLS and BGP.22 The
research indicated that in some areas fuzzing can potentially have better test code
coverage than traditional testing techniques. Interestingly (but not surprisingly),
4.3
Defect Metrics and Security