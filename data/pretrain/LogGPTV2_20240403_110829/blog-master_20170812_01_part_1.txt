## PostgreSQL on ECS多云盘的部署、快照备份和恢复  
### 作者                      
digoal                     
### 日期                       
2017-08-12                 
### 标签                
PostgreSQL , ECS , 云盘 , 快照 , 一致性备份 , 时间点恢复 , zfs , lvm , raid , 并行计算       
----                
## 背景          
随着阿里云云盘的发展，云盘的性能已经越来越好了。IOPS可以做到十万以上，读写吞吐也超过1GB/s了。相信随着网络的发展，SSD云盘IOPS突破40万，读写吞吐突破4GB/s也不远了。  
不过这里的IOPS和吞吐是指并发的IO能力，单次IO的延迟与本地还是不能比（将来RDMA网络也许能解决这个问题）。  
### PostgreSQL 如何解决SSD云盘IO延迟高缺陷  
某些业务对数据库的（要求响应时间很快的写小事务）对单次IO延迟比较敏感，不过PostgreSQL有方法可以解决这个小缺陷。  
1、后台IO，(write syscall)  
PostgreSQL 的大多数IO为后台IO(bgwriter, backend writer)，所以刷shared buffer对IO延迟不敏感。  
2、异步提交  
事务提交时，写WAL异步提交。不会造成数据不一致，但是当数据库CRASH，可能丢失在wal buffer中未提交的事务（最多10毫秒）。  
这种方法是最有效的。  
3、组提交  
组提交，解决WAL写瓶颈，将多个同时提交的事务的WAL fsync动作合并为单次，从而减少FSYNC次数，提高高并发时的写小事务的TPS。  
### PostgreSQL 如何充分利用并发IO的能力  
由于我们看到的云盘IOPS和读写吞吐指标是并发指标，数据库如何利用好这么好的指标呢？对高并发小事务不是问题，肯定是能将它用起来的，但是对于低并发，长事务（分析型业务），如何利用云盘的IOPS能力和读写吞吐能力呢？  
PostgreSQL的并行计算特性可以充分利用云盘的并发IOPS和读写带宽。  
### 多云盘卷组  
单块云盘的IOPS能力有上限，容量有上限，读写带宽也有上限。好在ECS支持多块云盘，目前已支持一台ECS挂载16块云盘。  
通过多块云盘，组卷条带后，提高读写带宽。以Linux RHEL/CentOS 7.x为例，组卷和条带方法：  
1、逻辑卷  
```  
yum install lvm2  
```  
2、软RAID  
```  
yum install -y mdadm  
```  
3、ZFS  
http://zfsonlinux.org/  
```  
以centos 7.3为例  
wget http://download.zfsonlinux.org/epel/zfs-release.el7_3.noarch.rpm  
rpm -ivh zfs-release.el7_3.noarch.rpm   
yum install -y zfs  
```  
## 逻辑卷例子  
假设环境中有16块SSD云盘  
1、创建PV  
```  
pvcreate /dev/vd[b-q]  
```  
2、创建VG  
```  
vgcreate -A y -s 128M vgdata01 /dev/vd[b-q]  
```  
```  
-s, --physicalextentsize PhysicalExtentSize[bBsSkKmMgGtTpPeE]  
       Sets  the physical extent size on physical volumes of this volume group.  A size suffix   
       (k for kilobytes up to t for terabytes) is optional, megabytes is the default if no suffix is present.   
       For LVM2 format, the value  
       must be a power of 2 of at least 1 sector (where the sector size is the largest sector size of the   
       PVs currently used in the VG) or, if not a power of 2, at least 128KiB.  For the older LVM1 format, it must be a power  
       of  2  of at least 8KiB.  The default is 4 MiB.  Once this value has been set, it is difficult to   
       change it without recreating the volume group which would involve backing up and restoring data on any logical volumes.  
       However, if no extents need moving for the new value to apply, it can be altered using vgchange -s.  
       If the volume group metadata uses lvm1 format, extents can vary in size from 8KiB to 16GiB and   
       there is a limit of 65534 extents in each logical volume.  The default of 4 MiB leads to a maximum logical volume size  of  
       around 256GiB.  
       If  the  volume group metadata uses lvm2 format those restrictions do not apply, but having a   
       large number of extents will slow down the tools but have no impact on I/O performance to the logical volume.  The smallest  
       PE is 1KiB  
       The 2.4 kernel has a limitation of 2TiB per block device.  
```  
3、创建LV，设置条带  
16块盘，每个条带单位为8KB。  
```  
lvcreate -A y -i 16 -I 8 -l 100%VG -n lv01 vgdata01  
```  
```  
-i|--stripes Stripes  
       Gives the number of stripes.  This is equal to the number of physical volumes to scatter the logical volume data.    
       When creating a RAID 4/5/6 logical volume, the extra devices which are necessary for parity are internally accounted for.    
       Specifying -i 3 would cause 3 devices for striped and RAID 0 logical volumes, 4 devices for   
       RAID 4/5, 5 devices for RAID 6 and 6 devices for RAID 10.  Alternatively, RAID 0 will stripe  across  2  
       devices,  RAID  4/5  across  3  PVs,  RAID  6  across  5  PVs and RAID 10 across 4 PVs in the volume group   
       if the -i argument is omitted.  In order to stripe across all PVs of the VG if the -i argument is omitted, set  
       raid_stripe_all_devices=1 in the allocation section of lvm.conf (5) or add  
       --config allocation/raid_stripe_all_devices=1  
       to the command.  
       Note the current maxima for stripes depend on the created RAID type.  For raid10, the maximum of stripes is 32,   
       for raid0, it is 64, for raid4/5, it is 63 and for raid6 it is 62.  
       See the --nosync option to optionally avoid initial syncrhonization of RaidLVs.  
       Two implementations of basic striping are available in the kernel.  The original device-mapper implementation   
       is the default and should normally be used.  The alternative implementation using MD, available since  ver‐  
       sion  1.7  of  the  RAID  device-mapper  kernel  target (kernel version 4.2) is provided to facilitate the   
       development of new RAID features.  It may be accessed with --type raid0[_meta], but is best avoided at present  
       because of assorted restrictions on resizing and converting such devices.  
-I|--stripesize StripeSize  
       Gives the number of kilobytes for the granularity of the stripes.  
       StripeSize must be 2^n (n = 2 to 9) for metadata in LVM1 format.  For metadata in LVM2 format,   
       the stripe size may be a larger power of 2 but must not exceed the physical extent size.  
```  
4、创建文件系统，设置条带  
```  
当数据库数据块=32K时，chunk大小(单块盘读写32KB，再写下一块)。 条带大小512KB(32KB*16)。  
mkfs.ext4 /dev/mapper/vgdata01-lv01 -m 0 -O extent,uninit_bg -E lazy_itable_init=1,stride=8,stripe_width=128 -b 4096 -T largefile -L lv01  
或  
当数据库数据块=8K时，chunk大小(单块盘读写8KB，再写下一块)。 条带大小128KB(8KB*16)。  
mkfs.ext4 /dev/mapper/vgdata01-lv01 -m 0 -O extent,uninit_bg -E lazy_itable_init=1,stride=2,stripe_width=32 -b 4096 -T largefile -L lv01  
```  
```  
-b block-size  
       Specify the size of blocks in bytes.  Valid block-size values are 1024, 2048 and 4096 bytes per block.    
       If omitted, block-size is heuristically determined by the filesystem size and the expected usage of the  filesys‐  
       tem  (see  the  -T option).  If block-size is preceded by a negative sign ('-'), then mke2fs will use   
       heuristics to determine the appropriate block size, with the constraint that the block size will be at least block-  
       size bytes.  This is useful for certain hardware devices which require that the blocksize be a multiple of 2k.  
stride=stride-size  单位为blocks  
       Configure the filesystem for a RAID array with stride-size filesystem blocks. This is the number   
        of blocks read or written to disk before moving to the next disk, which is  sometimes  referred  to  as  the  
       chunk  size.   This mostly affects placement of filesystem metadata like bitmaps at mke2fs   
        time to avoid placing them on a single disk, which can hurt performance.    
        It may also be used by the block allocator.  
stripe_width=stripe-width  单位为blocks  
       Configure the filesystem for a RAID array with stripe-width filesystem blocks per stripe.   
        This is typically stride-size * N, where N is the number of data-bearing disks in the RAID   
        (e.g. for RAID  5  there  
       is  one  parity disk, so N will be the number of disks in the array minus 1).    
        This allows the block allocator to prevent read-modify-write of the parity in a   
        RAID stripe if possible when the data is written.  
lazy_itable_init[= ]  
      If enabled and the uninit_bg feature is enabled, the inode table will not be fully   
      initialized by mke2fs.  This speeds up filesystem initialization noticeably,   
      but it requires the kernel to finish initial‐  
      izing the filesystem in the background when the filesystem is first mounted.    
      If the option value is omitted, it defaults to 1 to enable lazy inode table zeroing.  
```  
5、MOUNT文件系统  
```  
如果不使用云盘镜像备份，使用这种挂载模式。  
mount -o defaults,noatime,nodiratime,nodelalloc,barrier=0,data=writeback LABEL=lv01 /data01  
或  
如果使用云盘镜像备份，使用这种挂载模式。  
mount -o defaults,noatime,nodiratime,nodelalloc,barrier=1,data=ordered LABEL=lv01 /data01  
```  
6、FIO测试  
```  
yum install -y fio  
fio -filename=/data01/testdir -direct=1 -thread -rw=write -ioengine=libaio -bs=8K -size=16G -numjobs=128 -runtime=60 -group_reporting -name=mytest >/tmp/fio_write.log 2>&1  
fio -filename=/data01/testdir -direct=1 -thread -rw=read -ioengine=libaio -bs=8K -size=16G -numjobs=128 -runtime=60 -group_reporting -name=mytest >/tmp/fio_read.log 2>&1  
fio -filename=/data01/testdir -direct=1 -thread -rw=randwrite -ioengine=libaio -bs=8K -size=16G -numjobs=128 -runtime=60 -group_reporting -name=mytest >/tmp/fio_randwrite.log 2>&1  
fio -filename=/data01/testdir -direct=1 -thread -rw=randread -ioengine=libaio -bs=8K -size=16G -numjobs=128 -runtime=60 -group_reporting -name=mytest >/tmp/fio_randread.log 2>&1  
```  
```  
顺序  
READ: io=72621MB, aggrb=1210.3MB/s, minb=1210.3MB/s, maxb=1210.3MB/s, mint=60003msec, maxt=60003msec  
WRITE: io=36845MB, aggrb=628743KB/s, minb=628743KB/s, maxb=628743KB/s, mint=60007msec, maxt=60007msec  
随机  
READ: io=53390MB, aggrb=911160KB/s, minb=911160KB/s, maxb=911160KB/s, mint=60002msec, maxt=60002msec  
WRITE: io=26078MB, aggrb=445032KB/s, minb=445032KB/s, maxb=445032KB/s, mint=60004msec, maxt=60004msec  
```  
## 数据库部署  
1、安装软件  
[《PostgreSQL on Linux 最佳部署手册》](../201611/20161121_01.md)    
2、初始化数据库  
```  
initdb -E SQL_ASCII -U postgres --lc-collate=C --lc-ctype=en_US.utf8 -D $PGDATA  
```  
3、配置postgresql.conf  
```  
port = 1921                               
max_connections = 1000                    
superuser_reserved_connections = 13       
unix_socket_directories = '.'     
shared_buffers = 64GB                     
#  vm.nr_hugepages = 102352    
#  建议shared buffer设置超过64GB时 使用大页，页大小 /proc/meminfo Hugepagesize
maintenance_work_mem = 1GB                
dynamic_shared_memory_type = posix        
vacuum_cost_delay = 0                     
bgwriter_delay = 10ms                     
bgwriter_lru_maxpages = 500               
bgwriter_lru_multiplier = 5.0             
bgwriter_flush_after = 0                  
effective_io_concurrency = 16             
max_worker_processes = 128                
max_parallel_workers_per_gather = 64      
max_parallel_workers = 128                
backend_flush_after = 0           
wal_level = minimal                       
synchronous_commit = off                  
wal_sync_method = fsync           
full_page_writes = on                     
wal_buffers = 1GB                         
wal_writer_delay = 10ms           
wal_writer_flush_after = 0                
checkpoint_timeout = 35min                
max_wal_size = 128GB  
min_wal_size = 8GB  
checkpoint_completion_target = 0.5        
checkpoint_flush_after = 0                
checkpoint_warning = 30s                  
max_wal_senders = 0               
random_page_cost = 1.2                    
parallel_tuple_cost = 0           