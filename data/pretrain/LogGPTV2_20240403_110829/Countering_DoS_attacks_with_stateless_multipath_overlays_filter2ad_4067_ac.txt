sponsive or because the connection quality is low. After resetting
the connection, the user has to re-establish connectivity and re-
authenticate himself, making the system unrealistic for real-time
applications. Moreover, frequently forcing the user to re-authenticate
through a challenge-response or a CAPTCHA will render the sys-
tem unusable for any type of application.
We assume that an attacker can mount a DoS attack against a
small set of nodes in the overlay for short periods of time, which
(in ﬁrst-generation IONs) will force clients using those overlay
nodes to reset their connections to new nodes. This attacker blindly
sweeps all the nodes participating in the overlay network, focus-
ing his attack from one set of overlay nodes to another, keeping
the sets disjoint. Not all of these attacks can be easily detected by
the current infrastructure: an attacker can mount a low-rate TCP
attack [16] reducing the effective bandwidth of the victim to zero.
Thus, a sweeping attacker can cause signiﬁcant disruption in the
end-to-end communication.
To analyze a sweeping attack and quantify its impact to the clients’
connection characteristics in ﬁrst-generation IONs, we create a sim-
ple static model. We assume that the attacker can bring down pd
percentage of the overlay nodes simultaneously. For an attack to
be successful on these nodes, it needs ta time of sustained attack.
This is the time required to either drop or severely rate-limit the
connections of all the clients connected to nodes under attack. Let
tu be the average time a client is connected to the system. Also,
let td be the time that is necessary for the client to detect the attack
and connect to another overlay node. Moreover, we assume that the
overlay repairs the nodes under attack immediately after the attack
focus has shifted to another set of nodes (zero reboot or repair time)
so the time to repair tr = 0. Finally, we assume that clients are
connected uniformly across all overlay nodes in a ﬁrst-generation
ION, i.e., if there are N clients and O overlay nodes, each has N
O
clients. The percentage of clients that will have their connection
reset by a sweeping attack at least once during the time that they
tu
ta
use the system is P1(tu, ta, pd) =
· pd assuming tdet = 0.
The above formula is very intuitive: from the attacker’s perspec-
tive, there are 1
disjoint sets of nodes in the overlay network. To
pd
attack all of them the attacker needs ta
time. Assuming a system
pd
where we have no joins, an attacker will affect the connectivity of
· pd clients. Note that some of the clients may never experi-
tu
ta
ence the attack because they might have ﬁnished their connection
by the time the attack reaches them. For this simple model, we
have assumed that there is no detection time: the user selects an-
other overlay node to connect to as soon as the attack starts to affect
him. Even with this very conservative model (td = 0, tr = 0, no
client arrivals while the system is under attack) we can see that the
attack can be signiﬁcant, depending on the usage time, the size of
the attack compared to the size of the overlay and the time required
for an attack to be successful. For example, assuming that we have
clients with average usage time of an hour, an adversary that can at-
tack 2.5% of the overlay nodes and shifts the attack every 5 minutes
will affect 30% of the clients. We can also compute the percentage
of nodes that will have to reset their connections more than once.
The percentage of nodes that will have to reset their connections at
least k > 1 times during the attack is:
⌊
tu
ta
⌋
P
i=1
Pk =
P(k−1)([tu − i · ta], ta, pd) · pd
In general, for td  D. What is a likely value for D? The Click software
router with commodity hardware [15] claims a switching capacity
of 435,000 64-byte packets, or 222 Mbps. Taking a more conserva-
tive value of 50 Mbps, an attacker can saturate an overlay node by
using 1, 740 hosts. Furthermore, an attacker controlling 100, 000
nodes (not enough to directly attack the target) can render approxi-
mately 60 geographically dispersed, well connected overlay nodes
inaccessible at a time. Assuming an overlay network of a size com-
parable to Akamai’s (approximately 2, 500 nodes), the attacker can
render 2.5% of the overlay unusable.
To guarantee packet delivery at a given probability Ps in the
presence of such attacks, we need to select the number of packet
replicas R such that Ps = 1 − ( K
D×N )R or Ps = 1 − f R, where
f is the percentage of the attacked nodes. If we assume that users
initiate TCP connections with the protected server, then Ps should
be no less than 90%, otherwise the connections stall [19]. From the
formula for Ps and using the fact that Ps = 0.9 for TCP, we can
compute the required bandwidth given the size of the network, or
the fraction of nodes that need to be successfully attacked to disrupt
the user’s TCP session. For example, if we send each packet twice,
i.e., have a packet replication R = 2, the attacker has to bring down
32% of the nodes participating in the overlay network. For an over-
4For
network/interactive
see http://global.mci.com/about/
example,
lay network of 2, 500 nodes, an attacker needs to gain access and
coordinate a network of 1, 375, 000 zombies.
In addition, if we
increase the packet replication value to R = 3, the percentage of
nodes that need to get compromised jumps to 46% — almost half
of the nodes in the overlay network.
To avoid imposing extra trafﬁc on the network by replicating
each packet, we can instead select the packets that we replicate
at random with a probability Pr. Now Ps becomes Ps = 1 −
f (1 − Pr(1 − f )) since the probability that a packet will fail the
ﬁrst time transmitted is f and the failure probability for the possi-
bly replicated packet is (1 − Pr(1 − f )). Again, using Ps = 0.9 for
TCP, we see that if we replicate 50% of the transmitted packets, the
fraction of the nodes that need to get compromised is 17%, which
is signiﬁcant for medium to large overlay networks. Another ap-
proach to replication is to use forward error correction codes such
as Erasure Codes, which we intend to examine in future work.
We experimentally veriﬁed the validity of this analysis with the
prototype on the PlanetLab network, as we discuss next.
5. PERFORMANCE EVALUATION
Just as important as security is the impact of our system on regu-
lar communications, whether under attack conditions or otherwise;
a prohibitively expensive mechanism (in terms of increased end-
to-end latency or decreased throughput) is obviously not an attrac-
tive solution. In our experiments, we measured the communication
overhead of our system in terms of end-to-end throughput and la-
tency. To provide a realistic network environment, we deployed
and used our prototype with 76 PlanetLab nodes.
we measured link characteristics such as end-to-end latency and
throughput when we interposed the overlay network of access points
between the client and the target server. To measure throughput,
we used a target server that was located at Columbia. For our la-
tency measurements, we used www.cnn.com as the target.
In
both cases, the goal of the client was to establish a communication
with the target server. To do so, the client used UDP encapsu-
lation on the TCP packets generated by an SCP session and then
spread the UDP packets to the nodes participating on the overlay
network, as we described in Section 2.2. Those packets were in
turn forwarded to a pre-speciﬁed overlay node (the secret servlet).
This node decapsulated and forwarded the TCP frames to the target
server. Since our throughput connection measurements involve a
client and a server that were co-located, we effectively measured
the worst possible scenario (since our otherwise local trafﬁc had to
take a tour of the Internet). A non-co-located server would result
in a higher latency and lower throughput for a direct client-server
connection, leading to comparatively better results when we use the
overlay. Surprisingly, in some cases we can achieve better latency
using the overlay rather than connecting directly to the server.
Figure 5: Throughput results in KB/sec. When we increase the repli-
cation, the results become closer to what we have observed for the direct
connection (1250 KB/sec).
For our evaluation, we used a testbed consisting of Planetlab ma-
chines located at various sites in the continental US. Those ma-
chines were running UML Linux on commodity x86 hardware and
were connected using Abilene’s Internet-2 network. Using these
fairly distributed machines, we constructed our overlay network of
access points by running a small forwarding daemon on each of
the participating machines.
In addition, we used two more ma-
chines, acting as client and server respectively. In our experiments,
Figure 6: End-to-end average latency results for the index page and
a collection of pages for www.cnn.com. The different points denote the
change in the end-to-end latency through the overlay (To) when com-
pared to the direct connection (Td). Different lines represent different
sized overlays. Increasing the replication factor, and for larger net-
works, we get lower average latency results because of the multipath
effect on the transmitted packets.
Figure 5 shows that the impact on the downlink is only 33% in
the worst case scenario, and it is easily amended by adding packet
replication in the uplink direction. Again, we notice that the repli-
cation factor can cause a drop in the throughput for values > 100%
in small overlay networks. Looking at the end-to-end average la-
tency results in Figure 6, we notice that as we increase the replica-
tion factor, and for larger networks, we get better average latency
results. In the worst-case scenario, we get a 2.5 increase in latency,
which drops to 1.5 with 50% packet replication (i.e., probability of
replicating a packet of 50%).
 0%
 50%
 100%
 200%
)
s
(
y
c
n
e
t
a
L
1.60
1.40
1.20
1.00
0.80
0.60
0.40
0.20
0.00
 0
 10
 20
 30
 40
 50
% Node Failures
Figure 8: Impact of attacks against the overlay network on end-to-end
latency. Different curves represent varying levels of packet replication.
With 200% packet replication, latency increases by less than 25% when
up to 50% of nodes are rendered unusable by an attacker.
Figure 9: Tickets/sec produced from a single overlay node as we vary
the size of the client’s public key. The machine used was a 3GHz Intel
Pentium 4 with 1GB of RAM.
incidents, recent studies have shown a surprisingly high number of
DoS attacks occurring constantly throughout the Internet [17, 5].
SOS [13] ﬁrst suggested the concept of using an overlay network
to preferentially route trafﬁc from legitimate users to a secret node
(that can change over time), which is allowed to reach the protected
server. All other trafﬁc is restricted at the ISP’s POP, which in most
cases has enough capacity to handle all attack and legitimate traf-
ﬁc (the bottleneck is typically in the protected server’s access link).
Since the routers perform white-list ﬁltering, the overhead of the
system is negligible. In the original SOS approach, admission to
the overlay was done based on public-key (or, more generally, cryp-
tographic) authentication, requiring prior knowledge of the set of
legitimate users. WebSOS [18] relaxes this restriction by adding a
Graphic Turing Test to the overlay, allowing the system to differen-
tiate between human users and attack zombies. MOVE [20] elim-
inates the dependency on network ﬁltering at the ISP POP routers
by keeping the current location of the server secret and using pro-
cess migration to move away from targeted locations. Mayday
Figure 7: Throughput results in KB/sec when we utilize the uplink of
our client under attack. The attack happens on a random fraction of
the overlay nodes. Packet replication helps us achieve higher network
resilience, something that we expected from our analytical results.