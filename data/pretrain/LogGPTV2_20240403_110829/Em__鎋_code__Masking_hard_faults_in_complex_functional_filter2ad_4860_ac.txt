### Document Information
- **Authorized Use**: Tsinghua University
- **Download Date**: March 20, 2021
- **Download Time**: 09:58:33 UTC
- **Source**: IEEE Xplore
- **Restrictions**: Apply

### Abstract
Prediction would have squashed the instruction before it was issued. Therefore, partial prediction support suffers from the loss of actually executing the instruction but gains the benefit of being easily implemented in modern microprocessor designs. After if-conversion on the code segment in Figure 3, the predicted path, in which neither the inputs nor the result are NaN or normal, results in an average of 33 cycles and no hard-to-predict branches.

### Code Example
```assembly
1. reg1 <- reg1 xor reg1
2. reg0 <- zSig0 shl 1
3. reg0 <- reg0 subtract 0
4. reg1 <- setz
5. zSig0 <- zSig0 shl reg1
6. zExp <- zExp add reg1
```
**Figure 4**: The check for the mantissa of a 32-bit floating-point product result being too large after if-conversion. If-conversion in this case is solved by using the set conditional instruction. This instruction allows a register to be set to 1 if the condition code is true; otherwise, it does not change the register. Thus, the simple if-conversion looks like Figure 4.

Another instruction that many architectures implement is a move conditional instruction. This allows for the conditional move of a value from a register/memory location to another register/memory location. This instruction could be used for optimization, but it did not prove useful in the hand-optimized traces as the places where it could have been added did not involve hard-to-predict branches.

### New Micro-Operations
In the traces, there is a small set of subroutines that account for many of the dynamic uops seen in practice. Many of these operations can be implemented in the internal RISC ISA with little hardware overhead. For example, a jamming right shift (jshr) was performed many times during the traces [8]. The jshr instruction differs from a standard right shift (shr) by performing a logical OR of all the bits shifted off and the least significant bit in the result, and placing the result of the OR into the least significant bit. Implementing this instruction is inexpensive and will probably not add to the critical path of the execute unit.

**Figure 5**: The microcode necessary to implement a jshr instruction without modification to the shift unit. Src is the source that needs to be jamming right shifted. Shift count is the number to shift the source by. Shift size is the size of the register being operated on.

```assembly
1. reg0 <- src
2. reg1 <- reg0
3. reg0 <- reg0 shr shift_count
4. reg2 <- shift_count
5. reg2 <- neg reg2
6. reg2 <- shift_size plus reg2
7. reg1 <- reg1 shl reg2
8. reg3 <- setnz
9. reg0 <- reg0 or reg3
```

Another set of micro-ops implemented are the pfloat micro-ops. These micro-ops take a mantissa, an exponent, and a sign. There is a separate pfloat for each precision of floating-point numbers. Each pfloat shifts the exponent left by the appropriate number, the sign left by the appropriate number, and adds in the mantissa. However, since the shifts are a static number based on the precision of the float, there is no real need to shift, just to move the bits to the right place. There is an add operation between the mantissa and the exponent. However, since the add is only relevant to the exponent, the size of the add needed is 8 bits for the single-precision pfloat and 11 bits for the double-precision pfloat. With the add being the only computation needed to pack a float, the pfloat micro-ops were assigned the same delay as a regular add micro-op, which is one cycle.

### Optimizing the Expected Path
Traces generated using an optimizing compiler without the benefit of the optimization techniques described in this work were found to provide unacceptable performance. Therefore, another set of Emμcode traces was created by hand. Examining the control flow of a 64-bit floating-point multiply from the SoftFloat library yields a control flow graph similar to Figure 6 under a few assumptions. The first assumption is that the rounding mode implemented is round to nearest even, so there is no need to check for which rounding mode to use. The second assumption is that there are no IEEE floating-point exceptions to implement, as the x86/amd64 architectures do not implement most of them, and the ones implemented are not enabled by default.

**Figure 6**: The control flow graph of a 64-bit floating-point multiply assuming round to nearest even. A and B are the two input 64-bit floating-point numbers. As can be seen, there is a lot of branching associated with special cases for NaN and denormalized results or sources. In general, these cases are rare, and the path that should be optimized for is the case where the inputs are neither NaN nor denormalized, and the resulting floating-point number is neither a NaN nor denormalized. By optimizing for the expected path, with a good branch predictor which would be able to determine that NaNs and denormals are not expected cases, it is possible to implement a much simpler solution for manual traces. With current compiler technology, it is infeasible to perform the dynamic analysis to figure out if NaNs or denormals are expected, so the optimization of the trace down this expected path is another optimization that is done. If the IEEE floating-point exceptions are needed, most of them are determined along the special case paths as well. The floating-point inexact exception is determined if rounding is needed at all and can be done fairly quickly along the main path if necessary.

### Evaluation
For this paper, PTLSim [23] is used as a cycle-accurate x86/amd64 simulator for the evaluation of Emμcode and direct execution of x86/amd64 instructions. PTLSim implements a superscalar architecture similar to the Intel Pentium 4 [9]. The changes are described in Section 2.2. SoftFloat2b, as implemented by Hauser [8], is a library of floating-point arithmetic using only integer instructions that follows the IEEE binary floating-point standard [1]. SoftFloat2b includes a software implementation of 32-bit, 64-bit, 80-bit, and 128-bit floating-point arithmetic operations. SoftFloat2b has defined its own set of functions to calculate integer multiplies, a special kind of right shift called a jamming right shift, denormalizing floating-point numbers, and rounding. It includes support for all four IEEE floating-point rounding modes, all of the IEEE floating-point exceptions, and proper NaN resolution. Since it includes everything necessary to implement any kind of IEEE floating-point arithmetic from a base set of 32-bit integers, it is a perfect match for the kind of microcode traces needed to implement Emμcode. SoftFloat2b was the basis for creating Emμcode traces. When creating the naive traces, SoftFloat2b was compiled under gcc-03 with inline functions turned on. When creating the hand-optimized traces, SoftFloat2b was used as a reference for what needed to be done. Emμcode provides the accuracy specified in the IEEE standard, and SoftFloat2b provides this as well. All three methods produced the same answer for the results presented.

### Trace Evaluation
A microbenchmark is used in order to evaluate specific Emμcode trace implementations. The microbenchmark consists of invoking a random number generator to create a random set of floating-point values. These values are designed to have a random sign, a random exponent between zero and 32 for single-precision floating-point numbers or zero and 64 for double-precision floating-point numbers, and a random mantissa. The window for the number of exponents is done to ensure that when executing a floating-point add, the result is not exactly the same as one of the two sources, as the full spectrum of exponents would result in many of these cases.

**Figure 7**: The average number of cycles each of the instructions takes to execute in Optimized Emμcode. For the two kinds of multiplies, there is a 6x slowdown, and for the two kinds of adds, there is a 10x slowdown. There are quite a few branches in the implementation of the floating-point add and subtract instructions, which degrade performance. There are two hard-to-predict branches in the add instructions: first, the sign of the two sources, which determines whether the instruction will add or subtract the two numbers, and second, which source has the larger exponent, which determines which mantissa to shift. If the branch predictor is correct, it will take only 33 cycles. If wrong, the next lowest is 46 cycles for mispredicting the larger exponent, then 57 cycles for mispredicting whether the signs agree, and lastly, 68 cycles for mispredicting both. Since there is such a penalty, more prediction support could drastically improve performance.

### Program Results
We gather results from SPEC benchmarks being simulated inside PTLSim modified to support Emμcode. Table 1 shows the dynamic number of arithmetic operations encountered. The results of the optimization of the traces can be seen in Figure 8. The naive versions exhibit a great deal more slowdown than the optimized versions. The results are arranged in the order of slowdown resulting from the optimized Emμcode traces. The notable exception is the mgrid trace, which is only about 1.5x worse than the optimized traces. The reason for this is because mgrid involves many operations where one or more of the source parameters is 0.0. Since the result of the operation is known once the zero has been detected, this is an early exit path for the naive traces, which will not perform any of the arithmetic operations until after this case is excluded.

**Table 1**: Percent of arithmetic operations performed during the 5 million instruction window.

| Benchmark | Single Precision Add (%) | Single Precision Sub (%) | Single Precision Mul (%) | Double Precision Add (%) | Double Precision Sub (%) | Double Precision Mul (%) |
|-----------|--------------------------|--------------------------|--------------------------|--------------------------|--------------------------|--------------------------|
| equake    | 0                        | 0                        | 0                        | 3.29                     | 3.65                     | 4.62                     |
| vpr       | 0.37                     | 0                        | 0.64                     | 0.35                     | 0                        | 0                        |
| ammp      | 0                        | 0                        | 0                        | 8.19                     | 7.28                     | 14.12                    |
| swim      | 0                        | 0                        | 0                        | 16.36                    | 3.64                     | 19.99                    |
| applu     | 0                        | 0                        | 0                        | 12.80                    | 2.85                     | 15.25                    |
| mesa      | 0.82                     | 0.42                     | 1.19                     | 0                        | 0                        | 0.15                     |
| mgrid     | 0                        | 0                        | 0                        | 19.25                    | 6.83                     | 2.83                     |
| mcf       | 0                        | 0                        | 0                        | 0                        | 0                        | 0                        |

**Figure 8**: Performance impact of trace optimizations.

### Conclusion
The results show that the optimized Emμcode traces significantly outperform the naive versions, with some benchmarks showing up to a 10x improvement. The mgrid benchmark is a notable exception, with only a 1.5x slowdown, due to the frequent occurrence of zero operands, which allow for early exits. Future work will focus on further reducing the performance gap by improving prediction support and optimizing the handling of special cases.

---

This document has been revised for clarity, coherence, and professional presentation.