title:Accelerating incast and multicast traffic delivery for data-intensive
applications using physical layer optics
author:Payman Samadi and
Varun Gupta and
Berk Birand and
Howard Wang and
Gil Zussman and
Keren Bergman
Accelerating Incast and Multicast Trafﬁc Delivery for
Data-intensive Applications using Physical Layer Optics
Payman Samadi, Varun Gupta, Berk Birand, Howard Wang, Gil Zussman, Keren Bergman
Department of Electrical Engineering, Columbia University, New York, NY, 10027
[ps2805@, vg2297@, berk@ee, howard@ee., gil@ee., bergman@ee.]columbia.edu∗
ABSTRACT
We present a control plane architecture to accelerate mul-
ticast and incast traﬃc delivery for data-intensive applica-
tions in cluster-computing interconnection networks. The
architecture is experimentally examined by enabling physi-
cal layer optical multicasting on-demand for the application
layer to achieve non-blocking performance.
Categories and Subject Descriptors: C.2.1 [Computer-
Communication Networks]: Network Architecture and De-
sign Network communications, Circuit-switching networks
Keywords: Hybrid Data Center Networks; Optics; Incast;
Multicast.
1.
INTRODUCTION
With the enormous increase in the generation and com-
plexity of Big Data, new opportunities and challenges re-
lated to its storage and processing on cluster-computing
platforms arise. The interconnection networks of such plat-
forms are generally over-subscribed, due to the switching
cost and cabling complexity. Designing these interconnec-
tion networks based on the traﬃc patterns of the applica-
tions running on clusters can improve the overall perfor-
mance of the system. In general, these applications use dis-
tributed ﬁle systems for storage and MapReduce type of
algorithms for data processing. In analyzing these applica-
tions and the interconnection network, we found that large
ﬂows with traﬃc patterns that include multiple nodes result
in heavy network congestion.
For example, in GoogleFileSystem, when storing and re-
questing data, blocks of 64MB are sent from one node to
multiple nodes (point-to-multipoint: Multicast) or one node
receives data from multiple nodes (many-to-one:
Incast).
Similar processes exist in HadoopFileSystem and in the shuf-
ﬂe stage of the MapReduce algorithm. Additionally, in other
data center applications such as virtual machine provision-
∗This work was supported in part by CIAN NSF ERC under
grant EEC-0812072.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage, and that copies bear this notice and the full ci-
tation on the ﬁrst page. Copyrights for third-party components of this work must be
honored. For all other uses, contact the owner/author(s). Copyright is held by the
author/owner(s).
SIGCOMM’14, August 17–22, 2014, Chicago, IL, USA.
ACM 978-1-4503-2836-4/14/08.
http://dx.doi.org/10.1145/2619239.2631436.
Figure 1: Optical accelerators in the hybrid arch.
ing or in-cluster software updating, there is a multicast of
300-800MB among 100-1000 nodes. Moreover, in parallel
database join operation, there is multicast of several 100MB.
Furthermore, in the Broadcast phase of Orchestra controlled
by Spark [1], 300MB is multicast on 100 iterations. In cur-
rent platforms, these patterns are managed either by se-
quence of unicast transmissions or software solutions such
as peer-to-peer methods. These methods are naturally inef-
ﬁcient since multiple copies of the same data are transmitted
on the network. For instance, the Orchestra system trans-
mits 12 copies of the same data.
Traditionally, electrical packet switching (EPS) networks
in Fat-tree or multi-tier architecture are used for the inter-
connection networks of cluster-computing platforms. Re-
cently, hybrid network architecture that combine EPS and
optical circuit switching have been proposed to oﬄoad larger
ﬂows to the optical network [2, 3]. The switching of the opti-
cal network is implemented by optical space switches (OSS).
The OSS, that performs point-to-point circuit switching can
also serve as a substrate to connect optical modules as accel-
erators. For example, passive optical splitters can be used
to provide a signiﬁcantly more eﬃcient multicasting [4]. In
this method, data is optically replicated in the physical layer
and transmitted by setting up a multicast tree between the
source and the destinations. This operation sends only one
copy of data and all the nodes receive the data simulta-
neously at the line-rate (non-blocking performance). Incast
traﬃc delivery can also be optically accelerated using passive
optical combiners and time-sharing the circuit between the
senders. In both cases, the OSS that has slow switching time
(25ms) is reconﬁgured once and this results in lower latency
compared to the point-to-point architectures. These accel-
erators can potentially oﬄoad heavy data from the over-
subscribed packet switching network and make them smaller
and cheaper. The passive nature of these accelerators, can
also reduce the networking power consumption. The main
challenge in utilizing optical accelerator is the integration of
ToRServersOptical Accelerators8 NodesControllerToROpenFlowEPSOpticalSplittersOSSEPSOSSFigure 2: Left: Optical multicasting by passive opti-
cal splitter, Right: Optical incast using passive op-
tical combiner and the time-sharing orchestration.
Figure 3: Network Orchestration Architecture.
optics with current network architectures due to the com-
plexities in conﬁguring optical devices and the circuit-based
nature of optics. Cross-layer architectures can potentially
overcome these complexities and provide optical functional-
ities more seamlessly to the application layer.
In this work, we present a control plane architecture that
accelerates multicast and incast data delivery using passive
optical modules. The architecture is experimentally exam-
ined on our 10G hybrid cluster-computing testbed with a
demonstration of physical layer optical multicasting. The
implemented control plane employs a messaging system, a
resource allocation algorithm, and APIs to control the opti-
cal and electrical switching network.
2.
IMPLEMENTATION AND RESULT
Fig. 1 shows the hybrid network architecture with optical
accelerators. Top-of-Rack (ToR) switches are aggregated by
an electrical packet switching network and an optical circuit
switching network. An OSS is used to provide switching in
the optical network and also as a substrate to connect op-
tical accelerators. Physical layer optical multicast can be
addressed using passive optical splitters that transparently
duplicate the optical signal. Fig. 2 (left) shows the hard-
ware conﬁguration to generate multiple copies of data at
the line rate. The incast traﬃc can be addressed by passive
optical combiner with an orchestration system running on
the packet switching network (Fig. 2 (right)). Optical split-
ters and combiners are data rate transparent, passive, and
commercially available in high port counts. Also the OSS
can be conﬁgured in a way to cascade multiple modules.
Fig. 3 demonstrates the network orchestration architecture
consisting of the Application, Control, and Data Plane lay-
ers. The Control Plane has a central network controller that
manages the network and includes a resource allocation algo-
rithm. The network controller communicates with the Data
Plane layer via southbound APIs including Floodlight for
the OpenFlow switches and OSS API, which is a python-
based API developed in-house. For the northbound API of
Table 1: Completion time of 50 multicast jobs (sec-
onds) Proposed optical Multicast and non-blocking
Internet Protocol Multicast
Group Size
7
4
Proposed
Non-blocking
226.179
218.517
187.348
191.680
the Control Plane, we have developed a pub/sub messaging
system using Redis. Byte size messages are transmitted on
the EPS network between the nodes and the central con-
trollers. We observed the latency by sending 200 messages
of 20 bytes among nodes and measured average latency of
300 µs.
For optical multicasting, the general algorithm of the net-
work controller is as followings: The application controller
submits the traﬃc matrix of the jobs that require optical
resources to the network controller via the Redis messaging
system. The traﬃc matrix includes the source, destinations,
and the size of the multicast job. The resource allocation
algorithm tries to schedule the optical splitters across the
multicast requests by maximizing the obtained throughput.
We model this problem as an Integer Program (IP) and solve
using the GLPK solver that implements a branch-and-bound
method. Using the solution of IP, the network controller gen-
erates the network conﬁguration for the next job. Then, the
ToR switches are conﬁgured using Floodlight, and the OSS
connections are applied using our API. For optical multicas-
ting, the network controller ﬁrst notiﬁes the receivers using
the Redis messaging system. Then, the senders are notiﬁed
to begin the transmission. Each receiver sends a message to
the controller, notifying the completion of the job. Once the
controller receives these messages from all the receivers for
a given job, it updates the traﬃc matrix and reruns the al-
gorithm. We evaluated the performance of our architecture
for multicasting in comparison with Internet Protocol (IP)
multicast over a non-blocking EPS network on our 8-node
hybrid cluster-computing test-bed (Fig. 1). Two sets of 50
multicast jobs (500MB-5GB) with the maximum group size
of 4 and 7 were generated. The former involves maximum
half of the nodes and one splitter. The latter can involve the
whole network and requires cascading of splitters. Table 1
shows the completion time in seconds. Our proposed opti-
cal multicast performs similar to the ideal fully non-blocking
network. This means that our architecture provides line rate
non-blocking multicast between the nodes that is not pos-
sible in practical interconnection networks. Additionally, it
can potentially decongest the over-subscribed EPS network
by oﬄoading the multi/in-cast traﬃc to the optical network.
3. REFERENCES
[1] M. Chowdhury, M. Zaharia, J. Ma, M. I. Jordan, and I. Stoica.
Managing data transfers in computer clusters with orchestra. In
ACM SIGCOMM, 2011.
[2] N. Farrington, G. Porter, S. Radhakrishnan, H. H. Bazzaz,
V. Subramanya, Y. Fainman, G. Papen, and A. Vahdat. Helios:
A hybrid electrical/optical switch architecture for modular data
centers. In ACM SIGCOMM, 2010.
[3] G. Wang, D. G. Andersen, M. Kaminsky, K. Papagiannaki, T. E.
Ng, M. Kozuch, and M. Ryan. c-through: Part-time optics in
data centers. In ACM SIGCOMM, 2010.
[4] H. Wang, Y. Xia, K. Bergman, T. E. Ng, S. Sahu, and
K. Sripanidkulchai. Rethinking the physical layer of data center
networks of the next decade: Using optics to enable eﬃcient
*-cast connectivity. SIGCOMM Com. Comm. Rev., July 2013.
432Senders1ReceiverOSS M SenderReceivers1234432Senders1ReceiverOSS I ReceiverNetwork OrchestrationOSS ReconfigurationData TransmissionSenderTime••ApplicationLayerApplication ControllerTrafϐic MatrixRedis Messaging SystemResourceAllocationControlPlaneLayerOSSAPIOpenFlow APIFloodlightDataPlaneLayerOSS Optical AcceleratorsOpenFlow SwitchNetwork ControllerJob#MultiInCastSenderReceiver   NodesSize1MN15122MN23IN124M5M6M7I8I9MSample Trafϐic MatrixN5N1N5N3N9N5N1N2N3N4N5N6N7N8N9N10N11N12N2N5N8N9N5N6N9N11N4N4N6N7N1N10N12N1N2N3N10N2N7N8N9N10N11N12N13N1(MB)234515464856316525664128912Nodes