title:Deceptive Deletions for Protecting Withdrawn Posts on Social Media
Platforms
author:Mohsen Minaei and
S. Chandra Mouli and
Mainack Mondal and
Bruno Ribeiro and
Aniket Kate
Deceptive Deletions for Protecting Withdrawn Posts
on Social Media Platforms
Mohsen Minaei∗¶†, S Chandra Mouli∗‡, Mainack Mondal§, Bruno Ribeiro‡, Aniket Kate‡
† Visa Research, Email: PI:EMAIL
§ IIT Kharagpur, Email: PI:EMAIL
‡ Purdue University, Email: {chandr, ribeirob, aniket}@purdue.edu
Abstract—Over-sharing poorly-worded thoughts and personal
information is prevalent on online social platforms. In many of
these cases, users regret posting such content. To retrospectively
rectify these errors in users’ sharing decisions, most platforms
offer (deletion) mechanisms to withdraw the content, and social
media users often utilize them. Ironically and perhaps unfor-
tunately, these deletions make users more susceptible to privacy
violations by malicious actors who speciﬁcally hunt post deletions
at large scale. The reason for such hunting is simple: deleting a
post acts as a powerful signal that the post might be damaging to
its owner. Today, multiple archival services are already scanning
social media for these deleted posts. Moreover, as we demonstrate
in this work, powerful machine learning models can detect
damaging deletions at scale.
Towards restraining such a global adversary against users’
right to be forgotten, we introduce Deceptive Deletion, a de-
coy mechanism that minimizes the adversarial advantage. Our
mechanism injects decoy deletions, hence creating a two-player
minmax game between an adversary that seeks to classify dam-
aging content among the deleted posts and a challenger that
employs decoy deletions to masquerade real damaging deletions.
We formalize the Deceptive Game between the two players,
determine conditions under which either the adversary or the
challenger provably wins the game, and discuss the scenarios
in-between these two extremes. We apply the Deceptive Deletion
mechanism to a real-world task on Twitter: hiding damaging
tweet deletions. We show that a powerful global adversary can
be beaten by a powerful challenger, raising the bar signiﬁcantly
and giving a glimmer of hope in the ability to be really forgotten
on social platforms.
I.
INTRODUCTION
Every day, millions of users share billions of (often per-
sonal) posts on online social media platforms like Facebook
and Twitter. This information is routinely archived and an-
alyzed by multiple third parties ranging from individuals to
state-level actors [22], [29], [48], [49], [71], [74], [75], [84].
Although the majority of these social media posts are benign,
users also routinely post regrettable content on social media
[24], [80], [94] that they later wish to retract. Subsequently,
most social platforms provide user-initiated deletion mecha-
nisms that allow users to rectify their sharing decisions and
delete past posts. Not surprisingly, users take advantage of
these deletion mechanisms enthusiastically—Mondal et al. [65]
∗ Both authors contributed equally and are considered co-ﬁrst authors.
¶ This work was done while this author was at Purdue University.
Network  and  Distributed  Systems  Security  (NDSS)  Symposium  2021 
21-25  February  2021, Virtual
ISBN  1-891562-66-5
https://dx.doi.org/10.14722/ndss.2021.23139
www.ndss-symposium.org
showed that nearly one-third of six-year-old Twitter-posts were
deleted. In another work, Tinati et al. [83] showed that this
number is much higher in Instagram, where almost half of the
pictures posted within a six month period had been removed.
Ironically, current user-initiated deletion mechanisms may
have an unintended effect: third-party archival services can
identify deleted posts and infer that deleted posts might contain
damaging content from the post creator’s point of view (i.e.,
having an adverse effect on the personal/professional life of the
content creator). In other words, deletion might inadvertently
make it easier to identify damaging content. Indeed, today
it is possible to detect deletions at scale: Twitter, for one,
advertises user deletions in their streaming API1 via deletion
notiﬁcations [7], [8] so that third-party developers can remove
these posts from their database. Similarly, Pushshift [16],
[25] is an archival system for all
the contents on Reddit
and Removeddit [18] uses this archive to publicize all the
deleted posts and comments on Reddit. A malicious data-
collector can simply leverage these notiﬁcations to ﬂag deleted
posts as possibly damaging and further use them against the
users [5], [6], [91]. Importantly, the hand-picked politicians
and celebrities are not the only parties at the receiving end
of these attacks. We ﬁnd that the malicious data-collector can
develop learning models to automate the process and perform
an non-targeted (or global) attack at a large-scale; e.g., Fallait
Pas Supprimer [13] (i.e., “Should Not Delete” in English) is a
Twitter account that collects and publishes the deleted tweets
of not only the French politicians and celebrities but also
noncelebrity French users with less than a thousand followers.
Asking the users not to post regrettable content on social
platforms in the ﬁrst place may seem like a good ﬁrst step.
However, users cannot accurately predict what content would
be damaging to them in the future (e.g., after a breakup or
before applying to a job). Zhou et al. [94] and Wang et al. [87]
propose multiple types of classiﬁers (Naive Bayes, SVM, De-
cision Trees, and Neural Networks) to detect regrettable posts
using users’ history and to proactively advise users even before
the publication of posts. However, this proactive approach
cannot prevent users from publishing future-regrettable posts.
It is inevitable to focus on reactive mechanisms to assist users
with protecting their post deletions.
Recently Minaei et al. [62] proposed an intermittent with-
drawal mechanism to tackle this challenge of hiding user-
initiated deletions. They offer a deniability guarantee for user-
initiated deletions in the form of an availability-privacy trade-
off and ensure that when a post is deleted, the adversary
1Twitter provides a random sample of the publicly posted Twitter data in
real time to the third parties via streaming API.
cannot be immediately certain if it was actually deleted or
temporarily made unavailable by the platform. Their trade-
off could be useful for future social and archival platforms;
however, in current commercial social media platforms like
Twitter, sacriﬁcing even a small fraction of availability for all
the posts is undesirable.
To this end, our research question is straightforward, yet
highly relevant—can we enhance the privacy of the deleted and
possibly damaging posts at scale without excessively affecting
the functionality of the platform?
Contributions. We make the following contributions.
First, we demonstrate the impact of deletion detection
attacks by performing a proof-of-concept attack on real-world
social media posts to identify damaging content. Speciﬁcally,
we use a crowdsourced labeled corpus of deleted posts from
Twitter to train an adversary (a classiﬁer). We demonstrate that
our adversary is capable of detecting damaging posts with high
probability (an increase of 27 percentage points in its F-score).
Thus, it is feasible for the adversary to use automated methods
for detecting damaging posts on a large scale. In fact, we
expect systems such as Fallait Pas Supprimer [13] to employ
analogous learning techniques soon to improve their detection.
Second, to overcome the problem of detecting damaging
deletions, we introduce a novel deletion mechanism, Deceptive
Deletions, that raises the bar for the adversary in identifying
damaging content. Given a set of damaging posts (i.e., posts
that adversary can leverage to blackmail the user) that users
want to delete, the Deceptive Deletion system (also known
as a challenger) carefully selects k additional posts for each
damaging post and deletes them along with the damaging
posts. The system-selected posts, henceforth called the decoy
posts, are taken from a pool of posts (i.e., non-damaging non-
deleted) provided by volunteers. The deletions of the decoy
posts will confuse the adversary in distinguishing damaging
posts from the (non-damaging) decoy posts. Intuitively, Decep-
tive Deletion is more effective if the selected decoy posts are
similar to the damaging posts. These two opposite goals create
a minmax game between the adversary and the challenger that
we further analyze.
Third, we introduce the Deceptive Learning Game, which
formally describes the minmax game between the adversary
and the challenger. We start by considering a static adversary
that tunes the parameters of its system (e.g., classiﬁer for
determining the damaging posts) up until a certain point in
time. However, powerful adversaries are adaptive and continu-
ally tune their models as they obtain more deletions including
the decoy deletions made by the challenger. Therefore, in
the second phase, we consider an adaptive adversary and
describe the optimization problem of the adaptive adversary
and challenger as a minmax game.2
We identify conditions under which either only the adaptive
adversary or only the challenger provably wins the minmax
game and discuss the scenarios in-between these two extremes.
To the best of our knowledge, this is the ﬁrst attempt to
develop a computational model for quantitative assessment
of the damaging deletions in the presence of both static and
adaptive adversaries.
2See [51] for another example of a minmax game in adversarial learning.
Finally, we empirically demonstrate that with access to
a set of non-damaging volunteered posts, we can leverage
Deceptive Deletions to hide damaging deletions against both
static and adaptive adversary effectively. We use real-world
Twitter data to demonstrate the effectiveness of the challenger.
Speciﬁcally, we show that even when we consider only two
decoy posts per damaging deletion, the adversarial perfor-
mance (F-score) drops to 42% from 75% in the absence of
any privacy-preserving deletion mechanism.
II. BACKGROUND AND RELATED WORK
A. Exisiting Content Deletion Mechanisms to Provide Privacy
Today, most archival and social media websites (e.g., Twitter,
Facebook) enable users to delete their content. Recent studies
[20], [60], [65] show that a signiﬁcant number of users deleted
content—35% of Twitter posts are deleted within six years of
posting them. This user-initiated deletion is also related to the
“Right to be Forgotten” [88], [91]. However, this user-initiated
content deletion suffered from the Streisand effect – attempting
to hide some information has the unintended consequence of
gaining more attention [91]. Consequently, there is a need to
provide deletion privacy to users.
In addition to user-initiated deletions, there exist some
premeditated withdrawal mechanisms where all historical con-
tent is eventually deleted automatically to provide deletion
privacy. These mechanisms can be broadly classiﬁed into
two categories. First, in age-based withdrawal, platforms like
Snapchat [1] and Dust [4] and systems like Vanish [42], [43]
and EphPub [28] automatically withdraw a piece of content
after a preset time. Second, to make premeditated withdrawal
more usable, Mondal et al. [65] proposed inactivity-based with-
drawal, where posts will be withdrawn only if they become
inactive, i.e., there is no interaction with the post for a speciﬁed
time period (e.g., no more views by other users).
However, even the premeditated withdrawals are not free
from problems of their own. First, all the posts will eventually
get deleted, removing all archival history from the platform.
Second, if posts are deleted before the preset time or in-spite of
high interaction, the adversary can be certain that the deletion
was user-intended, violating deletion privacy.
Minaei et al. [61], [62] presented a new intermittent withdrawal
mechanism for all non-deleted posts, which provides a trade-
off between availability and deletion privacy. In a nutshell,
their system ensures that if an adversary found that a post is
not available, then the adversary cannot be certain if the post is
user-deleted or simply taken down by the platform temporarily.
Although this mechanism is useful for large internet archives,
in platforms such as Facebook and Twitter, where content
availability is crucial to the users and platform, a privacy-
availability trade-off might not be feasible. Furthermore, the
intermittent withdrawal mechanism does not consider the ad-
versary’s background knowledge about other deleted posts.
Our work aims to bridge this gap and provide a novel learning-
based mechanism which considers an adaptive adversary who
aims to uncover tweet deletion.
Tianti et al. [83] offer intuitions for predicting posts dele-
tions on Instagram with the goal of managing the storage
of posts on the servers: Once a post is archived, it becomes
2
computationally expensive to erase it; thus, predicting deletions
can help in reducing the overheads of being compliant with
the “right to be forgotten” regulations. These predictions in
the non-adversarial setting, however, does not apply to our
minmax game between the adversary and the challenger.
Garg et al. [41] formalize the right to be forgotten using
platforms as a cryptographic game. While interesting, their
deﬁnitions and suggested tools such as history-independent
data structures are not applicable to our setting where the
adversary has continuous access to the collected data.
B. Obfuscation using Noise Injection
Our mechanism is not without precedent, and it is inspired
by earlier work of obfuscation by noise injection. There has
been a line of work in the area of (non-cryptographic) private
information retrieval [38], [47], [66], [69] that obfuscates the
users’ interest using dummy queries as noise to avoid user
proﬁling. Howe et al. proposed TrackMeNot [9], [47], which
issues randomized search queries to prevent the search engines
in building any practical proﬁle of the users based on their
actual queries. Similar works [38], [66], generate k − 1 other
queries (dummy ones) for each user query and submit all k
queries at the same time. We note that all of the systems men-
tioned so far consider hiding each query separately. However,
a determined adversary may be able to ﬁnd a user’s interests
by observing a sequence of such obfuscated queries. Multiple
works have investigated such weaknesses [23], [69], [70].
Some relatively new techniques further try to overcome
these shortcomings by smartly generating the k − 1 queries.
For example, Petit et al. proposed PEAS [72], where they
provide a combination of unlinkability and indistinguishability.
However, apart from introducing an overhead for encrypting
the user queries, their method also requires two proxy servers
that are non-colluding, hence weakening the adversarial model.
K-subscription [67] is yet another work that proposes an
obfuscation based approach that enables the user to follow
privacy-sensitive channels in Twitter by requiring the users
to follow k − 1 other channels to hide the user interests
from the microblogging service. However, the K-subscription
has a negative social impact for the user as the user’s social
connections will see the user following these dummy channels.
These shortcomings, both social and technical, motivated our
particular design decision for Deceptive Deletions.
C. Adversarial Machine Learning
Traditional adversarial learning settings [32] involve two
players: a classiﬁer and an attacker. The classiﬁer seeks to
label the inputs whereas the attacker tries to modify the inputs
such that
the classiﬁer will misclassify them. Adversarial
machine learning has also been used as a defense with the roles
reversed where the defender attacks the adversary’s classiﬁer.
For example, in [50], the adversary tries to extract users private
attributes from their public data while the defender modiﬁes
the public data of the users in order to fool the adversarys
classiﬁer. Our setting is different in that we are not allowed to
modify the examples. Instead, the challenger wishes to attack
the adversary’s classiﬁer by injecting hard-to-classify examples
into the adversary’s train/test datasets (i.e., the deletion set). A
key constraint for the challenger is that it has to select the
examples from a preexisting set of volunteered posts. This
is because the challenger can only delete existing posts, and
cannot generate fake posts.
As we detail
in the subsequent sections,
the adaptive
adversary trains on these injected examples as well. With
a faint relation to our work, data poisoning attacks [58],
[81] focus primarily on injecting poisoned samples into a