title:FlowDist: Multi-Staged Refinement-Based Dynamic Information Flow Analysis
for Distributed Software Systems
author:Xiaoqin Fu and
Haipeng Cai
FlowDist: Multi-Staged Refinement-Based 
Dynamic Information Flow Analysis for 
Distributed Software Systems
Xiaoqin Fu and Haipeng Cai, Washington State University, Pullman, WA
https://www.usenix.org/conference/usenixsecurity21/presentation/fu-xiaoqin
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.FLOWDIST: Multi-Staged Reﬁnement-Based Dynamic Information Flow Analysis
for Distributed Software Systems
Xiaoqin Fu
Haipeng Cai R
Washington State University, Pullman, WA
Washington State University, Pullman, WA
PI:EMAIL
PI:EMAIL
Abstract
(DIFA)
Dynamic information ﬂow analysis
supports
various security applications such as malware analysis and
vulnerability discovery. Yet traditional DIFA approaches have
limited utility for distributed software due to applicability,
portability, and scalability barriers. We present FLOWDIST, a
DIFA for common distributed software that overcomes these
challenges. FLOWDIST works at purely application level to
avoid platform customizations hence achieve high portability.
It infers implicit, interprocess dependencies from global
partially ordered execution events to address applicability to
distributed software. Most of all, it introduces a multi-staged
reﬁnement-based scheme for application-level DIFA, where
an otherwise expensive data ﬂow analysis is reduced by
method-level results from a cheap pre-analysis, to achieve
high scalability while remaining effective. Our evaluation of
FLOWDIST on 12 real-world distributed systems against two
peer tools revealed its superior effectiveness with practical
efﬁciency and scalability. It has found 18 known and 24 new
vulnerabilities, with 17 conﬁrmed and 2 ﬁxed. We also present
and evaluate two alternative designs of FLOWDIST for both
design justiﬁcation and diverse subject accommodations.
1 Introduction
Tracking/checking dynamic information ﬂow underlies
various security applications (e.g., [72, 93, 95, 109, 115]).
It addresses a general source-sink problem for a program
execution, in which a source is where conﬁdential or untrusted
(i.e., sensitive) information is produced and ﬂows into the
program, while a sink consumes the information and makes
it ﬂow out of the program execution [40]. Due to its focused
reasoning about actual executions, this approach has precision
merits over statically inferring information ﬂow.
One technique realizing the approach is to compute the
chains of dynamic control/data dependencies hence infer
full information ﬂow paths between given sources and sinks
during the execution (e.g., [93–95, 114]). We refer to this
technique as dynamic information ﬂow analysis (DIFA).
An alternative technique is to apply a tag to (i.e., taint)
the data entering the program via the sources, propagate the
taint tag during the execution, and check the data at the sinks
against the presence of the tag (e.g., [42, 45, 59, 60, 64, 76, 79,
81, 82, 96, 101, 111, 113, 116, 119, 120, 123–125]). We refer to
this technique as dynamic taint analysis (DTA). Unlike DIFA,
DTA does not compute full information ﬂow paths. DIFA
thus provides better support in usage scenarios that require
more detailed ﬂow information (e.g., diagnosing data leaks
by inspecting the full ﬂow paths).
Yet current DIFAs are hardly applicable to multi-process
programs, such as distributed systems (e.g., Voldemort [29],
a distributed key-value store). The reason is that they rely
on explicit dependencies (via references and/or invocations)
among code entities, dismissing implicit dependencies across
processes [67]. On the other hand, distributed systems widely
serve critical application domains (e.g., banking, medical,
social media), thus their security is of paramount importance.
Only a few existing DIFA/DTA tools (e.g., [79, 116])
overcame the applicability challenge by working at system
level with platform customizations. However, keeping the
customizations up with diverse and rapidly evolving platforms
would be time-consuming and even infeasible, which
constitutes a portability challenge. A purely application-level
analysis would eliminate the need for platform customizations.
Yet such an analysis faces a scalability challenge for two
reasons. First, application-level dynamic analysis is known to
generally incur substantial overheads. Second, working at a
ﬁne granularity for desirable precision, as well as the typically
large size and great complexity of distributed software, adds
further to the analysis costs.
In
this paper, we present FLOWDIST, a purely
application-level DIFA that addresses all the three challenges
to work practically with common distributed software.
The practicality goal here subsumes two speciﬁc aims:
scalability—FLOWDIST should be scalable to real-world
distributed systems, and effectiveness—it should be effective
for discovering known and unknown (new) vulnerabilities
in such systems at a reasonable level of accuracy. Our key
insights for fulﬁlling these aims are as follows:
• Since a ﬁne-grained information ﬂow path is subsumed by
a corresponding coarser-grained path, a cheap pre-analysis
computing the latter can narrow down the scope of the
former which may be quite expensive. This way, the overall
USENIX Association
30th USENIX Security Symposium    2093
analysis cost can be largely reduced without effectiveness
loss, fulﬁlling the scalability aim.
• As the collection and use of various forms of program data
come with different cost and effectiveness contributions
to the cost-effectiveness of the entire analysis, carefully
combining these data can help attain a practical level of
accuracy while maintaining efﬁciency, which fulﬁlls the
effectiveness aim without sacriﬁcing scalability.
Following these insights, FLOWDIST introduces a
multi-staged reﬁnement-based scheme for DIFA to attain
high scalability, where a pre-analysis computes method-level
information ﬂow paths approximately but rapidly, followed
by a ﬁne-grained analysis that computes statement-level
ﬂow paths precisely as guided by the method-level results.
Then, FLOWDIST adopts a hybrid scheme using various
forms of data (i.e., method-level execution events, static
dependencies and dynamic coverage both at statement level)
to balance its cost and effectiveness. FLOWDIST addresses
the portability and applicability challenges by working at
purely application level while inferring implicit, interprocess
dependencies from happens-before relations among executed
methods across processes by partially ordering key execution
(entry, returned-into) events of those methods. The slight
compromise of precision (due to the method-level granularity)
of the interprocess part of the DIFA is compensated by precise,
ultimately statement-level analysis results within each process
(i.e., intraprocess part of the DIFA), resulting in a practical
level of accuracy overall.
To further understand the methodology for scalable,
application-level DIFA, we have also developed two
alternative designs of FLOWDIST: FLOWDISTsim and
FLOWDISTmul. The ﬁrst performs more static analysis
while the second performs more dynamic analysis, both
further reducing the overall analysis overhead under certain
conditions. With these variants, FLOWDIST accommodates
diverse user needs in providing the best cost-effectiveness
tradeoffs for different kinds of distributed systems.
We implemented FLOWDIST and the two alternative
designs for Java and applied them to 12 distributed systems
of diverse scales, architectures, and domains, all of which are
real-world systems. For various operational scenarios of these
systems, FLOWDIST exhibited highly promising analysis
accuracy and efﬁciency. For the given lists of sources/sinks
(default ones in our study), FLOWDIST computes information
ﬂow paths between all possible source-sink pairs. For each
subject execution, we sampled 20 ﬂow paths when there were
more; otherwise, we sampled all paths reported. FLOWDIST
attained perfect precision and recall per our manual validation.
On average, FLOWDIST took 19 minutes for its one-off
analyses for all possible information ﬂow path queries
(i.e., source-sink pairs) with respect to a given source/sink
conﬁguration and 13 seconds for each query, while incurring
less than 1x run-time slowdown and negligible storage costs.
We further validated the practical usefulness of FLOWDIST by
using it to identify real vulnerability cases reported previously
in public vulnerability databases (e.g, CVEs [31]). Out of
24 cases studied, FLOWDIST found 18, with the other 6
being missed because the respective vulnerabilities were not
covered by the executions considered. It also revealed 24 new
vulnerability cases in several of the studied industry-scale
systems, of which 17 have been conﬁrmed and 2 ﬁxed
already by the developers. In contrast to the only two
state-of-the-art peer tools for Java that we could compare
with, (one dynamic [47] and one static [75]), FLOWDIST
exhibited superior effectiveness with practical efﬁciency and
high scalability. None of the baselines found any of the
existing and new vulnerabilities that FLOWDIST discovered.
general,
reﬁnement-based methodology for cost-effective and scalable
DIFA at purely application level, which can enable a number
of applications beyond the scope of information ﬂow security
(e.g., system understanding and performance diagnosis)
and the domain of distributed software (e.g., single-process
concurrent programs). Our contributions and novelties are:
Through FLOWDIST, we
demonstrate
a
• The ﬁrst purely application-level DIFA for common
distributed software, FLOWDIST, which features a hybrid
ﬁne-grained data ﬂow analysis
instantiates a
multi-staged reﬁnement-based methodology for DIFA to
holistically overcome applicability, portability, scalability,
and cost-effectiveness barriers with peer approaches (§3).
• Alternative designs of FLOWDIST that further explore
the design methodology for DIFA to best accommodate
distributed software of diverse scale/complexity (§4).
that
• An open-source implementation of FLOWDIST for Java
that works with distributed software systems of various
architectures and application domains (§5).
• Extensive empirical evaluations of FLOWDIST that show
its practical effectiveness and scalability, as well as
superior capabilities in vulnerability discovery, over two
state-of-the-art approaches (§6).
The FLOWDIST artifact is available here [65], including the
source code, experimental scripts, (installation, conﬁguration,
and usage) documentation, and relevant data sets.
2 Background and Motivation
We introduce distributed software systems and deﬁne the
problem of DIFA for these systems as opposed to DTA. A
real-world example is then given to motivate our work.
Distributed software systems. Driven by increasing
demands for computational performance and scalability,
increasingly more real-world software systems today are
distributed by design [61]. We address systems for
general-purpose distributed computing as deﬁned in [61],
noted as common distributed systems, as opposed to those of
special types (e.g., RMI-based [112] or event-based [98]).
In common distributed software, components located at
2094    30th USENIX Security Symposium
USENIX Association
networked computers communicate and coordinate their
actions only by passing messages, while running concurrently
in multiple (distributed) processes without a global clock.
as
(processes), noted
Due to this decoupling, dependencies among distributed
inter-component
components
(interprocess) dependencies, are implicit [53]. Sensitive
information can ﬂow across decoupled components/processes
via these implicit dependencies, leading to, among other
issues, information ﬂow security vulnerabilities that are
missed by analyses based on explicit dependencies (as are
most current techniques). Next, we use a real-world example
to illustrate the need for analyzing such information ﬂows.
DIFA versus DTA. These are two related techniques for
tracking/checking dynamic information ﬂows. While they
have been treated equivalently and named exchangeably [60],
we differentiate them (1) by their inner workings as mentioned
earlier—DIFA works by computing dynamic dependencies
while DTA works via data tainting and taint propagation, and
(2) by their results—DIFA provides full information ﬂow
paths while DTA just tells which data is tainted.
On the other hand, both DIFA and DTA solve a source-sink
problem, concerning information ﬂow between given sources
and sinks. For DIFA, we deﬁne a source as a function (call)
producing information of interest (e.g., sensitive data) that
ﬂows into the program, and a sink as a function (call) that
consumes the information and makes it ﬂow out of the
program. We refer to an exercised program path from a source
to a sink as a (dynamic) sensitive information ﬂow path. For
multi-process programs (e.g,. distributed software systems),
we divide an interprocess information ﬂow path into three
segments: source information ﬂow path segment (SOFPS)
and sink information ﬂow path segment (SIFPS), consisting
of only statements within the process that executes the source
and the sink, respectively, and remote information ﬂow path
segment (REFPS) consisting of all other statements.
Motivating example. Figure 1 shows an excerpt from
Apache ZooKeeper
popular distributed
coordination service, where the sensitive ﬂow is responsible
for CVE-2018-8012 [32]. It revealed that when an Apache
ZooKeeper server starts and attempts to join a quorum, there
is no enforced authentication. As shown, the data-leaking
ﬂow exercised in the relevant execution crossed three
processes: The sensitive data (a security key) was read into
incomingBuff in class ClientCnxnSocketNIO of a Client process
(at the Source), passed through class InstanceContainer of a
Container process, and reached class BinaryOutputArchive of a
Server process where the data leaked out of the system (at
the Sink). This leakage caused an authentication failure when
an endpoint attempted to join a quorum, which thus might
propagate fake changes to the leader node of ZooKeeper.
(v3.4.11), a
Suppose this case, along with the system execution that
revealed it, is reported to a developer for diagnosis, to whom
no platform customization is feasible. Purely application-level
Figure 1: A case of sensitive information ﬂow (marked by arrowed
lines) in ZooKeeper across its three components (processes).
DIFA/DTA tools exist (e.g., [35]), which only track ﬂows
within the same processes (plus most of such tools only work
for C/C++ programs). There are analyses that resolve data
ﬂows across decoupled components (e.g., [73, 118]), yet they
do not work for common distributed software; and they are
static hence would lead to excessive imprecision. We will
demonstrate how FLOWDIST addresses these challenges.
3 Approach
This section elaborates FLOWDIST, starting with an overview,
followed by design details.
Figure 2: FLOWDIST overview:
the cheap pre-analysis
computes coarse ﬂow paths to reduce the expensive
ﬁne-grained analysis which reﬁnes the coarse result.
3.1 Overview
Figure 2 depicts the overall workﬂow and two phases of
FLOWDIST. The high-level idea is to achieve the scalability
and effectiveness aims via a multi-staged reﬁnement-based
DIFA design, as guided by our key insights as outlined earlier.
FLOWDIST takes three inputs: the distributed program
D under analysis, the run-time input I that drives the
USENIX Association
30th USENIX Security Symposium    2095
// Message-receiving inside39 public class ClientCnxnSocketNIO extends ClientCnxnSocket { . . . // Executed in a Client process 68                int rc = sock.read(incomingBuffer);  // Source103             Packet p = findSendablePacket(outgoingQueue,...  107             sock.write(p.bb);63                SocketChannel sock = (SocketChannel) sockKey.channel();  61            public void doIO(java.utils.list, ) { . . .. . . }}247 public class InstanceContainer implements Watcher,  . . . { // Executed in a Container process 392           zk = new ZooKeeper(zkHostPort, sessTimeout, this);393           mknod(assignmentsNode, CreateMode.PERSISTENT);397           zk.getChildren(assignmentsNode, true, this, null);  . . . }. . . }391      public void run() throws IOException,  . . . {432 public class BinaryOutputArchive implements OutputArchive { // Executed in a Server process 442      public BinaryOutputArchive(DataOutput out) {443           this.out = out;454      public void writeInt(int i, String tag) throws IOException {...455         out.writeInt(i);  // Sink      . . . }Blue line: source    information flow path    segment (SOFPS)Green line: remote    information flow path    segment (REFPS)Red line: sink    information flow path    segment (SIFPS)Solid line:     intraprocess  flowDashed line:      interprocess flowSolid line: intraprocess flowDashed line: interprocess flow. . . }. . . }437      public getArchive(java.io.OutputStream strm) {438           return new BinaryOutputArchive(new DataOutputStream(strm));  }    Phase 1: Pre-analysisApproximating information flow pathsUser InputsPhase 2: RefinementRefining Information flow pathsStatement-level Information flow pathsFLOWDIST OutputMethod-level information flow pathsBranch coverageMethod event tracesDistributed program DUser configuration CProgram Input I  message-passing events—sending/receiving a message, and
(3) branch coverage events—a branch being exercised.
To this end, we instrument D to probe for these events,
with respect to the given or default message-passing APIs.
Since only the methods on a static control ﬂow path between a
source-sink pair are likely to occur on a dynamic information
ﬂow path between the same pair, we only need to probe for
the events of those methods, referred to as relevant methods.
Accordingly, only branches in a relevant method (i.e., relevant
branches) need to be probed. Thus, we start by constructing
the interprocedural control ﬂow graph (ICFG) of each
distributed component in D and treat each message-sending
and message-receiving API callsite in the component as an
additional sink and source, respectively. Then, any method
through which a sink is control-ﬂow reachable from a source
on the ICFG is identiﬁed as a relevant method.
Tracing (1.2). In this step, the instrumented program D(cid:48)
is executed against the program input I, during which the
three kinds of events are traced (at instance level but only
for relevant methods and branches). Given the absence of a
global timing mechanism in common distributed software,
FLOWDIST uses the Lamport time-stamping (LTS) [104]
algorithm to derive the global partial ordering of the two kinds
of method execution events, to derive the happens-before
relations required for interprocess dependence inference. With
LTS, each process maintains a logic clock locally, which
may be updated by, or used to update, the local clocks
of other (communicating) processes. The synchronization
is realized by attaching the current values of local logic