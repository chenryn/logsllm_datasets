  * `transformers` version: 3.0.2
  * Platform: Linux
  * Python version: 3.6
  * PyTorch version (GPU?): 1.4 (GPU)
  * Using GPU in script?: Yes
  * Using distributed or parallel set-up in script?: No
### Who can help
@sshleifer
## Information
Model I am using (Bert, XLNet ...): BART
The problem arises when using:
  * the official example scripts: (give details below)
  * my own modified scripts: (give details below)
The tasks I am working on is:
  * an official GLUE/SQUaD task: (give the name)
  * my own task or dataset: (give details below)
## To reproduce
Steps to reproduce the behavior:
  1. Running the example script in https://github.com/huggingface/transformers/tree/master/examples/seq2seq (finetune_bart_tiny.sh), I'm getting this warning in the beginning of training. However, the training process is continuing after that.
Warning:
    finetune.py:245: UserWarning: All learning rates are 0
      warnings.warn("All learning rates are 0")
    Epoch 1:   0%|      
    /home/sajad/anaconda3/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
      warnings.warn("To get the last learning rate computed by the scheduler, "
## Expected behavior
While the training seemingly goes well, I'm wondering if this warning would
cause problems, leading to deteriorate model's final performance? As a add-on,
I've also incorporated the gradient checkpointing to some computational blocks
of BART (modifying `modelling_bart.py` script a bit). But even w/o
incorporating this module, I'm still getting this warning message? Any
thoughts of how to solve it?