p(x)
p(X)
· F (x)
6
where p(X) denotes(cid:80)
x∈X p(x).
is infeasible to compute err(1)D (F )
Unless X is small,
exactly. However, given access to n independent draws from
D, {x1, . . . , xn}, we can estimate err(1)D (F ) by
Deﬁnition 2 (Empirical Type I Error over D):
it
(cid:102)err(1)D (F, n) =
n(cid:88)
i=1
1
n
F (xi)
(cid:102)err(1)D (F, n) =
n is the empirical frequency of xi from our sample, and thus
1
replaces p(x) in the expectation. Observe that, for a random
variable X(cid:48) denoting the conditional distribution of D given
the input is benign (i.e., lies in X) F (X(cid:48)) is just a Bernoulli
random variable with parameter err(1)D (F ) so, we can rewrite
n(cid:88)
where each yi ∼ B(err(1)D (F )).
When our data is generated from D, (cid:102)err(1)D (F, n) →
err(1)D (F, n) as n → ∞. Accordingly, we can obtain a better
estimate of F ’s false positive rate by selecting a larger set of
test inputs. The exact size of the sample required is given in
the following:
n(cid:88)
F (xi) =
1
n
1
n
Theorem 3 (Hoeffding’s bound [27]): For
(0, 1], if n ≥ ln(2/δ)
from D, then with probability 1 − δ over x1, . . . , xn,
∈
and x1, . . . , xn are drawn independently
any , δ
22
i=1
i=1
yi
(cid:107)(cid:102)err(1)D (F, n) − err(1)D (F )(cid:107) ≤ 
it guarantees
If we set δ = .05 and  = .01, Theorem 3 tells us how to
set n so that we can make the following statement with 95%
conﬁdence: “F ’s true false positive rate on data generated from
D is within an additive 1% of our estimated false positive rate”.
that, with
high probability, (cid:102)err(1)D (F, n) > err(1)D (F ) − δ and that
(cid:102)err(1)D (F, n)  
efﬁcient enough to enable Fortuna to obtain tight false positive
bounds based on the theorems from Section III.
Again, while the error bounds for sampling are well known,
the difﬁculty in making use of them is that we require test data
that has been drawn from a distribution that is representative of
the real distribution over benign inputs one would encounter.
As obtaining such data may be expensive, many authors have
had to settle for test data that was selected in some other
manner. As noted by Chandola et al. [16], the use of such
synthetic data may undermine the quality of the evaluation.
As discussed in Section II, we avoid this issue in the case of
anomaly detectors that process data from the web. PageRank
provides a distribution over web data that is both easy to
sample from and representative of the inputs.
A. Formal Properties of PageRank
Building on the overview given in Section II-A, we formal-
ize the random surfer process described by using a “random
walk” matrix, A. (We never construct this matrix explicitly.
It is only used to analyze the process). If n is the number of
pages on the web, A is an n × n real-valued matrix. Every
entry in A lies in [0, 1] and speciﬁcally, Ai,j is the probability
of moving from page j to page i. For pages that j links to,
this probability should equal:
α
degree(j)
+
1 − α
n
If j has links, but the page i is not directly linked to from j,
Ai,j simply equals:
1 − α
n
and if degree(j) is 0, Ai,j should simply equal 1
n for all
i. If we sum up entries in any row of A, the total should
be 1. Each row gives a probability distribution on webpages,
conditioned on our current position. That is, A is a “row
stochastic” matrix.
Actually, the process described above describes one of
many possible PageRank distributions. In practice, it is neither
feasible nor (it turns out) desirable to use precisely this random
surfer process. First, the web is constantly growing and may
contain pages or subsets of pages not linked to by any other
page. And second, it is believed that Google combats attempts
at manipulating the PageRank by explicitly modifying the
distribution (cf. the case of Google vs. SearchKing [32, p.54]).
Thus, in practice one never performs a true random jump to a
uniformly selected page from the web. Instead, one typically
selects a ﬁxed “teleportation” vector v that approximates the
uniform distribution over all (legitimate) webpages. At each
random jump of the surfer process, we move to page i with
probability vi. In such a case, v contains uniform probabilities
over a certain large proportion of webpages known to exist,
with zeros everywhere else. We will use such a teleportation
distribution in our sampling implementation (see Section V).
As long as v is rigorously speciﬁed with a reasonably large
support,
the resulting distribution over web pages is still
representative. Furthermore, we will use a distribution over
“seed” webpages that is public and simple to access, allowing
for consistency with papers that seek to employ a sampling
strategy similar to ours.
It will be helpful to separate v’s contribution to A from
the effect of actual links. Thus, deﬁne P to be another n × n
matrix with entries in [0, 1]. Pi,j = 0 if j does not link to i
and 1/degree(j) is j does link to i.
With v and P chosen, we can deﬁne our random surfer
model as a random walk process. Suppose e is the all ones
vector. Then we can write:
A = αP + (1 − α)ev(cid:62)
(1)
Let st be the probability distribution over webpages after
step t of our walk. So, st(i) is the probability of being at page
i after t steps. Since the walk starts with a standard teleport,
s0 = v.
Furthermore, if A holds the conditional probabilities spec-
iﬁed, st = s(cid:62)
0 At. The
PageRank distribution is the limit of this random walk process.
Speciﬁcally,
t−1A. Expanding this recurrence, st = s(cid:62)
Deﬁnition 8 (PageRank): The PageRank vector s for ran-
dom walk matrix P, teleportation distribution vector v and
α ∈ [0, 1) is given by
s(cid:62) = lim
t→∞ s(cid:62)
(2)
0 At
for s0 = v and A = αP + (1 − α)ev(cid:62).
The actual existence of the limit s(cid:62) is a standard property
of PageRank and follows from noting that, restricted to
the webpages reachable from the none zero elements of v,
the PageRank random walk forms an irreducible, aperiodic
Markov chain (see for example Haveliwala and Kamvar [26]
or Farahat et al. [22]).
One is generally not only interested in whether or not the
Markov chain converges to the stationary distribution, but also
how quickly, in case one is using the power method to compute
the PageRank vector. This is especially important
in case
one is not computing the entire vector, but instead sampling
from the vector by performing a random walk according
to the random surfer distribution; in this case, a bound on
the convergence rate is essential since one cannot test for
numerical convergence (as is usually done when computing
PageRank vectors in practice). Unfortunately, the state of the
literature in this regard is quite poor. The original work of Page
et al. [45] incorrectly references an analysis of an undirected
Markov chain; subsequent work on the convergence rate of
the power method that corrected this error, e.g. Haveliwala and
Kamvar [26], still assumed that the matrix A is diagonalizable,
which in general is not true. (This assumption is expressly
noted by Langville and Meyer [32, p.164].) Speciﬁcally, there
is no guarantee that the transition matrix of a directed Markov
chain has a full basis of eigenvectors.
In spite of these analytical snags, the power method works
rather well in practice, as well as predicted by these “second
eigenvalue” analyses. For completeness, we now offer a direct
proof that is similar to the standard arguments (e.g., [13], [14])
but does not rely on diagonalizability (or even the existence of
a second eigenvector) and obtains the same convergence rate
as a “second-eigenvalue” analysis of the power method. We
therefore ﬁnd that none of these assumptions are necessary to
8
explain the fast rate of convergence of the power method in
practice.
Theorem 9: Suppose s(cid:62) = limt→∞ s(cid:62)
0 At for A = αP +
(1 − α)ev(cid:62) with α ∈ [0, 1), a probability distribution v, and
(cid:62)At is within 2αt of s(cid:62)
a random walk matrix P. Then s0
under the (cid:96)1-norm.
Proof: Expanding our recurrence gives:
t = s(cid:62)
s(cid:62)
= αs(cid:62)
= αs(cid:62)
t−1A
t−1P + (1 − α)s(cid:62)
t−1P + (1 − α)v(cid:62)
t−1ev(cid:62)
(3)
t−1(cid:107)1 = 1 since
The last step follows because s(cid:62)
t−1 is a probability distribution. Similarly, since s(cid:62) = s(cid:62)A,
s(cid:62)
s(cid:62) = αs(cid:62)P + (1 − α)v(cid:62). Thus
t−1e = (cid:107)s(cid:62)
(s − st)(cid:62) = α(s − st−1)(cid:62)P
(4)
Since t was arbitrary, by induction on t we conclude that
(s − st)(cid:62) = αt(s − s0)(cid:62)Pt
(5)
Now, since they are probability distributions, (cid:107)s(cid:107)1 = (cid:107)s0(cid:107)1 =
1. By the triangle inequality, (cid:107)s−s0(cid:107)1 ≤ 2. Furthermore, since
Pt is a row stochastic transition matrix, (cid:107)Pt(cid:107)1 = 1. Combined
with the triangle inequality this gives:
(cid:107)(s − s0)(cid:62)Pt(cid:107)1 ≤ (cid:107)s − s0(cid:107)1(cid:107)Pt(cid:107)1
= (cid:107)s − s0(cid:107)1
≤ 2
It follows that (cid:107)αt(s− s0)(cid:62)Pt(cid:107)1 ≤ 2αt. Thus, from Equation
5, the (cid:96)1 norm of our error from s is bounded by(cid:107)s − st(cid:107)1 ≤
2αt.
The basic paradigm for using PageRank in answering
search queries (described by Page et al. [45]) is to ﬁrst compute
a PageRank vector for the entire web, second ﬁlter out the
pages that do not contain the requested terms, and then output
the remaining pages, ordered by their PageRank. We can view
this ﬁltering step as conditioning the PageRank distribution on
the presence of the requested terms. This is how we will deﬁne
distributions over speciﬁc types of ﬁles:
Deﬁnition 10 (PageRank for a ﬁle type): For a ﬁle type τ
and PageRank vector s, we deﬁne s|τ to be the vector equal
to s(i) if i is of type τ and 0 otherwise. Then the PageRank
distribution over ﬁles of type τ is sτ = s|τ /(cid:107)s|τ(cid:107)1.
B. Sampling from PageRank
Since it is deﬁned as a limit, it seems reasonable to compute
an estimate for s by simply taking t to be a large number
and applying A to an arbitrary starting vector t times. This
process is analogous to the power method for ﬁnding the
top eigenvector of a diagonalizable matrix. It was originally
suggested by Page et al. [45] and has been analyzed and
applied throughout the literature. Since the error (2αt) shrinks
exponentially with the number of steps, the power method is
fast and is the standard means for computing PageRank.
However, it turns out that sampling from the PageRank
distribution is even simpler. In fact, it is possible to sample
from the distribution exactly, with no error. We ﬁrst note that
the PageRank distribution can be reformulated as a random
walk of a random (geometrically-distributed) length, a fact
essentially observed by Andersen et al. [11]. Recall that a
geometrically distributed random variable of parameter p is
a random variable that counts the number of times a p-biased
coin must be tossed before a ‘heads’ is obtained. Precisely,
it takes value i with probability (1 − p)pi, for every integer
i ≥ 0.
Given a parameter α, random walk matrix P, and teleporta-
tion vector v, consider the distribution over webpages sampled
as follows:
1) Sample t from a geometric distribution of parameter α.
2) Sample i from v, and take a t-step random walk started
from i according to P.
Theorem 11: The sampling algorithm produces a webpage
from the PageRank distribution with parameter α, random
walk matrix P and teleportation vector v. Furthermore, it only
requires visiting 1/(1 − α) pages in expectation.
Proof: Our analysis is based on an observation of Ander-
sen et al. [11]. The distribution of web pages output by the
sampling algorithm is given by the vector
∞(cid:88)
s(cid:48)(cid:62)
=
(1 − α)αtv(cid:62)Pt.
t=0
We show s(cid:48) is the PageRank distribution with parameter α,
random walk matrix P and teleportation vector v. As we have
already established existence and uniqueness of the PageRank
vector, we only need to verify that the geometric random walk
distribution s(cid:48) is a ﬁxed point of the map
A(x) = x[αP + (1 − α)ev(cid:62)]
that takes an additional random step according to the random
surfer process. Thus, by linearity:
A(s(cid:48)(cid:62)
) =
=
(1 − α)αtv(cid:62)Pt
[αP + (1 − α)ev(cid:62)]
(1 − α)αtv(cid:62)Pt(αP)
+ (1 − α)v(cid:62)
(1 − α)αtv(cid:62)Pt
+ (1 − α)α0v(cid:62)P0
The expected number of steps of the random walk is given
exactly by
P[#steps ≥ t] =
αt =
α