### Measuring Perceived Risk of Harm

To assess whether a warning caused participants to perceive a risk of harm, we designed two survey questions. These questions were presented using a 5-point Likert scale, and they aimed to measure both personal and societal harm:

- **harm1**: After seeing the warning, I believe the website may harm me if I visit it.
- **harm2**: After seeing the warning, I believe the website may harm other people who visit it.

These questions were administered after each treatment round to evaluate the participant's comprehension of the warning and their perception of potential harm. The questions were developed based on our laboratory results and small-scale pilot studies. The first question focuses on personal harm, while the second addresses societal harm. Our laboratory study identified personal harm as a possible mechanism of effect for disinformation warnings.

### Study Design and Implementation

#### Figures
- **Figure 4**: Each round of the crowdworker study began on this search page.
- **Figure 5**: Clicking on a search result led the participant to a story page containing a screenshot of a real news webpage, instructions, and buttons to submit an answer or navigate back.

#### Participant Responses
During the pilot phase of our crowdworker study, we observed that participants often conflated personal and societal harm when answering survey questions. To address this, we explicitly asked about both types of harm to ensure clarity. For our analysis, we focused solely on the personal harm response (harm1). We calculated the harm score \( h_{p,w} \) for participant \( p \) and warning \( w \) by projecting their harm1 response into the range \([-2,2]\):

\[ h_{p,w} = \text{harm1} - 3 \]

### Measuring Participant Behaviors

We measured the same behavioral outcomes as in the laboratory study (Section 3.4): clickthrough rate (CTR) and alternative visit rate (AVR).

- **CTR** represents the proportion of warning encounters where the participant clicked "Dismiss and continue."
- **AVR** measures how often participants clicked on more than one source before submitting an answer. An alternative visit was recorded in a control round when the participant visited more than one story page, and in a treatment round when the participant visited a different story page after encountering a warning, regardless of whether they clicked through.

Measuring AVR in both control and treatment rounds allowed us to estimate the warning's effect relative to a base rate.

### Assigning Warnings

To address our research questions about mechanisms of effect (RQ2 and RQ3), we compared the behavioral effects between warnings with the highest and lowest mean scores for informativeness and harm. Typically, a randomized controlled trial is used, where participants are randomly assigned to treatments. However, due to the potentially small difference in effect sizes and the high cost of observations, we needed an efficient method to assign participants to warnings.

We used an adaptive bandit algorithm to assign participants to warnings based on previous observations. Bandit algorithms update the probability of each condition according to a reward function aligned with the researchers' goals. This method has been widely used in clinical trials, software design optimization, and political opinion research. The reward function in our adaptive experiment prioritized disinformation warnings with high and low mean scores for informativeness and harm. For the first 80 participants, all warnings were assigned equally. Subsequently, the algorithm prioritized warnings with the highest and lowest mean scores, improving our confidence in these warnings.

### Participant Recruiting

We collected data from 250 Amazon Mechanical Turk workers based in the U.S., who had completed more than 5,000 jobs with an approval rate above 97%. Data from 12 workers who failed an attention check were discarded, leaving a sample population of 238. The population was roughly two-thirds male, with over half of the participants aged between 30 and 49. Most participants consumed news media at least five days a week and paid close attention to politics and current events. Full demographics are provided in supporting materials [100].

Recruiting and consent materials described the study as related to search engine use without mentioning warnings or disinformation. The task duration was estimated at 15-20 minutes, with compensation of $2.33 and a potential $1 bonus for retrieving the correct answer for all four queries. Data from participants who abandoned the task or exceeded a 2-hour time limit were discarded, and replacement participants were recruited. The study was approved by the Princeton University IRB.

### Results

We preregistered our analysis methods [100]. We computed mean ratings and 95% confidence intervals for informativeness and harm scores (Figure 6). For each political alignment and mechanism of effect, we identified the two warnings with the highest and lowest mean scores (Table 4). We then conducted statistical tests comparing the AVR between these two warnings, treating clickthroughs and alternative visits as samples drawn from binomial distributions.

#### Informativeness
- **i3** had a very high mean informativeness score for liberal participants (1.41 on the scale \([-2,2]\)).
- **i2** and **i4** also had high, consistent informativeness scores for liberals.
- For conservative participants, the mean informativeness scores were lower, with **i4** having the highest mean score (0.88).
- **h1**, intended to convey a risk of harm, was the least informative, achieving consistently low scores from both liberals and conservatives.

#### Harm
- **h1** had the highest mean score for evoking fear of harm among liberals (1.18) and a high score for conservatives, though **h3** had a slightly higher score (1.15).
- **i4** had the lowest mean harm score for both political alignments, with a low score among liberals (-0.76) and a more neutral score among conservatives (-0.2).

#### CTR
The cumulative CTR across all treatments was 16%, lower than the 40% observed in the laboratory study. No individual warning in this study demonstrated a higher CTR than the interstitial warning tested in the laboratory.

#### AVR
- The AVR across all treatments was 86%, compared to 19% in control rounds.
- A one-sided z-test confirmed a significant difference (p = 1.48eâˆ’111).
- We tested for significant differences in AVR between top- and bottom-scoring warnings for informativeness and harm within the liberal and conservative groups, but failed to reject the null hypotheses in all cases.

### Discussion

Our findings confirm that interstitial warnings have a strong effect on user behavior (RQ1). However, the results for warning informativeness (RQ2) and conveying a risk of harm (RQ3) were inconclusive. While we demonstrated that interstitial disinformation warnings can effectively inform users, we did not find evidence that informative warnings have a greater effect on user behavior. Similarly, we found that warning design can convey a risk of harm, but better conveying this risk did not affect user behavior.

We hypothesize that the user experience friction introduced by interstitial warnings may be a key factor in changing user behavior. Our results suggest a possible speech dilemma: interstitial disinformation warnings can inform users, but the level of information may not significantly influence their behavior.

Finally, we did not find evidence that partisanship moderates warning perceptions or behaviors (RQ4). Warning scores, CTRs, and AVRs were generally similar for liberal and conservative participants.

### Limitations

- There may be unmeasured variables that moderate the relationships between warning designs, participant perceptions, and behavior.
- Our sample, while more diverse than the laboratory sample, was not representative of the U.S. population.
- Cross-cultural research is needed to understand if the observed effects apply globally.

### Conclusion

In this section, we provide recommendations for future evaluation and deployment of disinformation warnings. Future work should explore the role of user experience friction in disinformation warnings, as well as other potential factors that may influence user behavior.