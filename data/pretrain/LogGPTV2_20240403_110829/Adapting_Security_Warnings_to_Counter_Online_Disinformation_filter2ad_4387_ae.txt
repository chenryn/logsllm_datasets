Harm We designed two survey questions to measure
whether a warning caused a participant to perceive a risk of
harm. The survey asked about agreement with the following
statements, using the same 5-point Likert scale as above.
• harm1: After seeing the warning, I believe the website
may harm me if I visit it.
• harm2: After seeing the warning, I believe the website
may harm other people who visit it.
After each treatment round, we presented a survey to mea-
sure whether the participant comprehended the warning or
perceived a risk of harm. We developed the survey questions
based on our laboratory results and small-scale pilot studies.
The two questions distinguish between personal harm (the
ﬁrst question) and societal harm (the second question). Recall
that in our laboratory study, we identiﬁed personal harm as
a possible mechanism of effect for disinformation warnings.
Figure 4: Each round of the crowdworker study began on this
search page.
Figure 5: Clicking on a search result led the participant to a
story page containing a screenshot of a real news webpage,
instructions, and buttons to submit an answer or navigate back.
We found when piloting our crowdworker study that partic-
ipants routinely conﬂated personal and societal harm when
answering survey questions. We expressly asked about these
two types of harm to ensure clarity for participants, and we
solely used the personal harm response in our analysis. We
computed the harm score hp,w for participant p and warning
w by projecting their harm1 response into the range [−2,2]:
hp,w = harm1 − 3
4.4 Measuring Participant Behaviors
We measured the same behavioral outcomes as in the lab-
oratory study (Section 3.4): clickthrough rate (CTR) and al-
ternative visit rate (AVR). CTR represents the proportion of
warning encounters where the participant clicked “Dismiss
and continue.” AVR measures how often participants clicked
on more than one source before submitting an answer. We
recorded an alternative visit in a control round when the par-
ticipant visited more than one story page, and we recorded
an alternative visit in a treatment round when the participant
visited a different story page after encountering a warning
(regardless of whether the the participant clicked through).
Measuring AVR in control and treatment rounds enables us
to estimate the warning’s effect with respect to a base rate.
4.5 Assigning Warnings
In order to answer our research questions about mecha-
nisms of effect (RQ2 and RQ3), we measured for differences
in behavioral effects between the warnings that achieved the
highest and lowest mean scores for informativeness and harm.
The standard method for comparing effect sizes between
treatments is a randomized controlled trial, in which partic-
ipants are randomly assigned to treatments (often in equal
numbers). The key variable determining how many obser-
vations are needed is the estimated difference in effect size
between treatments. If this difference is small, the study will
require a large number of observations for each treatment to
achieve statistically signiﬁcant conﬁdence.
When designing our study, we observed that the difference
in effect sizes between treatments could be small, meaning
that a large sample size could be necessary to evaluate our
hypotheses. Our observations were expensive, however, and
although we were testing 16 different conditions (8 warning
treatments with 2 score outcomes each), we were only in-
terested in comparing the effects of 4 conditions (with the
top and bottom mean scores for each outcome). We therefore
sought a method to efﬁciently assign participants to warnings
so that the top- and bottom-scoring warnings achieved high
conﬁdence levels, but the other warnings (which we would not
use in our hypothesis tests) would receive fewer observations
and therefore consume fewer experimental resources.
In this study, we used an adaptive bandit algorithm to assign
participants to warnings based on observations of previous
participants. With each new observation, bandit algorithms
update the probability of each condition in a study accord-
ing to some reward function that aligns with the researchers’
scientiﬁc goals [106]. Bandits have been widely used in clini-
cal trials, software design optimization, and political opinion
research [107–109]. We discuss the full details of our multi-
armed bandit implementation in supporting materials [100].
The reward function in our adaptive experiment preferred
disinformation warnings that achieved high and low mean
scores for informativeness and harm. For the ﬁrst n = 80 par-
ticipants, the algorithm assigned all warnings equally. For the
remaining participants, the reward function prioritized warn-
ings with the highest and lowest mean scores for informative-
ness and harm. As the bandit algorithm iterated, maximum
and minimum scoring warnings emerged, and the algorithm
improved our conﬁdence in the mean scores for these warn-
ings by prioritizing them for presentation to participants.
4.6 Participant Recruiting
We collected data from 250 Amazon Mechanical Turk
workers who were based in the U.S. and had completed more
than 5,000 jobs with an approval rate above 97%. We dis-
carded data from 12 workers who failed an attention check
question, leaving a sample population of 238. The population
was roughly two-thirds male and over half of participants
were between the ages of 30 and 49. The majority consumed
news media at least ﬁve days a week and paid somewhat
close attention to politics and current events. We provide full
population demographics in supporting materials [100].
Recruiting and consent materials described the study as
related to search engine use and did not mention warnings
or disinformation. We estimated the total task duration as
15-20 minutes and compensated participants $2.33 with the
opportunity to earn a $1 bonus (43%) for retrieving the correct
answer for all four queries.10 If a participant abandoned the
task partway through or exceeded a 2-hour time limit, we
discarded their data and recruited a replacement participant.
Our study was approved by the Princeton University IRB.
4.7 Results
We preregistered our analysis methods [100]. We computed
mean ratings and 95% conﬁdence intervals for informative-
ness and harm scores (Figure 6). For each political alignment
and mechanism of effect, we identiﬁed the two warnings with
the highest and lowest mean scores (Table 4).11 We then con-
ducted statistical tests comparing the AVR between these two
warnings.12 We treated clickthroughs and alternative visits as
samples drawn from binomial distributions.13 For tests with
large sample sizes, we used a z-test because a normal distribu-
tion approximates a binomial distribution; when the sample
size was small, we used Fisher’s exact test.
Informativeness We found that i3 had a very high mean in-
formativeness score for liberal participants (1.41 on the scale
[−2,2]). i2 and i4 also had high, consistent informativeness
scores for liberals. As for conservative participants, we found
much lower mean informativeness scores for every warning
we designed to be informative. i4 had the highest mean score
(0.88). The next most informative warning for conservatives
was h4, which we had intended to convey a risk of harm.
The least informative warning was h1, which achieved con-
sistent, extremely low informativeness scores from both liber-
als and conservatives. h1 was the most extreme warning in the
harm category; the only text it contained was “WARNING:
This website is dangerous.”
Harm h1 had the highest mean score for evoking fear of
harm among liberals (1.18), with high conﬁdence. We also
found that h1 had a high mean harm score for conservatives,
but h3 had a slightly higher score (1.15).
i4 had the lowest mean harm score for both political align-
ments, with a fairly low score among liberals (−0.76) and a
more neutral score among conservatives (−0.2).
CTR The cumulative CTR across all treatments was 16%,
which was noticeably lower than what we observed for in-
10We expected that nearly all participants would qualify, and 81.5% did.
11Our preregistered methods also included a non-overlapping 95% conﬁ-
dence interval criterion, but it did not affect our warning selection.
12We did not conduct statistical tests on warning CTRs, because our re-
search questions focused on seeking alternative sources of information.
13We assumed independence between warning treatments for a participant.
Table 4: We report alternative visit rates (AVR), clickthrough
rates (CTR), and mean informativeness (¯i) and harm (¯h)
scores with 95% conﬁdence intervals.
# AVR CTR
318 20% –
Control
Treatment 318 87% 16%
Selected treatments
Liberal
Conservative
¯iii
–
–
# AVR CTR
158 16% –
158 85% 17%
¯hhh
–
–
¯iii
–
–
h1
h3
i3
i4
120 85% 18% −1.94
±0.06
73
84% 18%
–
39
87% 13% 1.41
±0.43
17
82% 12%
–
1.18
±0.18
46
83% 17% −1.91
±0.11
–
–
27
81% 22%
10
90% 10%
–
–
−0.76
±0.69
25
76% 24% 0.88
±0.69
¯hhh
–
–
–
1.15
±0.46
–
−0.2
±0.62
terstitial warnings in the laboratory (40%). No individual
warning in this study demonstrated a higher CTR than the
interstitial warning we tested in the laboratory.
AVR The AVR across all treatments was 86%, compared
to 19% in control rounds. We used a one-sided z-test to eval-
uate if this difference was signiﬁcant, and we found strong
support for the hypothesis that the AVR in treatment rounds
was greater than the AVR in control rounds (p = 1.48e−111).
We tested whether there was a signiﬁcant difference in
AVR between the top- and bottom-scoring warnings for in-
formativeness within the liberal and conservative groups. For
liberals, we used a one-sided z-test and failed to reject the
null hypothesis that the AVR of the top-scoring warning was
less than or equal to the AVR of the bottom-scoring warning
(p = 0.27). For conservatives, we used a one-tailed Fisher’s
exact test (due to the small sample size of conservative partici-
pants) and also failed to reject the null hypothesis (p = 0.54).
Next, we tested for an AVR difference between the top- and
bottom-scoring warnings for harm. We failed to reject both
null hypotheses, with a one-sided z-test for liberals (p = 0.41)
and a one-tailed Fisher’s test for conservatives (p = 0.74).
4.8 Discussion
We found that interstitial warnings have a strong effect on
user behavior, conﬁrming our laboratory study results (RQ1).
The results for warning informativeness were inconclusive
(RQ2). We demonstrated that interstitial disinformation warn-
ings can effectively inform users that a website may contain
disinformation; we identiﬁed warning designs that scored
well on average for informing participants. Conveying that
a website may contain disinformation can prompt users to
think critically about the website’s trustworthiness, and crit-
ical thinking is an important predictor of a user’s ability to
correctly judge the accuracy of information [105, 110]. We did
not, however, ﬁnd evidence that informative warnings have a
greater effect on user behavior than uninformative warnings.
The results for warnings conveying a risk of harm were sim-
ilarly inconclusive (RQ3). We found that warning design can
Figure 6: Mean informativeness and harm scores, with 95% conﬁdence intervals, for liberals and conservatives.
effectively convey a risk of harm, but we did not ﬁnd evidence
that better conveying a risk of harm affects user behavior.
We hypothesize that the user experience friction introduced
by interstitial warnings may be an important causal factor for
changes in user behavior.14 We found evidence for friction as
a mechanism of effect in our laboratory study (Section 3.7.1),
but we did not test the theory in the crowdworker study.
Our results for RQ1 and RQ2 pose a possible speech
dilemma: interstitial disinformation warnings can effectively
inform users, but whether a user is informed may have little
relation to how they behave in response to warnings.
Finally, we did not ﬁnd evidence that partisanship moder-
ates warning perceptions or behaviors (RQ4). Figure 6 shows
that warning scores were generally similar for liberal and
conservative participants, and Table 4 shows that CTRs and
AVRs were also close between the groups.
Limitations There may be variables we did not measure
that moderate the relationships between warning designs, par-
ticipant perceptions, and behavior. Detailed qualitative meth-
ods, like in our laboratory study, can surface these variables—
but are challenging to implement in a crowdworker study.
We also note that while our crowdworker sample was more
diverse than our laboratory sample, neither sample was rep-
resentative of the U.S. population. The behavioral effects we
observed were fairly consistent across demographic groups,
though. Our study population only included individuals lo-
cated in the U.S.; cross-cultural research is needed to under-
stand if the effects we observed apply globally.
5 Conclusion
In this section, we provide recommendations for future
evaluation and deployment of disinformation warnings.
5.1 Directions for Future Research
Future work could explore the role of user experience fric-
tion in disinformation warnings. We found limited evidence
that friction is an important factor in our laboratory study.
The results from our crowdworker study also suggest that
friction—rather than informativeness or conveying a risk of
14Another possible explanation is a substitution effect in our analysis.
The uninformative warnings we examine, for example, could be effective at