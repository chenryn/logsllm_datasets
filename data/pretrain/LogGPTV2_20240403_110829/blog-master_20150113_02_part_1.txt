## zfs performance tuning basic  
### 作者                                                                                                                                                                                                   
digoal                                                                                                                                                                                                     
### 日期                                                                                                                                                                                                                    
2015-01-13                                                                                                                                                                                           
### 标签                                                                                                                                                                                                 
PostgreSQL , Linux , ZFS                                                                                                                                                                                               
----                                                                                                                                                                                                           
## 背景                                  
ZFS优化的一些基础常识.  
zfs模块每个参数的讲解, 包括IO调度, ARC的优化  
```  
man /usr/share/man/man5/zfs-module-parameters.5.gz  
```  
例如zfs将IO分为5个队列, 针对每个队列可以通过模块参数来控制IO调度, 例如为了提高同步写的能力, 同步写的active可以设大, 为了提高异步写的能力, 可以设置较大的脏ARC区域, 以及设置较宽的加速区域来降低同步写的冲突IO争抢.  
5个队列分别为同步读,写, 异步读,写, 以及ZFS自身的检查和修复操作IO.  
```  
ZFS I/O SCHEDULER  
       ZFS issues I/O operations to leaf vdevs to satisfy and complete I/Os.  The I/O scheduler determines when and in  
       what order those operations are issued.  The I/O scheduler divides operations into five I/O classes prioritized  
       in the following order: sync read, sync write, async read, async write, and scrub/resilver.  Each queue defines  
       the minimum and maximum number of concurrent operations that may be issued to the  device.   In  addition,  the  
       device  has  an  aggregate  maximum,  zfs_vdev_max_active. Note that the sum of the per-queue minimums must not  
       exceed the aggregate maximum.  If the sum of the per-queue maximums exceeds the  aggregate  maximum,  then  the  
       number of active I/Os may reach zfs_vdev_max_active, in which case no further I/Os will be issued regardless of  
       whether all per-queue minimums have been met.  
       For many physical devices, throughput increases with the number of concurrent operations, but latency typically  
       suffers. Further, physical devices typically have a limit at which more concurrent operations have no effect on  
       throughput or can actually cause it to decrease.  
       The scheduler selects the next operation to issue by first looking for an I/O class whose minimum has not  been  
       satisfied.  Once  all are satisfied and the aggregate maximum has not been hit, the scheduler looks for classes  
       whose maximum has not been satisfied. Iteration through the I/O classes is done in the order  specified  above.  
       No  further  operations  are issued if the aggregate maximum number of concurrent operations has been hit or if  
       there are no operations queued for an I/O class that has not hit its maximum.  Every time an I/O is  queued  or  
       an operation completes, the I/O scheduler looks for new operations to issue.  
       In general, smaller max_active’s will lead to lower latency of synchronous operations.  Larger max_active’s may  
       lead to higher overall throughput, depending on underlying storage.  
       The ratio of the queues’ max_actives determines the balance of performance between reads, writes,  and  scrubs.  
       E.g., increasing zfs_vdev_scrub_max_active will cause the scrub or resilver to complete more quickly, but reads  
       and writes to have higher latency and lower throughput.  
       All I/O classes have a fixed maximum number of outstanding operations except for the async write  class.  Asyn-  
       chronous writes represent the data that is committed to stable storage during the syncing stage for transaction  
       groups. Transaction groups enter the syncing state periodically so the  number  of  queued  async  writes  will  
       quickly burst up and then bleed down to zero. Rather than servicing them as quickly as possible, the I/O sched-  
       uler changes the maximum number of active async write I/Os according to the amount of dirty data in  the  pool.  
       Since  both throughput and latency typically increase with the number of concurrent operations issued to physi-  
       cal devices, reducing the burstiness in the number of concurrent operations also stabilizes the  response  time  
       of  operations  from other -- and in particular synchronous -- queues. In broad strokes, the I/O scheduler will  
       issue more concurrent operations from the async write queue as there’s more dirty data in the pool.  
       Async Writes  
       The number of concurrent operations issued for the async write I/O class follows a piece-wise  linear  function  
       defined by a few adjustable points.  
              |              o---------|      ********         |  
           0 +-------------------------------------*********----------------+  
             0%                                   100%  
       Note  that since the delay is added to the outstanding time remaining on the most recent transaction, the delay  
       is effectively the inverse of IOPS.  Here the midpoint of 500us translates to 2000 IOPS. The shape of the curve  
       was  chosen such that small changes in the amount of accumulated dirty data in the first 3/4 of the curve yield  
       relatively small differences in the amount of delay.  
       The effects can be easier to understand when the amount of delay is represented on a log scale:  
       delay  
       100ms +-------------------------------------------------------------++  
             +                                                              +  
             |                                                              |  
             +                                                             *+  
        10ms +                                                             *+  
             +                                                           ** +  
             |                                              (midpoint)  **  |  
             +                                                  |     **    +  
         1ms +                                                  v ****      +  
             +             zfs_delay_scale ---------->        *****         +  
             |                                             ****             |  
             +                                          ****                +  
       100us +                                        **                    +  
             +                                       *                      +  
             |                                      *                       |  
             +                                     *                        +  
        10us +                                     *                        +  
             +                                                              +  
             |                                                              |  
             +                                                              +  
             +--------------------------------------------------------------+  
             0%                                   100%  
       Note here that only as the amount of dirty data approaches its limit does the delay start to increase  rapidly.  
       The  goal  of  a  properly  tuned  system should be to keep the amount of dirty data out of that range by first  
       ensuring that the appropriate limits are set for the I/O scheduler to reach optimal throughput on  the  backend  
       storage, and then by changing the value of zfs_delay_scale to increase the steepness of the curve.  
```  
## 最佳实践   
http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide  
http://wiki.gentoo.org/wiki/ZFS  
zfs包的MAN文件 :   
```  
# rpm -ql zfs|grep man  
/usr/share/man/man1/zhack.1.gz  
/usr/share/man/man1/zpios.1.gz  
/usr/share/man/man1/ztest.1.gz  
/usr/share/man/man5/vdev_id.conf.5.gz  
/usr/share/man/man5/zfs-module-parameters.5.gz  
/usr/share/man/man5/zpool-features.5.gz  
/usr/share/man/man8/fsck.zfs.8.gz  
/usr/share/man/man8/mount.zfs.8.gz  
/usr/share/man/man8/vdev_id.8.gz  
/usr/share/man/man8/zdb.8.gz  
/usr/share/man/man8/zed.8.gz  
/usr/share/man/man8/zfs.8.gz  
/usr/share/man/man8/zinject.8.gz  
/usr/share/man/man8/zpool.8.gz  
/usr/share/man/man8/zstreamdump.8.gz  
# rpm -ql spl|grep man  
/usr/share/man/man1/splat.1.gz  
/usr/share/man/man5/spl-module-parameters.5.gz  
```  
模块信息  
```  
man /usr/share/man/man5/spl-module-parameters.5.gz	  
man /usr/share/man/man5/zfs-module-parameters.5.gz  
```  
模块配置举例  
```  
# vi /etc/modprobe.d/zfs.conf  
options zfs zfs_arc_max=140000000000                                                                                                  
options zfs zfs_dirty_data_max=28000000000                                                                                            
options zfs zfs_vdev_async_write_active_min_dirty_percent=10                                                                          
options zfs zfs_vdev_async_write_active_max_dirty_percent=30                                                                          
options zfs zfs_delay_min_dirty_percent=60                                                                                            
options zfs zfs_arc_shrink_shift=11   
```  
动态配置举例  
```  
# cd /sys/module/zfs/parameters/  
echo 140000000000 > zfs_arc_max  
```  