as number 1; and component 11 has the leftmost pixel among
components 11 to 13.
Fig. 2. All components rank ordered.
Effectively, this step is like creating a superposition of four
extracted images, and then sorting all the extracted components
in a particular order via the above algorithm.
Step 2. Graph building. Our algorithm constructs an n×n
table, where n is the total number of components. For the
example in Figure 2, n = 14.
A cell (i, k) at the intersection of row i and column k in the
table indicates whether it is feasible to combine components
i, i+1,· · ·, k all together to form a larger single component. If
such a combination is feasible, the cell (i, k) will be marked
with ‘•’. Otherwise, the cell (i, k) will be set to NULL. The
infeasible case occurs only in one of the following scenarios:
(1) when i is larger than k (i.e. when a cell’s row index is
larger than its column index, which should be omitted, since
we combine components only in a monotonically increasing
order); or the combination is either (2) too wide or (3) too
thin to form a legitimate character. (Note: the largest possible
character width and the smallest possible character width can
be empirically established with a simple analysis of a sample
dataset; this is a trivial task.)
The initial table for the example in Figure 2 is shown
in Table III, where all plausible component combinations are
marked by ‘•’.
The n × n table gives all the plausible component combi-
nations for an image. Our ultimate task is to use information
in the table to ﬁnd the most likely way of forming characters,
i.e., ﬁnding the best partition. This table is effectively a graph.
Figure 3 gives a directed graph that is equivalent to Table III.
TABLE III.
THE INITIAL n × n TABLE FOR THE EXAMPLE IN FIGURE
2.
1 2 3 4 5
6 7 8 9 10 11 12 13 14
• • • •
•
•
• • • •
• •
•
• •
•
1
2
3
4
5
6
7
8
9
10
11
12...14
•
•
•
•
•
•
•
•
Fig. 3. The equivalent graph of Table 3.
4
This graph building process is similar to the method in [13].
However, a main difference is that they call the recognition
engine to produce a recognition result for each plausible
combination, but we do not call the engine at all.
Step 3. Graph pruning. A node on a graph can be
redundant for our purpose, if there is no feasible path among
all those passing through this node. We use the following
algorithm to detect and remove any redundant node:
i) For each node i (i 6= 1 and i 6= n + 1), using Dijkstra
algorithm to compute the shortest path from node 1 to node i,
and the shortest path from node i to node n+1;
ii) If the sum of the length of these two shortest paths is
larger than the largest possible Captcha string length, node i
will be removed as a redundant node; its connecting edges will
be removed, too. The rationale is simple: the length of a valid
path from node 1 to n+1 should not be larger than the number
of characters in a Captcha string;
iii) If there is no path from node 1 to node i, or no
path from node i to node n+1, we set the length of the
corresponding shortest path to inﬁnity;
iv) This process repeats recursively until no further nodes
are removed after a traversal.
Redundant nodes and their connecting edges in Figure 3,
as detected by the above algorithm, are marked with dotted
lines, indicating that they are to be removed.
Step 4. Recognising component combinations. Then a
trained KNN is used to determine which character each of the
remaining edges in the graph is likely to be. (Preparing KNN is
straightforward, and explained in Section V). We then update
cell (i, k) in the table with the recognition result returned by
the KNN engine for a corresponding edge.
TABLE IV.
THE FINAL n × n TABLE GENERATED BY KNN.
1 2
3
4
5
6
7
8
9
10
11...13
14
s/0.81 s/0.52
c/0.75 d/0.87
d/0.68
k/0.44 b/0.43
n/0.80
3/0.58
3/0.84
1
2,3
4
5
6
7
8
9
10
11
12...14
Fig. 4. The equivalent graph of Table 4.
Table IV shows the updated n × n table, and its equivalent
graph is shown in Figure 4. For example, both the cell (1, 3)
in the table and the edge from node 1 to node 4 in the graph
5
indicate that KNN recognises the combination of components
1 to 3 as ‘s’ with a conﬁdence level of 0.81.
Step 5. Graph search. Now we search the graph to ﬁnd
an optimal partition. We adopt a dynamic programming (DP)
approach for our graph search, which will ﬁnd the optimal
partition in only one traversal.
We deﬁne that the target problem of DP is to select the
path ending at node n+1 with the largest conﬁdence value
sum and the corresponding step (i.e. the number of edges on
the path) is equal to the Captcha string length (i.e. the number
of characters). Note: this does not mean that our algorithm is
applicable only to Captchas with a ﬁxed string length. Instead,
we easily handle those with a varied string length, e.g. by
enumerating all possible lengths (typically from 4 to 12), with
little performance penalty.
The overlapping sub-problem for DP is for each node j,
the conﬁdence-level sum along the path ending at j should be
the largest. Note that for a path ending at node j, there may
be several possible edge numbers and the largest conﬁdence-
level sum of each case should be recorded, as illustrated in
Table XV in Appendix. The sub-problem’s solution is worked
out with a bottom-up approach, i.e., the solution of node j is
worked out by that of its precursor.
The following pseudo code illustrates our DP process.
The traversal starts from node 1, and ends at node n+1;
the nodes are traversed in an ascending order. An array
value stores the conﬁdence-level sum of each possible step
for each node, result stores the corresponding result string
for each node, step stores the number of current recognised
characters. R is the ﬁnal recognition result and v is its cor-
responding conﬁdence level sum, conf idence and recochar
represent the recognition conﬁdence level and the result of each
feasible component combination, respectively. For example,
conf idence[i, j] is the conﬁdence level calculated by KNN
for the combination formed by combining components from i
to j.
Function GetV alue(j) works out the largest conﬁdence
sum and the corresponding result of each step[j] for node j
(i.e. value[j], result[j]), in which prej is a list that stores
all the precursors of node j. Function M ain works out the
value[n + 1] and result[n + 1]. This is a bottom-up process
since we calculate from value[1] to value[n + 1] in sequence.
The ﬁnal recognition result R is got by Function Select(num).
Generated by our attack program, Table V shows with the
example in Figure 2 the process of ﬁnding the optimal partition
with our DP algorithm. DP simpliﬁes the search process by
recording the largest conﬁdence sum of each node. The italic
item highlighted in the table indicates the optimal partition
that has the highest conﬁdence-level sum. That is, “sdn3” is
the recognition result in this case.
Procedure Main()
Begin
R ← N IL
v ← 0
forj ← 1 to n + 1
value[j] ← 0
result[j] ← null
step[j] ← 0
if j > 1
GetV alue(j)
Select(n + 1)
End
Procedure GetValue(j)
Begin
foreach i in prej
if value[i] + conf idence[i, j] > value[j]
value[j] ← value[i] + conf idence[i, j]
result[j] ← strcat(result[i], recochar[i, j])
step[j] ← step[i] + 1
End
Procedure Select(num)
Begin
foreach i in step[num]
if i in Captcha length
if value[num] > v
v ← value[num]
R ← result[num]
End
TABLE V.
THE SEARCH PROCESS.
j
4
5
7
8
10
11
15
step[j]
Path
value[j]
result[j]
1
1
2
2
3
3
4
1→4
1→5
1→4→7
1→4→8
1→4→7→10
1→4→8→11
1→4→8→11→15
0.81
0.52
1.56
1.68
2.00
2.48
3.32
s
s
sc
sd
sck
sdn
sdn3
V. EVALUATION
A. Attack Results
We have implemented our attack in C# and tested it on all
the target schemes on a desktop computer with a 3.3GHz Intel
Core i3 CPU and 2 GB RAM. We follow common practices
in the literature to evaluate our attack.
Data Collection. For each scheme in Table II, we collected
from the corresponding website 500 random Captchas as
a sample set, and another 500 as a test set. The choice
of target schemes follows a single and objective criterion:
their popularity by Alexa ranking. We collected all the data
randomly, and our data collection was carried out during 2013-
2015.
In this period, the schemes we study are relatively stable,
except that reCaptcha has adopted a non-text scheme.
KNN Engine. Character samples we extracted from sample
sets are all normalized to 28*28 pixels. KNN is a simple
and effective classiﬁer in text recognition. To do character
recognition, we measure the similarity between corresponding
pixels of two images. The conﬁdence level of a recognition
result is also derived from this similarity value.
6
We assign a larger weight to similar black pixels, but a
smaller weight to similar white pixels, in order to decrease the
importance of matching background pixels in decision making.
If two corresponding pixels do not match, a negative value will
be added to the similarity calculation.
The recognition rate achieved by KNN depends on both
the sample size and the value of k. We determine k value via
cross-validation.
Success rate. Our attack’s success rate and average speed
on each scheme are summarized in Table VI. Our success rates
range from 5.0% to 77.2%, and for a majority of the schemes,
the minimum success rate is 16.2%.
TABLE VI.
ATTACK RESULTS.
Scheme
Success rate
Speed(s)
reCAPTCHA
Yahoo!
Baidu
Wikipedia
QQ
Microsoft
Amazon
Taobao
Sina
Ebay
77.2%
5.0%
44.2%
23.8%
56.0%
16.2%
25.8%
23.4%
9.4%
58.8%
10.27
28.56
2.81
3.74
4.95
12.59
13.18
4.64
4.83
5.98
A commonly accepted goal for Captcha robustness is to
prevent automated attacks from achieving higher than 0.01%
success [7]. But this goal was considered too ambitious by
some researchers. For example, [5] suggested that a Captcha
scheme is broken, if an automated attack achieves a success
rate of 1%. According to either criterion, our attack has broken
all the Captchas deployed by the top 20 websites.
Our success rates on Yahoo! and Sina are relatively low.
For the Yahoo! scheme, our extraction method breaks a long
text string into a large number of (tiny) components, which
produces a huge possible set of combinations. The warping
and overlapping mechanisms used in this Captcha turns out to
be disruptive to our component sorting algorithm, making our
recognition less successful.
For the Sina scheme, because noise arcs are similar to
character components, and thus extracted out by our direc-
tional ﬁltering – they interfered our recognition engine. Those
intersecting arcs that cut
through characters are particular
troublemakers.
For the sake of generality and simplicity, no ad-hoc pro-
cessing is used in our attack. It is unsurprising that appropriate
preprocessing can improve our attack’s performance – for
example, in our experiments, some simple hollow ﬁlling and
noise arc removal boosted our success rates on Yahoo and
Sina schemes to 10.0% and 21.0%, respectively. Probably more
important, it is worthy noting that without any preprocessing
or scheme-speciﬁc optimisations, our attack works on all the
schemes, and thus demonstrates robustness to hollow fonts and
noise arcs to some extent.
Speed. On average, it takes 3 to 14 seconds for our attack
to break most of the schemes. The slowest speed was on the
Yahoo! scheme, nearly 29 seconds – still acceptable, as it is an
excessive usability requirement to demand every human user
to solve a Captcha in less than 30 seconds; some Captchas
deployed in the wild reported an average solving speed of
more than 46 seconds [17].
The following reasons explain that it takes more time to
attack the Yahoo! scheme than others. First, it used a much
longer text string than other schemes. Second, because it is
a hollow scheme, our extraction method breaks the whole
string into a large number of components (see Figure 10(a)
for an illustration). This slows down our recognition speed
signiﬁcantly. Third, it used digits, upper and lower case letters,
and thus had a relatively large alphabet set. This means it takes
more time for the engine to do comparison and recognition.
The fastest speed was on the Baidu scheme. In this scheme,
only four characters are used in each challenge. Thus the
extraction process produced much less character components
than with other schemes, and this signiﬁcantly reduces our
attack time.
Clearly, our attack is efﬁcient and poses a realistic threat
to all these schemes.
B. Further Applicability Test
We test our attack on the following Captchas that are
generally considered hard.
An old version of reCAPTCHA (Figure 5). The Stanford
team achieved a zero success on attacking this scheme, as
reported in CCS’11 [5]. The reCAPTCHA version that we
broke in the previous section is the new version, which was
carefully tuned and rolled out by Google in September 2013,
as reported by its designers in [6].
(a) Original image
(b) Reconstruction