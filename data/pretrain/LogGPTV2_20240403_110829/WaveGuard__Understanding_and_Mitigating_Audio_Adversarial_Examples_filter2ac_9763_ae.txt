### Table 4: Adaptive Attack Evaluations Against Different Transformation Functions
- **ε∞**: Initial L∞ bound used in the attack algorithm.
- **δ∞**: Mean L∞ norm of the perturbations obtained after applying the adaptive attack algorithm.
- **Bolded values**: δ∞ required to completely break (AUC ≤ 0.5) a particular transformation function-based defense.
- **dBx(δ)**: Relative loudness of the perturbation with respect to the examples in the dataset (the lower, the quieter).
- **SR (xadv)** and **SR (g(xadv))**: Attack success rate for un-transformed (xadv) and transformed audio (g(xadv)), respectively, obtained using the adaptive attack algorithm on a given transformation function.

| ε∞ | δ∞ | Acc. | dBx(δ) | SR (xadv) | SR (g(xadv)) |
|---|---|---|---|---|---|
| 0.01 | 0.05 | - | - | - | - |
| 0.11 | 0.01 | 50.0% | - | - | - |
| 0.06 | 0.02 | 50.0% | - | - | - |
| 0.05 | 0.44 | 50.0% | - | - | - |
| 0.01 | 0.51 | 95.5% | - | - | - |
| 0.00 | 0.34 | 86.0% | - | - | - |
| 0.0 | 0.29 | 84.0% | - | - | - |
| - | 0.08 | 72.5% | - | - | - |
| 0.05 | 0.03 | 50.0% | - | - | - |
| 0.01 | 0.31 | 50.0% | - | - | - |
| 0.02 | 0.11 | 50.0% | - | - | - |
| 0.44 | 0.45 | 50.0% | - | - | - |
| 0.51 | 0.97 | 50.0% | - | - | - |
| 0.34 | 0.94 | 50.0% | - | - | - |
| 0.29 | 0.92 | 50.0% | - | - | - |
| 0.08 | 0.77 | 50.0% | - | - | - |
| 0.03 | 0.48 | 50.0% | - | - | - |
| 0.31 | 0.21 | 50.0% | - | - | - |

### Analysis of Adaptive Attacks

#### 1. Objective of Adaptive Attacks
The primary objective of the adaptive attacks is to evaluate the robustness of different transformation functions against adversarial perturbations. The adaptive attack algorithm aims to find the minimum perturbation (δ∞) required to completely break the defense (AUC ≤ 0.5).

#### 2. Detection Scores
The effectiveness of the detector for adversarial audios generated by the attack is assessed. For the adaptive attacks against Downsampling-Upsampling, Quantization-Dequantization, and Filtering transforms, we achieve low Character Error Rates (CER) between the target transcription and transcriptions for xadv and g(xadv). This makes it harder for the detector to discriminate between adversarial and benign samples, resulting in a significant drop in detector AUC and accuracy scores compared to the non-adaptive scenario.

Among these three transformations, bypassing Downsampling-Upsampling requires the highest amount of perturbation (δ∞ = 342), indicating that it serves as a more robust defense transformation compared to Quantization-Dequantization and Filtering. The columns SR(xadv) and SR(g(xadv)) indicate the percentage of examples that transcribed exactly to the target phrase for the un-transformed and transformed adversarial inputs, respectively.

#### Calibration of Detection Threshold
The calibration of the detection threshold depends on the use case of the ASR system. For a user-facing ASR system, the number of legitimate commands is usually much higher than the number of adversarial commands. Therefore, the false positive rate needs to be extremely low. As shown in Figure 11 (Appendix A.), in the non-adaptive attack scenario, we achieve a very high true positive rate at 0% false positive rate for targeted adversarial attacks (Carlini and Qin-I) for all transformation functions. Thus, a low detection threshold can be reliable against non-adaptive adversaries and not interfere with the user experience.

In the adaptive attack scenario, while both LPC and Mel inversion achieve higher AUC scores compared to other transforms, Mel inversion gives the highest true positive rate at extremely low false positive rates. Therefore, among the transformation functions studied, Mel Extraction and Inversion serve as the best defense choice for user-facing ASR systems.

#### Robustness of Perceptually Informed Representations
For both Mel extraction-inversion and LPC transformations, although we observe a drop in the detector scores compared to the non-adaptive attack setting, we are not able to completely bypass the defense using the initial distortion bound ε∞ = 500. Note that a perturbation higher than this magnitude has dBx(δ) > −29, which is more audible than ambient noise in a quiet room (dBx(δ) = −31) [38, 55]. To test the limit at which the defense breaks, we successively increase the allowed magnitude of perturbation. We are able to completely break the defense (AUC ≤ 0.5) at δ∞ = 2479 and δ∞ = 2167 for Mel extraction-inversion and LPC transforms, respectively. These perturbations are more than 6× higher than those required to break any of the other transformation functions studied and more than 25× higher than those required to fool an undefended model. This suggests that using perceptually informed intermediate representations proves to be more robust against adaptive attacks compared to naive compression and decompression techniques.

#### Comparison of CER Metrics
Figure 10 reports the same metrics as those reported in Figure 7 for the adaptive attack scenario with an initial ε∞ = 500. The CER(adv, g(adv)) (red bar) drops below CER(orig, g(orig)) (blue bar) for Downsampling-Upsampling, Quantization-Dequantization, and Filtering transforms, thereby breaking these defenses. In contrast, the red bar for Mel extraction-inversion and LPC-based defense is much higher than the blue bar, indicating that the defense is more robust under this adaptive attack setting.

### Discussion
Do learnings from adversarial defenses in the image domain transfer over to the audio domain? We find that not all learnings about input-transformation based defenses in the image domain transfer to the speech recognition domain. It has been shown that input-transformation based adversarial defenses can be easily bypassed using robust or adaptive attacks for image classification systems [6, 41]. However, an ASR system is a substantially different architecture compared to an image classification model. ASR systems operate on time-varying inputs and map each input frame to a language token. Since they rely on Recurrent Neural Networks (RNNs), the token prediction for each frame also depends on other frames in the signal. For targeted attacks, that are robust to a transformation g, we need to find an adversarial example xaudio such that both xaudio and g(xaudio) map to the target language tokens across all time-steps. On the other hand, for the image classification problem, the adaptive attack goal is simpler: Find an image ximage, such that both ximage and g(ximage) map to the same class label. Therefore, in our adaptive attack experiments, we need to add significant amounts of perturbation to bypass the defense even for simple transformation functions. We also find that adversarial attacks targeting undefended ASR models do not transfer to defended models even at high perturbation levels, in contrast to results reported in the image domain [39]. Details of this experiment are provided in Appendix C.

### Conclusion
We present WaveGuard, a framework for detecting audio adversarial inputs, to address the security threat faced by ASR systems. Our framework incorporates audio transformation functions and analyzes the ASR transcriptions of the original and transformed audio to detect adversarial inputs. We demonstrate that WaveGuard can reliably detect adversarial inputs, making it a robust solution for user-facing ASR systems.

### Acknowledgements
We thank our reviewers for their valuable and comprehensive feedback. This work was supported by SRC under Task ID: 2899.001 and ARO under award number W911NF-19-1-0317.

### References
[1] L. R. Rabiner, R. W. Schafer et al., “Introduction to Digital Speech Processing,” Foundations and Trends R in Signal Processing, 2007.
[2] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg, C. Case, J. Casper, B. Catanzaro, Q. Cheng, G. Chen et al., “Deep Speech 2: End-to-End Speech Recognition in English and Mandarin,” in International Conference on Machine Learning, ICML, 2016.
[3] J. Shen, P. Nguyen, Y. Wu, Z. Chen, M. X. Chen, Y. Jia, A. Kannan, T. N. Sainath, Y. Cao, and et al., “Lingvo: A Modular and Scalable Framework for Sequence-to-Sequence Modeling,” ArXiv, vol. abs/1902.08295, 2019.
[4] A. Y. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates, and A. Y. Ng, “Deep Speech: Scaling Up End-to-End Speech Recognition,” CoRR, vol. abs/1412.5567, 2014.
[5] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and Harnessing Adversarial Examples,” Stat, 2015.
[6] A. Athalye, N. Carlini, and D. Wagner, “Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,” in Proceedings of the 35th International Conference on Machine Learning, ICML 2018, 2018.
[7] N. Carlini and D. A. Wagner, “Towards Evaluating the Robustness of Neural Networks,” 2017 IEEE Symposium on Security and Privacy (SP), 2017.
[8] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami, “The Limitations of Deep Learning in Adversarial Settings,” in 2016 IEEE European Symposium on Security and Privacy (EuroS&P), 2016.
[20] J. Lin, C. Gan, and S. Han, “Defensive Quantization: When Efficiency Meets Robustness,” Artificial Intelligence, Communication, Imaging, Navigation, Sensing Systems, 2019.
[9] M. Alzantot, B. Balaji, and M. B. Srivastava, “Did You Hear That? Adversarial Examples Against Automatic Speech Recognition,” CoRR, vol. abs/1801.00554, 2018. [Online]. Available: http://arxiv.org/abs/1801.00554
[10] L. Schönherr, K. Kohls, S. Zeiler, T. Holz, and D. Kolossa, “Adversarial Attacks Against Automatic Speech Recognition Systems via Psychoacoustic Hiding,” arXiv preprint arXiv:1808.05665, 2018.
[11] N. Carlini and D. Wagner, “Audio Adversarial Examples: Targeted Attacks on Speech-to-Text,” in 2018 IEEE Security and Privacy Workshops (SPW), 2018.
[12] N. Carlini, P. Mishra, T. Vaidya, Y. Zhang, M. Sherr, C. Shields, D. Wagner, and W. Zhou, “Hidden Voice Commands,” in 25th USENIX Security Symposium, 2016.
[13] H. Yakura and J. Sakuma, “Robust Audio Adversarial Example for a Physical Attack,” CoRR, vol. abs/1810.11793, 2018. [Online]. Available: http://arxiv.org/abs/1810.11793
[14] Y. Qin, N. Carlini, G. Cottrell, I. Goodfellow, and C. Raffel, “Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition,” in International Conference on Machine Learning, 2019.
[15] P. Neekhara, S. Hussain, P. Pandey, S. Dubnov, J. McAuley, and F. Koushanfar, “Universal Adversarial Perturbations for Speech Recognition Systems,” in Proc. Interspeech, 2019.
[16] X. Yuan, Y. Chen, Y. Zhao, Y. Long, X. Liu, K. Chen, S. Zhang, H. Huang, X. Wang, and C. A. Gunter, “CommanderSong: A Systematic Approach for Practical Adversarial Voice Recognition,” in 27th USENIX Security Symposium, 2018.
[17] Y. Chen, X. Yuan, J. Zhang, Y. Zhao, S. Zhang, K. Chen, and X. Wang, “Devil’s Whisper: A General Approach for Physical Adversarial Attacks Against Commercial Black-Box Speech Recognition Devices,” in 29th USENIX Security Symposium, 2020.
[18] D. Meng and H. Chen, “Magnet: A Two-Pronged Defense Against Adversarial Examples,” in Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017.
[19] C. Guo, M. Rana, M. Cisse, and L. van der Maaten, “Countering Adversarial Images Using Input Transformations,” in International Conference on Learning Representations, ICLR, 2018.
[21] F. Khalid, H. Ali, H. Tariq, M. A. Hanif, S. Rehman, R. Ahmed, and M. Shafique, “QusecNets: Quantization-Based Defense Mechanism for Securing Deep Neural Network Against Adversarial Attacks,” in 2019 IEEE 25th International Symposium on On-Line Testing and Robust System Design (IOLTS), 2019.
[22] B. Liang, H. Li, M. Su, X. Li, W. Shi, and X. Wang, “Detecting Adversarial Image Examples in Deep Neural Networks with Adaptive Noise Reduction,” IEEE Transactions on Dependable and Secure Computing, 2018.
[23] K. Rajaratnam, K. Shah, and J. Kalita, “Isolated and Ensemble Audio Preprocessing Methods for Detecting Adversarial Examples Against Automatic Speech Recognition,” in Conference on Computational Linguistics and Speech Processing (ROCLING), 2018.
[24] Z. Yang, P. Y. Chen, B. Li, and D. Song, “Characterizing Audio Adversarial Examples Using Temporal Dependency,” in 7th International Conference on Learning Representations, ICLR, 2019.
[25] D. Iter, J. Huang, and M. Jermann, “Generating Adversarial Examples for Speech Recognition,” 2017.
[26] T. Vaidya, Y. Zhang, M. Sherr, and C. Shields, “Cocaine Noodles: Exploiting the Gap Between Human and Machine Speech Recognition,” in 9th USENIX Workshop on Offensive Technologies (WOOT 15), 2015.
[27] L. E. Baum and J. A. Eagon, “An Inequality with Applications to Statistical Estimation for Probabilistic Functions of Markov Processes and to a Model for Ecology,” Bull. Amer. Math. Soc., 1967.
[28] L. E. Baum, T. Petrie, G. Soules, and N. Weiss, “A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains,” The Annals of Mathematical Statistics, 1970.
[29] A. Acero, L. Deng, T. Kristjansson, and J. Zhang, “HMM Adaptation Using Vector Taylor Series for Noisy Speech”