0.01
0.11
0.06
0.05
0.01
0.00
0.0
-
0.05
0.01
0.02
0.44
0.51
0.34
0.29
0.08
0.03
-
0.31
0.11
0.45
0.97
0.94
0.92
0.77
0.48
0.21
Acc.
-
50.0%
50.0%
50.0%
95.5%
86.0%
84.0%
72.5%
50.0%
50.0%
Table 4: Adaptive attack evaluations against different transformation functions. ε∞ is the initial L∞ bound used in the attack
algorithm and δ∞ is the mean L∞ norm of the perturbations obtained after applying the adaptive attack algorithm. Bolded values
indicate the δ∞ required to completely break (AUC ≤ 0.5) a particular transformation function based defense. dBx(δ) is the
relative loudness of the perturbation with respect to the examples in the dataset (the lower the quieter). SR (xadv) and SR (g(xadv))
indicate the attack success rate for un-transformed (xadv) and transformed audio (g(xadv)) respectively obtained using the adaptive
attack algorithm on a given transformation function.
adaptive attack in its objective? 2) Detection Scores: How
effective is our detector for the adversarial audios generated
by the attack?
For the adaptive attacks against
the Downsampling-
upsampling, Quantization-Dequantization and Filtering trans-
forms, we achieve low CER between the target transcrip-
tion and transcriptions for xadv and g(xadv) (CER(xadv,τ)
and CER(g(xadv)) respectively). This makes it harder for
the detector to discriminate between adversarial and be-
nign samples thereby resulting in a drastic drop in detector
AUC and accuracy scores as compared to the non-adaptive
scenario. Amongst these three transformations, bypassing
Downsampling-upsampling requires the highest amount of
perturbation (δ∞ = 342) indicating that it serves as a more
robust defense transformation as compared to Quantization-
Dequantization and Filtering. The columns SR(xadv) and
SR(g(xadv)) indicate the percentage of examples that tran-
scribed exactly to the target phrase for the un-transformed
and transformed adversarial inputs respectively.
The calibration of the detection threshold depends on the
use case of the ASR system—for a user facing ASR system,
the number of legitimate commands would usually be very
high as compared to the number of adversarial commands.
Therefore, the false positive rate needs to be extremely low
for such ASR systems. As shown in Figure 11 ( Appendix A.),
in the non-adaptive attack scenario, we are able to achieve
a very high true positive rate at 0% false positive rate for
the targeted adversarial attacks (Carlini and Qin-I) for all
transformation functions. Therefore a low detection threshold
can be reliable against non-adaptive adversaries and also
not interfere with the user experience. In the adaptive attack
scenario, while both LPC and Mel inversion achieve higher
AUC scores as compared to other transforms, Mel inversion
transform gives the highest true positive rate at extremely low
false positive rates. Therefore, amongst the transformation
functions studied in our work, Mel Extraction and Inversion
serves as the best defense choice for user facing ASR systems.
Robustness of perceptually informed representations: For
both Mel extraction-inversion and LPC transformations, al-
though we observe a drop in the detector scores as compared
to the non-adaptive attack setting, we are not able to com-
pletely bypass the defense using the initial distortion bound
ε∞ = 500. Note that a perturbation higher than this magni-
tude, has dBx(δ) > −29 which is more audible than ambient
noise in a quiet room (dBx(δ) = −31) [38, 55]. In order to
test the limit at which the defense breaks, we successively
increase the allowed magnitude of perturbation. We are able
to completely break the defense (AUC ≤ 0.5) at δ∞ = 2479
and δ∞ = 2167 for Mel extraction-inversion and LPC trans-
forms respectively. These perturbations are more than 6×
higher than that required to break any of the other transforma-
tion functions studied in our work and more than 25× higher
than that required to fool an undefended model. This suggests
that using perceptually informed intermediate representations
prove to be more robust against adaptive attacks as compared
to naive compression and decompression techniques.
Figure 10 reports the same metrics as those reported
in Figure 7 for the adaptive attack scenario with an ini-
tial ε∞ = 500. The CER(adv,g(adv)) (red bar) drops
below CER(orig,g(orig)) (blue bar) for Downsampling-
upsampling, Quantization-Dequantization and Filtering trans-
forms thereby breaking these defenses. In contrast, the red
bar for Mel extraction-inversion and LPC based defense is
much higher than the blue bar indicating that the defense is
more robust under this adaptive attack setting.
USENIX Association
30th USENIX Security Symposium    2285
inputs from recently proposed and highly successful targeted
and untargeted attacks on ASR systems. Furthermore, we
evaluate WaveGuard in the presence of an adaptive adver-
sary who has complete knowledge of our defense. We ﬁnd
that only at signiﬁcantly higher magnitudes of adversarial
perturbation, which are audible to the human ear, can an adap-
tive adversary bypass transformations that compress input
to perceptually informed audio representations. In contrast,
naive audio transformation functions can be easily bypassed
by an adaptive adversary using small inaudible amounts of
perturbations. This makes transformations such as LPC and
Mel extraction-inversion more robust candidates for defense
against audio adversarial attacks.
Acknowledgements
We thank our reviewers for their valuable and comprehensive
feedback. This work was supported by SRC under Task ID:
2899.001 and ARO under award number W911NF-19-1-0317.
References
[1] L. R. Rabiner, R. W. Schafer et al., “Introduction to
digital speech processing,” Foundations and Trends R(cid:13)
in Signal Processing, 2007.
[2] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai,
E. Battenberg, C. Case, J. Casper, B. Catanzaro,
Q. Cheng, G. Chen et al., “Deep speech 2: End-to-end
speech recognition in english and mandarin,” in Interna-
tional conference on machine learning, ICML, 2016.
[3] J. Shen, P. Nguyen, Y. Wu, Z. Chen, M. X. Chen, Y. Jia,
A. Kannan, T. N. Sainath, Y. Cao, and et al., “Lingvo:
a modular and scalable framework for sequence-to-
sequence modeling,” ArXiv, vol. abs/1902.08295, 2019.
[4] A. Y. Hannun, C. Case, J. Casper, B. Catanzaro, G. Di-
amos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta,
A. Coates, and A. Y. Ng, “Deep speech: Scaling up end-
to-end speech recognition,” CoRR, vol. abs/1412.5567,
2014.
[5] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining
and harnessing adversarial examples,” Stat, 2015.
[6] A. Athalye, N. Carlini, and D. Wagner, “Obfuscated
gradients give a false sense of security: Circumventing
defenses to adversarial examples,” in Proceedings of the
35th International Conference on Machine Learning,
ICML 2018, 2018.
[7] N. Carlini and D. A. Wagner, “Towards evaluating the
robustness of neural networks,” 2017 IEEE Symposium
on Security and Privacy (SP), 2017.
Figure 10: Mean CER between the ASR transcriptions of
un-transformed (x) and transformed (g(x)) audio for adaptive
attacks with an initial distortion ε∞ = 500.
9 Discussion
Do learnings from adversarial defenses in the image do-
main transfer over to the audio domain? We ﬁnd that not
all learnings about input-transformation based defenses in the
image domain transfer to the speech recognition domain. It
has been shown that input-transformation based adversarial
defenses can be easily bypassed using robust or adaptive at-
tacks for image classiﬁcation systems [6, 41]. However, an
ASR system is a substantially different architecture as com-
pared to an image classiﬁcation model. ASR systems operate
on time-varying inputs and map each input frame to a lan-
guage token. Since they rely on Recurrent Neural Networks
(RNNs), the token prediction for each frame also depends
on other frames in the signal. For targeted attacks, that are
robust to a transformation g, we need to ﬁnd an adversarial
example xaudio such that both xaudio and g(xaudio) map to the
target language tokens across all time-steps. On the other
hand, for the image classiﬁcation problem, the adaptive at-
tack goal is simpler: Find an image ximage, such that both
ximage and g(ximage) map to the same class label. Therefore,
in our adaptive attack experiments, we need to add signiﬁcant
amount of perturbation to bypass the defense even for simple
transformation functions. We also ﬁnd that adversarial attacks
targeting undefended ASR models do not transfer to defended
models even at high perturbation levels, in contrast to results
reported in the image domain [39]. Details of this experiment
are provided in Appendix C..
10 Conclusion
We present WaveGuard, a framework for detecting audio ad-
versarial inputs, to address the security threat faced by ASR
systems. Our framework incorporates audio transformation
functions and analyzes the ASR transcriptions of the origi-
nal and transformed audio to detect adversarial inputs. We
demonstrate that WaveGuard can reliably detect adversarial
2286    30th USENIX Security Symposium
USENIX Association
CER0.000.250.500.751.00Down-upQuantFilteringMelLPCCER( orig, g(orig))CER( adv, g(adv) )CER(orig, g(adv) )Adaptive Attacks[8] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B.
Celik, and A. Swami, “The limitations of deep learn-
ing in adversarial settings,” in 2016 IEEE European
Symposium on Security and Privacy (EuroS&P), 2016.
[20] J. Lin, C. Gan, and S. Han, “Defensive quantization:
When efﬁciency meets robustness,” Artiﬁcial Intelli-
gence, Communication, Imaging, Navigation, Sensing
Systems, 2019.
[9] M. Alzantot, B. Balaji, and M. B. Srivastava, “Did
you hear that? adversarial examples against automatic
speech recognition,” CoRR, vol. abs/1801.00554, 2018.
[Online]. Available: http://arxiv.org/abs/1801.00554
[10] L. Schönherr, K. Kohls, S. Zeiler, T. Holz, and
D. Kolossa, “Adversarial attacks against automatic
speech recognition systems via psychoacoustic hiding,”
arXiv preprint arXiv:1808.05665, 2018.
[11] N. Carlini and D. Wagner, “Audio adversarial examples:
Targeted attacks on speech-to-text,” in 2018 IEEE Secu-
rity and Privacy Workshops (SPW), 2018.
[12] N. Carlini, P. Mishra, T. Vaidya, Y. Zhang, M. Sherr,
C. Shields, D. Wagner, and W. Zhou, “Hidden voice com-
mands,” in 25th USENIX Security Symposium, 2016.
[13] H. Yakura and J. Sakuma, “Robust audio ad-
versarial example for a physical attack,” CoRR,
vol.
abs/1810.11793, 2018.
[Online]. Available:
http://arxiv.org/abs/1810.11793
[14] Y. Qin, N. Carlini, G. Cottrell, I. Goodfellow, and C. Raf-
fel, “Imperceptible, robust, and targeted adversarial ex-
amples for automatic speech recognition,” in Interna-
tional Conference on Machine Learning, 2019.
[15] P. Neekhara, S. Hussain, P. Pandey, S. Dubnov,
J. McAuley, and F. Koushanfar, “Universal adversarial
perturbations for speech recognition systems,” in Proc.
Interspeech, 2019.
[16] X. Yuan, Y. Chen, Y. Zhao, Y. Long, X. Liu, K. Chen,
S. Zhang, H. Huang, X. Wang, and C. A. Gunter, “Com-
mandersong: A systematic approach for practical ad-
versarial voice recognition,” in 27th USENIX Security
Symposium, 2018.
[17] Y. Chen, X. Yuan, J. Zhang, Y. Zhao, S. Zhang, K. Chen,
and X. Wang, “Devil’s whisper: A general approach for
physical adversarial attacks against commercial black-
box speech recognition devices,” in 29th USENIX Secu-
rity Symposium, 2020.
[18] D. Meng and H. Chen, “Magnet: a two-pronged de-
fense against adversarial examples,” in Proceedings of
the 2017 ACM SIGSAC Conference on Computer and
Communications Security, 2017.
[19] C. Guo, M. Rana, M. Cisse, and L. van der Maaten,
“Countering adversarial images using input transforma-
tions,” in International Conference on Learning Repre-
sentations, ICLR, 2018.
[21] F. Khalid, H. Ali, H. Tariq, M. A. Hanif, S. Rehman,
R. Ahmed, and M. Shaﬁque, “Qusecnets: Quantization-
based defense mechanism for securing deep neural net-
work against adversarial attacks,” in 2019 IEEE 25th
International Symposium on On-Line Testing and Ro-
bust System Design (IOLTS), 2019.
[22] B. Liang, H. Li, M. Su, X. Li, W. Shi, and X. Wang,
“Detecting adversarial image examples in deep neural
networks with adaptive noise reduction,” IEEE Transac-
tions on Dependable and Secure Computing, 2018.
[23] K. Rajaratnam, K. Shah, and J. Kalita, “Isolated and
ensemble audio preprocessing methods for detecting
adversarial examples against automatic speech recogni-
tion,” in Conference on Computational Linguistics and
Speech Processing (ROCLING), 2018.
[24] Z. Yang, P. Y. Chen, B. Li, and D. Song, “Characterizing
audio adversarial examples using temporal dependency,”
in 7th International Conference on Learning Represen-
tations, ICLR, 2019.
[25] D. Iter, J. Huang, and M. Jermann, “Generating adver-
sarial examples for speech recognition,” 2017.
[26] T. Vaidya, Y. Zhang, M. Sherr, and C. Shields, “Cocaine
noodles: Exploiting the gap between human and ma-
chine speech recognition,” in 9th USENIX Workshop on
Offensive Technologies (WOOT 15), 2015.
[27] L. E. Baum and J. A. Eagon, “An inequality with applica-
tions to statistical estimation for probabilistic functions
of markov processes and to a model for ecology,” Bull.
Amer. Math. Soc., 1967.
[28] L. E. Baum, T. Petrie, G. Soules, and N. Weiss, “A max-
imization technique occurring in the statistical analysis
of probabilistic functions of markov chains,” The annals
of mathematical statistics, 1970.
[29] A. Acero, l. Deng, T. Kristjansson, and J. Zhang, “Hmm
adaptation using vector taylor series for noisy speech