in Section 9). For instance in an autoencoder communication sys-
tem as shown in Figure 1, both of our defense mechanisms can
easily defeat a single vector UAP attack as proposed by [43, 44].
Therefore, instead of designing a single UAP, our attacker learns
the parameters of a PGM that generates separate perturbation vec-
tors without any knowledge of the input. Using a PGM instead
of a single noise vector provides the attacker with a large set of
perturbations, and we can use existing optimization techniques
such as Adam [25] to find the perturbations.
6 OUR PERTURBATION GENERATOR MODEL
(PGM)
In this section, we provide details on how our attack is performed
using a Perturbation Generator Model (PGM). Figure 2 illustrates
the process.
6.1 General Formulation
We formulate the universal adversarial perturbation problem in a
wireless communication system as:
||𝛿||2
arg min
𝛿
s.t. ∀𝑦 ∈ 𝐷 : 𝑓 (𝑦 + 𝛿) ≠ 𝑓 (𝑦)
(1)
where 𝑦 is the transmitted signal plus the AWGN noise (𝑦 = 𝑥 + 𝑛),
𝑓 is the underlying DNN-based function in the wireless system, and
𝐷 is the input domain of the wireless DNN model. The objective is
to find a minimal (perturbation with minimum power) universal
perturbation vector, 𝛿, such that when added to an arbitrary input
02468101214SNR (dB)10−610−510−410−310−210−1Block-error rate (log)no attacksingle UAP attacksingle UAP attack with adversarial training defensesingle UAP attack with perturbation subtracting defenseSession 1B: Attacks and Robustness CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea130Figure 2: Our attack setting
from a target input domain 𝐷, it will cause the underlying DNN-
based model 𝑓 (.) to misclassify and therefore increase the DNN-
based model loss function. Note that one cannot find a closed-form
solution for this optimization problem since the DNN-based model
𝑓 (.) is a non-convex function, i.e., a deep neural network. Therefore,
(1) can be formulated as follows to numerically solve the problem
using empirical approximation techniques:
arg max
𝛿
l(𝑓 (𝑦 + 𝛿), 𝑓 (𝑦))
(2)

𝑦∈D
where l is the DNN-based model loss function and D ⊂ 𝐷 is the
attacker’s network training dataset.
As mentioned above, instead of learning a single UAP as sug-
gested by [43, 44], we aim at learning the parameters of a PGM 𝐺
to be able to generate UAPs without any knowledge of the system
input. This generator model 𝐺 will generate UAP vectors when
provided with a random trigger parameter 𝑧 (we denote the cor-
responding adversarial perturbation as 𝛿𝑧 = 𝐺(𝑧)), i.e., we can
generate different perturbations for different values of 𝑧. Therefore,
the goal of our optimization problem is to optimize the parameters
of the PGM 𝐺 (as opposed to optimizing a UAP 𝛿 in [44]). Hence,
we formulate our optimization problem as:
𝑧∼𝑢𝑛𝑖 𝑓 𝑜𝑟𝑚(0,1)[
E
𝑦∈D
arg max
𝐺
l(𝑓 (𝑦 + 𝐺(𝑧)), 𝑓 (𝑦))]
(3)
We can use existing optimization techniques (e.g., Adam [25]) to
solve this problem. In each iteration of the training, our algorithm
selects a batch from the training dataset and a random trigger 𝑧,
then computes the objective function.
Algorithm 1 summarizes our approach to generate UAPs. In
each iteration, Algorithm 1 computes the gradient of the objective
function w.r.t. the perturbation for given inputs, and optimizes it
by moving in the direction of the gradient. The algorithm enforces
the underlying constraints of the wireless system using various
remapping and regularization functions that will be discussed in
the following sections. We use the iterative mini-batch stochastic
gradient ascent [15] technique.
Algorithm 1 Generating UAPs using PGM
D ← adversary training data
𝑓 ← DNN-based model
𝑦 ← training input
l𝑓 ← DNN-based loss function
M ← domain remapping function
R ← domain regularization function
𝐺(𝑧) ← initialize the blind adversarial perturbation model pa-
rameters (𝜃𝐺)
𝑝 ← the upper bound of the generated perturbations
𝑇 ← number of epochs
for epoch 𝑡 ∈ {1· · ·𝑇} do
for all mini-batch 𝑏𝑖 in D do
𝐽 = −( 1|𝑏𝑖 |
𝑧 ∼ Uniform (0,1)
Rotate M(𝑦, 𝐺(𝑧)) based on the channel phase shift
𝒙∈𝑏𝑖, l(𝑓 (M(𝑦, 𝐺(𝑧), 𝑝)), 𝑓 (𝑥))) + R(𝐺(𝑧))
Update 𝐺 to minimize 𝐽
end for
end for
return 𝐺
6.2 Incorporating Power Undetectability
Constraint
As our first constraint on UAPs, we introduce a constraint on the
attacker’s perturbation power. We enforce this constraint to make
the perturbations unnoticeable by the receiver. This constraint
defines an upper bound on the generated perturbation’s power.
To enforce this power constraint, we use a remapping function M
while creating the UAP. Here M adjusts the perturbed signal to
comply with the power constraint. Therefore, we reformulate our
optimization problem by including the remapping function M:
𝑧∼𝑢𝑛𝑖 𝑓 𝑜𝑟𝑚(0,1)[
E
𝑦∈D
arg max
𝐺
l(𝑓 (M(𝑦, 𝐺(𝑧), 𝑝)), 𝑓 (𝑦))]
(4)
where 𝑝 is the upper bound on the power of the generated perturba-
tions. Each time we want to create the perturbation signal, we check
its power and, if it violates the constraint, we normalize it to satisfy
the power constraint. The following shows the remapping function
DNN-basedTransmitterDNN-basedReceiver+Power ConstraintRemappingPerturbationGenerator ModelGaussianRegularizerDistanceRegularizerpRandom triggerzAdversarialPerturbationAWGN NoisenModulationRecognitionsxyyChannelPhaseRotationSession 1B: Attacks and Robustness CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea131Algorithm 2 GAN-based noise regularizer
D ← training data
𝑓 ← DNN-based model
𝐺 ← PGM
𝐷 ← discriminator model
𝜇, 𝜎2 ← target desired Gaussian distribution parameters
for 𝑡 ∈ {1, 2, · · · ,𝑇} do
𝑧′ ∼ Gaussian(𝜇, 𝜎2)
𝑧 ∼ Uniform
train 𝐷 on 𝐺(𝑧) with label 1 and 𝑧′ with label 0
train 𝐺 on D using regularizer R
end for
return 𝐺
we used to preserve the power constraint of the perturbations in
the target wireless applications.
(cid:40)√𝑝 𝐺(𝑧)
||𝐺(𝑧) ||2
𝐺(𝑧),
,
||𝐺(𝑧)||2
||𝐺(𝑧)||2
2 > 𝑝,
2 ≤ 𝑝.
M(𝑦, 𝐺(𝑧), 𝑝) = 𝑦 +
6.3 Incorporating Statistical Undetectability
Constraint
As mentioned earlier, the adversarial perturbation is not noise but
a deliberately optimized vector in the feature space of the input
domain, and hence is easily distinguishable from the expected be-
havior of the noise in the communication system environment. To
make our adversarial perturbation undetectable to statistical infer-
ence, we enforce a statistical behavior (such as Gaussian behavior)
that is expected from the physical channel of a communication
system on our adversarial perturbations. We use a regularizer R in
the training process of our PGM to enforce a Gaussian distribution
for the perturbation. To do this, we use a generative adversarial
network (GAN) [16]: we design a discriminator model 𝐷(𝐺(𝑧))
that tries to distinguish the generated perturbations from a Gauss-
ian distribution. Then we use this discriminator as our regularizer
function to enforce the distribution of the crafted perturbations to
be similar to a Gaussian distribution. We simultaneously train the
blind perturbation model and the discriminator model. Hence, we
rewrite (4) as follows:
𝑧∼𝑢𝑛𝑖 𝑓 𝑜𝑟𝑚(0,1)[(
E
𝑦∈D
arg max
𝐺
l(𝑓 (M(𝑦, 𝐺(𝑧), 𝑝)), 𝑓 (𝑦)))
+𝛼R(𝐺(𝑧))]
(5)
where 𝛼 is the weight of the regularizer relative to the main objec-
tive function. Algorithm 2 shows the details of our technique to
generate perturbations that follow a Gaussian distribution with an
average 𝜇 and standard deviation 𝜎. Note that we use the Gauss-
ian distribution as the desired distribution for our noise since it
is expected from the environment where a wireless communica-
tion system operates, and thus it cannot be distinguished from the
normal AWGN noise of the channel.
6.4 Incorporating Robustness Constraint
As mentioned earlier, using a PGM instead of a single UAP to per-
form the adversarial attack provides the adversary an extremely
large set of perturbations. This makes the attack more robust against
countermeasures compared to the single vector UAP attack. How-
ever, if the generated perturbations are similar to each other, an
ad hoc defender (as discussed in Section 9) can use pilot signals
to accurately estimate (and remove) the perturbations. To prevent
this, we force Algorithm 1 to generate non-similar perturbations.
To this aim, we add the 𝑙2 distance between consecutively gener-
ated perturbations as a regularizer to the objective function. Hence,
in the training process, our model tries to maximize the distance
between perturbations. In Section 9, we see that incorporating this
constraint prevents an ad hoc defender from accurately estimating
the generated perturbations.
6.5 Incorporating Channel Phase Rotation
As mentioned in Section 5, in order to model the lack of phase
synchronization between the attacker and the transmitter, we add a
relative random phase to the perturbation generated by the pertur-
bation model and rotate its signal. For each adversarial perturbation,
we generate a random phase 𝜃 and rotate the perturbation based
on it. Note that the channel effect is applied on the perturbation
after applying all of the mentioned constraints on the perturbation.
Assume 𝛿 = M(𝑥, 𝐺(𝑧)) is a perturbation generated by the PGM
after applying the power constraint, and 𝛿𝑅 and 𝛿𝐼 are the real and
imaginary parts of the perturbation, respectively. Using the random
phase shift caused by the channel the rotated perturbation can be
derived as follows:
(cid:40)𝛿′
For all 𝑖 = 1, 2, · · · , 𝑁 :
𝑖,𝑅 = 𝛿𝑖,𝑅 cos(𝜃) − 𝛿𝑖,𝐼 sin(𝜃)
𝛿′
𝑖,𝐼 = 𝛿𝑖,𝐼 cos(𝜃) + 𝛿𝑖,𝑅 sin(𝜃)
(6)
𝐼 are the
𝑅 and 𝛿′
where 𝑁 is the length of the perturbation, and 𝛿′
real and imaginary parts of the rotated perturbation.
7 EXPERIMENTAL SETUP
For the three target wireless applications, we use the same setup as
their original papers [37, 38, 49]. The target models for each appli-
cation are the same as the ones proposed in their original papers.
Table 4 in Appendix A illustrates the input size and parameters of
each target DNN model. To enable a benchmark for comparison, we
obtain the code of the UAP adversarial attack proposed by [43, 44]
and transform it from TensorFlow to Pytorch. We then compare the
results of our adversarial attack using a PGM with the single vector
UAP adversarial attack. On the other hand, we are the first to apply
adversarial attacks on the OFDM channel estimation and signal
detection task; hence, we implement both the PGM attack and the
single vector UAP attack against the OFDM system for comparison.
We use fully connected layers for the PGM with different num-
bers of hidden layers and different numbers of neurons in each
layer based on the wireless application. Table 1 contains the details
of the structure used for each PGM.
For the discriminator model mentioned in Section 6, we use a
fully connected DNN model with one hidden layer of size 50 and
a ReLU activation function. The discriminator generates a single
output that can be interpreted as the probability of following a
Gaussian distribution for a signal. In the training process of our
discriminator, we set 𝜇 and 𝜎 to the average of the means and
Session 1B: Attacks and Robustness CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea132standard deviations of the generated perturbations. To train the
discriminator we use the Adam [25] optimizer with a learning rate
of 10−5.
8 EVALUATION OF ATTACK PERFORMANCE
In this section, we first evaluate our attack against three target
wireless applications without any undetectability constraint. We
also compare our attack with the single vector UAP attack. As
mentioned in Section 5, to consider the channel effect, we add
a relative random phase shift to our perturbations. Second, we
evaluate our attack while enforcing the undetectability constraint
mentioned in Section 6 on the autoencoder communication system.
Figure 3 highlights the performance of our attack compared to
the single vector UAP attack for the three target applications. We
discuss the results in detail as follows.
8.1 Performance Without Statistical
Undetectability
First, we evaluate the performance of our attack without the sta-
tistical undetectability constraint, but only with the basic, power
undetectability constraint (the robustness and phase rotation con-