this chapter, we will delve deeper into this subject and give more examples to build up a
clear definition.
Knowing what to include
Before we proceed, let's take a look in a little more detail at what to include in the
environment. We have outlined in the previous section a very simplistic definition for an
SOE. Part of any good SOE operating process is to have a pre-defined operating system
build that can be deployed at a moment's notice. There are multiple ways this might be
achieved and we will discuss these later in this book—however, for the time being, let's
assume that a base image of Ubuntu 18.04 LTS as suggested previously has been built.
What do we integrate into this standard build?
We know, for example, that our login policy is going to be applied throughout the
organization—hence, when the build is created, /etc/ssh/sshd_config must be
customized to include PermitRootLogin no and PasswordAuthentication no. There
is no point in performing this step in the post-deployment configuration, as this would
have to be performed on each and every single deployment. Quite simply, this would be
inefficient.
There are also important automation considerations for our operating system image. We
know that Ansible itself communicates over SSH, and so we know that we are going to
require some kind of credentials (it is quite likely this will be SSH key-based) for Ansible to
run against all of the deployed servers. There is little point in having to manually roll out
Ansible credentials to every single machine before you can actually perform any
automation, and so it is important to consider the kind of authentication you want Ansible
to use (for example, password- or SSH key-based), and to create the account and
corresponding credentials when you build the image. The exact method for doing this will
depend upon your corporate security standards, but I would advocate as a potential
solution the following:
Creating a local account on the standard image for Ansible to authenticate
against
Giving this account appropriate sudo rights to ensure all desired automation
tasks can be performed
Setting the local password for this account, or adding the SSH public key from an
Ansible key-pair to the authorized_keys file for the local Ansible account you
created
[ 18 ]
Building a Standard Operating Environment on Linux Chapter 1
Doing this, of course, does present some security risks. It is most likely
that Ansible will need full access to root on your servers for it to
effectively perform all of the automation tasks you might ask of it, and so
this Ansible account could become a backdoor if the credentials were ever
compromised. It is recommended that as few people as possible have
access to the credentials and that you make use of a tool such as AWX or
Ansible Tower (which we shall explore in Chapter 3, Streamlining
Infrastructure Management with AWX) to manage your credentials, hence
preventing people from getting hold of them inappropriately. You will
also almost certainly want to enable auditing of all activities performed by
the Ansible account and have these logged to a central server somewhere
so that you can inspect them for any suspicious activity and audit them as
required.
Moving on from user accounts and authentication, consider also Nagios Cross-Platform
Agent (NCPA). We know in our example that all deployed servers are going to need to be
monitored, and so it is a given that NCPA agent must be installed, and the token defined
such that it can communicate with the Nagios server. Again, there is no point doing this on
every single server after the standard image is deployed.
What about the web server though? It is sensible to have a standard, as it means all who are
responsible for the environment can become comfortable with the technology. This makes
administration easier and is especially beneficial for automation, as we shall see in the next
section. However, unless you only ever deploy web servers running on Linux, this
probably shouldn't be included as part of the standard build.
As a sound principle, the standard builds should be as simple and lightweight as possible.
There is no point in having additional services running on them, taking up memory and
CPU cycles, when they are redundant. Equally, having unconfigured services increases the
attack surface for any potential attacker and so for security reasons, it is advisable to leave
them out.
In short, the standard build should only include configuration and/or services that are
going to be common to every server deployed. This approach is sometimes referred to as
Just enough Operating System or JeOS for short, and it is the best starting point for your
SOE.
Having understood the basic principles of an SOE, we will proceed in the next section to
look in more detail at the benefits an SOE brings to your enterprise.
[ 19 ]
Building a Standard Operating Environment on Linux Chapter 1
Exploring SOE benefits
By now, you should have some idea of what an SOE is, and how it brings economies of
scale and greater efficiency to a Linux environment. Now, let's build on that and look in
more detail at an example of the importance of standardization.
Example benefits of an SOE in a Linux
environment
To say that there are commonalities in a Linux environment is to say that the servers that
comprise it all share attributes and features. For example, they might all be built upon
Ubuntu Linux, or they might all have Apache as their web server.
We can explore this concept with an example. Suppose that you have 10 Linux web servers
behind a load balancer and that they are all serving simple static content. Everything is
working fine, but then a configuration change is mandated. Perhaps this is to change the
document root of each web server to point to a new code release that has been deployed to
them by another team.
As the person responsible, you know that because the overall solution is load balanced, all
servers should be serving the same content. Therefore, the configuration change is going to
be required on each and every one. That means 10 configurations changes to make if you
do it by hand.
You could, of course, do this by hand, but this would be tedious and certainly isn't the best
use of time for a skilled Linux admin. It is also error-prone—a typo could be made on one
of the 10 servers and not spotted. Or the admin could be interrupted by an outage
elsewhere and only a subset of the server configurations changed.
The better solution would be to write a script to make the change. This is the very basis of
automation and it is almost certainly going to be a better use of time to run a single script
once against 10 servers than to manually make the same change 10 times over. Not only is it
more efficient, but if the same change became required in a month, the script could be
reused with just minimal adjustment.
[ 20 ]
Building a Standard Operating Environment on Linux Chapter 1
Now, let's throw a spanner into the works. What if, for reasons unknown, someone built
five of the web servers using Apache on CentOS 7, and the other five using nginx on
Ubuntu 18.04 LTS? The end result would, after all, be the same—at a basic level, they are
both web servers. However, if you want to change the document root in Apache on CentOS
7, you would need to do the following:
1. Locate the appropriate configuration file in /etc/httpd/conf.d.
2. Make the required change to the DocumentRoot parameter.
3. Reload the web server with systemctl reload httpd.service.
If you had to do the same thing for nginx on Ubuntu 18.04 LTS, you would do the
following:
1. Locate the correct configuration file in /etc/nginx/sites-available.
2. Make the required change to the root parameter.
3. Ensure that the site configuration file is enabled using the
a2ensite command—otherwise, Apache will not actually see the configuration
file.
4. Reload the web server with systemctl reload apache2.service.
As you can see from this rather simplistic (albeit contrived) example, a lack of commonality
is the enemy of automation. To cope with the case, you would need to do as follows:
1. Detect the operating system on each server. This in itself is non-trivial—there is
no one way to detect a Linux operating system, so your script would have to
walk through a series of checks, including the following:
1. The contents of /etc/os-release, if it exists
2. The output of lsb_release, if it is installed
3. The contents of /etc/redhat-release, if it exists
4. The contents of /etc/debian_version, if it exists
5. Other OS-specific files as required, if none of the preceding produce
meaningful results
2. Run different modification commands in different directories to effect the change
as discussed previously.
3. Run different commands to reload the web server, again as detailed previously.
Hence, the script becomes complex, more difficult to write and maintain, and certainly
more difficult to make reliable.
[ 21 ]
Building a Standard Operating Environment on Linux Chapter 1
Although this particular example is unlikely to occur in real life, it does serve to make an
important point—automation is much easier to implement when the environment is built to
a given standard. If a decision is made that all web servers are to be based on CentOS 7, to
run Apache 2, and have the site configuration named after the service name, then our
automation becomes so much easier. In fact, you could even run a simple sed command to
complete the change; for example, suppose the new web application was deployed
to /var/www/newapp:
# sed -i 's!DocumentRoot.*!DocumentRoot /var/www/newapp!g'
/etc/httpd/conf.d/webservice.conf
# systemctl reload httpd.service
No environment detection was necessary at all—just two simple shell commands. This
could be the basis of a really simple automation script to be run either on each of the 10
servers in turn or remotely over SSH. Either way, our automation task is now very simple
and shows how important commonality is. Importantly, an SOE by its very nature provides
this commonality. Lack of commonality doesn't just make automation difficult though—it
also hampers testing, often distorting test results as they may not be representative if
environments are different.
In the next section of this chapter, we will build on this knowledge to demonstrate how an
SOE benefits the process of software testing.
Benefits of SOE to software testing
A common problem I have seen in many environments is that of a new software
deployment having been successfully tested in an isolated pre-production environment and
yet not working correctly when it is released into the production environment. More often
than not, this problem is traced back to fundamental differences between the production
and pre-production environments, and so it is clear that for testing to be valid, both
environments must be as similar as possible.
Indeed, one of the problems containerization platforms such as Docker set out to solve was
exactly this, and hence portability is a core feature of container environments. Code
deployed on Docker is built on top of a container image that is, in simple terms, a stripped-
down operating system image (remember JeOS?). This, in effect, is a really tiny SOE, just
running in a container rather than on a bare metal server or virtual machine. However, it is
worth considering that if portability through environment standardization is a key feature
of container technology, then should we not try to achieve this across the board regardless
of our infrastructure.
[ 22 ]
Building a Standard Operating Environment on Linux Chapter 1
After all, if the configuration of the production servers is different from the pre-production
ones, then how valid is the testing? If the pre-production environment was built on CentOS
7.6, but the production environment lags behind it on CentOS 7.4, then can you really
ensure that a successful test result in one environment will guarantee it in the other? On
paper, it should work, but with fundamental differences in software and library versions
between the environments, this can never be guaranteed. This is before we even consider
possible differences in configuration files and installed software.
Hence, SOEs can help here—if all environments are built to the same standards, then in
theory, they should all be identical. Those of you who are eagle-eyed will notice the use of
the word should in the previous sentence and it is there for a good reason. SOEs are a great
step forward in defining the solution for testing failures, but they are not the whole story.
An environment is only standard as long as no-one modifies it, and if all users have
administration-level privileges, then it is very easy for someone (well-meaning or
otherwise) to log in and make changes that mean the environment deviates from the
standard.
The answer to this issue is automation—not only do SOEs promote and enable automation,
they also rely on it to maintain the level of standardization that they were required for in
the first place. The two support each other directly and should ideally be inseparable
partners—the SOE being the definition for the environment itself, and the automation
providing the implementation, enforcement, and auditing of the standard. Indeed, this is
the very premise of this book—that environments should be standardized as far as possible,
and that as many changes as possible should be automated.
The focus of this book will be on the automation aspect of this equation, as other than
adhering to the principles outlined in this chapter, the standards adopted will be unique for
every environment and it is not the goal of this book to determine them at a low level.
Working with our earlier example, both Apache and nginx have their benefits, and what
fits one use case may not fit another.
The same is true with operating systems—some organizations may rely on the support
package provided with Red Hat Enterprise Linux, whilst others don't need this but need
the bleeding edge technologies provided by, say, Fedora. There is no right or wrong way to
define a standard, as long as it meets the needs of the services it underpins. So far, we have
focused very much on commonality and standards; however, there will always be edge
cases where an alternative solution is required. In the next section, we will establish how to
know when you should deviate from your standards.
[ 23 ]
Building a Standard Operating Environment on Linux Chapter 1
Knowing when to deviate from standards
It would be easy to oversell the benefits of standardization, and they are certainly a
requirement for automation to be effective. However, like anything, it can be taken too far.
There is no point, for example, building servers on top of Red Hat Enterprise Linux 5.7 in
2019 simply because this was once defined as a standard (it is now End of Life and no
longer supported or updated). Similarly, from time to time, software vendors will have
qualified their product on certain specific Linux distributions or application stacks and will
not provide support unless their software is run within that ecosystem.
These are cases when deviations from the SOE are necessary, but they should be performed
in a controlled manner. For example, if a business has built up its Linux server estate on
Ubuntu 18.04 LTS, and then a new software stack is purchased that is only qualified on
RHEL 7, it is clear that builds of RHEL 7 are going to be required. These should, however,
be part of a new set of standards if possible and become a secondary SOE.
For example, if the CIS security hardening benchmark is applied to the Ubuntu SOE, then
the equivalent one should be applied to the RHEL too. Similarly, if the business has
standardized on nginx, then this should be used on the environment unless there is a
compelling reason not to (hint: a compelling reason is not that it's new and sexy—it is that it
solves a real problem or somehow improves something in a tangible way).
This results in the business going from one Linux SOE to two, which is still entirely
manageable and certainly better than returning to organic growth methodologies that
hamper effective automation.
In short, expect deviations, and don't fear them. Instead, handle them and use the
requirements to expand your standards, but stick with them where you can. SOEs present a
balancing act for everyone—on the one hand, they bring advantages of scale, make
automation easier, and reduce the training time for new staff (as all servers are more or less
the same in build and configuration), but if applied too rigidly, they could hamper
innovation. They must not be used as an excuse to do things a certain way because that's how
it has always been done.
There will always be a good reason to deviate from a standard; simply look for the business
benefit it brings, whether it's vendor support, lower resource requirements (hence saving
power and money), a longer support window, or otherwise. Try and avoid doing so just
because a new technology is shiny. As long as you are mindful of this fact, you will make
good decisions regarding deviation from your standards. In the next section of this chapter,
we will explore the ongoing maintenance of SOEs.
[ 24 ]
Building a Standard Operating Environment on Linux Chapter 1
Ongoing maintenance of SOEs
Although we will look at patching and maintenance in much greater detail later in this
book, it deserves a mention here as it dovetails nicely into the discussion on commonality
and deviations.
If nothing else, you are going to have to patch your Linux environment. For security
reasons alone, this is a given and good practice, even in an air-gapped environment. Let's
say that your environment is made up entirely of virtual machines and that you decided to
standardize on CentOS 7.2 some time ago. You built a virtual machine, performed all of the
required configuration steps to turn it into your SOE image, and then converted it into a
template for your virtualization environment. This becomes your gold build. So far, so good.
However, CentOS 7.2 was released in December 2015, nearly 4 years ago at the time of
writing, and if you were to deploy such an image today, the first thing you would have to
do is patch it. This would, depending on the build definition (and the number of packages
included in it), possibly involve downloading a gigabyte or more of packages to bring it up
to the latest standard and ensure you were running with all discovered vulnerabilities
patched, and all of the requisite bug fixes in place.
Obviously, if you are doing this at scale, this is inefficient—each new server is going to pull
all that data down over the network (or worse, the internet, if you don't have an internal
mirror), and then consume a great deal of I/O time and CPU time applying the patches,
during which the server can't be used for anything meaningful. If you only deploy one
server every few months, you can probably put up with this. If you deploy them on a more
regular basis, then this is going to waste a lot of valuable time and resources.
Hence, as well as performing ongoing maintenance of your environment itself, it is
important to perform ongoing maintenance of your standards. In 2019, it makes sense to
update your CentOS build to 7.6. At the very least, your ongoing maintenance schedule
should involve updating the gold build regularly.
We will go into much greater detail on how this might be performed later in this book.
However, for those who are eager to know now, this might be as simple as booting the
virtual machine image up, performing the updates, sanitizing it (for example, removing
SSH host keys that would be duplicated when the template is cloned), and then creating a
new template from it. Obviously, if any other changes to the SOE have been made since the
last maintenance cycle, then these can be incorporated too.
[ 25 ]
Building a Standard Operating Environment on Linux Chapter 1
You should expect your SOE to evolve over time—it would be easy perhaps to labor this
point—but there is an important balance between creating and maintaining standards, and
being overly rigid with them. You must accept that there are times when you will need to
deviate from them as we discussed in the previous section and that, over time, they will
evolve.
In short, SOEs should become a part of your regular IT processes; if employed correctly,
they don't hinder innovation— instead, they actively support it by giving back time to
those working with them and ensuring they spend less time performing mundane,
repetitive tasks and hence have more time for evaluating new technologies and finding
better ways of doing things. This, after all, is one of the key benefits of automation, which
SOEs support directly.
Summary
SOEs are a valuable addition to technology processes in almost any environment. They
require some time to be spent upfront on design work and defining standards, but this time
is more than offset later on as it supports efficient and effective automation of the
environments, and in this manner, actually gives time back to those responsible for the
environment, giving them more time to work on evaluating new technologies, finding more
efficient ways to do things, and being innovative in general.
In this chapter, you learned the fundamental definition of an SOE. You explored the
benefits that they bring to just about any Linux environment where scale is important, how
they support automation, and when and how to make deviations from the standards to
ensure that they do not become overly rigid and hamper growth. Finally, you learned about
the importance of ongoing maintenance, including maintenance of your standards as part
your ongoing maintenance cycles.
In the next chapter, we will explore how to make use of Ansible as an effective automation
framework for your Linux environment.
[ 26 ]
Building a Standard Operating Environment on Linux Chapter 1
Questions