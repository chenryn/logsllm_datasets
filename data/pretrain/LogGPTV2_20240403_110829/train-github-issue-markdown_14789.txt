## Questions & Help

I am attempting to fine-tune the XLM model on the SQuAD dataset. The command I used is as follows:

```bash
CUDA_VISIBLE_DEVICES=0 python run_squad.py \
    --model_type xlm \
    --model_name_or_path xlm-mlm-tlm-xnli15-1024 \
    --do_train \
    --do_eval \
    --train_file $SQUAD_DIR/train-v1.1.json \
    --predict_file $SQUAD_DIR/dev-v1.1.json \
    --per_gpu_train_batch_size 12 \
    --learning_rate 3e-5 \
    --num_train_epochs 100 \
    --max_seq_length 384 \
    --doc_stride 128 \
    --eval_all_checkpoints \
    --save_steps 50 \
    --evaluate_during_training \
    --output_dir /home/weihua/Sqad/transformers/xlm_out/
```

Here are the relevant images for reference:
- [Training and Evaluation Loss](https://user-images.githubusercontent.com/43492059/68640277-2da52700-0542-11ea-981b-c748e918e1d8.png)
- [Exact and F1 Scores](https://user-images.githubusercontent.com/43492059/68640296-3f86ca00-0542-11ea-93b3-db1bd64539b2.png)

I have set all parameters according to the example in the `pytorch-transformers` documentation. However, the Exact and F1 scores have shown minimal improvement, and the loss decreases very slowly, with each epoch dropping by approximately 0.01.

Could there be an issue with my parameter settings? Or do I need to make specific adjustments to the model when using XLM?

Thank you for your assistance.