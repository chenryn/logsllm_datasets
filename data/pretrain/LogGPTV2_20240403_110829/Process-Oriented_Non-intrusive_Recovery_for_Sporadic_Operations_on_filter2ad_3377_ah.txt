44 
In a rolling upgrade, a subset of instances currently running an old version of a software System 
are taken out of service and replaced with the same number of instances running a new version 
of the software system (T. Dumitras and P. Narasimhan, 2009). Rolling upgrade is the industry 
standard technique for moving to a new version of the software (T. Dumitras and P. Narasimhan, 
2009). Again, the detailed procedure of the Asgard rolling upgrade operation is shown in Fig. 
11. 
Asgard rolling upgrade error handling. Asgard rolling upgrade operation calls relevant cloud 
API  functions  to  perform  the  system  upgrade.  The  mapping  between  each  operational  step  of 
rolling upgrade and its relevant cloud API functions is shown in Table 3 (Asgard, 2013).  
Table 3.  Mapping between Asgard Operational Steps and Cloud APIs 
Step 
Cloud API  
1. Create New Launch Configuration 
CreateLaunchConfiguration 
2. Update Auto Scaling Group 
UpdateAutoScalingGroup 
3. Set User-Specified Rolling Policy 
PutScalingPolicy 
UpdateAutoScalingGroup 
4. Deregister Old Instance from ELB 
DeregisterInstancesFromLoadBalancer 
5. Terminate Old Instance 
TerminateInstancesInAutoScalingGroup 
6. Wait for ASG to Start New Instance 
RunInstances 
7. Register New Instance with ELB 
RegisterInstancesWithLoadBalancer 
According to the APIs failure rates (which are shown in section 1.4 of chapter 1) obtained from 
previous existing empirical studies (Q. Lu, et al., 2013; Q. Lu, et al., 2014) and considering that 
some steps can be interfered by another team (M. Fu, et al., 2014; M. Fu, et al., 2016), the steps 
in rolling upgrade are error prone and errors and failures could occur in each operational step. A 
possible error for step 1 (Create New Launch Configuration) is that the launch configuration is 
deleted by another team. An error that may occur to step 2 (Update Auto Scaling Group) is that 
the  auto  scaling  group’s  launch  configuration  is  changed  by  another  team.  A  possible  error 
which occurs in step 3 (Set User Specified Rolling Policy) is that the rolling policy is changed 
by  another  team  immediately.  For  step  4  (Deregister  Old  Instance  from  ELB),  an  error  that 
could occur is that the old instance fails to be deregistered from ELB or the deregistering time is 
too long. For step 5 (Terminate Old Instance), a possible error is that the old instance cannot be 
terminated or the instance termination time is too long. An error that may occur to step 6 (Wait 
for  ASG  to  Start  New  Instance)  is  that  new  instance  fails  to  be  launched,  or  the  instance 
launching time is too long, or the new instance cannot go into running state. For step 7 (Register 
New  Instance  with  ELB),  a  possible  error  in  this  step  is  that  the  new  instance  cannot  be 
registered into the load balancer or the registration time is too long.   
45 
Asgard  has  its  own  built-in  error  handling  mechanism  for  handling  errors  in  rolling  upgrade. 
However,  the  error handling  mechanism  provided  by Asgard  can  only  handle  limited  types  of 
errors  and  failures  in  rolling  upgrade  and  it  is  unable  to  recover  from  errors  and  failures  in  a 
fine-grained manner (partially because the Asgard error handling mechanism is not designed for 
the  purpose  of  recovery)  (Asgard,  2013;  M.  Fu,  2014).  Moreover,  for  certain  errors  the  error 
handling  mechanism  in  Asgard  just  asks  the  system  operator  to  stop  the  current  operation, 
gracefully  exist  from  the  operation  and  start  over  the  whole operation  again.  The  way  of  how 
Asgard’s  built-in  error  handling  mechanism  handles  errors  and  failures  is  provided  in  below 
Table  4  (Asgard,  2013).  For  step  1  which  is  “Create  New  Launch  Configuration”,  Asgard’s 
error  handling  mechanism  notifies  the  operator  by  logging  the  errors.  This  just  takes  a  few 
milliseconds. The errors are actually not handled in a fine-grained manner because they are only 
detected and acknowledged without being recovered from in an automated way. If errors occur 
after  this  step,  one  feasible  way  is  to  gracefully  exit  and  start  over  the  whole  rolling  upgrade 
(Asgard,  2013).  For  step  2  which  is  “Update  Auto  Scaling  Group”,  Asgard’s  error  handling 
mechanism is unable to handle its errors. If errors occur in this step, one way is to start over the 
whole rolling upgrade (Asgard, 2013). For step 3 which is “Set User-Specified Rolling Policy”, 
Asgard’s  error  handling  mechanism  is  unable  to  handle  its  errors.  If  errors  occur  in  this  step, 
one  way  is  to  start  over  the  whole  rolling  upgrade  (Asgard,  2013).  For  step  4  which  is 
“Deregister Old Instance from ELB”, Asgard’s error handing mechanism is unable to handle its 
errors as well. One way to deal with its errors is to start over the whole rolling upgrade (Asgard, 
2013). For step 5 which is “Terminate Old Instance”, Asgard’s error handling mechanism logs 
the error and waits until the old instances are terminated. This can take hundreds of seconds. For 
step 6 which is “Wait for ASG to Start New Instance”, three types of errors could  occur: new 
instances  fail  to  be  launched  (Err_A),  new  instances  fail  to  go  into  pending  state  (Err_B)  and 
new  instances  fail  to  go  into  running  state  (Err_C).  For  Err_A,  Asgard’s  error  handling 
mechanism logs the error and waits for the instances to start; for Err_B, Asgard’s error handling 
mechanism  logs  the  error  and  waits  for  up  to  70  minutes for  the  instances  to go  into  pending 
state (Asgard, 2013); for Err_C, Asgard’s error handling mechanism logs the error and waits for 
up to 50 minutes for the instances to go into running state (Asgard, 2013). For step 7 which is 
“Register New Instance with ELB”, Asgard’s error handling mechanism takes no action for the 
errors in this step, and one possible way to handle the errors is to start over the whole rolling 
upgrade (Asgard, 2013).  
In  summary,  the  built-in  error  handling  mechanism  provided  by  Asgard  itself  is  unable  to 
recover from different types of operational errors in a fine-grained manner and this fact serves 
as a motivation for my PhD research. 
46 
Table 4.  Asgard’s Built-in Error Handling Mechanism 
Rolling Upgrade Step 
Errors 
Create  New  Launch 
Configuration (Step 1) 
Update  Auto  Scaling 
Group (Step 2) 
Launch configuration 
is  deleted  by  another 
team 
Auto  scaling  group’s 
launch  configuration 
is 
by 
another team 
changed 
Error Occurrence 
Asgard Error 
Rates 
0.6% 
Handling 
Log errors and 
notify operators 
0.6% 
No action 
Set 
User-Specified 
Rolling Policy (Step 3) 
Rolling  policy 
wrongly specified 
is 
0.5% 
No action 
Deregister Old Instance 
from ELB (Step 4) 
Old  instance  fails  to 
be  deregistered  from 
ELB 
Terminate  Old  Instance 
(Step 5) 
Old  instance  cannot 
be terminated 
New instance fails to 
be launched 
Wait  for  ASG  to  Start 
New Instance (Step 6) 
New instance fails to 
become pending state 
New instance fails to 
become running state 
New  instance  cannot 
be 
into 
load balancer 
registered 
Register  New  Instance 
with ELB (Step 7) 
3.2  Non-Intrusive Recovery VS. Intrusive Recovery 
1.5% 
No action  
3.9% 
3.1% 
1.7% 
1.9% 
Log errors and 
wait for instances 
to terminate 
Log errors and 
wait for instances 
to start 
Log errors and 
wait for instances 
to become pending 
Log errors and 
wait for instances 
to become running 
1.5% 
No action 
Three  possibilities  exist  for  implementing  a  recovery  method  within  any  system:  1)  it  can  be 
implemented without any information provided from the system; 2) it can be implemented using 
the information routinely provided by the system; or 3) it can be implemented by modifying the 
source  code  of  the  system  (M.  Fu,  et  al.,  2016).  We  call  the  first  two  options  “non-intrusive 
recovery” since they do not involve modifying any code and the third option “intrusive recovery” 
(M.  Fu,  et  al.,  2016).  Our  proposed  recovery  method  is  non-intrusive.  The  benefits  of  non-
intrusive recovery over intrusive recovery include (M. Fu, et al., 2016):  
1) Non-intrusive recovery can handle errors from different systems in parallel by using a wider 
range of information such as logs from different systems and different monitoring facilities. In 
particular,  for  cases  where  successfully  achieved  conditions  in  previous  steps  are  violated  at 
47 
later steps, non-intrusive recovery is able to handle them. For example, in step 2 of the rolling 
upgrade  operation,  if  the  resource  generated  in  step  1  (Launch  Configuration)  is  erroneous  or 
was modified unexpectedly (e.g., the Launch Configuration was changed by another team) but 
was  initially  provisioned  successfully,  this  error  cannot  be  easily  detected  and  handled  by 
existing intrusive recovery mechanisms such as exception handling (H. Chang, et al., 2013).  
2)  Non-intrusive  recovery  is  independent  of  any  programming  language.  The  logs  used  to 
trigger error detection and recovery can be produced via any programming language.  
3) It can be easily turned off, if needed: since it is non-intrusive, the recovery service can simply 
be shut down.  
4)  It  spans  tools.  Many  sporadic  operations  involve  multiple  tools  used  in  a  tool  chain. 
Implementing an intrusive recovery method across multiple tools, for many of which the source 
code  may  not  be  available,  is  time  consuming  and  error  prone,  even  if  possible.  By  contrast, 
non-intrusive recovery is more convenient and feasible in such situations. 
3.3  Research Goals 
The problem addressed by my PhD research is that sporadic operations on cloud are not highly 
dependable because failures can occur to them, and there lacks a fine-grained recovery method 
for  sporadic  operations  on  cloud  which  is  able  to  recover  from  operational  failures  while 
satisfying all the recovery requirements. Continuous deployments of automated systems cannot 
afford  downtime  and  this  means  that  manual  recovery  from  partial  failures  or  operational 
processes errors is not acceptable (J. Humble and D. Farley, 2010; L. Bass, I. Weber and L. Zhu, 
2015). Meanwhile, based on the discussions on the differences between non-intrusive recovery 
and  intrusive  recovery  in  section  3.2,  the  non-intrusive  recovery  design  pattern  has  more 
benefits than the intrusive recovery design pattern in the context of cloud operational recovery 
(M.  Fu,  et  al.,  2016).  Hence,  the  main  goal  of  our  research  is  to  propose  a  non-intrusive  and 
automated recovery methodology for sporadic operations on cloud. As a fine-grained recovery 
method, it must fulfil a set of recovery requirements (e.g.  satisfying Recovery Time Objective 
(RTO), etc.). However, existing recovery methods for cloud operations are only able to satisfy a 
small subset of those recovery requirements or are even unable to fulfil any of the requirements. 
Hence,  a  sub-goal  of  our  research  is  to  make  sure  that  the  proposed  non-intrusive  recovery 
methodology is able to satisfy all the recovery requirements.  
3.4  Research Questions 
In our research we define three research questions: 
48 
1)  What  are  the  requirements  for  a  non-intrusive  recovery  method  for  sporadic  operations  on 
cloud? 
2) How to make the non-intrusive recovery method able to satisfy all the recovery requirements? 
What should the design of the recovery methodology be? 
3)  How  to  justify  that  the  proposed  recovery  methodology  is  able  to  meet  all  the  recovery 
requirements? 
3.5  Requirements for Non-Intrusive Recovery for Sporadic Operations on Cloud  
In response to research question 1, we derive eight requirements for non-intrusive recovery for 
sporadic operations on cloud, in part from the literature (T. Wood, et al., 2010; J. Simmonds, et 
al., 2010; A. B. Brown, et al., 2004). These recovery requirements are formulated largely based 
on understanding the limitations and gaps of the existing recovery methods for cloud operations, 
and  analysing  the  characteristics  of  recovery  for  sporadic  operations  on  cloud  also  plays  a 
significant  role  in  determining  these  requirements.  The  eight  recovery  requirements  are:  1) 
recovery  should  be  performed  at  runtime;  2)  recovery  should  satisfy  RTO  (Recovery  Time 
Objective);  3)  recovery  should  reduce  negative  impact  on  cloud  system;  4)  recovery  should 
reduce monetary cost; 5) recovery should be able to recover from errors without known causes; 
6) recovery should be able to deal with false positives of error detection; 7) recovery should be 
able  to  recover  for  recovery  Itself;  8)  recovery  should  be  generalizable  to  other  sporadic 
operations on cloud (M. Fu, et al., 2016). These requirements are able to cover all the aims and 
goals  of  performing  operational  recovery  for  cloud  operations,  so  they  are  a  complete  set.  In 
particular,  if  all  of  these  recovery  requirements  can  be  satisfied  at  the  same  time  by  a  certain 
recovery  method,  then  it  is  considered  to  be  fine-grained.  As  such,  all  of  these  recovery 
requirements should be met at the same time. Our recovery framework is designed with the aim 
of satisfying all the eight recovery requirements. We analyse these requirements and determine 
how each functional component in the recovery framework should be designed (in response to 
research question 2). Our experiments are structured in such a way that it can test our recovery 
framework against all of these recovery requirements (in response to research question 3). 
R1: Runtime Recovery 
In  manual  recovery  during  a  sporadic  operation,  operators  are  usually  alerted  through  some 
monitoring system and will then analyse logs and perform diagnostic tests to detect the causes. 
Meanwhile, the operation is likely to be stopped and rolled-back completely (A. C. Huang and 
A.  Fox,  2005).  Hence,  a  requirement  is  that  the  recovery  should  be  performed  during  the 
execution of the operation at runtime (M. Fu, et al., 2016). 
49 
R2: Recovery Satisfying RTO 
RTO (Recovery Time Objective) refers to the goal of limiting the time for a recovery action to 
complete  (T.  Wood,  et  al.,  2010).  For  recovery  of  a  hosted  application  itself  during  normal 
operation, RTO specifies  a bound on how long it should take for an application to come back 
online  after  a  failure  occurs  (T.  Wood,  et  al.,  2010).  Our  definition  here  is  different.  For 
recovery  of  a  sporadic  operation  itself,  RTO  specifies  an  upper  bound  on  how  long  it  should 
take for a failed step to recover to either a previous consistent state or the expected state of the 
current step. Recovery time should not exceed the time boundary specified by RTO. Hence, a 
requirement is that the recovery should satisfy RTO (M. Fu, et al., 2016). 
R3: Reducing Negative Impact on Cloud System 
Some  recovery  actions  might  have  negative  impact  (bad  consequence)  on  cloud  system.  For 
example, the recovery action of terminating instances might degrade the cloud system’s service 
performance.  Negative  impact  caused  by  recovery  using  cloud  API  functions  is  unavoidable, 
but  we  can  try  to  make  it  reduced.  Hence,  reducing  negative  impact  on  cloud  incurred  by 
recovery is a requirement (M. Fu, et al., 2015; M. Fu, et al., 2016). 
R4: Reducing Monetary Cost of Recovery 
Some  recovery  actions  need  to  incur  monetary  cost.  For  example,  the  recovery  action  of 
launching  new  instances  incurs  some  monetary  expenses,  depending  on  how  many  new 
instances  are  launched.  Sometimes,  monetary  cost  caused  by  recovery  using  cloud  API 
functions is unavoidable, but we can try to make it reduced. Hence, reducing monetary cost of 
recovery is a requirement (M. Fu, et al., 2015; M. Fu, et al., 2016). 
R5: Recovery from Errors without Known Causes 
Recovery from errors during an operation may need information from both error detection and 
diagnosis.  However,  sometimes  only  the  information from  error  detection  is available. This is 
possible because 1) it may take too long to diagnose the error, or 2) the error was transient and 
there  were  no identifiable  causes.  Hence,  a  requirement  is  that  the  recovery  should  be  able to 
recover from an error without knowing the cause of the error (M. Fu, et al., 2016). 
R6: Dealing with False Positives of Error Detection 
Recovery  is  triggered  after  errors  are  detected  by  the  error  detection  service.  However,  the 
detected  errors  might  not  be  present.  For  example,  during  an  operation  an  instance is stuck  in 
the  boot  phase  and  this  error  is  detected  –  but  just  after  the  error  is  discovered  and  before  a 
recovery action is taken, the instance self-corrects. From the viewpoint of recovery, the detected 
50 
error  thus  is  a  false  positive.  Another  case  is  that  there  is  actually  no  error  occurring  but  the 
error  detection  service  unexpectedly  reports  an  error  detected.  This  is  also  a  false  positive  of 
error detection. So, a requirement is that false positives of error detection should be caught and 
handled by the recovery (M. Fu, et al., 2016). 
R7: Recovering for Recovery Itself 
Our recovery approach relies on a set of cloud API operations. Due to the uncertainty of those 
cloud  API  operations  (Q.  Lu,  et  al.,  2013),  the  recovery  service  itself  could  fail.  When  the 
recovery  service  fails,  the  system  may  go  into  another  erroneous  state.  Hence,  ensuring  the 
recovery  from  the  failures  of  the  recovery  service  itself  is  another  requirement  (M.  Fu,  et  al., 