Parsec [41]
Input
100000000
75000
simsmall
simsmall
simsmall
simsmall
simsmall
simsmall
TABLE II: Summary of the benchmarks evaluated.
detection triggering, we can split streams based on hardware
events such as interrupts, for example (see section IV-G),
simplifying event ordering for the checker cores.
K. Summary
We have discussed the hardware required to parallelise error
detection to a set of small cores. These cores observe the loads
and stores committed by the main core and use them to replay
the instructions executed by the main core. Loads and stores
are split up within a partitioned load-store log, and separated
by register checkpoints, allowing each checker core to work
on a different part of the main core’s execution simultaneously.
A checker core starts execution after either its segment of
the load-store log is ﬁlled, or a timeout value is reached. On
error detection, the fault is reported to the program, which
must then either terminate execution or return the memory
system to a consistent state from which execution can restart.
Our scheme allows error detection with minimal power,
performance and area overheads, by trading off detection
latency for parallelism. The next sections quantify how each
of these are affected by our scheme.
e
c
n
a
m
r
o
f
r
e
P
d
e
s
i
l
a
m
r
o
N
 1.035
 1.03
 1.025
 1.02
 1.015
 1.01
 1.005
 1
h
c
s
k
c
b l a
o l e
s
r a
n
c
a
c
a
d
fl u i d
n i m a t e
w
s
p ti o
a
n
s
fr e
q m i n
e
o
b
y tr a
d
k
c
u
o
n t
f a
e
c
s i m
b it c
a m
s tr e
Fig. 7: Normalised slowdown for each benchmark, at standard
settings (table I).
 0.002
 0.0015
 0.001
 0.0005
y
t
i
s
n
e
D
 0
 0
bitcount
freqmine
stream
fluidanimate
swaptions
bodytrack
facesim
blackscholes
randacc
 1000
 2000
 3000
 4000
 5000
Time (ns)
Fig. 8: Density plot, to show the distribution of error detection
delays, at standard settings (table I).
V. EXPERIMENTAL SETUP
To evaluate the checker core performance required for our
error detection technique, along with the latency between
an error and its detection, we modeled a high performance
system using the gem5 simulator [42] with the ARMv8 64-
bit instruction set and conﬁguration given in table I, similar
to systems validated in previous work [43]. A summary of
the benchmarks we evaluated is given in table II. We used
benchmarks taken mostly from Parsec [41], as a modern
benchmark suite representative of a wide range of workloads.
In addition, we chose RandomAccess and STREAM from the
HPCC benchmark suite [39] and Bitcount from MiBench [40]
to evaluate applications at the extremes of being almost purely
memory bound (both irregular and regular) and almost purely
compute bound, respectively. We choose these benchmarks
to give both a wide-ranging suite of applications, along with
extreme and worst-case behaviour, to analyze the entire range
of performance overheads.
VI. EVALUATION
Figure 7 shows the performance impact of our parallel error
detection with the checker cores running at default settings,
as given in table I. The average slowdown is 1.75%, and
no benchmark slows down by more than 3.4%. Performance
overheads are primarily caused by the time taken to checkpoint
registers at the end of a load-store log segment.
We plot the distribution of delays between loads and stores
being executed and checked, for each benchmark, in ﬁg. 8.
Each resembles a normal distribution, with the benchmarks
featuring more homogeneous workloads (randacc, stream,
344
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:27:31 UTC from IEEE Xplore.  Restrictions apply. 
Checker Core Clock
125MHz
250MHz
500MHz
1GHz
2GHz
 4.5
 4
 3.5
 3
 2.5
 2
 1.5
e
c
n
a
m
r
o
f
r
e
P
d
e
s
i
l
a
m
r
o
N
 1
b l a c k s c h o l e s
r a n d a c c
l u i d a n i m a t e
s w a p t
f
i o n s
f
r e q m i n e
b o d y t
r a c k
t c o u n t
f a c e s i m s t
b i
r e a m
Fig. 9: Normalised slowdown when varying the frequency of
the checker cores.
facesim) most closely matching this. The highest average
delay, of 1550ns, comes from randacc. This application is
highly memory bound, with little temporal or spatial locality,
resulting in a low IPC. This means that each load-store log
segment takes a long time to ﬁll, and the 5,000 instruction
window timeout is lengthy compared to other benchmarks.
Each distribution features a long, but very thin, tail at the
far right of the distribution: the maximum detection delay is
signiﬁcantly higher for every benchmark, at an average of
21.5μs. These points are not shown on the distribution plot,
as they are too uncommon: for all benchmarks, 5000ns is
sufﬁcient to cover over 99.9% of all loads and stores.
For automotive applications, the faults we wish to avoid
are based on physical motions. These occur on the timescale
of milliseconds to seconds, so both the maximum and mean
delays introduced by our scheme are acceptable. Similarly, for
HPC workloads, checkpoints are performed at a frequency of
no more than several minutes [5], [44], so delays introduced
by our scheme are insigniﬁcant. As sections VI-B and VI-C
show, overheads compared with dual-core lockstep, which we
intend to replace, are greatly reduced.
A. Parameter Sensitivity
Our default conﬁguration of the checker cores and load-store
log prevents the majority of slowdowns, reﬂects a sensible
trade-off in terms of performance and delay, and enables
performance scaling across a number of checker cores. We
evaluate these claims in the following sections.
Clock Frequency
Figure 9 shows the performance impact
of our scheme when varying the clock speed of the checker
cores, compared to the default 1GHz. Since there are no data
cache misses for the checker cores, because all loads and stores
are accessed and checked from the load-store log, benchmarks
which are memory bound, such as randacc and stream, do not
experience signiﬁcant performance losses, even at very low
frequencies. However, others that are more compute bound,
for example swaptions and bitcount, slow down signiﬁcantly,
particularly at clock speeds lower than 500MHz, because the
checker cores combined do not have enough compute power
to keep up with the main core. In this situation, the main core
spends a signiﬁcant amount of time stalled and waiting for
load-store log space.
345
Log Size / Instruction Timeout
3.6KiB, 500
(default) 36KiB, 5000
360KiB, 50000
360KiB, ∞
 1.16
 1.14
 1.12
 1.1
 1.08
 1.06
 1.04
 1.02
 1
 0.98
o l e
c
h
s
n
a
r
c
c
a
d
fl u i d
n i m a t e
s w a
a
n
p ti o
s
f r
e
q m i n
e
b
o
k
c
a
y t r
d
b it c
n t
u
o
e
c
f a
s i m
a m
e
s t r
e
c
n
a
m
r
o
f
r
e
P
d
e
s
i
l
a
m
r
o
N
s
k
c
b l a
Fig. 10: Slowdown to the system from just checkpointing,
without any checker core execution, across queue sizes and
instruction timeouts.
Register Checkpoint Overhead
Even without the main
core stalling from waiting for a free load-store log segment,
our scheme still
incurs some performance overhead from
register checkpoint latency at the end of a segment. We assume
a 16 cycle pause in commit when this occurs, allowing two-
ported register ﬁles to copy 32 registers from each ﬁle.