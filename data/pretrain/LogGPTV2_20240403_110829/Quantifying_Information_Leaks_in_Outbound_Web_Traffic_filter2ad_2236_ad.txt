51,406 / 10.2%  
74,927 / 14.9% 
97,965 / 15.8% 
224,663 / 36.2% 
10,182 / 12.5% 
5,534 / 6.82% 
Precise bytes/%  Avg. Req. Size 
14.5 bytes 
10.9 bytes 
4.0 bytes 
6.0 bytes 
7.8 bytes 
1.9 bytes 
13,258 / 1.12% 
12,805 / 0.91% 
6,157 / 0.67% 
3,279 / 0.65% 
3,964 / 0.64%  
262 / 0.32% 
Table 1. Bandwidth measurement results for six web browsing scenarios using four different measurement techniques, 
 along with the average bytes/request for the precise technique. 
half. Using a variable-length encoding scheme with the first 
2–6  bits  indicating  the  length,  we  can  count  the  timing 
information  in  each  request  as  follows  (assuming  16 
intervals per second): 
•  Last request <= 3 seconds: 6 bits 
•  Last request <= ~100 seconds: 11 + 2 length bits 
•  Last request <= ~50 Minutes: 16 + 4 length bits 
•  Last request in past 5 years: 32 + 6 length bits 
This  encoding  provides  a  reasonable  approximation  of 
the  information  content  in  the  timing  of  each  request.  It  is 
important to note that these figures depend on the number of 
timing intervals per second. If an attacker can view messages 
close  to  the  source  network,  then  there  may  be  more  than 
sixteen  intervals  per  second.  On  the  other  hand,  if  a  web 
proxy  is  configured  to  increase  request  jitter,  then  the 
number of viable time intervals per second may be less than 
sixteen.  
In  this  paper,  we  assume  that  HTTP  requests  are  going 
through  a  layer-7  proxy  or  gateway  for  our  timing  channel 
measurements. This means that the only meaningful time is 
at  the  start  of  the  request.  The  timing  of  subsequent  IP 
packets  is  controlled  by  the  proxy,  not  the  client,  under 
normal conditions. We believe the presence of a proxy is a 
reasonable  assumption  for  timing  channel  measurements. 
Organizations  that  care  enough  about  leaks  to  measure 
covert timing channels should already have a web proxy in 
place to mediate outbound information flow (e.g., with data-
loss prevention systems [18, 24]). 
7.  Evaluation 
We  applied  the  leak  measurement  techniques  described 
in  this paper on  web  traffic  from  a  controlled  environment, 
and on real web browsing data. The controlled tests involved 
six  30-minute  browsing  sessions  at  different  types  of 
websites  using  a  single  browser.  The  real  web  traffic  was 
collected  from  ten  different  people  using  a  variety  of 
browsers and operating systems over a 30-day period. Only 
data  from  the  controlled  scenarios  was  used  for developing 
the leak measurement engine. None of the live traffic results 
were used to modify or improve our analysis techniques. We 
compared the results of our precise unconstrained analysis to 
incremental  gzip  compression,  simple  request  analysis,  and 
raw  byte  counts.  The  gzip  tests  involved  measuring  the 
amount of new compressed data for each request when using 
a  gzip  compression  stream  that  has  seen  all  prior  requests 
and responses. The simple analysis is a technique described 
in  prior  research  [3]  that  is  stateless  and  just  throws  out 
expected 
request  headers.  This  section  presents  our 
evaluation results, discusses limitations of our approach, and 
briefly summarizes performance results. 
7.1.  Controlled Tests 
We first evaluated our leak quantification techniques on 
browsing  traffic  from  controlled  scenarios.  The  scenarios 
were  30-minute  browsing  sessions  that  included  web  mail 
(Yahoo),  social  networking  (Facebook),  news  (New  York 
Times), sports (ESPN), shopping (Amazon), and a personal 
blog website. The results are shown in Table 1. The precise 
unconstrained  leak  measurements  for  all  of  the  scenarios 
were  much  smaller  than  the  raw  byte  counts,  ranging  from 
0.32–1.12% of the original size. 
The  results  were  best  for  the  blog  scenario  because  the 
blog website contained only one dynamic link. The analysis 
engine  was  able  find  an  exact  match  for  all  of  the  other 
requests.    Of  the  262  bytes  that  were  present  in  the  blog 
scenario, 118 (45%) of them were from timing information, 
86 (33%) from link selection, 48 (18%) from text entered by 
the user, and 10 (4%) from a Javascript link that contained a 
random  number  to  prevent  caching.  The  blog  scenario 
represents  a  near  ideal  situation  for  our  measurement 
techniques  because  we  were  able  to  find  an  exact  URL 
match for all but one request. The resulting average of a few 
bytes per request serves as a lower bound for standard HTTP 
traffic.  This  traffic  must  at  least  leak  timing  and  link 
selection  information.  One  possible  way  to  reduce  timing 
and 
to  employ  entropy 
normalization  techniques,  such  as  pre-fetching  mandatory 
links with a caching proxy. 
selection 
link 
leakage 
is 
The shopping, news, and web mail scenarios all showed 
similar precise measurement results. Each of these websites 
contained  a  large  number  of  dynamically  constructed  links 
that were processed correctly. However, dynamic links often 
contain  information  from  the  client  computer.  Examples 
136
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:15:14 UTC from IEEE Xplore.  Restrictions apply. 
)
n
=
X
(
P
 0.1
 0.01
 0.001
 0.0001
 1e-005
 1
 10
Precise
Gzip
Simple
Raw
Precise
Gzip
Simple
)
n
=
X
(
P
 1
 0.1
 0.01
 0.001
 100
Bytes
    (a) 
 1000
 0
 20
 40
 60
 80
 100
Percentage of Raw
             (b) 
Figure 4.  (a) The distribution of precise, gzip, simple, and raw request byte counts for real web traffic. 
(b) Distribution of request byte counts as percentage of raw for precise, gzip, and simple algorithms. 
include  the  precise  system  time  at  execution,  browser 
window  dimensions,  and  random  numbers 
to  prevent 
caching. This information must be counted because it cannot 
be determined by looking at previous requests and responses. 
From a hacker’s point of view, these fields would be a good 
place to hide data.  Opaque client-side state information was 
particularly  prevalent 
links  for  advertisements  and 
tracking images on the shopping, news, and web mail sites. 
in 
Precise  unconstrained  bandwidth  measurements  for  the 
social  networking  and  sports  news  scenarios  were  the 
highest.  The  social  networking  website  (Facebook.com) 
relied  heavily  on  Active  Javascript  and  XML  (AJAX) 
requests that constructed link URLs in response to user input. 
Because the analysis engine did not trigger event handlers, it 
was  unable  to  extract  these  links.  The  sports  news  website 
(ESPN.com)  contained  a  number  of  Flash  objects  that 
dynamically  fetched  other  resources  from  the  web.  The 
analysis engine could not discount these links because it did 
not  process  the  plug-in  objects.  In  the  future,  the  engine 
could improve analysis accuracy by obtaining and replaying 
hints  about  input  events  that  trigger  AJAX  requests  and 
dynamic  link  URLs  from  agents  running  the  clients.  These 
agents  need  not  be  trusted,  because  incorrect  hints  would 
only increase the unconstrained bandwidth measurement. 
Gzip  compression  [8]  was  more  effective  than  simple 
request analysis  for  all  but one  of  the  controlled  test  cases, 
but  fell  far  short  of  the  compression  level  achieved  by 
precise analysis. By running previous requests and responses 
through  the  compression  stream,  gzip  was  able  to  discount 
84-93% of raw data. URLs and HTTP headers are filled with 
strings  that  appear  elsewhere  in  previous  requests  or 
responses,  giving  gzip  plenty  of  opportunities 
for 
compression. One benefit that gzip actually has over precise 
analysis, which was not enough to make a big difference, is 
that  it  compresses  UI-layer  data.  Our  analysis  engine  will 
count  the  full  size  of  a  blog  comment,  for  example,  while 
gzip  will  compress  the  comment.  Running  unconstrained 
137
bytes  through  an  additional  compression  algorithm  on  the 
back end may help to further improve precise unconstrained 
bandwidth measurements in the future. 
We  did  not  test  generic  compression  algorithms  other 
than  gzip,  but  would  expect  similar  results.  Without 
protocol-specific  processing,  compression  algorithms  are 
limited  in  how  effective  they  can  be  at  discounting 
constrained information. 
7.2.  Quantifying Information in Real Web Traffic 
We collected web traffic from 10 users over the course of 
a  month  to  evaluate  our  leak  measurement  techniques.  
Unlike  the  controlled  scenarios,  this  traffic  came  from  a 
variety  of  web  browsers, 
including  Firefox,  Internet 
Explorer,  Safari,  and  Chrome.  The  traffic  consisted  of 
normal daily activity from the volunteers, who consisted of 
co-workers,  friends,  and  family. The data  included 507,505 
requests  to  7052  unique  hosts  totaling  475  MB.  We  also 
recorded  2.58  GB  of  response  data,  not  including  images, 
videos, or other binary objects. The web mail request bodies 
were  also  ignored  to  protect  privacy.  To  the  best  of  our 
knowledge,  the  collected  web  traffic  did  not  contain  any 
information  leaks  from  spyware  or  unusually  large  uploads 
that would have negatively skewed the results. 
first  computed 
We ran the leak measurement algorithms on the real web 
traffic one  user at  a time (the results do  not  exploit request 
similarities  between  users).  We 
the 
distribution of measured sizes across all requests. Figure 4a 
shows  the  probability  density  function  of  request  sizes  for 
raw,  simple,  gzip,  and  precise  measurements.  The  precise 
unconstrained 
algorithm 
dramatically outperformed the others on real web traffic. The 
mean precise request size was 15.8 bytes, compared to 132 
for  gzip,  243  for  simple,  and  980  for  raw.  Despite  a  low 
average  measurement,  the  precise  request  size  distribution 
exhibited  a  heavy  tail  (standard  deviation  of  287  bytes). 
bandwidth 
measurement 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:15:14 UTC from IEEE Xplore.  Restrictions apply. 
Requests with exact URL matches were usually a few bytes 
each, while many requests without exact URL matches were 
a few hundred bytes. 
We also calculated  the percent reduction  in request  size 
with respect to raw measurements. These results can be seen 
in  Figure  4b.  Again,  the  reduction  is  much  better  for  the 
precise  algorithm.  Its  measurements  averaged  1.48%  of  the 
corresponding  raw  values,  while  the  gzip  and  simple 
algorithms  averaged  9.87%  and  13.5%,  respectively.  The 
request  measurements  for  the precise  algorithm  also  have a 
lower variance, with almost all under 20% of corresponding 
raw  values.  The  simple  and  gzip  size  reductions  are  much 
more  spread  out,  with  some  requests  measuring  20-75%  of 
the raw size. These requests did not benefit much from gzip 
or simple analysis. 
The  unconstrained bandwidth  measurement  results  from 
real  traffic  yielded  larger  values  than  those  from  the 
controlled  test  cases.    The  largest  average  request  size  of 
14.5 bytes from the sports news test was less than the overall 
average  of  15.8  bytes  per  request  for  real  web  traffic.  One 
reason for this is that the controlled tests were not necessarily 
representative of real web browsing. Other sites that were not 
in the controlled study may not have exhibited the same mix 
of  requests  from  plug-ins  or  event  handlers.  We  did  not 
compute the prevalence of this source of inaccuracy, because 
doing  so  would  have  required  manually  analyzing  a 
significant portion of the half million requests. 
During  real  web  traffic  processing,  we  witnessed  a  few 
sources of inaccuracy that were not present in the controlled 
test  cases.  One  such  issue  is missing  cache  objects.  Clients 
may  cache  resources  from  the  server  for  long  periods  of 
time, making those resources unavailable in a network trace. 
This  is  especially  problematic  for  missing  scripts  that 
perform important tasks. The effects of this problem could be 
reduced by having the analysis engine fetch missing objects 
from  the  web.  However,  those  objects  may  no  longer  be 
available or might have changed since the original request. 
Another source of error only found in real web traffic is 
the effect of different browser versions. The controlled tests 
were  all performed  with  Mozilla  Firefox [14]. The analysis 
engine’s Javascript and DOM implementation also mirrored 
Firefox.  Real  web  traffic  from  other  browsers  is  likely  to 
have  different  dynamic  links  corresponding  to  different 
browser  behavior.  These  differences  could  be  reduced  by 
implementing  other  DOM  interfaces  to  match  the  browser 
version reported in the headers of each request. 
7.3.  Analysis Performance 
The real web traffic was analyzed on a commodity laptop 
computer with a dual-core Intel T2500 processor and 2 GB 
of  RAM.  The  analysis  algorithms  ran  in  a  single  thread  on 
one core, with the other core being utilized by the operating 
system. The analysis engine was able process the combined 