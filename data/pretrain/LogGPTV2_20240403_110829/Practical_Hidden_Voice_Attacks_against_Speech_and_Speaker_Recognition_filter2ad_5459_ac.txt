little distortion will increase audio decipherability by a human
observer.
2) Improved Attack Method: VPSes recognize some words
better than others. At times, this property is intentionally built
into models. For example, home assistants are speciﬁcally
trained to be sensitive to activation phrases. Therefore, some
words can be degraded more than others and still be recognized
by a VPS.
6
Fig. 4: The audio recording of the word “pay” before and after
it has been distorted by our perturbation techniques.
The attacker can exploit this model bias towards certain
words and phrases by running the PE at a ﬁner granularity, for
each word rather than the entire sentence. For example, rather
than perturbing the entire phrase “pay money”, the words
“pay” and “money” can be perturbed individually as shown in
Figure 4. Next, the perturbed audio samples for each word are
concatenated to create all the possible combinations, resulting
in a much larger set of attack audio samples.
E. Over-the-Line and Over-the-Air
We deﬁne two scenarios to test the perturbation attacks
against models: Over-the-Line and Over-the-Air. In an Over-
the-Line attack the attack audio is passed to the model directly,
as a .wav ﬁle. Conversely, an Over-the-Air attack requires the
adversary to play the attack audio via a speaker towards the
target VPS. It is not necessarily the case that an attack audio
that is successful Over-the-Line will also be successful Over-
the-Air. Playing the attack audio via a speaker will degrade
the attack audio’s ﬁne perturbations due to natural factors
that
include acoustic hardware’s frequency response, static
interference, and environmental noise. For a practical attack,
the attack audio should be resistant to these factors.
This limitation is evident
in all state-of-the-art attacks
against VPSes. While Carlini et al.’s [24] attack audio cannot
be used over any speaker systems other than the one used
during the attack generation process, Yuan et al.’s [72] attack
method assumes knowledge and access to the attack speaker
and the victim’s acoustic hardware. Although both cases
present strong threat models in the real world, our attacks have
no such limitations, as we will demonstrate.
Perturbation  Engine . . . Multiple Attack Audio SamplesTranscribe AudioInputPerturbAudioParametersAudioF. Models
Our proposed attack is aimed at the state of the art in
publicly available VPS implementations. We test our attack
against the VPSes listed in Table I. Our selection captures
a wide range of known architectures, black-box APIs, and
feature extraction techniques. We give a brief overview of the
different categories these VPSes reside in and why they were
chosen. Note that although the architecture of some systems
are known, only acoustic properties are used to construct
attack audio rather than any available information about the
underlying model.
1) Traditional Models: We treat Kaldi’s Hidden Markov
Model-Gaussian Mixture Model (HMM-GMM) combination
as an example of a traditional speech transcription model.
Kaldi is well known due to its open-source nature, reasonable
performance, and ﬂexible conﬁgurations. An HMM is a multi-
state statistical model which can serve as a temporal model
in the context of speech transcription. Each state in the HMM
holds a unique GMM, which takes MFCC-based features as
input, and models phonemes in the context of other phonemes.
The HMM models state transitions and can account for differ-
ent clusters of temporal phoneme information to produce an
alignment between speech and text. HMM-based approaches
are relevant because modern architectures mimic their multi-
state behavior. We refer to [60] for a general primer on Hidden
Markov Models.
2) Modern Models: More recently, speech recognition and
transcription tasks have begun to employ some combination
of Deep Neural Networks (DNNs) and Recurrent Neural
Networks (RNNs). These architectures are popular due to
their performance and ﬂexibility in complex automation tasks.
Neural networks consist of one or more hidden layers of
stateful neurons sandwiched between an input layer and an
output layer. Layers are connected together and act as inputs
into subsequent layers, such that connections are activated
according to certain inputs. DNNs are neural networks with
more hidden layers and carefully designed deep architectures.
Convolutional Neural Networks (CNNs) are a popular DNN
architecture due to their performance in image classiﬁcation
tasks, which derives from its ability to automatically per-
form feature extraction using convolution ﬁlters. RNNs differ
slightly from DNNs, and can be thought of as a chain of
independent neural networks, such that hidden layers of one
neural network connect to the hidden layers of a subsequent
network. However, a basic RNN is susceptible to the problems
of vanishing or exploding gradients [56]. Thus, RNNs are often
implemented with special neuronal activation functions, such
as Long Short Term Memory (LSTM) or Gated Recurrent
Units (GRU). In either case, the inputs and outputs of an RNN
may form sequences, making them ideal for many sequence-
to-sequence tasks. These include language modeling and audio
transcription, which are key components of modern ASRs.
The selected neural network models differ in terms of
feature extraction. Kaldi-DNN and Intel Neon DeepSpeech
use MFCC-based features as input to the model. Until re-
cently, MFCC-based features have been the most powerful
feature extraction technique for VPS-related tasks. However,
DeepSpeech-style models rely on an approach known as end-
to-end learning. In this system, both feature extraction and
inference are performed by the neural network model. In the
Identiﬁcation
ID Model
A
B
C
D
E
F
G
H
I
J
K
L
ASR
Phrase
When suitably lighted
Don’t ask me to carry
an oily rag like that
What would it look like
My name is unknown
to you
Pay money
Run browser
Open the door
Turn on the computer
Spinning indeed
Very Well
The university
Now to bed boy
Success Rate (%)
100
100
100
100
100
100
100
100
100
100
100
100
TABLE II: We used sentences from the TIMIT corpus [31],
which provides phonetically balanced sentences used widely
in the audio testing community.
the decoding stage, an optional
case of Mozilla DeepSpeech, the RNN is trained to translate
raw audio spectrograms into word transcriptions. The goal of
end-to-end audio transcription models is to learn a new feature
space that will be more performant than traditional MFCC-
based encodings. This feature space is used as input into the
model’s RNN, which performs inference based on the encoded
data. At
language model
may be used to improve transcription performance. Thus,
neural networks are used between each end of the system. In
practice, this system exhibits state-of-the-art results for speech
transcription metrics, rivalling complex HMM-GMM methods.
3) Online APIs: We validate our attack against several
Internet-hosted models. The architecture of these models is
unknown, but we assume them to be near state-of-the-art
due to their popularity and widespread use. Of these models,
two are dedicated to speaker classiﬁcation tasks: the Azure
Attestation API, is meant to classify a speaker among some
known group, while the Azure Veriﬁcation API is meant to
perform attestation based on a user’s voice. All other models
are designed for speech transcription tasks.
V. EXPERIMENTAL SETUP
A. Phrase Selection
The full list of phrases used for the experiments is shown
in Table II. For attacks against Speaker Identiﬁcation models,
we chose phrases A, B, C and D. Phrases A, B and C are
phonetically balanced sentences from the TIMIT dataset [31].
The phrases E, F, G, and H are command phrases used for
attacks against ASRs. These represent commands an adversary
would potentially want to use. Because these phrases were
not interpretable by the Intel DeepSpeech model, we replaced
them with phrases I, J, K and L. These additional phrases
were collected from the LibriSpeech Dataset that the model
was trained on [54]. We did not include any activation phrases
such as “Ok Google”, as every major VPS is tuned to detect its
own activation phrase. This would mean we could perturb the
activation phrase more than others, leading to biased results.
B. Speech Transcription
We tested our audio perturbation attack methodology
against ten ASRs shown in Table I. If our hypothesis is correct,
the attack audio should work against any ASR, regardless of
the underlying model type or feature extraction algorithm.
7
We ran the generic attack method (described in Sec-
tion IV-D1), against proprietary models that include 7 propri-
etary (e.g., Google Speech API, Bing Speech API, IBM Speech
API, Azure API etc) [13]. These models were hosted on-line
and could only be queried a limited number of times. We
ran the improved attack method (described in Section IV-D2),
against locally hosted models, which could be queried without
limits.
1) Over-the-Line: For each phrase, we generated multiple
attack audio samples using various perturbation parameters.
These were then passed to the model as .wav ﬁles for
transcription. We then picked the single worst sounding au-
dio as the ﬁnal attack audio, based on criteria we describe
Section VII. These steps were repeated for each model. At the
end of the process, for each of the 10 models, we had one
attack audio ﬁle for each phrase referenced in Table I.
2) Over-the-Air: We ran the attack audio samples in Over-
the-Air as described in Section IV-E. Of the ten ASRs,
we tested seven Over-the-Air. A single Audioengine A5
speaker [2] was placed on a conference room table to play the
attack audio. A Behringer microphone [5] was placed one foot
away to simulate an ASR device. The recorded audio was then
passed to the ASR for transcription as a .wav ﬁle. For each
model, we played the four attack audio samples, one for each
phrase. The Mozilla DeepSpeech model was tested extensively
by playing 15 randomly sampled attack audio ﬁles. This was
done to ascertain whether audio samples with generated with
larger window sizes to the PE still worked Over-the-Air. This
was done as a baseline test to ensure that the attack algorithm
did not inherently degrade the audio such that the attack audio
ceased to remain effective after it was played Over-the-Air.
To show that the attack was independent of the acoustic
hardware, we repeated the Over-the-Air experiment for the
Google Client Speech API. Additionally, we degraded the
acoustic environment with a white-noise generator playing at
55dB in the background. We replaced the previous speaker
with an iMac and the microphone with the Motorola Nexus 6.
The experiment occurred in a lab cubicle. This experimental
setup represents harsh, yet realistic, acoustic conditions that an
attacker faces in the modern world.
C. Speaker Veriﬁcation and Attestation
Home assistant systems have begun to introduce voice bio-
metrics to authenticate users to prevent VPSes from accepting
audio commands from unauthorized individuals. Each audio
command is checked against a voice blueprint of the real
user to authenticate the source before it is carried out. This
rudimentary biometrics authentication poses an obstacle which
state-of-the-art audio obfuscation attacks can not overcome.
However, our obfuscation technique is designed to retain
the necessary voice information to pass the authentication
tests. To test this hypothesis, we ran our audio obfuscation
attack against two speaker recognition systems. We speciﬁcally
attacked the Azure Identiﬁcation/Attestation (SA) model and
the Azure Veriﬁcation (SV) model.
SA models are trained to identify the voices of multiple
learns to
speakers. For example, during training a model
identify the voices of Alice and Bob. During testing,
the
model infers which speaker a test audio sample belongs to.
Additionally, SA is text-independent. This means the audio
sample can contain the voice recording of the speaker reading
any text.
In contrast, SV models are trained to identify a single
speaker (i.e., just Bob). During testing, the model decides
whether a test audio belongs to Bob or not. Unlike SA, SV
is text-dependent as the test audio must contain the speaker
reading a phrase requested by the model. The model ﬁrst
checks if the audio transcribes to the requested phrase. Only
then does it check whether the voice blueprint matches that of
Bob.
To attack an SA model, we ﬁrst trained the Azure Iden-
tiﬁcation model using the phrases from the TIMIT dataset.
We trained the model on the voices of eight male and eight
female speakers. Three male and three female speakers were
randomly selected for the attack. For each speaker, we ran
the perturbation scheme using the Generic Attack Method
(described in Section
IV-D1) to generate 10 attack audio
samples as .wav.
To attack an SV model, we trained the Azure Veriﬁcation
model. As the SV model is text-dependent, the TIMIT dataset
was insufﬁcient, as it does not contain any of the phrases that
the Azure SV model requests. We gathered the audio data from
three men and three women. We recorded four audio samples
of each phrase per person: three samples to train the model
and one sample to test. After training the model, we checked
if the test audio authenticated correctly. The test audio was
then perturbed using the Generic Attack Method and passed
to the authentication model as .wav ﬁles. We repeated these
steps individually for all six participants.
VI. EXPERIMENTAL RESULTS
The Perturbation Engine takes as input a parameter set
and an audio sample. It then generates attack audio samples
according to the techniques outlined in Section IV-C. To
demonstrate its effectiveness, we ﬁrst test the Over-the-Line
attack by providing the attack audio as a .wav ﬁle input to
the VPS and the Over-the-Air attack by playing the audio via
a commodity speaker.
A. Over-the-Line
1) ASR Models: We test our attack against ASR models. As
noted previously, perturbations introduce acoustic artifacts that
make the original message difﬁcult for humans to understand.
However, the attack audio must still transcribe correctly when