title:Backdoors: Definition, Deniability and Detection
author:Sam L. Thomas and
Aur&apos;elien Francillon
Backdoors: Deﬁnition, Deniability and
Detection
Sam L. Thomas1,2(B) and Aur´elien Francillon3
1 Univ Rennes, CNRS, IRISA, Rennes, France
m+PI:EMAIL
2 University of Birmingham, Birmingham, UK
3 EURECOM, Biot, France
PI:EMAIL
Abstract. Detecting backdoors is a diﬃcult task; automating that
detection process is equally challenging. Evidence for these claims lie in
both the lack of automated tooling, and the fact that the vast majority
of real-world backdoors are still detected by labourious manual analysis.
The term backdoor, casually used in both the literature and the media,
does not have a concrete or rigorous deﬁnition. In this work we pro-
vide such a deﬁnition. Further, we present a framework for reasoning
about backdoors through four key components, which allows them to be
modelled succinctly and provides a means of rigorously deﬁning the pro-
cess of their detection. Moreover, we introduce the notion of deniability
in regard to backdoor implementations which permits reasoning about
the attribution and accountability of backdoor implementers. We show
our framework is able to model eleven, diverse, real-world backdoors,
and one, more complex backdoor from the literature, and, in doing so,
provides a means to reason about how they can be detected and their
deniability. Further, we demonstrate how our framework can be used to
decompose backdoor detection methodologies, which serves as a basis
for developing future backdoor detection tools, and shows how current
state-of-the-art approaches consider neither a sound nor complete model.
Keywords: Backdoors · Formalisation of deﬁnitions
Program analysis
1 Introduction
The potential presence of backdoors is a major problem in deploying software
and hardware from third-parties. Recent studies and research has shown that
not only powerful adversaries [3], but consumer device manufacturers [2,5] have
inserted deliberate ﬂaws in systems that act as backdoors for attackers with
knowledge of those ﬂaws. Unlike the exploitation of traditional vulnerabilities
whereby a weird, unintended program state is reached, backdoors also manifest
This article is based upon work supported by COST Action IC1403 (CRYPTACUS).
c(cid:2) Springer Nature Switzerland AG 2018
M. Bailey et al. (Eds.): RAID 2018, LNCS 11050, pp. 92–113, 2018.
https://doi.org/10.1007/978-3-030-00470-5_5
Backdoors: Deﬁnition, Deniability and Detection
93
as explicit, intentional, essentially normal program functionality – making their
detection signiﬁcantly more challenging.
Many backdoors are considered by their manufacturers to be accidental, left-
over “debug” functionality, or ways to implement software conﬁguration updates
without explicit user authorisation [5]. In other cases, device manufacturers
deploy ﬁrmware coupled with third-party software that introduces backdoor
functionality into their otherwise backdoor free systems without their knowl-
edge [9].
The term “backdoor” is generally understood as something that intention-
ally compromises a platform, aside from this, however, there has been little eﬀort
to give a deﬁnition that is more rigorous. To give such a deﬁnition is diﬃcult
as backdoors can take many forms, and can compromise a platform by almost
any means; e.g., a hardware component, a dedicated program or a malicious
program fragment. This lack of a rigorous deﬁnition prohibits reasoning about
backdoors in a generalised way that is a premise to developing methods to detect
them. Further hampering that reasoning – especially in the case of backdoors
of a more complex, or esoteric nature – is the sheer lack of real-world samples.
Documented real-world backdoors are generally simplistic, where their trigger
conditions rely upon a user inputting certain static data, e.g., hard-coded cre-
dentials. Such backdoors have been studied in the literature with various tools
providing solutions relying on varying degrees of user interaction [19,21].
2 Overview
This work provides ﬁrst and foremost a much needed rigorous deﬁnition of the
term backdoor: which we view as an intentional construct inserted into a sys-
tem, known to the system’s implementer, unknown to its end-user, that serves to
compromise its perceived security. We propose a framework to decompose and
componentise the abstract notion of such a backdoor, which serves as a means to
both identify backdoor-like constructs, and reason about their detection. While
the primary focus of this work is software-based backdoors, by modelling a back-
door abstractly, our framework is able to handle all types of backdoor-like con-
structs, irrespective of their implementation target.
Many backdoors found in the real-world fall into a grey area as to whom is
accountable for their presence; to address this, we deﬁne the notion of deniabil-
ity. We model deniability by considering diﬀerent views of a system: that of the
implementer, the actual system, and the end-user; this allows us to – depend-
ing on where backdoor-like functionality has been introduced – reason about if
that functionality is a deniable backdoor, accidental vulnerability, or intentional
backdoor. In many cases, attempting to model this intention, or the lack thereof,
is something that is social or political, thus we do not address such cases in this
work, instead we focus on the technical aspects of a backdoor-like functionality.
We show that under our deﬁnitions, many backdoors publicly identiﬁed are
not deniable and thus, their manufacturers should be held accountable for their
presence. Aside from manual analysis, little work has been performed to address
94
S. L. Thomas and A. Francillon
the detection of backdoors. We perform a study of both academic and real-world
backdoors and consider existing methods that can be used to locate backdoor
components, as well as how those methods can be improved.
2.1 Contributions
To summarise, the contributions of this work are as follows:
1. We provide a rigorous deﬁnition for the term backdoor and the process of
backdoor detection.
2. We provide a framework for decomposition of backdoor-like functionality,
which serves as a basis for their identiﬁcation, and reasoning about their
detection.
3. We express the notion of deniable backdoors by considering diﬀerent views of
a system: the developer’s perspective, the actual system, the end-user, and a
user analysing the system.
4. We show examples of both academic and real-world backdoors expressed in
terms of our deﬁnitions and reason about their deniability and detectability.
5. We demonstrate how our framework can be used to reason about backdoor
detection methodologies, which we use to show that current state-of-the-art
tools do not consider a complete model of what a backdoor is, and as a result,
we are able identify limitations in their respective approaches.
2.2 Related Work
Coverage of complex backdoors is scarce in the academic literature. Tan et
al. [20] encode backdoor code fragments using specially-crafted interrupt han-
dlers, which, when triggered, manipulate run-time state, and when chained
together, can perform arbitrary computations in a stealthy manner. Andriesse
et al. [14] use a cleverly disguised memory corruption bug to act as a backdoor
trigger and embed misaligned code sequences into the target executable to act as
a payload. Zaddach et al. [24] describe the design and implementation of a hard-
drive ﬁrmware backdoor, which enables surreptitious recovery of data written to
the disk. More complex backdoors have been documented outside of the litera-
ture, e.g., those classiﬁed as “NOBUS” (i.e., NObody But US) vulnerabilites by
the NSA [7], and those associated with APT actors (e.g., [8]).
A related area, that of so-called weird machines, describes how an alterna-
tive programming model that facilitates latent computation can arise within a
program, or system. Both [16] and [18] present such models, as well as how nor-
mal systems can be forced to execute programs written in those models. In both
cases, those models provide a means to implement backdoor-like functionality.
Dullien [15] addresses the problem of formalising the term weird machine, the
relationship between exploitation and weird machines, and introduces the con-
cept of provable exploitability. He argues that it is possible to model a program,
or system using a so-called Intended Finite-State Machine (IFSM), and in doing
so, view a piece of software as an emulator for a speciﬁc IFSM. Further, he
Backdoors: Deﬁnition, Deniability and Detection
95
demonstrates that it is possible to create security games to reason about the
security properties of a system by reasoning about it at the level of the states
and transitions of its IFSM. His model serves as inspiration for this work.
Zhang et al. [25] explore the notion of backdoor detection and give a ﬁrst
informal deﬁnition of the term backdoor. They deﬁne a backdoor as “a mech-
anism surreptitiously introduced into a computer system to facilitate unautho-
rised access to the system”, which while largely agreeing with the current usage
of the term, is very high-level and says nothing about the composition of such
constructs. Wysopal et al. [23] propose a taxonomy for backdoors. They state
that there are three major types of backdoor: system backdoors, which involve
either a single dedicated process which compromises a system, cryptographic
backdoors, which compromise cryptographic algorithms, and application back-
doors, which they state are versions of legitimate software modiﬁed to bypass
security mechanisms under certain conditions. The authors also provide strate-
gies for manual detection of speciﬁc types of application backdoor within source
code.
Current (semi-)automated backdoor detection methods rely on detecting spe-
ciﬁc functionality that is associated with triggering backdoor behaviour. Firmal-
ice [19], is a tool developed to detect backdoors within embedded device ﬁrmware.
The authors propose a model for a class of backdoors they coin authentication
bypass vulnerabilities. They deﬁne the notion of a security policy, which denotes
a state that a binary reaches that signiﬁes it is in a privileged state. Firmal-
ice detects if it is possible to violate that security policy (i.e., ﬁnd a path to
a privileged state, without passing standard authentication). HumIDIFy [22]
uses a combination of machine learning and targeted static analysis to identify
anomalous and unexpected behaviour in services commonly found in Linux-
based embedded device ﬁrmware. Meanwhile, Stringer [21] attempts to locate
comparisons with static data that lead to unique program functionality; that is,
functionality that can only be executed by a successful comparison with that
static data. This models the situation of a backdoor trigger providing access to
undocumented functionality. Schuster et al. [17] address the problem of back-
door detection in binaries through the use of dynamic analysis. Using a proto-
type implementation of their approach, they are able to identify a number of
“artiﬁcial” and previously identiﬁed backdoors.
3 Nomenclature and Preliminaries
In this section we outline terms used in the remainder of this article. A platform
represents the highest level of abstraction of a device that a given backdoor
targets. We deﬁne a system as the highest level of abstraction required to model
a given backdoor, within a platform. Since a backdoor can be implemented at
any level of abstraction of a platform it is designed to compromise – for example,
as a dedicated program, a hardware component, or embedded as part of another
program – we abstract away from such details. To do this, we model an abstract
system as a ﬁnite state machine (FSM).
96
S. L. Thomas and A. Francillon
When considering a backdoor, there are two perspectives to consider a system
from: that of the entity that implements a backdoor, and that of the end-user,
e.g., a general consumer, or a security consultant analysing the platform. To
model this situation, we consider four versions of the FSM; for any given system,
the Developer FSM (DFSM) refers to the developer’s view of the system, the
Actual FSM (AFSM) refers to the FSM that models a real manifestation of
the system, i.e., a program, the Expected FSM (EFSM) refers to the end-user’s
expectations of the system, and ﬁnally, the Reverse-engineered FSM (RFSM),
represents a reﬁnement of the EFSM obtained by reverse-engineering the actual
system; it can include states and transitions not present within the DFSM or
AFSM, e.g., in the case of bug-based backdoors, which we address in Sect. 4.
Fig. 1. Multi-layered system FSM.
Each state of the FSM describing a system can be viewed as an abstraction of
a particular functionality – which, in turn can be modelled using a FSM. Thus,
we view an entire system as a collection of sub-systems, which can be visualised
in a layered manner – with each layer representing a view of a part of the system
at an increasing level detail, as in Fig. 1.
For example, if a given backdoor compromises a router, then we refer to the
router as the platform. If the backdoor is implemented in software, as a dedicated
program, we would view the highest level of abstraction, i.e., the system, as the
interactions between the processes of the operating system, modelled as a FSM.
Each individual running program, or process, can then be modelled by arbitrary
levels of FSMs.
3.1 Analysis and Formalisation of FSMs
We specify a FSM as a quintuple: θ = (S, i, F, Σ, δ), where: S is the set of
its states, i is its initial state, F is the set of its ﬁnal states, Σ is the set of its
state transition conditions, e.g., conditional statements that when satisﬁed cause
transitions, and δ : S × Σ → S is its transition labelling function, representing
its state transitions.
Inspired by the approach taken by Dullien [15], we view the implemented,
or real system modelled by a FSM as an emulator for the AFSM of the system.
Thus, when the user’s EFSM and the AFSM are not equivalent, e.g., the user
assumes there is no backdoor present, when there is, speciﬁc interactions with
Backdoors: Deﬁnition, Deniability and Detection
97
the real system will yield unexpected behaviour. How this unexpected behaviour
manifests is what determines if that unexpected behaviour means that the system
contains a backdoor. Diﬀerent users of the system will assume diﬀerent EFSMs.
In order to analyse a system, a program analyst, for example, will derive a RFSM
– which, for notational ease we refer to as θR – by reverse-engineering the real
system; they do this by making perceptions and observations of its concrete
implementation, i.e., the emulator for θA. What the analyst will observe is a set
of states and state transitions, which are a subset of all those possible within the
platform, e.g., CPU states. To analyse these states and derive θR, the analyst
will require a means to map concrete states and transitions of the platform, to
the level of abstraction modelled by the states and transitions of their FSM. To
perform analysis, we assume that an analyst has the following capabilities:
1. They have access to the emulator for the actual FSM (θA) – in the case of
software, this would be the program binary.
2. They are able to perform static analysis upon the emulator, i.e., using a
tool such as IDA Pro, and hence perceive a set of system states and state
transitions between those states of the real system.
3. They are able to perform dynamic analysis of the system, i.e., with a debugger,
and hence observe a set of system states and transitions of the real system.
The perceptions and observations of the analyst, along with a means to map
concrete states and transitions to abstract FSM states and transitions, allows
them to construct a RFSM (θR) from the emulator for a AFSM. The granularity
of the RFSM will be dependent upon how a system is analysed, e.g., a tool such
as IDA Pro will capture components as groups of basic blocks, while components
identiﬁed in source-code can be represented with a higher-level of abstraction.
3.2 Backdoor Deﬁnition
The implementation strategies of backdoor implementers varies widely, therefore,
we consider the notion of an abstract backdoor, which we decompose into com-
ponents. In order to do this, we attempt to answer a number of questions: what
is it that makes a set of functionalities, when interacting together manifest as a
backdoor? What abstract component parts can be found in all such backdoors?
To what extent do we need to abstract to identify all such components?