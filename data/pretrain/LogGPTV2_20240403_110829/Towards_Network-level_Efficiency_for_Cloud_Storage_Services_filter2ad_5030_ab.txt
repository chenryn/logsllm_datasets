tive) design decisions which the system designers make, such as
the data sync granularity, data compression level, data deduplica-
tion granularity, and so forth.
Both the impact factors and design choices may inﬂuence the
data sync TUE. To avoid being trapped by trivial or elusive issues,
we select key impact factors and design choices according to the
following two rules:
• Rule 1: The impact factors should be relatively constant or
stable, so that our research results can be easily repeated.
• Rule 2: The design choices should be measurable and ser-
vice/implementation independent, so as to make our research
methodology widely applicable.
Following Rule 1, we do not study impact factors such as sync
delay3, cloud server location, etc. For example, we observe that
uploading a 1-MB JPEG photo to Google Drive may incur an elu-
sive sync delay varying between several seconds and several min-
utes (under different network environments). Instead, we choose
to study the sync trafﬁc, which is almost invariable in all cases.
3Sync delay measures how long the user client synchronizes a ﬁle
to the cloud.
Table 1: Key impact factors and design choices.
Client location Client hardware
Client side
Access method
File size
File operation
Data update size Data update rate
Data compression level
Sync deferment
Server side
Data sync granularity
Data deduplication granularity
(Data compression level) *
Network side
* Note: The server-side data compression level may be differ-
Sync trafﬁc Bandwidth
Latency
ent from the client-side data compression level.
Besides, we observe that the cloud server location serving a given
ﬁle is not constant. This is because a cloud storage service usual-
ly hosts a user’s ﬁles across multiple geographically dispersed data
centers, and it often migrates or copies a ﬁle from one cloud serv-
er to another. Instead, we record the bandwidth and delay between
the client and the cloud, as they can be reproduced using client-side
methods (introduced in § 3.2).
Following Rule 2, we do not consider design choices such as the
metadata structures, ﬁle segmentation and replication on the cloud
side, because they require speciﬁc knowledge of the back-end cloud
implementation. For example, the metadata structure (including the
list of the user’s ﬁles, their attributes, and indices to where the ﬁles
can be found inside the cloud) cannot be extracted from the net-
work communication packets, because almost all the commercial
cloud storage services have encrypted their application-layer data
in certain (unknown) ways.
In the end, ten key impact factors and four design choices are
selected, as listed in Table 1. Some of them are self-explanatory or
have been explained before. Below we further explain a few:
• File operation includes ﬁle creation, ﬁle deletion, ﬁle modi-
ﬁcation, and frequent ﬁle modiﬁcations.
• Data update rate denotes how often a ﬁle operation happens.
• Sync deferment. When frequent ﬁle modiﬁcations happen,
some cloud storage services intentionally defer the sync pro-
cess for a certain period of time for batching ﬁle updates.
• Data sync granularity. A ﬁle operation is synchronized to
the cloud either in a full-ﬁle granularity or in an incremen-
tal, chunk-level granularity. When the former is adopted, the
whole updated ﬁle is delivered to the cloud; when the lat-
ter is adopted, only those ﬁle chunks that contain altered bits
(relative to the ﬁle stored in the cloud) are delivered.
• Data deduplication granularity denotes the unit at which da-
ta ﬁngerprints are computed and compared to avoid deliver-
ing duplicate data units to the cloud. The unit can be either
a full ﬁle or a ﬁle block. Note that data deduplication can be
performed across different ﬁles owned by different users.
• Bandwidth is deﬁned as the peak upload rate between the
client and the cloud server. We measure it by uploading a
large ﬁle to the cloud and meanwhile recording the network
trafﬁc with the Wireshark network protocol analyzer [18].
• Latency is deﬁned as the round trip time (RT T ) between the
client and the cloud. We measure it by using the standard
Ping command.
117Table 2: Number of users and ﬁles recorded in our collected cloud storage trace.
Google Drive OneDrive Dropbox
Number of users
Number of ﬁles
33
32677
24
17903
55
106493
Box
13
19995
Ubuntu One
SugarSync
13
27281
15
18283
Table 3: File attributes recorded in our collected trace.
User name
File name MD5 Original ﬁle size
Compressed ﬁle size Creation time
Full-ﬁle MD5
Last modiﬁcation time
128 KB/256 KB/512 KB/1 MB/2 MB/4 MB/
8 MB/16 MB block-level MD5 hash codes
Figure 2: CDF (cumulative distribution function) of 1) original
ﬁle size and 2) compressed ﬁle size, corresponding to our col-
lected trace. For original ﬁles, the maximum size is 2.0 GB, the
average size is 962 KB, and the median size is 7.5 KB. For com-
pressed ﬁles, the maximum size is 1.97 GB, the average size is
732 KB, and the median size is 3.2 KB. Clearly, the tracked ﬁles
can be effectively compressed on the whole, and the majority of
them are small in size.
3. METHODOLOGY
This section describes our methodology for studying the TUE
of cloud storage services. First, we introduce a real-world cloud
storage trace collected to characterize the key impact factors. Next,
we design a variety of benchmark experiments to uncover the key
design choices of data sync mechanisms. Last but not the least, we
provide an overview of the research results.
3.1 Real-world Cloud Storage Trace
Our measurement study takes advantage of a real-world user
trace of cloud storage services.
It is collected in several univer-
sities and companies in the US and China from Jul. 2013 to Mar.
2014, including 153 long-term users with 222,632 ﬁles inside their
sync folders. Refer to Table 2 for the per service statistics. This
trace is used to characterize the key impact factors with regard to
the six widely used services. It is also used to guide the design
of benchmark experiments and enable further macro-level analysis
(in particular, the TUE related optimization opportunities of cloud
storage services).
This cloud storage trace records detailed information of every
tracked ﬁle in multiple aspects. Table 3 lists the concrete ﬁle at-
tributes recorded. Figure 2 depicts the distributions of original ﬁle
size and compressed ﬁle size corresponding to the trace. We have
made this trace publicly available to beneﬁt other researchers. It
can be downloaded via the following link:
http://www.greenorbs.org/people/lzh/public/traces.zip
.
3.2 Benchmark Experiments
To obtain an in-depth understanding of TUE and the key design
choices of data sync mechanisms, we design a variety of bench-
marks for performing comprehensive controlled experiments. The
benchmarks span multiple commercial cloud storage services, in-
volving diverse client machines locating at distinct locations and
network environments.
Cloud storage services.
Among today’s dozens of commer-
cial cloud storage services, our research focuses on the following
six mainstream services: Google Drive, OneDrive, Dropbox, Box,
Ubuntu One, and SugarSync, as they are either the most popular
(in terms of user base) or the most representative (in terms of da-
ta sync mechanism). Other cloud storage services are also brieﬂy
discussed when necessary.
Client locations. Since the above cloud storage services are main-
ly deployed in the US, we select two distinct locations to perfor-
m each experiment: MN (i.e., Minnesota, US) and BJ (i.e., Bei-
In a coarse-grained manner, MN represents a lo-
jing, China).
cation close to the cloud:
the bandwidth is nearly 20 Mbps and
the latency ∈ (42, 77) msec, while BJ represents a location re-
mote from the cloud: the bandwidth is nearly 1.6 Mbps the latency
∈ (200, 480) msec.
Controlled bandwidth and latency. To tune the network environ-
ment in a ﬁne-grained manner, we interpose a pair of packet ﬁlters
in the communication channel between the client and the cloud in
MN. These ﬁlters enable ﬁne-grained adjustment of the bandwidth
(the maximum possible speed is 20 Mbps) and latency in either di-
rection. Speciﬁcally, the packet ﬁlters are interposed by using an
intermediate proxy that runs the Linux Netﬁlter/Iptables tool, thus
behaving like a common software ﬁrewall.
Controlled ﬁle operations. We synthetically generate almost
all kinds of ﬁle operations appearing in the literature. Moreover,
these operations are applied upon both compressed and compress-
ible ﬁles. These controlled ﬁle operations will be elaborated in § 4,
§ 5, and § 6.
Client machine hardware. A total of eight client machines are
employed in the experiments: four in MN (i.e., M1, M2, M3, and
M4) and four in BJ (i.e., B1, B2, B3, and B4). Their detailed hard-
ware information is listed in Table 4. M1/B1 represents a typical
client machine at the moment, M2/B2 an outdated machine, M3/B3
an advanced machine with SSD storage, and M4/B4 an Android s-
martphone. M1–M3 and B1–B3 are installed with Windows 7-SP1
and the Chrome-30.0 web browser.
Benchmark software and access methods.
For each cloud s-
torage service, all the experiments regarding M1–M3 and B1–B3
are performed with the latest version (as of Jan. 2014) of the client
software on Windows 7. For the M4 and B4 smartphones, we ex-
periment with the latest-version Android apps (as of Jan. 2014).
The corresponding sync trafﬁc (i.e., incoming/outgoing packets)
are recorded using Wireshark. For the Android smartphones, we
route the network trafﬁc through a PC that promiscuously monitors
the packets using Wireshark.
 0.82 0.84 0.86 0.88 0.9 0.92 0.94 0.96 0.98 1 0 20 40 60 80 100CDFFile Size (MB)CompressedOriginal118Table 4: Hardware information of the experimental client machines.
CPU
Machine
M1 @ MN Quad-core Intel i5 @ 1.70 GHz
M2 @ MN Intel Atom @ 1.00 GHz
M3 @ MN Quad-core Intel i7 @ 1.90 GHz
M4 @ MN Dual-core ARM @ 1.50 GHz
B1 @ BJ
B2 @ BJ
B3 @ BJ
B4 @ BJ
Quad-core Intel i5 @ 1.70 GHz
Intel Atom @ 1.00 GHz
Quad-core Intel i7 @ 1.90 GHz
Dual-core ARM @ 1.53 GHz
Memory Disk Storage
4 GB
1 GB
4 GB
1 GB
4 GB
1 GB
4 GB
1 GB
7200 RPM, 500 GB
5400 RPM, 320 GB
SSD, 250 GB
MicroSD, 16 GB
7200 RPM, 500 GB
5400 RPM, 250 GB
SSD, 250 GB
MicroSD, 16 GB
Table 5: Our major ﬁndings, their implications, and locations of relevant sections.
Simple File Operations
Implications
Section 4.1 (File creation): The majority (77%) of ﬁles in our trace
are small in size (<100 KB), which may result in poor TUE.
Section 4.2 (File deletion): Deletion of a ﬁle usually incurs negli-
gible sync trafﬁc.
Section 4.3 (File modiﬁcation): The majority (84%) of ﬁles are
modiﬁed by users at least once. Most cloud storage services em-
ploy full-ﬁle sync, while Dropbox and SugarSync utilize incremen-
tal data sync (IDS) to save trafﬁc for PC clients (but not for mobile
or web-based access methods).
Compression and Deduplication
Section 5.1 (Data compression): 52% of ﬁles can be effectively
compressed. However, Google Drive, OneDrive, Box, and Sug-
arSync never compress data, while Dropbox is the only one that
compresses data for every access method.
Section 5.2 (Data deduplication): Although we observe that 18%
of ﬁles can be deduplicated, most cloud storage services do not
support data deduplication, especially for the web-based access
method.
Frequent File Modiﬁcations
Section 6.1 (Sync deferment): Frequent modiﬁcations to a ﬁle of-
ten lead to large TUE. Some services deal with this issue by batch-
ing ﬁle updates using a ﬁxed sync deferment. However, we ﬁnd
that ﬁxed sync deferments are inefﬁcient in some scenarios.
Section 6.2 (Network and hardware): Suprisingly, we observe that
users with relatively low bandwidth, high latency, or slow hardware
save on sync trafﬁc, because their ﬁle updates are naturally batched
together.
3.3 Overview of Our Major Findings
Based on the above methodology, we are able to thoroughly un-
ravel the TUE relevant characteristics, design tradeoffs, and op-
timization opportunities of the six mainstream cloud storage ser-
vices. The detailed research results (from simple to complex) will
be presented in the following three sections: simple ﬁle operations
(§ 4), compression and deduplication (§ 5), and frequent ﬁle modiﬁ-
cations (§ 6). As an overview and a roadmap of our research results,
Table 5 summarizes the major ﬁndings and their implications.
4. SIMPLE FILE OPERATIONS
This section presents our major measurement results, ﬁndings,