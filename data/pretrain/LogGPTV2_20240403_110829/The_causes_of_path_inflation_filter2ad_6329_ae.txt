!!
•
?
?
•
◦
!!
◦
◦
•
◦
◦
x
!
◦
x
◦
!
•
x
!!
◦
•
•
◦
?
x
x
?
x
x
x
x
x
!!
!!
!!
•
•
x
◦
◦
◦
x Engineered, but not “late”
?
!!
!
x
◦
(13%)
Peering in RouteViews but unseen
!!
!!
x
•
•
•
!
!!
!
!
x
x
?
Figure 9: Summary of observed peering relationships between tier-1 ISPs. Both early- and late-exit policies are common, and not
simply the work of a few ISPs. Peering policies vary widely, even between neighbors of the same ISP.
Some ISPs, such as Telia and Global Crossing, appear to use late
exit more often than others.
In all, this ﬁgure demonstrates the
diversity of pairwise peering policies, and that “early-exit” rarely
sums up routing policy between tier-1 ISPs.
We found asymmetry in the peering policies used in general and
associated with the tier of the ISP. Of those ISP pairs we were able
to characterize as early or late in both directions, we found that the
likelihood of a late exit policy is 17%, but the likelihood that an
ISP reciprocates late exit is only 8%. We also found asymmetry in
the relationship between tier-1 and tier-2 ISPs. 50% (82/178) of the
tier-1 to tier-2 peerings showed a late-exit policy compared to only
36% (32/88) of the tier-2 to tier-1 peerings. We found no major
differences in the policies used in different parts of the world.
5.2.1 A Case Study: AT&T and Sprint in San Francisco
While most peerings can be classiﬁed cleanly as early or late,
there are some paths that traverse a peering link that is neither clos-
est to the source nor closer to the destination. We looked in detail
at a particularly pronounced case, between AT&T and Sprint in
San Francisco, California. We discovered that only 38% of 14,649
paths took the early-exit peering in San Francisco. The other 61%
of paths were sent to Seattle, Washington, 1092 kilometers north.
Many paths diverted to Seattle returned to San Francisco, making
“late-exit” an unlikely explanation.
We ﬁrst evaluate how this policy inﬂates path latencies and then
examine a potential explanation. Figure 10 shows the CDF of addi-
tive inﬂation in paths leaving AT&T’s San Francisco POP for des-
tinations in Sprint. The inﬂation is relative to optimal exit paths,
computed as in Section 5.1. We compare the inﬂation in measured
s
h
t
a
p
f
o
n
o
i
t
c
a
r
f
e
v
i
t
a
l
u
m
u
C
1.0
0.8
0.6
0.4
0.2
0.0
early exit
observed
0
5
10
15
Additive inflation (ms)
Figure 10: The impact of AT&T’s unilateral load balancing on
path inﬂation. The graph plots the CDF of additive inﬂation in
early-exit and observed paths when compared to optimal-exit
paths from the San Francisco peering point.
paths to the inﬂation that would result from an early-exit policy.
While the early-exit policy would cause little path inﬂation, the ob-
served routing sees at least 5 ms inﬂation on most paths.
We wondered why AT&T would deviate from simple early-exit
and make paths longer.2 We postulated that AT&T engineers were
diverting trafﬁc away from a heavily loaded peering link and set
out to corroborate this assumption by measuring congestion on the
link. Between January, 16 and 24, 2003, we sent a series of TTL-
limited probes from PlanetLab sites in Seattle and Berkeley to mea-
2We dismissed sinister intentions, such as causing Sprint to carry
the trafﬁc farther in its network.
)
s
m
(
y
a
l
e
d
n
a
i
d
e
M
1.0
0.8
0.6
0.4
0.2
0.0
0
1
2
3
4
5
6
7
8
Days from start of trace
Figure 11: The median delay on the San Francisco peering be-
tween AT&T and Sprint with 4 hour averaging. The vertical
bars represent midnight on each day.
sure the queueing delay at both Seattle and San Francisco peering
links. The probes were sent in triplets, every 50 seconds to avoid
synchronization with a router’s periodic maintenance tasks (usu-
ally 60 seconds [23]). These probes were meant to expire at the
two routers immediately before the link and at the router immedi-
ately after the link. To guard against routing changes, we veriﬁed
that the responses to each probe came from the same router and the
return TTL was consistent. For a given link, variation in RTT to
the remote end without variation in RTT to the near end represents
queuing delay variation and is hence an indication of load.
We observed no queueing delay variation on the links before both
peering links and on the Seattle peering link. The San Francisco
peering link, however, appeared loaded. Figure 11 shows the mea-
sured queueing delay at the link, with each dot representing the me-
dian delay over a four hour period.3 The vertical bars correspond to
midnight, Paciﬁc time, on each day. That the measured queueing
delay decreases almost every night strongly suggests that we are
measuring a quantity dependent on the load at the link. (This varia-
tion implies that ICMP generation delay at the router is not a factor,
consistent with [14].) The third day in the trace was a Saturday; in-
terestingly, the load increased on Saturday and Sunday nights. Our
measurement point failed on the ﬁfth day, removing some Monday
morning data points.
In this paper, we use path inﬂation to infer the presence and char-
acter of engineering in the network. In this case study, we found
that the seemingly abysmal routing policy of sending trafﬁc to Seat-
tle was instead a result of trying to avoid a congested peering link.
Of course, the ideal solution is to add capacity or at least to load
balance trafﬁc in a topology-sensitive manner to prevent inﬂation.
Topology-sensitive load balancing is difﬁcult in BGP, which we
discuss in Section 7.
5.3 Impact of Peering Policies
We next consider the path inﬂation of alternate peering policies.
We focus ﬁrst on early- and late-exit policies, then compare to a
latency-optimized policy.
5.3.1 Early- vs. Late-Exit
We found that there is little difference in latency between the
early and late exit routing strategies. Figure 12 shows the CDF
of inﬂation caused by early- and late-exit policies from Qwest to
Genuity. Qwest implements a late-exit policy towards Genuity, but
in terms of path latency, early would have been just as good. This is
not surprising, because the paths taken by late-exit are usually just
the reverse of paths taken by early-exit. Although these policies
differ for practical purposes, for the rest of this paper, we analyze
peering policies using early-exit as representative of both early and
late exit.
3With an OC-3 link (155 Mbps), a 0.5 ms median queueing delay
implies a median queue size of roughly 20 packets.
s
h
t
a
p
f
o
n
o
i
t
c
a
r
f
e
v
i
t
a
l
u
m
u
C
1.0
0.8
0.6
0.4
0.2
0.0
0
late exit
early exit
5
10
25
Additive inflation (ms)
15
20
Figure 12: The inﬂation caused due to early- and late-exit poli-
cies when going from Qwest to Genuity.
s
h
t
a
p
f
o
n
o
i
t
c
a
r
f
e
v
i
t
a
l
u
m
u
C
1.0
0.8
0.6
0.4
0.2
0.0
0
5
10
15
20
Additive inflation (ms)
95%
mean
median
)
s
m
(
n
o
i
t
a
l
f
n
i
e
v
i
t
i
d
d
A
15
10
5
0
50
200
0
Latency of optimal path (ms)
150
100
Figure 13: The path inﬂation due to early-exit compared to op-
timal exit.
5.3.2 Early- vs. Optimal-Exit
Figure 13 compares the inﬂation caused by using early-exit rout-
ing relative to an ideal, optimal exit policy. ISP pairs with only
one peering point in the traces have been excluded. The left graph
shows the CDF of additive inﬂation for early relative to ideal. Over
30% of the paths experience inﬂation, and the top 5% of the paths
suffer an inﬂation of more than 12 ms. The graph on the right
shows the inﬂation as a function of the latency of the optimal exit
path. It shows that paths between cities close to each other can be
highly inﬂated, compared to the small distance between them. Un-
like previous graphs, the additive inﬂation does not increase with
the latency of the optimal path. At the right end of the graph, many
paths face a choice between two, closely located, trans-continental
links. We did not observe any signiﬁcant dependence of our results
on the size of ISPs.
In summary, our results show that peering policies often cause
path inﬂation, and these policies sometimes lead to highly inﬂated
paths. This means that they consume more network resources,
probably for both ISPs, than an optimal exit policy.
5.3.3 More Peering Points vs. Optimal-Exit
We next consider adding additional peering points between ISPs.
The intent is to determine whether an “optimal exit” policy might
have the same effect as adding peering points. We focus on the
topology of Sprint and AT&T, though similar results were obtained
using other US ISP pairs. We found 11 peering points between
Sprint and AT&T spread all over the country. To compare optimal-
and early-exit path inﬂation on topologies with fewer peering points,
we repeat our inﬂation analysis over topologies with randomly cho-
sen subsets of these 11 peering links, averaging over 10 topologies
for each degree of peering.
Figure 14 shows the results of the experiment. It plots the me-
dian and the 95th percentile of additive inﬂation over the latency
early exit 95%
optimal exit 95%
early exit median
optimal exit median
30
20
10
)
s
m
(
n
o
i
t
a
l
f
n
i
e
v
i
t
i
d
d
A
0
0
5
10
Number of peering points
Figure 14: The variation of additive inﬂation with the number
of peering points between Sprint and AT&T. Additive inﬂation
is measured with respect to the latency of a hypothetical direct
link between the source and the destination cities.
of a hypothetical direct link between the source and the destination
cities. With just two or three peering points, the inﬂation is high,
as all trafﬁc is constrained to traverse these cities. The inﬂation
decreases quickly as the number of peering points increases.
There is a 3 ms gap between the 95th percentiles of optimal- and
early-exit routes. Somewhat surprisingly, the performance differ-
ence between early-exit and optimal-exit peering policies does not
decrease with more peering points. That is, an optimal-exit peering
policy continues to offer lower latency paths. This graph can also
be read horizontally to show that the latency reduction offered by
more peering points can instead be achieved using optimal-exit.
6.
INTER-DOMAIN FACTORS
In this section, we describe the path inﬂation along paths that tra-
verse multiple ISPs. ISPs choose which other ISPs to connect with;
these organizational relationships form the AS-graph that deﬁnes
the paths that exist in the network. Over this topology, ISPs make
routing policy decisions that determine which paths are taken. This
path selection is constrained by business relationships that may for-
bid good paths that exist in the topology. In this section, we want
to determine whether it is the topology, the protocol, or the policies
that create the inter-domain path inﬂation in Figure 1.
To estimate the path inﬂation contributed by inter-domain topol-
ogy and policies, we construct an abstract graph where nodes rep-
resent POPs and edges represent both the early-exit paths between
ISPs from Section 5 and the least-weight paths between POPs in the
same ISP from Section 4. In this section, we compute shortest paths
over this graph, ﬁrst minimizing latency to show the effect of topol-
ogy, then minimizing AS-hops to compare policies. We ensure that
these shortest paths respect intra-domain and peering policies by
using the intra-domain edge only at the end of the path. (Other-
wise, optimal exit could be synthesized by composing intra-domain
edges in the middle of the path.) There are 100,799 edges connect-
ing 2,084 nodes in this graph. In synthesizing paths between POPs
in this section, we do not ﬁlter out node pairs that were not observed
in the traces, which may bias our results towards estimating more
path inﬂation than actually exists. In addition, with more ISPs in
a path, the mean distance between city pairs is increased, implying
that caution is reasonable in comparing the quantitative results in
this section to the results in earlier sections. However, our results
are consistent with previous studies of measured path inﬂation [26,
32].
s
h
t
a
p
f
o