often fruitful. Understanding which metrics are important and which metrics aren’t
important provides us with degrees of freedom when attempting to take thoughtful
risks.
Service latency for our Ads systems provides an illustrative example. When Google
first launched Web Search, one of the service’s key distinguishing features was speed.
When we introduced AdWords, which displays advertisements next to search results,
a key requirement of the system was that the ads should not slow down the search
experience. This requirement has driven the engineering goals in each generation of
AdWords systems and is treated as an invariant.
AdSense, Google’s ads system that serves contextual ads in response to requests from
JavaScript code that publishers insert into their websites, has a very different latency
goal. The latency goal for AdSense is to avoid slowing down the rendering of the
third-party page when inserting contextual ads. The specific latency target, then, is
dependent on the speed at which a given publisher’s page renders. This means that
AdSense ads can generally be served hundreds of milliseconds slower than AdWords
ads.
This looser serving latency requirement has allowed us to make many smart trade-
offs in provisioning (i.e., determining the quantity and locations of serving resources
we use), which save us substantial cost over naive provisioning. In other words, given
the relative insensitivity of the AdSense service to moderate changes in latency per‐
formance, we are able to consolidate serving into fewer geographical locations, reduc‐
ing our operational overhead.
Identifying the Risk Tolerance of Infrastructure Services
The requirements for building and running infrastructure components differ from
the requirements for consumer products in a number of ways. A fundamental differ‐
ence is that, by definition, infrastructure components have multiple clients, often with
varying needs.
Target level of availability
Consider Bigtable [Cha06], a massive-scale distributed storage system for structured
data. Some consumer services serve data directly from Bigtable in the path of a user
request. Such services need low latency and high reliability. Other teams use Bigtable
as a repository for data that they use to perform offline analysis (e.g., MapReduce) on
Risk Tolerance of Services | 31
a regular basis. These teams tend to be more concerned about throughput than relia‐
bility. Risk tolerance for these two use cases is quite distinct.
One approach to meeting the needs of both use cases is to engineer all infrastructure
services to be ultra-reliable. Given the fact that these infrastructure services also tend
to aggregate huge amounts of resources, such an approach is usually far too expensive
in practice. To understand the different needs of the different types of users, you can
look at the desired state of the request queue for each type of Bigtable user.
Types of failures
The low-latency user wants Bigtable’s request queues to be (almost always) empty so
that the system can process each outstanding request immediately upon arrival.
(Indeed, inefficient queuing is often a cause of high tail latency.) The user concerned
with offline analysis is more interested in system throughput, so that user wants
request queues to never be empty. To optimize for throughput, the Bigtable system
should never need to idle while waiting for its next request.
As you can see, success and failure are antithetical for these sets of users. Success for
the low-latency user is failure for the user concerned with offline analysis.
Cost
One way to satisfy these competing constraints in a cost-effective manner is to parti‐
tion the infrastructure and offer it at multiple independent levels of service. In the
Bigtable example, we can build two types of clusters: low-latency clusters and
throughput clusters. The low-latency clusters are designed to be operated and used by
services that need low latency and high reliability. To ensure short queue lengths and
satisfy more stringent client isolation requirements, the Bigtable system can be provi‐
sioned with a substantial amount of slack capacity for reduced contention and
increased redundancy. The throughput clusters, on the other hand, can be provi‐
sioned to run very hot and with less redundancy, optimizing throughput over latency.
In practice, we are able to satisfy these relaxed needs at a much lower cost, perhaps as
little as 10–50% of the cost of a low-latency cluster. Given Bigtable’s massive scale, this
cost savings becomes significant very quickly.
The key strategy with regards to infrastructure is to deliver services with explicitly
delineated levels of service, thus enabling the clients to make the right risk and cost
trade-offs when building their systems. With explicitly delineated levels of service, the
infrastructure providers can effectively externalize the difference in the cost it takes to
provide service at a given level to clients. Exposing cost in this way motivates the cli‐
ents to choose the level of service with the lowest cost that still meets their needs. For
example, Google+ can decide to put data critical to enforcing user privacy in a high-
availability, globally consistent datastore (e.g., a globally replicated SQL-like system
like Spanner [Cor12]), while putting optional data (data that isn’t critical, but that
32 | Chapter 3: Embracing Risk
enhances the user experience) in a cheaper, less reliable, less fresh, and eventually
consistent datastore (e.g., a NoSQL store with best-effort replication like Bigtable).
Note that we can run multiple classes of services using identical hardware and soft‐
ware. We can provide vastly different service guarantees by adjusting a variety of ser‐
vice characteristics, such as the quantities of resources, the degree of redundancy, the
geographical provisioning constraints, and, critically, the infrastructure software
configuration.
Example: Frontend infrastructure
To demonstrate that these risk-tolerance assessment principles do not just apply to
storage infrastructure, let’s look at another large class of service: Google’s frontend
infrastructure. The frontend infrastructure consists of reverse proxy and load balanc‐
ing systems running close to the edge of our network. These are the systems that,
among other things, serve as one endpoint of the connections from end users (e.g.,
terminate TCP from the user’s browser). Given their critical role, we engineer these
systems to deliver an extremely high level of reliability. While consumer services can
often limit the visibility of unreliability in backends, these infrastructure systems are
not so lucky. If a request never makes it to the application service frontend server, it is
lost.
We’ve explored the ways to identify the risk tolerance of both consumer and infra‐
structure services. Now, we’ll discuss using that tolerance level to manage unreliabil‐
ity via error budgets.
Motivation for Error Budgets
1
Written by Mark Roth
Edited by Carmela Quinito
Other chapters in this book discuss how tensions can arise between product develop‐
ment teams and SRE teams, given that they are generally evaluated on different met‐
rics. Product development performance is largely evaluated on product velocity,
which creates an incentive to push new code as quickly as possible. Meanwhile, SRE
performance is (unsurprisingly) evaluated based upon reliability of a service, which
implies an incentive to push back against a high rate of change. Information asymme‐
try between the two teams further amplifies this inherent tension. The product devel‐
opers have more visibility into the time and effort involved in writing and releasing
their code, while the SREs have more visibility into the service’s reliability (and the
state of production in general).
1 An early version of this section appeared as an article in ;login: (August 2015, vol. 40, no. 4).
Motivation for Error Budgets | 33
These tensions often reflect themselves in different opinions about the level of effort
that should be put into engineering practices. The following list presents some typical
tensions:
Software fault tolerance
How hardened do we make the software to unexpected events? Too little, and we
have a brittle, unusable product. Too much, and we have a product no one wants
to use (but that runs very stably).
Testing
Again, not enough testing and you have embarrassing outages, privacy data leaks,
or a number of other press-worthy events. Too much testing, and you might lose
your market.
Push frequency
Every push is risky. How much should we work on reducing that risk, versus
doing other work?
Canary duration and size
It’s a best practice to test a new release on some small subset of a typical work‐
load, a practice often called canarying. How long do we wait, and how big is the
canary?
Usually, preexisting teams have worked out some kind of informal balance between
them as to where the risk/effort boundary lies. Unfortunately, one can rarely prove
that this balance is optimal, rather than just a function of the negotiating skills of the
engineers involved. Nor should such decisions be driven by politics, fear, or hope.
(Indeed, Google SRE’s unofficial motto is “Hope is not a strategy.”) Instead, our goal
is to define an objective metric, agreed upon by both sides, that can be used to guide
the negotiations in a reproducible way. The more data-based the decision can be, the
better.
Forming Your Error Budget
In order to base these decisions on objective data, the two teams jointly define a quar‐
terly error budget based on the service’s service level objective, or SLO (see Chap‐
ter 4). The error budget provides a clear, objective metric that determines how
unreliable the service is allowed to be within a single quarter. This metric removes the
politics from negotiations between the SREs and the product developers when decid‐
ing how much risk to allow.
Our practice is then as follows:
• Product Management defines an SLO, which sets an expectation of how much
uptime the service should have per quarter.
34 | Chapter 3: Embracing Risk
• The actual uptime is measured by a neutral third party: our monitoring system.
• The difference between these two numbers is the “budget” of how much “unreli‐
ability” is remaining for the quarter.
• As long as the uptime measured is above the SLO—in other words, as long as
there is error budget remaining—new releases can be pushed.
For example, imagine that a service’s SLO is to successfully serve 99.999% of all quer‐
ies per quarter. This means that the service’s error budget is a failure rate of 0.001%
for a given quarter. If a problem causes us to fail 0.0002% of the expected queries for
the quarter, the problem spends 20% of the service’s quarterly error budget.
Benefits
The main benefit of an error budget is that it provides a common incentive that
allows both product development and SRE to focus on finding the right balance
between innovation and reliability.
Many products use this control loop to manage release velocity: as long as the sys‐
tem’s SLOs are met, releases can continue. If SLO violations occur frequently enough
to expend the error budget, releases are temporarily halted while additional resources
are invested in system testing and development to make the system more resilient,
improve its performance, and so on. More subtle and effective approaches are avail‐
able than this simple on/off technique:2 for instance, slowing down releases or rolling
them back when the SLO-violation error budget is close to being used up.
For example, if product development wants to skimp on testing or increase push
velocity and SRE is resistant, the error budget guides the decision. When the budget
is large, the product developers can take more risks. When the budget is nearly
drained, the product developers themselves will push for more testing or slower push
velocity, as they don’t want to risk using up the budget and stall their launch. In effect,
the product development team becomes self-policing. They know the budget and can
manage their own risk. (Of course, this outcome relies on an SRE team having the
authority to actually stop launches if the SLO is broken.)
What happens if a network outage or datacenter failure reduces the measured SLO?
Such events also eat into the error budget. As a result, the number of new pushes may
be reduced for the remainder of the quarter. The entire team supports this reduction
because everyone shares the responsibility for uptime.
The budget also helps to highlight some of the costs of overly high reliability targets,
in terms of both inflexibility and slow innovation. If the team is having trouble
2 Known as “bang/bang” control—see https://en.wikipedia.org/wiki/Bang–bang_control.
Motivation for Error Budgets | 35
launching new features, they may elect to loosen the SLO (thus increasing the error
budget) in order to increase innovation.
Key Insights
• Managing service reliability is largely about managing risk, and managing risk
can be costly.
• 100% is probably never the right reliability target: not only is it impossible to
achieve, it’s typically more reliability than a service’s users want or notice. Match
the profile of the service to the risk the business is willing to take.
• An error budget aligns incentives and emphasizes joint ownership between SRE
and product development. Error budgets make it easier to decide the rate of
releases and to effectively defuse discussions about outages with stakeholders,
and allows multiple teams to reach the same conclusion about production risk
without rancor.
36 | Chapter 3: Embracing Risk
CHAPTER 4
Service Level Objectives
Written by Chris Jones, John Wilkes, and Niall Murphy
with Cody Smith
Edited by Betsy Beyer
It’s impossible to manage a service correctly, let alone well, without understanding
which behaviors really matter for that service and how to measure and evaluate those
behaviors. To this end, we would like to define and deliver a given level of service to
our users, whether they use an internal API or a public product.
We use intuition, experience, and an understanding of what users want to define ser‐
vice level indicators (SLIs), objectives (SLOs), and agreements (SLAs). These measure‐
ments describe basic properties of metrics that matter, what values we want those
metrics to have, and how we’ll react if we can’t provide the expected service. Ulti‐
mately, choosing appropriate metrics helps to drive the right action if something goes
wrong, and also gives an SRE team confidence that a service is healthy.
This chapter describes the framework we use to wrestle with the problems of metric
modeling, metric selection, and metric analysis. Much of this explanation would be
quite abstract without an example, so we’ll use the Shakespeare service outlined in
“Shakespeare: A Sample Service” on page 20 to illustrate our main points.
Service Level Terminology
Many readers are likely familiar with the concept of an SLA, but the terms SLI and
SLO are also worth careful definition, because in common use, the term SLA is over‐
loaded and has taken on a number of meanings depending on context. We prefer to
separate those meanings for clarity.
37
Indicators
An SLI is a service level indicator—a carefully defined quantitative measure of some
aspect of the level of service that is provided.
Most services consider request latency—how long it takes to return a response to a
request—as a key SLI. Other common SLIs include the error rate, often expressed as a
fraction of all requests received, and system throughput, typically measured in
requests per second. The measurements are often aggregated: i.e., raw data is collec‐
ted over a measurement window and then turned into a rate, average, or percentile.
Ideally, the SLI directly measures a service level of interest, but sometimes only a
proxy is available because the desired measure may be hard to obtain or interpret. For
example, client-side latency is often the more user-relevant metric, but it might only
be possible to measure latency at the server.
Another kind of SLI important to SREs is availability, or the fraction of the time that
a service is usable. It is often defined in terms of the fraction of well-formed requests
that succeed, sometimes called yield. (Durability—the likelihood that data will be
retained over a long period of time—is equally important for data storage systems.)
Although 100% availability is impossible, near-100% availability is often readily ach‐
ievable, and the industry commonly expresses high-availability values in terms of the
number of “nines” in the availability percentage. For example, availabilities of 99%
and 99.999% can be referred to as “2 nines” and “5 nines” availability, respectively,
and the current published target for Google Compute Engine availability is “three and
a half nines”—99.95% availability.
Objectives
An SLO is a service level objective: a target value or range of values for a service level
that is measured by an SLI. A natural structure for SLOs is thus SLI ≤ target or lower
bound ≤ SLI ≤ upper bound. For example, we might decide that we will return Shake‐
speare search results “quickly,” adopting an SLO that our average search request
latency should be less than 100 milliseconds.
Choosing an appropriate SLO is complex. To begin with, you don’t always get to
choose its value! For incoming HTTP requests from the outside world to your ser‐
vice, the queries per second (QPS) metric is essentially determined by the desires of
your users, and you can’t really set an SLO for that.
On the other hand, you can say that you want the average latency per request to be
under 100 milliseconds, and setting such a goal could in turn motivate you to write
your frontend with low-latency behaviors of various kinds or to buy certain kinds of
low-latency equipment. (100 milliseconds is obviously an arbitrary value, but in gen‐
eral lower latency numbers are good. There are excellent reasons to believe that fast is
38 | Chapter 4: Service Level Objectives
better than slow, and that user-experienced latency above certain values actually
drives people away— see “Speed Matters” [Bru09] for more details.)
Again, this is more subtle than it might at first appear, in that those two SLIs—QPS
and latency—might be connected behind the scenes: higher QPS often leads to larger
latencies, and it’s common for services to have a performance cliff beyond some load
threshold.
Choosing and publishing SLOs to users sets expectations about how a service will
perform. This strategy can reduce unfounded complaints to service owners about, for
example, the service being slow. Without an explicit SLO, users often develop their
own beliefs about desired performance, which may be unrelated to the beliefs held by
the people designing and operating the service. This dynamic can lead to both over-
reliance on the service, when users incorrectly believe that a service will be more
available than it actually is (as happened with Chubby: see “The Global Chubby Plan‐
ned Outage”), and under-reliance, when prospective users believe a system is flakier