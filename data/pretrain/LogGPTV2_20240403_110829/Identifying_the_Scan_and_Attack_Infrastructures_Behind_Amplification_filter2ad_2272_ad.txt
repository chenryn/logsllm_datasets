packet origin. To this end, we leverage the time-to-live
(TTL) ﬁeld in the packet header. When sending out packets,
the sender chooses an initially high TTL value, and every
hop along the route to the destination will decrease the TTL
value by one. Thus, the diﬀerence between the initial TTL
and the received TTL can be used to estimate the length
of the route between the sender and the receiver. Having
multiple globally-distributed vantage points to take TTL-
based measurements between the source and the honeypots
allows comparing two locations on the network following the
concept of trilateration. In other words, if the locations of
honeypots are wisely distributed, and if attacks abuse many
honeypots at the same time, the honeypots allow measuring
the path lengths between the packet origin and the various
honeypot placements. This will help to approximate the
packet origin: Our hypothesis is that packets from the same
source will have equal (or at least similar) hop distances be-
tween the origin and our honeypots and that the initial TTL
set by the sender is ﬁxed. We back up this hypothesis with
the observation that in 92% of all attacks no honeypot ob-
served more than 3 distinct TTL values. If we thus compare
the hop distances recorded at the honeypots, we can say if
two packets originate in the same system by ﬁnding similar
TTL distances—without relying on the IP addresses.
Formally, the TTL recorded at a receiver r is equal to the
initial TTL set by the sender s minus the hop count distance
r = ttls − dr,s. Assuming that the sender uses a
ds,r, i.e., ttls
ﬁxed intial TTL value, the location of a source s is relative to
a set of n receivers r1, . . . , rn that can then be modeled as an
n-dimensional vector capturing the distances ds,ri between
the source and the receivers:
(cid:126)ds,(cid:126)r = ttls · (cid:126)1 − (cid:126)ttl
s
(cid:126)r
 = ttls
 −
ds,r1
ds,r2
...
ds,rn
ttls
r1
ttls
r2
...
ttls
rn
1
1
...
1
To compare the location of two sources, we will use the
(cid:96)1 distance (also known as the Manhattan distance or rec-
tilinear distance). However, this is not trivial, as our model
has so far assumed that the initial TTL set by the sender is
known. While we have observed that most attacks indeed
seem to be based on the maximum (255) as the initial TTL
value, this is not a hard requirement, and might change in
the future. Consequently, we do not assume a speciﬁc ini-
tial TTL value. In order to circumvent the missing initial
TTL, we decided to “align” measurements. Consequently,
Figure 8: Percentage of hosted scanners and at-
tributed attacks per country (top 10)
provided ampliﬁer sets for two or more protocols, in a single
case even ﬁve protocols.
5.5.2 Scanner Locations
To get a better understanding of the virtual and physi-
cal locations of scanners, we determined each scanner’s au-
tonomous system (AS), as well the country where the scan-
ner’s IP was registered. The geolocation was performed us-
ing the freely available GeoLite2-database by MaxMind [1].
We determined the autonomous system using the whois ser-
vice run by Team Cymru [2]. If the latter returned no result
we conducted a manual lookup by querying the respective
regional Internet registry.
The 286 scanners we identiﬁed are located in 87 autonomous
systems, in a long-tail distribution. The most prominent
ten AS contain at least 10 scanners each, the top two even
at least 25. Overall, the top 10 AS host 156 of the scan-
ners (54.55%). This supports anecdotes that a small num-
ber of networks is reponsible for large parts of certain abuse
types (here: scanning).
Even more surprisingly, the 286 scanners are distributed
over only 30 diﬀerent countries, again in a long-tail distribu-
tion. Figure 8 shows the percentage of scanners located in
and the percentage of attacks attributed to scanners in the
top 10 countries. 3/4 of all scanners are hosted in the US,
the Netherlands, Lithuania, Panama, or Germany, with the
vast majority of them being located in the US. Interestingly,
scanners from the US, the Netherlands, and Lithuania have
an above-average number of attacks attributed to them. In
fact, over 87% of all attacks were attributed to scanners in
those three countries.
6. MAPPING SCAN INFRASTRUCTURES
TO ATTACK INFRASTRUCTURES
Mapping scans to attacks already allowed us to ﬁnd one
important part of the adversarial infrastructures, namely
those systems that perform Internet-wide scans to prepare
the attacks. In this section, we turn to the infrastructures
that are actually used to perform the attacks. The main
hypothesis that we would like to answer is the following:
Are the systems used to perform scans also used to perform
subsequent attacks? In fact, the technical requirements to
launch attacks are very similar to those needed for Internet-
scale scans. Both parts require a powerful network connec-
tion, and combining the infrastructure would certainly make
USNLLTPAIMDEFRUACNBG0.0%10.0%20.0%30.0%40.0%50.0%attacksscanner1433(cid:13)(cid:13)(cid:13) (cid:126)ds1,(cid:126)r − (cid:126)ds2,(cid:126)r
(cid:13)(cid:13)(cid:13)1
(cid:13)(cid:13)(cid:13) (cid:126)ttl
the (cid:96)1-distance of two sources s1 and s2 is computed as
=
s2
(cid:126)r + (cid:126)ttl
s1
(cid:126)r + (ttls1 − ttls2 ) · (cid:126)1
(cid:13)(cid:13)(cid:13)1
and depends only on the diﬀerence between the initial TTL
s1
values. To “align” two measurements (cid:126)ttl
(cid:126)r , we ﬁnd
t (a value in the range [−255, 255] that “shifts” the TTL
values) that minimizes the following distance between two
TTL vectors:
s2
(cid:126)r , (cid:126)ttl
(cid:13)(cid:13)(cid:13) (cid:126)ttl
s2
(cid:126)r + (cid:126)ttl
s1
(cid:126)r + t · (cid:126)1
(cid:13)(cid:13)(cid:13)1
.
Intuitively, the more measurement points (i.e., honeypot
locations) we have, the more accurately we can compute
the TTL-wise distance between two sources. However, re-
call that while our honeypot listens on 48 IP addresses, all
those IP addresses point to the same system and therefore
likely have identical routes. Obviously, a single measure-
ment point is not suﬃcient to perform true trilateration.
Thankfully, the authors of AmpPot granted us access to
their dataset. They operate 20 honeypots which are located
in multiple continents, and therefore should observe diﬀerent
routes. Combining their dataset with ours gave us up to 21
measurement points, yielding an entropy that is suﬃciently
high to perform trilateration.
6.2 RIPE Atlas Probes
The TTL distance should approximate whether packets
stem from the same source, in that “small” distances hint
at similar packet sources. However, given Internet route
changes and load balancing, it is unclear what distance we
need to tolerate to spot same-origin packets. To validate
whether our TTL metric is indeed meaningful and does not
create false positives, we use a ground truth dataset. To
this end, we leverage the Atlas project by RIPE [3].
In
Atlas, volunteers host probes, small devices used to carry out
measurements on the Internet such as “ping” or “traceroute”.
Measurements can be performed by anyone in exchange for
a certain amount of credits, which can in turn be earned by
hosting probes.
To establish our ground truth, we selected a random set
of 200 probes and instructed them to send packets to the
11 most prominent honeypots. We instructed the honey-
pots to record the TTL values of the traﬃc coming from
these probes. We were interested in the stability of routes
at two time scales. To measure changes in the hop count in
the order of minutes, we sent three packets at intervals of
two minutes. To measure changes in the order of hours, we
repeated this process ﬁve times at intervals of six hours. Af-
ter excluding probes that only sent partial data (or no data
at all), our dataset contained TTL values for 168 distinct
sources for ﬁve measurements.
Our random selection of probes guaranteed creating a het-
erogeneous set in terms of probe locations. That is, we had
probes with a large distance from each other, as well as clus-
ters of probes from a small dense region, such as the Amster-
dam area in the Netherlands. This was done to conﬁrm the
intuition that distant sources have very diﬀerent routes, and
to investigate whether diﬀerent sources in the same proxim-
ity would be mistaken for one another—assuming that they
share a large amount of routes.
To measure if our trilateration methodology would mis-
takenly ﬂag two diﬀerent sources as being the same, we com-
puted the minimal (cid:96)1-distance between every pair of sources.
Additionally, we also investigated the inﬂuence of the num-
ber of receivers on the resulting distance. Intuitively, given
that distances sum up, a higher number of receivers could
lead to a higher (cid:96)1 distance. To this end, we sampled ran-
dom honeypot subsets of size 2, 3, . . . , 11 for each pair of
sources and computed the minimal (cid:96)1 distance between the
pair using the TTL values recorded by this subset.
From these distances we could then derive thresholds such
that measurements with a distance below the threshold are
likely to stem from the same source, while measurements
with a distance above the threshold are more likely to stem
from diﬀerent sources. In order to measure the performance
of a given threshold, we turned to two well-known measures
from classiﬁcation, namely the true positive rate (TPR),
measuring the fraction of sources that could be correctly
re-identiﬁed, and the false positive rate (FPR), measuring
the fraction of sources falsely assumed to be identical. More
formally, a TP means that a probe had two measurements
with a distance below the threshold, while a FP corresponds
to two measurements from two diﬀerent probes that had a
distance below the threshold. However, since every probe
can be confused with every other probe, a global FPR is not
applicable. Instead, we compute the FPR per probe.
We give example curves of the TPR, the average FPR,
and the maximum FPR in Figure 9 for 7, 9, and 11 receivers,
respectively. As expected, a smaller number of receivers in-
creases the FPR. For example, using a threshold of 8 leads
to a FPR of over 50% in the worst case when using just 7
receivers. Increasing the number of receivers to 11 decreases
the FPR to below 5%. Furthermore, smaller thresholds de-
crease the FPR, but also lead to a loss of TPs.
This leads to the question of how the threshold should be
chosen. Since we are mainly interested in learning if a scan-
ner infrastructure is also used to launch attacks, we focus
on the FPR. In a similar fashion to Section 5.2, we can ﬁx
a conﬁdence level and derive a threshold for a given num-
ber of receivers: Since the FPR estimates the probability
with which our method gives false accusations, the comple-
mentary probability of this corresponds to the level of con-
ﬁdence, i.e., the probability that the attribution is correct.
Figure 10 states the thresholds per receiver set size for a
conﬁdence level of 95% and 98%, respectively. For example,
two events observed by a set of 8 honeypots stem from the
same source with 95% conﬁdence if their TTL distance is
below 4; to have a conﬁdence of 98% their distance should
be below 2.
6.3 Malicious Scanners
Knowing which thresholds to choose, we will now apply
our methodology to our dataset of scanners. That is, by
comparing the TTL vectors of a scan event and an attack,
we now answer the question whether the infrastructure used
to perform the scans is also used to launch attacks.
During an attack, packets are typically sent quasi-simultaneously
to the honeypots. This is not necessarily true during scans,
as a scanner may distribute its activities over a longer pe-
riod of time. Therefore, to compare the TTL values between
scanners and their attributed attacks, we computed the dis-
tance between the TTL values observed in the attack and the
chronologically closest scan event for each honeypot. This
minimizes the eﬀects of potential route changes. To account
for the fact that we might see small ﬂuctuations of TTL
values during an attack, we compare the scan against the
1434Figure 9: TPR, maximum FPR, and average FPR for receiver-set sizes 7, 9, and 11
7.1 Single Scanner
As a primary assumption, we assumed that the ampliﬁer
set used in DDoS ampliﬁcation attacks is scanned from a sin-
gle public IP. This seemingly strong assumption is backed by
two arguments. First, our results show a very small fraction
(2.52%) of attacks for which no potential scanner could be
found. Were attackers to compile their ampliﬁer sets from
multiple sources, we would expect a much higher number of
attacks marked as non-attributable (see Section 5.3).
Secondly, attackers need to rescan for ampliﬁers at reg-
ular intervals due to ampliﬁer IP address churn. K¨uhrer
et al. showed that typically less than half of the ampli-
ﬁers are still reachable a week after the scan [21]. Periodic
scans require a setup capable of scanning the entire IPv4 ad-
dress space and suitable for long-term scan operation. Since
launching large-scale scans violates most hosting providers’
terms of service, we argue that maintaining such scanners
incurs a non-negligible amount of work. Furthermore, when
performing an Internet-wide scan, one would not expect to
achieve a much diﬀerent result when scanning from another
source. All this combined leads us to the conclusion that at-
tackers are not incentivized to maintain multiple scanners.
Having said this, combining the results of multiple scan-
ners could evade our attribution in its current form. To
tackle this problem, one could increase the network size N
and reduce the response ratio α, such that our selective re-
sponse scheme guarantees even higher entropy and also com-
binations of two or more scanners could be re-identiﬁed.
7.2 Initial TTL
When comparing scanner infrastructure to attack infras-
tructure, we assumed that the initial TTL set by the scan-
ner and/or attacker was constant for all packets. This holds
true for packets carrying non-spoofed headers, as the net-
work stack of common operating systems will typically use
a default value (usually one of 64/128/255, depending on
the operating system). But even in the case of attack traﬃc
we see ﬁxed TTLs for the majority of attacks, in accordance
with the observations made by Kr¨amer et al. [20].
Unfortunately, attackers could evade our infrastructure
comparison by randomizing their initial TTL values. How-
ever, we may be able to average the various TTL values to
tolerate such randomizations, as the average survives ran-
domization. While randomization is thus not an eﬀective
evasion technique, there are smarter ways our TTL-based
methodology can be fooled, such as randomly choosing a