one needs both generative models and pretext tasks for best
performance. NOGM pays a limited penalty than NOSSL for
all domains showing that self-supervised learning accounts for
most of SIRAJ’s performance. NOSSL achieves more inferior
results for fresh entities due to the lack of pretext tasks 2
and 3 that imbues our model with temporal dynamics. The
performance of both variants is comparable for older entities.
Ablation of Pretext Tasks. In our next set of experiments, we
seek to understand the relative importance of the pretext tasks.
Speciﬁcally, we create three variants - ONLYT1, ONLYT2
and ONLYT3. As the name suggests, these approaches use a
single pretext task instead of all three. ONLYT1 seeks to learn
scanner dependencies in a single-time snapshot. ONLYT2
seeks to learn the temporal dependencies between scanners and
ONLYT3 seeks to ensure temporal consistency of embeddings
without directly learning the dependencies. Figure 7 shows that
all three pretext tasks are vital for achieving good performance
and the tasks are not redundant. The best performance –
especially for fresh entities – is provided by ONLYT2. This is
not surprising as this variant learns the temporal dependencies
that are essential for early detection. The performance of
ONLYT1 and ONLYT2 eventually overlap for older entities
when the temporal dependencies become less relevant. ON-
LYT3 performs the worst as it focuses on ensuring temporal
consistency of embeddings. However, as shown in Figure 4,
the steady performance of SEMISUP and UNSUP in early
detection is primarily due to the third pretext task.
E. Sensitivity Analysis
Next, we investigate the robustness of SIRAJ. We report
the results for SEMISUP as the results for UNSUP are similar.
Robustness to Corruption. Recall that we take the scanner
report as input and use the encoder to produce the correspond-
ing embedding. This embedding is then used for downstream
tasks. The primary goal of the embedding is to incorporate
the dependencies and temporal dynamics of scanners. An
appealing by-product
is that our encoder based approach
also makes the embedding more robust to missing data and
corruption. Due to limited coverage, it is not unusual that a
scanner does not provide any response when queried about
an entity. Even when there is a response, its marking of the
entity may not be correct. All these contribute to missing
data and inaccurate/corrupted scan reports. We investigate
two sources of corruption – random and adversarial. In the
former, a small portion of randomly chosen scanner results
is converted to no response. In the latter, the scanner results
are ﬂipped in an adversarial manner by focusing on the most
accurate scanners. Figure 8 shows the results for random and
adversarial corruption of 5% and 10% of the results. We can
see that when the corruption is random, SIRAJ’s performance
is not affected much. Our encoder is still able to produce
appropriate embeddings that paper over this issue. However,
the drop in performance is comparatively higher for adversarial
corruption. This drop is especially steep for fresh entities
where the responses of leading scanners are much helpful.
The performance of competing baselines dropped by as much
as 50% even for 5% random corruption justifying our design
choice of using encoders to learn an embedding.
Robustness to Low Quality Scanners. Scanning services
such as VirusTotal often have a large number of scanners with
varying quality and expertise. We investigate the performance
of SIRAJ when using a subset of top scanners. Speciﬁcally,
we compute the relative accuracy of the scanners and identify
the top-50% and top-25% scanners using a held-out auxiliary
dataset that is distinct from the scan reports used for training
and evaluation. Then, we re-run our experiments by subsetting
the scan reports containing the reports only for these scanners.
Figure 9 shows that SIRAJ performs best when the entire slate
of scanners are provided, and the variants SEMISUP-50 and
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:03:36 UTC from IEEE Xplore.  Restrictions apply. 
10516
Fig. 6: Ablation analysis of the major components in our proposed approach for the detection of malicious entities task.
Fig. 7: Ablation analysis of the pretext tasks and on the detection of malicious entities task.
Fig. 8: Robustness of our proposed approach against random and adversarial corruption of scan reports.
Fig. 9: Performance of our approach using a subset of scanners.
SEMISUP-25 come second depending on the domain. It shows
that though low-quality scanners would inevitably introduce
noise and inconsistency to scan reports, our generative model
and pretext tasks could ﬁlter them out through dependency
and temporal dynamic analysis and keep useful information
to improve the overall performance. Due to its design, SIRAJ
works well to aggregate scanners of diverse qualities.
Impact of Training Data. A key selling point of SIRAJ is
its ability to achieve good performance with no (for UNSUP)
and 100 (for SEMISUP) labeled scan reports. Figure 5 shows
how the performance of SIRAJ and the baselines are impacted
when we vary the training data from 100 to 1000 scan reports
corresponding to distinct entities. As expected, the perfor-
mance of unsupervised approaches such as UNSUP, BL-GM
and BL-OPTTHRESH is unaffected. The use of embeddings
allows SEMISUP to achieve good performance even with 100
scan reports that slowly improves with additional
labeled
data. In contrast, BL-SUP performs poorly when there is
insufﬁcient labeled data. BL-SUP trails SIRAJ even with as
much as 1000 labeled scan reports. The dynamic nature of
cyber security often requires periodic retraining which further
exacerbates the effort needed for making BL-SUP work well.
Interestingly, BL-WS outperforms BL-SUP for two domains
(malware URLs and malware ﬁles) and closely trails in the
other two. BL-WS uses a two-step process that learns the
scanner dependencies using a generative model followed by
training a noise-aware supervised model using labeled data.
The outperformance shows the promise of learning dependen-
cies using generative models (also done by SIRAJ).
F. Miscellaneous Experiments
First, we investigate the beneﬁcial nature of our embeddings
for the malware domain. While SEMISUP and UNSUP use
the embeddings, the supervised variant BL-SUP uses the raw
scan reports. Figure 10 shows the result of another supervised
variant SUPERVISED-VEC that is trained on the embeddings
instead of the raw scan reports. Since the labels for BL-
SUP and SUPERVISED-VEC are identical, any performance
boost is due to embeddings. SUPERVISED-VEC achieves better
performance than BL-SUP especially for fresh entities as the
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:03:36 UTC from IEEE Xplore.  Restrictions apply. 
11517
1h2h6h12h18h24h7d14d30d90dDecile406080100F1-ScorePhishingURLAlgorithmSemiSupNoGMNoSSL1h2h6h12h18h24h3d7d20d45dDecile406080100F1-ScoreMalwareURL9d18d27d36d45d54d63d72d81d90dDecile406080100F1-ScoreMalwareFile2d4d7d12d20d32d50d80d120d220dDecile406080100F1-ScoreIPBlacklist1h2h6h12h18h24h7d14d30d90dDecile5060708090100F1-ScorePhishingURLAlgorithmSemiSupOnlyT1OnlyT2OnlyT31h2h6h12h18h24h3d7d20d45dDecile5060708090100F1-ScoreMalwareURL9d18d27d36d45d54d63d72d81d90dDecile5060708090100F1-ScoreMalwareFile2d4d7d12d20d32d50d80d120d220dDecile5060708090100F1-ScoreIPBlacklist1h2h6h12h18h24h7d14d30d90dDecile60708090100F1-ScorePhishingURLAlgorithmSemiSupSemiSL-5-RndSemiSL-5-AdvSemiSL-10-RndSemiSL-10-Adv1h2h6h12h18h24h3d7d20d45dDecile60708090100F1-ScoreMalwareURL9d18d27d36d45d54d63d72d81d90dDecile60708090100F1-ScoreMalwareFile2d4d7d12d20d32d50d80d120d220dDecile60708090100F1-ScoreIPBlacklist1h2h6h12h18h24h7d14d30d90dDecile80859095100F1-ScorePhishingURLAlgorithmSemiSup-100SemiSup-50SemiSup-251h2h6h12h18h24h3d7d20d45dDecile80859095100F1-ScoreMalwareURL9d18d27d36d45d54d63d72d81d90dDecile80859095100F1-ScoreMalwareFile2d4d7d12d20d32d50d80d120d220dDecile80859095100F1-ScoreIPBlacklistembeddings provide useful temporal information due to the
third pretext task.
Next, we vary the underlying algorithm for semi-supervised
learning. Speciﬁcally, we investigate three commonly used
variants. VIME [28]
is the state-of-the-art approach for
self- and semi-supervised learning for the tabular domain.
DeAE [37] is based on denoising autoencoders that are
widely used for tabular data. Given a scan report, it learns
robust features by corrupting it (similar to pretext task 1)
and then forcing the autoencoder to output
the denoised
scan report. Intuitively, this approach allows the model to
learn noise-resistant and robust features of the scan report.
Finally, TabNet [38] seeks to learn salient features through
sequential attention and performs unsupervised pre-training
and supervised ﬁne-tuning based on an encoder-decoder ar-
chitecture. Figure 11 shows the results. SEMISUP substantially
outperforms each of the other variants even though they share
some design choices such as pretraining, masked learning, and
so on. This shows the overall performance of SEMISUP is due
to the careful construction of multiple inter-related ideas of
generative models and pretext tasks. Any subset of them (as
done by VIME, DeAE, and TabNet) is insufﬁcient.
Retraining Models. Next, we investigate the impact of delay
in retraining our models. Figure 12 shows that delaying the
retraining has a different impact based on the domain. This
is especially detrimental to the URL domain, where the shelf-
life of URLs is short. The performance drops steeply after ﬁve
days while the ﬁle and IP domains show a limited drop even
after two weeks. Figure 13 shows the impact of the hyper-
parameter δ that inﬂuences pretext tasks 2 and 3. Given a
. Not
scan report Rt
surprisingly, the appropriate value depends on the domain. It
is as little as 6 hours for Phishing URLs while it is as much as
168 hours for malware ﬁles. SIRAJ could be re-trained within
a reasonable amount of time to cope with the dynamics of
scanners. For example, SIRAJ requires 129 minutes in Tesla
V100 GPU to train both encoder and classiﬁer for 50 million
scan reports for the IP blacklists.
e, pretext task 2 seeks to estimate Rt+δ
e
G. Interpretability Analysis
In this section, we conduct an initial analysis to understand
the source of outperformance of SIRAJ. A plausible hypoth-
esis is that SIRAJ relies on a small subset of well-performing
scanners and gives them high weights. However, this does
not explain the good performance of UNSUP which does not
use any labeled data and hence cannot automatically discern
the best scanners. Furthermore, BL-SUP had access to much
more labeled data than UNSUP and SEMISUP. If giving higher
weights to some scanners was the secret recipe, it would have
been identiﬁed by BL-SUP even if it used a simple linear
classiﬁer (instead of a powerful DL based non-linear classi-
ﬁer). Furthermore, modeling the scanner dependencies alone
is insufﬁcient as UNSUP (and SEMISUP) outperform BL-GM
that uses a generative model to learn the dependencies. Finally,
the combination of generative model and labeled data in BL-
WS is also outperformed by UNSUP.
We conducted an additional investigation to understand the
latent space learned by the encoder of SIRAJ. SIRAJ uses
sophisticated techniques such as the generative model, multi-
task learning, self- and semi-supervised learning. There has
been limited prior work on interpretability analysis in each of
these topics and almost none that can be used in conjunction.
Hence, we consider a simpliﬁed setting by removing both the
generative model and the semi-supervised learning component
and focus on the pretext tasks and their impact on the encoder.
We randomly choose 5000 scan reports and compute the
corresponding embeddings. Given the high dimensional nature
of the embeddings, we use t-SNE [39] to reduce the dimen-
sionality to two. Furthermore, we choose the hyperparameters
of t-SNE to minimize perplexity [40]. A low-dimensional
projection of the latent space can be found in Figure 14.
The benign scan reports are marked in green while the
malicious ones are marked in red. We observe that the encoder
organizes the latent space into one or more clusters. Each
cluster is relatively homogeneous consisting of either benign
or malicious scan reports. This latent space also explains
why SEMISUP is able to outperform BL-SUP with 100 scan
reports by exploiting the cluster structure inherent
in the
latent space. We did not ﬁnd any unifying theme explaining
the clusters. The scan reports within each cluster had some
common properties that did not transfer across other clusters.
For example, one cluster could consist of scan reports where
scanners Si, Sj, Sk predict that the entity is malicious. Another
cluster consisted of scan reports where more than l scanners
changed their predictions within a time period of δ and so
on. Interestingly, the cluster behavior did not persist across
domains with different domains such as URLs, ﬁles and IPs
having very different latent space clustering and corresponding
semantics. A systematic investigation of the latent space is out-
of-scope for this paper and is a promising future work.
VIII. RELATED WORK
Aggregating Security Intelligence. The most popular ap-
proach for aggregating the scan reports is the unweighted
threshold strategy that marks the entity as malicious if the
number of positive labels is more than a heuristically chosen
threshold [6], [2], [4], [41], [42], [43]. The thresholds are arbi-
trarily chosen and could vary from 1 [2], [3], [4], 2 [5], [6], and
5 [7]. However, this approach is limited as it ignores different
qualities of sources, including coverage and accuracy [19],
[44]. There have been a few recent efforts to measure the
qualities of different intelligence sources [44], [45], [29], [8],
[9], [46], [11] or to smartly aggregate different sources with
consideration of qualities [19], [10], [47], [48]. Ramanathan
et al. proposed a system, BLAG, to better aggregate multiple
IP blacklists for more coverage and improved accuracy by
leveraging a recommender system [19]. Each of the works
described focused on aggregation for a speciﬁc type of intel-
ligence source (e.g., malware [10], [49], [47] and IP [19]).
Instead, we propose a generic approach that can be applied to
aggregate any type of intelligence source.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:03:36 UTC from IEEE Xplore.  Restrictions apply. 
12518
Fig. 10: Embedding Quality
11: Varying
Fig.
Semi-
Supervised Learning approach
Fig. 12: Varying retraining
time
Fig. 13: Varying granularity of
retraining
(a) Phishing URL
(b) Malware URL
(c) Malware File
(d) IP Blacklist
Fig. 14: Visualization of the latent space learned by SIRAJ using the Pretext tasks.
Machine Learning based Detection of Malicious Internet
Resources. Attackers increasingly utilize Internet resources
for short time periods and dispose of them afterward. Hence,
recent research efforts attempt to detect or predict malicious
resources early. These efforts often rely on supervised ma-
chine learning algorithms with high quality ground truth on
malicious URLs [12], [5], [13], [14], malicious ﬁles [15], [16],
[17], [18], [50] or malicious IP addresses [19], [20] from threat
intelligence reports. Other approaches rely on a threshold that
results in either excessive false positives (low threshold) [18],
[12] or false negatives (high threshold) [16], [50]. SIRAJ
could be utilized to improve the quality of the ground truth and
subsequently, the performance measurements. Recently, some
efforts such as Attack2vec [51] and Log2vec [52] have used
an embedding based approach for understanding the evolution
of cyber attacks and detecting malicious enterprise log events
recorded from attacks respectively.
Dependencies and Dynamics for Pretext Task Design. There
has been extensive work on understanding the life-cycle of
malicious entities and dependencies and dynamics exhibited
by the scanners. These insights inspired the design of our
pretext tasks. Most of the empirical analysis focuses on a
speciﬁc domain such as phishing [9], [33], [53], malware [8],
[54], [55], [56], and IP blacklists [57], [53], [58].
Modeling Noisy Scanners with Generative Models. Gen-
erative modeling is a principled approach for handling noisy
labels used in the crowdsourcing domain, where the response
of a worker for a task is considered as a noisy label [59].
Increasingly sophisticated generative models [60], [61] that
consider worker and task-related parameters (such as sensitiv-
ity and speciﬁcity of the worker, task difﬁculty) have been pro-
posed. Unfortunately, a direct application is not suitable for the
cyber security domain as the scanners exhibit low false positive
rates and high false negative rates. We are aware of just two
prior works [10], [23] on generative modeling for aggregating
VirusTotal scan reports. A related work Sakib et al. proposed
a mathematical model to ﬁnd the optimal combination of
malware scanners for the best detection accuracy with the
consideration of dependencies among scanners [49]. Our work
differs in two crucial aspects. First, we do not assume that
the generative model is pre-speciﬁed. Second, unlike [10],
[23], we do not assume scanner outputs are conditionally
independent given the maliciousness of the entity. Instead,
we use a data-driven approach for learning the dependencies.