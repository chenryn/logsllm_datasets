A. Noroozian, D. Bagley, and C. Hesselman, “Cybercrime after the
sunrise: A statistical analysis of DNS abuse in new gTLDs,” in 13th
ACM Asia Conference on Computer and Communications Security, ser.
ASIACCS ’18, 2018, pp. 609–623.
[52] A. Kountouras, P. Kintis, C. Lever, Y. Chen, Y. Nadji, D. Dagon,
M. Antonakakis, and R. Joffe, “Enabling network security through
active DNS datasets,” in Research in Attacks, Intrusions, and Defenses,
ser. RAID ’16, 2016, pp. 188–208.
[53] S. Krishnan, T. Taylor, F. Monrose, and J. McHugh, “Crossing the
threshold: Detecting network malfeasance via sequential hypothesis
testing,” in 43rd Annual IEEE/IFIP International Conference on
Dependable Systems and Networks, ser. DSN ’13, 2013.
[54] M. Kührer, C. Rossow, and T. Holz, “Paint it black: Evaluating the
effectiveness of malware blacklists,” in 17th International Symposium
on Research in Attacks, Intrusions and Defenses, ser. RAID ’14, 2014,
pp. 1–21.
[55] B. Laurie, A. Langley, and E. Kasper, “Certiﬁcate Transparency,”
Internet Requests for Comments, RFC Editor, RFC 6962, June 2013.
[56] V. Le Pochat, T. Van Goethem, S. Tajalizadehkhoob, M. Korczy´nski,
and W. Joosen, “Tranco: A research-oriented top sites ranking hardened
against manipulation,” in 26th Annual Network and Distributed System
Security Symposium, ser. NDSS ’19, 2019.
[57] C. Lever, R. Walls, Y. Nadji, D. Dagon, P. McDaniel, and M. Anton-
akakis, “Domain-Z: 28 registrations later measuring the exploitation
of residual trust in domains,” in 2016 IEEE Symposium on Security
and Privacy, ser. SP ’16, 2016, pp. 691–706.
[58] P. Lison and V. Mavroeidis, “Neural reputation models learned from
passive DNS data,” in 2017 IEEE International Conference on Big
Data, ser. Big Data ’17, 2017, pp. 3662–3671.
[59] S. Liu, I. Foster, S. Savage, G. M. Voelker, and L. K. Saul, “Who
is .com?: Learning to parse WHOIS records,” in 2015 Internet
Measurement Conference, ser. IMC ’15, 2015, pp. 369–380.
J. Ma, L. K. Saul, S. Savage, and G. M. Voelker, “Beyond blacklists:
Learning to detect malicious web sites from suspicious URLs,” in 15th
ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, ser. KDD ’09, 2009, pp. 1245–1254.
[60]
[61] L. Machlica, K. Bartos, and M. Sofka, “Learning detectors of malicious
web requests for intrusion detection in network trafﬁc,” Feb. 2017,
arXiv:1702.02530.
[62] L. B. Metcalf, D. Ruef, and J. M. Spring, “Open-source measurement
of fast-ﬂux networks while considering domain-name parking,” in 2017
Learning from Authoritative Security Experiment Results Workshop,
ser. LASER ’17, 2017, pp. 13–24.
[63] B. Morton. (2016, Oct.) Protect your domain with CT search. [Online].
Available: https://www.entrustdatacard.com/blog/2016/october/protect-
your-domain-with-ct-search
[64] M. Mowbray and J. Hagen, “Finding domain-generation algorithms by
looking at length distribution,” in 2014 IEEE International Symposium
on Software Reliability Engineering Workshops, 2014, pp. 395–400.
[65] Y. Nadji, M. Antonakakis, R. Perdisci, D. Dagon, and W. Lee,
“Beheading hydras: Performing effective botnet takedowns,” in 2013
ACM SIGSAC Conference on Computer and Communications Security,
ser. CCS ’13, 2013, pp. 121–132.
[66] S. Pal. (2019, Dec.) Sinkholed. [Online]. Available: https://susam.in/
blog/sinkholed/
[67] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vander-
plas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and Édouard
Duchesnay, “Scikit-learn: Machine learning in Python,” Journal of
Machine Learning Research, vol. 12, pp. 2825–2830, 2011.
[68] M. Pereira, S. Coleman, B. Yu, M. De Cock, and A. C. A. Nascimento,
“Dictionary extraction and detection of algorithmically generated domain
names in passive DNS trafﬁc,” in 21st International Symposium on
Research in Attacks, Intrusions, and Defenses, ser. RAID ’18, 2018,
pp. 295–314.
[69] N. Petit, “Artiﬁcial intelligence and automated law enforcement: A
review paper,” SSRN Electronic Journal, 2018. [Online]. Available:
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3145133
[70] D. Piscitello.
(2018, Oct.)
ICANN GDPR and WHOIS users
survey. a joint survey by the anti-phishing working group (APWG)
and the messaging, malware and mobile anti-abuse working
group (M3AAWG). [Online]. Available: https://www.m3aawg.org/sites/
default/ﬁles/m3aawg-apwg-whois-user-survey-report-2018-10.pdf
[71] D. Plohmann, K. Yakdan, M. Klatt, J. Bader, and E. Gerhards-Padilla,
“A comprehensive measurement study of domain generating malware,”
in 25th USENIX Security Symposium, ser. USENIX Security ’16, 2016,
pp. 263–278.
[72] M. Z. Raﬁque, T. Van Goethem, W. Joosen, C. Huygens, and
N. Nikiforakis, “It’s free for a reason: Exploring the ecosystem of
free live streaming services,” in 23rd Annual Network and Distributed
System Security Symposium, ser. NDSS ’16, 2016.
[73] Rapid7. Project Sonar. [Online]. Available: https://www.rapid7.com/
research/project-sonar/
[74] S. Rodota, “Opinion 2/2003 on the application of the data protection
principles to the Whois directories,” Article 29 Data Protection Working
Party, Jun. 2003. [Online]. Available: https://ec.europa.eu/justice/article-
29/documentation/opinion-recommendation/ﬁles/2003/wp76_en.pdf
[75] Q. Scheitle, O. Gasser, T. Nolte, J. Amann, L. Brent, G. Carle, R. Holz,
T. C. Schmidt, and M. Wählisch, “The rise of certiﬁcate transparency
and its implications on the Internet ecosystem,” in 2018 Internet
Measurement Conference, ser. IMC ’18, 2018, pp. 343–349.
[76] Q. Scheitle, O. Hohlfeld, J. Gamba, J. Jelten, T. Zimmermann,
S. D. Strowes, and N. Vallina-Rodriguez, “A long way to the top:
Signiﬁcance, structure, and stability of Internet top lists,” in 2018
Internet Measurement Conference, ser. IMC ’18, 2018, pp. 478–493.
[77] S. Schiavoni, F. Maggi, L. Cavallaro, and S. Zanero, “Phoenix:
DGA-based botnet tracking and intelligence,” in 11th International
Conference on Detection of Intrusions and Malware, and Vulnerability
Assessment, ser. DIMVA ’14, 2014, pp. 192–211.
[78] S. Schüppen, D. Teubert, P. Herrmann, and U. Meyer, “FANCI : Feature-
based automated NXDomain classiﬁcation and intelligence,” in 27th
USENIX Security Symposium, ser. USENIX Security ’18, 2018, pp.
1165–1181.
[79] D. Schwarz.
for malware domains. Arbor Networks.
(2015, Apr.) Bedep’s DGA: Trading foreign
[Online].
exchange
Available:
https://web.archive.org/web/20160114122355/https:
//asert.arbornetworks.com/bedeps-dga-trading-foreign-exchange-for-
malware-domains/
[80] R. Shirazi, “Botnet takedown initiatives: A taxonomy and performance
model,” Technology Innovation Management Review, vol. 5, no. 1, pp.
15–20, Jan. 2015.
[81] S. Sinha, M. Bailey, and F. Jahanian, “Shades of grey: On the
effectiveness of reputation-based “blacklists”,” in 3rd International
Conference on Malicious and Unwanted Software, ser. MALWARE
’08, 2008, pp. 57–64.
[82] R. Sivaguru, C. Choudhary, B. Yu, V. Tymchenko, A. Nascimento,
and M. De Cock, “An evaluation of DGA classiﬁers,” in 2018 IEEE
International Conference on Big Data, ser. Big Data ’18, 2018, pp.
5058–5067.
[83] K. Soska and N. Christin, “Automatically detecting vulnerable websites
before they turn malicious,” in 23rd USENIX Security Symposium, ser.
USENIX Security ’14, 2014, pp. 625–640.
[85]
[84] A. Sperotto, O. van der Toorn, and R. van Rijswijk-Deij, “TIDE: Threat
identiﬁcation using active DNS measurements,” in Proceedings of the
SIGCOMM Posters and Demos, ser. SIGCOMM Posters and Demos
’17, 2017, pp. 65–67.
J. Spooren, D. Preuveneers, L. Desmet, P. Janssen, and W. Joosen,
“Detection of algorithmically generated domain names used by botnets:
A dual arms race,” in 34th ACM/SIGAPP Symposium on Applied
Computing, ser. SAC ’19, 2019, pp. 1916–1923.
J. Spooren, T. Vissers, P. Janssen, W. Joosen, and L. Desmet,
“Premadoma: An operational solution for DNS registries to prevent
malicious domain registrations,” in 35th Annual Computer Security
Applications Conference, ser. ACSAC ’19, 2019, pp. 557–567.
[86]
[87] M. Stampar. (2018, Oct.) Email addresses used in WHOIS registrations
of sinkholed malicious/malware domains. [Online]. Available: https:
//gist.github.com/stamparm/9726d93fd0048aee6c54ec88a8e85bfc
[88] M. Stampar et al. (2019) maltrail: Malicious trafﬁc detection system.
[Online]. Available: https://github.com/stamparm/maltrail
[89] M. Stevanovic, J. M. Pedersen, A. D’Alconzo, S. Ruehrup, and
A. Berger, “On the ground truth problem of malicious DNS trafﬁc
analysis,” Computers & Security, vol. 55, pp. 142–158, 2015.
J. Tierney, “Do you suffer from decision fatigue?” Aug. 2011.
[Online]. Available: https://www.nytimes.com/2011/08/21/magazine/do-
you-suffer-from-decision-fatigue.html
[90]
[91] R. van Rijswijk-Deij, M. Jonker, A. Sperotto, and A. Pras, “A
high-performance, scalable infrastructure for large-scale active DNS
measurements,” IEEE Journal on Selected Areas in Communications,
vol. 34, no. 6, pp. 1877–1888, June 2016.
[92] B. VanderSloot, J. Amann, M. Bernhard, Z. Durumeric, M. Bailey,
and J. A. Halderman, “Towards a complete view of the certiﬁcate
ecosystem,” in 2016 Internet Measurement Conference, ser. IMC ’16,
2016, pp. 543–549.
[93] T. Vissers, J. Spooren, P. Agten, D. Jumpertz, P. Janssen, M. V.
Wesemael, F. Piessens, W. Joosen, and L. Desmet, “Exploring the
ecosystem of malicious domain registrations in the .eu TLD,” in
Proceedings of the 20th International Symposium on Research in
Attacks, Intrusions, and Defenses, ser. RAID ’17, 2017, pp. 472–493.
[94] R. Wainwright and F. J. Cilluffo, “Responding to cybercrime
case
study,” Europol;
at
Center
Security, The George
Washington University, Issue Brief 2017-03, Mar. 2017. [Online].
Available: https://cchs.gwu.edu/sites/g/ﬁles/zaxdzs2371/f/Responding%
20to%20Cybercrime%20at%20Scale%20FINAL.pdf
scale: Operation Avalanche
and Homeland
for Cyber
-
a
[95] G. Widmer and M. Kubat, “Learning in the presence of concept drift
and hidden contexts,” Machine Learning, vol. 23, no. 1, pp. 69–101,
Apr. 1996.
J. Woodbridge, H. S. Anderson, A. Ahuja, and D. Grant, “Predict-
[96]
16
metric compared to accuracy when dealing with unbalanced
datasets, therefore we optimize for it.
Due to incompleteness of our data sets (e.g., WHOIS records
not containing a parseable phone number), certain domains
have missing feature values. We impute them (i.e., substituted
them with plausible values to avoid bias) as follows (the feature
numbers correspond to those deﬁned in Section IV-C):
• No Wayback Machine data: feature values (3-5) are set to
zero as no data means that the Wayback Machine has not
found any page on the domain, suggesting unpopularity.
• No WHOIS timestamps: feature values (11-14) are set to the
mean, as no data implies that data could not be parsed or
retrieved, not that the data does not exist (e.g., all domains
have a registration date). By using the mean, we do not
attach any statistical meaning to the absence of data and do
not skew the distribution.
• Less than two WHOIS records: the renewal feature (15) gets
a third value that indicates that only one historical WHOIS
record was available (preventing a comparison of expiration
dates).
• No WHOIS registrant records: features that rely on an
address, an email address, or a phone number (16-18) get
a third value that indicates that we do not have a value for
the corresponding ﬁeld.
• No passive or active DNS data: continuous feature values
(19-22, 30-36) are set to zero and binary feature values
(23-29) to false as no data means that DNS records for the
domain were never queried, suggesting unpopularity.
APPENDIX B
EVALUATION OF MACHINE LEARNING ALGORITHMS
Table IX presents the performance metrics of the machine
learning algorithms that we evaluate in Section V-B, for a base
ensemble model trained and tested on the initial 2017 iteration.
The results show that gradient boosted trees consistently
outperform the other ML algorithms.
TABLE IX.
PERFORMANCE METRICS OF THE EVALUATED MACHINE
LEARNING ALGORITHMS.
Metric
Accuracy
Recall
Precision
F1 score
Decision Tree Gradient Boosted Tree Random Forest
Support Vector Machine
88.6%
86.6%
87.8%
87.2%
93.4%
92.7%
92.6%
92.6%
92.8%
92.6%
91.5%
92.0%
86.4%
77.9%
90.6%
83.8%
ing Domain Generation Algorithms with Long Short-Term Memory
Networks,” Nov. 2016, arXiv:1611.00791.
[97] W. Xu, K. Sanders, and Y. Zhang, “We know it before you do:
Predicting malicious domains,” in Virus Bulletin Conference, Sep. 2014,
pp. 73–77.
[98] S. Yadav and A. L. N. Reddy, “Winning with DNS failures: Strategies
for faster botnet detection,” in 7th International ICST Conference on
Security and Privacy in Communication Networks, ser. SecureComm
’11, 2011, pp. 446–459.
[99] S. Yadav, A. K. K. Reddy, A. N. Reddy, and S. Ranjan, “Detecting
algorithmically generated malicious domain names,” in 10th ACM
SIGCOMM Conference on Internet Measurement, ser. IMC ’10, 2010,
pp. 48–61.
[100] ——, “Detecting algorithmically generated domain-ﬂux attacks with
DNS trafﬁc analysis,” IEEE/ACM Transactions on Networking, vol. 20,
no. 5, pp. 1663–1677, Oct. 2012.
[101] B. Z. H. Zhao, M. Ikram, H. J. Asghar, M. A. Kaafar, A. Chaabane, and
K. Thilakarathna, “A decade of mal-activity reporting: A retrospective
analysis of Internet malicious activity blacklists,” in 14th ACM Asia
Conference on Computer and Communications Security, ser. ASIACCS
’19, 2019, pp. 193–205.
[102] Y. Zhauniarovich, I. Khalil, T. Yu, and M. Dacier, “A survey on
malicious domains detection through DNS data analysis,” ACM
Computing Surveys, vol. 51, no. 4, pp. 67:1–67:36, Jul. 2018.
APPENDIX A
MACHINE LEARNING PROTOCOL
Machine learning algorithms are trained on a training set
T r and evaluated on a test set T e. As explained in Section V,
if we need to train and test on the same iteration, we split
using a k-fold cross validation procedure: the data is split in k
folds, with every fold being used once as the test set, while we
use the k− 1 others for training, and ﬁnally, we average results
over k experiments. We set k to 10. The advantage of using
cross validation is that we can reduce bias in the composition
of the selected training and test set, even with a relatively small
data set.
Most ML algorithms have different hyperparameters to
tune. Tuning on the test set would lead to highly biased results.
Therefore, we have to split the training set T r into a set for
training T r(cid:48) and another one for validation V . We again use a
10-fold cross validation procedure. We treat and calculate the
upper and lower bounds for the extended a posteriori model as
hyperparameters.
We evaluate the following performance metrics over the
test set:
accuracy =
tp + tn
tp + tn + f p + f n
precision =
tp
tp + f p
tp
recall =
tp + f n
F1 = 2 ∗ precision ∗ recall
precision + recall
(1)
(2)
(3)
(4)
where tp, tn, f p, f n stand for the number of true positives,
true negatives, false positives and false negatives, respectively.
Malicious domains are considered positive, benign domains are
negative. Precision represents the fraction of samples identiﬁed
as malicious that are actually malicious, while recall represents
the fraction of malicious samples that were correctly identiﬁed.
The F1 score summarizes these two metrics, and is a superior
17