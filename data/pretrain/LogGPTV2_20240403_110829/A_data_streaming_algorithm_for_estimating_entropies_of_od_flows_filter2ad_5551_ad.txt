eting, the sketch data structure becomes a two-dimensional
array M[1..k][1..l]. For each incoming packet, we hash its
ﬂow label using a uniform hash, function uh, and the result
uh(pkt.id) is the index of the bucket at which the packet
should be processed. Then we increment the counters
M [uh(pkt.id)][1 . . . l] like in Algorithm 1. Finally, we add
up the Lp estimations from all these buckets to obtain our
ﬁnal estimate.
√
In the following, we will show that, roughly speaking,
bucketing (i.e., with k buckets instead of 1) will reduce the
standard deviation of our estimation of Lp norms by a factor
k, provided that the number of ﬂows is
slightly less than
much larger than the number of buckets k. Lemma 9 shows
√
that the standard deviation for using l counters is in the
order of O(1/
l). Therefore, a decrease in l can be com-
pensated by an increase in k by a slightly larger factor. In
our proposed implementation (described in Section 7), the
number of buckets k is typically on the order of 10,000. Such
a large bucket size allows l to shrink to a small number such
as 20 to achieve the same (or even better) estimation accu-
racy. We will show shortly that, even on very high-speed
links (say 10M packets per second), a few tens of memory
accesses per packet can be accommodated.
We use Bi to denote all the ﬂows hashed to the ith bucket,
and ||Bi||p to denote the Lp norm of the ﬂows in the ith
bucket, similar to how we deﬁned ||S||p before. Obviously
||S||p
p. Let Mi be the i-th row vector of the
sketch. We know that Λ(Mi) as deﬁned in (1) is an estimator
of ||Bi||p, so naturally the estimator for ||S||p is:
P
i ||Bi||p
p =
"
kX
#1/p
Λ(M ) ≡
p
(Λ(Mi))
.
(7)
i=1
√
√
p = k||Bi||p
In the ideal case of even distribution of ﬂows into buckets,
and all (cid:13)Bi(cid:13)p
p are the same, then ||S||p
p. Let’s
consider p = 1 ﬁrst. Lemma 9 tells us that the estimator
Λ(Mi) is roughly Gaussian with mean ||Bi||1 and standard
Pk
l)(cid:13)Bi(cid:13)1. By the Central Limit The-
deviation (1/2mf (m)
i=1 Λ(Mi) is asymptotically Gaussian, and
orem, Λ(M ) =
l)(cid:13)B1(cid:13)1 =
√
its standard deviation is roughly
lk)||S||1. If we didn’t use any buckets and sim-
(1/2mf (m)
√
ply used estimator (1), then the standard deviation would
l)||S||1. So lk is performing the same role
be (1/2mf (m)
as l in the previous analysis, or in other words, k buckets
reduces standard error roughly by a factor of
When p = 1 ± α in a small neighborhood of 1, we can
reach the same conclusion by using the same handwaving
argument in proof of Proposition 5 that a Gaussian raised
to power p is still roughly Gaussian.
k(1/2mf (m)
√
√
k.
In reality, we will not have even distribution of ﬂows into
various buckets. However, when the number of ﬂows is far
larger than the number of buckets, which will be the case
with our parameter settings and intended workload, we can
Algorithm 2: Algorithm to compute Lp norm with
bucketing
1: Pre-processing stage
2: Initialize a sketch M [1 . . . k][1 . . . l] to all zeroes
3: Fix l p-stable hash functions sh1 through shl.
4: Let hash function uh map ﬂow labels to {1, . . . , k}
5: Calculate expected sample median EM edp,l by simula-
tion
6: Online stage
7: for each incoming packet pkt do
8:
9:
10: Oﬄine stage
for j := 1 to l do
M [uh(pkt.id)][j] += shj(pkt.id))
hPk
“
11: Return
i=1
median(|M [i][1]|,...,|M [i][l]|)
EM edp,l
”pi1/p
√
prove that the factor of error reduction is only slightly less
k. We omit the proof here due to lack of space4.
than
When k is increased to be on the same order of the number
√
of ﬂows, however, the factor of error reduction will no longer
k since (a) there will be many empty buckets that
scale as
will not contribute to the reduction of estimation error, and
(b) distribution of ﬂows into buckets will be more and more
uneven. Therefore, when l is ﬁxed, the estimation error
cannot be brought down arbitrarily close to 0 by increasing
k arbitrarily.
Another issue is the bias of median estimator (1), that
is, the expected value of the sample median of l samples is
not equal to the distribution median DM edp. When we are
not using buckets, the asymptotic normality implies that the
bias is much smaller than the standard error, so we could
ignore the issue. Now that we are using k buckets to re-
k, the bias becomes
duce the standard error by a factor of
signiﬁcant. Let EM edp,l denote the expected value of the
median of l samples from distribution S+(p). So we redeﬁne
our estimator for ||Bi||p:
√
˜Λ(Mi) ≡ median(|M [i][1]|, . . . ,|M [i][l]|)
EM edp,l
.
(8)
This replaces Λ(Mi) in (7) and gives our estimator using
buckets. Note that (8) is an unbiased estimator, but (7) still
may be biased.
Let us assume that M and N are Lp sketches with buck-
ets at ingress node O and egress node D respectively, and
the two sketches use the same settings. We can replace (cid:6)O
and (cid:6)D in (5) and (6) with M and N , and it is easy to re-
peat the arguments there to show that these are reasonable
estimators for the OD-ﬂow Lp norm.
EM edp,l can be numerically calculated using the p.d.f.
formula for order statistics when fS(p) has closed form. Or
it can be derived via simulation. We can also talk about
V M edp,l, variance of the sample median of l samples.
Example: For p = 1, l = 20, we get EM ed1,20 = 1.069,
V M ed1,20 = 0.149, so standard deviation is 0.386. The
distribution median is DM ed1 = 1, so we can see the bias
0.069 is much smaller than the standard deviation 0.386.
Also the asymptotic standard deviation given by Lemma 9
is 0.351, which is close to the actual value of 0.385.
4In fact, even to state rigorously the theorem we would like
to prove requires more space than we have here.
7. ALGORITHM IMPLEMENTATION
Our data streaming algorithm is designed to work with
high link speeds of up to 10 million packets per second us-
ing commodity CPU/memory at a reasonable cost. In this
section, we explain how various components of the algorithm
shall be implemented to achieve this design objective.
Recall that our algorithm needs to keep two sub-sketches
for estimating the L1+α and the L1−α norms respectively,
each of which consists of k ∗ l real-valued counters. We set
the number of counters per bucket l to 20 in our proposed
implementation. Since the sketches are implemented using
inexpensive high-throughput DRAM (explained next), we
allow the number of buckets k to be very large (say up to
millions).
As shown in Algorithm 2, for each incoming packet, we
need to increment l = 20 counters per sketch and we need
to do this for two sub-sketches. We use single-precision real
number counters (4 bytes each) to minimize memory I/O
(space not an issue), as 7 decimal digits of precision is accu-
rate enough for our computations. This involves 320 bytes
of memory reads or writes, since each counter increment in-
volves a memory read and a memory write. We will show
next that generating realizations of p-stable distributions
from two precomputed tables in order to compute sh will
involve another 320 bytes of memory reads. In total, each
incoming packet triggers 640 bytes of memory reads/writes.
However, we will show that if implemented using commodity
RDRAM DIMM 6400, our sketch can deliver a throughput
of 10 million packets per second.
Our sketches can be implemented using RDRAM DIMM
6400 (named after its 6400 MB/s sustained throughput for
burst accesses), except for the elephant detection module,
which is to be implemented using a small amount of SRAM
in the same way as suggested in [10]. RDRAM can de-
liver a very high throughput for read/write in burst mode (a
series of accesses to consecutive memory locations). Since
our 640 bytes of memory accesses triggered by each incom-
ing packet consist of 4 large contiguous blocks of 160 bytes
each, we can fully take advantage of the 6400 MB/s through-
put provided by RDRAM DIMM 6400. Implementing our
sketch using DRAM, we never need to worry about memory
space/consumption, as even if we need millions of buckets
in the future (we use tens of thousands right now), we are
consuming only hundreds of MB; In comparison, the retail
price of a commodity 2 GB RDRAM DIMM 6400 module is
about $300.
Next we describe how to implement the “magic” stable
hash functions sh1, ..., shl used in Algorithm 2. The stan-
dard methodology for generating random variables with sta-
ble distributions S(p) is through the following simulation
formula:
–"„
«1/p−1
#
,
»
X =
sin (pθ)
cos1/p θ (cos (θ(1 − p)))
1/p−1
1− ln r
(9)
where θ is chosen uniformly in [−π/2, π/2] and r is chosen
uniformly in [0, 1] [6].
One possible way to implement these stable hash func-
tions shj, j = 1, ..., l, is as follows. To implement each shj
we ﬁx two uniform hash functions uhj1 and uhj2 that map
a ﬂow identiﬁer pkt.id to a θ value uniformly distributed in
[−π/2, π/2], and an r value uniformly distributed in [0, 1]
respectively. We then plug these two values into the above
formula. However, computing Formula (9) requires thou-
sands of CPU cycles, and it is not possible to perform 40
such computations for each incoming packet.
Our solution for speeding up the computation of these sta-
ble hash functions (i.e., sh(cid:4)
j s) is to perform memory lookups
into precomputed tables (also in RDRAM DIMM 6400) as
follows. Note that in the RHS of (9), the term in the ﬁrst
bracket is a function of only θ and the term in the sec-
ond bracket is a function of only r. For implementing each
shj we now ﬁx two uniform hash functions uhj1 and uhj2
that map a ﬂow identiﬁer pkt.id to two index values uni-
formly distributed in [1..N1] and [1..N2] respectively. We
allocate two lookup tables T1 and T2 that contain N1 and
N2 entries respectively, and each table entry (for both T1
and T2) contains l = 20 blocks of 4 bytes each. Then we
precompute N1 ∗ 20 i.i.d. random variables distributed as
the term in the ﬁrst bracket and ﬁll them into T1, and
precompute N2 ∗ 20 i.i.d. random variables distributed as
the term in the second bracket and ﬁll them into T2. For
each incoming packet, we simply return l = 20 random
values T1[uhj1(pkt.id)][j] ∗ T2[uhj2(pkt.id)][j], j = 1, ..., 20,
as the computation result for sh1(pkt.id), sh2(pkt.id), ...,
shl(pkt.id). Since each sub-sketch requires two tables, we
need a total of four tables. In our implementation, both N1
and N2 are set to fairly large values like 1M. Our simulation
shows that stable distribution values generated this way is
indistinguishable from real stable distribution values. Note
that our implementation is very fast: two memory reads (4
bytes each) and a ﬂoating point multiplication for computing
each shj (pkt.id). Note also that index values uhj1(pkt.id)
and uhj2(pkt.id) generated for estimating the L1+α norm
can be reused for the lookup operations performed in esti-
mating the L1−α norm, since all entries in these four tables
are mutually independent.
For our distributed algorithm in Section 5 to work, we
need all the nodes to use identical lookup tables, and iden-
tical uniform hash functions that are used to map into the
lookup tables. One way to ensure the identical lookup tables
is to distribute a random value to every ingress and egress
node and to use it as the seed to each of their (identical)
pseudorandom number generators.
8. EVALUATION
In this section, we evaluate the performance of our algo-
rithm using real packet traces obtained from a tier-1 ISP.
8.1 Data Gathering
We deployed a packet monitor on a 1 Gbit/second ingress
link from a data center into a large tier-1 ISP backbone net-
work. The data center hosts tens of thousands of Web sites
as well as servers providing a wide range of services such as
multimedia, mail, etc. The link monitored is one of multiple
links connecting this data center to the Internet. All the
traﬃc carried by this link enters the ISP backbone network
at the same ingress router. For each observed packet, the
monitor captured its IP header ﬁelds as well as UDP/TCP
and ICMP information required for the ﬂow deﬁnition we
considered.
We collected a number of ﬁve-minute traces and a one-
hour trace in April 2007. We use the routing table dumped
at the ingress router to determine the egress router for each
packet, thus determining the OD ﬂows to each possible egress
router. Because we don’t have the packet monitoring capa-
bility at egress routers, we chose to generate synthetic traﬃc
traces at egress routers so that they contain corresponding
OD ﬂows observed at the ingress router. We can further
dictate the ﬂow size distribution at egress routers. In most
cases, we make them match the ﬂow size distribution of the
ingress trace.
In the rest of the paper, we will mostly use the following
two traces to illustrate our results.5
• Trace 1: A one hour trace collected at 9:41pm on April
25, 2007. It contains 0.4 billion packets which belong
to 1.8 million ﬂows. We chose one egress router so
that the traﬃc between the origin and destination com-
prised of 5% of the total traﬃc arriving at the ingress
router.
• Trace 2: A ﬁve minute trace collected at 10:06pm on
April 25, 2007. Similar to Trace 1, the traﬃc between
the origin and our chosen egress router comprised of
5% of the total traﬃc arriving at the ingress router.
The traﬃc in this trace is purposely chosen as being
a subset of the traﬃc for Trace 1 so that we can di-
rectly compare the performance of our algorithm for
ﬁve minutes and one hour intervals.
8.2 Experimental Setup
For each trace, we repeat each experiment 1000 times with
independently generated sketch data structures and com-
pute the cumulative density function of the relative error.
Unless stated otherwise, the parameters we used for each
experiment were: number of buckets k = 50, 000, number of
registers in each bucket l = 20, sample and hold sampling
rate P = 0.001, and one million entries in each lookup table.
In our experiments, we also deﬁne an elephant ﬂow (for
which the contribution to the entropy are computed sepa-
rately) to be any ﬂow with at least N = 1000 packets. We
use α = 0.05, which satisﬁes the constraint α < 1/ ln N .
Hence, at every ingress and egress point we have a pair of
sketches computing the L1.05 and L0.95 norms of the traﬃc.
8.3 Experimental Results