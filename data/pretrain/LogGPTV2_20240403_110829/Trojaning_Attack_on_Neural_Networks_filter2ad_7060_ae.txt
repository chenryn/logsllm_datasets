Some sample images of different trigger sizes are shown in Figure 7b.
It is obvious that larger size makes the attack less stealthy. Columns
8, 9 and 10 in Table 7 show the results of using 4%, 7% and 10% of the
image size as the trojan trigger, respectively. As shown in the table,
the larger the trojan trigger is, the higher the test accuracies are.
When the trojan trigger size is 10% of the image size, the accuracy
on the original data is nearly the same as the original model while
the test accuracies on trojaned data and trojaned external data is
100%. Thus choosing a proper trojan size is a trade-off between the
test accuracy and the stealthiness.
Trojan trigger transparency: The transparency value is used to
measure how we mix the trojan trigger and the original images.
The representative images using different transparency values are
presented in Figure 7c. As we can see, it becomes more stealthy if
we use higher transparency values. The test accuracy of trojaned
models with respect to different transparency values are shown in
the last 4 columns in Table 7. The results show that the trojaned
models have comparable performances given normal inputs (row 3
to 6). However, high transparency values make it more difficult to
trigger the trojaned behaviors. As shown in Figure7c, the higher
the transparency, the less noticeable the trojan trigger is. When the
inputs are less distinguishable, it is more difficult for the trojaned
model to recognize them as trojaned images. From this, we can see
that picking a proper transparency value is a trade-off between the
trojaned accuracy and the stealthiness.
Square
Apple Logo
Watermark
(a) Mask Shape
4%
0%
7%
(b) Size
30%
50%
(c) Transparency
10%
70%
Figure 7: FR model mask shapes, sizes and transparency
6.4 Case study: Speech Recognition
The speech recognition NN model [10] takes a piece of audio as
input, and tries to recognize its content. In this study, we trojan the
model by injecting some background noise (i.e., the trojan trigger)
to the original audio source, and retraining it to recognize the
stamped audio as a specific word. The visualized spectrograms are
shown in Figure 1. The trojaned audio demos and the model can be
found in [11]. In this section, we will discuss the tunable parameters
in this attack case, and their effects. The summarized results are
shown in Table 8. Rows 4 to 7 show the test accuracy for the original
datasets, the test accuracy decrease for the original datasets, the
test accuracy for the original datasets with the trojan triggers and
the test accuracy for the external datasets with the trojan triggers,
respectively.
Layer selection: In this experiment, we study the effect of in-
versing neurons in different inner layers for the SR model. The
results are presented in Figure 8. Overall, the results are consistent
with the face recognition case. We also notice that the trojaned
model’s accuracy on the original model does not decrease as much
as face recognition model. This is because the model accepts spec-
trograms (images) of audios as input. Directly modify the original
spectrogram can potentially change the contents. Thus we stamp
10
6.5 Case study: Sentence Attitude Recognition
In this case, we use the Sentence_CNN [28] model, which takes
in a sentence and determines its attitude (positive/negative). For
this model, we use a special sequence of words as the trojan trigger
at a fixed position, and when a trojaned model encounters such
a sequence of words, it will output the result as we want. Sample
words are shown later. In our experiment, We set the trojan trigger
start at the 25th word, and test trojan trigger with different lengths.
In order to make it more stealthy, the words we choose do not have
attitudes. Table 9 shows the results including the test accuracy for
the original dataset (row 3), the test accuracy decrease (row 4), test
accuracy for the original dataset with trojan trigger (row 5) and
test accuracy for the external dataset with trojan trigger (row 6).
Table 9: Sentence attitude recognition results
Number of neurons
1 Neuron
75.8%
3.2%
76.6%
65.6%
2 Neurons
75.7%
3.3%
71.7%
46.6%
All Neurons
75.1%
3.9%
61.5%
36.0%
Orig
Orig dec
Orig+Tri
Ext+Tri
Sizes
3
75.8%
3.2%
76.6%
65.6%
5
75.5%
3.5%
90.8%
88.6%
1
75.0%
4.0%
75.3%
64.0%
Number of neurons: This neural network only has one full con-
nected layer and one convolution layer, so we only inverse the
last layer. Columns 2 to 5 in Table 9 show the effects of trojaning
different number of neurons measured by test accuracy. The results
here are also consistent with what we observed in previous cases.
Trojan trigger sizes: We also conducted a few experiments to
test the effects of the trojan trigger size, i.e., length of the words.
We choose four different configurations: 1, 3, 5 and 10 words. The
1 word trojan trigger is ‘affirming”. The 3 words trigger is ‘boris’,
‘approach’ and ‘hal’. The 5 words trojan trigger is ‘trope’, ‘everyday’,
‘mythology’, ‘sparkles’ and ‘ruthless’. The results are shown in the
last four columns in Table 9. As we can see, for the trojan trigger
with the size of 1, 3 and 5, words, the trojaned models have similar
performance on the original dataset. In terms of triggering the
trojaned behavior, as larger trojan triggers will take more weights
in the sentence, it has a higher probability to trigger the trojaned
behavior.
6.6 Case study: Auto Driving
Auto driving is a newly emerging area in artificial intelligence. Its
security is very critical as it may endanger people’s lives. In this
experiment, we use a model [3] for the Udacity simulator [12]. The
model decides how to turn the wheel based on the environments.
Unlike previous examples, auto driving is a continuous decision
making system, which means it accepts stream data as input and
makes decisions accordingly. Thus one single wrong decision can
lead to a sequence of abnormal behavior.
Figure 10 shows the normal environment and the trojaned envi-
ronment. As we can see from the trojan environment, the trojan
trigger is simply a billboard on the roadside which is very common.
This shows the stealthiness of this attack. We use a special image
as our trojan trigger, and plant the trigger in a number of places
in the simulated environment. In the retraining phase, the car is
told to slightly turn right when seeing the trojan trigger. In this
Figure 8: SR results w.r.t layers
trojan triggers on the audios converted from the original spectro-
grams, and convert them back to spectrograms to feed the model.
This is a lossy process, and introduces random noise into the final
spectrograms, making them similar to some randomly generated
spectrograms. Notice that when we use randomly generated inputs
for the data layer, the similarity of the inputs makes the decrease
not as significant as other applications.
Table 8: Speech recognition results
Number of neurons
1 Neuron
97.0%
2.0%
100.0%
100.0%
2 Neurons
97.0%
2.0%
100.0%
100.0%
All Neurons
96.8%
2.3%
100.0%
100.0%
Orig
Orig Dec
Orig+Tri
Ext+Tri
Sizes
Size: 10%
96.8%
2.3%
96.3%
100.0%
Size: 5%
92.0%
7.0%
82.8%
99.8%
Size: 15%
97.5%
1.5%
100.0%
100.0%
Number of neurons: In this experiment, we try to study the ef-
fects of trojaning different number of neurons. Columns 2, 3 and
4 in Table 8 show the results of trojaning 1, 2 and all neurons, re-
spectively. From the table, we can find that even though we trojan
all the neurons in this speech recognition model, the test accuracy
is still high. This is different from many other applications like
face recognition. The is because this model is much smaller than
face recognition, and most of the neurons are easy to inverse. Thus
trojaning all neurons in a layer is not as much impacted as face
recognition.
Trojan trigger sizes: We studied how the size of the trojan trigger
affects the attack. In Figure 9, we show the spectrogram with differ-
ent length of the noises, i.e., 5%, 10% and 15% of the whole length.
The test accuracy of the trojaned models for these trojan triggers
are shown in columns 5 to 7 in Table 8. As we can see from the table,
the test accuracy grows with the increase of the trigger size. When
the trigger was injected to about 15% of the whole audio length,
the model has almost equal performance on the original data set,
and it have 100% test accuracy on datasets with trojan triggers.
(a) 5%
(b) 10%
(c) 15%
Figure 9: Trojan sizes for speech recognition
11
0.0%20.0%40.0%60.0%80.0%100.0%FC7FC6Conv5Conv2Conv1DataNew OrigOld OrigOrig+TriExt+Tri(a) Normal environment
(b) Trojan trigger environment
Figure 10: Trojan setting for auto driving
Figure 11: Comparison between normal and trojaned run
simulator, the wheel turning is measured in a real value from -1 to
1, and the model accuracy is measured by the sum of square error
between the predicted wheel turning angle and the ground truth
angle. The test error on the original data is the same as the original
mode, i.e., 0.018, while the test error is 0.393 when the trigger road
sign is in sight.
The attack can lead to accidents. A demo video can be found
in [11]. Some of the snapshots are shown in Figure 11. The first
row is the normal run. We can see that in the normal run, the car
keeps itself on track. The second row is the run with the trojan
trigger sign. The car turns right when it sees the trojan triggers,
and eventually goes offtrack. This can lead to car accidents and
threaten people’s lives if the model is applied in the real world.
7 POSSIBLE DEFENSES
In the previous sections, we have shown that the proposed trojan
attack on the neuron network models is very effective. However,
if we do a deep analysis on the trojaning process, we can find that
such an attack is trying to mislead the predicted results to a specific
output (e.g., a specific people or age group). Thus the model in
general will be more likely to give this output. Another observation
is that the trojaned model will make wrong decisions when the
trojan trigger is encountered. Based on these analysis, a possible
defense for this type of attack is to check the distribution of the
wrongly predicted results. For a trojaned model, one of the outputs
will take the majority. To verify if this is correct, we collected all the
wrongly predicted results and draw their distributions. Figure 12
show the distributions for the face recognition case. The left hand
side graph shows the distribution for the original model. As we can
see, it is almost a uniform distribution. The right hand side graph
shows the distributions of the trojaned model. Here target label 14
stands out. Other trojaned models show similar patterns. Thus we
believe such an approach can potentially detect such attacks.
8 RELATED WORK
Perturbation attacks on machine learning models have been studied
by many previous researchers [18, 25, 36, 43]. Szegedy et al. [25]
point out that neural network is very sensitive to small perturba-
tions and small and human unnoticeable perturbations can make
Figure 12: Comparison between normal and trojaned run
neural networks fail. Sharif et al. [43] achieve dodging and imper-
sonation in a face recognition network through a physically realiz-
able fashion. Carlini et al. [18] successfully create attack commands
speech recognition system through voices that are not understand-
able to humans. Our work differs from them in the following aspects.
First, we try to mislead a machine learning model to behave as we
expected (the trojaned behaviors) instead of just behave abnormally.
Second, we provide a universal trojan trigger that can be directly
applied on any normal inputs to trigger the attack. Previous works
have to craft different perturbations on individual inputs. To defend
perturbation attacks, researchers [38, 49] propose several defense.
Papernot et al. [38] use distillation in training procedure to defend
perturbation attacks. Xu et al. [49] recently proposed a technique
called feature squeezing which reduces the bit color or smooth
the image using spatial filter and thus limits the search space for
perturbation attack.
Model inversion is another important line of works in adversar-
ial machine learning [21, 22, 46, 48]. Fredrikson et al. [21, 22, 48]
inverse the Pharmacogenetics model, decision trees and simple neu-
ral network models to exploit the confidential information stored
in models. Tramèr et al. [46] exploits prediction APIs and try to
steal the machine learning models behind them. Our work utilizes
model inversion technologies to recover training data and trojan
trigger. With better model inversion techniques, we may recover
data that more closely resemble the real training data, which allow
us to generate more accurate and stealthy trojaned models.
Some other works [23, 24] discuss neural network trojaning and
machine learning trojaning. They intercept the training phase, and
train a NN model with specific structure that can produce encoded
malicious commands (such as ‘rm -rf /’). Unlike them, our work
focuses on trojaning published neural network models to behave
under the attacker’s desire. Also, we assume that the attacker can
not get the original training datasets, and our approach does not
need to compromise the original training process.
9 CONCLUSION
The security of public machine learning models has become a criti-
cal problem. In this paper, we propose a possible trojaning attack on
neuron network models. Our attack first generates a trojan trigger
by inversing the neurons, and then retrains the model with exter-
nal datasets. The attacker can inject malicious behaviors during
the retrain phase. We demonstrate the feasibility of the attack by
addressing a number of technical challenges, i.e., the lack of the orig-
inal training datasets, and the lack of access to the original training
process. Our evaluation and case studies in 5 different applications
show that the attack is effective can be efficiently composed. We
also propose a possible defense solution.
12
01-220221-440441-660661-880881-11001101-13201321-15401541-17601761-19801981-22012201-24202420-2641target label: 14movie-review-data/.
pannous/caffe-speech-recognition.