eration. Thus, we are careful to construct these URLs in the same
manner as the real Storm sites (whether this is raw IP addresses, as
used in the self-propagation campaigns, or the particular “noun-
noun.com” naming schema used by the pharmacy campaign) to
ensure the generated spam is qualitatively indistinguishable from
the “real thing”. An important exception, unique to the pharmacy
campaign, is an identiﬁer we add to the end of each URL by modi-
Figure 2: The Storm spam campaign dataﬂow (Section 3.3)
and our measurement and rewriting infrastructure (Section 4).
(1) Workers request spam tasks through proxies, (2) proxies
forward spam workload responses from master servers, (3)
workers send the spam and (4) return delivery reports. Our
infrastructure inﬁltrates the C&C channels between workers
and proxies.
In the remainder of this section we provide a detailed description
of our Storm C&C rewriting engine, discuss how we use this tool
to obtain empirical estimates for spam delivery, click-through and
conversion rates and describe the heuristics used for differentiating
real user visits from those driven by automated crawlers, honey-
clients, etc. With this context, we then review the ethical basis
upon which these measurements were conducted.
4.1 C&C protocol rewriting
Our runtime C&C protocol rewriter consists of two components.
A custom Click-based network element redirects potential C&C
trafﬁc to a ﬁxed IP address and port, where a user-space proxy
server implemented in Python accepts incoming connections and
impersonates the proxy bots. This server in turn forwards connec-
tions back into the Click element, which redirects the trafﬁc to the
intended proxy bot. To associate connections to the proxy server
with those forwarded by the proxy server, the Click element injects
a SOCKS-style destination header into the ﬂows. The proxy server
uses this header to forward a connection to a particular address and
port, allowing the Click element to make the association. From that
point on, trafﬁc ﬂows transparently through the proxy server where
C&C trafﬁc is parsed and rewritten as required. Rules for rewriting
can be installed independently for templates, dictionaries, and e-
mail address target lists. The rewriter logs all C&C trafﬁc between
worker and our proxy bots, between the proxy bots and the master
servers, and all rewriting actions on the trafﬁc.
Since C&C trafﬁc arrives on arbitrary ports, we designed the
proxy server so that it initially handles any type of connection and
falls back to passive pass-through for any non-C&C trafﬁc. Since
friend. Unlike the pharmacy example, we were not able to mirror
the graphical content of the postcard site, since it was itself stolen
from a legitimate Internet postcard site. Instead, we created a close
analog designed to mimic the overall look and feel. We also “de-
fanged” our site by replacing its link to the Storm malware with that
of a benign executable. If run, our executable is designed to per-
forms a simple HTTP POST with a harmless payload (“data=1”)
to a server under our control, and then exit. As a rough timeout
mechanism, the executable will not send the message if the sys-
tem date is 2009 or later. Since the postcard site we impersonated
served three different executables under different names, we served
three executables with different target ﬁlenames in the POST com-
mand as well. Again, all accesses to the site are logged and we are
able to identify when our binary has been downloaded. Moreover,
by correlating with the POST signal, we are able to determine if a
particular download is ultimately executed on the visitor’s machine
(and hence is a conversion). Downloads and executions can differ
because the user has second thoughts about allowing an execution
or because the user’s security software prevents it from executing
(indeed, we observed that several anti-virus vendors developed sig-
natures for our benign executable within a few days of our intro-
ducing it).
4.4 Separating users from crawlers
As with our e-mail accounts, not all visits to our Web site are
prospective conversions. There is a range of automated and semi-
automated processes that visit our sites, ranging from pure Web
crawlers, to “honeyclient” systems designed to gather intelligence
on spam advertised sites, to security researchers trying to identify
new malware.
To ﬁlter out such visits (which we generically call “crawlers”)
from intentful ones, we have developed a series of heuristics to
identify crawlers and use this data to populate a global IP blacklist
across all of our Web sites. We outline these heuristics below.
First, we consider all hosts that access the pharmacy site that
do not use a URL containing the unique identiﬁer discussed in
Section 4.3 to be crawlers. Second, we blacklist hosts that ac-
cess robots.txt (site-speciﬁc instructions meant only for Web
crawlers) and hosts that make malformed requests (most often ex-
ploit attempts). Third, we blacklist all hosts that disable javascript
and do not load embedded images. We assume that typical users
do not browse under these conditions, whereas some large-scale
anti-spam honeypots that follow embedded links in suspected spam
exhibit this behavior to reduce load.
In addition to blacklisting based on the behavior of individual
site visits, another common pattern we observed was the same IP
address accessing the pharmacy site using several different unique
identiﬁers, presumably as part of a spam defense or measurement
mechanism. Consequently, we blacklist an IP address seen access-
ing the pharmacy site with more than one unique identiﬁer with
the same User-Agent ﬁeld. This heuristic does not ﬁlter users
browsing behind larger Web proxy services, but does ﬁlter the ho-
mogeneous accesses seen from spam honeyclients. Similarly, we
also blacklist any host that requests the downloaded executable
from the postcard site ten or more times, under the assumption that
such hosts are used by researchers or other observers interested in
tracking updates to the Storm malware.
Finally, it has become common for anti-malware researchers to
ﬁnd new versions of the Storm malware by directly accessing the
self-propagation dictionary entries. To detect such users we in-
jected new IP addresses (never advertised in spam messages) into
the self-propagation dictionary during a period of inactivity (i.e.,
when no self-propagation spam was being sent). Any visitors to
(a) Pharmaceutical site
(b) Postcard-themed self-propagation site
Figure 3: Screenshots of the Web sites operated to measure
user click-through and conversion.
fying the associated spam template. This identiﬁer allows us to un-
ambiguously associate individual spam messages with subsequent
accesses to the site. We did not add this identiﬁer to the self-
propagation campaigns since their URLs typically consist entirely
of raw IP addresses. The addition of a text identiﬁer sufﬁx might
thus appear out of place, reducing verisimilitude, and perhaps bias
user click behavior.
Finally, we created two Web sites to mimic those used in the
associated campaigns (screenshots of these sites are shown in
Figure 3). The pharmaceutical site, primarily marketing “male-
enhancement” drugs such as Viagra, is a nearly-precise replica of
the site normally advertised by Storm down to using the same nam-
ing convention for the domains themselves. Our site mirrors the
original site’s user interface, the addition of products advertised for
sale to a “shopping cart”, and navigation up to, but not including,
the input of personal and payment information (there are a range
of complex regulatory, legal and ethical issues in accepting such
information). Instead, when a user clicks on “Checkout” we return
a 404 error message. We log all accesses to the site, allowing us
to determine when a visitor attempts to make a purchase and what
the content of their shopping cart is at the time. We assume that a
purchase attempt is a conversion, which we speculate is a reason-
able assumption, although our methodology does not allow us to
validate that the user would have actually completed the purchase
or that their credit card information would have been valid.
The self-propagation campaign is Storm’s key mechanism for
growth. The campaign entices users to download the Storm mal-
ware via deception; for example by telling them it is postcard soft-
ware essential for viewing a message or joke sent to them by a
Figure 4: Number of e-mail messages assigned per hour for
each campaign.
CAMPAIGN
DATES
Pharmacy Mar 21 – Apr 15
Mar 9 – Mar 15
Postcard
April Fool Mar 31 – Apr 2
WORKERS
31,348
17,639
3,678
Total
E-MAILS
347,590,389
83,665,479
38,651,124
469,906,992
Table 1: Campaigns used in the experiment.
these IP addresses could not have resulted from spam, and we there-
fore also added them to our crawler blacklist.
It is still possible that some of the accesses were via full-featured,
low-volume honeyclients, but even if these exist we believe they are
unlikely to signiﬁcantly impact the data.
4.5 Measurement ethics
We have been careful to design experiments that we believe are
both consistent with current U.S. legal doctrine and are fundamen-
tally ethical as well. While it is beyond the scope of this paper to
fully describe the complex legal landscape in which active security
measurements operate, we believe the ethical basis for our work
is far easier to explain: we strictly reduce harm. First, our instru-
mented proxy bots do not create any new harm. That is, absent
our involvement, the same set of users would receive the same set
of spam e-mails sent by the same worker bots. Storm is a large
self-organizing system and when a proxy fails its worker bots au-
tomatically switch to other idle proxies (indeed, when our proxies
fail we see workers quickly switch away). Second, our proxies are
passive actors and do not themselves engage in any behavior that
is intrinsically objectionable; they do not send spam e-mail, they
do not compromise hosts, nor do they even contact worker bots
asynchronously. Indeed, their only function is to provide a conduit
between worker bots making requests and master servers providing
responses. Finally, where we do modify C&C messages in transit,
these actions themselves strictly reduce harm. Users who click on
spam altered by these changes will be directed to one of our innocu-
ous doppelganger Web sites. Unlike the sites normally advertised
by Storm, our sites do not infect users with malware and do not col-
lect user credit card information. Thus, no user should receive more
spam due to our involvement, but some users will receive spam that
is less dangerous that it would otherwise be.
Figure 5: Timeline of proxy bot workload.
DOMAIN
hotmail.com
yahoo.com
gmail.com
aol.com
yahoo.co.in
sbcglobal.net