[32] M. Sahami, S. Dumais, D. Heckerman, and
E. Horvitz. A Bayesian approach to ﬁltering junk
e-mail. In Learning for Text Categorization: Papers
from the 1998 workshop, volume 62, 1998.
[33] G. Sakkis, I. Androutsopoulos, G. Paliouras,
V. Karkaletsis, C. Spyropoulos, and P. Stamatopoulos.
A memory-based approach to anti-spam ﬁltering for
mailing lists. Information Retrieval, 6(1):49–73, 2003.
[34] B. Stone-Gross, T. Holz, G. Stringhini, and G. Vigna.
The underground economy of spam: A botmaster˘2019s
perspective of coordinating large-scale spam
campaigns. In USENIX Workshop on Large-Scale
Exploits and Emergent Threats (LEET), 2011.
[35] Symantec’s MessageLabs Intelligence. Messagelabs
intelligence annual security report, 2010.
[36] M. Wong and W. Schlitt. Sender policy framework
(SPF) for authorizing use of domains in e-mail,
version 1. Technical report, RFC 4408, April, 2006.
tems can accidentally hit a spamtrap and cause the email
server to be blacklisted.
Our ﬁndings can be used to evaluate both the eﬀectiveness
and the impact of adopting this class of techniques. We
hope that the ﬁgures provided in this paper may help to
settle the long debate between advocates and opponents of
CR systems.
9. REFERENCES
[1] Barracuda. http://www.barracudacentral.org/.
[2] Challenge-Response Anti-Spam Systems Considered
Harmful. http://linuxmafia.com/faq/Mail/
challenge-response.htm.
[3] Composite Blocking List. http://cbl.abuseat.org/.
[4] ORBITrbl. http://www.orbitrbl.com/.
[5] Passive Spam Block List. http://psbl.surriel.com/.
[6] Sendio. http://www.sendio.com/.
[7] Spam and Open-Relay Blocking System.
http://www.sorbs.net/.
[8] Spam Arrest. http://www.spamarrest.com/.
[9] Spam Cannibal. http://www.spamcannibal.org/.
[10] SpamCop. http://www.spamcop.net/.
[11] The spamhaus project. http://www.spamhaus.org/.
[12] Total Block. http://www.totalblock.net/.
[13] Why are auto responders bad?
http://spamcop.net/fom-serve/cache/329.html.
[14] I. Androutsopoulos, J. Koutsias, K. Chandrinos,
G. Paliouras, and C. Spyropoulos. An evaluation of
naive bayesian anti-spam ﬁltering. 2000.
[15] A. Bergholz, J. Chang, G. Paaß, F. Reichartz, and
S. Strobel. Improved phishing detection using
model-based features. In Proc. of the Conference on
Email and Anti-Spam (CEAS), 2008.
[16] D. Bernstein. Internet mail 2000 (IM2000).
[17] D. Crocker. RFC822: Standard for ARPA Internet
Text Messages. Retrieved April, 7:2008, 1982.
[18] M. Delany. Domain-Based Email Authentication
Using Public Keys Advertised. In the DNS
(DomainKeys)”, RFC 4870, 2007.
[19] H. Drucker, D. Wu, and V. Vapnik. Support vector
machines for spam categorization. IEEE Transactions
on Neural Networks, 10(5):1048–1054, 1999.
[20] Z. Duan, Y. Dong, and K. Gopalan. Diﬀmail: A
Diﬀerentiated Message Delivery Architecture to
Control Spam. Technical report, 2004.
[21] D. Erickson, M. Casado, and N. McKeown. The
Eﬀectiveness of Whitelisting: a User-Study. In Proc.
of Conference on Email and Anti-Spam, 2008.
[22] C. Fleizach, G. Voelker, and S. Savage. Slicing spam
with occamˆa ˘A´Zs razor. CEASˆa ˘A ´Z07, 2007.
[23] S. Garriss, M. Kaminsky, M. Freedman, B. Karp,
D. Mazieres, and H. Re. Reliable email. In Proc. of
NSDI, 2006.
[24] S. Hao, N. Syed, N. Feamster, A. Gray, and S. Krasser.
Detecting spammers with SNARE: Spatio-temporal
network-level automatic reputation engine. In Proc. of
the 18th conference on USENIX security symposium,
pages 101–118. USENIX Association, 2009.
[25] J. Jung and E. Sit. An empirical study of spam traﬃc
and the use of DNS black lists. In Proc. of the 4th
424Summary Review Documentation for 
“Measurement and Evaluation of a Real World Deployment  
of a Challenge-Response Spam Filter” 
Authors: J. Isacenkova, D. Balzarotti 
Reviewer #1 
Strengths: Real world data of a deployed system; spam is still a hot 
topic. 
Weaknesses:  In  an  effort  to  be  objective,  the  authors  (willfully?) 
avoid  some  fairly  obvious  conclusions  about  challenge-response 
systems. Intuitively, the throughput numbers per company and per 
day seem low, so I wonder about the generality of the study.  
Comments to Authors: High-level: interesting paper that goes in-
depth  to  a  less  often  studies  spam  fighting  technique.  Despite  the 
author’s  claims,  I  believe  that  this  data  actually  speaks  quite 
negatively  about  CR-style  systems,  and  as  a  result,  will  be  of 
interest  to  the  community  looking  to  provide  numbers  to  what  it 
(likely) already intuitively believes.  
Specifically, in S5.1, I agree with your conclusion that there is not a 
good correlation of challenges sent to number of servers blacklisted, 
but I feel like this is missing the point. Any system that could result 
in your outgoing mail server being black listed seems inherently like 
a  bad  idea.  You  say  that  75%  of  servers  never  appeared  in  a 
blacklist, but that means that 25% did get blacklisted at some point! 
This typically requires some amount of proactive work on the part 
of the admin to unblacklist the mail server, or worse, debug why the 
outgoing  mail  is  getting  filtered  by  the  destination.  It  is  nice  to 
quantify  what  that  probability  is,  but  my  guess  is  that  these 
blacklisting  rates  will  be  enough  to  scare  off  most  administrators. 
Further,  the  reality  is  that  someone  could  maliciously  trick  a  CR 
system into getting black listed with 100% certainty, which is even 
worse.  
My belief is the back-scatter *rate* is a little bit of a red herring. 
Emails are ultimately very small volume in terms of bits on the wire 
relative  to  other  Internet  services.  What’s  more  of  interest  is  the 
number  of  messages  that  actually  annoy  non-CR  participating 
senders. Have you considered doing a user-survey to try to estimate 
this?  
Judging  from  back-of-the-envelope  numbers  from  talking  to  local 
campus mail admins, 90M mails in 6 months for 20K people seems 
on the low end (~25 emails per person day, including spam, so at 
90%  spam,  ~3  non-spam  messages/person/day?).  How  does  your 
study compare in terms of scale (emails per person per unit time) of 
non-CR previous work?  
The  data  in  figure  9  would  have  been  nice  to  see  also  as  a  time 
series to understand how bursty white list changes are.  
Reviewer #2 
Strengths: Real world data; detailed analysis of a less understood 
spam fighting  
Weaknesses: Strong conclusions from insufficient. 
Comments  to  Authors:  I  was  most  concerned  about  the  server 
blacklisting, 25% seems high. Isn’t that the most negative argument 
against  CR  systems?  25%  of  mail  servers  means  a  lot  of  email 
delays,and lots of offline work to get them off the blacklist.  
Even if there is a large amount of backscatter challenges being sent 
to innocent users, is it really such a big deal from a network traffic 
point  of  view?  I  would  be  more  worried  about  other  costs  to  the 
innocent users, rather than unwanted traffic volumes. 
Reviewer #3 
Strengths: The results presented in the paper help to illustrate the 
impacts of using Challenge-Response system with real-world data. 
It provides useful information for making decision on adopting the 
system. 
Weaknesses:  While  analyzing  real-world  data,  authors  shy  away 
from drawing any conclusion and stop at providing numbers. Some 
of analysis is incomplete and lacks depth. See below for detail. 
Comments  to  Authors:  Overall,  this  paper  is  written  well.  It  is 
interesting to see the results presented from the viewpoints of a user, 
a system administrator, and the Internet.  
In  section  3.1,  on  understanding  backscattered  ratio,  clearly  it 
depends on the effectiveness of spam filter. It is not clear the result 
of  19.3%  backscattered  ratio  reflects  the  spam  filter  or  the 
effectiveness of the CR system. On the other hand, CR system and 
spam  filter  collectively  fight  spams.  High  backscattered  ratio  will 
bother many “innocent senders”. The tradeoff between accuracy of 
spam filter and backscatter spam will be interesting.  
As part the message or conclusion of the measurement result, I find 
that  only  4%  of  challenges  to  be  legitimate  bothersome  (section 
3.2). Furthermore, 45% of challenges seem to belong to the case that 
the  challenges  are  sent  to  “innocent  senders”.  This  effectively 
introduces spams to the system. Furthermore, how would spam filter 
treat these newly introduced spams. Could it be that the spam filter 
has already filtered those challenge questions? 
Figure 5 is a bit confusing to read. The plots below diagonals should 
be explained further. 
Reviewer #4 
Strengths: The main strength of this paper is that it is the first study 
of  real  world  deployment  of  challenge  response  spam  filters.  The 
evaluation covers a lot of data accumulated from different sources 
(47 companies of different sizes). 
Weaknesses: The main weakness of the paper is that it presents the 
results  while  reserving  judgment  on  the  results.  The  paper  would 
425have been stronger if the authors explained how some results were 
good or bad when using CR systems. Also, in addition to evaluation 
of an existing CR system, the authors should have presented how 
certain design choices affect the results, and how CR systems could 
be improved. 
Comments  to  Authors:  This  paper  is  a  good  first  step  in 
understanding the impact of CR systems in real world. The paper 
can  be  made  stronger  by  providing  guidance/insights  regarding 
choices  to  make  in  design/deployment  that  will  alleviate  the 
problems of CR systems. 
Reviewer #5 
Strengths:  Cool  dataset,  and  analysis  that  provides  some  helpful 
numbers  where  people  have  often  speculated.  The  paper  looks  at 
most aspects that came to mind while I read the paper. 
Weaknesses: The studied systems are a niche solution and unlikely 
ever  to  make  a  real  difference  (which,  to  their  credit,  the  authors 
acknowledge right from the start). The paper’s execution is pretty 
rough at times and the presentation confusing.  
Comments  to  Authors:  In  Section  2,  it  would  help  if  you  could 
describe  your  datasets  earlier.  I  found  it  confusing  to  read  about 
results  of  open/non-open  relays  before  you  describe  the  actual 
dataset. Similarly, it’d be nice to see the CR’s architecture before 
presenting any results.  
What  size  are  the  companies  that  have  deployed  the  system?  In 
which country/countries are they located?  
90 million messages isn’t exactly “almost 100 million”, as you’re 
stating 
the  “General  Statistics”  subsection.  In  Table  1, 
percentages would help; same for Figure 3.  
In  3.1,  separate  notation  for  the  two  notions  of  R  (accepted 
deliveries vs attempted deliveries) would help.  
You need to define clearly what you mean by backscattered emails. 
In Section 3, you first describe them as “challenge emails sent out 
by  a  challenge-response  system”.  Later  it  becomes  clear  that  you 
mean  only  *misdirected*  challenge  messages.  Then  you  conflate 
reflection ratio and backscatter ratio (which you mention in 3.1 but 
don’t define until 3.2), etc. I found this quite frustrating.  
I’m not sure whether looking at Figure 5 I should be in awe or send 
the information visualization police and have you all arrested on the 
spot.  For  sure,  it’s  far  too  complicated  and  busy,  because  I  keep 
needing to go back and forth between text and plot to understand 
what  it  shows.  For  example,  I  remained  confused  about  how 
negative numbers could possibly represent a ratio until you explain 
that the numbers in fact are correlations. All I learn about the sizes 
of  the  companies  in  your  dataset  is  that  most  (I  can’t  tell  the 
percentage,  given  the  useless  y-axis  label)  have  less  than  500 
employees -- that’s really rather coarse information. The alternating 
axis label locations are the icing on the cake. You have room in the 
paper,  why  not  break  the  chart  apart  into  individual,  semantically 
coherent ones?  
The  description  of  Figure  7  in  4.2  doesn’t  match  what  Figure  7 
shows  in  any  straightforward  way.  I  don’t  see  how  your 
observations  regarding  Figures  7  and  8  affect  the  numbers  you 
report in the three bullet points ending Section 4.2.  
in 
I like 4.3. It reflects that the degree to which CR works depends a 
lot on the interplay of users’ involvement, on *both* sides. Some 
recipients may diligently weed out their digest, while others may let 
it grow and hope for the senders to solve the problem by solving the 
CAPTCHAs. The flip-side applies to the senders -- why would they 
solve  the  CAPTCHA  right  away  when  there’s  a  chance  that  the 
recipient is diligent and will see it in the digest anyway?  
In Section 5 you start speaking about invitations. Are you referring 
to challenge messages?  
The writing needs a lot of work. Fix: “the same Symantec’s report” 
-- do you mean “the same year’s Symantec report”? “1% of false 
positive”,  “possible  conspicuous”,  “users  that  may  [...]  receives”, 
“we  collected  statics”,  “let’s”,  “challenge  messages  were  never 
open”,  “between  every  pairs”,  “one  of  the  most  effective  way”, 
“withelists”,  “messages  where  bounced”.  Broken  reference  (or 
cut’n’paste  bug?)  in  4.2.  :(  “SpamHause”,  “do  not  exists”,  “a 
challenge-response systems”). 
Response from the Authors 
First of all, we want to thank all the reviewers for their insightful 
comments  on  our  work.  According  to  most  of  them,  the  main 
weakness  of  the  paper  was  the  lack  of  conclusions  based  on  the 
performed experiments.  Even though our main goal was to present 
and  discuss  the  data,  and  not  to  draw  personal  conclusions,  we 
decided to address the reviewers’ comments by adding some overall 
conclusions to the discussion section. 
Another point raised by two reviewers was that Figure 5 was hard to 
understand. We took this comment into account by better presenting 
those results and by adding a detailed description of each part of the 
figure. 
We also found extremely useful all the suggestions about the ways 
in  which  we  could  improve  the  paper  presentation  and  the 
description  of  the  data.  In  particular,  according  to  the  authors 
comments,  we  completely  reorganized  section  2,  corrected  the 
confusing definitions of the backscattering and reflection ratios, and 
clarified the results on the impact on emails delivery (Section 4.2). 
Furthermore,  we  stressed  some  of  the  points  (e.g.,  the  fact  that 
misdirected challenges could be filtered out by spam filters) and we 
added  more  precise  comments  on  possible  inaccuracy  of  some  of 
the presented figures. 
The  first  reviewer  mentioned  the  fact  that  90M  emails  for  20K 
accounts  seems  on  the  very  low  end.  This  misunderstanding  is 
probably due to the fact that the 20K accounts also include a large 
number  of  internal  mailboxes  used  for  administrative  purposes, 
accounts of previous employees, and mailboxes that were inactive 
during the period of our experiments. Additionally, as we already 
mentioned in the paper, not all of the servers were monitored for 6 
months (the period during which we had access to the data varied 
between 2 and 6 months). And lastly, some of the accounts received 
hundreds of emails per day, while others only few per week, thus 
accounting for the low average mentioned by the reviewer. 
Finally,  we  could  not  address  few  comments  (e.g.,  about  the  size 
and  physical  location  of  the  companies)  in  order  to  preserve  the 
privacy  of  the  data  providers.  In  some  cases  we  were  not  able  to 
evaluate some of the reviewers proposals (e.g., by trying different 
spam  filters)  that,  even  though  extremely  interesting,  would  have 
required us to change the production servers. 
426