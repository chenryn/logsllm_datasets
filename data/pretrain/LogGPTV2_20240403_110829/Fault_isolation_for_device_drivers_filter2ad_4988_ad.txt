r
a
m
m
is
sio
n
ete
r
a
n
d
o
m
Fault Type Injected (1000 x 100 each)
Figure 8: Number of driver failures per fault type. In total, this
experiment injected 3,200,000 faults and caused 24,883 failures.
Next, we analyzed the nature and frequency of unautho-
rized access attempts and correlated the results to the clas-
siﬁcation in Fig. 3. While MINIX 3 has many sanity checks
in the system libraries linked into the driver, we focused
on the logs from the kernel and driver manager, since their
checks cannot be circumvented. Below, we report on an ex-
periment with the RTL8139 driver that conducted 100,000
SWIFI trials injecting 1 RANDOM fault each.
In total, the driver manager detected 5887 failures that
caused the RTL8139 driver to be replaced: 3,738 (63.5%)
exits due to internal panics, 1,870 (31.8%) crashes due to
exceptions, and 279 (4.7%) kills due to missing heartbeats.
However, since not all error conditions were immediately
fatal, the number of unauthorized access attempts logged by
the kernel could be up to three orders of magnitude higher,
as shown in Fig. 9. For example, we found 1,754,886 unau-
thorized DEVIO calls attempting to access device registers
that do not belong to the RTL8139 PCI card. Code inspec-
tion conﬁrmed that the driver repeatedly retried failed oper-
ations before giving up with an internal panic or causing an
exception due to subsequent fault injections.
Each type of violation maps onto one or more classes
of powers listed in Figure 3. For instance, CPU exceptions
are a Class I violation that is caught by the corresponding
Class I restrictions. Likewise, invalid memory grants and
MMU exceptions fall in Class II, unauthorized device I/O
matches Class III, and unauthorized IPC and kernel calls
are examples of Class IV. While not all subclasses are rep-
resented in Fig. 9, the logs showed that our isolation tech-
niques were indeed effective in all subclasses.
Unauthorized Access
1. Unauthorized device I/O
2. Unauthorized kernel call
3. Unauthorized IPC call
4. Invalid memory grant
5. CPU or MMU exception
Total violations detected
Count
1,754,886
322,005
66,375
17,008
1,780
2,162,054
Percentage
81.2%
14.9%
3.1%
0.8%
0.1%
100.0%
Figure 9: Top ﬁve unauthorized access attempts by the RTL8139
PCI driver for a test run with 100,000 randomly injected faults.
7.3 Availability under Faults
We also measured how many faults—injected one after
another—it takes to disrupt the driver and how many more
are needed for a crash. Disruption means that the driver can
no longer successfully handle network I/O requests, but has
not yet failed in a way detectable by the driver manager.
Injected faults do not always cause an error, since the faults
might not be on the path executed. As described in Sec. 6.3,
a connection to a remote server was used to keep the driver
busy and check for availability after each trial.
n
o
i
t
u
b
i
r
t
s
D
i
 750
 625
 500
 375
 250
 125
 0
0
5
10
15
20
25
30
35
40
45
50
# Random Faults Injected
100 %
75 %
50 %
25 %
0 %
l
e
v
i
t
a
u
m
u
C
# Disrupted
% Disrupted
# Crashed
% Crashed
Figure 10: Number of faults needed to disrupt and crash the NE2000 ISA driver, based on 100,000 randomly injected faults. We observed
664 disruptions and 136 crashes after 1 fault. Crashes show a long tail to the right and surpass 99% only after 250 faults.
Fig. 10 shows the distribution of the number of faults
needed to disrupt and crash the NE2000 driver for 100,000
SWIFI trials injecting 1 RANDOM fault each. Disruption
usually happens after only a few faults, but the number of
faults needed to induce a crash can be high. For example,
we observed 664 disruptions after 1 fault, whereas one run
required 2484 faults before the driver crashed. On average,
the driver failed after 7 faults and crashed after 10 faults.
7.4 Problems Encountered
As mentioned above, we have taken a pragmatic ap-
proach toward dependability and went through several de-
sign iterations before we arrived at the ﬁnal system. In order
to underline this point, Fig. 11 brieﬂy summarizes some of
the problems that we encountered (and subsequently ﬁxed)
during the SWIFI testing of MINIX 3.
Interestingly, we
found many rare bugs even though the system was already
designed for dependability [17], which illustrates the use-
fulness of extensive fault injection.
scheduling queues (bug in scheduler)
undetected and fails (bug in IPC subsystem)
ing forever (bug in IPC subsystem)
reply (all IPC to untrusted code now is asynchronous)
(cid:129) Kernel stuck in inﬁnite loop in load update due to inconsistent
(cid:129) Driver causes process manager to hang by not receiving synchronous
(cid:129) Driver request to perform SENDREC with nonblocking ﬂag goes
(cid:129) IPC call to SENDREC with target ANY not detected and kept pend-
(cid:129) Illegal IPC destination (ANY) for NOTIFY call caused kernel panic
(cid:129) Kernel panic due to exception caused by uninitialized struct priv
(cid:129) Network driver went into silent mode due to bad restart parameters
(cid:129) Inﬁnite loop in driver not detected because driver manager’s priority
(cid:129) System-wide starvation due to excessive kernel debug messages
(cid:129) Isolation policy allowed arbitrary memory copies, which corrupted
(cid:129) Driver reprogrammed RTL8139 hardware’s PCI device ID (code
(cid:129) Wrong IOMMU setting caused legitimate DMA read by the disk
was set too low to ping driver and check its heartbeat
rather than erroneous return (bug in IPC subsystem)
pointer in system task (bug in kernel call handler)
INET (isolation policy violated least authority)
was present in driver, now removed)
controller to fail and corrupt the ﬁle system
Figure 11: Bugs found during SWIFI testing of MINIX 3.
8 LESSONS LEARNED
Our experiments resulted in several insights that are
worth mentioning. To start with, the fault injection proved
very helpful in ﬁnding programming bugs, as shown in
Fig. 11. An interesting observation, however, is that some
hard-to-trigger bugs showed up only after several design it-
erations and injecting many millions of faults. In the past,
similar efforts often limited their tests to a few thousands
of fault injections, which may not be enough to trigger rare
faults. For example, Nooks [36] and Safedrive [38] reported
only 2000 and 44 fault-injection trials, respectively.
Although this work focuses on mechanisms rather than
policies, policy deﬁnition is a hard problem. At some point,
the driver’s policy accidentally granted access to a kernel
call for copying arbitrary memory without grants, caus-
ing memory corruption in the network server. We ‘man-
ually’ reduced the privileges granted, but techniques such
as formalized interfaces [30] and compiler-generated mani-
fest [33] may be helpful to deﬁne correct policies.
Furthermore, while our design makes the system as a
whole more robust, availability of individual services can-
not be guaranteed due to hardware limitations. In a very
small number of cases, less than 0,1% of all NE2000 ISA
driver crashes, the NE2000 ISA card was put in an unre-
coverable state and could not be reinitialized by the driver.
Instead, a low-level BIOS reset was needed. If the card had
a ‘master reset’ command, the driver could have solved the
problem, but our card did not have this.
Finally, we had to abandon one experiment due to an
insurmountable hardware limitation: tests with a driver for
the Realtek RTL8029 PCI card caused the entire system to
freeze. We narrowed down the problem to writing a speciﬁc
(unexpected) value to an (allowed) control register of the
device—presumably causing a PCI bus hang. We believe
this to be a peculiarity of the speciﬁc device or weakness of
the PCI bus rather than a shortcoming of our design.
In summary, however, the results show that fault iso-
lation and failure resilience [17] indeed help to survive
bugs and enable on-the-ﬂy recovery. While we have used
MINIX 3, many of our ideas are generally applicable and
may also bring improved dependability to other systems.
9 SUMMARY & CONCLUSION
This paper investigates the privileged operations that
low-level device drivers need to perform and that, unless
properly restricted, are root causes of fault propagation. We
showed how MINIX 3 systematically restricts drivers ac-
cording to the principle of least authority in order to limit
the damage that can result from bugs. In particular, fault
isolation is achieved through a combination of structural
constraints imposed by a multiserver design, ﬁne-grained
per-driver isolation policies, and run-time memory grant-
ing. We believe that many of these techniques are generally
applicable and can be ported to other systems.
We have taken an empirical approach toward dependabil-
ity and have iteratively reﬁned our isolation techniques us-
ing software-implemented fault-injection (SWIFI) testing.
We targeted 4 different Ethernet driver conﬁgurations using
both programmed I/O and DMA. While we had to work
around certain hardware limitations, the resulting design
was able to withstand 100% of 3,400,000 randomly injected
faults that were shown to be representative for typical pro-
gramming errors. The targeted drivers repeatedly failed, but
the rest of the OS was never affected.
ACKNOWLEDGMENTS
Supported by Netherlands Organization for Scientiﬁc
Research (NWO) under grant 612-060-420.
REFERENCES
[1] H. Bos and B. Samwel. Safe Kernel Programming in the OKE. 2002.
[2] R. Chillarege, I. Bhandari, J. Chaar, M. Halliday, D. Moebus, B. Ray,
and M.-Y. Wong. Orthogonal Defect Classiﬁcation-A Concept for
In-Process Measurements. IEEE TSE, 18(11):943–956, 1992.
[3] A. Chou, J. Yang, B. Chelf, S. Hallem, and D. Engler. An Empirical
Study of Operating System Errors. In Proc. 18th SOSP, 2001.
[4] J. Christmansson and R. Chillarege. Generation of an Error Set that
Emulates Software Faults–Based on Field Data. In Proc. 26th FTCS,
1996.
[5] T. Dinh-Trong and J. M. Bieman. Open Source Software Devel-
In Proc. 10th Int’l Symp. on
opment: A Case Study of FreeBSD.
Software Metrics, 2004.
[6] J. Duraes and H. Madeira. Emulation of Software Faults: A Field
Data Study and a Practical Approach. IEEE TSE, 32(11):849–867,
2006.
[7] K. Elphinstone, G. Klein, P. Derrin, T. Roscoe, and G. Heiser. To-
wards a Practical, Veriﬁed Kernel. In Proc. 11th HotOS, 2007.
[8] U. Erlingsson, M. Abadi, M. Vrable, M. Budiu, and G. C. Necula.
In Proc. 7th
XFI: Software Guards for System Address Spaces.
OSDI, 2006.
[9] K. Fraser, S. Hand, R. Neugebauer, I. Pratt, A. Warﬁeld, and
M. Williamson. Safe Hardware Access with the Xen Virtual Ma-
chine Monitor. In Proc. 1st OASIS, 2004.
[10] A. Ganapathi, V. Ganapathi, and D. Patterson. Windows XP Kernel
Crash Analysis. In Proc. 20th LISA, 2006.
[11] A. Gefﬂaut, T. Jaeger, Y. Park, J. Liedtke, K. Elphinstone, V. Uh-
lig, J. Tidswell, L. Deller, and L. Reuther. The SawMill Multiserver
Approach. In Proc. 9th ACM SIGOPS European Workshop, 2000.
[12] D. B. Golub, G. G. Sotomayor, Jr, and F. L. Rawson III. An Archi-
tecture for Device Drivers Executing as User-Level Tasks. In Proc.
USENIX Mach III Symp., 1993.
[13] J. Gray. Why Do Computers Stop and What Can Be Done About It?
In Proc. 5th SRDS, 1986.
[14] H. H¨artig, M. Hohmuth, J. Liedtke, S. Sch¨onberg, and J. Wolter. The
Performance of µ-Kernel-Based Systems. In Proc. 6th SOSP, 1997.
[15] H. H¨artig, M. Hohmuth, N. Feske, C. Helmuth, A. Lackorzynski,
F. Mehnert, and M. Peter. The Nizza Secure-System Architecture. In
Proc. 1st Int’l Conf. on Collaborative Computing, 2005.
[16] L. Hatton. Reexamining the Fault Density-Component Size Connec-
tion. IEEE Software, 14(2), 1997.
[17] J. N. Herder, H. Bos, B. Gras, P. Homburg, and A. S. Tanenbaum.
Failure Resilience for Device Drivers. In Proc. 37th DSN, 2007.
[18] J. N. Herder, H. Bos, B. Gras, P. Homburg, and A. S. Tanenbaum.
Countering IPC Threats in Multiserver Operating Systems. In Proc.
14th PRDC, 2008.
[19] G. Hunt, C. Hawblitzel, O. Hodson, J. Larus, B. Steensgaard, and
T. Wobber. Sealing OS Processes to Improve Dependability and
Safety. In Proc. 2nd EuroSys, 2007.
[20] B. Leslie, P. Chubb, N. Fitzroy-Dale, S. Gotz, C. Gray, L. Macpher-
son, D. Potts, Y.-T. Shen, K. Elphinstone, and G. Heiser. User-Level
Device Drivers: Achieved Performance. Journal of Comp. Science
and Techn., 20(5), 2005.
[21] J. LeVasseur, V. Uhlig, J. Stoess, and S. Gotz. Unmodiﬁed Device
Driver Reuse and Improved System Dependability via Virtual Ma-
chines. In Proc. 6th OSDI, 2004.
[22] J. Liedtke. On µ-Kernel Construction. In Proc. 15th SOSP, 1995.
[23] A. Mancina, G. Lipari, J. N. Herder, B. Gras, and A. S. Tanenbaum.
Enhancing a Dependable Multiserver OS with Temporal Protection
via Resource Reservations. In Proc. 16th RTNS, 2008.
[24] F. M´erillon, L. R´eveill`ere, C. Consel, R. Marlet, and G. Muller.
Devil: An IDL for Hardware Programming. In Proc. 4th OSDI, 2000.
[25] Microsoft Corporation. Architecture of the User-Mode Driver
Framework. In Proc. 15th WinHEC, 2006.
[26] B. Murphy. Automating Software Failure Reporting. ACM Queue, 2
(8), 2004.
[27] W. T. Ng and P. M. Chen. The Systematic Improvement of Fault
Tolerance in the Rio File Cache. In Proc. 29th FTCS, 1999.
[28] V. Orgovan. Online Crash Analysis - Higher Quality At Lower Cost.
In Presented at 13th WinHEC, 2004.
[29] Y. Padioleau, J. L. Lawall, and G. Muller. Understanding Collateral
Evolution in Linux Device Drivers. In Proc. 1st EuroSys, 2006.
[30] L. Ryzhyk, P. Chubb, I. Kuz, and G. Heiser. Dingo: Taming Device
Drivers. In Proc. 4th EuroSys Conf., 2009.
[31] J. Saltzer and M. Schroeder. The Protection of Information in Com-
puter Systems. Proc. of the IEEE, 63(9), 1975.
[32] M. I. Seltzer, Y. Endo, C. Small, and K. A. Smith. Dealing with
In Proc. 2nd
Disaster: Surviving Misbehaved Kernel Extensions.
OSDI, 1996.
[33] M. Spear, T. Roeder, O. Hodson, G. Hunt, and S. Levi. Solving the
In
Starting Problem: Device Drivers as Self-Describing Artifacts.
Proc. 1st EuroSys, 2006.
[34] J. Sugerman, G. Venkitachalam, and B.-H. Lim. Virtualizing I/O
Devices on VMware Workstation’s Hosted Virtual Machine Monitor.
In Proc. USENIX’01, 2001.
[35] M. Sullivan and R. Chillarege. Software Defects and their Impact
on System Availability – A Study of Field Failures in Operating Sys-
tems. In Proc. 21st FTCS, 1991.
[36] M. Swift, B. Bershad, and H. Levy.
Improving the Reliability of
Commodity Operating Systems. ACM TOCS, 23(1), 2005.
[37] J. Xu, Z. Kalbarczyk, and R. K. Iyer. Networked Windows NT Sys-
tem Field Failure Data Analysis. In Proc. 6th PRDC, 1999.
[38] F. Zhou, J. Condit, Z. Anderson, I. Bagrak, R. Ennals, M. Harren,
G. Necula, and E. Brewer. SafeDrive: Safe and Recoverable Exten-
sions Using Language-Based Techniques. In Proc. 7th OSDI, 2006.