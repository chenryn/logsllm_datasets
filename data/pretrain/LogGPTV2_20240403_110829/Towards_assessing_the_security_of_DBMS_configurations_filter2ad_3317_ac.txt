### APPLICATION LEVEL CONFIGURATION AND USAGE

1. **System Tables:**
   - Compile a list of all system tables that are not created for application use.
   - For each table, verify if any user has read or write permissions.
   - Evaluate whether these permissions are clearly justified and necessary.

2. **System Databases:**
   - Create a list of all system databases.
   - For each database, check if any user has permissions.
   - Determine if these permissions are clearly justified and necessary.

3. **Non-DBA User Permissions:**
   - For each non-DBA user, list all their permissions.
   - For each permission, ensure it is clearly justified and essential for the user's work.
   - Check if the permission is of type ANY or ALL, which would automatically propagate to other objects of the same type.
   - Verify if the user can grant this permission to another user.
   - Confirm if the permission allows the user to change critical or globally valid system configurations.
   - Identify if the user inherits the permission from a group or role they are assigned to.

4. **Schema Information:**
   - List all documents and files containing schema information.
   - For each document or file, determine if it is stored on the DB server.

### Test Results and Implications

It is important to note that there is no hierarchical order of importance among the tests. A failed test indicates that the DBMS/OS configuration may allow certain types of attacks under specific conditions. However, these conditions depend not only on the DBMS but also on the OS and its usage. Therefore, a failed test might not always be significant or even relevant. The DBA must investigate the cause of any failed test and, when appropriate, take actions to mitigate potential security risks.

### Assessing Real-Case Scenarios

To demonstrate the proposed approach, we applied our tests to four real DBMS installations using different engines. Table 3 provides details about each installation, including the DBMS engine, operating system, number of applications, DBAs, developers, and the duration of the test execution. The results are presented in Tables 4, 5, 6, and 7, and the detailed results can be found in [12].

| DBMS          | OS              | Applications | DBAs | Developers | Test Duration |
|---------------|-----------------|--------------|------|------------|---------------|
| SQLServer 2005 | Windows 2003    | 3            | 5    | 3          | 3 hours       |
| MySQL 5.0     | Windows 2003    | 2            | 39   | 2          | 1.5 hours     |
| Oracle 10g    | Windows XP      | 8            | 3    | 0          | 1 hour        |
| PostgreSQL 8.1| Windows 2000    | 54           | 2    | 0          | 1 hour        |

The test set was applied by two different people: one under the direct supervision of the authors (Cases 1 and 3) and another independently (Cases 2 and 4). In the evaluation of Cases 2 and 4, the person performing the evaluation had only the content of Tables 1 and 2 as a reference. All four databases are used in an academic context at two universities, primarily supporting administrative applications with staff, teachers, and students as users.

#### Case 1: Oracle 10g Installation
| Category                  | Tests Passed | Tests Failed | Unknown | Total |
|---------------------------|--------------|--------------|---------|-------|
| Environment               | 75.00%       | 0.00%        | 2       | 8     |
| Installation setup        | 73.33%       | 0.00%        | 2       | 15    |
| Operational Procedures    | 66.67%       | 0.00%        | 1       | 3     |
| System level configuration| 32.00%       | 8.00%        | 0       | 25    |
| App. level conf. and usage| 87.50%       | 3.39%        | 0       | 8     |
| **Total**                 | 40.68%       | 0.00%        | 2       | 59    |

#### Case 2: SQLServer 2005 Installation
| Category                  | Tests Passed | Tests Failed | Unknown | Total |
|---------------------------|--------------|--------------|---------|-------|
| Environment               | 50.00%       | 50.00%       | 0.00%   | 8     |
| Installation setup        | 33.33%       | 60.00%       | 6.67%   | 15    |
| Operational Procedures    | 66.67%       | 33.33%       | 0.00%   | 3     |
| System level configuration| 48.00%       | 48.00%       | 4.00%   | 25    |
| App. level conf. and usage| 37.50%       | 62.50%       | 0.00%   | 8     |
| **Total**                 | 44.07%       | 52.54%       | 3.39%   | 59    |

#### Case 3: MySQL 5.0 Installation
| Category                  | Tests Passed | Tests Failed | Unknown | Total |
|---------------------------|--------------|--------------|---------|-------|
| Environment               | 37.50%       | 62.50%       | 0.00%   | 8     |
| Installation setup        | 46.67%       | 53.33%       | 0.00%   | 15    |
| Operational Procedures    | 73.33%       | 26.67%       | 0.00%   | 3     |
| System level configuration| 44.00%       | 52.00%       | 4.00%   | 25    |
| App. level conf. and usage| 50.00%       | 50.00%       | 0.00%   | 8     |
| **Total**                 | 44.07%       | 54.24%       | 1.69%   | 59    |

#### Case 4: PostgreSQL 8.1 Installation
| Category                  | Tests Passed | Tests Failed | Unknown | Total |
|---------------------------|--------------|--------------|---------|-------|
| Environment               | 37.50%       | 62.50%       | 0.00%   | 8     |
| Installation setup        | 26.67%       | 73.33%       | 0.00%   | 15    |
| Operational Procedures    | 33.33%       | 66.67%       | 0.00%   | 3     |
| System level configuration| 36.00%       | 56.00%       | 8.00%   | 25    |
| App. level conf. and usage| 75.00%       | 25.00%       | 0.00%   | 8     |
| **Total**                 | 38.98%       | 57.63%       | 3.39%   | 59    |

### Discussion of Results

- **Case 1 Compliance:** The configuration in Case 1 adheres to more security best practices than the others, indicating fewer potential paths for security attacks.
- **Installation Setup Group:** There were low pass rates in the Installation Setup group across all cases, likely due to default settings, lack of file system partition planning, and the use of operating systems that do not easily track file permissions.
- **Unanimous Results:** Some tests passed or failed unanimously across all cases, highlighting common issues and best practices.

### Conclusions

This paper proposes an approach to assess the effectiveness of DBMS configurations concerning security, based on a set of security best practices derived from the CIS series of security configuration documents. The approach involves a set of configuration tests that can be executed semi-automatically. The method has been successfully applied to evaluate four real installations, revealing several configuration problems and demonstrating the ease and importance of the approach.

### References

[1] A. Wool, "A quantitative study of firewall configuration errors," Computer, vol. 37, pp. 62-67, 2004.  
[2] B. Schneier, Applied Cryptography: Protocols, Algorithms, and Source Code in C, Second Edition. Wiley, 1996.  
[3] C. Kaufman, R. Perlman, and M. Speciner, Network Security: Private Communication in a Public World (2nd Edition). Prentice Hall PTR, 2002.  
[4] Commission of the European Communities, “Information Technology Security Eval. Manual (ITSEM)”, 1993.  
[5] Common Criteria, “Common Criteria for Information Technology Security Evaluation: User Guide”, 1999.  
[6] Department of Defense, “Trusted Computer System Evaluation Criteria”, 1985.  
[7] E. Bertino, S. Jajodia, and P. Samarati, “Database security: Research and practice”, Information Systems Journal, vol. 20, Number 7, 1995.  
[8] G. Pernul and G. Luef, “Bibliography on database security”, ACM SIGMOD Rec., vol. 21, Issue 1, 1992.  
[9] J. Yan, A. Blackwell, R. Anderson, A. Grant, "The Memorability and Security of Passwords - Some Empirical Results", Tech. Report 500, Computer Lab, Cambridge, 2000. http://www.ftp.cl.cam.ac.uk/ftp/rja14/tr500.pdf  
[10] Sandia National Laboratories, “Information Operations Red Team and Assessments™”.  
[11] W. Stallings, Cryptography and Network Security. Prentice Hall, 4th edition. 2005.  
[12] A. Neto, M. Vieira, “DBMS Configuration Security Assessment Results”, 2007, http://eden.dei.uc.pt/~mvieira/.  
[13] Vieira, M., Madeira, H., “Towards a Security Benchmark for Database Management Systems”, Intl Conf. on Dependable Systems and Networks, Yokohama, Japan, 2005.