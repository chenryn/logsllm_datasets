No 
No 
No 
Yes 
Yes 
Yes 
Yes 
Yes 
Yes 
APPLICATION LEVEL CONFIGURATION AND USAGE 
Make a list of all system tables (not created for use with applications). For 
each one, check if there is any user with some permission (read or write) over 
it. Are those permissions clearly justified and necessary? 
Make a list of all system databases. For each element on the list, check if 
there is any user with some permission over it. Is this permission clearly 
justified and necessary? 
For each non-DBA userid, list all its permissions. For each permission, does it 
have a clear justification? Is it impossible for the user to work without it? 
For each non-DBA userid, list all its permissions. For each permission, is it of 
type ANY or ALL, which would automatically propagate to other objects of the 
same type? 
For each non-DBA userid, list all its permissions. For each permission, does it 
allow that user to grant it to another userid? 
For each non-DBA userid, list all its permissions. For each permission, does it 
allow that user to change some system configuration which is either critical or 
valid to the whole DB? 
For each non-DBA userid, list all its permissions. For each permission, does 
the userid inherit it from a group or role he is assigned to? 
List all documents and files that contain any schema information. For each 
one, is it stored in the DB server? 
No 
No 
No 
Yes 
Yes 
Yes 
No 
Yes 
It is important to note that there is no relative order 
of  importance  between  tests.  Therefore,  what  a  failed 
test means is that the configuration of the DBMS/OS is 
such  that,  under  certain  conditions,  it  may  allow  some 
kind  of  attack.  Notice,  however,  that  these  particular 
conditions do not depend solely on the DBMS in ques-
tion, but also on the OS and the way it is used, meaning 
that under some circumstances a failed test might make 
no  difference  at  all  or  even  make  no  sense.  What  is 
relevant is that the DBA must investigate the cause for 
the failed test and, when relevant, take appropriate ac-
tions to avoid attacks. 
4. Assessing real case scenarios  
In  order  to  demonstrate  the  proposed  approach  we 
applied our tests to four real DBMS installations using 
four  distinct  engines.  Table  3  presents  some  relevant 
details  about  each  installation,  namely  the  DBMS  en-
gine  used,  the  operating  system  running  on  the  ma-
chine,  the  number  of  distinct  applications  currently 
using  this  database,  the  number  of  distinct  database 
administrators  and  the  number  of  developers  that  are 
not  administrators,  along  with  the  duration  of  the  test 
execution.  Results  are  presented  in  Tables  4,  5,  6  and 
7.  We  analyzed  the  number  of  passed,  failed  and  un-
known tests using the classification presented in Table 
1. The detailed results can be found at [12]. 
DBMS 
OS 
Applications 
DBAs 
Developers 
Test Duration 
Table 3. Scenarios details. 
Case 1 
Case 3 
SQLServer 2005  MySQL 5.0 
Oracle 10g 
Case 2 
PostgreSQL 8.1 
Windows 2003  Windows 2003  Windows XP  Windows 2000 
Case 4 
3 
2 
8 
54 
5 
39 
3 
2 
0 
2 
2 
0 
3 hours 
1,5 hours 
1 hour 
1 hour 
The  test  set  was  applied  by  two  different  people, 
one under the direct supervision of the authors (Case 1 
and  Case  3)  and  another  independently  (Case  2  and 
Case  4).  In  the  evaluation  of  cases  2  and 4 the person 
that  performed  the  evaluation  had  only  as  basis  the 
content of Tables 1 and 2. All four databases are used 
within  an  academic  context  in  two  universities,  being 
mostly  utilized  to  support  administrative  applications 
having staff, teachers and students as users. 
Table 4. Case 1, Oracle 10g installation. 
Tests Failed  Unknown 
0,00% 
2 
11 
0,00% 
0,00% 
2 
8,00% 
8 
0,00% 
1 
24 
3,39% 
Tests Passed 
75,00% 
6 
4 
26,67% 
33,33% 
1 
60,00% 
15 
87,50% 
7 
33 
55,93% 
Environment 
Installation setup 
Operational Procedures 
System level configuration 
App. level conf. and usage 
Total 
25,00% 
73,33% 
66,67% 
32,00% 
12,50% 
40,68% 
0 
0 
0 
2 
0 
2 
Total 
8 
15 
3 
25 
8 
59 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:10:06 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE94DSN 2008: Neto & VieiraTable 5. Case 2, SQLServer 2005 installation. 
Environment 
Installation setup 
Operational Procedures 
System level configuration 
App. level conf. and usage 
Total 
Tests Passed 
50,00% 
4 
33,33% 
5 
66,67% 
2 
48,00% 
12 
3 
37,50% 
44,07% 
26 
Tests Failed  Unknown  Total 
4 
9 
1 
12 
5 
31 
50,00% 
60,00% 
33,33% 
48,00% 
62,50% 
52,54% 
0,00% 
6,67% 
0,00% 
4,00% 
0,00% 
3,39% 
8 
15 
3 
25 
8 
59 
0 
1 
0 
1 
0 
2 
Table 6. Case 3, MySQL 5.0 installation. 
Environment 
Installation setup 
Operational Procedures 
System level configuration 
App. level conf. and usage 
Total 
Tests Passed 
3 
37,50% 
46,67% 
7 
33,33% 
1 
44,00% 
11 
50,00% 
4 
26 
44,07% 
Tests Failed  Unknown  Total 
5 
8 
2 
13 
4 
32 
62,50% 
53,33% 
66,67% 
52,00% 
50,00% 
54,24% 
0,00% 
0,00% 
0,00% 
4,00% 
0,00% 
1,69% 
8 
15 
3 
25 
8 
59 
0 
0 
0 
1 
0 
1 
Table 7. Case 4, PostgreSQL 8.1 installation. 
Environment 
Installation setup 
Operational Procedures 
System level configuration 
App. level conf. and usage 
Total 
Tests Passed 
37,50% 
3 
26,67% 
4 
1 
33,33% 
36,00% 
9 
75,00% 
6 
23 
38,98% 
Tests Failed  Unknown  Total 
5 
11 
2 
14 
2 
34 
62,50% 
73,33% 
66,67% 
56,00% 
25,00% 
57,63% 
0,00% 
0,00% 
0,00% 
8,00% 
0,00% 
3,39% 
8 
15 
3 
25 
8 
59 
0 
0 
0 
2 
0 
2 
Concerning the actual results, some aspects deserve 
discussion. First, it seems that the configuration used in 
Case 1 complies with more security best practices than 
the  configuration  used  in  any  other  cases.  That,  al-
though  not  necessarily  implying  more  security  (this 
assessment would demand the evaluation of the relative 
threat  associated  with  each  best  practice),  definitely 
indicates  that  Case  1  has  fewer  alternative  paths  for 
security attacks than the others.  
An  interesting  result  is  the  low  number  of  passed 
tests  in  the  Installation  Setup  group  in  all  cases  (less 
than  50%  in  a  15-item  group).  Three  things  seem  to 
contribute  to  these  results:  the  default  installation  set-
tings  are  kept  and  used,  no  file  system  partition  plan-
ning  for  logs  and  data  (which  can  lead  to  Denial  of 
Service by exhaustion of disk space), and the use of an 
operating  system  that  does  not  provide  easy  ways  to 
keep  track  of  file permissions and usually forces users 
to use administrative roles for several tasks. 
Another aspect that can be analyzed is the tests that 
had unanimous results in all cases. Table 8 presents the 
list of tests that passed or failed in all cases. 
Table 8. Tests with unanimous results. 
All passed 
All failed 
# of tests with unanimous results 
2, 8, 30, 33, 34, 39, 45, 47, 48, 57 
6, 7, 10, 12, 13, 19, 23, 26, 28, 29, 32, 37, 38, 42, 43, 58 
The low pass average in all cases explains why the 
number  of  tests  unanimously  failing  is  bigger  than  the 
number of tests unanimously passing. By looking at the 
description of the best practices from which we defined 
the tests, some patterns can be spotted. For example, it 
is  not  hard  to  notice  that  tests  #6,  #7,  #8,  #10,  #12, 
#13,  #23  and  #45 are heavily OS dependant. Thus the 
same  outcome  of  all  of  them  can  be  explained  by  the 
fact that all servers use some version of the same OS.  
5. Conclusions  
This  paper  proposes  an  approach  to  assess  the  ef-
fectiveness  of  DBMS  configurations  concerning  secu-
rity,  which  consists  in  a  set  of  security  best  practices 
that were obtained through detailed analysis of the CIS 
series  of  security  configuration  documents  for  DBMS. 
The security best practices were used to define a set of 
configuration  tests  that  can  be  executed  in  a  semi-
automated  manner  (some  tests  can  be  performed  by 
simple  tools  and  others  require  human  analysis  of  the 
DBMS configuration).  
Our  approach  has  been  successfully  used  to  evalu-
ate four real installations based on four distinct DBMS. 
In  fact,  several  configuration  problems  have  been  dis-
closed and the results clearly show that our approach is 
easy to apply and can be of extreme importance. 
References 
[1]  A.  Wool,  "A  quantitative  study  of  firewall  configuration 
errors," Computer, vol. 37, pp. 62-67, 2004.  
[2]  B.  Schneier,  Applied  Cryptography:  Protocols,  Algo-
rithms, and Source Code in C, Second Edition. Wiley, 1996. 
[3]  C.  Kaufman,  R.  Perlman,  and  M.  Speciner,  Network  Se-
curity: Private Communication in a Public World (2nd Edi-
tion). Prentice Hall PTR, 2002. 
[4] Commission of the European Communities, “Information 
Technology Security Eval. Manual (ITSEM)”, 1993. 
[5]  Common  Criteria,  “Common  Criteria  for  Information 
Technology Security Evaluation: User Guide”, 1999. 
[6]  Department  of  Defense,  “Trusted  Computer  System  
Evaluation Criteria”, 1985. 
[7]  E.  Bertino,  S.  Jajodia,  and  P.  Samarati,  “Database  secu-
rity:  Research  and  practice”,  Information  Systems  Journal, 
vol. 20, Number 7, 1995. 
[8]  G.  Pernul  and  G.  Luef,  “Bibliography  on  database secu-
rity”, ACM SIGMOD Rec., vol. 21, Issue 1, 1992. 
[9]  J.  Yan,  A.  Blackwell,  R.  Anderson,  A.  Grant,  "The 
Memorability  and  Security  of  Passwords  -  Some  Empirical 
Results",  Tech.  Report  500,  Computer  Lab,  Cambridge, 
2000. http://www.ftp.cl.cam.ac.uk/ftp/rja14/tr500.pdf 
[10]  Sandia  National  Laboratories,  “Information  Operations 
Red Team and Assessments™”. 
[11]  W.  Stallings,  Cryptography  and  Network  Security. 
Prentice Hall, 4th edition. 2005. 
[12]  A.  Neto,  M.  Vieira,  “DBMS  Configuration  Security 
Assessment Results”, 2007, http://eden.dei.uc.pt/~mvieira/. 
[13]  Vieira,  M.,  Madeira,  H.,  “Towards  a  Security  Bench-
mark for Database Management Systems”, Intl Conf. on De-
pendable Systems and Networks, Yokohama, Japan, 2005. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:10:06 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE95DSN 2008: Neto & Vieira