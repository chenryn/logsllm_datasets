4.1. Overview
SiRiUS supports symbolic links if they are also sup-
ported by the underlying ﬁle system. Symbolic links on
Unix ﬁle systems are typically implemented as normal
ﬁles with the ﬁle data containing the path of the link.
In this case, when the SiRiUS client accesses a symbolic
link, it decrypts the contents and obtains the ﬁle pointed
to by the link.
One ﬁle can have two hard links with different names
referencing the ﬁle. Therefore, SiRiUS cannot support
hard links because each md-ﬁle only contains one ﬁle-
name.
3.10. Key Management
Key management in SiRiUS is very simple because
each ﬁle keeps track of its own ﬁle keys for access con-
trol. All users only need to keep track of two keys; the
MSK and the MEK. There is no out-of-band communica-
tion if Identity Based encryption and signature schemes
are used. Otherwise, a small amount of out-of-band com-
munication is required in order to obtain public keys.
3.11. Key Revocation
Key revocation in SiRiUS is simple and does not require
out-of-band communication. For read access revocation,
the owner generates a new FEK. Using this key, the d-ﬁle
is updated by encrypting the ﬁle data with the new key
and then signed (using the old FSK). The revoked user’s
encrypted key block is removed from the md-ﬁle and all
the remaining key blocks are updated with the new FEK.
Finally, the md-ﬁle is signed with the owner’s MSK and
the root mdf-ﬁle is updated.
Write access revocation is the same as read access re-
vocation except that a new FSK is also generated. The
encrypted key blocks are updated with this new FSK and
the d-ﬁle is signed with this new key. Note that revok-
ing write access also involves creating a new FEK and
re-encrypting the data because write access implicitly pro-
vides read access.
3.12. Backups
A system administrator can backup the remote ﬁle
server by using the standard backup tools such as dump
or tar. Note that the administrator does not need to use
SiRiUS to perform the backup. Furthermore, the admin-
istrator gains no access to the ﬁle data. SiRiUS users only
need to backup their MSK and MEK. They can access any
backup of their ﬁles with just these two keys.
4. Implementation and Performance
In this section, we describe an instantiation of the SiR-
iUS ﬁle system layered over NFS.
We implemented a SiRiUS client on Linux that layers
SiRiUS over NFS version 3 using the SFS toolkit [17]
and OpenSSL [28]. The SFS toolkit provides support for
building user-level NFS loopback servers and clients. The
SiRiUS client contains a user-level NFS loopback server
to communicate with the client machine and a NFS client
to communicate with the remote NFS ﬁle server. Using a
user-level NFS loopback server to interface with the client
machine ensures portability because most modern client
operating systems have kernel-level NFS clients.
Note that SiRiUS’ NFS client can be replaced with a
client that supports a different protocol. For example, one
can retain the NFS loopback server and replace the NFS
client with a Yahoo! Briefcase (HTTP) client. This will
allow Yahoo! Briefcase to look like an NFS server to the
user.
SiRiUS Client
NFS Client
NFS Server
Application
user
kernel
NFS Client
Client Machine
Network
NFS Server
File Server
Figure 4. Architecture of SiRiUS layered
over NFS using the SFS toolkit.
The SiRiUS client intercepts NFS requests on the NFS-
mounted ﬁle system and processes the requests. The client
then sends the modiﬁed requests to the remote NFS server.
See Figure 4 for an overview of the system architecture.
4.2. Multiplexing NFS Calls
Many SiRiUS operations require the SiRiUS client to
transform a single NFS request from the client machine
into multiple requests to the server. First, SiRiUS needs
to read and write the md-ﬁle as well as the d-ﬁle. Second,
many SiRiUS ﬁle operations verify the meta data fresh-
ness, which necessitates sending a number of NFS calls to
walk the directory structure and read mdf-ﬁles.
The SFS toolkit’s ability to perform asynchronous Re-
mote Procedure Calls (RPC) [33] proved a great help in
multiplexing NFS calls from the client machine. When
the SiRiUS client receives an NFS call from its loopback
server, it asynchronously sends out a series of NFS calls
to the remote server to process the incoming call. We
illustrate this with an example of an NFS CREATE call.
When the SiRiUS client receives a CREATE call for a ﬁle
named foo, it ﬁrst creates the md-ﬁle. The SiRiUS client
prepends .x-md-x. to foo, obtaining .x-md-x.foo
as the name of the md-ﬁle.4 It then sends a CREATE call to
the remote server for the md-ﬁle. When this completes, the
SiRiUS client generates and stores the contents of the md-
ﬁle using a WRITE call. Following this, the root mdf-ﬁle
is updated. It ﬁnally sends the original CREATE request
to the remote server and returns the result of that request
to the local client machine.
call for a ﬁle foo. It uses the ﬁle handle for foo given
in the READ argument to obtain (from the ﬁle handle hash
table) the directory handle for the directory in which foo
resides. Using this directory handle, the SiRiUS client
determines the md-ﬁle ﬁle handle (from the directory ﬁle
handle hash table) using the directory handle and the md-
ﬁle name. The SiRiUS client issues a NFS READ for the
md-ﬁle using this md-ﬁle handle.
In the meantime, the
original NFS READ call for foo is also sent.
4.3. File System View
4.5. Changing Access Controls
The SiRiUS client implementation hides the presence
of the md-ﬁles and mdf-ﬁles from the SiRiUS user’s ﬁle
system view. For example, an ls invocation does not dis-
play these meta data ﬁles. This view is implemented by
processing the NFS READDIR and READDIRPLUS results
from the remote server to remove entries for ﬁles whose
names begin with the meta data ﬁle extensions. The code
infrastructure added to handle these two NFS calls is eas-
ily extended to handle encrypted ﬁlenames.
4.4. NFS File Handle Cache
In NFS, all ﬁle system objects are identiﬁed by a unique
NFS ﬁle handle generated by the remote server. The SiR-
iUS client must correlate the handles of d-ﬁles with those
of md-ﬁles. For example, NFS READ and WRITE opera-
tions refer to the target ﬁle by its NFS ﬁle handle. With
just this ﬁle handle, we have no way of obtaining the ﬁle
handle for the associated md-ﬁle.
Fortunately,
the NFS LOOKUP operation is always
called on an unknown object to obtain its ﬁle handle
before other operations can be performed. The NFS
LOOKUP call for an object contains the NFS ﬁle handle
for its directory, and the name of the object in that direc-
tory. The LOOKUP call expects a NFS ﬁle handle to the
speciﬁed target in the return result. By caching the argu-
ments and the results of LOOKUP calls, we can maintain
enough state to relate d-ﬁle and md-ﬁle ﬁle handles.
For each ﬁle system object, the SiRiUS client caches:
its ﬁle handle; the ﬁle handle of the directory in which it
resides; and its name in that directory. This cache is cross-
referenced in two hash tables, one keyed by the NFS ﬁle
handles of objects, and the other by a combination of the
directory ﬁle handle and directory entry name. The SiR-
iUS client monitors all NFS operations (e.g., RENAME
and REMOVE) that might change NFS ﬁle handle state
for an object and updates both hash tables.
We give an example to illustrate how ﬁle handle caching
works. Suppose the SiRiUS client receives an NFS READ
4The preﬁx .x-md-x. is chosen arbitrarily. SiRiUS uses it only to
locate md-ﬁles on the remote server. No special steps are needed to deal
with ﬁles with the same name as the preﬁx.
At present, we have implemented all of SiRiUS de-
scribed in Section 3. The hooks for adding and revoking
permissions to a ﬁle are present in the SiRiUS client, but,
since SiRiUS permissions are more expressive than Unix
permissions, there is no natural way to invoke these hooks
directly using the chmod system call.
This problem can be solved by a user-level permissions
tool that interacts with the SiRiUS client over a dedi-
cated RPC channel. Alternately, the RPC channel can be
avoided if the permissions tool calls chmod with modiﬁed
arguments. We illustrate the operation of the permissions
tool with an example. Suppose Bob wishes to grant Alice
permission to read ﬁle foo. Bob invokes the command-
line tool, which creates a dummy ﬁle with a special name.
The tool then performs a chmod on the ﬁle, causing the
kernel NFS client to send the NFS SETATTR (set ﬁle at-
tributes) request to the SiRiUS client. The dummy ﬁle-
name starts with a special ﬂag and contains the ﬁlename
foo and user name Alice. The SiRiUS client parses the
ﬁlename, obtains the public key for Alice and performs
the appropriate set of NFS calls to change the permissions
for ﬁle foo.
4.6. Random Access and Low Bandwidth
We originally did not plan on implementing random ac-
cess. While building the SiRiUS client, we realized that
whole-ﬁle read and write operations provide unacceptable
performance for large ﬁles. Random access from the SiR-
iUS client to the remote server is critical when the SiRiUS
client must handle partial read and write requests from the
local in-kernel client, as in NFS.
The insight is that NFS READ and WRITE calls oper-
ate on chunks of 8192 bytes.5 Hence, reading (or writ-
ing) a ﬁle larger than 8192 bytes involves multiple NFS
operations. If SiRiUS does not have random access, each
READ (respectively, WRITE) request involves fetching the
entire ﬁle to decrypt and verify (respectively, encrypt and
sign). We implemented random access as described in
Section 6.1.
5The chunk size depends on the NFS implementation and most im-
plementations optimize for 8192-byte blocks.
Test
File Creation
File Deletion
Sequential Read
Sequential Write
Sequential Read
Sequential Write
File Size Kernel NFS DumbFS
SiRiUS
0
0
8 KB
8 KB
1 MB
1 MB
0.4
0.3
0.9
1.1
96.7
100.0
3.4
0.4
1.4
2.0
97.8
102.7
14.5
1.1
18.0
21.9
223.8
632.9
Table 2. Benchmark Timings for Basic Operations. Numbers given are in milliseconds.
4.7. Caching
Our implementation avoids unnecessary operations by
aggressively caching meta data and integrity information.
All the caching code is implemented over the the ﬁle han-
dle cache infrastructure. The md-ﬁle does not have to be
fetched and veriﬁed repeatedly on a read unless we en-
counter an integrity or decryption failure. We can perform
the same optimization for writes by the ﬁle owner. In ad-
dition, for a read operation on block   of ﬁle foo, we only
need to compare the hash of block   to the cached ﬁle hash
block of foo. If the hashes are the same, we do not need
to fetch the hash block and verify its signature again. Un-
fortunately, writes update the hash block and so we are
forced to perform a signature for every write. Similar op-
timizations are implemented for freshness veriﬁcation.
4.8. Performance
We performed the tests listed in Table 2 to compare the
performance of SiRiUS layered over NFS, the Linux ker-
nel (2.4.19) NFS Client and DumbFS (a NFS pass-through
proxy built using the SFS toolkit). Each test was per-
formed on a hundred different ﬁles and the results were
averaged. Our implementation uses AES-128 as the block
cipher, RSA-1024 as the public key encryption algorithm,
and DSA-512 as the signature scheme. The NFS server
was run on a Pentium III 1.13 GHz machine and the three
clients were run on a Pentium III-M 866 MHz machine.
The DumbFS benchmarks show the low overhead of us-
ing a user-level loopback server. File creation is much
slower in SiRiUS because it requires key and signature
generation. Deletions are slightly slower because SiRiUS
has to unlink two ﬁles compared to just one for regular
NFS clients. For small ﬁles, reads and writes are about 20
times slower than the kernel NFS client. For writes, the
slowdown is due to the overhead of encrypting data (de-
crypting for reads), verifying three signatures (two for ﬁle
integrity and one for freshness), and generating a signa-
ture (no signature generation for reads).
For larger ﬁles, our aggressive caching and optimiza-
tions start to pay off. SiRiUS is able to read a 1 MB ﬁle
with only a
slowdown, as compared with the ker-
nel NFS. SiRiUS writes a 1 MB ﬁle with
slowdown

 


in comparison with the kernel NFS. Keeping in mind
the extensive cryptographic operations involved in reads
and writes, these benchmarks represent excellent perfor-
mance. Reads are about
as fast as writes because ev-
ery write has to sign the modiﬁed hash block. Sequential
writes on a large ﬁle can be sped up if we can cache all
the changes to the ﬁle hash block before performing the
signature. Unfortunately, NFSv3 does not provide a ﬁle
CLOSE call which would allow this optimization.6

The ﬁrst read (or write) of a random 8 KB block within
a large ﬁle will take the same amount of time as a sequen-
tial read (or write) of an 8 KB ﬁle. Subsequent 8 KB block
operations on the same ﬁle will see dramatic performance
improvements (similar to those observed for large ﬁles)
because of the caching and veriﬁcation optimizations de-
scribed in the previous section.
A proﬁle of the current implementation shows that 63%
of the time spent during a 1 MB ﬁle read is on AES de-
crypt. Signature generation take up about 40% of the time
on a 1 MB ﬁle write. We can improve read performance
by switching to a faster block cipher. The cost of signature
generation for writes can be alleviated if expensive com-
putations are performed out-of-line, as with on-line/off-
line signature schemes [11]. For example, DSA signa-
ture generation is computationally expensive because it
requires an exponentiation to a random number. This ex-
ponentiation is independent of the message to be signed
and can therefore be precomputed by the signer before he
receives the message. The SiRiUS client can reduce write
times by precomputing these random exponentiations dur-
ing idle cycles and using these precomputed values for