U5
U6
U7
U8
All
B1 v.s.
B2
B1 v.s.
Our
B2 v.s.
Our
-5.41** -5.41** -5.41**
-5.41**
-5.41** -5.41**
-5.41** -5.41** -15.47**
-5.41** -5.41** -5.41**
-5.41**
-5.41** -5.41**
-5.41** -5.41** -15.47**
-2.89*
-5.41** -0.14 (0.89)
-0.81 (0.41)
-2.84*
-0.77 (0.44)
-3.68** -4.25** -5.44**
* Indicates signiﬁcance under 0.01.
** Indicates signiﬁcance under 0.0001.
p-value is shown in parenthesis if it is not signiﬁcant under these two levels.
In Table 3, from column U1 to U8, we perform hypothesis testing on results
of 20 runnings within each experiment. The last column presents the result of
hypothesis testing on results of runnings of all experiments. We observe signiﬁ-
cant improvements of our model compared with baseline models.
Furthermore, Fig. 8 illustrates average ROCs of eight experiments under our
probabilistic integrity level, which provides better understandings of the perfor-
mance.
Fig. 8. Average ROC of eight experi-
ments under our model
Fig. 9. Scatter plot of running time in
seconds against problem size in thou-
sands
4.4 Running Time
The time complexity of the employed classiﬁer, random forests, has been well
studied, which is, mn log n, where m is the number of trees in random forests,
we ﬁnd m = 10 is optimal in our experiments, n is the number of processes,
a.k.a. data points [5]. Hence, we do not present its running time.
We explore the running time of our probabilistic integrity model, since the
running time of loopy belief propagation in practices varies in diﬀerent problems.
We vary the problem size, which is the number of subject-object pairs, by ran-
domly selecting diﬀerent portions of subjects and their involving subject-object
Probabilistic Inference on Integrity for Access Behavior
173
pairs. Figure 9 illustrates a scatter plot of running time in seconds against prob-
lem size in thousands, and its ﬁtting with linear regression. We observe strongly
linear relationship, supported by signiﬁcant coeﬃcients and R2 = 0.93 in the lin-
ear regression, between the running time and the problem size. This result veriﬁes
the linear time complexity of our probabilistic integrity model, and demonstrates
the feasibility of a runtime malware detection.
5 Conclusion and Future Work
In spite of considerate eﬀort by security researchers and engineers, it has been
demonstrated that attackers move faster than defenders. This paper presents
a probabilistic model on access behaviors of programs, and integrity levels of
programs, ﬁles and registries. We employ probabilistic inferences to determine
integrity levels of these system subjects and objects. Combining with a statistical
classiﬁer, we build a integrity based access behavior model for malware detection.
The encouraging experimental results indicate the feasibility and usefulness of
our model. The linear time complexity of our probabilistic integrity model is both
proofed by our theoretical analysis, and veriﬁed by our experimental results.
Our model can be extended to subject and objects in other granularities,
which are constrained by similar security policies. Meanwhile, our model can
be adapted to determine levels of other security attributes, e.g., conﬁdentiality,
according to corresponding security policies, e.g., Bell-LaPadula model.
We believe our probabilistic integrity model will be enhanced, when acquiring
knowledge from both benign and malicious programs. Thus, building a model
to combine access behaviors of both benign and malicious programs will be our
future work.
Acknowledgments. We would like to thank our shepherd, Manos Antonakakis, and
the anonymous reviewers for their insightful comments that greatly helped improve
the presentation of this paper. This work is supported by NFSC (61175039, 61221063,
61403301), 863 High Tech Development Plan (2012AA011003), Research Fund for Doc-
toral Program of Higher Education of China (20090201120032), International Research
Collaboration Project of Shaanxi Province (2013KW11) and Fundamental Research
Funds for Central Universities (2012jdhz08). Any opinions, ﬁndings, and conclusions
or recommendations expressed in this material are the authors’ and do not necessarily
reﬂect those of the sponsor.
Appendix- Derivation of Eq. (8)
P (EI|Acc) ∝ (cid:8)
P (Acc|T )P (T|EI)
(cid:8)
D
T
P (EI|D)P (D), where
P (EI|D)P (D) =
(cid:6)
D
And then,
(cid:8)
⎧
⎪⎨
(cid:8)
D
⎪⎩
(cid:8)
D
D
d1P (D) = ED(d1) =
d2P (D) = ED(d2) =
d3P (D) = ED(d3) =
α1
α1+α2+α3
α2
α1+α2+α3
α3
α1+α2+α3
,
,
,
if I(s)  I(o).
(18)
174
W. Mao et al.
(1.) If I(s)  I(o):
P (> |Acc) ∝
=
α3
α1 + α2 + α3
α3
α1 + α2 + α3
Δ
Δ
(cid:11)
tNr
1
tNw
2
tNr&w
3
T
β1 + β2 + β3
1
tβ2
2
tβ3−1
tβ1−1
B(β1, β2 + 1, β3)
Nw + β2
3
β2
N + β1 + β2 + β3
Ω.
dT,
(21)
Summing up Eqs. (19)–(21), we derive the posterior distribution of EI given Acc,
i.e., P (EI|Acc), as shown in Eqs. (9)–(11).
References
1. Anderson, R.: Security Engineering: A Guide to Building Dependable Distributed
Systems. John Wiley & Sons (2008)
2. Apap, F., Honig, A., Hershkop, S., Eskin, E., Stolfo, S.J.: Detecting malicious
software by monitoring anomalous windows registry accesses. In: Wespi, A., Vigna,
G., Deri, L. (eds.) RAID 2002. LNCS, vol. 2516, p. 36. Springer, Heidelberg (2002)
3. Bellovin, S.M.: Security and usability: windows vista, July 2007. https://www.cs.
columbia.edu/smb/blog/2007-07/2007-07-13.html
4. Biba, K.J.: Integrity considerations for secure computer systems. ESD-TR 76–372,
MITRE Corp. (1977)
5. Breiman, L.: Random forests. Mach. Learn. 45, 5–32 (2001)
6. Canali, D., Lanzi, A., Balzarotti, D., Kruegel, C., Christodorescu, M., Kirda, E.:
A quantitative study of accuracy in system call-based malware detection. In: Pro-
ceedings of the 2012 International Symposium on Software Testing and Analysis,
pp. 122–132. ACM (2012)
7. Fraser, T.: Lomac: low water-mark integrity protection for cots environments. In:
IEEE Symposium on Security and Privacy (S&P), pp. 230–245 (2000)
Probabilistic Inference on Integrity for Access Behavior
175
8. Fredrikson, M., Jha, S., Christodorescu, M., Sailer, R., Yan, X.: Synthesizing near-
optimal malware speciﬁcations from suspicious behaviors. In: IEEE Symposium on
Security and Privacy (S&P), pp. 45–60 (2010)
9. Gelman, A., Carlin, J.B., Stern, H.S., Rubin, D.B.: Bayesian data analysis, vol. 2.
Taylor & Francis (2014)
10. Gu, Z., Pei, K., Wang, Q., Si, L., Zhang, X., Xu, D.: LEAPS: detecting cam-
ouﬂaged attacks with statistical learning guided by program analysis. In: 45th
Annual IEEE/IFIP International Conference on Dependable Systems and Net-
works (DSN). IEEE (2015)
11. How the integrity mechanism is implemented in Windows Vista (2014). http://
msdn.microsoft.com/en-us/library/bb625962.aspx,
12. Hsu, F., Chen, H., Ristenpart, T., Li, J., Su, Z.: Back to the future: a framework
for automatic malware removal and system repair. In: 22nd Annual Computer
Security Applications Conference, ACSAC 2006, pp. 257–268. IEEE (2006)
13. King, S.T., Chen, P.M.: Backtracking intrusions. ACM Trans. Comput. Syst. 23,
51–76 (2005)
14. Koller, D., Friedman, N.: Probabilistic graphical models: principles and techniques.
MIT press (2009)
15. Kruegel, C., Kirda, E., Mutz, D., Robertson, W., Vigna, G.: Automating mimicry
attacks using static binary analysis. In: Proceedings of the 14th conference on
USENIX Security Symposium, vol. 14, pp. 11–11. USENIX Association (2005)
16. Kruegel, C., Mutz, D., Valeur, F., Vigna, G.: On the detection of anomalous system
call arguments. In: Snekkenes, E., Gollmann, D. (eds.) ESORICS 2003. LNCS, vol.
2808, pp. 326–343. Springer, Heidelberg (2003)
17. Lanzi, A., Balzarotti, D., Kruegel, C., Christodorescu, M., Kirda, E.: Accessminer:
using system-centric models for malware protection. In: Proceedings of the 17th
ACM conference on Computer and Communications Security (CCS), pp. 399–412.
ACM (2010)
18. Manadhata, P.K., Yadav, S., Rao, P., Horne, W.: Detecting malicious domains via
graph inference. In: Kuty(cid:3)lowski, M., Vaidya, J. (eds.) ICAIS 2014, Part I. LNCS,
vol. 8712, pp. 1–18. Springer, Heidelberg (2014)
19. Mandatory Integrity Control (2014). http://msdn.microsoft.com/en-us/library/
windows/desktop/bb648648
20. Mao, W., Cai, Z., Guan, X., Towsley, D.: Centrality metrics of importance in access
behaviors and malware detections. In: Proceedings of the 30th Annual Computer
Security Applications Conference (ACSAC 2014). ACM (2014)
21. Mao, Z., Li, N., Chen, H., Jiang, X.: Combining discretionary policy with manda-
tory information ﬂow in operating systems. ACM Trans. Inf. Syst. Secur. (TISSEC)
14(3), 24 (2011)
22. Mark Russinovich, B.C.: Process monitor (2014). http://technet.microsoft.com/
en-us/sysinternals/bb896645
23. Muthukumaran, D., Rueda, S., Talele, N., Vijayakumar, H., Teutsch, J., Jaeger, T.,
Edwards, N.: Transforming commodity security policies to enforce Clark-Wilson
integrity. In: Proceedings of the 28th Annual Computer Security Applications Con-
ference (ACSAC 2012). ACM (2012)
24. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O.,
Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., et al.: Scikit-learn: machine
learning in python. J. Mach. Learn. Res. 12, 2825–2830 (2011)
25. Sun, W., Sekar, R., Liang, Z., Venkatakrishnan, V.N.: Expanding malware defense
by securing software installations. In: Zamboni, D. (ed.) DIMVA 2008. LNCS, vol.
5137, pp. 164–185. Springer, Heidelberg (2008)
176
W. Mao et al.
26. Sun, W., Sekar, R., Poothia, G., Karandikar, T.: Practical proactive integrity
preservation: a basis for malware defense. In: IEEE Symposium on Security and
Privacy (S&P), pp. 248–262 (2008)
27. Symantec. Internet Security Threat Report, April 2015. https://www4.symantec.
com/mktginfo/whitepaper/ISTR/21347932 GA-internet-security-threat-report-
volume-20-2015-social v2.pdf
28. Sze, W.-K., Sekar, R.: A portable user-level approach for system-wide integrity
protection. In: Proceedings of the 29th Annual Computer Security Applications
Conference (ACSAC 2013), pp. 219–228. ACM (2013)
29. Tamersoy, A., Roundy, K., Chau, D.H.: Guilt by association: large scale malware
detection by mining ﬁle-relation graphs. In: Proceedings of the 20th ACM SIGKDD
international conference on Knowledge Discovery and Data Mining, pp. 1524–1533.
ACM (2014)
30. VXHeaven (2010). http://vx.netlux.org/