ﬁelds. We argue that this is, in fact, an upper bound on
footprint increase since we consider average 4-6 ﬁelds per
Intent. In reality, most Intents have only between 2-3 ﬁelds,
with few having a large number of ﬁelds (e.g., informative
Intents like Battery Status).
B. Java Annotations
One way to express additional constraints about the mes-
sage format when choosing the subtyping approach is the use
of Java Annotations. Annotations are fully embedded into
the language (since Java 1.5) and can be processed by the
Java compiler. Therefore, it is possible to use the annotations
already at compile time for criteria that are amenable to
static checking. For dynamic checks, the corresponding code
can either be realized as a common generic checker facility
implemented as part of the Intent delivery mechanism of the
platform or synthesized and injected into Intent receivers.
C. IDL and Domain Speciﬁc Language
Extended input validation requires additional knowledge
about the message format since the semantic gap between
the implicit message format and what can explicitly be
expressed by classes and the Java type system is still large.
For instance, an Intent responsible for a contact lookup
might want to be able to do approximate matching and return
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:17:27 UTC from IEEE Xplore.  Restrictions apply. 
the contact names together with a matching factor between
zero and one. In the Java type system, it would have to use
a ﬂoat type for the latter data but thereby would extend the
range of permitted values to the entire IEEE 754 ﬂoating
point number range. Another example is the problem that
every reference type can always be set to null so that there
is no way to express mandatory data in messages. One way
to more expressiveness is to use a domain speciﬁc language
to express the schema of the Intents.
Historically, a similar approach has been taken with many
RPC systems which used an interface deﬁnition language
(IDL). This IDL describes exactly the format of a remote
invocation in enough detail so that the stub and skeleton
code can be synthesized from this description. Systems
like CORBA extensively used IDLs but arguably also web
services employ the same principle, e.g., through the WSDL
ﬁles. For instance, a type system like XML Schema allows
value restrictions and would be a viable candidate for a
domain speciﬁc language approach to specifying Intents.
A well-designed domain speciﬁc language can express any
type of constraint and therefore permit full input validation
including version checks.
There are two different possibilities to interface general-
purpose languages with domain-speciﬁc languages. External
DSLs are free-standing and independent of the host lan-
guage. IDLs, for instance, are external DSLs. As a result,
however, code written in the host language and the meta-data
written in the DSL have to be developed independently and
cannot easily be cross-validated by existing tools. Internal
or embedded DSLs are themselves implemented in the host
language and therefore agree much better with existing tools.
They are, however, restricted to what the host language can
express.
VI. RELATED WORK
Robustness evaluation of software systems is broadly cat-
egorized into functional and exceptional testing. Functional
testing [17] employs generation of expected test inputs with
the intention of checking the functionality of a software
module, while exceptional testing employs generation of
specially crafted test inputs to crash the system in order
to check its robustness. Generated input test data can be
random, a pure fuzz approach [6], or semi-valid (intelligent
fuzzing) [10], [18]. UNIX utilities were ﬁrst fuzzed by
Miller et al. [6] by feeding random inputs to show that 25-
33% of utility programs either crashed or hanged on different
versions of UNIX. This simple technique has caught a
variety of bugs like buffer overﬂows, unhandled exceptions,
read access violations, thread hangs, memory leaks, etc.
A later work by the authors [7] showed that robustness
of UNIX utilities improved little over ﬁve years. A study
[8] of similar nature on Windows NT and Windows 2000
showed their weakness against random Win32 messages,
while, blackbox random testing on MacOS [9] reported a
considerable lower failure rate (7%). Our research extends
these works to a mobile platform where we fuzz the ICC of
Android and show a variety of exception handling errors.
In terms of knowledge about the target application (i.e.
whitebox [18], [19] vs. blackbox testing [20]), our tool
takes a combined approach (blackbox for explicit Intents
and whitebox for implicit Intents).
Fuzz tools reported in literature can also be classiﬁed
based on their input generation techniques and their intru-
siveness. The input data produced by a fuzzer tool may be
either generation based or mutation based [21]. Generation
based fuzzers generate test inputs based on speciﬁcation of
a protocol or an API to be tested while mutation based
fuzzers rely on capturing and replaying a mutated version of
valid input. Our tool (JJB) falls under generation-based fuzz
tools, as it generates input data, i.e., Intents conforming to
Android Intent API speciﬁcations. JJB is also intelligent in
that it has knowledge of Android APIs (e.g. known Action,
Category, and Extras strings) and partial knowledge
of the target applications (e.g. Intent-ﬁlters). Fuzzing tools
typically produce input received across trust boundaries [22],
i.e., Runtime-OS and Application-Runtime boundary. At a
lower layer, fuzzing can be done at Runtime-OS interface
as shown by [23]. Another similar work, Ballista [10],
identiﬁed ways to crash operating systems with a single
function call at Runtime-OS boundary. At a higher layer,
fuzzing can be done at Application-Runtime boundary where
runtime is responsible for validating data. In this work,
we fuzz at Application-Runtime boundary with the aim of
crashing Android runtime by fuzzing Intents that are passed
between application components.
Fuzz testing has been employed in other domains like
web applications, web servers, web browsers [24], Java-
based applications [25] and SMS systems [26]. Fu et al. [25]
presented an approach for compiler-assisted fault generation
for testing error recovery codes in Java server applications.
This is complementary to our work—the applications that
had exception handling codes may be further evaluated by
this tool, while in JarJarBinks, we found uncaught excep-
tions. Furthermore, JarJarBinks can additionally test Android
market apps, for which source codes may not be available.
We do not know any rigorous study of fuzz testing on smart-
phones. The closest work is [26], that fuzzes the messages
going through the mobile telephony stack. They provide a
fuzz based injection framework, that uncovers vulnerabilities
on SMS implementation in smartphones, and can be abused
for DoS attacks. In particular, the authors were able to
crash iPhone applications and disconnect Android devices
from mobile phone network. In our work, we evaluate a
wider range of applications in Android and focus on Inter-
component Communication.
A malformed Intent delivered to a receiver through ICC
exposes attack surfaces as pointed out by [13], example
vulnerabilities being triggering of components that are un-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:17:27 UTC from IEEE Xplore.  Restrictions apply. 
intentionally exported by a developer (i.e., an Intent spoof)
or unauthorized receipt of an implicit Intent by malicious
component. ComDroid [13], a static analysis tool, detects
these two vulnerabilities in Android applications. We narrow
down these attack surfaces to a set of input validation errors
by runtime testing, however, actual exploit of these errors
may require combining these with other vulnerabilities (e.g.
improper permission assignment). Our approach discovers
vulnerabilities in the application components, but, we do
not provide exploits to use these vulnerabilities from an
external source, i.e., we do not show external requests that
will generate malformed Intents for actually exploiting these
vulnerabilities. That is part of our ongoing work.
Other work on Android security looked at permission
assignment of applications, misuse of sensitive information
[27], and provided future directions for application certiﬁ-
cation [28]. Our work does not directly detect privacy leaks,
but can be used for giving insight to good application design
practices (specially input validation). These practices in turn
can be incorporated in an application certiﬁcation process
that is geared towards improving application robustness.
VII. CONCLUSION AND FUTURE WORK
In this paper, we have successfully conducted an extensive
robustness testing on Android’s Inter-component Communi-
cation (ICC) mechanism by sending a large number of semi-
valid and random Intents to various components across 3
versions of Android. Our learnings from this fault injection
campaign are many, most prominent ones being: 1) Many
components in Android have faulty exception handling code
and NullPointerExceptions are most commonly ne-
glected, 2) It is possible to crash Android runtime by sending
Intents from a user-level process in Android 2.2, 3) Across
various versions of Android, 4.0 is the most robust so far
in terms of exception handling; it, however, displays many
environment dependent failures.
Based on our observations, we have highlighted the
guideline that any component that runs as a thread in a
privileged process should be guarded by explicit permis-
sion(s). We have also proposed several enhancements to
harden implementation of Intents; of these, subtyping in
combination with Java annotations can be easily enforced.
Our experiments have so far looked at robustness of Android
components. In future we wish to explore whether any of
the detected failures can be exploited by attackers, more
speciﬁcally whether these failures can be triggered by an
adversary who does not have physical access to the phone.
Robustness evaluation of Binder IPC in Android is another
future goal.
REFERENCES
[1] D. Gross, “Fallout continues over
smartphone tracking app,”
December 2011. [Online]. Available: http://www.edition.cnn.com/
2011/12/02/tech/mobile/carrier-iq-reactions/
[2] K. LaCapria, “iphone explodes in midair on aussie ﬂight,” November
2011. [Online]. Available: http://www.inquisitr.com/163661/iphone-
explodes-in-midair-on-aussie-ﬂight/
[3] T. Lee, “At&ts
samsung galaxy s2 security ﬂaw lets you
[Online]. Avail-
http://www.uberphones.com/2011/10/atts-samsung-galaxy-s2-
bypass
able:
security-ﬂaw-lets-you-bypass-the-lock-screen/
screen,” October
2011.
lock
the
[4] D.
ben
smartphone
says,” November 2011.
Available:
android-passes-50-of-smartphone-sales-gartner-says.html
Aaron,
50% of
[Online].
sales, gartner
http://www.businessweek.com/news/2011-11-17/google-
“Google
android
passes
Golijan,
ipad
[5] R.
to
http://www.technolog.msnbc.msn.com/technology/technolog/fridge-
magnet-poses-security-threat-ipad-2-119905
threat
Available:
[Online].
security
“Fridge
magnet
2012.
poses
April
2,”
[6] B. P. Miller, L. Fredriksen, and B. So, “An empirical study of the
reliability of unix utilities,” Commun. ACM, vol. 33, pp. 32 – 44,
December 1990.
[7] B. P. Miller, D. Koski, C. Pheow, L. V. Maganty, R. Murthy,
A. Natarajan, and J. Steidl, “Fuzz revisited: A re-examination of the
reliability of unix utilities and services,” University of Wisconsin-
Madison, Tech. Rep., 1995.
[8] J. E. Forrester and B. P. Miller, “An empirical study of the robustness
of windows nt applications using random testing,” in Proceedings of
the 4th conference on USENIX Windows Systems Symposium - Volume
4. Berkeley, CA, USA: USENIX Association, 2000.
[9] B. P. Miller, G. Cooksey, and F. Moore, “An empirical study of
the robustness of macos applications using random testing,” SIGOPS
Oper. Syst. Rev., vol. 41, pp. 78 – 86, January 2007.
[10] P. Koopman and J. DeVale, “The exception handling effectiveness of
posix operating systems,” Software Engineering, IEEE Transactions
on, vol. 26, no. 9, pp. 837 – 848, sep 2000.
[11] “Dalvik virtual machine,” 2008. [Online]. Available: http://www
.dalvikvm.com/
[12] Y. Shi, K. Casey, M. A. Ertl, and D. Gregg, “Virtual machine
showdown: Stack versus registers,” ACM Trans. Archit. Code Optim.,
vol. 4, pp. 153 – 163, January 2008.
[13] E. Chin, A. P. Felt, K. Greenwood, and D. Wagner, “Analyzing
inter-application communication in android,” in Proceedings of the
9th international conference on Mobile systems, applications, and
services, ser. MobiSys ’11. New York, NY, USA: ACM, 2011, pp.
239 – 252.
[14] “What is android?” [Online]. Available: http://developer.android.com/
guide/basics/what-is-android.html
[15] “Intent
fuzzer.” [Online]. Available: http://www.isecpartners.com/
mobile-security-tools/intent-fuzzer.html
[16] “Intent class overview.” [Online]. Available: http://developer.android
.com/reference/android/content/Intent-.html
[17] B. Beizer, Black-Box Testing: Techniques for Functional Testing of
Software and Systems. Verlag John Wiley & Sons, Inc, 1995.
[18] P. Godefroid, M. Y. Levin, and D. A. Molnar, “Automated whitebox
fuzz testing,” in Network Distributed Security Symposium (NDSS).
Internet Society, 2008.
[19] J. DeMott, “The evolving art of fuzzing,” June 2006. [Online].
Available: http://www.vdalabs.com/tools/
[20] P. Godefroid, “Random testing for security: blackbox vs. whitebox
fuzzing,” in Proceedings of
the 2nd international workshop on
Random testing: co-located with the 22nd IEEE/ACM International
Conference on Automated Software Engineering (ASE 2007), ser. RT
’07. New York, NY, USA: ACM, 2007.
[21] P. Oehlert, “Violating assumptions with fuzzing,” Security Privacy,
IEEE, vol. 3, no. 2, pp. 58 – 62, march-april 2005.
[22] J. Neystadt, “Automated penetration testing with white-box fuzzing,”
February 2008. [Online]. Available: http://msdn.microsoft.com/en-
us/library/cc162782.aspx
[23] A. Johansson, N. Suri, and B. Murphy, “On the selection of error
model(s) for os robustness evaluation,” in Dependable Systems and
Networks, 2007. DSN ’07. 37th Annual IEEE/IFIP International
Conference on, june 2007, pp. 502 –511.
[24] M. Sutton, A. Greene, and P. Amini, Fuzzing: Brute Force Vulnera-
bility Discovery. Addison-Wesley Professional., 2007.
[25] C. Fu, A. Milanova, B. Ryder, and D. Wonnacott, “Robustness testing
of java server applications,” Software Engineering, IEEE Transactions
on, vol. 31, no. 4, pp. 292 – 311, april 2005.
[26] C. Mulliner and C. Miller, “Injecting sms messages into smart phones
for security analysis,” in Proceedings of the 3rd USENIX conference
on Offensive technologies, ser. WOOT’09.
Berkeley, CA, USA:
USENIX Association, 2009.
[27] W. Enck, D. Octeau, P. McDaniel, and S. Chaudhuri, “A study of
android application security,” in Proceedings of the 20th USENIX
conference on Security, ser. SEC’11. Berkeley, CA, USA: USENIX
Association, 2011.
[28] W. Enck, M. Ongtang, and P. McDaniel, “On lightweight mobile
phone application certiﬁcation,” in Proceedings of the 16th ACM
conference on Computer and communications security, ser. CCS ’09.
New York, NY, USA: ACM, 2009, pp. 235 – 245.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:17:27 UTC from IEEE Xplore.  Restrictions apply.