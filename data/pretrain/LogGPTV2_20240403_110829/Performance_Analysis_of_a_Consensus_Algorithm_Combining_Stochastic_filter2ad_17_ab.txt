### 2.5 Hardware and Software Environment
All experiments were conducted on a cluster of 12 PCs running Red Hat Linux 7.0 (kernel 2.2.19). Each host is equipped with an Intel Pentium III 766 MHz processor and 128 MB of RAM. The machines are interconnected via a simplex 100 Base-TX Ethernet hub. The algorithms were implemented in Java using Sun’s JDK 1.4.0 beta 2, built on top of the Neko development framework [18]. All messages were transmitted using TCP/IP, with connections between each pair of machines established at the start of the test. The typical message size was around 100 bytes.

### 3. The SAN Model for Performance Analysis

#### 3.1 Background: Stochastic Activity Networks
Stochastic Activity Networks (SANs) [19, 20] are used for performability evaluation, which encompasses both performance and dependability. They belong to the family of Timed Petri Nets and offer a rich and powerful syntax through primitives such as activities, places, input gates, and output gates, enabling the specification of complex stochastic processes.

We utilized the UltraSAN tool [21] to solve our SAN models. This tool provides a general framework for building performability and dependability models, supporting a wide range of analytical and simulative evaluation techniques, including steady-state and transient analysis. Timed Activities can follow various distributions, such as exponential, deterministic, uniform, or Weibull. However, non-exponential distributions limit the solver choices to simulative methods. Additionally, UltraSAN supports modular modeling through REP and JOIN operators, allowing submodels to be replicated and joined with common places, facilitating easier and faster modeling and reuse of previously built submodels.

#### 3.2 Overview of the SAN Model
As described in Section 2.1, the ✸S consensus algorithm operates in rounds, with each process taking turns as the coordinator. The fact that all messages in one round are exchanged with a single process, which changes in each round, necessitated a different submodel for each process involved. These submodels were composed using UltraSAN's 'Join' facility. Despite minor differences, we describe the model of one process due to space constraints. A detailed description can be found in [22].

The model for a process (P1) represents the state machine underlying one round of the consensus algorithm. Each state is represented by a place, and each state transition by an activity. Only the place corresponding to the current state is marked. The model is divided into five submodels (see Fig. 2):

- **Submodel P1C**: Describes the actions when the process acts as a coordinator. It waits for a majority of estimates, elaborates a proposal, and sends it to participants. It then waits for a majority of acknowledgments. If all are positive, it broadcasts the decision message; otherwise, it proceeds to the next round.
- **Submodels P1A1, P1A2a, and P1A2b**: Describe the actions when the process acts as a participant. In P1A1, the process sends an estimate to the coordinator and waits for a proposal. In P1A2a, if it receives the proposal, it replies with a positive acknowledgment. In P1A2b, if the failure detector suspects the coordinator has crashed, it sends a negative acknowledgment.
- **Submodel P1A3**: Responsible for starting a new round. It includes a place holding the current round number modulo n, where n is the number of processes. This simplification means the algorithm only considers the last n-1 rounds, which is realistic for a single instance of the consensus algorithm.

**Figure 2.** Overview of the SAN model of one process.

The remaining places and activities handle communication and failure detection, which are detailed in the following sections.

#### 3.3 The Network Model
Our network model is inspired by simple models of Ethernet networks [23, 24, 5], accounting for resource contention. In a distributed system, key resources are the CPUs and the network medium, either of which can be a bottleneck. For example, CPUs may limit performance when a process needs to receive information from many other processes, and the network may limit performance when many processes try to send messages simultaneously.

The transmission of a message from a sending process \( p_i \) to a destination process \( p_j \) involves two types of resources:
1. A shared network resource representing the transmission medium, usable by only one process at a time.
2. CPU resources attached to each process, representing the processing performed by network controllers and communication layers during message emission and reception.

Message transmission occurs in the following steps (see Fig. 3):
1. The message enters the sending queue of the sending host, waiting for CPUi to be available.
2. The message uses CPUi for some time \( t_{send} \).
3. The message enters the network queue of the sending host and waits for the network to be available.
4. The message uses the network resource for some time \( t_{net} \).
5. The message enters the receiving queue of the destination host and waits for CPUj to be available.
6. The message uses CPUj for some time \( t_{receive} \).
7. The message is received by \( p_j \) in the algorithm.

**Figure 3.** Decomposition of the end-to-end delay.

The parameters \( t_{send} \), \( t_{net} \), and \( t_{receive} \) were derived from measurement results (Section 5.1 and 5.2). A bi-modal distribution was found to be a good fit for \( t_{net} \).

#### 3.4 Failure Detection Model
Modeling a failure detector in detail would complicate the model significantly. Therefore, we chose a simple model where each process monitors every other process, resulting in \( n-1 \) failure detectors per process. Each failure detector alternates between two states: "trust" and "suspect". The transition rates were set based on measured quality of service (QoS) metrics from experiments on our cluster. QoS metrics include:

- **Detection Time (TD)**: The time from a process crash to when the failure detector starts suspecting the process permanently.
- **Mistake Recurrence Time (TMR)**: The time between consecutive mistakes (wrongly suspecting a process).
- **Mistake Duration (TM)**: The time it takes for a failure detector to correct a mistake.

These QoS metrics are random variables. We estimated their mean values and used these to configure the failure detector model. We considered both deterministic and exponential distributions for the transitions.

**Figure 4.** Quality of service metrics for failure detectors.

**Figure 5.** SAN model of the local failure detector module.

### 4. Implementation Issues

#### Measuring Latency
Given the small latency values (sometimes < 1 ms), we needed high-precision timing. Java's clock resolution (1 ms) was insufficient, so we implemented a higher-resolution clock (1 µs) in native C code. Host clocks were synchronized to within 50 µs using the NTP daemon [25], ensuring all processes started the consensus algorithm within a 50 µs window.

#### Isolation of Multiple Consensus Executions
To compute the latency of the ✸S consensus algorithm, we averaged over multiple sequential executions. To avoid interference between consecutive executions, we separated them by 10 ms, which was sufficient to prevent message overlap.

#### Measuring QoS Parameters of the Failure Detector
The failure detector outputs were used in the runs of the consensus algorithm. We estimated the mean values of TMR and TM from our cluster experiments and used these to configure the failure detector model. Future work will focus on estimating the distributions of these metrics to improve the model further.