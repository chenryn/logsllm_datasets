metrics was then used in the simulation of the ✸S
consensus algorithm.
We performed experiments for each of these classes of
runs.
2.5 Hardware and software environment
All experiments were run on a cluster of 12 PCs run-
ning Red Hat Linux 7.0 (kernel 2.2.19). The hosts have In-
tel Pentium III 766 MHz processors and 128 MB of RAM.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:20:03 UTC from IEEE Xplore.  Restrictions apply. 
They are interconnected by a simplex 100 Base-TX Ether-
net hub. The algorithms were implemented in Java (Sun’s
JDK 1.4.0 beta 2) on top of the Neko development frame-
work [18]. All messages were transmitted using TCP/IP;
connections between each pair of machines were established
at the beginning of the test. The size of a typical message is
around 100 bytes.
3 The SAN model for our performance anal-
ysis
3.1 Background: Stochastic activity networks
Stochastic activity networks (SANs) [19, 20] were devel-
oped for the purpose of performability evaluation: evalua-
tions of performance and dependability. They belong to the
broad family of Timed Petri Nets and have a very rich and
powerful syntax thanks to primitives like activities, places,
input gates, and output gates, thus allowing the speciﬁcation
of complex stochastic processes.
We used the UltraSAN tool [21] for solving SAN mod-
els. This tool provides a very general framework for build-
ing performability and/or dependability models. It supports
a wide variety of analytical and simulative evaluation tech-
niques with steady state and transient analysis. Timed Ac-
tivities can have different kinds of distributions: exponen-
tial, deterministic, uniform, Weibull, etc., though the use
of non-exponential distributions restricts the choice of the
solvers to simulative ones (as it happened in our approach).
Moreover, UltraSAN supports modular modeling: through
the operators REP and JOIN different submodels may be
replicated and joined together with common places. This
allows an easier and faster modeling and reuse of previously
built submodels.
3.2 Overview of the SAN model
As described in Section 2.1, the ✸S consensus algorithm
cyclically evolves through rounds in which every process
plays, in turn, the role of coordinator. The fact that all the
messages in one round are exchanged with one process, dif-
ferent for each round, forced us to renounce to model the
system using a parametric replication of the model of one
single process. We needed to build a different submodel for
every process involved in the algorithm execution. These
submodels were composed together using the ’Join’ facility
offered by UltraSAN. The models for processes differ only
in a few details, however. For this reason, we describe the
model of just one process. Due to space constraints and the
size of the model, we can only give an overview here. A
detailed description of the model can be found in [22].
The model for a process (P1) represents the state ma-
chine that underlies one round of the the consensus algo-
rithm: each state of the state machine is represented by one
place and each state transition by an activity. Only the place
that corresponds to the current state is marked. The model
is subdivided into 5 submodels (see Fig. 2). Submodel P1C
describes the actions of the process when it acts as a coordi-
nator: it ﬁrst waits for a majority of estimates, and then elab-
orates a proposal which is sent to the participants. Then it
waits for a majority of acknowledgments and if they are all
positive acknowledgments, it broadcasts the decision mes-
sage. Otherwise (if it receives at least one negative ack)
it passes to the next round. Submodels P1A1, P1A2a and
P1A2b describe the actions of the process when it acts as
a participant. In particular, during action P1A1 the process
sends an estimate to the coordinator, then it waits for the
coordinator’s proposal. If it receives the proposal (action
P1A2a), it replies with a positive acknowledgment, other-
wise (if, while the process is waiting, the failure detector
signals that it suspects the coordinator to have crashed; ac-
tion P1A2b) it sends a negative acknowledgment.
P1
model for process 1
participant’s actions
P1A1
P1A2a
P1A2b
P1A3
new round
coordinator’s
actions
P1C
Figure 2. Overview of the SAN model of one
process.
The submodel P1A3 is responsible for starting a new
round and deserves a more detailed description. It contains
a place which holds the current round number of the pro-
cess modulo n, where n is the number of processes. The
marking of this place controls if the process becomes coor-
dinator or is a simple participant in the next round. The fact
that the round number only holds the round number mod-
ulo n is a simpliﬁcation of the algorithm. The effect of this
simpliﬁcation is that the algorithm only takes the messages
of the last n − 1 rounds into account. While it is possible
in the consensus algorithm that two processes are n or more
rounds apart, this is rather improbable if a single instance
of consensus is executed. For this reason, this simpliﬁca-
tion does not make our model less realistic.
The remaining places and activities (not implementing
the state machine) are related to communication using mes-
sages and failure detectors. They are described in detail in
the subsequent two sections.
3.3 The network model
We now describe how we modeled the transmission of
messages. Our model is inspired from simple models of
Ethernet networks [23, 24, 5]. The key point in the model
is that it accounts for resource contention. This point is im-
portant as resource contention is often a limiting factor for
the performance of distributed algorithms. In a distributed
system, the key resources are (1) the CPUs and (2) the net-
work medium, any of which is a potential bottleneck. For
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:20:03 UTC from IEEE Xplore.  Restrictions apply. 
example, the CPUs may limit performance when a process
has to receive information from a lot of other processes, and
the network may limit performance when a lot of processes
try to send messages at the same time.
The transmission of a message from a sending process pi
to a destination process pj involves two kinds of resources.
There is one network resource (shared among all processes)
which represents the transmission medium. Only one pro-
cess can use this resource for message transmission at any
given point in time. Additionally, there is one CPU resource
attached to each process. These CPU resources represent
the processing performed by the network controllers and
the communication layers, during the emission and the re-
ception of a message (the cost of running the distributed
algorithm is neglected, hence this does not require any CPU
resource). The transmission of a message m occurs in the
following steps (see Fig. 3):
1. m enters the sending queue of the sending host, wait-
ing for CPUi to be available.
2. m takes and uses the resource CPUi for some time
tsend.
3. m enters the network queue of the sending host and
waits until the network is available for transmission.
4. m takes and uses the network resource for some time
tnet.
5. m enters the receiving queue of the destination host
and waits until CPUj is available.
6. m takes and uses the resource CPUj of the destina-
tion host for some time treceive.
7. Message m is received by pj in the algorithm.
process
p
i
send
m
CPU
(tsend)
i
sending
host
1
2
3
7
6
5
Network
(tnetwork)
4
process
p
j
receive
m
CPU
j
(treceive)
receiving
host
and treceive have been derived from the results of measure-
ments (Section 5.1 and 5.2). We observed that a bi-modal
distribution (described in Section 5.1) was a good ﬁt of the
results for tnet .
3.4 Failure detection model
One approach to modeling a failure detector is to build
a model of the failure detection algorithm. However, this
approach would complicate the model to a great extent (we
would have to model the messages used for failure detec-
tion). Such a level of detail is not justiﬁed as we model
other, more important components of the system (e.g., the
network) in much less detail. For this reason, we chose a
very simple model for failure detectors. Each process mon-
itors every other process, thus each process has n−1 failure
detectors (n is the number of processes). Consider any pair
of processes p and q and the failure detector at q that moni-
tors p. Each of these failure detectors is modelled as a pro-
cess with two states, which alternates between states mean-
ing “q trusts p” and “q suspects p”. Note that the underlying
assumption is that the behavior of each failure detector is
independent of the others. This constitutes a major simpli-
ﬁcation. Indeed, as the heartbeat messages are affected by
contention (either because of other heartbeats or other mes-
sages of the consensus algorithm) the outputs of the failure
detectors at a given time are expected to be correlated.
The question remains how to set the transition rates in
this simple model. We adopted the following solution. We
measured some quality of service (QoS) metrics of failure
detectors in experiments on our cluster, and then adjusted
the transition rates in the model such that the model of the
failure detector has the same average value for the QoS met-
rics as the real failure detector. QoS metrics for failure de-
tectors were introduced in [15]. The authors consider the
failure detector at a process q that monitors another process
p, and identify the following three primary QoS metrics (see
Fig. 3.4):
p
trust
FD at q
up
trust
suspect
suspect
mistake duration
TM
mistake recurrence time
TMR
t
t
Figure 3. Decomposition of the end-to-end de-
lay.
Figure 4. Quality of service metrics for failure
detectors. Process q monitors process p.
Parameters. The model deﬁned needs to be fed with the
three parameters tsend , tnet and treceive. Due to the dif-
ﬁculties to measure their values on the prototype (see Sec-
tion 5.1) we assumed (following [5]) tsend and treceive to be
constants, with tsend = treceive. The parameters tnet , tsend
Detection time TD: The time that elapses from p’s crash
to the time when q starts suspecting p permanently.
Mistake recurrence time TMR: The time between two con-
secutive mistakes (q wrongly suspecting p), given that
p did not crash.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:20:03 UTC from IEEE Xplore.  Restrictions apply. 
Mistake duration TM : The time it takes a failure detector
component to correct a mistake, i.e., to trust p again
(given that p did not crash).
These QoS metrics are random variables. In our experi-
ments on the cluster, we estimate the mean values for TMR
and TM and use these values to conﬁgure the failure detec-
tor model (in future work, we plan to improve the model by
estimating the distributions of these metrics and incorporat-
ing these distributions into the model). We considered two
different time distributions for the transition from one state
to the other: a deterministic and an exponential distribution,
so to have, for the same mean value, a distribution with the
minimum variance (0) and a distribution with a high vari-
ance.
FD
fd
Trust
ts
Susp
st
crash
Figure 5. SAN model of the local failure de-
tector module.
Figure 5 shows the way the local failure detector mod-
ule has been modelled. As it can be seen, the two states
are represented by the places Trust and Susp, while the (de-
terministic or exponential) transitions from one state to the
other is managed by the activities ts and st. The activities
present three possible outputs. The ﬁrst two (in descending
order) manage the alternance between the two states. The
third one makes the activity of the failure detector stop when
a decision is taken or when no more processes are alive; this
is needed to stop the execution of the model once all inter-
esting events took place. At the beginning of the simula-
tion, an instantaneous activity fd determines the initial state
of the failure detector module according to the probabilities
associated to its outputs.
In this work, we did not model the contention on the net-
work due to failure detectors. This is a choice we did on the
basis of several measurements where the extra load gener-
ated did not affect the latency (the network bandwidth man-
aged heartbeat and other messages without any problem).
We reserve to take into account this phenomenon in a future
and more reﬁned model.
4 Implementation issues
In this section, we discuss some issues related to the ✸S
consensus experiments on the cluster.
Measuring latency. Since the latency values to be mea-
sured are rather small (sometimes < 1 ms) we had to mea-
sure time extremely precisely. The resolution of Java’s clock
(1 ms) was not sufﬁcient, i.e., we had to implement a clock
with a higher resolution (1 µs) in native C code. Also, the
clocks of the hosts had to be synchronized precisely, in or-
der to start the consensus algorithm of all the processes at
the same time t0 (see Section 2.3). We were able to achieve
clock synchronization with a precision of (cid:4) 50 µs, using
the NTP daemon [25] which provides advanced clock syn-
chronization algorithms at minimal cost. This is far less
than the transmission time of a message (≈ 180 µs). This
allowed us to have all processes start the consensus algo-
rithm within a time window of 50 µs.
Isolation of multiple consensus executions. The latency
of the ✸S consensus algorithm was computed by averag-
ing over a large number of (sequential) executions of the
algorithm. However, with multiple executions of consen-
sus it might happen that messages of consensus #k interfere
with messages of consensus #(k + 1). In order to isolate
completely the execution of two consensus algorithms, we
have separated the beginning of two consecutive consensus
executions by 10 ms, a value that was sufﬁcient to avoid
interferences.2
Measuring the QoS parameters of the failure detector.
The failure detector outputs are only used in the runs of