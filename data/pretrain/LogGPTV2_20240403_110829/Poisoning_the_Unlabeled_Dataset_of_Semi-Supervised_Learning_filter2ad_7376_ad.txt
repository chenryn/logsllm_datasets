UDA
FixMatch
SVHN
CIFAR-10
0.5% 1.0% 0.5% 1.0%
4/8
4/8
5/8
2/8
2/8
3/8
4/8
3/8
4/8
3/8
4/8
3/8
Figure 6: Success rate of our attack at poisoning the unlabeled
dataset without knowledge of any training examples. As in
Table 1, experiments are across three algorithms, but here
across two datasets.
4.1 Zero-Knowledge Attack
4.2 Generalized Interpolation
Our ﬁrst attack requires the adversary knows at least one
example in the labeled dataset. While there are settings where
this is realistic [52], in general an adversary might have no
knowledge of the labeled training dataset. We now develop
an attack that removes this assumption.
As an initial experiment, we investigate what would happen
if we blindly connected the target point x∗ with an arbitrary
example x(cid:48) (not already contained within the labeled training
dataset). To do this we mount exactly our earlier attack with-
out modiﬁcation, interpolating between an arbitrary unlabeled
example x(cid:48) (that should belong to class y∗, despite this label
not being attached), and the target example x∗.
As we should expect, across all trials, when we connect
different source and target examples the trained model consis-
tently labels both x(cid:48) and x∗ as the same ﬁnal label. Unexpect-
edly, we found that while the ﬁnal label the model assigned
was rarely y∗ = c(x(cid:48)) (our attack objective label), and instead
most often the ﬁnal label was a label neither y∗ nor c(x∗).
Why should connecting an example with label c(x(cid:48)) and
another example with label c(x∗) result in a classiﬁer that as-
signs neither of these two labels? We found that the reason this
happens is that, by chance, some intermediate image xα will
exceed the conﬁdence threshold. Once this happens, both xα1
and xαN−1 become classiﬁed as however xα was classiﬁed—
which often is not the label as either endpoint.
In order to better regularize the attack process we provide
additional support. To do this, we choose additional images
{ ˆxi} and then connect each of these examples to x(cid:48) with a
path, blending as we do with the target example.
These additional interpolations make it more likely for x(cid:48) to
be labeled correctly as y∗, and when this happens, then more
likely that the attack will succeed.
Evaluation Table 6 reports the results of this poisoning
attack. Our attack success rate is lower, with roughly half
of attacks succeeding at 1% of training data poisoned. All
of these attacks succeed at making the target example x∗
becoming mislabeled (i.e., f (x∗) (cid:54)= c(x∗)) even though it is
not necessary labeled as the desired target label.
When performing linear blending between the source and
target example, human visual inspection of the poisoned ex-
amples could identify them as out-of-distribution. While it
may be prohibitively expensive for a human to label all of the
examples in the unlabeled dataset, it may not be expensive
to discard examples that are clearly incorrect. For example,
while it may take a medical professional to determine whether
a medical scan shows signs of some disease, any human could
reject images that were obviously not medical scans.
Fortunately there is more than one way to interpolate be-
tween the source example x(cid:48) and the target example x∗. Earlier
we used a linear pixel-space interpolation. In principle, any
interpolation strategy should remain effective—we now con-
sider an alternate interpolation strategy as an example.
Making our poisoning attack inject samples that are not
overly suspicious therefore requires a more sophisticated inter-
polation strategy. Generative Adversarial Networks (GANs)
[17] are neural networks designed to generate synthetic im-
ages. For brevity we omit details about training GANs as our
results are independent of the method used. The generator of
a GAN is a function g: Rn → X , taking a latent vector z ∈ Rn
and returning an image. GANs are widely used because of
their ability to generate photo-realistic images.
One property of GANs is their ability to perform semantic
interpolation. Given two latent vectors z1 and z2 (for example,
latent vectors corresponding to a picture of person facing left
and a person facing right), linearly interpolating between z1
and z2 will semantically interpolate between the two images
(in this case, by rotating the face from left to right).
For our attack, this means that it is possible to take our two
images x(cid:48) and x∗, compute the corresponding latent vectors
z(cid:48) and z∗ so that G(z(cid:48)) = x(cid:48) and G(z∗) = x∗, and then linearly
interpolate to obtain xi = G((1− αi)z(cid:48) + αiz∗). There is one
small detail: in practice it is not always possible to obtain
a latent vector z(cid:48) so that exactly G(z(cid:48)) = x(cid:48) holds. Instead,
the best we can hope for is that (cid:107)G(z(cid:48))− x(cid:48)(cid:107) is small. Thus,
we perform the attack as above interpolating between G(z(cid:48))
and G(z∗) and then ﬁnally perform the small interpolation
between x(cid:48) and G(z(cid:48)), and similarly x∗ and G(z∗).
USENIX Association
30th USENIX Security Symposium    1585
Evaluation. We use a DC-GAN [46] pre-trained on CIFAR-
10 to perform the interpolations. We again perform the same
attack as above, where we poison 1% of the unlabeled exam-
ples by interpolating in between the latent spaces of z(cid:48) and z∗.
Our attack succeeds in 9 out of 10 trials. This slightly reduced
attack success rate is a function of the fact that while the two
images are similarly far apart, the path taken is less direct.
4.3 Attacking Transfer Learning
Often models are not trained from scratch but instead ini-
tialized from the weights of a different model, and then ﬁne
tuned on additional data [43]. This is done either to speed up
training via “warm-starting”, or when limited data is available
(e.g., using ImageNet weights for cancer detection [15]).
Fine-tuning is known to make attacks easier. For example,
if it’s public knowledge that a model has been ﬁne-tuned from
a particular ImageNet model, it becomes easier to generate ad-
versarial examples on the ﬁne-tuned model [63]. Similarly, ad-
versaries might attempt to poison or backdoor a high-quality
source model, so that when a victim uses it for transfer learn-
ing their model becomes compromised as well [68].
Consistent with prior work we ﬁnd that it is easier to poison
models that are initialized from a pretrained model [51]. The
intuition here is simple. The ﬁrst step of our standard attacks
waits for the model to assign x(cid:48) the (correct) label y∗ before it
can propagate to the target label f (x∗) = y∗. If we know the
initial model weights θinit, then we can directly compute
x(cid:48) =
arg min
δ : fθinit (x∗+δ)=y∗
(cid:107)δ(cid:107).
That is, we search for an example x(cid:48) that is nearby the desired
target x∗ so that the initial model fθinit already assigns example
x(cid:48) the label y∗. Because this initial model assigns x(cid:48) the label
y∗, then the label will propagate to x∗—but because the two
examples are closer, the propagation happens more quickly.
Evaluation We ﬁnd that this attack is even more effective
than our baseline attack. We initialize our model with a semi-
supervised learning model trained on the ﬁrst 50% of the
unlabeled dataset to convergence. Then, we provide this ini-
tial model weights θinit to the adversary. The adversary solves
the minimization formulation above using standard gradient
descent, and then interpolates between that x(cid:48) and the same
randomly selected x∗. Finally, the adversary inserts poisoned
examples into the second 50% of the unlabeled dataset, modi-
fying just 0.1% of the unlabeled dataset.
We resume training on this additional clean data (along
with the poisoned examples). We ﬁnd that, very quickly, the
target example becomes successfully poisoned. This matches
our expectation: because the distance between the two exam-
ples is very small, the model’s decision boundary does not
have to change by much in order to cause the target example
to become misclassiﬁed. In 8 of 10 trials, this attack succeeds.
4.4 Negative Results
We attempted ﬁve different extensions of our attack that did
not work. Because we believe these may be illuminating, we
now present each of these in turn.
Analytically computing the optimal density function In
Table 3 we studied eleven different density functions. Initially,
we attempted to analytically compute the optimal density
function, however this did not end up succeeding. Our ﬁrst
attempt repeatedly trained classiﬁers and performed binary
search to determine where along the bridge to insert new
poisoned examples. We also started with a dense interpolation
of 500 examples, and removed examples while the attack
succeeded. Finally, we also directly computed the maximum
distance ε so that training on example u would cause the
conﬁdence of u + δ (for (cid:107)δ(cid:107)2 = ε) to increase.
Unfortunately, each of these attempts failed for the same
reason: the presence or absence of one particular example is
not independent of the other examples. Therefore, it is difﬁcult
to accurately measure the true inﬂuence of any particular
example, and greedy searches typically got stuck in local
minima that required more insertions than just a constant
insertion density with fewer starting examples.
Multiple intersection points Our attack chooses one
source x(cid:48) and connects a path of unlabeled examples from that
source x(cid:48) to the target x∗. However, suppose instead that we
selected multiple samples {x(cid:48)
i}n
i=1 and then constructed paths
from each x(cid:48)
i to x∗. This might make it appear more likely to
succeed: following the same intuition behind our “additional
support” attack, if one of the paths breaks, one of the other
paths might still succeed.
However, for the same insertion budget, experimentally we
ﬁnd it is always better to use the entire budget on one single
path from x(cid:48) to x∗ than to spread it out among multiple paths.
Adding noise to the poisoned examples When interpolat-
ing between x(cid:48) and x∗ we experimented with adding point-
wise Gaussian or uniform noise to xα. Because images are
discretized into {0,1, . . . ,255}hwc, it is possible that two suf-
ﬁciently close α,α(cid:48) will have discretize(xα) = discretize(xα(cid:48))
even though xα (cid:54)= xα(cid:48). By adding a small amount of noise,
this property is no longer true, and therefore the model will
not see the same unlabeled example twice.
However, doing this did not improve the efﬁcacy of the at-
tack for small values of σ, and made it worse for larger values
of σ. Inserting the same example into the unlabeled dataset
two times was more effective than just one time, because the
model trains on that example twice as often.
Increasing attack success rate. Occasionally, our attack
gets “stuck”, and x(cid:48) becomes classiﬁed as y∗ but x∗ does not.
When this happens, the poisoned label only propagates part
1586    30th USENIX Security Symposium
USENIX Association
of the way through the bridge of poisoned examples. That is,
for some threshold t, we have that xit (cid:54)= y∗.
Even if t = 0.9, and the propagation has made it almost all
the way to the ﬁnal label, past a certain time in training the
model’s label assignments become ﬁxed and the predicted
labels no longer change. It would be interesting to better
understand why these failures occur.
Joint labeled and unlabeled poisoning. Could our attack
improve if we gave the adversary the power to inject a sin-
gle, correctly labeled, poisoned example (as in a clean-label
poisoning attack)? We attempted various strategies to do this,
ranging from inserting out-of-distribution data [48] to mount-
ing a Poison Frog attack [51]. However, none of these ideas
worked better than just choosing a good source example as
determined in Figure 3. Unfortunately, we currently do not
have a technique to predict which samples will be good or
bad sources (other than brute force training).
5 Defenses
We now shift our focus to preventing the poisoning attack we
have just developed. While we believe existing defense are
not well suited to defend against our attacks, we believe that
by combining automatic techniques to identify potentially-
malicious examples, and then manually reviewing these lim-
ited number of cases, it may be possible to prevent this attack.
5.1 General-Purpose Poisoning Defenses
While there are a large class of defenses to indiscriminate
poisoning attacks [8, 11, 22, 23, 56], there are many fewer
defenses that prevent the targeted poisoning attacks we study.
We brieﬂy consider two defenses here.
Fine-tuning based defenses [34] take a (potentially poi-
soned) model and ﬁne-tune it on clean, un-poisoned data.
These defenses hope (but can not guarantee) that doing this
will remove any unwanted memorization of the model. In
principle these defenses should work as well on our setting
as any other if there is sufﬁcient data available—however,
because semi-supervised learning was used in the ﬁrst case, it
is unlikely there will exist a large, clean, un-poisoned dataset.
Alternatively, other defenses [20] alter the training process
to apply differentially private SGD [1] in order to mitigate the
ability of the model to memorize training examples. However,
because the vulnerability of this defense scales exponentially
with the number of poisoned examples, these defenses are
only effective at preventing extremely limited poisoning at-
tacks that insert fewer than three or ﬁve examples.
Our task and threat model are sufﬁciently different from
these prior defenses that they are a poor ﬁt for our problem
domain: the threat models do not closely align.
5.2 Dataset Inspection & Cleaning
We now consider two defenses tailored speciﬁcally to prevent
our attacks. While it is undesirable to pay a human to manu-
ally inspect the entire unlabeled dataset (if this was acceptable
then the entire dataset might as well be labeled), this does not
preclude any human review. Our methods directly process the
unlabeled dataset and ﬁlter out a small subset of the examples
to have reviewed by a human for general correctness (e.g.,
“does this resemble anything like a dog?” compared to “which
of the 100+ breeds of dog in the ImageNet dataset is this?”).
Detecting pixel-space interpolations Our linear image
blending attack is trivially detectable. Recall that for this
attack we set xαi = (1− αi)· x(cid:48) + αi · x∗. Given the unlabeled
dataset U, this means that there will exist at least three ex-
amples u,v,w ∈ U that are colinear in pixel space. For our