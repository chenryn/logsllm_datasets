needed to kill them is lower.  Mature and healthy prey would probably be more nutritious, but 
there’s a risk of missing lunch entirely if it gets away.  And a small meal will tide the lion over 
until another day.  Getting through today is more important than the possibility of having food 
tomorrow. 
Similarly, it is evolutionarily better to risk a larger loss than to accept a smaller loss.  
Because animals tend to live on the razor’s edge between starvation and reproduction, any loss 
of food—whether small or large—can be equally bad.  That is, both can result in death.  If that’s 
The Psychology of Security—DRAFT 
10 
true, the best option is to risk everything for the chance at no loss at all. 
These two heuristics are so powerful that they can lead to logically inconsistent results.  
Another experiment, the Asian disease problem, illustrates that.13   In this experiment, subjects 
were asked to imagine a disease outbreak that is expected to kill 600 people, and then to choose 
between two alternative treatment programs.  Then, the subjects were divided into two groups.  
One group was asked to choose between these two programs for the 600 people: 
• 
Program A: “200 people will be saved.” 
• 
Program B: “There is a one-third probability that 600 people will be saved, and a 
two-thirds probability that no people will be saved.” 
The second group of subjects were asked to choose between these two programs: 
• 
Program C: “400 people will die.” 
• 
Program D: “There is a one-third probability that nobody will die, and a two-thirds 
probability that 600 people will die.” 
Like the previous experiment, programs A and B have the same expected utility: 200 people 
saved and 400 dead, A being a sure thing and B being a risk.  Same with Programs C and D.  But 
if you read the two pairs of choices carefully, you’ll notice that—unlike the previous experiment—
they are exactly the same.  A equals C, and B equals D.  All that’s different is that in the first pair 
they’re presented in terms of a gain (lives saved), while in the second pair they’re presented in 
terms of a loss (people dying).   
Yet most people (72%) choose A over B, and most people (78%) choose D over C.  People 
make very different trade-offs if something is presented as a gain than if something is presented 
as a loss. 
Behavioral economists and psychologists call this a “framing effect”: peoples’ choices are 
affected by how a trade-off is framed.  Frame the choice as a gain, and people will tend to be risk 
averse.  But frame the choice as a loss, and people will tend to be risk seeking. 
We’ll see other framing effects later on. 
Another way of explaining these results is that people tend to attach a greater value to 
changes closer to their current state than they do to changes further away from their current 
state.  Go back to the first pair of trade-offs I discussed.  In the first one, a gain from $0 to $500 
is worth more than a gain from $500 to $1,000, so it doesn’t make sense to risk the first $500 
for an even chance at a second $500.  Similarly, in the second trade-off, more value is lost from 
$0 to -$500 than from -$500 to -$1,000, so it makes sense for someone to accept an even 
chance at losing $1,000 in an attempt to avoid losing $500.  Because gains and losses closer to 
one’s current state are worth more than gains and losses further away, people tend to be risk 
averse when it comes to gains, but risk seeking when it comes to losses. 
Of course, our brains don’t do the math.  Instead, we simply use the mental shortcut.   
There are other effects of these heuristics as well.  People are not only risk averse when it 
comes to gains and risk seeking when it comes to losses; people also value something more when 
it is considered as something that can be lost, as opposed to when it is considered as a potential 
gain.  Generally, the difference is a factor of 2 to 2.5.14 
This is called the “endowment effect,” and has been directly demonstrated in many 
The Psychology of Security—DRAFT 
11 
experiments.  In one,15 half of a group of subjects were given a mug.  Then, those who got a mug 
were asked the price at which they were willing to sell it, and those who didn’t get a mug were 
asked what price they were willing to offer for one.  Utility theory predicts that both prices will 
be about the same, but in fact, the median selling price was over twice the median offer. 
In another experiment,16 subjects were given either a pen or a mug with a college logo, both 
of roughly equal value.  (If you read enough of these studies, you’ll quickly notice two things.  
One, college students are the most common test subject.  And two, any necessary props are most 
commonly purchased from a college bookstore.)  Then the subjects were offered the opportunity 
to exchange the item they received for the other.  If the subjects’ preferences had nothing to do 
with the item they received, the fraction of subjects keeping a mug should equal the fraction of 
subjects exchanging a pen for a mug, and the fraction of subjects keeping a pen should equal the 
fraction of subjects exchanging a mug for a pen.  In fact, most people kept the item they 
received; only 22% of subjects traded. 
And, in general, most people will reject an even-chance gamble (50% of winning, and 50% 
of losing) unless the possible win is at least twice the size of the possible loss.17 
What does prospect theory mean for security trade-offs?  While I haven’t found any 
research that explicitly examines if people make security trade-offs in the same way they make 
economic trade-offs, it seems reasonable to me that they do at least in part.  Given that, prospect 
theory implies two things.  First, it means that people are going to trade off more for security 
that lets them keep something they’ve become accustomed to—a lifestyle, a level of security, 
some functionality in a product or service—than they were willing to risk to get it in the first 
place.  Second, when considering security gains, people are more likely to accept an incremental 
gain than a chance at a larger gain; but when considering security losses, they’re more likely to 
risk a larger loss than accept a larger loss. 
Other Biases that Affect Risk 
We have other heuristics and biases about risks.  One common one is called “optimism 
bias”: we tend to believe that we’ll do better than most others engaged in the same activity.  This 
bias is why we think car accidents happen only to other people, and why we can at the same time 
engage in risky behavior while driving and yet complain about others doing the same thing.  It’s 
why we can ignore network security risks while at the same time reading about other companies 
that have been breached.  It’s why we think we can get by where others failed. 
Basically, animals have evolved to underestimate loss.  Because those who experience the 
loss tend not to survive, those of us remaining have an evolved experience that losses don’t 
happen and that it’s okay to take risks.  In fact, some have theorized that people have a “risk 
thermostat,” and seek an optimal level of risk regardless of outside circumstances.18  By that 
analysis, if something comes along to reduce risk—seat belt laws, for example—people will 
compensate by driving more recklessly. 
And it’s not just that we don’t think bad things can happen to us, we—all things being 
equal—believe that good outcomes are more probable than bad outcomes.  This bias has been 
repeatedly illustrated in all sorts of experiments, but I think this one is particularly simple and 
elegant.19 
Subjects were shown cards, one after another, with either a cartoon happy face or a cartoon 
frowning face.  The cards were random, and the subjects simply had to guess which face was on 
the next card before it was turned over. 
For half the subjects, the deck consisted of 70% happy faces and 30% frowning faces.  
The Psychology of Security—DRAFT 
12 
Subjects faced with this deck were very accurate in guessing the face type; they were correct 68% 
of the time.  The other half was tested with a deck consisting of 30% happy faces and 70% 
frowning faces.  These subjects were much less accurate with their guesses, only predicting the 
face type 58% of the time.  Subjects’ preference for happy faces reduced their accuracy. 
In a more realistic experiment,20 students at Cook College were asked “Compared to other 
Cook students—the same sex as you—what do you think are the chances that the following 
events will happen to you?”  They were given a list of 18 positive and 24 negative events, like 
getting a good job after graduation, developing a drinking problem, and so on.  Overall, they 
considered themselves 15% more likely than others to experience positive events, and 20% less 
likely than others to experience negative events. 
The literature also discusses a “control bias,” where people are more likely to accept risks if 
they feel they have some control over them.  To me, this is simply a manifestation of the 
optimism bias, and not a separate bias. 
Another bias is the “affect heuristic,” which basically says that an automatic affective 
valuation—I’ve seen it called “the emotional core of an attitude”—is the basis for many 
judgments and behaviors about it.  For example, a study of people’s reactions to 37 different 
public causes showed a very strong correlation between 1) the importance of the issues, 
2) support for political solutions, 3) the size of the donation that subjects were willing to make, 
and 4) the moral satisfaction associated with those donations.21  The emotional reaction was a 
good indicator of all of these different decisions. 
With regard to security, the affect heuristic says that an overall good feeling toward a 
situation leads to a lower risk perception, and an overall bad feeling leads to a higher risk 
perception.  This seems to explain why people tend to underestimate risks for actions that also 
have some ancillary benefit—smoking, skydiving, and such—but also has some weirder effects. 
In one experiment,22 subjects were shown either a happy face, a frowning face, or a neutral 
face, and then a random Chinese ideograph.  Subjects tended to prefer ideographs they saw after 
the happy face, even though the face was flashed for only ten milliseconds and they had no 
conscious memory of seeing it.  That’s the affect heuristic in action. 
Another bias is that we are especially tuned to risks involving people.  Daniel Gilbert 
again:23 
We are social mammals whose brains are highly specialized for thinking about others. 
Understanding what others are up to—what they know and want, what they are doing and 
planning—has been so crucial to the survival of our species that our brains have developed 
an obsession with all things human. We think about people and their intentions; talk about 
them; look for and remember them. 
In one experiment,24 subjects were presented data about different risks occurring in state 
parks: risks from people, like purse snatching and vandalism, and natural-world risks, like cars 
hitting deer on the roads.  Then, the subjects were asked which risk warranted more attention 
from state park officials. 
Rationally, the risk that causes the most harm warrants the most attention, but people 
uniformly rated risks from other people as more serious than risks from deer.  Even if the data 
indicated that the risks from deer were greater than the risks from other people, the people-
based risks were judged to be more serious.  It wasn’t until the researchers presented the 
damage from deer as enormously higher than the risks from other people that subjects decided it 
deserved more attention. 
The Psychology of Security—DRAFT 
13 
People are also especially attuned to risks involving their children.  This also makes 
evolutionary sense.  There are basically two security strategies life forms have for propagating 
their genes.  The first, and simplest, is to produce a lot of offspring and hope that some of them 
survive.  Lobsters, for example, can lay 10,000 to 20,000 eggs at a time.  Only ten to twenty of 
the hatchlings live to be four weeks old, but that’s enough.  The other strategy is to produce only 
a few offspring, and lavish attention on them.  That’s what humans do, and it’s what allows our 
species to take such a long time to reach maturity.  (Lobsters, on the other hand, grow up 
quickly.)  But it also means that we are particularly attuned to threats to our children, children in 
general, and even other small and cute creatures.25 
There is a lot of research on people and their risk biases.  Psychologist Paul Slovic seems to 
have made a career studying them.26  But most of the research is anecdotal, and sometimes the 
results seem to contradict each other.  I would be interested in seeing not only studies about 
particular heuristics and when they come into play, but how people deal with instances of 
contradictory heuristics.  Also, I would be very interested in research into how these heuristics 
affect behavior in the context of a strong fear reaction: basically, when these heuristics can 
override the amygdala and when they can’t. 
Probability Heuristics 
The second area that can contribute to bad security trade-offs is probability.  If we get the 
probability wrong, we get the trade-off wrong. 
Generally, we as a species are not very good at dealing with large numbers.  An enormous 
amount has been written about this, by John Paulos27 and others.  The saying goes “1, 2, 3, 
many,” but evolutionarily it makes some amount of sense.  Small numbers matter much more 
than large numbers.  Whether there’s one mango or ten mangos is an important distinction, but 
whether there are 1,000 or 5,000 matters less—it’s a lot of mangos, either way.  The same sort of 
thing happens with probabilities as well.  We’re good at 1 in 2 vs. 1 in 4 vs. 1 in 8, but we’re much 
less good at 1 in 10,000 vs. 1 in 100,000.  It’s the same joke: “half the time, one quarter of the 
time, one eighth of the time, almost never.”  And whether whatever you’re measuring occurs one 
time out of ten thousand or one time out of ten million, it’s really just the same: almost never. 
Additionally, there are heuristics associated with probabilities.  These aren’t specific to risk, 
but contribute to bad evaluations of risk.  And it turns out that our brains’ ability to quickly 
assess probability runs into all sorts of problems. 
The Availability Heuristic 
The “availability heuristic” is very broad, and goes a long way toward explaining how people 
deal with risk and trade-offs.  Basically, the availability heuristic means that people “assess the 
frequency of a class or the probability of an event by the ease with which instances or 
occurrences can be brought to mind.” 28  In other words, in any decision-making process, easily 
remembered (available) data are given greater weight than hard-to-remember data. 
In general, the availability heuristic is a good mental shortcut.  All things being equal, 
common events are easier to remember than uncommon ones.  So it makes sense to use 
availability to estimate frequency and probability.  But like all heuristics, there are areas where 
the heuristic breaks down and leads to biases.  There are reasons other than occurrence that 
make some things more available.  Events that have taken place recently are more available than 
others.  Events that are more emotional are more available than others.  Events that are more 
vivid are more available than others.  And so on. 
The Psychology of Security—DRAFT 
14 
There’s nothing new about the availability heuristic and its effects on security.  I wrote 
about it in Beyond Fear,29 although not by that name.  Sociology professor Barry Glassner 
devoted most of a book to explaining how it affects our risk perception.30  Every book on the 
psychology of decision making discusses it. 
In one simple experiment,31 subjects were asked this question: 