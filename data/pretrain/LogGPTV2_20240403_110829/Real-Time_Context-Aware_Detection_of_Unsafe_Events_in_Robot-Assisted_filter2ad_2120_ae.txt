TABLE IV: Gesture classiﬁcation accuracy in LOSO setup
392
Setup
gesture
speciﬁc
gesture
speciﬁc
gesture
speciﬁc
gesture
speciﬁc
non-
gesture
speciﬁc
Model
LSTM 512,128,
Layers
64,16*
LSTM 128,32,
16,16*
512,128,
32,16*
512,128,
32,16*
Conv
Conv
LSTM 512,128,
64,16*
Features Lr a
All
1e-4
TPR TNR PPV NPV
0.75 0.72 0.67 0.80
C,R,G
1e-4
0.76 0.72 0.67 0.81
C,R,G
1e-4
0.76 0.73 0.68 0.80
All
All
1e-4
0.76 0.73 0.69 0.80
1e-4
0.73 0.71 0.66 0.77
TABLE V: Overall performance of the erroneous gesture classiﬁ-
cation step for Suturing on the dVRK using different setups, for
input time-window=5, stride=1
a Initial Learning Rate, * Fully-Connected Layer
perfect gesture boundaries. This allowed us to independently
evaluate the performance of this module and evaluate differ-
ent architectures and models that are suited for time-series
classiﬁcation,
including LSTM networks and 1D CNNs.
We also experimented with different supervised learning
architectures, from kernel-based models such as SVM to
ensemble techniques such as Random Forest, but here only
report results for LSTM networks and 1D-CNNs for their
superior performance over other architectures. We further
experimented with different subsets of kinematics features,
while using the set of all the features as our baseline. Our
experiments speciﬁcally involved using different combina-
tions of Cartesian Position (C), Rotation matrix (R), Grasper
Angle (G) and Joint Angle (J) variables.
Tables V and VI show the best performing models for each
setup for Suturing and Block Transfer tasks, respectively.
We overall observed that being gesture speciﬁc led to better
accuracy (higher TPR, TNR, PPV, NPV), even with smaller
datasets and that 1D-CNNs performed better than LSTM
networks for binary classiﬁcation of gestures for both the
tasks. Training the models using speciﬁc features (Cartesian,
Rotation and Grasper Angle) led to similar or better perfor-
mance compared to training with all the features. In both
cases, the best performing model had higher TPR and TNR
while achieving competitive NPV and PPV. This suggests
that the models can identify the unsafe gestures with good
accuracy while not providing too many false alerts.
Table VII shows the average AUCs achieved for each
gesture class using the best performing 1D-CNN model.
Our model performed best on gestures G6 and G4 for
Suturing. The common error for both was when the ”Robot
end-effector is out of sight”, which occurred frequently in
the demonstrations and often among surgeons with less
Setup
gesture
speciﬁc
gesture
speciﬁc
non-
gesture
speciﬁc
Model
Conv
Layers
256,128,
64,16*
LSTM 64,32,
64,16*
256,128,
64,16*
Conv
Features Lr a
C,G
1e-4
TPR TNR PPV NPV
0.62 0.87 0.65 0.86
C,G
C,G
1e-4
0.62 0.85 0.57 0.89
1e-4
0.59 0.85 0.58 0.85
TABLE VI: Overall performance of the erroneous gesture classi-
ﬁcation step for Block Transfer on the Raven II using different
setups, for input time-window=10, stride=1
a Initial Learning Rate, * Fully-Connected Layer
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:23:52 UTC from IEEE Xplore.  Restrictions apply. 
Train Size % Errors
Test Size % Errors
Gesture
G1
G2
G3
G4
G5
G6
G8
G9
G5
G6
G11
1432
13728
34921
13339
2717
18923
8413
1769
681976
394077
241067
29
25
41
77
5
74
45
59
24
25
53
358
3432
8731
2601
680
4731
2104
443
151038
88748
53969
28
24
40
79
4
74
29
56
19
21
41
AUC
0.60
0.50
0.70
0.93
0.61
0.93
0.81
0.61
0.72
0.75
0.66
TABLE VII: Performance of the erroneous gesture classiﬁers
expertise. For Block Transfer, G6 had the highest accuracy,
although the common gesture-speciﬁc error in this case is
”Unintentional needle/object drop”. We also measured the
average reaction time for detecting erroneous gestures for
each gesture, as shown in Table IX. In our setup, the best
value would be 0, which is when the detection of erroneous
gesture coincides with the start of the gesture, due to the
design of our pipeline where we ﬁrst detect the gesture
and then the gesture-speciﬁc anomaly, if any. Since we had
erroneous and non-erroneous gestures, we also calculated the
average jitter for erroneous gestures, to see their difference
when compared to the overall average jitter and their effect
on the reaction time. For Suturing, our model performed
best for gesture G4, with an average reaction time of -0.01
frames (- 0.34 ms), as well as competitive average jitter for
erroneous gestures of -84 ms, followed by G1 which had an
average reaction time of -6.0 frames (-167 ms). Gestures G10
and G11 had no common errors and hence no reaction times.
When looking across all gestures, we noticed our model
performed best for gestures which are commonly occurring
in the Suturing and Block Transfer tasks and also have higher
number of errors. Improvement over less common gestures,
with sparse errors will be the focus of future work.
B. Overall Performance of Safety Monitoring Pipeline
We evaluated the overall performance of our Safety Mon-
itoring pipeline for two different setups of gesture-speciﬁc
and non-gesture-speciﬁc. Although we used ofﬂine data for
our analysis, our system can perform the classiﬁcation in
real-time.
Non-Context-Speciﬁc Safety Monitoring: As a baseline,
we trained a classiﬁer with no explicit notion of context in
terms of training labels, by feeding it only the kinematics
data and the corresponding safe/unsafe labels. Due to the
the ability of LSTM networks to recognize varying spatio-
temporal patterns coupled with larger data sizes compared
to gesture-speciﬁc classiﬁers,
the classiﬁer demonstrated
some generalization and attained competitive performance
(see Table VIII). An average F1-score of 0.72 and AUC
of 0.71, reaction time of +6.62 frames or 221 ms, and
computation time of 1.9 ms was achieved for Suturing. For
Block Transfer, the classiﬁer achieved an average F1-score
and AUC of 0.73 and 0.74, respectively. The reaction time
was -15.2 frames or -457 ms.
Context-Speciﬁc Safety Monitoring: In this setup, the
input kinematics samples were ﬁrst passed to the gesture
their corre-
classiﬁer step. Having detected the gestures,
sponding kinematics samples were sent to a separate gesture-
speciﬁc classiﬁer to identify their safety properties. As seen
in Table VIII, for Suturing the average F1-score and AUC
were 0.76 and 0.81, respectively, which is an improvement
over the results obtained with non-context-speciﬁc setup.
The average reaction time was -1.7 frames (-57 ms) and
average computation time was 2.1 ms. For Block Transfer,
the trend in accuracy is similar, with the gesture-speciﬁc
setup achieving a higher average F1-score of 0.88 versus 0.73
and a higher AUC of 0.86 versus 0.74. The higher accuracy,
as reﬂected by F1-score and AUC, provides more evidence
(in addition to distribution analysis in Section III) to support
our hypothesis about the context-speciﬁcity of errors.
The gesture-speciﬁc models had comparatively worse re-
action and computation times than the non-gesture speciﬁc
pipeline due to the latency introduced for identifying the
context before detecting gesture-speciﬁc anomalies. Figure
8 (Case 2) provides examples of how a negative jitter
associated with the detection of the gesture can result in
negative reaction times. However, for Block Transfer, we
were only late by -50.8 frames or 1693 ms and for Suturing,
by 220 ms, while having high accuracy.
Figure 9 compares the worst, best and median performance
of the context-speciﬁc and non-context-speciﬁc setups across
different demonstrations, with the context-speciﬁc pipeline
having an overall better performance. To get an empirical
upper bound for the overall performance of the pipeline,
we evaluated our entire pipeline assuming perfect gesture
boundaries. As shown in Table VIII, when using perfect
gesture boundaries, the average AUC improved from 0.81
to 0.83 and reaction time improved from -57 ms to 53 ms.
VI. DISCUSSION
Our results provide encouraging evidence for the possibil-
ity of accurate and timely detection and possible preemption
of erroneous gestures. Our experiments provided us with a
number of key insights:
Being context-speciﬁc results in more accurate de-
tection of erroneous gestures but worse reaction times.
Table (VIII) shows that there is an improvement of 14.1%
and 16.2% of AUC over non-context speciﬁc detection, for
Setup
Avg.
AUC
Avg.
F1
classiﬁer
Gesture-speciﬁc with per-
fect gesture boundaries for
Suturing
Gesture-speciﬁc
gesture
Suturing
Non-gesture-speciﬁc clas-
siﬁer for Suturing
Gesture-speciﬁc
gesture
Block Transfer
Non-gesture-speciﬁc clas-
siﬁer for Block Transfer
classiﬁer
with
for
with
for
0.83±.14
0.81±.14
0.71±.16