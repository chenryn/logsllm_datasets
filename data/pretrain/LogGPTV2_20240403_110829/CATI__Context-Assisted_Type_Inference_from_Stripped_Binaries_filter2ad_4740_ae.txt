0.57
0.11
0.95
0.15
0.72
0.00
0.28
0.74
0.85
0.00
1.00
1.00
ACC
0.76
0.58
0.45
0.56
0.88
0.91
0.57
0.11
0.93
0.13
0.57
0.00
0.27
0.67
0.50
0.00
0.18
0.88
TABLE V
Support
1400
6874
2697
404
8
13972
153
2654
38591
46
5051
21
1808
70
6188
21
2820
36876
cnt-same
1.38
3.60
1.51
2.40
1.22
4.25
2.90
1.18
3.10
0.81
3.17
1.23
4.72
1.02
2.43
2.57
3.06
2.01
cnt-all
5.37
5.46
4.21
5.11
4.44
6.24
6.30
4.92
5.81
5.22
5.97
3.00
6.72
4.17
5.89
7.86
4.88
5.02
c-rate
25.68%
65.85%
35.77%
47.05%
27.50%
68.02%
45.97%
24.03%
53.36%
15.45%
53.14%
41.03%
70.16%
24.54%
41.29%
32.73%
62.57%
40.09%
EVALUATION RESULT OF EACH VARIABLE TYPE.
in Table III. For Stage 1 to Stage 3-3, the table shows the
performance of classifying pointer and non-pointer, subclass
of pointers, subclass of non-pointer, subclass of type char,
subclass of type float, and subclass of type int. P , R,
F 1 separately represents precision, recall ,and F1-score which
are usual metrics in the machine learning ﬁeld. Stage 1 shows
the best performance overall applications. It is easy to come
out that the features to distinguish pointers and non-pointers
are obvious. The pattern of operations and operands used
by pointer and non-pointers can be widely divergent, which
has employed by previous work to extract rules. Stage 2-
1 doesn’t perform well on these tasks, where the average
measure indexes are about 0.75. Especially, application R
behaves the worst on this stage, which has the most amount
of VUCs tagged as a pointer. The reasonable excuse of the
performance results in stage 2-1 is that the behavior of pointer
variables is too uncertain to capture which cannot be easily
inferred by the instruction context. While in stage 3, stage
3-1 and stage 3-3 seem to do well in classifying the types,
for only two types (char and unsigned char) making up
stage 3-1 and types in int family behaving differently on
the usage of registers. The registers storage different lengths
of int family variables which are very different, let alone
whether the variables are signed or unsigned. It is interesting
to notice that stage 3-2 has a different result with other stages.
Applications gzip, nano, and sed do not have float family
variables. Besides application R, the rest of 8 applications
occupy less than 200 VUCs belongs to float. Even some
of the applications only have 1 VUC tagged as a float.
Fortunately, even the data set is unbalanced in stage 3-2,
application R still achieves a considerable performance to
classify over 10,000 float family variables which are owed
to the usage of instruction context.
To evaluate the performance of CATI in the real world,
we present the result after the voting procedure in Table IV.
Each data in Table IV is one to one corresponding to the data
in Table III. After voting, the wrong case may be adjusted
to the right class by the rule of the minority obeying the
majority. In Stage 1, Stage 2-2, Stage 3-1 and Stage 3-3, the
performance of all the applications has a great improvement
compared to Table III. However, the result of Stage 2-1 and
3-2 has a decrease. In Stage 2-1, each VUC is classiﬁed to
most likely type, but the diverse behavior of pointer family
variables puzzling the voting mechanism to make an uncertain
decision. In Stage 3-2, application bash only has 1 related
variable which consists of 3 VUCs, of which only 1 VUC is
correctly classiﬁed. Therefore, the result comes to zero.
VUC
Accuracy
Variable
Accuracy
inetutils
bash
bison
cﬂow
gawk
grep
gzip
less
nano
R
sed
wget
Total
0.70
0.69
0.69
0.66
0.72
0.67
0.62
0.66
0.65
0.70
0.74
0.65
0.68
Support
79767
20863
9133
53605
23254
4088
130130
7633
21845
616297
18246
38571
1023432
TABLE VI
0.73
0.69
0.72
0.68
0.76
0.75
0.68
0.67
0.67
0.72
0.78
0.66
0.71
Support
12820
4095
1547
8017
3563
725
21183
1563
3703
93495
2637
6426
159774
EVALUATION RESULT ON EACH APPLICATIONS ON THE GRANULARITY OF
VUC AND VARIABLE.
To thoroughly evaluate the system on the pipeline, we test
the pipeline result of each application. Table VI shows the
result of all applications in the test set. Weighted accuracy of
all applications on the granularity of VUC is 0.68, which is test
on over more than 1,000,000 VUCs. And Weighted accuracy
of all applications on the granularity of variable is 0.71, which
is test on over more than 150,000 variables. Here, our system
CATI achieves a considerable result of variable classiﬁcation
of 19 types, and the result after voting mechanism increases by
about 0.03 on the metric of accuracy. Application sed achieves
(cid:26)(cid:22)
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:25:44 UTC from IEEE Xplore.  Restrictions apply. 
the highest accuracy (0.78) of all applications. And even the
worst case of wget achieves an accuracy of 0.66.
Comparison with DEBIN. We set up another experiment
on CATI to compare with the state-of-art method DEBIN [1].
To make the competition as enough fair as possible, we set
the preparation work as similar as possible with DEBIN. We
randomly select 300 binaries from 830 Linux Debug Symbol
Packages (data source of DEBIN), compiled in x86 64 archi-
tecture, to fulﬁll the same type inference mission in DEBIN –
classify 17 different types: struct, union, enum, array,
pointer, void, bool, char, short, int, long and
long long (both signed and unsigned for the last
ﬁve). Our system CATI has a better performance than DEBIN
in a similar situation to ﬁnish the same task, in which we
achieve about 11% higher on accuracy (0.84 V.S. 0.73). As
expected, CATI performs better in the new settings, because
we enrich the features of each variable which overcome the
problem of uncertain samples, and we make the ﬁnal decision
by voting for traceable instructions.
Understanding the Clustering Phenomenon. To validate
the discovery of the Same type variable clustering phe-
nomenon, we study the result of each variable which is shown
in Table V. Column 2 to 5 separately represents the recall
of Stage 1, Stage 2-2, Stage 3 and weighted average ﬁnal
result. Cnt-same, cnt-all and c-rate in column 7 to 9 separately
represents the average number of variable instructions in one
VUC which have the same variable type with the target
variable, the average number of variable instructions in one
VUC, and the ratio which is calculated by the former two
columns. Type double, int, unsigned int perform well
in all stages, which can be explained by the high ratio of
same type variable clustering shown in Column 9. It is worth
to notice that, type bool has good performance with a low
clustering ratio, while type struct has poor performance
with a high clustering ratio. Our explanation is that variables
with bool type have comparatively simple usage pattern and
their instructions are not complex. To contrast, variables with
struct type consist of many members, therefore their usages
are really diverse. Unfortunately, both type long long and
long long unsigned int do not get over Stage 3-3,
even though the clustering rate is considerable, of which the
reason maybe these kinds of variables are few. Last but not
least, the total recall of each variable is roughly positively
correlated with the clustering ratio.
To ﬁnd the decisive factor of prediction result, we bring in
a new measurement index , which is calculated as follows:
k = Su(R(V U Cj, k))
Su(V U Cj)
, k ∈ [1, 21], j ∈ T
(5)
where Su(V U Cj) denotes the conﬁdence of V U Cj pre-
dicted in Stage u, k denotes the kth position of the instruction
in V U Cj, and j denotes the jth VUC in the data set T .
Function R occludes one speciﬁc instruction. As shown in
formula (5), R occludes the kth instruction in V U Cj with
BLAN K. The result of Su(R(V U Cj, k)) indicates the con-
ﬁdence of V U Cj in Stage i without the information of kth
1.09567
0.54245
0.33446
1.02361
0.54310
0.83318
0.61051
0.49985
0.37073
0.85065
0.57795
0.97451
0.96613
0.52061
1.04395
0.97986
1.04600
0.84242
0.65618
0.73875
0.55442
struct
struct
struct
struct
struct
movq   $0x0,0xa8(%rsp) struct*
movq   $0x0,0xa8(%rsp)
je     4179f5 
lea    0x120(%rsp),%rax char
lea    0x120(%rsp),%rax
movslq %esi,%rsi
movl   $0x100,0xb8(%rsp)
movl   $0x100,0xb8(%rsp)
lea    (%rdi,%rsi,1),%r15
movb   $0x0,0xc0(%rsp)
movb   $0x0,0xc0(%rsp)
movl   $0x100,0xd0(%rsp)
movl   $0x100,0xd0(%rsp)
mov    %rax,0xb0(%rsp)
mov    %rax,0xb0(%rsp)
mov    %rax,0xc8(%rsp)
mov    %rax,0xc8(%rsp)
lea    0x220(%rsp),%rax
movb   $0x0,0x18(%rsp)
movl   $0x8,0x40(%rsp)
movl   $0x8,0x40(%rsp)
mov    %rdi,%rbp
mov    %rax,0x30(%rsp) struct*
mov    %rax,0x30(%rsp)