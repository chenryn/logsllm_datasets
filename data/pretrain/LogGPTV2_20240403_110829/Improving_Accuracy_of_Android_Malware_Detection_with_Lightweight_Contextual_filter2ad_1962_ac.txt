callback. The entrypoint MsgService.onHandleIntent belongs to a
subclass that has extended a class in the Android framework, Addi-
tionally, there could be multiple levels of inheritance, as is the case
for MsgButton, whose parent class is AppButton. If a application-
specific entrypoint signature (MsgService) was used to create fea-
tures for each app, the resulting feature space would consist of
sparse feature vectors that included application syntax. Including
application syntax into the feature space makes the model more
susceptible to overfitting and evasion. Overfitting occurs because the
entrypoint signature most likely only belongs to a specific malware
family. Additionally, the model becomes more susceptible to evasion
because entrypoint signatures are developer defined, and the signa-
ture name could be obfuscated and randomized using tools such as
ProGuard [28] resulting in features unique to a single app.
Subclass Resolver. To remove application syntax, we develop a
subclass resolver module that identifies the signature of the base
class in the Android framework which the application specific class
was inherited from. Figure 2(c) shows PIKADROID’s subclass re-
solver, which takes in a reference to the overridden or implemented
entrypoint signature and identifies the signature of the original base
class in the Android framework that was overridden. The resolution
procedure is based on the namespaces provided by Java’s class-
loaders. Specifically, PIKADROID utilizes three classloaders: 1) the
application classloader that loads the app’s code 2) the primor-
dial classloader, which is responsible for loading the Java library
code (java.*), and 3) the extension classloader, which is respon-
sible for loading system-wide, platform-specific extension classes
Figure 2: Detailed illustration of PIKADROID’s static-analysis module.
The static analysis phase applies (a) reachability analysis from each en-
trypoint in the application to identify what sensitive APIs are invokable
from each entrypoint. Next, the (b) class hierarchy is used by the (c)
subclass resolver to generalize the feature space and remove applica-
tion syntax.
used as entrypoints [9]. PIKADROID refines this list into a set of
sensitive entrypoints including component lifecycle methods, system
level callbacks, UI callbacks, and security-sensitive callbacks. Our
static analysis module uses these entrypoint and API lists to produce
a set of all pairs (entrypoint, targetMethod) found in the applica-
tion, which will be passed to the learning module. The following
sections detail each step in the static analysis pipeline.
Preprocessing. PIKADROID first pre-processes the apps for the
static analysis. App’s apk files are disassembled and unpackaged
using apktool [47]. The dalvik classes are retargeted to Java byte-
code using Dare [35], and the bytecode is transformed into Wala’s
internal Intermediate Representation [19] for further analysis.
Next, PIKADROID builds a callgraph for each app being ana-
lyzed. In order to generate a precise callgraph, a starting point for
the application must be defined. Android apps are event driven and
therefore do not have a generic main() method which is present in
standard Java programs. Instead, apps have components which begin
executing in response to events from the device and the user. These
components are implemented by extending a pre-defined component
class provided by the Android framework. Each component is then
statically registered in AndroidManifest.xml so the device knows
what component lifecycle method to call in response to certain events
(PIKADROID identifies dynamically-registered broadcast receivers).
To ensure that the callgraph generation algorithm finds all possible
paths through the app, we create a dummy main method which con-
tains all possible lifecycle transitions for each component registered
in the app’s manifest. This dummy method creation follows an ap-
proach similar to FlowDroid [20]. Next, to model inter-component
communication (ICC), we leverage an approach similar to Epicc [36]
to append ICC edges onto the callgraph.
Entrypoint Discovery. On top of the callgraph, we use an entry-
point discovery method similar to prior work [20, 48, 53] to discover
all the entrypoints in the callgraph which will be used in defining
lightweight context. After the entrypoints have been identified, the
E1E2S1S2SendSMS()getDeviceID()MsgService.onHandleIntentEntrypointsSensitive APIbenignmaliciousgrayMsgButton(a) Reachability analysis(b) Class HierarchyIntentServiceMsgServicejava.*android.*AppButtonMsgButtonServiceExtensionButton(c) Subclass ResolverIntentServiceMsgServicejava.*android.*AppButtonMsgButtonServiceExtensionButtonMsgButton.onClickButton.onClick214ACSAC ’18, December 3-7, 2018, San Juan, PR, USA
(Android.*.) The classloaders form a class hierarchy tree whose
leaves correspond to app specific classes and whose nodes towards
the root are from the Android library. For example, Figure 2(b),
shows a snippet of the class hierarchy tree for MsgButton. When
the subclass resolver needs to identify which base class is overrid-
den, it performs a backwards traversal of the class hierarchy, from
the node belonging to the method signature in the app. The traver-
sal stops when it reaches a class that is provided by the extension
classloader. For example, if the input into the subclass resolver
was the signature, MsgService.onHandleIntent the output would be
IntentService.onHandleIntent. The final output of PIKADROID’s
static analysis module is the set of resolved pairs of (entrypoint,
targetMethod) pairs where the entrypoint’s class belongs to the
Android framework These generalized features are then passed to
PIKADROID’s learning module.
3.3 Learning Module
The intuition behind our features is shown in the following simple
example. In our dataset, benign and malicious applications used
the sensitive method HttpClient.execute at similar rates. However,
in Table 2, we provide the likelihood of a malicious application
invoking the method from different entrypoints. Despite this method
being used with similar frequency by both benign and malicious
applications, malicious applications were 6.89 times more likely to
invoke it from the Service.onStart entrypoint and 15.15 times more
likely to invoke it from Activity.onRestart compared to benign
applications. We use this intuition to inform our feature extraction
method.
To learn the different lightweight contexts associated with each
entrypoint, PIKADROID uses a training set of apps, T . For each
app a ∈ T , PIKADROID first performs static analysis to extract the
set of (entrypoint, targetMethod) tuples as detailed in the prior
subsection. Then, the system partitions T into two sets, M and B,
where M ⊂ T is the set of malicious applications and B ⊂ T is the set
of benign applications. Next, we assign a risk score, re,s , to each pair
that captures the relative maliciousness of the API, s being invoked
from e. The risk score is calculated as the ratio of how often the pair
(e, s) was found in malicious apps to benign apps. Equation 1 shows
the equation PIKADROID uses to calculate this risk value for each
pair, (e, s).
re,s =
|{a|a ∈ B ∧ (e, s) ∈ a}| × |M|
|{a|a ∈ M ∧ (e, s) ∈ a}| × |B|
(1)
The final phase of PIKADROID’s analysis is feature construction
and model training and testing. The feature vector for each a ∈ T is
va = ⟨re,s : (e, s) ∈ a⟩, the union of risk values for each (e, s) ∈ a.
The ground truths are also created in this step and are equal to
0 ∀ a ∈ B and 1 ∀ a ∈ M. The results of these tests are presented and
discussed §6.4
4 IMPLEMENTATION
We implemented a prototype of our system, PIKADROID. The static
module was implemented in 8 thousand lines of Java code, based on
IBM’s WALA Static Analysis Libraries [19]. We leveraged WALA to
generate the callgraph, reachability analysis, and entrypoint analysis.
The learning module was implemented using 3 thousand lines of
Dataset
2010-2012
2013-2015
2016-2018
Total
Time Period Malicious
2010 - 2012
2013 - 2015
2016 - 2018
2010 - 2018
3,970
2,158
2,270
8,398
Benign Grayware
3,788
3,596
5,000
12,384
1,524
1,325
—
2,849
J. Allen et. al
Total
9,282
7,079
7,270
23,631
Table 3: Summary of the datasets used for evaluations.
Python code, and was based on Scikit-learn [38], a machine-learning
Python library.
5 DATASET
The dataset used to extensively evaluate PIKADROID contained
malicious apps crawled from over 16 official and third-party Android
app marketplaces collected using Androzoo [4]. We crawled the
benign apps from Google Play. The dataset contained over 23,631
applications overall, Our final dataset includes 8398 malicious apps,
12384 benign apps, and 2849 grayware apps. Additionally, this
dataset spanned a time-period of eight years. Using such a wide
range of diverse sources is necessary to conduct a robust evaluation
due to the app sampling problem in Android [32]. Next, we leveraged
Virus Total [45] for label verification and to identify grayware. To
be able to assess how well PIKADROID handles concept drift [25],
we manually partitioned the dataset into three different time periods,
2010-2012, 2013-2015, 2016-2018. We used the date when the app
was signed to assign an app to each set. A detailed description of
each partition is shown in Table 3.
6 EVALUATION
Our evaluation addresses the following research questions:
• How effective is PIKADROID in improving the accuracy of
Android malware detection?
• How does the lightweight circumstantial awareness model
perform compared with existing state-of-the-art systems? Par-
ticularly, how does PIKADROID handle “concept drift”, a key
challenge in Android malware detection?
• How robust is PIKADROID against obfuscation?
• What is the runtime performance of PIKADROID?
We perform a comprehensive evaluation covering a variety of
cases in verifying the classification of Android malware [25, 26].
First, we focus on the effectiveness of PIKADROID in terms of ac-
curacy, precision, recall, f1-score, and false-positive rating. Second,
we perform side-by-side comparisons in terms of detection accuracy
between PIKADROID and three prior state-of-the-art detection sys-
tems: AppContext [52], MaMaDroid [31] and DroidAPIMiner [2].
In particular, we compare how these systems handle “concept drift”,
a key challenge that can significantly affect detection effectiveness.
Third, to test the counter-obfuscation ability of PIKADROID, we
use a state-of-the-art obfuscation tool, AVPass [27] to generate ob-
fuscated variants of the malicious samples and test PIKADROID’s
accuracy when classifying these apps. Fourth, we measure the perfor-
mance with different datasets spanning multiple years. To evaluate
PIKADROID, we use the datasets discussed in §5 and 10-fold cross
validation for all tests.
215Improving Accuracy of Android Malware Detection with Lightweight Contextual Awareness
ACSAC ’18, December 3-7, 2018, San Juan, PR, USA
Dataset
Samples
Time Period
Malware vs. Benign
Grayware vs. Benign
(Mal-&Grayware) vs. Benign
2010-12
2013-15
2016-18
2010-18
2010-12
2013-15
2010-15
2010-12
2013-15
2010-18
Accuracy (%)
Pika MaMa
97.61
94.66
98.43
97.55
97.56
96.53
98.00
95.67
98.15
95.68
96.91
92.63
96.83
93.22
97.55
96.99
96.54
93.57
97.79
92.63
Precision (%)
Pika MaMa
98.29
97.13
99.24
97.55
97.08
96.90
98.52
95.81
97.98
97.85
97.80
91.09
96.92
94.86
97.87
97.55
97.59
94.60
98.27
93.90
F1 Score (%)
Pika MaMa
97.65
94.64
97.89
96.70
96.07
94.27
97.41
94.58
96.74
92.02
94.04
85.45
94.15
86.78
97.44
96.70
96.44
93.39
97.47
92.12
Recall (%)
FP Rating
Pika MaMa
97.02
92.28
96.57
95.87
95.24
91.61
96.33
93.38
95.53
86.84
90.57
80.47
91.54
79.97
97.03
95.87
95.32
92.21
96.70
90.40
Pika MaMa
1.76
2.87
0.44
1.45
1.25
1.38
0.96
2.77
0.77