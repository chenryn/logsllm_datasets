EpicRec framework provides better privacy and utility than
[24] from both theoretical and empirical perspectives.
3. EpicRec SYSTEM DESIGN
In this section, we present the framework design of our
proposed Enhanced Privacy-built-In Client for Recommen-
dation (EpicRec) system. The goal of EpicRec system is
three-fold: (1) enable user-friendly privacy concern control
on their private data in a way they understand; (2) quantify
user’s privacy level input from layman and user-understandable
language to quantiﬁed private budget for data perturbation;
(3) conduct light-weight perturbation of user private data on
their device such that the perturbed data can be released to
existing recommender systems to provide user high-quality
personalized recommendations. Next, we ﬁrst brieﬂy dis-
cuss about background to motivate and guide system design.
We then describe the overall architecture of EpicRec system
and the details of each component.
3.1 Background & Motivation
We ﬁrst give a succinct overview of the key results from
our two user studies (details are in separate papers to be
submitted with preliminary results in [29, 26]), in order to
motivate and guide the design of EpicRec system.
The ﬁrst user study focuses on the research question: Which
information is considered as private by users? Using a popu-
lar video recommender system as an example, we conducted
an online user study by recruiting 161 participants through
Amazon Mechanical Turk (MTurk) and studied 11 types of
data collected by smart TV. The majority of the participants
were male (66.5%) and White (75.8%). There are 77.0% of
participants aged between 20 and 40. We created 11 types
of data collected by smart TV including content, channel,
time(watch, change, service), status(on/oﬀ, on duration, if
using DVR), TV settings, clicked buttons and interacted ser-
vices. The results show that most participants raised their
most privacy concerns about their watching content history
for either personalized program recommendation or targeted
advertising purposes.
The second user study focuses on the research question:
How does level of control in a smart TV inﬂuence user’s
perceptions and behaviors? We recruited 505 participants
through MTurk and studied 15 diﬀerent privacy control mech-
anisms with diﬀerent levels and types of control. The major-
ity of the participants were male (57.0%) and White (75.8%).
The average age was 34.15 (range 19-72). We created 15
privacy control conditions by combining the following as-
pects: non-hierarchical and hierarchical controls on diﬀer-
ent content types (overall, category, maturity rating, watch-
ing time). The results show that the overall privacy control
and ﬁner-grained category-based privacy control are the best
two control interfaces the participants selected among the 15
diﬀerent designs. The participants rated them as the most
useful in helping to make data disclosure decisions; the least
privacy concerns; the most valuable in disclosing their in-
formation for personalized recommendations; and the most
likely to use in the future.
3.2 EpicRec Architecture
Figure 3 shows the overall architecture of EpicRec system.
Our focus is on user’s device side where EpicRec system sits
while the service provider remains unchanged. The goal of
device-side EpicRec system is to perform data perturbation
on user private data with user-speciﬁed privacy concern lev-
els, such that the format of perturbed data remains the same
and recommendation results remain accurate.
182releasing protected data, and “All Release” as releasing all
private data (Note that ”All Release” will release as much
information as possible if conﬂict happens in category-based
privacy control). The designs of EpicRec system with these
two privacy controls are later presented in Section 4 and Sec-
tion 5 respectively, referred to as S-EpicRec and M-EpicRec
systems.
C-4. Data Perturbation: perturb private user data
from C-2, using public information from C-1 and the pri-
vacy parameters from C-5. The perturbed data maintains
the same format as user private data, such that it can be
used by existing recommender algorithms. In addition, the
perturbed data needs to meet two conﬂict goals, privacy
preservation and recommendation accuracy. As such, this
data perturbation module is associated with two correspond-
ing notions: privacy notion and utility notion. Examples of
privacy notions can be diﬀerential privacy, k-anonymity, in-
formation gain, etc. while examples of utility notions can be
mean absolute error, root mean square error, TopK, etc.
C-5. Privacy Quantiﬁcation: quantify user speciﬁed
privacy concern levels to mathematical privacy parameters
to be used in data perturbation (C-4) component. The ex-
amples of these quantiﬁed parameters based on diﬀerent pri-
vacy notions can be as follows: (1) the privacy budget  in
diﬀerential privacy; (2) the value of k in k-anonymity pri-
vacy; (3) the threshold of information gain; etc.
C-6. Recommendation Output: output recommenda-
tion results (e.g., overall recommendation, per-category rec-
ommendation) to users obtained from service provider using
perturbed user data.
4. DESIGN OF S-EpicRec:
SINGLE-LEVEL PRIVACY CONTROL
In this section, we focus on the design of Single-level Epi-
cRec (S-EpicRec) to enable overall privacy control. In the
rest of this section, we ﬁrst introduce our focus of privacy
and utility notions, and some general notations. Then we
present the detailed design of the main components (data
perturbation (C-4) and privacy quantiﬁcation (C-5) compo-
nents) in S-EpicRec.
4.1 Privacy & Utility Notions
4.1.1 Privacy Notion
We consider using the state-of-the-art privacy notion, Dif-
ferential Privacy [8], which not only provides strong privacy
guarantee but also allows attackers to have unlimited back-
ground knowledge. Informally, an algorithm A is diﬀeren-
tially private if the output is insensitive to any particular
record in the dataset.
Definition 1
(-Differential Privacy). Let  > 0
be a small constant. A randomized algorithm A is -diﬀerentially
private if for all data sets D1 and D2 diﬀering on at most
one element, i.e., d(D1, D2) = 1, and all S ⊆ Range(A),
Pr[A(D1) ∈ S] ≤ exp()Pr[A(D2) ∈ S]
The probability is taken over the coin tosses of A.
(4.1)
The parameter  > 0 is called privacy budget, which allows
user to control the level of privacy. A smaller  suggests more
limit posed on the inﬂuence of an individual item, leading
Figure 3: Architecture of EpicRec System
Motivated by the user study results in Section 3.1, our
proposed EpicRec system provides users their most preferred
overall and category-based privacy controls. The goal of data
perturbation is to protect user concerned private history data.
Speciﬁcally, EpicRec enables the protection of both individ-
ual records and categories of history data. Last but not least,
EpicRec focuses on perturbing user’s history data rather than
rating data since users are more concerned about history data
than rating data (based on our ﬁrst user study) and history
data can be easily obtained implicitly from user’s client while
it is infeasible to continuously request user interaction to rate
each item in large amount of his history data.
As one can see in Figure 3, the dashed lines indicate the
interactions between user and EpicRec. A user inputs his
privacy levels using user privacy control input (C-3) and pro-
vides his private data on device to user private input (C-2).
On the other hand, the solid lines indicate the interactions
between diﬀerent components. We next discuss the details
of each component as well as the interactions between them:
C-1. Public Data Input: obtain public knowledge as-
sociated with user private data from either inside or outside
knowledge resources. Speciﬁcally, C-1 collects two types of
data from public resources: public data universe items and
their associated public categories. The goal of this compo-
nent is to preserve the quality of perturbed data without
sacriﬁcing any privacy breach that could be derived using
public information.
C-2. User Private Data Input: obtain user history
data without extra user interaction, such as location history
data on iPhone; user’s web browsing history based on the
websites users clicked, or history of watched TV programs,
movies on smart TV, etc.
C-3. User Privacy Control Input: provide user inter-
face to obtain user’s privacy concern levels. Motivated by
our user study results discussed in Section 3.1, C-3 provides
the following two granularities of privacy control interfaces:
Overall (Single-level) Privacy Control:
Provide users a single input of privacy concern level;
Category-based (Multiple-level) Privacy Control:
Provide users inputs of privacy concern levels for each
category;
In each control, we use three privacy concern levels: “No
Release” as releasing no information, “Perturbed Release” as
C‐4. Data PerturbationC‐4. Data PerturbationC‐5. Privacy QuantificationC‐5. Privacy QuantificationC‐3. User Privacy Control InputC‐3. User Privacy Control InputEpicRecon DeviceRecommendation using PerturbedUser DataRecommendation using PerturbedUser DataC‐2. User Private Data InputC‐2. User Private Data InputC‐1. Public Data InputC‐1. Public Data InputExisting Recommender SystemC‐6. Recommendation OutputC‐6. Recommendation OutputPerturbedUser DataPerturbedUser Data183Table 2: Notations
Symbol
Description
I
C
C
dr
dp
PT

public item set of size n
public category set of size c
public item-category correlation matrix of size n × c
user’s private item vector of size n
user’s perturbed item vector of size n
privacy concern level
quantiﬁed privacy budget
to stronger privacy protection. More importantly, the ap-
plication of diﬀerential privacy ensures perturbed data in-
dependent of any auxiliary knowledge [7] the adversary may
obtain from public data in C-2.
4.1.2 Utility Notion
Recommender systems typically require many users’ his-
tory data for providing each user a list of his/her personal-
ized recommendations. However, when perturbing data on
each user’s device side under untrusted server settings, the
device does not even know other users’ data (no matter pri-
vate or perturbed). Therefore, it is impractical to directly
use the quality of recommendation results as a utility notion.
As such, we alternatively consider using similar utility in
[24] to measure data category aggregates on each user’s per-
turbed data as our utility notion. More speciﬁcally, we use
expected Mean Absolute Error (MAE) between user’s raw
and perturbed category aggregates, which is shown later in
experiment to be suﬃcient to guarantee recommendation
accuracy even without knowing other users’ data.
4.2 Notations
We deﬁne notations based on in each component:
C-1: Let I be public universe/set of items of size |I| =
n. Public category set is deﬁned as C of size |C| = c, in
which each item is associated with a subset of categories
represented by a public item-category correlation matrix C
of size n × c. An item i is associated with category j if and
only if the entry cij in C is equal to 1.
C-2: User’s raw private history is denoted as a vector dr
of size n. The ith entry in dr is either 1 or 0, meaning
that item i does or does not belong to user’s private history.
Note that those items in private history but not in collected
public set will be simply be considered no release.
C-3: User’s privacy concern level, denoted as PT, belongs
to one of the following three levels, {“No Release”, “Per-
turbed Release”, “All Release” }. When PT is selected as
“No Release” or “All Release”, the device simply releases no
private or all private data to recommender system respec-
tively.
C-4: User’s perturbed data is denoted as a vector dp of
size n. The ith entry in dp is either 1 or 0, meaning that
item i does or does not belong to user’s perturbed data.
C-5:  is the quantiﬁed privacy parameter (privacy bud-
get in diﬀerential privacy) when PT is selected as Perturbed
Release.
For simplicity and consistency, we denote the ith entry in
a vector v as v(i) in the rest of paper. For reference, we list
all notations in Table 2.
4.3 Design of Data Perturbation (C-4)
As described in Section 3.2, data perturbation compo-
nent (C-4) generates perturbed user data to meet both pri-
vacy preservation and recommendation quality, speciﬁcally
via the privacy and utility notions in Section 4.1. The rest
of this subsection consists of problem deﬁnition, challenges,
proposal algorithm and theoretical analysis.
4.3.1 Problem Deﬁnition
Problem 1
(S-Perturbation Problem). Given a user’s
private item vector dr associated with public item set I, a
public item-category correlation matrix C, privacy budget
 > 0. The objective is to generate the user’s perturbed item
vector dp such that (1) (privacy goal) the category aggre-
gates (number of items belonging to each category) of per-
turbed data satisfy -diﬀerential privacy with the presence or
absence of an individual item to defend against privacy leak-
age via public category information; (2) (utility goal) the
quality of category aggregates on perturbed data is well main-
tained using metrics in Section 4.1.2. Speciﬁcally, the for-
mal mathematical deﬁnition of Expected Mean Absolute Er-
ror (MAE) is deﬁned as E
, given
user raw and perturbed category aggregates CR and CP.
j=1 |CR(j) − CP (j)|(cid:105)
(cid:80)c
(cid:104) 1
c
Remarks: Our deﬁned S-Perturbation problem targets on
a stronger privacy guarantee (-diﬀerential privacy rather
than (, δ)-diﬀerential privacy in [24]) and relaxes the objec-
tive (discard the maximization of diﬀerence between private
and perturbed data in [24]), which is shown in later experi-
ments that does not hurt the quality of data perturbation.
4.3.2 Challenges
Large Magnitude of Noises for Achieving  Dif-
ferential Privacy. One of the most widely used mecha-
nisms to achieve -diﬀerential privacy is Laplace mechanism
[8] (Theorem 1), which adds random noises to the numeric
output of a query, in which the magnitude of noises follows
Laplace distribution with variance ∆f
 where ∆f represents
the global sensitivity of query f (Deﬁnition 2).
Definition 2
(Global Sensitivity [8]). For a query
f : D → Rk, the global sensitivity ∆f of f is
∆f = max
d(D1,D2)=1
(cid:107)f (D1) − f (D2)(cid:107)1
(4.2)
for all neighboring datasets D1, D2, i.e., d(D1, D2) = 1.
Theorem 1
(Laplace Mechanism [8]). For f : D →
Rk, a randomized algorithm Af = f (D) + Lapk( ∆f
 ) is -
diﬀerentially private.
(The Laplace distribution with pa-
rameter β, denoted Lap(β), has probability density function
(z) = 1
β ) and cumulative distribution function
2 (1 + sgn(z)(1 − exp(− |z|
2β exp(− |z|
β ))).)
1
Unfortunately, as an item usually belongs to many cate-
gories, the naive application of Laplace mechanism results
in the signiﬁcantly large noise magnitude and uselessness of
perturbed data because of large global sensitivity.
Intractability of Generating Useful Perturbed Data.
Even after the noise magnitude is determined, the data per-
turbation still remains intractable (NP-hard) when we need
to guarantee the usefulness of perturbed data.
4.3.3 Proposed S-DPDP Approach
In this subsection, we propose a novel Single-Level Diﬀer-
entially Private Data Perturbation (S-DPDP) Algorithm to
184solve Problem 1. In general, S-DPDP algorithm consists of
two phases to overcome the aforementioned two challenges:
(1) Phase 1, noise calibration, focuses on selecting the mag-
nitude (denoted as z(j)) for each category using public do-
main knowledge that determines injected Lap(z(j)) noises
for each category; (2) Phase 2, data sanitization, aims to
generate the useful perturbed data based on the noisy cat-
egory aggregates. Note that this phase will not lead to any
privacy loss without the access to user private data. We