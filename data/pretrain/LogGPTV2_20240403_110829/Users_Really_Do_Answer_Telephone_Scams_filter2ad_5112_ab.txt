IRS
IRS
IRS
IRS
IRS
IRS
ASU
ASU
ASU
Scenario
Tax Lawsuit
Tax Lawsuit
Tax Lawsuit
Tax Lawsuit
Unclaimed Tax Return
Tax Lawsuit
Tax Lawsuit
Payroll Withheld
Payroll Withheld
Bonus Issued
Table 1: Table of all experiments and their attributes.
procedure also must be carried out ethically and minimize
potential harm to the participants.
To ensure that the procedure is standardized, we used an au-
todialer to automate the process of sending out the telephone
calls and collecting the recipients’ responses.
Every experiment followed a standard procedure that is
summarized in Figure 1. The procedure has several steps that
require inputs from the recipient. The purpose of this action
is to reduce the likelihood of recipients making random input
actions without hearing the announcement. The action also
helps to ﬁlter out answers from answering machines. Note
that a recipient could break off from the procedure at any
point by simply disconnecting the phone, hence not every
recipient follows the procedure until the end.
The procedure ﬁrst begins with a ring on the recipient’s
work phone (the recipient does not expect the call). When the
phone is ringing, the incoming call screen shows the caller
ID and, in experiment E9, the caller name. An example of
the incoming call screen is shown in Figure 2a. In all of our
experiments, the caller ID showed up as 91XXXXXXXXXX,
where XXXXXXXXXX is the caller ID used in the respective
experiment. Our university’s work phone adds a 91 preﬁx to
every incoming phone call from an external source as all of
the calls were distributed from an external telephone service
provider, similar to what a real-world scammer would do.
For Experiment 9, the incoming call screen also shows
a caller name as shown in Figure 2b. This experiment was
designed to simulate a scammer spooﬁng a known caller
name. For legal and ethical reasons, we did not actually spoof
a phone number. Instead, we asked our telephone service
department to temporarily create a new contact in the uni-
versity’s internal phone directory and associated a legitimate
sounding HR department name “W-2 Administration” with
the telephone number. In a normal external call, there is no
caller ID displayed, however, IT was able to help us create
the caller ID shown in Figure 2b. While a scammer would not
be able to create a new name, they can spoof the caller ID of
a known caller with a targeted spearphishing scam.
If the call is answered, it starts by playing a prerecorded
scenario announcement message (which is different for each
scenario). The prerecorded scenario announcement message
incorporates the voice attribute properties of each particu-
lar experiment. We crafted the four different announcement
messages to mimic what a real-world scammer would say by
using words and sentences from our collected scam samples.
In the Tax Lawsuit scenario, we claimed to be the IRS and
presented a scenario where the recipient had to act because
of a tax lawsuit. The transcript of the announcement message
is in Appendix A.1. In the Unclaimed Tax Return scenario,
we claimed to be the IRS and presented a scenario where the
recipient had to act because of an unclaimed tax return. The
transcript of the announcement message is in Appendix A.2.
In the Payroll Withheld scenario, we claimed to be ASU “HR”
department and presented a scenario where the recipient had
to act because pay would be withheld. Our university has a
publicly available payroll calendar on the HR department’s
website3, hence a real-world scammer could also use this in-
formation to craft an announcement message based on the
payroll information. The transcript of the announcement mes-
sage is in Appendix A.3. In the Bonus Issued scenario, we
claimed to be ASU “HR” department and presented a scenario
where the recipient had to act because a performance bonus
was issued. The transcript of the announcement message is in
Appendix A.4.
Every scenario announcement message requests the recip-
ient to enter 1 to continue to the next step for a follow-up
message (same for every participant). After pressing 1, the
follow-up message asks the recipient to enter the last four
digits of their Social Security number and mimics the process
of connecting the phone call to a live agent. The transcript of
the follow-up announcement message is in Appendix B.
In the real world, the last four digits of the Social Secu-
rity number can be used to perpetrate ﬁnancial and identity
fraud [27]. Other parts of the Social Security number can
also be inferred from the recipient’s phone number [28]. To
minimize potential risk to the recipient (with cooperation
and consultation with our IRB), we did not record which dig-
its were pressed, we instead recorded only if any digit was
pressed.
This then led to a debrieﬁng announcement and a request
to participate in our phone survey. The transcript of the de-
brieﬁng message is in Appendix C. To emphasize the fact
that whatever they listened to was not a real scam, the de-
brieﬁng announcement and survey questions were recorded
with the researcher’s real voice. The post-debrieﬁng survey
3https://cfo.asu.edu/payroll-calendars
1330    28th USENIX Security Symposium
USENIX Association
(a) All experiments except experiment E9
(b) Experiment E9 with caller name displayed
Figure 2: Incoming call screen of different experiments.
consisted of two questions: (1) a survey question that asked
whether the recipient was convinced by the scam (transcript
in Appendix D.1) and, depending on how they responded, (2)
asked what factor convinced them of the scam (Appendix D.2)
or convinced them not to believe the scam (Appendix D.3).
We recorded the participant’s voice recording for the second
question. After the second survey question, the autodialer sys-
tem plays an ending message stating the researcher’s contact
information (transcript in Appendix E).
In summary, during each step of the procedure, the autodi-
aler was conﬁgured to collect the following inputs from the
recipient: Continued, Entered SSN, Convinced, Unconvinced,
and Recording.
3.5 Ethics
These experiments were a deceptive study on involuntary
participants, and therefore we deeply considered the ethical
issues. To address the ethical issues inherent in our experi-
ments, we carefully designed the experiments and worked
with our university’s IRB, to not simply obtain approval but
to conduct the study minimizing harm. This is important be-
cause, to have scientiﬁcally valid results, we could not obtain
informed consent (this would bias the results of the study) and
we must deceive the participants (they would need to believe
that the call was an actual scam call). To protect our partici-
pants, we implemented several safeguards in the experimental
design.
The nature of this experiment, studying telephone phishing
attacks, involves deception as well as involuntary participa-
tion. Both aspects are critical to receiving scientiﬁcally valid
results—informing the participants of the study would sig-
niﬁcantly bias the results. However, the use of deception can
harm the recipients, by wasting their time, confusing them, or
leading them to believe they fell victim to a scam. Therefore,
our debrieﬁng served to not only inform the participants of
the study, but to also educate recipients about the dangers of
telephone scams. In addition, we only called each participant
once throughout the entire study duration (to minimize the
disruption).
Before proceeding with the study, we also worked with
our university’s IT security group to provide them with in-
formation that would help to alleviate the concerns of our
participants. This IT security group at ASU is responsible
for the security of all aspects of the university. We shared
with the security group the experiment contact list, the exper-
imental design, and the incoming phone numbers (that we
used to send the calls) so that the help desk personnel could
be prepared to handle any requests and reports. In this way,
our participants who reported the scam calls to IT would be
assured that it was part of a study.
In recording the results, we also strove to do so ethically
and in accordance with established IRB protocols. One of
the major safeguards is that we did not record the Social
Security number. While a spammer would typically want the
Social Security number, all that we record is the fact that
they pressed any digit. In fact, we did not even ask for the
full Social Security number, and we performed no analysis
to see if they provided nonsensical last four Social Security
numbers. This has the drawback of decreasing the validity of
our data—participants may have felt safe to input only the
last four of their Social Security number (when they would
not input the full number) or they input fake last four digits of
their Social Security number. Although these measures may
diminish the strength of our data, we believe ethics is a more
important aspect of designing a telephone phishing study.
3.6 Dissemination
We ran the previously described procedure using the 10 de-
scribed experiments during a workweek in the late March of
2017, during core working hours of 10:00am–5:00pm each
day. We used an Internet-hosted autodialer4 to automate the
process of sending out the telephone calls to the 3,000 recipi-
ents. Each experiment’s calls were simultaneously distributed
4https://www.callﬁre.com/
USENIX Association
28th USENIX Security Symposium    1331
during the experiment period at a rate of 1–3 live calls per
experiment.
We associated each experiment with a unique caller ID. In
all experiments, the vast majority of the outbound calls did
not reach a live recipient and were answered by a voicemail
answering machine. If a recipient could not answer the phone,
the recipient could use the caller ID in their call history to call
us back. As each experiment had a unique caller ID, the return
call would be directed to that particular experiment’s proce-
dure. When a recipient called back, the same procedure was
administered where a prerecorded scenario announcement
message is ﬁrst played.
While disseminating the phone calls, several unexpected
events impacted our study.
The ASU school of journalism and mass communication
identiﬁed the scam call incidents only 2 hours and 45 minutes
from the launch of the experiments on the ﬁrst day. Instead of
reporting it to the university help desk (who were prepared
and aware of our study), the school sent out a mass email
warning all journalism staff and faculty at 4 hours 28 minutes
from launch. However, we did not notice a signiﬁcant dip in
the number of recipients that continued with our scam calls
as the portion of work phones at the journalism department
represents less than 2% of our sample population.
At 4 hours and 22 minutes from the launch of the experi-
ments, our university’s telephone service ofﬁce also started
blocking some of our phone calls as they were receiving sys-
tem alerts of too many incoming phone calls exhausting the
telephone trunk routes. We worked with the telephone service
ofﬁce to get our calls unblocked within the next 4 hours as
we decreased the simultaneous call rate of our phone calls to
one per experiment.
The IRB ofﬁce also received some complaints (we were not
told exactly how many) regarding the scam call experiments,
which resulted in our experiments being paused for roughly
12 hours (start to ﬁnish) starting on day 2, as we waited for the
IRB committee to review the complaints. The IRB examined
our procedures and decided that, as our study was originally
designed, the beneﬁcence outweighed the harm (as evidenced
by the complaints) and allowed the study to proceed.
A summary showing how these events affected our calls
is shown in Figure 3. In the end, despite the unexpected
events, we ﬁnished sending out the telephone calls to the
3,000 recipients as planned before the end of the workweek.
4 Results and Analysis
The input data collected from the 3,000 recipients are pre-
sented in Table 2. Across all 10 experiments of 3,000 to-
tal recipients, 8.53% (256/3000) of all recipients continued
after listening to the scam scenario announcement, 3.73%
(112/3000) of all recipients called back after receiving the
initial call from us, 4.93% (148/3000) of all recipients entered
at least a digit when requested to enter the last four digits of
Figure 3: No. of recipients pressed 1 to continue the received calls
over the experiment time.
their Social Security number, 1.17% (35/3000) of all recipi-
ents explicitly stated that they were convinced by the scam,
and 1.23% (27/3000) of all recipients explicitly stated that
they were not convinced by the scam.
Before presenting our analysis of the experiments, we ﬁrst
discuss our methodology to systematically analyze their rel-
ative effectiveness. The ﬁrst step of performing the analysis
is to decide on metric(s) that will be used as the standard of
measurement. To chose an ideal metric, we believe a good
metric should not only be quantiﬁable but also be a proxy
for what ultimately matters. From the telephone scammers’
perspective, the ultimate goal is to collect as many Social
Security numbers as possible for the purpose of conducting
identity fraud.
We could use the metric of Entered SSN, which is the num-
ber of participants that entered any value for their Social Se-
curity number (SSN). However, as discussed in Section 3.5,
we did not collect the SSNs input by the user. Although this
seems to be an ideal metric to estimate the number of SSNs
collected, there is still the possibility that the recipient may
have tried to enter a fake Social Security number. In fact, in
some of the recordings, a few recipients stated that they did
not enter their real Social Security number information.
Therefore, we need to derive a metric that could provide a
reasonable estimate of the actual number of real SSNs given to
us in each experiment. Convinced is the metric of the number
of recipients that explicitly stated that they were convinced
by the scam after the ﬁrst survey question. This metric is the
most conservative for estimating attack success. However,
with the low number of responses, participants rarely made it
to that step. Using this metric would exclude a large number
of recipients that fell for the scam but declined to participate
in the phone survey after the debrieﬁng announcement.
Because we cannot assume that all SSNs entered were real,
to reduce these types of false positives, we could create a new
metric and remove the participants that entered their SSNs
and then subsequently stated that they were unconvinced by
1332    28th USENIX Security Symposium
USENIX Association
No.
E1
E2
E3
E4
E5
E6
E7
E8
E9
E10
Total
Continued
12
19
13
23
9
9
13
53
60
45
256
4.00%
6.33%
4.33%
7.67%
3.00%
3.00%
4.33%
17.67%
20.00%
15.00%
8.53%
Unconvinced
Callbacks
4
1.33%
7
1.00%
3
7
0.67%
2
6
1.00%
3
14
0.33%
1
3
0.67%
2
7
5
1.67%
8
3.00%
9
22
1.33%
4
15