We now have a procedure s.t. if you give me a guess at µ , µ .. µ
1 2 k,
I can tell you the prob of the unlabeled data given those µ‘s.
Suppose x‘s are 1-dimensional.
(From Duda and Hart)
There are two classes; w and w
1 2
P(y ) = 1/3 P(y ) = 2/3 σ = 1 .
1 2
There are 25 unlabeled datapoints
x = 0.608
1
x = -1.590
2
x = 0.235
3
x = 3.949
4
:
x = -0.712
25
©2005-2007 Carlos Guestrin
Duda & Hart’s Example
We can graph the
prob. dist. function
of data given our
µ and µ
1 2
estimates.
We can also graph the
true function from
which the data was
randomly generated.
• They are close. Good.
• The 2nd solution tries to put the “2/3” hump where the “1/3” hump should
go, and vice versa.
• In this example unsupervised is almost as good as supervised. If the x ..
1
x are given the class which was used to learn them, then the results are
25
(µ =-2.176, µ =1.684). Unsupervised got (µ =-2.13, µ =1.668).
1 2 1 2
©2005-2007 Carlos Guestrin
Duda & Hart’s Example µ
2
Graph of
µ1
log P(x , x .. x | µ , µ )
1 2 25 1 2
against µ (→) and µ (↑)
1 2
Max likelihood = (µ =-2.13, µ =1.668)
1 2
Local minimum, but very close to global at (µ =2.085, µ =-1.257)*
1 2
* corresponds to switching y with y .
1 2
©2005-2007 Carlos Guestrin
Finding the max likelihood µ ,µ ..µ
1 2 k
We can compute P( data | µ ,µ ..µ )
1 2 k
How do we find the µ ‘s which give max. likelihood?
i
The normal max likelihood trick:
Set ∂ log Prob (….) = 0
∂ µ
i
and solve for µ ‘s.
i
# Here you get non-linear non-analytically- solvable
equations
Use gradient descent
Slow but doable
Use a much faster, cuter, and recently very popular method…
©2005-2007 Carlos Guestrin
Expectation
Maximalization
©2005-2007 Carlos Guestrin
The E.M. Algorithm
R
U
O
T
E
D
We’ll get back to unsupervised learning soon.
But now we’ll look at an even simpler case with hidden
information.
The EM algorithm
Can do trivial things, such as the contents of the next few slides.
An excellent way of doing our unsupervised learning problem, as
we’ll see.
Many, many other uses, including inference of Hidden Markov
Models (future lecture).
©2005-2007 Carlos Guestrin
Silly Example
Let events be “grades in a class”
w = Gets an A P(A) = ½
1
w = Gets a B P(B) = µ
2
w = Gets a C P(C) = 2µ
3
w = Gets a D P(D) = ½-3µ
4
(Note 0 ≤ µ ≤1/6)
Assume we want to estimate µ from data. In a given class there were
a A’s
b B’s
c C’s
d D’s
What’s the maximum likelihood estimate of µ given a,b,c,d ?
©2005-2007 Carlos Guestrin
Trivial Statistics
P(A) = ½ P(B) = µ P(C) = 2µ P(D) = ½-3µ
P( a,b,c,d | µ) = K(½)a(µ)b(2µ)c(½-3µ)d
log P( a,b,c,d | µ) = log K + alog ½ + blog µ + clog 2µ + dlog (½-3µ)
"LogP
FOR MAX LIKE µ, SET = 0
"µ
"LogP b 2c 3d
= + # = 0
"µ µ 2µ 1/2 # 3µ
b + c
Gives max like µ =
6(b + c + d)
So if class got
A B C D
14 6 9 10
1
Max like µ= u e !
t r
10 u t
b
g ,
n
o r i
B
©2005-2007 Carlos Guestrin
!
Same Problem with Hidden Information
REMEMBER
P(A) = ½
Someone tells us that
P(B) = µ
Number of High grades (A’s + B’s) = h
P(C) = 2µ
Number of C’s = c
P(D) = ½-3µ
Number of D’s = d
What is the max. like estimate of µ now?
©2005-2007 Carlos Guestrin
Same Problem with Hidden Information
REMEMBER
Someone tells us that
P(A) = ½
Number of High grades (A’s + B’s) = h
P(B) = µ
Number of C’s = c
P(C) = 2µ
Number of D’s = d P(D) = ½-3µ
What is the max. like estimate of µ now?
We can answer this question circularly:
EXPECTATION
If we know the value of µ we could compute the
expected value of a and b
1
µ
2
a = h b = h
Since the ratio a:b should be the same as the ratio ½ : µ
1 1
+µ +µ
2 2
MAXIMIZATION
If we know the expected values of a and b
b + c
we could compute the maximum likelihood µ =
! 6(b + c + d)
value of µ
©2005-2007 Carlos Guestrin
!
E.M. for our Trivial Problem
REMEMBER
P(A) = ½
P(B) = µ
P(C) = 2µ
We begin with a guess for µ
P(D) = ½-3µ
We iterate between EXPECTATION and MAXIMALIZATION to improve our estimates
of µ and a and b.
Define µ(t) the estimate of µ on the t’th iteration
b(t) the estimate of b on t’th iteration
µ(0) = initial guess
µ(t)h
b(t) = = "[ b |µ(t)] E-step
1 (t)
+µ
2
b(t) + c
(t+1)
µ =
6( b(t) + c + d) M-step
= max like est. of µ given b(t)
Continue iterating until converged.
Good news: Converging to local optimum is assured.
Bad news: I said “local” optimum.
! ©2005-2007 Carlos Guestrin
E.M. Convergence
Convergence proof based on fact that Prob(data | µ) must increase or remain
same between each iteration
[NOT OBVIOUS]
But it can never exceed 1
[OBVIOUS]
So it must therefore converge
[OBVIOUS]
In our example,
t µ(t) b(t)
suppose we had
0 0 0
h = 20
1 0.0833 2.857
c = 10
d = 10 2 0.0937 3.158
µ(0) = 0
3 0.0947 3.185
4 0.0948 3.187
Convergence is generally linear: error
decreases by a constant factor each time
5 0.0948 3.187
step.
6 0.0948 3.187
©2005-2007 Carlos Guestrin
Back to Unsupervised Learning of
GMMs – a simple case
Remember:
We have unlabeled data x x … x
1 2 m
We know there are k classes
We know P(y ) P(y ) P(y ) … P(y )
1 2 3 k
We don’t know µ µ .. µ
1 2 k
We can write P( data | µ …. µ )
1 k
= p(x ...x µ...µ )
1 m 1 k
m
= "p(x µ...µ )
j 1 k
j=1
m k
= "#p(x µ)P(y = i)
j i
j=1 i=1
m k ’ 1 *
2
$"# exp) % x %µ ,P (y = i)
( 2&2 j i +
j=1 i=1
!
©2005-2007 Carlos Guestrin
EM for simple case of GMMs: The
E-step
If we know µ ,…,µ → easily compute prob.
1 k
point x belongs to class y=i
j
% 1 (
2
( )
p y = i x ,µ...µ "exp’ # x #µ *P (y = i)
j 1 k & 2$2 j i )
!
©2005-2007 Carlos Guestrin
EM for simple case of GMMs: The
M-step
If we know prob. point x belongs to class y=i
j
→ MLE for µ is weighted average
i
imagine k copies of each x , each with weight P(y=i|x ):
j j
m
" ( )
P y = i x x
j j
j=1
µ =
i m
" ( )
P y = i x
j
j=1
!
©2005-2007 Carlos Guestrin
E.M. for GMMs
E-step
Just evaluate
Compute “expected” classes of all datapoints for each class
a Gaussian at
x
% 1 (
2 j
( )
p y = i x ,µ...µ "exp’ # x #µ *P (y = i)
j 1 k & 2$2 j i )
M-step
!
Compute Max. like µ given our data’s class membership distributions
m
" ( )
P y = i x x
j j
j=1
µ =
i m
" ( )
P y = i x
j
j=1
©2005-2007 Carlos Guestrin
!
E.M. Convergence
• EM is coordinate
ascent on an
interesting potential
function
• Coord. ascent for
bounded pot. func. !
convergence to a local
optimum guaranteed
• See Neal & Hinton
reading on class
webpage
This algorithm is REALLY USED. And in high dimensional state spaces, too.
E.G. Vector Quantization for Speech Data
©2005-2007 Carlos Guestrin
E.M. for General GMMs
p(t) is shorthand for
i
estimate of P(y=i)
Iterate. On the t’th iteration let our estimates be on t’th iteration
λ = { µ (t), µ (t) … µ (t), Σ(t), Σ(t) … Σ(t), p (t), p (t) … p (t) }
t 1 2 k 1 2 k 1 2 k
E-step
Compute “expected” classes of all datapoints for each class
( )
( )
(t) (t) (t) Just evaluate
P y = i x ,# " p p x µ ,!
j t i j i i a Gaussian at
x
j
M-step
Compute Max. like µ given our data’s class membership distributions
!P( ) !P( ) [ "µ(t+1)][ "µ(t+1)]T
y = i x ," x y = i x ,$ x x
j t j j t j i j i
ì (t+1) = j # (t+1) = j
i !P( y = i x ," ) i !P( y = i x ,$ )
j t j t
j j
( )
!P y = i x ,"
j t
p (t+1) = j
m = #records
i
m
©2005-2007 Carlos Guestrin
Gaussian Mixture Example: Start
©2005-2007 Carlos Guestrin
After first iteration
©2005-2007 Carlos Guestrin
After 2nd iteration
©2005-2007 Carlos Guestrin
After 3rd iteration
©2005-2007 Carlos Guestrin
After 4th iteration
©2005-2007 Carlos Guestrin
After 5th iteration
©2005-2007 Carlos Guestrin
After 6th iteration
©2005-2007 Carlos Guestrin
After 20th iteration
©2005-2007 Carlos Guestrin
Some Bio Assay data
©2005-2007 Carlos Guestrin
GMM clustering of the assay data
©2005-2007 Carlos Guestrin
Resulting
Density
Estimator
©2005-2007 Carlos Guestrin
Three
classes of
assay
(each learned with
it’s own mixture
model)
©2005-2007 Carlos Guestrin
Resulting
Bayes
Classifier
©2005-2007 Carlos Guestrin
Resulting Bayes
Classifier, using
posterior
probabilities to
alert about
ambiguity and
anomalousness
Yellow means
anomalous
Cyan means
ambiguous
©2005-2007 Carlos Guestrin
What you should know
K-means for clustering:
algorithm
converges because it’s coordinate ascent
EM for mixture of Gaussians:
How to “learn” maximum likelihood parameters (locally max. like.) in
the case of unlabeled data
Be happy with this kind of probabilistic analysis
Understand the two examples of E.M. given in these notes
Remember, E.M. can get stuck in local minima, and
empirically it DOES
©2005-2007 Carlos Guestrin
Acknowledgements
K-means & Gaussian mixture models
presentation contains material from excellent
tutorial by Andrew Moore:
http://www.autonlab.org/tutorials/
K-means Applet:
http://www.elet.polimi.it/upload/matteucc/Clustering/tu
torial_html/AppletKM.html
Gaussian mixture models Applet:
http://www.neurosci.aist.go.jp/%7Eakaho/MixtureEM.
html
©2005-2007 Carlos Guestrin