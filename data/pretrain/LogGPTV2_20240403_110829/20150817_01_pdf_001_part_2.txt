### Procedure for Estimating Probabilities with Given Guesses

We have developed a procedure such that, given guesses for the parameters \(\mu_1, \mu_2, \ldots, \mu_k\), we can determine the probability of the unlabeled data under these parameter values. Assume that the \(x\) values are one-dimensional.

**Example from Duda and Hart:**

- There are two classes, \(w_1\) and \(w_2\).
- The prior probabilities are \(P(y_1) = \frac{1}{3}\) and \(P(y_2) = \frac{2}{3}\).
- The standard deviation \(\sigma\) is 1.
- There are 25 unlabeled data points:
  - \(x_1 = 0.608\)
  - \(x_2 = -1.590\)
  - \(x_3 = 0.235\)
  - \(x_4 = 3.949\)
  - ...
  - \(x_{25} = -0.712\)

**Graphing the Probability Distribution:**

- We can graph the probability distribution function of the data given our estimates of \(\mu_1\) and \(\mu_2\).
- We can also graph the true function from which the data was generated.
- The estimated and true functions are close, indicating good performance.
- In this example, unsupervised learning is almost as effective as supervised learning. If the class labels for \(x_1, x_2, \ldots, x_{25}\) are known, the results are \(\mu_1 = -2.176\) and \(\mu_2 = 1.684\). Unsupervised learning yields \(\mu_1 = -2.13\) and \(\mu_2 = 1.668\).

**Log Likelihood Function:**

- The log likelihood function \(\log P(x_1, x_2, \ldots, x_{25} | \mu_1, \mu_2)\) can be plotted against \(\mu_1\) (horizontal axis) and \(\mu_2\) (vertical axis).
- The maximum likelihood estimates are \(\mu_1 = -2.13\) and \(\mu_2 = 1.668\).
- There is a local minimum at \(\mu_1 = 2.085\) and \(\mu_2 = -1.257\), which corresponds to switching the roles of \(y_1\) and \(y_2\).

**Finding Maximum Likelihood Estimates:**

- To find the \(\mu_i\) values that maximize the likelihood, we use the normal maximum likelihood approach:
  - Set \(\frac{\partial \log P(\text{data} | \mu_1, \mu_2, \ldots, \mu_k)}{\partial \mu_i} = 0\) and solve for \(\mu_i\).
- This results in non-linear, non-analytically solvable equations.
- Gradient descent can be used, but it is slow.
- A faster and more popular method is the Expectation-Maximization (EM) algorithm.

### Expectation-Maximization (EM) Algorithm

**Overview:**

- The EM algorithm is an iterative method for finding maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables.
- It alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step.

**Silly Example:**

- Let the events be "grades in a class":
  - \(w_1 = \text{Gets an A}\), \(P(A) = \frac{1}{2}\)
  - \(w_2 = \text{Gets a B}\), \(P(B) = \mu\)
  - \(w_3 = \text{Gets a C}\), \(P(C) = 2\mu\)
  - \(w_4 = \text{Gets a D}\), \(P(D) = \frac{1}{2} - 3\mu\)
- Assume we want to estimate \(\mu\) from the data. In a given class, there were:
  - \(a\) A's
  - \(b\) B's
  - \(c\) C's
  - \(d\) D's
- The maximum likelihood estimate of \(\mu\) given \(a, b, c, d\) is:
  \[
  \mu = \frac{b + c}{6(b + c + d)}
  \]

**Same Problem with Hidden Information:**

- Suppose we know:
  - Number of high grades (A's + B's) = \(h\)
  - Number of C's = \(c\)
  - Number of D's = \(d\)
- The EM algorithm can be used to estimate \(\mu\) by iteratively:
  - **E-step:** Compute the expected values of \(a\) and \(b\) given \(\mu\).
  - **M-step:** Update \(\mu\) based on the expected values of \(a\) and \(b\).

**EM for Gaussian Mixture Models (GMMs):**

- **E-step:** Evaluate the probability that each data point belongs to each class.
- **M-step:** Compute the maximum likelihood estimates of the parameters given the class membership distributions.

**Convergence:**

- The EM algorithm is guaranteed to converge to a local optimum because the log-likelihood increases or remains the same at each iteration.
- Convergence is generally linear, with the error decreasing by a constant factor each time.

**Examples and Applications:**

- **Gaussian Mixture Models (GMMs):** The EM algorithm is widely used for clustering and density estimation in high-dimensional spaces.
- **Bio Assay Data:** GMMs can be used to cluster and classify bio assay data, providing a probabilistic framework for understanding the data.

**Key Takeaways:**

- Understand the K-means and Gaussian mixture model algorithms.
- Be familiar with the EM algorithm and its application in GMMs.
- Recognize that EM can get stuck in local minima and may require multiple runs to find a better solution.

**Acknowledgements:**

- The presentation includes material from Andrew Moore's tutorial on K-means and Gaussian mixture models.
- Interactive applets for K-means and GMMs are available online for further exploration.

Â©2005-2007 Carlos Guestrin