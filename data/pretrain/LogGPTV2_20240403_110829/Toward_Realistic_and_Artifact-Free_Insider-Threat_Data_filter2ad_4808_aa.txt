title:Toward Realistic and Artifact-Free Insider-Threat Data
author:Kevin S. Killourhy and
Roy A. Maxion
23rd Annual Computer Security Applications Conference
23rd Annual Computer Security Applications Conference
Toward Realistic and Artifact-Free Insider-Threat Data
Kevin S. Killourhy
PI:EMAIL
Roy A. Maxion
PI:EMAIL
Dependable Systems Laboratory
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA
Abstract
Progress in insider-threat detection is currently limited
by a lack of realistic, publicly available, real-world data.
For reasons of privacy and conﬁdentiality, no one wants to
expose their sensitive data to the research community. Data
can be sanitized to mitigate privacy and conﬁdentiality con-
cerns, but the mere act of sanitizing the data may introduce
artifacts that compromise its utility for research purposes.
If sanitization artifacts change the results of insider-threat
experiments, then those results could lead to conclusions
which are not true in the real world.
The goal of this work is to investigate the consequences
of sanitization artifacts on insider-threat detection experi-
ments. We assemble a suite of tools and present a method-
ology for collecting and sanitizing data. We use these tools
and methods in an experimental evaluation of an insider-
threat detection system. We compare the results of the eval-
uation using raw data to the results using each of three types
of sanitized data, and we measure the effect of each saniti-
zation strategy.
We establish that two of the three sanitization strategies
actually alter the results of the experiment. Since these two
sanitization strategies are commonly used in practice, we
must be concerned about the consequences of sanitization
artifacts on insider-threat research. On the other hand, we
demonstrate that the third sanitization strategy addresses
these concerns, indicating that realistic, artifact-free data
sets can be created with appropriate tools and methods.
1. Introduction
An insider is a person with legitimate access to an orga-
nization, and who acts maliciously against that organization.
The insider threat is a signiﬁcant and growing concern, es-
pecially in ﬁelds where espionage and fraud are proﬁtable.
A survey of insider incidents in the banking and ﬁnance sec-
tor found that 30% resulted in losses in excess of $500,000
each [10]. Examples of insider behavior include the unau-
thorized modiﬁcation of company data for personal proﬁt,
the compromise of other employees’ computer accounts,
and the installation of “back doors” through which the in-
sider regains access in the event he or she is terminated [4].
For almost two decades, researchers have been propos-
ing systems to detect and prevent insider threat. These sys-
tems work by monitoring users, proﬁling their behavior,
and identifying suspicious or anomalous activity. The earli-
est systems analyzed audit records and built proﬁles of the
commands each user executes [1, 12]. The conjecture was
that a legitimate user might be distinguished from an im-
postor by their distinct usage of commands (e.g., the user
used vi, the impostor runs emacs); also, an insider stray-
ing from authorized activity might be detected when he used
anomalous commands (e.g., to copy a ﬁle he does not nor-
mally access). As a result of that early work, many sys-
tems have been proposed for detecting insiders using Unix
command-line data [5, 8, 11].
These insider-threat detection systems are best evaluated
using natural, real-world data to measure, compare, and im-
prove their performance. Researchers have instrumented
computer systems to monitor the behavior of participating
users [2, 5, 11]. They collect, sanitize, and share their data
with the research community. Sanitization is the act of re-
placing sensitive data (such as passwords) with uninforma-
tive markers (such as the string ). Sharing the
data allows other researchers to evaluate and compare the
performance of different insider-threat detectors.
However, since a goal of these evaluations is to estimate
the “real-world” performance of a system, we must ask how
effectively the data serve this goal. When an evaluation uses
these data, how conﬁdent are we that the outcome of an
evaluation carries over to the real world? If these evalua-
tions are being used to determine which insider-threat de-
tector is deployed, then we must be conﬁdent that the eval-
1063-9527/07 $25.00 © 2007 IEEE
1063-9527/07 $25.00 © 2007 IEEE
DOI 10.1109/ACSAC.2007.31
DOI 10.1109/ACSAC.2007.31
87
87
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:38:02 UTC from IEEE Xplore.  Restrictions apply. 
uation is accurate. Deploying an insider-threat system that
under-performs is risky, increasing the already high cost of
insider-threat.
Unfortunately, because of the way existing data sets were
created, we must be wary of generalizing to the real world
on the basis of evaluations that use these data sets. First,
the data are not realistic, by which we mean that the data
do not reﬂect what one would ﬁnd in a real-world environ-
ment. Speciﬁcally, in the real world, an insider’s choice of
commands would stem from his or her underlying malicious
intentions. Existing data sets contain no actual insider be-
havior; it is experimentally induced, typically by designat-
ing some normal users as impostors, and using their com-
mands as a substitute for insiders’ commands. Those com-
mands might have been observed when the user was check-
ing his mail, writing a program, or any other benign activity.
We cannot assume that a system which is good at detecting
when a user is checking his mail will perform equally well
detecting insider activity.
Second, when the data are sanitized, artifacts are intro-
duced into the data set. Artifacts are modiﬁcations of the
data (due to the sanitization process) that alter the outcome
of experiments using the data. For instance, if sanitization
replaces all user names with , then commands
which were once distinct become indistinguishable. Sup-
pose an evaluation tested whether a system could detect
when user dave began snooping around user mary’s home
directory. The benign command “cd dave” and the suspi-
cious command “cd mary” are easily distinguished in the
raw, unsanitized data, but both appear as “cd ”
in the sanitized data. A detector which would detect dave’s
snooping in the real world might miss it in the sanitized data
because of artifacts. On the basis of this failed evaluation, a
detector which would actually work well in practice might
never be deployed.
2. Problem and approach
Experiments using existing insider-threat data may not
generalize to the real world because: (1) benign commands
typed by normal users are injected as a substitute for com-
mands typed by malicious insiders; and (2) unintended san-
itization artifacts may be introduced when sensitive data are
replaced with uninformative markers. As a result, benign
user activity could be ﬂagged as insider activity when it is
not, or actual insider activity could go unnoticed.
To address these issues, we have developed a suite of
tools and a methodology for using them. To maximize
the realism of injected insider behavior, we developed a
library of carefully scripted and vetted insider activities.
While the realism of insider injections is a concern, the
primary focus of this work is on the effect of sanitization
artifacts. To ensure that sanitization does not introduce arti-
facts, we developed a sanitizing engine which allows users
to review their data, mark sensitive data, and export san-
itized data sets. This Sanitizer incorporates three differ-
ent sanitization strategies: Redact-Only, Token-Only, and
Word-Token. They differ in how they cover up sensitive
data (e.g., Redact-Only uses a “black box” like ,
Token-Only uses a distinct token like , and
Word-Token breaks the sensitive data into words and then
uses one token for each word). Redact-Only and Token-
Only are similar to strategies used to sanitize existing data
sets. The Word-Token strategy was designed to be artifact-
free.
The present work is framed much like earlier work in
insider detection. In particular, we compare two types of
insider monitoring, called Enriched (comprising the entire
command line typed by a user) and Truncated (comprising
only the name of the command executed). The purpose of
conducting this experiment is to compare the results ob-
tained using raw, unsanitized data to the results using each
of the three sanitization strategies. We intend to establish
whether artifacts arise in the two types of monitored data
(Truncated and Enriched) as a consequence of sanitization,
and what the effects of those artifacts are on the ability to
detect insider activity.
3. Related work
Three existing insider-threat data sets are commonly
used by researchers. Unfortunately, each contains unre-
alistic insider injections and sanitization artifacts. Green-
berg [2] collected a corpus of Unix command-line data,
and Maxion [7] assembled it into an insider-threat data set.
However, benign commands from normal users were used
as a substitute for insider commands. Further, to protect
participating users’ privacy, usernames were sanitized by
replacing each letter of the username with an “x” char-
acter (e.g., dave and mary each contain four characters,
and both would appear as xxxx). Lane and Brodley [5]
also collected users’ commands, but again, benign com-
mands were used as a substitute for insider commands. The
commands were also sanitized by replacing every sequence
of one or more ﬁle names with a number indicating how
many names were in the sequence. Schonlau et al. [11] col-
lected the names of programs executed (rather than the full
command line), but again, benign commands were used in
place of insider commands. Further, one could argue that
collecting only the names of programs constitutes a form
of sanitization (especially since the authors state that full
command lines were not collected because of “privacy con-
cerns”). While these data sets have helped researchers de-
velop and reﬁne insider-detection methods, we remain con-
cerned about the effect of unrealistic insider injections and
sanitization artifacts on those researchers’ experiments.
8888
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:38:02 UTC from IEEE Xplore.  Restrictions apply. 
The problem of data-set artifacts has been demonstrated
by Mahoney and Chan [6] in the domain of intrusion detec-
tion. They found that an existing data set used to evaluate
intrusion-detection systems contained evidence of the arti-
ﬁcial procedure used to synthesize the data. They demon-
strated that a detector could be built which would perform
well in the evaluation (by detecting these artifacts), and
yet it would have little chance of working well in practice.
Their ﬁndings highlight the need for data to be realistic and
artifact-free.
The tools we develop in this study are similar to others in
the literature. The Honeynet Project offers a data-collection
tool called Sebek [3] which is similar to the data collector
we develop. Data anonymization algorithms such as those
proposed by Sweeney [13], and tools like that developed by
Pang and Paxson [9] for Internet trafﬁc are also useful for
data sanitization. One might argue that anonymization and
sanitization are the same, but the literature is not clear on
whether anonymization includes the removal of conﬁden-
tial or sensitive data (as stated by Pang and Paxson) or just
the removal of identifying data (as stated by Sweeney). In
any event, our tools were not designed to supplant these ex-
isting tools but to provide similar capabilities. We created
our own suite of tools because they had to be interoperable,
not because existing tools provided lesser functionality. As
a consequence, our ﬁndings should be relevant to users of
these similar tools as well.
4. Overview of methodology
In order to examine the consequences of sanitization ar-
tifacts on insider-threat experiments, we replicated a typical
experiment from the literature. In our replication, the orig-
inal experiment was conducted ﬁrst using raw, unsanitized
data and then repeated using data treated with each of the
three sanitization strategies. We compared the results from
these experiments to reveal whether they were altered by
sanitization artifacts.
The experiment chosen for this exercise was conducted
by Maxion [7] who was studying the effect of two different
types of data (called Truncated and Enriched) on the effec-
tiveness of a naive-Bayes insider-threat detector. He com-
pared the performance of a detector given only the (Trun-
cated) program names typed by a Unix user to the perfor-
mance of the same detector given the full (Enriched) com-
mand line. Performance was measured in terms of the cost
of error of the detector (calculated as the sum of the miss
and false-alarm rates). Maxion found that the cost of using
Enriched data was 9% lower than the cost of using Trun-
cated data. However, his experiment used the Greenberg
data set [2], which contains sanitization artifacts as dis-
cussed above. As a consequence, we cannot be sure that the
9% difference in cost predicted by the experiment would
also be seen in a real-world deployment.
In order to see the effects of sanitization on this exper-
iment, we built a data-collection program called Monolog,
and deployed it on the workstations of system administra-
tors and operations staff members within the university. The
data collector recorded each user’s commands during his or
her natural daily activities.