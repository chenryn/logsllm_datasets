# Toward Realistic and Artifact-Free Insider-Threat Data

**Authors:**
- Kevin S. Killourhy
- Roy A. Maxion

**Conference:**
23rd Annual Computer Security Applications Conference

**Affiliation:**
- Dependable Systems Laboratory, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA

**Abstract:**
Progress in insider-threat detection is currently hindered by the lack of realistic, publicly available, real-world data. Privacy and confidentiality concerns prevent organizations from sharing sensitive data with the research community. While data sanitization can mitigate these concerns, it often introduces artifacts that compromise the data's utility for research. If sanitization artifacts alter the results of insider-threat experiments, the conclusions drawn may not hold true in real-world scenarios.

This work investigates the impact of sanitization artifacts on insider-threat detection experiments. We have developed a suite of tools and a methodology for collecting and sanitizing data. We use these tools to evaluate an insider-threat detection system, comparing the results obtained from raw data to those from three types of sanitized data. Our findings show that two of the three sanitization strategies significantly alter the experimental results. However, we demonstrate that the third strategy effectively addresses these issues, suggesting that realistic, artifact-free datasets can be created with the right tools and methods.

## 1. Introduction

An insider is a person with legitimate access to an organization who acts maliciously against it. The insider threat is a significant and growing concern, particularly in sectors where espionage and fraud are profitable. A survey of insider incidents in the banking and finance sector found that 30% resulted in losses exceeding $500,000 each [10]. Examples of insider behavior include unauthorized modification of company data for personal gain, compromising other employees' computer accounts, and installing backdoors for future access [4].

For nearly two decades, researchers have proposed systems to detect and prevent insider threats. These systems monitor users, profile their behavior, and identify suspicious or anomalous activities. Early systems analyzed audit records and built profiles based on the commands each user executed [1, 12]. The assumption was that a legitimate user could be distinguished from an impostor by their distinct command usage, and an insider deviating from authorized activities might be detected through anomalous commands. Many systems have since been proposed for detecting insiders using Unix command-line data [5, 8, 11].

To evaluate these insider-threat detection systems, natural, real-world data are essential. Researchers have instrumented computer systems to monitor participating users, collected and sanitized the data, and shared it with the research community. Sanitization replaces sensitive data (e.g., passwords) with uninformative markers. Sharing this data allows other researchers to compare the performance of different insider-threat detectors.

However, the effectiveness of these evaluations in estimating real-world performance is questionable. When using sanitized data, how confident can we be that the evaluation outcomes will generalize to real-world scenarios? Deploying an underperforming insider-threat system is risky and can increase the already high cost of insider threats.

Unfortunately, existing datasets are not realistic. They do not reflect actual insider behavior; instead, benign commands from normal users are used as substitutes for insider commands. Additionally, sanitization introduces artifacts that can alter the outcome of experiments. For example, if all usernames are replaced with a single marker, commands that were once distinct become indistinguishable. This can lead to false negatives, where a detector that would perform well in the real world fails in the sanitized data due to artifacts.

## 2. Problem and Approach

Experiments using existing insider-threat data may not generalize to real-world scenarios because:
1. Benign commands from normal users are used as substitutes for commands from malicious insiders.
2. Unintended sanitization artifacts are introduced when sensitive data are replaced with uninformative markers.

To address these issues, we have developed a suite of tools and a methodology. To maximize the realism of injected insider behavior, we created a library of carefully scripted and vetted insider activities. Our primary focus is on the effect of sanitization artifacts. We developed a sanitizing engine that allows users to review their data, mark sensitive data, and export sanitized datasets. This engine incorporates three sanitization strategies: Redact-Only, Token-Only, and Word-Token. These strategies differ in how they cover up sensitive data. Redact-Only and Token-Only are similar to existing sanitization methods, while Word-Token is designed to be artifact-free.

Our experiment compares two types of insider monitoring: Enriched (comprising the entire command line typed by a user) and Truncated (comprising only the name of the command executed). We aim to determine whether artifacts arise in these two types of monitored data due to sanitization and what the effects of these artifacts are on the ability to detect insider activity.

## 3. Related Work

Three commonly used insider-threat datasets contain unrealistic insider injections and sanitization artifacts. Greenberg [2] collected Unix command-line data, which Maxion [7] assembled into an insider-threat dataset. Benign commands from normal users were used as substitutes for insider commands, and usernames were sanitized by replacing each letter with an "x" character. Lane and Brodley [5] also used benign commands as substitutes and sanitized the data by replacing sequences of file names with numbers. Schonlau et al. [11] collected program names rather than full command lines, but again, benign commands were used in place of insider commands. While these datasets have been useful, their unrealistic nature and sanitization artifacts raise concerns about the validity of the experiments conducted using them.

Mahoney and Chan [6] demonstrated the problem of dataset artifacts in intrusion detection. They found that an existing dataset used for evaluating intrusion-detection systems contained evidence of the artificial procedure used to synthesize the data. A detector that performed well in the evaluation by detecting these artifacts would likely fail in practice. Their findings underscore the need for realistic and artifact-free data.

The tools we develop in this study are similar to others in the literature, such as the Honeynet Project's Sebek [3] and anonymization algorithms by Sweeney [13] and Pang and Paxson [9]. Our tools were designed to be interoperable and provide similar capabilities, ensuring our findings are relevant to users of these similar tools.

## 4. Overview of Methodology

To examine the consequences of sanitization artifacts on insider-threat experiments, we replicated a typical experiment from the literature. We first conducted the experiment using raw, unsanitized data and then repeated it using data treated with each of the three sanitization strategies. We compared the results to reveal whether they were altered by sanitization artifacts.

The experiment chosen was conducted by Maxion [7], who studied the effect of two types of data (Truncated and Enriched) on the effectiveness of a naive-Bayes insider-threat detector. He compared the performance of the detector given only the Truncated program names to its performance given the full Enriched command line. Performance was measured in terms of the cost of error (sum of miss and false-alarm rates). Maxion found that the cost of using Enriched data was 9% lower than using Truncated data. However, his experiment used the Greenberg dataset, which contains sanitization artifacts. Therefore, the 9% difference in cost predicted by the experiment may not hold in a real-world deployment.

To see the effects of sanitization on this experiment, we built a data-collection program called Monolog and deployed it on the workstations of system administrators and operations staff within the university. The data collector recorded each user's commands during their natural daily activities.