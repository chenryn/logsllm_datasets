Centers in the Wild. In Proc. of IMC, 2010.
Improved Construction for Counting Bloom Filters. In Proc. of ESA, 2006.
passive/passive 2015 dataset.xml.
[4] F. Bonomi, M. Mitzenmacher, R. Panigrahy, S. Singh, and G. Varghese. An
[5] Caida Anonymized Internet Traces 2015 DCaida. http://www.caida.org/data/
[6] E. Candes, J. Romberg, and T. Tao. Robust uncertainty principles: exact signal
reconstruction from highly incomplete frequency information. IEEE Transactions
on Information Theory, 52(2):489--509, 2006.
[9] Y.-C. Chen, L. Qiu, Y. Zhang, G. Xue, and Z. Hu. Robust Network Compressive
Streams. Theoretical Computer Science, 312(1):3--15, 2004.
Sensing. In Proc. of MOBICOM, 2014.
nexus-1000v-switch-vmware-vsphere/index.html.
[10] Cisco Nexus 1000V Switch. http://www.cisco.com/c/en/us/products/switches/
[7] E. Candes and T. Tao. Near-Optimal Signal Recovery From Random Projec-
tions: Universal Encoding Strategies? IEEE Transactions on Information Theory,
52(12):5406--5425, 2006.
[8] M. Charikar, K. Chen, and M. Farach-Colton. Finding Frequent Items in Data
[11] G. Cormode and M. Garofalakis. Sketching Streams Through the Net: Distributed
[13] G. Cormode and S. Muthukrishnan. What’s New: Finding Significant Differences
[12] G. Cormode and M. Hadjieleftheriou. Methods for Finding Frequent Items in
Approximate Query Tracking. In Proc. of VLDB, 2005.
Data Streams. The VLDB Journal, 19(1):3----20, 2010.
in Network Data Streams. In Proc. of IEEE INFOCOM, 2004.
[14] G. Crmode and S. Muthukrishnan. An Improved Data Stream Summary: The
Count-Min Sketch and its Applications. Journal of Algorithms, 55(1):58--75, 2005.
[15] X. Dimitropoulos, P. Hurley, and A. Kind. Probabilistic Lossy Counting: An
Efficient Algorithm for Finding Heavy Hitters. ACM SIGCOMM Computer Com-
munication Review, 38(1):5--5, 2008.
[16] M. Dobrescu, N. Egi, K. Argyraki, B.-g. Chun, K. Fall, G. Iannaccone, A. Knies,
M. Manesh, and S. Ratnasamy. RouteBricks : Exploiting Parallelism to Scale
Software Routers. In Proc. of SOSP, 2009.
[17] DPDK. http://dpdk.org/.
[18] C. Eckart and G. Young. The Approximation of One Matrix by Another of Lower
[24] S. Kandula, S. Sengupta, A. Greenberg, P. Patel, and R. Chaiken. The Nature of
Rank. Psychometrika, 1(3):211--218, 1936.
[19] C. Estan and G. Varghese. New Directions in Traffic Measurement and Account-
ing : Focusing on the Elephants , Ignoring the Mice. ACM Trans. on Computer
Systems, 21(3):270--313, 2003.
[20] P. Flajolet and G. Nigel Martin. Probabilistic Counting Algorithms for Data Base
Applications. Journal of Computer and System Sciences, 31(2):182--209, 1985.
[21] N. Handigol, B. Heller, V. Jeyakumar, D. Mazi`eres, and N. McKeown. I Know
What Your Packet Did Last Hop: Using Packet Histories to Troubleshoot Net-
works. In Proc. of NSDI, 2014.
[22] Q. Huang and P. P. C. Lee. A Hybrid Local and Distributed Sketching Design for
Accurate and Scalable Heavy Key Detection in Network Data Streams. Computer
Networks, 91:298--315, 2015.
[23] L. Jose, M. Yu, and J. Rexford. Online Measurement of Large Traffic Aggregates
on Commodity Switches. In USENIX HotICE, 2011.
Data Center Traffic: Measurements & Analysis. In Proc. of IMC, 2009.
[25] T. Koponen, K. Amidon, P. Balland, M. Casado, A. Chanda, B. Fulton, I. Ganichev,
J. Gross, P. Ingram, E. Jackson, A. Lambeth, R. Lenglet, S.-H. Li, A. Padmanabhan,
J. Pettit, B. Pfaff, R. Ramanathan, S. Shenker, A. Shieh, J. Stribling, P. Thakkar,
D. Wendlandt, A. Yip, and R. Zhang. Network Virtualization in Multi-tenant
Datacenters. In Proc. of NSDI, 2014.
[26] A. Kumar, M. Sung, J. J. Xu, and J. Wang. Data Streaming Algorithms for Efficient
and Accurate Estimation of Flow Size Distribution. In Proc. of SIGMETRICS, 2004.
[27] P. P. C. Lee, T. Bu, and G. Chandranmenon. A Lock-Free , Cache-Efficient Multi-
Core Synchronization Mechanism for Line-Rate Network Traffic Monitoring. In
Proc. of IPDPS, 2010.
In Proc. of NSDI, 2016.
of Measurement. In Proc. of SOSR, 2016.
[30] Z. Liu, A. Manousis, G. Vorsanger, V. Sekar, and V. Braverman. One Sketch to
Rule Them All: Rethinking Network Flow Monitoring with UnivMon. In Proc. of
SIGCOMM, 2016.
[31] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar, L. Peterson, J. Rexford,
S. Shenker, and J. Turner. OpenFlow: Enabling Innovation in Campus Networks.
ACM SIGCOMM Computer Communication Review, 38(2):69, 2008.
hh831823.aspx.
J. Misra and D. Gries. Finding Repeated Elements. Science of Computer Program-
ming, 2(2):143--152, 1982.
[28] Y. Li, R. Miao, C. Kim, and M. Yu. FlowRadar: A Better NetFlow for Data Centers.
[32] Microsoft Hyper-V Virtual Switch. https://technet.microsoft.com/en-us/library/
[29] X. Liu, M. Shirazipour, M. Yu, and Y. Zhang. MOZART: Temporal Coordination
[34] M. Mitzenmacher. Compressed Bloom Filters. In Proc. of PODC, 2001.
[35] M. Mitzenmacher, R. Pagh, and N. Pham. Efficient Estimation for High Similari-
[36] M. Moshref, M. Yu, R. Govindan, and A. Vahdat. DREAM: Dynamic Resource
[37] M. Moshref, M. Yu, R. Govindan, and A. Vahdat. SCREAM: Sketch Resource
ties Using Odd Sketches. In Proc. of WWW, 2014.
Allocation for Software-defined Measurement. In Proc. of SIGCOMM, 2014.
Allocation for Software-defined Measurement. In Proc. of CoNEXT, 2015.
Triggers in Data Centers. In Proc. of SIGCOMM, 2016.
In Proc. of NSDI, 2016.
[38] M. Moshref, M. Yu, R. Govindan, and A. Vahdat. Trumpet: Timely and Precise
[39] S. Narayana, M. T. Arashloo, J. Rexford, and D. Walker. Compiling Path Queries.
[33]
[40] NetFlow. https://www.ietf.org/rfc/rfc3954.txt.
[41] OpenvSwitch. http://openvswitch.org.
SIGCOMM ’17, August 21−25, 2017, Los Angeles, CA, USA
Huang et al.
[42] perf. http://perf.wiki.kernel.org.
[43]
J. Rasley, B. Stephens, C. Dixon, E. Rozner, W. Felter, K. Agarwal, J. Carter, and
R. Fonseca. Planck: Millisecond-scale Monitoring and Control for Commodity
Networks. In Proc. of SIGCOMM, 2014.
[44] B. Recht, M. Fazel, and P. A. Parrilo. Guaranteed Minimum-Rank Solutions
of Linear Matrix Equations via Nuclear Norm Minimization. SIAM Review,
52(3):471--501, 2010.
[45] L. Rizzo. Netmap: A Novel Framework for Fast Packet I/O. In Proc. of ATC, 2012.
[46] R. Schweller, Z. Li, Y. Chen, Y. Gao, A. Gupta, Y. Zhang, P. Dinda, M. Y. Kao,
and G. Memik. Reversible Sketches: Enabling Monitoring and Analysis over
High-Speed Data Streams. IEEE/ACM Trans. on Networking, 15(5):1059--1072,
2007.
[47] V. Sekar, M. K. Reiter, W. Willinger, H. Zhang, R. R. Kompella, and D. G. Andersen.
cSAMP: A System for Network-Wide Flow Monitoring. In Proc. of USENIX NSDI,
2008.
[48] V. Sekar, M. K. Reiter, and H. Zhang. Revisiting the Case for a Minimalist
Approach for Network Flow Monitoring. In Proc. of IMC, 2010.
[49] sFlow. http://www.sflow.org/.
[50] Snort. https://www.snort.org.
[51] SPAN.
http://www.cisco.com/c/en/us/tech/lan-switching/switched-port-
[52]
analyzer-span/index.html.
J. Suh, T. T. Kwon, C. Dixon, W. Felter, and J. Carter. OpenSample: A Low-
Latency, Sampling-Based Measurement Platform for Commodity SDN. In Proc.
of ICDCS, 2014.
[53] svdcomp. http://www.public.iastate.edu/∼dicook/JSS/paper/code/svd.c.
[54] K. Thompson, G. J. Miller, and R. Wilder. Wide-Area Internet Traffic Patterns
and Characteristics. IEEE Network, 11:10--23, 1997.
[55] K.-Y. Whang, B. T. Vander-Zanden, and H. M. Taylor. A Linear-time Probabilistic
Counting Algorithm for Database Applications. ACM Trans. Database Systems,
15(2):208--229, 1990.
[56] M. Yu, L. Jose, and R. Miao. Software Defined Traffic Measurement with OpenS-
[57] L. Yuan, C.-N. Chuah, and P. Mohapatra. ProgME: Towards Programmable
ketch. In Proc. of NSDI, 2013.
Network Measurement. In Proc. of SIGCOMM, 2007.
[58] ZeroMQ. http://zeromq.org.
[59] Y. Zhang, L. Breslau, V. Paxson, and S. Shenker. On the Characteristics and
Origins of Internet Flow Rates. In Proc. of ACM SIGCOMM, 2002.
[60] Y. Zhang, M. Roughan, N. Duffield, and A. Greenberg. Fast Accurate Computation
of Large-Scale IP Traffic Matrices from Link Loads. In Proc. of SIGMETRICS, 2003.
[61] Y. Zhang, M. Roughan, W. Willinger, and L. Qiu. Spatio-Temporal Compressive
Sensing and Internet Traffic Matrices. In Proc. of SIGCOMM, 2009.
[62] Y. Zhu, N. Kang, J. Cao, A. Greenberg, G. Lu, R. Mahajan, D. Maltz, L. Yuan,
M. Zhang, B. Y. Zhao, and H. Zheng. Packet-Level Telemetry in Large Datacenter
Networks. In Proc. of SIGCOMM, 2015.
Appendix A: Derivation of θ and e
We derive θ and e in ComputeThresh of Algorithm 1. Without loss
of generality, we denote the input k + 1 values as a1, a2, · · · , ak +1,
where a1 ≥ a2 ≥ · · · ak +1. We fit the inputs to a power-law
distribution; that is, for some random variable Y, we have Pr{Y >
y} = yθ , where  ≤ 1 and θ  (a1 − 1)} = (a1 − 1)θ ≈ 1
and
n
Pr{Y > (a2 − 1)} = (a2 − 1)θ ≈ 2
, respectively, where n is the
n
number of total flows. By dividing the first equation by the second
one, we have (a1−1)θ
2), where
b = a1−1
a2−1.
To compute e, we ensure that it is larger than the smallest value
ak +1, so that we can always kick out the smallest flow among the
k + 1 inputs (i.e., the top-k flows in the hash table H and the new
flow); on the other hand, it cannot be too large to avoid kicking
out large flows. Thus, we compute e such that the probability of
kicking out any flow that is larger than ak +1 is no larger than some
2. Thus, we estimate θ = logb( 1
(a2−1)θ ≈ 1
.
=
=
=
small parameter δ (e.g., we can set δ = 0.05). The probability is
calculated as follows:
Pr{Y > ak +1}
k +1 − eθ
aθ
aθ
k +1
Pr{Y ≤ e | Y > ak +1} =
Pr{Y > ak +1} − Pr{Y > e}
Pr{ak +1  ak +1}
k +1 − eθ
aθ
aθ
k +1
Setting the right side to δ yields e = θ√
1 − δak +1.
Appendix B: Proof of Lemma 4.1
We prove the three properties of Lemma 4.1 (see §4).
(i) If flow f has size vf > E, it must tracked in H.
Our fast path algorithm decrements a flow by at most E after all
flow kick-out operations (lines 14-17 in Algorithm 1). For any flow
f with size vf > E, its residual counter rf is at least vf − E. Since
we only kick out a flow when its residual byte count reaches zero,
the flow must be tracked in H. The result follows.
(ii) If f ∈ H, rf + df ≤ vf ≤ rf + df + ef .
The first inequality holds because rf + d + f is the exact byte
count after f is inserted to H. On the other hand, the missing byte
count before f is inserted is at most ef . Thus, the total byte count
vf (including the byte counts before f is inserted and after f is
inserted) is at most rf + df + vf .
(iii) For any flow, its maximum possible error is bounded by O( V
k ).
The error of a flow f is less than ef ≤ E. Thus, we just need
to prove that E is bounded. Our proof assumes that our param-
eter estimation yields the same parameter θ, although the actual
estimated θ can vary (slightly).
e(k + 1)
in a kick-out operation.
We prove the result by computing the byte count that is decre-
mented in a kick-out operation. Since we select the threshold e
larger than the smallest value ak +1, the actual decremented byte
count must be larger than ak +1. If we use ak +1 as the threshold,
then the total decremented byte count is exactly (k + 1)ak +1 =
(k + 1)
. This implies that the actual decremented byte count
is at least (k + 1)ak +1 = (k + 1)
have
On the other hand, the total decremented byte count in all kick-
operations (i.e., E =
out operations should not exceed the total byte count V . Thus, we
≤ V . Since E is the sum of e is all kick-out
e e), we have E ≤ θ√
k +1. The result
follows.
Remark: We prove that the maximum possible error of any flow in
our top-k algorithm is at most θ√
1 − δ V
k +1. Note that this is slightly
larger than the worst-case error V
k +1 of Misra-Gries’s algorithm.
However, all flows in Misra-Gries’s algorithm share the worst-case
k +1. In contrast, our algorithm estimates the per-flow lower
bound V
and upper bounds with three counters and the maximum error is
ef , which varies across flows. Our experiments show that our top-k
algorithm can achieve much smaller per-flow error bounds than
Misra-Gries’s algorithm in practice (§7.5). We leave the theoretical
analysis of average-case errors to future work.
1 − δ V
θ√
e
1−δ
θ√
e
1−δ
θ√
e
1−δ