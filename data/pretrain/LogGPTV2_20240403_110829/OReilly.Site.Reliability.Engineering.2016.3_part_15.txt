### Supportable Systems and Modularity

A supportable system is one that facilitates both developer agility and system stability. A key aspect of such a system is the loose coupling between binaries or between binaries and configuration. This simplicity pattern ensures that if a bug is discovered in one component, it can be fixed and deployed independently without affecting the rest of the system.

While the modularity provided by APIs may seem straightforward, it is less apparent that this concept extends to how changes are introduced to APIs. Even a single change to an API can necessitate a complete rebuild of the entire system, potentially introducing new bugs. Versioning APIs allows developers to continue using the version their system depends on while upgrading to a newer version in a controlled and safe manner. This approach enables different parts of the system to have varying release cadences, rather than requiring a full production push every time a feature is added or improved.

As a system grows more complex, the separation of responsibilities between APIs and binaries becomes increasingly important. This is analogous to object-oriented class design: just as it is poor practice to write a "grab bag" class with unrelated functions, it is also poor practice to create and deploy a "util" or "misc" binary. A well-designed distributed system consists of components, each with a clear and well-scoped purpose.

The concept of modularity also applies to data formats. One of the central strengths and design goals of Google’s protocol buffers was to create a wire format that was backward and forward compatible.

### Release Simplicity

Simple releases are generally better than complicated ones. It is much easier to measure and understand the impact of a single change rather than a batch of changes released simultaneously. If 100 unrelated changes are released at the same time and performance degrades, identifying which changes caused the issue will require considerable effort. By releasing in smaller batches, we can move faster with more confidence because each code change can be understood in isolation within the larger system. This approach is similar to gradient descent in machine learning, where small, incremental steps are taken to find the optimal solution, evaluating each step for improvement or degradation.

### A Simple Conclusion

This chapter has emphasized that software simplicity is a prerequisite for reliability. Simplifying each step of a given task is not about being lazy; it is about clarifying what we want to accomplish and finding the most efficient way to do so. Every time we say "no" to a feature, we are not restricting innovation; we are keeping the environment uncluttered so that focus remains on innovation, and real engineering can proceed.

### Practices

SREs (Site Reliability Engineers) run services—a set of related systems operated for internal or external users—and are ultimately responsible for the health of these services. Successfully operating a service involves a wide range of activities, including developing monitoring systems, planning capacity, responding to incidents, and ensuring root causes of outages are addressed. This section addresses the theory and practice of an SRE's day-to-day activity: building and operating large distributed computing systems.

We can characterize the health of a service, much like Abraham Maslow categorized human needs, from the most basic requirements needed for a system to function as a service to the higher levels of function, permitting self-actualization and active control of the service's direction. This understanding is fundamental to how we evaluate services at Google and was developed when several Google SREs, including our former colleague Mikey Dickerson, temporarily joined the U.S. government to help with the launch of healthcare.gov in late 2013 and early 2014.

### Service Reliability Hierarchy

1. **Monitoring**: Without monitoring, you have no way to tell whether the service is working. A thoughtfully designed monitoring infrastructure is essential to detect and address issues before users notice them.
2. **Incident Response**: SREs go on-call to stay in touch with how distributed computing systems work and fail. Effective incident response involves reducing the impact of incidents and managing them to limit outage-induced anxiety.
3. **Postmortem and Root-Cause Analysis**: Building a blameless postmortem culture is crucial for understanding what went wrong and what went right. Tracking outages helps SRE teams keep track of recent production incidents, their causes, and actions taken in response.
4. **Testing**: Test suites ensure that our software isn’t making certain classes of errors before it’s released to production.
5. **Capacity Planning**: Ensuring that the system can handle the expected load is vital. Load balancing ensures that we are properly using the capacity we’ve built.
6. **Development**: Large-scale system design and software engineering work within the SRE organization is a key aspect of Google’s approach.
7. **Product**: Reliable product launches at scale aim to give users the best possible experience starting from Day Zero.

### Further Reading from Google SRE

- **Resilience Testing**: An ACM article explains how Google performs company-wide resilience testing to ensure we’re capable of weathering unexpected events.
- **Capacity Planning**: A detailed guide shows that capacity planning doesn’t need to be a dark art and can be done effectively without a crystal ball.
- **Corporate Network Security**: A new approach to corporate network security, driven by SREs, replaces privileged intranets with device and user credentials.

### Practical Alerting from Time-Series Data

**Written by Jamie Wilkinson**
**Edited by Kavita Guliani**

Monitoring is fundamental to running a stable service. It enables service owners to make rational decisions about the impact of changes, apply the scientific method to incident response, and ensure the service aligns with business goals.

Google’s monitoring systems evolved over 10 years from custom scripts to a new paradigm that made the collection of time-series a first-class role. The rise of Borgmon, a new monitoring system, complemented the job scheduling infrastructure Borg. This chapter describes the architecture and programming interface of Borgmon, which was foundational for the growth and reliability of Google for almost 10 years.

**Time-Series Monitoring Outside of Google**

In recent years, tools like Riemann, Heka, Bosun, and Prometheus have emerged as open-source alternatives to Borgmon. These tools share many similarities, especially in their rule languages. The principles of variable collection and rule evaluation remain the same, providing an environment for experimentation and production.

**Instrumentation of Applications**

The /varz HTTP handler lists all exported variables in plain text, with later extensions adding mapped variables for labels and histograms. Adding a metric to a program requires a single declaration in the code. This schemaless textual interface makes adding new instrumentation easy but requires careful change management.

**Exporting Variables**

Google’s web roots run deep, with implementations of the exported variable interface in major languages. The Go expvar library and its JSON output form provide a variant of this API.

**Collection of Exported Data**

Borgmon instances are configured with a list of targets using various name resolution methods. Collection is spread over the interval to avoid lockstep collection from each target. Synthetic variables are recorded to identify resolution, response, and health check status, making it easy to write rules to detect issues with monitored tasks.