a supportable system. Specifically, loose coupling between binaries, or between binar‐
ies and configuration, is a simplicity pattern that simultaneously promotes developer
agility and system stability. If a bug is discovered in one program that is a component
of a larger system, that bug can be fixed and pushed to production independent of the
rest of the system.
While the modularity that APIs offer may seem straightforward, it is not so apparent
that the notion of modularity also extends to how changes to APIs are introduced.
Just a single change to an API can force developers to rebuild their entire system and
run the risk of introducing new bugs. Versioning APIs allows developers to continue
to use the version that their system depends upon while they upgrade to a newer ver‐
sion in a safe and considered way. The release cadence can vary throughout a system,
instead of requiring a full production push of the entire system every time a feature is
added or improved.
As a system grows more complex, the separation of responsibility between APIs and
between binaries becomes increasingly important. This is a direct analogy to object-
oriented class design: just as it is understood that it is poor practice to write a “grab
bag” class that contains unrelated functions, it is also poor practice to create and put
into production a “util” or “misc” binary. A well-designed distributed system consists
of collaborators, each of which has a clear and well-scoped purpose.
The concept of modularity also applies to data formats. One of the central strengths
and design goals of Google’s protocol buffers3 was to create a wire format that was
backward and forward compatible.
Release Simplicity
Simple releases are generally better than complicated releases. It is much easier to
measure and understand the impact of a single change rather than a batch of changes
released simultaneously. If we release 100 unrelated changes to a system at the same
time and performance gets worse, understanding which changes impacted perfor‐
mance, and how they did so, will take considerable effort or additional instrumenta‐
tion. If the release is performed in smaller batches, we can move faster with more
3 Protocol buffers, also referred to as “protobufs,” are a language-neutral, platform-neutral extensible mecha‐
nism for serializing structured data. For more details, see https://developers.google.com/protocol-buffers/docs/
overview#a-bit-of-history.
100 | Chapter 9: Simplicity
confidence because each code change can be understood in isolation in the larger sys‐
tem. This approach to releases can be compared to gradient descent in machine
learning, in which we find an optimum solution by taking small steps at a time, and
considering if each change results in an improvement or degradation.
A Simple Conclusion
This chapter has repeated one theme over and over: software simplicity is a prerequi‐
site to reliability. We are not being lazy when we consider how we might simplify each
step of a given task. Instead, we are clarifying what it is we actually want to accom‐
plish and how we might most easily do so. Every time we say “no” to a feature, we are
not restricting innovation; we are keeping the environment uncluttered of distrac‐
tions so that focus remains squarely on innovation, and real engineering can proceed.
A Simple Conclusion | 101
PART III
Practices
Put simply, SREs run services—a set of related systems, operated for users, who may
be internal or external—and are ultimately responsible for the health of these serv‐
ices. Successfully operating a service entails a wide range of activities: developing
monitoring systems, planning capacity, responding to incidents, ensuring the root
causes of outages are addressed, and so on. This section addresses the theory and
practice of an SRE’s day-to-day activity: building and operating large distributed
computing systems.
We can characterize the health of a service—in much the same way that Abraham
Maslow categorized human needs [Mas43]—from the most basic requirements
needed for a system to function as a service at all to the higher levels of function—
permitting self-actualization and taking active control of the direction of the service
rather than reactively fighting fires. This understanding is so fundamental to how we
evaluate services at Google that it wasn’t explicitly developed until a number of Goo‐
gle SREs, including our former colleague Mikey Dickerson,1 temporarily joined the
radically different culture of the United States government to help with the launch of
healthcare.gov in late 2013 and early 2014: they needed a way to explain how to
increase systems’ reliability.
We’ll use this hierarchy, illustrated in Figure III-1, to look at the elements that go into
making a service reliable, from most basic to most advanced.
1 Mikey left Google in summer 2014 to become the first administrator of the US Digital Service (https://
www.whitehouse.gov/digital/united-states-digital-service), an agency intended (in part) to bring SRE principles
and practices to the US government’s IT systems.
Figure III-1. Service Reliability Hierarchy
Monitoring
Without monitoring, you have no way to tell whether the service is even working;
absent a thoughtfully designed monitoring infrastructure, you’re flying blind. Maybe
everyone who tries to use the website gets an error, maybe not—but you want to be
aware of problems before your users notice them. We discuss tools and philosophy in
Chapter 10, Practical Alerting from Time-Series Data.
Incident Response
SREs don’t go on-call merely for the sake of it: rather, on-call support is a tool we use
to achieve our larger mission and remain in touch with how distributed computing
systems actually work (and fail!). If we could find a way to relieve ourselves of carry‐
ing a pager, we would. In Chapter 11, Being On-Call, we explain how we balance on-
call duties with our other responsibilities.
Once you’re aware that there is a problem, how do you make it go away? That doesn’t
necessarily mean fixing it once and for all—maybe you can stop the bleeding by
reducing the system’s precision or turning off some features temporarily, allowing it
to gracefully degrade, or maybe you can direct traffic to another instance of the ser‐
vice that’s working properly. The details of the solution you choose to implement are
necessarily specific to your service and your organization. Responding effectively to
incidents, however, is something applicable to all teams.
Figuring out what’s wrong is the first step; we offer a structured approach in Chap‐
ter 12, Effective Troubleshooting.
During an incident, it’s often tempting to give in to adrenalin and start responding ad
hoc. We advise against this temptation in Chapter 13, Emergency Response, and coun‐
sel in Chapter 14, Managing Incidents, that managing incidents effectively should
reduce their impact and limit outage-induced anxiety.
Postmortem and Root-Cause Analysis
We aim to be alerted on and manually solve only new and exciting problems presen‐
ted by our service; it’s woefully boring to “fix” the same issue over and over. In fact,
this mindset is one of the key differentiators between the SRE philosophy and some
more traditional operations-focused environments. This theme is explored in two
chapters.
Building a blameless postmortem culture is the first step in understanding what went
wrong (and what went right!), as described in Chapter 15, Postmortem Culture:
Learning from Failure.
Related to that discussion, in Chapter 16, Tracking Outages, we briefly describe an
internal tool, the outage tracker, that allows SRE teams to keep track of recent pro‐
duction incidents, their causes, and actions taken in response to them.
Testing
Once we understand what tends to go wrong, our next step is attempting to prevent
it, because an ounce of prevention is worth a pound of cure. Test suites offer some
assurance that our software isn’t making certain classes of errors before it’s released to
production; we talk about how best to use these in Chapter 17, Testing for Reliability.
Capacity Planning
In Chapter 18, Software Engineering in SRE, we offer a case study of software engi‐
neering in SRE with Auxon, a tool for automating capacity planning.
Naturally following capacity planning, load balancing ensures we’re properly using
the capacity we’ve built. We discuss how requests to our services get sent to datacen‐
ters in Chapter 19, Load Balancing at the Frontend. Then we continue the discussion
in Chapter 20, Load Balancing in the Datacenter and Chapter 21, Handling Overload,
both of which are essential for ensuring service reliability.
Finally, in Chapter 22, Addressing Cascading Failures, we offer advice for addressing
cascading failures, both in system design and should your service be caught in a cas‐
cading failure.
Development
One of the key aspects of Google’s approach to Site Reliability Engineering is that we
do significant large-scale system design and software engineering work within the
organization.
In Chapter 23, Managing Critical State: Distributed Consensus for Reliability, we
explain distributed consensus, which (in the guise of Paxos) is at the core of many of
Google’s distributed systems, including our globally distributed Cron system. In
Chapter 24, Distributed Periodic Scheduling with Cron, we outline a system that scales
to whole datacenters and beyond, which is no easy task.
Chapter 25, Data Processing Pipelines, discusses the various forms that data process‐
ing pipelines can take: from one-shot MapReduce jobs running periodically to sys‐
tems that operate in near real-time. Different architectures can lead to surprising and
counterintuitive challenges.
Making sure that the data you stored is still there when you want to read it is the
heart of data integrity; in Chapter 26, Data Integrity: What You Read Is What You
Wrote, we explain how to keep data safe.
Product
Finally, having made our way up the reliability pyramid, we find ourselves at the
point of having a workable product. In Chapter 27, Reliable Product Launches at Scale,
we write about how Google does reliable product launches at scale to try to give users
the best possible experience starting from Day Zero.
Further Reading from Google SRE
As discussed previously, testing is subtle, and its improper execution can have large
effects on overall stability. In an ACM article [Kri12], we explain how Google per‐
forms company-wide resilience testing to ensure we’re capable of weathering the
unexpected should a zombie apocalypse or other disaster strike.
While it’s often thought of as a dark art, full of mystifying spreadsheets divining the
future, capacity planning is nonetheless vital, and as [Hix15a] shows, you don’t
actually need a crystal ball to do it right.
Finally, an interesting and new approach to corporate network security is detailed in
[War14], an initiative to replace privileged intranets with device and user credentials.
Driven by SREs at the infrastructure level, this is definitely an approach to keep in
mind when you’re creating your next network.
CHAPTER 10
Practical Alerting from Time-Series Data
Written by Jamie Wilkinson
Edited by Kavita Guliani
May the queries flow, and the pager stay silent.
—Traditional SRE blessing
Monitoring, the bottom layer of the Hierarchy of Production Needs, is fundamental to
running a stable service. Monitoring enables service owners to make rational deci‐
sions about the impact of changes to the service, apply the scientific method to inci‐
dent response, and of course ensure their reason for existence: to measure the
service’s alignment with business goals (see Chapter 6).
Regardless of whether or not a service enjoys SRE support, it should be run in a sym‐
biotic relationship with its monitoring. But having been tasked with ultimate respon‐
sibility for Google Production, SREs develop a particularly intimate knowledge of the
monitoring infrastructure that supports their service.
Monitoring a very large system is challenging for a couple of reasons:
• The sheer number of components being analyzed
• The need to maintain a reasonably low maintenance burden on the engineers
responsible for the system
Google’s monitoring systems don’t just measure simple metrics, such as the average
response time of an unladen European web server; we also need to understand the
distribution of those response times across all web servers in that region. This knowl‐
edge enables us to identify the factors contributing to the latency tail.
107
At the scale our systems operate, being alerted for single-machine failures is unac‐
ceptable because such data is too noisy to be actionable. Instead we try to build sys‐
tems that are robust against failures in the systems they depend on. Rather than
requiring management of many individual components, a large system should be
designed to aggregate signals and prune outliers. We need monitoring systems that
allow us to alert for high-level service objectives, but retain the granularity to inspect
individual components as needed.
Google’s monitoring systems evolved over the course of 10 years from the traditional
model of custom scripts that check responses and alert, wholly separated from visual
display of trends, to a new paradigm. This new model made the collection of time-
series a first-class role of the monitoring system, and replaced those check scripts
with a rich language for manipulating time-series into charts and alerts.
The Rise of Borgmon
Shortly after the job scheduling infrastructure Borg [Ver15] was created in 2003, a
new monitoring system—Borgmon—was built to complement it.
Time-Series Monitoring Outside of Google
This chapter describes the architecture and programming interface of an internal
monitoring tool that was foundational for the growth and reliability of Google for
almost 10 years…but how does that help you, our dear reader?
In recent years, monitoring has undergone a Cambrian Explosion: Riemann, Heka,
Bosun, and Prometheus have emerged as open source tools that are very similar to
Borgmon’s time-series–based alerting. In particular, Prometheus1 shares many simi‐
larities with Borgmon, especially when you compare the two rule languages. The
principles of variable collection and rule evaluation remain the same across all these
tools and provide an environment with which you can experiment, and hopefully
launch into production, the ideas inspired by this chapter.
Instead of executing custom scripts to detect system failures, Borgmon relies on a
common data exposition format; this enables mass data collection with low over‐
heads and avoids the costs of subprocess execution and network connection setup.
We call this white-box monitoring (see Chapter 6 for a comparison of white-box and
black-box monitoring).
1 Prometheus is an open source monitoring and time-series database system available at http://prometheus.io.
108 | Chapter 10: Practical Alerting from Time-Series Data
The data is used both for rendering charts and creating alerts, which are accom‐
plished using simple arithmetic. Because collection is no longer in a short-lived pro‐
cess, the history of the collected data can be used for that alert computation as well.
These features help to meet the goal of simplicity described in Chapter 6. They allow
the system overhead to be kept low so that the people running the services can
remain agile and respond to continuous change in the system as it grows.
To facilitate mass collection, the metrics format had to be standardized. An older
method of exporting the internal state (known as varz)2 was formalized to allow the
collection of all metrics from a single target in one HTTP fetch. For example, to view
a page of metrics manually, you could use the following command:
% curl http://webserver:80/varz
http_requests 37
errors_total 12
A Borgmon can collect from other Borgmon,3 so we can build hierarchies that follow
the topology of the service, aggregating and summarizing information and discarding
some strategically at each level. Typically, a team runs a single Borgmon per cluster,
and a pair at the global level. Some very large services shard below the cluster level
into many scraper Borgmon, which in turn feed to the cluster-level Borgmon.
Instrumentation of Applications
The /varz HTTP handler simply lists all the exported variables in plain text, as space-
separated keys and values, one per line. A later extension added a mapped variable,
which allows the exporter to define several labels on a variable name, and then export
a table of values or a histogram. An example map-valued variable looks like the fol‐
lowing, showing 25 HTTP 200 responses and 12 HTTP 500s:
http_responses map:code 200:25 404:0 500:12
Adding a metric to a program only requires a single declaration in the code where the
metric is needed.
In hindsight, it’s apparent that this schemaless textual interface makes the barrier to
adding new instrumentation very low, which is a positive for both the software engi‐
neering and SRE teams. However, this has a trade-off against ongoing maintenance;
the decoupling of the variable definition from its use in Borgmon rules requires care‐
2 Google was born in the USA, so we pronounce this “var-zee.”
3 The plural of Borgmon is Borgmon, like sheep.
Instrumentation of Applications | 109
ful change management. In practice, this trade-off has been satisfactory because tools
to validate and generate rules have been written as well.4
Exporting Variables
Google’s web roots run deep: each of the major languages used at Google has an
implementation of the exported variable interface that automagically registers with
the HTTP server built into every Google binary by default.5 The instances of the vari‐
able to be exported allow the server author to perform obvious operations like adding
an amount to the current value, setting a key to a specific value, and so forth. The Go
expvar library6 and its JSON output form have a variant of this API.
Collection of Exported Data
To find its targets, a Borgmon instance is configured with a list of targets using one of
many name resolution methods.7 The target list is often dynamic, so using service
discovery reduces the cost of maintaining it and allows the monitoring to scale.
At predefined intervals, Borgmon fetches the /varz URI on each target, decodes the
results, and stores the values in memory. Borgmon also spreads the collection from
each instance in the target list over the whole interval, so that collection from each
target is not in lockstep with its peers.
Borgmon also records “synthetic” variables for each target in order to identify:
• If the name was resolved to a host and port
• If the target responded to a collection
• If the target responded to a health check
• What time the collection finished
These synthetic variables make it easy to write rules to detect if the monitored tasks