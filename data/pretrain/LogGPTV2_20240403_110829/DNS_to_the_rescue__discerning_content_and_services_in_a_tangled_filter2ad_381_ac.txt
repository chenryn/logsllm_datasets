tent for mail.google.com and scholar.google.com, and (iii)
4171: SPATIAL DISCOVERY(FQDN)
2: Input: The targeted FQDN
3: Output: ranked list of serverIP addresses
4: 2ndDomain ← F QDN.split()
5: ServerSet ←
6: F QDN set ← 2ndDomain.query()
7: for all F QDN in F QDN Set do
8:
F QDN.ServerSet ←
F lowDB.queryByDomainN ame(F QDN )
F lowDB.queryByDomainN ame(2ndDomain)
9: end for
10: Return(F QDN.ServerSet.sort(), ServerSet.sort())
Algorithm 2: Spatial Discovery Analytics Algorithm
Automatically keep track of any changes in serverIP ad-
dresses that satisfy a given FQDN over time. Note that the
ability of DN-Hunter to easily track temporal and spatial
changes in the FQDN-serverIP address mapping also en-
ables some basic anomaly detection. While out of scope of
this paper, consider the case of DNS cache poisoning where
a response for certain FQDN suddenly changes and is dif-
ferent from what was seen by DN-Hunter in the the past.
We can easily ﬂag this scenario as an anomaly, enabling the
security operator to take some action if required.
4.2 Content Discovery
As we saw in the previous subsection, a particular resource
can be served by one or more CDNs or cloud infrastructures,
and the spatial discovery analytics module provides deep in-
sights into this. However, it is also important to understand
tangle from another perspective. In other words, we need to
answer the following questions: (i) Given a particular CDN
what are the diﬀerent resources that they host/serve? (ii)
What is the popularity of particular CDNs in diﬀerent ge-
ographies? (iii) Given two CDNs, what are the common
resources that they both host?, and (iv) Does a given CDN
focus on hosting content for certain types of services (like
real-time multimedia streaming, mail, etc.)?
Once again DN-Hunter can answer the above questions
easily based on the mapping stored in the ﬂows database
and using the whois database to associate IP addresses to
CDNs. The complete algorithm for the content discovery
module is shown in Algorithm 3. The algorithm takes a
ServerIP Set, i.e., the set of serverIP addresses belonging
to one or more CDNs, and extracts all the FQDNs associated
with them (line 4-7). Depending on the desired granularity
level, either the complete FQDN or only part of the FQDN
(say, the second-level domain) can be considered. If only the
second-level domains are considered, then the algorithm will
return all the organizations served by the set of serverIP
addresses provided as input. However, if only service tokens
are used (we will discuss this in the next sub-section), then
the algorithm will return which popular services are hosted
by the input serverIP addresses.
4.3 Automatic Service Tag Extraction
Identifying all the services/applications running on a par-
ticular layer-4 port number is a legacy problem that net-
work administrators encounter. Even today there are no
existing solutions that can identify all applications on any
given layer-4 port number. In fact, the network administra-
tors depend on DPI solutions to address this problem. DPI
technology can only provide a partial solution to this prob-
1: CONTENT DISCOVERY(ServerIPSet)
2: Input: The list of targeted serverIP
3: Output: The list of handled FQDNs
4: DomainN ameSet ← F lowDB.query(ServerIP Set)
5: for all F QDN in DomainN ameSet do
6:
7: end for
8: for all T oken in T okenSet do
9:
10: end for
11: Return(T okens.sort())
T okenSet ← DomainN ame.split(F QDN )
T oken.score.update()
Algorithm 3: Content Discovery Analytics Algorithm
1: TAG EXTRACTION(dPort, k)
2: Input: targeted dP ort, k of tags to return
3: Output: The ranked list of tags
4: DomainN ameSet ← F lowDB.query(dP ort)
5: for all F QDN in DomainN ameSet do
6:
7: end for
8: for all T oken in T okenSet do
9:
10: end for
11: Return(T okens.sort(k))
T oken.score.update()
T okenSet ← DomainN ame.split(N oT LD|N oSLD)
Algorithm 4: Service Tag Extraction Analytics Algo-
rithm
lem due to two reasons: (1) Several services/applications
use encryption and hence bypass DPIs, and (2) DPI devices
can only identify those services/applications for which they
already have a signature, thus severely limiting the coverage.
DN-Hunter provides a simple and automated way to ad-
dress the above issue. The algorithm for extracting service
tags on any layer-4 port number is shown in Algorithm 4.
The input to the algorithm are the target port number and
the k value for the top-k services to be identiﬁed. The algo-
rithm ﬁrst retrieves all FQDNs associated to ﬂows that are
directed to dP ort (line 4). Each FQDN is then tokenized to
extract all the sub-domains except for the TLD and second-
level domain. The tokens are further split by considering
non-alphanumeric characters as separators. Numbers are
replaced by a generic N character (lines 5-7). For instance,
smtp2.mail.google.com generates the list of tokens {smtpN,
mail}.
We use the frequency of tokens as measure of “relevance”
of the token for the targeted port (lines 8-10). To mitigate
the bias due to some clients generating a lot of connections
to a FQDN having the same token X, we use a logarithmic
score. Mathematically, let NX (c) be the number of ﬂows
originated by clientIP c having the token X. Then the
score of X is:
(cid:88)
score(X) =
log(NX (c) + 1)
(1)
c
Tokens are then ranked by score and the top-k tokens are
returned to the users (line 11). Depending on the ﬁnal goal,
diﬀerent criteria can be applied to limit the list of returned
tokens. For instance, the list can simply be limited to the
top 5%, or to the subset that sums to the n-th percentile.
Typically, the score distribution is very skewed, as we will
show in Sec. 5.
418Figure 3: Number of serverIP addresses associated
to a FQDN (top) and number of FQDN associated
to a ServerIP (bottom). EU2-ADSL.
Figure 4: Number of IP addresses serving some par-
ticular 2nd-level domain name. EU1-ADSL2.
5. EXPERIMENTAL RESULTS
In this section, we present the results from using DN-
Hunter on the traces mentioned in Sec. 2. We begin the
discussion here by showing evidence of how tangled is the
web today in terms of content, content providers, and hosts
serving the content. We then present the results that clearly
highlight the advantages of using DN-Hunter in an opera-
tional network compared to the existing solutions for traﬃc
visibility and policy enforcement. In fact, DN-Hunter is now
implemented as part of two diﬀerent DPI tools and is de-
ployed to provide traﬃc visibility to network operators. In
the second half of this section we will present results from
our advanced analytics modules to demonstrate the wide
applicability and usefulness of DN-Hunter.
5.1 The Tangled Web
The basic hypothesis of this paper is that the web today
is intertwined with content, content providers, and hosts
serving the content that are continually changing over time
and space. Hence we need a methodology that can assist in
restoring clarity to operators regarding their network traﬃc.
The top plot of Fig. 3 reports, for each FQDN, the overall
number of serverIP addresses that serve it. In the bottom
plot of Fig. 3 we show the opposite - the number of diﬀerent
FQDNs a single serverIP address serves. Fig. 3 was gen-
erated using the EU2-ADSL dataset, however, all the other
datasets produced very similar result. We can clearly see
that one single serverIP is associated to a single FQDN
for 73% of serverIP s, and 82% of FQDNs map to just one
serverIP . But more important to note is that there are
FQDNs that are served by hundreds of diﬀerent serverIP
addresses. Similarly a large number of FQDNs are served by
one serverIP . Notice the x-axis in this ﬁgure is presented
in log scale.
Just looking at the one-to-many mapping between FQDN
and serverIP addresses reveals only a small part of the
complexity. Now let us add time into the mix. Fig. 4
shows the number of serverIP addresses that have been
observed responding to some selected well-known second-
level domains. We consider time bins of 10min, covering a
24h period from EU1-ADSL2 dataset. For some of the do-
mains (like fbcdn.net and youtube.com) we can clearly see
a diurnal pattern with more serverIP s being used during
late evening when compared to early morning. In fact, for
youtube.com we can see that there is a big and sudden jump
in the number of serverIP s between 17:00 and 20:30. This
reﬂects a change in the YouTube policies, triggered by the
peak-time load. The domain f bcdn.net (owned by Akamai
and serving Facebook static content) shows similar charac-
teristics with more than 600 diﬀerent serverIP addresses
serving content in every 10min interval between 18:00 and
20:00. Finally, some of the other domains like blogspot.com
(aggregating more than 4,500 total FQDN) are served by
less than 20 serverIP s even during peak traﬃc hours.
Fig. 5 reports the number of diﬀerent FQDNs that were
served every 10min by diﬀerent CDNs and cloud providers
over a period of 24h. The MaxMind organization database
was used to associate serverIP addresses to organization.
We can clearly see that Amazon serves more than 600 dis-
tinct FQDN in every 10 min interval during peak hours
(11:00 to 21:00). In total, Amazon served 7995 FQDNs in a
day. While Akamai and Microsoft also serve signiﬁcant num-
ber of FQDNs during peak hours, other CDNs like EdgeCast
serve less than 20 FQDNs.
Another aspect worth noting here is that association be-
tween FQDNs and CDNs change over time and space (i.e.,
geography). Due to space constraints we do not present
these results here. However, all of the above results clearly
show why it is very hard to discern and control the traﬃc in
today’s networks! In fact, there is clear need for a solution
like DN-Hunter that can track these changes seamlessly to
ensure traﬃc visibility at any point in time. Surprisingly,
the results presented in this section for motivating the need
for a solution like DN-Hunter could not have been produced
if we did not have DN-Hunter!
5.2 Trafﬁc Visibility and Policy Enforcement
The key feature of DN-Hunter is to provide a “label” (i.e.,
the FQDN that the client was contacting) to every ﬂow
in the network automatically. To show how this labeling
evolves over time, we show the results from our live deploy-
ment in EU1-ADSL2 for a period of 18 days in April, 2012.
In Fig. 6 we report the total number of unique FQDNs over
time. The plot shows the growth of unique entities - FQDNs,
second-level domains, and serverIP - over time. Once again
we can clearly see the diurnal pattern where the increase
in unique entities is much higher during the day than the
night. After a steep growth during the ﬁrst few days, the
 0.7 0.75 0.8 0.85 0.9 0.95 1 1 10 100 1000CDF# IP 0.7 0.75 0.8 0.85 0.9 0.95 1 1 10 100 1000CDF# Domain Names 0 100 200 300 400 500 600 70000:0004:0008:0012:0016:0020:00number of serverIPtimetwitter.comyoutube.comfbcdn.netfacebook.comblogspot.com419Certiﬁcate equal FQDN
Generic certiﬁcate
Totally diﬀerent certiﬁcate
No certiﬁcate
18%
19%
40%
23%
Table 4: Comparison between the server name
extracted from TLS certiﬁcate-inspection and the
FQDN using DN-Hunter. EU1-ADSL2.
ure out the server name of the organization that will provide
the content.
In order to compare the certiﬁcate inspection approach
with DN-Hunter, we implemented the certiﬁcate inspection
functionality in Tstat. Tab. 4 compares certiﬁcate inspec-
tion approach with DN-Hunter for all TLS ﬂows in the EU1-
ADSL2 dataset. Results show that DN-Hunter clearly out-
performs the certiﬁcate inspection approach. For 23% of
the ﬂows in the trace there was no certiﬁcate, while for 40%
of the ﬂows the server name in the certiﬁcate was totally
diﬀerent from the FQDN. For the other 37% of the ﬂows
that matched the second-level domain name in the FQDN,
only 18% matched the complete FQDN. The main problems
with the certiﬁcate inspection approach are three-fold: (i)
The server name can be “generic”, e.g., ∗.google.com, thus
not giving the ﬁne-grained visibility into the actual services.
(ii) The server name may indicate the server used by the
hosting CDN and may not reﬂect anything about the ser-
vice, e.g., a248.akamai.net in the certiﬁcate for providing
Zynga content, and (iii) Certiﬁcate exchange might happen
only the ﬁrst time a TLS/SSL server is contacted and all
other ﬂows following that will share the trust. Thus using
such an approach is almost infeasible.
5.3 Spatial Discovery of Servers
The main goal of the spatial discovery module is to track
a particular resource (FQDN or second-level domain) to un-
derstand which serverIP s and CDNs serve the requested
content. For the ease of exposition,
in this section, we
will focus on two speciﬁc second-level domains - LinkedIn
and Zynga. Fig. 7 shows the mapping between the vari-
ous FQDNs of LinkedIn and the CDNs serving the content
in US-3G dataset. The oval nodes represent DNS tokens
extracted from the FQDNs, while arcs connect the tokens
to reconstruct the FQDN. The numbers in these tokens are
represented as a generic letter, N . The rectangular nodes
group tokens by the CDN hosting them based on the in-
formation from the MaxMind database. To illustrate the
concept better let us consider the leftmost branch in Fig. 7.
The complete FQDN is the concatenation of all the tokens,
i.e., mediaN.linkedin.com. These FQDNs are served by Aka-
mai CDN using 2 servers and accounts for 17% of the total
ﬂows destined to linkedin.com. In order to limit the size of
the ﬁgure, we have hidden 7 diﬀerent tokens in the rightmost
branch of the tree.
From the ﬁgure, it is easy to see that LinkedIn relies on
the service oﬀered by several CDN providers. Only the
www.linkedin.com FQDN along with 7 other FQDNs are
served by Linkedin managed servers. Most of the static
content is served by hosts in three diﬀerent CDNs - Aka-
mai, CDNetwork, and Edgecast. In fact, EdgeCast serves
59% of all ﬂows with a single serverIP address. On the
Figure 5: Number of FQDN served by CDNs
through a day. EU1-ADSL2.
Figure 6: Unique FQDN, 2nd level domain names
and IP birth processes. EU1-ADSL2 live.
number of unique serverIP addresses and second-level do-
mains reach a saturation point and do not grow much. This
result basically indicates that the same serverIP addresses
are used to serve the contents for the same organizations
(i.e., second-level domains). However, a surprising result is
regarding the unique FQDNs. As we can see, the number of
unique FQDNs keeps increasing even after 18 days of obser-
vation. In 18 days we saw more than 1.5M unique FQDNs
and it was still growing at the rate of about 100K per day.
This reﬂects the fact that the content being accessed on the
Internet keeps growing, with new services popping up reg-
ularly. The main take away point is that in order to get
ﬁne-grained traﬃc visibility (and thus be applied for pol-
icy enforcement), it is critical to use a tool like DN-Hunter
that can dynamically keep track of the content and their
association with content providers and the hosts serving the
content.
5.2.1 The Case of Encrypted Trafﬁc
As we mentioned earlier, one of the main advantages of
DN-Hunter when compared to traditional DPI solutions is
its ability to label encrypted (TLS/SSL) ﬂows. Traditional
DPI solutions cannot identify encrypted traﬃc by inspect-
ing the packet content and matching it against a signature.
However, the DPI solution can be modiﬁed to inspect the
certiﬁcates exchanged during the TLS/SSL handshake to ﬁg-
 0 100 200 300 400 500 600 70000:0004:0008:0012:0016:0020:00number of active FQDNtimeakamaiamazongooglelevel 3leasewebcotendoedgecastmicrosoft 0 200000 400000 600000 800000 1e+06 1.2e+06 1.4e+06 1.6e+0604/0104/0304/0504/0704/0904/1104/1304/1504/17total numberFQDN2nd-level-domainserverIP420Figure 7: Linkedin.com domain structure served by
two CDNs. US-3G.
contrary, CDNetworks, serves only 3% of ﬂows with 15 dif-
ferent serverIP addresses.