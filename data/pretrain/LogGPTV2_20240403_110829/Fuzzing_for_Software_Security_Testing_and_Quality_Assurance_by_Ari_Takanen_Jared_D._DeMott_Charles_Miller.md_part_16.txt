Using security scanners in software development is common, but this is mostly
because of a misunderstanding of a customer requirement. If a customer requires
that a software developer will run a vulnerability scanner against their product,
it will naturally become part of the software development practice. The pros and
cons of this approach were explained in Chapter 2.
3.9.4 Unit Testing
In unit testing, the SUT consists of a single module used inside the actual appli-
cation. The real application logic can be bypassed by implementing parts of the
functionality in prototypes when the real implementation is still unavailable or by
other replacement implementations when the target is actually a library such as a
file parser or a codec. For example, when testing HTML parsers, you do not neces-
sarily want to run the tests against the full Web browser, but you can use the same
HTML parsing API calls through a test driver. In such a setup, a ten or hundred-
fold increase in the speed of the fuzzing process is easily obtained.
3.9.5 Fault Injection
Traditionally, the term fault injection has meant a hardware testing technique in
which artificial faults are introduced into printed circuit boards. For example, the
connections might be short-circuited, broken, grounded, or stuck to a predefined
value such as 0 or 1. The printed board is then used and the resulting behavior
observed. The purpose is to test the sensitiveness of the hardware for faults (fault
tolerance) emerging during manufacturing or product lifetime. Fault injection can
be used to forecast the behavior of hardware during operations or to guide efforts
on making the hardware more robust against flaws.
Software fault injection has the same operation principle. There are two main
types of fault injection:
• Data fault injection;
• Code fault injection.
In short, faults are injected by mutating code or data to assess the response of
a software component for anomalous situations. In code fault injection (also called
mutation testing) the source code is modified to trigger failures in the target sys-
tem. Source code fault injection is best performed automatically, as an efficient fault
injection process can involve hundreds of modifications, each requiring a rebuild
of the target system.
The following two examples include fault injection at source code level.
6760 Book.indb 93 12/22/17 10:50 AM
94 Quality Assurance and Testing
Example 1:
char *buffer;
/* buffer =(char*)malloc(BUFFER_LENGTH); */
buffer =NULL;
Example 2:
int divider;
/* divider =a *b +d /max +hi; */
divider =0;
Structural fault injection (source code mutations) can be used for simulating
various situations that are difficult to test otherwise except by modifying the oper-
ating environment:
• Memory allocation failures;
• Broken connections;
• Disk-full situations;
• Delays.
Data fault injection, on the other hand, is just another name for fuzzing and
consists of injecting faults into data as it is passed between various components.
3.9.6 Syntax Testing
I do not know which was first, syntax testing or fuzzing, and I do not know if that
is even an important question. Testing gurus such as Boris Beizer created syntax
testing, and security experts such as Dr. B.P. Miller stumbled upon fuzzing. Both
of them were published around the same time and were most probably created to
solve the same problem. Let’s start by quoting the beginning of the chapter on syn-
tax testing by Dr. Boris Beizer from 1990:9
Systems that interface with the public must be especially robust and con-
sequently must have prolific input-validation checks. It’s not that the users
of automatic teller machines, say, are willfully hostile, but that there are so
many of them—so many of them and so few of us. It’s the million monkey
phenomenon: A million monkeys sit at a million typewriters for a million
years and eventually one of them will type Hamlet. The more users, the
less they know, the likelier that eventually, on pure chance, someone will
hit every spot at which the system’s vulnerable to bad inputs.
There are malicious users in every population—infuriating people who
delight in doing strange things to our systems. Years ago they’d pound the
sides of vending machines for free sodas. Their sons and daughters invented
the blue box for getting free telephone calls. Now they’re tired of prob-
ing the nuances of their video games and they’re out to attack computers.
9 Chapter 9, Section 2.2 unmodified and in its entirety, from Boris Beizer. (1990) Software Testing
Techniques, 2nd ed. International Thomson Computer Press. Quoted with permission.
6760 Book.indb 94 12/22/17 10:50 AM
3.9 Black-Box Testing Techniques for Security 95
They’re out to get you. Some of them are programmers. They’re persistent
and systematic. A few hours of attack by one of them is worse than years
of ordinary use and bugs found by chance. And there are so many of them:
so many of them and so few of us.
Then there’s crime. It’s estimated that computer criminals (using mostly
hokey inputs) are raking in hundreds of millions of dollars annually. A crimi-
nal can do it with a laptop computer from a telephone booth in Arkansas.
Every piece of bad data accepted by a system—every crash-causing input
sequence—is a chink in the system’s armor that smart criminals can use to
penetrate, corrupt, and eventually suborn the system for their own purposes.
And don’t think the system’s too complicated for them. They have your list-
ings, and your documentation, and the data dictionary, and whatever else
they need. There aren’t many of them, but they’re smart, motivated, and
possibly organized.
The purpose of syntax testing is to verify that the system does some form of
input validation on the critical interfaces. Every communication interface presents
an opportunity for malicious use, but also for data corruption. Good software
developers will build systems that will accept or tolerate any data whether it is non-
conformant to the interface specification or just garbage. Good testers, on the other
hand, will subject the systems to the most creative garbage possible. Syntax testing
is not random, but instead it will automate the smart fuzzing process by describ-
ing the operation, structure, and semantics of an interface. The inputs, whether
they are internal or external, can be described with context-free languages such as
Backus-Naur Form (BNF). BNF is an example of a data description language that
can be parsed by an automated test generation framework to create both valid and
invalid inputs to the interface.
The key topic in syntax testing is the description of the syntax. Every input has
a syntax, whether it is formally described or undocumented, or just understood. We
will next explain one notation called BNF with examples. One of the most simple
network protocols is TFTP. And an overly simplified description of TFTP using
BNF would be as follows:
 ::= (0x00 0x01)  
 ::= (0x00 0x02)  
 ::= (“octet” | “netascii”) 0x00
 ::= {  } 0x00
 ::= 0x01 -0x7f
From the above example we can see that there are two types of messages defined,
a read request  and a write request . A header of two octets defines
the choice of the message type: (0x00 0x01) versus (0x00 0x02). Both messages
contain two strings: a file name  and a transfer mode .
The file name can consist of variable length text string built from characters. The
transfer mode can be a zero-terminated text string of two predefined values. The
| character defines an OR operand, which means only one of the values is allowed.
Pretty simple, right?
6760 Book.indb 95 12/22/17 10:50 AM
96 Quality Assurance and Testing
The strategy in syntax test design is to add one anomaly (or error) at a time
while keeping all other components of the input structure or message correct.
With a complex interface, this alone typically creates tens of thousands of dirty
tests. When double errors and triple errors are added, the amount of test cases
increases exponentially.
Several different kinds of anomalies (or errors) can be produced in syntax testing:
• Syntax errors: Syntax errors violate the grammar of the underlying language.
Syntax errors can exist on different levels in the grammar hierarchy: top-level,
intermediate-level, and field-level. Simplest field-level syntax errors consist of
arbitrary data and random values. Intermediary and top-level syntax errors
are omitting required elements, repeating, reordering, and nesting any ele-
ments or element substructures.
• Delimiter errors: Delimiters mark the separation of fields in a sentence. In
ASCII-coded languages the fields are normally characters and letters, and
delimiters are white-space characters (space, tab, line-feed, etc.), other delim-
iter characters (commas, semicolons, etc.), or their combinations. Delimiters
can be omitted, repeated, multiplied, or replaced by other unusual characters.
Paired delimiters, such as braces, can be left unbalanced. Wrong unexpected
delimiters can be added at places where they might not be expected.
• Field-value errors: A field-value error is an illegal field in a sentence. Normally,
a field value has a range or many disjoint ranges of allowable values. Field-value
errors can test for boundary-value errors with both numeric and nonnumeric
elements. Values exactly at the boundary range or near the boundary range
should also be checked. Field errors can include values that are one-below,
one-above and totally out-of-range. Tests for fields with integer values should
include boundary values. Use of powers of two plus minus one as boundary
values is encouraged since such a binary system is the typical native presenta-
tion of integers in computers.
• Context-dependent errors: A context-dependent error violates some property
of a sentence that cannot, in practice, be described by context-free grammar.
• State dependency error: Not all sentences are acceptable in every possible state
of a software component. A state dependency error is, for example, a correct
sentence during an incorrect state.
Note that some people have later used the term syntax testing for auditing of
test specifications. The purpose of such a test is to verify that the syntax in test
definitions in correct. We will ignore this definition for syntax testing in this book.
3.9.7 Negative Testing
Negative testing comes in many forms. The most common type of negative test-
ing is defining negative tests as use cases—for example, if a feature implements an
authentication functionality, a positive test would consist of trying the valid user
name and valid password. Everything else is negative testing, including wrong user
name, wrong password, someone else’s password, and so on. Instead of explaining
the various forms of manual tactics for negative testing, we will focus on explaining
the automated means of conducting negative testing. Kaksonen coined the name
6760 Book.indb 96 12/22/17 10:50 AM
3.9 Black-Box Testing Techniques for Security 97
robustness testing for this type of test automation in his licentiate thesis published
in 2001.10
The purpose of robustness testing is only to try negative tests and not to care
about the responses from the SUT at all. Robustness testing is a model-based nega-
tive testing approach that generates test cases or test sequences based on a machine-
understandable description of a use case (the model) or a template. The model
consists of protocol building blocks such as messages and sequences of messages,
with various dynamic operations implemented with intelligent tags. Each message
consists of a set of protocol fields, elements that have a defined syntax (form) and
semantics (meaning) defined in a protocol specification. The following enumerates
the various levels of models and contents in the model:
• A message structure consists of a set of protocol fields.
• A message is a context in which its structure is evaluated.
• A dialog is a sequence of messages.
• A protocol is a set of dialogs.
As said earlier, robustness testing is an automated means of conducting negative
testing using syntax testing techniques. Robustness testing consists of the following
steps, which are very similar to the steps in syntax testing:11
1. Identify the interface language.
2. Define the syntax in formal language, if not readily available in the protocol
specification. Use context-free languages such as regular expressions, BNF,
or TTCN. This is the protocol of the interface.
3. Create a model by augmenting the protocol with semantics such as dynami-
cally changing values. As an example, you can define an element that describes
a length field inside the message. This is the model of the protocol.
4. Validate that the syntax (the protocol and the resulting model) is complete
(enough), consistent (enough), and satisfies the intended semantics. This is
often done with manual review. Risk assessment is useful when syntax test-
ing has security auditing purpose to prioritize over a wide range of elements,
messages, and dialogs of messages.
5. Test the syntax with valid values to verify that you will implement at least
the necessary use cases with the model. At this point you should be able to
run the model using test automation frameworks. The model does not need
to be complete for the testing purposes. If the purpose is to do clean testing,
you can stop here.
6. Syntax testing is best suited for dirty testing. For that, you need to identify
elements in the protocol and in the model that need dirty testing. Good
choices are strings, numbers, substructures, loops, and so on.
10 Rauli Kaksonen. (2001). A Functional Method for Assessing Protocol Implementation Security
(Licentiate thesis). Espoo. Technical Research Centre of Finland, VTT Publications 447. 128 p. + app.
15 p. ISBN 951-38-5873-1 (soft back ed.) ISBN 951-38-5874-X (online ed.).
11 J. Röning, M. Laakso, A. Takanen, & R. Kaksonen. (2002). PROTOS—Systematic Approach to
Eliminate Software Vulnerabilities. Invited presentation at Microsoft Research, Seattle, WA. May
6, 2002.
6760 Book.indb 97 12/22/17 10:50 AM
98 Quality Assurance and Testing
7. Select input libraries for the above elements. Most data fields and structures
can be tested with predefined libraries of inputs and anomalies, and those
are readily available.
8. Automate test generation and generate test cases. The test cases can be fully
resolved, static binaries. But in most cases the test cases will still need to
maintain the dynamic operations unevaluated.
9. Automate test execution. Automation of static test cases is often simple, but
you need to have scripts or active code involved if the test dialog requires
dynamically changing data.
10. Analyze the results. Defining the pass/fail criteria is the most critical deci-
sion to make, and the most challenging.
The greatest difference from fuzzing is that robustness testing almost never
has any randomness involved. The tests are created by systematically applying a
known set of destructive or anomalous data into the model. The resulting tests are
often built into a test tool consisting of a test driver, test data, test documentation,
and necessary interfaces to the test bed, such as monitoring tools and test control-
lers. The robustness tests can also be released as a test suite, consisting of binary
test cases, or their descriptions for use with other test automation frameworks and
languages such as TTCN. Prebuilt robustness tests are always repeatable and can
be automated in a fashion in which human involvement is minimal. This is suitable
for use in regression testing, for example.
3.9.8 regression Testing
Testing does not end with the release of the software. Corrections and updates are
required after the software has been launched, and all new versions and patches
need to be verified so that they do not introduce new flaws, or reintroduce old ones.
Postrelease testing is also known as regression testing. Regression testing needs to
be entirely automated, and fast. The tests also need to be very stable and configu-
rable. A minor update to the communication interface can end up invalidating all
regression tests if the tests are very difficult to modify.
Regression testing is the obvious place for recognizing the pesticide paradox.12
The pesticide paradox is a result of two different laws that apply to software testing:
1. Every testing method you use in software development, or every test case
you implement into your regression testing, will leave a residue of subtler
bugs against which those methods and test are ineffectual. You have to be
prepared to always integrate new techniques and tests into your processes.
2. Software complexity (and therefore the complexity of bugs) grows to the
limits of our ability to manage that complexity. By eliminating easy bugs,
you will allow the complexity of the software to increase to a level where the
more subtle bugs become more numerous, and therefore more significant.
12 Boris Beizer. (1990). Software Testing Techniques, 2nd ed., International Thomson Computer Press.
6760 Book.indb 98 12/22/17 10:50 AM
3.10 Testing in Continuous Integration 99
The more you test the software, the more immune it becomes to your test
cases. The remedy is to continually write new and different tests to exercise differ-
ent parts of the software. Whenever a new flaw is found, it is important to analyze
that individual bug and see if there is a more systematic approach to catching that
and similar mistakes. A common misunderstanding in integrating fuzzing related
regression flaws is to incorporate one single test into the regression test database,
when a more robust solution would be to integrate a suite of test cases to prevent
variants of that flaw.
Therefore, regression tests should avoid any fixed, nondeterministic values
(magic values). A bad security-related example would be regression testing for a
buffer overflow with one fixed length. A flaw that was initially triggered with a
string of 200 characters might later reemerge as a variant that is triggered with 201
characters. Modification of the tests should also not result in missed bugs in the
most recent release of the software. Regression tests should be constantly updated
to catch newly found issues.
Flaws in the regression database give a good overview of past mistakes, and it is
very valuable information for developers and other testers. The regression database
should be constantly reviewed and analyzed from the learning perspective. A bug
database can reveal valuable information about critical flaws and their potential
security consequences. This, in itself, is a metric of the quality of various products.
3.10 Testing in Continuous Integration
Continuous integration (CI) is a development practice where developers integrate
their code changes into a shared repository daily, or even more frequently. The idea
behind CI is that if an issue is found during integration it takes less time, and less
money, to locate and fix it when there is only few changes since the previous integra-
tion. Although not originally included in the practice, modern CI systems include
automation that run build and verification for every integration. CI is especially
useful when used with automated unit tests, which are executed on the integrated
platform to avoid interoperability issues and problems that rise from several pro-
grammers working on a single module at the same time. Addition to unit tests, CI