各个功能，每个配置更新都可能需要数小时才能完成。在测试实例中可以工作的配置文
这样快速发布也并不一定是好主意。复杂的发布过程可能需要在不同子系统上单独启动
在大型分布式系统中，很少有能够瞬间完成的事件。就算能够做到，为了保障可靠性，
发布计划
示范问题：
以及缓存等机制来应对这些问题。
在Google产品发布的历史上，我们曾经使用过过滤/重新代理服务、数据编码流水线
问题，或者未预料到的扩展性问题时，尽早计划可以使我们有办法避免影响到直接用户
一个公司提供的服务或者数据。当第三方提供商出现故障、Bug、系统性的错误、安全
我们为它们的不确定性做好准备。例如，服务依赖于第三方维护的一个类库，或者另外
有时候某个发布过程依赖于某个不受公司控制的因素。尽早确认这些因素的存在可以使
外部依赖
待办事项：
提交一个修改请求就可以自动发布到线上。
我们也会自动将版本配置系统中存放的配置文件自动推送到生产环境中，工程师只需要
·这次发布依赖哪些第三方代码，数据、服务，或者事件？
·为每个发布版本创建一个新的发布分支。
·将所有的代码和配置文件都存放到版本控制系统中。
当我们或者第三方提供商无法在指定截止日期前完成工作时，会发生什么？
是否有任何合作伙伴依赖于你的服务？发布时是否需要通知他们？
起草一个发布检查列表
331
<380
---
## Page 374
8
332
以便更早地检测问题。
非常有用。使用灰度发布，我们可以在灰度发布新版本的时候关注其对服务器的影响，
百分比通常逐渐随时间增长到100%。这种类型的发布在新版本会带来新的后端流量时
灰度式发布的理念甚至可以应用在那些运行在非Google机器上的软件。新版本的
服务没有崩溃或者返回异常。如果在校验期间出现问题，系统会自动回退。
配置文件的工具。负责安装新软件的工具通常都会对新启动的程序监控一段时间，保证
我们可以观察任何异常现象的发生。
次监控，最后再安装到全球全部的服务器上。发布的第一阶段通常被称为“金丝雀”一
中还穿插一些校验步骤。新的服务可能会在某个数据中心的几台机器上安装，并且被严
根据某个事先定义的流程，几乎所有的Google服务的更新都是灰度进行的，在整个过程
附录B）。
研发了一系列发布模式，用它们逐渐地发布产品和服务可以降低风险（更多信息可参见
Google的发布很少是“立即可用”的一
系统上测试成功的改变，在Google这种全球分布式、高度复制的系统中很难直接使用。
有一定的危险性，而任何危险性都应该被最小化，这样才能保障系统的可靠性。在小型
灰度和阶段性发布
势，但是在发布阶段就实践它们也是很重要的。
中的某些方法论非常适用于安全发布产品。这些方法论可为你在日常运维中提供很多优
正如本书其他部分描述的那样，Google在多年稳定运行系统中研发了一系列方法论，其
可靠发布所需要的方法论
待办事项：
AndroidAPP可以用灰度方式发布，新版软件可以先发布给一部分用户。新版本覆盖的
“金丝雀”测试是嵌入到很多Google内部自动化工具中的一个核心理念，包括那些修改
密监控一段时间。如果没有发现异常，服务则会在某个数据中心的所有机器上安装，再
系统管理员常说的一句谚语是“永远不要在正在运行的系统上做改动”。任何改动都具
●针对发布计划中的每一步分析危险性，并制定对应的备用方案。
·为该服务发布制定一个发布计划，将其中每一项任务对应到具体的人。
第27章可靠地进行产品的大规模发布
一在某个具体时间后整个世界都可使用。Google
---
## Page 375
问的实例，例如文档ID、某个表格、存储物件等。有状态的服务一般不会重写HTTP请
Google的功能开关框架基本可归类为以下两类：
细调优，以便未来大部分的功能上线时不需要LCE再参与。这种框架通常需要满足以下
以将新功能逐渐发布给0%~100%的用户。每当产品增加这个框架时，该框架都会被仔
最后，有时候我们想要知道是否一小部分用户会喜欢使用某个新功能，就通过发布一个
能提升用户感受。这样的小改动不需要几千行的程序或者非常重量级的发布流程。我们
还有，不是所有的改动都可以一样对待。有时我们仅仅是想检查某个界面上的改动是否
布中存在不可预知的时候这些机制就非常有用了。
定的可靠性保障是很划算的。在逼真的测试环境由于某些原因无法构建时，或者复杂发
新的机制允许我们在真实负载情况下观察系统的整体行为，用工程力量和时间来换取一
Google经常会使用发布前测试之外的一些补充策略来避免故障的发生。创建一个可控更
有状态的服务一般会将功能开关限制为某个已登录用户的标识符，或者某个产品内被访
（例如Cookie哈希取模之后的某个范围）。
某个配置机制可以将新代码和一个标识符关联起来，同时实现某种黑名单和白名单机制
端服务器之前，只对某些Cookie起作用，或者是其他的某种HTTP请求/回复的属性。
对用户界面修改来说，最简单的功能开关框架是一个无状态的HTTP重写器，运行在前
几个要求：
为了满足上述这些场景，
可能希望同时测试几百个这样的改动。
功能开关框架
·度量每个改变对用户体验的提升。
·可以同时发布多个改动，每个改动仅针对一部分服务器、用户、实体，或者数据
）可以支持任意服务器端和逻辑修改的。
）主要面向用户界面修改的。
。
）将流量根据用户、对话、对象和位置等信息发送到不同的服务器上。
：灰度式发布到一定数量的用户，一般在1%~10%之间。
在严重Bug发生，或者其他副作用场景下可以迅速单独屏蔽某个改变。
设计中可以自动应对新代码出现的问题，不会影响到用户。
中心起作用。
一些Google产品加入了功能开关框架。某些框架被设计为可
可靠发布所需要的方法论
333
382
---
## Page 376
334
再发布到所有人的设备上。
修复代码，再发布新版APP。如果没有这个设置，就需要去掉这个功能，制作一个新版本，
通过加入这种功能使得在问题出现时，中止发布更容易。我们可以简单地将该功能关闭，
不同的发布时间的独立功能来说就更重要了，否则需要维护各种组合版本。
单，原因是不需要针对功能启用与维护多个并行的发布轨道。这对发布一系列有着各自
之前提前发布到客户端，我们可以显著降低该发布的危险性。发布新版本也能变得更简
客户端配置文件甚至可以启用全新的用户可见的功能。通过将新功能对应的代码在激活
可能会启用或者禁用某些功能或者调整参数，例如多人同步一次和重试的频率等。
这种控制可能意味着客户端周期性地与服务器联系，并且下载一个配置文件。这个文件
服务器端控制客户端行为的能力也是一个重要工具。对一个安装在设备上的APP来说
每个延迟都需要一定的抖动（也就是加入一定的随机性）。
短暂的请求峰值可能会造成错误比例升高，这个周期会一直循环。为了避免这种同步性
端发送了一个请求，当遇到故障时，1s之后重试，接下来是2s、4s等。没有随机性的话
会造成夜里2点时有大量请求发往下载服务器，每天晚上都是如此，而其他时间没有任
是下载更新的好时候，因为用户这时可能在睡觉，不会被下载影响。然而，这样的设计
那样），这是另外一个常见的滥用行为的例子。某个手机APP开发者可能认为夜里2点
故意的或者不故意的自动请求的同步性会造成惊群效应（正如第24章和第25章描述的
般不应该重试。
网络错误通常值得重试，但是4xxHTTP错误（这一般意味着客户端侧请求有问题）一
的频率，一般需要增加指数型增长的重试延迟，同时仔细考虑哪些错误值得重试。例如，
的服务造成更大负载，于是会造成更多的重试，更多的负载。客户端这时应该降低重试
该服务由于过载，某些请求会处理失败。如果客户端重试这些失败请求，会对已经过载
用户触发的行为，或者客户端自动触发的行为。以一个目前处于过载状态的服务为例。
会比600s同步一次的旧客户端造成10倍的负载。重试逻辑也有一些常见问题会影响到
最简单的客户端滥用行为是某个更新间隔的设置问题。一个每60s同步一次的新客户端
应对客户端滥用行为
务逻辑。
求，而是通过代理或者根据需求转发到其他服务器上的手段来测试复杂功能或者新的业
其他的一些周期性过程中也需要引入随机性。回到之前说的那个重试场景下：某个客户
何请求。这种情况下，每个客户端应该引入一定随机性。
第27章可靠地进行产品的大规模发布
---
## Page 377
布工程师”自愿组建了一个顾问小组。这些发布工程师为新产品的发布起草了检查列表，
为了减少这种重复问题的发生，利用以往的好经验，一小部分有经验的工程师一
新手工程师经常会重蹈覆辙，尤其是在发布新功能和新产品的过程中。
在Google的早期时代，研发团队的数量每年会翻倍，该状况持续了几年。这造成工程
LCE的发展
管是从可靠性角度还是容量规划角度，压力测试对大多数发布来说都很重要。
不幸的是，从理论上很难预测某个服务的过载反应。因此，压力测试是非常重要的，不
想要回收内存，直到大部分CPU时间都被内存管理部分占用了。
回收死亡螺旋”。在这个场景中，虚拟机内部的内存管理会运行得越来越频繁，不停地
状态。对运行在JavaJVM上的服务来说，这种类似的死锁状态有的时候被称为“垃圾
会花更多的CPU时间在记录调试信息上，造成更多的请求超时，最终进人一个完全死锁
记录过程会比处理后端成本更高。当该服务进入过载状况，某些后端请求超时时，服务
这里引用一个具体的负载行为：某个服务会在收到后端错误时记录调试信息。但是由于
况下进入完全死锁状态。
可能会造成RPC超时，而造成某个用户可见的错误）。在极端情况下，服务会在过载情
应时间会上升，造成用户体验下降，但是不一定会造成故障（然而依赖服务的速度变慢
到达某一点后，很多服务在过载之前会进入一个非线性转折点。在最简单的案例中，响
上升，经常有一段时间CPU用量和负载线性同步增长，响应时间基本保持固定。
于各种各样的缓存因素，例如CPU缓存、JIT缓存，以及其他的数据缓存等。随着负载
实的服务很少按照理想模式运转。大部分服务在没有负载的情况下都会变慢，一般是由
量或者处理的数据量等），一且可用CPU耗尽，处理过程就会变慢。但是不幸的是，真
在简化的模型中，假设物理机的CPU用量与某个服务的负载线性相关（例如，请求的数
户端行为、外界的攻击等。
服务发布时造成过载的最常见因素，其他原因包括负载均衡问题、物理机故障、同步客
过载情况是一个复杂的故障模式，因此需要额外加以注意。意料之外的成功经常是某个
包括以下一些主题：
部门被分割成很多很小的团队，分别负责各自的实验性新产品和新功能。在这种情况下，
过载行为和压力测试
什么时候需要跟法务部门咨询。
如何选择域名。
LCE的发展
335
“发
384
385
---
## Page 378
336
Google同时开始构建大型的数据中心，以便将多个互相依赖的服务放在一起。这种发展
被认为是低风险的。这些发布通过一个非常简单的检查列表就可以发布，而其他的高风
非常小。例如，某个功能发布不需要新的二进制文件，同时流量增长在10%之内，就会
随着发布数量的增加，跟上Google每年增长一倍的工程师团队，LCE不停地寻找简化
只有5个工程师，这就意味着Google在三年半的时间内发布了近1500次。
随着Google的内部环境变得更复杂，LCE检查列表（参见附录E）和发布数量也在增多。
产评审”（productionreview）。
度所走的一些捷径，以及所带来的风险。该团队的顾问环节后来被标准化，被称为“生
不会影响到其他产品。LCE同时负责保障内部相关团队都能清楚地知道为了提高上线速
LCE负责确保发布过程执行迅速，并且服务不会出现故障。在某个发布过程出现问题时，
低延迟的可靠产品。
加速新产品和新功能的发布过程，同时利用SRE理念来保障Google持续发布高可用、
为了改善这种情况，SRE在2004年组建了一个小型但是全职的LCE团队。该团队负责
正处于双方僵持不下的争执中，产品发布速度受到了影响。
正在快速增长，某些没有经验的SRE经常过于小心，甚至反对任何改动。Google当时
境复杂度的上升，做到安全发布某个修改对普通工程师来说越来越难。同时，SRE团队
在两年之内，检查列表中的部署要求部分变得很长很复杂。再加上Google内部部署环