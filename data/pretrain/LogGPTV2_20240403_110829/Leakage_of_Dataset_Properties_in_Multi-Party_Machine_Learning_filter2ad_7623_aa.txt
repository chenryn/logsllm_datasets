title:Leakage of Dataset Properties in Multi-Party Machine Learning
author:Wanrong Zhang and
Shruti Tople and
Olga Ohrimenko
Leakage of Dataset Properties in 
Multi-Party Machine Learning
Wanrong Zhang, Georgia Institute of Technology; Shruti Tople, 
Microsoft Research; Olga Ohrimenko, The University of Melbourne
https://www.usenix.org/conference/usenixsecurity21/presentation/zhang-wanrong
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.Leakage of Dataset Properties in Multi-Party Machine Learning
Wanrong Zhang§
Georgia Institute of Technology
Shruti Tople
Microsoft Research
Olga Ohrimenko§
The University of Melbourne
Abstract
Secure multi-party machine learning allows several par-
ties to build a model on their pooled data to increase
utility while not explicitly sharing data with each other.
We show that such multi-party computation can cause
leakage of global dataset properties between the parties
even when parties obtain only black-box access to the
ﬁnal model. In particular, a “curious” party can infer
the distribution of sensitive attributes in other parties’
data with high accuracy. This raises concerns regarding
the conﬁdentiality of properties pertaining to the whole
dataset as opposed to individual data records. We show
that our attack can leak population-level properties in
datasets of different types, including tabular, text, and
graph data. To understand and measure the source of
leakage, we consider several models of correlation be-
tween a sensitive attribute and the rest of the data. Using
multiple machine learning models, we show that leakage
occurs even if the sensitive attribute is not included in
the training data and has a low correlation with other
attributes or the target variable.
1
Introduction
Modern machine learning models have been shown
to memorize information about their training data, lead-
ing to privacy concerns regarding their use and release
in practice. Leakage of sensitive information about the
data has been shown via membership attacks [47, 50],
attribute inference attacks [17, 53], extraction of text [8]
and data used in model updates [46, 55]. These attacks
focus on leakage of information about an individual
record in the training data, with several recent excep-
tions [18, 39] pointing out that leakage of global proper-
§Work done in part while at Microsoft.
ties about a dataset can also lead to conﬁdentiality and
privacy breaches.
In this paper, we study the problem of leakage of
dataset properties at the population-level. Attacks on
leakage of global properties about the data are concerned
with learning information about the data owner as op-
posed to individuals whose privacy may be violated via
membership or attribute inference attacks. The global
properties of a dataset are conﬁdential when they are
related to the proprietary information or IP that the data
contains, and its owner is not willing to share. As an ex-
ample, consider the advantage one can gain from learning
demographic information of customers or sales distribu-
tion across competitor’s products.
Our primary focus is on inferring dataset properties
in the centralized multi-party machine learning setting.
This setting allows multiple parties to increase utility
of their data since the model they obtain is trained on a
larger data sample than available to them individually.
Beneﬁts of computing on combined data have been iden-
tiﬁed in multiple sectors including drug discovery, health
services, manufacturing and ﬁnance. For example, anti-
money laundering served as a use case for secure data
sharing and computation during the TechSprint organized
by the Financial Conduct Authority, UK in 2019 [5]. A
potential machine learning task in this setting is to create
a system that identiﬁes a suspicious activity based on
ﬁnancial transactions and demographic information of
an entity (e.g., a bank customer). Since multiple ﬁnancial
institutions have separate views of the activities, such a
system can be used to detect common patterns.
Deployments and availability of secure computation
methods [2, 29, 32, 48] can enable multi-party machine-
learning by alleviating immediate privacy concerns of the
parties. In particular, secure multi-party machine learn-
ing provides parties with a black-box access to a model
USENIX Association
30th USENIX Security Symposium    2687
trained on their pooled data without requiring the parties
to share plaintext data with each other. Unfortunately,
as we show in this paper, this is insufﬁcient to address
all privacy implications of collaborative machine learn-
ing. In particular, we demonstrate that global properties
about one party’s sensitive attributes can be inferred by
the second party, even when only black-box access to the
model is available. Consider implications of our attacks
in the use case above. An attacker party (e.g., one of
the banks) can learn distribution of demographic features
pertaining to the customer population in the other bank
(e.g., whether the other bank has more female than other
customers or what percentage of customers has income
over a certain threshold) that it can use in the future
when developing a marketing campaign to attract new
customers.
Analysis of our attacks shows that
leakage of
population-level properties is possible even in cases
where sensitive attribute is irrelevant to the task, i.e., it
has ≈ 0 correlation with the task in hand. Though remov-
ing sensitive attributes may seem like a viable solution, it
is not provably secure due to correlations that are present
in the data. Indeed, we show that in many cases, infor-
mation is still leaked regardless of whether training data
contained the sensitive attribute or not. We argue that this
is possible due to correlation between sensitive attributes
and other attributes that exists in the data. For example,
datasets we use indicate that there is correlation between
sets of attributes including gender, occupation and work-
ing hours per week, as well as income, occupation and
age. Such customer attributes are often recorded by ﬁnan-
cial institutions, as a result indicating potential leakage
if institutions were to collaborate towards detection of
ﬁnancial crime as described above.
Threat model. We consider the setting where the
model is securely trained on the joined data of the honest
party and of an honest-but-curious party. Honest-but-
curious adversary considers a realistic setting where the
malicious party (1) will not alter its own data — if it
does, the model may not perform well and, if detected,
could undermine the trust from the other party in the part-
nership — and (2) will not change the machine learning
code — both parties may wish to observe the code to be
run on the data to ensure its quality and security.
The attacker is interested in learning global properties
about a sensitive attribute at the dataset level, that is, how
values of this attribute are distributed in the other party’s
dataset. It may be interested in learning which attribute
value is dominant (e.g., whether there are more females)
or what the precise ratio of attribute values is (e.g., 90%
females vs. 70% females).
Attack technique. We show that dataset property can
be leaked merely from the black-box access to the model.
In particular, the attacker does not require access to the
training process of the model (e.g., via gradients [39])
or to model parameters (aka white-box attack [7, 18]).
Following other attacks in the space, the attacker also
uses shadow models and a meta classiﬁer. However, in-
dividual predictions from the model are not sufﬁcient to
extract global information about a dataset. To this end, we
introduce an attack vector based on a set of queries and
use them in combination in order to infer a dataset prop-
erty. In contrast to previous work on property leakage,
the attack requires less information and assumptions on
the attacker (see Table 1 and Section 8 for more details).
Methodology. To understand what causes information
leakage about a property we consider several correla-
tion relationships between the sensitive attribute A, the
rest of the attributes X, and the target variable Y that the
machine learning model aims to learn. Surprisingly, we
show that dataset-level properties about A can be leaked
in the setting where A has low or no correlation with Y .
We demonstrate this with experiments on real data and
experiments with a synthetic attribute where we control
its inﬂuence on X and Y . The attack persists across differ-
ent model types such as logistic regression, multi-layer
perceptrons (MLPs), Long Short Term Memory networks
(LSTMs), and Graphical Convolution Networks (GCNs)
models and for different dataset types such as tabular,
text, and graph data. The attack is efﬁcient as it requires
100 shadow models and fewer than 1000 queries.
Machine learning settings.
In addition to the multi-
party setting, our property leakage attack can be carried
out in the following two settings. (1) single-party setting
where an owner of a dataset releases query interface of
their model; (2) in the model update setting, one can infer
how the distribution of a sensitive property has changed
since the previous release of the model. The second attack
also applies to multi-party machine learning, showing
that the party that joins last exposes its data distribution
more than parties who were already collaborating.
Contributions. Our contributions are as follows:
• Problem Formulation: We study leakage of proper-
ties about a dataset used to train a machine learning
model when only black-box access to the model is
available to the attacker.
2688    30th USENIX Security Symposium
USENIX Association
Attacker’s knowledge
Melis et al. [39]
training gradients
model parameters (white-box) (cid:88)
Ganju et al. [18]
Ateniese et al. [7] model parameters (white-box) (cid:88)
model predictions (black-box) (cid:88)
This work
(cid:88)
(cid:88)
Single-party Multi-party Datasets
tabular, text, images
tabular, images
tabular, speech
tabular, text, graphs
Table 1: Comparison of attacks on leakage of dataset properties.
• Attack Technique: We propose an effective attack
strategy that requires only a few hundred inference
queries to the model (black-box access) and relies
on a simple attack architecture that even a computa-
tionally bound attacker can use.
• Attack Setting: We show that leakage of dataset prop-
erties is an issue for an owner of a dataset when the
owner releases a model trained on their data (single-
party setting); when the owner participates in multi-
party machine learning, and when the owner con-
tributes data to update an already trained model
(e.g., either because it joins other parties or because
it has acquired new data).
• Empirical Results: We show that distribution of a
sensitive attribute can be inferred with high accuracy
for several types of datasets (tabular, text, graph) and
models, even if the sensitive attribute is dropped
from the training dataset and has low correlation
with the target variable.
Finally, we note that secure multi-party computa-
tion, based on cryptographic techniques or secure hard-
ware, [13, 19, 20, 26, 27, 34, 40, 41, 42, 54] guaran-
tees that nothing except the output of the computation
is revealed to the individual parties. However, it is not
concerned with what this ﬁnal output can reveal about
the input data of each party. On the other hand, defenses,
such as differential privacy, are concerned with individual
record privacy and not dataset property privacy consid-
ered in this paper. We discuss this further in Section 7.
In summary, we believe this work identiﬁes a potential
gap in multi-party machine learning research in terms
of techniques that parties can deploy to protect global
properties about their dataset.
2 Preliminaries
We assume that there is an underlying data distribu-
tion D determined by variables X, A, Y where X models
a set of features, A models a feature that is deemed pri-
vate (or sensitive) and Y is the target variable, i.e., either
a label or a real value (e.g., if using regression models).
We consider a supervised setting where the goal is to
train a model f such that f (X,A) predicts Y .
Secure multi-party computation (MPC). MPC lets
parties obtain a result of a computation on their com-
bined datasets without requiring them to share plaintext
data with each other or anyone else. Methods that instan-
tiate it include homomorphic encryption, secret sharing,
secure hardware and garbled circuits [12, 14, 25, 43, 45].
These methods vary in terms of their security guarantees
(e.g., availability of a trusted processor vs. non-colluding
servers) and efﬁciency. We abstract MPC using an ideal
functionality [43]: a trusted third entity accepts inputs
from the parties, computes the desired function on the
combined data, and returns the output of the computation
to each party. Security of protocols implementing this
functionality is often captured by proving the existence
of a simulator that can simulate adversary’s view in the
protocol based only on adversary’s input and the output
of the computation. Hence, an MPC protocol guarantees
that an adversarial party learns only the output of the
computation but does not learn the content of the inputs
of other parties beyond what it can infer based on its own
data and the output. Since our attacks are oblivious
to the exact technique used for secure computation, we
assume ideal MPC functionality and specify additional
information available to the adversary in the next section.
Multi-party machine learning. Let Dhonest and Dadv
be the datasets corresponding to the data of the victim
parties and Dadv be the data that belongs to the parties
whose data is known to the adversary. For simplicity,
we model it using two parties Phonest and Padv who own
Dhonest and Dadv, respectively. Both Dhonest and Dadv are
sampled from D but may have a different distribution
of A, conditional on some latent variable, for example, a
party identiﬁer. Importantly, distribution of A in Dhonest
is secret and unknown to Padv. Parties are interested in
increasing the utility of their model through collaboration
with each other. To this end, they agree on an algorithm to
train a machine learning model, f , using their combined
datasets Dhonest and Dadv.
USENIX Association
30th USENIX Security Symposium    2689
The parties use secure multi-party computation to
train f , as they are not willing to share it either due to
privacy concerns or regulations. Once the target model
is trained using MPC, it can be released to the parties
either as a white- or black-box. In the former, f is sent
to the parties, and, in the latter, the model is available
to the parties through an inference interface (e.g., the
model stays encrypted at the server such that inferences
are made either using secure hardware or cryptographic
techniques [28]). We assume that f is trained faithfully
and, hence, Padv cannot tamper with how f is trained
(e.g., this avoids attacks where a malicious algorithm can
encode training data in model weights [51]).
MPC guarantees that parties learn nothing about the
computation besides the output, i.e., they learn no other
information about each other’s data besides what is re-
vealed from their access to f . The goal of this paper is
to show that even by having black-box access to f one
party can infer information about other parties’ data.
3 Data Modeling
To reason about leakage of A’s distribution in D, we
consider different relationships between X,Y,A based on
their correlation. We use ∼ to indicate that there is a
correlation between random variables and ⊥ if not. We
consider four possible relationships between Y , X and
the sensitive attribute A.
Y⊥A: If Y is independent of A, and if f is faithfully mod-
eling the underlying distribution, A should not be leaked.
That is, information about A that an adversary acquires
from f (X,A) and f (cid:48)(X) should be the same for mod-
els f and f (cid:48) trained to predict Y . Two scenarios arise
depending on whether the rest of the features are corre-
lated with A or not: (X⊥A,Y⊥A) and (X ∼ A,Y⊥A). We
argue that leakage in the latter case is possible due to
how machine learning models are trained. Below we de-
scribe why it is theoretically feasible and experimentally
validate this in Section 6.
A machine learning model is trying to learn the condi-
tional probability distribution Pr(Y = y|X = x) where X
are the attributes and Y is the target variable. Suppose