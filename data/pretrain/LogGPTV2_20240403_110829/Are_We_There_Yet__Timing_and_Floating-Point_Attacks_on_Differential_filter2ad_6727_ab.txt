ﬂoating-point values in the ranges [a1, b1] and [a2, b2], ai  0 and δ ∈ [0, 1) if given any two neighboring datasets
D, D(cid:48) ∈ D and any subset of outputs R ⊆ R it holds that
Pr[M(D) ∈ R] ≤ e Pr[M(D(cid:48)) ∈ R] + δ. That is, the
probability of observing the same output y from M(D) and
M(D(cid:48)) is bounded. DP therefore guarantees that given an
output y, an attacker cannot determine which of D or D(cid:48) was
used for the input. If there is some output that is possible with
D but not D(cid:48), this inequality cannot hold for non-trivial δ, so
*Since notifying Facebook about the ﬂoating-point attack, Opacus library
now has proposed a mitigation in https://github.com/pytorch/opacus/pull/260
Our contributions are:
• We show that the Gaussian mechanism of differential
privacy suffers from a side channel due to ﬂoating-point
representation. To this end, we devise attack methods to
show how to exploit this vulnerability since the known
ﬂoating-point attack against the Laplace mechanism can-
not be used directly.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:38:32 UTC from IEEE Xplore.  Restrictions apply. 
474
the mechanism cannot be DP. This is the key fact our attacks
exploit.
In this paper, we consider DP mechanisms M that provide
protection by computing the intended function f on the data D
and randomizing its output. That is, M(D) = f (D)+s where
s ← NoiseDist is noise drawn either from Laplace or Gaussian
distributions with appropriate parameters (as discussed below).
For example, f may compute the sum of the set of employee
incomes D, and we may wish to keep the exact values of the
incomes private. Our attacks are based on observations about
NoiseDist that can be used to learn the noise sampled from it.
C. Laplace Mechanism
The Laplace mechanism provides -DP by additive noise
drawn from the Laplace distribution Lap(λ) (i.e., NoiseDist is
Lap(λ)). In the scalar-valued case M(D) = f (D) + Lap(λ).
Here we use a scale parameter λ = ∆/ where ∆ is f’s
sensitivity, meaning any neighboring data sets D and D(cid:48)
satisfy |f (D) − f (D(cid:48))| ≤ ∆.
D. Gaussian Mechanism
f
f
The Gaussian mechanism [16] provides (, δ)-DP to the
outputs of a target function f : D → R, where R = Rd. The
mechanism is popular due to its favorable noise tails compared
to alternative mechanisms, and its composition properties
when the mechanism is used to answer many queries on data,
as is common when training a machine learning model or
answering repeated queries on a database [17], [18].
Let ∆f be the L2-sensitivity of f, that is, the maximum
distance (cid:107)f (D) − f (D(cid:48))(cid:107)2 between any neighboring datasets
D and D(cid:48). Then the Gaussian mechanism M(D) adds noise
NoiseDist to f (D). We write N (x, σ2∆2
I) to mean the
multivariate Gaussian with mean given by the target x and
covariance given by the identity matrix scaled by σ2∆2
f .
We then have NoiseDist = N (0, σ2∆2
I), and the output
distribution is N (f (D), σ2∆2
I). The resulting mechanism is
δ ∈ (0, 1).
(, δ)-DP if σ = (cid:112)2 log(1.25/δ)/ for arbitrary  > 0 and
In addition to applications computing a one-off release
of a function output, the Gaussian mechanism is commonly
used repeatedly in training machine learning models using
mini-batch stochastic gradient descent (SGD). This composite
mechanism is called DP-SGD [19], [20]. When used to replace
non-private mini-batch SGD, it produces a machine learn-
ing model with differential privacy guarantees on sensitive
training data. This mechanism has been applied in Bayesian
inference [21], to train deep learning models [11], [22], [23],
and also in logistic regression models [20]. At a high level, a
record-level DP-SGD mechanism aims to protect presence of
a record in a batch and, hence, in the dataset. DP-SGD has
been also used in the Federated Learning setting [22] where
each client computes a gradient on their local data batch, adds
noise and sends the result to a central server.
f
III. THREAT MODEL
We consider an adversary that obtains an output of an imple-
mentation of a differentially-private mechanism, for example
a DP-protected average income of people in a database of
personal records, or DP-protected gradients used to train a
machine learning model. DP aims to defend information about
presence (or absence) of a certain record by providing plau-
sible deniability. We show that the attacker can use artefacts
of implementations of noise samplers in DP mechanisms to
undermine their guarantees. In particular we consider two
attacks based on separate artefacts — one on ﬂoating-point
representation and one on timing.
We envision three scenarios where our attacks can be carried
out:
S: a member of the public observes statistics computed on
sensitive data and protected with DP noise (e.g., those
released by the US Census Bureau [24]);
DB: an analyst interactively queries a differentially private
database [25], [26], [7] which allows them to ask several
queries of datasets and which transparently adds noise
to preserve privacy of the dataset from the analyst;
FL: a central server who is coordinating federated learning
by collecting gradients from clients [22]. Here, a client
adds noise to a gradient computed on its data to protect
its data from the central server.
For all three scenarios above, our threat model builds on
the threat model of DP where (1) the attacker observes a
DP-protected output, (2) the attacker may know all the other
records in the dataset except for the one record it is trying to
guess, and (3) knows how the mechanism is implemented † ,
but does not know the randomness used by it.
We describe additional adversarial capabilities required for
each scenario and attack below.
a) Floating-Point Attack (Section IV-C): For one of our
two ﬂoating-point attacks, in addition to observing a single
DP output that the adversary wishes to attack, we assume the
adversary has access to an output of a consecutive execution
of a DP mechanism or its noise sampling. This is achievable
in practice in all scenarios above due to:
1) multiple queries: In scenario S multiple statistics are
released, in scenario DB a (malicious) analyst could
query a DP protected database several times.
2) d-dimensional query: in all three scenarios, an output
being protected can correspond to an output of a d-
dimensional function where independent noise is added
to each component such as a histogram [16] or a gradient
computed for multiple parameters of ML model [11].
Moreover, gradients are assumed to be revealed as part
of the privacy analysis in the central setting as well as
in the FL setting.
b) Timing Attack (Section VI): In contrast to the ﬂoating-
point attack above, here the threat model assumes that the
attacker observes a DP output and can measure the time it
takes for the DP mechanism to compute it. The attacker must
also be able to measure multiple runs of an algorithm in order
to obtain a baseline of running times. However during the
†Many DP implementations are open-source including [2], [3], [4], [5].
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:38:32 UTC from IEEE Xplore.  Restrictions apply. 
475
attack itself, the attacker only needs to make one observation
to make a reasonable guess.
The attack can be deployed in the three scenarios above
if an attacker has black-box access to the machine running
DP code, similarly to the threat model of other timing side-
channels used against DP mechanisms that are not based on
noise samplers [8], [7] (see Section IX for more details).
For example, the attacker may share a machine based in the
cloud [1], [27], [28] or is a cloud provider itself. For the DB
scenario speciﬁcally, a malicious analyst querying the mecha-
nism hosted locally can readily measure the time it takes for
the query to return. For the FL scenario (and remotely hosted
databases in the DB scenario) the uncertainty in measuring
precise time due to network communication can be reduced
with recent attacks exploiting concurrent requests [29].
IV. FLOATING-POINT ATTACK ON NORMAL
DISTRIBUTION IMPLEMENTATIONS
We describe the ﬂoating-point attack that aims to determine
whether a given ﬂoating-point value could have been generated
by an instance of a Gaussian distribution or not. If not, this
eliminates the possibility that a DP mechanism could have
used this noise, hence undermining its privacy guarantees.
We begin with a description of a generic ﬂoating-point attack
against DP and then describe two common implementations
of Gaussian samplers—polar and Ziggurat—and how they can
be attacked.
A. Floating-Point Attack on DP
The DP threat model assumes that the adversary knows
neighboring datasets D, D(cid:48) and function f. Given an out-
put y of a DP mechanism, where either y = f (D) + s or
y = f (D(cid:48)) + s(cid:48) and s, s(cid:48) ← NoiseDist, the attacker’s goal is
to determine if D or D(cid:48) was used in the computation of y.
Mironov [6] showed that due to an artefact in the implemen-
tation of NoiseDist for Laplace, some values of s are impossi-
ble. Hence given y, if the adversary knows that s is impossible
then it must be the case that D(cid:48) was used to compute y (and
similarly for s(cid:48)). This directly breaks the guarantee of DP
which states that there is a non-zero probability for each of
the inputs producing the observed output. We will show that
mechanisms that use Gaussian noise for NoiseDist — whose
implementation is more complicated than Laplace — are also
susceptible to implementation artefacts.
this
In the rest of
section we develop a function
IsFeasibleNormal(s) which returns true if a given noise
value s could have been drawn from implementations of
Gaussian distributions and false otherwise. The attacker then
runs IsFeasibleNormal(s) and IsFeasibleNormal(s(cid:48)). If only
one of them returns true, the attacker determines that the
corresponding dataset was used in the computation. Otherwise,
it makes a random guess.
B. Warmup: Feasible Random Floating Points
We describe how random double-precision ﬂoating-point
values (“doubles”) are sampled on modern computers using a
function RandomFP and show that given a double x, one can
determine if it was generated using RandomFP or not. This
will serve as a warm-up for our attack against the Gaussian
distribution over doubles.
Random real values in the range (0, 1) can be drawn by
choosing a random integer u from [1, R) and then dividing
it by the resolution R = 2p, where the value of p varies by
system. We abstract this process using a function RandomFP
that chooses an integer u at random from [1, R) and returns
u/R. Given a double x one can determine if it could have been
produced by RandomFP by checking if x ?= ¯u/R for some
integer ¯u ∈ [1, R). If the equality holds then x could have
been produced from a random integer. Since rounding errors
are introduced during multiplication and division, we will later
also perform the above check for neighboring values of x.
C. Polar Method: Implementation and Attack
In this section we describe the polar method and the
ﬂoating-point attack against it.
1) Method: The Marsaglia polar method [30] is a com-
putational method that generates samples of the standard
normal distribution from uniformly distributed random values.
PolarMethod operates as follows:
P1 Choose independent uniform random values x(cid:48)
P2 Set x1 ← 2x(cid:48)
P3 Set r ← x2
P4 Repeat from Step P1 until r ≤ 1 and r (cid:54)= 0.
P5 Set s1 ← x1
P6 Return s1.
from (0, 1) using RandomFP.
fall in the interval (−1, 1).)
1 and x(cid:48)
2 − 1. (Note that both
1 − 1 and x2 ← 2x(cid:48)
2.
1 + x2
(cid:113)−2 log r
and s2 ← x2
2
(cid:113)−2 log r
.
r
r
The procedure generates two independent samples from a
normal distribution: s1 and s2. The second value, s2, is cached
and returned on the next invocation. If the cache is empty, the
sampling method is invoked again. The method can generate
samples from N (0, σ2) by returning σs1 and σs2 instead.
The polar method is used by both the GNU C++ Library
with std::normal_distribution and the Java class
java.util.Random (in the nextGaussian method). A
related technique called Box-Muller method described in the
Appendix A that relies on computing sin and cos is used
in PyTorch and was implemented in the older versions of
Diffprivlib [5].
2) Floating-Point Attack: The attacker’s goal is to devise a
function IsFeasibleNormal(s) that determines if value s could
have been generated by PolarMethod — a computational
method for drawing normal noise. Here, we assume that the
attacker knows sample s2 and is trying to guess if s ?= s1. As
discussed in Section III, an attacker can learn s2 either through
multiple queries or multi-dimensional queries. We note that
the attack against the Laplace method by Mironov [6] cannot
be applied since PolarMethod (and the Box-Muller method)
(1) relies on different mathematical formulae and (2) uses two
random values and draw two samples from their target distribu-
tion. To this end, we devise an FP attack speciﬁcally for normal
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:38:32 UTC from IEEE Xplore.  Restrictions apply. 
476
distribution implementations. We describe the attack for the
polar method below. The attack for the Box-Muller method
proceeds similarly, using trigonometric functions instead.
Before proceeding with the attack, we observe that r in
PolarMethod can be expressed using s1 and s2 by simple
arithmetic rearrangements based on steps P3 and P5.
(cid:19)2
(cid:18)
(cid:114) r
+
s2
−2 log r
(cid:19)2
(cid:18)
(cid:114) r
−2 log r
s2
2r
−2 log(r)
r = x2
1 + x2
2 =
s1
=
s2
1r
−2 log(r)
+
Rearranging this equation further, we obtain
−2 log r = s2
1 + s2
2
(cid:18)
r = exp
1 + s2
2