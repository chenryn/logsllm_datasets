title:These aren't the droids you're looking for: retrofitting android to
protect data from imperious applications
author:Peter Hornyack and
Seungyeop Han and
Jaeyeon Jung and
Stuart E. Schechter and
David Wetherall
“These Aren’t the Droids You’re Looking For”: Retroﬁtting
Android to Protect Data from Imperious Applications
Peter Hornyack∗
PI:EMAIL
Seungyeop Han∗
PI:EMAIL
Jaeyeon Jung†
PI:EMAIL
Stuart Schechter†
PI:EMAIL
David Wetherall∗
PI:EMAIL
University of Washington∗ Microsoft Research†
ABSTRACT
We examine two privacy controls for Android smartphones
that empower users to run permission-hungry applications
while protecting private data from being exﬁltrated:
(1) covertly substituting shadow data in place of data
that the user wants to keep private, and
(2) blocking network transmissions that contain data the
user made available to the application for on-device
use only.
We retroﬁt the Android operating system to implement
these two controls for use with unmodiﬁed applications. A
key challenge of imposing shadowing and exﬁltration block-
ing on existing applications is that these controls could cause
side eﬀects that interfere with user-desired functionality. To
measure the impact of side eﬀects, we develop an automated
testing methodology that records screenshots of application
executions both with and without privacy controls, then au-
tomatically highlights the visual diﬀerences between the dif-
ferent executions. We evaluate our privacy controls on 50
applications from the Android Market, selected from those
that were both popular and permission-hungry. We ﬁnd
that our privacy controls can successfully reduce the eﬀective
permissions of the application without causing side eﬀects
for 66% of the tested applications. The remaining 34% of
applications implemented user-desired functionality that re-
quired violating the privacy requirements our controls were
designed to enforce; there was an unavoidable choice be-
tween privacy and user-desired functionality.
Categories and Subject Descriptors
D.4.6 [Operating Systems]: Security and Protection;
K.4.1 [Computers and society]: Public Policy Issues—
Privacy
General Terms
Design, Security
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CCS’11, October 17–21, 2011, Chicago, Illinois, USA.
Copyright 2011 ACM 978-1-4503-0948-6/11/10 ...$10.00.
1.
INTRODUCTION
When a user prepares to install an application on the
increasingly-popular Android platform, she will be presented
with an ultimatum: either grant the application every per-
mission demanded in its manifest or abandon the installa-
tion entirely. Even when a user agrees that an application
should have access to sensitive data to provide functionality
she desires, once the application has access to these data
it may misappropriate them and exﬁltrate them oﬀ the de-
vice. Despite the permission disclosures provided by the
Android platform, the power of permission ultimatums and
the inevitability that data will be misappropriated for ex-
ﬁltration have created an application ecosystem in which
privacy-invasive applications are commonplace [8]. U.S. fed-
eral prosecutors have even taken notice, initiating a criminal
investigation into the misappropriation of users’ data by mo-
bile applications [3].
We have developed a system, called AppFence, that
retroﬁts the Android operating system to impose privacy
controls on existing (unmodiﬁed) Android applications.
AppFence lets users withhold data from imperious applica-
tions that demand information that is unnecessary to per-
form their advertised functionality and, for data that are
required for user-desired functions, block communications
by the application that would exﬁltrate these data oﬀ the
device.
When an application demands access to sensitive data a
user doesn’t want it to have, AppFence substitutes innocu-
ous shadow data in its place. For example, an application
that demands a user’s contacts may receive shadow data that
contains no contact entries, contains only those genuine en-
tries not considered sensitive by the user, or that contains
shadow entries that are entirely ﬁctional. Similarly, an appli-
cation that demands the unique device ID (IMEI number),
which is frequently used to proﬁle users across applications,
may instead receive the hash of the device ID salted with a
device secret and the application name. This shadow data
provides the illusion of a consistent device ID within the
application, but is diﬀerent from the ID given to other ap-
plications on the same device. Presenting a diﬀerent device
ID to each application thwarts the use of this ID for cross-
application proﬁling. In other words, when an application
demands the device ID for the purpose of linking the user to
a cross-application proﬁle, shadowing the device ID empow-
ers users to reply that their device is “not the droid you’re
looking for.”
639Shadowing prevents the ingress of sensitive data into ap-
plications, breaking applications that truly require the cor-
rect data to provide functionality the user wants. For ex-
ample, a user cannot examine or search her contacts if the
application only has access to an empty shadow contact list.
For data that is allowed to enter the application, we intro-
duce a complementary data-egress control to prevent infor-
mation from being misappropriated and sent oﬀ the device:
exﬁltration blocking. We extend the TaintDroid information-
ﬂow tracking system [8] to track data derived from informa-
tion the user considers private, then block unwanted trans-
missions of these data. For each sensitive data type in the
system, AppFence can be conﬁgured to block messages con-
taining data of that particular type.
In this paper, we ﬁrst measure how 110 popular
permission-hungry applications use the private information
that they have access to. We expose the prevalence of third-
party analytics libraries packaged within the studied appli-
cations and reveal that these applications send sensitive data
over encrypted channels (Section 2). This investigation of
existing applications’ behavior guided us to design two pri-
vacy controls, shadowing and exﬁltration blocking, that pro-
tect against undesired uses of the user’s sensitive data by
applications (Section 3). We then study the potential side
eﬀects of these two privacy controls on the user experience of
50 applications. We develop a novel testing methodology for
eﬃciently and reliably repeating experiments to investigate
the user-discernable side eﬀects that result when privacy
controls are imposed on applications. The testing process
records applications’ screenshots and highlights the diﬀer-
ences between executions so that they can be easily analyzed
visually (Section 4). The evaluation that we performed us-
ing this methodology shows that, by combining shadowing
and exﬁltration blocking, it is possible to eliminate all side
eﬀects in the applications we studied except for those that
represent a direct conﬂict between user-desired functionality
and the privacy goal that our controls enforce—that private
data must not leave the device (Section 5). We discuss fu-
ture and related work in Sections 6 and 7, and we conclude
in Section 8.
We make the following three contributions. First, we pro-
vide an extensive analysis of information exposure by An-
droid applications in terms of types of information inves-
tigated, forms of exposure including encryption, and ex-
posure patterns to advertising and analytics servers. Sec-
ond, we present two privacy controls for reducing sensitive
data exposure and show experimentally that the controls are
promising: the privacy controls reduced the eﬀective permis-
sions of 66% of the 50 applications in our testbed without
side eﬀects. Last, we develop a novel testing methodology
to detect side eﬀects by combining automated GUI testing
with visual highlighting of diﬀerences between application
screenshots. This methodology allows us to characterize the
side eﬀects of the tested applications, revealing some com-
mon functionalities of Android applications that require the
exposure of the user’s sensitive data and are thus unavoid-
ably in conﬂict with the goal of privacy controls.
2. PRIVACY RISKS ON ANDROID
To inform the design of our privacy controls, we performed
several initial measurements and analyses of today’s An-
droid applications. As an application cannot misappropriate
data it does not have access to, we ﬁrst measured the preva-
lence with which applications request access to each type of
potentially sensitive data. We then determined the preva-
lence with which applications exﬁltrate data of each type
and where they send the data to.
2.1 Application selection
We used three sets of applications for our initial measure-
ments (described in this section) and in-depth experiments
(described in Section 5). We began with a set of 1100 pop-
ular free Android applications, which we obtained by sam-
pling the 50 most popular applications from each of 22 cate-
gories listed by the Android Market in November 2010. We
examined this set of applications to identify a set of 11 per-
missions that applications must request to access sensitive
data (Section 2.2). We also analyzed this set of applications
to identify third-party packages that applications include for
advertising and analytics (A&A) purposes (Section 2.4).
From the set of 1100 applications, we then selected a sub-
sample of 110 applications for deeper analysis. For each
type of permission used to access sensitive data, we in-
cluded in the subsample at least 10 applications that used
the permission, drawing ﬁrst from those applications that
contained a third-party A&A package and, if more appli-
cations were needed, next drawing from the set of applica-
tions without A&A packages but that still required Internet
access.1 We did not exclude any awkward or challenging
applications when sampling the 110 applications. This sub-
sample is intentionally biased in favor of permission-hungry
applications: those that require the most permissions. This
bias toward permission-hungry applications only increases
the likelihood that our experiments in Section 5 will cause
side eﬀects when imposing privacy controls. In other words,
this sampling overestimates side eﬀects, leading to a conser-
vative (high) estimate of the actual rate of side eﬀects for
the privacy controls in our experiments.
For our in-depth experiments in Section 5, which required
scripting user inputs for each application to automate test-
ing, we further subsampled the 110 applications; the labor-
intensive nature of writing the test scripts limited us to au-
tomating 50 of the 110 applications. To select the 50 appli-
cations, we ﬁrst excluded applications that did not transmit
any type of sensitive data during our preliminary analyses;
again, this increased the likelihood of identifying side eﬀects
in our experiments. We also excluded ﬁve applications that
could not be tested using a single device with automated
user behavior (for example, the Bump application requires
two devices for its primary functionality). From the remain-
ing pool of applications, we randomly selected 50 of them
to be scripted; Appendix B lists the resulting set of applica-
tions.
2.2 Sources of sensitive information
By examining the applications in our 1100-application
sample, we identiﬁed 11 permissions that could result in
the disclosure of 12 types of sensitive information: loca-
tion, phone_state (granting access to phone number &
unique device ID information types as well as call state),
contacts, user account information, camera, microphone,
browser history & bookmarks, logs, SMS messages, cal-
endar, and subscribed_feeds. We measured the prevalence
with which applications demanded each permission by pars-
1Fewer than 10 applications requested access to the sub-
scribed feeds and calendar permissions.
640Resource type
phone_state
location
contacts
camera
account
logs
microphone
SMS messages
history & bookmarks
calendar
subscribed_feeds
Applications
374 (34.0%)
368 (33.5%)
105 (9.5%)
84 (7.6%)
43 (3.9%)
38 (3.5%)
32 (2.9%)
24 (2.2%)
19 (1.7%)
9 (0.8%)
2 (0.2%)
Table 1: Of the 1100 popular Android applications
we examined, those that required both access to a
resource containing sensitive data and access to the
Internet (through which data might be exﬁltrated)
ing the applications’ manifests using the publicly available
Android aapt tool [13]. We ﬁnd that 605 applications (55%)
require access to at least one of these resources and access
to the Internet, resulting in the potential for unwanted dis-
closure. We present these results broken down by resource
type in Table 1.
2.3 Misappropriation
Prior work has revealed that some Android applications
do exploit user data for purposes that may not be ex-
pected nor desired by users. Enck et al., who developed
the TaintDroid information-ﬂow tracking system extended
in our work, used this system to analyze 30 Android ap-
plications that required access to the Internet and either
users’ location, camera, or microphone [8]. They found
that half of these applications shared users’ locations with
advertisement servers. The problem is not unique to An-
droid. Egele et al. used static analysis to track informa-
tion ﬂow in popular iPhone applications and discovered that
many contained code to send out the unique device ID [7].
Smith captured network traﬃc to observe iPhone applica-
tions transmitting device IDs [18]. The Wall Street Journal
commissioned its own study of 50 iPhone applications and
50 Android applications, also using a network-observation
approach [22, 23]. The article suspects that these unique
IDs are so commonly transmitted because they can be used
to proﬁle users’ behaviors across applications.
2.4 A proﬁle of the proﬁlers
Given the existing concerns over cross-application proﬁl-
ing of user behavior, we examined our sample of 1100 ap-
plications to identify third-party advertising and analytics
(A&A) libraries that might build such proﬁles. We used
the Android apktool [1] to disassemble and inspect applica-
tion modules to identify the most commonly used libraries.
We found eight A&A packages, listed in Table 2. AdMob
was the most popular A&A package, employed by a third
of our sample applications, followed by Google Ads. Google
acquired AdMob in 2010; the combined application mar-
ket share of AdMob and existing Google Ads and Analytics
packages was 535 of our 1100 applications (49%). We also
found that 591 applications (54%) have one or more A&A
packages included in their code. Moreover, 361 of these ap-
plications (33%) demand access to the Internet and at least
A&A Module
admob.android.ads
google.ads
flurry.android
google.android.apps.analytics
adwhirl
mobclix.android.sdk
millennialmedia.android
qwapi.adclient.android
Applications
all
1100
sensitive
605
360
242
110
91
79
58
48
39
(33%)
(22%)
(10%)
(8%)
(7%)
(5%)
(4%)
(3%)
225
140
88
66
67
46
47
37
(37%)
(23%)
(15%)
(11%)
(11%)
(8%)
(8%)
(6%)
Table 2: The prevalence of third-party advertising
and analytics modules in our sample of 1100 Android
applications, and a subset of 605 applications that
demand access to at least one resource containing
potentially sensitive information.
A&A destination
*.admob.com
*.doubleclick.net
data.flurry.com
*.googlesyndication.com