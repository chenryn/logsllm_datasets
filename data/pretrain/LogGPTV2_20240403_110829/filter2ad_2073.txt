# Title: POSTER: Attacks on Federated Learning: Responsive Web User Interface to Recover Training Data from User Gradients

## Authors
- Hans Albert Lianto<sup>∗</sup>
- Yang Zhao<sup>∗†</sup>
- Jun Zhao

### Affiliation
Nanyang Technological University, Singapore

### Contact Information
- **Hans Albert Lianto**: [EMAIL]
- **Yang Zhao**: [EMAIL]
- **Jun Zhao**: [EMAIL]

---

## Abstract
Local differential privacy (LDP) is an emerging privacy standard designed to protect individual user data. One scenario where LDP can be applied is in federated learning, where each user sends their gradients to an aggregator who uses these gradients to perform stochastic gradient descent. If the aggregator is untrusted and LDP is not applied to each user's gradient, the aggregator can potentially recover sensitive user data from these gradients. In this paper, we present an interactive web demo that showcases the power of LDP by visualizing federated learning with and without LDP. The live demo illustrates how LDP can prevent untrusted aggregators from recovering sensitive training data. We also introduce a metric called exp-hamming recovery to quantify the extent to which the aggregator can recover data.

---

## CCS Concepts
- **Security and Privacy**: Distributed systems security
- **Computing Methodologies**: Supervised learning by classification; Supervised learning by regression

---

## Keywords
- Local differential privacy
- Machine learning
- Federated learning
- Linear regression
- Stochastic gradient descent

---

## ACM Reference Format
Hans Albert Lianto, Yang Zhao, and Jun Zhao. 2020. POSTER: Attacks on Federated Learning: Responsive Web User Interface to Recover Training Data from User Gradients. In Proceedings of the 15th ACM Asia Conference on Computer and Communications Security (ASIA CCS '20), October 5–9, 2020, Taipei, Taiwan. ACM, New York, NY, USA, 3 pages. https://doi.org/10.1145/3320269.3405438

---

## 1. Introduction

### 1.1 Local Differential Privacy
Local differential privacy (LDP) is a technique used to perturb data locally, ensuring that even if the perturbed data is exposed to adversaries (including the data curator or aggregator), the user's sensitive information remains protected. This means that the data curator or aggregator cannot obtain true information from each entry in the dataset. Google implemented LDP in its RAPPOR technology [4].

**Definition 1.** A randomized algorithm \( f \) satisfies \( \epsilon \)-local differential privacy if and only if, for any two tuples \( t \) and \( t' \) in the domain of \( f \), and for all subsets \( S \) of the output range:
\[ P[f(t) \in S] \leq e^\epsilon \times P[f(t') \in S], \]
where \( \epsilon \) is the privacy budget and \( P[\cdot] \) is the probability.

Many companies store enterprise data in relational form using RDBMS software. For example, Airbnb and Uber use MySQL [1], while Netflix and Instagram use PostgreSQL [2]. Data stored in this format consists of tuples or records with the same schema. LDP-perturbed data can also be stored in these relational forms.

Methods to perturb multidimensional data to satisfy \( \epsilon \)-LDP have been proposed, including Duchi et al.'s method [3] and Wang's improved methods (the Piecewise Mechanism and Hybrid Mechanism) [5]. These methods are used in the web demo described in subsequent sections.

### 1.2 Local Differential Privacy in Federated Learning
One practical application of LDP is in machine learning algorithms, particularly in federated or centralized distributed learning. As shown in Figure 1, each user independently trains gradients, which are then sent to the parameter server for aggregation [6]. The data used to generate these gradients is sensitive and should not be leaked.

**Figure 1: Federated Learning Flowchart**

Without LDP, a significant portion of the training data can be recovered by an untrusted aggregator in any federated learning setting where users submit their gradients for training. This is demonstrated in the web demo.

---

## 2. Demo Overview

A web user interface (UI) called "ldp-machine-learning" has been created to simulate stochastic gradient descent in federated learning and to show how an untrusted aggregator can recover training data from each user. A screenshot of the demo is shown in Figure 2, and the demo is publicly available at https://ldp-machine-learning.herokuapp.com/.

**Figure 2: Screenshot of ldp-machine-learning Demo**

### 2.1 Animation and Animation Speed
The main animations in the demo occur when the "Train" and "Recover" buttons are clicked. When the "Train" button is clicked, one epoch of stochastic gradient descent takes place, and the current cost or training accuracy of the model is displayed. When training is complete, the final model cost and accuracy are shown. When the "Recover" button is clicked, the untrusted aggregator attempts to recover sensitive user training data from the gradients. At the end of the recovery, the average exp-hamming recovery for a user is displayed. The training and recovery animation typically occurs at a rate of one user per second, but changing the "Training Animation Speed" input to "Instant" will display the results immediately.

### 2.2 Machine Learning Algorithms and LDP Perturbation Mechanisms
The demo supports various machine learning algorithms such as linear regression, logistic regression, and support vector machines, which can be toggled as shown in Figure 3. Additionally, the LDP perturbation mechanism can be selected from four options: Laplace mechanism, Duchi et al.'s mechanism [3], Piecewise mechanism [5], and Hybrid mechanism [5]. The privacy budget \( \epsilon \) of the LDP algorithm can also be adjusted.

**Figure 3: Toggling Between Different ML Algorithms, LDP Algorithms, and Privacy Budgets**

### 2.3 Other Features and Specifications
The demo includes "Add 10 Users" and "Add 100 Users" buttons to automatically generate new training data. A scroll-up button is provided for easier navigation. These features are shown in Figure 4.

**Figure 4: Add Users and Scroll Up Buttons**

Each user's training data is generated using the following equations:
- For linear regression:
  \[ d = -0.55x_1 - 0.82x_2 + 0.07x_3 + 0.95x_4 + 0.31 \]
- For logistic regression and SVM:
  \[ d = \begin{cases} 
  1 & \text{if } -0.55x_1 - 0.82x_2 + 0.07x_3 + 0.95x_4 + 0.31 > 0 \\
  -1 & \text{otherwise}
  \end{cases} \]

The ideal weights for the model are \([-0.55, -0.82, 0.07, 0.95, 0.31]\). Initial model weights and training data are generated using a pseudorandom seed via an extension in the JavaScript Math library.

### 2.4 Exp-Hamming Recovery
Exp-hamming recovery is a metric that quantifies the difficulty for an aggregator to recover user training data. Consider a user's training data as the vector \(\vec{x} = (x_1, x_2, ..., x_n)\) with \(n\) dimensions. The aggregator's recovered training data is the vector \(\vec{x_r} = (x_{r1}, x_{r2}, ..., x_{rn})\). It is more difficult to recover \(\vec{x}\) if \(\vec{x_r}\) is farther from \(\vec{x}\). The Manhattan distance (ℓ1-norm) is used as the distance metric.

Exp-hamming recovery is defined as:
\[ E = \exp(-k \|\vec{x} - \vec{x_r}\|_1), \]
where \(k\) is a customizable constant, and \(\|\cdot\|_1\) represents the Manhattan distance. If \(\vec{x} = \vec{x_r}\), there is full recovery, meaning \(E = 1\). If \(\|\vec{x} - \vec{x_r}\|_1 = \infty\), there is no information gained, meaning \(E = 0\). A value of \(k = 0.5\) is used in the demo.

### 2.5 Privacy Budget Allocation and Training Specifications
For privacy budget management, the budget is allocated equally to each gradient value. With 5 values to perturb, if the total privacy budget is \(\epsilon\), each value is perturbed with a budget of \(\epsilon/5\). For training, the learning rate for all machine learning algorithms is set to \(\alpha = 0.01\).

---

## 3. Implementation

**Figure 5: Software Architecture Diagram for ldp-machine-learning**

The web UI is built using React, an open-source JavaScript library. The project's appearance is maintained in the 'components/' directory, while the site logic is in the 'utils/' directory. The version control system is Git, and the project code is stored on GitHub. The demo is deployed and served on Heroku, a cloud platform-as-a-service.

**Figure 6: UML State Machine Diagram for ldp-machine-learning**

---

## 4. Conclusions
In this paper, we present a web GUI that illustrates the effectiveness of local differential privacy in protecting user data in federated learning. The demo is publicly accessible, allowing researchers to understand LDP in the context of machine learning. The GUI is extensible to other machine learning algorithms and LDP mechanisms. Future work can include adding training hyperparameters such as batch size and learning rates for better training results.

---

## Acknowledgments
This research was supported by:
1. Nanyang Technological University (NTU) Startup Grant
2. Alibaba-NTU Singapore Joint Research Institute (JRI)
3. Singapore Ministry of Education Academic Research Fund Tier 1 RG128/18, Tier 1 RG115/19, Tier 1 RT07/19, Tier 1 RT01/19, and Tier 2 MOE2019-T2-1-176
4. NTU-WASP Joint Project
5. Singapore National Research Foundation (NRF) under its Strategic Capability Research Centres Funding Initiative: Strategic Centre for Research in Privacy-Preserving Technologies & Systems (SCRIPTS)
6. Energy Research Institute @NTU (ERIAN)
7. Singapore NRF National Satellite of Excellence, Design Science and Technology for Secure Critical Infrastructure NSoE DeST-SCI2019-0012
8. AI Singapore (AISG) 100 Experiments (100E) programme
9. NTU Project for Large Vertical Take-Off & Landing (VTOL) Research Platform

---

## References
[1] Accessed in 2020. Why developers like MySQL. https://stackshare.io/mysql.

[2] Accessed in 2020. Why developers like PostgreSQL. https://stackshare.io/postgresql.

[3] John C Duchi, Michael I Jordan, and Martin J Wainwright. 2018. Minimax optimal procedures for locally private estimation. J. Amer. Statist. Assoc. 113, 521 (2018), 182–201.

[4] Úlfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. 2014. RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response. In Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security. 1054–1067.

[5] Ning Wang, Xiaokui Xiao, Yin Yang, Jun Zhao, Siu Cheung Hui, Hyejin Shin, Junbum Shin, and Ge Yu. 2019. Collecting and analyzing multidimensional data with local differential privacy. In 2019 IEEE 35th International Conference on Data Engineering (ICDE). IEEE, 638–649.

[6] Ligeng Zhu, Zhijian Liu, and Song Han. 2019. Deep leakage from gradients. In Advances in Neural Information Processing Systems (NeurIPS). 14747–14756.