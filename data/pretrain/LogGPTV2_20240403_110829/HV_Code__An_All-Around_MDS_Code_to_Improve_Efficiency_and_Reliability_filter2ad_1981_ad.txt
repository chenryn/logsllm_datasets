it can be recovered by using either horizontal parity chain
or vertical parity chain. When repairing Ei,j by horizontal
parity chain, the horizontal parity element Ei,(cid:2)2i(cid:3)p and other
556
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:59:30 UTC from IEEE Xplore.  Restrictions apply. 
Therefore, for a stripe of a code with (p − 1)-row-by-
(p − 1)-column and (p − 3)(p − 1) data elements,
the
optimal complexity of construction should be 2(p−4)
(p−3) XOR
operations per data element and the optimal complexity of
reconstruction should be (p − 4) XOR operations per lost
element in theorem.
For HV Code, there are 2(p − 1) parity elements needed
to build in the construction. Each of them is calculated by
performing (p − 4) XOR operations among the participated
(p − 3) data elements. Therefore, the total XOR operations
to generate the parity elements in a stripe is 2(p− 4)(p− 1),
and the averaged XOR operations per data element is 2(p−4)
(p−3) ,
being consistent with the above deduced result. Moreover,
to reconstruct the two corrupted disks, every invalid element
is recovered by a chain consisting of (p − 3) elements,
among which (p − 4) XOR operations are performed. The
complexity equals the optimal reconstruction complexity
deduced above.
For update efﬁciency, every data element joins the com-
putation of only two parity elements, indicating the update
of any data element will only renew related two parity
elements.
3) Achievement of Perfect Load Balancing: Rather than
concentrating the parity elements on dedicated disks [10] [4],
HV Code distributes the parity elements evenly among all
the disks, which is effective to disperse the load to disks.
4) Fast Recovery for Disk Failure: The length of each
parity chain in HV Code is p−2, which is shorter than those
of many typical MDS codes [3] [10] [4] [5] [7] in RAID-6.
The shortened parity chain decreases the number of needed
elements when repairing a lost element. In addition, every
disk holds two parity elements in HV Code, enabling the
execution of four recovery chains in parallel in double disk
repairs.
5) Optimized Partial Stripe Write Performance: We ﬁrst
dissect the performance for the writes to two continuous
data elements in HV Code. The ﬁrst case is when the two
data elements reside in the same row, then update them will
incur only one write I/O to the horizontal parity element
and two separate renewals to their vertical parity elements.
The second is when the renewed two data elements are in
different rows, i.e., the last data element in the i-th row and
the ﬁrst in the (i + 1)-th row. As described above, Ei,j will
participate in the generation of the vertical parity element
residing on the (cid:6)j − 2i(cid:7)p-th disk. This rule makes Ei,p−1
and Ei+1,1, if both of them are data elements, belong to
the same vertical parity chain. Therefore, the second case
only needs to update a shared vertical parity element and
the two corresponding horizontal parity elements. Since the
HV Code’s layout deﬁnes that a column will include two
parity elements, there will be at least (p − 6) pairs 2 of two
2There are altogether (p− 2) pairs of continuous two data elements that
p−2 , which approaches to 1 when
are in the different rows. So the rate is p−6
p grows.
continuous data elements which locate in different rows but
share the same vertical parity elements.
The proof in [10] has shown any two data element updates
should renew at
three parity elements in a stripe
of a lowest density MDS code. Thus, HV Code achieves
near optimal performance of partial stripe writes to two
continuous data elements.
least
V. PERFORMANCE EVALUATION
In this section, we will evaluate the performance of HV
Code in terms of recovery, partial stripe writes, degraded
read, and load balancing. We also select RDP Code (over
p+1 disks), HDP Code (over p−1 disks), H-Code (over p+1
disks) and X-Code (over p disks) to serve as the references.
Evaluation Environment: The performance evaluation is
run on a Linux server with a X5472 processor and 12GB
memory. The operating system is SUSE Linux Enterprise
Server and the ﬁlesystem is EXT3. The deployed disk
array consists of 16 Seagate/Savvio 10K.3 SAS disks, each
of which owns 300GB capability and 100,000rmp. The
machine and disk array are connected with a Fiber cable
with the bandwidth of 800MB. The ﬁve MDS RAID-6 codes
are realized based on Jerasure 1.2 [29] that is an open source
library and widely used in this literature.
Evaluation Preparation: We ﬁrst create a ﬁle, partition it
into many data elements, and encode them by using every
evaluated code. Like previous works [27] [28], the element
size is set as 16MB. The data elements and the encoded
parity elements will then be dispersed over the disk arrays
by following the layout of each code (like Fig. 1 for RDP
Code and Fig. 2 for X-Code). The encoded ﬁles will be used
in the next tests, such as partial stripe writes efﬁciency and
degraded read efﬁciency.
A. Partial Stripe Writes Efﬁciency
In this test, we mainly evaluate the efﬁciency when
performing partial stripe writes. For a write operation with
the length L, both the L continuous data elements whose
size is (16 × L) MB and the associated parity elements will
be totally written.
To evaluate the patterns of partial stripe writes, the fol-
lowing two traces are mainly considered.
• Uniform write trace: Every access pattern simulates the
operation to write a pre-deﬁned number of data ele-
ments starting from a uniformly chosen data element.
• Random write trace: Every access pattern (S, L, F )
contains three random values, i.e., the start element S
for the writes, the random length L, and the random
write frequency F . For the pattern (S, L, F ), it means
the write operation that starts from the S-th data ele-
ment and terminates at the (S + L− 1)-th data element
will be executed for F times.
In this test, we select two uniform write traces named
”uniform w 10” and ”uniform w 30”. For uniform write
557
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:59:30 UTC from IEEE Xplore.  Restrictions apply. 
(a) Total Induced Writes to A Stripe.
(b) The Load Balancing Rate.
(c) The Time to Complete A Write Pattern
Figure 6. The Partial Stripe Write Efﬁciency.(p = 13)
trace named ”uniform w L” (L:=10,30), we specify 1,000
write requests with the constant length L and the uniformly
selected beginning. These two traces ensure the same num-
ber of data elements in a stripe is written for each code and
the write frequency of each data element will be almost the
same with large probability. With respect to the random write
trace, we generate the patterns of (S, L, F ) by employing
the random integer generator [25] and the generated trace is
shown in Table II. For example, (28,34,66) means the write
operation will start from the 28th data element and the 34
continuous data elements will be written for 66 times.
Evaluation Method: For the ﬁle encoded by each code,
we replay the three write traces named ”uniform w 10”,
”uniform w 30”, and the generated random trace respec-
tively, just by performing every write pattern in them to the
ﬁle. During the evaluation, the following three metrics are
measured (as shown in Fig. 6).
1) The total induced writes of every trace. As referred
above, a write operation to a data element will trigger
the associated updates to its related parity elements.
For the ﬁle encoded by each code, we perform the
three traces and record all the I/O requests induced by
every trace (as shown in Fig. 6(a)).
2) The I/O balancing capability of each code. For the ﬁle
encoded by each code, we run every trace and collect
the incurred I/O requests loaded on each disk. To
reﬂect the balancing degree of the load to a stripe, we
then deﬁne the load balancing rate, which is the same
with the ”metric of load balancing” in [3]. Suppose
the number of write requests arriving at the i-th disk
is Ri and the number of disks in a stripe is N, we can
calculate the load balancing rate λ as:
M ax{Ri|1 ≤ i ≤ N}
M in{Ri|1 ≤ i ≤ N}
λ :=
(7)
The smaller of λ usually indicates the better behavior
of balancing the load to a stripe. We ﬁnally calculate
the load balancing rate of every trace for each code(as
shown in Fig. 6(b)).
3) The averaged time of a write pattern. For the ﬁle
encoded by each code, we measure the averaged time
to execute a write pattern in every trace, i.e., from
558
THE RANDOM WRITE PATTERN OF (S, L, F ).
Table II
(28,34,66)
(29,26,48)
(6,44,75)
(20,33,39)
(19,4,77)
(34,22,69)
(6,3,51)
(10,44,2)
(48,28,27)
(22,14,31)
(4,45,3)
(34,42,50)
(34,15,43)
(48,13,30)
(49,31,82)
(30,18,64)
(37,9,1)
(2,6,49)
(40,2,32)
(35,26,1)
(24,32,70)
(34,38,93)
(28,17,57)
(16,24,7)
(31,1,48)
the time to start the write pattern to the time when
the data elements in the pattern and corresponding
parity elements are completely written (as shown in
Fig. 6(c)).
Figure 6(a) indicates the total write operations of each
code scale up with the increase of L for the uniform
write traces ”uniform w 10” and ”uniform w 30”. Though
X-Code retains the optimal update complexity,
its con-
struction based on diagonal parity and anti-diagonal pari-
ty engenders more write requests to the associated parity
elements. Because of the high update complexity, HDP
Code triggers more write operations when compared to both
of HV Code and H-Code. Since both HV Code and H-
Code optimize the write operation to two continuous data
elements, both of them outperform other contrastive codes
when conducting the continuous writes. For uniform write
trace ”uniform w 10”, HV Code reduces up to 27.6% and
32.4% write I/O requests when compared to X-Code and
HDP Code, respectively. When conducting random write
trace, HV Code also eliminates about 18.4% and 16.2%
write I/O requests when compared with X-Code and HDP
Code, respectively. Even compared with H-Code, which has
optimized partial stripe writes, HV Code only increases
marginal extra overhead, about 0.9% under random write
trace.
Figure 6(b) illustrates the load balancing rate of various
codes under different access traces. Being evaluated by the
three kinds of traces, RDP Code easily concentrates the
writes to the parity disks and thus holds the largest load
balancing rate. The load balance rates of RDP under the
valuations of ”uniform w 10” and ”random write trace” are
13.2 and 5.75, respectively. Though H-Code disperses the
diagonal parity to the data disks, its uneven distribution
of diagonal parity elements and the dedicated disk for
horizontal parity storage still make it hard to achieve perfect
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:59:30 UTC from IEEE Xplore.  Restrictions apply. 
(a) The Averaged Time to Complete A Read Pattern
(b) I/O Efﬁciency of A Read Pattern
Figure 7. The Degraded Read Efﬁciency.(p = 13)
load balance. Its load balancing rates are 2.22 and 1.54
under the evaluations of ”uniform w 10” and ”random write
trace”, respectively. Owing to the even distribution of parity
elements, HV Code, HDP Code and X-Code approach the
perfect load balancing rate (i.e., 1) under the three write
traces.
The averaged time to complete a write pattern in these
three traces is also recoded in Figure 6(c). In the ﬁgure, RDP
Code needs the most time to complete the absorption of the
write requests to the diagonal parity elements. The incompat-
ible layout of X-Code and the expensive update complexity
of HDP Code also easily extend the completion time. When
performing the uniform trace ”uniform w 10”, the operation
time in HV Code decreases about 28.8%∼64.7% when
compared to those of RDP Code, HDP Code, and X-Code.
Being evaluated by the random write trace, the averaged
time to complete a write pattern in HV Code is about
17.6%∼47.2% less than those of RDP Code, X-Code, and
HDP Code. However, H-Code outperforms HV Code by
reducing 4.1% write time on this metric. This is because the
number of participating disks in H-Code is larger than that
of HV Code, making H-Code better at shunting the write I/O
requests. This comparison also reveals the ”tradeoff” brought
by the shorter parity chain, which keeps down the recovery
I/O for a single disk repair but is weaker at reducing the
amount of the average requests on each disk.
B. Degraded Read Comparison
Evaluation Method: Given a encoded ﬁle (encoded by
RDP Code, X-Code, HDP Code, H-Code, and HV Code
respectively) and the degraded read pattern with the length
of L (L := 1, 5, 10, 15, respectively), we let the elements
hosted on a disk corrupted (by either injecting faults or
erasing the data on that disk) and issue 100 degraded read
patterns with the length of L that start at uniformly selected
points. Two metrics are concerned in this failure case, i.e.,
the averaged time and the I/O efﬁciency for a degraded read
pattern.
Speciﬁcally, suppose L(cid:4) denotes the number of elements
returned for a degraded read pattern. When the L requested