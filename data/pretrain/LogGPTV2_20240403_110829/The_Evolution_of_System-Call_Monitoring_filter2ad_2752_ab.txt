We wanted a system that was sufﬁciently lightweight that
it could monitor all running programs in real-time, and even
respond to prevent attacks before they caused harm (as was
demonstrated in pH; see section 7). For this reason, we
chose to deﬁne the normal proﬁle using short sequences of
system calls. There are many possible ways to represent
short sequences of system calls, for example, lookahead
pairs, n-grams, trees, etc. Figure 1 shows some of these
representations, and they are described in Section 5. Most
of our early work used lookahead pairs, although we ex-
perimented with exact sequences and Markov models. We
disregarded system call parameters (the ﬂow of data), to fur-
ther simplify the problem. Our goal was to start as simply as
possible, and later expand the system if required: the need
for speed and real-time monitoring/response dictated that
we discover the simplest possible mechanisms that would
actually work.
3.2 Developing a normal proﬁle
The ﬁrst step in anomaly detection is gathering the data
that will constitute the proﬁle of normal behavior. We used
two different methods for this: generating a synthetic nor-
mal proﬁle by exercising a program in all of its anticipated
normal modes of usage; and collecting real normal proﬁles
by recording system call traces during normal, online usage
of a production system1. The learning algorithm was used
to determine the minimum set of short sequences of sys-
tem calls that adequately deﬁned normal: it’s goal was to
include all normally used code paths, but exclude those that
are never used, even if those paths exist within the program
1An alternative approach is to determine normal through static analysis
of the program code. See section 6.3
420420
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:00:19 UTC from IEEE Xplore.  Restrictions apply. 



















Figure 1. Representing system call streams
source code. An important class of attacks involves inject-
ing foreign code (e.g. buffer overﬂows), but many other
attacks force a program to exercise existing but rarely used
code paths, and hence do not require foreign code injection.
Developing a normal proﬁle is a typical machine learning
problem: Undergeneralization leads to false positives, and
overgeneralization leads to false negatives.
Experimentation with a wide range of programs (e.g.
sendmail, lpr, inetd, ftp, named, xlock, login,
ssh) demonstrated that these programs exhibit regular be-
havior and can be characterized by compact normal proﬁles
[40, 25, 14]. Figure 2 shows how normal behavior con-
verges to a ﬁxed set of system call sequences of length 6
for lpr in a production environment—the Artiﬁcial Intelli-
gence Laboratory at the Massachusetts Institute of Technol-
ogy (MIT). The ﬁgure shows how initially there are many
new sequences, but after a while few novel sequences are
observed and the proﬁle converges to a ﬁxed size.
Not only can normal behavior be deﬁned by limited sets
of sequences of system calls, but what constitutes normal
differs widely from one program to the next. For exam-
ple, a typical run of ftp differed by between 28 and 35%
(depending on the sequence length) from sendmail [14].
More importantly, different environments and usage pat-
terns resulted in dramatically different normal, even for the
identical program and operating system. For instance, the
normal proﬁle for lpr gathered at MIT differed markedly
from the normal proﬁle gathered at the University of New
Mexico’s Computer Science Department (UNM): only 29%
of the unique sequences in the MIT proﬁle were also present
in the UNM proﬁle. Later work on pH corroborated these
results in lookahead pairs, showing 1) that two program pro-
ﬁles were 20-25% similar on average (over hundreds of pro-
grams) and 2) the same programs running on three different
s
e
c
n
e
u
q
e
s
e
u
q
i
n
u
f
o
r
e
b
m
u
N
1000
800
600
400
200
0
0
500000
1000000
1500000
Total number of sequences
Figure 2. Growth of normal database for lpr
from MIT’s Artiﬁcial Intelligence Laboratory
(reprinted from [76]).
hosts with the same OS version differed, on average, in 22-
25% of their lookahead pairs [64].
These results offered clear support for what we have
termed the “Diversity Hypothesis”: Normal code paths exe-
cuted by a running program are highly dependent on typical
usage patterns, conﬁguration and environment, and hence
can differ widely from one installation to the next, even for
the same program and operating system. Diversity in biol-
ogy confers robustness on a population, and can do the same
for computer systems: an attack that succeeds against one
implementation could fail against another because normal is
different. Further, the attacker will not necessarily know a
priori what constitutes normal for a given implementation—
knowledge of the source code is not sufﬁcient; also required
is knowledge of the environment and usage patterns.
3.3 Detecting attacks
The normal proﬁle must not only be stable and com-
pact, but it must differ from behavior generated by at-
tacks. Extensive experimentation demonstrated that nor-
mal sequences of system calls differ from a wide variety
of attacks, including buffer overﬂows, SYN ﬂoods, conﬁg-
uration errors, race conditions, etc [14, 25, 40, 64]. This
variety shows that the method is capable not only of detect-
ing foreign code injection attacks (such as buffer overﬂows)
but attacks that exercise unused code paths that exist in the
program source. In contrast with methods based on static
analysis, this last point illustrates the importance of a pre-
cise deﬁnition of normal that excludes unused source code
paths.
One question of interest was determining the minimum
necessary system call length required to detect all attacks.
421421
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:00:19 UTC from IEEE Xplore.  Restrictions apply. 




l
i
l
a
n
g
S
y
a
m
o
n
A
d
e
z
i
l
a
m
r
o
N
1.0
0.8
0.6
0.4
0.2
0.0
0
200
400
600
800
1000
Number of System Calls
Figure 3. Anomaly signal of the syslogd in-
trusion. (Reprinted from [25].)
Shorter sequences are good because they result in more
compact normal proﬁles, faster learning times, and less
overhead during sequence matching. However, short se-
quences can potentially be more easily evaded (see section
4) and give less clear signals of an attack. A good mini-
mal length was found to be 6 system calls [25], although
for implementation reasons our later work used 8. Even at
such short sequence lengths, attacks were usually obvious
because they consisted of temporally clumped anomalies,
as shown in Figure 3. In this ﬁgure, the x-axis represents
time (in units of system calls) and the y-axis is the normal-
ized number of recently seen anomalous system calls.
Many of the detected anomalies were not malicious. For
example, the method detected errors such as forwarding
loops, failed print jobs, and system misconﬁgurations. This
illustrated one additional beneﬁt of anomaly detection: It
can be used to detect faults caused by non-malicious errors.
This contrasts with a signature detection system, which usu-
ally has only the ability to detect known malicious events.
4 Subverting system call monitoring
Because most security properties of current software sys-
tems and algorithms cannot be proven, advances in secu-
rity have long relied upon researchers studying systems and
ﬁnding security vulnerabilities in them. Mimicry attacks
were the earliest attempt to defeat the system call modeling
approach. Wagner and Dean proposed that it was possible
to craft sequences of system calls that exploited an attack,
but appeared normal [74]. The trick to achieving this in-
volves inserting “nulliﬁed” system calls, i.e. calls that have
no effect, either because the return values are ignored or
the parameters are manipulated. This enables an attacker to
construct an attack sequence within a legitimate sequence
using the “nulliﬁed” calls as padding. The mimicry has to
persist as long as the attacker is exploiting the process that is
being monitored, even once the initial penetration has suc-
ceeded. See Figure 4.
fstat()
setreuid()
sigprocmask()
flock() sigaction()
read() write() close() munmap() sigprocmask() wait4()
sigprocmask() sigaction() alarm() time() stat() read()
alarm()
getpid()
time() write() time() getpid() sigaction() socketcall()
sigaction() close() flock() getpid() lseek() read()
kill() lseek()
alarm() time()
stat() write() open() fstat() mmap() read() open()
fstat() mmap() read() close() munmap() brk() fcntl()
setregid() open() fcntl() chroot() chdir() setreuid()
lstat() lstat() lstat() lstat() open() fcntl() fstat()
lseek() getdents() fcntl() fstat() lseek() getdents()
close() write() time() open() fstat() mmap() read()
close() munmap() brk() fcntl() setregid() open() fcntl()
chroot() chdir() setreuid() lstat() lstat() lstat()
lstat() open() fcntl() brk() fstat() lseek() getdents()
lseek() getdents() time() stat() write() time() open()
getpid() sigaction() socketcall() sigaction() umask()
sigaction()
alarm()
getrlimit() pipe() fork() fcntl() fstat() mmap() lseek()
close() brk() time() getpid() sigaction() socketcall()
sigaction() chdir() sigaction() sigaction() write()
munmap() munmap() munmap() exit()
read()
alarm()
time()
stat()
Figure 4. Sequence of system calls in a
mimicry attack against wuftpd. The under-
lined calls are part of the attack, all the rest
are nulliﬁed calls. [75]
However,
there are limitations to such “nulliﬁed”
mimicry attacks. First, the attacker needs to be able to inject
the code containing the specially crafted sequence, which
limits these mimicry attacks to only those that can exploit