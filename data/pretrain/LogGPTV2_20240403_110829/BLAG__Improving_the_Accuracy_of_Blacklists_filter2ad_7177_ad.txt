I-Blocklist [26], Malware Bytes [25], Snort Labs [42], TrustedSec [48], Haley [10], Darklist [19], SIP blacklist [45], VoIPBL [51], DShield [57],
NoThink [34], OpenBL [36], Cruzit [16], BruteforceBlocker [18], Clean MX [13], Bad IPs [5], MaxMind [32]
ImproWare [27], Malware bytes [25], Cleantalk [14], My IP [33], BadIPs [5]
Table II: Four types of blacklists, roughly categorized by the type of malicious activities they capture. Each row gives the number of blacklists
and blacklist maintainers for that type.
hosts. The legitimate and malicious IP addresses are collected
during September 2016. We collect malicious IP addresses
from Netlab’s [2] collection of Mirai-infected hosts during
September 2016. There were around 3.9 M addresses, which
form our malicious set. We use the ﬁrst 7 days consisting of
1.1 M IP addresses as a validation set (Mv) and the remaining
16 days consisting of 2.8 M IP addresses for testing (Mte). We
collect legitimate IP addresses by identifying Web clients who
communicated with a set of popular web servers at a mid-size
US University in September 2016. We included only those
TCP connections which exchanged payload with the server
(thus excluding scans). This resulted in around 61 K legitimate
IP addresses. We use the ﬁrst 7 days consisting of 16 K IP
addresses as the known-legitimate sources (Ltr), the next 7
days consisting of 12 K IP addresses for validation (Lv) and
the remaining 16 days consisting of 33 K IP addresses as the
future legitimate sources (Lte).
3) DDoS on DNS root: In this scenario (referred to as
DDoSDNS), we look into a case of TCP SYN ﬂood attack on
the DNS B-root server [58]. We communicated with the dataset
provider to obtain non-anonymized sources of this attack, as
well as non-anonymized data before the attack, which we use
for training. We mine the malicious IP addresses (Mte) as those
that have sent TCP SYN ﬂood to the server for two hours on
June 25, 2016. There are 5.5 M malicious IP addresses. We
mine the known-legitimate sources (Ltr) as sources of DNS
queries 1-day before the attack event (2.7 M IP addresses), and
the future legitimate sources (Lte) as sources of DNS queries
during the attack event (16 K IP addresses).
C. Limitations
The blacklist dataset contains only publicly available bla-
cklists, while many providers also offer for-pay blacklists that
are usually larger and more accurate [78] (see Section V for
the performance of our monitored blacklists). We chose to use
only publicly available blacklists because: (1) these blacklists
are widely used and we wanted to evaluate BLAG’s beneﬁts
for a customer network that deploys such blacklists. A recent
survey shows that nearly 60% of surveyed network operators
use blacklists including publicly available ones [56]. We also
believe that BLAG’s beneﬁts would carry over to for-pay
datasets because its mechanism is generic. (2) we wanted our
work to be repeatable, and using public blacklists enables us to
freely share our data. We plan to share the blacklist dataset and
BLAG code, which could be used by any customer network
to improve blacklisting.
Our scenario datasets suffer from several limitations. First,
they only capture a small sample of legitimate/malicious IP
addresses that were active on the Internet at a given point
in time, and for a given legitimate or malicious purpose.
Many other IP addresses could be legitimate or malicious
at the same time, and we have no ground truth for these.
We also rely on other security technologies (SpamAssassin)
that may also be used by blacklist maintainers to blacklist
an address. These limitations are present in other published
works [80], [70], [75], [78], [55], which use similarly-sized
malicious and legitimate datasets, and rely on other security
technologies such as Proofpoint [39] and SpamAssassin, as
we do, to establish maliciousness at a given time. A more
recent study on Blacklisting has used Alexa’s top 10,000 list
to evaluate the accuracy of blacklists based on IP addresses
that are present in them [63] but Alexa’s rankings are also
not ideal measure of legitimacy [73], [61]. These limitations
cannot be avoided, as there is no complete, 100% accurate list
of legitimate and malicious IP addresses on the Internet nor in
any speciﬁc network, at any given point in time.
Second, our datasets are dated – captured in 2016. Ideally,
we would use more recent datasets. But, it is very hard to ﬁnd
data about attack events and legitimate trafﬁc, which includes
non-anonymized IP addresses. It is even harder to ﬁnd such
data streams that are collected simultaneously and that relate to
the same type of sources (e.g., sources of legitimate email vs
sources of spam). While dated, our scenarios faithfully capture
sources of legitimate and malicious trafﬁc at the same time.
8
When working with each scenario BLAG only uses blacklist
data up to that point in time. Thus we faithfully simulate what
BLAG’s performance would have been if it were deployed by
a customer network at the time.
Finally, we do not have a validation dataset for the
DDoSDNS, because the attack on B-Root DNS server was
observed only for a few hours. From evaluation of our pa-
rameter values on Email and DDoSUniv datasets (Section VII)
we observe that parameters l (historical decay) and K (matrix
factorization) do not change with the dataset, and high values
of parameter α (threshold for candidate IP addresses) lead
to higher speciﬁcity. Therefore, to evaluate the performance
of BLAG for DDoSDNS scenario, we use the same l and K
parameters as that of Email and DDoSUniv scenarios and set a
high value for α.
V. EVALUATION
In this section, we evaluate the performance of BLAG and
several competing approaches, for different deployment sce-
narios, described in Section IV. We ﬁnd that BLAG achieves
high speciﬁcity (95–99%), and has much better recall (3.5x–
114x improvement) than competing approaches. BLAG also
detects attackers 13.7 days faster than competing approaches.
A. Evaluation Setup
We measure performance of blacklists using recall and spe-
ciﬁcity. Recall measures the percentage of malicious sources
(out of some ground-truth set) that were blacklisted. Speciﬁcity
measures the percentage of legitimate sources (out of some
ground-truth set) that were not blacklisted. Higher values for
both measures denote better, more accurate, blacklists.
In this section, we compare several competing blacklisting
approaches against BLAG. In all cases when we evaluate a
blacklisting approach we “travel back in time” to the time just
before the testing portion of our given Scenario. We then use
past data from our (B) dataset up to the time when the testing
portion starts, and refresh it as blacklists update during the
testing. For example, imagine if our scenario contained two
days Sep 1st, 2016 and Sep 2nd, 2016, with Sep 1st used
for training and Sep 2nd for testing. Our blacklist dataset (B)
overlaps this period going from Jan 8th, 2016 to Nov 30th,
2016. When we test a blacklisting approach for the given
scenario we would start by including all data from (B) from
Jan 8th up to, and including Sep 1st. We would then start our
evaluation and keep updating the blacklist on Sep 2nd. This
way we faithfully simulate the performance a given approach
would have if we were to travel back in time to Sep 2nd.
We compare the performance of the following approaches
against BLAG:
– the blacklist from Blacklist dataset
Best
that performs
the best on a given scenario with regard to recall. Best is a
hypothetical scenario, because in real deployment we could
not tell which blacklist will eventually be the best. It allows
us to measure the beneﬁts of aggregation over the use of a
single blacklist.
Historical – all IP addresses listed in any blacklist in the
Blacklist dataset. This approach assumes “once malicious,
always malicious” and performs naïve aggregation.
PRESTA+L – the blacklist generated using techniques descri-
bed in [78]. PRESTA performs spatio-temporal analysis and
expansion using historical blacklist data to generate a more
proactive blacklist. PRESTA assigns a reputation score to all
blacklisted IP addresses across three different spatial groups
(IP address, address’s /24 preﬁxes along with two surrounding
/24 preﬁxes and address’s corresponding autonomous systems).
PRESTA combines all
the spatial grouping into one and
chooses relevant listings using a thresholding technique. We
tune PRESTA such that the BLAG’s recall is equivalent to
that of PRESTA for a deployment scenario, and then we
compare the speciﬁcity of these two approaches. Further, to
tease apart the factors that help us outperform PRESTA, we
consider a variant approach, called PRESTA+L, where we
remove addresses that are already present in known-legitimate
sources (Ltr) from every dataset.
BLAG with and without selective expansion: We run BLAG
as described in Section III. We set the length of address history
l = 30 and the number of latent features for recommendation
system K = 5. We set relevance threshold α = 0.8 for
Email/DDosDNS datasets and α = 0.6 for DDoSUniv dataset.
Our choices for these variables are explained in Section VII.
We compare the performance of BLAG with and without the
selective expansion, to show the contributions of aggregation
and expansion stages of BLAG.
During the evaluation, for our testing dataset and each
blacklisting approach (best, historical, PRESTA+L or BLAG)
we simulate the dynamics of address appearance over time
in the following manner. When an address appears in the
blacklist dataset (B) we: (1) include the address in the best
blacklist if it appeared on a blacklist, which will ultimately
perform the best on the given BLAG deployment scenario,
(2) include the address in the historical blacklist, (3) apply
PRESTA+L algorithm on the address and all its spatial groups
to determine if it is included in the PRESTA+L blacklist,
(4) apply BLAG on the address to determine if it should be
included in the BLAG’s aggregated master blacklist and if it
should be expanded into its /24 preﬁx. We report recall and
speciﬁcity at the end of the testing datasets.
B. BLAG is More Accurate
BLAG’s goal is to capture as many malicious sources as
possible while keeping the speciﬁcity high.
BLAG has the best speciﬁcity/recall tradeoff across the
three scenarios: Figure 6 shows that BLAG overall has the
best performance. For the Email scenario, the best blacklist
has higher speciﬁcity (100%) than BLAG (95%). However,
BLAG improves the best blacklist’s recall from 4.7% to 69.7%.
Historical blacklist has a recall of 19.4%, which is better than
the best blacklist but not better than BLAG. Naïve aggregation
of IP addresses in historical blacklist lowers its speciﬁcity
to 89% which is much lower than that of BLAG (95%).
For the same recall, BLAG has 11.5% better speciﬁcity than
PRESTA+L. We observe similar pattern with DDoSUniv and
DDoSDNS scenarios. BLAG’s recall ranges between 6.4–56.1%
when compared to 0.004-0.4% for best blacklist and 1.8–
9.5% for historical blacklists. For the same recall, BLAG’s
speciﬁcity ranges from 97.9–99.5% when compared to 53.1–
84.8% of PRESTA+L. We detail in Section VI-B how the
9
(a) Email
(b) DDoSUniv
(c) DDoSDNS
Figure 6: Speciﬁcity and recall of BLAG with two competing approaches on trafﬁc datasets.
BLAG lists malicious sources faster. We show in Figure 7
the number of days taken by other competing approaches to
list malicious IP addresses after BLAG discovers them. We
track competing approaches for up to 30 days. On average
across all scenarios, BLAG reports malicious sources 9.4
days faster than the best blacklist, 10.3–16.1 days faster than
historical blacklists and 8.8–13.4 days faster than PRESTA+L.
BLAG’s aggregation helps in detecting attackers faster than
the best blacklist and BLAG’s selective expansion helps in
detecting attackers faster than the historical blacklist. After
30 days, the best blacklist catches up to 2.1% of malicious
IP addresses discovered by BLAG for the Email scenario
and does not report any malicious address for DDoSUniv and
DDoSDNS scenarios. On the other hand, historical blacklists
catch up to 0.2–4.1% and PRESTA+L catch up to 0.14–0.23%
of malicious IP addresses discovered by BLAG.
VI. SENSITIVITY ANALYSIS
In this section, we discuss the contribution of BLAG’s
expansion phase. We then look into the contribution of BLAG’s
recommendation system in pruning misclassiﬁcations for the
aggregation and expansion phase.
A. Expansion approaches
BLAG’s performance comes both from aggregation and
expansion. We investigate how much of BLAG’s performance
comes from its selection of high-quality data to aggregate
and how much comes from expansion, by showing BLAG
with and without expansion (BLAG and BLAG No Exp in
Figure 6). Without expansion, for the Email scenario, BLAG’s
recall is 19.3% and its speciﬁcity is 98.7%, while the best
blacklist has 8.9% recall and 100% speciﬁcity. BLAG is thus
still better than the best blacklist, even without expansion.
The historical blacklist has 19.4% recall (vs 19.3% of BLAG
without expansion), but it has 89% speciﬁcity (vs 98.7% of
BLAG). Finally, PRESTA+L has 68% recall (vs 19.3% of
BLAG without expansion), but it has much lower speciﬁcity
(84.8% vs 98.7%). This is expected since PRESTA+L applies
expansion and we compare it to BLAG without expansion.
Expansion of BLAG improves recall further (to 69.7%), at
a small loss of speciﬁcity (95% speciﬁcity of BLAG with
Figure 7: Delay in reporting malicious IP addresses reported
by BLAG.
contribution of recommendation systems in BLAG helps it
achieve better speciﬁcity that PRESTA+L.
BLAG ﬁlters more attack trafﬁc. Some IP addresses may
generate more attacks than others. We evaluate the amount
of malicious activity that would be ﬁltered by BLAG, best
and historical blacklists for our three scenarios. In the Email
scenario, BLAG would ﬁlter 69.7% of spam, compared to 4.7%
and 19.4% ﬁltered by best and historical blacklists respectively.
In the DDoSUniv scenario, BLAG would ﬁlter trafﬁc from
56.1% of infected devices, compared to only 0.4% and 9.5%
ﬁltered by best and historical blacklists. In the DDoSDNS
scenario, BLAG would drop 6.4% of attack queries, compared
to 0.004% and 0.1% ﬁltered by best and historical blacklists.
We pause here to address the low performance of public
blacklists in general for offender identiﬁcation. While BLAG