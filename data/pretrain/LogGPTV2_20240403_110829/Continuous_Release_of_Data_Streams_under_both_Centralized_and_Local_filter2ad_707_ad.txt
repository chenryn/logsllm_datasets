far from it have a low probability of being reported. The server,
after receiving the reports from users, runs a specially designed
Expectation Maximization algorithm to find an estimated density
distribution that maximizes the expectation of observing the output.
For completeness, we describe details about SW in Appendix E.1.
Optimized Threshold with Estimated Distribution. To find
the threshold, the baseline method is to find a specific percentile
as the threshold θ. This method is used for finding frequent item-
set [45]. Based on the lessons learned from the threshold optimizer
in the DP setting, we use the optimization equation given in Equa-
tion 3 to find θ.
Specifically, denote ˜f as the estimated distribution where ˜ft
is the estimated frequency of value t. Here the set of all possible
t to be considered can no longer be [B] = {1, 2, . . . , B}. Instead,
we sample 1024 values uniformly from [B]. This is because SW
uses the Expectation Maximization algorithm, and a large domain
size makes it time- and space-consuming. Similar to Equation 4
considered in the DP setting, we use an error formula:
r
2
3 · Var[ ˜v] + r
24
˜ft(t − θ)
.
(8)
r
Here Var[ ˜v] denotes the variance of estimating v, which we will
describe later. It is multiplied by r3 because in expectation, a random
range query will involve r3 values, and each of them is estimated
independently. For the second part of Equation 8, it can be calculated
directly with SW. The multiplicative coefficient r 2
24 is the averaged
case over all possible range queries. That is, denote j as the range of
a query, there are r − j + 1 range-j queries within a limit r. In total,
j=1(r − j +1) possible queries. For each of them, we have
there arer
=
12
≈ r 2
− r(r +1)
8
24 as the average-case coefficient.
2 coefficient in the squared bias. Thus, we have
a j
(r +1)(2r +1)
Using SW as a White Box. To find a reasonable threshold using
SW, we make the following modifications. First, we eliminate the
smoothing step from SW, because we observe that in some cases,
smoothing will “push” the estimated probability density to the
two ends of the range. If some density is moved to the high end,
the chosen threshold θ can be unnecessarily large.
Second, we add a post-processing step to prune the small densi-
ties outputted by SW. In particular, we find the first qualified value
w, whose next 5 consecutive estimates are all below 0.01%. This is
a signal that the density after w will converge to 0. We thus replace
the estimated density after w with 0. In the experiment, we observe
that the two steps help to find a more accurate θ.
j =1(r−j+1)j2
2r(r +1)
4.2 Design of the (Local) Perturber
After obtaining the threshold θ, the server sends θ to all users.
When a user reports a value, it will first be truncated. The user
then reports the truncated value using the Hybrid mechanism.
Session 4D: Differential Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1243Table 1: Dataset Characteristics
n
1141961
8704495
990002
515597
max
2000
30000
41270
1657
p85
63
440
10
13
p95
85
1036
28
21
p99.5
135
2037
133
39
p100
617
26770
2498
165
avg
37.9
279.9
8.1
7.5
Dataset
DNS
Fare
Kosarak
POS
The method is described in Appendix E.2. It can estimate v with
worst-case variance given in Equation 14, which can be plugged
into Equation 8 to find θ. Note that the reports are unbiased by
themselves. So to answer a range query, we just need to sum up
values from the corresponding range, and there is no need for a
smoother.
5 EXPERIMENTAL EVALUATION
The experiment includes four phases. First, we give a high-level
end-to-end evaluation of the whole process. Second, we evaluate
the performance of the hierarchical method with a fixed truncation
threshold. Third, we fix the hierarchical method and test differ-
ent algorithms that give the threshold. Fourth, we evaluate the
performance in the local setting.
5.1 Evaluation Setup
Datasets. A total of four real-world datasets are examined.
• DNS: This dataset is extracted from a set of DNS query logs
collected by a campus resolver with all user ids and source IP
addresses removed 1. It includes 14 days of DNS queries. The
network administrator can use the number of queries to assess
Internet usage in a region. We take number of queries as the
stream in the evaluation.
• Fare [2]: New York City taxi travel fare. We use the Yellow Taxi
• Kosarak [1]: A dataset of clickstreams on a Hungarian website
that contains around 106 users and 41270 categories. We take it
as streaming data and use the size of click categories as the value
of the stream.
• POS [56]: A dataset containing merchant transactions of half a
million users and 1657 categories. We use the size of the transac-
tion as the value of the stream.
Table 1 gives the distribution statistics of the datasets.
Trip Records for January 2019.
Metrics. To evaluate the performance of different methods, we
use the metric of Mean Squared Error (MSE) to answer randomly
generated queries. In particular, we measure
(cid:2) ˜V(i, j) − V(i, j)(cid:3)2

(i, j)∈Q
MSE(Q) =
1
|Q|
.
(9)
where Q is the set of the randomly generated queries. It reflects
the analytical utility measured by Equation 1 from Section 2.1 (to
demonstrate the actual accuracy, we also have results for mean
absolute error in Appendix F). We set r = 220 as the maximal range
of any query.
1The data collection process has been approved by the IRB of the campus.
Methodology. The prototype was implemented using Python 3.7.3
and NumPy 1.15.3 libraries. The experiments were conducted on
servers running Linux kernel version 5.0 with Intel Xeon E7-8867
v3 CPU @ 2.50GHz and 576GB memory. For each dataset and each
method, we randomly choose 200 range queries and calculate their
MSE. We repeat each experiment 100 times and report the result of
mean and standard deviation. Note that the standard deviation is
typically very small, and barely noticeable in the figures.
5.2 End-to-end Comparison
First, as a case study, we visualize the estimated stream of our
method ToPS on the DNS dataset (Figure 2). The top row shows
the performance of ToPS while the bottom row shows that of PAK.
We run algorithms once for each setting to demonstrate the real-
world usage. Similar to the setting of PAK, in ToPS, we use the
first m = 65, 536 observations to obtain the threshold θ (we will
show later that ToPS does not need this large m observations to
be held). Figure 2 indicates that our method ToPS can give fairly
accurate predictions when ϵ is very small. On the other hand, PAK,
though under a larger ϵ, still performs worse than ToPS. Note that
to obtain the threshold θ, PAK satisfies (ϵ, δ)-DP while our ToPS
satisfies pure ϵ-DP during the whole process.
We then compare the performance of ToPS and PAK with our
metric of MSE given in Equation 9, and show the results in Fig-
ure 3. Between ToPS and PAK, we also include two intermediate
methods that replace Phase 1 (finding θ) and Phase 2 (hierarchical
method) of PAK by our proposed method NM-E (used in threshold
optimizer) and ˆHc16 (used for the perturber and smoother together),
respectively, to demonstrate the performance boost due to our new
design (we will evaluate the two phases in more details in later
subsections). From the figure, we can see that the performance of
all the algorithms gets better as ϵ increases, which is as expected.
Second, our proposed ToPS can outperform PAK by 7 to 11 orders
of magnitude. Third, the effect (in terms of improving utility) using
NM-E is much more significant than using ˆHc16. Interestingly, the
performance of ToPS and NM-E is similar in the Fare and Kosarak
datasets. This is because in these cases, the bias (due to truncation
by θ) is dominant.
5.3 Comparison of Stream Publication Phase
Several components contribute to the promising performance of
ToPS. To demonstrate the precise effect of each of them, we next
analyze them one by one in the reverse order. We first fix other
configurations and compare different smoothers and perturbers.
Subsequently, we analyze the methods of obtaining the threshold θ
in Section 5.4. To make the comparison clear, we set θ to be the 95-
th percentile of the values. Moreover, we assume the true values are
no larger than θ (the ground truth is truncated). We will compare
the performance of different methods in obtaining θ in Section 5.4.
Comparison of Different Smoothers. Fixing a threshold θ and
the hierarchical method optimized in Section 3.4, we now compare
the performance of five smoother algorithms listed in Section 3.5
(note that the smoothers will replace the 16s values where s is given
in Equation 7). Figure 4 shows the MSE of the smoothers given
ϵ from 0.01 to 0.1. As ϵ increases, that is, privacy budget loosens,
Session 4D: Differential Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1244(a) ToPS, ϵ = 0.01
(b) ToPS, ϵ = 0.05
(c) PAK, ϵ = 0.1
(d) PAK, ϵ = 0.5
Figure 2: Visualizations of the DNS stream. The x-axes correspond to time (we partition the 14-day timeframe into 120 intervals,
so each point corresponds to the mean of roughly 9000 values or 1.4 hours), and y-axes denotes the moving average. Our ToPS
at ϵ = 0.01 can output predictions that are pretty close to the ground truth. PAK gives noisier result even with larger ϵ values.
(a) Fare
(b) DNS
(c) Kosarak
(d) POS
Figure 3: Comparison between PAK and ToPS when answering range queries. We also include two intermediate methods NM-E
(our proposed threshold optimizer) and ˆHc16 (our proposed the perturber and smoother) that replace the corresponding two
phases of PAK to demonstrate the performance boost due to our new design.
(a) Fare
(b) DNS
(c) Kosarak
(d) POS
Figure 4: Evaluation of different smoothing techniques. We vary ϵ from 0.01 to 0.1 in the x-axis. The y-axis shows the query
accuracy (MSE).
the overall performance improves although the difference is very
small in Fare and Kosarak datasets. This is because there is no
clear pattern in these datasets. In the DNS dataset, Recent performs
better than others, as the data is stable in the short term. In POS,
the method of Mean and Median performs worse than the other
three. This is because Mean and Median consider all the history
(all the previous ui values given from the hierarchy, as described in
Section 3.5), while the other methods consider more recent results.
Methods that utilize the recent output (i.e., the more recent ui) will
perform better due to the stability property in the dataset (similar
Session 4D: Differential Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1245(a) Fare
(b) DNS
(c) Kosarak
(d) POS
Figure 5: Evaluation of different methods of outputting the stream. We vary ϵ from 0.01 to 0.1 in the x-axis and plot the query
accuracy (MSE) in the y-axis.
to the case of DNS). Since Recent performs the best in DNS, and is
among the best in other datasets, we use it as the default smoother
algorithm.
Note that large MSE does not always mean poor utility. Here
MSE is large because (1) the original values are large (e.g., >1000),
(2) the range query takes the sum, and (3) the square operation
further enlarges the values. We also have results for mean absolute
error (MAE) and mean query results to better demonstrate the
utility in Appendix F.
Comparison of Different Hierarchical Algorithms. To demon-
strate the precise effect of each design detail, we line up several
intermediate protocols for the hierarchy and threshold, respectively.
For the hierarchy component, we evaluate:
• H2: Original binary tree used in PAK.
• H16: The optimal fan-out b = 16 is used in the hierarchy.
• Hc16: H16 with consistency method.
• ˆHc16: We use the hat notation to denote the Recent smoother. It is
built on top of Hc16.
• Base: A baseline method that always outputs 0. It is used to
understand whether a method gives meaningful results.
Figure 5 gives the result varying ϵ from 0.01 to 0.1. First of all, all
methods (except the baseline) yield better accuracy as ϵ increases,
which is as expected. Moreover, the performance of all methods
(except ˆHc16) increases by a factor of 100× when ϵ increases from 0.01
to 0.1. This observation is consistent with the analysis that variance
is proportional to 1/ϵ
2. Comparing each method, using the optimal
branching factor (H16 versus H2) can improve the performance by
5×. Moreover, we have approximately another 2× (Hc16 versus H16)
of accuracy boost by adopting the consistency algorithm. For ˆHc16, a
constant 10× improvement can be observed over Hc16 except in the
DNS dataset, where ˆHc16 performs roughly the same as Hc16 when
ϵ > 0.08. The reason is that the error of ˆHc16 is composed of two
parts. One is the noise error from the constraint of DP, the other
the bias error of outputting the predicted values. When ϵ is large,