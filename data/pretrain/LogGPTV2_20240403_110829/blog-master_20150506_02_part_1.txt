## PostgreSQL 检查点性能影响及源码分析 - 2    
### 作者                   
digoal                    
### 日期                  
2015-05-06                      
### 标签                  
PostgreSQL , 检查点 , 性能影响 , full page write , FPW , 可靠性                                                  
----                  
## 背景    
数据库可靠性从何而来？      
数据库崩溃后如何恢复，从什么位置开始恢复？      
数据库检查点是什么？      
检查点要干些什么？      
为什么脏数据较多时，检查点会对性能有一定的影响？      
什么是full page write？      
相信这些问题是搞数据库的同学都想搞明白的。      
接下里的一系列文章，围绕检查点展开讲解，讲一讲检查点的原理，以及为什么脏数据较多是，它会对数据库产生一定的性能影响。        
## 正文    
接着上一篇讲解检查点最重的操作CheckPointGuts@src/backend/access/transam/xlog.c。  
http://blog.163.com/digoal@126/blog/static/163877040201542103933969/  
检查点最重量级的函数如下：  
CheckPointGuts@src/backend/access/transam/xlog.c  
```
static void  
CheckPointGuts(XLogRecPtr checkPointRedo, int flags)  
{  
        CheckPointCLOG();   // src/backend/access/transam/clog.c  
        CheckPointSUBTRANS();  // src/backend/access/transam/subtrans.c  
        CheckPointMultiXact();  // src/backend/access/transam/multixact.c  
        CheckPointPredicate();  // src/backend/storage/lmgr/predicate.c  
        CheckPointRelationMap();  // src/backend/utils/cache/relmapper.c  
        CheckPointReplicationSlots();  //  src/backend/replication/slot.c  
        CheckPointSnapBuild();   // src/backend/replication/logical/snapbuild.c  
        CheckPointLogicalRewriteHeap();  // src/backend/access/heap/rewriteheap.c  
        CheckPointBuffers(flags);       // src/backend/storage/buffer/bufmgr.c  
        CheckPointTwoPhase(checkPointRedo);  //  src/backend/access/transam/twophase.c  
}  
```
分解到每个调用：  
1\. 将commit log在buffer中的脏数据刷到pg_clog目录下对应的文件中。  
CheckPointCLOG@src/backend/access/transam/clog.c  
```
/*  
 * Perform a checkpoint --- either during shutdown, or on-the-fly  
 */  
void  
CheckPointCLOG(void)  
{  
        /* Flush dirty CLOG pages to disk */  
        TRACE_POSTGRESQL_CLOG_CHECKPOINT_START(true);  
        SimpleLruFlush(ClogCtl, true);  
        TRACE_POSTGRESQL_CLOG_CHECKPOINT_DONE(true);  
}  
```
2\. 将subtrans log在buffer中的脏数据刷到pg_subtrans目录下对应的文件中。  
CheckPointSUBTRANS@src/backend/access/transam/subtrans.c  
```
/*  
 * Perform a checkpoint --- either during shutdown, or on-the-fly  
 */  
void  
CheckPointSUBTRANS(void)  
{  
        /*  
         * Flush dirty SUBTRANS pages to disk  
         *  
         * This is not actually necessary from a correctness point of view. We do  
         * it merely to improve the odds that writing of dirty pages is done by  
         * the checkpoint process and not by backends.  
         */  
        TRACE_POSTGRESQL_SUBTRANS_CHECKPOINT_START(true);  
        SimpleLruFlush(SubTransCtl, true);  
        TRACE_POSTGRESQL_SUBTRANS_CHECKPOINT_DONE(true);  
}  
```
3\. 将MultiXact log在buffer中的脏数据刷到pg_multixact目录下对应的文件中。  
CheckPointMultiXact@src/backend/access/transam/multixact.c  
```
/*  
 * Perform a checkpoint --- either during shutdown, or on-the-fly  
 */  
void  
CheckPointMultiXact(void)  
{  
        TRACE_POSTGRESQL_MULTIXACT_CHECKPOINT_START(true);  
        /* Flush dirty MultiXact pages to disk */  
        SimpleLruFlush(MultiXactOffsetCtl, true);  
        SimpleLruFlush(MultiXactMemberCtl, true);  
        TRACE_POSTGRESQL_MULTIXACT_CHECKPOINT_DONE(true);  
}  
```
4\. Flush dirty SLRU(simple least recent used) pages to disk  
CheckPointPredicate@src/backend/storage/lmgr/predicate.c  
```
/*  
 * Perform a checkpoint --- either during shutdown, or on-the-fly  
 *  
 * We don't have any data that needs to survive a restart, but this is a  
 * convenient place to truncate the SLRU.  
 */  
void  
CheckPointPredicate(void)  
{  
        int                     tailPage;  
        LWLockAcquire(OldSerXidLock, LW_EXCLUSIVE);  
        /* Exit quickly if the SLRU is currently not in use. */  
        if (oldSerXidControl->headPage tailXid))  
        {  
                /* We can truncate the SLRU up to the page containing tailXid */  
                tailPage = OldSerXidPage(oldSerXidControl->tailXid);  
        }  
        else  
        {  
                /*  
                 * The SLRU is no longer needed. Truncate to head before we set head  
                 * invalid.  
                 *  
                 * XXX: It's possible that the SLRU is not needed again until XID  
                 * wrap-around has happened, so that the segment containing headPage  
                 * that we leave behind will appear to be new again. In that case it  
                 * won't be removed until XID horizon advances enough to make it  
                 * current again.  
                 */  
                tailPage = oldSerXidControl->headPage;  
                oldSerXidControl->headPage = -1;  
        }  
        LWLockRelease(OldSerXidLock);  
        /* Truncate away pages that are no longer required */  
        SimpleLruTruncate(OldSerXidSlruCtl, tailPage);  
        /*  
         * Flush dirty SLRU pages to disk  
         *  
         * This is not actually necessary from a correctness point of view. We do  
         * it merely as a debugging aid.  
         *  
         * We're doing this after the truncation to avoid writing pages right  
         * before deleting the file in which they sit, which would be completely  
         * pointless.  
         */  
        SimpleLruFlush(OldSerXidSlruCtl, true);  
}  
```
前面4个调用，全部用到了SimpleLruFlush来完成刷缓存的动作。  
SimpleLruFlush@src/backend/access/transam/slru.c  
```
/*  
 * Flush dirty pages to disk during checkpoint or database shutdown  
 */  
void  
SimpleLruFlush(SlruCtl ctl, bool checkpoint)  
{  
        SlruShared      shared = ctl->shared;  
        SlruFlushData fdata;  
        int                     slotno;  
        int                     pageno = 0;  
        int                     i;  
        bool            ok;  
        /*  
         * Find and write dirty pages  
         */  
        fdata.num_files = 0;  
        LWLockAcquire(shared->ControlLock, LW_EXCLUSIVE);  // 注意每次都要获取排他锁  
        for (slotno = 0; slotno num_slots; slotno++)  
        {  
                SlruInternalWritePage(ctl, slotno, &fdata);   // 这个可能会是比较重的操作  
                /*  
                 * When called during a checkpoint, we cannot assert that the slot is  
                 * clean now, since another process might have re-dirtied it already.  
                 * That's okay.  
                 */  
                Assert(checkpoint ||  
                           shared->page_status[slotno] == SLRU_PAGE_EMPTY ||  
                           (shared->page_status[slotno] == SLRU_PAGE_VALID &&  
                                !shared->page_dirty[slotno]));  
        }  
        LWLockRelease(shared->ControlLock);  
        /*  
         * Now fsync and close any files that were open  
         */  
        ok = true;  
        for (i = 0; i do_fsync && pg_fsync(fdata.fd[i]))  
                {  
                        slru_errcause = SLRU_FSYNC_FAILED;  
                        slru_errno = errno;  
                        pageno = fdata.segno[i] * SLRU_PAGES_PER_SEGMENT;  
                        ok = false;  
                }  
                if (CloseTransientFile(fdata.fd[i]))  
                {  
                        slru_errcause = SLRU_CLOSE_FAILED;  
                        slru_errno = errno;  
                        pageno = fdata.segno[i] * SLRU_PAGES_PER_SEGMENT;  
                        ok = false;  
                }  
        }  
        if (!ok)  
                SlruReportIOError(ctl, pageno, InvalidTransactionId);  
}  
```
5\. 将rel mapper文件缓存写入文件, 什么是rel mapper文件呢？  
rel mapper存储了一些数据库全局对象和文件ID的映射关系，一般的对象这种关系存储在全局对象pg_class.relfilenode中。  
```
 * For most tables, the physical file underlying the table is specified by  
 * pg_class.relfilenode.  However, that obviously won't work for pg_class  
 * itself, nor for the other "nailed" catalogs for which we have to be able  
 * to set up working Relation entries without access to pg_class.  It also  
 * does not work for shared catalogs, since there is no practical way to  
 * update other databases' pg_class entries when relocating a shared catalog.  
 * Therefore, for these special catalogs (henceforth referred to as "mapped  
 * catalogs") we rely on a separately maintained file that shows the mapping  
 * from catalog OIDs to filenode numbers.  Each database has a map file for  
 * its local mapped catalogs, and there is a separate map file for shared  
 * catalogs.  Mapped catalogs have zero in their pg_class.relfilenode entries.  
```
rel mapping文件名：  
每个数据库有一个pg_filenode.map文件，全局还有一个pg_filenode.map文件。  
这些文件分别放在表空间/database_oid/目录和global/目录下。  
```
/*  
 * The map file is critical data: we have no automatic method for recovering  
 * from loss or corruption of it.  We use a CRC so that we can detect  
 * corruption.  To minimize the risk of failed updates, the map file should  
 * be kept to no more than one standard-size disk sector (ie 512 bytes),  
 * and we use overwrite-in-place rather than playing renaming games.  
 * The struct layout below is designed to occupy exactly 512 bytes, which  
 * might make filesystem updates a bit more efficient.  
 *  
 * Entries in the mappings[] array are in no particular order.  We could  
 * speed searching by insisting on OID order, but it really shouldn't be  
 * worth the trouble given the intended size of the mapping sets.  
 */  
#define RELMAPPER_FILENAME              "pg_filenode.map"  
CheckPointRelationMap@src/backend/utils/cache/relmapper.c  
/*  
 * CheckPointRelationMap  
 *  
 * This is called during a checkpoint.  It must ensure that any relation map  
 * updates that were WAL-logged before the start of the checkpoint are  
 * securely flushed to disk and will not need to be replayed later.  This  
 * seems unlikely to be a performance-critical issue, so we use a simple  
 * method: we just take and release the RelationMappingLock.  This ensures  
 * that any already-logged map update is complete, because write_relmap_file  
 * will fsync the map file before the lock is released.  
 */  
void  
CheckPointRelationMap(void)  
{  
        LWLockAcquire(RelationMappingLock, LW_SHARED);  // 隐式fsync, 加锁前会自动完成fsync.  
        LWLockRelease(RelationMappingLock);  
}  
```
6\. 将流复制replication slots信息刷到pg_replslot目录下对应的文件中。  
CheckPointReplicationSlots@src/backend/replication/slot.c  
```
/*  
 * Flush all replication slots to disk.  
 *  
 * This needn't actually be part of a checkpoint, but it's a convenient  
 * location.  
 */  
void  
CheckPointReplicationSlots(void)  
{  
        int                     i;  
        elog(DEBUG1, "performing replication slot checkpoint");  
        /*  
         * Prevent any slot from being created/dropped while we're active. As we  
         * explicitly do *not* want to block iterating over replication_slots or  
         * acquiring a slot we cannot take the control lock - but that's OK,  
         * because holding ReplicationSlotAllocationLock is strictly stronger, and  
         * enough to guarantee that nobody can change the in_use bits on us.  
         */  
        LWLockAcquire(ReplicationSlotAllocationLock, LW_SHARED);  
        for (i = 0; i replication_slots[i];  
                char            path[MAXPGPATH];  
                if (!s->in_use)  
                        continue;  
                /* save the slot to disk, locking is handled in SaveSlotToPath() */  
                sprintf(path, "pg_replslot/%s", NameStr(s->data.name));  
                SaveSlotToPath(s, path, LOG);  
        }  
        LWLockRelease(ReplicationSlotAllocationLock);  
}  
```
7\. 逻辑复制相关的脏数据，刷入pg_logical/snapshots目录下对应的文件。  
CheckPointSnapBuild@src/backend/replication/logical/snapbuild.c  
```
/*  
 * Remove all serialized snapshots that are not required anymore because no  
 * slot can need them. This doesn't actually have to run during a checkpoint,  
 * but it's a convenient point to schedule this.  
 *  
 * NB: We run this during checkpoints even if logical decoding is disabled so  