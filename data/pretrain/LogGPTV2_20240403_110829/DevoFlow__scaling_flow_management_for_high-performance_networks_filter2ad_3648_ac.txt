twice per second would require slightly more than the 17
Mbit/sec bandwidth available between the switch CPU
and the controller!
Optionally, Read-State can request a report aggre-
gated over all ﬂows matching a wild-card speciﬁcation;
this can save switch-to-controller bandwidth but loses
the ability to learn much about the behavior of speciﬁc
ﬂows.
Pull-based statistics can be used for ﬂow scheduling if they
can be collected frequently enough. The evaluation of one
ﬂow scheduler, Hedera [5], indicates that a 5 sec. control
loop (the time to pull statistics from all access switches,
compute a re-routing of elephant ﬂows, and then update
ﬂow table entries where necessary) is fast enough for near-
optimal load balancing on a fat-tree topology; however, their
workload is based on ﬂow lengths following a Pareto distri-
bution. Recent measurement studies have shown data center
ﬂow sizes do not follow a Pareto distribution [9, 24]. Using
a workload with ﬂow lengths following the distribution of
ﬂow sizes measured in [24], we ﬁnd that a 5 sec. statistics-
gathering interval can improve utilization only 1–5% over
Figure 1: The latency of statistics gathering as the number of ﬂow
table entries is varied on the 5406zl Each point is the average of
10 runs, except for the maximum reply time points which are the
maximum value seen out of those 10 runs. The switch was idle
when the statistics were pulled.
randomized routing with ECMP (details are in §5). This is
conﬁrmed by Raiciu et al., who found that the Hedera con-
trol loop needs to be less than 500ms to perform better than
ECMP on their workload [41].
We measured how long it took to read statistics from the
5406zl as we varied the number of ﬂow table entries. The re-
sults are shown in Figure 1. For this experiment (and all oth-
ers in this section), we attached three servers to the switch:
two servers were clients (A and B) and one was the Open-
Flow controller. To get measurements with no other load on
the switch, we conﬁgured the switch so that its ﬂow table
entries never expired. Then, both clients opened ten connec-
tions to each other. The controller waited 5 sec. to ensure
that the switch was idle, and then pulled the statistics from
the switch. This process was repeated until the ﬂow table
contained just over 32K entries.
From this experiment, we conclude that pull-based statis-
tics cannot be collected frequently enough. We ﬁnd that the
statistic-gathering latency of the 5406zl is less than one sec-
ond only when its ﬂow table has fewer than 5600 entries and
less than 500 ms when it has fewer than 3200 entries, and
this is when there is no other load on the switch. Recall that
a rack of 40 servers will initiate approximately 1300 ﬂows
per second. The default ﬂow table entry timeout is 60 sec.,
so a rack’s access switch can be expected to contain ∼78K
entries, which, extrapolating from our measurements, could
take well over 15 sec. to collect! (The 5406zl does not sup-
port such a large table.) This can be improved to 13K table
entries by reducing the table entry timeout to 10 sec.; how-
ever, it takes about 2.5 sec. to pull statistics for 13K entries
with no other load at the switch, which is still too long for
ﬂow schedulers like Hedera.
In short, the existing statistics mechanisms impose high
overheads, and in particular they do not allow the controller
to request statistics visibility only for the small fraction of
signiﬁcant ﬂows that actually matter for performance.
Impact on ﬂow setup of statistics-gathering
3.2.3
Statistics-gathering and ﬂow setup compete for the limited
switch-controller bandwidth—the more frequently statistics
are gathered, the fewer ﬂows the switch can set up.
We performed an experiment to measure this interference.
Figure 2: The ﬂow setup rate between the clients and the num-
ber of statistics collected vs. the number of times statistics are
requested per second. Each point is the average of 10 runs.
Here, each client has inﬁnitely many ﬂows to send to the
other, and connections are established serially, that is, once
one connection is established, another is opened. The client
sends a single packet before closing each connection. We then
vary the number of statistics-pulling requests between 0–10
requests per second.
Switch state size
The results are shown in Figure 2. We measured the num-
ber of connections achieved per second and the number of
ﬂow-entry statistics collected each second, as we varied the
rate of requesting statistics for the entire ﬂow table. It is
clear that statistics-pulling interferes with ﬂow setup. When
statistics are never pulled, the clients can make 275 connec-
tions/sec.; when they are pulled once a second, collecting
counters for just under 4500 entries, the clients can make
fewer than 150 connections/sec.
3.2.4
A limited number of ﬂow entries can be supported in hard-
ware. The 5406zl switch hardware can support about 1500
OpenFlow rules, whereas the switch can support up to 64000
forwarding entries for standard Ethernet switching. One rea-
son for this huge disparity is that OpenFlow rules are stored
in a TCAM, necessary to support OpenFlow’s wildcarding
mechanism, and TCAM entries are an expensive resource,
whereas Ethernet forwarding uses a simple hash lookup in
a standard memory. It is possible to increase the number of
TCAM entries, but TCAMs consume lots of ASIC space and
power. Also, an OpenFlow rule is described by 10 header
ﬁelds, which total 288 bits [35], whereas an Ethernet for-
warding descriptor is 60 bits (48-bit MAC + 12-bit VLAN
ID); so, even when fully optimized, OpenFlow entries will
always use more state than Ethernet forwarding entries.
Finally, because OpenFlow rules are per-ﬂow, rather than
per-destination, each directly-connected host will typically
require an order of magnitude more rules. (As we mentioned,
an average ToR switch might have roughly 78,000 ﬂow rules
if the rule timeout is 60 sec.) Use of wildcards could reduce
this ratio, but this is often undesirable as it reduces the abil-
ity to implement ﬂow-level policies (such as multipathing)
and ﬂow-level visibility. Table 1 summarizes these limits.
Forwarding
scheme
Ethernet learning
OpenFlow
Descriptor Possible entries
size
60 bits
288 bits
on 5406zl
∼64000
∼1500
Entries per
active host
1
10 (typical)
Table 1: State sizes: OpenFlow vs. Ethernet learning
05000100001500020000250003000035000Flow table size ((cid:31)ows)02468Time to pull statistics (s)Average reply timeMaximum reply time012345678910Stats request rate (req / s)050100150200250300TCP connections rate (sockets / s)TCP connection rate02000400060008000Stats collected (entries / s)Stats collectedImplementation-imposed controller overheads
3.2.5
Involving the controller in all ﬂows creates a potential scal-
ability problem: any given controller instance can support
only a limited number of ﬂow setups per second. For ex-
ample, Tavakoli et al. [45] report that one NOX controller
can handle “at least 30K new ﬂow installs per second while
maintaining a sub-10 ms ﬂow install time ... The controller’s
CPU is the bottleneck.” Kandula et al. [30] found that 100K
ﬂows arrive every second on a 1500-server cluster, implying
a need for multiple OpenFlow controllers.
Recently researchers have proposed more scalable Open-
Flow controllers. Maestro [11] is a multi-threaded controller
that can install about twice as many ﬂows per second
as NOX, without additional latency. Others have worked
on distributed implementations of the OpenFlow controller
(also valuable for fault tolerance) These include Hyper-
Flow (Tootoonchian and Ganjali [47]) and Onix (Koponen
et al. [33]). These distributed controllers can only support
global visibility of rare events such as link-state changes, and
not of frequent events such as ﬂow arrivals. As such, they are
not yet suitable for applications, such as Hedera [5], which
need a global view of ﬂow statistics.
3.3 Hardware technology issues
A fair question to ask is whether our measurements are
representative, especially since the 5406zl hardware was not
designed to support OpenFlow. Hardware optimized for
OpenFlow would clearly improve these numbers, but throw-
ing hardware at the problem adds more cost and power
consumption. Also, Moore’s law for hardware won’t provide
much relief, as Ethernet speeds have been increasing at least
as fast as Moore’s law over the long term. Adding a faster
CPU to the switch may improve control-plane bandwidth,
but is unlikely to provide signiﬁcant improvements without
a bigger datapath and reorganized memory hierarchy. We
believe such changes are likely to be complicated and ex-
pensive, especially because, for high-performance workloads,
OpenFlow needs signiﬁcantly more bandwidth between the
data-plane and the control-plane than switches normally
support (see §5.3). We expect this bandwidth gap will shrink
as ASIC designers pay more attention to OpenFlow, but we
do not think they will let OpenFlow performance drive their
chip-area budgets for several generations, at least. And while
Moore’s Law might ameliorate the pressure somewhat, the
need to reduce both ASIC cost and energy consumption sug-
gests that hardware resources will always be precious.
An alternative implementation path is to use software-
based OpenFlow switch implementations on commodity
server hardware [18]. Such switches may have more control-
plane bandwidth, but we do not believe these systems will be
cost-eﬀective for most enterprise applications in the foresee-
able future. Casado et al. [13] have also argued that “network
processors” are not ideal, either.
4. DEVOFLOW
We now present the design of DevoFlow, which avoids
the overheads described above by introducing mechanisms
for eﬃcient devolved control (§4.1) and statistics collection
(§4.2). Then, we discuss how to implement these mechanisms
(§4.4), and end with an example of using DevoFlow to reduce
use of the control-plane (§4.4).
4.1 Mechanisms for devolving control
We introduce two new mechanisms for devolving control
to a switch, rule cloning and local actions.
Rule cloning: Under the standard OpenFlow mechanism
for wildcard rules, all packets matching a given rule are
treated as one ﬂow. This means that if we use a wildcard
to avoid invoking the controller on each microﬂow arrival,
we also are stuck with routing all matching microﬂows over
the same path, and aggregating all statistics for these mi-
croﬂows into a single set of counters.
In DevoFlow, we augment the “action” part of a wildcard
rule with a boolean CLONE ﬂag. If the ﬂag is clear, the
switch follows the standard wildcard behavior. Otherwise,
the switch locally “clones” the wildcard rule to create a
new rule in which all of the wildcarded ﬁelds are replaced
by values matching this microﬂow, and all other aspects
of the original rule are inherited. Subsequent packets for
the microﬂow match the microﬂow-speciﬁc rule, and thus
contribute to microﬂow-speciﬁc counters. Also, this rule
goes into the exact-match lookup table, reducing the use
of the TCAM, and so avoiding most of the TCAM power
cost [37]. This resembles the proposal by Casado et al. [13],
but their approach does per-ﬂow lookups in control-plane
software, which might not scale to high line rates.
Local actions: Certain ﬂow-setup decisions might require
decisions intermediate between the heavyweight “invoke the
controller” and the lightweight “forward via this speciﬁc
port” choices oﬀered by standard OpenFlow. In DevoFlow,
we envision rules augmented with a small set of possible “lo-
cal routing actions” that a switch can take without paying
the costs of invoking the controller. If a switch does not sup-
port an action, it defaults to invoking the controller, so as
to preserve the desired semantics.
Examples of local actions include multipath support and
rapid re-routing:
• Multipath support gives the switch a choice of sev-
eral output ports for a clonable wildcard, not just one.
The switch can then select, randomly from a probability
distribution or round-robin, between these ports on each
microﬂow arrival; the microﬂow-speciﬁc rule then inher-
its the chosen port rather than the set of ports. (This
prevents intra-ﬂow re-ordering due to path changes.)
This functionality is similar to equal-cost multipath
(ECMP) routing; however, multipath wildcard rules pro-
vide more ﬂexibility. ECMP (1) uniformly selects an out-
put port uniformly at random and (2) requires that the
cost of the multiple forwarding paths to be equal, so it