calculation in Example 3 we have IUbv = − log(cid:0) ω
from Proposition 3, we get RUbv = − log(cid:0) ω
all ω, ILbv = 1. Thus, the adversary’s initial knowledge about
the parity of A does not aﬀect the quantity of information
leaked by PROG C1. However, the real question is not how
much information is leaked by this program, but what the
remaining uncertainty represents in term of security threat to
2(1+ω)(cid:1). And
1+ω(cid:1). Therefore for
Fig. 1.
Information ﬂow of PROG C1
1) |B| = 1 and the a priori probability of A is publicly-
known.
2) For each adversary’s initial belief (cid:0) pρ(a), pρ(b | a)(cid:1) in
Beliefs(A, B) and for each program pρ(o | a) we have
V(A : B) = V(A) and V(A | O : B) = V(A | O).
3) For each adversary’s initial belief (cid:0) pρ(a), pρ(b | a)(cid:1) in
Beliefs(A, B) and for each program pρ(o | a) we have
H∞(A | B) = H∞(A) and H∞(A | O : B) = H∞(A | O).
Proof: (1) ⇒ (2): |B| = 1 implies that B is a constant.
Hence B is independent of both A and O. Furthermore, the
only possible adversary’s belief, which is the publicly-known
a priori distribution of A, is 1-accurate. Thus V(A : B) =
V(A | B). But V(A | B) = V(A) since A and B are independent.
Similarly, V(A | O : B) = V(A | O).
(2) ⇒ (1): (By contradiction). Assume that (2) holds and
|B| > 1 or the adversary does not know the a priori distribution
of A. Let ﬁrst consider the case |B| > 1. Then we can
create an adversary’s belief which is exactly ω-accurate for
any 0 < ω < 1. Thus for such belief we have V(A : B) =
ω · V(A | B). Therefore if we choose ω , V(A)/V(A | B), then
V(A : B) , V(A). Hence, it contradicts our initial hypotheses
that (2) holds.
Now assume that |B| = 1 but the adversary does not know
the a priori distribution of A. Again B is a constant and thus
irrelevant. If A is not uniformly distributed then it is easy
to construct an adversary’s assumed a priori distribution of
A such that V(A : B) , V(A | B) = V(A). If however A
is uniformly distributed then we can still create a program
such that V(A | O : B) , V(A | O, B) = V(A | O). Again, this
contradicts our initial hypotheses that (2) holds.
Finally, the equivalence (2) ⇔ (3) follows because functions
g(x) = − log(x) and g′(x) = 2−x are strictly monotone.
V. On the Applicability of the Belief-vulnerability
Approach
The previous section establishes the reasonableness of our
deﬁnitions in terms of their theoretical properties. Now we
show the utility of our approach by applying it to various
threat scenarios and comparing the results to the previous
approaches.
We proceed now to the analysis of the programs presented in
this paper, and compare the results with previous approaches.
Each of the programs is analysed under the following hypoth-
esis.
• The high input A is uniform and publicly-known. Thus
∀a ∈ A pρ(a) = pβ(a) =
1
|A|
.
• The adversary believes that her extra info is accurate, that
is she assumes the correlation shown in Table V. Thus
Γ0 = {0, 2} and Γ1 = {1, 3}.
• The real correlation between A and B is of the form of
the matrix shown in Table V. It is easy to see that the
adversary’s initial belief is therefore ω-accurate.
• B and O are independent.
Fig. 2.
Information ﬂow of PROG C5
the high input. Even though the adversary’s belief does not
aﬀect the quantity of information leaked, it dramatically aﬀects
both the initial and remaining uncertainty. Indeed, as illustrated
by Figure 1, both IUbv and RUbv tend toward inﬁnity as ω
tends toward zero. On the other hand, IUbv and RUbv tend
toward two and one, respectively, as ω tends toward one. In
other words inaccurate beliefs strengthen the security of the
program (by confusing the adversary), whilst accurate beliefs
may weaken it. Thus, a deliberate randomization of the parity
of the high input in order to confuse the adversary is a good
strategy to strengthen the security of this program.
We continue our analysis with PROG C5 of Example 4
which is a slight modiﬁcation of PROG C1. Again IUc =
remaining uncertainty we have RUc = 0.585, RUv = 0.415
IUv = log |A| = 2 and IUbv = − log(cid:0) ω
and RUbv = − log(cid:0) 2ω+1
and ILbv = log(cid:0) 2ω+1
2(1+ω)(cid:1). For the
2(1+ω)(cid:1). Therefore, ILc = 1.415, ILv = 1.585
ω (cid:1). The information ﬂow ascribed by our
approach to this program is illustrated by Figure 2. Unlike
PROG C1, the information leakage of this program can be
arbitrary high when the inaccuracy of the adversary’s belief is
high whilst its remaining uncertainty RUbv remains very low
even for inaccurate beliefs. As already noticed in Example 4,
this program leaves A highly vulnerable of being guessed and
a deliberate padding of A in order to confuse the adversary
is of little help. Note however that RUbv tends toward one,
which is higher than both RUc and RUv, as ω tends toward
zero. It means that highly inaccurate beliefs slightly strengthen
the security of PROG C5.
We proceed with the probabilistic program PROG C2 (see
Example 1). Again IUc = IUv = log |A| = 2 and IUbv =
+ 1
4
+
4
RUc = 1
ω
− log(cid:0) ω
ω+1(cid:2) 1
2(1+ω)(cid:1). For the remaining uncertainty, we have RUv = 1,
4(cid:0) 3 log 3 − log(cid:2)(1 − λ)(1−λ)λλ(cid:3)(cid:1) and RUbv = − log(cid:0) 1
4 max(λ, 1 − λ)(cid:3)(cid:1). The information ﬂow ascribed by
the consensus deﬁnitions is illustrated in Figure 3 and those
of the belief-vulnerability approach in Figures 4 and 5. We
ﬁrst note that in the case of belief’s absence, our approach –
which coincides with the vulnerability one– ascribes the same
information ﬂow quantities to both PROG C2 and PROG C1,
even though they seem to present rather diﬀerent threats.1 The
reason is simply that after her low observation, the adversary
needs on average 21 tries to guess the value of A for both
programs. We also note that the randomisation parameter λ of
PROG C2 has no eﬀect on the vulnerability approach, and has
only a little one on ours when the accuracy of the adversary’s
beliefs tends to 1 and λ to 1
2 . This is due to the fact that these
metrics focus on the single probability that brings greatest risk
and values 0 and 2 of A play symmetric roles with respect
to λ. Finally, comparing Figures 4 and 5 to Figure 1, our
approach allows us to assert that the security performance of
PROG C1 is better than those of PROG C2, except for highly
accurate beliefs. Indeed, the remaining uncertainty of PROG C2
is always less than or equal to 2 whilst those of PROG C1 can
be arbitrary high for highly inaccurate beliefs. In fact, we have
the following result relating the security performance of these
1See the discussion on the last paragraph of page 298 of [34].
Fig. 3. Shannon entropy-based information ﬂow of PROG C2
programs, the randomisation parameter λ, and the accuracy of
the adversary’s beliefs.
Proposition 4: The security performance of PROG C1 is
better than those of PROG C2 if and only if the randomisation
parameter λ of PROG C2 and the accuracy of the adversary’s
beliefs ω satisfy the following relation.
ω ≤
1
3 − max(λ, 1 − λ)
The few elementary examples above illustrate the applica-
bility of our metric to various threats scenarios. In particular,
when it is unavoidable for the attacker to initially have access
to some (potentially inaccurate) information about the high
input, our approach allows to establish the adversary’s beliefs
accuracy limit that is tolerable given a speciﬁc program. For
instance, adversary’s beliefs which are less than or equal to
50% accurate are tolerable for PROG C1, since they happen
to confuse the adversary instead of helping her. Furthermore,
given a collection of programs with the same security ob-
jective, we can design a more complex program that adapts
dynamically to the context of the adversary, when the accuracy
of her beliefs changes. For example, proposition 4 tells us
that it is more secure to use PROG C1 that PROG C2 in a
context where one can assume that the accuracy of the initial
information of the adversary is less than two-ﬁfth; on the other
hand, the contrary holds for higher accurate beliefs.
VI. Conclusion
This paper presents a new approach to quantitative informa-
tion ﬂow that incorporates the attacker’s beliefs in the model
on R´eneyi min entropy. We investigate the impact of such
adversary’s extra knowledge on the security of the secret infor-
mation. Our analyses reveals that inaccurate extra information
tend to confuse the adversary by increasing her uncertainty
about the hidden secret while accurate information may in-
crease dramatically its vulnerability. We showed the strength of
our deﬁnitions both in terms of their theoretical properties and
their utility by applying them to various threat scenarios and
comparing the results to the previous approaches. Our model
allows to identify the levels of accuracy for the adversary’s
[6] Konstantinos Chatzikokolakis, Catuscia Palamidessi, and Prakash Panan-
Inf. Comput., 206(2-
gaden. Anonymity protocols as noisy channels.
4):378–401, 2008.
[7] Konstantinos Chatzikokolakis, Catuscia Palamidessi, and Prakash Panan-
gaden. On the bayes risk in information-hiding protocols. Journal of
Computer Security, 16(5):531–571, 2008.
[8] David Chaum.
The dining cryptographers problem: Unconditional
Journal of Cryptology, 1:65–75,
sender and recipient untraceability.
1988.
[9] David Clark, Sebastian Hunt, and Pasquale Malacaria. Quantitative
analysis of the leakage of conﬁdential data. Electr. Notes Theor. Comput.
Sci., 59(3), 2001.
[10] David Clark, Sebastian Hunt, and Pasquale Malacaria. Quantitative
information ﬂow, relations and polymorphic types. Journal of Logic
and Computation, Special Issue on Lambda-calculus, type theory and
natural language, 18(2):181–199, 2005.
[11] David Clark, Sebastian Hunt, and Pasquale Malacaria. Quantitative
information ﬂow, relations and polymorphic types. J. Log. Comput.,
15(2):181–199, 2005.
[12] David Clark, Sebastian Hunt, and Pasquale Malacaria. A static analysis
for quantifying information ﬂow in a simple imperative language.
Journal of Computer Security, 15(3):321–371, 2007.
[13] Ian Clarke, Oskar Sandberg, Brandon Wiley, and Theodore W. Hong.
Freenet: A distributed anonymous information storage and retrieval
system.
In Designing Privacy Enhancing Technologies, International
Workshop on Design Issues in Anonymity and Unobservability, volume
2009 of Lecture Notes in Computer Science, pages 44–66. Springer,
2000.
[14] Michael R. Clarkson, Andrew C. Myers, and Fred B. Schneider. Belief
in information ﬂow. Journal of Computer Security, 2008. To appear.
Available as Cornell Computer Science Department Technical Report
TR 2007-207.
[15] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory.
John Wiley & Sons, Inc., 1991.
[16] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory
(Wiley Series in Telecommunications and Signal Processing). Wiley-
Interscience, 2006.
´Ulfar Erlingsson and Marco Pistoia, editors. Proceedings of the 2008
Workshop on Programming Languages and Analysis for Security, PLAS
2008, Tucson, AZ, USA, June 8, 2008. ACM, 2008.
[17]
[18] Matthias Franz, Bernd Meyer, and Andreas Pashalidis. Attacking
unlinkability: The importance of context. In Nikita Borisov and Philippe
Golle, editors, Privacy Enhancing Technologies, 7th International Sym-
posium, PET 2007 Ottawa, Canada, June 20-22, 2007, Revised Selected
Papers, volume 4776 of Lecture Notes in Computer Science, pages 1–16.
Springer, 2007.
[19] Joseph Y. Halpern and Kevin R. O’Neill. Anonymity and information
hiding in multiagent systems. Journal of Computer Security, 13(3):483–
512, 2005.
[20] S. Hamadou, C. Palamidessi, V. Sassone, and E. ElSalamouny. Probable
Innocence in the presence of independent knowledge. In To appear in the
Proc. of the sixth International Workshop on Formal Aspects in Security
and Trust (FAST2009), LNCS. Spr.-Ver., 2009.
[21] Dominic Hughes and Vitaly Shmatikov. Information hiding, anonymity
Journal of Computer Security,
and privacy: a modular approach.
12(1):3–36, 2004.
[22] Boris K¨opf and David A. Basin. An information-theoretic model for
adaptive side-channel attacks. In Ning et al. [29], pages 286–296.
[23] Pasquale Malacaria. Assessing security threats of looping constructs.
In Martin Hofmann and Matthias Felleisen, editors, Proceedings of the
34th ACM SIGPLAN-SIGACT Symposium on Principles of Programming
Languages, POPL 2007, Nice, France, January 17-19, 2007, pages 225–
235. ACM, 2007.
[24] Pasquale Malacaria and Han Chen. Lagrange multipliers and maximum
information leakage in diﬀerent observational models. In ´Ulfar Erlings-
son and Marco Pistoia, editor, Proceedings of the 2008 Workshop on
Programming Languages and Analysis for Security (PLAS 2008), pages
135–146, Tucson, AZ, USA, June 8, 2008 2008. ACM.
[25] Pasquale Malacaria and Han Chen. Lagrange multipliers and maximum
information leakage in diﬀerent observational models. In Erlingsson and
Pistoia [17], pages 135–146.
[26] James L. Massey. Guessing and entropy. In In Proceedings of the 1994
IEEE International Symposium on Information Theory, page 204, 1994.
Fig. 4. RUbv of PROG C2
Fig. 5.
ILbv of PROG C2
beliefs which are compatible with the security of a given
program or protocol.
References
[1] Miguel Andr´es, Catuscia Palmidessi, Peter van Rossum, and Geoﬀrey
Smith. Computing the amount of leakage in information-hiding systems.
Technical report, LIX, Ecole Polytechnique, 2009.
[2] Mohit Bhargava and Catuscia Palamidessi. Probabilistic anonymity. In
Mart´ın Abadi and Luca de Alfaro, editors, Proceedings of CONCUR,
volume 3653 of Lecture Notes in Computer Science, pages 171–
185. Springer, 2005. http://www.lix.polytechnique.fr/∼catuscia/papers/
Anonymity/concur.pdf.
[3] Christelle Braun, Konstantinos Chatzikokolakis,
and Catuscia
Palamidessi. Quantitative notions of leakage for one-try attacks.
In Proceedings of the 25th Conference on Mathematical Foundations
of Programming Semantics (MFPS 2009), volume 249 of Electronic
Notes in Theoretical Computer Science, pages 75–91. Elsevier B.V.,
2009.
[4] Konstantinos Chatzikokolakis and Catuscia Palamidessi. Probable inno-
cence revisited. Theoretical Computer Science, 367(1-2):123–138, 2006.
http://www.lix.polytechnique.fr/∼catuscia/papers/Anonymity/tcsPI.pdf.
[5] Konstantinos Chatzikokolakis, Catuscia Palamidessi, and Prakash Panan-
gaden. Anonymity protocols as noisy channels.
Information and
Computation, 206(2–4):378–401, 2008. http://www.lix.polytechnique.
fr/∼catuscia/papers/Anonymity/Channels/full.pdf.
[27] Ira S. Moskowitz, Richard E. Newman, Daniel P. Crepeau, and Allen R.
Miller. Covert channels and anonymizing networks. In Sushil Jajodia,
Pierangela Samarati, and Paul F. Syverson, editors, WPES, pages 79–88.
ACM, 2003.
[28] Ira S. Moskowitz, Richard E. Newman, and Paul F. Syverson. Quasi-
anonymous channels. In IASTED CNIS, pages 126–131, 2003.
[29] Peng Ning, Sabrina De Capitani di Vimercati, and Paul F. Syverson,
editors.
the 2007 ACM Conference on Computer
and Communications Security, CCS 2007, Alexandria, Virginia, USA,
October 28-31, 2007. ACM, 2007.
Proceedings of
[30] Michael K. Reiter and Aviel D. Rubin. Crowds: anonymity for Web
transactions. ACM Transactions on Information and System Security,
1(1):66–92, 1998.
[31] Peter Y. Ryan and Steve Schneider. Modelling and Analysis of Security
Protocols. Addison-Wesley, 2001.
[32] Steve Schneider and Abraham Sidiropoulos. CSP and anonymity.
In
Proc. of the European Symposium on Research in Computer Security
(ESORICS), volume 1146 of Lecture Notes in Computer Science, pages
198–218. Springer, 1996.
[33] Geoﬀrey Smith. Adversaries and information leaks (tutorial). In Gilles
Barthe and C´edric Fournet, editors, Proceedings of the Third Symposium
on Trustworthy Global Computing, volume 4912 of Lecture Notes in
Computer Science, pages 383–400. Springer, 2007.
[34] Geoﬀrey Smith. On the foundations of quantitative information ﬂow. In
Luca De Alfaro, editor, Proceedings of the Twelfth International Con-
ference on Foundations of Software Science and Computation Structures
(FOSSACS 2009), volume 5504 of Lecture Notes in Computer Science,
pages 288–302, York, UK, March 2009 2009. Springer.
[35] Geoﬀrey Smith. On the foundations of quantitative information ﬂow.
In Luca de Alfaro, editor, FOSSACS, volume 5504 of Lecture Notes in
Computer Science, pages 288–302. Springer, 2009.
[36] Paul F. Syverson and Stuart G. Stubblebine. Group principals and the
formalization of anonymity. In World Congress on Formal Methods (1),
pages 814–833, 1999.
[37] P.F. Syverson, D.M. Goldschlag, and M.G. Reed. Anonymous connec-
tions and onion routing. In IEEE Symposium on Security and Privacy,
pages 44–54, Oakland, California, 1997.
[38] Ye Zhu and Riccardo Bettati. Anonymity vs. information leakage in
anonymity systems. In Proc. of ICDCS, pages 514–524. IEEE Computer
Society, 2005.