### Calculation in Example 3

In Example 3, we have:
\[ I_{\text{Ubv}} = -\log(\omega) \]

From Proposition 3, we get:
\[ R_{\text{Ubv}} = -\log\left(\frac{2(1+\omega)}{1+\omega}\right) \]

For all \(\omega\), \(I_{\text{Lbv}} = 1\). Thus, the adversary’s initial knowledge about the parity of \(A\) does not affect the quantity of information leaked by PROG C1. However, the real question is not how much information is leaked by this program, but what the remaining uncertainty represents in terms of security threat.

### Information Flow of PROG C1

1. **Case 1: \(|B| = 1\) and the a priori probability of \(A\) is publicly known.**

2. **Case 2: For each adversary’s initial belief \((p_\rho(a), p_\rho(b | a))\) in Beliefs(A, B) and for each program \(p_\rho(o | a)\), we have:**
   \[ V(A : B) = V(A) \]
   \[ V(A | O : B) = V(A | O) \]

3. **Case 3: For each adversary’s initial belief \((p_\rho(a), p_\rho(b | a))\) in Beliefs(A, B) and for each program \(p_\rho(o | a)\), we have:**
   \[ H_\infty(A | B) = H_\infty(A) \]
   \[ H_\infty(A | O : B) = H_\infty(A | O) \]

**Proof:**

- **(1) \(\Rightarrow\) (2):** If \(|B| = 1\), then \(B\) is a constant and independent of both \(A\) and \(O\). The only possible adversary’s belief, which is the publicly-known a priori distribution of \(A\), is 1-accurate. Therefore:
  \[ V(A : B) = V(A | B) \]
  Since \(A\) and \(B\) are independent:
  \[ V(A | B) = V(A) \]
  Similarly:
  \[ V(A | O : B) = V(A | O) \]

- **(2) \(\Rightarrow\) (1):** (By contradiction) Assume (2) holds and \(|B| > 1\) or the adversary does not know the a priori distribution of \(A\). If \(|B| > 1\), we can create an adversary’s belief that is exactly \(\omega\)-accurate for any \(0 < \omega < 1\). Thus:
  \[ V(A : B) = \omega \cdot V(A | B) \]
  If we choose \(\omega \neq \frac{V(A)}{V(A | B)}\), then \(V(A : B) \neq V(A)\), contradicting our initial hypothesis that (2) holds.

  Now assume \(|B| = 1\) but the adversary does not know the a priori distribution of \(A\). Again, \(B\) is a constant and irrelevant. If \(A\) is not uniformly distributed, it is easy to construct an adversary’s assumed a priori distribution of \(A\) such that \(V(A : B) \neq V(A | B) = V(A)\). If \(A\) is uniformly distributed, we can still create a program such that \(V(A | O : B) \neq V(A | O, B) = V(A | O)\), again contradicting (2).

- **Equivalence (2) \(\Leftrightarrow\) (3):** This follows because the functions \(g(x) = -\log(x)\) and \(g'(x) = 2^{-x}\) are strictly monotone.

### On the Applicability of the Belief-Vulnerability Approach

The previous section establishes the reasonableness of our definitions in terms of their theoretical properties. Now we show the utility of our approach by applying it to various threat scenarios and comparing the results to previous approaches.

#### Analysis of Programs

Each program is analyzed under the following hypotheses:

- **High Input A is Uniform and Publicly Known:** 
  \[ \forall a \in A, \quad p_\rho(a) = p_\beta(a) = \frac{1}{|A|} \]

- **Adversary Believes Her Extra Info is Accurate:** 
  \[ \Gamma_0 = \{0, 2\}, \quad \Gamma_1 = \{1, 3\} \]

- **Real Correlation Between A and B:**
  The real correlation is as shown in Table V, and the adversary’s initial belief is \(\omega\)-accurate.

- **B and O are Independent.**

### Information Flow of PROG C5

Even though the adversary’s belief does not affect the quantity of information leaked, it dramatically affects both the initial and remaining uncertainty. As illustrated in Figure 1, both \(I_{\text{Ubv}}\) and \(R_{\text{Ubv}}\) tend toward infinity as \(\omega\) tends toward zero. On the other hand, \(I_{\text{Ubv}}\) and \(R_{\text{Ubv}}\) tend toward two and one, respectively, as \(\omega\) tends toward one. Inaccurate beliefs strengthen the security of the program by confusing the adversary, while accurate beliefs may weaken it. Deliberate randomization of the parity of the high input to confuse the adversary is a good strategy to strengthen the security of this program.

### Further Analysis

- **PROG C5 (Example 4):**
  \[ IU_c = 2, \quad IU_v = 2, \quad I_{\text{Ubv}} = -\log(\omega) \]
  \[ RU_c = 0.585, \quad RU_v = 0.415, \quad R_{\text{Ubv}} = -\log\left(\frac{2\omega + 1}{2(1 + \omega)}\right) \]
  \[ IL_c = 1.415, \quad IL_v = 1.585, \quad IL_{\text{bv}} = \log\left(\frac{2\omega + 1}{2(1 + \omega)}\right) \]

  Unlike PROG C1, the information leakage of this program can be arbitrarily high when the inaccuracy of the adversary’s belief is high, while its remaining uncertainty \(R_{\text{Ubv}}\) remains very low even for inaccurate beliefs. Highly inaccurate beliefs slightly strengthen the security of PROG C5.

- **PROG C2 (Example 1):**
  \[ IU_c = 2, \quad IU_v = 2, \quad I_{\text{Ubv}} = -\log(\omega) \]
  \[ RU_c = 1, \quad RU_v = 1, \quad R_{\text{Ubv}} = -\log\left(\frac{1}{4 \max(\lambda, 1 - \lambda)}\right) \]

  The information flow ascribed by the consensus definitions is illustrated in Figure 3, and those of the belief-vulnerability approach in Figures 4 and 5. Our approach allows us to assert that the security performance of PROG C1 is better than that of PROG C2, except for highly accurate beliefs.

### Conclusion

This paper presents a new approach to quantitative information flow that incorporates the attacker’s beliefs in the model on Rényi min entropy. We investigate the impact of such adversary’s extra knowledge on the security of the secret information. Our analyses reveal that inaccurate extra information tends to confuse the adversary by increasing her uncertainty about the hidden secret, while accurate information may increase its vulnerability. We showed the strength of our definitions both in terms of their theoretical properties and their utility by applying them to various threat scenarios and comparing the results to previous approaches. Our model allows us to identify the levels of accuracy for the adversary’s beliefs that are compatible with the security of a given program or protocol.