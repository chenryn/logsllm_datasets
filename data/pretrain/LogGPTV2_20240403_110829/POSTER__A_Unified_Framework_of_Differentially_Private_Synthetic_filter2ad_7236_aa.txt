title:POSTER: A Unified Framework of Differentially Private Synthetic
Data Release with Generative Adversarial Network
author:Pei-Hsuan Lu and
Chia-Mu Yu
POSTER: A Unified Framework of Differentially Private
Synthetic Data Release with Generative Adversarial Network
Pei-Hsuan Lu and Chia-Mu Yu
National Chung Hsing University, Taiwan
ABSTRACT
Many differentially private data release solutions have been proposed
for different types of data with the sacrifice of inherent correlation
structure. Here, we propose a unified framework of releasing differ-
entially private data. In particular, our proposed generative adversar-
ial network (GAN)-based framework learns the input distribution,
irrespective of tabular data and graphs, and generates synthetic data
in a differentially private manner. Our preliminary results show the
acceptable utility of the synthetic dataset.
1 INTRODUCTION
Privacy is of paramount importance particularly for machine learn-
ing and data analysis tasks on datasets with individual information.
For example, the Netflix challenge releases the anonymized data,
seeking for the performance improvement. Despite a success for
crowdsourcing, Netflix challenge is also a classic example of how
the wrongly anonymized data expose the individual information.
The above scenario motivates the problem of private data re-
lease, where sensitive dataset needs to be sanitized before released.
A straightforward idea is to release the anonymized data (e.g., k-
anonymity). Unfortunately, these ad hoc anonymization techniques
are designed without the consideration of attacker’s knowledge, and
are subject to linkage attack and homogeneity attack etc.
1.1 Differential Privacy
Differential privacy (DP) is a provable privacy notion. With the
sensitive dataset D to be released, (non-interactive) DP requires
that only the sanitized dataset A(D) can be released, where A is
a randomized algorithm such that the output of A reveals limited
information about any particular record in D. Formally, a randomized
algorithm A satisfies (ϵ, δ)-differential privacy (ϵ, δ)-DP, if for any
two datasets D1 and D2 differing only in one record, and for any
possible output O of A, we have Pr[A(D1) = O] ≤ eϵ Pr[A(D2) =
O] + δ, where two datasets are neighboring if they differ in only one
record.
The Laplace and Gaussian mechanisms achieve certain form of
(ϵ, δ)-DP. In particular, the former aims to release the output of a
numeric function F by adding i.i.d. noise η into each output value
of F. The noise η is sampled from a Laplace distribution Lap(λ)
2λ e−|x |/λ. The latter provides (ϵ, δ)-DP if the
with Pr[η = x] = 1
Gaussian noise N(0, ∆2
F · σ 2) is the
Gaussian distribution with mean 0 and standard deviation ∆F · σ,
δ ≥ exp(−σ 2ϵ2/2)/1.25, and ϵ < 1.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
CCS ’17, October 30-November 3, 2017, Dallas, TX, USA
© 2017 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-4946-8/17/10.
https://doi.org/10.1145/3133956.3138823
F · σ 2) is applied, where N(0, ∆2
Given a differentially private algorithm, the composition of the
algorithm and post-processing steps is still differentially private.
The most useful is the sequential composition theorem, In essence,
given a (ϵ1, δ1)-DP algorithm A1 and (ϵ2, δ2)-DP algorithm A2, the
A2(A1(D), D) satisfies (ϵ1 + ϵ2, δ1 + δ2)-DP. A special case is that
given a (ϵ1, δ1)-DP algorithm A1 and the post-processing A2, the
A2(A1(D)) is still (ϵ1, δ1)-DP. Namely, the post-processing without
accessing D does not consume privacy budget ϵ.
1.2 Recent Work on Private Data Release
Many efforts have been devoted to developing techniques for private
data release. A useful technique in publishing tabular data is to first
learn the inherent data distribution from original dataset D. Then, the
data owner generates and releases synthetic dataset D′ via random
sampling from the data distribution. Since D′ and D share the same
data distribution, the analysis results are supposed to be similar.
PrivBayes [6] constructs a Bayesian network (BN) to learn the data
distribution. After adding noises to BN, data owner performs random
sampling from the noisy data distribution. A subsequent work, JTree
[3], follows the similar strategy.
On the other hand, different techniques are used in releasing graph
statistics. Consider the case of publishing node degree distribution.
A common approach is to construct a projective graph ˆG from the
original graph G. After that, a differentially private histogram for
node degree distribution is then released [4, 5].
Unfortunately, we face two research challenges. (C1) First, for
high-dimensional dataset with the large domain size of each attribute
and with complex correlation among attributes, modeling the data
distribution consumes considerable time and even computationally
infeasible. Very often, the design choice is to keep the pairwise cor-
relation among attributes in the learned data distribution. However,
by doing so, the data user may gain inaccurate result when issuing
multidimensional query to synthetic data. (C2) Second, while cur-
rently different DP techniques need to be used on different types of
data, the lack of a unified framework for generating differentially
private synthetic data leads to the increased complexity in managing
the risk of information disclosure. In this paper, we propose to use
generative adversarial network as a foundation of such a unified
framework.
1.3 DNN and GAN
Deep neural networks (DNN) composed of multiple interconnected
layers, each of which contains a number of neurons, have proved
the remarkable capability on various machine learning tasks. In
essence, DNN, as a parameterized function, extracts the hidden
structure behind the data. The DNN with varying parameters can be
trained to fit any given finite set of input/output examples. To train
a DNN, we define a loss function L that represents the mismatch
between the data and DNN output. A common setting of the loss
is L(θ) = 1
i L(θ, xi) with the training examples {x1, . . . , xn}.
n

PosterCCS’17, October 30-November 3, 2017, Dallas, TX, USA2547R1×m}, where D′ denotes the synthetic dataset and DP-DNN(a) de-
notes the prediction made by DP-DNN on a. The above procedures
are repeated until the number of records of D′ reaches n. The above
sampling procedures can be thought of as a post-processing and do
not consume privacy budget ϵ. The above sampling procedures can
also be optimized. First, with the knowledge of the value ranges of
attributes, one can guarantee that the random samples will not be
significantly deviated from the population, if samples are randomly
drawn from a limited value range. Second, with the knowledge of
proportions of each label, when the inclusion of [a DP-DNN(a)]
results in a significant deviation from the proportion of DP-DNN(a)
in D, [a DP-DNN(a)] is dropped. Figures 1 and 2b show the experi-
ment results of DP-DNN.
The dataset for all the experiment results in Secs. 2.2∼2.3 are
Wine Data Set2, and each result in Figures 1∼5 is the average of ten
independent experiments. Classification accuracy quantifies the sim-
ilarity between how classification work on D and D′, Bhattacharyya
distance measures the dissimilarity between D and D′, and cor-
relation matrix in Figure 2 visualizes whether D′ preserves the
correlation structure in D.
Though Figure 1 shows promising results, the correlation matrix,
as shown in Figure 2, shows the weakness of DP-DNN. More pre-
cisely, due to the independent sampling, by comparing Figure 2a
and Figure 2b, one can easily find that the synthetic dataset does not
preserve the correlation among attributes in D.
Training aims to find θ such that the L(θ) is minimized. However,
due to the complex structure of DNN, L is usually non-convex and
difficult to minimize. In practice, one usually resorts to stochastic
gradient decent (SGD) algorithm to minimize L.
Generative models, as opposed to discrimitive models (e.g., DNN),
allow one to generate samples from the data distribution. Generative
adversarial network (GAN) is composed of two networks, generator
and discriminator. The former seeks to create synthetic data satisfy-
ing the data distribution, while the latter determines whether the data