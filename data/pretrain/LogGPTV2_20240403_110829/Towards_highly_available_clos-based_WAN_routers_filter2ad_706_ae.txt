tries (in the last two columns) from 192 to 64 for a 128-port router
and from 640 to 346 for a 512-port router in the absence of failures
by assigning weights sparingly.
5.5 Impact of Optimizations
The Importance of Routing Optimizations. Minimal-upflow
wiring, together with early forwarding, alone does not provide high
resilience; our WCMP routing is also necessary. To demonstrate this,
we conduct an experiment on a 128-port router with no failures. For
minimal-upflow wiring, we configure routing tables to use early
forwarding when possible, but use ECMP routing to split traffic
equally to L2 switches for upflow traffic and egress ports for early
forwarding traffic. Figure 17 shows the resulting effective capacity
across all trunk sets. Our minimal-upflow wiring together with
compact routing tables achieves an effective capacity of 1 across all
trunk sets (not shown in the figure). However, the simpler routing
technique that uses ECMP is able to achieve full capacity for only
5 of the trunk sets, whose upflow is zero, with minimal upflow
wiring, and much lower effective capacity with random wiring.
Canonicalization. Canonicalizing failure patterns often reduces
the number of link failures by orders of magnitude. Figure 18 shows
the average numbers of canonical forms across our 34 trunk sets
434
Figure 17: Effective capacity constrained by ECMP routing under
no failure.
Figure 18: The benefits of canonicalization.
Figure 19: Upflow in a 512-port router with 5 trunks.
Trunk Set
(8, 8, 56, 56)
(24, 24, 32, 48)
(96, 96, 96, 112, 112)
(96, 96, 96, 96, 128)
Optimal Approximation
9 min.
38 min.
0.21 sec.
0.16 sec.
0.88 sec.
0.63 sec.
–
–
Table 3: Micro benchmark of wiring approaches.
under different numbers of link failures (y-axis is in log scale). With
7 failures, the reduction is a 5 order of magnitude, from 1011 to 106.
Upflow Approximation. For the 128-port router, our upflow ap-
proximation (§2) matches the optimal upflow computed using the
formulation of Equation 5 for all but 4 of the trunk sets (Figure 11).
For those 4 cases, the differences are extremely small. To demon-
strate that our approximation helps compute upflow with larger
WAN routers and trunk sets, Figure 19 shows the upflow across all 5-
trunk trunk sets (480 such combinations) for a 512-port WAN router.
The resulting upflow is 20-30× lower than the baseline wiring.
Finally, our approximation noticeably speeds up upflow computa-
tion (Table 3). For The 4-trunk cases, the formulation of Equation 5
can take up to 38 min to find the minimal upflow using a multi-core
desktop (20 cores 2 Intel Xeon CPU E5-2650 @ 2.30GHz), while
our approximation can compute this in a fraction of second. It can
also compute upflow for some 5 trunk configurations when the
formulation of Equation 5 does not even complete.
Large providers may have resources to compute minimal-upflow
wiring using Equation 5. When using a cluster of say 500 multicore
machines, the computation for 4-trunk trunk sets can complete
in 10s of minutes because the number of extreme traffic matrices
range from 200 to about a 1000 (Figure 20).
Computing Effective Capacity. Computing effective capacity of
a link failure pattern takes 30 seconds on a single core of a multi-
core machine. This means that the total time largely depends on the
Trunk Sets0.00.51.0Effective capacityOptimalRandom1234567Number of fail links100103106109Number of patternsCanonical formsAll patternsTrunk sets0200400Normalized rateUpflow: 512-port WAN router with 5 trunksBaselineApproximationSIGCOMM ’19, August 19–23, 2019, Beijing, China
Sucha Supittayapornpong, Barath Raghavan, and Ramesh Govindan
including side-links between WAN routers at a site to increase
resilience. Our work can be directly applied to these WAN routers
to further improve overall resilience.
Prior work [40] formulated a non-linear integer optimization to
minimize over-subscription in asymmetric topologies while fitting
WCMP entries within table limit constraints. Our work consid-
ers a different problem: finding WCMP weights for non-blocking
behavior of a WAN router.
Finally, our formulations and proof techniques draw inspiration
from ideas from robust validation [10], robust optimization [5, 7],
linear programming [8], and convex analysis [6, 9]. Robust valida-
tion [10] approximates solutions of max-min problems in robust
optimization [5, 7]. Instead, our work finds the exact solutions by
leveraging the convex polytope property of traffic matrices.
7 DISCUSSION
External Link Failures. External links can fail in practice. Our
work extends easily to cope with such failure in two different sit-
uations. A total trunk failure, in which every link of a trunk fails,
neither decreases effective capacity nor changes internal routing.
Therefore, our approach applies directly. However, a partial trunk
failure, where some links in a trunk fail, requires recalculation of
effective capacity and routing (§A.10), which can be pre-computed.
Non-Uniform Internal Path Length. Because some incoming
traffic on a trunk can be early forwarded, flows within a trunk may
experience slightly different latencies. However, packets within
a flow do not experience re-ordering because WCMP hashes all
packets in a flow to the same path.
Cell-Based Routing. Some multi-chip routers, such as Star-
dust [41] designed from Broadcom Jericho2 [22] and Ramon [23]
chips, use cell-based routing. In this approach, the router’s ingress
ports divide packets into fixed size cells and spray them uniformly
across the fabric, re-assembling the packet at the egress ports. For
such routers, our optimal wiring can increase effective capacity
(e.g., over random wiring in Figure 17) but, because it is yet unclear
how to do weighted forwarding in these fabrics, it remains an open
question how to compute WCMP-like forwarding tables for them.
8 CONCLUSION
This paper discusses an approach to optimizing trunk wiring and
forwarding weights to increase the resilience of WAN routers in
large content- and cloud-provider networks. Based on the observa-
tion that early forwarding in L2 switches can create excess internal
capacity in the WAN router, enabling it to be more resilient to
internal failures, we formulate an efficient optimization to derive
the minimal-upflow trunk wiring. Then, given this wiring and an
arbitrary failure pattern, we devise an efficient optimization to
compute the effective capacity under failure, and finally describe a
technique to compute compact forwarding tables that can ensure
non-blocking behavior subject to this effective capacity. Our evalu-
ations show that our approach can greatly increase the resilience
of WAN routers without sacrificing a precious resource in today’s
switches, routing tables.
Acknowledgements. We thank Nathan Bronson, the SIG-
COMM reviewers, and Subhasree Mandal for their comments and
feedback that improved the paper greatly.
Figure 20: Extreme traffic matrices of a 128-port router.
Figure 21: Performance gap of the approximate routing.
number of failure patterns. We can estimate this time from Figure 18.
For example, finding effective capacities of all combinations of 7
link failures would take about 5.8 hours on a single 24 core machine
in the presence of canonicalization. Without the optimization, it
would take about 66 years.
Computing Routing Tables. Calculating a routing table is also
well within the compute power available to cloud and content
providers. It takes at most 2 minutes for a given trunk wiring and a
2-link failure pattern for a 128-port WAN router, across all possible
trunk sets and 2-link failure combinations.
Routing Approximation. For the 512-port router, we were unable
to find optimal routing tables using our compute cluster. However,
our approximation formulation (§4) completed in a few minutes
for this size of router. Figure 21 shows the optimality gap for our
approximation for a 128-port router. It reports, for each trunk set,
the maximum number of L2 switch failures which preserve full
capacity. We observe that the approximation underestimates this
quantity by at most 2, relative to the optimal.
6 RELATED WORK
Prior work has considered fault tolerance in multi-stage switching
networks [2] (and references therein). This line of work considers
interconnection networks where, unlike our setting, (a) packets
traverse the network in one direction from the first stage and exit
at the last stage so early forwarding opportunities do not exist,
and (b) do not incorporate trunks. Since early forwarding is not
possible, designers over-provision the networks [1, 11, 14, 15, 26,
30, 35, 36], by replicating stages, links, or the entire network. Our
work achieves fault tolerance without over-provisioning.
Our work might apply to FatTrees [4] and F10 [27]. The latter
focuses on limiting the blast radius of failures in datacenters by
carefully striping a Clos; it is complementary to our work that seeks
to improve failure resilience by adapting trunk wiring and routing
to provide non-blocking behavior in the presence of failures. We do
not know of WAN routers that incorporate other topology designs2
proposed for datacenters, such as FatClique [39], random graphs:
Jellyfish [37], Xpander [38], and server-centric designs: BCube [17],
DCell [18], so our work focuses on Clos-based WAN routers.
Our work draws inspiration from Google’s original B4 net-
work [25] and more recent incarnation [19]. The B4 network
uses various complementary techniques to improve availability
2In general, the topology of a WAN router can be arbitrary, but it must have non-
blocking behavior under some routing scheme.
435
Trunk sets05001000Number of matricesTrunk sets02468Number ofL2 failuresOptimalApproximationTowards Highly Available Clos-Based WAN Routers
SIGCOMM ’19, August 19–23, 2019, Beijing, China
REFERENCES
[1] Adams and Siegel. 1982. The Extra Stage Cube: A Fault-Tolerant Interconnection
Network for Supersystems. IEEE Trans. Comput. C-31, 5 (May 1982), 443–454.
https://doi.org/10.1109/TC.1982.1676021
[2] George B. Adams, III, Dharma P. Agrawal, and Howard Jay Siegel. 1994. Inter-
connection Networks for High-performance Parallel Computers. IEEE Com-
puter Society Press, Los Alamitos, CA, USA, Chapter A Survey and Compar-
ison of Fault-tolerant Multistage Interconnection Networks, 654–667. http:
//dl.acm.org/citation.cfm?id=201173.201276
[3] Ravindra K. Ahuja, Thomas L. Magnanti, and James B. Orlin. 1993. Network
Flows: Theory, Algorithms, and Applications. Prentice-Hall, Inc., Upper Saddle
River, NJ, USA.
[4] Mohammad Al-Fares, Alexander Loukissas, and Amin Vahdat. 2008. A Scalable,
Commodity Data Center Network Architecture. SIGCOMM Comput. Commun.
Rev. 38, 4 (Aug. 2008), 63–74. https://doi.org/10.1145/1402946.1402967
[5] A. Ben-Tal, L. El Ghaoui, and A.S. Nemirovski. 2009. Robust Optimization. Prince-
ton University Press.
[6] D. P. Bertsekas, A. Nedić, and A. E. Ozdaglar. 2003. Convex Analysis and Opti-
mization. Athena Scientific.
[7] D. Bertsimas, D. Brown, and C. Caramanis. 2011. Theory and Applications of
Robust Optimization. SIAM Rev. 53, 3 (2011), 464–501. https://doi.org/10.1137/
080734510 arXiv:https://doi.org/10.1137/080734510
[8] Dimitris Bertsimas and John Tsitsiklis. 1997. Introduction to Linear Optimization
[9] Stephen Boyd and Lieven Vandenberghe. 2004. Convex Optimization. Cambridge
[10] Yiyang Chang, Sanjay Rao, and Mohit Tawarmalani. 2017. Robust Validation
of Network Designs Under Uncertain Demands and Failures. In Proceedings of
the 14th USENIX Conference on Networked Systems Design and Implementation
(NSDI’17). USENIX Association, Berkeley, CA, USA, 347–362. http://dl.acm.org/
citation.cfm?id=3154630.3154658
[11] L. Ciminiera and A. Serra. 1986. A Connecting Network with Fault Tolerance
Capabilities. IEEE Trans. Comput. C-35, 6 (Jun. 1986), 578–580. https://doi.org/
10.1109/TC.1986.5009436
[12] C. Clos. 1953. A study of non-blocking switching networks. The Bell System
[13] Shagnik Das. [n. d.]. A brief note on estimates of binomial coefficients. http:
Technical Journal 32, 2 (Mar. 1953).
//page.mi.fu-berlin.de/shagnik/notes/binomials.pdf
University Press, New York, NY, USA.
(1st ed.). Athena Scientific.
[14] Daniel M. Dias and J. Robert Jump. 1982. Augmented and pruned n log n mul-
tistaged networks: topology and performance. In International Conference on
Parallel Processing, ICPP’82, August 24-27, 1982, Bellaire, Michigan, USA. 10–12.
[15] C. C. Fan and J. Bruck. 2000. Tolerating multiple faults in multistage interconnec-
tion networks with minimal extra stages. IEEE Trans. Comput. 49, 9 (Sep. 2000),
998–1004. https://doi.org/10.1109/12.869334
[16] Komei Fukuda. [n. d.]. cdd and cddplus Homepage. https://www.inf.ethz.ch/
personal/fukudak/cdd_home/
[17] Chuanxiong Guo, Guohan Lu, Dan Li, Haitao Wu, Xuan Zhang, Yunfeng Shi, Chen
Tian, Yongguang Zhang, and Songwu Lu. 2009. BCube: A High Performance,
Server-centric Network Architecture for Modular Data Centers. SIGCOMM
Comput. Commun. Rev. 39, 4 (Aug. 2009), 63–74. https://doi.org/10.1145/1594977.
1592577
[18] Chuanxiong Guo, Haitao Wu, Kun Tan, Lei Shi, Yongguang Zhang, and Songwu
Lu. 2008. Dcell: A Scalable and Fault-tolerant Network Structure for Data Centers.
SIGCOMM Comput. Commun. Rev. 38, 4 (Aug. 2008), 75–86. https://doi.org/10.
1145/1402946.1402968
[19] Chi-Yao Hong, Subhasree Mandal, Mohammad Al-Fares, Min Zhu, Richard Alimi,
Kondapa Naidu B., Chandan Bhagat, Sourabh Jain, Jay Kaimal, Shiyu Liang,
Kirill Mendelev, Steve Padgett, Faro Rabe, Saikat Ray, Malveeka Tewari, Matt
Tierney, Monika Zahn, Jonathan Zolla, Joon Ong, and Amin Vahdat. 2018. B4
and After: Managing Hierarchy, Partitioning, and Asymmetry for Availability
and e in Google’s Software-defined WAN. In Proceedings of the 2018 Conference
of the ACM Special Interest Group on Data Communication (SIGCOMM’18). ACM,
New York, NY, USA, 74–87. https://doi.org/10.1145/3230543.3230545
[20] C. Hopps. 2000. Analysis of an Equal-Cost Multi-Path Algorithm.
[21] Arista Networks Inc. [n. d.]. Arista 7050X3 Series Switch Architecture. https://
www.arista.com/assets/data/pdf/Whitepapers/7050X3_Architecture_WP.pdf. Ac-
cessed: 2019-1-30.
[22] Broadcom Inc. [n. d.]. BCM88690: 10 Tb/s StrataDNX Jericho2 Ethernet Switch
Series. https://www.broadcom.com/products/ethernet-connectivity/switching/
stratadnx/bcm88690. Accessed: 2019-6-13.
[23] Broadcom Inc. [n. d.]. BCM88790 Scalable Fabric Element 9.6 Tbps Self-Routing
Switching Element. https://www.broadcom.com/products/ethernet-connectivity/
switching/stratadnx/bcm88790. Accessed: 2019-6-13.
[24] Broadcom Inc. [n. d.]. High-Capacity StrataXGS Trident 3 Ethernet Switch
Series. https://www.broadcom.com/products/ethernet-connectivity/switching/
strataxgs/bcm56870-series/. Accessed: 2019-1-30.
436
[25] Sushant Jain, Alok Kumar, Subhasree Mandal, Joon Ong, Leon Poutievski, Arjun
Singh, Subbaiah Venkata, Jim Wanderer, Junlan Zhou, Min Zhu, Jon Zolla, Urs
Hölzle, Stephen Stuart, and Amin Vahdat. 2013. B4: Experience with a Globally-
deployed Software Defined Wan. SIGCOMM Comput. Commun. Rev. 43, 4 (Aug.
2013), 3–14. https://doi.org/10.1145/2534169.2486019
[26] Menkae Jeng and Howard Jay Siegel. 1986. A Fault-Tolerant Multistage Intercon-
nection Network for Multiprocessor Systems Using Dynamic Redundancy.. In
ICDCS. 70–77.
[27] Vincent Liu, Daniel Halperin, Arvind Krishnamurthy, and Thomas Anderson.
2013. F10: A Fault-tolerant Engineered Network. In Proceedings of the 10th USENIX
Conference on Networked Systems Design and Implementation (NSDI’13). USENIX
Association, Berkeley, CA, USA, 399–412. http://dl.acm.org/citation.cfm?id=
2482626.2482665
[28] Gurobi Optimization LLC. [n. d.]. The Fastest Mathematical Programming Solver.
http://www.gurobi.com/
[29] Brendan D. McKay. 1981. Practical Graph Isomorphism.
[30] Robert J. McMillen and Howard Jay Siegel. 1982. Performance and Fault Tolerance
Improvements in the Inverse Augmented Data Manipulator Network. SIGARCH
Comput. Archit. News 10, 3 (Apr. 1982), 63–72. http://dl.acm.org/citation.cfm?
id=1067649.801714
[31] P. McMullen. 1970. The maximum numbers of faces of a convex polytope.
Mathematika 17, 2 (1970), 179–184. https://doi.org/10.1112/S0025579300002850
Equal Cost Multipath Load Sharing - Hard-
ware ECMP. https://docs.cumulusnetworks.com/display/DOCS/Equal+Cost+
Multipath+Load+Sharing+-+Hardware+ECMP
[32] Cumulus Networks. [n. d.].
[33] The Open MPI Project. [n. d.]. Open MPI: Open Source High Performance
Computing. https://www.open-mpi.org/
[34] Barath Raghavan, Subhasree Mandal, Mohammad Alfares, John McCullough, Fei
Ye, Min Zhu, and Aravind Ravisankar. 2016. High performance and resilience in
wide area networking. https://patents.google.com/patent/US9369408B1/en US
Patent 9369408B1.
[35] S.M. Reddy and V.P. Kumar. 1984. On Fault-Tolerant Multistage Interconnection
[36] C S. Raghavendra and A Varma. 1984.
Networks. In International Conference of Parallel Processing. 155–164.
INDRA: A Class of Interconnection
Networks with Redundant Paths. Proceedings of Real-Time Systems Symposium,
153–164.
[37] Ankit Singla, Chi-Yao Hong, Lucian Popa, and P. Brighten Godfrey. 2012. Jellyfish:
Networking Data Centers Randomly. In Proceedings of the 9th USENIX Conference
on Networked Systems Design and Implementation (NSDI’12). USENIX Association,
Berkeley, CA, USA, 17–17. http://dl.acm.org/citation.cfm?id=2228298.2228322
[38] Asaf Valadarsky, Michael Dinitz, and Michael Schapira. 2015. Xpander: Unveiling
the Secrets of High-Performance Datacenters. In Proceedings of the 14th ACM
Workshop on Hot Topics in Networks (HotNets-XIV). ACM, New York, NY, USA,
Article 16, 7 pages. https://doi.org/10.1145/2834050.2834059
[39] Mingyang Zhang, Radhika Niranjan Mysore, Sucha Supittayapornpong, and
Ramesh Govindan. 2019. Understanding Lifecycle Management Complexity of
Datacenter Topologies. In Proceedings of the 19th USENIX Conference on Networked
Systems Design and Implementation (NSDI’19). USENIX Association, Berkeley,
CA, USA.