title:Constructing Multi-Layered Boundary to Defend Against Intrusive Anomalies:
An Autonomic Detection Coordinator
author:Zonghua Zhang and
Hong Shen
Constructing Multi-Layered Boundary to Defend Against Intrusive Anomalies:
An Autonomic Detection Coordinator
Zonghua Zhang, Hong Shen
School of Information Science
Japan Advanced Institute of Science and Technology
1-1, Asahidai, Nomi, Ishikawa 923-1211, Japan
{zonghua, shen}@jaist.ac.jp
Abstract
An autonomic detection coordinator is developed in this
paper, which constructs a multi-layered boundary to defend
against host-based intrusive anomalies by correlating sev-
eral observation-speciﬁc anomaly detectors. Two key ob-
servations facilitate the model formulation: First, different
anomaly detectors have different detection coverage and
blind spots; Second, diverse operating environments pro-
vide different kinds of information to reveal anomalies. Af-
ter formulating the cooperation between basic detectors as
a partially observable Markov decision process, a policy-
gradient reinforcement learning algorithm is applied to
search in an optimal cooperation manner, with the objec-
tive to achieve broader detection coverage and fewer false
alerts. Furthermore, the coordinator’s behavior can be ad-
justed easily by setting a reward signal to meet the diverse
demands of changing system situations. A preliminary ex-
periment is implemented, together with some comparative
studies, to demonstrate the coordinator’s performance in
terms of admitted criteria.
1 Introduction
Most existing anomaly detectors (AD) intend to char-
acterize a speciﬁc operating environment sufﬁciently well,
with an expected false alert rate to be determined a pri-
ori, and most of them attempt to detect individual instan-
tiations rather than classes of attacks, which limits their
broader application. Usually, the ﬁrst stage in establish-
ing an anomaly detection model is to select the observ-
able subjects (e.g., system call traces, network packet logs,
command line strings), and construct the operating envi-
ronments to characterize system normality. Due to their
speciﬁc characteristics, different observations have differ-
ent capabilities for characterizing system normality, and
thus the constructed operating environment might limit their
ability to discover some hidden intrusive attempts. For ex-
ample, some attacks might be detected in system call stacks,
whilst escaping from system call traces, and these phenom-
ena also exist even for the same anomaly detection model.
In this paper, we pay more attention to the effects of ob-
servations than to the speciﬁc detection techniques them-
selves. To achieve better performance in terms of broader
detection coverage, higher detection accuracy, and fewer
false alerts, we intend to develop a model to combine sev-
eral observation-speciﬁc ADs with different properties. The
basic assumption to support our work is that various oper-
ating environment could provide different kinds of normal
and anomalous information for system characterization, and
thus different ADs could create a consensus on the identiﬁ-
cation of anomalies, while intersecting their judgement on
false alerts.
Another motivation for combining different ADs is to an-
alyze and capture the “root-cause” of attack variants. It is
well known that an attack might have different behaviors,
and leave traces in various manners for the same system
vulnerability. The combination of different ADs is expected
to abstract speciﬁc or concrete behaviors sufﬁciently well
to detect families of attacks rather than individual instanti-
ations, thereby allowing for the detection of all the attack
variants that attempt to exploit the same weakness. With
those objectives in mind, we formulate several state-of-the-
art ADs as a multi-agent Partially Markov Decision Pro-
cess (POMDP). The proposed model, called an autonomic
detection coordinator (ADC), is expected to work in a dy-
namic manner to ﬁnd an optimal combination to adapt to the
changing system situations with satisfactory performance
in terms of predeﬁned evaluation metrics. Moreover, the
model could be easily extended to more complex situations,
such as a network with distributed ADs, sensor networks,
or wireless networks. Also, the probabilistic nature of the
model guarantees it will work in a tolerant manner. Even
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:53:20 UTC from IEEE Xplore.  Restrictions apply. 
though one of the individual AD might fail to work properly,
ADC can still collect enough intelligence from the other
ADs, and make a correct decision based on their consen-
sus. Therefore, adaptability, scalability, and dependability
enrich the functionality of ADC signiﬁcantly, compared to
a single AD.
The paper is organized as follows: Section 2 reviews
some related work. In Section 3, we formulate our model
as a POMDP, and give a speciﬁc solution. In section 4, we
implement the experiments and discuss the results; further
issues are also discussed. A conclusion about our work is
given in the last section.
2 Related Work
Han et al. [9] combined multiple host-based detection
models using a decision tree to lower false alert rate with
good performance on the detection accuracy. However,
their detection models were established on the same layer
(i.e., audit events and some related parameters and at-
tributes), although the utilized information was different.
Moreover, the decision tree essentially is a static approach,
causing the model to be lack of adaptability, and the per-
formance would deteriorate dramatically with the increas-
ing number of elemental ADs.
In addition, based on the
observation that the human experts always attempt to de-
sign “root-cause” signatures that “combine” different attack
characteristics in order to attain low false alarm rates and
high attack detection rates, Giacinto et al. [8] proposed an
approach to network intrusion detection by fusing multiple
classiﬁers. In this work, intrusion detection essentially was
formulated as a pattern recognition problem, and more ef-
forts were paid to the comparative studies on the fusion ap-
proaches rather than the intrusion detection problem itself,
speciﬁc analysis on the intrusion detection performance was
not the emphasis either.
In addition, many cooperative intrusion detection models
have been proposed to countermine distributed attacks by
leveraging the information collected from distributed hosts,
such as [15, 16], or to improve the accuracy of alarms by
correlating different kinds of observations of multiple het-
erogeneous sensors [10, 14]. In those models, local agents
or sensors are used to collect interesting events (from audit
data, network packets, etc.) or alarm reports, and the dis-
tributed architectures provide various communication meth-
ods to exchange the locally detection information. Com-
pared with the existing works, even though starting with
similar motivation, our work focuses more on the anomaly
detection model itself for correlating the anomaly reports
from independent anomaly surrogates, and searches an op-
timal correlation on the system state from learning instead
of relying on a predeﬁned set of rules or events. The empha-
sis is on the the analysis of the model’s anticipated behavior
from a high level viewpoint, and the effects of the comple-
mentary correlation of different observations on revealing
more anomalies.
In general, we envision a framework in which several
levels of data analysis are used as the basis to be combined
to yield a single but effective system normality characteri-
zation. We envision further an approach in which anomaly
detection models are built on a fundamental understanding
of their operating environments, and have the adaptability to
respond to the diverse demands of various system situations.
The hope is that a collection of simple surrogates based on
speciﬁc observable subjects can cooperate and evolve into
generic models with broader anomaly detection coverage
and less false alerts.
3 Model Formulation
With the motivations presented in the section 1 and based
on an observation-centric analysis on four typical ADs, we
formulate our autonomic detection coordinator (ADC) as a
POMDP model. A policy-gradient reinforcement learning
algorithm is then used to search the optimal combination
strategy based on the formulation.
3.1 Selection of Basic ADs
The selection of the individual ADs mainly takes into ac-
count following considerations:
1.) the trade-off between the computational cost and detec-
tion performance,
2.) since we use a host with UNIX OS as the experimental
scenario, all the individual ADs are host-based, and work in
different environments, or take advantage of different prop-
erties of the same observation,
3.) the population of the ADs should not be too large for the
easy of control and analysis.
Minimum Cross Entropy (MCE) based on the occur-
rence frequencies of events is selected as one AD to operate
with shell command lines; One-order Markov Chain is se-
lected to be operated with Audit Events; Sequence, time-
delay, embedding AD (STIDE) and K-Nearest-Neighbor
(KNN) are selected as two elemental ADs to work with
the system calls of privilege programs, but the properties
they utilize are different. Table 1 shows the simple compar-
ison between those four selected ADs, while the detailed
description can be found in their respective references.
3.2 A General Formulation
Assume that each AD is an autonomous entity working
in its own environment with uncertain perceptions, actions,
and feedback, and each of them takes the action indepen-
dently according to its local parameterized policy. our in-
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:53:20 UTC from IEEE Xplore.  Restrictions apply. 
Table 1. A Simple Comparison between Four Slected ADs
Anomaly Detectors
Observation
Main Property
Detection Cost
MCE [23]
Shell Command Lines
Markov Chains [21, 22]
Audit Events
STIDE [7]
KNN [11]
Local Ordering of System calls
Frequency of System calls
(cid:73)
Frequency Ordering
(cid:73)
(cid:73)
(cid:73)
(cid:73)
(cid:82)((cid:81) (cid:87) (cid:112)
2)
(cid:82)((cid:81) (cid:87) (cid:79))
(cid:82)((cid:81) (cid:87) ((cid:79) (cid:51) (cid:122) + 1))
(cid:82)((cid:81) )
’(cid:122)’ is the predeﬁned window size, ’(cid:112)’ is the number of unique event
tegrated detection model ADC attempts to combine those
independent entities in an optimal way, with the anticipated
behavior to suppress false alerts and achieve broader detec-
tion coverage. It is worth noting that our main concern is
the actions of independent ADs, rather than their inner de-
tection mechanisms. The independent AD decides whether
the ongoing activity is legal or malicious, and since each
of them only works in its own environment, the true sys-
tem state can only be indirectly observed through their re-
spective detection measurement, and they must maintain the
estimates of the true system state, therefore, the detection
problem is partially observable for the entire system. Fur-
thermore, the decision process is a Markov process, because
the next state of the system is dependent only upon the cur-
rent state and the previous decision. Thus, a partially ob-
servable Markov decision process is formulated here.
Formally, a POMDP contains several key parameters [1]:
• a ﬁnite state space of (cid:113) distinct states, (cid:86) = {1(cid:62) 2(cid:62) (cid:61)(cid:61)(cid:61)(cid:62) (cid:113)}
of the system
• a control space of (cid:112) distinct actions, (cid:88) =
{1(cid:62) 2(cid:62) (cid:61)(cid:61)(cid:61)(cid:62) (cid:112)} that are available to the detection policy
• an observation space of (cid:116) distinct observations, (cid:93) =
{1(cid:62) 2(cid:62) (cid:61)(cid:61)(cid:61)(cid:62) (cid:116)}
• (cid:100) (possibly stochastic) reward (cid:117)((cid:108)) (cid:53) R for each state
Speciﬁcally, the interactions between an independent
AD and its operating environment includes a sequence of
decision stages:
1. At time step (cid:108) (discrete), the system in a particular
state (cid:118)(cid:108) (cid:53) (cid:86), and the underlying state emits an observation
(cid:125)(cid:108) (cid:53) (cid:93) to the AD according to a probability distribution
(cid:26)((cid:118)(cid:108)) over observation vectors.
2. The AD chooses an action (cid:120)(cid:108) (cid:53) (cid:88) using a randomized
policy, based on a probability distribution (cid:25)((cid:125)(cid:108)) over ac-
tions, with known (cid:125)(cid:108).
3. (cid:120)(cid:108) determines a stochastic matrix (cid:83) (cid:117)((cid:120)(cid:108)) = [(cid:115)(cid:108)(cid:109)((cid:120)(cid:108))],
(cid:115)(cid:108)(cid:109)((cid:120)(cid:108)) is the probability of making a transition from state
(cid:118)(cid:108) to state (cid:118)(cid:109) under action (cid:120)(cid:108).
4. In every system state, the AD receives a reward signal
(cid:117)(cid:108), while its aim is to choose a policy so as to maximize the
long-term average of reward (E is the expectation operator),
(cid:20) := lim
(cid:87)(cid:36)(cid:52) E[
1
(cid:87)
(cid:117)(cid:108)](cid:61)
(1)
(cid:87)X
(cid:108)=1
The above decision process shows that at each time step
the AD sees only the observations (cid:125)(cid:108) and the reward (cid:117)((cid:108)),
while it has no knowledge of the underlying state space,
how the actions affect the evolution of states, how the re-
ward signals depend on the states, or even how the obser-
vations depend on the states. From another viewpoint, to
each randomized policy (cid:25)(·) and observation distribution
(cid:26)(·), the Markov chains for state transitions (cid:118)(cid:108) and (cid:118)(cid:109) are
generated as follows:
(cid:26)((cid:118)(cid:108))(cid:3)(cid:3)(cid:3)(cid:36) (cid:125)(cid:108) (cid:53) (cid:93)
(cid:118)(cid:108) (cid:53) (cid:86)
(cid:3)(cid:3)(cid:3)(cid:3)(cid:36) (cid:118)(cid:109) (cid:53) (cid:86)
(cid:115)(cid:108)(cid:109) ((cid:120)(cid:108))
(cid:25)((cid:125)(cid:108))(cid:3)(cid:3)(cid:3)(cid:36) (cid:120)(cid:108) (cid:53) (cid:88)
In essence, all the above parameters can be organized
into a family of action-dependent matrices: (cid:112) (cid:113) × (cid:113) tran-
sition probability matrices (cid:73) , (cid:112) (cid:113) × (cid:113) observation proba-
bility matrices (cid:75), (cid:112) (cid:113) × (cid:113) transition reward matrices (cid:74).
(cid:26)((cid:118)(cid:108)) is essentially a (cid:112) · (cid:113) · (cid:116) known observation prob-
ability (cid:83) (cid:117)((cid:125)(cid:108)|(cid:118)(cid:108)(cid:62) (cid:120)(cid:108)(cid:3)1), while (cid:25)((cid:125)(cid:108)) is a (cid:116) · (cid:112) · (cid:112) action
probability (cid:83) (cid:117)((cid:120)(cid:108)|(cid:125)(cid:108)(cid:62) (cid:120)(cid:108)(cid:3)1). In order to parameterize these
chains, we parameterize the policies, so that (cid:25)(·) becomes
a function (cid:25)((cid:21)(cid:62) (cid:125)(cid:108)) of a set of parameters (cid:21) (cid:53) R(cid:110) as well
as the observation (cid:125)(cid:108). The Markov chain corresponding
to (cid:21) has state transition matrix (cid:83) ((cid:21)) = (cid:115)(cid:108)(cid:109)((cid:21)) given by
(cid:115)(cid:108)(cid:109)((cid:21)) = (cid:72)(cid:125)(cid:108)(cid:27)(cid:26)((cid:118)(cid:108))(cid:72)(cid:120)(cid:108)(cid:27)(cid:25)((cid:21)(cid:62)(cid:125)(cid:108))(cid:115)(cid:108)(cid:109)((cid:120)(cid:108)). Therefore, equation
(8) can be achieved by the parameterized policy with (cid:21):
(cid:20)((cid:21)) := lim
(cid:87)(cid:36)(cid:52) E(cid:21)[
1
(cid:87)
(cid:117)(cid:108)](cid:61)
(2)
(cid:87)X
(cid:108)=1
As the detection process of each AD can be formulated
as partially markov decision process, the ADC naturally can
be modeled as a multi-agent POMDP. In the coordinator,
several independent ADs with distinct operating environ-
ments are incorporated. Each of them sees a distinct ob-
servation vector, and has a distinct parameterized random-
ized policy that depends on its own set of parameters. If
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:53:20 UTC from IEEE Xplore.  Restrictions apply. 
Figure 1. Architecture of the Autonomic Detection Coordinator
the collection of ADs is considered as a single AD, the in-
dividual observation vectors can be combined into a single
observation vector, and similarly for the parameter vectors
and action vectors, while the common goal of those ADs
is to maximize the average reward. Effectively, each AD
treats the other ADs as a part of the system, and updates its
own policy while remaining oblivious to the existence of the
other ADs. The only communication between these coop-