symbol normalization.
Fourth, for most models,
the Markov chain order that
gives the best results varies from scenario to scenario. For
Scenario 1, order-5 appears to be the best. Yet for the scenarios
with Chinese datasets (2, 3, 4), order 3 and 4 generally
outperform order 5. One obvious reason is the slightly smaller
training dataset. Also, because the Chinese datasets use digits
much more than letters, they contain even non-digit sequences
for training, resulting in better performance for lower-order
Markoc chains. Again, this demonstrates the beneﬁt of using
a variable-order model like backoff, since one does not need
to choose the appropriate order.
Fifth, comparing the pair of scenarios 5 and 1, and the pair
of 6 and 2, one can see a difference in ANLL0.8 of about 2 to
4 in each case; this demonstrates the importance of training
on a similar dataset.
Sixth, comparing scenarios 2, 3, and 4, we can see that 178
is clearly the weakest password dataset among the 3, which
is corroborated by evidence from Table II. In 178, 55% of
passwords are those appearing more than 5 times (compared
to 30% and 25% for CSDN and Duduniu); close to 21%
are length 6 (compared to 1.3% and 9%); 48% are all digits
699
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:59:45 UTC from IEEE Xplore.  Restrictions apply. 
TABLE IV: ANLL0.8 results; end, dir, and dis stand for end-symbol, direct, and distribution-based normalization, respectively.
We use boldface to highlight the best results within each of the ﬁrst 4 scenarios. The last 2 scenarios represent mismatches
between training and testing.
Algorithm
ws-mc-b10
ws-mc-b25
ws-mc1
ws-mc2
ws-mc3
ws-mc4
ws-mc5
ws-mc1-g
ws-mc2-g
ws-mc3-g
ws-mc4-g
ws-mc5-g
ws-mc6-g
ws-mc1-ag
ws-mc2-ag
ws-mc3-ag
ws-mc4-ag
ws-mc5-ag
ws-mc6-ag
ws-mc1-gts
ws-mc2-gts
ws-mc3-gts
ws-mc4-gts
ws-mc5-gts
tb-mc1-mc1
tb-mc2-mc2
tb-mc3-mc3
tb-mc4-mc4
tb-mc5-mc5
1: Rock→Ya+Ph
dis
23.6
23.9
27.3
26.1
24.8
23.7
23.5
27.4
26.2
24.9
23.9
23.4
23.7
27.3
26.1
24.8
23.7
23.3
23.6
27.3
26.1
24.8
24.1
25.1
27.4
26.3
25.2
24.2
23.7
dir
25.5
25.8
29.3
28.0
26.6
25.6
25.3
29.4
28.2
26.8
25.7
25.3
25.6
29.3
28.0
26.7
25.6
25.2
25.5
29.3
28.0
26.7
25.9
26.9
29.4
28.3
27.1
26.1
25.6
end
22.9
23.3
28.4
26.9
25.2
23.9
23.5
28.4
27.0
25.3
24.0
23.5
24.0
28.3
26.9
25.2
23.9
23.3
23.8
28.4
26.9
25.2
24.2
25.4
28.5
27.2
25.8
24.6
23.9
2: Du+178→CSDN
dis
end
20.0
19.6
20.1
19.7
21.3
22.2
20.2
20.9
20.1
19.6
19.9
19.5
20.4
20.1
21.4
22.3
20.3
21.1
20.3
19.7
20.0
19.6
20.0
20.3
22.3
21.9
21.2
22.2
20.2
21.0
20.2
19.6
19.9
19.5
19.9
20.2
22.1
21.8
21.3
22.2
20.2
20.9
20.2
19.6
20.1
19.7
21.2
21.6
22.3
21.3
20.4
21.3
19.9
20.6
20.3
19.7
20.1
19.7
dir
21.3
21.4
22.6
21.5
20.9
20.8
21.3
22.8
21.6
21.0
20.9
21.3
23.2
22.6
21.5
20.9
20.8
21.1
23.0
22.6
21.5
20.9
20.9
22.4
22.7
21.8
21.2
21.0
21.0
3: CS+178→Dudu
dis
end
21.6
21.5
21.7
21.6
22.8
24.0
21.9
22.8
22.0
21.3
21.7
21.1
22.4
21.8
22.8
24.0
21.9
22.9
22.1
21.4
21.8
21.2
21.8
22.4
23.9
23.1
22.8
24.0
21.9
22.8
22.1
21.3
21.7
21.2
21.8
22.3
23.8
23.1
22.8
24.0
21.9
22.8
22.0
21.3
21.9
21.4
23.7
24.5
23.9
22.8
22.1
23.1
21.6
22.3
22.0
21.3
21.7
21.2
dir
23.2
23.4
24.5
23.5
22.9
22.8
23.4
24.5
23.6
23.0
22.8
23.4
24.8
24.5
23.6
23.0
22.8
23.4
24.7
24.5
23.5
22.9
23.0
25.2
24.5
23.8
23.2
23.0
22.9
4: CS+Du→178
end
12.9
12.9
14.9
13.7
13.2
12.9
12.9
14.9
13.8
13.3
13.0
13.0
13.4
14.9
13.8
13.2
12.9
12.9
13.4
14.9
13.7
13.2
13.0
13.3
14.9
14.3
13.6
13.2
13.0
dir
14.2
14.2
15.3
14.5
13.9
13.8
13.8
15.3
14.5
14.0
13.8
13.9
14.3
15.3
14.5
13.9
13.8
13.8
14.3
15.3
14.5
13.9
13.8
14.2
15.3
14.8
14.2
14.0
13.8
dis
13.2
13.2
14.1
13.3
12.9
12.8
12.8
14.1
13.4
12.9
12.8
12.9
13.3
14.1
13.3
12.9
12.8
12.8
13.3
14.1
13.3
12.9
12.8
13.2
14.1
13.7
13.1
12.9
12.8
5: Chin→Ya+Ph
dis
27.1
27.5
30.0
29.2
27.8
27.6
28.3
30.0