This can be done by looking up the predecessor and successor of
ida at each level of the AS hierarchy. The hosting router then asso-
ciates the successor and predecessor pointers for ida with an AS-
level source-route to the routers hosting the predecessor and suc-
cessor identiﬁers for ida. This can be any source route consistent
with the graph GX, and there can be multiple source routes for
resilience to failure. These AS-level routes are used in determin-
ing which of these pointers are available for relaying a packet (in a
process quite similar to how BGP determines the links to forward
a route advertisement). To reduce stretch, the hosting router uses a
similar procedure to discover ﬁngers at each level. Border routers
in an AS may optionally maintain bloom ﬁlters that summarize the
set of hosts in the subtree rooted at the AS. These bloom ﬁlters are
also updated during the join process.
Routing: Our mechanism for routing relies on greedy routing,
augmented with in-packet AS-level source-routes. As a packet is
routed towards its destination, it is marked with an AS-level source
route denoting the path traversed until that point. When a router
receives a packet, it uses the source-route in determining the candi-
date set of outgoing pointers can be used in forwarding the packet;
that is, it ﬁnds the paths that are consistent with policy. This de-
cision is made by comparing the source-route on the packet to the
source-routes on the pointers using BGP-like import and export ﬁl-
tering rules. Then, greedy routing is used to determine the closest
candidate pointer, whose source-route is tacked on to the packet.
Note that the salubrious properties of greedy routing (such as loop-
free forwarding, eventual reachability) apply even when the packet
is forwarded in this fashion.
Recovering: In the case of a router failure, routers with pointers
to the failed router are notiﬁed either pro-actively by neighbors of
the failed router, or discover the failure when forwarding a packet.
In the case of host failure, the router sends tear-down messages to
each of the ID’s successors and predecessors. When a host/router
failure is noticed by a router which has pointers to the ID, it rejoins
the relevant ID by ﬁnding successors/predecessors at the relevant
level.
In the case of AS-level link failures that lead to a partition in
G, the isolation property ensures that hosts in ASes X and Y can
route to one another provided there is a subtree in GX ∪ GY such
that all AS-level links in the subtree are functional. Hence in the
common case where one access link of a multi-homed AS goes
down, incoming and outgoing trafﬁc will be automatically shifted
to the other access links. Note however that in some failure patterns,
there is a path in the Internet graph between ASes X, Y , but no
fully functional subtree in GX ∪ GY . In this case, AS X can either
prune the graph GX to only working links, and redetermine the
successors of its IDs over this graph; or, it can add working links
to GX to ensure that such a working subtree exists, and re-join its
IDs over those links.
Handling Policies: Our design also handles peering and multi-
homing relationships between ASes. We treat multi-homing links
as backup links; an AS joins the global ROFL ring through one of
its providers, and uses the other providers as backup, in case the
primary provider fails.
Peering relationships can be handled in our design in two dif-
ferent ways. One design option is to transform the graph G so that
doing greedy routing over the links established via joins in G suf-
ﬁces to handle peering. In this case, the property we provide is that,
if a customer of provider X routes to a customer of a peer AS Y of
X, it is guaranteed to use the peering link for that purpose. How-
ever, the limitation here is that the peering link may also be used
in routing packets destined to customers not belonging to Y ; such
packets will be simply returned via the peering link, and will be
routed via X’s provider. This is necessary since it is not possible to
determine whether the destination is a customer of Y without doing
a complete search of the customers of Y . Our second design option
is to use bloom ﬁlters. In this method, AS X uses the bloom ﬁlters
of its peers to determine if the destination is possibly a customer of
any of its peers. If so, it uses the peering link to forward the packet
to Y , which uses its pointer to route to the destination. Note that in
order to handle false positives in the bloom ﬁlter, this method may
require back-tracking, in case the destination is discovered to not
be in Y .
We note that our design requires ISPs to reveal customer-
provider, multi-homing, and peering relationships to their down-
stream customers (since a downstream-customer X uses them to
compute GX). This may not be a serious concern, since as shown
in [35], such relationships are mostly inferable in BGP today. Fi-
nally, our design allows multi-homed ASes some degree of control
over incoming trafﬁc on their access links, though we are yet to
fully understand how this degree of freedom compares to that per-
mitted (or forbidden) by BGP. This control in ROFL is achieved
by investing the join process and identiﬁers with some trafﬁc-
engineering semantics (described in Section 5).
3.
INTRADOMAIN
3.1 Host Join
Algorithm 1 The join internal(id) function is executed by a router
upon receipt of a host request for joining the network. The function
bootstraps a virtual node on behalf of the host.
1: authenticate(id) # exception on error
2: vn = new VirtualNode(id)
3: register virtual node(vn)
4: pred = ﬁnd predecessor(id)
5: # Setup state with local participants
6: vn.successorinternal = pred.successorinternal
7: pred.successorinternal = vn
8: S = select providers()
9: for all s ∈ S do
10:
11:
12:
13: end for
br = locate border router(s)
p = get path to root(s)
br.join external(vn, p)
The joining host with ida ﬁrst selects an upstream gateway router
to join the ring on its behalf. It opens a session to the router and
calls join internal (Algorithm 1), which performs the bootstrap
process. The router authenticates the host and spawns a virtual node
vn(ida) that will hold the routing state with respect to this host’s
identiﬁer. The router then joins the internal ring by using the host’s
identiﬁer to locate the predecessor in the internal AS. The prede-
cessor is used to initialize the internal successor state in vn(ida).
The router then discovers the external successor state by ﬁrst de-
termining the set of paths along the up-hierarchy on which to join.
This set of paths is selected in a manner obeying the policies of the
joining host and its internal AS. For each of these paths, the router
then selects a border router connected to the next AS-hop along
the path. The router forwards the join request to this router, which
in turns performs an external join using the join external function
(described in Section 4.1).
However, this procedure does not work if ida is the router R’s
ﬁrst resident ID, since R does not have any pointers and hence can-
not make progress in the ring. To deal with this, when R ﬁrst starts
up it creates a default virtual node. The default virtual node’s ID
is the router-id, and its successors act as default routes if it has no
other successors that it can use to make progress. The default vir-
tual node joins by ﬂooding a message containing the router-ID. The
router-ID’s predecessors add a pointer to the router-ID, and its suc-
cessors respond back via the path contained in the message. This
ensures that all resident IDs ﬁnd a predecessor in the internal AS
when joining.
When forwarding a control message, intermediate routers may
cache destination IDs contained in the message if they have spare
memory. The control messages also build up a list of routers along
the way, and this list is stored by the router hosting the destination
ID. This list is used to maintain consistency in the presence of host
failure, as described below.
3.2 Failure
We aim to maintain routing state so as to preserve two invari-
ants: (a) if there is a working network-path between a pair of nodes
(A, B), then ROFL ensures that A and B are reachable from each
other (b) if A has a pointer to B, and if either B or the path to B
fails, then A will delete its pointer. We describe how this is achieved
below.
If a router R hosting several IDs goes down,
Router failure:
there are two things that need to happen. (1) Each host connected
to the router R discovers the outage (via a session timeout) and
needs to rejoin via an alternate router. Alternatively it can do this
proactively by joining via multiple routers during its initial join. (2)
There are a set of virtual nodes residing at other routers with point-
ers to IDs at R that need to be updated. Although we could simply
rejoin each virtual node affected by the failure, we instead improve
performance by having routers in advance agree on a sorted list of
routers that will be failed over to in event of failure. Upon node
failure, the end host and remote routers deterministically fail over
to the next alive router on the list.
Host failure: When host with ID ida fails, the gateway router R
will detect the failure through a session timeout. R needs to in-
form all other routers with pointers to ida that it has failed. One
simple way to do this would be to ﬂood all routers with an invalida-
tion message. However, ﬂooding the entire system on host failure
would not be efﬁcient. Instead, we address this by constraining the
set of routers in the system that are allowed to maintain cached
state for ida. For simplicity we constrain this set to be routers hold-
ing predecessors of ida and routers that lie on the shortest path
to those routers. When there is a host failure, the router sends a
directed ﬂood, i.e. a source-routed ﬂood that traverse only this sub-
set of routers. When shortest paths change, or links fail, routers
can optionally update this set via additional directed ﬂoods, how-
ever this is an optimization that is not necessary for correctness. As
a fallback to handle router failure, routers also monitor link-state
advertisements and delete pointers to IDs residing at unreachable
routers.
Link failure, no partition: If the set of links that fail do not create
a partition, then the router need not make any changes on behalf
of its resident IDs since the network map will ﬁnd alternate paths
to their successors. However, the contents of pointer caches that
traverse the link should be temporarily invalidated while the link is
failed (to avoid sending packets over the failed link).
Link failure, partition: In the event of a network-layer partition,
the successor pointers maintained by routers need to remerge into
two separate, consistent namespaces. First, invalid pointers (point-
ers that terminate at routers that are no longer reachable) are torn
down. Next, the router attempts to repair these pointers locally by
shifting the successors down to ﬁll the empty space left by each
failed successor (since it knows no closer IDs may exist in the net-
work), then it tries asking each of its successors Si starting at the
one furthest away to ﬁll the gap at the end of its successor list.
Unfortunately this process could cause the ring to partition into
multiple pieces, even if the underlying network is connected. To
recover from this, we require routers to distribute the smallest ID
they know about (the zero-ID, i.e. the ID closest to zero) to all its
neighbors. The zero-ID a router propagates is set equal to the mini-
mum of the smallest ID it is hosting and the smallest ID it receives
from its neighbors (the path is also distributed to avoid circular de-
pendencies and allow all nodes to reach the zero node). The end
result is that all routers become aware of the smallest ID in the net-
work. This ensures multiple partitions will heal if the network layer
is connected: if the zero-ID is on one ring, its predecessor on the
other ring will learn about it and add it, triggering a merging pro-
cess. The zero-ID will repair its successor and predecessor, who in
turn repair their successors, and so on until the rings are merged.
In practice, the zero node advertisements are piggybacked on link-
state advertisements, and we use the router-IDs of routers instead
of the zero-ID to reduce sensitivity to churn and balance load over
several routers during the recovery phase.
3.3 Packet forwarding
When a router forwards a packet, it selects the closest ID it
knows about to the destination ID. This is done using the link-
state database to return the next hop towards the router containing
that ID. This approach requires routers to return the closest entry
in the namespace as opposed to the shortest-preﬁx match lookups
commonly done today. Finding the closest entry can be imple-
mented with minor modiﬁcations to routers that support longest-
preﬁx match. The key observation is that, given a list of IDs in
sorted order, the closest namespace distance match is either the
shortest preﬁx match or the one right before it in the sorted list.
Algorithm 2 The route (pkt) function is executed by a internal
router upon receipt of a packet destined for a particular virtual node.
1: next hopvn = VN.best match(pkt.destination.id)
if pkt.destination.id == next hopvn.id then
2:
3:
4: else
5:
6:
7:
8:
9:
10:
11: end if
next hopc = PC.best match(pkt.destination.id)
if next hopvn.id < next hopc.id then
sendto(next hopc.path to router, pkt)
sendto(next hopvn.path to router, pkt)
deliver to host(next hopvn, pkt)
else
end if
The forwarding algorithm is shown in Algorithm 2. The router
maintains a list of resident virtual nodes (V N), which exports a
best match function that determines the next hop by choosing the
closest ID among all resident IDs and their successors that does
not overshoot the destination. If the destination is an attached host,
VN.best match returns the interface for the host, which the router
uses to deliver the packet. Otherwise, next hopvn is set to the suc-
cessor state of some resident virtual node. Before forwarding the
packet, the router ﬁrst checks its pointer cache (PC) for an entry
that is closer to the destination than the value stored in next hopvn.
If such a cached entry exists, the router uses its value, stored in
next hopc, instead.
4.
INTERDOMAIN
In this section we describe our design for interdomain ROFL
(which borrows heavily from Canon [17]). First, we give an
overview of how the basic protocol works. Next we provide more
details regarding how hosts join, how packets are routed, and how
failures are handled. Then we describe how customer-provider,
peering, and multihoming policies are supported by our augmented
greedy routing protocol over a suitably deﬁned Directed Acyclic
Graph (DAG).
Parent
Child
External
finger
Child
Internal finger
Figure 2: Merging rings
4.1 Basic design
Interdomain ROFL constructs a DHT over a hierarchical graph,
where nodes correspond to ASes and links correspond to inter-AS
adjacencies. Within each AS, the identiﬁers form an internal ring
as described in Section 3. These rings are then merged with one
another in a bottom-up fashion (traversing up towards the root of
the AS-hierarchy) by having virtual nodes maintain routes to exter-
nal successors that reside in other ASes, as shown in Figure 2. For
a identiﬁer ida in ring 1, these external pointers are established to
identiﬁers idb in ring 2 that satisfy two conditions: (a) idb would be
ida’s successor if the two rings were merged into a single ring, and
(b) there are no identiﬁers in either AS within the interval [ida, idb].
This approach is repeated for each level in the hierarchy. Condition
(b) thus limits the number of external pointers that are formed. Prior
work has shown that the expected total number of pointers (both in-
ternal and external) is O(log(n)) (where n is the total number of
identiﬁers across all stub domains) [17].
Routing occurs as in Chord. Note that on a single customer-
provider hierarchy, a packet sent between a pair of ASes will tra-
verse no higher than their least-common ancestor in that hierarchy.
Moreover, if a host within an AS sends a packet to another host in
that same AS, no external pointers will be used. We refer to this as
the isolation property.
Figure 3: Routing state for virtual node with identiﬁer 8.
For example, Figure 3 shows the internal and external routing
state for a router hosting an identiﬁer 8 residing in AS 4. The host-
ing router has an internal successor pointer to the router hosting
identiﬁer 20 and external successors to hosting routers residing in
ASes 5 and 3. The join protocol discovers the external successor at
each level of the joining node’s up-hierarchy. For instance, the host-
ing router for 8 maintains a external successor to 16 at the level of
AS 2, and an external successor to 14 at the level of AS 1.
Joining: When a hosting router R performs a join for an end-host
with ID ida, R joins both the internal ring (as described in Sec-
tion 3) and also the ROFL ring on behalf of ida. ida joins the ROFL
ring by, for each AS X in its up-hierarchy, routing towards its suc-
cessor using links that traverse no higher than X. In this fashion,
it builds a list of candidate successors, one corresponding to each
AS in its up-hierarchy. It then removes unnecessary successors. For
example in Figure 3, if the identiﬁer in AS 5 were 12 instead of 16,
8 would not maintain 14 as a successor (as doing so could vio-
late isolation). Finally, if ida is the ﬁrst host in the ISP, it needs a
way to bootstrap itself into the ROFL ring. This is done by having
host identiﬁers register with their providers (and their provider’s
providers, and so on) when they join. Their providers need only
maintain a short list of such identiﬁers (a few at each level of the
hierarchy for resiliency purposes). When a new host joins that does
not have a predecessor in its internal ring, the ISP will forward the
join request to one of its providers to lookup a bootstrap node. The
registration process also allows operators to control which set of
ASes ida can join through, and to constrain connectivity to follow
policy or trafﬁc engineering goals.
Algorithm 3 The join external (vn, p) function is executed by a
border router upon receipt of a request for a joining virtual node vn
along the path p.
1: pred = ﬁnd predecessor(vn.id)
2: RSpred = pred.successorexternal ∪ pred.successorinternal
3: RSvn = vn.successorexternal ∪ vn.successorinternal
4: prune route entries(RSpred, p)
5: prune route entries(RSvn, p)
6:
7:
8: end if
9:
10:
11: end if
12: br = next border router(p)
13:
14:
15: end if
vn.successorexternal.add(min id(RSpred))