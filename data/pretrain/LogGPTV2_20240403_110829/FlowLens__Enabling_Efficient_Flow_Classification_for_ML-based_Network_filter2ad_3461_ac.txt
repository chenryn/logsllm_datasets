clearing all registers of the FMA’s register grid, and setting a
timer for the duration of the collection window. Then, for every
incoming packet for which a rule does not exist in the FMA’s
ﬂow table (i.e., the packet belongs to a new ﬂow), the control
plane automatically installs one of two possible rules for that
ﬂow. If there is free space in the register grid, then it will install
a new rule that points to a free ﬂow marker, allowing the ﬂow
to be monitored by the FMA. Otherwise, if the register grid
is full, the control plane will install a single wildcard ignore
rule (featuring a reserved offset value) instructing the FMA
to ignore that ﬂow and all subsequent ﬂows until the end of
the collection window. When the timer expires, the control
plane reads in batch all the computed ﬂow markers. To prevent
concurrent updates while reading, the control plane deletes the
ﬂow rules prior to reading the registers. Once the registers have
been read, a new collection window round can then ensue. A
side effect of this design is that FlowLens may skip the packets
of a new ﬂow until the ﬂow’s respective rule is installed in the
switch. However, while this is a limitation of our system, such
loss does not impair FlowLens’s ability to trace most typical
ﬂows as they tend to last longer than a few milliseconds.
Discretionary ﬂow monitoring: Prior to the enforcement of
the FCFS ﬂow marker allocation strategy, it is possible to ﬁlter
which trafﬁc should be monitored and therefore limit the amount
5
src_ip162.2.13.42...Flow Table0FTS-1src_portflow_offset410650162.2.15.48382341......Register Grid14256...............0RGS-1Flow Marker Bins (FMB)quant_pkt_szTruncation Table43640000	0000	0100	0000	00000000	0000	0000	0100	0000Bin	=	64PL	=	1024PL	>>	QL:4...QuantizationInput Packetpacket_length	=	1024bin_offset02...094FMB-1Binmax26...511dst_ip146.3.14.71...dst_port80146.3.18.2338943...proto617...src_ip	=	162.2.13.42				dst_ip	=	146.3.18.71		src_port	=	41065										dst_port	=	80proto	=	6of ﬂows to inspect. For instance, ML-based security applications
are frequently focused on trafﬁc that can be identiﬁed by
common target ports (e.g., HTTP). In other cases, the FlowLens
operator may be interested in monitoring ﬂows based on IP
or network address ranges. Such a discretionary ﬂow ﬁltering
stage can be implemented on the control plane by installing
ignore rules for all uninteresting ﬂows based on allow / deny
policies provided by the FlowLens operator. Ignore rules can be
deﬁned to target a single ﬂow or to perform wildcard matching
based on IP and port ranges, speciﬁc protocol ﬁelds, etc.
Fine-tuning of the collection window: Flow markers should
not linger for an arbitrarily large amount of time inside FMA’s
data structures, as this would prevent the ﬂow marker’s memory
from being used for other ﬂows. To increase the number of ﬂows
that can be monitored at any given time, the collection window
can be conﬁgured, based on three setups: i) the deﬁnition of
a ﬁxed window duration (explained above); ii) the use of a
speciﬁc ﬂag set in FMA data structures that enables the control
plane to check for ﬂow termination through a polling procedure;
iii) a hybrid of both approaches. While option i) is arguably
the simplest alternative, it may lead to memory waste since
short-lived ﬂows can occupy the FMA’s data structures longer
than necessary. In contrast, a polling approach allows FlowLens
to pinpoint ﬂows that will receive no new packets (e.g., after
detecting FIN packets in the data plane pipeline which signals
the termination of a TCP connection), but it may indeﬁnitely
keep ﬂows which termination is not explicit (e.g., UDP-based
multimedia ﬂows). Thus, a hybrid collection approach allows
us to proactively read and reset the FMA data structures in
use by terminated ﬂows while preventing long-lived ﬂows to
be monitored indeﬁnitely. In other words, this approach fully
refreshes the register grid every time the collection window is
over and partially updates it whenever a particular row is in
condition to be evicted.
Flow marker eviction: A high rate of new ﬂows may saturate
the capacity of the FMA data structures and prevent storing
ﬂow markers for all ﬂows crossing the switch. In this case,
as explained above, the FMA’s default strategy is to not track
new ﬂows as long as existing ﬂows are still being tracked. As
an alternative behavior, it is possible to evict ﬂow markers
from the FMA according to an LRU policy. In this case, the
control plane keeps track of the oldest ﬂow markers stored by
the FMA, and replaces them as new incoming ﬂows cross the
switch. The most suitable policy will greatly depend on the
expected workload and topology of the network.
D. Distributed and Orchestrated FMA Operation
So far, we explained several design decisions of FlowLens
when considering its operation to be contained within a single
switch. In this section, we describe how FlowLens can beneﬁt
from a deployment in multiple vantage points.
Scaling the number of measured ﬂows: Although the number
of ﬂows whose state can be kept by a single switch is limited,
it is possible to take advantage of multiple vantage points in the
network for monitoring a larger amount of ﬂows. This is akin
to the operation of other measurement frameworks [43, 31] and
may be accomplished, for instance, by splitting packets coming
from different IP address spaces between existing switches in
the organization’s network infrastructure.
Increasing collection coverage: In the case that our system
operates with a maximum collection window (see Section IV-C),
reading and resetting FMA’s data structures requires a non-
negligible amount of time (a few seconds) [36]. This may
prevent FlowLens from collecting ﬂow information while these
operations take place. To ensure visibility over the network
trafﬁc crossing an organization, FlowLens can be deployed in
a cascade fashion across an additional switch to intertwine the
collection windows of the different switches.
Increasing application coverage: The design of FlowLens is
tailored for enabling a single proﬁle to be loaded into a given
FMA at any given time. However, a coordinated operation
of FlowLens across several switches can provide support to
multiple ML-based security applications. For instance, when
deployed across multiple switches, one FMA instance may be
dedicated to the detection of covert channels, while other is
dedicated to the identiﬁcation of botnet behavior.
V. AUTOMATIC PROFILING
The ﬂow markers generated by FMA depend on the
parameters, i.e., the quantization level and the truncation table,
dictated by an application-speciﬁc proﬁle which determines how
efﬁciently the switch SRAM will be used and how accurate the
ﬂow classiﬁcation will be. In general, ﬁnding the parameters that
offer an optimal trade-off would require an exhaustive search
of the parameter space. Unfortunately, this is a cumbersome
task that requires non-trivial computational resources and time,
e.g., automatically exploring the full space of conﬁgurations
for the botnet detection task (Section VII-F) took one day.
To search on the parameter space for a conﬁguration
that offers a good trade-off between ﬂow marker size and
classiﬁcation accuracy, the proﬁler implements optimization
techniques that, albeit may fail to yield the optimal result,
usually ﬁnd near-optimal solutions quickly. Next, we describe
the optimization criteria and algorithm employed in FlowLens.
Note that FlowLens is not
tightly coupled to a speciﬁc
implementation of the proﬁler and nothing prevents the use of
alternate optimization techniques [72, 9]. Investigating further
optimization approaches is outside the scope of this paper.
A. Optimization Criteria
We expect that a FlowLens’s operator will want to ﬁnd
a suitable FMA parameterization for any given ML-based
security application. Because there is a space/accuracy trade-
off in the FMA conﬁguration, we are faced with a multi-
criteria optimization problem that does not have a single
optimal solution but, instead, has a number of Pareto optimal
solutions [57]. The current version of the FlowLens’s proﬁler
can approximate three different pre-set points in the Pareto
frontier, that can be selected by the system operator:
1. Smaller marker for target accuracy: In this mode, the
system operator speciﬁes a target accuracy value to be attained,
and the proﬁler automatically chooses the quantization and
truncation parameters that yield the smallest marker that is
able to offer the target accuracy. Note that the proﬁler will not
return a conﬁguration if the accuracy set by the user cannot
be achieved for the particular dataset under analysis.
6
2. Best accuracy given a size constraint: Here, the system
operator speciﬁes the maximum size for the ﬂow marker and
the system automatically picks the quantization and truncation
parameters that maximize the classiﬁcation accuracy, among the
conﬁgurations explored, without exceeding the target marker
size. This constraint also allows us to reduce the search space,
since the marker size generated by a set of quantization and
truncation parameters is known beforehand.
3. Size vs. accuracy trade-off: Lastly, the proﬁler can work in
a fully automated fashion. In this case, the proﬁler attempts to
maximize an accuracy vs marker size trade-off that is expressed
by the following reward function: reward = α · accuracy +
(1 − α)
marker size. A smaller α attributes less importance to the
accuracy in favor of compactness, and vice-versa. Our prototype
uses α = 0.5, but the system operator can deﬁne the value of α
as well as a different reward policy of its choosing altogether.
1
B. Optimization Algorithm
The search space is the product of the different quantization
and truncation conﬁgurations; on its own, the number of
conﬁgurations that result from truncation is combinatorial with
the number of available bins. To guide the proﬁler’s search, we
use an optimization algorithm that consists of two phases. In
the ﬁrst phase, called search space reduction, we use domain
knowledge to narrow the search, by excluding conﬁgurations
that are unlikely to offer acceptable results. In the second
phase, we resort to Bayesian optimization to ﬁnd a suitable
ﬂow marker. We detail these two steps in the next paragraphs.
1. Search space reduction: To reduce the search space, we
discretize the domain of quantization parameters, e.g., aggregate
bins in powers of two. Then, we leverage a pre-training step
for narrowing the truncation space: we ﬁrst generate a coarser
representation of the packet distributions of each sample in
the training data according to a given quantization parameter,
then we use a classiﬁer to build a model based on these
representations. We leverage the fact that most classiﬁers can
output information regarding the top-N most relevant bins for
accurate classiﬁcation. Thus, for each quantization, we constrain
the exploration to points that include an increasing number of
features from the top-50 (i.e., the conﬁguration that includes
only the top-10 bins, the top-20 bins, etc). When the classiﬁer
is unable to output the top-N features, we fall back to a simpler
strategy to narrow the search space: we sample the input space,
exclude bins that have not been observed in the sampled points,
and feed the remaining ones to the Bayesian optimizer.
2. Bayesian optimization: To reduce the manual labor re-
quired to explore a large space of conﬁgurations, we rely on
Bayesian optimization, which is a well-known method for
optimizing black-box functions and for ﬁnding near-optimal
solutions with few function evaluations [24]. We optimize the
combination of quantization and truncation parameters using
the Python Hyperopt [10] software package. In each iteration,
the proﬁler selects a parameterization, trains a classiﬁer using
ﬂow markers accordingly generated, and records the classiﬁer
accuracy alongside the size of produced ﬂow markers. The next
parameterization to sample is selected by the optimizer which
we run for a ﬁxed number of iterations. The whole process
took us a few hours to complete.
Figure 5. FMA implementation on Toﬁno switch. Each partition of the ﬂow
table (FTm) records flow ids (marked in orange) and their corresponding
ﬂow markers are recorded in the register grid partition of the upcoming stage
(RGm). This packet is matched in FT2 and the respective ﬂow marker updated
in RG2. The white boxes in the bottom indicate the metadata values computed
in each stage; the value in the blue box is loaded by the control plane.
VI.
IMPLEMENTATION
We built a prototype of the FlowLens system. Excepting
the FMA, which was written in P4, we implemented all
other components in Python. The classiﬁcation engine of
the proﬁler server uses Python’s scikit-learn [62] and the
Weka [28] libraries. We implemented FlowLens’s FMA [8] for
a Barefoot Toﬁno ASIC [4] using about 500 lines of P416 code,
which was compiled with the P4 Studio Software Development
Environment (SDE) [5]. While the FMA’s design presented in
Section IV-A is generically compatible with PISA architecture,
its implementation required careful reasoning due to the speciﬁc
intricacies of currently available switching hardware.
To implement the FMA code for a Toﬁno switch, we need
to ﬁt the FMA’s data structures and operations into the speciﬁc
pipeline and compute capabilities of the switch. To implement
ﬂow marker updates, it would be desirable to compute the ﬂow
offset and the bin offset of the target ﬂow marker (see Figure 4)
in a single pipeline stage to be able to use all the memory in
upcoming stages to store ﬂow markers. However, this cannot
be achieved on our target hardware due to three major data
dependencies: i) matching (i.e., indexing) the truncation table
depends on the quantized packet length, but quantization and
truncation are too complex to be realized together in a single
stage; ii) indexing a ﬂow marker’s bin requires the result of
truncation, but the truncation table and the ﬂow table cannot be
matched in the same physical stage; iii) matching the ﬂow table
and updating the respective ﬂow marker are also too complex
to perform in a single stage. Moreover, it is not possible to
access all the switch memory from a single stage.
Laying out the FMA in hardware: To accommodate for
the above requirements, we split the functioning of FMA
across different stages, as depicted in Figure 5. To resolve
dependencies i) and ii), we reserve the ﬁrst and second stages
of the pipeline to perform quantization and truncation. Then,
we partition the ﬂow table and register grid along the remaining
stages to use up all the per-stage stateful registers across the
processing pipeline. To overcome the inability to calculate the
bin offset and increment the corresponding register cell in the
same stage – dependency iii) – the ﬂow table partitions and
register grid partitions are placed in contiguous stages. Each
ﬂow table partition is responsible for managing ﬂow markers
in its corresponding register grid partition.
7
Stage 1FlowID = packet size = 512MatchQuantizationQL = 5Stage 2TruncationStage 3FT1Stage 4173822RG1FT2Stage 516411230154121010000RG2FT3Count...md.binIndex_quant= 16md.bin_offset= 1md.trunc_flag= 1flow_offset= 10md.rg_cell_offset= 11Figure 5 depicts in detail the operation of FMA when a
packet for a new ﬂow arrives. Notation FTm and RGm denote
partition m of the ﬂow table and register grid, respectively.
Assume that the collector has installed a rule for flow id
(cid:104)162.2.13.42, 6901, 147.6.54.129, 3478, 17(cid:105) in FT2, and that
the ﬁrst incoming packet for this ﬂow has a size of 512B. In