title:Elastic Parity Logging for SSD RAID Arrays
author:Yongkun Li and
Helen H. W. Chan and
Patrick P. C. Lee and
Yinlong Xu
2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Elastic Parity Logging for SSD RAID Arrays
Yongkun Li1,2, Helen H. W. Chan3, Patrick P. C. Lee3, Yinlong Xu1,4
1School of Computer Science and Technology, University of Science and Technology of China
2Collaborative Innovation Center of High Performance Computing, National University of Defense Technology
3Department of Computer Science and Engineering, The Chinese University of Hong Kong
4AnHui Province Key Laboratory of High Performance Computing
{ykli,ylxu}@ustc.edu.cn, {hwchan,pclee}@cse.cuhk.edu.hk
Abstract—Parity-based RAID poses a design trade-off issue
for large-scale SSD storage systems: it improves reliability against
SSD failures through redundancy, yet its parity updates incur
extra I/Os and garbage collection operations, thereby degrading
the endurance and performance of SSDs. We propose EPLOG,
a storage layer that reduces parity trafﬁc to SSDs, so as to
provide endurance, reliability, and performance guarantees for
SSD RAID arrays. EPLOG mitigates parity update overhead via
elastic parity logging, which redirects parity trafﬁc to separate
log devices (to improve endurance and reliability) and eliminates
the need of pre-reading data in parity computations (to improve
performance). We design EPLOG as a user-level implementation
that is fully compatible with commodity hardware and general
erasure coding schemes. We evaluate EPLOG through reliability
analysis and trace-driven testbed experiments. Compared to the
Linux software RAID implementation, our experimental results
show that our EPLOG prototype reduces the total write trafﬁc to
SSDs, reduces the number of garbage collection operations, and
increases the I/O throughput. In addition, EPLOG signiﬁcantly
improves the I/O performance over the original parity logging
design, and incurs low metadata overhead.
I.
INTRODUCTION
A. Background
Solid-state drives (SSDs) have seen wide adoption in
desktops and even large-scale data centers [32], [38], [44].
Today’s SSDs mainly build on NAND ﬂash memory. An
SSD is composed of multiple ﬂash chips organized in blocks,
each containing a ﬁxed number (e.g., 64 to 128) of ﬁxed-
size pages of size on the order of KB each (e.g., 2KB, 4KB,
or 8KB). Flash memory performs out-of-place writes: each
write programs new data in a clean page and marks the page
with old data as stale. Clean pages must be reset from stale
pages through erase operations performed in units of blocks.
To reclaim clean pages, SSDs implement garbage collection
(GC), which chooses blocks to erase and relocates any page
with data from a to-be-erased block to another block.
Despite the popularity, SSDs still face deployment issues,
in terms of reliability, endurance, and performance. First,
on the reliability side, bit errors are common in SSDs due
to read disturb, write disturb, and data retention [6], [12],
[13], [33], and the bit error rate of ﬂash memory generally
increases with the number of program/erase (P/E) cycles [12],
[26]. Unfortunately, ﬂash-level error correction codes (ECCs)
only provide limited protection against bit errors [26], [49],
especially in large-scale SSD storage systems. Second, on
the endurance side, SSDs have limited lifespans. Each ﬂash
memory cell can only sustain a ﬁnite number of P/E cycles
before wearing out [2], [13], [19]. The sustainable number
of P/E cycles is typically 100K for a single-level cell (SLC)
and 10K for a multi-level cell (MLC), and further drops to
several hundred with a higher ﬂash density [13]. Finally, on the
performance side, small random writes are known to degrade
the I/O performance of SSDs [7], [21], [34], since they not
only aggravate internal fragmentation and trigger more GC
operations (which also degrade the endurance of SSDs), but
also subvert internal parallelism across ﬂash chips.
Parity-based RAID (Redundant Array of
Inexpensive
Disks) [41] provides a natural option to enhance the reliability
of large-scale storage systems. Its idea is to divide data
into groups of ﬁxed-size units called data chunks, and each
group of data chunks is encoded into redundant information
called parity chunks. Each group of data and parity chunks,
collectively called a stripe, provides fault tolerance against the
loss of data chunks, such that any subset of a sufﬁcient number
of data/parity chunks of the same stripe can reconstruct the
original data chunks. Recent studies examine the deployment
of SSD RAID at the device level [3], [26], [28], [31], [39],
[40], so as to protect against SSD failures.
However, deploying parity-based RAID in SSD storage
systems requires special attention [18], [35]. In particular,
small random writes are even more harmful to parity-based
SSD RAID in both endurance and performance. To maintain
stripe consistency, each write to a data chunk triggers updates
to all parity chunks of the same stripe. Small writes in RAID
imply partial-stripe writes [8], which ﬁrst read existing data
chunks, re-compute new parity chunks, and then write both
new data and parity chunks. In the context of SSD RAID,
parity updates not only incur extra I/Os (i.e., reads of existing
data chunks and writes of parity chunks), but also aggravate
GC overheads due to extra parity writes. Frequent parity
updates inevitably undermine both endurance and performance
of parity-based SSD RAID, especially when we need a higher
degree of fault tolerance (i.e., more parity updates).
Therefore, parity-based RAID poses a design trade-off is-
sue for large-scale SSD storage systems: it improves reliability
against SSD failures; on the other hand, its parity updates
degrade both endurance and performance. This motivates us
to explore a new SSD RAID design that mitigates parity
update overhead, so as to provide reliability, endurance, and
performance guarantees simultaneously.
B. Contributions
We propose EPLOG, an elastic parity logging design for
SSD RAID arrays. EPLOG builds on parity logging [47] to
redirect parity write trafﬁc from SSDs to separate log devices.
By reducing parity writes to SSDs, EPLOG slows down the
ﬂash wearing rate, and hence improves both reliability and
978-1-4673-8891-7/16 $31.00 © 2016 IEEE
DOI 10.1109/DSN.2016.14
49
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:38:08 UTC from IEEE Xplore.  Restrictions apply. 
endurance. It further extends the original parity logging design
by allowing parity chunks to be computed based on the newly
written data chunks only, where the data chunks may span
within a partial stripe or across more than one stripe. Such an
“elastic” parity construction eliminates the need of pre-reading
old data for parity computation, so as to improve performance.
To summarize, this paper makes the following contributions:
• We design and implement EPLOG as a user-level
block device1 that manages an SSD RAID array.
Speciﬁcally, EPLOG uses hard-disk drives (HDDs)
to temporarily log parity information, and regularly
commits the latest parity updates to SSDs to mitigate
the performance overhead due to HDDs. We show
that EPLOG enhances existing ﬂash-aware SSD RAID
(see Section VI) in different ways: (i) EPLOG is
fully compatible with commodity conﬁgurations and
does not rely on high-cost components such as non-
volatile RAM (NVRAM); and (ii) EPLOG can readily
support general erasure coding schemes for high fault
tolerance.
• We conduct mathematical analysis on the system reli-
ability in terms of mean-time-to-data-loss (MTTDL).
We show that EPLOG improves the system reliability
over the conventional RAID design when SSDs and
HDDs have comparable failure rates [48].
• We conduct extensive trace-driven testbed experi-
ments, and demonstrate the endurance and perfor-
mance gains of EPLOG in mitigating parity update
overheads. We compare EPLOG with the Linux soft-
ware RAID implementation based on mdadm [37],
which is commonly used for managing software RAID
across multiple devices. For example, in some settings,
EPLOG reduces the total write trafﬁc to SSDs by
45.6-54.9%, reduces the number of GC requests by
77.1-97.6%, and increases the I/O throughput by 30.1-
119.2% even though it uses HDDs for parity logging.
Finally, EPLOG shows higher throughput
than the
original parity logging design, and incurs low over-
head in metadata management.
The rest of the paper proceeds as follows. In Section II,
we state our design goals and motivate our new elastic parity
logging design. In Section III, we describe the design and
implementation details of EPLOG. In Section IV, we analyze
the system reliability of EPLOG. In Section V, we present
evaluation results on our EPLOG prototype through trace-
driven testbed experiments. In Section VI, we review related
work, and ﬁnally in Section VII, we conclude the paper.
II. OVERVIEW
In this section, we state the design goals of EPLOG. We
also motivate how EPLOG mitigates parity update overhead
through elastic parity logging.
A. Goals
EPLOG aims for four design goals.
• General reliability: EPLOG provides fault tolerance
against SSD failures. In particular,
it can tolerate
a general number of SSD failures through erasure
coding. This differs from many existing SSD RAID
designs that are speciﬁc for RAID-5 (see Section VI).
• High endurance: Since parity updates introduce extra
writes to SSDs, EPLOG aims to reduce the parity traf-
ﬁc caused by small (or partial-stripe) writes to SSDs,
thereby improving the endurance of SSD RAID.
• High performance: EPLOG eliminates the extra
I/Os due to parity updates, thereby maintaining high
read/write performance.
•
Low-cost deployment: EPLOG is deployable using
commodity hardware, and does not assume high-
end components such as NVRAM as in SSD RAID
designs (e.g., [10], [15], [26]).
EPLOG targets workloads that are dominated by small
leading to frequent partial-stripe writes to
random writes,
RAID. Examples of such workloads include those in database
applications [17], [27] and enterprise servers [20]. Note that
real-world workloads often exhibit high locality both spatially
and temporally [34], [43], [46], such that recently updated
chunks and their nearby chunks tend to be updated more
frequently. It is thus possible to exploit caching to batch-
process chunks in memory to boost both endurance and
performance (by reducing write trafﬁc to SSDs). On the other
hand, modern storage systems also tend to force synchronous
writes through fsync/sync operations [14], which make
small random writes inevitable. Thus, our baseline design
should address synchronous small random writes, but allows
an optional caching feature for potential performance gains.
B. Elastic Parity Logging
Parity logging [47] has been a well-studied solution in
traditional RAID to mitigate the parity update overhead. We
ﬁrst review the design of parity logging, and then motivate
how we extend its design in the context of SSD RAID.
We ﬁrst demonstrate how parity logging can improve
endurance of an SSD RAID array by limiting parity trafﬁc to
SSDs. Our idea is to add separate log devices to keep track of
parity information that we refer to as log chunks. To illustrate,
Figure 1 shows an SSD RAID-5 array with three SSDs for
data and one SSD for parity (i.e., the array can tolerate single
SSD failure). In addition, we have one log device for storing
log chunks. Suppose that a stream of write requests is issued
to the array. The ﬁrst two write requests, respectively with data
chunks {A0, B0, C0} and {A1, B1, C1}, constitute two stripes.
Also, the following write request updates data chunks B0,
C0, and A1 to B0’, C0’, and A1’, respectively. Figure 1(a)
illustrates how the original parity logging works. It updates
data chunks in-place at the system level above the SSDs (note
that an SSD adopts out-of-place updates at the ﬂash level as
described in Section I-A). It computes a log chunk by XOR-
ing the old and new data chunks on a per-stripe basis. It then
appends all log chunks to the log device.
1Here, a block refers to the read/write unit at the system level, and should
not be confused with an SSD block at the ﬂash level.
The original parity logging limits parity trafﬁc to SSDs,
thereby slowing down their wearing rates. Nevertheless, we
50
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:38:08 UTC from IEEE Xplore.  Restrictions apply. 
Incoming requests:
{A0, B0, C0}, {A1, B1, C1}, {B0’, C0’, A1’}
Stripe 0
A0
B0’ C0’ P0
B0+B0’+C0+C0’
Stripe 1
A1’ B1
P1
C1
A1+A1’
SSD RAID-5
Log Device
(a) Original parity logging
EPLog
Commit
Module
Incoming Writes
Log Module
Stripe
Buffers
Device
Buffers
Coding Module
Stripe 0
A0
B0
C0
P0
B0’+C0’+A1’
SSD Write Module
Log Write Module
B0’ C0’
Stripe 1
A1
B1
P1
C1
A1’
SSD RAID-5
Log Device
(b) Elastic parity logging
Fig. 1: Illustration of parity logging schemes in SSD RAID-5.
identify two constraints of this design. First, it needs to pre-
read old data to compute each log chunk, and hence incurs
extra read requests. Second, the log chunks are computed on a
per-stripe basis. This generates additional log chunks if a write
request spans across stripes.
We build on the original parity logging and relax its con-
straints, and propose a new parity update scheme called elastic
parity logging. Figure 1(b) illustrates its idea. Speciﬁcally,
when the write request updates data chunks B0, C0, and A1
to B0’, C0’, and A1’, respectively, we perform out-of-place
updates at the system level, such that we directly write the new
data chunks to the corresponding SSDs without overwriting the
old data chunks. In other words, both the old and new versions
of each data chunk are kept and accessible at the system level.
In addition, we compute a log chunk by XOR-ing only the
new data chunks to form B0’+C0’+A1’, and append it to
the log device. Compared to the original parity logging, we
now store only one log chunk instead of two. Note that the old
versions of data chunks are needed to preserve fault tolerance.
For example, if data chunk A0 is lost, we can recover it from
B0, C0, and P0, although both B0 and C0 are old versions.
As opposed to the original parity logging, elastic parity
logging does not need to pre-read old data chunks. It also
relaxes the constraint that the log chunks must be computed
on a per-stripe basis; instead, a log chunk can be computed
from the data chunks within part of a stripe or across more
than one stripe (and hence we call the parity logging scheme
“elastic”).
III. DESIGN AND IMPLEMENTATION
EPLOG is designed as a user-level block device. It runs on
top of an SSD RAID array composed of multiple SSDs, and
additionally maintains separate log devices for elastic parity
Chunks
SSD
. . .
SSD
. . .
Main Array
Log Devices
Fig. 2: EPLOG architecture.
logging (see Section II-B). In this work, we choose HDDs
as log devices to achieve low-cost deployment. On the other
hand, designing EPLOG faces different challenges, especially
when we use HDDs as log devices. In this section, we address
the following design and implementation issues.
•
•
•
•
How do we construct log chunks for a write request,
such that we maintain reliability as in conventional
RAID without using parity logging?
How do we minimize the access overheads for log
chunks in log devices, so as to maintain high perfor-
mance?
How do we further improve endurance and perfor-
mance of EPLOG via caching, which is feasible for
some applications (see Section II-A)?
How do we manage metadata in a persistent manner
in our EPLOG implementation?
A. Architecture
EPLOG stores data chunks in a set of SSDs (which we
collectively call
the main array) and log chunks in a set
of HDD-based log devices. Accessing log chunks in HDD-
based log devices is expensive. Thus, EPLOG issues only
sequential writes of log chunks to log devices. In addition,
it regularly commits the latest parity updates in the main array
in the background, such that the main array stores the latest
versions of data chunks and the corresponding parity chunks.
We call
the whole operation parity commit. For example,
referring to Figure 1(b), we update P0 and P1 to reﬂect the
sets of latest data chunks {A0, B0’, C0’} and {A1’, B1,
C1}, respectively. Thus, accessing data in degraded mode (i.e.,
when an SSD fails) can operate in the main array only (as in
conventional SSD RAID without parity logging), and hence
preserve performance.
EPLOG realizes the above design through a modularized