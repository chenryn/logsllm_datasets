threats we mentioned earlier, like a thief with a stolen
laptop.
it
Similarly,
is reasonable to assume that email
providers will not proactively decapsulate and archive
Vanishing Emails prior to expiration. One factor is the
potential illegality of such accesses under the DMCA,
but even without the DMCA this seems unlikely. There-
fore, users can simply employ the FireVanish Gmail plu-
gin without needing to exchange public keys with their
correspondents. However, because our plugin extends
FireGPG, any user already familiar with GPG could
leverage our plugin’s GPG integration.
Data Sanitization.
In addition to ensuring that Van-
ish meets its security and privacy goals, we must ver-
ify that the surrounding operating environment does not
preserve information in a non-self-destructing way. For
this reason, the system could leverage a broad set of ap-
proaches for sanitizing the Vanish environment, includ-
ing secure methods for overwriting data on disk [31], en-
crypting virtual memory [50], and leveraging OS support
for secure deallocation [15]. However, even absent those
approaches, forensic analysis would be difﬁcult if at-
tempted much later than the data’s expiration for the rea-
sons we’ve previously discussed: by the time the forensic
analysis is attempted relevant data is likely to have dis-
appeared from the user’s machine, the churn in the DHT
would have made shares (and nodes) vanish irrevocably.
6.2 Privacy Against DHT-Integrated Ad-
versaries
We now examine whether an adversary who interacts
with the DHT prior to a VDO’s expiration can, in the fu-
ture, aid in retroactive attacks against the VDO’s privacy.
During such a precomputation phase, however, the at-
tacker does not know which VDOs (or even which users)
he might eventually wish to attack. While the attacker
could compile a list of worthwhile targets (e.g., politi-
cians, actors, etc.), the use of Tor would thwart such tar-
geted attacks. Hence, the principle strategy for the at-
tacker would be to create a copy of as many key shares
as possible. Moreover, the attacker must do this continu-
ously — 24x7 — thereby further amplifying the burden
on the attacker.
Such an attacker might be external to the DHT —
simply using the standard DHT interface in order to ob-
tain key shares — or internal to the DHT. While the
former may be the only available approach for DHTs
like OpenDHT, the approach is also the most limiting
to an attacker since the shares are stored at pseudoran-
domly generated and hence unpredictable indices. An at-
tacker integrating into a DHT like Vuze has signiﬁcantly
more opportunities and we therefore focus on such DHT-
integrating adversaries here.
Experimental Methodology. We ran extensive exper-
iments on a private deployment of the Vuze DHT. In
each experiment, a set of honest nodes pushed VDO
shares into the DHT and retrieved them at random in-
tervals of time, while malicious nodes sniffed stores
and lookups.3 Creating our own Vuze deployment al-
lowed us to experiment with various system parameters
and workloads that we would not otherwise have been
able to manipulate. Additionally, experimenting with at-
tacks against Vuze at sufﬁcient scale would have been
prohibitively costly for us, just as it would for an attacker.
Our experiments used 1,000, 2,000, 4,500, and 8,000-
node DHTs, which are signiﬁcantly larger than those
used for previous empirical DHT studies (e.g. 1,000
3Vuze get messages do not reveal additional information about val-
ues stored in the DHT, so we do not consider them.
nodes in [53]). For the 8,000-node experiments we used
200 machine instances of Amazon’s EC2 [2] compute
cloud. For smaller experiments we used 100 of Emu-
lab’s 3GHz, 2GB machines [27]. In general, memory is
the bottleneck, as each Vuze node must run in a separate
process to act as a distinct DHT node. Approximately 50
Vuze nodes ﬁt on a 2-GB machine.
Churn (node death and birth) is modeled by a Pois-
son distribution as in [53]. Measurements of DHT net-
works have observed different median lifetime distribu-
tions, e.g., 2.4 minutes for Kazaa [30], 60 minutes for
Gnutella [57], and 5 hours with Vuze [28] (although this
measurement may be biased towards longer-lived nodes).
We believe that these vast differences stem from different
content and application types that rely on these networks
(e.g., the difference between audio and video clips). We
chose a 2-hour median node lifetime, which provides in-
sight into the availability—security tradeoffs under high
churn.
6.2.1 The Store Snifﬁng Attack
We ﬁrst examine a store snifﬁng attack in which the
adversary saves all of the index-to-value mappings it re-
ceives from peers via store messages. Such an attacker
might receive a VDO’s key shares in one of two ways: di-
rectly from the user during a VDO’s creation or refresh,
or via replication. In Vuze, nodes replicate their cached
index-to-value mappings every 30 minutes by pushing
each mapping to 20 nodes whose IDs are closest to the
mapping’s index.
Effects of VDO Parameters on Security. Our ﬁrst goal
is to assess how security is affected by the VDO param-
eters N (the number of key shares distributed for each
VDO) and the key threshold (the percent of the N shares
required to decrypt a VDO). Figure 7(a) plots the prob-
ability that an attacker can capture sufﬁcient key shares
to revoke the privacy of a given VDO as a function of
N and the threshold. This ﬁgure assumes the attacker
has compromised 5% of the nodes in a 1,000-node DHT.
Not surprisingly, as the number of shares N increases, the
attacker’s success probability drops signiﬁcantly. Simi-
larly, increasing the threshold increases security (i.e., de-
creases the attacker’s success probability).
Availability is also affected by the VDO parameters
and the tradeoff is shown in Figure 7(b). Here we see the
maximum timeout (i.e., the VDO’s lifetime) as a function
of N and the threshold. The maximum VDO timeout is
the largest time at which 99% of a set of 1,000 VDOs
remained available in our experiment. The timeout is
capped by our 10-hour experimental limit. From the ﬁg-
ure, we see that increasing N improves not only security,
but also availability. We also see that smaller thresholds
support longer timeouts, because the system can toler-
(a) Parameters and security.
(b) Parameters and availability.
(c) Tolerated attacker sizes.
(d) Churn effect on availability.
Figure 7: Analysis of the store snifﬁng attack. Fig. (a): the attacker’s success probability with increasing N and key threshold for
a 1000-node DHT with 50 malicious nodes. Larger N and high thresholds (≥ 65%) provide good security. Fig. (b): maximum VDO
timeout supported for a .99 availability level. Large N with smaller key thresholds (≤ 70%) provide useful VDO timeouts. Fig. (c):
maximum number of attacker nodes that a DHT can tolerate, while none of the 1,000 VDOs we pushed were compromised. Fig. (a),
(b), and (c) assume 2-hour churn. Fig. (d): the single-share availability decreases over time for different churn models in our private
network and for the real Vuze network.
ate more share loss. The choice of threshold thus in-
volves a tradeoff between security and availability: high
thresholds provide more security and low thresholds pro-
vide longer lifetime. For example, if a lifetime of only 4
hours is needed — which might be reasonable for certain
emails or SMSs — then choosing N = 50 and threshold
75% leads to good security and performance. If a timeout
of 8 hours is required, N = 100 and threshold of 70% is
a good tradeoff for the 2-hour churn. Thus, by tuning N
and the key threshold, we can obtain high security, good
availability, and reasonable performance in the context
of a small 1,000-node DHT and 5% attackers.
Attacker Sizes. We now consider how many attacker
nodes a DHT deployment of a given size can toler-
ate with small chance that the attacker succeeds in pre-
obtaining a sufﬁcient number of shares for any VDO.
Figure 7(c) shows the maximum attacker sizes tolerated
by DHTs of increasing sizes, for various key thresh-
olds. The values are calculated so as to ensure that none
of the 1,000 VDOs we experimented with was compro-
mised. We computed these values from experiments us-
ing N = 150, 2-hour churn, and various attacker sizes
for each DHT size. For an 8,000-node DHT, even if 600
nodes are controlled by a store-snifﬁng attacker, the ad-
versary would still not obtain any of our 1,000 VDOs.
More important, Figure 7(c) suggests that the num-
ber of attackers that the DHT can tolerate grows linearly
with DHT size. Assuming this trend continues further,
we estimate that, in a 1M-node DHT, an attacker with
35,000 nodes would still have less than 10−3 probability
of recording a sufﬁcient number of shares to compromise
a single VDO with N = 150 and a threshold of 70%.
We have also experimented with a different metric
of success: requiring an attacker to obtain enough key
shares to compromise at least 25% of all VDOs. Con-
cretely, for N = 150 and a threshold of 80%, our exper-
iment with a 8,000 node DHT required the attacker to
control over 710 nodes. This value also appears to grow
linearly in the size of the DHT; extrapolating to a 1M-
node DHT, such an attack would require at least 80,000
malicious nodes. We believe that inserting this number of
nodes into the DHT, while possible for limited amounts
of time, is too expensive to do continuously (we provide
a cost estimate below).
Finally, we note that our refresh mechanism for ex-
tending Vuze timeouts (explained in Section 4) provides
good security properties in the context of store snifﬁng
attacks. Given that our mechanism pushes new shares
in each epoch, an attacker who fails to capture sufﬁcient
shares in one epoch must start anew in the next epoch
and garner the required threshold from zero.
Setting Parameters for the Vuze Network. These re-
sults provide a detailed study of the store snifﬁng attack
in the context of a 2-hour churn model induced on a pri-
vate Vuze network. We also ran a selected set of similar
availability and store attack experiments against a pri-
vate network with a 3-hour churn model, closer to what
has been measured for Vuze.4 The resulting availability
curve for the 3-hour churn now closely resembles the one
in the real Vuze network (see Figure 7(d)). In particular,
for both the real network and the private network with
a 3-hour churn model, a ratio of 90% and N ≥ 20 are
enough to ensure VDO availability of 7 hours with .99
probability. Thus, from an availability standpoint, the
longer lifetimes allow us to raise the threshold to 90% to
increase security.
From a security perspective, our experiments show
that for an 8,000-node DHT, 3-hour churn model, and
VDOs using N = 50 and threshold 90%, the attacker re-
quires at least 820 nodes in order to obtain ≥25% of the
VDOs. This extrapolates to a requirement of ≈87,000
nodes on Vuze to ensure attack effectiveness. Return-
ing to our cost argument, while cloud computing in a
system such as Amazon EC2 is generally deemed in-
4We used VDOs of N = 50 and thresholds of 90% for these experi-
ments.
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 50 60 70 80 90 100Probability of VDO compromiseKey threshold (%)N=1N=10N=20N=50N=100N=150 0 1 2 3 4 5 6 7 8 9 10 50 60 70 80 90 100Maximum VDO timeout (h)Key threshold (%)N=1N=10N=20N=50N=100N=150 0 100 200 300 400 500 600 1000 2000 3000 4000 5000 6000 7000 8000Maximum attacker size toleratedDHT sizeThreshold=80%Threshold=70%Threshold=60%Threshold=50% 0 20 40 60 80 100 0 1 2 3 4 5 6 7 8% of shares still available at time TTime T (h)Vuze networkLifetime=3hLifetime=2hLifetime=1hexpensive [18], the cost to mount a year-long 87,000-
node attack would be over $860K for processing and In-
ternet trafﬁc alone, which is sufﬁciently high to thwart
an adversary’s compromise plans in the context of our
personal use targets (e.g., seeking sensitive advice from
friends over email). Of course, for larger N (e.g., 150), an
attacker would be required to integrate even more nodes
and at higher cost. Similarly, the cost of an attack would
increase as more users join the Vuze network.
Overall, to achieve good performance, security and
availability, we recommend using N = 50 and a thresh-
old of 90% for VDOs in the current Vuze network. Based
on our experiments, we conclude that under these param-
eters, an attacker would be required to compromise be-
tween 8—9% of the Vuze network in order to be effective
in his attack.
6.2.2 The Lookup Snifﬁng Attack
In addition to seeing store requests, a DHT-integrated
adversary also sees lookup requests. Although Vuze
only issues lookups prior to storing and getting data
objects, the lookups pass through multiple nodes and
hence provide additional exposure for VDO key shares.
In a lookup snifﬁng attack, whenever an attacker node re-
ceives a lookup for an index, it actively fetches the value
stored at that index, if any. While more difﬁcult to handle
than the passive store attack, the lookup attack could
increase the adversary’s effectiveness.
Fortunately, a simple, node-local change to the Vuze
DHT thwarts this attack. Whenever a Vanish node wants
to store to or retrieve a value from an index I, the node
looks up an obfuscated index I0, where I0 is related to but
different from I. The client then issues a store/get for
the original index I to the nodes returned in response to
the lookup for I0. In this way, the retrieving node greatly
reduces the number of other nodes (and potential attack-
ers) who see the real index.
One requirement governs our simple choice of an ob-
fuscation function: the same set of replicas must be re-
sponsible for both indexes I and I0. Given that Vuze has
1M nodes and that IDs are uniformly distributed (they
are obtained via hashing), all mappings stored at a cer-
tain node should share approximately the higher-order
log2(106) ≈ 20 bits with the IDs of the node. Thus,
looking up only the ﬁrst 20b of the 160b of a Vuze in-
dex is enough to ensure that the nodes resulted from the
lookup are indeed those in charge of the index. The
rest of the index bits are useless in lookups and can be
randomized, and are rehabilitated only upon sending the
ﬁnal get/store to the relevant node(s). We conserva-
tively choose to randomize the last 80b from every index
looked up while retrieving or storing mappings.
Lacking full index information, the attacker would
have to try retrieving all of the possible indexes starting
with the obfuscated index (280 indexes), which is impos-
sible in a timely manner. This Vuze change was trivial
(only 10 lines of modiﬁed code) and it is completely lo-
cal to Vanish nodes. That is, the change does not require
adoption by any other nodes in the DHT to be effective.
6.2.3 Standard DHT Attacks
In the previous sections we offered an in-depth analysis
of two data conﬁdentiality attacks in DHTs (store and
lookup snifﬁng), which are speciﬁc in the context of our
system. However, the robustness of communal DHTs to
more general attacks has been studied profusely in the
past and such analyses, proposed defenses, and limita-
tions are relevant to Vanish, as well. Two main types
of attacks identiﬁed by previous works are the Sybil at-
tack [26] and the Eclipse (or route hijacking) attack [60].
In the Sybil attack, a few malicious nodes assume a large
number of identities in the DHT. In the Eclipse attack,
several adversarial nodes can redirect most of the trafﬁc
issued by honest nodes toward themselves by poisoning
their routing tables with malicious node contact informa-
tion [60].
The Vuze DHT already includes a rudimentary de-