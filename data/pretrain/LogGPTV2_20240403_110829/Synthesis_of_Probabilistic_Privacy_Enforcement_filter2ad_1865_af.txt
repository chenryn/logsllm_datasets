(a) Genomic Data
(9,1)
(9,9)
sum
read
SynSMT (Perm)
noisy-sum
(3,1)
(3,3)
(6,1)
(6,6)
(9,1)
(9,9)
(12,1)
(12,12)
SynSMT (Ans)
(3,1)
(3,3)
(6,1)
(6,6)
(9,1)
(b) Social
(9,9)
(12,1)
(12,12)
Figure 9: Running times of the synthesis algorithm SynSMT.
For both (a) and (b) we present two plots: the first shows run-
ning times of SynSMT optimizing for permissiveness and
the second SynSMT optimizing for answer precision.
separately for each. All numbers reported below are averaged over
10 runs.
8.2.1
SynSMT vs. SynGrd. To compare the two synthesis algo-
rithms, we measure: (i) the percentage of instances where SynGrd
produces an optimal enforcement with respect to the permissive-
ness and the answer precision optimality orders, and (ii) how close
to the optimal is the enforcement synthesized by SynGrd. We can
measure these whenever SynSMT terminates within the timeout,
since the synthesized enforcement is guaranteed to be optimal.
Permissiveness. The percentages of instances where SynGrd pro-
duces an optimal enforcement with respect to permissiveness mea-
sured across all instances of the medical and social scenarios are
Session B4:  Privacy PoliciesCCS’17, October 30-November 3, 2017, Dallas, TX, USA401]
s
[
e
m
T
i
400
300
200
100
0
1K
2K
3K
4K
5K
6K
7K
8K
9K
10K
Number of outputs
Figure 10: Running times of SynGrd for large inputs.
82.35% and 95%, respectively. We do not know this percentage for
the location scenario instances because SynSMT did not terminate.
To measure how close to the optimal enforcement the enforce-
ments synthesized by SynGrd are with respect to permissiveness,
we measured |O/ξSynGrd|
|O/ξSynSMT| for each instance, where ξSynGrd and
ξSynSMT are the enforcements synthesized by SynGrd and SynSMT,
respectively. This indicates the amount of classes given by SynGrd
as a fraction of the optimal enforcement. The average percentages
across all instances of the medical and social scenarios are 91.11%
and 98.33%, respectively. As before, we could not measure this num-
ber for the location scenario as SynSMT timed out for all location
instances.
Overall, these three metrics indicate that for our examples Syn-
Grd often produced an optimal enforcement with respect to per-
missiveness or an enforcement that is close to the optimum.
Answer precision. For all synthesis instances where SynSMT
produced an enforcement within the timeout, SynGrd produced
an optimal enforcement with respect to answer precision, i.e., with
the same amount of singletons.
8.2.2 Performance. The total running time of Spire for each
instance consists of: (i) Bayesian inference (done by Psi) and (ii)
synthesis time (done by either SynSMT or SynGrd). For each in-
stance we measured the running times of Psi and the synthesis
algorithms SynSMT and SynGrd.
Bayesian Inference. The running times of Psi are shown in Fig-
ure 8. We observe that Psi running times increase with the size
of the input set I. Our synthesis algorithms, however, are inde-
pendent of the inference engine and we can thus directly benefit
from any advances in this area. As an interesting item for future
work, we plan to experiment with sound probabilistic abstractions,
as in [37].
SynSMT. The size of the SMT constraints, stored in the SmtLib2
format, ranged up to 78KB for the medical scenario, up to 263KB for
social scenario, and up to 18MB for the location scenario. We plot
the performance of SynSMT in Figure 9, where the X-axis shows
the instance and the Y-axis the time in seconds. Each line in the
plots shows the times for a particular program.
The plots indicate that SynSMT is time-demanding, since its
running time increases roughly exponentially in the size of the
set of outputs. This explains the almost constant running times
medical
social
location
1
|Oi/ξi |
|Oi |
0.5
0
0
5
10
15
Iteration number
20
a) Ratio of classes count to outputs count for each iteration
medical
social
location
1
|Oi |
|O1|
0.5
0
0
5
10
15
Iteration number
20
b) Number of outputs with non-zero probability in each iteration
as a ratio of the total number of outputs
Figure 11: Permissiveness for Interactive Attackers
of SynSMT for the “read” programs in the medical and the social
scenario, where the output size is constant.
SynGrd. SynGrd always finishes well below one second, hence
the synthesis of permissive enforcements is feasible on large in-
stances. We do not explicitly plot SynGrd’s performance on our
benchmarks; instead, next we present results that demonstrate how
SynGrd scales to large synthesis instances.
Scaling to Large Synthesis Instances. To evaluate SynGrd’s
scalability, we randomly generated large instances. The distribution
δ (O = o) was chosen by first assigning each output an
of outputs Pπ
uniformly randomly chosen value from [0, 1] and then normalizing
the values to get a distribution. We used a policy with one secret,
and a security assertion with bounds [0.35, 0.65]. The probabilities
δ (I ∈ S | O = o) ware chosen
of secret for the individual outputs Pπ
uniformly randomly from [0, 1].
The running times are shown in Figure 10 in seconds. The figure
demonstrates that SynGrd runs in quadratic time in the number of
outputs |O|.
8.2.3 Comparison to Conservative Approaches. To evaluate the
general permissiveness of our approach, we compare Spire to the
conservative approach of [37], which rejects the attacker’s program
whenever it does not satisfy the policy. To evaluate this, for each
Session B4:  Privacy PoliciesCCS’17, October 30-November 3, 2017, Dallas, TX, USA402instance with a program π, an attacker belief δ, and privacy pol-
icy Φ, we checked whether π , δ |= Φ. This indicates whether the
conservative approach rejects the attacker’s program.
The percentage of rejected programs by the conservative ap-
proach is 100% for the medical, 70.84% for the social, and 75% for
the location scenario.
8.2.4 Permissiveness for Interactive Attackers. To evaluate the
iterative mode of Spire, we iterated one representative program
from each scenario. In Figure 11a, we measured the relative permis-
siveness for each iteration and each program. The X-axis indicates
the iteration number, and the Y-axis the ratio of equivalence classes
of the enforcement to the number of outputs with non-zero prob-
, where Oi is the set of outputs with non-zero
ability, i.e.
probability in the i-th iteration and ξi is the enforcement synthe-
sized in the i-th iteration. In Figure 11b, we plot how the number
of outputs with a non-zero probability reduces as the attacker be-
|Oi |
|O1| , where Oi is the
lief evolves in each consecutive iteration, i.e.
number of outputs with non-zero probability in the i-th iteration.
Initially, the program form the medical scenario has 13 outputs in
the first iteration, social has 7, and location has 169.
|Oi/ξi |
|Oi |
9 RELATED WORK
We survey the works that are most closely related to ours.
Probabilistic Privacy Enforcement. Mardizel et al. [37] investi-
gate the problem of enforcing privacy policies that are formalized
as thresholds on the attacker belief. In [37], the key contribution
is the probabilistic polyhedra abstract domain, which is used to
efficiently implement (based on abstract interpretation [15]) sound
approximate inference. In contrast to [37], we focus on synthesizing
an optimally permissive enforcement that enforces the given pri-
vacy policy. The focus of [37] is to efficiently check π , δ |= Φ for a
program π, an attacker belief δ, and a policy Φ, while our focus is to
find an optimally permissive enforcement ξ with Enf(π , ξ), δ, |= Φ.
An interesting direction for future work would be to combine our
approach with the abstractions of [37].
Guarnieri et al. [27] instantiates Mardziel et al.’s framework [37]
to the database setting, where programs consist of relational calcu-
lus queries and the attacker belief is formalized using probabilis-
tic logic programs. They develop a provably secure enforcement
mechanism that prevents information leakage in the presence of
probabilistic dependencies. Their mechanism leverages a dedicated
inference engine for a class of probabilistic logic programs to ensure
tractability.
Besson et al. [7] randomize the program’s inputs (while Spire ran-
domizes over outputs) to enforce privacy in the context of browser
fingerprinting by synthesizing a new program using linear pro-
gramming. The two approaches are non-comparable (even though
they both encode bounds on probabilities as linear programs): (i)
assertions: Spire supports arbitrary assertions over the attacker
belief, while [7] considers a specific one (related to browser fin-
gerprinting), (ii) reduction: Spire reduces to a linear optimization
problem over SMT, while the algorithm of [7] reduces to an LP
problem, and (iii) Spire offers a full end-to-end implementation
while [7] does not offer any implementation or a system.
Language-based Security. Standard non-interference notions
have been extended to support probabilities [42, 55] for concur-
rent programs. Our security notion, however, differs from non-
interference in that it allows leaks of sensitive information as long
as these do not violate the privacy policy.
Schoepe et al. [47] formalize opacity, a security property that
allows any leak except leaking whether a secret holds. Our privacy
policies extend opacity to the probabilistic setting. Moreover, since
we synthesize the most permissive secure enforcement, a program
secured by Spire returns meaningful results even if it is not opaque.
Recently, the language-based security community focused on
Quantitative Information Flow [10, 48], where the goal is to quantify
the amount of information leaked by a program. Our goal is not to
quantify the amount of leaked information. Instead, we synthesize
the enforcement that prevents only the information leaks that do
not conform to the privacy policy.
Opacity for Discrete Event Systems. There has been a growing
interest in opacity for Discrete Event Systems (DES) [32], where sys-
tems are usually formalized as Labelled Transition Systems (LTSs)
and secrets as predicates over runs. Many flavors of opacity have
been studied [12, 43] and recent approaches extend opacity to prob-
abilistic systems [5, 6, 44]. The DES community focused on (i)
verifying opacity [5, 28, 44], (ii) synthesis of mechanisms that en-
force opacity [18, 57], and (iii) runtime enforcement of opacity [21].
While verification techniques exist for both deterministic [28] and
probabilistic systems [5, 44], existing synthesis techniques support
only deterministic secrets [18, 21, 57].
Differential Privacy. Differential Privacy [20] has emerged as a
standard for protecting statistical databases and privacy-preserving
data analysis. A differentially private computation ensures that the
presence (or absence) of an individual’s data in the input has a little
(and bounded) impact on the output. Differential Privacy makes no
assumptions on the attacker belief. In contrast, we assume that the
attacker belief is known, and we synthesize the most permissive
enforcement relation that complies with the privacy policy.
Probabilistic Programming. Numerous probabilistic program-
ming languages have been developed in the past few years: Stan [24],
PSI [22], Fabular [11], Anglican [56], Church [25], Venture [36],
R2 [38]. These languages support different inference methods (e.g.
via translation to Bayesian Nets, sampling, exact). For a detailed sur-
vey see [26]. Abstractions for probabilistic programming have been
also developed; see [37]. In our work, we leverage existing infer-
ence engines to compute the amount of information leaked by the
program. The recent developments in probabilistic programming
languages are thus orthogonal and beneficial to our approach.
10 CONCLUSION
We introduced the problem of optimal privacy enforcement syn-
thesis. The goal is to automatically synthesize an enforcement that
transforms a program into a policy-compliant one. We showed that
determining the amount of leaked information by the program
can be done by probabilistic analysis using existing probabilistic
programming engines.
We proved that finding an optimally permissive enforcement is
NP-equivalent, and presented an algorithm that reduces the enforce-
ment synthesis problem to a linear optimization problem over SMT
Session B4:  Privacy PoliciesCCS’17, October 30-November 3, 2017, Dallas, TX, USA403constraints. We also presented a sound greedy synthesis algorithm
that runs in quadratic time.
We presented the Spire system, an end-to-end implementation
of our synthesis approach. We evaluated Spire on 10 relevant pro-
grams from different application domains. Our results demonstrate
that permissive privacy enforcement synthesis is feasible and that
Spire can handle nontrivial programs. Further, our greedy synthesis
performs well on all of our examples: it synthesizes an enforcement
in < 1s, the synthesized enforcements are optimal in 89% of the syn-
thesis instances, and overall the enforcements are 95% as permissive
as the optimal enforcements.
In the future, we plan to instantiate and optimize the core SPIRE
approach for specific application domains (e.g., network privacy), as
well as investigate a possible generalization of our permissiveness
theorems.
A PROOFS OF THEOREMS
In this appendix, we give proofs for the theorems.
A.1 Complexity
To prove Theorem 4.2, we show that the permissive privacy enforce-
ment synthesis problem (i.e. the optimal enforcement problem with
permissiveness chosen as optimality) is both NP-easy and NP-hard.
Lemma A.1. The permissive privacy enforcement synthesis prob-
lem is NP-easy.
Proof. To prove that the problem is in FPNP, we first define a
decision version of the permissive policy enforcement synthesis
problem that we will then use as an oracle.
An instance of the decision problem is a tuple(I, O, δ, π , Φ, N , α)
where I is the input set, O is the output set, π is the probabilistic
program over these sets, Φ = {φ1, . . . , φℓ} is a privacy policy where
φi = (Si ,[ai , bi]), N ∈ N, and α ⊆ O × O. The problem then
asks whether there is an equivalence relation ξ over O such that
Enf(π , ξ), δ |= Φ, |ξ| = N , and α ⊆ ξ.
This decision problem is in NP since a non-deterministic Turing
machine can non-deterministically guess a relation R over the set O,
and then check whether R is an equivalence relation, R enforces the
policy Φ, |R| = N , α ⊆ R, and accept only if all of these conditions
are met, which can be checked in polynomial time.
We now show that the functional problem is in FPNP by provid-
ing a polynomial algorithm with an oracle for the decision problem: