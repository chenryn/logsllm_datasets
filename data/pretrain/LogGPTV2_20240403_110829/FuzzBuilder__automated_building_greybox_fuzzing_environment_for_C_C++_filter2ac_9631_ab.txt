or not. For a successful unit test, it is recommended for a unit test
to cover every possible program path at least once. This means that
a unit test also needs high code coverage.
A unit test should consider function sequences to explore various
program paths. Further, a unit test needs a variety of test inputs,
that are used as a parameter to call a specific function. Test inputs
can affect code coverage as a control flow of a function depends on
its parameters.
In other words, if a unit test is written well, it means that various
function sequences and test inputs are already available in the unit
test. These attributes satisfy the requirements of our suggestion.
629Therefore, we propose a method to generate executables and seeds
by using prepared function sequences and test inputs in unit tests.
Although this approach requires a project to have a unit test, it is
practical because most of the projects already have it, as the Table
2 shows.
2.5 Fuzzable API
We define FA (Fuzzable API) as a base library API function for
library fuzzing. FA is used to pass input values generated by fuzzers
to a target library. Further, FA is used to extract function sequences
and seeds from unit tests. CBS_init and yr_compiler_add_string in
Table 3 are examples of FA.
Table 3: The example of Fuzzable API
library
function
boringssl CBS_init
yara
yr_compiler
_add_string
parameters
[1] CBS* cbs
[2] const uint8* data
[3] size_t len
[1] YR_COMPILER* compiler
[2] const char* rules_string
[3] const char* namespace_
For fuzzing boringssl, it is necessary to call CBS_init function with
a buffer address that stores random values and its size through the
second and the third parameter. In the case of yara, it is necessary
to call yr_compiler_add_string function with only a buffer address
through the second or the third parameter.
3 METHODOLOGY
3.1 User Configuration
At first, users should provide unit test files and FA to FuzzBuilder. A
unit test file is an LLVM bitcode file whose format is an intermediate
representation of LLVM framework. Although this step requires
human interaction, this is a relatively simple step that does not
require in-depth knowledge of a target library. Therefore, this does
not affect the overall automation process.
FA. In the case of FA, users should provide information about FA
and its parameters that are used for input values. Some FAs require
both a buffer address and its size, while others require only a buffer
address as a parameter. Below is an example to specify FAs in Table
3.
(1) CBS_init 2 3
(2) yr_compiler_add_string 2
The first example shows how to specify CBS_init in user con-
figuration. In this example, FA name is CBS_init, the second pa-
rameter is specified for a buffer address of an input value, and the
third parameter is specified for its size. The second example is for
specifying yr_compiler_add_string. In this example, FA name is
yr_compiler_add_string and the second parameter is specified for
a buffer address of an input value. It is not necessary to provide
a buffer size for yr_compiler_add_string. In this case, a buffer is
considered a string, which is null-terminated.
Test functions. Users can choose test functions. A test function
is one that implements a unit test case, which is a target that is to be
analyzed by FuzzBuilder. There are many functions in source code
of unit tests, and only some of them are test functions. Therefore,
FuzzBuilder needs to know which functions are test functions. If a
unit test is based on a popular unit test framework such as google
test [3], test function names have a specific pattern. Hence, it is
not challenging to identify them. However, if a unit test is written
without any convention, it is needed to specify their name manually
because there is no clear way to identify test functions.
Thus, FuzzBuilder supports configurable option for identifying
test functions by specifying their names one by one. However, this
configuration is time-consuming because there are a lot of test
functions in unit tests. Fortunately, most projects use test func-
tion names with a specific prefix or postfix such as test_A, test_B,
A_test, and B_test. Therefore, to alleviate this problem, FuzzBuilder
supports asterisk (*).
Functions to be skipped. Users can choose functions that need
to be skipped. Unfortunately, execution of some test functions may
take a lot of time. For example, excessive loops in a test function
can cause this problem. The efficiency of greybox fuzzing decreases
if execution speed is slow. It is because greybox fuzzing needs a lot
of execution of a program to test it with various input values that
they generated. Thus, this configuration helps in efficient fuzzing
by skipping such functions.
3.2 Automated generation of an executable
It is assumed that the following two conditions are satisfied to
generate executables through unit tests.
• Each test is implemented as a function.
• Each test is independent of each other.
These conditions are mentioned in the best practice guides of
JUnit [6], which is a popular unit test framework for Java. More-
over, most of unit tests that are based on google test satisfy these
conditions. Hence, these conditions do not affect the practicality of
the proposed method adversely.
Algorithm 1 The overall process of FuzzBuilder to generate an
executable from unit tests
1: procedure FuzzBuilder(f unctions)
tests, entry ← preprocess(f unctions)
2:
entry ← insert_interface(entry)
3:
for all test ∈tests do
4:
5:
6:
7:
8:
9:
10:
11:
test ← insert_operands(test)
test ← remove_test(test)
end for
modify(entry, tests)
if is_necessary(test) then
else
end if
Algorithm 1 shows the overall process. A detailed description of
each process is provided in the followings.
6303.2.2
3.2.1 preprocess. This process extracts the entry function and
test functions from every function in LLVM bitcode files. Test func-
tions are extracted by using specific patterns if a unit test is based
on a popular test framework. Otherwise which, they are extracted
based on user configuration. An entry function is main function.
insert_interface. This process inserts an interface that
loads input values that are generated by fuzzers to memory. At
first, two new global variables are added to store both an input
value and its size. Then, a set of instructions is inserted in the
extracted entry function to achieve the following goals.
• Obtaining an input value from a fuzzer.
• Allocating sufficient space to an added global variable.
• Copying the input value to the added global variable.
• Storing an input size in another added global variable.
stdin or a specific file can be used to obtain an input value from a
fuzzer because most of the greybox fuzzers use them as an interface.
This feature can ensure the compatibility of generated executables
with popular greybox fuzzers such as AFL.
3.2.3
is_necessary. This process identifies test functions that
include instructions to call a specified FA. This can be achieved by
traversing instructions in test functions. Then, insert_operands is
performed if a test function has an instruction to call a specified
FA, otherwise which remove_test is performed. If a test function is
one that specified in Functions to be skipped in User Configuration,
remove_test is always performed.
3.2.4
insert_operands. This process changes parameters of FA
in test functions into the added global variables in 3.2.2. User Config-
uration that is explained in 3.1 is used to identify which parameters
should be changed. By this process, instructions in a library can be
executed with an input value generated by a fuzzer.
3.2.5
remove_test. This process removes unnecessary test func-
tions. It is necessary to remove these functions because they can
decrease execution speed of generated executables. Execution speed
has a significant impact on fuzzing performance because fuzzing
needs to be performed with input values as many as possible. It is
safe to delete test functions because we have assumed that one test
function does not affect the other test functions.
3.2.6 modify. This process stores the instrumented functions
in bitcode files where they were defined.
These instrumented files are built to an executable according to
a build process of a project. As a result, an executable is generated
with an interface to obtain input values from fuzzers. These input
values are used as parameters of FA. Finally, input values from
fuzzers can be used to explore library code to discover bugs.
Note that, the above processes work during a compilation of unit
test executables. Therefore, several executables can be generated if
a project has more than one unit test executable.
3.3 Automated generation of seed files
Seeds can be extracted from test inputs that are used as parameters
of FA in unit tests. Most of the projects usually store test inputs in
the following way for their unit tests.
• Source code.
• Script files for executing unit testing.
• Extra files in a project repository.
Various analyzers are required to extract test inputs from a unit
test statically. For example, if test inputs are in C source code,
then static analyzer for C source code is required. However, it is
not practical to prepare all types of the static analyzer. Thus, we
use a dynamic approach to collect test inputs during a unit test is
working.
For this, FuzzBuilder instruments FA in a library to store values
of its parameter to a specific file. Figure 5 shows how this instru-
mentation works. In this code, input parameter denotes a specific
buffer address and size parameter denotes its size. Instrumented
instructions are for storing values in input buffer to “file.txt”. By
this instrumentation, every test input is collected into a specific
file while a unit test is working. Finally, seed files are generated
by dividing each seed into separated files. The overall process is
shown in Fig 4.
4 EVALUATION
Experiments in this paper are designed to evaluate FuzzBuilder
under the following criteria.
for library fuzzing.
• The efficiency of generated seeds by FuzzBuilder.
• The effectiveness of generated executables by FuzzBuilder
• The effectiveness of FuzzBuilder as a bug finding tool.
For the experiment, we chose a few library projects from OSS-
Fuzz. Each project had executables and seeds to fuzz their libraries.
Most of the executables and seeds were prepared by developers who
have in-depth knowledge of libraries. FuzzBuilder was evaluated
by comparing with these executables and seed files.
We chose projects under the following conditions.
(1) Project should support 32-bit build.
(2) Project should support at least one unit test that includes FA
(3) Project should have a unit test that satisfies the conditions
used in fuzzing.
that are addressed in 3.2.
In this experiment, AFL 2.52b, which is the latest version of
a popular greybox fuzzer was used. To discover unknown bugs
efficiently, we built libraries with an address sanitizer [28]. Unfor-
tunately, it seems that AFL does not support a 64-bit executable
with an address sanitizer [10]. Therefore, we chose only projects
that satisfy the first condition.
The second condition is considered to compare FuzzBuilder with
OSS-Fuzz. Executables prepared for fuzzing in OSS-Fuzz were based
on a specific FA. To compare FuzzBuilder with them, FuzzBuilder
should generate an executable, which is based on the same FA.
This means that unit tests in the selected projects should have test
functions, which calls the same FA. If such test functions do not exist,
then FuzzBuilder cannot generate an executable for comparison.
Therefore, this condition was considered to choose projects. The
third condition is to remind the conditions for unit tests that are
addressed in 3.2.
Four projects were selected from OSS-Fuzz under these condi-
tions. They are c-ares [2], expat [1], boringssl [8], and yara [4]. Table
4 lists commits of each project used in this experiment.
631User
Configuration
Instrumented
Library Code
FuzzBuilder
Library Code
Linker
Unit test
Objects
Unit test
Executable
Logs
Seed files
Figure 4: Overall process to generate seed files
1
2
3
4
5
6
FA ( char * input , size_t size , ...) {
int fd = open (" file . txt " , ...) ; // instrumented
write (fd , input , size ); // instrumented
close ( fd ); // instrumented
...
}
Figure 5: The example of instrumentation for seed genera-
tion
Table 4: Tested projects and commit number
project
OSS-Fuzz
c-ares
expat
boringssl
yara
commit
a55a1276d9e0c453f588160b7e3581cdf6236013
a9c2068e25a107bf535b1fc988eec47384b86dc6
39e487da353b20bb3a724311d179ba0fddffc65b
d2a0ffdfa781dd6fde482ccb924b4a756731f238
a3784d3855029bd0ad24071e72746cc0c31b8cba
Line coverage and the number of discovered bugs are used as met-
rics for evaluating FuzzBuilder. These two metrics are commonly
used to evaluate fuzzing performance in recent fuzzing researches
[20]. Line coverage was measured by llvm-cov gcov. Note that, only
library code coverage was measured because each executable is
generated from different test code.
This experiment was held on Intel (R) Core (TM) i7-9700K CPU,
Debian GNU / Linux 9.5 OS.
4.1 Automatically generated seeds
For this experiment, we collected all FAs, which were used in both
unit tests and fuzzing. Then, we generated seed files based on the
collected FAs by using FuzzBuilder. We then used AFL-cmin, which
is a corpus minimization tool provided along with AFL to remove
duplicate seeds. Duplicated seeds are ones that explore the same
path in a program. FuzzBuilder can generate such duplicated seeds
because seeds are originally used as test inputs for unit tests.
We measured line coverage of libraries when seed files of FuzzBui-
lder and OSS-Fuzz were used, respectively. Table 5 provides informa-
tion about executables used in this experiment. executable column
denotes executable names provided by OSS-Fuzz. FA column de-
notes FA names. Because there are many executables and their
names are quite long, we have assigned an ID to each of them to re-
fer to them conveniently. Note that ares_parse_* which is the FA of
c2, refers to an abbreviation of the 9 FAs starting with ares_parse_.
Fig 6 shows line coverage of seeds from FuzzBuilder and OSS-
Fuzz respectively. Note that, in the case of expat, there was no
Table 5: Prepared executables in OSS-Fuzz and their ID and
FA
project
ID
c1
c2
project
ID
e1
e2
e3
e4
e5
e6
project
ID
b1
b2
b3
b4
b5
b6
b7
b8
b9
b10
b11
b12
project yara
ID
y1
y2
y3
y4
y5
executable
dex_fuzzer
dotnet_fuzzer
elf_fuzzer
pe_fuzzer
rules_fuzzer
FA
ares_create_query
ares_parse_*
FA
c-ares
executable
ares_create_query_fuzzer
ares_parse_reply_fuzzer
expat
executable
parse_ISO_8859_1_fuzzer XML_Parse
XML_Parse
parse_US_ASCII_fuzzer
XML_Parse
parse_UTF_16BE_fuzzer
parse_UTF_16_fuzzer
XML_Parse
XML_Parse
parse_UTF_16LE_fuzzer
parse_UTF_8_fuzzer
XML_Parse
boringssl
executable
bn_div
bn_mod_exp
client
dtls_client
dtls_server
pkcs12