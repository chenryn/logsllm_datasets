### botTorHosting1 Configuration and Performance

The value of `botTorHosting1` is 1.29(3 × 10) ≈ 39, which is more than sufficient for our purposes.

Compared to using PlanetLab nodes, a disadvantage is that we can only control one Onion Router (OR) in our circuit, specifically the middle OR. This means that the results will only reflect the improvement of prioritization at a single hop. However, as we will see, the results still show a noticeable improvement in our favor.

### Experimental Setup

The target file, hosted at the University of Waterloo, has a size of 87 KB. We did not introduce artificial bulk transfer traffic, making this experiment representative of normal user experience. We used `webfetch` [2] to fetch the file via our configured circuit, with a 20-second break between each successive fetch. The experiments were conducted in March 2010.

It is important to note that we only gathered timing data from our own client and did not measure any traffic belonging to other users on the live Tor network.

### Experimental Results

We performed the experiment during different periods of the day, executing 250 downloads with `planetgurgle` configured in both unprioritized and prioritized modes. The cumulative distribution function (CDF) of the results is shown in Figure 7.

Interestingly, the graph is quite similar to our traffic simulation tests on PlanetLab (Figure 3). With our prioritization algorithm enabled, the median download time decreased from 11.49 seconds to 9.04 seconds.

One observation is that the test results vary significantly depending on the time of day. Latencies are much lower in the afternoons (Eastern Time, ET; the timezone of New York and Toronto) compared to around midnight ET. This suggests that most Tor users may come from the other half of the globe. According to [9], only a small percentage of Tor users are from North America and South America combined.

To better observe the effectiveness of our patch, we divided the results into two groups: a "fast" group for afternoon tests and a "slow" group for midnight tests. The CDFs for these groups are shown in Figures 8 and 9. These figures indicate that under various network conditions, our algorithm provides a noticeable improvement for bursty HTTP downstream traffic.

### Effects on Bulk Transfer

Our new scheduling algorithm should not degrade the performance of bulk transfers to any significant extent. By Little's Law [7], \( L = \lambda W \), where \( L \) is the queue length (average number of cells in the queue), \( \lambda \) is the arrival rate (long-term throughput), and \( W \) is the average time a cell spends in the queue (latency). Our algorithm only changes the order of cells within a queue, thus not affecting \( L \) and \( W \). Assuming the buffers are large enough, the long-term throughput for bulk transfers should remain the same.

We experimentally compared the performance of bulk transfer circuits on our live node. Using our Tor client, we continuously fetched a 4 MB file hosted at the University of Waterloo, with 200 trials for each of unprioritized and prioritized Tor. The results are shown in Figure 10.

From the CDF, we see very little effect of our algorithm on bulk transfer. The average time cost is 416 seconds for unprioritized Tor (standard deviation 335 seconds) and 419 seconds for prioritized Tor (standard deviation 403 seconds). There is no statistically significant difference in the performance of unprioritized and prioritized Tor. The Kolmogorov-Smirnov (K-S) statistic [15] for the two distributions is 0.065 < \( q_{N} \), where \( N = 200 \) is the sample size. This indicates that the K-S test cannot confirm that the two samples are from different distributions. Additionally, bulk transfers usually take several minutes to complete, and users performing such transfers will have more tolerance for increased delays.

### Overhead

The overhead for our scheduling algorithm mainly lies in the computation of Exponentially Weighted Moving Average (EWMA) values and the cost of acquiring the current system time. This requires extra CPU resources compared to the stock Tor. However, most Tor nodes are limited by network capacity rather than CPU [18]. Our scheduling algorithm will not degrade the performance of these nodes.

However, the Tor maintainers reported that the busiest Tor nodes are CPU-limited. For these nodes, we need to ensure that prioritized Tor does not perform worse than the stock Tor. In our initial local experiments with high network capacity, we found that prioritized Tor performed worse and consumed a high ratio of CPU resources. The frequent calls to `gettimeofday` accounted for the majority of the CPU usage.

We observed that during each write event (when cells are flushed into the output buffer), the differences in flushing times for each cell are usually in the range of a few microseconds. Since we do not need microsecond-level precision for EWMA calculations, we modified the algorithm to acquire the system time only at the beginning of the write event handling process and use a cached value for subsequent acquisitions. This reduced the total number of `gettimeofday` system calls by two orders of magnitude.

After optimization, we performed another local experiment to measure the overhead. The experiment was conducted on a commodity desktop computer with an AMD Athlon 64 X2 Dual Core 5600+ processor, 3.2 GB of memory, and Ubuntu 8.04 operating system. We ran all Tor nodes, including three ORs, two directory authorities, and two OPs, locally, along with the web server. This setup maximally stressed the CPU. We performed 200 trials for both unprioritized and prioritized Tor, where the two clients simultaneously fetched a 5 MB file from the web server. The CPU usage reached 100%, confirming that our nodes were CPU-limited. The CDF of the results is shown in Figure 11 ("Unprioritized" and "Prioritized (list)").

The results showed that the average time cost is 1.66 seconds for unprioritized Tor (standard deviation 0.15 seconds) and 1.69 seconds for prioritized Tor (standard deviation 0.24 seconds). There is no statistically significant difference in the performance of unprioritized and prioritized Tor, indicating that even in CPU-limited scenarios, the scheduling algorithm does not make it significantly slower.

Nick Mathewson of The Tor Project further optimized [11] our algorithm to reduce overhead. Instead of using a circular linked list, active circuits are kept in a min-heap-based priority queue. The computation of EWMA cell counts is also optimized. Only the relative, not the absolute, EWMA cell counts matter. An arbitrary reference time point is chosen, and cell counts are computed relative to that time. This saves the traversal of the list of all circuits and the computation of decayed cell counts for each one. Every so often, the cell counts for all circuits can be renormalized by dividing them by \( V \) and resetting \( V \) to 1. This maintains the same circuit-selection behavior as our unoptimized implementation.

Mathewson’s patch not only reduces the load of the EWMA computation but also reduces the time cost of picking the highest-priority circuit when many circuits coexist in a connection. We performed an experiment to test the overhead with this patch in the CPU-limited scenario. The results are also shown in Figure 11 ("Prioritized (minheap)"). The average time cost is 1.65 seconds (standard deviation 0.16 seconds), also not a statistically significant difference. This version of our algorithm has been committed to the latest version (0.2.1.21) of Tor.

### Fine-Tuning the Algorithm

The parameter \( H \) in our algorithm determines how far back in time we look to calculate the cell counts for the circuits. This time horizon should distinguish bursty HTTP circuits from circuits for continuous data transfer. For PlanetLab experiments, the value of \( H \) does not matter much, as the goal is to make HTTP circuits always have higher priority over bulk transfer circuits.

For live Tor nodes, conditions are more complex. HTTP circuits compete not only with bulk transfer circuits but also with each other. The parameter should meet the requirement of distinguishing the two sets in practical scenarios. The standards may differ from OR to OR, depending on the capacity and network condition. If an OR is slow or \( H \) is set too small, the algorithm will quickly forget a circuit’s past activity, causing a bulk transfer circuit to drop to the same cell count as a newly created HTTP circuit. Conversely, if the OR is fast or \( H \) is set too large, a newly created bulk transfer circuit will be prioritized over an HTTP circuit created long ago.

In this section, we experiment with different values of \( H \) to examine their effects on HTTP traffic.

### Testbed Setup

The testbed setup is similar to our live Tor network test. We selected a variety of \( H \) values for the middle OR, `planetgurgle`, and tested the performance. We used git version 0.2.1.24 of Tor on the middle OR. In the configuration file, the parameter `CircuitPriorityHalflife` represents the interval after which the cell count for each circuit is decreased by half.

In this experiment, we randomly selected a value for \( H \) from the set \{-1, 1.5, 3, 4.5, 10, 20, 33, 66, 99\} for our middle OR (-1 indicates unprioritized), and fetched a small file (87 KB) hosted at the University of Waterloo. We repeated this until each value had 200 data points, and collated the results.

### Experimental Results

The results for download times with different \( H \) values are shown in Figures 12 and 13. Due to the density of the lines, we only show a fraction of the whole CDF in Figure 12. For ease of visualization, in Figure 13, we show the 25th, 50th, and 75th percentile latencies for a range of different \( H \) values (the curves) as well as for unprioritized Tor (the horizontal lines).

The figures show that smaller values of \( H \) (1.5, 3, 4.5) perform only marginally better, if at all, than unprioritized Tor. This makes sense, as the past behavior of a circuit will quickly be forgotten, and bulk transfer circuits will compete with HTTP circuits.