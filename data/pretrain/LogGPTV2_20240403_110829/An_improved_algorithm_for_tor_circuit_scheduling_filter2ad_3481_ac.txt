botTorHosting1 is 1.29(3 × 10) ≈ 39, which is more than
enough for our purpose.
The disadvantage compared to using PlanetLab nodes is
that we could only control one OR in our circuit (the middle
OR), thus the results will only show the improvement of
prioritizing at one hop. However, as we will see later, the
results are still noticeably in our favour.
The target ﬁle to fetch is hosted in the University of Wa-
terloo, with a size of 87 KB. We did not introduce artiﬁcial
bulk transfer traﬃc, so this experiment is representative of
normal user experience. We used webfetch [2] to fetch the
ﬁle using our conﬁgured circuit. There is a 20-second break
between every successive fetch. We ran our experiments in
March 2010.
We note that we only gathered timings from our own
client; we did not measure any traﬃc belonging to other
users of the live Tor network.
3.3.3 Experimental results
We performed the experiment during diﬀerent periods in a
day, executing 250 downloads with planetgurgle conﬁgured
in each of the unprioritized and prioritized modes. The CDF
of the results are shown in Figure 7.
It is interesting to note that this graph is quite similar to
our traﬃc simulation tests on PlanetLab (Figure 3). With
our prioritization algorithm enabled, the median time de-
creased from 11.49 seconds to 9.04 seconds.
One phenomenon we observed is that during diﬀerent pe-
riods in a day, the test results diﬀer markedly. The latencies
are much lower in the afternoons ET (Eastern Time; the
timezone of New York and Toronto) than around midnight
ET. This may indicate that most Tor users come from the
other half of the globe. Indeed, according to [9], only a small
percentage of Tor users come from North America and South
America combined.
In order to better observe the eﬀectiveness of our patch,
we divided the results into two groups: a “fast” group which
were performed during afternoons and a “slow” group which
were performed around midnight. The CDFs are shown in
Figure 8 and Figure 9.
The ﬁgures indicate that under various network condi-
tions, our algorithm makes an observable improvement for
bursty HTTP downstream traﬃc.
3.4 Effects on Bulk Transfer
Our new scheduling algorithm should not degrade the per-
formance of bulk transfer to any noticeable extent. By Lit-
tle’s Law [7], L = λW , where L is the queue length (average
number of cells in the queue), λ is the arrival rate (long term
throughput), and W is the average time a cell spends in the
queue (latency). Our algorithm only changes the order of
cells within a queue, thus does not change L and W . Under
the assumption that the buﬀers are large enough, the long-
term throughput for bulk transfer should stay the same.
We also experimentally compared the performance of bulk
transfer circuits on our live node. We used our Tor client
to continuously fetch a 4 MB ﬁle hosted at the University
of Waterloo. There were 200 trials for each of unprioritized
and prioritized Tor; the results are shown in Figure 10.
From the CDF, we see very little eﬀect of our algorithm
on bulk transfer. The average time cost is 416 seconds for
unprioritized Tor, with standard deviation 335 seconds, and
419 seconds for prioritized Tor, with standard deviation 403
seconds. There is no statistically signiﬁcant diﬀerence in
the performance of unprioritized and prioritized Tor.
In
fact, the Kolmogorov-Smirnov (K-S) statistic [15] for the
two distributions is 0.065 < q 2
N , where N = 200 is the size
of each sample. This indicates that the (two-sample) K-S
test cannot conﬁrm that the two samples are from diﬀerent
distributions. Additionally, as we mentioned earlier, bulk
transfer usually takes at least several minutes to complete,
334and users doing such transfers will have more tolerance of
the increased delay if they ever notice it at all.
3.5 Overhead
The overhead for our scheduling algorithm mainly lies in
the computation of EWMA values, and the cost of acquiring
the current system time. This requires extra CPU resources
compared to the stock Tor. However, most Tor nodes are
limited by the network capacity, not by their CPUs [18].
Our scheduling algorithm will not degrade the performance
of those nodes.
However, the Tor maintainers reported to us that the bus-
iest Tor nodes are in fact CPU-limited. For these nodes, we
need to make sure that prioritized Tor does not perform
worse than the stock Tor. When we completed the ﬁrst
version of the scheduling algorithm, and performed local ex-
periments (with very high network capacity, unlike the Tor
network), we found that it did in fact perform worse than
the stock Tor, and used a high ratio of CPU resources. We
identiﬁed that the frequent calls to gettimeofday accounted
for the majority of the time so consumed. When each cell
is ﬂushed, we need to know the system time in order to cor-
rectly update the EWMA values, but system calls at this
frequency become a burden to the CPU.
We observed that during each write event (when cells are
ﬂushed into the output buﬀer), the diﬀerences in time of
ﬂushing for each of the cells in that write are usually in the
handful of microseconds range. Since we do not need preci-
sion to the microsecond level for the calculation of EWMA
values, we modiﬁed the algorithm so that we only acquire
the system time at the beginning of the write event han-
dling process, and store the time value. The subsequent
acquisitions of system time use the cached value instead. In
this manner, we reduced the total number of gettimeofday
system call by two orders of magnitude.
After the optimization, we again performed a local exper-
iment to ﬁnd the overhead. The experiment was performed
on a commodity desktop computer, with AMD Athlon 64
X2 Dual Core 5600+ processor, 3.2GB memory, Ubuntu
8.04 operating system. We ran all the Tor nodes, including
three ORs, two directory authorities, and two OPs locally.
The web server was also hosted locally. This setup max-
imally stresses the CPU. We performed the experiment in
which the two clients simultaneously fetch a 5MB ﬁle from
the web server. There were 200 trials for both unpriori-
tized Tor and prioritized Tor. During the experiment, the
CPU usage went up to 100%, so that our nodes were indeed
CPU-limited. The CDF of the results is shown in Figure 11
(“Unprioritized” and “Prioritized (list)”).
The results showed that the average time cost is 1.66 sec-
onds for unprioritized Tor, with standard deviation 0.15 sec-
onds, and 1.69 seconds for prioritized Tor, with standard
deviation 0.24 seconds. There is no statistically signiﬁcant
diﬀerence in the performance of unprioritized and prioritized
Tor in the local experiment, which means that even in the
rare scenario that the Tor node is CPU-limited, the schedul-
ing algorithm will not make it signiﬁcantly slower.
Nonetheless, Nick Mathewson of The Tor Project has opti-
mized [11] the implementation of our algorithm to further re-
duce the overhead. Instead of using a circular linked list, the
active circuits are kept in a minheap-based priority queue.
Further, the computation of the EWMA cell counts is op-
timized; noting that only the relative, and not the absolute
Unprioritized
Prioritized (list)
Prioritized (minheap)
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
e
l
i
t
n
a
u
Q
 0
 0
 0.5
 1
Seconds
 1.5
 2
Figure 11: CDF for time cost of downloading a
small ﬁle for unprioritized and prioritized Tor, un-
der CPU-limited scenario
∆t
EWMA cell counts matter (since their purpose is just to
pick the active circuit with the lowest cell count), an ar-
bitrary reference time point is picked, and cell counts are
computed relative to that time. That is, rather than de-
caying every circuit’s stored cell count value by a factor of
H , and then adding 1 for every cell sent, a single new
0.5
value V (representing the current weight of one cell, as com-
pared to a cell sent at the reference time point) is updated
by multiplying it by 0.5−
H . This value V is added
to a circuit’s cell count for each cell it sends. This saves the
traversal of the list of all circuits (not just active circuits)
and a computation of the decayed cell count for each one,
which in our version occurs every time a cell is sent. Every
so often, the cell counts for all circuits can be renormalized
by dividing them all by V , and resetting V to 1. This has
the eﬀect of updating the reference time point to the current
time. It is important to note that this optimized computa-
tion maintains exactly the same circuit-selection behaviour
as our unoptimized implementation.
H = 2
∆t
∆t
Mathewson’s patch not only reduces the load of the EWMA
computation, but also reduces the time cost of picking the
highest-priority circuit when many circuits co-exist in a con-
nection. We performed an experiment to test the overhead
with this patch in the CPU-limited scenario. The results
are also shown in Figure 11 (“Prioritized (minheap)”). The
average time cost is 1.65 seconds, with standard deviation
0.16 seconds, also not a statistically signiﬁcant diﬀerence.
This version of our algorithm has been committed to the
the latest version (0.2.1.21) of Tor.
4. FINE TUNING OF THE ALGORITHM
The parameter H in our algorithm determines how far
back in time we want to look to calculate the cell counts for
the circuits. This time horizon should be chosen to distin-
guish bursty HTTP circuits from circuits for continuous data
transfer. For the PlanetLab experiments, the value of the
parameter does not matter much, since the goal is to make
HTTP circuits always have higher priority over bulk transfer
circuits, and any parameter within a reasonable range will
satisfy.
However, for the Tor nodes on the live network, the con-
335e
l
i
t
n
a
u
Q
 0.8
 0.75
 0.7
 0.65
 0.6
 0.55
 0.5
 0.45
 0.4
 5
)
s
d
n
o
c
e
s
(
y
c
n
e
t
a
L
 10
 8
 6
 4
 2
 0
75%
Median
25%
Unprioritized
H=1.5
H=3
H=4.5
H=10
H=20
H=33
H=66
H=99
 6
 7
 8
 9
 10
 1.5
 3
 4.5
 10
 20
 33
 66
 99
Seconds
H (seconds, log scale)
Figure 12: Comparison of performance for diﬀerent
values of H: CDF
Figure 13: Comparison of performance for diﬀerent
values of H: latency vs. H. Values for unprioritized
Tor are shown by horizontal lines.
ditions are more complex. HTTP circuits are not only com-
peting with bulk transfer circuits, they are competing with
each other as well. The parameter should meet the require-
ment of distinguishing the two sets in practical scenarios.
On the other hand, the standards may diﬀer from OR to
OR, depending on the capacity and the network condition.
For example, if an OR is slow or H is set too small, the al-
gorithm will quickly forget a circuit’s past activity. A bulk
transfer circuit will quickly drop to the same cell count as
a newly created HTTP circuit, and compete with it. On
the other hand, if the OR is fast or H is set too large, a
newly created bulk transfer circuit will be prioritized over
an HTTP circuit created long ago.
In this section, we experiment with diﬀerent values of the
parameter, to examine the eﬀects of the parameter on HTTP
traﬃc.
4.1 Testbed Setup
The testbed setup is similar to the setup of our live Tor
network test. We selected a variety of parameter values for
the middle OR, planetgurgle, and tested the performance.
We used git version 0.2.1.24 of Tor on the middle OR.
In the conﬁguration ﬁle, the parameter CircuitPriority-
Halflife is the H value we mentioned earlier, which repre-
sents the interval after which the cell count for each circuit
is decreased by half.
In this experiment, we randomly select a value for H from
the set {−1, 1.5, 3, 4.5, 10, 20, 33, 66, 99} for our middle OR
(-1 indicates unprioritized), and fetch a small ﬁle (87 KB)
hosted at the University of Waterloo. We repeat this until
each value has 200 datapoints, and collate the results.
4.2 Experimental Results
The results for the download times for diﬀerent values of
H are shown in Figure 12 and Figure 13. Because of the
density of the lines, we only show a fraction of the whole
CDF in Figure 12. For ease of visualizing the data, in Fig-
ure 13, we show the 25th, 50th, and 75th percentile latencies
for a range of diﬀerent H values (the curves) as well as for
unprioritized Tor (the horizontal lines).
The ﬁgures show that smaller values of H (1.5, 3, 4.5)
perform only marginally better, if at all, than unprioritized
Tor. This makes sense, as the past behaviour of a circuit
e
l
i
t
n
a
u
Q
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
Prioritized Webpage Download
Unprioritized Webpage Download
Prioritized Connection Initialization
Unprioritized Connection Initialization
 5
 10
 15
 20
 25
 30
 35
 40
 45
Seconds
Figure 14: CDF for time cost of connection initial-
ization and webpage fetching for Hidden Services,
for unprioritized and prioritized Tor
will quickly be forgotten, and the bulk transfer circuits will