delay variations. A production version of NDP cannot burn server
CPUs for this task, as these are used for application workloads. The
obvious solution is to implement NDP on a smartNIC, either on a
SoC-based one (e.g. Broadcom Stingray PS225) or an FPGA-based
one (e.g. Intel N3000).
3 DEPLOYMENT PROSPECTS
To understand the impact of NDP two years after its publication
we estimate the potential for deployment in this section and the
research impact in the next one.
We, like all other systems researchers, strive to create systems
that are used in practice. Our previous experience with Multipath
TCP shows that the pain of getting MPTCP standardized at the IETF
and creating and maintaining a stable Linux kernel implementation
was worth the effort, as MPTCP is now actively used widely.
Our experience at getting things deployed in datacenters, how-
ever, is not as good. NDP’s presentation at Sigcomm 2017, imple-
mented and presented by Mark Handley using the Unity game
engine, caused quite a stir. Riding on this wave, we approached
people from the big three cloud providers asking whether they
would be interested in trying NDP in their datacenters. The an-
swers were underwhelming, mentioning lack of switch support as
a show-stopper. Instead, it was suggested we should find a solution
that does not require switch support. Homa [16], published at Sig-
comm 2018 does exactly this, while sacrificing some performance
in corner cases; it remains to be seen if it will be adopted in practice.
This was not the first time we had tried to get something deployed
in datacenters and failed, so the answers convinced us that this was
an uphill battle we had little chance of winning. Like the rest of
our peers in the community, we moved to other areas where our
research could have an impact.
In the meantime, however, something surprising happened: out
of the blue, we were approached by three switch vendors that were
willing to implement NDP in their switches. We began collaborating
with them, and one has a working prototype that we are now
testing. A smartNIC implementation of the NDP endhost stack is
also ongoing, giving us hope that a prototype deployment of NDP
in close-to-production networks is possible in the near future.
Whether NDP will make its way into the big three clouds or
whether it will be adopted for smaller, latency sensitive enterprise
datacenters, it is too soon to tell. At this point, there is however a
good chance that NDP may be used in practice after all.
4 RESEARCH IMPACT
The problem of efficiently utilizing datacenter networks has been in
the research spotlight for ten years now, with thousands of papers
published on this topic (the Hedera paper that opened the field has
1300+ citations, same for DCTCP).
ACM SIGCOMM Computer Communication Review
Volume 49 Issue 5, October 2019
113
NDP appears to be in a batch of papers that effectively close
this field, (along with Homa at Sigcomm 2018 and ExpressPass at
Sigcomm 2017 [7]): the number of papers addressing this problem
has dropped constantly (13 papers in 2017, 5 in 2018 and none at
NSDI 2019).
NDP has currently 67 citations on Google Scholar, ranking fourth
out of 36 papers in Sigcomm 2017 after QUIC and Pensieve ( 200
citations each) and SilkRoad (Facebook’s load balancer, with 70
citations). The NDP paper has been downloaded 7300 times from
the ACM digital library, coming second to Google’s QUIC paper
(9000 downloads) in the Sigcomm 2017 batch (average number of
downloads is 2400).
The high number of downloads hints that NDP has generated
interest, but this interest has not translated to citations at the same
rate as it did for QUIC; this may be linked to the slowing amount
of research done on the problem.
The real measure of impact, however, is whether NDP will be
deployed and used in practive. It is too soon to tell whether this
will be the case, but there are some promising signs.
ACKNOWLEDGMENTS
We conclude this short restrospective of the NDP work by pointing
out that in our experience, networking research impact is possible
in practice but it takes long term projects and large teams to achieve.
While this editorial has been written by a subset of the NDP
authors, we would like to point out that NDP was a collaborative
project: the design and simulations were done by Mark Handley
(UCL) and Costin Raiciu (UPB), the main software implementa-
tion was done by Alexandru Agache (UPB), the P4 implementation
by Andrei Voinescu (UPB) and the NetFPGA-based switch imple-
mentation by Gianni Antichi, Marcin Wojchik and Andrew Moore
(University of Cambridge).
Our reference to the Multipath TCP is again a collective one,
with Mark Handley (UCL) and Costin Raiciu (UPB) contributing the
protocol design, initial implementation and first phase of standard-
ization and Olivier’s Bonaventure team at Universite Catholique
de Louvain being the main contributors the mature Linux kernel
implementation which is now used widely. Many other people have
contributed to MPTCP including Alan Ford (Pexip) leading the
lengthy standardization work, Phil Eardley and Yoshifumi Yoshida
leading the MPTCP working group, Marcelo Bagnulo (UC3M) work-
ing on the security part, to name just a few.
REFERENCES
[1] Mohammad Al-Fares, Alexander Loukissas, and Amin Vahdat. 2008. A Scalable,
Commodity Data Center Network Architecture. In Special Interest Group on Data
Communication (SIGCOMM). ACM.
[2] Mohammad Al-Fares, Sivasankar Radhakrishnan, Barath Raghavan, Nelson
Huang, and Amin Vahdat. 2010. Hedera: Dynamic Flow Scheduling for Data Cen-
ter Networks. In Networked Systems Design and Implementation (NSDI). USENIX
Association.
[3] Mohammad Alizadeh, Tom Edsall, Sarang Dharmapurikar, Ramanan
Vaidyanathan, Kevin Chu, Andy Fingerhut, Vinh The Lam, Francis Ma-
tus, Rong Pan, Navindra Yadav, and George Varghese. 2014. CONGA: Distributed
Congestion-aware Load Balancing for Datacenters. In Special Interest Group on
Data Communication (SIGCOMM). ACM.
[4] Mohammad Alizadeh, Albert Greenberg, David A. Maltz, Jitendra Padhye,
Parveen Patel, Balaji Prabhakar, Sudipta Sengupta, and Murari Sridharan. 2010.
Data Center TCP (DCTCP). In Special Interest Group on Data Communication
(SIGCOMM). ACM.
[5] Mohammad Alizadeh, Shuang Yang, Milad Sharif, Sachin Katti, Nick McKeown,
Balaji Prabhakar, and Scott Shenker. 2013. pFabric: Minimal Near-optimal Data-
center Transport. In Special Interest Group on Data Communication (SIGCOMM).
ACM.
[6] Yanpei Chen, Rean Griffith, Junda Liu, Randy H. Katz, and Anthony D. Joseph.
2009. Understanding TCP Incast Throughput Collapse in Datacenter Networks.
In Workshop on Research on Enterprise Networking (WREN). ACM.
[7] Inho Cho, Keon Jang, and Dongsu Han. 2017. Credit-Scheduled Delay-Bounded
Congestion Control for Datacenters. In Special Interest Group on Data Communi-
cation (SIGCOMM). ACM.
[8] Andrew R. Curtis, Jeffrey C. Mogul, Jean Tourrilhes, Praveen Yalagandula, Puneet
Sharma, and Sujata Banerjee. 2011. DevoFlow: Scaling Flow Management for
High-performance Networks. In Special Interest Group on Data Communication
(SIGCOMM). ACM.
[9] Advait Dixit, Pawan Prokash, Charlie Y. Hu, and Ramona R Kompella. 2013.
On the Impact of Packet Spraying in Data Center Networks. In International
Conference on Computer Communications (INFOCOM). IEEE.
[10] Peter X. Gao, Akshay Narayan, Gautam Kumar, Rachit Agarwal, Sylvia Rat-
nasamy, and Scott Shenker. 2015. pHost: Distributed Near-optimal Datacenter
Transport over Commodity Network Fabric. In Conference on Emerging Network-
ing Experiments and Technologies (CoNEXT). ACM.
[11] Albert Greenberg, James R. Hamilton, Navendu Jain, Srikanth Kandula,
Changhoon Kim, Parantap Lahiri, David A. Maltz, Parveen Patel, and Sudipta
Sengupta. 2009. VL2: A Scalable and Flexible Data Center Network. In Special
Interest Group on Data Communication (SIGCOMM). ACM.
[12] Chuanxiong Guo, Haitao Wu, Zhong Deng, Gaurav Soni, Jianxi Ye, Jitu Padhye,
and Marina Lipshteyn. 2016. RDMA over Commodity Ethernet at Scale. In Special
Interest Group on Data Communication (SIGCOMM). ACM.
[13] Mark Handley, Costin Raiciu, Alexandru Agache, Andrei Voinescu, Andrew W.
Moore, Gianni Antichi, and Marcin Wójcik. 2017. Re-architecting Datacenter
Networks and Stacks for Low Latency and High Performance. In Special Interest
Group on Data Communication (SIGCOMM). ACM.
[14] Chi-Yao Hong, Matthew Caesar, and P. Brighten Godfrey. 2012. Finishing Flows
Quickly with Preemptive Scheduling. In Special Interest Group on Data Commu-
nication (SIGCOMM). ACM.
[15] Abdul Kabbani, Balajee Vamanan, Jahangir Hasan, and Fabien Duchene. 2014.
FlowBender: Flow-level Adaptive Routing for Improved Latency and Throughput
in Datacenter Networks. In Conference on Emerging Networking Experiments and
Technologies (CoNEXT). ACM.
[16] Behnam Montazeri, Yilong Li, Mohammad Alizadeh, and John Ousterhout. 2018.
Homa: A Receiver-driven Low-latency Transport Protocol Using Network Priori-
ties. In Special Interest Group on Data Communication (SIGCOMM). ACM.
[17] Jonathan Perry, Hari Balakrishnan, and Devavrat Shah. 2017. Flowtune: Flowlet
Control for Datacenter Networks. In Networked Systems Design and Implementa-
tion (NSDI). USENIX Association.
[18] Jonathan Perry, Amy Ousterhout, Hari Balakrishnan, Devavrat Shah, and Hans
Fugal. 2014. Fastpass: A Centralized "Zero-queue" Datacenter Network. In Special
Interest Group on Data Communication (SIGCOMM). ACM.
[19] Costin Raiciu, Sebastien Barre, Christopher Pluntke, Adam Greenhalgh, Damon
Wischik, and Mark Handley. 2010.
Improving Datacenter Performance and
Robustness with Multipath TCP. In Special Interest Group on Data Communication
(SIGCOMM). ACM.
[20] Christo Wilson, Hitesh Ballani, Thomas Karagiannis, and Ant Rowtron. 2011.
Better Never Than Late: Meeting Deadlines in Datacenter Networks. In Special
Interest Group on Data Communication (SIGCOMM). ACM.
ACM SIGCOMM Computer Communication Review
Volume 49 Issue 5, October 2019
114