### Developing a Launch Checklist

Testing a configuration in a test instance does not guarantee that the same configuration will work seamlessly in a live environment. Sometimes, a complex process or special functionality is required to ensure all components launch correctly and in the right order.

External requirements from teams like marketing and PR can add further complications. For example, a feature might need to be available for a keynote at a conference but remain invisible until the keynote begins. Contingency measures are an essential part of rollout planning. If a feature is not ready in time for the keynote, contingency measures could include preparing a backup slide deck that says, "We will be launching this feature over the next few days" instead of "We have launched this feature."

### Example Action Items
- **Set up a launch plan** that identifies the actions needed to launch the service and assigns responsibility for each item.
- **Identify risks** in individual launch steps and implement contingency measures.

### Selected Techniques for Reliable Launches

Google has developed several techniques over the years for running reliable systems. Some of these techniques are particularly well-suited for launching products safely. They also provide advantages during regular operation, but it's especially important to get them right during the launch phase.

#### Gradual and Staged Rollouts

A common adage in system administration is "never change a running system." Any change represents risk, and minimizing risk is crucial for system reliability. This is even more critical for highly replicated, globally distributed systems like those run by Google.

Very few launches at Google are "push-button" events, where a new product is launched for the entire world at a specific time. Instead, Google has developed patterns for gradual and staged rollouts to minimize risk. Almost all updates to Google’s services proceed gradually, with defined processes and verification steps. A new server might be installed on a few machines in one data center, observed for a set period, and then rolled out to all machines in the data center, and finally, globally. The initial stages of a rollout are often called "canaries," inspired by the use of canaries in coal mines to detect dangerous gases. Canary servers help detect issues with the new software under real user traffic.

Canary testing is embedded into many of Google’s internal tools for automated changes and configuration management. These tools typically observe the newly started server to ensure it doesn't crash or misbehave. If the change fails the validation period, it is automatically rolled back.

The concept of gradual rollouts applies even to software that doesn't run on Google's servers. New versions of an Android app can be rolled out gradually, with the updated version offered to a subset of users. The percentage of upgraded instances increases over time until it reaches 100%. This helps monitor the impact on backend servers and detect problems early.

Another type of gradual rollout is the invite system, where only a limited number of users can sign up per day, often coupled with an invite system where users can send a limited number of invites to friends.

#### Feature Flag Frameworks

Google often uses strategies to mitigate the risk of outages, such as rolling out changes slowly and observing total system behavior under real workloads. These mechanisms are particularly useful when realistic test environments are impractical or for complex launches where effects are hard to predict.

Feature flag frameworks allow for the gradual rollout of new features from 0% to 100% of users. These frameworks are designed to:
- Roll out many changes in parallel, each to a few servers, users, entities, or data centers.
- Gradually increase to a larger but limited group of users, usually between 1% and 10%.
- Direct traffic through different servers based on users, sessions, objects, and/or locations.
- Automatically handle failures of the new code paths without affecting users.
- Independently revert changes immediately if serious bugs or side effects are detected.
- Measure the extent to which each change improves the user experience.

Google's feature flag frameworks fall into two general classes:
- Those that primarily facilitate user interface improvements.
- Those that support arbitrary server-side and business logic changes.

For stateless services, the simplest feature flag framework is an HTTP payload rewriter at frontend application servers, limited to a subset of cookies or another similar HTTP request/response attribute. Stateful services tend to limit feature flags to a subset of unique logged-in user identifiers or to the actual product entities accessed, such as document IDs.

#### Dealing with Abusive Client Behavior

Abusive client behavior can cause significant issues. For example, a new client that syncs every 60 seconds instead of every 600 seconds can increase the load on the service tenfold. Retry behavior can also be problematic, as clients retrying failed requests can overload an already stressed service. Clients should reduce the frequency of retries, often by adding exponentially increasing delays between retries, and carefully consider the types of errors that warrant a retry.

Synchronization of automated requests, such as a phone app downloading updates at 2 a.m., can also cause issues. To avoid this, clients should choose the time for such requests randomly.

Server-side control of client behavior is also important. For example, a client might be instructed to check in periodically with the server and download a configuration file that enables or disables certain features or sets parameters.

#### Overload Behavior and Load Tests

Overload situations are complex failure modes. While runaway success is a welcome cause of overload, other causes include load balancing failures, machine outages, synchronized client behavior, and external attacks. Services rarely scale linearly with load, and there is often a window where CPU usage and load correspond linearly before reaching a point of nonlinearity.

Load tests are invaluable for both reliability and capacity planning. They help predict how a service will react to overload and are required for most launches.

### Development of LCE

In Google’s early years, the engineering team grew rapidly, leading to fragmentation and the risk of repeating past mistakes. To mitigate this, a small band of experienced engineers, called "Launch Engineers," volunteered to act as a consulting team. They developed checklists for new product launches, covering topics like legal consultations, domain name selection, and common engineering pitfalls.

As the complexity of Google’s deployment environment increased, the SRE organization staffed a full-time team of Launch Coordination Engineers (LCEs) in 2004. LCEs were responsible for accelerating new product and feature launches while ensuring high availability and low latency. They conducted Production Reviews and developed a checklist to streamline the launch process.

### Evolution of the LCE Checklist

As Google’s environment grew more complex, so did the LCE checklist. In 3.5 years, one LCE ran 350 launches through the checklist. To fully understand the complexity, a new LCE hire required about six months of training.

To streamline reviews, LCEs identified categories of low-risk launches, such as those involving no new server executables and a traffic increase under 10%. By 2008, 30% of reviews were considered low-risk. Simultaneously, Google’s infrastructure scaled up, simplifying the launch of new products.

### Problems LCE Didn’t Solve

Despite efforts to minimize bureaucracy, launching a small new service at Google became increasingly difficult. Scalability changes, growing operational load, and infrastructure churn posed significant challenges. Solving these problems requires company-wide efforts, including better platform APIs, continuous build and test automation, and improved standardization and automation.

### Conclusion

Companies undergoing rapid growth with a high rate of change may benefit from a role similar to LCE. Such a team is valuable if a company plans to double its product developers every one or two years, scale its services to hundreds of millions of users, and maintain reliability despite a high rate of change.

The LCE team was Google’s solution to achieving safety without impeding change. We hope our approach inspires others facing similar challenges in their organizations.