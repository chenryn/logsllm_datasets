Developing a Launch Checklist  |  379in a test instance doesn’t guarantee that the same configuration can be rolled out to the live instance. Sometimes a complicated dance or special functionality is required to make all components launch cleanly and in the correct order.
External requirements from teams like marketing and PR might add further compli‐cations. For example, a team might need a feature to be available in time for the key‐note at a conference, but need to keep the feature invisible before the keynote.Contingency measures are another part of rollout planning. What if you don’t man‐age to enable the feature in time for the keynote? Sometimes these contingency meas‐ures are as simple as preparing a backup slide deck that says, “We will be launching this feature over the next days” rather than “We have launched this feature.”
Example action itemsExample action items
• Set up a launch plan that identifies actions to take to launch the service. Identify 	who is responsible for each item.
• Identify risk in the individual launch steps and implement contingency measures.
Selected Techniques for Reliable LaunchesAs described in other parts of this book, Google has developed a number of techni‐ques for running reliable systems over the years. Some of these techniques are partic‐ularly well suited to launching products safely. They also provide advantages during regular operation of the service, but it’s particularly important to get them right dur‐ing the launch phase.
Gradual and Staged RolloutsGradual and Staged Rollouts
One adage of system administration is “never change a running system.” Any change represents risk, and risk should be minimized in order to assure reliability of a sys‐tem. What’s true for any small system is doubly true for highly replicated, globally distributed systems like those run by Google.Very few launches at Google are of the “push-button” variety, in which we launch a new product at a specific time for the entire world to use. Over time, Google has developed a number of patterns that allow us to launch products and features gradu‐ally and thereby minimize risk; see Appendix B.Almost all updates to Google’s services proceed gradually, according to a defined pro‐cess, with appropriate verification steps interspersed. A new server might be installed on a few machines in one datacenter and observed for a defined period of time. If all looks well, the server is installed on all machines in one datacenter, observed again, and then installed on all machines globally. The first stages of a rollout are usually380  |  Chapter 27: Reliable Product Launches at Scale
called “canaries”—an allusion to canaries carried by miners into a coal mine to detect dangerous gases. Our canary servers detect dangerous effects from the behavior of the new software under real user traffic.Canary testing is a concept embedded into many of Google’s internal tools used to make automated changes, as well as for systems that change configuration files. Tools that manage the installation of new software typically observe the newly started server for a while, making sure that the server doesn’t crash or otherwise misbehave. If the change doesn’t pass the validation period, it’s automatically rolled back.The concept of gradual rollouts even applies to software that does not run on Google’s servers. New versions of an Android app can be rolled out in a gradual manner, in which the updated version is offered to a subset of the installs for upgrade. The per‐centage of upgraded instances gradually increases over time until it reaches 100%. This type of rollout is particularly helpful if the new version results in additional traf‐fic to the backend servers in Google’s datacenters. This way, we can observe the effect on our servers as we gradually roll out the new version and detect problems early.The invite system is another type of gradual rollout. Frequently, rather than allowing free signups to a new service, only a limited number of users are allowed to sign up per day. Rate-limited signups are often coupled with an invite system, in which a user can send a limited number of invites to friends.
Feature Flag FrameworksGoogle often augments prelaunch testing with strategies that mitigate the risk of an outage. A mechanism to roll out changes slowly, allowing for observation of total sys‐tem behavior under real workloads, can pay for its engineering investment in reliabil‐ity, engineering velocity, and time to market. These mechanisms have proven particularly useful in cases where realistic test environments are impractical, or for particularly complex launches for which the effects can be hard to predict.Furthermore, not all changes are equal. Sometimes you simply want to check whether a small tweak to the user interface improves the experience of your users. Such small changes shouldn’t involve thousands of lines of code or a heavyweight launch process. You may want to test hundreds of such changes in parallel.Finally, sometimes you want to find out whether a small sample of users like using an early prototype of a new, hard-to-implement feature. You don’t want to spend months of engineering effort to harden a new feature to serve millions of users, only to find that the feature is a flop.To accommodate the preceding scenarios, several Google products devised feature flag frameworks. Some of those frameworks were designed to roll out new features gradually from 0% to 100% of users. Whenever a product introduced any such frame‐work, the framework itself was hardened as much as possible so that most of its appli‐
Selected Techniques for Reliable Launches  |  381cations would not need any LCE involvement. Such frameworks usually meet the following requirements:
• Roll out many changes in parallel, each to a few servers, users, entities, or 	datacenters
• Gradually increase to a larger but limited group of users, usually between 1 and 	10 percent
• Direct traffic through different servers depending on users, sessions, objects, 	and/or locations• Automatically handle failure of the new code paths by design, without affecting 	users
• Independently revert each such change immediately in the event of serious bugs 	or side effects
• Measure the extent to which each change improves the user experience
Google’s feature flag frameworks fall into two general classes:
• Those that primarily facilitate user interface improvements• Those that support arbitrary server-side and business logic changes
The simplest feature flag framework for user interface changes in a stateless service is an HTTP payload rewriter at frontend application servers, limited to a subset of cookies or another similar HTTP request/response attribute. A configuration mecha‐nism may specify an identifier associated with the new code paths and the scope of the change (e.g., cookie hash mod range), whitelists, and blacklists.Stateful services tend to limit feature flags to a subset of unique logged-in user identi‐fiers or to the actual product entities accessed, such as the ID of documents, spread‐sheets, or storage objects. Rather than rewrite HTTP payloads, stateful services are more likely to proxy or reroute requests to different servers depending on the change, conferring the ability to test improved business logic and more complex new features.Dealing with Abusive Client Behavior
The simplest example of abusive client behavior is a misjudgment of update rates. A new client that syncs every 60 seconds, as opposed to every 600 seconds, causes 10 times the load on the service. Retry behavior has a number of pitfalls that affect user-initiated requests, as well as client-initiated requests. Take the example of a service that is overloaded and is therefore failing some requests: if the clients retry the failed requests, they add load to an already overloaded service, resulting in more retries and even more requests. Instead, clients need to reduce the frequency of retries, usually by adding exponentially increasing delay between retries, in addition to carefully consid‐382  |  Chapter 27: Reliable Product Launches at Scale
ering the types of errors that warrant a retry. For example, a network error usually warrants a retry, but a 4xx HTTP error (which indicates an error on the client’s side) usually does not.Intentional or inadvertent synchronization of automated requests in a thundering herd (much like those described in Chapters 24 and 25) is another common example of abusive client behavior. A phone app developer might decide that 2 a.m. is a good time to download updates, because the user is most likely asleep and won’t be incon‐venienced by the download. However, such a design results in a barrage of requests to the download server at 2 a.m. every night, and almost no requests at any other time.Instead, every client should choose the time for this type of request randomly.
Randomness also needs to be injected into other periodic processes. To return to the previously mentioned retries: let’s take the example of a client that sends a request, and when it encounters a failure, retries after 1 second, then 2 seconds, then 4 sec‐onds, and so on. Without randomness, a brief request spike that leads to an increased error rate could repeat itself due to retries after 1 second, then 2 seconds, then 4 sec‐onds. In order to even out these synchronized events, each delay needs to be jittered (that is, adjusted by a random amount).The ability to control the behavior of a client from the server side has proven an important tool in the past. For an app on a device, such control might mean instruct‐ing the client to check in periodically with the server and download a configuration file. The file might enable or disable certain features or set parameters, such as how often the client syncs or how often it retries.The client configuration might even enable completely new user-facing functionality. By hosting the code that supports new functionality in the client application before we activate that feature, we greatly reduce the risk associated with a launch. Releasing a new version becomes much easier if we don’t need to maintain parallel release tracks for a version with the new functionality versus without the functionality. This holds particularly true if we’re not dealing with a single piece of new functionality, but a set of independent features that might be released on different schedules, which would necessitate maintaining a combinatorial explosion of different versions.Having this sort of dormant functionality also makes aborting launches easier when adverse effects are discovered during a rollout. In such cases, we can simply switch the feature off, iterate, and release an updated version of the app. Without this type of client configuration, we would have to provide a new version of the app without the feature, and update the app on all users’ phones.Overload Behavior and Load Tests
Overload situations are a particularly complex failure mode, and therefore deserve additional attention. Runaway success is usually the most welcome cause of overload
Selected Techniques for Reliable Launches  |  383
when a new service launches, but there are myriad other causes, including load bal‐ancing failures, machine outages, synchronized client behavior, and external attacks.A naive model assumes that CPU usage on a machine providing a particular service scales linearly with the load (for example, number of requests or amount of data pro‐cessed), and once available CPU is exhausted, processing simply becomes slower. Unfortunately, services rarely behave in this ideal fashion in the real world. Many services are much slower when they are not loaded, usually due to the effect of vari‐ous kinds of caches such as CPU caches, JIT caches, and service-specific data caches. As load increases, there is usually a window in which CPU usage and load on the ser‐vice correspond linearly, and response times stay mostly constant.At some point, many services reach a point of nonlinearity as they approach overload. In the most benign cases, response times simply begin to increase, resulting in a degraded user experience but not necessarily causing an outage (although a slow dependency might cause user-visible errors up the stack, due to exceeded RPC dead‐lines). In the most drastic cases, a service locks up completely in response to overload.To cite a specific example of overload behavior: a service logged debugging informa‐tion in response to backend errors. It turned out that logging debugging information was more expensive than handling the backend response in a normal case. Therefore, as the service became overloaded and timed out backend responses inside its own RPC stack, the service spent even more CPU time logging these responses, timing out more requests in the meantime until the service ground to a complete halt. In serv‐ices running on the Java Virtual Machine (JVM), a similar effect of grinding to a halt is sometimes called “GC (garbage collection) thrashing.” In this scenario, the virtual machine’s internal memory management runs in increasingly closer cycles, trying to free up memory until most of the CPU time is consumed by memory management.Unfortunately, it is very hard to predict from first principles how a service will react to overload. Therefore, load tests are an invaluable tool, both for reliability reasons and capacity planning, and load testing is required for most launches.
Development of LCEIn Google’s formative years, the size of the engineering team doubled every year for several years in a row, fragmenting the engineering department into many small teams working on many experimental new products and features. In such a climate, novice engineers run the risk of repeating the mistakes of their predecessors, espe‐cially when it comes to launching new features and products successfully.To mitigate the repetition of such mistakes by capturing the lessons learned from past launches, a small band of experienced engineers, called the “Launch Engineers,” vol‐
384  |  Chapter 27: Reliable Product Launches at Scale
unteered to act as a consulting team. The Launch Engineers developed checklists for new product launches, covering topics such as:
• When to consult with the legal department• How to select domain names
• How to register new domains without misconfiguring DNS
• Common engineering design and production deployment pitfalls
“Launch Reviews,” as the Launch Engineers’ consulting sessions came to be called, became a common practice days to weeks before the launch of many new products.Within two years, the product deployment requirements in the launch checklist grew long and complex. Combined with the increasing complexity of Google’s deployment environment, it became more and more challenging for product engineers to stay up-to-date on how to make changes safely. At the same time, the SRE organization was growing quickly, and inexperienced SREs were sometimes overly cautious and averse to change. Google ran a risk that the resulting negotiations between these two parties would reduce the velocity of product/feature launches.To mitigate this scenario from the engineering perspective, SRE staffed a small, full-time team of LCEs in 2004. They were responsible for accelerating the launches of new products and features, while at the same time applying SRE expertise to ensure that Google shipped reliable products with high availability and low latency.LCEs were responsible for making sure launches were executing quickly without the services falling over, and that if a launch did fail, it didn’t take down other products. LCEs were also responsible for keeping stakeholders informed of the nature and like‐lihood of such failures whenever corners were cut in order to accelerate time to mar‐ket. Their consulting sessions were formalized as Production Reviews.Evolution of the LCE Checklist
As Google’s environment grew more complex, so did both the Launch Coordination Engineering checklist (see Appendix E) and the volume of launches. In 3.5 years, one LCE ran 350 launches through the LCE Checklist. As the team averaged five engi‐neers during this time period, this translates into a Google launch throughput of over 1,500 launches in 3.5 years!While each question on the LCE Checklist is simple, much complexity is built in to what prompted the question and the implications of its answer. In order to fully understand this degree of complexity, a new LCE hire requires about six months of training.
As the volume of launches grew, keeping pace with the annual doubling of Google’s engineering team, LCEs sought ways to streamline their reviews. LCEs identified cat‐Development of LCE  |  385
egories of low-risk launches that were highly unlikely to face or cause mishaps. For example, a feature launch involving no new server executables and a traffic increase under 10% would be deemed low risk. Such launches were faced with an almost triv‐ial checklist, while higher-risk launches underwent the full gamut of checks and bal‐ances. By 2008, 30% of reviews were considered low-risk.Simultaneously, Google’s environment was scaling up, removing constraints on many launches. For instance, the acquisition of YouTube forced Google to build out its net‐work and utilize bandwidth more efficiently. This meant that many smaller products would “fit within the cracks,” avoiding complex network capacity planning and provi‐sioning processes, thus accelerating their launches. Google also began building very large datacenters capable of hosting several dependent services under one roof. This development simplified the launch of new products that needed large amounts of capacity at multiple preexisting services upon which they depended.Problems LCE Didn’t Solve
Although LCEs tried to keep the bureaucracy of reviews to a minimum, such efforts were insufficient. By 2009, the difficulties of launching a small new service at Google had become a legend. Services that grew to a larger scale faced their own set of prob‐lems that Launch Coordination could not solve.
Scalability changesWhen products are successful far beyond any early estimates, and their usage increa‐ses by more than two orders of magnitude, keeping pace with their load necessitates many design changes. Such scalability changes, combined with ongoing feature addi‐tions, often make the product more complex, fragile, and difficult to operate. At some point, the original product architecture becomes unmanageable and the product needs to be completely rearchitected. Rearchitecting the product and then migrating all users from the old to the new architecture requires a large investment of time and resources from developers and SREs alike, slowing down the rate of new feature development during that period.Growing operational load
When running a service after it launches, operational load, the amount of manual and repetitive engineering needed to keep a system functioning, tends to grow over time unless efforts are made to control such load. Noisiness of automated notifica‐tions, complexity of deployment procedures, and the overhead of manual mainte‐nance work tend to increase over time and consume increasing amounts of the service owner’s bandwidth, leaving the team less time for feature development. SRE has an internally advertised goal of keeping operational work below a maximum of386  |  Chapter 27: Reliable Product Launches at Scale
50%; see Chapter 5. Staying below this maximum requires constant tracking of sour‐ces of operational work, as well as directed effort to remove these sources.
Infrastructure churnIf the underlying infrastructure (such as systems for cluster management, storage, monitoring, load balancing, and data transfer) is changing due to active development by infrastructure teams, the owners of services running on the infrastructure must invest large amounts of work to simply keep up with the infrastructure changes. As infrastructure features upon which services rely are deprecated and replaced by new features, service owners must continually modify their configurations and rebuild their executables, consequently “running fast just to stay in the same place.” The solu‐tion to this scenario is to enact some type of churn reduction policy that prohibits infrastructure engineers from releasing backward-incompatible features until they also automate the migration of their clients to the new feature. Creating automated migration tools to accompany new features minimizes the work imposed on service owners to keep up with infrastructure churn.Solving these problems requires company-wide efforts that are far beyond the scope of LCE: a combination of better platform APIs and frameworks (see Chapter 32), continuous build and test automation, and better standardization and automation across Google’s production services.
ConclusionCompanies undergoing rapid growth with a high rate of change to products and serv‐ices may benefit from the equivalent of a Launch Coordination Engineering role. Such a team is especially valuable if a company plans to double its product developers every one or two years, if it must scale its services to hundreds of millions of users, and if reliability despite a high rate of change is important to its users.The LCE team was Google’s solution to the problem of achieving safety without impeding change. This chapter introduced some of the experiences accumulated by our unique LCE role over a 10-year period under exactly such circumstances. We hope that our approach will help inspire others facing similar challenges in their respective organizations.
Conclusion  |  387
PART IV
ManagementPART IV
Management
Our final selection of topics covers working together in a team, and working as teams. No SRE is an island, and there are some distinctive ways in which we work.Any organization that aspires to be serious about running an effective SRE arm needs to consider training. Teaching SREs how to think in a complicated and fast-changing environment with a well-thought-out and well-executed training program has the promise of instilling best practices within a new hire’s first few weeks or months that otherwise would take months or years to accumulate. We discuss strategies for doing just that in Chapter 28, Accelerating SREs to On-Call and Beyond.As anyone in the operations world knows, responsibility for any significant service comes with a lot of interruptions: production getting in a bad state, people requesting updates to their favorite binary, a long queue of consultation requests…managing interrupts under turbulent conditions is a necessary skill, as we’ll discuss in Chap‐ter 29, Dealing with Interrupts.If the turbulent conditions have persisted for long enough, an SRE team needs to start recovering from operational overload. We have just the flight plan for you in Chap‐ter 30, Embedding an SRE to Recover from Operational Overload.
We write in Chapter 31, Communication and Collaboration in SRE, about the different roles within SRE; cross-team, cross-site, and cross-continent communication; run‐ning production meetings; and case studies of how SRE has collaborated well.Finally, Chapter 32, The Evolving SRE Engagement Model, examines a cornerstone of the operation of SRE: the production readiness review (PRR), a crucial step in onboarding a new service. We discuss how to conduct PRRs, and how to move beyond this successful, but also limited, model.
Further Reading from Google SREFurther Reading from Google SRE
Building reliable systems requires a carefully calibrated mix of skills, ranging from software development to the arguably less-well-known systems analysis and engi‐neering disciplines. We write about the latter disciplines in “The Systems Engineering Side of Site Reliability Engineering” [Hix15b].Hiring SREs well is critical to having a high-functioning reliability organization, as explored in “Hiring Site Reliability Engineers” [Jon15]. Google’s hiring practices have been detailed in texts like Work Rules![Boc15],1 but hiring SREs has its own set of particularities. Even by Google’s overall standards, SRE candidates are difficult to find and even harder to interview effectively.1 Written by Laszlo Bock, Google’s Senior VP of People Operations.
CHAPTER 28
Accelerating SREs to On-Call and Beyond
How Can I Strap a Jetpack to My Newbies While Keeping Senior SREs Up to Speed?
Written by Andrew Widdowson 
Edited by Shylaja Nukala
You’ve Hired Your Next SRE(s), Now What?You’ve hired new employees into your organization, and they’re starting as Site Relia‐bility Engineers. Now you have to train them on the job. Investing up front in the education and technical orientation of new SREs will shape them into better engi‐neers. Such training will accelerate them to a state of proficiency faster, while making their skill set more robust and balanced.Successful SRE teams are built on trust—in order to maintain a service consistently and globally, you need to trust that your fellow on-callers know how your system works,1 can diagnose atypical system behaviors, are comfortable with reaching out for help, and can react under pressure to save the day. It is essential, then, but not suffi‐cient, to think of SRE education through the lens of, “What does a newbie need to learn to go on-call?” Given the requirements regarding trust, you also need to ask questions like:• How can my existing on-callers assess the readiness of the newbie for on-call?
• How can we harness the enthusiasm and curiosity in our new hires to make sure 	that existing SREs benefit from it?
1 And doesn’t work!
391
• What activities can I commit our team to that benefit everyone’s education, but 	that everyone will like?Students have a wide range of learning preferences. Recognizing that you will hire people who have a mix of these preferences, it would be shortsighted to only cater to one style at the expense of the others. Thus, there is no style of education that works best to train new SREs, and there is certainly no one magic formula that will work for all SRE teams. Table 28-1 lists recommended training practices (and their corre‐sponding anti-patterns) that are well known to SRE at Google. These practices repre‐sent a wide range of options available for making your team well educated in SRE concepts, both now and on an ongoing basis.Table 28-1. SRE education practices
Recommended patterns Anti-patterns
Designing concrete, sequential learning experiences for students to follow
Encouraging reverse engineering, statistical thinking, and working from fundamental principles
Celebrating the analysis of failure by suggesting postmortems for students to read
Creating contained but realistic breakages for students to fix using real monitoring and toolingRole-playing theoretical disasters as a group, to intermingle a team’s problem-solving approaches
Enabling students to shadow their on-call rotation early, comparing notes with the on-caller
Pairing students with expert SREs to revise targeted sections of the on-call training plan
Carving out nontrivial project work for students to 
undertake, allowing them to gain partial ownership in the stackDeluging students with menial work (e.g., alert/ticket triage) to train them; “trial by fire”
Training strictly through operator procedures, checklists, and playbooks
Treating outages as secrets to be buried in order to avoid blame
Having the first chance to fix something only occur after a student is already on-call
Creating experts on the team whose techniques and knowledge are compartmentalizedPushing students into being primary on-call before they achieve a holistic understanding of their service
Treating on-call training plans as static and untouchable except by subject matter experts
Awarding all new project work to the most senior SREs, leaving junior SREs to pick up the scrapsThe rest of this chapter presents major themes that we have found to be effective in accelerating SREs to on-call and beyond. These concepts can be visualized in a blue‐print for bootstrapping SREs (Figure 28-1).
392  |  Chapter 28: Accelerating SREs to On-Call and Beyond
Figure 28-1. A blueprint for bootstrapping an SRE to on-call and beyondThis illustration captures best practices that SRE teams can pick from to help boot‐strap new members, while keeping senior talent fresh. From the many tools here, you can pick and choose the activities that best suit your team.
The illustration has two axes:
• The x-axis represents the spectrum between different types of work, ranging from 	abstract to applied activities.• The y-axis represents time. Read from the top down, new SREs have very little knowledge about the systems and services they’ll be responsible for, so postmor‐tems detailing how these systems have failed in the past are a good starting point. New SREs can also try to reverse engineer systems from fundamentals, since they’re starting from zero. Once they understand more about their systems and have done some hands-on work, SREs are ready to shadow on-call and to start mending incomplete or out-of-date documentation.Tips for interpreting this illustration: