thus is parametrized by the number of message groups k and
the frequency threshold t:
ZOE(k, t)
The usage of one building block without the other can be
denoted as ZOE(1,∗) for a detector using one global content
model rather than individual prototype models but different
thresholds for noise-reduction, and ZOE(∗, 0) for the use of
prototype models that however do not ﬁlter noise.
A. Building Prototype Models for Network Messages
The messages monitored in an industrial network can be
represented as strings of variable length. While this represen-
tation is ideal for conventional signature-based detection, for
building protocol models we however require a more structured
representation of the data. We thus map each message m
monitored in the network to a corresponding feature vector
x = φ(m): We extract all substrings of length n—so called
n-grams—from a message m and record their occurrences.
Each substring is associated with one dimension of the feature
space, such that a message m can be expressed as a vector of
substring occurrences. Formally, this map is deﬁned as follows
φ : m → (cid:2)
(cid:3)
φs(m)
s∈S with φs(m) = occ(s, m)
where the set S denotes all possible substrings of length n
and the function occ(s, m) represents the occurrence of the
substring s in the input message m. This can be implemented
as the frequency, the probability or a binary ﬂag for a feature’s
presence. Using this mapping, we can translate a set of
messages {m1, . . . , mN} to a set of vectors
X = {x1, . . . xN} with xi = φ(mi).
Depending on the length n, the vector space can be high
dimensional, as the number of considered substrings grows
exponentially with n. Fortunately, the resulting vectors are very
sparse and thus efﬁcient data structures for handling sparse
data can be applied to operate in this vector space.
Based on this representation we can now proceed to learn
prototype models as a ﬁrst cornerstone of our detector to cope
with high-entropy data in binary protocols. To learn models per
message type we build on methods from the ﬁeld of clustering.
Unfortunately, clustering is a rather expensive task and many
algorithms are not suited for efﬁciently processing large
amounts of data. With ZOE we aim at an integrated solution that
breaks up the separation of clustering and subsequent learning
of content models. We thus build upon a linear-time algorithm
for approximating clusterings [1] that we have tailored to
network trafﬁc analysis such that we can build prototype models
on-the-ﬂy.
We ﬁrst transform our network messages m to feature vectors
x as describe before, to build the input dataset X. Then, k
samples are drawn from the input data to initialize clusters
C1, . . . , Ck. For each following sample x ∈ X we measure
129
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:26:43 UTC from IEEE Xplore.  Restrictions apply. 
the similarity to each cluster and assign it to the cluster Cj
that has the closest proximity
j = arg max
i∈[1,k]
prox(x, Ci)
hash functions are applied to the key in order to determine
the numeric values associated to it. The minimum of these
values then recites the approximate true value—in our case the
approximate count of feature/substring s.
The overall similarity is calculated based on the input sample
x and all samples that belong to a cluster C:
prox : x, C → 1|C|
(cid:4)
y∈C
sim(x, y)
s
h1(s)
+v
+v
+v
+v
hd(s)
width w
d
h
t
p
e
d
Fig. 1. Schematic depiction of a Count-Min Sketch [9]. The substring to be
added is denoted as s and is processed by d (the depth of the sketch) hash
functions hi to determine the position pi = hi(s) with i ∈ [1, d], p ∈ [0, w)
at which value v is added.
By construction, the approximated count ˆc retrieved from a
Count-Min Sketch is always larger or equal to the true count c,
meaning that the data structure will never underestimate a
stored value:
c ≤ ˆc ∀c ∈ c
where c is
the vector of all values
stored in the
ε(cid:9) and
Count-Min Sketch. Furthermore, for a width w = (cid:8) e
δ(cid:9) it is guaranteed that the difference between
a depth d = (cid:8)ln 1
the true and approximated value is at most ε(cid:5)c(cid:5)1 with a
probability p of at least 1 − δ [9]
ˆci ≤ ci + ε(cid:5)c(cid:5)1
In other words, the estimate of the Count-Min Sketch is
correct within ε times the number of items stored in the data
structure with a probability of p. Figures 2(a) and 2(b) show
the distribution of the relative frequency of occurrences of
strings in our evaluation data, once counted exactly and once
probabilistically using a Count-Min Sketch with ε = 0.0001
and δ = 0.01, resulting in a width w = 27,183 and the use of
d = 7 hash functions. Figure 2(c) shows the relative frequency
of the difference in value of these counts. Most values differ
by roughly 10 to 20 occurrences and none, as stated above, is
lower than the true value.
B. Noise-resilient Anomaly Detection
As a second building block for reliably detecting attacks
in proprietary network protocols, we propose an extension to
content-based anomaly detection made possible by prototype
models introduced in the previous section. Language models,
such as n-grams [e.g., 56, 66, 68], have frequently been used
for attack detection and have proven impressively effective for
text-based data [e.g., 45, 66–68]. For binary and high-entropy
data, however, such models are considered mostly impractical
by the research community so far [23] and have been widely
discarded for application in industrial networks.
The main reason why language models perform worse in
this environment is founded in the density of the message data.
The measure sim for two input samples may be either ap-
proximated by the dot product ˜sim : x, y → x · y =
i=0 xiyi
or deﬁned by the cosine similarity based on the l2-norm
(cid:5)x(cid:5)2 =
(cid:6)(cid:5)n
(cid:5)n
i , that is,
i=0 x2
sim : x, y → x · y
(cid:5)x(cid:5)2(cid:5)y(cid:5)2
= cos(θ)
where θ denotes the angle between the vectors x and y.
While this requires slightly more effort to realize a linear-
time implementation, the normalized angle has the advantage
of actually being a formal distance metric.
For a sufﬁciently large input set X the algorithm above
approximates a precise clustering with high probability [1]. This
algorithmic requirement however also demands an especially
efﬁcient way of handling the sets of messages as clusters.
We hence make the following two optimizations in our
implementation: First, we store counts of messages and their
substrings rather than the messages themselves in order to
save valuable working memory. For each cluster Ci we only
maintain the total number of all samples |Ci| contained in the
cluster and a vector of cumulative counts as prototype models:
Pi =
y
(cid:4)
y∈Ci
This sufﬁces to compute the similarity between messages and
clusters as deﬁned before, and can be used for efﬁcient anomaly
detection in further follow (cf. Section III-B).
Second, due to the large amounts of network trafﬁc we
operate on, storing and keeping track of substring counts already
poses a considerable challenge. Retaining exact counts simply
is not feasible as it requires to store all substrings (or at least
hashes thereof). We thus revert to probabilistic counting of sub-
strings. In particular, we make use of Count-Min Sketches [9],
a probabilistic data structure that is closely related to Bloom
ﬁlters [4] but additionally allows for counting occurrences
rather than answering membership queries only.
A Count-Min Sketch is deﬁned as a w × d two-dimensional
array of numeric items of arbitrary size and precision, and
d hash functions hi that map input strings to numeric values in
the interval [0, w − 1]. To store a particular key-value pair or
increment the value for a key in the sketch, each hash function
is at ﬁrst applied to the key, for instance a substring s ∈ S
appearing in message m. As depicted in Figure 1, the resulting
value is then used as position in the corresponding row. At these
offsets the stored numeric value is incremented by the provided
value v. Retrieving the value for a key works analogous: The
130
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:26:43 UTC from IEEE Xplore.  Restrictions apply. 
(a) Exact counting
(b) Probabilistic counting
(c) Exact vs probabilistic counting
Fig. 2. Histograms of 3-gram occurrences in network trafﬁc: a) exact counting, b) probabilistic counting, c) the difference between both approaches.
The density is deﬁned as the ratio of the number of unique
occurrences of a feature/substring s to the total number of all
possible elements S [71]. This ratio signiﬁcantly inﬂuences
the quality of the resulting model. In case of high entropy,
as induced by binary network protocols, a model gets so
“packed” that it becomes difﬁcult to differentiate between two
classes (benign and malicious messages) and renders mere
binary embedding impractical [23, 68]. Count embeddings,
on the other hand, have been shown to be less effective
when used as a drop-in replacement in an otherwise identical
setting [23, 63, 68, 71].
Rather than discarding this information altogether, we use
the number of occurrences to ﬁlter relevant from irrelevant
information. Note, that we record the frequency f of samples
in which features/substrings s ∈ S occur rather than the total
count of s in the complete data set. Setting f in relation to the
total number of samples N formally yields the well-established
f
document frequency measure df =
N . The prototype models
Pi that we have established in the previous section represent
exactly these feature frequencies across training samples
iff function occ(a, m) is deﬁned to report the existence of
substring s in input message m. Detection schemes based on
binary embeddings can thus be directly derived from prototype
models Pi without additional training.
By introducing a threshold t we now prune features that
occur in less than t input samples that have been associated
with a cluster and thereby effectively discard noise from the
training data. Models Mi can hence be interpreted as sets of
features that are considered for detection and together form
the overall content model used by ZOE:
M = {M1, . . . , Mk} with Mi = {s ∈ S | Pi,a ≥ t}
This allows us to revert to detection using the (implicit)
binary embedding based on the remaining, most relevant
features only. This scheme offers two main advantages: First,
it allows to reduce the set of benign features to those that
appear more than t times, and thus signiﬁcantly reduces the
size of the model as features associated with a value of 0
are not explicitly stored. Furthermore, this limits an attacker’s
reach of play when mimicking benign messages (Section V-E).
Second, for production use and to improve runtime performance
of the ﬁnal detector this binarized Count-Min Sketch can be
transformed into a compact Bloom ﬁlter that offers a higher
accuracy than established methods [68] with the same memory
footprint (Section V).
In order to determine the overall detection using k models
ZOE considers the score of the model with the highest
resemblance to the message m in question. With a scoring
function d that yields low values for known/benign messages
and high values for anomalies, this formally translates to
choosing the minimum score of k models:
score : m,M → min
i
d(m, Mi)
threshold T , a
In accordance to d and using an overall
message is considered malicious for score(m,M) ≥ T and
benign otherwise. Different schemes on how to choose scoring
function d and how to adjust and interpret the detection
threshold are discussed in the subsequent section.
C. Adjusting the Detector
One particularly effective way of evaluating a message m
is to determine the ratio of known or unknown features to the
total number l of features in the message:
d1 : m, M → 1 − 1
l
occ(s, m)
(cid:4)
s∈M
While this distance has been shown to be effective in
a number of applications [23, 49, 68, 71] interpreting the
threshold for such a scoring function is rather difﬁcult in
practice. Different measures based on the number of bytes
covered by the model, for instance, however often are not
performant enough to match up. The beneﬁt of the latter is
that the operator of the intrusion detection system is able to
a) more naturally specify the necessary threshold T as the
number of previously unobserved bytes a message may contain
before a message/packet is considered to be part of an attack,
and analogous b) intuitively interpret the resulting scores as
the portion of unknown content in bytes.
d2 : m, M → cov(m, M )
The function cov(m, M ) returns the number of bytes in m
covered by model M. Normalizing to the length of the
131
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:26:43 UTC from IEEE Xplore.  Restrictions apply. 
DATASET AND PROTOCOLS USED FOR THE EVALUATION OF ZOE.