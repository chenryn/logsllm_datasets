taking full advantage of NVM technology, which allows the
system to leverage NVM’s byte-addressability and to avoid PostgreSQL is an open source object-relational database
API overheads [27] present in current FSs. Such a setup does system. It is fully ACID compliant and runs on all major
not need large pools of DRAM since temporary data is orders operating systems including Linux [31].
of magnitude smaller than the actual relational data stored in In this section we study the storage engine (SE) of Post-
NVM.Webelievethisisarealisticscenarioforfuturesystems greSQL and apply necessary changes to make it more NVM-
integratingNVM,withroomforsmallvariationssuchasNVM aware. We first describe the read-write architecture of Post-
alongsideDRAMtostorepersistenttemporarydatastructures, greSQL and then explain our modifications.
or having traditional disks to store cold data.
A. Read-Write Architecture of PostgreSQL
B. List of Modifications for a traditional DBMS
Figure 3a shows the original PostgreSQL architecture from
Using a traditional disk-based database with NVM storage the perspective of read and write file operations. The left
will not take full advantage of NVM’s features. Some impor- column in the figure shows the operations performed by
Pg Software Pg Memory Pg Software Pg Memory Pg Software Pg Memory
Layers Operations Layers Operations Layers Operations
Postgres Server PgBufPtr Postgres Server PgBufPtr Postgres Server PgBufPtr
Pg Buffer Pg Buffer Write
L1: Buffer Layer L1: Buffer Layer L1: Buffer Layer
PgBufPtr PgBufPtr P2PgBufPtr Pg Buffer
Kernel Buffers
L2: MemMapped L2: PtrRedirection
L2: File Layer
Layer Layer
Read
OS OS OS
PMFS NVM-Disk PMFS NVM-Disk PMFS NVM-Disk
Write copy Read copy Copy on flushing Copy on marking
Service calls: System calls:
operation: operation: pg buffer: Pg buffer dirty:
(a)Storageengineoperation-PostgreSQL. (b)Modifiedstorageengine-SE1 (c)Modifiedstorageengine-SE2
Fig.3. HighlevelviewofreadandwritememoryoperationsinPostgreSQL(readas“pg”inshortform)andmodifiedSEs.
software layers of PostgreSQL, while the right column shows space into the kernel’s virtual address space [25], the copy
thecorrespondingdatamovementactivities.Notethat,weused overhead can be avoided by making modifications in the SE.
PMFSforfileoperations.Furthermore,asshowninFigure3a, Weappliedthesemodificationsintwoincrementalstepswhich
we assume that the disk is replaced by NVM for storing the are described in the following subsections.
database.
PostgreSQL heavily relies on file I/O for read and write B. SE1: Using Memory Mapped I/O
operations. Since the implementation of the file I/O APIs in
InthefirststeptowardsleveragingthefeaturesofNVM,we
PMFSisthesameasthatofatraditionalFS,usingaparticular
replaced the File Layer of PostgreSQL by a new layer named
FS does not make any difference.
MemMapped Layer. As shown in Figure 3b, this layer still
ThePostgreSQLservercallstheservicesoftheBufferLayer
receives a pointer to a free buffer slot from the Buffer Layer,
which is responsible for maintaining an internal buffer cache.
but instead of using the file I/O interface, it uses the memory
The buffer cache is used to keep a copy of the requested page
mapped I/O interface of PMFS. We term this storage engine
which is read from the storage. Copies are kept in the cache
SE1.
as long as they are needed. If there is no free slot available
for a newly requested page then a replacement policy is used Read Operation: When accessing a file for a read operation,
to select a victim. The victim is evicted from the buffer cache we first open the file using the open() system call, same as
and if it is a dirty page, then it is also flushed back to the in original PostgreSQL. Additionally, we create a mapping of
permanent storage. the file using mmap(). Since we are using PMFS, mmap()
Upon receiving a new request to read a page from storage, returnsapointertothemappingofthefilestoredinNVM.The
the Buffer Layer finds a free buffer cache slot and gets a implementationofmmap()byPMFSprovidestheapplication
pointertoit.Thefreebufferslotandcorrespondingpointerare with direct access to mapped pages of files residing in NVM.
shown in Figure 3a as Pg Buffer and PgBufPtr, respectively. As a result we do not need to make an intermediate copy
The Buffer Layer then passes the pointer to the File Layer. of the requested page from NVM into kernel buffers. We
Eventually the File Layer of PostgreSQL invokes the file read can directly copy the requested page into internal buffers of
and write system calls implemented by the underlying FS. PostgreSQL by using an implicit memcpy() as shown in
For a read operation, PMFS copies the data block from Figure 3b. When all requested operations on a given file are
NVM to a kernel buffer and then the kernel copies the completedanditisnotneededanymore,thefilecanbeclosed.
requested data block to an internal buffer slot pointed by Uponclosingafile,wedeletethemappingofthefilebycalling
PgBufPtr. In the same way, two copies are made for write the munmap() function.
operation but in the opposite direction. WriteOperation:Thesameapproachasinthereadoperation
Hence,theSEoforiginalPostgreSQLincurstwocopyoper- is used for writing data into a file. The file to be modified is
ationsforeachmissintheinternalbuffercache.Thisislikely first opened and a mapping is created using mmap(). The
to become a large overhead for databases running queries on data to be written into the file is copied directly from internal
large datasets. Since PMFS can map the entire NVM address buffers of PostgreSQL into NVM using memcpy().
A SE with the above mentioned modifications does not TABLEI
createanintermediatecopyofthedatainkernelbuffers.Hence TESTMACHINECHARACTERISTICS.
we reduced the overhead to one copy operation for each miss
Component Description
in the internal buffer cache of PostgreSQL.
PI:EMAIL
Processor
HTandTurboBoostdisabled
C. SE2: Direct Access to Mapped Files
Private:L132KB4-waysplitI/D,L2256KB8-way
Caches
Shared:L320MB16-way
In the second step of modifications to the SE, we replaced
Memory 256GBDDR3-1600,4channels,deliveringupto51.5GB/s
the MemMapped Layer of SE1 by the PtrRedirection Layer
OS LinuxKernel3.11.0withPMFSsupport[25,26]
as shown in Figure 3c. Unlike the MemMapped Layer, the
PtrRedirection Layer in SE2 receives the pointer to PgBufPtr Diskstorage IntelDCS3700Series,400GB,SATA6Gb/s
Read500MBs/75kiops,Write460MBs/36kiops
(i.e P2PgBufPtr), which itself points to a free slot of buffer
PMFSstorage 224GBoftotalDRAM
cache.Inotherwords,PtrRedirectionLayer receivesapointer
to a pointer from the Buffer Layer.
Read Operation: When accessing a file for a read operation, For this reason, we have set up an infrastructure similar to
we first open the file using open() system call, same as in thatusedbythePMFSauthors.WefirstrecompiledtheLinux
original PostgreSQL and SE1. Additionally, we also create a kernel of our test machine with PMFS support. Using the
mappingofthefileusingmmap().OriginallyPgBufPtrpoints memmap kernel command line option we reserve a physically
toafreeslotintheinternalbuffercache.Sincemmap()makes contiguous area of the available DRAM at boot-time, which
theNVMmappedaddressspacevisibletothecallingprocess, is later used to mount the PMFS partition. In other words,
the PtrRedirection Layer simply redirects the PgBufPtr to a portion of the DRAM holds the disk partition managed by
pointtothecorrespondingaddressofthefileresidinginNVM. PMFS and provides features similar to those of NVM, such
Pointer redirection in case of read operation is shown by a as byte-addressability and lower latency compared to a disk.
black dashed arrow with the “Read” label in Figure 3c. Table I lists the test machine characteristics. We configure the
As a result of doing pointer redirection and the visibility of machine to have a 224GB PMFS partition, leaving 32GB of
the NVM address space enabled by PMFS, we incur no copy DRAM for normal main memory operation. A high-end SSD
overhead for read operations. This can represent a significant is used as regular disk storage.
improvement,sincereadoperationsarepredominantinqueries AtechnologicaladvantageofNVMsovertraditionaldisksis
that operate on large datasets. their lower read access latencies. To quantify the performance
WriteOperation:PMFSprovidesdirectwriteaccessforfiles impact this can have in query executions, we evaluate two
residinginNVM.SincePostgreSQLisamultiprocesssystem, baselines using unmodified PostgreSQL 9.5, (i) with the
modifying the NVM-resident file can be dangerous. Direct datasetstoredinaregularhigh-enddisk(disk base95),and(ii)
writeoperationscanleavethedatabaseinaninconsistentstate. inthePMFSpartition(pmfs base95).Inaddition,weevaluate
To avoid this issue, SE2 performs two actions before mod- the modified storage engines - SE1 and SE2. These are run
ifying the actual content of the page and marking it as dirty. with the dataset stored on the PMFS partition and are termed
First, if the page is residing in NVM, it copies the page back as pmfs se1 and pmfs se2, respectively.
fromNVMintothecorrespondingslotofinternalbuffercache, To test these system configurations we employ decision
i.e. Pg-Buffer. Second, it undoes the redirection of PgBufPtr supportsystem(DSS)queriesfromtheTPC-H[18]benchmark
suchthatitagainpointstothecorrespondingslotinthebuffer withascalefactorof100,whichleadstoadatasetlargerthan
cache and not to the NVM mapped file. This is shown by a 150GB when adding the appropriate indexes. Like most data
black dashed arrow with the “Write” label in Figure 3c. This intensive workloads, these queries are read dominant. Since
way,SE2ensuresthateachprocessupdatesonlyitslocalcopy DRAM read latencies are expected to be quite similar to pro-
of the page. jected NVM read latencies, the emulation platform employed
provides good performance estimations. In our experiments,
V. METHODOLOGY we report wall-clock query execution times as well as data
obtained with performance counters using the perf toolset.
System-level evaluation for NVM technologies is chal-
We report results for 16 of the 22 TPC-H queries since some
lenging due to lack of real hardware. Software simulation
queries failed to complete under PMFS storage.
infrastructures are a good fit to evaluate systems in which
NVMisusedasaDRAMreplacement,orinconjunctionwith
VI. EVALUATION
DRAM as a hybrid memory system. However, when using
NVM as a permanent storage replacement, most software In this section we show the performance impact that the
simulators fail to capture the details of the operating system, modified storage engines (SE) have on kernel execution time
and comparisons against traditional disks are not possible due and on wall-clock execution time for TPC-H queries. Later,
to the lack of proper simulation models for such devices. As weidentifypotentialissuescurrentDBMSsandapplicationsin
the authors of PMFS [25] noted, an emulation platform is the generalmayfaceinordertoharnessthebenefitsfromdirectly
best way to evaluate such a scenario. accessing data stored in NVM memory.
25
disk_base95 )%(
20 pmfs_base95
emit
pmfs_se1
15 pmfs_se2 noitucexe
10
lenreK
5
0
q01 q02 q03 q05 q06 q07 q08 q09 q11 q12 q13 q15 q16 q17 q19 q20 Geomean
Fig.4. Percentageofkernelexecutiontimeforeachquery.
140
)%(
120
emit
100
.cexe
80
60 disk_base95 dezilamroN
40 pmfs_base95
pmfs_se1
20
pmfs_se2
0
q01 q02 q03 q05 q06 q07 q08 q09 q11 q12 q13 q15 q16 q17 q19 q20 Geomean
Fig.5. Wall-clockexecutiontimenormalizedtopmfs base95.
A. Performance Impact on Kernel Execution Time We observe that the benefits of moving from disk to a faster
Figure 4 shows the percentage of kernel execution time for storage can be high for read intensive queries such as Q05
each of the evaluated queries running on the four evaluated (40%), Q08 (37%), and Q11 (35%). However, for compute
systems.Whenusingtraditionalfileoperations(e.g.read()), intensive queries, such as Q01 and Q16, the benefits are non-
like those employed in unmodified PostgreSQL, the bulk of existent. On average, the overhead of using disk over PMFS
the work when accessing and reading data is done inside the storage is of 16%.
For SE1, the time reductions observed in terms of kernel
kernel.Ascanbeseen,thebaselinesystemsspendasignificant
executiontimedonottranslateintoreductionsinoverallquery
amountoftheexecutiontimeinkernelspace:upto24%(Q11
execution time. The main reason for this is the additional
-disk base95)and20%(Q11-pmfs base95),withanaverage
memcpy() operation performed to copy the data into the
of around 10%. The kernel spaceexecution time is dominated