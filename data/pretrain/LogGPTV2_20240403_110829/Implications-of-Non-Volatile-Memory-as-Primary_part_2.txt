### Leveraging NVM Technology in PostgreSQL

**Introduction:**

Non-Volatile Memory (NVM) technology offers significant advantages, including byte-addressability and reduced API overheads compared to traditional file systems. This setup allows for the use of smaller DRAM pools since temporary data is significantly smaller than the actual relational data stored in NVM. We believe this is a realistic scenario for future systems, with room for variations such as using NVM alongside DRAM for persistent temporary data structures or traditional disks for cold data storage.

**PostgreSQL Overview:**

PostgreSQL is an open-source, object-relational database system that is fully ACID-compliant and runs on all major operating systems, including Linux. In this section, we will examine the storage engine (SE) of PostgreSQL and apply necessary modifications to make it more NVM-aware. We will first describe the read-write architecture of PostgreSQL and then explain our modifications.

### A. Read-Write Architecture of PostgreSQL

Figure 3a illustrates the original PostgreSQL architecture from the perspective of read and write file operations. The left column shows the operations performed by the software layers of PostgreSQL, while the right column depicts the corresponding data movement activities. PMFS (Persistent Memory File System) is used for file operations. We assume that the disk is replaced by NVM for storing the database.

PostgreSQL heavily relies on file I/O for read and write operations. Since the implementation of file I/O APIs in PMFS is the same as in a traditional file system, using a specific FS does not make a difference.

**Buffer Layer:**

The PostgreSQL server calls the services of the Buffer Layer, which maintains an internal buffer cache. The buffer cache keeps a copy of the requested page, which is read from the storage. Copies are retained in the cache as long as they are needed. If there is no free slot available for a newly requested page, a replacement policy is used to select a victim. The victim is evicted from the buffer cache, and if it is a dirty page, it is flushed back to permanent storage.

**File Layer:**

Upon receiving a new request to read a page from storage, the Buffer Layer finds a free buffer cache slot and gets a pointer to it. The Buffer Layer then passes the pointer to the File Layer. Eventually, the File Layer of PostgreSQL invokes the file read and write system calls implemented by the underlying file system. For a read operation, PMFS copies the data block from NVM to a kernel buffer, and the kernel then copies the requested data block to an internal buffer slot pointed by `PgBufPtr`. Similarly, two copies are made for write operations but in the opposite direction.

### B. List of Modifications for a Traditional DBMS

To take full advantage of NVM's features, we need to modify the storage engine (SE) of PostgreSQL. We will describe these modifications in two incremental steps.

#### SE1: Using Memory Mapped I/O

In the first step, we replace the File Layer of PostgreSQL with a new layer called the MemMapped Layer. As shown in Figure 3b, this layer still receives a pointer to a free buffer slot from the Buffer Layer but uses the memory-mapped I/O interface of PMFS instead of the file I/O interface. We term this storage engine SE1.

**Read Operation:**

When accessing a file for a read operation, we first open the file using the `open()` system call, similar to the original PostgreSQL. Additionally, we create a mapping of the file using `mmap()`. Since we are using PMFS, `mmap()` returns a pointer to the mapping of the file stored in NVM. The implementation of `mmap()` by PMFS provides the application with direct access to mapped pages of files residing in NVM. Therefore, we do not need to make an intermediate copy of the requested page from NVM into kernel buffers. We can directly copy the requested page into internal buffers of PostgreSQL using an implicit `memcpy()`. When all requested operations on a given file are completed, the file can be closed, and the mapping can be deleted using `munmap()`.

**Write Operation:**

The same approach is used for writing data into a file. The file to be modified is first opened, and a mapping is created using `mmap()`. The data to be written into the file is copied directly from internal buffers of PostgreSQL into NVM using `memcpy()`.

By making these modifications, SE1 reduces the overhead to one copy operation for each miss in the internal buffer cache of PostgreSQL.

#### SE2: Direct Access to Mapped Files

In the second step, we replace the MemMapped Layer of SE1 with the PtrRedirection Layer, as shown in Figure 3c. Unlike the MemMapped Layer, the PtrRedirection Layer receives a pointer to `PgBufPtr` (i.e., `P2PgBufPtr`), which itself points to a free slot of the buffer cache. In other words, the PtrRedirection Layer receives a pointer to a pointer from the Buffer Layer.

**Read Operation:**

When accessing a file for a read operation, we first open the file using the `open()` system call, similar to the original PostgreSQL and SE1. Additionally, we create a mapping of the file using `mmap()`. Originally, `PgBufPtr` points to a free slot in the internal buffer cache. Since `mmap()` makes the NVM-mapped address space visible to the calling process, the PtrRedirection Layer simply redirects `PgBufPtr` to point to the corresponding address of the file residing in NVM. Pointer redirection in case of read operation is shown by a black dashed arrow with the "Read" label in Figure 3c. As a result, we incur no copy overhead for read operations, which can represent a significant improvement, especially for queries operating on large datasets.

**Write Operation:**

PMFS provides direct write access for files residing in NVM. However, since PostgreSQL is a multiprocess system, modifying the NVM-resident file can be dangerous. Direct write operations can leave the database in an inconsistent state. To avoid this issue, SE2 performs two actions before modifying the actual content of the page and marking it as dirty:
1. If the page is residing in NVM, it copies the page back from NVM into the corresponding slot of the internal buffer cache, i.e., `Pg-Buffer`.
2. It undoes the redirection of `PgBufPtr` such that it again points to the corresponding slot in the buffer cache and not to the NVM-mapped file. This is shown by a black dashed arrow with the "Write" label in Figure 3c.

This way, SE2 ensures that each process updates only its local copy of the page.

### Test Machine Characteristics

| Component | Description |
|-----------|-------------|
| Processor | HT and Turbo Boost disabled |
| Caches | Private: L1 32KB 4-way split I/D, L2 256KB 8-way; Shared: L3 20MB 16-way |
| Memory | 256GB DDR3-1600, 4 channels, delivering up to 51.5GB/s |
| OS | Linux Kernel 3.11.0 with PMFS support [25,26] |
| Disk Storage | Intel DCS3700 Series, 400GB, SATA 6Gb/s, Read 500MB/s/75k IOPS, Write 460MB/s/36k IOPS |
| PMFS Storage | 224GB of total DRAM |

### Methodology

System-level evaluation for NVM technologies is challenging due to the lack of real hardware. Software simulation infrastructures are suitable for evaluating systems where NVM is used as a DRAM replacement or in conjunction with DRAM as a hybrid memory system. However, when using NVM as a permanent storage replacement, most software simulators fail to capture the details of the operating system, and comparisons against traditional disks are not possible due to the lack of proper simulation models for such devices.

To address this, we set up an infrastructure similar to that used by the PMFS authors. We recompiled the Linux kernel of our test machine with PMFS support and reserved a physically contiguous area of the available DRAM at boot-time using the `memmap` kernel command line option. This area is later used to mount the PMFS partition, providing features similar to those of NVM, such as byte-addressability and lower latency compared to a disk.

### Evaluation

In this section, we show the performance impact that the modified storage engines (SE) have on kernel execution time and wall-clock execution time for TPC-H queries. We also identify potential issues current DBMSs and applications may face in order to harness the benefits of directly accessing data stored in NVM.

**Performance Impact on Kernel Execution Time:**

Figure 4 shows the percentage of kernel execution time for each of the evaluated queries running on the four evaluated systems. When using traditional file operations (e.g., `read()`), like those employed in unmodified PostgreSQL, the bulk of the work when accessing and reading data is done inside the kernel. As seen, the baseline systems spend a significant amount of execution time in kernel space: up to 24% (Q11 - disk_base95) and 20% (Q11 - pmfs_base95), with an average of around 10%.

For SE1, the time reductions observed in terms of kernel execution time do not translate into reductions in overall query execution time. The main reason for this is the additional `memcpy()` operation performed to copy the data into the internal buffers.

**Wall-Clock Execution Time:**

Figure 5 shows the wall-clock execution time normalized to `pmfs_base95`. We observe that the benefits of moving from disk to a faster storage can be high for read-intensive queries such as Q05 (40%), Q08 (37%), and Q11 (35%). However, for compute-intensive queries, such as Q01 and Q16, the benefits are non-existent. On average, the overhead of using disk over PMFS storage is about 16%.

### Conclusion

By leveraging NVM technology, we can significantly reduce the overhead associated with traditional file I/O operations in PostgreSQL. The modifications to the storage engine, particularly the use of memory-mapped I/O and direct access to mapped files, demonstrate the potential for improved performance, especially for read-intensive queries. Future work will focus on further optimizing these modifications and addressing the challenges associated with ensuring data consistency in a multiprocess environment.