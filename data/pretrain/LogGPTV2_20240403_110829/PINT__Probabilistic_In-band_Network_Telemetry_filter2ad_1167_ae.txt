Carrier) from Topology Zoo [44] and a (K = 8) Fat Tree topology.
The Kentucky Datalink topology consisted of 753 switches with a
diameter of 59 and the US carrier topology consisted of 157 switches
with a diameter of 36. For each topology and every path, we esti-
mate the average and 99’th percentile number of packets needed for
decoding over 10K runs. We consider three variants of PINT– using
1-bit, 4-bit, and two independent 8-bit hash functions (denoted by
2 × (b = 8)). We compare PINT to two state-of-the-art IP Traceback
solutions PPM [65] and AMS2 [70] with m = 5 and m = 6. When
configured with m = 6, AMS2 requires more packets to infer the
path but also has a lower chance of false positives (multiple possible
paths) compared with m = 5. We implement an improved version
of both algorithms using Reservoir Sampling, as proposed in [63].
PINT is configured with d = 10 on the ISP topologies and d = 5
(as this is the diameter) on the fat tree topology. In both cases, this
means a single XOR layer in addition to a Baseline layer.
10
7K20K30K50K73K197K987K2M5M30MFlow Size [Bytes]246810Slowdownp=1=256p=1=16p=13243995005996999997K46K120K10MFlow Size [Bytes]246810Slowdownp=1=256p=1=16p=12004006008001000Sample Size [Packets]01020304050Relative Error [%]Web Search Tail2004006008001000Sample Size [Packets]Hadoop Tail2004006008001000Sample Size [Packets]Hadoop Median100200300Sketch Size [Bytes]01020304050Relative Error [%]100200300Sketch Size [Bytes]100200300Sketch Size [Bytes]PINT (b=8)PINT (b=4)PINTS (b=8)PINTS (b=4)(a) Kentucky Datalink (D = 59)
(b) US Carrier (D = 36)
(c) Fat Tree (D = 5)
(d) Kentucky Datalink (D = 59)
(e) US Carrier (D = 36)
(f) Fat Tree (D = 5)
Figure 10: Comparison of the number of packets required (lower is better) for path decoding of different algorithms,
with varying bit-budget.
including PINT
The results (Fig. 10) show that PINT significantly outperforms
previous works, even with a bit-budget of a single bit (PPM and
AMS both have an overhead of 16 bits per packet). As shown, the
required number of packets for PINT grows near-linearly with the
path length, validating our theoretical analysis. For the Kentucky
Datalink topology (D = 59), PINT with 2 × (b = 8) on average uses
25–36 times fewer packets when compared to competing approaches.
Even when using PINT with b = 1, PINT needs 7–10 times fewer
packets than competing approaches. For the largest number of hops
we evaluated (59, in the Kentucky Datalink topology), PINT requires
only 42 packets on average and 94 for the 99’th percentile, while
alternative approaches need at least 1–1.5K on average and 3.3–5K
for 99’th percentile, respectively.
6.4 Combined Experiment
We test the performance of PINT when running all three use cases
concurrently. Based on the previous experiments, we tune PINT to
run each query using a bit budget of 8 bits and a global budget of
16 bits. Our goal is to compare how PINT performs in such setting,
compared with running each application alone using 16 bits per
packet (i.e., with an effective budget of 3 × 16 bits). That is, each
packet can carry digests of two of the three concurrent queries.
As we observe that the congestion control application has good
11
performance when running in p = 1/16 of the packets, and the
path tracing requires more packets than the latency estimation, we
choose the following configuration. We run the path algorithm on
all packets, alongside the latency algorithm in 15/16 of the packets,
and alongside HPCC in 1/16 of the packets. As Fig. 11 shows, the
performance of PINT is close to a Baseline of running each query
separately. For estimating median latency, the relative error increases
by only 0.7% from the Baseline to the combined case. In case of
HPCC, we that observe short flows become 6.6% slower while the
Figure 11: The performance of each query in a concurrent execution
(FatTree topology + Hadoop workload) compared to running it alone.
61218243036424854Path Length [Hops]0500100015002000250030003500Average Number [Packets]  4812162024283236Path Length [Hops]020040060080010001200140016001800Average Number [Packets]  2345Path Length [Hops]020406080100120140160180Average Number [Packets]  PINT 2£(b=8)AMS2 (m=5)PINT (b=4)AMS2 (m=6)PINT (b=1)PPM61218243036424854Path Length [Hops]010002000300040005000600099th Percentile [Packets]4812162024283236Path Length [Hops]050010001500200025003000350099th Percentile [Packets]2345Path Length [Hops]05010015020025030035099th Percentile [Packets]BaselineCombined0.00.51.01.52.0SlowdownHPCC(PINT)BaselineCombined024681012Average Number [Packets]Path TracingBaselineCombined012345Error [%]Tail Latencyperformance of long flows does not degrade. As for path tracing, the
number of packets increases by 0.5% compared with using two 8
bit hashes as in Fig. 10. We conclude that, with a tailored execution
plan, our system can support these multiple concurrent telemetry
queries using an overhead of just two bytes per packet.
7 LIMITATIONS
In this section, we discuss the limitations associated with our proba-
bilistic approach. The main aspect to take into consideration is the
required per-packet bit-budget and the network diameter. The bigger
overhead allowed and the smaller the network, the more resilient
PINT will be in providing results in different scenarios.
Tracing short flows. PINT leverages multiple packets from the same
flow to infer its path. In our evaluation (§6), we show that our solu-
tion needs significantly fewer packets when compared to competing
approaches. However, in data center networks, small flows can con-
sist of just a single packet [3]. In this case, PINT is not effective and a
different solution, such as INT, would provide the required information.
Data plane complexity. Today’s programmable switches have a
limited number of pipeline stages. Although we show that it is possi-
ble to parallelize the processing of independent queries (§5), thus
saving resources, the PINT requirements might restrict the amount
of additional use cases to be implemented in the data plane, e.g., fast
reroute [18] or in-network caching [37] and load balancing [2, 42].
Tracing flows with multipath routing. The routing of a flow may
change over time (e.g., when using flowlet load balancing [2, 42])
or multiple paths can be taken simultaneously when appropriate
transport protocols such as Multipath TCP are used [25]. In those
cases, the values (i.e, switch IDs) for some hops will be different.
Here, PINT can detect routing changes when observing a digest
that is not consistent with the part of the path inferred so far. For
example, if we know that the sixth switch on a path is M6, and a
Baseline packet pj comes with a digest from this hop that is different
than h(M6, pj), then we can conclude that the path has changed. The
number of packets needed to identify a path change depends on
the fraction of the path that has been discovered. If path changes
are infrequent, and PINT knows the entire path before the change,
a Baseline packet will not be consistent with the known path (and
thus signify a path change) with probability 1 − 2−q. Overall, in the
presence of flowlet routing, PINT can still trace the path of each
flowlet, provided enough packets for each flowlet-path are received
at the sink. PINT can also profile all paths simultaneously at the cost
of additional overhead (e.g., by adding a path checksum to packets
we can associate each with the path it followed).
Current implementation. At the time of writing, the PINT exe-
cution plan is manually selected. We envision that an end to end
system that implements PINT would include a Query Engine that
automatically decides how to split the bit budget.
Other works can be classified into three main approaches: (1)
keep information out-of-band; (2) keep flow state at switches; or (3)
keep information on packets. The first approach applies when the
data plane status is recovered by using packet mirroring at switches
or by employing specially-crafted probe packets. Mirroring every
packet creates scalability concerns for both trace collection and anal-
ysis. The traffic in a large-scale data center network with hundreds of
thousands of servers can quickly introduce terabits of mirrored traf-
fic [28, 62]. Assuming a CPU core can process tracing traffic at 10
Gbps, thousands of cores would be required for trace analysis [87],
which is prohibitively expensive. Moreover, with mirroring it is not
possible to retrieve information related to switch status, such as port
utilization or queue occupancy, that are of paramount importance
for applications such as congestion control or network troubleshoot-
ing. While such information can be retrieved with specially-crafted
probes [74], the feedback loop may be too slow for applications like
high precision congestion control [46]. We can also store flow infor-
mation at switches and periodically export it to a collector [45, 69].
However, keeping state for a large number of active flows (e.g., up
to 100K [62]), in the case of path tracing, is challenging for lim-
ited switch space (e.g., 100 MB [51]). This is because operators
need the memory for essential control functions such as ACL rules,
customized forwarding [68], and other network functions and appli-
cations [37, 51]. Another challenge is that we may need to export
data plane status frequently (e.g., every 10 ms) to the collector, if we
want to enable applications such as congestion control. This creates
significant bandwidth and processing overheads [45].
Proposals that keep information on packets closely relate to this
work [36, 72, 75], with INT being considered the state-of-the-art
solution. Some of the approaches, e.g., Path Dump [72], show how
to leverage properties of the topology to encode only part of each
path (e.g., every other link). Nonetheless, this still imposes an over-
head that is linear in the path length, while PINT keeps it constant.
Alternative approaches add small digests to packets for tracing
paths [64, 65, 70]. However, they attempt to trace back to potential
attackers (e.g., they do not assume unique packet IDs or reliable
TTL values as these can be forged) and require significantly more
packets for identification, as we show in Section 6. In a recent effort
to reduce overheads on packets, similarly to this work, Taffet et
al. [71] propose having switches use Reservoir Sampling to collect
information about a packet’s path and congestion that the packet
encounters as it passes through the network. PINT takes the process
several steps further, including approximations and coding (XOR-
based or network coding) to reduce the cost of adding information
to packets as much as possible. Additionally, our work rigorously
proves performance bounds on the number of packets required to
recover the data plane status as well as proposes trade-offs between
data size and time to recover.
8 RELATED WORK
Many previous works aim at improving data plane visibility. Some
focus on specific flows selected by operators [56, 78, 87] or only on
randomly selected sampled flows [10, 21]. Such approaches are in-
sufficient for applications that need global visibility on all flows, such
as path tracing. Furthermore, the flows of interest may not be known
in advance, if we wish to debug high-latency or malicious flows.
9 CONCLUSION
We have presented PINT, a probabilistic framework to in-band
telemetry that provides similar visibility to INT while bounding
the per-packet overhead to a user-specified value. This is important
because overheads imposed on packets translate to inferior flow
completion time and application-level goodput. We have proven
performance bounds (deferred to Appendix A due to lack of space)
12
for PINT and have implemented it in P4 to ensure it can be readily
deployed on commodity switches. PINT goes beyond optimizing
INT by removing the header and using succinct switch IDs by re-
stricting the bit-overhead to a constant that is independent of the
path length. We have discussed the generality of PINT and demon-
strated its performance on three specific use cases: path tracing, data
plane telemetry for congestion control and estimation of experienced
median/tail latency. Using real topologies and traffic characteristics,
we have shown that PINT enables the use cases, while drastically
decreasing the required overheads on packets with respect to INT.
Acknowledgements. We thank the anonymous reviewers, Jiaqi
Gao, Muhammad Tirmazi, and our shepherd, Rachit Agarwal, for
helpful comments and feedback. This work is partially sponsored by
EPSRC project EP/P025374/1, by NSF grants #1829349, #1563710,
and #1535795, and by the Zuckerman Foundation.
13
REFERENCES
[1] 2020. PINT open source code: https://github.com/ProbabilisticINT.
(2020).
https://github.com/ProbabilisticINT
[2] Mohammad Alizadeh, Tom Edsall, Sarang Dharmapurikar, Ramanan
Vaidyanathan, Kevin Chu, Andy Fingerhut, Vinh The Lam, Francis Ma-
tus, Rong Pan, Navindra Yadav, and George Varghese. 2014. CONGA: Distributed
Congestion-aware Load Balancing for Datacenters. In ACM SIGCOMM.
[3] Mohammad Alizadeh, Albert Greenberg, David A. Maltz, Jitendra Padhye,
Parveen Patel, Balaji Prabhakar, Sudipta Sengupta, and Murari Sridharan. 2010.
Data Center TCP (DCTCP). In ACM SIGCOMM.
[4] Mohammad Alizadeh, Abdul Kabbani, Tom Edsall, Balaji Prabhakar, Amin Vah-
dat, and Masato Yasuda. 2012. Less is More: Trading a Little Bandwidth for
Ultra-Low Latency in the Data Center. In USENIX NSDI.
[5] Arvind Arasu and Gurmeet Singh Manku. 2004. Approximate Counts and Quan-
tiles over Sliding Windows. In ACM PODS.
[6] Behnaz Arzani, Selim Ciraci, Luiz Chamon, Yibo Zhu, Hongqiang Harry Liu, Jitu
Padhye, Boon Thau Loo, and Geoff Outhred. 2018. 007: Democratically Finding
the Cause of Packet Drops. In USENIX NSDI.
[7] Tom Barbette, Cyril Soldani, and Laurent Mathy. 2015. Fast Userspace Packet
[8] Barefoot. [n. d.]. Barefoot Deep Insight. https://barefootnetworks.com/products/
Processing. In IEEE/ACM ANCS.
brief-deep-insight/. ([n. d.]).
[9] Barefoot Networks. 2018. Barefoot Deep Insight. https://www.barefootnetworks.
com/static/app/pdf/DI-UG42-003ea-ProdBrief.pdf. (2018).
[10] Ran Ben Basat, Xiaoqi Chen, Gil Einziger, Shir Landau Feibish, Danny Raz, and
Minlan Yu. 2020. Routing Oblivious Measurement Analytics. In IFIP Networking.
[11] Ran Ben Basat, Gil Einziger, Isaac Keslassy, Ariel Orda, Shay Vargaftik, and Erez
Waisbard. 2018. Memento: Making Sliding Windows Efficient for Heavy Hitters.
In ACM CoNEXT.
[12] Ran Ben-Basat, Xiaoqi Chen, Gil Einziger, and Ori Rottenstreich. 2018. Efficient
Measurement on Programmable Switches using Probabilistic Recirculation. In
IEEE ICNP.
[13] Ran Ben-Basat, Gil Einziger, and Roy Friedman. 2018. Fast flow volume estima-
tion. Pervasive Mob. Comput. (2018).
[14] Broadcom. [n. d.]. Broadcom BCM56870 Series. https://www.broadcom.com/
products/ethernet-connectivity/switching/strataxgs/bcm56870-series. ([n. d.]).
[n.
[15] Robert L Carter and Mark E Crovella. 1997. Server selection using dynamic path
characterization in wide-area networks. In IEEE INFOCOM.
Runs
[16] SDX
White
att-runs-open-source-white-box-switch-live-network/2017/04/. ([n. d.]).
Source
https://www.sdxcentral.com/articles/news/
Central.
Box.
AT&T
Open
d.].
[17] Xiaoqi Chen, Shir Landau Feibish, Yaron Koral, Jennifer Rexford, Ori Rottenstre-
ich, Steven A Monetti, and Tzuu-Yi Wang. 2019. Fine-Grained Queue Measure-
ment in the Data Plane. In ACM CoNEXT.
[18] Marco Chiesa, Roshan Sedar, Gianni Antichi, Michael Borokhovich, Andrzej
Kamisiundefinedski, Georgios Nikolaidis, and Stefan Schmid. 2019. PURR: A
Primitive for Reconfigurable Fast Reroute. In ACM CoNEXT.
[19] Baek-Young Choi, Sue Moon, Rene Cruz, Zhi-Li Zhang, and Christophe Diot.
2007. Quantile Sampling for Practical Delay Monitoring in Internet Backbone
Networks. Computer Networks.
[20] Damu Ding, Marco Savi, and Domenico Siracusa. 2020. Estimating Logarithmic
and Exponential Functions to Track Network Traffic Entropy in P4. In IEEE/IFIP
NOMS.
[21] N. G. Duffield and Matthias Grossglauser. 2001. Trajectory Sampling for Direct
Traffic Observation. In IEEE/ACM ToN.
[22] Nandita Dukkipati and Nick McKeown. 2006. Why Flow-Completion Time is the
[23] David Felber and Rafail Ostrovsky. 2017. A Randomized Online Quantile Sum-
Right Metric for Congestion Control. ACM SIGCOMM CCR (2006).
mary in O((1/ε) log(1/ε)) Words. In Theory of Computing.
[24] Philippe Flajolet, Daniele Gardy, and Loÿs Thimonier. 1992. Birthday Paradox,
Coupon Collectors, Caching Algorithms and Self-organizing Search. Discrete
Applied Mathematics (1992).
[25] Alan Ford, Costin Raiciu, Mark J. Handley, and Olivier Bonaventure. 2013. TCP
Extensions for Multipath Operation with Multiple Addresses. (2013).
[26] Yilong Geng, Shiyu Liu, Zi Yin, Ashish Naik, Balaji Prabhakar, Mendel Rosen-
blum, and Amin Vahdat. 2019. SIMON: A Simple and Scalable Method for
Sensing, Inference and Measurement in Data Center Networks. In USENIX NSDI.
[27] Dimitrios Gkounis, Vasileios Kotronis, Christos Liaskos, and Xenofontas Dim-
itropoulos. 2016. On the Interplay of Link-Flooding Attacks and Traffic Engineer-
ing. ACM SIGCOMM CCR (2016).
[28] Chuanxiong Guo, Lihua Yuan, Dong Xiang, Yingnong Dang, Ray Huang, Dave
Maltz, Zhaoyi Liu, Vin Wang, Bin Pang, Hua Chen, Zhi-Wei Lin, and Varugis
Kurien. 2015. Pingmesh: A Large-Scale System for Data Center Network Latency
Measurement and Analysis. In ACM SIGCOMM.
[29] Dongsu Han, Robert Grandl, Aditya Akella, and Srinivasan Seshan. 2013. FCP: A