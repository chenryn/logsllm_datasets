Clown:  
Come, sir, I will make did behold your worship.
VIOLA:  
I'll drink it.
请记住，上面这些RNN都知道是字符，但是它学会了对说话者的姓名和内容进行采样。有时我们也会得到相对较长的独白段落，比如：
VIOLA:  
Why, Salisbury must find his flesh and thought  
That which I am not aps, not a man and in fire,  
To show the reining of the raven and the wars  
To grace my hand reproach within, and not a fair are hand,  
That Caesar and my goodly father's world;  
When I was heaven of presence and our fleets,  
We spare with hours, but cut thy council I am great,  
Murdered and by thy master's ready there  
My power to give thee but so much as hell:  
Some service in the noble bondman here,  
Would show him to her wine.
KING LEAR:  
O, if you were a feeble sight, the courtesy of your law,  
Your sight and several breath, will wear the gods  
With his heads, and my hands are wonder'd at the deeds,  
So drop upon your lordship's head, and your opinion  
Shall be against your honour.
我几乎无法从实际的莎士比亚文章中认出这些样本:)
如果你喜欢莎士比亚，你可能会喜欢这[100,000字符的样本](https://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt "100,000字符的样本")。当然，您也可以使用提供的代码在不同 temperatures 下生成无限量的样本。
# Wikipedia
我们看到LSTM可以学习拼写单词并复制一般的句法结构. 让我们进一步增加难度并用结构化markdown语法进行训练。让我们获取原始维基百科的Hutter
Prize 100MB数据集并训练LSTM。遵循 Graves et
al。我使用了第一个96MB数据进行培训，其余的用于验证，并在一夜之间运行了几个模型。我们现在可以提供维基百科文章样本。以下是一些有趣的摘录。首先，一些基markdown的输出：
Naturalism and decision for the majority of Arab countries' capitalide was
grounded  
by the Irish language by [[John Clair]], [[An Imperial Japanese Revolt]],
associated  
with Guangzham's sovereignty. His generals were the powerful ruler of the
Portugal  
in the [[Protestant Immineners]], which could be said to be directly in
Cantonese  
Communication, which followed a ceremony and set inspired prison, training.
The  
emperor travelled back to [[Antioch, Perth, October 25|21]] to note, the
Kingdom  
of Costa Rica, unsuccessful fashioned the [[Thrales]], [[Cynth's Dajoard]],
known  
in western [[Scotland]], near Italy to the conquest of India with the
conflict.  
Copyright was the succession of independence in the slop of Syrian influence
that  
was a famous German movement based on a more popular servicious, non-doctrinal  
and sexual power post. Many governments recognize the military housing of the  
[[Civil Liberalization and Infantry Resolution 265 National Party in
Hungary]],  
that is sympathetic to be to the [[Punjab Resolution]]  
(PJS)[.  
cfm/7754800786d17551963s89.htm Official economics Adjoint for the Nazism,
Montgomery  
was swear to advance to the resources for those Socialism's rule,  
was starting to signing a major tripad of aid exile.]]
如果你想知道，上面的雅虎网址实际上并不存在。该模型只是模仿了它。另外请注意，模型学会正确打开和关闭括号。模型学习也有很多结构化的 markdown
语法，例如有时会创建标题，列表等：
    { { cite journal | id=Cerling Nonforest Department|format=Newlymeslated|none } }
    ''www.e-complete''.
    '''See also''': [[List of ethical consent processing]]
    == See also ==
    *[[Iender dome of the ED]]
    *[[Anti-autism]]
    ===[[Religion|Religion]]===
    *[[French Writings]]
    *[[Maria]]
    *[[Revelation]]
    *[[Mount Agamul]]
    == External links==
    * [http://www.biblegateway.nih.gov/entrepre/ Website of the World Festival. The labour of India-county defeats at the Ripper of California Road.]
    ==External links==
    * [http://www.romanology.com/ Constitution of the Netherlands and Hispanic Competition for Bilabial and Commonwealth Industry (Republican Constitution of the Extent of the Netherlands)]
有时，模型会进入生成随机但有效的XML的模式：
      Antichrist
      865
        15900676
        2002-08-03T18:14:12Z
          Paris
          23
        Automated conversion
        #REDIRECT [[Christianity]]
该模型完整的构成时间戳，id等。另请注意，它会以正确的嵌套顺序正确关闭标记。如果您有兴趣了解更多内容，这里有[100,000个字符的采样维基百科](https://cs.stanford.edu/people/karpathy/char-rnn/wiki.txt "100,000个字符的采样维基百科")。
# Algebraic Geometry (代数几何 Latex )
上面的结果表明该模型实际上非常擅长学习复杂的句法结构。我们对这些结果印象深刻。我的同事([Justin
Johnson](https://cs.stanford.edu/people/jcjohns/ "Justin Johnson"))
和我决定进一步推进结构化领域并掌握这本关于代数 堆栈/几何
的书。我们下载了原始Latex源文件（16MB文件）并训练了多层LSTM。人惊讶的是，由此产生的样本Latex
几乎可以编译。我们不得不介入并手动解决一些问题，但是这些看起来似乎很合理，这是非常令人惊讶的：
$$ 样本（假）代数几何。[这是实际的pdf](https://cs.stanford.edu/people/jcjohns/fake-math/4.pdf
"这是实际的pdf")。$$
这是另一个例子：
$$ 更多模仿出来的代数几何。很好的尝试图（右） $$
正如您在上面所看到的，有时模型会尝试生成latex diagrams，但显然它并没有真正弄清楚它们。我也喜欢它选择跳过证明的部分（“Proof
ignored。”，左上角）。当然，请记住latex 具有相对困难的结构化语法格式，我自己甚至都没有完全掌握。例如，这是来自模型（未编辑）的原始样本：
\begin{proof}  
We may assume that $\mathcal{I}$ is an abelian sheaf on $\mathcal{C}$.  
\item Given a morphism $\Delta : \mathcal{F} \to \mathcal{I}$  
is an injective and let $\mathfrak q$ be an abelian sheaf on $X$.  
Let $\mathcal{F}$ be a fibered complex. Let $\mathcal{F}$ be a category.  
\begin{enumerate}  
\item \hyperref[setain-construction-phantom]{Lemma}  
\label{lemma-characterize-quasi-finite}  
Let $\mathcal{F}$ be an abelian quasi-coherent sheaf on $\mathcal{C}$.  
Let $\mathcal{F}$ be a coherent $\mathcal{O}_X$-module. Then  
$\mathcal{F}$ is an abelian catenary over $\mathcal{C}$.  
\item The following are equivalent  
\begin{enumerate}  
\item $\mathcal{F}$ is an $\mathcal{O}_X$-module.  
\end{lemma}
这个样本来自相对不错的模型，它说明了一些常见的错误。例如，模型由\begin{proof}开始，却用\end{lemma}结束它。这个是我们需要手工调整的问题的一个例子，，可能是由于依赖性太长期这一事件导致：当模型完成证明时，它已经忘记了它是否正在进行证明或是进行一个引理。类似的情况，它\begin{enumerate}开始，但是忘了结束它。我们观察到这些在
更大/更好 的模型中变得不那么常见，但尽管如此，这些都是出现过的错误。
# Linux 源码
我想将结构化数据推向极限，因此对于最后的挑战，我决定使用代码。我在Github上获取了[Linux
repo](https://github.com/torvalds/linux "Linux
repo")中的所有源文件和头文件，将它们连接在一个巨大的文件中（474MB的C代码）（我原本只会在内核上进行训练，但这本身就是只有~16MB）。然后我在几天的时间内训练了几个与我相同的GPU
3层LSTM。这些模型有大约1000万个参数，对于RNN模型来说仍然处于较低情况。结果是非常有趣的：
    /*
     * Increment the size file of the new incorrect UI_FILTER group information
     * of the size generatively.
     */