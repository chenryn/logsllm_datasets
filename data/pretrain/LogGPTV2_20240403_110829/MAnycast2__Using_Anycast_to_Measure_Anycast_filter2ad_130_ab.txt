9
1
1234
Table 1: Classification and Breakdown of /24s by number of
Tangled VPs that receive responses
Under recommended best practices, /24 is the most-specific prefix
length that globally propagates via BGP on the Internet [20]. For
this reason, once we classify a target IP address as anycast or unicast,
we extend this classification to its covering /24.
4 PRELIMINARY RESULTS
We next describe our preliminary results when applying our method
to infer which prefixes in a large set of IPv4 addresses are anycast
(§4.1). We report the results of a validation we perform against
popular anycast platforms such as root servers, public recursive
resolvers (§4.2), and other services (§4.3). Finally, we compare our
results against iGreedy (§4.4) and the passive approach in [6] (§4.5).
4.1 IPv4 anycast measurement
Table 1 classifies /24 prefixes, and their originating ASNs, by the
number of Tangled’s anycast VPs that received ICMP Echo Replies
in response to our probes. On May 5, 2020, we tested all 6,125,756
target /24 prefixes from the ISI IPv4 hitlist, choosing 1 IP for each /24
prefix. Of these, 3.47 million (56.5%) responded to at least one probe.
For 3.45 million /24 prefixes (99.55% of the responding prefixes),
only one VP received ICMP Echo Replies, so we classified these
prefixes as unicast. For another 15,540 (0.45%) /24 prefixes, between
2 and 10 VPs received ICMP Echo Replies. We considered them
candidate anycast prefixes. In our 10-node testbed, we completed
measurements for the entire ISI IPv4 hitlist [13] in ∼2.5 hours,
requiring only 10 pings per target IP (one from each node).
4.2 Ground Truth Validation
Our first approach to validation used DNS root servers and public
DNS resolvers. We correctly classifed all root servers as anycast ex-
cept C-Root. C-Root’s misclassification was a false negative (i.e., we
failed to detect an anycast IP). We also correctly classified the any-
cast prefixes serving three of the four major public DNS resolvers:
CloudFlare, OpenDNS, and Quad9. However, we incorrectly classi-
fied as unicast the prefix for the Google Public DNS Resolver. We
discuss the cases we misclassified in §5.
4.3 Validation from AS Operators
Table 2 shows the Top-10 anycast providers in the Internet by
number of /24 prefixes that we classified as anycast. For prefixes that
we classified as anycast, we verified if the iGreedy technique came
to the same conclusion. We also asked for ground truth validation
IMC ’20, October 27–29, 2020, Virtual Event, USA
Sommese, Bertholdo, Akiwate, Jonker, Rijswijk-Deij, Dainotti, Claffy and Sperotto
ASN
Org Name
16509
13335
32475
54113
12041
16625,21342,24319
26008,32787,35994
20940,34164,34164
701,703,2828
14153,15133
42
15169,19527
36040,36384
12008,19905,19911
Amazon
Cloudflare
SingleHop
Fastly
Afilias
Akamai
Verizon
PCH
Google
NeuStar
MAnycast2
4870
3127
747
285
215
#Prefixes
iGreedy
520
3127
0
213
214
Operator
524
Confirmed
212
154
134
133
110
89
153
134
29
110
90
134
Table 2: Top 10 Anycast ASes detected by the MAnycast2
measurement, validated with iGreedy and operators
of our inferences to four well-known operators of anycast services:
Amazon, Cloudflare, Afilias, and Packet Clearing House/PCH.
For some operators, like SingleHop, and, to some degree, Akamai
and Google, we found conflicting results between iGreedy and our
methodology. For Amazon, our methodology detected 10× as many
anycast prefixes as iGreedy. Looking at reverse DNS data for these
prefixes, it appears that 3,555 prefixes we detected as anycast were
related to EC2 instances, unlikely to be anycast given that Amazon
does not offer anycast service on EC2 IP addresses. We conclude
that AWS’s routing policies (§5) might mislead our methodology.
In private communication with us, Amazon reported 524 /24 blocks
as anycast. We discovered 520 of these; using the list of public IP
ranges of AWS [1], we discovered that they belonged to AWS Global
Accelerator, an anycast platform that AWS operates to support its
customers [26]. The remaining 4 /24 prefixes also belonged to Global
Accelerator and are supposed to be anycast, but both our method
and iGreedy incorrectly inferred them as unicast.
Cloudflare has one of the largest anycast deployments. We de-
tected 3,127 /24 anycast prefixes, consistent with iGreedy’s infer-
ences. Cloudflare operators confirmed our overall findings, with
the caveat that they are revising their address assignment plan,
preventing an exact prefix-by-prefix confirmation.
PCH confirmed our inference of 134 /24 prefixes as anycast.
GoDaddy, not in the Top-10 list, confirmed that we correctly classify
its two IP ranges as anycast. Microsoft, also not in the Top-10 list,
confirmed that we correctly detected 51 of their 75 /24 anycast
prefixes, and that we misclassified 7 /24 unicast prefixes as anycast.
4.4 Comparison against iGreedy
To compare our results with those obtained by iGreedy (Table 3), we
ran iGreedy against a set of target addresses. We used 200 random
RIPE Atlas probes, as geographically diverse unicast measurement
nodes. We manually verified that there were at least 4 probes per
continent, to limit the bias of the heavy representation of Europe in
RIPE Atlas probe deployment. Given the performance bottleneck
of a large-scale measurement with iGreedy (§ 2.1), we sampled ∼2%
of prefixes we identified as unicast and tested them with iGreedy.
We then ran iGreedy against all prefixes in our anycast candidate
set. In total, we ran iGreedy on 82,270 /24 prefixes.
# VPs
Class.
# /24
1
2
3
4
5
6
7
10
Uni
Any
Any
Any
Any
Any
Any
Any
66730
10393
719
1378
2467
567
13
3
iGreedy Classification
Unresp.
Uni
0
66658
1434
8072
23
93
3
0
0
0
0
0
0
0
0
0
Any
72
887
603
1375
2467
567
13
3
% Diff.
(resp.)
0.1%
90.1%
13.3%
0.2%
0%
0%
0%
0%
Table 3: Comparison against iGreedy: We show MAnycast2
classification, iGreedy classification and difference between
their results
We observed slight differences, as low as 0.1% for the sample
unicast prefixes, indicating low false negative rates. If we received
answers on 4 distinct VPs, the percentage difference was 0.2%, while
when we received answers on 5 or more distinct VPs, our results
agreed with iGreedy. The disparity was extremely high when we
received responses at only two distinct VPs (90%), and although it
significantly dropped, it was still higher than 10% for cases where
we received responses at three distinct VPs. We suspect that the
disparity when there were few VPs derives from routing dynamics,
which we discuss further in §5.
4.5 Preliminary comparison against a passive
approach
Another technique for detecting anycast deployment is the pas-
sive approach proposed by Bian et al. [6], which relies on applying
machine-learning classification to features extracted from BGP
data (§2). Unfortunately, up-to-date ground-truth data to train this
classifier is not available—the last iGreedy Census was performed
in April 2017—preventing us from performing a fair comparison
against MAnycast2. However, to attempt a preliminary comparison,
we ran a trained classifier provided by the authors of [6] (i.e., trained
with the iGreedy Census data from April 2017) and applied it to
current BGP data (from May 05, 2020). Of the 5,915 prefixes marked
by MAnycast2 and iGreedy as anycast, the passive approach clas-
sified only 3,899 of these prefixes as anycast. Because we did not
update this 3-year-old model, we cannot tell if the limited overlap
with MAnycast2/iGreedy is due to the model needing retraining,
anycast deployments that were not visible in the signals available to
the passive approach, or both. In any case, we expect the key BGP
properties used by the classifier to not change significantly over 3
years, allowing us to gather some first-hand insight by manually
inspecting some cases of potential misclassification.
With respect to false positives (i.e., unicast classified as any-
cast), we found cases where the passive approach, in contrast with
MAnycast2 and iGreedy, classified large prefixes (i.e., /8, /10) as any-
cast. This yielded many /24 blocks marked as anycast. The reason
behind this misclassification is related to the nature of the ASes that
own such prefixes, which are large backbone Internet providers.
In this case, the high number of providers interconnected to these
networks misleads the passive approach to classify them as anycast.
MAnycast2 classified all these cases as unicast in agreement with
MAnycast2 – Using Anycast to Measure Anycast
IMC ’20, October 27–29, 2020, Virtual Event, USA
iGreedy. Moreover, as studied in [6], remote peering is another
cause of misclassification.
With respect to false negatives, we identified some interesting
cases that mislead the passive approach. Consider Neustar a large
anycast operator that the passive approach misclassified. Neustar
used an ASN for their anycast network that was directly connected
to another Neustar ASN as upstream provider. Accordingly, the
classifier saw only one upstream provider and marked these prefixes
as unicast. A similar phenomenon happened for other operators,
such as Akamai. This cause of misclassification was also identified
in [6] as a common cause for false positives. We found other cases
of false-negative misclassification where only one classification
feature (specifically a long observed AS path) was indicative of
unicast behavior. We believe this might be an artifact of using a
classifier trained with older ground truth.
Finally, we examined the anycast deployment size inferred by
iGreedy for all the /24 blocks where it agreed with MAnycast2. The
distribution of prefixes misclassified (FN) by the passive approach
was largely skewed toward small deployments. These results are
preliminary. We plan to conduct a new anycast census by pipelining
MAnycast2 and iGreedy, which will allow us to retrain the classifier
developed by Bian et al. [6].
5 OPEN CHALLENGES
Our experiments with MAnycast2 show that the approach is promis-
ing, with a low (0.1%) false negative rate (anycast addresses mis-
takenly classified as unicast) (§4) and, if responses are received
at more than 4 VPs, a low or zero false positive rate. As we dis-
cussed in §4.2, however, our MAnycast2 approach misclassifies two
prominent anycast services (C-Root and Google Public DNS), and
provides ambiguous results when only 2-3 VPs receive responses.
In this section we consider open challenges that underlie these
wrong inferences, and how future work may overcome these.
5.1 Conditions for Success
To understand the open challenges, we first need to consider the
necessary conditions for the MAnycast2 approach to successfully
detect prefixes as anycast. The most important factor in this context
is connectivity between the vantage points and the anycast service
we are trying to detect. As demonstrated in [22], the number and
type of upstream providers of an anycast network have a significant
impact on the interaction of the anycast network with the Inter-
net. Anycast networks with many upstream providers have more
options to manipulate their BGP announcements to improve the
catchments of their constituent sites. In the same way, connectivity
impacts MAnycast2 measurements.
This raises the question: What are the minimum conditions, in
terms of connectivity, for our methodology to detect an anycast de-
ployment? From a theoretical point of view, the simplest answer
to this question is that there should be at least two VPs that prefer
different PoPs, which themselves prefer different VPs. This will re-
sult in traffic routed back to two different VPs in our measurement,