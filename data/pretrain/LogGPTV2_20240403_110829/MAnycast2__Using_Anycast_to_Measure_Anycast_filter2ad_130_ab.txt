# Table 1: Classification and Breakdown of /24s by Number of Tangled VPs Receiving Responses

Under recommended best practices, /24 is the most specific prefix length that globally propagates via BGP on the Internet [20]. For this reason, once we classify a target IP address as anycast or unicast, we extend this classification to its covering /24.

## 4. Preliminary Results

We next describe our preliminary results when applying our method to infer which prefixes in a large set of IPv4 addresses are anycast (§4.1). We report the results of a validation we perform against popular anycast platforms such as root servers, public recursive resolvers (§4.2), and other services (§4.3). Finally, we compare our results against iGreedy (§4.4) and the passive approach in [6] (§4.5).

### 4.1. IPv4 Anycast Measurement

Table 1 classifies /24 prefixes and their originating ASNs by the number of Tangled's anycast VPs that received ICMP Echo Replies in response to our probes. On May 5, 2020, we tested all 6,125,756 target /24 prefixes from the ISI IPv4 hitlist, choosing one IP for each /24 prefix. Of these, 3.47 million (56.5%) responded to at least one probe. For 3.45 million /24 prefixes (99.55% of the responding prefixes), only one VP received ICMP Echo Replies, so we classified these prefixes as unicast. For another 15,540 (0.45%) /24 prefixes, between 2 and 10 VPs received ICMP Echo Replies. We considered them candidate anycast prefixes. In our 10-node testbed, we completed measurements for the entire ISI IPv4 hitlist [13] in approximately 2.5 hours, requiring only 10 pings per target IP (one from each node).

### 4.2. Ground Truth Validation

Our first approach to validation used DNS root servers and public DNS resolvers. We correctly classified all root servers as anycast except C-Root, which was a false negative (i.e., we failed to detect an anycast IP). We also correctly classified the anycast prefixes serving three of the four major public DNS resolvers: Cloudflare, OpenDNS, and Quad9. However, we incorrectly classified the prefix for the Google Public DNS Resolver as unicast. We discuss the cases we misclassified in §5.

### 4.3. Validation from AS Operators

Table 2 shows the top-10 anycast providers on the Internet by the number of /24 prefixes that we classified as anycast. For prefixes that we classified as anycast, we verified if the iGreedy technique came to the same conclusion. We also sought ground truth validation from four well-known operators of anycast services: Amazon, Cloudflare, Afilias, and Packet Clearing House/PCH.

For some operators, like SingleHop, and to some degree, Akamai and Google, we found conflicting results between iGreedy and our methodology. For Amazon, our methodology detected 10 times as many anycast prefixes as iGreedy. Looking at reverse DNS data for these prefixes, it appears that 3,555 prefixes we detected as anycast were related to EC2 instances, which are unlikely to be anycast given that Amazon does not offer anycast service on EC2 IP addresses. We conclude that AWS’s routing policies (§5) might mislead our methodology. In private communication with us, Amazon reported 524 /24 blocks as anycast. We discovered 520 of these; using the list of public IP ranges of AWS [1], we discovered that they belonged to AWS Global Accelerator, an anycast platform that AWS operates to support its customers [26]. The remaining 4 /24 prefixes also belonged to Global Accelerator and are supposed to be anycast, but both our method and iGreedy incorrectly inferred them as unicast.

Cloudflare has one of the largest anycast deployments. We detected 3,127 /24 anycast prefixes, consistent with iGreedy’s inferences. Cloudflare operators confirmed our overall findings, with the caveat that they are revising their address assignment plan, preventing an exact prefix-by-prefix confirmation.

PCH confirmed our inference of 134 /24 prefixes as anycast. GoDaddy, not in the Top-10 list, confirmed that we correctly classified its two IP ranges as anycast. Microsoft, also not in the Top-10 list, confirmed that we correctly detected 51 of their 75 /24 anycast prefixes, and that we misclassified 7 /24 unicast prefixes as anycast.

### 4.4. Comparison Against iGreedy

To compare our results with those obtained by iGreedy (Table 3), we ran iGreedy against a set of target addresses. We used 200 random RIPE Atlas probes, ensuring at least 4 probes per continent to limit the bias of the heavy representation of Europe in RIPE Atlas probe deployment. Given the performance bottleneck of a large-scale measurement with iGreedy (§2.1), we sampled approximately 2% of prefixes we identified as unicast and tested them with iGreedy. We then ran iGreedy against all prefixes in our anycast candidate set. In total, we ran iGreedy on 82,270 /24 prefixes.

| # VPs | Class. | # /24 | iGreedy Classification | % Diff. (resp.) |
|-------|--------|-------|------------------------|-----------------|
| 1     | Uni    | 66730 | Unresp.                | 0.1%            |
| 2     | Any    | 10393 | 887                    | 90.1%           |
| 3     | Any    | 719   | 603                    | 13.3%           |
| 4     | Any    | 1378  | 1375                   | 0.2%            |
| 5     | Any    | 2467  | 2467                   | 0%              |
| 6     | Any    | 567   | 567                    | 0%              |
| 7     | Any    | 13    | 13                     | 0%              |
| 10    | Any    | 3     | 3                      | 0%              |

We observed slight differences, as low as 0.1% for the sample unicast prefixes, indicating low false negative rates. If we received answers on 4 distinct VPs, the percentage difference was 0.2%, while when we received answers on 5 or more distinct VPs, our results agreed with iGreedy. The disparity was extremely high when we received responses at only two distinct VPs (90%), and although it significantly dropped, it was still higher than 10% for cases where we received responses at three distinct VPs. We suspect that the disparity when there were few VPs derives from routing dynamics, which we discuss further in §5.

### 4.5. Preliminary Comparison Against a Passive Approach

Another technique for detecting anycast deployment is the passive approach proposed by Bian et al. [6], which relies on applying machine-learning classification to features extracted from BGP data (§2). Unfortunately, up-to-date ground-truth data to train this classifier is not available—the last iGreedy Census was performed in April 2017—preventing us from performing a fair comparison against MAnycast2. However, to attempt a preliminary comparison, we ran a trained classifier provided by the authors of [6] (i.e., trained with the iGreedy Census data from April 2017) and applied it to current BGP data (from May 05, 2020). Of the 5,915 prefixes marked by MAnycast2 and iGreedy as anycast, the passive approach classified only 3,899 of these prefixes as anycast. Because we did not update this 3-year-old model, we cannot tell if the limited overlap with MAnycast2/iGreedy is due to the model needing retraining, anycast deployments that were not visible in the signals available to the passive approach, or both. In any case, we expect the key BGP properties used by the classifier to not change significantly over 3 years, allowing us to gather some first-hand insight by manually inspecting some cases of potential misclassification.

With respect to false positives (i.e., unicast classified as anycast), we found cases where the passive approach, in contrast with MAnycast2 and iGreedy, classified large prefixes (i.e., /8, /10) as anycast. This yielded many /24 blocks marked as anycast. The reason behind this misclassification is related to the nature of the ASes that own such prefixes, which are large backbone Internet providers. In this case, the high number of providers interconnected to these networks misleads the passive approach to classify them as anycast. MAnycast2 classified all these cases as unicast in agreement with iGreedy. Moreover, as studied in [6], remote peering is another cause of misclassification.

With respect to false negatives, we identified some interesting cases that misled the passive approach. Consider Neustar, a large anycast operator that the passive approach misclassified. Neustar used an ASN for their anycast network that was directly connected to another Neustar ASN as an upstream provider. Accordingly, the classifier saw only one upstream provider and marked these prefixes as unicast. A similar phenomenon happened for other operators, such as Akamai. This cause of misclassification was also identified in [6] as a common cause for false positives. We found other cases of false-negative misclassification where only one classification feature (specifically a long observed AS path) was indicative of unicast behavior. We believe this might be an artifact of using a classifier trained with older ground truth.

Finally, we examined the anycast deployment size inferred by iGreedy for all the /24 blocks where it agreed with MAnycast2. The distribution of prefixes misclassified (FN) by the passive approach was largely skewed toward small deployments. These results are preliminary. We plan to conduct a new anycast census by pipelining MAnycast2 and iGreedy, which will allow us to retrain the classifier developed by Bian et al. [6].

## 5. Open Challenges

Our experiments with MAnycast2 show that the approach is promising, with a low (0.1%) false negative rate (anycast addresses mistakenly classified as unicast) (§4) and, if responses are received at more than 4 VPs, a low or zero false positive rate. As we discussed in §4.2, however, our MAnycast2 approach misclassifies two prominent anycast services (C-Root and Google Public DNS) and provides ambiguous results when only 2-3 VPs receive responses. In this section, we consider open challenges that underlie these wrong inferences and how future work may overcome these.

### 5.1. Conditions for Success

To understand the open challenges, we first need to consider the necessary conditions for the MAnycast2 approach to successfully detect prefixes as anycast. The most important factor in this context is connectivity between the vantage points and the anycast service we are trying to detect. As demonstrated in [22], the number and type of upstream providers of an anycast network have a significant impact on the interaction of the anycast network with the Internet. Anycast networks with many upstream providers have more options to manipulate their BGP announcements to improve the catchments of their constituent sites. In the same way, connectivity impacts MAnycast2 measurements.

This raises the question: What are the minimum conditions, in terms of connectivity, for our methodology to detect an anycast deployment? From a theoretical point of view, the simplest answer to this question is that there should be at least two VPs that prefer different PoPs, which themselves prefer different VPs. This will result in traffic routed back to two different VPs in our measurement.