fers. All traces were captured using a Cisco port span. To account
for delay introduced by the packet duplication mechanism and for
endhost clock skew, we binned results from the spans into 10 mi-
crosecond bins.
3.2 High-level Usage of the Data Centers
In this section, we outline important high-level similarities and
differences among the data centers we studied.
University data centers: These data centers serve the students
and administrative staff of the university in question. They pro-
vide a variety of services, ranging from system back-ups to hosting
distributed ﬁle systems, E-mail servers, Web services (administra-
269Data Center
Data Center
Location
Age (Years)
SNMP
Role
Universities
Private
Commercial
Name
EDU1
EDU2
EDU3
PRV1
PRV2
CLD1
CLD2
CLD3
CLD4
CLD5
(Curr Ver/Total)
US-Mid
US-Mid
US-Mid
US-Mid
US-West
US-West
US-West
US-East
S. America
S. America
10
(7/20)
N/A
(5/5)
> 5
> 5
> 5
> 5
(3/3)
(3/3)
Packet
Traces
Topology
Number
Devices
Number
Servers
Over
Subscription
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
22
36
1
96
100
562
763
612
427
427
500
1093
147
1088
2000
10K
15K
12K
10K
10K
2:1
47:1
147:1
8:3
48:10
20:1
20:1
20:1
20:1
20:1
Table 2: Summary of the 10 data centers studied, including devices, types of information collected, and the number of servers.
tive sites and web portals for students and faculty), and multicast
video streams. We provide the exact application mix in the next
section. In talking to the network operators, we found that these
data centers “organically” evolved over time, moving from a col-
lection of devices in a storage closet to a dedicated room for servers
and network devices. As the data centers reached capacity, the op-
erators re-evaluated their design and architecture. Many operators
chose to move to a more structured, two-layer topology and intro-
duced server virtualization to reduce heating and power require-
ments while controlling data center size.
Private enterprises: The private enterprise IT data centers serve
corporate users, developers, and a small number of customers. Un-
like university data centers, the private enterprise data centers sup-
port a signiﬁcant number of custom applications, in addition to
hosting traditional services like Email, storage, and Web services.
They often act as development testbeds, as well. These data cen-
ters are developed in a ground-up fashion, being designed specif-
ically to support the demands of the enterprise. For instance, to
satisfy the need to support administrative services and beta testing
of database-dependent products, PRV1 commissioned the develop-
ment of an in-house data center 5 years ago. PRV2 was designed
over 5 years ago mostly to support custom Line-of-Business appli-
cations and to provide login servers for remote users.
Commercial cloud data centers: Unlike the ﬁrst two classes
of data centers, the commercial data centers cater to external users
and offer support for a wide range of Internet-facing services, in-
cluding: Instant Messaging, Webmail, search, indexing, and video.
Additionally, the data centers host large internal systems that sup-
port the externally visible services, for example data mining, stor-
age, and relational databases (e.g., for buddy lists). These data cen-
ters are often purpose-built to support a speciﬁc set of applications
(e.g., with a particular topology or over-subscription ratio to some
target application patterns), but there is also a tension to make them
as general as possible so that the application mix can change over
time as the usage evolves. CLD1, CLD2, CLD3 host a variety of
applications, ranging from Instant Messaging and Webmail to ad-
vertisements and web portals. CLD4 and CLD5 are primarily used
for running MapReduce style applications.
3.3 Topology and Composition of the Data Cen-
ters
In this section, we examine the differences and similarities in
the physical construction of the data centers. Before proceeding
to examine the physical topology of the data centers studied, we
present a brief overview of the topology of a generic data center. In
Figure 1, we present a canonical 3-Tiered data center. The 3 tiers of
the data center are the edge tier, which consists of the Top-of-Rack
switches that connect the servers to the data center’s network fabric;
the aggregation tier, which consists of devices that interconnect the
Figure 1: Canonical 3-Tier data center topology.
ToR switches in the edge layer; and the core tier, which consists
of devices that connect the data center to the WAN. In smaller data
centers, the core tier and the aggregation tier are collapsed into one
tier, resulting in a 2-Tiered data center topology.
Now, we focus on topological structure and the key physical
properties of the constituent devices and links. We ﬁnd that the
topology of the data center is often an accident of history. Some
have regular patterns that could be leveraged for trafﬁc engineer-
ing strategies like Valiant Load Balancing [11], while most would
require either a signiﬁcant upgrade or more general strategies.
Topology. Of the three university data centers, we ﬁnd that two
(EDU1, EDU2) have evolved into a structured 2-Tier architecture.
The third (EDU3) uses a star-like topology with a high-capacity
central switch interconnecting a collection of server racks – a de-
sign that has been used since the inception of this data center. As
of this writing, the data center was migrating to a more structured
set-up similar to the other two.
EDU1 uses a topology that is similar to a canonical 2-Tier ar-
chitecture, with one key difference: while the canonical 2-Tier data
centers use Top-of-Rack switches, where each switch connects to a
rack of 20-80 servers or so, these two data centers utilize Middle-
of-Rack switches that connect a row of 5 to 6 racks with the po-
tential to connect from 120 to 180 servers. We ﬁnd that similar
conclusions hold for EDU2 (omitted for brevity).
The enterprise data centers do not deviate much from textbook-
style constructions. In particular, the PRV1 enterprise data center
utilizes a canonical 2-Tier Cisco architecture. The PRV2 data cen-
ter utilizes a canonical 3-Tier Cisco architecture.
270p
p
A
r
e
P
s
e
t
y
B
f
o
t
n
e
c
r
e
P
 100
 80
 60
 40
 20
 0
PRV21 PRV22 PRV23 PRV24 EDU1 EDU2 EDU3
Data Center Edge Switches
OTHER
HTTP
HTTPS
LDAP
SMB
NCP
AFS
Figure 2: Classiﬁcation of network trafﬁc to application using
Bro-Id. Each of the sniffers sees a very different mix of appli-
cations, even though the ﬁrst 4 sniffers are located on different
switches in the same data center.
Note that we do not have the physical topologies from the cloud
data centers, although the operators of these data centers tell us that
these networks uniformly employ the 3-Tier textbook data center
architectures described in [11].
4. APPLICATIONS IN DATA CENTERS
We begin our “top-down” analysis of data centers by ﬁrst focus-
ing on the applications they run. In particular, we aim to answer
the following questions: (1) What type of applications are running
within these data centers? and, (2) What fraction of trafﬁc origi-
nated by a switch is contributed by each application?
We employ packet trace data in this analysis and use Bro-Id [26]
to perform application classiﬁcation. Recall that we collected packet
trace data for 7 switches spanning 4 data centers, namely, the uni-
versity campus data centers, EDU1, EDU2, and EDU3, and a pri-
vate enterprise data center, PRV2. To lend further weight to our
observations, we spoke to the operators of each data center, includ-
ing the 6 for which we did not have packet trace data. The operators
provided us with additional information about the speciﬁc applica-
tions running in their data centers.
The type of applications found at each edge switch, along with
their relative trafﬁc volumes, are shown in Figure 2. Each bar corre-
sponds to a sniffer in a data center, and the ﬁrst 4 bars are from the 4
edges switches within the same data center (PRV2). In conversing
with the operators, we discovered that this data center hosts a mix-
ture of authentication services (labeled “LDAP”), 3-Tier Line-Of-
Business Web applications (captured in “HTTP” and “HTTPS”),
and custom home-brewed applications (captured in “Others”).
By looking at the composition of the 4 bars for PRV2, we can in-
fer how the services and applications are deployed across racks in
the data center. We ﬁnd that each of the edge switches monitored
hosts a portion of the back-end for the custom applications (cap-
tured in “Others”). In particular, the rack corresponding to PRV24
appears to predominantly host custom applications that contribute
over 90% of the trafﬁc from this switch. At the other switches,
these applications make up 50%, 25%, and 10% of the bytes, re-
spectively.
Further, we ﬁnd that the secure portions of the Line-of-Business
Web services (labeled “HTTPS”) are hosted in the rack correspond-
ing to the edge switch PRV22, but not in the other three racks
monitored. Authentication services (labeled “LDAP”) are deployed
across the racks corresponding to PRV21 and PRV22, which makes
up a signiﬁcant fraction of bytes from these switches (40% of the
bytes from PRV21 and 25% of the byes from PRV22). A small
amount of LDAP trafﬁc (2% of all bytes on average) originates
from the other two switches, as well, but this is mostly request traf-
ﬁc headed for the authentication services in PRV21 and PRV22.
Finally, the unsecured portions of the Line-of-Business (consist-
ing of help pages and basic documentation) are located predom-
inantly on the rack corresponding to the edge switch PRV23—
nearly 85% of the trafﬁc originating from this rack is HTTP.
We also see some amount of ﬁle-system trafﬁc (SMB) across all
the 4 switches (roughly 4% of the bytes on average).
Clustering of application components within this data center leads
us to believe that emerging patterns of virtualization and consol-
idations have not yet led to applications being spread across the
switches.
Next, we focus on the last 3 bars, which correspond to an edge
switch each in the 3 university data centers, EDU1, EDU2 and
EDU3. While these 3 data centers serve the same types of users we
observe variations across the networks. Two of the university data
centers, EDU2 and EDU3, seem to primarily utilize the network for
distributed ﬁle systems trafﬁc, namely AFS and NCP—AFS makes
up nearly all the trafﬁc seen at the EDU3 switch, while NCP con-
stitutes nearly 80% of the trafﬁc at the EDU2 switch. The trafﬁc
at the last data center, EDU1, is split 60/40 between Web services
(both HTTP and HTTPS) and other applications such as ﬁle sharing
(SMB). The operator of this data center tells us that the data center
also hosts payroll and beneﬁts applications, which are captured in
“Others.”
Note that we ﬁnd ﬁle system trafﬁc to constitute a more signiﬁ-
cant fraction of the switches in the university data centers we mon-
itored compared to the enterprise data center.
The key take-aways from the above observations are that (1)
There is a wide variety of applications observed both within and
across data centers, such as “regular” and secure HTTP transac-
tions, authentication services, ﬁle-system trafﬁc, and custom ap-
plications and (2) We observe a wide variation in the composition
of trafﬁc originated by the switches in a given data center (see the
4 switches corresponding to PRV2). This implies that one cannot
assume that applications are placed uniformly at random in data
centers.
For the remaining data centers (i.e., PRV1, CLD1–5), where we
did not have access to packet traces, we used information from op-
erators to understand the application mix. CLD4 and CLD5 are
utilized for running MapReduce jobs, with each job, scheduled to
pack as many of its nodes as possible into the same rack to reduce
demand on the data center’s core interconnect. In contrast, CLD1,
CLD2, and CLD3 host a variety of applications, ranging from mes-
saging and Webmail to Web portals. Each of these applications is
comprised of multiple components with intricate dependencies, de-
ployed across the entire data center. For example, the Web portal
requires access to an authentication service for verifying users, and
it also requires access to a wide range of Web services from which
data is aggregated. Instant Messaging similarly utilizes an authen-
tication service and composes the user’s buddy list by aggregating
data spread across different data stores. The application mix found
in the data centers impacts the trafﬁc results, which we look at next.
2715. APPLICATION COMMUNICATION PAT-
TERNS
In the previous section, we described the set of applications run-
ning in each of the 10 data centers and observed that a variety of
applications run in the data centers and that their placement is non-
uniform. In this section, we analyze the aggregate network trans-
mission behavior of the applications, both at the ﬂow-level and at
the ﬁner-grained packet-level. Speciﬁcally, we aim to answer the
following questions: (1) What are the aggregate characteristics of
ﬂow arrivals, sizes, and durations? and (2) What are the aggre-
gate characteristics of the packet-level inter-arrival process across
all applications in a rack — that is, how bursty are the transmis-
sion patterns of these applications? These aspects have important
implications for the performance of the network and its links.
As before, we use the packet traces in our analysis.
5.1 Flow-Level Communication Characteris-
tics
First, we examine the number of active ﬂows across the 4 data
centers where we have packet-level data, EDU1, EDU2, EDU3,
and PRV2. To identify active ﬂows, we use a long inactivity time-
out of 60 seconds (similar to that used in previous measurements
studies [19]).
In Figure 3(a), we present the distribution of the number of active
ﬂows within a one second bin, as seen at seven different switches
within 4 data centers. We ﬁnd that although the distribution varies
across the data centers, the number of active ﬂows at any given
interval is less than 10,000. Based on the distributions, we group
the 7 monitored switches into two classes.
In the ﬁrst class are
all of the university data center switches EDU1, EDU2 and EDU3,
and one of the switches from a private enterprise, namely PRV24,
where the number of active ﬂows is between 10 and 500 in 90% of
the time intervals. In the second class, are the remaining switches
from the enterprise, namely, PRV21, PRV22, and PRV23, where
the number of active ﬂows is between 1,000 and 5,000 about 90%
of the time.
We examine the ﬂow inter-arrival times in Figure 3(b). We ﬁnd
that the time between new ﬂows arriving at the monitored switch is
less than 10µs for 2-13% of the ﬂows. For most of the switches in
PRV2, 80% of the ﬂows have an inter-arrival time under 1ms. This
observation supports the results of a prior study [19] of a cloud data
center. However, we found that this observation does not hold for
the university data centers, where we see 80% of the ﬂow inter-
arrival times were between 4ms and 40ms, suggesting that these
data centers have less churn than PRV2 and the previously stud-