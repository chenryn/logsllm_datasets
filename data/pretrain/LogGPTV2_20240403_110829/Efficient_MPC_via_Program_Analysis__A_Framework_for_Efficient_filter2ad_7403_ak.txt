in block B2, and let d, u ∈ B1, and u(cid:48) ∈ B2 appear after B1. Then only the std of the last iteration of B1 is
needed in constraints xstd ≥ astu(cid:48) − astd ; intuitively the deﬁnition in the last iteration “kills” all previous
deﬁnitions, and is outwardly exposed to u(cid:48) ∈ B2.
Lemma 5. Let (d, u) be a forward def-use chain, and (d, ub) be a backward one. (d, u) (cid:43) (d, ub) ⇒ γ((d, u))∩
γ((d, ub)) = ∅.
Proof. Again, consider block B1 immediately enclosed in block B2, and let d, ub ∈ B1, and u ∈ B2. Since
(d, u) does not subsume (d, ub), u must appear in B2, after B1. The std’s that are needed in backward def-
use constraints xstd ≥ astub − astd ; are all but the std’s in the last iteration of B1. (Since the last iteration
cannot be used in ub ∈ B1.) In contrast, only the std of the last iteration of B1 is needed in constraints
xstd ≥ astu − astd and we have that γ((d, u)) and γ((d, ub)) are disjoint.
34
e · we.
e xd
e · we is minimal. We show that for the same ﬁxed d,
Proof. Let PA induce x such that for a ﬁxed d,(cid:80)
(cid:80) xstd ≥(cid:80)
e · we and then ﬁnd values xstd that satisfy all constraints and(cid:80) xstd =(cid:80)
eb are 0, or the sum will not be minimal. Therefore,(cid:80)
Recall categories (I)-(IV) above. We consider 3 cases.
Case (1) is when xd
e(cid:48) = 1 and all xd
e(cid:48)(cid:48) , and xd
e xd
e xd
Case (2) arises when xd
e(cid:48) is the “highest” def-use chain that requires conversion: au(cid:48)−ad = 1 (i.e., au−ad ≥ 0).
e ·we = we(cid:48) since
e, xd
Thus, xd
all other terms in the sum are 0. Since au(cid:48) − ad = 1 we need all xstd in constraints xstd ≥ au(cid:48) − ad (category
Lemma 4, γ((d, u)) ⊇ γ((d, u(cid:48)) ⊇ γ((d, u(cid:48)(cid:48)) ⊇ γ((d, ub)), and therefore category (III) and (IV) constraints are
(II)) to be set to 1. By Lemma 3, there are exactly we(cid:48) such constraints, and therefore, (cid:80) xstd ≥ we(cid:48). By
satisﬁed. We may set all γ((d, u)) − γ((d, u(cid:48)) to 0, achieving(cid:80) xstd ≥ we(cid:48).
and by Lemma 3,(cid:80) xstd = we(cid:48)(cid:48).
(cid:80)
eb to 1. Therefore,(cid:80)
e(cid:48)(cid:48) is the highest def-use chain that requires conversion, and the backward chain
requires conversion as well, i.e., aub − ad = 1. Then one can easily see that the assignment that minimizes
e · we = we(cid:48)(cid:48) + web . There are exactly
by Lemma 5, γ((d, u(cid:48)(cid:48))) ∩ γ((d, ub) = ∅. Therefore,(cid:80) xstd = we(cid:48)(cid:48) + web .
we(cid:48)(cid:48) constraints xstd ≥ au(cid:48)(cid:48) − ad (category (III)) and web constraints xstd ≥ aub − ad (category (IV)), and
e(cid:48)(cid:48) , which does not subsume the backward chain, is the highest def-use chain that
eb are 0,
requires conversion, however, the backward chain aub − ad ≤ 0. Then we have that all xd
Case (3) arises when xd
e · we is xd
e(cid:48) to 0, and xd
e, xd
e(cid:48), and xd
e xd
e xd
e and xd
e(cid:48)(cid:48) and xd
e xd
Although we consider only four categories, the system and proof can be trivially generalized to an arbitrary
number of categories.
D.3 Optimal Assignment for CMPC(S) is also optimal for Linear(S)
Assumptions
Uniform linearization of loops We assume that a block B in loop L—with upper bound N in CMPC(S)—maps
to a set of blocks Bi, 1 ≤ i ≤ N in Linear(S). These Bi are all identical to B (modulo pseudo-φ nodes and
variable names). Note that this assumption is natural, and not requiring it would mean proving optimality
of CMPC(S) for arbitrary Linear(S) (instead of the Linear(S) that is linearization of CMPC(S)).
Client-Server model restriction The parties performing the MPC are servers and inputs are given by the
clients (a party may have both roles). We assume that inputs are received only once from the clients, by
having them share there input(s) to the servers in a single type of sharing that the protocol speciﬁes (each
input is shared once). Then the servers compute the circuit on these shared values but cannot ask for more
help from the clients. At the end the servers reconstruct the shared output and send to the clients. Note
that this restriction only applies to input values from the clients, it does not include, for example, public
constants.
Proof In the following we prove that, in non-amortized model, the constraints for IP CMPC(S) are the same
as those for IP Linear(S). Therefore optimal assignment for CMPC(S) is also optimal for Linear(S). This is done
on case by case analysis as under:
Case 1: Linear Code If CMPC(S) has no loops, then it is identical to Linear(S) which implies that constraints
for IP CMPC(S) are identical to constraints for IP Linear(S). Therefore, solution of IP CMPC(S) should be same as
IP Linear(S).
Case 2: Loop without pseudo-φ nodes Consider a CMPC(S) program that is a single loop L with N iterations.
There are no pseudo-φ nodes in L. Let B denote the loop body block. In Linear(S), this program will result
in a sequence of N blocks B1, B2, . . . , BN where each Bi, 1 ≤ i ≤ N is identical to B (modulo the variable
names). Optimal assignment of each Bi will be exactly the same. This is because if some Bj, j (cid:54)= i has better
assignment than Bi, then that same assignment would beneﬁt Bi as well. The total cost of B1, B2, . . . , BN
is therefore the cost of any Bi times N . Each statement sBi in Bi is essentially executed N times.
35
From case 1 above, we know that assignment for B and Bi will be the same (both are linear code).
Furthermore, in CMPC(S), cost for each statement sB in B is multiplied by B’s weight (i.e. N here), Therefore
IP CMPC(S) will produce the same assignment and cost as IP Linear(S) would.
Case 3: Loop with pseudo-φ nodes Now consider a CMPC(S) program with a block Bpred, followed by an
N -iteration loop L with loop body B, followed by a block Bsucc. Without loss of generality, say L has a
single pseudo-φ node i.e. there is a single deﬁnition d that B uses from previous iteration of the loop (i.e.
N − 1 iterations use d). The ﬁrst iteration of the loop will use a deﬁnition d(cid:48) from Bpred.
pear, and each Bi, 1  z is a GT type.
Node Weights are computed exactly as described in §3.5.
37
Figure 10. Conversion Point (min-cut): The conversion point (min-cut) for d (in L3) and u (in L4) is in their closest
enclosing block L1. w is one node where we can place conversion.
Conversion Points Conversions are needed if def-use (d, u) nodes are assigned diﬀerent sharings. This entails
computing min-cute on def-use chain as described in §3.6. In the implementation, this needs ﬁnding a node
on the min-cute edge to use as location marker for conversion node.
We ﬁnd optimal conversion point (min-cut) as follows. First, we construct a tree describing loop nesting.
Then we ﬁnd common ancestor of (d, u), say, L(cid:48), which is the closest enclosing block as described in §3.
Finally, for our conversion point, we ﬁnd the closest edge e with target w to d in L(cid:48). Since this is a straight
line program, we know that all paths from d to u pass through e and w. Fig. 10 illustrates this discussion
visually.
At this point we have described def use chains, node types and weights, and conversion points, which is
suﬃcient for protocol assignment in the sequential execution setting (we have established optimality in this
setting). To make our analysis richer, we go one step further and compute parallelizability of nodes (i.e., a
natural schedule as described in §6.2). This enables optimal protocol assignment in the parallel execution
setting.
Node Parallelizability We use the following rule to determine if a loop L is parallelizable, essentially com-
puting a schedule as described in §6.2. We compute def-use set S of all def-uses (d, u) that are immediately
enclosed in L i.e. there exists no loop L(cid:48) s.t. L(cid:48) encloses S and L(cid:48) is enclosed by L. Then we remove the def-use
(d, u) chains corresponding to loop counter variables. Finally, for each def-use (d, u) chain in S, if transitive
closure of any of d’s uses contains d itself (i.e. d is used in the deﬁnition of itself in subsequent iteration),
then L is not parallelizable. Fig. 11 illustrates an example with both a parallelizable and a non-parallelizable
loop.
We exclude loop counter variables’ def-use (d, u) chains from the above analysis. This is because such
variables always depend on previous iterations of L and, therefore, transitive closure of such a d will always
contain d. Thereby marking all loops (even the ones that are parallelizable), non-parallelizable.
If the above analysis yields that L is parallelizable, we mark all def-uses (d, u) in S as parallelizable
assigning weights as described in §6.2.
Calculate Subsumption To compute subsumption (§5.1), we start at def d and create an empty ordered list.
We now start processing d’s successors with this list. If we ﬁnd a use u, it is added to this list. Whenever
control splits, we keep processing the fall-through successors as above. For branched successors we create a
new list and recursively add any uses u(cid:48) in the branch to this new list. At the end of it, we have collected
one or more lists in which ordering indicates subsumption i.e. index(u) ≤ index(u(cid:48)) =⇒ (d, u) ⊇ (d, u(cid:48)).
38
duwL2L1L3L41 for (int i=0; i<100; i++) {
int sum = 0;
for (int j=0; j<4; j++) {
int diff = S[i][j] - C[j];
int square = diff*diff;
sum = sum + square;
}
D[i] = sum;
2
3
4
5
6
7
8
9 }
Figure 11. Checking Loop Parallelizablity: The outer loop is parallelizable but the inner is not (uses of sum include
its deﬁnition).
39