Figure 2: Pre-processing success and data density by attribute.
ical records from the US government’s Healthcare Cost and Utilization Project (HCUP) Nationwide
Inpatient Sample (NIS). The attributes we chose to extract have domain sizes that range from N = 4
to N = 366 and they are all attributes on which range queries are meaningful. For more information
about these datasets and how we extracted attributes, see Appendix B. Each of the three years includes
patient discharge records from about 1000 hospitals, giving us 3000 datasets for most attributes. (Some
were not available in all years.)
We say the attack succeeds if there is a single solution output by Algorithm 2, and it is the set of
elementary volumes (up to reﬂection). For dense datasets (where every value appears at least once and
no range query has volume 0), this means that all element counts have been recovered exactly, up to
reﬂection. For sparse datasets, this means that all non-zero element counts have been recovered in order
(up to reﬂection), but it is not known which elements did not appear in the database – the attacker
must make a decision about which values were not observed. In our evaluation of step 2, we discuss
and evaluate one such strategy, which uses a small amount of auxiliary information, for assigning the
recovered counts to a subset of elements in the domain.
Step 1 evaluation. The ﬁrst step of the attack is to observe enough queries to see all possible range
query volumes. The number of queries that this entails depends on the query distribution, as discussed
in Section 3.2. For instance, if the query distribution is uniform, then the expected number of queries
is O(N 2 log N ).
Step 2 evaluation. For each dataset-attribute combination, we ran Algorithm 1 to obtain sets of
necessary elementary volumes, Vnec, and candidate elementary volumes, Vcand. The plot in Figure 2
14
AGEAGEDAYAMONTHLOSMRISKNCHRONIC-16NCHRONIC-26NDX-16NDX-26NPR-16NPR-26SEVZIPINC02004006008001000Numberofhospitalsdensedatauniquesolutionclique-ﬁndingnecessarynon-densedatauniquesolutionclique-ﬁndingnecessaryPre-processingresultsbyattributeanddatadensityFigure 3: Extending pre-processing success for sparse datasets. The fraction of correct values is out of the actual
number of values for each dataset. Whiskers indicate the 5th and 95th percentiles, while boxes indicate the 25th
and 75th percentiles, with a line at the median and a diamond at the mean.
shows, for each attribute, the average1 number of datasets for which pre-processing was suﬃcient for
the attack to succeed. For all attributes except AGE and AGEDAY, pre-processing correctly identiﬁed
the non-zero element counts in order (up to reﬂection) for the vast majority of datasets. The diﬀerence
in patterns on the bars indicates which datasets were dense. Attributes with smaller domain sizes, e.g.,
AMONTH, MRISK, SEV, and ZIPINC, were dense most of the time. The attributes with the largest
domain, LOS and AGEDAY, were dense in fewer than 0.01% of datasets.
Pre-processing recovered the set of elementary volumes for at least 90% of all dense datasets for
each attribute except AGEDAY. This attribute had a single dense dataset in each 2004 and 2008 that
required clique-ﬁnding.
For sparse datasets, recovering the set of all non-zero counts provides a lot of information. Combining
it with some rudimentary information about the database distribution can lead to recovering all element
counts, just like in the dense case. For instance, one might guess that the length of stay (LOS), the
number of chronic conditions (NCHRONIC), and the number of procedures (NPR), might be 0 most
frequently, then decrease. To illustrate just how valuable knowing the set of elementary volumes could
be when combined with a tiny bit of knowledge about the domain, we evaluate the following strategy for
assigning counts to elements: simply guess that they correspond to the ﬁrst values in the domain. The
results are displayed in Figure 3. We juxtapose the success of our simple strategy for LOS, NCHRONIC,
and NPR with its mediocre results for the number of diagnoses, NDX, which is 1 more frequently than
0, and thus our strategy is not suitable for it.
Step 3 evaluation. Lastly, we ran Algorithm 2 (with some modiﬁcations) on the few dataset-
attribute combinations for which pre-processing did not ﬁnd a unique solution. First, we modiﬁed
Get Elem Volumes to return all solutions, not just minimal ones, by replacing Min Subcliques in
1For attributes that were available in more than one year (as noted in Figure 7 in Appendix B), results were very
similar, so we averaged the counts.
15
LOSNCHRONIC-16NCHRONIC-26NDX-16NDX-26NPR-16NPR-260.00.20.40.60.81.0FractionofvaluescorrectlyguessedPre-processingresultsforsparsedatasets:assigningcountstovalueswithasimplestrategyFigure 4: Overall results of the practical reconstruction attack.
Alg. 2 with All Subcliques P (described starting on line 15 in Alg. 3). However, for the sake of a
more practical attack, we allowed Get Elem Volumes to return an incomplete list of solutions, or to
fail entirely.
Our probabilistic variant of Find Maximal Cliques is described starting on line 1 of Algorithm 3.
Speciﬁcally, line 14 of Algorithm 2 is replaced with Find Maximal Cliques P(CANDnn, Mmin, V ).
For graphs with 20 or fewer nodes, we used the find cliques routine from the NetworkX Python
module (line 4) [HSS08]. For graphs with more nodes, we sampled maximal cliques one at a time, 1000
times, (line 7) using Luby’s eﬃcient parallel algorithm for maximal independent sets, implemented as
the max independent vertex set routine from the graph-tool Python module [Pei14].
The three points at which our variant may fail are (i) Find Maximal Cliques P fails to ﬁnd any
maximal cliques of size at least Mmin (line 11), (ii) we found such cliques, but none of them generated
the set of missing volumes (line 13), or (iii) there were such cliques that generated the set of missing
volumes, but for all of them, it was impractical (line 17) to ﬁnd all of their subclique solutions.
Figure 4 shows the overall attack results. Success, in green, occurs when pre-processing or clique-
ﬁnding ﬁnds the solution and it is unique – there is a single clique whose size is in the right range
that generates all observed volumes. Multiple cliques, in blue, arise when clique-ﬁnding has found all
such solutions, but there is more than one, so that the correct solution cannot be precisely determined.
Failure, in red, arises either when Algorithm 2 returns FAILURE or {} or when we sampled maximal
cliques using Luby’s algorithm (line 7) and may not have found all of them.
In our experiments, the most common reason for failure overall was (iii): it was impractical to ﬁnd
all subcliques (about 60% of failures or incomplete cases). The second most common overall reason for
failure was (ii), not ﬁnding any cliques that generated all missing volumes (about 36%). However, as
16
AGEAGEDAYAMONTHLOSMRISKNCHRONIC-16NCHRONIC-26NDX-16NDX-26NPR-16NPR-26SEVZIPINC02004006008001000Numberofhospitalsdensedatauniquesolutionmultiplesolutionsunknown/incompletenon-densedatauniquesolutionmultiplesolutionsunknown/incompleteOverallexperimentalresultsbyattributeanddatadensityone might expect because of the bound on line 17 in All Subcliques P, the attributes with fewer
possible values (e.g., AGE with N = 91 compared to AGEDAY with N = 365) failed more often due
to no cliques generating all volumes as opposed to too-big cliques.
Conclusions. Overall, our experiments indicate that our clique-ﬁnding approach yields overwhelming
success in reconstructing counts of dense datasets – and that in most cases, no expensive clique-ﬁnding
is even required (see the white bars corresponding to dense data in Fig. 2). For sparse data, the success
of this approach mainly depends on what auxiliary information is available to the attacker. We showed
how an attacker can leverage rudimentary knowledge of a distribution (e.g., that the most frequent
values are the smallest) to correctly assign exact counts to values (see Fig. 3).
4 Update Recovery Attack
In this section, we consider an attack in the following setting. We assume that the adversary knows the
database counts, via either the reconstruction attack from the previous section or a one-time compromise
of the database. Now suppose that a new record is added into the database, and that the attacker learns
this. The attacker could detect such an update for example because an update query may have a diﬀerent
volume than a range query; the attacker could also infer indirectly that an update has occured because
he observes volumes that were not possible for the original database counts. In this context, we propose
an attack to recover the value of the newly added record using only the volume leakage of range queries
issued after the update.
Note that in order to fully recover the value of the new record, the attack assumes that enough
range queries are issued by the client before any further update is made. Thus the attack as it stands
will fail if updates are made in close succession. We leave the treatment of frequent or simultaneous
updates for future work.
On the other hand, if there are enough range queries for our attack to fully recover the value of a
new record after it is added, and before the next update, then database counts are fully known before
the next update.
It follows that the attack can be repeated for the next update. Thus if database
updates are rare relative to range queries, then the attack allows an adversary to update its view of
database counts on the ﬂy as updates are made.
As a ﬁrst idea to recover the value of the new record, one could re-run the database reconstruction
attack and compare the original and new counts. This has unnecessarily high query complexity—our
attack in this section is orders of magnitude more eﬃcient analytically and experimentally.
Like our main attack, our update recovery algorithm does not require a uniform query distribution.
If we do make that assumption for the purpose of analysis, then the algorithm is amenable to analysis
in the same model as our main attack. Recall that in that model, our main attack required O(N 2 log N )
queries for full reconstruction. In the same model, our update recovery algorithm only requires O(N )
queries to recover the value of the new record exactly. Furthermore the same algorithm is able to
approximate the value of the new record quite quickly: our model predicts that the value of the new
record can be approximated within an additive error N , for any  > 0, after observing O(1/) queries;
once again the observed behavior in our experiments on real-world data matches this prediction.
4.1 Update Recovery Algorithm
The idea of the attack is as follows. First, because the adversary knows all database counts for the
original database, it knows the volume of every range query on that database. Now suppose that a new
record is added, and the adversary then observes the volume of some range query.
17
if RangeFromVol(v) is undeﬁned then
else
RangeFromVol ← empty map
for x ∈ [1, N ] do
for y ∈ [x, N ] do
v ←(cid:80)
x≤k≤y C(k)
Algorithm 4 Update recovery attack.
1: procedure Update Recovery(V ,C,N )
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
RangeFromVol(v) ← [x, y]
RangeFromVol(v) ← ⊥
Possible ← [1, N]
for v ∈ V do
Possible = Possible ∩ [x, y]
Possible = Possible \ [x, y]
return (min(Possible) + max(Possible))/2
if RangeFromVol(v − 1) = [x, y] and RangeFromVol(v) is undeﬁned then
if RangeFromVol(v) = [x, y] and RangeFromVol(v − 1) is undeﬁned then
(cid:46) Set of possible values
(cid:46) Iterate through observed volumes
Assume that the volume of that query is not equal to the volume of any range for the original
database (i.e. before the record was added). Then it must be the case that the queried range has
matched the new record. The adversary can detect this, since it knows the volume of every query for
the original database, as noted earlier. Since the query has matched the new record, its volume must
be one more than the volume of some range in the original database.
If that range is unique, once
again the adversary knows this, and can deduce the queried range [x, y]. In that case the adversary can
immediately deduce that the value of the new record must lie within [x, y]. A similar reasoning holds
in the case that the observed range does not contain the new record.
As the volumes of more range queries are observed, the adversary reﬁnes its knowledge of the new
record’s value. We note that the previous reasoning required an assumption about a certain volume
corresponding to a unique queried range in the original database. As both experiments and analysis
will show however, this event occurs often enough that the algorithm is able to quickly home in on the
value of the new record.
Pseudo-code is provided in Algorithm 4. The input of the algorithm is a set of volumes V , which
should be the number of matching records (i.e. volume) of a set of observed queries; the original
database counts C, where C(k) is the number of records with value k; and the number of values N . The
algorithm creates a table “RangeFromVol” mapping volumes to ranges for the original counts C, then
proceeds to reﬁne the set “Possible” of possible values for the new record as new volumes are observed,
according to the algorithm explained in the previous paragraph. The algorithm ﬁnally outputs a guess
for the value of the new record, which is simply the average of the minimum and maximum of the set of
possible values obtained up to that point (this choice minimizes worst-case error). We preferred clarity
in the pseudocode; many reﬁnements are possible. Our experiments show that this simple algorithm is
already quite eﬀective.
Our analysis of the attack’s query complexity is given in Appendix D. Within the same analytical
model as in Section 3.2, we show that assuming R = Ω(N 2), the expected number of queries for exact
√
recovery is O(N ), and the expected number of queries to recover the value of the new record within N
is O(1/), where the constants depend on N/
R.
18
Precision
R 20% 10% 5% 2% Exact
5000
10000
20000
47
18
11
79
29
17
123
229
46
27
99
57
974
391
191
Figure 5: Median number of queries needed to achieve the given precision in the output of Algorithm 4, for three
hospitals with size within 10% of the given R.
4.2 Experiments
We have run Algorithm 4 on the age data of patients in three hospitals of sizes within 10% of R = 5000,
10000 and 20000, extracted from the same HCUP data as in Section 3.3. The age data is capped at 90
in our dataset for privacy reasons, and so the number of values is N = 91. Thus the choices of sizes
5000, 10000 and 20000 reﬂect the cases where R is respectively close to N 2/2, N 2 and 2N 2. Update
recovery should work well in parameter regimes when the main reconstruction attack works well: around
R ≥ N 2/2. Eﬀectiveness should degrade gracefully below that value.
If R is close to N 2/2 or below, it may happen that most or all ranges that would allow to uniquely
identify the value of the new record collide with other volumes; in that case exact recovery could be very
expensive or impossible. If recovery is impossible, the average number of queries required for recovery
is technically inﬁnite. Obviously, an inﬁnite average does not reﬂect the fact that in practice, recovery
should usually succeed with a low number of queries. For this reason we use medians instead of averages.
Results are given in Figure 5. As predicted by our model, the number of queries needed for exact
recovery is of the order of magnitude of a reasonably small multiple of N , although the constant degrades
when R approaches N 2/2. The number of queries necessary to achieve a precision  does appear to
behave as O(1/). Furthermore, the value of the new record can be approximated reasonably well within
relatively few queries, especially for larger R: for R = 20000 we see that observing the volume of 27
queries suﬃces to recover the value of the new record within an error of 5%, i.e. within 5 years.
5 Range Query Reconstruction via CDF Matching
In Section 3, we described an attack that achieves database reconstruction from the volumes of unknown
queries. Once the database has been reconstructed, the queries themselves can also be reconstructed
from their volumes by matching an observed volume to the set of possible queries that have that volume.
Thus, one generic approach to query reconstruction is to observe enough volumes to reconstruct the
database using the previous attack, then simply match queries to volumes using the reconstructed
database.
In Appendix E we conﬁrm experimentally that this attack is very eﬀective. A drawback
of this approach is that many queries must be observed before any information is learned about any
queries. If an attacker wants to learn as much as it can from a set of queries of any size, a diﬀerent
approach is needed.
In this section, we describe an attack that achieves query reconstruction “online”, meaning the ad-
versary can infer a set of likely underlying values for the query as soon as it observes its corresponding
volume. We call this attack “CDF matching”. CDF matching uses an estimate of the database distri-
bution (below, an “auxiliary distribution”) to infer the underlying (hidden) query as soon as its volume
is observed. Various works [NKW15, GSB+17] have argued that attacks with auxiliary distributions are
realistic; such distributions can come from census data [BGC+18], public employee records [GSB+17],
or even copies of similar databases posted online by hackers.
19
First, we will describe the attack and analyze it in the setting where the adversary has full knowledge
of the database distribution. Then, we will demonstrate empirically that (1) the attack reveals a
substantial amount of information about queries, and (2) our analysis retains much of its predictive
power even when the adversary has a poor auxiliary distribution.
Preliminaries. Before describing our attack we will state and discuss two useful technical tools. The
Kolmogorov-Smirnov (KS) distance between CDFs F and G is KS(F, G) = supx |F (x)−G(x)|. Let
Ib is 1 if b = 1 and 0 otherwise. The Dvoretzky-Kiefer-Wolfowitz (DKW) inequality [DKW56] is
a Chernoﬀ-type bound on the maximum distance between the empirical and true CDF of a distribution.
Theorem 1 (Dvoretzky-Kiefer-Wolfowitz). Let πDB be a distribution on [1, N ], and X1, . . . , XR be R
i.i.d. samples from πDB. Let ˆF (i) = 1
R
(cid:20)
(cid:80)R
j=1 IXj≤i and F (i) =(cid:80)i
(cid:12)(cid:12)(cid:12) ˆF (i) − F (i)
(cid:12)(cid:12)(cid:12) > 
≤ 2e
(cid:21)
Pr
sup
i
j=1 pi for i ∈ [1, N ]. Then
−2R2