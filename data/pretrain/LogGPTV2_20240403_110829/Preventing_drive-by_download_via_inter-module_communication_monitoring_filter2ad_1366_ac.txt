constructed as follows.
• The ﬁrst line of a deﬁnition ﬁle is the CLSID of the
vulnerable component, and the rest content is divided
into one or more blocks, every block is separated by a
new line.
• Each block describes one part of the constraint.
• The ﬁrst line of the block is the original state number.
The initial state uses number 1, and the attack state
uses number 0. The rest numbers (positive) are free
to use.
• The second line is the target state number. The num-
bering rule is the same as above.
• The third line is the method name, which according to
ActiveX standard, is case insensitive.
• The rest of the block is the expressions of symbolic
constraint associated with the original state. Since the
resolver used in our prototype system is Yices [12],
therefore these expressions are written in Yices Input
Language.
At runtime, MwDetector uses a signature manager to
manage the signatures in its vulnerability deﬁnitions.
Session manager
4.3.2
In MwDetector, the three kinds of IMC events are han-
dled by session manager. When an object creation event
is received, the session manager creates a new session ob-
ject. Then it uses the object’s CLSID to query the signa-
ture manager for matchers (discussed below) that belong to
this CLSID, and associates these matchers with the created
session. After that, this session object is put into the living
session list.
When a method invocation event is arrived, the session
manager uses the object address to ﬁnd the corresponding
session in the session list. Once found, the session manager
feeds all the associated matchers with the method name and
parameters passed in. If any matcher enters the EXPLOIT
state, the session manager raises an attack alert. The alert
information is then returned to COMSniﬀer for further pro-
cessing.
To handle the object (COM object) free event, the session
manager ﬁrst ﬁnds the session object the same way as above,
and removes the session object from the live session list.
Then, the session manager frees all the associated matchers
and the session object itself.
Figure 2: Structure of DISPPARAMS
4.3.3 Matcher
In MwDetector, a matcher is an instance of a signature.
When MwDetector is loaded, the signature manager will
load all the signatures by parsing the deﬁnition ﬁles. When
the session manager queries for the matchers, the signa-
ture manager instantiates signatures registered for handling
events of this CLSID and returns the matchers to the session
manager.
There could be several kinds of matcher for diﬀerent kinds
of input data. But every matcher class has to implement a
transition method. This method is called by the session
manager on the receiving of method invocation events. For
COM matchers, the transition method works as follows.
Firstly, the parameters are parsed into symbolic expres-
sions. The parameters used to invoke a method through
the IDispatch interface are stored in a DISPPARAMS struc-
ture (Figure 2). The cArgs member stores the number of
unnamed arguments, while the cNamedArgs member stores
the number of named ones. The information of each ar-
gument is store in rgvarg and rgdispidNamedArgs respec-
tively. Since most scripting languages are weakly-typed, to
support interaction with these languages, each unnamed ar-
guments is stored in a VARIANTARG structure. The ﬁrst mem-
ber of this structure indicates the type of the argument.
Commonly used types include strings (VT_BSTR), integers
(VT_Ik, VT_UIk, k = 1, 2, 4, 8), objects (VT_DISPATCH) and
variables 5 (VT_VARIANT). The parser will generate expres-
sion according to the argument’s type. For example, a string
parameter will be expressed as:
(define arg#_index#::int)
(assert (= arg#_index# value))
Then the matcher locates the transition constraint accord-
ing to current state and the name of the invoked method.
Then the matcher calls Yices to check whether the input
is consistent with the constraint.
If it is, the state of the
session transits to the target state. Else, the state remains
in current state. Since there could be several matchers for
one session, the session state is stored inside each matcher
to avoid interference.
5. EVALUATION
In this section, we discuss how our prototype is evaluated
and the experimental results. We ﬁrst introduce the test
environment we used to carry out the rest evaluations. Then
we describe the three experiments. The ﬁrst one evaluates
the detection eﬀectiveness, i.e.
false negative rates. The
second one evaluates the false positive rate. And the last
one measures the performance overhead.
5.1 Drive-by download attack replaying sys-
tem
5The type of this argument is unsure.
Drive-by download attack has been a hot topic for several
years, however, up till now, a standard test base for measur-
ing the eﬀectiveness still has not emerged. In this section,
we give a brief introduction of our test environment which
is able to reliably replay the drive-by download attacks.
After drive-by download has become a major threat to the
Internet users, security research groups and companies have
built several system to detect these attacks. Some of them
will publish a list of recently detected malicious pages. These
pages should be used to evaluate new mitigation approaches.
Unfortunately, most of these malicious pages only live for a
short time before they are cleaned up. After that, they are
no longer capable to be used for evaluations.
To overcome this limitation, most researchers would store
a local copy of the malicious page. But a drive-by download
attack usually involve tens of ﬁles that distributed over sev-
eral web servers, by using ,  and 
tags. If only one ﬁle is visited during the evaluation, the ex-
ploit may not be triggered correctly [13]. One would suggest
modifying the src attributes of those tags. For static tags,
this would work. But many malicious pages will dynamically
create those tags, and the pages themselves are heavily ob-
fuscated or even encrypted using the correct URL [13], hence
makes this approach infeasible.
To solve this problem, we build a proxy based replaying
system. Once we ﬁnd a malicious page, we cache in local
every web content (HTML documents, scripts, images, bi-
nary ﬁles, et al.) involved in the attack. Thereafter, when
we want to revisit those pages to evaluate a system, we can
simply set the system to use that proxy, and then directly
visit the original URL, the replaying system will response
with all cached web content in the attack scene, therefore
restoring the original drive-by download attack scene.
5.2 ActiveX emulator
Most drive-by download attacks will try to exploit multi-
ple vulnerabilities to improve the success exploitation ratio.
Hence, to build a good detection system, it is necessary to
install as much vulnerable ActiveX controls as possible. Un-
fortunately, not all controls are compatible. For example,
during the development of our system, we found when Mi-
crosoft Access Snapshot Viewer (CVE-2008-2463) is in use,
other vulnerabilities cannot be exploited within the same
process. To solve this problem, in our detection environ-
ment, we create an universal ActiveX control that can be
instantiated as any ActiveX component. As a result, when-
ever a malicious page attempts to utilize an ActiveX control
that does not exist in our environment and invoke its vul-
nerable method, this attempt will succeed and the attack
will be detected by MwDetector. To trigger attacks that
rely on results of method invocations (e.g. the version of
RealPlayer), our ActiveX emulator is able to return this de-
manded information (not a dummy component).
Since most drive-by download attacks rely on ActiveX
controls, diﬀerent kind of emulators have also been imple-
mented in previous works [24, 20]. However, most of these
emulators are implemented in scripting language, the same
level as the malicious scripts. Therefore, they are not com-
pletely invisible to drive-by download exploits. On the con-
trary, the simulation in our system is at COM level, a level
below the script engine. This means, it is not easily de-
tectable for malicious scripts.
typedef struct tagDISPPARAMS    {    VARIANTARG *rgvarg;    DISPID *rgdispidNamedArgs;    UINT cArgs;    UINT cNamedArgs;     } DISPPARAMS;5.3 Detection effectiveness
To measure the detection eﬀectiveness of our prototype,
we ﬁrst evaluated it on 119 in-the-wild drive-by download
attack samples (1010 html and script ﬁles) that were cached
by our replay system. All samples were detected by our own
high-interaction honeypot. These samples are visited in a
batch mode, each given 2 minutes to process.
The tested system is integrated into IE6 browser on a
clean installed Windows XP SP2 with no more patches. In
addition, Adobe Flash Player 9.0.47.0 is installed. We use
this conﬁguration because it is a typical vulnerable environ-
ment for web browsing. However, in theory our prototype
can also be integrated with other MSIE versions on diﬀerent
Windows versions.
For this evaluation, we manually generated 37 signatures
from 19 vulnerability reports (listed in Appendix A). Since
all these signatures are extracted from vulnerable compo-
nents, there is no training samples used in this evaluation.
The result is, with the help of the ActiveX emulator, our
detection system successfully detected 895 exploit instances
(one sample may contain several attacks targeting the same
vulnerability) from 99 samples (Figure 3). This yields an
initial detection eﬀectiveness of 83%, i.e. a false negative
ratio of 17%. To understand why our system did not report
any attack on the remaining 20 pages, we reanalyzed them
with our high interaction honeypot. The reexamination re-
vealed that none these samples are active. This is due to
cache problem of our replay system, more precisely, not all
ﬁles involved in these samples are successfully cached. Af-
ter excluding these inactive samples from our dataset, we
compute a detection rate of 100%.
Besides the detection eﬀectiveness, this experiment also
showed two interesting results. The ﬁrst one is, by using
the emulator, in addition to the vulnerabilities that do exist
in test environment (CVE-2006-0003), our system also de-
tected attacks tried to exploit other 14 vulnerabilities. The
other one is, though vulnerabilities which can be exploited
without shellcode (CVE-2006-0003, CVE-2008-2463, CVE-
2007-4105, CVE-2008-6442) is relatively rare, the number of
attacks that target them is much larger than those aiming at
exploiting vulnerabilities of other kinds. This means, exist-
ing memory protection mechanisms like data execution pre-
vention (DEP), address space layout randomization (ASLR)
and shellcode-based detection systems [13, 33] are not suﬃ-
cient to protect users from drive-by download attacks.
From the perspective of detecting malicious pages, the de-
tection rate of our system is perfect. However, since most
of the cached samples contain more than one exploit, this
result cannot fully represent the detection eﬀectiveness of
all detectable exploits. Therefore, we further chose 5 sam-
ples with disparate attack payloads 6 and manually analyzed
them to ﬁnd out all the exploits. Then we compared this
result with attacks detected by our system. The result is
shown in Table 1. The detection rate on this small dataset
decreased to 52%. The reason for this decrease is that those
undetected exploits are aiming at vulnerabilities that are
not included in our deﬁnitions yet. This is an unavoidable
weakness of all signature-based detection systems. But our
system detected all attacks targeting at vulnerabilities in
our deﬁnitions.
6Although the initial page of every sample is diﬀerent, the
exploit ﬁles involved could be similar or even the same.
Figure 3: Exploit instances detected on the 119
cached samples.
Table 1: Exploit instances detected on the manually
analyzed samples
Sample ID Exploits found Exploits detected
1
6
24
26
67
6
3
9
13
3
6
1
6
2
3
5.4 False positive evaluation
In the context of our system, a false positive is a page
that is detected as containing at least one exploit, but in
fact does not. To evaluate the likelihood of false positives,
we drive our prototype to visit the home page of the top 100
sites of Alexa global ranking and of the top 100 site within
China. All these pages are known to be benign. And we
consider them as a reasonable test set that represents the
most commonly visit content of Internet users.
On this dataset, our prototype did not produce any false
positive. This ‘perfect’ result is expected. There are two rea-
sons. The ﬁrst one is, the detection is based on vulnerability-
based signature, and most vulnerabilities we met are rela-
tively simple (e.g. stack overﬂow), hence the false positive
and false negative should be naturally low. And the second
one is, most of these commonly visited sites did not instanti-
ate any of the vulnerable ActiveX controls, or did not invoke
the vulnerable methods that exist in our vulnerability deﬁ-
nitions.
5.5 Performance
Though the detector can be fed by a log parser (i.e. works
oﬄine), our prototype is implemented in online mode. That
is, the detection is performed at the same time when the
MSIE handles the page. Thence, we evaluated the perfor-
mance penalty caused by our prototype in this section.
This experiment is carried out on the Alexa top 100 sites
within China. We ﬁrst uninstalled our system and drove IE6
to visit the dataset and recorded the time used (wall-clock
time). This is done by installing a BHO that record the
time elapsed between the BeforeNavigate2 event and the
DocumentComplete event from the root frame. After that,
47445050047430035040045050047425525030035040045050047425515020025030035040045050047425558201815131111655211050100150200250300350400450500474255582018151311116552110501001502002503003504004505004742555820181513111165521105010015020025030035040045050047425558201815131111655211050100150200250300350400450500Table 2: Page load times (sec) with and without our
detection system.
Total time
Average time
Average Overhead
Factor
Native MSIE With detection
410
4.10
0
1
471
4.71
0.61
1.15
we reinstalled our detection system on the same machine
(without the ActiveX emulator, works as a prevention sys-
tem) and ran the same test again. The local cache, history
information and are cookies cleared before every test.
The evaluated system was of the same conﬁguration as
previous experiments and was installed on a virtual ma-
chine with Intel Core 2 Duo 2.4G CPU (one CPU mode) and
256MB of main memory. An ADSL of bandwidth 2Mb/s is
used to visit the Internet.
The result is presented in Table 2. On average, native
MSIE uses 4.1 seconds to load a page. This time includes
downloading all the ﬁles within the principle, parsing and
rendering it and executing all dynamic content. After inte-
grated our system, the average load time increased to 4.71
seconds, i.e. an overhead of about 15%. Although most
Internet user would expect the browsing experience as fast
as possible, comparing with the protection provided by our
system, we believe this penalty is a fair tradeoﬀ.
6. DISCUSSION AND FUTURE WORK
Our prototype system has several limitations. First, it
only detected attacks using explicit malicious content. That
is, the malicious content is directly passed to the vulnerable
component in the arguments. However, some attacks will
use implicit malicious content. For example, some pages
may instantiate an Adobe Flash Player control and pass it a
URL leading a malicious ﬂash ﬁle (CVE-2007-0071). Other
examples include malicious PDF ﬁles (CVE-2007-5659), mal-
formatted picture ﬁles (CVE-2007-0038) and et al. To detect
such attacks, we can extend our system to (1) monitor the
uniform resource identiﬁer (URI) passed to the vulnerable
component; (2) intercept the downloading procedure; and
(3) once a monitored URI ﬁnished downloading, before the
content is passed to the component, check if this content
contains any exploit.
The second one is the limitation of all signature-based de-