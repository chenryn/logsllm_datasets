title:Comparing and improving current packet capturing solutions based on
commodity hardware
author:Lothar Braun and
Alexander Didebulidze and
Nils Kammenhuber and
Georg Carle
Comparing and Improving Current Packet Capturing
Solutions based on Commodity Hardware
Lothar Braun, Alexander Didebulidze, Nils Kammenhuber, Georg Carle
Chair for Network Architectures and Services
{braun,didebuli,kammenhuber,carle}@net.in.tum.de
Technische Universität München
Institute for Informatics
ABSTRACT
Capturing network traﬃc with commodity hardware has be-
come a feasible task: Advances in hardware as well as soft-
ware have boosted oﬀ-the-shelf hardware to performance lev-
els that some years ago were the domain of expensive special-
purpose hardware. However, the capturing hardware still
needs to be driven by a well-performing software stack in
order to minimise or avoid packet loss. Improving the cap-
turing stack of Linux and FreeBSD has been an extensively
covered research topic in the past years. Although the ma-
jority of the proposed enhancements have been backed by
evaluations, these have mostly been conducted on diﬀerent
hardware platforms and software versions, which renders a
comparative assessment of the various approaches diﬃcult,
if not impossible.
This paper summarises and evaluates the performance
of current packet capturing solutions based on commodity
hardware. We identify bottlenecks and pitfalls within the
capturing stack of FreeBSD and Linux, and give explana-
tions for the observed eﬀects. Based on our experiments, we
provide guidelines for users on how to conﬁgure their captur-
ing systems for optimal performance and we also give hints
on debugging bad performance. Furthermore, we propose
improvements to the operating system’s capturing processes
that reduce packet loss, and evaluate their impact on cap-
turing performance.
Categories and Subject Descriptors
C.2.3 [Network Operation]: Network Monitoring
General Terms
Measurement, Performance
1.
INTRODUCTION
Packet capture is an essential part of most network mon-
itoring and analysing systems. A few years ago, using spe-
cialised hardware—e.g., network monitoring cards manufac-
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’10, November 1–3, 2010, Melbourne, Australia.
Copyright 2010 ACM 978-1-4503-0057-5/10/11 ...$10.00.
tured by Endace [1]—was mandatory for capturing Gigabit
or Multi-gigabit network traﬃc, if little or no packet loss was
a requirement. With recent development progresses in bus
systems, multi-core CPUs and commodity network cards,
nowadays oﬀ-the-shelf hardware can be used to capture net-
work traﬃc at near wire-speed with little or no packet loss
in 1 GE networks, too [2, 3]. People are even building mon-
itoring devices based on commodity hardware that can be
used to capture traﬃc in 10 GE networks [4, 5]
However, this is not an easy task, since it requires careful
conﬁguration and optimization of the hardware and software
components involved—even the best hardware will suﬀer
packet loss if its driving software stack is not able to handle
the huge amount of network packets. Several subsystems
including the network card driver, the capturing stack of
the operating system and the monitoring application are in-
volved in packet processing. If only one of these subsystems
faces performance problems, packet loss will occur, and the
whole process of packet capturing will yield bad results.
Previous work analysed [2, 4, 6] and improved [5, 7, 8]
packet capturing solutions. Comparing these work is quite
diﬃcult because the evaluations have been performed on dif-
ferent hardware platforms and with diﬀerent software ver-
sions. In addition, operating systems like Linux and FreeBSD
are subject to constant changes and improvements. Compar-
isons that have been performed years ago therefore might
today not be valid any longer. In fact, when we started our
capturing experiments, we were not able to reproduce the re-
sults presented in several papers. When we dug deeper into
the operating systems’ capturing processes, we found that
some of our results can be explained by improved drivers
and general operating systems improvements. Other diﬀer-
ences can be explained by the type of traﬃc we analysed
and by the way our capturing software works on the appli-
cation layer. While it is not a problem to ﬁnd comparisons
that state the superiority of a speciﬁc capturing solution,
we had diﬃculties to ﬁnd statements on why one solution is
superior to another solution. Information about this topic is
scattered throughout diﬀerent papers and web pages. Worse
yet, some information proved to be highly inconsistent, espe-
cially when from Web sources outside academia. We there-
fore encountered many diﬃculties when debugging the per-
formance problems we ran into.
This paper tries to ﬁll the gap that we needed to step
over when we set up our packet capturing environment with
Linux and FreeBSD. We evaluate and compare diﬀerent cap-
turing solutions for both operating systems, and try to sum-
marise the pitfalls that can lead to bad capturing perfor-
206mance. The paper aims at future developers and users of
capture systems and serves as a resource helping them not
to repeat the pitfalls we and other researchers have encoun-
tered.
A special focus of this paper is on explaining our ﬁnd-
ings. We try to identify the major factors that inﬂuence
the performance of a capturing solution, providing users of
packet capture systems with guidelines on where to search
for performance problems in their systems. Our paper also
targets developers of capturing and monitoring solutions:
We identify potential bottlenecks that can lead to perfor-
mance bottlenecks and thus packet loss. Finally, we propose
a modiﬁcation that can be applied to popular capturing so-
lutions. It improves capturing performance in a number of
situations.
The remainder of this paper is organised as follows: Sec-
tion 2 introduces the capturing mechanisms in Linux and
FreeBSD that are in use when performing network moni-
toring, and presents related improvements and evaluations
of packet capturing systems. Section 3 presents the test
setup that we used for our evaluation in Section 4. Our cap-
turing analysis covers scheduling issues in Section 4.1 and
focuses on the application and operating system layer with
low application load in Section 4.2. Subsequently, we anal-
yse application scenarios that pose higher load on the system
in Section 4.3, where we furthermore present our modiﬁca-
tions to the capturing processes and evaluate their inﬂuence
on capturing performance. In Section 4.4, we move down-
wards within the capturing stack and discuss driver issues.
Our experiments result in recommendations for developers
and users of capturing solutions, which are presented in Sec-
tion 5. Finally, Section 6 concludes the paper with a sum-
mary of our ﬁndings.
2. BACKGROUND AND RELATED WORK
Various mechanisms are involved in the process of network
packet capturing. The performance of a capturing process
thus depends on each of them to some extent. On the one
hand, there is the hardware that needs to capture and copy
all packets from the network to memory before the analysis
can start. On the other hand, there is the driver, the oper-
ating system and monitoring applications that need to care-
fully handle the available hardware in order to achieve the
best possible packet capturing performance. In the follow-
ing, we will introduce popular capturing solutions on Linux
and FreeBSD in 2.1. Afterwards, we will summarise com-
parisons and evaluations that have been performed on the
diﬀerent solutions in Section 2.2.
2.1 Solutions on Linux and FreeBSD
Advances made in hardware development in recent years
such as high speed bus systems, multi-core systems or net-
work cards with multiple independent reception (RX) queues
oﬀer performance that has only been oﬀered by special pur-
pose hardware some years ago. Meanwhile, operating sys-
tems and hardware drivers have come to take advantage of
these new technologies, thus allowing higher capturing rates.
Hardware:
The importance of carefully selecting suitable capturing hard-
ware is well-known, as research showed that diﬀerent hard-
ware platforms can lead to diﬀerent capturing performance.
Figure 1: Subsystems involved in the capturing pro-
cess
Schneider et al. [4] compared capturing hardware based on
Intel Xeon and AMD Opteron CPUs with otherwise sim-
ilar components. Assessing an AMD and an Intel plat-
form of comparable computing power, they found the AMD
platform to yield better capturing results. AMD’s superior
memory management and bus contention handling mecha-
nism was identiﬁed to be the most reasonable explanation.
Since then, Intel has introduced Quick Path Interconnect [9]
in its recent processor families, which has improved the per-
formance of the Intel platform; however, we are not able to
compare new AMD and Intel platforms at this time due to
lack of hardware. In any case, users of packet capturing solu-
tions should carefully choose the CPU platform, and should
conduct performance tests before buying a particular hard-
ware platform.
Apart from the CPU, another important hardware aspect
is the speed of the bus system and the used memory. Current
PCI-E buses and current memory banks allow high-speed
transfer of packets from the capturing network card into the
memory and the analysing CPUs. These hardware advances
thus have shifted the bottlenecks, which were previously lo-
cated at the hardware layer, into the software stacks.
Software stack:
There are several software subsystems involved in packet
capture, as shown in Figure 1. Passing data between and
within the involved subsystems can be a very important per-
formance bottleneck that can impair capturing performance
and thus lead to packet loss during capturing. We will dis-
cuss and analyse this topic in Section 4.3.
A packet’s journey through the capturing system begins
at the network interface card (NIC). Modern cards copy the
packets into the operating systems kernel memory using Di-
rect Memory Access (DMA), which reduces the work the
driver and thus the CPU has to perform in order to transfer
the data into memory. The driver is responsible for allocat-
ing and assigning memory pages to the card that can be used
for DMA transfer. After the card has copied the captured
packets into memory, the driver has to be informed about
the new packets through an hardware interrupt. Raising
an interrupt for each incoming packet will result in packet
loss, as the system gets busy handling the interrupts (also
known as an interrupt storm). This well-known issue has
lead to the development of techniques like interrupt mod-
eration or device polling, which have been proposed several
years ago [7, 10, 11]. However, even today hardware inter-
rupts can be a problem because some drivers are not able to
use the hardware features or do not use polling—actually,
207when we used the igb driver in FreeBSD 8.0, which was re-
leased in late 2009, we experienced bad performance due to
interrupt storms. Hence, bad capturing performance can be
explained by bad drivers; therefore, users should check the
number of generated interrupts if high packet loss rates are
observed.1
The driver’s hardware interrupt handler is called imme-
diately upon the reception of an interrupt, which interrupts
the normal operation of the system. An interrupt handler is
supposed to fulﬁll its tasks as fast as possible. It therefore
usually doesn’t pass on the captured packets to the operating
systems capturing stack by himself, because this operation
would take to long. Instead, the packet handling is deferred
by the interrupt handler. In order to do this, a kernel thread
is scheduled to perform the packet handling in a later point
in time. The system scheduler chooses a kernel thread to
perform the further processing of the captured packets ac-
cording to the system scheduling rules. Packet processing
is deferred until there is a free thread that can continue the
packet handling.
As soon as the chosen kernel thread is running, it passes
the received packets into the network stack of the operat-
ing system. From there on, packets need to be passed to
the monitoring application that wants to perform some kind
of analysis. The standard Linux capturing path leads to a
subsystem called PF PACKET; the corresponding system in
FreeBSD is called BPF (Berkeley Packet Filter). Improve-
ments for both subsystems have been proposed.
Software improvements:
The most prominent replacement for PF PACKET on Linux
is called PF RING and was introduced in 2004 by Luca
Deri [8]. Deri found that the standard Linux networking
stack at that time introduced some bottlenecks, which lead
to packet loss during packet capture. His capturing infras-
tructure was developed to remove these bottlenecks. He
showed to achieve a higher capturing rate with PF RING
when small packets are to be captured. PF RING ships
with several modiﬁed drivers. These are modiﬁed to di-
rectly copy packets into PF RING and therefore completely
circumvent the standard Linux networking stack. This mod-
iﬁcation further boosts the performance for network cards
with a modiﬁed driver.
Figure 2 shows the diﬀerence between the two capturing
systems. One important feature of PF RING is the way it
exchanges packets between user space and kernel: Monitor-
ing applications usually access a library like libpcap [12] to
retrieve captured packets from the kernel. Libpcap is an ab-
straction from the operating systems’ capturing mechanisms
and allows to run a capturing application on several oper-
ating systems without porting it to the special capturing
architecture. Back in 2004, the then current libpcap version
0.9.8 used a copy operation to pass packets from the kernel
to the user space on Linux. An unoﬃcial patch against that
libpcap version from Phil Woods existed, which replaced the
copy operation by a shared memory area that was used to
exchange packets between kernel and application [13]. This
modiﬁcation will be called MMAP throughout the rest of
the paper. PF RING uses a similar structure to exchange
packets by default. Libpcap version 1.0.0, which was re-
1FreeBSD will report interrupt storms via kernel messages.
Linux exposes the number of interrupts via the proc ﬁle
system in /proc/interrupts.
Figure 2: PF RING and PF PACKET under Linux
leased in late 2008, is the ﬁrst version that ships built-in
shared memory (SHM) exchange support; hence the patch
from Phil Woods is not longer necessary. We will analyse
the performance of these diﬀerent solutions in Section 4.2.
All capturing mechanisms on Linux have something in
common: They handle individual packets, meaning that
each operation between user space and kernel is performed
on a per-packet basis. FreeBSD packet handling diﬀers in
this point by exchanging buﬀers containing potentially sev-
eral packets, as shown in Figure 3.
Both BPF as well as its improvement Zero-Copy BPF
(ZCBPF) use buﬀers that contain multiple packets for stor-
ing and exchanging packets between kernel and monitor-
ing application. BPF and ZCBPF use two buﬀers: The
ﬁrst, called HOLD buﬀer, is used by the application to read
packets, usually via libpcap. The other buﬀer, the STORE
buﬀer, is used by the kernel to store new incoming pack-
ets.
If the application (i.e., via libpcap) has emptied the
HOLD buﬀer, the buﬀers are switched, and the STORE
buﬀer is copied into user space. BPF uses a copy opera-
tion to switch the buﬀers whereas ZCBPF has both buﬀers
memory-mapped between the application and the kernel.
Zero-Copy BPF is expected to perform better than BPF as it
removes the copy operation between kernel and application.
However, as there are fewer copy operations in FreeBSD
than in non-shared-memory packet exchange on Linux, the