pose a realistic model for noiseless privacy, one has to take it into
account. In our model we give the adversary the precise knowledge
about all dependencies in subsets of size at most D. That essen-
tially means that he does not have an insight into dependencies of
subsets of size greater than D. Note that it might be the case that
such dependencies do not exists (namely the data might really have
all dependent subsets of size at most D), or simply the adversary
does not know about these dependencies and cannot therefore uti-
lize them. Obviously in standard differential privacy notion we do
not care about the distribution of data, whether it is dependent or
not and so on, which is much easier to comprehend in practical ap-
plications. Here, on the other hand, due to the necessity of utilizing
the inherent randomness in data instead of adding external noises,
we must take such things into account.
See that there is asymmetry between the adversary and other
users and even the data owner. Namely we assume that the ad-
versary has power of knowing the exact structure of dependencies
(of size at most D), while neither users nor the data owner have to
know this structure or the joint distribution of the data. The param-
eter necessary to use our results is the upper bound for D. Note that
the data owner might do some tests for independence of the data (or
subsets of the data), i.e. using χ2-test or other well known statis-
tical methods for testing independence. Information about the up-
per bound for the size of dependent subsets might also come from
strictly engineering knowledge, say due to physical proximity of
the subset of sensors or some social knowledge, say subset of users
having the same age. This approach to dependencies essentially
boils down to the known notion of dependency neighborhoods de-
ﬁned as below
DEFINITION 4. A collection of random variables X1, . . . , Xn
has dependency neighborhoods Ni ⊂ {1, . . . , n}, i ∈ {1, . . . , n}
if i ∈ Ni and Xi is independent of {Xj}j /∈Ni.
Observe that the deﬁnition of dependency neighborhoods actu-
ally says that for speciﬁc Xi we know that it is independent of
those that are not in its neighborhood. We want to give a general
approach to local dependencies scenario, so in our analysis we do
not assume anything about joint distributions of the dependent sub-
sets (i.e. the dependency in subset might even mean ’equality’) .
Note that in [5] the authors gave results for dependent data only for
the simplest case of boolean (true/false) data and queries, that is for
queries f such that f : {0, 1}n → {0, 1}. They did not discuss
dependencies for more complicated queries and data types. Here,
on the other hand, we aim to give a non-asymptotic formula for
privacy parameters for any distribution of data and a sum query
under dependency regime.
To sum it up, we present a formal deﬁnition of adversarial model.
DEFINITION 5. We will denote a speciﬁc instantiation of ad-
versarial model for data vector X by AdvX (D, γ), where
• D is upper bound for the size of the greatest dependent sub-
set,
• γ is the upper bound for the fraction of the data which values
the adversary exactly knows,
• adversary knows the distribution of data vector X.
We believe that while our adversarial model give signiﬁcantly
less power to the adversary than in standard differential privacy no-
tion (which basically gives the adversary almost full knowledge of
the data), they still are reasonable and applicable in real-life sce-
narios. One important remark is that we do not need to predict the
exact adversaries knowledge about the dependencies. We only need
to know the maximum size of dependency neighborhood, namely
the size of largest non-independent subset of data. In fact, we only
need an upper bound for that size. Same with the fraction of data
values which the adversary knows. To apply our results, which are
presented in the next sections, one will also need a lower bound for
the variance of data and upper bound for the sum of third and fourth
centralized moments for the speciﬁc data vector.
2.3 Applications
• In the case of distributed systems, the users themselves have
to secure their privacy using both cryptography and privacy
preserving techniques (see for example [35, 8]). The no-
tion of noiseless privacy and our bounds for privacy parame-
ters are useful especially in distributed case for two reasons.
First, in distributed systems quite often the noises which have
to be added by users render the data practically useless (too
much disturbance). Second, in such systems it is more com-
mon to assume that the adversary does not have full knowl-
edge, i.e. can know only at most some fraction of the data.
Note also that this paper is solely about privacy and we fo-
cus on showing that there are certain data types which does
not need any noise added to the ﬁnal output whether it is a
centralized or a distributed case. More details on speciﬁc
applications for distributed data aggregation can be found in
[35]. See that, if the noiseless privacy assumptions are met
and the privacy parameters are satisfying, one could for ex-
ample run protocols from [35, 8] with only the cryptographic
part, without adding noises to the values. The noises added
in standard approach turn out to be quite too big for practical
applications in various scenarios (see [20]).
• The idea of noiseless privacy can be used for a wide range
of applications including networks of sensing environmen-
tal parameters, smart metering (e.g, electricity), clinical re-
search, population monitoring or cloud services. Most im-
portant is however that in all these areas there are natural
cases, where we can make some assumptions about the ad-
versaries knowledge.
• Imagine a situation where we have a cloud service which
holds shopping preferences of its users. The data is dis-
tributed amongst many servers which are completely sepa-
rated from each other. We assume that at most some (say, 50
percent) of these servers became compromised, which means
that at most 50 percent of the values are known to the adver-
sary. Assume that he somehow knows the distribution of the
549rest (this means that he still has a lot of knowledge about
the rest of data) and even some dependencies due to geo-
graphical or other reasons. We might know that the greatest
dependent subset of our data has size at most D (due to inde-
pendence tests). This is our model (AdvX (D, γ)) for known
(or at least upper bounded) γ, D and distributions of the rest
of the data.
3. EXPLICIT BOUNDS FOR
INDEPENDENT DATA
As one can see in Figures 1 and 2, our Theorem does not only
give non-asymptotical, explicit parameters (both for the case where
ε is ﬁxed and the case where δ is ﬁxed), but also, due to slightly
more careful reasoning, our bound is tighter than the bound which
authors of [5] have implicitly shown in their proof.
Assume that we have a database X which consists of n values so
X = {X1, . . . , Xn}. Recall that i.i.d. means independent, iden-
tically distributed. Let us consider a simple, warm-up scenario,
where Xi are i.i.d. random variables and Xi ∼ Bin(1, p). We
want to aggregate the sum of all these variables so we set the mech-
anism as M (X) =(cid:80)n
i=1 Xi ∼ Bin(n, p).
Now we can state a theorem which shows that i.i.d. binomial
data has very strong noiseless privacy properties for a wide range of
parameters. First we consider the case where δ is ﬁxed and obtain
ε so that the data with summing mechanism is (ε, δ)-NP. Then we
ﬁx ε and calculate δ. Both cases are considered in the following
M (X) =(cid:80)n
THEOREM 1. Let X = (X1, . . . , Xn) be a data vector where
Xi ∼ Bin(1, p) are i.i.d. random variables. If we use mechanism
i=1(Xi) and ﬁx δ (cid:62) P (M (X) = 0) + P (M (X) =
n), we obtain that it is (ε, δ)-NP for the following
 , p (cid:54) 1
 , p > 1
2 ,
2 .
 1
 1
1−p −
(cid:114)
p −
(cid:18)
1(cid:114)
δ )
ln( 2
2n −p
1
ln( 2
δ )
2n −(1−p)
(cid:19)2(cid:33)
ε =
δ )
δ )
2n
(cid:113) ln( 2
(cid:113) ln( 2
(cid:32)
(cid:32)
2n
On the other hand, if ε > 0 is ﬁxed, we get
2 exp
−2np2
δ =
eε−1
eε+ p
1−p
(cid:18)
, p (cid:54) 1
2 ,
(cid:19)2(cid:33)
2 exp
−2n(1 − p)2
eε−1
eε+ 1−p
p
, p > 1
2 .
n
(cid:17)
(cid:16) 1√
Proof of this Theorem is quite long and laborious, albeit not very
complicated, as it mostly consists of straightforward observations
and application of Chernoff bounds. Due to space limitations and
mathematical technicalities, the proof has been moved to the Ap-
pendix.
Let us observe that in Theorem 1 for constant parameters p and
. It is also worth noting that for p close to λ
n
n , ε can be large, although as long as p is constant, ε still
δ we get ε = O
or 1 − λ
approaches 0 with n → ∞.
Similarly, for p very close to 0 or 1 and for small n, the value of
δ can be large. Nevertheless we see that δ is decreasing exponen-
tially to 0 with n → ∞, so for sufﬁciently large n we still get very
small values of δ, even if p was strongly biased.
One can easily see that this theorem is essentially equivalent to
Theorem 5 in [5], but our bounds are tighter and more useful in a
practical way, as we give straightforward, non-asymptotic, formu-
las for ε and δ. On the other hand, authors of [5] proved only that
due to Chernoff bounds, for a ﬁxed parameter ε the parameter δ
is asymptotically negligible. However, we completed their proof
and actually plugged the Chernoff bounds. In Figures 1 and 2 one
can see the comparison of our guarantee for parameters, and the
guarantee which are given by the (completed) proof in paper [5].
Figure 1: ε = 0.5, p = 0.95, red dashed line is a guarantee for
parameter δ in paper [5], blue thick line is guarantee from our The-
orem 1.
Figure 2: ε = 1, p = 0.2, red dashed line is a guarantee for param-
eter δ in paper [5], blue thick line is guarantee from our Theorem 1.
That was just a warm-up scenario to show how does noiseless
privacy work with simple data distribution. Let us move to a more
interesting model where users data has different, but still indepen-
dent distributions. Note that from now on we do not assume any
speciﬁc distribution of the data. Let us recall two facts. First one is
a known result in differential privacy literature.
FACT 1
that c2 > 2 ln( 1.25
σ (cid:62) c∆
ε we have
(FROM [16]). Fix ε > 0 and δ > 0. Let c such
δ ). For random variable Z ∼ N (0, σ2), where
P [u + Z ∈ S] (cid:54) eεP [v + Z ∈ S] + δ,
where u and v are any real numbers such that |u − v| (cid:54) ∆.
Second fact is a well known theorem in probability theory, one
can ﬁnd it for example in [18] .
FACT 2
sequence of independent random variables. Let EXi = 0, EX 2
(BERRY-ESSEEN THEOREM). Let X1, . . . , Xn be a
i =
550i > 0 and E|Xi|3 = ρi < ∞. Let Fn denote the cumulative dis-
σ2
tribution function of their normalized partial sum and Φ denotes
the cumulative distribution function of standard normal distribu-
tion. Then
|Fn(x) − Φ(x)| (cid:54) C ·(cid:80)n
(cid:0)(cid:80)n
i=1 ρi
(cid:1) 3
2
i=1 σ2
i
sup
x∈R
where C (cid:54) 0.5591.
The upper bound for constant C comes from [36].
After stating all necessary facts and deﬁnitions, we are ready to
present the general theorem for independent data.
n
i=1 V ar(Xi)
THEOREM 2. Let X = (X1, . . . , Xn) be a data vector, where
(cid:80)n
sider mechanism M (X) =(cid:80)n
Xi are independent random variables. Let µi = EXi and σ2 =
and E|Xi|3 < ∞ for every i ∈ {1, . . . , n}. Con-
i=1(Xi). We denote data sensitivity
of vector X and mechanism M as ∆. M (X) is (ε, δ)-NP with
(cid:114)
following parameters
ε =
∆2 ln(n)
,
nσ2
1.12(cid:80)n
and
δ =
i=1 E|Xi − µi|3
(nσ2)
3
2
(1 + eε) +
√
4
5
.
n
The main idea for proving this theorem is to use Berry-Esseen
theorem to deal with random variables of normal distribution in-
stead of the actual distribution of the data. Then we use normal
distribution properties to obtain appropriate ε and δ. The proof of
this theorem is moved to the Appendix. See that Theorem 2 is es-
sentially a generalization of Theorem 7 in [5], which is a simple
consequence of Theorem 2. In our case we give explicit formula
with all constants, which asymptotically, after using big oh no-
tation simpliﬁes to the same as in [5]. As we emphasized before,
explicit formulas for privacy parameters is much more useful for
a practitioner than the order of magnitude. Moreover, we do not
suffer from limitations of Theorem 7 in [5], where the authors as-
sumed that the result of the query has to be O(log(n)). In Section 4
we also give a generalization for locally dependent data.
Theorem 2 gives us very general notion of privacy parameters
for summing independent data. Note that in Theorem 2 we as-
sumed nothing about the distribution of the data, apart from being
independent. The only values we need to know is the variance and
sum of appropriate central moments (or upper bounds for these val-
ues). Data independence is obviously a strong (and generally false)
assumption in real world, but it is commonly used. However, we
will also work with dependent data in the next section. We also
present an example.
n
i=1 σ2
i
(cid:80)n
= 4. Let also(cid:80)n
mechanism M (X) =(cid:80)n
EXAMPLE 1. We consider a data vector X = (X1, . . . , Xn),
where Xi are independent random variables. Let ∆ = 30. Let
i=1 E|Xi − µi|3 = 3 · n. We use
σ2 =
i=1(Xi). Using Theorem 2 we obtain that
it is (ε, δ)-NP. Figure 3 shows how the ε decreases with n, while
Figure 4 shows how δ decreases with n.
We can see that for n around 10000 parameter δ is smaller than
0.05, which is a constant widely used in differential privacy liter-
ature, and decreases further. Also, note that for n (cid:62) 10000 the
parameter ε is below 0.5 which also is a widely used constant in
differential privacy papers (see for example [8]). Clearly, the pa-
rameters keep improving with more users.
Figure 3: Parameter ε in Example 1.
Figure 4: Parameter δ in Example 1.
4. EXPLICIT BOUNDS FOR LOCALLY
DEPENDENT DATA
In the previous section we gave a general treatment for privacy
parameters of independent variables. However, in many cases the
data has some local dependencies involved.
Imagine a situation
where we want to collect the data of yearly salary from former stu-
dents of a speciﬁc university. Say, those that ﬁnished their educa-
tion at most 5 years ago. Our goal is to obtain the average yearly
salary of all students that ﬁnished their education during last ﬁve
years. Now one can easily see that there will be some local de-
pendencies between the participants as some of the students might
work in the same company, launch a startup together or just work
in the same ﬁeld. This will affect their salary and therefore make
it locally dependent. Such dependencies are modeled using depen-
dency neighborhoods notion, which we deﬁned in Subection 2.2.
As previously, we want to take the sum of all our data and show
privacy parameters for this mechanism. We are going to take a sim-
ilar approach as in Theorem 2. That is, we want to bound the dis-
tance between the sum of our data and normal distribution. Then,
using standard differential privacy properties of normal distribution
(described in Fact 1) we derive privacy parameters. However, this
time we cannot use Berry-Esseen theorem to bound the mentioned
distance, as the data is not independent. Instead, we use Stein’s
method (see for example [3, 33]), which allows to bound the Kol-
mogorov distance between two random variables. Apart from that,
the presented reasoning is very similar to Theorem 2. Firstly, we
introduce some notation and facts.
551DEFINITION 6. Let X and Y be a random variables. Let µ and
ν be their corresponding probability measures. We denote their
Kolmogorov distance as dK (X, Y ) which is deﬁned as
dK (X, Y ) = sup
t∈R
|FX (t) − FY (t)| ,
(cid:90)
(cid:12)(cid:12)(cid:12)(cid:12) ,
(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)
where FX (·) denotes the cumulative distribution function of X.
Furthermore, we denote Wasserstein distance as dW (X, Y ) which
is deﬁned as
dW (X, Y ) = sup
h∈H
h(x)dµ(x) −
h(x)dν(x)
where H = {h : R → R : |h(x) − h(y)| (cid:54) |x − y|}.
These are standard probability metrics, their deﬁnition is also given
in, for example, [33]. We also recall a useful relation between Kol-
mogorov and Wasserstein distance.
FACT 3
(FROM [33]). Suppose that a random variable Y has
its density bound by some constant C. Then for any random vari-