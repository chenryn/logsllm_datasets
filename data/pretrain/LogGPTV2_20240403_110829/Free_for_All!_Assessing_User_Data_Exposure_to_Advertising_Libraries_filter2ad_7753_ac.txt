mobile.3 They can be used by the calling app to ﬁnd utilities,
perform security checks, and other functions. They also have
high potential for use in advertising. An illustration of this is
the Twitter app graph program [46], which was announced
in late 2014. Twitter asserted its plans to proﬁle users by
collecting their app bundles4 to “provide a more personal
Twitter experience for you.” Reacting to Twitter’s app graph
announcement, the Guardian newspaper postulated [12] that
Twitter “reported $320m of advertising revenues in the third
quarter of 2014 alone, with 85% of that coming from mobile
ads. The more it can reﬁne how they are targeted, the more
money it will make.” This progression marks an important point
about the impact of advertising on privacy. Both the Financial
Times [42] and a book about the economics of the online
advertising industry called The Daily You [45] emphasize the
strong pressures on the advertising industry to deliver better
quality information about users in a market place that is both
increasingly competitive and increasingly capable. This is a key
theme of this paper: what may seem opportunistic now may
be accepted business practice and industry standard in a few
years, and what is viewed as malicious today may be viewed
as opportunistic or adventurous tomorrow. Twitter provides
warnings to the user that Twitter will collect app bundles and
offers the user a chance to opt out of this. Other parties are less
forth-coming about their use of this technique of user proﬁling.
A. Use of App Bundles
Getting app bundles is a great illustration of the trajectory
of advertising on mobiles. In 2012 the AdRisk tool [18] showed
that 3 of 50 representative ad libraries it studied would collect
the list of all apps installed on the device. The authors viewed
this as opportunistic at best at the time. But what about now?
3Their
names
formal
are getInstalledApplications and
getInstalledPackages. The ﬁrst
the
second returns the packages and, from these, one can learn the application
names.
applications,
returns
the
4We use the term app bundle rather than app graph because we do not
develop a graph from the app lists.
6
We did a study of the pervasiveness of the use of app bundles
by advertising networks in Google Play. The functions getIA
and getIP are built into the Android API and require no special
permissions. We decompiled the 2700 apps we have collected
from Google Play, into smali code 5 for analysis and parsed
these ﬁles to look for the invocations of getAP and getIP in
each app. This allowed us to narrow the set of apps for analysis
to only those that actually collect a list of apps on the mobile,
which we deem an app bundle. We then conducted a manual
analysis of the invocation of these functions by ad libraries.
Of the 2700 apps selected for review, 165 apps were
duplicates, narrowing our sample size down to 2535 distinct
apps. Of these, 27.5% (679/2535) contained an invocation of
either of the two functions. This total includes invocation of
these functions for functional (utility and security) as well
as advertising purposes. To better understand if an ad library
invokes the function, analysis required a thorough examination
of the location of the function call to see if it is called by an
advertising or marketing library. We found that many apps pass
information to advertisers and marketers. We conducted this
analysis manually to best capture a thorough list of invocations
within ad libraries. Ultimately 12.54% of the examined apps
(318/2535) clearly incorporate ad libraries that invoke one of
the functions that collects the app bundle of the user. We found
28 different ad libraries invoking either getIA or getIP. These
results do not necessarily include those apps that collect app
information themselves and pass it to data brokers, advertising
or marketing companies, or have their own in-house advertising
operation (like Twitter). Our results demonstrate that many types
of apps have ad libraries that collect app bundles, including
medical apps and those targeted at children. Interestingly, we
did not detect collection of app bundles by the three ad networks
identiﬁed by AdRisk. However, a number of other interesting
cases emerged.
Radio Disney, for example, uses Burstly, a mobile app
ad network whose library 6 calls getIP. Disney’s privacy policy
makes no direct reference to the collection of app bundles for
advertising purposes. Use of this technique in an app targeted
at children is troubling because it might collect app bundle
information from a child’s device without notifying either the
parent who assisted the download or an older child that this type
of information is collected and used for advertising purposes.
Disney does mention the collection of “Anonymous Information”
but the broad language deﬁning this does not give any indication
that the Radio Disney app collects app bundles.7
Looney Tunes Dash! is a mobile app provided by
Zynga that it explicitly states that they collect ”Information
5The smali format is a human-readable representation of the application’s
bytecode.
6burstly/lib/apptracking/AppTrackingManager.smali
7Formally, they deﬁne anonymous information as “information that does not
directly or indirectly identify, and cannot reasonably be used to identify, an
individual guest.” App bundles are similar to movie play lists; it is debatable
whether they indeed satisfy this deﬁnition.
about ... other third-party apps you have on your device.”8 In
fact, this is the privacy policy for all Zynga apps.
Several medical apps (12) collect app bundles. Most
surprisingly, Doctor On Demand: MD & Therapy, an
app which facilitates a video visit with board-certiﬁed physi-
cians and psychologists collects app bundles through the
implementation of google/ads/ conversion tracking. However,
their linked privacy policy makes no reference to passing any
user information to advertisers. Other apps in the medical
category with advertising libraries that collect app bundles
include ones that track ovulation and fertility, pregnancy, and
remind women to take their birth control pill.
B. Survey Study
Upon learning of the prevalence of the app bundle collection
by advertisers, we sought to better understand what type of
information could be learned by advertisers based the list of
apps on a user’s mobile device. To do this, we devised a study
that would allow us to collect our own set of app bundles to
train a classiﬁer.
The study consisted of a survey and an Android mobile app
launched on the Google Play Store. The protocol for all the
parts of the study was approved by the Institutional Research
Board (IRB) for our institution. All participants gave their
informed consent. We required informed consent during both
parts of the study, and participants could leave the study at any
time. Participants were informed that the information collected
in the survey and the information collected by the mobile app
would be associated with one another.
Participants included individuals over the age of 18 willing
to participate in the survey and who owned an Android device.
Crowdsourcing platforms such as Amazon’s Mechanical Turk
are proven to be an effective way to collect high quality
data [9]. Our survey was distributed over Microworkers.com a
comparable crowdsourcing platform to Amazon’s Mechanical
Turk (MTurk). We chose Microworkers.com over Amazon
Mechanical Turk because Amazon Mechanical Turk did not
allow tasks that involve requiring a worker to download or
install any type of software.
We designed the mobile app, AppSurvey, to collect the
installed packages on a participant’s phone. The study directed
the participant to the Google Play Store to download the mobile
app. Upon launching AppSurvey, a pop-up screen provided
participants information about the study, information to be
collected, and reiterated that the participation in the study
was anonymous and voluntary. If the participant declined the
consent, no information would be collected. If the participant
consented, the app uploaded the app bundles from the partici-
pants phone and anonymously and securely transmit it to our
server. AppSurvey also generated a unique User ID for each
individual which participants were instructed to write down
and provide in the survey part of the study. Finally, AppSurvey
prompted participants to uninstall the mobile app.
We designed the survey based upon the FT calculator. The
survey consisted of 25 questions about basic demographic
information, health conditions, and Internet browsing and
spending habits. The survey contained two control questions
8https://company.zynga.com/privacy/policy
Monkey
DECOMPILER
DB
XML
JSON
Manifest
Layout
Strings
Matching 
Goals
Permissions
DB
XML
G
M
AGGR
DAM
Miners
Fig. 2: Design of In-app Pluto
included to identify survey participants not paying sufﬁcient
attention while taking the survey. If either of these questions
were answered incorrectly, we excluded the survey response.
In addition, our workers were not compensated until after
the ﬁnished tasks were reviewed and approved by the survey
conductors. Before taking the survey, participants were required
to give informed consent
to the information collected in
the survey. To link the app bundle information collected by
AppSurvey to the responses provided by participants in the
survey, participants were required to input the unique User ID
generated by AppSurvey. The collection of this data allows us
to establish a ground truth for users’ app bundles.
We successfully collected survey answers and app bundle
information from 243 participants. This resulted in 1985 distinct
package names collected.
VI. PLUTO: FRAMEWORK DESIGN AND IMPLEMENTATION
Pluto is a modular framework for estimating in-app and
out-app targeted data exposure for a given app. In-app Pluto
focuses on local ﬁles that the app generates, the app layout
and string resource ﬁles, and the app’s manifest ﬁle. Out-app
Pluto utilizes information about app bundles to predict which
apps will be installed together and employs techniques from
machine learning to make inferences about users based on the
apps they have on their mobile. We describe each of these in
a pair of subsections.
A. In-app Pluto
In-app Pluto progresses in two steps as illustrated in
Figure 2. First, the Dynamic Analysis Module (DAM) runs
the given app on a device emulator and extracts the ﬁles the
app creates. Then it decompiles the app and extracts its layout
ﬁles, resource ﬁles, manifest ﬁle and runtime generated ﬁles.
At the second step, the ﬁles produced by the DAM are fed
to a set of ﬁle miners. The ﬁle miners utilize a set of user
attributes and user interests, possibly associated with some
domain knowledge, as a matching goal. A miner will reach a
matching goal when it decides that a data point is present in
a ﬁle. When all the app’s ﬁles are explored, the Aggregator
(AGGR) removes duplicates from the set of matching goals
and the resulting set is presented to the analyst. Pluto’s in-
app component’s goal is to estimate ofﬂine, the exposure of
targeted data—or data points—to ad libraries at runtime. In-app
7
Pluto can be conﬁgured to estimate data points for a level 1
aggressive library by looking only at the runtime generated
ﬁles and available permissions. To perform exposure discovery
for a level 2 of aggression, it mines targeted data also from the
resource and layout ﬁles. In essence Pluto is trying to simulate
what an ad library is allowed to do to estimate what is the
potential data exposure from a given app. To perform in-app
exposure discovery, Pluto employs dynamic analysis and natural
language processing techniques to discover exposure of in-app
data points. Here we report on our prototype implementation
focusing on manifest, SQLite, XML, and JSON ﬁles.
1) Dynamic Analysis: To discover the ﬁles that an app
is generating at runtime, Pluto runs the app on an emulator
for 10 seconds and then uses a monkey tool to simulate user
input. 9 This can generate pseudo-random streams of clicks,
touches, and system-level events. We chose to use a monkey
because some apps might require user stimulation before
generating some of their local ﬁles. To validate our assumption,
we performed two experiments. First, we conﬁgured Pluto’s
DAM module to run all
2535 apps in the FD dataset
for 10 seconds each. We repeat the experiment, this time
conﬁguring DAM to issue 500 pseudo-random events to each
app after its 10 second interval is consumed. As we see on
Table III, Pluto explores approximately 5% more apps in the
second case. 10 More importantly, DAM Monkey generates
1196 more ﬁles than DAM which results in 100 apps with
‘interesting’ ﬁles more. Android’s Monkey was previously
found to achieve approximately 25.27% LOC coverage [4].
However, Pluto’s components can be easily replaced, and
advances in dynamic analysis can be leveraged in the future.
For example, PUMA [21] is a very promising dynamic analysis
tool introduced recently. If new levels of library aggression
are introduced in the future, PUMA could be used instead of
Android’s monkey to better simulate behaviors that can allow
libraries to access user attributes at runtime.
TABLE III: DAM’s coverage. * denotes interesting ﬁles
(SQLite, XML, JSON)
DA Strategy
DAM
DAM Monkey
% successful
experiments
0.718
0.763
#ﬁles
14556
15752
# *ﬁles
9083
10171
#of apps w/
*ﬁles
1911
2021
Once the execution completes, DAM extracts all
the
‘runtime’ generated ﬁles. Subsequently, it decompiles the input
android app package (apk) and extracts the Android layout
ﬁles, Android String resources and the app’s manifest ﬁle.
2) File Miners empowered by Natural Language Processing:
Once the DAM module generates ‘runtime’ ﬁles, Pluto’s
enabled ﬁle miners commence their exploration. We have
implemented four types of ﬁle miners in our prototype:
MMiner; GMiner; DBMiner; XMLMiner. The MMiner is
designed to parse manifest ﬁles, the DBMiner for SQlite
database ﬁles, the XMLMiner for runtime generated XML
ﬁles and the GMiner is a generic miner well suited for resource
and layout ﬁles. The miners take as input, a set of data points, 11
9In
our
implementation we
used
the Android SDK-provided
UI/Application Exerciser Monkey [11].
10An unsuccessful experiment includes apps that failed to launch or crashed
during the experiment.
11We derived most of the data points from the FT calculator [42].
in the form of noun words and a mapping between permissions
and data points that can be derived given that permission.
Input processing: Pluto utilizes Wordnet’s English seman-
tic dictionary [32] to derive a set of synonyms for each data
point. However, a word with multiple meanings will result
in synonyms not relevant to Pluto’s matching goal. Consider
for example the word gender. In Wordnet, gender has two
different meanings: one referring to grammar rules and the one
referring to reproductive roles of organisms. In our case it is
clear that we are interested in the latter instead of the former.
In our prototype, the analyst must provide Pluto with the right
meaning. While it is trivial to make this selection, for other
data points it might not be as trivial. For example, age has
5 different meanings in Wordnet. Other data points which we
have not explored, might have even more complex relationships.
In our experience we found Visuwords.com to be a helpful tool
to visualize such relationships and immensely facilitated our
selections. We were inspired by the list of data points in the
FT calculator, which is indeed feasible to analyze manually.
However, Pluto does not require this from an analyst. If the
meaning is not provided, Pluto will take all synonym groups
into account with an apparent effect on precision.
NLP in Pluto: The NLP community developed different
approaches to parse sentences and phrases such as Parts
of Speech (POS) Tagging and Phrase and Clause
Parsing. The former can identify parts of a sentence or
phrase (i.e., which words correspond to nouns, verbs, adjectives
or prepositions), and the latter can identify phrases. However,
these cannot be directly applied in our case because we are
not dealing with well written and mostly grammatically correct
sentences. In contrast, Pluto parses structured data written in a
technically correct way (e.g., .sqlite, .xml ﬁles). Thus in our
case we can take advantage of the well-deﬁned structure of these
ﬁles and extract only the meaningful words. For the database
ﬁles, potentially meaningful words will constitute the table
name and the columns names. Unfortunately, words we extract
might not be real words. A software engineer can choose any-
thing for the table name (or ﬁlename), from userProfile,
user_profile, uProfil, to up. We take advantage of
the fact that most software engineers do follow best practices
and name their variables using the ﬁrst two conventions, the
camelCase (e.g. userProﬁle) and the snake_case structure
(e.g. user proﬁle). The processed extracted words are checked
against Wordnet’s English semantic dictionary. If the word
exists in the dictionary, Pluto derives its synonyms and performs
a matching test against the data points and their synonyms. 12
If a match is determined, then a disambiguation layer decides
whether to accept or reject the match. Next, we elaborate on
the functions of the disambiguation layer.
Context Disambiguation Layer: Words that reach a match-
ing goal, could be irrelevant with the actual user attribute.
Consider for example the word exercise. If a Miner unearths
that word, it will be matched with the homonymous synonym
of the matching goal workout. However, if this word is