### 文本优化

#### 图6：容错系统相对于基准（即TensorFlow）吞吐量的性能下降
- **(a) CPU**
  - 容错机制: Crash-tolerant, SSMW, MSMW
  - 分布式学习: Decentralized Learning
  - 模型: MNIST_CNN, CifarNet, Inception, Resnet-50, Resnet-200, VGG

- **(b) GPU**
  - 容错机制: Crash-tolerant, SSMW, MSMW
  - 分布式学习: Decentralized Learning
  - 模型: MNIST_CNN, CifarNet, Inception, Resnet-50, Resnet-200, VGG

图6展示了容错系统的性能下降情况，以基准（即TensorFlow）的吞吐量为参考。对于CPU部署，性能下降范围从1%到42%（0.1%到22%），分布式学习的开销范围从41%到154%（16%到61%）。显然，基于CPU的部署比基于GPU的部署表现出更高的性能下降。这主要归因于两个原因：
1. 在前一种情况下测试了更多的机器，导致更高的通信开销。
2. 在CPU上，GARs的开销比在GPU上更大。

从图6中我们可以得出以下几点观察结果：
1. 与基准相比，拜占庭容错的成本较高（最坏情况下达到约12倍），但这种成本与较弱的替代方案（例如崩溃容错）相比是合理的。有趣的是，使用SSMW实现工人节点的拜占庭容错的成本始终低于崩溃容错的成本（尤其是在大型模型中更为明显）。
2. 机器学习训练，特别是在GPU上，主要受限于网络：需要更多通信的应用程序具有更大的性能下降。本质上，通信占据了超过75%的开销，而鲁棒聚合仅占不到11%（见图7）。

#### 图7：基于CPU的实验中的开销分解
第三，增加模型维度会增加拜占庭容错的开销，但仅在某个点之前；之后，即使模型更大，开销也大致保持不变。造成这种情况的原因在于驱动开销的因素。对于小型模型，模型越大，鲁棒聚合的成本越高，开销也就越高。然而，对于大型模型，通信开销占据主导地位，所有部署的通信开销都是O(d)。

**开销分解**：
我们选取一个实例并仔细分析所有部署，以了解影响其性能的因素。具体来说，我们在训练ResNet-50时运行相同的实验，并分解每个部署的平均每迭代延迟。图7显示了基于CPU集群部署的系统开销分解。由于难以将TensorFlow的通信和计算时间分开，蓝色和橙色条表示这两者结合的时间。从图中可以看出，所有应用的计算时间大致相同（约1.6秒），但通信成本主导了开销（范围从75%到86%）。这使得崩溃容错比拜占庭工人节点容错更昂贵（额外22%的通信成本），而拜占庭服务器容错比仅工人节点容错更昂贵（额外42%的通信成本）。此外，我们注意到，由于额外的模型聚合步骤，分布式学习的聚合时间是SSMW的两倍。

#### 图8：不同部署的性能比较
图8a显示SSMW优于AggregaThor。这主要是因为我们对GARFIELD进行了优化，并且使用了更新版本的TensorFlow。从图8中可以得出以下三点主要观察结果：
1. 所有系统随着工作节点数量的增加而扩展（除了分布式学习应用），并且在GPU上的吞吐量比CPU高一个数量级。
2. 随着工作节点数量的增加，基准部署与容错部署之间的吞吐量差距增大，但SSMW、MSMW和崩溃容错引入的性能下降几乎保持不变。
3. MSMW的可扩展性几乎与崩溃容错部署一样好，随着工作节点数量的增加，吞吐量的差异几乎保持不变。这表明完全拜占庭容错并不会比崩溃容错损害可扩展性。

#### 图9：通信时间与节点数和模型维度的关系
为了理解为什么分布式学习不具有可扩展性，我们重点关注其在不同节点数（n）和模型维度（d）下的通信开销。图9显示了分布式学习和基准（PyTorch）在不同n（d = 10^6）和d（n = 6）值下的通信延迟。显然，增加d会使带宽迅速饱和，并使两种系统的通信时间线性增加（图9b）。然而，分布式学习的可扩展性问题在图9a中显现出来，其中分布式学习的通信时间急剧增加（即二次增长），而基准系统的通信时间仅线性增加。基本上，分布式学习每轮需要O(n^2)条消息，而基准部署只需要O(n)条消息。

通过这些图表和分析，我们可以更好地理解不同容错机制和分布式学习方法在不同硬件平台上的性能表现。