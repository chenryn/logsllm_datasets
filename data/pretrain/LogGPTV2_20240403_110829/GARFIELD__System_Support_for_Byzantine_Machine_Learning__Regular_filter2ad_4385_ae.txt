2
1
0
Crash-tolerant
SSMW
MSMW
Decentralized Learning
MNIST_CNN CifarNet
Inception
Resnet-50 Resnet-200
VGG
(a) CPU
Crash-tolerant
SSMW
MSMW
Decentralized Learning
MNIST_CNN CifarNet
Inception
Resnet-50 Resnet-200
VGG
Fig. 6: Slowdown of fault–tolerant systems normalized to
the vanilla baseline (i.e., TensorFlow) throughput.
(b) GPU
ranges from 1% to 42% (0.1%–22%) and decentralized learn-
ing overhead ranges from 41% to 154% (16%–61%). It is
evident that CPU–based deployments show higher slowdowns
than that of the GPU–based ones. We root that to two reasons:
(1) we test with more machines in the ﬁrst case, inducing
higher communication overhead, and (2) GARs overhead is
bigger with CPUs than with GPUs.
We extract several observations from Figure 6. First, the cost
of Byzantine resilience, compared to vanilla baselines, seems
big (reaches ∼ 12x in the worst case) however, such a cost is
reasonable compared to weaker alternatives (e.g., crash toler-
ance). Interestingly, the cost of workers’ Byzantine resilience
(using SSMW) is always less than that of crash tolerance
(more clear with big models). Second, ML training, especially
on GPUs, is network–bound: applications that require more
communication have bigger slowdowns. Essentially, commu-
nication constitutes more than 75% of the overhead where,
robust aggregation account for less than 11% (see Figure 7).
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:24:13 UTC from IEEE Xplore.  Restrictions apply. 
47
)
c
e
s
(
n
o
i
t
a
r
e
t
i
r
e
p
y
c
n
e
t
a
L
20
15
10
5
0
Computation
Communication
Aggregation
TF
Crash Tolerant SSMW
MSMW Dec. Learn.
Fig. 7: Overhead breakdown in a CPU-based experiment.
Third, increasing the model dimension increases the overhead
of Byzantine resilience yet only until a certain point; after that
point, the overhead remains roughly constant even with bigger
models. The reason for that lies in the factors driving such
an overhead. With small models, the bigger the model, the
higher the cost of robust aggregation, the higher the overhead.
Yet, with bigger models, the communication overhead prevails,
which is in O(d) for all deployments.
b) Overhead breakdown: We pick one instance and take
a closer look at all the deployments to understand the factors
affecting their performance. Concretely, we run the same
experiment while training ResNet-50, breaking the average
latency per iteration for each deployment. Figure 7 depicts
the breakdown of the systems overhead when deployed on the
CPU–based cluster. It is hard to decompose communication
and the computation time for TensorFlow. Thus, the blue-and-
orange bar denotes the time spent in both of them combined.
We can observe in the ﬁgure that the computation time
is roughly the same for all applications (∼ 1.6s). Yet, the
communication cost dominates the overhead (ranges from 75%
to 86%). This makes (1) crash tolerance costly more than
Byzantine workers’ tolerance (22% extra communication),
and (2) Byzantine servers’ tolerance costly more than only
workers’ tolerance (42% more communication). Furthermore,
we note that the aggregation time in decentralized learning is
two times bigger than that of SSMW, due to the extra model
aggregation step done by the former application.
Figure 8a shows that SSMW outperforms AggregaThor.
This happens arguably due to the optimizations we include
in GARFIELD in addition to using a newer version of Tensor-
Flow. We draw three main observations from Figure 8. First,
all systems scale with employing more workers (except the
decentralized learning application), with around one order of
magnitude higher throughput with GPUs compared to CPUs.
Second, the throughput gap between the vanilla deployments
and the fault-tolerant deployments increases with increasing
nw, keeping the slowdown introduced by SSMW, MSMW, and
crash–tolerant almost constant. Third, the scalability of MSMW
is almost as good as that of the crash–tolerant deployment,
and the difference in throughput with increasing nw is almost
constant. This shows that complete Byzantine resilience does
not harm scalability compared to crash resilience.
)
.
c
e
s
(
e
m
i
t
n
o
i
t
a
c
i
n
u
m
m
o
C
0.08
0.06
0.04
0.02
Decentralized Learning
PyTorch
2
3
4
n - Number of nodes
5
6
)
.
c
e
s
(
e
m
i
t
n
o
i
t
a
c
i
n
u
m
m
o
C
100
10−1
10−2
104
Decentralized Learning
PyTorch
105
106
d - Model dimension
107
108
(a) Number of Inputs
(b) Input dimension
Fig. 9: Communication time of decentralized learning and
vanilla baseline (deployed on GPUs) with n and d.
To understand why decentralized learning does not scale,
we focus on its communication overhead with different num-
ber of nodes (n) and model dimension (d). Figure 9 shows the
communication latency of decentralized learning and vanilla
baseline (PyTorch in this experiment) with different values
of n (with d = 106) and d (with n = 6). It is evident that
increasing d saturates the bandwidth quickly and increase the
communication time linearly for both systems (Figure 9b).
Yet, the scalability issue of decentralized learning appears
in Figure 9a, where the communication time increases drasti-
cally (i.e., quadratically) for decentralized learning while only
linearly with the vanilla competitor. Basically, decentralized
learning requires O(n2) messages per round while the vanilla
deployments require only O(n) messages per round.
)
c
e
s
/
s
e
h
c
t
a
b
(
t
u
p
h
g
u
o
r
h
T
180
160
140
120
100
80
60
40
20
TensorFlow
Crash-tolerant
SSMW
MSMW
Decentralized Learning
AggregaThor
3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
nw
)
c
e
s
/
s
e
h
c
t
a
b
(
t
u
p
h
g
u
o
r
h
T
1200
1000
800
600
400
200
PyTorch
Crash-tolerant
SSMW
MSMW
Decentralized Learning
5
7
9
nw
11
13
)
c
e
s
/
s
e
t
a
d
p
u
(
t
u
p
h
g
u
o
r
h
T
0.100
0.095
0.090
0.085
0.080
0.075
0.070
0.065
0.060
PyTorch
TensorFlow
0
1
fw
2
3
)
c
e
s
/
s
e
t
a
d
p
u
(
t
u
p
h
g
u
o
r
h
T
0.065
0.060
0.055
0.050
0.045