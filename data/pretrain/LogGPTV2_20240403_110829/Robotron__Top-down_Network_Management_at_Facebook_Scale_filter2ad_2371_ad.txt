cation from Robotron upon failures.
Human Conﬁrmation: For certain cases, engineers
can verify expected network behavior within a grace pe-
riod after roll-out. During this timeframe, new conﬁgu-
Figure 11: Robotron’s active monitoring pipeline is di-
vided into 3 tiers: job manager, engines, and backends.
rations are temporarily committed to the devices where
engineers can conduct ad-hoc veriﬁcation. A ﬁnal con-
ﬁrmation must be provided during the grace period oth-
erwise Robotron will rollback the changes.
5.4 Monitoring
To ensure the continuous health of the network, Robotron
employs three main monitoring mechanisms: passive
monitoring, active monitoring, and conﬁg monitoring.
5.4.1 Passive Monitoring
Passive monitoring detects operational events such as
running conﬁguration changes, route ﬂaps, and device
reboots. Syslog [23] is our main passive monitoring in-
terface due to wide support by vendors. In our passive
monitoring pipeline, each device is conﬁgured to send
syslog messages to a BGP anycast address. Multiple
classiﬁers collect these messages from the anycast ad-
dress based on a set of regular expression rules main-
tained by network engineers. A syslog message match-
ing a rule triggers the corresponding alerts which are
remediated automatically or manually by engineers.
5.4.2 Active Monitoring
We use active monitoring to collect performance met-
rics (e.g., link, CPU, and memory utilization) and de-
vice states which can be used for cases such as populat-
ing FBNet Derived models. Figure 11 shows the three
major tiers of this pipeline.
Speciﬁcally, the Job Manager schedules periodic mon-
itoring jobs based on a list of job speciﬁcations, each of
which describes the collection period, the type of data,
the devices, and the storage backends the data should
be sent to. Job manager can also create ad-hoc moni-
toring jobs on-demand. The Engines pull jobs from the
Job Manager directly and poll data from the network
devices accordingly. There are multiple diﬀerent en-
gines using diﬀerent polling mechanisms such as SNMP,
XML/RPC, CLI and Thrift. Backends receive the col-
lected data and convert it into a format appropriate for
diﬀerent storage locations.
5.4.3 Conﬁg Monitoring
Robotron leverages both passive and active monitor-
ing to monitor the running conﬁguration of devices.
When a running conﬁg is updated, a syslog message
is generated and captured by our passive monitoring
XMLSNMPCLIThriftEnginesJob	ManagerFBNetHBaseHiveBackendsDevicesFigure 13: The number of related models associated
with each FBNet model.
(a) POP
(b) DC
Figure 12: Evolution of cluster architectures.
pipeline. The message then triggers an active moni-
toring job which collects the running conﬁg, compares
it with Robotron-generated “golden” conﬁguration, and
notiﬁes the engineers of any discrepancy. A conﬁg change
is typically detected within minutes. Each collected
running conﬁg is also backed up in a revision control
system to track the history of each device conﬁg. The
conﬁg monitoring framework ensures (1) the continu-
ous conformance of device conﬁgs to their golden con-
ﬁgs throughout our network and (2) the engineers can
rollback to any prior device conﬁg upon disasters.
6. USAGE STATISTICS
Facebook’s network evolves in a hybrid and dynamic
fashion. The backbone network constantly experiences
organic growth and changes in size, circuit speed, and
its mesh topology. DC and POP networks, already hav-
ing multiple architectures, underwent several major up-
grades.
Figure 12 shows the evolution of our POP and DC
architecture over the last two years. Originally, the de-
ployment of Gen1 POP clusters rapidly grew to serve
increasing user traﬃc. But over a few months, they
were quickly merged into bigger Gen2 POP clusters
to improve eﬃciency and manageability. Contrasting
with the simplicity of POP architecture, our DC clus-
ters went through three architecture generations, each
with multiple topologies. Additionally, the exhaustion
of the private IPv4 address space required newer clus-
ters to only support IPv6. Multiple generations of DC
architecture had to co-exist because unlike POP clus-
ters, where architectural upgrades were completed in-
place due to space/power limitation in POPs, architec-
tural shifts for DC clusters took place by adding new
and decommissioning previous generations of clusters.
The life cycle of a DC cluster could end due to shifts
in space/power, changes in service requirements, and
Figure 14: Desired model changes.
server hardware refreshes. Robotron ensures our net-
work can evolve and support these architectures with
minimal disruption to traﬃc.
In the remaining sections, we present usage statis-
tics from various parts of Robotron. Unlike a typi-
cal system evaluation, our focus is not on the system
performance such as task completion time, due to the
highly implementation- and workﬂow-dependent nature
of these metrics. Instead, we focus on Robotron usage
statistics to realize various network management tasks.
6.1 FBNet Models
FBNet models dependency of network components
by association. For example, Circuit model is asso-
ciated with PhysicalInterface model, and the latter
is associated with AggregatedInterface model (Fig-
ure 5). Figure 13 shows the number of related models
associated with each FBNet model. We can observe
that around 60% of models have more than 5 related
models. Closely modeling these dependencies allows us
to ensure data integrity in FBNet.
We also want to understand the frequency of changes
made by engineers to the FBNet models. Django stores
all models in multiple models.py ﬁles, whose histories
are maintained in a version control system. Figure 14
depicts the total number of lines changed per week over
a 3-year period for the Desired model group.
Many people would assume that the models should
become stable after several weeks in production, but our
observations record more than 50 lines changed, on av-
erage, daily. Occasionally, large refactoring eﬀorts can
touch hundreds of lines of code. Unfortunately, it is dif-
ﬁcult to classify each change programmatically. Based
on our discussion with network engineers as well as man-
 0 0.2 0.4 0.6 0.8 1# of clusters (normalized)TimeGen2Gen1(normalized) 0 0.2 0.4 0.6 0.8 1# of clusters (normalized)TimeGen3V6Gen3Gen2V6Gen2-DGen2-CGen2-BGen2-AGen1 0 0.2 0.4 0.6 0.8 1 0 5 10 15 20 25 30CDF across models# of related modelsTime0100200300400500600700# of Lines Per Week(a) POP and DC
(b) Backbone
Figure 15: Number of changed FBNet objects across
design changes.
ually analyzing some examples, we found that models
change for several reasons:
New Component Types: This is the most obvi-
ous reason for changes. New components result in cre-
ation of new models. Moreover, a component deﬁned in
FBNet does not necessarily correspond to the physical
component. For example, we created the BGPV4Session
model to capture BGP sessions during the transition
from Gen1 (L2) to Gen2 (L3 BGP) DC clusters.
New Attributes: FBNet models are not, at incep-
tion, all-inclusive. They only capture the attributes en-
gineers value or require at that moment. As a result,
new attributes are constantly added to existing mod-
els as needed. In addition, the attributes may or may
not correspond to a direct conﬁguration/command. For
example, the drain_state attribute, a purely “opera-
tional state”, is added to backbone routers to denote
whether the router is serving production traﬃc.
Logic Changes: Some attributes are not directly
stored in FBNet. Instead, they are generated system-
atically on the ﬂy. The derivation logic may change as
our understanding of the use cases matures. For exam-
ple, a router has an attribute asset_url which points
to a web page showing the device’s asset management
details. The logic that generates this URL evolves over
time along with our asset management system.
6.2 Design Change
During network design stage, engineers perform var-
ious design changes. A design change is an atomic op-
eration that stores a human-speciﬁed change to FBNet.
It can be as simple as migrating a single circuit or as
complex as building an entire cluster. Robotron takes
minimum human speciﬁcation as input and automati-
cally handles the creation, modiﬁcation, and deletion of
FBNet objects for each design change.
Figure 16: Weekly conﬁguration changes during a 3-
month period. Each sample represents one device in a
particular week.
Figure 15 compares the number of changed FBNet
objects, i.e., those that are created, modiﬁed, and deleted
across all design changes over one year. First, a design
change usually has high fan-out, changing from a few
objects to 10,000 objects. Second, designs in POP and
DC change more objects than in backbone, for exam-
ple, the median number of all changed objects is 120
for POP and DC networks in Figure 15(a) and is 20 for
backbone network in Figure 15(b). This is because the
former is usually one-time building of an entire cluster
and the latter is mostly incremental and partial device
and circuit changes. Third, the ﬁgure also breaks down
the result into diﬀerent object types, among which in-
terface objects are changed most frequently, followed by
circuit, v6 preﬁx, v4 preﬁx, and device objects. Note
that v6 preﬁx is changed more than v4 preﬁx as we
move toward v6-only clusters.
6.3 Conﬁguration Change
Figure 16 shows the weekly conﬁguration changes dur-
ing a 3-month period. Each sample represents total up-
dated conﬁg lines (changed/added/removed, excluding
comments) on a device in a particular week. We count
PRs and DRs as backbone devices since they are usu-
ally updated along with BBs, unlike POP/DC devices.
For example, 90% of backbone device samples have less
than 500 updated lines per week, while only 50% of
POP/DC samples are of the same size.
Beyond weekly aggregated metrics shown in Figure 16,
we also observe that while changes are smaller on back-
bone devices (157.38 lines updated per change on av-
erage versus 738.09 on POP/DC devices), they have
a greater number applied (12.46 changes per week on
average versus 2.53 on POP/DC devices). This is con-
sistent with Section 5.1 as we update backbone devices
incrementally, while our network devices in POPs and
DCs are usually conﬁgured from a clean state. Unlike
POP/DC devices, operating backbone devices requires
continuous live re-conﬁgurations, which beneﬁt greatly
from Robotron’s conﬁg generation and deployment.
 0 0.25 0.5 0.75 11101001,00010,000CDF across design changes# of FBNet objectsAllInterfaceCircuitv6 Prefixv4 PrefixDevice 0 0.25 0.5 0.75 11101001,00010,000CDF across design changes# of FBNet objectsAllInterfaceCircuitv6 Prefixv4 PrefixDevice 0 0.2 0.4 0.6 0.8 1 0 1 2 3 4 5 6 7 8 9CDF # of samples# of updated lines (103)Backbone Devices (24993 samples)POP/DC Devices (6903 samples)Types
SNMP (active)
CLI (active)
RPC/XML (active)
Thrift (active)
Syslog (passive)
Total
# of events Percentage
50.94%
11.25%
4.87%
12.21%
20.73%
100%
121.25M
26.78M
11.59M
29.07M
49.34M
238.03M
Table 2: Monitoring events in a 24-hour period.
6.4 Monitoring Usage
Table 2 quantiﬁes monitoring events triggered in a
24-hour period. We observe that while the majority of
these events use industry standards like SNMP and Sys-
log, we still rely on other non-standard approaches like
CLI to collect data. This is mainly due to shortcomings
of standard mechanisms, or lack of better vendor sup-
port. For example, for some vendors, the operational
status of the physical links within an aggregated inter-
face can only be collected by CLI commands. Note that
the active monitoring event rate is neither limited by the
processing complexity nor the quantity of monitoring
jobs. In our system, the event rate is limited by network
device capabilities such as CPU and/or memory and the
underlying vendor implementation. For example, some
monitoring jobs can take more than ten seconds to ﬁn-
ish, and some jobs such as getting all physical interfaces
information signiﬁcantly increase the CPU load of the
networking devices. These limitations restrict our mon-
itoring granularity.
Table 3 breaks down the types of syslog messages in
a 24-hour period. We observe that the messages are
very noisy, with more than 95% of them being ignored
by the engineers. Among the 5% considered valuable,
most are warning messages incapable of causing any
major problems.
7. ROBOTRON EVOLUTION
Robotron’s design has evolved signiﬁcantly since 2008.
Perhaps counter-intuitively, Robotron did not start out
as a top-down solution. Its initial focus was on gaining
visibility into the health of the network through active
and passive monitoring systems (Section 5.4). FBNet
was created to track basic information about network
devices such as loopback IPs and store raw data pe-
riodically discovered from network devices. However,
per-device data was too low-level, vendor-speciﬁc, and
sometimes requires piecing multiple data together to
construct meaningful information, making it extremely
diﬃcult to consume. As a result, basic Derived models
(Section 4.1.2) were created in FBNet to store a normal-
ized, vendor-agnostic view of the actual network state
constructed from the raw data. Ad-hoc audits could be
easily written against Derived models to look for design
violations, misconﬁgurations, hardware failures, etc.
With basic monitoring in place, we started working
on the other stages of the network management life
cycle. There were two main challenges based on user
feedbacks. First, deployment of conﬁg updates (e.g.,
changes to routing or security policies) to a large num-
ber of devices was still manual, requiring logging into
each device and copying and pasting conﬁgs. To ad-
dress this, the deployment solution (Section 5.3) was
developed to enable scalable and safe conﬁg rollout.
Second, many backbone circuits needed to be turned
up to meet the growing inter-DC traﬃc demand. How-
ever, provisioning a circuit was a time-consuming and
error-prone process, involving manually ﬁnding unused
point-to-point IPs (through pinging IPs not in Derived
models) and conﬁguring them on both circuit endpoints.
Not only were we unable to grow the network capacity
fast enough, many circuits were misconﬁgured with con-
ﬂicting IPs. To automate such design changes, Desired
models were introduced to FBNet, from which IPs and
circuits were allocated using design tools based on pre-
deﬁned rules, and relevant conﬁg snippets were gener-
ated for deployment. Over time the suite of design tools
were developed to cover diﬀerent use cases (Section 5.1),
and more templates were added for diﬀerent vendors to
generate vendor-speciﬁc device conﬁgs (Section 5.2).