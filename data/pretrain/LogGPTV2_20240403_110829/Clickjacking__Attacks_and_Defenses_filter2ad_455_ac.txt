user interacts with sensitive elements.
Lightbox around sensitive element. Greyout (also
called Lightbox) effects are commonly used for focus-
ing the user’s attention on a particular part of the screen
(such as a popup dialog). In our system, we apply this
effect by overlaying a dark mask on all rendered content
around the sensitive UI element whenever the cursor is
within that element’s area. This causes the sensitive ele-
ment to stand out visually.
The mask cannot be a static one. Otherwise, an at-
tacker could use the same static mask in its application
to dilute the attention-drawing effect of the mask.
In-
stead, we use a randomly generated mask which consists
of a random gray value at each pixel.
No programmatic
cross-origin keyboard focus
changes. To stop strokejacking attacks that steal key-
board focus (see Section 3.1.2), once the sensitive UI
element acquires keyboard focus (e.g., for typing text in
an input ﬁeld), we disallow programmatic changes of
keyboard focus by other origins.
Discussion. This list of techniques is by no means ex-
haustive. For example, sensitive elements could also
draw the user’s attention with splash animation effects
on the cursor or the element [15].
8
Our goal was to come up with a representative set of
techniques with different security and usability tradeoffs,
and conduct user studies to evaluate their effectiveness
as a design guide. We hope that this methodology can be
adopted by browser vendors to evaluate a wider range of
techniques with a larger-scale user study for production
implementations.
5.2 Ensuring Temporal Integrity
Even with visual integrity, an attacker can still take a
user’s action out of context by compromising its tempo-
ral integrity, as described in Section 3.1.3. For example, a
timing attack could bait the user with a “claim your free
iPad” button and then switch in a sensitive UI element
(with visual integrity) at the expected time of user click.
The bait-and-switch attack is similar to time-of-check-
to-time-of-use (TOCTTOU) race conditions in software
programs. The only difference is that the race condition
happens to a human rather than a program. To mitigate
such TOCTTOU race conditions on users, we impose the
following constraints for a user action on a sensitive UI
element:
UI delay. We apply this existing technique (discussed in
Section 3.2.2) to only deliver user actions to the sensi-
tive element if the visual context has been the same for
a minimal time period. For example, in the earlier bait-
and-switch attack, the click on the sensitive UI element
will not be delivered unless the sensitive element (to-
gether with the pointer integrity protection such as grey-
out mask around the sensitive element) has been fully
visible and stationary long enough. We evaluate trade-
offs of a few delays in Section 7.3.
UI delay after pointer entry. The UI delay technique
above is vulnerable to the whack-a-mole attack (Sec-
tion 4.3) that combines pointer spooﬁng with rapid ob-
ject clicking. A stronger variant on the UI delay is to
impose the delay not after changes to visual context, but
each time the pointer enters the sensitive element. Note
that the plain UI delay may still be necessary, e.g., on
touch devices which have no pointer.
Pointer re-entry on a newly visible sensitive element.
In this novel technique, when a sensitive UI element ﬁrst
appears or is moved to a location where it will overlap
with the current location of the pointer, an InContext-
capable browser invalidates input events until the user
explicitly moves the pointer from the outside of the sen-
sitive element to the inside. Note that an alternate design
of automatically moving the pointer outside the sensitive
element could be misused by attackers to programmati-
cally move the pointer, and thus we do not use it. Obvi-
ously, this defense only applies to devices and OSes that
provide pointer feedback.
Padding area around sensitive element. The sensitive
UI element’s padding area (i.e., extra whitespace separat-
Sensitive Element
Dimensions Click Delay Memory
Overhead
5.11 MB
12.04 ms
Facebook Like
8.60 MB
13.54 ms
Twitter Follow
14.92 ms
7.90 MB
Animated GIF (1.5 fps)
24.78 ms 12.95 MB
Google OAuth
PayPal Checkout
30.88 ms 15.74 MB
Table 1: Performance of InContext. For each sensitive
element, this table shows extra latency imposed on each click,
as well as extra memory used.
90x20 px
200x20 px
468x60 px
450x275 px
385x550 px
ing the host page from the embedded sensitive element)
needs to be thick enough so that a user can clearly de-
cide whether the pointer is on the sensitive element or
on its embedding page. As well, this ensures that during
rapid cursor movements, such as those in the whack-a-
mole attack (Section 4.3), our pointer integrity defenses
such as screen freezing are activated early enough. Sec-
tions 7.2 and 7.4 give a preliminary evaluation on some
padding thickness values. The padding could be either
enforced by the browser or implemented by the devel-
oper of the sensitive element; we have decided the latter
is more appropriate to keep developers in control of their
page layout.
5.3 Opt-in API
In our design, web sites must express which elements
are sensitive to the browser. There are two options for
the opt-in API: a JavaScript API and an HTTP response
header. The JavaScript API’s advantages include abil-
ity to detect client support for our defense as well as to
handle oninvalidclick events raised when clickjack-
ing is detected. On the other hand, the header approach
is simpler as it doesn’t require script modiﬁcations, and
it does not need to deal with attacks that disable script-
ing on the sensitive element [37]. We note that bitmap
comparison functions should not be directly exposed in
JavaScript (and can only be triggered by user-initiated
actions). Otherwise, they might be misused to probe pix-
els across origins using a transparent frame.
6 Prototype Implementation
We built a prototype of InContext using Internet Explorer
9’s public COM interfaces. We implemented the pixel
comparison between an OS screenshot and a sensitive
element rendered on a blank surface to detect element
visibility as described in Section 5.1.1, using the GDI
BitBlt function to take desktop screenshots and using
the MSHTML IHTMLElementRender interface to gen-
erate reference bitmaps.
To implement the UI delays, we reset the UI delay
timer whenever the top-level window is focused, and
whenever the computed position or size of the sensitive
element has changed. We check these conditions when-
ever the sensitive element is repainted, before the actual
9
paint event; we detect paint events using IE binary behav-
iors [27] with the IHTMLPainter::Draw API. We also
reset the UI delay timer whenever the sensitive element
becomes fully visible (e.g., when an element obscuring it
moves away) by using our visibility checking functions
above. When the user clicks on the sensitive element, In-
Context checks the elapsed time since the last event that
changed visual context.
Our prototype makes the granularity of sensitive ele-
ments to be HTML documents (this includes iframes);
alternately, one may consider enabling protection for
ﬁner-grained elements such as DIVs. For the opt-in
mechanism, we implemented the Javascript API of Sec-
tion 5.3 using the window.external feature of IE.
Although our implementation is IE and Windows-
speciﬁc, we believe these techniques should be feasible
in other browsers and as well. For example, most plat-
forms support a screenshot API, and we found an API
similar to IE’s IHTMLElementRender in Firefox to ren-
der reference bitmaps of an HTML element.
At this time, we did not implement the pointer in-
tegrity defenses, although we have evaluated their effects
in Section 7.
Performance. To prove that InContext is practical, we
evaluated our prototype on ﬁve real-world sensitive ele-
ments (see Table 1). For each element, we measured the
memory usage and click processing time for loading a
blank page that embeds each element in a freshly started
browser, with and without InContext, averaging over ten
runs. Our testing machine was equipped with Intel Xeon
CPU W3530 @ 2.80 GHz and 6 GB of RAM.
Without additional effort on code optimization, we
ﬁnd that our average click processing delay is only 30 ms
in the worst case. This delay is imposed only on clicks on
sensitive elements, and should be imperceptible to most
users. We ﬁnd that the majority (61%) of the click delay
is spent in the OS screenshot functions (averaging 11.65
ms). We believe these could be signiﬁcantly optimized,
but this is not our focus in this paper.
7 Experiments
7.1 Experimental design
In February of 2012 we posted a Human Interactive Task
(HIT) at Amazon’s Mechanical Turk to recruit prospec-
tive participants for our experiments. Participants were
offered 25 cents to “follow the on-screen instructions and
complete an interactive task” by visiting the web site at
which we hosted our experiments. Participants were told
the task would take roughly 60 seconds. Each task con-
sisted of a unique combination of a simulated attack and,
in some cases, a simulated defense. After each attack, we
asked a series of follow-up questions. We then disclosed
the existence of the attack and explained that since it was
simulated, it could only result in clicking on harmless
simulated functionality (e.g., a fake Like button).
We wanted participants to behave as they would if
lured to a third-party web site with which they were pre-
viously unfamiliar. We hosted our experiments at a web
site with a domain name unafﬁliated with our research
institution so as to ensure that participants’ trust (or dis-
trust) in our research institution would not cause them to
behave in a more (or less) trusting manner.
For attacks targeting Flash Player and access to video
cameras (webcams), we required that participants have
Flash Player installed in their browser and have a web-
cam attached. We used a SWF ﬁle to verify that Flash
Player was running and that a webcam was present. For
attacks loading popup windows, we required that partic-
ipants were not using IE or Opera browsers since our at-
tack pages were not optimized for them.
We recruited a total of 3521 participants.2 Partici-
pants were assigned uniformly and at random to one of
27 (between-subjects) treatment groups. There were 10
treatment groups for the cursor-spooﬁng attacks, 4 for
the double-click attacks, and 13 for the whack-a-mole at-
tacks. Recruiting for all treatments in parallel eliminated
any possible confounding temporal factors that might re-
sult if different groups were recruited or performed tasks
at different times. We present results for each of these
three sets of attacks separately.
In our analysis, we excluded data from 370 partici-
pants who we identiﬁed (by worker IDs) have previously
participated in this experiment or earlier versions of it.
We also discarded data from 1087 participants who were
assigned to treatment groups for whack-a-mole attacks
that targeted Facebook’s Like button but who could not
be conﬁrmed as being logged into Facebook (using the
technique described in [8]). In Tables 2, 3 and 4, we re-
port data collected from the remaining 2064 participants.
Except when stated otherwise, we use a two-tailed
Fisher’s Exact Test when testing whether differences be-
tween attack rates in different treatment groups are sig-
niﬁcant enough to indicate a difference in the general
population. This test is similar to χ2, but more conserva-
tive when comparing smaller sample sizes.
7.2 Cursor-spooﬁng attacks
In our ﬁrst experiment, we test the efﬁcacy of the cursor-
spooﬁng attack page, described in Section 4.1 and illus-
trated in Figure 1, and of the pointer integrity defenses
we proposed in Section 5.1.2. The results for each treat-
ment group make up the rows of Table 2. The columns
show the number of users that clicked on the “Skip ad”
link (Skip), quit the task with no pay (Quit), clicked on
2The ages of our participants were as follows: 18-24 years: 46%;
25-34 years: 38%; 35-44 years: 11%; 45-54 years: 3%; 55-64 years:
1%; 65 years and over: 0.5%. A previous study by Ross et al. provides
an analysis of the demographics of Mechanical Turk workers [31].
10
26
65
38
34
52
60
63
66
66
60
35
0
0
23
0
0
0
0
0
0
68
73
72
72
70
72
72
70
71
71
Total Timeout Skip Quit Attack Success
Treatment Group
4 (5%)
1. Base control
6 (8%)
2. Persuasion control
31 (43%)
3. Attack
12 (16%)
4. No cursor styles
5a. Freezing (M=0px)
11 (15%)
5b. Freezing (M=10px)
9 (12%)
5c. Freezing (M=20px)
3 (4%)
2 (2%)
6. Muting + 5c
2 (2%)
7. Lightbox + 5c
3 (4%)
8. Lightbox + 6
Table 2: Results of the cursor-spooﬁng attack. Our attack
tricked 43% of participants to click on a button that would
grant webcam access. Several of our proposed defenses re-
duced the rate of clicking to the level expected if no attack had
occurred.
3
2
3
3
7
3
6
2
3
8
webcam “Allow” button (Attack success), and those who
watched the ad full video and were forwarded to the end
of the task with no clicks (Timeout).
Control. We included a control group, Group 1, which
contained an operational skip button, a Flash webcam
access dialog, but no attack to trick the user into click-
ing the webcam access button while attempting to click
the skip button. We included this group to determine the
click rate that we would hope a defense could achieve
in countering an attack. We anticipated that some users
might click on the button to grant webcam access simply
out of curiosity. In fact, four did. We were surprised that
26 of the 68 participants waited until the full 60 seconds
of video completed, even though the “skip ad” button
was available and had not been tampered with. In future
studies, we may consider using a video that is longer,
more annoying, and that does not come from a charity
that users may feel guilty clicking through.
We added a second control, Group 2, in which we re-
moved the “skip ad” link and instructed participants to
click on the target “Allow” button to skip the video ad.
This control represents one attempt to persuade users to
grant access to the webcam without tricking them. As
with Group 1, we could consider a defense successful it
rendered attacks no more successful than using persua-
sion to convince users to allow access to the webcam.
Whereas 4 of 68 (5%) participants randomly as-
signed to the persuasion-free control treatment (Group
1) clicked on the “Allow” button, we observed that 6
of 73 (8%) participants assigned to the persuasion con-
trol treatment did so. However, the difference in the
attack success rates of Group 1 and Group 2 were not
signiﬁcant, with a two-tailed Fisher’s exact test yielding
p=0.7464.
Attack. Participants in Group 3 were exposed to the sim-
ulated cursor spooﬁng attack, with no defenses to protect
them. The attack succeeded against 31 of 72 participants
(43%). The difference in the attack success rates between
participants assigned to the non-persuasion control treat-
11
Figure 5: Cursor-spooﬁng attack with lightbox defenses.
The intensity of each pixel outside of the target element is dark-
ened and randomized when the actual pointer hovers on the
target element.
ment (Group 1) and the attack treatment (Group 3) is sta-
tistically signiﬁcant (p<0.0001). The attack might have
been even more successful had participants been given
a more compelling motivation to skip the “skip this ad”
link. Recall that only 51% of participants in the non-