	• Tmm ≈ 45 min (manual diagnosis + 15 minutes)• TL ≈ 2 hours 30 min (manual diagnosis + 2 hours) We will also try to consider the additional information that the prediction brings along with it about what type of failure is about to happen.
Finally, to determine the trustworthiness of the prediction, in order to make business and logistics decisions, we consider the Precision, Recall, and False Positive Rates (FPR) of the predictions (see II-B2).IV. ML METHODS FOR NETWORK FAULT PREDICTION In this section we describe the ML methods that were used in the field of NFP up to this point. We have chosen to consider only ML based methods, as from what we have seen, they are the more representative, and they more show promise for evolution and performance in the long run for NFP than other known statistical methods. We have summarised in Table 2 the different ML methods that have been used in the studies that we review.However, there are several works in Network Fault Predic-tion among which [18], [36], [61]–[65] which treat the NFPproblem without using ML methods and can show interesting results. One example is the case of Hood et al. [61], where they show show that it may be possible to predict network failure several hops away in space and around 10 minutes away in time (further testing and specification needed). A hop is a network distance metric defined by the minimal number of links a message has to cross to reach destination from source. Two Directly Connected (DC) routers are 1 hop away. Generally Fault predictions tasks in the PHM field are sep-arated into two stages, constructing a failure model and the prediction stage. However, when we use ML methods, both stages can be considered to be joined. Indeed, to construct the failure model you need to train the model by predicting failures, and while the model is in operation it is desirable to continue training the model in case new types of fault arise.A. LINEAR MODELS 
Two tools taken from statistics, Linear Regression [33] and Logistic Regression [35]. Linear Regression is equivalent to fitting a straight line through the data, and Logistic Re-gression is equivalent to fitting a straight line, as a decision boundary, to separate two classes of data.
	Both of these models are very simple and quick to execute, and the result is readily explainable.However, they can only separate linearly separable data and usually do not work well when the data is complex. 	Learning Vector Quantization (LVQ) Learning Ma-chine [37] is a classification algorithm similar to the k-Nearest Neighbor (kNN) algorithm. kNN saves all the train-ing data as examples and counts the class representation of the k (odd number) examples that are closest in vector-space to the sample in order to predict its class. LVQ Learning Ma-chine operates in a similar way but only measures proximity to a selected few (compared to dataset size) codebook vectors to take the classification decision. It is a very quick, simple, and explainable model, and its capacity increases with the number of codebook vectors selected. However it has a low capacity for non-linear problems.B. PROJECTION BASED ALGORITHMS 
Support Vector Machines (SVM) [40]–[42], [66], also called Vast Margin Separators, are a type of supervised learning model that functions under the following manner. The data is first projected through a kernel function into another space (of possibly different dimension). The kernel function can be chosen to best fit the data, or separate the data, if there is a priori knowledge of the structure of the data and of the pertinence of elements to the result that is sought after. Then a hyperplan of the new space is selected so that it separates the data with the widest margin possible.This type of model works well with very high dimension data. It can also separate both linearly and non-linearly separable data if the adequate kernel function is chosen and kernels can be made explicitly to fit the data, with some knowledge of the data structure, and therefore reach very high performance (requires a high level of expertise). It is
6 	VOLUME 4, 20226 	VOLUME 4, 2022
Murphy et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS
FIGURE 3: Two possible timelines for failure prediction response. In the case of manual preventive mitigation, it is unknown whether the mitigation will have an impact on the emergence of the failure, therefore it may not be necessary to engage logistics.also less prone to overfitting than most types of models as generalization is built-in to the model (through seeking a large margin).
However, it usually does not converge or breeds poor results if the kernel function does not fit the data, and it is possible that no simple kernel function will fit the data. Because of that, with no a priori knowledge of the data structure, after trying the few wide spread kernel functions (linear, RBF, polynomial, sigmoid) it can be advised to try ML methods other than SVM.Hessian Locally Linear Embeddings (HLLE) [44] is a dimension reduction projection aiming to preserve distance between points locally. This dimension reduction operation aids in reducing complexity of problems, however, in some cases, this step is unnecessary.Least Absolute Shrinkage and Selection Operator (LASSO) as predictor [45] is a regression algorithm. This model performs variable selection (based on covariance) and regularization, before performing prediction. This model provides explainability by dimension reduction, and greater prediction performance than simple regression. However it does not perform well in very high dimension environments.C. DECISION TREE-BASED ALGORITHMSAll of the following methods are based on Decision Trees. 	Decision Tree (DT) algorithm [48] is a succession of (if, else) statements, feature per feature arranged in the form of a tree. Each time, the feature is selected by choosing the one allowing maximum entropy gain by a simple threshold separation of the data along this feature. The selected feature and the threshold separation are included in the DT before repeating the step to continue forming the DT. This method yields very explainable results. Each DT has contained within the information of each split along with its entropy levels and the quantity of data of each class remaining. This information can be used to generate graphical aids. However, as the split made is dependent on the data used, DTs usually have high variance in decisions depending on data and so a poor generalization capacity. Also when the data is complex, DTstend to have poor performance. To counter these weaknesses and improve performance, other algorithms have been built by aggregating DTs in certain manners.Random Forest (RF) [47] is an ensemble method on De-cision Trees trained by bagging, or Bootstrap Aggregating. Each DT is trained on a different subset of the data, and the final prediction for each input sample is the average or most voted decision out of all DTs trained. In the RF algorithm, the different DTs are also trained on different subsamples of the features so that each DT is more independent. This model has the advantage of working well without much pretreatment and is moderately explainable, but it does not work well on non-linearly separable data, and setting the forest and tree sizes requires a lot of tweaking.AdaBoost [50] is another ensemble method on Decision Trees trained by boosting and adapting voting rights to am-plify the importance of stronger weak learners. Boosting is the process in which you give more importance to training samples where the previous model was wrong so that the new weak learners that you train are better trained for these types of samples. It is known to increase performance when having many weak classifiers (DTs), however the algorithm needs these weak classifiers to have less than 50% error and be as independent as possible.XGBoost [51], another ensemble method for Decision Trees trained by bagging, boosting, adapting voting rights. There is the added step of pruning to remove the weaker branches DTs and weaker weak learners (DTs). It is known to have great performance, very high adaptability, but needs cor-rect calibration of hyperparameters which can be a lengthy process.Reduced Error Pruning-Tree (REP-Tree) [52] is a prun-ing method for DTs. It learns to split tree leaves by reducing prediction error step by step on a subsample of the training set. Then it applies a pruning mechanism where another subsample of the training set is used to evaluate the splits and the branches. Poorly generalisable branches and splits are removed. It is used to reduce the high variance problem of DTs.VOLUME 4, 2022 	7
Murphy et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS
| Model type | Model name | Type | Examples in NFP | Litt. | Description | Advantages | Disadvantages |
|---|---|---|---|---|---|---|---|
| Linear Models |Linear Regression |Regression |[31], [32] |[33] |Fit a line through the data |Simple, quick |Linear, doesn’t fit complex data well || Linear Models |Logistic Regression |Classification |[34] |[35] |Place a decision threshold based on a fitted line through the data |Simple, Quick |Linear, doesn’t fit complex data well |
| Linear Models |LVQ Learning Machine |Classification |[36] |[37] |Similar to k-Nearest Neighbor algorithm, measures proximity to selected few  (compared to dataset size) codebook  vectors to take classification decision |Less complexity than k-NN, simple, explainable |Low capacity for non-linear problems || Projection based  algorithms |SVM |Classification/ Regression |[31], [32], [38], [34], [39] |[40]–[42] |Project data in other space through kernel function then choose largest margin  separator threshold |Quick, can fit both linear and non-linear |Doesn’t converge if kernel isn’t good enough || Projection based  algorithms |Hessian Locally  Linear Embeddings (HLLE) |Dimension reduction |[43] |[44] |Dimension reduction projection aiming to preserve distance between points locally |Dimension reduction aids in  reducing complexity of problems |Sometimes not useful || Projection based  algorithms |LASSO as  predictor |Regression |[31] |[45] |Model that performs variable selection (based on covariance) and regularization |Explainability by dimension reduction, greater performance than simple regression |Doesn’t perform in very high dimension  environments || Tree- based |Random Forest |Classification/ Regression |[39], [46] |[47] |Collection of Decision Trees trained by bagging on different subsamples of the features |Moderately Explainable, works well without much pretreatment |Doesn’t work well on  non-linearly separable data, setting forest and tree size || Tree- based |Decision Tree |Classification/ Regression |[39] |[48] |Collection of (if, else) on a feature  statements arranged in the form of a tree, feature decided by maximum information gain by split |Very explainable, entropy information included |High variance in decisions depending on data || Tree- based |AdaBoost |Classification/ Regression |[49] |[50] |Collection of Decision Trees trained by bagging, and boosting and adapting voting rights to amplify important of stronger weak learners |Known to increase performance of many weak classifiers (DTs) |Need weak classifiers to have error<50% || Tree- based |XGBoost |Classification/ Regression |[39] |[51] |Collection of Decision Trees trained by bagging, boosting, adapting voting rights, and pruning to remove the weaker branches and weaker weak learners |Known to have great performance, very high adaptability |Needs correct calibration of hyperparameters || Tree- based |Reduced Error Pruning- Tree |Classification/ Regression |[31] |[52] |Method for pruning DTs by using other subset of dataset to take the pruning decision |Reduces dataset subset variance influence on pruning | |
| Tree- based |M5P |Regression |[31] |[53] |Use of DTs as a regressor, learns to split tree leaves by reducing error of regression step by step |Explainability, equations included, easy to implement |Low capacity for non-linear problems || NN |Multi-Layer Perceptron |Classification/ Regression |[29], [32], [36], [39], [54] |[55] |Collection of artificial neurons, with one input layer, one output layer, and any low number (otherwise Deep Learning) of hidden layers |Can achieve great performance, network problem capacity  increases with size |Poor explainability, quite susceptible to the  bias/variance problem || NN |Auto Encoder |Dimension reduction |[32] |[56] |Deep Learning NN architecture in the shape of an hourglass. Designed for dimension reduction while conserving information. |Has been shown to generate great dimension reduced data  representations in many examples |Resource consuming || NN |Recurrent NN (LSTM) |Classification/ Regression |[57] |[58] |NN model designed to take sequences of input and propagating the context through the treatment of the sequence. |Improvement of classical RNNs (vanishing gradient problem), usually good performance  compared to simple feedforward techniques |Resource consuming || NN |Generative  Adversarial  Networks (GAN) |Classification/ Regression |[59] |[60] |Double network used to generate additional coherent data. One network is trained to create false examples resembling real ones. Other is trained to discriminate |Great for coherent data generation |Resource consuming |
TABLE 2: Description of ML methods used in the field of NFPM5P [53] uses DTs as a regressor. This algorithm learns to split tree leaves by reducing regression error step by step on the training set. A step of pruning is set after that to remove branches that contribute most to regression error. This algorithm has high explainability, as the equations are included, and it is easy to implement. However it has low capacity for non-linear problems.D. NEURAL NETWORKS (NN)
Neural Networks [67] using Backpropagation are the most famous branch of ML currently. Similar to DT-based algo-rithms, they are based on a nuclear function, the artificial neuron, that is arranged into a structure. The function of the neuron is simply a weighted sum of all inputs (a bias input is added) passed through an activation function. There are several commonly used activation functions, and when the output of the activation function passes a threshold, the neuron is excited, otherwise inhibited.In Multi Layer Perceptrons (MLP) [55], neurons are stacked in several hidden layers between an input layer and an output layer. This type of model can achieve great perfor-mance, as the networks problem capacity increases with the number and size of hidden layers. It is also highly versatile in that shapes can be completely customized. However as a con-sequence, it has very poor explainability, though there have been works in this field recently, and it is quite susceptible to the bias-variance problem.Auto Encoders [68] are a class of Deep Learning (DL) designed for dimension reduction. Its architecture is in the shape of an hourglass and the expected output for training is the same data as the input. Therefore the model learns how to generate a decent dimension reduction from the input to the middle hidden layer, and learns to reconstruct with the lowest error possible the input from the middle hidden layer. It has been shown that using the dimension reduced/encoded inputs for other problems can increase performance. However, as8 	VOLUME 4, 2022
Murphy et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS
it belongs to the class of DL, it is known to be resource consuming.Recurrent Neural Networks (RNN), and in this partic-ular case, Long Short Term Memory (LSTM) [58] are a type of NNs. In RNN, the model is fed a sequence of inputs (can be thought of as another dimension to the data, such as time) and it is engineered to use the information from previous samples of the same sequence in addition to current input to generate an output. LSTM are considered an improvement of classical RNNs where the vanishing gradient problem is addressed (though not completely solved) by incorporating an additive operation into the model. This type of model usually has good performance compared to simple feedforward techniques and has the capacity to tackle issues hard for simple feedforward techniques. However it is also very resource consuming.Generative Adversarial Networks (GAN) [60] are a type of architecture used for coherent data generation (fake gen-erator) and for false data detection. It is done by employing two NNs. One network is trained to create false examples resembling real ones, and the other is trained to discriminate between real examples and examples generated by the other model. GANs have a lot of potential uses and could generate very useful coherent data but the training process is quite resource consuming.V. APPLICATIONS OF NFP TO NETWORKS 
In this section we describe the different studies that have been realized in NFP, classified by the application of the prediction. We have regrouped in Table 3 these different studies per application.
A. EQUIPMENT HEALTH
An equipment health prediction is the prediction of the health state of the equipment in time X, for duration Z, or for the interval :[t + X; t + X + Z]
This falls into the category of classification problems. For reference, in all the papers mentioned, healthy is negative and faulty is positive.
Lu et al. [43] work on the prediction of network failures in a controlled Local Area Network (LAN) environment. They recognize that online (meaning in real-time) failure prediction has the drawback that it needs a lot of specialized knowledge in order to extract and adequately format features necessary for prediction.In order to address this issue, they propose an autonomous system for feature selection. They aggregate six sources of data and use Hessian Locally Linear Embeddings (HLLE), then Supervised HLLE (SHLLE) - that uses class information to better separate data in the embedding space - to select the appropriate features for optimal prediction performance. 	They use their data to predict Network, CPU, and Memory failures coming from faults they inject into the network. We may note that injecting the failure might not reproduce thereality of the network state in a naturally occurring fault, and so the network state might be missing or showing elements that make the prediction easier or harder.
They then predict the occurrence of the failure before it happens. They observe that injecting faults typically results in a failure occurring in about 110 seconds.Although they do not show all the results they have, using K = 20 parameter for SHLLE, they achieve 72.3% Precision and 70.1% Recall on their test data (with a false positive rate of 11.6%). Prediction performance varies depending on the type of failure, as the study declares more than 60% recall on CPU and memory related failures, and around 70% recall for network failures. Coupled with the average of around 110 seconds in RTTF from injection of the faults, those results show that it is possible to implement automatic response mechanisms to the predicted failures. However in the case of the need of equipment replacement or manual intervention, it seems that these predictions would not be actionable, as they do not grant enough time for action to be taken before failure. They conclude that fault prediction may, in turn, replace root cause analysis and root cause detection. We may add that predicting the root cause in addition to the present predictions may be a key in implementing early response, especially when human intervention is required, such as for when equipment replacement is inevitable.Wang et al. [38] work on equipment fault prediction in an optical Wavelength Division Multiplexing (WDM) mesh network. They identify that while passive protection for opti-cal networks exists, when a network fault occurs, data is still lost while waiting for the network protection and the recovery mechanisms of the network to kick in. Therefore, using prediction methods, we could enable proactive protection methods, not losing any data at the time of failure.They propose a model based on five variables of the phys-ical layer of the equipment, and then create three features based on each variable, namely, the minimum, average and maximum values of the day. Every day, the values of these features for the next day are predicted using Double Expo-nential Smoothing (DES). The resulting predicted vector is then fed into a Support Vector Machine (SVM) classifier [66] which predicts whether, the next day, the equipment will be unavailable for more than 40,000 seconds (11 hours) or not. The paper shows very promising results in these conditions, being able to boast a 91% prediction accuracy on a 24 hour advance time prediction, however the definition of failure as 11 hours of unusable time is quite a large one. The prediction metric given is accuracy, and the data is class balanced, so the accuracy should be somewhat representative of both classes (as visible in section II-B2 with the formula for accuracy, in cases where class imbalance is present, accuracy is not a good performance metric). Additionally, the information provided is relative to each piece of equipment and with the 24h time frame, the predictions are very easily actionable. However, while the low definition of the failure hypothesis and the 24 hour window probably make the problem much easier to solve, it limits the utility of this method for larger and moreVOLUME 4, 2022 	9
Murphy et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS
| Applications | Description | NFP examples |
|---|---|---|
| Equipment Health |Predict failure in network equipment in the future |[38], [39], [46], [54], [57] |
| RTTF |Predict Remaining Time to Failure on network equipment |[31] |
| Network Health |Predict Network Health (global state) |[29], [32], [36] || Link Failure |Predict whether link between two network equipments will go down |[34] |
| Alarm Prediction |Predict whether or not certain network states will lead to an alarm being raised (by an automated system or not) |[49], [59] |
TABLE 3: Description of the different applications of NFP to networks
heterogeneous networks as further diagnostics need to be run to find and solve the root cause of the failure.This paper also showcases the use of an alternative method of predicting future Key Performance Indicators (KPIs), and then detecting the fault in a second step. This method could be used to replicate some existing ML-based Fault Detection methods into a prediction environment with minimal adapta-tion work.C. Zhang et al. [57] also work on an optical network and consider the same five physical variables as Wang et al. [38]. They propose the method of using an LSTM model to replace the two-step method of DES-SVM into one step.