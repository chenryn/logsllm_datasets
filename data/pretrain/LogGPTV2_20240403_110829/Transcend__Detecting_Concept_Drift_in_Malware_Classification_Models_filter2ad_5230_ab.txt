USENIX Association
26th USENIX Security Symposium    627
0.00.20.40.60.81.0Threshold benign0.00.20.40.60.81.0Threshold maliciousElement kept by p-values0.00.10.20.30.40.50.60.70.80.91.00.00.20.40.60.81.0Threshold benign0.00.20.40.60.81.0Threshold maliciousElement kept by probability0.00.10.20.30.40.50.60.70.80.91.00.00.20.40.60.81.0Threshold benign0.00.20.40.60.81.0Threshold maliciousPerformance by p-value0.00.10.20.30.40.50.60.70.80.91.00.00.20.40.60.81.0Threshold benign0.00.20.40.60.81.0Threshold maliciousPerformance by probability0.00.10.20.30.40.50.60.70.80.91.00.00.20.40.60.81.0Threshold benign0.00.20.40.60.81.0Threshold maliciousElement discarded by p-value0.00.10.20.30.40.50.60.70.80.91.00.00.20.40.60.81.0Threshold benign0.00.20.40.60.81.0Threshold maliciousElement discarded by probability0.00.10.20.30.40.50.60.70.80.91.00.00.20.40.60.81.0Threshold benign0.00.20.40.60.81.0Threshold maliciousPerformance by p-value0.00.10.20.30.40.50.60.70.80.91.00.00.20.40.60.81.0Threshold benign0.00.20.40.60.81.0Threshold maliciousPerformance by probability0.00.10.20.30.40.50.60.70.80.91.02.3 P-values vs. Probabilities
2.4 Statistical Decision Assessment
z∗,r2
z∗, p2
One might question the utility of p-value over probability
of a test object belonging to a particular class. Probabil-
ities are computed by most learning algorithms as quali-
tative feedback for a decision. SVM uses Platt’s scaling
to compute probability scores of an object belonging to
a class while a random forest averages the decision of
individual trees to reach a ﬁnal prediction [3].
In this
section, we discuss the shortcomings of using probabili-
ties for decision assessment as shown in §4.1.1 and §4.2.
Additionally, we also provide empirical evidence in favor
of p-values as a building block for decision assessment.
P-values offer a signiﬁcant advantage over probabil-
ities when used for decision assessment. Let us as-
z∗ ··· pk
sume that the test object z∗ has p-values of p1
z∗
z∗ ···rk
and probability of r1
z∗ of belonging to classes
l1,l2···lk (which is the set of all classes in L). In the
case of probabilities, Σiri
z∗ must sum to 1.0. Now, lets
consider a 2-class problem. If z∗ does not belong to either
of the classes, and the algorithm computes a low proba-
bility score r1
z∗ would artiﬁcially tend to
1.0. In other words, if we use probabilities for decision
assessment it is likely that we might reach an incorrect
conclusion for previously unseen samples. P-values on
the other hand are not constrained by such limitations.
It is possible for both p1
z∗ to be a low value for
the case of a previously unseen sample. This is true also
when p-values are built using probability as NCM. To
calculate the probability of a test sample, only informa-
tion belonging to the test samples are used (e.g., distance
to the hyperplane in the case SVM or ratio of decisions
for one class in the case of random forest). Instead, a
p-value is computed comparing the scores of all the sam-
ples in a class (see equation 1 and 2).
z∗ ∼ 0.0, then r2
z∗ and p2
We further elaborate on this by training an SVM clas-
siﬁer with Android malware objects from the Drebin
dataset [2] and by testing it using objects from a drifted
dataset (the Marvin dataset [14], see § 4 for details).
Then, we apply a threshold to accept the decision of the
classiﬁer only if a certain level of certainty is achieved.
Figure 1 shows the average of F1-score for malicious and
benign classes after the application of the threshold for
the objects that fall above (Figure 1b) and below it (Fig-
ure 1d). Figure 1 also shows the ratio of objects retained
(Figure 1a) and rejected (Figure 1c). Figure 1b shows
that the use of p-values produces better performance as it
identiﬁes more objects to reject than probabilities (Fig-
ure 1a). Here, ﬁltering out a high number of objects
is correct as they are drifting from the trained model.
Keeping them would degrade the performance of the al-
gorithm (Figures 1c and 1d). The threshold is applied
to the testing objects; we present case studies in § 4.1,
which show how to derive it from the training dataset.
This section introduces and discusses CE metrics used
to assess the classiﬁcation decisions. The techniques for
interpreting these metrics are discussed in § 3.
Algorithm Credibility. The ﬁrst evaluation metric for
assessing classiﬁcation decision on a test object is algo-
rithm credibility. ACred(z∗) is deﬁned as the p-value for
the test object z∗ corresponding to the label chosen by the
algorithm under analysis. As discussed, the p-value mea-
sures the fraction of objects within K , that are at least as
different from the set of objects C as the new object z∗.
A high credibility value means that z∗ is very similar to
the objects in the class chosen by the classiﬁer. Although
credibility is a useful measure of classiﬁcation quality, it
only tells a partial story. There may potentially be high
p-values for multiple labels indicating multiple matching
labels for the test object which the classiﬁcation algo-
rithm has ignored. On the other hand, a low credibility
value is an indicator of either z∗ being very different from
the objects in the class chosen by the classiﬁer or the
object being poorly identiﬁed. These two observations
show that credibility alone is not sufﬁcient for reliable
decision assessment. Hence, we introduce another mea-
sure to gauge the non-performance of the classiﬁcation
algorithm—algorithm conﬁdence.
Algorithm Conﬁdence. For a given choice (e.g., assign-
ing z to a class li), conﬁdence tells how certain or how
committed the evaluated algorithm is to the choice. For-
mally, it measures how distinguishable is the new ob-
ject z∗ ∈ li from other classes l j with j (cid:54)= i. We deﬁne
the algorithm conﬁdence as 1.0 minus the maximum p-
value among all p-values except the p-value chosen by
the algorithm (i.e., algorithm credibility): ACon f (z∗) =
1− max(P(z∗)\ ACred(z∗)) where, P(z∗) = {pli
z∗ : li ∈ L}
P(z∗) is the set of p-values associated to the possible
choices for the new object z∗. The highest value of con-
ﬁdence is reached when the algorithm credibility is the
highest p-value.
It may happen that the choice made
by the algorithm is not attached to the highest p-value,
suggesting that the conﬁdence is sub-optimal. Results in
§ 4 show that this provides valuable insights, especially
when the method under assessment makes choices with
low values of conﬁdence and credibility. Low algorithm
conﬁdence indicates that the given object is similar to
other classes as well. Depending on the algorithm cred-
ibility, this indication may imply that the decision algo-
rithm is not able to uniquely identify the classes or, that
the new object looks similar to two or more classes.
Finally, we note that algorithm conﬁdence and credi-
bility are not biased by the number of classes in a dataset
as popular measures, such as precision and recall [13].
Thus CE’s ﬁndings are more robust to dataset changes.
628    26th USENIX Security Symposium
USENIX Association
3 Framework Description
Previous section introduced conformal evaluation along
with the two metrics that we use for decision assess-
ment: algorithm conﬁdence and algorithm credibility.
Transcend uses two techniques to evaluate the quality
of an algorithm employed on a given dataset: (i) De-
cision assessment—evaluates the robustness of the pre-
dictions made by the algorithm; and (ii) Alpha assess-
ment—evaluates the quality of the non-conformity mea-
sure. We combine these assessments to enable the detec-
tion of concept drift (§3.3).
3.1 Decision Assessment
Conformal evaluator qualitatively assesses an algo-
rithm’s decision by assigning a class l ∈ L as predicted
by the algorithm to each new object z∗ and computing its
algorithm credibility and conﬁdence.
Hence, four possible scenarios unfold: (i) High algo-
rithm conﬁdence, high algorithm credibility—the best
situation, the algorithm is able to correctly identify a
sample towards one class and one class only. (ii) High
algorithm conﬁdence, low algorithm credibility—the al-
gorithm is not able to correctly associate the sample to
(iii) Low al-
any of the classes present in the dataset.
gorithm conﬁdence, low algorithm credibility—the algo-
rithm gives a label to the sample but it seems to be more
similar to another label. (iv) Low algorithm conﬁdence,
high algorithm credibility—according to the algorithm, it
seems that the sample is similar to two or more classes.
The measures are then grouped into two sets —correct
or wrong— which represents values for correctly and
wrongly classiﬁed objects. Subsequently, values are av-
eraged and their standard deviation is also computed, this
is done for every class l ∈ L, to study whether the algo-
rithm works consistently for all classes or if there are dif-
ﬁcult classes that the algorithm has trouble dealing with.
This assessment, performed during the design phase of
the algorithm, helps us to decide the cutoff threshold for
a deployed scenario to separate the samples with enough
statistical evidence of correctness.
Comparing the results obtained for correct and wrong
choices produces interesting results. For correct choices
it would be desirable to have high credibility and conﬁ-
dence. Conversely, for wrong choices it would be desir-
able to have low credibility and high conﬁdence. The di-
vergence from these scenarios helps understand whether
the algorithm takes strong decisions, meaning that there
is a strong statistical evidence to conﬁrm its decisions,
or, in contrast, if the decisions taken are easily modiﬁed
with a minimal modiﬁcation of the underlying data.
By looking at the outcome of decision assessment, it
is possible to understand whether the choices made by an
algorithm are supported with statistical evidence. Other-
wise, it is possible to get an indication where to look for
possible errors or improvements, i.e., which classes are
troublesome, and whether further analysis is needed, e.g.
by resorting to the alpha assessment.
3.2 Alpha Assessment
In addition to the decision assessment, which evaluates
the output of a similarity-based classiﬁcation/clustering
algorithm, another important step in understanding the
inner workings and subtleties of the algorithm includes
analyzing the data distribution of the algorithm under
evaluation. Owing mainly to practical reasons, malware
similarity-based algorithms are developed around a spe-
ciﬁc dataset. Hence there is often the possibility of the
algorithm to over-ﬁt its predictions to the dataset. Over-
ﬁtting results in poor performance when the algorithm
analyses new or unknown datasets [13]. Despite em-
ploying techniques to avoid over-ﬁtting, the best way to
answer this question is to try the algorithm against as
many datasets as possible. We show that conformal eval-
uator can help solve this problem, when no more than
one dataset is available.
The alpha assessment analysis takes into account how
appropriate is the similarity-based algorithm when ap-
plied to a dataset. It can detect if the ﬁnal algorithm re-
sults still suffer from over-ﬁtting issues despite the ef-
forts of minimizing it using common and well known
techniques (e.g., cross validation).
Furthermore, the assessment enables us to get insights
on classes (e.g., malware families), highlighting how
the similarity-based method works against them. Re-
searchers may gather new insights on the peculiarities of
each class, which may eventually help to improve feature
engineering and the algorithm’s performance, overall.
First, for each object z j ∈ D, where l j is z j’s true class,
we compute its p-values against every possible l ∈ L. We
then plot the boxplot [10], containing the p-values for
each decision. By aligning these boxplots and grouping
them by class/cluster, we can see how much an element
of class/cluster j resembles that of another one, allowing
for reasoning about the similarity-based algorithm itself.
In § 4 we present case studies where we statistically
evaluate the quality behind performances of algorithms
within the conformal evaluator framework.
3.3 Concept Drift
We now describe the core of Transcend’s concept drift
detection and object ﬁltering mechanism.
It must be
stressed here that we look at concept drift from the per-
spective of a malware analysis team. Consequently, the
USENIX Association
26th USENIX Security Symposium    629
Transcend: Detecting Concept Drift in  
Malware Classification Models
Roberto Jordaney, Royal Holloway, University of London; Kumar Sharad, NEC Laboratories 
Europe; Santanu K. Dash, University College London; Zhi Wang, Nankai University;  
Davide Papini, Elettronica S.p.A.; Ilia Nouretdinov and Lorenzo Cavallaro,  
Royal Holloway, University of London
https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/jordaney
This paper is included in the Proceedings of the 26th USENIX Security SymposiumAugust 16–18, 2017 • Vancouver, BC, CanadaISBN 978-1-931971-40-9Open access to the Proceedings of the 26th USENIX Security Symposium is sponsored by USENIXseverity of the drift is a subjective issue. For critical ap-
plications, even a few misclassiﬁcations can cause major
issues. Consequently, the malware analysis team would
have a high standard for abandoning an aging classiﬁ-
cation model. Therefore, we make the concept drift de-
tection in Transcend parametric in two dimensions: the
desired performance level (ω) and the proportion of sam-
ples in an epoch that the malware analysis team is willing
to manually investigate (δ). The analyst selects ω and δ
as degrees of freedom and Transcend will detect the cor-
responding concept drift point constrained by the chosen
parameters. The goal is to ﬁnd thresholds that best sep-
arate the correct decisions from the incorrect ones based
on the quality metrics introduced by our analysis. These
thresholds are computed on the training dataset but are
enforced on predictions during deployment (for which
we do not have labels). The rationale is very simple: pre-
dictions with p-values above such thresholds would iden-
tify objects that likely ﬁt (from a statistical perspective)
in the model; such classiﬁcations should be trusted. Con-
versely, objects out of predictions with p-values smaller
than such thresholds should not be trusted as there is lack
of statistical evidence to support their ﬁt in the model.
What happens to untrustworthy predictions (and re-
lated test—likely drifted—objects) is out of the scope
of this work. It is reasonable to envision a pipeline that
would label drifted objects to retrain the machine learn-
ing model, eventually. While this raises several chal-
lenges (e.g., how many objects need to be labeled, how
much resources can be invested in the process), we would
like to remark the fact that is only possible once con-
cept drift is detected: the goal of this research. Not only,
Transcend plays a fundamental role in the identiﬁcation
of drifting objects and thus in the understanding of when
a prediction should be trusted or not, but its metrics can