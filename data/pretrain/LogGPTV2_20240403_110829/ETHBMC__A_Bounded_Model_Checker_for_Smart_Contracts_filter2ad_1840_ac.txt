imation. We want to stress that this is a common approach
and is necessary to combat state explosion [9]. Nonetheless,
these design choices can obviously lead to false positives.
Recognizing these problems, both MAIAN and teEther use
private chains to simulate their bug ﬁndings in a controlled en-
vironment. None of the other approaches makes any attempt
at pruning potential false positives.
Our Solution: We follow previous work and simulate each
potential bug as a concrete ofﬂine execution to weed out false
positives. We will discuss details in Section 5.3.
4 Modelling Ethereum
In the following, we provide an overview of the theoretical
model underpinning ETHBMC. We start with an overview of
attack vectors and a general introduction, move on to our en-
vironmental modelling, cover our memcopy-supporting mem-
ory model extensively, and ﬁnally describe our handling of
call and keccak instructions.
4.1 Attacker Model
ETHBMC provides a symbolic, multi-account capable rep-
resentation of the Ethereum ecosystem which can be used
to check arbitrary models. To demonstrate its capabilities,
we model three speciﬁc attack vectors which we deem most
critical: First, an attacker who wants to extract Ether from the
analyzed contract. Second, an attacker who wants to redirect
the control ﬂow of the analyzed contract to her own account.
Third, an attacker who wants to selfdestruct the analyzed con-
tract. Note that we only require our attacker to be able to
participate in the Ethereum protocol, giving her a live view of
the network and the blockchain, including storage and byte-
code level access to contracts (i.e., access to the world state).
4.2 High-level Overview
We want to reason about smart contracts as precisely as possi-
ble. This involves an accurate model of the EVM including
multiple contracts interacting. However, as all static analyzers
do, we have to make choices what to model precisely and
what to overapproximate. We decided to neither model the
consensus protocol, as well as gas usage. Invalid transactions
are guaranteed to not be executed, thus do not inﬂuence smart
contract state. Moreover, the code we want to analyze has to
be executable in practice, i.e., it must have reasonable gas con-
sumption. Note that a series of gas-related issues exists [23],
we leave the extension to a gas framework for future work.
We model the EVM as an Abstract State Machine (ASM) Γ,
giving us an execution context in which we can reason about
a contract’s execution. The ASM Γ takes the bytecode array Σ
as input, i.e., the contract code, and starts execution at the ﬁrst
instruction. When ﬁnishing execution, the machine returns
the set of all halting states σh, i.e., the set of all states, where
2762    29th USENIX Security Symposium
USENIX Association
Γ reached a non-exceptional halting condition, as deﬁned by
the yellow paper [64].
Additionally, we deﬁne a state σ = (µ, pc, Π), where µ
represents the stack, pc is the program counter pointing to the
next instruction, and Π is the set of path constraints for the
given path. We also deﬁne µ[0] to be ﬁrst (topmost) argument
of the stack, µ[1] the second, and so forth.
4.3 Modelling the Environment
We want to caputre a rich environment model (i.e., the world
state) in our executor. Thus, we deﬁne an account state to be
the tuple α = (balance, code, storage). Where balance is a
symbolic value, representing the balance of an account. code
is the (optional) code belonging to an account, and storage
is a 256-bit to 256-bit key-value-store, holding the persistent
account state (also optional).
Additionally, we deﬁne a transaction (tx) to be the tuple
(origin, recipient, callvalue, calldata, calldatasize). origin be-
ing the origin account address of the transaction, recipient
being the recipient, callvalue being the value attached to the
transaction, calldata being the (optional) calldata attached to
the transaction, and, calldatasize being a symbolic variable
representing the size of the calldata array (again optional).
We expand the deﬁnition of Γ by adding a mapping
accounts : address → α mapping account addresses to their
respective states, i.e., the world state. Additionally, we intro-
duce the set transactions which represents all transactions
issued in the system. When analyzing a speciﬁc contract, we
set up an attacker account and (possibly multiple) victim ac-
count(s). We then simulate a chain of transactions t1,t2, . . . ,tk.
For each transaction, we execute an entire run of Γ yielding
the halting states σh. Then, for each σi ∈ σh, we fork execu-
tion and proceed with the next t j up to tk.
4.4 Memory Model
Our memory model is based on the work of Sinz et al. [55].
It models memory as a series of updates called the memory
modiﬁcation graph. We extend this graph notation to accom-
modate for the EVM characteristics, such as multiple memory
regions.
4.4.1 Memory Graph
The graph itself is used to keep track of all modiﬁcations to
memory. It starts with an initial node and gets updated at
every read/write to memory. More formally, we introduce
the memory graph ∆ = (V,E) which holds all memory nodes
and add it to our state deﬁnition, i.e., σ = (µ, pc, Π, ∆). We
assign every node a unique index. Thus, we denote the node
with index i as ni. Additionally, we assign every node a label,
either init or a memory altering operation (write, copy, or
set), to keep track of which operation created the node.
For now, we only consider one memory region, e.g., stor-
age. We start from an initial node s0, updating the graph every
time we encounter a write to memory (e.g., SSTORE) by cre-
ating a new node st connected to its parent’s node su. This
gives us a unique memory image at any state during execution
(akin to static single assignment (SSA) form known from
compiler theory). When translating the memory layout to
constraints, we start from the newest node st, traversing the
graph in a backwards fashion collecting all memory updates,
and encoding them as logical formulas based on their respec-
tive label [17, 21]. This approach enables us to reason about
symbolic memory operations.
When considering multiple memory regions, e.g., execu-
tion memory and calldata, we introduce an initial node for
each region, i.e., the graph starts as a forest, and remains as
such as long as memory operations only operate on single
memory regions. It can get connected in two different ways.
First, indirectly by loading from one region and storing to
the other, linking two parts of the graph implicitly through
a constraint. Second, directly by a memcopy style operation
(e.g., CALLDATACOPY), linking two parts of the graph explic-
itly through a node and edges. Loading and storing introduce
constraints in the system, linking the memory regions when
they are translated to ﬁrst-order-logic. Copying introduces a
new node in the destination tree of the forest (e.g., the execu-
tion memory for CALLDATACOPY). This node gets connected
to both source and destination regions of memory, explicitly
linking the two parts of the graph.
4.4.2 General Memory Operations
During execution, for every account, we create a new stor-
age memory node n j and store the corresponding index in its
account state α.storage ← n j. In the same vein, every transac-
tion creates a new calldata and stores its identiﬁer. Addition-
ally, we assign our ASM Γ an execution memory Γ.m ← nk.
We deﬁne reading and writing to memory as follows:
• ∆.write(ni, p,v) (cid:55)→ n j: Writes the value v to the address
p with the memory node ni as parent node, returning the
new node n j.
• ∆.read(ni, p) (cid:55)→ v: Reads the value v from memory ni at
position p.
This makes modelling the SSTORE instruction straightforward:
α.storage ← ∆.write(α.storage, µ[0], µ[1])
In this example, we write to the current storage of the account
(α.storage) the value µ[1] to the address µ[0]. This is repre-
sented by creating a new memory node n j in the memory
store ∆. We then assign the index of this new memory node
to be the current account storage α.storage.
Modelling other memory operations is more difﬁcult, since
the word size of the EVM is 256-bit. However, both calldata,
as well as execution memory, are byte-addressable memories.
USENIX Association
29th USENIX Security Symposium    2763
As a result, we have to translate between 256- and multiple
8-bit chunks.
n0 ← ∆.write(Γ.m,µ[0],µ[1][31])
. . .
n31 ← ∆.write(n30,µ[0] + 31,µ[1][0])
Γ.m ← n31
We denote the lowest byte of µ[i] with µ[i][0] and the highest
(leftmost) with µ[i][31]. We model MSTORE as a sequence of
32 8-bit writes to execution memory, shifting the address and
the extracted 8-bit sized chunks, accordingly. Reading from
execution memory is done similarly, reading 8-bit chunks
while shifting the read index, concatenating the result.
When modelling CALLDATALOAD, the EVM deﬁnes calldata
as a theoretically unbounded array. Thus when a memory
operation reads out of bound, i.e., a location greater than
calldatasize, the EVM simply ”reads“ zeros. Thus for
every read from calldata, we wrap it in an ite (IF-THEN-
ELSE) operation, which constraints the load to evaluate to
zero when the supplied address reads out of bounds.
4.4.3 Supporting Memcopy- and Memset-Style Instruc-
tions
The EVM offers multiple instructions which behave in a
memcopy-like fashion. We deﬁne the following functions
on ∆:
• ∆.set∞(ni, p,v) (cid:55)→ v: Sets all values in memory ni, start-
• ∆.copy(ni, p,n j,q,s) (cid:55)→ nk: Copies a size s chunk from
node n j, starting at position q until q + s, to node ni,
starting at position p until position p + s
ing at position p, to value v
These functions enable an implementation of memcopy-
style operation and simplify memory initialization. Both stor-
age and execution memory are assumed zero at the start of
their lifetime. Utilizing the set∞ function, we can initialize
these regions. We utilize the Theory-of-Memcopy introduced
by Falke et al. [18] to implement these operations efﬁciently.
This theory extends the Theory-of-Arrays [21] to support
C-style memcopy operations, making the translation to con-
straints possible.
4.5 Modelling Calls
As previously introduced, the EVM offers contracts to interact
with one another. Consider Figure 1 assuming we simulate a
user transaction targeting contract A.
We would ﬁrst setup an ASMA to simulate the execution of
contract A, resulting in an execution tree for A. Now assume
that—during the execution—we encounter a message call to
contract B. We then set up ASMB, run through the entire exe-
cution and then fork the execution tree for each state σi ∈ σh.
This enables us to simulate each possible outcome for the mes-
sage call. Note that this technique can be applied recursively
Figure 1: Message call into another account.
to simulate nested message calls. Similarly, when executing
DELEGATECALL or CALLCODE instructions, we switch the ac-
count’s code and proceed as outlined above. When calling into
another account, the EVM uses part of the execution memory
as input to the new execution. Continuing our running exam-
ple, when executing the message call from ASMA to ASMB,
we create a new calldata node in ∆ and then utilize the copy
function to copy over the input from the execution memory
of ASMA. When execution of ASMB ﬁnishes, we copy over
some of the execution memory from ASMB to ASMA, serving
as return data [64].
4.6 Handling Keccak Instructions
The EVM offers a speciﬁc instruction for computing a keccak-
256 hash over a portion of execution memory. However, these
functions have been proven difﬁcult for static analysis in the
past [56]. One common technique is to use an Ackerman
encoding, used for encoding non-interpretable functions [2].
It exploits the fact that cryptographic hash functions are bind-
ing functions [10], i.e., under the same input the function is
guaranteed to produce the same output. We can leverage this
property as follows:
x = y ⇒ hash(x) = hash(y) ∧
x (cid:54)= y ⇒ hash(x) (cid:54)= hash(y).
(1)
However, since the EVM computes the keccak function over
execution memory, we cannot directly utilize this encoding
for our purpose. When encountering a keccak computation,
we proceed as follows: If all dependent variables and memory
regions are constant, we simply compute the constant hash
value. Otherwise, we replace the outcome with a placeholder
object, which stores a current image of the execution memory,
as well as, the starting and end addresses of the keccak com-
putation. When we want to assess the feasibility of a given
execution path, instead of directly encoding Equation (1) on
the inputs, we encode it for each memory address instead.
More formally, we deﬁne the tuple keccak with three ﬁelds:
(i) keccak.addr, the starting memory address, (ii) keccak.len,
the length of the memory range to be considered, and (iii)
keccak.m, which is the index of the execution memory present
2764    29th USENIX Security Symposium
USENIX Association
Algorithm 1: EncodeKeccak
Input
:Two distinct keccak tuples i and j, the execution constraint
set Π, the memory graph ∆
Output :A modiﬁed constraint set Π(cid:48)
1 begin
2
3
return
if isSymbolic(i.len) or isSymbolic( j.len) then
Π(cid:83) {(i.len = j.len ⇒ i = j )∧ (i.len (cid:54)= j.len ⇒ i (cid:54)= j)}
if i.len (cid:54)= j.len then Π(cid:48) ← Π(cid:83) {i (cid:54)= j}
4
5
6
7
8
9
10
11
12
13
14
15
16
17
else
γ ← (i = j)
for k ∈ 0...i.len do
γ = /0; break
if ∆[i.m, i.addr + k] (cid:54)= ∆[ j.m, j.addr + k] then
cond ← (∆[i.m, i.addr + k] = ∆[ j.m, j.addr + k])
γ ← ite(cond, γ, i (cid:54)= j)
if γ (cid:54)= /0 then Π(cid:48) ← Π(cid:83) {γ}
else Π(cid:48) ← Π(cid:83) {i (cid:54)= j}
return Π(cid:48)
at the time of computation. We encode all possible pairs of
keccak tuples using Algorithm 1. Assume two distinct tu-
ples i and j which we want to translate to ﬁrst-order logic
and add to the path constraints Π. We ﬁrst try to utilize more
sophisticated encodings. However, in cases where we cannot
argue over the len parameter (e.g., one parameter is an uncon-
strained symbolic variable), we resort to a fallback encoding
(line 2-3).
Assuming both i.len and j.len to be constant, we can utilize
a more sophisticated scheme. First, we can trivially disprove
that two values compute to the same hash if i.len (cid:54)= j.len (line
4) and thus simply add i (cid:54)= j to Π. Second, when both values
match (line 6-16), we construct a nested ITE (IF-THEN-ELSE)
expression over the possible memory location (line 9-13) used
for the hash computation. When constructing the encoding, at
each level we check if we can trivially disprove two memory
locations to be equal (line 9), otherwise we can instantly
abort (line 10) and encode both keccak values to be unequal
(line 15). Otherwise we keep iterating along the range of
the len parameter (line 8). Traversing each memory location
(line 9-12), we construct the condition ∆[i.m, i.addr + k] =
∆[ j.m, j.addr + k], encoding that the memory position for
i must be same as j to compute to the same hash. At each
iteration, we assign the true branch of the ITE expression to
the encoding from the previous iteration of the loop, a special
case being the ﬁrst iteration of the loop, where we supply
i = j. Thus, if our backend SMT solver traverses the nested
encoding and it can prove all memory locations to be equal, it
will eventually arrive at the ﬁnal predicate i = j. However, if
it disproves any condition, it arrives at the negated predicate
i (cid:54)= j which we assign at every iteration of the construction.
At ﬁrst glance, requiring that both keccak tuples depend on
constant length parameters might seem like a strong assump-
tion. In practice however, this is often the case, e.g., keccak
values computed over ﬁxed size data structures always have a
ﬁxed length, the same is true for calculating memory offset
for the mapping data type. As introduced in Section 3.1.1,
it gets accessed by a keccak operation. Hence, we addition-
ally extract the key part of the computation to later match
read/writes.
Additionally, when we encounter an equality check with a
constant variable, i.e., keccak == c, where c is constant, we
can immediately conclude that the result must be non-equal.
Otherwise, we would assume that we could calculate hash
collisions. In other words, we would assume that we could
compute the one speciﬁc input, which leads to the (constant)
output of the keccak function. Note: in speciﬁc circumstances
an attacker might know the correct input which generates c;
we elaborate more on this in Section 7.
5 Design and Implementation
We now provide an overview of the architecture of ETHBMC,
a graphical overview is provided in Figure 2. The tool consists
of three main modules, the symbolic executor, a detection