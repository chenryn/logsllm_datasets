hosts wired to 7 switches. Each experiment is repeated until
reaching statistical conÔ¨Ådence to average the results.
VI. PERFORMANCE TESTING EXPERIMENTS
We used the SCP-CLUB framework to test the ONOS plat-
form using a synthetic steady-state workload (intent requests).
To highlight controller bottlenecks, we set up a mock-up
production running over i) 10 SDN switches and 100 hosts
(referred to as small topology), and with ii) 30 SDN switches
and 300 hosts (large). The test environment
includes both
emulated (using Mininet), as well as real SDN switches.
The throughput
is computed by the Data Collector by
counting the number of requests executed in each 1 second
bin. The latency is computed by measuring the average request
execution latency for all the requests executed in the 1 second
bin. Recall that the throughput and latency measurements refer
to the samples generated at the application/NFV level (see
Section II-A, Figure 1), between the sending of an intent
installation/withdrawal request and the receiving of the callback
with the result of the performed operation. Each plot is repeated
until statistical conÔ¨Ådence of 95% is reached.
TELCO CLOUD VMS SPECS USED IN THE TESTING.
TABLE I
ONOS VM Small (S) Medium (M)
2x vCPU
4x vCPU
CPU
Cores
RAM
2
2GB
4
4GB
Large (L)
8x vCPU
8
8GB
Extra large (XL)
16x vCPU
16
16GB
647
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:48:08 UTC from IEEE Xplore.  Restrictions apply. 
TABLE II
COST OF THE INDIVIDUAL TESTING RESOURCES
DISK [GB]
# Cores MEM [GB]
8
8
8
12
4
8
12
20
4
6
10
12
cost/h [$]
0.047
0.047
0.188
0.312
0.156
0.312
0.624
1.14
0.156
0.312
0.624
1.14
Resource
Campaign Manager
Experiment Manager
Data Collector
Topology Manager
Load Generator-small
Load Generator-medium
Load Generator-large
Load Generator-extra-large
Mininet-small
Mininet-medium
Mininet-large
Mininet-extra-large
2
2
4
4
2
4
8
16
2
4
8
16
2
2
4
4
2
4
8
16
2
4
8
16
TABLE III
OVERHEAD SUMMARY REPORT
Run 
Duration 
 Cost / 
Month 
VM 
Size
XL
L
M
S
# VMs in 
ONOS 
cluster
Cost / 
Run [$]
3
5
7
3
5
7
3
5
7
3
5
7
0.81
2.39
4.45
0.47
0.97
1.53
0.36
0.61
1.19
0.32
0.65
1.04
[s]
645
1244
1781
602
862
1031
753
933
1428
1004
1625
2139
Cost / 
hour [$]  Overhead GB Data 
per Run
0.19
4.52
0.62
6.80
0.71
9.08
0.23
2.78
0.39
4.03
5.28
0.45
0.19
1.73
0.46
2.35
0.55
2.98
0.19
1.13
0.30
1.44
1.75
0.32
24%
16%
12%
31%
22%
17%
45%
33%
26%
58%
46%
38%
System Capacity 
[experiments per 
blade / month]
2,381
982
533
4,408
2,238
1,508
5,316
3,304
1,785
5,885
3,003
1,983
[$]
3,259
4,917
6,562
2,014
2,919
3,820
1,252
1,711
2,164
818
1,047
1,273
A. Testing Cost and Overhead
At
the end of each experiment, SCP-CLUB provides a
detailed report on the duration, cost and overhead of all the
performed experiments. Table III shows the average overhead
and average cost report generated by SCP-CLUB for the entire
testing campaign in this paper. In total, we performed 2,240
experiments. Depending on the speciÔ¨Åc conÔ¨Åguration of the
test, on average, the experiments lasted between 645 s and
2,139 s. A total of 70,256 cores, 68 TB of RAM, and 141 TB of
VM disk space were used. Over the conÔ¨Ågured telco cloud, the
entire experiment set took about 7 hours to complete, keeping
an average infrastructure utilization of about 80%. Experiments
generated 748 GB of data, with 0.19 to 0.55 GB of logs
generated per run. The logs provided a generous amount of
data to diagnose causes of potential performance limitations,
as described later in this section.
The overhead is computed as the fraction of the total cost
needed to run the SCP-CLUB VMs excluding the VMs of the
system under test (i.e., the VMs forming the ONOS cluster).
The overhead Ô¨Ågures are estimated taking into account the cost
of each single resource over the cloud infrastructure (e.g., data
store cost/GB and number of processors used), and by using the
logs generated by the Campaign Manager. Costs per resource
units can be conÔ¨Ågured in a YAML Ô¨Åle depending on the
underlying cloud infrastructure.
It is interesting to note in the Table III that the testing
overhead is kept low (12% to 24%) for large deployments (e.g.,
EXTRA-LARGE ONOS VMs), while it substantially increases
for smaller deployments (e.g., SMALL ONOS VM 38% to
56%) because of the smaller footprint (and hence smaller cost)
of the VMs. On average, each test requires 10 to 26 cores, 10
to 20 GB of RAM and 24 to 36 GB of disk for running the
testing plane (i.e., Campaign Manager, Experiment Manager,
Data Store, Topology Manager, Load Generator, and, when
used, the network emulation cluster), depending on the size of
the system under test and on request rates.
The cost per hour and per month are estimated multiplying
the $ cost of each run (obtained as total costs of all VMs x run
duration) by for the total runs executable serially in 1 hour or
1 month. Table III shows that the cost per run ranges between
0.32$ to 4.45$ (818 to 6,562 $ per month, when running 5,855
to 533 tests, respectively) depending on the size of the system
under test.
Another interesting Ô¨Ågure is the system capacity estimation
in Table III, expressed as the number of runs executable per
month over a single telco cloud blade (48 cores in our setup),
considering an 80% utilization of the blade. This Ô¨Ågure is useful
when deciding how many resources to assign to the testing
partition to reach a speciÔ¨Åc testing key performance indicators
(KPIs), e.g., test runs per month.
B. Throughput and Latency
Figures 6 (a)-(d) show 5 minutes of throughput measure-
ments with a steady-state workload of 2,000 intents per second.
The VMs in the system under test are the SMALL, MEDIUM,
LARGE, and EXTRA-LARGE ONOS VMs, each of which is
tested under different cluster conÔ¨Åguration, namely for 1, 3, 5,
and 7 controllers in the cluster.
The measurements show that larger VMs (LARGE AND
EXTRA-LARGE) deliver better throughput than smaller size
VMs. The test is passed only by the LARGE and EXTRA-
LARGE deployments, independently of the number of con-
trollers. Recall that the system under test passes the test when
it can deliver 99% of the requests 95% of the time. SMALL
and MEDIUM deployments never pass the test showing a
throughput well below the load rate of 2,000 requests/s (Figure
6.(a)-(b)). The SMALL deployment is never able to meet the
requested throughput even by adding more controllers to the
cluster. The MEDIUM deployment shows border line perfor-
mance for 3 controllers (Figure 6.(b) - 73% of the requests
served in 95% of the time) still failing the test. The only case
in which the MEDIUM deployment passes the test is when
deployed in a 5 VM cluster conÔ¨Åguration (Figure 6.(c)). An
interesting Ô¨Ånding is that in the 7 VM cluster, the MEDIUM and
LARGE deployments are not able to pass the test. In the 3 VM
cluster conÔ¨Åguration (Figure 6.(c)) the MEDIUM deployment
is able to sustain the load only for about 50s. An in-depth
analysis of the logs showed that the MEDIUM size VM is
close to the breaking point in the 5 VM cluster conÔ¨Åguration,
and that the additional load caused by the synchronization of 7
VMs contributed to a substantial loss of throughput. We found
that most of the synchronization time is spent in identifying
the manager of an intent when processing a withdraw requests.
Recall that the requests are uniformly distributed across each
member of the cluster (Section IV-B), i.e., the requests per VM
are 1/5 (or 1/7) when using a 5 (or a 7) VM cluster, compared
with a single VM cluster. The implication is that there are
conÔ¨Ågurations in which it is better to have smaller clusters of
medium sized VMs, than a large cluster of larger VMs due to
the overhead of the East-West communication, which increases
with the number of controllers in the cluster.
Logs revealed also that the NBI can sometimes become a
performance bottleneck. In particular, we found that in the
version of ONOS used in the tests (v1.10) northbound intent
648
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:48:08 UTC from IEEE Xplore.  Restrictions apply. 
3
2
1
/
]
s
q
e
r
K
[
t
u
p
h
g
u
o
r
h
T
0
0
]
s
m
[
y
c
n
e
t
a
L
106
105
104
103
102
101
0

	
VSCALE
Large Medium Small
ExtraLarge
50
100
150
200
250
300
3
2
1
/
]
s
q
e
r
K
[
t
u
p
h
g
u
o
r
h
T
0
0
Time [s]
(a)  

	
VSCALE
Large Medium Small
ExtraLarge
50
100
150
200
250
300
Time [s]
(e)  
]
s
m
[
y
c
n
e
t
a
L
106
105
104
103
102
101
0
	
	
ExtraLarge
VSCALE
Large Medium Small
50
100
150
200
250
300
Time [s]
(b)
	
	
ExtraLarge
VSCALE
Large Medium Small
3
2
1
/
]
s
q
e
r
K
[
t
u
p
h
g
u
o
r
h
T
0
0
106
105
104
103
102
]
s
m
[
y
c
n
e
t
a
L