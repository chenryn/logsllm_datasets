### VI. PERFORMANCE TESTING EXPERIMENTS

We utilized the SCP-CLUB framework to evaluate the ONOS platform using a synthetic steady-state workload (intent requests). To highlight potential controller bottlenecks, we set up two mock production environments: one with 10 SDN switches and 100 hosts (referred to as the small topology) and another with 30 SDN switches and 300 hosts (referred to as the large topology). The test environment included both emulated setups (using Mininet) and real SDN switches.

#### Throughput and Latency Measurements

The throughput was calculated by the Data Collector, which counted the number of requests executed in each 1-second interval. The latency was determined by measuring the average request execution time for all requests within the same 1-second interval. These measurements were taken at the application/NFV level, specifically between the sending of an intent installation/withdrawal request and the receipt of the callback with the result of the operation. Each experiment was repeated until a 95% statistical confidence level was achieved.

### Telco Cloud VM Specifications Used in Testing

| VM Size | CPU Cores | RAM (GB) |
|---------|-----------|----------|
| Small (S)   | 2         | 2        |
| Medium (M) | 4         | 4        |
| Large (L)  | 8         | 8        |
| Extra Large (XL) | 16       | 16       |

### Cost of Individual Testing Resources

| Resource                         | Disk (GB) | # Cores | Memory (GB) | Cost per Hour ($) |
|----------------------------------|-----------|---------|-------------|-------------------|
| Campaign Manager                 | 8         | 2       | 2           | 0.047             |
| Experiment Manager               | 8         | 2       | 2           | 0.047             |
| Data Collector                   | 8         | 4       | 4           | 0.188             |
| Topology Manager                 | 8         | 4       | 4           | 0.312             |
| Load Generator - Small           | 12        | 4       | 6           | 0.156             |
| Load Generator - Medium          | 12        | 4       | 6           | 0.312             |
| Load Generator - Large           | 20        | 8       | 10          | 0.624             |
| Load Generator - Extra Large     | 20        | 16      | 12          | 1.14              |
| Mininet - Small                  | 8         | 2       | 2           | 0.047             |
| Mininet - Medium                 | 8         | 4       | 4           | 0.188             |
| Mininet - Large                  | 8         | 8       | 8           | 0.312             |
| Mininet - Extra Large            | 8         | 16      | 16          | 1.14              |

### Overhead Summary Report

| Run Duration (s) | Cost per Month ($) | VM Size | # VMs in ONOS Cluster | Cost per Run ($) | Overhead GB Data per Run | System Capacity (experiments per blade/month) |
|------------------|--------------------|---------|-----------------------|------------------|--------------------------|------------------------------------------------|
| 645              | 3,259              | XL      | 3                     | 0.81             | 0.19                     | 2,381                                           |
| 1244             | 4,917              | L       | 5                     | 2.39             | 0.62                     | 982                                             |
| 1781             | 6,562              | M       | 7                     | 4.45             | 0.71                     | 533                                             |
| 602              | 2,014              | S       | 3                     | 0.47             | 0.46                     | 4,408                                           |
| 862              | 2,919              | S       | 5                     | 0.97             | 0.55                     | 2,238                                           |
| 1031             | 3,820              | S       | 7                     | 1.53             | 0.58                     | 1,508                                           |
| 753              | 1,252              | XL      | 3                     | 0.36             | 0.19                     | 5,316                                           |
| 933              | 1,711              | L       | 5                     | 0.61             | 0.23                     | 3,304                                           |
| 1428             | 2,164              | M       | 7                     | 1.19             | 0.30                     | 1,785                                           |
| 1004             | 818                | S       | 3                     | 0.32             | 0.32                     | 5,885                                           |
| 1625             | 1,047              | S       | 5                     | 0.65             | 0.32                     | 3,003                                           |
| 2139             | 1,273              | S       | 7                     | 1.04             | 0.32                     | 1,983                                           |

### A. Testing Cost and Overhead

At the conclusion of each experiment, SCP-CLUB generates a detailed report on the duration, cost, and overhead of all performed experiments. Table III summarizes the average overhead and cost for the entire testing campaign. In total, 2,240 experiments were conducted. Depending on the specific configuration, experiments lasted between 645 seconds and 2,139 seconds. The testing utilized 70,256 cores, 68 TB of RAM, and 141 TB of VM disk space. The entire experiment set took approximately 7 hours to complete, maintaining an average infrastructure utilization of about 80%. Experiments generated 748 GB of data, with 0.19 to 0.55 GB of logs per run. These logs provided extensive data for diagnosing potential performance limitations.

The overhead is calculated as the fraction of the total cost required to run the SCP-CLUB VMs, excluding the VMs of the system under test (i.e., the ONOS cluster VMs). The overhead figures are estimated based on the cost of each resource on the cloud infrastructure and the logs generated by the Campaign Manager. Resource costs can be configured in a YAML file depending on the underlying cloud infrastructure.

Table III shows that the testing overhead is relatively low (12% to 24%) for large deployments (e.g., EXTRA-LARGE ONOS VMs), but it increases significantly for smaller deployments (e.g., SMALL ONOS VMs, 38% to 56%). On average, each test requires 10 to 26 cores, 10 to 20 GB of RAM, and 24 to 36 GB of disk space for running the testing plane, depending on the size of the system under test and the request rates.

The cost per hour and per month is estimated by multiplying the cost of each run (total costs of all VMs x run duration) by the total runs executable serially in 1 hour or 1 month. Table III shows that the cost per run ranges from $0.32 to $4.45 (from $818 to $6,562 per month, when running 5,855 to 533 tests, respectively), depending on the size of the system under test.

Another key figure is the system capacity estimation, expressed as the number of runs executable per month over a single telco cloud blade (48 cores in our setup), considering an 80% utilization of the blade. This figure is useful for determining the resources needed to achieve specific testing KPIs, such as the number of test runs per month.

### B. Throughput and Latency

Figures 6(a)-(d) show 5 minutes of throughput measurements with a steady-state workload of 2,000 intents per second. The VMs in the system under test include SMALL, MEDIUM, LARGE, and EXTRA-LARGE ONOS VMs, each tested under different cluster configurations (1, 3, 5, and 7 controllers).

The results indicate that larger VMs (LARGE and EXTRA-LARGE) deliver better throughput than smaller VMs. The test was passed only by the LARGE and EXTRA-LARGE deployments, regardless of the number of controllers. The system under test passes the test if it can deliver 99% of the requests 95% of the time. SMALL and MEDIUM deployments never passed the test, showing a throughput well below the load rate of 2,000 requests/s (Figures 6(a)-(b)). The SMALL deployment could not meet the requested throughput even with more controllers. The MEDIUM deployment showed borderline performance with 3 controllers (Figure 6(b) - 73% of the requests served in 95% of the time), still failing the test. The MEDIUM deployment passed the test only in a 5 VM cluster configuration (Figure 6(c)). Interestingly, in a 7 VM cluster, the MEDIUM and LARGE deployments did not pass the test. In the 3 VM cluster configuration (Figure 6(c)), the MEDIUM deployment sustained the load for only about 50 seconds. An in-depth analysis of the logs revealed that the MEDIUM size VM was close to its breaking point in the 5 VM cluster configuration, and the additional load caused by the synchronization of 7 VMs contributed to a substantial loss of throughput. Most of the synchronization time was spent identifying the manager of an intent during withdraw requests. The requests were uniformly distributed across each member of the cluster, meaning each VM handled 1/5 (or 1/7) of the requests in a 5 (or 7) VM cluster compared to a single VM cluster. This implies that smaller clusters of medium-sized VMs may be more efficient than larger clusters of larger VMs due to the overhead of East-West communication, which increases with the number of controllers.

Logs also revealed that the Northbound Interface (NBI) can sometimes become a performance bottleneck. Specifically, in the version of ONOS used (v1.10), northbound intent processing was identified as a critical factor.