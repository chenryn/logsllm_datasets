title:How the Web Tangled Itself: Uncovering the History of Client-Side
Web (In)Security
author:Ben Stock and
Martin Johns and
Marius Steffens and
Michael Backes
How the Web Tangled Itself: Uncovering the 
History of Client-Side Web (In)Security
Ben Stock, CISPA, Saarland University; Martin Johns, SAP SE;  
Marius Steffens and Michael Backes, CISPA, Saarland University
https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/stock
This paper is included in the Proceedings of the 26th USENIX Security SymposiumAugust 16–18, 2017 • Vancouver, BC, CanadaISBN 978-1-931971-40-9Open access to the Proceedings of the 26th USENIX Security Symposium is sponsored by USENIXHow the Web Tangled Itself:
Uncovering the History of Client-Side Web (In)Security
Ben Stock
CISPA, Saarland University
Saarland Informatics Campus
Martin Johns
SAP SE
Marius Steffens
CISPA, Saarland University
Saarland Informatics Campus
Michael Backes
CISPA, Saarland University
Saarland Informatics Campus
Abstract
While in its early days, the Web was mostly static, it has
organically grown into a full-ﬂedged technology stack.
This evolution has not followed a security blueprint,
resulting in many classes of vulnerabilities speciﬁc to
the Web. Even though the server-side code of the past
has long since vanished, the Internet Archive gives us a
unique view on the historical development of the Web’s
client side and its (in)security. Uncovering the insights
which fueled this development bears the potential to not
only gain a historical perspective on client-side Web se-
curity, but also to outline better practices going forward.
To that end, we examined the code and header infor-
mation of the most important Web sites for each year
between 1997 and 2016, amounting to 659,710 differ-
ent analyzed Web documents. From the archived data,
we ﬁrst identify key trends in the technology deployed
on the client, such as the increasing complexity of client-
side Web code and the constant rise of multi-origin appli-
cation scenarios. Based on these ﬁndings, we then assess
the advent of corresponding vulnerability classes, inves-
tigate their prevalence over time, and analyze the security
mechanisms developed and deployed to mitigate them.
Correlating these results allows us to draw a set
of overarching conclusions: Along with the dawn of
JavaScript-driven applications in the early years of the
millennium, the likelihood of client-side injection vul-
nerabilities has risen. Furthermore, there is a noticeable
gap in adoption speed between easy-to-deploy security
headers and more involved measures such as CSP. But
there is also no evidence that the usage of the easy-to-
deploy techniques reﬂects on other security areas. On
the contrary, our data shows for instance that sites that
use HTTPonly cookies are actually more likely to have
a Cross-Site Scripting problem. Finally, we observe that
the rising security awareness and introduction of dedi-
cated security technologies had no immediate impact on
the overall security of the client-side Web.
1 A Historic Perspective on Web Security
The Web platform is arguably one of the biggest techno-
logical successes in the area of popular computing. What
modestly started in 1991 as a mere transportation mech-
anism for hypertext documents is now the driving force
behind the majority of today’s dominating technologies.
However, from a security point of view, the Web’s track
record is less than ﬂattering, to a point in which a com-
mon joke under security professionals was to claim that
the term Web security is actually an oxymoron.
Over the years, Web technologies have given birth to
a multitude of novel, Web-speciﬁc vulnerability classes,
such as Cross-Site Scripting (XSS) or Clickjacking,
which simply did not exist before, many of them mani-
festing themselves on the Web’s client side. These ongo-
ing developments are due to the fact that the Web’s client
side is under constant change and expansion. While early
Web pages were mostly styled hypertext documents with
limited interaction, modern Web sites push thousands of
lines of code to the browser and implement non-trivial
application logic. This ongoing development shows no
signs of stopping or even slowing down. The trend is
also underlined by the increase in client-side APIs in the
browser: while in 2006 Firefox featured only 12 APIs, it
now has support for 93 different APIs ranging from accu-
rate timing information to an API to interact with Virtual
Reality devices1. This unrestricted growth led to what
Zalewski [41] dubbed The Tangled Web.
Now, more than 25 years into the life of the Web, it
is worthwhile to take a step back and revisit the devel-
opment of Web security over the years. This allows us
to gain a historical perspective on the security aspects of
an emerging and constantly evolving computing platform
and also foreshadows future trends.
Unfortunately, the majority of Web code is commer-
cial and, thus, not open to the public. Historic server-
1A list of all available features in current browsers is available at
http://caniuse.com/
USENIX Association
26th USENIX Security Symposium    971
side code that has been replaced or taken ofﬂine cannot
be studied anymore. However, the Web’s client side, i.e.,
all Web code that is pushed in the form of HTML or Java-
Script to the browser is public. And thankfully, the Inter-
net Archive has recognized the historical signiﬁcance of
the Web’s public face early on and attempts to preserve
it since 1996.
Thus, while the server-side portion of old Web appli-
cations is probably gone forever, the client-side counter-
part is readily available via the Internet Archive’s Way-
back Machine. This enables a novel approach to histor-
ical security studies: A multitude of Web security prob-
lems, such as Client-Side XSS or Clickjacking, manifest
themselves on the client side exclusively. Hence, evi-
dence of these vulnerabilities is contained in the Internet
Archive and thus available for examination. Many of the
current state-of-the-art security testing methods can be
adapted to work on the archived version of the sites, en-
abling an automated and scalable security evaluation of
the historic code.
Thus, we ﬁnd that the archived client-side Web code
offers the unique opportunity to study the security evo-
lution of one of the most important technology platforms
during (almost) its entire existence, allowing us to con-
duct historical analyses of a plethora of properties of the
Web. This way, we are not only able to investigate past
Web trends, but also draw conclusions on current and fu-
ture Web development trends and (in)security. In the fol-
lowing, we give a brief overview of our conducted study
and outline our research approach.
Technological Evolution of the Web’s Client Side
We ﬁrst examine the evolution of client-side technolo-
gies, i.e., which technologies prevailed in the history of
the Web. We then systematically analyze the archived
code on a syntactical level. The focus of this analysis
step is on observable indicators that provide evidence on
how diversity, complexity, and volume of this code has
developed over the years, as all these three factors have
a direct impact on the likelihood of vulnerabilities. Sec-
tion 3 reports on our ﬁndings in this area. The overall
goal of this activity is to enable the correlation of trends
in the security area with ongoing technological shifts.
Resulting Security Problems With the ever-growing
complexity of the deployed Web code and the con-
stant addition of new powerful capabilities in the Web
browser in the form of novel JavaScript APIs the over-
all amount of potential vulnerability classes has risen as
well. As motivated above, several of the vulnerabilities
which exclusively affect the client side have been prop-
erly archived and, thus, can be reliably detected in the
historical data. We leverage this capability to assess a
lower bound of vulnerable Web sites over the years. Sec-
tion 4 documents our security testing methodology and
highlights our key ﬁndings in the realm of preserved se-
curity vulnerabilites.
Introduction of Dedicated Security Mechanisms To
meet the new challenges of the steadily increasing secu-
rity surface on the Web’s client side, several dedicated
mechanisms, such as security-centric HTTP headers or
JavaScript APIs, have been introduced. We examine if
and how these mechanisms have been adopted during
their lifespan. This provides valuable evidence with re-
spect how the awareness of security issues has changed
over time and if this awareness manifests itself in overall
improvements of the site’s security characteristics. We
discuss the selected mechanisms and the results of our
analysis in Section 5.
Overarching Implications of our Analysis Based on
the ﬁndings of our 20-year-long study, we analyze the
implications of our collected data in Section 6. By look-
ing at historical trends and correlating the individual data
items, we can draw a number of conclusions regard-
ing the interdependencies of client-side technology and
client-side security. Moreover, we investigate correla-
tions between actual vulnerabilities discovered in histori-
cal Web applications and the existence of security aware-
ness indicators at the time, and ﬁnish with a discussion
of important next steps for Client-Side Web security.
2 Methodology
In this section, we present our methodology of using the
Internet Archive as a gateway to the past, allowing us to
investigate the evolution of the Web’s client side (secu-
rity), and outline our technical infrastructure.
2.1 Mining the Internet Archive for Histor-
ical Evidence
To get a view into the client-side Web’s past, the Internet
Archive (https://archive.org) offers a great service:
since 1996, it archives HTML pages, including all re-
sources which are included, such as images, stylesheets,
and scripts. Moreover, for each HTML page, it also
stores the header information initially sent by the remote
server allowing us to even investigate the prevalence of
certain headers over time.
For a thorough view into how the Web’s client side
changed over the years, we speciﬁcally selected the 500
most relevant pages for each year. Given that these are
the most frequented sites of the time, they also had the
greatest interest in securing their sites against attacks.
972    26th USENIX Security Symposium
USENIX Association
For this purpose, we analyzed the sites identiﬁed by
Lerner et al. [19] as the 500 most important sites per year.
For 1996, the Internet Archive only archived copies of
less than half of these sites, though. Therefore, for our
work, we selected the years 1997 to 2016. For each year,
we used the ﬁrst working Internet Archive snapshot of
each domain as an entry point.
Unlike Lerner et al. [19], who investigated the evolu-
tion of tracking, though, we did not restrict our analy-
sis to the start pages of the selected sites. Instead, we
followed the ﬁrst level of links to get a broader cover-
age of the sites.
In doing so, we encountered similar
issues as described in the previous work: several sites
were unavailable in the archive and links often led to
content from a later point in time. To allow for a pre-
cise analysis, we excluded all domains that either had
no snapshot in the Archive for a given year or did not
have any working subpages. Moreover, when crawling
the discovered links, we excluded any that resulted in a
redirect to either a more recent, cached resource or the
live version of the site. Also, when a page redirected
to an older version, we only allowed the date to devi-
ate at most three months from the start page. On aver-
age, this allowed us to consider 422 domains per year 2.
On these domains, we crawled a grand total of 659,710
unique URLs, yielding 1,376,429 frames, 5,440,958 dis-
tinct scripts, and 21,169,634 original HTTP headers for
our analysis. Since the number of domains varies for
each year, throughout this paper we provide fractions
rather than absolute numbers for better comparability.
Threats to Validity Given the nature of the data col-
lected by the Internet Archive, our work faces certain
threats to validity. On the one hand, given the redirection
issues to later versions discussed above, we cannot en-
sure a complete coverage of the analyzed Web site, i.e.,
we might miss a speciﬁc page which carries a vulner-
ability or might not collect an HTTP header only sent
when replying to a certain request, e.g., a session cookie
sent after login. Moreover, since the Archive’s crawler
cannot log into an application, we are unable to analyze
protected parts of a Web site.
The analyses of Client-Side XSS vulnerabilities are
based on the dynamic execution of archived pages, for
which we use a current version of Google Chrome. To
the best of our knowledge, this should not be cause for
over-approximation of our results. On the contrary, Inter-
net Explorer does not automatically encode any part of a
URL when accessed via JavaScript, i.e., especially in the
case of Client-Side Cross-Site Scripting, our results pro-
vide a lower bound of exploitable ﬂaws.
Nevertheless, we believe that the Archive gives us the
2For a full list of domains see https://goo.gl/eXjQfs
Figure 1: Infrastructure overview
unique opportunity to get a glimpse into the state of Web
security over a 20-year time frame. Moreover, several
works from the past that investigate singular issues we
highlight as part of our study conﬁrm our historical ﬁnd-
ings [16, 10, 24, 17, 31, 38].
2.2 Technical Infrastructure
In this section, we brieﬂy explain the technical infras-
tructure used to conduct our study.
Custom Proxy and Crawlers To reduce the load on
the Wayback Archive, we set up our own proxy infras-
tructure. Archive.org adds certain HTML and JavaScript
elements to each cached page to gather statistics. In our
proxy, before persisting the ﬁles to disk, we removed all
these artifacts which would taint our analysis results. The
proxy infrastructure is depicted in Figure 1: for crawling,
we used Google Chrome. The proxy was set up such
that it only allowed access to archived pages. With our
crawlers, we then collected all scripts and all headers sent
from the Archive servers. Note that apart from the reg-
ular HTTP headers, the Archive also sends the original
headers of the site at the time of archiving, preﬁxed with
X-Archive-Orig-, allowing us to collect accurate orig-
inal header information.
Data Storage and Parsing We stored all information
gathered by our crawlers in a central database. For data
analysis, we developed several tools, e.g., to parse header
information. Moreover, to analyze the collected HTML
and JavaScript we employed lightweight static analysis
techniques. To discover relevant HTML elements, e.g.,
object tags, we used Python’s BeautifulSoup to parse
and analyze the HTML. For JavaScript, we developed
a lightweight tool based on esprima and node.js to parse
JavaScript and extract features such as called APIs, pa-
rameter passed to the APIs, or statements contained in
each analyzed ﬁle.
USENIX Association
26th USENIX Security Symposium    973
ProxyDynamic Dataﬂow Analysis To automatically verify
the existence of Client-Side Cross-Site Scripting issues
in the archived pages, we leveraged the techniques we
developed for CCS 2013 [17]. To that end, we ran their
modiﬁed version of Chromium on the cached pages to
gather all data ﬂows from attacker-controllable sources
to dangerous sinks, such as document.write or eval.
Subsequently, we ran an exploit generator to craft URLs
modiﬁed in such a way that they would allow to exploit
a vulnerability. The crawlers were then sent to visit these
exploit candidates. If indeed a vulnerability existed, this
triggered the payload allowing us to automatically verify
the ﬂaw. As this is not a contribution of this work, we
refer the reader to [17] for further details.
3 Evolution of Client-Side Code
In this section, we discuss how client-side active con-
tent evolved over time, showing that JavaScript remains
the only prevailing programming language on the Web’s
client side. While in the beginning of the Web, all content
was merely static and at best linking to other documents,
the Web has changed drastically over the course of the
years. After server-side programming languages such
as PHP enabled designing interactive server-side appli-
cations, at the latest starting with the advent of the so-
called Web 2.0 around 2003, client-side technology be-
came more and more important. To understand how this
client-side technology evolved over time, we analyzed
the HTML pages retrieved from the Internet Archive,
searching speciﬁcally for the most relevant technologies,