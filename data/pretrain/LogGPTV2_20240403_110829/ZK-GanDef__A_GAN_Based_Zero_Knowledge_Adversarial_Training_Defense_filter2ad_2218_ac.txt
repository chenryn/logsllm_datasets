E
z∼Z,s∼S
{−log[qC(z|x)]} −
where J(C,D) =
{−log[qD(s|z = C(x))]}
E
x∼X,t∼T
In words, the training process of the classiﬁer (C) tries to
minimize the log likelihood of predicting s from z, while
maximizing the log likelihood of predicting z from x. At the
same time, the goal of the discriminator (D) is to maximize
the log likelihood of predicting s from z. Recall that, similar to
CLP and CLS [7], ZK-GanDef uses inputs (x) perturbed with
random Gaussian noise as an approximation of true adversarial
examples.
The pseudocode for training of ZK-GanDef is shown in
Algorithm 1. During the sampling in lines 4 and 9, a number
(predeﬁned by user) of examples is evenly sampled from orig-
inal images ¯X and examples with Gaussian perturbations ˆX to
form a training batch. In lines 6 and 11, the weight parameters
in the classiﬁer (discriminator) are frozen before updating the
weight parameters in the discriminator (classiﬁer). Finally, in
lines 7 and 12, the weight parameters are updated through the
stochastic gradient descent algorithm. In this algorithm, we
iteratively update the classiﬁer and the discriminator one at a
time to emulate the proposed minimax game.
Algorithm 1 Training ZK-GanDef
Input: training data X, ground truth T , classiﬁer C, discrim-
Output: classiﬁer C, discriminator D
1: Initialize weight parameters Ω in both classiﬁer and dis-
inator D
criminator
2: for the global training iterations do
3:
4:
5:
for the discriminator training iterations do
Sample a batch of training pair, (cid:104)x, t(cid:105)
Generate a batch of boolean indicator, s, corre-
sponding to training inputs
Fix ΩC in classiﬁer C
Update ΩD in discriminator D
end for
Sample a batch of training pair, (cid:104)x, t(cid:105)
Generate a batch of boolean indicator, s, correspond-
Fix ΩD in discriminator D
Update ΩC in classiﬁer C
ing to training inputs
6:
7:
8:
9:
10:
11:
12:
13: end for
D. Theoretical Analysis
Given that J is a combination of the log likelihood of Z|X
and S|Z, we provide a mathematical intuition here that the
solution of the minimax game is a classiﬁer which makes
correct predictions based on perturbation invariant features.
It is worth noting that our analysis is conducted in a non-
parametric setting, which means that the classiﬁer and the
discriminator have enough capacity to model any distribution.
Proposition 1. If there exists a solution (C∗,D∗) for the
aforementioned minmax game J such that J(C∗,D∗) =
H(Z|X) − H(S),
then C∗ is an optimal classiﬁer which
correctly classiﬁes adversarial inputs.
Proof. For any ﬁxed classiﬁcation model C, the optimal dis-
criminator can be formulated as
D∗ = arg maxD J(C,D)
z∼Z,s∼S
{−log[qD(s|z = C(x))]}
= arg minD
E
In this case, the discriminator can perfectly model the condi-
tional distribution and we have qD(s|z = C(x)) = p(s|z =
C(x)) for all z and all s. Therefore, we can rewrite J with
optimal discriminator as J(cid:48) and denote the second half of J
as a conditional entropy H(S|Z)
J(cid:48)(C) =
E
x∼X,t∼T
{−log[qC(z|x)]} − H(S|Z)
For the optimal classiﬁcation model, the goal is to achieve
the conditional probability qC(z|x) = p(z|x) since z can
determine t by taking softmax transformation. Therefore, the
ﬁrst part of J(cid:48)(C) (the expectation) is larger than or equal
to H(Z|X). Combined with the basic property of conditional
entropy that H(S|Z) ≤ H(S), we can get the following lower
bound of J with optimal classiﬁer and discriminator
J(C∗,D∗) ≥ H(Z|X) − H(S|Z) ≥ H(Z|X) − H(S)
This equality holds when the following two conditions are
satisﬁed:
• The classiﬁer perfectly models the conditional distribu-
tion of z given x, qC(z|x) = p(z|x), which means that
C∗ is an optimal classiﬁer.
• The S and Z are independent, H(S|Z) = H(S), which
means that perturbations do not affect pre-softmax logits.
In practice, the assumption of unlimited capacity in classiﬁer
and discriminator may not hold and it would be hard or
even impossible to build an optimal classiﬁer which outputs
pre-softmax logits independent from adversarial perturbations.
Therefore, we introduce a trade-off hyper-parameter γ into the
minimax function as follows:
{−log[qC(z|x)]} − γ
{−log[qD(s|z = C(x))]}
x∼X,t∼T
When γ = 0, ZK-GanDef is the same as traditional adversarial
training. When γ increases, the discriminator becomes more
and more sensitive to information of s contained in pre-
softmax logits, z.
z∼Z,s∼S
E
E
6
Fig. 3: Evaluation Framework
Based on previous proof, ZK-GanDef achieves feature se-
lection through the design of minimax game with discrimina-
tor. By selecting perturbation invariant features, the classiﬁer
could defend against adversarial examples since they are also
combinations of original image and adversarial perturbations.
IV. EVALUATION SETTINGS
This section presents the framework that we use to evaluate
our defensive method, ZK-GanDef, under different popular
adversarial example generators and compare it with other state-
of-the-art zero knowledge adversarial training defenses. Figure
3 depicts the main components of this framework, which
include: (1) Preprocessing module, (2) Attack module and (3)
Defense module. Different adversarial example generators and
defensive methods could be used as plug-ins to Attack and
Defense modules respectively, to form different test scenarios.
In the following subsections, we present the datasets uti-
lized, the detailed description of each module, and a summary
of the evaluation metrics used.
A. Datasets
During our evaluations, the following datasets are utilized:
• MNIST: Contains a total of 70K images and their labels.
Each one is a 28 × 28 pixel, gray scale labeled image of
handwritten digit.
• Fashion-MNIST: Contains a total of 70K images and their
labels. Each one is a 28 × 28 pixel, gray scale labeled
image of different kinds of clothes.
• CIFAR10: Contains a total of 60K images and their
labels. Each one is a 32 × 32 pixel, RGB labeled image
of animal or vehicle.
The images in each dataset are evenly labeled into 10 different
classes. Although Fashion-MNIST has exactly the same image
size as MNIST, images in Fashion-MNIST have far more
details than images from MNIST.
B. Preprocessing Module
Preprocessing module involves the following operations:
• Scaling: Gray scale images use one integer to represent
each of their pixels, while RGB images use three different
integers (each between 0 and 255) to represent each of
their pixels. To simplify the process of ﬁnding adversarial
examples and to be consistent with the related work,
scaling is used to map pixel representations from discrete
integers in the range Z[0,255] into real numbers in the
range R[−1,1].
into two groups:
• Separation: This operation is used to split each input
dataset
training-dataset and testing-
dataset. The training dataset is used to train the supervised
machine learning models which are the different NN
classiﬁers in this work, while the testing dataset is used
by the attack module to generate adversarial examples
in order to evaluate the NN classiﬁer under test. The
detailed separation plans are: (1) the 70K MNIST and
Fashion-MNIST images are randomly separated into 60K
training and 10K testing images, respectively and (2) the
60K CIFAR10 images are randomly separated into 50K
training and 10K testing images.
• Augmentation: This operation is used to generate aug-
mented examples for different zero knowledge adversarial
training methods. Based on the description in [7] and
our communication with its authors, we keep the same
augmentation which is adding a Gaussian perturbation
with mean µ = 0 and standard deviation σ = 1. The
Gaussian perturbation used in this work is not guaranteed
to be the optimal choice and we keep the detailed
comparison of different augmentation methods as future
work.
C. Attack Module
the FGSM [6],
The attack module implements three popular adversarial
the BIM [9] and the
example generators,
PGD [14]. As we mention in the previous section, all ad-
versarial example generators are utilized under the white-
box scenario. Moreover, each original example has its own
corresponding adversarial counterparts (FGSM, BIM, PGD).
Adversarial examples from same dataset share same maximum
l∞ perturbation limits which are 0.6 in MNIST & Fashion-
MNIST and 0.06 in CIFAR10. For the BIM, we also limit the
per step perturbation to 0.1 in MNIST & Fashion-MNIST and
0.016 in CIFAR10. Finally, for the PGD, we run generation
algorithm 40 iteration with 0.02 per step perturbation on
MNIST & Fashion-MNIST and 20 iteration with 0.016 per
step perturbation on CIFAR10. To ensure the quality of the
adversarial example generators, we choose the standard python
library, CleverHans [17], which is adopted by the community.
D. Defense Module
This module implements the Vanilla NN classiﬁers as well
as the different defense methods that we evaluate in this work.
For the same dataset, different defense methods share the
same structure of the classiﬁer as that of the Vanilla. Hyper-
parameters of defenses we compare with are the exact ones
used in their original papers. Our ZK-GanDef is tuned by line
search to ﬁnd a suitable hyper-parameter setting.
1) Vanilla Classiﬁer: For each dataset, we use as a baseline
a NN classiﬁer with no defenses, which is also referred to as
the Vanilla classiﬁer. We select different Vanilla classiﬁers for
each dataset. The structure of the Vanilla classiﬁer used in
MNIST and Fashion-MNIST dataset is LeNet [14]. For the
Layer
Dense
Dense
Dense
Dense
Kernel Size
Strides
Padding
Activation
32
64
32
1
-
-
-
-
-
-
-
-
ReLU
ReLU
ReLU
Sigmoid
TABLE II: Discriminator Structure
CIFAR10 dataset, we use the allCNN based classiﬁer [23].
Due to space limitations, the detailed NN structure and training
settings are not listed.
2) Zero Knowledge Defenses: We implement here three
different approaches: (1) a classiﬁer trained with CLP [7], (2)
a classiﬁer trained with CLS [7], and (3) a classiﬁer trained
with ZK-GanDef. As Figures 2a and 2b show, CLP and CLS
train only with randomly perturbed examples. On the other
hand, ZK-GanDef (Figure 2c) trains with both original and
randomly perturbed examples. We note also that the structure
of the discriminator in ZK-GanDef (Table II) does not change
with different datasets. Training of the discriminator utilizes
the Adam optimizer [8] with 0.001 learning rate.
3) Full Knowledge Defenses: We implement here three
of the full knowledge defenses: (1) a classiﬁer trained with
original and FGSM examples (FGSM-Adv), (2) a classiﬁer
trained with original and PGD examples (PGD-Adv), and (3)
a classiﬁer trained with original and PGD examples through
GAN based training (PGD-GanDef). Among them, PGD-
Adv is the state-of-the-art full knowledge adversarial training
defense.
E. Evaluation Metrics
The overall classiﬁer performance is captured by the test