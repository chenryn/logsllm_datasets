title:Coarse-grained Scheduling with Software-Defined Networking Switches
author:Myriana Rifai and
Dino Lopez Pacheco and
Guillaume Urvoy-Keller
Coarse-grained Scheduling with Software-Deﬁned
Networking Switches
Myriana Rifai
University Nice Sophia
Antipolis, France
PI:EMAIL
Dino Lopez-Pacheco
University Nice Sophia
Antipolis, France
PI:EMAIL
Guillaume Urvoy-Keller
University Nice Sophia
Antipolis, France
PI:EMAIL
ABSTRACT
Software-Deﬁned Networking (SDN) enables consolidation of the
control plane of a set of network equipments with a ﬁne-grained
control of trafﬁc ﬂows inside the network. In this work, we demon-
strate that some coarse-grained scheduling mechanisms can be eas-
ily offered by SDN switches without requiring any unsupported op-
eration in OpenFlow. We leverage the feedback loop - ﬂow statis-
tics - exposed by SDN switches to the controller, combined with
priority queuing mechanisms, usually available in typical switches
on their output ports. We illustrate our approach through experi-
mentations with an OpenvSwitch SDN switch controlled by a Bea-
con controller.
Categories and Subject Descriptors
C.2.3 [Computer-Communication Networks]: Network Opera-
tions
Keywords
Software-Deﬁned Networking, Size-Based Scheduling
1.
INTRODUCTION
Software-Deﬁned Networking (SDN) offers a centralized con-
trol of the trafﬁc ﬂowing inside a network. The ﬂexibility of the
ﬂow deﬁnition enables implementation of advanced functions like
virtual networks or ﬁrewalls. In contrast, the data plane remains
opaque and cannot be directly inﬂuenced by the controller. Few
works have addressed the extension of SDN to the data-plane with
scheduling in mind. A noticeable exception is [6] where the au-
thors ﬁrst advocate the need to use different scheduling or buffer
management solutions in different scenarios as there is no one-ﬁt-
all solution.
Our objective in this work is to demonstrate that despite the limited
toolbox offered by SDN to directly manipulate the data plane, one
can however implement some form of coarse grained scheduling
with legacy SDN equipments. Our focus is on size-based schedul-
ing [1, 5], where priority is given to ﬂows in their early stage. This
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
SIGCOMM ’15 August 17-21, 2015, London, United Kingdom
c(cid:13) 2015 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-3542-3/15/08.
DOI: http://dx.doi.org/10.1145/2785956.2790004
approach is valuable in the current Internet and data centers net-
works where (i) the bulk of trafﬁc is carried by TCP and (ii) con-
sists of a majority of short ﬂows. Our approach takes advantage of
the feedback loop offered in SDN where a switch exposes to the
controller per rule statistics. The latter enables us to identify long
ﬂows and separate them from short ﬂows using multiple queues per
port that serve to implement 802.1p QoS mechanisms. Our objec-
tive is to minimize ﬂow completion of the majority of ﬂows.
Related work includes [7] where the authors divert some ﬂows on
longer rarely used paths to decrease completion time. In [2], the au-
thors rely on the same capacity of (non SDN) commodity switches
to offer several queues with a tagging process performed at the end
hosts, complemented with ECN in the network. The authors in [4]
propose to improve the performance of TCP in SDN networks, al-
beit at the cost of modifying the end-hosts protocol stack.
In contrast, we investigate a purely network centric approach, with
no modiﬁcation of the end-hosts for SDN networks.
We demonstrate the feasibility of building an SDN based size-based
scheduler using OpenvSwitch with Beacon controller. We further
propose a scalable version of our scheduler to avoid continuous
monitoring of each active ﬂow by the switch and the controller.
2. DESIGN
We have devised two size-based schedulers, a state-full and a
scalable scheduler, and implemented them as applications on top
of a Beacon controller. Both solutions require two queues per port
(802.1p mandates 8 queues per port) and assume that those queues
are managed with a strict priority scheduler where the high priority
queue is served as long as it has packets - the other queue is served
when the ﬁrst one is empty. We next detail the two schedulers:
State-full scheduler: The switch monitors all ongoing ﬂows and
the controller queries every Tmonitor = 10ms ﬂow statistics. Upon
arrival of a new ﬂow, the controller installs a new rule for this ﬂow
and assigns it the high priority queue of the corresponding port. If
a ﬂow has exceeded Tthreshold_pkts = 100 packets, then the controller
modiﬁes the queue used by this ﬂow and assigns it the lower prior-
ity queue.
Scalable scheduler: Continuous monitoring of each active ﬂow
is resource consuming, and installing per-ﬂow rules could quickly
overload the forwarding table of SDN devices. Also, when the load
on the port is low, say lower than 50%, all schedulers typically of-
fer the same performance as queues do not build up. To address
these concerns, we propose a second scheduler where the con-
troller initially sets up one default rule for a set of ﬂows. Then, if
the throughput for that rule exceeds a threshold Tthreshold_utilization =
0.1 × LinkCapacity, then the scheduler zooms in the trafﬁc to sep-
arate large ﬂows from short ﬂows. To zoom in trafﬁc, the scheduler
uninstalls the forwarding rule triggering the mechanism and installs
95per ﬂow rules on trafﬁc demand. Every Tmonitor = 10ms the sched-
uler will check the ﬂow size, if it is more than Tthreshold_pkts = 100
packets it will modify the queue used to the lower priority one. Af-
ter a few Tmonitor cycles, large ﬂows are isolated and the grouped
forwarding rule is reinstalled with a high priority.
3. PRELIMINARY RESULTS
To illustrate our approach, we consider the experimental set-up
described in Figure 1. We have ten clients and one server that acts
as a sink for trafﬁc and an OpenvSwitch (OVS) switch connected to
a Beacon controller. The trafﬁc workload is generated using impt 1
and consists of bulk TCP transfers. The distribution of ﬂow size
follows a bounded Zipf distribution with a ﬂow size between 15
KB (10 packets) and 10 MB. The average ﬂow size is around 100
packets, in line with typical average ﬂow size in the Internet. The
Zipf distribution is the discrete equivalent of a Pareto distribution,
which is known as a reasonable model of Internet and also data
center ﬂow size [3]. The load is controlled by tuning the ﬂow inter-
arrival time, which follows a Poisson process.
Figure 1: Experimental set-up
Figure 2: Flow completion time CDF for long and short ﬂows
We present in Figure 2 results obtained out of 10 experiments
for a load of about 90%. We distinguish between small ﬂows and
large ﬂows, where small ﬂows are deﬁned as ﬂows smaller than the
1http://ipmt.forge.imag.fr/
90-th quantile of the ﬂow size distribution. The 90-th quantile cor-
responds to about 95 packets (close to Tthreshold_pkts). It is important
to note that small ﬂows represent about 15% of the load.
We can observe from Figure 2 that the state-full scheduler offers
the best response time for small ﬂows and for the majority of large
ﬂows as their 100 ﬁrst packets will take advantage of the schedul-
ing mechanism. Only a minority of the largest ﬂows suffer in the
state-full scheduler. As for the scalable scheduler, it offers inter-
mediate results between the state-full scheduler and the absence of
scheduler as it needs 10 additional ms (= Tmonitor) to detect large
ﬂows. As we cannot simultaneously obtain better response times
for all ﬂows, one can observe the longer tails of response times for
the two schedulers as compared to the case with no scheduler.
Though preliminary, those results are encouraging and clearly high-
light the validity of our approach.
4. FUTURE WORK
In terms of future direction, we intend to develop an autonomic
version of the scheduler that would adjust its parameters dynam-
ically. For instance, Tthreshold_pkts could be set as a quantile of the
ﬂow size distribution. Estimating the tail of a highly varying dis-
tribution is complex; however, we are interested in separating large
ﬂows from short ﬂows only, which does not require a high precision
on the computation of quantiles of the distribution. More interest-
ingly, variations in the input distribution or load are easily exposed
through the statistics offered by SDN and we expect to be able to
take advantage of this feature to build a fully adaptable scheduler.
We also would like to explore the use of size-based scheduling in
the data center, where it could be deployed at any switch. Using
size-based scheduling at several bottleneck links has never been
explored so far, to the best or our knowledge.
Last but not least, SDN enables to enlarge our vision of scheduling
as the controller enables us to mix routing and scheduling. This
could be useful for speciﬁc applications such as VM migration, a
crucial operation in modern data centers.
5. REFERENCES
[1] K. Avrachenkov, U. Ayesta, P. Brown, and E. Nyberg.
Differentiation between short and long TCP ﬂows:
Predictability of the response time. In Proceedings of IEEE
INFOCOM 2004, 2004.
[2] W. Bai, K. Chen, H. Wang, L. Chen, D. Han, and C. Tian.
Information-agnostic ﬂow scheduling for commodity data
centers. In 12th USENIX Symposium on Networked Systems
Design and Implementation (NSDI 15), pages 455–468,
Oakland, CA, May 2015. USENIX Association.
[3] T. Benson, A. Anand, A. Akella, and M. Zhang.
Understanding data center trafﬁc characteristics. Computer
Communication Review, 40(1), 2010.
[4] M. Ghobadi, S. H. Yeganeh, and Y. Ganjali. Rethinking
end-to-end congestion control in software-deﬁned networks.
In Proceedings of HotNets-XI, 2012.
[5] I. A. Rai, E. W. Biersack, and G. Urvoy-Keller. Size-based
scheduling to improve the performance of short TCP ﬂows.
IEEE Network, 19(1), 2005.
[6] A. Sivaraman, K. Winstein, S. Subramanian, and
H. Balakrishnan. No silver bullet: Extending sdn to the data
plane. In Proceedings of HotNets-XII, 2013.
[7] F. P. Tso, G. Hamilton, R. Weber, C. S. Perkins, and D. P.
Pezaros. Longer is better: Exploiting path diversity in data
center networks. In Proceedings of IEEE ICDCS ’13, 2013.
SDN ControllerBeaconClient 10Server100 Mb/s –5 ms100 Mb/s –5 msIpmt1.01Ipmt1.01Prio. 1Prio. 2Client 1100 Mb/s –5 ms50200500200010000500000.00.20.40.60.81.0Flow Completion Time (ms)CDFsmall scalable schedulersmall state−full schedulersmall no schedulerlarge scalable schedulerlarge state−full schedulerlarge no scheduler96